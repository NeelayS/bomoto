Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=188, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10528-10583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1076
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987817
Iteration 2/25 | Loss: 0.00987817
Iteration 3/25 | Loss: 0.00351174
Iteration 4/25 | Loss: 0.00206213
Iteration 5/25 | Loss: 0.00187411
Iteration 6/25 | Loss: 0.00183903
Iteration 7/25 | Loss: 0.00167192
Iteration 8/25 | Loss: 0.00163561
Iteration 9/25 | Loss: 0.00160549
Iteration 10/25 | Loss: 0.00157931
Iteration 11/25 | Loss: 0.00156124
Iteration 12/25 | Loss: 0.00154900
Iteration 13/25 | Loss: 0.00155034
Iteration 14/25 | Loss: 0.00155104
Iteration 15/25 | Loss: 0.00154250
Iteration 16/25 | Loss: 0.00153864
Iteration 17/25 | Loss: 0.00153738
Iteration 18/25 | Loss: 0.00153486
Iteration 19/25 | Loss: 0.00153395
Iteration 20/25 | Loss: 0.00153535
Iteration 21/25 | Loss: 0.00153616
Iteration 22/25 | Loss: 0.00153507
Iteration 23/25 | Loss: 0.00153402
Iteration 24/25 | Loss: 0.00153244
Iteration 25/25 | Loss: 0.00153354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38412988
Iteration 2/25 | Loss: 0.00431807
Iteration 3/25 | Loss: 0.00412781
Iteration 4/25 | Loss: 0.00412810
Iteration 5/25 | Loss: 0.00408474
Iteration 6/25 | Loss: 0.00408474
Iteration 7/25 | Loss: 0.00408474
Iteration 8/25 | Loss: 0.00408474
Iteration 9/25 | Loss: 0.00408473
Iteration 10/25 | Loss: 0.00408473
Iteration 11/25 | Loss: 0.00408473
Iteration 12/25 | Loss: 0.00408473
Iteration 13/25 | Loss: 0.00408473
Iteration 14/25 | Loss: 0.00408473
Iteration 15/25 | Loss: 0.00408473
Iteration 16/25 | Loss: 0.00408473
Iteration 17/25 | Loss: 0.00408473
Iteration 18/25 | Loss: 0.00408473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004084733780473471, 0.004084733780473471, 0.004084733780473471, 0.004084733780473471, 0.004084733780473471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004084733780473471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00408473
Iteration 2/1000 | Loss: 0.00120750
Iteration 3/1000 | Loss: 0.00389413
Iteration 4/1000 | Loss: 0.00053121
Iteration 5/1000 | Loss: 0.00106938
Iteration 6/1000 | Loss: 0.00047145
Iteration 7/1000 | Loss: 0.00299257
Iteration 8/1000 | Loss: 0.00437931
Iteration 9/1000 | Loss: 0.00102168
Iteration 10/1000 | Loss: 0.00145232
Iteration 11/1000 | Loss: 0.00044643
Iteration 12/1000 | Loss: 0.00038139
Iteration 13/1000 | Loss: 0.00119997
Iteration 14/1000 | Loss: 0.00229104
Iteration 15/1000 | Loss: 0.00120909
Iteration 16/1000 | Loss: 0.00051540
Iteration 17/1000 | Loss: 0.00113937
Iteration 18/1000 | Loss: 0.00086339
Iteration 19/1000 | Loss: 0.00082752
Iteration 20/1000 | Loss: 0.00068870
Iteration 21/1000 | Loss: 0.00073285
Iteration 22/1000 | Loss: 0.00026504
Iteration 23/1000 | Loss: 0.00096272
Iteration 24/1000 | Loss: 0.00034133
Iteration 25/1000 | Loss: 0.00045055
Iteration 26/1000 | Loss: 0.00056931
Iteration 27/1000 | Loss: 0.00054518
Iteration 28/1000 | Loss: 0.00035996
Iteration 29/1000 | Loss: 0.00047651
Iteration 30/1000 | Loss: 0.00033481
Iteration 31/1000 | Loss: 0.00033729
Iteration 32/1000 | Loss: 0.00089563
Iteration 33/1000 | Loss: 0.00102909
Iteration 34/1000 | Loss: 0.00194345
Iteration 35/1000 | Loss: 0.00053029
Iteration 36/1000 | Loss: 0.00055671
Iteration 37/1000 | Loss: 0.00029454
Iteration 38/1000 | Loss: 0.00053107
Iteration 39/1000 | Loss: 0.00020472
Iteration 40/1000 | Loss: 0.00024180
Iteration 41/1000 | Loss: 0.00084726
Iteration 42/1000 | Loss: 0.00114320
Iteration 43/1000 | Loss: 0.00053235
Iteration 44/1000 | Loss: 0.00044810
Iteration 45/1000 | Loss: 0.00025252
Iteration 46/1000 | Loss: 0.00025107
Iteration 47/1000 | Loss: 0.00019332
Iteration 48/1000 | Loss: 0.00101917
Iteration 49/1000 | Loss: 0.00030643
Iteration 50/1000 | Loss: 0.00026035
Iteration 51/1000 | Loss: 0.00042486
Iteration 52/1000 | Loss: 0.00042925
Iteration 53/1000 | Loss: 0.00026502
Iteration 54/1000 | Loss: 0.00021670
Iteration 55/1000 | Loss: 0.00027423
Iteration 56/1000 | Loss: 0.00068134
Iteration 57/1000 | Loss: 0.00064712
Iteration 58/1000 | Loss: 0.00053727
Iteration 59/1000 | Loss: 0.00051967
Iteration 60/1000 | Loss: 0.00034891
Iteration 61/1000 | Loss: 0.00114991
Iteration 62/1000 | Loss: 0.00047107
Iteration 63/1000 | Loss: 0.00027293
Iteration 64/1000 | Loss: 0.00022591
Iteration 65/1000 | Loss: 0.00021421
Iteration 66/1000 | Loss: 0.00038163
Iteration 67/1000 | Loss: 0.00019865
Iteration 68/1000 | Loss: 0.00019710
Iteration 69/1000 | Loss: 0.00032211
Iteration 70/1000 | Loss: 0.00063979
Iteration 71/1000 | Loss: 0.00018246
Iteration 72/1000 | Loss: 0.00064376
Iteration 73/1000 | Loss: 0.00332780
Iteration 74/1000 | Loss: 0.00038428
Iteration 75/1000 | Loss: 0.00043587
Iteration 76/1000 | Loss: 0.00075503
Iteration 77/1000 | Loss: 0.00075563
Iteration 78/1000 | Loss: 0.00062144
Iteration 79/1000 | Loss: 0.00106288
Iteration 80/1000 | Loss: 0.00057607
Iteration 81/1000 | Loss: 0.00017316
Iteration 82/1000 | Loss: 0.00031408
Iteration 83/1000 | Loss: 0.00052974
Iteration 84/1000 | Loss: 0.00013461
Iteration 85/1000 | Loss: 0.00035050
Iteration 86/1000 | Loss: 0.00065280
Iteration 87/1000 | Loss: 0.00021510
Iteration 88/1000 | Loss: 0.00009962
Iteration 89/1000 | Loss: 0.00029026
Iteration 90/1000 | Loss: 0.00027157
Iteration 91/1000 | Loss: 0.00028803
Iteration 92/1000 | Loss: 0.00024817
Iteration 93/1000 | Loss: 0.00023125
Iteration 94/1000 | Loss: 0.00028369
Iteration 95/1000 | Loss: 0.00015302
Iteration 96/1000 | Loss: 0.00038601
Iteration 97/1000 | Loss: 0.00056637
Iteration 98/1000 | Loss: 0.00044442
Iteration 99/1000 | Loss: 0.00239014
Iteration 100/1000 | Loss: 0.00052211
Iteration 101/1000 | Loss: 0.00010415
Iteration 102/1000 | Loss: 0.00026245
Iteration 103/1000 | Loss: 0.00015904
Iteration 104/1000 | Loss: 0.00026593
Iteration 105/1000 | Loss: 0.00022323
Iteration 106/1000 | Loss: 0.00023886
Iteration 107/1000 | Loss: 0.00046515
Iteration 108/1000 | Loss: 0.00067619
Iteration 109/1000 | Loss: 0.00046119
Iteration 110/1000 | Loss: 0.00040130
Iteration 111/1000 | Loss: 0.00025942
Iteration 112/1000 | Loss: 0.00064195
Iteration 113/1000 | Loss: 0.00052192
Iteration 114/1000 | Loss: 0.00036592
Iteration 115/1000 | Loss: 0.00016682
Iteration 116/1000 | Loss: 0.00011121
Iteration 117/1000 | Loss: 0.00019563
Iteration 118/1000 | Loss: 0.00008956
Iteration 119/1000 | Loss: 0.00026197
Iteration 120/1000 | Loss: 0.00010622
Iteration 121/1000 | Loss: 0.00012586
Iteration 122/1000 | Loss: 0.00017554
Iteration 123/1000 | Loss: 0.00009060
Iteration 124/1000 | Loss: 0.00008752
Iteration 125/1000 | Loss: 0.00013308
Iteration 126/1000 | Loss: 0.00023525
Iteration 127/1000 | Loss: 0.00007991
Iteration 128/1000 | Loss: 0.00010113
Iteration 129/1000 | Loss: 0.00025316
Iteration 130/1000 | Loss: 0.00114615
Iteration 131/1000 | Loss: 0.00024995
Iteration 132/1000 | Loss: 0.00024686
Iteration 133/1000 | Loss: 0.00017889
Iteration 134/1000 | Loss: 0.00010722
Iteration 135/1000 | Loss: 0.00009348
Iteration 136/1000 | Loss: 0.00006511
Iteration 137/1000 | Loss: 0.00016956
Iteration 138/1000 | Loss: 0.00042386
Iteration 139/1000 | Loss: 0.00020978
Iteration 140/1000 | Loss: 0.00007465
Iteration 141/1000 | Loss: 0.00009604
Iteration 142/1000 | Loss: 0.00005495
Iteration 143/1000 | Loss: 0.00026880
Iteration 144/1000 | Loss: 0.00005789
Iteration 145/1000 | Loss: 0.00021497
Iteration 146/1000 | Loss: 0.00073492
Iteration 147/1000 | Loss: 0.00028340
Iteration 148/1000 | Loss: 0.00019853
Iteration 149/1000 | Loss: 0.00009901
Iteration 150/1000 | Loss: 0.00008340
Iteration 151/1000 | Loss: 0.00004780
Iteration 152/1000 | Loss: 0.00004656
Iteration 153/1000 | Loss: 0.00021152
Iteration 154/1000 | Loss: 0.00027532
Iteration 155/1000 | Loss: 0.00015212
Iteration 156/1000 | Loss: 0.00013268
Iteration 157/1000 | Loss: 0.00005905
Iteration 158/1000 | Loss: 0.00005290
Iteration 159/1000 | Loss: 0.00006322
Iteration 160/1000 | Loss: 0.00025725
Iteration 161/1000 | Loss: 0.00005501
Iteration 162/1000 | Loss: 0.00022063
Iteration 163/1000 | Loss: 0.00067554
Iteration 164/1000 | Loss: 0.00009401
Iteration 165/1000 | Loss: 0.00008629
Iteration 166/1000 | Loss: 0.00005981
Iteration 167/1000 | Loss: 0.00005315
Iteration 168/1000 | Loss: 0.00005230
Iteration 169/1000 | Loss: 0.00004809
Iteration 170/1000 | Loss: 0.00020752
Iteration 171/1000 | Loss: 0.00044445
Iteration 172/1000 | Loss: 0.00145731
Iteration 173/1000 | Loss: 0.00012797
Iteration 174/1000 | Loss: 0.00005174
Iteration 175/1000 | Loss: 0.00013197
Iteration 176/1000 | Loss: 0.00051442
Iteration 177/1000 | Loss: 0.00004954
Iteration 178/1000 | Loss: 0.00003913
Iteration 179/1000 | Loss: 0.00023068
Iteration 180/1000 | Loss: 0.00012375
Iteration 181/1000 | Loss: 0.00003672
Iteration 182/1000 | Loss: 0.00003309
Iteration 183/1000 | Loss: 0.00002953
Iteration 184/1000 | Loss: 0.00004268
Iteration 185/1000 | Loss: 0.00002673
Iteration 186/1000 | Loss: 0.00003857
Iteration 187/1000 | Loss: 0.00002554
Iteration 188/1000 | Loss: 0.00003504
Iteration 189/1000 | Loss: 0.00005303
Iteration 190/1000 | Loss: 0.00006231
Iteration 191/1000 | Loss: 0.00002483
Iteration 192/1000 | Loss: 0.00003450
Iteration 193/1000 | Loss: 0.00002446
Iteration 194/1000 | Loss: 0.00002842
Iteration 195/1000 | Loss: 0.00002426
Iteration 196/1000 | Loss: 0.00002795
Iteration 197/1000 | Loss: 0.00002472
Iteration 198/1000 | Loss: 0.00003232
Iteration 199/1000 | Loss: 0.00005548
Iteration 200/1000 | Loss: 0.00002796
Iteration 201/1000 | Loss: 0.00002396
Iteration 202/1000 | Loss: 0.00002394
Iteration 203/1000 | Loss: 0.00002394
Iteration 204/1000 | Loss: 0.00002394
Iteration 205/1000 | Loss: 0.00002393
Iteration 206/1000 | Loss: 0.00002392
Iteration 207/1000 | Loss: 0.00002392
Iteration 208/1000 | Loss: 0.00002391
Iteration 209/1000 | Loss: 0.00002391
Iteration 210/1000 | Loss: 0.00002388
Iteration 211/1000 | Loss: 0.00002387
Iteration 212/1000 | Loss: 0.00002387
Iteration 213/1000 | Loss: 0.00002387
Iteration 214/1000 | Loss: 0.00002386
Iteration 215/1000 | Loss: 0.00003017
Iteration 216/1000 | Loss: 0.00002412
Iteration 217/1000 | Loss: 0.00002444
Iteration 218/1000 | Loss: 0.00002444
Iteration 219/1000 | Loss: 0.00003542
Iteration 220/1000 | Loss: 0.00002377
Iteration 221/1000 | Loss: 0.00004403
Iteration 222/1000 | Loss: 0.00002511
Iteration 223/1000 | Loss: 0.00002375
Iteration 224/1000 | Loss: 0.00002375
Iteration 225/1000 | Loss: 0.00002375
Iteration 226/1000 | Loss: 0.00002375
Iteration 227/1000 | Loss: 0.00002375
Iteration 228/1000 | Loss: 0.00002375
Iteration 229/1000 | Loss: 0.00002375
Iteration 230/1000 | Loss: 0.00002375
Iteration 231/1000 | Loss: 0.00002375
Iteration 232/1000 | Loss: 0.00002375
Iteration 233/1000 | Loss: 0.00002375
Iteration 234/1000 | Loss: 0.00002375
Iteration 235/1000 | Loss: 0.00002375
Iteration 236/1000 | Loss: 0.00002374
Iteration 237/1000 | Loss: 0.00002651
Iteration 238/1000 | Loss: 0.00002373
Iteration 239/1000 | Loss: 0.00002373
Iteration 240/1000 | Loss: 0.00002373
Iteration 241/1000 | Loss: 0.00002373
Iteration 242/1000 | Loss: 0.00002373
Iteration 243/1000 | Loss: 0.00002373
Iteration 244/1000 | Loss: 0.00002373
Iteration 245/1000 | Loss: 0.00002372
Iteration 246/1000 | Loss: 0.00002372
Iteration 247/1000 | Loss: 0.00002372
Iteration 248/1000 | Loss: 0.00002372
Iteration 249/1000 | Loss: 0.00002372
Iteration 250/1000 | Loss: 0.00002372
Iteration 251/1000 | Loss: 0.00002372
Iteration 252/1000 | Loss: 0.00002372
Iteration 253/1000 | Loss: 0.00002372
Iteration 254/1000 | Loss: 0.00002372
Iteration 255/1000 | Loss: 0.00002371
Iteration 256/1000 | Loss: 0.00002371
Iteration 257/1000 | Loss: 0.00002370
Iteration 258/1000 | Loss: 0.00002370
Iteration 259/1000 | Loss: 0.00002562
Iteration 260/1000 | Loss: 0.00002369
Iteration 261/1000 | Loss: 0.00002369
Iteration 262/1000 | Loss: 0.00002369
Iteration 263/1000 | Loss: 0.00002368
Iteration 264/1000 | Loss: 0.00002368
Iteration 265/1000 | Loss: 0.00002368
Iteration 266/1000 | Loss: 0.00002379
Iteration 267/1000 | Loss: 0.00002368
Iteration 268/1000 | Loss: 0.00002368
Iteration 269/1000 | Loss: 0.00002368
Iteration 270/1000 | Loss: 0.00002368
Iteration 271/1000 | Loss: 0.00002368
Iteration 272/1000 | Loss: 0.00002368
Iteration 273/1000 | Loss: 0.00002368
Iteration 274/1000 | Loss: 0.00002368
Iteration 275/1000 | Loss: 0.00002368
Iteration 276/1000 | Loss: 0.00002367
Iteration 277/1000 | Loss: 0.00002367
Iteration 278/1000 | Loss: 0.00002410
Iteration 279/1000 | Loss: 0.00002378
Iteration 280/1000 | Loss: 0.00002377
Iteration 281/1000 | Loss: 0.00002377
Iteration 282/1000 | Loss: 0.00002362
Iteration 283/1000 | Loss: 0.00002362
Iteration 284/1000 | Loss: 0.00002362
Iteration 285/1000 | Loss: 0.00002361
Iteration 286/1000 | Loss: 0.00002361
Iteration 287/1000 | Loss: 0.00002361
Iteration 288/1000 | Loss: 0.00002361
Iteration 289/1000 | Loss: 0.00002361
Iteration 290/1000 | Loss: 0.00002361
Iteration 291/1000 | Loss: 0.00002361
Iteration 292/1000 | Loss: 0.00002360
Iteration 293/1000 | Loss: 0.00002360
Iteration 294/1000 | Loss: 0.00002360
Iteration 295/1000 | Loss: 0.00002360
Iteration 296/1000 | Loss: 0.00002360
Iteration 297/1000 | Loss: 0.00002360
Iteration 298/1000 | Loss: 0.00002360
Iteration 299/1000 | Loss: 0.00002360
Iteration 300/1000 | Loss: 0.00002360
Iteration 301/1000 | Loss: 0.00002360
Iteration 302/1000 | Loss: 0.00002360
Iteration 303/1000 | Loss: 0.00002360
Iteration 304/1000 | Loss: 0.00002360
Iteration 305/1000 | Loss: 0.00002360
Iteration 306/1000 | Loss: 0.00002360
Iteration 307/1000 | Loss: 0.00002360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [2.3602344299433753e-05, 2.3602344299433753e-05, 2.3602344299433753e-05, 2.3602344299433753e-05, 2.3602344299433753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3602344299433753e-05

Optimization complete. Final v2v error: 3.090845823287964 mm

Highest mean error: 11.495357513427734 mm for frame 91

Lowest mean error: 2.554856777191162 mm for frame 128

Saving results

Total time: 10263.033220767975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1031
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489785
Iteration 2/25 | Loss: 0.00130704
Iteration 3/25 | Loss: 0.00118731
Iteration 4/25 | Loss: 0.00117462
Iteration 5/25 | Loss: 0.00117125
Iteration 6/25 | Loss: 0.00117079
Iteration 7/25 | Loss: 0.00117079
Iteration 8/25 | Loss: 0.00117079
Iteration 9/25 | Loss: 0.00117079
Iteration 10/25 | Loss: 0.00117079
Iteration 11/25 | Loss: 0.00117079
Iteration 12/25 | Loss: 0.00117079
Iteration 13/25 | Loss: 0.00117079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011707913363352418, 0.0011707913363352418, 0.0011707913363352418, 0.0011707913363352418, 0.0011707913363352418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011707913363352418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38704932
Iteration 2/25 | Loss: 0.00073718
Iteration 3/25 | Loss: 0.00073717
Iteration 4/25 | Loss: 0.00073717
Iteration 5/25 | Loss: 0.00073717
Iteration 6/25 | Loss: 0.00073717
Iteration 7/25 | Loss: 0.00073717
Iteration 8/25 | Loss: 0.00073717
Iteration 9/25 | Loss: 0.00073717
Iteration 10/25 | Loss: 0.00073717
Iteration 11/25 | Loss: 0.00073717
Iteration 12/25 | Loss: 0.00073717
Iteration 13/25 | Loss: 0.00073717
Iteration 14/25 | Loss: 0.00073717
Iteration 15/25 | Loss: 0.00073717
Iteration 16/25 | Loss: 0.00073717
Iteration 17/25 | Loss: 0.00073717
Iteration 18/25 | Loss: 0.00073717
Iteration 19/25 | Loss: 0.00073717
Iteration 20/25 | Loss: 0.00073717
Iteration 21/25 | Loss: 0.00073717
Iteration 22/25 | Loss: 0.00073717
Iteration 23/25 | Loss: 0.00073717
Iteration 24/25 | Loss: 0.00073717
Iteration 25/25 | Loss: 0.00073717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073717
Iteration 2/1000 | Loss: 0.00004668
Iteration 3/1000 | Loss: 0.00002895
Iteration 4/1000 | Loss: 0.00002394
Iteration 5/1000 | Loss: 0.00002237
Iteration 6/1000 | Loss: 0.00002145
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002033
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001965
Iteration 12/1000 | Loss: 0.00001962
Iteration 13/1000 | Loss: 0.00001959
Iteration 14/1000 | Loss: 0.00001958
Iteration 15/1000 | Loss: 0.00001957
Iteration 16/1000 | Loss: 0.00001956
Iteration 17/1000 | Loss: 0.00001955
Iteration 18/1000 | Loss: 0.00001954
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001951
Iteration 21/1000 | Loss: 0.00001951
Iteration 22/1000 | Loss: 0.00001950
Iteration 23/1000 | Loss: 0.00001949
Iteration 24/1000 | Loss: 0.00001948
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001943
Iteration 29/1000 | Loss: 0.00001943
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001943
Iteration 34/1000 | Loss: 0.00001943
Iteration 35/1000 | Loss: 0.00001943
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001940
Iteration 40/1000 | Loss: 0.00001940
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001937
Iteration 46/1000 | Loss: 0.00001937
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001935
Iteration 51/1000 | Loss: 0.00001935
Iteration 52/1000 | Loss: 0.00001934
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001934
Iteration 55/1000 | Loss: 0.00001933
Iteration 56/1000 | Loss: 0.00001932
Iteration 57/1000 | Loss: 0.00001932
Iteration 58/1000 | Loss: 0.00001930
Iteration 59/1000 | Loss: 0.00001930
Iteration 60/1000 | Loss: 0.00001930
Iteration 61/1000 | Loss: 0.00001930
Iteration 62/1000 | Loss: 0.00001930
Iteration 63/1000 | Loss: 0.00001930
Iteration 64/1000 | Loss: 0.00001930
Iteration 65/1000 | Loss: 0.00001930
Iteration 66/1000 | Loss: 0.00001930
Iteration 67/1000 | Loss: 0.00001930
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00001929
Iteration 70/1000 | Loss: 0.00001929
Iteration 71/1000 | Loss: 0.00001929
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001929
Iteration 74/1000 | Loss: 0.00001929
Iteration 75/1000 | Loss: 0.00001929
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001928
Iteration 78/1000 | Loss: 0.00001927
Iteration 79/1000 | Loss: 0.00001927
Iteration 80/1000 | Loss: 0.00001926
Iteration 81/1000 | Loss: 0.00001926
Iteration 82/1000 | Loss: 0.00001926
Iteration 83/1000 | Loss: 0.00001926
Iteration 84/1000 | Loss: 0.00001926
Iteration 85/1000 | Loss: 0.00001925
Iteration 86/1000 | Loss: 0.00001925
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001925
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001924
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001923
Iteration 95/1000 | Loss: 0.00001922
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001921
Iteration 98/1000 | Loss: 0.00001921
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001920
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001919
Iteration 108/1000 | Loss: 0.00001919
Iteration 109/1000 | Loss: 0.00001919
Iteration 110/1000 | Loss: 0.00001919
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00001919
Iteration 113/1000 | Loss: 0.00001919
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001917
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001917
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001917
Iteration 129/1000 | Loss: 0.00001916
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001916
Iteration 132/1000 | Loss: 0.00001916
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001915
Iteration 135/1000 | Loss: 0.00001915
Iteration 136/1000 | Loss: 0.00001915
Iteration 137/1000 | Loss: 0.00001915
Iteration 138/1000 | Loss: 0.00001915
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001915
Iteration 141/1000 | Loss: 0.00001915
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001915
Iteration 146/1000 | Loss: 0.00001914
Iteration 147/1000 | Loss: 0.00001914
Iteration 148/1000 | Loss: 0.00001914
Iteration 149/1000 | Loss: 0.00001914
Iteration 150/1000 | Loss: 0.00001914
Iteration 151/1000 | Loss: 0.00001914
Iteration 152/1000 | Loss: 0.00001914
Iteration 153/1000 | Loss: 0.00001914
Iteration 154/1000 | Loss: 0.00001914
Iteration 155/1000 | Loss: 0.00001914
Iteration 156/1000 | Loss: 0.00001914
Iteration 157/1000 | Loss: 0.00001914
Iteration 158/1000 | Loss: 0.00001913
Iteration 159/1000 | Loss: 0.00001913
Iteration 160/1000 | Loss: 0.00001913
Iteration 161/1000 | Loss: 0.00001913
Iteration 162/1000 | Loss: 0.00001913
Iteration 163/1000 | Loss: 0.00001913
Iteration 164/1000 | Loss: 0.00001913
Iteration 165/1000 | Loss: 0.00001913
Iteration 166/1000 | Loss: 0.00001913
Iteration 167/1000 | Loss: 0.00001913
Iteration 168/1000 | Loss: 0.00001913
Iteration 169/1000 | Loss: 0.00001913
Iteration 170/1000 | Loss: 0.00001913
Iteration 171/1000 | Loss: 0.00001913
Iteration 172/1000 | Loss: 0.00001912
Iteration 173/1000 | Loss: 0.00001912
Iteration 174/1000 | Loss: 0.00001912
Iteration 175/1000 | Loss: 0.00001912
Iteration 176/1000 | Loss: 0.00001911
Iteration 177/1000 | Loss: 0.00001911
Iteration 178/1000 | Loss: 0.00001911
Iteration 179/1000 | Loss: 0.00001911
Iteration 180/1000 | Loss: 0.00001911
Iteration 181/1000 | Loss: 0.00001911
Iteration 182/1000 | Loss: 0.00001911
Iteration 183/1000 | Loss: 0.00001911
Iteration 184/1000 | Loss: 0.00001911
Iteration 185/1000 | Loss: 0.00001911
Iteration 186/1000 | Loss: 0.00001911
Iteration 187/1000 | Loss: 0.00001911
Iteration 188/1000 | Loss: 0.00001911
Iteration 189/1000 | Loss: 0.00001910
Iteration 190/1000 | Loss: 0.00001910
Iteration 191/1000 | Loss: 0.00001910
Iteration 192/1000 | Loss: 0.00001910
Iteration 193/1000 | Loss: 0.00001910
Iteration 194/1000 | Loss: 0.00001910
Iteration 195/1000 | Loss: 0.00001910
Iteration 196/1000 | Loss: 0.00001910
Iteration 197/1000 | Loss: 0.00001910
Iteration 198/1000 | Loss: 0.00001909
Iteration 199/1000 | Loss: 0.00001909
Iteration 200/1000 | Loss: 0.00001909
Iteration 201/1000 | Loss: 0.00001909
Iteration 202/1000 | Loss: 0.00001909
Iteration 203/1000 | Loss: 0.00001909
Iteration 204/1000 | Loss: 0.00001909
Iteration 205/1000 | Loss: 0.00001909
Iteration 206/1000 | Loss: 0.00001908
Iteration 207/1000 | Loss: 0.00001908
Iteration 208/1000 | Loss: 0.00001908
Iteration 209/1000 | Loss: 0.00001908
Iteration 210/1000 | Loss: 0.00001908
Iteration 211/1000 | Loss: 0.00001908
Iteration 212/1000 | Loss: 0.00001908
Iteration 213/1000 | Loss: 0.00001908
Iteration 214/1000 | Loss: 0.00001908
Iteration 215/1000 | Loss: 0.00001908
Iteration 216/1000 | Loss: 0.00001908
Iteration 217/1000 | Loss: 0.00001908
Iteration 218/1000 | Loss: 0.00001908
Iteration 219/1000 | Loss: 0.00001908
Iteration 220/1000 | Loss: 0.00001908
Iteration 221/1000 | Loss: 0.00001908
Iteration 222/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.9075347154284827e-05, 1.9075347154284827e-05, 1.9075347154284827e-05, 1.9075347154284827e-05, 1.9075347154284827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9075347154284827e-05

Optimization complete. Final v2v error: 3.5402798652648926 mm

Highest mean error: 4.175253391265869 mm for frame 132

Lowest mean error: 3.0352766513824463 mm for frame 1

Saving results

Total time: 895.4044489860535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1068
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781249
Iteration 2/25 | Loss: 0.00154434
Iteration 3/25 | Loss: 0.00119894
Iteration 4/25 | Loss: 0.00116754
Iteration 5/25 | Loss: 0.00116174
Iteration 6/25 | Loss: 0.00116082
Iteration 7/25 | Loss: 0.00116082
Iteration 8/25 | Loss: 0.00116082
Iteration 9/25 | Loss: 0.00116082
Iteration 10/25 | Loss: 0.00116082
Iteration 11/25 | Loss: 0.00116082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011608185013756156, 0.0011608185013756156, 0.0011608185013756156, 0.0011608185013756156, 0.0011608185013756156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011608185013756156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35817933
Iteration 2/25 | Loss: 0.00057610
Iteration 3/25 | Loss: 0.00057609
Iteration 4/25 | Loss: 0.00057609
Iteration 5/25 | Loss: 0.00057609
Iteration 6/25 | Loss: 0.00057609
Iteration 7/25 | Loss: 0.00057609
Iteration 8/25 | Loss: 0.00057609
Iteration 9/25 | Loss: 0.00057609
Iteration 10/25 | Loss: 0.00057609
Iteration 11/25 | Loss: 0.00057609
Iteration 12/25 | Loss: 0.00057609
Iteration 13/25 | Loss: 0.00057609
Iteration 14/25 | Loss: 0.00057609
Iteration 15/25 | Loss: 0.00057609
Iteration 16/25 | Loss: 0.00057609
Iteration 17/25 | Loss: 0.00057609
Iteration 18/25 | Loss: 0.00057609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000576088554225862, 0.000576088554225862, 0.000576088554225862, 0.000576088554225862, 0.000576088554225862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000576088554225862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057609
Iteration 2/1000 | Loss: 0.00002953
Iteration 3/1000 | Loss: 0.00002272
Iteration 4/1000 | Loss: 0.00002104
Iteration 5/1000 | Loss: 0.00002008
Iteration 6/1000 | Loss: 0.00001950
Iteration 7/1000 | Loss: 0.00001922
Iteration 8/1000 | Loss: 0.00001885
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001844
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001820
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001812
Iteration 16/1000 | Loss: 0.00001810
Iteration 17/1000 | Loss: 0.00001809
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001807
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001803
Iteration 28/1000 | Loss: 0.00001802
Iteration 29/1000 | Loss: 0.00001802
Iteration 30/1000 | Loss: 0.00001802
Iteration 31/1000 | Loss: 0.00001802
Iteration 32/1000 | Loss: 0.00001801
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001801
Iteration 38/1000 | Loss: 0.00001801
Iteration 39/1000 | Loss: 0.00001801
Iteration 40/1000 | Loss: 0.00001801
Iteration 41/1000 | Loss: 0.00001801
Iteration 42/1000 | Loss: 0.00001800
Iteration 43/1000 | Loss: 0.00001800
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001800
Iteration 46/1000 | Loss: 0.00001800
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001800
Iteration 49/1000 | Loss: 0.00001800
Iteration 50/1000 | Loss: 0.00001799
Iteration 51/1000 | Loss: 0.00001799
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001799
Iteration 54/1000 | Loss: 0.00001799
Iteration 55/1000 | Loss: 0.00001799
Iteration 56/1000 | Loss: 0.00001798
Iteration 57/1000 | Loss: 0.00001798
Iteration 58/1000 | Loss: 0.00001798
Iteration 59/1000 | Loss: 0.00001798
Iteration 60/1000 | Loss: 0.00001798
Iteration 61/1000 | Loss: 0.00001798
Iteration 62/1000 | Loss: 0.00001798
Iteration 63/1000 | Loss: 0.00001798
Iteration 64/1000 | Loss: 0.00001798
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001797
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001796
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001796
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001796
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001796
Iteration 84/1000 | Loss: 0.00001796
Iteration 85/1000 | Loss: 0.00001796
Iteration 86/1000 | Loss: 0.00001796
Iteration 87/1000 | Loss: 0.00001796
Iteration 88/1000 | Loss: 0.00001796
Iteration 89/1000 | Loss: 0.00001796
Iteration 90/1000 | Loss: 0.00001796
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001795
Iteration 95/1000 | Loss: 0.00001795
Iteration 96/1000 | Loss: 0.00001795
Iteration 97/1000 | Loss: 0.00001795
Iteration 98/1000 | Loss: 0.00001795
Iteration 99/1000 | Loss: 0.00001795
Iteration 100/1000 | Loss: 0.00001795
Iteration 101/1000 | Loss: 0.00001795
Iteration 102/1000 | Loss: 0.00001795
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001794
Iteration 105/1000 | Loss: 0.00001794
Iteration 106/1000 | Loss: 0.00001794
Iteration 107/1000 | Loss: 0.00001794
Iteration 108/1000 | Loss: 0.00001794
Iteration 109/1000 | Loss: 0.00001794
Iteration 110/1000 | Loss: 0.00001794
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001794
Iteration 113/1000 | Loss: 0.00001794
Iteration 114/1000 | Loss: 0.00001794
Iteration 115/1000 | Loss: 0.00001794
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001794
Iteration 120/1000 | Loss: 0.00001794
Iteration 121/1000 | Loss: 0.00001794
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001793
Iteration 125/1000 | Loss: 0.00001793
Iteration 126/1000 | Loss: 0.00001793
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001793
Iteration 129/1000 | Loss: 0.00001793
Iteration 130/1000 | Loss: 0.00001793
Iteration 131/1000 | Loss: 0.00001793
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001792
Iteration 135/1000 | Loss: 0.00001792
Iteration 136/1000 | Loss: 0.00001792
Iteration 137/1000 | Loss: 0.00001792
Iteration 138/1000 | Loss: 0.00001792
Iteration 139/1000 | Loss: 0.00001792
Iteration 140/1000 | Loss: 0.00001792
Iteration 141/1000 | Loss: 0.00001792
Iteration 142/1000 | Loss: 0.00001792
Iteration 143/1000 | Loss: 0.00001791
Iteration 144/1000 | Loss: 0.00001791
Iteration 145/1000 | Loss: 0.00001791
Iteration 146/1000 | Loss: 0.00001791
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001790
Iteration 149/1000 | Loss: 0.00001790
Iteration 150/1000 | Loss: 0.00001790
Iteration 151/1000 | Loss: 0.00001790
Iteration 152/1000 | Loss: 0.00001790
Iteration 153/1000 | Loss: 0.00001790
Iteration 154/1000 | Loss: 0.00001790
Iteration 155/1000 | Loss: 0.00001790
Iteration 156/1000 | Loss: 0.00001790
Iteration 157/1000 | Loss: 0.00001789
Iteration 158/1000 | Loss: 0.00001789
Iteration 159/1000 | Loss: 0.00001789
Iteration 160/1000 | Loss: 0.00001788
Iteration 161/1000 | Loss: 0.00001788
Iteration 162/1000 | Loss: 0.00001788
Iteration 163/1000 | Loss: 0.00001788
Iteration 164/1000 | Loss: 0.00001788
Iteration 165/1000 | Loss: 0.00001788
Iteration 166/1000 | Loss: 0.00001788
Iteration 167/1000 | Loss: 0.00001788
Iteration 168/1000 | Loss: 0.00001788
Iteration 169/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.7881064195535146e-05, 1.7881064195535146e-05, 1.7881064195535146e-05, 1.7881064195535146e-05, 1.7881064195535146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7881064195535146e-05

Optimization complete. Final v2v error: 3.5283637046813965 mm

Highest mean error: 3.675495147705078 mm for frame 174

Lowest mean error: 3.3471944332122803 mm for frame 0

Saving results

Total time: 1211.7195529937744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1081
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975405
Iteration 2/25 | Loss: 0.00256846
Iteration 3/25 | Loss: 0.00204973
Iteration 4/25 | Loss: 0.00191961
Iteration 5/25 | Loss: 0.00181361
Iteration 6/25 | Loss: 0.00170814
Iteration 7/25 | Loss: 0.00161483
Iteration 8/25 | Loss: 0.00156762
Iteration 9/25 | Loss: 0.00152019
Iteration 10/25 | Loss: 0.00147890
Iteration 11/25 | Loss: 0.00143470
Iteration 12/25 | Loss: 0.00142117
Iteration 13/25 | Loss: 0.00141590
Iteration 14/25 | Loss: 0.00140955
Iteration 15/25 | Loss: 0.00140632
Iteration 16/25 | Loss: 0.00140502
Iteration 17/25 | Loss: 0.00140703
Iteration 18/25 | Loss: 0.00140819
Iteration 19/25 | Loss: 0.00140427
Iteration 20/25 | Loss: 0.00140202
Iteration 21/25 | Loss: 0.00140152
Iteration 22/25 | Loss: 0.00140139
Iteration 23/25 | Loss: 0.00140136
Iteration 24/25 | Loss: 0.00140136
Iteration 25/25 | Loss: 0.00140136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36496770
Iteration 2/25 | Loss: 0.00198971
Iteration 3/25 | Loss: 0.00198971
Iteration 4/25 | Loss: 0.00198971
Iteration 5/25 | Loss: 0.00198971
Iteration 6/25 | Loss: 0.00198970
Iteration 7/25 | Loss: 0.00198970
Iteration 8/25 | Loss: 0.00198970
Iteration 9/25 | Loss: 0.00198970
Iteration 10/25 | Loss: 0.00198970
Iteration 11/25 | Loss: 0.00198970
Iteration 12/25 | Loss: 0.00198970
Iteration 13/25 | Loss: 0.00198970
Iteration 14/25 | Loss: 0.00198970
Iteration 15/25 | Loss: 0.00198970
Iteration 16/25 | Loss: 0.00198970
Iteration 17/25 | Loss: 0.00198970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019897031597793102, 0.0019897031597793102, 0.0019897031597793102, 0.0019897031597793102, 0.0019897031597793102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019897031597793102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198970
Iteration 2/1000 | Loss: 0.00032659
Iteration 3/1000 | Loss: 0.00023035
Iteration 4/1000 | Loss: 0.00019961
Iteration 5/1000 | Loss: 0.00018047
Iteration 6/1000 | Loss: 0.00016634
Iteration 7/1000 | Loss: 0.00015545
Iteration 8/1000 | Loss: 0.00014585
Iteration 9/1000 | Loss: 0.00014013
Iteration 10/1000 | Loss: 0.00013555
Iteration 11/1000 | Loss: 0.00012918
Iteration 12/1000 | Loss: 0.00012469
Iteration 13/1000 | Loss: 0.00166118
Iteration 14/1000 | Loss: 0.00582934
Iteration 15/1000 | Loss: 0.00116961
Iteration 16/1000 | Loss: 0.00065625
Iteration 17/1000 | Loss: 0.00029289
Iteration 18/1000 | Loss: 0.00026132
Iteration 19/1000 | Loss: 0.00017886
Iteration 20/1000 | Loss: 0.00011252
Iteration 21/1000 | Loss: 0.00007917
Iteration 22/1000 | Loss: 0.00012505
Iteration 23/1000 | Loss: 0.00006210
Iteration 24/1000 | Loss: 0.00004891
Iteration 25/1000 | Loss: 0.00003945
Iteration 26/1000 | Loss: 0.00003411
Iteration 27/1000 | Loss: 0.00003019
Iteration 28/1000 | Loss: 0.00002790
Iteration 29/1000 | Loss: 0.00003637
Iteration 30/1000 | Loss: 0.00002643
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002749
Iteration 34/1000 | Loss: 0.00002477
Iteration 35/1000 | Loss: 0.00002737
Iteration 36/1000 | Loss: 0.00002240
Iteration 37/1000 | Loss: 0.00002188
Iteration 38/1000 | Loss: 0.00002156
Iteration 39/1000 | Loss: 0.00002144
Iteration 40/1000 | Loss: 0.00002126
Iteration 41/1000 | Loss: 0.00002116
Iteration 42/1000 | Loss: 0.00014533
Iteration 43/1000 | Loss: 0.00002344
Iteration 44/1000 | Loss: 0.00002190
Iteration 45/1000 | Loss: 0.00002082
Iteration 46/1000 | Loss: 0.00002042
Iteration 47/1000 | Loss: 0.00002018
Iteration 48/1000 | Loss: 0.00002004
Iteration 49/1000 | Loss: 0.00002002
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00002001
Iteration 53/1000 | Loss: 0.00002000
Iteration 54/1000 | Loss: 0.00001999
Iteration 55/1000 | Loss: 0.00001999
Iteration 56/1000 | Loss: 0.00001999
Iteration 57/1000 | Loss: 0.00001998
Iteration 58/1000 | Loss: 0.00001998
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001997
Iteration 62/1000 | Loss: 0.00001997
Iteration 63/1000 | Loss: 0.00001997
Iteration 64/1000 | Loss: 0.00001997
Iteration 65/1000 | Loss: 0.00001997
Iteration 66/1000 | Loss: 0.00001997
Iteration 67/1000 | Loss: 0.00001996
Iteration 68/1000 | Loss: 0.00001996
Iteration 69/1000 | Loss: 0.00001996
Iteration 70/1000 | Loss: 0.00001996
Iteration 71/1000 | Loss: 0.00001996
Iteration 72/1000 | Loss: 0.00001995
Iteration 73/1000 | Loss: 0.00001995
Iteration 74/1000 | Loss: 0.00001995
Iteration 75/1000 | Loss: 0.00001995
Iteration 76/1000 | Loss: 0.00001995
Iteration 77/1000 | Loss: 0.00001995
Iteration 78/1000 | Loss: 0.00001995
Iteration 79/1000 | Loss: 0.00001995
Iteration 80/1000 | Loss: 0.00001995
Iteration 81/1000 | Loss: 0.00001995
Iteration 82/1000 | Loss: 0.00001995
Iteration 83/1000 | Loss: 0.00001995
Iteration 84/1000 | Loss: 0.00001995
Iteration 85/1000 | Loss: 0.00001995
Iteration 86/1000 | Loss: 0.00001995
Iteration 87/1000 | Loss: 0.00001995
Iteration 88/1000 | Loss: 0.00001995
Iteration 89/1000 | Loss: 0.00001995
Iteration 90/1000 | Loss: 0.00001995
Iteration 91/1000 | Loss: 0.00001994
Iteration 92/1000 | Loss: 0.00001994
Iteration 93/1000 | Loss: 0.00001994
Iteration 94/1000 | Loss: 0.00001994
Iteration 95/1000 | Loss: 0.00001994
Iteration 96/1000 | Loss: 0.00001994
Iteration 97/1000 | Loss: 0.00001994
Iteration 98/1000 | Loss: 0.00001994
Iteration 99/1000 | Loss: 0.00001994
Iteration 100/1000 | Loss: 0.00001994
Iteration 101/1000 | Loss: 0.00001994
Iteration 102/1000 | Loss: 0.00001994
Iteration 103/1000 | Loss: 0.00001994
Iteration 104/1000 | Loss: 0.00001994
Iteration 105/1000 | Loss: 0.00001994
Iteration 106/1000 | Loss: 0.00001994
Iteration 107/1000 | Loss: 0.00001994
Iteration 108/1000 | Loss: 0.00001994
Iteration 109/1000 | Loss: 0.00001994
Iteration 110/1000 | Loss: 0.00001994
Iteration 111/1000 | Loss: 0.00001994
Iteration 112/1000 | Loss: 0.00001994
Iteration 113/1000 | Loss: 0.00001994
Iteration 114/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.994346166611649e-05, 1.994346166611649e-05, 1.994346166611649e-05, 1.994346166611649e-05, 1.994346166611649e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.994346166611649e-05

Optimization complete. Final v2v error: 3.773181438446045 mm

Highest mean error: 4.7455902099609375 mm for frame 13

Lowest mean error: 3.653970241546631 mm for frame 87

Saving results

Total time: 3815.758220911026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1035
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00220029
Iteration 2/25 | Loss: 0.00112075
Iteration 3/25 | Loss: 0.00104896
Iteration 4/25 | Loss: 0.00103078
Iteration 5/25 | Loss: 0.00102372
Iteration 6/25 | Loss: 0.00102155
Iteration 7/25 | Loss: 0.00102071
Iteration 8/25 | Loss: 0.00102071
Iteration 9/25 | Loss: 0.00102071
Iteration 10/25 | Loss: 0.00102071
Iteration 11/25 | Loss: 0.00102071
Iteration 12/25 | Loss: 0.00102071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010207053273916245, 0.0010207053273916245, 0.0010207053273916245, 0.0010207053273916245, 0.0010207053273916245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010207053273916245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34044313
Iteration 2/25 | Loss: 0.00098724
Iteration 3/25 | Loss: 0.00098724
Iteration 4/25 | Loss: 0.00098723
Iteration 5/25 | Loss: 0.00098723
Iteration 6/25 | Loss: 0.00098723
Iteration 7/25 | Loss: 0.00098723
Iteration 8/25 | Loss: 0.00098723
Iteration 9/25 | Loss: 0.00098723
Iteration 10/25 | Loss: 0.00098723
Iteration 11/25 | Loss: 0.00098723
Iteration 12/25 | Loss: 0.00098723
Iteration 13/25 | Loss: 0.00098723
Iteration 14/25 | Loss: 0.00098723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009872333612293005, 0.0009872333612293005, 0.0009872333612293005, 0.0009872333612293005, 0.0009872333612293005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009872333612293005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098723
Iteration 2/1000 | Loss: 0.00003765
Iteration 3/1000 | Loss: 0.00002200
Iteration 4/1000 | Loss: 0.00001643
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001357
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001250
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001219
Iteration 11/1000 | Loss: 0.00001214
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001167
Iteration 19/1000 | Loss: 0.00001167
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001155
Iteration 26/1000 | Loss: 0.00001154
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00001154
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001154
Iteration 31/1000 | Loss: 0.00001153
Iteration 32/1000 | Loss: 0.00001153
Iteration 33/1000 | Loss: 0.00001153
Iteration 34/1000 | Loss: 0.00001153
Iteration 35/1000 | Loss: 0.00001152
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001148
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001144
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001143
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001141
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001138
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001135
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001134
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001131
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001130
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001130
Iteration 149/1000 | Loss: 0.00001130
Iteration 150/1000 | Loss: 0.00001130
Iteration 151/1000 | Loss: 0.00001130
Iteration 152/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.130118289438542e-05, 1.130118289438542e-05, 1.130118289438542e-05, 1.130118289438542e-05, 1.130118289438542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.130118289438542e-05

Optimization complete. Final v2v error: 2.886500597000122 mm

Highest mean error: 3.3055338859558105 mm for frame 43

Lowest mean error: 2.6502959728240967 mm for frame 188

Saving results

Total time: 1059.0892009735107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1026
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969850
Iteration 2/25 | Loss: 0.00969850
Iteration 3/25 | Loss: 0.00297079
Iteration 4/25 | Loss: 0.00238114
Iteration 5/25 | Loss: 0.00249206
Iteration 6/25 | Loss: 0.00191124
Iteration 7/25 | Loss: 0.00183547
Iteration 8/25 | Loss: 0.00181135
Iteration 9/25 | Loss: 0.00180851
Iteration 10/25 | Loss: 0.00180167
Iteration 11/25 | Loss: 0.00179698
Iteration 12/25 | Loss: 0.00179087
Iteration 13/25 | Loss: 0.00178845
Iteration 14/25 | Loss: 0.00178816
Iteration 15/25 | Loss: 0.00179077
Iteration 16/25 | Loss: 0.00178790
Iteration 17/25 | Loss: 0.00178769
Iteration 18/25 | Loss: 0.00178731
Iteration 19/25 | Loss: 0.00178952
Iteration 20/25 | Loss: 0.00178925
Iteration 21/25 | Loss: 0.00178718
Iteration 22/25 | Loss: 0.00178715
Iteration 23/25 | Loss: 0.00178715
Iteration 24/25 | Loss: 0.00178715
Iteration 25/25 | Loss: 0.00178715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32613266
Iteration 2/25 | Loss: 0.00754695
Iteration 3/25 | Loss: 0.00384903
Iteration 4/25 | Loss: 0.00384853
Iteration 5/25 | Loss: 0.00384853
Iteration 6/25 | Loss: 0.00384853
Iteration 7/25 | Loss: 0.00384853
Iteration 8/25 | Loss: 0.00384853
Iteration 9/25 | Loss: 0.00384853
Iteration 10/25 | Loss: 0.00384853
Iteration 11/25 | Loss: 0.00384853
Iteration 12/25 | Loss: 0.00384853
Iteration 13/25 | Loss: 0.00384853
Iteration 14/25 | Loss: 0.00384853
Iteration 15/25 | Loss: 0.00384853
Iteration 16/25 | Loss: 0.00384853
Iteration 17/25 | Loss: 0.00384853
Iteration 18/25 | Loss: 0.00384853
Iteration 19/25 | Loss: 0.00384853
Iteration 20/25 | Loss: 0.00384853
Iteration 21/25 | Loss: 0.00384853
Iteration 22/25 | Loss: 0.00384853
Iteration 23/25 | Loss: 0.00384853
Iteration 24/25 | Loss: 0.00384853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003848529886454344, 0.003848529886454344, 0.003848529886454344, 0.003848529886454344, 0.003848529886454344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003848529886454344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00384853
Iteration 2/1000 | Loss: 0.00320168
Iteration 3/1000 | Loss: 0.00113969
Iteration 4/1000 | Loss: 0.00100024
Iteration 5/1000 | Loss: 0.00608299
Iteration 6/1000 | Loss: 0.00155096
Iteration 7/1000 | Loss: 0.00040157
Iteration 8/1000 | Loss: 0.00043856
Iteration 9/1000 | Loss: 0.00036565
Iteration 10/1000 | Loss: 0.00031590
Iteration 11/1000 | Loss: 0.00033852
Iteration 12/1000 | Loss: 0.00141036
Iteration 13/1000 | Loss: 0.01119900
Iteration 14/1000 | Loss: 0.00950482
Iteration 15/1000 | Loss: 0.00179773
Iteration 16/1000 | Loss: 0.00088219
Iteration 17/1000 | Loss: 0.00181344
Iteration 18/1000 | Loss: 0.00238471
Iteration 19/1000 | Loss: 0.00074101
Iteration 20/1000 | Loss: 0.00094857
Iteration 21/1000 | Loss: 0.00036150
Iteration 22/1000 | Loss: 0.00155746
Iteration 23/1000 | Loss: 0.00156984
Iteration 24/1000 | Loss: 0.00068974
Iteration 25/1000 | Loss: 0.00073049
Iteration 26/1000 | Loss: 0.00005370
Iteration 27/1000 | Loss: 0.00163927
Iteration 28/1000 | Loss: 0.00065733
Iteration 29/1000 | Loss: 0.00085738
Iteration 30/1000 | Loss: 0.00015335
Iteration 31/1000 | Loss: 0.00019572
Iteration 32/1000 | Loss: 0.00079511
Iteration 33/1000 | Loss: 0.00015705
Iteration 34/1000 | Loss: 0.00064643
Iteration 35/1000 | Loss: 0.00224522
Iteration 36/1000 | Loss: 0.00020043
Iteration 37/1000 | Loss: 0.00310101
Iteration 38/1000 | Loss: 0.00158229
Iteration 39/1000 | Loss: 0.00146967
Iteration 40/1000 | Loss: 0.00105873
Iteration 41/1000 | Loss: 0.00128369
Iteration 42/1000 | Loss: 0.00024490
Iteration 43/1000 | Loss: 0.00012195
Iteration 44/1000 | Loss: 0.00005922
Iteration 45/1000 | Loss: 0.00004212
Iteration 46/1000 | Loss: 0.00006622
Iteration 47/1000 | Loss: 0.00003280
Iteration 48/1000 | Loss: 0.00020428
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00023154
Iteration 51/1000 | Loss: 0.00005795
Iteration 52/1000 | Loss: 0.00014437
Iteration 53/1000 | Loss: 0.00018925
Iteration 54/1000 | Loss: 0.00003150
Iteration 55/1000 | Loss: 0.00005953
Iteration 56/1000 | Loss: 0.00001952
Iteration 57/1000 | Loss: 0.00014260
Iteration 58/1000 | Loss: 0.00008258
Iteration 59/1000 | Loss: 0.00006142
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00011823
Iteration 62/1000 | Loss: 0.00003016
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00003109
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001581
Iteration 68/1000 | Loss: 0.00001577
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00011971
Iteration 71/1000 | Loss: 0.00109035
Iteration 72/1000 | Loss: 0.00581436
Iteration 73/1000 | Loss: 0.00595573
Iteration 74/1000 | Loss: 0.00743042
Iteration 75/1000 | Loss: 0.00441222
Iteration 76/1000 | Loss: 0.00291296
Iteration 77/1000 | Loss: 0.00025206
Iteration 78/1000 | Loss: 0.00146154
Iteration 79/1000 | Loss: 0.00188000
Iteration 80/1000 | Loss: 0.00179215
Iteration 81/1000 | Loss: 0.00095680
Iteration 82/1000 | Loss: 0.00125324
Iteration 83/1000 | Loss: 0.00134370
Iteration 84/1000 | Loss: 0.00076561
Iteration 85/1000 | Loss: 0.00128389
Iteration 86/1000 | Loss: 0.00029283
Iteration 87/1000 | Loss: 0.00028749
Iteration 88/1000 | Loss: 0.00008114
Iteration 89/1000 | Loss: 0.00017710
Iteration 90/1000 | Loss: 0.00006180
Iteration 91/1000 | Loss: 0.00053665
Iteration 92/1000 | Loss: 0.00043785
Iteration 93/1000 | Loss: 0.00011008
Iteration 94/1000 | Loss: 0.00060198
Iteration 95/1000 | Loss: 0.00133680
Iteration 96/1000 | Loss: 0.00045065
Iteration 97/1000 | Loss: 0.00043223
Iteration 98/1000 | Loss: 0.00038552
Iteration 99/1000 | Loss: 0.00080586
Iteration 100/1000 | Loss: 0.00270659
Iteration 101/1000 | Loss: 0.00110216
Iteration 102/1000 | Loss: 0.00128813
Iteration 103/1000 | Loss: 0.00102122
Iteration 104/1000 | Loss: 0.00036579
Iteration 105/1000 | Loss: 0.00093798
Iteration 106/1000 | Loss: 0.00026411
Iteration 107/1000 | Loss: 0.00008302
Iteration 108/1000 | Loss: 0.00045639
Iteration 109/1000 | Loss: 0.00023458
Iteration 110/1000 | Loss: 0.00007158
Iteration 111/1000 | Loss: 0.00036160
Iteration 112/1000 | Loss: 0.00005114
Iteration 113/1000 | Loss: 0.00011219
Iteration 114/1000 | Loss: 0.00033553
Iteration 115/1000 | Loss: 0.00042083
Iteration 116/1000 | Loss: 0.00026104
Iteration 117/1000 | Loss: 0.00040339
Iteration 118/1000 | Loss: 0.00032416
Iteration 119/1000 | Loss: 0.00072847
Iteration 120/1000 | Loss: 0.00123098
Iteration 121/1000 | Loss: 0.00097973
Iteration 122/1000 | Loss: 0.00007649
Iteration 123/1000 | Loss: 0.00066546
Iteration 124/1000 | Loss: 0.00030649
Iteration 125/1000 | Loss: 0.00129210
Iteration 126/1000 | Loss: 0.00004100
Iteration 127/1000 | Loss: 0.00011041
Iteration 128/1000 | Loss: 0.00005679
Iteration 129/1000 | Loss: 0.00012209
Iteration 130/1000 | Loss: 0.00001996
Iteration 131/1000 | Loss: 0.00001764
Iteration 132/1000 | Loss: 0.00004282
Iteration 133/1000 | Loss: 0.00001664
Iteration 134/1000 | Loss: 0.00003259
Iteration 135/1000 | Loss: 0.00001621
Iteration 136/1000 | Loss: 0.00001598
Iteration 137/1000 | Loss: 0.00001594
Iteration 138/1000 | Loss: 0.00004256
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001551
Iteration 141/1000 | Loss: 0.00018779
Iteration 142/1000 | Loss: 0.00002117
Iteration 143/1000 | Loss: 0.00010235
Iteration 144/1000 | Loss: 0.00001610
Iteration 145/1000 | Loss: 0.00001523
Iteration 146/1000 | Loss: 0.00001492
Iteration 147/1000 | Loss: 0.00015157
Iteration 148/1000 | Loss: 0.00002065
Iteration 149/1000 | Loss: 0.00001454
Iteration 150/1000 | Loss: 0.00001453
Iteration 151/1000 | Loss: 0.00020591
Iteration 152/1000 | Loss: 0.00037325
Iteration 153/1000 | Loss: 0.00004301
Iteration 154/1000 | Loss: 0.00001433
Iteration 155/1000 | Loss: 0.00001367
Iteration 156/1000 | Loss: 0.00009971
Iteration 157/1000 | Loss: 0.00097229
Iteration 158/1000 | Loss: 0.00051581
Iteration 159/1000 | Loss: 0.00023557
Iteration 160/1000 | Loss: 0.00004347
Iteration 161/1000 | Loss: 0.00035003
Iteration 162/1000 | Loss: 0.00028090
Iteration 163/1000 | Loss: 0.00003589
Iteration 164/1000 | Loss: 0.00001845
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00003775
Iteration 167/1000 | Loss: 0.00003266
Iteration 168/1000 | Loss: 0.00001696
Iteration 169/1000 | Loss: 0.00001270
Iteration 170/1000 | Loss: 0.00001270
Iteration 171/1000 | Loss: 0.00001267
Iteration 172/1000 | Loss: 0.00001266
Iteration 173/1000 | Loss: 0.00001266
Iteration 174/1000 | Loss: 0.00001265
Iteration 175/1000 | Loss: 0.00001265
Iteration 176/1000 | Loss: 0.00001264
Iteration 177/1000 | Loss: 0.00001264
Iteration 178/1000 | Loss: 0.00001263
Iteration 179/1000 | Loss: 0.00001263
Iteration 180/1000 | Loss: 0.00001262
Iteration 181/1000 | Loss: 0.00001261
Iteration 182/1000 | Loss: 0.00001261
Iteration 183/1000 | Loss: 0.00001260
Iteration 184/1000 | Loss: 0.00001260
Iteration 185/1000 | Loss: 0.00004835
Iteration 186/1000 | Loss: 0.00001269
Iteration 187/1000 | Loss: 0.00001257
Iteration 188/1000 | Loss: 0.00001256
Iteration 189/1000 | Loss: 0.00001256
Iteration 190/1000 | Loss: 0.00001256
Iteration 191/1000 | Loss: 0.00001256
Iteration 192/1000 | Loss: 0.00001256
Iteration 193/1000 | Loss: 0.00001256
Iteration 194/1000 | Loss: 0.00001256
Iteration 195/1000 | Loss: 0.00001256
Iteration 196/1000 | Loss: 0.00001256
Iteration 197/1000 | Loss: 0.00001256
Iteration 198/1000 | Loss: 0.00001256
Iteration 199/1000 | Loss: 0.00001256
Iteration 200/1000 | Loss: 0.00001256
Iteration 201/1000 | Loss: 0.00001255
Iteration 202/1000 | Loss: 0.00001255
Iteration 203/1000 | Loss: 0.00001255
Iteration 204/1000 | Loss: 0.00001255
Iteration 205/1000 | Loss: 0.00001255
Iteration 206/1000 | Loss: 0.00001255
Iteration 207/1000 | Loss: 0.00001255
Iteration 208/1000 | Loss: 0.00001255
Iteration 209/1000 | Loss: 0.00001255
Iteration 210/1000 | Loss: 0.00001254
Iteration 211/1000 | Loss: 0.00001254
Iteration 212/1000 | Loss: 0.00001254
Iteration 213/1000 | Loss: 0.00001254
Iteration 214/1000 | Loss: 0.00001254
Iteration 215/1000 | Loss: 0.00001254
Iteration 216/1000 | Loss: 0.00001254
Iteration 217/1000 | Loss: 0.00001254
Iteration 218/1000 | Loss: 0.00001254
Iteration 219/1000 | Loss: 0.00001254
Iteration 220/1000 | Loss: 0.00001254
Iteration 221/1000 | Loss: 0.00001254
Iteration 222/1000 | Loss: 0.00001254
Iteration 223/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.2540540410554968e-05, 1.2540540410554968e-05, 1.2540540410554968e-05, 1.2540540410554968e-05, 1.2540540410554968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2540540410554968e-05

Optimization complete. Final v2v error: 3.036595582962036 mm

Highest mean error: 5.68772554397583 mm for frame 69

Lowest mean error: 2.7908241748809814 mm for frame 33

Saving results

Total time: 8330.969372987747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1083
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999667
Iteration 2/25 | Loss: 0.00167944
Iteration 3/25 | Loss: 0.00123308
Iteration 4/25 | Loss: 0.00116123
Iteration 5/25 | Loss: 0.00114192
Iteration 6/25 | Loss: 0.00115719
Iteration 7/25 | Loss: 0.00114308
Iteration 8/25 | Loss: 0.00113443
Iteration 9/25 | Loss: 0.00112272
Iteration 10/25 | Loss: 0.00111902
Iteration 11/25 | Loss: 0.00111806
Iteration 12/25 | Loss: 0.00111859
Iteration 13/25 | Loss: 0.00111932
Iteration 14/25 | Loss: 0.00111687
Iteration 15/25 | Loss: 0.00111522
Iteration 16/25 | Loss: 0.00111637
Iteration 17/25 | Loss: 0.00111563
Iteration 18/25 | Loss: 0.00111342
Iteration 19/25 | Loss: 0.00111556
Iteration 20/25 | Loss: 0.00111514
Iteration 21/25 | Loss: 0.00111004
Iteration 22/25 | Loss: 0.00110838
Iteration 23/25 | Loss: 0.00110829
Iteration 24/25 | Loss: 0.00110516
Iteration 25/25 | Loss: 0.00110570

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45380521
Iteration 2/25 | Loss: 0.00071937
Iteration 3/25 | Loss: 0.00071937
Iteration 4/25 | Loss: 0.00071937
Iteration 5/25 | Loss: 0.00071937
Iteration 6/25 | Loss: 0.00071937
Iteration 7/25 | Loss: 0.00071937
Iteration 8/25 | Loss: 0.00071937
Iteration 9/25 | Loss: 0.00071937
Iteration 10/25 | Loss: 0.00071937
Iteration 11/25 | Loss: 0.00071937
Iteration 12/25 | Loss: 0.00071937
Iteration 13/25 | Loss: 0.00071937
Iteration 14/25 | Loss: 0.00071937
Iteration 15/25 | Loss: 0.00071937
Iteration 16/25 | Loss: 0.00071937
Iteration 17/25 | Loss: 0.00071937
Iteration 18/25 | Loss: 0.00071937
Iteration 19/25 | Loss: 0.00071937
Iteration 20/25 | Loss: 0.00071937
Iteration 21/25 | Loss: 0.00071937
Iteration 22/25 | Loss: 0.00071937
Iteration 23/25 | Loss: 0.00071937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000719365372788161, 0.000719365372788161, 0.000719365372788161, 0.000719365372788161, 0.000719365372788161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000719365372788161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071937
Iteration 2/1000 | Loss: 0.00003075
Iteration 3/1000 | Loss: 0.00002197
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00002936
Iteration 7/1000 | Loss: 0.00002356
Iteration 8/1000 | Loss: 0.00002925
Iteration 9/1000 | Loss: 0.00003178
Iteration 10/1000 | Loss: 0.00002662
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00002186
Iteration 13/1000 | Loss: 0.00001991
Iteration 14/1000 | Loss: 0.00002680
Iteration 15/1000 | Loss: 0.00002929
Iteration 16/1000 | Loss: 0.00002586
Iteration 17/1000 | Loss: 0.00002272
Iteration 18/1000 | Loss: 0.00002486
Iteration 19/1000 | Loss: 0.00002184
Iteration 20/1000 | Loss: 0.00002633
Iteration 21/1000 | Loss: 0.00002581
Iteration 22/1000 | Loss: 0.00002369
Iteration 23/1000 | Loss: 0.00002925
Iteration 24/1000 | Loss: 0.00002356
Iteration 25/1000 | Loss: 0.00002356
Iteration 26/1000 | Loss: 0.00002659
Iteration 27/1000 | Loss: 0.00002323
Iteration 28/1000 | Loss: 0.00002357
Iteration 29/1000 | Loss: 0.00002665
Iteration 30/1000 | Loss: 0.00004050
Iteration 31/1000 | Loss: 0.00002599
Iteration 32/1000 | Loss: 0.00002598
Iteration 33/1000 | Loss: 0.00002009
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001455
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001400
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001389
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001388
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001385
Iteration 48/1000 | Loss: 0.00001385
Iteration 49/1000 | Loss: 0.00001371
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001351
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001342
Iteration 75/1000 | Loss: 0.00001342
Iteration 76/1000 | Loss: 0.00001342
Iteration 77/1000 | Loss: 0.00001342
Iteration 78/1000 | Loss: 0.00001342
Iteration 79/1000 | Loss: 0.00001342
Iteration 80/1000 | Loss: 0.00001342
Iteration 81/1000 | Loss: 0.00001342
Iteration 82/1000 | Loss: 0.00001342
Iteration 83/1000 | Loss: 0.00001342
Iteration 84/1000 | Loss: 0.00001342
Iteration 85/1000 | Loss: 0.00001341
Iteration 86/1000 | Loss: 0.00001341
Iteration 87/1000 | Loss: 0.00001341
Iteration 88/1000 | Loss: 0.00001341
Iteration 89/1000 | Loss: 0.00001341
Iteration 90/1000 | Loss: 0.00001340
Iteration 91/1000 | Loss: 0.00001340
Iteration 92/1000 | Loss: 0.00001340
Iteration 93/1000 | Loss: 0.00001340
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001339
Iteration 96/1000 | Loss: 0.00001339
Iteration 97/1000 | Loss: 0.00001339
Iteration 98/1000 | Loss: 0.00001339
Iteration 99/1000 | Loss: 0.00001339
Iteration 100/1000 | Loss: 0.00001339
Iteration 101/1000 | Loss: 0.00001339
Iteration 102/1000 | Loss: 0.00001339
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001338
Iteration 105/1000 | Loss: 0.00001338
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001338
Iteration 110/1000 | Loss: 0.00001338
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001338
Iteration 114/1000 | Loss: 0.00001338
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001337
Iteration 117/1000 | Loss: 0.00001337
Iteration 118/1000 | Loss: 0.00001337
Iteration 119/1000 | Loss: 0.00001337
Iteration 120/1000 | Loss: 0.00001337
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001337
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Iteration 131/1000 | Loss: 0.00001337
Iteration 132/1000 | Loss: 0.00001337
Iteration 133/1000 | Loss: 0.00001337
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001336
Iteration 137/1000 | Loss: 0.00001336
Iteration 138/1000 | Loss: 0.00001336
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001336
Iteration 141/1000 | Loss: 0.00001336
Iteration 142/1000 | Loss: 0.00001336
Iteration 143/1000 | Loss: 0.00001336
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001336
Iteration 153/1000 | Loss: 0.00001336
Iteration 154/1000 | Loss: 0.00001336
Iteration 155/1000 | Loss: 0.00001336
Iteration 156/1000 | Loss: 0.00001336
Iteration 157/1000 | Loss: 0.00001336
Iteration 158/1000 | Loss: 0.00001336
Iteration 159/1000 | Loss: 0.00001336
Iteration 160/1000 | Loss: 0.00001335
Iteration 161/1000 | Loss: 0.00001335
Iteration 162/1000 | Loss: 0.00001335
Iteration 163/1000 | Loss: 0.00001335
Iteration 164/1000 | Loss: 0.00001335
Iteration 165/1000 | Loss: 0.00001335
Iteration 166/1000 | Loss: 0.00001335
Iteration 167/1000 | Loss: 0.00001335
Iteration 168/1000 | Loss: 0.00001335
Iteration 169/1000 | Loss: 0.00001335
Iteration 170/1000 | Loss: 0.00001335
Iteration 171/1000 | Loss: 0.00001335
Iteration 172/1000 | Loss: 0.00001335
Iteration 173/1000 | Loss: 0.00001335
Iteration 174/1000 | Loss: 0.00001335
Iteration 175/1000 | Loss: 0.00001335
Iteration 176/1000 | Loss: 0.00001335
Iteration 177/1000 | Loss: 0.00001335
Iteration 178/1000 | Loss: 0.00001335
Iteration 179/1000 | Loss: 0.00001335
Iteration 180/1000 | Loss: 0.00001335
Iteration 181/1000 | Loss: 0.00001335
Iteration 182/1000 | Loss: 0.00001334
Iteration 183/1000 | Loss: 0.00001334
Iteration 184/1000 | Loss: 0.00001334
Iteration 185/1000 | Loss: 0.00001334
Iteration 186/1000 | Loss: 0.00001334
Iteration 187/1000 | Loss: 0.00001334
Iteration 188/1000 | Loss: 0.00001334
Iteration 189/1000 | Loss: 0.00001334
Iteration 190/1000 | Loss: 0.00001334
Iteration 191/1000 | Loss: 0.00001334
Iteration 192/1000 | Loss: 0.00001334
Iteration 193/1000 | Loss: 0.00001334
Iteration 194/1000 | Loss: 0.00001334
Iteration 195/1000 | Loss: 0.00001334
Iteration 196/1000 | Loss: 0.00001334
Iteration 197/1000 | Loss: 0.00001334
Iteration 198/1000 | Loss: 0.00001334
Iteration 199/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.3342191778065171e-05, 1.3342191778065171e-05, 1.3342191778065171e-05, 1.3342191778065171e-05, 1.3342191778065171e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3342191778065171e-05

Optimization complete. Final v2v error: 3.0692882537841797 mm

Highest mean error: 3.6249585151672363 mm for frame 186

Lowest mean error: 2.618694543838501 mm for frame 16

Saving results

Total time: 4198.256388902664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1069
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495449
Iteration 2/25 | Loss: 0.00137296
Iteration 3/25 | Loss: 0.00119437
Iteration 4/25 | Loss: 0.00117537
Iteration 5/25 | Loss: 0.00117014
Iteration 6/25 | Loss: 0.00116859
Iteration 7/25 | Loss: 0.00116859
Iteration 8/25 | Loss: 0.00116859
Iteration 9/25 | Loss: 0.00116859
Iteration 10/25 | Loss: 0.00116859
Iteration 11/25 | Loss: 0.00116859
Iteration 12/25 | Loss: 0.00116859
Iteration 13/25 | Loss: 0.00116859
Iteration 14/25 | Loss: 0.00116859
Iteration 15/25 | Loss: 0.00116859
Iteration 16/25 | Loss: 0.00116859
Iteration 17/25 | Loss: 0.00116859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001168589573353529, 0.001168589573353529, 0.001168589573353529, 0.001168589573353529, 0.001168589573353529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001168589573353529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37918305
Iteration 2/25 | Loss: 0.00074429
Iteration 3/25 | Loss: 0.00074428
Iteration 4/25 | Loss: 0.00074428
Iteration 5/25 | Loss: 0.00074428
Iteration 6/25 | Loss: 0.00074428
Iteration 7/25 | Loss: 0.00074428
Iteration 8/25 | Loss: 0.00074428
Iteration 9/25 | Loss: 0.00074428
Iteration 10/25 | Loss: 0.00074428
Iteration 11/25 | Loss: 0.00074428
Iteration 12/25 | Loss: 0.00074428
Iteration 13/25 | Loss: 0.00074428
Iteration 14/25 | Loss: 0.00074428
Iteration 15/25 | Loss: 0.00074428
Iteration 16/25 | Loss: 0.00074428
Iteration 17/25 | Loss: 0.00074428
Iteration 18/25 | Loss: 0.00074428
Iteration 19/25 | Loss: 0.00074428
Iteration 20/25 | Loss: 0.00074428
Iteration 21/25 | Loss: 0.00074428
Iteration 22/25 | Loss: 0.00074428
Iteration 23/25 | Loss: 0.00074428
Iteration 24/25 | Loss: 0.00074428
Iteration 25/25 | Loss: 0.00074428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074428
Iteration 2/1000 | Loss: 0.00004984
Iteration 3/1000 | Loss: 0.00003256
Iteration 4/1000 | Loss: 0.00003004
Iteration 5/1000 | Loss: 0.00002905
Iteration 6/1000 | Loss: 0.00002793
Iteration 7/1000 | Loss: 0.00002692
Iteration 8/1000 | Loss: 0.00002638
Iteration 9/1000 | Loss: 0.00002600
Iteration 10/1000 | Loss: 0.00002578
Iteration 11/1000 | Loss: 0.00002563
Iteration 12/1000 | Loss: 0.00002543
Iteration 13/1000 | Loss: 0.00002528
Iteration 14/1000 | Loss: 0.00002511
Iteration 15/1000 | Loss: 0.00002507
Iteration 16/1000 | Loss: 0.00002507
Iteration 17/1000 | Loss: 0.00002501
Iteration 18/1000 | Loss: 0.00002499
Iteration 19/1000 | Loss: 0.00002497
Iteration 20/1000 | Loss: 0.00002496
Iteration 21/1000 | Loss: 0.00002491
Iteration 22/1000 | Loss: 0.00002490
Iteration 23/1000 | Loss: 0.00002488
Iteration 24/1000 | Loss: 0.00002488
Iteration 25/1000 | Loss: 0.00002487
Iteration 26/1000 | Loss: 0.00002485
Iteration 27/1000 | Loss: 0.00002485
Iteration 28/1000 | Loss: 0.00002485
Iteration 29/1000 | Loss: 0.00002485
Iteration 30/1000 | Loss: 0.00002485
Iteration 31/1000 | Loss: 0.00002485
Iteration 32/1000 | Loss: 0.00002485
Iteration 33/1000 | Loss: 0.00002485
Iteration 34/1000 | Loss: 0.00002485
Iteration 35/1000 | Loss: 0.00002485
Iteration 36/1000 | Loss: 0.00002485
Iteration 37/1000 | Loss: 0.00002485
Iteration 38/1000 | Loss: 0.00002485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [2.4854954972397536e-05, 2.4854954972397536e-05, 2.4854954972397536e-05, 2.4854954972397536e-05, 2.4854954972397536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4854954972397536e-05

Optimization complete. Final v2v error: 4.152729034423828 mm

Highest mean error: 4.876821041107178 mm for frame 179

Lowest mean error: 3.697221279144287 mm for frame 152

Saving results

Total time: 1147.679365158081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1034
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015601
Iteration 2/25 | Loss: 0.00181546
Iteration 3/25 | Loss: 0.00128625
Iteration 4/25 | Loss: 0.00119471
Iteration 5/25 | Loss: 0.00117137
Iteration 6/25 | Loss: 0.00116203
Iteration 7/25 | Loss: 0.00115977
Iteration 8/25 | Loss: 0.00115925
Iteration 9/25 | Loss: 0.00115914
Iteration 10/25 | Loss: 0.00115914
Iteration 11/25 | Loss: 0.00115914
Iteration 12/25 | Loss: 0.00115915
Iteration 13/25 | Loss: 0.00115914
Iteration 14/25 | Loss: 0.00115914
Iteration 15/25 | Loss: 0.00115914
Iteration 16/25 | Loss: 0.00115914
Iteration 17/25 | Loss: 0.00115914
Iteration 18/25 | Loss: 0.00115914
Iteration 19/25 | Loss: 0.00115914
Iteration 20/25 | Loss: 0.00115914
Iteration 21/25 | Loss: 0.00115914
Iteration 22/25 | Loss: 0.00115914
Iteration 23/25 | Loss: 0.00115914
Iteration 24/25 | Loss: 0.00115914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011591447982937098, 0.0011591447982937098, 0.0011591447982937098, 0.0011591447982937098, 0.0011591447982937098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011591447982937098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35991716
Iteration 2/25 | Loss: 0.00061729
Iteration 3/25 | Loss: 0.00061729
Iteration 4/25 | Loss: 0.00061729
Iteration 5/25 | Loss: 0.00061729
Iteration 6/25 | Loss: 0.00061729
Iteration 7/25 | Loss: 0.00061729
Iteration 8/25 | Loss: 0.00061729
Iteration 9/25 | Loss: 0.00061729
Iteration 10/25 | Loss: 0.00061729
Iteration 11/25 | Loss: 0.00061729
Iteration 12/25 | Loss: 0.00061729
Iteration 13/25 | Loss: 0.00061729
Iteration 14/25 | Loss: 0.00061729
Iteration 15/25 | Loss: 0.00061729
Iteration 16/25 | Loss: 0.00061729
Iteration 17/25 | Loss: 0.00061729
Iteration 18/25 | Loss: 0.00061729
Iteration 19/25 | Loss: 0.00061729
Iteration 20/25 | Loss: 0.00061729
Iteration 21/25 | Loss: 0.00061729
Iteration 22/25 | Loss: 0.00061729
Iteration 23/25 | Loss: 0.00061729
Iteration 24/25 | Loss: 0.00061729
Iteration 25/25 | Loss: 0.00061729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006172861903905869, 0.0006172861903905869, 0.0006172861903905869, 0.0006172861903905869, 0.0006172861903905869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006172861903905869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061729
Iteration 2/1000 | Loss: 0.00003396
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00002198
Iteration 5/1000 | Loss: 0.00002136
Iteration 6/1000 | Loss: 0.00002091
Iteration 7/1000 | Loss: 0.00002063
Iteration 8/1000 | Loss: 0.00002045
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002030
Iteration 11/1000 | Loss: 0.00002029
Iteration 12/1000 | Loss: 0.00002027
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002010
Iteration 15/1000 | Loss: 0.00002005
Iteration 16/1000 | Loss: 0.00002004
Iteration 17/1000 | Loss: 0.00002003
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00002001
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00002000
Iteration 22/1000 | Loss: 0.00001999
Iteration 23/1000 | Loss: 0.00001998
Iteration 24/1000 | Loss: 0.00001998
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001996
Iteration 27/1000 | Loss: 0.00001996
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001995
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001994
Iteration 32/1000 | Loss: 0.00001994
Iteration 33/1000 | Loss: 0.00001994
Iteration 34/1000 | Loss: 0.00001994
Iteration 35/1000 | Loss: 0.00001993
Iteration 36/1000 | Loss: 0.00001992
Iteration 37/1000 | Loss: 0.00001991
Iteration 38/1000 | Loss: 0.00001991
Iteration 39/1000 | Loss: 0.00001991
Iteration 40/1000 | Loss: 0.00001991
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001990
Iteration 45/1000 | Loss: 0.00001989
Iteration 46/1000 | Loss: 0.00001989
Iteration 47/1000 | Loss: 0.00001989
Iteration 48/1000 | Loss: 0.00001989
Iteration 49/1000 | Loss: 0.00001988
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001987
Iteration 52/1000 | Loss: 0.00001987
Iteration 53/1000 | Loss: 0.00001987
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001986
Iteration 57/1000 | Loss: 0.00001986
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001986
Iteration 60/1000 | Loss: 0.00001986
Iteration 61/1000 | Loss: 0.00001985
Iteration 62/1000 | Loss: 0.00001985
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001985
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001985
Iteration 69/1000 | Loss: 0.00001985
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001985
Iteration 73/1000 | Loss: 0.00001984
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001984
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001984
Iteration 81/1000 | Loss: 0.00001983
Iteration 82/1000 | Loss: 0.00001983
Iteration 83/1000 | Loss: 0.00001983
Iteration 84/1000 | Loss: 0.00001983
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001983
Iteration 94/1000 | Loss: 0.00001983
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001983
Iteration 100/1000 | Loss: 0.00001983
Iteration 101/1000 | Loss: 0.00001983
Iteration 102/1000 | Loss: 0.00001983
Iteration 103/1000 | Loss: 0.00001983
Iteration 104/1000 | Loss: 0.00001983
Iteration 105/1000 | Loss: 0.00001983
Iteration 106/1000 | Loss: 0.00001983
Iteration 107/1000 | Loss: 0.00001983
Iteration 108/1000 | Loss: 0.00001983
Iteration 109/1000 | Loss: 0.00001983
Iteration 110/1000 | Loss: 0.00001983
Iteration 111/1000 | Loss: 0.00001983
Iteration 112/1000 | Loss: 0.00001983
Iteration 113/1000 | Loss: 0.00001983
Iteration 114/1000 | Loss: 0.00001983
Iteration 115/1000 | Loss: 0.00001983
Iteration 116/1000 | Loss: 0.00001983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.9826229618047364e-05, 1.9826229618047364e-05, 1.9826229618047364e-05, 1.9826229618047364e-05, 1.9826229618047364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9826229618047364e-05

Optimization complete. Final v2v error: 3.7374625205993652 mm

Highest mean error: 3.948930263519287 mm for frame 22

Lowest mean error: 3.586581230163574 mm for frame 90

Saving results

Total time: 730.8168127536774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1067
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848293
Iteration 2/25 | Loss: 0.00152295
Iteration 3/25 | Loss: 0.00121230
Iteration 4/25 | Loss: 0.00118350
Iteration 5/25 | Loss: 0.00117781
Iteration 6/25 | Loss: 0.00117765
Iteration 7/25 | Loss: 0.00117765
Iteration 8/25 | Loss: 0.00117765
Iteration 9/25 | Loss: 0.00117765
Iteration 10/25 | Loss: 0.00117765
Iteration 11/25 | Loss: 0.00117765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011776514584198594, 0.0011776514584198594, 0.0011776514584198594, 0.0011776514584198594, 0.0011776514584198594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011776514584198594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.15177816
Iteration 2/25 | Loss: 0.00056296
Iteration 3/25 | Loss: 0.00056294
Iteration 4/25 | Loss: 0.00056294
Iteration 5/25 | Loss: 0.00056294
Iteration 6/25 | Loss: 0.00056294
Iteration 7/25 | Loss: 0.00056294
Iteration 8/25 | Loss: 0.00056294
Iteration 9/25 | Loss: 0.00056294
Iteration 10/25 | Loss: 0.00056294
Iteration 11/25 | Loss: 0.00056294
Iteration 12/25 | Loss: 0.00056294
Iteration 13/25 | Loss: 0.00056294
Iteration 14/25 | Loss: 0.00056294
Iteration 15/25 | Loss: 0.00056294
Iteration 16/25 | Loss: 0.00056294
Iteration 17/25 | Loss: 0.00056294
Iteration 18/25 | Loss: 0.00056294
Iteration 19/25 | Loss: 0.00056294
Iteration 20/25 | Loss: 0.00056294
Iteration 21/25 | Loss: 0.00056294
Iteration 22/25 | Loss: 0.00056294
Iteration 23/25 | Loss: 0.00056294
Iteration 24/25 | Loss: 0.00056294
Iteration 25/25 | Loss: 0.00056294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056294
Iteration 2/1000 | Loss: 0.00006584
Iteration 3/1000 | Loss: 0.00003077
Iteration 4/1000 | Loss: 0.00002589
Iteration 5/1000 | Loss: 0.00002424
Iteration 6/1000 | Loss: 0.00002335
Iteration 7/1000 | Loss: 0.00002280
Iteration 8/1000 | Loss: 0.00002243
Iteration 9/1000 | Loss: 0.00002208
Iteration 10/1000 | Loss: 0.00002183
Iteration 11/1000 | Loss: 0.00002163
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00002141
Iteration 14/1000 | Loss: 0.00002135
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002129
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002129
Iteration 19/1000 | Loss: 0.00002128
Iteration 20/1000 | Loss: 0.00002128
Iteration 21/1000 | Loss: 0.00002127
Iteration 22/1000 | Loss: 0.00002127
Iteration 23/1000 | Loss: 0.00002127
Iteration 24/1000 | Loss: 0.00002127
Iteration 25/1000 | Loss: 0.00002126
Iteration 26/1000 | Loss: 0.00002126
Iteration 27/1000 | Loss: 0.00002126
Iteration 28/1000 | Loss: 0.00002126
Iteration 29/1000 | Loss: 0.00002126
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002126
Iteration 32/1000 | Loss: 0.00002126
Iteration 33/1000 | Loss: 0.00002126
Iteration 34/1000 | Loss: 0.00002126
Iteration 35/1000 | Loss: 0.00002126
Iteration 36/1000 | Loss: 0.00002126
Iteration 37/1000 | Loss: 0.00002126
Iteration 38/1000 | Loss: 0.00002126
Iteration 39/1000 | Loss: 0.00002125
Iteration 40/1000 | Loss: 0.00002125
Iteration 41/1000 | Loss: 0.00002125
Iteration 42/1000 | Loss: 0.00002124
Iteration 43/1000 | Loss: 0.00002124
Iteration 44/1000 | Loss: 0.00002124
Iteration 45/1000 | Loss: 0.00002123
Iteration 46/1000 | Loss: 0.00002123
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002120
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002119
Iteration 55/1000 | Loss: 0.00002119
Iteration 56/1000 | Loss: 0.00002119
Iteration 57/1000 | Loss: 0.00002119
Iteration 58/1000 | Loss: 0.00002119
Iteration 59/1000 | Loss: 0.00002118
Iteration 60/1000 | Loss: 0.00002118
Iteration 61/1000 | Loss: 0.00002118
Iteration 62/1000 | Loss: 0.00002118
Iteration 63/1000 | Loss: 0.00002118
Iteration 64/1000 | Loss: 0.00002118
Iteration 65/1000 | Loss: 0.00002118
Iteration 66/1000 | Loss: 0.00002117
Iteration 67/1000 | Loss: 0.00002117
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002115
Iteration 75/1000 | Loss: 0.00002115
Iteration 76/1000 | Loss: 0.00002115
Iteration 77/1000 | Loss: 0.00002115
Iteration 78/1000 | Loss: 0.00002114
Iteration 79/1000 | Loss: 0.00002114
Iteration 80/1000 | Loss: 0.00002114
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002113
Iteration 85/1000 | Loss: 0.00002113
Iteration 86/1000 | Loss: 0.00002113
Iteration 87/1000 | Loss: 0.00002113
Iteration 88/1000 | Loss: 0.00002113
Iteration 89/1000 | Loss: 0.00002113
Iteration 90/1000 | Loss: 0.00002113
Iteration 91/1000 | Loss: 0.00002113
Iteration 92/1000 | Loss: 0.00002113
Iteration 93/1000 | Loss: 0.00002113
Iteration 94/1000 | Loss: 0.00002113
Iteration 95/1000 | Loss: 0.00002113
Iteration 96/1000 | Loss: 0.00002113
Iteration 97/1000 | Loss: 0.00002113
Iteration 98/1000 | Loss: 0.00002113
Iteration 99/1000 | Loss: 0.00002113
Iteration 100/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.1129313608980738e-05, 2.1129313608980738e-05, 2.1129313608980738e-05, 2.1129313608980738e-05, 2.1129313608980738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1129313608980738e-05

Optimization complete. Final v2v error: 3.9085605144500732 mm

Highest mean error: 4.154970169067383 mm for frame 52

Lowest mean error: 3.4986419677734375 mm for frame 102

Saving results

Total time: 1073.1933345794678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1080
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833436
Iteration 2/25 | Loss: 0.00153292
Iteration 3/25 | Loss: 0.00123197
Iteration 4/25 | Loss: 0.00120295
Iteration 5/25 | Loss: 0.00119797
Iteration 6/25 | Loss: 0.00119695
Iteration 7/25 | Loss: 0.00119695
Iteration 8/25 | Loss: 0.00119695
Iteration 9/25 | Loss: 0.00119695
Iteration 10/25 | Loss: 0.00119695
Iteration 11/25 | Loss: 0.00119695
Iteration 12/25 | Loss: 0.00119695
Iteration 13/25 | Loss: 0.00119695
Iteration 14/25 | Loss: 0.00119695
Iteration 15/25 | Loss: 0.00119695
Iteration 16/25 | Loss: 0.00119695
Iteration 17/25 | Loss: 0.00119695
Iteration 18/25 | Loss: 0.00119695
Iteration 19/25 | Loss: 0.00119695
Iteration 20/25 | Loss: 0.00119695
Iteration 21/25 | Loss: 0.00119695
Iteration 22/25 | Loss: 0.00119695
Iteration 23/25 | Loss: 0.00119695
Iteration 24/25 | Loss: 0.00119695
Iteration 25/25 | Loss: 0.00119695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39096153
Iteration 2/25 | Loss: 0.00036576
Iteration 3/25 | Loss: 0.00036576
Iteration 4/25 | Loss: 0.00036576
Iteration 5/25 | Loss: 0.00036576
Iteration 6/25 | Loss: 0.00036576
Iteration 7/25 | Loss: 0.00036576
Iteration 8/25 | Loss: 0.00036576
Iteration 9/25 | Loss: 0.00036576
Iteration 10/25 | Loss: 0.00036576
Iteration 11/25 | Loss: 0.00036576
Iteration 12/25 | Loss: 0.00036576
Iteration 13/25 | Loss: 0.00036576
Iteration 14/25 | Loss: 0.00036576
Iteration 15/25 | Loss: 0.00036576
Iteration 16/25 | Loss: 0.00036576
Iteration 17/25 | Loss: 0.00036576
Iteration 18/25 | Loss: 0.00036576
Iteration 19/25 | Loss: 0.00036576
Iteration 20/25 | Loss: 0.00036576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003657573543023318, 0.0003657573543023318, 0.0003657573543023318, 0.0003657573543023318, 0.0003657573543023318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003657573543023318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036576
Iteration 2/1000 | Loss: 0.00005271
Iteration 3/1000 | Loss: 0.00004429
Iteration 4/1000 | Loss: 0.00004246
Iteration 5/1000 | Loss: 0.00004107
Iteration 6/1000 | Loss: 0.00003998
Iteration 7/1000 | Loss: 0.00003915
Iteration 8/1000 | Loss: 0.00003857
Iteration 9/1000 | Loss: 0.00003833
Iteration 10/1000 | Loss: 0.00003808
Iteration 11/1000 | Loss: 0.00003792
Iteration 12/1000 | Loss: 0.00003780
Iteration 13/1000 | Loss: 0.00003777
Iteration 14/1000 | Loss: 0.00003771
Iteration 15/1000 | Loss: 0.00003768
Iteration 16/1000 | Loss: 0.00003768
Iteration 17/1000 | Loss: 0.00003768
Iteration 18/1000 | Loss: 0.00003765
Iteration 19/1000 | Loss: 0.00003765
Iteration 20/1000 | Loss: 0.00003764
Iteration 21/1000 | Loss: 0.00003764
Iteration 22/1000 | Loss: 0.00003762
Iteration 23/1000 | Loss: 0.00003762
Iteration 24/1000 | Loss: 0.00003762
Iteration 25/1000 | Loss: 0.00003762
Iteration 26/1000 | Loss: 0.00003762
Iteration 27/1000 | Loss: 0.00003762
Iteration 28/1000 | Loss: 0.00003762
Iteration 29/1000 | Loss: 0.00003762
Iteration 30/1000 | Loss: 0.00003762
Iteration 31/1000 | Loss: 0.00003762
Iteration 32/1000 | Loss: 0.00003762
Iteration 33/1000 | Loss: 0.00003762
Iteration 34/1000 | Loss: 0.00003762
Iteration 35/1000 | Loss: 0.00003761
Iteration 36/1000 | Loss: 0.00003761
Iteration 37/1000 | Loss: 0.00003760
Iteration 38/1000 | Loss: 0.00003760
Iteration 39/1000 | Loss: 0.00003760
Iteration 40/1000 | Loss: 0.00003760
Iteration 41/1000 | Loss: 0.00003760
Iteration 42/1000 | Loss: 0.00003760
Iteration 43/1000 | Loss: 0.00003760
Iteration 44/1000 | Loss: 0.00003760
Iteration 45/1000 | Loss: 0.00003760
Iteration 46/1000 | Loss: 0.00003760
Iteration 47/1000 | Loss: 0.00003760
Iteration 48/1000 | Loss: 0.00003760
Iteration 49/1000 | Loss: 0.00003760
Iteration 50/1000 | Loss: 0.00003760
Iteration 51/1000 | Loss: 0.00003760
Iteration 52/1000 | Loss: 0.00003760
Iteration 53/1000 | Loss: 0.00003760
Iteration 54/1000 | Loss: 0.00003760
Iteration 55/1000 | Loss: 0.00003760
Iteration 56/1000 | Loss: 0.00003760
Iteration 57/1000 | Loss: 0.00003760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [3.7595080357277766e-05, 3.7595080357277766e-05, 3.7595080357277766e-05, 3.7595080357277766e-05, 3.7595080357277766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7595080357277766e-05

Optimization complete. Final v2v error: 5.071681022644043 mm

Highest mean error: 5.526729106903076 mm for frame 17

Lowest mean error: 4.561176776885986 mm for frame 30

Saving results

Total time: 502.5299530029297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1071
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449319
Iteration 2/25 | Loss: 0.00119571
Iteration 3/25 | Loss: 0.00109253
Iteration 4/25 | Loss: 0.00107225
Iteration 5/25 | Loss: 0.00106763
Iteration 6/25 | Loss: 0.00106727
Iteration 7/25 | Loss: 0.00106727
Iteration 8/25 | Loss: 0.00106727
Iteration 9/25 | Loss: 0.00106727
Iteration 10/25 | Loss: 0.00106727
Iteration 11/25 | Loss: 0.00106727
Iteration 12/25 | Loss: 0.00106727
Iteration 13/25 | Loss: 0.00106727
Iteration 14/25 | Loss: 0.00106727
Iteration 15/25 | Loss: 0.00106727
Iteration 16/25 | Loss: 0.00106727
Iteration 17/25 | Loss: 0.00106727
Iteration 18/25 | Loss: 0.00106727
Iteration 19/25 | Loss: 0.00106727
Iteration 20/25 | Loss: 0.00106727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010672652861103415, 0.0010672652861103415, 0.0010672652861103415, 0.0010672652861103415, 0.0010672652861103415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010672652861103415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.22199249
Iteration 2/25 | Loss: 0.00068505
Iteration 3/25 | Loss: 0.00068502
Iteration 4/25 | Loss: 0.00068502
Iteration 5/25 | Loss: 0.00068502
Iteration 6/25 | Loss: 0.00068502
Iteration 7/25 | Loss: 0.00068502
Iteration 8/25 | Loss: 0.00068502
Iteration 9/25 | Loss: 0.00068502
Iteration 10/25 | Loss: 0.00068502
Iteration 11/25 | Loss: 0.00068502
Iteration 12/25 | Loss: 0.00068502
Iteration 13/25 | Loss: 0.00068502
Iteration 14/25 | Loss: 0.00068502
Iteration 15/25 | Loss: 0.00068502
Iteration 16/25 | Loss: 0.00068502
Iteration 17/25 | Loss: 0.00068502
Iteration 18/25 | Loss: 0.00068502
Iteration 19/25 | Loss: 0.00068502
Iteration 20/25 | Loss: 0.00068502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000685016973875463, 0.000685016973875463, 0.000685016973875463, 0.000685016973875463, 0.000685016973875463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000685016973875463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068502
Iteration 2/1000 | Loss: 0.00002525
Iteration 3/1000 | Loss: 0.00001626
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001395
Iteration 6/1000 | Loss: 0.00001337
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001243
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001202
Iteration 23/1000 | Loss: 0.00001201
Iteration 24/1000 | Loss: 0.00001201
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00001197
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001195
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001192
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001191
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001185
Iteration 57/1000 | Loss: 0.00001185
Iteration 58/1000 | Loss: 0.00001185
Iteration 59/1000 | Loss: 0.00001185
Iteration 60/1000 | Loss: 0.00001184
Iteration 61/1000 | Loss: 0.00001183
Iteration 62/1000 | Loss: 0.00001183
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001182
Iteration 66/1000 | Loss: 0.00001182
Iteration 67/1000 | Loss: 0.00001182
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001181
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001179
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001178
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001178
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001168
Iteration 154/1000 | Loss: 0.00001168
Iteration 155/1000 | Loss: 0.00001168
Iteration 156/1000 | Loss: 0.00001168
Iteration 157/1000 | Loss: 0.00001168
Iteration 158/1000 | Loss: 0.00001167
Iteration 159/1000 | Loss: 0.00001167
Iteration 160/1000 | Loss: 0.00001167
Iteration 161/1000 | Loss: 0.00001167
Iteration 162/1000 | Loss: 0.00001167
Iteration 163/1000 | Loss: 0.00001167
Iteration 164/1000 | Loss: 0.00001167
Iteration 165/1000 | Loss: 0.00001167
Iteration 166/1000 | Loss: 0.00001167
Iteration 167/1000 | Loss: 0.00001167
Iteration 168/1000 | Loss: 0.00001167
Iteration 169/1000 | Loss: 0.00001167
Iteration 170/1000 | Loss: 0.00001167
Iteration 171/1000 | Loss: 0.00001167
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001166
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001165
Iteration 179/1000 | Loss: 0.00001165
Iteration 180/1000 | Loss: 0.00001165
Iteration 181/1000 | Loss: 0.00001165
Iteration 182/1000 | Loss: 0.00001165
Iteration 183/1000 | Loss: 0.00001164
Iteration 184/1000 | Loss: 0.00001164
Iteration 185/1000 | Loss: 0.00001164
Iteration 186/1000 | Loss: 0.00001164
Iteration 187/1000 | Loss: 0.00001164
Iteration 188/1000 | Loss: 0.00001164
Iteration 189/1000 | Loss: 0.00001164
Iteration 190/1000 | Loss: 0.00001164
Iteration 191/1000 | Loss: 0.00001164
Iteration 192/1000 | Loss: 0.00001164
Iteration 193/1000 | Loss: 0.00001164
Iteration 194/1000 | Loss: 0.00001164
Iteration 195/1000 | Loss: 0.00001164
Iteration 196/1000 | Loss: 0.00001164
Iteration 197/1000 | Loss: 0.00001164
Iteration 198/1000 | Loss: 0.00001164
Iteration 199/1000 | Loss: 0.00001164
Iteration 200/1000 | Loss: 0.00001164
Iteration 201/1000 | Loss: 0.00001164
Iteration 202/1000 | Loss: 0.00001164
Iteration 203/1000 | Loss: 0.00001164
Iteration 204/1000 | Loss: 0.00001164
Iteration 205/1000 | Loss: 0.00001164
Iteration 206/1000 | Loss: 0.00001164
Iteration 207/1000 | Loss: 0.00001164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.1636449016805273e-05, 1.1636449016805273e-05, 1.1636449016805273e-05, 1.1636449016805273e-05, 1.1636449016805273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1636449016805273e-05

Optimization complete. Final v2v error: 2.9070591926574707 mm

Highest mean error: 3.3113036155700684 mm for frame 69

Lowest mean error: 2.5177605152130127 mm for frame 12

Saving results

Total time: 1392.3292019367218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1054
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011174
Iteration 2/25 | Loss: 0.00262635
Iteration 3/25 | Loss: 0.00195454
Iteration 4/25 | Loss: 0.00180928
Iteration 5/25 | Loss: 0.00167827
Iteration 6/25 | Loss: 0.00162462
Iteration 7/25 | Loss: 0.00159527
Iteration 8/25 | Loss: 0.00152004
Iteration 9/25 | Loss: 0.00143464
Iteration 10/25 | Loss: 0.00140518
Iteration 11/25 | Loss: 0.00137777
Iteration 12/25 | Loss: 0.00136498
Iteration 13/25 | Loss: 0.00135701
Iteration 14/25 | Loss: 0.00135613
Iteration 15/25 | Loss: 0.00134784
Iteration 16/25 | Loss: 0.00133969
Iteration 17/25 | Loss: 0.00133342
Iteration 18/25 | Loss: 0.00133219
Iteration 19/25 | Loss: 0.00133507
Iteration 20/25 | Loss: 0.00133396
Iteration 21/25 | Loss: 0.00133454
Iteration 22/25 | Loss: 0.00133008
Iteration 23/25 | Loss: 0.00132667
Iteration 24/25 | Loss: 0.00132560
Iteration 25/25 | Loss: 0.00132665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41484535
Iteration 2/25 | Loss: 0.00205868
Iteration 3/25 | Loss: 0.00205868
Iteration 4/25 | Loss: 0.00205868
Iteration 5/25 | Loss: 0.00205868
Iteration 6/25 | Loss: 0.00205868
Iteration 7/25 | Loss: 0.00205868
Iteration 8/25 | Loss: 0.00205868
Iteration 9/25 | Loss: 0.00205868
Iteration 10/25 | Loss: 0.00205868
Iteration 11/25 | Loss: 0.00205868
Iteration 12/25 | Loss: 0.00205868
Iteration 13/25 | Loss: 0.00205868
Iteration 14/25 | Loss: 0.00205868
Iteration 15/25 | Loss: 0.00205868
Iteration 16/25 | Loss: 0.00205868
Iteration 17/25 | Loss: 0.00205868
Iteration 18/25 | Loss: 0.00205868
Iteration 19/25 | Loss: 0.00205868
Iteration 20/25 | Loss: 0.00205868
Iteration 21/25 | Loss: 0.00205868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002058681333437562, 0.002058681333437562, 0.002058681333437562, 0.002058681333437562, 0.002058681333437562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002058681333437562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205868
Iteration 2/1000 | Loss: 0.00022832
Iteration 3/1000 | Loss: 0.00018327
Iteration 4/1000 | Loss: 0.00015628
Iteration 5/1000 | Loss: 0.00014734
Iteration 6/1000 | Loss: 0.00013539
Iteration 7/1000 | Loss: 0.00014923
Iteration 8/1000 | Loss: 0.00014084
Iteration 9/1000 | Loss: 0.00013186
Iteration 10/1000 | Loss: 0.00012687
Iteration 11/1000 | Loss: 0.00012313
Iteration 12/1000 | Loss: 0.00012337
Iteration 13/1000 | Loss: 0.00012867
Iteration 14/1000 | Loss: 0.00013739
Iteration 15/1000 | Loss: 0.00012428
Iteration 16/1000 | Loss: 0.00013052
Iteration 17/1000 | Loss: 0.00013279
Iteration 18/1000 | Loss: 0.00029854
Iteration 19/1000 | Loss: 0.00214596
Iteration 20/1000 | Loss: 0.00264427
Iteration 21/1000 | Loss: 0.00148715
Iteration 22/1000 | Loss: 0.00163174
Iteration 23/1000 | Loss: 0.00074860
Iteration 24/1000 | Loss: 0.00025975
Iteration 25/1000 | Loss: 0.00026455
Iteration 26/1000 | Loss: 0.00023091
Iteration 27/1000 | Loss: 0.00019268
Iteration 28/1000 | Loss: 0.00070585
Iteration 29/1000 | Loss: 0.00016373
Iteration 30/1000 | Loss: 0.00108971
Iteration 31/1000 | Loss: 0.00063952
Iteration 32/1000 | Loss: 0.00014557
Iteration 33/1000 | Loss: 0.00068218
Iteration 34/1000 | Loss: 0.00080438
Iteration 35/1000 | Loss: 0.00011905
Iteration 36/1000 | Loss: 0.00034549
Iteration 37/1000 | Loss: 0.00011840
Iteration 38/1000 | Loss: 0.00154441
Iteration 39/1000 | Loss: 0.00011238
Iteration 40/1000 | Loss: 0.00009872
Iteration 41/1000 | Loss: 0.00009154
Iteration 42/1000 | Loss: 0.00008782
Iteration 43/1000 | Loss: 0.00008742
Iteration 44/1000 | Loss: 0.00008309
Iteration 45/1000 | Loss: 0.00008136
Iteration 46/1000 | Loss: 0.00054167
Iteration 47/1000 | Loss: 0.00008816
Iteration 48/1000 | Loss: 0.00093403
Iteration 49/1000 | Loss: 0.00010491
Iteration 50/1000 | Loss: 0.00022503
Iteration 51/1000 | Loss: 0.00107868
Iteration 52/1000 | Loss: 0.00029135
Iteration 53/1000 | Loss: 0.00013744
Iteration 54/1000 | Loss: 0.00007616
Iteration 55/1000 | Loss: 0.00012760
Iteration 56/1000 | Loss: 0.00006451
Iteration 57/1000 | Loss: 0.00031985
Iteration 58/1000 | Loss: 0.00010680
Iteration 59/1000 | Loss: 0.00008814
Iteration 60/1000 | Loss: 0.00014203
Iteration 61/1000 | Loss: 0.00012047
Iteration 62/1000 | Loss: 0.00029659
Iteration 63/1000 | Loss: 0.00006304
Iteration 64/1000 | Loss: 0.00005621
Iteration 65/1000 | Loss: 0.00018931
Iteration 66/1000 | Loss: 0.00005454
Iteration 67/1000 | Loss: 0.00004984
Iteration 68/1000 | Loss: 0.00004747
Iteration 69/1000 | Loss: 0.00021440
Iteration 70/1000 | Loss: 0.00004734
Iteration 71/1000 | Loss: 0.00004274
Iteration 72/1000 | Loss: 0.00004194
Iteration 73/1000 | Loss: 0.00017832
Iteration 74/1000 | Loss: 0.00004378
Iteration 75/1000 | Loss: 0.00019117
Iteration 76/1000 | Loss: 0.00004511
Iteration 77/1000 | Loss: 0.00004157
Iteration 78/1000 | Loss: 0.00004025
Iteration 79/1000 | Loss: 0.00003942
Iteration 80/1000 | Loss: 0.00003872
Iteration 81/1000 | Loss: 0.00019401
Iteration 82/1000 | Loss: 0.00004396
Iteration 83/1000 | Loss: 0.00003782
Iteration 84/1000 | Loss: 0.00003682
Iteration 85/1000 | Loss: 0.00003649
Iteration 86/1000 | Loss: 0.00003623
Iteration 87/1000 | Loss: 0.00003609
Iteration 88/1000 | Loss: 0.00003605
Iteration 89/1000 | Loss: 0.00003604
Iteration 90/1000 | Loss: 0.00003603
Iteration 91/1000 | Loss: 0.00003602
Iteration 92/1000 | Loss: 0.00003602
Iteration 93/1000 | Loss: 0.00003594
Iteration 94/1000 | Loss: 0.00003594
Iteration 95/1000 | Loss: 0.00003593
Iteration 96/1000 | Loss: 0.00003593
Iteration 97/1000 | Loss: 0.00003584
Iteration 98/1000 | Loss: 0.00003577
Iteration 99/1000 | Loss: 0.00003576
Iteration 100/1000 | Loss: 0.00003575
Iteration 101/1000 | Loss: 0.00003561
Iteration 102/1000 | Loss: 0.00003560
Iteration 103/1000 | Loss: 0.00003556
Iteration 104/1000 | Loss: 0.00003546
Iteration 105/1000 | Loss: 0.00003545
Iteration 106/1000 | Loss: 0.00003545
Iteration 107/1000 | Loss: 0.00003544
Iteration 108/1000 | Loss: 0.00003543
Iteration 109/1000 | Loss: 0.00003542
Iteration 110/1000 | Loss: 0.00003542
Iteration 111/1000 | Loss: 0.00003541
Iteration 112/1000 | Loss: 0.00003541
Iteration 113/1000 | Loss: 0.00003541
Iteration 114/1000 | Loss: 0.00003541
Iteration 115/1000 | Loss: 0.00003538
Iteration 116/1000 | Loss: 0.00003538
Iteration 117/1000 | Loss: 0.00003538
Iteration 118/1000 | Loss: 0.00003538
Iteration 119/1000 | Loss: 0.00003538
Iteration 120/1000 | Loss: 0.00003538
Iteration 121/1000 | Loss: 0.00003537
Iteration 122/1000 | Loss: 0.00003537
Iteration 123/1000 | Loss: 0.00003537
Iteration 124/1000 | Loss: 0.00003537
Iteration 125/1000 | Loss: 0.00003537
Iteration 126/1000 | Loss: 0.00003537
Iteration 127/1000 | Loss: 0.00003537
Iteration 128/1000 | Loss: 0.00003534
Iteration 129/1000 | Loss: 0.00003534
Iteration 130/1000 | Loss: 0.00003534
Iteration 131/1000 | Loss: 0.00003534
Iteration 132/1000 | Loss: 0.00003533
Iteration 133/1000 | Loss: 0.00003533
Iteration 134/1000 | Loss: 0.00003532
Iteration 135/1000 | Loss: 0.00003531
Iteration 136/1000 | Loss: 0.00003530
Iteration 137/1000 | Loss: 0.00003529
Iteration 138/1000 | Loss: 0.00003528
Iteration 139/1000 | Loss: 0.00003525
Iteration 140/1000 | Loss: 0.00003524
Iteration 141/1000 | Loss: 0.00003524
Iteration 142/1000 | Loss: 0.00003524
Iteration 143/1000 | Loss: 0.00003524
Iteration 144/1000 | Loss: 0.00003524
Iteration 145/1000 | Loss: 0.00003524
Iteration 146/1000 | Loss: 0.00003524
Iteration 147/1000 | Loss: 0.00003524
Iteration 148/1000 | Loss: 0.00003524
Iteration 149/1000 | Loss: 0.00003524
Iteration 150/1000 | Loss: 0.00003523
Iteration 151/1000 | Loss: 0.00003522
Iteration 152/1000 | Loss: 0.00003522
Iteration 153/1000 | Loss: 0.00003522
Iteration 154/1000 | Loss: 0.00003522
Iteration 155/1000 | Loss: 0.00003522
Iteration 156/1000 | Loss: 0.00003522
Iteration 157/1000 | Loss: 0.00003521
Iteration 158/1000 | Loss: 0.00003521
Iteration 159/1000 | Loss: 0.00003521
Iteration 160/1000 | Loss: 0.00003521
Iteration 161/1000 | Loss: 0.00003520
Iteration 162/1000 | Loss: 0.00003520
Iteration 163/1000 | Loss: 0.00003520
Iteration 164/1000 | Loss: 0.00003520
Iteration 165/1000 | Loss: 0.00003520
Iteration 166/1000 | Loss: 0.00003520
Iteration 167/1000 | Loss: 0.00003520
Iteration 168/1000 | Loss: 0.00003520
Iteration 169/1000 | Loss: 0.00003520
Iteration 170/1000 | Loss: 0.00003520
Iteration 171/1000 | Loss: 0.00003520
Iteration 172/1000 | Loss: 0.00003520
Iteration 173/1000 | Loss: 0.00003520
Iteration 174/1000 | Loss: 0.00003520
Iteration 175/1000 | Loss: 0.00003520
Iteration 176/1000 | Loss: 0.00003520
Iteration 177/1000 | Loss: 0.00003520
Iteration 178/1000 | Loss: 0.00003520
Iteration 179/1000 | Loss: 0.00003520
Iteration 180/1000 | Loss: 0.00003520
Iteration 181/1000 | Loss: 0.00003520
Iteration 182/1000 | Loss: 0.00003520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [3.519970050547272e-05, 3.519970050547272e-05, 3.519970050547272e-05, 3.519970050547272e-05, 3.519970050547272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.519970050547272e-05

Optimization complete. Final v2v error: 3.9034292697906494 mm

Highest mean error: 12.296906471252441 mm for frame 12

Lowest mean error: 2.903620958328247 mm for frame 6

Saving results

Total time: 5947.865962982178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1087
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439512
Iteration 2/25 | Loss: 0.00116172
Iteration 3/25 | Loss: 0.00107618
Iteration 4/25 | Loss: 0.00107052
Iteration 5/25 | Loss: 0.00106837
Iteration 6/25 | Loss: 0.00106819
Iteration 7/25 | Loss: 0.00106819
Iteration 8/25 | Loss: 0.00106819
Iteration 9/25 | Loss: 0.00106819
Iteration 10/25 | Loss: 0.00106819
Iteration 11/25 | Loss: 0.00106819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010681927669793367, 0.0010681927669793367, 0.0010681927669793367, 0.0010681927669793367, 0.0010681927669793367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010681927669793367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39019656
Iteration 2/25 | Loss: 0.00077425
Iteration 3/25 | Loss: 0.00077424
Iteration 4/25 | Loss: 0.00077424
Iteration 5/25 | Loss: 0.00077424
Iteration 6/25 | Loss: 0.00077424
Iteration 7/25 | Loss: 0.00077424
Iteration 8/25 | Loss: 0.00077424
Iteration 9/25 | Loss: 0.00077424
Iteration 10/25 | Loss: 0.00077424
Iteration 11/25 | Loss: 0.00077424
Iteration 12/25 | Loss: 0.00077424
Iteration 13/25 | Loss: 0.00077424
Iteration 14/25 | Loss: 0.00077424
Iteration 15/25 | Loss: 0.00077424
Iteration 16/25 | Loss: 0.00077424
Iteration 17/25 | Loss: 0.00077424
Iteration 18/25 | Loss: 0.00077424
Iteration 19/25 | Loss: 0.00077424
Iteration 20/25 | Loss: 0.00077424
Iteration 21/25 | Loss: 0.00077424
Iteration 22/25 | Loss: 0.00077424
Iteration 23/25 | Loss: 0.00077424
Iteration 24/25 | Loss: 0.00077424
Iteration 25/25 | Loss: 0.00077424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077424
Iteration 2/1000 | Loss: 0.00002872
Iteration 3/1000 | Loss: 0.00001730
Iteration 4/1000 | Loss: 0.00001331
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001097
Iteration 8/1000 | Loss: 0.00001058
Iteration 9/1000 | Loss: 0.00001050
Iteration 10/1000 | Loss: 0.00001040
Iteration 11/1000 | Loss: 0.00001031
Iteration 12/1000 | Loss: 0.00001015
Iteration 13/1000 | Loss: 0.00001009
Iteration 14/1000 | Loss: 0.00001007
Iteration 15/1000 | Loss: 0.00001001
Iteration 16/1000 | Loss: 0.00001000
Iteration 17/1000 | Loss: 0.00001000
Iteration 18/1000 | Loss: 0.00000999
Iteration 19/1000 | Loss: 0.00000998
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000997
Iteration 22/1000 | Loss: 0.00000996
Iteration 23/1000 | Loss: 0.00000994
Iteration 24/1000 | Loss: 0.00000994
Iteration 25/1000 | Loss: 0.00000994
Iteration 26/1000 | Loss: 0.00000994
Iteration 27/1000 | Loss: 0.00000993
Iteration 28/1000 | Loss: 0.00000993
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000991
Iteration 33/1000 | Loss: 0.00000991
Iteration 34/1000 | Loss: 0.00000991
Iteration 35/1000 | Loss: 0.00000991
Iteration 36/1000 | Loss: 0.00000990
Iteration 37/1000 | Loss: 0.00000990
Iteration 38/1000 | Loss: 0.00000990
Iteration 39/1000 | Loss: 0.00000990
Iteration 40/1000 | Loss: 0.00000990
Iteration 41/1000 | Loss: 0.00000990
Iteration 42/1000 | Loss: 0.00000990
Iteration 43/1000 | Loss: 0.00000989
Iteration 44/1000 | Loss: 0.00000989
Iteration 45/1000 | Loss: 0.00000988
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000987
Iteration 50/1000 | Loss: 0.00000987
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000986
Iteration 55/1000 | Loss: 0.00000986
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000985
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000985
Iteration 60/1000 | Loss: 0.00000985
Iteration 61/1000 | Loss: 0.00000985
Iteration 62/1000 | Loss: 0.00000985
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000984
Iteration 65/1000 | Loss: 0.00000984
Iteration 66/1000 | Loss: 0.00000984
Iteration 67/1000 | Loss: 0.00000984
Iteration 68/1000 | Loss: 0.00000984
Iteration 69/1000 | Loss: 0.00000984
Iteration 70/1000 | Loss: 0.00000983
Iteration 71/1000 | Loss: 0.00000983
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000983
Iteration 74/1000 | Loss: 0.00000983
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000982
Iteration 79/1000 | Loss: 0.00000982
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000981
Iteration 82/1000 | Loss: 0.00000981
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000979
Iteration 85/1000 | Loss: 0.00000979
Iteration 86/1000 | Loss: 0.00000979
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000979
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000979
Iteration 92/1000 | Loss: 0.00000979
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000977
Iteration 98/1000 | Loss: 0.00000977
Iteration 99/1000 | Loss: 0.00000977
Iteration 100/1000 | Loss: 0.00000977
Iteration 101/1000 | Loss: 0.00000977
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000976
Iteration 106/1000 | Loss: 0.00000976
Iteration 107/1000 | Loss: 0.00000976
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000975
Iteration 110/1000 | Loss: 0.00000975
Iteration 111/1000 | Loss: 0.00000975
Iteration 112/1000 | Loss: 0.00000974
Iteration 113/1000 | Loss: 0.00000974
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000973
Iteration 117/1000 | Loss: 0.00000973
Iteration 118/1000 | Loss: 0.00000973
Iteration 119/1000 | Loss: 0.00000973
Iteration 120/1000 | Loss: 0.00000973
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000973
Iteration 124/1000 | Loss: 0.00000972
Iteration 125/1000 | Loss: 0.00000972
Iteration 126/1000 | Loss: 0.00000972
Iteration 127/1000 | Loss: 0.00000972
Iteration 128/1000 | Loss: 0.00000972
Iteration 129/1000 | Loss: 0.00000972
Iteration 130/1000 | Loss: 0.00000971
Iteration 131/1000 | Loss: 0.00000971
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000971
Iteration 136/1000 | Loss: 0.00000971
Iteration 137/1000 | Loss: 0.00000971
Iteration 138/1000 | Loss: 0.00000971
Iteration 139/1000 | Loss: 0.00000971
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000971
Iteration 143/1000 | Loss: 0.00000971
Iteration 144/1000 | Loss: 0.00000971
Iteration 145/1000 | Loss: 0.00000971
Iteration 146/1000 | Loss: 0.00000971
Iteration 147/1000 | Loss: 0.00000971
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000969
Iteration 156/1000 | Loss: 0.00000969
Iteration 157/1000 | Loss: 0.00000969
Iteration 158/1000 | Loss: 0.00000969
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000968
Iteration 161/1000 | Loss: 0.00000968
Iteration 162/1000 | Loss: 0.00000968
Iteration 163/1000 | Loss: 0.00000967
Iteration 164/1000 | Loss: 0.00000967
Iteration 165/1000 | Loss: 0.00000966
Iteration 166/1000 | Loss: 0.00000966
Iteration 167/1000 | Loss: 0.00000966
Iteration 168/1000 | Loss: 0.00000966
Iteration 169/1000 | Loss: 0.00000966
Iteration 170/1000 | Loss: 0.00000966
Iteration 171/1000 | Loss: 0.00000965
Iteration 172/1000 | Loss: 0.00000965
Iteration 173/1000 | Loss: 0.00000965
Iteration 174/1000 | Loss: 0.00000965
Iteration 175/1000 | Loss: 0.00000965
Iteration 176/1000 | Loss: 0.00000964
Iteration 177/1000 | Loss: 0.00000964
Iteration 178/1000 | Loss: 0.00000964
Iteration 179/1000 | Loss: 0.00000964
Iteration 180/1000 | Loss: 0.00000963
Iteration 181/1000 | Loss: 0.00000963
Iteration 182/1000 | Loss: 0.00000963
Iteration 183/1000 | Loss: 0.00000963
Iteration 184/1000 | Loss: 0.00000963
Iteration 185/1000 | Loss: 0.00000963
Iteration 186/1000 | Loss: 0.00000963
Iteration 187/1000 | Loss: 0.00000963
Iteration 188/1000 | Loss: 0.00000962
Iteration 189/1000 | Loss: 0.00000962
Iteration 190/1000 | Loss: 0.00000961
Iteration 191/1000 | Loss: 0.00000961
Iteration 192/1000 | Loss: 0.00000961
Iteration 193/1000 | Loss: 0.00000960
Iteration 194/1000 | Loss: 0.00000960
Iteration 195/1000 | Loss: 0.00000960
Iteration 196/1000 | Loss: 0.00000960
Iteration 197/1000 | Loss: 0.00000960
Iteration 198/1000 | Loss: 0.00000960
Iteration 199/1000 | Loss: 0.00000960
Iteration 200/1000 | Loss: 0.00000960
Iteration 201/1000 | Loss: 0.00000960
Iteration 202/1000 | Loss: 0.00000960
Iteration 203/1000 | Loss: 0.00000960
Iteration 204/1000 | Loss: 0.00000959
Iteration 205/1000 | Loss: 0.00000959
Iteration 206/1000 | Loss: 0.00000959
Iteration 207/1000 | Loss: 0.00000959
Iteration 208/1000 | Loss: 0.00000959
Iteration 209/1000 | Loss: 0.00000959
Iteration 210/1000 | Loss: 0.00000959
Iteration 211/1000 | Loss: 0.00000959
Iteration 212/1000 | Loss: 0.00000959
Iteration 213/1000 | Loss: 0.00000959
Iteration 214/1000 | Loss: 0.00000959
Iteration 215/1000 | Loss: 0.00000959
Iteration 216/1000 | Loss: 0.00000959
Iteration 217/1000 | Loss: 0.00000959
Iteration 218/1000 | Loss: 0.00000959
Iteration 219/1000 | Loss: 0.00000959
Iteration 220/1000 | Loss: 0.00000959
Iteration 221/1000 | Loss: 0.00000959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [9.590648915036581e-06, 9.590648915036581e-06, 9.590648915036581e-06, 9.590648915036581e-06, 9.590648915036581e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.590648915036581e-06

Optimization complete. Final v2v error: 2.6117632389068604 mm

Highest mean error: 2.9401164054870605 mm for frame 110

Lowest mean error: 2.3840138912200928 mm for frame 19

Saving results

Total time: 684.1299471855164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1018
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458299
Iteration 2/25 | Loss: 0.00121241
Iteration 3/25 | Loss: 0.00111337
Iteration 4/25 | Loss: 0.00110529
Iteration 5/25 | Loss: 0.00110243
Iteration 6/25 | Loss: 0.00110205
Iteration 7/25 | Loss: 0.00110205
Iteration 8/25 | Loss: 0.00110205
Iteration 9/25 | Loss: 0.00110205
Iteration 10/25 | Loss: 0.00110205
Iteration 11/25 | Loss: 0.00110205
Iteration 12/25 | Loss: 0.00110205
Iteration 13/25 | Loss: 0.00110205
Iteration 14/25 | Loss: 0.00110205
Iteration 15/25 | Loss: 0.00110205
Iteration 16/25 | Loss: 0.00110205
Iteration 17/25 | Loss: 0.00110205
Iteration 18/25 | Loss: 0.00110205
Iteration 19/25 | Loss: 0.00110205
Iteration 20/25 | Loss: 0.00110205
Iteration 21/25 | Loss: 0.00110205
Iteration 22/25 | Loss: 0.00110205
Iteration 23/25 | Loss: 0.00110205
Iteration 24/25 | Loss: 0.00110205
Iteration 25/25 | Loss: 0.00110205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49805951
Iteration 2/25 | Loss: 0.00061716
Iteration 3/25 | Loss: 0.00061714
Iteration 4/25 | Loss: 0.00061714
Iteration 5/25 | Loss: 0.00061714
Iteration 6/25 | Loss: 0.00061714
Iteration 7/25 | Loss: 0.00061714
Iteration 8/25 | Loss: 0.00061714
Iteration 9/25 | Loss: 0.00061714
Iteration 10/25 | Loss: 0.00061714
Iteration 11/25 | Loss: 0.00061714
Iteration 12/25 | Loss: 0.00061714
Iteration 13/25 | Loss: 0.00061714
Iteration 14/25 | Loss: 0.00061714
Iteration 15/25 | Loss: 0.00061714
Iteration 16/25 | Loss: 0.00061714
Iteration 17/25 | Loss: 0.00061714
Iteration 18/25 | Loss: 0.00061714
Iteration 19/25 | Loss: 0.00061714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000617141486145556, 0.000617141486145556, 0.000617141486145556, 0.000617141486145556, 0.000617141486145556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000617141486145556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061714
Iteration 2/1000 | Loss: 0.00002970
Iteration 3/1000 | Loss: 0.00002180
Iteration 4/1000 | Loss: 0.00001929
Iteration 5/1000 | Loss: 0.00001849
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001713
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001651
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001644
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001632
Iteration 19/1000 | Loss: 0.00001631
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001624
Iteration 23/1000 | Loss: 0.00001622
Iteration 24/1000 | Loss: 0.00001621
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001620
Iteration 28/1000 | Loss: 0.00001620
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001615
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001610
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001609
Iteration 59/1000 | Loss: 0.00001609
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001608
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001606
Iteration 67/1000 | Loss: 0.00001606
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001606
Iteration 70/1000 | Loss: 0.00001606
Iteration 71/1000 | Loss: 0.00001605
Iteration 72/1000 | Loss: 0.00001605
Iteration 73/1000 | Loss: 0.00001605
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001603
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001603
Iteration 82/1000 | Loss: 0.00001603
Iteration 83/1000 | Loss: 0.00001602
Iteration 84/1000 | Loss: 0.00001600
Iteration 85/1000 | Loss: 0.00001600
Iteration 86/1000 | Loss: 0.00001600
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001600
Iteration 89/1000 | Loss: 0.00001599
Iteration 90/1000 | Loss: 0.00001599
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001598
Iteration 93/1000 | Loss: 0.00001597
Iteration 94/1000 | Loss: 0.00001597
Iteration 95/1000 | Loss: 0.00001596
Iteration 96/1000 | Loss: 0.00001596
Iteration 97/1000 | Loss: 0.00001596
Iteration 98/1000 | Loss: 0.00001596
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001595
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001593
Iteration 107/1000 | Loss: 0.00001593
Iteration 108/1000 | Loss: 0.00001592
Iteration 109/1000 | Loss: 0.00001592
Iteration 110/1000 | Loss: 0.00001592
Iteration 111/1000 | Loss: 0.00001591
Iteration 112/1000 | Loss: 0.00001591
Iteration 113/1000 | Loss: 0.00001591
Iteration 114/1000 | Loss: 0.00001591
Iteration 115/1000 | Loss: 0.00001590
Iteration 116/1000 | Loss: 0.00001590
Iteration 117/1000 | Loss: 0.00001590
Iteration 118/1000 | Loss: 0.00001589
Iteration 119/1000 | Loss: 0.00001589
Iteration 120/1000 | Loss: 0.00001589
Iteration 121/1000 | Loss: 0.00001589
Iteration 122/1000 | Loss: 0.00001589
Iteration 123/1000 | Loss: 0.00001588
Iteration 124/1000 | Loss: 0.00001588
Iteration 125/1000 | Loss: 0.00001588
Iteration 126/1000 | Loss: 0.00001588
Iteration 127/1000 | Loss: 0.00001588
Iteration 128/1000 | Loss: 0.00001588
Iteration 129/1000 | Loss: 0.00001587
Iteration 130/1000 | Loss: 0.00001587
Iteration 131/1000 | Loss: 0.00001587
Iteration 132/1000 | Loss: 0.00001587
Iteration 133/1000 | Loss: 0.00001587
Iteration 134/1000 | Loss: 0.00001587
Iteration 135/1000 | Loss: 0.00001586
Iteration 136/1000 | Loss: 0.00001586
Iteration 137/1000 | Loss: 0.00001586
Iteration 138/1000 | Loss: 0.00001586
Iteration 139/1000 | Loss: 0.00001585
Iteration 140/1000 | Loss: 0.00001585
Iteration 141/1000 | Loss: 0.00001585
Iteration 142/1000 | Loss: 0.00001585
Iteration 143/1000 | Loss: 0.00001585
Iteration 144/1000 | Loss: 0.00001584
Iteration 145/1000 | Loss: 0.00001584
Iteration 146/1000 | Loss: 0.00001584
Iteration 147/1000 | Loss: 0.00001584
Iteration 148/1000 | Loss: 0.00001584
Iteration 149/1000 | Loss: 0.00001584
Iteration 150/1000 | Loss: 0.00001584
Iteration 151/1000 | Loss: 0.00001583
Iteration 152/1000 | Loss: 0.00001583
Iteration 153/1000 | Loss: 0.00001583
Iteration 154/1000 | Loss: 0.00001583
Iteration 155/1000 | Loss: 0.00001583
Iteration 156/1000 | Loss: 0.00001583
Iteration 157/1000 | Loss: 0.00001583
Iteration 158/1000 | Loss: 0.00001583
Iteration 159/1000 | Loss: 0.00001583
Iteration 160/1000 | Loss: 0.00001583
Iteration 161/1000 | Loss: 0.00001583
Iteration 162/1000 | Loss: 0.00001583
Iteration 163/1000 | Loss: 0.00001583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.5827896277187392e-05, 1.5827896277187392e-05, 1.5827896277187392e-05, 1.5827896277187392e-05, 1.5827896277187392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5827896277187392e-05

Optimization complete. Final v2v error: 3.2565581798553467 mm

Highest mean error: 4.138679027557373 mm for frame 147

Lowest mean error: 2.451401710510254 mm for frame 1

Saving results

Total time: 817.4423985481262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1003
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710438
Iteration 2/25 | Loss: 0.00129468
Iteration 3/25 | Loss: 0.00119842
Iteration 4/25 | Loss: 0.00119388
Iteration 5/25 | Loss: 0.00119338
Iteration 6/25 | Loss: 0.00119338
Iteration 7/25 | Loss: 0.00119338
Iteration 8/25 | Loss: 0.00119338
Iteration 9/25 | Loss: 0.00119338
Iteration 10/25 | Loss: 0.00119338
Iteration 11/25 | Loss: 0.00119338
Iteration 12/25 | Loss: 0.00119338
Iteration 13/25 | Loss: 0.00119338
Iteration 14/25 | Loss: 0.00119338
Iteration 15/25 | Loss: 0.00119338
Iteration 16/25 | Loss: 0.00119338
Iteration 17/25 | Loss: 0.00119338
Iteration 18/25 | Loss: 0.00119338
Iteration 19/25 | Loss: 0.00119338
Iteration 20/25 | Loss: 0.00119338
Iteration 21/25 | Loss: 0.00119338
Iteration 22/25 | Loss: 0.00119338
Iteration 23/25 | Loss: 0.00119338
Iteration 24/25 | Loss: 0.00119338
Iteration 25/25 | Loss: 0.00119338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.83819771
Iteration 2/25 | Loss: 0.00066099
Iteration 3/25 | Loss: 0.00066099
Iteration 4/25 | Loss: 0.00066099
Iteration 5/25 | Loss: 0.00066099
Iteration 6/25 | Loss: 0.00066099
Iteration 7/25 | Loss: 0.00066099
Iteration 8/25 | Loss: 0.00066099
Iteration 9/25 | Loss: 0.00066099
Iteration 10/25 | Loss: 0.00066099
Iteration 11/25 | Loss: 0.00066099
Iteration 12/25 | Loss: 0.00066099
Iteration 13/25 | Loss: 0.00066099
Iteration 14/25 | Loss: 0.00066099
Iteration 15/25 | Loss: 0.00066099
Iteration 16/25 | Loss: 0.00066099
Iteration 17/25 | Loss: 0.00066099
Iteration 18/25 | Loss: 0.00066099
Iteration 19/25 | Loss: 0.00066099
Iteration 20/25 | Loss: 0.00066099
Iteration 21/25 | Loss: 0.00066099
Iteration 22/25 | Loss: 0.00066099
Iteration 23/25 | Loss: 0.00066099
Iteration 24/25 | Loss: 0.00066099
Iteration 25/25 | Loss: 0.00066099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006609875708818436, 0.0006609875708818436, 0.0006609875708818436, 0.0006609875708818436, 0.0006609875708818436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006609875708818436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066099
Iteration 2/1000 | Loss: 0.00003886
Iteration 3/1000 | Loss: 0.00002608
Iteration 4/1000 | Loss: 0.00002222
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001977
Iteration 8/1000 | Loss: 0.00001948
Iteration 9/1000 | Loss: 0.00001916
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001877
Iteration 12/1000 | Loss: 0.00001874
Iteration 13/1000 | Loss: 0.00001862
Iteration 14/1000 | Loss: 0.00001862
Iteration 15/1000 | Loss: 0.00001862
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00001861
Iteration 18/1000 | Loss: 0.00001861
Iteration 19/1000 | Loss: 0.00001860
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001855
Iteration 24/1000 | Loss: 0.00001855
Iteration 25/1000 | Loss: 0.00001855
Iteration 26/1000 | Loss: 0.00001843
Iteration 27/1000 | Loss: 0.00001842
Iteration 28/1000 | Loss: 0.00001841
Iteration 29/1000 | Loss: 0.00001840
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001837
Iteration 32/1000 | Loss: 0.00001837
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001833
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001832
Iteration 37/1000 | Loss: 0.00001832
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001831
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001830
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001829
Iteration 44/1000 | Loss: 0.00001829
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001828
Iteration 47/1000 | Loss: 0.00001828
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001826
Iteration 51/1000 | Loss: 0.00001826
Iteration 52/1000 | Loss: 0.00001826
Iteration 53/1000 | Loss: 0.00001826
Iteration 54/1000 | Loss: 0.00001825
Iteration 55/1000 | Loss: 0.00001825
Iteration 56/1000 | Loss: 0.00001825
Iteration 57/1000 | Loss: 0.00001822
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001819
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001815
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001813
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001812
Iteration 69/1000 | Loss: 0.00001812
Iteration 70/1000 | Loss: 0.00001812
Iteration 71/1000 | Loss: 0.00001812
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001811
Iteration 74/1000 | Loss: 0.00001811
Iteration 75/1000 | Loss: 0.00001811
Iteration 76/1000 | Loss: 0.00001811
Iteration 77/1000 | Loss: 0.00001811
Iteration 78/1000 | Loss: 0.00001811
Iteration 79/1000 | Loss: 0.00001810
Iteration 80/1000 | Loss: 0.00001809
Iteration 81/1000 | Loss: 0.00001809
Iteration 82/1000 | Loss: 0.00001809
Iteration 83/1000 | Loss: 0.00001809
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001808
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001807
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001806
Iteration 100/1000 | Loss: 0.00001806
Iteration 101/1000 | Loss: 0.00001806
Iteration 102/1000 | Loss: 0.00001805
Iteration 103/1000 | Loss: 0.00001805
Iteration 104/1000 | Loss: 0.00001805
Iteration 105/1000 | Loss: 0.00001805
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001804
Iteration 108/1000 | Loss: 0.00001804
Iteration 109/1000 | Loss: 0.00001804
Iteration 110/1000 | Loss: 0.00001804
Iteration 111/1000 | Loss: 0.00001804
Iteration 112/1000 | Loss: 0.00001804
Iteration 113/1000 | Loss: 0.00001804
Iteration 114/1000 | Loss: 0.00001803
Iteration 115/1000 | Loss: 0.00001803
Iteration 116/1000 | Loss: 0.00001803
Iteration 117/1000 | Loss: 0.00001803
Iteration 118/1000 | Loss: 0.00001803
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001801
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001800
Iteration 135/1000 | Loss: 0.00001800
Iteration 136/1000 | Loss: 0.00001800
Iteration 137/1000 | Loss: 0.00001800
Iteration 138/1000 | Loss: 0.00001800
Iteration 139/1000 | Loss: 0.00001800
Iteration 140/1000 | Loss: 0.00001800
Iteration 141/1000 | Loss: 0.00001800
Iteration 142/1000 | Loss: 0.00001799
Iteration 143/1000 | Loss: 0.00001799
Iteration 144/1000 | Loss: 0.00001799
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001799
Iteration 147/1000 | Loss: 0.00001799
Iteration 148/1000 | Loss: 0.00001799
Iteration 149/1000 | Loss: 0.00001799
Iteration 150/1000 | Loss: 0.00001799
Iteration 151/1000 | Loss: 0.00001799
Iteration 152/1000 | Loss: 0.00001799
Iteration 153/1000 | Loss: 0.00001798
Iteration 154/1000 | Loss: 0.00001798
Iteration 155/1000 | Loss: 0.00001798
Iteration 156/1000 | Loss: 0.00001798
Iteration 157/1000 | Loss: 0.00001798
Iteration 158/1000 | Loss: 0.00001798
Iteration 159/1000 | Loss: 0.00001797
Iteration 160/1000 | Loss: 0.00001797
Iteration 161/1000 | Loss: 0.00001797
Iteration 162/1000 | Loss: 0.00001797
Iteration 163/1000 | Loss: 0.00001797
Iteration 164/1000 | Loss: 0.00001797
Iteration 165/1000 | Loss: 0.00001797
Iteration 166/1000 | Loss: 0.00001797
Iteration 167/1000 | Loss: 0.00001797
Iteration 168/1000 | Loss: 0.00001797
Iteration 169/1000 | Loss: 0.00001797
Iteration 170/1000 | Loss: 0.00001797
Iteration 171/1000 | Loss: 0.00001797
Iteration 172/1000 | Loss: 0.00001797
Iteration 173/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.7965247025131248e-05, 1.7965247025131248e-05, 1.7965247025131248e-05, 1.7965247025131248e-05, 1.7965247025131248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7965247025131248e-05

Optimization complete. Final v2v error: 3.515800952911377 mm

Highest mean error: 4.0084547996521 mm for frame 198

Lowest mean error: 3.1681969165802 mm for frame 0

Saving results

Total time: 1278.0308799743652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1047
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815646
Iteration 2/25 | Loss: 0.00126570
Iteration 3/25 | Loss: 0.00109855
Iteration 4/25 | Loss: 0.00108575
Iteration 5/25 | Loss: 0.00108213
Iteration 6/25 | Loss: 0.00108118
Iteration 7/25 | Loss: 0.00108118
Iteration 8/25 | Loss: 0.00108118
Iteration 9/25 | Loss: 0.00108118
Iteration 10/25 | Loss: 0.00108118
Iteration 11/25 | Loss: 0.00108118
Iteration 12/25 | Loss: 0.00108118
Iteration 13/25 | Loss: 0.00108118
Iteration 14/25 | Loss: 0.00108118
Iteration 15/25 | Loss: 0.00108118
Iteration 16/25 | Loss: 0.00108118
Iteration 17/25 | Loss: 0.00108118
Iteration 18/25 | Loss: 0.00108118
Iteration 19/25 | Loss: 0.00108118
Iteration 20/25 | Loss: 0.00108118
Iteration 21/25 | Loss: 0.00108118
Iteration 22/25 | Loss: 0.00108118
Iteration 23/25 | Loss: 0.00108118
Iteration 24/25 | Loss: 0.00108118
Iteration 25/25 | Loss: 0.00108118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.36738253
Iteration 2/25 | Loss: 0.00061858
Iteration 3/25 | Loss: 0.00061852
Iteration 4/25 | Loss: 0.00061852
Iteration 5/25 | Loss: 0.00061852
Iteration 6/25 | Loss: 0.00061852
Iteration 7/25 | Loss: 0.00061852
Iteration 8/25 | Loss: 0.00061852
Iteration 9/25 | Loss: 0.00061852
Iteration 10/25 | Loss: 0.00061852
Iteration 11/25 | Loss: 0.00061852
Iteration 12/25 | Loss: 0.00061852
Iteration 13/25 | Loss: 0.00061852
Iteration 14/25 | Loss: 0.00061852
Iteration 15/25 | Loss: 0.00061852
Iteration 16/25 | Loss: 0.00061852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006185204838402569, 0.0006185204838402569, 0.0006185204838402569, 0.0006185204838402569, 0.0006185204838402569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006185204838402569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061852
Iteration 2/1000 | Loss: 0.00002781
Iteration 3/1000 | Loss: 0.00002058
Iteration 4/1000 | Loss: 0.00001896
Iteration 5/1000 | Loss: 0.00001804
Iteration 6/1000 | Loss: 0.00001738
Iteration 7/1000 | Loss: 0.00001692
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001621
Iteration 10/1000 | Loss: 0.00001602
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001567
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001555
Iteration 17/1000 | Loss: 0.00001554
Iteration 18/1000 | Loss: 0.00001553
Iteration 19/1000 | Loss: 0.00001546
Iteration 20/1000 | Loss: 0.00001546
Iteration 21/1000 | Loss: 0.00001545
Iteration 22/1000 | Loss: 0.00001542
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001541
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001535
Iteration 31/1000 | Loss: 0.00001533
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001532
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001528
Iteration 41/1000 | Loss: 0.00001527
Iteration 42/1000 | Loss: 0.00001527
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00001526
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001525
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001524
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001523
Iteration 53/1000 | Loss: 0.00001523
Iteration 54/1000 | Loss: 0.00001523
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001522
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001520
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001518
Iteration 66/1000 | Loss: 0.00001518
Iteration 67/1000 | Loss: 0.00001517
Iteration 68/1000 | Loss: 0.00001517
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001516
Iteration 71/1000 | Loss: 0.00001516
Iteration 72/1000 | Loss: 0.00001516
Iteration 73/1000 | Loss: 0.00001515
Iteration 74/1000 | Loss: 0.00001515
Iteration 75/1000 | Loss: 0.00001515
Iteration 76/1000 | Loss: 0.00001514
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001513
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001510
Iteration 92/1000 | Loss: 0.00001510
Iteration 93/1000 | Loss: 0.00001509
Iteration 94/1000 | Loss: 0.00001509
Iteration 95/1000 | Loss: 0.00001509
Iteration 96/1000 | Loss: 0.00001509
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001507
Iteration 108/1000 | Loss: 0.00001507
Iteration 109/1000 | Loss: 0.00001507
Iteration 110/1000 | Loss: 0.00001507
Iteration 111/1000 | Loss: 0.00001507
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001507
Iteration 120/1000 | Loss: 0.00001507
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001507
Iteration 129/1000 | Loss: 0.00001507
Iteration 130/1000 | Loss: 0.00001507
Iteration 131/1000 | Loss: 0.00001507
Iteration 132/1000 | Loss: 0.00001507
Iteration 133/1000 | Loss: 0.00001507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.506894204794662e-05, 1.506894204794662e-05, 1.506894204794662e-05, 1.506894204794662e-05, 1.506894204794662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.506894204794662e-05

Optimization complete. Final v2v error: 3.274742603302002 mm

Highest mean error: 4.36795711517334 mm for frame 66

Lowest mean error: 2.683800458908081 mm for frame 174

Saving results

Total time: 1341.5652768611908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1016
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619393
Iteration 2/25 | Loss: 0.00134927
Iteration 3/25 | Loss: 0.00116154
Iteration 4/25 | Loss: 0.00112142
Iteration 5/25 | Loss: 0.00109779
Iteration 6/25 | Loss: 0.00109395
Iteration 7/25 | Loss: 0.00109331
Iteration 8/25 | Loss: 0.00109293
Iteration 9/25 | Loss: 0.00109246
Iteration 10/25 | Loss: 0.00109546
Iteration 11/25 | Loss: 0.00108562
Iteration 12/25 | Loss: 0.00108422
Iteration 13/25 | Loss: 0.00108410
Iteration 14/25 | Loss: 0.00108410
Iteration 15/25 | Loss: 0.00108410
Iteration 16/25 | Loss: 0.00108410
Iteration 17/25 | Loss: 0.00108410
Iteration 18/25 | Loss: 0.00108410
Iteration 19/25 | Loss: 0.00108410
Iteration 20/25 | Loss: 0.00108410
Iteration 21/25 | Loss: 0.00108410
Iteration 22/25 | Loss: 0.00108410
Iteration 23/25 | Loss: 0.00108409
Iteration 24/25 | Loss: 0.00108409
Iteration 25/25 | Loss: 0.00108409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56647587
Iteration 2/25 | Loss: 0.00080880
Iteration 3/25 | Loss: 0.00080880
Iteration 4/25 | Loss: 0.00080880
Iteration 5/25 | Loss: 0.00080879
Iteration 6/25 | Loss: 0.00080879
Iteration 7/25 | Loss: 0.00080879
Iteration 8/25 | Loss: 0.00080879
Iteration 9/25 | Loss: 0.00080879
Iteration 10/25 | Loss: 0.00080879
Iteration 11/25 | Loss: 0.00080879
Iteration 12/25 | Loss: 0.00080879
Iteration 13/25 | Loss: 0.00080879
Iteration 14/25 | Loss: 0.00080879
Iteration 15/25 | Loss: 0.00080879
Iteration 16/25 | Loss: 0.00080879
Iteration 17/25 | Loss: 0.00080879
Iteration 18/25 | Loss: 0.00080879
Iteration 19/25 | Loss: 0.00080879
Iteration 20/25 | Loss: 0.00080879
Iteration 21/25 | Loss: 0.00080879
Iteration 22/25 | Loss: 0.00080879
Iteration 23/25 | Loss: 0.00080879
Iteration 24/25 | Loss: 0.00080879
Iteration 25/25 | Loss: 0.00080879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080879
Iteration 2/1000 | Loss: 0.00003290
Iteration 3/1000 | Loss: 0.00002201
Iteration 4/1000 | Loss: 0.00001964
Iteration 5/1000 | Loss: 0.00001843
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001656
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001596
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001578
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001553
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001551
Iteration 22/1000 | Loss: 0.00001551
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001548
Iteration 28/1000 | Loss: 0.00001548
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001547
Iteration 31/1000 | Loss: 0.00001547
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001546
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001545
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001545
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001543
Iteration 40/1000 | Loss: 0.00001543
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001542
Iteration 43/1000 | Loss: 0.00001542
Iteration 44/1000 | Loss: 0.00001542
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001540
Iteration 50/1000 | Loss: 0.00001540
Iteration 51/1000 | Loss: 0.00001540
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00001540
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001539
Iteration 57/1000 | Loss: 0.00001539
Iteration 58/1000 | Loss: 0.00001539
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001538
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001537
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001537
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001537
Iteration 69/1000 | Loss: 0.00001536
Iteration 70/1000 | Loss: 0.00001536
Iteration 71/1000 | Loss: 0.00001535
Iteration 72/1000 | Loss: 0.00001535
Iteration 73/1000 | Loss: 0.00001535
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001533
Iteration 80/1000 | Loss: 0.00001533
Iteration 81/1000 | Loss: 0.00001533
Iteration 82/1000 | Loss: 0.00001533
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001533
Iteration 86/1000 | Loss: 0.00001532
Iteration 87/1000 | Loss: 0.00001532
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001532
Iteration 91/1000 | Loss: 0.00001532
Iteration 92/1000 | Loss: 0.00001532
Iteration 93/1000 | Loss: 0.00001532
Iteration 94/1000 | Loss: 0.00001532
Iteration 95/1000 | Loss: 0.00001531
Iteration 96/1000 | Loss: 0.00001531
Iteration 97/1000 | Loss: 0.00001531
Iteration 98/1000 | Loss: 0.00001531
Iteration 99/1000 | Loss: 0.00001531
Iteration 100/1000 | Loss: 0.00001531
Iteration 101/1000 | Loss: 0.00001531
Iteration 102/1000 | Loss: 0.00001530
Iteration 103/1000 | Loss: 0.00001530
Iteration 104/1000 | Loss: 0.00001530
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001529
Iteration 107/1000 | Loss: 0.00001529
Iteration 108/1000 | Loss: 0.00001529
Iteration 109/1000 | Loss: 0.00001529
Iteration 110/1000 | Loss: 0.00001529
Iteration 111/1000 | Loss: 0.00001529
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001529
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001528
Iteration 118/1000 | Loss: 0.00001528
Iteration 119/1000 | Loss: 0.00001528
Iteration 120/1000 | Loss: 0.00001528
Iteration 121/1000 | Loss: 0.00001528
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001528
Iteration 124/1000 | Loss: 0.00001528
Iteration 125/1000 | Loss: 0.00001528
Iteration 126/1000 | Loss: 0.00001528
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.5276125850505196e-05, 1.5276125850505196e-05, 1.5276125850505196e-05, 1.5276125850505196e-05, 1.5276125850505196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5276125850505196e-05

Optimization complete. Final v2v error: 3.2092344760894775 mm

Highest mean error: 4.152978420257568 mm for frame 126

Lowest mean error: 2.657782554626465 mm for frame 48

Saving results

Total time: 1914.3864748477936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1065
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832095
Iteration 2/25 | Loss: 0.00130164
Iteration 3/25 | Loss: 0.00112098
Iteration 4/25 | Loss: 0.00108626
Iteration 5/25 | Loss: 0.00108001
Iteration 6/25 | Loss: 0.00107961
Iteration 7/25 | Loss: 0.00107961
Iteration 8/25 | Loss: 0.00107961
Iteration 9/25 | Loss: 0.00107961
Iteration 10/25 | Loss: 0.00107961
Iteration 11/25 | Loss: 0.00107961
Iteration 12/25 | Loss: 0.00107961
Iteration 13/25 | Loss: 0.00107961
Iteration 14/25 | Loss: 0.00107961
Iteration 15/25 | Loss: 0.00107961
Iteration 16/25 | Loss: 0.00107961
Iteration 17/25 | Loss: 0.00107961
Iteration 18/25 | Loss: 0.00107961
Iteration 19/25 | Loss: 0.00107961
Iteration 20/25 | Loss: 0.00107961
Iteration 21/25 | Loss: 0.00107961
Iteration 22/25 | Loss: 0.00107961
Iteration 23/25 | Loss: 0.00107961
Iteration 24/25 | Loss: 0.00107961
Iteration 25/25 | Loss: 0.00107961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98591602
Iteration 2/25 | Loss: 0.00036656
Iteration 3/25 | Loss: 0.00036655
Iteration 4/25 | Loss: 0.00036655
Iteration 5/25 | Loss: 0.00036655
Iteration 6/25 | Loss: 0.00036655
Iteration 7/25 | Loss: 0.00036655
Iteration 8/25 | Loss: 0.00036655
Iteration 9/25 | Loss: 0.00036655
Iteration 10/25 | Loss: 0.00036655
Iteration 11/25 | Loss: 0.00036655
Iteration 12/25 | Loss: 0.00036654
Iteration 13/25 | Loss: 0.00036654
Iteration 14/25 | Loss: 0.00036654
Iteration 15/25 | Loss: 0.00036655
Iteration 16/25 | Loss: 0.00036655
Iteration 17/25 | Loss: 0.00036655
Iteration 18/25 | Loss: 0.00036655
Iteration 19/25 | Loss: 0.00036655
Iteration 20/25 | Loss: 0.00036655
Iteration 21/25 | Loss: 0.00036655
Iteration 22/25 | Loss: 0.00036655
Iteration 23/25 | Loss: 0.00036655
Iteration 24/25 | Loss: 0.00036655
Iteration 25/25 | Loss: 0.00036655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036655
Iteration 2/1000 | Loss: 0.00003168
Iteration 3/1000 | Loss: 0.00002563
Iteration 4/1000 | Loss: 0.00002372
Iteration 5/1000 | Loss: 0.00002255
Iteration 6/1000 | Loss: 0.00002152
Iteration 7/1000 | Loss: 0.00002096
Iteration 8/1000 | Loss: 0.00002057
Iteration 9/1000 | Loss: 0.00002027
Iteration 10/1000 | Loss: 0.00002002
Iteration 11/1000 | Loss: 0.00001991
Iteration 12/1000 | Loss: 0.00001979
Iteration 13/1000 | Loss: 0.00001967
Iteration 14/1000 | Loss: 0.00001962
Iteration 15/1000 | Loss: 0.00001953
Iteration 16/1000 | Loss: 0.00001952
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00001952
Iteration 19/1000 | Loss: 0.00001952
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001951
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001951
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00001948
Iteration 26/1000 | Loss: 0.00001948
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001945
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001944
Iteration 32/1000 | Loss: 0.00001944
Iteration 33/1000 | Loss: 0.00001944
Iteration 34/1000 | Loss: 0.00001943
Iteration 35/1000 | Loss: 0.00001943
Iteration 36/1000 | Loss: 0.00001943
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001942
Iteration 40/1000 | Loss: 0.00001942
Iteration 41/1000 | Loss: 0.00001942
Iteration 42/1000 | Loss: 0.00001942
Iteration 43/1000 | Loss: 0.00001942
Iteration 44/1000 | Loss: 0.00001942
Iteration 45/1000 | Loss: 0.00001942
Iteration 46/1000 | Loss: 0.00001942
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001942
Iteration 49/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.942283779499121e-05, 1.942283779499121e-05, 1.942283779499121e-05, 1.942283779499121e-05, 1.942283779499121e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.942283779499121e-05

Optimization complete. Final v2v error: 3.7813045978546143 mm

Highest mean error: 4.409679412841797 mm for frame 239

Lowest mean error: 3.3716588020324707 mm for frame 3

Saving results

Total time: 1095.7874975204468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1095
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00613047
Iteration 2/25 | Loss: 0.00130292
Iteration 3/25 | Loss: 0.00121622
Iteration 4/25 | Loss: 0.00119923
Iteration 5/25 | Loss: 0.00119684
Iteration 6/25 | Loss: 0.00119602
Iteration 7/25 | Loss: 0.00119588
Iteration 8/25 | Loss: 0.00119588
Iteration 9/25 | Loss: 0.00119588
Iteration 10/25 | Loss: 0.00119588
Iteration 11/25 | Loss: 0.00119588
Iteration 12/25 | Loss: 0.00119588
Iteration 13/25 | Loss: 0.00119588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00119587907101959, 0.00119587907101959, 0.00119587907101959, 0.00119587907101959, 0.00119587907101959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00119587907101959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70719290
Iteration 2/25 | Loss: 0.00108469
Iteration 3/25 | Loss: 0.00108468
Iteration 4/25 | Loss: 0.00108468
Iteration 5/25 | Loss: 0.00108468
Iteration 6/25 | Loss: 0.00108468
Iteration 7/25 | Loss: 0.00108467
Iteration 8/25 | Loss: 0.00108467
Iteration 9/25 | Loss: 0.00108467
Iteration 10/25 | Loss: 0.00108467
Iteration 11/25 | Loss: 0.00108467
Iteration 12/25 | Loss: 0.00108467
Iteration 13/25 | Loss: 0.00108467
Iteration 14/25 | Loss: 0.00108467
Iteration 15/25 | Loss: 0.00108467
Iteration 16/25 | Loss: 0.00108467
Iteration 17/25 | Loss: 0.00108467
Iteration 18/25 | Loss: 0.00108467
Iteration 19/25 | Loss: 0.00108467
Iteration 20/25 | Loss: 0.00108467
Iteration 21/25 | Loss: 0.00108467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010846739169210196, 0.0010846739169210196, 0.0010846739169210196, 0.0010846739169210196, 0.0010846739169210196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010846739169210196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108467
Iteration 2/1000 | Loss: 0.00004812
Iteration 3/1000 | Loss: 0.00003128
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002053
Iteration 6/1000 | Loss: 0.00001935
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001798
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001758
Iteration 11/1000 | Loss: 0.00001750
Iteration 12/1000 | Loss: 0.00001734
Iteration 13/1000 | Loss: 0.00001722
Iteration 14/1000 | Loss: 0.00001719
Iteration 15/1000 | Loss: 0.00001718
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001716
Iteration 18/1000 | Loss: 0.00001716
Iteration 19/1000 | Loss: 0.00001716
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001716
Iteration 22/1000 | Loss: 0.00001715
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00001715
Iteration 25/1000 | Loss: 0.00001715
Iteration 26/1000 | Loss: 0.00001715
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001715
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001715
Iteration 33/1000 | Loss: 0.00001715
Iteration 34/1000 | Loss: 0.00001715
Iteration 35/1000 | Loss: 0.00001714
Iteration 36/1000 | Loss: 0.00001714
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00001714
Iteration 40/1000 | Loss: 0.00001714
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.7142197975772433e-05, 1.7142197975772433e-05, 1.7142197975772433e-05, 1.7142197975772433e-05, 1.7142197975772433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7142197975772433e-05

Optimization complete. Final v2v error: 3.490769147872925 mm

Highest mean error: 4.33831262588501 mm for frame 0

Lowest mean error: 3.37894868850708 mm for frame 29

Saving results

Total time: 483.8410758972168
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1093
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029959
Iteration 2/25 | Loss: 0.00356224
Iteration 3/25 | Loss: 0.00376079
Iteration 4/25 | Loss: 0.00214066
Iteration 5/25 | Loss: 0.00170091
Iteration 6/25 | Loss: 0.00156029
Iteration 7/25 | Loss: 0.00147177
Iteration 8/25 | Loss: 0.00130960
Iteration 9/25 | Loss: 0.00124966
Iteration 10/25 | Loss: 0.00119264
Iteration 11/25 | Loss: 0.00117598
Iteration 12/25 | Loss: 0.00117033
Iteration 13/25 | Loss: 0.00117081
Iteration 14/25 | Loss: 0.00116237
Iteration 15/25 | Loss: 0.00116187
Iteration 16/25 | Loss: 0.00116175
Iteration 17/25 | Loss: 0.00116169
Iteration 18/25 | Loss: 0.00116169
Iteration 19/25 | Loss: 0.00116169
Iteration 20/25 | Loss: 0.00116169
Iteration 21/25 | Loss: 0.00116169
Iteration 22/25 | Loss: 0.00116169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011616861447691917, 0.0011616861447691917, 0.0011616861447691917, 0.0011616861447691917, 0.0011616861447691917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011616861447691917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36807871
Iteration 2/25 | Loss: 0.00076257
Iteration 3/25 | Loss: 0.00066231
Iteration 4/25 | Loss: 0.00066231
Iteration 5/25 | Loss: 0.00066231
Iteration 6/25 | Loss: 0.00066231
Iteration 7/25 | Loss: 0.00066231
Iteration 8/25 | Loss: 0.00066231
Iteration 9/25 | Loss: 0.00066231
Iteration 10/25 | Loss: 0.00066231
Iteration 11/25 | Loss: 0.00066231
Iteration 12/25 | Loss: 0.00066231
Iteration 13/25 | Loss: 0.00066231
Iteration 14/25 | Loss: 0.00066231
Iteration 15/25 | Loss: 0.00066231
Iteration 16/25 | Loss: 0.00066231
Iteration 17/25 | Loss: 0.00066231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006623052759096026, 0.0006623052759096026, 0.0006623052759096026, 0.0006623052759096026, 0.0006623052759096026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006623052759096026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066231
Iteration 2/1000 | Loss: 0.00033499
Iteration 3/1000 | Loss: 0.00084920
Iteration 4/1000 | Loss: 0.00003607
Iteration 5/1000 | Loss: 0.00018028
Iteration 6/1000 | Loss: 0.00002816
Iteration 7/1000 | Loss: 0.00002493
Iteration 8/1000 | Loss: 0.00010423
Iteration 9/1000 | Loss: 0.00005363
Iteration 10/1000 | Loss: 0.00002741
Iteration 11/1000 | Loss: 0.00002295
Iteration 12/1000 | Loss: 0.00002252
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00012807
Iteration 15/1000 | Loss: 0.00002199
Iteration 16/1000 | Loss: 0.00006129
Iteration 17/1000 | Loss: 0.00002691
Iteration 18/1000 | Loss: 0.00002307
Iteration 19/1000 | Loss: 0.00002161
Iteration 20/1000 | Loss: 0.00002071
Iteration 21/1000 | Loss: 0.00002012
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001923
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001919
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001919
Iteration 31/1000 | Loss: 0.00001918
Iteration 32/1000 | Loss: 0.00001914
Iteration 33/1000 | Loss: 0.00001914
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001904
Iteration 37/1000 | Loss: 0.00001902
Iteration 38/1000 | Loss: 0.00001898
Iteration 39/1000 | Loss: 0.00001898
Iteration 40/1000 | Loss: 0.00001898
Iteration 41/1000 | Loss: 0.00001898
Iteration 42/1000 | Loss: 0.00001898
Iteration 43/1000 | Loss: 0.00001898
Iteration 44/1000 | Loss: 0.00001898
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001896
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001895
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001893
Iteration 59/1000 | Loss: 0.00001893
Iteration 60/1000 | Loss: 0.00001893
Iteration 61/1000 | Loss: 0.00001893
Iteration 62/1000 | Loss: 0.00001892
Iteration 63/1000 | Loss: 0.00001892
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001891
Iteration 66/1000 | Loss: 0.00001890
Iteration 67/1000 | Loss: 0.00001890
Iteration 68/1000 | Loss: 0.00001890
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001889
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001885
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001885
Iteration 91/1000 | Loss: 0.00001884
Iteration 92/1000 | Loss: 0.00001883
Iteration 93/1000 | Loss: 0.00001883
Iteration 94/1000 | Loss: 0.00001883
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001882
Iteration 98/1000 | Loss: 0.00001882
Iteration 99/1000 | Loss: 0.00001882
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001880
Iteration 107/1000 | Loss: 0.00001880
Iteration 108/1000 | Loss: 0.00001880
Iteration 109/1000 | Loss: 0.00001880
Iteration 110/1000 | Loss: 0.00001880
Iteration 111/1000 | Loss: 0.00001880
Iteration 112/1000 | Loss: 0.00001880
Iteration 113/1000 | Loss: 0.00001880
Iteration 114/1000 | Loss: 0.00001879
Iteration 115/1000 | Loss: 0.00001879
Iteration 116/1000 | Loss: 0.00001879
Iteration 117/1000 | Loss: 0.00001879
Iteration 118/1000 | Loss: 0.00001879
Iteration 119/1000 | Loss: 0.00001879
Iteration 120/1000 | Loss: 0.00001879
Iteration 121/1000 | Loss: 0.00001879
Iteration 122/1000 | Loss: 0.00001879
Iteration 123/1000 | Loss: 0.00001879
Iteration 124/1000 | Loss: 0.00001879
Iteration 125/1000 | Loss: 0.00001878
Iteration 126/1000 | Loss: 0.00001878
Iteration 127/1000 | Loss: 0.00001878
Iteration 128/1000 | Loss: 0.00001878
Iteration 129/1000 | Loss: 0.00001878
Iteration 130/1000 | Loss: 0.00001878
Iteration 131/1000 | Loss: 0.00001878
Iteration 132/1000 | Loss: 0.00001878
Iteration 133/1000 | Loss: 0.00001878
Iteration 134/1000 | Loss: 0.00001878
Iteration 135/1000 | Loss: 0.00001878
Iteration 136/1000 | Loss: 0.00001878
Iteration 137/1000 | Loss: 0.00001878
Iteration 138/1000 | Loss: 0.00001878
Iteration 139/1000 | Loss: 0.00001877
Iteration 140/1000 | Loss: 0.00001877
Iteration 141/1000 | Loss: 0.00001877
Iteration 142/1000 | Loss: 0.00001877
Iteration 143/1000 | Loss: 0.00001877
Iteration 144/1000 | Loss: 0.00001877
Iteration 145/1000 | Loss: 0.00001877
Iteration 146/1000 | Loss: 0.00001877
Iteration 147/1000 | Loss: 0.00001877
Iteration 148/1000 | Loss: 0.00001877
Iteration 149/1000 | Loss: 0.00001877
Iteration 150/1000 | Loss: 0.00001877
Iteration 151/1000 | Loss: 0.00001877
Iteration 152/1000 | Loss: 0.00001877
Iteration 153/1000 | Loss: 0.00001877
Iteration 154/1000 | Loss: 0.00001877
Iteration 155/1000 | Loss: 0.00001877
Iteration 156/1000 | Loss: 0.00001877
Iteration 157/1000 | Loss: 0.00001877
Iteration 158/1000 | Loss: 0.00001877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.8773955162032507e-05, 1.8773955162032507e-05, 1.8773955162032507e-05, 1.8773955162032507e-05, 1.8773955162032507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8773955162032507e-05

Optimization complete. Final v2v error: 3.696352958679199 mm

Highest mean error: 4.47661828994751 mm for frame 102

Lowest mean error: 3.5236663818359375 mm for frame 139

Saving results

Total time: 2820.950394630432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1092
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971883
Iteration 2/25 | Loss: 0.00232693
Iteration 3/25 | Loss: 0.00159544
Iteration 4/25 | Loss: 0.00150658
Iteration 5/25 | Loss: 0.00154087
Iteration 6/25 | Loss: 0.00150028
Iteration 7/25 | Loss: 0.00145068
Iteration 8/25 | Loss: 0.00142198
Iteration 9/25 | Loss: 0.00141123
Iteration 10/25 | Loss: 0.00141641
Iteration 11/25 | Loss: 0.00141691
Iteration 12/25 | Loss: 0.00140688
Iteration 13/25 | Loss: 0.00140550
Iteration 14/25 | Loss: 0.00140738
Iteration 15/25 | Loss: 0.00139881
Iteration 16/25 | Loss: 0.00139347
Iteration 17/25 | Loss: 0.00139593
Iteration 18/25 | Loss: 0.00139643
Iteration 19/25 | Loss: 0.00139088
Iteration 20/25 | Loss: 0.00138450
Iteration 21/25 | Loss: 0.00138087
Iteration 22/25 | Loss: 0.00138019
Iteration 23/25 | Loss: 0.00138255
Iteration 24/25 | Loss: 0.00138238
Iteration 25/25 | Loss: 0.00138118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38890886
Iteration 2/25 | Loss: 0.00235530
Iteration 3/25 | Loss: 0.00235530
Iteration 4/25 | Loss: 0.00235530
Iteration 5/25 | Loss: 0.00235530
Iteration 6/25 | Loss: 0.00235530
Iteration 7/25 | Loss: 0.00235530
Iteration 8/25 | Loss: 0.00235530
Iteration 9/25 | Loss: 0.00235530
Iteration 10/25 | Loss: 0.00235530
Iteration 11/25 | Loss: 0.00235530
Iteration 12/25 | Loss: 0.00235529
Iteration 13/25 | Loss: 0.00235529
Iteration 14/25 | Loss: 0.00235529
Iteration 15/25 | Loss: 0.00235529
Iteration 16/25 | Loss: 0.00235529
Iteration 17/25 | Loss: 0.00235529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023552945349365473, 0.0023552945349365473, 0.0023552945349365473, 0.0023552945349365473, 0.0023552945349365473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023552945349365473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235529
Iteration 2/1000 | Loss: 0.00053305
Iteration 3/1000 | Loss: 0.00047920
Iteration 4/1000 | Loss: 0.00024911
Iteration 5/1000 | Loss: 0.00059958
Iteration 6/1000 | Loss: 0.00056623
Iteration 7/1000 | Loss: 0.00043104
Iteration 8/1000 | Loss: 0.00043651
Iteration 9/1000 | Loss: 0.00020002
Iteration 10/1000 | Loss: 0.00019130
Iteration 11/1000 | Loss: 0.00017693
Iteration 12/1000 | Loss: 0.00021071
Iteration 13/1000 | Loss: 0.00015876
Iteration 14/1000 | Loss: 0.00016165
Iteration 15/1000 | Loss: 0.00014412
Iteration 16/1000 | Loss: 0.00015805
Iteration 17/1000 | Loss: 0.00038471
Iteration 18/1000 | Loss: 0.00035118
Iteration 19/1000 | Loss: 0.00028518
Iteration 20/1000 | Loss: 0.00038238
Iteration 21/1000 | Loss: 0.00035475
Iteration 22/1000 | Loss: 0.00037000
Iteration 23/1000 | Loss: 0.00033655
Iteration 24/1000 | Loss: 0.00040803
Iteration 25/1000 | Loss: 0.00015682
Iteration 26/1000 | Loss: 0.00016110
Iteration 27/1000 | Loss: 0.00075210
Iteration 28/1000 | Loss: 0.00172126
Iteration 29/1000 | Loss: 0.00083053
Iteration 30/1000 | Loss: 0.00539527
Iteration 31/1000 | Loss: 0.01073615
Iteration 32/1000 | Loss: 0.00056843
Iteration 33/1000 | Loss: 0.00027710
Iteration 34/1000 | Loss: 0.00020962
Iteration 35/1000 | Loss: 0.00050446
Iteration 36/1000 | Loss: 0.00011109
Iteration 37/1000 | Loss: 0.00009293
Iteration 38/1000 | Loss: 0.00008351
Iteration 39/1000 | Loss: 0.00006610
Iteration 40/1000 | Loss: 0.00004517
Iteration 41/1000 | Loss: 0.00003822
Iteration 42/1000 | Loss: 0.00005288
Iteration 43/1000 | Loss: 0.00004936
Iteration 44/1000 | Loss: 0.00004362
Iteration 45/1000 | Loss: 0.00003318
Iteration 46/1000 | Loss: 0.00002885
Iteration 47/1000 | Loss: 0.00003196
Iteration 48/1000 | Loss: 0.00004633
Iteration 49/1000 | Loss: 0.00003519
Iteration 50/1000 | Loss: 0.00003857
Iteration 51/1000 | Loss: 0.00003840
Iteration 52/1000 | Loss: 0.00002947
Iteration 53/1000 | Loss: 0.00002368
Iteration 54/1000 | Loss: 0.00003473
Iteration 55/1000 | Loss: 0.00003390
Iteration 56/1000 | Loss: 0.00003925
Iteration 57/1000 | Loss: 0.00004196
Iteration 58/1000 | Loss: 0.00004017
Iteration 59/1000 | Loss: 0.00004217
Iteration 60/1000 | Loss: 0.00004000
Iteration 61/1000 | Loss: 0.00005358
Iteration 62/1000 | Loss: 0.00005123
Iteration 63/1000 | Loss: 0.00003821
Iteration 64/1000 | Loss: 0.00004094
Iteration 65/1000 | Loss: 0.00003697
Iteration 66/1000 | Loss: 0.00004048
Iteration 67/1000 | Loss: 0.00003743
Iteration 68/1000 | Loss: 0.00004020
Iteration 69/1000 | Loss: 0.00004076
Iteration 70/1000 | Loss: 0.00002869
Iteration 71/1000 | Loss: 0.00002889
Iteration 72/1000 | Loss: 0.00004002
Iteration 73/1000 | Loss: 0.00004095
Iteration 74/1000 | Loss: 0.00003901
Iteration 75/1000 | Loss: 0.00003851
Iteration 76/1000 | Loss: 0.00003999
Iteration 77/1000 | Loss: 0.00004172
Iteration 78/1000 | Loss: 0.00002849
Iteration 79/1000 | Loss: 0.00003373
Iteration 80/1000 | Loss: 0.00004075
Iteration 81/1000 | Loss: 0.00003875
Iteration 82/1000 | Loss: 0.00003995
Iteration 83/1000 | Loss: 0.00003854
Iteration 84/1000 | Loss: 0.00003947
Iteration 85/1000 | Loss: 0.00004152
Iteration 86/1000 | Loss: 0.00003929
Iteration 87/1000 | Loss: 0.00003804
Iteration 88/1000 | Loss: 0.00003836
Iteration 89/1000 | Loss: 0.00003785
Iteration 90/1000 | Loss: 0.00003696
Iteration 91/1000 | Loss: 0.00002900
Iteration 92/1000 | Loss: 0.00004555
Iteration 93/1000 | Loss: 0.00004970
Iteration 94/1000 | Loss: 0.00004230
Iteration 95/1000 | Loss: 0.00004748
Iteration 96/1000 | Loss: 0.00004067
Iteration 97/1000 | Loss: 0.00004557
Iteration 98/1000 | Loss: 0.00003029
Iteration 99/1000 | Loss: 0.00002615
Iteration 100/1000 | Loss: 0.00002425
Iteration 101/1000 | Loss: 0.00002264
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002025
Iteration 104/1000 | Loss: 0.00001997
Iteration 105/1000 | Loss: 0.00001997
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001965
Iteration 109/1000 | Loss: 0.00001949
Iteration 110/1000 | Loss: 0.00001942
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001936
Iteration 114/1000 | Loss: 0.00001935
Iteration 115/1000 | Loss: 0.00001934
Iteration 116/1000 | Loss: 0.00001934
Iteration 117/1000 | Loss: 0.00001933
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001930
Iteration 120/1000 | Loss: 0.00001930
Iteration 121/1000 | Loss: 0.00001930
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001928
Iteration 125/1000 | Loss: 0.00001928
Iteration 126/1000 | Loss: 0.00001928
Iteration 127/1000 | Loss: 0.00001927
Iteration 128/1000 | Loss: 0.00001927
Iteration 129/1000 | Loss: 0.00001927
Iteration 130/1000 | Loss: 0.00001927
Iteration 131/1000 | Loss: 0.00001926
Iteration 132/1000 | Loss: 0.00001926
Iteration 133/1000 | Loss: 0.00001926
Iteration 134/1000 | Loss: 0.00001926
Iteration 135/1000 | Loss: 0.00001926
Iteration 136/1000 | Loss: 0.00001926
Iteration 137/1000 | Loss: 0.00001926
Iteration 138/1000 | Loss: 0.00001925
Iteration 139/1000 | Loss: 0.00001925
Iteration 140/1000 | Loss: 0.00001925
Iteration 141/1000 | Loss: 0.00001925
Iteration 142/1000 | Loss: 0.00001925
Iteration 143/1000 | Loss: 0.00001925
Iteration 144/1000 | Loss: 0.00001925
Iteration 145/1000 | Loss: 0.00001924
Iteration 146/1000 | Loss: 0.00001924
Iteration 147/1000 | Loss: 0.00001924
Iteration 148/1000 | Loss: 0.00001924
Iteration 149/1000 | Loss: 0.00001923
Iteration 150/1000 | Loss: 0.00001923
Iteration 151/1000 | Loss: 0.00001923
Iteration 152/1000 | Loss: 0.00001922
Iteration 153/1000 | Loss: 0.00001922
Iteration 154/1000 | Loss: 0.00001922
Iteration 155/1000 | Loss: 0.00001921
Iteration 156/1000 | Loss: 0.00001921
Iteration 157/1000 | Loss: 0.00001921
Iteration 158/1000 | Loss: 0.00001920
Iteration 159/1000 | Loss: 0.00001920
Iteration 160/1000 | Loss: 0.00003222
Iteration 161/1000 | Loss: 0.00002860
Iteration 162/1000 | Loss: 0.00002541
Iteration 163/1000 | Loss: 0.00002139
Iteration 164/1000 | Loss: 0.00001942
Iteration 165/1000 | Loss: 0.00001824
Iteration 166/1000 | Loss: 0.00001775
Iteration 167/1000 | Loss: 0.00001745
Iteration 168/1000 | Loss: 0.00001740
Iteration 169/1000 | Loss: 0.00001739
Iteration 170/1000 | Loss: 0.00001739
Iteration 171/1000 | Loss: 0.00001734
Iteration 172/1000 | Loss: 0.00001733
Iteration 173/1000 | Loss: 0.00001732
Iteration 174/1000 | Loss: 0.00001729
Iteration 175/1000 | Loss: 0.00001729
Iteration 176/1000 | Loss: 0.00001728
Iteration 177/1000 | Loss: 0.00001727
Iteration 178/1000 | Loss: 0.00001726
Iteration 179/1000 | Loss: 0.00001726
Iteration 180/1000 | Loss: 0.00001725
Iteration 181/1000 | Loss: 0.00001725
Iteration 182/1000 | Loss: 0.00001724
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001719
Iteration 186/1000 | Loss: 0.00001719
Iteration 187/1000 | Loss: 0.00001718
Iteration 188/1000 | Loss: 0.00001717
Iteration 189/1000 | Loss: 0.00001717
Iteration 190/1000 | Loss: 0.00001715
Iteration 191/1000 | Loss: 0.00001715
Iteration 192/1000 | Loss: 0.00001715
Iteration 193/1000 | Loss: 0.00001715
Iteration 194/1000 | Loss: 0.00001715
Iteration 195/1000 | Loss: 0.00001715
Iteration 196/1000 | Loss: 0.00001714
Iteration 197/1000 | Loss: 0.00001714
Iteration 198/1000 | Loss: 0.00001714
Iteration 199/1000 | Loss: 0.00001714
Iteration 200/1000 | Loss: 0.00001713
Iteration 201/1000 | Loss: 0.00001713
Iteration 202/1000 | Loss: 0.00001713
Iteration 203/1000 | Loss: 0.00001712
Iteration 204/1000 | Loss: 0.00001712
Iteration 205/1000 | Loss: 0.00001712
Iteration 206/1000 | Loss: 0.00001711
Iteration 207/1000 | Loss: 0.00001711
Iteration 208/1000 | Loss: 0.00001711
Iteration 209/1000 | Loss: 0.00001711
Iteration 210/1000 | Loss: 0.00001711
Iteration 211/1000 | Loss: 0.00001711
Iteration 212/1000 | Loss: 0.00001711
Iteration 213/1000 | Loss: 0.00001711
Iteration 214/1000 | Loss: 0.00001711
Iteration 215/1000 | Loss: 0.00001711
Iteration 216/1000 | Loss: 0.00001710
Iteration 217/1000 | Loss: 0.00001710
Iteration 218/1000 | Loss: 0.00001710
Iteration 219/1000 | Loss: 0.00001710
Iteration 220/1000 | Loss: 0.00001710
Iteration 221/1000 | Loss: 0.00001710
Iteration 222/1000 | Loss: 0.00001710
Iteration 223/1000 | Loss: 0.00001710
Iteration 224/1000 | Loss: 0.00001710
Iteration 225/1000 | Loss: 0.00001710
Iteration 226/1000 | Loss: 0.00001709
Iteration 227/1000 | Loss: 0.00001709
Iteration 228/1000 | Loss: 0.00001709
Iteration 229/1000 | Loss: 0.00001709
Iteration 230/1000 | Loss: 0.00001709
Iteration 231/1000 | Loss: 0.00001709
Iteration 232/1000 | Loss: 0.00001709
Iteration 233/1000 | Loss: 0.00001709
Iteration 234/1000 | Loss: 0.00001709
Iteration 235/1000 | Loss: 0.00001709
Iteration 236/1000 | Loss: 0.00001709
Iteration 237/1000 | Loss: 0.00001709
Iteration 238/1000 | Loss: 0.00001708
Iteration 239/1000 | Loss: 0.00001708
Iteration 240/1000 | Loss: 0.00001708
Iteration 241/1000 | Loss: 0.00001708
Iteration 242/1000 | Loss: 0.00001708
Iteration 243/1000 | Loss: 0.00001708
Iteration 244/1000 | Loss: 0.00001708
Iteration 245/1000 | Loss: 0.00001708
Iteration 246/1000 | Loss: 0.00001708
Iteration 247/1000 | Loss: 0.00001708
Iteration 248/1000 | Loss: 0.00001708
Iteration 249/1000 | Loss: 0.00001708
Iteration 250/1000 | Loss: 0.00001708
Iteration 251/1000 | Loss: 0.00001708
Iteration 252/1000 | Loss: 0.00001708
Iteration 253/1000 | Loss: 0.00001708
Iteration 254/1000 | Loss: 0.00001708
Iteration 255/1000 | Loss: 0.00001708
Iteration 256/1000 | Loss: 0.00001708
Iteration 257/1000 | Loss: 0.00001708
Iteration 258/1000 | Loss: 0.00001708
Iteration 259/1000 | Loss: 0.00001708
Iteration 260/1000 | Loss: 0.00001708
Iteration 261/1000 | Loss: 0.00001707
Iteration 262/1000 | Loss: 0.00001707
Iteration 263/1000 | Loss: 0.00001707
Iteration 264/1000 | Loss: 0.00001707
Iteration 265/1000 | Loss: 0.00001707
Iteration 266/1000 | Loss: 0.00001707
Iteration 267/1000 | Loss: 0.00001707
Iteration 268/1000 | Loss: 0.00001707
Iteration 269/1000 | Loss: 0.00001707
Iteration 270/1000 | Loss: 0.00001707
Iteration 271/1000 | Loss: 0.00001707
Iteration 272/1000 | Loss: 0.00001707
Iteration 273/1000 | Loss: 0.00001707
Iteration 274/1000 | Loss: 0.00001707
Iteration 275/1000 | Loss: 0.00001707
Iteration 276/1000 | Loss: 0.00001707
Iteration 277/1000 | Loss: 0.00001707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.707356932456605e-05, 1.707356932456605e-05, 1.707356932456605e-05, 1.707356932456605e-05, 1.707356932456605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.707356932456605e-05

Optimization complete. Final v2v error: 3.3678841590881348 mm

Highest mean error: 5.1865105628967285 mm for frame 5

Lowest mean error: 2.9888052940368652 mm for frame 103

Saving results

Total time: 4446.905380010605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1021
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406076
Iteration 2/25 | Loss: 0.00134594
Iteration 3/25 | Loss: 0.00110826
Iteration 4/25 | Loss: 0.00107762
Iteration 5/25 | Loss: 0.00107356
Iteration 6/25 | Loss: 0.00107246
Iteration 7/25 | Loss: 0.00107246
Iteration 8/25 | Loss: 0.00107246
Iteration 9/25 | Loss: 0.00107246
Iteration 10/25 | Loss: 0.00107246
Iteration 11/25 | Loss: 0.00107246
Iteration 12/25 | Loss: 0.00107246
Iteration 13/25 | Loss: 0.00107246
Iteration 14/25 | Loss: 0.00107246
Iteration 15/25 | Loss: 0.00107246
Iteration 16/25 | Loss: 0.00107246
Iteration 17/25 | Loss: 0.00107246
Iteration 18/25 | Loss: 0.00107246
Iteration 19/25 | Loss: 0.00107246
Iteration 20/25 | Loss: 0.00107246
Iteration 21/25 | Loss: 0.00107246
Iteration 22/25 | Loss: 0.00107246
Iteration 23/25 | Loss: 0.00107246
Iteration 24/25 | Loss: 0.00107246
Iteration 25/25 | Loss: 0.00107246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38090718
Iteration 2/25 | Loss: 0.00069852
Iteration 3/25 | Loss: 0.00069852
Iteration 4/25 | Loss: 0.00069852
Iteration 5/25 | Loss: 0.00069852
Iteration 6/25 | Loss: 0.00069852
Iteration 7/25 | Loss: 0.00069852
Iteration 8/25 | Loss: 0.00069852
Iteration 9/25 | Loss: 0.00069852
Iteration 10/25 | Loss: 0.00069852
Iteration 11/25 | Loss: 0.00069852
Iteration 12/25 | Loss: 0.00069852
Iteration 13/25 | Loss: 0.00069852
Iteration 14/25 | Loss: 0.00069852
Iteration 15/25 | Loss: 0.00069852
Iteration 16/25 | Loss: 0.00069852
Iteration 17/25 | Loss: 0.00069852
Iteration 18/25 | Loss: 0.00069852
Iteration 19/25 | Loss: 0.00069852
Iteration 20/25 | Loss: 0.00069852
Iteration 21/25 | Loss: 0.00069852
Iteration 22/25 | Loss: 0.00069852
Iteration 23/25 | Loss: 0.00069852
Iteration 24/25 | Loss: 0.00069852
Iteration 25/25 | Loss: 0.00069852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069852
Iteration 2/1000 | Loss: 0.00002703
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001371
Iteration 5/1000 | Loss: 0.00001251
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001133
Iteration 8/1000 | Loss: 0.00001096
Iteration 9/1000 | Loss: 0.00001079
Iteration 10/1000 | Loss: 0.00001073
Iteration 11/1000 | Loss: 0.00001052
Iteration 12/1000 | Loss: 0.00001041
Iteration 13/1000 | Loss: 0.00001034
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001024
Iteration 16/1000 | Loss: 0.00001017
Iteration 17/1000 | Loss: 0.00001014
Iteration 18/1000 | Loss: 0.00001013
Iteration 19/1000 | Loss: 0.00001013
Iteration 20/1000 | Loss: 0.00001013
Iteration 21/1000 | Loss: 0.00001012
Iteration 22/1000 | Loss: 0.00001012
Iteration 23/1000 | Loss: 0.00001011
Iteration 24/1000 | Loss: 0.00001010
Iteration 25/1000 | Loss: 0.00001009
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00001008
Iteration 28/1000 | Loss: 0.00001008
Iteration 29/1000 | Loss: 0.00001008
Iteration 30/1000 | Loss: 0.00001008
Iteration 31/1000 | Loss: 0.00001007
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001006
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001006
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001005
Iteration 39/1000 | Loss: 0.00001004
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001004
Iteration 42/1000 | Loss: 0.00001003
Iteration 43/1000 | Loss: 0.00001003
Iteration 44/1000 | Loss: 0.00001003
Iteration 45/1000 | Loss: 0.00001003
Iteration 46/1000 | Loss: 0.00001003
Iteration 47/1000 | Loss: 0.00001003
Iteration 48/1000 | Loss: 0.00001003
Iteration 49/1000 | Loss: 0.00001002
Iteration 50/1000 | Loss: 0.00001002
Iteration 51/1000 | Loss: 0.00001002
Iteration 52/1000 | Loss: 0.00001002
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001002
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001002
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00001000
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00001000
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000999
Iteration 80/1000 | Loss: 0.00000999
Iteration 81/1000 | Loss: 0.00000999
Iteration 82/1000 | Loss: 0.00000998
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000998
Iteration 90/1000 | Loss: 0.00000997
Iteration 91/1000 | Loss: 0.00000997
Iteration 92/1000 | Loss: 0.00000997
Iteration 93/1000 | Loss: 0.00000997
Iteration 94/1000 | Loss: 0.00000997
Iteration 95/1000 | Loss: 0.00000997
Iteration 96/1000 | Loss: 0.00000997
Iteration 97/1000 | Loss: 0.00000997
Iteration 98/1000 | Loss: 0.00000997
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000996
Iteration 102/1000 | Loss: 0.00000996
Iteration 103/1000 | Loss: 0.00000996
Iteration 104/1000 | Loss: 0.00000996
Iteration 105/1000 | Loss: 0.00000996
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000995
Iteration 108/1000 | Loss: 0.00000995
Iteration 109/1000 | Loss: 0.00000995
Iteration 110/1000 | Loss: 0.00000995
Iteration 111/1000 | Loss: 0.00000995
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000994
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000994
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000992
Iteration 123/1000 | Loss: 0.00000992
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000992
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000992
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000992
Iteration 131/1000 | Loss: 0.00000991
Iteration 132/1000 | Loss: 0.00000991
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000990
Iteration 137/1000 | Loss: 0.00000990
Iteration 138/1000 | Loss: 0.00000990
Iteration 139/1000 | Loss: 0.00000990
Iteration 140/1000 | Loss: 0.00000990
Iteration 141/1000 | Loss: 0.00000990
Iteration 142/1000 | Loss: 0.00000990
Iteration 143/1000 | Loss: 0.00000989
Iteration 144/1000 | Loss: 0.00000989
Iteration 145/1000 | Loss: 0.00000989
Iteration 146/1000 | Loss: 0.00000989
Iteration 147/1000 | Loss: 0.00000989
Iteration 148/1000 | Loss: 0.00000989
Iteration 149/1000 | Loss: 0.00000989
Iteration 150/1000 | Loss: 0.00000989
Iteration 151/1000 | Loss: 0.00000989
Iteration 152/1000 | Loss: 0.00000988
Iteration 153/1000 | Loss: 0.00000988
Iteration 154/1000 | Loss: 0.00000988
Iteration 155/1000 | Loss: 0.00000988
Iteration 156/1000 | Loss: 0.00000987
Iteration 157/1000 | Loss: 0.00000987
Iteration 158/1000 | Loss: 0.00000987
Iteration 159/1000 | Loss: 0.00000987
Iteration 160/1000 | Loss: 0.00000987
Iteration 161/1000 | Loss: 0.00000986
Iteration 162/1000 | Loss: 0.00000986
Iteration 163/1000 | Loss: 0.00000986
Iteration 164/1000 | Loss: 0.00000986
Iteration 165/1000 | Loss: 0.00000986
Iteration 166/1000 | Loss: 0.00000986
Iteration 167/1000 | Loss: 0.00000986
Iteration 168/1000 | Loss: 0.00000985
Iteration 169/1000 | Loss: 0.00000985
Iteration 170/1000 | Loss: 0.00000985
Iteration 171/1000 | Loss: 0.00000985
Iteration 172/1000 | Loss: 0.00000985
Iteration 173/1000 | Loss: 0.00000985
Iteration 174/1000 | Loss: 0.00000985
Iteration 175/1000 | Loss: 0.00000985
Iteration 176/1000 | Loss: 0.00000985
Iteration 177/1000 | Loss: 0.00000985
Iteration 178/1000 | Loss: 0.00000985
Iteration 179/1000 | Loss: 0.00000984
Iteration 180/1000 | Loss: 0.00000984
Iteration 181/1000 | Loss: 0.00000984
Iteration 182/1000 | Loss: 0.00000984
Iteration 183/1000 | Loss: 0.00000984
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000983
Iteration 186/1000 | Loss: 0.00000983
Iteration 187/1000 | Loss: 0.00000983
Iteration 188/1000 | Loss: 0.00000983
Iteration 189/1000 | Loss: 0.00000983
Iteration 190/1000 | Loss: 0.00000983
Iteration 191/1000 | Loss: 0.00000983
Iteration 192/1000 | Loss: 0.00000983
Iteration 193/1000 | Loss: 0.00000982
Iteration 194/1000 | Loss: 0.00000982
Iteration 195/1000 | Loss: 0.00000982
Iteration 196/1000 | Loss: 0.00000982
Iteration 197/1000 | Loss: 0.00000982
Iteration 198/1000 | Loss: 0.00000982
Iteration 199/1000 | Loss: 0.00000982
Iteration 200/1000 | Loss: 0.00000982
Iteration 201/1000 | Loss: 0.00000982
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Iteration 204/1000 | Loss: 0.00000982
Iteration 205/1000 | Loss: 0.00000982
Iteration 206/1000 | Loss: 0.00000982
Iteration 207/1000 | Loss: 0.00000982
Iteration 208/1000 | Loss: 0.00000982
Iteration 209/1000 | Loss: 0.00000982
Iteration 210/1000 | Loss: 0.00000982
Iteration 211/1000 | Loss: 0.00000982
Iteration 212/1000 | Loss: 0.00000982
Iteration 213/1000 | Loss: 0.00000982
Iteration 214/1000 | Loss: 0.00000982
Iteration 215/1000 | Loss: 0.00000982
Iteration 216/1000 | Loss: 0.00000982
Iteration 217/1000 | Loss: 0.00000982
Iteration 218/1000 | Loss: 0.00000982
Iteration 219/1000 | Loss: 0.00000982
Iteration 220/1000 | Loss: 0.00000982
Iteration 221/1000 | Loss: 0.00000982
Iteration 222/1000 | Loss: 0.00000982
Iteration 223/1000 | Loss: 0.00000982
Iteration 224/1000 | Loss: 0.00000982
Iteration 225/1000 | Loss: 0.00000982
Iteration 226/1000 | Loss: 0.00000982
Iteration 227/1000 | Loss: 0.00000982
Iteration 228/1000 | Loss: 0.00000982
Iteration 229/1000 | Loss: 0.00000982
Iteration 230/1000 | Loss: 0.00000982
Iteration 231/1000 | Loss: 0.00000982
Iteration 232/1000 | Loss: 0.00000982
Iteration 233/1000 | Loss: 0.00000982
Iteration 234/1000 | Loss: 0.00000982
Iteration 235/1000 | Loss: 0.00000982
Iteration 236/1000 | Loss: 0.00000982
Iteration 237/1000 | Loss: 0.00000982
Iteration 238/1000 | Loss: 0.00000982
Iteration 239/1000 | Loss: 0.00000982
Iteration 240/1000 | Loss: 0.00000982
Iteration 241/1000 | Loss: 0.00000982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [9.823149412113708e-06, 9.823149412113708e-06, 9.823149412113708e-06, 9.823149412113708e-06, 9.823149412113708e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.823149412113708e-06

Optimization complete. Final v2v error: 2.694563865661621 mm

Highest mean error: 3.3685925006866455 mm for frame 75

Lowest mean error: 2.473458766937256 mm for frame 166

Saving results

Total time: 1007.7314245700836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1041
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776820
Iteration 2/25 | Loss: 0.00124563
Iteration 3/25 | Loss: 0.00108369
Iteration 4/25 | Loss: 0.00106628
Iteration 5/25 | Loss: 0.00106362
Iteration 6/25 | Loss: 0.00106362
Iteration 7/25 | Loss: 0.00106362
Iteration 8/25 | Loss: 0.00106362
Iteration 9/25 | Loss: 0.00106362
Iteration 10/25 | Loss: 0.00106362
Iteration 11/25 | Loss: 0.00106362
Iteration 12/25 | Loss: 0.00106362
Iteration 13/25 | Loss: 0.00106362
Iteration 14/25 | Loss: 0.00106362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010636183433234692, 0.0010636183433234692, 0.0010636183433234692, 0.0010636183433234692, 0.0010636183433234692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010636183433234692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37550974
Iteration 2/25 | Loss: 0.00070693
Iteration 3/25 | Loss: 0.00070693
Iteration 4/25 | Loss: 0.00070693
Iteration 5/25 | Loss: 0.00070693
Iteration 6/25 | Loss: 0.00070692
Iteration 7/25 | Loss: 0.00070692
Iteration 8/25 | Loss: 0.00070692
Iteration 9/25 | Loss: 0.00070692
Iteration 10/25 | Loss: 0.00070692
Iteration 11/25 | Loss: 0.00070692
Iteration 12/25 | Loss: 0.00070692
Iteration 13/25 | Loss: 0.00070692
Iteration 14/25 | Loss: 0.00070692
Iteration 15/25 | Loss: 0.00070692
Iteration 16/25 | Loss: 0.00070692
Iteration 17/25 | Loss: 0.00070692
Iteration 18/25 | Loss: 0.00070692
Iteration 19/25 | Loss: 0.00070692
Iteration 20/25 | Loss: 0.00070692
Iteration 21/25 | Loss: 0.00070692
Iteration 22/25 | Loss: 0.00070692
Iteration 23/25 | Loss: 0.00070692
Iteration 24/25 | Loss: 0.00070692
Iteration 25/25 | Loss: 0.00070692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070692
Iteration 2/1000 | Loss: 0.00002147
Iteration 3/1000 | Loss: 0.00001454
Iteration 4/1000 | Loss: 0.00001226
Iteration 5/1000 | Loss: 0.00001131
Iteration 6/1000 | Loss: 0.00001073
Iteration 7/1000 | Loss: 0.00001023
Iteration 8/1000 | Loss: 0.00000999
Iteration 9/1000 | Loss: 0.00000979
Iteration 10/1000 | Loss: 0.00000960
Iteration 11/1000 | Loss: 0.00000940
Iteration 12/1000 | Loss: 0.00000936
Iteration 13/1000 | Loss: 0.00000936
Iteration 14/1000 | Loss: 0.00000934
Iteration 15/1000 | Loss: 0.00000934
Iteration 16/1000 | Loss: 0.00000934
Iteration 17/1000 | Loss: 0.00000933
Iteration 18/1000 | Loss: 0.00000932
Iteration 19/1000 | Loss: 0.00000931
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000930
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000926
Iteration 24/1000 | Loss: 0.00000924
Iteration 25/1000 | Loss: 0.00000921
Iteration 26/1000 | Loss: 0.00000919
Iteration 27/1000 | Loss: 0.00000919
Iteration 28/1000 | Loss: 0.00000919
Iteration 29/1000 | Loss: 0.00000918
Iteration 30/1000 | Loss: 0.00000918
Iteration 31/1000 | Loss: 0.00000917
Iteration 32/1000 | Loss: 0.00000917
Iteration 33/1000 | Loss: 0.00000917
Iteration 34/1000 | Loss: 0.00000917
Iteration 35/1000 | Loss: 0.00000917
Iteration 36/1000 | Loss: 0.00000917
Iteration 37/1000 | Loss: 0.00000916
Iteration 38/1000 | Loss: 0.00000916
Iteration 39/1000 | Loss: 0.00000916
Iteration 40/1000 | Loss: 0.00000915
Iteration 41/1000 | Loss: 0.00000915
Iteration 42/1000 | Loss: 0.00000915
Iteration 43/1000 | Loss: 0.00000914
Iteration 44/1000 | Loss: 0.00000914
Iteration 45/1000 | Loss: 0.00000913
Iteration 46/1000 | Loss: 0.00000913
Iteration 47/1000 | Loss: 0.00000912
Iteration 48/1000 | Loss: 0.00000912
Iteration 49/1000 | Loss: 0.00000912
Iteration 50/1000 | Loss: 0.00000912
Iteration 51/1000 | Loss: 0.00000911
Iteration 52/1000 | Loss: 0.00000911
Iteration 53/1000 | Loss: 0.00000911
Iteration 54/1000 | Loss: 0.00000911
Iteration 55/1000 | Loss: 0.00000910
Iteration 56/1000 | Loss: 0.00000910
Iteration 57/1000 | Loss: 0.00000910
Iteration 58/1000 | Loss: 0.00000909
Iteration 59/1000 | Loss: 0.00000909
Iteration 60/1000 | Loss: 0.00000908
Iteration 61/1000 | Loss: 0.00000908
Iteration 62/1000 | Loss: 0.00000908
Iteration 63/1000 | Loss: 0.00000907
Iteration 64/1000 | Loss: 0.00000907
Iteration 65/1000 | Loss: 0.00000907
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000906
Iteration 68/1000 | Loss: 0.00000906
Iteration 69/1000 | Loss: 0.00000906
Iteration 70/1000 | Loss: 0.00000906
Iteration 71/1000 | Loss: 0.00000906
Iteration 72/1000 | Loss: 0.00000905
Iteration 73/1000 | Loss: 0.00000905
Iteration 74/1000 | Loss: 0.00000903
Iteration 75/1000 | Loss: 0.00000903
Iteration 76/1000 | Loss: 0.00000903
Iteration 77/1000 | Loss: 0.00000903
Iteration 78/1000 | Loss: 0.00000903
Iteration 79/1000 | Loss: 0.00000903
Iteration 80/1000 | Loss: 0.00000903
Iteration 81/1000 | Loss: 0.00000903
Iteration 82/1000 | Loss: 0.00000903
Iteration 83/1000 | Loss: 0.00000903
Iteration 84/1000 | Loss: 0.00000902
Iteration 85/1000 | Loss: 0.00000902
Iteration 86/1000 | Loss: 0.00000902
Iteration 87/1000 | Loss: 0.00000901
Iteration 88/1000 | Loss: 0.00000901
Iteration 89/1000 | Loss: 0.00000900
Iteration 90/1000 | Loss: 0.00000900
Iteration 91/1000 | Loss: 0.00000900
Iteration 92/1000 | Loss: 0.00000900
Iteration 93/1000 | Loss: 0.00000900
Iteration 94/1000 | Loss: 0.00000899
Iteration 95/1000 | Loss: 0.00000899
Iteration 96/1000 | Loss: 0.00000899
Iteration 97/1000 | Loss: 0.00000899
Iteration 98/1000 | Loss: 0.00000899
Iteration 99/1000 | Loss: 0.00000899
Iteration 100/1000 | Loss: 0.00000899
Iteration 101/1000 | Loss: 0.00000899
Iteration 102/1000 | Loss: 0.00000898
Iteration 103/1000 | Loss: 0.00000898
Iteration 104/1000 | Loss: 0.00000898
Iteration 105/1000 | Loss: 0.00000898
Iteration 106/1000 | Loss: 0.00000897
Iteration 107/1000 | Loss: 0.00000897
Iteration 108/1000 | Loss: 0.00000897
Iteration 109/1000 | Loss: 0.00000897
Iteration 110/1000 | Loss: 0.00000897
Iteration 111/1000 | Loss: 0.00000896
Iteration 112/1000 | Loss: 0.00000896
Iteration 113/1000 | Loss: 0.00000896
Iteration 114/1000 | Loss: 0.00000896
Iteration 115/1000 | Loss: 0.00000896
Iteration 116/1000 | Loss: 0.00000895
Iteration 117/1000 | Loss: 0.00000895
Iteration 118/1000 | Loss: 0.00000895
Iteration 119/1000 | Loss: 0.00000895
Iteration 120/1000 | Loss: 0.00000895
Iteration 121/1000 | Loss: 0.00000895
Iteration 122/1000 | Loss: 0.00000895
Iteration 123/1000 | Loss: 0.00000895
Iteration 124/1000 | Loss: 0.00000895
Iteration 125/1000 | Loss: 0.00000895
Iteration 126/1000 | Loss: 0.00000894
Iteration 127/1000 | Loss: 0.00000894
Iteration 128/1000 | Loss: 0.00000894
Iteration 129/1000 | Loss: 0.00000894
Iteration 130/1000 | Loss: 0.00000894
Iteration 131/1000 | Loss: 0.00000894
Iteration 132/1000 | Loss: 0.00000894
Iteration 133/1000 | Loss: 0.00000894
Iteration 134/1000 | Loss: 0.00000894
Iteration 135/1000 | Loss: 0.00000894
Iteration 136/1000 | Loss: 0.00000894
Iteration 137/1000 | Loss: 0.00000893
Iteration 138/1000 | Loss: 0.00000893
Iteration 139/1000 | Loss: 0.00000893
Iteration 140/1000 | Loss: 0.00000892
Iteration 141/1000 | Loss: 0.00000892
Iteration 142/1000 | Loss: 0.00000892
Iteration 143/1000 | Loss: 0.00000892
Iteration 144/1000 | Loss: 0.00000892
Iteration 145/1000 | Loss: 0.00000892
Iteration 146/1000 | Loss: 0.00000891
Iteration 147/1000 | Loss: 0.00000891
Iteration 148/1000 | Loss: 0.00000891
Iteration 149/1000 | Loss: 0.00000891
Iteration 150/1000 | Loss: 0.00000891
Iteration 151/1000 | Loss: 0.00000890
Iteration 152/1000 | Loss: 0.00000890
Iteration 153/1000 | Loss: 0.00000890
Iteration 154/1000 | Loss: 0.00000890
Iteration 155/1000 | Loss: 0.00000890
Iteration 156/1000 | Loss: 0.00000890
Iteration 157/1000 | Loss: 0.00000889
Iteration 158/1000 | Loss: 0.00000889
Iteration 159/1000 | Loss: 0.00000889
Iteration 160/1000 | Loss: 0.00000889
Iteration 161/1000 | Loss: 0.00000889
Iteration 162/1000 | Loss: 0.00000889
Iteration 163/1000 | Loss: 0.00000889
Iteration 164/1000 | Loss: 0.00000889
Iteration 165/1000 | Loss: 0.00000889
Iteration 166/1000 | Loss: 0.00000888
Iteration 167/1000 | Loss: 0.00000888
Iteration 168/1000 | Loss: 0.00000888
Iteration 169/1000 | Loss: 0.00000888
Iteration 170/1000 | Loss: 0.00000888
Iteration 171/1000 | Loss: 0.00000887
Iteration 172/1000 | Loss: 0.00000887
Iteration 173/1000 | Loss: 0.00000887
Iteration 174/1000 | Loss: 0.00000887
Iteration 175/1000 | Loss: 0.00000887
Iteration 176/1000 | Loss: 0.00000886
Iteration 177/1000 | Loss: 0.00000886
Iteration 178/1000 | Loss: 0.00000886
Iteration 179/1000 | Loss: 0.00000885
Iteration 180/1000 | Loss: 0.00000885
Iteration 181/1000 | Loss: 0.00000885
Iteration 182/1000 | Loss: 0.00000885
Iteration 183/1000 | Loss: 0.00000885
Iteration 184/1000 | Loss: 0.00000885
Iteration 185/1000 | Loss: 0.00000885
Iteration 186/1000 | Loss: 0.00000884
Iteration 187/1000 | Loss: 0.00000884
Iteration 188/1000 | Loss: 0.00000884
Iteration 189/1000 | Loss: 0.00000884
Iteration 190/1000 | Loss: 0.00000884
Iteration 191/1000 | Loss: 0.00000884
Iteration 192/1000 | Loss: 0.00000884
Iteration 193/1000 | Loss: 0.00000884
Iteration 194/1000 | Loss: 0.00000884
Iteration 195/1000 | Loss: 0.00000884
Iteration 196/1000 | Loss: 0.00000884
Iteration 197/1000 | Loss: 0.00000884
Iteration 198/1000 | Loss: 0.00000884
Iteration 199/1000 | Loss: 0.00000884
Iteration 200/1000 | Loss: 0.00000884
Iteration 201/1000 | Loss: 0.00000884
Iteration 202/1000 | Loss: 0.00000883
Iteration 203/1000 | Loss: 0.00000883
Iteration 204/1000 | Loss: 0.00000883
Iteration 205/1000 | Loss: 0.00000883
Iteration 206/1000 | Loss: 0.00000883
Iteration 207/1000 | Loss: 0.00000883
Iteration 208/1000 | Loss: 0.00000883
Iteration 209/1000 | Loss: 0.00000883
Iteration 210/1000 | Loss: 0.00000883
Iteration 211/1000 | Loss: 0.00000883
Iteration 212/1000 | Loss: 0.00000883
Iteration 213/1000 | Loss: 0.00000883
Iteration 214/1000 | Loss: 0.00000883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [8.832470484776422e-06, 8.832470484776422e-06, 8.832470484776422e-06, 8.832470484776422e-06, 8.832470484776422e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.832470484776422e-06

Optimization complete. Final v2v error: 2.5372676849365234 mm

Highest mean error: 2.9257864952087402 mm for frame 122

Lowest mean error: 2.3303470611572266 mm for frame 174

Saving results

Total time: 1300.492312669754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1019
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826969
Iteration 2/25 | Loss: 0.00198242
Iteration 3/25 | Loss: 0.00144503
Iteration 4/25 | Loss: 0.00142651
Iteration 5/25 | Loss: 0.00142242
Iteration 6/25 | Loss: 0.00142092
Iteration 7/25 | Loss: 0.00142092
Iteration 8/25 | Loss: 0.00142092
Iteration 9/25 | Loss: 0.00142092
Iteration 10/25 | Loss: 0.00142092
Iteration 11/25 | Loss: 0.00142092
Iteration 12/25 | Loss: 0.00142092
Iteration 13/25 | Loss: 0.00142092
Iteration 14/25 | Loss: 0.00142092
Iteration 15/25 | Loss: 0.00142092
Iteration 16/25 | Loss: 0.00142092
Iteration 17/25 | Loss: 0.00142092
Iteration 18/25 | Loss: 0.00142092
Iteration 19/25 | Loss: 0.00142092
Iteration 20/25 | Loss: 0.00142092
Iteration 21/25 | Loss: 0.00142092
Iteration 22/25 | Loss: 0.00142092
Iteration 23/25 | Loss: 0.00142092
Iteration 24/25 | Loss: 0.00142092
Iteration 25/25 | Loss: 0.00142092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59114903
Iteration 2/25 | Loss: 0.00095664
Iteration 3/25 | Loss: 0.00095664
Iteration 4/25 | Loss: 0.00095664
Iteration 5/25 | Loss: 0.00095664
Iteration 6/25 | Loss: 0.00095664
Iteration 7/25 | Loss: 0.00095664
Iteration 8/25 | Loss: 0.00095664
Iteration 9/25 | Loss: 0.00095664
Iteration 10/25 | Loss: 0.00095664
Iteration 11/25 | Loss: 0.00095664
Iteration 12/25 | Loss: 0.00095664
Iteration 13/25 | Loss: 0.00095664
Iteration 14/25 | Loss: 0.00095664
Iteration 15/25 | Loss: 0.00095664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009566393564455211, 0.0009566393564455211, 0.0009566393564455211, 0.0009566393564455211, 0.0009566393564455211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009566393564455211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095664
Iteration 2/1000 | Loss: 0.00010128
Iteration 3/1000 | Loss: 0.00006136
Iteration 4/1000 | Loss: 0.00005183
Iteration 5/1000 | Loss: 0.00004793
Iteration 6/1000 | Loss: 0.00004598
Iteration 7/1000 | Loss: 0.00004510
Iteration 8/1000 | Loss: 0.00004434
Iteration 9/1000 | Loss: 0.00004344
Iteration 10/1000 | Loss: 0.00004278
Iteration 11/1000 | Loss: 0.00004212
Iteration 12/1000 | Loss: 0.00004152
Iteration 13/1000 | Loss: 0.00004107
Iteration 14/1000 | Loss: 0.00004057
Iteration 15/1000 | Loss: 0.00004012
Iteration 16/1000 | Loss: 0.00003977
Iteration 17/1000 | Loss: 0.00003952
Iteration 18/1000 | Loss: 0.00003933
Iteration 19/1000 | Loss: 0.00003910
Iteration 20/1000 | Loss: 0.00003901
Iteration 21/1000 | Loss: 0.00003894
Iteration 22/1000 | Loss: 0.00003893
Iteration 23/1000 | Loss: 0.00003891
Iteration 24/1000 | Loss: 0.00003891
Iteration 25/1000 | Loss: 0.00003891
Iteration 26/1000 | Loss: 0.00003891
Iteration 27/1000 | Loss: 0.00003891
Iteration 28/1000 | Loss: 0.00003891
Iteration 29/1000 | Loss: 0.00003889
Iteration 30/1000 | Loss: 0.00003883
Iteration 31/1000 | Loss: 0.00003871
Iteration 32/1000 | Loss: 0.00003859
Iteration 33/1000 | Loss: 0.00003853
Iteration 34/1000 | Loss: 0.00003852
Iteration 35/1000 | Loss: 0.00003852
Iteration 36/1000 | Loss: 0.00003851
Iteration 37/1000 | Loss: 0.00003850
Iteration 38/1000 | Loss: 0.00003850
Iteration 39/1000 | Loss: 0.00003849
Iteration 40/1000 | Loss: 0.00003849
Iteration 41/1000 | Loss: 0.00003849
Iteration 42/1000 | Loss: 0.00003848
Iteration 43/1000 | Loss: 0.00003848
Iteration 44/1000 | Loss: 0.00003848
Iteration 45/1000 | Loss: 0.00003848
Iteration 46/1000 | Loss: 0.00003848
Iteration 47/1000 | Loss: 0.00003848
Iteration 48/1000 | Loss: 0.00003847
Iteration 49/1000 | Loss: 0.00003847
Iteration 50/1000 | Loss: 0.00003847
Iteration 51/1000 | Loss: 0.00003846
Iteration 52/1000 | Loss: 0.00003846
Iteration 53/1000 | Loss: 0.00003846
Iteration 54/1000 | Loss: 0.00003846
Iteration 55/1000 | Loss: 0.00003846
Iteration 56/1000 | Loss: 0.00003846
Iteration 57/1000 | Loss: 0.00003845
Iteration 58/1000 | Loss: 0.00003845
Iteration 59/1000 | Loss: 0.00003845
Iteration 60/1000 | Loss: 0.00003845
Iteration 61/1000 | Loss: 0.00003845
Iteration 62/1000 | Loss: 0.00003845
Iteration 63/1000 | Loss: 0.00003845
Iteration 64/1000 | Loss: 0.00003845
Iteration 65/1000 | Loss: 0.00003844
Iteration 66/1000 | Loss: 0.00003844
Iteration 67/1000 | Loss: 0.00003843
Iteration 68/1000 | Loss: 0.00003843
Iteration 69/1000 | Loss: 0.00003843
Iteration 70/1000 | Loss: 0.00003843
Iteration 71/1000 | Loss: 0.00003842
Iteration 72/1000 | Loss: 0.00003842
Iteration 73/1000 | Loss: 0.00003842
Iteration 74/1000 | Loss: 0.00003842
Iteration 75/1000 | Loss: 0.00003842
Iteration 76/1000 | Loss: 0.00003842
Iteration 77/1000 | Loss: 0.00003842
Iteration 78/1000 | Loss: 0.00003842
Iteration 79/1000 | Loss: 0.00003841
Iteration 80/1000 | Loss: 0.00003841
Iteration 81/1000 | Loss: 0.00003841
Iteration 82/1000 | Loss: 0.00003841
Iteration 83/1000 | Loss: 0.00003841
Iteration 84/1000 | Loss: 0.00003841
Iteration 85/1000 | Loss: 0.00003841
Iteration 86/1000 | Loss: 0.00003841
Iteration 87/1000 | Loss: 0.00003840
Iteration 88/1000 | Loss: 0.00003840
Iteration 89/1000 | Loss: 0.00003840
Iteration 90/1000 | Loss: 0.00003840
Iteration 91/1000 | Loss: 0.00003840
Iteration 92/1000 | Loss: 0.00003840
Iteration 93/1000 | Loss: 0.00003840
Iteration 94/1000 | Loss: 0.00003840
Iteration 95/1000 | Loss: 0.00003840
Iteration 96/1000 | Loss: 0.00003840
Iteration 97/1000 | Loss: 0.00003839
Iteration 98/1000 | Loss: 0.00003839
Iteration 99/1000 | Loss: 0.00003839
Iteration 100/1000 | Loss: 0.00003839
Iteration 101/1000 | Loss: 0.00003839
Iteration 102/1000 | Loss: 0.00003838
Iteration 103/1000 | Loss: 0.00003838
Iteration 104/1000 | Loss: 0.00003838
Iteration 105/1000 | Loss: 0.00003838
Iteration 106/1000 | Loss: 0.00003838
Iteration 107/1000 | Loss: 0.00003837
Iteration 108/1000 | Loss: 0.00003837
Iteration 109/1000 | Loss: 0.00003837
Iteration 110/1000 | Loss: 0.00003837
Iteration 111/1000 | Loss: 0.00003837
Iteration 112/1000 | Loss: 0.00003837
Iteration 113/1000 | Loss: 0.00003837
Iteration 114/1000 | Loss: 0.00003837
Iteration 115/1000 | Loss: 0.00003836
Iteration 116/1000 | Loss: 0.00003836
Iteration 117/1000 | Loss: 0.00003836
Iteration 118/1000 | Loss: 0.00003836
Iteration 119/1000 | Loss: 0.00003836
Iteration 120/1000 | Loss: 0.00003836
Iteration 121/1000 | Loss: 0.00003835
Iteration 122/1000 | Loss: 0.00003835
Iteration 123/1000 | Loss: 0.00003835
Iteration 124/1000 | Loss: 0.00003835
Iteration 125/1000 | Loss: 0.00003835
Iteration 126/1000 | Loss: 0.00003835
Iteration 127/1000 | Loss: 0.00003834
Iteration 128/1000 | Loss: 0.00003834
Iteration 129/1000 | Loss: 0.00003834
Iteration 130/1000 | Loss: 0.00003834
Iteration 131/1000 | Loss: 0.00003834
Iteration 132/1000 | Loss: 0.00003833
Iteration 133/1000 | Loss: 0.00003833
Iteration 134/1000 | Loss: 0.00003833
Iteration 135/1000 | Loss: 0.00003833
Iteration 136/1000 | Loss: 0.00003833
Iteration 137/1000 | Loss: 0.00003833
Iteration 138/1000 | Loss: 0.00003833
Iteration 139/1000 | Loss: 0.00003833
Iteration 140/1000 | Loss: 0.00003833
Iteration 141/1000 | Loss: 0.00003833
Iteration 142/1000 | Loss: 0.00003832
Iteration 143/1000 | Loss: 0.00003832
Iteration 144/1000 | Loss: 0.00003832
Iteration 145/1000 | Loss: 0.00003832
Iteration 146/1000 | Loss: 0.00003832
Iteration 147/1000 | Loss: 0.00003832
Iteration 148/1000 | Loss: 0.00003832
Iteration 149/1000 | Loss: 0.00003832
Iteration 150/1000 | Loss: 0.00003832
Iteration 151/1000 | Loss: 0.00003831
Iteration 152/1000 | Loss: 0.00003831
Iteration 153/1000 | Loss: 0.00003831
Iteration 154/1000 | Loss: 0.00003831
Iteration 155/1000 | Loss: 0.00003831
Iteration 156/1000 | Loss: 0.00003831
Iteration 157/1000 | Loss: 0.00003831
Iteration 158/1000 | Loss: 0.00003831
Iteration 159/1000 | Loss: 0.00003831
Iteration 160/1000 | Loss: 0.00003831
Iteration 161/1000 | Loss: 0.00003831
Iteration 162/1000 | Loss: 0.00003831
Iteration 163/1000 | Loss: 0.00003831
Iteration 164/1000 | Loss: 0.00003831
Iteration 165/1000 | Loss: 0.00003831
Iteration 166/1000 | Loss: 0.00003831
Iteration 167/1000 | Loss: 0.00003830
Iteration 168/1000 | Loss: 0.00003830
Iteration 169/1000 | Loss: 0.00003830
Iteration 170/1000 | Loss: 0.00003830
Iteration 171/1000 | Loss: 0.00003830
Iteration 172/1000 | Loss: 0.00003830
Iteration 173/1000 | Loss: 0.00003830
Iteration 174/1000 | Loss: 0.00003830
Iteration 175/1000 | Loss: 0.00003830
Iteration 176/1000 | Loss: 0.00003830
Iteration 177/1000 | Loss: 0.00003830
Iteration 178/1000 | Loss: 0.00003829
Iteration 179/1000 | Loss: 0.00003829
Iteration 180/1000 | Loss: 0.00003829
Iteration 181/1000 | Loss: 0.00003829
Iteration 182/1000 | Loss: 0.00003829
Iteration 183/1000 | Loss: 0.00003829
Iteration 184/1000 | Loss: 0.00003829
Iteration 185/1000 | Loss: 0.00003829
Iteration 186/1000 | Loss: 0.00003829
Iteration 187/1000 | Loss: 0.00003829
Iteration 188/1000 | Loss: 0.00003829
Iteration 189/1000 | Loss: 0.00003829
Iteration 190/1000 | Loss: 0.00003829
Iteration 191/1000 | Loss: 0.00003829
Iteration 192/1000 | Loss: 0.00003829
Iteration 193/1000 | Loss: 0.00003829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [3.828782791970298e-05, 3.828782791970298e-05, 3.828782791970298e-05, 3.828782791970298e-05, 3.828782791970298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.828782791970298e-05

Optimization complete. Final v2v error: 5.0344390869140625 mm

Highest mean error: 6.174685478210449 mm for frame 164

Lowest mean error: 4.041191577911377 mm for frame 10

Saving results

Total time: 1130.6939408779144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1022
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581092
Iteration 2/25 | Loss: 0.00143746
Iteration 3/25 | Loss: 0.00115058
Iteration 4/25 | Loss: 0.00111743
Iteration 5/25 | Loss: 0.00111386
Iteration 6/25 | Loss: 0.00111299
Iteration 7/25 | Loss: 0.00111299
Iteration 8/25 | Loss: 0.00111299
Iteration 9/25 | Loss: 0.00111299
Iteration 10/25 | Loss: 0.00111299
Iteration 11/25 | Loss: 0.00111299
Iteration 12/25 | Loss: 0.00111299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001112986239604652, 0.001112986239604652, 0.001112986239604652, 0.001112986239604652, 0.001112986239604652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001112986239604652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33812106
Iteration 2/25 | Loss: 0.00052614
Iteration 3/25 | Loss: 0.00052610
Iteration 4/25 | Loss: 0.00052610
Iteration 5/25 | Loss: 0.00052610
Iteration 6/25 | Loss: 0.00052610
Iteration 7/25 | Loss: 0.00052609
Iteration 8/25 | Loss: 0.00052609
Iteration 9/25 | Loss: 0.00052609
Iteration 10/25 | Loss: 0.00052609
Iteration 11/25 | Loss: 0.00052609
Iteration 12/25 | Loss: 0.00052609
Iteration 13/25 | Loss: 0.00052609
Iteration 14/25 | Loss: 0.00052609
Iteration 15/25 | Loss: 0.00052609
Iteration 16/25 | Loss: 0.00052609
Iteration 17/25 | Loss: 0.00052609
Iteration 18/25 | Loss: 0.00052609
Iteration 19/25 | Loss: 0.00052609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005260937614366412, 0.0005260937614366412, 0.0005260937614366412, 0.0005260937614366412, 0.0005260937614366412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005260937614366412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052609
Iteration 2/1000 | Loss: 0.00003535
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002089
Iteration 5/1000 | Loss: 0.00001906
Iteration 6/1000 | Loss: 0.00001787
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001532
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001495
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001490
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001488
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00001485
Iteration 41/1000 | Loss: 0.00001485
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001474
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001473
Iteration 57/1000 | Loss: 0.00001473
Iteration 58/1000 | Loss: 0.00001473
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001472
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001472
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001472
Iteration 66/1000 | Loss: 0.00001472
Iteration 67/1000 | Loss: 0.00001472
Iteration 68/1000 | Loss: 0.00001471
Iteration 69/1000 | Loss: 0.00001471
Iteration 70/1000 | Loss: 0.00001471
Iteration 71/1000 | Loss: 0.00001471
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001470
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001468
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001465
Iteration 96/1000 | Loss: 0.00001465
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001464
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001464
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001463
Iteration 108/1000 | Loss: 0.00001463
Iteration 109/1000 | Loss: 0.00001463
Iteration 110/1000 | Loss: 0.00001463
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001463
Iteration 114/1000 | Loss: 0.00001463
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001462
Iteration 117/1000 | Loss: 0.00001462
Iteration 118/1000 | Loss: 0.00001462
Iteration 119/1000 | Loss: 0.00001462
Iteration 120/1000 | Loss: 0.00001462
Iteration 121/1000 | Loss: 0.00001461
Iteration 122/1000 | Loss: 0.00001461
Iteration 123/1000 | Loss: 0.00001461
Iteration 124/1000 | Loss: 0.00001461
Iteration 125/1000 | Loss: 0.00001461
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001460
Iteration 129/1000 | Loss: 0.00001460
Iteration 130/1000 | Loss: 0.00001460
Iteration 131/1000 | Loss: 0.00001460
Iteration 132/1000 | Loss: 0.00001460
Iteration 133/1000 | Loss: 0.00001460
Iteration 134/1000 | Loss: 0.00001460
Iteration 135/1000 | Loss: 0.00001460
Iteration 136/1000 | Loss: 0.00001460
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001459
Iteration 139/1000 | Loss: 0.00001459
Iteration 140/1000 | Loss: 0.00001459
Iteration 141/1000 | Loss: 0.00001459
Iteration 142/1000 | Loss: 0.00001459
Iteration 143/1000 | Loss: 0.00001459
Iteration 144/1000 | Loss: 0.00001459
Iteration 145/1000 | Loss: 0.00001459
Iteration 146/1000 | Loss: 0.00001459
Iteration 147/1000 | Loss: 0.00001459
Iteration 148/1000 | Loss: 0.00001458
Iteration 149/1000 | Loss: 0.00001458
Iteration 150/1000 | Loss: 0.00001458
Iteration 151/1000 | Loss: 0.00001458
Iteration 152/1000 | Loss: 0.00001458
Iteration 153/1000 | Loss: 0.00001458
Iteration 154/1000 | Loss: 0.00001458
Iteration 155/1000 | Loss: 0.00001458
Iteration 156/1000 | Loss: 0.00001458
Iteration 157/1000 | Loss: 0.00001458
Iteration 158/1000 | Loss: 0.00001458
Iteration 159/1000 | Loss: 0.00001458
Iteration 160/1000 | Loss: 0.00001458
Iteration 161/1000 | Loss: 0.00001458
Iteration 162/1000 | Loss: 0.00001458
Iteration 163/1000 | Loss: 0.00001458
Iteration 164/1000 | Loss: 0.00001458
Iteration 165/1000 | Loss: 0.00001458
Iteration 166/1000 | Loss: 0.00001458
Iteration 167/1000 | Loss: 0.00001458
Iteration 168/1000 | Loss: 0.00001458
Iteration 169/1000 | Loss: 0.00001458
Iteration 170/1000 | Loss: 0.00001458
Iteration 171/1000 | Loss: 0.00001458
Iteration 172/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4579331946151797e-05, 1.4579331946151797e-05, 1.4579331946151797e-05, 1.4579331946151797e-05, 1.4579331946151797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4579331946151797e-05

Optimization complete. Final v2v error: 3.217116355895996 mm

Highest mean error: 3.5128445625305176 mm for frame 67

Lowest mean error: 2.851646900177002 mm for frame 18

Saving results

Total time: 688.7331926822662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1053
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878348
Iteration 2/25 | Loss: 0.00138237
Iteration 3/25 | Loss: 0.00114987
Iteration 4/25 | Loss: 0.00113044
Iteration 5/25 | Loss: 0.00112719
Iteration 6/25 | Loss: 0.00112691
Iteration 7/25 | Loss: 0.00112691
Iteration 8/25 | Loss: 0.00112691
Iteration 9/25 | Loss: 0.00112691
Iteration 10/25 | Loss: 0.00112691
Iteration 11/25 | Loss: 0.00112691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011269132373854518, 0.0011269132373854518, 0.0011269132373854518, 0.0011269132373854518, 0.0011269132373854518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011269132373854518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92900461
Iteration 2/25 | Loss: 0.00040710
Iteration 3/25 | Loss: 0.00040709
Iteration 4/25 | Loss: 0.00040709
Iteration 5/25 | Loss: 0.00040709
Iteration 6/25 | Loss: 0.00040709
Iteration 7/25 | Loss: 0.00040709
Iteration 8/25 | Loss: 0.00040709
Iteration 9/25 | Loss: 0.00040709
Iteration 10/25 | Loss: 0.00040709
Iteration 11/25 | Loss: 0.00040709
Iteration 12/25 | Loss: 0.00040709
Iteration 13/25 | Loss: 0.00040709
Iteration 14/25 | Loss: 0.00040709
Iteration 15/25 | Loss: 0.00040709
Iteration 16/25 | Loss: 0.00040709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00040709212771616876, 0.00040709212771616876, 0.00040709212771616876, 0.00040709212771616876, 0.00040709212771616876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00040709212771616876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040709
Iteration 2/1000 | Loss: 0.00003794
Iteration 3/1000 | Loss: 0.00003150
Iteration 4/1000 | Loss: 0.00002943
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002737
Iteration 7/1000 | Loss: 0.00002684
Iteration 8/1000 | Loss: 0.00002639
Iteration 9/1000 | Loss: 0.00002608
Iteration 10/1000 | Loss: 0.00002580
Iteration 11/1000 | Loss: 0.00002560
Iteration 12/1000 | Loss: 0.00002548
Iteration 13/1000 | Loss: 0.00002546
Iteration 14/1000 | Loss: 0.00002526
Iteration 15/1000 | Loss: 0.00002522
Iteration 16/1000 | Loss: 0.00002520
Iteration 17/1000 | Loss: 0.00002516
Iteration 18/1000 | Loss: 0.00002515
Iteration 19/1000 | Loss: 0.00002514
Iteration 20/1000 | Loss: 0.00002514
Iteration 21/1000 | Loss: 0.00002514
Iteration 22/1000 | Loss: 0.00002514
Iteration 23/1000 | Loss: 0.00002514
Iteration 24/1000 | Loss: 0.00002514
Iteration 25/1000 | Loss: 0.00002514
Iteration 26/1000 | Loss: 0.00002514
Iteration 27/1000 | Loss: 0.00002511
Iteration 28/1000 | Loss: 0.00002511
Iteration 29/1000 | Loss: 0.00002511
Iteration 30/1000 | Loss: 0.00002511
Iteration 31/1000 | Loss: 0.00002511
Iteration 32/1000 | Loss: 0.00002511
Iteration 33/1000 | Loss: 0.00002511
Iteration 34/1000 | Loss: 0.00002511
Iteration 35/1000 | Loss: 0.00002511
Iteration 36/1000 | Loss: 0.00002510
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002509
Iteration 41/1000 | Loss: 0.00002509
Iteration 42/1000 | Loss: 0.00002509
Iteration 43/1000 | Loss: 0.00002509
Iteration 44/1000 | Loss: 0.00002508
Iteration 45/1000 | Loss: 0.00002508
Iteration 46/1000 | Loss: 0.00002508
Iteration 47/1000 | Loss: 0.00002508
Iteration 48/1000 | Loss: 0.00002508
Iteration 49/1000 | Loss: 0.00002508
Iteration 50/1000 | Loss: 0.00002508
Iteration 51/1000 | Loss: 0.00002508
Iteration 52/1000 | Loss: 0.00002508
Iteration 53/1000 | Loss: 0.00002508
Iteration 54/1000 | Loss: 0.00002508
Iteration 55/1000 | Loss: 0.00002508
Iteration 56/1000 | Loss: 0.00002508
Iteration 57/1000 | Loss: 0.00002508
Iteration 58/1000 | Loss: 0.00002507
Iteration 59/1000 | Loss: 0.00002507
Iteration 60/1000 | Loss: 0.00002507
Iteration 61/1000 | Loss: 0.00002507
Iteration 62/1000 | Loss: 0.00002507
Iteration 63/1000 | Loss: 0.00002507
Iteration 64/1000 | Loss: 0.00002507
Iteration 65/1000 | Loss: 0.00002507
Iteration 66/1000 | Loss: 0.00002506
Iteration 67/1000 | Loss: 0.00002506
Iteration 68/1000 | Loss: 0.00002506
Iteration 69/1000 | Loss: 0.00002506
Iteration 70/1000 | Loss: 0.00002506
Iteration 71/1000 | Loss: 0.00002506
Iteration 72/1000 | Loss: 0.00002505
Iteration 73/1000 | Loss: 0.00002505
Iteration 74/1000 | Loss: 0.00002505
Iteration 75/1000 | Loss: 0.00002505
Iteration 76/1000 | Loss: 0.00002505
Iteration 77/1000 | Loss: 0.00002505
Iteration 78/1000 | Loss: 0.00002505
Iteration 79/1000 | Loss: 0.00002505
Iteration 80/1000 | Loss: 0.00002505
Iteration 81/1000 | Loss: 0.00002505
Iteration 82/1000 | Loss: 0.00002505
Iteration 83/1000 | Loss: 0.00002505
Iteration 84/1000 | Loss: 0.00002505
Iteration 85/1000 | Loss: 0.00002504
Iteration 86/1000 | Loss: 0.00002504
Iteration 87/1000 | Loss: 0.00002504
Iteration 88/1000 | Loss: 0.00002503
Iteration 89/1000 | Loss: 0.00002503
Iteration 90/1000 | Loss: 0.00002503
Iteration 91/1000 | Loss: 0.00002503
Iteration 92/1000 | Loss: 0.00002503
Iteration 93/1000 | Loss: 0.00002503
Iteration 94/1000 | Loss: 0.00002503
Iteration 95/1000 | Loss: 0.00002503
Iteration 96/1000 | Loss: 0.00002503
Iteration 97/1000 | Loss: 0.00002503
Iteration 98/1000 | Loss: 0.00002503
Iteration 99/1000 | Loss: 0.00002503
Iteration 100/1000 | Loss: 0.00002503
Iteration 101/1000 | Loss: 0.00002502
Iteration 102/1000 | Loss: 0.00002502
Iteration 103/1000 | Loss: 0.00002502
Iteration 104/1000 | Loss: 0.00002502
Iteration 105/1000 | Loss: 0.00002502
Iteration 106/1000 | Loss: 0.00002502
Iteration 107/1000 | Loss: 0.00002502
Iteration 108/1000 | Loss: 0.00002502
Iteration 109/1000 | Loss: 0.00002502
Iteration 110/1000 | Loss: 0.00002502
Iteration 111/1000 | Loss: 0.00002502
Iteration 112/1000 | Loss: 0.00002502
Iteration 113/1000 | Loss: 0.00002502
Iteration 114/1000 | Loss: 0.00002502
Iteration 115/1000 | Loss: 0.00002502
Iteration 116/1000 | Loss: 0.00002502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.502455208741594e-05, 2.502455208741594e-05, 2.502455208741594e-05, 2.502455208741594e-05, 2.502455208741594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.502455208741594e-05

Optimization complete. Final v2v error: 4.225014686584473 mm

Highest mean error: 4.534343719482422 mm for frame 1

Lowest mean error: 3.8918051719665527 mm for frame 81

Saving results

Total time: 650.6039841175079
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1052
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796838
Iteration 2/25 | Loss: 0.00146634
Iteration 3/25 | Loss: 0.00124532
Iteration 4/25 | Loss: 0.00115020
Iteration 5/25 | Loss: 0.00111743
Iteration 6/25 | Loss: 0.00109000
Iteration 7/25 | Loss: 0.00108770
Iteration 8/25 | Loss: 0.00107754
Iteration 9/25 | Loss: 0.00107581
Iteration 10/25 | Loss: 0.00107566
Iteration 11/25 | Loss: 0.00107565
Iteration 12/25 | Loss: 0.00107563
Iteration 13/25 | Loss: 0.00107563
Iteration 14/25 | Loss: 0.00107562
Iteration 15/25 | Loss: 0.00107562
Iteration 16/25 | Loss: 0.00107562
Iteration 17/25 | Loss: 0.00107562
Iteration 18/25 | Loss: 0.00107562
Iteration 19/25 | Loss: 0.00107562
Iteration 20/25 | Loss: 0.00107562
Iteration 21/25 | Loss: 0.00107561
Iteration 22/25 | Loss: 0.00107561
Iteration 23/25 | Loss: 0.00107561
Iteration 24/25 | Loss: 0.00107561
Iteration 25/25 | Loss: 0.00107561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97300911
Iteration 2/25 | Loss: 0.00073150
Iteration 3/25 | Loss: 0.00073150
Iteration 4/25 | Loss: 0.00073150
Iteration 5/25 | Loss: 0.00073150
Iteration 6/25 | Loss: 0.00073150
Iteration 7/25 | Loss: 0.00073150
Iteration 8/25 | Loss: 0.00073150
Iteration 9/25 | Loss: 0.00073150
Iteration 10/25 | Loss: 0.00073150
Iteration 11/25 | Loss: 0.00073150
Iteration 12/25 | Loss: 0.00073150
Iteration 13/25 | Loss: 0.00073150
Iteration 14/25 | Loss: 0.00073150
Iteration 15/25 | Loss: 0.00073150
Iteration 16/25 | Loss: 0.00073150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007314954418689013, 0.0007314954418689013, 0.0007314954418689013, 0.0007314954418689013, 0.0007314954418689013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007314954418689013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073150
Iteration 2/1000 | Loss: 0.00002759
Iteration 3/1000 | Loss: 0.00001939
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00001605
Iteration 6/1000 | Loss: 0.00001559
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00012614
Iteration 9/1000 | Loss: 0.00001492
Iteration 10/1000 | Loss: 0.00001467
Iteration 11/1000 | Loss: 0.00001449
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001448
Iteration 15/1000 | Loss: 0.00001447
Iteration 16/1000 | Loss: 0.00001442
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001428
Iteration 21/1000 | Loss: 0.00001423
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001419
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001417
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001417
Iteration 30/1000 | Loss: 0.00001417
Iteration 31/1000 | Loss: 0.00001417
Iteration 32/1000 | Loss: 0.00001417
Iteration 33/1000 | Loss: 0.00001416
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001415
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001414
Iteration 39/1000 | Loss: 0.00001414
Iteration 40/1000 | Loss: 0.00001414
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001412
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001409
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001408
Iteration 54/1000 | Loss: 0.00001408
Iteration 55/1000 | Loss: 0.00001407
Iteration 56/1000 | Loss: 0.00001407
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001406
Iteration 63/1000 | Loss: 0.00001406
Iteration 64/1000 | Loss: 0.00001406
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001405
Iteration 67/1000 | Loss: 0.00001405
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001405
Iteration 73/1000 | Loss: 0.00001404
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001403
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001402
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001401
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001399
Iteration 87/1000 | Loss: 0.00001399
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001398
Iteration 91/1000 | Loss: 0.00001398
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001398
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001396
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001396
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001395
Iteration 105/1000 | Loss: 0.00001394
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001394
Iteration 108/1000 | Loss: 0.00001394
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001393
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001392
Iteration 114/1000 | Loss: 0.00001392
Iteration 115/1000 | Loss: 0.00001392
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001390
Iteration 123/1000 | Loss: 0.00001389
Iteration 124/1000 | Loss: 0.00001389
Iteration 125/1000 | Loss: 0.00001388
Iteration 126/1000 | Loss: 0.00001388
Iteration 127/1000 | Loss: 0.00001388
Iteration 128/1000 | Loss: 0.00001388
Iteration 129/1000 | Loss: 0.00001388
Iteration 130/1000 | Loss: 0.00001388
Iteration 131/1000 | Loss: 0.00001388
Iteration 132/1000 | Loss: 0.00001388
Iteration 133/1000 | Loss: 0.00001388
Iteration 134/1000 | Loss: 0.00001388
Iteration 135/1000 | Loss: 0.00001387
Iteration 136/1000 | Loss: 0.00001387
Iteration 137/1000 | Loss: 0.00001387
Iteration 138/1000 | Loss: 0.00001387
Iteration 139/1000 | Loss: 0.00001387
Iteration 140/1000 | Loss: 0.00001387
Iteration 141/1000 | Loss: 0.00001387
Iteration 142/1000 | Loss: 0.00001387
Iteration 143/1000 | Loss: 0.00001387
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001386
Iteration 149/1000 | Loss: 0.00001386
Iteration 150/1000 | Loss: 0.00001386
Iteration 151/1000 | Loss: 0.00001386
Iteration 152/1000 | Loss: 0.00001386
Iteration 153/1000 | Loss: 0.00001386
Iteration 154/1000 | Loss: 0.00001386
Iteration 155/1000 | Loss: 0.00001386
Iteration 156/1000 | Loss: 0.00001386
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001385
Iteration 160/1000 | Loss: 0.00001385
Iteration 161/1000 | Loss: 0.00001385
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001384
Iteration 164/1000 | Loss: 0.00001384
Iteration 165/1000 | Loss: 0.00001384
Iteration 166/1000 | Loss: 0.00001384
Iteration 167/1000 | Loss: 0.00001384
Iteration 168/1000 | Loss: 0.00001384
Iteration 169/1000 | Loss: 0.00001384
Iteration 170/1000 | Loss: 0.00001383
Iteration 171/1000 | Loss: 0.00001383
Iteration 172/1000 | Loss: 0.00001383
Iteration 173/1000 | Loss: 0.00001383
Iteration 174/1000 | Loss: 0.00001383
Iteration 175/1000 | Loss: 0.00001383
Iteration 176/1000 | Loss: 0.00001383
Iteration 177/1000 | Loss: 0.00001383
Iteration 178/1000 | Loss: 0.00001383
Iteration 179/1000 | Loss: 0.00001383
Iteration 180/1000 | Loss: 0.00001383
Iteration 181/1000 | Loss: 0.00001383
Iteration 182/1000 | Loss: 0.00001383
Iteration 183/1000 | Loss: 0.00001383
Iteration 184/1000 | Loss: 0.00001383
Iteration 185/1000 | Loss: 0.00001383
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001383
Iteration 188/1000 | Loss: 0.00001382
Iteration 189/1000 | Loss: 0.00001382
Iteration 190/1000 | Loss: 0.00001382
Iteration 191/1000 | Loss: 0.00001382
Iteration 192/1000 | Loss: 0.00001382
Iteration 193/1000 | Loss: 0.00001382
Iteration 194/1000 | Loss: 0.00001382
Iteration 195/1000 | Loss: 0.00001382
Iteration 196/1000 | Loss: 0.00001382
Iteration 197/1000 | Loss: 0.00001382
Iteration 198/1000 | Loss: 0.00001382
Iteration 199/1000 | Loss: 0.00001382
Iteration 200/1000 | Loss: 0.00001382
Iteration 201/1000 | Loss: 0.00001382
Iteration 202/1000 | Loss: 0.00001382
Iteration 203/1000 | Loss: 0.00001382
Iteration 204/1000 | Loss: 0.00001382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.3815220881951973e-05, 1.3815220881951973e-05, 1.3815220881951973e-05, 1.3815220881951973e-05, 1.3815220881951973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3815220881951973e-05

Optimization complete. Final v2v error: 3.1053993701934814 mm

Highest mean error: 3.734741449356079 mm for frame 107

Lowest mean error: 2.6731343269348145 mm for frame 0

Saving results

Total time: 1297.4860513210297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1050
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973962
Iteration 2/25 | Loss: 0.00255633
Iteration 3/25 | Loss: 0.00173650
Iteration 4/25 | Loss: 0.00159470
Iteration 5/25 | Loss: 0.00145684
Iteration 6/25 | Loss: 0.00140076
Iteration 7/25 | Loss: 0.00143589
Iteration 8/25 | Loss: 0.00125395
Iteration 9/25 | Loss: 0.00120957
Iteration 10/25 | Loss: 0.00118056
Iteration 11/25 | Loss: 0.00118949
Iteration 12/25 | Loss: 0.00116044
Iteration 13/25 | Loss: 0.00115521
Iteration 14/25 | Loss: 0.00113623
Iteration 15/25 | Loss: 0.00111945
Iteration 16/25 | Loss: 0.00111398
Iteration 17/25 | Loss: 0.00111047
Iteration 18/25 | Loss: 0.00110681
Iteration 19/25 | Loss: 0.00110744
Iteration 20/25 | Loss: 0.00110938
Iteration 21/25 | Loss: 0.00110654
Iteration 22/25 | Loss: 0.00110869
Iteration 23/25 | Loss: 0.00110203
Iteration 24/25 | Loss: 0.00110071
Iteration 25/25 | Loss: 0.00109831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37973523
Iteration 2/25 | Loss: 0.00136614
Iteration 3/25 | Loss: 0.00100423
Iteration 4/25 | Loss: 0.00100422
Iteration 5/25 | Loss: 0.00100422
Iteration 6/25 | Loss: 0.00100422
Iteration 7/25 | Loss: 0.00100422
Iteration 8/25 | Loss: 0.00100422
Iteration 9/25 | Loss: 0.00100422
Iteration 10/25 | Loss: 0.00100422
Iteration 11/25 | Loss: 0.00100422
Iteration 12/25 | Loss: 0.00100422
Iteration 13/25 | Loss: 0.00100422
Iteration 14/25 | Loss: 0.00100422
Iteration 15/25 | Loss: 0.00100422
Iteration 16/25 | Loss: 0.00100422
Iteration 17/25 | Loss: 0.00100422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010042231297120452, 0.0010042231297120452, 0.0010042231297120452, 0.0010042231297120452, 0.0010042231297120452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010042231297120452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100422
Iteration 2/1000 | Loss: 0.00037023
Iteration 3/1000 | Loss: 0.00048606
Iteration 4/1000 | Loss: 0.00024315
Iteration 5/1000 | Loss: 0.00027949
Iteration 6/1000 | Loss: 0.00026356
Iteration 7/1000 | Loss: 0.00022982
Iteration 8/1000 | Loss: 0.00020937
Iteration 9/1000 | Loss: 0.00019573
Iteration 10/1000 | Loss: 0.00018702
Iteration 11/1000 | Loss: 0.00024462
Iteration 12/1000 | Loss: 0.00022402
Iteration 13/1000 | Loss: 0.00039848
Iteration 14/1000 | Loss: 0.00030017
Iteration 15/1000 | Loss: 0.00031447
Iteration 16/1000 | Loss: 0.00025168
Iteration 17/1000 | Loss: 0.00024573
Iteration 18/1000 | Loss: 0.00026647
Iteration 19/1000 | Loss: 0.00007787
Iteration 20/1000 | Loss: 0.00026755
Iteration 21/1000 | Loss: 0.00035919
Iteration 22/1000 | Loss: 0.00003324
Iteration 23/1000 | Loss: 0.00002345
Iteration 24/1000 | Loss: 0.00006474
Iteration 25/1000 | Loss: 0.00007885
Iteration 26/1000 | Loss: 0.00001682
Iteration 27/1000 | Loss: 0.00002179
Iteration 28/1000 | Loss: 0.00001488
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00002107
Iteration 31/1000 | Loss: 0.00001455
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00002591
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001508
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001891
Iteration 39/1000 | Loss: 0.00001422
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001389
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001388
Iteration 45/1000 | Loss: 0.00001388
Iteration 46/1000 | Loss: 0.00001388
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001388
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001386
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001385
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001385
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001384
Iteration 83/1000 | Loss: 0.00001384
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001384
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001383
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001382
Iteration 105/1000 | Loss: 0.00001382
Iteration 106/1000 | Loss: 0.00001382
Iteration 107/1000 | Loss: 0.00001382
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001382
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001381
Iteration 114/1000 | Loss: 0.00001381
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001380
Iteration 124/1000 | Loss: 0.00001380
Iteration 125/1000 | Loss: 0.00001380
Iteration 126/1000 | Loss: 0.00001380
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001380
Iteration 159/1000 | Loss: 0.00001380
Iteration 160/1000 | Loss: 0.00001380
Iteration 161/1000 | Loss: 0.00001380
Iteration 162/1000 | Loss: 0.00001380
Iteration 163/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.3800141459796578e-05, 1.3800141459796578e-05, 1.3800141459796578e-05, 1.3800141459796578e-05, 1.3800141459796578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3800141459796578e-05

Optimization complete. Final v2v error: 2.947162628173828 mm

Highest mean error: 10.879044532775879 mm for frame 53

Lowest mean error: 2.4370291233062744 mm for frame 85

Saving results

Total time: 3925.313145875931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1089
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596918
Iteration 2/25 | Loss: 0.00127285
Iteration 3/25 | Loss: 0.00114862
Iteration 4/25 | Loss: 0.00112766
Iteration 5/25 | Loss: 0.00112006
Iteration 6/25 | Loss: 0.00111790
Iteration 7/25 | Loss: 0.00111726
Iteration 8/25 | Loss: 0.00111726
Iteration 9/25 | Loss: 0.00111726
Iteration 10/25 | Loss: 0.00111726
Iteration 11/25 | Loss: 0.00111726
Iteration 12/25 | Loss: 0.00111726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011172608938068151, 0.0011172608938068151, 0.0011172608938068151, 0.0011172608938068151, 0.0011172608938068151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011172608938068151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32500124
Iteration 2/25 | Loss: 0.00103468
Iteration 3/25 | Loss: 0.00103468
Iteration 4/25 | Loss: 0.00103467
Iteration 5/25 | Loss: 0.00103467
Iteration 6/25 | Loss: 0.00103467
Iteration 7/25 | Loss: 0.00103467
Iteration 8/25 | Loss: 0.00103467
Iteration 9/25 | Loss: 0.00103467
Iteration 10/25 | Loss: 0.00103467
Iteration 11/25 | Loss: 0.00103467
Iteration 12/25 | Loss: 0.00103467
Iteration 13/25 | Loss: 0.00103467
Iteration 14/25 | Loss: 0.00103467
Iteration 15/25 | Loss: 0.00103467
Iteration 16/25 | Loss: 0.00103467
Iteration 17/25 | Loss: 0.00103467
Iteration 18/25 | Loss: 0.00103467
Iteration 19/25 | Loss: 0.00103467
Iteration 20/25 | Loss: 0.00103467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010346723720431328, 0.0010346723720431328, 0.0010346723720431328, 0.0010346723720431328, 0.0010346723720431328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010346723720431328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103467
Iteration 2/1000 | Loss: 0.00006832
Iteration 3/1000 | Loss: 0.00004270
Iteration 4/1000 | Loss: 0.00003007
Iteration 5/1000 | Loss: 0.00002630
Iteration 6/1000 | Loss: 0.00002457
Iteration 7/1000 | Loss: 0.00002337
Iteration 8/1000 | Loss: 0.00002226
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002095
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00002029
Iteration 13/1000 | Loss: 0.00002005
Iteration 14/1000 | Loss: 0.00001994
Iteration 15/1000 | Loss: 0.00001981
Iteration 16/1000 | Loss: 0.00001964
Iteration 17/1000 | Loss: 0.00001963
Iteration 18/1000 | Loss: 0.00001963
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001952
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001948
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00001948
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001944
Iteration 32/1000 | Loss: 0.00001944
Iteration 33/1000 | Loss: 0.00001944
Iteration 34/1000 | Loss: 0.00001944
Iteration 35/1000 | Loss: 0.00001944
Iteration 36/1000 | Loss: 0.00001944
Iteration 37/1000 | Loss: 0.00001944
Iteration 38/1000 | Loss: 0.00001943
Iteration 39/1000 | Loss: 0.00001940
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001938
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001936
Iteration 51/1000 | Loss: 0.00001934
Iteration 52/1000 | Loss: 0.00001934
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001934
Iteration 55/1000 | Loss: 0.00001934
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001933
Iteration 60/1000 | Loss: 0.00001933
Iteration 61/1000 | Loss: 0.00001933
Iteration 62/1000 | Loss: 0.00001933
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001930
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00001930
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001929
Iteration 72/1000 | Loss: 0.00001929
Iteration 73/1000 | Loss: 0.00001929
Iteration 74/1000 | Loss: 0.00001929
Iteration 75/1000 | Loss: 0.00001929
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001929
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001927
Iteration 88/1000 | Loss: 0.00001926
Iteration 89/1000 | Loss: 0.00001926
Iteration 90/1000 | Loss: 0.00001926
Iteration 91/1000 | Loss: 0.00001926
Iteration 92/1000 | Loss: 0.00001925
Iteration 93/1000 | Loss: 0.00001925
Iteration 94/1000 | Loss: 0.00001925
Iteration 95/1000 | Loss: 0.00001925
Iteration 96/1000 | Loss: 0.00001925
Iteration 97/1000 | Loss: 0.00001924
Iteration 98/1000 | Loss: 0.00001924
Iteration 99/1000 | Loss: 0.00001924
Iteration 100/1000 | Loss: 0.00001923
Iteration 101/1000 | Loss: 0.00001923
Iteration 102/1000 | Loss: 0.00001923
Iteration 103/1000 | Loss: 0.00001923
Iteration 104/1000 | Loss: 0.00001923
Iteration 105/1000 | Loss: 0.00001923
Iteration 106/1000 | Loss: 0.00001922
Iteration 107/1000 | Loss: 0.00001922
Iteration 108/1000 | Loss: 0.00001922
Iteration 109/1000 | Loss: 0.00001922
Iteration 110/1000 | Loss: 0.00001922
Iteration 111/1000 | Loss: 0.00001922
Iteration 112/1000 | Loss: 0.00001921
Iteration 113/1000 | Loss: 0.00001921
Iteration 114/1000 | Loss: 0.00001921
Iteration 115/1000 | Loss: 0.00001921
Iteration 116/1000 | Loss: 0.00001921
Iteration 117/1000 | Loss: 0.00001921
Iteration 118/1000 | Loss: 0.00001921
Iteration 119/1000 | Loss: 0.00001921
Iteration 120/1000 | Loss: 0.00001921
Iteration 121/1000 | Loss: 0.00001921
Iteration 122/1000 | Loss: 0.00001920
Iteration 123/1000 | Loss: 0.00001920
Iteration 124/1000 | Loss: 0.00001920
Iteration 125/1000 | Loss: 0.00001920
Iteration 126/1000 | Loss: 0.00001920
Iteration 127/1000 | Loss: 0.00001920
Iteration 128/1000 | Loss: 0.00001920
Iteration 129/1000 | Loss: 0.00001919
Iteration 130/1000 | Loss: 0.00001919
Iteration 131/1000 | Loss: 0.00001919
Iteration 132/1000 | Loss: 0.00001919
Iteration 133/1000 | Loss: 0.00001919
Iteration 134/1000 | Loss: 0.00001919
Iteration 135/1000 | Loss: 0.00001918
Iteration 136/1000 | Loss: 0.00001918
Iteration 137/1000 | Loss: 0.00001918
Iteration 138/1000 | Loss: 0.00001918
Iteration 139/1000 | Loss: 0.00001917
Iteration 140/1000 | Loss: 0.00001917
Iteration 141/1000 | Loss: 0.00001917
Iteration 142/1000 | Loss: 0.00001916
Iteration 143/1000 | Loss: 0.00001916
Iteration 144/1000 | Loss: 0.00001916
Iteration 145/1000 | Loss: 0.00001916
Iteration 146/1000 | Loss: 0.00001915
Iteration 147/1000 | Loss: 0.00001915
Iteration 148/1000 | Loss: 0.00001915
Iteration 149/1000 | Loss: 0.00001915
Iteration 150/1000 | Loss: 0.00001915
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001915
Iteration 153/1000 | Loss: 0.00001915
Iteration 154/1000 | Loss: 0.00001915
Iteration 155/1000 | Loss: 0.00001914
Iteration 156/1000 | Loss: 0.00001914
Iteration 157/1000 | Loss: 0.00001914
Iteration 158/1000 | Loss: 0.00001914
Iteration 159/1000 | Loss: 0.00001914
Iteration 160/1000 | Loss: 0.00001914
Iteration 161/1000 | Loss: 0.00001914
Iteration 162/1000 | Loss: 0.00001913
Iteration 163/1000 | Loss: 0.00001913
Iteration 164/1000 | Loss: 0.00001913
Iteration 165/1000 | Loss: 0.00001913
Iteration 166/1000 | Loss: 0.00001913
Iteration 167/1000 | Loss: 0.00001912
Iteration 168/1000 | Loss: 0.00001912
Iteration 169/1000 | Loss: 0.00001912
Iteration 170/1000 | Loss: 0.00001912
Iteration 171/1000 | Loss: 0.00001912
Iteration 172/1000 | Loss: 0.00001912
Iteration 173/1000 | Loss: 0.00001912
Iteration 174/1000 | Loss: 0.00001912
Iteration 175/1000 | Loss: 0.00001912
Iteration 176/1000 | Loss: 0.00001912
Iteration 177/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.9119879652862437e-05, 1.9119879652862437e-05, 1.9119879652862437e-05, 1.9119879652862437e-05, 1.9119879652862437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9119879652862437e-05

Optimization complete. Final v2v error: 3.718090772628784 mm

Highest mean error: 4.262940883636475 mm for frame 94

Lowest mean error: 3.06853985786438 mm for frame 57

Saving results

Total time: 815.5558679103851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1088
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445289
Iteration 2/25 | Loss: 0.00136825
Iteration 3/25 | Loss: 0.00111301
Iteration 4/25 | Loss: 0.00108013
Iteration 5/25 | Loss: 0.00107473
Iteration 6/25 | Loss: 0.00107305
Iteration 7/25 | Loss: 0.00107305
Iteration 8/25 | Loss: 0.00107305
Iteration 9/25 | Loss: 0.00107305
Iteration 10/25 | Loss: 0.00107305
Iteration 11/25 | Loss: 0.00107305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010730467038229108, 0.0010730467038229108, 0.0010730467038229108, 0.0010730467038229108, 0.0010730467038229108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010730467038229108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48777604
Iteration 2/25 | Loss: 0.00062727
Iteration 3/25 | Loss: 0.00062727
Iteration 4/25 | Loss: 0.00062727
Iteration 5/25 | Loss: 0.00062727
Iteration 6/25 | Loss: 0.00062727
Iteration 7/25 | Loss: 0.00062727
Iteration 8/25 | Loss: 0.00062727
Iteration 9/25 | Loss: 0.00062727
Iteration 10/25 | Loss: 0.00062727
Iteration 11/25 | Loss: 0.00062727
Iteration 12/25 | Loss: 0.00062727
Iteration 13/25 | Loss: 0.00062727
Iteration 14/25 | Loss: 0.00062727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006272702594287694, 0.0006272702594287694, 0.0006272702594287694, 0.0006272702594287694, 0.0006272702594287694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006272702594287694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062727
Iteration 2/1000 | Loss: 0.00002620
Iteration 3/1000 | Loss: 0.00001639
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001312
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001246
Iteration 9/1000 | Loss: 0.00001231
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001196
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001188
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001177
Iteration 27/1000 | Loss: 0.00001177
Iteration 28/1000 | Loss: 0.00001177
Iteration 29/1000 | Loss: 0.00001177
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001173
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001173
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001168
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001161
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001158
Iteration 105/1000 | Loss: 0.00001158
Iteration 106/1000 | Loss: 0.00001158
Iteration 107/1000 | Loss: 0.00001158
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001157
Iteration 115/1000 | Loss: 0.00001157
Iteration 116/1000 | Loss: 0.00001157
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001156
Iteration 123/1000 | Loss: 0.00001156
Iteration 124/1000 | Loss: 0.00001156
Iteration 125/1000 | Loss: 0.00001156
Iteration 126/1000 | Loss: 0.00001155
Iteration 127/1000 | Loss: 0.00001155
Iteration 128/1000 | Loss: 0.00001155
Iteration 129/1000 | Loss: 0.00001155
Iteration 130/1000 | Loss: 0.00001155
Iteration 131/1000 | Loss: 0.00001154
Iteration 132/1000 | Loss: 0.00001154
Iteration 133/1000 | Loss: 0.00001154
Iteration 134/1000 | Loss: 0.00001154
Iteration 135/1000 | Loss: 0.00001154
Iteration 136/1000 | Loss: 0.00001154
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001153
Iteration 141/1000 | Loss: 0.00001153
Iteration 142/1000 | Loss: 0.00001153
Iteration 143/1000 | Loss: 0.00001153
Iteration 144/1000 | Loss: 0.00001153
Iteration 145/1000 | Loss: 0.00001153
Iteration 146/1000 | Loss: 0.00001153
Iteration 147/1000 | Loss: 0.00001153
Iteration 148/1000 | Loss: 0.00001152
Iteration 149/1000 | Loss: 0.00001152
Iteration 150/1000 | Loss: 0.00001152
Iteration 151/1000 | Loss: 0.00001152
Iteration 152/1000 | Loss: 0.00001152
Iteration 153/1000 | Loss: 0.00001152
Iteration 154/1000 | Loss: 0.00001152
Iteration 155/1000 | Loss: 0.00001152
Iteration 156/1000 | Loss: 0.00001152
Iteration 157/1000 | Loss: 0.00001152
Iteration 158/1000 | Loss: 0.00001152
Iteration 159/1000 | Loss: 0.00001152
Iteration 160/1000 | Loss: 0.00001152
Iteration 161/1000 | Loss: 0.00001151
Iteration 162/1000 | Loss: 0.00001151
Iteration 163/1000 | Loss: 0.00001151
Iteration 164/1000 | Loss: 0.00001151
Iteration 165/1000 | Loss: 0.00001151
Iteration 166/1000 | Loss: 0.00001151
Iteration 167/1000 | Loss: 0.00001151
Iteration 168/1000 | Loss: 0.00001151
Iteration 169/1000 | Loss: 0.00001151
Iteration 170/1000 | Loss: 0.00001151
Iteration 171/1000 | Loss: 0.00001151
Iteration 172/1000 | Loss: 0.00001150
Iteration 173/1000 | Loss: 0.00001150
Iteration 174/1000 | Loss: 0.00001150
Iteration 175/1000 | Loss: 0.00001150
Iteration 176/1000 | Loss: 0.00001150
Iteration 177/1000 | Loss: 0.00001149
Iteration 178/1000 | Loss: 0.00001149
Iteration 179/1000 | Loss: 0.00001149
Iteration 180/1000 | Loss: 0.00001149
Iteration 181/1000 | Loss: 0.00001149
Iteration 182/1000 | Loss: 0.00001149
Iteration 183/1000 | Loss: 0.00001149
Iteration 184/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.1489561074995436e-05, 1.1489561074995436e-05, 1.1489561074995436e-05, 1.1489561074995436e-05, 1.1489561074995436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1489561074995436e-05

Optimization complete. Final v2v error: 2.839331865310669 mm

Highest mean error: 4.051085472106934 mm for frame 106

Lowest mean error: 2.4599804878234863 mm for frame 159

Saving results

Total time: 1182.3926758766174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1017
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549315
Iteration 2/25 | Loss: 0.00133799
Iteration 3/25 | Loss: 0.00121094
Iteration 4/25 | Loss: 0.00118992
Iteration 5/25 | Loss: 0.00118773
Iteration 6/25 | Loss: 0.00118773
Iteration 7/25 | Loss: 0.00118773
Iteration 8/25 | Loss: 0.00118773
Iteration 9/25 | Loss: 0.00118773
Iteration 10/25 | Loss: 0.00118773
Iteration 11/25 | Loss: 0.00118773
Iteration 12/25 | Loss: 0.00118773
Iteration 13/25 | Loss: 0.00118773
Iteration 14/25 | Loss: 0.00118773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00118773162830621, 0.00118773162830621, 0.00118773162830621, 0.00118773162830621, 0.00118773162830621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00118773162830621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40165854
Iteration 2/25 | Loss: 0.00084441
Iteration 3/25 | Loss: 0.00084441
Iteration 4/25 | Loss: 0.00084441
Iteration 5/25 | Loss: 0.00084441
Iteration 6/25 | Loss: 0.00084441
Iteration 7/25 | Loss: 0.00084441
Iteration 8/25 | Loss: 0.00084441
Iteration 9/25 | Loss: 0.00084441
Iteration 10/25 | Loss: 0.00084441
Iteration 11/25 | Loss: 0.00084441
Iteration 12/25 | Loss: 0.00084441
Iteration 13/25 | Loss: 0.00084441
Iteration 14/25 | Loss: 0.00084441
Iteration 15/25 | Loss: 0.00084441
Iteration 16/25 | Loss: 0.00084441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008444113191217184, 0.0008444113191217184, 0.0008444113191217184, 0.0008444113191217184, 0.0008444113191217184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008444113191217184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084441
Iteration 2/1000 | Loss: 0.00004814
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002875
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002635
Iteration 8/1000 | Loss: 0.00002608
Iteration 9/1000 | Loss: 0.00002584
Iteration 10/1000 | Loss: 0.00002578
Iteration 11/1000 | Loss: 0.00002558
Iteration 12/1000 | Loss: 0.00002542
Iteration 13/1000 | Loss: 0.00002529
Iteration 14/1000 | Loss: 0.00002529
Iteration 15/1000 | Loss: 0.00002524
Iteration 16/1000 | Loss: 0.00002524
Iteration 17/1000 | Loss: 0.00002520
Iteration 18/1000 | Loss: 0.00002519
Iteration 19/1000 | Loss: 0.00002519
Iteration 20/1000 | Loss: 0.00002518
Iteration 21/1000 | Loss: 0.00002518
Iteration 22/1000 | Loss: 0.00002517
Iteration 23/1000 | Loss: 0.00002517
Iteration 24/1000 | Loss: 0.00002516
Iteration 25/1000 | Loss: 0.00002515
Iteration 26/1000 | Loss: 0.00002514
Iteration 27/1000 | Loss: 0.00002514
Iteration 28/1000 | Loss: 0.00002513
Iteration 29/1000 | Loss: 0.00002513
Iteration 30/1000 | Loss: 0.00002511
Iteration 31/1000 | Loss: 0.00002511
Iteration 32/1000 | Loss: 0.00002510
Iteration 33/1000 | Loss: 0.00002510
Iteration 34/1000 | Loss: 0.00002510
Iteration 35/1000 | Loss: 0.00002510
Iteration 36/1000 | Loss: 0.00002510
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002509
Iteration 40/1000 | Loss: 0.00002509
Iteration 41/1000 | Loss: 0.00002509
Iteration 42/1000 | Loss: 0.00002509
Iteration 43/1000 | Loss: 0.00002509
Iteration 44/1000 | Loss: 0.00002509
Iteration 45/1000 | Loss: 0.00002508
Iteration 46/1000 | Loss: 0.00002508
Iteration 47/1000 | Loss: 0.00002508
Iteration 48/1000 | Loss: 0.00002508
Iteration 49/1000 | Loss: 0.00002508
Iteration 50/1000 | Loss: 0.00002507
Iteration 51/1000 | Loss: 0.00002507
Iteration 52/1000 | Loss: 0.00002507
Iteration 53/1000 | Loss: 0.00002507
Iteration 54/1000 | Loss: 0.00002507
Iteration 55/1000 | Loss: 0.00002507
Iteration 56/1000 | Loss: 0.00002507
Iteration 57/1000 | Loss: 0.00002507
Iteration 58/1000 | Loss: 0.00002507
Iteration 59/1000 | Loss: 0.00002507
Iteration 60/1000 | Loss: 0.00002506
Iteration 61/1000 | Loss: 0.00002506
Iteration 62/1000 | Loss: 0.00002506
Iteration 63/1000 | Loss: 0.00002506
Iteration 64/1000 | Loss: 0.00002506
Iteration 65/1000 | Loss: 0.00002506
Iteration 66/1000 | Loss: 0.00002506
Iteration 67/1000 | Loss: 0.00002506
Iteration 68/1000 | Loss: 0.00002506
Iteration 69/1000 | Loss: 0.00002506
Iteration 70/1000 | Loss: 0.00002506
Iteration 71/1000 | Loss: 0.00002506
Iteration 72/1000 | Loss: 0.00002506
Iteration 73/1000 | Loss: 0.00002506
Iteration 74/1000 | Loss: 0.00002506
Iteration 75/1000 | Loss: 0.00002506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.5059134713956155e-05, 2.5059134713956155e-05, 2.5059134713956155e-05, 2.5059134713956155e-05, 2.5059134713956155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5059134713956155e-05

Optimization complete. Final v2v error: 4.110755920410156 mm

Highest mean error: 4.760533809661865 mm for frame 88

Lowest mean error: 3.6653759479522705 mm for frame 123

Saving results

Total time: 988.5735940933228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1011
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389985
Iteration 2/25 | Loss: 0.00111241
Iteration 3/25 | Loss: 0.00103865
Iteration 4/25 | Loss: 0.00102636
Iteration 5/25 | Loss: 0.00102250
Iteration 6/25 | Loss: 0.00102178
Iteration 7/25 | Loss: 0.00102178
Iteration 8/25 | Loss: 0.00102178
Iteration 9/25 | Loss: 0.00102178
Iteration 10/25 | Loss: 0.00102178
Iteration 11/25 | Loss: 0.00102178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010217808885499835, 0.0010217808885499835, 0.0010217808885499835, 0.0010217808885499835, 0.0010217808885499835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010217808885499835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29918933
Iteration 2/25 | Loss: 0.00071877
Iteration 3/25 | Loss: 0.00071877
Iteration 4/25 | Loss: 0.00071877
Iteration 5/25 | Loss: 0.00071877
Iteration 6/25 | Loss: 0.00071877
Iteration 7/25 | Loss: 0.00071877
Iteration 8/25 | Loss: 0.00071877
Iteration 9/25 | Loss: 0.00071877
Iteration 10/25 | Loss: 0.00071877
Iteration 11/25 | Loss: 0.00071877
Iteration 12/25 | Loss: 0.00071877
Iteration 13/25 | Loss: 0.00071877
Iteration 14/25 | Loss: 0.00071877
Iteration 15/25 | Loss: 0.00071877
Iteration 16/25 | Loss: 0.00071877
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007187701994553208, 0.0007187701994553208, 0.0007187701994553208, 0.0007187701994553208, 0.0007187701994553208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007187701994553208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071877
Iteration 2/1000 | Loss: 0.00001550
Iteration 3/1000 | Loss: 0.00001150
Iteration 4/1000 | Loss: 0.00001041
Iteration 5/1000 | Loss: 0.00000991
Iteration 6/1000 | Loss: 0.00000955
Iteration 7/1000 | Loss: 0.00000931
Iteration 8/1000 | Loss: 0.00000907
Iteration 9/1000 | Loss: 0.00000886
Iteration 10/1000 | Loss: 0.00000884
Iteration 11/1000 | Loss: 0.00000884
Iteration 12/1000 | Loss: 0.00000875
Iteration 13/1000 | Loss: 0.00000875
Iteration 14/1000 | Loss: 0.00000867
Iteration 15/1000 | Loss: 0.00000864
Iteration 16/1000 | Loss: 0.00000862
Iteration 17/1000 | Loss: 0.00000860
Iteration 18/1000 | Loss: 0.00000860
Iteration 19/1000 | Loss: 0.00000859
Iteration 20/1000 | Loss: 0.00000859
Iteration 21/1000 | Loss: 0.00000859
Iteration 22/1000 | Loss: 0.00000858
Iteration 23/1000 | Loss: 0.00000858
Iteration 24/1000 | Loss: 0.00000857
Iteration 25/1000 | Loss: 0.00000856
Iteration 26/1000 | Loss: 0.00000855
Iteration 27/1000 | Loss: 0.00000855
Iteration 28/1000 | Loss: 0.00000854
Iteration 29/1000 | Loss: 0.00000851
Iteration 30/1000 | Loss: 0.00000850
Iteration 31/1000 | Loss: 0.00000850
Iteration 32/1000 | Loss: 0.00000850
Iteration 33/1000 | Loss: 0.00000850
Iteration 34/1000 | Loss: 0.00000850
Iteration 35/1000 | Loss: 0.00000850
Iteration 36/1000 | Loss: 0.00000849
Iteration 37/1000 | Loss: 0.00000849
Iteration 38/1000 | Loss: 0.00000848
Iteration 39/1000 | Loss: 0.00000848
Iteration 40/1000 | Loss: 0.00000848
Iteration 41/1000 | Loss: 0.00000848
Iteration 42/1000 | Loss: 0.00000848
Iteration 43/1000 | Loss: 0.00000847
Iteration 44/1000 | Loss: 0.00000846
Iteration 45/1000 | Loss: 0.00000845
Iteration 46/1000 | Loss: 0.00000844
Iteration 47/1000 | Loss: 0.00000843
Iteration 48/1000 | Loss: 0.00000843
Iteration 49/1000 | Loss: 0.00000841
Iteration 50/1000 | Loss: 0.00000840
Iteration 51/1000 | Loss: 0.00000840
Iteration 52/1000 | Loss: 0.00000839
Iteration 53/1000 | Loss: 0.00000839
Iteration 54/1000 | Loss: 0.00000833
Iteration 55/1000 | Loss: 0.00000833
Iteration 56/1000 | Loss: 0.00000832
Iteration 57/1000 | Loss: 0.00000831
Iteration 58/1000 | Loss: 0.00000830
Iteration 59/1000 | Loss: 0.00000829
Iteration 60/1000 | Loss: 0.00000828
Iteration 61/1000 | Loss: 0.00000828
Iteration 62/1000 | Loss: 0.00000827
Iteration 63/1000 | Loss: 0.00000827
Iteration 64/1000 | Loss: 0.00000827
Iteration 65/1000 | Loss: 0.00000826
Iteration 66/1000 | Loss: 0.00000826
Iteration 67/1000 | Loss: 0.00000825
Iteration 68/1000 | Loss: 0.00000824
Iteration 69/1000 | Loss: 0.00000824
Iteration 70/1000 | Loss: 0.00000824
Iteration 71/1000 | Loss: 0.00000823
Iteration 72/1000 | Loss: 0.00000823
Iteration 73/1000 | Loss: 0.00000823
Iteration 74/1000 | Loss: 0.00000823
Iteration 75/1000 | Loss: 0.00000823
Iteration 76/1000 | Loss: 0.00000822
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000822
Iteration 79/1000 | Loss: 0.00000822
Iteration 80/1000 | Loss: 0.00000822
Iteration 81/1000 | Loss: 0.00000822
Iteration 82/1000 | Loss: 0.00000822
Iteration 83/1000 | Loss: 0.00000822
Iteration 84/1000 | Loss: 0.00000822
Iteration 85/1000 | Loss: 0.00000821
Iteration 86/1000 | Loss: 0.00000821
Iteration 87/1000 | Loss: 0.00000821
Iteration 88/1000 | Loss: 0.00000821
Iteration 89/1000 | Loss: 0.00000821
Iteration 90/1000 | Loss: 0.00000821
Iteration 91/1000 | Loss: 0.00000820
Iteration 92/1000 | Loss: 0.00000820
Iteration 93/1000 | Loss: 0.00000820
Iteration 94/1000 | Loss: 0.00000820
Iteration 95/1000 | Loss: 0.00000820
Iteration 96/1000 | Loss: 0.00000819
Iteration 97/1000 | Loss: 0.00000819
Iteration 98/1000 | Loss: 0.00000818
Iteration 99/1000 | Loss: 0.00000818
Iteration 100/1000 | Loss: 0.00000818
Iteration 101/1000 | Loss: 0.00000818
Iteration 102/1000 | Loss: 0.00000818
Iteration 103/1000 | Loss: 0.00000818
Iteration 104/1000 | Loss: 0.00000818
Iteration 105/1000 | Loss: 0.00000818
Iteration 106/1000 | Loss: 0.00000818
Iteration 107/1000 | Loss: 0.00000817
Iteration 108/1000 | Loss: 0.00000817
Iteration 109/1000 | Loss: 0.00000817
Iteration 110/1000 | Loss: 0.00000817
Iteration 111/1000 | Loss: 0.00000817
Iteration 112/1000 | Loss: 0.00000817
Iteration 113/1000 | Loss: 0.00000817
Iteration 114/1000 | Loss: 0.00000816
Iteration 115/1000 | Loss: 0.00000816
Iteration 116/1000 | Loss: 0.00000816
Iteration 117/1000 | Loss: 0.00000816
Iteration 118/1000 | Loss: 0.00000815
Iteration 119/1000 | Loss: 0.00000815
Iteration 120/1000 | Loss: 0.00000815
Iteration 121/1000 | Loss: 0.00000815
Iteration 122/1000 | Loss: 0.00000815
Iteration 123/1000 | Loss: 0.00000815
Iteration 124/1000 | Loss: 0.00000815
Iteration 125/1000 | Loss: 0.00000815
Iteration 126/1000 | Loss: 0.00000815
Iteration 127/1000 | Loss: 0.00000814
Iteration 128/1000 | Loss: 0.00000814
Iteration 129/1000 | Loss: 0.00000814
Iteration 130/1000 | Loss: 0.00000814
Iteration 131/1000 | Loss: 0.00000814
Iteration 132/1000 | Loss: 0.00000814
Iteration 133/1000 | Loss: 0.00000814
Iteration 134/1000 | Loss: 0.00000813
Iteration 135/1000 | Loss: 0.00000813
Iteration 136/1000 | Loss: 0.00000812
Iteration 137/1000 | Loss: 0.00000812
Iteration 138/1000 | Loss: 0.00000812
Iteration 139/1000 | Loss: 0.00000812
Iteration 140/1000 | Loss: 0.00000812
Iteration 141/1000 | Loss: 0.00000812
Iteration 142/1000 | Loss: 0.00000811
Iteration 143/1000 | Loss: 0.00000811
Iteration 144/1000 | Loss: 0.00000811
Iteration 145/1000 | Loss: 0.00000811
Iteration 146/1000 | Loss: 0.00000811
Iteration 147/1000 | Loss: 0.00000811
Iteration 148/1000 | Loss: 0.00000810
Iteration 149/1000 | Loss: 0.00000810
Iteration 150/1000 | Loss: 0.00000810
Iteration 151/1000 | Loss: 0.00000810
Iteration 152/1000 | Loss: 0.00000810
Iteration 153/1000 | Loss: 0.00000809
Iteration 154/1000 | Loss: 0.00000809
Iteration 155/1000 | Loss: 0.00000809
Iteration 156/1000 | Loss: 0.00000809
Iteration 157/1000 | Loss: 0.00000809
Iteration 158/1000 | Loss: 0.00000809
Iteration 159/1000 | Loss: 0.00000809
Iteration 160/1000 | Loss: 0.00000809
Iteration 161/1000 | Loss: 0.00000809
Iteration 162/1000 | Loss: 0.00000809
Iteration 163/1000 | Loss: 0.00000809
Iteration 164/1000 | Loss: 0.00000809
Iteration 165/1000 | Loss: 0.00000809
Iteration 166/1000 | Loss: 0.00000809
Iteration 167/1000 | Loss: 0.00000809
Iteration 168/1000 | Loss: 0.00000809
Iteration 169/1000 | Loss: 0.00000809
Iteration 170/1000 | Loss: 0.00000809
Iteration 171/1000 | Loss: 0.00000809
Iteration 172/1000 | Loss: 0.00000809
Iteration 173/1000 | Loss: 0.00000809
Iteration 174/1000 | Loss: 0.00000809
Iteration 175/1000 | Loss: 0.00000809
Iteration 176/1000 | Loss: 0.00000809
Iteration 177/1000 | Loss: 0.00000809
Iteration 178/1000 | Loss: 0.00000809
Iteration 179/1000 | Loss: 0.00000809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [8.087323294603266e-06, 8.087323294603266e-06, 8.087323294603266e-06, 8.087323294603266e-06, 8.087323294603266e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.087323294603266e-06

Optimization complete. Final v2v error: 2.484590768814087 mm

Highest mean error: 2.644972085952759 mm for frame 84

Lowest mean error: 2.390256404876709 mm for frame 162

Saving results

Total time: 1165.1956350803375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1099
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466448
Iteration 2/25 | Loss: 0.00114306
Iteration 3/25 | Loss: 0.00104517
Iteration 4/25 | Loss: 0.00102844
Iteration 5/25 | Loss: 0.00102295
Iteration 6/25 | Loss: 0.00102173
Iteration 7/25 | Loss: 0.00102173
Iteration 8/25 | Loss: 0.00102173
Iteration 9/25 | Loss: 0.00102173
Iteration 10/25 | Loss: 0.00102173
Iteration 11/25 | Loss: 0.00102173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001021728152409196, 0.001021728152409196, 0.001021728152409196, 0.001021728152409196, 0.001021728152409196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001021728152409196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49603784
Iteration 2/25 | Loss: 0.00062192
Iteration 3/25 | Loss: 0.00062191
Iteration 4/25 | Loss: 0.00062191
Iteration 5/25 | Loss: 0.00062191
Iteration 6/25 | Loss: 0.00062191
Iteration 7/25 | Loss: 0.00062191
Iteration 8/25 | Loss: 0.00062191
Iteration 9/25 | Loss: 0.00062191
Iteration 10/25 | Loss: 0.00062191
Iteration 11/25 | Loss: 0.00062191
Iteration 12/25 | Loss: 0.00062191
Iteration 13/25 | Loss: 0.00062191
Iteration 14/25 | Loss: 0.00062191
Iteration 15/25 | Loss: 0.00062191
Iteration 16/25 | Loss: 0.00062191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006219103233888745, 0.0006219103233888745, 0.0006219103233888745, 0.0006219103233888745, 0.0006219103233888745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006219103233888745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062191
Iteration 2/1000 | Loss: 0.00001652
Iteration 3/1000 | Loss: 0.00001248
Iteration 4/1000 | Loss: 0.00001139
Iteration 5/1000 | Loss: 0.00001076
Iteration 6/1000 | Loss: 0.00001043
Iteration 7/1000 | Loss: 0.00001021
Iteration 8/1000 | Loss: 0.00001016
Iteration 9/1000 | Loss: 0.00001015
Iteration 10/1000 | Loss: 0.00000999
Iteration 11/1000 | Loss: 0.00000996
Iteration 12/1000 | Loss: 0.00000980
Iteration 13/1000 | Loss: 0.00000973
Iteration 14/1000 | Loss: 0.00000973
Iteration 15/1000 | Loss: 0.00000972
Iteration 16/1000 | Loss: 0.00000969
Iteration 17/1000 | Loss: 0.00000965
Iteration 18/1000 | Loss: 0.00000961
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000953
Iteration 21/1000 | Loss: 0.00000953
Iteration 22/1000 | Loss: 0.00000948
Iteration 23/1000 | Loss: 0.00000946
Iteration 24/1000 | Loss: 0.00000945
Iteration 25/1000 | Loss: 0.00000943
Iteration 26/1000 | Loss: 0.00000943
Iteration 27/1000 | Loss: 0.00000942
Iteration 28/1000 | Loss: 0.00000939
Iteration 29/1000 | Loss: 0.00000938
Iteration 30/1000 | Loss: 0.00000938
Iteration 31/1000 | Loss: 0.00000938
Iteration 32/1000 | Loss: 0.00000938
Iteration 33/1000 | Loss: 0.00000938
Iteration 34/1000 | Loss: 0.00000938
Iteration 35/1000 | Loss: 0.00000937
Iteration 36/1000 | Loss: 0.00000937
Iteration 37/1000 | Loss: 0.00000937
Iteration 38/1000 | Loss: 0.00000936
Iteration 39/1000 | Loss: 0.00000935
Iteration 40/1000 | Loss: 0.00000935
Iteration 41/1000 | Loss: 0.00000934
Iteration 42/1000 | Loss: 0.00000934
Iteration 43/1000 | Loss: 0.00000934
Iteration 44/1000 | Loss: 0.00000934
Iteration 45/1000 | Loss: 0.00000933
Iteration 46/1000 | Loss: 0.00000933
Iteration 47/1000 | Loss: 0.00000931
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000931
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000930
Iteration 52/1000 | Loss: 0.00000930
Iteration 53/1000 | Loss: 0.00000929
Iteration 54/1000 | Loss: 0.00000929
Iteration 55/1000 | Loss: 0.00000929
Iteration 56/1000 | Loss: 0.00000929
Iteration 57/1000 | Loss: 0.00000928
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000927
Iteration 60/1000 | Loss: 0.00000926
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000924
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00000923
Iteration 66/1000 | Loss: 0.00000923
Iteration 67/1000 | Loss: 0.00000922
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000921
Iteration 70/1000 | Loss: 0.00000921
Iteration 71/1000 | Loss: 0.00000920
Iteration 72/1000 | Loss: 0.00000920
Iteration 73/1000 | Loss: 0.00000920
Iteration 74/1000 | Loss: 0.00000920
Iteration 75/1000 | Loss: 0.00000920
Iteration 76/1000 | Loss: 0.00000920
Iteration 77/1000 | Loss: 0.00000920
Iteration 78/1000 | Loss: 0.00000920
Iteration 79/1000 | Loss: 0.00000919
Iteration 80/1000 | Loss: 0.00000919
Iteration 81/1000 | Loss: 0.00000918
Iteration 82/1000 | Loss: 0.00000918
Iteration 83/1000 | Loss: 0.00000918
Iteration 84/1000 | Loss: 0.00000918
Iteration 85/1000 | Loss: 0.00000918
Iteration 86/1000 | Loss: 0.00000917
Iteration 87/1000 | Loss: 0.00000917
Iteration 88/1000 | Loss: 0.00000917
Iteration 89/1000 | Loss: 0.00000917
Iteration 90/1000 | Loss: 0.00000917
Iteration 91/1000 | Loss: 0.00000916
Iteration 92/1000 | Loss: 0.00000916
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000916
Iteration 95/1000 | Loss: 0.00000915
Iteration 96/1000 | Loss: 0.00000915
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000915
Iteration 100/1000 | Loss: 0.00000915
Iteration 101/1000 | Loss: 0.00000915
Iteration 102/1000 | Loss: 0.00000915
Iteration 103/1000 | Loss: 0.00000915
Iteration 104/1000 | Loss: 0.00000914
Iteration 105/1000 | Loss: 0.00000914
Iteration 106/1000 | Loss: 0.00000914
Iteration 107/1000 | Loss: 0.00000914
Iteration 108/1000 | Loss: 0.00000914
Iteration 109/1000 | Loss: 0.00000914
Iteration 110/1000 | Loss: 0.00000914
Iteration 111/1000 | Loss: 0.00000914
Iteration 112/1000 | Loss: 0.00000914
Iteration 113/1000 | Loss: 0.00000914
Iteration 114/1000 | Loss: 0.00000914
Iteration 115/1000 | Loss: 0.00000914
Iteration 116/1000 | Loss: 0.00000914
Iteration 117/1000 | Loss: 0.00000913
Iteration 118/1000 | Loss: 0.00000913
Iteration 119/1000 | Loss: 0.00000913
Iteration 120/1000 | Loss: 0.00000912
Iteration 121/1000 | Loss: 0.00000912
Iteration 122/1000 | Loss: 0.00000911
Iteration 123/1000 | Loss: 0.00000911
Iteration 124/1000 | Loss: 0.00000911
Iteration 125/1000 | Loss: 0.00000911
Iteration 126/1000 | Loss: 0.00000911
Iteration 127/1000 | Loss: 0.00000911
Iteration 128/1000 | Loss: 0.00000911
Iteration 129/1000 | Loss: 0.00000911
Iteration 130/1000 | Loss: 0.00000911
Iteration 131/1000 | Loss: 0.00000911
Iteration 132/1000 | Loss: 0.00000911
Iteration 133/1000 | Loss: 0.00000911
Iteration 134/1000 | Loss: 0.00000911
Iteration 135/1000 | Loss: 0.00000911
Iteration 136/1000 | Loss: 0.00000911
Iteration 137/1000 | Loss: 0.00000911
Iteration 138/1000 | Loss: 0.00000911
Iteration 139/1000 | Loss: 0.00000911
Iteration 140/1000 | Loss: 0.00000911
Iteration 141/1000 | Loss: 0.00000910
Iteration 142/1000 | Loss: 0.00000910
Iteration 143/1000 | Loss: 0.00000910
Iteration 144/1000 | Loss: 0.00000910
Iteration 145/1000 | Loss: 0.00000910
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000910
Iteration 151/1000 | Loss: 0.00000910
Iteration 152/1000 | Loss: 0.00000910
Iteration 153/1000 | Loss: 0.00000910
Iteration 154/1000 | Loss: 0.00000910
Iteration 155/1000 | Loss: 0.00000910
Iteration 156/1000 | Loss: 0.00000910
Iteration 157/1000 | Loss: 0.00000910
Iteration 158/1000 | Loss: 0.00000910
Iteration 159/1000 | Loss: 0.00000910
Iteration 160/1000 | Loss: 0.00000910
Iteration 161/1000 | Loss: 0.00000910
Iteration 162/1000 | Loss: 0.00000910
Iteration 163/1000 | Loss: 0.00000910
Iteration 164/1000 | Loss: 0.00000910
Iteration 165/1000 | Loss: 0.00000910
Iteration 166/1000 | Loss: 0.00000910
Iteration 167/1000 | Loss: 0.00000910
Iteration 168/1000 | Loss: 0.00000910
Iteration 169/1000 | Loss: 0.00000910
Iteration 170/1000 | Loss: 0.00000910
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000910
Iteration 173/1000 | Loss: 0.00000910
Iteration 174/1000 | Loss: 0.00000910
Iteration 175/1000 | Loss: 0.00000910
Iteration 176/1000 | Loss: 0.00000910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [9.10401013243245e-06, 9.10401013243245e-06, 9.10401013243245e-06, 9.10401013243245e-06, 9.10401013243245e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.10401013243245e-06

Optimization complete. Final v2v error: 2.607137441635132 mm

Highest mean error: 2.923341989517212 mm for frame 38

Lowest mean error: 2.4734060764312744 mm for frame 11

Saving results

Total time: 889.4597940444946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1042
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048441
Iteration 2/25 | Loss: 0.00311378
Iteration 3/25 | Loss: 0.00241783
Iteration 4/25 | Loss: 0.00198207
Iteration 5/25 | Loss: 0.00173632
Iteration 6/25 | Loss: 0.00177446
Iteration 7/25 | Loss: 0.00141955
Iteration 8/25 | Loss: 0.00139521
Iteration 9/25 | Loss: 0.00131605
Iteration 10/25 | Loss: 0.00127549
Iteration 11/25 | Loss: 0.00123364
Iteration 12/25 | Loss: 0.00122189
Iteration 13/25 | Loss: 0.00122524
Iteration 14/25 | Loss: 0.00121412
Iteration 15/25 | Loss: 0.00125495
Iteration 16/25 | Loss: 0.00125641
Iteration 17/25 | Loss: 0.00120444
Iteration 18/25 | Loss: 0.00117930
Iteration 19/25 | Loss: 0.00117742
Iteration 20/25 | Loss: 0.00118035
Iteration 21/25 | Loss: 0.00117669
Iteration 22/25 | Loss: 0.00117502
Iteration 23/25 | Loss: 0.00117441
Iteration 24/25 | Loss: 0.00117421
Iteration 25/25 | Loss: 0.00117470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33920431
Iteration 2/25 | Loss: 0.00338914
Iteration 3/25 | Loss: 0.00134554
Iteration 4/25 | Loss: 0.00134554
Iteration 5/25 | Loss: 0.00134554
Iteration 6/25 | Loss: 0.00134554
Iteration 7/25 | Loss: 0.00134554
Iteration 8/25 | Loss: 0.00134554
Iteration 9/25 | Loss: 0.00134554
Iteration 10/25 | Loss: 0.00134554
Iteration 11/25 | Loss: 0.00134554
Iteration 12/25 | Loss: 0.00134554
Iteration 13/25 | Loss: 0.00134554
Iteration 14/25 | Loss: 0.00134554
Iteration 15/25 | Loss: 0.00134554
Iteration 16/25 | Loss: 0.00134554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013455389998853207, 0.0013455389998853207, 0.0013455389998853207, 0.0013455389998853207, 0.0013455389998853207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013455389998853207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134554
Iteration 2/1000 | Loss: 0.00119867
Iteration 3/1000 | Loss: 0.00026749
Iteration 4/1000 | Loss: 0.00008836
Iteration 5/1000 | Loss: 0.00019660
Iteration 6/1000 | Loss: 0.00007156
Iteration 7/1000 | Loss: 0.00006664
Iteration 8/1000 | Loss: 0.00006359
Iteration 9/1000 | Loss: 0.00006000
Iteration 10/1000 | Loss: 0.00057902
Iteration 11/1000 | Loss: 0.00593214
Iteration 12/1000 | Loss: 0.00227856
Iteration 13/1000 | Loss: 0.00010540
Iteration 14/1000 | Loss: 0.00005561
Iteration 15/1000 | Loss: 0.00004273
Iteration 16/1000 | Loss: 0.00003316
Iteration 17/1000 | Loss: 0.00002751
Iteration 18/1000 | Loss: 0.00002442
Iteration 19/1000 | Loss: 0.00002252
Iteration 20/1000 | Loss: 0.00002097
Iteration 21/1000 | Loss: 0.00001996
Iteration 22/1000 | Loss: 0.00001913
Iteration 23/1000 | Loss: 0.00001834
Iteration 24/1000 | Loss: 0.00001770
Iteration 25/1000 | Loss: 0.00001723
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001674
Iteration 28/1000 | Loss: 0.00001661
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001656
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001655
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001647
Iteration 44/1000 | Loss: 0.00001646
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001645
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001643
Iteration 56/1000 | Loss: 0.00001643
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001642
Iteration 60/1000 | Loss: 0.00001642
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001641
Iteration 64/1000 | Loss: 0.00001641
Iteration 65/1000 | Loss: 0.00001640
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001637
Iteration 83/1000 | Loss: 0.00001637
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001636
Iteration 87/1000 | Loss: 0.00001636
Iteration 88/1000 | Loss: 0.00001636
Iteration 89/1000 | Loss: 0.00001636
Iteration 90/1000 | Loss: 0.00001636
Iteration 91/1000 | Loss: 0.00001636
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001634
Iteration 102/1000 | Loss: 0.00001634
Iteration 103/1000 | Loss: 0.00001634
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001634
Iteration 109/1000 | Loss: 0.00001634
Iteration 110/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.6341866285074502e-05, 1.6341866285074502e-05, 1.6341866285074502e-05, 1.6341866285074502e-05, 1.6341866285074502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6341866285074502e-05

Optimization complete. Final v2v error: 3.424798011779785 mm

Highest mean error: 4.2491841316223145 mm for frame 48

Lowest mean error: 2.9755442142486572 mm for frame 23

Saving results

Total time: 2465.2520699501038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1044
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754575
Iteration 2/25 | Loss: 0.00168033
Iteration 3/25 | Loss: 0.00126329
Iteration 4/25 | Loss: 0.00118701
Iteration 5/25 | Loss: 0.00119638
Iteration 6/25 | Loss: 0.00120536
Iteration 7/25 | Loss: 0.00116410
Iteration 8/25 | Loss: 0.00114387
Iteration 9/25 | Loss: 0.00113545
Iteration 10/25 | Loss: 0.00113239
Iteration 11/25 | Loss: 0.00112339
Iteration 12/25 | Loss: 0.00111734
Iteration 13/25 | Loss: 0.00111409
Iteration 14/25 | Loss: 0.00111562
Iteration 15/25 | Loss: 0.00111326
Iteration 16/25 | Loss: 0.00111248
Iteration 17/25 | Loss: 0.00111110
Iteration 18/25 | Loss: 0.00111062
Iteration 19/25 | Loss: 0.00111057
Iteration 20/25 | Loss: 0.00111057
Iteration 21/25 | Loss: 0.00111057
Iteration 22/25 | Loss: 0.00111057
Iteration 23/25 | Loss: 0.00111057
Iteration 24/25 | Loss: 0.00111057
Iteration 25/25 | Loss: 0.00111056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34613693
Iteration 2/25 | Loss: 0.00085552
Iteration 3/25 | Loss: 0.00085551
Iteration 4/25 | Loss: 0.00085551
Iteration 5/25 | Loss: 0.00085551
Iteration 6/25 | Loss: 0.00085551
Iteration 7/25 | Loss: 0.00085551
Iteration 8/25 | Loss: 0.00085551
Iteration 9/25 | Loss: 0.00085551
Iteration 10/25 | Loss: 0.00085551
Iteration 11/25 | Loss: 0.00085551
Iteration 12/25 | Loss: 0.00085551
Iteration 13/25 | Loss: 0.00085551
Iteration 14/25 | Loss: 0.00085551
Iteration 15/25 | Loss: 0.00085551
Iteration 16/25 | Loss: 0.00085551
Iteration 17/25 | Loss: 0.00085551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008555091335438192, 0.0008555091335438192, 0.0008555091335438192, 0.0008555091335438192, 0.0008555091335438192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008555091335438192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085551
Iteration 2/1000 | Loss: 0.00006964
Iteration 3/1000 | Loss: 0.00004908
Iteration 4/1000 | Loss: 0.00004357
Iteration 5/1000 | Loss: 0.00004021
Iteration 6/1000 | Loss: 0.00003864
Iteration 7/1000 | Loss: 0.00003755
Iteration 8/1000 | Loss: 0.00003694
Iteration 9/1000 | Loss: 0.00003642
Iteration 10/1000 | Loss: 0.00003582
Iteration 11/1000 | Loss: 0.00003537
Iteration 12/1000 | Loss: 0.00003494
Iteration 13/1000 | Loss: 0.00003466
Iteration 14/1000 | Loss: 0.00003430
Iteration 15/1000 | Loss: 0.00035612
Iteration 16/1000 | Loss: 0.00003734
Iteration 17/1000 | Loss: 0.00003414
Iteration 18/1000 | Loss: 0.00087753
Iteration 19/1000 | Loss: 0.00042176
Iteration 20/1000 | Loss: 0.00029086
Iteration 21/1000 | Loss: 0.00005087
Iteration 22/1000 | Loss: 0.00043168
Iteration 23/1000 | Loss: 0.00005565
Iteration 24/1000 | Loss: 0.00003440
Iteration 25/1000 | Loss: 0.00003103
Iteration 26/1000 | Loss: 0.00002928
Iteration 27/1000 | Loss: 0.00002825
Iteration 28/1000 | Loss: 0.00002756
Iteration 29/1000 | Loss: 0.00002716
Iteration 30/1000 | Loss: 0.00002683
Iteration 31/1000 | Loss: 0.00002657
Iteration 32/1000 | Loss: 0.00002646
Iteration 33/1000 | Loss: 0.00002634
Iteration 34/1000 | Loss: 0.00002629
Iteration 35/1000 | Loss: 0.00002614
Iteration 36/1000 | Loss: 0.00002611
Iteration 37/1000 | Loss: 0.00002608
Iteration 38/1000 | Loss: 0.00002607
Iteration 39/1000 | Loss: 0.00002604
Iteration 40/1000 | Loss: 0.00002603
Iteration 41/1000 | Loss: 0.00002603
Iteration 42/1000 | Loss: 0.00002601
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00002601
Iteration 45/1000 | Loss: 0.00002600
Iteration 46/1000 | Loss: 0.00002596
Iteration 47/1000 | Loss: 0.00002593
Iteration 48/1000 | Loss: 0.00002593
Iteration 49/1000 | Loss: 0.00002592
Iteration 50/1000 | Loss: 0.00002592
Iteration 51/1000 | Loss: 0.00002592
Iteration 52/1000 | Loss: 0.00002592
Iteration 53/1000 | Loss: 0.00002591
Iteration 54/1000 | Loss: 0.00002591
Iteration 55/1000 | Loss: 0.00002590
Iteration 56/1000 | Loss: 0.00002589
Iteration 57/1000 | Loss: 0.00002589
Iteration 58/1000 | Loss: 0.00002588
Iteration 59/1000 | Loss: 0.00002588
Iteration 60/1000 | Loss: 0.00002587
Iteration 61/1000 | Loss: 0.00002587
Iteration 62/1000 | Loss: 0.00002587
Iteration 63/1000 | Loss: 0.00002586
Iteration 64/1000 | Loss: 0.00002586
Iteration 65/1000 | Loss: 0.00002586
Iteration 66/1000 | Loss: 0.00002586
Iteration 67/1000 | Loss: 0.00002585
Iteration 68/1000 | Loss: 0.00002585
Iteration 69/1000 | Loss: 0.00002584
Iteration 70/1000 | Loss: 0.00002583
Iteration 71/1000 | Loss: 0.00002583
Iteration 72/1000 | Loss: 0.00002582
Iteration 73/1000 | Loss: 0.00002581
Iteration 74/1000 | Loss: 0.00002581
Iteration 75/1000 | Loss: 0.00002577
Iteration 76/1000 | Loss: 0.00002574
Iteration 77/1000 | Loss: 0.00002574
Iteration 78/1000 | Loss: 0.00002573
Iteration 79/1000 | Loss: 0.00002573
Iteration 80/1000 | Loss: 0.00002572
Iteration 81/1000 | Loss: 0.00002572
Iteration 82/1000 | Loss: 0.00002572
Iteration 83/1000 | Loss: 0.00002572
Iteration 84/1000 | Loss: 0.00002571
Iteration 85/1000 | Loss: 0.00002571
Iteration 86/1000 | Loss: 0.00002571
Iteration 87/1000 | Loss: 0.00002571
Iteration 88/1000 | Loss: 0.00002571
Iteration 89/1000 | Loss: 0.00002571
Iteration 90/1000 | Loss: 0.00002570
Iteration 91/1000 | Loss: 0.00002570
Iteration 92/1000 | Loss: 0.00022766
Iteration 93/1000 | Loss: 0.00008683
Iteration 94/1000 | Loss: 0.00003016
Iteration 95/1000 | Loss: 0.00002723
Iteration 96/1000 | Loss: 0.00002598
Iteration 97/1000 | Loss: 0.00002543
Iteration 98/1000 | Loss: 0.00002497
Iteration 99/1000 | Loss: 0.00002471
Iteration 100/1000 | Loss: 0.00002457
Iteration 101/1000 | Loss: 0.00002452
Iteration 102/1000 | Loss: 0.00002449
Iteration 103/1000 | Loss: 0.00002447
Iteration 104/1000 | Loss: 0.00002447
Iteration 105/1000 | Loss: 0.00002446
Iteration 106/1000 | Loss: 0.00002445
Iteration 107/1000 | Loss: 0.00002445
Iteration 108/1000 | Loss: 0.00002444
Iteration 109/1000 | Loss: 0.00002443
Iteration 110/1000 | Loss: 0.00002442
Iteration 111/1000 | Loss: 0.00002442
Iteration 112/1000 | Loss: 0.00002442
Iteration 113/1000 | Loss: 0.00002438
Iteration 114/1000 | Loss: 0.00002436
Iteration 115/1000 | Loss: 0.00002435
Iteration 116/1000 | Loss: 0.00002435
Iteration 117/1000 | Loss: 0.00002434
Iteration 118/1000 | Loss: 0.00002434
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002434
Iteration 121/1000 | Loss: 0.00002433
Iteration 122/1000 | Loss: 0.00002433
Iteration 123/1000 | Loss: 0.00002433
Iteration 124/1000 | Loss: 0.00002433
Iteration 125/1000 | Loss: 0.00002432
Iteration 126/1000 | Loss: 0.00002432
Iteration 127/1000 | Loss: 0.00002432
Iteration 128/1000 | Loss: 0.00002432
Iteration 129/1000 | Loss: 0.00002432
Iteration 130/1000 | Loss: 0.00002432
Iteration 131/1000 | Loss: 0.00002432
Iteration 132/1000 | Loss: 0.00002431
Iteration 133/1000 | Loss: 0.00002431
Iteration 134/1000 | Loss: 0.00002431
Iteration 135/1000 | Loss: 0.00002431
Iteration 136/1000 | Loss: 0.00002431
Iteration 137/1000 | Loss: 0.00002431
Iteration 138/1000 | Loss: 0.00002431
Iteration 139/1000 | Loss: 0.00002431
Iteration 140/1000 | Loss: 0.00002430
Iteration 141/1000 | Loss: 0.00002430
Iteration 142/1000 | Loss: 0.00002430
Iteration 143/1000 | Loss: 0.00002430
Iteration 144/1000 | Loss: 0.00002430
Iteration 145/1000 | Loss: 0.00002430
Iteration 146/1000 | Loss: 0.00002430
Iteration 147/1000 | Loss: 0.00002430
Iteration 148/1000 | Loss: 0.00002430
Iteration 149/1000 | Loss: 0.00002430
Iteration 150/1000 | Loss: 0.00002430
Iteration 151/1000 | Loss: 0.00002430
Iteration 152/1000 | Loss: 0.00002430
Iteration 153/1000 | Loss: 0.00002430
Iteration 154/1000 | Loss: 0.00002430
Iteration 155/1000 | Loss: 0.00002429
Iteration 156/1000 | Loss: 0.00002429
Iteration 157/1000 | Loss: 0.00002429
Iteration 158/1000 | Loss: 0.00002429
Iteration 159/1000 | Loss: 0.00002429
Iteration 160/1000 | Loss: 0.00002429
Iteration 161/1000 | Loss: 0.00002428
Iteration 162/1000 | Loss: 0.00002428
Iteration 163/1000 | Loss: 0.00002428
Iteration 164/1000 | Loss: 0.00002428
Iteration 165/1000 | Loss: 0.00002428
Iteration 166/1000 | Loss: 0.00002427
Iteration 167/1000 | Loss: 0.00002427
Iteration 168/1000 | Loss: 0.00002427
Iteration 169/1000 | Loss: 0.00002427
Iteration 170/1000 | Loss: 0.00002427
Iteration 171/1000 | Loss: 0.00002427
Iteration 172/1000 | Loss: 0.00002427
Iteration 173/1000 | Loss: 0.00002427
Iteration 174/1000 | Loss: 0.00002427
Iteration 175/1000 | Loss: 0.00002427
Iteration 176/1000 | Loss: 0.00002427
Iteration 177/1000 | Loss: 0.00002427
Iteration 178/1000 | Loss: 0.00002427
Iteration 179/1000 | Loss: 0.00002427
Iteration 180/1000 | Loss: 0.00002426
Iteration 181/1000 | Loss: 0.00002426
Iteration 182/1000 | Loss: 0.00002426
Iteration 183/1000 | Loss: 0.00002426
Iteration 184/1000 | Loss: 0.00002426
Iteration 185/1000 | Loss: 0.00002426
Iteration 186/1000 | Loss: 0.00002426
Iteration 187/1000 | Loss: 0.00002426
Iteration 188/1000 | Loss: 0.00002426
Iteration 189/1000 | Loss: 0.00002426
Iteration 190/1000 | Loss: 0.00002426
Iteration 191/1000 | Loss: 0.00002426
Iteration 192/1000 | Loss: 0.00002426
Iteration 193/1000 | Loss: 0.00002426
Iteration 194/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.425882121315226e-05, 2.425882121315226e-05, 2.425882121315226e-05, 2.425882121315226e-05, 2.425882121315226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.425882121315226e-05

Optimization complete. Final v2v error: 3.5721163749694824 mm

Highest mean error: 11.184375762939453 mm for frame 20

Lowest mean error: 2.618427038192749 mm for frame 107

Saving results

Total time: 3479.850772380829
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1024
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060765
Iteration 2/25 | Loss: 0.01060765
Iteration 3/25 | Loss: 0.01060765
Iteration 4/25 | Loss: 0.01060765
Iteration 5/25 | Loss: 0.01060765
Iteration 6/25 | Loss: 0.01060765
Iteration 7/25 | Loss: 0.01060765
Iteration 8/25 | Loss: 0.01060765
Iteration 9/25 | Loss: 0.01060765
Iteration 10/25 | Loss: 0.01060765
Iteration 11/25 | Loss: 0.01060765
Iteration 12/25 | Loss: 0.01060765
Iteration 13/25 | Loss: 0.01060764
Iteration 14/25 | Loss: 0.01060764
Iteration 15/25 | Loss: 0.01060764
Iteration 16/25 | Loss: 0.01060764
Iteration 17/25 | Loss: 0.01060764
Iteration 18/25 | Loss: 0.01060764
Iteration 19/25 | Loss: 0.01060764
Iteration 20/25 | Loss: 0.01060764
Iteration 21/25 | Loss: 0.01060764
Iteration 22/25 | Loss: 0.01060764
Iteration 23/25 | Loss: 0.01060764
Iteration 24/25 | Loss: 0.01060764
Iteration 25/25 | Loss: 0.01060764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62823200
Iteration 2/25 | Loss: 0.08244555
Iteration 3/25 | Loss: 0.08243559
Iteration 4/25 | Loss: 0.08243557
Iteration 5/25 | Loss: 0.08243557
Iteration 6/25 | Loss: 0.08243557
Iteration 7/25 | Loss: 0.08243557
Iteration 8/25 | Loss: 0.08243557
Iteration 9/25 | Loss: 0.08243557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.08243557065725327, 0.08243557065725327, 0.08243557065725327, 0.08243557065725327, 0.08243557065725327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08243557065725327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08243556
Iteration 2/1000 | Loss: 0.00046305
Iteration 3/1000 | Loss: 0.00013835
Iteration 4/1000 | Loss: 0.00005640
Iteration 5/1000 | Loss: 0.00003141
Iteration 6/1000 | Loss: 0.00002479
Iteration 7/1000 | Loss: 0.00002190
Iteration 8/1000 | Loss: 0.00001972
Iteration 9/1000 | Loss: 0.00001845
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001472
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001331
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001188
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001072
Iteration 21/1000 | Loss: 0.00001045
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000960
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000952
Iteration 28/1000 | Loss: 0.00000946
Iteration 29/1000 | Loss: 0.00000942
Iteration 30/1000 | Loss: 0.00000939
Iteration 31/1000 | Loss: 0.00000938
Iteration 32/1000 | Loss: 0.00000931
Iteration 33/1000 | Loss: 0.00000929
Iteration 34/1000 | Loss: 0.00000926
Iteration 35/1000 | Loss: 0.00000924
Iteration 36/1000 | Loss: 0.00000924
Iteration 37/1000 | Loss: 0.00000924
Iteration 38/1000 | Loss: 0.00000923
Iteration 39/1000 | Loss: 0.00000923
Iteration 40/1000 | Loss: 0.00000923
Iteration 41/1000 | Loss: 0.00000923
Iteration 42/1000 | Loss: 0.00000923
Iteration 43/1000 | Loss: 0.00000923
Iteration 44/1000 | Loss: 0.00000923
Iteration 45/1000 | Loss: 0.00000922
Iteration 46/1000 | Loss: 0.00000922
Iteration 47/1000 | Loss: 0.00000921
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000919
Iteration 50/1000 | Loss: 0.00000919
Iteration 51/1000 | Loss: 0.00000918
Iteration 52/1000 | Loss: 0.00000918
Iteration 53/1000 | Loss: 0.00000917
Iteration 54/1000 | Loss: 0.00000917
Iteration 55/1000 | Loss: 0.00000917
Iteration 56/1000 | Loss: 0.00000917
Iteration 57/1000 | Loss: 0.00000917
Iteration 58/1000 | Loss: 0.00000916
Iteration 59/1000 | Loss: 0.00000916
Iteration 60/1000 | Loss: 0.00000915
Iteration 61/1000 | Loss: 0.00000915
Iteration 62/1000 | Loss: 0.00000914
Iteration 63/1000 | Loss: 0.00000914
Iteration 64/1000 | Loss: 0.00000914
Iteration 65/1000 | Loss: 0.00000914
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000913
Iteration 68/1000 | Loss: 0.00000913
Iteration 69/1000 | Loss: 0.00000913
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000913
Iteration 72/1000 | Loss: 0.00000912
Iteration 73/1000 | Loss: 0.00000912
Iteration 74/1000 | Loss: 0.00000912
Iteration 75/1000 | Loss: 0.00000911
Iteration 76/1000 | Loss: 0.00000911
Iteration 77/1000 | Loss: 0.00000911
Iteration 78/1000 | Loss: 0.00000910
Iteration 79/1000 | Loss: 0.00000910
Iteration 80/1000 | Loss: 0.00000910
Iteration 81/1000 | Loss: 0.00000909
Iteration 82/1000 | Loss: 0.00000909
Iteration 83/1000 | Loss: 0.00000909
Iteration 84/1000 | Loss: 0.00000909
Iteration 85/1000 | Loss: 0.00000909
Iteration 86/1000 | Loss: 0.00000909
Iteration 87/1000 | Loss: 0.00000909
Iteration 88/1000 | Loss: 0.00000909
Iteration 89/1000 | Loss: 0.00000908
Iteration 90/1000 | Loss: 0.00000908
Iteration 91/1000 | Loss: 0.00000908
Iteration 92/1000 | Loss: 0.00000907
Iteration 93/1000 | Loss: 0.00000907
Iteration 94/1000 | Loss: 0.00000907
Iteration 95/1000 | Loss: 0.00000906
Iteration 96/1000 | Loss: 0.00000906
Iteration 97/1000 | Loss: 0.00000906
Iteration 98/1000 | Loss: 0.00000905
Iteration 99/1000 | Loss: 0.00000905
Iteration 100/1000 | Loss: 0.00000905
Iteration 101/1000 | Loss: 0.00000905
Iteration 102/1000 | Loss: 0.00000905
Iteration 103/1000 | Loss: 0.00000905
Iteration 104/1000 | Loss: 0.00000905
Iteration 105/1000 | Loss: 0.00000905
Iteration 106/1000 | Loss: 0.00000905
Iteration 107/1000 | Loss: 0.00000905
Iteration 108/1000 | Loss: 0.00000905
Iteration 109/1000 | Loss: 0.00000905
Iteration 110/1000 | Loss: 0.00000905
Iteration 111/1000 | Loss: 0.00000905
Iteration 112/1000 | Loss: 0.00000905
Iteration 113/1000 | Loss: 0.00000905
Iteration 114/1000 | Loss: 0.00000905
Iteration 115/1000 | Loss: 0.00000905
Iteration 116/1000 | Loss: 0.00000905
Iteration 117/1000 | Loss: 0.00000905
Iteration 118/1000 | Loss: 0.00000905
Iteration 119/1000 | Loss: 0.00000905
Iteration 120/1000 | Loss: 0.00000905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [9.051625056599732e-06, 9.051625056599732e-06, 9.051625056599732e-06, 9.051625056599732e-06, 9.051625056599732e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.051625056599732e-06

Optimization complete. Final v2v error: 2.584329605102539 mm

Highest mean error: 2.7681376934051514 mm for frame 158

Lowest mean error: 2.41446852684021 mm for frame 191

Saving results

Total time: 1220.4459450244904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1028
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908590
Iteration 2/25 | Loss: 0.00132811
Iteration 3/25 | Loss: 0.00119062
Iteration 4/25 | Loss: 0.00117490
Iteration 5/25 | Loss: 0.00117077
Iteration 6/25 | Loss: 0.00117052
Iteration 7/25 | Loss: 0.00117052
Iteration 8/25 | Loss: 0.00117052
Iteration 9/25 | Loss: 0.00117052
Iteration 10/25 | Loss: 0.00117052
Iteration 11/25 | Loss: 0.00117052
Iteration 12/25 | Loss: 0.00117052
Iteration 13/25 | Loss: 0.00117052
Iteration 14/25 | Loss: 0.00117052
Iteration 15/25 | Loss: 0.00117052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011705232318490744, 0.0011705232318490744, 0.0011705232318490744, 0.0011705232318490744, 0.0011705232318490744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011705232318490744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31157041
Iteration 2/25 | Loss: 0.00072391
Iteration 3/25 | Loss: 0.00072391
Iteration 4/25 | Loss: 0.00072391
Iteration 5/25 | Loss: 0.00072391
Iteration 6/25 | Loss: 0.00072391
Iteration 7/25 | Loss: 0.00072391
Iteration 8/25 | Loss: 0.00072391
Iteration 9/25 | Loss: 0.00072391
Iteration 10/25 | Loss: 0.00072391
Iteration 11/25 | Loss: 0.00072391
Iteration 12/25 | Loss: 0.00072391
Iteration 13/25 | Loss: 0.00072391
Iteration 14/25 | Loss: 0.00072391
Iteration 15/25 | Loss: 0.00072391
Iteration 16/25 | Loss: 0.00072391
Iteration 17/25 | Loss: 0.00072391
Iteration 18/25 | Loss: 0.00072391
Iteration 19/25 | Loss: 0.00072391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007239116239361465, 0.0007239116239361465, 0.0007239116239361465, 0.0007239116239361465, 0.0007239116239361465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007239116239361465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072391
Iteration 2/1000 | Loss: 0.00004062
Iteration 3/1000 | Loss: 0.00003253
Iteration 4/1000 | Loss: 0.00003019
Iteration 5/1000 | Loss: 0.00002918
Iteration 6/1000 | Loss: 0.00002799
Iteration 7/1000 | Loss: 0.00002759
Iteration 8/1000 | Loss: 0.00002719
Iteration 9/1000 | Loss: 0.00002682
Iteration 10/1000 | Loss: 0.00002649
Iteration 11/1000 | Loss: 0.00002625
Iteration 12/1000 | Loss: 0.00002604
Iteration 13/1000 | Loss: 0.00002598
Iteration 14/1000 | Loss: 0.00002590
Iteration 15/1000 | Loss: 0.00002573
Iteration 16/1000 | Loss: 0.00002571
Iteration 17/1000 | Loss: 0.00002571
Iteration 18/1000 | Loss: 0.00002561
Iteration 19/1000 | Loss: 0.00002557
Iteration 20/1000 | Loss: 0.00002557
Iteration 21/1000 | Loss: 0.00002557
Iteration 22/1000 | Loss: 0.00002555
Iteration 23/1000 | Loss: 0.00002553
Iteration 24/1000 | Loss: 0.00002551
Iteration 25/1000 | Loss: 0.00002551
Iteration 26/1000 | Loss: 0.00002551
Iteration 27/1000 | Loss: 0.00002551
Iteration 28/1000 | Loss: 0.00002551
Iteration 29/1000 | Loss: 0.00002551
Iteration 30/1000 | Loss: 0.00002551
Iteration 31/1000 | Loss: 0.00002551
Iteration 32/1000 | Loss: 0.00002551
Iteration 33/1000 | Loss: 0.00002551
Iteration 34/1000 | Loss: 0.00002550
Iteration 35/1000 | Loss: 0.00002550
Iteration 36/1000 | Loss: 0.00002549
Iteration 37/1000 | Loss: 0.00002549
Iteration 38/1000 | Loss: 0.00002548
Iteration 39/1000 | Loss: 0.00002548
Iteration 40/1000 | Loss: 0.00002546
Iteration 41/1000 | Loss: 0.00002546
Iteration 42/1000 | Loss: 0.00002546
Iteration 43/1000 | Loss: 0.00002546
Iteration 44/1000 | Loss: 0.00002546
Iteration 45/1000 | Loss: 0.00002546
Iteration 46/1000 | Loss: 0.00002546
Iteration 47/1000 | Loss: 0.00002546
Iteration 48/1000 | Loss: 0.00002546
Iteration 49/1000 | Loss: 0.00002546
Iteration 50/1000 | Loss: 0.00002546
Iteration 51/1000 | Loss: 0.00002545
Iteration 52/1000 | Loss: 0.00002545
Iteration 53/1000 | Loss: 0.00002545
Iteration 54/1000 | Loss: 0.00002545
Iteration 55/1000 | Loss: 0.00002545
Iteration 56/1000 | Loss: 0.00002543
Iteration 57/1000 | Loss: 0.00002542
Iteration 58/1000 | Loss: 0.00002542
Iteration 59/1000 | Loss: 0.00002542
Iteration 60/1000 | Loss: 0.00002542
Iteration 61/1000 | Loss: 0.00002542
Iteration 62/1000 | Loss: 0.00002542
Iteration 63/1000 | Loss: 0.00002541
Iteration 64/1000 | Loss: 0.00002541
Iteration 65/1000 | Loss: 0.00002541
Iteration 66/1000 | Loss: 0.00002541
Iteration 67/1000 | Loss: 0.00002541
Iteration 68/1000 | Loss: 0.00002541
Iteration 69/1000 | Loss: 0.00002541
Iteration 70/1000 | Loss: 0.00002540
Iteration 71/1000 | Loss: 0.00002540
Iteration 72/1000 | Loss: 0.00002540
Iteration 73/1000 | Loss: 0.00002540
Iteration 74/1000 | Loss: 0.00002539
Iteration 75/1000 | Loss: 0.00002538
Iteration 76/1000 | Loss: 0.00002538
Iteration 77/1000 | Loss: 0.00002538
Iteration 78/1000 | Loss: 0.00002538
Iteration 79/1000 | Loss: 0.00002538
Iteration 80/1000 | Loss: 0.00002538
Iteration 81/1000 | Loss: 0.00002537
Iteration 82/1000 | Loss: 0.00002536
Iteration 83/1000 | Loss: 0.00002535
Iteration 84/1000 | Loss: 0.00002534
Iteration 85/1000 | Loss: 0.00002534
Iteration 86/1000 | Loss: 0.00002534
Iteration 87/1000 | Loss: 0.00002534
Iteration 88/1000 | Loss: 0.00002533
Iteration 89/1000 | Loss: 0.00002533
Iteration 90/1000 | Loss: 0.00002533
Iteration 91/1000 | Loss: 0.00002533
Iteration 92/1000 | Loss: 0.00002533
Iteration 93/1000 | Loss: 0.00002532
Iteration 94/1000 | Loss: 0.00002532
Iteration 95/1000 | Loss: 0.00002532
Iteration 96/1000 | Loss: 0.00002532
Iteration 97/1000 | Loss: 0.00002531
Iteration 98/1000 | Loss: 0.00002531
Iteration 99/1000 | Loss: 0.00002530
Iteration 100/1000 | Loss: 0.00002530
Iteration 101/1000 | Loss: 0.00002529
Iteration 102/1000 | Loss: 0.00002528
Iteration 103/1000 | Loss: 0.00002528
Iteration 104/1000 | Loss: 0.00002528
Iteration 105/1000 | Loss: 0.00002527
Iteration 106/1000 | Loss: 0.00002527
Iteration 107/1000 | Loss: 0.00002527
Iteration 108/1000 | Loss: 0.00002526
Iteration 109/1000 | Loss: 0.00002526
Iteration 110/1000 | Loss: 0.00002526
Iteration 111/1000 | Loss: 0.00002526
Iteration 112/1000 | Loss: 0.00002526
Iteration 113/1000 | Loss: 0.00002525
Iteration 114/1000 | Loss: 0.00002525
Iteration 115/1000 | Loss: 0.00002525
Iteration 116/1000 | Loss: 0.00002525
Iteration 117/1000 | Loss: 0.00002525
Iteration 118/1000 | Loss: 0.00002525
Iteration 119/1000 | Loss: 0.00002525
Iteration 120/1000 | Loss: 0.00002525
Iteration 121/1000 | Loss: 0.00002525
Iteration 122/1000 | Loss: 0.00002525
Iteration 123/1000 | Loss: 0.00002525
Iteration 124/1000 | Loss: 0.00002525
Iteration 125/1000 | Loss: 0.00002525
Iteration 126/1000 | Loss: 0.00002525
Iteration 127/1000 | Loss: 0.00002525
Iteration 128/1000 | Loss: 0.00002524
Iteration 129/1000 | Loss: 0.00002524
Iteration 130/1000 | Loss: 0.00002524
Iteration 131/1000 | Loss: 0.00002524
Iteration 132/1000 | Loss: 0.00002524
Iteration 133/1000 | Loss: 0.00002524
Iteration 134/1000 | Loss: 0.00002524
Iteration 135/1000 | Loss: 0.00002524
Iteration 136/1000 | Loss: 0.00002524
Iteration 137/1000 | Loss: 0.00002524
Iteration 138/1000 | Loss: 0.00002523
Iteration 139/1000 | Loss: 0.00002523
Iteration 140/1000 | Loss: 0.00002523
Iteration 141/1000 | Loss: 0.00002523
Iteration 142/1000 | Loss: 0.00002523
Iteration 143/1000 | Loss: 0.00002523
Iteration 144/1000 | Loss: 0.00002523
Iteration 145/1000 | Loss: 0.00002523
Iteration 146/1000 | Loss: 0.00002523
Iteration 147/1000 | Loss: 0.00002523
Iteration 148/1000 | Loss: 0.00002522
Iteration 149/1000 | Loss: 0.00002522
Iteration 150/1000 | Loss: 0.00002522
Iteration 151/1000 | Loss: 0.00002522
Iteration 152/1000 | Loss: 0.00002522
Iteration 153/1000 | Loss: 0.00002522
Iteration 154/1000 | Loss: 0.00002522
Iteration 155/1000 | Loss: 0.00002522
Iteration 156/1000 | Loss: 0.00002521
Iteration 157/1000 | Loss: 0.00002521
Iteration 158/1000 | Loss: 0.00002521
Iteration 159/1000 | Loss: 0.00002521
Iteration 160/1000 | Loss: 0.00002521
Iteration 161/1000 | Loss: 0.00002521
Iteration 162/1000 | Loss: 0.00002521
Iteration 163/1000 | Loss: 0.00002521
Iteration 164/1000 | Loss: 0.00002521
Iteration 165/1000 | Loss: 0.00002521
Iteration 166/1000 | Loss: 0.00002521
Iteration 167/1000 | Loss: 0.00002521
Iteration 168/1000 | Loss: 0.00002521
Iteration 169/1000 | Loss: 0.00002521
Iteration 170/1000 | Loss: 0.00002521
Iteration 171/1000 | Loss: 0.00002521
Iteration 172/1000 | Loss: 0.00002521
Iteration 173/1000 | Loss: 0.00002521
Iteration 174/1000 | Loss: 0.00002521
Iteration 175/1000 | Loss: 0.00002521
Iteration 176/1000 | Loss: 0.00002521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.5207431463059038e-05, 2.5207431463059038e-05, 2.5207431463059038e-05, 2.5207431463059038e-05, 2.5207431463059038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5207431463059038e-05

Optimization complete. Final v2v error: 4.120146751403809 mm

Highest mean error: 4.678308010101318 mm for frame 0

Lowest mean error: 3.7184102535247803 mm for frame 55

Saving results

Total time: 1274.1599969863892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1059
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770225
Iteration 2/25 | Loss: 0.00138928
Iteration 3/25 | Loss: 0.00110699
Iteration 4/25 | Loss: 0.00108913
Iteration 5/25 | Loss: 0.00108532
Iteration 6/25 | Loss: 0.00108511
Iteration 7/25 | Loss: 0.00108511
Iteration 8/25 | Loss: 0.00108511
Iteration 9/25 | Loss: 0.00108511
Iteration 10/25 | Loss: 0.00108511
Iteration 11/25 | Loss: 0.00108511
Iteration 12/25 | Loss: 0.00108511
Iteration 13/25 | Loss: 0.00108511
Iteration 14/25 | Loss: 0.00108511
Iteration 15/25 | Loss: 0.00108511
Iteration 16/25 | Loss: 0.00108511
Iteration 17/25 | Loss: 0.00108511
Iteration 18/25 | Loss: 0.00108511
Iteration 19/25 | Loss: 0.00108511
Iteration 20/25 | Loss: 0.00108511
Iteration 21/25 | Loss: 0.00108511
Iteration 22/25 | Loss: 0.00108511
Iteration 23/25 | Loss: 0.00108511
Iteration 24/25 | Loss: 0.00108511
Iteration 25/25 | Loss: 0.00108511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34561920
Iteration 2/25 | Loss: 0.00055773
Iteration 3/25 | Loss: 0.00055769
Iteration 4/25 | Loss: 0.00055769
Iteration 5/25 | Loss: 0.00055769
Iteration 6/25 | Loss: 0.00055769
Iteration 7/25 | Loss: 0.00055769
Iteration 8/25 | Loss: 0.00055769
Iteration 9/25 | Loss: 0.00055769
Iteration 10/25 | Loss: 0.00055769
Iteration 11/25 | Loss: 0.00055769
Iteration 12/25 | Loss: 0.00055769
Iteration 13/25 | Loss: 0.00055769
Iteration 14/25 | Loss: 0.00055769
Iteration 15/25 | Loss: 0.00055769
Iteration 16/25 | Loss: 0.00055769
Iteration 17/25 | Loss: 0.00055769
Iteration 18/25 | Loss: 0.00055769
Iteration 19/25 | Loss: 0.00055769
Iteration 20/25 | Loss: 0.00055769
Iteration 21/25 | Loss: 0.00055769
Iteration 22/25 | Loss: 0.00055769
Iteration 23/25 | Loss: 0.00055769
Iteration 24/25 | Loss: 0.00055769
Iteration 25/25 | Loss: 0.00055769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055769
Iteration 2/1000 | Loss: 0.00003031
Iteration 3/1000 | Loss: 0.00002106
Iteration 4/1000 | Loss: 0.00001722
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001508
Iteration 7/1000 | Loss: 0.00001450
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001366
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001240
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001238
Iteration 24/1000 | Loss: 0.00001238
Iteration 25/1000 | Loss: 0.00001237
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001237
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001235
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001233
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001230
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001229
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001226
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001221
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001221
Iteration 82/1000 | Loss: 0.00001221
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.2209890883241314e-05, 1.2209890883241314e-05, 1.2209890883241314e-05, 1.2209890883241314e-05, 1.2209890883241314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2209890883241314e-05

Optimization complete. Final v2v error: 2.96791934967041 mm

Highest mean error: 3.590569257736206 mm for frame 21

Lowest mean error: 2.55770206451416 mm for frame 74

Saving results

Total time: 1123.17236661911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1079
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928777
Iteration 2/25 | Loss: 0.00182180
Iteration 3/25 | Loss: 0.00139236
Iteration 4/25 | Loss: 0.00134341
Iteration 5/25 | Loss: 0.00133731
Iteration 6/25 | Loss: 0.00133605
Iteration 7/25 | Loss: 0.00133605
Iteration 8/25 | Loss: 0.00133605
Iteration 9/25 | Loss: 0.00133605
Iteration 10/25 | Loss: 0.00133605
Iteration 11/25 | Loss: 0.00133605
Iteration 12/25 | Loss: 0.00133605
Iteration 13/25 | Loss: 0.00133605
Iteration 14/25 | Loss: 0.00133605
Iteration 15/25 | Loss: 0.00133605
Iteration 16/25 | Loss: 0.00133605
Iteration 17/25 | Loss: 0.00133605
Iteration 18/25 | Loss: 0.00133605
Iteration 19/25 | Loss: 0.00133605
Iteration 20/25 | Loss: 0.00133605
Iteration 21/25 | Loss: 0.00133605
Iteration 22/25 | Loss: 0.00133605
Iteration 23/25 | Loss: 0.00133605
Iteration 24/25 | Loss: 0.00133605
Iteration 25/25 | Loss: 0.00133605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76732349
Iteration 2/25 | Loss: 0.00062669
Iteration 3/25 | Loss: 0.00062668
Iteration 4/25 | Loss: 0.00062668
Iteration 5/25 | Loss: 0.00062668
Iteration 6/25 | Loss: 0.00062668
Iteration 7/25 | Loss: 0.00062668
Iteration 8/25 | Loss: 0.00062668
Iteration 9/25 | Loss: 0.00062668
Iteration 10/25 | Loss: 0.00062668
Iteration 11/25 | Loss: 0.00062668
Iteration 12/25 | Loss: 0.00062668
Iteration 13/25 | Loss: 0.00062668
Iteration 14/25 | Loss: 0.00062668
Iteration 15/25 | Loss: 0.00062668
Iteration 16/25 | Loss: 0.00062668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006266813143156469, 0.0006266813143156469, 0.0006266813143156469, 0.0006266813143156469, 0.0006266813143156469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006266813143156469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062668
Iteration 2/1000 | Loss: 0.00005599
Iteration 3/1000 | Loss: 0.00004055
Iteration 4/1000 | Loss: 0.00003650
Iteration 5/1000 | Loss: 0.00003528
Iteration 6/1000 | Loss: 0.00003451
Iteration 7/1000 | Loss: 0.00003406
Iteration 8/1000 | Loss: 0.00003369
Iteration 9/1000 | Loss: 0.00003344
Iteration 10/1000 | Loss: 0.00003323
Iteration 11/1000 | Loss: 0.00003319
Iteration 12/1000 | Loss: 0.00003305
Iteration 13/1000 | Loss: 0.00003289
Iteration 14/1000 | Loss: 0.00003277
Iteration 15/1000 | Loss: 0.00003268
Iteration 16/1000 | Loss: 0.00003265
Iteration 17/1000 | Loss: 0.00003260
Iteration 18/1000 | Loss: 0.00003256
Iteration 19/1000 | Loss: 0.00003256
Iteration 20/1000 | Loss: 0.00003256
Iteration 21/1000 | Loss: 0.00003256
Iteration 22/1000 | Loss: 0.00003256
Iteration 23/1000 | Loss: 0.00003256
Iteration 24/1000 | Loss: 0.00003256
Iteration 25/1000 | Loss: 0.00003255
Iteration 26/1000 | Loss: 0.00003255
Iteration 27/1000 | Loss: 0.00003255
Iteration 28/1000 | Loss: 0.00003255
Iteration 29/1000 | Loss: 0.00003255
Iteration 30/1000 | Loss: 0.00003254
Iteration 31/1000 | Loss: 0.00003254
Iteration 32/1000 | Loss: 0.00003253
Iteration 33/1000 | Loss: 0.00003242
Iteration 34/1000 | Loss: 0.00003241
Iteration 35/1000 | Loss: 0.00003239
Iteration 36/1000 | Loss: 0.00003238
Iteration 37/1000 | Loss: 0.00003238
Iteration 38/1000 | Loss: 0.00003237
Iteration 39/1000 | Loss: 0.00003237
Iteration 40/1000 | Loss: 0.00003233
Iteration 41/1000 | Loss: 0.00003233
Iteration 42/1000 | Loss: 0.00003233
Iteration 43/1000 | Loss: 0.00003233
Iteration 44/1000 | Loss: 0.00003233
Iteration 45/1000 | Loss: 0.00003232
Iteration 46/1000 | Loss: 0.00003232
Iteration 47/1000 | Loss: 0.00003231
Iteration 48/1000 | Loss: 0.00003230
Iteration 49/1000 | Loss: 0.00003230
Iteration 50/1000 | Loss: 0.00003230
Iteration 51/1000 | Loss: 0.00003230
Iteration 52/1000 | Loss: 0.00003230
Iteration 53/1000 | Loss: 0.00003230
Iteration 54/1000 | Loss: 0.00003230
Iteration 55/1000 | Loss: 0.00003230
Iteration 56/1000 | Loss: 0.00003229
Iteration 57/1000 | Loss: 0.00003229
Iteration 58/1000 | Loss: 0.00003229
Iteration 59/1000 | Loss: 0.00003229
Iteration 60/1000 | Loss: 0.00003229
Iteration 61/1000 | Loss: 0.00003228
Iteration 62/1000 | Loss: 0.00003228
Iteration 63/1000 | Loss: 0.00003228
Iteration 64/1000 | Loss: 0.00003228
Iteration 65/1000 | Loss: 0.00003227
Iteration 66/1000 | Loss: 0.00003227
Iteration 67/1000 | Loss: 0.00003227
Iteration 68/1000 | Loss: 0.00003227
Iteration 69/1000 | Loss: 0.00003227
Iteration 70/1000 | Loss: 0.00003226
Iteration 71/1000 | Loss: 0.00003226
Iteration 72/1000 | Loss: 0.00003226
Iteration 73/1000 | Loss: 0.00003226
Iteration 74/1000 | Loss: 0.00003226
Iteration 75/1000 | Loss: 0.00003226
Iteration 76/1000 | Loss: 0.00003226
Iteration 77/1000 | Loss: 0.00003226
Iteration 78/1000 | Loss: 0.00003225
Iteration 79/1000 | Loss: 0.00003225
Iteration 80/1000 | Loss: 0.00003224
Iteration 81/1000 | Loss: 0.00003224
Iteration 82/1000 | Loss: 0.00003224
Iteration 83/1000 | Loss: 0.00003224
Iteration 84/1000 | Loss: 0.00003224
Iteration 85/1000 | Loss: 0.00003224
Iteration 86/1000 | Loss: 0.00003224
Iteration 87/1000 | Loss: 0.00003224
Iteration 88/1000 | Loss: 0.00003223
Iteration 89/1000 | Loss: 0.00003223
Iteration 90/1000 | Loss: 0.00003223
Iteration 91/1000 | Loss: 0.00003223
Iteration 92/1000 | Loss: 0.00003223
Iteration 93/1000 | Loss: 0.00003223
Iteration 94/1000 | Loss: 0.00003222
Iteration 95/1000 | Loss: 0.00003222
Iteration 96/1000 | Loss: 0.00003222
Iteration 97/1000 | Loss: 0.00003222
Iteration 98/1000 | Loss: 0.00003222
Iteration 99/1000 | Loss: 0.00003222
Iteration 100/1000 | Loss: 0.00003222
Iteration 101/1000 | Loss: 0.00003222
Iteration 102/1000 | Loss: 0.00003222
Iteration 103/1000 | Loss: 0.00003222
Iteration 104/1000 | Loss: 0.00003221
Iteration 105/1000 | Loss: 0.00003220
Iteration 106/1000 | Loss: 0.00003220
Iteration 107/1000 | Loss: 0.00003220
Iteration 108/1000 | Loss: 0.00003220
Iteration 109/1000 | Loss: 0.00003220
Iteration 110/1000 | Loss: 0.00003220
Iteration 111/1000 | Loss: 0.00003220
Iteration 112/1000 | Loss: 0.00003219
Iteration 113/1000 | Loss: 0.00003219
Iteration 114/1000 | Loss: 0.00003219
Iteration 115/1000 | Loss: 0.00003219
Iteration 116/1000 | Loss: 0.00003219
Iteration 117/1000 | Loss: 0.00003219
Iteration 118/1000 | Loss: 0.00003219
Iteration 119/1000 | Loss: 0.00003219
Iteration 120/1000 | Loss: 0.00003219
Iteration 121/1000 | Loss: 0.00003219
Iteration 122/1000 | Loss: 0.00003218
Iteration 123/1000 | Loss: 0.00003218
Iteration 124/1000 | Loss: 0.00003218
Iteration 125/1000 | Loss: 0.00003218
Iteration 126/1000 | Loss: 0.00003218
Iteration 127/1000 | Loss: 0.00003218
Iteration 128/1000 | Loss: 0.00003218
Iteration 129/1000 | Loss: 0.00003218
Iteration 130/1000 | Loss: 0.00003218
Iteration 131/1000 | Loss: 0.00003218
Iteration 132/1000 | Loss: 0.00003218
Iteration 133/1000 | Loss: 0.00003218
Iteration 134/1000 | Loss: 0.00003218
Iteration 135/1000 | Loss: 0.00003218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [3.217999983462505e-05, 3.217999983462505e-05, 3.217999983462505e-05, 3.217999983462505e-05, 3.217999983462505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.217999983462505e-05

Optimization complete. Final v2v error: 4.624082565307617 mm

Highest mean error: 4.898468971252441 mm for frame 35

Lowest mean error: 4.248569965362549 mm for frame 0

Saving results

Total time: 868.498663187027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1012
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406065
Iteration 2/25 | Loss: 0.00134799
Iteration 3/25 | Loss: 0.00109386
Iteration 4/25 | Loss: 0.00107531
Iteration 5/25 | Loss: 0.00107247
Iteration 6/25 | Loss: 0.00107196
Iteration 7/25 | Loss: 0.00107196
Iteration 8/25 | Loss: 0.00107196
Iteration 9/25 | Loss: 0.00107196
Iteration 10/25 | Loss: 0.00107196
Iteration 11/25 | Loss: 0.00107196
Iteration 12/25 | Loss: 0.00107196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010719603160396218, 0.0010719603160396218, 0.0010719603160396218, 0.0010719603160396218, 0.0010719603160396218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010719603160396218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38092840
Iteration 2/25 | Loss: 0.00070147
Iteration 3/25 | Loss: 0.00070147
Iteration 4/25 | Loss: 0.00070147
Iteration 5/25 | Loss: 0.00070147
Iteration 6/25 | Loss: 0.00070147
Iteration 7/25 | Loss: 0.00070147
Iteration 8/25 | Loss: 0.00070147
Iteration 9/25 | Loss: 0.00070147
Iteration 10/25 | Loss: 0.00070147
Iteration 11/25 | Loss: 0.00070147
Iteration 12/25 | Loss: 0.00070147
Iteration 13/25 | Loss: 0.00070147
Iteration 14/25 | Loss: 0.00070147
Iteration 15/25 | Loss: 0.00070147
Iteration 16/25 | Loss: 0.00070147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007014719885773957, 0.0007014719885773957, 0.0007014719885773957, 0.0007014719885773957, 0.0007014719885773957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007014719885773957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070147
Iteration 2/1000 | Loss: 0.00003148
Iteration 3/1000 | Loss: 0.00001778
Iteration 4/1000 | Loss: 0.00001410
Iteration 5/1000 | Loss: 0.00001286
Iteration 6/1000 | Loss: 0.00001204
Iteration 7/1000 | Loss: 0.00001150
Iteration 8/1000 | Loss: 0.00001103
Iteration 9/1000 | Loss: 0.00001079
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001037
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001025
Iteration 17/1000 | Loss: 0.00001024
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001023
Iteration 20/1000 | Loss: 0.00001022
Iteration 21/1000 | Loss: 0.00001018
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001017
Iteration 24/1000 | Loss: 0.00001016
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001011
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001005
Iteration 32/1000 | Loss: 0.00001005
Iteration 33/1000 | Loss: 0.00001005
Iteration 34/1000 | Loss: 0.00001005
Iteration 35/1000 | Loss: 0.00001005
Iteration 36/1000 | Loss: 0.00001005
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001005
Iteration 39/1000 | Loss: 0.00001005
Iteration 40/1000 | Loss: 0.00001005
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00001004
Iteration 43/1000 | Loss: 0.00001004
Iteration 44/1000 | Loss: 0.00001004
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001003
Iteration 49/1000 | Loss: 0.00001003
Iteration 50/1000 | Loss: 0.00001001
Iteration 51/1000 | Loss: 0.00001001
Iteration 52/1000 | Loss: 0.00001001
Iteration 53/1000 | Loss: 0.00001001
Iteration 54/1000 | Loss: 0.00001000
Iteration 55/1000 | Loss: 0.00001000
Iteration 56/1000 | Loss: 0.00001000
Iteration 57/1000 | Loss: 0.00000999
Iteration 58/1000 | Loss: 0.00000999
Iteration 59/1000 | Loss: 0.00000999
Iteration 60/1000 | Loss: 0.00000998
Iteration 61/1000 | Loss: 0.00000998
Iteration 62/1000 | Loss: 0.00000998
Iteration 63/1000 | Loss: 0.00000998
Iteration 64/1000 | Loss: 0.00000997
Iteration 65/1000 | Loss: 0.00000997
Iteration 66/1000 | Loss: 0.00000997
Iteration 67/1000 | Loss: 0.00000997
Iteration 68/1000 | Loss: 0.00000997
Iteration 69/1000 | Loss: 0.00000997
Iteration 70/1000 | Loss: 0.00000997
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000996
Iteration 74/1000 | Loss: 0.00000996
Iteration 75/1000 | Loss: 0.00000995
Iteration 76/1000 | Loss: 0.00000995
Iteration 77/1000 | Loss: 0.00000995
Iteration 78/1000 | Loss: 0.00000995
Iteration 79/1000 | Loss: 0.00000995
Iteration 80/1000 | Loss: 0.00000995
Iteration 81/1000 | Loss: 0.00000995
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000994
Iteration 84/1000 | Loss: 0.00000993
Iteration 85/1000 | Loss: 0.00000993
Iteration 86/1000 | Loss: 0.00000993
Iteration 87/1000 | Loss: 0.00000993
Iteration 88/1000 | Loss: 0.00000993
Iteration 89/1000 | Loss: 0.00000992
Iteration 90/1000 | Loss: 0.00000992
Iteration 91/1000 | Loss: 0.00000992
Iteration 92/1000 | Loss: 0.00000992
Iteration 93/1000 | Loss: 0.00000992
Iteration 94/1000 | Loss: 0.00000992
Iteration 95/1000 | Loss: 0.00000992
Iteration 96/1000 | Loss: 0.00000992
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000992
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000991
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000991
Iteration 104/1000 | Loss: 0.00000991
Iteration 105/1000 | Loss: 0.00000991
Iteration 106/1000 | Loss: 0.00000991
Iteration 107/1000 | Loss: 0.00000990
Iteration 108/1000 | Loss: 0.00000990
Iteration 109/1000 | Loss: 0.00000990
Iteration 110/1000 | Loss: 0.00000989
Iteration 111/1000 | Loss: 0.00000989
Iteration 112/1000 | Loss: 0.00000989
Iteration 113/1000 | Loss: 0.00000989
Iteration 114/1000 | Loss: 0.00000989
Iteration 115/1000 | Loss: 0.00000989
Iteration 116/1000 | Loss: 0.00000989
Iteration 117/1000 | Loss: 0.00000989
Iteration 118/1000 | Loss: 0.00000989
Iteration 119/1000 | Loss: 0.00000989
Iteration 120/1000 | Loss: 0.00000989
Iteration 121/1000 | Loss: 0.00000989
Iteration 122/1000 | Loss: 0.00000989
Iteration 123/1000 | Loss: 0.00000989
Iteration 124/1000 | Loss: 0.00000989
Iteration 125/1000 | Loss: 0.00000988
Iteration 126/1000 | Loss: 0.00000988
Iteration 127/1000 | Loss: 0.00000988
Iteration 128/1000 | Loss: 0.00000987
Iteration 129/1000 | Loss: 0.00000987
Iteration 130/1000 | Loss: 0.00000987
Iteration 131/1000 | Loss: 0.00000987
Iteration 132/1000 | Loss: 0.00000987
Iteration 133/1000 | Loss: 0.00000986
Iteration 134/1000 | Loss: 0.00000986
Iteration 135/1000 | Loss: 0.00000986
Iteration 136/1000 | Loss: 0.00000986
Iteration 137/1000 | Loss: 0.00000986
Iteration 138/1000 | Loss: 0.00000986
Iteration 139/1000 | Loss: 0.00000986
Iteration 140/1000 | Loss: 0.00000986
Iteration 141/1000 | Loss: 0.00000986
Iteration 142/1000 | Loss: 0.00000986
Iteration 143/1000 | Loss: 0.00000986
Iteration 144/1000 | Loss: 0.00000986
Iteration 145/1000 | Loss: 0.00000986
Iteration 146/1000 | Loss: 0.00000985
Iteration 147/1000 | Loss: 0.00000985
Iteration 148/1000 | Loss: 0.00000985
Iteration 149/1000 | Loss: 0.00000985
Iteration 150/1000 | Loss: 0.00000985
Iteration 151/1000 | Loss: 0.00000985
Iteration 152/1000 | Loss: 0.00000985
Iteration 153/1000 | Loss: 0.00000985
Iteration 154/1000 | Loss: 0.00000985
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000985
Iteration 158/1000 | Loss: 0.00000984
Iteration 159/1000 | Loss: 0.00000984
Iteration 160/1000 | Loss: 0.00000984
Iteration 161/1000 | Loss: 0.00000984
Iteration 162/1000 | Loss: 0.00000984
Iteration 163/1000 | Loss: 0.00000984
Iteration 164/1000 | Loss: 0.00000984
Iteration 165/1000 | Loss: 0.00000984
Iteration 166/1000 | Loss: 0.00000984
Iteration 167/1000 | Loss: 0.00000984
Iteration 168/1000 | Loss: 0.00000983
Iteration 169/1000 | Loss: 0.00000983
Iteration 170/1000 | Loss: 0.00000983
Iteration 171/1000 | Loss: 0.00000983
Iteration 172/1000 | Loss: 0.00000983
Iteration 173/1000 | Loss: 0.00000983
Iteration 174/1000 | Loss: 0.00000983
Iteration 175/1000 | Loss: 0.00000983
Iteration 176/1000 | Loss: 0.00000983
Iteration 177/1000 | Loss: 0.00000983
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000982
Iteration 182/1000 | Loss: 0.00000982
Iteration 183/1000 | Loss: 0.00000982
Iteration 184/1000 | Loss: 0.00000982
Iteration 185/1000 | Loss: 0.00000982
Iteration 186/1000 | Loss: 0.00000982
Iteration 187/1000 | Loss: 0.00000982
Iteration 188/1000 | Loss: 0.00000982
Iteration 189/1000 | Loss: 0.00000982
Iteration 190/1000 | Loss: 0.00000982
Iteration 191/1000 | Loss: 0.00000982
Iteration 192/1000 | Loss: 0.00000981
Iteration 193/1000 | Loss: 0.00000981
Iteration 194/1000 | Loss: 0.00000981
Iteration 195/1000 | Loss: 0.00000981
Iteration 196/1000 | Loss: 0.00000981
Iteration 197/1000 | Loss: 0.00000981
Iteration 198/1000 | Loss: 0.00000980
Iteration 199/1000 | Loss: 0.00000980
Iteration 200/1000 | Loss: 0.00000980
Iteration 201/1000 | Loss: 0.00000980
Iteration 202/1000 | Loss: 0.00000980
Iteration 203/1000 | Loss: 0.00000980
Iteration 204/1000 | Loss: 0.00000979
Iteration 205/1000 | Loss: 0.00000979
Iteration 206/1000 | Loss: 0.00000979
Iteration 207/1000 | Loss: 0.00000979
Iteration 208/1000 | Loss: 0.00000979
Iteration 209/1000 | Loss: 0.00000979
Iteration 210/1000 | Loss: 0.00000978
Iteration 211/1000 | Loss: 0.00000978
Iteration 212/1000 | Loss: 0.00000978
Iteration 213/1000 | Loss: 0.00000978
Iteration 214/1000 | Loss: 0.00000978
Iteration 215/1000 | Loss: 0.00000978
Iteration 216/1000 | Loss: 0.00000977
Iteration 217/1000 | Loss: 0.00000977
Iteration 218/1000 | Loss: 0.00000977
Iteration 219/1000 | Loss: 0.00000977
Iteration 220/1000 | Loss: 0.00000977
Iteration 221/1000 | Loss: 0.00000977
Iteration 222/1000 | Loss: 0.00000977
Iteration 223/1000 | Loss: 0.00000977
Iteration 224/1000 | Loss: 0.00000977
Iteration 225/1000 | Loss: 0.00000977
Iteration 226/1000 | Loss: 0.00000977
Iteration 227/1000 | Loss: 0.00000977
Iteration 228/1000 | Loss: 0.00000977
Iteration 229/1000 | Loss: 0.00000977
Iteration 230/1000 | Loss: 0.00000977
Iteration 231/1000 | Loss: 0.00000976
Iteration 232/1000 | Loss: 0.00000976
Iteration 233/1000 | Loss: 0.00000976
Iteration 234/1000 | Loss: 0.00000976
Iteration 235/1000 | Loss: 0.00000976
Iteration 236/1000 | Loss: 0.00000976
Iteration 237/1000 | Loss: 0.00000976
Iteration 238/1000 | Loss: 0.00000976
Iteration 239/1000 | Loss: 0.00000976
Iteration 240/1000 | Loss: 0.00000976
Iteration 241/1000 | Loss: 0.00000976
Iteration 242/1000 | Loss: 0.00000976
Iteration 243/1000 | Loss: 0.00000976
Iteration 244/1000 | Loss: 0.00000976
Iteration 245/1000 | Loss: 0.00000976
Iteration 246/1000 | Loss: 0.00000976
Iteration 247/1000 | Loss: 0.00000975
Iteration 248/1000 | Loss: 0.00000975
Iteration 249/1000 | Loss: 0.00000975
Iteration 250/1000 | Loss: 0.00000975
Iteration 251/1000 | Loss: 0.00000975
Iteration 252/1000 | Loss: 0.00000975
Iteration 253/1000 | Loss: 0.00000975
Iteration 254/1000 | Loss: 0.00000975
Iteration 255/1000 | Loss: 0.00000975
Iteration 256/1000 | Loss: 0.00000975
Iteration 257/1000 | Loss: 0.00000975
Iteration 258/1000 | Loss: 0.00000975
Iteration 259/1000 | Loss: 0.00000975
Iteration 260/1000 | Loss: 0.00000975
Iteration 261/1000 | Loss: 0.00000975
Iteration 262/1000 | Loss: 0.00000974
Iteration 263/1000 | Loss: 0.00000974
Iteration 264/1000 | Loss: 0.00000974
Iteration 265/1000 | Loss: 0.00000974
Iteration 266/1000 | Loss: 0.00000974
Iteration 267/1000 | Loss: 0.00000974
Iteration 268/1000 | Loss: 0.00000974
Iteration 269/1000 | Loss: 0.00000974
Iteration 270/1000 | Loss: 0.00000974
Iteration 271/1000 | Loss: 0.00000974
Iteration 272/1000 | Loss: 0.00000974
Iteration 273/1000 | Loss: 0.00000974
Iteration 274/1000 | Loss: 0.00000973
Iteration 275/1000 | Loss: 0.00000973
Iteration 276/1000 | Loss: 0.00000973
Iteration 277/1000 | Loss: 0.00000973
Iteration 278/1000 | Loss: 0.00000973
Iteration 279/1000 | Loss: 0.00000973
Iteration 280/1000 | Loss: 0.00000973
Iteration 281/1000 | Loss: 0.00000973
Iteration 282/1000 | Loss: 0.00000973
Iteration 283/1000 | Loss: 0.00000973
Iteration 284/1000 | Loss: 0.00000973
Iteration 285/1000 | Loss: 0.00000973
Iteration 286/1000 | Loss: 0.00000973
Iteration 287/1000 | Loss: 0.00000973
Iteration 288/1000 | Loss: 0.00000973
Iteration 289/1000 | Loss: 0.00000972
Iteration 290/1000 | Loss: 0.00000972
Iteration 291/1000 | Loss: 0.00000972
Iteration 292/1000 | Loss: 0.00000972
Iteration 293/1000 | Loss: 0.00000972
Iteration 294/1000 | Loss: 0.00000972
Iteration 295/1000 | Loss: 0.00000972
Iteration 296/1000 | Loss: 0.00000972
Iteration 297/1000 | Loss: 0.00000972
Iteration 298/1000 | Loss: 0.00000972
Iteration 299/1000 | Loss: 0.00000972
Iteration 300/1000 | Loss: 0.00000972
Iteration 301/1000 | Loss: 0.00000972
Iteration 302/1000 | Loss: 0.00000971
Iteration 303/1000 | Loss: 0.00000971
Iteration 304/1000 | Loss: 0.00000971
Iteration 305/1000 | Loss: 0.00000971
Iteration 306/1000 | Loss: 0.00000971
Iteration 307/1000 | Loss: 0.00000971
Iteration 308/1000 | Loss: 0.00000971
Iteration 309/1000 | Loss: 0.00000971
Iteration 310/1000 | Loss: 0.00000971
Iteration 311/1000 | Loss: 0.00000971
Iteration 312/1000 | Loss: 0.00000971
Iteration 313/1000 | Loss: 0.00000971
Iteration 314/1000 | Loss: 0.00000971
Iteration 315/1000 | Loss: 0.00000971
Iteration 316/1000 | Loss: 0.00000971
Iteration 317/1000 | Loss: 0.00000971
Iteration 318/1000 | Loss: 0.00000971
Iteration 319/1000 | Loss: 0.00000971
Iteration 320/1000 | Loss: 0.00000971
Iteration 321/1000 | Loss: 0.00000971
Iteration 322/1000 | Loss: 0.00000971
Iteration 323/1000 | Loss: 0.00000971
Iteration 324/1000 | Loss: 0.00000971
Iteration 325/1000 | Loss: 0.00000971
Iteration 326/1000 | Loss: 0.00000971
Iteration 327/1000 | Loss: 0.00000971
Iteration 328/1000 | Loss: 0.00000971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 328. Stopping optimization.
Last 5 losses: [9.709736332297325e-06, 9.709736332297325e-06, 9.709736332297325e-06, 9.709736332297325e-06, 9.709736332297325e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.709736332297325e-06

Optimization complete. Final v2v error: 2.6734819412231445 mm

Highest mean error: 3.327573299407959 mm for frame 75

Lowest mean error: 2.446537733078003 mm for frame 166

Saving results

Total time: 1126.5710451602936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1002
Device is either invalid or not available. Using CPU.
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061467
Iteration 2/25 | Loss: 0.00210273
Iteration 3/25 | Loss: 0.00143447
Iteration 4/25 | Loss: 0.00133288
Iteration 5/25 | Loss: 0.00131577
Iteration 6/25 | Loss: 0.00131285
Iteration 7/25 | Loss: 0.00131087
Iteration 8/25 | Loss: 0.00131032
Iteration 9/25 | Loss: 0.00130970
Iteration 10/25 | Loss: 0.00131048
Iteration 11/25 | Loss: 0.00130836
Iteration 12/25 | Loss: 0.00130514
Iteration 13/25 | Loss: 0.00130579
Iteration 14/25 | Loss: 0.00130657
Iteration 15/25 | Loss: 0.00130569
Iteration 16/25 | Loss: 0.00130601
Iteration 17/25 | Loss: 0.00130596
Iteration 18/25 | Loss: 0.00130848
Iteration 19/25 | Loss: 0.00130772
Iteration 20/25 | Loss: 0.00130622
Iteration 21/25 | Loss: 0.00130567
Iteration 22/25 | Loss: 0.00130496
Iteration 23/25 | Loss: 0.00130350
Iteration 24/25 | Loss: 0.00130265
Iteration 25/25 | Loss: 0.00130219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85968161
Iteration 2/25 | Loss: 0.00081607
Iteration 3/25 | Loss: 0.00081607
Iteration 4/25 | Loss: 0.00081607
Iteration 5/25 | Loss: 0.00081607
Iteration 6/25 | Loss: 0.00081607
Iteration 7/25 | Loss: 0.00081607
Iteration 8/25 | Loss: 0.00081607
Iteration 9/25 | Loss: 0.00081607
Iteration 10/25 | Loss: 0.00081607
Iteration 11/25 | Loss: 0.00081607
Iteration 12/25 | Loss: 0.00081607
Iteration 13/25 | Loss: 0.00081607
Iteration 14/25 | Loss: 0.00081607
Iteration 15/25 | Loss: 0.00081607
Iteration 16/25 | Loss: 0.00081607
Iteration 17/25 | Loss: 0.00081607
Iteration 18/25 | Loss: 0.00081607
Iteration 19/25 | Loss: 0.00081607
Iteration 20/25 | Loss: 0.00081607
Iteration 21/25 | Loss: 0.00081607
Iteration 22/25 | Loss: 0.00081607
Iteration 23/25 | Loss: 0.00081607
Iteration 24/25 | Loss: 0.00081607
Iteration 25/25 | Loss: 0.00081607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081607
Iteration 2/1000 | Loss: 0.00016390
Iteration 3/1000 | Loss: 0.00010711
Iteration 4/1000 | Loss: 0.00006265
Iteration 5/1000 | Loss: 0.00004793
Iteration 6/1000 | Loss: 0.00006587
Iteration 7/1000 | Loss: 0.00006382
Iteration 8/1000 | Loss: 0.00006532
Iteration 9/1000 | Loss: 0.00006008
Iteration 10/1000 | Loss: 0.00006612
Iteration 11/1000 | Loss: 0.00005619
Iteration 12/1000 | Loss: 0.00006377
Iteration 13/1000 | Loss: 0.00004998
Iteration 14/1000 | Loss: 0.00005354
Iteration 15/1000 | Loss: 0.00008414
Iteration 16/1000 | Loss: 0.00005126
Iteration 17/1000 | Loss: 0.00006119
Iteration 18/1000 | Loss: 0.00004838
Iteration 19/1000 | Loss: 0.00006463
Iteration 20/1000 | Loss: 0.00004785
Iteration 21/1000 | Loss: 0.00007306
Iteration 22/1000 | Loss: 0.00004943
Iteration 23/1000 | Loss: 0.00007285
Iteration 24/1000 | Loss: 0.00005410
Iteration 25/1000 | Loss: 0.00005136
Iteration 26/1000 | Loss: 0.00005120
Iteration 27/1000 | Loss: 0.00005978
Iteration 28/1000 | Loss: 0.00005945
Iteration 29/1000 | Loss: 0.00004181
Iteration 30/1000 | Loss: 0.00006095
Iteration 31/1000 | Loss: 0.00005138
Iteration 32/1000 | Loss: 0.00006086
Iteration 33/1000 | Loss: 0.00006025
Iteration 34/1000 | Loss: 0.00005498
Iteration 35/1000 | Loss: 0.00005468
Iteration 36/1000 | Loss: 0.00004683
Iteration 37/1000 | Loss: 0.00005408
Iteration 38/1000 | Loss: 0.00004728
Iteration 39/1000 | Loss: 0.00005194
Iteration 40/1000 | Loss: 0.00005474
Iteration 41/1000 | Loss: 0.00007075
Iteration 42/1000 | Loss: 0.00004209
Iteration 43/1000 | Loss: 0.00005419
Iteration 44/1000 | Loss: 0.00006300
Iteration 45/1000 | Loss: 0.00006690
Iteration 46/1000 | Loss: 0.00006309
Iteration 47/1000 | Loss: 0.00006869
Iteration 48/1000 | Loss: 0.00006446
Iteration 49/1000 | Loss: 0.00006797
Iteration 50/1000 | Loss: 0.00003930
Iteration 51/1000 | Loss: 0.00006385
Iteration 52/1000 | Loss: 0.00004003
Iteration 53/1000 | Loss: 0.00004887
Iteration 54/1000 | Loss: 0.00003902
Iteration 55/1000 | Loss: 0.00003635
Iteration 56/1000 | Loss: 0.00003498
Iteration 57/1000 | Loss: 0.00004029
Iteration 58/1000 | Loss: 0.00003764
Iteration 59/1000 | Loss: 0.00003929
Iteration 60/1000 | Loss: 0.00004068
Iteration 61/1000 | Loss: 0.00005024
Iteration 62/1000 | Loss: 0.00004572
Iteration 63/1000 | Loss: 0.00003428
Iteration 64/1000 | Loss: 0.00003419
Iteration 65/1000 | Loss: 0.00003416
Iteration 66/1000 | Loss: 0.00003390
Iteration 67/1000 | Loss: 0.00004745
Iteration 68/1000 | Loss: 0.00004909
Iteration 69/1000 | Loss: 0.00004735
Iteration 70/1000 | Loss: 0.00004635
Iteration 71/1000 | Loss: 0.00004591
Iteration 72/1000 | Loss: 0.00004589
Iteration 73/1000 | Loss: 0.00004239
Iteration 74/1000 | Loss: 0.00003995
Iteration 75/1000 | Loss: 0.00004299
Iteration 76/1000 | Loss: 0.00003783
Iteration 77/1000 | Loss: 0.00003594
Iteration 78/1000 | Loss: 0.00003817
Iteration 79/1000 | Loss: 0.00003695
Iteration 80/1000 | Loss: 0.00003817
Iteration 81/1000 | Loss: 0.00004779
Iteration 82/1000 | Loss: 0.00004555
Iteration 83/1000 | Loss: 0.00004657
Iteration 84/1000 | Loss: 0.00004539
Iteration 85/1000 | Loss: 0.00004673
Iteration 86/1000 | Loss: 0.00005363
Iteration 87/1000 | Loss: 0.00003434
Iteration 88/1000 | Loss: 0.00004684
Iteration 89/1000 | Loss: 0.00004360
Iteration 90/1000 | Loss: 0.00005418
Iteration 91/1000 | Loss: 0.00003989
Iteration 92/1000 | Loss: 0.00003708
Iteration 93/1000 | Loss: 0.00003455
Iteration 94/1000 | Loss: 0.00003364
Iteration 95/1000 | Loss: 0.00003340
Iteration 96/1000 | Loss: 0.00003318
Iteration 97/1000 | Loss: 0.00003299
Iteration 98/1000 | Loss: 0.00003293
Iteration 99/1000 | Loss: 0.00003292
Iteration 100/1000 | Loss: 0.00003292
Iteration 101/1000 | Loss: 0.00003292
Iteration 102/1000 | Loss: 0.00003292
Iteration 103/1000 | Loss: 0.00003292
Iteration 104/1000 | Loss: 0.00003292
Iteration 105/1000 | Loss: 0.00003292
Iteration 106/1000 | Loss: 0.00003292
Iteration 107/1000 | Loss: 0.00003289
Iteration 108/1000 | Loss: 0.00003289
Iteration 109/1000 | Loss: 0.00003289
Iteration 110/1000 | Loss: 0.00003289
Iteration 111/1000 | Loss: 0.00003289
Iteration 112/1000 | Loss: 0.00003289
Iteration 113/1000 | Loss: 0.00003289
Iteration 114/1000 | Loss: 0.00003289
Iteration 115/1000 | Loss: 0.00003289
Iteration 116/1000 | Loss: 0.00003288
Iteration 117/1000 | Loss: 0.00003288
Iteration 118/1000 | Loss: 0.00003288
Iteration 119/1000 | Loss: 0.00003288
Iteration 120/1000 | Loss: 0.00003288
Iteration 121/1000 | Loss: 0.00003285
Iteration 122/1000 | Loss: 0.00003285
Iteration 123/1000 | Loss: 0.00003285
Iteration 124/1000 | Loss: 0.00003285
Iteration 125/1000 | Loss: 0.00003285
Iteration 126/1000 | Loss: 0.00003285
Iteration 127/1000 | Loss: 0.00003285
Iteration 128/1000 | Loss: 0.00003284
Iteration 129/1000 | Loss: 0.00003284
Iteration 130/1000 | Loss: 0.00003284
Iteration 131/1000 | Loss: 0.00003284
Iteration 132/1000 | Loss: 0.00003284
Iteration 133/1000 | Loss: 0.00003284
Iteration 134/1000 | Loss: 0.00003283
Iteration 135/1000 | Loss: 0.00003283
Iteration 136/1000 | Loss: 0.00003283
Iteration 137/1000 | Loss: 0.00003281
Iteration 138/1000 | Loss: 0.00003281
Iteration 139/1000 | Loss: 0.00003281
Iteration 140/1000 | Loss: 0.00003281
Iteration 141/1000 | Loss: 0.00003281
Iteration 142/1000 | Loss: 0.00003281
Iteration 143/1000 | Loss: 0.00003281
Iteration 144/1000 | Loss: 0.00003281
Iteration 145/1000 | Loss: 0.00003281
Iteration 146/1000 | Loss: 0.00003281
Iteration 147/1000 | Loss: 0.00003281
Iteration 148/1000 | Loss: 0.00003281
Iteration 149/1000 | Loss: 0.00003281
Iteration 150/1000 | Loss: 0.00003280
Iteration 151/1000 | Loss: 0.00003280
Iteration 152/1000 | Loss: 0.00003280
Iteration 153/1000 | Loss: 0.00003279
Iteration 154/1000 | Loss: 0.00003279
Iteration 155/1000 | Loss: 0.00003279
Iteration 156/1000 | Loss: 0.00003279
Iteration 157/1000 | Loss: 0.00003279
Iteration 158/1000 | Loss: 0.00003279
Iteration 159/1000 | Loss: 0.00003279
Iteration 160/1000 | Loss: 0.00003279
Iteration 161/1000 | Loss: 0.00003278
Iteration 162/1000 | Loss: 0.00003278
Iteration 163/1000 | Loss: 0.00003278
Iteration 164/1000 | Loss: 0.00003278
Iteration 165/1000 | Loss: 0.00003278
Iteration 166/1000 | Loss: 0.00003278
Iteration 167/1000 | Loss: 0.00003278
Iteration 168/1000 | Loss: 0.00003278
Iteration 169/1000 | Loss: 0.00003277
Iteration 170/1000 | Loss: 0.00003277
Iteration 171/1000 | Loss: 0.00003277
Iteration 172/1000 | Loss: 0.00003277
Iteration 173/1000 | Loss: 0.00003276
Iteration 174/1000 | Loss: 0.00003276
Iteration 175/1000 | Loss: 0.00003276
Iteration 176/1000 | Loss: 0.00003275
Iteration 177/1000 | Loss: 0.00003275
Iteration 178/1000 | Loss: 0.00003275
Iteration 179/1000 | Loss: 0.00003275
Iteration 180/1000 | Loss: 0.00003275
Iteration 181/1000 | Loss: 0.00003275
Iteration 182/1000 | Loss: 0.00003275
Iteration 183/1000 | Loss: 0.00003275
Iteration 184/1000 | Loss: 0.00003275
Iteration 185/1000 | Loss: 0.00003274
Iteration 186/1000 | Loss: 0.00003273
Iteration 187/1000 | Loss: 0.00003273
Iteration 188/1000 | Loss: 0.00003273
Iteration 189/1000 | Loss: 0.00003273
Iteration 190/1000 | Loss: 0.00003273
Iteration 191/1000 | Loss: 0.00003273
Iteration 192/1000 | Loss: 0.00003273
Iteration 193/1000 | Loss: 0.00003273
Iteration 194/1000 | Loss: 0.00003273
Iteration 195/1000 | Loss: 0.00003272
Iteration 196/1000 | Loss: 0.00003271
Iteration 197/1000 | Loss: 0.00003271
Iteration 198/1000 | Loss: 0.00003271
Iteration 199/1000 | Loss: 0.00003271
Iteration 200/1000 | Loss: 0.00003271
Iteration 201/1000 | Loss: 0.00003271
Iteration 202/1000 | Loss: 0.00003270
Iteration 203/1000 | Loss: 0.00003270
Iteration 204/1000 | Loss: 0.00003270
Iteration 205/1000 | Loss: 0.00003270
Iteration 206/1000 | Loss: 0.00003269
Iteration 207/1000 | Loss: 0.00003269
Iteration 208/1000 | Loss: 0.00003269
Iteration 209/1000 | Loss: 0.00003269
Iteration 210/1000 | Loss: 0.00003268
Iteration 211/1000 | Loss: 0.00003268
Iteration 212/1000 | Loss: 0.00003268
Iteration 213/1000 | Loss: 0.00003268
Iteration 214/1000 | Loss: 0.00003268
Iteration 215/1000 | Loss: 0.00003268
Iteration 216/1000 | Loss: 0.00003267
Iteration 217/1000 | Loss: 0.00003267
Iteration 218/1000 | Loss: 0.00003266
Iteration 219/1000 | Loss: 0.00003266
Iteration 220/1000 | Loss: 0.00003266
Iteration 221/1000 | Loss: 0.00003266
Iteration 222/1000 | Loss: 0.00003266
Iteration 223/1000 | Loss: 0.00003266
Iteration 224/1000 | Loss: 0.00003266
Iteration 225/1000 | Loss: 0.00003266
Iteration 226/1000 | Loss: 0.00003266
Iteration 227/1000 | Loss: 0.00003265
Iteration 228/1000 | Loss: 0.00003265
Iteration 229/1000 | Loss: 0.00003265
Iteration 230/1000 | Loss: 0.00003264
Iteration 231/1000 | Loss: 0.00003264
Iteration 232/1000 | Loss: 0.00003264
Iteration 233/1000 | Loss: 0.00003264
Iteration 234/1000 | Loss: 0.00003264
Iteration 235/1000 | Loss: 0.00003264
Iteration 236/1000 | Loss: 0.00003264
Iteration 237/1000 | Loss: 0.00003264
Iteration 238/1000 | Loss: 0.00003263
Iteration 239/1000 | Loss: 0.00003263
Iteration 240/1000 | Loss: 0.00003263
Iteration 241/1000 | Loss: 0.00003263
Iteration 242/1000 | Loss: 0.00003263
Iteration 243/1000 | Loss: 0.00003263
Iteration 244/1000 | Loss: 0.00003263
Iteration 245/1000 | Loss: 0.00003262
Iteration 246/1000 | Loss: 0.00003262
Iteration 247/1000 | Loss: 0.00003262
Iteration 248/1000 | Loss: 0.00003262
Iteration 249/1000 | Loss: 0.00003262
Iteration 250/1000 | Loss: 0.00003262
Iteration 251/1000 | Loss: 0.00003261
Iteration 252/1000 | Loss: 0.00003261
Iteration 253/1000 | Loss: 0.00003261
Iteration 254/1000 | Loss: 0.00003261
Iteration 255/1000 | Loss: 0.00003261
Iteration 256/1000 | Loss: 0.00003261
Iteration 257/1000 | Loss: 0.00003261
Iteration 258/1000 | Loss: 0.00003261
Iteration 259/1000 | Loss: 0.00003261
Iteration 260/1000 | Loss: 0.00003260
Iteration 261/1000 | Loss: 0.00003260
Iteration 262/1000 | Loss: 0.00003260
Iteration 263/1000 | Loss: 0.00003260
Iteration 264/1000 | Loss: 0.00003260
Iteration 265/1000 | Loss: 0.00003260
Iteration 266/1000 | Loss: 0.00003260
Iteration 267/1000 | Loss: 0.00003260
Iteration 268/1000 | Loss: 0.00003260
Iteration 269/1000 | Loss: 0.00003260
Iteration 270/1000 | Loss: 0.00003260
Iteration 271/1000 | Loss: 0.00003259
Iteration 272/1000 | Loss: 0.00003259
Iteration 273/1000 | Loss: 0.00003259
Iteration 274/1000 | Loss: 0.00003259
Iteration 275/1000 | Loss: 0.00003259
Iteration 276/1000 | Loss: 0.00003259
Iteration 277/1000 | Loss: 0.00003259
Iteration 278/1000 | Loss: 0.00003259
Iteration 279/1000 | Loss: 0.00003259
Iteration 280/1000 | Loss: 0.00003259
Iteration 281/1000 | Loss: 0.00003259
Iteration 282/1000 | Loss: 0.00003259
Iteration 283/1000 | Loss: 0.00003258
Iteration 284/1000 | Loss: 0.00003258
Iteration 285/1000 | Loss: 0.00003258
Iteration 286/1000 | Loss: 0.00003258
Iteration 287/1000 | Loss: 0.00003258
Iteration 288/1000 | Loss: 0.00003258
Iteration 289/1000 | Loss: 0.00003258
Iteration 290/1000 | Loss: 0.00003258
Iteration 291/1000 | Loss: 0.00003258
Iteration 292/1000 | Loss: 0.00003258
Iteration 293/1000 | Loss: 0.00003258
Iteration 294/1000 | Loss: 0.00003258
Iteration 295/1000 | Loss: 0.00003258
Iteration 296/1000 | Loss: 0.00003258
Iteration 297/1000 | Loss: 0.00003258
Iteration 298/1000 | Loss: 0.00003258
Iteration 299/1000 | Loss: 0.00003258
Iteration 300/1000 | Loss: 0.00003258
Iteration 301/1000 | Loss: 0.00003257
Iteration 302/1000 | Loss: 0.00003257
Iteration 303/1000 | Loss: 0.00003257
Iteration 304/1000 | Loss: 0.00003257
Iteration 305/1000 | Loss: 0.00003257
Iteration 306/1000 | Loss: 0.00003257
Iteration 307/1000 | Loss: 0.00003257
Iteration 308/1000 | Loss: 0.00003257
Iteration 309/1000 | Loss: 0.00003257
Iteration 310/1000 | Loss: 0.00003257
Iteration 311/1000 | Loss: 0.00003257
Iteration 312/1000 | Loss: 0.00003257
Iteration 313/1000 | Loss: 0.00003257
Iteration 314/1000 | Loss: 0.00003257
Iteration 315/1000 | Loss: 0.00003257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [3.256771742599085e-05, 3.256771742599085e-05, 3.256771742599085e-05, 3.256771742599085e-05, 3.256771742599085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.256771742599085e-05

Optimization complete. Final v2v error: 4.640609264373779 mm

Highest mean error: 5.53895378112793 mm for frame 139

Lowest mean error: 4.125473499298096 mm for frame 221

Saving results

Total time: 5994.940329790115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_003/1064/motion_seq.npz
File motion_seq.npz already exists in /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_003/1064. Skipping.
