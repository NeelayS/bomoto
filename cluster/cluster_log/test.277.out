Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=277, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15512-15567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024038
Iteration 2/25 | Loss: 0.00473509
Iteration 3/25 | Loss: 0.00217802
Iteration 4/25 | Loss: 0.00198907
Iteration 5/25 | Loss: 0.00183232
Iteration 6/25 | Loss: 0.00166384
Iteration 7/25 | Loss: 0.00167779
Iteration 8/25 | Loss: 0.00160250
Iteration 9/25 | Loss: 0.00150361
Iteration 10/25 | Loss: 0.00142424
Iteration 11/25 | Loss: 0.00137409
Iteration 12/25 | Loss: 0.00137456
Iteration 13/25 | Loss: 0.00135071
Iteration 14/25 | Loss: 0.00132940
Iteration 15/25 | Loss: 0.00128961
Iteration 16/25 | Loss: 0.00126761
Iteration 17/25 | Loss: 0.00126383
Iteration 18/25 | Loss: 0.00123976
Iteration 19/25 | Loss: 0.00122578
Iteration 20/25 | Loss: 0.00121875
Iteration 21/25 | Loss: 0.00120569
Iteration 22/25 | Loss: 0.00119966
Iteration 23/25 | Loss: 0.00119455
Iteration 24/25 | Loss: 0.00119113
Iteration 25/25 | Loss: 0.00119134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52707314
Iteration 2/25 | Loss: 0.00755230
Iteration 3/25 | Loss: 0.00469967
Iteration 4/25 | Loss: 0.00469966
Iteration 5/25 | Loss: 0.00469966
Iteration 6/25 | Loss: 0.00469966
Iteration 7/25 | Loss: 0.00469966
Iteration 8/25 | Loss: 0.00469966
Iteration 9/25 | Loss: 0.00469966
Iteration 10/25 | Loss: 0.00469966
Iteration 11/25 | Loss: 0.00469966
Iteration 12/25 | Loss: 0.00469966
Iteration 13/25 | Loss: 0.00469966
Iteration 14/25 | Loss: 0.00469966
Iteration 15/25 | Loss: 0.00469966
Iteration 16/25 | Loss: 0.00469966
Iteration 17/25 | Loss: 0.00469966
Iteration 18/25 | Loss: 0.00469966
Iteration 19/25 | Loss: 0.00469966
Iteration 20/25 | Loss: 0.00469966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004699657205492258, 0.004699657205492258, 0.004699657205492258, 0.004699657205492258, 0.004699657205492258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004699657205492258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00469966
Iteration 2/1000 | Loss: 0.00466989
Iteration 3/1000 | Loss: 0.00170671
Iteration 4/1000 | Loss: 0.00194053
Iteration 5/1000 | Loss: 0.00346377
Iteration 6/1000 | Loss: 0.00065045
Iteration 7/1000 | Loss: 0.00048633
Iteration 8/1000 | Loss: 0.00079016
Iteration 9/1000 | Loss: 0.00100616
Iteration 10/1000 | Loss: 0.00046668
Iteration 11/1000 | Loss: 0.00078535
Iteration 12/1000 | Loss: 0.00137121
Iteration 13/1000 | Loss: 0.00059683
Iteration 14/1000 | Loss: 0.00132080
Iteration 15/1000 | Loss: 0.00142077
Iteration 16/1000 | Loss: 0.00230722
Iteration 17/1000 | Loss: 0.00180501
Iteration 18/1000 | Loss: 0.00092001
Iteration 19/1000 | Loss: 0.00062821
Iteration 20/1000 | Loss: 0.00027771
Iteration 21/1000 | Loss: 0.00048592
Iteration 22/1000 | Loss: 0.00029164
Iteration 23/1000 | Loss: 0.00192481
Iteration 24/1000 | Loss: 0.00383926
Iteration 25/1000 | Loss: 0.00023342
Iteration 26/1000 | Loss: 0.00103880
Iteration 27/1000 | Loss: 0.00021256
Iteration 28/1000 | Loss: 0.00026742
Iteration 29/1000 | Loss: 0.00060118
Iteration 30/1000 | Loss: 0.00023233
Iteration 31/1000 | Loss: 0.00060036
Iteration 32/1000 | Loss: 0.00108510
Iteration 33/1000 | Loss: 0.00033024
Iteration 34/1000 | Loss: 0.00033985
Iteration 35/1000 | Loss: 0.00110129
Iteration 36/1000 | Loss: 0.00061649
Iteration 37/1000 | Loss: 0.00145168
Iteration 38/1000 | Loss: 0.00157657
Iteration 39/1000 | Loss: 0.00085375
Iteration 40/1000 | Loss: 0.00050645
Iteration 41/1000 | Loss: 0.00050062
Iteration 42/1000 | Loss: 0.00014469
Iteration 43/1000 | Loss: 0.00057621
Iteration 44/1000 | Loss: 0.00111226
Iteration 45/1000 | Loss: 0.00014790
Iteration 46/1000 | Loss: 0.00051033
Iteration 47/1000 | Loss: 0.00013485
Iteration 48/1000 | Loss: 0.00084357
Iteration 49/1000 | Loss: 0.00051920
Iteration 50/1000 | Loss: 0.00014290
Iteration 51/1000 | Loss: 0.00014128
Iteration 52/1000 | Loss: 0.00013228
Iteration 53/1000 | Loss: 0.00011806
Iteration 54/1000 | Loss: 0.00012496
Iteration 55/1000 | Loss: 0.00088974
Iteration 56/1000 | Loss: 0.00021797
Iteration 57/1000 | Loss: 0.00012538
Iteration 58/1000 | Loss: 0.00011803
Iteration 59/1000 | Loss: 0.00011670
Iteration 60/1000 | Loss: 0.00118649
Iteration 61/1000 | Loss: 0.00036883
Iteration 62/1000 | Loss: 0.00126048
Iteration 63/1000 | Loss: 0.00254025
Iteration 64/1000 | Loss: 0.00029734
Iteration 65/1000 | Loss: 0.00033010
Iteration 66/1000 | Loss: 0.00011227
Iteration 67/1000 | Loss: 0.00012533
Iteration 68/1000 | Loss: 0.00053066
Iteration 69/1000 | Loss: 0.00041076
Iteration 70/1000 | Loss: 0.00080886
Iteration 71/1000 | Loss: 0.00020072
Iteration 72/1000 | Loss: 0.00008959
Iteration 73/1000 | Loss: 0.00009146
Iteration 74/1000 | Loss: 0.00009111
Iteration 75/1000 | Loss: 0.00024438
Iteration 76/1000 | Loss: 0.00030247
Iteration 77/1000 | Loss: 0.00024427
Iteration 78/1000 | Loss: 0.00029688
Iteration 79/1000 | Loss: 0.00010905
Iteration 80/1000 | Loss: 0.00008849
Iteration 81/1000 | Loss: 0.00007762
Iteration 82/1000 | Loss: 0.00016491
Iteration 83/1000 | Loss: 0.00023652
Iteration 84/1000 | Loss: 0.00008232
Iteration 85/1000 | Loss: 0.00006316
Iteration 86/1000 | Loss: 0.00014836
Iteration 87/1000 | Loss: 0.00006256
Iteration 88/1000 | Loss: 0.00008665
Iteration 89/1000 | Loss: 0.00024694
Iteration 90/1000 | Loss: 0.00010260
Iteration 91/1000 | Loss: 0.00009301
Iteration 92/1000 | Loss: 0.00008726
Iteration 93/1000 | Loss: 0.00010016
Iteration 94/1000 | Loss: 0.00008749
Iteration 95/1000 | Loss: 0.00007480
Iteration 96/1000 | Loss: 0.00007481
Iteration 97/1000 | Loss: 0.00010763
Iteration 98/1000 | Loss: 0.00024093
Iteration 99/1000 | Loss: 0.00006902
Iteration 100/1000 | Loss: 0.00006635
Iteration 101/1000 | Loss: 0.00006273
Iteration 102/1000 | Loss: 0.00025683
Iteration 103/1000 | Loss: 0.00041588
Iteration 104/1000 | Loss: 0.00016329
Iteration 105/1000 | Loss: 0.00006358
Iteration 106/1000 | Loss: 0.00005202
Iteration 107/1000 | Loss: 0.00026712
Iteration 108/1000 | Loss: 0.00036465
Iteration 109/1000 | Loss: 0.00028966
Iteration 110/1000 | Loss: 0.00007588
Iteration 111/1000 | Loss: 0.00004776
Iteration 112/1000 | Loss: 0.00007071
Iteration 113/1000 | Loss: 0.00006567
Iteration 114/1000 | Loss: 0.00004890
Iteration 115/1000 | Loss: 0.00006343
Iteration 116/1000 | Loss: 0.00006005
Iteration 117/1000 | Loss: 0.00007412
Iteration 118/1000 | Loss: 0.00005702
Iteration 119/1000 | Loss: 0.00006435
Iteration 120/1000 | Loss: 0.00007092
Iteration 121/1000 | Loss: 0.00004792
Iteration 122/1000 | Loss: 0.00004159
Iteration 123/1000 | Loss: 0.00009606
Iteration 124/1000 | Loss: 0.00004895
Iteration 125/1000 | Loss: 0.00004221
Iteration 126/1000 | Loss: 0.00003748
Iteration 127/1000 | Loss: 0.00003513
Iteration 128/1000 | Loss: 0.00003398
Iteration 129/1000 | Loss: 0.00003304
Iteration 130/1000 | Loss: 0.00003251
Iteration 131/1000 | Loss: 0.00003215
Iteration 132/1000 | Loss: 0.00003195
Iteration 133/1000 | Loss: 0.00003195
Iteration 134/1000 | Loss: 0.00003193
Iteration 135/1000 | Loss: 0.00003192
Iteration 136/1000 | Loss: 0.00003188
Iteration 137/1000 | Loss: 0.00003187
Iteration 138/1000 | Loss: 0.00003186
Iteration 139/1000 | Loss: 0.00003181
Iteration 140/1000 | Loss: 0.00003181
Iteration 141/1000 | Loss: 0.00003175
Iteration 142/1000 | Loss: 0.00003173
Iteration 143/1000 | Loss: 0.00003172
Iteration 144/1000 | Loss: 0.00003160
Iteration 145/1000 | Loss: 0.00003149
Iteration 146/1000 | Loss: 0.00003146
Iteration 147/1000 | Loss: 0.00003146
Iteration 148/1000 | Loss: 0.00003146
Iteration 149/1000 | Loss: 0.00003146
Iteration 150/1000 | Loss: 0.00003146
Iteration 151/1000 | Loss: 0.00003146
Iteration 152/1000 | Loss: 0.00003146
Iteration 153/1000 | Loss: 0.00003146
Iteration 154/1000 | Loss: 0.00003146
Iteration 155/1000 | Loss: 0.00003146
Iteration 156/1000 | Loss: 0.00003145
Iteration 157/1000 | Loss: 0.00003145
Iteration 158/1000 | Loss: 0.00003145
Iteration 159/1000 | Loss: 0.00003144
Iteration 160/1000 | Loss: 0.00003142
Iteration 161/1000 | Loss: 0.00003139
Iteration 162/1000 | Loss: 0.00003139
Iteration 163/1000 | Loss: 0.00003139
Iteration 164/1000 | Loss: 0.00003139
Iteration 165/1000 | Loss: 0.00003139
Iteration 166/1000 | Loss: 0.00003138
Iteration 167/1000 | Loss: 0.00003138
Iteration 168/1000 | Loss: 0.00003138
Iteration 169/1000 | Loss: 0.00003138
Iteration 170/1000 | Loss: 0.00003137
Iteration 171/1000 | Loss: 0.00003137
Iteration 172/1000 | Loss: 0.00003136
Iteration 173/1000 | Loss: 0.00003136
Iteration 174/1000 | Loss: 0.00003136
Iteration 175/1000 | Loss: 0.00003136
Iteration 176/1000 | Loss: 0.00003136
Iteration 177/1000 | Loss: 0.00003136
Iteration 178/1000 | Loss: 0.00003136
Iteration 179/1000 | Loss: 0.00003136
Iteration 180/1000 | Loss: 0.00003135
Iteration 181/1000 | Loss: 0.00003135
Iteration 182/1000 | Loss: 0.00003135
Iteration 183/1000 | Loss: 0.00003135
Iteration 184/1000 | Loss: 0.00003135
Iteration 185/1000 | Loss: 0.00003135
Iteration 186/1000 | Loss: 0.00003135
Iteration 187/1000 | Loss: 0.00003135
Iteration 188/1000 | Loss: 0.00003135
Iteration 189/1000 | Loss: 0.00003135
Iteration 190/1000 | Loss: 0.00003135
Iteration 191/1000 | Loss: 0.00003135
Iteration 192/1000 | Loss: 0.00003134
Iteration 193/1000 | Loss: 0.00003134
Iteration 194/1000 | Loss: 0.00003133
Iteration 195/1000 | Loss: 0.00003133
Iteration 196/1000 | Loss: 0.00003133
Iteration 197/1000 | Loss: 0.00003133
Iteration 198/1000 | Loss: 0.00003133
Iteration 199/1000 | Loss: 0.00003132
Iteration 200/1000 | Loss: 0.00003132
Iteration 201/1000 | Loss: 0.00003132
Iteration 202/1000 | Loss: 0.00003132
Iteration 203/1000 | Loss: 0.00003132
Iteration 204/1000 | Loss: 0.00003132
Iteration 205/1000 | Loss: 0.00003132
Iteration 206/1000 | Loss: 0.00003132
Iteration 207/1000 | Loss: 0.00003131
Iteration 208/1000 | Loss: 0.00003131
Iteration 209/1000 | Loss: 0.00003131
Iteration 210/1000 | Loss: 0.00003131
Iteration 211/1000 | Loss: 0.00003131
Iteration 212/1000 | Loss: 0.00003131
Iteration 213/1000 | Loss: 0.00003131
Iteration 214/1000 | Loss: 0.00003131
Iteration 215/1000 | Loss: 0.00003131
Iteration 216/1000 | Loss: 0.00003131
Iteration 217/1000 | Loss: 0.00003130
Iteration 218/1000 | Loss: 0.00003130
Iteration 219/1000 | Loss: 0.00003130
Iteration 220/1000 | Loss: 0.00003130
Iteration 221/1000 | Loss: 0.00003130
Iteration 222/1000 | Loss: 0.00003130
Iteration 223/1000 | Loss: 0.00003129
Iteration 224/1000 | Loss: 0.00003129
Iteration 225/1000 | Loss: 0.00003129
Iteration 226/1000 | Loss: 0.00003129
Iteration 227/1000 | Loss: 0.00003129
Iteration 228/1000 | Loss: 0.00003129
Iteration 229/1000 | Loss: 0.00019669
Iteration 230/1000 | Loss: 0.00017777
Iteration 231/1000 | Loss: 0.00032287
Iteration 232/1000 | Loss: 0.00008713
Iteration 233/1000 | Loss: 0.00012537
Iteration 234/1000 | Loss: 0.00059253
Iteration 235/1000 | Loss: 0.00058822
Iteration 236/1000 | Loss: 0.00091831
Iteration 237/1000 | Loss: 0.00060904
Iteration 238/1000 | Loss: 0.00005280
Iteration 239/1000 | Loss: 0.00003807
Iteration 240/1000 | Loss: 0.00003492
Iteration 241/1000 | Loss: 0.00003336
Iteration 242/1000 | Loss: 0.00003260
Iteration 243/1000 | Loss: 0.00003192
Iteration 244/1000 | Loss: 0.00027379
Iteration 245/1000 | Loss: 0.00012815
Iteration 246/1000 | Loss: 0.00087639
Iteration 247/1000 | Loss: 0.00103671
Iteration 248/1000 | Loss: 0.00013690
Iteration 249/1000 | Loss: 0.00025903
Iteration 250/1000 | Loss: 0.00005102
Iteration 251/1000 | Loss: 0.00037023
Iteration 252/1000 | Loss: 0.00004270
Iteration 253/1000 | Loss: 0.00003043
Iteration 254/1000 | Loss: 0.00002850
Iteration 255/1000 | Loss: 0.00002651
Iteration 256/1000 | Loss: 0.00002506
Iteration 257/1000 | Loss: 0.00002432
Iteration 258/1000 | Loss: 0.00002385
Iteration 259/1000 | Loss: 0.00002359
Iteration 260/1000 | Loss: 0.00002338
Iteration 261/1000 | Loss: 0.00002321
Iteration 262/1000 | Loss: 0.00002307
Iteration 263/1000 | Loss: 0.00002304
Iteration 264/1000 | Loss: 0.00002303
Iteration 265/1000 | Loss: 0.00002299
Iteration 266/1000 | Loss: 0.00002296
Iteration 267/1000 | Loss: 0.00002296
Iteration 268/1000 | Loss: 0.00002295
Iteration 269/1000 | Loss: 0.00002294
Iteration 270/1000 | Loss: 0.00002293
Iteration 271/1000 | Loss: 0.00002292
Iteration 272/1000 | Loss: 0.00002291
Iteration 273/1000 | Loss: 0.00002291
Iteration 274/1000 | Loss: 0.00002291
Iteration 275/1000 | Loss: 0.00002291
Iteration 276/1000 | Loss: 0.00002291
Iteration 277/1000 | Loss: 0.00002291
Iteration 278/1000 | Loss: 0.00002291
Iteration 279/1000 | Loss: 0.00002291
Iteration 280/1000 | Loss: 0.00002291
Iteration 281/1000 | Loss: 0.00002291
Iteration 282/1000 | Loss: 0.00002291
Iteration 283/1000 | Loss: 0.00002291
Iteration 284/1000 | Loss: 0.00002291
Iteration 285/1000 | Loss: 0.00002291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [2.290642623847816e-05, 2.290642623847816e-05, 2.290642623847816e-05, 2.290642623847816e-05, 2.290642623847816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.290642623847816e-05

Optimization complete. Final v2v error: 3.3638336658477783 mm

Highest mean error: 11.321977615356445 mm for frame 160

Lowest mean error: 2.851327896118164 mm for frame 70

Saving results

Total time: 331.1687207221985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812937
Iteration 2/25 | Loss: 0.00126741
Iteration 3/25 | Loss: 0.00098242
Iteration 4/25 | Loss: 0.00091397
Iteration 5/25 | Loss: 0.00089011
Iteration 6/25 | Loss: 0.00088587
Iteration 7/25 | Loss: 0.00088440
Iteration 8/25 | Loss: 0.00088389
Iteration 9/25 | Loss: 0.00088389
Iteration 10/25 | Loss: 0.00088389
Iteration 11/25 | Loss: 0.00088389
Iteration 12/25 | Loss: 0.00088389
Iteration 13/25 | Loss: 0.00088389
Iteration 14/25 | Loss: 0.00088389
Iteration 15/25 | Loss: 0.00088389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008838926441967487, 0.0008838926441967487, 0.0008838926441967487, 0.0008838926441967487, 0.0008838926441967487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008838926441967487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50321543
Iteration 2/25 | Loss: 0.00072635
Iteration 3/25 | Loss: 0.00072634
Iteration 4/25 | Loss: 0.00072634
Iteration 5/25 | Loss: 0.00072634
Iteration 6/25 | Loss: 0.00072634
Iteration 7/25 | Loss: 0.00072634
Iteration 8/25 | Loss: 0.00072634
Iteration 9/25 | Loss: 0.00072634
Iteration 10/25 | Loss: 0.00072634
Iteration 11/25 | Loss: 0.00072634
Iteration 12/25 | Loss: 0.00072634
Iteration 13/25 | Loss: 0.00072634
Iteration 14/25 | Loss: 0.00072634
Iteration 15/25 | Loss: 0.00072634
Iteration 16/25 | Loss: 0.00072634
Iteration 17/25 | Loss: 0.00072634
Iteration 18/25 | Loss: 0.00072634
Iteration 19/25 | Loss: 0.00072634
Iteration 20/25 | Loss: 0.00072634
Iteration 21/25 | Loss: 0.00072634
Iteration 22/25 | Loss: 0.00072634
Iteration 23/25 | Loss: 0.00072634
Iteration 24/25 | Loss: 0.00072634
Iteration 25/25 | Loss: 0.00072634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072634
Iteration 2/1000 | Loss: 0.00005015
Iteration 3/1000 | Loss: 0.00003703
Iteration 4/1000 | Loss: 0.00003064
Iteration 5/1000 | Loss: 0.00002789
Iteration 6/1000 | Loss: 0.00002655
Iteration 7/1000 | Loss: 0.00002550
Iteration 8/1000 | Loss: 0.00002473
Iteration 9/1000 | Loss: 0.00002420
Iteration 10/1000 | Loss: 0.00002389
Iteration 11/1000 | Loss: 0.00002386
Iteration 12/1000 | Loss: 0.00002363
Iteration 13/1000 | Loss: 0.00002343
Iteration 14/1000 | Loss: 0.00002332
Iteration 15/1000 | Loss: 0.00002315
Iteration 16/1000 | Loss: 0.00002296
Iteration 17/1000 | Loss: 0.00002289
Iteration 18/1000 | Loss: 0.00002286
Iteration 19/1000 | Loss: 0.00002285
Iteration 20/1000 | Loss: 0.00002285
Iteration 21/1000 | Loss: 0.00002284
Iteration 22/1000 | Loss: 0.00002283
Iteration 23/1000 | Loss: 0.00002281
Iteration 24/1000 | Loss: 0.00002278
Iteration 25/1000 | Loss: 0.00002273
Iteration 26/1000 | Loss: 0.00002273
Iteration 27/1000 | Loss: 0.00002272
Iteration 28/1000 | Loss: 0.00002272
Iteration 29/1000 | Loss: 0.00002271
Iteration 30/1000 | Loss: 0.00002271
Iteration 31/1000 | Loss: 0.00002270
Iteration 32/1000 | Loss: 0.00002269
Iteration 33/1000 | Loss: 0.00002269
Iteration 34/1000 | Loss: 0.00002269
Iteration 35/1000 | Loss: 0.00002269
Iteration 36/1000 | Loss: 0.00002269
Iteration 37/1000 | Loss: 0.00002268
Iteration 38/1000 | Loss: 0.00002267
Iteration 39/1000 | Loss: 0.00002267
Iteration 40/1000 | Loss: 0.00002266
Iteration 41/1000 | Loss: 0.00002266
Iteration 42/1000 | Loss: 0.00002266
Iteration 43/1000 | Loss: 0.00002265
Iteration 44/1000 | Loss: 0.00002265
Iteration 45/1000 | Loss: 0.00002265
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002263
Iteration 50/1000 | Loss: 0.00002263
Iteration 51/1000 | Loss: 0.00002263
Iteration 52/1000 | Loss: 0.00002263
Iteration 53/1000 | Loss: 0.00002262
Iteration 54/1000 | Loss: 0.00002262
Iteration 55/1000 | Loss: 0.00002262
Iteration 56/1000 | Loss: 0.00002262
Iteration 57/1000 | Loss: 0.00002261
Iteration 58/1000 | Loss: 0.00002261
Iteration 59/1000 | Loss: 0.00002261
Iteration 60/1000 | Loss: 0.00002260
Iteration 61/1000 | Loss: 0.00002260
Iteration 62/1000 | Loss: 0.00002260
Iteration 63/1000 | Loss: 0.00002259
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002259
Iteration 66/1000 | Loss: 0.00002258
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002258
Iteration 74/1000 | Loss: 0.00002257
Iteration 75/1000 | Loss: 0.00002257
Iteration 76/1000 | Loss: 0.00002257
Iteration 77/1000 | Loss: 0.00002257
Iteration 78/1000 | Loss: 0.00002257
Iteration 79/1000 | Loss: 0.00002257
Iteration 80/1000 | Loss: 0.00002257
Iteration 81/1000 | Loss: 0.00002257
Iteration 82/1000 | Loss: 0.00002257
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002256
Iteration 87/1000 | Loss: 0.00002256
Iteration 88/1000 | Loss: 0.00002256
Iteration 89/1000 | Loss: 0.00002256
Iteration 90/1000 | Loss: 0.00002256
Iteration 91/1000 | Loss: 0.00002256
Iteration 92/1000 | Loss: 0.00002256
Iteration 93/1000 | Loss: 0.00002256
Iteration 94/1000 | Loss: 0.00002256
Iteration 95/1000 | Loss: 0.00002256
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002256
Iteration 99/1000 | Loss: 0.00002256
Iteration 100/1000 | Loss: 0.00002256
Iteration 101/1000 | Loss: 0.00002256
Iteration 102/1000 | Loss: 0.00002256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.2563397578778677e-05, 2.2563397578778677e-05, 2.2563397578778677e-05, 2.2563397578778677e-05, 2.2563397578778677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2563397578778677e-05

Optimization complete. Final v2v error: 3.979109764099121 mm

Highest mean error: 4.517531871795654 mm for frame 58

Lowest mean error: 3.343797445297241 mm for frame 132

Saving results

Total time: 40.345202684402466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834929
Iteration 2/25 | Loss: 0.00098255
Iteration 3/25 | Loss: 0.00082885
Iteration 4/25 | Loss: 0.00081649
Iteration 5/25 | Loss: 0.00081315
Iteration 6/25 | Loss: 0.00081217
Iteration 7/25 | Loss: 0.00081217
Iteration 8/25 | Loss: 0.00081217
Iteration 9/25 | Loss: 0.00081217
Iteration 10/25 | Loss: 0.00081217
Iteration 11/25 | Loss: 0.00081217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008121656137518585, 0.0008121656137518585, 0.0008121656137518585, 0.0008121656137518585, 0.0008121656137518585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008121656137518585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51126933
Iteration 2/25 | Loss: 0.00054442
Iteration 3/25 | Loss: 0.00054442
Iteration 4/25 | Loss: 0.00054442
Iteration 5/25 | Loss: 0.00054442
Iteration 6/25 | Loss: 0.00054442
Iteration 7/25 | Loss: 0.00054442
Iteration 8/25 | Loss: 0.00054442
Iteration 9/25 | Loss: 0.00054442
Iteration 10/25 | Loss: 0.00054442
Iteration 11/25 | Loss: 0.00054442
Iteration 12/25 | Loss: 0.00054442
Iteration 13/25 | Loss: 0.00054442
Iteration 14/25 | Loss: 0.00054442
Iteration 15/25 | Loss: 0.00054442
Iteration 16/25 | Loss: 0.00054442
Iteration 17/25 | Loss: 0.00054442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005444178241305053, 0.0005444178241305053, 0.0005444178241305053, 0.0005444178241305053, 0.0005444178241305053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005444178241305053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054442
Iteration 2/1000 | Loss: 0.00002505
Iteration 3/1000 | Loss: 0.00001679
Iteration 4/1000 | Loss: 0.00001430
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001226
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001203
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001171
Iteration 14/1000 | Loss: 0.00001163
Iteration 15/1000 | Loss: 0.00001157
Iteration 16/1000 | Loss: 0.00001157
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001148
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001145
Iteration 23/1000 | Loss: 0.00001145
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001142
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001141
Iteration 32/1000 | Loss: 0.00001141
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001141
Iteration 35/1000 | Loss: 0.00001141
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001139
Iteration 42/1000 | Loss: 0.00001139
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001138
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001137
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001135
Iteration 52/1000 | Loss: 0.00001135
Iteration 53/1000 | Loss: 0.00001135
Iteration 54/1000 | Loss: 0.00001134
Iteration 55/1000 | Loss: 0.00001134
Iteration 56/1000 | Loss: 0.00001134
Iteration 57/1000 | Loss: 0.00001134
Iteration 58/1000 | Loss: 0.00001134
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001128
Iteration 69/1000 | Loss: 0.00001128
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001127
Iteration 72/1000 | Loss: 0.00001127
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001127
Iteration 76/1000 | Loss: 0.00001127
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001127
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001126
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001126
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001125
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001122
Iteration 98/1000 | Loss: 0.00001122
Iteration 99/1000 | Loss: 0.00001122
Iteration 100/1000 | Loss: 0.00001121
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001121
Iteration 103/1000 | Loss: 0.00001121
Iteration 104/1000 | Loss: 0.00001121
Iteration 105/1000 | Loss: 0.00001121
Iteration 106/1000 | Loss: 0.00001121
Iteration 107/1000 | Loss: 0.00001121
Iteration 108/1000 | Loss: 0.00001121
Iteration 109/1000 | Loss: 0.00001120
Iteration 110/1000 | Loss: 0.00001120
Iteration 111/1000 | Loss: 0.00001120
Iteration 112/1000 | Loss: 0.00001120
Iteration 113/1000 | Loss: 0.00001120
Iteration 114/1000 | Loss: 0.00001120
Iteration 115/1000 | Loss: 0.00001120
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001120
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001119
Iteration 120/1000 | Loss: 0.00001119
Iteration 121/1000 | Loss: 0.00001119
Iteration 122/1000 | Loss: 0.00001119
Iteration 123/1000 | Loss: 0.00001119
Iteration 124/1000 | Loss: 0.00001119
Iteration 125/1000 | Loss: 0.00001119
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001119
Iteration 130/1000 | Loss: 0.00001119
Iteration 131/1000 | Loss: 0.00001119
Iteration 132/1000 | Loss: 0.00001119
Iteration 133/1000 | Loss: 0.00001118
Iteration 134/1000 | Loss: 0.00001118
Iteration 135/1000 | Loss: 0.00001118
Iteration 136/1000 | Loss: 0.00001118
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001118
Iteration 139/1000 | Loss: 0.00001118
Iteration 140/1000 | Loss: 0.00001118
Iteration 141/1000 | Loss: 0.00001118
Iteration 142/1000 | Loss: 0.00001118
Iteration 143/1000 | Loss: 0.00001118
Iteration 144/1000 | Loss: 0.00001118
Iteration 145/1000 | Loss: 0.00001118
Iteration 146/1000 | Loss: 0.00001118
Iteration 147/1000 | Loss: 0.00001118
Iteration 148/1000 | Loss: 0.00001118
Iteration 149/1000 | Loss: 0.00001118
Iteration 150/1000 | Loss: 0.00001118
Iteration 151/1000 | Loss: 0.00001118
Iteration 152/1000 | Loss: 0.00001118
Iteration 153/1000 | Loss: 0.00001118
Iteration 154/1000 | Loss: 0.00001118
Iteration 155/1000 | Loss: 0.00001118
Iteration 156/1000 | Loss: 0.00001118
Iteration 157/1000 | Loss: 0.00001118
Iteration 158/1000 | Loss: 0.00001118
Iteration 159/1000 | Loss: 0.00001118
Iteration 160/1000 | Loss: 0.00001118
Iteration 161/1000 | Loss: 0.00001118
Iteration 162/1000 | Loss: 0.00001118
Iteration 163/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1181808986293618e-05, 1.1181808986293618e-05, 1.1181808986293618e-05, 1.1181808986293618e-05, 1.1181808986293618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1181808986293618e-05

Optimization complete. Final v2v error: 2.8234975337982178 mm

Highest mean error: 3.068582773208618 mm for frame 55

Lowest mean error: 2.632319211959839 mm for frame 13

Saving results

Total time: 37.289151430130005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550232
Iteration 2/25 | Loss: 0.00108995
Iteration 3/25 | Loss: 0.00092008
Iteration 4/25 | Loss: 0.00087573
Iteration 5/25 | Loss: 0.00086241
Iteration 6/25 | Loss: 0.00086007
Iteration 7/25 | Loss: 0.00085954
Iteration 8/25 | Loss: 0.00085954
Iteration 9/25 | Loss: 0.00085954
Iteration 10/25 | Loss: 0.00085954
Iteration 11/25 | Loss: 0.00085954
Iteration 12/25 | Loss: 0.00085954
Iteration 13/25 | Loss: 0.00085954
Iteration 14/25 | Loss: 0.00085954
Iteration 15/25 | Loss: 0.00085954
Iteration 16/25 | Loss: 0.00085954
Iteration 17/25 | Loss: 0.00085954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008595362887717783, 0.0008595362887717783, 0.0008595362887717783, 0.0008595362887717783, 0.0008595362887717783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008595362887717783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.08715200
Iteration 2/25 | Loss: 0.00057528
Iteration 3/25 | Loss: 0.00057527
Iteration 4/25 | Loss: 0.00057527
Iteration 5/25 | Loss: 0.00057527
Iteration 6/25 | Loss: 0.00057527
Iteration 7/25 | Loss: 0.00057527
Iteration 8/25 | Loss: 0.00057527
Iteration 9/25 | Loss: 0.00057527
Iteration 10/25 | Loss: 0.00057527
Iteration 11/25 | Loss: 0.00057527
Iteration 12/25 | Loss: 0.00057527
Iteration 13/25 | Loss: 0.00057527
Iteration 14/25 | Loss: 0.00057527
Iteration 15/25 | Loss: 0.00057527
Iteration 16/25 | Loss: 0.00057527
Iteration 17/25 | Loss: 0.00057527
Iteration 18/25 | Loss: 0.00057527
Iteration 19/25 | Loss: 0.00057527
Iteration 20/25 | Loss: 0.00057527
Iteration 21/25 | Loss: 0.00057527
Iteration 22/25 | Loss: 0.00057527
Iteration 23/25 | Loss: 0.00057527
Iteration 24/25 | Loss: 0.00057527
Iteration 25/25 | Loss: 0.00057527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057527
Iteration 2/1000 | Loss: 0.00003921
Iteration 3/1000 | Loss: 0.00002637
Iteration 4/1000 | Loss: 0.00002454
Iteration 5/1000 | Loss: 0.00002308
Iteration 6/1000 | Loss: 0.00002244
Iteration 7/1000 | Loss: 0.00002182
Iteration 8/1000 | Loss: 0.00002148
Iteration 9/1000 | Loss: 0.00002121
Iteration 10/1000 | Loss: 0.00002097
Iteration 11/1000 | Loss: 0.00002092
Iteration 12/1000 | Loss: 0.00002091
Iteration 13/1000 | Loss: 0.00002074
Iteration 14/1000 | Loss: 0.00002069
Iteration 15/1000 | Loss: 0.00002057
Iteration 16/1000 | Loss: 0.00002057
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002054
Iteration 20/1000 | Loss: 0.00002054
Iteration 21/1000 | Loss: 0.00002053
Iteration 22/1000 | Loss: 0.00002053
Iteration 23/1000 | Loss: 0.00002052
Iteration 24/1000 | Loss: 0.00002052
Iteration 25/1000 | Loss: 0.00002051
Iteration 26/1000 | Loss: 0.00002050
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002048
Iteration 29/1000 | Loss: 0.00002048
Iteration 30/1000 | Loss: 0.00002047
Iteration 31/1000 | Loss: 0.00002047
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002033
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00002033
Iteration 45/1000 | Loss: 0.00002033
Iteration 46/1000 | Loss: 0.00002033
Iteration 47/1000 | Loss: 0.00002033
Iteration 48/1000 | Loss: 0.00002033
Iteration 49/1000 | Loss: 0.00002032
Iteration 50/1000 | Loss: 0.00002032
Iteration 51/1000 | Loss: 0.00002032
Iteration 52/1000 | Loss: 0.00002031
Iteration 53/1000 | Loss: 0.00002031
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002030
Iteration 57/1000 | Loss: 0.00002030
Iteration 58/1000 | Loss: 0.00002030
Iteration 59/1000 | Loss: 0.00002030
Iteration 60/1000 | Loss: 0.00002030
Iteration 61/1000 | Loss: 0.00002030
Iteration 62/1000 | Loss: 0.00002030
Iteration 63/1000 | Loss: 0.00002030
Iteration 64/1000 | Loss: 0.00002029
Iteration 65/1000 | Loss: 0.00002029
Iteration 66/1000 | Loss: 0.00002029
Iteration 67/1000 | Loss: 0.00002029
Iteration 68/1000 | Loss: 0.00002029
Iteration 69/1000 | Loss: 0.00002029
Iteration 70/1000 | Loss: 0.00002028
Iteration 71/1000 | Loss: 0.00002028
Iteration 72/1000 | Loss: 0.00002028
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002027
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002025
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002025
Iteration 91/1000 | Loss: 0.00002025
Iteration 92/1000 | Loss: 0.00002025
Iteration 93/1000 | Loss: 0.00002024
Iteration 94/1000 | Loss: 0.00002024
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002023
Iteration 99/1000 | Loss: 0.00002023
Iteration 100/1000 | Loss: 0.00002023
Iteration 101/1000 | Loss: 0.00002023
Iteration 102/1000 | Loss: 0.00002023
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002023
Iteration 105/1000 | Loss: 0.00002023
Iteration 106/1000 | Loss: 0.00002022
Iteration 107/1000 | Loss: 0.00002022
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002022
Iteration 110/1000 | Loss: 0.00002022
Iteration 111/1000 | Loss: 0.00002022
Iteration 112/1000 | Loss: 0.00002021
Iteration 113/1000 | Loss: 0.00002021
Iteration 114/1000 | Loss: 0.00002021
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00002021
Iteration 117/1000 | Loss: 0.00002021
Iteration 118/1000 | Loss: 0.00002021
Iteration 119/1000 | Loss: 0.00002020
Iteration 120/1000 | Loss: 0.00002020
Iteration 121/1000 | Loss: 0.00002020
Iteration 122/1000 | Loss: 0.00002020
Iteration 123/1000 | Loss: 0.00002020
Iteration 124/1000 | Loss: 0.00002019
Iteration 125/1000 | Loss: 0.00002019
Iteration 126/1000 | Loss: 0.00002019
Iteration 127/1000 | Loss: 0.00002019
Iteration 128/1000 | Loss: 0.00002019
Iteration 129/1000 | Loss: 0.00002019
Iteration 130/1000 | Loss: 0.00002019
Iteration 131/1000 | Loss: 0.00002019
Iteration 132/1000 | Loss: 0.00002019
Iteration 133/1000 | Loss: 0.00002019
Iteration 134/1000 | Loss: 0.00002019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.0191939256619662e-05, 2.0191939256619662e-05, 2.0191939256619662e-05, 2.0191939256619662e-05, 2.0191939256619662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0191939256619662e-05

Optimization complete. Final v2v error: 3.774994134902954 mm

Highest mean error: 4.314276695251465 mm for frame 0

Lowest mean error: 3.2284939289093018 mm for frame 88

Saving results

Total time: 40.41301894187927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544549
Iteration 2/25 | Loss: 0.00142036
Iteration 3/25 | Loss: 0.00097584
Iteration 4/25 | Loss: 0.00093873
Iteration 5/25 | Loss: 0.00093169
Iteration 6/25 | Loss: 0.00092935
Iteration 7/25 | Loss: 0.00092917
Iteration 8/25 | Loss: 0.00092917
Iteration 9/25 | Loss: 0.00092917
Iteration 10/25 | Loss: 0.00092917
Iteration 11/25 | Loss: 0.00092917
Iteration 12/25 | Loss: 0.00092917
Iteration 13/25 | Loss: 0.00092917
Iteration 14/25 | Loss: 0.00092917
Iteration 15/25 | Loss: 0.00092917
Iteration 16/25 | Loss: 0.00092917
Iteration 17/25 | Loss: 0.00092917
Iteration 18/25 | Loss: 0.00092917
Iteration 19/25 | Loss: 0.00092917
Iteration 20/25 | Loss: 0.00092917
Iteration 21/25 | Loss: 0.00092917
Iteration 22/25 | Loss: 0.00092917
Iteration 23/25 | Loss: 0.00092917
Iteration 24/25 | Loss: 0.00092917
Iteration 25/25 | Loss: 0.00092917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03079998
Iteration 2/25 | Loss: 0.00060598
Iteration 3/25 | Loss: 0.00060597
Iteration 4/25 | Loss: 0.00060597
Iteration 5/25 | Loss: 0.00060597
Iteration 6/25 | Loss: 0.00060597
Iteration 7/25 | Loss: 0.00060597
Iteration 8/25 | Loss: 0.00060597
Iteration 9/25 | Loss: 0.00060597
Iteration 10/25 | Loss: 0.00060597
Iteration 11/25 | Loss: 0.00060597
Iteration 12/25 | Loss: 0.00060597
Iteration 13/25 | Loss: 0.00060597
Iteration 14/25 | Loss: 0.00060597
Iteration 15/25 | Loss: 0.00060597
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000605972483754158, 0.000605972483754158, 0.000605972483754158, 0.000605972483754158, 0.000605972483754158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000605972483754158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060597
Iteration 2/1000 | Loss: 0.00005464
Iteration 3/1000 | Loss: 0.00003988
Iteration 4/1000 | Loss: 0.00003433
Iteration 5/1000 | Loss: 0.00003131
Iteration 6/1000 | Loss: 0.00002999
Iteration 7/1000 | Loss: 0.00002889
Iteration 8/1000 | Loss: 0.00002813
Iteration 9/1000 | Loss: 0.00002773
Iteration 10/1000 | Loss: 0.00002740
Iteration 11/1000 | Loss: 0.00002710
Iteration 12/1000 | Loss: 0.00002684
Iteration 13/1000 | Loss: 0.00002659
Iteration 14/1000 | Loss: 0.00002632
Iteration 15/1000 | Loss: 0.00002603
Iteration 16/1000 | Loss: 0.00002583
Iteration 17/1000 | Loss: 0.00002572
Iteration 18/1000 | Loss: 0.00002571
Iteration 19/1000 | Loss: 0.00002564
Iteration 20/1000 | Loss: 0.00002549
Iteration 21/1000 | Loss: 0.00002536
Iteration 22/1000 | Loss: 0.00002534
Iteration 23/1000 | Loss: 0.00002533
Iteration 24/1000 | Loss: 0.00002531
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002531
Iteration 27/1000 | Loss: 0.00002530
Iteration 28/1000 | Loss: 0.00002530
Iteration 29/1000 | Loss: 0.00002530
Iteration 30/1000 | Loss: 0.00002529
Iteration 31/1000 | Loss: 0.00002528
Iteration 32/1000 | Loss: 0.00002528
Iteration 33/1000 | Loss: 0.00002528
Iteration 34/1000 | Loss: 0.00002528
Iteration 35/1000 | Loss: 0.00002528
Iteration 36/1000 | Loss: 0.00002527
Iteration 37/1000 | Loss: 0.00002527
Iteration 38/1000 | Loss: 0.00002527
Iteration 39/1000 | Loss: 0.00002527
Iteration 40/1000 | Loss: 0.00002526
Iteration 41/1000 | Loss: 0.00002526
Iteration 42/1000 | Loss: 0.00002526
Iteration 43/1000 | Loss: 0.00002525
Iteration 44/1000 | Loss: 0.00002525
Iteration 45/1000 | Loss: 0.00002525
Iteration 46/1000 | Loss: 0.00002525
Iteration 47/1000 | Loss: 0.00002525
Iteration 48/1000 | Loss: 0.00002525
Iteration 49/1000 | Loss: 0.00002524
Iteration 50/1000 | Loss: 0.00002523
Iteration 51/1000 | Loss: 0.00002523
Iteration 52/1000 | Loss: 0.00002523
Iteration 53/1000 | Loss: 0.00002522
Iteration 54/1000 | Loss: 0.00002521
Iteration 55/1000 | Loss: 0.00002521
Iteration 56/1000 | Loss: 0.00002521
Iteration 57/1000 | Loss: 0.00002521
Iteration 58/1000 | Loss: 0.00002521
Iteration 59/1000 | Loss: 0.00002520
Iteration 60/1000 | Loss: 0.00002520
Iteration 61/1000 | Loss: 0.00002520
Iteration 62/1000 | Loss: 0.00002520
Iteration 63/1000 | Loss: 0.00002520
Iteration 64/1000 | Loss: 0.00002520
Iteration 65/1000 | Loss: 0.00002520
Iteration 66/1000 | Loss: 0.00002520
Iteration 67/1000 | Loss: 0.00002520
Iteration 68/1000 | Loss: 0.00002520
Iteration 69/1000 | Loss: 0.00002519
Iteration 70/1000 | Loss: 0.00002518
Iteration 71/1000 | Loss: 0.00002518
Iteration 72/1000 | Loss: 0.00002518
Iteration 73/1000 | Loss: 0.00002517
Iteration 74/1000 | Loss: 0.00002517
Iteration 75/1000 | Loss: 0.00002517
Iteration 76/1000 | Loss: 0.00002517
Iteration 77/1000 | Loss: 0.00002517
Iteration 78/1000 | Loss: 0.00002517
Iteration 79/1000 | Loss: 0.00002517
Iteration 80/1000 | Loss: 0.00002517
Iteration 81/1000 | Loss: 0.00002517
Iteration 82/1000 | Loss: 0.00002517
Iteration 83/1000 | Loss: 0.00002516
Iteration 84/1000 | Loss: 0.00002516
Iteration 85/1000 | Loss: 0.00002516
Iteration 86/1000 | Loss: 0.00002516
Iteration 87/1000 | Loss: 0.00002516
Iteration 88/1000 | Loss: 0.00002516
Iteration 89/1000 | Loss: 0.00002516
Iteration 90/1000 | Loss: 0.00002516
Iteration 91/1000 | Loss: 0.00002516
Iteration 92/1000 | Loss: 0.00002516
Iteration 93/1000 | Loss: 0.00002516
Iteration 94/1000 | Loss: 0.00002515
Iteration 95/1000 | Loss: 0.00002515
Iteration 96/1000 | Loss: 0.00002515
Iteration 97/1000 | Loss: 0.00002515
Iteration 98/1000 | Loss: 0.00002515
Iteration 99/1000 | Loss: 0.00002515
Iteration 100/1000 | Loss: 0.00002515
Iteration 101/1000 | Loss: 0.00002515
Iteration 102/1000 | Loss: 0.00002514
Iteration 103/1000 | Loss: 0.00002514
Iteration 104/1000 | Loss: 0.00002513
Iteration 105/1000 | Loss: 0.00002513
Iteration 106/1000 | Loss: 0.00002513
Iteration 107/1000 | Loss: 0.00002513
Iteration 108/1000 | Loss: 0.00002513
Iteration 109/1000 | Loss: 0.00002513
Iteration 110/1000 | Loss: 0.00002513
Iteration 111/1000 | Loss: 0.00002513
Iteration 112/1000 | Loss: 0.00002512
Iteration 113/1000 | Loss: 0.00002512
Iteration 114/1000 | Loss: 0.00002512
Iteration 115/1000 | Loss: 0.00002512
Iteration 116/1000 | Loss: 0.00002512
Iteration 117/1000 | Loss: 0.00002511
Iteration 118/1000 | Loss: 0.00002511
Iteration 119/1000 | Loss: 0.00002511
Iteration 120/1000 | Loss: 0.00002511
Iteration 121/1000 | Loss: 0.00002511
Iteration 122/1000 | Loss: 0.00002510
Iteration 123/1000 | Loss: 0.00002510
Iteration 124/1000 | Loss: 0.00002510
Iteration 125/1000 | Loss: 0.00002510
Iteration 126/1000 | Loss: 0.00002510
Iteration 127/1000 | Loss: 0.00002510
Iteration 128/1000 | Loss: 0.00002510
Iteration 129/1000 | Loss: 0.00002510
Iteration 130/1000 | Loss: 0.00002510
Iteration 131/1000 | Loss: 0.00002510
Iteration 132/1000 | Loss: 0.00002510
Iteration 133/1000 | Loss: 0.00002510
Iteration 134/1000 | Loss: 0.00002509
Iteration 135/1000 | Loss: 0.00002509
Iteration 136/1000 | Loss: 0.00002509
Iteration 137/1000 | Loss: 0.00002509
Iteration 138/1000 | Loss: 0.00002509
Iteration 139/1000 | Loss: 0.00002509
Iteration 140/1000 | Loss: 0.00002509
Iteration 141/1000 | Loss: 0.00002509
Iteration 142/1000 | Loss: 0.00002509
Iteration 143/1000 | Loss: 0.00002509
Iteration 144/1000 | Loss: 0.00002508
Iteration 145/1000 | Loss: 0.00002508
Iteration 146/1000 | Loss: 0.00002508
Iteration 147/1000 | Loss: 0.00002508
Iteration 148/1000 | Loss: 0.00002508
Iteration 149/1000 | Loss: 0.00002507
Iteration 150/1000 | Loss: 0.00002507
Iteration 151/1000 | Loss: 0.00002507
Iteration 152/1000 | Loss: 0.00002507
Iteration 153/1000 | Loss: 0.00002507
Iteration 154/1000 | Loss: 0.00002507
Iteration 155/1000 | Loss: 0.00002507
Iteration 156/1000 | Loss: 0.00002507
Iteration 157/1000 | Loss: 0.00002507
Iteration 158/1000 | Loss: 0.00002507
Iteration 159/1000 | Loss: 0.00002507
Iteration 160/1000 | Loss: 0.00002506
Iteration 161/1000 | Loss: 0.00002506
Iteration 162/1000 | Loss: 0.00002506
Iteration 163/1000 | Loss: 0.00002506
Iteration 164/1000 | Loss: 0.00002506
Iteration 165/1000 | Loss: 0.00002506
Iteration 166/1000 | Loss: 0.00002506
Iteration 167/1000 | Loss: 0.00002506
Iteration 168/1000 | Loss: 0.00002505
Iteration 169/1000 | Loss: 0.00002505
Iteration 170/1000 | Loss: 0.00002505
Iteration 171/1000 | Loss: 0.00002505
Iteration 172/1000 | Loss: 0.00002505
Iteration 173/1000 | Loss: 0.00002505
Iteration 174/1000 | Loss: 0.00002505
Iteration 175/1000 | Loss: 0.00002505
Iteration 176/1000 | Loss: 0.00002504
Iteration 177/1000 | Loss: 0.00002504
Iteration 178/1000 | Loss: 0.00002504
Iteration 179/1000 | Loss: 0.00002504
Iteration 180/1000 | Loss: 0.00002504
Iteration 181/1000 | Loss: 0.00002504
Iteration 182/1000 | Loss: 0.00002503
Iteration 183/1000 | Loss: 0.00002503
Iteration 184/1000 | Loss: 0.00002503
Iteration 185/1000 | Loss: 0.00002503
Iteration 186/1000 | Loss: 0.00002503
Iteration 187/1000 | Loss: 0.00002503
Iteration 188/1000 | Loss: 0.00002503
Iteration 189/1000 | Loss: 0.00002502
Iteration 190/1000 | Loss: 0.00002502
Iteration 191/1000 | Loss: 0.00002502
Iteration 192/1000 | Loss: 0.00002502
Iteration 193/1000 | Loss: 0.00002502
Iteration 194/1000 | Loss: 0.00002502
Iteration 195/1000 | Loss: 0.00002502
Iteration 196/1000 | Loss: 0.00002502
Iteration 197/1000 | Loss: 0.00002501
Iteration 198/1000 | Loss: 0.00002501
Iteration 199/1000 | Loss: 0.00002501
Iteration 200/1000 | Loss: 0.00002501
Iteration 201/1000 | Loss: 0.00002501
Iteration 202/1000 | Loss: 0.00002501
Iteration 203/1000 | Loss: 0.00002501
Iteration 204/1000 | Loss: 0.00002501
Iteration 205/1000 | Loss: 0.00002501
Iteration 206/1000 | Loss: 0.00002501
Iteration 207/1000 | Loss: 0.00002501
Iteration 208/1000 | Loss: 0.00002500
Iteration 209/1000 | Loss: 0.00002500
Iteration 210/1000 | Loss: 0.00002500
Iteration 211/1000 | Loss: 0.00002500
Iteration 212/1000 | Loss: 0.00002500
Iteration 213/1000 | Loss: 0.00002500
Iteration 214/1000 | Loss: 0.00002500
Iteration 215/1000 | Loss: 0.00002500
Iteration 216/1000 | Loss: 0.00002500
Iteration 217/1000 | Loss: 0.00002500
Iteration 218/1000 | Loss: 0.00002500
Iteration 219/1000 | Loss: 0.00002500
Iteration 220/1000 | Loss: 0.00002500
Iteration 221/1000 | Loss: 0.00002500
Iteration 222/1000 | Loss: 0.00002500
Iteration 223/1000 | Loss: 0.00002500
Iteration 224/1000 | Loss: 0.00002500
Iteration 225/1000 | Loss: 0.00002500
Iteration 226/1000 | Loss: 0.00002499
Iteration 227/1000 | Loss: 0.00002499
Iteration 228/1000 | Loss: 0.00002499
Iteration 229/1000 | Loss: 0.00002499
Iteration 230/1000 | Loss: 0.00002499
Iteration 231/1000 | Loss: 0.00002499
Iteration 232/1000 | Loss: 0.00002499
Iteration 233/1000 | Loss: 0.00002499
Iteration 234/1000 | Loss: 0.00002499
Iteration 235/1000 | Loss: 0.00002499
Iteration 236/1000 | Loss: 0.00002499
Iteration 237/1000 | Loss: 0.00002499
Iteration 238/1000 | Loss: 0.00002499
Iteration 239/1000 | Loss: 0.00002499
Iteration 240/1000 | Loss: 0.00002499
Iteration 241/1000 | Loss: 0.00002499
Iteration 242/1000 | Loss: 0.00002499
Iteration 243/1000 | Loss: 0.00002499
Iteration 244/1000 | Loss: 0.00002499
Iteration 245/1000 | Loss: 0.00002499
Iteration 246/1000 | Loss: 0.00002499
Iteration 247/1000 | Loss: 0.00002499
Iteration 248/1000 | Loss: 0.00002499
Iteration 249/1000 | Loss: 0.00002499
Iteration 250/1000 | Loss: 0.00002499
Iteration 251/1000 | Loss: 0.00002499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [2.498747198842466e-05, 2.498747198842466e-05, 2.498747198842466e-05, 2.498747198842466e-05, 2.498747198842466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.498747198842466e-05

Optimization complete. Final v2v error: 4.070476531982422 mm

Highest mean error: 5.508846282958984 mm for frame 18

Lowest mean error: 3.203360080718994 mm for frame 90

Saving results

Total time: 53.14317750930786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888859
Iteration 2/25 | Loss: 0.00207130
Iteration 3/25 | Loss: 0.00156697
Iteration 4/25 | Loss: 0.00142500
Iteration 5/25 | Loss: 0.00131881
Iteration 6/25 | Loss: 0.00108775
Iteration 7/25 | Loss: 0.00104517
Iteration 8/25 | Loss: 0.00102613
Iteration 9/25 | Loss: 0.00100628
Iteration 10/25 | Loss: 0.00100105
Iteration 11/25 | Loss: 0.00098875
Iteration 12/25 | Loss: 0.00098720
Iteration 13/25 | Loss: 0.00098562
Iteration 14/25 | Loss: 0.00098261
Iteration 15/25 | Loss: 0.00098223
Iteration 16/25 | Loss: 0.00098240
Iteration 17/25 | Loss: 0.00098138
Iteration 18/25 | Loss: 0.00098194
Iteration 19/25 | Loss: 0.00098223
Iteration 20/25 | Loss: 0.00098474
Iteration 21/25 | Loss: 0.00098193
Iteration 22/25 | Loss: 0.00098118
Iteration 23/25 | Loss: 0.00098073
Iteration 24/25 | Loss: 0.00098021
Iteration 25/25 | Loss: 0.00097948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51559067
Iteration 2/25 | Loss: 0.00065360
Iteration 3/25 | Loss: 0.00065358
Iteration 4/25 | Loss: 0.00065358
Iteration 5/25 | Loss: 0.00065358
Iteration 6/25 | Loss: 0.00065358
Iteration 7/25 | Loss: 0.00065358
Iteration 8/25 | Loss: 0.00065358
Iteration 9/25 | Loss: 0.00065357
Iteration 10/25 | Loss: 0.00065357
Iteration 11/25 | Loss: 0.00065357
Iteration 12/25 | Loss: 0.00065357
Iteration 13/25 | Loss: 0.00065357
Iteration 14/25 | Loss: 0.00065357
Iteration 15/25 | Loss: 0.00065357
Iteration 16/25 | Loss: 0.00065357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000653574476018548, 0.000653574476018548, 0.000653574476018548, 0.000653574476018548, 0.000653574476018548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000653574476018548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065357
Iteration 2/1000 | Loss: 0.00005931
Iteration 3/1000 | Loss: 0.00006504
Iteration 4/1000 | Loss: 0.00005920
Iteration 5/1000 | Loss: 0.00004198
Iteration 6/1000 | Loss: 0.00004151
Iteration 7/1000 | Loss: 0.00003873
Iteration 8/1000 | Loss: 0.00004447
Iteration 9/1000 | Loss: 0.00005931
Iteration 10/1000 | Loss: 0.00006809
Iteration 11/1000 | Loss: 0.00004596
Iteration 12/1000 | Loss: 0.00007227
Iteration 13/1000 | Loss: 0.00005431
Iteration 14/1000 | Loss: 0.00006152
Iteration 15/1000 | Loss: 0.00004781
Iteration 16/1000 | Loss: 0.00003866
Iteration 17/1000 | Loss: 0.00003090
Iteration 18/1000 | Loss: 0.00003153
Iteration 19/1000 | Loss: 0.00004385
Iteration 20/1000 | Loss: 0.00004299
Iteration 21/1000 | Loss: 0.00004188
Iteration 22/1000 | Loss: 0.00003891
Iteration 23/1000 | Loss: 0.00004537
Iteration 24/1000 | Loss: 0.00005410
Iteration 25/1000 | Loss: 0.00005120
Iteration 26/1000 | Loss: 0.00005675
Iteration 27/1000 | Loss: 0.00004629
Iteration 28/1000 | Loss: 0.00005798
Iteration 29/1000 | Loss: 0.00004925
Iteration 30/1000 | Loss: 0.00005381
Iteration 31/1000 | Loss: 0.00004300
Iteration 32/1000 | Loss: 0.00005512
Iteration 33/1000 | Loss: 0.00004424
Iteration 34/1000 | Loss: 0.00005400
Iteration 35/1000 | Loss: 0.00005064
Iteration 36/1000 | Loss: 0.00004847
Iteration 37/1000 | Loss: 0.00003125
Iteration 38/1000 | Loss: 0.00005357
Iteration 39/1000 | Loss: 0.00003116
Iteration 40/1000 | Loss: 0.00002440
Iteration 41/1000 | Loss: 0.00002258
Iteration 42/1000 | Loss: 0.00002157
Iteration 43/1000 | Loss: 0.00002096
Iteration 44/1000 | Loss: 0.00002027
Iteration 45/1000 | Loss: 0.00001990
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001977
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001973
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001971
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001970
Iteration 58/1000 | Loss: 0.00001970
Iteration 59/1000 | Loss: 0.00001969
Iteration 60/1000 | Loss: 0.00001969
Iteration 61/1000 | Loss: 0.00001969
Iteration 62/1000 | Loss: 0.00001969
Iteration 63/1000 | Loss: 0.00001969
Iteration 64/1000 | Loss: 0.00001968
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001966
Iteration 69/1000 | Loss: 0.00001966
Iteration 70/1000 | Loss: 0.00001966
Iteration 71/1000 | Loss: 0.00001966
Iteration 72/1000 | Loss: 0.00001966
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001965
Iteration 80/1000 | Loss: 0.00001965
Iteration 81/1000 | Loss: 0.00001965
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001964
Iteration 86/1000 | Loss: 0.00001964
Iteration 87/1000 | Loss: 0.00001964
Iteration 88/1000 | Loss: 0.00001964
Iteration 89/1000 | Loss: 0.00001964
Iteration 90/1000 | Loss: 0.00001964
Iteration 91/1000 | Loss: 0.00001964
Iteration 92/1000 | Loss: 0.00001964
Iteration 93/1000 | Loss: 0.00001964
Iteration 94/1000 | Loss: 0.00001963
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001959
Iteration 105/1000 | Loss: 0.00001959
Iteration 106/1000 | Loss: 0.00001959
Iteration 107/1000 | Loss: 0.00001959
Iteration 108/1000 | Loss: 0.00001959
Iteration 109/1000 | Loss: 0.00001959
Iteration 110/1000 | Loss: 0.00001958
Iteration 111/1000 | Loss: 0.00001957
Iteration 112/1000 | Loss: 0.00001956
Iteration 113/1000 | Loss: 0.00001956
Iteration 114/1000 | Loss: 0.00001954
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001953
Iteration 117/1000 | Loss: 0.00001953
Iteration 118/1000 | Loss: 0.00001953
Iteration 119/1000 | Loss: 0.00001953
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001951
Iteration 128/1000 | Loss: 0.00001950
Iteration 129/1000 | Loss: 0.00001950
Iteration 130/1000 | Loss: 0.00001950
Iteration 131/1000 | Loss: 0.00001949
Iteration 132/1000 | Loss: 0.00001949
Iteration 133/1000 | Loss: 0.00001949
Iteration 134/1000 | Loss: 0.00001949
Iteration 135/1000 | Loss: 0.00001949
Iteration 136/1000 | Loss: 0.00001949
Iteration 137/1000 | Loss: 0.00001948
Iteration 138/1000 | Loss: 0.00001948
Iteration 139/1000 | Loss: 0.00001948
Iteration 140/1000 | Loss: 0.00001948
Iteration 141/1000 | Loss: 0.00001948
Iteration 142/1000 | Loss: 0.00001948
Iteration 143/1000 | Loss: 0.00001948
Iteration 144/1000 | Loss: 0.00001948
Iteration 145/1000 | Loss: 0.00001947
Iteration 146/1000 | Loss: 0.00001947
Iteration 147/1000 | Loss: 0.00001947
Iteration 148/1000 | Loss: 0.00001947
Iteration 149/1000 | Loss: 0.00001947
Iteration 150/1000 | Loss: 0.00001947
Iteration 151/1000 | Loss: 0.00001947
Iteration 152/1000 | Loss: 0.00001946
Iteration 153/1000 | Loss: 0.00001946
Iteration 154/1000 | Loss: 0.00001946
Iteration 155/1000 | Loss: 0.00001946
Iteration 156/1000 | Loss: 0.00001946
Iteration 157/1000 | Loss: 0.00001946
Iteration 158/1000 | Loss: 0.00001946
Iteration 159/1000 | Loss: 0.00001945
Iteration 160/1000 | Loss: 0.00001945
Iteration 161/1000 | Loss: 0.00001945
Iteration 162/1000 | Loss: 0.00001945
Iteration 163/1000 | Loss: 0.00001945
Iteration 164/1000 | Loss: 0.00001945
Iteration 165/1000 | Loss: 0.00001945
Iteration 166/1000 | Loss: 0.00001945
Iteration 167/1000 | Loss: 0.00001944
Iteration 168/1000 | Loss: 0.00001944
Iteration 169/1000 | Loss: 0.00001944
Iteration 170/1000 | Loss: 0.00001944
Iteration 171/1000 | Loss: 0.00001944
Iteration 172/1000 | Loss: 0.00001944
Iteration 173/1000 | Loss: 0.00001944
Iteration 174/1000 | Loss: 0.00001944
Iteration 175/1000 | Loss: 0.00001943
Iteration 176/1000 | Loss: 0.00001943
Iteration 177/1000 | Loss: 0.00001943
Iteration 178/1000 | Loss: 0.00001942
Iteration 179/1000 | Loss: 0.00001942
Iteration 180/1000 | Loss: 0.00001942
Iteration 181/1000 | Loss: 0.00001942
Iteration 182/1000 | Loss: 0.00001942
Iteration 183/1000 | Loss: 0.00001942
Iteration 184/1000 | Loss: 0.00001942
Iteration 185/1000 | Loss: 0.00001942
Iteration 186/1000 | Loss: 0.00001942
Iteration 187/1000 | Loss: 0.00001941
Iteration 188/1000 | Loss: 0.00001941
Iteration 189/1000 | Loss: 0.00001941
Iteration 190/1000 | Loss: 0.00001941
Iteration 191/1000 | Loss: 0.00001941
Iteration 192/1000 | Loss: 0.00001941
Iteration 193/1000 | Loss: 0.00001941
Iteration 194/1000 | Loss: 0.00001941
Iteration 195/1000 | Loss: 0.00001941
Iteration 196/1000 | Loss: 0.00001941
Iteration 197/1000 | Loss: 0.00001941
Iteration 198/1000 | Loss: 0.00001941
Iteration 199/1000 | Loss: 0.00001941
Iteration 200/1000 | Loss: 0.00001941
Iteration 201/1000 | Loss: 0.00001941
Iteration 202/1000 | Loss: 0.00001941
Iteration 203/1000 | Loss: 0.00001941
Iteration 204/1000 | Loss: 0.00001941
Iteration 205/1000 | Loss: 0.00001941
Iteration 206/1000 | Loss: 0.00001941
Iteration 207/1000 | Loss: 0.00001941
Iteration 208/1000 | Loss: 0.00001941
Iteration 209/1000 | Loss: 0.00001941
Iteration 210/1000 | Loss: 0.00001941
Iteration 211/1000 | Loss: 0.00001941
Iteration 212/1000 | Loss: 0.00001941
Iteration 213/1000 | Loss: 0.00001941
Iteration 214/1000 | Loss: 0.00001941
Iteration 215/1000 | Loss: 0.00001941
Iteration 216/1000 | Loss: 0.00001941
Iteration 217/1000 | Loss: 0.00001941
Iteration 218/1000 | Loss: 0.00001941
Iteration 219/1000 | Loss: 0.00001941
Iteration 220/1000 | Loss: 0.00001941
Iteration 221/1000 | Loss: 0.00001941
Iteration 222/1000 | Loss: 0.00001941
Iteration 223/1000 | Loss: 0.00001941
Iteration 224/1000 | Loss: 0.00001941
Iteration 225/1000 | Loss: 0.00001941
Iteration 226/1000 | Loss: 0.00001941
Iteration 227/1000 | Loss: 0.00001941
Iteration 228/1000 | Loss: 0.00001941
Iteration 229/1000 | Loss: 0.00001941
Iteration 230/1000 | Loss: 0.00001941
Iteration 231/1000 | Loss: 0.00001941
Iteration 232/1000 | Loss: 0.00001941
Iteration 233/1000 | Loss: 0.00001941
Iteration 234/1000 | Loss: 0.00001941
Iteration 235/1000 | Loss: 0.00001941
Iteration 236/1000 | Loss: 0.00001941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.9410288587096147e-05, 1.9410288587096147e-05, 1.9410288587096147e-05, 1.9410288587096147e-05, 1.9410288587096147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9410288587096147e-05

Optimization complete. Final v2v error: 3.723043441772461 mm

Highest mean error: 4.3357014656066895 mm for frame 99

Lowest mean error: 3.582556962966919 mm for frame 73

Saving results

Total time: 138.5412540435791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034175
Iteration 2/25 | Loss: 0.00198447
Iteration 3/25 | Loss: 0.00134241
Iteration 4/25 | Loss: 0.00124627
Iteration 5/25 | Loss: 0.00119095
Iteration 6/25 | Loss: 0.00118349
Iteration 7/25 | Loss: 0.00115534
Iteration 8/25 | Loss: 0.00114313
Iteration 9/25 | Loss: 0.00112535
Iteration 10/25 | Loss: 0.00111091
Iteration 11/25 | Loss: 0.00111207
Iteration 12/25 | Loss: 0.00110748
Iteration 13/25 | Loss: 0.00110857
Iteration 14/25 | Loss: 0.00110719
Iteration 15/25 | Loss: 0.00113235
Iteration 16/25 | Loss: 0.00114649
Iteration 17/25 | Loss: 0.00113621
Iteration 18/25 | Loss: 0.00113559
Iteration 19/25 | Loss: 0.00112557
Iteration 20/25 | Loss: 0.00112780
Iteration 21/25 | Loss: 0.00112308
Iteration 22/25 | Loss: 0.00111601
Iteration 23/25 | Loss: 0.00111391
Iteration 24/25 | Loss: 0.00109611
Iteration 25/25 | Loss: 0.00108741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15370464
Iteration 2/25 | Loss: 0.01231384
Iteration 3/25 | Loss: 0.00275249
Iteration 4/25 | Loss: 0.00275249
Iteration 5/25 | Loss: 0.00275249
Iteration 6/25 | Loss: 0.00275249
Iteration 7/25 | Loss: 0.00275249
Iteration 8/25 | Loss: 0.00275249
Iteration 9/25 | Loss: 0.00275249
Iteration 10/25 | Loss: 0.00275249
Iteration 11/25 | Loss: 0.00275249
Iteration 12/25 | Loss: 0.00275249
Iteration 13/25 | Loss: 0.00275249
Iteration 14/25 | Loss: 0.00275249
Iteration 15/25 | Loss: 0.00275249
Iteration 16/25 | Loss: 0.00275249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002752485917881131, 0.002752485917881131, 0.002752485917881131, 0.002752485917881131, 0.002752485917881131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002752485917881131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275249
Iteration 2/1000 | Loss: 0.00457613
Iteration 3/1000 | Loss: 0.00326112
Iteration 4/1000 | Loss: 0.00315563
Iteration 5/1000 | Loss: 0.00298719
Iteration 6/1000 | Loss: 0.00315910
Iteration 7/1000 | Loss: 0.00228926
Iteration 8/1000 | Loss: 0.00294332
Iteration 9/1000 | Loss: 0.00276134
Iteration 10/1000 | Loss: 0.00332268
Iteration 11/1000 | Loss: 0.00174046
Iteration 12/1000 | Loss: 0.00395742
Iteration 13/1000 | Loss: 0.00279681
Iteration 14/1000 | Loss: 0.00283042
Iteration 15/1000 | Loss: 0.00299833
Iteration 16/1000 | Loss: 0.00303010
Iteration 17/1000 | Loss: 0.00306221
Iteration 18/1000 | Loss: 0.00273564
Iteration 19/1000 | Loss: 0.00306273
Iteration 20/1000 | Loss: 0.00509518
Iteration 21/1000 | Loss: 0.00369964
Iteration 22/1000 | Loss: 0.00221539
Iteration 23/1000 | Loss: 0.00190690
Iteration 24/1000 | Loss: 0.00330788
Iteration 25/1000 | Loss: 0.00391343
Iteration 26/1000 | Loss: 0.00304334
Iteration 27/1000 | Loss: 0.00146561
Iteration 28/1000 | Loss: 0.00600984
Iteration 29/1000 | Loss: 0.00716962
Iteration 30/1000 | Loss: 0.00201739
Iteration 31/1000 | Loss: 0.00469027
Iteration 32/1000 | Loss: 0.00169108
Iteration 33/1000 | Loss: 0.00128961
Iteration 34/1000 | Loss: 0.00094087
Iteration 35/1000 | Loss: 0.00180533
Iteration 36/1000 | Loss: 0.00167432
Iteration 37/1000 | Loss: 0.00136342
Iteration 38/1000 | Loss: 0.00176783
Iteration 39/1000 | Loss: 0.00113673
Iteration 40/1000 | Loss: 0.00141110
Iteration 41/1000 | Loss: 0.00118238
Iteration 42/1000 | Loss: 0.00146869
Iteration 43/1000 | Loss: 0.00285678
Iteration 44/1000 | Loss: 0.00289836
Iteration 45/1000 | Loss: 0.00256656
Iteration 46/1000 | Loss: 0.00156558
Iteration 47/1000 | Loss: 0.00175883
Iteration 48/1000 | Loss: 0.00442194
Iteration 49/1000 | Loss: 0.00148725
Iteration 50/1000 | Loss: 0.00130534
Iteration 51/1000 | Loss: 0.00186027
Iteration 52/1000 | Loss: 0.00159057
Iteration 53/1000 | Loss: 0.00460126
Iteration 54/1000 | Loss: 0.00172647
Iteration 55/1000 | Loss: 0.00159754
Iteration 56/1000 | Loss: 0.00219528
Iteration 57/1000 | Loss: 0.00193797
Iteration 58/1000 | Loss: 0.00346430
Iteration 59/1000 | Loss: 0.00193761
Iteration 60/1000 | Loss: 0.00175048
Iteration 61/1000 | Loss: 0.00376015
Iteration 62/1000 | Loss: 0.00116195
Iteration 63/1000 | Loss: 0.00177373
Iteration 64/1000 | Loss: 0.00239588
Iteration 65/1000 | Loss: 0.00356702
Iteration 66/1000 | Loss: 0.00440352
Iteration 67/1000 | Loss: 0.00751872
Iteration 68/1000 | Loss: 0.00315991
Iteration 69/1000 | Loss: 0.00174253
Iteration 70/1000 | Loss: 0.00171150
Iteration 71/1000 | Loss: 0.00139595
Iteration 72/1000 | Loss: 0.00177515
Iteration 73/1000 | Loss: 0.00708684
Iteration 74/1000 | Loss: 0.00215522
Iteration 75/1000 | Loss: 0.00259362
Iteration 76/1000 | Loss: 0.00269799
Iteration 77/1000 | Loss: 0.00282925
Iteration 78/1000 | Loss: 0.00149234
Iteration 79/1000 | Loss: 0.00109829
Iteration 80/1000 | Loss: 0.00112577
Iteration 81/1000 | Loss: 0.00131330
Iteration 82/1000 | Loss: 0.00149166
Iteration 83/1000 | Loss: 0.00128607
Iteration 84/1000 | Loss: 0.00149137
Iteration 85/1000 | Loss: 0.00162991
Iteration 86/1000 | Loss: 0.00164039
Iteration 87/1000 | Loss: 0.00159335
Iteration 88/1000 | Loss: 0.00199650
Iteration 89/1000 | Loss: 0.00342929
Iteration 90/1000 | Loss: 0.00223119
Iteration 91/1000 | Loss: 0.00115738
Iteration 92/1000 | Loss: 0.00134888
Iteration 93/1000 | Loss: 0.00192628
Iteration 94/1000 | Loss: 0.00156010
Iteration 95/1000 | Loss: 0.00216550
Iteration 96/1000 | Loss: 0.00182100
Iteration 97/1000 | Loss: 0.00153974
Iteration 98/1000 | Loss: 0.00217849
Iteration 99/1000 | Loss: 0.00136077
Iteration 100/1000 | Loss: 0.00382913
Iteration 101/1000 | Loss: 0.00156688
Iteration 102/1000 | Loss: 0.00210987
Iteration 103/1000 | Loss: 0.00116255
Iteration 104/1000 | Loss: 0.00134964
Iteration 105/1000 | Loss: 0.00141011
Iteration 106/1000 | Loss: 0.00111861
Iteration 107/1000 | Loss: 0.00225205
Iteration 108/1000 | Loss: 0.00102264
Iteration 109/1000 | Loss: 0.00130116
Iteration 110/1000 | Loss: 0.00104042
Iteration 111/1000 | Loss: 0.00151312
Iteration 112/1000 | Loss: 0.00270367
Iteration 113/1000 | Loss: 0.00165258
Iteration 114/1000 | Loss: 0.00117528
Iteration 115/1000 | Loss: 0.00099368
Iteration 116/1000 | Loss: 0.00140763
Iteration 117/1000 | Loss: 0.00160658
Iteration 118/1000 | Loss: 0.00136619
Iteration 119/1000 | Loss: 0.00154925
Iteration 120/1000 | Loss: 0.00187065
Iteration 121/1000 | Loss: 0.00294834
Iteration 122/1000 | Loss: 0.00193979
Iteration 123/1000 | Loss: 0.00183557
Iteration 124/1000 | Loss: 0.00123521
Iteration 125/1000 | Loss: 0.00145073
Iteration 126/1000 | Loss: 0.00120996
Iteration 127/1000 | Loss: 0.00327134
Iteration 128/1000 | Loss: 0.00265137
Iteration 129/1000 | Loss: 0.00270818
Iteration 130/1000 | Loss: 0.00209899
Iteration 131/1000 | Loss: 0.00131612
Iteration 132/1000 | Loss: 0.00138303
Iteration 133/1000 | Loss: 0.00221932
Iteration 134/1000 | Loss: 0.00208873
Iteration 135/1000 | Loss: 0.00265524
Iteration 136/1000 | Loss: 0.00464828
Iteration 137/1000 | Loss: 0.00129441
Iteration 138/1000 | Loss: 0.00141450
Iteration 139/1000 | Loss: 0.00136985
Iteration 140/1000 | Loss: 0.00115428
Iteration 141/1000 | Loss: 0.00104420
Iteration 142/1000 | Loss: 0.00146505
Iteration 143/1000 | Loss: 0.00142050
Iteration 144/1000 | Loss: 0.00120905
Iteration 145/1000 | Loss: 0.00112040
Iteration 146/1000 | Loss: 0.00105671
Iteration 147/1000 | Loss: 0.00111656
Iteration 148/1000 | Loss: 0.00121550
Iteration 149/1000 | Loss: 0.00131073
Iteration 150/1000 | Loss: 0.00137293
Iteration 151/1000 | Loss: 0.00182406
Iteration 152/1000 | Loss: 0.00154727
Iteration 153/1000 | Loss: 0.00137606
Iteration 154/1000 | Loss: 0.00125559
Iteration 155/1000 | Loss: 0.00182262
Iteration 156/1000 | Loss: 0.00153792
Iteration 157/1000 | Loss: 0.00256665
Iteration 158/1000 | Loss: 0.00186832
Iteration 159/1000 | Loss: 0.00264558
Iteration 160/1000 | Loss: 0.00201334
Iteration 161/1000 | Loss: 0.00142949
Iteration 162/1000 | Loss: 0.00167585
Iteration 163/1000 | Loss: 0.00174700
Iteration 164/1000 | Loss: 0.00225855
Iteration 165/1000 | Loss: 0.00164916
Iteration 166/1000 | Loss: 0.00234172
Iteration 167/1000 | Loss: 0.00111960
Iteration 168/1000 | Loss: 0.00104196
Iteration 169/1000 | Loss: 0.00137552
Iteration 170/1000 | Loss: 0.00122973
Iteration 171/1000 | Loss: 0.00074534
Iteration 172/1000 | Loss: 0.00063681
Iteration 173/1000 | Loss: 0.00123445
Iteration 174/1000 | Loss: 0.00049487
Iteration 175/1000 | Loss: 0.00139936
Iteration 176/1000 | Loss: 0.00079580
Iteration 177/1000 | Loss: 0.00094316
Iteration 178/1000 | Loss: 0.00175287
Iteration 179/1000 | Loss: 0.00062086
Iteration 180/1000 | Loss: 0.00036992
Iteration 181/1000 | Loss: 0.00041138
Iteration 182/1000 | Loss: 0.00078601
Iteration 183/1000 | Loss: 0.00070155
Iteration 184/1000 | Loss: 0.00098926
Iteration 185/1000 | Loss: 0.00191894
Iteration 186/1000 | Loss: 0.00066449
Iteration 187/1000 | Loss: 0.00055141
Iteration 188/1000 | Loss: 0.00058281
Iteration 189/1000 | Loss: 0.00059476
Iteration 190/1000 | Loss: 0.00106590
Iteration 191/1000 | Loss: 0.00048770
Iteration 192/1000 | Loss: 0.00105103
Iteration 193/1000 | Loss: 0.00078453
Iteration 194/1000 | Loss: 0.00054959
Iteration 195/1000 | Loss: 0.00121052
Iteration 196/1000 | Loss: 0.00093017
Iteration 197/1000 | Loss: 0.00044973
Iteration 198/1000 | Loss: 0.00083916
Iteration 199/1000 | Loss: 0.00063048
Iteration 200/1000 | Loss: 0.00044599
Iteration 201/1000 | Loss: 0.00060071
Iteration 202/1000 | Loss: 0.00039080
Iteration 203/1000 | Loss: 0.00072838
Iteration 204/1000 | Loss: 0.00061727
Iteration 205/1000 | Loss: 0.00054628
Iteration 206/1000 | Loss: 0.00211107
Iteration 207/1000 | Loss: 0.00090649
Iteration 208/1000 | Loss: 0.00072727
Iteration 209/1000 | Loss: 0.00045468
Iteration 210/1000 | Loss: 0.00068987
Iteration 211/1000 | Loss: 0.00289219
Iteration 212/1000 | Loss: 0.00067284
Iteration 213/1000 | Loss: 0.00081006
Iteration 214/1000 | Loss: 0.00085053
Iteration 215/1000 | Loss: 0.00111229
Iteration 216/1000 | Loss: 0.00221364
Iteration 217/1000 | Loss: 0.00242458
Iteration 218/1000 | Loss: 0.00110871
Iteration 219/1000 | Loss: 0.00086972
Iteration 220/1000 | Loss: 0.00054440
Iteration 221/1000 | Loss: 0.00043279
Iteration 222/1000 | Loss: 0.00060963
Iteration 223/1000 | Loss: 0.00063274
Iteration 224/1000 | Loss: 0.00059233
Iteration 225/1000 | Loss: 0.00081030
Iteration 226/1000 | Loss: 0.00078347
Iteration 227/1000 | Loss: 0.00081089
Iteration 228/1000 | Loss: 0.00359995
Iteration 229/1000 | Loss: 0.00206821
Iteration 230/1000 | Loss: 0.00054667
Iteration 231/1000 | Loss: 0.00088867
Iteration 232/1000 | Loss: 0.00084646
Iteration 233/1000 | Loss: 0.00123363
Iteration 234/1000 | Loss: 0.00067043
Iteration 235/1000 | Loss: 0.00041280
Iteration 236/1000 | Loss: 0.00098725
Iteration 237/1000 | Loss: 0.00429401
Iteration 238/1000 | Loss: 0.00076030
Iteration 239/1000 | Loss: 0.00049600
Iteration 240/1000 | Loss: 0.00043083
Iteration 241/1000 | Loss: 0.00049325
Iteration 242/1000 | Loss: 0.00044911
Iteration 243/1000 | Loss: 0.00072767
Iteration 244/1000 | Loss: 0.00256448
Iteration 245/1000 | Loss: 0.00099739
Iteration 246/1000 | Loss: 0.00072514
Iteration 247/1000 | Loss: 0.00135778
Iteration 248/1000 | Loss: 0.00046024
Iteration 249/1000 | Loss: 0.00051960
Iteration 250/1000 | Loss: 0.00061712
Iteration 251/1000 | Loss: 0.00113363
Iteration 252/1000 | Loss: 0.00087718
Iteration 253/1000 | Loss: 0.00062035
Iteration 254/1000 | Loss: 0.00056596
Iteration 255/1000 | Loss: 0.00062760
Iteration 256/1000 | Loss: 0.00117077
Iteration 257/1000 | Loss: 0.00124071
Iteration 258/1000 | Loss: 0.00166361
Iteration 259/1000 | Loss: 0.00184925
Iteration 260/1000 | Loss: 0.00110055
Iteration 261/1000 | Loss: 0.00074866
Iteration 262/1000 | Loss: 0.00073907
Iteration 263/1000 | Loss: 0.00093233
Iteration 264/1000 | Loss: 0.00099591
Iteration 265/1000 | Loss: 0.00078225
Iteration 266/1000 | Loss: 0.00083742
Iteration 267/1000 | Loss: 0.00084287
Iteration 268/1000 | Loss: 0.00093969
Iteration 269/1000 | Loss: 0.00097458
Iteration 270/1000 | Loss: 0.00132616
Iteration 271/1000 | Loss: 0.00110849
Iteration 272/1000 | Loss: 0.00115323
Iteration 273/1000 | Loss: 0.00107612
Iteration 274/1000 | Loss: 0.00099672
Iteration 275/1000 | Loss: 0.00092401
Iteration 276/1000 | Loss: 0.00088340
Iteration 277/1000 | Loss: 0.00066626
Iteration 278/1000 | Loss: 0.00103666
Iteration 279/1000 | Loss: 0.00078790
Iteration 280/1000 | Loss: 0.00181776
Iteration 281/1000 | Loss: 0.00098528
Iteration 282/1000 | Loss: 0.00097617
Iteration 283/1000 | Loss: 0.00261237
Iteration 284/1000 | Loss: 0.00054536
Iteration 285/1000 | Loss: 0.00051812
Iteration 286/1000 | Loss: 0.00054057
Iteration 287/1000 | Loss: 0.00055902
Iteration 288/1000 | Loss: 0.00067508
Iteration 289/1000 | Loss: 0.00080831
Iteration 290/1000 | Loss: 0.00092929
Iteration 291/1000 | Loss: 0.00077894
Iteration 292/1000 | Loss: 0.00099978
Iteration 293/1000 | Loss: 0.00083990
Iteration 294/1000 | Loss: 0.00173535
Iteration 295/1000 | Loss: 0.00087544
Iteration 296/1000 | Loss: 0.00089424
Iteration 297/1000 | Loss: 0.00077880
Iteration 298/1000 | Loss: 0.00072084
Iteration 299/1000 | Loss: 0.00068535
Iteration 300/1000 | Loss: 0.00082138
Iteration 301/1000 | Loss: 0.00098740
Iteration 302/1000 | Loss: 0.00026151
Iteration 303/1000 | Loss: 0.00070007
Iteration 304/1000 | Loss: 0.00136152
Iteration 305/1000 | Loss: 0.00103258
Iteration 306/1000 | Loss: 0.00132603
Iteration 307/1000 | Loss: 0.00129888
Iteration 308/1000 | Loss: 0.00095186
Iteration 309/1000 | Loss: 0.00082925
Iteration 310/1000 | Loss: 0.00100486
Iteration 311/1000 | Loss: 0.00097096
Iteration 312/1000 | Loss: 0.00103065
Iteration 313/1000 | Loss: 0.00091123
Iteration 314/1000 | Loss: 0.00123747
Iteration 315/1000 | Loss: 0.00091986
Iteration 316/1000 | Loss: 0.00083497
Iteration 317/1000 | Loss: 0.00105516
Iteration 318/1000 | Loss: 0.00070501
Iteration 319/1000 | Loss: 0.00062621
Iteration 320/1000 | Loss: 0.00048635
Iteration 321/1000 | Loss: 0.00058138
Iteration 322/1000 | Loss: 0.00040162
Iteration 323/1000 | Loss: 0.00141355
Iteration 324/1000 | Loss: 0.00083858
Iteration 325/1000 | Loss: 0.00068775
Iteration 326/1000 | Loss: 0.00167450
Iteration 327/1000 | Loss: 0.00068945
Iteration 328/1000 | Loss: 0.00078281
Iteration 329/1000 | Loss: 0.00205798
Iteration 330/1000 | Loss: 0.00044785
Iteration 331/1000 | Loss: 0.00044261
Iteration 332/1000 | Loss: 0.00074580
Iteration 333/1000 | Loss: 0.00053324
Iteration 334/1000 | Loss: 0.00059637
Iteration 335/1000 | Loss: 0.00061870
Iteration 336/1000 | Loss: 0.00061819
Iteration 337/1000 | Loss: 0.00077458
Iteration 338/1000 | Loss: 0.00047127
Iteration 339/1000 | Loss: 0.00111514
Iteration 340/1000 | Loss: 0.00062169
Iteration 341/1000 | Loss: 0.00061746
Iteration 342/1000 | Loss: 0.00039171
Iteration 343/1000 | Loss: 0.00045390
Iteration 344/1000 | Loss: 0.00042936
Iteration 345/1000 | Loss: 0.00044446
Iteration 346/1000 | Loss: 0.00069241
Iteration 347/1000 | Loss: 0.00047207
Iteration 348/1000 | Loss: 0.00077259
Iteration 349/1000 | Loss: 0.00037245
Iteration 350/1000 | Loss: 0.00086071
Iteration 351/1000 | Loss: 0.00076498
Iteration 352/1000 | Loss: 0.00070077
Iteration 353/1000 | Loss: 0.00069428
Iteration 354/1000 | Loss: 0.00089011
Iteration 355/1000 | Loss: 0.00076948
Iteration 356/1000 | Loss: 0.00058430
Iteration 357/1000 | Loss: 0.00070669
Iteration 358/1000 | Loss: 0.00063092
Iteration 359/1000 | Loss: 0.00157602
Iteration 360/1000 | Loss: 0.00077030
Iteration 361/1000 | Loss: 0.00055577
Iteration 362/1000 | Loss: 0.00046928
Iteration 363/1000 | Loss: 0.00077969
Iteration 364/1000 | Loss: 0.00119764
Iteration 365/1000 | Loss: 0.00074251
Iteration 366/1000 | Loss: 0.00056709
Iteration 367/1000 | Loss: 0.00057451
Iteration 368/1000 | Loss: 0.00073802
Iteration 369/1000 | Loss: 0.00038947
Iteration 370/1000 | Loss: 0.00041794
Iteration 371/1000 | Loss: 0.00032742
Iteration 372/1000 | Loss: 0.00024079
Iteration 373/1000 | Loss: 0.00053561
Iteration 374/1000 | Loss: 0.00033027
Iteration 375/1000 | Loss: 0.00088647
Iteration 376/1000 | Loss: 0.00034107
Iteration 377/1000 | Loss: 0.00016841
Iteration 378/1000 | Loss: 0.00033721
Iteration 379/1000 | Loss: 0.00025315
Iteration 380/1000 | Loss: 0.00030397
Iteration 381/1000 | Loss: 0.00048801
Iteration 382/1000 | Loss: 0.00043088
Iteration 383/1000 | Loss: 0.00080592
Iteration 384/1000 | Loss: 0.00066442
Iteration 385/1000 | Loss: 0.00054362
Iteration 386/1000 | Loss: 0.00016184
Iteration 387/1000 | Loss: 0.00027741
Iteration 388/1000 | Loss: 0.00034223
Iteration 389/1000 | Loss: 0.00033811
Iteration 390/1000 | Loss: 0.00038258
Iteration 391/1000 | Loss: 0.00050608
Iteration 392/1000 | Loss: 0.00053374
Iteration 393/1000 | Loss: 0.00032735
Iteration 394/1000 | Loss: 0.00042201
Iteration 395/1000 | Loss: 0.00040298
Iteration 396/1000 | Loss: 0.00031419
Iteration 397/1000 | Loss: 0.00032222
Iteration 398/1000 | Loss: 0.00036520
Iteration 399/1000 | Loss: 0.00045493
Iteration 400/1000 | Loss: 0.00054249
Iteration 401/1000 | Loss: 0.00034652
Iteration 402/1000 | Loss: 0.00027374
Iteration 403/1000 | Loss: 0.00032706
Iteration 404/1000 | Loss: 0.00031249
Iteration 405/1000 | Loss: 0.00066602
Iteration 406/1000 | Loss: 0.00051382
Iteration 407/1000 | Loss: 0.00059820
Iteration 408/1000 | Loss: 0.00053733
Iteration 409/1000 | Loss: 0.00036027
Iteration 410/1000 | Loss: 0.00037564
Iteration 411/1000 | Loss: 0.00021554
Iteration 412/1000 | Loss: 0.00067534
Iteration 413/1000 | Loss: 0.00042439
Iteration 414/1000 | Loss: 0.00033209
Iteration 415/1000 | Loss: 0.00057677
Iteration 416/1000 | Loss: 0.00049577
Iteration 417/1000 | Loss: 0.00045354
Iteration 418/1000 | Loss: 0.00037744
Iteration 419/1000 | Loss: 0.00032819
Iteration 420/1000 | Loss: 0.00049054
Iteration 421/1000 | Loss: 0.00081109
Iteration 422/1000 | Loss: 0.00064685
Iteration 423/1000 | Loss: 0.00063293
Iteration 424/1000 | Loss: 0.00016913
Iteration 425/1000 | Loss: 0.00039470
Iteration 426/1000 | Loss: 0.00027380
Iteration 427/1000 | Loss: 0.00029029
Iteration 428/1000 | Loss: 0.00046922
Iteration 429/1000 | Loss: 0.00058085
Iteration 430/1000 | Loss: 0.00048370
Iteration 431/1000 | Loss: 0.00045316
Iteration 432/1000 | Loss: 0.00063920
Iteration 433/1000 | Loss: 0.00055114
Iteration 434/1000 | Loss: 0.00065102
Iteration 435/1000 | Loss: 0.00039552
Iteration 436/1000 | Loss: 0.00056841
Iteration 437/1000 | Loss: 0.00060683
Iteration 438/1000 | Loss: 0.00060964
Iteration 439/1000 | Loss: 0.00059117
Iteration 440/1000 | Loss: 0.00041424
Iteration 441/1000 | Loss: 0.00024231
Iteration 442/1000 | Loss: 0.00026641
Iteration 443/1000 | Loss: 0.00051148
Iteration 444/1000 | Loss: 0.00048198
Iteration 445/1000 | Loss: 0.00055704
Iteration 446/1000 | Loss: 0.00053283
Iteration 447/1000 | Loss: 0.00060521
Iteration 448/1000 | Loss: 0.00053739
Iteration 449/1000 | Loss: 0.00028564
Iteration 450/1000 | Loss: 0.00050695
Iteration 451/1000 | Loss: 0.00048404
Iteration 452/1000 | Loss: 0.00036009
Iteration 453/1000 | Loss: 0.00034237
Iteration 454/1000 | Loss: 0.00034170
Iteration 455/1000 | Loss: 0.00052944
Iteration 456/1000 | Loss: 0.00036934
Iteration 457/1000 | Loss: 0.00050856
Iteration 458/1000 | Loss: 0.00051555
Iteration 459/1000 | Loss: 0.00056075
Iteration 460/1000 | Loss: 0.00046827
Iteration 461/1000 | Loss: 0.00014483
Iteration 462/1000 | Loss: 0.00018722
Iteration 463/1000 | Loss: 0.00027672
Iteration 464/1000 | Loss: 0.00017304
Iteration 465/1000 | Loss: 0.00100129
Iteration 466/1000 | Loss: 0.00032923
Iteration 467/1000 | Loss: 0.00018347
Iteration 468/1000 | Loss: 0.00096446
Iteration 469/1000 | Loss: 0.00048679
Iteration 470/1000 | Loss: 0.00050372
Iteration 471/1000 | Loss: 0.00076952
Iteration 472/1000 | Loss: 0.00039042
Iteration 473/1000 | Loss: 0.00146010
Iteration 474/1000 | Loss: 0.00023190
Iteration 475/1000 | Loss: 0.00026543
Iteration 476/1000 | Loss: 0.00097838
Iteration 477/1000 | Loss: 0.00035310
Iteration 478/1000 | Loss: 0.00038734
Iteration 479/1000 | Loss: 0.00043473
Iteration 480/1000 | Loss: 0.00038100
Iteration 481/1000 | Loss: 0.00055709
Iteration 482/1000 | Loss: 0.00043643
Iteration 483/1000 | Loss: 0.00103997
Iteration 484/1000 | Loss: 0.00076720
Iteration 485/1000 | Loss: 0.00019349
Iteration 486/1000 | Loss: 0.00031234
Iteration 487/1000 | Loss: 0.00014061
Iteration 488/1000 | Loss: 0.00019496
Iteration 489/1000 | Loss: 0.00021710
Iteration 490/1000 | Loss: 0.00017734
Iteration 491/1000 | Loss: 0.00016896
Iteration 492/1000 | Loss: 0.00024039
Iteration 493/1000 | Loss: 0.00022248
Iteration 494/1000 | Loss: 0.00016773
Iteration 495/1000 | Loss: 0.00016599
Iteration 496/1000 | Loss: 0.00018339
Iteration 497/1000 | Loss: 0.00020606
Iteration 498/1000 | Loss: 0.00023104
Iteration 499/1000 | Loss: 0.00015048
Iteration 500/1000 | Loss: 0.00032182
Iteration 501/1000 | Loss: 0.00022543
Iteration 502/1000 | Loss: 0.00040607
Iteration 503/1000 | Loss: 0.00028085
Iteration 504/1000 | Loss: 0.00006839
Iteration 505/1000 | Loss: 0.00006624
Iteration 506/1000 | Loss: 0.00004167
Iteration 507/1000 | Loss: 0.00030459
Iteration 508/1000 | Loss: 0.00051523
Iteration 509/1000 | Loss: 0.00022935
Iteration 510/1000 | Loss: 0.00021585
Iteration 511/1000 | Loss: 0.00008615
Iteration 512/1000 | Loss: 0.00011370
Iteration 513/1000 | Loss: 0.00063067
Iteration 514/1000 | Loss: 0.00072046
Iteration 515/1000 | Loss: 0.00036115
Iteration 516/1000 | Loss: 0.00005427
Iteration 517/1000 | Loss: 0.00014918
Iteration 518/1000 | Loss: 0.00014747
Iteration 519/1000 | Loss: 0.00016239
Iteration 520/1000 | Loss: 0.00014363
Iteration 521/1000 | Loss: 0.00030210
Iteration 522/1000 | Loss: 0.00016008
Iteration 523/1000 | Loss: 0.00016664
Iteration 524/1000 | Loss: 0.00016974
Iteration 525/1000 | Loss: 0.00016804
Iteration 526/1000 | Loss: 0.00021450
Iteration 527/1000 | Loss: 0.00015538
Iteration 528/1000 | Loss: 0.00017370
Iteration 529/1000 | Loss: 0.00016005
Iteration 530/1000 | Loss: 0.00016391
Iteration 531/1000 | Loss: 0.00010588
Iteration 532/1000 | Loss: 0.00023538
Iteration 533/1000 | Loss: 0.00229305
Iteration 534/1000 | Loss: 0.00065550
Iteration 535/1000 | Loss: 0.00034519
Iteration 536/1000 | Loss: 0.00034992
Iteration 537/1000 | Loss: 0.00017210
Iteration 538/1000 | Loss: 0.00003866
Iteration 539/1000 | Loss: 0.00003471
Iteration 540/1000 | Loss: 0.00011743
Iteration 541/1000 | Loss: 0.00009671
Iteration 542/1000 | Loss: 0.00003143
Iteration 543/1000 | Loss: 0.00002997
Iteration 544/1000 | Loss: 0.00002870
Iteration 545/1000 | Loss: 0.00013142
Iteration 546/1000 | Loss: 0.00012933
Iteration 547/1000 | Loss: 0.00012103
Iteration 548/1000 | Loss: 0.00021372
Iteration 549/1000 | Loss: 0.00005166
Iteration 550/1000 | Loss: 0.00013054
Iteration 551/1000 | Loss: 0.00007261
Iteration 552/1000 | Loss: 0.00002995
Iteration 553/1000 | Loss: 0.00007589
Iteration 554/1000 | Loss: 0.00010189
Iteration 555/1000 | Loss: 0.00002828
Iteration 556/1000 | Loss: 0.00002745
Iteration 557/1000 | Loss: 0.00002704
Iteration 558/1000 | Loss: 0.00030887
Iteration 559/1000 | Loss: 0.00012261
Iteration 560/1000 | Loss: 0.00045931
Iteration 561/1000 | Loss: 0.00035187
Iteration 562/1000 | Loss: 0.00016147
Iteration 563/1000 | Loss: 0.00013678
Iteration 564/1000 | Loss: 0.00006055
Iteration 565/1000 | Loss: 0.00015012
Iteration 566/1000 | Loss: 0.00005312
Iteration 567/1000 | Loss: 0.00003100
Iteration 568/1000 | Loss: 0.00020000
Iteration 569/1000 | Loss: 0.00009153
Iteration 570/1000 | Loss: 0.00004682
Iteration 571/1000 | Loss: 0.00003859
Iteration 572/1000 | Loss: 0.00002748
Iteration 573/1000 | Loss: 0.00002696
Iteration 574/1000 | Loss: 0.00015276
Iteration 575/1000 | Loss: 0.00008878
Iteration 576/1000 | Loss: 0.00007703
Iteration 577/1000 | Loss: 0.00003159
Iteration 578/1000 | Loss: 0.00003023
Iteration 579/1000 | Loss: 0.00002870
Iteration 580/1000 | Loss: 0.00002805
Iteration 581/1000 | Loss: 0.00002762
Iteration 582/1000 | Loss: 0.00002696
Iteration 583/1000 | Loss: 0.00002651
Iteration 584/1000 | Loss: 0.00002605
Iteration 585/1000 | Loss: 0.00009196
Iteration 586/1000 | Loss: 0.00028613
Iteration 587/1000 | Loss: 0.00014700
Iteration 588/1000 | Loss: 0.00009512
Iteration 589/1000 | Loss: 0.00006909
Iteration 590/1000 | Loss: 0.00004194
Iteration 591/1000 | Loss: 0.00008445
Iteration 592/1000 | Loss: 0.00012657
Iteration 593/1000 | Loss: 0.00019979
Iteration 594/1000 | Loss: 0.00009867
Iteration 595/1000 | Loss: 0.00026653
Iteration 596/1000 | Loss: 0.00020378
Iteration 597/1000 | Loss: 0.00025658
Iteration 598/1000 | Loss: 0.00019810
Iteration 599/1000 | Loss: 0.00021258
Iteration 600/1000 | Loss: 0.00016585
Iteration 601/1000 | Loss: 0.00002856
Iteration 602/1000 | Loss: 0.00017185
Iteration 603/1000 | Loss: 0.00019654
Iteration 604/1000 | Loss: 0.00036180
Iteration 605/1000 | Loss: 0.00018773
Iteration 606/1000 | Loss: 0.00009048
Iteration 607/1000 | Loss: 0.00003015
Iteration 608/1000 | Loss: 0.00002815
Iteration 609/1000 | Loss: 0.00002712
Iteration 610/1000 | Loss: 0.00002618
Iteration 611/1000 | Loss: 0.00022973
Iteration 612/1000 | Loss: 0.00021994
Iteration 613/1000 | Loss: 0.00022697
Iteration 614/1000 | Loss: 0.00021114
Iteration 615/1000 | Loss: 0.00033647
Iteration 616/1000 | Loss: 0.00016167
Iteration 617/1000 | Loss: 0.00022933
Iteration 618/1000 | Loss: 0.00022747
Iteration 619/1000 | Loss: 0.00023604
Iteration 620/1000 | Loss: 0.00024429
Iteration 621/1000 | Loss: 0.00019159
Iteration 622/1000 | Loss: 0.00045225
Iteration 623/1000 | Loss: 0.00032357
Iteration 624/1000 | Loss: 0.00003363
Iteration 625/1000 | Loss: 0.00002916
Iteration 626/1000 | Loss: 0.00002699
Iteration 627/1000 | Loss: 0.00002617
Iteration 628/1000 | Loss: 0.00002511
Iteration 629/1000 | Loss: 0.00002451
Iteration 630/1000 | Loss: 0.00002413
Iteration 631/1000 | Loss: 0.00002387
Iteration 632/1000 | Loss: 0.00002358
Iteration 633/1000 | Loss: 0.00002345
Iteration 634/1000 | Loss: 0.00002342
Iteration 635/1000 | Loss: 0.00002320
Iteration 636/1000 | Loss: 0.00002319
Iteration 637/1000 | Loss: 0.00002307
Iteration 638/1000 | Loss: 0.00002305
Iteration 639/1000 | Loss: 0.00002289
Iteration 640/1000 | Loss: 0.00002289
Iteration 641/1000 | Loss: 0.00002284
Iteration 642/1000 | Loss: 0.00002280
Iteration 643/1000 | Loss: 0.00002280
Iteration 644/1000 | Loss: 0.00002275
Iteration 645/1000 | Loss: 0.00002271
Iteration 646/1000 | Loss: 0.00002271
Iteration 647/1000 | Loss: 0.00002271
Iteration 648/1000 | Loss: 0.00002271
Iteration 649/1000 | Loss: 0.00002270
Iteration 650/1000 | Loss: 0.00002270
Iteration 651/1000 | Loss: 0.00002270
Iteration 652/1000 | Loss: 0.00002270
Iteration 653/1000 | Loss: 0.00002269
Iteration 654/1000 | Loss: 0.00002269
Iteration 655/1000 | Loss: 0.00002269
Iteration 656/1000 | Loss: 0.00002269
Iteration 657/1000 | Loss: 0.00002269
Iteration 658/1000 | Loss: 0.00002269
Iteration 659/1000 | Loss: 0.00002269
Iteration 660/1000 | Loss: 0.00002268
Iteration 661/1000 | Loss: 0.00002268
Iteration 662/1000 | Loss: 0.00002268
Iteration 663/1000 | Loss: 0.00002268
Iteration 664/1000 | Loss: 0.00002268
Iteration 665/1000 | Loss: 0.00002268
Iteration 666/1000 | Loss: 0.00002268
Iteration 667/1000 | Loss: 0.00002268
Iteration 668/1000 | Loss: 0.00002268
Iteration 669/1000 | Loss: 0.00002268
Iteration 670/1000 | Loss: 0.00002268
Iteration 671/1000 | Loss: 0.00002267
Iteration 672/1000 | Loss: 0.00002267
Iteration 673/1000 | Loss: 0.00002267
Iteration 674/1000 | Loss: 0.00002267
Iteration 675/1000 | Loss: 0.00002266
Iteration 676/1000 | Loss: 0.00002266
Iteration 677/1000 | Loss: 0.00002266
Iteration 678/1000 | Loss: 0.00002266
Iteration 679/1000 | Loss: 0.00002265
Iteration 680/1000 | Loss: 0.00002265
Iteration 681/1000 | Loss: 0.00002264
Iteration 682/1000 | Loss: 0.00002264
Iteration 683/1000 | Loss: 0.00002264
Iteration 684/1000 | Loss: 0.00002263
Iteration 685/1000 | Loss: 0.00002263
Iteration 686/1000 | Loss: 0.00002263
Iteration 687/1000 | Loss: 0.00002263
Iteration 688/1000 | Loss: 0.00002263
Iteration 689/1000 | Loss: 0.00002263
Iteration 690/1000 | Loss: 0.00002262
Iteration 691/1000 | Loss: 0.00002262
Iteration 692/1000 | Loss: 0.00002262
Iteration 693/1000 | Loss: 0.00002262
Iteration 694/1000 | Loss: 0.00002262
Iteration 695/1000 | Loss: 0.00002262
Iteration 696/1000 | Loss: 0.00002261
Iteration 697/1000 | Loss: 0.00002261
Iteration 698/1000 | Loss: 0.00002261
Iteration 699/1000 | Loss: 0.00002261
Iteration 700/1000 | Loss: 0.00002260
Iteration 701/1000 | Loss: 0.00002260
Iteration 702/1000 | Loss: 0.00002260
Iteration 703/1000 | Loss: 0.00002260
Iteration 704/1000 | Loss: 0.00002260
Iteration 705/1000 | Loss: 0.00002260
Iteration 706/1000 | Loss: 0.00002259
Iteration 707/1000 | Loss: 0.00002259
Iteration 708/1000 | Loss: 0.00002259
Iteration 709/1000 | Loss: 0.00002259
Iteration 710/1000 | Loss: 0.00002259
Iteration 711/1000 | Loss: 0.00002259
Iteration 712/1000 | Loss: 0.00002259
Iteration 713/1000 | Loss: 0.00002259
Iteration 714/1000 | Loss: 0.00002259
Iteration 715/1000 | Loss: 0.00002259
Iteration 716/1000 | Loss: 0.00002258
Iteration 717/1000 | Loss: 0.00002257
Iteration 718/1000 | Loss: 0.00002257
Iteration 719/1000 | Loss: 0.00002256
Iteration 720/1000 | Loss: 0.00002256
Iteration 721/1000 | Loss: 0.00002255
Iteration 722/1000 | Loss: 0.00002254
Iteration 723/1000 | Loss: 0.00002254
Iteration 724/1000 | Loss: 0.00002254
Iteration 725/1000 | Loss: 0.00002253
Iteration 726/1000 | Loss: 0.00002253
Iteration 727/1000 | Loss: 0.00002253
Iteration 728/1000 | Loss: 0.00002252
Iteration 729/1000 | Loss: 0.00002252
Iteration 730/1000 | Loss: 0.00002252
Iteration 731/1000 | Loss: 0.00002252
Iteration 732/1000 | Loss: 0.00002251
Iteration 733/1000 | Loss: 0.00002251
Iteration 734/1000 | Loss: 0.00002251
Iteration 735/1000 | Loss: 0.00002251
Iteration 736/1000 | Loss: 0.00002251
Iteration 737/1000 | Loss: 0.00002251
Iteration 738/1000 | Loss: 0.00002251
Iteration 739/1000 | Loss: 0.00002251
Iteration 740/1000 | Loss: 0.00002250
Iteration 741/1000 | Loss: 0.00002250
Iteration 742/1000 | Loss: 0.00002250
Iteration 743/1000 | Loss: 0.00002250
Iteration 744/1000 | Loss: 0.00002250
Iteration 745/1000 | Loss: 0.00002250
Iteration 746/1000 | Loss: 0.00002250
Iteration 747/1000 | Loss: 0.00002250
Iteration 748/1000 | Loss: 0.00002250
Iteration 749/1000 | Loss: 0.00002250
Iteration 750/1000 | Loss: 0.00002250
Iteration 751/1000 | Loss: 0.00002250
Iteration 752/1000 | Loss: 0.00002250
Iteration 753/1000 | Loss: 0.00002250
Iteration 754/1000 | Loss: 0.00002250
Iteration 755/1000 | Loss: 0.00002250
Iteration 756/1000 | Loss: 0.00002250
Iteration 757/1000 | Loss: 0.00002249
Iteration 758/1000 | Loss: 0.00002249
Iteration 759/1000 | Loss: 0.00002249
Iteration 760/1000 | Loss: 0.00002249
Iteration 761/1000 | Loss: 0.00002249
Iteration 762/1000 | Loss: 0.00002249
Iteration 763/1000 | Loss: 0.00002249
Iteration 764/1000 | Loss: 0.00002249
Iteration 765/1000 | Loss: 0.00002249
Iteration 766/1000 | Loss: 0.00002249
Iteration 767/1000 | Loss: 0.00002249
Iteration 768/1000 | Loss: 0.00002249
Iteration 769/1000 | Loss: 0.00002249
Iteration 770/1000 | Loss: 0.00002249
Iteration 771/1000 | Loss: 0.00002249
Iteration 772/1000 | Loss: 0.00002249
Iteration 773/1000 | Loss: 0.00002249
Iteration 774/1000 | Loss: 0.00002249
Iteration 775/1000 | Loss: 0.00002249
Iteration 776/1000 | Loss: 0.00002248
Iteration 777/1000 | Loss: 0.00002248
Iteration 778/1000 | Loss: 0.00002248
Iteration 779/1000 | Loss: 0.00002248
Iteration 780/1000 | Loss: 0.00002248
Iteration 781/1000 | Loss: 0.00002247
Iteration 782/1000 | Loss: 0.00002247
Iteration 783/1000 | Loss: 0.00002247
Iteration 784/1000 | Loss: 0.00002247
Iteration 785/1000 | Loss: 0.00002247
Iteration 786/1000 | Loss: 0.00002247
Iteration 787/1000 | Loss: 0.00002246
Iteration 788/1000 | Loss: 0.00002246
Iteration 789/1000 | Loss: 0.00002246
Iteration 790/1000 | Loss: 0.00002245
Iteration 791/1000 | Loss: 0.00002245
Iteration 792/1000 | Loss: 0.00002245
Iteration 793/1000 | Loss: 0.00002245
Iteration 794/1000 | Loss: 0.00002245
Iteration 795/1000 | Loss: 0.00002245
Iteration 796/1000 | Loss: 0.00002245
Iteration 797/1000 | Loss: 0.00002245
Iteration 798/1000 | Loss: 0.00002245
Iteration 799/1000 | Loss: 0.00002244
Iteration 800/1000 | Loss: 0.00002244
Iteration 801/1000 | Loss: 0.00002244
Iteration 802/1000 | Loss: 0.00002244
Iteration 803/1000 | Loss: 0.00002244
Iteration 804/1000 | Loss: 0.00002244
Iteration 805/1000 | Loss: 0.00002244
Iteration 806/1000 | Loss: 0.00002244
Iteration 807/1000 | Loss: 0.00002244
Iteration 808/1000 | Loss: 0.00002244
Iteration 809/1000 | Loss: 0.00002243
Iteration 810/1000 | Loss: 0.00002243
Iteration 811/1000 | Loss: 0.00002243
Iteration 812/1000 | Loss: 0.00002243
Iteration 813/1000 | Loss: 0.00002243
Iteration 814/1000 | Loss: 0.00002243
Iteration 815/1000 | Loss: 0.00002243
Iteration 816/1000 | Loss: 0.00002243
Iteration 817/1000 | Loss: 0.00002243
Iteration 818/1000 | Loss: 0.00002243
Iteration 819/1000 | Loss: 0.00002243
Iteration 820/1000 | Loss: 0.00002243
Iteration 821/1000 | Loss: 0.00002243
Iteration 822/1000 | Loss: 0.00002243
Iteration 823/1000 | Loss: 0.00002243
Iteration 824/1000 | Loss: 0.00002243
Iteration 825/1000 | Loss: 0.00002243
Iteration 826/1000 | Loss: 0.00002243
Iteration 827/1000 | Loss: 0.00002243
Iteration 828/1000 | Loss: 0.00002243
Iteration 829/1000 | Loss: 0.00002243
Iteration 830/1000 | Loss: 0.00002243
Iteration 831/1000 | Loss: 0.00002243
Iteration 832/1000 | Loss: 0.00002243
Iteration 833/1000 | Loss: 0.00002243
Iteration 834/1000 | Loss: 0.00002243
Iteration 835/1000 | Loss: 0.00002243
Iteration 836/1000 | Loss: 0.00002243
Iteration 837/1000 | Loss: 0.00002243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 837. Stopping optimization.
Last 5 losses: [2.2425356291932985e-05, 2.2425356291932985e-05, 2.2425356291932985e-05, 2.2425356291932985e-05, 2.2425356291932985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2425356291932985e-05

Optimization complete. Final v2v error: 3.9300341606140137 mm

Highest mean error: 5.275964260101318 mm for frame 214

Lowest mean error: 3.5318846702575684 mm for frame 232

Saving results

Total time: 1062.9306371212006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054059
Iteration 2/25 | Loss: 0.00281768
Iteration 3/25 | Loss: 0.00229364
Iteration 4/25 | Loss: 0.00207892
Iteration 5/25 | Loss: 0.00200137
Iteration 6/25 | Loss: 0.00182564
Iteration 7/25 | Loss: 0.00172778
Iteration 8/25 | Loss: 0.00165056
Iteration 9/25 | Loss: 0.00156831
Iteration 10/25 | Loss: 0.00151842
Iteration 11/25 | Loss: 0.00148003
Iteration 12/25 | Loss: 0.00141898
Iteration 13/25 | Loss: 0.00138513
Iteration 14/25 | Loss: 0.00137473
Iteration 15/25 | Loss: 0.00135316
Iteration 16/25 | Loss: 0.00130572
Iteration 17/25 | Loss: 0.00129015
Iteration 18/25 | Loss: 0.00127031
Iteration 19/25 | Loss: 0.00127180
Iteration 20/25 | Loss: 0.00126979
Iteration 21/25 | Loss: 0.00125616
Iteration 22/25 | Loss: 0.00124777
Iteration 23/25 | Loss: 0.00124523
Iteration 24/25 | Loss: 0.00124697
Iteration 25/25 | Loss: 0.00124463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49717021
Iteration 2/25 | Loss: 0.00395038
Iteration 3/25 | Loss: 0.00395035
Iteration 4/25 | Loss: 0.00395035
Iteration 5/25 | Loss: 0.00395035
Iteration 6/25 | Loss: 0.00395035
Iteration 7/25 | Loss: 0.00395035
Iteration 8/25 | Loss: 0.00395035
Iteration 9/25 | Loss: 0.00395035
Iteration 10/25 | Loss: 0.00395035
Iteration 11/25 | Loss: 0.00395035
Iteration 12/25 | Loss: 0.00395035
Iteration 13/25 | Loss: 0.00395035
Iteration 14/25 | Loss: 0.00395035
Iteration 15/25 | Loss: 0.00395035
Iteration 16/25 | Loss: 0.00395035
Iteration 17/25 | Loss: 0.00395035
Iteration 18/25 | Loss: 0.00395035
Iteration 19/25 | Loss: 0.00395035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003950347658246756, 0.003950347658246756, 0.003950347658246756, 0.003950347658246756, 0.003950347658246756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003950347658246756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395035
Iteration 2/1000 | Loss: 0.00951822
Iteration 3/1000 | Loss: 0.00060452
Iteration 4/1000 | Loss: 0.00047399
Iteration 5/1000 | Loss: 0.00042785
Iteration 6/1000 | Loss: 0.00023132
Iteration 7/1000 | Loss: 0.00016218
Iteration 8/1000 | Loss: 0.00013051
Iteration 9/1000 | Loss: 0.00011490
Iteration 10/1000 | Loss: 0.00010526
Iteration 11/1000 | Loss: 0.00009772
Iteration 12/1000 | Loss: 0.00009105
Iteration 13/1000 | Loss: 0.00008787
Iteration 14/1000 | Loss: 0.00008357
Iteration 15/1000 | Loss: 0.00064597
Iteration 16/1000 | Loss: 0.00009020
Iteration 17/1000 | Loss: 0.00007706
Iteration 18/1000 | Loss: 0.00007097
Iteration 19/1000 | Loss: 0.00006609
Iteration 20/1000 | Loss: 0.00006250
Iteration 21/1000 | Loss: 0.00006044
Iteration 22/1000 | Loss: 0.00005933
Iteration 23/1000 | Loss: 0.00005836
Iteration 24/1000 | Loss: 0.00005756
Iteration 25/1000 | Loss: 0.00005713
Iteration 26/1000 | Loss: 0.00005666
Iteration 27/1000 | Loss: 0.00005627
Iteration 28/1000 | Loss: 0.00005607
Iteration 29/1000 | Loss: 0.00005594
Iteration 30/1000 | Loss: 0.00005592
Iteration 31/1000 | Loss: 0.00005583
Iteration 32/1000 | Loss: 0.00005570
Iteration 33/1000 | Loss: 0.00005570
Iteration 34/1000 | Loss: 0.00005567
Iteration 35/1000 | Loss: 0.00005565
Iteration 36/1000 | Loss: 0.00005564
Iteration 37/1000 | Loss: 0.00005563
Iteration 38/1000 | Loss: 0.00005559
Iteration 39/1000 | Loss: 0.00005559
Iteration 40/1000 | Loss: 0.00005559
Iteration 41/1000 | Loss: 0.00005558
Iteration 42/1000 | Loss: 0.00005557
Iteration 43/1000 | Loss: 0.00005557
Iteration 44/1000 | Loss: 0.00005557
Iteration 45/1000 | Loss: 0.00005557
Iteration 46/1000 | Loss: 0.00005557
Iteration 47/1000 | Loss: 0.00005557
Iteration 48/1000 | Loss: 0.00005557
Iteration 49/1000 | Loss: 0.00005557
Iteration 50/1000 | Loss: 0.00005557
Iteration 51/1000 | Loss: 0.00005557
Iteration 52/1000 | Loss: 0.00005557
Iteration 53/1000 | Loss: 0.00005556
Iteration 54/1000 | Loss: 0.00005556
Iteration 55/1000 | Loss: 0.00005556
Iteration 56/1000 | Loss: 0.00005555
Iteration 57/1000 | Loss: 0.00005555
Iteration 58/1000 | Loss: 0.00005555
Iteration 59/1000 | Loss: 0.00005555
Iteration 60/1000 | Loss: 0.00005555
Iteration 61/1000 | Loss: 0.00005555
Iteration 62/1000 | Loss: 0.00005555
Iteration 63/1000 | Loss: 0.00005555
Iteration 64/1000 | Loss: 0.00005555
Iteration 65/1000 | Loss: 0.00005555
Iteration 66/1000 | Loss: 0.00005555
Iteration 67/1000 | Loss: 0.00039315
Iteration 68/1000 | Loss: 0.00448926
Iteration 69/1000 | Loss: 0.00216408
Iteration 70/1000 | Loss: 0.00202488
Iteration 71/1000 | Loss: 0.00197549
Iteration 72/1000 | Loss: 0.00021761
Iteration 73/1000 | Loss: 0.00178322
Iteration 74/1000 | Loss: 0.00014672
Iteration 75/1000 | Loss: 0.00026085
Iteration 76/1000 | Loss: 0.00076313
Iteration 77/1000 | Loss: 0.00007574
Iteration 78/1000 | Loss: 0.00005263
Iteration 79/1000 | Loss: 0.00004172
Iteration 80/1000 | Loss: 0.00003627
Iteration 81/1000 | Loss: 0.00003038
Iteration 82/1000 | Loss: 0.00002765
Iteration 83/1000 | Loss: 0.00038236
Iteration 84/1000 | Loss: 0.00003095
Iteration 85/1000 | Loss: 0.00002778
Iteration 86/1000 | Loss: 0.00002641
Iteration 87/1000 | Loss: 0.00002398
Iteration 88/1000 | Loss: 0.00002220
Iteration 89/1000 | Loss: 0.00002081
Iteration 90/1000 | Loss: 0.00002021
Iteration 91/1000 | Loss: 0.00002006
Iteration 92/1000 | Loss: 0.00001982
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001944
Iteration 95/1000 | Loss: 0.00042538
Iteration 96/1000 | Loss: 0.00003642
Iteration 97/1000 | Loss: 0.00002458
Iteration 98/1000 | Loss: 0.00002010
Iteration 99/1000 | Loss: 0.00001887
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001819
Iteration 103/1000 | Loss: 0.00001810
Iteration 104/1000 | Loss: 0.00001809
Iteration 105/1000 | Loss: 0.00001799
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00001797
Iteration 113/1000 | Loss: 0.00001797
Iteration 114/1000 | Loss: 0.00001796
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001796
Iteration 117/1000 | Loss: 0.00001796
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001796
Iteration 121/1000 | Loss: 0.00001796
Iteration 122/1000 | Loss: 0.00001796
Iteration 123/1000 | Loss: 0.00001796
Iteration 124/1000 | Loss: 0.00001796
Iteration 125/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.7959890101337805e-05, 1.7959890101337805e-05, 1.7959890101337805e-05, 1.7959890101337805e-05, 1.7959890101337805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7959890101337805e-05

Optimization complete. Final v2v error: 3.3652186393737793 mm

Highest mean error: 10.606547355651855 mm for frame 122

Lowest mean error: 3.1390151977539062 mm for frame 0

Saving results

Total time: 142.72804880142212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490054
Iteration 2/25 | Loss: 0.00131386
Iteration 3/25 | Loss: 0.00110452
Iteration 4/25 | Loss: 0.00106431
Iteration 5/25 | Loss: 0.00105077
Iteration 6/25 | Loss: 0.00104684
Iteration 7/25 | Loss: 0.00104474
Iteration 8/25 | Loss: 0.00104338
Iteration 9/25 | Loss: 0.00105121
Iteration 10/25 | Loss: 0.00104574
Iteration 11/25 | Loss: 0.00104547
Iteration 12/25 | Loss: 0.00103993
Iteration 13/25 | Loss: 0.00103681
Iteration 14/25 | Loss: 0.00103858
Iteration 15/25 | Loss: 0.00103920
Iteration 16/25 | Loss: 0.00103794
Iteration 17/25 | Loss: 0.00103872
Iteration 18/25 | Loss: 0.00103770
Iteration 19/25 | Loss: 0.00103834
Iteration 20/25 | Loss: 0.00103740
Iteration 21/25 | Loss: 0.00103708
Iteration 22/25 | Loss: 0.00103711
Iteration 23/25 | Loss: 0.00103831
Iteration 24/25 | Loss: 0.00103824
Iteration 25/25 | Loss: 0.00103745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49098837
Iteration 2/25 | Loss: 0.00227609
Iteration 3/25 | Loss: 0.00227609
Iteration 4/25 | Loss: 0.00227609
Iteration 5/25 | Loss: 0.00227609
Iteration 6/25 | Loss: 0.00227609
Iteration 7/25 | Loss: 0.00227609
Iteration 8/25 | Loss: 0.00227609
Iteration 9/25 | Loss: 0.00227609
Iteration 10/25 | Loss: 0.00227609
Iteration 11/25 | Loss: 0.00227609
Iteration 12/25 | Loss: 0.00227609
Iteration 13/25 | Loss: 0.00227609
Iteration 14/25 | Loss: 0.00227609
Iteration 15/25 | Loss: 0.00227609
Iteration 16/25 | Loss: 0.00227609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022760885767638683, 0.0022760885767638683, 0.0022760885767638683, 0.0022760885767638683, 0.0022760885767638683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022760885767638683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227609
Iteration 2/1000 | Loss: 0.00029114
Iteration 3/1000 | Loss: 0.00019056
Iteration 4/1000 | Loss: 0.00016055
Iteration 5/1000 | Loss: 0.00014248
Iteration 6/1000 | Loss: 0.00013227
Iteration 7/1000 | Loss: 0.00012412
Iteration 8/1000 | Loss: 0.00011691
Iteration 9/1000 | Loss: 0.00011141
Iteration 10/1000 | Loss: 0.00010807
Iteration 11/1000 | Loss: 0.00010607
Iteration 12/1000 | Loss: 0.00010479
Iteration 13/1000 | Loss: 0.00010366
Iteration 14/1000 | Loss: 0.00010225
Iteration 15/1000 | Loss: 0.00010111
Iteration 16/1000 | Loss: 0.00029326
Iteration 17/1000 | Loss: 0.00406819
Iteration 18/1000 | Loss: 0.00021878
Iteration 19/1000 | Loss: 0.00014372
Iteration 20/1000 | Loss: 0.00011951
Iteration 21/1000 | Loss: 0.00016836
Iteration 22/1000 | Loss: 0.00012673
Iteration 23/1000 | Loss: 0.00009943
Iteration 24/1000 | Loss: 0.00011327
Iteration 25/1000 | Loss: 0.00017456
Iteration 26/1000 | Loss: 0.00012472
Iteration 27/1000 | Loss: 0.00009379
Iteration 28/1000 | Loss: 0.00017003
Iteration 29/1000 | Loss: 0.00016370
Iteration 30/1000 | Loss: 0.00014746
Iteration 31/1000 | Loss: 0.00013815
Iteration 32/1000 | Loss: 0.00013871
Iteration 33/1000 | Loss: 0.00009618
Iteration 34/1000 | Loss: 0.00009156
Iteration 35/1000 | Loss: 0.00008839
Iteration 36/1000 | Loss: 0.00008706
Iteration 37/1000 | Loss: 0.00179109
Iteration 38/1000 | Loss: 0.01023111
Iteration 39/1000 | Loss: 0.00547595
Iteration 40/1000 | Loss: 0.00078548
Iteration 41/1000 | Loss: 0.00034610
Iteration 42/1000 | Loss: 0.00019598
Iteration 43/1000 | Loss: 0.00035016
Iteration 44/1000 | Loss: 0.00012041
Iteration 45/1000 | Loss: 0.00010434
Iteration 46/1000 | Loss: 0.00009325
Iteration 47/1000 | Loss: 0.00076533
Iteration 48/1000 | Loss: 0.00010240
Iteration 49/1000 | Loss: 0.00008723
Iteration 50/1000 | Loss: 0.00008291
Iteration 51/1000 | Loss: 0.00008061
Iteration 52/1000 | Loss: 0.00007902
Iteration 53/1000 | Loss: 0.00176922
Iteration 54/1000 | Loss: 0.00797471
Iteration 55/1000 | Loss: 0.00520635
Iteration 56/1000 | Loss: 0.00097628
Iteration 57/1000 | Loss: 0.00069463
Iteration 58/1000 | Loss: 0.00030518
Iteration 59/1000 | Loss: 0.00077354
Iteration 60/1000 | Loss: 0.00009075
Iteration 61/1000 | Loss: 0.00017193
Iteration 62/1000 | Loss: 0.00006573
Iteration 63/1000 | Loss: 0.00005750
Iteration 64/1000 | Loss: 0.00005337
Iteration 65/1000 | Loss: 0.00032831
Iteration 66/1000 | Loss: 0.00086084
Iteration 67/1000 | Loss: 0.00005264
Iteration 68/1000 | Loss: 0.00004607
Iteration 69/1000 | Loss: 0.00004421
Iteration 70/1000 | Loss: 0.00004293
Iteration 71/1000 | Loss: 0.00004190
Iteration 72/1000 | Loss: 0.00004106
Iteration 73/1000 | Loss: 0.00163812
Iteration 74/1000 | Loss: 0.00048291
Iteration 75/1000 | Loss: 0.00157179
Iteration 76/1000 | Loss: 0.00043890
Iteration 77/1000 | Loss: 0.00005324
Iteration 78/1000 | Loss: 0.00004767
Iteration 79/1000 | Loss: 0.00004322
Iteration 80/1000 | Loss: 0.00131960
Iteration 81/1000 | Loss: 0.00045002
Iteration 82/1000 | Loss: 0.00005356
Iteration 83/1000 | Loss: 0.00004547
Iteration 84/1000 | Loss: 0.00003865
Iteration 85/1000 | Loss: 0.00070079
Iteration 86/1000 | Loss: 0.00005711
Iteration 87/1000 | Loss: 0.00003569
Iteration 88/1000 | Loss: 0.00003392
Iteration 89/1000 | Loss: 0.00003301
Iteration 90/1000 | Loss: 0.00003238
Iteration 91/1000 | Loss: 0.00003166
Iteration 92/1000 | Loss: 0.00003126
Iteration 93/1000 | Loss: 0.00003089
Iteration 94/1000 | Loss: 0.00003061
Iteration 95/1000 | Loss: 0.00003041
Iteration 96/1000 | Loss: 0.00003021
Iteration 97/1000 | Loss: 0.00003021
Iteration 98/1000 | Loss: 0.00003021
Iteration 99/1000 | Loss: 0.00075906
Iteration 100/1000 | Loss: 0.00040298
Iteration 101/1000 | Loss: 0.00004502
Iteration 102/1000 | Loss: 0.00003840
Iteration 103/1000 | Loss: 0.00003627
Iteration 104/1000 | Loss: 0.00003365
Iteration 105/1000 | Loss: 0.00003238
Iteration 106/1000 | Loss: 0.00003096
Iteration 107/1000 | Loss: 0.00003041
Iteration 108/1000 | Loss: 0.00003013
Iteration 109/1000 | Loss: 0.00002993
Iteration 110/1000 | Loss: 0.00002989
Iteration 111/1000 | Loss: 0.00002988
Iteration 112/1000 | Loss: 0.00002988
Iteration 113/1000 | Loss: 0.00002985
Iteration 114/1000 | Loss: 0.00002984
Iteration 115/1000 | Loss: 0.00002982
Iteration 116/1000 | Loss: 0.00002982
Iteration 117/1000 | Loss: 0.00002982
Iteration 118/1000 | Loss: 0.00002982
Iteration 119/1000 | Loss: 0.00002982
Iteration 120/1000 | Loss: 0.00002982
Iteration 121/1000 | Loss: 0.00002982
Iteration 122/1000 | Loss: 0.00002981
Iteration 123/1000 | Loss: 0.00002981
Iteration 124/1000 | Loss: 0.00002981
Iteration 125/1000 | Loss: 0.00002981
Iteration 126/1000 | Loss: 0.00002981
Iteration 127/1000 | Loss: 0.00002981
Iteration 128/1000 | Loss: 0.00002981
Iteration 129/1000 | Loss: 0.00002981
Iteration 130/1000 | Loss: 0.00002980
Iteration 131/1000 | Loss: 0.00002980
Iteration 132/1000 | Loss: 0.00002980
Iteration 133/1000 | Loss: 0.00002978
Iteration 134/1000 | Loss: 0.00002977
Iteration 135/1000 | Loss: 0.00002977
Iteration 136/1000 | Loss: 0.00002977
Iteration 137/1000 | Loss: 0.00002977
Iteration 138/1000 | Loss: 0.00002977
Iteration 139/1000 | Loss: 0.00002976
Iteration 140/1000 | Loss: 0.00075458
Iteration 141/1000 | Loss: 0.00050999
Iteration 142/1000 | Loss: 0.00004422
Iteration 143/1000 | Loss: 0.00003034
Iteration 144/1000 | Loss: 0.00002866
Iteration 145/1000 | Loss: 0.00002808
Iteration 146/1000 | Loss: 0.00002782
Iteration 147/1000 | Loss: 0.00002781
Iteration 148/1000 | Loss: 0.00002771
Iteration 149/1000 | Loss: 0.00002761
Iteration 150/1000 | Loss: 0.00002758
Iteration 151/1000 | Loss: 0.00002757
Iteration 152/1000 | Loss: 0.00002757
Iteration 153/1000 | Loss: 0.00002757
Iteration 154/1000 | Loss: 0.00002755
Iteration 155/1000 | Loss: 0.00002751
Iteration 156/1000 | Loss: 0.00002748
Iteration 157/1000 | Loss: 0.00002745
Iteration 158/1000 | Loss: 0.00002743
Iteration 159/1000 | Loss: 0.00002743
Iteration 160/1000 | Loss: 0.00002742
Iteration 161/1000 | Loss: 0.00002742
Iteration 162/1000 | Loss: 0.00002741
Iteration 163/1000 | Loss: 0.00002741
Iteration 164/1000 | Loss: 0.00002741
Iteration 165/1000 | Loss: 0.00002740
Iteration 166/1000 | Loss: 0.00002739
Iteration 167/1000 | Loss: 0.00002739
Iteration 168/1000 | Loss: 0.00002738
Iteration 169/1000 | Loss: 0.00002738
Iteration 170/1000 | Loss: 0.00002738
Iteration 171/1000 | Loss: 0.00002737
Iteration 172/1000 | Loss: 0.00002737
Iteration 173/1000 | Loss: 0.00002737
Iteration 174/1000 | Loss: 0.00002737
Iteration 175/1000 | Loss: 0.00002736
Iteration 176/1000 | Loss: 0.00002736
Iteration 177/1000 | Loss: 0.00002736
Iteration 178/1000 | Loss: 0.00002735
Iteration 179/1000 | Loss: 0.00002735
Iteration 180/1000 | Loss: 0.00002735
Iteration 181/1000 | Loss: 0.00002734
Iteration 182/1000 | Loss: 0.00002734
Iteration 183/1000 | Loss: 0.00002734
Iteration 184/1000 | Loss: 0.00002734
Iteration 185/1000 | Loss: 0.00002733
Iteration 186/1000 | Loss: 0.00002733
Iteration 187/1000 | Loss: 0.00002733
Iteration 188/1000 | Loss: 0.00002733
Iteration 189/1000 | Loss: 0.00002732
Iteration 190/1000 | Loss: 0.00002732
Iteration 191/1000 | Loss: 0.00002732
Iteration 192/1000 | Loss: 0.00002732
Iteration 193/1000 | Loss: 0.00002732
Iteration 194/1000 | Loss: 0.00002731
Iteration 195/1000 | Loss: 0.00002731
Iteration 196/1000 | Loss: 0.00002731
Iteration 197/1000 | Loss: 0.00002731
Iteration 198/1000 | Loss: 0.00002730
Iteration 199/1000 | Loss: 0.00002730
Iteration 200/1000 | Loss: 0.00002730
Iteration 201/1000 | Loss: 0.00002730
Iteration 202/1000 | Loss: 0.00002729
Iteration 203/1000 | Loss: 0.00002729
Iteration 204/1000 | Loss: 0.00002729
Iteration 205/1000 | Loss: 0.00002728
Iteration 206/1000 | Loss: 0.00002728
Iteration 207/1000 | Loss: 0.00002728
Iteration 208/1000 | Loss: 0.00002727
Iteration 209/1000 | Loss: 0.00002727
Iteration 210/1000 | Loss: 0.00002727
Iteration 211/1000 | Loss: 0.00002727
Iteration 212/1000 | Loss: 0.00002727
Iteration 213/1000 | Loss: 0.00002726
Iteration 214/1000 | Loss: 0.00002726
Iteration 215/1000 | Loss: 0.00002726
Iteration 216/1000 | Loss: 0.00002726
Iteration 217/1000 | Loss: 0.00002725
Iteration 218/1000 | Loss: 0.00002725
Iteration 219/1000 | Loss: 0.00002725
Iteration 220/1000 | Loss: 0.00002725
Iteration 221/1000 | Loss: 0.00002724
Iteration 222/1000 | Loss: 0.00002724
Iteration 223/1000 | Loss: 0.00002724
Iteration 224/1000 | Loss: 0.00002723
Iteration 225/1000 | Loss: 0.00002723
Iteration 226/1000 | Loss: 0.00002723
Iteration 227/1000 | Loss: 0.00002722
Iteration 228/1000 | Loss: 0.00002722
Iteration 229/1000 | Loss: 0.00002722
Iteration 230/1000 | Loss: 0.00002722
Iteration 231/1000 | Loss: 0.00002722
Iteration 232/1000 | Loss: 0.00002722
Iteration 233/1000 | Loss: 0.00002722
Iteration 234/1000 | Loss: 0.00002722
Iteration 235/1000 | Loss: 0.00002722
Iteration 236/1000 | Loss: 0.00002721
Iteration 237/1000 | Loss: 0.00002721
Iteration 238/1000 | Loss: 0.00002721
Iteration 239/1000 | Loss: 0.00002721
Iteration 240/1000 | Loss: 0.00002720
Iteration 241/1000 | Loss: 0.00002720
Iteration 242/1000 | Loss: 0.00002720
Iteration 243/1000 | Loss: 0.00002720
Iteration 244/1000 | Loss: 0.00002720
Iteration 245/1000 | Loss: 0.00002720
Iteration 246/1000 | Loss: 0.00002720
Iteration 247/1000 | Loss: 0.00002719
Iteration 248/1000 | Loss: 0.00002719
Iteration 249/1000 | Loss: 0.00002719
Iteration 250/1000 | Loss: 0.00002719
Iteration 251/1000 | Loss: 0.00002719
Iteration 252/1000 | Loss: 0.00002719
Iteration 253/1000 | Loss: 0.00002719
Iteration 254/1000 | Loss: 0.00002719
Iteration 255/1000 | Loss: 0.00002719
Iteration 256/1000 | Loss: 0.00002719
Iteration 257/1000 | Loss: 0.00002719
Iteration 258/1000 | Loss: 0.00002719
Iteration 259/1000 | Loss: 0.00002719
Iteration 260/1000 | Loss: 0.00002719
Iteration 261/1000 | Loss: 0.00002719
Iteration 262/1000 | Loss: 0.00002719
Iteration 263/1000 | Loss: 0.00002719
Iteration 264/1000 | Loss: 0.00002719
Iteration 265/1000 | Loss: 0.00002719
Iteration 266/1000 | Loss: 0.00002719
Iteration 267/1000 | Loss: 0.00002719
Iteration 268/1000 | Loss: 0.00002719
Iteration 269/1000 | Loss: 0.00002719
Iteration 270/1000 | Loss: 0.00002719
Iteration 271/1000 | Loss: 0.00002719
Iteration 272/1000 | Loss: 0.00002719
Iteration 273/1000 | Loss: 0.00002719
Iteration 274/1000 | Loss: 0.00002719
Iteration 275/1000 | Loss: 0.00002719
Iteration 276/1000 | Loss: 0.00002719
Iteration 277/1000 | Loss: 0.00002719
Iteration 278/1000 | Loss: 0.00002719
Iteration 279/1000 | Loss: 0.00002719
Iteration 280/1000 | Loss: 0.00002719
Iteration 281/1000 | Loss: 0.00002719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.7187330488231964e-05, 2.7187330488231964e-05, 2.7187330488231964e-05, 2.7187330488231964e-05, 2.7187330488231964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7187330488231964e-05

Optimization complete. Final v2v error: 4.127881050109863 mm

Highest mean error: 11.638382911682129 mm for frame 54

Lowest mean error: 3.3941173553466797 mm for frame 26

Saving results

Total time: 223.0358989238739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008214
Iteration 2/25 | Loss: 0.00231796
Iteration 3/25 | Loss: 0.00189139
Iteration 4/25 | Loss: 0.00159731
Iteration 5/25 | Loss: 0.00186203
Iteration 6/25 | Loss: 0.00161397
Iteration 7/25 | Loss: 0.00122620
Iteration 8/25 | Loss: 0.00101176
Iteration 9/25 | Loss: 0.00098038
Iteration 10/25 | Loss: 0.00097324
Iteration 11/25 | Loss: 0.00097104
Iteration 12/25 | Loss: 0.00097269
Iteration 13/25 | Loss: 0.00096912
Iteration 14/25 | Loss: 0.00096730
Iteration 15/25 | Loss: 0.00096647
Iteration 16/25 | Loss: 0.00096625
Iteration 17/25 | Loss: 0.00096615
Iteration 18/25 | Loss: 0.00096599
Iteration 19/25 | Loss: 0.00096584
Iteration 20/25 | Loss: 0.00096575
Iteration 21/25 | Loss: 0.00096575
Iteration 22/25 | Loss: 0.00096575
Iteration 23/25 | Loss: 0.00096575
Iteration 24/25 | Loss: 0.00096575
Iteration 25/25 | Loss: 0.00096574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51449084
Iteration 2/25 | Loss: 0.00084092
Iteration 3/25 | Loss: 0.00084092
Iteration 4/25 | Loss: 0.00084092
Iteration 5/25 | Loss: 0.00084092
Iteration 6/25 | Loss: 0.00084092
Iteration 7/25 | Loss: 0.00084092
Iteration 8/25 | Loss: 0.00084092
Iteration 9/25 | Loss: 0.00084092
Iteration 10/25 | Loss: 0.00084092
Iteration 11/25 | Loss: 0.00084092
Iteration 12/25 | Loss: 0.00084092
Iteration 13/25 | Loss: 0.00084092
Iteration 14/25 | Loss: 0.00084092
Iteration 15/25 | Loss: 0.00084092
Iteration 16/25 | Loss: 0.00084092
Iteration 17/25 | Loss: 0.00084092
Iteration 18/25 | Loss: 0.00084092
Iteration 19/25 | Loss: 0.00084092
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008409154834225774, 0.0008409154834225774, 0.0008409154834225774, 0.0008409154834225774, 0.0008409154834225774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008409154834225774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084092
Iteration 2/1000 | Loss: 0.00049615
Iteration 3/1000 | Loss: 0.00037498
Iteration 4/1000 | Loss: 0.00009524
Iteration 5/1000 | Loss: 0.00005883
Iteration 6/1000 | Loss: 0.00004425
Iteration 7/1000 | Loss: 0.00003669
Iteration 8/1000 | Loss: 0.00159057
Iteration 9/1000 | Loss: 0.00015096
Iteration 10/1000 | Loss: 0.00018155
Iteration 11/1000 | Loss: 0.00017570
Iteration 12/1000 | Loss: 0.00005570
Iteration 13/1000 | Loss: 0.00060957
Iteration 14/1000 | Loss: 0.00041936
Iteration 15/1000 | Loss: 0.00042247
Iteration 16/1000 | Loss: 0.00060271
Iteration 17/1000 | Loss: 0.00014019
Iteration 18/1000 | Loss: 0.00005311
Iteration 19/1000 | Loss: 0.00067686
Iteration 20/1000 | Loss: 0.00010239
Iteration 21/1000 | Loss: 0.00004385
Iteration 22/1000 | Loss: 0.00003972
Iteration 23/1000 | Loss: 0.00003510
Iteration 24/1000 | Loss: 0.00021188
Iteration 25/1000 | Loss: 0.00076649
Iteration 26/1000 | Loss: 0.00005197
Iteration 27/1000 | Loss: 0.00003901
Iteration 28/1000 | Loss: 0.00003400
Iteration 29/1000 | Loss: 0.00049264
Iteration 30/1000 | Loss: 0.00028060
Iteration 31/1000 | Loss: 0.00050335
Iteration 32/1000 | Loss: 0.00013959
Iteration 33/1000 | Loss: 0.00003938
Iteration 34/1000 | Loss: 0.00002865
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002307
Iteration 37/1000 | Loss: 0.00002098
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001796
Iteration 42/1000 | Loss: 0.00001781
Iteration 43/1000 | Loss: 0.00001781
Iteration 44/1000 | Loss: 0.00001778
Iteration 45/1000 | Loss: 0.00001777
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001772
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001770
Iteration 53/1000 | Loss: 0.00001769
Iteration 54/1000 | Loss: 0.00001763
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001759
Iteration 58/1000 | Loss: 0.00001759
Iteration 59/1000 | Loss: 0.00001759
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001759
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001758
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001757
Iteration 71/1000 | Loss: 0.00001757
Iteration 72/1000 | Loss: 0.00001757
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001756
Iteration 77/1000 | Loss: 0.00001756
Iteration 78/1000 | Loss: 0.00001756
Iteration 79/1000 | Loss: 0.00001756
Iteration 80/1000 | Loss: 0.00001756
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001755
Iteration 83/1000 | Loss: 0.00001755
Iteration 84/1000 | Loss: 0.00001755
Iteration 85/1000 | Loss: 0.00001755
Iteration 86/1000 | Loss: 0.00001755
Iteration 87/1000 | Loss: 0.00001755
Iteration 88/1000 | Loss: 0.00001755
Iteration 89/1000 | Loss: 0.00001755
Iteration 90/1000 | Loss: 0.00001755
Iteration 91/1000 | Loss: 0.00001755
Iteration 92/1000 | Loss: 0.00001754
Iteration 93/1000 | Loss: 0.00001754
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001754
Iteration 98/1000 | Loss: 0.00001754
Iteration 99/1000 | Loss: 0.00001754
Iteration 100/1000 | Loss: 0.00001754
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001753
Iteration 104/1000 | Loss: 0.00001753
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001753
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001752
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001752
Iteration 116/1000 | Loss: 0.00001752
Iteration 117/1000 | Loss: 0.00001752
Iteration 118/1000 | Loss: 0.00001752
Iteration 119/1000 | Loss: 0.00001752
Iteration 120/1000 | Loss: 0.00001752
Iteration 121/1000 | Loss: 0.00001752
Iteration 122/1000 | Loss: 0.00001752
Iteration 123/1000 | Loss: 0.00001752
Iteration 124/1000 | Loss: 0.00001751
Iteration 125/1000 | Loss: 0.00001751
Iteration 126/1000 | Loss: 0.00001751
Iteration 127/1000 | Loss: 0.00001751
Iteration 128/1000 | Loss: 0.00001751
Iteration 129/1000 | Loss: 0.00001751
Iteration 130/1000 | Loss: 0.00001751
Iteration 131/1000 | Loss: 0.00001751
Iteration 132/1000 | Loss: 0.00001751
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001751
Iteration 135/1000 | Loss: 0.00001751
Iteration 136/1000 | Loss: 0.00001751
Iteration 137/1000 | Loss: 0.00001751
Iteration 138/1000 | Loss: 0.00001751
Iteration 139/1000 | Loss: 0.00001751
Iteration 140/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.750997398630716e-05, 1.750997398630716e-05, 1.750997398630716e-05, 1.750997398630716e-05, 1.750997398630716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.750997398630716e-05

Optimization complete. Final v2v error: 3.4661128520965576 mm

Highest mean error: 3.981043577194214 mm for frame 1

Lowest mean error: 3.25232195854187 mm for frame 14

Saving results

Total time: 96.41716885566711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030656
Iteration 2/25 | Loss: 0.00282644
Iteration 3/25 | Loss: 0.00214042
Iteration 4/25 | Loss: 0.00158171
Iteration 5/25 | Loss: 0.00165469
Iteration 6/25 | Loss: 0.00156174
Iteration 7/25 | Loss: 0.00131453
Iteration 8/25 | Loss: 0.00124558
Iteration 9/25 | Loss: 0.00120422
Iteration 10/25 | Loss: 0.00117249
Iteration 11/25 | Loss: 0.00114088
Iteration 12/25 | Loss: 0.00110892
Iteration 13/25 | Loss: 0.00108199
Iteration 14/25 | Loss: 0.00106316
Iteration 15/25 | Loss: 0.00104837
Iteration 16/25 | Loss: 0.00104442
Iteration 17/25 | Loss: 0.00104759
Iteration 18/25 | Loss: 0.00104716
Iteration 19/25 | Loss: 0.00104603
Iteration 20/25 | Loss: 0.00104104
Iteration 21/25 | Loss: 0.00104072
Iteration 22/25 | Loss: 0.00103972
Iteration 23/25 | Loss: 0.00103929
Iteration 24/25 | Loss: 0.00103896
Iteration 25/25 | Loss: 0.00103892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45698440
Iteration 2/25 | Loss: 0.00067027
Iteration 3/25 | Loss: 0.00067027
Iteration 4/25 | Loss: 0.00067027
Iteration 5/25 | Loss: 0.00067027
Iteration 6/25 | Loss: 0.00067026
Iteration 7/25 | Loss: 0.00067026
Iteration 8/25 | Loss: 0.00067026
Iteration 9/25 | Loss: 0.00067026
Iteration 10/25 | Loss: 0.00067026
Iteration 11/25 | Loss: 0.00067026
Iteration 12/25 | Loss: 0.00067026
Iteration 13/25 | Loss: 0.00067026
Iteration 14/25 | Loss: 0.00067026
Iteration 15/25 | Loss: 0.00067026
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006702641840092838, 0.0006702641840092838, 0.0006702641840092838, 0.0006702641840092838, 0.0006702641840092838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006702641840092838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067026
Iteration 2/1000 | Loss: 0.00018261
Iteration 3/1000 | Loss: 0.00012437
Iteration 4/1000 | Loss: 0.00007938
Iteration 5/1000 | Loss: 0.00004344
Iteration 6/1000 | Loss: 0.00004002
Iteration 7/1000 | Loss: 0.00003769
Iteration 8/1000 | Loss: 0.00022058
Iteration 9/1000 | Loss: 0.00015614
Iteration 10/1000 | Loss: 0.00005792
Iteration 11/1000 | Loss: 0.00004109
Iteration 12/1000 | Loss: 0.00014856
Iteration 13/1000 | Loss: 0.00009441
Iteration 14/1000 | Loss: 0.00011958
Iteration 15/1000 | Loss: 0.00013229
Iteration 16/1000 | Loss: 0.00003641
Iteration 17/1000 | Loss: 0.00005837
Iteration 18/1000 | Loss: 0.00016148
Iteration 19/1000 | Loss: 0.00023101
Iteration 20/1000 | Loss: 0.00007011
Iteration 21/1000 | Loss: 0.00018577
Iteration 22/1000 | Loss: 0.00017086
Iteration 23/1000 | Loss: 0.00019091
Iteration 24/1000 | Loss: 0.00013618
Iteration 25/1000 | Loss: 0.00004235
Iteration 26/1000 | Loss: 0.00007395
Iteration 27/1000 | Loss: 0.00018540
Iteration 28/1000 | Loss: 0.00010640
Iteration 29/1000 | Loss: 0.00026906
Iteration 30/1000 | Loss: 0.00022217
Iteration 31/1000 | Loss: 0.00008137
Iteration 32/1000 | Loss: 0.00005089
Iteration 33/1000 | Loss: 0.00014752
Iteration 34/1000 | Loss: 0.00010273
Iteration 35/1000 | Loss: 0.00011039
Iteration 36/1000 | Loss: 0.00039205
Iteration 37/1000 | Loss: 0.00011876
Iteration 38/1000 | Loss: 0.00006169
Iteration 39/1000 | Loss: 0.00014928
Iteration 40/1000 | Loss: 0.00028400
Iteration 41/1000 | Loss: 0.00007859
Iteration 42/1000 | Loss: 0.00008741
Iteration 43/1000 | Loss: 0.00007319
Iteration 44/1000 | Loss: 0.00004206
Iteration 45/1000 | Loss: 0.00007957
Iteration 46/1000 | Loss: 0.00011991
Iteration 47/1000 | Loss: 0.00008102
Iteration 48/1000 | Loss: 0.00008422
Iteration 49/1000 | Loss: 0.00007283
Iteration 50/1000 | Loss: 0.00008264
Iteration 51/1000 | Loss: 0.00008691
Iteration 52/1000 | Loss: 0.00007331
Iteration 53/1000 | Loss: 0.00010482
Iteration 54/1000 | Loss: 0.00012770
Iteration 55/1000 | Loss: 0.00003750
Iteration 56/1000 | Loss: 0.00007321
Iteration 57/1000 | Loss: 0.00010654
Iteration 58/1000 | Loss: 0.00012133
Iteration 59/1000 | Loss: 0.00016236
Iteration 60/1000 | Loss: 0.00009647
Iteration 61/1000 | Loss: 0.00007130
Iteration 62/1000 | Loss: 0.00009024
Iteration 63/1000 | Loss: 0.00009578
Iteration 64/1000 | Loss: 0.00005561
Iteration 65/1000 | Loss: 0.00010270
Iteration 66/1000 | Loss: 0.00013610
Iteration 67/1000 | Loss: 0.00011131
Iteration 68/1000 | Loss: 0.00016776
Iteration 69/1000 | Loss: 0.00011133
Iteration 70/1000 | Loss: 0.00133336
Iteration 71/1000 | Loss: 0.00017345
Iteration 72/1000 | Loss: 0.00007929
Iteration 73/1000 | Loss: 0.00011564
Iteration 74/1000 | Loss: 0.00019617
Iteration 75/1000 | Loss: 0.00011179
Iteration 76/1000 | Loss: 0.00010212
Iteration 77/1000 | Loss: 0.00015655
Iteration 78/1000 | Loss: 0.00007327
Iteration 79/1000 | Loss: 0.00038943
Iteration 80/1000 | Loss: 0.00013297
Iteration 81/1000 | Loss: 0.00015468
Iteration 82/1000 | Loss: 0.00011748
Iteration 83/1000 | Loss: 0.00011767
Iteration 84/1000 | Loss: 0.00011931
Iteration 85/1000 | Loss: 0.00014255
Iteration 86/1000 | Loss: 0.00012408
Iteration 87/1000 | Loss: 0.00039257
Iteration 88/1000 | Loss: 0.00013802
Iteration 89/1000 | Loss: 0.00007965
Iteration 90/1000 | Loss: 0.00006368
Iteration 91/1000 | Loss: 0.00024516
Iteration 92/1000 | Loss: 0.00003647
Iteration 93/1000 | Loss: 0.00005346
Iteration 94/1000 | Loss: 0.00003524
Iteration 95/1000 | Loss: 0.00003347
Iteration 96/1000 | Loss: 0.00009118
Iteration 97/1000 | Loss: 0.00005814
Iteration 98/1000 | Loss: 0.00007572
Iteration 99/1000 | Loss: 0.00006003
Iteration 100/1000 | Loss: 0.00008787
Iteration 101/1000 | Loss: 0.00013218
Iteration 102/1000 | Loss: 0.00004952
Iteration 103/1000 | Loss: 0.00007628
Iteration 104/1000 | Loss: 0.00013333
Iteration 105/1000 | Loss: 0.00029584
Iteration 106/1000 | Loss: 0.00010482
Iteration 107/1000 | Loss: 0.00015201
Iteration 108/1000 | Loss: 0.00007721
Iteration 109/1000 | Loss: 0.00009374
Iteration 110/1000 | Loss: 0.00007071
Iteration 111/1000 | Loss: 0.00006867
Iteration 112/1000 | Loss: 0.00008125
Iteration 113/1000 | Loss: 0.00006315
Iteration 114/1000 | Loss: 0.00007226
Iteration 115/1000 | Loss: 0.00008947
Iteration 116/1000 | Loss: 0.00008192
Iteration 117/1000 | Loss: 0.00018764
Iteration 118/1000 | Loss: 0.00004235
Iteration 119/1000 | Loss: 0.00003608
Iteration 120/1000 | Loss: 0.00003202
Iteration 121/1000 | Loss: 0.00003051
Iteration 122/1000 | Loss: 0.00002991
Iteration 123/1000 | Loss: 0.00002961
Iteration 124/1000 | Loss: 0.00002946
Iteration 125/1000 | Loss: 0.00002943
Iteration 126/1000 | Loss: 0.00002942
Iteration 127/1000 | Loss: 0.00002942
Iteration 128/1000 | Loss: 0.00002942
Iteration 129/1000 | Loss: 0.00002941
Iteration 130/1000 | Loss: 0.00002941
Iteration 131/1000 | Loss: 0.00002941
Iteration 132/1000 | Loss: 0.00002940
Iteration 133/1000 | Loss: 0.00002940
Iteration 134/1000 | Loss: 0.00002940
Iteration 135/1000 | Loss: 0.00002939
Iteration 136/1000 | Loss: 0.00002939
Iteration 137/1000 | Loss: 0.00002939
Iteration 138/1000 | Loss: 0.00002939
Iteration 139/1000 | Loss: 0.00002939
Iteration 140/1000 | Loss: 0.00002939
Iteration 141/1000 | Loss: 0.00002938
Iteration 142/1000 | Loss: 0.00002938
Iteration 143/1000 | Loss: 0.00002938
Iteration 144/1000 | Loss: 0.00002938
Iteration 145/1000 | Loss: 0.00002937
Iteration 146/1000 | Loss: 0.00002937
Iteration 147/1000 | Loss: 0.00002937
Iteration 148/1000 | Loss: 0.00002937
Iteration 149/1000 | Loss: 0.00002936
Iteration 150/1000 | Loss: 0.00002935
Iteration 151/1000 | Loss: 0.00002935
Iteration 152/1000 | Loss: 0.00002934
Iteration 153/1000 | Loss: 0.00002934
Iteration 154/1000 | Loss: 0.00002934
Iteration 155/1000 | Loss: 0.00002933
Iteration 156/1000 | Loss: 0.00002933
Iteration 157/1000 | Loss: 0.00002932
Iteration 158/1000 | Loss: 0.00002932
Iteration 159/1000 | Loss: 0.00002932
Iteration 160/1000 | Loss: 0.00002931
Iteration 161/1000 | Loss: 0.00002931
Iteration 162/1000 | Loss: 0.00002931
Iteration 163/1000 | Loss: 0.00002931
Iteration 164/1000 | Loss: 0.00002930
Iteration 165/1000 | Loss: 0.00002930
Iteration 166/1000 | Loss: 0.00002930
Iteration 167/1000 | Loss: 0.00002930
Iteration 168/1000 | Loss: 0.00002930
Iteration 169/1000 | Loss: 0.00002930
Iteration 170/1000 | Loss: 0.00002930
Iteration 171/1000 | Loss: 0.00002930
Iteration 172/1000 | Loss: 0.00002929
Iteration 173/1000 | Loss: 0.00002929
Iteration 174/1000 | Loss: 0.00002929
Iteration 175/1000 | Loss: 0.00002928
Iteration 176/1000 | Loss: 0.00002928
Iteration 177/1000 | Loss: 0.00002928
Iteration 178/1000 | Loss: 0.00002927
Iteration 179/1000 | Loss: 0.00002927
Iteration 180/1000 | Loss: 0.00002927
Iteration 181/1000 | Loss: 0.00002927
Iteration 182/1000 | Loss: 0.00002927
Iteration 183/1000 | Loss: 0.00002927
Iteration 184/1000 | Loss: 0.00002926
Iteration 185/1000 | Loss: 0.00002926
Iteration 186/1000 | Loss: 0.00002926
Iteration 187/1000 | Loss: 0.00002926
Iteration 188/1000 | Loss: 0.00002925
Iteration 189/1000 | Loss: 0.00002925
Iteration 190/1000 | Loss: 0.00002925
Iteration 191/1000 | Loss: 0.00002925
Iteration 192/1000 | Loss: 0.00002925
Iteration 193/1000 | Loss: 0.00002925
Iteration 194/1000 | Loss: 0.00002925
Iteration 195/1000 | Loss: 0.00002925
Iteration 196/1000 | Loss: 0.00002925
Iteration 197/1000 | Loss: 0.00002925
Iteration 198/1000 | Loss: 0.00002925
Iteration 199/1000 | Loss: 0.00002925
Iteration 200/1000 | Loss: 0.00002925
Iteration 201/1000 | Loss: 0.00002925
Iteration 202/1000 | Loss: 0.00002925
Iteration 203/1000 | Loss: 0.00002925
Iteration 204/1000 | Loss: 0.00002924
Iteration 205/1000 | Loss: 0.00002924
Iteration 206/1000 | Loss: 0.00002924
Iteration 207/1000 | Loss: 0.00002924
Iteration 208/1000 | Loss: 0.00002924
Iteration 209/1000 | Loss: 0.00002924
Iteration 210/1000 | Loss: 0.00002924
Iteration 211/1000 | Loss: 0.00002924
Iteration 212/1000 | Loss: 0.00002924
Iteration 213/1000 | Loss: 0.00002924
Iteration 214/1000 | Loss: 0.00002924
Iteration 215/1000 | Loss: 0.00002924
Iteration 216/1000 | Loss: 0.00002924
Iteration 217/1000 | Loss: 0.00002924
Iteration 218/1000 | Loss: 0.00002924
Iteration 219/1000 | Loss: 0.00002924
Iteration 220/1000 | Loss: 0.00002924
Iteration 221/1000 | Loss: 0.00002923
Iteration 222/1000 | Loss: 0.00002923
Iteration 223/1000 | Loss: 0.00002923
Iteration 224/1000 | Loss: 0.00002923
Iteration 225/1000 | Loss: 0.00002923
Iteration 226/1000 | Loss: 0.00002923
Iteration 227/1000 | Loss: 0.00002923
Iteration 228/1000 | Loss: 0.00002923
Iteration 229/1000 | Loss: 0.00002923
Iteration 230/1000 | Loss: 0.00002923
Iteration 231/1000 | Loss: 0.00002922
Iteration 232/1000 | Loss: 0.00002922
Iteration 233/1000 | Loss: 0.00002922
Iteration 234/1000 | Loss: 0.00002922
Iteration 235/1000 | Loss: 0.00002922
Iteration 236/1000 | Loss: 0.00002921
Iteration 237/1000 | Loss: 0.00002921
Iteration 238/1000 | Loss: 0.00002921
Iteration 239/1000 | Loss: 0.00002921
Iteration 240/1000 | Loss: 0.00002921
Iteration 241/1000 | Loss: 0.00002921
Iteration 242/1000 | Loss: 0.00002920
Iteration 243/1000 | Loss: 0.00002920
Iteration 244/1000 | Loss: 0.00002920
Iteration 245/1000 | Loss: 0.00002920
Iteration 246/1000 | Loss: 0.00002920
Iteration 247/1000 | Loss: 0.00002920
Iteration 248/1000 | Loss: 0.00002920
Iteration 249/1000 | Loss: 0.00002920
Iteration 250/1000 | Loss: 0.00002919
Iteration 251/1000 | Loss: 0.00002919
Iteration 252/1000 | Loss: 0.00002919
Iteration 253/1000 | Loss: 0.00002919
Iteration 254/1000 | Loss: 0.00002919
Iteration 255/1000 | Loss: 0.00002919
Iteration 256/1000 | Loss: 0.00002919
Iteration 257/1000 | Loss: 0.00002919
Iteration 258/1000 | Loss: 0.00002919
Iteration 259/1000 | Loss: 0.00002919
Iteration 260/1000 | Loss: 0.00002919
Iteration 261/1000 | Loss: 0.00002919
Iteration 262/1000 | Loss: 0.00002919
Iteration 263/1000 | Loss: 0.00002919
Iteration 264/1000 | Loss: 0.00002919
Iteration 265/1000 | Loss: 0.00002918
Iteration 266/1000 | Loss: 0.00002918
Iteration 267/1000 | Loss: 0.00002918
Iteration 268/1000 | Loss: 0.00002918
Iteration 269/1000 | Loss: 0.00002918
Iteration 270/1000 | Loss: 0.00002918
Iteration 271/1000 | Loss: 0.00002918
Iteration 272/1000 | Loss: 0.00002918
Iteration 273/1000 | Loss: 0.00002918
Iteration 274/1000 | Loss: 0.00002918
Iteration 275/1000 | Loss: 0.00002918
Iteration 276/1000 | Loss: 0.00002918
Iteration 277/1000 | Loss: 0.00002918
Iteration 278/1000 | Loss: 0.00002918
Iteration 279/1000 | Loss: 0.00002918
Iteration 280/1000 | Loss: 0.00002918
Iteration 281/1000 | Loss: 0.00002917
Iteration 282/1000 | Loss: 0.00002917
Iteration 283/1000 | Loss: 0.00002917
Iteration 284/1000 | Loss: 0.00002917
Iteration 285/1000 | Loss: 0.00002917
Iteration 286/1000 | Loss: 0.00002917
Iteration 287/1000 | Loss: 0.00002917
Iteration 288/1000 | Loss: 0.00002917
Iteration 289/1000 | Loss: 0.00002917
Iteration 290/1000 | Loss: 0.00002917
Iteration 291/1000 | Loss: 0.00002917
Iteration 292/1000 | Loss: 0.00002917
Iteration 293/1000 | Loss: 0.00002917
Iteration 294/1000 | Loss: 0.00002916
Iteration 295/1000 | Loss: 0.00002916
Iteration 296/1000 | Loss: 0.00002916
Iteration 297/1000 | Loss: 0.00002916
Iteration 298/1000 | Loss: 0.00002916
Iteration 299/1000 | Loss: 0.00002916
Iteration 300/1000 | Loss: 0.00002915
Iteration 301/1000 | Loss: 0.00002915
Iteration 302/1000 | Loss: 0.00002915
Iteration 303/1000 | Loss: 0.00002915
Iteration 304/1000 | Loss: 0.00002915
Iteration 305/1000 | Loss: 0.00002914
Iteration 306/1000 | Loss: 0.00002914
Iteration 307/1000 | Loss: 0.00002914
Iteration 308/1000 | Loss: 0.00002914
Iteration 309/1000 | Loss: 0.00002914
Iteration 310/1000 | Loss: 0.00002914
Iteration 311/1000 | Loss: 0.00002914
Iteration 312/1000 | Loss: 0.00002914
Iteration 313/1000 | Loss: 0.00002914
Iteration 314/1000 | Loss: 0.00002913
Iteration 315/1000 | Loss: 0.00002913
Iteration 316/1000 | Loss: 0.00002913
Iteration 317/1000 | Loss: 0.00002913
Iteration 318/1000 | Loss: 0.00002913
Iteration 319/1000 | Loss: 0.00002913
Iteration 320/1000 | Loss: 0.00002913
Iteration 321/1000 | Loss: 0.00002913
Iteration 322/1000 | Loss: 0.00002913
Iteration 323/1000 | Loss: 0.00002913
Iteration 324/1000 | Loss: 0.00002913
Iteration 325/1000 | Loss: 0.00002913
Iteration 326/1000 | Loss: 0.00002913
Iteration 327/1000 | Loss: 0.00002912
Iteration 328/1000 | Loss: 0.00002912
Iteration 329/1000 | Loss: 0.00002912
Iteration 330/1000 | Loss: 0.00002912
Iteration 331/1000 | Loss: 0.00002912
Iteration 332/1000 | Loss: 0.00002912
Iteration 333/1000 | Loss: 0.00002911
Iteration 334/1000 | Loss: 0.00002911
Iteration 335/1000 | Loss: 0.00002911
Iteration 336/1000 | Loss: 0.00002911
Iteration 337/1000 | Loss: 0.00002911
Iteration 338/1000 | Loss: 0.00002911
Iteration 339/1000 | Loss: 0.00002911
Iteration 340/1000 | Loss: 0.00002911
Iteration 341/1000 | Loss: 0.00002911
Iteration 342/1000 | Loss: 0.00002911
Iteration 343/1000 | Loss: 0.00002911
Iteration 344/1000 | Loss: 0.00002911
Iteration 345/1000 | Loss: 0.00002910
Iteration 346/1000 | Loss: 0.00002910
Iteration 347/1000 | Loss: 0.00002910
Iteration 348/1000 | Loss: 0.00002910
Iteration 349/1000 | Loss: 0.00002910
Iteration 350/1000 | Loss: 0.00002910
Iteration 351/1000 | Loss: 0.00002910
Iteration 352/1000 | Loss: 0.00002910
Iteration 353/1000 | Loss: 0.00002910
Iteration 354/1000 | Loss: 0.00002910
Iteration 355/1000 | Loss: 0.00002910
Iteration 356/1000 | Loss: 0.00002910
Iteration 357/1000 | Loss: 0.00002910
Iteration 358/1000 | Loss: 0.00002910
Iteration 359/1000 | Loss: 0.00002909
Iteration 360/1000 | Loss: 0.00002909
Iteration 361/1000 | Loss: 0.00002909
Iteration 362/1000 | Loss: 0.00002909
Iteration 363/1000 | Loss: 0.00002909
Iteration 364/1000 | Loss: 0.00002909
Iteration 365/1000 | Loss: 0.00002909
Iteration 366/1000 | Loss: 0.00002908
Iteration 367/1000 | Loss: 0.00011797
Iteration 368/1000 | Loss: 0.00010533
Iteration 369/1000 | Loss: 0.00011646
Iteration 370/1000 | Loss: 0.00015003
Iteration 371/1000 | Loss: 0.00007775
Iteration 372/1000 | Loss: 0.00009360
Iteration 373/1000 | Loss: 0.00009727
Iteration 374/1000 | Loss: 0.00011107
Iteration 375/1000 | Loss: 0.00011597
Iteration 376/1000 | Loss: 0.00053999
Iteration 377/1000 | Loss: 0.00006639
Iteration 378/1000 | Loss: 0.00009656
Iteration 379/1000 | Loss: 0.00010329
Iteration 380/1000 | Loss: 0.00005500
Iteration 381/1000 | Loss: 0.00007398
Iteration 382/1000 | Loss: 0.00012267
Iteration 383/1000 | Loss: 0.00016265
Iteration 384/1000 | Loss: 0.00003258
Iteration 385/1000 | Loss: 0.00003007
Iteration 386/1000 | Loss: 0.00002916
Iteration 387/1000 | Loss: 0.00002886
Iteration 388/1000 | Loss: 0.00002879
Iteration 389/1000 | Loss: 0.00002879
Iteration 390/1000 | Loss: 0.00002878
Iteration 391/1000 | Loss: 0.00002878
Iteration 392/1000 | Loss: 0.00002877
Iteration 393/1000 | Loss: 0.00002876
Iteration 394/1000 | Loss: 0.00002876
Iteration 395/1000 | Loss: 0.00002876
Iteration 396/1000 | Loss: 0.00002876
Iteration 397/1000 | Loss: 0.00002875
Iteration 398/1000 | Loss: 0.00002875
Iteration 399/1000 | Loss: 0.00002874
Iteration 400/1000 | Loss: 0.00002873
Iteration 401/1000 | Loss: 0.00002872
Iteration 402/1000 | Loss: 0.00002872
Iteration 403/1000 | Loss: 0.00002871
Iteration 404/1000 | Loss: 0.00002868
Iteration 405/1000 | Loss: 0.00002868
Iteration 406/1000 | Loss: 0.00002868
Iteration 407/1000 | Loss: 0.00002868
Iteration 408/1000 | Loss: 0.00002868
Iteration 409/1000 | Loss: 0.00002866
Iteration 410/1000 | Loss: 0.00002865
Iteration 411/1000 | Loss: 0.00002865
Iteration 412/1000 | Loss: 0.00002865
Iteration 413/1000 | Loss: 0.00002865
Iteration 414/1000 | Loss: 0.00002864
Iteration 415/1000 | Loss: 0.00002864
Iteration 416/1000 | Loss: 0.00002864
Iteration 417/1000 | Loss: 0.00002864
Iteration 418/1000 | Loss: 0.00002864
Iteration 419/1000 | Loss: 0.00002864
Iteration 420/1000 | Loss: 0.00002863
Iteration 421/1000 | Loss: 0.00002863
Iteration 422/1000 | Loss: 0.00002863
Iteration 423/1000 | Loss: 0.00002863
Iteration 424/1000 | Loss: 0.00002863
Iteration 425/1000 | Loss: 0.00002863
Iteration 426/1000 | Loss: 0.00002863
Iteration 427/1000 | Loss: 0.00002863
Iteration 428/1000 | Loss: 0.00002863
Iteration 429/1000 | Loss: 0.00002863
Iteration 430/1000 | Loss: 0.00002863
Iteration 431/1000 | Loss: 0.00002863
Iteration 432/1000 | Loss: 0.00002863
Iteration 433/1000 | Loss: 0.00002863
Iteration 434/1000 | Loss: 0.00002862
Iteration 435/1000 | Loss: 0.00002862
Iteration 436/1000 | Loss: 0.00002862
Iteration 437/1000 | Loss: 0.00002862
Iteration 438/1000 | Loss: 0.00002862
Iteration 439/1000 | Loss: 0.00002862
Iteration 440/1000 | Loss: 0.00002861
Iteration 441/1000 | Loss: 0.00002861
Iteration 442/1000 | Loss: 0.00002861
Iteration 443/1000 | Loss: 0.00002861
Iteration 444/1000 | Loss: 0.00002861
Iteration 445/1000 | Loss: 0.00002861
Iteration 446/1000 | Loss: 0.00002861
Iteration 447/1000 | Loss: 0.00002861
Iteration 448/1000 | Loss: 0.00002861
Iteration 449/1000 | Loss: 0.00002861
Iteration 450/1000 | Loss: 0.00002861
Iteration 451/1000 | Loss: 0.00002861
Iteration 452/1000 | Loss: 0.00002861
Iteration 453/1000 | Loss: 0.00002861
Iteration 454/1000 | Loss: 0.00002860
Iteration 455/1000 | Loss: 0.00002860
Iteration 456/1000 | Loss: 0.00002860
Iteration 457/1000 | Loss: 0.00002860
Iteration 458/1000 | Loss: 0.00002860
Iteration 459/1000 | Loss: 0.00002860
Iteration 460/1000 | Loss: 0.00002860
Iteration 461/1000 | Loss: 0.00002860
Iteration 462/1000 | Loss: 0.00002860
Iteration 463/1000 | Loss: 0.00002860
Iteration 464/1000 | Loss: 0.00002860
Iteration 465/1000 | Loss: 0.00002860
Iteration 466/1000 | Loss: 0.00002860
Iteration 467/1000 | Loss: 0.00002860
Iteration 468/1000 | Loss: 0.00002860
Iteration 469/1000 | Loss: 0.00002860
Iteration 470/1000 | Loss: 0.00002860
Iteration 471/1000 | Loss: 0.00002860
Iteration 472/1000 | Loss: 0.00002860
Iteration 473/1000 | Loss: 0.00002860
Iteration 474/1000 | Loss: 0.00002859
Iteration 475/1000 | Loss: 0.00002859
Iteration 476/1000 | Loss: 0.00002859
Iteration 477/1000 | Loss: 0.00002859
Iteration 478/1000 | Loss: 0.00002859
Iteration 479/1000 | Loss: 0.00002859
Iteration 480/1000 | Loss: 0.00002859
Iteration 481/1000 | Loss: 0.00002859
Iteration 482/1000 | Loss: 0.00002858
Iteration 483/1000 | Loss: 0.00002858
Iteration 484/1000 | Loss: 0.00002858
Iteration 485/1000 | Loss: 0.00002858
Iteration 486/1000 | Loss: 0.00002858
Iteration 487/1000 | Loss: 0.00002858
Iteration 488/1000 | Loss: 0.00002858
Iteration 489/1000 | Loss: 0.00002858
Iteration 490/1000 | Loss: 0.00002858
Iteration 491/1000 | Loss: 0.00002858
Iteration 492/1000 | Loss: 0.00002858
Iteration 493/1000 | Loss: 0.00002858
Iteration 494/1000 | Loss: 0.00002858
Iteration 495/1000 | Loss: 0.00002858
Iteration 496/1000 | Loss: 0.00002858
Iteration 497/1000 | Loss: 0.00002858
Iteration 498/1000 | Loss: 0.00002858
Iteration 499/1000 | Loss: 0.00002858
Iteration 500/1000 | Loss: 0.00002858
Iteration 501/1000 | Loss: 0.00002858
Iteration 502/1000 | Loss: 0.00002858
Iteration 503/1000 | Loss: 0.00002858
Iteration 504/1000 | Loss: 0.00002858
Iteration 505/1000 | Loss: 0.00002858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 505. Stopping optimization.
Last 5 losses: [2.8578282581293024e-05, 2.8578282581293024e-05, 2.8578282581293024e-05, 2.8578282581293024e-05, 2.8578282581293024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8578282581293024e-05

Optimization complete. Final v2v error: 4.510034084320068 mm

Highest mean error: 5.495822429656982 mm for frame 233

Lowest mean error: 4.268618106842041 mm for frame 50

Saving results

Total time: 302.6228857040405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824593
Iteration 2/25 | Loss: 0.00111581
Iteration 3/25 | Loss: 0.00090316
Iteration 4/25 | Loss: 0.00086796
Iteration 5/25 | Loss: 0.00085726
Iteration 6/25 | Loss: 0.00085429
Iteration 7/25 | Loss: 0.00085400
Iteration 8/25 | Loss: 0.00085400
Iteration 9/25 | Loss: 0.00085400
Iteration 10/25 | Loss: 0.00085400
Iteration 11/25 | Loss: 0.00085400
Iteration 12/25 | Loss: 0.00085400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008540007984265685, 0.0008540007984265685, 0.0008540007984265685, 0.0008540007984265685, 0.0008540007984265685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008540007984265685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47762942
Iteration 2/25 | Loss: 0.00060535
Iteration 3/25 | Loss: 0.00060531
Iteration 4/25 | Loss: 0.00060531
Iteration 5/25 | Loss: 0.00060531
Iteration 6/25 | Loss: 0.00060531
Iteration 7/25 | Loss: 0.00060530
Iteration 8/25 | Loss: 0.00060530
Iteration 9/25 | Loss: 0.00060530
Iteration 10/25 | Loss: 0.00060530
Iteration 11/25 | Loss: 0.00060530
Iteration 12/25 | Loss: 0.00060530
Iteration 13/25 | Loss: 0.00060530
Iteration 14/25 | Loss: 0.00060530
Iteration 15/25 | Loss: 0.00060530
Iteration 16/25 | Loss: 0.00060530
Iteration 17/25 | Loss: 0.00060530
Iteration 18/25 | Loss: 0.00060530
Iteration 19/25 | Loss: 0.00060530
Iteration 20/25 | Loss: 0.00060530
Iteration 21/25 | Loss: 0.00060530
Iteration 22/25 | Loss: 0.00060530
Iteration 23/25 | Loss: 0.00060530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006053036195226014, 0.0006053036195226014, 0.0006053036195226014, 0.0006053036195226014, 0.0006053036195226014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006053036195226014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060530
Iteration 2/1000 | Loss: 0.00003341
Iteration 3/1000 | Loss: 0.00002286
Iteration 4/1000 | Loss: 0.00002017
Iteration 5/1000 | Loss: 0.00001906
Iteration 6/1000 | Loss: 0.00001799
Iteration 7/1000 | Loss: 0.00001741
Iteration 8/1000 | Loss: 0.00001702
Iteration 9/1000 | Loss: 0.00001671
Iteration 10/1000 | Loss: 0.00001653
Iteration 11/1000 | Loss: 0.00001634
Iteration 12/1000 | Loss: 0.00001623
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001622
Iteration 15/1000 | Loss: 0.00001610
Iteration 16/1000 | Loss: 0.00001610
Iteration 17/1000 | Loss: 0.00001608
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001606
Iteration 20/1000 | Loss: 0.00001605
Iteration 21/1000 | Loss: 0.00001604
Iteration 22/1000 | Loss: 0.00001604
Iteration 23/1000 | Loss: 0.00001604
Iteration 24/1000 | Loss: 0.00001603
Iteration 25/1000 | Loss: 0.00001603
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001601
Iteration 30/1000 | Loss: 0.00001601
Iteration 31/1000 | Loss: 0.00001600
Iteration 32/1000 | Loss: 0.00001600
Iteration 33/1000 | Loss: 0.00001599
Iteration 34/1000 | Loss: 0.00001598
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001597
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001593
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001590
Iteration 51/1000 | Loss: 0.00001589
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001589
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001589
Iteration 56/1000 | Loss: 0.00001589
Iteration 57/1000 | Loss: 0.00001589
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001588
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001588
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001588
Iteration 67/1000 | Loss: 0.00001588
Iteration 68/1000 | Loss: 0.00001588
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001587
Iteration 71/1000 | Loss: 0.00001587
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001587
Iteration 75/1000 | Loss: 0.00001587
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001586
Iteration 78/1000 | Loss: 0.00001586
Iteration 79/1000 | Loss: 0.00001586
Iteration 80/1000 | Loss: 0.00001586
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001585
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001585
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001584
Iteration 96/1000 | Loss: 0.00001584
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001584
Iteration 99/1000 | Loss: 0.00001584
Iteration 100/1000 | Loss: 0.00001584
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001583
Iteration 103/1000 | Loss: 0.00001583
Iteration 104/1000 | Loss: 0.00001583
Iteration 105/1000 | Loss: 0.00001583
Iteration 106/1000 | Loss: 0.00001583
Iteration 107/1000 | Loss: 0.00001583
Iteration 108/1000 | Loss: 0.00001583
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001582
Iteration 111/1000 | Loss: 0.00001582
Iteration 112/1000 | Loss: 0.00001582
Iteration 113/1000 | Loss: 0.00001582
Iteration 114/1000 | Loss: 0.00001582
Iteration 115/1000 | Loss: 0.00001582
Iteration 116/1000 | Loss: 0.00001582
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001581
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001580
Iteration 127/1000 | Loss: 0.00001580
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001580
Iteration 135/1000 | Loss: 0.00001580
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001579
Iteration 149/1000 | Loss: 0.00001579
Iteration 150/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.578742012497969e-05, 1.578742012497969e-05, 1.578742012497969e-05, 1.578742012497969e-05, 1.578742012497969e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.578742012497969e-05

Optimization complete. Final v2v error: 3.350461959838867 mm

Highest mean error: 4.7223801612854 mm for frame 116

Lowest mean error: 2.8451297283172607 mm for frame 182

Saving results

Total time: 43.15162420272827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427473
Iteration 2/25 | Loss: 0.00093784
Iteration 3/25 | Loss: 0.00085061
Iteration 4/25 | Loss: 0.00082875
Iteration 5/25 | Loss: 0.00082081
Iteration 6/25 | Loss: 0.00081918
Iteration 7/25 | Loss: 0.00081858
Iteration 8/25 | Loss: 0.00081857
Iteration 9/25 | Loss: 0.00081857
Iteration 10/25 | Loss: 0.00081857
Iteration 11/25 | Loss: 0.00081857
Iteration 12/25 | Loss: 0.00081857
Iteration 13/25 | Loss: 0.00081857
Iteration 14/25 | Loss: 0.00081857
Iteration 15/25 | Loss: 0.00081857
Iteration 16/25 | Loss: 0.00081857
Iteration 17/25 | Loss: 0.00081857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008185660699382424, 0.0008185660699382424, 0.0008185660699382424, 0.0008185660699382424, 0.0008185660699382424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008185660699382424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71177244
Iteration 2/25 | Loss: 0.00055184
Iteration 3/25 | Loss: 0.00055184
Iteration 4/25 | Loss: 0.00055184
Iteration 5/25 | Loss: 0.00055183
Iteration 6/25 | Loss: 0.00055183
Iteration 7/25 | Loss: 0.00055183
Iteration 8/25 | Loss: 0.00055183
Iteration 9/25 | Loss: 0.00055183
Iteration 10/25 | Loss: 0.00055183
Iteration 11/25 | Loss: 0.00055183
Iteration 12/25 | Loss: 0.00055183
Iteration 13/25 | Loss: 0.00055183
Iteration 14/25 | Loss: 0.00055183
Iteration 15/25 | Loss: 0.00055183
Iteration 16/25 | Loss: 0.00055183
Iteration 17/25 | Loss: 0.00055183
Iteration 18/25 | Loss: 0.00055183
Iteration 19/25 | Loss: 0.00055183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005518331308849156, 0.0005518331308849156, 0.0005518331308849156, 0.0005518331308849156, 0.0005518331308849156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005518331308849156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055183
Iteration 2/1000 | Loss: 0.00002862
Iteration 3/1000 | Loss: 0.00002085
Iteration 4/1000 | Loss: 0.00001892
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001720
Iteration 7/1000 | Loss: 0.00001674
Iteration 8/1000 | Loss: 0.00001639
Iteration 9/1000 | Loss: 0.00001616
Iteration 10/1000 | Loss: 0.00001595
Iteration 11/1000 | Loss: 0.00001585
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001574
Iteration 16/1000 | Loss: 0.00001573
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001571
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001567
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001557
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001557
Iteration 33/1000 | Loss: 0.00001556
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001556
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001556
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001554
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001548
Iteration 64/1000 | Loss: 0.00001548
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001543
Iteration 78/1000 | Loss: 0.00001543
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001541
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001536
Iteration 92/1000 | Loss: 0.00001536
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001535
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001534
Iteration 98/1000 | Loss: 0.00001534
Iteration 99/1000 | Loss: 0.00001533
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001533
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001532
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001531
Iteration 110/1000 | Loss: 0.00001531
Iteration 111/1000 | Loss: 0.00001531
Iteration 112/1000 | Loss: 0.00001531
Iteration 113/1000 | Loss: 0.00001530
Iteration 114/1000 | Loss: 0.00001530
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Iteration 123/1000 | Loss: 0.00001529
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001528
Iteration 126/1000 | Loss: 0.00001528
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001527
Iteration 135/1000 | Loss: 0.00001527
Iteration 136/1000 | Loss: 0.00001527
Iteration 137/1000 | Loss: 0.00001527
Iteration 138/1000 | Loss: 0.00001527
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001526
Iteration 145/1000 | Loss: 0.00001526
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001526
Iteration 148/1000 | Loss: 0.00001526
Iteration 149/1000 | Loss: 0.00001526
Iteration 150/1000 | Loss: 0.00001526
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Iteration 154/1000 | Loss: 0.00001525
Iteration 155/1000 | Loss: 0.00001525
Iteration 156/1000 | Loss: 0.00001525
Iteration 157/1000 | Loss: 0.00001525
Iteration 158/1000 | Loss: 0.00001525
Iteration 159/1000 | Loss: 0.00001525
Iteration 160/1000 | Loss: 0.00001525
Iteration 161/1000 | Loss: 0.00001525
Iteration 162/1000 | Loss: 0.00001525
Iteration 163/1000 | Loss: 0.00001525
Iteration 164/1000 | Loss: 0.00001524
Iteration 165/1000 | Loss: 0.00001524
Iteration 166/1000 | Loss: 0.00001524
Iteration 167/1000 | Loss: 0.00001524
Iteration 168/1000 | Loss: 0.00001524
Iteration 169/1000 | Loss: 0.00001524
Iteration 170/1000 | Loss: 0.00001524
Iteration 171/1000 | Loss: 0.00001524
Iteration 172/1000 | Loss: 0.00001524
Iteration 173/1000 | Loss: 0.00001524
Iteration 174/1000 | Loss: 0.00001524
Iteration 175/1000 | Loss: 0.00001524
Iteration 176/1000 | Loss: 0.00001524
Iteration 177/1000 | Loss: 0.00001524
Iteration 178/1000 | Loss: 0.00001524
Iteration 179/1000 | Loss: 0.00001523
Iteration 180/1000 | Loss: 0.00001523
Iteration 181/1000 | Loss: 0.00001523
Iteration 182/1000 | Loss: 0.00001523
Iteration 183/1000 | Loss: 0.00001523
Iteration 184/1000 | Loss: 0.00001523
Iteration 185/1000 | Loss: 0.00001523
Iteration 186/1000 | Loss: 0.00001523
Iteration 187/1000 | Loss: 0.00001523
Iteration 188/1000 | Loss: 0.00001523
Iteration 189/1000 | Loss: 0.00001523
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001523
Iteration 193/1000 | Loss: 0.00001523
Iteration 194/1000 | Loss: 0.00001523
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001523
Iteration 197/1000 | Loss: 0.00001523
Iteration 198/1000 | Loss: 0.00001523
Iteration 199/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.5228732081595808e-05, 1.5228732081595808e-05, 1.5228732081595808e-05, 1.5228732081595808e-05, 1.5228732081595808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5228732081595808e-05

Optimization complete. Final v2v error: 3.3134427070617676 mm

Highest mean error: 3.837289810180664 mm for frame 45

Lowest mean error: 2.9498236179351807 mm for frame 103

Saving results

Total time: 39.921279191970825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00635863
Iteration 2/25 | Loss: 0.00138214
Iteration 3/25 | Loss: 0.00128995
Iteration 4/25 | Loss: 0.00127720
Iteration 5/25 | Loss: 0.00127116
Iteration 6/25 | Loss: 0.00126961
Iteration 7/25 | Loss: 0.00126961
Iteration 8/25 | Loss: 0.00126961
Iteration 9/25 | Loss: 0.00126961
Iteration 10/25 | Loss: 0.00126961
Iteration 11/25 | Loss: 0.00126961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012696129269897938, 0.0012696129269897938, 0.0012696129269897938, 0.0012696129269897938, 0.0012696129269897938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012696129269897938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.91639137
Iteration 2/25 | Loss: 0.00104234
Iteration 3/25 | Loss: 0.00104234
Iteration 4/25 | Loss: 0.00104234
Iteration 5/25 | Loss: 0.00104234
Iteration 6/25 | Loss: 0.00104234
Iteration 7/25 | Loss: 0.00104234
Iteration 8/25 | Loss: 0.00104234
Iteration 9/25 | Loss: 0.00104234
Iteration 10/25 | Loss: 0.00104234
Iteration 11/25 | Loss: 0.00104234
Iteration 12/25 | Loss: 0.00104234
Iteration 13/25 | Loss: 0.00104234
Iteration 14/25 | Loss: 0.00104234
Iteration 15/25 | Loss: 0.00104234
Iteration 16/25 | Loss: 0.00104234
Iteration 17/25 | Loss: 0.00104234
Iteration 18/25 | Loss: 0.00104234
Iteration 19/25 | Loss: 0.00104234
Iteration 20/25 | Loss: 0.00104234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010423382045701146, 0.0010423382045701146, 0.0010423382045701146, 0.0010423382045701146, 0.0010423382045701146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010423382045701146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104234
Iteration 2/1000 | Loss: 0.00003732
Iteration 3/1000 | Loss: 0.00002370
Iteration 4/1000 | Loss: 0.00002097
Iteration 5/1000 | Loss: 0.00002007
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001882
Iteration 10/1000 | Loss: 0.00001881
Iteration 11/1000 | Loss: 0.00001871
Iteration 12/1000 | Loss: 0.00001852
Iteration 13/1000 | Loss: 0.00001851
Iteration 14/1000 | Loss: 0.00001842
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001823
Iteration 17/1000 | Loss: 0.00001819
Iteration 18/1000 | Loss: 0.00001817
Iteration 19/1000 | Loss: 0.00001817
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001816
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001815
Iteration 24/1000 | Loss: 0.00001810
Iteration 25/1000 | Loss: 0.00001810
Iteration 26/1000 | Loss: 0.00001810
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001809
Iteration 31/1000 | Loss: 0.00001809
Iteration 32/1000 | Loss: 0.00001809
Iteration 33/1000 | Loss: 0.00001809
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001808
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001806
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001805
Iteration 41/1000 | Loss: 0.00001805
Iteration 42/1000 | Loss: 0.00001804
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001804
Iteration 45/1000 | Loss: 0.00001801
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001801
Iteration 48/1000 | Loss: 0.00001800
Iteration 49/1000 | Loss: 0.00001800
Iteration 50/1000 | Loss: 0.00001800
Iteration 51/1000 | Loss: 0.00001799
Iteration 52/1000 | Loss: 0.00001799
Iteration 53/1000 | Loss: 0.00001798
Iteration 54/1000 | Loss: 0.00001798
Iteration 55/1000 | Loss: 0.00001797
Iteration 56/1000 | Loss: 0.00001797
Iteration 57/1000 | Loss: 0.00001797
Iteration 58/1000 | Loss: 0.00001797
Iteration 59/1000 | Loss: 0.00001797
Iteration 60/1000 | Loss: 0.00001796
Iteration 61/1000 | Loss: 0.00001796
Iteration 62/1000 | Loss: 0.00001796
Iteration 63/1000 | Loss: 0.00001796
Iteration 64/1000 | Loss: 0.00001796
Iteration 65/1000 | Loss: 0.00001796
Iteration 66/1000 | Loss: 0.00001796
Iteration 67/1000 | Loss: 0.00001796
Iteration 68/1000 | Loss: 0.00001795
Iteration 69/1000 | Loss: 0.00001795
Iteration 70/1000 | Loss: 0.00001795
Iteration 71/1000 | Loss: 0.00001795
Iteration 72/1000 | Loss: 0.00001794
Iteration 73/1000 | Loss: 0.00001794
Iteration 74/1000 | Loss: 0.00001794
Iteration 75/1000 | Loss: 0.00001793
Iteration 76/1000 | Loss: 0.00001793
Iteration 77/1000 | Loss: 0.00001793
Iteration 78/1000 | Loss: 0.00001792
Iteration 79/1000 | Loss: 0.00001792
Iteration 80/1000 | Loss: 0.00001792
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001792
Iteration 83/1000 | Loss: 0.00001792
Iteration 84/1000 | Loss: 0.00001792
Iteration 85/1000 | Loss: 0.00001791
Iteration 86/1000 | Loss: 0.00001791
Iteration 87/1000 | Loss: 0.00001791
Iteration 88/1000 | Loss: 0.00001791
Iteration 89/1000 | Loss: 0.00001791
Iteration 90/1000 | Loss: 0.00001791
Iteration 91/1000 | Loss: 0.00001791
Iteration 92/1000 | Loss: 0.00001791
Iteration 93/1000 | Loss: 0.00001791
Iteration 94/1000 | Loss: 0.00001791
Iteration 95/1000 | Loss: 0.00001791
Iteration 96/1000 | Loss: 0.00001791
Iteration 97/1000 | Loss: 0.00001791
Iteration 98/1000 | Loss: 0.00001790
Iteration 99/1000 | Loss: 0.00001790
Iteration 100/1000 | Loss: 0.00001790
Iteration 101/1000 | Loss: 0.00001790
Iteration 102/1000 | Loss: 0.00001790
Iteration 103/1000 | Loss: 0.00001790
Iteration 104/1000 | Loss: 0.00001789
Iteration 105/1000 | Loss: 0.00001789
Iteration 106/1000 | Loss: 0.00001789
Iteration 107/1000 | Loss: 0.00001789
Iteration 108/1000 | Loss: 0.00001789
Iteration 109/1000 | Loss: 0.00001789
Iteration 110/1000 | Loss: 0.00001789
Iteration 111/1000 | Loss: 0.00001789
Iteration 112/1000 | Loss: 0.00001789
Iteration 113/1000 | Loss: 0.00001789
Iteration 114/1000 | Loss: 0.00001788
Iteration 115/1000 | Loss: 0.00001788
Iteration 116/1000 | Loss: 0.00001788
Iteration 117/1000 | Loss: 0.00001788
Iteration 118/1000 | Loss: 0.00001788
Iteration 119/1000 | Loss: 0.00001788
Iteration 120/1000 | Loss: 0.00001788
Iteration 121/1000 | Loss: 0.00001787
Iteration 122/1000 | Loss: 0.00001787
Iteration 123/1000 | Loss: 0.00001787
Iteration 124/1000 | Loss: 0.00001787
Iteration 125/1000 | Loss: 0.00001787
Iteration 126/1000 | Loss: 0.00001787
Iteration 127/1000 | Loss: 0.00001787
Iteration 128/1000 | Loss: 0.00001787
Iteration 129/1000 | Loss: 0.00001787
Iteration 130/1000 | Loss: 0.00001787
Iteration 131/1000 | Loss: 0.00001787
Iteration 132/1000 | Loss: 0.00001787
Iteration 133/1000 | Loss: 0.00001787
Iteration 134/1000 | Loss: 0.00001787
Iteration 135/1000 | Loss: 0.00001786
Iteration 136/1000 | Loss: 0.00001786
Iteration 137/1000 | Loss: 0.00001786
Iteration 138/1000 | Loss: 0.00001786
Iteration 139/1000 | Loss: 0.00001786
Iteration 140/1000 | Loss: 0.00001786
Iteration 141/1000 | Loss: 0.00001786
Iteration 142/1000 | Loss: 0.00001786
Iteration 143/1000 | Loss: 0.00001786
Iteration 144/1000 | Loss: 0.00001786
Iteration 145/1000 | Loss: 0.00001786
Iteration 146/1000 | Loss: 0.00001786
Iteration 147/1000 | Loss: 0.00001786
Iteration 148/1000 | Loss: 0.00001786
Iteration 149/1000 | Loss: 0.00001786
Iteration 150/1000 | Loss: 0.00001786
Iteration 151/1000 | Loss: 0.00001786
Iteration 152/1000 | Loss: 0.00001786
Iteration 153/1000 | Loss: 0.00001786
Iteration 154/1000 | Loss: 0.00001786
Iteration 155/1000 | Loss: 0.00001786
Iteration 156/1000 | Loss: 0.00001786
Iteration 157/1000 | Loss: 0.00001786
Iteration 158/1000 | Loss: 0.00001786
Iteration 159/1000 | Loss: 0.00001786
Iteration 160/1000 | Loss: 0.00001786
Iteration 161/1000 | Loss: 0.00001786
Iteration 162/1000 | Loss: 0.00001786
Iteration 163/1000 | Loss: 0.00001786
Iteration 164/1000 | Loss: 0.00001786
Iteration 165/1000 | Loss: 0.00001786
Iteration 166/1000 | Loss: 0.00001786
Iteration 167/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.7860318621387705e-05, 1.7860318621387705e-05, 1.7860318621387705e-05, 1.7860318621387705e-05, 1.7860318621387705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7860318621387705e-05

Optimization complete. Final v2v error: 3.5664422512054443 mm

Highest mean error: 3.914064645767212 mm for frame 117

Lowest mean error: 3.4012157917022705 mm for frame 48

Saving results

Total time: 36.38534998893738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421682
Iteration 2/25 | Loss: 0.00138011
Iteration 3/25 | Loss: 0.00129533
Iteration 4/25 | Loss: 0.00127586
Iteration 5/25 | Loss: 0.00127083
Iteration 6/25 | Loss: 0.00127046
Iteration 7/25 | Loss: 0.00127046
Iteration 8/25 | Loss: 0.00127046
Iteration 9/25 | Loss: 0.00127046
Iteration 10/25 | Loss: 0.00127046
Iteration 11/25 | Loss: 0.00127046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012704608961939812, 0.0012704608961939812, 0.0012704608961939812, 0.0012704608961939812, 0.0012704608961939812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012704608961939812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35320294
Iteration 2/25 | Loss: 0.00096990
Iteration 3/25 | Loss: 0.00096990
Iteration 4/25 | Loss: 0.00096990
Iteration 5/25 | Loss: 0.00096990
Iteration 6/25 | Loss: 0.00096989
Iteration 7/25 | Loss: 0.00096989
Iteration 8/25 | Loss: 0.00096989
Iteration 9/25 | Loss: 0.00096989
Iteration 10/25 | Loss: 0.00096989
Iteration 11/25 | Loss: 0.00096989
Iteration 12/25 | Loss: 0.00096989
Iteration 13/25 | Loss: 0.00096989
Iteration 14/25 | Loss: 0.00096989
Iteration 15/25 | Loss: 0.00096989
Iteration 16/25 | Loss: 0.00096989
Iteration 17/25 | Loss: 0.00096989
Iteration 18/25 | Loss: 0.00096989
Iteration 19/25 | Loss: 0.00096989
Iteration 20/25 | Loss: 0.00096989
Iteration 21/25 | Loss: 0.00096989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009698934154585004, 0.0009698934154585004, 0.0009698934154585004, 0.0009698934154585004, 0.0009698934154585004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009698934154585004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096989
Iteration 2/1000 | Loss: 0.00004174
Iteration 3/1000 | Loss: 0.00002342
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00002015
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001881
Iteration 10/1000 | Loss: 0.00001864
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001834
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001827
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001816
Iteration 19/1000 | Loss: 0.00001816
Iteration 20/1000 | Loss: 0.00001816
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001815
Iteration 23/1000 | Loss: 0.00001809
Iteration 24/1000 | Loss: 0.00001803
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001801
Iteration 27/1000 | Loss: 0.00001801
Iteration 28/1000 | Loss: 0.00001801
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001801
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001798
Iteration 38/1000 | Loss: 0.00001798
Iteration 39/1000 | Loss: 0.00001798
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001797
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001796
Iteration 48/1000 | Loss: 0.00001796
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001796
Iteration 51/1000 | Loss: 0.00001796
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001796
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001792
Iteration 59/1000 | Loss: 0.00001792
Iteration 60/1000 | Loss: 0.00001792
Iteration 61/1000 | Loss: 0.00001791
Iteration 62/1000 | Loss: 0.00001791
Iteration 63/1000 | Loss: 0.00001791
Iteration 64/1000 | Loss: 0.00001790
Iteration 65/1000 | Loss: 0.00001790
Iteration 66/1000 | Loss: 0.00001790
Iteration 67/1000 | Loss: 0.00001790
Iteration 68/1000 | Loss: 0.00001790
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001790
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001788
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001787
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001787
Iteration 84/1000 | Loss: 0.00001785
Iteration 85/1000 | Loss: 0.00001785
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001784
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001783
Iteration 104/1000 | Loss: 0.00001783
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001781
Iteration 110/1000 | Loss: 0.00001781
Iteration 111/1000 | Loss: 0.00001781
Iteration 112/1000 | Loss: 0.00001781
Iteration 113/1000 | Loss: 0.00001781
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001781
Iteration 116/1000 | Loss: 0.00001781
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001780
Iteration 120/1000 | Loss: 0.00001780
Iteration 121/1000 | Loss: 0.00001780
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001780
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001780
Iteration 126/1000 | Loss: 0.00001780
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001780
Iteration 130/1000 | Loss: 0.00001780
Iteration 131/1000 | Loss: 0.00001780
Iteration 132/1000 | Loss: 0.00001780
Iteration 133/1000 | Loss: 0.00001780
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001780
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.7804253729991615e-05, 1.7804253729991615e-05, 1.7804253729991615e-05, 1.7804253729991615e-05, 1.7804253729991615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7804253729991615e-05

Optimization complete. Final v2v error: 3.6002683639526367 mm

Highest mean error: 3.8116586208343506 mm for frame 26

Lowest mean error: 3.340303897857666 mm for frame 114

Saving results

Total time: 33.64242458343506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00939118
Iteration 2/25 | Loss: 0.00198053
Iteration 3/25 | Loss: 0.00148258
Iteration 4/25 | Loss: 0.00146284
Iteration 5/25 | Loss: 0.00145703
Iteration 6/25 | Loss: 0.00145459
Iteration 7/25 | Loss: 0.00145396
Iteration 8/25 | Loss: 0.00145396
Iteration 9/25 | Loss: 0.00145396
Iteration 10/25 | Loss: 0.00145396
Iteration 11/25 | Loss: 0.00145396
Iteration 12/25 | Loss: 0.00145396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014539631083607674, 0.0014539631083607674, 0.0014539631083607674, 0.0014539631083607674, 0.0014539631083607674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014539631083607674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85490721
Iteration 2/25 | Loss: 0.00122325
Iteration 3/25 | Loss: 0.00122325
Iteration 4/25 | Loss: 0.00122325
Iteration 5/25 | Loss: 0.00122325
Iteration 6/25 | Loss: 0.00122325
Iteration 7/25 | Loss: 0.00122325
Iteration 8/25 | Loss: 0.00122325
Iteration 9/25 | Loss: 0.00122325
Iteration 10/25 | Loss: 0.00122325
Iteration 11/25 | Loss: 0.00122325
Iteration 12/25 | Loss: 0.00122325
Iteration 13/25 | Loss: 0.00122325
Iteration 14/25 | Loss: 0.00122325
Iteration 15/25 | Loss: 0.00122325
Iteration 16/25 | Loss: 0.00122325
Iteration 17/25 | Loss: 0.00122325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012232462177053094, 0.0012232462177053094, 0.0012232462177053094, 0.0012232462177053094, 0.0012232462177053094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012232462177053094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122325
Iteration 2/1000 | Loss: 0.00013350
Iteration 3/1000 | Loss: 0.00008213
Iteration 4/1000 | Loss: 0.00006848
Iteration 5/1000 | Loss: 0.00006273
Iteration 6/1000 | Loss: 0.00006018
Iteration 7/1000 | Loss: 0.00005833
Iteration 8/1000 | Loss: 0.00005712
Iteration 9/1000 | Loss: 0.00005608
Iteration 10/1000 | Loss: 0.00005515
Iteration 11/1000 | Loss: 0.00005428
Iteration 12/1000 | Loss: 0.00005332
Iteration 13/1000 | Loss: 0.00005253
Iteration 14/1000 | Loss: 0.00005194
Iteration 15/1000 | Loss: 0.00005148
Iteration 16/1000 | Loss: 0.00005107
Iteration 17/1000 | Loss: 0.00005063
Iteration 18/1000 | Loss: 0.00005018
Iteration 19/1000 | Loss: 0.00004988
Iteration 20/1000 | Loss: 0.00004962
Iteration 21/1000 | Loss: 0.00004944
Iteration 22/1000 | Loss: 0.00004931
Iteration 23/1000 | Loss: 0.00004918
Iteration 24/1000 | Loss: 0.00004912
Iteration 25/1000 | Loss: 0.00004907
Iteration 26/1000 | Loss: 0.00004906
Iteration 27/1000 | Loss: 0.00004903
Iteration 28/1000 | Loss: 0.00004902
Iteration 29/1000 | Loss: 0.00004902
Iteration 30/1000 | Loss: 0.00004901
Iteration 31/1000 | Loss: 0.00004899
Iteration 32/1000 | Loss: 0.00004896
Iteration 33/1000 | Loss: 0.00004895
Iteration 34/1000 | Loss: 0.00004895
Iteration 35/1000 | Loss: 0.00004895
Iteration 36/1000 | Loss: 0.00004895
Iteration 37/1000 | Loss: 0.00004895
Iteration 38/1000 | Loss: 0.00004894
Iteration 39/1000 | Loss: 0.00004894
Iteration 40/1000 | Loss: 0.00004893
Iteration 41/1000 | Loss: 0.00004893
Iteration 42/1000 | Loss: 0.00004892
Iteration 43/1000 | Loss: 0.00004892
Iteration 44/1000 | Loss: 0.00004891
Iteration 45/1000 | Loss: 0.00004890
Iteration 46/1000 | Loss: 0.00004890
Iteration 47/1000 | Loss: 0.00004890
Iteration 48/1000 | Loss: 0.00004890
Iteration 49/1000 | Loss: 0.00004890
Iteration 50/1000 | Loss: 0.00004890
Iteration 51/1000 | Loss: 0.00004890
Iteration 52/1000 | Loss: 0.00004890
Iteration 53/1000 | Loss: 0.00004890
Iteration 54/1000 | Loss: 0.00004888
Iteration 55/1000 | Loss: 0.00004887
Iteration 56/1000 | Loss: 0.00004887
Iteration 57/1000 | Loss: 0.00004887
Iteration 58/1000 | Loss: 0.00004886
Iteration 59/1000 | Loss: 0.00004886
Iteration 60/1000 | Loss: 0.00004886
Iteration 61/1000 | Loss: 0.00004886
Iteration 62/1000 | Loss: 0.00004885
Iteration 63/1000 | Loss: 0.00004885
Iteration 64/1000 | Loss: 0.00004885
Iteration 65/1000 | Loss: 0.00004885
Iteration 66/1000 | Loss: 0.00004884
Iteration 67/1000 | Loss: 0.00004884
Iteration 68/1000 | Loss: 0.00004884
Iteration 69/1000 | Loss: 0.00004884
Iteration 70/1000 | Loss: 0.00004884
Iteration 71/1000 | Loss: 0.00004883
Iteration 72/1000 | Loss: 0.00004883
Iteration 73/1000 | Loss: 0.00004883
Iteration 74/1000 | Loss: 0.00004883
Iteration 75/1000 | Loss: 0.00004882
Iteration 76/1000 | Loss: 0.00004882
Iteration 77/1000 | Loss: 0.00004882
Iteration 78/1000 | Loss: 0.00004882
Iteration 79/1000 | Loss: 0.00004881
Iteration 80/1000 | Loss: 0.00004881
Iteration 81/1000 | Loss: 0.00004881
Iteration 82/1000 | Loss: 0.00004881
Iteration 83/1000 | Loss: 0.00004881
Iteration 84/1000 | Loss: 0.00004881
Iteration 85/1000 | Loss: 0.00004881
Iteration 86/1000 | Loss: 0.00004881
Iteration 87/1000 | Loss: 0.00004881
Iteration 88/1000 | Loss: 0.00004880
Iteration 89/1000 | Loss: 0.00004880
Iteration 90/1000 | Loss: 0.00004880
Iteration 91/1000 | Loss: 0.00004880
Iteration 92/1000 | Loss: 0.00004880
Iteration 93/1000 | Loss: 0.00004880
Iteration 94/1000 | Loss: 0.00004880
Iteration 95/1000 | Loss: 0.00004880
Iteration 96/1000 | Loss: 0.00004880
Iteration 97/1000 | Loss: 0.00004879
Iteration 98/1000 | Loss: 0.00004879
Iteration 99/1000 | Loss: 0.00004879
Iteration 100/1000 | Loss: 0.00004879
Iteration 101/1000 | Loss: 0.00004879
Iteration 102/1000 | Loss: 0.00004879
Iteration 103/1000 | Loss: 0.00004878
Iteration 104/1000 | Loss: 0.00004878
Iteration 105/1000 | Loss: 0.00004878
Iteration 106/1000 | Loss: 0.00004878
Iteration 107/1000 | Loss: 0.00004878
Iteration 108/1000 | Loss: 0.00004878
Iteration 109/1000 | Loss: 0.00004878
Iteration 110/1000 | Loss: 0.00004878
Iteration 111/1000 | Loss: 0.00004878
Iteration 112/1000 | Loss: 0.00004877
Iteration 113/1000 | Loss: 0.00004877
Iteration 114/1000 | Loss: 0.00004877
Iteration 115/1000 | Loss: 0.00004877
Iteration 116/1000 | Loss: 0.00004877
Iteration 117/1000 | Loss: 0.00004877
Iteration 118/1000 | Loss: 0.00004877
Iteration 119/1000 | Loss: 0.00004877
Iteration 120/1000 | Loss: 0.00004876
Iteration 121/1000 | Loss: 0.00004876
Iteration 122/1000 | Loss: 0.00004876
Iteration 123/1000 | Loss: 0.00004876
Iteration 124/1000 | Loss: 0.00004876
Iteration 125/1000 | Loss: 0.00004876
Iteration 126/1000 | Loss: 0.00004876
Iteration 127/1000 | Loss: 0.00004876
Iteration 128/1000 | Loss: 0.00004876
Iteration 129/1000 | Loss: 0.00004876
Iteration 130/1000 | Loss: 0.00004876
Iteration 131/1000 | Loss: 0.00004876
Iteration 132/1000 | Loss: 0.00004876
Iteration 133/1000 | Loss: 0.00004876
Iteration 134/1000 | Loss: 0.00004876
Iteration 135/1000 | Loss: 0.00004876
Iteration 136/1000 | Loss: 0.00004876
Iteration 137/1000 | Loss: 0.00004875
Iteration 138/1000 | Loss: 0.00004875
Iteration 139/1000 | Loss: 0.00004875
Iteration 140/1000 | Loss: 0.00004875
Iteration 141/1000 | Loss: 0.00004875
Iteration 142/1000 | Loss: 0.00004875
Iteration 143/1000 | Loss: 0.00004875
Iteration 144/1000 | Loss: 0.00004875
Iteration 145/1000 | Loss: 0.00004875
Iteration 146/1000 | Loss: 0.00004875
Iteration 147/1000 | Loss: 0.00004875
Iteration 148/1000 | Loss: 0.00004875
Iteration 149/1000 | Loss: 0.00004875
Iteration 150/1000 | Loss: 0.00004875
Iteration 151/1000 | Loss: 0.00004875
Iteration 152/1000 | Loss: 0.00004875
Iteration 153/1000 | Loss: 0.00004875
Iteration 154/1000 | Loss: 0.00004875
Iteration 155/1000 | Loss: 0.00004875
Iteration 156/1000 | Loss: 0.00004874
Iteration 157/1000 | Loss: 0.00004874
Iteration 158/1000 | Loss: 0.00004874
Iteration 159/1000 | Loss: 0.00004874
Iteration 160/1000 | Loss: 0.00004874
Iteration 161/1000 | Loss: 0.00004874
Iteration 162/1000 | Loss: 0.00004874
Iteration 163/1000 | Loss: 0.00004874
Iteration 164/1000 | Loss: 0.00004874
Iteration 165/1000 | Loss: 0.00004874
Iteration 166/1000 | Loss: 0.00004874
Iteration 167/1000 | Loss: 0.00004874
Iteration 168/1000 | Loss: 0.00004874
Iteration 169/1000 | Loss: 0.00004874
Iteration 170/1000 | Loss: 0.00004874
Iteration 171/1000 | Loss: 0.00004874
Iteration 172/1000 | Loss: 0.00004873
Iteration 173/1000 | Loss: 0.00004873
Iteration 174/1000 | Loss: 0.00004873
Iteration 175/1000 | Loss: 0.00004873
Iteration 176/1000 | Loss: 0.00004873
Iteration 177/1000 | Loss: 0.00004873
Iteration 178/1000 | Loss: 0.00004873
Iteration 179/1000 | Loss: 0.00004873
Iteration 180/1000 | Loss: 0.00004873
Iteration 181/1000 | Loss: 0.00004873
Iteration 182/1000 | Loss: 0.00004873
Iteration 183/1000 | Loss: 0.00004873
Iteration 184/1000 | Loss: 0.00004873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [4.8730678827269e-05, 4.8730678827269e-05, 4.8730678827269e-05, 4.8730678827269e-05, 4.8730678827269e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.8730678827269e-05

Optimization complete. Final v2v error: 5.452390670776367 mm

Highest mean error: 6.414639949798584 mm for frame 28

Lowest mean error: 3.60306453704834 mm for frame 133

Saving results

Total time: 53.71458697319031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546264
Iteration 2/25 | Loss: 0.00161444
Iteration 3/25 | Loss: 0.00149289
Iteration 4/25 | Loss: 0.00147567
Iteration 5/25 | Loss: 0.00147163
Iteration 6/25 | Loss: 0.00147163
Iteration 7/25 | Loss: 0.00147163
Iteration 8/25 | Loss: 0.00147163
Iteration 9/25 | Loss: 0.00147163
Iteration 10/25 | Loss: 0.00147163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014716254081577063, 0.0014716254081577063, 0.0014716254081577063, 0.0014716254081577063, 0.0014716254081577063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014716254081577063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77202851
Iteration 2/25 | Loss: 0.00115012
Iteration 3/25 | Loss: 0.00115011
Iteration 4/25 | Loss: 0.00115011
Iteration 5/25 | Loss: 0.00115011
Iteration 6/25 | Loss: 0.00115011
Iteration 7/25 | Loss: 0.00115011
Iteration 8/25 | Loss: 0.00115011
Iteration 9/25 | Loss: 0.00115011
Iteration 10/25 | Loss: 0.00115011
Iteration 11/25 | Loss: 0.00115011
Iteration 12/25 | Loss: 0.00115011
Iteration 13/25 | Loss: 0.00115011
Iteration 14/25 | Loss: 0.00115011
Iteration 15/25 | Loss: 0.00115011
Iteration 16/25 | Loss: 0.00115011
Iteration 17/25 | Loss: 0.00115011
Iteration 18/25 | Loss: 0.00115011
Iteration 19/25 | Loss: 0.00115011
Iteration 20/25 | Loss: 0.00115011
Iteration 21/25 | Loss: 0.00115011
Iteration 22/25 | Loss: 0.00115011
Iteration 23/25 | Loss: 0.00115011
Iteration 24/25 | Loss: 0.00115011
Iteration 25/25 | Loss: 0.00115011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115011
Iteration 2/1000 | Loss: 0.00007892
Iteration 3/1000 | Loss: 0.00006410
Iteration 4/1000 | Loss: 0.00006071
Iteration 5/1000 | Loss: 0.00005926
Iteration 6/1000 | Loss: 0.00005821
Iteration 7/1000 | Loss: 0.00005733
Iteration 8/1000 | Loss: 0.00005658
Iteration 9/1000 | Loss: 0.00005619
Iteration 10/1000 | Loss: 0.00005586
Iteration 11/1000 | Loss: 0.00005551
Iteration 12/1000 | Loss: 0.00005508
Iteration 13/1000 | Loss: 0.00005472
Iteration 14/1000 | Loss: 0.00005429
Iteration 15/1000 | Loss: 0.00005403
Iteration 16/1000 | Loss: 0.00005375
Iteration 17/1000 | Loss: 0.00005353
Iteration 18/1000 | Loss: 0.00005331
Iteration 19/1000 | Loss: 0.00005318
Iteration 20/1000 | Loss: 0.00005307
Iteration 21/1000 | Loss: 0.00005305
Iteration 22/1000 | Loss: 0.00005305
Iteration 23/1000 | Loss: 0.00005293
Iteration 24/1000 | Loss: 0.00005286
Iteration 25/1000 | Loss: 0.00005285
Iteration 26/1000 | Loss: 0.00005280
Iteration 27/1000 | Loss: 0.00005280
Iteration 28/1000 | Loss: 0.00005280
Iteration 29/1000 | Loss: 0.00005279
Iteration 30/1000 | Loss: 0.00005279
Iteration 31/1000 | Loss: 0.00005279
Iteration 32/1000 | Loss: 0.00005279
Iteration 33/1000 | Loss: 0.00005278
Iteration 34/1000 | Loss: 0.00005278
Iteration 35/1000 | Loss: 0.00005278
Iteration 36/1000 | Loss: 0.00005277
Iteration 37/1000 | Loss: 0.00005277
Iteration 38/1000 | Loss: 0.00005277
Iteration 39/1000 | Loss: 0.00005277
Iteration 40/1000 | Loss: 0.00005277
Iteration 41/1000 | Loss: 0.00005277
Iteration 42/1000 | Loss: 0.00005277
Iteration 43/1000 | Loss: 0.00005277
Iteration 44/1000 | Loss: 0.00005277
Iteration 45/1000 | Loss: 0.00005277
Iteration 46/1000 | Loss: 0.00005277
Iteration 47/1000 | Loss: 0.00005277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 47. Stopping optimization.
Last 5 losses: [5.2768846217077225e-05, 5.2768846217077225e-05, 5.2768846217077225e-05, 5.2768846217077225e-05, 5.2768846217077225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.2768846217077225e-05

Optimization complete. Final v2v error: 5.476365566253662 mm

Highest mean error: 5.889577865600586 mm for frame 19

Lowest mean error: 5.185870170593262 mm for frame 137

Saving results

Total time: 46.40821027755737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877105
Iteration 2/25 | Loss: 0.00140204
Iteration 3/25 | Loss: 0.00131834
Iteration 4/25 | Loss: 0.00130933
Iteration 5/25 | Loss: 0.00130624
Iteration 6/25 | Loss: 0.00130571
Iteration 7/25 | Loss: 0.00130571
Iteration 8/25 | Loss: 0.00130569
Iteration 9/25 | Loss: 0.00130569
Iteration 10/25 | Loss: 0.00130569
Iteration 11/25 | Loss: 0.00130569
Iteration 12/25 | Loss: 0.00130569
Iteration 13/25 | Loss: 0.00130569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013056948082521558, 0.0013056948082521558, 0.0013056948082521558, 0.0013056948082521558, 0.0013056948082521558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013056948082521558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45569181
Iteration 2/25 | Loss: 0.00101603
Iteration 3/25 | Loss: 0.00101601
Iteration 4/25 | Loss: 0.00101601
Iteration 5/25 | Loss: 0.00101601
Iteration 6/25 | Loss: 0.00101601
Iteration 7/25 | Loss: 0.00101601
Iteration 8/25 | Loss: 0.00101601
Iteration 9/25 | Loss: 0.00101601
Iteration 10/25 | Loss: 0.00101601
Iteration 11/25 | Loss: 0.00101601
Iteration 12/25 | Loss: 0.00101601
Iteration 13/25 | Loss: 0.00101601
Iteration 14/25 | Loss: 0.00101601
Iteration 15/25 | Loss: 0.00101601
Iteration 16/25 | Loss: 0.00101601
Iteration 17/25 | Loss: 0.00101601
Iteration 18/25 | Loss: 0.00101601
Iteration 19/25 | Loss: 0.00101601
Iteration 20/25 | Loss: 0.00101601
Iteration 21/25 | Loss: 0.00101601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001016009016893804, 0.001016009016893804, 0.001016009016893804, 0.001016009016893804, 0.001016009016893804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001016009016893804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101601
Iteration 2/1000 | Loss: 0.00004422
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002505
Iteration 5/1000 | Loss: 0.00002358
Iteration 6/1000 | Loss: 0.00002266
Iteration 7/1000 | Loss: 0.00002188
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002117
Iteration 10/1000 | Loss: 0.00002106
Iteration 11/1000 | Loss: 0.00002092
Iteration 12/1000 | Loss: 0.00002080
Iteration 13/1000 | Loss: 0.00002075
Iteration 14/1000 | Loss: 0.00002068
Iteration 15/1000 | Loss: 0.00002068
Iteration 16/1000 | Loss: 0.00002068
Iteration 17/1000 | Loss: 0.00002068
Iteration 18/1000 | Loss: 0.00002066
Iteration 19/1000 | Loss: 0.00002066
Iteration 20/1000 | Loss: 0.00002065
Iteration 21/1000 | Loss: 0.00002065
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002064
Iteration 25/1000 | Loss: 0.00002064
Iteration 26/1000 | Loss: 0.00002064
Iteration 27/1000 | Loss: 0.00002064
Iteration 28/1000 | Loss: 0.00002063
Iteration 29/1000 | Loss: 0.00002063
Iteration 30/1000 | Loss: 0.00002063
Iteration 31/1000 | Loss: 0.00002063
Iteration 32/1000 | Loss: 0.00002063
Iteration 33/1000 | Loss: 0.00002062
Iteration 34/1000 | Loss: 0.00002062
Iteration 35/1000 | Loss: 0.00002062
Iteration 36/1000 | Loss: 0.00002062
Iteration 37/1000 | Loss: 0.00002061
Iteration 38/1000 | Loss: 0.00002061
Iteration 39/1000 | Loss: 0.00002061
Iteration 40/1000 | Loss: 0.00002061
Iteration 41/1000 | Loss: 0.00002060
Iteration 42/1000 | Loss: 0.00002060
Iteration 43/1000 | Loss: 0.00002060
Iteration 44/1000 | Loss: 0.00002060
Iteration 45/1000 | Loss: 0.00002059
Iteration 46/1000 | Loss: 0.00002059
Iteration 47/1000 | Loss: 0.00002059
Iteration 48/1000 | Loss: 0.00002058
Iteration 49/1000 | Loss: 0.00002058
Iteration 50/1000 | Loss: 0.00002058
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002057
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002055
Iteration 58/1000 | Loss: 0.00002055
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002055
Iteration 62/1000 | Loss: 0.00002055
Iteration 63/1000 | Loss: 0.00002054
Iteration 64/1000 | Loss: 0.00002054
Iteration 65/1000 | Loss: 0.00002054
Iteration 66/1000 | Loss: 0.00002054
Iteration 67/1000 | Loss: 0.00002054
Iteration 68/1000 | Loss: 0.00002053
Iteration 69/1000 | Loss: 0.00002053
Iteration 70/1000 | Loss: 0.00002053
Iteration 71/1000 | Loss: 0.00002053
Iteration 72/1000 | Loss: 0.00002053
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002052
Iteration 76/1000 | Loss: 0.00002052
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002051
Iteration 85/1000 | Loss: 0.00002050
Iteration 86/1000 | Loss: 0.00002050
Iteration 87/1000 | Loss: 0.00002050
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002050
Iteration 91/1000 | Loss: 0.00002050
Iteration 92/1000 | Loss: 0.00002050
Iteration 93/1000 | Loss: 0.00002050
Iteration 94/1000 | Loss: 0.00002050
Iteration 95/1000 | Loss: 0.00002050
Iteration 96/1000 | Loss: 0.00002050
Iteration 97/1000 | Loss: 0.00002050
Iteration 98/1000 | Loss: 0.00002049
Iteration 99/1000 | Loss: 0.00002049
Iteration 100/1000 | Loss: 0.00002049
Iteration 101/1000 | Loss: 0.00002049
Iteration 102/1000 | Loss: 0.00002048
Iteration 103/1000 | Loss: 0.00002048
Iteration 104/1000 | Loss: 0.00002048
Iteration 105/1000 | Loss: 0.00002048
Iteration 106/1000 | Loss: 0.00002048
Iteration 107/1000 | Loss: 0.00002048
Iteration 108/1000 | Loss: 0.00002048
Iteration 109/1000 | Loss: 0.00002048
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002048
Iteration 112/1000 | Loss: 0.00002048
Iteration 113/1000 | Loss: 0.00002048
Iteration 114/1000 | Loss: 0.00002048
Iteration 115/1000 | Loss: 0.00002048
Iteration 116/1000 | Loss: 0.00002048
Iteration 117/1000 | Loss: 0.00002048
Iteration 118/1000 | Loss: 0.00002048
Iteration 119/1000 | Loss: 0.00002048
Iteration 120/1000 | Loss: 0.00002048
Iteration 121/1000 | Loss: 0.00002048
Iteration 122/1000 | Loss: 0.00002048
Iteration 123/1000 | Loss: 0.00002048
Iteration 124/1000 | Loss: 0.00002048
Iteration 125/1000 | Loss: 0.00002048
Iteration 126/1000 | Loss: 0.00002048
Iteration 127/1000 | Loss: 0.00002048
Iteration 128/1000 | Loss: 0.00002048
Iteration 129/1000 | Loss: 0.00002048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.0475659766816534e-05, 2.0475659766816534e-05, 2.0475659766816534e-05, 2.0475659766816534e-05, 2.0475659766816534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0475659766816534e-05

Optimization complete. Final v2v error: 3.7909717559814453 mm

Highest mean error: 4.53624963760376 mm for frame 142

Lowest mean error: 3.2434582710266113 mm for frame 168

Saving results

Total time: 32.77375364303589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876883
Iteration 2/25 | Loss: 0.00188972
Iteration 3/25 | Loss: 0.00152236
Iteration 4/25 | Loss: 0.00149402
Iteration 5/25 | Loss: 0.00148451
Iteration 6/25 | Loss: 0.00148180
Iteration 7/25 | Loss: 0.00148125
Iteration 8/25 | Loss: 0.00148125
Iteration 9/25 | Loss: 0.00148125
Iteration 10/25 | Loss: 0.00148125
Iteration 11/25 | Loss: 0.00148125
Iteration 12/25 | Loss: 0.00148125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014812533045187593, 0.0014812533045187593, 0.0014812533045187593, 0.0014812533045187593, 0.0014812533045187593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014812533045187593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18503249
Iteration 2/25 | Loss: 0.00136570
Iteration 3/25 | Loss: 0.00136570
Iteration 4/25 | Loss: 0.00136570
Iteration 5/25 | Loss: 0.00136569
Iteration 6/25 | Loss: 0.00136569
Iteration 7/25 | Loss: 0.00136569
Iteration 8/25 | Loss: 0.00136569
Iteration 9/25 | Loss: 0.00136569
Iteration 10/25 | Loss: 0.00136569
Iteration 11/25 | Loss: 0.00136569
Iteration 12/25 | Loss: 0.00136569
Iteration 13/25 | Loss: 0.00136569
Iteration 14/25 | Loss: 0.00136569
Iteration 15/25 | Loss: 0.00136569
Iteration 16/25 | Loss: 0.00136569
Iteration 17/25 | Loss: 0.00136569
Iteration 18/25 | Loss: 0.00136569
Iteration 19/25 | Loss: 0.00136569
Iteration 20/25 | Loss: 0.00136569
Iteration 21/25 | Loss: 0.00136569
Iteration 22/25 | Loss: 0.00136569
Iteration 23/25 | Loss: 0.00136569
Iteration 24/25 | Loss: 0.00136569
Iteration 25/25 | Loss: 0.00136569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136569
Iteration 2/1000 | Loss: 0.00012341
Iteration 3/1000 | Loss: 0.00007481
Iteration 4/1000 | Loss: 0.00005740
Iteration 5/1000 | Loss: 0.00005275
Iteration 6/1000 | Loss: 0.00004987
Iteration 7/1000 | Loss: 0.00004779
Iteration 8/1000 | Loss: 0.00004650
Iteration 9/1000 | Loss: 0.00004520
Iteration 10/1000 | Loss: 0.00004437
Iteration 11/1000 | Loss: 0.00004367
Iteration 12/1000 | Loss: 0.00004312
Iteration 13/1000 | Loss: 0.00004259
Iteration 14/1000 | Loss: 0.00004205
Iteration 15/1000 | Loss: 0.00004162
Iteration 16/1000 | Loss: 0.00004134
Iteration 17/1000 | Loss: 0.00004125
Iteration 18/1000 | Loss: 0.00004106
Iteration 19/1000 | Loss: 0.00004090
Iteration 20/1000 | Loss: 0.00004081
Iteration 21/1000 | Loss: 0.00004064
Iteration 22/1000 | Loss: 0.00004044
Iteration 23/1000 | Loss: 0.00004027
Iteration 24/1000 | Loss: 0.00004025
Iteration 25/1000 | Loss: 0.00004021
Iteration 26/1000 | Loss: 0.00004016
Iteration 27/1000 | Loss: 0.00004016
Iteration 28/1000 | Loss: 0.00004014
Iteration 29/1000 | Loss: 0.00004013
Iteration 30/1000 | Loss: 0.00004013
Iteration 31/1000 | Loss: 0.00004011
Iteration 32/1000 | Loss: 0.00004011
Iteration 33/1000 | Loss: 0.00004011
Iteration 34/1000 | Loss: 0.00004010
Iteration 35/1000 | Loss: 0.00004009
Iteration 36/1000 | Loss: 0.00004008
Iteration 37/1000 | Loss: 0.00004008
Iteration 38/1000 | Loss: 0.00004007
Iteration 39/1000 | Loss: 0.00004005
Iteration 40/1000 | Loss: 0.00004002
Iteration 41/1000 | Loss: 0.00003999
Iteration 42/1000 | Loss: 0.00003998
Iteration 43/1000 | Loss: 0.00003998
Iteration 44/1000 | Loss: 0.00003997
Iteration 45/1000 | Loss: 0.00003997
Iteration 46/1000 | Loss: 0.00003997
Iteration 47/1000 | Loss: 0.00003996
Iteration 48/1000 | Loss: 0.00003996
Iteration 49/1000 | Loss: 0.00003995
Iteration 50/1000 | Loss: 0.00003995
Iteration 51/1000 | Loss: 0.00003995
Iteration 52/1000 | Loss: 0.00003992
Iteration 53/1000 | Loss: 0.00003992
Iteration 54/1000 | Loss: 0.00003990
Iteration 55/1000 | Loss: 0.00003990
Iteration 56/1000 | Loss: 0.00003990
Iteration 57/1000 | Loss: 0.00003990
Iteration 58/1000 | Loss: 0.00003990
Iteration 59/1000 | Loss: 0.00003990
Iteration 60/1000 | Loss: 0.00003989
Iteration 61/1000 | Loss: 0.00003988
Iteration 62/1000 | Loss: 0.00003987
Iteration 63/1000 | Loss: 0.00003987
Iteration 64/1000 | Loss: 0.00003987
Iteration 65/1000 | Loss: 0.00003986
Iteration 66/1000 | Loss: 0.00003986
Iteration 67/1000 | Loss: 0.00003985
Iteration 68/1000 | Loss: 0.00003984
Iteration 69/1000 | Loss: 0.00003984
Iteration 70/1000 | Loss: 0.00003984
Iteration 71/1000 | Loss: 0.00003983
Iteration 72/1000 | Loss: 0.00003983
Iteration 73/1000 | Loss: 0.00003983
Iteration 74/1000 | Loss: 0.00003982
Iteration 75/1000 | Loss: 0.00003982
Iteration 76/1000 | Loss: 0.00003982
Iteration 77/1000 | Loss: 0.00003982
Iteration 78/1000 | Loss: 0.00003982
Iteration 79/1000 | Loss: 0.00003981
Iteration 80/1000 | Loss: 0.00003981
Iteration 81/1000 | Loss: 0.00003981
Iteration 82/1000 | Loss: 0.00003980
Iteration 83/1000 | Loss: 0.00003980
Iteration 84/1000 | Loss: 0.00003980
Iteration 85/1000 | Loss: 0.00003980
Iteration 86/1000 | Loss: 0.00003979
Iteration 87/1000 | Loss: 0.00003979
Iteration 88/1000 | Loss: 0.00003979
Iteration 89/1000 | Loss: 0.00003979
Iteration 90/1000 | Loss: 0.00003978
Iteration 91/1000 | Loss: 0.00003978
Iteration 92/1000 | Loss: 0.00003978
Iteration 93/1000 | Loss: 0.00003978
Iteration 94/1000 | Loss: 0.00003978
Iteration 95/1000 | Loss: 0.00003977
Iteration 96/1000 | Loss: 0.00003977
Iteration 97/1000 | Loss: 0.00003977
Iteration 98/1000 | Loss: 0.00003977
Iteration 99/1000 | Loss: 0.00003977
Iteration 100/1000 | Loss: 0.00003977
Iteration 101/1000 | Loss: 0.00003977
Iteration 102/1000 | Loss: 0.00003976
Iteration 103/1000 | Loss: 0.00003976
Iteration 104/1000 | Loss: 0.00003976
Iteration 105/1000 | Loss: 0.00003976
Iteration 106/1000 | Loss: 0.00003976
Iteration 107/1000 | Loss: 0.00003976
Iteration 108/1000 | Loss: 0.00003976
Iteration 109/1000 | Loss: 0.00003975
Iteration 110/1000 | Loss: 0.00003975
Iteration 111/1000 | Loss: 0.00003975
Iteration 112/1000 | Loss: 0.00003975
Iteration 113/1000 | Loss: 0.00003975
Iteration 114/1000 | Loss: 0.00003974
Iteration 115/1000 | Loss: 0.00003974
Iteration 116/1000 | Loss: 0.00003974
Iteration 117/1000 | Loss: 0.00003974
Iteration 118/1000 | Loss: 0.00003974
Iteration 119/1000 | Loss: 0.00003974
Iteration 120/1000 | Loss: 0.00003974
Iteration 121/1000 | Loss: 0.00003974
Iteration 122/1000 | Loss: 0.00003974
Iteration 123/1000 | Loss: 0.00003974
Iteration 124/1000 | Loss: 0.00003973
Iteration 125/1000 | Loss: 0.00003973
Iteration 126/1000 | Loss: 0.00003973
Iteration 127/1000 | Loss: 0.00003973
Iteration 128/1000 | Loss: 0.00003973
Iteration 129/1000 | Loss: 0.00003973
Iteration 130/1000 | Loss: 0.00003973
Iteration 131/1000 | Loss: 0.00003973
Iteration 132/1000 | Loss: 0.00003973
Iteration 133/1000 | Loss: 0.00003973
Iteration 134/1000 | Loss: 0.00003973
Iteration 135/1000 | Loss: 0.00003973
Iteration 136/1000 | Loss: 0.00003973
Iteration 137/1000 | Loss: 0.00003973
Iteration 138/1000 | Loss: 0.00003972
Iteration 139/1000 | Loss: 0.00003972
Iteration 140/1000 | Loss: 0.00003972
Iteration 141/1000 | Loss: 0.00003972
Iteration 142/1000 | Loss: 0.00003972
Iteration 143/1000 | Loss: 0.00003972
Iteration 144/1000 | Loss: 0.00003972
Iteration 145/1000 | Loss: 0.00003971
Iteration 146/1000 | Loss: 0.00003971
Iteration 147/1000 | Loss: 0.00003971
Iteration 148/1000 | Loss: 0.00003971
Iteration 149/1000 | Loss: 0.00003971
Iteration 150/1000 | Loss: 0.00003971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [3.971491241827607e-05, 3.971491241827607e-05, 3.971491241827607e-05, 3.971491241827607e-05, 3.971491241827607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.971491241827607e-05

Optimization complete. Final v2v error: 5.00818395614624 mm

Highest mean error: 6.874826908111572 mm for frame 158

Lowest mean error: 3.840329885482788 mm for frame 197

Saving results

Total time: 55.385029792785645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861179
Iteration 2/25 | Loss: 0.00195764
Iteration 3/25 | Loss: 0.00147693
Iteration 4/25 | Loss: 0.00144582
Iteration 5/25 | Loss: 0.00143958
Iteration 6/25 | Loss: 0.00143787
Iteration 7/25 | Loss: 0.00143780
Iteration 8/25 | Loss: 0.00143780
Iteration 9/25 | Loss: 0.00143780
Iteration 10/25 | Loss: 0.00143780
Iteration 11/25 | Loss: 0.00143780
Iteration 12/25 | Loss: 0.00143780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014377962797880173, 0.0014377962797880173, 0.0014377962797880173, 0.0014377962797880173, 0.0014377962797880173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014377962797880173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43500066
Iteration 2/25 | Loss: 0.00107516
Iteration 3/25 | Loss: 0.00107514
Iteration 4/25 | Loss: 0.00107514
Iteration 5/25 | Loss: 0.00107514
Iteration 6/25 | Loss: 0.00107514
Iteration 7/25 | Loss: 0.00107514
Iteration 8/25 | Loss: 0.00107514
Iteration 9/25 | Loss: 0.00107514
Iteration 10/25 | Loss: 0.00107514
Iteration 11/25 | Loss: 0.00107514
Iteration 12/25 | Loss: 0.00107514
Iteration 13/25 | Loss: 0.00107514
Iteration 14/25 | Loss: 0.00107514
Iteration 15/25 | Loss: 0.00107514
Iteration 16/25 | Loss: 0.00107514
Iteration 17/25 | Loss: 0.00107514
Iteration 18/25 | Loss: 0.00107514
Iteration 19/25 | Loss: 0.00107514
Iteration 20/25 | Loss: 0.00107514
Iteration 21/25 | Loss: 0.00107514
Iteration 22/25 | Loss: 0.00107514
Iteration 23/25 | Loss: 0.00107514
Iteration 24/25 | Loss: 0.00107514
Iteration 25/25 | Loss: 0.00107514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107514
Iteration 2/1000 | Loss: 0.00013126
Iteration 3/1000 | Loss: 0.00008791
Iteration 4/1000 | Loss: 0.00007817
Iteration 5/1000 | Loss: 0.00007410
Iteration 6/1000 | Loss: 0.00007187
Iteration 7/1000 | Loss: 0.00007024
Iteration 8/1000 | Loss: 0.00006888
Iteration 9/1000 | Loss: 0.00006766
Iteration 10/1000 | Loss: 0.00006660
Iteration 11/1000 | Loss: 0.00006547
Iteration 12/1000 | Loss: 0.00006449
Iteration 13/1000 | Loss: 0.00006389
Iteration 14/1000 | Loss: 0.00006328
Iteration 15/1000 | Loss: 0.00006271
Iteration 16/1000 | Loss: 0.00006231
Iteration 17/1000 | Loss: 0.00006174
Iteration 18/1000 | Loss: 0.00006134
Iteration 19/1000 | Loss: 0.00006106
Iteration 20/1000 | Loss: 0.00006081
Iteration 21/1000 | Loss: 0.00006057
Iteration 22/1000 | Loss: 0.00006042
Iteration 23/1000 | Loss: 0.00006035
Iteration 24/1000 | Loss: 0.00006035
Iteration 25/1000 | Loss: 0.00006033
Iteration 26/1000 | Loss: 0.00006032
Iteration 27/1000 | Loss: 0.00006032
Iteration 28/1000 | Loss: 0.00006032
Iteration 29/1000 | Loss: 0.00006032
Iteration 30/1000 | Loss: 0.00006032
Iteration 31/1000 | Loss: 0.00006032
Iteration 32/1000 | Loss: 0.00006032
Iteration 33/1000 | Loss: 0.00006032
Iteration 34/1000 | Loss: 0.00006032
Iteration 35/1000 | Loss: 0.00006031
Iteration 36/1000 | Loss: 0.00006031
Iteration 37/1000 | Loss: 0.00006030
Iteration 38/1000 | Loss: 0.00006029
Iteration 39/1000 | Loss: 0.00006029
Iteration 40/1000 | Loss: 0.00006029
Iteration 41/1000 | Loss: 0.00006029
Iteration 42/1000 | Loss: 0.00006028
Iteration 43/1000 | Loss: 0.00006028
Iteration 44/1000 | Loss: 0.00006028
Iteration 45/1000 | Loss: 0.00006028
Iteration 46/1000 | Loss: 0.00006028
Iteration 47/1000 | Loss: 0.00006028
Iteration 48/1000 | Loss: 0.00006028
Iteration 49/1000 | Loss: 0.00006027
Iteration 50/1000 | Loss: 0.00006027
Iteration 51/1000 | Loss: 0.00006026
Iteration 52/1000 | Loss: 0.00006026
Iteration 53/1000 | Loss: 0.00006026
Iteration 54/1000 | Loss: 0.00006026
Iteration 55/1000 | Loss: 0.00006026
Iteration 56/1000 | Loss: 0.00006026
Iteration 57/1000 | Loss: 0.00006026
Iteration 58/1000 | Loss: 0.00006025
Iteration 59/1000 | Loss: 0.00006025
Iteration 60/1000 | Loss: 0.00006025
Iteration 61/1000 | Loss: 0.00006024
Iteration 62/1000 | Loss: 0.00006024
Iteration 63/1000 | Loss: 0.00006023
Iteration 64/1000 | Loss: 0.00006023
Iteration 65/1000 | Loss: 0.00006023
Iteration 66/1000 | Loss: 0.00006023
Iteration 67/1000 | Loss: 0.00006023
Iteration 68/1000 | Loss: 0.00006023
Iteration 69/1000 | Loss: 0.00006023
Iteration 70/1000 | Loss: 0.00006023
Iteration 71/1000 | Loss: 0.00006023
Iteration 72/1000 | Loss: 0.00006023
Iteration 73/1000 | Loss: 0.00006022
Iteration 74/1000 | Loss: 0.00006022
Iteration 75/1000 | Loss: 0.00006022
Iteration 76/1000 | Loss: 0.00006022
Iteration 77/1000 | Loss: 0.00006022
Iteration 78/1000 | Loss: 0.00006022
Iteration 79/1000 | Loss: 0.00006022
Iteration 80/1000 | Loss: 0.00006022
Iteration 81/1000 | Loss: 0.00006022
Iteration 82/1000 | Loss: 0.00006022
Iteration 83/1000 | Loss: 0.00006022
Iteration 84/1000 | Loss: 0.00006022
Iteration 85/1000 | Loss: 0.00006022
Iteration 86/1000 | Loss: 0.00006022
Iteration 87/1000 | Loss: 0.00006022
Iteration 88/1000 | Loss: 0.00006022
Iteration 89/1000 | Loss: 0.00006022
Iteration 90/1000 | Loss: 0.00006022
Iteration 91/1000 | Loss: 0.00006022
Iteration 92/1000 | Loss: 0.00006022
Iteration 93/1000 | Loss: 0.00006022
Iteration 94/1000 | Loss: 0.00006022
Iteration 95/1000 | Loss: 0.00006022
Iteration 96/1000 | Loss: 0.00006022
Iteration 97/1000 | Loss: 0.00006022
Iteration 98/1000 | Loss: 0.00006022
Iteration 99/1000 | Loss: 0.00006022
Iteration 100/1000 | Loss: 0.00006022
Iteration 101/1000 | Loss: 0.00006022
Iteration 102/1000 | Loss: 0.00006022
Iteration 103/1000 | Loss: 0.00006022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [6.0223592299735174e-05, 6.0223592299735174e-05, 6.0223592299735174e-05, 6.0223592299735174e-05, 6.0223592299735174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.0223592299735174e-05

Optimization complete. Final v2v error: 5.669931888580322 mm

Highest mean error: 8.274577140808105 mm for frame 61

Lowest mean error: 3.69912052154541 mm for frame 98

Saving results

Total time: 47.17698788642883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040143
Iteration 2/25 | Loss: 0.00312224
Iteration 3/25 | Loss: 0.00235589
Iteration 4/25 | Loss: 0.00229037
Iteration 5/25 | Loss: 0.00232321
Iteration 6/25 | Loss: 0.00228207
Iteration 7/25 | Loss: 0.00217266
Iteration 8/25 | Loss: 0.00212705
Iteration 9/25 | Loss: 0.00206887
Iteration 10/25 | Loss: 0.00205208
Iteration 11/25 | Loss: 0.00204364
Iteration 12/25 | Loss: 0.00204167
Iteration 13/25 | Loss: 0.00203484
Iteration 14/25 | Loss: 0.00203369
Iteration 15/25 | Loss: 0.00203499
Iteration 16/25 | Loss: 0.00203835
Iteration 17/25 | Loss: 0.00203804
Iteration 18/25 | Loss: 0.00204629
Iteration 19/25 | Loss: 0.00204074
Iteration 20/25 | Loss: 0.00203659
Iteration 21/25 | Loss: 0.00203096
Iteration 22/25 | Loss: 0.00203092
Iteration 23/25 | Loss: 0.00203027
Iteration 24/25 | Loss: 0.00203026
Iteration 25/25 | Loss: 0.00203020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31257951
Iteration 2/25 | Loss: 0.00648125
Iteration 3/25 | Loss: 0.00648125
Iteration 4/25 | Loss: 0.00648125
Iteration 5/25 | Loss: 0.00648125
Iteration 6/25 | Loss: 0.00648125
Iteration 7/25 | Loss: 0.00648125
Iteration 8/25 | Loss: 0.00648125
Iteration 9/25 | Loss: 0.00648125
Iteration 10/25 | Loss: 0.00648125
Iteration 11/25 | Loss: 0.00648125
Iteration 12/25 | Loss: 0.00648125
Iteration 13/25 | Loss: 0.00648125
Iteration 14/25 | Loss: 0.00648125
Iteration 15/25 | Loss: 0.00648125
Iteration 16/25 | Loss: 0.00648125
Iteration 17/25 | Loss: 0.00648125
Iteration 18/25 | Loss: 0.00648125
Iteration 19/25 | Loss: 0.00648125
Iteration 20/25 | Loss: 0.00648125
Iteration 21/25 | Loss: 0.00648125
Iteration 22/25 | Loss: 0.00648125
Iteration 23/25 | Loss: 0.00648125
Iteration 24/25 | Loss: 0.00648125
Iteration 25/25 | Loss: 0.00648125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00648125
Iteration 2/1000 | Loss: 0.00099360
Iteration 3/1000 | Loss: 0.00074077
Iteration 4/1000 | Loss: 0.00063469
Iteration 5/1000 | Loss: 0.00056691
Iteration 6/1000 | Loss: 0.00051851
Iteration 7/1000 | Loss: 0.00050733
Iteration 8/1000 | Loss: 0.00047635
Iteration 9/1000 | Loss: 0.00046529
Iteration 10/1000 | Loss: 0.00044301
Iteration 11/1000 | Loss: 0.00043867
Iteration 12/1000 | Loss: 0.00041287
Iteration 13/1000 | Loss: 0.00052674
Iteration 14/1000 | Loss: 0.02285051
Iteration 15/1000 | Loss: 0.01088298
Iteration 16/1000 | Loss: 0.00250431
Iteration 17/1000 | Loss: 0.00112319
Iteration 18/1000 | Loss: 0.00075215
Iteration 19/1000 | Loss: 0.00044183
Iteration 20/1000 | Loss: 0.00048476
Iteration 21/1000 | Loss: 0.00022765
Iteration 22/1000 | Loss: 0.00077588
Iteration 23/1000 | Loss: 0.00016831
Iteration 24/1000 | Loss: 0.00013425
Iteration 25/1000 | Loss: 0.00016036
Iteration 26/1000 | Loss: 0.00009126
Iteration 27/1000 | Loss: 0.00007159
Iteration 28/1000 | Loss: 0.00007220
Iteration 29/1000 | Loss: 0.00005373
Iteration 30/1000 | Loss: 0.00017299
Iteration 31/1000 | Loss: 0.00005263
Iteration 32/1000 | Loss: 0.00004542
Iteration 33/1000 | Loss: 0.00004050
Iteration 34/1000 | Loss: 0.00003637
Iteration 35/1000 | Loss: 0.00003239
Iteration 36/1000 | Loss: 0.00002946
Iteration 37/1000 | Loss: 0.00003979
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00002731
Iteration 40/1000 | Loss: 0.00002618
Iteration 41/1000 | Loss: 0.00002518
Iteration 42/1000 | Loss: 0.00003524
Iteration 43/1000 | Loss: 0.00002557
Iteration 44/1000 | Loss: 0.00004426
Iteration 45/1000 | Loss: 0.00003766
Iteration 46/1000 | Loss: 0.00004266
Iteration 47/1000 | Loss: 0.00003801
Iteration 48/1000 | Loss: 0.00004234
Iteration 49/1000 | Loss: 0.00002292
Iteration 50/1000 | Loss: 0.00002259
Iteration 51/1000 | Loss: 0.00002224
Iteration 52/1000 | Loss: 0.00002195
Iteration 53/1000 | Loss: 0.00002180
Iteration 54/1000 | Loss: 0.00002176
Iteration 55/1000 | Loss: 0.00002166
Iteration 56/1000 | Loss: 0.00002164
Iteration 57/1000 | Loss: 0.00002163
Iteration 58/1000 | Loss: 0.00002159
Iteration 59/1000 | Loss: 0.00002155
Iteration 60/1000 | Loss: 0.00002155
Iteration 61/1000 | Loss: 0.00002154
Iteration 62/1000 | Loss: 0.00002153
Iteration 63/1000 | Loss: 0.00002152
Iteration 64/1000 | Loss: 0.00002152
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002151
Iteration 68/1000 | Loss: 0.00002151
Iteration 69/1000 | Loss: 0.00002151
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002150
Iteration 72/1000 | Loss: 0.00002150
Iteration 73/1000 | Loss: 0.00002149
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00002147
Iteration 76/1000 | Loss: 0.00002147
Iteration 77/1000 | Loss: 0.00002147
Iteration 78/1000 | Loss: 0.00002146
Iteration 79/1000 | Loss: 0.00002143
Iteration 80/1000 | Loss: 0.00002142
Iteration 81/1000 | Loss: 0.00002141
Iteration 82/1000 | Loss: 0.00002141
Iteration 83/1000 | Loss: 0.00002141
Iteration 84/1000 | Loss: 0.00002141
Iteration 85/1000 | Loss: 0.00002140
Iteration 86/1000 | Loss: 0.00002140
Iteration 87/1000 | Loss: 0.00002139
Iteration 88/1000 | Loss: 0.00002139
Iteration 89/1000 | Loss: 0.00002139
Iteration 90/1000 | Loss: 0.00002139
Iteration 91/1000 | Loss: 0.00002138
Iteration 92/1000 | Loss: 0.00002138
Iteration 93/1000 | Loss: 0.00002138
Iteration 94/1000 | Loss: 0.00002138
Iteration 95/1000 | Loss: 0.00002138
Iteration 96/1000 | Loss: 0.00002138
Iteration 97/1000 | Loss: 0.00002137
Iteration 98/1000 | Loss: 0.00002137
Iteration 99/1000 | Loss: 0.00002137
Iteration 100/1000 | Loss: 0.00002136
Iteration 101/1000 | Loss: 0.00002136
Iteration 102/1000 | Loss: 0.00002136
Iteration 103/1000 | Loss: 0.00002136
Iteration 104/1000 | Loss: 0.00002135
Iteration 105/1000 | Loss: 0.00002135
Iteration 106/1000 | Loss: 0.00002135
Iteration 107/1000 | Loss: 0.00002135
Iteration 108/1000 | Loss: 0.00002134
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002132
Iteration 112/1000 | Loss: 0.00002131
Iteration 113/1000 | Loss: 0.00002131
Iteration 114/1000 | Loss: 0.00002131
Iteration 115/1000 | Loss: 0.00002131
Iteration 116/1000 | Loss: 0.00002131
Iteration 117/1000 | Loss: 0.00002131
Iteration 118/1000 | Loss: 0.00002131
Iteration 119/1000 | Loss: 0.00002131
Iteration 120/1000 | Loss: 0.00002131
Iteration 121/1000 | Loss: 0.00002131
Iteration 122/1000 | Loss: 0.00002130
Iteration 123/1000 | Loss: 0.00002130
Iteration 124/1000 | Loss: 0.00002129
Iteration 125/1000 | Loss: 0.00002129
Iteration 126/1000 | Loss: 0.00002129
Iteration 127/1000 | Loss: 0.00002128
Iteration 128/1000 | Loss: 0.00002128
Iteration 129/1000 | Loss: 0.00002127
Iteration 130/1000 | Loss: 0.00002127
Iteration 131/1000 | Loss: 0.00002127
Iteration 132/1000 | Loss: 0.00002127
Iteration 133/1000 | Loss: 0.00002126
Iteration 134/1000 | Loss: 0.00002126
Iteration 135/1000 | Loss: 0.00002126
Iteration 136/1000 | Loss: 0.00002125
Iteration 137/1000 | Loss: 0.00002125
Iteration 138/1000 | Loss: 0.00002125
Iteration 139/1000 | Loss: 0.00002125
Iteration 140/1000 | Loss: 0.00002125
Iteration 141/1000 | Loss: 0.00002125
Iteration 142/1000 | Loss: 0.00002125
Iteration 143/1000 | Loss: 0.00002124
Iteration 144/1000 | Loss: 0.00002124
Iteration 145/1000 | Loss: 0.00002124
Iteration 146/1000 | Loss: 0.00002124
Iteration 147/1000 | Loss: 0.00002123
Iteration 148/1000 | Loss: 0.00002123
Iteration 149/1000 | Loss: 0.00002123
Iteration 150/1000 | Loss: 0.00002123
Iteration 151/1000 | Loss: 0.00002123
Iteration 152/1000 | Loss: 0.00002123
Iteration 153/1000 | Loss: 0.00002123
Iteration 154/1000 | Loss: 0.00002123
Iteration 155/1000 | Loss: 0.00002123
Iteration 156/1000 | Loss: 0.00002122
Iteration 157/1000 | Loss: 0.00002122
Iteration 158/1000 | Loss: 0.00002122
Iteration 159/1000 | Loss: 0.00002122
Iteration 160/1000 | Loss: 0.00002122
Iteration 161/1000 | Loss: 0.00002122
Iteration 162/1000 | Loss: 0.00002122
Iteration 163/1000 | Loss: 0.00002121
Iteration 164/1000 | Loss: 0.00002121
Iteration 165/1000 | Loss: 0.00002121
Iteration 166/1000 | Loss: 0.00002121
Iteration 167/1000 | Loss: 0.00002121
Iteration 168/1000 | Loss: 0.00002121
Iteration 169/1000 | Loss: 0.00002121
Iteration 170/1000 | Loss: 0.00002121
Iteration 171/1000 | Loss: 0.00002120
Iteration 172/1000 | Loss: 0.00002120
Iteration 173/1000 | Loss: 0.00002120
Iteration 174/1000 | Loss: 0.00002120
Iteration 175/1000 | Loss: 0.00002120
Iteration 176/1000 | Loss: 0.00002120
Iteration 177/1000 | Loss: 0.00002120
Iteration 178/1000 | Loss: 0.00002120
Iteration 179/1000 | Loss: 0.00002120
Iteration 180/1000 | Loss: 0.00002120
Iteration 181/1000 | Loss: 0.00002120
Iteration 182/1000 | Loss: 0.00002120
Iteration 183/1000 | Loss: 0.00002120
Iteration 184/1000 | Loss: 0.00002119
Iteration 185/1000 | Loss: 0.00002119
Iteration 186/1000 | Loss: 0.00002119
Iteration 187/1000 | Loss: 0.00002119
Iteration 188/1000 | Loss: 0.00002119
Iteration 189/1000 | Loss: 0.00002119
Iteration 190/1000 | Loss: 0.00002119
Iteration 191/1000 | Loss: 0.00002119
Iteration 192/1000 | Loss: 0.00002119
Iteration 193/1000 | Loss: 0.00002119
Iteration 194/1000 | Loss: 0.00002119
Iteration 195/1000 | Loss: 0.00002119
Iteration 196/1000 | Loss: 0.00002119
Iteration 197/1000 | Loss: 0.00002119
Iteration 198/1000 | Loss: 0.00002119
Iteration 199/1000 | Loss: 0.00002119
Iteration 200/1000 | Loss: 0.00002119
Iteration 201/1000 | Loss: 0.00002119
Iteration 202/1000 | Loss: 0.00002119
Iteration 203/1000 | Loss: 0.00002119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.118921656801831e-05, 2.118921656801831e-05, 2.118921656801831e-05, 2.118921656801831e-05, 2.118921656801831e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.118921656801831e-05

Optimization complete. Final v2v error: 3.4143900871276855 mm

Highest mean error: 20.41839599609375 mm for frame 221

Lowest mean error: 3.0988616943359375 mm for frame 233

Saving results

Total time: 149.84447503089905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908602
Iteration 2/25 | Loss: 0.00167621
Iteration 3/25 | Loss: 0.00147413
Iteration 4/25 | Loss: 0.00145616
Iteration 5/25 | Loss: 0.00145395
Iteration 6/25 | Loss: 0.00145395
Iteration 7/25 | Loss: 0.00145395
Iteration 8/25 | Loss: 0.00145395
Iteration 9/25 | Loss: 0.00145395
Iteration 10/25 | Loss: 0.00145395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014539549592882395, 0.0014539549592882395, 0.0014539549592882395, 0.0014539549592882395, 0.0014539549592882395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014539549592882395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96067923
Iteration 2/25 | Loss: 0.00057442
Iteration 3/25 | Loss: 0.00057442
Iteration 4/25 | Loss: 0.00057442
Iteration 5/25 | Loss: 0.00057442
Iteration 6/25 | Loss: 0.00057442
Iteration 7/25 | Loss: 0.00057442
Iteration 8/25 | Loss: 0.00057442
Iteration 9/25 | Loss: 0.00057442
Iteration 10/25 | Loss: 0.00057442
Iteration 11/25 | Loss: 0.00057442
Iteration 12/25 | Loss: 0.00057442
Iteration 13/25 | Loss: 0.00057442
Iteration 14/25 | Loss: 0.00057442
Iteration 15/25 | Loss: 0.00057442
Iteration 16/25 | Loss: 0.00057442
Iteration 17/25 | Loss: 0.00057442
Iteration 18/25 | Loss: 0.00057442
Iteration 19/25 | Loss: 0.00057442
Iteration 20/25 | Loss: 0.00057442
Iteration 21/25 | Loss: 0.00057442
Iteration 22/25 | Loss: 0.00057442
Iteration 23/25 | Loss: 0.00057442
Iteration 24/25 | Loss: 0.00057442
Iteration 25/25 | Loss: 0.00057442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057442
Iteration 2/1000 | Loss: 0.00005218
Iteration 3/1000 | Loss: 0.00003795
Iteration 4/1000 | Loss: 0.00003077
Iteration 5/1000 | Loss: 0.00002908
Iteration 6/1000 | Loss: 0.00002827
Iteration 7/1000 | Loss: 0.00002769
Iteration 8/1000 | Loss: 0.00002737
Iteration 9/1000 | Loss: 0.00002704
Iteration 10/1000 | Loss: 0.00002683
Iteration 11/1000 | Loss: 0.00002669
Iteration 12/1000 | Loss: 0.00002646
Iteration 13/1000 | Loss: 0.00002643
Iteration 14/1000 | Loss: 0.00002642
Iteration 15/1000 | Loss: 0.00002636
Iteration 16/1000 | Loss: 0.00002634
Iteration 17/1000 | Loss: 0.00002632
Iteration 18/1000 | Loss: 0.00002626
Iteration 19/1000 | Loss: 0.00002625
Iteration 20/1000 | Loss: 0.00002621
Iteration 21/1000 | Loss: 0.00002621
Iteration 22/1000 | Loss: 0.00002620
Iteration 23/1000 | Loss: 0.00002616
Iteration 24/1000 | Loss: 0.00002613
Iteration 25/1000 | Loss: 0.00002613
Iteration 26/1000 | Loss: 0.00002613
Iteration 27/1000 | Loss: 0.00002613
Iteration 28/1000 | Loss: 0.00002613
Iteration 29/1000 | Loss: 0.00002613
Iteration 30/1000 | Loss: 0.00002612
Iteration 31/1000 | Loss: 0.00002610
Iteration 32/1000 | Loss: 0.00002610
Iteration 33/1000 | Loss: 0.00002610
Iteration 34/1000 | Loss: 0.00002610
Iteration 35/1000 | Loss: 0.00002610
Iteration 36/1000 | Loss: 0.00002609
Iteration 37/1000 | Loss: 0.00002609
Iteration 38/1000 | Loss: 0.00002607
Iteration 39/1000 | Loss: 0.00002607
Iteration 40/1000 | Loss: 0.00002607
Iteration 41/1000 | Loss: 0.00002607
Iteration 42/1000 | Loss: 0.00002606
Iteration 43/1000 | Loss: 0.00002605
Iteration 44/1000 | Loss: 0.00002602
Iteration 45/1000 | Loss: 0.00002598
Iteration 46/1000 | Loss: 0.00002597
Iteration 47/1000 | Loss: 0.00002597
Iteration 48/1000 | Loss: 0.00002597
Iteration 49/1000 | Loss: 0.00002597
Iteration 50/1000 | Loss: 0.00002597
Iteration 51/1000 | Loss: 0.00002597
Iteration 52/1000 | Loss: 0.00002597
Iteration 53/1000 | Loss: 0.00002597
Iteration 54/1000 | Loss: 0.00002597
Iteration 55/1000 | Loss: 0.00002597
Iteration 56/1000 | Loss: 0.00002597
Iteration 57/1000 | Loss: 0.00002597
Iteration 58/1000 | Loss: 0.00002596
Iteration 59/1000 | Loss: 0.00002596
Iteration 60/1000 | Loss: 0.00002596
Iteration 61/1000 | Loss: 0.00002596
Iteration 62/1000 | Loss: 0.00002596
Iteration 63/1000 | Loss: 0.00002596
Iteration 64/1000 | Loss: 0.00002596
Iteration 65/1000 | Loss: 0.00002595
Iteration 66/1000 | Loss: 0.00002594
Iteration 67/1000 | Loss: 0.00002594
Iteration 68/1000 | Loss: 0.00002593
Iteration 69/1000 | Loss: 0.00002593
Iteration 70/1000 | Loss: 0.00002593
Iteration 71/1000 | Loss: 0.00002593
Iteration 72/1000 | Loss: 0.00002590
Iteration 73/1000 | Loss: 0.00002589
Iteration 74/1000 | Loss: 0.00002589
Iteration 75/1000 | Loss: 0.00002589
Iteration 76/1000 | Loss: 0.00002589
Iteration 77/1000 | Loss: 0.00002589
Iteration 78/1000 | Loss: 0.00002589
Iteration 79/1000 | Loss: 0.00002589
Iteration 80/1000 | Loss: 0.00002589
Iteration 81/1000 | Loss: 0.00002588
Iteration 82/1000 | Loss: 0.00002588
Iteration 83/1000 | Loss: 0.00002588
Iteration 84/1000 | Loss: 0.00002588
Iteration 85/1000 | Loss: 0.00002587
Iteration 86/1000 | Loss: 0.00002587
Iteration 87/1000 | Loss: 0.00002587
Iteration 88/1000 | Loss: 0.00002587
Iteration 89/1000 | Loss: 0.00002587
Iteration 90/1000 | Loss: 0.00002587
Iteration 91/1000 | Loss: 0.00002587
Iteration 92/1000 | Loss: 0.00002586
Iteration 93/1000 | Loss: 0.00002586
Iteration 94/1000 | Loss: 0.00002586
Iteration 95/1000 | Loss: 0.00002586
Iteration 96/1000 | Loss: 0.00002586
Iteration 97/1000 | Loss: 0.00002586
Iteration 98/1000 | Loss: 0.00002586
Iteration 99/1000 | Loss: 0.00002586
Iteration 100/1000 | Loss: 0.00002586
Iteration 101/1000 | Loss: 0.00002586
Iteration 102/1000 | Loss: 0.00002586
Iteration 103/1000 | Loss: 0.00002585
Iteration 104/1000 | Loss: 0.00002585
Iteration 105/1000 | Loss: 0.00002585
Iteration 106/1000 | Loss: 0.00002584
Iteration 107/1000 | Loss: 0.00002584
Iteration 108/1000 | Loss: 0.00002584
Iteration 109/1000 | Loss: 0.00002584
Iteration 110/1000 | Loss: 0.00002584
Iteration 111/1000 | Loss: 0.00002584
Iteration 112/1000 | Loss: 0.00002583
Iteration 113/1000 | Loss: 0.00002583
Iteration 114/1000 | Loss: 0.00002583
Iteration 115/1000 | Loss: 0.00002583
Iteration 116/1000 | Loss: 0.00002583
Iteration 117/1000 | Loss: 0.00002582
Iteration 118/1000 | Loss: 0.00002582
Iteration 119/1000 | Loss: 0.00002582
Iteration 120/1000 | Loss: 0.00002582
Iteration 121/1000 | Loss: 0.00002582
Iteration 122/1000 | Loss: 0.00002582
Iteration 123/1000 | Loss: 0.00002582
Iteration 124/1000 | Loss: 0.00002582
Iteration 125/1000 | Loss: 0.00002582
Iteration 126/1000 | Loss: 0.00002582
Iteration 127/1000 | Loss: 0.00002582
Iteration 128/1000 | Loss: 0.00002582
Iteration 129/1000 | Loss: 0.00002582
Iteration 130/1000 | Loss: 0.00002582
Iteration 131/1000 | Loss: 0.00002581
Iteration 132/1000 | Loss: 0.00002581
Iteration 133/1000 | Loss: 0.00002581
Iteration 134/1000 | Loss: 0.00002581
Iteration 135/1000 | Loss: 0.00002581
Iteration 136/1000 | Loss: 0.00002581
Iteration 137/1000 | Loss: 0.00002581
Iteration 138/1000 | Loss: 0.00002581
Iteration 139/1000 | Loss: 0.00002581
Iteration 140/1000 | Loss: 0.00002581
Iteration 141/1000 | Loss: 0.00002581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.581383887445554e-05, 2.581383887445554e-05, 2.581383887445554e-05, 2.581383887445554e-05, 2.581383887445554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.581383887445554e-05

Optimization complete. Final v2v error: 4.267100811004639 mm

Highest mean error: 4.3794379234313965 mm for frame 19

Lowest mean error: 4.188090801239014 mm for frame 140

Saving results

Total time: 35.29418706893921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01299761
Iteration 2/25 | Loss: 0.01299761
Iteration 3/25 | Loss: 0.01299761
Iteration 4/25 | Loss: 0.00438783
Iteration 5/25 | Loss: 0.00295275
Iteration 6/25 | Loss: 0.00266884
Iteration 7/25 | Loss: 0.00236002
Iteration 8/25 | Loss: 0.00205442
Iteration 9/25 | Loss: 0.00204951
Iteration 10/25 | Loss: 0.00194435
Iteration 11/25 | Loss: 0.00192045
Iteration 12/25 | Loss: 0.00190617
Iteration 13/25 | Loss: 0.00189609
Iteration 14/25 | Loss: 0.00188539
Iteration 15/25 | Loss: 0.00188480
Iteration 16/25 | Loss: 0.00188260
Iteration 17/25 | Loss: 0.00187280
Iteration 18/25 | Loss: 0.00186398
Iteration 19/25 | Loss: 0.00187488
Iteration 20/25 | Loss: 0.00186627
Iteration 21/25 | Loss: 0.00186374
Iteration 22/25 | Loss: 0.00185273
Iteration 23/25 | Loss: 0.00185045
Iteration 24/25 | Loss: 0.00186432
Iteration 25/25 | Loss: 0.00185476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63410103
Iteration 2/25 | Loss: 0.00390938
Iteration 3/25 | Loss: 0.00390938
Iteration 4/25 | Loss: 0.00390938
Iteration 5/25 | Loss: 0.00390938
Iteration 6/25 | Loss: 0.00390938
Iteration 7/25 | Loss: 0.00390938
Iteration 8/25 | Loss: 0.00390938
Iteration 9/25 | Loss: 0.00390938
Iteration 10/25 | Loss: 0.00390938
Iteration 11/25 | Loss: 0.00390938
Iteration 12/25 | Loss: 0.00390938
Iteration 13/25 | Loss: 0.00390938
Iteration 14/25 | Loss: 0.00390938
Iteration 15/25 | Loss: 0.00390938
Iteration 16/25 | Loss: 0.00390938
Iteration 17/25 | Loss: 0.00390938
Iteration 18/25 | Loss: 0.00390938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003909376449882984, 0.003909376449882984, 0.003909376449882984, 0.003909376449882984, 0.003909376449882984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003909376449882984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390938
Iteration 2/1000 | Loss: 0.00293309
Iteration 3/1000 | Loss: 0.00116397
Iteration 4/1000 | Loss: 0.00173085
Iteration 5/1000 | Loss: 0.00137104
Iteration 6/1000 | Loss: 0.00105624
Iteration 7/1000 | Loss: 0.00054464
Iteration 8/1000 | Loss: 0.00252249
Iteration 9/1000 | Loss: 0.00197726
Iteration 10/1000 | Loss: 0.00195651
Iteration 11/1000 | Loss: 0.00277057
Iteration 12/1000 | Loss: 0.00120490
Iteration 13/1000 | Loss: 0.00207180
Iteration 14/1000 | Loss: 0.00109496
Iteration 15/1000 | Loss: 0.00219805
Iteration 16/1000 | Loss: 0.00060078
Iteration 17/1000 | Loss: 0.00069511
Iteration 18/1000 | Loss: 0.00179020
Iteration 19/1000 | Loss: 0.00128705
Iteration 20/1000 | Loss: 0.00083942
Iteration 21/1000 | Loss: 0.00048530
Iteration 22/1000 | Loss: 0.00033165
Iteration 23/1000 | Loss: 0.00040796
Iteration 24/1000 | Loss: 0.00134237
Iteration 25/1000 | Loss: 0.00133766
Iteration 26/1000 | Loss: 0.00026622
Iteration 27/1000 | Loss: 0.00050505
Iteration 28/1000 | Loss: 0.00163531
Iteration 29/1000 | Loss: 0.00035825
Iteration 30/1000 | Loss: 0.00034818
Iteration 31/1000 | Loss: 0.00083841
Iteration 32/1000 | Loss: 0.00079412
Iteration 33/1000 | Loss: 0.00071583
Iteration 34/1000 | Loss: 0.00104334
Iteration 35/1000 | Loss: 0.00047975
Iteration 36/1000 | Loss: 0.00135976
Iteration 37/1000 | Loss: 0.00030441
Iteration 38/1000 | Loss: 0.00067258
Iteration 39/1000 | Loss: 0.00074982
Iteration 40/1000 | Loss: 0.00058382
Iteration 41/1000 | Loss: 0.00048646
Iteration 42/1000 | Loss: 0.00049430
Iteration 43/1000 | Loss: 0.00093791
Iteration 44/1000 | Loss: 0.00075433
Iteration 45/1000 | Loss: 0.00105196
Iteration 46/1000 | Loss: 0.00084888
Iteration 47/1000 | Loss: 0.00064225
Iteration 48/1000 | Loss: 0.00141238
Iteration 49/1000 | Loss: 0.00042726
Iteration 50/1000 | Loss: 0.00128598
Iteration 51/1000 | Loss: 0.00186160
Iteration 52/1000 | Loss: 0.00108806
Iteration 53/1000 | Loss: 0.00123762
Iteration 54/1000 | Loss: 0.00062920
Iteration 55/1000 | Loss: 0.00056574
Iteration 56/1000 | Loss: 0.00022236
Iteration 57/1000 | Loss: 0.00039667
Iteration 58/1000 | Loss: 0.00134813
Iteration 59/1000 | Loss: 0.00107856
Iteration 60/1000 | Loss: 0.00055735
Iteration 61/1000 | Loss: 0.00059661
Iteration 62/1000 | Loss: 0.00154814
Iteration 63/1000 | Loss: 0.00145696
Iteration 64/1000 | Loss: 0.00083444
Iteration 65/1000 | Loss: 0.00094426
Iteration 66/1000 | Loss: 0.00091333
Iteration 67/1000 | Loss: 0.00099115
Iteration 68/1000 | Loss: 0.00109312
Iteration 69/1000 | Loss: 0.00079067
Iteration 70/1000 | Loss: 0.00081745
Iteration 71/1000 | Loss: 0.00065311
Iteration 72/1000 | Loss: 0.00044212
Iteration 73/1000 | Loss: 0.00048615
Iteration 74/1000 | Loss: 0.00061535
Iteration 75/1000 | Loss: 0.00086983
Iteration 76/1000 | Loss: 0.00063634
Iteration 77/1000 | Loss: 0.00084482
Iteration 78/1000 | Loss: 0.00089131
Iteration 79/1000 | Loss: 0.00060018
Iteration 80/1000 | Loss: 0.00068102
Iteration 81/1000 | Loss: 0.00070980
Iteration 82/1000 | Loss: 0.00062156
Iteration 83/1000 | Loss: 0.00068218
Iteration 84/1000 | Loss: 0.00142241
Iteration 85/1000 | Loss: 0.00068674
Iteration 86/1000 | Loss: 0.00094342
Iteration 87/1000 | Loss: 0.00105450
Iteration 88/1000 | Loss: 0.00092675
Iteration 89/1000 | Loss: 0.00053563
Iteration 90/1000 | Loss: 0.00066602
Iteration 91/1000 | Loss: 0.00056919
Iteration 92/1000 | Loss: 0.00066250
Iteration 93/1000 | Loss: 0.00056398
Iteration 94/1000 | Loss: 0.00055615
Iteration 95/1000 | Loss: 0.00049643
Iteration 96/1000 | Loss: 0.00045820
Iteration 97/1000 | Loss: 0.00058731
Iteration 98/1000 | Loss: 0.00140111
Iteration 99/1000 | Loss: 0.00123904
Iteration 100/1000 | Loss: 0.00071872
Iteration 101/1000 | Loss: 0.00085952
Iteration 102/1000 | Loss: 0.00098183
Iteration 103/1000 | Loss: 0.00091424
Iteration 104/1000 | Loss: 0.00105909
Iteration 105/1000 | Loss: 0.00097577
Iteration 106/1000 | Loss: 0.00068602
Iteration 107/1000 | Loss: 0.00087527
Iteration 108/1000 | Loss: 0.00115972
Iteration 109/1000 | Loss: 0.00093471
Iteration 110/1000 | Loss: 0.00062453
Iteration 111/1000 | Loss: 0.00033385
Iteration 112/1000 | Loss: 0.00051099
Iteration 113/1000 | Loss: 0.00039274
Iteration 114/1000 | Loss: 0.00022650
Iteration 115/1000 | Loss: 0.00021861
Iteration 116/1000 | Loss: 0.00024083
Iteration 117/1000 | Loss: 0.00101769
Iteration 118/1000 | Loss: 0.00049783
Iteration 119/1000 | Loss: 0.00049433
Iteration 120/1000 | Loss: 0.00041430
Iteration 121/1000 | Loss: 0.00026107
Iteration 122/1000 | Loss: 0.00075129
Iteration 123/1000 | Loss: 0.00069860
Iteration 124/1000 | Loss: 0.00059989
Iteration 125/1000 | Loss: 0.00062087
Iteration 126/1000 | Loss: 0.00079945
Iteration 127/1000 | Loss: 0.00075980
Iteration 128/1000 | Loss: 0.00169662
Iteration 129/1000 | Loss: 0.00079433
Iteration 130/1000 | Loss: 0.00043257
Iteration 131/1000 | Loss: 0.00056661
Iteration 132/1000 | Loss: 0.00031025
Iteration 133/1000 | Loss: 0.00035026
Iteration 134/1000 | Loss: 0.00043023
Iteration 135/1000 | Loss: 0.00045048
Iteration 136/1000 | Loss: 0.00043680
Iteration 137/1000 | Loss: 0.00044241
Iteration 138/1000 | Loss: 0.00027070
Iteration 139/1000 | Loss: 0.00028126
Iteration 140/1000 | Loss: 0.00028947
Iteration 141/1000 | Loss: 0.00037370
Iteration 142/1000 | Loss: 0.00032288
Iteration 143/1000 | Loss: 0.00038965
Iteration 144/1000 | Loss: 0.00039082
Iteration 145/1000 | Loss: 0.00021792
Iteration 146/1000 | Loss: 0.00017505
Iteration 147/1000 | Loss: 0.00075038
Iteration 148/1000 | Loss: 0.00033802
Iteration 149/1000 | Loss: 0.00020629
Iteration 150/1000 | Loss: 0.00016450
Iteration 151/1000 | Loss: 0.00044396
Iteration 152/1000 | Loss: 0.00036318
Iteration 153/1000 | Loss: 0.00046700
Iteration 154/1000 | Loss: 0.00019409
Iteration 155/1000 | Loss: 0.00015468
Iteration 156/1000 | Loss: 0.00082808
Iteration 157/1000 | Loss: 0.00073930
Iteration 158/1000 | Loss: 0.00032703
Iteration 159/1000 | Loss: 0.00114905
Iteration 160/1000 | Loss: 0.00048188
Iteration 161/1000 | Loss: 0.00094373
Iteration 162/1000 | Loss: 0.00057647
Iteration 163/1000 | Loss: 0.00043465
Iteration 164/1000 | Loss: 0.00082512
Iteration 165/1000 | Loss: 0.00077723
Iteration 166/1000 | Loss: 0.00045043
Iteration 167/1000 | Loss: 0.00052520
Iteration 168/1000 | Loss: 0.00045439
Iteration 169/1000 | Loss: 0.00038827
Iteration 170/1000 | Loss: 0.00024880
Iteration 171/1000 | Loss: 0.00020093
Iteration 172/1000 | Loss: 0.00019101
Iteration 173/1000 | Loss: 0.00068890
Iteration 174/1000 | Loss: 0.00068702
Iteration 175/1000 | Loss: 0.00060731
Iteration 176/1000 | Loss: 0.00021501
Iteration 177/1000 | Loss: 0.00047189
Iteration 178/1000 | Loss: 0.00026187
Iteration 179/1000 | Loss: 0.00045244
Iteration 180/1000 | Loss: 0.00068970
Iteration 181/1000 | Loss: 0.00084245
Iteration 182/1000 | Loss: 0.00017227
Iteration 183/1000 | Loss: 0.00016761
Iteration 184/1000 | Loss: 0.00015105
Iteration 185/1000 | Loss: 0.00058625
Iteration 186/1000 | Loss: 0.00042303
Iteration 187/1000 | Loss: 0.00092199
Iteration 188/1000 | Loss: 0.00028521
Iteration 189/1000 | Loss: 0.00071806
Iteration 190/1000 | Loss: 0.00072959
Iteration 191/1000 | Loss: 0.00032872
Iteration 192/1000 | Loss: 0.00080505
Iteration 193/1000 | Loss: 0.00071637
Iteration 194/1000 | Loss: 0.00124635
Iteration 195/1000 | Loss: 0.00064097
Iteration 196/1000 | Loss: 0.00091079
Iteration 197/1000 | Loss: 0.00093876
Iteration 198/1000 | Loss: 0.00087482
Iteration 199/1000 | Loss: 0.00049256
Iteration 200/1000 | Loss: 0.00027530
Iteration 201/1000 | Loss: 0.00060800
Iteration 202/1000 | Loss: 0.00035254
Iteration 203/1000 | Loss: 0.00075471
Iteration 204/1000 | Loss: 0.00095311
Iteration 205/1000 | Loss: 0.00079076
Iteration 206/1000 | Loss: 0.00085168
Iteration 207/1000 | Loss: 0.00088620
Iteration 208/1000 | Loss: 0.00067732
Iteration 209/1000 | Loss: 0.00041016
Iteration 210/1000 | Loss: 0.00064758
Iteration 211/1000 | Loss: 0.00053386
Iteration 212/1000 | Loss: 0.00068493
Iteration 213/1000 | Loss: 0.00064651
Iteration 214/1000 | Loss: 0.00017955
Iteration 215/1000 | Loss: 0.00053738
Iteration 216/1000 | Loss: 0.00060335
Iteration 217/1000 | Loss: 0.00062496
Iteration 218/1000 | Loss: 0.00030656
Iteration 219/1000 | Loss: 0.00025831
Iteration 220/1000 | Loss: 0.00043746
Iteration 221/1000 | Loss: 0.00109665
Iteration 222/1000 | Loss: 0.00029950
Iteration 223/1000 | Loss: 0.00061872
Iteration 224/1000 | Loss: 0.00042030
Iteration 225/1000 | Loss: 0.00025115
Iteration 226/1000 | Loss: 0.00017461
Iteration 227/1000 | Loss: 0.00065049
Iteration 228/1000 | Loss: 0.00048989
Iteration 229/1000 | Loss: 0.00097911
Iteration 230/1000 | Loss: 0.00056181
Iteration 231/1000 | Loss: 0.00050938
Iteration 232/1000 | Loss: 0.00027230
Iteration 233/1000 | Loss: 0.00017950
Iteration 234/1000 | Loss: 0.00015076
Iteration 235/1000 | Loss: 0.00025742
Iteration 236/1000 | Loss: 0.00026445
Iteration 237/1000 | Loss: 0.00026267
Iteration 238/1000 | Loss: 0.00095730
Iteration 239/1000 | Loss: 0.00095419
Iteration 240/1000 | Loss: 0.00113869
Iteration 241/1000 | Loss: 0.00045492
Iteration 242/1000 | Loss: 0.00020172
Iteration 243/1000 | Loss: 0.00016500
Iteration 244/1000 | Loss: 0.00044054
Iteration 245/1000 | Loss: 0.00036315
Iteration 246/1000 | Loss: 0.00024016
Iteration 247/1000 | Loss: 0.00031495
Iteration 248/1000 | Loss: 0.00093777
Iteration 249/1000 | Loss: 0.00033225
Iteration 250/1000 | Loss: 0.00080837
Iteration 251/1000 | Loss: 0.00122410
Iteration 252/1000 | Loss: 0.00050678
Iteration 253/1000 | Loss: 0.00054906
Iteration 254/1000 | Loss: 0.00050476
Iteration 255/1000 | Loss: 0.00054555
Iteration 256/1000 | Loss: 0.00092653
Iteration 257/1000 | Loss: 0.00041802
Iteration 258/1000 | Loss: 0.00086342
Iteration 259/1000 | Loss: 0.00051345
Iteration 260/1000 | Loss: 0.00041686
Iteration 261/1000 | Loss: 0.00090585
Iteration 262/1000 | Loss: 0.00044050
Iteration 263/1000 | Loss: 0.00040422
Iteration 264/1000 | Loss: 0.00023958
Iteration 265/1000 | Loss: 0.00028888
Iteration 266/1000 | Loss: 0.00098543
Iteration 267/1000 | Loss: 0.00069173
Iteration 268/1000 | Loss: 0.00079403
Iteration 269/1000 | Loss: 0.00037842
Iteration 270/1000 | Loss: 0.00021622
Iteration 271/1000 | Loss: 0.00029930
Iteration 272/1000 | Loss: 0.00050439
Iteration 273/1000 | Loss: 0.00022423
Iteration 274/1000 | Loss: 0.00024193
Iteration 275/1000 | Loss: 0.00104367
Iteration 276/1000 | Loss: 0.00093856
Iteration 277/1000 | Loss: 0.00040272
Iteration 278/1000 | Loss: 0.00085056
Iteration 279/1000 | Loss: 0.00019871
Iteration 280/1000 | Loss: 0.00026212
Iteration 281/1000 | Loss: 0.00130629
Iteration 282/1000 | Loss: 0.00024217
Iteration 283/1000 | Loss: 0.00014910
Iteration 284/1000 | Loss: 0.00013944
Iteration 285/1000 | Loss: 0.00051960
Iteration 286/1000 | Loss: 0.00038991
Iteration 287/1000 | Loss: 0.00013886
Iteration 288/1000 | Loss: 0.00034770
Iteration 289/1000 | Loss: 0.00023024
Iteration 290/1000 | Loss: 0.00018985
Iteration 291/1000 | Loss: 0.00114720
Iteration 292/1000 | Loss: 0.00106633
Iteration 293/1000 | Loss: 0.00160921
Iteration 294/1000 | Loss: 0.00154230
Iteration 295/1000 | Loss: 0.00081324
Iteration 296/1000 | Loss: 0.00016850
Iteration 297/1000 | Loss: 0.00024147
Iteration 298/1000 | Loss: 0.00022247
Iteration 299/1000 | Loss: 0.00025804
Iteration 300/1000 | Loss: 0.00025716
Iteration 301/1000 | Loss: 0.00021785
Iteration 302/1000 | Loss: 0.00030106
Iteration 303/1000 | Loss: 0.00037726
Iteration 304/1000 | Loss: 0.00014416
Iteration 305/1000 | Loss: 0.00013613
Iteration 306/1000 | Loss: 0.00014554
Iteration 307/1000 | Loss: 0.00031691
Iteration 308/1000 | Loss: 0.00023617
Iteration 309/1000 | Loss: 0.00015899
Iteration 310/1000 | Loss: 0.00014993
Iteration 311/1000 | Loss: 0.00030580
Iteration 312/1000 | Loss: 0.00016302
Iteration 313/1000 | Loss: 0.00016965
Iteration 314/1000 | Loss: 0.00025406
Iteration 315/1000 | Loss: 0.00044483
Iteration 316/1000 | Loss: 0.00046140
Iteration 317/1000 | Loss: 0.00066012
Iteration 318/1000 | Loss: 0.00016904
Iteration 319/1000 | Loss: 0.00029888
Iteration 320/1000 | Loss: 0.00028077
Iteration 321/1000 | Loss: 0.00027832
Iteration 322/1000 | Loss: 0.00027320
Iteration 323/1000 | Loss: 0.00024853
Iteration 324/1000 | Loss: 0.00016024
Iteration 325/1000 | Loss: 0.00014971
Iteration 326/1000 | Loss: 0.00021657
Iteration 327/1000 | Loss: 0.00024310
Iteration 328/1000 | Loss: 0.00024438
Iteration 329/1000 | Loss: 0.00023653
Iteration 330/1000 | Loss: 0.00024138
Iteration 331/1000 | Loss: 0.00015994
Iteration 332/1000 | Loss: 0.00035596
Iteration 333/1000 | Loss: 0.00109236
Iteration 334/1000 | Loss: 0.00040993
Iteration 335/1000 | Loss: 0.00013718
Iteration 336/1000 | Loss: 0.00013722
Iteration 337/1000 | Loss: 0.00012299
Iteration 338/1000 | Loss: 0.00091277
Iteration 339/1000 | Loss: 0.00037204
Iteration 340/1000 | Loss: 0.00016161
Iteration 341/1000 | Loss: 0.00092586
Iteration 342/1000 | Loss: 0.00036824
Iteration 343/1000 | Loss: 0.00035487
Iteration 344/1000 | Loss: 0.00017918
Iteration 345/1000 | Loss: 0.00013417
Iteration 346/1000 | Loss: 0.00013405
Iteration 347/1000 | Loss: 0.00095311
Iteration 348/1000 | Loss: 0.00076705
Iteration 349/1000 | Loss: 0.00016414
Iteration 350/1000 | Loss: 0.00013903
Iteration 351/1000 | Loss: 0.00013243
Iteration 352/1000 | Loss: 0.00012355
Iteration 353/1000 | Loss: 0.00011923
Iteration 354/1000 | Loss: 0.00013282
Iteration 355/1000 | Loss: 0.00098758
Iteration 356/1000 | Loss: 0.00499010
Iteration 357/1000 | Loss: 0.00022045
Iteration 358/1000 | Loss: 0.00015379
Iteration 359/1000 | Loss: 0.00014027
Iteration 360/1000 | Loss: 0.00012589
Iteration 361/1000 | Loss: 0.00090060
Iteration 362/1000 | Loss: 0.00190723
Iteration 363/1000 | Loss: 0.00101563
Iteration 364/1000 | Loss: 0.00020558
Iteration 365/1000 | Loss: 0.00108838
Iteration 366/1000 | Loss: 0.00102843
Iteration 367/1000 | Loss: 0.00079302
Iteration 368/1000 | Loss: 0.00110896
Iteration 369/1000 | Loss: 0.00117567
Iteration 370/1000 | Loss: 0.00051962
Iteration 371/1000 | Loss: 0.00014273
Iteration 372/1000 | Loss: 0.00012497
Iteration 373/1000 | Loss: 0.00030895
Iteration 374/1000 | Loss: 0.00029438
Iteration 375/1000 | Loss: 0.00021689
Iteration 376/1000 | Loss: 0.00010967
Iteration 377/1000 | Loss: 0.00010585
Iteration 378/1000 | Loss: 0.00011012
Iteration 379/1000 | Loss: 0.00059773
Iteration 380/1000 | Loss: 0.00010797
Iteration 381/1000 | Loss: 0.00010189
Iteration 382/1000 | Loss: 0.00009978
Iteration 383/1000 | Loss: 0.00071460
Iteration 384/1000 | Loss: 0.00042368
Iteration 385/1000 | Loss: 0.00054962
Iteration 386/1000 | Loss: 0.00009695
Iteration 387/1000 | Loss: 0.00188520
Iteration 388/1000 | Loss: 0.00147322
Iteration 389/1000 | Loss: 0.00051458
Iteration 390/1000 | Loss: 0.00012101
Iteration 391/1000 | Loss: 0.00010541
Iteration 392/1000 | Loss: 0.00009654
Iteration 393/1000 | Loss: 0.00009073
Iteration 394/1000 | Loss: 0.00008756
Iteration 395/1000 | Loss: 0.00008449
Iteration 396/1000 | Loss: 0.00008239
Iteration 397/1000 | Loss: 0.00008108
Iteration 398/1000 | Loss: 0.00067634
Iteration 399/1000 | Loss: 0.00009611
Iteration 400/1000 | Loss: 0.00008205
Iteration 401/1000 | Loss: 0.00007945
Iteration 402/1000 | Loss: 0.00007745
Iteration 403/1000 | Loss: 0.00007629
Iteration 404/1000 | Loss: 0.00007590
Iteration 405/1000 | Loss: 0.00007539
Iteration 406/1000 | Loss: 0.00007489
Iteration 407/1000 | Loss: 0.00007457
Iteration 408/1000 | Loss: 0.00070981
Iteration 409/1000 | Loss: 0.00008478
Iteration 410/1000 | Loss: 0.00007484
Iteration 411/1000 | Loss: 0.00007281
Iteration 412/1000 | Loss: 0.00007156
Iteration 413/1000 | Loss: 0.00007067
Iteration 414/1000 | Loss: 0.00007018
Iteration 415/1000 | Loss: 0.00006975
Iteration 416/1000 | Loss: 0.00006951
Iteration 417/1000 | Loss: 0.00006933
Iteration 418/1000 | Loss: 0.00006927
Iteration 419/1000 | Loss: 0.00075049
Iteration 420/1000 | Loss: 0.00007220
Iteration 421/1000 | Loss: 0.00006925
Iteration 422/1000 | Loss: 0.00006790
Iteration 423/1000 | Loss: 0.00006687
Iteration 424/1000 | Loss: 0.00006632
Iteration 425/1000 | Loss: 0.00006612
Iteration 426/1000 | Loss: 0.00006598
Iteration 427/1000 | Loss: 0.00006595
Iteration 428/1000 | Loss: 0.00006594
Iteration 429/1000 | Loss: 0.00006594
Iteration 430/1000 | Loss: 0.00006594
Iteration 431/1000 | Loss: 0.00006593
Iteration 432/1000 | Loss: 0.00006593
Iteration 433/1000 | Loss: 0.00006584
Iteration 434/1000 | Loss: 0.00006584
Iteration 435/1000 | Loss: 0.00006584
Iteration 436/1000 | Loss: 0.00006584
Iteration 437/1000 | Loss: 0.00006584
Iteration 438/1000 | Loss: 0.00006584
Iteration 439/1000 | Loss: 0.00006584
Iteration 440/1000 | Loss: 0.00006584
Iteration 441/1000 | Loss: 0.00006584
Iteration 442/1000 | Loss: 0.00006584
Iteration 443/1000 | Loss: 0.00006584
Iteration 444/1000 | Loss: 0.00006583
Iteration 445/1000 | Loss: 0.00006583
Iteration 446/1000 | Loss: 0.00006583
Iteration 447/1000 | Loss: 0.00006583
Iteration 448/1000 | Loss: 0.00006583
Iteration 449/1000 | Loss: 0.00006583
Iteration 450/1000 | Loss: 0.00006583
Iteration 451/1000 | Loss: 0.00006583
Iteration 452/1000 | Loss: 0.00006583
Iteration 453/1000 | Loss: 0.00006583
Iteration 454/1000 | Loss: 0.00006583
Iteration 455/1000 | Loss: 0.00006582
Iteration 456/1000 | Loss: 0.00006582
Iteration 457/1000 | Loss: 0.00006582
Iteration 458/1000 | Loss: 0.00006582
Iteration 459/1000 | Loss: 0.00006581
Iteration 460/1000 | Loss: 0.00006581
Iteration 461/1000 | Loss: 0.00006581
Iteration 462/1000 | Loss: 0.00006581
Iteration 463/1000 | Loss: 0.00006580
Iteration 464/1000 | Loss: 0.00006580
Iteration 465/1000 | Loss: 0.00006580
Iteration 466/1000 | Loss: 0.00006579
Iteration 467/1000 | Loss: 0.00006579
Iteration 468/1000 | Loss: 0.00006579
Iteration 469/1000 | Loss: 0.00006579
Iteration 470/1000 | Loss: 0.00006579
Iteration 471/1000 | Loss: 0.00006579
Iteration 472/1000 | Loss: 0.00006579
Iteration 473/1000 | Loss: 0.00006579
Iteration 474/1000 | Loss: 0.00006579
Iteration 475/1000 | Loss: 0.00006579
Iteration 476/1000 | Loss: 0.00006579
Iteration 477/1000 | Loss: 0.00006579
Iteration 478/1000 | Loss: 0.00006579
Iteration 479/1000 | Loss: 0.00006579
Iteration 480/1000 | Loss: 0.00006578
Iteration 481/1000 | Loss: 0.00006578
Iteration 482/1000 | Loss: 0.00006578
Iteration 483/1000 | Loss: 0.00006578
Iteration 484/1000 | Loss: 0.00006578
Iteration 485/1000 | Loss: 0.00006578
Iteration 486/1000 | Loss: 0.00006577
Iteration 487/1000 | Loss: 0.00006577
Iteration 488/1000 | Loss: 0.00006577
Iteration 489/1000 | Loss: 0.00006577
Iteration 490/1000 | Loss: 0.00006577
Iteration 491/1000 | Loss: 0.00006577
Iteration 492/1000 | Loss: 0.00006577
Iteration 493/1000 | Loss: 0.00006577
Iteration 494/1000 | Loss: 0.00006577
Iteration 495/1000 | Loss: 0.00006577
Iteration 496/1000 | Loss: 0.00006577
Iteration 497/1000 | Loss: 0.00006577
Iteration 498/1000 | Loss: 0.00006576
Iteration 499/1000 | Loss: 0.00006576
Iteration 500/1000 | Loss: 0.00006576
Iteration 501/1000 | Loss: 0.00006576
Iteration 502/1000 | Loss: 0.00006576
Iteration 503/1000 | Loss: 0.00006576
Iteration 504/1000 | Loss: 0.00006576
Iteration 505/1000 | Loss: 0.00006576
Iteration 506/1000 | Loss: 0.00006576
Iteration 507/1000 | Loss: 0.00006576
Iteration 508/1000 | Loss: 0.00006575
Iteration 509/1000 | Loss: 0.00006575
Iteration 510/1000 | Loss: 0.00006575
Iteration 511/1000 | Loss: 0.00006575
Iteration 512/1000 | Loss: 0.00006575
Iteration 513/1000 | Loss: 0.00006575
Iteration 514/1000 | Loss: 0.00006575
Iteration 515/1000 | Loss: 0.00006575
Iteration 516/1000 | Loss: 0.00006575
Iteration 517/1000 | Loss: 0.00006575
Iteration 518/1000 | Loss: 0.00006575
Iteration 519/1000 | Loss: 0.00006574
Iteration 520/1000 | Loss: 0.00006574
Iteration 521/1000 | Loss: 0.00006574
Iteration 522/1000 | Loss: 0.00006574
Iteration 523/1000 | Loss: 0.00006574
Iteration 524/1000 | Loss: 0.00006574
Iteration 525/1000 | Loss: 0.00006573
Iteration 526/1000 | Loss: 0.00006573
Iteration 527/1000 | Loss: 0.00006573
Iteration 528/1000 | Loss: 0.00006573
Iteration 529/1000 | Loss: 0.00006573
Iteration 530/1000 | Loss: 0.00006573
Iteration 531/1000 | Loss: 0.00006573
Iteration 532/1000 | Loss: 0.00006573
Iteration 533/1000 | Loss: 0.00006573
Iteration 534/1000 | Loss: 0.00006573
Iteration 535/1000 | Loss: 0.00006573
Iteration 536/1000 | Loss: 0.00006573
Iteration 537/1000 | Loss: 0.00006573
Iteration 538/1000 | Loss: 0.00006573
Iteration 539/1000 | Loss: 0.00006573
Iteration 540/1000 | Loss: 0.00006573
Iteration 541/1000 | Loss: 0.00006572
Iteration 542/1000 | Loss: 0.00006572
Iteration 543/1000 | Loss: 0.00006572
Iteration 544/1000 | Loss: 0.00006572
Iteration 545/1000 | Loss: 0.00006571
Iteration 546/1000 | Loss: 0.00006571
Iteration 547/1000 | Loss: 0.00006571
Iteration 548/1000 | Loss: 0.00006571
Iteration 549/1000 | Loss: 0.00006570
Iteration 550/1000 | Loss: 0.00006570
Iteration 551/1000 | Loss: 0.00006570
Iteration 552/1000 | Loss: 0.00006570
Iteration 553/1000 | Loss: 0.00006570
Iteration 554/1000 | Loss: 0.00006570
Iteration 555/1000 | Loss: 0.00006570
Iteration 556/1000 | Loss: 0.00006570
Iteration 557/1000 | Loss: 0.00006570
Iteration 558/1000 | Loss: 0.00006569
Iteration 559/1000 | Loss: 0.00006569
Iteration 560/1000 | Loss: 0.00006569
Iteration 561/1000 | Loss: 0.00006569
Iteration 562/1000 | Loss: 0.00006569
Iteration 563/1000 | Loss: 0.00006569
Iteration 564/1000 | Loss: 0.00006569
Iteration 565/1000 | Loss: 0.00006569
Iteration 566/1000 | Loss: 0.00006569
Iteration 567/1000 | Loss: 0.00006569
Iteration 568/1000 | Loss: 0.00006568
Iteration 569/1000 | Loss: 0.00006568
Iteration 570/1000 | Loss: 0.00006568
Iteration 571/1000 | Loss: 0.00006568
Iteration 572/1000 | Loss: 0.00006568
Iteration 573/1000 | Loss: 0.00006568
Iteration 574/1000 | Loss: 0.00006568
Iteration 575/1000 | Loss: 0.00006568
Iteration 576/1000 | Loss: 0.00006568
Iteration 577/1000 | Loss: 0.00006568
Iteration 578/1000 | Loss: 0.00006568
Iteration 579/1000 | Loss: 0.00006568
Iteration 580/1000 | Loss: 0.00006568
Iteration 581/1000 | Loss: 0.00006568
Iteration 582/1000 | Loss: 0.00006568
Iteration 583/1000 | Loss: 0.00006568
Iteration 584/1000 | Loss: 0.00006568
Iteration 585/1000 | Loss: 0.00006567
Iteration 586/1000 | Loss: 0.00006567
Iteration 587/1000 | Loss: 0.00006567
Iteration 588/1000 | Loss: 0.00006567
Iteration 589/1000 | Loss: 0.00006567
Iteration 590/1000 | Loss: 0.00006567
Iteration 591/1000 | Loss: 0.00006567
Iteration 592/1000 | Loss: 0.00006567
Iteration 593/1000 | Loss: 0.00006567
Iteration 594/1000 | Loss: 0.00006567
Iteration 595/1000 | Loss: 0.00006567
Iteration 596/1000 | Loss: 0.00006567
Iteration 597/1000 | Loss: 0.00006567
Iteration 598/1000 | Loss: 0.00006567
Iteration 599/1000 | Loss: 0.00006567
Iteration 600/1000 | Loss: 0.00006567
Iteration 601/1000 | Loss: 0.00006567
Iteration 602/1000 | Loss: 0.00006567
Iteration 603/1000 | Loss: 0.00006567
Iteration 604/1000 | Loss: 0.00006567
Iteration 605/1000 | Loss: 0.00006567
Iteration 606/1000 | Loss: 0.00006567
Iteration 607/1000 | Loss: 0.00006567
Iteration 608/1000 | Loss: 0.00006567
Iteration 609/1000 | Loss: 0.00006567
Iteration 610/1000 | Loss: 0.00006567
Iteration 611/1000 | Loss: 0.00006567
Iteration 612/1000 | Loss: 0.00006567
Iteration 613/1000 | Loss: 0.00006567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 613. Stopping optimization.
Last 5 losses: [6.566893716808408e-05, 6.566893716808408e-05, 6.566893716808408e-05, 6.566893716808408e-05, 6.566893716808408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.566893716808408e-05

Optimization complete. Final v2v error: 5.899139881134033 mm

Highest mean error: 13.63913631439209 mm for frame 81

Lowest mean error: 4.693010330200195 mm for frame 42

Saving results

Total time: 635.8747825622559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00637747
Iteration 2/25 | Loss: 0.00202653
Iteration 3/25 | Loss: 0.00143218
Iteration 4/25 | Loss: 0.00140036
Iteration 5/25 | Loss: 0.00139597
Iteration 6/25 | Loss: 0.00139362
Iteration 7/25 | Loss: 0.00139331
Iteration 8/25 | Loss: 0.00139331
Iteration 9/25 | Loss: 0.00139331
Iteration 10/25 | Loss: 0.00139331
Iteration 11/25 | Loss: 0.00139331
Iteration 12/25 | Loss: 0.00139331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013933085137978196, 0.0013933085137978196, 0.0013933085137978196, 0.0013933085137978196, 0.0013933085137978196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013933085137978196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99941903
Iteration 2/25 | Loss: 0.00129307
Iteration 3/25 | Loss: 0.00129305
Iteration 4/25 | Loss: 0.00129305
Iteration 5/25 | Loss: 0.00129305
Iteration 6/25 | Loss: 0.00129305
Iteration 7/25 | Loss: 0.00129305
Iteration 8/25 | Loss: 0.00129305
Iteration 9/25 | Loss: 0.00129305
Iteration 10/25 | Loss: 0.00129305
Iteration 11/25 | Loss: 0.00129305
Iteration 12/25 | Loss: 0.00129305
Iteration 13/25 | Loss: 0.00129305
Iteration 14/25 | Loss: 0.00129305
Iteration 15/25 | Loss: 0.00129305
Iteration 16/25 | Loss: 0.00129305
Iteration 17/25 | Loss: 0.00129305
Iteration 18/25 | Loss: 0.00129305
Iteration 19/25 | Loss: 0.00129305
Iteration 20/25 | Loss: 0.00129305
Iteration 21/25 | Loss: 0.00129305
Iteration 22/25 | Loss: 0.00129305
Iteration 23/25 | Loss: 0.00129305
Iteration 24/25 | Loss: 0.00129305
Iteration 25/25 | Loss: 0.00129305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129305
Iteration 2/1000 | Loss: 0.00014379
Iteration 3/1000 | Loss: 0.00010268
Iteration 4/1000 | Loss: 0.00009202
Iteration 5/1000 | Loss: 0.00008665
Iteration 6/1000 | Loss: 0.00008382
Iteration 7/1000 | Loss: 0.00008159
Iteration 8/1000 | Loss: 0.00007957
Iteration 9/1000 | Loss: 0.00007816
Iteration 10/1000 | Loss: 0.00007642
Iteration 11/1000 | Loss: 0.00007511
Iteration 12/1000 | Loss: 0.00007390
Iteration 13/1000 | Loss: 0.00007311
Iteration 14/1000 | Loss: 0.00007233
Iteration 15/1000 | Loss: 0.00007153
Iteration 16/1000 | Loss: 0.00007077
Iteration 17/1000 | Loss: 0.00007010
Iteration 18/1000 | Loss: 0.00006978
Iteration 19/1000 | Loss: 0.00006958
Iteration 20/1000 | Loss: 0.00006940
Iteration 21/1000 | Loss: 0.00006925
Iteration 22/1000 | Loss: 0.00006917
Iteration 23/1000 | Loss: 0.00006917
Iteration 24/1000 | Loss: 0.00006916
Iteration 25/1000 | Loss: 0.00006916
Iteration 26/1000 | Loss: 0.00006915
Iteration 27/1000 | Loss: 0.00006915
Iteration 28/1000 | Loss: 0.00006915
Iteration 29/1000 | Loss: 0.00006914
Iteration 30/1000 | Loss: 0.00006913
Iteration 31/1000 | Loss: 0.00006913
Iteration 32/1000 | Loss: 0.00006913
Iteration 33/1000 | Loss: 0.00006912
Iteration 34/1000 | Loss: 0.00006912
Iteration 35/1000 | Loss: 0.00006910
Iteration 36/1000 | Loss: 0.00006910
Iteration 37/1000 | Loss: 0.00006910
Iteration 38/1000 | Loss: 0.00006910
Iteration 39/1000 | Loss: 0.00006910
Iteration 40/1000 | Loss: 0.00006909
Iteration 41/1000 | Loss: 0.00006909
Iteration 42/1000 | Loss: 0.00006909
Iteration 43/1000 | Loss: 0.00006909
Iteration 44/1000 | Loss: 0.00006908
Iteration 45/1000 | Loss: 0.00006908
Iteration 46/1000 | Loss: 0.00006907
Iteration 47/1000 | Loss: 0.00006907
Iteration 48/1000 | Loss: 0.00006907
Iteration 49/1000 | Loss: 0.00006906
Iteration 50/1000 | Loss: 0.00006906
Iteration 51/1000 | Loss: 0.00006906
Iteration 52/1000 | Loss: 0.00006905
Iteration 53/1000 | Loss: 0.00006905
Iteration 54/1000 | Loss: 0.00006905
Iteration 55/1000 | Loss: 0.00006904
Iteration 56/1000 | Loss: 0.00006904
Iteration 57/1000 | Loss: 0.00006904
Iteration 58/1000 | Loss: 0.00006904
Iteration 59/1000 | Loss: 0.00006904
Iteration 60/1000 | Loss: 0.00006904
Iteration 61/1000 | Loss: 0.00006904
Iteration 62/1000 | Loss: 0.00006903
Iteration 63/1000 | Loss: 0.00006903
Iteration 64/1000 | Loss: 0.00006903
Iteration 65/1000 | Loss: 0.00006903
Iteration 66/1000 | Loss: 0.00006903
Iteration 67/1000 | Loss: 0.00006903
Iteration 68/1000 | Loss: 0.00006903
Iteration 69/1000 | Loss: 0.00006903
Iteration 70/1000 | Loss: 0.00006903
Iteration 71/1000 | Loss: 0.00006903
Iteration 72/1000 | Loss: 0.00006903
Iteration 73/1000 | Loss: 0.00006902
Iteration 74/1000 | Loss: 0.00006902
Iteration 75/1000 | Loss: 0.00006902
Iteration 76/1000 | Loss: 0.00006902
Iteration 77/1000 | Loss: 0.00006902
Iteration 78/1000 | Loss: 0.00006902
Iteration 79/1000 | Loss: 0.00006902
Iteration 80/1000 | Loss: 0.00006902
Iteration 81/1000 | Loss: 0.00006901
Iteration 82/1000 | Loss: 0.00006901
Iteration 83/1000 | Loss: 0.00006901
Iteration 84/1000 | Loss: 0.00006901
Iteration 85/1000 | Loss: 0.00006901
Iteration 86/1000 | Loss: 0.00006901
Iteration 87/1000 | Loss: 0.00006901
Iteration 88/1000 | Loss: 0.00006901
Iteration 89/1000 | Loss: 0.00006901
Iteration 90/1000 | Loss: 0.00006900
Iteration 91/1000 | Loss: 0.00006900
Iteration 92/1000 | Loss: 0.00006900
Iteration 93/1000 | Loss: 0.00006900
Iteration 94/1000 | Loss: 0.00006900
Iteration 95/1000 | Loss: 0.00006900
Iteration 96/1000 | Loss: 0.00006900
Iteration 97/1000 | Loss: 0.00006899
Iteration 98/1000 | Loss: 0.00006899
Iteration 99/1000 | Loss: 0.00006899
Iteration 100/1000 | Loss: 0.00006899
Iteration 101/1000 | Loss: 0.00006899
Iteration 102/1000 | Loss: 0.00006898
Iteration 103/1000 | Loss: 0.00006898
Iteration 104/1000 | Loss: 0.00006898
Iteration 105/1000 | Loss: 0.00006898
Iteration 106/1000 | Loss: 0.00006898
Iteration 107/1000 | Loss: 0.00006898
Iteration 108/1000 | Loss: 0.00006898
Iteration 109/1000 | Loss: 0.00006897
Iteration 110/1000 | Loss: 0.00006897
Iteration 111/1000 | Loss: 0.00006897
Iteration 112/1000 | Loss: 0.00006896
Iteration 113/1000 | Loss: 0.00006896
Iteration 114/1000 | Loss: 0.00006896
Iteration 115/1000 | Loss: 0.00006896
Iteration 116/1000 | Loss: 0.00006896
Iteration 117/1000 | Loss: 0.00006896
Iteration 118/1000 | Loss: 0.00006895
Iteration 119/1000 | Loss: 0.00006895
Iteration 120/1000 | Loss: 0.00006895
Iteration 121/1000 | Loss: 0.00006895
Iteration 122/1000 | Loss: 0.00006894
Iteration 123/1000 | Loss: 0.00006894
Iteration 124/1000 | Loss: 0.00006894
Iteration 125/1000 | Loss: 0.00006894
Iteration 126/1000 | Loss: 0.00006894
Iteration 127/1000 | Loss: 0.00006893
Iteration 128/1000 | Loss: 0.00006893
Iteration 129/1000 | Loss: 0.00006893
Iteration 130/1000 | Loss: 0.00006893
Iteration 131/1000 | Loss: 0.00006893
Iteration 132/1000 | Loss: 0.00006893
Iteration 133/1000 | Loss: 0.00006893
Iteration 134/1000 | Loss: 0.00006893
Iteration 135/1000 | Loss: 0.00006893
Iteration 136/1000 | Loss: 0.00006892
Iteration 137/1000 | Loss: 0.00006892
Iteration 138/1000 | Loss: 0.00006892
Iteration 139/1000 | Loss: 0.00006892
Iteration 140/1000 | Loss: 0.00006892
Iteration 141/1000 | Loss: 0.00006892
Iteration 142/1000 | Loss: 0.00006892
Iteration 143/1000 | Loss: 0.00006891
Iteration 144/1000 | Loss: 0.00006891
Iteration 145/1000 | Loss: 0.00006891
Iteration 146/1000 | Loss: 0.00006891
Iteration 147/1000 | Loss: 0.00006891
Iteration 148/1000 | Loss: 0.00006891
Iteration 149/1000 | Loss: 0.00006891
Iteration 150/1000 | Loss: 0.00006891
Iteration 151/1000 | Loss: 0.00006891
Iteration 152/1000 | Loss: 0.00006890
Iteration 153/1000 | Loss: 0.00006890
Iteration 154/1000 | Loss: 0.00006890
Iteration 155/1000 | Loss: 0.00006890
Iteration 156/1000 | Loss: 0.00006890
Iteration 157/1000 | Loss: 0.00006890
Iteration 158/1000 | Loss: 0.00006890
Iteration 159/1000 | Loss: 0.00006890
Iteration 160/1000 | Loss: 0.00006890
Iteration 161/1000 | Loss: 0.00006889
Iteration 162/1000 | Loss: 0.00006889
Iteration 163/1000 | Loss: 0.00006889
Iteration 164/1000 | Loss: 0.00006889
Iteration 165/1000 | Loss: 0.00006889
Iteration 166/1000 | Loss: 0.00006889
Iteration 167/1000 | Loss: 0.00006889
Iteration 168/1000 | Loss: 0.00006889
Iteration 169/1000 | Loss: 0.00006889
Iteration 170/1000 | Loss: 0.00006889
Iteration 171/1000 | Loss: 0.00006888
Iteration 172/1000 | Loss: 0.00006888
Iteration 173/1000 | Loss: 0.00006888
Iteration 174/1000 | Loss: 0.00006888
Iteration 175/1000 | Loss: 0.00006888
Iteration 176/1000 | Loss: 0.00006888
Iteration 177/1000 | Loss: 0.00006888
Iteration 178/1000 | Loss: 0.00006888
Iteration 179/1000 | Loss: 0.00006888
Iteration 180/1000 | Loss: 0.00006888
Iteration 181/1000 | Loss: 0.00006887
Iteration 182/1000 | Loss: 0.00006887
Iteration 183/1000 | Loss: 0.00006887
Iteration 184/1000 | Loss: 0.00006887
Iteration 185/1000 | Loss: 0.00006887
Iteration 186/1000 | Loss: 0.00006887
Iteration 187/1000 | Loss: 0.00006887
Iteration 188/1000 | Loss: 0.00006887
Iteration 189/1000 | Loss: 0.00006887
Iteration 190/1000 | Loss: 0.00006887
Iteration 191/1000 | Loss: 0.00006887
Iteration 192/1000 | Loss: 0.00006887
Iteration 193/1000 | Loss: 0.00006887
Iteration 194/1000 | Loss: 0.00006887
Iteration 195/1000 | Loss: 0.00006887
Iteration 196/1000 | Loss: 0.00006887
Iteration 197/1000 | Loss: 0.00006887
Iteration 198/1000 | Loss: 0.00006887
Iteration 199/1000 | Loss: 0.00006887
Iteration 200/1000 | Loss: 0.00006887
Iteration 201/1000 | Loss: 0.00006887
Iteration 202/1000 | Loss: 0.00006887
Iteration 203/1000 | Loss: 0.00006887
Iteration 204/1000 | Loss: 0.00006887
Iteration 205/1000 | Loss: 0.00006887
Iteration 206/1000 | Loss: 0.00006887
Iteration 207/1000 | Loss: 0.00006887
Iteration 208/1000 | Loss: 0.00006887
Iteration 209/1000 | Loss: 0.00006887
Iteration 210/1000 | Loss: 0.00006887
Iteration 211/1000 | Loss: 0.00006887
Iteration 212/1000 | Loss: 0.00006887
Iteration 213/1000 | Loss: 0.00006887
Iteration 214/1000 | Loss: 0.00006887
Iteration 215/1000 | Loss: 0.00006887
Iteration 216/1000 | Loss: 0.00006887
Iteration 217/1000 | Loss: 0.00006887
Iteration 218/1000 | Loss: 0.00006887
Iteration 219/1000 | Loss: 0.00006887
Iteration 220/1000 | Loss: 0.00006887
Iteration 221/1000 | Loss: 0.00006887
Iteration 222/1000 | Loss: 0.00006887
Iteration 223/1000 | Loss: 0.00006887
Iteration 224/1000 | Loss: 0.00006887
Iteration 225/1000 | Loss: 0.00006887
Iteration 226/1000 | Loss: 0.00006887
Iteration 227/1000 | Loss: 0.00006887
Iteration 228/1000 | Loss: 0.00006887
Iteration 229/1000 | Loss: 0.00006887
Iteration 230/1000 | Loss: 0.00006887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [6.887140625622123e-05, 6.887140625622123e-05, 6.887140625622123e-05, 6.887140625622123e-05, 6.887140625622123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.887140625622123e-05

Optimization complete. Final v2v error: 5.912529468536377 mm

Highest mean error: 7.8987274169921875 mm for frame 144

Lowest mean error: 3.930483102798462 mm for frame 0

Saving results

Total time: 52.96712064743042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405896
Iteration 2/25 | Loss: 0.00151432
Iteration 3/25 | Loss: 0.00137015
Iteration 4/25 | Loss: 0.00135572
Iteration 5/25 | Loss: 0.00135315
Iteration 6/25 | Loss: 0.00135254
Iteration 7/25 | Loss: 0.00135254
Iteration 8/25 | Loss: 0.00135254
Iteration 9/25 | Loss: 0.00135254
Iteration 10/25 | Loss: 0.00135254
Iteration 11/25 | Loss: 0.00135254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013525366084650159, 0.0013525366084650159, 0.0013525366084650159, 0.0013525366084650159, 0.0013525366084650159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013525366084650159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32957613
Iteration 2/25 | Loss: 0.00098054
Iteration 3/25 | Loss: 0.00098054
Iteration 4/25 | Loss: 0.00098054
Iteration 5/25 | Loss: 0.00098054
Iteration 6/25 | Loss: 0.00098054
Iteration 7/25 | Loss: 0.00098054
Iteration 8/25 | Loss: 0.00098054
Iteration 9/25 | Loss: 0.00098054
Iteration 10/25 | Loss: 0.00098054
Iteration 11/25 | Loss: 0.00098054
Iteration 12/25 | Loss: 0.00098054
Iteration 13/25 | Loss: 0.00098054
Iteration 14/25 | Loss: 0.00098054
Iteration 15/25 | Loss: 0.00098054
Iteration 16/25 | Loss: 0.00098054
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009805378504097462, 0.0009805378504097462, 0.0009805378504097462, 0.0009805378504097462, 0.0009805378504097462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009805378504097462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098054
Iteration 2/1000 | Loss: 0.00004984
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002203
Iteration 5/1000 | Loss: 0.00001994
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001815
Iteration 9/1000 | Loss: 0.00001796
Iteration 10/1000 | Loss: 0.00001777
Iteration 11/1000 | Loss: 0.00001770
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001761
Iteration 15/1000 | Loss: 0.00001760
Iteration 16/1000 | Loss: 0.00001755
Iteration 17/1000 | Loss: 0.00001753
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001752
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001746
Iteration 23/1000 | Loss: 0.00001737
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001736
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001734
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001727
Iteration 36/1000 | Loss: 0.00001727
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001726
Iteration 42/1000 | Loss: 0.00001726
Iteration 43/1000 | Loss: 0.00001726
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001725
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001724
Iteration 48/1000 | Loss: 0.00001724
Iteration 49/1000 | Loss: 0.00001724
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001720
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001719
Iteration 69/1000 | Loss: 0.00001719
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001716
Iteration 84/1000 | Loss: 0.00001716
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001715
Iteration 87/1000 | Loss: 0.00001715
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001711
Iteration 109/1000 | Loss: 0.00001711
Iteration 110/1000 | Loss: 0.00001711
Iteration 111/1000 | Loss: 0.00001711
Iteration 112/1000 | Loss: 0.00001711
Iteration 113/1000 | Loss: 0.00001711
Iteration 114/1000 | Loss: 0.00001711
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001710
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001709
Iteration 124/1000 | Loss: 0.00001709
Iteration 125/1000 | Loss: 0.00001709
Iteration 126/1000 | Loss: 0.00001709
Iteration 127/1000 | Loss: 0.00001709
Iteration 128/1000 | Loss: 0.00001709
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001709
Iteration 133/1000 | Loss: 0.00001709
Iteration 134/1000 | Loss: 0.00001708
Iteration 135/1000 | Loss: 0.00001708
Iteration 136/1000 | Loss: 0.00001708
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001708
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001707
Iteration 142/1000 | Loss: 0.00001707
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001707
Iteration 146/1000 | Loss: 0.00001707
Iteration 147/1000 | Loss: 0.00001707
Iteration 148/1000 | Loss: 0.00001707
Iteration 149/1000 | Loss: 0.00001706
Iteration 150/1000 | Loss: 0.00001706
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001706
Iteration 155/1000 | Loss: 0.00001706
Iteration 156/1000 | Loss: 0.00001706
Iteration 157/1000 | Loss: 0.00001706
Iteration 158/1000 | Loss: 0.00001705
Iteration 159/1000 | Loss: 0.00001705
Iteration 160/1000 | Loss: 0.00001705
Iteration 161/1000 | Loss: 0.00001705
Iteration 162/1000 | Loss: 0.00001705
Iteration 163/1000 | Loss: 0.00001705
Iteration 164/1000 | Loss: 0.00001705
Iteration 165/1000 | Loss: 0.00001705
Iteration 166/1000 | Loss: 0.00001705
Iteration 167/1000 | Loss: 0.00001705
Iteration 168/1000 | Loss: 0.00001705
Iteration 169/1000 | Loss: 0.00001705
Iteration 170/1000 | Loss: 0.00001705
Iteration 171/1000 | Loss: 0.00001705
Iteration 172/1000 | Loss: 0.00001705
Iteration 173/1000 | Loss: 0.00001705
Iteration 174/1000 | Loss: 0.00001705
Iteration 175/1000 | Loss: 0.00001705
Iteration 176/1000 | Loss: 0.00001704
Iteration 177/1000 | Loss: 0.00001704
Iteration 178/1000 | Loss: 0.00001704
Iteration 179/1000 | Loss: 0.00001704
Iteration 180/1000 | Loss: 0.00001704
Iteration 181/1000 | Loss: 0.00001704
Iteration 182/1000 | Loss: 0.00001704
Iteration 183/1000 | Loss: 0.00001704
Iteration 184/1000 | Loss: 0.00001704
Iteration 185/1000 | Loss: 0.00001704
Iteration 186/1000 | Loss: 0.00001704
Iteration 187/1000 | Loss: 0.00001704
Iteration 188/1000 | Loss: 0.00001704
Iteration 189/1000 | Loss: 0.00001704
Iteration 190/1000 | Loss: 0.00001704
Iteration 191/1000 | Loss: 0.00001704
Iteration 192/1000 | Loss: 0.00001704
Iteration 193/1000 | Loss: 0.00001704
Iteration 194/1000 | Loss: 0.00001704
Iteration 195/1000 | Loss: 0.00001704
Iteration 196/1000 | Loss: 0.00001703
Iteration 197/1000 | Loss: 0.00001703
Iteration 198/1000 | Loss: 0.00001703
Iteration 199/1000 | Loss: 0.00001703
Iteration 200/1000 | Loss: 0.00001703
Iteration 201/1000 | Loss: 0.00001703
Iteration 202/1000 | Loss: 0.00001703
Iteration 203/1000 | Loss: 0.00001703
Iteration 204/1000 | Loss: 0.00001703
Iteration 205/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.703307316347491e-05, 1.703307316347491e-05, 1.703307316347491e-05, 1.703307316347491e-05, 1.703307316347491e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.703307316347491e-05

Optimization complete. Final v2v error: 3.4574742317199707 mm

Highest mean error: 3.8936245441436768 mm for frame 152

Lowest mean error: 3.2633416652679443 mm for frame 45

Saving results

Total time: 37.78786611557007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640507
Iteration 2/25 | Loss: 0.00165355
Iteration 3/25 | Loss: 0.00139212
Iteration 4/25 | Loss: 0.00137591
Iteration 5/25 | Loss: 0.00137347
Iteration 6/25 | Loss: 0.00137300
Iteration 7/25 | Loss: 0.00137300
Iteration 8/25 | Loss: 0.00137300
Iteration 9/25 | Loss: 0.00137300
Iteration 10/25 | Loss: 0.00137300
Iteration 11/25 | Loss: 0.00137300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013729989295825362, 0.0013729989295825362, 0.0013729989295825362, 0.0013729989295825362, 0.0013729989295825362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013729989295825362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19819224
Iteration 2/25 | Loss: 0.00064665
Iteration 3/25 | Loss: 0.00064665
Iteration 4/25 | Loss: 0.00064665
Iteration 5/25 | Loss: 0.00064664
Iteration 6/25 | Loss: 0.00064664
Iteration 7/25 | Loss: 0.00064664
Iteration 8/25 | Loss: 0.00064664
Iteration 9/25 | Loss: 0.00064664
Iteration 10/25 | Loss: 0.00064664
Iteration 11/25 | Loss: 0.00064664
Iteration 12/25 | Loss: 0.00064664
Iteration 13/25 | Loss: 0.00064664
Iteration 14/25 | Loss: 0.00064664
Iteration 15/25 | Loss: 0.00064664
Iteration 16/25 | Loss: 0.00064664
Iteration 17/25 | Loss: 0.00064664
Iteration 18/25 | Loss: 0.00064664
Iteration 19/25 | Loss: 0.00064664
Iteration 20/25 | Loss: 0.00064664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000646643340587616, 0.000646643340587616, 0.000646643340587616, 0.000646643340587616, 0.000646643340587616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000646643340587616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064664
Iteration 2/1000 | Loss: 0.00007812
Iteration 3/1000 | Loss: 0.00004132
Iteration 4/1000 | Loss: 0.00003310
Iteration 5/1000 | Loss: 0.00003036
Iteration 6/1000 | Loss: 0.00002905
Iteration 7/1000 | Loss: 0.00002825
Iteration 8/1000 | Loss: 0.00002762
Iteration 9/1000 | Loss: 0.00002734
Iteration 10/1000 | Loss: 0.00002704
Iteration 11/1000 | Loss: 0.00002676
Iteration 12/1000 | Loss: 0.00002663
Iteration 13/1000 | Loss: 0.00002655
Iteration 14/1000 | Loss: 0.00002636
Iteration 15/1000 | Loss: 0.00002634
Iteration 16/1000 | Loss: 0.00002633
Iteration 17/1000 | Loss: 0.00002632
Iteration 18/1000 | Loss: 0.00002615
Iteration 19/1000 | Loss: 0.00002609
Iteration 20/1000 | Loss: 0.00002604
Iteration 21/1000 | Loss: 0.00002594
Iteration 22/1000 | Loss: 0.00002593
Iteration 23/1000 | Loss: 0.00002592
Iteration 24/1000 | Loss: 0.00002591
Iteration 25/1000 | Loss: 0.00002591
Iteration 26/1000 | Loss: 0.00002591
Iteration 27/1000 | Loss: 0.00002590
Iteration 28/1000 | Loss: 0.00002589
Iteration 29/1000 | Loss: 0.00002586
Iteration 30/1000 | Loss: 0.00002586
Iteration 31/1000 | Loss: 0.00002586
Iteration 32/1000 | Loss: 0.00002586
Iteration 33/1000 | Loss: 0.00002585
Iteration 34/1000 | Loss: 0.00002585
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00002582
Iteration 37/1000 | Loss: 0.00002582
Iteration 38/1000 | Loss: 0.00002582
Iteration 39/1000 | Loss: 0.00002582
Iteration 40/1000 | Loss: 0.00002582
Iteration 41/1000 | Loss: 0.00002582
Iteration 42/1000 | Loss: 0.00002582
Iteration 43/1000 | Loss: 0.00002582
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002582
Iteration 46/1000 | Loss: 0.00002582
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002581
Iteration 50/1000 | Loss: 0.00002581
Iteration 51/1000 | Loss: 0.00002581
Iteration 52/1000 | Loss: 0.00002581
Iteration 53/1000 | Loss: 0.00002581
Iteration 54/1000 | Loss: 0.00002581
Iteration 55/1000 | Loss: 0.00002581
Iteration 56/1000 | Loss: 0.00002581
Iteration 57/1000 | Loss: 0.00002581
Iteration 58/1000 | Loss: 0.00002581
Iteration 59/1000 | Loss: 0.00002578
Iteration 60/1000 | Loss: 0.00002578
Iteration 61/1000 | Loss: 0.00002578
Iteration 62/1000 | Loss: 0.00002578
Iteration 63/1000 | Loss: 0.00002578
Iteration 64/1000 | Loss: 0.00002578
Iteration 65/1000 | Loss: 0.00002578
Iteration 66/1000 | Loss: 0.00002578
Iteration 67/1000 | Loss: 0.00002577
Iteration 68/1000 | Loss: 0.00002577
Iteration 69/1000 | Loss: 0.00002577
Iteration 70/1000 | Loss: 0.00002577
Iteration 71/1000 | Loss: 0.00002577
Iteration 72/1000 | Loss: 0.00002574
Iteration 73/1000 | Loss: 0.00002574
Iteration 74/1000 | Loss: 0.00002574
Iteration 75/1000 | Loss: 0.00002574
Iteration 76/1000 | Loss: 0.00002574
Iteration 77/1000 | Loss: 0.00002574
Iteration 78/1000 | Loss: 0.00002574
Iteration 79/1000 | Loss: 0.00002573
Iteration 80/1000 | Loss: 0.00002573
Iteration 81/1000 | Loss: 0.00002573
Iteration 82/1000 | Loss: 0.00002573
Iteration 83/1000 | Loss: 0.00002573
Iteration 84/1000 | Loss: 0.00002573
Iteration 85/1000 | Loss: 0.00002572
Iteration 86/1000 | Loss: 0.00002571
Iteration 87/1000 | Loss: 0.00002570
Iteration 88/1000 | Loss: 0.00002570
Iteration 89/1000 | Loss: 0.00002570
Iteration 90/1000 | Loss: 0.00002569
Iteration 91/1000 | Loss: 0.00002569
Iteration 92/1000 | Loss: 0.00002568
Iteration 93/1000 | Loss: 0.00002568
Iteration 94/1000 | Loss: 0.00002568
Iteration 95/1000 | Loss: 0.00002566
Iteration 96/1000 | Loss: 0.00002566
Iteration 97/1000 | Loss: 0.00002566
Iteration 98/1000 | Loss: 0.00002566
Iteration 99/1000 | Loss: 0.00002566
Iteration 100/1000 | Loss: 0.00002566
Iteration 101/1000 | Loss: 0.00002566
Iteration 102/1000 | Loss: 0.00002566
Iteration 103/1000 | Loss: 0.00002566
Iteration 104/1000 | Loss: 0.00002566
Iteration 105/1000 | Loss: 0.00002566
Iteration 106/1000 | Loss: 0.00002565
Iteration 107/1000 | Loss: 0.00002565
Iteration 108/1000 | Loss: 0.00002565
Iteration 109/1000 | Loss: 0.00002564
Iteration 110/1000 | Loss: 0.00002564
Iteration 111/1000 | Loss: 0.00002563
Iteration 112/1000 | Loss: 0.00002563
Iteration 113/1000 | Loss: 0.00002563
Iteration 114/1000 | Loss: 0.00002563
Iteration 115/1000 | Loss: 0.00002563
Iteration 116/1000 | Loss: 0.00002562
Iteration 117/1000 | Loss: 0.00002562
Iteration 118/1000 | Loss: 0.00002562
Iteration 119/1000 | Loss: 0.00002561
Iteration 120/1000 | Loss: 0.00002561
Iteration 121/1000 | Loss: 0.00002561
Iteration 122/1000 | Loss: 0.00002561
Iteration 123/1000 | Loss: 0.00002561
Iteration 124/1000 | Loss: 0.00002560
Iteration 125/1000 | Loss: 0.00002560
Iteration 126/1000 | Loss: 0.00002560
Iteration 127/1000 | Loss: 0.00002560
Iteration 128/1000 | Loss: 0.00002560
Iteration 129/1000 | Loss: 0.00002560
Iteration 130/1000 | Loss: 0.00002560
Iteration 131/1000 | Loss: 0.00002560
Iteration 132/1000 | Loss: 0.00002560
Iteration 133/1000 | Loss: 0.00002560
Iteration 134/1000 | Loss: 0.00002560
Iteration 135/1000 | Loss: 0.00002560
Iteration 136/1000 | Loss: 0.00002560
Iteration 137/1000 | Loss: 0.00002560
Iteration 138/1000 | Loss: 0.00002559
Iteration 139/1000 | Loss: 0.00002559
Iteration 140/1000 | Loss: 0.00002559
Iteration 141/1000 | Loss: 0.00002559
Iteration 142/1000 | Loss: 0.00002559
Iteration 143/1000 | Loss: 0.00002558
Iteration 144/1000 | Loss: 0.00002558
Iteration 145/1000 | Loss: 0.00002558
Iteration 146/1000 | Loss: 0.00002558
Iteration 147/1000 | Loss: 0.00002557
Iteration 148/1000 | Loss: 0.00002557
Iteration 149/1000 | Loss: 0.00002557
Iteration 150/1000 | Loss: 0.00002557
Iteration 151/1000 | Loss: 0.00002557
Iteration 152/1000 | Loss: 0.00002557
Iteration 153/1000 | Loss: 0.00002557
Iteration 154/1000 | Loss: 0.00002557
Iteration 155/1000 | Loss: 0.00002556
Iteration 156/1000 | Loss: 0.00002556
Iteration 157/1000 | Loss: 0.00002556
Iteration 158/1000 | Loss: 0.00002556
Iteration 159/1000 | Loss: 0.00002556
Iteration 160/1000 | Loss: 0.00002556
Iteration 161/1000 | Loss: 0.00002556
Iteration 162/1000 | Loss: 0.00002556
Iteration 163/1000 | Loss: 0.00002555
Iteration 164/1000 | Loss: 0.00002555
Iteration 165/1000 | Loss: 0.00002555
Iteration 166/1000 | Loss: 0.00002554
Iteration 167/1000 | Loss: 0.00002554
Iteration 168/1000 | Loss: 0.00002554
Iteration 169/1000 | Loss: 0.00002554
Iteration 170/1000 | Loss: 0.00002554
Iteration 171/1000 | Loss: 0.00002553
Iteration 172/1000 | Loss: 0.00002553
Iteration 173/1000 | Loss: 0.00002553
Iteration 174/1000 | Loss: 0.00002553
Iteration 175/1000 | Loss: 0.00002553
Iteration 176/1000 | Loss: 0.00002553
Iteration 177/1000 | Loss: 0.00002553
Iteration 178/1000 | Loss: 0.00002553
Iteration 179/1000 | Loss: 0.00002553
Iteration 180/1000 | Loss: 0.00002552
Iteration 181/1000 | Loss: 0.00002552
Iteration 182/1000 | Loss: 0.00002552
Iteration 183/1000 | Loss: 0.00002552
Iteration 184/1000 | Loss: 0.00002552
Iteration 185/1000 | Loss: 0.00002552
Iteration 186/1000 | Loss: 0.00002552
Iteration 187/1000 | Loss: 0.00002551
Iteration 188/1000 | Loss: 0.00002551
Iteration 189/1000 | Loss: 0.00002551
Iteration 190/1000 | Loss: 0.00002551
Iteration 191/1000 | Loss: 0.00002551
Iteration 192/1000 | Loss: 0.00002551
Iteration 193/1000 | Loss: 0.00002551
Iteration 194/1000 | Loss: 0.00002551
Iteration 195/1000 | Loss: 0.00002551
Iteration 196/1000 | Loss: 0.00002551
Iteration 197/1000 | Loss: 0.00002551
Iteration 198/1000 | Loss: 0.00002550
Iteration 199/1000 | Loss: 0.00002550
Iteration 200/1000 | Loss: 0.00002550
Iteration 201/1000 | Loss: 0.00002549
Iteration 202/1000 | Loss: 0.00002549
Iteration 203/1000 | Loss: 0.00002549
Iteration 204/1000 | Loss: 0.00002549
Iteration 205/1000 | Loss: 0.00002549
Iteration 206/1000 | Loss: 0.00002549
Iteration 207/1000 | Loss: 0.00002549
Iteration 208/1000 | Loss: 0.00002549
Iteration 209/1000 | Loss: 0.00002549
Iteration 210/1000 | Loss: 0.00002549
Iteration 211/1000 | Loss: 0.00002549
Iteration 212/1000 | Loss: 0.00002549
Iteration 213/1000 | Loss: 0.00002549
Iteration 214/1000 | Loss: 0.00002548
Iteration 215/1000 | Loss: 0.00002548
Iteration 216/1000 | Loss: 0.00002548
Iteration 217/1000 | Loss: 0.00002548
Iteration 218/1000 | Loss: 0.00002548
Iteration 219/1000 | Loss: 0.00002548
Iteration 220/1000 | Loss: 0.00002548
Iteration 221/1000 | Loss: 0.00002548
Iteration 222/1000 | Loss: 0.00002548
Iteration 223/1000 | Loss: 0.00002548
Iteration 224/1000 | Loss: 0.00002548
Iteration 225/1000 | Loss: 0.00002548
Iteration 226/1000 | Loss: 0.00002548
Iteration 227/1000 | Loss: 0.00002548
Iteration 228/1000 | Loss: 0.00002548
Iteration 229/1000 | Loss: 0.00002548
Iteration 230/1000 | Loss: 0.00002548
Iteration 231/1000 | Loss: 0.00002548
Iteration 232/1000 | Loss: 0.00002548
Iteration 233/1000 | Loss: 0.00002548
Iteration 234/1000 | Loss: 0.00002548
Iteration 235/1000 | Loss: 0.00002548
Iteration 236/1000 | Loss: 0.00002548
Iteration 237/1000 | Loss: 0.00002548
Iteration 238/1000 | Loss: 0.00002548
Iteration 239/1000 | Loss: 0.00002548
Iteration 240/1000 | Loss: 0.00002548
Iteration 241/1000 | Loss: 0.00002548
Iteration 242/1000 | Loss: 0.00002548
Iteration 243/1000 | Loss: 0.00002548
Iteration 244/1000 | Loss: 0.00002548
Iteration 245/1000 | Loss: 0.00002548
Iteration 246/1000 | Loss: 0.00002548
Iteration 247/1000 | Loss: 0.00002548
Iteration 248/1000 | Loss: 0.00002548
Iteration 249/1000 | Loss: 0.00002548
Iteration 250/1000 | Loss: 0.00002548
Iteration 251/1000 | Loss: 0.00002548
Iteration 252/1000 | Loss: 0.00002548
Iteration 253/1000 | Loss: 0.00002548
Iteration 254/1000 | Loss: 0.00002548
Iteration 255/1000 | Loss: 0.00002548
Iteration 256/1000 | Loss: 0.00002548
Iteration 257/1000 | Loss: 0.00002548
Iteration 258/1000 | Loss: 0.00002548
Iteration 259/1000 | Loss: 0.00002548
Iteration 260/1000 | Loss: 0.00002548
Iteration 261/1000 | Loss: 0.00002548
Iteration 262/1000 | Loss: 0.00002548
Iteration 263/1000 | Loss: 0.00002548
Iteration 264/1000 | Loss: 0.00002548
Iteration 265/1000 | Loss: 0.00002548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.5484550860710442e-05, 2.5484550860710442e-05, 2.5484550860710442e-05, 2.5484550860710442e-05, 2.5484550860710442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5484550860710442e-05

Optimization complete. Final v2v error: 3.9914300441741943 mm

Highest mean error: 6.601047992706299 mm for frame 58

Lowest mean error: 3.421147584915161 mm for frame 153

Saving results

Total time: 44.955090284347534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790050
Iteration 2/25 | Loss: 0.00163583
Iteration 3/25 | Loss: 0.00143987
Iteration 4/25 | Loss: 0.00141840
Iteration 5/25 | Loss: 0.00141145
Iteration 6/25 | Loss: 0.00141051
Iteration 7/25 | Loss: 0.00141051
Iteration 8/25 | Loss: 0.00141051
Iteration 9/25 | Loss: 0.00141051
Iteration 10/25 | Loss: 0.00141051
Iteration 11/25 | Loss: 0.00141051
Iteration 12/25 | Loss: 0.00141051
Iteration 13/25 | Loss: 0.00141051
Iteration 14/25 | Loss: 0.00141051
Iteration 15/25 | Loss: 0.00141051
Iteration 16/25 | Loss: 0.00141051
Iteration 17/25 | Loss: 0.00141051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014105070149526, 0.0014105070149526, 0.0014105070149526, 0.0014105070149526, 0.0014105070149526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014105070149526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.73127460
Iteration 2/25 | Loss: 0.00083828
Iteration 3/25 | Loss: 0.00083825
Iteration 4/25 | Loss: 0.00083825
Iteration 5/25 | Loss: 0.00083825
Iteration 6/25 | Loss: 0.00083825
Iteration 7/25 | Loss: 0.00083825
Iteration 8/25 | Loss: 0.00083825
Iteration 9/25 | Loss: 0.00083825
Iteration 10/25 | Loss: 0.00083825
Iteration 11/25 | Loss: 0.00083825
Iteration 12/25 | Loss: 0.00083825
Iteration 13/25 | Loss: 0.00083825
Iteration 14/25 | Loss: 0.00083825
Iteration 15/25 | Loss: 0.00083825
Iteration 16/25 | Loss: 0.00083825
Iteration 17/25 | Loss: 0.00083825
Iteration 18/25 | Loss: 0.00083825
Iteration 19/25 | Loss: 0.00083825
Iteration 20/25 | Loss: 0.00083825
Iteration 21/25 | Loss: 0.00083825
Iteration 22/25 | Loss: 0.00083825
Iteration 23/25 | Loss: 0.00083825
Iteration 24/25 | Loss: 0.00083825
Iteration 25/25 | Loss: 0.00083825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083825
Iteration 2/1000 | Loss: 0.00008060
Iteration 3/1000 | Loss: 0.00004586
Iteration 4/1000 | Loss: 0.00003746
Iteration 5/1000 | Loss: 0.00003513
Iteration 6/1000 | Loss: 0.00003427
Iteration 7/1000 | Loss: 0.00003363
Iteration 8/1000 | Loss: 0.00003314
Iteration 9/1000 | Loss: 0.00003263
Iteration 10/1000 | Loss: 0.00003228
Iteration 11/1000 | Loss: 0.00003195
Iteration 12/1000 | Loss: 0.00003172
Iteration 13/1000 | Loss: 0.00003152
Iteration 14/1000 | Loss: 0.00003129
Iteration 15/1000 | Loss: 0.00003113
Iteration 16/1000 | Loss: 0.00003112
Iteration 17/1000 | Loss: 0.00003108
Iteration 18/1000 | Loss: 0.00003108
Iteration 19/1000 | Loss: 0.00003108
Iteration 20/1000 | Loss: 0.00003107
Iteration 21/1000 | Loss: 0.00003105
Iteration 22/1000 | Loss: 0.00003103
Iteration 23/1000 | Loss: 0.00003103
Iteration 24/1000 | Loss: 0.00003103
Iteration 25/1000 | Loss: 0.00003102
Iteration 26/1000 | Loss: 0.00003102
Iteration 27/1000 | Loss: 0.00003102
Iteration 28/1000 | Loss: 0.00003101
Iteration 29/1000 | Loss: 0.00003101
Iteration 30/1000 | Loss: 0.00003101
Iteration 31/1000 | Loss: 0.00003100
Iteration 32/1000 | Loss: 0.00003099
Iteration 33/1000 | Loss: 0.00003099
Iteration 34/1000 | Loss: 0.00003099
Iteration 35/1000 | Loss: 0.00003098
Iteration 36/1000 | Loss: 0.00003098
Iteration 37/1000 | Loss: 0.00003098
Iteration 38/1000 | Loss: 0.00003097
Iteration 39/1000 | Loss: 0.00003097
Iteration 40/1000 | Loss: 0.00003096
Iteration 41/1000 | Loss: 0.00003096
Iteration 42/1000 | Loss: 0.00003096
Iteration 43/1000 | Loss: 0.00003096
Iteration 44/1000 | Loss: 0.00003096
Iteration 45/1000 | Loss: 0.00003096
Iteration 46/1000 | Loss: 0.00003095
Iteration 47/1000 | Loss: 0.00003095
Iteration 48/1000 | Loss: 0.00003095
Iteration 49/1000 | Loss: 0.00003095
Iteration 50/1000 | Loss: 0.00003095
Iteration 51/1000 | Loss: 0.00003095
Iteration 52/1000 | Loss: 0.00003095
Iteration 53/1000 | Loss: 0.00003095
Iteration 54/1000 | Loss: 0.00003095
Iteration 55/1000 | Loss: 0.00003095
Iteration 56/1000 | Loss: 0.00003095
Iteration 57/1000 | Loss: 0.00003094
Iteration 58/1000 | Loss: 0.00003094
Iteration 59/1000 | Loss: 0.00003093
Iteration 60/1000 | Loss: 0.00003093
Iteration 61/1000 | Loss: 0.00003092
Iteration 62/1000 | Loss: 0.00003092
Iteration 63/1000 | Loss: 0.00003092
Iteration 64/1000 | Loss: 0.00003091
Iteration 65/1000 | Loss: 0.00003090
Iteration 66/1000 | Loss: 0.00003090
Iteration 67/1000 | Loss: 0.00003090
Iteration 68/1000 | Loss: 0.00003090
Iteration 69/1000 | Loss: 0.00003090
Iteration 70/1000 | Loss: 0.00003090
Iteration 71/1000 | Loss: 0.00003090
Iteration 72/1000 | Loss: 0.00003090
Iteration 73/1000 | Loss: 0.00003090
Iteration 74/1000 | Loss: 0.00003090
Iteration 75/1000 | Loss: 0.00003090
Iteration 76/1000 | Loss: 0.00003090
Iteration 77/1000 | Loss: 0.00003090
Iteration 78/1000 | Loss: 0.00003089
Iteration 79/1000 | Loss: 0.00003089
Iteration 80/1000 | Loss: 0.00003089
Iteration 81/1000 | Loss: 0.00003089
Iteration 82/1000 | Loss: 0.00003089
Iteration 83/1000 | Loss: 0.00003089
Iteration 84/1000 | Loss: 0.00003089
Iteration 85/1000 | Loss: 0.00003089
Iteration 86/1000 | Loss: 0.00003088
Iteration 87/1000 | Loss: 0.00003088
Iteration 88/1000 | Loss: 0.00003088
Iteration 89/1000 | Loss: 0.00003087
Iteration 90/1000 | Loss: 0.00003087
Iteration 91/1000 | Loss: 0.00003087
Iteration 92/1000 | Loss: 0.00003087
Iteration 93/1000 | Loss: 0.00003087
Iteration 94/1000 | Loss: 0.00003087
Iteration 95/1000 | Loss: 0.00003087
Iteration 96/1000 | Loss: 0.00003087
Iteration 97/1000 | Loss: 0.00003087
Iteration 98/1000 | Loss: 0.00003087
Iteration 99/1000 | Loss: 0.00003087
Iteration 100/1000 | Loss: 0.00003087
Iteration 101/1000 | Loss: 0.00003087
Iteration 102/1000 | Loss: 0.00003087
Iteration 103/1000 | Loss: 0.00003087
Iteration 104/1000 | Loss: 0.00003087
Iteration 105/1000 | Loss: 0.00003087
Iteration 106/1000 | Loss: 0.00003087
Iteration 107/1000 | Loss: 0.00003087
Iteration 108/1000 | Loss: 0.00003087
Iteration 109/1000 | Loss: 0.00003087
Iteration 110/1000 | Loss: 0.00003087
Iteration 111/1000 | Loss: 0.00003087
Iteration 112/1000 | Loss: 0.00003087
Iteration 113/1000 | Loss: 0.00003087
Iteration 114/1000 | Loss: 0.00003087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.086608921876177e-05, 3.086608921876177e-05, 3.086608921876177e-05, 3.086608921876177e-05, 3.086608921876177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.086608921876177e-05

Optimization complete. Final v2v error: 4.49099063873291 mm

Highest mean error: 5.417665004730225 mm for frame 33

Lowest mean error: 3.691600799560547 mm for frame 238

Saving results

Total time: 41.717719316482544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603940
Iteration 2/25 | Loss: 0.00137219
Iteration 3/25 | Loss: 0.00128512
Iteration 4/25 | Loss: 0.00127485
Iteration 5/25 | Loss: 0.00127107
Iteration 6/25 | Loss: 0.00127008
Iteration 7/25 | Loss: 0.00127008
Iteration 8/25 | Loss: 0.00127008
Iteration 9/25 | Loss: 0.00127008
Iteration 10/25 | Loss: 0.00127008
Iteration 11/25 | Loss: 0.00127008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012700783554464579, 0.0012700783554464579, 0.0012700783554464579, 0.0012700783554464579, 0.0012700783554464579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012700783554464579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87056684
Iteration 2/25 | Loss: 0.00099655
Iteration 3/25 | Loss: 0.00099655
Iteration 4/25 | Loss: 0.00099655
Iteration 5/25 | Loss: 0.00099655
Iteration 6/25 | Loss: 0.00099655
Iteration 7/25 | Loss: 0.00099655
Iteration 8/25 | Loss: 0.00099655
Iteration 9/25 | Loss: 0.00099655
Iteration 10/25 | Loss: 0.00099655
Iteration 11/25 | Loss: 0.00099655
Iteration 12/25 | Loss: 0.00099655
Iteration 13/25 | Loss: 0.00099655
Iteration 14/25 | Loss: 0.00099655
Iteration 15/25 | Loss: 0.00099655
Iteration 16/25 | Loss: 0.00099655
Iteration 17/25 | Loss: 0.00099655
Iteration 18/25 | Loss: 0.00099655
Iteration 19/25 | Loss: 0.00099655
Iteration 20/25 | Loss: 0.00099655
Iteration 21/25 | Loss: 0.00099655
Iteration 22/25 | Loss: 0.00099655
Iteration 23/25 | Loss: 0.00099655
Iteration 24/25 | Loss: 0.00099655
Iteration 25/25 | Loss: 0.00099655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099655
Iteration 2/1000 | Loss: 0.00003729
Iteration 3/1000 | Loss: 0.00002410
Iteration 4/1000 | Loss: 0.00002087
Iteration 5/1000 | Loss: 0.00001986
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001866
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001826
Iteration 14/1000 | Loss: 0.00001812
Iteration 15/1000 | Loss: 0.00001812
Iteration 16/1000 | Loss: 0.00001812
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001806
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001799
Iteration 22/1000 | Loss: 0.00001799
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001794
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001791
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001782
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001781
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001775
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001774
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001774
Iteration 104/1000 | Loss: 0.00001774
Iteration 105/1000 | Loss: 0.00001774
Iteration 106/1000 | Loss: 0.00001774
Iteration 107/1000 | Loss: 0.00001774
Iteration 108/1000 | Loss: 0.00001774
Iteration 109/1000 | Loss: 0.00001774
Iteration 110/1000 | Loss: 0.00001774
Iteration 111/1000 | Loss: 0.00001774
Iteration 112/1000 | Loss: 0.00001774
Iteration 113/1000 | Loss: 0.00001774
Iteration 114/1000 | Loss: 0.00001774
Iteration 115/1000 | Loss: 0.00001774
Iteration 116/1000 | Loss: 0.00001774
Iteration 117/1000 | Loss: 0.00001774
Iteration 118/1000 | Loss: 0.00001774
Iteration 119/1000 | Loss: 0.00001774
Iteration 120/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.7740203475113958e-05, 1.7740203475113958e-05, 1.7740203475113958e-05, 1.7740203475113958e-05, 1.7740203475113958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7740203475113958e-05

Optimization complete. Final v2v error: 3.522543430328369 mm

Highest mean error: 3.795440435409546 mm for frame 78

Lowest mean error: 3.350625514984131 mm for frame 0

Saving results

Total time: 32.1953227519989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903927
Iteration 2/25 | Loss: 0.00184148
Iteration 3/25 | Loss: 0.00149472
Iteration 4/25 | Loss: 0.00144488
Iteration 5/25 | Loss: 0.00143510
Iteration 6/25 | Loss: 0.00143239
Iteration 7/25 | Loss: 0.00142947
Iteration 8/25 | Loss: 0.00142828
Iteration 9/25 | Loss: 0.00142871
Iteration 10/25 | Loss: 0.00142747
Iteration 11/25 | Loss: 0.00142691
Iteration 12/25 | Loss: 0.00142598
Iteration 13/25 | Loss: 0.00142536
Iteration 14/25 | Loss: 0.00142512
Iteration 15/25 | Loss: 0.00142508
Iteration 16/25 | Loss: 0.00142507
Iteration 17/25 | Loss: 0.00142504
Iteration 18/25 | Loss: 0.00142504
Iteration 19/25 | Loss: 0.00142504
Iteration 20/25 | Loss: 0.00142504
Iteration 21/25 | Loss: 0.00142504
Iteration 22/25 | Loss: 0.00142504
Iteration 23/25 | Loss: 0.00142504
Iteration 24/25 | Loss: 0.00142504
Iteration 25/25 | Loss: 0.00142503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27434278
Iteration 2/25 | Loss: 0.00088366
Iteration 3/25 | Loss: 0.00088366
Iteration 4/25 | Loss: 0.00088365
Iteration 5/25 | Loss: 0.00088365
Iteration 6/25 | Loss: 0.00088365
Iteration 7/25 | Loss: 0.00088365
Iteration 8/25 | Loss: 0.00088365
Iteration 9/25 | Loss: 0.00088365
Iteration 10/25 | Loss: 0.00088365
Iteration 11/25 | Loss: 0.00088365
Iteration 12/25 | Loss: 0.00088365
Iteration 13/25 | Loss: 0.00088365
Iteration 14/25 | Loss: 0.00088365
Iteration 15/25 | Loss: 0.00088365
Iteration 16/25 | Loss: 0.00088365
Iteration 17/25 | Loss: 0.00088365
Iteration 18/25 | Loss: 0.00088365
Iteration 19/25 | Loss: 0.00088365
Iteration 20/25 | Loss: 0.00088365
Iteration 21/25 | Loss: 0.00088365
Iteration 22/25 | Loss: 0.00088365
Iteration 23/25 | Loss: 0.00088365
Iteration 24/25 | Loss: 0.00088365
Iteration 25/25 | Loss: 0.00088365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088365
Iteration 2/1000 | Loss: 0.00007746
Iteration 3/1000 | Loss: 0.00004619
Iteration 4/1000 | Loss: 0.00003700
Iteration 5/1000 | Loss: 0.00003395
Iteration 6/1000 | Loss: 0.00003237
Iteration 7/1000 | Loss: 0.00003112
Iteration 8/1000 | Loss: 0.00003037
Iteration 9/1000 | Loss: 0.00002964
Iteration 10/1000 | Loss: 0.00002923
Iteration 11/1000 | Loss: 0.00002877
Iteration 12/1000 | Loss: 0.00002830
Iteration 13/1000 | Loss: 0.00002789
Iteration 14/1000 | Loss: 0.00002765
Iteration 15/1000 | Loss: 0.00002740
Iteration 16/1000 | Loss: 0.00002722
Iteration 17/1000 | Loss: 0.00002718
Iteration 18/1000 | Loss: 0.00002716
Iteration 19/1000 | Loss: 0.00002715
Iteration 20/1000 | Loss: 0.00002715
Iteration 21/1000 | Loss: 0.00002714
Iteration 22/1000 | Loss: 0.00002714
Iteration 23/1000 | Loss: 0.00002714
Iteration 24/1000 | Loss: 0.00002713
Iteration 25/1000 | Loss: 0.00002713
Iteration 26/1000 | Loss: 0.00002713
Iteration 27/1000 | Loss: 0.00002713
Iteration 28/1000 | Loss: 0.00002713
Iteration 29/1000 | Loss: 0.00002712
Iteration 30/1000 | Loss: 0.00002712
Iteration 31/1000 | Loss: 0.00002711
Iteration 32/1000 | Loss: 0.00002711
Iteration 33/1000 | Loss: 0.00002711
Iteration 34/1000 | Loss: 0.00002711
Iteration 35/1000 | Loss: 0.00002711
Iteration 36/1000 | Loss: 0.00002710
Iteration 37/1000 | Loss: 0.00002710
Iteration 38/1000 | Loss: 0.00002710
Iteration 39/1000 | Loss: 0.00002709
Iteration 40/1000 | Loss: 0.00002709
Iteration 41/1000 | Loss: 0.00002709
Iteration 42/1000 | Loss: 0.00002708
Iteration 43/1000 | Loss: 0.00002708
Iteration 44/1000 | Loss: 0.00002707
Iteration 45/1000 | Loss: 0.00002707
Iteration 46/1000 | Loss: 0.00002704
Iteration 47/1000 | Loss: 0.00002704
Iteration 48/1000 | Loss: 0.00002704
Iteration 49/1000 | Loss: 0.00002703
Iteration 50/1000 | Loss: 0.00002703
Iteration 51/1000 | Loss: 0.00002702
Iteration 52/1000 | Loss: 0.00002702
Iteration 53/1000 | Loss: 0.00002702
Iteration 54/1000 | Loss: 0.00002702
Iteration 55/1000 | Loss: 0.00002702
Iteration 56/1000 | Loss: 0.00002702
Iteration 57/1000 | Loss: 0.00002701
Iteration 58/1000 | Loss: 0.00002701
Iteration 59/1000 | Loss: 0.00002701
Iteration 60/1000 | Loss: 0.00002700
Iteration 61/1000 | Loss: 0.00002700
Iteration 62/1000 | Loss: 0.00002700
Iteration 63/1000 | Loss: 0.00002700
Iteration 64/1000 | Loss: 0.00002699
Iteration 65/1000 | Loss: 0.00002699
Iteration 66/1000 | Loss: 0.00002699
Iteration 67/1000 | Loss: 0.00002699
Iteration 68/1000 | Loss: 0.00002698
Iteration 69/1000 | Loss: 0.00002698
Iteration 70/1000 | Loss: 0.00002698
Iteration 71/1000 | Loss: 0.00002698
Iteration 72/1000 | Loss: 0.00002698
Iteration 73/1000 | Loss: 0.00002698
Iteration 74/1000 | Loss: 0.00002697
Iteration 75/1000 | Loss: 0.00002697
Iteration 76/1000 | Loss: 0.00002697
Iteration 77/1000 | Loss: 0.00002697
Iteration 78/1000 | Loss: 0.00002697
Iteration 79/1000 | Loss: 0.00002697
Iteration 80/1000 | Loss: 0.00002697
Iteration 81/1000 | Loss: 0.00002697
Iteration 82/1000 | Loss: 0.00002696
Iteration 83/1000 | Loss: 0.00002696
Iteration 84/1000 | Loss: 0.00002696
Iteration 85/1000 | Loss: 0.00002696
Iteration 86/1000 | Loss: 0.00002696
Iteration 87/1000 | Loss: 0.00002696
Iteration 88/1000 | Loss: 0.00002696
Iteration 89/1000 | Loss: 0.00002696
Iteration 90/1000 | Loss: 0.00002696
Iteration 91/1000 | Loss: 0.00002695
Iteration 92/1000 | Loss: 0.00002695
Iteration 93/1000 | Loss: 0.00002695
Iteration 94/1000 | Loss: 0.00002695
Iteration 95/1000 | Loss: 0.00002695
Iteration 96/1000 | Loss: 0.00002694
Iteration 97/1000 | Loss: 0.00002694
Iteration 98/1000 | Loss: 0.00002694
Iteration 99/1000 | Loss: 0.00002694
Iteration 100/1000 | Loss: 0.00002694
Iteration 101/1000 | Loss: 0.00002694
Iteration 102/1000 | Loss: 0.00002694
Iteration 103/1000 | Loss: 0.00002694
Iteration 104/1000 | Loss: 0.00002694
Iteration 105/1000 | Loss: 0.00002694
Iteration 106/1000 | Loss: 0.00002694
Iteration 107/1000 | Loss: 0.00002694
Iteration 108/1000 | Loss: 0.00002694
Iteration 109/1000 | Loss: 0.00002694
Iteration 110/1000 | Loss: 0.00002694
Iteration 111/1000 | Loss: 0.00002694
Iteration 112/1000 | Loss: 0.00002693
Iteration 113/1000 | Loss: 0.00002693
Iteration 114/1000 | Loss: 0.00002693
Iteration 115/1000 | Loss: 0.00002693
Iteration 116/1000 | Loss: 0.00002693
Iteration 117/1000 | Loss: 0.00002693
Iteration 118/1000 | Loss: 0.00002693
Iteration 119/1000 | Loss: 0.00002693
Iteration 120/1000 | Loss: 0.00002693
Iteration 121/1000 | Loss: 0.00002693
Iteration 122/1000 | Loss: 0.00002693
Iteration 123/1000 | Loss: 0.00002693
Iteration 124/1000 | Loss: 0.00002692
Iteration 125/1000 | Loss: 0.00002692
Iteration 126/1000 | Loss: 0.00002692
Iteration 127/1000 | Loss: 0.00002692
Iteration 128/1000 | Loss: 0.00002692
Iteration 129/1000 | Loss: 0.00002692
Iteration 130/1000 | Loss: 0.00002692
Iteration 131/1000 | Loss: 0.00002692
Iteration 132/1000 | Loss: 0.00002692
Iteration 133/1000 | Loss: 0.00002692
Iteration 134/1000 | Loss: 0.00002692
Iteration 135/1000 | Loss: 0.00002692
Iteration 136/1000 | Loss: 0.00002692
Iteration 137/1000 | Loss: 0.00002691
Iteration 138/1000 | Loss: 0.00002691
Iteration 139/1000 | Loss: 0.00002691
Iteration 140/1000 | Loss: 0.00002691
Iteration 141/1000 | Loss: 0.00002691
Iteration 142/1000 | Loss: 0.00002691
Iteration 143/1000 | Loss: 0.00002691
Iteration 144/1000 | Loss: 0.00002691
Iteration 145/1000 | Loss: 0.00002691
Iteration 146/1000 | Loss: 0.00002690
Iteration 147/1000 | Loss: 0.00002690
Iteration 148/1000 | Loss: 0.00002690
Iteration 149/1000 | Loss: 0.00002690
Iteration 150/1000 | Loss: 0.00002690
Iteration 151/1000 | Loss: 0.00002690
Iteration 152/1000 | Loss: 0.00002690
Iteration 153/1000 | Loss: 0.00002690
Iteration 154/1000 | Loss: 0.00002690
Iteration 155/1000 | Loss: 0.00002690
Iteration 156/1000 | Loss: 0.00002690
Iteration 157/1000 | Loss: 0.00002690
Iteration 158/1000 | Loss: 0.00002690
Iteration 159/1000 | Loss: 0.00002690
Iteration 160/1000 | Loss: 0.00002690
Iteration 161/1000 | Loss: 0.00002690
Iteration 162/1000 | Loss: 0.00002690
Iteration 163/1000 | Loss: 0.00002690
Iteration 164/1000 | Loss: 0.00002690
Iteration 165/1000 | Loss: 0.00002690
Iteration 166/1000 | Loss: 0.00002690
Iteration 167/1000 | Loss: 0.00002690
Iteration 168/1000 | Loss: 0.00002690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.6896466806647368e-05, 2.6896466806647368e-05, 2.6896466806647368e-05, 2.6896466806647368e-05, 2.6896466806647368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6896466806647368e-05

Optimization complete. Final v2v error: 4.375178813934326 mm

Highest mean error: 4.659981727600098 mm for frame 146

Lowest mean error: 3.909641981124878 mm for frame 51

Saving results

Total time: 66.09432554244995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431324
Iteration 2/25 | Loss: 0.00139904
Iteration 3/25 | Loss: 0.00129613
Iteration 4/25 | Loss: 0.00128910
Iteration 5/25 | Loss: 0.00128643
Iteration 6/25 | Loss: 0.00128619
Iteration 7/25 | Loss: 0.00128619
Iteration 8/25 | Loss: 0.00128619
Iteration 9/25 | Loss: 0.00128619
Iteration 10/25 | Loss: 0.00128619
Iteration 11/25 | Loss: 0.00128619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012861934956163168, 0.0012861934956163168, 0.0012861934956163168, 0.0012861934956163168, 0.0012861934956163168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012861934956163168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36448348
Iteration 2/25 | Loss: 0.00100790
Iteration 3/25 | Loss: 0.00100790
Iteration 4/25 | Loss: 0.00100790
Iteration 5/25 | Loss: 0.00100790
Iteration 6/25 | Loss: 0.00100790
Iteration 7/25 | Loss: 0.00100790
Iteration 8/25 | Loss: 0.00100790
Iteration 9/25 | Loss: 0.00100790
Iteration 10/25 | Loss: 0.00100790
Iteration 11/25 | Loss: 0.00100790
Iteration 12/25 | Loss: 0.00100790
Iteration 13/25 | Loss: 0.00100790
Iteration 14/25 | Loss: 0.00100790
Iteration 15/25 | Loss: 0.00100790
Iteration 16/25 | Loss: 0.00100790
Iteration 17/25 | Loss: 0.00100790
Iteration 18/25 | Loss: 0.00100790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010078956838697195, 0.0010078956838697195, 0.0010078956838697195, 0.0010078956838697195, 0.0010078956838697195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010078956838697195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100790
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002099
Iteration 5/1000 | Loss: 0.00002023
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001849
Iteration 11/1000 | Loss: 0.00001839
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001818
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001814
Iteration 20/1000 | Loss: 0.00001813
Iteration 21/1000 | Loss: 0.00001812
Iteration 22/1000 | Loss: 0.00001812
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001811
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001809
Iteration 30/1000 | Loss: 0.00001806
Iteration 31/1000 | Loss: 0.00001806
Iteration 32/1000 | Loss: 0.00001806
Iteration 33/1000 | Loss: 0.00001806
Iteration 34/1000 | Loss: 0.00001806
Iteration 35/1000 | Loss: 0.00001806
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001806
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001804
Iteration 40/1000 | Loss: 0.00001804
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001803
Iteration 43/1000 | Loss: 0.00001802
Iteration 44/1000 | Loss: 0.00001802
Iteration 45/1000 | Loss: 0.00001801
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001801
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001795
Iteration 54/1000 | Loss: 0.00001795
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001795
Iteration 59/1000 | Loss: 0.00001795
Iteration 60/1000 | Loss: 0.00001795
Iteration 61/1000 | Loss: 0.00001795
Iteration 62/1000 | Loss: 0.00001795
Iteration 63/1000 | Loss: 0.00001795
Iteration 64/1000 | Loss: 0.00001794
Iteration 65/1000 | Loss: 0.00001794
Iteration 66/1000 | Loss: 0.00001794
Iteration 67/1000 | Loss: 0.00001794
Iteration 68/1000 | Loss: 0.00001794
Iteration 69/1000 | Loss: 0.00001794
Iteration 70/1000 | Loss: 0.00001794
Iteration 71/1000 | Loss: 0.00001794
Iteration 72/1000 | Loss: 0.00001794
Iteration 73/1000 | Loss: 0.00001794
Iteration 74/1000 | Loss: 0.00001794
Iteration 75/1000 | Loss: 0.00001794
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001794
Iteration 79/1000 | Loss: 0.00001794
Iteration 80/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.7940068573807366e-05, 1.7940068573807366e-05, 1.7940068573807366e-05, 1.7940068573807366e-05, 1.7940068573807366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7940068573807366e-05

Optimization complete. Final v2v error: 3.5381340980529785 mm

Highest mean error: 3.6383063793182373 mm for frame 106

Lowest mean error: 3.4560892581939697 mm for frame 148

Saving results

Total time: 32.86699032783508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905774
Iteration 2/25 | Loss: 0.00142755
Iteration 3/25 | Loss: 0.00132317
Iteration 4/25 | Loss: 0.00131273
Iteration 5/25 | Loss: 0.00131140
Iteration 6/25 | Loss: 0.00131140
Iteration 7/25 | Loss: 0.00131140
Iteration 8/25 | Loss: 0.00131140
Iteration 9/25 | Loss: 0.00131140
Iteration 10/25 | Loss: 0.00131140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013113999739289284, 0.0013113999739289284, 0.0013113999739289284, 0.0013113999739289284, 0.0013113999739289284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013113999739289284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36750054
Iteration 2/25 | Loss: 0.00094084
Iteration 3/25 | Loss: 0.00094084
Iteration 4/25 | Loss: 0.00094084
Iteration 5/25 | Loss: 0.00094084
Iteration 6/25 | Loss: 0.00094084
Iteration 7/25 | Loss: 0.00094084
Iteration 8/25 | Loss: 0.00094084
Iteration 9/25 | Loss: 0.00094084
Iteration 10/25 | Loss: 0.00094083
Iteration 11/25 | Loss: 0.00094083
Iteration 12/25 | Loss: 0.00094083
Iteration 13/25 | Loss: 0.00094083
Iteration 14/25 | Loss: 0.00094083
Iteration 15/25 | Loss: 0.00094083
Iteration 16/25 | Loss: 0.00094083
Iteration 17/25 | Loss: 0.00094083
Iteration 18/25 | Loss: 0.00094083
Iteration 19/25 | Loss: 0.00094083
Iteration 20/25 | Loss: 0.00094083
Iteration 21/25 | Loss: 0.00094083
Iteration 22/25 | Loss: 0.00094083
Iteration 23/25 | Loss: 0.00094083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009408341138623655, 0.0009408341138623655, 0.0009408341138623655, 0.0009408341138623655, 0.0009408341138623655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009408341138623655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094083
Iteration 2/1000 | Loss: 0.00003919
Iteration 3/1000 | Loss: 0.00002404
Iteration 4/1000 | Loss: 0.00002065
Iteration 5/1000 | Loss: 0.00001973
Iteration 6/1000 | Loss: 0.00001919
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001825
Iteration 10/1000 | Loss: 0.00001809
Iteration 11/1000 | Loss: 0.00001801
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001796
Iteration 14/1000 | Loss: 0.00001795
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001794
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001778
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001775
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001772
Iteration 28/1000 | Loss: 0.00001772
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001771
Iteration 32/1000 | Loss: 0.00001771
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001769
Iteration 38/1000 | Loss: 0.00001769
Iteration 39/1000 | Loss: 0.00001769
Iteration 40/1000 | Loss: 0.00001769
Iteration 41/1000 | Loss: 0.00001769
Iteration 42/1000 | Loss: 0.00001769
Iteration 43/1000 | Loss: 0.00001769
Iteration 44/1000 | Loss: 0.00001768
Iteration 45/1000 | Loss: 0.00001768
Iteration 46/1000 | Loss: 0.00001767
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001759
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001758
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001758
Iteration 62/1000 | Loss: 0.00001758
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001757
Iteration 65/1000 | Loss: 0.00001754
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001751
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001748
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001745
Iteration 95/1000 | Loss: 0.00001745
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001744
Iteration 101/1000 | Loss: 0.00001744
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001743
Iteration 104/1000 | Loss: 0.00001743
Iteration 105/1000 | Loss: 0.00001743
Iteration 106/1000 | Loss: 0.00001743
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001741
Iteration 111/1000 | Loss: 0.00001741
Iteration 112/1000 | Loss: 0.00001741
Iteration 113/1000 | Loss: 0.00001741
Iteration 114/1000 | Loss: 0.00001741
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00001740
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001739
Iteration 132/1000 | Loss: 0.00001739
Iteration 133/1000 | Loss: 0.00001739
Iteration 134/1000 | Loss: 0.00001739
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001739
Iteration 137/1000 | Loss: 0.00001739
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001738
Iteration 141/1000 | Loss: 0.00001738
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00001738
Iteration 145/1000 | Loss: 0.00001738
Iteration 146/1000 | Loss: 0.00001738
Iteration 147/1000 | Loss: 0.00001738
Iteration 148/1000 | Loss: 0.00001738
Iteration 149/1000 | Loss: 0.00001738
Iteration 150/1000 | Loss: 0.00001738
Iteration 151/1000 | Loss: 0.00001738
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001737
Iteration 157/1000 | Loss: 0.00001737
Iteration 158/1000 | Loss: 0.00001737
Iteration 159/1000 | Loss: 0.00001737
Iteration 160/1000 | Loss: 0.00001737
Iteration 161/1000 | Loss: 0.00001737
Iteration 162/1000 | Loss: 0.00001737
Iteration 163/1000 | Loss: 0.00001737
Iteration 164/1000 | Loss: 0.00001737
Iteration 165/1000 | Loss: 0.00001737
Iteration 166/1000 | Loss: 0.00001737
Iteration 167/1000 | Loss: 0.00001737
Iteration 168/1000 | Loss: 0.00001737
Iteration 169/1000 | Loss: 0.00001736
Iteration 170/1000 | Loss: 0.00001736
Iteration 171/1000 | Loss: 0.00001736
Iteration 172/1000 | Loss: 0.00001736
Iteration 173/1000 | Loss: 0.00001736
Iteration 174/1000 | Loss: 0.00001736
Iteration 175/1000 | Loss: 0.00001736
Iteration 176/1000 | Loss: 0.00001736
Iteration 177/1000 | Loss: 0.00001736
Iteration 178/1000 | Loss: 0.00001736
Iteration 179/1000 | Loss: 0.00001736
Iteration 180/1000 | Loss: 0.00001736
Iteration 181/1000 | Loss: 0.00001736
Iteration 182/1000 | Loss: 0.00001736
Iteration 183/1000 | Loss: 0.00001736
Iteration 184/1000 | Loss: 0.00001736
Iteration 185/1000 | Loss: 0.00001735
Iteration 186/1000 | Loss: 0.00001735
Iteration 187/1000 | Loss: 0.00001735
Iteration 188/1000 | Loss: 0.00001735
Iteration 189/1000 | Loss: 0.00001735
Iteration 190/1000 | Loss: 0.00001735
Iteration 191/1000 | Loss: 0.00001735
Iteration 192/1000 | Loss: 0.00001735
Iteration 193/1000 | Loss: 0.00001735
Iteration 194/1000 | Loss: 0.00001735
Iteration 195/1000 | Loss: 0.00001735
Iteration 196/1000 | Loss: 0.00001735
Iteration 197/1000 | Loss: 0.00001735
Iteration 198/1000 | Loss: 0.00001735
Iteration 199/1000 | Loss: 0.00001735
Iteration 200/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.735079422360286e-05, 1.735079422360286e-05, 1.735079422360286e-05, 1.735079422360286e-05, 1.735079422360286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.735079422360286e-05

Optimization complete. Final v2v error: 3.556058406829834 mm

Highest mean error: 3.809778928756714 mm for frame 202

Lowest mean error: 3.229957103729248 mm for frame 0

Saving results

Total time: 43.33172035217285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01186631
Iteration 2/25 | Loss: 0.00196328
Iteration 3/25 | Loss: 0.00152553
Iteration 4/25 | Loss: 0.00149311
Iteration 5/25 | Loss: 0.00148415
Iteration 6/25 | Loss: 0.00148562
Iteration 7/25 | Loss: 0.00148385
Iteration 8/25 | Loss: 0.00148273
Iteration 9/25 | Loss: 0.00148308
Iteration 10/25 | Loss: 0.00148191
Iteration 11/25 | Loss: 0.00148027
Iteration 12/25 | Loss: 0.00148490
Iteration 13/25 | Loss: 0.00148544
Iteration 14/25 | Loss: 0.00148969
Iteration 15/25 | Loss: 0.00148050
Iteration 16/25 | Loss: 0.00147628
Iteration 17/25 | Loss: 0.00148371
Iteration 18/25 | Loss: 0.00148549
Iteration 19/25 | Loss: 0.00148373
Iteration 20/25 | Loss: 0.00147703
Iteration 21/25 | Loss: 0.00147167
Iteration 22/25 | Loss: 0.00147081
Iteration 23/25 | Loss: 0.00146981
Iteration 24/25 | Loss: 0.00147000
Iteration 25/25 | Loss: 0.00146971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20239413
Iteration 2/25 | Loss: 0.00120015
Iteration 3/25 | Loss: 0.00120015
Iteration 4/25 | Loss: 0.00120015
Iteration 5/25 | Loss: 0.00120015
Iteration 6/25 | Loss: 0.00120015
Iteration 7/25 | Loss: 0.00120014
Iteration 8/25 | Loss: 0.00120014
Iteration 9/25 | Loss: 0.00120014
Iteration 10/25 | Loss: 0.00120014
Iteration 11/25 | Loss: 0.00120014
Iteration 12/25 | Loss: 0.00120014
Iteration 13/25 | Loss: 0.00120014
Iteration 14/25 | Loss: 0.00120014
Iteration 15/25 | Loss: 0.00120014
Iteration 16/25 | Loss: 0.00120014
Iteration 17/25 | Loss: 0.00120014
Iteration 18/25 | Loss: 0.00120014
Iteration 19/25 | Loss: 0.00120014
Iteration 20/25 | Loss: 0.00120014
Iteration 21/25 | Loss: 0.00120014
Iteration 22/25 | Loss: 0.00120014
Iteration 23/25 | Loss: 0.00120014
Iteration 24/25 | Loss: 0.00120014
Iteration 25/25 | Loss: 0.00120014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120014
Iteration 2/1000 | Loss: 0.00009171
Iteration 3/1000 | Loss: 0.00006633
Iteration 4/1000 | Loss: 0.00006571
Iteration 5/1000 | Loss: 0.00005410
Iteration 6/1000 | Loss: 0.00005652
Iteration 7/1000 | Loss: 0.00005203
Iteration 8/1000 | Loss: 0.00006013
Iteration 9/1000 | Loss: 0.00005230
Iteration 10/1000 | Loss: 0.00005903
Iteration 11/1000 | Loss: 0.00006596
Iteration 12/1000 | Loss: 0.00005859
Iteration 13/1000 | Loss: 0.00005359
Iteration 14/1000 | Loss: 0.00005382
Iteration 15/1000 | Loss: 0.00005409
Iteration 16/1000 | Loss: 0.00005374
Iteration 17/1000 | Loss: 0.00005601
Iteration 18/1000 | Loss: 0.00005256
Iteration 19/1000 | Loss: 0.00005752
Iteration 20/1000 | Loss: 0.00005706
Iteration 21/1000 | Loss: 0.00004832
Iteration 22/1000 | Loss: 0.00005385
Iteration 23/1000 | Loss: 0.00005886
Iteration 24/1000 | Loss: 0.00006003
Iteration 25/1000 | Loss: 0.00005781
Iteration 26/1000 | Loss: 0.00005705
Iteration 27/1000 | Loss: 0.00005639
Iteration 28/1000 | Loss: 0.00006122
Iteration 29/1000 | Loss: 0.00006030
Iteration 30/1000 | Loss: 0.00005168
Iteration 31/1000 | Loss: 0.00004812
Iteration 32/1000 | Loss: 0.00004704
Iteration 33/1000 | Loss: 0.00004635
Iteration 34/1000 | Loss: 0.00004579
Iteration 35/1000 | Loss: 0.00004531
Iteration 36/1000 | Loss: 0.00004496
Iteration 37/1000 | Loss: 0.00004471
Iteration 38/1000 | Loss: 0.00004459
Iteration 39/1000 | Loss: 0.00004458
Iteration 40/1000 | Loss: 0.00004440
Iteration 41/1000 | Loss: 0.00004431
Iteration 42/1000 | Loss: 0.00004420
Iteration 43/1000 | Loss: 0.00004410
Iteration 44/1000 | Loss: 0.00004402
Iteration 45/1000 | Loss: 0.00004395
Iteration 46/1000 | Loss: 0.00004394
Iteration 47/1000 | Loss: 0.00004391
Iteration 48/1000 | Loss: 0.00004377
Iteration 49/1000 | Loss: 0.00004373
Iteration 50/1000 | Loss: 0.00004372
Iteration 51/1000 | Loss: 0.00004372
Iteration 52/1000 | Loss: 0.00004372
Iteration 53/1000 | Loss: 0.00004370
Iteration 54/1000 | Loss: 0.00004369
Iteration 55/1000 | Loss: 0.00004367
Iteration 56/1000 | Loss: 0.00004365
Iteration 57/1000 | Loss: 0.00004364
Iteration 58/1000 | Loss: 0.00004363
Iteration 59/1000 | Loss: 0.00004362
Iteration 60/1000 | Loss: 0.00004360
Iteration 61/1000 | Loss: 0.00004360
Iteration 62/1000 | Loss: 0.00004359
Iteration 63/1000 | Loss: 0.00004359
Iteration 64/1000 | Loss: 0.00004359
Iteration 65/1000 | Loss: 0.00004358
Iteration 66/1000 | Loss: 0.00004358
Iteration 67/1000 | Loss: 0.00004358
Iteration 68/1000 | Loss: 0.00004358
Iteration 69/1000 | Loss: 0.00004357
Iteration 70/1000 | Loss: 0.00004357
Iteration 71/1000 | Loss: 0.00004357
Iteration 72/1000 | Loss: 0.00004357
Iteration 73/1000 | Loss: 0.00004357
Iteration 74/1000 | Loss: 0.00004357
Iteration 75/1000 | Loss: 0.00004357
Iteration 76/1000 | Loss: 0.00004356
Iteration 77/1000 | Loss: 0.00004356
Iteration 78/1000 | Loss: 0.00004356
Iteration 79/1000 | Loss: 0.00004356
Iteration 80/1000 | Loss: 0.00004356
Iteration 81/1000 | Loss: 0.00004356
Iteration 82/1000 | Loss: 0.00004355
Iteration 83/1000 | Loss: 0.00004355
Iteration 84/1000 | Loss: 0.00004355
Iteration 85/1000 | Loss: 0.00004355
Iteration 86/1000 | Loss: 0.00004355
Iteration 87/1000 | Loss: 0.00004354
Iteration 88/1000 | Loss: 0.00004354
Iteration 89/1000 | Loss: 0.00004354
Iteration 90/1000 | Loss: 0.00004354
Iteration 91/1000 | Loss: 0.00004353
Iteration 92/1000 | Loss: 0.00004353
Iteration 93/1000 | Loss: 0.00004353
Iteration 94/1000 | Loss: 0.00004353
Iteration 95/1000 | Loss: 0.00004353
Iteration 96/1000 | Loss: 0.00004353
Iteration 97/1000 | Loss: 0.00004353
Iteration 98/1000 | Loss: 0.00004353
Iteration 99/1000 | Loss: 0.00004352
Iteration 100/1000 | Loss: 0.00004352
Iteration 101/1000 | Loss: 0.00004352
Iteration 102/1000 | Loss: 0.00004352
Iteration 103/1000 | Loss: 0.00004352
Iteration 104/1000 | Loss: 0.00004352
Iteration 105/1000 | Loss: 0.00004352
Iteration 106/1000 | Loss: 0.00004352
Iteration 107/1000 | Loss: 0.00004352
Iteration 108/1000 | Loss: 0.00004352
Iteration 109/1000 | Loss: 0.00004352
Iteration 110/1000 | Loss: 0.00004352
Iteration 111/1000 | Loss: 0.00004352
Iteration 112/1000 | Loss: 0.00004352
Iteration 113/1000 | Loss: 0.00004352
Iteration 114/1000 | Loss: 0.00004352
Iteration 115/1000 | Loss: 0.00004351
Iteration 116/1000 | Loss: 0.00004351
Iteration 117/1000 | Loss: 0.00004351
Iteration 118/1000 | Loss: 0.00004351
Iteration 119/1000 | Loss: 0.00004351
Iteration 120/1000 | Loss: 0.00004351
Iteration 121/1000 | Loss: 0.00004351
Iteration 122/1000 | Loss: 0.00004351
Iteration 123/1000 | Loss: 0.00004351
Iteration 124/1000 | Loss: 0.00004351
Iteration 125/1000 | Loss: 0.00004351
Iteration 126/1000 | Loss: 0.00004351
Iteration 127/1000 | Loss: 0.00004351
Iteration 128/1000 | Loss: 0.00004351
Iteration 129/1000 | Loss: 0.00004351
Iteration 130/1000 | Loss: 0.00004351
Iteration 131/1000 | Loss: 0.00004351
Iteration 132/1000 | Loss: 0.00004351
Iteration 133/1000 | Loss: 0.00004351
Iteration 134/1000 | Loss: 0.00004351
Iteration 135/1000 | Loss: 0.00004351
Iteration 136/1000 | Loss: 0.00004350
Iteration 137/1000 | Loss: 0.00004350
Iteration 138/1000 | Loss: 0.00004350
Iteration 139/1000 | Loss: 0.00004350
Iteration 140/1000 | Loss: 0.00004350
Iteration 141/1000 | Loss: 0.00004350
Iteration 142/1000 | Loss: 0.00004350
Iteration 143/1000 | Loss: 0.00004350
Iteration 144/1000 | Loss: 0.00004350
Iteration 145/1000 | Loss: 0.00004350
Iteration 146/1000 | Loss: 0.00004350
Iteration 147/1000 | Loss: 0.00004350
Iteration 148/1000 | Loss: 0.00004350
Iteration 149/1000 | Loss: 0.00004350
Iteration 150/1000 | Loss: 0.00004350
Iteration 151/1000 | Loss: 0.00004350
Iteration 152/1000 | Loss: 0.00004349
Iteration 153/1000 | Loss: 0.00004349
Iteration 154/1000 | Loss: 0.00004349
Iteration 155/1000 | Loss: 0.00004349
Iteration 156/1000 | Loss: 0.00004349
Iteration 157/1000 | Loss: 0.00004349
Iteration 158/1000 | Loss: 0.00004349
Iteration 159/1000 | Loss: 0.00004349
Iteration 160/1000 | Loss: 0.00004349
Iteration 161/1000 | Loss: 0.00004349
Iteration 162/1000 | Loss: 0.00004348
Iteration 163/1000 | Loss: 0.00004348
Iteration 164/1000 | Loss: 0.00004348
Iteration 165/1000 | Loss: 0.00004348
Iteration 166/1000 | Loss: 0.00004348
Iteration 167/1000 | Loss: 0.00004348
Iteration 168/1000 | Loss: 0.00004348
Iteration 169/1000 | Loss: 0.00004348
Iteration 170/1000 | Loss: 0.00004348
Iteration 171/1000 | Loss: 0.00004348
Iteration 172/1000 | Loss: 0.00004348
Iteration 173/1000 | Loss: 0.00004348
Iteration 174/1000 | Loss: 0.00004347
Iteration 175/1000 | Loss: 0.00004347
Iteration 176/1000 | Loss: 0.00004347
Iteration 177/1000 | Loss: 0.00004347
Iteration 178/1000 | Loss: 0.00004347
Iteration 179/1000 | Loss: 0.00004347
Iteration 180/1000 | Loss: 0.00004347
Iteration 181/1000 | Loss: 0.00004347
Iteration 182/1000 | Loss: 0.00004347
Iteration 183/1000 | Loss: 0.00004347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [4.347381764091551e-05, 4.347381764091551e-05, 4.347381764091551e-05, 4.347381764091551e-05, 4.347381764091551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.347381764091551e-05

Optimization complete. Final v2v error: 5.199953556060791 mm

Highest mean error: 6.840155601501465 mm for frame 82

Lowest mean error: 4.126039028167725 mm for frame 0

Saving results

Total time: 130.32692456245422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757662
Iteration 2/25 | Loss: 0.00143598
Iteration 3/25 | Loss: 0.00133725
Iteration 4/25 | Loss: 0.00130650
Iteration 5/25 | Loss: 0.00129818
Iteration 6/25 | Loss: 0.00129564
Iteration 7/25 | Loss: 0.00129498
Iteration 8/25 | Loss: 0.00129466
Iteration 9/25 | Loss: 0.00129454
Iteration 10/25 | Loss: 0.00129454
Iteration 11/25 | Loss: 0.00129454
Iteration 12/25 | Loss: 0.00129454
Iteration 13/25 | Loss: 0.00129454
Iteration 14/25 | Loss: 0.00129454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012945367489010096, 0.0012945367489010096, 0.0012945367489010096, 0.0012945367489010096, 0.0012945367489010096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012945367489010096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89680922
Iteration 2/25 | Loss: 0.00112418
Iteration 3/25 | Loss: 0.00112418
Iteration 4/25 | Loss: 0.00112418
Iteration 5/25 | Loss: 0.00112418
Iteration 6/25 | Loss: 0.00112418
Iteration 7/25 | Loss: 0.00112418
Iteration 8/25 | Loss: 0.00112418
Iteration 9/25 | Loss: 0.00112418
Iteration 10/25 | Loss: 0.00112418
Iteration 11/25 | Loss: 0.00112418
Iteration 12/25 | Loss: 0.00112418
Iteration 13/25 | Loss: 0.00112418
Iteration 14/25 | Loss: 0.00112418
Iteration 15/25 | Loss: 0.00112418
Iteration 16/25 | Loss: 0.00112418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011241763131693006, 0.0011241763131693006, 0.0011241763131693006, 0.0011241763131693006, 0.0011241763131693006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011241763131693006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112418
Iteration 2/1000 | Loss: 0.00004185
Iteration 3/1000 | Loss: 0.00002886
Iteration 4/1000 | Loss: 0.00002404
Iteration 5/1000 | Loss: 0.00009288
Iteration 6/1000 | Loss: 0.00009396
Iteration 7/1000 | Loss: 0.00005413
Iteration 8/1000 | Loss: 0.00006075
Iteration 9/1000 | Loss: 0.00002540
Iteration 10/1000 | Loss: 0.00003790
Iteration 11/1000 | Loss: 0.00002196
Iteration 12/1000 | Loss: 0.00002136
Iteration 13/1000 | Loss: 0.00009681
Iteration 14/1000 | Loss: 0.00025694
Iteration 15/1000 | Loss: 0.00014030
Iteration 16/1000 | Loss: 0.00003604
Iteration 17/1000 | Loss: 0.00002333
Iteration 18/1000 | Loss: 0.00003110
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002056
Iteration 21/1000 | Loss: 0.00002027
Iteration 22/1000 | Loss: 0.00002001
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001969
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001958
Iteration 27/1000 | Loss: 0.00012050
Iteration 28/1000 | Loss: 0.00001975
Iteration 29/1000 | Loss: 0.00001951
Iteration 30/1000 | Loss: 0.00001947
Iteration 31/1000 | Loss: 0.00001946
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001946
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001945
Iteration 39/1000 | Loss: 0.00001945
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001945
Iteration 48/1000 | Loss: 0.00001944
Iteration 49/1000 | Loss: 0.00001944
Iteration 50/1000 | Loss: 0.00001944
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001943
Iteration 53/1000 | Loss: 0.00001943
Iteration 54/1000 | Loss: 0.00001943
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001941
Iteration 57/1000 | Loss: 0.00001941
Iteration 58/1000 | Loss: 0.00001941
Iteration 59/1000 | Loss: 0.00001941
Iteration 60/1000 | Loss: 0.00001941
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001941
Iteration 63/1000 | Loss: 0.00001941
Iteration 64/1000 | Loss: 0.00001940
Iteration 65/1000 | Loss: 0.00001940
Iteration 66/1000 | Loss: 0.00001940
Iteration 67/1000 | Loss: 0.00001940
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001939
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001938
Iteration 74/1000 | Loss: 0.00001938
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001938
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001936
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001935
Iteration 90/1000 | Loss: 0.00001935
Iteration 91/1000 | Loss: 0.00001935
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001933
Iteration 96/1000 | Loss: 0.00001933
Iteration 97/1000 | Loss: 0.00001933
Iteration 98/1000 | Loss: 0.00001932
Iteration 99/1000 | Loss: 0.00001932
Iteration 100/1000 | Loss: 0.00001932
Iteration 101/1000 | Loss: 0.00001932
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001931
Iteration 104/1000 | Loss: 0.00001931
Iteration 105/1000 | Loss: 0.00001931
Iteration 106/1000 | Loss: 0.00001931
Iteration 107/1000 | Loss: 0.00001931
Iteration 108/1000 | Loss: 0.00001931
Iteration 109/1000 | Loss: 0.00001931
Iteration 110/1000 | Loss: 0.00001930
Iteration 111/1000 | Loss: 0.00001930
Iteration 112/1000 | Loss: 0.00001930
Iteration 113/1000 | Loss: 0.00001930
Iteration 114/1000 | Loss: 0.00001929
Iteration 115/1000 | Loss: 0.00001929
Iteration 116/1000 | Loss: 0.00001929
Iteration 117/1000 | Loss: 0.00001929
Iteration 118/1000 | Loss: 0.00001929
Iteration 119/1000 | Loss: 0.00001929
Iteration 120/1000 | Loss: 0.00001929
Iteration 121/1000 | Loss: 0.00001929
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001928
Iteration 127/1000 | Loss: 0.00001928
Iteration 128/1000 | Loss: 0.00001928
Iteration 129/1000 | Loss: 0.00001928
Iteration 130/1000 | Loss: 0.00001928
Iteration 131/1000 | Loss: 0.00001928
Iteration 132/1000 | Loss: 0.00001928
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001927
Iteration 136/1000 | Loss: 0.00001927
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001926
Iteration 140/1000 | Loss: 0.00001926
Iteration 141/1000 | Loss: 0.00001926
Iteration 142/1000 | Loss: 0.00001926
Iteration 143/1000 | Loss: 0.00001926
Iteration 144/1000 | Loss: 0.00001925
Iteration 145/1000 | Loss: 0.00001925
Iteration 146/1000 | Loss: 0.00001925
Iteration 147/1000 | Loss: 0.00001925
Iteration 148/1000 | Loss: 0.00001925
Iteration 149/1000 | Loss: 0.00001925
Iteration 150/1000 | Loss: 0.00001925
Iteration 151/1000 | Loss: 0.00001925
Iteration 152/1000 | Loss: 0.00001924
Iteration 153/1000 | Loss: 0.00001924
Iteration 154/1000 | Loss: 0.00001924
Iteration 155/1000 | Loss: 0.00001924
Iteration 156/1000 | Loss: 0.00001924
Iteration 157/1000 | Loss: 0.00001923
Iteration 158/1000 | Loss: 0.00001923
Iteration 159/1000 | Loss: 0.00001923
Iteration 160/1000 | Loss: 0.00001923
Iteration 161/1000 | Loss: 0.00001922
Iteration 162/1000 | Loss: 0.00001922
Iteration 163/1000 | Loss: 0.00001922
Iteration 164/1000 | Loss: 0.00001922
Iteration 165/1000 | Loss: 0.00001922
Iteration 166/1000 | Loss: 0.00001922
Iteration 167/1000 | Loss: 0.00001922
Iteration 168/1000 | Loss: 0.00001921
Iteration 169/1000 | Loss: 0.00001921
Iteration 170/1000 | Loss: 0.00001921
Iteration 171/1000 | Loss: 0.00001921
Iteration 172/1000 | Loss: 0.00001921
Iteration 173/1000 | Loss: 0.00001921
Iteration 174/1000 | Loss: 0.00001921
Iteration 175/1000 | Loss: 0.00001921
Iteration 176/1000 | Loss: 0.00001921
Iteration 177/1000 | Loss: 0.00001921
Iteration 178/1000 | Loss: 0.00001921
Iteration 179/1000 | Loss: 0.00001921
Iteration 180/1000 | Loss: 0.00001921
Iteration 181/1000 | Loss: 0.00001921
Iteration 182/1000 | Loss: 0.00001921
Iteration 183/1000 | Loss: 0.00001921
Iteration 184/1000 | Loss: 0.00001921
Iteration 185/1000 | Loss: 0.00001921
Iteration 186/1000 | Loss: 0.00001921
Iteration 187/1000 | Loss: 0.00001921
Iteration 188/1000 | Loss: 0.00001921
Iteration 189/1000 | Loss: 0.00001921
Iteration 190/1000 | Loss: 0.00001921
Iteration 191/1000 | Loss: 0.00001921
Iteration 192/1000 | Loss: 0.00001921
Iteration 193/1000 | Loss: 0.00001921
Iteration 194/1000 | Loss: 0.00001921
Iteration 195/1000 | Loss: 0.00001921
Iteration 196/1000 | Loss: 0.00001921
Iteration 197/1000 | Loss: 0.00001921
Iteration 198/1000 | Loss: 0.00001921
Iteration 199/1000 | Loss: 0.00001921
Iteration 200/1000 | Loss: 0.00001921
Iteration 201/1000 | Loss: 0.00001921
Iteration 202/1000 | Loss: 0.00001921
Iteration 203/1000 | Loss: 0.00001921
Iteration 204/1000 | Loss: 0.00001921
Iteration 205/1000 | Loss: 0.00001921
Iteration 206/1000 | Loss: 0.00001921
Iteration 207/1000 | Loss: 0.00001921
Iteration 208/1000 | Loss: 0.00001921
Iteration 209/1000 | Loss: 0.00001921
Iteration 210/1000 | Loss: 0.00001921
Iteration 211/1000 | Loss: 0.00001921
Iteration 212/1000 | Loss: 0.00001921
Iteration 213/1000 | Loss: 0.00001921
Iteration 214/1000 | Loss: 0.00001921
Iteration 215/1000 | Loss: 0.00001921
Iteration 216/1000 | Loss: 0.00001921
Iteration 217/1000 | Loss: 0.00001921
Iteration 218/1000 | Loss: 0.00001921
Iteration 219/1000 | Loss: 0.00001921
Iteration 220/1000 | Loss: 0.00001921
Iteration 221/1000 | Loss: 0.00001921
Iteration 222/1000 | Loss: 0.00001921
Iteration 223/1000 | Loss: 0.00001921
Iteration 224/1000 | Loss: 0.00001921
Iteration 225/1000 | Loss: 0.00001921
Iteration 226/1000 | Loss: 0.00001921
Iteration 227/1000 | Loss: 0.00001921
Iteration 228/1000 | Loss: 0.00001921
Iteration 229/1000 | Loss: 0.00001921
Iteration 230/1000 | Loss: 0.00001921
Iteration 231/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.920911017805338e-05, 1.920911017805338e-05, 1.920911017805338e-05, 1.920911017805338e-05, 1.920911017805338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.920911017805338e-05

Optimization complete. Final v2v error: 3.7060298919677734 mm

Highest mean error: 9.24445915222168 mm for frame 140

Lowest mean error: 3.4650187492370605 mm for frame 53

Saving results

Total time: 72.49679732322693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01150231
Iteration 2/25 | Loss: 0.00228821
Iteration 3/25 | Loss: 0.00176314
Iteration 4/25 | Loss: 0.00174817
Iteration 5/25 | Loss: 0.00157888
Iteration 6/25 | Loss: 0.00149030
Iteration 7/25 | Loss: 0.00145621
Iteration 8/25 | Loss: 0.00143382
Iteration 9/25 | Loss: 0.00143058
Iteration 10/25 | Loss: 0.00142498
Iteration 11/25 | Loss: 0.00142753
Iteration 12/25 | Loss: 0.00142351
Iteration 13/25 | Loss: 0.00142221
Iteration 14/25 | Loss: 0.00141783
Iteration 15/25 | Loss: 0.00141618
Iteration 16/25 | Loss: 0.00142238
Iteration 17/25 | Loss: 0.00142129
Iteration 18/25 | Loss: 0.00142218
Iteration 19/25 | Loss: 0.00141972
Iteration 20/25 | Loss: 0.00141975
Iteration 21/25 | Loss: 0.00142020
Iteration 22/25 | Loss: 0.00141966
Iteration 23/25 | Loss: 0.00141602
Iteration 24/25 | Loss: 0.00141597
Iteration 25/25 | Loss: 0.00141599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.39380312
Iteration 2/25 | Loss: 0.00101367
Iteration 3/25 | Loss: 0.00101363
Iteration 4/25 | Loss: 0.00101363
Iteration 5/25 | Loss: 0.00101363
Iteration 6/25 | Loss: 0.00101363
Iteration 7/25 | Loss: 0.00101363
Iteration 8/25 | Loss: 0.00101363
Iteration 9/25 | Loss: 0.00101363
Iteration 10/25 | Loss: 0.00101363
Iteration 11/25 | Loss: 0.00101363
Iteration 12/25 | Loss: 0.00101363
Iteration 13/25 | Loss: 0.00101363
Iteration 14/25 | Loss: 0.00101363
Iteration 15/25 | Loss: 0.00101363
Iteration 16/25 | Loss: 0.00101363
Iteration 17/25 | Loss: 0.00101363
Iteration 18/25 | Loss: 0.00101363
Iteration 19/25 | Loss: 0.00101363
Iteration 20/25 | Loss: 0.00101363
Iteration 21/25 | Loss: 0.00101363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010136294877156615, 0.0010136294877156615, 0.0010136294877156615, 0.0010136294877156615, 0.0010136294877156615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010136294877156615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101363
Iteration 2/1000 | Loss: 0.00012900
Iteration 3/1000 | Loss: 0.00014123
Iteration 4/1000 | Loss: 0.00028570
Iteration 5/1000 | Loss: 0.00019823
Iteration 6/1000 | Loss: 0.00007054
Iteration 7/1000 | Loss: 0.00008228
Iteration 8/1000 | Loss: 0.00006744
Iteration 9/1000 | Loss: 0.00006805
Iteration 10/1000 | Loss: 0.00009527
Iteration 11/1000 | Loss: 0.00006790
Iteration 12/1000 | Loss: 0.00008896
Iteration 13/1000 | Loss: 0.00009118
Iteration 14/1000 | Loss: 0.00006439
Iteration 15/1000 | Loss: 0.00007356
Iteration 16/1000 | Loss: 0.00007367
Iteration 17/1000 | Loss: 0.00008341
Iteration 18/1000 | Loss: 0.00021802
Iteration 19/1000 | Loss: 0.00008811
Iteration 20/1000 | Loss: 0.00005932
Iteration 21/1000 | Loss: 0.00005337
Iteration 22/1000 | Loss: 0.00004754
Iteration 23/1000 | Loss: 0.00027462
Iteration 24/1000 | Loss: 0.00004095
Iteration 25/1000 | Loss: 0.00005255
Iteration 26/1000 | Loss: 0.00003811
Iteration 27/1000 | Loss: 0.00003688
Iteration 28/1000 | Loss: 0.00003624
Iteration 29/1000 | Loss: 0.00003565
Iteration 30/1000 | Loss: 0.00003518
Iteration 31/1000 | Loss: 0.00003469
Iteration 32/1000 | Loss: 0.00003439
Iteration 33/1000 | Loss: 0.00003420
Iteration 34/1000 | Loss: 0.00003405
Iteration 35/1000 | Loss: 0.00003388
Iteration 36/1000 | Loss: 0.00003386
Iteration 37/1000 | Loss: 0.00003386
Iteration 38/1000 | Loss: 0.00003386
Iteration 39/1000 | Loss: 0.00003378
Iteration 40/1000 | Loss: 0.00003376
Iteration 41/1000 | Loss: 0.00003375
Iteration 42/1000 | Loss: 0.00003374
Iteration 43/1000 | Loss: 0.00003373
Iteration 44/1000 | Loss: 0.00003373
Iteration 45/1000 | Loss: 0.00003372
Iteration 46/1000 | Loss: 0.00003371
Iteration 47/1000 | Loss: 0.00003371
Iteration 48/1000 | Loss: 0.00003371
Iteration 49/1000 | Loss: 0.00003371
Iteration 50/1000 | Loss: 0.00003371
Iteration 51/1000 | Loss: 0.00003371
Iteration 52/1000 | Loss: 0.00003371
Iteration 53/1000 | Loss: 0.00003371
Iteration 54/1000 | Loss: 0.00003370
Iteration 55/1000 | Loss: 0.00003369
Iteration 56/1000 | Loss: 0.00003368
Iteration 57/1000 | Loss: 0.00003367
Iteration 58/1000 | Loss: 0.00003367
Iteration 59/1000 | Loss: 0.00003367
Iteration 60/1000 | Loss: 0.00003367
Iteration 61/1000 | Loss: 0.00003367
Iteration 62/1000 | Loss: 0.00003367
Iteration 63/1000 | Loss: 0.00003366
Iteration 64/1000 | Loss: 0.00003366
Iteration 65/1000 | Loss: 0.00003366
Iteration 66/1000 | Loss: 0.00003366
Iteration 67/1000 | Loss: 0.00003364
Iteration 68/1000 | Loss: 0.00003364
Iteration 69/1000 | Loss: 0.00003363
Iteration 70/1000 | Loss: 0.00003363
Iteration 71/1000 | Loss: 0.00003363
Iteration 72/1000 | Loss: 0.00003363
Iteration 73/1000 | Loss: 0.00003363
Iteration 74/1000 | Loss: 0.00003361
Iteration 75/1000 | Loss: 0.00003360
Iteration 76/1000 | Loss: 0.00003359
Iteration 77/1000 | Loss: 0.00003359
Iteration 78/1000 | Loss: 0.00003359
Iteration 79/1000 | Loss: 0.00003358
Iteration 80/1000 | Loss: 0.00003358
Iteration 81/1000 | Loss: 0.00003358
Iteration 82/1000 | Loss: 0.00003357
Iteration 83/1000 | Loss: 0.00003357
Iteration 84/1000 | Loss: 0.00003357
Iteration 85/1000 | Loss: 0.00003357
Iteration 86/1000 | Loss: 0.00003357
Iteration 87/1000 | Loss: 0.00003357
Iteration 88/1000 | Loss: 0.00003357
Iteration 89/1000 | Loss: 0.00003357
Iteration 90/1000 | Loss: 0.00003357
Iteration 91/1000 | Loss: 0.00003356
Iteration 92/1000 | Loss: 0.00003356
Iteration 93/1000 | Loss: 0.00003356
Iteration 94/1000 | Loss: 0.00003356
Iteration 95/1000 | Loss: 0.00003355
Iteration 96/1000 | Loss: 0.00003355
Iteration 97/1000 | Loss: 0.00003355
Iteration 98/1000 | Loss: 0.00003355
Iteration 99/1000 | Loss: 0.00003355
Iteration 100/1000 | Loss: 0.00003354
Iteration 101/1000 | Loss: 0.00003354
Iteration 102/1000 | Loss: 0.00003354
Iteration 103/1000 | Loss: 0.00003353
Iteration 104/1000 | Loss: 0.00003353
Iteration 105/1000 | Loss: 0.00003353
Iteration 106/1000 | Loss: 0.00003353
Iteration 107/1000 | Loss: 0.00003352
Iteration 108/1000 | Loss: 0.00003352
Iteration 109/1000 | Loss: 0.00003352
Iteration 110/1000 | Loss: 0.00003352
Iteration 111/1000 | Loss: 0.00003352
Iteration 112/1000 | Loss: 0.00003351
Iteration 113/1000 | Loss: 0.00003351
Iteration 114/1000 | Loss: 0.00003351
Iteration 115/1000 | Loss: 0.00003351
Iteration 116/1000 | Loss: 0.00003350
Iteration 117/1000 | Loss: 0.00003350
Iteration 118/1000 | Loss: 0.00003350
Iteration 119/1000 | Loss: 0.00003350
Iteration 120/1000 | Loss: 0.00003350
Iteration 121/1000 | Loss: 0.00003350
Iteration 122/1000 | Loss: 0.00003350
Iteration 123/1000 | Loss: 0.00003349
Iteration 124/1000 | Loss: 0.00003349
Iteration 125/1000 | Loss: 0.00003349
Iteration 126/1000 | Loss: 0.00003349
Iteration 127/1000 | Loss: 0.00003349
Iteration 128/1000 | Loss: 0.00003349
Iteration 129/1000 | Loss: 0.00003349
Iteration 130/1000 | Loss: 0.00003349
Iteration 131/1000 | Loss: 0.00003349
Iteration 132/1000 | Loss: 0.00003348
Iteration 133/1000 | Loss: 0.00003348
Iteration 134/1000 | Loss: 0.00003348
Iteration 135/1000 | Loss: 0.00003348
Iteration 136/1000 | Loss: 0.00003348
Iteration 137/1000 | Loss: 0.00003348
Iteration 138/1000 | Loss: 0.00003348
Iteration 139/1000 | Loss: 0.00003348
Iteration 140/1000 | Loss: 0.00003348
Iteration 141/1000 | Loss: 0.00003348
Iteration 142/1000 | Loss: 0.00003348
Iteration 143/1000 | Loss: 0.00003348
Iteration 144/1000 | Loss: 0.00003348
Iteration 145/1000 | Loss: 0.00003348
Iteration 146/1000 | Loss: 0.00003348
Iteration 147/1000 | Loss: 0.00003348
Iteration 148/1000 | Loss: 0.00003348
Iteration 149/1000 | Loss: 0.00003348
Iteration 150/1000 | Loss: 0.00003347
Iteration 151/1000 | Loss: 0.00003347
Iteration 152/1000 | Loss: 0.00003347
Iteration 153/1000 | Loss: 0.00003347
Iteration 154/1000 | Loss: 0.00003347
Iteration 155/1000 | Loss: 0.00003347
Iteration 156/1000 | Loss: 0.00003347
Iteration 157/1000 | Loss: 0.00003347
Iteration 158/1000 | Loss: 0.00003347
Iteration 159/1000 | Loss: 0.00003347
Iteration 160/1000 | Loss: 0.00003347
Iteration 161/1000 | Loss: 0.00003347
Iteration 162/1000 | Loss: 0.00003347
Iteration 163/1000 | Loss: 0.00003347
Iteration 164/1000 | Loss: 0.00003346
Iteration 165/1000 | Loss: 0.00003346
Iteration 166/1000 | Loss: 0.00003346
Iteration 167/1000 | Loss: 0.00003346
Iteration 168/1000 | Loss: 0.00003346
Iteration 169/1000 | Loss: 0.00003346
Iteration 170/1000 | Loss: 0.00003346
Iteration 171/1000 | Loss: 0.00003346
Iteration 172/1000 | Loss: 0.00003346
Iteration 173/1000 | Loss: 0.00003346
Iteration 174/1000 | Loss: 0.00003346
Iteration 175/1000 | Loss: 0.00003346
Iteration 176/1000 | Loss: 0.00003345
Iteration 177/1000 | Loss: 0.00003345
Iteration 178/1000 | Loss: 0.00003345
Iteration 179/1000 | Loss: 0.00003345
Iteration 180/1000 | Loss: 0.00003345
Iteration 181/1000 | Loss: 0.00003345
Iteration 182/1000 | Loss: 0.00003345
Iteration 183/1000 | Loss: 0.00003345
Iteration 184/1000 | Loss: 0.00003345
Iteration 185/1000 | Loss: 0.00003345
Iteration 186/1000 | Loss: 0.00003345
Iteration 187/1000 | Loss: 0.00003345
Iteration 188/1000 | Loss: 0.00003345
Iteration 189/1000 | Loss: 0.00003345
Iteration 190/1000 | Loss: 0.00003345
Iteration 191/1000 | Loss: 0.00003345
Iteration 192/1000 | Loss: 0.00003345
Iteration 193/1000 | Loss: 0.00003345
Iteration 194/1000 | Loss: 0.00003345
Iteration 195/1000 | Loss: 0.00003345
Iteration 196/1000 | Loss: 0.00003345
Iteration 197/1000 | Loss: 0.00003345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [3.344896322232671e-05, 3.344896322232671e-05, 3.344896322232671e-05, 3.344896322232671e-05, 3.344896322232671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.344896322232671e-05

Optimization complete. Final v2v error: 4.3948469161987305 mm

Highest mean error: 20.063188552856445 mm for frame 1

Lowest mean error: 3.973470449447632 mm for frame 188

Saving results

Total time: 109.00418305397034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926968
Iteration 2/25 | Loss: 0.00178980
Iteration 3/25 | Loss: 0.00150236
Iteration 4/25 | Loss: 0.00140930
Iteration 5/25 | Loss: 0.00138835
Iteration 6/25 | Loss: 0.00136179
Iteration 7/25 | Loss: 0.00136108
Iteration 8/25 | Loss: 0.00134613
Iteration 9/25 | Loss: 0.00133608
Iteration 10/25 | Loss: 0.00134165
Iteration 11/25 | Loss: 0.00133701
Iteration 12/25 | Loss: 0.00133180
Iteration 13/25 | Loss: 0.00133047
Iteration 14/25 | Loss: 0.00133106
Iteration 15/25 | Loss: 0.00133044
Iteration 16/25 | Loss: 0.00132946
Iteration 17/25 | Loss: 0.00132996
Iteration 18/25 | Loss: 0.00133017
Iteration 19/25 | Loss: 0.00133100
Iteration 20/25 | Loss: 0.00132943
Iteration 21/25 | Loss: 0.00133074
Iteration 22/25 | Loss: 0.00132974
Iteration 23/25 | Loss: 0.00132958
Iteration 24/25 | Loss: 0.00133000
Iteration 25/25 | Loss: 0.00133086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.31069469
Iteration 2/25 | Loss: 0.00137517
Iteration 3/25 | Loss: 0.00137515
Iteration 4/25 | Loss: 0.00137515
Iteration 5/25 | Loss: 0.00137515
Iteration 6/25 | Loss: 0.00137515
Iteration 7/25 | Loss: 0.00137515
Iteration 8/25 | Loss: 0.00137515
Iteration 9/25 | Loss: 0.00137515
Iteration 10/25 | Loss: 0.00137515
Iteration 11/25 | Loss: 0.00137515
Iteration 12/25 | Loss: 0.00137515
Iteration 13/25 | Loss: 0.00137515
Iteration 14/25 | Loss: 0.00137515
Iteration 15/25 | Loss: 0.00137515
Iteration 16/25 | Loss: 0.00137515
Iteration 17/25 | Loss: 0.00137515
Iteration 18/25 | Loss: 0.00137515
Iteration 19/25 | Loss: 0.00137515
Iteration 20/25 | Loss: 0.00137515
Iteration 21/25 | Loss: 0.00137515
Iteration 22/25 | Loss: 0.00137515
Iteration 23/25 | Loss: 0.00137515
Iteration 24/25 | Loss: 0.00137515
Iteration 25/25 | Loss: 0.00137515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137515
Iteration 2/1000 | Loss: 0.00007485
Iteration 3/1000 | Loss: 0.00005081
Iteration 4/1000 | Loss: 0.00026139
Iteration 5/1000 | Loss: 0.00003597
Iteration 6/1000 | Loss: 0.00003307
Iteration 7/1000 | Loss: 0.00003560
Iteration 8/1000 | Loss: 0.00003155
Iteration 9/1000 | Loss: 0.00003600
Iteration 10/1000 | Loss: 0.00002863
Iteration 11/1000 | Loss: 0.00002628
Iteration 12/1000 | Loss: 0.00002532
Iteration 13/1000 | Loss: 0.00002495
Iteration 14/1000 | Loss: 0.00002484
Iteration 15/1000 | Loss: 0.00002478
Iteration 16/1000 | Loss: 0.00002458
Iteration 17/1000 | Loss: 0.00002458
Iteration 18/1000 | Loss: 0.00002451
Iteration 19/1000 | Loss: 0.00002448
Iteration 20/1000 | Loss: 0.00002444
Iteration 21/1000 | Loss: 0.00002434
Iteration 22/1000 | Loss: 0.00002418
Iteration 23/1000 | Loss: 0.00002410
Iteration 24/1000 | Loss: 0.00002409
Iteration 25/1000 | Loss: 0.00002407
Iteration 26/1000 | Loss: 0.00002403
Iteration 27/1000 | Loss: 0.00002401
Iteration 28/1000 | Loss: 0.00002401
Iteration 29/1000 | Loss: 0.00002400
Iteration 30/1000 | Loss: 0.00002400
Iteration 31/1000 | Loss: 0.00002399
Iteration 32/1000 | Loss: 0.00002399
Iteration 33/1000 | Loss: 0.00002398
Iteration 34/1000 | Loss: 0.00002397
Iteration 35/1000 | Loss: 0.00002396
Iteration 36/1000 | Loss: 0.00002396
Iteration 37/1000 | Loss: 0.00002395
Iteration 38/1000 | Loss: 0.00002394
Iteration 39/1000 | Loss: 0.00002394
Iteration 40/1000 | Loss: 0.00002393
Iteration 41/1000 | Loss: 0.00002392
Iteration 42/1000 | Loss: 0.00002392
Iteration 43/1000 | Loss: 0.00002390
Iteration 44/1000 | Loss: 0.00002389
Iteration 45/1000 | Loss: 0.00002388
Iteration 46/1000 | Loss: 0.00002388
Iteration 47/1000 | Loss: 0.00002387
Iteration 48/1000 | Loss: 0.00002387
Iteration 49/1000 | Loss: 0.00002387
Iteration 50/1000 | Loss: 0.00002387
Iteration 51/1000 | Loss: 0.00002387
Iteration 52/1000 | Loss: 0.00002387
Iteration 53/1000 | Loss: 0.00002386
Iteration 54/1000 | Loss: 0.00002386
Iteration 55/1000 | Loss: 0.00002386
Iteration 56/1000 | Loss: 0.00002386
Iteration 57/1000 | Loss: 0.00002386
Iteration 58/1000 | Loss: 0.00002386
Iteration 59/1000 | Loss: 0.00002385
Iteration 60/1000 | Loss: 0.00002385
Iteration 61/1000 | Loss: 0.00002385
Iteration 62/1000 | Loss: 0.00002385
Iteration 63/1000 | Loss: 0.00002385
Iteration 64/1000 | Loss: 0.00002385
Iteration 65/1000 | Loss: 0.00002385
Iteration 66/1000 | Loss: 0.00002385
Iteration 67/1000 | Loss: 0.00002384
Iteration 68/1000 | Loss: 0.00002384
Iteration 69/1000 | Loss: 0.00002384
Iteration 70/1000 | Loss: 0.00002384
Iteration 71/1000 | Loss: 0.00002384
Iteration 72/1000 | Loss: 0.00002384
Iteration 73/1000 | Loss: 0.00002384
Iteration 74/1000 | Loss: 0.00002384
Iteration 75/1000 | Loss: 0.00002384
Iteration 76/1000 | Loss: 0.00002384
Iteration 77/1000 | Loss: 0.00002383
Iteration 78/1000 | Loss: 0.00002383
Iteration 79/1000 | Loss: 0.00002383
Iteration 80/1000 | Loss: 0.00002383
Iteration 81/1000 | Loss: 0.00002383
Iteration 82/1000 | Loss: 0.00002383
Iteration 83/1000 | Loss: 0.00002382
Iteration 84/1000 | Loss: 0.00002382
Iteration 85/1000 | Loss: 0.00002382
Iteration 86/1000 | Loss: 0.00002382
Iteration 87/1000 | Loss: 0.00002382
Iteration 88/1000 | Loss: 0.00002382
Iteration 89/1000 | Loss: 0.00002382
Iteration 90/1000 | Loss: 0.00002381
Iteration 91/1000 | Loss: 0.00002381
Iteration 92/1000 | Loss: 0.00002381
Iteration 93/1000 | Loss: 0.00002381
Iteration 94/1000 | Loss: 0.00002381
Iteration 95/1000 | Loss: 0.00002381
Iteration 96/1000 | Loss: 0.00002381
Iteration 97/1000 | Loss: 0.00002381
Iteration 98/1000 | Loss: 0.00002381
Iteration 99/1000 | Loss: 0.00002381
Iteration 100/1000 | Loss: 0.00002381
Iteration 101/1000 | Loss: 0.00002381
Iteration 102/1000 | Loss: 0.00002381
Iteration 103/1000 | Loss: 0.00002381
Iteration 104/1000 | Loss: 0.00002381
Iteration 105/1000 | Loss: 0.00002380
Iteration 106/1000 | Loss: 0.00002380
Iteration 107/1000 | Loss: 0.00002380
Iteration 108/1000 | Loss: 0.00002380
Iteration 109/1000 | Loss: 0.00002380
Iteration 110/1000 | Loss: 0.00002380
Iteration 111/1000 | Loss: 0.00002380
Iteration 112/1000 | Loss: 0.00002380
Iteration 113/1000 | Loss: 0.00002380
Iteration 114/1000 | Loss: 0.00002379
Iteration 115/1000 | Loss: 0.00002379
Iteration 116/1000 | Loss: 0.00002379
Iteration 117/1000 | Loss: 0.00002379
Iteration 118/1000 | Loss: 0.00002379
Iteration 119/1000 | Loss: 0.00002379
Iteration 120/1000 | Loss: 0.00002379
Iteration 121/1000 | Loss: 0.00002379
Iteration 122/1000 | Loss: 0.00002379
Iteration 123/1000 | Loss: 0.00002379
Iteration 124/1000 | Loss: 0.00002379
Iteration 125/1000 | Loss: 0.00002379
Iteration 126/1000 | Loss: 0.00002379
Iteration 127/1000 | Loss: 0.00002379
Iteration 128/1000 | Loss: 0.00002379
Iteration 129/1000 | Loss: 0.00002378
Iteration 130/1000 | Loss: 0.00002378
Iteration 131/1000 | Loss: 0.00002378
Iteration 132/1000 | Loss: 0.00002378
Iteration 133/1000 | Loss: 0.00002378
Iteration 134/1000 | Loss: 0.00002378
Iteration 135/1000 | Loss: 0.00002378
Iteration 136/1000 | Loss: 0.00002378
Iteration 137/1000 | Loss: 0.00002378
Iteration 138/1000 | Loss: 0.00002378
Iteration 139/1000 | Loss: 0.00002378
Iteration 140/1000 | Loss: 0.00002378
Iteration 141/1000 | Loss: 0.00002378
Iteration 142/1000 | Loss: 0.00002377
Iteration 143/1000 | Loss: 0.00002377
Iteration 144/1000 | Loss: 0.00002377
Iteration 145/1000 | Loss: 0.00002377
Iteration 146/1000 | Loss: 0.00002377
Iteration 147/1000 | Loss: 0.00002377
Iteration 148/1000 | Loss: 0.00002377
Iteration 149/1000 | Loss: 0.00002377
Iteration 150/1000 | Loss: 0.00002377
Iteration 151/1000 | Loss: 0.00002377
Iteration 152/1000 | Loss: 0.00002377
Iteration 153/1000 | Loss: 0.00002377
Iteration 154/1000 | Loss: 0.00002377
Iteration 155/1000 | Loss: 0.00002377
Iteration 156/1000 | Loss: 0.00002377
Iteration 157/1000 | Loss: 0.00002377
Iteration 158/1000 | Loss: 0.00002376
Iteration 159/1000 | Loss: 0.00002376
Iteration 160/1000 | Loss: 0.00002376
Iteration 161/1000 | Loss: 0.00002376
Iteration 162/1000 | Loss: 0.00002376
Iteration 163/1000 | Loss: 0.00002376
Iteration 164/1000 | Loss: 0.00002376
Iteration 165/1000 | Loss: 0.00002376
Iteration 166/1000 | Loss: 0.00002376
Iteration 167/1000 | Loss: 0.00002376
Iteration 168/1000 | Loss: 0.00002376
Iteration 169/1000 | Loss: 0.00002376
Iteration 170/1000 | Loss: 0.00002375
Iteration 171/1000 | Loss: 0.00002375
Iteration 172/1000 | Loss: 0.00002375
Iteration 173/1000 | Loss: 0.00002375
Iteration 174/1000 | Loss: 0.00002375
Iteration 175/1000 | Loss: 0.00002375
Iteration 176/1000 | Loss: 0.00002375
Iteration 177/1000 | Loss: 0.00002375
Iteration 178/1000 | Loss: 0.00002375
Iteration 179/1000 | Loss: 0.00002375
Iteration 180/1000 | Loss: 0.00002375
Iteration 181/1000 | Loss: 0.00002375
Iteration 182/1000 | Loss: 0.00002375
Iteration 183/1000 | Loss: 0.00002375
Iteration 184/1000 | Loss: 0.00002375
Iteration 185/1000 | Loss: 0.00002375
Iteration 186/1000 | Loss: 0.00002375
Iteration 187/1000 | Loss: 0.00002375
Iteration 188/1000 | Loss: 0.00002375
Iteration 189/1000 | Loss: 0.00002375
Iteration 190/1000 | Loss: 0.00002375
Iteration 191/1000 | Loss: 0.00002375
Iteration 192/1000 | Loss: 0.00002375
Iteration 193/1000 | Loss: 0.00002375
Iteration 194/1000 | Loss: 0.00002375
Iteration 195/1000 | Loss: 0.00002375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.3750877517159097e-05, 2.3750877517159097e-05, 2.3750877517159097e-05, 2.3750877517159097e-05, 2.3750877517159097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3750877517159097e-05

Optimization complete. Final v2v error: 4.0140814781188965 mm

Highest mean error: 9.703496932983398 mm for frame 52

Lowest mean error: 3.26180362701416 mm for frame 0

Saving results

Total time: 85.19965124130249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546392
Iteration 2/25 | Loss: 0.00145714
Iteration 3/25 | Loss: 0.00132923
Iteration 4/25 | Loss: 0.00131090
Iteration 5/25 | Loss: 0.00130742
Iteration 6/25 | Loss: 0.00130388
Iteration 7/25 | Loss: 0.00130513
Iteration 8/25 | Loss: 0.00130360
Iteration 9/25 | Loss: 0.00130300
Iteration 10/25 | Loss: 0.00130294
Iteration 11/25 | Loss: 0.00130294
Iteration 12/25 | Loss: 0.00130294
Iteration 13/25 | Loss: 0.00130294
Iteration 14/25 | Loss: 0.00130294
Iteration 15/25 | Loss: 0.00130294
Iteration 16/25 | Loss: 0.00130294
Iteration 17/25 | Loss: 0.00130294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001302942167967558, 0.001302942167967558, 0.001302942167967558, 0.001302942167967558, 0.001302942167967558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001302942167967558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.83909893
Iteration 2/25 | Loss: 0.00102399
Iteration 3/25 | Loss: 0.00102398
Iteration 4/25 | Loss: 0.00102398
Iteration 5/25 | Loss: 0.00102398
Iteration 6/25 | Loss: 0.00102398
Iteration 7/25 | Loss: 0.00102398
Iteration 8/25 | Loss: 0.00102398
Iteration 9/25 | Loss: 0.00102398
Iteration 10/25 | Loss: 0.00102398
Iteration 11/25 | Loss: 0.00102398
Iteration 12/25 | Loss: 0.00102398
Iteration 13/25 | Loss: 0.00102398
Iteration 14/25 | Loss: 0.00102398
Iteration 15/25 | Loss: 0.00102398
Iteration 16/25 | Loss: 0.00102398
Iteration 17/25 | Loss: 0.00102398
Iteration 18/25 | Loss: 0.00102398
Iteration 19/25 | Loss: 0.00102398
Iteration 20/25 | Loss: 0.00102398
Iteration 21/25 | Loss: 0.00102398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010239765979349613, 0.0010239765979349613, 0.0010239765979349613, 0.0010239765979349613, 0.0010239765979349613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010239765979349613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102398
Iteration 2/1000 | Loss: 0.00004176
Iteration 3/1000 | Loss: 0.00002753
Iteration 4/1000 | Loss: 0.00002341
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00002099
Iteration 7/1000 | Loss: 0.00004327
Iteration 8/1000 | Loss: 0.00002513
Iteration 9/1000 | Loss: 0.00002086
Iteration 10/1000 | Loss: 0.00002001
Iteration 11/1000 | Loss: 0.00001997
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00005301
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001958
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001936
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001929
Iteration 24/1000 | Loss: 0.00001928
Iteration 25/1000 | Loss: 0.00001927
Iteration 26/1000 | Loss: 0.00001924
Iteration 27/1000 | Loss: 0.00001923
Iteration 28/1000 | Loss: 0.00001923
Iteration 29/1000 | Loss: 0.00001923
Iteration 30/1000 | Loss: 0.00001922
Iteration 31/1000 | Loss: 0.00001921
Iteration 32/1000 | Loss: 0.00001916
Iteration 33/1000 | Loss: 0.00001916
Iteration 34/1000 | Loss: 0.00001915
Iteration 35/1000 | Loss: 0.00001914
Iteration 36/1000 | Loss: 0.00001914
Iteration 37/1000 | Loss: 0.00001913
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001904
Iteration 46/1000 | Loss: 0.00001904
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001903
Iteration 50/1000 | Loss: 0.00001903
Iteration 51/1000 | Loss: 0.00001902
Iteration 52/1000 | Loss: 0.00001902
Iteration 53/1000 | Loss: 0.00001901
Iteration 54/1000 | Loss: 0.00001901
Iteration 55/1000 | Loss: 0.00001900
Iteration 56/1000 | Loss: 0.00001900
Iteration 57/1000 | Loss: 0.00001900
Iteration 58/1000 | Loss: 0.00001899
Iteration 59/1000 | Loss: 0.00001899
Iteration 60/1000 | Loss: 0.00001899
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00003682
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001892
Iteration 71/1000 | Loss: 0.00001892
Iteration 72/1000 | Loss: 0.00001891
Iteration 73/1000 | Loss: 0.00001891
Iteration 74/1000 | Loss: 0.00001891
Iteration 75/1000 | Loss: 0.00001891
Iteration 76/1000 | Loss: 0.00001891
Iteration 77/1000 | Loss: 0.00001891
Iteration 78/1000 | Loss: 0.00001891
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001891
Iteration 81/1000 | Loss: 0.00001891
Iteration 82/1000 | Loss: 0.00001891
Iteration 83/1000 | Loss: 0.00001890
Iteration 84/1000 | Loss: 0.00001890
Iteration 85/1000 | Loss: 0.00001890
Iteration 86/1000 | Loss: 0.00001890
Iteration 87/1000 | Loss: 0.00001890
Iteration 88/1000 | Loss: 0.00001890
Iteration 89/1000 | Loss: 0.00001890
Iteration 90/1000 | Loss: 0.00001890
Iteration 91/1000 | Loss: 0.00001889
Iteration 92/1000 | Loss: 0.00001889
Iteration 93/1000 | Loss: 0.00001889
Iteration 94/1000 | Loss: 0.00001889
Iteration 95/1000 | Loss: 0.00001889
Iteration 96/1000 | Loss: 0.00001888
Iteration 97/1000 | Loss: 0.00001888
Iteration 98/1000 | Loss: 0.00001888
Iteration 99/1000 | Loss: 0.00001888
Iteration 100/1000 | Loss: 0.00001888
Iteration 101/1000 | Loss: 0.00001888
Iteration 102/1000 | Loss: 0.00001888
Iteration 103/1000 | Loss: 0.00001888
Iteration 104/1000 | Loss: 0.00001888
Iteration 105/1000 | Loss: 0.00001888
Iteration 106/1000 | Loss: 0.00001888
Iteration 107/1000 | Loss: 0.00001888
Iteration 108/1000 | Loss: 0.00001887
Iteration 109/1000 | Loss: 0.00001887
Iteration 110/1000 | Loss: 0.00001887
Iteration 111/1000 | Loss: 0.00001887
Iteration 112/1000 | Loss: 0.00001887
Iteration 113/1000 | Loss: 0.00001887
Iteration 114/1000 | Loss: 0.00001887
Iteration 115/1000 | Loss: 0.00001887
Iteration 116/1000 | Loss: 0.00001886
Iteration 117/1000 | Loss: 0.00001886
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001886
Iteration 120/1000 | Loss: 0.00001886
Iteration 121/1000 | Loss: 0.00001886
Iteration 122/1000 | Loss: 0.00001886
Iteration 123/1000 | Loss: 0.00001886
Iteration 124/1000 | Loss: 0.00001886
Iteration 125/1000 | Loss: 0.00001886
Iteration 126/1000 | Loss: 0.00001886
Iteration 127/1000 | Loss: 0.00001886
Iteration 128/1000 | Loss: 0.00001886
Iteration 129/1000 | Loss: 0.00001886
Iteration 130/1000 | Loss: 0.00001885
Iteration 131/1000 | Loss: 0.00001885
Iteration 132/1000 | Loss: 0.00001885
Iteration 133/1000 | Loss: 0.00001885
Iteration 134/1000 | Loss: 0.00001885
Iteration 135/1000 | Loss: 0.00001885
Iteration 136/1000 | Loss: 0.00001885
Iteration 137/1000 | Loss: 0.00001885
Iteration 138/1000 | Loss: 0.00001885
Iteration 139/1000 | Loss: 0.00001885
Iteration 140/1000 | Loss: 0.00001885
Iteration 141/1000 | Loss: 0.00001885
Iteration 142/1000 | Loss: 0.00001885
Iteration 143/1000 | Loss: 0.00001885
Iteration 144/1000 | Loss: 0.00001885
Iteration 145/1000 | Loss: 0.00001885
Iteration 146/1000 | Loss: 0.00001885
Iteration 147/1000 | Loss: 0.00001885
Iteration 148/1000 | Loss: 0.00001885
Iteration 149/1000 | Loss: 0.00001884
Iteration 150/1000 | Loss: 0.00001884
Iteration 151/1000 | Loss: 0.00001884
Iteration 152/1000 | Loss: 0.00001884
Iteration 153/1000 | Loss: 0.00001884
Iteration 154/1000 | Loss: 0.00001884
Iteration 155/1000 | Loss: 0.00001884
Iteration 156/1000 | Loss: 0.00001884
Iteration 157/1000 | Loss: 0.00001884
Iteration 158/1000 | Loss: 0.00001884
Iteration 159/1000 | Loss: 0.00001884
Iteration 160/1000 | Loss: 0.00001884
Iteration 161/1000 | Loss: 0.00001884
Iteration 162/1000 | Loss: 0.00001884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.884231278381776e-05, 1.884231278381776e-05, 1.884231278381776e-05, 1.884231278381776e-05, 1.884231278381776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.884231278381776e-05

Optimization complete. Final v2v error: 3.6835856437683105 mm

Highest mean error: 4.264528751373291 mm for frame 200

Lowest mean error: 3.1787362098693848 mm for frame 7

Saving results

Total time: 56.17794585227966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050955
Iteration 2/25 | Loss: 0.00299195
Iteration 3/25 | Loss: 0.00225476
Iteration 4/25 | Loss: 0.00201784
Iteration 5/25 | Loss: 0.00192950
Iteration 6/25 | Loss: 0.00189782
Iteration 7/25 | Loss: 0.00188312
Iteration 8/25 | Loss: 0.00185850
Iteration 9/25 | Loss: 0.00182768
Iteration 10/25 | Loss: 0.00178527
Iteration 11/25 | Loss: 0.00176711
Iteration 12/25 | Loss: 0.00175267
Iteration 13/25 | Loss: 0.00173529
Iteration 14/25 | Loss: 0.00171800
Iteration 15/25 | Loss: 0.00171591
Iteration 16/25 | Loss: 0.00170680
Iteration 17/25 | Loss: 0.00170574
Iteration 18/25 | Loss: 0.00170528
Iteration 19/25 | Loss: 0.00170639
Iteration 20/25 | Loss: 0.00170582
Iteration 21/25 | Loss: 0.00170563
Iteration 22/25 | Loss: 0.00170433
Iteration 23/25 | Loss: 0.00170515
Iteration 24/25 | Loss: 0.00170441
Iteration 25/25 | Loss: 0.00170401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.34220314
Iteration 2/25 | Loss: 0.00668814
Iteration 3/25 | Loss: 0.00668814
Iteration 4/25 | Loss: 0.00668814
Iteration 5/25 | Loss: 0.00668814
Iteration 6/25 | Loss: 0.00668814
Iteration 7/25 | Loss: 0.00668814
Iteration 8/25 | Loss: 0.00668814
Iteration 9/25 | Loss: 0.00668813
Iteration 10/25 | Loss: 0.00668813
Iteration 11/25 | Loss: 0.00668813
Iteration 12/25 | Loss: 0.00668813
Iteration 13/25 | Loss: 0.00668813
Iteration 14/25 | Loss: 0.00668813
Iteration 15/25 | Loss: 0.00668813
Iteration 16/25 | Loss: 0.00668813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0066881338134408, 0.0066881338134408, 0.0066881338134408, 0.0066881338134408, 0.0066881338134408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0066881338134408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00668813
Iteration 2/1000 | Loss: 0.00072861
Iteration 3/1000 | Loss: 0.00048002
Iteration 4/1000 | Loss: 0.00041346
Iteration 5/1000 | Loss: 0.00037962
Iteration 6/1000 | Loss: 0.00034537
Iteration 7/1000 | Loss: 0.00927274
Iteration 8/1000 | Loss: 0.00070329
Iteration 9/1000 | Loss: 0.00024938
Iteration 10/1000 | Loss: 0.00017607
Iteration 11/1000 | Loss: 0.00012588
Iteration 12/1000 | Loss: 0.00009460
Iteration 13/1000 | Loss: 0.00007437
Iteration 14/1000 | Loss: 0.00005926
Iteration 15/1000 | Loss: 0.00004902
Iteration 16/1000 | Loss: 0.00004261
Iteration 17/1000 | Loss: 0.00003897
Iteration 18/1000 | Loss: 0.00003677
Iteration 19/1000 | Loss: 0.00003514
Iteration 20/1000 | Loss: 0.00003555
Iteration 21/1000 | Loss: 0.00004516
Iteration 22/1000 | Loss: 0.00003461
Iteration 23/1000 | Loss: 0.00003386
Iteration 24/1000 | Loss: 0.00003303
Iteration 25/1000 | Loss: 0.00003262
Iteration 26/1000 | Loss: 0.00003415
Iteration 27/1000 | Loss: 0.00003278
Iteration 28/1000 | Loss: 0.00003244
Iteration 29/1000 | Loss: 0.00004058
Iteration 30/1000 | Loss: 0.00004392
Iteration 31/1000 | Loss: 0.00004239
Iteration 32/1000 | Loss: 0.00003366
Iteration 33/1000 | Loss: 0.00003215
Iteration 34/1000 | Loss: 0.00003219
Iteration 35/1000 | Loss: 0.00003434
Iteration 36/1000 | Loss: 0.00003564
Iteration 37/1000 | Loss: 0.00003361
Iteration 38/1000 | Loss: 0.00003378
Iteration 39/1000 | Loss: 0.00003169
Iteration 40/1000 | Loss: 0.00003355
Iteration 41/1000 | Loss: 0.00003334
Iteration 42/1000 | Loss: 0.00003744
Iteration 43/1000 | Loss: 0.00003709
Iteration 44/1000 | Loss: 0.00003161
Iteration 45/1000 | Loss: 0.00003623
Iteration 46/1000 | Loss: 0.00003688
Iteration 47/1000 | Loss: 0.00003461
Iteration 48/1000 | Loss: 0.00003668
Iteration 49/1000 | Loss: 0.00003196
Iteration 50/1000 | Loss: 0.00004110
Iteration 51/1000 | Loss: 0.00003230
Iteration 52/1000 | Loss: 0.00003341
Iteration 53/1000 | Loss: 0.00003669
Iteration 54/1000 | Loss: 0.00003261
Iteration 55/1000 | Loss: 0.00003202
Iteration 56/1000 | Loss: 0.00003208
Iteration 57/1000 | Loss: 0.00003236
Iteration 58/1000 | Loss: 0.00003286
Iteration 59/1000 | Loss: 0.00003422
Iteration 60/1000 | Loss: 0.00003212
Iteration 61/1000 | Loss: 0.00003177
Iteration 62/1000 | Loss: 0.00003176
Iteration 63/1000 | Loss: 0.00003148
Iteration 64/1000 | Loss: 0.00003147
Iteration 65/1000 | Loss: 0.00003147
Iteration 66/1000 | Loss: 0.00003146
Iteration 67/1000 | Loss: 0.00003146
Iteration 68/1000 | Loss: 0.00003146
Iteration 69/1000 | Loss: 0.00003146
Iteration 70/1000 | Loss: 0.00003146
Iteration 71/1000 | Loss: 0.00003146
Iteration 72/1000 | Loss: 0.00003146
Iteration 73/1000 | Loss: 0.00003146
Iteration 74/1000 | Loss: 0.00003146
Iteration 75/1000 | Loss: 0.00003146
Iteration 76/1000 | Loss: 0.00003146
Iteration 77/1000 | Loss: 0.00003145
Iteration 78/1000 | Loss: 0.00003145
Iteration 79/1000 | Loss: 0.00003145
Iteration 80/1000 | Loss: 0.00003145
Iteration 81/1000 | Loss: 0.00003145
Iteration 82/1000 | Loss: 0.00003145
Iteration 83/1000 | Loss: 0.00003145
Iteration 84/1000 | Loss: 0.00003145
Iteration 85/1000 | Loss: 0.00003145
Iteration 86/1000 | Loss: 0.00003144
Iteration 87/1000 | Loss: 0.00003144
Iteration 88/1000 | Loss: 0.00003144
Iteration 89/1000 | Loss: 0.00003144
Iteration 90/1000 | Loss: 0.00003144
Iteration 91/1000 | Loss: 0.00003144
Iteration 92/1000 | Loss: 0.00003144
Iteration 93/1000 | Loss: 0.00003144
Iteration 94/1000 | Loss: 0.00003144
Iteration 95/1000 | Loss: 0.00003144
Iteration 96/1000 | Loss: 0.00003144
Iteration 97/1000 | Loss: 0.00003144
Iteration 98/1000 | Loss: 0.00003143
Iteration 99/1000 | Loss: 0.00003143
Iteration 100/1000 | Loss: 0.00003143
Iteration 101/1000 | Loss: 0.00003143
Iteration 102/1000 | Loss: 0.00003143
Iteration 103/1000 | Loss: 0.00003143
Iteration 104/1000 | Loss: 0.00003143
Iteration 105/1000 | Loss: 0.00003143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [3.143340290989727e-05, 3.143340290989727e-05, 3.143340290989727e-05, 3.143340290989727e-05, 3.143340290989727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.143340290989727e-05

Optimization complete. Final v2v error: 3.9317080974578857 mm

Highest mean error: 20.955936431884766 mm for frame 10

Lowest mean error: 2.8361692428588867 mm for frame 8

Saving results

Total time: 134.3739936351776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2592/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2592/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501327
Iteration 2/25 | Loss: 0.00164609
Iteration 3/25 | Loss: 0.00138108
Iteration 4/25 | Loss: 0.00136323
Iteration 5/25 | Loss: 0.00135577
Iteration 6/25 | Loss: 0.00135385
Iteration 7/25 | Loss: 0.00135385
Iteration 8/25 | Loss: 0.00135385
Iteration 9/25 | Loss: 0.00135385
Iteration 10/25 | Loss: 0.00135385
Iteration 11/25 | Loss: 0.00135385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001353845582343638, 0.001353845582343638, 0.001353845582343638, 0.001353845582343638, 0.001353845582343638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001353845582343638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76456535
Iteration 2/25 | Loss: 0.00111112
Iteration 3/25 | Loss: 0.00111112
Iteration 4/25 | Loss: 0.00111112
Iteration 5/25 | Loss: 0.00111112
Iteration 6/25 | Loss: 0.00111112
Iteration 7/25 | Loss: 0.00111112
Iteration 8/25 | Loss: 0.00111111
Iteration 9/25 | Loss: 0.00111111
Iteration 10/25 | Loss: 0.00111111
Iteration 11/25 | Loss: 0.00111111
Iteration 12/25 | Loss: 0.00111111
Iteration 13/25 | Loss: 0.00111111
Iteration 14/25 | Loss: 0.00111111
Iteration 15/25 | Loss: 0.00111111
Iteration 16/25 | Loss: 0.00111111
Iteration 17/25 | Loss: 0.00111111
Iteration 18/25 | Loss: 0.00111111
Iteration 19/25 | Loss: 0.00111111
Iteration 20/25 | Loss: 0.00111111
Iteration 21/25 | Loss: 0.00111111
Iteration 22/25 | Loss: 0.00111111
Iteration 23/25 | Loss: 0.00111111
Iteration 24/25 | Loss: 0.00111111
Iteration 25/25 | Loss: 0.00111111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111111
Iteration 2/1000 | Loss: 0.00007419
Iteration 3/1000 | Loss: 0.00004852
Iteration 4/1000 | Loss: 0.00004264
Iteration 5/1000 | Loss: 0.00004042
Iteration 6/1000 | Loss: 0.00003934
Iteration 7/1000 | Loss: 0.00003852
Iteration 8/1000 | Loss: 0.00003802
Iteration 9/1000 | Loss: 0.00003746
Iteration 10/1000 | Loss: 0.00003699
Iteration 11/1000 | Loss: 0.00003673
Iteration 12/1000 | Loss: 0.00003641
Iteration 13/1000 | Loss: 0.00003613
Iteration 14/1000 | Loss: 0.00003590
Iteration 15/1000 | Loss: 0.00003568
Iteration 16/1000 | Loss: 0.00003549
Iteration 17/1000 | Loss: 0.00003520
Iteration 18/1000 | Loss: 0.00003499
Iteration 19/1000 | Loss: 0.00003471
Iteration 20/1000 | Loss: 0.00003444
Iteration 21/1000 | Loss: 0.00003420
Iteration 22/1000 | Loss: 0.00003405
Iteration 23/1000 | Loss: 0.00003389
Iteration 24/1000 | Loss: 0.00003386
Iteration 25/1000 | Loss: 0.00003382
Iteration 26/1000 | Loss: 0.00003382
Iteration 27/1000 | Loss: 0.00003381
Iteration 28/1000 | Loss: 0.00003377
Iteration 29/1000 | Loss: 0.00003375
Iteration 30/1000 | Loss: 0.00003374
Iteration 31/1000 | Loss: 0.00003373
Iteration 32/1000 | Loss: 0.00003373
Iteration 33/1000 | Loss: 0.00003372
Iteration 34/1000 | Loss: 0.00003372
Iteration 35/1000 | Loss: 0.00003371
Iteration 36/1000 | Loss: 0.00003371
Iteration 37/1000 | Loss: 0.00003367
Iteration 38/1000 | Loss: 0.00003366
Iteration 39/1000 | Loss: 0.00003366
Iteration 40/1000 | Loss: 0.00003365
Iteration 41/1000 | Loss: 0.00003365
Iteration 42/1000 | Loss: 0.00003365
Iteration 43/1000 | Loss: 0.00003365
Iteration 44/1000 | Loss: 0.00003365
Iteration 45/1000 | Loss: 0.00003364
Iteration 46/1000 | Loss: 0.00003364
Iteration 47/1000 | Loss: 0.00003364
Iteration 48/1000 | Loss: 0.00003364
Iteration 49/1000 | Loss: 0.00003364
Iteration 50/1000 | Loss: 0.00003363
Iteration 51/1000 | Loss: 0.00003363
Iteration 52/1000 | Loss: 0.00003363
Iteration 53/1000 | Loss: 0.00003363
Iteration 54/1000 | Loss: 0.00003363
Iteration 55/1000 | Loss: 0.00003363
Iteration 56/1000 | Loss: 0.00003360
Iteration 57/1000 | Loss: 0.00003359
Iteration 58/1000 | Loss: 0.00003359
Iteration 59/1000 | Loss: 0.00003359
Iteration 60/1000 | Loss: 0.00003359
Iteration 61/1000 | Loss: 0.00003358
Iteration 62/1000 | Loss: 0.00003357
Iteration 63/1000 | Loss: 0.00003357
Iteration 64/1000 | Loss: 0.00003356
Iteration 65/1000 | Loss: 0.00003355
Iteration 66/1000 | Loss: 0.00003355
Iteration 67/1000 | Loss: 0.00003355
Iteration 68/1000 | Loss: 0.00003355
Iteration 69/1000 | Loss: 0.00003355
Iteration 70/1000 | Loss: 0.00003355
Iteration 71/1000 | Loss: 0.00003355
Iteration 72/1000 | Loss: 0.00003355
Iteration 73/1000 | Loss: 0.00003355
Iteration 74/1000 | Loss: 0.00003355
Iteration 75/1000 | Loss: 0.00003355
Iteration 76/1000 | Loss: 0.00003355
Iteration 77/1000 | Loss: 0.00003355
Iteration 78/1000 | Loss: 0.00003355
Iteration 79/1000 | Loss: 0.00003355
Iteration 80/1000 | Loss: 0.00003354
Iteration 81/1000 | Loss: 0.00003354
Iteration 82/1000 | Loss: 0.00003354
Iteration 83/1000 | Loss: 0.00003354
Iteration 84/1000 | Loss: 0.00003354
Iteration 85/1000 | Loss: 0.00003354
Iteration 86/1000 | Loss: 0.00003353
Iteration 87/1000 | Loss: 0.00003353
Iteration 88/1000 | Loss: 0.00003353
Iteration 89/1000 | Loss: 0.00003353
Iteration 90/1000 | Loss: 0.00003353
Iteration 91/1000 | Loss: 0.00003352
Iteration 92/1000 | Loss: 0.00003352
Iteration 93/1000 | Loss: 0.00003352
Iteration 94/1000 | Loss: 0.00003352
Iteration 95/1000 | Loss: 0.00003351
Iteration 96/1000 | Loss: 0.00003351
Iteration 97/1000 | Loss: 0.00003351
Iteration 98/1000 | Loss: 0.00003351
Iteration 99/1000 | Loss: 0.00003351
Iteration 100/1000 | Loss: 0.00003351
Iteration 101/1000 | Loss: 0.00003351
Iteration 102/1000 | Loss: 0.00003351
Iteration 103/1000 | Loss: 0.00003351
Iteration 104/1000 | Loss: 0.00003351
Iteration 105/1000 | Loss: 0.00003351
Iteration 106/1000 | Loss: 0.00003351
Iteration 107/1000 | Loss: 0.00003351
Iteration 108/1000 | Loss: 0.00003350
Iteration 109/1000 | Loss: 0.00003350
Iteration 110/1000 | Loss: 0.00003350
Iteration 111/1000 | Loss: 0.00003350
Iteration 112/1000 | Loss: 0.00003349
Iteration 113/1000 | Loss: 0.00003349
Iteration 114/1000 | Loss: 0.00003349
Iteration 115/1000 | Loss: 0.00003349
Iteration 116/1000 | Loss: 0.00003349
Iteration 117/1000 | Loss: 0.00003348
Iteration 118/1000 | Loss: 0.00003348
Iteration 119/1000 | Loss: 0.00003348
Iteration 120/1000 | Loss: 0.00003348
Iteration 121/1000 | Loss: 0.00003348
Iteration 122/1000 | Loss: 0.00003348
Iteration 123/1000 | Loss: 0.00003348
Iteration 124/1000 | Loss: 0.00003348
Iteration 125/1000 | Loss: 0.00003348
Iteration 126/1000 | Loss: 0.00003348
Iteration 127/1000 | Loss: 0.00003348
Iteration 128/1000 | Loss: 0.00003347
Iteration 129/1000 | Loss: 0.00003347
Iteration 130/1000 | Loss: 0.00003347
Iteration 131/1000 | Loss: 0.00003347
Iteration 132/1000 | Loss: 0.00003347
Iteration 133/1000 | Loss: 0.00003347
Iteration 134/1000 | Loss: 0.00003347
Iteration 135/1000 | Loss: 0.00003347
Iteration 136/1000 | Loss: 0.00003347
Iteration 137/1000 | Loss: 0.00003347
Iteration 138/1000 | Loss: 0.00003347
Iteration 139/1000 | Loss: 0.00003347
Iteration 140/1000 | Loss: 0.00003347
Iteration 141/1000 | Loss: 0.00003347
Iteration 142/1000 | Loss: 0.00003346
Iteration 143/1000 | Loss: 0.00003346
Iteration 144/1000 | Loss: 0.00003346
Iteration 145/1000 | Loss: 0.00003346
Iteration 146/1000 | Loss: 0.00003346
Iteration 147/1000 | Loss: 0.00003346
Iteration 148/1000 | Loss: 0.00003346
Iteration 149/1000 | Loss: 0.00003346
Iteration 150/1000 | Loss: 0.00003346
Iteration 151/1000 | Loss: 0.00003346
Iteration 152/1000 | Loss: 0.00003346
Iteration 153/1000 | Loss: 0.00003346
Iteration 154/1000 | Loss: 0.00003346
Iteration 155/1000 | Loss: 0.00003346
Iteration 156/1000 | Loss: 0.00003346
Iteration 157/1000 | Loss: 0.00003346
Iteration 158/1000 | Loss: 0.00003346
Iteration 159/1000 | Loss: 0.00003346
Iteration 160/1000 | Loss: 0.00003346
Iteration 161/1000 | Loss: 0.00003346
Iteration 162/1000 | Loss: 0.00003346
Iteration 163/1000 | Loss: 0.00003346
Iteration 164/1000 | Loss: 0.00003346
Iteration 165/1000 | Loss: 0.00003346
Iteration 166/1000 | Loss: 0.00003346
Iteration 167/1000 | Loss: 0.00003346
Iteration 168/1000 | Loss: 0.00003346
Iteration 169/1000 | Loss: 0.00003346
Iteration 170/1000 | Loss: 0.00003346
Iteration 171/1000 | Loss: 0.00003346
Iteration 172/1000 | Loss: 0.00003346
Iteration 173/1000 | Loss: 0.00003346
Iteration 174/1000 | Loss: 0.00003346
Iteration 175/1000 | Loss: 0.00003346
Iteration 176/1000 | Loss: 0.00003346
Iteration 177/1000 | Loss: 0.00003346
Iteration 178/1000 | Loss: 0.00003346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.346375888213515e-05, 3.346375888213515e-05, 3.346375888213515e-05, 3.346375888213515e-05, 3.346375888213515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.346375888213515e-05

Optimization complete. Final v2v error: 4.744688987731934 mm

Highest mean error: 5.012608528137207 mm for frame 256

Lowest mean error: 4.568606376647949 mm for frame 239

Saving results

Total time: 61.10851430892944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869280
Iteration 2/25 | Loss: 0.00223434
Iteration 3/25 | Loss: 0.00180262
Iteration 4/25 | Loss: 0.00186838
Iteration 5/25 | Loss: 0.00179700
Iteration 6/25 | Loss: 0.00173330
Iteration 7/25 | Loss: 0.00165695
Iteration 8/25 | Loss: 0.00162359
Iteration 9/25 | Loss: 0.00162935
Iteration 10/25 | Loss: 0.00159315
Iteration 11/25 | Loss: 0.00159048
Iteration 12/25 | Loss: 0.00158322
Iteration 13/25 | Loss: 0.00158256
Iteration 14/25 | Loss: 0.00158201
Iteration 15/25 | Loss: 0.00159150
Iteration 16/25 | Loss: 0.00158011
Iteration 17/25 | Loss: 0.00157443
Iteration 18/25 | Loss: 0.00157359
Iteration 19/25 | Loss: 0.00157334
Iteration 20/25 | Loss: 0.00157223
Iteration 21/25 | Loss: 0.00157366
Iteration 22/25 | Loss: 0.00157069
Iteration 23/25 | Loss: 0.00157009
Iteration 24/25 | Loss: 0.00156993
Iteration 25/25 | Loss: 0.00156986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26209521
Iteration 2/25 | Loss: 0.00255338
Iteration 3/25 | Loss: 0.00255337
Iteration 4/25 | Loss: 0.00255337
Iteration 5/25 | Loss: 0.00255337
Iteration 6/25 | Loss: 0.00255337
Iteration 7/25 | Loss: 0.00255337
Iteration 8/25 | Loss: 0.00255337
Iteration 9/25 | Loss: 0.00255337
Iteration 10/25 | Loss: 0.00255337
Iteration 11/25 | Loss: 0.00255337
Iteration 12/25 | Loss: 0.00255337
Iteration 13/25 | Loss: 0.00255337
Iteration 14/25 | Loss: 0.00255337
Iteration 15/25 | Loss: 0.00255337
Iteration 16/25 | Loss: 0.00255337
Iteration 17/25 | Loss: 0.00255337
Iteration 18/25 | Loss: 0.00255337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0025533686857670546, 0.0025533686857670546, 0.0025533686857670546, 0.0025533686857670546, 0.0025533686857670546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025533686857670546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255337
Iteration 2/1000 | Loss: 0.00019436
Iteration 3/1000 | Loss: 0.00091855
Iteration 4/1000 | Loss: 0.00096084
Iteration 5/1000 | Loss: 0.00099327
Iteration 6/1000 | Loss: 0.00045825
Iteration 7/1000 | Loss: 0.00047264
Iteration 8/1000 | Loss: 0.00028594
Iteration 9/1000 | Loss: 0.00028592
Iteration 10/1000 | Loss: 0.00011483
Iteration 11/1000 | Loss: 0.00005852
Iteration 12/1000 | Loss: 0.00005386
Iteration 13/1000 | Loss: 0.00014212
Iteration 14/1000 | Loss: 0.00064502
Iteration 15/1000 | Loss: 0.00057828
Iteration 16/1000 | Loss: 0.00045308
Iteration 17/1000 | Loss: 0.00007978
Iteration 18/1000 | Loss: 0.00036109
Iteration 19/1000 | Loss: 0.00034046
Iteration 20/1000 | Loss: 0.00058404
Iteration 21/1000 | Loss: 0.00065087
Iteration 22/1000 | Loss: 0.00046194
Iteration 23/1000 | Loss: 0.00034580
Iteration 24/1000 | Loss: 0.00015767
Iteration 25/1000 | Loss: 0.00065283
Iteration 26/1000 | Loss: 0.00006251
Iteration 27/1000 | Loss: 0.00005040
Iteration 28/1000 | Loss: 0.00004605
Iteration 29/1000 | Loss: 0.00004395
Iteration 30/1000 | Loss: 0.00004298
Iteration 31/1000 | Loss: 0.00004194
Iteration 32/1000 | Loss: 0.00004107
Iteration 33/1000 | Loss: 0.00025415
Iteration 34/1000 | Loss: 0.00013064
Iteration 35/1000 | Loss: 0.00035765
Iteration 36/1000 | Loss: 0.00004683
Iteration 37/1000 | Loss: 0.00044736
Iteration 38/1000 | Loss: 0.00015339
Iteration 39/1000 | Loss: 0.00004114
Iteration 40/1000 | Loss: 0.00016365
Iteration 41/1000 | Loss: 0.00008217
Iteration 42/1000 | Loss: 0.00013795
Iteration 43/1000 | Loss: 0.00034124
Iteration 44/1000 | Loss: 0.00030313
Iteration 45/1000 | Loss: 0.00016544
Iteration 46/1000 | Loss: 0.00027811
Iteration 47/1000 | Loss: 0.00005811
Iteration 48/1000 | Loss: 0.00058716
Iteration 49/1000 | Loss: 0.00065331
Iteration 50/1000 | Loss: 0.00070967
Iteration 51/1000 | Loss: 0.00025685
Iteration 52/1000 | Loss: 0.00022226
Iteration 53/1000 | Loss: 0.00018416
Iteration 54/1000 | Loss: 0.00040437
Iteration 55/1000 | Loss: 0.00015372
Iteration 56/1000 | Loss: 0.00004582
Iteration 57/1000 | Loss: 0.00004049
Iteration 58/1000 | Loss: 0.00003855
Iteration 59/1000 | Loss: 0.00003753
Iteration 60/1000 | Loss: 0.00003698
Iteration 61/1000 | Loss: 0.00003672
Iteration 62/1000 | Loss: 0.00003662
Iteration 63/1000 | Loss: 0.00003638
Iteration 64/1000 | Loss: 0.00003635
Iteration 65/1000 | Loss: 0.00003634
Iteration 66/1000 | Loss: 0.00003629
Iteration 67/1000 | Loss: 0.00003627
Iteration 68/1000 | Loss: 0.00003614
Iteration 69/1000 | Loss: 0.00003609
Iteration 70/1000 | Loss: 0.00003607
Iteration 71/1000 | Loss: 0.00003607
Iteration 72/1000 | Loss: 0.00003606
Iteration 73/1000 | Loss: 0.00003606
Iteration 74/1000 | Loss: 0.00003604
Iteration 75/1000 | Loss: 0.00003603
Iteration 76/1000 | Loss: 0.00003603
Iteration 77/1000 | Loss: 0.00003602
Iteration 78/1000 | Loss: 0.00003598
Iteration 79/1000 | Loss: 0.00003595
Iteration 80/1000 | Loss: 0.00003595
Iteration 81/1000 | Loss: 0.00003595
Iteration 82/1000 | Loss: 0.00003595
Iteration 83/1000 | Loss: 0.00003595
Iteration 84/1000 | Loss: 0.00003594
Iteration 85/1000 | Loss: 0.00003594
Iteration 86/1000 | Loss: 0.00003594
Iteration 87/1000 | Loss: 0.00003593
Iteration 88/1000 | Loss: 0.00003593
Iteration 89/1000 | Loss: 0.00003593
Iteration 90/1000 | Loss: 0.00003592
Iteration 91/1000 | Loss: 0.00003592
Iteration 92/1000 | Loss: 0.00003591
Iteration 93/1000 | Loss: 0.00003591
Iteration 94/1000 | Loss: 0.00003591
Iteration 95/1000 | Loss: 0.00003591
Iteration 96/1000 | Loss: 0.00003591
Iteration 97/1000 | Loss: 0.00003591
Iteration 98/1000 | Loss: 0.00003590
Iteration 99/1000 | Loss: 0.00003590
Iteration 100/1000 | Loss: 0.00003590
Iteration 101/1000 | Loss: 0.00003590
Iteration 102/1000 | Loss: 0.00003589
Iteration 103/1000 | Loss: 0.00003589
Iteration 104/1000 | Loss: 0.00003589
Iteration 105/1000 | Loss: 0.00003589
Iteration 106/1000 | Loss: 0.00042320
Iteration 107/1000 | Loss: 0.00012724
Iteration 108/1000 | Loss: 0.00003658
Iteration 109/1000 | Loss: 0.00003600
Iteration 110/1000 | Loss: 0.00003592
Iteration 111/1000 | Loss: 0.00003591
Iteration 112/1000 | Loss: 0.00003590
Iteration 113/1000 | Loss: 0.00003589
Iteration 114/1000 | Loss: 0.00003589
Iteration 115/1000 | Loss: 0.00003589
Iteration 116/1000 | Loss: 0.00003588
Iteration 117/1000 | Loss: 0.00038585
Iteration 118/1000 | Loss: 0.00008627
Iteration 119/1000 | Loss: 0.00003651
Iteration 120/1000 | Loss: 0.00003600
Iteration 121/1000 | Loss: 0.00003590
Iteration 122/1000 | Loss: 0.00003590
Iteration 123/1000 | Loss: 0.00003590
Iteration 124/1000 | Loss: 0.00003589
Iteration 125/1000 | Loss: 0.00003589
Iteration 126/1000 | Loss: 0.00003589
Iteration 127/1000 | Loss: 0.00003589
Iteration 128/1000 | Loss: 0.00003589
Iteration 129/1000 | Loss: 0.00003589
Iteration 130/1000 | Loss: 0.00003588
Iteration 131/1000 | Loss: 0.00003588
Iteration 132/1000 | Loss: 0.00003587
Iteration 133/1000 | Loss: 0.00003587
Iteration 134/1000 | Loss: 0.00003587
Iteration 135/1000 | Loss: 0.00003587
Iteration 136/1000 | Loss: 0.00003587
Iteration 137/1000 | Loss: 0.00003587
Iteration 138/1000 | Loss: 0.00003587
Iteration 139/1000 | Loss: 0.00003587
Iteration 140/1000 | Loss: 0.00003587
Iteration 141/1000 | Loss: 0.00003587
Iteration 142/1000 | Loss: 0.00003586
Iteration 143/1000 | Loss: 0.00003586
Iteration 144/1000 | Loss: 0.00003586
Iteration 145/1000 | Loss: 0.00003585
Iteration 146/1000 | Loss: 0.00003584
Iteration 147/1000 | Loss: 0.00003584
Iteration 148/1000 | Loss: 0.00003584
Iteration 149/1000 | Loss: 0.00003583
Iteration 150/1000 | Loss: 0.00003583
Iteration 151/1000 | Loss: 0.00003583
Iteration 152/1000 | Loss: 0.00003583
Iteration 153/1000 | Loss: 0.00003583
Iteration 154/1000 | Loss: 0.00003583
Iteration 155/1000 | Loss: 0.00003582
Iteration 156/1000 | Loss: 0.00003582
Iteration 157/1000 | Loss: 0.00003582
Iteration 158/1000 | Loss: 0.00003581
Iteration 159/1000 | Loss: 0.00003581
Iteration 160/1000 | Loss: 0.00003581
Iteration 161/1000 | Loss: 0.00003580
Iteration 162/1000 | Loss: 0.00003580
Iteration 163/1000 | Loss: 0.00003580
Iteration 164/1000 | Loss: 0.00003579
Iteration 165/1000 | Loss: 0.00003579
Iteration 166/1000 | Loss: 0.00003579
Iteration 167/1000 | Loss: 0.00003578
Iteration 168/1000 | Loss: 0.00003578
Iteration 169/1000 | Loss: 0.00003578
Iteration 170/1000 | Loss: 0.00003577
Iteration 171/1000 | Loss: 0.00003577
Iteration 172/1000 | Loss: 0.00003577
Iteration 173/1000 | Loss: 0.00003577
Iteration 174/1000 | Loss: 0.00003577
Iteration 175/1000 | Loss: 0.00003577
Iteration 176/1000 | Loss: 0.00003577
Iteration 177/1000 | Loss: 0.00003576
Iteration 178/1000 | Loss: 0.00003576
Iteration 179/1000 | Loss: 0.00042978
Iteration 180/1000 | Loss: 0.00004032
Iteration 181/1000 | Loss: 0.00003652
Iteration 182/1000 | Loss: 0.00003526
Iteration 183/1000 | Loss: 0.00003440
Iteration 184/1000 | Loss: 0.00003392
Iteration 185/1000 | Loss: 0.00003363
Iteration 186/1000 | Loss: 0.00003350
Iteration 187/1000 | Loss: 0.00003341
Iteration 188/1000 | Loss: 0.00003339
Iteration 189/1000 | Loss: 0.00003336
Iteration 190/1000 | Loss: 0.00003335
Iteration 191/1000 | Loss: 0.00003335
Iteration 192/1000 | Loss: 0.00003329
Iteration 193/1000 | Loss: 0.00003327
Iteration 194/1000 | Loss: 0.00003326
Iteration 195/1000 | Loss: 0.00003325
Iteration 196/1000 | Loss: 0.00003325
Iteration 197/1000 | Loss: 0.00003318
Iteration 198/1000 | Loss: 0.00003317
Iteration 199/1000 | Loss: 0.00003316
Iteration 200/1000 | Loss: 0.00003316
Iteration 201/1000 | Loss: 0.00003315
Iteration 202/1000 | Loss: 0.00003315
Iteration 203/1000 | Loss: 0.00003314
Iteration 204/1000 | Loss: 0.00003314
Iteration 205/1000 | Loss: 0.00003313
Iteration 206/1000 | Loss: 0.00003313
Iteration 207/1000 | Loss: 0.00003313
Iteration 208/1000 | Loss: 0.00003313
Iteration 209/1000 | Loss: 0.00003313
Iteration 210/1000 | Loss: 0.00003313
Iteration 211/1000 | Loss: 0.00003313
Iteration 212/1000 | Loss: 0.00003313
Iteration 213/1000 | Loss: 0.00003313
Iteration 214/1000 | Loss: 0.00003312
Iteration 215/1000 | Loss: 0.00003311
Iteration 216/1000 | Loss: 0.00003311
Iteration 217/1000 | Loss: 0.00003311
Iteration 218/1000 | Loss: 0.00003311
Iteration 219/1000 | Loss: 0.00003310
Iteration 220/1000 | Loss: 0.00003310
Iteration 221/1000 | Loss: 0.00003310
Iteration 222/1000 | Loss: 0.00003309
Iteration 223/1000 | Loss: 0.00003309
Iteration 224/1000 | Loss: 0.00003309
Iteration 225/1000 | Loss: 0.00003309
Iteration 226/1000 | Loss: 0.00003309
Iteration 227/1000 | Loss: 0.00003309
Iteration 228/1000 | Loss: 0.00003308
Iteration 229/1000 | Loss: 0.00003308
Iteration 230/1000 | Loss: 0.00003308
Iteration 231/1000 | Loss: 0.00003308
Iteration 232/1000 | Loss: 0.00003308
Iteration 233/1000 | Loss: 0.00003308
Iteration 234/1000 | Loss: 0.00003308
Iteration 235/1000 | Loss: 0.00003307
Iteration 236/1000 | Loss: 0.00003307
Iteration 237/1000 | Loss: 0.00003307
Iteration 238/1000 | Loss: 0.00003307
Iteration 239/1000 | Loss: 0.00003307
Iteration 240/1000 | Loss: 0.00003307
Iteration 241/1000 | Loss: 0.00003307
Iteration 242/1000 | Loss: 0.00003307
Iteration 243/1000 | Loss: 0.00003307
Iteration 244/1000 | Loss: 0.00003306
Iteration 245/1000 | Loss: 0.00003306
Iteration 246/1000 | Loss: 0.00003306
Iteration 247/1000 | Loss: 0.00003306
Iteration 248/1000 | Loss: 0.00003306
Iteration 249/1000 | Loss: 0.00003306
Iteration 250/1000 | Loss: 0.00003306
Iteration 251/1000 | Loss: 0.00003306
Iteration 252/1000 | Loss: 0.00003306
Iteration 253/1000 | Loss: 0.00003306
Iteration 254/1000 | Loss: 0.00003306
Iteration 255/1000 | Loss: 0.00003306
Iteration 256/1000 | Loss: 0.00003306
Iteration 257/1000 | Loss: 0.00003306
Iteration 258/1000 | Loss: 0.00003306
Iteration 259/1000 | Loss: 0.00003306
Iteration 260/1000 | Loss: 0.00003306
Iteration 261/1000 | Loss: 0.00003306
Iteration 262/1000 | Loss: 0.00003306
Iteration 263/1000 | Loss: 0.00003306
Iteration 264/1000 | Loss: 0.00003306
Iteration 265/1000 | Loss: 0.00003306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [3.305576683487743e-05, 3.305576683487743e-05, 3.305576683487743e-05, 3.305576683487743e-05, 3.305576683487743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.305576683487743e-05

Optimization complete. Final v2v error: 4.908121585845947 mm

Highest mean error: 5.568812370300293 mm for frame 30

Lowest mean error: 4.382169723510742 mm for frame 99

Saving results

Total time: 192.93807530403137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831150
Iteration 2/25 | Loss: 0.00166725
Iteration 3/25 | Loss: 0.00154734
Iteration 4/25 | Loss: 0.00153335
Iteration 5/25 | Loss: 0.00152919
Iteration 6/25 | Loss: 0.00152866
Iteration 7/25 | Loss: 0.00152866
Iteration 8/25 | Loss: 0.00152866
Iteration 9/25 | Loss: 0.00152866
Iteration 10/25 | Loss: 0.00152866
Iteration 11/25 | Loss: 0.00152866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015286605339497328, 0.0015286605339497328, 0.0015286605339497328, 0.0015286605339497328, 0.0015286605339497328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015286605339497328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14366269
Iteration 2/25 | Loss: 0.00355431
Iteration 3/25 | Loss: 0.00355431
Iteration 4/25 | Loss: 0.00355431
Iteration 5/25 | Loss: 0.00355431
Iteration 6/25 | Loss: 0.00355431
Iteration 7/25 | Loss: 0.00355431
Iteration 8/25 | Loss: 0.00355431
Iteration 9/25 | Loss: 0.00355431
Iteration 10/25 | Loss: 0.00355431
Iteration 11/25 | Loss: 0.00355431
Iteration 12/25 | Loss: 0.00355431
Iteration 13/25 | Loss: 0.00355431
Iteration 14/25 | Loss: 0.00355431
Iteration 15/25 | Loss: 0.00355431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003554306458681822, 0.003554306458681822, 0.003554306458681822, 0.003554306458681822, 0.003554306458681822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003554306458681822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00355431
Iteration 2/1000 | Loss: 0.00007673
Iteration 3/1000 | Loss: 0.00005444
Iteration 4/1000 | Loss: 0.00004450
Iteration 5/1000 | Loss: 0.00003898
Iteration 6/1000 | Loss: 0.00003508
Iteration 7/1000 | Loss: 0.00003330
Iteration 8/1000 | Loss: 0.00003239
Iteration 9/1000 | Loss: 0.00003158
Iteration 10/1000 | Loss: 0.00003083
Iteration 11/1000 | Loss: 0.00003026
Iteration 12/1000 | Loss: 0.00002981
Iteration 13/1000 | Loss: 0.00002958
Iteration 14/1000 | Loss: 0.00002949
Iteration 15/1000 | Loss: 0.00002935
Iteration 16/1000 | Loss: 0.00002931
Iteration 17/1000 | Loss: 0.00002931
Iteration 18/1000 | Loss: 0.00002928
Iteration 19/1000 | Loss: 0.00002925
Iteration 20/1000 | Loss: 0.00002924
Iteration 21/1000 | Loss: 0.00002923
Iteration 22/1000 | Loss: 0.00002923
Iteration 23/1000 | Loss: 0.00002923
Iteration 24/1000 | Loss: 0.00002922
Iteration 25/1000 | Loss: 0.00002922
Iteration 26/1000 | Loss: 0.00002921
Iteration 27/1000 | Loss: 0.00002920
Iteration 28/1000 | Loss: 0.00002920
Iteration 29/1000 | Loss: 0.00002919
Iteration 30/1000 | Loss: 0.00002916
Iteration 31/1000 | Loss: 0.00002915
Iteration 32/1000 | Loss: 0.00002915
Iteration 33/1000 | Loss: 0.00002914
Iteration 34/1000 | Loss: 0.00002913
Iteration 35/1000 | Loss: 0.00002913
Iteration 36/1000 | Loss: 0.00002912
Iteration 37/1000 | Loss: 0.00002912
Iteration 38/1000 | Loss: 0.00002911
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00002911
Iteration 41/1000 | Loss: 0.00002910
Iteration 42/1000 | Loss: 0.00002909
Iteration 43/1000 | Loss: 0.00002909
Iteration 44/1000 | Loss: 0.00002908
Iteration 45/1000 | Loss: 0.00002908
Iteration 46/1000 | Loss: 0.00002908
Iteration 47/1000 | Loss: 0.00002908
Iteration 48/1000 | Loss: 0.00002908
Iteration 49/1000 | Loss: 0.00002908
Iteration 50/1000 | Loss: 0.00002908
Iteration 51/1000 | Loss: 0.00002907
Iteration 52/1000 | Loss: 0.00002907
Iteration 53/1000 | Loss: 0.00002907
Iteration 54/1000 | Loss: 0.00002907
Iteration 55/1000 | Loss: 0.00002907
Iteration 56/1000 | Loss: 0.00002907
Iteration 57/1000 | Loss: 0.00002907
Iteration 58/1000 | Loss: 0.00002907
Iteration 59/1000 | Loss: 0.00002907
Iteration 60/1000 | Loss: 0.00002906
Iteration 61/1000 | Loss: 0.00002906
Iteration 62/1000 | Loss: 0.00002906
Iteration 63/1000 | Loss: 0.00002905
Iteration 64/1000 | Loss: 0.00002905
Iteration 65/1000 | Loss: 0.00002904
Iteration 66/1000 | Loss: 0.00002904
Iteration 67/1000 | Loss: 0.00002904
Iteration 68/1000 | Loss: 0.00002904
Iteration 69/1000 | Loss: 0.00002904
Iteration 70/1000 | Loss: 0.00002904
Iteration 71/1000 | Loss: 0.00002904
Iteration 72/1000 | Loss: 0.00002904
Iteration 73/1000 | Loss: 0.00002904
Iteration 74/1000 | Loss: 0.00002904
Iteration 75/1000 | Loss: 0.00002903
Iteration 76/1000 | Loss: 0.00002903
Iteration 77/1000 | Loss: 0.00002903
Iteration 78/1000 | Loss: 0.00002903
Iteration 79/1000 | Loss: 0.00002903
Iteration 80/1000 | Loss: 0.00002903
Iteration 81/1000 | Loss: 0.00002902
Iteration 82/1000 | Loss: 0.00002902
Iteration 83/1000 | Loss: 0.00002902
Iteration 84/1000 | Loss: 0.00002902
Iteration 85/1000 | Loss: 0.00002902
Iteration 86/1000 | Loss: 0.00002902
Iteration 87/1000 | Loss: 0.00002902
Iteration 88/1000 | Loss: 0.00002901
Iteration 89/1000 | Loss: 0.00002901
Iteration 90/1000 | Loss: 0.00002901
Iteration 91/1000 | Loss: 0.00002901
Iteration 92/1000 | Loss: 0.00002901
Iteration 93/1000 | Loss: 0.00002901
Iteration 94/1000 | Loss: 0.00002901
Iteration 95/1000 | Loss: 0.00002901
Iteration 96/1000 | Loss: 0.00002901
Iteration 97/1000 | Loss: 0.00002901
Iteration 98/1000 | Loss: 0.00002901
Iteration 99/1000 | Loss: 0.00002901
Iteration 100/1000 | Loss: 0.00002901
Iteration 101/1000 | Loss: 0.00002901
Iteration 102/1000 | Loss: 0.00002900
Iteration 103/1000 | Loss: 0.00002900
Iteration 104/1000 | Loss: 0.00002900
Iteration 105/1000 | Loss: 0.00002900
Iteration 106/1000 | Loss: 0.00002900
Iteration 107/1000 | Loss: 0.00002900
Iteration 108/1000 | Loss: 0.00002900
Iteration 109/1000 | Loss: 0.00002900
Iteration 110/1000 | Loss: 0.00002900
Iteration 111/1000 | Loss: 0.00002899
Iteration 112/1000 | Loss: 0.00002899
Iteration 113/1000 | Loss: 0.00002899
Iteration 114/1000 | Loss: 0.00002899
Iteration 115/1000 | Loss: 0.00002899
Iteration 116/1000 | Loss: 0.00002899
Iteration 117/1000 | Loss: 0.00002899
Iteration 118/1000 | Loss: 0.00002898
Iteration 119/1000 | Loss: 0.00002898
Iteration 120/1000 | Loss: 0.00002898
Iteration 121/1000 | Loss: 0.00002898
Iteration 122/1000 | Loss: 0.00002898
Iteration 123/1000 | Loss: 0.00002898
Iteration 124/1000 | Loss: 0.00002898
Iteration 125/1000 | Loss: 0.00002898
Iteration 126/1000 | Loss: 0.00002898
Iteration 127/1000 | Loss: 0.00002898
Iteration 128/1000 | Loss: 0.00002898
Iteration 129/1000 | Loss: 0.00002898
Iteration 130/1000 | Loss: 0.00002898
Iteration 131/1000 | Loss: 0.00002898
Iteration 132/1000 | Loss: 0.00002898
Iteration 133/1000 | Loss: 0.00002898
Iteration 134/1000 | Loss: 0.00002898
Iteration 135/1000 | Loss: 0.00002898
Iteration 136/1000 | Loss: 0.00002897
Iteration 137/1000 | Loss: 0.00002897
Iteration 138/1000 | Loss: 0.00002897
Iteration 139/1000 | Loss: 0.00002897
Iteration 140/1000 | Loss: 0.00002897
Iteration 141/1000 | Loss: 0.00002897
Iteration 142/1000 | Loss: 0.00002897
Iteration 143/1000 | Loss: 0.00002897
Iteration 144/1000 | Loss: 0.00002897
Iteration 145/1000 | Loss: 0.00002897
Iteration 146/1000 | Loss: 0.00002897
Iteration 147/1000 | Loss: 0.00002897
Iteration 148/1000 | Loss: 0.00002897
Iteration 149/1000 | Loss: 0.00002897
Iteration 150/1000 | Loss: 0.00002897
Iteration 151/1000 | Loss: 0.00002897
Iteration 152/1000 | Loss: 0.00002897
Iteration 153/1000 | Loss: 0.00002897
Iteration 154/1000 | Loss: 0.00002897
Iteration 155/1000 | Loss: 0.00002897
Iteration 156/1000 | Loss: 0.00002897
Iteration 157/1000 | Loss: 0.00002897
Iteration 158/1000 | Loss: 0.00002897
Iteration 159/1000 | Loss: 0.00002897
Iteration 160/1000 | Loss: 0.00002897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.8974207452847622e-05, 2.8974207452847622e-05, 2.8974207452847622e-05, 2.8974207452847622e-05, 2.8974207452847622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8974207452847622e-05

Optimization complete. Final v2v error: 4.578164577484131 mm

Highest mean error: 4.873974323272705 mm for frame 239

Lowest mean error: 4.254086017608643 mm for frame 14

Saving results

Total time: 42.55947494506836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908761
Iteration 2/25 | Loss: 0.00172417
Iteration 3/25 | Loss: 0.00156983
Iteration 4/25 | Loss: 0.00154726
Iteration 5/25 | Loss: 0.00154076
Iteration 6/25 | Loss: 0.00153995
Iteration 7/25 | Loss: 0.00153995
Iteration 8/25 | Loss: 0.00153995
Iteration 9/25 | Loss: 0.00153995
Iteration 10/25 | Loss: 0.00153995
Iteration 11/25 | Loss: 0.00153995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015399542171508074, 0.0015399542171508074, 0.0015399542171508074, 0.0015399542171508074, 0.0015399542171508074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015399542171508074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17265165
Iteration 2/25 | Loss: 0.00269773
Iteration 3/25 | Loss: 0.00269769
Iteration 4/25 | Loss: 0.00269769
Iteration 5/25 | Loss: 0.00269769
Iteration 6/25 | Loss: 0.00269769
Iteration 7/25 | Loss: 0.00269769
Iteration 8/25 | Loss: 0.00269769
Iteration 9/25 | Loss: 0.00269769
Iteration 10/25 | Loss: 0.00269769
Iteration 11/25 | Loss: 0.00269769
Iteration 12/25 | Loss: 0.00269769
Iteration 13/25 | Loss: 0.00269769
Iteration 14/25 | Loss: 0.00269769
Iteration 15/25 | Loss: 0.00269769
Iteration 16/25 | Loss: 0.00269769
Iteration 17/25 | Loss: 0.00269769
Iteration 18/25 | Loss: 0.00269769
Iteration 19/25 | Loss: 0.00269769
Iteration 20/25 | Loss: 0.00269769
Iteration 21/25 | Loss: 0.00269769
Iteration 22/25 | Loss: 0.00269769
Iteration 23/25 | Loss: 0.00269769
Iteration 24/25 | Loss: 0.00269769
Iteration 25/25 | Loss: 0.00269769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269769
Iteration 2/1000 | Loss: 0.00011461
Iteration 3/1000 | Loss: 0.00006233
Iteration 4/1000 | Loss: 0.00004937
Iteration 5/1000 | Loss: 0.00004322
Iteration 6/1000 | Loss: 0.00003984
Iteration 7/1000 | Loss: 0.00003776
Iteration 8/1000 | Loss: 0.00003641
Iteration 9/1000 | Loss: 0.00003519
Iteration 10/1000 | Loss: 0.00003429
Iteration 11/1000 | Loss: 0.00003367
Iteration 12/1000 | Loss: 0.00003323
Iteration 13/1000 | Loss: 0.00003300
Iteration 14/1000 | Loss: 0.00003284
Iteration 15/1000 | Loss: 0.00003276
Iteration 16/1000 | Loss: 0.00003271
Iteration 17/1000 | Loss: 0.00003270
Iteration 18/1000 | Loss: 0.00003269
Iteration 19/1000 | Loss: 0.00003268
Iteration 20/1000 | Loss: 0.00003268
Iteration 21/1000 | Loss: 0.00003266
Iteration 22/1000 | Loss: 0.00003263
Iteration 23/1000 | Loss: 0.00003256
Iteration 24/1000 | Loss: 0.00003255
Iteration 25/1000 | Loss: 0.00003254
Iteration 26/1000 | Loss: 0.00003254
Iteration 27/1000 | Loss: 0.00003253
Iteration 28/1000 | Loss: 0.00003253
Iteration 29/1000 | Loss: 0.00003252
Iteration 30/1000 | Loss: 0.00003251
Iteration 31/1000 | Loss: 0.00003250
Iteration 32/1000 | Loss: 0.00003250
Iteration 33/1000 | Loss: 0.00003250
Iteration 34/1000 | Loss: 0.00003249
Iteration 35/1000 | Loss: 0.00003249
Iteration 36/1000 | Loss: 0.00003248
Iteration 37/1000 | Loss: 0.00003247
Iteration 38/1000 | Loss: 0.00003247
Iteration 39/1000 | Loss: 0.00003246
Iteration 40/1000 | Loss: 0.00003246
Iteration 41/1000 | Loss: 0.00003246
Iteration 42/1000 | Loss: 0.00003245
Iteration 43/1000 | Loss: 0.00003244
Iteration 44/1000 | Loss: 0.00003244
Iteration 45/1000 | Loss: 0.00003244
Iteration 46/1000 | Loss: 0.00003244
Iteration 47/1000 | Loss: 0.00003244
Iteration 48/1000 | Loss: 0.00003244
Iteration 49/1000 | Loss: 0.00003244
Iteration 50/1000 | Loss: 0.00003244
Iteration 51/1000 | Loss: 0.00003244
Iteration 52/1000 | Loss: 0.00003244
Iteration 53/1000 | Loss: 0.00003244
Iteration 54/1000 | Loss: 0.00003244
Iteration 55/1000 | Loss: 0.00003244
Iteration 56/1000 | Loss: 0.00003244
Iteration 57/1000 | Loss: 0.00003244
Iteration 58/1000 | Loss: 0.00003244
Iteration 59/1000 | Loss: 0.00003244
Iteration 60/1000 | Loss: 0.00003244
Iteration 61/1000 | Loss: 0.00003244
Iteration 62/1000 | Loss: 0.00003244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [3.244090476073325e-05, 3.244090476073325e-05, 3.244090476073325e-05, 3.244090476073325e-05, 3.244090476073325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.244090476073325e-05

Optimization complete. Final v2v error: 4.772500991821289 mm

Highest mean error: 5.499658107757568 mm for frame 63

Lowest mean error: 4.435458183288574 mm for frame 18

Saving results

Total time: 37.63080143928528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975563
Iteration 2/25 | Loss: 0.00190130
Iteration 3/25 | Loss: 0.00160063
Iteration 4/25 | Loss: 0.00157074
Iteration 5/25 | Loss: 0.00156664
Iteration 6/25 | Loss: 0.00156578
Iteration 7/25 | Loss: 0.00156578
Iteration 8/25 | Loss: 0.00156578
Iteration 9/25 | Loss: 0.00156578
Iteration 10/25 | Loss: 0.00156578
Iteration 11/25 | Loss: 0.00156578
Iteration 12/25 | Loss: 0.00156578
Iteration 13/25 | Loss: 0.00156578
Iteration 14/25 | Loss: 0.00156578
Iteration 15/25 | Loss: 0.00156578
Iteration 16/25 | Loss: 0.00156578
Iteration 17/25 | Loss: 0.00156578
Iteration 18/25 | Loss: 0.00156578
Iteration 19/25 | Loss: 0.00156578
Iteration 20/25 | Loss: 0.00156578
Iteration 21/25 | Loss: 0.00156578
Iteration 22/25 | Loss: 0.00156578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015657765325158834, 0.0015657765325158834, 0.0015657765325158834, 0.0015657765325158834, 0.0015657765325158834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015657765325158834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.49903941
Iteration 2/25 | Loss: 0.00157586
Iteration 3/25 | Loss: 0.00157586
Iteration 4/25 | Loss: 0.00157586
Iteration 5/25 | Loss: 0.00157586
Iteration 6/25 | Loss: 0.00157586
Iteration 7/25 | Loss: 0.00157586
Iteration 8/25 | Loss: 0.00157586
Iteration 9/25 | Loss: 0.00157586
Iteration 10/25 | Loss: 0.00157586
Iteration 11/25 | Loss: 0.00157586
Iteration 12/25 | Loss: 0.00157586
Iteration 13/25 | Loss: 0.00157586
Iteration 14/25 | Loss: 0.00157586
Iteration 15/25 | Loss: 0.00157586
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015758572844788432, 0.0015758572844788432, 0.0015758572844788432, 0.0015758572844788432, 0.0015758572844788432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015758572844788432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157586
Iteration 2/1000 | Loss: 0.00010331
Iteration 3/1000 | Loss: 0.00006140
Iteration 4/1000 | Loss: 0.00005085
Iteration 5/1000 | Loss: 0.00004531
Iteration 6/1000 | Loss: 0.00004283
Iteration 7/1000 | Loss: 0.00004054
Iteration 8/1000 | Loss: 0.00003925
Iteration 9/1000 | Loss: 0.00003828
Iteration 10/1000 | Loss: 0.00003786
Iteration 11/1000 | Loss: 0.00003747
Iteration 12/1000 | Loss: 0.00003720
Iteration 13/1000 | Loss: 0.00003703
Iteration 14/1000 | Loss: 0.00003701
Iteration 15/1000 | Loss: 0.00003700
Iteration 16/1000 | Loss: 0.00003694
Iteration 17/1000 | Loss: 0.00003691
Iteration 18/1000 | Loss: 0.00003691
Iteration 19/1000 | Loss: 0.00003691
Iteration 20/1000 | Loss: 0.00003690
Iteration 21/1000 | Loss: 0.00003689
Iteration 22/1000 | Loss: 0.00003687
Iteration 23/1000 | Loss: 0.00003687
Iteration 24/1000 | Loss: 0.00003686
Iteration 25/1000 | Loss: 0.00003684
Iteration 26/1000 | Loss: 0.00003683
Iteration 27/1000 | Loss: 0.00003674
Iteration 28/1000 | Loss: 0.00003673
Iteration 29/1000 | Loss: 0.00003672
Iteration 30/1000 | Loss: 0.00003671
Iteration 31/1000 | Loss: 0.00003670
Iteration 32/1000 | Loss: 0.00003669
Iteration 33/1000 | Loss: 0.00003669
Iteration 34/1000 | Loss: 0.00003668
Iteration 35/1000 | Loss: 0.00003668
Iteration 36/1000 | Loss: 0.00003668
Iteration 37/1000 | Loss: 0.00003668
Iteration 38/1000 | Loss: 0.00003667
Iteration 39/1000 | Loss: 0.00003667
Iteration 40/1000 | Loss: 0.00003667
Iteration 41/1000 | Loss: 0.00003666
Iteration 42/1000 | Loss: 0.00003666
Iteration 43/1000 | Loss: 0.00003666
Iteration 44/1000 | Loss: 0.00003666
Iteration 45/1000 | Loss: 0.00003666
Iteration 46/1000 | Loss: 0.00003666
Iteration 47/1000 | Loss: 0.00003666
Iteration 48/1000 | Loss: 0.00003665
Iteration 49/1000 | Loss: 0.00003665
Iteration 50/1000 | Loss: 0.00003665
Iteration 51/1000 | Loss: 0.00003665
Iteration 52/1000 | Loss: 0.00003665
Iteration 53/1000 | Loss: 0.00003665
Iteration 54/1000 | Loss: 0.00003665
Iteration 55/1000 | Loss: 0.00003664
Iteration 56/1000 | Loss: 0.00003664
Iteration 57/1000 | Loss: 0.00003664
Iteration 58/1000 | Loss: 0.00003664
Iteration 59/1000 | Loss: 0.00003663
Iteration 60/1000 | Loss: 0.00003663
Iteration 61/1000 | Loss: 0.00003663
Iteration 62/1000 | Loss: 0.00003663
Iteration 63/1000 | Loss: 0.00003662
Iteration 64/1000 | Loss: 0.00003662
Iteration 65/1000 | Loss: 0.00003662
Iteration 66/1000 | Loss: 0.00003662
Iteration 67/1000 | Loss: 0.00003662
Iteration 68/1000 | Loss: 0.00003661
Iteration 69/1000 | Loss: 0.00003661
Iteration 70/1000 | Loss: 0.00003661
Iteration 71/1000 | Loss: 0.00003661
Iteration 72/1000 | Loss: 0.00003661
Iteration 73/1000 | Loss: 0.00003661
Iteration 74/1000 | Loss: 0.00003660
Iteration 75/1000 | Loss: 0.00003660
Iteration 76/1000 | Loss: 0.00003660
Iteration 77/1000 | Loss: 0.00003660
Iteration 78/1000 | Loss: 0.00003660
Iteration 79/1000 | Loss: 0.00003660
Iteration 80/1000 | Loss: 0.00003660
Iteration 81/1000 | Loss: 0.00003660
Iteration 82/1000 | Loss: 0.00003660
Iteration 83/1000 | Loss: 0.00003660
Iteration 84/1000 | Loss: 0.00003660
Iteration 85/1000 | Loss: 0.00003660
Iteration 86/1000 | Loss: 0.00003660
Iteration 87/1000 | Loss: 0.00003660
Iteration 88/1000 | Loss: 0.00003660
Iteration 89/1000 | Loss: 0.00003660
Iteration 90/1000 | Loss: 0.00003660
Iteration 91/1000 | Loss: 0.00003660
Iteration 92/1000 | Loss: 0.00003660
Iteration 93/1000 | Loss: 0.00003660
Iteration 94/1000 | Loss: 0.00003660
Iteration 95/1000 | Loss: 0.00003660
Iteration 96/1000 | Loss: 0.00003660
Iteration 97/1000 | Loss: 0.00003660
Iteration 98/1000 | Loss: 0.00003660
Iteration 99/1000 | Loss: 0.00003660
Iteration 100/1000 | Loss: 0.00003660
Iteration 101/1000 | Loss: 0.00003660
Iteration 102/1000 | Loss: 0.00003660
Iteration 103/1000 | Loss: 0.00003660
Iteration 104/1000 | Loss: 0.00003660
Iteration 105/1000 | Loss: 0.00003660
Iteration 106/1000 | Loss: 0.00003660
Iteration 107/1000 | Loss: 0.00003660
Iteration 108/1000 | Loss: 0.00003660
Iteration 109/1000 | Loss: 0.00003660
Iteration 110/1000 | Loss: 0.00003660
Iteration 111/1000 | Loss: 0.00003660
Iteration 112/1000 | Loss: 0.00003660
Iteration 113/1000 | Loss: 0.00003660
Iteration 114/1000 | Loss: 0.00003660
Iteration 115/1000 | Loss: 0.00003660
Iteration 116/1000 | Loss: 0.00003660
Iteration 117/1000 | Loss: 0.00003660
Iteration 118/1000 | Loss: 0.00003660
Iteration 119/1000 | Loss: 0.00003660
Iteration 120/1000 | Loss: 0.00003660
Iteration 121/1000 | Loss: 0.00003660
Iteration 122/1000 | Loss: 0.00003660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [3.660173388198018e-05, 3.660173388198018e-05, 3.660173388198018e-05, 3.660173388198018e-05, 3.660173388198018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.660173388198018e-05

Optimization complete. Final v2v error: 5.2197418212890625 mm

Highest mean error: 5.802427768707275 mm for frame 19

Lowest mean error: 4.982694625854492 mm for frame 80

Saving results

Total time: 35.06584715843201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055690
Iteration 2/25 | Loss: 0.00290407
Iteration 3/25 | Loss: 0.00192570
Iteration 4/25 | Loss: 0.00176469
Iteration 5/25 | Loss: 0.00184685
Iteration 6/25 | Loss: 0.00177566
Iteration 7/25 | Loss: 0.00165837
Iteration 8/25 | Loss: 0.00161838
Iteration 9/25 | Loss: 0.00149515
Iteration 10/25 | Loss: 0.00147717
Iteration 11/25 | Loss: 0.00143985
Iteration 12/25 | Loss: 0.00148227
Iteration 13/25 | Loss: 0.00140628
Iteration 14/25 | Loss: 0.00138516
Iteration 15/25 | Loss: 0.00137575
Iteration 16/25 | Loss: 0.00139065
Iteration 17/25 | Loss: 0.00139437
Iteration 18/25 | Loss: 0.00137701
Iteration 19/25 | Loss: 0.00137409
Iteration 20/25 | Loss: 0.00137258
Iteration 21/25 | Loss: 0.00137223
Iteration 22/25 | Loss: 0.00136909
Iteration 23/25 | Loss: 0.00136522
Iteration 24/25 | Loss: 0.00136483
Iteration 25/25 | Loss: 0.00136060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29709232
Iteration 2/25 | Loss: 0.00310119
Iteration 3/25 | Loss: 0.00310118
Iteration 4/25 | Loss: 0.00273331
Iteration 5/25 | Loss: 0.00273327
Iteration 6/25 | Loss: 0.00273327
Iteration 7/25 | Loss: 0.00273327
Iteration 8/25 | Loss: 0.00273327
Iteration 9/25 | Loss: 0.00273327
Iteration 10/25 | Loss: 0.00273327
Iteration 11/25 | Loss: 0.00273327
Iteration 12/25 | Loss: 0.00273327
Iteration 13/25 | Loss: 0.00273327
Iteration 14/25 | Loss: 0.00273327
Iteration 15/25 | Loss: 0.00273327
Iteration 16/25 | Loss: 0.00273327
Iteration 17/25 | Loss: 0.00273327
Iteration 18/25 | Loss: 0.00273327
Iteration 19/25 | Loss: 0.00273327
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00273326993919909, 0.00273326993919909, 0.00273326993919909, 0.00273326993919909, 0.00273326993919909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00273326993919909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273327
Iteration 2/1000 | Loss: 0.00026800
Iteration 3/1000 | Loss: 0.00010047
Iteration 4/1000 | Loss: 0.00007418
Iteration 5/1000 | Loss: 0.00006308
Iteration 6/1000 | Loss: 0.00016985
Iteration 7/1000 | Loss: 0.00005296
Iteration 8/1000 | Loss: 0.00026468
Iteration 9/1000 | Loss: 0.00018173
Iteration 10/1000 | Loss: 0.00021309
Iteration 11/1000 | Loss: 0.00017911
Iteration 12/1000 | Loss: 0.00019160
Iteration 13/1000 | Loss: 0.00017025
Iteration 14/1000 | Loss: 0.00016841
Iteration 15/1000 | Loss: 0.00015582
Iteration 16/1000 | Loss: 0.00012908
Iteration 17/1000 | Loss: 0.00004371
Iteration 18/1000 | Loss: 0.00004138
Iteration 19/1000 | Loss: 0.00004672
Iteration 20/1000 | Loss: 0.00011920
Iteration 21/1000 | Loss: 0.00025478
Iteration 22/1000 | Loss: 0.00015479
Iteration 23/1000 | Loss: 0.00007835
Iteration 24/1000 | Loss: 0.00005628
Iteration 25/1000 | Loss: 0.00004253
Iteration 26/1000 | Loss: 0.00014924
Iteration 27/1000 | Loss: 0.00094528
Iteration 28/1000 | Loss: 0.00007976
Iteration 29/1000 | Loss: 0.00028488
Iteration 30/1000 | Loss: 0.00010943
Iteration 31/1000 | Loss: 0.00031223
Iteration 32/1000 | Loss: 0.00014595
Iteration 33/1000 | Loss: 0.00029470
Iteration 34/1000 | Loss: 0.00013623
Iteration 35/1000 | Loss: 0.00024452
Iteration 36/1000 | Loss: 0.00013266
Iteration 37/1000 | Loss: 0.00016523
Iteration 38/1000 | Loss: 0.00073734
Iteration 39/1000 | Loss: 0.00015335
Iteration 40/1000 | Loss: 0.00042482
Iteration 41/1000 | Loss: 0.00004714
Iteration 42/1000 | Loss: 0.00005518
Iteration 43/1000 | Loss: 0.00008194
Iteration 44/1000 | Loss: 0.00020760
Iteration 45/1000 | Loss: 0.00022833
Iteration 46/1000 | Loss: 0.00023756
Iteration 47/1000 | Loss: 0.00008200
Iteration 48/1000 | Loss: 0.00004402
Iteration 49/1000 | Loss: 0.00004367
Iteration 50/1000 | Loss: 0.00004026
Iteration 51/1000 | Loss: 0.00003712
Iteration 52/1000 | Loss: 0.00003570
Iteration 53/1000 | Loss: 0.00003525
Iteration 54/1000 | Loss: 0.00003516
Iteration 55/1000 | Loss: 0.00003510
Iteration 56/1000 | Loss: 0.00003508
Iteration 57/1000 | Loss: 0.00003497
Iteration 58/1000 | Loss: 0.00005435
Iteration 59/1000 | Loss: 0.00078966
Iteration 60/1000 | Loss: 0.00003908
Iteration 61/1000 | Loss: 0.00003525
Iteration 62/1000 | Loss: 0.00003378
Iteration 63/1000 | Loss: 0.00005563
Iteration 64/1000 | Loss: 0.00008706
Iteration 65/1000 | Loss: 0.00003182
Iteration 66/1000 | Loss: 0.00003152
Iteration 67/1000 | Loss: 0.00003143
Iteration 68/1000 | Loss: 0.00005321
Iteration 69/1000 | Loss: 0.00003140
Iteration 70/1000 | Loss: 0.00003121
Iteration 71/1000 | Loss: 0.00003121
Iteration 72/1000 | Loss: 0.00003121
Iteration 73/1000 | Loss: 0.00003121
Iteration 74/1000 | Loss: 0.00003121
Iteration 75/1000 | Loss: 0.00003121
Iteration 76/1000 | Loss: 0.00003121
Iteration 77/1000 | Loss: 0.00003121
Iteration 78/1000 | Loss: 0.00003120
Iteration 79/1000 | Loss: 0.00003120
Iteration 80/1000 | Loss: 0.00003117
Iteration 81/1000 | Loss: 0.00003116
Iteration 82/1000 | Loss: 0.00003115
Iteration 83/1000 | Loss: 0.00003114
Iteration 84/1000 | Loss: 0.00003114
Iteration 85/1000 | Loss: 0.00003114
Iteration 86/1000 | Loss: 0.00003113
Iteration 87/1000 | Loss: 0.00003113
Iteration 88/1000 | Loss: 0.00003112
Iteration 89/1000 | Loss: 0.00003111
Iteration 90/1000 | Loss: 0.00003106
Iteration 91/1000 | Loss: 0.00003102
Iteration 92/1000 | Loss: 0.00003102
Iteration 93/1000 | Loss: 0.00003101
Iteration 94/1000 | Loss: 0.00003100
Iteration 95/1000 | Loss: 0.00003100
Iteration 96/1000 | Loss: 0.00003100
Iteration 97/1000 | Loss: 0.00003099
Iteration 98/1000 | Loss: 0.00003099
Iteration 99/1000 | Loss: 0.00003099
Iteration 100/1000 | Loss: 0.00003097
Iteration 101/1000 | Loss: 0.00003097
Iteration 102/1000 | Loss: 0.00005923
Iteration 103/1000 | Loss: 0.00003102
Iteration 104/1000 | Loss: 0.00003088
Iteration 105/1000 | Loss: 0.00003087
Iteration 106/1000 | Loss: 0.00003086
Iteration 107/1000 | Loss: 0.00003086
Iteration 108/1000 | Loss: 0.00003086
Iteration 109/1000 | Loss: 0.00003085
Iteration 110/1000 | Loss: 0.00003085
Iteration 111/1000 | Loss: 0.00003084
Iteration 112/1000 | Loss: 0.00003084
Iteration 113/1000 | Loss: 0.00003084
Iteration 114/1000 | Loss: 0.00003084
Iteration 115/1000 | Loss: 0.00003084
Iteration 116/1000 | Loss: 0.00003083
Iteration 117/1000 | Loss: 0.00003083
Iteration 118/1000 | Loss: 0.00003083
Iteration 119/1000 | Loss: 0.00003082
Iteration 120/1000 | Loss: 0.00003082
Iteration 121/1000 | Loss: 0.00003081
Iteration 122/1000 | Loss: 0.00003081
Iteration 123/1000 | Loss: 0.00003080
Iteration 124/1000 | Loss: 0.00003080
Iteration 125/1000 | Loss: 0.00003080
Iteration 126/1000 | Loss: 0.00003080
Iteration 127/1000 | Loss: 0.00003079
Iteration 128/1000 | Loss: 0.00003079
Iteration 129/1000 | Loss: 0.00003079
Iteration 130/1000 | Loss: 0.00003078
Iteration 131/1000 | Loss: 0.00003078
Iteration 132/1000 | Loss: 0.00003078
Iteration 133/1000 | Loss: 0.00003078
Iteration 134/1000 | Loss: 0.00003077
Iteration 135/1000 | Loss: 0.00003077
Iteration 136/1000 | Loss: 0.00003077
Iteration 137/1000 | Loss: 0.00003077
Iteration 138/1000 | Loss: 0.00003077
Iteration 139/1000 | Loss: 0.00003076
Iteration 140/1000 | Loss: 0.00003076
Iteration 141/1000 | Loss: 0.00003075
Iteration 142/1000 | Loss: 0.00003075
Iteration 143/1000 | Loss: 0.00003075
Iteration 144/1000 | Loss: 0.00003074
Iteration 145/1000 | Loss: 0.00003074
Iteration 146/1000 | Loss: 0.00003073
Iteration 147/1000 | Loss: 0.00003073
Iteration 148/1000 | Loss: 0.00003073
Iteration 149/1000 | Loss: 0.00003072
Iteration 150/1000 | Loss: 0.00003072
Iteration 151/1000 | Loss: 0.00003072
Iteration 152/1000 | Loss: 0.00003071
Iteration 153/1000 | Loss: 0.00003071
Iteration 154/1000 | Loss: 0.00003071
Iteration 155/1000 | Loss: 0.00003071
Iteration 156/1000 | Loss: 0.00003071
Iteration 157/1000 | Loss: 0.00003071
Iteration 158/1000 | Loss: 0.00003071
Iteration 159/1000 | Loss: 0.00003071
Iteration 160/1000 | Loss: 0.00003071
Iteration 161/1000 | Loss: 0.00003071
Iteration 162/1000 | Loss: 0.00003071
Iteration 163/1000 | Loss: 0.00003071
Iteration 164/1000 | Loss: 0.00003071
Iteration 165/1000 | Loss: 0.00003071
Iteration 166/1000 | Loss: 0.00003071
Iteration 167/1000 | Loss: 0.00003071
Iteration 168/1000 | Loss: 0.00003071
Iteration 169/1000 | Loss: 0.00003070
Iteration 170/1000 | Loss: 0.00003070
Iteration 171/1000 | Loss: 0.00003070
Iteration 172/1000 | Loss: 0.00003070
Iteration 173/1000 | Loss: 0.00003070
Iteration 174/1000 | Loss: 0.00003070
Iteration 175/1000 | Loss: 0.00003070
Iteration 176/1000 | Loss: 0.00003070
Iteration 177/1000 | Loss: 0.00003070
Iteration 178/1000 | Loss: 0.00003070
Iteration 179/1000 | Loss: 0.00003070
Iteration 180/1000 | Loss: 0.00003070
Iteration 181/1000 | Loss: 0.00003070
Iteration 182/1000 | Loss: 0.00003069
Iteration 183/1000 | Loss: 0.00003069
Iteration 184/1000 | Loss: 0.00003069
Iteration 185/1000 | Loss: 0.00003069
Iteration 186/1000 | Loss: 0.00003069
Iteration 187/1000 | Loss: 0.00003069
Iteration 188/1000 | Loss: 0.00003068
Iteration 189/1000 | Loss: 0.00003068
Iteration 190/1000 | Loss: 0.00003068
Iteration 191/1000 | Loss: 0.00003068
Iteration 192/1000 | Loss: 0.00003068
Iteration 193/1000 | Loss: 0.00003068
Iteration 194/1000 | Loss: 0.00003068
Iteration 195/1000 | Loss: 0.00003068
Iteration 196/1000 | Loss: 0.00003068
Iteration 197/1000 | Loss: 0.00003068
Iteration 198/1000 | Loss: 0.00003068
Iteration 199/1000 | Loss: 0.00003068
Iteration 200/1000 | Loss: 0.00003068
Iteration 201/1000 | Loss: 0.00003067
Iteration 202/1000 | Loss: 0.00003067
Iteration 203/1000 | Loss: 0.00003067
Iteration 204/1000 | Loss: 0.00003067
Iteration 205/1000 | Loss: 0.00003067
Iteration 206/1000 | Loss: 0.00003067
Iteration 207/1000 | Loss: 0.00003067
Iteration 208/1000 | Loss: 0.00003066
Iteration 209/1000 | Loss: 0.00003066
Iteration 210/1000 | Loss: 0.00003066
Iteration 211/1000 | Loss: 0.00003066
Iteration 212/1000 | Loss: 0.00003066
Iteration 213/1000 | Loss: 0.00003066
Iteration 214/1000 | Loss: 0.00003066
Iteration 215/1000 | Loss: 0.00003066
Iteration 216/1000 | Loss: 0.00003066
Iteration 217/1000 | Loss: 0.00003066
Iteration 218/1000 | Loss: 0.00003066
Iteration 219/1000 | Loss: 0.00003066
Iteration 220/1000 | Loss: 0.00003066
Iteration 221/1000 | Loss: 0.00003066
Iteration 222/1000 | Loss: 0.00003065
Iteration 223/1000 | Loss: 0.00003065
Iteration 224/1000 | Loss: 0.00003065
Iteration 225/1000 | Loss: 0.00003065
Iteration 226/1000 | Loss: 0.00003065
Iteration 227/1000 | Loss: 0.00003065
Iteration 228/1000 | Loss: 0.00003065
Iteration 229/1000 | Loss: 0.00003065
Iteration 230/1000 | Loss: 0.00003065
Iteration 231/1000 | Loss: 0.00003065
Iteration 232/1000 | Loss: 0.00003065
Iteration 233/1000 | Loss: 0.00003065
Iteration 234/1000 | Loss: 0.00003065
Iteration 235/1000 | Loss: 0.00003065
Iteration 236/1000 | Loss: 0.00003065
Iteration 237/1000 | Loss: 0.00003065
Iteration 238/1000 | Loss: 0.00003065
Iteration 239/1000 | Loss: 0.00003065
Iteration 240/1000 | Loss: 0.00003065
Iteration 241/1000 | Loss: 0.00003065
Iteration 242/1000 | Loss: 0.00003065
Iteration 243/1000 | Loss: 0.00003065
Iteration 244/1000 | Loss: 0.00003065
Iteration 245/1000 | Loss: 0.00003065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [3.0646002414869145e-05, 3.0646002414869145e-05, 3.0646002414869145e-05, 3.0646002414869145e-05, 3.0646002414869145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0646002414869145e-05

Optimization complete. Final v2v error: 4.489992141723633 mm

Highest mean error: 11.606992721557617 mm for frame 91

Lowest mean error: 4.024831771850586 mm for frame 0

Saving results

Total time: 147.937650680542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00587446
Iteration 2/25 | Loss: 0.00198381
Iteration 3/25 | Loss: 0.00155641
Iteration 4/25 | Loss: 0.00152919
Iteration 5/25 | Loss: 0.00152501
Iteration 6/25 | Loss: 0.00152443
Iteration 7/25 | Loss: 0.00152443
Iteration 8/25 | Loss: 0.00152443
Iteration 9/25 | Loss: 0.00152443
Iteration 10/25 | Loss: 0.00152443
Iteration 11/25 | Loss: 0.00152443
Iteration 12/25 | Loss: 0.00152443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001524434075690806, 0.001524434075690806, 0.001524434075690806, 0.001524434075690806, 0.001524434075690806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001524434075690806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97732222
Iteration 2/25 | Loss: 0.00159145
Iteration 3/25 | Loss: 0.00159144
Iteration 4/25 | Loss: 0.00159143
Iteration 5/25 | Loss: 0.00159143
Iteration 6/25 | Loss: 0.00159143
Iteration 7/25 | Loss: 0.00159143
Iteration 8/25 | Loss: 0.00159143
Iteration 9/25 | Loss: 0.00159143
Iteration 10/25 | Loss: 0.00159143
Iteration 11/25 | Loss: 0.00159143
Iteration 12/25 | Loss: 0.00159143
Iteration 13/25 | Loss: 0.00159143
Iteration 14/25 | Loss: 0.00159143
Iteration 15/25 | Loss: 0.00159143
Iteration 16/25 | Loss: 0.00159143
Iteration 17/25 | Loss: 0.00159143
Iteration 18/25 | Loss: 0.00159143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015914321411401033, 0.0015914321411401033, 0.0015914321411401033, 0.0015914321411401033, 0.0015914321411401033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015914321411401033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159143
Iteration 2/1000 | Loss: 0.00011221
Iteration 3/1000 | Loss: 0.00007078
Iteration 4/1000 | Loss: 0.00005992
Iteration 5/1000 | Loss: 0.00005394
Iteration 6/1000 | Loss: 0.00005025
Iteration 7/1000 | Loss: 0.00004806
Iteration 8/1000 | Loss: 0.00004692
Iteration 9/1000 | Loss: 0.00004573
Iteration 10/1000 | Loss: 0.00004489
Iteration 11/1000 | Loss: 0.00004420
Iteration 12/1000 | Loss: 0.00004376
Iteration 13/1000 | Loss: 0.00004344
Iteration 14/1000 | Loss: 0.00004318
Iteration 15/1000 | Loss: 0.00004296
Iteration 16/1000 | Loss: 0.00004275
Iteration 17/1000 | Loss: 0.00004255
Iteration 18/1000 | Loss: 0.00004235
Iteration 19/1000 | Loss: 0.00004221
Iteration 20/1000 | Loss: 0.00004205
Iteration 21/1000 | Loss: 0.00004204
Iteration 22/1000 | Loss: 0.00004196
Iteration 23/1000 | Loss: 0.00004185
Iteration 24/1000 | Loss: 0.00004181
Iteration 25/1000 | Loss: 0.00004181
Iteration 26/1000 | Loss: 0.00004179
Iteration 27/1000 | Loss: 0.00004174
Iteration 28/1000 | Loss: 0.00004172
Iteration 29/1000 | Loss: 0.00004171
Iteration 30/1000 | Loss: 0.00004170
Iteration 31/1000 | Loss: 0.00004170
Iteration 32/1000 | Loss: 0.00004170
Iteration 33/1000 | Loss: 0.00004170
Iteration 34/1000 | Loss: 0.00004170
Iteration 35/1000 | Loss: 0.00004169
Iteration 36/1000 | Loss: 0.00004169
Iteration 37/1000 | Loss: 0.00004168
Iteration 38/1000 | Loss: 0.00004167
Iteration 39/1000 | Loss: 0.00004167
Iteration 40/1000 | Loss: 0.00004167
Iteration 41/1000 | Loss: 0.00004167
Iteration 42/1000 | Loss: 0.00004166
Iteration 43/1000 | Loss: 0.00004166
Iteration 44/1000 | Loss: 0.00004166
Iteration 45/1000 | Loss: 0.00004165
Iteration 46/1000 | Loss: 0.00004164
Iteration 47/1000 | Loss: 0.00004164
Iteration 48/1000 | Loss: 0.00004164
Iteration 49/1000 | Loss: 0.00004164
Iteration 50/1000 | Loss: 0.00004164
Iteration 51/1000 | Loss: 0.00004164
Iteration 52/1000 | Loss: 0.00004163
Iteration 53/1000 | Loss: 0.00004163
Iteration 54/1000 | Loss: 0.00004163
Iteration 55/1000 | Loss: 0.00004163
Iteration 56/1000 | Loss: 0.00004163
Iteration 57/1000 | Loss: 0.00004162
Iteration 58/1000 | Loss: 0.00004162
Iteration 59/1000 | Loss: 0.00004162
Iteration 60/1000 | Loss: 0.00004162
Iteration 61/1000 | Loss: 0.00004162
Iteration 62/1000 | Loss: 0.00004162
Iteration 63/1000 | Loss: 0.00004162
Iteration 64/1000 | Loss: 0.00004162
Iteration 65/1000 | Loss: 0.00004161
Iteration 66/1000 | Loss: 0.00004161
Iteration 67/1000 | Loss: 0.00004161
Iteration 68/1000 | Loss: 0.00004161
Iteration 69/1000 | Loss: 0.00004161
Iteration 70/1000 | Loss: 0.00004161
Iteration 71/1000 | Loss: 0.00004161
Iteration 72/1000 | Loss: 0.00004161
Iteration 73/1000 | Loss: 0.00004161
Iteration 74/1000 | Loss: 0.00004161
Iteration 75/1000 | Loss: 0.00004161
Iteration 76/1000 | Loss: 0.00004160
Iteration 77/1000 | Loss: 0.00004160
Iteration 78/1000 | Loss: 0.00004160
Iteration 79/1000 | Loss: 0.00004160
Iteration 80/1000 | Loss: 0.00004160
Iteration 81/1000 | Loss: 0.00004160
Iteration 82/1000 | Loss: 0.00004160
Iteration 83/1000 | Loss: 0.00004160
Iteration 84/1000 | Loss: 0.00004159
Iteration 85/1000 | Loss: 0.00004159
Iteration 86/1000 | Loss: 0.00004159
Iteration 87/1000 | Loss: 0.00004159
Iteration 88/1000 | Loss: 0.00004159
Iteration 89/1000 | Loss: 0.00004159
Iteration 90/1000 | Loss: 0.00004159
Iteration 91/1000 | Loss: 0.00004159
Iteration 92/1000 | Loss: 0.00004159
Iteration 93/1000 | Loss: 0.00004159
Iteration 94/1000 | Loss: 0.00004158
Iteration 95/1000 | Loss: 0.00004158
Iteration 96/1000 | Loss: 0.00004158
Iteration 97/1000 | Loss: 0.00004158
Iteration 98/1000 | Loss: 0.00004158
Iteration 99/1000 | Loss: 0.00004158
Iteration 100/1000 | Loss: 0.00004158
Iteration 101/1000 | Loss: 0.00004158
Iteration 102/1000 | Loss: 0.00004158
Iteration 103/1000 | Loss: 0.00004157
Iteration 104/1000 | Loss: 0.00004157
Iteration 105/1000 | Loss: 0.00004157
Iteration 106/1000 | Loss: 0.00004157
Iteration 107/1000 | Loss: 0.00004157
Iteration 108/1000 | Loss: 0.00004157
Iteration 109/1000 | Loss: 0.00004157
Iteration 110/1000 | Loss: 0.00004157
Iteration 111/1000 | Loss: 0.00004157
Iteration 112/1000 | Loss: 0.00004156
Iteration 113/1000 | Loss: 0.00004156
Iteration 114/1000 | Loss: 0.00004156
Iteration 115/1000 | Loss: 0.00004156
Iteration 116/1000 | Loss: 0.00004156
Iteration 117/1000 | Loss: 0.00004155
Iteration 118/1000 | Loss: 0.00004155
Iteration 119/1000 | Loss: 0.00004155
Iteration 120/1000 | Loss: 0.00004155
Iteration 121/1000 | Loss: 0.00004155
Iteration 122/1000 | Loss: 0.00004155
Iteration 123/1000 | Loss: 0.00004155
Iteration 124/1000 | Loss: 0.00004155
Iteration 125/1000 | Loss: 0.00004155
Iteration 126/1000 | Loss: 0.00004155
Iteration 127/1000 | Loss: 0.00004154
Iteration 128/1000 | Loss: 0.00004154
Iteration 129/1000 | Loss: 0.00004154
Iteration 130/1000 | Loss: 0.00004154
Iteration 131/1000 | Loss: 0.00004154
Iteration 132/1000 | Loss: 0.00004154
Iteration 133/1000 | Loss: 0.00004154
Iteration 134/1000 | Loss: 0.00004154
Iteration 135/1000 | Loss: 0.00004154
Iteration 136/1000 | Loss: 0.00004154
Iteration 137/1000 | Loss: 0.00004154
Iteration 138/1000 | Loss: 0.00004154
Iteration 139/1000 | Loss: 0.00004154
Iteration 140/1000 | Loss: 0.00004153
Iteration 141/1000 | Loss: 0.00004153
Iteration 142/1000 | Loss: 0.00004153
Iteration 143/1000 | Loss: 0.00004153
Iteration 144/1000 | Loss: 0.00004153
Iteration 145/1000 | Loss: 0.00004153
Iteration 146/1000 | Loss: 0.00004153
Iteration 147/1000 | Loss: 0.00004153
Iteration 148/1000 | Loss: 0.00004153
Iteration 149/1000 | Loss: 0.00004153
Iteration 150/1000 | Loss: 0.00004153
Iteration 151/1000 | Loss: 0.00004153
Iteration 152/1000 | Loss: 0.00004153
Iteration 153/1000 | Loss: 0.00004153
Iteration 154/1000 | Loss: 0.00004153
Iteration 155/1000 | Loss: 0.00004153
Iteration 156/1000 | Loss: 0.00004153
Iteration 157/1000 | Loss: 0.00004152
Iteration 158/1000 | Loss: 0.00004152
Iteration 159/1000 | Loss: 0.00004152
Iteration 160/1000 | Loss: 0.00004152
Iteration 161/1000 | Loss: 0.00004152
Iteration 162/1000 | Loss: 0.00004152
Iteration 163/1000 | Loss: 0.00004152
Iteration 164/1000 | Loss: 0.00004152
Iteration 165/1000 | Loss: 0.00004152
Iteration 166/1000 | Loss: 0.00004152
Iteration 167/1000 | Loss: 0.00004152
Iteration 168/1000 | Loss: 0.00004152
Iteration 169/1000 | Loss: 0.00004152
Iteration 170/1000 | Loss: 0.00004152
Iteration 171/1000 | Loss: 0.00004152
Iteration 172/1000 | Loss: 0.00004152
Iteration 173/1000 | Loss: 0.00004152
Iteration 174/1000 | Loss: 0.00004152
Iteration 175/1000 | Loss: 0.00004152
Iteration 176/1000 | Loss: 0.00004152
Iteration 177/1000 | Loss: 0.00004152
Iteration 178/1000 | Loss: 0.00004152
Iteration 179/1000 | Loss: 0.00004152
Iteration 180/1000 | Loss: 0.00004152
Iteration 181/1000 | Loss: 0.00004152
Iteration 182/1000 | Loss: 0.00004152
Iteration 183/1000 | Loss: 0.00004152
Iteration 184/1000 | Loss: 0.00004152
Iteration 185/1000 | Loss: 0.00004152
Iteration 186/1000 | Loss: 0.00004152
Iteration 187/1000 | Loss: 0.00004152
Iteration 188/1000 | Loss: 0.00004152
Iteration 189/1000 | Loss: 0.00004152
Iteration 190/1000 | Loss: 0.00004152
Iteration 191/1000 | Loss: 0.00004152
Iteration 192/1000 | Loss: 0.00004152
Iteration 193/1000 | Loss: 0.00004152
Iteration 194/1000 | Loss: 0.00004152
Iteration 195/1000 | Loss: 0.00004152
Iteration 196/1000 | Loss: 0.00004152
Iteration 197/1000 | Loss: 0.00004152
Iteration 198/1000 | Loss: 0.00004152
Iteration 199/1000 | Loss: 0.00004152
Iteration 200/1000 | Loss: 0.00004152
Iteration 201/1000 | Loss: 0.00004152
Iteration 202/1000 | Loss: 0.00004152
Iteration 203/1000 | Loss: 0.00004152
Iteration 204/1000 | Loss: 0.00004152
Iteration 205/1000 | Loss: 0.00004152
Iteration 206/1000 | Loss: 0.00004152
Iteration 207/1000 | Loss: 0.00004152
Iteration 208/1000 | Loss: 0.00004152
Iteration 209/1000 | Loss: 0.00004152
Iteration 210/1000 | Loss: 0.00004152
Iteration 211/1000 | Loss: 0.00004152
Iteration 212/1000 | Loss: 0.00004152
Iteration 213/1000 | Loss: 0.00004152
Iteration 214/1000 | Loss: 0.00004152
Iteration 215/1000 | Loss: 0.00004152
Iteration 216/1000 | Loss: 0.00004152
Iteration 217/1000 | Loss: 0.00004152
Iteration 218/1000 | Loss: 0.00004152
Iteration 219/1000 | Loss: 0.00004152
Iteration 220/1000 | Loss: 0.00004152
Iteration 221/1000 | Loss: 0.00004152
Iteration 222/1000 | Loss: 0.00004152
Iteration 223/1000 | Loss: 0.00004152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [4.152185283601284e-05, 4.152185283601284e-05, 4.152185283601284e-05, 4.152185283601284e-05, 4.152185283601284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.152185283601284e-05

Optimization complete. Final v2v error: 5.204845905303955 mm

Highest mean error: 6.264441967010498 mm for frame 75

Lowest mean error: 4.181023597717285 mm for frame 132

Saving results

Total time: 51.15447974205017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01129190
Iteration 2/25 | Loss: 0.01129190
Iteration 3/25 | Loss: 0.01129190
Iteration 4/25 | Loss: 0.01129189
Iteration 5/25 | Loss: 0.01129189
Iteration 6/25 | Loss: 0.01129189
Iteration 7/25 | Loss: 0.01129189
Iteration 8/25 | Loss: 0.01129189
Iteration 9/25 | Loss: 0.01129188
Iteration 10/25 | Loss: 0.01129188
Iteration 11/25 | Loss: 0.01129188
Iteration 12/25 | Loss: 0.01129188
Iteration 13/25 | Loss: 0.01129188
Iteration 14/25 | Loss: 0.01129188
Iteration 15/25 | Loss: 0.01129188
Iteration 16/25 | Loss: 0.01129188
Iteration 17/25 | Loss: 0.01129187
Iteration 18/25 | Loss: 0.01129187
Iteration 19/25 | Loss: 0.01129187
Iteration 20/25 | Loss: 0.01129187
Iteration 21/25 | Loss: 0.01129187
Iteration 22/25 | Loss: 0.01129187
Iteration 23/25 | Loss: 0.01129186
Iteration 24/25 | Loss: 0.01129186
Iteration 25/25 | Loss: 0.01129186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34790313
Iteration 2/25 | Loss: 0.12744723
Iteration 3/25 | Loss: 0.12694381
Iteration 4/25 | Loss: 0.12636888
Iteration 5/25 | Loss: 0.12636888
Iteration 6/25 | Loss: 0.12632985
Iteration 7/25 | Loss: 0.12632887
Iteration 8/25 | Loss: 0.12632886
Iteration 9/25 | Loss: 0.12632886
Iteration 10/25 | Loss: 0.12632886
Iteration 11/25 | Loss: 0.12632886
Iteration 12/25 | Loss: 0.12632886
Iteration 13/25 | Loss: 0.12632886
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.12632885575294495, 0.12632885575294495, 0.12632885575294495, 0.12632885575294495, 0.12632885575294495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12632885575294495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12632886
Iteration 2/1000 | Loss: 0.00204131
Iteration 3/1000 | Loss: 0.00087734
Iteration 4/1000 | Loss: 0.00061373
Iteration 5/1000 | Loss: 0.00250733
Iteration 6/1000 | Loss: 0.00071617
Iteration 7/1000 | Loss: 0.00037175
Iteration 8/1000 | Loss: 0.00033815
Iteration 9/1000 | Loss: 0.00011482
Iteration 10/1000 | Loss: 0.00024586
Iteration 11/1000 | Loss: 0.00011096
Iteration 12/1000 | Loss: 0.00004598
Iteration 13/1000 | Loss: 0.00009098
Iteration 14/1000 | Loss: 0.00003949
Iteration 15/1000 | Loss: 0.00004376
Iteration 16/1000 | Loss: 0.00006683
Iteration 17/1000 | Loss: 0.00004971
Iteration 18/1000 | Loss: 0.00004857
Iteration 19/1000 | Loss: 0.00003423
Iteration 20/1000 | Loss: 0.00004512
Iteration 21/1000 | Loss: 0.00003983
Iteration 22/1000 | Loss: 0.00003257
Iteration 23/1000 | Loss: 0.00003139
Iteration 24/1000 | Loss: 0.00004441
Iteration 25/1000 | Loss: 0.00003064
Iteration 26/1000 | Loss: 0.00004127
Iteration 27/1000 | Loss: 0.00003990
Iteration 28/1000 | Loss: 0.00003174
Iteration 29/1000 | Loss: 0.00002997
Iteration 30/1000 | Loss: 0.00002986
Iteration 31/1000 | Loss: 0.00002975
Iteration 32/1000 | Loss: 0.00002990
Iteration 33/1000 | Loss: 0.00002957
Iteration 34/1000 | Loss: 0.00002956
Iteration 35/1000 | Loss: 0.00002948
Iteration 36/1000 | Loss: 0.00002942
Iteration 37/1000 | Loss: 0.00002942
Iteration 38/1000 | Loss: 0.00002939
Iteration 39/1000 | Loss: 0.00002936
Iteration 40/1000 | Loss: 0.00002935
Iteration 41/1000 | Loss: 0.00002933
Iteration 42/1000 | Loss: 0.00002932
Iteration 43/1000 | Loss: 0.00002932
Iteration 44/1000 | Loss: 0.00002931
Iteration 45/1000 | Loss: 0.00002931
Iteration 46/1000 | Loss: 0.00002931
Iteration 47/1000 | Loss: 0.00002931
Iteration 48/1000 | Loss: 0.00002930
Iteration 49/1000 | Loss: 0.00002930
Iteration 50/1000 | Loss: 0.00002930
Iteration 51/1000 | Loss: 0.00002929
Iteration 52/1000 | Loss: 0.00002929
Iteration 53/1000 | Loss: 0.00002929
Iteration 54/1000 | Loss: 0.00003864
Iteration 55/1000 | Loss: 0.00002929
Iteration 56/1000 | Loss: 0.00002927
Iteration 57/1000 | Loss: 0.00002927
Iteration 58/1000 | Loss: 0.00002927
Iteration 59/1000 | Loss: 0.00002926
Iteration 60/1000 | Loss: 0.00002926
Iteration 61/1000 | Loss: 0.00002926
Iteration 62/1000 | Loss: 0.00002926
Iteration 63/1000 | Loss: 0.00002926
Iteration 64/1000 | Loss: 0.00002925
Iteration 65/1000 | Loss: 0.00002925
Iteration 66/1000 | Loss: 0.00002925
Iteration 67/1000 | Loss: 0.00002924
Iteration 68/1000 | Loss: 0.00002924
Iteration 69/1000 | Loss: 0.00002924
Iteration 70/1000 | Loss: 0.00002924
Iteration 71/1000 | Loss: 0.00002924
Iteration 72/1000 | Loss: 0.00002923
Iteration 73/1000 | Loss: 0.00002923
Iteration 74/1000 | Loss: 0.00003289
Iteration 75/1000 | Loss: 0.00002958
Iteration 76/1000 | Loss: 0.00003192
Iteration 77/1000 | Loss: 0.00002955
Iteration 78/1000 | Loss: 0.00004264
Iteration 79/1000 | Loss: 0.00002919
Iteration 80/1000 | Loss: 0.00002918
Iteration 81/1000 | Loss: 0.00002918
Iteration 82/1000 | Loss: 0.00002918
Iteration 83/1000 | Loss: 0.00002918
Iteration 84/1000 | Loss: 0.00002918
Iteration 85/1000 | Loss: 0.00002917
Iteration 86/1000 | Loss: 0.00002917
Iteration 87/1000 | Loss: 0.00002917
Iteration 88/1000 | Loss: 0.00002917
Iteration 89/1000 | Loss: 0.00002917
Iteration 90/1000 | Loss: 0.00002917
Iteration 91/1000 | Loss: 0.00002917
Iteration 92/1000 | Loss: 0.00002917
Iteration 93/1000 | Loss: 0.00002953
Iteration 94/1000 | Loss: 0.00002953
Iteration 95/1000 | Loss: 0.00002953
Iteration 96/1000 | Loss: 0.00002953
Iteration 97/1000 | Loss: 0.00002953
Iteration 98/1000 | Loss: 0.00002953
Iteration 99/1000 | Loss: 0.00002953
Iteration 100/1000 | Loss: 0.00003143
Iteration 101/1000 | Loss: 0.00002911
Iteration 102/1000 | Loss: 0.00002910
Iteration 103/1000 | Loss: 0.00002910
Iteration 104/1000 | Loss: 0.00002909
Iteration 105/1000 | Loss: 0.00002909
Iteration 106/1000 | Loss: 0.00002909
Iteration 107/1000 | Loss: 0.00002909
Iteration 108/1000 | Loss: 0.00002909
Iteration 109/1000 | Loss: 0.00002909
Iteration 110/1000 | Loss: 0.00002909
Iteration 111/1000 | Loss: 0.00002909
Iteration 112/1000 | Loss: 0.00002909
Iteration 113/1000 | Loss: 0.00002909
Iteration 114/1000 | Loss: 0.00002909
Iteration 115/1000 | Loss: 0.00002909
Iteration 116/1000 | Loss: 0.00002909
Iteration 117/1000 | Loss: 0.00002909
Iteration 118/1000 | Loss: 0.00002909
Iteration 119/1000 | Loss: 0.00002909
Iteration 120/1000 | Loss: 0.00002909
Iteration 121/1000 | Loss: 0.00002909
Iteration 122/1000 | Loss: 0.00002909
Iteration 123/1000 | Loss: 0.00002909
Iteration 124/1000 | Loss: 0.00002909
Iteration 125/1000 | Loss: 0.00002909
Iteration 126/1000 | Loss: 0.00002909
Iteration 127/1000 | Loss: 0.00002909
Iteration 128/1000 | Loss: 0.00002909
Iteration 129/1000 | Loss: 0.00002909
Iteration 130/1000 | Loss: 0.00002909
Iteration 131/1000 | Loss: 0.00002909
Iteration 132/1000 | Loss: 0.00002909
Iteration 133/1000 | Loss: 0.00002909
Iteration 134/1000 | Loss: 0.00002909
Iteration 135/1000 | Loss: 0.00002909
Iteration 136/1000 | Loss: 0.00002909
Iteration 137/1000 | Loss: 0.00002909
Iteration 138/1000 | Loss: 0.00002909
Iteration 139/1000 | Loss: 0.00002909
Iteration 140/1000 | Loss: 0.00002909
Iteration 141/1000 | Loss: 0.00002909
Iteration 142/1000 | Loss: 0.00002909
Iteration 143/1000 | Loss: 0.00002909
Iteration 144/1000 | Loss: 0.00002909
Iteration 145/1000 | Loss: 0.00002909
Iteration 146/1000 | Loss: 0.00002909
Iteration 147/1000 | Loss: 0.00002909
Iteration 148/1000 | Loss: 0.00002909
Iteration 149/1000 | Loss: 0.00002909
Iteration 150/1000 | Loss: 0.00002909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.908575152105186e-05, 2.908575152105186e-05, 2.908575152105186e-05, 2.908575152105186e-05, 2.908575152105186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.908575152105186e-05

Optimization complete. Final v2v error: 4.474735736846924 mm

Highest mean error: 10.046011924743652 mm for frame 220

Lowest mean error: 4.2456793785095215 mm for frame 209

Saving results

Total time: 79.22011876106262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781933
Iteration 2/25 | Loss: 0.00177197
Iteration 3/25 | Loss: 0.00158160
Iteration 4/25 | Loss: 0.00155731
Iteration 5/25 | Loss: 0.00155132
Iteration 6/25 | Loss: 0.00155039
Iteration 7/25 | Loss: 0.00155039
Iteration 8/25 | Loss: 0.00155039
Iteration 9/25 | Loss: 0.00155039
Iteration 10/25 | Loss: 0.00155039
Iteration 11/25 | Loss: 0.00155039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015503928298130631, 0.0015503928298130631, 0.0015503928298130631, 0.0015503928298130631, 0.0015503928298130631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015503928298130631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10795748
Iteration 2/25 | Loss: 0.00257901
Iteration 3/25 | Loss: 0.00257898
Iteration 4/25 | Loss: 0.00257898
Iteration 5/25 | Loss: 0.00257898
Iteration 6/25 | Loss: 0.00257898
Iteration 7/25 | Loss: 0.00257898
Iteration 8/25 | Loss: 0.00257898
Iteration 9/25 | Loss: 0.00257898
Iteration 10/25 | Loss: 0.00257898
Iteration 11/25 | Loss: 0.00257898
Iteration 12/25 | Loss: 0.00257898
Iteration 13/25 | Loss: 0.00257898
Iteration 14/25 | Loss: 0.00257898
Iteration 15/25 | Loss: 0.00257898
Iteration 16/25 | Loss: 0.00257898
Iteration 17/25 | Loss: 0.00257898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0025789823848754168, 0.0025789823848754168, 0.0025789823848754168, 0.0025789823848754168, 0.0025789823848754168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025789823848754168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257898
Iteration 2/1000 | Loss: 0.00010694
Iteration 3/1000 | Loss: 0.00007197
Iteration 4/1000 | Loss: 0.00005711
Iteration 5/1000 | Loss: 0.00004734
Iteration 6/1000 | Loss: 0.00004132
Iteration 7/1000 | Loss: 0.00003819
Iteration 8/1000 | Loss: 0.00003634
Iteration 9/1000 | Loss: 0.00003480
Iteration 10/1000 | Loss: 0.00003335
Iteration 11/1000 | Loss: 0.00003243
Iteration 12/1000 | Loss: 0.00003199
Iteration 13/1000 | Loss: 0.00003159
Iteration 14/1000 | Loss: 0.00003147
Iteration 15/1000 | Loss: 0.00003135
Iteration 16/1000 | Loss: 0.00003134
Iteration 17/1000 | Loss: 0.00003132
Iteration 18/1000 | Loss: 0.00003119
Iteration 19/1000 | Loss: 0.00003114
Iteration 20/1000 | Loss: 0.00003112
Iteration 21/1000 | Loss: 0.00003106
Iteration 22/1000 | Loss: 0.00003102
Iteration 23/1000 | Loss: 0.00003102
Iteration 24/1000 | Loss: 0.00003098
Iteration 25/1000 | Loss: 0.00003095
Iteration 26/1000 | Loss: 0.00003094
Iteration 27/1000 | Loss: 0.00003094
Iteration 28/1000 | Loss: 0.00003094
Iteration 29/1000 | Loss: 0.00003093
Iteration 30/1000 | Loss: 0.00003093
Iteration 31/1000 | Loss: 0.00003091
Iteration 32/1000 | Loss: 0.00003091
Iteration 33/1000 | Loss: 0.00003089
Iteration 34/1000 | Loss: 0.00003089
Iteration 35/1000 | Loss: 0.00003089
Iteration 36/1000 | Loss: 0.00003088
Iteration 37/1000 | Loss: 0.00003088
Iteration 38/1000 | Loss: 0.00003087
Iteration 39/1000 | Loss: 0.00003087
Iteration 40/1000 | Loss: 0.00003087
Iteration 41/1000 | Loss: 0.00003087
Iteration 42/1000 | Loss: 0.00003086
Iteration 43/1000 | Loss: 0.00003086
Iteration 44/1000 | Loss: 0.00003086
Iteration 45/1000 | Loss: 0.00003086
Iteration 46/1000 | Loss: 0.00003086
Iteration 47/1000 | Loss: 0.00003086
Iteration 48/1000 | Loss: 0.00003086
Iteration 49/1000 | Loss: 0.00003086
Iteration 50/1000 | Loss: 0.00003086
Iteration 51/1000 | Loss: 0.00003086
Iteration 52/1000 | Loss: 0.00003086
Iteration 53/1000 | Loss: 0.00003086
Iteration 54/1000 | Loss: 0.00003085
Iteration 55/1000 | Loss: 0.00003085
Iteration 56/1000 | Loss: 0.00003084
Iteration 57/1000 | Loss: 0.00003084
Iteration 58/1000 | Loss: 0.00003083
Iteration 59/1000 | Loss: 0.00003083
Iteration 60/1000 | Loss: 0.00003083
Iteration 61/1000 | Loss: 0.00003082
Iteration 62/1000 | Loss: 0.00003082
Iteration 63/1000 | Loss: 0.00003082
Iteration 64/1000 | Loss: 0.00003081
Iteration 65/1000 | Loss: 0.00003081
Iteration 66/1000 | Loss: 0.00003081
Iteration 67/1000 | Loss: 0.00003081
Iteration 68/1000 | Loss: 0.00003081
Iteration 69/1000 | Loss: 0.00003081
Iteration 70/1000 | Loss: 0.00003081
Iteration 71/1000 | Loss: 0.00003081
Iteration 72/1000 | Loss: 0.00003081
Iteration 73/1000 | Loss: 0.00003080
Iteration 74/1000 | Loss: 0.00003080
Iteration 75/1000 | Loss: 0.00003080
Iteration 76/1000 | Loss: 0.00003080
Iteration 77/1000 | Loss: 0.00003080
Iteration 78/1000 | Loss: 0.00003080
Iteration 79/1000 | Loss: 0.00003080
Iteration 80/1000 | Loss: 0.00003080
Iteration 81/1000 | Loss: 0.00003080
Iteration 82/1000 | Loss: 0.00003079
Iteration 83/1000 | Loss: 0.00003079
Iteration 84/1000 | Loss: 0.00003079
Iteration 85/1000 | Loss: 0.00003079
Iteration 86/1000 | Loss: 0.00003078
Iteration 87/1000 | Loss: 0.00003078
Iteration 88/1000 | Loss: 0.00003078
Iteration 89/1000 | Loss: 0.00003078
Iteration 90/1000 | Loss: 0.00003078
Iteration 91/1000 | Loss: 0.00003077
Iteration 92/1000 | Loss: 0.00003077
Iteration 93/1000 | Loss: 0.00003077
Iteration 94/1000 | Loss: 0.00003077
Iteration 95/1000 | Loss: 0.00003077
Iteration 96/1000 | Loss: 0.00003077
Iteration 97/1000 | Loss: 0.00003077
Iteration 98/1000 | Loss: 0.00003077
Iteration 99/1000 | Loss: 0.00003077
Iteration 100/1000 | Loss: 0.00003077
Iteration 101/1000 | Loss: 0.00003076
Iteration 102/1000 | Loss: 0.00003076
Iteration 103/1000 | Loss: 0.00003076
Iteration 104/1000 | Loss: 0.00003076
Iteration 105/1000 | Loss: 0.00003076
Iteration 106/1000 | Loss: 0.00003076
Iteration 107/1000 | Loss: 0.00003076
Iteration 108/1000 | Loss: 0.00003076
Iteration 109/1000 | Loss: 0.00003076
Iteration 110/1000 | Loss: 0.00003076
Iteration 111/1000 | Loss: 0.00003076
Iteration 112/1000 | Loss: 0.00003076
Iteration 113/1000 | Loss: 0.00003076
Iteration 114/1000 | Loss: 0.00003075
Iteration 115/1000 | Loss: 0.00003075
Iteration 116/1000 | Loss: 0.00003075
Iteration 117/1000 | Loss: 0.00003075
Iteration 118/1000 | Loss: 0.00003075
Iteration 119/1000 | Loss: 0.00003075
Iteration 120/1000 | Loss: 0.00003075
Iteration 121/1000 | Loss: 0.00003075
Iteration 122/1000 | Loss: 0.00003075
Iteration 123/1000 | Loss: 0.00003075
Iteration 124/1000 | Loss: 0.00003075
Iteration 125/1000 | Loss: 0.00003075
Iteration 126/1000 | Loss: 0.00003075
Iteration 127/1000 | Loss: 0.00003075
Iteration 128/1000 | Loss: 0.00003075
Iteration 129/1000 | Loss: 0.00003074
Iteration 130/1000 | Loss: 0.00003074
Iteration 131/1000 | Loss: 0.00003074
Iteration 132/1000 | Loss: 0.00003074
Iteration 133/1000 | Loss: 0.00003074
Iteration 134/1000 | Loss: 0.00003074
Iteration 135/1000 | Loss: 0.00003074
Iteration 136/1000 | Loss: 0.00003074
Iteration 137/1000 | Loss: 0.00003074
Iteration 138/1000 | Loss: 0.00003074
Iteration 139/1000 | Loss: 0.00003074
Iteration 140/1000 | Loss: 0.00003074
Iteration 141/1000 | Loss: 0.00003074
Iteration 142/1000 | Loss: 0.00003073
Iteration 143/1000 | Loss: 0.00003073
Iteration 144/1000 | Loss: 0.00003073
Iteration 145/1000 | Loss: 0.00003073
Iteration 146/1000 | Loss: 0.00003073
Iteration 147/1000 | Loss: 0.00003073
Iteration 148/1000 | Loss: 0.00003073
Iteration 149/1000 | Loss: 0.00003073
Iteration 150/1000 | Loss: 0.00003073
Iteration 151/1000 | Loss: 0.00003073
Iteration 152/1000 | Loss: 0.00003073
Iteration 153/1000 | Loss: 0.00003073
Iteration 154/1000 | Loss: 0.00003073
Iteration 155/1000 | Loss: 0.00003073
Iteration 156/1000 | Loss: 0.00003073
Iteration 157/1000 | Loss: 0.00003073
Iteration 158/1000 | Loss: 0.00003073
Iteration 159/1000 | Loss: 0.00003073
Iteration 160/1000 | Loss: 0.00003073
Iteration 161/1000 | Loss: 0.00003073
Iteration 162/1000 | Loss: 0.00003073
Iteration 163/1000 | Loss: 0.00003073
Iteration 164/1000 | Loss: 0.00003073
Iteration 165/1000 | Loss: 0.00003073
Iteration 166/1000 | Loss: 0.00003073
Iteration 167/1000 | Loss: 0.00003073
Iteration 168/1000 | Loss: 0.00003073
Iteration 169/1000 | Loss: 0.00003073
Iteration 170/1000 | Loss: 0.00003073
Iteration 171/1000 | Loss: 0.00003073
Iteration 172/1000 | Loss: 0.00003073
Iteration 173/1000 | Loss: 0.00003073
Iteration 174/1000 | Loss: 0.00003073
Iteration 175/1000 | Loss: 0.00003073
Iteration 176/1000 | Loss: 0.00003073
Iteration 177/1000 | Loss: 0.00003073
Iteration 178/1000 | Loss: 0.00003073
Iteration 179/1000 | Loss: 0.00003073
Iteration 180/1000 | Loss: 0.00003073
Iteration 181/1000 | Loss: 0.00003073
Iteration 182/1000 | Loss: 0.00003073
Iteration 183/1000 | Loss: 0.00003073
Iteration 184/1000 | Loss: 0.00003073
Iteration 185/1000 | Loss: 0.00003073
Iteration 186/1000 | Loss: 0.00003073
Iteration 187/1000 | Loss: 0.00003073
Iteration 188/1000 | Loss: 0.00003073
Iteration 189/1000 | Loss: 0.00003073
Iteration 190/1000 | Loss: 0.00003073
Iteration 191/1000 | Loss: 0.00003073
Iteration 192/1000 | Loss: 0.00003073
Iteration 193/1000 | Loss: 0.00003073
Iteration 194/1000 | Loss: 0.00003073
Iteration 195/1000 | Loss: 0.00003073
Iteration 196/1000 | Loss: 0.00003073
Iteration 197/1000 | Loss: 0.00003073
Iteration 198/1000 | Loss: 0.00003073
Iteration 199/1000 | Loss: 0.00003073
Iteration 200/1000 | Loss: 0.00003073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [3.073034167755395e-05, 3.073034167755395e-05, 3.073034167755395e-05, 3.073034167755395e-05, 3.073034167755395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.073034167755395e-05

Optimization complete. Final v2v error: 4.611299991607666 mm

Highest mean error: 5.207658290863037 mm for frame 188

Lowest mean error: 4.1706624031066895 mm for frame 231

Saving results

Total time: 46.917072772979736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491461
Iteration 2/25 | Loss: 0.00160240
Iteration 3/25 | Loss: 0.00150666
Iteration 4/25 | Loss: 0.00149198
Iteration 5/25 | Loss: 0.00148965
Iteration 6/25 | Loss: 0.00148947
Iteration 7/25 | Loss: 0.00148947
Iteration 8/25 | Loss: 0.00148947
Iteration 9/25 | Loss: 0.00148947
Iteration 10/25 | Loss: 0.00148947
Iteration 11/25 | Loss: 0.00148947
Iteration 12/25 | Loss: 0.00148947
Iteration 13/25 | Loss: 0.00148947
Iteration 14/25 | Loss: 0.00148947
Iteration 15/25 | Loss: 0.00148947
Iteration 16/25 | Loss: 0.00148947
Iteration 17/25 | Loss: 0.00148947
Iteration 18/25 | Loss: 0.00148947
Iteration 19/25 | Loss: 0.00148947
Iteration 20/25 | Loss: 0.00148947
Iteration 21/25 | Loss: 0.00148947
Iteration 22/25 | Loss: 0.00148947
Iteration 23/25 | Loss: 0.00148947
Iteration 24/25 | Loss: 0.00148947
Iteration 25/25 | Loss: 0.00148947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17217922
Iteration 2/25 | Loss: 0.00241691
Iteration 3/25 | Loss: 0.00241691
Iteration 4/25 | Loss: 0.00241691
Iteration 5/25 | Loss: 0.00241691
Iteration 6/25 | Loss: 0.00241691
Iteration 7/25 | Loss: 0.00241691
Iteration 8/25 | Loss: 0.00241691
Iteration 9/25 | Loss: 0.00241691
Iteration 10/25 | Loss: 0.00241691
Iteration 11/25 | Loss: 0.00241691
Iteration 12/25 | Loss: 0.00241691
Iteration 13/25 | Loss: 0.00241691
Iteration 14/25 | Loss: 0.00241691
Iteration 15/25 | Loss: 0.00241691
Iteration 16/25 | Loss: 0.00241691
Iteration 17/25 | Loss: 0.00241691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00241690780967474, 0.00241690780967474, 0.00241690780967474, 0.00241690780967474, 0.00241690780967474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00241690780967474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241691
Iteration 2/1000 | Loss: 0.00008806
Iteration 3/1000 | Loss: 0.00006066
Iteration 4/1000 | Loss: 0.00004519
Iteration 5/1000 | Loss: 0.00003826
Iteration 6/1000 | Loss: 0.00003476
Iteration 7/1000 | Loss: 0.00003317
Iteration 8/1000 | Loss: 0.00003221
Iteration 9/1000 | Loss: 0.00003134
Iteration 10/1000 | Loss: 0.00003075
Iteration 11/1000 | Loss: 0.00003045
Iteration 12/1000 | Loss: 0.00003022
Iteration 13/1000 | Loss: 0.00003019
Iteration 14/1000 | Loss: 0.00003013
Iteration 15/1000 | Loss: 0.00003008
Iteration 16/1000 | Loss: 0.00003007
Iteration 17/1000 | Loss: 0.00003007
Iteration 18/1000 | Loss: 0.00003007
Iteration 19/1000 | Loss: 0.00003006
Iteration 20/1000 | Loss: 0.00003006
Iteration 21/1000 | Loss: 0.00003004
Iteration 22/1000 | Loss: 0.00003004
Iteration 23/1000 | Loss: 0.00003003
Iteration 24/1000 | Loss: 0.00003003
Iteration 25/1000 | Loss: 0.00003003
Iteration 26/1000 | Loss: 0.00003002
Iteration 27/1000 | Loss: 0.00003002
Iteration 28/1000 | Loss: 0.00003002
Iteration 29/1000 | Loss: 0.00003001
Iteration 30/1000 | Loss: 0.00003001
Iteration 31/1000 | Loss: 0.00003001
Iteration 32/1000 | Loss: 0.00003001
Iteration 33/1000 | Loss: 0.00003000
Iteration 34/1000 | Loss: 0.00003000
Iteration 35/1000 | Loss: 0.00003000
Iteration 36/1000 | Loss: 0.00002999
Iteration 37/1000 | Loss: 0.00002999
Iteration 38/1000 | Loss: 0.00002999
Iteration 39/1000 | Loss: 0.00002999
Iteration 40/1000 | Loss: 0.00002999
Iteration 41/1000 | Loss: 0.00002999
Iteration 42/1000 | Loss: 0.00002999
Iteration 43/1000 | Loss: 0.00002999
Iteration 44/1000 | Loss: 0.00002999
Iteration 45/1000 | Loss: 0.00002999
Iteration 46/1000 | Loss: 0.00002999
Iteration 47/1000 | Loss: 0.00002999
Iteration 48/1000 | Loss: 0.00002999
Iteration 49/1000 | Loss: 0.00002999
Iteration 50/1000 | Loss: 0.00002999
Iteration 51/1000 | Loss: 0.00002999
Iteration 52/1000 | Loss: 0.00002999
Iteration 53/1000 | Loss: 0.00002999
Iteration 54/1000 | Loss: 0.00002999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [2.9989625545567833e-05, 2.9989625545567833e-05, 2.9989625545567833e-05, 2.9989625545567833e-05, 2.9989625545567833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9989625545567833e-05

Optimization complete. Final v2v error: 4.619880199432373 mm

Highest mean error: 4.8314208984375 mm for frame 145

Lowest mean error: 4.362312316894531 mm for frame 49

Saving results

Total time: 27.174143075942993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086112
Iteration 2/25 | Loss: 0.00236680
Iteration 3/25 | Loss: 0.00180816
Iteration 4/25 | Loss: 0.00167798
Iteration 5/25 | Loss: 0.00163585
Iteration 6/25 | Loss: 0.00158765
Iteration 7/25 | Loss: 0.00156705
Iteration 8/25 | Loss: 0.00156249
Iteration 9/25 | Loss: 0.00155372
Iteration 10/25 | Loss: 0.00155152
Iteration 11/25 | Loss: 0.00154935
Iteration 12/25 | Loss: 0.00154863
Iteration 13/25 | Loss: 0.00154822
Iteration 14/25 | Loss: 0.00154803
Iteration 15/25 | Loss: 0.00154792
Iteration 16/25 | Loss: 0.00154783
Iteration 17/25 | Loss: 0.00155143
Iteration 18/25 | Loss: 0.00154798
Iteration 19/25 | Loss: 0.00154702
Iteration 20/25 | Loss: 0.00154665
Iteration 21/25 | Loss: 0.00154661
Iteration 22/25 | Loss: 0.00154661
Iteration 23/25 | Loss: 0.00154661
Iteration 24/25 | Loss: 0.00154661
Iteration 25/25 | Loss: 0.00154661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10031688
Iteration 2/25 | Loss: 0.00341348
Iteration 3/25 | Loss: 0.00304436
Iteration 4/25 | Loss: 0.00304436
Iteration 5/25 | Loss: 0.00304436
Iteration 6/25 | Loss: 0.00304436
Iteration 7/25 | Loss: 0.00304436
Iteration 8/25 | Loss: 0.00304435
Iteration 9/25 | Loss: 0.00304435
Iteration 10/25 | Loss: 0.00304435
Iteration 11/25 | Loss: 0.00304435
Iteration 12/25 | Loss: 0.00304435
Iteration 13/25 | Loss: 0.00304435
Iteration 14/25 | Loss: 0.00304435
Iteration 15/25 | Loss: 0.00304435
Iteration 16/25 | Loss: 0.00304435
Iteration 17/25 | Loss: 0.00304435
Iteration 18/25 | Loss: 0.00304435
Iteration 19/25 | Loss: 0.00304435
Iteration 20/25 | Loss: 0.00304435
Iteration 21/25 | Loss: 0.00304435
Iteration 22/25 | Loss: 0.00304435
Iteration 23/25 | Loss: 0.00304435
Iteration 24/25 | Loss: 0.00304435
Iteration 25/25 | Loss: 0.00304435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304435
Iteration 2/1000 | Loss: 0.00049526
Iteration 3/1000 | Loss: 0.00031148
Iteration 4/1000 | Loss: 0.00008081
Iteration 5/1000 | Loss: 0.00030556
Iteration 6/1000 | Loss: 0.00006059
Iteration 7/1000 | Loss: 0.00019848
Iteration 8/1000 | Loss: 0.00010188
Iteration 9/1000 | Loss: 0.00018582
Iteration 10/1000 | Loss: 0.00066242
Iteration 11/1000 | Loss: 0.00007499
Iteration 12/1000 | Loss: 0.00010478
Iteration 13/1000 | Loss: 0.00008036
Iteration 14/1000 | Loss: 0.00007523
Iteration 15/1000 | Loss: 0.00008809
Iteration 16/1000 | Loss: 0.00005772
Iteration 17/1000 | Loss: 0.00016807
Iteration 18/1000 | Loss: 0.00011734
Iteration 19/1000 | Loss: 0.00013250
Iteration 20/1000 | Loss: 0.00019866
Iteration 21/1000 | Loss: 0.00027720
Iteration 22/1000 | Loss: 0.00006802
Iteration 23/1000 | Loss: 0.00005290
Iteration 24/1000 | Loss: 0.00025546
Iteration 25/1000 | Loss: 0.00010718
Iteration 26/1000 | Loss: 0.00004058
Iteration 27/1000 | Loss: 0.00009329
Iteration 28/1000 | Loss: 0.00003574
Iteration 29/1000 | Loss: 0.00017829
Iteration 30/1000 | Loss: 0.00003451
Iteration 31/1000 | Loss: 0.00003335
Iteration 32/1000 | Loss: 0.00014568
Iteration 33/1000 | Loss: 0.00018213
Iteration 34/1000 | Loss: 0.00006781
Iteration 35/1000 | Loss: 0.00003206
Iteration 36/1000 | Loss: 0.00003094
Iteration 37/1000 | Loss: 0.00003025
Iteration 38/1000 | Loss: 0.00002982
Iteration 39/1000 | Loss: 0.00021590
Iteration 40/1000 | Loss: 0.00003012
Iteration 41/1000 | Loss: 0.00002956
Iteration 42/1000 | Loss: 0.00002944
Iteration 43/1000 | Loss: 0.00002943
Iteration 44/1000 | Loss: 0.00002934
Iteration 45/1000 | Loss: 0.00002929
Iteration 46/1000 | Loss: 0.00002927
Iteration 47/1000 | Loss: 0.00002926
Iteration 48/1000 | Loss: 0.00002926
Iteration 49/1000 | Loss: 0.00002925
Iteration 50/1000 | Loss: 0.00002925
Iteration 51/1000 | Loss: 0.00002924
Iteration 52/1000 | Loss: 0.00002923
Iteration 53/1000 | Loss: 0.00002923
Iteration 54/1000 | Loss: 0.00002923
Iteration 55/1000 | Loss: 0.00002922
Iteration 56/1000 | Loss: 0.00002922
Iteration 57/1000 | Loss: 0.00002922
Iteration 58/1000 | Loss: 0.00002922
Iteration 59/1000 | Loss: 0.00002922
Iteration 60/1000 | Loss: 0.00002922
Iteration 61/1000 | Loss: 0.00002922
Iteration 62/1000 | Loss: 0.00002920
Iteration 63/1000 | Loss: 0.00002920
Iteration 64/1000 | Loss: 0.00002919
Iteration 65/1000 | Loss: 0.00002918
Iteration 66/1000 | Loss: 0.00002918
Iteration 67/1000 | Loss: 0.00002917
Iteration 68/1000 | Loss: 0.00002917
Iteration 69/1000 | Loss: 0.00002916
Iteration 70/1000 | Loss: 0.00002916
Iteration 71/1000 | Loss: 0.00002915
Iteration 72/1000 | Loss: 0.00002915
Iteration 73/1000 | Loss: 0.00002914
Iteration 74/1000 | Loss: 0.00002914
Iteration 75/1000 | Loss: 0.00002914
Iteration 76/1000 | Loss: 0.00002913
Iteration 77/1000 | Loss: 0.00002913
Iteration 78/1000 | Loss: 0.00002913
Iteration 79/1000 | Loss: 0.00002913
Iteration 80/1000 | Loss: 0.00002913
Iteration 81/1000 | Loss: 0.00002912
Iteration 82/1000 | Loss: 0.00002912
Iteration 83/1000 | Loss: 0.00002912
Iteration 84/1000 | Loss: 0.00002912
Iteration 85/1000 | Loss: 0.00002912
Iteration 86/1000 | Loss: 0.00002912
Iteration 87/1000 | Loss: 0.00002912
Iteration 88/1000 | Loss: 0.00002912
Iteration 89/1000 | Loss: 0.00002912
Iteration 90/1000 | Loss: 0.00002911
Iteration 91/1000 | Loss: 0.00002911
Iteration 92/1000 | Loss: 0.00002910
Iteration 93/1000 | Loss: 0.00002910
Iteration 94/1000 | Loss: 0.00002910
Iteration 95/1000 | Loss: 0.00002910
Iteration 96/1000 | Loss: 0.00002909
Iteration 97/1000 | Loss: 0.00002909
Iteration 98/1000 | Loss: 0.00002909
Iteration 99/1000 | Loss: 0.00002908
Iteration 100/1000 | Loss: 0.00002908
Iteration 101/1000 | Loss: 0.00002907
Iteration 102/1000 | Loss: 0.00002905
Iteration 103/1000 | Loss: 0.00002904
Iteration 104/1000 | Loss: 0.00002904
Iteration 105/1000 | Loss: 0.00002904
Iteration 106/1000 | Loss: 0.00002903
Iteration 107/1000 | Loss: 0.00002903
Iteration 108/1000 | Loss: 0.00002903
Iteration 109/1000 | Loss: 0.00002902
Iteration 110/1000 | Loss: 0.00002902
Iteration 111/1000 | Loss: 0.00002902
Iteration 112/1000 | Loss: 0.00002901
Iteration 113/1000 | Loss: 0.00002901
Iteration 114/1000 | Loss: 0.00002901
Iteration 115/1000 | Loss: 0.00002901
Iteration 116/1000 | Loss: 0.00002900
Iteration 117/1000 | Loss: 0.00002900
Iteration 118/1000 | Loss: 0.00002900
Iteration 119/1000 | Loss: 0.00002900
Iteration 120/1000 | Loss: 0.00002899
Iteration 121/1000 | Loss: 0.00002899
Iteration 122/1000 | Loss: 0.00002899
Iteration 123/1000 | Loss: 0.00002899
Iteration 124/1000 | Loss: 0.00002899
Iteration 125/1000 | Loss: 0.00002899
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002899
Iteration 128/1000 | Loss: 0.00002899
Iteration 129/1000 | Loss: 0.00002899
Iteration 130/1000 | Loss: 0.00002898
Iteration 131/1000 | Loss: 0.00002898
Iteration 132/1000 | Loss: 0.00002898
Iteration 133/1000 | Loss: 0.00002898
Iteration 134/1000 | Loss: 0.00002898
Iteration 135/1000 | Loss: 0.00002898
Iteration 136/1000 | Loss: 0.00002898
Iteration 137/1000 | Loss: 0.00002897
Iteration 138/1000 | Loss: 0.00002897
Iteration 139/1000 | Loss: 0.00002897
Iteration 140/1000 | Loss: 0.00002897
Iteration 141/1000 | Loss: 0.00002897
Iteration 142/1000 | Loss: 0.00002897
Iteration 143/1000 | Loss: 0.00002897
Iteration 144/1000 | Loss: 0.00002897
Iteration 145/1000 | Loss: 0.00002897
Iteration 146/1000 | Loss: 0.00002897
Iteration 147/1000 | Loss: 0.00002896
Iteration 148/1000 | Loss: 0.00002896
Iteration 149/1000 | Loss: 0.00002896
Iteration 150/1000 | Loss: 0.00002896
Iteration 151/1000 | Loss: 0.00002896
Iteration 152/1000 | Loss: 0.00002896
Iteration 153/1000 | Loss: 0.00002896
Iteration 154/1000 | Loss: 0.00002896
Iteration 155/1000 | Loss: 0.00002895
Iteration 156/1000 | Loss: 0.00002895
Iteration 157/1000 | Loss: 0.00002895
Iteration 158/1000 | Loss: 0.00002895
Iteration 159/1000 | Loss: 0.00002894
Iteration 160/1000 | Loss: 0.00002894
Iteration 161/1000 | Loss: 0.00002894
Iteration 162/1000 | Loss: 0.00002894
Iteration 163/1000 | Loss: 0.00002894
Iteration 164/1000 | Loss: 0.00002894
Iteration 165/1000 | Loss: 0.00002894
Iteration 166/1000 | Loss: 0.00002894
Iteration 167/1000 | Loss: 0.00002894
Iteration 168/1000 | Loss: 0.00002894
Iteration 169/1000 | Loss: 0.00002894
Iteration 170/1000 | Loss: 0.00002894
Iteration 171/1000 | Loss: 0.00002893
Iteration 172/1000 | Loss: 0.00002893
Iteration 173/1000 | Loss: 0.00002893
Iteration 174/1000 | Loss: 0.00002893
Iteration 175/1000 | Loss: 0.00002893
Iteration 176/1000 | Loss: 0.00002893
Iteration 177/1000 | Loss: 0.00002893
Iteration 178/1000 | Loss: 0.00002893
Iteration 179/1000 | Loss: 0.00002893
Iteration 180/1000 | Loss: 0.00002893
Iteration 181/1000 | Loss: 0.00002893
Iteration 182/1000 | Loss: 0.00002893
Iteration 183/1000 | Loss: 0.00002893
Iteration 184/1000 | Loss: 0.00002893
Iteration 185/1000 | Loss: 0.00002892
Iteration 186/1000 | Loss: 0.00002892
Iteration 187/1000 | Loss: 0.00002892
Iteration 188/1000 | Loss: 0.00002892
Iteration 189/1000 | Loss: 0.00002892
Iteration 190/1000 | Loss: 0.00002892
Iteration 191/1000 | Loss: 0.00002892
Iteration 192/1000 | Loss: 0.00002892
Iteration 193/1000 | Loss: 0.00002892
Iteration 194/1000 | Loss: 0.00002892
Iteration 195/1000 | Loss: 0.00002892
Iteration 196/1000 | Loss: 0.00002892
Iteration 197/1000 | Loss: 0.00002892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.8923539503011853e-05, 2.8923539503011853e-05, 2.8923539503011853e-05, 2.8923539503011853e-05, 2.8923539503011853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8923539503011853e-05

Optimization complete. Final v2v error: 4.448592185974121 mm

Highest mean error: 12.138410568237305 mm for frame 211

Lowest mean error: 3.8825926780700684 mm for frame 213

Saving results

Total time: 120.23100733757019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787448
Iteration 2/25 | Loss: 0.00193096
Iteration 3/25 | Loss: 0.00159317
Iteration 4/25 | Loss: 0.00156872
Iteration 5/25 | Loss: 0.00156407
Iteration 6/25 | Loss: 0.00156251
Iteration 7/25 | Loss: 0.00156193
Iteration 8/25 | Loss: 0.00156191
Iteration 9/25 | Loss: 0.00156191
Iteration 10/25 | Loss: 0.00156191
Iteration 11/25 | Loss: 0.00156191
Iteration 12/25 | Loss: 0.00156191
Iteration 13/25 | Loss: 0.00156191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015619146870449185, 0.0015619146870449185, 0.0015619146870449185, 0.0015619146870449185, 0.0015619146870449185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015619146870449185

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58285922
Iteration 2/25 | Loss: 0.00175774
Iteration 3/25 | Loss: 0.00175773
Iteration 4/25 | Loss: 0.00175773
Iteration 5/25 | Loss: 0.00175773
Iteration 6/25 | Loss: 0.00175773
Iteration 7/25 | Loss: 0.00175773
Iteration 8/25 | Loss: 0.00175773
Iteration 9/25 | Loss: 0.00175773
Iteration 10/25 | Loss: 0.00175773
Iteration 11/25 | Loss: 0.00175773
Iteration 12/25 | Loss: 0.00175773
Iteration 13/25 | Loss: 0.00175773
Iteration 14/25 | Loss: 0.00175773
Iteration 15/25 | Loss: 0.00175773
Iteration 16/25 | Loss: 0.00175773
Iteration 17/25 | Loss: 0.00175773
Iteration 18/25 | Loss: 0.00175773
Iteration 19/25 | Loss: 0.00175773
Iteration 20/25 | Loss: 0.00175773
Iteration 21/25 | Loss: 0.00175773
Iteration 22/25 | Loss: 0.00175773
Iteration 23/25 | Loss: 0.00175773
Iteration 24/25 | Loss: 0.00175773
Iteration 25/25 | Loss: 0.00175773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175773
Iteration 2/1000 | Loss: 0.00011389
Iteration 3/1000 | Loss: 0.00006873
Iteration 4/1000 | Loss: 0.00005109
Iteration 5/1000 | Loss: 0.00004523
Iteration 6/1000 | Loss: 0.00004075
Iteration 7/1000 | Loss: 0.00003872
Iteration 8/1000 | Loss: 0.00003677
Iteration 9/1000 | Loss: 0.00003550
Iteration 10/1000 | Loss: 0.00003472
Iteration 11/1000 | Loss: 0.00003418
Iteration 12/1000 | Loss: 0.00003378
Iteration 13/1000 | Loss: 0.00003355
Iteration 14/1000 | Loss: 0.00003338
Iteration 15/1000 | Loss: 0.00003323
Iteration 16/1000 | Loss: 0.00003321
Iteration 17/1000 | Loss: 0.00003318
Iteration 18/1000 | Loss: 0.00003317
Iteration 19/1000 | Loss: 0.00003310
Iteration 20/1000 | Loss: 0.00003303
Iteration 21/1000 | Loss: 0.00003297
Iteration 22/1000 | Loss: 0.00003296
Iteration 23/1000 | Loss: 0.00003294
Iteration 24/1000 | Loss: 0.00003294
Iteration 25/1000 | Loss: 0.00003294
Iteration 26/1000 | Loss: 0.00003294
Iteration 27/1000 | Loss: 0.00003294
Iteration 28/1000 | Loss: 0.00003294
Iteration 29/1000 | Loss: 0.00003294
Iteration 30/1000 | Loss: 0.00003293
Iteration 31/1000 | Loss: 0.00003293
Iteration 32/1000 | Loss: 0.00003293
Iteration 33/1000 | Loss: 0.00003293
Iteration 34/1000 | Loss: 0.00003292
Iteration 35/1000 | Loss: 0.00003292
Iteration 36/1000 | Loss: 0.00003291
Iteration 37/1000 | Loss: 0.00003291
Iteration 38/1000 | Loss: 0.00003290
Iteration 39/1000 | Loss: 0.00003290
Iteration 40/1000 | Loss: 0.00003290
Iteration 41/1000 | Loss: 0.00003290
Iteration 42/1000 | Loss: 0.00003290
Iteration 43/1000 | Loss: 0.00003290
Iteration 44/1000 | Loss: 0.00003290
Iteration 45/1000 | Loss: 0.00003290
Iteration 46/1000 | Loss: 0.00003290
Iteration 47/1000 | Loss: 0.00003290
Iteration 48/1000 | Loss: 0.00003290
Iteration 49/1000 | Loss: 0.00003289
Iteration 50/1000 | Loss: 0.00003289
Iteration 51/1000 | Loss: 0.00003289
Iteration 52/1000 | Loss: 0.00003289
Iteration 53/1000 | Loss: 0.00003288
Iteration 54/1000 | Loss: 0.00003288
Iteration 55/1000 | Loss: 0.00003288
Iteration 56/1000 | Loss: 0.00003288
Iteration 57/1000 | Loss: 0.00003288
Iteration 58/1000 | Loss: 0.00003288
Iteration 59/1000 | Loss: 0.00003288
Iteration 60/1000 | Loss: 0.00003288
Iteration 61/1000 | Loss: 0.00003288
Iteration 62/1000 | Loss: 0.00003288
Iteration 63/1000 | Loss: 0.00003287
Iteration 64/1000 | Loss: 0.00003287
Iteration 65/1000 | Loss: 0.00003287
Iteration 66/1000 | Loss: 0.00003287
Iteration 67/1000 | Loss: 0.00003287
Iteration 68/1000 | Loss: 0.00003285
Iteration 69/1000 | Loss: 0.00003285
Iteration 70/1000 | Loss: 0.00003285
Iteration 71/1000 | Loss: 0.00003285
Iteration 72/1000 | Loss: 0.00003285
Iteration 73/1000 | Loss: 0.00003285
Iteration 74/1000 | Loss: 0.00003285
Iteration 75/1000 | Loss: 0.00003285
Iteration 76/1000 | Loss: 0.00003285
Iteration 77/1000 | Loss: 0.00003285
Iteration 78/1000 | Loss: 0.00003284
Iteration 79/1000 | Loss: 0.00003284
Iteration 80/1000 | Loss: 0.00003284
Iteration 81/1000 | Loss: 0.00003284
Iteration 82/1000 | Loss: 0.00003284
Iteration 83/1000 | Loss: 0.00003283
Iteration 84/1000 | Loss: 0.00003283
Iteration 85/1000 | Loss: 0.00003283
Iteration 86/1000 | Loss: 0.00003283
Iteration 87/1000 | Loss: 0.00003283
Iteration 88/1000 | Loss: 0.00003282
Iteration 89/1000 | Loss: 0.00003282
Iteration 90/1000 | Loss: 0.00003282
Iteration 91/1000 | Loss: 0.00003282
Iteration 92/1000 | Loss: 0.00003282
Iteration 93/1000 | Loss: 0.00003282
Iteration 94/1000 | Loss: 0.00003281
Iteration 95/1000 | Loss: 0.00003281
Iteration 96/1000 | Loss: 0.00003281
Iteration 97/1000 | Loss: 0.00003281
Iteration 98/1000 | Loss: 0.00003280
Iteration 99/1000 | Loss: 0.00003280
Iteration 100/1000 | Loss: 0.00003280
Iteration 101/1000 | Loss: 0.00003280
Iteration 102/1000 | Loss: 0.00003280
Iteration 103/1000 | Loss: 0.00003279
Iteration 104/1000 | Loss: 0.00003279
Iteration 105/1000 | Loss: 0.00003279
Iteration 106/1000 | Loss: 0.00003279
Iteration 107/1000 | Loss: 0.00003279
Iteration 108/1000 | Loss: 0.00003279
Iteration 109/1000 | Loss: 0.00003279
Iteration 110/1000 | Loss: 0.00003279
Iteration 111/1000 | Loss: 0.00003279
Iteration 112/1000 | Loss: 0.00003279
Iteration 113/1000 | Loss: 0.00003279
Iteration 114/1000 | Loss: 0.00003279
Iteration 115/1000 | Loss: 0.00003279
Iteration 116/1000 | Loss: 0.00003279
Iteration 117/1000 | Loss: 0.00003278
Iteration 118/1000 | Loss: 0.00003278
Iteration 119/1000 | Loss: 0.00003278
Iteration 120/1000 | Loss: 0.00003278
Iteration 121/1000 | Loss: 0.00003278
Iteration 122/1000 | Loss: 0.00003278
Iteration 123/1000 | Loss: 0.00003277
Iteration 124/1000 | Loss: 0.00003277
Iteration 125/1000 | Loss: 0.00003277
Iteration 126/1000 | Loss: 0.00003277
Iteration 127/1000 | Loss: 0.00003277
Iteration 128/1000 | Loss: 0.00003277
Iteration 129/1000 | Loss: 0.00003277
Iteration 130/1000 | Loss: 0.00003277
Iteration 131/1000 | Loss: 0.00003277
Iteration 132/1000 | Loss: 0.00003277
Iteration 133/1000 | Loss: 0.00003277
Iteration 134/1000 | Loss: 0.00003276
Iteration 135/1000 | Loss: 0.00003276
Iteration 136/1000 | Loss: 0.00003276
Iteration 137/1000 | Loss: 0.00003276
Iteration 138/1000 | Loss: 0.00003276
Iteration 139/1000 | Loss: 0.00003276
Iteration 140/1000 | Loss: 0.00003276
Iteration 141/1000 | Loss: 0.00003276
Iteration 142/1000 | Loss: 0.00003276
Iteration 143/1000 | Loss: 0.00003276
Iteration 144/1000 | Loss: 0.00003276
Iteration 145/1000 | Loss: 0.00003276
Iteration 146/1000 | Loss: 0.00003276
Iteration 147/1000 | Loss: 0.00003276
Iteration 148/1000 | Loss: 0.00003276
Iteration 149/1000 | Loss: 0.00003276
Iteration 150/1000 | Loss: 0.00003276
Iteration 151/1000 | Loss: 0.00003276
Iteration 152/1000 | Loss: 0.00003276
Iteration 153/1000 | Loss: 0.00003276
Iteration 154/1000 | Loss: 0.00003276
Iteration 155/1000 | Loss: 0.00003276
Iteration 156/1000 | Loss: 0.00003276
Iteration 157/1000 | Loss: 0.00003276
Iteration 158/1000 | Loss: 0.00003276
Iteration 159/1000 | Loss: 0.00003276
Iteration 160/1000 | Loss: 0.00003276
Iteration 161/1000 | Loss: 0.00003276
Iteration 162/1000 | Loss: 0.00003276
Iteration 163/1000 | Loss: 0.00003276
Iteration 164/1000 | Loss: 0.00003276
Iteration 165/1000 | Loss: 0.00003276
Iteration 166/1000 | Loss: 0.00003276
Iteration 167/1000 | Loss: 0.00003276
Iteration 168/1000 | Loss: 0.00003276
Iteration 169/1000 | Loss: 0.00003276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.275741983088665e-05, 3.275741983088665e-05, 3.275741983088665e-05, 3.275741983088665e-05, 3.275741983088665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.275741983088665e-05

Optimization complete. Final v2v error: 4.804893970489502 mm

Highest mean error: 6.13484001159668 mm for frame 34

Lowest mean error: 4.092015743255615 mm for frame 0

Saving results

Total time: 43.003146171569824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593766
Iteration 2/25 | Loss: 0.00166568
Iteration 3/25 | Loss: 0.00153947
Iteration 4/25 | Loss: 0.00152940
Iteration 5/25 | Loss: 0.00152709
Iteration 6/25 | Loss: 0.00152709
Iteration 7/25 | Loss: 0.00152709
Iteration 8/25 | Loss: 0.00152709
Iteration 9/25 | Loss: 0.00152709
Iteration 10/25 | Loss: 0.00152709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015270945150405169, 0.0015270945150405169, 0.0015270945150405169, 0.0015270945150405169, 0.0015270945150405169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015270945150405169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18359041
Iteration 2/25 | Loss: 0.00215050
Iteration 3/25 | Loss: 0.00215049
Iteration 4/25 | Loss: 0.00215049
Iteration 5/25 | Loss: 0.00215049
Iteration 6/25 | Loss: 0.00215049
Iteration 7/25 | Loss: 0.00215049
Iteration 8/25 | Loss: 0.00215049
Iteration 9/25 | Loss: 0.00215049
Iteration 10/25 | Loss: 0.00215049
Iteration 11/25 | Loss: 0.00215049
Iteration 12/25 | Loss: 0.00215049
Iteration 13/25 | Loss: 0.00215049
Iteration 14/25 | Loss: 0.00215049
Iteration 15/25 | Loss: 0.00215049
Iteration 16/25 | Loss: 0.00215049
Iteration 17/25 | Loss: 0.00215049
Iteration 18/25 | Loss: 0.00215049
Iteration 19/25 | Loss: 0.00215049
Iteration 20/25 | Loss: 0.00215049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002150491112843156, 0.002150491112843156, 0.002150491112843156, 0.002150491112843156, 0.002150491112843156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002150491112843156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215049
Iteration 2/1000 | Loss: 0.00009331
Iteration 3/1000 | Loss: 0.00006260
Iteration 4/1000 | Loss: 0.00004922
Iteration 5/1000 | Loss: 0.00004375
Iteration 6/1000 | Loss: 0.00004184
Iteration 7/1000 | Loss: 0.00004107
Iteration 8/1000 | Loss: 0.00004031
Iteration 9/1000 | Loss: 0.00003982
Iteration 10/1000 | Loss: 0.00003952
Iteration 11/1000 | Loss: 0.00003935
Iteration 12/1000 | Loss: 0.00003931
Iteration 13/1000 | Loss: 0.00003927
Iteration 14/1000 | Loss: 0.00003926
Iteration 15/1000 | Loss: 0.00003914
Iteration 16/1000 | Loss: 0.00003913
Iteration 17/1000 | Loss: 0.00003913
Iteration 18/1000 | Loss: 0.00003912
Iteration 19/1000 | Loss: 0.00003912
Iteration 20/1000 | Loss: 0.00003909
Iteration 21/1000 | Loss: 0.00003909
Iteration 22/1000 | Loss: 0.00003909
Iteration 23/1000 | Loss: 0.00003909
Iteration 24/1000 | Loss: 0.00003909
Iteration 25/1000 | Loss: 0.00003909
Iteration 26/1000 | Loss: 0.00003909
Iteration 27/1000 | Loss: 0.00003908
Iteration 28/1000 | Loss: 0.00003908
Iteration 29/1000 | Loss: 0.00003908
Iteration 30/1000 | Loss: 0.00003905
Iteration 31/1000 | Loss: 0.00003905
Iteration 32/1000 | Loss: 0.00003905
Iteration 33/1000 | Loss: 0.00003903
Iteration 34/1000 | Loss: 0.00003903
Iteration 35/1000 | Loss: 0.00003902
Iteration 36/1000 | Loss: 0.00003901
Iteration 37/1000 | Loss: 0.00003898
Iteration 38/1000 | Loss: 0.00003898
Iteration 39/1000 | Loss: 0.00003895
Iteration 40/1000 | Loss: 0.00003894
Iteration 41/1000 | Loss: 0.00003894
Iteration 42/1000 | Loss: 0.00003893
Iteration 43/1000 | Loss: 0.00003893
Iteration 44/1000 | Loss: 0.00003893
Iteration 45/1000 | Loss: 0.00003893
Iteration 46/1000 | Loss: 0.00003893
Iteration 47/1000 | Loss: 0.00003893
Iteration 48/1000 | Loss: 0.00003892
Iteration 49/1000 | Loss: 0.00003892
Iteration 50/1000 | Loss: 0.00003891
Iteration 51/1000 | Loss: 0.00003891
Iteration 52/1000 | Loss: 0.00003891
Iteration 53/1000 | Loss: 0.00003890
Iteration 54/1000 | Loss: 0.00003890
Iteration 55/1000 | Loss: 0.00003890
Iteration 56/1000 | Loss: 0.00003889
Iteration 57/1000 | Loss: 0.00003889
Iteration 58/1000 | Loss: 0.00003889
Iteration 59/1000 | Loss: 0.00003889
Iteration 60/1000 | Loss: 0.00003889
Iteration 61/1000 | Loss: 0.00003889
Iteration 62/1000 | Loss: 0.00003889
Iteration 63/1000 | Loss: 0.00003889
Iteration 64/1000 | Loss: 0.00003888
Iteration 65/1000 | Loss: 0.00003888
Iteration 66/1000 | Loss: 0.00003888
Iteration 67/1000 | Loss: 0.00003888
Iteration 68/1000 | Loss: 0.00003888
Iteration 69/1000 | Loss: 0.00003888
Iteration 70/1000 | Loss: 0.00003888
Iteration 71/1000 | Loss: 0.00003888
Iteration 72/1000 | Loss: 0.00003887
Iteration 73/1000 | Loss: 0.00003887
Iteration 74/1000 | Loss: 0.00003887
Iteration 75/1000 | Loss: 0.00003887
Iteration 76/1000 | Loss: 0.00003887
Iteration 77/1000 | Loss: 0.00003887
Iteration 78/1000 | Loss: 0.00003887
Iteration 79/1000 | Loss: 0.00003887
Iteration 80/1000 | Loss: 0.00003887
Iteration 81/1000 | Loss: 0.00003887
Iteration 82/1000 | Loss: 0.00003887
Iteration 83/1000 | Loss: 0.00003887
Iteration 84/1000 | Loss: 0.00003887
Iteration 85/1000 | Loss: 0.00003886
Iteration 86/1000 | Loss: 0.00003886
Iteration 87/1000 | Loss: 0.00003886
Iteration 88/1000 | Loss: 0.00003886
Iteration 89/1000 | Loss: 0.00003886
Iteration 90/1000 | Loss: 0.00003886
Iteration 91/1000 | Loss: 0.00003886
Iteration 92/1000 | Loss: 0.00003886
Iteration 93/1000 | Loss: 0.00003886
Iteration 94/1000 | Loss: 0.00003886
Iteration 95/1000 | Loss: 0.00003886
Iteration 96/1000 | Loss: 0.00003886
Iteration 97/1000 | Loss: 0.00003886
Iteration 98/1000 | Loss: 0.00003886
Iteration 99/1000 | Loss: 0.00003886
Iteration 100/1000 | Loss: 0.00003886
Iteration 101/1000 | Loss: 0.00003886
Iteration 102/1000 | Loss: 0.00003886
Iteration 103/1000 | Loss: 0.00003886
Iteration 104/1000 | Loss: 0.00003886
Iteration 105/1000 | Loss: 0.00003886
Iteration 106/1000 | Loss: 0.00003886
Iteration 107/1000 | Loss: 0.00003886
Iteration 108/1000 | Loss: 0.00003886
Iteration 109/1000 | Loss: 0.00003886
Iteration 110/1000 | Loss: 0.00003886
Iteration 111/1000 | Loss: 0.00003886
Iteration 112/1000 | Loss: 0.00003886
Iteration 113/1000 | Loss: 0.00003886
Iteration 114/1000 | Loss: 0.00003886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.8862417568452656e-05, 3.8862417568452656e-05, 3.8862417568452656e-05, 3.8862417568452656e-05, 3.8862417568452656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8862417568452656e-05

Optimization complete. Final v2v error: 5.209778785705566 mm

Highest mean error: 5.713355541229248 mm for frame 56

Lowest mean error: 4.640335559844971 mm for frame 235

Saving results

Total time: 36.72296166419983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007560
Iteration 2/25 | Loss: 0.00183513
Iteration 3/25 | Loss: 0.00163625
Iteration 4/25 | Loss: 0.00156974
Iteration 5/25 | Loss: 0.00155856
Iteration 6/25 | Loss: 0.00155545
Iteration 7/25 | Loss: 0.00155428
Iteration 8/25 | Loss: 0.00155392
Iteration 9/25 | Loss: 0.00155375
Iteration 10/25 | Loss: 0.00155364
Iteration 11/25 | Loss: 0.00155356
Iteration 12/25 | Loss: 0.00155354
Iteration 13/25 | Loss: 0.00155354
Iteration 14/25 | Loss: 0.00155343
Iteration 15/25 | Loss: 0.00156483
Iteration 16/25 | Loss: 0.00155370
Iteration 17/25 | Loss: 0.00154549
Iteration 18/25 | Loss: 0.00154478
Iteration 19/25 | Loss: 0.00154466
Iteration 20/25 | Loss: 0.00154465
Iteration 21/25 | Loss: 0.00154465
Iteration 22/25 | Loss: 0.00154465
Iteration 23/25 | Loss: 0.00154465
Iteration 24/25 | Loss: 0.00154465
Iteration 25/25 | Loss: 0.00154465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13822842
Iteration 2/25 | Loss: 0.00245266
Iteration 3/25 | Loss: 0.00245265
Iteration 4/25 | Loss: 0.00245265
Iteration 5/25 | Loss: 0.00245265
Iteration 6/25 | Loss: 0.00245265
Iteration 7/25 | Loss: 0.00245265
Iteration 8/25 | Loss: 0.00245265
Iteration 9/25 | Loss: 0.00245265
Iteration 10/25 | Loss: 0.00245265
Iteration 11/25 | Loss: 0.00245265
Iteration 12/25 | Loss: 0.00245265
Iteration 13/25 | Loss: 0.00245265
Iteration 14/25 | Loss: 0.00245265
Iteration 15/25 | Loss: 0.00245265
Iteration 16/25 | Loss: 0.00245265
Iteration 17/25 | Loss: 0.00245265
Iteration 18/25 | Loss: 0.00245265
Iteration 19/25 | Loss: 0.00245265
Iteration 20/25 | Loss: 0.00245265
Iteration 21/25 | Loss: 0.00245265
Iteration 22/25 | Loss: 0.00245265
Iteration 23/25 | Loss: 0.00245265
Iteration 24/25 | Loss: 0.00245265
Iteration 25/25 | Loss: 0.00245265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245265
Iteration 2/1000 | Loss: 0.00012841
Iteration 3/1000 | Loss: 0.00008768
Iteration 4/1000 | Loss: 0.00006374
Iteration 5/1000 | Loss: 0.00005233
Iteration 6/1000 | Loss: 0.00078859
Iteration 7/1000 | Loss: 0.00033277
Iteration 8/1000 | Loss: 0.00042451
Iteration 9/1000 | Loss: 0.00014355
Iteration 10/1000 | Loss: 0.00051753
Iteration 11/1000 | Loss: 0.00015277
Iteration 12/1000 | Loss: 0.00035432
Iteration 13/1000 | Loss: 0.00071928
Iteration 14/1000 | Loss: 0.00005335
Iteration 15/1000 | Loss: 0.00004471
Iteration 16/1000 | Loss: 0.00004250
Iteration 17/1000 | Loss: 0.00033016
Iteration 18/1000 | Loss: 0.00042879
Iteration 19/1000 | Loss: 0.00026451
Iteration 20/1000 | Loss: 0.00037755
Iteration 21/1000 | Loss: 0.00033224
Iteration 22/1000 | Loss: 0.00028233
Iteration 23/1000 | Loss: 0.00030282
Iteration 24/1000 | Loss: 0.00038963
Iteration 25/1000 | Loss: 0.00033267
Iteration 26/1000 | Loss: 0.00043355
Iteration 27/1000 | Loss: 0.00047447
Iteration 28/1000 | Loss: 0.00018678
Iteration 29/1000 | Loss: 0.00026171
Iteration 30/1000 | Loss: 0.00007619
Iteration 31/1000 | Loss: 0.00043167
Iteration 32/1000 | Loss: 0.00008665
Iteration 33/1000 | Loss: 0.00027846
Iteration 34/1000 | Loss: 0.00021889
Iteration 35/1000 | Loss: 0.00013589
Iteration 36/1000 | Loss: 0.00034147
Iteration 37/1000 | Loss: 0.00038783
Iteration 38/1000 | Loss: 0.00044719
Iteration 39/1000 | Loss: 0.00029329
Iteration 40/1000 | Loss: 0.00016858
Iteration 41/1000 | Loss: 0.00015100
Iteration 42/1000 | Loss: 0.00010039
Iteration 43/1000 | Loss: 0.00044064
Iteration 44/1000 | Loss: 0.00046628
Iteration 45/1000 | Loss: 0.00007577
Iteration 46/1000 | Loss: 0.00016491
Iteration 47/1000 | Loss: 0.00005479
Iteration 48/1000 | Loss: 0.00004440
Iteration 49/1000 | Loss: 0.00004293
Iteration 50/1000 | Loss: 0.00080573
Iteration 51/1000 | Loss: 0.00061258
Iteration 52/1000 | Loss: 0.00032814
Iteration 53/1000 | Loss: 0.00016585
Iteration 54/1000 | Loss: 0.00004324
Iteration 55/1000 | Loss: 0.00003750
Iteration 56/1000 | Loss: 0.00003551
Iteration 57/1000 | Loss: 0.00003384
Iteration 58/1000 | Loss: 0.00003270
Iteration 59/1000 | Loss: 0.00003211
Iteration 60/1000 | Loss: 0.00003178
Iteration 61/1000 | Loss: 0.00003137
Iteration 62/1000 | Loss: 0.00003101
Iteration 63/1000 | Loss: 0.00003076
Iteration 64/1000 | Loss: 0.00003045
Iteration 65/1000 | Loss: 0.00003012
Iteration 66/1000 | Loss: 0.00003007
Iteration 67/1000 | Loss: 0.00002992
Iteration 68/1000 | Loss: 0.00002990
Iteration 69/1000 | Loss: 0.00002982
Iteration 70/1000 | Loss: 0.00002982
Iteration 71/1000 | Loss: 0.00002981
Iteration 72/1000 | Loss: 0.00002979
Iteration 73/1000 | Loss: 0.00002979
Iteration 74/1000 | Loss: 0.00002978
Iteration 75/1000 | Loss: 0.00002974
Iteration 76/1000 | Loss: 0.00002974
Iteration 77/1000 | Loss: 0.00002968
Iteration 78/1000 | Loss: 0.00002965
Iteration 79/1000 | Loss: 0.00002965
Iteration 80/1000 | Loss: 0.00002964
Iteration 81/1000 | Loss: 0.00002964
Iteration 82/1000 | Loss: 0.00002964
Iteration 83/1000 | Loss: 0.00002963
Iteration 84/1000 | Loss: 0.00002963
Iteration 85/1000 | Loss: 0.00002963
Iteration 86/1000 | Loss: 0.00002963
Iteration 87/1000 | Loss: 0.00002962
Iteration 88/1000 | Loss: 0.00002962
Iteration 89/1000 | Loss: 0.00002962
Iteration 90/1000 | Loss: 0.00002962
Iteration 91/1000 | Loss: 0.00002962
Iteration 92/1000 | Loss: 0.00002962
Iteration 93/1000 | Loss: 0.00002962
Iteration 94/1000 | Loss: 0.00002962
Iteration 95/1000 | Loss: 0.00002962
Iteration 96/1000 | Loss: 0.00002962
Iteration 97/1000 | Loss: 0.00002962
Iteration 98/1000 | Loss: 0.00002962
Iteration 99/1000 | Loss: 0.00002962
Iteration 100/1000 | Loss: 0.00002962
Iteration 101/1000 | Loss: 0.00002961
Iteration 102/1000 | Loss: 0.00002961
Iteration 103/1000 | Loss: 0.00002961
Iteration 104/1000 | Loss: 0.00002961
Iteration 105/1000 | Loss: 0.00002961
Iteration 106/1000 | Loss: 0.00002961
Iteration 107/1000 | Loss: 0.00002961
Iteration 108/1000 | Loss: 0.00002961
Iteration 109/1000 | Loss: 0.00002960
Iteration 110/1000 | Loss: 0.00002960
Iteration 111/1000 | Loss: 0.00002960
Iteration 112/1000 | Loss: 0.00002960
Iteration 113/1000 | Loss: 0.00002960
Iteration 114/1000 | Loss: 0.00002960
Iteration 115/1000 | Loss: 0.00002959
Iteration 116/1000 | Loss: 0.00002959
Iteration 117/1000 | Loss: 0.00002959
Iteration 118/1000 | Loss: 0.00002959
Iteration 119/1000 | Loss: 0.00002959
Iteration 120/1000 | Loss: 0.00002958
Iteration 121/1000 | Loss: 0.00002958
Iteration 122/1000 | Loss: 0.00002958
Iteration 123/1000 | Loss: 0.00002958
Iteration 124/1000 | Loss: 0.00002957
Iteration 125/1000 | Loss: 0.00002957
Iteration 126/1000 | Loss: 0.00002957
Iteration 127/1000 | Loss: 0.00002957
Iteration 128/1000 | Loss: 0.00002957
Iteration 129/1000 | Loss: 0.00002956
Iteration 130/1000 | Loss: 0.00002956
Iteration 131/1000 | Loss: 0.00002956
Iteration 132/1000 | Loss: 0.00002956
Iteration 133/1000 | Loss: 0.00002956
Iteration 134/1000 | Loss: 0.00002956
Iteration 135/1000 | Loss: 0.00002956
Iteration 136/1000 | Loss: 0.00002956
Iteration 137/1000 | Loss: 0.00002956
Iteration 138/1000 | Loss: 0.00002956
Iteration 139/1000 | Loss: 0.00002956
Iteration 140/1000 | Loss: 0.00002955
Iteration 141/1000 | Loss: 0.00002955
Iteration 142/1000 | Loss: 0.00002955
Iteration 143/1000 | Loss: 0.00002955
Iteration 144/1000 | Loss: 0.00002954
Iteration 145/1000 | Loss: 0.00002954
Iteration 146/1000 | Loss: 0.00002954
Iteration 147/1000 | Loss: 0.00002954
Iteration 148/1000 | Loss: 0.00002954
Iteration 149/1000 | Loss: 0.00002954
Iteration 150/1000 | Loss: 0.00002954
Iteration 151/1000 | Loss: 0.00002954
Iteration 152/1000 | Loss: 0.00002954
Iteration 153/1000 | Loss: 0.00002954
Iteration 154/1000 | Loss: 0.00002954
Iteration 155/1000 | Loss: 0.00002953
Iteration 156/1000 | Loss: 0.00002953
Iteration 157/1000 | Loss: 0.00002953
Iteration 158/1000 | Loss: 0.00002953
Iteration 159/1000 | Loss: 0.00002953
Iteration 160/1000 | Loss: 0.00002953
Iteration 161/1000 | Loss: 0.00002953
Iteration 162/1000 | Loss: 0.00002953
Iteration 163/1000 | Loss: 0.00002953
Iteration 164/1000 | Loss: 0.00002953
Iteration 165/1000 | Loss: 0.00002953
Iteration 166/1000 | Loss: 0.00002953
Iteration 167/1000 | Loss: 0.00002953
Iteration 168/1000 | Loss: 0.00002953
Iteration 169/1000 | Loss: 0.00002953
Iteration 170/1000 | Loss: 0.00002952
Iteration 171/1000 | Loss: 0.00002952
Iteration 172/1000 | Loss: 0.00002952
Iteration 173/1000 | Loss: 0.00002952
Iteration 174/1000 | Loss: 0.00002952
Iteration 175/1000 | Loss: 0.00002952
Iteration 176/1000 | Loss: 0.00002952
Iteration 177/1000 | Loss: 0.00002952
Iteration 178/1000 | Loss: 0.00002952
Iteration 179/1000 | Loss: 0.00002952
Iteration 180/1000 | Loss: 0.00002952
Iteration 181/1000 | Loss: 0.00002951
Iteration 182/1000 | Loss: 0.00002951
Iteration 183/1000 | Loss: 0.00002951
Iteration 184/1000 | Loss: 0.00002951
Iteration 185/1000 | Loss: 0.00002951
Iteration 186/1000 | Loss: 0.00002951
Iteration 187/1000 | Loss: 0.00002951
Iteration 188/1000 | Loss: 0.00002950
Iteration 189/1000 | Loss: 0.00002950
Iteration 190/1000 | Loss: 0.00002950
Iteration 191/1000 | Loss: 0.00002950
Iteration 192/1000 | Loss: 0.00002950
Iteration 193/1000 | Loss: 0.00002950
Iteration 194/1000 | Loss: 0.00002949
Iteration 195/1000 | Loss: 0.00002949
Iteration 196/1000 | Loss: 0.00002949
Iteration 197/1000 | Loss: 0.00002949
Iteration 198/1000 | Loss: 0.00002949
Iteration 199/1000 | Loss: 0.00002949
Iteration 200/1000 | Loss: 0.00002948
Iteration 201/1000 | Loss: 0.00002948
Iteration 202/1000 | Loss: 0.00002948
Iteration 203/1000 | Loss: 0.00002948
Iteration 204/1000 | Loss: 0.00002948
Iteration 205/1000 | Loss: 0.00002948
Iteration 206/1000 | Loss: 0.00002948
Iteration 207/1000 | Loss: 0.00002948
Iteration 208/1000 | Loss: 0.00002947
Iteration 209/1000 | Loss: 0.00002947
Iteration 210/1000 | Loss: 0.00002947
Iteration 211/1000 | Loss: 0.00002947
Iteration 212/1000 | Loss: 0.00002947
Iteration 213/1000 | Loss: 0.00002947
Iteration 214/1000 | Loss: 0.00002947
Iteration 215/1000 | Loss: 0.00002947
Iteration 216/1000 | Loss: 0.00002947
Iteration 217/1000 | Loss: 0.00002947
Iteration 218/1000 | Loss: 0.00002947
Iteration 219/1000 | Loss: 0.00002946
Iteration 220/1000 | Loss: 0.00002946
Iteration 221/1000 | Loss: 0.00002946
Iteration 222/1000 | Loss: 0.00002946
Iteration 223/1000 | Loss: 0.00002946
Iteration 224/1000 | Loss: 0.00002946
Iteration 225/1000 | Loss: 0.00002946
Iteration 226/1000 | Loss: 0.00002946
Iteration 227/1000 | Loss: 0.00002946
Iteration 228/1000 | Loss: 0.00002946
Iteration 229/1000 | Loss: 0.00002946
Iteration 230/1000 | Loss: 0.00002945
Iteration 231/1000 | Loss: 0.00002945
Iteration 232/1000 | Loss: 0.00002945
Iteration 233/1000 | Loss: 0.00002945
Iteration 234/1000 | Loss: 0.00002945
Iteration 235/1000 | Loss: 0.00002945
Iteration 236/1000 | Loss: 0.00002945
Iteration 237/1000 | Loss: 0.00002945
Iteration 238/1000 | Loss: 0.00002945
Iteration 239/1000 | Loss: 0.00002945
Iteration 240/1000 | Loss: 0.00002945
Iteration 241/1000 | Loss: 0.00002945
Iteration 242/1000 | Loss: 0.00002945
Iteration 243/1000 | Loss: 0.00002945
Iteration 244/1000 | Loss: 0.00002945
Iteration 245/1000 | Loss: 0.00002945
Iteration 246/1000 | Loss: 0.00002945
Iteration 247/1000 | Loss: 0.00002945
Iteration 248/1000 | Loss: 0.00002945
Iteration 249/1000 | Loss: 0.00002945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.945492997241672e-05, 2.945492997241672e-05, 2.945492997241672e-05, 2.945492997241672e-05, 2.945492997241672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.945492997241672e-05

Optimization complete. Final v2v error: 4.610319137573242 mm

Highest mean error: 5.678446292877197 mm for frame 137

Lowest mean error: 4.146710395812988 mm for frame 35

Saving results

Total time: 134.39125680923462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072548
Iteration 2/25 | Loss: 0.00275122
Iteration 3/25 | Loss: 0.00211129
Iteration 4/25 | Loss: 0.00215267
Iteration 5/25 | Loss: 0.00174609
Iteration 6/25 | Loss: 0.00163447
Iteration 7/25 | Loss: 0.00160545
Iteration 8/25 | Loss: 0.00158894
Iteration 9/25 | Loss: 0.00158089
Iteration 10/25 | Loss: 0.00156567
Iteration 11/25 | Loss: 0.00156663
Iteration 12/25 | Loss: 0.00155927
Iteration 13/25 | Loss: 0.00155144
Iteration 14/25 | Loss: 0.00155534
Iteration 15/25 | Loss: 0.00154806
Iteration 16/25 | Loss: 0.00154855
Iteration 17/25 | Loss: 0.00154634
Iteration 18/25 | Loss: 0.00154259
Iteration 19/25 | Loss: 0.00154094
Iteration 20/25 | Loss: 0.00154201
Iteration 21/25 | Loss: 0.00154542
Iteration 22/25 | Loss: 0.00153627
Iteration 23/25 | Loss: 0.00153378
Iteration 24/25 | Loss: 0.00153585
Iteration 25/25 | Loss: 0.00153275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14127481
Iteration 2/25 | Loss: 0.00283386
Iteration 3/25 | Loss: 0.00283386
Iteration 4/25 | Loss: 0.00283386
Iteration 5/25 | Loss: 0.00283386
Iteration 6/25 | Loss: 0.00283386
Iteration 7/25 | Loss: 0.00283386
Iteration 8/25 | Loss: 0.00283386
Iteration 9/25 | Loss: 0.00283386
Iteration 10/25 | Loss: 0.00283386
Iteration 11/25 | Loss: 0.00283386
Iteration 12/25 | Loss: 0.00283386
Iteration 13/25 | Loss: 0.00283386
Iteration 14/25 | Loss: 0.00283386
Iteration 15/25 | Loss: 0.00283386
Iteration 16/25 | Loss: 0.00283386
Iteration 17/25 | Loss: 0.00283386
Iteration 18/25 | Loss: 0.00283386
Iteration 19/25 | Loss: 0.00283386
Iteration 20/25 | Loss: 0.00283386
Iteration 21/25 | Loss: 0.00283386
Iteration 22/25 | Loss: 0.00283386
Iteration 23/25 | Loss: 0.00283386
Iteration 24/25 | Loss: 0.00283386
Iteration 25/25 | Loss: 0.00283386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00283386
Iteration 2/1000 | Loss: 0.00011977
Iteration 3/1000 | Loss: 0.00008121
Iteration 4/1000 | Loss: 0.00012152
Iteration 5/1000 | Loss: 0.00035626
Iteration 6/1000 | Loss: 0.00029430
Iteration 7/1000 | Loss: 0.00008889
Iteration 8/1000 | Loss: 0.00006540
Iteration 9/1000 | Loss: 0.00005374
Iteration 10/1000 | Loss: 0.00004748
Iteration 11/1000 | Loss: 0.00024390
Iteration 12/1000 | Loss: 0.00008344
Iteration 13/1000 | Loss: 0.00011480
Iteration 14/1000 | Loss: 0.00004343
Iteration 15/1000 | Loss: 0.00003824
Iteration 16/1000 | Loss: 0.00003312
Iteration 17/1000 | Loss: 0.00003105
Iteration 18/1000 | Loss: 0.00003002
Iteration 19/1000 | Loss: 0.00002935
Iteration 20/1000 | Loss: 0.00002895
Iteration 21/1000 | Loss: 0.00002857
Iteration 22/1000 | Loss: 0.00002830
Iteration 23/1000 | Loss: 0.00002809
Iteration 24/1000 | Loss: 0.00002792
Iteration 25/1000 | Loss: 0.00002779
Iteration 26/1000 | Loss: 0.00002778
Iteration 27/1000 | Loss: 0.00002777
Iteration 28/1000 | Loss: 0.00002777
Iteration 29/1000 | Loss: 0.00002776
Iteration 30/1000 | Loss: 0.00002776
Iteration 31/1000 | Loss: 0.00002775
Iteration 32/1000 | Loss: 0.00002774
Iteration 33/1000 | Loss: 0.00002774
Iteration 34/1000 | Loss: 0.00002774
Iteration 35/1000 | Loss: 0.00002774
Iteration 36/1000 | Loss: 0.00002773
Iteration 37/1000 | Loss: 0.00002772
Iteration 38/1000 | Loss: 0.00002772
Iteration 39/1000 | Loss: 0.00002772
Iteration 40/1000 | Loss: 0.00002771
Iteration 41/1000 | Loss: 0.00002771
Iteration 42/1000 | Loss: 0.00002771
Iteration 43/1000 | Loss: 0.00002771
Iteration 44/1000 | Loss: 0.00002771
Iteration 45/1000 | Loss: 0.00002770
Iteration 46/1000 | Loss: 0.00002770
Iteration 47/1000 | Loss: 0.00002770
Iteration 48/1000 | Loss: 0.00002770
Iteration 49/1000 | Loss: 0.00002770
Iteration 50/1000 | Loss: 0.00002769
Iteration 51/1000 | Loss: 0.00002769
Iteration 52/1000 | Loss: 0.00002769
Iteration 53/1000 | Loss: 0.00002769
Iteration 54/1000 | Loss: 0.00002769
Iteration 55/1000 | Loss: 0.00002769
Iteration 56/1000 | Loss: 0.00002769
Iteration 57/1000 | Loss: 0.00002769
Iteration 58/1000 | Loss: 0.00002769
Iteration 59/1000 | Loss: 0.00002769
Iteration 60/1000 | Loss: 0.00002768
Iteration 61/1000 | Loss: 0.00002768
Iteration 62/1000 | Loss: 0.00002768
Iteration 63/1000 | Loss: 0.00002767
Iteration 64/1000 | Loss: 0.00002767
Iteration 65/1000 | Loss: 0.00002767
Iteration 66/1000 | Loss: 0.00002767
Iteration 67/1000 | Loss: 0.00002767
Iteration 68/1000 | Loss: 0.00002767
Iteration 69/1000 | Loss: 0.00002767
Iteration 70/1000 | Loss: 0.00002766
Iteration 71/1000 | Loss: 0.00002766
Iteration 72/1000 | Loss: 0.00002766
Iteration 73/1000 | Loss: 0.00002765
Iteration 74/1000 | Loss: 0.00002765
Iteration 75/1000 | Loss: 0.00002765
Iteration 76/1000 | Loss: 0.00002765
Iteration 77/1000 | Loss: 0.00002764
Iteration 78/1000 | Loss: 0.00002764
Iteration 79/1000 | Loss: 0.00002764
Iteration 80/1000 | Loss: 0.00002764
Iteration 81/1000 | Loss: 0.00002764
Iteration 82/1000 | Loss: 0.00002764
Iteration 83/1000 | Loss: 0.00002764
Iteration 84/1000 | Loss: 0.00002763
Iteration 85/1000 | Loss: 0.00002763
Iteration 86/1000 | Loss: 0.00002763
Iteration 87/1000 | Loss: 0.00002763
Iteration 88/1000 | Loss: 0.00002763
Iteration 89/1000 | Loss: 0.00002763
Iteration 90/1000 | Loss: 0.00002763
Iteration 91/1000 | Loss: 0.00002763
Iteration 92/1000 | Loss: 0.00002763
Iteration 93/1000 | Loss: 0.00002763
Iteration 94/1000 | Loss: 0.00002763
Iteration 95/1000 | Loss: 0.00002763
Iteration 96/1000 | Loss: 0.00002762
Iteration 97/1000 | Loss: 0.00002762
Iteration 98/1000 | Loss: 0.00002762
Iteration 99/1000 | Loss: 0.00002762
Iteration 100/1000 | Loss: 0.00002762
Iteration 101/1000 | Loss: 0.00002762
Iteration 102/1000 | Loss: 0.00002762
Iteration 103/1000 | Loss: 0.00002761
Iteration 104/1000 | Loss: 0.00002761
Iteration 105/1000 | Loss: 0.00002761
Iteration 106/1000 | Loss: 0.00002761
Iteration 107/1000 | Loss: 0.00002760
Iteration 108/1000 | Loss: 0.00002760
Iteration 109/1000 | Loss: 0.00002760
Iteration 110/1000 | Loss: 0.00002760
Iteration 111/1000 | Loss: 0.00002760
Iteration 112/1000 | Loss: 0.00002760
Iteration 113/1000 | Loss: 0.00002759
Iteration 114/1000 | Loss: 0.00002759
Iteration 115/1000 | Loss: 0.00002759
Iteration 116/1000 | Loss: 0.00002758
Iteration 117/1000 | Loss: 0.00002758
Iteration 118/1000 | Loss: 0.00002758
Iteration 119/1000 | Loss: 0.00002757
Iteration 120/1000 | Loss: 0.00002757
Iteration 121/1000 | Loss: 0.00002757
Iteration 122/1000 | Loss: 0.00002756
Iteration 123/1000 | Loss: 0.00002756
Iteration 124/1000 | Loss: 0.00002756
Iteration 125/1000 | Loss: 0.00002756
Iteration 126/1000 | Loss: 0.00002756
Iteration 127/1000 | Loss: 0.00002756
Iteration 128/1000 | Loss: 0.00002756
Iteration 129/1000 | Loss: 0.00002756
Iteration 130/1000 | Loss: 0.00002756
Iteration 131/1000 | Loss: 0.00002756
Iteration 132/1000 | Loss: 0.00002756
Iteration 133/1000 | Loss: 0.00002756
Iteration 134/1000 | Loss: 0.00002756
Iteration 135/1000 | Loss: 0.00002756
Iteration 136/1000 | Loss: 0.00002756
Iteration 137/1000 | Loss: 0.00002756
Iteration 138/1000 | Loss: 0.00002756
Iteration 139/1000 | Loss: 0.00002756
Iteration 140/1000 | Loss: 0.00002756
Iteration 141/1000 | Loss: 0.00002756
Iteration 142/1000 | Loss: 0.00002756
Iteration 143/1000 | Loss: 0.00002756
Iteration 144/1000 | Loss: 0.00002756
Iteration 145/1000 | Loss: 0.00002756
Iteration 146/1000 | Loss: 0.00002756
Iteration 147/1000 | Loss: 0.00002756
Iteration 148/1000 | Loss: 0.00002756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.7563459298107773e-05, 2.7563459298107773e-05, 2.7563459298107773e-05, 2.7563459298107773e-05, 2.7563459298107773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7563459298107773e-05

Optimization complete. Final v2v error: 4.331029891967773 mm

Highest mean error: 6.302261829376221 mm for frame 24

Lowest mean error: 4.11085844039917 mm for frame 73

Saving results

Total time: 101.04167366027832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107558
Iteration 2/25 | Loss: 0.01107558
Iteration 3/25 | Loss: 0.01107558
Iteration 4/25 | Loss: 0.01107558
Iteration 5/25 | Loss: 0.01107557
Iteration 6/25 | Loss: 0.01107557
Iteration 7/25 | Loss: 0.01107556
Iteration 8/25 | Loss: 0.01107556
Iteration 9/25 | Loss: 0.01107556
Iteration 10/25 | Loss: 0.01107556
Iteration 11/25 | Loss: 0.01107556
Iteration 12/25 | Loss: 0.01107555
Iteration 13/25 | Loss: 0.01107555
Iteration 14/25 | Loss: 0.01107555
Iteration 15/25 | Loss: 0.01107554
Iteration 16/25 | Loss: 0.01107554
Iteration 17/25 | Loss: 0.01107554
Iteration 18/25 | Loss: 0.01107553
Iteration 19/25 | Loss: 0.01107553
Iteration 20/25 | Loss: 0.01107553
Iteration 21/25 | Loss: 0.01107553
Iteration 22/25 | Loss: 0.01107553
Iteration 23/25 | Loss: 0.01107552
Iteration 24/25 | Loss: 0.01107552
Iteration 25/25 | Loss: 0.01107552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46274412
Iteration 2/25 | Loss: 0.12461060
Iteration 3/25 | Loss: 0.09148192
Iteration 4/25 | Loss: 0.08630220
Iteration 5/25 | Loss: 0.08627278
Iteration 6/25 | Loss: 0.08627042
Iteration 7/25 | Loss: 0.08627042
Iteration 8/25 | Loss: 0.08627041
Iteration 9/25 | Loss: 0.08627041
Iteration 10/25 | Loss: 0.08627041
Iteration 11/25 | Loss: 0.08627040
Iteration 12/25 | Loss: 0.08627040
Iteration 13/25 | Loss: 0.08627039
Iteration 14/25 | Loss: 0.08627039
Iteration 15/25 | Loss: 0.08627039
Iteration 16/25 | Loss: 0.08627039
Iteration 17/25 | Loss: 0.08627039
Iteration 18/25 | Loss: 0.08627040
Iteration 19/25 | Loss: 0.08627040
Iteration 20/25 | Loss: 0.08627039
Iteration 21/25 | Loss: 0.08627039
Iteration 22/25 | Loss: 0.08627039
Iteration 23/25 | Loss: 0.08627039
Iteration 24/25 | Loss: 0.08627039
Iteration 25/25 | Loss: 0.08627039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.08627039194107056, 0.08627039194107056, 0.08627039194107056, 0.08627039194107056, 0.08627039194107056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08627039194107056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08627039
Iteration 2/1000 | Loss: 0.00112920
Iteration 3/1000 | Loss: 0.00066857
Iteration 4/1000 | Loss: 0.00052503
Iteration 5/1000 | Loss: 0.00145559
Iteration 6/1000 | Loss: 0.00018207
Iteration 7/1000 | Loss: 0.00020973
Iteration 8/1000 | Loss: 0.00011358
Iteration 9/1000 | Loss: 0.00012221
Iteration 10/1000 | Loss: 0.00008389
Iteration 11/1000 | Loss: 0.00023514
Iteration 12/1000 | Loss: 0.00019239
Iteration 13/1000 | Loss: 0.00007561
Iteration 14/1000 | Loss: 0.00004416
Iteration 15/1000 | Loss: 0.00004046
Iteration 16/1000 | Loss: 0.00004166
Iteration 17/1000 | Loss: 0.00011754
Iteration 18/1000 | Loss: 0.00010567
Iteration 19/1000 | Loss: 0.00003685
Iteration 20/1000 | Loss: 0.00003670
Iteration 21/1000 | Loss: 0.00003823
Iteration 22/1000 | Loss: 0.00003571
Iteration 23/1000 | Loss: 0.00003414
Iteration 24/1000 | Loss: 0.00006708
Iteration 25/1000 | Loss: 0.00003338
Iteration 26/1000 | Loss: 0.00003169
Iteration 27/1000 | Loss: 0.00003381
Iteration 28/1000 | Loss: 0.00004811
Iteration 29/1000 | Loss: 0.00004140
Iteration 30/1000 | Loss: 0.00003184
Iteration 31/1000 | Loss: 0.00003216
Iteration 32/1000 | Loss: 0.00003066
Iteration 33/1000 | Loss: 0.00003143
Iteration 34/1000 | Loss: 0.00003070
Iteration 35/1000 | Loss: 0.00003425
Iteration 36/1000 | Loss: 0.00003034
Iteration 37/1000 | Loss: 0.00003034
Iteration 38/1000 | Loss: 0.00003031
Iteration 39/1000 | Loss: 0.00003031
Iteration 40/1000 | Loss: 0.00003028
Iteration 41/1000 | Loss: 0.00003114
Iteration 42/1000 | Loss: 0.00003094
Iteration 43/1000 | Loss: 0.00003094
Iteration 44/1000 | Loss: 0.00004961
Iteration 45/1000 | Loss: 0.00003252
Iteration 46/1000 | Loss: 0.00003950
Iteration 47/1000 | Loss: 0.00003400
Iteration 48/1000 | Loss: 0.00003097
Iteration 49/1000 | Loss: 0.00004330
Iteration 50/1000 | Loss: 0.00003266
Iteration 51/1000 | Loss: 0.00003053
Iteration 52/1000 | Loss: 0.00003039
Iteration 53/1000 | Loss: 0.00003563
Iteration 54/1000 | Loss: 0.00003079
Iteration 55/1000 | Loss: 0.00002983
Iteration 56/1000 | Loss: 0.00003078
Iteration 57/1000 | Loss: 0.00002980
Iteration 58/1000 | Loss: 0.00002980
Iteration 59/1000 | Loss: 0.00002980
Iteration 60/1000 | Loss: 0.00002979
Iteration 61/1000 | Loss: 0.00002979
Iteration 62/1000 | Loss: 0.00002979
Iteration 63/1000 | Loss: 0.00002979
Iteration 64/1000 | Loss: 0.00002979
Iteration 65/1000 | Loss: 0.00002979
Iteration 66/1000 | Loss: 0.00002979
Iteration 67/1000 | Loss: 0.00002978
Iteration 68/1000 | Loss: 0.00002977
Iteration 69/1000 | Loss: 0.00002976
Iteration 70/1000 | Loss: 0.00002975
Iteration 71/1000 | Loss: 0.00002975
Iteration 72/1000 | Loss: 0.00002975
Iteration 73/1000 | Loss: 0.00002975
Iteration 74/1000 | Loss: 0.00002975
Iteration 75/1000 | Loss: 0.00002975
Iteration 76/1000 | Loss: 0.00002974
Iteration 77/1000 | Loss: 0.00002974
Iteration 78/1000 | Loss: 0.00002974
Iteration 79/1000 | Loss: 0.00002973
Iteration 80/1000 | Loss: 0.00002973
Iteration 81/1000 | Loss: 0.00002973
Iteration 82/1000 | Loss: 0.00002972
Iteration 83/1000 | Loss: 0.00002972
Iteration 84/1000 | Loss: 0.00002972
Iteration 85/1000 | Loss: 0.00002971
Iteration 86/1000 | Loss: 0.00002971
Iteration 87/1000 | Loss: 0.00002971
Iteration 88/1000 | Loss: 0.00002971
Iteration 89/1000 | Loss: 0.00002971
Iteration 90/1000 | Loss: 0.00002970
Iteration 91/1000 | Loss: 0.00002970
Iteration 92/1000 | Loss: 0.00002970
Iteration 93/1000 | Loss: 0.00002966
Iteration 94/1000 | Loss: 0.00002966
Iteration 95/1000 | Loss: 0.00002966
Iteration 96/1000 | Loss: 0.00002966
Iteration 97/1000 | Loss: 0.00002966
Iteration 98/1000 | Loss: 0.00002966
Iteration 99/1000 | Loss: 0.00002966
Iteration 100/1000 | Loss: 0.00002966
Iteration 101/1000 | Loss: 0.00002966
Iteration 102/1000 | Loss: 0.00002966
Iteration 103/1000 | Loss: 0.00002965
Iteration 104/1000 | Loss: 0.00002965
Iteration 105/1000 | Loss: 0.00002965
Iteration 106/1000 | Loss: 0.00002964
Iteration 107/1000 | Loss: 0.00002964
Iteration 108/1000 | Loss: 0.00002964
Iteration 109/1000 | Loss: 0.00002964
Iteration 110/1000 | Loss: 0.00002964
Iteration 111/1000 | Loss: 0.00002964
Iteration 112/1000 | Loss: 0.00002964
Iteration 113/1000 | Loss: 0.00002964
Iteration 114/1000 | Loss: 0.00002969
Iteration 115/1000 | Loss: 0.00002969
Iteration 116/1000 | Loss: 0.00002967
Iteration 117/1000 | Loss: 0.00002967
Iteration 118/1000 | Loss: 0.00002967
Iteration 119/1000 | Loss: 0.00002967
Iteration 120/1000 | Loss: 0.00002966
Iteration 121/1000 | Loss: 0.00002966
Iteration 122/1000 | Loss: 0.00002966
Iteration 123/1000 | Loss: 0.00002966
Iteration 124/1000 | Loss: 0.00002966
Iteration 125/1000 | Loss: 0.00002965
Iteration 126/1000 | Loss: 0.00002965
Iteration 127/1000 | Loss: 0.00002965
Iteration 128/1000 | Loss: 0.00002965
Iteration 129/1000 | Loss: 0.00002963
Iteration 130/1000 | Loss: 0.00002962
Iteration 131/1000 | Loss: 0.00002962
Iteration 132/1000 | Loss: 0.00002967
Iteration 133/1000 | Loss: 0.00002967
Iteration 134/1000 | Loss: 0.00002967
Iteration 135/1000 | Loss: 0.00002967
Iteration 136/1000 | Loss: 0.00002961
Iteration 137/1000 | Loss: 0.00002961
Iteration 138/1000 | Loss: 0.00002961
Iteration 139/1000 | Loss: 0.00002961
Iteration 140/1000 | Loss: 0.00002961
Iteration 141/1000 | Loss: 0.00002961
Iteration 142/1000 | Loss: 0.00002960
Iteration 143/1000 | Loss: 0.00002960
Iteration 144/1000 | Loss: 0.00002960
Iteration 145/1000 | Loss: 0.00002963
Iteration 146/1000 | Loss: 0.00002963
Iteration 147/1000 | Loss: 0.00002963
Iteration 148/1000 | Loss: 0.00002963
Iteration 149/1000 | Loss: 0.00002963
Iteration 150/1000 | Loss: 0.00002959
Iteration 151/1000 | Loss: 0.00002959
Iteration 152/1000 | Loss: 0.00002959
Iteration 153/1000 | Loss: 0.00002959
Iteration 154/1000 | Loss: 0.00002959
Iteration 155/1000 | Loss: 0.00002959
Iteration 156/1000 | Loss: 0.00002959
Iteration 157/1000 | Loss: 0.00002959
Iteration 158/1000 | Loss: 0.00002959
Iteration 159/1000 | Loss: 0.00002959
Iteration 160/1000 | Loss: 0.00002959
Iteration 161/1000 | Loss: 0.00002959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.9590622943942435e-05, 2.9590622943942435e-05, 2.9590622943942435e-05, 2.9590622943942435e-05, 2.9590622943942435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9590622943942435e-05

Optimization complete. Final v2v error: 4.37040901184082 mm

Highest mean error: 11.364800453186035 mm for frame 157

Lowest mean error: 4.037436485290527 mm for frame 181

Saving results

Total time: 91.29743504524231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104552
Iteration 2/25 | Loss: 0.01104551
Iteration 3/25 | Loss: 0.00337671
Iteration 4/25 | Loss: 0.00195496
Iteration 5/25 | Loss: 0.00180235
Iteration 6/25 | Loss: 0.00180626
Iteration 7/25 | Loss: 0.00151387
Iteration 8/25 | Loss: 0.00140533
Iteration 9/25 | Loss: 0.00133378
Iteration 10/25 | Loss: 0.00131016
Iteration 11/25 | Loss: 0.00129895
Iteration 12/25 | Loss: 0.00127476
Iteration 13/25 | Loss: 0.00126897
Iteration 14/25 | Loss: 0.00126384
Iteration 15/25 | Loss: 0.00126273
Iteration 16/25 | Loss: 0.00126249
Iteration 17/25 | Loss: 0.00126240
Iteration 18/25 | Loss: 0.00126240
Iteration 19/25 | Loss: 0.00126239
Iteration 20/25 | Loss: 0.00126239
Iteration 21/25 | Loss: 0.00126238
Iteration 22/25 | Loss: 0.00126238
Iteration 23/25 | Loss: 0.00126238
Iteration 24/25 | Loss: 0.00126238
Iteration 25/25 | Loss: 0.00126238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16031957
Iteration 2/25 | Loss: 0.00245347
Iteration 3/25 | Loss: 0.00245346
Iteration 4/25 | Loss: 0.00245346
Iteration 5/25 | Loss: 0.00245346
Iteration 6/25 | Loss: 0.00245346
Iteration 7/25 | Loss: 0.00245346
Iteration 8/25 | Loss: 0.00245346
Iteration 9/25 | Loss: 0.00245346
Iteration 10/25 | Loss: 0.00245346
Iteration 11/25 | Loss: 0.00245346
Iteration 12/25 | Loss: 0.00245346
Iteration 13/25 | Loss: 0.00245346
Iteration 14/25 | Loss: 0.00245346
Iteration 15/25 | Loss: 0.00245346
Iteration 16/25 | Loss: 0.00245346
Iteration 17/25 | Loss: 0.00245346
Iteration 18/25 | Loss: 0.00245346
Iteration 19/25 | Loss: 0.00245346
Iteration 20/25 | Loss: 0.00245346
Iteration 21/25 | Loss: 0.00245346
Iteration 22/25 | Loss: 0.00245346
Iteration 23/25 | Loss: 0.00245346
Iteration 24/25 | Loss: 0.00245346
Iteration 25/25 | Loss: 0.00245346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245346
Iteration 2/1000 | Loss: 0.00011229
Iteration 3/1000 | Loss: 0.00008232
Iteration 4/1000 | Loss: 0.00006816
Iteration 5/1000 | Loss: 0.00006367
Iteration 6/1000 | Loss: 0.00006008
Iteration 7/1000 | Loss: 0.00005857
Iteration 8/1000 | Loss: 0.00005740
Iteration 9/1000 | Loss: 0.00005632
Iteration 10/1000 | Loss: 0.00005571
Iteration 11/1000 | Loss: 0.00005529
Iteration 12/1000 | Loss: 0.00005509
Iteration 13/1000 | Loss: 0.00005489
Iteration 14/1000 | Loss: 0.00005489
Iteration 15/1000 | Loss: 0.00005487
Iteration 16/1000 | Loss: 0.00005476
Iteration 17/1000 | Loss: 0.00005469
Iteration 18/1000 | Loss: 0.00005459
Iteration 19/1000 | Loss: 0.00005458
Iteration 20/1000 | Loss: 0.00005458
Iteration 21/1000 | Loss: 0.00005458
Iteration 22/1000 | Loss: 0.00005458
Iteration 23/1000 | Loss: 0.00005458
Iteration 24/1000 | Loss: 0.00005458
Iteration 25/1000 | Loss: 0.00005458
Iteration 26/1000 | Loss: 0.00005457
Iteration 27/1000 | Loss: 0.00005457
Iteration 28/1000 | Loss: 0.00005456
Iteration 29/1000 | Loss: 0.00005455
Iteration 30/1000 | Loss: 0.00005455
Iteration 31/1000 | Loss: 0.00005455
Iteration 32/1000 | Loss: 0.00005455
Iteration 33/1000 | Loss: 0.00005455
Iteration 34/1000 | Loss: 0.00005454
Iteration 35/1000 | Loss: 0.00005454
Iteration 36/1000 | Loss: 0.00005454
Iteration 37/1000 | Loss: 0.00005453
Iteration 38/1000 | Loss: 0.00005453
Iteration 39/1000 | Loss: 0.00005452
Iteration 40/1000 | Loss: 0.00005452
Iteration 41/1000 | Loss: 0.00005452
Iteration 42/1000 | Loss: 0.00005451
Iteration 43/1000 | Loss: 0.00005450
Iteration 44/1000 | Loss: 0.00005450
Iteration 45/1000 | Loss: 0.00005449
Iteration 46/1000 | Loss: 0.00005449
Iteration 47/1000 | Loss: 0.00005449
Iteration 48/1000 | Loss: 0.00005449
Iteration 49/1000 | Loss: 0.00005448
Iteration 50/1000 | Loss: 0.00005448
Iteration 51/1000 | Loss: 0.00005448
Iteration 52/1000 | Loss: 0.00005447
Iteration 53/1000 | Loss: 0.00005447
Iteration 54/1000 | Loss: 0.00005447
Iteration 55/1000 | Loss: 0.00005447
Iteration 56/1000 | Loss: 0.00005446
Iteration 57/1000 | Loss: 0.00005446
Iteration 58/1000 | Loss: 0.00005446
Iteration 59/1000 | Loss: 0.00005446
Iteration 60/1000 | Loss: 0.00005446
Iteration 61/1000 | Loss: 0.00005445
Iteration 62/1000 | Loss: 0.00005445
Iteration 63/1000 | Loss: 0.00005445
Iteration 64/1000 | Loss: 0.00005445
Iteration 65/1000 | Loss: 0.00005444
Iteration 66/1000 | Loss: 0.00005444
Iteration 67/1000 | Loss: 0.00005444
Iteration 68/1000 | Loss: 0.00005444
Iteration 69/1000 | Loss: 0.00005443
Iteration 70/1000 | Loss: 0.00005442
Iteration 71/1000 | Loss: 0.00005442
Iteration 72/1000 | Loss: 0.00005442
Iteration 73/1000 | Loss: 0.00005441
Iteration 74/1000 | Loss: 0.00005441
Iteration 75/1000 | Loss: 0.00005441
Iteration 76/1000 | Loss: 0.00005441
Iteration 77/1000 | Loss: 0.00005441
Iteration 78/1000 | Loss: 0.00005441
Iteration 79/1000 | Loss: 0.00005441
Iteration 80/1000 | Loss: 0.00005441
Iteration 81/1000 | Loss: 0.00005441
Iteration 82/1000 | Loss: 0.00005441
Iteration 83/1000 | Loss: 0.00005440
Iteration 84/1000 | Loss: 0.00005440
Iteration 85/1000 | Loss: 0.00005440
Iteration 86/1000 | Loss: 0.00005440
Iteration 87/1000 | Loss: 0.00005440
Iteration 88/1000 | Loss: 0.00005439
Iteration 89/1000 | Loss: 0.00005439
Iteration 90/1000 | Loss: 0.00005439
Iteration 91/1000 | Loss: 0.00005438
Iteration 92/1000 | Loss: 0.00005438
Iteration 93/1000 | Loss: 0.00005438
Iteration 94/1000 | Loss: 0.00005438
Iteration 95/1000 | Loss: 0.00005437
Iteration 96/1000 | Loss: 0.00005436
Iteration 97/1000 | Loss: 0.00005436
Iteration 98/1000 | Loss: 0.00005436
Iteration 99/1000 | Loss: 0.00005436
Iteration 100/1000 | Loss: 0.00005436
Iteration 101/1000 | Loss: 0.00005436
Iteration 102/1000 | Loss: 0.00005436
Iteration 103/1000 | Loss: 0.00005436
Iteration 104/1000 | Loss: 0.00005436
Iteration 105/1000 | Loss: 0.00005435
Iteration 106/1000 | Loss: 0.00005435
Iteration 107/1000 | Loss: 0.00005435
Iteration 108/1000 | Loss: 0.00005435
Iteration 109/1000 | Loss: 0.00005435
Iteration 110/1000 | Loss: 0.00005435
Iteration 111/1000 | Loss: 0.00005435
Iteration 112/1000 | Loss: 0.00005435
Iteration 113/1000 | Loss: 0.00005435
Iteration 114/1000 | Loss: 0.00005435
Iteration 115/1000 | Loss: 0.00005435
Iteration 116/1000 | Loss: 0.00005435
Iteration 117/1000 | Loss: 0.00005435
Iteration 118/1000 | Loss: 0.00005435
Iteration 119/1000 | Loss: 0.00005435
Iteration 120/1000 | Loss: 0.00005435
Iteration 121/1000 | Loss: 0.00005435
Iteration 122/1000 | Loss: 0.00005435
Iteration 123/1000 | Loss: 0.00005435
Iteration 124/1000 | Loss: 0.00005435
Iteration 125/1000 | Loss: 0.00005435
Iteration 126/1000 | Loss: 0.00005435
Iteration 127/1000 | Loss: 0.00005435
Iteration 128/1000 | Loss: 0.00005435
Iteration 129/1000 | Loss: 0.00005435
Iteration 130/1000 | Loss: 0.00005435
Iteration 131/1000 | Loss: 0.00005435
Iteration 132/1000 | Loss: 0.00005435
Iteration 133/1000 | Loss: 0.00005435
Iteration 134/1000 | Loss: 0.00005435
Iteration 135/1000 | Loss: 0.00005435
Iteration 136/1000 | Loss: 0.00005435
Iteration 137/1000 | Loss: 0.00005435
Iteration 138/1000 | Loss: 0.00005435
Iteration 139/1000 | Loss: 0.00005435
Iteration 140/1000 | Loss: 0.00005435
Iteration 141/1000 | Loss: 0.00005435
Iteration 142/1000 | Loss: 0.00005435
Iteration 143/1000 | Loss: 0.00005435
Iteration 144/1000 | Loss: 0.00005435
Iteration 145/1000 | Loss: 0.00005435
Iteration 146/1000 | Loss: 0.00005435
Iteration 147/1000 | Loss: 0.00005435
Iteration 148/1000 | Loss: 0.00005435
Iteration 149/1000 | Loss: 0.00005435
Iteration 150/1000 | Loss: 0.00005435
Iteration 151/1000 | Loss: 0.00005435
Iteration 152/1000 | Loss: 0.00005435
Iteration 153/1000 | Loss: 0.00005435
Iteration 154/1000 | Loss: 0.00005435
Iteration 155/1000 | Loss: 0.00005435
Iteration 156/1000 | Loss: 0.00005434
Iteration 157/1000 | Loss: 0.00005434
Iteration 158/1000 | Loss: 0.00005434
Iteration 159/1000 | Loss: 0.00005434
Iteration 160/1000 | Loss: 0.00005434
Iteration 161/1000 | Loss: 0.00005434
Iteration 162/1000 | Loss: 0.00005434
Iteration 163/1000 | Loss: 0.00005434
Iteration 164/1000 | Loss: 0.00005434
Iteration 165/1000 | Loss: 0.00005434
Iteration 166/1000 | Loss: 0.00005434
Iteration 167/1000 | Loss: 0.00005434
Iteration 168/1000 | Loss: 0.00005434
Iteration 169/1000 | Loss: 0.00005434
Iteration 170/1000 | Loss: 0.00005434
Iteration 171/1000 | Loss: 0.00005434
Iteration 172/1000 | Loss: 0.00005434
Iteration 173/1000 | Loss: 0.00005434
Iteration 174/1000 | Loss: 0.00005434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [5.434435297502205e-05, 5.434435297502205e-05, 5.434435297502205e-05, 5.434435297502205e-05, 5.434435297502205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.434435297502205e-05

Optimization complete. Final v2v error: 4.748535633087158 mm

Highest mean error: 21.556793212890625 mm for frame 128

Lowest mean error: 3.9688186645507812 mm for frame 57

Saving results

Total time: 58.38631558418274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00705135
Iteration 2/25 | Loss: 0.00168985
Iteration 3/25 | Loss: 0.00153413
Iteration 4/25 | Loss: 0.00151634
Iteration 5/25 | Loss: 0.00151129
Iteration 6/25 | Loss: 0.00151031
Iteration 7/25 | Loss: 0.00151031
Iteration 8/25 | Loss: 0.00151031
Iteration 9/25 | Loss: 0.00151031
Iteration 10/25 | Loss: 0.00151031
Iteration 11/25 | Loss: 0.00151031
Iteration 12/25 | Loss: 0.00151031
Iteration 13/25 | Loss: 0.00151031
Iteration 14/25 | Loss: 0.00151031
Iteration 15/25 | Loss: 0.00151031
Iteration 16/25 | Loss: 0.00151031
Iteration 17/25 | Loss: 0.00151031
Iteration 18/25 | Loss: 0.00151031
Iteration 19/25 | Loss: 0.00151031
Iteration 20/25 | Loss: 0.00151031
Iteration 21/25 | Loss: 0.00151031
Iteration 22/25 | Loss: 0.00151031
Iteration 23/25 | Loss: 0.00151031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015103121986612678, 0.0015103121986612678, 0.0015103121986612678, 0.0015103121986612678, 0.0015103121986612678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015103121986612678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.70159340
Iteration 2/25 | Loss: 0.00200133
Iteration 3/25 | Loss: 0.00200133
Iteration 4/25 | Loss: 0.00200133
Iteration 5/25 | Loss: 0.00200133
Iteration 6/25 | Loss: 0.00200133
Iteration 7/25 | Loss: 0.00200133
Iteration 8/25 | Loss: 0.00200133
Iteration 9/25 | Loss: 0.00200133
Iteration 10/25 | Loss: 0.00200133
Iteration 11/25 | Loss: 0.00200133
Iteration 12/25 | Loss: 0.00200133
Iteration 13/25 | Loss: 0.00200133
Iteration 14/25 | Loss: 0.00200133
Iteration 15/25 | Loss: 0.00200133
Iteration 16/25 | Loss: 0.00200133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020013272296637297, 0.0020013272296637297, 0.0020013272296637297, 0.0020013272296637297, 0.0020013272296637297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020013272296637297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200133
Iteration 2/1000 | Loss: 0.00010150
Iteration 3/1000 | Loss: 0.00005504
Iteration 4/1000 | Loss: 0.00004494
Iteration 5/1000 | Loss: 0.00003913
Iteration 6/1000 | Loss: 0.00003642
Iteration 7/1000 | Loss: 0.00003392
Iteration 8/1000 | Loss: 0.00003262
Iteration 9/1000 | Loss: 0.00003165
Iteration 10/1000 | Loss: 0.00003110
Iteration 11/1000 | Loss: 0.00003065
Iteration 12/1000 | Loss: 0.00003027
Iteration 13/1000 | Loss: 0.00003005
Iteration 14/1000 | Loss: 0.00002988
Iteration 15/1000 | Loss: 0.00002984
Iteration 16/1000 | Loss: 0.00002984
Iteration 17/1000 | Loss: 0.00002984
Iteration 18/1000 | Loss: 0.00002983
Iteration 19/1000 | Loss: 0.00002983
Iteration 20/1000 | Loss: 0.00002982
Iteration 21/1000 | Loss: 0.00002981
Iteration 22/1000 | Loss: 0.00002981
Iteration 23/1000 | Loss: 0.00002980
Iteration 24/1000 | Loss: 0.00002979
Iteration 25/1000 | Loss: 0.00002979
Iteration 26/1000 | Loss: 0.00002978
Iteration 27/1000 | Loss: 0.00002978
Iteration 28/1000 | Loss: 0.00002977
Iteration 29/1000 | Loss: 0.00002976
Iteration 30/1000 | Loss: 0.00002975
Iteration 31/1000 | Loss: 0.00002975
Iteration 32/1000 | Loss: 0.00002975
Iteration 33/1000 | Loss: 0.00002975
Iteration 34/1000 | Loss: 0.00002974
Iteration 35/1000 | Loss: 0.00002974
Iteration 36/1000 | Loss: 0.00002974
Iteration 37/1000 | Loss: 0.00002973
Iteration 38/1000 | Loss: 0.00002973
Iteration 39/1000 | Loss: 0.00002971
Iteration 40/1000 | Loss: 0.00002971
Iteration 41/1000 | Loss: 0.00002971
Iteration 42/1000 | Loss: 0.00002971
Iteration 43/1000 | Loss: 0.00002971
Iteration 44/1000 | Loss: 0.00002971
Iteration 45/1000 | Loss: 0.00002971
Iteration 46/1000 | Loss: 0.00002971
Iteration 47/1000 | Loss: 0.00002971
Iteration 48/1000 | Loss: 0.00002971
Iteration 49/1000 | Loss: 0.00002971
Iteration 50/1000 | Loss: 0.00002971
Iteration 51/1000 | Loss: 0.00002970
Iteration 52/1000 | Loss: 0.00002970
Iteration 53/1000 | Loss: 0.00002970
Iteration 54/1000 | Loss: 0.00002969
Iteration 55/1000 | Loss: 0.00002969
Iteration 56/1000 | Loss: 0.00002969
Iteration 57/1000 | Loss: 0.00002968
Iteration 58/1000 | Loss: 0.00002968
Iteration 59/1000 | Loss: 0.00002968
Iteration 60/1000 | Loss: 0.00002968
Iteration 61/1000 | Loss: 0.00002968
Iteration 62/1000 | Loss: 0.00002968
Iteration 63/1000 | Loss: 0.00002968
Iteration 64/1000 | Loss: 0.00002968
Iteration 65/1000 | Loss: 0.00002968
Iteration 66/1000 | Loss: 0.00002968
Iteration 67/1000 | Loss: 0.00002968
Iteration 68/1000 | Loss: 0.00002967
Iteration 69/1000 | Loss: 0.00002967
Iteration 70/1000 | Loss: 0.00002967
Iteration 71/1000 | Loss: 0.00002967
Iteration 72/1000 | Loss: 0.00002967
Iteration 73/1000 | Loss: 0.00002967
Iteration 74/1000 | Loss: 0.00002967
Iteration 75/1000 | Loss: 0.00002966
Iteration 76/1000 | Loss: 0.00002966
Iteration 77/1000 | Loss: 0.00002966
Iteration 78/1000 | Loss: 0.00002966
Iteration 79/1000 | Loss: 0.00002966
Iteration 80/1000 | Loss: 0.00002966
Iteration 81/1000 | Loss: 0.00002966
Iteration 82/1000 | Loss: 0.00002966
Iteration 83/1000 | Loss: 0.00002966
Iteration 84/1000 | Loss: 0.00002966
Iteration 85/1000 | Loss: 0.00002965
Iteration 86/1000 | Loss: 0.00002965
Iteration 87/1000 | Loss: 0.00002965
Iteration 88/1000 | Loss: 0.00002964
Iteration 89/1000 | Loss: 0.00002964
Iteration 90/1000 | Loss: 0.00002964
Iteration 91/1000 | Loss: 0.00002964
Iteration 92/1000 | Loss: 0.00002964
Iteration 93/1000 | Loss: 0.00002963
Iteration 94/1000 | Loss: 0.00002963
Iteration 95/1000 | Loss: 0.00002963
Iteration 96/1000 | Loss: 0.00002963
Iteration 97/1000 | Loss: 0.00002963
Iteration 98/1000 | Loss: 0.00002963
Iteration 99/1000 | Loss: 0.00002963
Iteration 100/1000 | Loss: 0.00002963
Iteration 101/1000 | Loss: 0.00002963
Iteration 102/1000 | Loss: 0.00002963
Iteration 103/1000 | Loss: 0.00002963
Iteration 104/1000 | Loss: 0.00002963
Iteration 105/1000 | Loss: 0.00002962
Iteration 106/1000 | Loss: 0.00002962
Iteration 107/1000 | Loss: 0.00002962
Iteration 108/1000 | Loss: 0.00002962
Iteration 109/1000 | Loss: 0.00002962
Iteration 110/1000 | Loss: 0.00002961
Iteration 111/1000 | Loss: 0.00002961
Iteration 112/1000 | Loss: 0.00002961
Iteration 113/1000 | Loss: 0.00002961
Iteration 114/1000 | Loss: 0.00002961
Iteration 115/1000 | Loss: 0.00002961
Iteration 116/1000 | Loss: 0.00002961
Iteration 117/1000 | Loss: 0.00002961
Iteration 118/1000 | Loss: 0.00002961
Iteration 119/1000 | Loss: 0.00002961
Iteration 120/1000 | Loss: 0.00002961
Iteration 121/1000 | Loss: 0.00002960
Iteration 122/1000 | Loss: 0.00002960
Iteration 123/1000 | Loss: 0.00002960
Iteration 124/1000 | Loss: 0.00002960
Iteration 125/1000 | Loss: 0.00002960
Iteration 126/1000 | Loss: 0.00002960
Iteration 127/1000 | Loss: 0.00002960
Iteration 128/1000 | Loss: 0.00002960
Iteration 129/1000 | Loss: 0.00002960
Iteration 130/1000 | Loss: 0.00002959
Iteration 131/1000 | Loss: 0.00002959
Iteration 132/1000 | Loss: 0.00002959
Iteration 133/1000 | Loss: 0.00002959
Iteration 134/1000 | Loss: 0.00002959
Iteration 135/1000 | Loss: 0.00002959
Iteration 136/1000 | Loss: 0.00002959
Iteration 137/1000 | Loss: 0.00002959
Iteration 138/1000 | Loss: 0.00002959
Iteration 139/1000 | Loss: 0.00002958
Iteration 140/1000 | Loss: 0.00002958
Iteration 141/1000 | Loss: 0.00002958
Iteration 142/1000 | Loss: 0.00002958
Iteration 143/1000 | Loss: 0.00002958
Iteration 144/1000 | Loss: 0.00002958
Iteration 145/1000 | Loss: 0.00002958
Iteration 146/1000 | Loss: 0.00002958
Iteration 147/1000 | Loss: 0.00002958
Iteration 148/1000 | Loss: 0.00002958
Iteration 149/1000 | Loss: 0.00002958
Iteration 150/1000 | Loss: 0.00002958
Iteration 151/1000 | Loss: 0.00002958
Iteration 152/1000 | Loss: 0.00002957
Iteration 153/1000 | Loss: 0.00002957
Iteration 154/1000 | Loss: 0.00002957
Iteration 155/1000 | Loss: 0.00002957
Iteration 156/1000 | Loss: 0.00002957
Iteration 157/1000 | Loss: 0.00002957
Iteration 158/1000 | Loss: 0.00002957
Iteration 159/1000 | Loss: 0.00002957
Iteration 160/1000 | Loss: 0.00002957
Iteration 161/1000 | Loss: 0.00002957
Iteration 162/1000 | Loss: 0.00002957
Iteration 163/1000 | Loss: 0.00002957
Iteration 164/1000 | Loss: 0.00002957
Iteration 165/1000 | Loss: 0.00002956
Iteration 166/1000 | Loss: 0.00002956
Iteration 167/1000 | Loss: 0.00002956
Iteration 168/1000 | Loss: 0.00002956
Iteration 169/1000 | Loss: 0.00002956
Iteration 170/1000 | Loss: 0.00002956
Iteration 171/1000 | Loss: 0.00002956
Iteration 172/1000 | Loss: 0.00002956
Iteration 173/1000 | Loss: 0.00002956
Iteration 174/1000 | Loss: 0.00002956
Iteration 175/1000 | Loss: 0.00002956
Iteration 176/1000 | Loss: 0.00002956
Iteration 177/1000 | Loss: 0.00002956
Iteration 178/1000 | Loss: 0.00002956
Iteration 179/1000 | Loss: 0.00002956
Iteration 180/1000 | Loss: 0.00002956
Iteration 181/1000 | Loss: 0.00002956
Iteration 182/1000 | Loss: 0.00002956
Iteration 183/1000 | Loss: 0.00002956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.9556395020335913e-05, 2.9556395020335913e-05, 2.9556395020335913e-05, 2.9556395020335913e-05, 2.9556395020335913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9556395020335913e-05

Optimization complete. Final v2v error: 4.632431507110596 mm

Highest mean error: 5.334228515625 mm for frame 32

Lowest mean error: 4.230082988739014 mm for frame 22

Saving results

Total time: 37.45947217941284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_it_4305/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_it_4305/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004404
Iteration 2/25 | Loss: 0.00201947
Iteration 3/25 | Loss: 0.00161320
Iteration 4/25 | Loss: 0.00158481
Iteration 5/25 | Loss: 0.00157813
Iteration 6/25 | Loss: 0.00157635
Iteration 7/25 | Loss: 0.00157635
Iteration 8/25 | Loss: 0.00157635
Iteration 9/25 | Loss: 0.00157635
Iteration 10/25 | Loss: 0.00157635
Iteration 11/25 | Loss: 0.00157635
Iteration 12/25 | Loss: 0.00157635
Iteration 13/25 | Loss: 0.00157635
Iteration 14/25 | Loss: 0.00157635
Iteration 15/25 | Loss: 0.00157635
Iteration 16/25 | Loss: 0.00157635
Iteration 17/25 | Loss: 0.00157635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015763546107336879, 0.0015763546107336879, 0.0015763546107336879, 0.0015763546107336879, 0.0015763546107336879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015763546107336879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87674087
Iteration 2/25 | Loss: 0.00192859
Iteration 3/25 | Loss: 0.00192858
Iteration 4/25 | Loss: 0.00192858
Iteration 5/25 | Loss: 0.00192858
Iteration 6/25 | Loss: 0.00192858
Iteration 7/25 | Loss: 0.00192858
Iteration 8/25 | Loss: 0.00192858
Iteration 9/25 | Loss: 0.00192858
Iteration 10/25 | Loss: 0.00192858
Iteration 11/25 | Loss: 0.00192858
Iteration 12/25 | Loss: 0.00192858
Iteration 13/25 | Loss: 0.00192858
Iteration 14/25 | Loss: 0.00192858
Iteration 15/25 | Loss: 0.00192858
Iteration 16/25 | Loss: 0.00192858
Iteration 17/25 | Loss: 0.00192858
Iteration 18/25 | Loss: 0.00192858
Iteration 19/25 | Loss: 0.00192858
Iteration 20/25 | Loss: 0.00192858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019285765010863543, 0.0019285765010863543, 0.0019285765010863543, 0.0019285765010863543, 0.0019285765010863543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019285765010863543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192858
Iteration 2/1000 | Loss: 0.00011639
Iteration 3/1000 | Loss: 0.00007848
Iteration 4/1000 | Loss: 0.00005907
Iteration 5/1000 | Loss: 0.00005162
Iteration 6/1000 | Loss: 0.00004584
Iteration 7/1000 | Loss: 0.00004213
Iteration 8/1000 | Loss: 0.00003983
Iteration 9/1000 | Loss: 0.00003815
Iteration 10/1000 | Loss: 0.00003705
Iteration 11/1000 | Loss: 0.00003632
Iteration 12/1000 | Loss: 0.00003584
Iteration 13/1000 | Loss: 0.00003580
Iteration 14/1000 | Loss: 0.00003558
Iteration 15/1000 | Loss: 0.00003553
Iteration 16/1000 | Loss: 0.00003539
Iteration 17/1000 | Loss: 0.00003534
Iteration 18/1000 | Loss: 0.00003531
Iteration 19/1000 | Loss: 0.00003530
Iteration 20/1000 | Loss: 0.00003530
Iteration 21/1000 | Loss: 0.00003530
Iteration 22/1000 | Loss: 0.00003530
Iteration 23/1000 | Loss: 0.00003529
Iteration 24/1000 | Loss: 0.00003528
Iteration 25/1000 | Loss: 0.00003527
Iteration 26/1000 | Loss: 0.00003527
Iteration 27/1000 | Loss: 0.00003526
Iteration 28/1000 | Loss: 0.00003526
Iteration 29/1000 | Loss: 0.00003523
Iteration 30/1000 | Loss: 0.00003522
Iteration 31/1000 | Loss: 0.00003522
Iteration 32/1000 | Loss: 0.00003521
Iteration 33/1000 | Loss: 0.00003519
Iteration 34/1000 | Loss: 0.00003518
Iteration 35/1000 | Loss: 0.00003518
Iteration 36/1000 | Loss: 0.00003518
Iteration 37/1000 | Loss: 0.00003517
Iteration 38/1000 | Loss: 0.00003516
Iteration 39/1000 | Loss: 0.00003516
Iteration 40/1000 | Loss: 0.00003515
Iteration 41/1000 | Loss: 0.00003515
Iteration 42/1000 | Loss: 0.00003515
Iteration 43/1000 | Loss: 0.00003514
Iteration 44/1000 | Loss: 0.00003514
Iteration 45/1000 | Loss: 0.00003513
Iteration 46/1000 | Loss: 0.00003513
Iteration 47/1000 | Loss: 0.00003513
Iteration 48/1000 | Loss: 0.00003513
Iteration 49/1000 | Loss: 0.00003513
Iteration 50/1000 | Loss: 0.00003513
Iteration 51/1000 | Loss: 0.00003513
Iteration 52/1000 | Loss: 0.00003513
Iteration 53/1000 | Loss: 0.00003513
Iteration 54/1000 | Loss: 0.00003513
Iteration 55/1000 | Loss: 0.00003512
Iteration 56/1000 | Loss: 0.00003512
Iteration 57/1000 | Loss: 0.00003512
Iteration 58/1000 | Loss: 0.00003512
Iteration 59/1000 | Loss: 0.00003511
Iteration 60/1000 | Loss: 0.00003511
Iteration 61/1000 | Loss: 0.00003511
Iteration 62/1000 | Loss: 0.00003511
Iteration 63/1000 | Loss: 0.00003511
Iteration 64/1000 | Loss: 0.00003511
Iteration 65/1000 | Loss: 0.00003511
Iteration 66/1000 | Loss: 0.00003510
Iteration 67/1000 | Loss: 0.00003510
Iteration 68/1000 | Loss: 0.00003510
Iteration 69/1000 | Loss: 0.00003510
Iteration 70/1000 | Loss: 0.00003510
Iteration 71/1000 | Loss: 0.00003510
Iteration 72/1000 | Loss: 0.00003510
Iteration 73/1000 | Loss: 0.00003510
Iteration 74/1000 | Loss: 0.00003510
Iteration 75/1000 | Loss: 0.00003510
Iteration 76/1000 | Loss: 0.00003509
Iteration 77/1000 | Loss: 0.00003509
Iteration 78/1000 | Loss: 0.00003509
Iteration 79/1000 | Loss: 0.00003509
Iteration 80/1000 | Loss: 0.00003509
Iteration 81/1000 | Loss: 0.00003509
Iteration 82/1000 | Loss: 0.00003509
Iteration 83/1000 | Loss: 0.00003509
Iteration 84/1000 | Loss: 0.00003509
Iteration 85/1000 | Loss: 0.00003509
Iteration 86/1000 | Loss: 0.00003509
Iteration 87/1000 | Loss: 0.00003509
Iteration 88/1000 | Loss: 0.00003509
Iteration 89/1000 | Loss: 0.00003508
Iteration 90/1000 | Loss: 0.00003508
Iteration 91/1000 | Loss: 0.00003508
Iteration 92/1000 | Loss: 0.00003508
Iteration 93/1000 | Loss: 0.00003508
Iteration 94/1000 | Loss: 0.00003507
Iteration 95/1000 | Loss: 0.00003507
Iteration 96/1000 | Loss: 0.00003507
Iteration 97/1000 | Loss: 0.00003507
Iteration 98/1000 | Loss: 0.00003507
Iteration 99/1000 | Loss: 0.00003507
Iteration 100/1000 | Loss: 0.00003507
Iteration 101/1000 | Loss: 0.00003507
Iteration 102/1000 | Loss: 0.00003507
Iteration 103/1000 | Loss: 0.00003507
Iteration 104/1000 | Loss: 0.00003507
Iteration 105/1000 | Loss: 0.00003507
Iteration 106/1000 | Loss: 0.00003507
Iteration 107/1000 | Loss: 0.00003507
Iteration 108/1000 | Loss: 0.00003506
Iteration 109/1000 | Loss: 0.00003506
Iteration 110/1000 | Loss: 0.00003506
Iteration 111/1000 | Loss: 0.00003506
Iteration 112/1000 | Loss: 0.00003506
Iteration 113/1000 | Loss: 0.00003506
Iteration 114/1000 | Loss: 0.00003506
Iteration 115/1000 | Loss: 0.00003505
Iteration 116/1000 | Loss: 0.00003505
Iteration 117/1000 | Loss: 0.00003505
Iteration 118/1000 | Loss: 0.00003505
Iteration 119/1000 | Loss: 0.00003505
Iteration 120/1000 | Loss: 0.00003505
Iteration 121/1000 | Loss: 0.00003505
Iteration 122/1000 | Loss: 0.00003505
Iteration 123/1000 | Loss: 0.00003504
Iteration 124/1000 | Loss: 0.00003504
Iteration 125/1000 | Loss: 0.00003504
Iteration 126/1000 | Loss: 0.00003504
Iteration 127/1000 | Loss: 0.00003504
Iteration 128/1000 | Loss: 0.00003504
Iteration 129/1000 | Loss: 0.00003504
Iteration 130/1000 | Loss: 0.00003504
Iteration 131/1000 | Loss: 0.00003504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.504214691929519e-05, 3.504214691929519e-05, 3.504214691929519e-05, 3.504214691929519e-05, 3.504214691929519e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.504214691929519e-05

Optimization complete. Final v2v error: 4.873030185699463 mm

Highest mean error: 5.521900653839111 mm for frame 138

Lowest mean error: 4.3903398513793945 mm for frame 30

Saving results

Total time: 37.368890047073364
