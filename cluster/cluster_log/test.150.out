Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=150, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8400-8455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420118
Iteration 2/25 | Loss: 0.00136690
Iteration 3/25 | Loss: 0.00130755
Iteration 4/25 | Loss: 0.00129883
Iteration 5/25 | Loss: 0.00129768
Iteration 6/25 | Loss: 0.00129768
Iteration 7/25 | Loss: 0.00129768
Iteration 8/25 | Loss: 0.00129768
Iteration 9/25 | Loss: 0.00129768
Iteration 10/25 | Loss: 0.00129768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012976822908967733, 0.0012976822908967733, 0.0012976822908967733, 0.0012976822908967733, 0.0012976822908967733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012976822908967733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34060168
Iteration 2/25 | Loss: 0.00086086
Iteration 3/25 | Loss: 0.00086085
Iteration 4/25 | Loss: 0.00086085
Iteration 5/25 | Loss: 0.00086085
Iteration 6/25 | Loss: 0.00086085
Iteration 7/25 | Loss: 0.00086085
Iteration 8/25 | Loss: 0.00086085
Iteration 9/25 | Loss: 0.00086085
Iteration 10/25 | Loss: 0.00086085
Iteration 11/25 | Loss: 0.00086085
Iteration 12/25 | Loss: 0.00086085
Iteration 13/25 | Loss: 0.00086085
Iteration 14/25 | Loss: 0.00086085
Iteration 15/25 | Loss: 0.00086085
Iteration 16/25 | Loss: 0.00086085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008608524804003537, 0.0008608524804003537, 0.0008608524804003537, 0.0008608524804003537, 0.0008608524804003537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008608524804003537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086085
Iteration 2/1000 | Loss: 0.00003653
Iteration 3/1000 | Loss: 0.00002562
Iteration 4/1000 | Loss: 0.00002298
Iteration 5/1000 | Loss: 0.00002152
Iteration 6/1000 | Loss: 0.00002076
Iteration 7/1000 | Loss: 0.00002026
Iteration 8/1000 | Loss: 0.00001991
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001931
Iteration 11/1000 | Loss: 0.00001929
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001891
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001877
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001868
Iteration 18/1000 | Loss: 0.00001867
Iteration 19/1000 | Loss: 0.00001867
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001865
Iteration 22/1000 | Loss: 0.00001863
Iteration 23/1000 | Loss: 0.00001863
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001862
Iteration 26/1000 | Loss: 0.00001862
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001856
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001852
Iteration 33/1000 | Loss: 0.00001851
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001847
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001845
Iteration 40/1000 | Loss: 0.00001844
Iteration 41/1000 | Loss: 0.00001844
Iteration 42/1000 | Loss: 0.00001844
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001844
Iteration 45/1000 | Loss: 0.00001844
Iteration 46/1000 | Loss: 0.00001843
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001842
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001841
Iteration 53/1000 | Loss: 0.00001841
Iteration 54/1000 | Loss: 0.00001840
Iteration 55/1000 | Loss: 0.00001840
Iteration 56/1000 | Loss: 0.00001840
Iteration 57/1000 | Loss: 0.00001839
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001839
Iteration 60/1000 | Loss: 0.00001838
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001837
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001835
Iteration 67/1000 | Loss: 0.00001835
Iteration 68/1000 | Loss: 0.00001833
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001826
Iteration 95/1000 | Loss: 0.00001826
Iteration 96/1000 | Loss: 0.00001826
Iteration 97/1000 | Loss: 0.00001826
Iteration 98/1000 | Loss: 0.00001826
Iteration 99/1000 | Loss: 0.00001826
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001826
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8259408534504473e-05, 1.8259408534504473e-05, 1.8259408534504473e-05, 1.8259408534504473e-05, 1.8259408534504473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8259408534504473e-05

Optimization complete. Final v2v error: 3.5151681900024414 mm

Highest mean error: 4.076618671417236 mm for frame 139

Lowest mean error: 3.0982775688171387 mm for frame 3

Saving results

Total time: 38.02105736732483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400702
Iteration 2/25 | Loss: 0.00154272
Iteration 3/25 | Loss: 0.00133093
Iteration 4/25 | Loss: 0.00130379
Iteration 5/25 | Loss: 0.00129973
Iteration 6/25 | Loss: 0.00129918
Iteration 7/25 | Loss: 0.00129918
Iteration 8/25 | Loss: 0.00129918
Iteration 9/25 | Loss: 0.00129918
Iteration 10/25 | Loss: 0.00129918
Iteration 11/25 | Loss: 0.00129918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012991796247661114, 0.0012991796247661114, 0.0012991796247661114, 0.0012991796247661114, 0.0012991796247661114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012991796247661114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39884186
Iteration 2/25 | Loss: 0.00091744
Iteration 3/25 | Loss: 0.00091744
Iteration 4/25 | Loss: 0.00091744
Iteration 5/25 | Loss: 0.00091744
Iteration 6/25 | Loss: 0.00091744
Iteration 7/25 | Loss: 0.00091744
Iteration 8/25 | Loss: 0.00091744
Iteration 9/25 | Loss: 0.00091744
Iteration 10/25 | Loss: 0.00091744
Iteration 11/25 | Loss: 0.00091744
Iteration 12/25 | Loss: 0.00091744
Iteration 13/25 | Loss: 0.00091744
Iteration 14/25 | Loss: 0.00091744
Iteration 15/25 | Loss: 0.00091744
Iteration 16/25 | Loss: 0.00091744
Iteration 17/25 | Loss: 0.00091744
Iteration 18/25 | Loss: 0.00091744
Iteration 19/25 | Loss: 0.00091744
Iteration 20/25 | Loss: 0.00091744
Iteration 21/25 | Loss: 0.00091744
Iteration 22/25 | Loss: 0.00091744
Iteration 23/25 | Loss: 0.00091744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009174391161650419, 0.0009174391161650419, 0.0009174391161650419, 0.0009174391161650419, 0.0009174391161650419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009174391161650419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091744
Iteration 2/1000 | Loss: 0.00003859
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00001902
Iteration 5/1000 | Loss: 0.00001739
Iteration 6/1000 | Loss: 0.00001647
Iteration 7/1000 | Loss: 0.00001570
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001470
Iteration 11/1000 | Loss: 0.00001448
Iteration 12/1000 | Loss: 0.00001443
Iteration 13/1000 | Loss: 0.00001441
Iteration 14/1000 | Loss: 0.00001440
Iteration 15/1000 | Loss: 0.00001435
Iteration 16/1000 | Loss: 0.00001434
Iteration 17/1000 | Loss: 0.00001433
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001420
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001417
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001413
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001405
Iteration 30/1000 | Loss: 0.00001402
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001401
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001399
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001396
Iteration 41/1000 | Loss: 0.00001395
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001389
Iteration 50/1000 | Loss: 0.00001389
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001387
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001383
Iteration 62/1000 | Loss: 0.00001383
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001381
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001377
Iteration 81/1000 | Loss: 0.00001377
Iteration 82/1000 | Loss: 0.00001377
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001376
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001376
Iteration 88/1000 | Loss: 0.00001376
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001370
Iteration 104/1000 | Loss: 0.00001370
Iteration 105/1000 | Loss: 0.00001369
Iteration 106/1000 | Loss: 0.00001369
Iteration 107/1000 | Loss: 0.00001368
Iteration 108/1000 | Loss: 0.00001368
Iteration 109/1000 | Loss: 0.00001368
Iteration 110/1000 | Loss: 0.00001368
Iteration 111/1000 | Loss: 0.00001368
Iteration 112/1000 | Loss: 0.00001368
Iteration 113/1000 | Loss: 0.00001368
Iteration 114/1000 | Loss: 0.00001367
Iteration 115/1000 | Loss: 0.00001367
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001366
Iteration 119/1000 | Loss: 0.00001366
Iteration 120/1000 | Loss: 0.00001366
Iteration 121/1000 | Loss: 0.00001366
Iteration 122/1000 | Loss: 0.00001366
Iteration 123/1000 | Loss: 0.00001365
Iteration 124/1000 | Loss: 0.00001365
Iteration 125/1000 | Loss: 0.00001365
Iteration 126/1000 | Loss: 0.00001365
Iteration 127/1000 | Loss: 0.00001365
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001364
Iteration 130/1000 | Loss: 0.00001364
Iteration 131/1000 | Loss: 0.00001363
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001362
Iteration 135/1000 | Loss: 0.00001362
Iteration 136/1000 | Loss: 0.00001362
Iteration 137/1000 | Loss: 0.00001361
Iteration 138/1000 | Loss: 0.00001361
Iteration 139/1000 | Loss: 0.00001361
Iteration 140/1000 | Loss: 0.00001360
Iteration 141/1000 | Loss: 0.00001360
Iteration 142/1000 | Loss: 0.00001360
Iteration 143/1000 | Loss: 0.00001359
Iteration 144/1000 | Loss: 0.00001359
Iteration 145/1000 | Loss: 0.00001359
Iteration 146/1000 | Loss: 0.00001359
Iteration 147/1000 | Loss: 0.00001358
Iteration 148/1000 | Loss: 0.00001358
Iteration 149/1000 | Loss: 0.00001358
Iteration 150/1000 | Loss: 0.00001358
Iteration 151/1000 | Loss: 0.00001358
Iteration 152/1000 | Loss: 0.00001358
Iteration 153/1000 | Loss: 0.00001358
Iteration 154/1000 | Loss: 0.00001358
Iteration 155/1000 | Loss: 0.00001358
Iteration 156/1000 | Loss: 0.00001358
Iteration 157/1000 | Loss: 0.00001357
Iteration 158/1000 | Loss: 0.00001357
Iteration 159/1000 | Loss: 0.00001357
Iteration 160/1000 | Loss: 0.00001357
Iteration 161/1000 | Loss: 0.00001357
Iteration 162/1000 | Loss: 0.00001357
Iteration 163/1000 | Loss: 0.00001357
Iteration 164/1000 | Loss: 0.00001357
Iteration 165/1000 | Loss: 0.00001357
Iteration 166/1000 | Loss: 0.00001356
Iteration 167/1000 | Loss: 0.00001356
Iteration 168/1000 | Loss: 0.00001356
Iteration 169/1000 | Loss: 0.00001356
Iteration 170/1000 | Loss: 0.00001356
Iteration 171/1000 | Loss: 0.00001356
Iteration 172/1000 | Loss: 0.00001355
Iteration 173/1000 | Loss: 0.00001355
Iteration 174/1000 | Loss: 0.00001355
Iteration 175/1000 | Loss: 0.00001355
Iteration 176/1000 | Loss: 0.00001355
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001354
Iteration 182/1000 | Loss: 0.00001354
Iteration 183/1000 | Loss: 0.00001354
Iteration 184/1000 | Loss: 0.00001354
Iteration 185/1000 | Loss: 0.00001354
Iteration 186/1000 | Loss: 0.00001354
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001354
Iteration 189/1000 | Loss: 0.00001353
Iteration 190/1000 | Loss: 0.00001353
Iteration 191/1000 | Loss: 0.00001353
Iteration 192/1000 | Loss: 0.00001353
Iteration 193/1000 | Loss: 0.00001353
Iteration 194/1000 | Loss: 0.00001353
Iteration 195/1000 | Loss: 0.00001352
Iteration 196/1000 | Loss: 0.00001352
Iteration 197/1000 | Loss: 0.00001352
Iteration 198/1000 | Loss: 0.00001352
Iteration 199/1000 | Loss: 0.00001352
Iteration 200/1000 | Loss: 0.00001352
Iteration 201/1000 | Loss: 0.00001352
Iteration 202/1000 | Loss: 0.00001352
Iteration 203/1000 | Loss: 0.00001352
Iteration 204/1000 | Loss: 0.00001352
Iteration 205/1000 | Loss: 0.00001352
Iteration 206/1000 | Loss: 0.00001352
Iteration 207/1000 | Loss: 0.00001352
Iteration 208/1000 | Loss: 0.00001352
Iteration 209/1000 | Loss: 0.00001352
Iteration 210/1000 | Loss: 0.00001351
Iteration 211/1000 | Loss: 0.00001351
Iteration 212/1000 | Loss: 0.00001351
Iteration 213/1000 | Loss: 0.00001351
Iteration 214/1000 | Loss: 0.00001351
Iteration 215/1000 | Loss: 0.00001351
Iteration 216/1000 | Loss: 0.00001351
Iteration 217/1000 | Loss: 0.00001351
Iteration 218/1000 | Loss: 0.00001351
Iteration 219/1000 | Loss: 0.00001350
Iteration 220/1000 | Loss: 0.00001350
Iteration 221/1000 | Loss: 0.00001350
Iteration 222/1000 | Loss: 0.00001350
Iteration 223/1000 | Loss: 0.00001350
Iteration 224/1000 | Loss: 0.00001350
Iteration 225/1000 | Loss: 0.00001350
Iteration 226/1000 | Loss: 0.00001350
Iteration 227/1000 | Loss: 0.00001350
Iteration 228/1000 | Loss: 0.00001350
Iteration 229/1000 | Loss: 0.00001350
Iteration 230/1000 | Loss: 0.00001350
Iteration 231/1000 | Loss: 0.00001350
Iteration 232/1000 | Loss: 0.00001350
Iteration 233/1000 | Loss: 0.00001350
Iteration 234/1000 | Loss: 0.00001350
Iteration 235/1000 | Loss: 0.00001350
Iteration 236/1000 | Loss: 0.00001350
Iteration 237/1000 | Loss: 0.00001350
Iteration 238/1000 | Loss: 0.00001350
Iteration 239/1000 | Loss: 0.00001350
Iteration 240/1000 | Loss: 0.00001350
Iteration 241/1000 | Loss: 0.00001350
Iteration 242/1000 | Loss: 0.00001350
Iteration 243/1000 | Loss: 0.00001350
Iteration 244/1000 | Loss: 0.00001350
Iteration 245/1000 | Loss: 0.00001350
Iteration 246/1000 | Loss: 0.00001350
Iteration 247/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.3497897271008696e-05, 1.3497897271008696e-05, 1.3497897271008696e-05, 1.3497897271008696e-05, 1.3497897271008696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3497897271008696e-05

Optimization complete. Final v2v error: 3.1316373348236084 mm

Highest mean error: 3.679715633392334 mm for frame 80

Lowest mean error: 2.874948501586914 mm for frame 217

Saving results

Total time: 50.35707664489746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811162
Iteration 2/25 | Loss: 0.00143336
Iteration 3/25 | Loss: 0.00133050
Iteration 4/25 | Loss: 0.00131599
Iteration 5/25 | Loss: 0.00131170
Iteration 6/25 | Loss: 0.00131159
Iteration 7/25 | Loss: 0.00131158
Iteration 8/25 | Loss: 0.00130775
Iteration 9/25 | Loss: 0.00130692
Iteration 10/25 | Loss: 0.00130762
Iteration 11/25 | Loss: 0.00130711
Iteration 12/25 | Loss: 0.00130609
Iteration 13/25 | Loss: 0.00130573
Iteration 14/25 | Loss: 0.00130566
Iteration 15/25 | Loss: 0.00130565
Iteration 16/25 | Loss: 0.00130565
Iteration 17/25 | Loss: 0.00130565
Iteration 18/25 | Loss: 0.00130565
Iteration 19/25 | Loss: 0.00130565
Iteration 20/25 | Loss: 0.00130565
Iteration 21/25 | Loss: 0.00130565
Iteration 22/25 | Loss: 0.00130565
Iteration 23/25 | Loss: 0.00130565
Iteration 24/25 | Loss: 0.00130564
Iteration 25/25 | Loss: 0.00130564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66552973
Iteration 2/25 | Loss: 0.00081938
Iteration 3/25 | Loss: 0.00081937
Iteration 4/25 | Loss: 0.00081937
Iteration 5/25 | Loss: 0.00081937
Iteration 6/25 | Loss: 0.00081937
Iteration 7/25 | Loss: 0.00081937
Iteration 8/25 | Loss: 0.00081937
Iteration 9/25 | Loss: 0.00081937
Iteration 10/25 | Loss: 0.00081937
Iteration 11/25 | Loss: 0.00081937
Iteration 12/25 | Loss: 0.00081937
Iteration 13/25 | Loss: 0.00081937
Iteration 14/25 | Loss: 0.00081937
Iteration 15/25 | Loss: 0.00081937
Iteration 16/25 | Loss: 0.00081937
Iteration 17/25 | Loss: 0.00081937
Iteration 18/25 | Loss: 0.00081937
Iteration 19/25 | Loss: 0.00081937
Iteration 20/25 | Loss: 0.00081937
Iteration 21/25 | Loss: 0.00081937
Iteration 22/25 | Loss: 0.00081936
Iteration 23/25 | Loss: 0.00081936
Iteration 24/25 | Loss: 0.00081936
Iteration 25/25 | Loss: 0.00081936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081936
Iteration 2/1000 | Loss: 0.00002985
Iteration 3/1000 | Loss: 0.00002260
Iteration 4/1000 | Loss: 0.00002074
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001883
Iteration 7/1000 | Loss: 0.00001838
Iteration 8/1000 | Loss: 0.00001815
Iteration 9/1000 | Loss: 0.00001780
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001746
Iteration 12/1000 | Loss: 0.00001746
Iteration 13/1000 | Loss: 0.00001744
Iteration 14/1000 | Loss: 0.00001744
Iteration 15/1000 | Loss: 0.00001743
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001728
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001722
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001705
Iteration 46/1000 | Loss: 0.00001705
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001704
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001701
Iteration 68/1000 | Loss: 0.00001701
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001701
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001700
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001696
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001696
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001695
Iteration 99/1000 | Loss: 0.00001695
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001695
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001694
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001693
Iteration 115/1000 | Loss: 0.00001693
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001693
Iteration 120/1000 | Loss: 0.00001692
Iteration 121/1000 | Loss: 0.00001692
Iteration 122/1000 | Loss: 0.00001692
Iteration 123/1000 | Loss: 0.00001692
Iteration 124/1000 | Loss: 0.00001692
Iteration 125/1000 | Loss: 0.00001692
Iteration 126/1000 | Loss: 0.00001692
Iteration 127/1000 | Loss: 0.00001692
Iteration 128/1000 | Loss: 0.00001692
Iteration 129/1000 | Loss: 0.00001692
Iteration 130/1000 | Loss: 0.00001692
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001691
Iteration 133/1000 | Loss: 0.00001691
Iteration 134/1000 | Loss: 0.00001691
Iteration 135/1000 | Loss: 0.00001691
Iteration 136/1000 | Loss: 0.00001691
Iteration 137/1000 | Loss: 0.00001691
Iteration 138/1000 | Loss: 0.00001691
Iteration 139/1000 | Loss: 0.00001691
Iteration 140/1000 | Loss: 0.00001691
Iteration 141/1000 | Loss: 0.00001690
Iteration 142/1000 | Loss: 0.00001690
Iteration 143/1000 | Loss: 0.00001690
Iteration 144/1000 | Loss: 0.00001689
Iteration 145/1000 | Loss: 0.00001689
Iteration 146/1000 | Loss: 0.00001689
Iteration 147/1000 | Loss: 0.00001689
Iteration 148/1000 | Loss: 0.00001689
Iteration 149/1000 | Loss: 0.00001689
Iteration 150/1000 | Loss: 0.00001689
Iteration 151/1000 | Loss: 0.00001689
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001688
Iteration 154/1000 | Loss: 0.00001688
Iteration 155/1000 | Loss: 0.00001688
Iteration 156/1000 | Loss: 0.00001688
Iteration 157/1000 | Loss: 0.00001688
Iteration 158/1000 | Loss: 0.00001688
Iteration 159/1000 | Loss: 0.00001688
Iteration 160/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.6882826457731426e-05, 1.6882826457731426e-05, 1.6882826457731426e-05, 1.6882826457731426e-05, 1.6882826457731426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6882826457731426e-05

Optimization complete. Final v2v error: 3.4714434146881104 mm

Highest mean error: 3.874987840652466 mm for frame 204

Lowest mean error: 3.213270664215088 mm for frame 106

Saving results

Total time: 60.865620851516724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026642
Iteration 2/25 | Loss: 0.01026642
Iteration 3/25 | Loss: 0.01026641
Iteration 4/25 | Loss: 0.01026641
Iteration 5/25 | Loss: 0.01026641
Iteration 6/25 | Loss: 0.01026641
Iteration 7/25 | Loss: 0.01026641
Iteration 8/25 | Loss: 0.01026641
Iteration 9/25 | Loss: 0.01026641
Iteration 10/25 | Loss: 0.01026640
Iteration 11/25 | Loss: 0.01026640
Iteration 12/25 | Loss: 0.01026640
Iteration 13/25 | Loss: 0.01026640
Iteration 14/25 | Loss: 0.01026640
Iteration 15/25 | Loss: 0.01026639
Iteration 16/25 | Loss: 0.01026639
Iteration 17/25 | Loss: 0.01026639
Iteration 18/25 | Loss: 0.01026639
Iteration 19/25 | Loss: 0.01026639
Iteration 20/25 | Loss: 0.01026639
Iteration 21/25 | Loss: 0.01026638
Iteration 22/25 | Loss: 0.01026638
Iteration 23/25 | Loss: 0.01026638
Iteration 24/25 | Loss: 0.01026638
Iteration 25/25 | Loss: 0.01026638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66996753
Iteration 2/25 | Loss: 0.08356360
Iteration 3/25 | Loss: 0.08355618
Iteration 4/25 | Loss: 0.08355616
Iteration 5/25 | Loss: 0.08355615
Iteration 6/25 | Loss: 0.08355613
Iteration 7/25 | Loss: 0.08355607
Iteration 8/25 | Loss: 0.08273184
Iteration 9/25 | Loss: 0.08273177
Iteration 10/25 | Loss: 0.08273175
Iteration 11/25 | Loss: 0.08273175
Iteration 12/25 | Loss: 0.08273174
Iteration 13/25 | Loss: 0.08273173
Iteration 14/25 | Loss: 0.08273173
Iteration 15/25 | Loss: 0.08273173
Iteration 16/25 | Loss: 0.08273173
Iteration 17/25 | Loss: 0.08273173
Iteration 18/25 | Loss: 0.08273173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.08273173123598099, 0.08273173123598099, 0.08273173123598099, 0.08273173123598099, 0.08273173123598099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08273173123598099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08273173
Iteration 2/1000 | Loss: 0.00077056
Iteration 3/1000 | Loss: 0.00025549
Iteration 4/1000 | Loss: 0.00026983
Iteration 5/1000 | Loss: 0.00015103
Iteration 6/1000 | Loss: 0.00010095
Iteration 7/1000 | Loss: 0.00011700
Iteration 8/1000 | Loss: 0.00013775
Iteration 9/1000 | Loss: 0.00007149
Iteration 10/1000 | Loss: 0.00004637
Iteration 11/1000 | Loss: 0.00003615
Iteration 12/1000 | Loss: 0.00005101
Iteration 13/1000 | Loss: 0.00008905
Iteration 14/1000 | Loss: 0.00002813
Iteration 15/1000 | Loss: 0.00007124
Iteration 16/1000 | Loss: 0.00002494
Iteration 17/1000 | Loss: 0.00002359
Iteration 18/1000 | Loss: 0.00010463
Iteration 19/1000 | Loss: 0.00003707
Iteration 20/1000 | Loss: 0.00006284
Iteration 21/1000 | Loss: 0.00003102
Iteration 22/1000 | Loss: 0.00007702
Iteration 23/1000 | Loss: 0.00002052
Iteration 24/1000 | Loss: 0.00003105
Iteration 25/1000 | Loss: 0.00006468
Iteration 26/1000 | Loss: 0.00004990
Iteration 27/1000 | Loss: 0.00009595
Iteration 28/1000 | Loss: 0.00003817
Iteration 29/1000 | Loss: 0.00001857
Iteration 30/1000 | Loss: 0.00003340
Iteration 31/1000 | Loss: 0.00001810
Iteration 32/1000 | Loss: 0.00006428
Iteration 33/1000 | Loss: 0.00002570
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00005231
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00003607
Iteration 38/1000 | Loss: 0.00001623
Iteration 39/1000 | Loss: 0.00001623
Iteration 40/1000 | Loss: 0.00002736
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00003334
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00003226
Iteration 46/1000 | Loss: 0.00007147
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001505
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001505
Iteration 52/1000 | Loss: 0.00001505
Iteration 53/1000 | Loss: 0.00001505
Iteration 54/1000 | Loss: 0.00001505
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001498
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00003406
Iteration 62/1000 | Loss: 0.00001577
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001789
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001482
Iteration 67/1000 | Loss: 0.00001482
Iteration 68/1000 | Loss: 0.00001482
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001481
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00002812
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001492
Iteration 82/1000 | Loss: 0.00001580
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001471
Iteration 86/1000 | Loss: 0.00001471
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001471
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001468
Iteration 104/1000 | Loss: 0.00001479
Iteration 105/1000 | Loss: 0.00001468
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00004036
Iteration 109/1000 | Loss: 0.00001466
Iteration 110/1000 | Loss: 0.00001466
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001467
Iteration 113/1000 | Loss: 0.00001465
Iteration 114/1000 | Loss: 0.00002436
Iteration 115/1000 | Loss: 0.00002123
Iteration 116/1000 | Loss: 0.00001676
Iteration 117/1000 | Loss: 0.00001466
Iteration 118/1000 | Loss: 0.00001466
Iteration 119/1000 | Loss: 0.00001485
Iteration 120/1000 | Loss: 0.00001483
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00001481
Iteration 124/1000 | Loss: 0.00001481
Iteration 125/1000 | Loss: 0.00001500
Iteration 126/1000 | Loss: 0.00001500
Iteration 127/1000 | Loss: 0.00001500
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001478
Iteration 130/1000 | Loss: 0.00001460
Iteration 131/1000 | Loss: 0.00001460
Iteration 132/1000 | Loss: 0.00001460
Iteration 133/1000 | Loss: 0.00001460
Iteration 134/1000 | Loss: 0.00001460
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001513
Iteration 137/1000 | Loss: 0.00001509
Iteration 138/1000 | Loss: 0.00001457
Iteration 139/1000 | Loss: 0.00001457
Iteration 140/1000 | Loss: 0.00001457
Iteration 141/1000 | Loss: 0.00001457
Iteration 142/1000 | Loss: 0.00001457
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001457
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001457
Iteration 150/1000 | Loss: 0.00001457
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001457
Iteration 153/1000 | Loss: 0.00001457
Iteration 154/1000 | Loss: 0.00001457
Iteration 155/1000 | Loss: 0.00001457
Iteration 156/1000 | Loss: 0.00001457
Iteration 157/1000 | Loss: 0.00001457
Iteration 158/1000 | Loss: 0.00001457
Iteration 159/1000 | Loss: 0.00001457
Iteration 160/1000 | Loss: 0.00001457
Iteration 161/1000 | Loss: 0.00001457
Iteration 162/1000 | Loss: 0.00001457
Iteration 163/1000 | Loss: 0.00001457
Iteration 164/1000 | Loss: 0.00001457
Iteration 165/1000 | Loss: 0.00001457
Iteration 166/1000 | Loss: 0.00001457
Iteration 167/1000 | Loss: 0.00001457
Iteration 168/1000 | Loss: 0.00001457
Iteration 169/1000 | Loss: 0.00001457
Iteration 170/1000 | Loss: 0.00001457
Iteration 171/1000 | Loss: 0.00001457
Iteration 172/1000 | Loss: 0.00001457
Iteration 173/1000 | Loss: 0.00001457
Iteration 174/1000 | Loss: 0.00001457
Iteration 175/1000 | Loss: 0.00001457
Iteration 176/1000 | Loss: 0.00001457
Iteration 177/1000 | Loss: 0.00001457
Iteration 178/1000 | Loss: 0.00001457
Iteration 179/1000 | Loss: 0.00001457
Iteration 180/1000 | Loss: 0.00001457
Iteration 181/1000 | Loss: 0.00001457
Iteration 182/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.4569732229574583e-05, 1.4569732229574583e-05, 1.4569732229574583e-05, 1.4569732229574583e-05, 1.4569732229574583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4569732229574583e-05

Optimization complete. Final v2v error: 3.3053946495056152 mm

Highest mean error: 3.583446979522705 mm for frame 62

Lowest mean error: 3.032524585723877 mm for frame 31

Saving results

Total time: 108.61043572425842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791491
Iteration 2/25 | Loss: 0.00171920
Iteration 3/25 | Loss: 0.00142408
Iteration 4/25 | Loss: 0.00139579
Iteration 5/25 | Loss: 0.00138784
Iteration 6/25 | Loss: 0.00138490
Iteration 7/25 | Loss: 0.00138465
Iteration 8/25 | Loss: 0.00138465
Iteration 9/25 | Loss: 0.00138465
Iteration 10/25 | Loss: 0.00138465
Iteration 11/25 | Loss: 0.00138465
Iteration 12/25 | Loss: 0.00138465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013846486108377576, 0.0013846486108377576, 0.0013846486108377576, 0.0013846486108377576, 0.0013846486108377576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013846486108377576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17532802
Iteration 2/25 | Loss: 0.00101500
Iteration 3/25 | Loss: 0.00101499
Iteration 4/25 | Loss: 0.00101499
Iteration 5/25 | Loss: 0.00101499
Iteration 6/25 | Loss: 0.00101499
Iteration 7/25 | Loss: 0.00101499
Iteration 8/25 | Loss: 0.00101499
Iteration 9/25 | Loss: 0.00101499
Iteration 10/25 | Loss: 0.00101499
Iteration 11/25 | Loss: 0.00101499
Iteration 12/25 | Loss: 0.00101499
Iteration 13/25 | Loss: 0.00101499
Iteration 14/25 | Loss: 0.00101499
Iteration 15/25 | Loss: 0.00101499
Iteration 16/25 | Loss: 0.00101499
Iteration 17/25 | Loss: 0.00101499
Iteration 18/25 | Loss: 0.00101499
Iteration 19/25 | Loss: 0.00101499
Iteration 20/25 | Loss: 0.00101499
Iteration 21/25 | Loss: 0.00101499
Iteration 22/25 | Loss: 0.00101499
Iteration 23/25 | Loss: 0.00101499
Iteration 24/25 | Loss: 0.00101499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010149911977350712, 0.0010149911977350712, 0.0010149911977350712, 0.0010149911977350712, 0.0010149911977350712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010149911977350712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101499
Iteration 2/1000 | Loss: 0.00007175
Iteration 3/1000 | Loss: 0.00004724
Iteration 4/1000 | Loss: 0.00004152
Iteration 5/1000 | Loss: 0.00003947
Iteration 6/1000 | Loss: 0.00003782
Iteration 7/1000 | Loss: 0.00003657
Iteration 8/1000 | Loss: 0.00003563
Iteration 9/1000 | Loss: 0.00003516
Iteration 10/1000 | Loss: 0.00003471
Iteration 11/1000 | Loss: 0.00003424
Iteration 12/1000 | Loss: 0.00003392
Iteration 13/1000 | Loss: 0.00003371
Iteration 14/1000 | Loss: 0.00003346
Iteration 15/1000 | Loss: 0.00003334
Iteration 16/1000 | Loss: 0.00003326
Iteration 17/1000 | Loss: 0.00003321
Iteration 18/1000 | Loss: 0.00003308
Iteration 19/1000 | Loss: 0.00003303
Iteration 20/1000 | Loss: 0.00003299
Iteration 21/1000 | Loss: 0.00003292
Iteration 22/1000 | Loss: 0.00003290
Iteration 23/1000 | Loss: 0.00003287
Iteration 24/1000 | Loss: 0.00003285
Iteration 25/1000 | Loss: 0.00003283
Iteration 26/1000 | Loss: 0.00003282
Iteration 27/1000 | Loss: 0.00003281
Iteration 28/1000 | Loss: 0.00003280
Iteration 29/1000 | Loss: 0.00003280
Iteration 30/1000 | Loss: 0.00003279
Iteration 31/1000 | Loss: 0.00003279
Iteration 32/1000 | Loss: 0.00003278
Iteration 33/1000 | Loss: 0.00003277
Iteration 34/1000 | Loss: 0.00003276
Iteration 35/1000 | Loss: 0.00003276
Iteration 36/1000 | Loss: 0.00003276
Iteration 37/1000 | Loss: 0.00003273
Iteration 38/1000 | Loss: 0.00003272
Iteration 39/1000 | Loss: 0.00003272
Iteration 40/1000 | Loss: 0.00003271
Iteration 41/1000 | Loss: 0.00003271
Iteration 42/1000 | Loss: 0.00003269
Iteration 43/1000 | Loss: 0.00003269
Iteration 44/1000 | Loss: 0.00003269
Iteration 45/1000 | Loss: 0.00003268
Iteration 46/1000 | Loss: 0.00003268
Iteration 47/1000 | Loss: 0.00003267
Iteration 48/1000 | Loss: 0.00003267
Iteration 49/1000 | Loss: 0.00003267
Iteration 50/1000 | Loss: 0.00003266
Iteration 51/1000 | Loss: 0.00003266
Iteration 52/1000 | Loss: 0.00003266
Iteration 53/1000 | Loss: 0.00003264
Iteration 54/1000 | Loss: 0.00003264
Iteration 55/1000 | Loss: 0.00003263
Iteration 56/1000 | Loss: 0.00003263
Iteration 57/1000 | Loss: 0.00003262
Iteration 58/1000 | Loss: 0.00003262
Iteration 59/1000 | Loss: 0.00003262
Iteration 60/1000 | Loss: 0.00003262
Iteration 61/1000 | Loss: 0.00003262
Iteration 62/1000 | Loss: 0.00003262
Iteration 63/1000 | Loss: 0.00003262
Iteration 64/1000 | Loss: 0.00003261
Iteration 65/1000 | Loss: 0.00003261
Iteration 66/1000 | Loss: 0.00003261
Iteration 67/1000 | Loss: 0.00003261
Iteration 68/1000 | Loss: 0.00003261
Iteration 69/1000 | Loss: 0.00003261
Iteration 70/1000 | Loss: 0.00003261
Iteration 71/1000 | Loss: 0.00003261
Iteration 72/1000 | Loss: 0.00003261
Iteration 73/1000 | Loss: 0.00003261
Iteration 74/1000 | Loss: 0.00003260
Iteration 75/1000 | Loss: 0.00003260
Iteration 76/1000 | Loss: 0.00003260
Iteration 77/1000 | Loss: 0.00003260
Iteration 78/1000 | Loss: 0.00003260
Iteration 79/1000 | Loss: 0.00003260
Iteration 80/1000 | Loss: 0.00003260
Iteration 81/1000 | Loss: 0.00003260
Iteration 82/1000 | Loss: 0.00003260
Iteration 83/1000 | Loss: 0.00003260
Iteration 84/1000 | Loss: 0.00003260
Iteration 85/1000 | Loss: 0.00003260
Iteration 86/1000 | Loss: 0.00003259
Iteration 87/1000 | Loss: 0.00003259
Iteration 88/1000 | Loss: 0.00003259
Iteration 89/1000 | Loss: 0.00003259
Iteration 90/1000 | Loss: 0.00003259
Iteration 91/1000 | Loss: 0.00003259
Iteration 92/1000 | Loss: 0.00003259
Iteration 93/1000 | Loss: 0.00003259
Iteration 94/1000 | Loss: 0.00003258
Iteration 95/1000 | Loss: 0.00003258
Iteration 96/1000 | Loss: 0.00003258
Iteration 97/1000 | Loss: 0.00003258
Iteration 98/1000 | Loss: 0.00003258
Iteration 99/1000 | Loss: 0.00003258
Iteration 100/1000 | Loss: 0.00003258
Iteration 101/1000 | Loss: 0.00003257
Iteration 102/1000 | Loss: 0.00003257
Iteration 103/1000 | Loss: 0.00003257
Iteration 104/1000 | Loss: 0.00003257
Iteration 105/1000 | Loss: 0.00003257
Iteration 106/1000 | Loss: 0.00003257
Iteration 107/1000 | Loss: 0.00003257
Iteration 108/1000 | Loss: 0.00003257
Iteration 109/1000 | Loss: 0.00003257
Iteration 110/1000 | Loss: 0.00003257
Iteration 111/1000 | Loss: 0.00003256
Iteration 112/1000 | Loss: 0.00003256
Iteration 113/1000 | Loss: 0.00003256
Iteration 114/1000 | Loss: 0.00003256
Iteration 115/1000 | Loss: 0.00003256
Iteration 116/1000 | Loss: 0.00003256
Iteration 117/1000 | Loss: 0.00003256
Iteration 118/1000 | Loss: 0.00003256
Iteration 119/1000 | Loss: 0.00003256
Iteration 120/1000 | Loss: 0.00003256
Iteration 121/1000 | Loss: 0.00003255
Iteration 122/1000 | Loss: 0.00003255
Iteration 123/1000 | Loss: 0.00003255
Iteration 124/1000 | Loss: 0.00003255
Iteration 125/1000 | Loss: 0.00003255
Iteration 126/1000 | Loss: 0.00003255
Iteration 127/1000 | Loss: 0.00003255
Iteration 128/1000 | Loss: 0.00003255
Iteration 129/1000 | Loss: 0.00003255
Iteration 130/1000 | Loss: 0.00003255
Iteration 131/1000 | Loss: 0.00003255
Iteration 132/1000 | Loss: 0.00003255
Iteration 133/1000 | Loss: 0.00003255
Iteration 134/1000 | Loss: 0.00003255
Iteration 135/1000 | Loss: 0.00003254
Iteration 136/1000 | Loss: 0.00003254
Iteration 137/1000 | Loss: 0.00003254
Iteration 138/1000 | Loss: 0.00003254
Iteration 139/1000 | Loss: 0.00003254
Iteration 140/1000 | Loss: 0.00003254
Iteration 141/1000 | Loss: 0.00003254
Iteration 142/1000 | Loss: 0.00003254
Iteration 143/1000 | Loss: 0.00003254
Iteration 144/1000 | Loss: 0.00003254
Iteration 145/1000 | Loss: 0.00003254
Iteration 146/1000 | Loss: 0.00003254
Iteration 147/1000 | Loss: 0.00003254
Iteration 148/1000 | Loss: 0.00003254
Iteration 149/1000 | Loss: 0.00003254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [3.254167313571088e-05, 3.254167313571088e-05, 3.254167313571088e-05, 3.254167313571088e-05, 3.254167313571088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.254167313571088e-05

Optimization complete. Final v2v error: 4.648775100708008 mm

Highest mean error: 5.454855442047119 mm for frame 158

Lowest mean error: 3.206803321838379 mm for frame 198

Saving results

Total time: 47.79887676239014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791352
Iteration 2/25 | Loss: 0.00147380
Iteration 3/25 | Loss: 0.00137742
Iteration 4/25 | Loss: 0.00136713
Iteration 5/25 | Loss: 0.00136524
Iteration 6/25 | Loss: 0.00136524
Iteration 7/25 | Loss: 0.00136524
Iteration 8/25 | Loss: 0.00136524
Iteration 9/25 | Loss: 0.00136524
Iteration 10/25 | Loss: 0.00136524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001365244621410966, 0.001365244621410966, 0.001365244621410966, 0.001365244621410966, 0.001365244621410966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001365244621410966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29967976
Iteration 2/25 | Loss: 0.00075658
Iteration 3/25 | Loss: 0.00075653
Iteration 4/25 | Loss: 0.00075653
Iteration 5/25 | Loss: 0.00075653
Iteration 6/25 | Loss: 0.00075653
Iteration 7/25 | Loss: 0.00075652
Iteration 8/25 | Loss: 0.00075652
Iteration 9/25 | Loss: 0.00075652
Iteration 10/25 | Loss: 0.00075652
Iteration 11/25 | Loss: 0.00075652
Iteration 12/25 | Loss: 0.00075652
Iteration 13/25 | Loss: 0.00075652
Iteration 14/25 | Loss: 0.00075652
Iteration 15/25 | Loss: 0.00075652
Iteration 16/25 | Loss: 0.00075652
Iteration 17/25 | Loss: 0.00075652
Iteration 18/25 | Loss: 0.00075652
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007565240957774222, 0.0007565240957774222, 0.0007565240957774222, 0.0007565240957774222, 0.0007565240957774222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007565240957774222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075652
Iteration 2/1000 | Loss: 0.00004479
Iteration 3/1000 | Loss: 0.00003422
Iteration 4/1000 | Loss: 0.00003055
Iteration 5/1000 | Loss: 0.00002912
Iteration 6/1000 | Loss: 0.00002781
Iteration 7/1000 | Loss: 0.00002696
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002599
Iteration 10/1000 | Loss: 0.00002566
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002556
Iteration 13/1000 | Loss: 0.00002553
Iteration 14/1000 | Loss: 0.00002552
Iteration 15/1000 | Loss: 0.00002525
Iteration 16/1000 | Loss: 0.00002517
Iteration 17/1000 | Loss: 0.00002511
Iteration 18/1000 | Loss: 0.00002510
Iteration 19/1000 | Loss: 0.00002492
Iteration 20/1000 | Loss: 0.00002481
Iteration 21/1000 | Loss: 0.00002480
Iteration 22/1000 | Loss: 0.00002476
Iteration 23/1000 | Loss: 0.00002474
Iteration 24/1000 | Loss: 0.00002471
Iteration 25/1000 | Loss: 0.00002467
Iteration 26/1000 | Loss: 0.00002466
Iteration 27/1000 | Loss: 0.00002466
Iteration 28/1000 | Loss: 0.00002465
Iteration 29/1000 | Loss: 0.00002464
Iteration 30/1000 | Loss: 0.00002462
Iteration 31/1000 | Loss: 0.00002461
Iteration 32/1000 | Loss: 0.00002461
Iteration 33/1000 | Loss: 0.00002456
Iteration 34/1000 | Loss: 0.00002456
Iteration 35/1000 | Loss: 0.00002455
Iteration 36/1000 | Loss: 0.00002455
Iteration 37/1000 | Loss: 0.00002455
Iteration 38/1000 | Loss: 0.00002454
Iteration 39/1000 | Loss: 0.00002454
Iteration 40/1000 | Loss: 0.00002454
Iteration 41/1000 | Loss: 0.00002453
Iteration 42/1000 | Loss: 0.00002451
Iteration 43/1000 | Loss: 0.00002450
Iteration 44/1000 | Loss: 0.00002449
Iteration 45/1000 | Loss: 0.00002449
Iteration 46/1000 | Loss: 0.00002449
Iteration 47/1000 | Loss: 0.00002449
Iteration 48/1000 | Loss: 0.00002448
Iteration 49/1000 | Loss: 0.00002447
Iteration 50/1000 | Loss: 0.00002447
Iteration 51/1000 | Loss: 0.00002447
Iteration 52/1000 | Loss: 0.00002446
Iteration 53/1000 | Loss: 0.00002446
Iteration 54/1000 | Loss: 0.00002445
Iteration 55/1000 | Loss: 0.00002445
Iteration 56/1000 | Loss: 0.00002444
Iteration 57/1000 | Loss: 0.00002444
Iteration 58/1000 | Loss: 0.00002443
Iteration 59/1000 | Loss: 0.00002443
Iteration 60/1000 | Loss: 0.00002443
Iteration 61/1000 | Loss: 0.00002442
Iteration 62/1000 | Loss: 0.00002442
Iteration 63/1000 | Loss: 0.00002442
Iteration 64/1000 | Loss: 0.00002441
Iteration 65/1000 | Loss: 0.00002441
Iteration 66/1000 | Loss: 0.00002440
Iteration 67/1000 | Loss: 0.00002440
Iteration 68/1000 | Loss: 0.00002440
Iteration 69/1000 | Loss: 0.00002440
Iteration 70/1000 | Loss: 0.00002440
Iteration 71/1000 | Loss: 0.00002440
Iteration 72/1000 | Loss: 0.00002440
Iteration 73/1000 | Loss: 0.00002440
Iteration 74/1000 | Loss: 0.00002440
Iteration 75/1000 | Loss: 0.00002439
Iteration 76/1000 | Loss: 0.00002439
Iteration 77/1000 | Loss: 0.00002439
Iteration 78/1000 | Loss: 0.00002438
Iteration 79/1000 | Loss: 0.00002438
Iteration 80/1000 | Loss: 0.00002438
Iteration 81/1000 | Loss: 0.00002438
Iteration 82/1000 | Loss: 0.00002438
Iteration 83/1000 | Loss: 0.00002438
Iteration 84/1000 | Loss: 0.00002437
Iteration 85/1000 | Loss: 0.00002437
Iteration 86/1000 | Loss: 0.00002437
Iteration 87/1000 | Loss: 0.00002437
Iteration 88/1000 | Loss: 0.00002437
Iteration 89/1000 | Loss: 0.00002437
Iteration 90/1000 | Loss: 0.00002436
Iteration 91/1000 | Loss: 0.00002436
Iteration 92/1000 | Loss: 0.00002436
Iteration 93/1000 | Loss: 0.00002436
Iteration 94/1000 | Loss: 0.00002436
Iteration 95/1000 | Loss: 0.00002436
Iteration 96/1000 | Loss: 0.00002436
Iteration 97/1000 | Loss: 0.00002436
Iteration 98/1000 | Loss: 0.00002436
Iteration 99/1000 | Loss: 0.00002436
Iteration 100/1000 | Loss: 0.00002436
Iteration 101/1000 | Loss: 0.00002436
Iteration 102/1000 | Loss: 0.00002436
Iteration 103/1000 | Loss: 0.00002436
Iteration 104/1000 | Loss: 0.00002436
Iteration 105/1000 | Loss: 0.00002436
Iteration 106/1000 | Loss: 0.00002436
Iteration 107/1000 | Loss: 0.00002436
Iteration 108/1000 | Loss: 0.00002435
Iteration 109/1000 | Loss: 0.00002435
Iteration 110/1000 | Loss: 0.00002435
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002435
Iteration 113/1000 | Loss: 0.00002435
Iteration 114/1000 | Loss: 0.00002434
Iteration 115/1000 | Loss: 0.00002434
Iteration 116/1000 | Loss: 0.00002434
Iteration 117/1000 | Loss: 0.00002434
Iteration 118/1000 | Loss: 0.00002434
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002434
Iteration 121/1000 | Loss: 0.00002434
Iteration 122/1000 | Loss: 0.00002434
Iteration 123/1000 | Loss: 0.00002434
Iteration 124/1000 | Loss: 0.00002434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.4343175027752295e-05, 2.4343175027752295e-05, 2.4343175027752295e-05, 2.4343175027752295e-05, 2.4343175027752295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4343175027752295e-05

Optimization complete. Final v2v error: 4.1475396156311035 mm

Highest mean error: 4.342520236968994 mm for frame 74

Lowest mean error: 3.8372669219970703 mm for frame 211

Saving results

Total time: 41.133665323257446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928590
Iteration 2/25 | Loss: 0.00210852
Iteration 3/25 | Loss: 0.00163198
Iteration 4/25 | Loss: 0.00153724
Iteration 5/25 | Loss: 0.00151574
Iteration 6/25 | Loss: 0.00150447
Iteration 7/25 | Loss: 0.00147302
Iteration 8/25 | Loss: 0.00145379
Iteration 9/25 | Loss: 0.00144156
Iteration 10/25 | Loss: 0.00142700
Iteration 11/25 | Loss: 0.00141425
Iteration 12/25 | Loss: 0.00141214
Iteration 13/25 | Loss: 0.00141173
Iteration 14/25 | Loss: 0.00141119
Iteration 15/25 | Loss: 0.00141328
Iteration 16/25 | Loss: 0.00141100
Iteration 17/25 | Loss: 0.00140942
Iteration 18/25 | Loss: 0.00140882
Iteration 19/25 | Loss: 0.00140845
Iteration 20/25 | Loss: 0.00140810
Iteration 21/25 | Loss: 0.00140755
Iteration 22/25 | Loss: 0.00140866
Iteration 23/25 | Loss: 0.00140786
Iteration 24/25 | Loss: 0.00140773
Iteration 25/25 | Loss: 0.00140813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43890059
Iteration 2/25 | Loss: 0.00101824
Iteration 3/25 | Loss: 0.00101823
Iteration 4/25 | Loss: 0.00101717
Iteration 5/25 | Loss: 0.00101717
Iteration 6/25 | Loss: 0.00101716
Iteration 7/25 | Loss: 0.00101716
Iteration 8/25 | Loss: 0.00101716
Iteration 9/25 | Loss: 0.00101716
Iteration 10/25 | Loss: 0.00101716
Iteration 11/25 | Loss: 0.00101716
Iteration 12/25 | Loss: 0.00101716
Iteration 13/25 | Loss: 0.00101716
Iteration 14/25 | Loss: 0.00101716
Iteration 15/25 | Loss: 0.00101716
Iteration 16/25 | Loss: 0.00101716
Iteration 17/25 | Loss: 0.00101716
Iteration 18/25 | Loss: 0.00101716
Iteration 19/25 | Loss: 0.00101716
Iteration 20/25 | Loss: 0.00101716
Iteration 21/25 | Loss: 0.00101716
Iteration 22/25 | Loss: 0.00101716
Iteration 23/25 | Loss: 0.00101716
Iteration 24/25 | Loss: 0.00101716
Iteration 25/25 | Loss: 0.00101716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101716
Iteration 2/1000 | Loss: 0.00009033
Iteration 3/1000 | Loss: 0.00006206
Iteration 4/1000 | Loss: 0.00020559
Iteration 5/1000 | Loss: 0.00020069
Iteration 6/1000 | Loss: 0.00005725
Iteration 7/1000 | Loss: 0.00019500
Iteration 8/1000 | Loss: 0.00075774
Iteration 9/1000 | Loss: 0.00075293
Iteration 10/1000 | Loss: 0.00032279
Iteration 11/1000 | Loss: 0.00017161
Iteration 12/1000 | Loss: 0.00037112
Iteration 13/1000 | Loss: 0.00043034
Iteration 14/1000 | Loss: 0.00051788
Iteration 15/1000 | Loss: 0.00117917
Iteration 16/1000 | Loss: 0.00114515
Iteration 17/1000 | Loss: 0.00074041
Iteration 18/1000 | Loss: 0.00089597
Iteration 19/1000 | Loss: 0.00106305
Iteration 20/1000 | Loss: 0.00101248
Iteration 21/1000 | Loss: 0.00050176
Iteration 22/1000 | Loss: 0.00050665
Iteration 23/1000 | Loss: 0.00057503
Iteration 24/1000 | Loss: 0.00035673
Iteration 25/1000 | Loss: 0.00048368
Iteration 26/1000 | Loss: 0.00053387
Iteration 27/1000 | Loss: 0.00028884
Iteration 28/1000 | Loss: 0.00034016
Iteration 29/1000 | Loss: 0.00059973
Iteration 30/1000 | Loss: 0.00035779
Iteration 31/1000 | Loss: 0.00054252
Iteration 32/1000 | Loss: 0.00033387
Iteration 33/1000 | Loss: 0.00061999
Iteration 34/1000 | Loss: 0.00009341
Iteration 35/1000 | Loss: 0.00084330
Iteration 36/1000 | Loss: 0.00047260
Iteration 37/1000 | Loss: 0.00063103
Iteration 38/1000 | Loss: 0.00050956
Iteration 39/1000 | Loss: 0.00045835
Iteration 40/1000 | Loss: 0.00058934
Iteration 41/1000 | Loss: 0.00043238
Iteration 42/1000 | Loss: 0.00028468
Iteration 43/1000 | Loss: 0.00038138
Iteration 44/1000 | Loss: 0.00019917
Iteration 45/1000 | Loss: 0.00024700
Iteration 46/1000 | Loss: 0.00034387
Iteration 47/1000 | Loss: 0.00007447
Iteration 48/1000 | Loss: 0.00030091
Iteration 49/1000 | Loss: 0.00007361
Iteration 50/1000 | Loss: 0.00004969
Iteration 51/1000 | Loss: 0.00030348
Iteration 52/1000 | Loss: 0.00005521
Iteration 53/1000 | Loss: 0.00005709
Iteration 54/1000 | Loss: 0.00021959
Iteration 55/1000 | Loss: 0.00028627
Iteration 56/1000 | Loss: 0.00019445
Iteration 57/1000 | Loss: 0.00035936
Iteration 58/1000 | Loss: 0.00029501
Iteration 59/1000 | Loss: 0.00086919
Iteration 60/1000 | Loss: 0.00057960
Iteration 61/1000 | Loss: 0.00109833
Iteration 62/1000 | Loss: 0.00056066
Iteration 63/1000 | Loss: 0.00056728
Iteration 64/1000 | Loss: 0.00067053
Iteration 65/1000 | Loss: 0.00034742
Iteration 66/1000 | Loss: 0.00080188
Iteration 67/1000 | Loss: 0.00068411
Iteration 68/1000 | Loss: 0.00015233
Iteration 69/1000 | Loss: 0.00005730
Iteration 70/1000 | Loss: 0.00043090
Iteration 71/1000 | Loss: 0.00034943
Iteration 72/1000 | Loss: 0.00086113
Iteration 73/1000 | Loss: 0.00014719
Iteration 74/1000 | Loss: 0.00018593
Iteration 75/1000 | Loss: 0.00003893
Iteration 76/1000 | Loss: 0.00003576
Iteration 77/1000 | Loss: 0.00023269
Iteration 78/1000 | Loss: 0.00019426
Iteration 79/1000 | Loss: 0.00025545
Iteration 80/1000 | Loss: 0.00014422
Iteration 81/1000 | Loss: 0.00013549
Iteration 82/1000 | Loss: 0.00020884
Iteration 83/1000 | Loss: 0.00021842
Iteration 84/1000 | Loss: 0.00003266
Iteration 85/1000 | Loss: 0.00003948
Iteration 86/1000 | Loss: 0.00017652
Iteration 87/1000 | Loss: 0.00071203
Iteration 88/1000 | Loss: 0.00034382
Iteration 89/1000 | Loss: 0.00015533
Iteration 90/1000 | Loss: 0.00003808
Iteration 91/1000 | Loss: 0.00037599
Iteration 92/1000 | Loss: 0.00013151
Iteration 93/1000 | Loss: 0.00012988
Iteration 94/1000 | Loss: 0.00016853
Iteration 95/1000 | Loss: 0.00004631
Iteration 96/1000 | Loss: 0.00003948
Iteration 97/1000 | Loss: 0.00003437
Iteration 98/1000 | Loss: 0.00018055
Iteration 99/1000 | Loss: 0.00043681
Iteration 100/1000 | Loss: 0.00068892
Iteration 101/1000 | Loss: 0.00049328
Iteration 102/1000 | Loss: 0.00008310
Iteration 103/1000 | Loss: 0.00045957
Iteration 104/1000 | Loss: 0.00028930
Iteration 105/1000 | Loss: 0.00025790
Iteration 106/1000 | Loss: 0.00006724
Iteration 107/1000 | Loss: 0.00006305
Iteration 108/1000 | Loss: 0.00005625
Iteration 109/1000 | Loss: 0.00004487
Iteration 110/1000 | Loss: 0.00004035
Iteration 111/1000 | Loss: 0.00026773
Iteration 112/1000 | Loss: 0.00033384
Iteration 113/1000 | Loss: 0.00038255
Iteration 114/1000 | Loss: 0.00038257
Iteration 115/1000 | Loss: 0.00060279
Iteration 116/1000 | Loss: 0.00046969
Iteration 117/1000 | Loss: 0.00013611
Iteration 118/1000 | Loss: 0.00066459
Iteration 119/1000 | Loss: 0.00058199
Iteration 120/1000 | Loss: 0.00037838
Iteration 121/1000 | Loss: 0.00025788
Iteration 122/1000 | Loss: 0.00017096
Iteration 123/1000 | Loss: 0.00019874
Iteration 124/1000 | Loss: 0.00025171
Iteration 125/1000 | Loss: 0.00019773
Iteration 126/1000 | Loss: 0.00022352
Iteration 127/1000 | Loss: 0.00016143
Iteration 128/1000 | Loss: 0.00016964
Iteration 129/1000 | Loss: 0.00025926
Iteration 130/1000 | Loss: 0.00015271
Iteration 131/1000 | Loss: 0.00020384
Iteration 132/1000 | Loss: 0.00014349
Iteration 133/1000 | Loss: 0.00018991
Iteration 134/1000 | Loss: 0.00023048
Iteration 135/1000 | Loss: 0.00017399
Iteration 136/1000 | Loss: 0.00004490
Iteration 137/1000 | Loss: 0.00010196
Iteration 138/1000 | Loss: 0.00012096
Iteration 139/1000 | Loss: 0.00004609
Iteration 140/1000 | Loss: 0.00009601
Iteration 141/1000 | Loss: 0.00003745
Iteration 142/1000 | Loss: 0.00003493
Iteration 143/1000 | Loss: 0.00005336
Iteration 144/1000 | Loss: 0.00032622
Iteration 145/1000 | Loss: 0.00003819
Iteration 146/1000 | Loss: 0.00003890
Iteration 147/1000 | Loss: 0.00003853
Iteration 148/1000 | Loss: 0.00004112
Iteration 149/1000 | Loss: 0.00026474
Iteration 150/1000 | Loss: 0.00020463
Iteration 151/1000 | Loss: 0.00067031
Iteration 152/1000 | Loss: 0.00027133
Iteration 153/1000 | Loss: 0.00015860
Iteration 154/1000 | Loss: 0.00019722
Iteration 155/1000 | Loss: 0.00006063
Iteration 156/1000 | Loss: 0.00019053
Iteration 157/1000 | Loss: 0.00021385
Iteration 158/1000 | Loss: 0.00004856
Iteration 159/1000 | Loss: 0.00005005
Iteration 160/1000 | Loss: 0.00003476
Iteration 161/1000 | Loss: 0.00014873
Iteration 162/1000 | Loss: 0.00003323
Iteration 163/1000 | Loss: 0.00034066
Iteration 164/1000 | Loss: 0.00005578
Iteration 165/1000 | Loss: 0.00027003
Iteration 166/1000 | Loss: 0.00021533
Iteration 167/1000 | Loss: 0.00025141
Iteration 168/1000 | Loss: 0.00010469
Iteration 169/1000 | Loss: 0.00006994
Iteration 170/1000 | Loss: 0.00020165
Iteration 171/1000 | Loss: 0.00015298
Iteration 172/1000 | Loss: 0.00018769
Iteration 173/1000 | Loss: 0.00017274
Iteration 174/1000 | Loss: 0.00017981
Iteration 175/1000 | Loss: 0.00004761
Iteration 176/1000 | Loss: 0.00003477
Iteration 177/1000 | Loss: 0.00042251
Iteration 178/1000 | Loss: 0.00024432
Iteration 179/1000 | Loss: 0.00024620
Iteration 180/1000 | Loss: 0.00016827
Iteration 181/1000 | Loss: 0.00023625
Iteration 182/1000 | Loss: 0.00022310
Iteration 183/1000 | Loss: 0.00016012
Iteration 184/1000 | Loss: 0.00023710
Iteration 185/1000 | Loss: 0.00034680
Iteration 186/1000 | Loss: 0.00003272
Iteration 187/1000 | Loss: 0.00019754
Iteration 188/1000 | Loss: 0.00015391
Iteration 189/1000 | Loss: 0.00008782
Iteration 190/1000 | Loss: 0.00004171
Iteration 191/1000 | Loss: 0.00005734
Iteration 192/1000 | Loss: 0.00012791
Iteration 193/1000 | Loss: 0.00004506
Iteration 194/1000 | Loss: 0.00003158
Iteration 195/1000 | Loss: 0.00003572
Iteration 196/1000 | Loss: 0.00003260
Iteration 197/1000 | Loss: 0.00003023
Iteration 198/1000 | Loss: 0.00004498
Iteration 199/1000 | Loss: 0.00004270
Iteration 200/1000 | Loss: 0.00006050
Iteration 201/1000 | Loss: 0.00003652
Iteration 202/1000 | Loss: 0.00003161
Iteration 203/1000 | Loss: 0.00002969
Iteration 204/1000 | Loss: 0.00004109
Iteration 205/1000 | Loss: 0.00002995
Iteration 206/1000 | Loss: 0.00034867
Iteration 207/1000 | Loss: 0.00002859
Iteration 208/1000 | Loss: 0.00002653
Iteration 209/1000 | Loss: 0.00002573
Iteration 210/1000 | Loss: 0.00002503
Iteration 211/1000 | Loss: 0.00002440
Iteration 212/1000 | Loss: 0.00002395
Iteration 213/1000 | Loss: 0.00002344
Iteration 214/1000 | Loss: 0.00002314
Iteration 215/1000 | Loss: 0.00002276
Iteration 216/1000 | Loss: 0.00002247
Iteration 217/1000 | Loss: 0.00002231
Iteration 218/1000 | Loss: 0.00002219
Iteration 219/1000 | Loss: 0.00002214
Iteration 220/1000 | Loss: 0.00002198
Iteration 221/1000 | Loss: 0.00002179
Iteration 222/1000 | Loss: 0.00002177
Iteration 223/1000 | Loss: 0.00002175
Iteration 224/1000 | Loss: 0.00002156
Iteration 225/1000 | Loss: 0.00024876
Iteration 226/1000 | Loss: 0.00012068
Iteration 227/1000 | Loss: 0.00021106
Iteration 228/1000 | Loss: 0.00018648
Iteration 229/1000 | Loss: 0.00002894
Iteration 230/1000 | Loss: 0.00018790
Iteration 231/1000 | Loss: 0.00018452
Iteration 232/1000 | Loss: 0.00038757
Iteration 233/1000 | Loss: 0.00018574
Iteration 234/1000 | Loss: 0.00003241
Iteration 235/1000 | Loss: 0.00002806
Iteration 236/1000 | Loss: 0.00002564
Iteration 237/1000 | Loss: 0.00002427
Iteration 238/1000 | Loss: 0.00003166
Iteration 239/1000 | Loss: 0.00015484
Iteration 240/1000 | Loss: 0.00024989
Iteration 241/1000 | Loss: 0.00015529
Iteration 242/1000 | Loss: 0.00013949
Iteration 243/1000 | Loss: 0.00014437
Iteration 244/1000 | Loss: 0.00009838
Iteration 245/1000 | Loss: 0.00014036
Iteration 246/1000 | Loss: 0.00007768
Iteration 247/1000 | Loss: 0.00003915
Iteration 248/1000 | Loss: 0.00002686
Iteration 249/1000 | Loss: 0.00017357
Iteration 250/1000 | Loss: 0.00005361
Iteration 251/1000 | Loss: 0.00003294
Iteration 252/1000 | Loss: 0.00002584
Iteration 253/1000 | Loss: 0.00002460
Iteration 254/1000 | Loss: 0.00002369
Iteration 255/1000 | Loss: 0.00002325
Iteration 256/1000 | Loss: 0.00019749
Iteration 257/1000 | Loss: 0.00033832
Iteration 258/1000 | Loss: 0.00002941
Iteration 259/1000 | Loss: 0.00015535
Iteration 260/1000 | Loss: 0.00023241
Iteration 261/1000 | Loss: 0.00051458
Iteration 262/1000 | Loss: 0.00018211
Iteration 263/1000 | Loss: 0.00027075
Iteration 264/1000 | Loss: 0.00029055
Iteration 265/1000 | Loss: 0.00033340
Iteration 266/1000 | Loss: 0.00069287
Iteration 267/1000 | Loss: 0.00011565
Iteration 268/1000 | Loss: 0.00003914
Iteration 269/1000 | Loss: 0.00053509
Iteration 270/1000 | Loss: 0.00004636
Iteration 271/1000 | Loss: 0.00002953
Iteration 272/1000 | Loss: 0.00004982
Iteration 273/1000 | Loss: 0.00004662
Iteration 274/1000 | Loss: 0.00004901
Iteration 275/1000 | Loss: 0.00004715
Iteration 276/1000 | Loss: 0.00002407
Iteration 277/1000 | Loss: 0.00002320
Iteration 278/1000 | Loss: 0.00002209
Iteration 279/1000 | Loss: 0.00002131
Iteration 280/1000 | Loss: 0.00002098
Iteration 281/1000 | Loss: 0.00002067
Iteration 282/1000 | Loss: 0.00002063
Iteration 283/1000 | Loss: 0.00002062
Iteration 284/1000 | Loss: 0.00002052
Iteration 285/1000 | Loss: 0.00002049
Iteration 286/1000 | Loss: 0.00002048
Iteration 287/1000 | Loss: 0.00002043
Iteration 288/1000 | Loss: 0.00002042
Iteration 289/1000 | Loss: 0.00002041
Iteration 290/1000 | Loss: 0.00002040
Iteration 291/1000 | Loss: 0.00002039
Iteration 292/1000 | Loss: 0.00002038
Iteration 293/1000 | Loss: 0.00002038
Iteration 294/1000 | Loss: 0.00002037
Iteration 295/1000 | Loss: 0.00002034
Iteration 296/1000 | Loss: 0.00002032
Iteration 297/1000 | Loss: 0.00002032
Iteration 298/1000 | Loss: 0.00002031
Iteration 299/1000 | Loss: 0.00002031
Iteration 300/1000 | Loss: 0.00002030
Iteration 301/1000 | Loss: 0.00002029
Iteration 302/1000 | Loss: 0.00002029
Iteration 303/1000 | Loss: 0.00002029
Iteration 304/1000 | Loss: 0.00002029
Iteration 305/1000 | Loss: 0.00002028
Iteration 306/1000 | Loss: 0.00002028
Iteration 307/1000 | Loss: 0.00002028
Iteration 308/1000 | Loss: 0.00002027
Iteration 309/1000 | Loss: 0.00002027
Iteration 310/1000 | Loss: 0.00002027
Iteration 311/1000 | Loss: 0.00002027
Iteration 312/1000 | Loss: 0.00002027
Iteration 313/1000 | Loss: 0.00002027
Iteration 314/1000 | Loss: 0.00002026
Iteration 315/1000 | Loss: 0.00002026
Iteration 316/1000 | Loss: 0.00002026
Iteration 317/1000 | Loss: 0.00002026
Iteration 318/1000 | Loss: 0.00002026
Iteration 319/1000 | Loss: 0.00002025
Iteration 320/1000 | Loss: 0.00002025
Iteration 321/1000 | Loss: 0.00002025
Iteration 322/1000 | Loss: 0.00002024
Iteration 323/1000 | Loss: 0.00002024
Iteration 324/1000 | Loss: 0.00002024
Iteration 325/1000 | Loss: 0.00002024
Iteration 326/1000 | Loss: 0.00002024
Iteration 327/1000 | Loss: 0.00002023
Iteration 328/1000 | Loss: 0.00002023
Iteration 329/1000 | Loss: 0.00002022
Iteration 330/1000 | Loss: 0.00002022
Iteration 331/1000 | Loss: 0.00002022
Iteration 332/1000 | Loss: 0.00002021
Iteration 333/1000 | Loss: 0.00002021
Iteration 334/1000 | Loss: 0.00002021
Iteration 335/1000 | Loss: 0.00002021
Iteration 336/1000 | Loss: 0.00002021
Iteration 337/1000 | Loss: 0.00002020
Iteration 338/1000 | Loss: 0.00002020
Iteration 339/1000 | Loss: 0.00002019
Iteration 340/1000 | Loss: 0.00002019
Iteration 341/1000 | Loss: 0.00002019
Iteration 342/1000 | Loss: 0.00002019
Iteration 343/1000 | Loss: 0.00002019
Iteration 344/1000 | Loss: 0.00002018
Iteration 345/1000 | Loss: 0.00002018
Iteration 346/1000 | Loss: 0.00002018
Iteration 347/1000 | Loss: 0.00002018
Iteration 348/1000 | Loss: 0.00002018
Iteration 349/1000 | Loss: 0.00002018
Iteration 350/1000 | Loss: 0.00002017
Iteration 351/1000 | Loss: 0.00002017
Iteration 352/1000 | Loss: 0.00002017
Iteration 353/1000 | Loss: 0.00002017
Iteration 354/1000 | Loss: 0.00002017
Iteration 355/1000 | Loss: 0.00002016
Iteration 356/1000 | Loss: 0.00002016
Iteration 357/1000 | Loss: 0.00002016
Iteration 358/1000 | Loss: 0.00002016
Iteration 359/1000 | Loss: 0.00002016
Iteration 360/1000 | Loss: 0.00002015
Iteration 361/1000 | Loss: 0.00002015
Iteration 362/1000 | Loss: 0.00002015
Iteration 363/1000 | Loss: 0.00002015
Iteration 364/1000 | Loss: 0.00002015
Iteration 365/1000 | Loss: 0.00002015
Iteration 366/1000 | Loss: 0.00002015
Iteration 367/1000 | Loss: 0.00002015
Iteration 368/1000 | Loss: 0.00002015
Iteration 369/1000 | Loss: 0.00002014
Iteration 370/1000 | Loss: 0.00002014
Iteration 371/1000 | Loss: 0.00002014
Iteration 372/1000 | Loss: 0.00002014
Iteration 373/1000 | Loss: 0.00002014
Iteration 374/1000 | Loss: 0.00002014
Iteration 375/1000 | Loss: 0.00002014
Iteration 376/1000 | Loss: 0.00002014
Iteration 377/1000 | Loss: 0.00002014
Iteration 378/1000 | Loss: 0.00002014
Iteration 379/1000 | Loss: 0.00002014
Iteration 380/1000 | Loss: 0.00002014
Iteration 381/1000 | Loss: 0.00002014
Iteration 382/1000 | Loss: 0.00002014
Iteration 383/1000 | Loss: 0.00002014
Iteration 384/1000 | Loss: 0.00002013
Iteration 385/1000 | Loss: 0.00002013
Iteration 386/1000 | Loss: 0.00002013
Iteration 387/1000 | Loss: 0.00002013
Iteration 388/1000 | Loss: 0.00002013
Iteration 389/1000 | Loss: 0.00002013
Iteration 390/1000 | Loss: 0.00002013
Iteration 391/1000 | Loss: 0.00002013
Iteration 392/1000 | Loss: 0.00002013
Iteration 393/1000 | Loss: 0.00002012
Iteration 394/1000 | Loss: 0.00002012
Iteration 395/1000 | Loss: 0.00002012
Iteration 396/1000 | Loss: 0.00002012
Iteration 397/1000 | Loss: 0.00002012
Iteration 398/1000 | Loss: 0.00002012
Iteration 399/1000 | Loss: 0.00002012
Iteration 400/1000 | Loss: 0.00002012
Iteration 401/1000 | Loss: 0.00002012
Iteration 402/1000 | Loss: 0.00002012
Iteration 403/1000 | Loss: 0.00002011
Iteration 404/1000 | Loss: 0.00002011
Iteration 405/1000 | Loss: 0.00002011
Iteration 406/1000 | Loss: 0.00002011
Iteration 407/1000 | Loss: 0.00002011
Iteration 408/1000 | Loss: 0.00002011
Iteration 409/1000 | Loss: 0.00002011
Iteration 410/1000 | Loss: 0.00002011
Iteration 411/1000 | Loss: 0.00002011
Iteration 412/1000 | Loss: 0.00002011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 412. Stopping optimization.
Last 5 losses: [2.011452306760475e-05, 2.011452306760475e-05, 2.011452306760475e-05, 2.011452306760475e-05, 2.011452306760475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.011452306760475e-05

Optimization complete. Final v2v error: 3.6035878658294678 mm

Highest mean error: 5.49407958984375 mm for frame 121

Lowest mean error: 3.0611305236816406 mm for frame 39

Saving results

Total time: 512.4818487167358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560569
Iteration 2/25 | Loss: 0.00172569
Iteration 3/25 | Loss: 0.00147876
Iteration 4/25 | Loss: 0.00146117
Iteration 5/25 | Loss: 0.00145839
Iteration 6/25 | Loss: 0.00145786
Iteration 7/25 | Loss: 0.00145786
Iteration 8/25 | Loss: 0.00145786
Iteration 9/25 | Loss: 0.00145786
Iteration 10/25 | Loss: 0.00145786
Iteration 11/25 | Loss: 0.00145786
Iteration 12/25 | Loss: 0.00145786
Iteration 13/25 | Loss: 0.00145786
Iteration 14/25 | Loss: 0.00145786
Iteration 15/25 | Loss: 0.00145786
Iteration 16/25 | Loss: 0.00145786
Iteration 17/25 | Loss: 0.00145786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014578575501218438, 0.0014578575501218438, 0.0014578575501218438, 0.0014578575501218438, 0.0014578575501218438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014578575501218438

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00461280
Iteration 2/25 | Loss: 0.00112021
Iteration 3/25 | Loss: 0.00112019
Iteration 4/25 | Loss: 0.00112019
Iteration 5/25 | Loss: 0.00112019
Iteration 6/25 | Loss: 0.00112019
Iteration 7/25 | Loss: 0.00112019
Iteration 8/25 | Loss: 0.00112019
Iteration 9/25 | Loss: 0.00112019
Iteration 10/25 | Loss: 0.00112019
Iteration 11/25 | Loss: 0.00112019
Iteration 12/25 | Loss: 0.00112019
Iteration 13/25 | Loss: 0.00112019
Iteration 14/25 | Loss: 0.00112019
Iteration 15/25 | Loss: 0.00112019
Iteration 16/25 | Loss: 0.00112019
Iteration 17/25 | Loss: 0.00112019
Iteration 18/25 | Loss: 0.00112019
Iteration 19/25 | Loss: 0.00112019
Iteration 20/25 | Loss: 0.00112019
Iteration 21/25 | Loss: 0.00112019
Iteration 22/25 | Loss: 0.00112019
Iteration 23/25 | Loss: 0.00112019
Iteration 24/25 | Loss: 0.00112019
Iteration 25/25 | Loss: 0.00112019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112019
Iteration 2/1000 | Loss: 0.00005812
Iteration 3/1000 | Loss: 0.00003904
Iteration 4/1000 | Loss: 0.00003057
Iteration 5/1000 | Loss: 0.00002846
Iteration 6/1000 | Loss: 0.00002736
Iteration 7/1000 | Loss: 0.00002668
Iteration 8/1000 | Loss: 0.00002601
Iteration 9/1000 | Loss: 0.00002553
Iteration 10/1000 | Loss: 0.00002515
Iteration 11/1000 | Loss: 0.00002482
Iteration 12/1000 | Loss: 0.00002453
Iteration 13/1000 | Loss: 0.00002428
Iteration 14/1000 | Loss: 0.00002404
Iteration 15/1000 | Loss: 0.00002383
Iteration 16/1000 | Loss: 0.00002377
Iteration 17/1000 | Loss: 0.00002372
Iteration 18/1000 | Loss: 0.00002371
Iteration 19/1000 | Loss: 0.00002370
Iteration 20/1000 | Loss: 0.00002359
Iteration 21/1000 | Loss: 0.00002343
Iteration 22/1000 | Loss: 0.00002337
Iteration 23/1000 | Loss: 0.00002333
Iteration 24/1000 | Loss: 0.00002332
Iteration 25/1000 | Loss: 0.00002330
Iteration 26/1000 | Loss: 0.00002328
Iteration 27/1000 | Loss: 0.00002328
Iteration 28/1000 | Loss: 0.00002325
Iteration 29/1000 | Loss: 0.00002325
Iteration 30/1000 | Loss: 0.00002319
Iteration 31/1000 | Loss: 0.00002319
Iteration 32/1000 | Loss: 0.00002319
Iteration 33/1000 | Loss: 0.00002316
Iteration 34/1000 | Loss: 0.00002316
Iteration 35/1000 | Loss: 0.00002315
Iteration 36/1000 | Loss: 0.00002314
Iteration 37/1000 | Loss: 0.00002314
Iteration 38/1000 | Loss: 0.00002314
Iteration 39/1000 | Loss: 0.00002314
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00002314
Iteration 42/1000 | Loss: 0.00002314
Iteration 43/1000 | Loss: 0.00002314
Iteration 44/1000 | Loss: 0.00002314
Iteration 45/1000 | Loss: 0.00002314
Iteration 46/1000 | Loss: 0.00002314
Iteration 47/1000 | Loss: 0.00002314
Iteration 48/1000 | Loss: 0.00002313
Iteration 49/1000 | Loss: 0.00002310
Iteration 50/1000 | Loss: 0.00002310
Iteration 51/1000 | Loss: 0.00002310
Iteration 52/1000 | Loss: 0.00002310
Iteration 53/1000 | Loss: 0.00002310
Iteration 54/1000 | Loss: 0.00002310
Iteration 55/1000 | Loss: 0.00002309
Iteration 56/1000 | Loss: 0.00002309
Iteration 57/1000 | Loss: 0.00002309
Iteration 58/1000 | Loss: 0.00002308
Iteration 59/1000 | Loss: 0.00002307
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00002307
Iteration 62/1000 | Loss: 0.00002307
Iteration 63/1000 | Loss: 0.00002306
Iteration 64/1000 | Loss: 0.00002306
Iteration 65/1000 | Loss: 0.00002306
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002305
Iteration 68/1000 | Loss: 0.00002305
Iteration 69/1000 | Loss: 0.00002305
Iteration 70/1000 | Loss: 0.00002305
Iteration 71/1000 | Loss: 0.00002305
Iteration 72/1000 | Loss: 0.00002305
Iteration 73/1000 | Loss: 0.00002305
Iteration 74/1000 | Loss: 0.00002305
Iteration 75/1000 | Loss: 0.00002305
Iteration 76/1000 | Loss: 0.00002304
Iteration 77/1000 | Loss: 0.00002304
Iteration 78/1000 | Loss: 0.00002304
Iteration 79/1000 | Loss: 0.00002304
Iteration 80/1000 | Loss: 0.00002304
Iteration 81/1000 | Loss: 0.00002303
Iteration 82/1000 | Loss: 0.00002303
Iteration 83/1000 | Loss: 0.00002303
Iteration 84/1000 | Loss: 0.00002303
Iteration 85/1000 | Loss: 0.00002303
Iteration 86/1000 | Loss: 0.00002303
Iteration 87/1000 | Loss: 0.00002303
Iteration 88/1000 | Loss: 0.00002303
Iteration 89/1000 | Loss: 0.00002303
Iteration 90/1000 | Loss: 0.00002303
Iteration 91/1000 | Loss: 0.00002302
Iteration 92/1000 | Loss: 0.00002302
Iteration 93/1000 | Loss: 0.00002302
Iteration 94/1000 | Loss: 0.00002302
Iteration 95/1000 | Loss: 0.00002302
Iteration 96/1000 | Loss: 0.00002302
Iteration 97/1000 | Loss: 0.00002301
Iteration 98/1000 | Loss: 0.00002301
Iteration 99/1000 | Loss: 0.00002301
Iteration 100/1000 | Loss: 0.00002301
Iteration 101/1000 | Loss: 0.00002301
Iteration 102/1000 | Loss: 0.00002301
Iteration 103/1000 | Loss: 0.00002300
Iteration 104/1000 | Loss: 0.00002300
Iteration 105/1000 | Loss: 0.00002300
Iteration 106/1000 | Loss: 0.00002300
Iteration 107/1000 | Loss: 0.00002300
Iteration 108/1000 | Loss: 0.00002300
Iteration 109/1000 | Loss: 0.00002299
Iteration 110/1000 | Loss: 0.00002299
Iteration 111/1000 | Loss: 0.00002299
Iteration 112/1000 | Loss: 0.00002299
Iteration 113/1000 | Loss: 0.00002299
Iteration 114/1000 | Loss: 0.00002299
Iteration 115/1000 | Loss: 0.00002298
Iteration 116/1000 | Loss: 0.00002298
Iteration 117/1000 | Loss: 0.00002298
Iteration 118/1000 | Loss: 0.00002298
Iteration 119/1000 | Loss: 0.00002298
Iteration 120/1000 | Loss: 0.00002298
Iteration 121/1000 | Loss: 0.00002298
Iteration 122/1000 | Loss: 0.00002297
Iteration 123/1000 | Loss: 0.00002297
Iteration 124/1000 | Loss: 0.00002297
Iteration 125/1000 | Loss: 0.00002297
Iteration 126/1000 | Loss: 0.00002297
Iteration 127/1000 | Loss: 0.00002297
Iteration 128/1000 | Loss: 0.00002297
Iteration 129/1000 | Loss: 0.00002297
Iteration 130/1000 | Loss: 0.00002297
Iteration 131/1000 | Loss: 0.00002297
Iteration 132/1000 | Loss: 0.00002297
Iteration 133/1000 | Loss: 0.00002297
Iteration 134/1000 | Loss: 0.00002297
Iteration 135/1000 | Loss: 0.00002296
Iteration 136/1000 | Loss: 0.00002296
Iteration 137/1000 | Loss: 0.00002296
Iteration 138/1000 | Loss: 0.00002296
Iteration 139/1000 | Loss: 0.00002296
Iteration 140/1000 | Loss: 0.00002296
Iteration 141/1000 | Loss: 0.00002296
Iteration 142/1000 | Loss: 0.00002295
Iteration 143/1000 | Loss: 0.00002295
Iteration 144/1000 | Loss: 0.00002295
Iteration 145/1000 | Loss: 0.00002295
Iteration 146/1000 | Loss: 0.00002295
Iteration 147/1000 | Loss: 0.00002295
Iteration 148/1000 | Loss: 0.00002295
Iteration 149/1000 | Loss: 0.00002295
Iteration 150/1000 | Loss: 0.00002295
Iteration 151/1000 | Loss: 0.00002295
Iteration 152/1000 | Loss: 0.00002295
Iteration 153/1000 | Loss: 0.00002295
Iteration 154/1000 | Loss: 0.00002295
Iteration 155/1000 | Loss: 0.00002295
Iteration 156/1000 | Loss: 0.00002295
Iteration 157/1000 | Loss: 0.00002295
Iteration 158/1000 | Loss: 0.00002295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.2953228835831396e-05, 2.2953228835831396e-05, 2.2953228835831396e-05, 2.2953228835831396e-05, 2.2953228835831396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2953228835831396e-05

Optimization complete. Final v2v error: 3.8862149715423584 mm

Highest mean error: 4.847325801849365 mm for frame 58

Lowest mean error: 3.1131715774536133 mm for frame 137

Saving results

Total time: 46.648833990097046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364762
Iteration 2/25 | Loss: 0.00137101
Iteration 3/25 | Loss: 0.00128042
Iteration 4/25 | Loss: 0.00127072
Iteration 5/25 | Loss: 0.00126754
Iteration 6/25 | Loss: 0.00126623
Iteration 7/25 | Loss: 0.00126623
Iteration 8/25 | Loss: 0.00126623
Iteration 9/25 | Loss: 0.00126623
Iteration 10/25 | Loss: 0.00126623
Iteration 11/25 | Loss: 0.00126623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012662315275520086, 0.0012662315275520086, 0.0012662315275520086, 0.0012662315275520086, 0.0012662315275520086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012662315275520086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35526609
Iteration 2/25 | Loss: 0.00082038
Iteration 3/25 | Loss: 0.00082038
Iteration 4/25 | Loss: 0.00082038
Iteration 5/25 | Loss: 0.00082038
Iteration 6/25 | Loss: 0.00082038
Iteration 7/25 | Loss: 0.00082038
Iteration 8/25 | Loss: 0.00082038
Iteration 9/25 | Loss: 0.00082038
Iteration 10/25 | Loss: 0.00082038
Iteration 11/25 | Loss: 0.00082038
Iteration 12/25 | Loss: 0.00082038
Iteration 13/25 | Loss: 0.00082038
Iteration 14/25 | Loss: 0.00082038
Iteration 15/25 | Loss: 0.00082038
Iteration 16/25 | Loss: 0.00082038
Iteration 17/25 | Loss: 0.00082038
Iteration 18/25 | Loss: 0.00082038
Iteration 19/25 | Loss: 0.00082038
Iteration 20/25 | Loss: 0.00082038
Iteration 21/25 | Loss: 0.00082038
Iteration 22/25 | Loss: 0.00082038
Iteration 23/25 | Loss: 0.00082038
Iteration 24/25 | Loss: 0.00082038
Iteration 25/25 | Loss: 0.00082038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082038
Iteration 2/1000 | Loss: 0.00003614
Iteration 3/1000 | Loss: 0.00002556
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00002027
Iteration 6/1000 | Loss: 0.00001911
Iteration 7/1000 | Loss: 0.00001848
Iteration 8/1000 | Loss: 0.00001795
Iteration 9/1000 | Loss: 0.00001762
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001713
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001686
Iteration 14/1000 | Loss: 0.00001682
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001673
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001664
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001654
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001647
Iteration 27/1000 | Loss: 0.00001647
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001635
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001633
Iteration 34/1000 | Loss: 0.00001632
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001631
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001630
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001629
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001622
Iteration 53/1000 | Loss: 0.00001622
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001620
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001619
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001616
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001613
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001611
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001610
Iteration 86/1000 | Loss: 0.00001610
Iteration 87/1000 | Loss: 0.00001609
Iteration 88/1000 | Loss: 0.00001609
Iteration 89/1000 | Loss: 0.00001609
Iteration 90/1000 | Loss: 0.00001608
Iteration 91/1000 | Loss: 0.00001608
Iteration 92/1000 | Loss: 0.00001608
Iteration 93/1000 | Loss: 0.00001608
Iteration 94/1000 | Loss: 0.00001607
Iteration 95/1000 | Loss: 0.00001607
Iteration 96/1000 | Loss: 0.00001607
Iteration 97/1000 | Loss: 0.00001607
Iteration 98/1000 | Loss: 0.00001607
Iteration 99/1000 | Loss: 0.00001607
Iteration 100/1000 | Loss: 0.00001606
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001606
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001606
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001606
Iteration 109/1000 | Loss: 0.00001606
Iteration 110/1000 | Loss: 0.00001606
Iteration 111/1000 | Loss: 0.00001606
Iteration 112/1000 | Loss: 0.00001606
Iteration 113/1000 | Loss: 0.00001606
Iteration 114/1000 | Loss: 0.00001606
Iteration 115/1000 | Loss: 0.00001606
Iteration 116/1000 | Loss: 0.00001606
Iteration 117/1000 | Loss: 0.00001606
Iteration 118/1000 | Loss: 0.00001606
Iteration 119/1000 | Loss: 0.00001606
Iteration 120/1000 | Loss: 0.00001606
Iteration 121/1000 | Loss: 0.00001606
Iteration 122/1000 | Loss: 0.00001606
Iteration 123/1000 | Loss: 0.00001606
Iteration 124/1000 | Loss: 0.00001606
Iteration 125/1000 | Loss: 0.00001606
Iteration 126/1000 | Loss: 0.00001606
Iteration 127/1000 | Loss: 0.00001606
Iteration 128/1000 | Loss: 0.00001606
Iteration 129/1000 | Loss: 0.00001606
Iteration 130/1000 | Loss: 0.00001606
Iteration 131/1000 | Loss: 0.00001606
Iteration 132/1000 | Loss: 0.00001606
Iteration 133/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.6056030290201306e-05, 1.6056030290201306e-05, 1.6056030290201306e-05, 1.6056030290201306e-05, 1.6056030290201306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6056030290201306e-05

Optimization complete. Final v2v error: 3.44374680519104 mm

Highest mean error: 3.865938186645508 mm for frame 71

Lowest mean error: 3.1331593990325928 mm for frame 156

Saving results

Total time: 37.810325622558594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583734
Iteration 2/25 | Loss: 0.00154200
Iteration 3/25 | Loss: 0.00138205
Iteration 4/25 | Loss: 0.00136227
Iteration 5/25 | Loss: 0.00135725
Iteration 6/25 | Loss: 0.00135561
Iteration 7/25 | Loss: 0.00135515
Iteration 8/25 | Loss: 0.00135493
Iteration 9/25 | Loss: 0.00135482
Iteration 10/25 | Loss: 0.00135473
Iteration 11/25 | Loss: 0.00135462
Iteration 12/25 | Loss: 0.00135449
Iteration 13/25 | Loss: 0.00135429
Iteration 14/25 | Loss: 0.00135375
Iteration 15/25 | Loss: 0.00135347
Iteration 16/25 | Loss: 0.00135336
Iteration 17/25 | Loss: 0.00135336
Iteration 18/25 | Loss: 0.00135335
Iteration 19/25 | Loss: 0.00135335
Iteration 20/25 | Loss: 0.00135335
Iteration 21/25 | Loss: 0.00135335
Iteration 22/25 | Loss: 0.00135335
Iteration 23/25 | Loss: 0.00135335
Iteration 24/25 | Loss: 0.00135335
Iteration 25/25 | Loss: 0.00135335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07756066
Iteration 2/25 | Loss: 0.00095984
Iteration 3/25 | Loss: 0.00095980
Iteration 4/25 | Loss: 0.00095980
Iteration 5/25 | Loss: 0.00095980
Iteration 6/25 | Loss: 0.00095980
Iteration 7/25 | Loss: 0.00095980
Iteration 8/25 | Loss: 0.00095980
Iteration 9/25 | Loss: 0.00095980
Iteration 10/25 | Loss: 0.00095980
Iteration 11/25 | Loss: 0.00095980
Iteration 12/25 | Loss: 0.00095980
Iteration 13/25 | Loss: 0.00095980
Iteration 14/25 | Loss: 0.00095980
Iteration 15/25 | Loss: 0.00095980
Iteration 16/25 | Loss: 0.00095980
Iteration 17/25 | Loss: 0.00095980
Iteration 18/25 | Loss: 0.00095980
Iteration 19/25 | Loss: 0.00095980
Iteration 20/25 | Loss: 0.00095980
Iteration 21/25 | Loss: 0.00095980
Iteration 22/25 | Loss: 0.00095980
Iteration 23/25 | Loss: 0.00095980
Iteration 24/25 | Loss: 0.00095980
Iteration 25/25 | Loss: 0.00095980

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095980
Iteration 2/1000 | Loss: 0.00004850
Iteration 3/1000 | Loss: 0.00002814
Iteration 4/1000 | Loss: 0.00002185
Iteration 5/1000 | Loss: 0.00002031
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001868
Iteration 8/1000 | Loss: 0.00001810
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001680
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001667
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001657
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001652
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001646
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001644
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001643
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001642
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001635
Iteration 60/1000 | Loss: 0.00001635
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001634
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001634
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001632
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001630
Iteration 80/1000 | Loss: 0.00001630
Iteration 81/1000 | Loss: 0.00001630
Iteration 82/1000 | Loss: 0.00001630
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001629
Iteration 85/1000 | Loss: 0.00001629
Iteration 86/1000 | Loss: 0.00001629
Iteration 87/1000 | Loss: 0.00001629
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001628
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001628
Iteration 93/1000 | Loss: 0.00001628
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001628
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001627
Iteration 99/1000 | Loss: 0.00001627
Iteration 100/1000 | Loss: 0.00001627
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001626
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001625
Iteration 120/1000 | Loss: 0.00001625
Iteration 121/1000 | Loss: 0.00001625
Iteration 122/1000 | Loss: 0.00001625
Iteration 123/1000 | Loss: 0.00001625
Iteration 124/1000 | Loss: 0.00001625
Iteration 125/1000 | Loss: 0.00001625
Iteration 126/1000 | Loss: 0.00001625
Iteration 127/1000 | Loss: 0.00001625
Iteration 128/1000 | Loss: 0.00001624
Iteration 129/1000 | Loss: 0.00001624
Iteration 130/1000 | Loss: 0.00001624
Iteration 131/1000 | Loss: 0.00001624
Iteration 132/1000 | Loss: 0.00001624
Iteration 133/1000 | Loss: 0.00001624
Iteration 134/1000 | Loss: 0.00001623
Iteration 135/1000 | Loss: 0.00001623
Iteration 136/1000 | Loss: 0.00001623
Iteration 137/1000 | Loss: 0.00001623
Iteration 138/1000 | Loss: 0.00001623
Iteration 139/1000 | Loss: 0.00001623
Iteration 140/1000 | Loss: 0.00001622
Iteration 141/1000 | Loss: 0.00001622
Iteration 142/1000 | Loss: 0.00001622
Iteration 143/1000 | Loss: 0.00001622
Iteration 144/1000 | Loss: 0.00001622
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001621
Iteration 148/1000 | Loss: 0.00001621
Iteration 149/1000 | Loss: 0.00001621
Iteration 150/1000 | Loss: 0.00001621
Iteration 151/1000 | Loss: 0.00001621
Iteration 152/1000 | Loss: 0.00001621
Iteration 153/1000 | Loss: 0.00001621
Iteration 154/1000 | Loss: 0.00001620
Iteration 155/1000 | Loss: 0.00001620
Iteration 156/1000 | Loss: 0.00001620
Iteration 157/1000 | Loss: 0.00001620
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001620
Iteration 160/1000 | Loss: 0.00001620
Iteration 161/1000 | Loss: 0.00001620
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001619
Iteration 167/1000 | Loss: 0.00001619
Iteration 168/1000 | Loss: 0.00001619
Iteration 169/1000 | Loss: 0.00001619
Iteration 170/1000 | Loss: 0.00001619
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001618
Iteration 174/1000 | Loss: 0.00001618
Iteration 175/1000 | Loss: 0.00001618
Iteration 176/1000 | Loss: 0.00001618
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001617
Iteration 180/1000 | Loss: 0.00001617
Iteration 181/1000 | Loss: 0.00001617
Iteration 182/1000 | Loss: 0.00001617
Iteration 183/1000 | Loss: 0.00001617
Iteration 184/1000 | Loss: 0.00001617
Iteration 185/1000 | Loss: 0.00001617
Iteration 186/1000 | Loss: 0.00001617
Iteration 187/1000 | Loss: 0.00001617
Iteration 188/1000 | Loss: 0.00001617
Iteration 189/1000 | Loss: 0.00001617
Iteration 190/1000 | Loss: 0.00001617
Iteration 191/1000 | Loss: 0.00001617
Iteration 192/1000 | Loss: 0.00001616
Iteration 193/1000 | Loss: 0.00001616
Iteration 194/1000 | Loss: 0.00001616
Iteration 195/1000 | Loss: 0.00001616
Iteration 196/1000 | Loss: 0.00001616
Iteration 197/1000 | Loss: 0.00001616
Iteration 198/1000 | Loss: 0.00001616
Iteration 199/1000 | Loss: 0.00001616
Iteration 200/1000 | Loss: 0.00001616
Iteration 201/1000 | Loss: 0.00001616
Iteration 202/1000 | Loss: 0.00001616
Iteration 203/1000 | Loss: 0.00001616
Iteration 204/1000 | Loss: 0.00001616
Iteration 205/1000 | Loss: 0.00001616
Iteration 206/1000 | Loss: 0.00001616
Iteration 207/1000 | Loss: 0.00001616
Iteration 208/1000 | Loss: 0.00001616
Iteration 209/1000 | Loss: 0.00001616
Iteration 210/1000 | Loss: 0.00001616
Iteration 211/1000 | Loss: 0.00001616
Iteration 212/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.6164784028660506e-05, 1.6164784028660506e-05, 1.6164784028660506e-05, 1.6164784028660506e-05, 1.6164784028660506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6164784028660506e-05

Optimization complete. Final v2v error: 3.37319016456604 mm

Highest mean error: 5.608103275299072 mm for frame 92

Lowest mean error: 3.0519676208496094 mm for frame 0

Saving results

Total time: 60.46300268173218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416095
Iteration 2/25 | Loss: 0.00139167
Iteration 3/25 | Loss: 0.00130754
Iteration 4/25 | Loss: 0.00129696
Iteration 5/25 | Loss: 0.00129361
Iteration 6/25 | Loss: 0.00129262
Iteration 7/25 | Loss: 0.00129262
Iteration 8/25 | Loss: 0.00129262
Iteration 9/25 | Loss: 0.00129262
Iteration 10/25 | Loss: 0.00129262
Iteration 11/25 | Loss: 0.00129262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012926229974254966, 0.0012926229974254966, 0.0012926229974254966, 0.0012926229974254966, 0.0012926229974254966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012926229974254966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37083042
Iteration 2/25 | Loss: 0.00094228
Iteration 3/25 | Loss: 0.00094226
Iteration 4/25 | Loss: 0.00094226
Iteration 5/25 | Loss: 0.00094226
Iteration 6/25 | Loss: 0.00094226
Iteration 7/25 | Loss: 0.00094226
Iteration 8/25 | Loss: 0.00094226
Iteration 9/25 | Loss: 0.00094226
Iteration 10/25 | Loss: 0.00094226
Iteration 11/25 | Loss: 0.00094226
Iteration 12/25 | Loss: 0.00094226
Iteration 13/25 | Loss: 0.00094226
Iteration 14/25 | Loss: 0.00094226
Iteration 15/25 | Loss: 0.00094226
Iteration 16/25 | Loss: 0.00094226
Iteration 17/25 | Loss: 0.00094226
Iteration 18/25 | Loss: 0.00094226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009422595612704754, 0.0009422595612704754, 0.0009422595612704754, 0.0009422595612704754, 0.0009422595612704754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009422595612704754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094226
Iteration 2/1000 | Loss: 0.00005470
Iteration 3/1000 | Loss: 0.00003414
Iteration 4/1000 | Loss: 0.00002553
Iteration 5/1000 | Loss: 0.00002266
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00001969
Iteration 8/1000 | Loss: 0.00001893
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001760
Iteration 12/1000 | Loss: 0.00001734
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001706
Iteration 16/1000 | Loss: 0.00001705
Iteration 17/1000 | Loss: 0.00001705
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001702
Iteration 20/1000 | Loss: 0.00001702
Iteration 21/1000 | Loss: 0.00001701
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001700
Iteration 26/1000 | Loss: 0.00001700
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001699
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001698
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001693
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001690
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001689
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001686
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001682
Iteration 53/1000 | Loss: 0.00001682
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001677
Iteration 59/1000 | Loss: 0.00001677
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001675
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001670
Iteration 73/1000 | Loss: 0.00001670
Iteration 74/1000 | Loss: 0.00001669
Iteration 75/1000 | Loss: 0.00001669
Iteration 76/1000 | Loss: 0.00001668
Iteration 77/1000 | Loss: 0.00001667
Iteration 78/1000 | Loss: 0.00001667
Iteration 79/1000 | Loss: 0.00001667
Iteration 80/1000 | Loss: 0.00001666
Iteration 81/1000 | Loss: 0.00001666
Iteration 82/1000 | Loss: 0.00001666
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001662
Iteration 86/1000 | Loss: 0.00001662
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001661
Iteration 90/1000 | Loss: 0.00001661
Iteration 91/1000 | Loss: 0.00001661
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001660
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001660
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001659
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001658
Iteration 107/1000 | Loss: 0.00001658
Iteration 108/1000 | Loss: 0.00001658
Iteration 109/1000 | Loss: 0.00001658
Iteration 110/1000 | Loss: 0.00001657
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001656
Iteration 113/1000 | Loss: 0.00001656
Iteration 114/1000 | Loss: 0.00001656
Iteration 115/1000 | Loss: 0.00001656
Iteration 116/1000 | Loss: 0.00001656
Iteration 117/1000 | Loss: 0.00001656
Iteration 118/1000 | Loss: 0.00001656
Iteration 119/1000 | Loss: 0.00001656
Iteration 120/1000 | Loss: 0.00001656
Iteration 121/1000 | Loss: 0.00001656
Iteration 122/1000 | Loss: 0.00001655
Iteration 123/1000 | Loss: 0.00001655
Iteration 124/1000 | Loss: 0.00001655
Iteration 125/1000 | Loss: 0.00001655
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.655477171880193e-05, 1.655477171880193e-05, 1.655477171880193e-05, 1.655477171880193e-05, 1.655477171880193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.655477171880193e-05

Optimization complete. Final v2v error: 3.3729798793792725 mm

Highest mean error: 5.2450079917907715 mm for frame 84

Lowest mean error: 2.8906357288360596 mm for frame 170

Saving results

Total time: 37.8822865486145
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570533
Iteration 2/25 | Loss: 0.00146309
Iteration 3/25 | Loss: 0.00136461
Iteration 4/25 | Loss: 0.00134596
Iteration 5/25 | Loss: 0.00133985
Iteration 6/25 | Loss: 0.00133853
Iteration 7/25 | Loss: 0.00133834
Iteration 8/25 | Loss: 0.00133834
Iteration 9/25 | Loss: 0.00133834
Iteration 10/25 | Loss: 0.00133834
Iteration 11/25 | Loss: 0.00133834
Iteration 12/25 | Loss: 0.00133834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013383374316617846, 0.0013383374316617846, 0.0013383374316617846, 0.0013383374316617846, 0.0013383374316617846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013383374316617846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.82668495
Iteration 2/25 | Loss: 0.00104075
Iteration 3/25 | Loss: 0.00104075
Iteration 4/25 | Loss: 0.00104075
Iteration 5/25 | Loss: 0.00104075
Iteration 6/25 | Loss: 0.00104075
Iteration 7/25 | Loss: 0.00104075
Iteration 8/25 | Loss: 0.00104075
Iteration 9/25 | Loss: 0.00104075
Iteration 10/25 | Loss: 0.00104075
Iteration 11/25 | Loss: 0.00104075
Iteration 12/25 | Loss: 0.00104075
Iteration 13/25 | Loss: 0.00104075
Iteration 14/25 | Loss: 0.00104075
Iteration 15/25 | Loss: 0.00104075
Iteration 16/25 | Loss: 0.00104075
Iteration 17/25 | Loss: 0.00104075
Iteration 18/25 | Loss: 0.00104075
Iteration 19/25 | Loss: 0.00104075
Iteration 20/25 | Loss: 0.00104075
Iteration 21/25 | Loss: 0.00104075
Iteration 22/25 | Loss: 0.00104075
Iteration 23/25 | Loss: 0.00104075
Iteration 24/25 | Loss: 0.00104075
Iteration 25/25 | Loss: 0.00104075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104075
Iteration 2/1000 | Loss: 0.00004547
Iteration 3/1000 | Loss: 0.00003137
Iteration 4/1000 | Loss: 0.00002539
Iteration 5/1000 | Loss: 0.00002409
Iteration 6/1000 | Loss: 0.00002308
Iteration 7/1000 | Loss: 0.00002255
Iteration 8/1000 | Loss: 0.00002204
Iteration 9/1000 | Loss: 0.00002168
Iteration 10/1000 | Loss: 0.00002143
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002101
Iteration 13/1000 | Loss: 0.00002086
Iteration 14/1000 | Loss: 0.00002070
Iteration 15/1000 | Loss: 0.00002068
Iteration 16/1000 | Loss: 0.00002067
Iteration 17/1000 | Loss: 0.00002066
Iteration 18/1000 | Loss: 0.00002066
Iteration 19/1000 | Loss: 0.00002065
Iteration 20/1000 | Loss: 0.00002064
Iteration 21/1000 | Loss: 0.00002059
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002057
Iteration 24/1000 | Loss: 0.00002057
Iteration 25/1000 | Loss: 0.00002057
Iteration 26/1000 | Loss: 0.00002057
Iteration 27/1000 | Loss: 0.00002056
Iteration 28/1000 | Loss: 0.00002056
Iteration 29/1000 | Loss: 0.00002056
Iteration 30/1000 | Loss: 0.00002055
Iteration 31/1000 | Loss: 0.00002055
Iteration 32/1000 | Loss: 0.00002055
Iteration 33/1000 | Loss: 0.00002054
Iteration 34/1000 | Loss: 0.00002054
Iteration 35/1000 | Loss: 0.00002054
Iteration 36/1000 | Loss: 0.00002054
Iteration 37/1000 | Loss: 0.00002053
Iteration 38/1000 | Loss: 0.00002052
Iteration 39/1000 | Loss: 0.00002052
Iteration 40/1000 | Loss: 0.00002052
Iteration 41/1000 | Loss: 0.00002051
Iteration 42/1000 | Loss: 0.00002051
Iteration 43/1000 | Loss: 0.00002050
Iteration 44/1000 | Loss: 0.00002050
Iteration 45/1000 | Loss: 0.00002050
Iteration 46/1000 | Loss: 0.00002049
Iteration 47/1000 | Loss: 0.00002049
Iteration 48/1000 | Loss: 0.00002049
Iteration 49/1000 | Loss: 0.00002048
Iteration 50/1000 | Loss: 0.00002048
Iteration 51/1000 | Loss: 0.00002048
Iteration 52/1000 | Loss: 0.00002047
Iteration 53/1000 | Loss: 0.00002047
Iteration 54/1000 | Loss: 0.00002046
Iteration 55/1000 | Loss: 0.00002046
Iteration 56/1000 | Loss: 0.00002045
Iteration 57/1000 | Loss: 0.00002045
Iteration 58/1000 | Loss: 0.00002045
Iteration 59/1000 | Loss: 0.00002044
Iteration 60/1000 | Loss: 0.00002043
Iteration 61/1000 | Loss: 0.00002042
Iteration 62/1000 | Loss: 0.00002042
Iteration 63/1000 | Loss: 0.00002041
Iteration 64/1000 | Loss: 0.00002041
Iteration 65/1000 | Loss: 0.00002041
Iteration 66/1000 | Loss: 0.00002040
Iteration 67/1000 | Loss: 0.00002040
Iteration 68/1000 | Loss: 0.00002039
Iteration 69/1000 | Loss: 0.00002039
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002038
Iteration 72/1000 | Loss: 0.00002038
Iteration 73/1000 | Loss: 0.00002038
Iteration 74/1000 | Loss: 0.00002037
Iteration 75/1000 | Loss: 0.00002037
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002036
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002034
Iteration 84/1000 | Loss: 0.00002033
Iteration 85/1000 | Loss: 0.00002033
Iteration 86/1000 | Loss: 0.00002033
Iteration 87/1000 | Loss: 0.00002033
Iteration 88/1000 | Loss: 0.00002032
Iteration 89/1000 | Loss: 0.00002032
Iteration 90/1000 | Loss: 0.00002032
Iteration 91/1000 | Loss: 0.00002032
Iteration 92/1000 | Loss: 0.00002032
Iteration 93/1000 | Loss: 0.00002032
Iteration 94/1000 | Loss: 0.00002032
Iteration 95/1000 | Loss: 0.00002032
Iteration 96/1000 | Loss: 0.00002031
Iteration 97/1000 | Loss: 0.00002031
Iteration 98/1000 | Loss: 0.00002031
Iteration 99/1000 | Loss: 0.00002031
Iteration 100/1000 | Loss: 0.00002031
Iteration 101/1000 | Loss: 0.00002031
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002030
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002028
Iteration 109/1000 | Loss: 0.00002028
Iteration 110/1000 | Loss: 0.00002028
Iteration 111/1000 | Loss: 0.00002028
Iteration 112/1000 | Loss: 0.00002028
Iteration 113/1000 | Loss: 0.00002028
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002027
Iteration 116/1000 | Loss: 0.00002027
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002026
Iteration 120/1000 | Loss: 0.00002025
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002024
Iteration 124/1000 | Loss: 0.00002024
Iteration 125/1000 | Loss: 0.00002024
Iteration 126/1000 | Loss: 0.00002024
Iteration 127/1000 | Loss: 0.00002024
Iteration 128/1000 | Loss: 0.00002024
Iteration 129/1000 | Loss: 0.00002023
Iteration 130/1000 | Loss: 0.00002023
Iteration 131/1000 | Loss: 0.00002023
Iteration 132/1000 | Loss: 0.00002023
Iteration 133/1000 | Loss: 0.00002023
Iteration 134/1000 | Loss: 0.00002023
Iteration 135/1000 | Loss: 0.00002022
Iteration 136/1000 | Loss: 0.00002022
Iteration 137/1000 | Loss: 0.00002022
Iteration 138/1000 | Loss: 0.00002022
Iteration 139/1000 | Loss: 0.00002022
Iteration 140/1000 | Loss: 0.00002022
Iteration 141/1000 | Loss: 0.00002022
Iteration 142/1000 | Loss: 0.00002022
Iteration 143/1000 | Loss: 0.00002022
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00002021
Iteration 146/1000 | Loss: 0.00002021
Iteration 147/1000 | Loss: 0.00002021
Iteration 148/1000 | Loss: 0.00002021
Iteration 149/1000 | Loss: 0.00002021
Iteration 150/1000 | Loss: 0.00002021
Iteration 151/1000 | Loss: 0.00002021
Iteration 152/1000 | Loss: 0.00002021
Iteration 153/1000 | Loss: 0.00002021
Iteration 154/1000 | Loss: 0.00002021
Iteration 155/1000 | Loss: 0.00002020
Iteration 156/1000 | Loss: 0.00002020
Iteration 157/1000 | Loss: 0.00002020
Iteration 158/1000 | Loss: 0.00002020
Iteration 159/1000 | Loss: 0.00002020
Iteration 160/1000 | Loss: 0.00002020
Iteration 161/1000 | Loss: 0.00002020
Iteration 162/1000 | Loss: 0.00002020
Iteration 163/1000 | Loss: 0.00002020
Iteration 164/1000 | Loss: 0.00002020
Iteration 165/1000 | Loss: 0.00002020
Iteration 166/1000 | Loss: 0.00002020
Iteration 167/1000 | Loss: 0.00002020
Iteration 168/1000 | Loss: 0.00002020
Iteration 169/1000 | Loss: 0.00002020
Iteration 170/1000 | Loss: 0.00002019
Iteration 171/1000 | Loss: 0.00002019
Iteration 172/1000 | Loss: 0.00002019
Iteration 173/1000 | Loss: 0.00002019
Iteration 174/1000 | Loss: 0.00002019
Iteration 175/1000 | Loss: 0.00002019
Iteration 176/1000 | Loss: 0.00002019
Iteration 177/1000 | Loss: 0.00002019
Iteration 178/1000 | Loss: 0.00002019
Iteration 179/1000 | Loss: 0.00002019
Iteration 180/1000 | Loss: 0.00002019
Iteration 181/1000 | Loss: 0.00002019
Iteration 182/1000 | Loss: 0.00002019
Iteration 183/1000 | Loss: 0.00002018
Iteration 184/1000 | Loss: 0.00002018
Iteration 185/1000 | Loss: 0.00002018
Iteration 186/1000 | Loss: 0.00002018
Iteration 187/1000 | Loss: 0.00002018
Iteration 188/1000 | Loss: 0.00002018
Iteration 189/1000 | Loss: 0.00002018
Iteration 190/1000 | Loss: 0.00002018
Iteration 191/1000 | Loss: 0.00002018
Iteration 192/1000 | Loss: 0.00002018
Iteration 193/1000 | Loss: 0.00002018
Iteration 194/1000 | Loss: 0.00002018
Iteration 195/1000 | Loss: 0.00002018
Iteration 196/1000 | Loss: 0.00002018
Iteration 197/1000 | Loss: 0.00002018
Iteration 198/1000 | Loss: 0.00002018
Iteration 199/1000 | Loss: 0.00002018
Iteration 200/1000 | Loss: 0.00002018
Iteration 201/1000 | Loss: 0.00002018
Iteration 202/1000 | Loss: 0.00002018
Iteration 203/1000 | Loss: 0.00002018
Iteration 204/1000 | Loss: 0.00002018
Iteration 205/1000 | Loss: 0.00002018
Iteration 206/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.0175190002191812e-05, 2.0175190002191812e-05, 2.0175190002191812e-05, 2.0175190002191812e-05, 2.0175190002191812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0175190002191812e-05

Optimization complete. Final v2v error: 3.731485366821289 mm

Highest mean error: 5.200989246368408 mm for frame 97

Lowest mean error: 3.072317600250244 mm for frame 146

Saving results

Total time: 42.30538868904114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906755
Iteration 2/25 | Loss: 0.00175213
Iteration 3/25 | Loss: 0.00144346
Iteration 4/25 | Loss: 0.00141904
Iteration 5/25 | Loss: 0.00141142
Iteration 6/25 | Loss: 0.00140966
Iteration 7/25 | Loss: 0.00140948
Iteration 8/25 | Loss: 0.00140948
Iteration 9/25 | Loss: 0.00140948
Iteration 10/25 | Loss: 0.00140948
Iteration 11/25 | Loss: 0.00140948
Iteration 12/25 | Loss: 0.00140948
Iteration 13/25 | Loss: 0.00140948
Iteration 14/25 | Loss: 0.00140948
Iteration 15/25 | Loss: 0.00140948
Iteration 16/25 | Loss: 0.00140948
Iteration 17/25 | Loss: 0.00140948
Iteration 18/25 | Loss: 0.00140948
Iteration 19/25 | Loss: 0.00140948
Iteration 20/25 | Loss: 0.00140948
Iteration 21/25 | Loss: 0.00140948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014094790676608682, 0.0014094790676608682, 0.0014094790676608682, 0.0014094790676608682, 0.0014094790676608682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014094790676608682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14130735
Iteration 2/25 | Loss: 0.00088560
Iteration 3/25 | Loss: 0.00088559
Iteration 4/25 | Loss: 0.00088559
Iteration 5/25 | Loss: 0.00088559
Iteration 6/25 | Loss: 0.00088559
Iteration 7/25 | Loss: 0.00088559
Iteration 8/25 | Loss: 0.00088559
Iteration 9/25 | Loss: 0.00088559
Iteration 10/25 | Loss: 0.00088559
Iteration 11/25 | Loss: 0.00088559
Iteration 12/25 | Loss: 0.00088559
Iteration 13/25 | Loss: 0.00088559
Iteration 14/25 | Loss: 0.00088559
Iteration 15/25 | Loss: 0.00088559
Iteration 16/25 | Loss: 0.00088559
Iteration 17/25 | Loss: 0.00088559
Iteration 18/25 | Loss: 0.00088559
Iteration 19/25 | Loss: 0.00088559
Iteration 20/25 | Loss: 0.00088559
Iteration 21/25 | Loss: 0.00088559
Iteration 22/25 | Loss: 0.00088559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008855893975123763, 0.0008855893975123763, 0.0008855893975123763, 0.0008855893975123763, 0.0008855893975123763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008855893975123763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088559
Iteration 2/1000 | Loss: 0.00005871
Iteration 3/1000 | Loss: 0.00004110
Iteration 4/1000 | Loss: 0.00003468
Iteration 5/1000 | Loss: 0.00003271
Iteration 6/1000 | Loss: 0.00003156
Iteration 7/1000 | Loss: 0.00003091
Iteration 8/1000 | Loss: 0.00003016
Iteration 9/1000 | Loss: 0.00002971
Iteration 10/1000 | Loss: 0.00002933
Iteration 11/1000 | Loss: 0.00002896
Iteration 12/1000 | Loss: 0.00002869
Iteration 13/1000 | Loss: 0.00002842
Iteration 14/1000 | Loss: 0.00002819
Iteration 15/1000 | Loss: 0.00002799
Iteration 16/1000 | Loss: 0.00002778
Iteration 17/1000 | Loss: 0.00002757
Iteration 18/1000 | Loss: 0.00002747
Iteration 19/1000 | Loss: 0.00002747
Iteration 20/1000 | Loss: 0.00002737
Iteration 21/1000 | Loss: 0.00002731
Iteration 22/1000 | Loss: 0.00002726
Iteration 23/1000 | Loss: 0.00002722
Iteration 24/1000 | Loss: 0.00002722
Iteration 25/1000 | Loss: 0.00002721
Iteration 26/1000 | Loss: 0.00002717
Iteration 27/1000 | Loss: 0.00002714
Iteration 28/1000 | Loss: 0.00002714
Iteration 29/1000 | Loss: 0.00002713
Iteration 30/1000 | Loss: 0.00002713
Iteration 31/1000 | Loss: 0.00002711
Iteration 32/1000 | Loss: 0.00002711
Iteration 33/1000 | Loss: 0.00002711
Iteration 34/1000 | Loss: 0.00002711
Iteration 35/1000 | Loss: 0.00002709
Iteration 36/1000 | Loss: 0.00002709
Iteration 37/1000 | Loss: 0.00002709
Iteration 38/1000 | Loss: 0.00002709
Iteration 39/1000 | Loss: 0.00002708
Iteration 40/1000 | Loss: 0.00002708
Iteration 41/1000 | Loss: 0.00002708
Iteration 42/1000 | Loss: 0.00002708
Iteration 43/1000 | Loss: 0.00002708
Iteration 44/1000 | Loss: 0.00002708
Iteration 45/1000 | Loss: 0.00002708
Iteration 46/1000 | Loss: 0.00002708
Iteration 47/1000 | Loss: 0.00002708
Iteration 48/1000 | Loss: 0.00002707
Iteration 49/1000 | Loss: 0.00002707
Iteration 50/1000 | Loss: 0.00002706
Iteration 51/1000 | Loss: 0.00002706
Iteration 52/1000 | Loss: 0.00002706
Iteration 53/1000 | Loss: 0.00002706
Iteration 54/1000 | Loss: 0.00002706
Iteration 55/1000 | Loss: 0.00002706
Iteration 56/1000 | Loss: 0.00002706
Iteration 57/1000 | Loss: 0.00002705
Iteration 58/1000 | Loss: 0.00002705
Iteration 59/1000 | Loss: 0.00002705
Iteration 60/1000 | Loss: 0.00002705
Iteration 61/1000 | Loss: 0.00002705
Iteration 62/1000 | Loss: 0.00002705
Iteration 63/1000 | Loss: 0.00002705
Iteration 64/1000 | Loss: 0.00002704
Iteration 65/1000 | Loss: 0.00002704
Iteration 66/1000 | Loss: 0.00002704
Iteration 67/1000 | Loss: 0.00002704
Iteration 68/1000 | Loss: 0.00002704
Iteration 69/1000 | Loss: 0.00002704
Iteration 70/1000 | Loss: 0.00002704
Iteration 71/1000 | Loss: 0.00002704
Iteration 72/1000 | Loss: 0.00002704
Iteration 73/1000 | Loss: 0.00002704
Iteration 74/1000 | Loss: 0.00002704
Iteration 75/1000 | Loss: 0.00002703
Iteration 76/1000 | Loss: 0.00002703
Iteration 77/1000 | Loss: 0.00002703
Iteration 78/1000 | Loss: 0.00002703
Iteration 79/1000 | Loss: 0.00002703
Iteration 80/1000 | Loss: 0.00002703
Iteration 81/1000 | Loss: 0.00002703
Iteration 82/1000 | Loss: 0.00002703
Iteration 83/1000 | Loss: 0.00002703
Iteration 84/1000 | Loss: 0.00002703
Iteration 85/1000 | Loss: 0.00002702
Iteration 86/1000 | Loss: 0.00002702
Iteration 87/1000 | Loss: 0.00002702
Iteration 88/1000 | Loss: 0.00002702
Iteration 89/1000 | Loss: 0.00002702
Iteration 90/1000 | Loss: 0.00002701
Iteration 91/1000 | Loss: 0.00002701
Iteration 92/1000 | Loss: 0.00002701
Iteration 93/1000 | Loss: 0.00002701
Iteration 94/1000 | Loss: 0.00002701
Iteration 95/1000 | Loss: 0.00002701
Iteration 96/1000 | Loss: 0.00002701
Iteration 97/1000 | Loss: 0.00002701
Iteration 98/1000 | Loss: 0.00002701
Iteration 99/1000 | Loss: 0.00002701
Iteration 100/1000 | Loss: 0.00002701
Iteration 101/1000 | Loss: 0.00002701
Iteration 102/1000 | Loss: 0.00002701
Iteration 103/1000 | Loss: 0.00002701
Iteration 104/1000 | Loss: 0.00002700
Iteration 105/1000 | Loss: 0.00002700
Iteration 106/1000 | Loss: 0.00002700
Iteration 107/1000 | Loss: 0.00002700
Iteration 108/1000 | Loss: 0.00002700
Iteration 109/1000 | Loss: 0.00002700
Iteration 110/1000 | Loss: 0.00002700
Iteration 111/1000 | Loss: 0.00002700
Iteration 112/1000 | Loss: 0.00002700
Iteration 113/1000 | Loss: 0.00002700
Iteration 114/1000 | Loss: 0.00002699
Iteration 115/1000 | Loss: 0.00002699
Iteration 116/1000 | Loss: 0.00002699
Iteration 117/1000 | Loss: 0.00002699
Iteration 118/1000 | Loss: 0.00002699
Iteration 119/1000 | Loss: 0.00002699
Iteration 120/1000 | Loss: 0.00002699
Iteration 121/1000 | Loss: 0.00002699
Iteration 122/1000 | Loss: 0.00002698
Iteration 123/1000 | Loss: 0.00002698
Iteration 124/1000 | Loss: 0.00002698
Iteration 125/1000 | Loss: 0.00002698
Iteration 126/1000 | Loss: 0.00002698
Iteration 127/1000 | Loss: 0.00002697
Iteration 128/1000 | Loss: 0.00002697
Iteration 129/1000 | Loss: 0.00002697
Iteration 130/1000 | Loss: 0.00002697
Iteration 131/1000 | Loss: 0.00002697
Iteration 132/1000 | Loss: 0.00002697
Iteration 133/1000 | Loss: 0.00002697
Iteration 134/1000 | Loss: 0.00002697
Iteration 135/1000 | Loss: 0.00002697
Iteration 136/1000 | Loss: 0.00002696
Iteration 137/1000 | Loss: 0.00002696
Iteration 138/1000 | Loss: 0.00002696
Iteration 139/1000 | Loss: 0.00002696
Iteration 140/1000 | Loss: 0.00002696
Iteration 141/1000 | Loss: 0.00002696
Iteration 142/1000 | Loss: 0.00002696
Iteration 143/1000 | Loss: 0.00002696
Iteration 144/1000 | Loss: 0.00002696
Iteration 145/1000 | Loss: 0.00002696
Iteration 146/1000 | Loss: 0.00002696
Iteration 147/1000 | Loss: 0.00002696
Iteration 148/1000 | Loss: 0.00002696
Iteration 149/1000 | Loss: 0.00002696
Iteration 150/1000 | Loss: 0.00002695
Iteration 151/1000 | Loss: 0.00002695
Iteration 152/1000 | Loss: 0.00002695
Iteration 153/1000 | Loss: 0.00002695
Iteration 154/1000 | Loss: 0.00002695
Iteration 155/1000 | Loss: 0.00002695
Iteration 156/1000 | Loss: 0.00002695
Iteration 157/1000 | Loss: 0.00002695
Iteration 158/1000 | Loss: 0.00002695
Iteration 159/1000 | Loss: 0.00002695
Iteration 160/1000 | Loss: 0.00002695
Iteration 161/1000 | Loss: 0.00002695
Iteration 162/1000 | Loss: 0.00002694
Iteration 163/1000 | Loss: 0.00002694
Iteration 164/1000 | Loss: 0.00002694
Iteration 165/1000 | Loss: 0.00002694
Iteration 166/1000 | Loss: 0.00002694
Iteration 167/1000 | Loss: 0.00002694
Iteration 168/1000 | Loss: 0.00002694
Iteration 169/1000 | Loss: 0.00002694
Iteration 170/1000 | Loss: 0.00002694
Iteration 171/1000 | Loss: 0.00002694
Iteration 172/1000 | Loss: 0.00002694
Iteration 173/1000 | Loss: 0.00002694
Iteration 174/1000 | Loss: 0.00002694
Iteration 175/1000 | Loss: 0.00002694
Iteration 176/1000 | Loss: 0.00002693
Iteration 177/1000 | Loss: 0.00002693
Iteration 178/1000 | Loss: 0.00002693
Iteration 179/1000 | Loss: 0.00002693
Iteration 180/1000 | Loss: 0.00002693
Iteration 181/1000 | Loss: 0.00002693
Iteration 182/1000 | Loss: 0.00002693
Iteration 183/1000 | Loss: 0.00002693
Iteration 184/1000 | Loss: 0.00002693
Iteration 185/1000 | Loss: 0.00002693
Iteration 186/1000 | Loss: 0.00002692
Iteration 187/1000 | Loss: 0.00002692
Iteration 188/1000 | Loss: 0.00002692
Iteration 189/1000 | Loss: 0.00002692
Iteration 190/1000 | Loss: 0.00002692
Iteration 191/1000 | Loss: 0.00002692
Iteration 192/1000 | Loss: 0.00002692
Iteration 193/1000 | Loss: 0.00002692
Iteration 194/1000 | Loss: 0.00002692
Iteration 195/1000 | Loss: 0.00002692
Iteration 196/1000 | Loss: 0.00002692
Iteration 197/1000 | Loss: 0.00002692
Iteration 198/1000 | Loss: 0.00002692
Iteration 199/1000 | Loss: 0.00002692
Iteration 200/1000 | Loss: 0.00002692
Iteration 201/1000 | Loss: 0.00002692
Iteration 202/1000 | Loss: 0.00002692
Iteration 203/1000 | Loss: 0.00002692
Iteration 204/1000 | Loss: 0.00002692
Iteration 205/1000 | Loss: 0.00002692
Iteration 206/1000 | Loss: 0.00002692
Iteration 207/1000 | Loss: 0.00002692
Iteration 208/1000 | Loss: 0.00002692
Iteration 209/1000 | Loss: 0.00002692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.6915860871667974e-05, 2.6915860871667974e-05, 2.6915860871667974e-05, 2.6915860871667974e-05, 2.6915860871667974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6915860871667974e-05

Optimization complete. Final v2v error: 4.26104211807251 mm

Highest mean error: 5.455481052398682 mm for frame 86

Lowest mean error: 3.495497941970825 mm for frame 121

Saving results

Total time: 49.92765808105469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835732
Iteration 2/25 | Loss: 0.00174480
Iteration 3/25 | Loss: 0.00144149
Iteration 4/25 | Loss: 0.00140792
Iteration 5/25 | Loss: 0.00141177
Iteration 6/25 | Loss: 0.00141173
Iteration 7/25 | Loss: 0.00139889
Iteration 8/25 | Loss: 0.00139532
Iteration 9/25 | Loss: 0.00139482
Iteration 10/25 | Loss: 0.00139026
Iteration 11/25 | Loss: 0.00138780
Iteration 12/25 | Loss: 0.00139145
Iteration 13/25 | Loss: 0.00138524
Iteration 14/25 | Loss: 0.00138227
Iteration 15/25 | Loss: 0.00138153
Iteration 16/25 | Loss: 0.00138112
Iteration 17/25 | Loss: 0.00138093
Iteration 18/25 | Loss: 0.00138080
Iteration 19/25 | Loss: 0.00138079
Iteration 20/25 | Loss: 0.00138078
Iteration 21/25 | Loss: 0.00138077
Iteration 22/25 | Loss: 0.00138077
Iteration 23/25 | Loss: 0.00138077
Iteration 24/25 | Loss: 0.00138077
Iteration 25/25 | Loss: 0.00138077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66510630
Iteration 2/25 | Loss: 0.00120030
Iteration 3/25 | Loss: 0.00104719
Iteration 4/25 | Loss: 0.00104718
Iteration 5/25 | Loss: 0.00104718
Iteration 6/25 | Loss: 0.00104718
Iteration 7/25 | Loss: 0.00104718
Iteration 8/25 | Loss: 0.00104718
Iteration 9/25 | Loss: 0.00104718
Iteration 10/25 | Loss: 0.00104718
Iteration 11/25 | Loss: 0.00104718
Iteration 12/25 | Loss: 0.00104718
Iteration 13/25 | Loss: 0.00104718
Iteration 14/25 | Loss: 0.00104718
Iteration 15/25 | Loss: 0.00104718
Iteration 16/25 | Loss: 0.00104718
Iteration 17/25 | Loss: 0.00104718
Iteration 18/25 | Loss: 0.00104718
Iteration 19/25 | Loss: 0.00104718
Iteration 20/25 | Loss: 0.00104718
Iteration 21/25 | Loss: 0.00104718
Iteration 22/25 | Loss: 0.00104718
Iteration 23/25 | Loss: 0.00104718
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010471809655427933, 0.0010471809655427933, 0.0010471809655427933, 0.0010471809655427933, 0.0010471809655427933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010471809655427933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104718
Iteration 2/1000 | Loss: 0.00008468
Iteration 3/1000 | Loss: 0.00004467
Iteration 4/1000 | Loss: 0.00003715
Iteration 5/1000 | Loss: 0.00003385
Iteration 6/1000 | Loss: 0.00003206
Iteration 7/1000 | Loss: 0.00003061
Iteration 8/1000 | Loss: 0.00002969
Iteration 9/1000 | Loss: 0.00011279
Iteration 10/1000 | Loss: 0.00018729
Iteration 11/1000 | Loss: 0.00003454
Iteration 12/1000 | Loss: 0.00003032
Iteration 13/1000 | Loss: 0.00002768
Iteration 14/1000 | Loss: 0.00002589
Iteration 15/1000 | Loss: 0.00002498
Iteration 16/1000 | Loss: 0.00016724
Iteration 17/1000 | Loss: 0.00003003
Iteration 18/1000 | Loss: 0.00002556
Iteration 19/1000 | Loss: 0.00002417
Iteration 20/1000 | Loss: 0.00002343
Iteration 21/1000 | Loss: 0.00002271
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002195
Iteration 24/1000 | Loss: 0.00002175
Iteration 25/1000 | Loss: 0.00002167
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002150
Iteration 29/1000 | Loss: 0.00002147
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002146
Iteration 32/1000 | Loss: 0.00002145
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002139
Iteration 36/1000 | Loss: 0.00002139
Iteration 37/1000 | Loss: 0.00002138
Iteration 38/1000 | Loss: 0.00002138
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002137
Iteration 41/1000 | Loss: 0.00002137
Iteration 42/1000 | Loss: 0.00002136
Iteration 43/1000 | Loss: 0.00002136
Iteration 44/1000 | Loss: 0.00002136
Iteration 45/1000 | Loss: 0.00002135
Iteration 46/1000 | Loss: 0.00002133
Iteration 47/1000 | Loss: 0.00002133
Iteration 48/1000 | Loss: 0.00002130
Iteration 49/1000 | Loss: 0.00002129
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002126
Iteration 52/1000 | Loss: 0.00002123
Iteration 53/1000 | Loss: 0.00002122
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002120
Iteration 58/1000 | Loss: 0.00002119
Iteration 59/1000 | Loss: 0.00002119
Iteration 60/1000 | Loss: 0.00002119
Iteration 61/1000 | Loss: 0.00002119
Iteration 62/1000 | Loss: 0.00002119
Iteration 63/1000 | Loss: 0.00002119
Iteration 64/1000 | Loss: 0.00002119
Iteration 65/1000 | Loss: 0.00002119
Iteration 66/1000 | Loss: 0.00002119
Iteration 67/1000 | Loss: 0.00002118
Iteration 68/1000 | Loss: 0.00002118
Iteration 69/1000 | Loss: 0.00002118
Iteration 70/1000 | Loss: 0.00002117
Iteration 71/1000 | Loss: 0.00002117
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002117
Iteration 74/1000 | Loss: 0.00002117
Iteration 75/1000 | Loss: 0.00002116
Iteration 76/1000 | Loss: 0.00002116
Iteration 77/1000 | Loss: 0.00002116
Iteration 78/1000 | Loss: 0.00002116
Iteration 79/1000 | Loss: 0.00002116
Iteration 80/1000 | Loss: 0.00002115
Iteration 81/1000 | Loss: 0.00002115
Iteration 82/1000 | Loss: 0.00002115
Iteration 83/1000 | Loss: 0.00002115
Iteration 84/1000 | Loss: 0.00002114
Iteration 85/1000 | Loss: 0.00002114
Iteration 86/1000 | Loss: 0.00002114
Iteration 87/1000 | Loss: 0.00002114
Iteration 88/1000 | Loss: 0.00002113
Iteration 89/1000 | Loss: 0.00002113
Iteration 90/1000 | Loss: 0.00002113
Iteration 91/1000 | Loss: 0.00002113
Iteration 92/1000 | Loss: 0.00002113
Iteration 93/1000 | Loss: 0.00002113
Iteration 94/1000 | Loss: 0.00002112
Iteration 95/1000 | Loss: 0.00002112
Iteration 96/1000 | Loss: 0.00002112
Iteration 97/1000 | Loss: 0.00002112
Iteration 98/1000 | Loss: 0.00002112
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002111
Iteration 102/1000 | Loss: 0.00002111
Iteration 103/1000 | Loss: 0.00002111
Iteration 104/1000 | Loss: 0.00002111
Iteration 105/1000 | Loss: 0.00002111
Iteration 106/1000 | Loss: 0.00002111
Iteration 107/1000 | Loss: 0.00002111
Iteration 108/1000 | Loss: 0.00002110
Iteration 109/1000 | Loss: 0.00002110
Iteration 110/1000 | Loss: 0.00002110
Iteration 111/1000 | Loss: 0.00002110
Iteration 112/1000 | Loss: 0.00002110
Iteration 113/1000 | Loss: 0.00002110
Iteration 114/1000 | Loss: 0.00002110
Iteration 115/1000 | Loss: 0.00002110
Iteration 116/1000 | Loss: 0.00002109
Iteration 117/1000 | Loss: 0.00002109
Iteration 118/1000 | Loss: 0.00002109
Iteration 119/1000 | Loss: 0.00002109
Iteration 120/1000 | Loss: 0.00002109
Iteration 121/1000 | Loss: 0.00002108
Iteration 122/1000 | Loss: 0.00002108
Iteration 123/1000 | Loss: 0.00002108
Iteration 124/1000 | Loss: 0.00002108
Iteration 125/1000 | Loss: 0.00002108
Iteration 126/1000 | Loss: 0.00002107
Iteration 127/1000 | Loss: 0.00002107
Iteration 128/1000 | Loss: 0.00002107
Iteration 129/1000 | Loss: 0.00002107
Iteration 130/1000 | Loss: 0.00002107
Iteration 131/1000 | Loss: 0.00002107
Iteration 132/1000 | Loss: 0.00002107
Iteration 133/1000 | Loss: 0.00002106
Iteration 134/1000 | Loss: 0.00002106
Iteration 135/1000 | Loss: 0.00002106
Iteration 136/1000 | Loss: 0.00002106
Iteration 137/1000 | Loss: 0.00002106
Iteration 138/1000 | Loss: 0.00002106
Iteration 139/1000 | Loss: 0.00002106
Iteration 140/1000 | Loss: 0.00002106
Iteration 141/1000 | Loss: 0.00002106
Iteration 142/1000 | Loss: 0.00002106
Iteration 143/1000 | Loss: 0.00002106
Iteration 144/1000 | Loss: 0.00002106
Iteration 145/1000 | Loss: 0.00002106
Iteration 146/1000 | Loss: 0.00002106
Iteration 147/1000 | Loss: 0.00002106
Iteration 148/1000 | Loss: 0.00002106
Iteration 149/1000 | Loss: 0.00002106
Iteration 150/1000 | Loss: 0.00002106
Iteration 151/1000 | Loss: 0.00002105
Iteration 152/1000 | Loss: 0.00002105
Iteration 153/1000 | Loss: 0.00002105
Iteration 154/1000 | Loss: 0.00002105
Iteration 155/1000 | Loss: 0.00002105
Iteration 156/1000 | Loss: 0.00002105
Iteration 157/1000 | Loss: 0.00002105
Iteration 158/1000 | Loss: 0.00002105
Iteration 159/1000 | Loss: 0.00002105
Iteration 160/1000 | Loss: 0.00002105
Iteration 161/1000 | Loss: 0.00002105
Iteration 162/1000 | Loss: 0.00002105
Iteration 163/1000 | Loss: 0.00002105
Iteration 164/1000 | Loss: 0.00002105
Iteration 165/1000 | Loss: 0.00002105
Iteration 166/1000 | Loss: 0.00002105
Iteration 167/1000 | Loss: 0.00002105
Iteration 168/1000 | Loss: 0.00002105
Iteration 169/1000 | Loss: 0.00002105
Iteration 170/1000 | Loss: 0.00002105
Iteration 171/1000 | Loss: 0.00002105
Iteration 172/1000 | Loss: 0.00002105
Iteration 173/1000 | Loss: 0.00002105
Iteration 174/1000 | Loss: 0.00002105
Iteration 175/1000 | Loss: 0.00002105
Iteration 176/1000 | Loss: 0.00002105
Iteration 177/1000 | Loss: 0.00002105
Iteration 178/1000 | Loss: 0.00002105
Iteration 179/1000 | Loss: 0.00002105
Iteration 180/1000 | Loss: 0.00002105
Iteration 181/1000 | Loss: 0.00002105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.1045505491201766e-05, 2.1045505491201766e-05, 2.1045505491201766e-05, 2.1045505491201766e-05, 2.1045505491201766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1045505491201766e-05

Optimization complete. Final v2v error: 3.778961181640625 mm

Highest mean error: 4.530228137969971 mm for frame 222

Lowest mean error: 3.0068626403808594 mm for frame 130

Saving results

Total time: 91.37949991226196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806996
Iteration 2/25 | Loss: 0.00154139
Iteration 3/25 | Loss: 0.00135757
Iteration 4/25 | Loss: 0.00134323
Iteration 5/25 | Loss: 0.00134006
Iteration 6/25 | Loss: 0.00134006
Iteration 7/25 | Loss: 0.00134006
Iteration 8/25 | Loss: 0.00134006
Iteration 9/25 | Loss: 0.00134006
Iteration 10/25 | Loss: 0.00134006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013400629395619035, 0.0013400629395619035, 0.0013400629395619035, 0.0013400629395619035, 0.0013400629395619035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013400629395619035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93599790
Iteration 2/25 | Loss: 0.00066151
Iteration 3/25 | Loss: 0.00066150
Iteration 4/25 | Loss: 0.00066150
Iteration 5/25 | Loss: 0.00066150
Iteration 6/25 | Loss: 0.00066150
Iteration 7/25 | Loss: 0.00066150
Iteration 8/25 | Loss: 0.00066150
Iteration 9/25 | Loss: 0.00066150
Iteration 10/25 | Loss: 0.00066150
Iteration 11/25 | Loss: 0.00066150
Iteration 12/25 | Loss: 0.00066150
Iteration 13/25 | Loss: 0.00066150
Iteration 14/25 | Loss: 0.00066150
Iteration 15/25 | Loss: 0.00066150
Iteration 16/25 | Loss: 0.00066150
Iteration 17/25 | Loss: 0.00066150
Iteration 18/25 | Loss: 0.00066150
Iteration 19/25 | Loss: 0.00066150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006615026504732668, 0.0006615026504732668, 0.0006615026504732668, 0.0006615026504732668, 0.0006615026504732668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006615026504732668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066150
Iteration 2/1000 | Loss: 0.00003459
Iteration 3/1000 | Loss: 0.00002757
Iteration 4/1000 | Loss: 0.00002583
Iteration 5/1000 | Loss: 0.00002470
Iteration 6/1000 | Loss: 0.00002416
Iteration 7/1000 | Loss: 0.00002373
Iteration 8/1000 | Loss: 0.00002334
Iteration 9/1000 | Loss: 0.00002314
Iteration 10/1000 | Loss: 0.00002308
Iteration 11/1000 | Loss: 0.00002299
Iteration 12/1000 | Loss: 0.00002297
Iteration 13/1000 | Loss: 0.00002293
Iteration 14/1000 | Loss: 0.00002287
Iteration 15/1000 | Loss: 0.00002283
Iteration 16/1000 | Loss: 0.00002271
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002270
Iteration 19/1000 | Loss: 0.00002270
Iteration 20/1000 | Loss: 0.00002269
Iteration 21/1000 | Loss: 0.00002268
Iteration 22/1000 | Loss: 0.00002267
Iteration 23/1000 | Loss: 0.00002267
Iteration 24/1000 | Loss: 0.00002267
Iteration 25/1000 | Loss: 0.00002267
Iteration 26/1000 | Loss: 0.00002266
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002263
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002263
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002262
Iteration 33/1000 | Loss: 0.00002260
Iteration 34/1000 | Loss: 0.00002260
Iteration 35/1000 | Loss: 0.00002260
Iteration 36/1000 | Loss: 0.00002260
Iteration 37/1000 | Loss: 0.00002259
Iteration 38/1000 | Loss: 0.00002259
Iteration 39/1000 | Loss: 0.00002259
Iteration 40/1000 | Loss: 0.00002259
Iteration 41/1000 | Loss: 0.00002259
Iteration 42/1000 | Loss: 0.00002259
Iteration 43/1000 | Loss: 0.00002259
Iteration 44/1000 | Loss: 0.00002259
Iteration 45/1000 | Loss: 0.00002258
Iteration 46/1000 | Loss: 0.00002258
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002258
Iteration 49/1000 | Loss: 0.00002258
Iteration 50/1000 | Loss: 0.00002258
Iteration 51/1000 | Loss: 0.00002257
Iteration 52/1000 | Loss: 0.00002257
Iteration 53/1000 | Loss: 0.00002257
Iteration 54/1000 | Loss: 0.00002257
Iteration 55/1000 | Loss: 0.00002257
Iteration 56/1000 | Loss: 0.00002257
Iteration 57/1000 | Loss: 0.00002257
Iteration 58/1000 | Loss: 0.00002256
Iteration 59/1000 | Loss: 0.00002256
Iteration 60/1000 | Loss: 0.00002256
Iteration 61/1000 | Loss: 0.00002256
Iteration 62/1000 | Loss: 0.00002256
Iteration 63/1000 | Loss: 0.00002256
Iteration 64/1000 | Loss: 0.00002256
Iteration 65/1000 | Loss: 0.00002256
Iteration 66/1000 | Loss: 0.00002256
Iteration 67/1000 | Loss: 0.00002256
Iteration 68/1000 | Loss: 0.00002256
Iteration 69/1000 | Loss: 0.00002255
Iteration 70/1000 | Loss: 0.00002255
Iteration 71/1000 | Loss: 0.00002255
Iteration 72/1000 | Loss: 0.00002255
Iteration 73/1000 | Loss: 0.00002254
Iteration 74/1000 | Loss: 0.00002254
Iteration 75/1000 | Loss: 0.00002254
Iteration 76/1000 | Loss: 0.00002252
Iteration 77/1000 | Loss: 0.00002252
Iteration 78/1000 | Loss: 0.00002252
Iteration 79/1000 | Loss: 0.00002252
Iteration 80/1000 | Loss: 0.00002252
Iteration 81/1000 | Loss: 0.00002252
Iteration 82/1000 | Loss: 0.00002251
Iteration 83/1000 | Loss: 0.00002251
Iteration 84/1000 | Loss: 0.00002251
Iteration 85/1000 | Loss: 0.00002250
Iteration 86/1000 | Loss: 0.00002249
Iteration 87/1000 | Loss: 0.00002249
Iteration 88/1000 | Loss: 0.00002249
Iteration 89/1000 | Loss: 0.00002249
Iteration 90/1000 | Loss: 0.00002248
Iteration 91/1000 | Loss: 0.00002248
Iteration 92/1000 | Loss: 0.00002247
Iteration 93/1000 | Loss: 0.00002247
Iteration 94/1000 | Loss: 0.00002247
Iteration 95/1000 | Loss: 0.00002247
Iteration 96/1000 | Loss: 0.00002247
Iteration 97/1000 | Loss: 0.00002247
Iteration 98/1000 | Loss: 0.00002247
Iteration 99/1000 | Loss: 0.00002247
Iteration 100/1000 | Loss: 0.00002247
Iteration 101/1000 | Loss: 0.00002247
Iteration 102/1000 | Loss: 0.00002246
Iteration 103/1000 | Loss: 0.00002246
Iteration 104/1000 | Loss: 0.00002246
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002245
Iteration 108/1000 | Loss: 0.00002245
Iteration 109/1000 | Loss: 0.00002245
Iteration 110/1000 | Loss: 0.00002244
Iteration 111/1000 | Loss: 0.00002244
Iteration 112/1000 | Loss: 0.00002244
Iteration 113/1000 | Loss: 0.00002243
Iteration 114/1000 | Loss: 0.00002242
Iteration 115/1000 | Loss: 0.00002242
Iteration 116/1000 | Loss: 0.00002242
Iteration 117/1000 | Loss: 0.00002242
Iteration 118/1000 | Loss: 0.00002242
Iteration 119/1000 | Loss: 0.00002242
Iteration 120/1000 | Loss: 0.00002242
Iteration 121/1000 | Loss: 0.00002242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.2419950255425647e-05, 2.2419950255425647e-05, 2.2419950255425647e-05, 2.2419950255425647e-05, 2.2419950255425647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2419950255425647e-05

Optimization complete. Final v2v error: 3.9314236640930176 mm

Highest mean error: 4.041119575500488 mm for frame 103

Lowest mean error: 3.815115213394165 mm for frame 185

Saving results

Total time: 37.5276083946228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822989
Iteration 2/25 | Loss: 0.00164017
Iteration 3/25 | Loss: 0.00144435
Iteration 4/25 | Loss: 0.00142317
Iteration 5/25 | Loss: 0.00141587
Iteration 6/25 | Loss: 0.00140732
Iteration 7/25 | Loss: 0.00140489
Iteration 8/25 | Loss: 0.00140385
Iteration 9/25 | Loss: 0.00140333
Iteration 10/25 | Loss: 0.00140315
Iteration 11/25 | Loss: 0.00140306
Iteration 12/25 | Loss: 0.00140304
Iteration 13/25 | Loss: 0.00140301
Iteration 14/25 | Loss: 0.00140300
Iteration 15/25 | Loss: 0.00140300
Iteration 16/25 | Loss: 0.00140300
Iteration 17/25 | Loss: 0.00140300
Iteration 18/25 | Loss: 0.00140300
Iteration 19/25 | Loss: 0.00140299
Iteration 20/25 | Loss: 0.00140299
Iteration 21/25 | Loss: 0.00140299
Iteration 22/25 | Loss: 0.00140299
Iteration 23/25 | Loss: 0.00140299
Iteration 24/25 | Loss: 0.00140299
Iteration 25/25 | Loss: 0.00140299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24918091
Iteration 2/25 | Loss: 0.00129196
Iteration 3/25 | Loss: 0.00129191
Iteration 4/25 | Loss: 0.00129191
Iteration 5/25 | Loss: 0.00129191
Iteration 6/25 | Loss: 0.00129191
Iteration 7/25 | Loss: 0.00129191
Iteration 8/25 | Loss: 0.00129191
Iteration 9/25 | Loss: 0.00129191
Iteration 10/25 | Loss: 0.00129191
Iteration 11/25 | Loss: 0.00129191
Iteration 12/25 | Loss: 0.00129191
Iteration 13/25 | Loss: 0.00129191
Iteration 14/25 | Loss: 0.00129191
Iteration 15/25 | Loss: 0.00129191
Iteration 16/25 | Loss: 0.00129191
Iteration 17/25 | Loss: 0.00129191
Iteration 18/25 | Loss: 0.00129191
Iteration 19/25 | Loss: 0.00129191
Iteration 20/25 | Loss: 0.00129191
Iteration 21/25 | Loss: 0.00129191
Iteration 22/25 | Loss: 0.00129191
Iteration 23/25 | Loss: 0.00129191
Iteration 24/25 | Loss: 0.00129191
Iteration 25/25 | Loss: 0.00129191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129191
Iteration 2/1000 | Loss: 0.00014095
Iteration 3/1000 | Loss: 0.00010004
Iteration 4/1000 | Loss: 0.00008394
Iteration 5/1000 | Loss: 0.00007388
Iteration 6/1000 | Loss: 0.00007041
Iteration 7/1000 | Loss: 0.00006760
Iteration 8/1000 | Loss: 0.00006588
Iteration 9/1000 | Loss: 0.00006433
Iteration 10/1000 | Loss: 0.00006335
Iteration 11/1000 | Loss: 0.00006254
Iteration 12/1000 | Loss: 0.00006162
Iteration 13/1000 | Loss: 0.00006061
Iteration 14/1000 | Loss: 0.00005990
Iteration 15/1000 | Loss: 0.00005935
Iteration 16/1000 | Loss: 0.00005885
Iteration 17/1000 | Loss: 0.00005830
Iteration 18/1000 | Loss: 0.00005771
Iteration 19/1000 | Loss: 0.00005723
Iteration 20/1000 | Loss: 0.00005682
Iteration 21/1000 | Loss: 0.00191348
Iteration 22/1000 | Loss: 0.00439777
Iteration 23/1000 | Loss: 0.00274973
Iteration 24/1000 | Loss: 0.00064551
Iteration 25/1000 | Loss: 0.00126896
Iteration 26/1000 | Loss: 0.00055771
Iteration 27/1000 | Loss: 0.00105314
Iteration 28/1000 | Loss: 0.00100034
Iteration 29/1000 | Loss: 0.00101409
Iteration 30/1000 | Loss: 0.00059262
Iteration 31/1000 | Loss: 0.00098835
Iteration 32/1000 | Loss: 0.00074971
Iteration 33/1000 | Loss: 0.00023789
Iteration 34/1000 | Loss: 0.00038381
Iteration 35/1000 | Loss: 0.00081610
Iteration 36/1000 | Loss: 0.00058539
Iteration 37/1000 | Loss: 0.00055452
Iteration 38/1000 | Loss: 0.00042247
Iteration 39/1000 | Loss: 0.00027388
Iteration 40/1000 | Loss: 0.00030462
Iteration 41/1000 | Loss: 0.00036413
Iteration 42/1000 | Loss: 0.00077039
Iteration 43/1000 | Loss: 0.00035183
Iteration 44/1000 | Loss: 0.00006158
Iteration 45/1000 | Loss: 0.00031442
Iteration 46/1000 | Loss: 0.00027216
Iteration 47/1000 | Loss: 0.00004858
Iteration 48/1000 | Loss: 0.00004532
Iteration 49/1000 | Loss: 0.00021547
Iteration 50/1000 | Loss: 0.00019361
Iteration 51/1000 | Loss: 0.00020039
Iteration 52/1000 | Loss: 0.00021030
Iteration 53/1000 | Loss: 0.00035672
Iteration 54/1000 | Loss: 0.00007379
Iteration 55/1000 | Loss: 0.00004172
Iteration 56/1000 | Loss: 0.00004043
Iteration 57/1000 | Loss: 0.00003981
Iteration 58/1000 | Loss: 0.00003931
Iteration 59/1000 | Loss: 0.00003883
Iteration 60/1000 | Loss: 0.00041622
Iteration 61/1000 | Loss: 0.00040897
Iteration 62/1000 | Loss: 0.00038122
Iteration 63/1000 | Loss: 0.00017535
Iteration 64/1000 | Loss: 0.00092382
Iteration 65/1000 | Loss: 0.00091778
Iteration 66/1000 | Loss: 0.00110842
Iteration 67/1000 | Loss: 0.00084724
Iteration 68/1000 | Loss: 0.00075783
Iteration 69/1000 | Loss: 0.00073140
Iteration 70/1000 | Loss: 0.00054729
Iteration 71/1000 | Loss: 0.00009724
Iteration 72/1000 | Loss: 0.00004208
Iteration 73/1000 | Loss: 0.00030222
Iteration 74/1000 | Loss: 0.00022839
Iteration 75/1000 | Loss: 0.00042326
Iteration 76/1000 | Loss: 0.00050023
Iteration 77/1000 | Loss: 0.00005452
Iteration 78/1000 | Loss: 0.00011863
Iteration 79/1000 | Loss: 0.00009997
Iteration 80/1000 | Loss: 0.00009690
Iteration 81/1000 | Loss: 0.00042394
Iteration 82/1000 | Loss: 0.00006845
Iteration 83/1000 | Loss: 0.00004596
Iteration 84/1000 | Loss: 0.00004239
Iteration 85/1000 | Loss: 0.00004097
Iteration 86/1000 | Loss: 0.00004036
Iteration 87/1000 | Loss: 0.00003960
Iteration 88/1000 | Loss: 0.00085779
Iteration 89/1000 | Loss: 0.00031907
Iteration 90/1000 | Loss: 0.00022150
Iteration 91/1000 | Loss: 0.00010327
Iteration 92/1000 | Loss: 0.00059690
Iteration 93/1000 | Loss: 0.00026296
Iteration 94/1000 | Loss: 0.00008547
Iteration 95/1000 | Loss: 0.00013375
Iteration 96/1000 | Loss: 0.00056710
Iteration 97/1000 | Loss: 0.00021856
Iteration 98/1000 | Loss: 0.00062977
Iteration 99/1000 | Loss: 0.00097110
Iteration 100/1000 | Loss: 0.00105774
Iteration 101/1000 | Loss: 0.00120078
Iteration 102/1000 | Loss: 0.00125026
Iteration 103/1000 | Loss: 0.00082192
Iteration 104/1000 | Loss: 0.00093397
Iteration 105/1000 | Loss: 0.00067571
Iteration 106/1000 | Loss: 0.00061116
Iteration 107/1000 | Loss: 0.00073238
Iteration 108/1000 | Loss: 0.00064514
Iteration 109/1000 | Loss: 0.00072719
Iteration 110/1000 | Loss: 0.00077534
Iteration 111/1000 | Loss: 0.00050252
Iteration 112/1000 | Loss: 0.00032420
Iteration 113/1000 | Loss: 0.00006945
Iteration 114/1000 | Loss: 0.00027809
Iteration 115/1000 | Loss: 0.00034905
Iteration 116/1000 | Loss: 0.00037254
Iteration 117/1000 | Loss: 0.00006349
Iteration 118/1000 | Loss: 0.00004834
Iteration 119/1000 | Loss: 0.00004321
Iteration 120/1000 | Loss: 0.00003872
Iteration 121/1000 | Loss: 0.00003640
Iteration 122/1000 | Loss: 0.00003483
Iteration 123/1000 | Loss: 0.00003403
Iteration 124/1000 | Loss: 0.00003347
Iteration 125/1000 | Loss: 0.00003311
Iteration 126/1000 | Loss: 0.00003282
Iteration 127/1000 | Loss: 0.00003252
Iteration 128/1000 | Loss: 0.00003226
Iteration 129/1000 | Loss: 0.00003223
Iteration 130/1000 | Loss: 0.00003205
Iteration 131/1000 | Loss: 0.00003205
Iteration 132/1000 | Loss: 0.00003201
Iteration 133/1000 | Loss: 0.00003200
Iteration 134/1000 | Loss: 0.00003200
Iteration 135/1000 | Loss: 0.00003199
Iteration 136/1000 | Loss: 0.00003195
Iteration 137/1000 | Loss: 0.00003193
Iteration 138/1000 | Loss: 0.00003186
Iteration 139/1000 | Loss: 0.00003181
Iteration 140/1000 | Loss: 0.00003181
Iteration 141/1000 | Loss: 0.00003177
Iteration 142/1000 | Loss: 0.00003177
Iteration 143/1000 | Loss: 0.00003176
Iteration 144/1000 | Loss: 0.00003174
Iteration 145/1000 | Loss: 0.00003174
Iteration 146/1000 | Loss: 0.00003173
Iteration 147/1000 | Loss: 0.00003159
Iteration 148/1000 | Loss: 0.00003157
Iteration 149/1000 | Loss: 0.00003152
Iteration 150/1000 | Loss: 0.00003150
Iteration 151/1000 | Loss: 0.00003150
Iteration 152/1000 | Loss: 0.00003149
Iteration 153/1000 | Loss: 0.00003148
Iteration 154/1000 | Loss: 0.00003147
Iteration 155/1000 | Loss: 0.00003145
Iteration 156/1000 | Loss: 0.00038047
Iteration 157/1000 | Loss: 0.00115453
Iteration 158/1000 | Loss: 0.00006131
Iteration 159/1000 | Loss: 0.00004626
Iteration 160/1000 | Loss: 0.00004079
Iteration 161/1000 | Loss: 0.00022121
Iteration 162/1000 | Loss: 0.00013980
Iteration 163/1000 | Loss: 0.00018213
Iteration 164/1000 | Loss: 0.00016709
Iteration 165/1000 | Loss: 0.00022097
Iteration 166/1000 | Loss: 0.00017589
Iteration 167/1000 | Loss: 0.00022470
Iteration 168/1000 | Loss: 0.00027573
Iteration 169/1000 | Loss: 0.00021104
Iteration 170/1000 | Loss: 0.00010367
Iteration 171/1000 | Loss: 0.00003023
Iteration 172/1000 | Loss: 0.00002926
Iteration 173/1000 | Loss: 0.00002824
Iteration 174/1000 | Loss: 0.00036003
Iteration 175/1000 | Loss: 0.00018724
Iteration 176/1000 | Loss: 0.00012492
Iteration 177/1000 | Loss: 0.00004597
Iteration 178/1000 | Loss: 0.00003807
Iteration 179/1000 | Loss: 0.00003275
Iteration 180/1000 | Loss: 0.00003025
Iteration 181/1000 | Loss: 0.00002920
Iteration 182/1000 | Loss: 0.00002829
Iteration 183/1000 | Loss: 0.00002716
Iteration 184/1000 | Loss: 0.00002618
Iteration 185/1000 | Loss: 0.00002567
Iteration 186/1000 | Loss: 0.00002525
Iteration 187/1000 | Loss: 0.00002484
Iteration 188/1000 | Loss: 0.00002458
Iteration 189/1000 | Loss: 0.00002430
Iteration 190/1000 | Loss: 0.00002414
Iteration 191/1000 | Loss: 0.00002404
Iteration 192/1000 | Loss: 0.00002402
Iteration 193/1000 | Loss: 0.00002402
Iteration 194/1000 | Loss: 0.00002402
Iteration 195/1000 | Loss: 0.00002402
Iteration 196/1000 | Loss: 0.00002402
Iteration 197/1000 | Loss: 0.00002401
Iteration 198/1000 | Loss: 0.00002401
Iteration 199/1000 | Loss: 0.00002401
Iteration 200/1000 | Loss: 0.00002401
Iteration 201/1000 | Loss: 0.00002401
Iteration 202/1000 | Loss: 0.00002401
Iteration 203/1000 | Loss: 0.00002401
Iteration 204/1000 | Loss: 0.00002399
Iteration 205/1000 | Loss: 0.00002399
Iteration 206/1000 | Loss: 0.00002399
Iteration 207/1000 | Loss: 0.00002399
Iteration 208/1000 | Loss: 0.00002399
Iteration 209/1000 | Loss: 0.00002399
Iteration 210/1000 | Loss: 0.00002399
Iteration 211/1000 | Loss: 0.00002399
Iteration 212/1000 | Loss: 0.00002398
Iteration 213/1000 | Loss: 0.00002398
Iteration 214/1000 | Loss: 0.00002398
Iteration 215/1000 | Loss: 0.00002398
Iteration 216/1000 | Loss: 0.00002398
Iteration 217/1000 | Loss: 0.00002398
Iteration 218/1000 | Loss: 0.00002398
Iteration 219/1000 | Loss: 0.00002398
Iteration 220/1000 | Loss: 0.00002398
Iteration 221/1000 | Loss: 0.00002398
Iteration 222/1000 | Loss: 0.00002398
Iteration 223/1000 | Loss: 0.00002397
Iteration 224/1000 | Loss: 0.00002397
Iteration 225/1000 | Loss: 0.00002397
Iteration 226/1000 | Loss: 0.00002397
Iteration 227/1000 | Loss: 0.00002396
Iteration 228/1000 | Loss: 0.00002396
Iteration 229/1000 | Loss: 0.00002396
Iteration 230/1000 | Loss: 0.00002396
Iteration 231/1000 | Loss: 0.00002396
Iteration 232/1000 | Loss: 0.00002396
Iteration 233/1000 | Loss: 0.00002396
Iteration 234/1000 | Loss: 0.00002396
Iteration 235/1000 | Loss: 0.00002395
Iteration 236/1000 | Loss: 0.00002395
Iteration 237/1000 | Loss: 0.00002395
Iteration 238/1000 | Loss: 0.00002395
Iteration 239/1000 | Loss: 0.00002395
Iteration 240/1000 | Loss: 0.00002395
Iteration 241/1000 | Loss: 0.00002394
Iteration 242/1000 | Loss: 0.00002394
Iteration 243/1000 | Loss: 0.00002394
Iteration 244/1000 | Loss: 0.00002394
Iteration 245/1000 | Loss: 0.00002394
Iteration 246/1000 | Loss: 0.00002394
Iteration 247/1000 | Loss: 0.00002394
Iteration 248/1000 | Loss: 0.00002394
Iteration 249/1000 | Loss: 0.00002394
Iteration 250/1000 | Loss: 0.00002394
Iteration 251/1000 | Loss: 0.00002394
Iteration 252/1000 | Loss: 0.00002394
Iteration 253/1000 | Loss: 0.00002394
Iteration 254/1000 | Loss: 0.00002394
Iteration 255/1000 | Loss: 0.00002394
Iteration 256/1000 | Loss: 0.00002394
Iteration 257/1000 | Loss: 0.00002394
Iteration 258/1000 | Loss: 0.00002394
Iteration 259/1000 | Loss: 0.00002394
Iteration 260/1000 | Loss: 0.00002394
Iteration 261/1000 | Loss: 0.00002394
Iteration 262/1000 | Loss: 0.00002394
Iteration 263/1000 | Loss: 0.00002394
Iteration 264/1000 | Loss: 0.00002394
Iteration 265/1000 | Loss: 0.00002394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.3939219317981042e-05, 2.3939219317981042e-05, 2.3939219317981042e-05, 2.3939219317981042e-05, 2.3939219317981042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3939219317981042e-05

Optimization complete. Final v2v error: 3.686789035797119 mm

Highest mean error: 13.108108520507812 mm for frame 70

Lowest mean error: 3.2691490650177 mm for frame 59

Saving results

Total time: 252.27206659317017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032114
Iteration 2/25 | Loss: 0.01032114
Iteration 3/25 | Loss: 0.01032114
Iteration 4/25 | Loss: 0.01032114
Iteration 5/25 | Loss: 0.01032114
Iteration 6/25 | Loss: 0.01032114
Iteration 7/25 | Loss: 0.01032114
Iteration 8/25 | Loss: 0.01032113
Iteration 9/25 | Loss: 0.01032113
Iteration 10/25 | Loss: 0.01032113
Iteration 11/25 | Loss: 0.01032113
Iteration 12/25 | Loss: 0.01032113
Iteration 13/25 | Loss: 0.01032113
Iteration 14/25 | Loss: 0.01032113
Iteration 15/25 | Loss: 0.01032113
Iteration 16/25 | Loss: 0.01032113
Iteration 17/25 | Loss: 0.01032113
Iteration 18/25 | Loss: 0.01032113
Iteration 19/25 | Loss: 0.01032112
Iteration 20/25 | Loss: 0.01032112
Iteration 21/25 | Loss: 0.01032112
Iteration 22/25 | Loss: 0.01032112
Iteration 23/25 | Loss: 0.01032112
Iteration 24/25 | Loss: 0.01032112
Iteration 25/25 | Loss: 0.01032112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72527909
Iteration 2/25 | Loss: 0.08578655
Iteration 3/25 | Loss: 0.08578639
Iteration 4/25 | Loss: 0.08578637
Iteration 5/25 | Loss: 0.08578636
Iteration 6/25 | Loss: 0.08578635
Iteration 7/25 | Loss: 0.08578635
Iteration 8/25 | Loss: 0.08578635
Iteration 9/25 | Loss: 0.08578635
Iteration 10/25 | Loss: 0.08578635
Iteration 11/25 | Loss: 0.08578635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0857863500714302, 0.0857863500714302, 0.0857863500714302, 0.0857863500714302, 0.0857863500714302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0857863500714302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08578634
Iteration 2/1000 | Loss: 0.00804645
Iteration 3/1000 | Loss: 0.00305683
Iteration 4/1000 | Loss: 0.00193037
Iteration 5/1000 | Loss: 0.00173816
Iteration 6/1000 | Loss: 0.00358277
Iteration 7/1000 | Loss: 0.00071054
Iteration 8/1000 | Loss: 0.00087675
Iteration 9/1000 | Loss: 0.00163907
Iteration 10/1000 | Loss: 0.00036369
Iteration 11/1000 | Loss: 0.00081611
Iteration 12/1000 | Loss: 0.00048516
Iteration 13/1000 | Loss: 0.00019490
Iteration 14/1000 | Loss: 0.00017159
Iteration 15/1000 | Loss: 0.00014282
Iteration 16/1000 | Loss: 0.00013414
Iteration 17/1000 | Loss: 0.00014061
Iteration 18/1000 | Loss: 0.00011756
Iteration 19/1000 | Loss: 0.00623564
Iteration 20/1000 | Loss: 0.00099798
Iteration 21/1000 | Loss: 0.00032411
Iteration 22/1000 | Loss: 0.00092878
Iteration 23/1000 | Loss: 0.00321383
Iteration 24/1000 | Loss: 0.00239332
Iteration 25/1000 | Loss: 0.00190227
Iteration 26/1000 | Loss: 0.00048047
Iteration 27/1000 | Loss: 0.00012141
Iteration 28/1000 | Loss: 0.00007461
Iteration 29/1000 | Loss: 0.00016313
Iteration 30/1000 | Loss: 0.00005574
Iteration 31/1000 | Loss: 0.00010011
Iteration 32/1000 | Loss: 0.00017351
Iteration 33/1000 | Loss: 0.00018765
Iteration 34/1000 | Loss: 0.00033900
Iteration 35/1000 | Loss: 0.00005099
Iteration 36/1000 | Loss: 0.00032348
Iteration 37/1000 | Loss: 0.00023889
Iteration 38/1000 | Loss: 0.00004468
Iteration 39/1000 | Loss: 0.00004817
Iteration 40/1000 | Loss: 0.00004333
Iteration 41/1000 | Loss: 0.00004099
Iteration 42/1000 | Loss: 0.00036997
Iteration 43/1000 | Loss: 0.00020397
Iteration 44/1000 | Loss: 0.00010133
Iteration 45/1000 | Loss: 0.00003930
Iteration 46/1000 | Loss: 0.00003847
Iteration 47/1000 | Loss: 0.00017393
Iteration 48/1000 | Loss: 0.00004173
Iteration 49/1000 | Loss: 0.00004912
Iteration 50/1000 | Loss: 0.00032555
Iteration 51/1000 | Loss: 0.00005412
Iteration 52/1000 | Loss: 0.00012682
Iteration 53/1000 | Loss: 0.00004270
Iteration 54/1000 | Loss: 0.00005600
Iteration 55/1000 | Loss: 0.00008060
Iteration 56/1000 | Loss: 0.00004023
Iteration 57/1000 | Loss: 0.00003630
Iteration 58/1000 | Loss: 0.00004884
Iteration 59/1000 | Loss: 0.00004401
Iteration 60/1000 | Loss: 0.00004591
Iteration 61/1000 | Loss: 0.00004351
Iteration 62/1000 | Loss: 0.00004329
Iteration 63/1000 | Loss: 0.00004527
Iteration 64/1000 | Loss: 0.00004388
Iteration 65/1000 | Loss: 0.00010461
Iteration 66/1000 | Loss: 0.00005251
Iteration 67/1000 | Loss: 0.00006623
Iteration 68/1000 | Loss: 0.00005426
Iteration 69/1000 | Loss: 0.00005675
Iteration 70/1000 | Loss: 0.00004724
Iteration 71/1000 | Loss: 0.00004982
Iteration 72/1000 | Loss: 0.00004460
Iteration 73/1000 | Loss: 0.00004718
Iteration 74/1000 | Loss: 0.00004543
Iteration 75/1000 | Loss: 0.00005195
Iteration 76/1000 | Loss: 0.00004875
Iteration 77/1000 | Loss: 0.00004969
Iteration 78/1000 | Loss: 0.00004291
Iteration 79/1000 | Loss: 0.00004181
Iteration 80/1000 | Loss: 0.00003782
Iteration 81/1000 | Loss: 0.00003550
Iteration 82/1000 | Loss: 0.00003411
Iteration 83/1000 | Loss: 0.00003641
Iteration 84/1000 | Loss: 0.00003399
Iteration 85/1000 | Loss: 0.00003305
Iteration 86/1000 | Loss: 0.00003234
Iteration 87/1000 | Loss: 0.00003202
Iteration 88/1000 | Loss: 0.00003188
Iteration 89/1000 | Loss: 0.00003187
Iteration 90/1000 | Loss: 0.00003187
Iteration 91/1000 | Loss: 0.00003185
Iteration 92/1000 | Loss: 0.00003185
Iteration 93/1000 | Loss: 0.00003181
Iteration 94/1000 | Loss: 0.00003175
Iteration 95/1000 | Loss: 0.00003174
Iteration 96/1000 | Loss: 0.00003153
Iteration 97/1000 | Loss: 0.00003146
Iteration 98/1000 | Loss: 0.00003136
Iteration 99/1000 | Loss: 0.00003135
Iteration 100/1000 | Loss: 0.00003134
Iteration 101/1000 | Loss: 0.00003134
Iteration 102/1000 | Loss: 0.00003134
Iteration 103/1000 | Loss: 0.00003134
Iteration 104/1000 | Loss: 0.00003134
Iteration 105/1000 | Loss: 0.00003134
Iteration 106/1000 | Loss: 0.00003133
Iteration 107/1000 | Loss: 0.00003133
Iteration 108/1000 | Loss: 0.00003133
Iteration 109/1000 | Loss: 0.00003133
Iteration 110/1000 | Loss: 0.00003133
Iteration 111/1000 | Loss: 0.00003133
Iteration 112/1000 | Loss: 0.00003133
Iteration 113/1000 | Loss: 0.00003133
Iteration 114/1000 | Loss: 0.00003133
Iteration 115/1000 | Loss: 0.00003133
Iteration 116/1000 | Loss: 0.00003133
Iteration 117/1000 | Loss: 0.00003133
Iteration 118/1000 | Loss: 0.00003133
Iteration 119/1000 | Loss: 0.00003132
Iteration 120/1000 | Loss: 0.00003132
Iteration 121/1000 | Loss: 0.00003132
Iteration 122/1000 | Loss: 0.00003132
Iteration 123/1000 | Loss: 0.00003132
Iteration 124/1000 | Loss: 0.00003132
Iteration 125/1000 | Loss: 0.00003131
Iteration 126/1000 | Loss: 0.00003131
Iteration 127/1000 | Loss: 0.00003131
Iteration 128/1000 | Loss: 0.00003130
Iteration 129/1000 | Loss: 0.00003127
Iteration 130/1000 | Loss: 0.00003127
Iteration 131/1000 | Loss: 0.00003125
Iteration 132/1000 | Loss: 0.00003124
Iteration 133/1000 | Loss: 0.00003124
Iteration 134/1000 | Loss: 0.00003121
Iteration 135/1000 | Loss: 0.00003121
Iteration 136/1000 | Loss: 0.00003118
Iteration 137/1000 | Loss: 0.00003113
Iteration 138/1000 | Loss: 0.00003108
Iteration 139/1000 | Loss: 0.00003103
Iteration 140/1000 | Loss: 0.00003103
Iteration 141/1000 | Loss: 0.00003102
Iteration 142/1000 | Loss: 0.00003102
Iteration 143/1000 | Loss: 0.00003101
Iteration 144/1000 | Loss: 0.00003101
Iteration 145/1000 | Loss: 0.00003100
Iteration 146/1000 | Loss: 0.00003100
Iteration 147/1000 | Loss: 0.00003099
Iteration 148/1000 | Loss: 0.00003099
Iteration 149/1000 | Loss: 0.00003098
Iteration 150/1000 | Loss: 0.00003097
Iteration 151/1000 | Loss: 0.00003095
Iteration 152/1000 | Loss: 0.00003095
Iteration 153/1000 | Loss: 0.00003095
Iteration 154/1000 | Loss: 0.00003095
Iteration 155/1000 | Loss: 0.00003095
Iteration 156/1000 | Loss: 0.00003095
Iteration 157/1000 | Loss: 0.00003094
Iteration 158/1000 | Loss: 0.00003094
Iteration 159/1000 | Loss: 0.00003094
Iteration 160/1000 | Loss: 0.00003094
Iteration 161/1000 | Loss: 0.00003094
Iteration 162/1000 | Loss: 0.00003094
Iteration 163/1000 | Loss: 0.00003094
Iteration 164/1000 | Loss: 0.00003094
Iteration 165/1000 | Loss: 0.00003094
Iteration 166/1000 | Loss: 0.00003094
Iteration 167/1000 | Loss: 0.00003094
Iteration 168/1000 | Loss: 0.00003094
Iteration 169/1000 | Loss: 0.00003094
Iteration 170/1000 | Loss: 0.00003094
Iteration 171/1000 | Loss: 0.00003093
Iteration 172/1000 | Loss: 0.00003093
Iteration 173/1000 | Loss: 0.00003093
Iteration 174/1000 | Loss: 0.00003093
Iteration 175/1000 | Loss: 0.00003093
Iteration 176/1000 | Loss: 0.00003093
Iteration 177/1000 | Loss: 0.00003092
Iteration 178/1000 | Loss: 0.00003092
Iteration 179/1000 | Loss: 0.00003092
Iteration 180/1000 | Loss: 0.00003091
Iteration 181/1000 | Loss: 0.00003091
Iteration 182/1000 | Loss: 0.00003091
Iteration 183/1000 | Loss: 0.00003091
Iteration 184/1000 | Loss: 0.00003090
Iteration 185/1000 | Loss: 0.00003090
Iteration 186/1000 | Loss: 0.00003089
Iteration 187/1000 | Loss: 0.00003089
Iteration 188/1000 | Loss: 0.00003089
Iteration 189/1000 | Loss: 0.00003088
Iteration 190/1000 | Loss: 0.00003087
Iteration 191/1000 | Loss: 0.00003087
Iteration 192/1000 | Loss: 0.00003087
Iteration 193/1000 | Loss: 0.00003087
Iteration 194/1000 | Loss: 0.00003087
Iteration 195/1000 | Loss: 0.00003087
Iteration 196/1000 | Loss: 0.00003087
Iteration 197/1000 | Loss: 0.00003087
Iteration 198/1000 | Loss: 0.00003087
Iteration 199/1000 | Loss: 0.00003087
Iteration 200/1000 | Loss: 0.00003086
Iteration 201/1000 | Loss: 0.00003086
Iteration 202/1000 | Loss: 0.00003086
Iteration 203/1000 | Loss: 0.00003086
Iteration 204/1000 | Loss: 0.00003085
Iteration 205/1000 | Loss: 0.00003085
Iteration 206/1000 | Loss: 0.00003085
Iteration 207/1000 | Loss: 0.00003085
Iteration 208/1000 | Loss: 0.00003084
Iteration 209/1000 | Loss: 0.00003084
Iteration 210/1000 | Loss: 0.00003084
Iteration 211/1000 | Loss: 0.00003084
Iteration 212/1000 | Loss: 0.00003084
Iteration 213/1000 | Loss: 0.00003084
Iteration 214/1000 | Loss: 0.00003084
Iteration 215/1000 | Loss: 0.00003083
Iteration 216/1000 | Loss: 0.00003083
Iteration 217/1000 | Loss: 0.00003083
Iteration 218/1000 | Loss: 0.00003083
Iteration 219/1000 | Loss: 0.00003083
Iteration 220/1000 | Loss: 0.00003083
Iteration 221/1000 | Loss: 0.00003083
Iteration 222/1000 | Loss: 0.00003083
Iteration 223/1000 | Loss: 0.00003083
Iteration 224/1000 | Loss: 0.00003083
Iteration 225/1000 | Loss: 0.00003082
Iteration 226/1000 | Loss: 0.00003082
Iteration 227/1000 | Loss: 0.00003082
Iteration 228/1000 | Loss: 0.00003082
Iteration 229/1000 | Loss: 0.00003082
Iteration 230/1000 | Loss: 0.00003082
Iteration 231/1000 | Loss: 0.00003082
Iteration 232/1000 | Loss: 0.00003082
Iteration 233/1000 | Loss: 0.00003082
Iteration 234/1000 | Loss: 0.00003082
Iteration 235/1000 | Loss: 0.00003082
Iteration 236/1000 | Loss: 0.00003081
Iteration 237/1000 | Loss: 0.00003081
Iteration 238/1000 | Loss: 0.00003081
Iteration 239/1000 | Loss: 0.00003081
Iteration 240/1000 | Loss: 0.00003081
Iteration 241/1000 | Loss: 0.00003081
Iteration 242/1000 | Loss: 0.00003081
Iteration 243/1000 | Loss: 0.00003081
Iteration 244/1000 | Loss: 0.00003081
Iteration 245/1000 | Loss: 0.00003081
Iteration 246/1000 | Loss: 0.00003081
Iteration 247/1000 | Loss: 0.00003081
Iteration 248/1000 | Loss: 0.00003081
Iteration 249/1000 | Loss: 0.00003081
Iteration 250/1000 | Loss: 0.00003081
Iteration 251/1000 | Loss: 0.00003081
Iteration 252/1000 | Loss: 0.00003081
Iteration 253/1000 | Loss: 0.00003081
Iteration 254/1000 | Loss: 0.00003081
Iteration 255/1000 | Loss: 0.00003081
Iteration 256/1000 | Loss: 0.00003081
Iteration 257/1000 | Loss: 0.00003081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [3.0811919714324176e-05, 3.0811919714324176e-05, 3.0811919714324176e-05, 3.0811919714324176e-05, 3.0811919714324176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0811919714324176e-05

Optimization complete. Final v2v error: 4.268768310546875 mm

Highest mean error: 6.8928542137146 mm for frame 39

Lowest mean error: 3.3276469707489014 mm for frame 85

Saving results

Total time: 165.3727147579193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409841
Iteration 2/25 | Loss: 0.00136137
Iteration 3/25 | Loss: 0.00127710
Iteration 4/25 | Loss: 0.00126335
Iteration 5/25 | Loss: 0.00125945
Iteration 6/25 | Loss: 0.00125844
Iteration 7/25 | Loss: 0.00125840
Iteration 8/25 | Loss: 0.00125840
Iteration 9/25 | Loss: 0.00125840
Iteration 10/25 | Loss: 0.00125840
Iteration 11/25 | Loss: 0.00125840
Iteration 12/25 | Loss: 0.00125840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012583989882841706, 0.0012583989882841706, 0.0012583989882841706, 0.0012583989882841706, 0.0012583989882841706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012583989882841706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99269140
Iteration 2/25 | Loss: 0.00082772
Iteration 3/25 | Loss: 0.00082772
Iteration 4/25 | Loss: 0.00082771
Iteration 5/25 | Loss: 0.00082771
Iteration 6/25 | Loss: 0.00082771
Iteration 7/25 | Loss: 0.00082771
Iteration 8/25 | Loss: 0.00082771
Iteration 9/25 | Loss: 0.00082771
Iteration 10/25 | Loss: 0.00082771
Iteration 11/25 | Loss: 0.00082771
Iteration 12/25 | Loss: 0.00082771
Iteration 13/25 | Loss: 0.00082771
Iteration 14/25 | Loss: 0.00082771
Iteration 15/25 | Loss: 0.00082771
Iteration 16/25 | Loss: 0.00082771
Iteration 17/25 | Loss: 0.00082771
Iteration 18/25 | Loss: 0.00082771
Iteration 19/25 | Loss: 0.00082771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008277108427137136, 0.0008277108427137136, 0.0008277108427137136, 0.0008277108427137136, 0.0008277108427137136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008277108427137136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082771
Iteration 2/1000 | Loss: 0.00003935
Iteration 3/1000 | Loss: 0.00002615
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00001956
Iteration 6/1000 | Loss: 0.00001857
Iteration 7/1000 | Loss: 0.00001795
Iteration 8/1000 | Loss: 0.00001753
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001674
Iteration 11/1000 | Loss: 0.00001653
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001586
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001538
Iteration 24/1000 | Loss: 0.00001537
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001531
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001525
Iteration 35/1000 | Loss: 0.00001525
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001523
Iteration 39/1000 | Loss: 0.00001523
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001520
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001517
Iteration 51/1000 | Loss: 0.00001517
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001515
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001513
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001512
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001512
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001511
Iteration 81/1000 | Loss: 0.00001511
Iteration 82/1000 | Loss: 0.00001510
Iteration 83/1000 | Loss: 0.00001510
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001509
Iteration 86/1000 | Loss: 0.00001509
Iteration 87/1000 | Loss: 0.00001509
Iteration 88/1000 | Loss: 0.00001509
Iteration 89/1000 | Loss: 0.00001509
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001509
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001508
Iteration 97/1000 | Loss: 0.00001508
Iteration 98/1000 | Loss: 0.00001507
Iteration 99/1000 | Loss: 0.00001507
Iteration 100/1000 | Loss: 0.00001507
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001506
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001506
Iteration 113/1000 | Loss: 0.00001506
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001503
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001503
Iteration 133/1000 | Loss: 0.00001503
Iteration 134/1000 | Loss: 0.00001503
Iteration 135/1000 | Loss: 0.00001503
Iteration 136/1000 | Loss: 0.00001503
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001503
Iteration 144/1000 | Loss: 0.00001503
Iteration 145/1000 | Loss: 0.00001503
Iteration 146/1000 | Loss: 0.00001503
Iteration 147/1000 | Loss: 0.00001503
Iteration 148/1000 | Loss: 0.00001503
Iteration 149/1000 | Loss: 0.00001503
Iteration 150/1000 | Loss: 0.00001503
Iteration 151/1000 | Loss: 0.00001503
Iteration 152/1000 | Loss: 0.00001503
Iteration 153/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5027582776383497e-05, 1.5027582776383497e-05, 1.5027582776383497e-05, 1.5027582776383497e-05, 1.5027582776383497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5027582776383497e-05

Optimization complete. Final v2v error: 3.3101747035980225 mm

Highest mean error: 4.284005641937256 mm for frame 57

Lowest mean error: 3.043386459350586 mm for frame 91

Saving results

Total time: 43.08631777763367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542129
Iteration 2/25 | Loss: 0.00134594
Iteration 3/25 | Loss: 0.00128218
Iteration 4/25 | Loss: 0.00127200
Iteration 5/25 | Loss: 0.00126777
Iteration 6/25 | Loss: 0.00126760
Iteration 7/25 | Loss: 0.00126760
Iteration 8/25 | Loss: 0.00126760
Iteration 9/25 | Loss: 0.00126760
Iteration 10/25 | Loss: 0.00126760
Iteration 11/25 | Loss: 0.00126760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012676019687205553, 0.0012676019687205553, 0.0012676019687205553, 0.0012676019687205553, 0.0012676019687205553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012676019687205553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22851610
Iteration 2/25 | Loss: 0.00085293
Iteration 3/25 | Loss: 0.00085292
Iteration 4/25 | Loss: 0.00085292
Iteration 5/25 | Loss: 0.00085292
Iteration 6/25 | Loss: 0.00085292
Iteration 7/25 | Loss: 0.00085292
Iteration 8/25 | Loss: 0.00085292
Iteration 9/25 | Loss: 0.00085292
Iteration 10/25 | Loss: 0.00085292
Iteration 11/25 | Loss: 0.00085292
Iteration 12/25 | Loss: 0.00085292
Iteration 13/25 | Loss: 0.00085292
Iteration 14/25 | Loss: 0.00085292
Iteration 15/25 | Loss: 0.00085292
Iteration 16/25 | Loss: 0.00085292
Iteration 17/25 | Loss: 0.00085292
Iteration 18/25 | Loss: 0.00085292
Iteration 19/25 | Loss: 0.00085292
Iteration 20/25 | Loss: 0.00085292
Iteration 21/25 | Loss: 0.00085292
Iteration 22/25 | Loss: 0.00085292
Iteration 23/25 | Loss: 0.00085292
Iteration 24/25 | Loss: 0.00085292
Iteration 25/25 | Loss: 0.00085292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085292
Iteration 2/1000 | Loss: 0.00002711
Iteration 3/1000 | Loss: 0.00001795
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001555
Iteration 6/1000 | Loss: 0.00001486
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001374
Iteration 10/1000 | Loss: 0.00001365
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001334
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001303
Iteration 22/1000 | Loss: 0.00001303
Iteration 23/1000 | Loss: 0.00001302
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001296
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001295
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001285
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001281
Iteration 45/1000 | Loss: 0.00001280
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001280
Iteration 48/1000 | Loss: 0.00001280
Iteration 49/1000 | Loss: 0.00001280
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001279
Iteration 52/1000 | Loss: 0.00001279
Iteration 53/1000 | Loss: 0.00001278
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001274
Iteration 66/1000 | Loss: 0.00001273
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001273
Iteration 70/1000 | Loss: 0.00001273
Iteration 71/1000 | Loss: 0.00001272
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001268
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001265
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001264
Iteration 86/1000 | Loss: 0.00001264
Iteration 87/1000 | Loss: 0.00001264
Iteration 88/1000 | Loss: 0.00001264
Iteration 89/1000 | Loss: 0.00001263
Iteration 90/1000 | Loss: 0.00001263
Iteration 91/1000 | Loss: 0.00001263
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001263
Iteration 94/1000 | Loss: 0.00001262
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001260
Iteration 100/1000 | Loss: 0.00001260
Iteration 101/1000 | Loss: 0.00001260
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001260
Iteration 106/1000 | Loss: 0.00001260
Iteration 107/1000 | Loss: 0.00001260
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001257
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001255
Iteration 128/1000 | Loss: 0.00001255
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001252
Iteration 137/1000 | Loss: 0.00001252
Iteration 138/1000 | Loss: 0.00001252
Iteration 139/1000 | Loss: 0.00001252
Iteration 140/1000 | Loss: 0.00001252
Iteration 141/1000 | Loss: 0.00001252
Iteration 142/1000 | Loss: 0.00001252
Iteration 143/1000 | Loss: 0.00001251
Iteration 144/1000 | Loss: 0.00001251
Iteration 145/1000 | Loss: 0.00001251
Iteration 146/1000 | Loss: 0.00001251
Iteration 147/1000 | Loss: 0.00001251
Iteration 148/1000 | Loss: 0.00001251
Iteration 149/1000 | Loss: 0.00001251
Iteration 150/1000 | Loss: 0.00001251
Iteration 151/1000 | Loss: 0.00001251
Iteration 152/1000 | Loss: 0.00001251
Iteration 153/1000 | Loss: 0.00001250
Iteration 154/1000 | Loss: 0.00001250
Iteration 155/1000 | Loss: 0.00001250
Iteration 156/1000 | Loss: 0.00001250
Iteration 157/1000 | Loss: 0.00001250
Iteration 158/1000 | Loss: 0.00001250
Iteration 159/1000 | Loss: 0.00001250
Iteration 160/1000 | Loss: 0.00001250
Iteration 161/1000 | Loss: 0.00001250
Iteration 162/1000 | Loss: 0.00001250
Iteration 163/1000 | Loss: 0.00001250
Iteration 164/1000 | Loss: 0.00001250
Iteration 165/1000 | Loss: 0.00001250
Iteration 166/1000 | Loss: 0.00001250
Iteration 167/1000 | Loss: 0.00001249
Iteration 168/1000 | Loss: 0.00001249
Iteration 169/1000 | Loss: 0.00001249
Iteration 170/1000 | Loss: 0.00001249
Iteration 171/1000 | Loss: 0.00001249
Iteration 172/1000 | Loss: 0.00001249
Iteration 173/1000 | Loss: 0.00001249
Iteration 174/1000 | Loss: 0.00001249
Iteration 175/1000 | Loss: 0.00001249
Iteration 176/1000 | Loss: 0.00001249
Iteration 177/1000 | Loss: 0.00001249
Iteration 178/1000 | Loss: 0.00001249
Iteration 179/1000 | Loss: 0.00001249
Iteration 180/1000 | Loss: 0.00001249
Iteration 181/1000 | Loss: 0.00001249
Iteration 182/1000 | Loss: 0.00001249
Iteration 183/1000 | Loss: 0.00001249
Iteration 184/1000 | Loss: 0.00001249
Iteration 185/1000 | Loss: 0.00001249
Iteration 186/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2488742868299596e-05, 1.2488742868299596e-05, 1.2488742868299596e-05, 1.2488742868299596e-05, 1.2488742868299596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2488742868299596e-05

Optimization complete. Final v2v error: 3.036205530166626 mm

Highest mean error: 3.315579891204834 mm for frame 96

Lowest mean error: 2.8459668159484863 mm for frame 21

Saving results

Total time: 41.64480018615723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00614000
Iteration 2/25 | Loss: 0.00178490
Iteration 3/25 | Loss: 0.00155613
Iteration 4/25 | Loss: 0.00152788
Iteration 5/25 | Loss: 0.00151704
Iteration 6/25 | Loss: 0.00151096
Iteration 7/25 | Loss: 0.00151012
Iteration 8/25 | Loss: 0.00150530
Iteration 9/25 | Loss: 0.00149718
Iteration 10/25 | Loss: 0.00149320
Iteration 11/25 | Loss: 0.00149477
Iteration 12/25 | Loss: 0.00149310
Iteration 13/25 | Loss: 0.00149121
Iteration 14/25 | Loss: 0.00148969
Iteration 15/25 | Loss: 0.00148876
Iteration 16/25 | Loss: 0.00148771
Iteration 17/25 | Loss: 0.00148736
Iteration 18/25 | Loss: 0.00148722
Iteration 19/25 | Loss: 0.00148683
Iteration 20/25 | Loss: 0.00148885
Iteration 21/25 | Loss: 0.00149547
Iteration 22/25 | Loss: 0.00148729
Iteration 23/25 | Loss: 0.00148133
Iteration 24/25 | Loss: 0.00147771
Iteration 25/25 | Loss: 0.00147702

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35036957
Iteration 2/25 | Loss: 0.00200569
Iteration 3/25 | Loss: 0.00200569
Iteration 4/25 | Loss: 0.00200569
Iteration 5/25 | Loss: 0.00200569
Iteration 6/25 | Loss: 0.00200569
Iteration 7/25 | Loss: 0.00200569
Iteration 8/25 | Loss: 0.00200569
Iteration 9/25 | Loss: 0.00200569
Iteration 10/25 | Loss: 0.00200569
Iteration 11/25 | Loss: 0.00200569
Iteration 12/25 | Loss: 0.00200569
Iteration 13/25 | Loss: 0.00200569
Iteration 14/25 | Loss: 0.00200569
Iteration 15/25 | Loss: 0.00200569
Iteration 16/25 | Loss: 0.00200569
Iteration 17/25 | Loss: 0.00200569
Iteration 18/25 | Loss: 0.00200569
Iteration 19/25 | Loss: 0.00200569
Iteration 20/25 | Loss: 0.00200569
Iteration 21/25 | Loss: 0.00200569
Iteration 22/25 | Loss: 0.00200569
Iteration 23/25 | Loss: 0.00200569
Iteration 24/25 | Loss: 0.00200569
Iteration 25/25 | Loss: 0.00200569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200569
Iteration 2/1000 | Loss: 0.00021570
Iteration 3/1000 | Loss: 0.00076566
Iteration 4/1000 | Loss: 0.00014464
Iteration 5/1000 | Loss: 0.00035085
Iteration 6/1000 | Loss: 0.00012397
Iteration 7/1000 | Loss: 0.00011541
Iteration 8/1000 | Loss: 0.00010907
Iteration 9/1000 | Loss: 0.00084214
Iteration 10/1000 | Loss: 0.00051216
Iteration 11/1000 | Loss: 0.00076992
Iteration 12/1000 | Loss: 0.00034314
Iteration 13/1000 | Loss: 0.00102652
Iteration 14/1000 | Loss: 0.00064604
Iteration 15/1000 | Loss: 0.00010796
Iteration 16/1000 | Loss: 0.00035311
Iteration 17/1000 | Loss: 0.00100447
Iteration 18/1000 | Loss: 0.00032748
Iteration 19/1000 | Loss: 0.00078194
Iteration 20/1000 | Loss: 0.00076612
Iteration 21/1000 | Loss: 0.00244750
Iteration 22/1000 | Loss: 0.00068122
Iteration 23/1000 | Loss: 0.00067520
Iteration 24/1000 | Loss: 0.00014727
Iteration 25/1000 | Loss: 0.00009978
Iteration 26/1000 | Loss: 0.00017243
Iteration 27/1000 | Loss: 0.00021269
Iteration 28/1000 | Loss: 0.00050061
Iteration 29/1000 | Loss: 0.00037154
Iteration 30/1000 | Loss: 0.00008810
Iteration 31/1000 | Loss: 0.00008141
Iteration 32/1000 | Loss: 0.00056227
Iteration 33/1000 | Loss: 0.00009132
Iteration 34/1000 | Loss: 0.00007764
Iteration 35/1000 | Loss: 0.00022616
Iteration 36/1000 | Loss: 0.00007334
Iteration 37/1000 | Loss: 0.00006840
Iteration 38/1000 | Loss: 0.00006506
Iteration 39/1000 | Loss: 0.00020158
Iteration 40/1000 | Loss: 0.00055156
Iteration 41/1000 | Loss: 0.00008704
Iteration 42/1000 | Loss: 0.00081096
Iteration 43/1000 | Loss: 0.00022712
Iteration 44/1000 | Loss: 0.00007440
Iteration 45/1000 | Loss: 0.00007718
Iteration 46/1000 | Loss: 0.00006792
Iteration 47/1000 | Loss: 0.00006352
Iteration 48/1000 | Loss: 0.00018206
Iteration 49/1000 | Loss: 0.00011199
Iteration 50/1000 | Loss: 0.00006998
Iteration 51/1000 | Loss: 0.00006581
Iteration 52/1000 | Loss: 0.00006267
Iteration 53/1000 | Loss: 0.00006055
Iteration 54/1000 | Loss: 0.00005868
Iteration 55/1000 | Loss: 0.00005725
Iteration 56/1000 | Loss: 0.00005630
Iteration 57/1000 | Loss: 0.00045680
Iteration 58/1000 | Loss: 0.00006841
Iteration 59/1000 | Loss: 0.00006059
Iteration 60/1000 | Loss: 0.00005846
Iteration 61/1000 | Loss: 0.00005557
Iteration 62/1000 | Loss: 0.00005425
Iteration 63/1000 | Loss: 0.00005341
Iteration 64/1000 | Loss: 0.00005276
Iteration 65/1000 | Loss: 0.00005203
Iteration 66/1000 | Loss: 0.00005129
Iteration 67/1000 | Loss: 0.00005087
Iteration 68/1000 | Loss: 0.00005064
Iteration 69/1000 | Loss: 0.00005062
Iteration 70/1000 | Loss: 0.00005056
Iteration 71/1000 | Loss: 0.00005041
Iteration 72/1000 | Loss: 0.00005037
Iteration 73/1000 | Loss: 0.00005036
Iteration 74/1000 | Loss: 0.00005035
Iteration 75/1000 | Loss: 0.00005035
Iteration 76/1000 | Loss: 0.00005034
Iteration 77/1000 | Loss: 0.00005028
Iteration 78/1000 | Loss: 0.00005027
Iteration 79/1000 | Loss: 0.00005022
Iteration 80/1000 | Loss: 0.00005020
Iteration 81/1000 | Loss: 0.00020869
Iteration 82/1000 | Loss: 0.00018019
Iteration 83/1000 | Loss: 0.00006375
Iteration 84/1000 | Loss: 0.00005745
Iteration 85/1000 | Loss: 0.00005371
Iteration 86/1000 | Loss: 0.00015477
Iteration 87/1000 | Loss: 0.00005731
Iteration 88/1000 | Loss: 0.00005262
Iteration 89/1000 | Loss: 0.00005168
Iteration 90/1000 | Loss: 0.00005089
Iteration 91/1000 | Loss: 0.00022172
Iteration 92/1000 | Loss: 0.00009345
Iteration 93/1000 | Loss: 0.00018688
Iteration 94/1000 | Loss: 0.00011082
Iteration 95/1000 | Loss: 0.00005083
Iteration 96/1000 | Loss: 0.00020092
Iteration 97/1000 | Loss: 0.00006837
Iteration 98/1000 | Loss: 0.00005204
Iteration 99/1000 | Loss: 0.00005071
Iteration 100/1000 | Loss: 0.00005042
Iteration 101/1000 | Loss: 0.00005037
Iteration 102/1000 | Loss: 0.00005037
Iteration 103/1000 | Loss: 0.00020951
Iteration 104/1000 | Loss: 0.00010330
Iteration 105/1000 | Loss: 0.00005474
Iteration 106/1000 | Loss: 0.00005274
Iteration 107/1000 | Loss: 0.00005159
Iteration 108/1000 | Loss: 0.00005113
Iteration 109/1000 | Loss: 0.00005099
Iteration 110/1000 | Loss: 0.00005069
Iteration 111/1000 | Loss: 0.00019798
Iteration 112/1000 | Loss: 0.00019977
Iteration 113/1000 | Loss: 0.00005786
Iteration 114/1000 | Loss: 0.00005430
Iteration 115/1000 | Loss: 0.00005189
Iteration 116/1000 | Loss: 0.00005259
Iteration 117/1000 | Loss: 0.00005199
Iteration 118/1000 | Loss: 0.00005134
Iteration 119/1000 | Loss: 0.00005056
Iteration 120/1000 | Loss: 0.00005039
Iteration 121/1000 | Loss: 0.00024423
Iteration 122/1000 | Loss: 0.00005461
Iteration 123/1000 | Loss: 0.00005150
Iteration 124/1000 | Loss: 0.00005051
Iteration 125/1000 | Loss: 0.00005012
Iteration 126/1000 | Loss: 0.00004994
Iteration 127/1000 | Loss: 0.00004977
Iteration 128/1000 | Loss: 0.00004976
Iteration 129/1000 | Loss: 0.00004960
Iteration 130/1000 | Loss: 0.00004958
Iteration 131/1000 | Loss: 0.00004958
Iteration 132/1000 | Loss: 0.00004957
Iteration 133/1000 | Loss: 0.00004957
Iteration 134/1000 | Loss: 0.00004956
Iteration 135/1000 | Loss: 0.00004956
Iteration 136/1000 | Loss: 0.00004956
Iteration 137/1000 | Loss: 0.00004956
Iteration 138/1000 | Loss: 0.00004956
Iteration 139/1000 | Loss: 0.00004956
Iteration 140/1000 | Loss: 0.00004955
Iteration 141/1000 | Loss: 0.00004955
Iteration 142/1000 | Loss: 0.00004955
Iteration 143/1000 | Loss: 0.00004955
Iteration 144/1000 | Loss: 0.00004955
Iteration 145/1000 | Loss: 0.00004955
Iteration 146/1000 | Loss: 0.00004955
Iteration 147/1000 | Loss: 0.00004955
Iteration 148/1000 | Loss: 0.00004955
Iteration 149/1000 | Loss: 0.00004955
Iteration 150/1000 | Loss: 0.00004955
Iteration 151/1000 | Loss: 0.00004954
Iteration 152/1000 | Loss: 0.00004954
Iteration 153/1000 | Loss: 0.00004954
Iteration 154/1000 | Loss: 0.00004954
Iteration 155/1000 | Loss: 0.00004954
Iteration 156/1000 | Loss: 0.00004953
Iteration 157/1000 | Loss: 0.00004953
Iteration 158/1000 | Loss: 0.00004953
Iteration 159/1000 | Loss: 0.00004953
Iteration 160/1000 | Loss: 0.00004953
Iteration 161/1000 | Loss: 0.00004953
Iteration 162/1000 | Loss: 0.00004953
Iteration 163/1000 | Loss: 0.00004953
Iteration 164/1000 | Loss: 0.00004953
Iteration 165/1000 | Loss: 0.00004953
Iteration 166/1000 | Loss: 0.00004953
Iteration 167/1000 | Loss: 0.00004952
Iteration 168/1000 | Loss: 0.00004952
Iteration 169/1000 | Loss: 0.00004952
Iteration 170/1000 | Loss: 0.00004952
Iteration 171/1000 | Loss: 0.00004952
Iteration 172/1000 | Loss: 0.00004952
Iteration 173/1000 | Loss: 0.00004952
Iteration 174/1000 | Loss: 0.00004952
Iteration 175/1000 | Loss: 0.00004952
Iteration 176/1000 | Loss: 0.00004952
Iteration 177/1000 | Loss: 0.00004952
Iteration 178/1000 | Loss: 0.00004952
Iteration 179/1000 | Loss: 0.00004952
Iteration 180/1000 | Loss: 0.00004952
Iteration 181/1000 | Loss: 0.00004952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [4.952463859808631e-05, 4.952463859808631e-05, 4.952463859808631e-05, 4.952463859808631e-05, 4.952463859808631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.952463859808631e-05

Optimization complete. Final v2v error: 4.199251651763916 mm

Highest mean error: 11.2272310256958 mm for frame 20

Lowest mean error: 2.8409392833709717 mm for frame 228

Saving results

Total time: 241.96936440467834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469415
Iteration 2/25 | Loss: 0.00137719
Iteration 3/25 | Loss: 0.00130608
Iteration 4/25 | Loss: 0.00130084
Iteration 5/25 | Loss: 0.00129944
Iteration 6/25 | Loss: 0.00129943
Iteration 7/25 | Loss: 0.00129943
Iteration 8/25 | Loss: 0.00129943
Iteration 9/25 | Loss: 0.00129943
Iteration 10/25 | Loss: 0.00129943
Iteration 11/25 | Loss: 0.00129943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012994250282645226, 0.0012994250282645226, 0.0012994250282645226, 0.0012994250282645226, 0.0012994250282645226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012994250282645226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39615786
Iteration 2/25 | Loss: 0.00096193
Iteration 3/25 | Loss: 0.00096191
Iteration 4/25 | Loss: 0.00096191
Iteration 5/25 | Loss: 0.00096191
Iteration 6/25 | Loss: 0.00096191
Iteration 7/25 | Loss: 0.00096191
Iteration 8/25 | Loss: 0.00096191
Iteration 9/25 | Loss: 0.00096191
Iteration 10/25 | Loss: 0.00096191
Iteration 11/25 | Loss: 0.00096191
Iteration 12/25 | Loss: 0.00096191
Iteration 13/25 | Loss: 0.00096191
Iteration 14/25 | Loss: 0.00096191
Iteration 15/25 | Loss: 0.00096191
Iteration 16/25 | Loss: 0.00096191
Iteration 17/25 | Loss: 0.00096191
Iteration 18/25 | Loss: 0.00096191
Iteration 19/25 | Loss: 0.00096191
Iteration 20/25 | Loss: 0.00096191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009619093616493046, 0.0009619093616493046, 0.0009619093616493046, 0.0009619093616493046, 0.0009619093616493046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009619093616493046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096191
Iteration 2/1000 | Loss: 0.00002643
Iteration 3/1000 | Loss: 0.00001739
Iteration 4/1000 | Loss: 0.00001517
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001300
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001271
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001265
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001252
Iteration 25/1000 | Loss: 0.00001252
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001245
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001233
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001229
Iteration 63/1000 | Loss: 0.00001229
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001227
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001224
Iteration 78/1000 | Loss: 0.00001224
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001222
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001220
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001220
Iteration 93/1000 | Loss: 0.00001220
Iteration 94/1000 | Loss: 0.00001220
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001218
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001217
Iteration 100/1000 | Loss: 0.00001217
Iteration 101/1000 | Loss: 0.00001217
Iteration 102/1000 | Loss: 0.00001217
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001217
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001216
Iteration 112/1000 | Loss: 0.00001216
Iteration 113/1000 | Loss: 0.00001216
Iteration 114/1000 | Loss: 0.00001216
Iteration 115/1000 | Loss: 0.00001216
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001215
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001214
Iteration 123/1000 | Loss: 0.00001214
Iteration 124/1000 | Loss: 0.00001214
Iteration 125/1000 | Loss: 0.00001214
Iteration 126/1000 | Loss: 0.00001214
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001213
Iteration 130/1000 | Loss: 0.00001213
Iteration 131/1000 | Loss: 0.00001213
Iteration 132/1000 | Loss: 0.00001213
Iteration 133/1000 | Loss: 0.00001213
Iteration 134/1000 | Loss: 0.00001213
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001210
Iteration 145/1000 | Loss: 0.00001210
Iteration 146/1000 | Loss: 0.00001209
Iteration 147/1000 | Loss: 0.00001209
Iteration 148/1000 | Loss: 0.00001208
Iteration 149/1000 | Loss: 0.00001208
Iteration 150/1000 | Loss: 0.00001208
Iteration 151/1000 | Loss: 0.00001207
Iteration 152/1000 | Loss: 0.00001207
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Iteration 159/1000 | Loss: 0.00001204
Iteration 160/1000 | Loss: 0.00001204
Iteration 161/1000 | Loss: 0.00001204
Iteration 162/1000 | Loss: 0.00001204
Iteration 163/1000 | Loss: 0.00001204
Iteration 164/1000 | Loss: 0.00001204
Iteration 165/1000 | Loss: 0.00001204
Iteration 166/1000 | Loss: 0.00001204
Iteration 167/1000 | Loss: 0.00001204
Iteration 168/1000 | Loss: 0.00001203
Iteration 169/1000 | Loss: 0.00001203
Iteration 170/1000 | Loss: 0.00001203
Iteration 171/1000 | Loss: 0.00001203
Iteration 172/1000 | Loss: 0.00001202
Iteration 173/1000 | Loss: 0.00001202
Iteration 174/1000 | Loss: 0.00001202
Iteration 175/1000 | Loss: 0.00001202
Iteration 176/1000 | Loss: 0.00001202
Iteration 177/1000 | Loss: 0.00001202
Iteration 178/1000 | Loss: 0.00001202
Iteration 179/1000 | Loss: 0.00001202
Iteration 180/1000 | Loss: 0.00001202
Iteration 181/1000 | Loss: 0.00001202
Iteration 182/1000 | Loss: 0.00001202
Iteration 183/1000 | Loss: 0.00001202
Iteration 184/1000 | Loss: 0.00001202
Iteration 185/1000 | Loss: 0.00001201
Iteration 186/1000 | Loss: 0.00001201
Iteration 187/1000 | Loss: 0.00001201
Iteration 188/1000 | Loss: 0.00001201
Iteration 189/1000 | Loss: 0.00001201
Iteration 190/1000 | Loss: 0.00001201
Iteration 191/1000 | Loss: 0.00001201
Iteration 192/1000 | Loss: 0.00001201
Iteration 193/1000 | Loss: 0.00001201
Iteration 194/1000 | Loss: 0.00001200
Iteration 195/1000 | Loss: 0.00001200
Iteration 196/1000 | Loss: 0.00001200
Iteration 197/1000 | Loss: 0.00001200
Iteration 198/1000 | Loss: 0.00001200
Iteration 199/1000 | Loss: 0.00001200
Iteration 200/1000 | Loss: 0.00001200
Iteration 201/1000 | Loss: 0.00001200
Iteration 202/1000 | Loss: 0.00001200
Iteration 203/1000 | Loss: 0.00001200
Iteration 204/1000 | Loss: 0.00001199
Iteration 205/1000 | Loss: 0.00001199
Iteration 206/1000 | Loss: 0.00001199
Iteration 207/1000 | Loss: 0.00001199
Iteration 208/1000 | Loss: 0.00001199
Iteration 209/1000 | Loss: 0.00001199
Iteration 210/1000 | Loss: 0.00001199
Iteration 211/1000 | Loss: 0.00001199
Iteration 212/1000 | Loss: 0.00001199
Iteration 213/1000 | Loss: 0.00001199
Iteration 214/1000 | Loss: 0.00001199
Iteration 215/1000 | Loss: 0.00001199
Iteration 216/1000 | Loss: 0.00001199
Iteration 217/1000 | Loss: 0.00001199
Iteration 218/1000 | Loss: 0.00001199
Iteration 219/1000 | Loss: 0.00001199
Iteration 220/1000 | Loss: 0.00001199
Iteration 221/1000 | Loss: 0.00001199
Iteration 222/1000 | Loss: 0.00001199
Iteration 223/1000 | Loss: 0.00001199
Iteration 224/1000 | Loss: 0.00001199
Iteration 225/1000 | Loss: 0.00001198
Iteration 226/1000 | Loss: 0.00001198
Iteration 227/1000 | Loss: 0.00001198
Iteration 228/1000 | Loss: 0.00001198
Iteration 229/1000 | Loss: 0.00001198
Iteration 230/1000 | Loss: 0.00001198
Iteration 231/1000 | Loss: 0.00001198
Iteration 232/1000 | Loss: 0.00001197
Iteration 233/1000 | Loss: 0.00001197
Iteration 234/1000 | Loss: 0.00001197
Iteration 235/1000 | Loss: 0.00001197
Iteration 236/1000 | Loss: 0.00001197
Iteration 237/1000 | Loss: 0.00001197
Iteration 238/1000 | Loss: 0.00001197
Iteration 239/1000 | Loss: 0.00001197
Iteration 240/1000 | Loss: 0.00001196
Iteration 241/1000 | Loss: 0.00001196
Iteration 242/1000 | Loss: 0.00001196
Iteration 243/1000 | Loss: 0.00001196
Iteration 244/1000 | Loss: 0.00001196
Iteration 245/1000 | Loss: 0.00001196
Iteration 246/1000 | Loss: 0.00001196
Iteration 247/1000 | Loss: 0.00001196
Iteration 248/1000 | Loss: 0.00001196
Iteration 249/1000 | Loss: 0.00001196
Iteration 250/1000 | Loss: 0.00001196
Iteration 251/1000 | Loss: 0.00001195
Iteration 252/1000 | Loss: 0.00001195
Iteration 253/1000 | Loss: 0.00001195
Iteration 254/1000 | Loss: 0.00001195
Iteration 255/1000 | Loss: 0.00001195
Iteration 256/1000 | Loss: 0.00001194
Iteration 257/1000 | Loss: 0.00001194
Iteration 258/1000 | Loss: 0.00001194
Iteration 259/1000 | Loss: 0.00001194
Iteration 260/1000 | Loss: 0.00001194
Iteration 261/1000 | Loss: 0.00001193
Iteration 262/1000 | Loss: 0.00001193
Iteration 263/1000 | Loss: 0.00001192
Iteration 264/1000 | Loss: 0.00001192
Iteration 265/1000 | Loss: 0.00001192
Iteration 266/1000 | Loss: 0.00001192
Iteration 267/1000 | Loss: 0.00001192
Iteration 268/1000 | Loss: 0.00001192
Iteration 269/1000 | Loss: 0.00001191
Iteration 270/1000 | Loss: 0.00001191
Iteration 271/1000 | Loss: 0.00001191
Iteration 272/1000 | Loss: 0.00001191
Iteration 273/1000 | Loss: 0.00001190
Iteration 274/1000 | Loss: 0.00001190
Iteration 275/1000 | Loss: 0.00001190
Iteration 276/1000 | Loss: 0.00001190
Iteration 277/1000 | Loss: 0.00001190
Iteration 278/1000 | Loss: 0.00001190
Iteration 279/1000 | Loss: 0.00001190
Iteration 280/1000 | Loss: 0.00001190
Iteration 281/1000 | Loss: 0.00001190
Iteration 282/1000 | Loss: 0.00001190
Iteration 283/1000 | Loss: 0.00001190
Iteration 284/1000 | Loss: 0.00001190
Iteration 285/1000 | Loss: 0.00001190
Iteration 286/1000 | Loss: 0.00001189
Iteration 287/1000 | Loss: 0.00001189
Iteration 288/1000 | Loss: 0.00001189
Iteration 289/1000 | Loss: 0.00001189
Iteration 290/1000 | Loss: 0.00001189
Iteration 291/1000 | Loss: 0.00001189
Iteration 292/1000 | Loss: 0.00001189
Iteration 293/1000 | Loss: 0.00001189
Iteration 294/1000 | Loss: 0.00001189
Iteration 295/1000 | Loss: 0.00001189
Iteration 296/1000 | Loss: 0.00001189
Iteration 297/1000 | Loss: 0.00001189
Iteration 298/1000 | Loss: 0.00001189
Iteration 299/1000 | Loss: 0.00001189
Iteration 300/1000 | Loss: 0.00001189
Iteration 301/1000 | Loss: 0.00001189
Iteration 302/1000 | Loss: 0.00001189
Iteration 303/1000 | Loss: 0.00001189
Iteration 304/1000 | Loss: 0.00001189
Iteration 305/1000 | Loss: 0.00001189
Iteration 306/1000 | Loss: 0.00001189
Iteration 307/1000 | Loss: 0.00001188
Iteration 308/1000 | Loss: 0.00001188
Iteration 309/1000 | Loss: 0.00001188
Iteration 310/1000 | Loss: 0.00001188
Iteration 311/1000 | Loss: 0.00001188
Iteration 312/1000 | Loss: 0.00001188
Iteration 313/1000 | Loss: 0.00001188
Iteration 314/1000 | Loss: 0.00001188
Iteration 315/1000 | Loss: 0.00001188
Iteration 316/1000 | Loss: 0.00001188
Iteration 317/1000 | Loss: 0.00001188
Iteration 318/1000 | Loss: 0.00001188
Iteration 319/1000 | Loss: 0.00001188
Iteration 320/1000 | Loss: 0.00001188
Iteration 321/1000 | Loss: 0.00001188
Iteration 322/1000 | Loss: 0.00001188
Iteration 323/1000 | Loss: 0.00001188
Iteration 324/1000 | Loss: 0.00001188
Iteration 325/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [1.1881044883921277e-05, 1.1881044883921277e-05, 1.1881044883921277e-05, 1.1881044883921277e-05, 1.1881044883921277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1881044883921277e-05

Optimization complete. Final v2v error: 2.906769037246704 mm

Highest mean error: 3.201192855834961 mm for frame 62

Lowest mean error: 2.7700355052948 mm for frame 26

Saving results

Total time: 45.41033172607422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025842
Iteration 2/25 | Loss: 0.00204344
Iteration 3/25 | Loss: 0.00209379
Iteration 4/25 | Loss: 0.00170351
Iteration 5/25 | Loss: 0.00149692
Iteration 6/25 | Loss: 0.00141708
Iteration 7/25 | Loss: 0.00141246
Iteration 8/25 | Loss: 0.00141182
Iteration 9/25 | Loss: 0.00138293
Iteration 10/25 | Loss: 0.00138142
Iteration 11/25 | Loss: 0.00138008
Iteration 12/25 | Loss: 0.00137690
Iteration 13/25 | Loss: 0.00137617
Iteration 14/25 | Loss: 0.00139606
Iteration 15/25 | Loss: 0.00136959
Iteration 16/25 | Loss: 0.00136809
Iteration 17/25 | Loss: 0.00136767
Iteration 18/25 | Loss: 0.00136739
Iteration 19/25 | Loss: 0.00136732
Iteration 20/25 | Loss: 0.00136732
Iteration 21/25 | Loss: 0.00136731
Iteration 22/25 | Loss: 0.00136731
Iteration 23/25 | Loss: 0.00136730
Iteration 24/25 | Loss: 0.00136730
Iteration 25/25 | Loss: 0.00136730

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64981210
Iteration 2/25 | Loss: 0.00126050
Iteration 3/25 | Loss: 0.00116098
Iteration 4/25 | Loss: 0.00116098
Iteration 5/25 | Loss: 0.00116098
Iteration 6/25 | Loss: 0.00116098
Iteration 7/25 | Loss: 0.00116098
Iteration 8/25 | Loss: 0.00116098
Iteration 9/25 | Loss: 0.00116098
Iteration 10/25 | Loss: 0.00116097
Iteration 11/25 | Loss: 0.00116097
Iteration 12/25 | Loss: 0.00116097
Iteration 13/25 | Loss: 0.00116097
Iteration 14/25 | Loss: 0.00116097
Iteration 15/25 | Loss: 0.00116097
Iteration 16/25 | Loss: 0.00116097
Iteration 17/25 | Loss: 0.00116097
Iteration 18/25 | Loss: 0.00116097
Iteration 19/25 | Loss: 0.00116097
Iteration 20/25 | Loss: 0.00116097
Iteration 21/25 | Loss: 0.00116097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011609737994149327, 0.0011609737994149327, 0.0011609737994149327, 0.0011609737994149327, 0.0011609737994149327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011609737994149327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116097
Iteration 2/1000 | Loss: 0.00010010
Iteration 3/1000 | Loss: 0.00006104
Iteration 4/1000 | Loss: 0.00006396
Iteration 5/1000 | Loss: 0.00005994
Iteration 6/1000 | Loss: 0.00002735
Iteration 7/1000 | Loss: 0.00002670
Iteration 8/1000 | Loss: 0.00007217
Iteration 9/1000 | Loss: 0.00009488
Iteration 10/1000 | Loss: 0.00002555
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00007476
Iteration 14/1000 | Loss: 0.00002424
Iteration 15/1000 | Loss: 0.00002403
Iteration 16/1000 | Loss: 0.00082612
Iteration 17/1000 | Loss: 0.00042241
Iteration 18/1000 | Loss: 0.00002865
Iteration 19/1000 | Loss: 0.00006470
Iteration 20/1000 | Loss: 0.00003124
Iteration 21/1000 | Loss: 0.00004977
Iteration 22/1000 | Loss: 0.00002276
Iteration 23/1000 | Loss: 0.00004723
Iteration 24/1000 | Loss: 0.00002066
Iteration 25/1000 | Loss: 0.00002025
Iteration 26/1000 | Loss: 0.00001985
Iteration 27/1000 | Loss: 0.00008133
Iteration 28/1000 | Loss: 0.00002631
Iteration 29/1000 | Loss: 0.00002607
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00007032
Iteration 33/1000 | Loss: 0.00003639
Iteration 34/1000 | Loss: 0.00005727
Iteration 35/1000 | Loss: 0.00001900
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001894
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001893
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001891
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001891
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001888
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001888
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001886
Iteration 67/1000 | Loss: 0.00001886
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001884
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001883
Iteration 76/1000 | Loss: 0.00001883
Iteration 77/1000 | Loss: 0.00001883
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001883
Iteration 81/1000 | Loss: 0.00001882
Iteration 82/1000 | Loss: 0.00001882
Iteration 83/1000 | Loss: 0.00001882
Iteration 84/1000 | Loss: 0.00001882
Iteration 85/1000 | Loss: 0.00001882
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001882
Iteration 89/1000 | Loss: 0.00001882
Iteration 90/1000 | Loss: 0.00001882
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001880
Iteration 98/1000 | Loss: 0.00001880
Iteration 99/1000 | Loss: 0.00001880
Iteration 100/1000 | Loss: 0.00001880
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001880
Iteration 107/1000 | Loss: 0.00001880
Iteration 108/1000 | Loss: 0.00001879
Iteration 109/1000 | Loss: 0.00001879
Iteration 110/1000 | Loss: 0.00001879
Iteration 111/1000 | Loss: 0.00001879
Iteration 112/1000 | Loss: 0.00001879
Iteration 113/1000 | Loss: 0.00001878
Iteration 114/1000 | Loss: 0.00001878
Iteration 115/1000 | Loss: 0.00001878
Iteration 116/1000 | Loss: 0.00001878
Iteration 117/1000 | Loss: 0.00001878
Iteration 118/1000 | Loss: 0.00001877
Iteration 119/1000 | Loss: 0.00001877
Iteration 120/1000 | Loss: 0.00001877
Iteration 121/1000 | Loss: 0.00001877
Iteration 122/1000 | Loss: 0.00001877
Iteration 123/1000 | Loss: 0.00001877
Iteration 124/1000 | Loss: 0.00001877
Iteration 125/1000 | Loss: 0.00001877
Iteration 126/1000 | Loss: 0.00001877
Iteration 127/1000 | Loss: 0.00001876
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001876
Iteration 130/1000 | Loss: 0.00001876
Iteration 131/1000 | Loss: 0.00001876
Iteration 132/1000 | Loss: 0.00001876
Iteration 133/1000 | Loss: 0.00001875
Iteration 134/1000 | Loss: 0.00001875
Iteration 135/1000 | Loss: 0.00001875
Iteration 136/1000 | Loss: 0.00001875
Iteration 137/1000 | Loss: 0.00001875
Iteration 138/1000 | Loss: 0.00001875
Iteration 139/1000 | Loss: 0.00001875
Iteration 140/1000 | Loss: 0.00001875
Iteration 141/1000 | Loss: 0.00001875
Iteration 142/1000 | Loss: 0.00001874
Iteration 143/1000 | Loss: 0.00001874
Iteration 144/1000 | Loss: 0.00001874
Iteration 145/1000 | Loss: 0.00001874
Iteration 146/1000 | Loss: 0.00001874
Iteration 147/1000 | Loss: 0.00001874
Iteration 148/1000 | Loss: 0.00001874
Iteration 149/1000 | Loss: 0.00001874
Iteration 150/1000 | Loss: 0.00001874
Iteration 151/1000 | Loss: 0.00001874
Iteration 152/1000 | Loss: 0.00001874
Iteration 153/1000 | Loss: 0.00001874
Iteration 154/1000 | Loss: 0.00001874
Iteration 155/1000 | Loss: 0.00001874
Iteration 156/1000 | Loss: 0.00001874
Iteration 157/1000 | Loss: 0.00001874
Iteration 158/1000 | Loss: 0.00001874
Iteration 159/1000 | Loss: 0.00001874
Iteration 160/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.8742914107860997e-05, 1.8742914107860997e-05, 1.8742914107860997e-05, 1.8742914107860997e-05, 1.8742914107860997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8742914107860997e-05

Optimization complete. Final v2v error: 3.646914482116699 mm

Highest mean error: 4.8358612060546875 mm for frame 72

Lowest mean error: 3.238619804382324 mm for frame 201

Saving results

Total time: 102.7795147895813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00353628
Iteration 2/25 | Loss: 0.00141365
Iteration 3/25 | Loss: 0.00127929
Iteration 4/25 | Loss: 0.00126395
Iteration 5/25 | Loss: 0.00126030
Iteration 6/25 | Loss: 0.00126030
Iteration 7/25 | Loss: 0.00126030
Iteration 8/25 | Loss: 0.00126030
Iteration 9/25 | Loss: 0.00126030
Iteration 10/25 | Loss: 0.00126030
Iteration 11/25 | Loss: 0.00126030
Iteration 12/25 | Loss: 0.00126030
Iteration 13/25 | Loss: 0.00126030
Iteration 14/25 | Loss: 0.00126030
Iteration 15/25 | Loss: 0.00126030
Iteration 16/25 | Loss: 0.00126030
Iteration 17/25 | Loss: 0.00126030
Iteration 18/25 | Loss: 0.00126030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012603015638887882, 0.0012603015638887882, 0.0012603015638887882, 0.0012603015638887882, 0.0012603015638887882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012603015638887882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37073088
Iteration 2/25 | Loss: 0.00084927
Iteration 3/25 | Loss: 0.00084927
Iteration 4/25 | Loss: 0.00084927
Iteration 5/25 | Loss: 0.00084927
Iteration 6/25 | Loss: 0.00084927
Iteration 7/25 | Loss: 0.00084926
Iteration 8/25 | Loss: 0.00084926
Iteration 9/25 | Loss: 0.00084926
Iteration 10/25 | Loss: 0.00084926
Iteration 11/25 | Loss: 0.00084926
Iteration 12/25 | Loss: 0.00084926
Iteration 13/25 | Loss: 0.00084926
Iteration 14/25 | Loss: 0.00084926
Iteration 15/25 | Loss: 0.00084926
Iteration 16/25 | Loss: 0.00084926
Iteration 17/25 | Loss: 0.00084926
Iteration 18/25 | Loss: 0.00084926
Iteration 19/25 | Loss: 0.00084926
Iteration 20/25 | Loss: 0.00084926
Iteration 21/25 | Loss: 0.00084926
Iteration 22/25 | Loss: 0.00084926
Iteration 23/25 | Loss: 0.00084926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008492632186971605, 0.0008492632186971605, 0.0008492632186971605, 0.0008492632186971605, 0.0008492632186971605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008492632186971605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084926
Iteration 2/1000 | Loss: 0.00003826
Iteration 3/1000 | Loss: 0.00002618
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00001987
Iteration 6/1000 | Loss: 0.00001866
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001688
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001635
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001608
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001598
Iteration 20/1000 | Loss: 0.00001597
Iteration 21/1000 | Loss: 0.00001597
Iteration 22/1000 | Loss: 0.00001597
Iteration 23/1000 | Loss: 0.00001593
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001583
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001572
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001563
Iteration 40/1000 | Loss: 0.00001562
Iteration 41/1000 | Loss: 0.00001562
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001532
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001529
Iteration 64/1000 | Loss: 0.00001529
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001526
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001525
Iteration 84/1000 | Loss: 0.00001525
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.5243733287206851e-05, 1.5243733287206851e-05, 1.5243733287206851e-05, 1.5243733287206851e-05, 1.5243733287206851e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5243733287206851e-05

Optimization complete. Final v2v error: 3.356304883956909 mm

Highest mean error: 3.9499099254608154 mm for frame 21

Lowest mean error: 3.0556538105010986 mm for frame 246

Saving results

Total time: 44.1203191280365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417770
Iteration 2/25 | Loss: 0.00156337
Iteration 3/25 | Loss: 0.00133736
Iteration 4/25 | Loss: 0.00130882
Iteration 5/25 | Loss: 0.00130487
Iteration 6/25 | Loss: 0.00130405
Iteration 7/25 | Loss: 0.00130405
Iteration 8/25 | Loss: 0.00130405
Iteration 9/25 | Loss: 0.00130405
Iteration 10/25 | Loss: 0.00130405
Iteration 11/25 | Loss: 0.00130405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001304048579186201, 0.001304048579186201, 0.001304048579186201, 0.001304048579186201, 0.001304048579186201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001304048579186201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39942467
Iteration 2/25 | Loss: 0.00087814
Iteration 3/25 | Loss: 0.00087814
Iteration 4/25 | Loss: 0.00087814
Iteration 5/25 | Loss: 0.00087814
Iteration 6/25 | Loss: 0.00087814
Iteration 7/25 | Loss: 0.00087814
Iteration 8/25 | Loss: 0.00087814
Iteration 9/25 | Loss: 0.00087814
Iteration 10/25 | Loss: 0.00087814
Iteration 11/25 | Loss: 0.00087814
Iteration 12/25 | Loss: 0.00087814
Iteration 13/25 | Loss: 0.00087814
Iteration 14/25 | Loss: 0.00087814
Iteration 15/25 | Loss: 0.00087814
Iteration 16/25 | Loss: 0.00087814
Iteration 17/25 | Loss: 0.00087814
Iteration 18/25 | Loss: 0.00087814
Iteration 19/25 | Loss: 0.00087814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008781368378549814, 0.0008781368378549814, 0.0008781368378549814, 0.0008781368378549814, 0.0008781368378549814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008781368378549814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087814
Iteration 2/1000 | Loss: 0.00003609
Iteration 3/1000 | Loss: 0.00002183
Iteration 4/1000 | Loss: 0.00001908
Iteration 5/1000 | Loss: 0.00001777
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001529
Iteration 10/1000 | Loss: 0.00001528
Iteration 11/1000 | Loss: 0.00001499
Iteration 12/1000 | Loss: 0.00001472
Iteration 13/1000 | Loss: 0.00001466
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001453
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001449
Iteration 19/1000 | Loss: 0.00001443
Iteration 20/1000 | Loss: 0.00001438
Iteration 21/1000 | Loss: 0.00001434
Iteration 22/1000 | Loss: 0.00001433
Iteration 23/1000 | Loss: 0.00001433
Iteration 24/1000 | Loss: 0.00001432
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001427
Iteration 27/1000 | Loss: 0.00001426
Iteration 28/1000 | Loss: 0.00001426
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001420
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001416
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001416
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001416
Iteration 43/1000 | Loss: 0.00001416
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001415
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001413
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001411
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001409
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001398
Iteration 79/1000 | Loss: 0.00001398
Iteration 80/1000 | Loss: 0.00001398
Iteration 81/1000 | Loss: 0.00001397
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001397
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001396
Iteration 86/1000 | Loss: 0.00001396
Iteration 87/1000 | Loss: 0.00001395
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001390
Iteration 115/1000 | Loss: 0.00001390
Iteration 116/1000 | Loss: 0.00001390
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001389
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001384
Iteration 140/1000 | Loss: 0.00001384
Iteration 141/1000 | Loss: 0.00001384
Iteration 142/1000 | Loss: 0.00001383
Iteration 143/1000 | Loss: 0.00001383
Iteration 144/1000 | Loss: 0.00001383
Iteration 145/1000 | Loss: 0.00001383
Iteration 146/1000 | Loss: 0.00001382
Iteration 147/1000 | Loss: 0.00001382
Iteration 148/1000 | Loss: 0.00001382
Iteration 149/1000 | Loss: 0.00001382
Iteration 150/1000 | Loss: 0.00001382
Iteration 151/1000 | Loss: 0.00001382
Iteration 152/1000 | Loss: 0.00001382
Iteration 153/1000 | Loss: 0.00001382
Iteration 154/1000 | Loss: 0.00001382
Iteration 155/1000 | Loss: 0.00001382
Iteration 156/1000 | Loss: 0.00001382
Iteration 157/1000 | Loss: 0.00001382
Iteration 158/1000 | Loss: 0.00001382
Iteration 159/1000 | Loss: 0.00001381
Iteration 160/1000 | Loss: 0.00001381
Iteration 161/1000 | Loss: 0.00001381
Iteration 162/1000 | Loss: 0.00001380
Iteration 163/1000 | Loss: 0.00001380
Iteration 164/1000 | Loss: 0.00001380
Iteration 165/1000 | Loss: 0.00001380
Iteration 166/1000 | Loss: 0.00001380
Iteration 167/1000 | Loss: 0.00001380
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001379
Iteration 170/1000 | Loss: 0.00001379
Iteration 171/1000 | Loss: 0.00001379
Iteration 172/1000 | Loss: 0.00001379
Iteration 173/1000 | Loss: 0.00001379
Iteration 174/1000 | Loss: 0.00001379
Iteration 175/1000 | Loss: 0.00001379
Iteration 176/1000 | Loss: 0.00001379
Iteration 177/1000 | Loss: 0.00001379
Iteration 178/1000 | Loss: 0.00001379
Iteration 179/1000 | Loss: 0.00001379
Iteration 180/1000 | Loss: 0.00001379
Iteration 181/1000 | Loss: 0.00001379
Iteration 182/1000 | Loss: 0.00001379
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001378
Iteration 189/1000 | Loss: 0.00001378
Iteration 190/1000 | Loss: 0.00001378
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001377
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001377
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001376
Iteration 207/1000 | Loss: 0.00001376
Iteration 208/1000 | Loss: 0.00001376
Iteration 209/1000 | Loss: 0.00001376
Iteration 210/1000 | Loss: 0.00001376
Iteration 211/1000 | Loss: 0.00001376
Iteration 212/1000 | Loss: 0.00001376
Iteration 213/1000 | Loss: 0.00001376
Iteration 214/1000 | Loss: 0.00001376
Iteration 215/1000 | Loss: 0.00001376
Iteration 216/1000 | Loss: 0.00001376
Iteration 217/1000 | Loss: 0.00001376
Iteration 218/1000 | Loss: 0.00001376
Iteration 219/1000 | Loss: 0.00001376
Iteration 220/1000 | Loss: 0.00001376
Iteration 221/1000 | Loss: 0.00001376
Iteration 222/1000 | Loss: 0.00001376
Iteration 223/1000 | Loss: 0.00001376
Iteration 224/1000 | Loss: 0.00001376
Iteration 225/1000 | Loss: 0.00001376
Iteration 226/1000 | Loss: 0.00001376
Iteration 227/1000 | Loss: 0.00001376
Iteration 228/1000 | Loss: 0.00001376
Iteration 229/1000 | Loss: 0.00001376
Iteration 230/1000 | Loss: 0.00001376
Iteration 231/1000 | Loss: 0.00001376
Iteration 232/1000 | Loss: 0.00001376
Iteration 233/1000 | Loss: 0.00001376
Iteration 234/1000 | Loss: 0.00001376
Iteration 235/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.3764954928774387e-05, 1.3764954928774387e-05, 1.3764954928774387e-05, 1.3764954928774387e-05, 1.3764954928774387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3764954928774387e-05

Optimization complete. Final v2v error: 3.1805624961853027 mm

Highest mean error: 3.7626595497131348 mm for frame 76

Lowest mean error: 2.9581246376037598 mm for frame 156

Saving results

Total time: 43.81510353088379
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794462
Iteration 2/25 | Loss: 0.00165210
Iteration 3/25 | Loss: 0.00152290
Iteration 4/25 | Loss: 0.00151134
Iteration 5/25 | Loss: 0.00150799
Iteration 6/25 | Loss: 0.00150762
Iteration 7/25 | Loss: 0.00150762
Iteration 8/25 | Loss: 0.00150762
Iteration 9/25 | Loss: 0.00150762
Iteration 10/25 | Loss: 0.00150762
Iteration 11/25 | Loss: 0.00150762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001507623353973031, 0.001507623353973031, 0.001507623353973031, 0.001507623353973031, 0.001507623353973031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001507623353973031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.03156281
Iteration 2/25 | Loss: 0.00140656
Iteration 3/25 | Loss: 0.00140656
Iteration 4/25 | Loss: 0.00140656
Iteration 5/25 | Loss: 0.00140656
Iteration 6/25 | Loss: 0.00140656
Iteration 7/25 | Loss: 0.00140656
Iteration 8/25 | Loss: 0.00140656
Iteration 9/25 | Loss: 0.00140656
Iteration 10/25 | Loss: 0.00140656
Iteration 11/25 | Loss: 0.00140656
Iteration 12/25 | Loss: 0.00140656
Iteration 13/25 | Loss: 0.00140656
Iteration 14/25 | Loss: 0.00140656
Iteration 15/25 | Loss: 0.00140656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014065603027120233, 0.0014065603027120233, 0.0014065603027120233, 0.0014065603027120233, 0.0014065603027120233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014065603027120233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140656
Iteration 2/1000 | Loss: 0.00006662
Iteration 3/1000 | Loss: 0.00003751
Iteration 4/1000 | Loss: 0.00003131
Iteration 5/1000 | Loss: 0.00002933
Iteration 6/1000 | Loss: 0.00002794
Iteration 7/1000 | Loss: 0.00002715
Iteration 8/1000 | Loss: 0.00002670
Iteration 9/1000 | Loss: 0.00002639
Iteration 10/1000 | Loss: 0.00002635
Iteration 11/1000 | Loss: 0.00002616
Iteration 12/1000 | Loss: 0.00002605
Iteration 13/1000 | Loss: 0.00002603
Iteration 14/1000 | Loss: 0.00002602
Iteration 15/1000 | Loss: 0.00002602
Iteration 16/1000 | Loss: 0.00002601
Iteration 17/1000 | Loss: 0.00002596
Iteration 18/1000 | Loss: 0.00002595
Iteration 19/1000 | Loss: 0.00002592
Iteration 20/1000 | Loss: 0.00002589
Iteration 21/1000 | Loss: 0.00002585
Iteration 22/1000 | Loss: 0.00002585
Iteration 23/1000 | Loss: 0.00002585
Iteration 24/1000 | Loss: 0.00002582
Iteration 25/1000 | Loss: 0.00002581
Iteration 26/1000 | Loss: 0.00002580
Iteration 27/1000 | Loss: 0.00002580
Iteration 28/1000 | Loss: 0.00002579
Iteration 29/1000 | Loss: 0.00002579
Iteration 30/1000 | Loss: 0.00002578
Iteration 31/1000 | Loss: 0.00002577
Iteration 32/1000 | Loss: 0.00002575
Iteration 33/1000 | Loss: 0.00002575
Iteration 34/1000 | Loss: 0.00002574
Iteration 35/1000 | Loss: 0.00002574
Iteration 36/1000 | Loss: 0.00002574
Iteration 37/1000 | Loss: 0.00002574
Iteration 38/1000 | Loss: 0.00002573
Iteration 39/1000 | Loss: 0.00002573
Iteration 40/1000 | Loss: 0.00002573
Iteration 41/1000 | Loss: 0.00002572
Iteration 42/1000 | Loss: 0.00002572
Iteration 43/1000 | Loss: 0.00002572
Iteration 44/1000 | Loss: 0.00002571
Iteration 45/1000 | Loss: 0.00002571
Iteration 46/1000 | Loss: 0.00002571
Iteration 47/1000 | Loss: 0.00002571
Iteration 48/1000 | Loss: 0.00002571
Iteration 49/1000 | Loss: 0.00002571
Iteration 50/1000 | Loss: 0.00002571
Iteration 51/1000 | Loss: 0.00002571
Iteration 52/1000 | Loss: 0.00002570
Iteration 53/1000 | Loss: 0.00002570
Iteration 54/1000 | Loss: 0.00002570
Iteration 55/1000 | Loss: 0.00002570
Iteration 56/1000 | Loss: 0.00002570
Iteration 57/1000 | Loss: 0.00002570
Iteration 58/1000 | Loss: 0.00002570
Iteration 59/1000 | Loss: 0.00002570
Iteration 60/1000 | Loss: 0.00002569
Iteration 61/1000 | Loss: 0.00002569
Iteration 62/1000 | Loss: 0.00002569
Iteration 63/1000 | Loss: 0.00002569
Iteration 64/1000 | Loss: 0.00002569
Iteration 65/1000 | Loss: 0.00002569
Iteration 66/1000 | Loss: 0.00002569
Iteration 67/1000 | Loss: 0.00002569
Iteration 68/1000 | Loss: 0.00002569
Iteration 69/1000 | Loss: 0.00002569
Iteration 70/1000 | Loss: 0.00002568
Iteration 71/1000 | Loss: 0.00002568
Iteration 72/1000 | Loss: 0.00002568
Iteration 73/1000 | Loss: 0.00002568
Iteration 74/1000 | Loss: 0.00002568
Iteration 75/1000 | Loss: 0.00002568
Iteration 76/1000 | Loss: 0.00002568
Iteration 77/1000 | Loss: 0.00002568
Iteration 78/1000 | Loss: 0.00002567
Iteration 79/1000 | Loss: 0.00002567
Iteration 80/1000 | Loss: 0.00002567
Iteration 81/1000 | Loss: 0.00002567
Iteration 82/1000 | Loss: 0.00002567
Iteration 83/1000 | Loss: 0.00002567
Iteration 84/1000 | Loss: 0.00002567
Iteration 85/1000 | Loss: 0.00002567
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002567
Iteration 90/1000 | Loss: 0.00002566
Iteration 91/1000 | Loss: 0.00002566
Iteration 92/1000 | Loss: 0.00002566
Iteration 93/1000 | Loss: 0.00002566
Iteration 94/1000 | Loss: 0.00002566
Iteration 95/1000 | Loss: 0.00002566
Iteration 96/1000 | Loss: 0.00002566
Iteration 97/1000 | Loss: 0.00002566
Iteration 98/1000 | Loss: 0.00002566
Iteration 99/1000 | Loss: 0.00002566
Iteration 100/1000 | Loss: 0.00002565
Iteration 101/1000 | Loss: 0.00002565
Iteration 102/1000 | Loss: 0.00002565
Iteration 103/1000 | Loss: 0.00002565
Iteration 104/1000 | Loss: 0.00002565
Iteration 105/1000 | Loss: 0.00002565
Iteration 106/1000 | Loss: 0.00002565
Iteration 107/1000 | Loss: 0.00002565
Iteration 108/1000 | Loss: 0.00002565
Iteration 109/1000 | Loss: 0.00002565
Iteration 110/1000 | Loss: 0.00002565
Iteration 111/1000 | Loss: 0.00002565
Iteration 112/1000 | Loss: 0.00002565
Iteration 113/1000 | Loss: 0.00002565
Iteration 114/1000 | Loss: 0.00002565
Iteration 115/1000 | Loss: 0.00002565
Iteration 116/1000 | Loss: 0.00002565
Iteration 117/1000 | Loss: 0.00002565
Iteration 118/1000 | Loss: 0.00002565
Iteration 119/1000 | Loss: 0.00002565
Iteration 120/1000 | Loss: 0.00002565
Iteration 121/1000 | Loss: 0.00002565
Iteration 122/1000 | Loss: 0.00002565
Iteration 123/1000 | Loss: 0.00002565
Iteration 124/1000 | Loss: 0.00002565
Iteration 125/1000 | Loss: 0.00002565
Iteration 126/1000 | Loss: 0.00002565
Iteration 127/1000 | Loss: 0.00002565
Iteration 128/1000 | Loss: 0.00002565
Iteration 129/1000 | Loss: 0.00002565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.565132308518514e-05, 2.565132308518514e-05, 2.565132308518514e-05, 2.565132308518514e-05, 2.565132308518514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.565132308518514e-05

Optimization complete. Final v2v error: 4.260608196258545 mm

Highest mean error: 4.757723331451416 mm for frame 114

Lowest mean error: 3.888698101043701 mm for frame 173

Saving results

Total time: 32.933589220047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810270
Iteration 2/25 | Loss: 0.00150707
Iteration 3/25 | Loss: 0.00140243
Iteration 4/25 | Loss: 0.00138093
Iteration 5/25 | Loss: 0.00137532
Iteration 6/25 | Loss: 0.00137504
Iteration 7/25 | Loss: 0.00137504
Iteration 8/25 | Loss: 0.00137504
Iteration 9/25 | Loss: 0.00137504
Iteration 10/25 | Loss: 0.00137504
Iteration 11/25 | Loss: 0.00137504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013750427169725299, 0.0013750427169725299, 0.0013750427169725299, 0.0013750427169725299, 0.0013750427169725299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013750427169725299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94127309
Iteration 2/25 | Loss: 0.00071241
Iteration 3/25 | Loss: 0.00071240
Iteration 4/25 | Loss: 0.00071239
Iteration 5/25 | Loss: 0.00071239
Iteration 6/25 | Loss: 0.00071239
Iteration 7/25 | Loss: 0.00071239
Iteration 8/25 | Loss: 0.00071239
Iteration 9/25 | Loss: 0.00071239
Iteration 10/25 | Loss: 0.00071239
Iteration 11/25 | Loss: 0.00071239
Iteration 12/25 | Loss: 0.00071239
Iteration 13/25 | Loss: 0.00071239
Iteration 14/25 | Loss: 0.00071239
Iteration 15/25 | Loss: 0.00071239
Iteration 16/25 | Loss: 0.00071239
Iteration 17/25 | Loss: 0.00071239
Iteration 18/25 | Loss: 0.00071239
Iteration 19/25 | Loss: 0.00071239
Iteration 20/25 | Loss: 0.00071239
Iteration 21/25 | Loss: 0.00071239
Iteration 22/25 | Loss: 0.00071239
Iteration 23/25 | Loss: 0.00071239
Iteration 24/25 | Loss: 0.00071239
Iteration 25/25 | Loss: 0.00071239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071239
Iteration 2/1000 | Loss: 0.00004595
Iteration 3/1000 | Loss: 0.00003589
Iteration 4/1000 | Loss: 0.00003316
Iteration 5/1000 | Loss: 0.00003184
Iteration 6/1000 | Loss: 0.00003119
Iteration 7/1000 | Loss: 0.00003050
Iteration 8/1000 | Loss: 0.00003015
Iteration 9/1000 | Loss: 0.00002990
Iteration 10/1000 | Loss: 0.00002951
Iteration 11/1000 | Loss: 0.00002926
Iteration 12/1000 | Loss: 0.00002905
Iteration 13/1000 | Loss: 0.00002889
Iteration 14/1000 | Loss: 0.00002879
Iteration 15/1000 | Loss: 0.00002876
Iteration 16/1000 | Loss: 0.00002871
Iteration 17/1000 | Loss: 0.00002870
Iteration 18/1000 | Loss: 0.00002868
Iteration 19/1000 | Loss: 0.00002866
Iteration 20/1000 | Loss: 0.00002865
Iteration 21/1000 | Loss: 0.00002865
Iteration 22/1000 | Loss: 0.00002863
Iteration 23/1000 | Loss: 0.00002863
Iteration 24/1000 | Loss: 0.00002863
Iteration 25/1000 | Loss: 0.00002863
Iteration 26/1000 | Loss: 0.00002862
Iteration 27/1000 | Loss: 0.00002861
Iteration 28/1000 | Loss: 0.00002860
Iteration 29/1000 | Loss: 0.00002860
Iteration 30/1000 | Loss: 0.00002859
Iteration 31/1000 | Loss: 0.00002859
Iteration 32/1000 | Loss: 0.00002859
Iteration 33/1000 | Loss: 0.00002859
Iteration 34/1000 | Loss: 0.00002859
Iteration 35/1000 | Loss: 0.00002859
Iteration 36/1000 | Loss: 0.00002858
Iteration 37/1000 | Loss: 0.00002857
Iteration 38/1000 | Loss: 0.00002855
Iteration 39/1000 | Loss: 0.00002852
Iteration 40/1000 | Loss: 0.00002852
Iteration 41/1000 | Loss: 0.00002852
Iteration 42/1000 | Loss: 0.00002852
Iteration 43/1000 | Loss: 0.00002852
Iteration 44/1000 | Loss: 0.00002852
Iteration 45/1000 | Loss: 0.00002851
Iteration 46/1000 | Loss: 0.00002851
Iteration 47/1000 | Loss: 0.00002850
Iteration 48/1000 | Loss: 0.00002850
Iteration 49/1000 | Loss: 0.00002850
Iteration 50/1000 | Loss: 0.00002849
Iteration 51/1000 | Loss: 0.00002849
Iteration 52/1000 | Loss: 0.00002849
Iteration 53/1000 | Loss: 0.00002849
Iteration 54/1000 | Loss: 0.00002849
Iteration 55/1000 | Loss: 0.00002849
Iteration 56/1000 | Loss: 0.00002848
Iteration 57/1000 | Loss: 0.00002848
Iteration 58/1000 | Loss: 0.00002848
Iteration 59/1000 | Loss: 0.00002848
Iteration 60/1000 | Loss: 0.00002848
Iteration 61/1000 | Loss: 0.00002848
Iteration 62/1000 | Loss: 0.00002848
Iteration 63/1000 | Loss: 0.00002848
Iteration 64/1000 | Loss: 0.00002848
Iteration 65/1000 | Loss: 0.00002848
Iteration 66/1000 | Loss: 0.00002848
Iteration 67/1000 | Loss: 0.00002847
Iteration 68/1000 | Loss: 0.00002847
Iteration 69/1000 | Loss: 0.00002847
Iteration 70/1000 | Loss: 0.00002847
Iteration 71/1000 | Loss: 0.00002847
Iteration 72/1000 | Loss: 0.00002846
Iteration 73/1000 | Loss: 0.00002846
Iteration 74/1000 | Loss: 0.00002846
Iteration 75/1000 | Loss: 0.00002846
Iteration 76/1000 | Loss: 0.00002845
Iteration 77/1000 | Loss: 0.00002845
Iteration 78/1000 | Loss: 0.00002845
Iteration 79/1000 | Loss: 0.00002845
Iteration 80/1000 | Loss: 0.00002844
Iteration 81/1000 | Loss: 0.00002844
Iteration 82/1000 | Loss: 0.00002844
Iteration 83/1000 | Loss: 0.00002843
Iteration 84/1000 | Loss: 0.00002843
Iteration 85/1000 | Loss: 0.00002843
Iteration 86/1000 | Loss: 0.00002842
Iteration 87/1000 | Loss: 0.00002842
Iteration 88/1000 | Loss: 0.00002842
Iteration 89/1000 | Loss: 0.00002842
Iteration 90/1000 | Loss: 0.00002842
Iteration 91/1000 | Loss: 0.00002842
Iteration 92/1000 | Loss: 0.00002842
Iteration 93/1000 | Loss: 0.00002841
Iteration 94/1000 | Loss: 0.00002841
Iteration 95/1000 | Loss: 0.00002841
Iteration 96/1000 | Loss: 0.00002841
Iteration 97/1000 | Loss: 0.00002841
Iteration 98/1000 | Loss: 0.00002841
Iteration 99/1000 | Loss: 0.00002840
Iteration 100/1000 | Loss: 0.00002840
Iteration 101/1000 | Loss: 0.00002840
Iteration 102/1000 | Loss: 0.00002840
Iteration 103/1000 | Loss: 0.00002840
Iteration 104/1000 | Loss: 0.00002838
Iteration 105/1000 | Loss: 0.00002838
Iteration 106/1000 | Loss: 0.00002836
Iteration 107/1000 | Loss: 0.00002836
Iteration 108/1000 | Loss: 0.00002836
Iteration 109/1000 | Loss: 0.00002836
Iteration 110/1000 | Loss: 0.00002836
Iteration 111/1000 | Loss: 0.00002835
Iteration 112/1000 | Loss: 0.00002835
Iteration 113/1000 | Loss: 0.00002835
Iteration 114/1000 | Loss: 0.00002834
Iteration 115/1000 | Loss: 0.00002834
Iteration 116/1000 | Loss: 0.00002834
Iteration 117/1000 | Loss: 0.00002833
Iteration 118/1000 | Loss: 0.00002833
Iteration 119/1000 | Loss: 0.00002833
Iteration 120/1000 | Loss: 0.00002833
Iteration 121/1000 | Loss: 0.00002833
Iteration 122/1000 | Loss: 0.00002833
Iteration 123/1000 | Loss: 0.00002832
Iteration 124/1000 | Loss: 0.00002832
Iteration 125/1000 | Loss: 0.00002832
Iteration 126/1000 | Loss: 0.00002832
Iteration 127/1000 | Loss: 0.00002832
Iteration 128/1000 | Loss: 0.00002831
Iteration 129/1000 | Loss: 0.00002831
Iteration 130/1000 | Loss: 0.00002831
Iteration 131/1000 | Loss: 0.00002831
Iteration 132/1000 | Loss: 0.00002831
Iteration 133/1000 | Loss: 0.00002831
Iteration 134/1000 | Loss: 0.00002831
Iteration 135/1000 | Loss: 0.00002831
Iteration 136/1000 | Loss: 0.00002831
Iteration 137/1000 | Loss: 0.00002831
Iteration 138/1000 | Loss: 0.00002831
Iteration 139/1000 | Loss: 0.00002831
Iteration 140/1000 | Loss: 0.00002831
Iteration 141/1000 | Loss: 0.00002831
Iteration 142/1000 | Loss: 0.00002831
Iteration 143/1000 | Loss: 0.00002831
Iteration 144/1000 | Loss: 0.00002831
Iteration 145/1000 | Loss: 0.00002830
Iteration 146/1000 | Loss: 0.00002830
Iteration 147/1000 | Loss: 0.00002830
Iteration 148/1000 | Loss: 0.00002830
Iteration 149/1000 | Loss: 0.00002830
Iteration 150/1000 | Loss: 0.00002830
Iteration 151/1000 | Loss: 0.00002830
Iteration 152/1000 | Loss: 0.00002830
Iteration 153/1000 | Loss: 0.00002830
Iteration 154/1000 | Loss: 0.00002830
Iteration 155/1000 | Loss: 0.00002830
Iteration 156/1000 | Loss: 0.00002830
Iteration 157/1000 | Loss: 0.00002830
Iteration 158/1000 | Loss: 0.00002830
Iteration 159/1000 | Loss: 0.00002829
Iteration 160/1000 | Loss: 0.00002829
Iteration 161/1000 | Loss: 0.00002829
Iteration 162/1000 | Loss: 0.00002829
Iteration 163/1000 | Loss: 0.00002829
Iteration 164/1000 | Loss: 0.00002829
Iteration 165/1000 | Loss: 0.00002829
Iteration 166/1000 | Loss: 0.00002829
Iteration 167/1000 | Loss: 0.00002829
Iteration 168/1000 | Loss: 0.00002829
Iteration 169/1000 | Loss: 0.00002829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.8292015485931188e-05, 2.8292015485931188e-05, 2.8292015485931188e-05, 2.8292015485931188e-05, 2.8292015485931188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8292015485931188e-05

Optimization complete. Final v2v error: 4.541413307189941 mm

Highest mean error: 4.8323822021484375 mm for frame 29

Lowest mean error: 4.4194793701171875 mm for frame 12

Saving results

Total time: 38.881044149398804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812990
Iteration 2/25 | Loss: 0.00142508
Iteration 3/25 | Loss: 0.00133299
Iteration 4/25 | Loss: 0.00131914
Iteration 5/25 | Loss: 0.00131473
Iteration 6/25 | Loss: 0.00131448
Iteration 7/25 | Loss: 0.00131448
Iteration 8/25 | Loss: 0.00131448
Iteration 9/25 | Loss: 0.00131448
Iteration 10/25 | Loss: 0.00131448
Iteration 11/25 | Loss: 0.00131448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013144816039130092, 0.0013144816039130092, 0.0013144816039130092, 0.0013144816039130092, 0.0013144816039130092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013144816039130092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47631395
Iteration 2/25 | Loss: 0.00076920
Iteration 3/25 | Loss: 0.00076919
Iteration 4/25 | Loss: 0.00076919
Iteration 5/25 | Loss: 0.00076919
Iteration 6/25 | Loss: 0.00076919
Iteration 7/25 | Loss: 0.00076919
Iteration 8/25 | Loss: 0.00076919
Iteration 9/25 | Loss: 0.00076919
Iteration 10/25 | Loss: 0.00076919
Iteration 11/25 | Loss: 0.00076919
Iteration 12/25 | Loss: 0.00076919
Iteration 13/25 | Loss: 0.00076919
Iteration 14/25 | Loss: 0.00076919
Iteration 15/25 | Loss: 0.00076919
Iteration 16/25 | Loss: 0.00076919
Iteration 17/25 | Loss: 0.00076919
Iteration 18/25 | Loss: 0.00076919
Iteration 19/25 | Loss: 0.00076919
Iteration 20/25 | Loss: 0.00076919
Iteration 21/25 | Loss: 0.00076919
Iteration 22/25 | Loss: 0.00076919
Iteration 23/25 | Loss: 0.00076919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00076918990816921, 0.00076918990816921, 0.00076918990816921, 0.00076918990816921, 0.00076918990816921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00076918990816921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076919
Iteration 2/1000 | Loss: 0.00003192
Iteration 3/1000 | Loss: 0.00002303
Iteration 4/1000 | Loss: 0.00002096
Iteration 5/1000 | Loss: 0.00001986
Iteration 6/1000 | Loss: 0.00001909
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001851
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001764
Iteration 14/1000 | Loss: 0.00001764
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001754
Iteration 19/1000 | Loss: 0.00001753
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001728
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001723
Iteration 31/1000 | Loss: 0.00001723
Iteration 32/1000 | Loss: 0.00001722
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001718
Iteration 35/1000 | Loss: 0.00001717
Iteration 36/1000 | Loss: 0.00001715
Iteration 37/1000 | Loss: 0.00001715
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001704
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001701
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001698
Iteration 59/1000 | Loss: 0.00001698
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001693
Iteration 74/1000 | Loss: 0.00001693
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001692
Iteration 79/1000 | Loss: 0.00001692
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001690
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001689
Iteration 96/1000 | Loss: 0.00001689
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001687
Iteration 106/1000 | Loss: 0.00001687
Iteration 107/1000 | Loss: 0.00001687
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001686
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001686
Iteration 112/1000 | Loss: 0.00001686
Iteration 113/1000 | Loss: 0.00001686
Iteration 114/1000 | Loss: 0.00001686
Iteration 115/1000 | Loss: 0.00001686
Iteration 116/1000 | Loss: 0.00001686
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001686
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001685
Iteration 128/1000 | Loss: 0.00001685
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001685
Iteration 132/1000 | Loss: 0.00001685
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001684
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001684
Iteration 172/1000 | Loss: 0.00001684
Iteration 173/1000 | Loss: 0.00001684
Iteration 174/1000 | Loss: 0.00001684
Iteration 175/1000 | Loss: 0.00001684
Iteration 176/1000 | Loss: 0.00001684
Iteration 177/1000 | Loss: 0.00001684
Iteration 178/1000 | Loss: 0.00001684
Iteration 179/1000 | Loss: 0.00001684
Iteration 180/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.684160451986827e-05, 1.684160451986827e-05, 1.684160451986827e-05, 1.684160451986827e-05, 1.684160451986827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.684160451986827e-05

Optimization complete. Final v2v error: 3.4641685485839844 mm

Highest mean error: 3.8217015266418457 mm for frame 176

Lowest mean error: 3.16571307182312 mm for frame 132

Saving results

Total time: 40.341395139694214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435849
Iteration 2/25 | Loss: 0.00138642
Iteration 3/25 | Loss: 0.00131301
Iteration 4/25 | Loss: 0.00130395
Iteration 5/25 | Loss: 0.00130170
Iteration 6/25 | Loss: 0.00130170
Iteration 7/25 | Loss: 0.00130170
Iteration 8/25 | Loss: 0.00130170
Iteration 9/25 | Loss: 0.00130170
Iteration 10/25 | Loss: 0.00130170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013017042074352503, 0.0013017042074352503, 0.0013017042074352503, 0.0013017042074352503, 0.0013017042074352503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013017042074352503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38171053
Iteration 2/25 | Loss: 0.00103408
Iteration 3/25 | Loss: 0.00103408
Iteration 4/25 | Loss: 0.00103408
Iteration 5/25 | Loss: 0.00103407
Iteration 6/25 | Loss: 0.00103407
Iteration 7/25 | Loss: 0.00103407
Iteration 8/25 | Loss: 0.00103407
Iteration 9/25 | Loss: 0.00103407
Iteration 10/25 | Loss: 0.00103407
Iteration 11/25 | Loss: 0.00103407
Iteration 12/25 | Loss: 0.00103407
Iteration 13/25 | Loss: 0.00103407
Iteration 14/25 | Loss: 0.00103407
Iteration 15/25 | Loss: 0.00103407
Iteration 16/25 | Loss: 0.00103407
Iteration 17/25 | Loss: 0.00103407
Iteration 18/25 | Loss: 0.00103407
Iteration 19/25 | Loss: 0.00103407
Iteration 20/25 | Loss: 0.00103407
Iteration 21/25 | Loss: 0.00103407
Iteration 22/25 | Loss: 0.00103407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010340723674744368, 0.0010340723674744368, 0.0010340723674744368, 0.0010340723674744368, 0.0010340723674744368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010340723674744368

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103407
Iteration 2/1000 | Loss: 0.00003741
Iteration 3/1000 | Loss: 0.00002256
Iteration 4/1000 | Loss: 0.00001940
Iteration 5/1000 | Loss: 0.00001798
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001634
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001528
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001503
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001491
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001488
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001486
Iteration 22/1000 | Loss: 0.00001484
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001481
Iteration 25/1000 | Loss: 0.00001478
Iteration 26/1000 | Loss: 0.00001478
Iteration 27/1000 | Loss: 0.00001477
Iteration 28/1000 | Loss: 0.00001477
Iteration 29/1000 | Loss: 0.00001476
Iteration 30/1000 | Loss: 0.00001476
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001462
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001459
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001456
Iteration 44/1000 | Loss: 0.00001455
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001454
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001445
Iteration 68/1000 | Loss: 0.00001445
Iteration 69/1000 | Loss: 0.00001445
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001444
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001443
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001442
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001441
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001440
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001432
Iteration 104/1000 | Loss: 0.00001432
Iteration 105/1000 | Loss: 0.00001432
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001431
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001430
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001429
Iteration 117/1000 | Loss: 0.00001429
Iteration 118/1000 | Loss: 0.00001429
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001429
Iteration 121/1000 | Loss: 0.00001429
Iteration 122/1000 | Loss: 0.00001429
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001427
Iteration 128/1000 | Loss: 0.00001427
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001426
Iteration 132/1000 | Loss: 0.00001426
Iteration 133/1000 | Loss: 0.00001426
Iteration 134/1000 | Loss: 0.00001425
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001424
Iteration 142/1000 | Loss: 0.00001424
Iteration 143/1000 | Loss: 0.00001424
Iteration 144/1000 | Loss: 0.00001424
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001422
Iteration 153/1000 | Loss: 0.00001422
Iteration 154/1000 | Loss: 0.00001422
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001422
Iteration 157/1000 | Loss: 0.00001422
Iteration 158/1000 | Loss: 0.00001422
Iteration 159/1000 | Loss: 0.00001422
Iteration 160/1000 | Loss: 0.00001422
Iteration 161/1000 | Loss: 0.00001422
Iteration 162/1000 | Loss: 0.00001422
Iteration 163/1000 | Loss: 0.00001422
Iteration 164/1000 | Loss: 0.00001422
Iteration 165/1000 | Loss: 0.00001422
Iteration 166/1000 | Loss: 0.00001422
Iteration 167/1000 | Loss: 0.00001422
Iteration 168/1000 | Loss: 0.00001422
Iteration 169/1000 | Loss: 0.00001422
Iteration 170/1000 | Loss: 0.00001422
Iteration 171/1000 | Loss: 0.00001422
Iteration 172/1000 | Loss: 0.00001422
Iteration 173/1000 | Loss: 0.00001422
Iteration 174/1000 | Loss: 0.00001422
Iteration 175/1000 | Loss: 0.00001422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.4216891941032372e-05, 1.4216891941032372e-05, 1.4216891941032372e-05, 1.4216891941032372e-05, 1.4216891941032372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4216891941032372e-05

Optimization complete. Final v2v error: 3.193779230117798 mm

Highest mean error: 3.7129311561584473 mm for frame 12

Lowest mean error: 2.8876466751098633 mm for frame 134

Saving results

Total time: 45.66255974769592
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986553
Iteration 2/25 | Loss: 0.00234942
Iteration 3/25 | Loss: 0.00194118
Iteration 4/25 | Loss: 0.00188241
Iteration 5/25 | Loss: 0.00181614
Iteration 6/25 | Loss: 0.00176438
Iteration 7/25 | Loss: 0.00166629
Iteration 8/25 | Loss: 0.00163335
Iteration 9/25 | Loss: 0.00160052
Iteration 10/25 | Loss: 0.00158904
Iteration 11/25 | Loss: 0.00158236
Iteration 12/25 | Loss: 0.00157970
Iteration 13/25 | Loss: 0.00157450
Iteration 14/25 | Loss: 0.00157325
Iteration 15/25 | Loss: 0.00156940
Iteration 16/25 | Loss: 0.00156406
Iteration 17/25 | Loss: 0.00156216
Iteration 18/25 | Loss: 0.00156372
Iteration 19/25 | Loss: 0.00156319
Iteration 20/25 | Loss: 0.00155938
Iteration 21/25 | Loss: 0.00155566
Iteration 22/25 | Loss: 0.00155686
Iteration 23/25 | Loss: 0.00155949
Iteration 24/25 | Loss: 0.00155277
Iteration 25/25 | Loss: 0.00154897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35468066
Iteration 2/25 | Loss: 0.00360937
Iteration 3/25 | Loss: 0.00360936
Iteration 4/25 | Loss: 0.00349814
Iteration 5/25 | Loss: 0.00349812
Iteration 6/25 | Loss: 0.00349812
Iteration 7/25 | Loss: 0.00349812
Iteration 8/25 | Loss: 0.00349812
Iteration 9/25 | Loss: 0.00349812
Iteration 10/25 | Loss: 0.00349812
Iteration 11/25 | Loss: 0.00349812
Iteration 12/25 | Loss: 0.00349812
Iteration 13/25 | Loss: 0.00349812
Iteration 14/25 | Loss: 0.00349812
Iteration 15/25 | Loss: 0.00349812
Iteration 16/25 | Loss: 0.00349812
Iteration 17/25 | Loss: 0.00349812
Iteration 18/25 | Loss: 0.00349812
Iteration 19/25 | Loss: 0.00349812
Iteration 20/25 | Loss: 0.00349811
Iteration 21/25 | Loss: 0.00349811
Iteration 22/25 | Loss: 0.00349811
Iteration 23/25 | Loss: 0.00349811
Iteration 24/25 | Loss: 0.00349811
Iteration 25/25 | Loss: 0.00349811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0034981148783117533, 0.0034981148783117533, 0.0034981148783117533, 0.0034981148783117533, 0.0034981148783117533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034981148783117533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00349811
Iteration 2/1000 | Loss: 0.00346104
Iteration 3/1000 | Loss: 0.00039308
Iteration 4/1000 | Loss: 0.00064436
Iteration 5/1000 | Loss: 0.00039865
Iteration 6/1000 | Loss: 0.00100090
Iteration 7/1000 | Loss: 0.00070597
Iteration 8/1000 | Loss: 0.00126197
Iteration 9/1000 | Loss: 0.00058243
Iteration 10/1000 | Loss: 0.00069757
Iteration 11/1000 | Loss: 0.00043767
Iteration 12/1000 | Loss: 0.00097444
Iteration 13/1000 | Loss: 0.00110329
Iteration 14/1000 | Loss: 0.00044964
Iteration 15/1000 | Loss: 0.00057854
Iteration 16/1000 | Loss: 0.00067540
Iteration 17/1000 | Loss: 0.00017334
Iteration 18/1000 | Loss: 0.00023948
Iteration 19/1000 | Loss: 0.00130358
Iteration 20/1000 | Loss: 0.00106353
Iteration 21/1000 | Loss: 0.00095858
Iteration 22/1000 | Loss: 0.00090315
Iteration 23/1000 | Loss: 0.00072169
Iteration 24/1000 | Loss: 0.00026068
Iteration 25/1000 | Loss: 0.00032811
Iteration 26/1000 | Loss: 0.00047866
Iteration 27/1000 | Loss: 0.00074188
Iteration 28/1000 | Loss: 0.00031296
Iteration 29/1000 | Loss: 0.00054018
Iteration 30/1000 | Loss: 0.00044924
Iteration 31/1000 | Loss: 0.00018285
Iteration 32/1000 | Loss: 0.00023331
Iteration 33/1000 | Loss: 0.00040982
Iteration 34/1000 | Loss: 0.00064717
Iteration 35/1000 | Loss: 0.00047138
Iteration 36/1000 | Loss: 0.00016340
Iteration 37/1000 | Loss: 0.00035262
Iteration 38/1000 | Loss: 0.00061091
Iteration 39/1000 | Loss: 0.00074874
Iteration 40/1000 | Loss: 0.00027124
Iteration 41/1000 | Loss: 0.00039317
Iteration 42/1000 | Loss: 0.00052355
Iteration 43/1000 | Loss: 0.00021074
Iteration 44/1000 | Loss: 0.00015080
Iteration 45/1000 | Loss: 0.00010446
Iteration 46/1000 | Loss: 0.00026161
Iteration 47/1000 | Loss: 0.00041001
Iteration 48/1000 | Loss: 0.00050078
Iteration 49/1000 | Loss: 0.00024155
Iteration 50/1000 | Loss: 0.00027040
Iteration 51/1000 | Loss: 0.00066332
Iteration 52/1000 | Loss: 0.00032319
Iteration 53/1000 | Loss: 0.00042333
Iteration 54/1000 | Loss: 0.00032640
Iteration 55/1000 | Loss: 0.00062912
Iteration 56/1000 | Loss: 0.00015768
Iteration 57/1000 | Loss: 0.00025261
Iteration 58/1000 | Loss: 0.00027149
Iteration 59/1000 | Loss: 0.00048092
Iteration 60/1000 | Loss: 0.00028964
Iteration 61/1000 | Loss: 0.00030043
Iteration 62/1000 | Loss: 0.00048747
Iteration 63/1000 | Loss: 0.00011727
Iteration 64/1000 | Loss: 0.00027207
Iteration 65/1000 | Loss: 0.00021773
Iteration 66/1000 | Loss: 0.00043386
Iteration 67/1000 | Loss: 0.00024004
Iteration 68/1000 | Loss: 0.00009515
Iteration 69/1000 | Loss: 0.00008446
Iteration 70/1000 | Loss: 0.00013416
Iteration 71/1000 | Loss: 0.00126948
Iteration 72/1000 | Loss: 0.00025103
Iteration 73/1000 | Loss: 0.00008465
Iteration 74/1000 | Loss: 0.00132232
Iteration 75/1000 | Loss: 0.00335309
Iteration 76/1000 | Loss: 0.00033826
Iteration 77/1000 | Loss: 0.00027312
Iteration 78/1000 | Loss: 0.00020117
Iteration 79/1000 | Loss: 0.00069096
Iteration 80/1000 | Loss: 0.00061407
Iteration 81/1000 | Loss: 0.00058487
Iteration 82/1000 | Loss: 0.00009092
Iteration 83/1000 | Loss: 0.00008864
Iteration 84/1000 | Loss: 0.00020651
Iteration 85/1000 | Loss: 0.00014309
Iteration 86/1000 | Loss: 0.00006692
Iteration 87/1000 | Loss: 0.00008127
Iteration 88/1000 | Loss: 0.00007529
Iteration 89/1000 | Loss: 0.00006180
Iteration 90/1000 | Loss: 0.00072621
Iteration 91/1000 | Loss: 0.00042191
Iteration 92/1000 | Loss: 0.00052229
Iteration 93/1000 | Loss: 0.00027744
Iteration 94/1000 | Loss: 0.00013650
Iteration 95/1000 | Loss: 0.00010834
Iteration 96/1000 | Loss: 0.00015403
Iteration 97/1000 | Loss: 0.00005669
Iteration 98/1000 | Loss: 0.00007094
Iteration 99/1000 | Loss: 0.00005193
Iteration 100/1000 | Loss: 0.00005573
Iteration 101/1000 | Loss: 0.00034568
Iteration 102/1000 | Loss: 0.00025390
Iteration 103/1000 | Loss: 0.00024352
Iteration 104/1000 | Loss: 0.00036272
Iteration 105/1000 | Loss: 0.00034285
Iteration 106/1000 | Loss: 0.00017845
Iteration 107/1000 | Loss: 0.00025012
Iteration 108/1000 | Loss: 0.00015207
Iteration 109/1000 | Loss: 0.00032590
Iteration 110/1000 | Loss: 0.00011235
Iteration 111/1000 | Loss: 0.00005659
Iteration 112/1000 | Loss: 0.00005912
Iteration 113/1000 | Loss: 0.00005801
Iteration 114/1000 | Loss: 0.00006027
Iteration 115/1000 | Loss: 0.00006184
Iteration 116/1000 | Loss: 0.00005563
Iteration 117/1000 | Loss: 0.00009624
Iteration 118/1000 | Loss: 0.00017267
Iteration 119/1000 | Loss: 0.00014441
Iteration 120/1000 | Loss: 0.00005540
Iteration 121/1000 | Loss: 0.00005483
Iteration 122/1000 | Loss: 0.00060498
Iteration 123/1000 | Loss: 0.00049549
Iteration 124/1000 | Loss: 0.00026166
Iteration 125/1000 | Loss: 0.00013228
Iteration 126/1000 | Loss: 0.00014002
Iteration 127/1000 | Loss: 0.00009486
Iteration 128/1000 | Loss: 0.00019393
Iteration 129/1000 | Loss: 0.00006568
Iteration 130/1000 | Loss: 0.00005035
Iteration 131/1000 | Loss: 0.00004765
Iteration 132/1000 | Loss: 0.00004493
Iteration 133/1000 | Loss: 0.00008222
Iteration 134/1000 | Loss: 0.00004188
Iteration 135/1000 | Loss: 0.00020748
Iteration 136/1000 | Loss: 0.00005012
Iteration 137/1000 | Loss: 0.00004699
Iteration 138/1000 | Loss: 0.00004221
Iteration 139/1000 | Loss: 0.00005835
Iteration 140/1000 | Loss: 0.00008190
Iteration 141/1000 | Loss: 0.00004034
Iteration 142/1000 | Loss: 0.00008092
Iteration 143/1000 | Loss: 0.00009010
Iteration 144/1000 | Loss: 0.00003756
Iteration 145/1000 | Loss: 0.00004179
Iteration 146/1000 | Loss: 0.00003673
Iteration 147/1000 | Loss: 0.00003642
Iteration 148/1000 | Loss: 0.00003486
Iteration 149/1000 | Loss: 0.00003502
Iteration 150/1000 | Loss: 0.00003419
Iteration 151/1000 | Loss: 0.00003381
Iteration 152/1000 | Loss: 0.00003348
Iteration 153/1000 | Loss: 0.00016032
Iteration 154/1000 | Loss: 0.00003581
Iteration 155/1000 | Loss: 0.00003347
Iteration 156/1000 | Loss: 0.00003601
Iteration 157/1000 | Loss: 0.00003239
Iteration 158/1000 | Loss: 0.00003736
Iteration 159/1000 | Loss: 0.00003156
Iteration 160/1000 | Loss: 0.00003115
Iteration 161/1000 | Loss: 0.00003097
Iteration 162/1000 | Loss: 0.00003097
Iteration 163/1000 | Loss: 0.00003096
Iteration 164/1000 | Loss: 0.00003092
Iteration 165/1000 | Loss: 0.00003089
Iteration 166/1000 | Loss: 0.00003086
Iteration 167/1000 | Loss: 0.00003085
Iteration 168/1000 | Loss: 0.00003081
Iteration 169/1000 | Loss: 0.00003081
Iteration 170/1000 | Loss: 0.00003081
Iteration 171/1000 | Loss: 0.00003081
Iteration 172/1000 | Loss: 0.00003081
Iteration 173/1000 | Loss: 0.00005937
Iteration 174/1000 | Loss: 0.00003081
Iteration 175/1000 | Loss: 0.00003503
Iteration 176/1000 | Loss: 0.00003075
Iteration 177/1000 | Loss: 0.00003075
Iteration 178/1000 | Loss: 0.00003074
Iteration 179/1000 | Loss: 0.00003074
Iteration 180/1000 | Loss: 0.00003074
Iteration 181/1000 | Loss: 0.00003074
Iteration 182/1000 | Loss: 0.00003074
Iteration 183/1000 | Loss: 0.00003074
Iteration 184/1000 | Loss: 0.00003074
Iteration 185/1000 | Loss: 0.00003074
Iteration 186/1000 | Loss: 0.00003074
Iteration 187/1000 | Loss: 0.00003073
Iteration 188/1000 | Loss: 0.00003073
Iteration 189/1000 | Loss: 0.00003072
Iteration 190/1000 | Loss: 0.00003072
Iteration 191/1000 | Loss: 0.00003071
Iteration 192/1000 | Loss: 0.00003071
Iteration 193/1000 | Loss: 0.00003071
Iteration 194/1000 | Loss: 0.00003071
Iteration 195/1000 | Loss: 0.00003071
Iteration 196/1000 | Loss: 0.00003070
Iteration 197/1000 | Loss: 0.00003070
Iteration 198/1000 | Loss: 0.00003070
Iteration 199/1000 | Loss: 0.00003070
Iteration 200/1000 | Loss: 0.00003070
Iteration 201/1000 | Loss: 0.00003070
Iteration 202/1000 | Loss: 0.00003070
Iteration 203/1000 | Loss: 0.00003069
Iteration 204/1000 | Loss: 0.00003069
Iteration 205/1000 | Loss: 0.00003069
Iteration 206/1000 | Loss: 0.00003069
Iteration 207/1000 | Loss: 0.00003068
Iteration 208/1000 | Loss: 0.00003068
Iteration 209/1000 | Loss: 0.00003068
Iteration 210/1000 | Loss: 0.00003068
Iteration 211/1000 | Loss: 0.00003068
Iteration 212/1000 | Loss: 0.00003068
Iteration 213/1000 | Loss: 0.00003068
Iteration 214/1000 | Loss: 0.00003068
Iteration 215/1000 | Loss: 0.00003068
Iteration 216/1000 | Loss: 0.00003068
Iteration 217/1000 | Loss: 0.00003068
Iteration 218/1000 | Loss: 0.00003068
Iteration 219/1000 | Loss: 0.00003067
Iteration 220/1000 | Loss: 0.00003067
Iteration 221/1000 | Loss: 0.00003067
Iteration 222/1000 | Loss: 0.00003067
Iteration 223/1000 | Loss: 0.00003067
Iteration 224/1000 | Loss: 0.00003067
Iteration 225/1000 | Loss: 0.00003067
Iteration 226/1000 | Loss: 0.00003067
Iteration 227/1000 | Loss: 0.00003067
Iteration 228/1000 | Loss: 0.00003067
Iteration 229/1000 | Loss: 0.00003067
Iteration 230/1000 | Loss: 0.00003067
Iteration 231/1000 | Loss: 0.00003067
Iteration 232/1000 | Loss: 0.00003067
Iteration 233/1000 | Loss: 0.00003067
Iteration 234/1000 | Loss: 0.00003067
Iteration 235/1000 | Loss: 0.00003067
Iteration 236/1000 | Loss: 0.00003067
Iteration 237/1000 | Loss: 0.00003067
Iteration 238/1000 | Loss: 0.00003067
Iteration 239/1000 | Loss: 0.00003067
Iteration 240/1000 | Loss: 0.00003067
Iteration 241/1000 | Loss: 0.00003067
Iteration 242/1000 | Loss: 0.00003067
Iteration 243/1000 | Loss: 0.00003067
Iteration 244/1000 | Loss: 0.00003067
Iteration 245/1000 | Loss: 0.00003067
Iteration 246/1000 | Loss: 0.00003067
Iteration 247/1000 | Loss: 0.00003067
Iteration 248/1000 | Loss: 0.00003067
Iteration 249/1000 | Loss: 0.00003067
Iteration 250/1000 | Loss: 0.00003067
Iteration 251/1000 | Loss: 0.00003067
Iteration 252/1000 | Loss: 0.00003067
Iteration 253/1000 | Loss: 0.00003067
Iteration 254/1000 | Loss: 0.00003067
Iteration 255/1000 | Loss: 0.00003067
Iteration 256/1000 | Loss: 0.00003067
Iteration 257/1000 | Loss: 0.00003067
Iteration 258/1000 | Loss: 0.00003067
Iteration 259/1000 | Loss: 0.00003067
Iteration 260/1000 | Loss: 0.00003067
Iteration 261/1000 | Loss: 0.00003067
Iteration 262/1000 | Loss: 0.00003067
Iteration 263/1000 | Loss: 0.00003067
Iteration 264/1000 | Loss: 0.00003067
Iteration 265/1000 | Loss: 0.00003067
Iteration 266/1000 | Loss: 0.00003067
Iteration 267/1000 | Loss: 0.00003067
Iteration 268/1000 | Loss: 0.00003067
Iteration 269/1000 | Loss: 0.00003067
Iteration 270/1000 | Loss: 0.00003067
Iteration 271/1000 | Loss: 0.00003067
Iteration 272/1000 | Loss: 0.00003067
Iteration 273/1000 | Loss: 0.00003067
Iteration 274/1000 | Loss: 0.00003067
Iteration 275/1000 | Loss: 0.00003067
Iteration 276/1000 | Loss: 0.00003067
Iteration 277/1000 | Loss: 0.00003067
Iteration 278/1000 | Loss: 0.00003067
Iteration 279/1000 | Loss: 0.00003067
Iteration 280/1000 | Loss: 0.00003067
Iteration 281/1000 | Loss: 0.00003067
Iteration 282/1000 | Loss: 0.00003067
Iteration 283/1000 | Loss: 0.00003067
Iteration 284/1000 | Loss: 0.00003067
Iteration 285/1000 | Loss: 0.00003067
Iteration 286/1000 | Loss: 0.00003067
Iteration 287/1000 | Loss: 0.00003067
Iteration 288/1000 | Loss: 0.00003067
Iteration 289/1000 | Loss: 0.00003067
Iteration 290/1000 | Loss: 0.00003067
Iteration 291/1000 | Loss: 0.00003067
Iteration 292/1000 | Loss: 0.00003067
Iteration 293/1000 | Loss: 0.00003067
Iteration 294/1000 | Loss: 0.00003067
Iteration 295/1000 | Loss: 0.00003067
Iteration 296/1000 | Loss: 0.00003067
Iteration 297/1000 | Loss: 0.00003067
Iteration 298/1000 | Loss: 0.00003067
Iteration 299/1000 | Loss: 0.00003067
Iteration 300/1000 | Loss: 0.00003067
Iteration 301/1000 | Loss: 0.00003067
Iteration 302/1000 | Loss: 0.00003067
Iteration 303/1000 | Loss: 0.00003067
Iteration 304/1000 | Loss: 0.00003067
Iteration 305/1000 | Loss: 0.00003067
Iteration 306/1000 | Loss: 0.00003067
Iteration 307/1000 | Loss: 0.00003067
Iteration 308/1000 | Loss: 0.00003067
Iteration 309/1000 | Loss: 0.00003067
Iteration 310/1000 | Loss: 0.00003067
Iteration 311/1000 | Loss: 0.00003067
Iteration 312/1000 | Loss: 0.00003067
Iteration 313/1000 | Loss: 0.00003067
Iteration 314/1000 | Loss: 0.00003067
Iteration 315/1000 | Loss: 0.00003067
Iteration 316/1000 | Loss: 0.00003067
Iteration 317/1000 | Loss: 0.00003067
Iteration 318/1000 | Loss: 0.00003067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [3.066514909733087e-05, 3.066514909733087e-05, 3.066514909733087e-05, 3.066514909733087e-05, 3.066514909733087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.066514909733087e-05

Optimization complete. Final v2v error: 4.20546817779541 mm

Highest mean error: 11.60464859008789 mm for frame 40

Lowest mean error: 3.4955873489379883 mm for frame 8

Saving results

Total time: 324.21030020713806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786463
Iteration 2/25 | Loss: 0.00136562
Iteration 3/25 | Loss: 0.00128614
Iteration 4/25 | Loss: 0.00127509
Iteration 5/25 | Loss: 0.00127190
Iteration 6/25 | Loss: 0.00127190
Iteration 7/25 | Loss: 0.00127190
Iteration 8/25 | Loss: 0.00127190
Iteration 9/25 | Loss: 0.00127190
Iteration 10/25 | Loss: 0.00127190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012719000224024057, 0.0012719000224024057, 0.0012719000224024057, 0.0012719000224024057, 0.0012719000224024057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012719000224024057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39299071
Iteration 2/25 | Loss: 0.00095142
Iteration 3/25 | Loss: 0.00095142
Iteration 4/25 | Loss: 0.00095142
Iteration 5/25 | Loss: 0.00095142
Iteration 6/25 | Loss: 0.00095142
Iteration 7/25 | Loss: 0.00095142
Iteration 8/25 | Loss: 0.00095142
Iteration 9/25 | Loss: 0.00095142
Iteration 10/25 | Loss: 0.00095142
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009514212724752724, 0.0009514212724752724, 0.0009514212724752724, 0.0009514212724752724, 0.0009514212724752724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009514212724752724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095142
Iteration 2/1000 | Loss: 0.00004636
Iteration 3/1000 | Loss: 0.00002625
Iteration 4/1000 | Loss: 0.00002074
Iteration 5/1000 | Loss: 0.00001874
Iteration 6/1000 | Loss: 0.00001743
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001605
Iteration 9/1000 | Loss: 0.00001562
Iteration 10/1000 | Loss: 0.00001558
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001536
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001492
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001482
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001479
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001479
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001479
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001478
Iteration 35/1000 | Loss: 0.00001478
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001478
Iteration 40/1000 | Loss: 0.00001477
Iteration 41/1000 | Loss: 0.00001477
Iteration 42/1000 | Loss: 0.00001476
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00001475
Iteration 45/1000 | Loss: 0.00001475
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001474
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001472
Iteration 52/1000 | Loss: 0.00001472
Iteration 53/1000 | Loss: 0.00001472
Iteration 54/1000 | Loss: 0.00001471
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001470
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001470
Iteration 63/1000 | Loss: 0.00001468
Iteration 64/1000 | Loss: 0.00001468
Iteration 65/1000 | Loss: 0.00001466
Iteration 66/1000 | Loss: 0.00001466
Iteration 67/1000 | Loss: 0.00001466
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001465
Iteration 70/1000 | Loss: 0.00001464
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001464
Iteration 77/1000 | Loss: 0.00001464
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001462
Iteration 83/1000 | Loss: 0.00001462
Iteration 84/1000 | Loss: 0.00001461
Iteration 85/1000 | Loss: 0.00001461
Iteration 86/1000 | Loss: 0.00001460
Iteration 87/1000 | Loss: 0.00001460
Iteration 88/1000 | Loss: 0.00001460
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001455
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001454
Iteration 103/1000 | Loss: 0.00001454
Iteration 104/1000 | Loss: 0.00001454
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001453
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001452
Iteration 111/1000 | Loss: 0.00001452
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001452
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001450
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001449
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001448
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001447
Iteration 137/1000 | Loss: 0.00001447
Iteration 138/1000 | Loss: 0.00001447
Iteration 139/1000 | Loss: 0.00001447
Iteration 140/1000 | Loss: 0.00001447
Iteration 141/1000 | Loss: 0.00001447
Iteration 142/1000 | Loss: 0.00001447
Iteration 143/1000 | Loss: 0.00001447
Iteration 144/1000 | Loss: 0.00001447
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001446
Iteration 149/1000 | Loss: 0.00001446
Iteration 150/1000 | Loss: 0.00001446
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Iteration 153/1000 | Loss: 0.00001446
Iteration 154/1000 | Loss: 0.00001445
Iteration 155/1000 | Loss: 0.00001445
Iteration 156/1000 | Loss: 0.00001445
Iteration 157/1000 | Loss: 0.00001445
Iteration 158/1000 | Loss: 0.00001445
Iteration 159/1000 | Loss: 0.00001445
Iteration 160/1000 | Loss: 0.00001444
Iteration 161/1000 | Loss: 0.00001444
Iteration 162/1000 | Loss: 0.00001444
Iteration 163/1000 | Loss: 0.00001444
Iteration 164/1000 | Loss: 0.00001443
Iteration 165/1000 | Loss: 0.00001443
Iteration 166/1000 | Loss: 0.00001443
Iteration 167/1000 | Loss: 0.00001443
Iteration 168/1000 | Loss: 0.00001442
Iteration 169/1000 | Loss: 0.00001442
Iteration 170/1000 | Loss: 0.00001442
Iteration 171/1000 | Loss: 0.00001441
Iteration 172/1000 | Loss: 0.00001441
Iteration 173/1000 | Loss: 0.00001441
Iteration 174/1000 | Loss: 0.00001440
Iteration 175/1000 | Loss: 0.00001440
Iteration 176/1000 | Loss: 0.00001440
Iteration 177/1000 | Loss: 0.00001440
Iteration 178/1000 | Loss: 0.00001440
Iteration 179/1000 | Loss: 0.00001440
Iteration 180/1000 | Loss: 0.00001440
Iteration 181/1000 | Loss: 0.00001440
Iteration 182/1000 | Loss: 0.00001440
Iteration 183/1000 | Loss: 0.00001440
Iteration 184/1000 | Loss: 0.00001439
Iteration 185/1000 | Loss: 0.00001439
Iteration 186/1000 | Loss: 0.00001439
Iteration 187/1000 | Loss: 0.00001439
Iteration 188/1000 | Loss: 0.00001439
Iteration 189/1000 | Loss: 0.00001439
Iteration 190/1000 | Loss: 0.00001439
Iteration 191/1000 | Loss: 0.00001439
Iteration 192/1000 | Loss: 0.00001439
Iteration 193/1000 | Loss: 0.00001439
Iteration 194/1000 | Loss: 0.00001439
Iteration 195/1000 | Loss: 0.00001439
Iteration 196/1000 | Loss: 0.00001438
Iteration 197/1000 | Loss: 0.00001438
Iteration 198/1000 | Loss: 0.00001438
Iteration 199/1000 | Loss: 0.00001438
Iteration 200/1000 | Loss: 0.00001438
Iteration 201/1000 | Loss: 0.00001438
Iteration 202/1000 | Loss: 0.00001438
Iteration 203/1000 | Loss: 0.00001438
Iteration 204/1000 | Loss: 0.00001438
Iteration 205/1000 | Loss: 0.00001438
Iteration 206/1000 | Loss: 0.00001438
Iteration 207/1000 | Loss: 0.00001438
Iteration 208/1000 | Loss: 0.00001438
Iteration 209/1000 | Loss: 0.00001438
Iteration 210/1000 | Loss: 0.00001437
Iteration 211/1000 | Loss: 0.00001437
Iteration 212/1000 | Loss: 0.00001437
Iteration 213/1000 | Loss: 0.00001437
Iteration 214/1000 | Loss: 0.00001437
Iteration 215/1000 | Loss: 0.00001437
Iteration 216/1000 | Loss: 0.00001437
Iteration 217/1000 | Loss: 0.00001437
Iteration 218/1000 | Loss: 0.00001437
Iteration 219/1000 | Loss: 0.00001437
Iteration 220/1000 | Loss: 0.00001437
Iteration 221/1000 | Loss: 0.00001436
Iteration 222/1000 | Loss: 0.00001436
Iteration 223/1000 | Loss: 0.00001436
Iteration 224/1000 | Loss: 0.00001436
Iteration 225/1000 | Loss: 0.00001436
Iteration 226/1000 | Loss: 0.00001436
Iteration 227/1000 | Loss: 0.00001436
Iteration 228/1000 | Loss: 0.00001436
Iteration 229/1000 | Loss: 0.00001436
Iteration 230/1000 | Loss: 0.00001436
Iteration 231/1000 | Loss: 0.00001436
Iteration 232/1000 | Loss: 0.00001435
Iteration 233/1000 | Loss: 0.00001435
Iteration 234/1000 | Loss: 0.00001435
Iteration 235/1000 | Loss: 0.00001435
Iteration 236/1000 | Loss: 0.00001435
Iteration 237/1000 | Loss: 0.00001434
Iteration 238/1000 | Loss: 0.00001434
Iteration 239/1000 | Loss: 0.00001434
Iteration 240/1000 | Loss: 0.00001434
Iteration 241/1000 | Loss: 0.00001434
Iteration 242/1000 | Loss: 0.00001434
Iteration 243/1000 | Loss: 0.00001434
Iteration 244/1000 | Loss: 0.00001433
Iteration 245/1000 | Loss: 0.00001433
Iteration 246/1000 | Loss: 0.00001433
Iteration 247/1000 | Loss: 0.00001433
Iteration 248/1000 | Loss: 0.00001433
Iteration 249/1000 | Loss: 0.00001433
Iteration 250/1000 | Loss: 0.00001433
Iteration 251/1000 | Loss: 0.00001433
Iteration 252/1000 | Loss: 0.00001433
Iteration 253/1000 | Loss: 0.00001433
Iteration 254/1000 | Loss: 0.00001432
Iteration 255/1000 | Loss: 0.00001432
Iteration 256/1000 | Loss: 0.00001432
Iteration 257/1000 | Loss: 0.00001432
Iteration 258/1000 | Loss: 0.00001432
Iteration 259/1000 | Loss: 0.00001432
Iteration 260/1000 | Loss: 0.00001432
Iteration 261/1000 | Loss: 0.00001432
Iteration 262/1000 | Loss: 0.00001432
Iteration 263/1000 | Loss: 0.00001432
Iteration 264/1000 | Loss: 0.00001432
Iteration 265/1000 | Loss: 0.00001432
Iteration 266/1000 | Loss: 0.00001431
Iteration 267/1000 | Loss: 0.00001431
Iteration 268/1000 | Loss: 0.00001431
Iteration 269/1000 | Loss: 0.00001431
Iteration 270/1000 | Loss: 0.00001431
Iteration 271/1000 | Loss: 0.00001431
Iteration 272/1000 | Loss: 0.00001431
Iteration 273/1000 | Loss: 0.00001431
Iteration 274/1000 | Loss: 0.00001431
Iteration 275/1000 | Loss: 0.00001431
Iteration 276/1000 | Loss: 0.00001431
Iteration 277/1000 | Loss: 0.00001431
Iteration 278/1000 | Loss: 0.00001431
Iteration 279/1000 | Loss: 0.00001430
Iteration 280/1000 | Loss: 0.00001430
Iteration 281/1000 | Loss: 0.00001430
Iteration 282/1000 | Loss: 0.00001430
Iteration 283/1000 | Loss: 0.00001429
Iteration 284/1000 | Loss: 0.00001429
Iteration 285/1000 | Loss: 0.00001429
Iteration 286/1000 | Loss: 0.00001429
Iteration 287/1000 | Loss: 0.00001429
Iteration 288/1000 | Loss: 0.00001429
Iteration 289/1000 | Loss: 0.00001429
Iteration 290/1000 | Loss: 0.00001429
Iteration 291/1000 | Loss: 0.00001429
Iteration 292/1000 | Loss: 0.00001428
Iteration 293/1000 | Loss: 0.00001428
Iteration 294/1000 | Loss: 0.00001428
Iteration 295/1000 | Loss: 0.00001428
Iteration 296/1000 | Loss: 0.00001428
Iteration 297/1000 | Loss: 0.00001428
Iteration 298/1000 | Loss: 0.00001428
Iteration 299/1000 | Loss: 0.00001428
Iteration 300/1000 | Loss: 0.00001428
Iteration 301/1000 | Loss: 0.00001428
Iteration 302/1000 | Loss: 0.00001428
Iteration 303/1000 | Loss: 0.00001428
Iteration 304/1000 | Loss: 0.00001428
Iteration 305/1000 | Loss: 0.00001428
Iteration 306/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [1.428310224582674e-05, 1.428310224582674e-05, 1.428310224582674e-05, 1.428310224582674e-05, 1.428310224582674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.428310224582674e-05

Optimization complete. Final v2v error: 3.252790689468384 mm

Highest mean error: 3.7696785926818848 mm for frame 239

Lowest mean error: 2.9657511711120605 mm for frame 173

Saving results

Total time: 51.62031102180481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506935
Iteration 2/25 | Loss: 0.00141397
Iteration 3/25 | Loss: 0.00134672
Iteration 4/25 | Loss: 0.00134089
Iteration 5/25 | Loss: 0.00133977
Iteration 6/25 | Loss: 0.00133977
Iteration 7/25 | Loss: 0.00133977
Iteration 8/25 | Loss: 0.00133977
Iteration 9/25 | Loss: 0.00133977
Iteration 10/25 | Loss: 0.00133977
Iteration 11/25 | Loss: 0.00133977
Iteration 12/25 | Loss: 0.00133977
Iteration 13/25 | Loss: 0.00133977
Iteration 14/25 | Loss: 0.00133977
Iteration 15/25 | Loss: 0.00133977
Iteration 16/25 | Loss: 0.00133977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013397663133218884, 0.0013397663133218884, 0.0013397663133218884, 0.0013397663133218884, 0.0013397663133218884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013397663133218884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85461825
Iteration 2/25 | Loss: 0.00074930
Iteration 3/25 | Loss: 0.00074929
Iteration 4/25 | Loss: 0.00074929
Iteration 5/25 | Loss: 0.00074929
Iteration 6/25 | Loss: 0.00074929
Iteration 7/25 | Loss: 0.00074929
Iteration 8/25 | Loss: 0.00074929
Iteration 9/25 | Loss: 0.00074929
Iteration 10/25 | Loss: 0.00074929
Iteration 11/25 | Loss: 0.00074929
Iteration 12/25 | Loss: 0.00074929
Iteration 13/25 | Loss: 0.00074929
Iteration 14/25 | Loss: 0.00074929
Iteration 15/25 | Loss: 0.00074929
Iteration 16/25 | Loss: 0.00074929
Iteration 17/25 | Loss: 0.00074929
Iteration 18/25 | Loss: 0.00074929
Iteration 19/25 | Loss: 0.00074929
Iteration 20/25 | Loss: 0.00074929
Iteration 21/25 | Loss: 0.00074929
Iteration 22/25 | Loss: 0.00074929
Iteration 23/25 | Loss: 0.00074929
Iteration 24/25 | Loss: 0.00074929
Iteration 25/25 | Loss: 0.00074929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074929
Iteration 2/1000 | Loss: 0.00003035
Iteration 3/1000 | Loss: 0.00002204
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001918
Iteration 6/1000 | Loss: 0.00001858
Iteration 7/1000 | Loss: 0.00001830
Iteration 8/1000 | Loss: 0.00001823
Iteration 9/1000 | Loss: 0.00001802
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001707
Iteration 13/1000 | Loss: 0.00001690
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001662
Iteration 17/1000 | Loss: 0.00001661
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001652
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001643
Iteration 30/1000 | Loss: 0.00001640
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001637
Iteration 37/1000 | Loss: 0.00001636
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001632
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001629
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001628
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001626
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001621
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001621
Iteration 97/1000 | Loss: 0.00001621
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001621
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001621
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.620549119252246e-05, 1.620549119252246e-05, 1.620549119252246e-05, 1.620549119252246e-05, 1.620549119252246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.620549119252246e-05

Optimization complete. Final v2v error: 3.3908073902130127 mm

Highest mean error: 3.423105239868164 mm for frame 157

Lowest mean error: 3.3388428688049316 mm for frame 69

Saving results

Total time: 35.64367938041687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795657
Iteration 2/25 | Loss: 0.00147195
Iteration 3/25 | Loss: 0.00135830
Iteration 4/25 | Loss: 0.00131679
Iteration 5/25 | Loss: 0.00130936
Iteration 6/25 | Loss: 0.00130678
Iteration 7/25 | Loss: 0.00130637
Iteration 8/25 | Loss: 0.00130618
Iteration 9/25 | Loss: 0.00130610
Iteration 10/25 | Loss: 0.00130610
Iteration 11/25 | Loss: 0.00130610
Iteration 12/25 | Loss: 0.00130610
Iteration 13/25 | Loss: 0.00130610
Iteration 14/25 | Loss: 0.00130609
Iteration 15/25 | Loss: 0.00130609
Iteration 16/25 | Loss: 0.00130609
Iteration 17/25 | Loss: 0.00130609
Iteration 18/25 | Loss: 0.00130609
Iteration 19/25 | Loss: 0.00130609
Iteration 20/25 | Loss: 0.00130609
Iteration 21/25 | Loss: 0.00130609
Iteration 22/25 | Loss: 0.00130609
Iteration 23/25 | Loss: 0.00130609
Iteration 24/25 | Loss: 0.00130609
Iteration 25/25 | Loss: 0.00130609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49059868
Iteration 2/25 | Loss: 0.00092529
Iteration 3/25 | Loss: 0.00092528
Iteration 4/25 | Loss: 0.00092528
Iteration 5/25 | Loss: 0.00092527
Iteration 6/25 | Loss: 0.00092527
Iteration 7/25 | Loss: 0.00092527
Iteration 8/25 | Loss: 0.00092527
Iteration 9/25 | Loss: 0.00092527
Iteration 10/25 | Loss: 0.00092527
Iteration 11/25 | Loss: 0.00092527
Iteration 12/25 | Loss: 0.00092527
Iteration 13/25 | Loss: 0.00092527
Iteration 14/25 | Loss: 0.00092527
Iteration 15/25 | Loss: 0.00092527
Iteration 16/25 | Loss: 0.00092527
Iteration 17/25 | Loss: 0.00092527
Iteration 18/25 | Loss: 0.00092527
Iteration 19/25 | Loss: 0.00092527
Iteration 20/25 | Loss: 0.00092527
Iteration 21/25 | Loss: 0.00092527
Iteration 22/25 | Loss: 0.00092527
Iteration 23/25 | Loss: 0.00092527
Iteration 24/25 | Loss: 0.00092527
Iteration 25/25 | Loss: 0.00092527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092527
Iteration 2/1000 | Loss: 0.00002965
Iteration 3/1000 | Loss: 0.00002123
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001684
Iteration 6/1000 | Loss: 0.00001610
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001510
Iteration 9/1000 | Loss: 0.00001482
Iteration 10/1000 | Loss: 0.00001460
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001423
Iteration 13/1000 | Loss: 0.00001407
Iteration 14/1000 | Loss: 0.00001406
Iteration 15/1000 | Loss: 0.00001406
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001384
Iteration 25/1000 | Loss: 0.00001383
Iteration 26/1000 | Loss: 0.00001382
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001380
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001379
Iteration 34/1000 | Loss: 0.00001379
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001378
Iteration 37/1000 | Loss: 0.00001378
Iteration 38/1000 | Loss: 0.00001377
Iteration 39/1000 | Loss: 0.00001377
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001377
Iteration 42/1000 | Loss: 0.00001377
Iteration 43/1000 | Loss: 0.00001376
Iteration 44/1000 | Loss: 0.00001376
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001376
Iteration 47/1000 | Loss: 0.00001376
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001373
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001373
Iteration 62/1000 | Loss: 0.00001373
Iteration 63/1000 | Loss: 0.00001373
Iteration 64/1000 | Loss: 0.00001373
Iteration 65/1000 | Loss: 0.00001373
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001372
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001371
Iteration 72/1000 | Loss: 0.00001371
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001370
Iteration 76/1000 | Loss: 0.00001370
Iteration 77/1000 | Loss: 0.00001370
Iteration 78/1000 | Loss: 0.00001369
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001368
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001365
Iteration 92/1000 | Loss: 0.00001365
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001364
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001362
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001361
Iteration 106/1000 | Loss: 0.00001361
Iteration 107/1000 | Loss: 0.00001361
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001360
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001360
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001358
Iteration 117/1000 | Loss: 0.00001358
Iteration 118/1000 | Loss: 0.00001358
Iteration 119/1000 | Loss: 0.00001357
Iteration 120/1000 | Loss: 0.00001357
Iteration 121/1000 | Loss: 0.00001357
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001356
Iteration 126/1000 | Loss: 0.00001356
Iteration 127/1000 | Loss: 0.00001356
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001355
Iteration 131/1000 | Loss: 0.00001355
Iteration 132/1000 | Loss: 0.00001355
Iteration 133/1000 | Loss: 0.00001355
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001355
Iteration 137/1000 | Loss: 0.00001355
Iteration 138/1000 | Loss: 0.00001355
Iteration 139/1000 | Loss: 0.00001355
Iteration 140/1000 | Loss: 0.00001355
Iteration 141/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3553585631598253e-05, 1.3553585631598253e-05, 1.3553585631598253e-05, 1.3553585631598253e-05, 1.3553585631598253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3553585631598253e-05

Optimization complete. Final v2v error: 3.0931854248046875 mm

Highest mean error: 3.8374640941619873 mm for frame 49

Lowest mean error: 2.738471269607544 mm for frame 30

Saving results

Total time: 43.31398606300354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764831
Iteration 2/25 | Loss: 0.00181161
Iteration 3/25 | Loss: 0.00135928
Iteration 4/25 | Loss: 0.00130625
Iteration 5/25 | Loss: 0.00129637
Iteration 6/25 | Loss: 0.00129819
Iteration 7/25 | Loss: 0.00129544
Iteration 8/25 | Loss: 0.00128979
Iteration 9/25 | Loss: 0.00128728
Iteration 10/25 | Loss: 0.00128472
Iteration 11/25 | Loss: 0.00128287
Iteration 12/25 | Loss: 0.00128224
Iteration 13/25 | Loss: 0.00128200
Iteration 14/25 | Loss: 0.00128190
Iteration 15/25 | Loss: 0.00128190
Iteration 16/25 | Loss: 0.00128190
Iteration 17/25 | Loss: 0.00128190
Iteration 18/25 | Loss: 0.00128190
Iteration 19/25 | Loss: 0.00128189
Iteration 20/25 | Loss: 0.00128189
Iteration 21/25 | Loss: 0.00128189
Iteration 22/25 | Loss: 0.00128189
Iteration 23/25 | Loss: 0.00128189
Iteration 24/25 | Loss: 0.00128189
Iteration 25/25 | Loss: 0.00128189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92929864
Iteration 2/25 | Loss: 0.00086437
Iteration 3/25 | Loss: 0.00086436
Iteration 4/25 | Loss: 0.00086436
Iteration 5/25 | Loss: 0.00086436
Iteration 6/25 | Loss: 0.00086436
Iteration 7/25 | Loss: 0.00086436
Iteration 8/25 | Loss: 0.00086436
Iteration 9/25 | Loss: 0.00086436
Iteration 10/25 | Loss: 0.00086436
Iteration 11/25 | Loss: 0.00086436
Iteration 12/25 | Loss: 0.00086436
Iteration 13/25 | Loss: 0.00086436
Iteration 14/25 | Loss: 0.00086436
Iteration 15/25 | Loss: 0.00086436
Iteration 16/25 | Loss: 0.00086436
Iteration 17/25 | Loss: 0.00086436
Iteration 18/25 | Loss: 0.00086436
Iteration 19/25 | Loss: 0.00086436
Iteration 20/25 | Loss: 0.00086436
Iteration 21/25 | Loss: 0.00086436
Iteration 22/25 | Loss: 0.00086436
Iteration 23/25 | Loss: 0.00086436
Iteration 24/25 | Loss: 0.00086436
Iteration 25/25 | Loss: 0.00086436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086436
Iteration 2/1000 | Loss: 0.00002512
Iteration 3/1000 | Loss: 0.00002048
Iteration 4/1000 | Loss: 0.00001904
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00001748
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001631
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001589
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001530
Iteration 18/1000 | Loss: 0.00001530
Iteration 19/1000 | Loss: 0.00001529
Iteration 20/1000 | Loss: 0.00001528
Iteration 21/1000 | Loss: 0.00001528
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001525
Iteration 24/1000 | Loss: 0.00001525
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001524
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001523
Iteration 29/1000 | Loss: 0.00001523
Iteration 30/1000 | Loss: 0.00001522
Iteration 31/1000 | Loss: 0.00001522
Iteration 32/1000 | Loss: 0.00001522
Iteration 33/1000 | Loss: 0.00001521
Iteration 34/1000 | Loss: 0.00001521
Iteration 35/1000 | Loss: 0.00001521
Iteration 36/1000 | Loss: 0.00001521
Iteration 37/1000 | Loss: 0.00001521
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001521
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001521
Iteration 46/1000 | Loss: 0.00001521
Iteration 47/1000 | Loss: 0.00001521
Iteration 48/1000 | Loss: 0.00001521
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001521
Iteration 57/1000 | Loss: 0.00001521
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001518
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001518
Iteration 65/1000 | Loss: 0.00001515
Iteration 66/1000 | Loss: 0.00001514
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001508
Iteration 80/1000 | Loss: 0.00001508
Iteration 81/1000 | Loss: 0.00001508
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001507
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001507
Iteration 89/1000 | Loss: 0.00001507
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001504
Iteration 106/1000 | Loss: 0.00001504
Iteration 107/1000 | Loss: 0.00001504
Iteration 108/1000 | Loss: 0.00001504
Iteration 109/1000 | Loss: 0.00001504
Iteration 110/1000 | Loss: 0.00001504
Iteration 111/1000 | Loss: 0.00001504
Iteration 112/1000 | Loss: 0.00001504
Iteration 113/1000 | Loss: 0.00001504
Iteration 114/1000 | Loss: 0.00001503
Iteration 115/1000 | Loss: 0.00001503
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001502
Iteration 120/1000 | Loss: 0.00001502
Iteration 121/1000 | Loss: 0.00001502
Iteration 122/1000 | Loss: 0.00001502
Iteration 123/1000 | Loss: 0.00001502
Iteration 124/1000 | Loss: 0.00001502
Iteration 125/1000 | Loss: 0.00001501
Iteration 126/1000 | Loss: 0.00001501
Iteration 127/1000 | Loss: 0.00001501
Iteration 128/1000 | Loss: 0.00001501
Iteration 129/1000 | Loss: 0.00001501
Iteration 130/1000 | Loss: 0.00001501
Iteration 131/1000 | Loss: 0.00001501
Iteration 132/1000 | Loss: 0.00001501
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001501
Iteration 137/1000 | Loss: 0.00001501
Iteration 138/1000 | Loss: 0.00001501
Iteration 139/1000 | Loss: 0.00001501
Iteration 140/1000 | Loss: 0.00001501
Iteration 141/1000 | Loss: 0.00001501
Iteration 142/1000 | Loss: 0.00001501
Iteration 143/1000 | Loss: 0.00001501
Iteration 144/1000 | Loss: 0.00001501
Iteration 145/1000 | Loss: 0.00001501
Iteration 146/1000 | Loss: 0.00001501
Iteration 147/1000 | Loss: 0.00001501
Iteration 148/1000 | Loss: 0.00001501
Iteration 149/1000 | Loss: 0.00001501
Iteration 150/1000 | Loss: 0.00001501
Iteration 151/1000 | Loss: 0.00001501
Iteration 152/1000 | Loss: 0.00001501
Iteration 153/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.5012465155450627e-05, 1.5012465155450627e-05, 1.5012465155450627e-05, 1.5012465155450627e-05, 1.5012465155450627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5012465155450627e-05

Optimization complete. Final v2v error: 3.3248825073242188 mm

Highest mean error: 3.7814528942108154 mm for frame 81

Lowest mean error: 3.114069700241089 mm for frame 9

Saving results

Total time: 56.147749185562134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584135
Iteration 2/25 | Loss: 0.00141493
Iteration 3/25 | Loss: 0.00131103
Iteration 4/25 | Loss: 0.00128698
Iteration 5/25 | Loss: 0.00128241
Iteration 6/25 | Loss: 0.00128110
Iteration 7/25 | Loss: 0.00128110
Iteration 8/25 | Loss: 0.00128110
Iteration 9/25 | Loss: 0.00128110
Iteration 10/25 | Loss: 0.00128110
Iteration 11/25 | Loss: 0.00128110
Iteration 12/25 | Loss: 0.00128110
Iteration 13/25 | Loss: 0.00128110
Iteration 14/25 | Loss: 0.00128110
Iteration 15/25 | Loss: 0.00128110
Iteration 16/25 | Loss: 0.00128110
Iteration 17/25 | Loss: 0.00128110
Iteration 18/25 | Loss: 0.00128110
Iteration 19/25 | Loss: 0.00128110
Iteration 20/25 | Loss: 0.00128110
Iteration 21/25 | Loss: 0.00128110
Iteration 22/25 | Loss: 0.00128110
Iteration 23/25 | Loss: 0.00128110
Iteration 24/25 | Loss: 0.00128110
Iteration 25/25 | Loss: 0.00128110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55487239
Iteration 2/25 | Loss: 0.00113450
Iteration 3/25 | Loss: 0.00113450
Iteration 4/25 | Loss: 0.00113450
Iteration 5/25 | Loss: 0.00113450
Iteration 6/25 | Loss: 0.00113450
Iteration 7/25 | Loss: 0.00113450
Iteration 8/25 | Loss: 0.00113450
Iteration 9/25 | Loss: 0.00113450
Iteration 10/25 | Loss: 0.00113450
Iteration 11/25 | Loss: 0.00113450
Iteration 12/25 | Loss: 0.00113450
Iteration 13/25 | Loss: 0.00113450
Iteration 14/25 | Loss: 0.00113450
Iteration 15/25 | Loss: 0.00113450
Iteration 16/25 | Loss: 0.00113450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011344971135258675, 0.0011344971135258675, 0.0011344971135258675, 0.0011344971135258675, 0.0011344971135258675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011344971135258675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113450
Iteration 2/1000 | Loss: 0.00005551
Iteration 3/1000 | Loss: 0.00003366
Iteration 4/1000 | Loss: 0.00002843
Iteration 5/1000 | Loss: 0.00002622
Iteration 6/1000 | Loss: 0.00002480
Iteration 7/1000 | Loss: 0.00002389
Iteration 8/1000 | Loss: 0.00002315
Iteration 9/1000 | Loss: 0.00002250
Iteration 10/1000 | Loss: 0.00002211
Iteration 11/1000 | Loss: 0.00002181
Iteration 12/1000 | Loss: 0.00002170
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002154
Iteration 15/1000 | Loss: 0.00002147
Iteration 16/1000 | Loss: 0.00002127
Iteration 17/1000 | Loss: 0.00002119
Iteration 18/1000 | Loss: 0.00002114
Iteration 19/1000 | Loss: 0.00002111
Iteration 20/1000 | Loss: 0.00002102
Iteration 21/1000 | Loss: 0.00002099
Iteration 22/1000 | Loss: 0.00002098
Iteration 23/1000 | Loss: 0.00002098
Iteration 24/1000 | Loss: 0.00002098
Iteration 25/1000 | Loss: 0.00002098
Iteration 26/1000 | Loss: 0.00002097
Iteration 27/1000 | Loss: 0.00002097
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002091
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002087
Iteration 33/1000 | Loss: 0.00002087
Iteration 34/1000 | Loss: 0.00002087
Iteration 35/1000 | Loss: 0.00002087
Iteration 36/1000 | Loss: 0.00002086
Iteration 37/1000 | Loss: 0.00002086
Iteration 38/1000 | Loss: 0.00002085
Iteration 39/1000 | Loss: 0.00002081
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002078
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002076
Iteration 44/1000 | Loss: 0.00002076
Iteration 45/1000 | Loss: 0.00002075
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002075
Iteration 48/1000 | Loss: 0.00002075
Iteration 49/1000 | Loss: 0.00002075
Iteration 50/1000 | Loss: 0.00002075
Iteration 51/1000 | Loss: 0.00002075
Iteration 52/1000 | Loss: 0.00002075
Iteration 53/1000 | Loss: 0.00002074
Iteration 54/1000 | Loss: 0.00002074
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002069
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002068
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002066
Iteration 75/1000 | Loss: 0.00002066
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002066
Iteration 81/1000 | Loss: 0.00002066
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002066
Iteration 85/1000 | Loss: 0.00002066
Iteration 86/1000 | Loss: 0.00002066
Iteration 87/1000 | Loss: 0.00002066
Iteration 88/1000 | Loss: 0.00002066
Iteration 89/1000 | Loss: 0.00002066
Iteration 90/1000 | Loss: 0.00002066
Iteration 91/1000 | Loss: 0.00002066
Iteration 92/1000 | Loss: 0.00002066
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00002066
Iteration 97/1000 | Loss: 0.00002066
Iteration 98/1000 | Loss: 0.00002066
Iteration 99/1000 | Loss: 0.00002066
Iteration 100/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.066199704131577e-05, 2.066199704131577e-05, 2.066199704131577e-05, 2.066199704131577e-05, 2.066199704131577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.066199704131577e-05

Optimization complete. Final v2v error: 3.938016414642334 mm

Highest mean error: 4.279955863952637 mm for frame 38

Lowest mean error: 3.781973361968994 mm for frame 239

Saving results

Total time: 42.8703773021698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812293
Iteration 2/25 | Loss: 0.00174692
Iteration 3/25 | Loss: 0.00149335
Iteration 4/25 | Loss: 0.00147622
Iteration 5/25 | Loss: 0.00147107
Iteration 6/25 | Loss: 0.00146973
Iteration 7/25 | Loss: 0.00146973
Iteration 8/25 | Loss: 0.00146972
Iteration 9/25 | Loss: 0.00146972
Iteration 10/25 | Loss: 0.00146972
Iteration 11/25 | Loss: 0.00146972
Iteration 12/25 | Loss: 0.00146972
Iteration 13/25 | Loss: 0.00146972
Iteration 14/25 | Loss: 0.00146972
Iteration 15/25 | Loss: 0.00146972
Iteration 16/25 | Loss: 0.00146972
Iteration 17/25 | Loss: 0.00146972
Iteration 18/25 | Loss: 0.00146972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014697163132950664, 0.0014697163132950664, 0.0014697163132950664, 0.0014697163132950664, 0.0014697163132950664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014697163132950664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.37583032
Iteration 2/25 | Loss: 0.00096856
Iteration 3/25 | Loss: 0.00096854
Iteration 4/25 | Loss: 0.00096854
Iteration 5/25 | Loss: 0.00096854
Iteration 6/25 | Loss: 0.00096854
Iteration 7/25 | Loss: 0.00096854
Iteration 8/25 | Loss: 0.00096854
Iteration 9/25 | Loss: 0.00096854
Iteration 10/25 | Loss: 0.00096853
Iteration 11/25 | Loss: 0.00096853
Iteration 12/25 | Loss: 0.00096853
Iteration 13/25 | Loss: 0.00096853
Iteration 14/25 | Loss: 0.00096853
Iteration 15/25 | Loss: 0.00096853
Iteration 16/25 | Loss: 0.00096853
Iteration 17/25 | Loss: 0.00096853
Iteration 18/25 | Loss: 0.00096853
Iteration 19/25 | Loss: 0.00096853
Iteration 20/25 | Loss: 0.00096853
Iteration 21/25 | Loss: 0.00096853
Iteration 22/25 | Loss: 0.00096853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009685345576144755, 0.0009685345576144755, 0.0009685345576144755, 0.0009685345576144755, 0.0009685345576144755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009685345576144755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096853
Iteration 2/1000 | Loss: 0.00008924
Iteration 3/1000 | Loss: 0.00005443
Iteration 4/1000 | Loss: 0.00004592
Iteration 5/1000 | Loss: 0.00004286
Iteration 6/1000 | Loss: 0.00004102
Iteration 7/1000 | Loss: 0.00003974
Iteration 8/1000 | Loss: 0.00003861
Iteration 9/1000 | Loss: 0.00003778
Iteration 10/1000 | Loss: 0.00003712
Iteration 11/1000 | Loss: 0.00003673
Iteration 12/1000 | Loss: 0.00003633
Iteration 13/1000 | Loss: 0.00003607
Iteration 14/1000 | Loss: 0.00003572
Iteration 15/1000 | Loss: 0.00003550
Iteration 16/1000 | Loss: 0.00003533
Iteration 17/1000 | Loss: 0.00003517
Iteration 18/1000 | Loss: 0.00003504
Iteration 19/1000 | Loss: 0.00003488
Iteration 20/1000 | Loss: 0.00003483
Iteration 21/1000 | Loss: 0.00003482
Iteration 22/1000 | Loss: 0.00003480
Iteration 23/1000 | Loss: 0.00003469
Iteration 24/1000 | Loss: 0.00003460
Iteration 25/1000 | Loss: 0.00003457
Iteration 26/1000 | Loss: 0.00003457
Iteration 27/1000 | Loss: 0.00003456
Iteration 28/1000 | Loss: 0.00003456
Iteration 29/1000 | Loss: 0.00003455
Iteration 30/1000 | Loss: 0.00003455
Iteration 31/1000 | Loss: 0.00003454
Iteration 32/1000 | Loss: 0.00003454
Iteration 33/1000 | Loss: 0.00003451
Iteration 34/1000 | Loss: 0.00003449
Iteration 35/1000 | Loss: 0.00003449
Iteration 36/1000 | Loss: 0.00003449
Iteration 37/1000 | Loss: 0.00003449
Iteration 38/1000 | Loss: 0.00003448
Iteration 39/1000 | Loss: 0.00003448
Iteration 40/1000 | Loss: 0.00003447
Iteration 41/1000 | Loss: 0.00003446
Iteration 42/1000 | Loss: 0.00003446
Iteration 43/1000 | Loss: 0.00003446
Iteration 44/1000 | Loss: 0.00003445
Iteration 45/1000 | Loss: 0.00003445
Iteration 46/1000 | Loss: 0.00003445
Iteration 47/1000 | Loss: 0.00003445
Iteration 48/1000 | Loss: 0.00003445
Iteration 49/1000 | Loss: 0.00003445
Iteration 50/1000 | Loss: 0.00003445
Iteration 51/1000 | Loss: 0.00003445
Iteration 52/1000 | Loss: 0.00003445
Iteration 53/1000 | Loss: 0.00003445
Iteration 54/1000 | Loss: 0.00003444
Iteration 55/1000 | Loss: 0.00003443
Iteration 56/1000 | Loss: 0.00003443
Iteration 57/1000 | Loss: 0.00003443
Iteration 58/1000 | Loss: 0.00003442
Iteration 59/1000 | Loss: 0.00003442
Iteration 60/1000 | Loss: 0.00003442
Iteration 61/1000 | Loss: 0.00003442
Iteration 62/1000 | Loss: 0.00003442
Iteration 63/1000 | Loss: 0.00003442
Iteration 64/1000 | Loss: 0.00003442
Iteration 65/1000 | Loss: 0.00003442
Iteration 66/1000 | Loss: 0.00003442
Iteration 67/1000 | Loss: 0.00003442
Iteration 68/1000 | Loss: 0.00003440
Iteration 69/1000 | Loss: 0.00003440
Iteration 70/1000 | Loss: 0.00003440
Iteration 71/1000 | Loss: 0.00003439
Iteration 72/1000 | Loss: 0.00003439
Iteration 73/1000 | Loss: 0.00003439
Iteration 74/1000 | Loss: 0.00003439
Iteration 75/1000 | Loss: 0.00003438
Iteration 76/1000 | Loss: 0.00003438
Iteration 77/1000 | Loss: 0.00003438
Iteration 78/1000 | Loss: 0.00003437
Iteration 79/1000 | Loss: 0.00003437
Iteration 80/1000 | Loss: 0.00003436
Iteration 81/1000 | Loss: 0.00003436
Iteration 82/1000 | Loss: 0.00003436
Iteration 83/1000 | Loss: 0.00003436
Iteration 84/1000 | Loss: 0.00003436
Iteration 85/1000 | Loss: 0.00003436
Iteration 86/1000 | Loss: 0.00003435
Iteration 87/1000 | Loss: 0.00003435
Iteration 88/1000 | Loss: 0.00003435
Iteration 89/1000 | Loss: 0.00003435
Iteration 90/1000 | Loss: 0.00003434
Iteration 91/1000 | Loss: 0.00003434
Iteration 92/1000 | Loss: 0.00003434
Iteration 93/1000 | Loss: 0.00003434
Iteration 94/1000 | Loss: 0.00003434
Iteration 95/1000 | Loss: 0.00003434
Iteration 96/1000 | Loss: 0.00003433
Iteration 97/1000 | Loss: 0.00003433
Iteration 98/1000 | Loss: 0.00003433
Iteration 99/1000 | Loss: 0.00003433
Iteration 100/1000 | Loss: 0.00003433
Iteration 101/1000 | Loss: 0.00003433
Iteration 102/1000 | Loss: 0.00003433
Iteration 103/1000 | Loss: 0.00003432
Iteration 104/1000 | Loss: 0.00003432
Iteration 105/1000 | Loss: 0.00003432
Iteration 106/1000 | Loss: 0.00003432
Iteration 107/1000 | Loss: 0.00003432
Iteration 108/1000 | Loss: 0.00003432
Iteration 109/1000 | Loss: 0.00003432
Iteration 110/1000 | Loss: 0.00003432
Iteration 111/1000 | Loss: 0.00003432
Iteration 112/1000 | Loss: 0.00003432
Iteration 113/1000 | Loss: 0.00003432
Iteration 114/1000 | Loss: 0.00003432
Iteration 115/1000 | Loss: 0.00003432
Iteration 116/1000 | Loss: 0.00003432
Iteration 117/1000 | Loss: 0.00003432
Iteration 118/1000 | Loss: 0.00003432
Iteration 119/1000 | Loss: 0.00003432
Iteration 120/1000 | Loss: 0.00003432
Iteration 121/1000 | Loss: 0.00003432
Iteration 122/1000 | Loss: 0.00003432
Iteration 123/1000 | Loss: 0.00003432
Iteration 124/1000 | Loss: 0.00003432
Iteration 125/1000 | Loss: 0.00003432
Iteration 126/1000 | Loss: 0.00003432
Iteration 127/1000 | Loss: 0.00003432
Iteration 128/1000 | Loss: 0.00003432
Iteration 129/1000 | Loss: 0.00003432
Iteration 130/1000 | Loss: 0.00003432
Iteration 131/1000 | Loss: 0.00003432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.431661752983928e-05, 3.431661752983928e-05, 3.431661752983928e-05, 3.431661752983928e-05, 3.431661752983928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.431661752983928e-05

Optimization complete. Final v2v error: 4.71523380279541 mm

Highest mean error: 6.181282043457031 mm for frame 58

Lowest mean error: 3.4606454372406006 mm for frame 97

Saving results

Total time: 49.640666246414185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949095
Iteration 2/25 | Loss: 0.00949094
Iteration 3/25 | Loss: 0.00232638
Iteration 4/25 | Loss: 0.00163016
Iteration 5/25 | Loss: 0.00160391
Iteration 6/25 | Loss: 0.00156926
Iteration 7/25 | Loss: 0.00143286
Iteration 8/25 | Loss: 0.00139979
Iteration 9/25 | Loss: 0.00138360
Iteration 10/25 | Loss: 0.00138120
Iteration 11/25 | Loss: 0.00138177
Iteration 12/25 | Loss: 0.00138103
Iteration 13/25 | Loss: 0.00137053
Iteration 14/25 | Loss: 0.00136700
Iteration 15/25 | Loss: 0.00136909
Iteration 16/25 | Loss: 0.00136777
Iteration 17/25 | Loss: 0.00136524
Iteration 18/25 | Loss: 0.00136404
Iteration 19/25 | Loss: 0.00136377
Iteration 20/25 | Loss: 0.00136368
Iteration 21/25 | Loss: 0.00136454
Iteration 22/25 | Loss: 0.00136353
Iteration 23/25 | Loss: 0.00136302
Iteration 24/25 | Loss: 0.00136278
Iteration 25/25 | Loss: 0.00136273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38102794
Iteration 2/25 | Loss: 0.00081481
Iteration 3/25 | Loss: 0.00076518
Iteration 4/25 | Loss: 0.00076518
Iteration 5/25 | Loss: 0.00076518
Iteration 6/25 | Loss: 0.00076518
Iteration 7/25 | Loss: 0.00076518
Iteration 8/25 | Loss: 0.00076518
Iteration 9/25 | Loss: 0.00076518
Iteration 10/25 | Loss: 0.00076518
Iteration 11/25 | Loss: 0.00076518
Iteration 12/25 | Loss: 0.00076518
Iteration 13/25 | Loss: 0.00076518
Iteration 14/25 | Loss: 0.00076518
Iteration 15/25 | Loss: 0.00076518
Iteration 16/25 | Loss: 0.00076518
Iteration 17/25 | Loss: 0.00076518
Iteration 18/25 | Loss: 0.00076518
Iteration 19/25 | Loss: 0.00076518
Iteration 20/25 | Loss: 0.00076518
Iteration 21/25 | Loss: 0.00076518
Iteration 22/25 | Loss: 0.00076518
Iteration 23/25 | Loss: 0.00076518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007651810301467776, 0.0007651810301467776, 0.0007651810301467776, 0.0007651810301467776, 0.0007651810301467776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007651810301467776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076518
Iteration 2/1000 | Loss: 0.00004040
Iteration 3/1000 | Loss: 0.00002543
Iteration 4/1000 | Loss: 0.00002286
Iteration 5/1000 | Loss: 0.00002155
Iteration 6/1000 | Loss: 0.00002086
Iteration 7/1000 | Loss: 0.00002021
Iteration 8/1000 | Loss: 0.00001987
Iteration 9/1000 | Loss: 0.00001945
Iteration 10/1000 | Loss: 0.00001911
Iteration 11/1000 | Loss: 0.00001886
Iteration 12/1000 | Loss: 0.00001869
Iteration 13/1000 | Loss: 0.00001869
Iteration 14/1000 | Loss: 0.00001868
Iteration 15/1000 | Loss: 0.00001866
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001858
Iteration 19/1000 | Loss: 0.00001856
Iteration 20/1000 | Loss: 0.00001854
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001853
Iteration 23/1000 | Loss: 0.00001853
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001853
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001852
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001852
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001852
Iteration 33/1000 | Loss: 0.00001852
Iteration 34/1000 | Loss: 0.00001852
Iteration 35/1000 | Loss: 0.00001852
Iteration 36/1000 | Loss: 0.00001852
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001841
Iteration 46/1000 | Loss: 0.00001841
Iteration 47/1000 | Loss: 0.00001841
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001837
Iteration 50/1000 | Loss: 0.00001837
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001836
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001836
Iteration 61/1000 | Loss: 0.00001836
Iteration 62/1000 | Loss: 0.00001836
Iteration 63/1000 | Loss: 0.00001836
Iteration 64/1000 | Loss: 0.00001836
Iteration 65/1000 | Loss: 0.00001836
Iteration 66/1000 | Loss: 0.00001835
Iteration 67/1000 | Loss: 0.00001835
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001833
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001827
Iteration 83/1000 | Loss: 0.00001827
Iteration 84/1000 | Loss: 0.00001826
Iteration 85/1000 | Loss: 0.00001826
Iteration 86/1000 | Loss: 0.00001826
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001825
Iteration 89/1000 | Loss: 0.00001825
Iteration 90/1000 | Loss: 0.00001824
Iteration 91/1000 | Loss: 0.00001824
Iteration 92/1000 | Loss: 0.00001824
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001823
Iteration 95/1000 | Loss: 0.00001823
Iteration 96/1000 | Loss: 0.00001823
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001822
Iteration 102/1000 | Loss: 0.00001822
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001821
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001820
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001819
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001818
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001817
Iteration 126/1000 | Loss: 0.00001817
Iteration 127/1000 | Loss: 0.00001817
Iteration 128/1000 | Loss: 0.00001817
Iteration 129/1000 | Loss: 0.00001817
Iteration 130/1000 | Loss: 0.00001817
Iteration 131/1000 | Loss: 0.00001817
Iteration 132/1000 | Loss: 0.00001817
Iteration 133/1000 | Loss: 0.00001817
Iteration 134/1000 | Loss: 0.00001817
Iteration 135/1000 | Loss: 0.00001816
Iteration 136/1000 | Loss: 0.00001816
Iteration 137/1000 | Loss: 0.00001815
Iteration 138/1000 | Loss: 0.00001815
Iteration 139/1000 | Loss: 0.00001815
Iteration 140/1000 | Loss: 0.00001815
Iteration 141/1000 | Loss: 0.00001815
Iteration 142/1000 | Loss: 0.00001814
Iteration 143/1000 | Loss: 0.00001814
Iteration 144/1000 | Loss: 0.00001814
Iteration 145/1000 | Loss: 0.00001814
Iteration 146/1000 | Loss: 0.00001814
Iteration 147/1000 | Loss: 0.00001814
Iteration 148/1000 | Loss: 0.00001814
Iteration 149/1000 | Loss: 0.00001814
Iteration 150/1000 | Loss: 0.00001814
Iteration 151/1000 | Loss: 0.00001813
Iteration 152/1000 | Loss: 0.00001813
Iteration 153/1000 | Loss: 0.00001813
Iteration 154/1000 | Loss: 0.00001813
Iteration 155/1000 | Loss: 0.00001813
Iteration 156/1000 | Loss: 0.00001813
Iteration 157/1000 | Loss: 0.00001812
Iteration 158/1000 | Loss: 0.00001812
Iteration 159/1000 | Loss: 0.00001812
Iteration 160/1000 | Loss: 0.00001812
Iteration 161/1000 | Loss: 0.00001812
Iteration 162/1000 | Loss: 0.00001812
Iteration 163/1000 | Loss: 0.00001812
Iteration 164/1000 | Loss: 0.00001812
Iteration 165/1000 | Loss: 0.00001812
Iteration 166/1000 | Loss: 0.00001812
Iteration 167/1000 | Loss: 0.00001812
Iteration 168/1000 | Loss: 0.00001811
Iteration 169/1000 | Loss: 0.00001811
Iteration 170/1000 | Loss: 0.00001811
Iteration 171/1000 | Loss: 0.00001811
Iteration 172/1000 | Loss: 0.00001811
Iteration 173/1000 | Loss: 0.00001810
Iteration 174/1000 | Loss: 0.00001810
Iteration 175/1000 | Loss: 0.00001810
Iteration 176/1000 | Loss: 0.00001810
Iteration 177/1000 | Loss: 0.00001810
Iteration 178/1000 | Loss: 0.00001810
Iteration 179/1000 | Loss: 0.00001810
Iteration 180/1000 | Loss: 0.00001810
Iteration 181/1000 | Loss: 0.00001810
Iteration 182/1000 | Loss: 0.00001809
Iteration 183/1000 | Loss: 0.00001809
Iteration 184/1000 | Loss: 0.00001809
Iteration 185/1000 | Loss: 0.00001809
Iteration 186/1000 | Loss: 0.00001809
Iteration 187/1000 | Loss: 0.00001808
Iteration 188/1000 | Loss: 0.00001808
Iteration 189/1000 | Loss: 0.00001808
Iteration 190/1000 | Loss: 0.00001808
Iteration 191/1000 | Loss: 0.00001807
Iteration 192/1000 | Loss: 0.00001807
Iteration 193/1000 | Loss: 0.00001807
Iteration 194/1000 | Loss: 0.00001807
Iteration 195/1000 | Loss: 0.00001807
Iteration 196/1000 | Loss: 0.00001807
Iteration 197/1000 | Loss: 0.00001807
Iteration 198/1000 | Loss: 0.00001807
Iteration 199/1000 | Loss: 0.00001807
Iteration 200/1000 | Loss: 0.00001807
Iteration 201/1000 | Loss: 0.00001807
Iteration 202/1000 | Loss: 0.00001807
Iteration 203/1000 | Loss: 0.00001807
Iteration 204/1000 | Loss: 0.00001806
Iteration 205/1000 | Loss: 0.00001806
Iteration 206/1000 | Loss: 0.00001806
Iteration 207/1000 | Loss: 0.00001806
Iteration 208/1000 | Loss: 0.00001806
Iteration 209/1000 | Loss: 0.00001806
Iteration 210/1000 | Loss: 0.00001806
Iteration 211/1000 | Loss: 0.00001806
Iteration 212/1000 | Loss: 0.00001806
Iteration 213/1000 | Loss: 0.00001806
Iteration 214/1000 | Loss: 0.00001806
Iteration 215/1000 | Loss: 0.00001806
Iteration 216/1000 | Loss: 0.00001806
Iteration 217/1000 | Loss: 0.00001806
Iteration 218/1000 | Loss: 0.00001806
Iteration 219/1000 | Loss: 0.00001806
Iteration 220/1000 | Loss: 0.00001806
Iteration 221/1000 | Loss: 0.00001806
Iteration 222/1000 | Loss: 0.00001806
Iteration 223/1000 | Loss: 0.00001806
Iteration 224/1000 | Loss: 0.00001806
Iteration 225/1000 | Loss: 0.00001806
Iteration 226/1000 | Loss: 0.00001806
Iteration 227/1000 | Loss: 0.00001806
Iteration 228/1000 | Loss: 0.00001806
Iteration 229/1000 | Loss: 0.00001806
Iteration 230/1000 | Loss: 0.00001806
Iteration 231/1000 | Loss: 0.00001806
Iteration 232/1000 | Loss: 0.00001806
Iteration 233/1000 | Loss: 0.00001806
Iteration 234/1000 | Loss: 0.00001806
Iteration 235/1000 | Loss: 0.00001806
Iteration 236/1000 | Loss: 0.00001806
Iteration 237/1000 | Loss: 0.00001806
Iteration 238/1000 | Loss: 0.00001806
Iteration 239/1000 | Loss: 0.00001806
Iteration 240/1000 | Loss: 0.00001806
Iteration 241/1000 | Loss: 0.00001806
Iteration 242/1000 | Loss: 0.00001806
Iteration 243/1000 | Loss: 0.00001806
Iteration 244/1000 | Loss: 0.00001806
Iteration 245/1000 | Loss: 0.00001806
Iteration 246/1000 | Loss: 0.00001806
Iteration 247/1000 | Loss: 0.00001806
Iteration 248/1000 | Loss: 0.00001806
Iteration 249/1000 | Loss: 0.00001806
Iteration 250/1000 | Loss: 0.00001806
Iteration 251/1000 | Loss: 0.00001806
Iteration 252/1000 | Loss: 0.00001806
Iteration 253/1000 | Loss: 0.00001806
Iteration 254/1000 | Loss: 0.00001806
Iteration 255/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.8064134565065615e-05, 1.8064134565065615e-05, 1.8064134565065615e-05, 1.8064134565065615e-05, 1.8064134565065615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8064134565065615e-05

Optimization complete. Final v2v error: 3.5408897399902344 mm

Highest mean error: 4.7532501220703125 mm for frame 8

Lowest mean error: 3.0892505645751953 mm for frame 132

Saving results

Total time: 88.5106406211853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398260
Iteration 2/25 | Loss: 0.00133811
Iteration 3/25 | Loss: 0.00127920
Iteration 4/25 | Loss: 0.00127034
Iteration 5/25 | Loss: 0.00126859
Iteration 6/25 | Loss: 0.00126859
Iteration 7/25 | Loss: 0.00126859
Iteration 8/25 | Loss: 0.00126859
Iteration 9/25 | Loss: 0.00126859
Iteration 10/25 | Loss: 0.00126859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00126859440933913, 0.00126859440933913, 0.00126859440933913, 0.00126859440933913, 0.00126859440933913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00126859440933913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39917707
Iteration 2/25 | Loss: 0.00080419
Iteration 3/25 | Loss: 0.00080419
Iteration 4/25 | Loss: 0.00080419
Iteration 5/25 | Loss: 0.00080419
Iteration 6/25 | Loss: 0.00080419
Iteration 7/25 | Loss: 0.00080419
Iteration 8/25 | Loss: 0.00080419
Iteration 9/25 | Loss: 0.00080419
Iteration 10/25 | Loss: 0.00080419
Iteration 11/25 | Loss: 0.00080419
Iteration 12/25 | Loss: 0.00080419
Iteration 13/25 | Loss: 0.00080419
Iteration 14/25 | Loss: 0.00080419
Iteration 15/25 | Loss: 0.00080419
Iteration 16/25 | Loss: 0.00080419
Iteration 17/25 | Loss: 0.00080419
Iteration 18/25 | Loss: 0.00080419
Iteration 19/25 | Loss: 0.00080419
Iteration 20/25 | Loss: 0.00080419
Iteration 21/25 | Loss: 0.00080419
Iteration 22/25 | Loss: 0.00080419
Iteration 23/25 | Loss: 0.00080419
Iteration 24/25 | Loss: 0.00080419
Iteration 25/25 | Loss: 0.00080419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080419
Iteration 2/1000 | Loss: 0.00002312
Iteration 3/1000 | Loss: 0.00001829
Iteration 4/1000 | Loss: 0.00001703
Iteration 5/1000 | Loss: 0.00001620
Iteration 6/1000 | Loss: 0.00001590
Iteration 7/1000 | Loss: 0.00001552
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001517
Iteration 10/1000 | Loss: 0.00001501
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001487
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001467
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001463
Iteration 20/1000 | Loss: 0.00001461
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001442
Iteration 29/1000 | Loss: 0.00001441
Iteration 30/1000 | Loss: 0.00001440
Iteration 31/1000 | Loss: 0.00001440
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001437
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001435
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00001435
Iteration 50/1000 | Loss: 0.00001435
Iteration 51/1000 | Loss: 0.00001435
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001435
Iteration 54/1000 | Loss: 0.00001434
Iteration 55/1000 | Loss: 0.00001434
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001429
Iteration 63/1000 | Loss: 0.00001429
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001428
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001426
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001425
Iteration 77/1000 | Loss: 0.00001425
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001423
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001420
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001419
Iteration 92/1000 | Loss: 0.00001419
Iteration 93/1000 | Loss: 0.00001419
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001418
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001418
Iteration 98/1000 | Loss: 0.00001417
Iteration 99/1000 | Loss: 0.00001417
Iteration 100/1000 | Loss: 0.00001417
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001416
Iteration 103/1000 | Loss: 0.00001416
Iteration 104/1000 | Loss: 0.00001416
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001414
Iteration 108/1000 | Loss: 0.00001414
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00001411
Iteration 112/1000 | Loss: 0.00001411
Iteration 113/1000 | Loss: 0.00001409
Iteration 114/1000 | Loss: 0.00001408
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001406
Iteration 118/1000 | Loss: 0.00001406
Iteration 119/1000 | Loss: 0.00001405
Iteration 120/1000 | Loss: 0.00001405
Iteration 121/1000 | Loss: 0.00001404
Iteration 122/1000 | Loss: 0.00001404
Iteration 123/1000 | Loss: 0.00001404
Iteration 124/1000 | Loss: 0.00001404
Iteration 125/1000 | Loss: 0.00001403
Iteration 126/1000 | Loss: 0.00001403
Iteration 127/1000 | Loss: 0.00001403
Iteration 128/1000 | Loss: 0.00001403
Iteration 129/1000 | Loss: 0.00001403
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001402
Iteration 138/1000 | Loss: 0.00001401
Iteration 139/1000 | Loss: 0.00001401
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001401
Iteration 142/1000 | Loss: 0.00001401
Iteration 143/1000 | Loss: 0.00001401
Iteration 144/1000 | Loss: 0.00001401
Iteration 145/1000 | Loss: 0.00001401
Iteration 146/1000 | Loss: 0.00001400
Iteration 147/1000 | Loss: 0.00001400
Iteration 148/1000 | Loss: 0.00001400
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Iteration 159/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.4001067029312253e-05, 1.4001067029312253e-05, 1.4001067029312253e-05, 1.4001067029312253e-05, 1.4001067029312253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4001067029312253e-05

Optimization complete. Final v2v error: 3.1795403957366943 mm

Highest mean error: 3.273223876953125 mm for frame 208

Lowest mean error: 3.1035804748535156 mm for frame 231

Saving results

Total time: 42.921695709228516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499191
Iteration 2/25 | Loss: 0.00157229
Iteration 3/25 | Loss: 0.00135407
Iteration 4/25 | Loss: 0.00133275
Iteration 5/25 | Loss: 0.00132869
Iteration 6/25 | Loss: 0.00132786
Iteration 7/25 | Loss: 0.00132786
Iteration 8/25 | Loss: 0.00132786
Iteration 9/25 | Loss: 0.00132786
Iteration 10/25 | Loss: 0.00132786
Iteration 11/25 | Loss: 0.00132786
Iteration 12/25 | Loss: 0.00132786
Iteration 13/25 | Loss: 0.00132786
Iteration 14/25 | Loss: 0.00132786
Iteration 15/25 | Loss: 0.00132786
Iteration 16/25 | Loss: 0.00132786
Iteration 17/25 | Loss: 0.00132786
Iteration 18/25 | Loss: 0.00132786
Iteration 19/25 | Loss: 0.00132786
Iteration 20/25 | Loss: 0.00132786
Iteration 21/25 | Loss: 0.00132786
Iteration 22/25 | Loss: 0.00132786
Iteration 23/25 | Loss: 0.00132786
Iteration 24/25 | Loss: 0.00132786
Iteration 25/25 | Loss: 0.00132786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41850877
Iteration 2/25 | Loss: 0.00084037
Iteration 3/25 | Loss: 0.00084037
Iteration 4/25 | Loss: 0.00084037
Iteration 5/25 | Loss: 0.00084036
Iteration 6/25 | Loss: 0.00084036
Iteration 7/25 | Loss: 0.00084036
Iteration 8/25 | Loss: 0.00084036
Iteration 9/25 | Loss: 0.00084036
Iteration 10/25 | Loss: 0.00084036
Iteration 11/25 | Loss: 0.00084036
Iteration 12/25 | Loss: 0.00084036
Iteration 13/25 | Loss: 0.00084036
Iteration 14/25 | Loss: 0.00084036
Iteration 15/25 | Loss: 0.00084036
Iteration 16/25 | Loss: 0.00084036
Iteration 17/25 | Loss: 0.00084036
Iteration 18/25 | Loss: 0.00084036
Iteration 19/25 | Loss: 0.00084036
Iteration 20/25 | Loss: 0.00084036
Iteration 21/25 | Loss: 0.00084036
Iteration 22/25 | Loss: 0.00084036
Iteration 23/25 | Loss: 0.00084036
Iteration 24/25 | Loss: 0.00084036
Iteration 25/25 | Loss: 0.00084036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084036
Iteration 2/1000 | Loss: 0.00003357
Iteration 3/1000 | Loss: 0.00002405
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00002059
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001914
Iteration 8/1000 | Loss: 0.00001874
Iteration 9/1000 | Loss: 0.00001850
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001812
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001802
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001784
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001779
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001778
Iteration 20/1000 | Loss: 0.00001778
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001775
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001774
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001772
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001769
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001766
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001766
Iteration 43/1000 | Loss: 0.00001766
Iteration 44/1000 | Loss: 0.00001765
Iteration 45/1000 | Loss: 0.00001765
Iteration 46/1000 | Loss: 0.00001765
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001761
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001760
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001758
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001757
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001750
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001750
Iteration 86/1000 | Loss: 0.00001750
Iteration 87/1000 | Loss: 0.00001749
Iteration 88/1000 | Loss: 0.00001749
Iteration 89/1000 | Loss: 0.00001749
Iteration 90/1000 | Loss: 0.00001749
Iteration 91/1000 | Loss: 0.00001749
Iteration 92/1000 | Loss: 0.00001749
Iteration 93/1000 | Loss: 0.00001749
Iteration 94/1000 | Loss: 0.00001748
Iteration 95/1000 | Loss: 0.00001748
Iteration 96/1000 | Loss: 0.00001748
Iteration 97/1000 | Loss: 0.00001748
Iteration 98/1000 | Loss: 0.00001748
Iteration 99/1000 | Loss: 0.00001748
Iteration 100/1000 | Loss: 0.00001748
Iteration 101/1000 | Loss: 0.00001747
Iteration 102/1000 | Loss: 0.00001747
Iteration 103/1000 | Loss: 0.00001747
Iteration 104/1000 | Loss: 0.00001747
Iteration 105/1000 | Loss: 0.00001747
Iteration 106/1000 | Loss: 0.00001746
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001746
Iteration 109/1000 | Loss: 0.00001746
Iteration 110/1000 | Loss: 0.00001746
Iteration 111/1000 | Loss: 0.00001746
Iteration 112/1000 | Loss: 0.00001746
Iteration 113/1000 | Loss: 0.00001746
Iteration 114/1000 | Loss: 0.00001745
Iteration 115/1000 | Loss: 0.00001745
Iteration 116/1000 | Loss: 0.00001745
Iteration 117/1000 | Loss: 0.00001745
Iteration 118/1000 | Loss: 0.00001745
Iteration 119/1000 | Loss: 0.00001745
Iteration 120/1000 | Loss: 0.00001744
Iteration 121/1000 | Loss: 0.00001744
Iteration 122/1000 | Loss: 0.00001744
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001743
Iteration 125/1000 | Loss: 0.00001743
Iteration 126/1000 | Loss: 0.00001743
Iteration 127/1000 | Loss: 0.00001742
Iteration 128/1000 | Loss: 0.00001742
Iteration 129/1000 | Loss: 0.00001742
Iteration 130/1000 | Loss: 0.00001742
Iteration 131/1000 | Loss: 0.00001742
Iteration 132/1000 | Loss: 0.00001742
Iteration 133/1000 | Loss: 0.00001742
Iteration 134/1000 | Loss: 0.00001742
Iteration 135/1000 | Loss: 0.00001741
Iteration 136/1000 | Loss: 0.00001741
Iteration 137/1000 | Loss: 0.00001741
Iteration 138/1000 | Loss: 0.00001741
Iteration 139/1000 | Loss: 0.00001741
Iteration 140/1000 | Loss: 0.00001741
Iteration 141/1000 | Loss: 0.00001740
Iteration 142/1000 | Loss: 0.00001740
Iteration 143/1000 | Loss: 0.00001740
Iteration 144/1000 | Loss: 0.00001740
Iteration 145/1000 | Loss: 0.00001740
Iteration 146/1000 | Loss: 0.00001740
Iteration 147/1000 | Loss: 0.00001739
Iteration 148/1000 | Loss: 0.00001739
Iteration 149/1000 | Loss: 0.00001739
Iteration 150/1000 | Loss: 0.00001739
Iteration 151/1000 | Loss: 0.00001739
Iteration 152/1000 | Loss: 0.00001739
Iteration 153/1000 | Loss: 0.00001739
Iteration 154/1000 | Loss: 0.00001739
Iteration 155/1000 | Loss: 0.00001739
Iteration 156/1000 | Loss: 0.00001739
Iteration 157/1000 | Loss: 0.00001738
Iteration 158/1000 | Loss: 0.00001738
Iteration 159/1000 | Loss: 0.00001738
Iteration 160/1000 | Loss: 0.00001738
Iteration 161/1000 | Loss: 0.00001738
Iteration 162/1000 | Loss: 0.00001738
Iteration 163/1000 | Loss: 0.00001738
Iteration 164/1000 | Loss: 0.00001737
Iteration 165/1000 | Loss: 0.00001737
Iteration 166/1000 | Loss: 0.00001737
Iteration 167/1000 | Loss: 0.00001737
Iteration 168/1000 | Loss: 0.00001736
Iteration 169/1000 | Loss: 0.00001736
Iteration 170/1000 | Loss: 0.00001736
Iteration 171/1000 | Loss: 0.00001736
Iteration 172/1000 | Loss: 0.00001735
Iteration 173/1000 | Loss: 0.00001735
Iteration 174/1000 | Loss: 0.00001735
Iteration 175/1000 | Loss: 0.00001735
Iteration 176/1000 | Loss: 0.00001735
Iteration 177/1000 | Loss: 0.00001734
Iteration 178/1000 | Loss: 0.00001734
Iteration 179/1000 | Loss: 0.00001734
Iteration 180/1000 | Loss: 0.00001734
Iteration 181/1000 | Loss: 0.00001733
Iteration 182/1000 | Loss: 0.00001733
Iteration 183/1000 | Loss: 0.00001733
Iteration 184/1000 | Loss: 0.00001733
Iteration 185/1000 | Loss: 0.00001733
Iteration 186/1000 | Loss: 0.00001733
Iteration 187/1000 | Loss: 0.00001733
Iteration 188/1000 | Loss: 0.00001733
Iteration 189/1000 | Loss: 0.00001733
Iteration 190/1000 | Loss: 0.00001732
Iteration 191/1000 | Loss: 0.00001732
Iteration 192/1000 | Loss: 0.00001732
Iteration 193/1000 | Loss: 0.00001732
Iteration 194/1000 | Loss: 0.00001732
Iteration 195/1000 | Loss: 0.00001732
Iteration 196/1000 | Loss: 0.00001732
Iteration 197/1000 | Loss: 0.00001732
Iteration 198/1000 | Loss: 0.00001732
Iteration 199/1000 | Loss: 0.00001732
Iteration 200/1000 | Loss: 0.00001732
Iteration 201/1000 | Loss: 0.00001732
Iteration 202/1000 | Loss: 0.00001732
Iteration 203/1000 | Loss: 0.00001732
Iteration 204/1000 | Loss: 0.00001732
Iteration 205/1000 | Loss: 0.00001732
Iteration 206/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.732047530822456e-05, 1.732047530822456e-05, 1.732047530822456e-05, 1.732047530822456e-05, 1.732047530822456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.732047530822456e-05

Optimization complete. Final v2v error: 3.4843966960906982 mm

Highest mean error: 4.4712724685668945 mm for frame 92

Lowest mean error: 3.143761157989502 mm for frame 25

Saving results

Total time: 43.08036518096924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500179
Iteration 2/25 | Loss: 0.00151700
Iteration 3/25 | Loss: 0.00138242
Iteration 4/25 | Loss: 0.00135136
Iteration 5/25 | Loss: 0.00134527
Iteration 6/25 | Loss: 0.00134368
Iteration 7/25 | Loss: 0.00134316
Iteration 8/25 | Loss: 0.00134292
Iteration 9/25 | Loss: 0.00134284
Iteration 10/25 | Loss: 0.00134284
Iteration 11/25 | Loss: 0.00134284
Iteration 12/25 | Loss: 0.00134284
Iteration 13/25 | Loss: 0.00134284
Iteration 14/25 | Loss: 0.00134284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013428444508463144, 0.0013428444508463144, 0.0013428444508463144, 0.0013428444508463144, 0.0013428444508463144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013428444508463144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43841469
Iteration 2/25 | Loss: 0.00127432
Iteration 3/25 | Loss: 0.00127432
Iteration 4/25 | Loss: 0.00127432
Iteration 5/25 | Loss: 0.00127432
Iteration 6/25 | Loss: 0.00127432
Iteration 7/25 | Loss: 0.00127432
Iteration 8/25 | Loss: 0.00127432
Iteration 9/25 | Loss: 0.00127432
Iteration 10/25 | Loss: 0.00127432
Iteration 11/25 | Loss: 0.00127432
Iteration 12/25 | Loss: 0.00127432
Iteration 13/25 | Loss: 0.00127431
Iteration 14/25 | Loss: 0.00127431
Iteration 15/25 | Loss: 0.00127431
Iteration 16/25 | Loss: 0.00127431
Iteration 17/25 | Loss: 0.00127431
Iteration 18/25 | Loss: 0.00127431
Iteration 19/25 | Loss: 0.00127431
Iteration 20/25 | Loss: 0.00127431
Iteration 21/25 | Loss: 0.00127431
Iteration 22/25 | Loss: 0.00127431
Iteration 23/25 | Loss: 0.00127431
Iteration 24/25 | Loss: 0.00127431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00127431470900774, 0.00127431470900774, 0.00127431470900774, 0.00127431470900774, 0.00127431470900774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127431470900774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127431
Iteration 2/1000 | Loss: 0.00006550
Iteration 3/1000 | Loss: 0.00008641
Iteration 4/1000 | Loss: 0.00003908
Iteration 5/1000 | Loss: 0.00003362
Iteration 6/1000 | Loss: 0.00006115
Iteration 7/1000 | Loss: 0.00003237
Iteration 8/1000 | Loss: 0.00002775
Iteration 9/1000 | Loss: 0.00002625
Iteration 10/1000 | Loss: 0.00002543
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002474
Iteration 13/1000 | Loss: 0.00002450
Iteration 14/1000 | Loss: 0.00002429
Iteration 15/1000 | Loss: 0.00002408
Iteration 16/1000 | Loss: 0.00002401
Iteration 17/1000 | Loss: 0.00002398
Iteration 18/1000 | Loss: 0.00002397
Iteration 19/1000 | Loss: 0.00002396
Iteration 20/1000 | Loss: 0.00002396
Iteration 21/1000 | Loss: 0.00002395
Iteration 22/1000 | Loss: 0.00002394
Iteration 23/1000 | Loss: 0.00002393
Iteration 24/1000 | Loss: 0.00002393
Iteration 25/1000 | Loss: 0.00002392
Iteration 26/1000 | Loss: 0.00002392
Iteration 27/1000 | Loss: 0.00002392
Iteration 28/1000 | Loss: 0.00002391
Iteration 29/1000 | Loss: 0.00002391
Iteration 30/1000 | Loss: 0.00002390
Iteration 31/1000 | Loss: 0.00002390
Iteration 32/1000 | Loss: 0.00002390
Iteration 33/1000 | Loss: 0.00002389
Iteration 34/1000 | Loss: 0.00002389
Iteration 35/1000 | Loss: 0.00002389
Iteration 36/1000 | Loss: 0.00002389
Iteration 37/1000 | Loss: 0.00002388
Iteration 38/1000 | Loss: 0.00002388
Iteration 39/1000 | Loss: 0.00002388
Iteration 40/1000 | Loss: 0.00002387
Iteration 41/1000 | Loss: 0.00002387
Iteration 42/1000 | Loss: 0.00002387
Iteration 43/1000 | Loss: 0.00002386
Iteration 44/1000 | Loss: 0.00002386
Iteration 45/1000 | Loss: 0.00002386
Iteration 46/1000 | Loss: 0.00002386
Iteration 47/1000 | Loss: 0.00002386
Iteration 48/1000 | Loss: 0.00002386
Iteration 49/1000 | Loss: 0.00002386
Iteration 50/1000 | Loss: 0.00002386
Iteration 51/1000 | Loss: 0.00002385
Iteration 52/1000 | Loss: 0.00002385
Iteration 53/1000 | Loss: 0.00002384
Iteration 54/1000 | Loss: 0.00002384
Iteration 55/1000 | Loss: 0.00002383
Iteration 56/1000 | Loss: 0.00002383
Iteration 57/1000 | Loss: 0.00002383
Iteration 58/1000 | Loss: 0.00002383
Iteration 59/1000 | Loss: 0.00002383
Iteration 60/1000 | Loss: 0.00002383
Iteration 61/1000 | Loss: 0.00002382
Iteration 62/1000 | Loss: 0.00002382
Iteration 63/1000 | Loss: 0.00002382
Iteration 64/1000 | Loss: 0.00002382
Iteration 65/1000 | Loss: 0.00002381
Iteration 66/1000 | Loss: 0.00002380
Iteration 67/1000 | Loss: 0.00002380
Iteration 68/1000 | Loss: 0.00002379
Iteration 69/1000 | Loss: 0.00002375
Iteration 70/1000 | Loss: 0.00002375
Iteration 71/1000 | Loss: 0.00002374
Iteration 72/1000 | Loss: 0.00002374
Iteration 73/1000 | Loss: 0.00002374
Iteration 74/1000 | Loss: 0.00002373
Iteration 75/1000 | Loss: 0.00002373
Iteration 76/1000 | Loss: 0.00002373
Iteration 77/1000 | Loss: 0.00002373
Iteration 78/1000 | Loss: 0.00002373
Iteration 79/1000 | Loss: 0.00002373
Iteration 80/1000 | Loss: 0.00002373
Iteration 81/1000 | Loss: 0.00002373
Iteration 82/1000 | Loss: 0.00002373
Iteration 83/1000 | Loss: 0.00002372
Iteration 84/1000 | Loss: 0.00002372
Iteration 85/1000 | Loss: 0.00002372
Iteration 86/1000 | Loss: 0.00002372
Iteration 87/1000 | Loss: 0.00002372
Iteration 88/1000 | Loss: 0.00002371
Iteration 89/1000 | Loss: 0.00002371
Iteration 90/1000 | Loss: 0.00002371
Iteration 91/1000 | Loss: 0.00002371
Iteration 92/1000 | Loss: 0.00002371
Iteration 93/1000 | Loss: 0.00002371
Iteration 94/1000 | Loss: 0.00002371
Iteration 95/1000 | Loss: 0.00002370
Iteration 96/1000 | Loss: 0.00002370
Iteration 97/1000 | Loss: 0.00002370
Iteration 98/1000 | Loss: 0.00002370
Iteration 99/1000 | Loss: 0.00002369
Iteration 100/1000 | Loss: 0.00002369
Iteration 101/1000 | Loss: 0.00002369
Iteration 102/1000 | Loss: 0.00002369
Iteration 103/1000 | Loss: 0.00002369
Iteration 104/1000 | Loss: 0.00002369
Iteration 105/1000 | Loss: 0.00002369
Iteration 106/1000 | Loss: 0.00002368
Iteration 107/1000 | Loss: 0.00002368
Iteration 108/1000 | Loss: 0.00002368
Iteration 109/1000 | Loss: 0.00002368
Iteration 110/1000 | Loss: 0.00002368
Iteration 111/1000 | Loss: 0.00002368
Iteration 112/1000 | Loss: 0.00002368
Iteration 113/1000 | Loss: 0.00002368
Iteration 114/1000 | Loss: 0.00002368
Iteration 115/1000 | Loss: 0.00002368
Iteration 116/1000 | Loss: 0.00002368
Iteration 117/1000 | Loss: 0.00002368
Iteration 118/1000 | Loss: 0.00002368
Iteration 119/1000 | Loss: 0.00002368
Iteration 120/1000 | Loss: 0.00002368
Iteration 121/1000 | Loss: 0.00002368
Iteration 122/1000 | Loss: 0.00002368
Iteration 123/1000 | Loss: 0.00002368
Iteration 124/1000 | Loss: 0.00002368
Iteration 125/1000 | Loss: 0.00002368
Iteration 126/1000 | Loss: 0.00002368
Iteration 127/1000 | Loss: 0.00002367
Iteration 128/1000 | Loss: 0.00002367
Iteration 129/1000 | Loss: 0.00002367
Iteration 130/1000 | Loss: 0.00002367
Iteration 131/1000 | Loss: 0.00002367
Iteration 132/1000 | Loss: 0.00002367
Iteration 133/1000 | Loss: 0.00002367
Iteration 134/1000 | Loss: 0.00002367
Iteration 135/1000 | Loss: 0.00002367
Iteration 136/1000 | Loss: 0.00002367
Iteration 137/1000 | Loss: 0.00002367
Iteration 138/1000 | Loss: 0.00002367
Iteration 139/1000 | Loss: 0.00002367
Iteration 140/1000 | Loss: 0.00002367
Iteration 141/1000 | Loss: 0.00002367
Iteration 142/1000 | Loss: 0.00002367
Iteration 143/1000 | Loss: 0.00002367
Iteration 144/1000 | Loss: 0.00002367
Iteration 145/1000 | Loss: 0.00002367
Iteration 146/1000 | Loss: 0.00002366
Iteration 147/1000 | Loss: 0.00002366
Iteration 148/1000 | Loss: 0.00002366
Iteration 149/1000 | Loss: 0.00002366
Iteration 150/1000 | Loss: 0.00002366
Iteration 151/1000 | Loss: 0.00002366
Iteration 152/1000 | Loss: 0.00002366
Iteration 153/1000 | Loss: 0.00002366
Iteration 154/1000 | Loss: 0.00002366
Iteration 155/1000 | Loss: 0.00002366
Iteration 156/1000 | Loss: 0.00002366
Iteration 157/1000 | Loss: 0.00002366
Iteration 158/1000 | Loss: 0.00002365
Iteration 159/1000 | Loss: 0.00002365
Iteration 160/1000 | Loss: 0.00002365
Iteration 161/1000 | Loss: 0.00002365
Iteration 162/1000 | Loss: 0.00002365
Iteration 163/1000 | Loss: 0.00002365
Iteration 164/1000 | Loss: 0.00002365
Iteration 165/1000 | Loss: 0.00002365
Iteration 166/1000 | Loss: 0.00002365
Iteration 167/1000 | Loss: 0.00002365
Iteration 168/1000 | Loss: 0.00002365
Iteration 169/1000 | Loss: 0.00002365
Iteration 170/1000 | Loss: 0.00002364
Iteration 171/1000 | Loss: 0.00002364
Iteration 172/1000 | Loss: 0.00002364
Iteration 173/1000 | Loss: 0.00002364
Iteration 174/1000 | Loss: 0.00002364
Iteration 175/1000 | Loss: 0.00002364
Iteration 176/1000 | Loss: 0.00002364
Iteration 177/1000 | Loss: 0.00002364
Iteration 178/1000 | Loss: 0.00002364
Iteration 179/1000 | Loss: 0.00002364
Iteration 180/1000 | Loss: 0.00002364
Iteration 181/1000 | Loss: 0.00002364
Iteration 182/1000 | Loss: 0.00002364
Iteration 183/1000 | Loss: 0.00002364
Iteration 184/1000 | Loss: 0.00002364
Iteration 185/1000 | Loss: 0.00002364
Iteration 186/1000 | Loss: 0.00002364
Iteration 187/1000 | Loss: 0.00002364
Iteration 188/1000 | Loss: 0.00002364
Iteration 189/1000 | Loss: 0.00002364
Iteration 190/1000 | Loss: 0.00002364
Iteration 191/1000 | Loss: 0.00002364
Iteration 192/1000 | Loss: 0.00002364
Iteration 193/1000 | Loss: 0.00002364
Iteration 194/1000 | Loss: 0.00002364
Iteration 195/1000 | Loss: 0.00002364
Iteration 196/1000 | Loss: 0.00002364
Iteration 197/1000 | Loss: 0.00002364
Iteration 198/1000 | Loss: 0.00002364
Iteration 199/1000 | Loss: 0.00002364
Iteration 200/1000 | Loss: 0.00002364
Iteration 201/1000 | Loss: 0.00002364
Iteration 202/1000 | Loss: 0.00002364
Iteration 203/1000 | Loss: 0.00002364
Iteration 204/1000 | Loss: 0.00002364
Iteration 205/1000 | Loss: 0.00002364
Iteration 206/1000 | Loss: 0.00002364
Iteration 207/1000 | Loss: 0.00002364
Iteration 208/1000 | Loss: 0.00002364
Iteration 209/1000 | Loss: 0.00002364
Iteration 210/1000 | Loss: 0.00002364
Iteration 211/1000 | Loss: 0.00002364
Iteration 212/1000 | Loss: 0.00002364
Iteration 213/1000 | Loss: 0.00002364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.364210013183765e-05, 2.364210013183765e-05, 2.364210013183765e-05, 2.364210013183765e-05, 2.364210013183765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.364210013183765e-05

Optimization complete. Final v2v error: 3.974109411239624 mm

Highest mean error: 5.453775405883789 mm for frame 94

Lowest mean error: 3.0299956798553467 mm for frame 49

Saving results

Total time: 47.86534285545349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00680245
Iteration 2/25 | Loss: 0.00144298
Iteration 3/25 | Loss: 0.00135472
Iteration 4/25 | Loss: 0.00129509
Iteration 5/25 | Loss: 0.00129267
Iteration 6/25 | Loss: 0.00129296
Iteration 7/25 | Loss: 0.00128923
Iteration 8/25 | Loss: 0.00128000
Iteration 9/25 | Loss: 0.00127981
Iteration 10/25 | Loss: 0.00127981
Iteration 11/25 | Loss: 0.00127981
Iteration 12/25 | Loss: 0.00127981
Iteration 13/25 | Loss: 0.00127981
Iteration 14/25 | Loss: 0.00127981
Iteration 15/25 | Loss: 0.00127981
Iteration 16/25 | Loss: 0.00127980
Iteration 17/25 | Loss: 0.00127980
Iteration 18/25 | Loss: 0.00127980
Iteration 19/25 | Loss: 0.00127980
Iteration 20/25 | Loss: 0.00127980
Iteration 21/25 | Loss: 0.00127980
Iteration 22/25 | Loss: 0.00127980
Iteration 23/25 | Loss: 0.00127980
Iteration 24/25 | Loss: 0.00127980
Iteration 25/25 | Loss: 0.00127980

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.65881729
Iteration 2/25 | Loss: 0.00092582
Iteration 3/25 | Loss: 0.00092581
Iteration 4/25 | Loss: 0.00092581
Iteration 5/25 | Loss: 0.00092581
Iteration 6/25 | Loss: 0.00092581
Iteration 7/25 | Loss: 0.00092581
Iteration 8/25 | Loss: 0.00092581
Iteration 9/25 | Loss: 0.00092581
Iteration 10/25 | Loss: 0.00092581
Iteration 11/25 | Loss: 0.00092581
Iteration 12/25 | Loss: 0.00092581
Iteration 13/25 | Loss: 0.00092581
Iteration 14/25 | Loss: 0.00092581
Iteration 15/25 | Loss: 0.00092581
Iteration 16/25 | Loss: 0.00092581
Iteration 17/25 | Loss: 0.00092581
Iteration 18/25 | Loss: 0.00092581
Iteration 19/25 | Loss: 0.00092581
Iteration 20/25 | Loss: 0.00092581
Iteration 21/25 | Loss: 0.00092581
Iteration 22/25 | Loss: 0.00092581
Iteration 23/25 | Loss: 0.00092581
Iteration 24/25 | Loss: 0.00092581
Iteration 25/25 | Loss: 0.00092581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092581
Iteration 2/1000 | Loss: 0.00002109
Iteration 3/1000 | Loss: 0.00001695
Iteration 4/1000 | Loss: 0.00001559
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001451
Iteration 7/1000 | Loss: 0.00001437
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001375
Iteration 10/1000 | Loss: 0.00001371
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001361
Iteration 13/1000 | Loss: 0.00001360
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001343
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001334
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001333
Iteration 25/1000 | Loss: 0.00001332
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001328
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001325
Iteration 34/1000 | Loss: 0.00001325
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001321
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001319
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001317
Iteration 44/1000 | Loss: 0.00001315
Iteration 45/1000 | Loss: 0.00001314
Iteration 46/1000 | Loss: 0.00001314
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001306
Iteration 52/1000 | Loss: 0.00001306
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001305
Iteration 55/1000 | Loss: 0.00001305
Iteration 56/1000 | Loss: 0.00001304
Iteration 57/1000 | Loss: 0.00001303
Iteration 58/1000 | Loss: 0.00001303
Iteration 59/1000 | Loss: 0.00001303
Iteration 60/1000 | Loss: 0.00001302
Iteration 61/1000 | Loss: 0.00001302
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001300
Iteration 65/1000 | Loss: 0.00001300
Iteration 66/1000 | Loss: 0.00001299
Iteration 67/1000 | Loss: 0.00001299
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001299
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001296
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001296
Iteration 91/1000 | Loss: 0.00001296
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001296
Iteration 94/1000 | Loss: 0.00001296
Iteration 95/1000 | Loss: 0.00001296
Iteration 96/1000 | Loss: 0.00001296
Iteration 97/1000 | Loss: 0.00001296
Iteration 98/1000 | Loss: 0.00001296
Iteration 99/1000 | Loss: 0.00001296
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001296
Iteration 117/1000 | Loss: 0.00001296
Iteration 118/1000 | Loss: 0.00001296
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.2960635103809182e-05, 1.2960635103809182e-05, 1.2960635103809182e-05, 1.2960635103809182e-05, 1.2960635103809182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2960635103809182e-05

Optimization complete. Final v2v error: 3.0807766914367676 mm

Highest mean error: 3.431868553161621 mm for frame 201

Lowest mean error: 2.863363027572632 mm for frame 41

Saving results

Total time: 45.05861234664917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821814
Iteration 2/25 | Loss: 0.00148998
Iteration 3/25 | Loss: 0.00134525
Iteration 4/25 | Loss: 0.00132626
Iteration 5/25 | Loss: 0.00132584
Iteration 6/25 | Loss: 0.00132798
Iteration 7/25 | Loss: 0.00132572
Iteration 8/25 | Loss: 0.00132347
Iteration 9/25 | Loss: 0.00132511
Iteration 10/25 | Loss: 0.00132635
Iteration 11/25 | Loss: 0.00132365
Iteration 12/25 | Loss: 0.00132284
Iteration 13/25 | Loss: 0.00132005
Iteration 14/25 | Loss: 0.00131955
Iteration 15/25 | Loss: 0.00132047
Iteration 16/25 | Loss: 0.00131988
Iteration 17/25 | Loss: 0.00131938
Iteration 18/25 | Loss: 0.00131900
Iteration 19/25 | Loss: 0.00131905
Iteration 20/25 | Loss: 0.00131899
Iteration 21/25 | Loss: 0.00131868
Iteration 22/25 | Loss: 0.00131915
Iteration 23/25 | Loss: 0.00131915
Iteration 24/25 | Loss: 0.00131901
Iteration 25/25 | Loss: 0.00131810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.67940712
Iteration 2/25 | Loss: 0.00085677
Iteration 3/25 | Loss: 0.00085672
Iteration 4/25 | Loss: 0.00085672
Iteration 5/25 | Loss: 0.00085672
Iteration 6/25 | Loss: 0.00085671
Iteration 7/25 | Loss: 0.00085671
Iteration 8/25 | Loss: 0.00085671
Iteration 9/25 | Loss: 0.00085671
Iteration 10/25 | Loss: 0.00085671
Iteration 11/25 | Loss: 0.00085671
Iteration 12/25 | Loss: 0.00085671
Iteration 13/25 | Loss: 0.00085671
Iteration 14/25 | Loss: 0.00085671
Iteration 15/25 | Loss: 0.00085671
Iteration 16/25 | Loss: 0.00085671
Iteration 17/25 | Loss: 0.00085671
Iteration 18/25 | Loss: 0.00085671
Iteration 19/25 | Loss: 0.00085671
Iteration 20/25 | Loss: 0.00085671
Iteration 21/25 | Loss: 0.00085671
Iteration 22/25 | Loss: 0.00085671
Iteration 23/25 | Loss: 0.00085671
Iteration 24/25 | Loss: 0.00085671
Iteration 25/25 | Loss: 0.00085671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085671
Iteration 2/1000 | Loss: 0.00003784
Iteration 3/1000 | Loss: 0.00003522
Iteration 4/1000 | Loss: 0.00003674
Iteration 5/1000 | Loss: 0.00003370
Iteration 6/1000 | Loss: 0.00002626
Iteration 7/1000 | Loss: 0.00004304
Iteration 8/1000 | Loss: 0.00003569
Iteration 9/1000 | Loss: 0.00002823
Iteration 10/1000 | Loss: 0.00002517
Iteration 11/1000 | Loss: 0.00002323
Iteration 12/1000 | Loss: 0.00002242
Iteration 13/1000 | Loss: 0.00002162
Iteration 14/1000 | Loss: 0.00002116
Iteration 15/1000 | Loss: 0.00002084
Iteration 16/1000 | Loss: 0.00002050
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00033967
Iteration 19/1000 | Loss: 0.00002230
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00001954
Iteration 22/1000 | Loss: 0.00001901
Iteration 23/1000 | Loss: 0.00001845
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001818
Iteration 27/1000 | Loss: 0.00001815
Iteration 28/1000 | Loss: 0.00001809
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001795
Iteration 32/1000 | Loss: 0.00001795
Iteration 33/1000 | Loss: 0.00001795
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001795
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001793
Iteration 41/1000 | Loss: 0.00001792
Iteration 42/1000 | Loss: 0.00001789
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001776
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001769
Iteration 56/1000 | Loss: 0.00001766
Iteration 57/1000 | Loss: 0.00001764
Iteration 58/1000 | Loss: 0.00001762
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001760
Iteration 62/1000 | Loss: 0.00001760
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00001759
Iteration 65/1000 | Loss: 0.00001759
Iteration 66/1000 | Loss: 0.00001758
Iteration 67/1000 | Loss: 0.00001758
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001754
Iteration 74/1000 | Loss: 0.00001754
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001753
Iteration 79/1000 | Loss: 0.00001753
Iteration 80/1000 | Loss: 0.00001753
Iteration 81/1000 | Loss: 0.00001753
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001752
Iteration 87/1000 | Loss: 0.00001752
Iteration 88/1000 | Loss: 0.00001752
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001751
Iteration 93/1000 | Loss: 0.00001751
Iteration 94/1000 | Loss: 0.00001751
Iteration 95/1000 | Loss: 0.00001751
Iteration 96/1000 | Loss: 0.00001751
Iteration 97/1000 | Loss: 0.00001750
Iteration 98/1000 | Loss: 0.00001750
Iteration 99/1000 | Loss: 0.00001750
Iteration 100/1000 | Loss: 0.00001750
Iteration 101/1000 | Loss: 0.00001750
Iteration 102/1000 | Loss: 0.00001749
Iteration 103/1000 | Loss: 0.00001749
Iteration 104/1000 | Loss: 0.00001749
Iteration 105/1000 | Loss: 0.00001749
Iteration 106/1000 | Loss: 0.00001749
Iteration 107/1000 | Loss: 0.00001748
Iteration 108/1000 | Loss: 0.00001748
Iteration 109/1000 | Loss: 0.00001748
Iteration 110/1000 | Loss: 0.00001748
Iteration 111/1000 | Loss: 0.00001748
Iteration 112/1000 | Loss: 0.00001748
Iteration 113/1000 | Loss: 0.00001748
Iteration 114/1000 | Loss: 0.00001748
Iteration 115/1000 | Loss: 0.00001748
Iteration 116/1000 | Loss: 0.00001748
Iteration 117/1000 | Loss: 0.00001748
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001747
Iteration 138/1000 | Loss: 0.00001747
Iteration 139/1000 | Loss: 0.00001747
Iteration 140/1000 | Loss: 0.00001747
Iteration 141/1000 | Loss: 0.00001747
Iteration 142/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.74742908711778e-05, 1.74742908711778e-05, 1.74742908711778e-05, 1.74742908711778e-05, 1.74742908711778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.74742908711778e-05

Optimization complete. Final v2v error: 3.514176845550537 mm

Highest mean error: 4.701679706573486 mm for frame 183

Lowest mean error: 3.0553131103515625 mm for frame 87

Saving results

Total time: 105.45215129852295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897756
Iteration 2/25 | Loss: 0.00167248
Iteration 3/25 | Loss: 0.00145710
Iteration 4/25 | Loss: 0.00143072
Iteration 5/25 | Loss: 0.00142264
Iteration 6/25 | Loss: 0.00142166
Iteration 7/25 | Loss: 0.00142166
Iteration 8/25 | Loss: 0.00142166
Iteration 9/25 | Loss: 0.00142166
Iteration 10/25 | Loss: 0.00142166
Iteration 11/25 | Loss: 0.00142166
Iteration 12/25 | Loss: 0.00142166
Iteration 13/25 | Loss: 0.00142166
Iteration 14/25 | Loss: 0.00142166
Iteration 15/25 | Loss: 0.00142166
Iteration 16/25 | Loss: 0.00142166
Iteration 17/25 | Loss: 0.00142166
Iteration 18/25 | Loss: 0.00142166
Iteration 19/25 | Loss: 0.00142166
Iteration 20/25 | Loss: 0.00142166
Iteration 21/25 | Loss: 0.00142166
Iteration 22/25 | Loss: 0.00142166
Iteration 23/25 | Loss: 0.00142166
Iteration 24/25 | Loss: 0.00142166
Iteration 25/25 | Loss: 0.00142166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06869256
Iteration 2/25 | Loss: 0.00107604
Iteration 3/25 | Loss: 0.00107604
Iteration 4/25 | Loss: 0.00107603
Iteration 5/25 | Loss: 0.00107603
Iteration 6/25 | Loss: 0.00107603
Iteration 7/25 | Loss: 0.00107603
Iteration 8/25 | Loss: 0.00107603
Iteration 9/25 | Loss: 0.00107603
Iteration 10/25 | Loss: 0.00107603
Iteration 11/25 | Loss: 0.00107603
Iteration 12/25 | Loss: 0.00107603
Iteration 13/25 | Loss: 0.00107603
Iteration 14/25 | Loss: 0.00107603
Iteration 15/25 | Loss: 0.00107603
Iteration 16/25 | Loss: 0.00107603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010760326404124498, 0.0010760326404124498, 0.0010760326404124498, 0.0010760326404124498, 0.0010760326404124498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010760326404124498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107603
Iteration 2/1000 | Loss: 0.00006399
Iteration 3/1000 | Loss: 0.00004472
Iteration 4/1000 | Loss: 0.00004071
Iteration 5/1000 | Loss: 0.00003860
Iteration 6/1000 | Loss: 0.00003727
Iteration 7/1000 | Loss: 0.00003621
Iteration 8/1000 | Loss: 0.00003550
Iteration 9/1000 | Loss: 0.00003498
Iteration 10/1000 | Loss: 0.00003457
Iteration 11/1000 | Loss: 0.00003422
Iteration 12/1000 | Loss: 0.00003390
Iteration 13/1000 | Loss: 0.00003356
Iteration 14/1000 | Loss: 0.00003328
Iteration 15/1000 | Loss: 0.00003302
Iteration 16/1000 | Loss: 0.00003278
Iteration 17/1000 | Loss: 0.00003261
Iteration 18/1000 | Loss: 0.00003247
Iteration 19/1000 | Loss: 0.00003235
Iteration 20/1000 | Loss: 0.00003233
Iteration 21/1000 | Loss: 0.00003228
Iteration 22/1000 | Loss: 0.00003220
Iteration 23/1000 | Loss: 0.00003219
Iteration 24/1000 | Loss: 0.00003217
Iteration 25/1000 | Loss: 0.00003215
Iteration 26/1000 | Loss: 0.00003215
Iteration 27/1000 | Loss: 0.00003215
Iteration 28/1000 | Loss: 0.00003215
Iteration 29/1000 | Loss: 0.00003214
Iteration 30/1000 | Loss: 0.00003214
Iteration 31/1000 | Loss: 0.00003208
Iteration 32/1000 | Loss: 0.00003206
Iteration 33/1000 | Loss: 0.00003204
Iteration 34/1000 | Loss: 0.00003204
Iteration 35/1000 | Loss: 0.00003204
Iteration 36/1000 | Loss: 0.00003204
Iteration 37/1000 | Loss: 0.00003204
Iteration 38/1000 | Loss: 0.00003204
Iteration 39/1000 | Loss: 0.00003204
Iteration 40/1000 | Loss: 0.00003204
Iteration 41/1000 | Loss: 0.00003204
Iteration 42/1000 | Loss: 0.00003203
Iteration 43/1000 | Loss: 0.00003203
Iteration 44/1000 | Loss: 0.00003203
Iteration 45/1000 | Loss: 0.00003202
Iteration 46/1000 | Loss: 0.00003201
Iteration 47/1000 | Loss: 0.00003201
Iteration 48/1000 | Loss: 0.00003200
Iteration 49/1000 | Loss: 0.00003200
Iteration 50/1000 | Loss: 0.00003199
Iteration 51/1000 | Loss: 0.00003199
Iteration 52/1000 | Loss: 0.00003198
Iteration 53/1000 | Loss: 0.00003198
Iteration 54/1000 | Loss: 0.00003198
Iteration 55/1000 | Loss: 0.00003197
Iteration 56/1000 | Loss: 0.00003197
Iteration 57/1000 | Loss: 0.00003196
Iteration 58/1000 | Loss: 0.00003195
Iteration 59/1000 | Loss: 0.00003195
Iteration 60/1000 | Loss: 0.00003195
Iteration 61/1000 | Loss: 0.00003195
Iteration 62/1000 | Loss: 0.00003194
Iteration 63/1000 | Loss: 0.00003194
Iteration 64/1000 | Loss: 0.00003194
Iteration 65/1000 | Loss: 0.00003194
Iteration 66/1000 | Loss: 0.00003193
Iteration 67/1000 | Loss: 0.00003193
Iteration 68/1000 | Loss: 0.00003192
Iteration 69/1000 | Loss: 0.00003192
Iteration 70/1000 | Loss: 0.00003192
Iteration 71/1000 | Loss: 0.00003192
Iteration 72/1000 | Loss: 0.00003192
Iteration 73/1000 | Loss: 0.00003192
Iteration 74/1000 | Loss: 0.00003192
Iteration 75/1000 | Loss: 0.00003192
Iteration 76/1000 | Loss: 0.00003192
Iteration 77/1000 | Loss: 0.00003192
Iteration 78/1000 | Loss: 0.00003192
Iteration 79/1000 | Loss: 0.00003191
Iteration 80/1000 | Loss: 0.00003191
Iteration 81/1000 | Loss: 0.00003191
Iteration 82/1000 | Loss: 0.00003191
Iteration 83/1000 | Loss: 0.00003191
Iteration 84/1000 | Loss: 0.00003191
Iteration 85/1000 | Loss: 0.00003191
Iteration 86/1000 | Loss: 0.00003191
Iteration 87/1000 | Loss: 0.00003191
Iteration 88/1000 | Loss: 0.00003191
Iteration 89/1000 | Loss: 0.00003191
Iteration 90/1000 | Loss: 0.00003191
Iteration 91/1000 | Loss: 0.00003191
Iteration 92/1000 | Loss: 0.00003191
Iteration 93/1000 | Loss: 0.00003190
Iteration 94/1000 | Loss: 0.00003190
Iteration 95/1000 | Loss: 0.00003190
Iteration 96/1000 | Loss: 0.00003190
Iteration 97/1000 | Loss: 0.00003190
Iteration 98/1000 | Loss: 0.00003189
Iteration 99/1000 | Loss: 0.00003189
Iteration 100/1000 | Loss: 0.00003189
Iteration 101/1000 | Loss: 0.00003189
Iteration 102/1000 | Loss: 0.00003189
Iteration 103/1000 | Loss: 0.00003189
Iteration 104/1000 | Loss: 0.00003189
Iteration 105/1000 | Loss: 0.00003188
Iteration 106/1000 | Loss: 0.00003188
Iteration 107/1000 | Loss: 0.00003188
Iteration 108/1000 | Loss: 0.00003188
Iteration 109/1000 | Loss: 0.00003188
Iteration 110/1000 | Loss: 0.00003188
Iteration 111/1000 | Loss: 0.00003188
Iteration 112/1000 | Loss: 0.00003187
Iteration 113/1000 | Loss: 0.00003187
Iteration 114/1000 | Loss: 0.00003187
Iteration 115/1000 | Loss: 0.00003187
Iteration 116/1000 | Loss: 0.00003187
Iteration 117/1000 | Loss: 0.00003187
Iteration 118/1000 | Loss: 0.00003187
Iteration 119/1000 | Loss: 0.00003187
Iteration 120/1000 | Loss: 0.00003187
Iteration 121/1000 | Loss: 0.00003187
Iteration 122/1000 | Loss: 0.00003187
Iteration 123/1000 | Loss: 0.00003186
Iteration 124/1000 | Loss: 0.00003186
Iteration 125/1000 | Loss: 0.00003186
Iteration 126/1000 | Loss: 0.00003186
Iteration 127/1000 | Loss: 0.00003186
Iteration 128/1000 | Loss: 0.00003186
Iteration 129/1000 | Loss: 0.00003186
Iteration 130/1000 | Loss: 0.00003186
Iteration 131/1000 | Loss: 0.00003185
Iteration 132/1000 | Loss: 0.00003185
Iteration 133/1000 | Loss: 0.00003185
Iteration 134/1000 | Loss: 0.00003185
Iteration 135/1000 | Loss: 0.00003185
Iteration 136/1000 | Loss: 0.00003185
Iteration 137/1000 | Loss: 0.00003185
Iteration 138/1000 | Loss: 0.00003184
Iteration 139/1000 | Loss: 0.00003184
Iteration 140/1000 | Loss: 0.00003184
Iteration 141/1000 | Loss: 0.00003184
Iteration 142/1000 | Loss: 0.00003184
Iteration 143/1000 | Loss: 0.00003184
Iteration 144/1000 | Loss: 0.00003184
Iteration 145/1000 | Loss: 0.00003184
Iteration 146/1000 | Loss: 0.00003184
Iteration 147/1000 | Loss: 0.00003183
Iteration 148/1000 | Loss: 0.00003183
Iteration 149/1000 | Loss: 0.00003183
Iteration 150/1000 | Loss: 0.00003183
Iteration 151/1000 | Loss: 0.00003183
Iteration 152/1000 | Loss: 0.00003183
Iteration 153/1000 | Loss: 0.00003183
Iteration 154/1000 | Loss: 0.00003183
Iteration 155/1000 | Loss: 0.00003183
Iteration 156/1000 | Loss: 0.00003183
Iteration 157/1000 | Loss: 0.00003183
Iteration 158/1000 | Loss: 0.00003183
Iteration 159/1000 | Loss: 0.00003183
Iteration 160/1000 | Loss: 0.00003183
Iteration 161/1000 | Loss: 0.00003182
Iteration 162/1000 | Loss: 0.00003182
Iteration 163/1000 | Loss: 0.00003182
Iteration 164/1000 | Loss: 0.00003182
Iteration 165/1000 | Loss: 0.00003182
Iteration 166/1000 | Loss: 0.00003182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.1823648896533996e-05, 3.1823648896533996e-05, 3.1823648896533996e-05, 3.1823648896533996e-05, 3.1823648896533996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1823648896533996e-05

Optimization complete. Final v2v error: 4.6400041580200195 mm

Highest mean error: 5.8129425048828125 mm for frame 144

Lowest mean error: 3.9429879188537598 mm for frame 45

Saving results

Total time: 55.77707815170288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503749
Iteration 2/25 | Loss: 0.00161201
Iteration 3/25 | Loss: 0.00141321
Iteration 4/25 | Loss: 0.00139381
Iteration 5/25 | Loss: 0.00138816
Iteration 6/25 | Loss: 0.00138786
Iteration 7/25 | Loss: 0.00138786
Iteration 8/25 | Loss: 0.00138786
Iteration 9/25 | Loss: 0.00138786
Iteration 10/25 | Loss: 0.00138786
Iteration 11/25 | Loss: 0.00138786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013878559693694115, 0.0013878559693694115, 0.0013878559693694115, 0.0013878559693694115, 0.0013878559693694115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013878559693694115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39823592
Iteration 2/25 | Loss: 0.00091956
Iteration 3/25 | Loss: 0.00091955
Iteration 4/25 | Loss: 0.00091955
Iteration 5/25 | Loss: 0.00091955
Iteration 6/25 | Loss: 0.00091955
Iteration 7/25 | Loss: 0.00091955
Iteration 8/25 | Loss: 0.00091955
Iteration 9/25 | Loss: 0.00091955
Iteration 10/25 | Loss: 0.00091955
Iteration 11/25 | Loss: 0.00091955
Iteration 12/25 | Loss: 0.00091955
Iteration 13/25 | Loss: 0.00091955
Iteration 14/25 | Loss: 0.00091955
Iteration 15/25 | Loss: 0.00091955
Iteration 16/25 | Loss: 0.00091955
Iteration 17/25 | Loss: 0.00091955
Iteration 18/25 | Loss: 0.00091955
Iteration 19/25 | Loss: 0.00091955
Iteration 20/25 | Loss: 0.00091955
Iteration 21/25 | Loss: 0.00091955
Iteration 22/25 | Loss: 0.00091955
Iteration 23/25 | Loss: 0.00091955
Iteration 24/25 | Loss: 0.00091955
Iteration 25/25 | Loss: 0.00091955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091955
Iteration 2/1000 | Loss: 0.00005181
Iteration 3/1000 | Loss: 0.00003588
Iteration 4/1000 | Loss: 0.00003246
Iteration 5/1000 | Loss: 0.00003132
Iteration 6/1000 | Loss: 0.00003016
Iteration 7/1000 | Loss: 0.00002914
Iteration 8/1000 | Loss: 0.00002859
Iteration 9/1000 | Loss: 0.00002816
Iteration 10/1000 | Loss: 0.00002785
Iteration 11/1000 | Loss: 0.00002761
Iteration 12/1000 | Loss: 0.00002739
Iteration 13/1000 | Loss: 0.00002719
Iteration 14/1000 | Loss: 0.00002699
Iteration 15/1000 | Loss: 0.00002683
Iteration 16/1000 | Loss: 0.00002682
Iteration 17/1000 | Loss: 0.00002681
Iteration 18/1000 | Loss: 0.00002679
Iteration 19/1000 | Loss: 0.00002679
Iteration 20/1000 | Loss: 0.00002667
Iteration 21/1000 | Loss: 0.00002665
Iteration 22/1000 | Loss: 0.00002659
Iteration 23/1000 | Loss: 0.00002652
Iteration 24/1000 | Loss: 0.00002652
Iteration 25/1000 | Loss: 0.00002649
Iteration 26/1000 | Loss: 0.00002647
Iteration 27/1000 | Loss: 0.00002647
Iteration 28/1000 | Loss: 0.00002647
Iteration 29/1000 | Loss: 0.00002645
Iteration 30/1000 | Loss: 0.00002645
Iteration 31/1000 | Loss: 0.00002645
Iteration 32/1000 | Loss: 0.00002643
Iteration 33/1000 | Loss: 0.00002643
Iteration 34/1000 | Loss: 0.00002642
Iteration 35/1000 | Loss: 0.00002642
Iteration 36/1000 | Loss: 0.00002642
Iteration 37/1000 | Loss: 0.00002642
Iteration 38/1000 | Loss: 0.00002642
Iteration 39/1000 | Loss: 0.00002642
Iteration 40/1000 | Loss: 0.00002642
Iteration 41/1000 | Loss: 0.00002642
Iteration 42/1000 | Loss: 0.00002642
Iteration 43/1000 | Loss: 0.00002642
Iteration 44/1000 | Loss: 0.00002642
Iteration 45/1000 | Loss: 0.00002641
Iteration 46/1000 | Loss: 0.00002641
Iteration 47/1000 | Loss: 0.00002641
Iteration 48/1000 | Loss: 0.00002640
Iteration 49/1000 | Loss: 0.00002640
Iteration 50/1000 | Loss: 0.00002640
Iteration 51/1000 | Loss: 0.00002639
Iteration 52/1000 | Loss: 0.00002639
Iteration 53/1000 | Loss: 0.00002639
Iteration 54/1000 | Loss: 0.00002639
Iteration 55/1000 | Loss: 0.00002639
Iteration 56/1000 | Loss: 0.00002639
Iteration 57/1000 | Loss: 0.00002639
Iteration 58/1000 | Loss: 0.00002639
Iteration 59/1000 | Loss: 0.00002638
Iteration 60/1000 | Loss: 0.00002638
Iteration 61/1000 | Loss: 0.00002638
Iteration 62/1000 | Loss: 0.00002638
Iteration 63/1000 | Loss: 0.00002638
Iteration 64/1000 | Loss: 0.00002638
Iteration 65/1000 | Loss: 0.00002638
Iteration 66/1000 | Loss: 0.00002637
Iteration 67/1000 | Loss: 0.00002637
Iteration 68/1000 | Loss: 0.00002636
Iteration 69/1000 | Loss: 0.00002636
Iteration 70/1000 | Loss: 0.00002636
Iteration 71/1000 | Loss: 0.00002636
Iteration 72/1000 | Loss: 0.00002636
Iteration 73/1000 | Loss: 0.00002636
Iteration 74/1000 | Loss: 0.00002636
Iteration 75/1000 | Loss: 0.00002636
Iteration 76/1000 | Loss: 0.00002636
Iteration 77/1000 | Loss: 0.00002636
Iteration 78/1000 | Loss: 0.00002636
Iteration 79/1000 | Loss: 0.00002636
Iteration 80/1000 | Loss: 0.00002636
Iteration 81/1000 | Loss: 0.00002636
Iteration 82/1000 | Loss: 0.00002636
Iteration 83/1000 | Loss: 0.00002636
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002636
Iteration 86/1000 | Loss: 0.00002636
Iteration 87/1000 | Loss: 0.00002636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.636151111801155e-05, 2.636151111801155e-05, 2.636151111801155e-05, 2.636151111801155e-05, 2.636151111801155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.636151111801155e-05

Optimization complete. Final v2v error: 4.388587474822998 mm

Highest mean error: 4.817619800567627 mm for frame 105

Lowest mean error: 3.9959490299224854 mm for frame 223

Saving results

Total time: 42.888516902923584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475184
Iteration 2/25 | Loss: 0.00143531
Iteration 3/25 | Loss: 0.00136404
Iteration 4/25 | Loss: 0.00134957
Iteration 5/25 | Loss: 0.00134434
Iteration 6/25 | Loss: 0.00134350
Iteration 7/25 | Loss: 0.00134350
Iteration 8/25 | Loss: 0.00134350
Iteration 9/25 | Loss: 0.00134350
Iteration 10/25 | Loss: 0.00134350
Iteration 11/25 | Loss: 0.00134350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013435047585517168, 0.0013435047585517168, 0.0013435047585517168, 0.0013435047585517168, 0.0013435047585517168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013435047585517168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14357996
Iteration 2/25 | Loss: 0.00098852
Iteration 3/25 | Loss: 0.00098851
Iteration 4/25 | Loss: 0.00098851
Iteration 5/25 | Loss: 0.00098851
Iteration 6/25 | Loss: 0.00098851
Iteration 7/25 | Loss: 0.00098851
Iteration 8/25 | Loss: 0.00098851
Iteration 9/25 | Loss: 0.00098851
Iteration 10/25 | Loss: 0.00098851
Iteration 11/25 | Loss: 0.00098851
Iteration 12/25 | Loss: 0.00098851
Iteration 13/25 | Loss: 0.00098851
Iteration 14/25 | Loss: 0.00098851
Iteration 15/25 | Loss: 0.00098851
Iteration 16/25 | Loss: 0.00098851
Iteration 17/25 | Loss: 0.00098851
Iteration 18/25 | Loss: 0.00098851
Iteration 19/25 | Loss: 0.00098851
Iteration 20/25 | Loss: 0.00098851
Iteration 21/25 | Loss: 0.00098851
Iteration 22/25 | Loss: 0.00098851
Iteration 23/25 | Loss: 0.00098851
Iteration 24/25 | Loss: 0.00098851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009885072940960526, 0.0009885072940960526, 0.0009885072940960526, 0.0009885072940960526, 0.0009885072940960526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009885072940960526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098851
Iteration 2/1000 | Loss: 0.00003568
Iteration 3/1000 | Loss: 0.00002588
Iteration 4/1000 | Loss: 0.00002351
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002235
Iteration 7/1000 | Loss: 0.00002200
Iteration 8/1000 | Loss: 0.00002177
Iteration 9/1000 | Loss: 0.00002149
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002116
Iteration 12/1000 | Loss: 0.00002115
Iteration 13/1000 | Loss: 0.00002112
Iteration 14/1000 | Loss: 0.00002111
Iteration 15/1000 | Loss: 0.00002111
Iteration 16/1000 | Loss: 0.00002109
Iteration 17/1000 | Loss: 0.00002108
Iteration 18/1000 | Loss: 0.00002101
Iteration 19/1000 | Loss: 0.00002091
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002079
Iteration 22/1000 | Loss: 0.00002078
Iteration 23/1000 | Loss: 0.00002077
Iteration 24/1000 | Loss: 0.00002077
Iteration 25/1000 | Loss: 0.00002077
Iteration 26/1000 | Loss: 0.00002075
Iteration 27/1000 | Loss: 0.00002074
Iteration 28/1000 | Loss: 0.00002073
Iteration 29/1000 | Loss: 0.00002073
Iteration 30/1000 | Loss: 0.00002072
Iteration 31/1000 | Loss: 0.00002072
Iteration 32/1000 | Loss: 0.00002072
Iteration 33/1000 | Loss: 0.00002071
Iteration 34/1000 | Loss: 0.00002070
Iteration 35/1000 | Loss: 0.00002070
Iteration 36/1000 | Loss: 0.00002069
Iteration 37/1000 | Loss: 0.00002069
Iteration 38/1000 | Loss: 0.00002069
Iteration 39/1000 | Loss: 0.00002069
Iteration 40/1000 | Loss: 0.00002068
Iteration 41/1000 | Loss: 0.00002068
Iteration 42/1000 | Loss: 0.00002067
Iteration 43/1000 | Loss: 0.00002067
Iteration 44/1000 | Loss: 0.00002067
Iteration 45/1000 | Loss: 0.00002067
Iteration 46/1000 | Loss: 0.00002066
Iteration 47/1000 | Loss: 0.00002066
Iteration 48/1000 | Loss: 0.00002066
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002065
Iteration 51/1000 | Loss: 0.00002065
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002065
Iteration 54/1000 | Loss: 0.00002064
Iteration 55/1000 | Loss: 0.00002064
Iteration 56/1000 | Loss: 0.00002064
Iteration 57/1000 | Loss: 0.00002064
Iteration 58/1000 | Loss: 0.00002063
Iteration 59/1000 | Loss: 0.00002063
Iteration 60/1000 | Loss: 0.00002062
Iteration 61/1000 | Loss: 0.00002062
Iteration 62/1000 | Loss: 0.00002061
Iteration 63/1000 | Loss: 0.00002061
Iteration 64/1000 | Loss: 0.00002061
Iteration 65/1000 | Loss: 0.00002061
Iteration 66/1000 | Loss: 0.00002060
Iteration 67/1000 | Loss: 0.00002060
Iteration 68/1000 | Loss: 0.00002060
Iteration 69/1000 | Loss: 0.00002059
Iteration 70/1000 | Loss: 0.00002058
Iteration 71/1000 | Loss: 0.00002058
Iteration 72/1000 | Loss: 0.00002058
Iteration 73/1000 | Loss: 0.00002058
Iteration 74/1000 | Loss: 0.00002058
Iteration 75/1000 | Loss: 0.00002057
Iteration 76/1000 | Loss: 0.00002057
Iteration 77/1000 | Loss: 0.00002057
Iteration 78/1000 | Loss: 0.00002056
Iteration 79/1000 | Loss: 0.00002056
Iteration 80/1000 | Loss: 0.00002056
Iteration 81/1000 | Loss: 0.00002056
Iteration 82/1000 | Loss: 0.00002056
Iteration 83/1000 | Loss: 0.00002056
Iteration 84/1000 | Loss: 0.00002056
Iteration 85/1000 | Loss: 0.00002055
Iteration 86/1000 | Loss: 0.00002055
Iteration 87/1000 | Loss: 0.00002055
Iteration 88/1000 | Loss: 0.00002055
Iteration 89/1000 | Loss: 0.00002055
Iteration 90/1000 | Loss: 0.00002054
Iteration 91/1000 | Loss: 0.00002054
Iteration 92/1000 | Loss: 0.00002054
Iteration 93/1000 | Loss: 0.00002054
Iteration 94/1000 | Loss: 0.00002054
Iteration 95/1000 | Loss: 0.00002054
Iteration 96/1000 | Loss: 0.00002054
Iteration 97/1000 | Loss: 0.00002054
Iteration 98/1000 | Loss: 0.00002054
Iteration 99/1000 | Loss: 0.00002054
Iteration 100/1000 | Loss: 0.00002053
Iteration 101/1000 | Loss: 0.00002053
Iteration 102/1000 | Loss: 0.00002053
Iteration 103/1000 | Loss: 0.00002053
Iteration 104/1000 | Loss: 0.00002053
Iteration 105/1000 | Loss: 0.00002053
Iteration 106/1000 | Loss: 0.00002053
Iteration 107/1000 | Loss: 0.00002053
Iteration 108/1000 | Loss: 0.00002053
Iteration 109/1000 | Loss: 0.00002053
Iteration 110/1000 | Loss: 0.00002053
Iteration 111/1000 | Loss: 0.00002052
Iteration 112/1000 | Loss: 0.00002052
Iteration 113/1000 | Loss: 0.00002052
Iteration 114/1000 | Loss: 0.00002052
Iteration 115/1000 | Loss: 0.00002052
Iteration 116/1000 | Loss: 0.00002052
Iteration 117/1000 | Loss: 0.00002052
Iteration 118/1000 | Loss: 0.00002052
Iteration 119/1000 | Loss: 0.00002051
Iteration 120/1000 | Loss: 0.00002051
Iteration 121/1000 | Loss: 0.00002051
Iteration 122/1000 | Loss: 0.00002050
Iteration 123/1000 | Loss: 0.00002050
Iteration 124/1000 | Loss: 0.00002050
Iteration 125/1000 | Loss: 0.00002049
Iteration 126/1000 | Loss: 0.00002049
Iteration 127/1000 | Loss: 0.00002049
Iteration 128/1000 | Loss: 0.00002049
Iteration 129/1000 | Loss: 0.00002049
Iteration 130/1000 | Loss: 0.00002048
Iteration 131/1000 | Loss: 0.00002048
Iteration 132/1000 | Loss: 0.00002048
Iteration 133/1000 | Loss: 0.00002048
Iteration 134/1000 | Loss: 0.00002048
Iteration 135/1000 | Loss: 0.00002048
Iteration 136/1000 | Loss: 0.00002048
Iteration 137/1000 | Loss: 0.00002048
Iteration 138/1000 | Loss: 0.00002047
Iteration 139/1000 | Loss: 0.00002047
Iteration 140/1000 | Loss: 0.00002047
Iteration 141/1000 | Loss: 0.00002047
Iteration 142/1000 | Loss: 0.00002047
Iteration 143/1000 | Loss: 0.00002047
Iteration 144/1000 | Loss: 0.00002047
Iteration 145/1000 | Loss: 0.00002047
Iteration 146/1000 | Loss: 0.00002047
Iteration 147/1000 | Loss: 0.00002047
Iteration 148/1000 | Loss: 0.00002047
Iteration 149/1000 | Loss: 0.00002046
Iteration 150/1000 | Loss: 0.00002046
Iteration 151/1000 | Loss: 0.00002046
Iteration 152/1000 | Loss: 0.00002046
Iteration 153/1000 | Loss: 0.00002046
Iteration 154/1000 | Loss: 0.00002045
Iteration 155/1000 | Loss: 0.00002045
Iteration 156/1000 | Loss: 0.00002045
Iteration 157/1000 | Loss: 0.00002045
Iteration 158/1000 | Loss: 0.00002045
Iteration 159/1000 | Loss: 0.00002044
Iteration 160/1000 | Loss: 0.00002044
Iteration 161/1000 | Loss: 0.00002044
Iteration 162/1000 | Loss: 0.00002044
Iteration 163/1000 | Loss: 0.00002044
Iteration 164/1000 | Loss: 0.00002044
Iteration 165/1000 | Loss: 0.00002043
Iteration 166/1000 | Loss: 0.00002043
Iteration 167/1000 | Loss: 0.00002043
Iteration 168/1000 | Loss: 0.00002043
Iteration 169/1000 | Loss: 0.00002042
Iteration 170/1000 | Loss: 0.00002042
Iteration 171/1000 | Loss: 0.00002042
Iteration 172/1000 | Loss: 0.00002042
Iteration 173/1000 | Loss: 0.00002042
Iteration 174/1000 | Loss: 0.00002042
Iteration 175/1000 | Loss: 0.00002042
Iteration 176/1000 | Loss: 0.00002042
Iteration 177/1000 | Loss: 0.00002042
Iteration 178/1000 | Loss: 0.00002042
Iteration 179/1000 | Loss: 0.00002042
Iteration 180/1000 | Loss: 0.00002042
Iteration 181/1000 | Loss: 0.00002042
Iteration 182/1000 | Loss: 0.00002042
Iteration 183/1000 | Loss: 0.00002042
Iteration 184/1000 | Loss: 0.00002041
Iteration 185/1000 | Loss: 0.00002041
Iteration 186/1000 | Loss: 0.00002041
Iteration 187/1000 | Loss: 0.00002041
Iteration 188/1000 | Loss: 0.00002041
Iteration 189/1000 | Loss: 0.00002041
Iteration 190/1000 | Loss: 0.00002040
Iteration 191/1000 | Loss: 0.00002040
Iteration 192/1000 | Loss: 0.00002040
Iteration 193/1000 | Loss: 0.00002040
Iteration 194/1000 | Loss: 0.00002040
Iteration 195/1000 | Loss: 0.00002040
Iteration 196/1000 | Loss: 0.00002040
Iteration 197/1000 | Loss: 0.00002039
Iteration 198/1000 | Loss: 0.00002039
Iteration 199/1000 | Loss: 0.00002039
Iteration 200/1000 | Loss: 0.00002039
Iteration 201/1000 | Loss: 0.00002039
Iteration 202/1000 | Loss: 0.00002039
Iteration 203/1000 | Loss: 0.00002039
Iteration 204/1000 | Loss: 0.00002039
Iteration 205/1000 | Loss: 0.00002039
Iteration 206/1000 | Loss: 0.00002039
Iteration 207/1000 | Loss: 0.00002039
Iteration 208/1000 | Loss: 0.00002039
Iteration 209/1000 | Loss: 0.00002039
Iteration 210/1000 | Loss: 0.00002039
Iteration 211/1000 | Loss: 0.00002039
Iteration 212/1000 | Loss: 0.00002039
Iteration 213/1000 | Loss: 0.00002038
Iteration 214/1000 | Loss: 0.00002038
Iteration 215/1000 | Loss: 0.00002038
Iteration 216/1000 | Loss: 0.00002038
Iteration 217/1000 | Loss: 0.00002038
Iteration 218/1000 | Loss: 0.00002038
Iteration 219/1000 | Loss: 0.00002038
Iteration 220/1000 | Loss: 0.00002038
Iteration 221/1000 | Loss: 0.00002038
Iteration 222/1000 | Loss: 0.00002038
Iteration 223/1000 | Loss: 0.00002038
Iteration 224/1000 | Loss: 0.00002038
Iteration 225/1000 | Loss: 0.00002038
Iteration 226/1000 | Loss: 0.00002038
Iteration 227/1000 | Loss: 0.00002038
Iteration 228/1000 | Loss: 0.00002038
Iteration 229/1000 | Loss: 0.00002038
Iteration 230/1000 | Loss: 0.00002038
Iteration 231/1000 | Loss: 0.00002038
Iteration 232/1000 | Loss: 0.00002038
Iteration 233/1000 | Loss: 0.00002038
Iteration 234/1000 | Loss: 0.00002038
Iteration 235/1000 | Loss: 0.00002038
Iteration 236/1000 | Loss: 0.00002038
Iteration 237/1000 | Loss: 0.00002038
Iteration 238/1000 | Loss: 0.00002038
Iteration 239/1000 | Loss: 0.00002038
Iteration 240/1000 | Loss: 0.00002038
Iteration 241/1000 | Loss: 0.00002038
Iteration 242/1000 | Loss: 0.00002038
Iteration 243/1000 | Loss: 0.00002038
Iteration 244/1000 | Loss: 0.00002038
Iteration 245/1000 | Loss: 0.00002038
Iteration 246/1000 | Loss: 0.00002038
Iteration 247/1000 | Loss: 0.00002038
Iteration 248/1000 | Loss: 0.00002038
Iteration 249/1000 | Loss: 0.00002038
Iteration 250/1000 | Loss: 0.00002038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.03812014660798e-05, 2.03812014660798e-05, 2.03812014660798e-05, 2.03812014660798e-05, 2.03812014660798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.03812014660798e-05

Optimization complete. Final v2v error: 3.788201332092285 mm

Highest mean error: 4.539829730987549 mm for frame 182

Lowest mean error: 3.648073434829712 mm for frame 6

Saving results

Total time: 45.56763172149658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864003
Iteration 2/25 | Loss: 0.00230141
Iteration 3/25 | Loss: 0.00174657
Iteration 4/25 | Loss: 0.00162156
Iteration 5/25 | Loss: 0.00172298
Iteration 6/25 | Loss: 0.00159776
Iteration 7/25 | Loss: 0.00149207
Iteration 8/25 | Loss: 0.00149847
Iteration 9/25 | Loss: 0.00146760
Iteration 10/25 | Loss: 0.00149410
Iteration 11/25 | Loss: 0.00147129
Iteration 12/25 | Loss: 0.00147380
Iteration 13/25 | Loss: 0.00143237
Iteration 14/25 | Loss: 0.00141598
Iteration 15/25 | Loss: 0.00140944
Iteration 16/25 | Loss: 0.00140867
Iteration 17/25 | Loss: 0.00141012
Iteration 18/25 | Loss: 0.00140215
Iteration 19/25 | Loss: 0.00140491
Iteration 20/25 | Loss: 0.00139669
Iteration 21/25 | Loss: 0.00139948
Iteration 22/25 | Loss: 0.00139545
Iteration 23/25 | Loss: 0.00139375
Iteration 24/25 | Loss: 0.00139841
Iteration 25/25 | Loss: 0.00138822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50398827
Iteration 2/25 | Loss: 0.00090635
Iteration 3/25 | Loss: 0.00084301
Iteration 4/25 | Loss: 0.00084301
Iteration 5/25 | Loss: 0.00084301
Iteration 6/25 | Loss: 0.00084301
Iteration 7/25 | Loss: 0.00084301
Iteration 8/25 | Loss: 0.00084301
Iteration 9/25 | Loss: 0.00084301
Iteration 10/25 | Loss: 0.00084301
Iteration 11/25 | Loss: 0.00084301
Iteration 12/25 | Loss: 0.00084301
Iteration 13/25 | Loss: 0.00084301
Iteration 14/25 | Loss: 0.00084301
Iteration 15/25 | Loss: 0.00084301
Iteration 16/25 | Loss: 0.00084301
Iteration 17/25 | Loss: 0.00084301
Iteration 18/25 | Loss: 0.00084301
Iteration 19/25 | Loss: 0.00084301
Iteration 20/25 | Loss: 0.00084301
Iteration 21/25 | Loss: 0.00084301
Iteration 22/25 | Loss: 0.00084301
Iteration 23/25 | Loss: 0.00084301
Iteration 24/25 | Loss: 0.00084301
Iteration 25/25 | Loss: 0.00084301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084301
Iteration 2/1000 | Loss: 0.00006187
Iteration 3/1000 | Loss: 0.00011263
Iteration 4/1000 | Loss: 0.00004085
Iteration 5/1000 | Loss: 0.00008674
Iteration 6/1000 | Loss: 0.00023511
Iteration 7/1000 | Loss: 0.00013682
Iteration 8/1000 | Loss: 0.00003740
Iteration 9/1000 | Loss: 0.00022803
Iteration 10/1000 | Loss: 0.00008856
Iteration 11/1000 | Loss: 0.00019648
Iteration 12/1000 | Loss: 0.00004169
Iteration 13/1000 | Loss: 0.00003764
Iteration 14/1000 | Loss: 0.00003574
Iteration 15/1000 | Loss: 0.00003463
Iteration 16/1000 | Loss: 0.00003422
Iteration 17/1000 | Loss: 0.00003381
Iteration 18/1000 | Loss: 0.00015357
Iteration 19/1000 | Loss: 0.00003344
Iteration 20/1000 | Loss: 0.00003313
Iteration 21/1000 | Loss: 0.00003294
Iteration 22/1000 | Loss: 0.00003267
Iteration 23/1000 | Loss: 0.00003266
Iteration 24/1000 | Loss: 0.00003245
Iteration 25/1000 | Loss: 0.00003241
Iteration 26/1000 | Loss: 0.00003241
Iteration 27/1000 | Loss: 0.00003239
Iteration 28/1000 | Loss: 0.00003234
Iteration 29/1000 | Loss: 0.00003233
Iteration 30/1000 | Loss: 0.00003233
Iteration 31/1000 | Loss: 0.00003232
Iteration 32/1000 | Loss: 0.00003232
Iteration 33/1000 | Loss: 0.00003229
Iteration 34/1000 | Loss: 0.00003229
Iteration 35/1000 | Loss: 0.00003228
Iteration 36/1000 | Loss: 0.00003227
Iteration 37/1000 | Loss: 0.00003227
Iteration 38/1000 | Loss: 0.00003227
Iteration 39/1000 | Loss: 0.00003227
Iteration 40/1000 | Loss: 0.00003227
Iteration 41/1000 | Loss: 0.00003227
Iteration 42/1000 | Loss: 0.00003227
Iteration 43/1000 | Loss: 0.00003227
Iteration 44/1000 | Loss: 0.00003227
Iteration 45/1000 | Loss: 0.00003227
Iteration 46/1000 | Loss: 0.00003226
Iteration 47/1000 | Loss: 0.00003226
Iteration 48/1000 | Loss: 0.00003225
Iteration 49/1000 | Loss: 0.00003225
Iteration 50/1000 | Loss: 0.00003224
Iteration 51/1000 | Loss: 0.00003223
Iteration 52/1000 | Loss: 0.00003223
Iteration 53/1000 | Loss: 0.00003223
Iteration 54/1000 | Loss: 0.00003223
Iteration 55/1000 | Loss: 0.00003219
Iteration 56/1000 | Loss: 0.00003219
Iteration 57/1000 | Loss: 0.00003218
Iteration 58/1000 | Loss: 0.00003218
Iteration 59/1000 | Loss: 0.00003218
Iteration 60/1000 | Loss: 0.00003218
Iteration 61/1000 | Loss: 0.00003217
Iteration 62/1000 | Loss: 0.00003216
Iteration 63/1000 | Loss: 0.00003216
Iteration 64/1000 | Loss: 0.00003216
Iteration 65/1000 | Loss: 0.00003215
Iteration 66/1000 | Loss: 0.00003215
Iteration 67/1000 | Loss: 0.00003215
Iteration 68/1000 | Loss: 0.00003215
Iteration 69/1000 | Loss: 0.00003215
Iteration 70/1000 | Loss: 0.00003215
Iteration 71/1000 | Loss: 0.00003215
Iteration 72/1000 | Loss: 0.00003215
Iteration 73/1000 | Loss: 0.00003215
Iteration 74/1000 | Loss: 0.00003215
Iteration 75/1000 | Loss: 0.00003215
Iteration 76/1000 | Loss: 0.00003215
Iteration 77/1000 | Loss: 0.00003215
Iteration 78/1000 | Loss: 0.00003215
Iteration 79/1000 | Loss: 0.00003215
Iteration 80/1000 | Loss: 0.00003215
Iteration 81/1000 | Loss: 0.00003215
Iteration 82/1000 | Loss: 0.00003215
Iteration 83/1000 | Loss: 0.00003215
Iteration 84/1000 | Loss: 0.00003215
Iteration 85/1000 | Loss: 0.00003215
Iteration 86/1000 | Loss: 0.00003215
Iteration 87/1000 | Loss: 0.00003215
Iteration 88/1000 | Loss: 0.00003215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [3.214629759895615e-05, 3.214629759895615e-05, 3.214629759895615e-05, 3.214629759895615e-05, 3.214629759895615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.214629759895615e-05

Optimization complete. Final v2v error: 4.532200813293457 mm

Highest mean error: 11.605185508728027 mm for frame 33

Lowest mean error: 3.8261280059814453 mm for frame 59

Saving results

Total time: 94.62290811538696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805748
Iteration 2/25 | Loss: 0.00150714
Iteration 3/25 | Loss: 0.00130608
Iteration 4/25 | Loss: 0.00128675
Iteration 5/25 | Loss: 0.00128078
Iteration 6/25 | Loss: 0.00127955
Iteration 7/25 | Loss: 0.00127955
Iteration 8/25 | Loss: 0.00127955
Iteration 9/25 | Loss: 0.00127955
Iteration 10/25 | Loss: 0.00127955
Iteration 11/25 | Loss: 0.00127955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00127954944036901, 0.00127954944036901, 0.00127954944036901, 0.00127954944036901, 0.00127954944036901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00127954944036901

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75615549
Iteration 2/25 | Loss: 0.00090569
Iteration 3/25 | Loss: 0.00090569
Iteration 4/25 | Loss: 0.00090569
Iteration 5/25 | Loss: 0.00090569
Iteration 6/25 | Loss: 0.00090569
Iteration 7/25 | Loss: 0.00090569
Iteration 8/25 | Loss: 0.00090569
Iteration 9/25 | Loss: 0.00090569
Iteration 10/25 | Loss: 0.00090569
Iteration 11/25 | Loss: 0.00090568
Iteration 12/25 | Loss: 0.00090568
Iteration 13/25 | Loss: 0.00090568
Iteration 14/25 | Loss: 0.00090568
Iteration 15/25 | Loss: 0.00090568
Iteration 16/25 | Loss: 0.00090568
Iteration 17/25 | Loss: 0.00090568
Iteration 18/25 | Loss: 0.00090568
Iteration 19/25 | Loss: 0.00090568
Iteration 20/25 | Loss: 0.00090568
Iteration 21/25 | Loss: 0.00090568
Iteration 22/25 | Loss: 0.00090568
Iteration 23/25 | Loss: 0.00090568
Iteration 24/25 | Loss: 0.00090568
Iteration 25/25 | Loss: 0.00090568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090568
Iteration 2/1000 | Loss: 0.00002233
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00016193
Iteration 7/1000 | Loss: 0.00031205
Iteration 8/1000 | Loss: 0.00042348
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00004118
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00016497
Iteration 13/1000 | Loss: 0.00003308
Iteration 14/1000 | Loss: 0.00001449
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00027775
Iteration 17/1000 | Loss: 0.00002746
Iteration 18/1000 | Loss: 0.00002008
Iteration 19/1000 | Loss: 0.00001433
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001390
Iteration 23/1000 | Loss: 0.00001388
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00012274
Iteration 29/1000 | Loss: 0.00020870
Iteration 30/1000 | Loss: 0.00007816
Iteration 31/1000 | Loss: 0.00001400
Iteration 32/1000 | Loss: 0.00018693
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001321
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001315
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001315
Iteration 42/1000 | Loss: 0.00001315
Iteration 43/1000 | Loss: 0.00001315
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001314
Iteration 46/1000 | Loss: 0.00001314
Iteration 47/1000 | Loss: 0.00001314
Iteration 48/1000 | Loss: 0.00001314
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001311
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001310
Iteration 57/1000 | Loss: 0.00001310
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001308
Iteration 62/1000 | Loss: 0.00017763
Iteration 63/1000 | Loss: 0.00025091
Iteration 64/1000 | Loss: 0.00002702
Iteration 65/1000 | Loss: 0.00004590
Iteration 66/1000 | Loss: 0.00012382
Iteration 67/1000 | Loss: 0.00016360
Iteration 68/1000 | Loss: 0.00010900
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00004480
Iteration 71/1000 | Loss: 0.00002089
Iteration 72/1000 | Loss: 0.00004663
Iteration 73/1000 | Loss: 0.00005007
Iteration 74/1000 | Loss: 0.00013680
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001318
Iteration 78/1000 | Loss: 0.00001318
Iteration 79/1000 | Loss: 0.00001316
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001305
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001304
Iteration 94/1000 | Loss: 0.00001304
Iteration 95/1000 | Loss: 0.00001304
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001302
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001302
Iteration 101/1000 | Loss: 0.00001302
Iteration 102/1000 | Loss: 0.00001302
Iteration 103/1000 | Loss: 0.00001302
Iteration 104/1000 | Loss: 0.00001302
Iteration 105/1000 | Loss: 0.00001302
Iteration 106/1000 | Loss: 0.00001302
Iteration 107/1000 | Loss: 0.00001302
Iteration 108/1000 | Loss: 0.00001302
Iteration 109/1000 | Loss: 0.00001302
Iteration 110/1000 | Loss: 0.00001302
Iteration 111/1000 | Loss: 0.00001302
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.3015687727602199e-05, 1.3015687727602199e-05, 1.3015687727602199e-05, 1.3015687727602199e-05, 1.3015687727602199e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3015687727602199e-05

Optimization complete. Final v2v error: 3.0889170169830322 mm

Highest mean error: 3.5107274055480957 mm for frame 8

Lowest mean error: 2.849921703338623 mm for frame 190

Saving results

Total time: 85.44424653053284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960486
Iteration 2/25 | Loss: 0.00239017
Iteration 3/25 | Loss: 0.00194454
Iteration 4/25 | Loss: 0.00189683
Iteration 5/25 | Loss: 0.00189105
Iteration 6/25 | Loss: 0.00189107
Iteration 7/25 | Loss: 0.00188294
Iteration 8/25 | Loss: 0.00187930
Iteration 9/25 | Loss: 0.00187794
Iteration 10/25 | Loss: 0.00188081
Iteration 11/25 | Loss: 0.00187856
Iteration 12/25 | Loss: 0.00187409
Iteration 13/25 | Loss: 0.00187237
Iteration 14/25 | Loss: 0.00187203
Iteration 15/25 | Loss: 0.00187199
Iteration 16/25 | Loss: 0.00187199
Iteration 17/25 | Loss: 0.00187199
Iteration 18/25 | Loss: 0.00187199
Iteration 19/25 | Loss: 0.00187199
Iteration 20/25 | Loss: 0.00187198
Iteration 21/25 | Loss: 0.00187198
Iteration 22/25 | Loss: 0.00187198
Iteration 23/25 | Loss: 0.00187198
Iteration 24/25 | Loss: 0.00187198
Iteration 25/25 | Loss: 0.00187198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35938430
Iteration 2/25 | Loss: 0.00416862
Iteration 3/25 | Loss: 0.00416862
Iteration 4/25 | Loss: 0.00416862
Iteration 5/25 | Loss: 0.00416862
Iteration 6/25 | Loss: 0.00416862
Iteration 7/25 | Loss: 0.00416862
Iteration 8/25 | Loss: 0.00416862
Iteration 9/25 | Loss: 0.00416862
Iteration 10/25 | Loss: 0.00416862
Iteration 11/25 | Loss: 0.00416862
Iteration 12/25 | Loss: 0.00416862
Iteration 13/25 | Loss: 0.00416862
Iteration 14/25 | Loss: 0.00416862
Iteration 15/25 | Loss: 0.00416862
Iteration 16/25 | Loss: 0.00416862
Iteration 17/25 | Loss: 0.00416862
Iteration 18/25 | Loss: 0.00416862
Iteration 19/25 | Loss: 0.00416862
Iteration 20/25 | Loss: 0.00416862
Iteration 21/25 | Loss: 0.00416862
Iteration 22/25 | Loss: 0.00416862
Iteration 23/25 | Loss: 0.00416862
Iteration 24/25 | Loss: 0.00416862
Iteration 25/25 | Loss: 0.00416862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00416862
Iteration 2/1000 | Loss: 0.00056321
Iteration 3/1000 | Loss: 0.00042896
Iteration 4/1000 | Loss: 0.00036817
Iteration 5/1000 | Loss: 0.00033567
Iteration 6/1000 | Loss: 0.00030852
Iteration 7/1000 | Loss: 0.00029600
Iteration 8/1000 | Loss: 0.00028111
Iteration 9/1000 | Loss: 0.00027027
Iteration 10/1000 | Loss: 0.00026160
Iteration 11/1000 | Loss: 0.00025294
Iteration 12/1000 | Loss: 0.00106012
Iteration 13/1000 | Loss: 0.01346875
Iteration 14/1000 | Loss: 0.02376764
Iteration 15/1000 | Loss: 0.02006370
Iteration 16/1000 | Loss: 0.05104426
Iteration 17/1000 | Loss: 0.03420596
Iteration 18/1000 | Loss: 0.00334680
Iteration 19/1000 | Loss: 0.00229378
Iteration 20/1000 | Loss: 0.00127972
Iteration 21/1000 | Loss: 0.00067596
Iteration 22/1000 | Loss: 0.00042737
Iteration 23/1000 | Loss: 0.00045241
Iteration 24/1000 | Loss: 0.00043598
Iteration 25/1000 | Loss: 0.00114950
Iteration 26/1000 | Loss: 0.01010380
Iteration 27/1000 | Loss: 0.00469509
Iteration 28/1000 | Loss: 0.00324658
Iteration 29/1000 | Loss: 0.00247422
Iteration 30/1000 | Loss: 0.00114940
Iteration 31/1000 | Loss: 0.00202576
Iteration 32/1000 | Loss: 0.00190142
Iteration 33/1000 | Loss: 0.00365321
Iteration 34/1000 | Loss: 0.00608090
Iteration 35/1000 | Loss: 0.00405812
Iteration 36/1000 | Loss: 0.00384344
Iteration 37/1000 | Loss: 0.00526000
Iteration 38/1000 | Loss: 0.00543780
Iteration 39/1000 | Loss: 0.00593959
Iteration 40/1000 | Loss: 0.00333930
Iteration 41/1000 | Loss: 0.00476223
Iteration 42/1000 | Loss: 0.00556545
Iteration 43/1000 | Loss: 0.00321694
Iteration 44/1000 | Loss: 0.00182577
Iteration 45/1000 | Loss: 0.00192428
Iteration 46/1000 | Loss: 0.00237739
Iteration 47/1000 | Loss: 0.00195570
Iteration 48/1000 | Loss: 0.00251278
Iteration 49/1000 | Loss: 0.00220967
Iteration 50/1000 | Loss: 0.00144901
Iteration 51/1000 | Loss: 0.00165809
Iteration 52/1000 | Loss: 0.00139615
Iteration 53/1000 | Loss: 0.00143493
Iteration 54/1000 | Loss: 0.00165086
Iteration 55/1000 | Loss: 0.00194422
Iteration 56/1000 | Loss: 0.00261463
Iteration 57/1000 | Loss: 0.00125850
Iteration 58/1000 | Loss: 0.00099028
Iteration 59/1000 | Loss: 0.00123338
Iteration 60/1000 | Loss: 0.00109566
Iteration 61/1000 | Loss: 0.00135347
Iteration 62/1000 | Loss: 0.00140503
Iteration 63/1000 | Loss: 0.00138361
Iteration 64/1000 | Loss: 0.00127825
Iteration 65/1000 | Loss: 0.00110622
Iteration 66/1000 | Loss: 0.00090473
Iteration 67/1000 | Loss: 0.00124702
Iteration 68/1000 | Loss: 0.00124179
Iteration 69/1000 | Loss: 0.00142744
Iteration 70/1000 | Loss: 0.00131849
Iteration 71/1000 | Loss: 0.00167927
Iteration 72/1000 | Loss: 0.00079209
Iteration 73/1000 | Loss: 0.00134902
Iteration 74/1000 | Loss: 0.00086813
Iteration 75/1000 | Loss: 0.00098515
Iteration 76/1000 | Loss: 0.00102946
Iteration 77/1000 | Loss: 0.00054109
Iteration 78/1000 | Loss: 0.00076689
Iteration 79/1000 | Loss: 0.00079713
Iteration 80/1000 | Loss: 0.00112281
Iteration 81/1000 | Loss: 0.00064477
Iteration 82/1000 | Loss: 0.00035470
Iteration 83/1000 | Loss: 0.00037102
Iteration 84/1000 | Loss: 0.00027698
Iteration 85/1000 | Loss: 0.00026785
Iteration 86/1000 | Loss: 0.00020968
Iteration 87/1000 | Loss: 0.00029370
Iteration 88/1000 | Loss: 0.00020254
Iteration 89/1000 | Loss: 0.00022177
Iteration 90/1000 | Loss: 0.00032748
Iteration 91/1000 | Loss: 0.00027682
Iteration 92/1000 | Loss: 0.00013871
Iteration 93/1000 | Loss: 0.00032061
Iteration 94/1000 | Loss: 0.00024982
Iteration 95/1000 | Loss: 0.00016345
Iteration 96/1000 | Loss: 0.00012925
Iteration 97/1000 | Loss: 0.00015588
Iteration 98/1000 | Loss: 0.00057724
Iteration 99/1000 | Loss: 0.00053032
Iteration 100/1000 | Loss: 0.00034539
Iteration 101/1000 | Loss: 0.00093133
Iteration 102/1000 | Loss: 0.00058790
Iteration 103/1000 | Loss: 0.00071453
Iteration 104/1000 | Loss: 0.00031274
Iteration 105/1000 | Loss: 0.00096951
Iteration 106/1000 | Loss: 0.00051974
Iteration 107/1000 | Loss: 0.00021047
Iteration 108/1000 | Loss: 0.00016466
Iteration 109/1000 | Loss: 0.00027763
Iteration 110/1000 | Loss: 0.00030353
Iteration 111/1000 | Loss: 0.00022788
Iteration 112/1000 | Loss: 0.00035134
Iteration 113/1000 | Loss: 0.00022028
Iteration 114/1000 | Loss: 0.00043920
Iteration 115/1000 | Loss: 0.00077308
Iteration 116/1000 | Loss: 0.00050390
Iteration 117/1000 | Loss: 0.00037874
Iteration 118/1000 | Loss: 0.00013331
Iteration 119/1000 | Loss: 0.00044645
Iteration 120/1000 | Loss: 0.00054071
Iteration 121/1000 | Loss: 0.00053594
Iteration 122/1000 | Loss: 0.00052606
Iteration 123/1000 | Loss: 0.00026216
Iteration 124/1000 | Loss: 0.00035901
Iteration 125/1000 | Loss: 0.00021838
Iteration 126/1000 | Loss: 0.00007115
Iteration 127/1000 | Loss: 0.00006513
Iteration 128/1000 | Loss: 0.00059016
Iteration 129/1000 | Loss: 0.00048693
Iteration 130/1000 | Loss: 0.00008795
Iteration 131/1000 | Loss: 0.00006236
Iteration 132/1000 | Loss: 0.00078152
Iteration 133/1000 | Loss: 0.00067921
Iteration 134/1000 | Loss: 0.00048581
Iteration 135/1000 | Loss: 0.00027335
Iteration 136/1000 | Loss: 0.00012446
Iteration 137/1000 | Loss: 0.00012768
Iteration 138/1000 | Loss: 0.00009224
Iteration 139/1000 | Loss: 0.00027443
Iteration 140/1000 | Loss: 0.00018603
Iteration 141/1000 | Loss: 0.00022656
Iteration 142/1000 | Loss: 0.00008762
Iteration 143/1000 | Loss: 0.00009840
Iteration 144/1000 | Loss: 0.00010550
Iteration 145/1000 | Loss: 0.00005737
Iteration 146/1000 | Loss: 0.00004827
Iteration 147/1000 | Loss: 0.00004452
Iteration 148/1000 | Loss: 0.00039577
Iteration 149/1000 | Loss: 0.00026691
Iteration 150/1000 | Loss: 0.00029107
Iteration 151/1000 | Loss: 0.00022853
Iteration 152/1000 | Loss: 0.00014508
Iteration 153/1000 | Loss: 0.00022830
Iteration 154/1000 | Loss: 0.00016775
Iteration 155/1000 | Loss: 0.00019291
Iteration 156/1000 | Loss: 0.00006593
Iteration 157/1000 | Loss: 0.00004201
Iteration 158/1000 | Loss: 0.00004935
Iteration 159/1000 | Loss: 0.00004018
Iteration 160/1000 | Loss: 0.00004076
Iteration 161/1000 | Loss: 0.00004974
Iteration 162/1000 | Loss: 0.00051182
Iteration 163/1000 | Loss: 0.00055682
Iteration 164/1000 | Loss: 0.00004804
Iteration 165/1000 | Loss: 0.00004405
Iteration 166/1000 | Loss: 0.00038715
Iteration 167/1000 | Loss: 0.00005827
Iteration 168/1000 | Loss: 0.00028097
Iteration 169/1000 | Loss: 0.00043672
Iteration 170/1000 | Loss: 0.00083678
Iteration 171/1000 | Loss: 0.00022957
Iteration 172/1000 | Loss: 0.00010057
Iteration 173/1000 | Loss: 0.00032232
Iteration 174/1000 | Loss: 0.00081078
Iteration 175/1000 | Loss: 0.00028756
Iteration 176/1000 | Loss: 0.00021105
Iteration 177/1000 | Loss: 0.00015830
Iteration 178/1000 | Loss: 0.00017455
Iteration 179/1000 | Loss: 0.00022103
Iteration 180/1000 | Loss: 0.00005121
Iteration 181/1000 | Loss: 0.00031354
Iteration 182/1000 | Loss: 0.00093991
Iteration 183/1000 | Loss: 0.00013763
Iteration 184/1000 | Loss: 0.00008448
Iteration 185/1000 | Loss: 0.00006910
Iteration 186/1000 | Loss: 0.00006244
Iteration 187/1000 | Loss: 0.00006534
Iteration 188/1000 | Loss: 0.00086857
Iteration 189/1000 | Loss: 0.00083068
Iteration 190/1000 | Loss: 0.00038492
Iteration 191/1000 | Loss: 0.00055341
Iteration 192/1000 | Loss: 0.00026882
Iteration 193/1000 | Loss: 0.00023058
Iteration 194/1000 | Loss: 0.00036607
Iteration 195/1000 | Loss: 0.00030652
Iteration 196/1000 | Loss: 0.00018114
Iteration 197/1000 | Loss: 0.00016352
Iteration 198/1000 | Loss: 0.00006410
Iteration 199/1000 | Loss: 0.00007895
Iteration 200/1000 | Loss: 0.00021332
Iteration 201/1000 | Loss: 0.00005901
Iteration 202/1000 | Loss: 0.00004882
Iteration 203/1000 | Loss: 0.00004582
Iteration 204/1000 | Loss: 0.00004704
Iteration 205/1000 | Loss: 0.00004176
Iteration 206/1000 | Loss: 0.00016181
Iteration 207/1000 | Loss: 0.00035670
Iteration 208/1000 | Loss: 0.00011147
Iteration 209/1000 | Loss: 0.00007824
Iteration 210/1000 | Loss: 0.00036921
Iteration 211/1000 | Loss: 0.00016345
Iteration 212/1000 | Loss: 0.00017007
Iteration 213/1000 | Loss: 0.00005157
Iteration 214/1000 | Loss: 0.00004499
Iteration 215/1000 | Loss: 0.00033264
Iteration 216/1000 | Loss: 0.00008561
Iteration 217/1000 | Loss: 0.00005222
Iteration 218/1000 | Loss: 0.00004451
Iteration 219/1000 | Loss: 0.00037085
Iteration 220/1000 | Loss: 0.00013361
Iteration 221/1000 | Loss: 0.00020029
Iteration 222/1000 | Loss: 0.00034661
Iteration 223/1000 | Loss: 0.00004311
Iteration 224/1000 | Loss: 0.00003765
Iteration 225/1000 | Loss: 0.00003536
Iteration 226/1000 | Loss: 0.00003376
Iteration 227/1000 | Loss: 0.00014757
Iteration 228/1000 | Loss: 0.00018400
Iteration 229/1000 | Loss: 0.00015725
Iteration 230/1000 | Loss: 0.00003911
Iteration 231/1000 | Loss: 0.00003311
Iteration 232/1000 | Loss: 0.00011243
Iteration 233/1000 | Loss: 0.00003098
Iteration 234/1000 | Loss: 0.00003033
Iteration 235/1000 | Loss: 0.00002988
Iteration 236/1000 | Loss: 0.00018229
Iteration 237/1000 | Loss: 0.00064802
Iteration 238/1000 | Loss: 0.00076446
Iteration 239/1000 | Loss: 0.00068988
Iteration 240/1000 | Loss: 0.00020995
Iteration 241/1000 | Loss: 0.00044869
Iteration 242/1000 | Loss: 0.00015466
Iteration 243/1000 | Loss: 0.00014918
Iteration 244/1000 | Loss: 0.00021547
Iteration 245/1000 | Loss: 0.00013863
Iteration 246/1000 | Loss: 0.00010663
Iteration 247/1000 | Loss: 0.00008299
Iteration 248/1000 | Loss: 0.00003912
Iteration 249/1000 | Loss: 0.00003630
Iteration 250/1000 | Loss: 0.00031311
Iteration 251/1000 | Loss: 0.00020336
Iteration 252/1000 | Loss: 0.00015120
Iteration 253/1000 | Loss: 0.00013807
Iteration 254/1000 | Loss: 0.00014151
Iteration 255/1000 | Loss: 0.00030137
Iteration 256/1000 | Loss: 0.00023212
Iteration 257/1000 | Loss: 0.00022328
Iteration 258/1000 | Loss: 0.00008425
Iteration 259/1000 | Loss: 0.00003587
Iteration 260/1000 | Loss: 0.00004686
Iteration 261/1000 | Loss: 0.00004393
Iteration 262/1000 | Loss: 0.00004121
Iteration 263/1000 | Loss: 0.00003123
Iteration 264/1000 | Loss: 0.00026954
Iteration 265/1000 | Loss: 0.00028511
Iteration 266/1000 | Loss: 0.00003740
Iteration 267/1000 | Loss: 0.00003038
Iteration 268/1000 | Loss: 0.00002869
Iteration 269/1000 | Loss: 0.00024592
Iteration 270/1000 | Loss: 0.00011028
Iteration 271/1000 | Loss: 0.00022397
Iteration 272/1000 | Loss: 0.00010674
Iteration 273/1000 | Loss: 0.00009757
Iteration 274/1000 | Loss: 0.00002861
Iteration 275/1000 | Loss: 0.00007517
Iteration 276/1000 | Loss: 0.00054383
Iteration 277/1000 | Loss: 0.00031437
Iteration 278/1000 | Loss: 0.00006821
Iteration 279/1000 | Loss: 0.00023958
Iteration 280/1000 | Loss: 0.00139659
Iteration 281/1000 | Loss: 0.00007040
Iteration 282/1000 | Loss: 0.00011001
Iteration 283/1000 | Loss: 0.00014575
Iteration 284/1000 | Loss: 0.00004581
Iteration 285/1000 | Loss: 0.00022312
Iteration 286/1000 | Loss: 0.00009185
Iteration 287/1000 | Loss: 0.00006316
Iteration 288/1000 | Loss: 0.00017472
Iteration 289/1000 | Loss: 0.00006351
Iteration 290/1000 | Loss: 0.00012493
Iteration 291/1000 | Loss: 0.00009373
Iteration 292/1000 | Loss: 0.00017344
Iteration 293/1000 | Loss: 0.00013122
Iteration 294/1000 | Loss: 0.00025765
Iteration 295/1000 | Loss: 0.00005845
Iteration 296/1000 | Loss: 0.00006163
Iteration 297/1000 | Loss: 0.00008076
Iteration 298/1000 | Loss: 0.00017408
Iteration 299/1000 | Loss: 0.00003499
Iteration 300/1000 | Loss: 0.00008452
Iteration 301/1000 | Loss: 0.00014063
Iteration 302/1000 | Loss: 0.00023945
Iteration 303/1000 | Loss: 0.00007212
Iteration 304/1000 | Loss: 0.00013712
Iteration 305/1000 | Loss: 0.00007428
Iteration 306/1000 | Loss: 0.00011862
Iteration 307/1000 | Loss: 0.00007359
Iteration 308/1000 | Loss: 0.00032933
Iteration 309/1000 | Loss: 0.00011745
Iteration 310/1000 | Loss: 0.00004582
Iteration 311/1000 | Loss: 0.00019962
Iteration 312/1000 | Loss: 0.00033905
Iteration 313/1000 | Loss: 0.00029627
Iteration 314/1000 | Loss: 0.00005851
Iteration 315/1000 | Loss: 0.00009265
Iteration 316/1000 | Loss: 0.00005637
Iteration 317/1000 | Loss: 0.00010531
Iteration 318/1000 | Loss: 0.00015763
Iteration 319/1000 | Loss: 0.00009574
Iteration 320/1000 | Loss: 0.00002885
Iteration 321/1000 | Loss: 0.00002684
Iteration 322/1000 | Loss: 0.00007003
Iteration 323/1000 | Loss: 0.00002732
Iteration 324/1000 | Loss: 0.00002664
Iteration 325/1000 | Loss: 0.00021510
Iteration 326/1000 | Loss: 0.00052094
Iteration 327/1000 | Loss: 0.00015633
Iteration 328/1000 | Loss: 0.00029446
Iteration 329/1000 | Loss: 0.00016713
Iteration 330/1000 | Loss: 0.00021138
Iteration 331/1000 | Loss: 0.00019811
Iteration 332/1000 | Loss: 0.00018104
Iteration 333/1000 | Loss: 0.00019693
Iteration 334/1000 | Loss: 0.00017032
Iteration 335/1000 | Loss: 0.00021055
Iteration 336/1000 | Loss: 0.00003479
Iteration 337/1000 | Loss: 0.00002953
Iteration 338/1000 | Loss: 0.00002595
Iteration 339/1000 | Loss: 0.00002470
Iteration 340/1000 | Loss: 0.00008139
Iteration 341/1000 | Loss: 0.00002405
Iteration 342/1000 | Loss: 0.00022957
Iteration 343/1000 | Loss: 0.00002929
Iteration 344/1000 | Loss: 0.00002704
Iteration 345/1000 | Loss: 0.00002608
Iteration 346/1000 | Loss: 0.00002537
Iteration 347/1000 | Loss: 0.00002501
Iteration 348/1000 | Loss: 0.00002487
Iteration 349/1000 | Loss: 0.00002458
Iteration 350/1000 | Loss: 0.00022995
Iteration 351/1000 | Loss: 0.00006759
Iteration 352/1000 | Loss: 0.00021579
Iteration 353/1000 | Loss: 0.00009166
Iteration 354/1000 | Loss: 0.00014632
Iteration 355/1000 | Loss: 0.00002659
Iteration 356/1000 | Loss: 0.00002423
Iteration 357/1000 | Loss: 0.00002369
Iteration 358/1000 | Loss: 0.00002337
Iteration 359/1000 | Loss: 0.00002333
Iteration 360/1000 | Loss: 0.00002332
Iteration 361/1000 | Loss: 0.00002323
Iteration 362/1000 | Loss: 0.00002320
Iteration 363/1000 | Loss: 0.00002316
Iteration 364/1000 | Loss: 0.00002316
Iteration 365/1000 | Loss: 0.00002316
Iteration 366/1000 | Loss: 0.00002316
Iteration 367/1000 | Loss: 0.00002316
Iteration 368/1000 | Loss: 0.00002315
Iteration 369/1000 | Loss: 0.00002315
Iteration 370/1000 | Loss: 0.00002315
Iteration 371/1000 | Loss: 0.00002314
Iteration 372/1000 | Loss: 0.00002313
Iteration 373/1000 | Loss: 0.00002311
Iteration 374/1000 | Loss: 0.00002310
Iteration 375/1000 | Loss: 0.00002310
Iteration 376/1000 | Loss: 0.00002310
Iteration 377/1000 | Loss: 0.00002310
Iteration 378/1000 | Loss: 0.00002310
Iteration 379/1000 | Loss: 0.00002310
Iteration 380/1000 | Loss: 0.00002309
Iteration 381/1000 | Loss: 0.00002309
Iteration 382/1000 | Loss: 0.00002309
Iteration 383/1000 | Loss: 0.00002308
Iteration 384/1000 | Loss: 0.00002308
Iteration 385/1000 | Loss: 0.00002307
Iteration 386/1000 | Loss: 0.00002303
Iteration 387/1000 | Loss: 0.00002303
Iteration 388/1000 | Loss: 0.00002302
Iteration 389/1000 | Loss: 0.00002301
Iteration 390/1000 | Loss: 0.00002300
Iteration 391/1000 | Loss: 0.00002300
Iteration 392/1000 | Loss: 0.00002299
Iteration 393/1000 | Loss: 0.00002299
Iteration 394/1000 | Loss: 0.00002299
Iteration 395/1000 | Loss: 0.00002298
Iteration 396/1000 | Loss: 0.00002298
Iteration 397/1000 | Loss: 0.00002297
Iteration 398/1000 | Loss: 0.00002297
Iteration 399/1000 | Loss: 0.00002296
Iteration 400/1000 | Loss: 0.00002296
Iteration 401/1000 | Loss: 0.00002296
Iteration 402/1000 | Loss: 0.00002295
Iteration 403/1000 | Loss: 0.00002294
Iteration 404/1000 | Loss: 0.00002294
Iteration 405/1000 | Loss: 0.00002294
Iteration 406/1000 | Loss: 0.00002294
Iteration 407/1000 | Loss: 0.00002294
Iteration 408/1000 | Loss: 0.00002294
Iteration 409/1000 | Loss: 0.00002294
Iteration 410/1000 | Loss: 0.00002294
Iteration 411/1000 | Loss: 0.00002293
Iteration 412/1000 | Loss: 0.00002293
Iteration 413/1000 | Loss: 0.00002293
Iteration 414/1000 | Loss: 0.00002293
Iteration 415/1000 | Loss: 0.00002293
Iteration 416/1000 | Loss: 0.00002292
Iteration 417/1000 | Loss: 0.00002292
Iteration 418/1000 | Loss: 0.00002292
Iteration 419/1000 | Loss: 0.00002291
Iteration 420/1000 | Loss: 0.00002288
Iteration 421/1000 | Loss: 0.00002288
Iteration 422/1000 | Loss: 0.00002288
Iteration 423/1000 | Loss: 0.00002288
Iteration 424/1000 | Loss: 0.00002288
Iteration 425/1000 | Loss: 0.00002288
Iteration 426/1000 | Loss: 0.00002288
Iteration 427/1000 | Loss: 0.00002288
Iteration 428/1000 | Loss: 0.00002288
Iteration 429/1000 | Loss: 0.00002287
Iteration 430/1000 | Loss: 0.00002287
Iteration 431/1000 | Loss: 0.00002287
Iteration 432/1000 | Loss: 0.00002287
Iteration 433/1000 | Loss: 0.00002287
Iteration 434/1000 | Loss: 0.00002287
Iteration 435/1000 | Loss: 0.00002286
Iteration 436/1000 | Loss: 0.00002286
Iteration 437/1000 | Loss: 0.00002286
Iteration 438/1000 | Loss: 0.00002286
Iteration 439/1000 | Loss: 0.00002286
Iteration 440/1000 | Loss: 0.00002286
Iteration 441/1000 | Loss: 0.00002286
Iteration 442/1000 | Loss: 0.00002285
Iteration 443/1000 | Loss: 0.00002285
Iteration 444/1000 | Loss: 0.00002285
Iteration 445/1000 | Loss: 0.00002285
Iteration 446/1000 | Loss: 0.00002285
Iteration 447/1000 | Loss: 0.00002285
Iteration 448/1000 | Loss: 0.00002285
Iteration 449/1000 | Loss: 0.00002285
Iteration 450/1000 | Loss: 0.00002285
Iteration 451/1000 | Loss: 0.00002285
Iteration 452/1000 | Loss: 0.00002285
Iteration 453/1000 | Loss: 0.00002285
Iteration 454/1000 | Loss: 0.00002285
Iteration 455/1000 | Loss: 0.00002285
Iteration 456/1000 | Loss: 0.00002285
Iteration 457/1000 | Loss: 0.00002285
Iteration 458/1000 | Loss: 0.00002285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 458. Stopping optimization.
Last 5 losses: [2.2851014364277944e-05, 2.2851014364277944e-05, 2.2851014364277944e-05, 2.2851014364277944e-05, 2.2851014364277944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2851014364277944e-05

Optimization complete. Final v2v error: 3.611801862716675 mm

Highest mean error: 12.486997604370117 mm for frame 10

Lowest mean error: 3.366340398788452 mm for frame 143

Saving results

Total time: 607.8879725933075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801210
Iteration 2/25 | Loss: 0.00165307
Iteration 3/25 | Loss: 0.00140900
Iteration 4/25 | Loss: 0.00139762
Iteration 5/25 | Loss: 0.00139545
Iteration 6/25 | Loss: 0.00139498
Iteration 7/25 | Loss: 0.00139498
Iteration 8/25 | Loss: 0.00139498
Iteration 9/25 | Loss: 0.00139498
Iteration 10/25 | Loss: 0.00139498
Iteration 11/25 | Loss: 0.00139498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001394979190081358, 0.001394979190081358, 0.001394979190081358, 0.001394979190081358, 0.001394979190081358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001394979190081358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40142822
Iteration 2/25 | Loss: 0.00051612
Iteration 3/25 | Loss: 0.00051612
Iteration 4/25 | Loss: 0.00051612
Iteration 5/25 | Loss: 0.00051612
Iteration 6/25 | Loss: 0.00051612
Iteration 7/25 | Loss: 0.00051612
Iteration 8/25 | Loss: 0.00051612
Iteration 9/25 | Loss: 0.00051612
Iteration 10/25 | Loss: 0.00051612
Iteration 11/25 | Loss: 0.00051612
Iteration 12/25 | Loss: 0.00051612
Iteration 13/25 | Loss: 0.00051612
Iteration 14/25 | Loss: 0.00051612
Iteration 15/25 | Loss: 0.00051612
Iteration 16/25 | Loss: 0.00051612
Iteration 17/25 | Loss: 0.00051612
Iteration 18/25 | Loss: 0.00051612
Iteration 19/25 | Loss: 0.00051612
Iteration 20/25 | Loss: 0.00051612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005161220906302333, 0.0005161220906302333, 0.0005161220906302333, 0.0005161220906302333, 0.0005161220906302333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005161220906302333

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051612
Iteration 2/1000 | Loss: 0.00006415
Iteration 3/1000 | Loss: 0.00004704
Iteration 4/1000 | Loss: 0.00004447
Iteration 5/1000 | Loss: 0.00004266
Iteration 6/1000 | Loss: 0.00004157
Iteration 7/1000 | Loss: 0.00004067
Iteration 8/1000 | Loss: 0.00004015
Iteration 9/1000 | Loss: 0.00003975
Iteration 10/1000 | Loss: 0.00003956
Iteration 11/1000 | Loss: 0.00003934
Iteration 12/1000 | Loss: 0.00003920
Iteration 13/1000 | Loss: 0.00003920
Iteration 14/1000 | Loss: 0.00003906
Iteration 15/1000 | Loss: 0.00003901
Iteration 16/1000 | Loss: 0.00003899
Iteration 17/1000 | Loss: 0.00003896
Iteration 18/1000 | Loss: 0.00003896
Iteration 19/1000 | Loss: 0.00003896
Iteration 20/1000 | Loss: 0.00003895
Iteration 21/1000 | Loss: 0.00003895
Iteration 22/1000 | Loss: 0.00003894
Iteration 23/1000 | Loss: 0.00003894
Iteration 24/1000 | Loss: 0.00003892
Iteration 25/1000 | Loss: 0.00003892
Iteration 26/1000 | Loss: 0.00003891
Iteration 27/1000 | Loss: 0.00003891
Iteration 28/1000 | Loss: 0.00003891
Iteration 29/1000 | Loss: 0.00003891
Iteration 30/1000 | Loss: 0.00003891
Iteration 31/1000 | Loss: 0.00003890
Iteration 32/1000 | Loss: 0.00003890
Iteration 33/1000 | Loss: 0.00003889
Iteration 34/1000 | Loss: 0.00003889
Iteration 35/1000 | Loss: 0.00003888
Iteration 36/1000 | Loss: 0.00003888
Iteration 37/1000 | Loss: 0.00003888
Iteration 38/1000 | Loss: 0.00003887
Iteration 39/1000 | Loss: 0.00003887
Iteration 40/1000 | Loss: 0.00003886
Iteration 41/1000 | Loss: 0.00003886
Iteration 42/1000 | Loss: 0.00003886
Iteration 43/1000 | Loss: 0.00003886
Iteration 44/1000 | Loss: 0.00003886
Iteration 45/1000 | Loss: 0.00003886
Iteration 46/1000 | Loss: 0.00003886
Iteration 47/1000 | Loss: 0.00003885
Iteration 48/1000 | Loss: 0.00003885
Iteration 49/1000 | Loss: 0.00003885
Iteration 50/1000 | Loss: 0.00003885
Iteration 51/1000 | Loss: 0.00003885
Iteration 52/1000 | Loss: 0.00003884
Iteration 53/1000 | Loss: 0.00003884
Iteration 54/1000 | Loss: 0.00003884
Iteration 55/1000 | Loss: 0.00003884
Iteration 56/1000 | Loss: 0.00003884
Iteration 57/1000 | Loss: 0.00003884
Iteration 58/1000 | Loss: 0.00003884
Iteration 59/1000 | Loss: 0.00003884
Iteration 60/1000 | Loss: 0.00003884
Iteration 61/1000 | Loss: 0.00003884
Iteration 62/1000 | Loss: 0.00003884
Iteration 63/1000 | Loss: 0.00003884
Iteration 64/1000 | Loss: 0.00003883
Iteration 65/1000 | Loss: 0.00003883
Iteration 66/1000 | Loss: 0.00003883
Iteration 67/1000 | Loss: 0.00003883
Iteration 68/1000 | Loss: 0.00003883
Iteration 69/1000 | Loss: 0.00003883
Iteration 70/1000 | Loss: 0.00003883
Iteration 71/1000 | Loss: 0.00003883
Iteration 72/1000 | Loss: 0.00003883
Iteration 73/1000 | Loss: 0.00003883
Iteration 74/1000 | Loss: 0.00003883
Iteration 75/1000 | Loss: 0.00003883
Iteration 76/1000 | Loss: 0.00003883
Iteration 77/1000 | Loss: 0.00003883
Iteration 78/1000 | Loss: 0.00003883
Iteration 79/1000 | Loss: 0.00003883
Iteration 80/1000 | Loss: 0.00003883
Iteration 81/1000 | Loss: 0.00003883
Iteration 82/1000 | Loss: 0.00003883
Iteration 83/1000 | Loss: 0.00003883
Iteration 84/1000 | Loss: 0.00003883
Iteration 85/1000 | Loss: 0.00003883
Iteration 86/1000 | Loss: 0.00003883
Iteration 87/1000 | Loss: 0.00003883
Iteration 88/1000 | Loss: 0.00003883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [3.8828195101814345e-05, 3.8828195101814345e-05, 3.8828195101814345e-05, 3.8828195101814345e-05, 3.8828195101814345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8828195101814345e-05

Optimization complete. Final v2v error: 5.111633777618408 mm

Highest mean error: 5.462228775024414 mm for frame 41

Lowest mean error: 4.615837574005127 mm for frame 120

Saving results

Total time: 30.049039363861084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983571
Iteration 2/25 | Loss: 0.00253618
Iteration 3/25 | Loss: 0.00200167
Iteration 4/25 | Loss: 0.00207435
Iteration 5/25 | Loss: 0.00171085
Iteration 6/25 | Loss: 0.00151389
Iteration 7/25 | Loss: 0.00145658
Iteration 8/25 | Loss: 0.00144472
Iteration 9/25 | Loss: 0.00144389
Iteration 10/25 | Loss: 0.00143850
Iteration 11/25 | Loss: 0.00143230
Iteration 12/25 | Loss: 0.00142239
Iteration 13/25 | Loss: 0.00142219
Iteration 14/25 | Loss: 0.00142210
Iteration 15/25 | Loss: 0.00141948
Iteration 16/25 | Loss: 0.00141725
Iteration 17/25 | Loss: 0.00141801
Iteration 18/25 | Loss: 0.00141722
Iteration 19/25 | Loss: 0.00141586
Iteration 20/25 | Loss: 0.00141425
Iteration 21/25 | Loss: 0.00141823
Iteration 22/25 | Loss: 0.00141771
Iteration 23/25 | Loss: 0.00141981
Iteration 24/25 | Loss: 0.00141862
Iteration 25/25 | Loss: 0.00141720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34198809
Iteration 2/25 | Loss: 0.00173113
Iteration 3/25 | Loss: 0.00173113
Iteration 4/25 | Loss: 0.00173113
Iteration 5/25 | Loss: 0.00173113
Iteration 6/25 | Loss: 0.00173113
Iteration 7/25 | Loss: 0.00173112
Iteration 8/25 | Loss: 0.00173112
Iteration 9/25 | Loss: 0.00173112
Iteration 10/25 | Loss: 0.00173112
Iteration 11/25 | Loss: 0.00173112
Iteration 12/25 | Loss: 0.00173112
Iteration 13/25 | Loss: 0.00173112
Iteration 14/25 | Loss: 0.00173112
Iteration 15/25 | Loss: 0.00173112
Iteration 16/25 | Loss: 0.00173112
Iteration 17/25 | Loss: 0.00173112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017311236588284373, 0.0017311236588284373, 0.0017311236588284373, 0.0017311236588284373, 0.0017311236588284373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017311236588284373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173112
Iteration 2/1000 | Loss: 0.00020839
Iteration 3/1000 | Loss: 0.00016947
Iteration 4/1000 | Loss: 0.00015886
Iteration 5/1000 | Loss: 0.00052936
Iteration 6/1000 | Loss: 0.00012573
Iteration 7/1000 | Loss: 0.00009260
Iteration 8/1000 | Loss: 0.00008766
Iteration 9/1000 | Loss: 0.00008168
Iteration 10/1000 | Loss: 0.00117529
Iteration 11/1000 | Loss: 0.00014601
Iteration 12/1000 | Loss: 0.00039498
Iteration 13/1000 | Loss: 0.00024318
Iteration 14/1000 | Loss: 0.00031521
Iteration 15/1000 | Loss: 0.00007353
Iteration 16/1000 | Loss: 0.00007047
Iteration 17/1000 | Loss: 0.00037578
Iteration 18/1000 | Loss: 0.00007360
Iteration 19/1000 | Loss: 0.00006888
Iteration 20/1000 | Loss: 0.00006677
Iteration 21/1000 | Loss: 0.00006500
Iteration 22/1000 | Loss: 0.00006434
Iteration 23/1000 | Loss: 0.00006376
Iteration 24/1000 | Loss: 0.00006337
Iteration 25/1000 | Loss: 0.00006304
Iteration 26/1000 | Loss: 0.00006277
Iteration 27/1000 | Loss: 0.00006258
Iteration 28/1000 | Loss: 0.00006238
Iteration 29/1000 | Loss: 0.00006224
Iteration 30/1000 | Loss: 0.00006222
Iteration 31/1000 | Loss: 0.00006219
Iteration 32/1000 | Loss: 0.00006217
Iteration 33/1000 | Loss: 0.00006216
Iteration 34/1000 | Loss: 0.00006216
Iteration 35/1000 | Loss: 0.00006215
Iteration 36/1000 | Loss: 0.00006215
Iteration 37/1000 | Loss: 0.00006215
Iteration 38/1000 | Loss: 0.00006214
Iteration 39/1000 | Loss: 0.00006214
Iteration 40/1000 | Loss: 0.00006210
Iteration 41/1000 | Loss: 0.00006210
Iteration 42/1000 | Loss: 0.00006210
Iteration 43/1000 | Loss: 0.00006210
Iteration 44/1000 | Loss: 0.00006210
Iteration 45/1000 | Loss: 0.00006210
Iteration 46/1000 | Loss: 0.00006210
Iteration 47/1000 | Loss: 0.00006210
Iteration 48/1000 | Loss: 0.00006209
Iteration 49/1000 | Loss: 0.00006209
Iteration 50/1000 | Loss: 0.00006209
Iteration 51/1000 | Loss: 0.00006209
Iteration 52/1000 | Loss: 0.00006209
Iteration 53/1000 | Loss: 0.00006208
Iteration 54/1000 | Loss: 0.00006208
Iteration 55/1000 | Loss: 0.00006206
Iteration 56/1000 | Loss: 0.00006205
Iteration 57/1000 | Loss: 0.00006205
Iteration 58/1000 | Loss: 0.00006204
Iteration 59/1000 | Loss: 0.00006203
Iteration 60/1000 | Loss: 0.00006203
Iteration 61/1000 | Loss: 0.00006203
Iteration 62/1000 | Loss: 0.00006203
Iteration 63/1000 | Loss: 0.00006203
Iteration 64/1000 | Loss: 0.00006202
Iteration 65/1000 | Loss: 0.00006202
Iteration 66/1000 | Loss: 0.00006201
Iteration 67/1000 | Loss: 0.00006201
Iteration 68/1000 | Loss: 0.00006200
Iteration 69/1000 | Loss: 0.00006200
Iteration 70/1000 | Loss: 0.00006199
Iteration 71/1000 | Loss: 0.00006199
Iteration 72/1000 | Loss: 0.00006199
Iteration 73/1000 | Loss: 0.00006199
Iteration 74/1000 | Loss: 0.00006199
Iteration 75/1000 | Loss: 0.00006199
Iteration 76/1000 | Loss: 0.00006199
Iteration 77/1000 | Loss: 0.00006199
Iteration 78/1000 | Loss: 0.00006199
Iteration 79/1000 | Loss: 0.00006199
Iteration 80/1000 | Loss: 0.00006199
Iteration 81/1000 | Loss: 0.00006199
Iteration 82/1000 | Loss: 0.00006198
Iteration 83/1000 | Loss: 0.00006198
Iteration 84/1000 | Loss: 0.00006196
Iteration 85/1000 | Loss: 0.00006195
Iteration 86/1000 | Loss: 0.00006194
Iteration 87/1000 | Loss: 0.00006194
Iteration 88/1000 | Loss: 0.00006194
Iteration 89/1000 | Loss: 0.00006194
Iteration 90/1000 | Loss: 0.00006194
Iteration 91/1000 | Loss: 0.00006194
Iteration 92/1000 | Loss: 0.00006194
Iteration 93/1000 | Loss: 0.00006194
Iteration 94/1000 | Loss: 0.00006193
Iteration 95/1000 | Loss: 0.00006193
Iteration 96/1000 | Loss: 0.00006192
Iteration 97/1000 | Loss: 0.00006192
Iteration 98/1000 | Loss: 0.00006191
Iteration 99/1000 | Loss: 0.00006191
Iteration 100/1000 | Loss: 0.00006191
Iteration 101/1000 | Loss: 0.00006190
Iteration 102/1000 | Loss: 0.00006190
Iteration 103/1000 | Loss: 0.00006190
Iteration 104/1000 | Loss: 0.00006190
Iteration 105/1000 | Loss: 0.00006189
Iteration 106/1000 | Loss: 0.00006189
Iteration 107/1000 | Loss: 0.00006189
Iteration 108/1000 | Loss: 0.00006189
Iteration 109/1000 | Loss: 0.00006189
Iteration 110/1000 | Loss: 0.00006189
Iteration 111/1000 | Loss: 0.00006189
Iteration 112/1000 | Loss: 0.00006189
Iteration 113/1000 | Loss: 0.00006189
Iteration 114/1000 | Loss: 0.00006188
Iteration 115/1000 | Loss: 0.00006188
Iteration 116/1000 | Loss: 0.00006188
Iteration 117/1000 | Loss: 0.00006188
Iteration 118/1000 | Loss: 0.00006188
Iteration 119/1000 | Loss: 0.00006188
Iteration 120/1000 | Loss: 0.00006188
Iteration 121/1000 | Loss: 0.00006187
Iteration 122/1000 | Loss: 0.00006187
Iteration 123/1000 | Loss: 0.00006187
Iteration 124/1000 | Loss: 0.00006186
Iteration 125/1000 | Loss: 0.00006186
Iteration 126/1000 | Loss: 0.00006186
Iteration 127/1000 | Loss: 0.00006186
Iteration 128/1000 | Loss: 0.00006186
Iteration 129/1000 | Loss: 0.00006186
Iteration 130/1000 | Loss: 0.00006186
Iteration 131/1000 | Loss: 0.00006186
Iteration 132/1000 | Loss: 0.00006186
Iteration 133/1000 | Loss: 0.00006186
Iteration 134/1000 | Loss: 0.00006186
Iteration 135/1000 | Loss: 0.00006185
Iteration 136/1000 | Loss: 0.00006185
Iteration 137/1000 | Loss: 0.00006184
Iteration 138/1000 | Loss: 0.00006184
Iteration 139/1000 | Loss: 0.00006183
Iteration 140/1000 | Loss: 0.00006183
Iteration 141/1000 | Loss: 0.00006182
Iteration 142/1000 | Loss: 0.00006182
Iteration 143/1000 | Loss: 0.00006182
Iteration 144/1000 | Loss: 0.00006182
Iteration 145/1000 | Loss: 0.00006181
Iteration 146/1000 | Loss: 0.00006181
Iteration 147/1000 | Loss: 0.00006181
Iteration 148/1000 | Loss: 0.00006181
Iteration 149/1000 | Loss: 0.00006180
Iteration 150/1000 | Loss: 0.00006180
Iteration 151/1000 | Loss: 0.00006179
Iteration 152/1000 | Loss: 0.00006179
Iteration 153/1000 | Loss: 0.00006179
Iteration 154/1000 | Loss: 0.00006179
Iteration 155/1000 | Loss: 0.00006179
Iteration 156/1000 | Loss: 0.00006179
Iteration 157/1000 | Loss: 0.00006178
Iteration 158/1000 | Loss: 0.00006178
Iteration 159/1000 | Loss: 0.00006178
Iteration 160/1000 | Loss: 0.00006178
Iteration 161/1000 | Loss: 0.00006178
Iteration 162/1000 | Loss: 0.00006178
Iteration 163/1000 | Loss: 0.00006178
Iteration 164/1000 | Loss: 0.00006178
Iteration 165/1000 | Loss: 0.00006178
Iteration 166/1000 | Loss: 0.00006178
Iteration 167/1000 | Loss: 0.00006178
Iteration 168/1000 | Loss: 0.00006178
Iteration 169/1000 | Loss: 0.00006178
Iteration 170/1000 | Loss: 0.00006178
Iteration 171/1000 | Loss: 0.00006177
Iteration 172/1000 | Loss: 0.00006177
Iteration 173/1000 | Loss: 0.00006176
Iteration 174/1000 | Loss: 0.00006176
Iteration 175/1000 | Loss: 0.00006176
Iteration 176/1000 | Loss: 0.00006176
Iteration 177/1000 | Loss: 0.00006176
Iteration 178/1000 | Loss: 0.00006175
Iteration 179/1000 | Loss: 0.00006175
Iteration 180/1000 | Loss: 0.00006174
Iteration 181/1000 | Loss: 0.00006174
Iteration 182/1000 | Loss: 0.00006174
Iteration 183/1000 | Loss: 0.00006174
Iteration 184/1000 | Loss: 0.00006173
Iteration 185/1000 | Loss: 0.00006173
Iteration 186/1000 | Loss: 0.00006172
Iteration 187/1000 | Loss: 0.00006172
Iteration 188/1000 | Loss: 0.00006172
Iteration 189/1000 | Loss: 0.00006172
Iteration 190/1000 | Loss: 0.00006172
Iteration 191/1000 | Loss: 0.00006172
Iteration 192/1000 | Loss: 0.00006172
Iteration 193/1000 | Loss: 0.00006171
Iteration 194/1000 | Loss: 0.00006171
Iteration 195/1000 | Loss: 0.00006171
Iteration 196/1000 | Loss: 0.00006170
Iteration 197/1000 | Loss: 0.00006170
Iteration 198/1000 | Loss: 0.00006170
Iteration 199/1000 | Loss: 0.00006170
Iteration 200/1000 | Loss: 0.00006169
Iteration 201/1000 | Loss: 0.00006169
Iteration 202/1000 | Loss: 0.00006169
Iteration 203/1000 | Loss: 0.00006169
Iteration 204/1000 | Loss: 0.00006168
Iteration 205/1000 | Loss: 0.00006168
Iteration 206/1000 | Loss: 0.00006168
Iteration 207/1000 | Loss: 0.00006168
Iteration 208/1000 | Loss: 0.00006168
Iteration 209/1000 | Loss: 0.00006167
Iteration 210/1000 | Loss: 0.00006167
Iteration 211/1000 | Loss: 0.00006167
Iteration 212/1000 | Loss: 0.00006166
Iteration 213/1000 | Loss: 0.00006165
Iteration 214/1000 | Loss: 0.00006162
Iteration 215/1000 | Loss: 0.00006162
Iteration 216/1000 | Loss: 0.00006158
Iteration 217/1000 | Loss: 0.00014683
Iteration 218/1000 | Loss: 0.00007804
Iteration 219/1000 | Loss: 0.00006170
Iteration 220/1000 | Loss: 0.00015623
Iteration 221/1000 | Loss: 0.00006772
Iteration 222/1000 | Loss: 0.00006160
Iteration 223/1000 | Loss: 0.00015240
Iteration 224/1000 | Loss: 0.00006817
Iteration 225/1000 | Loss: 0.00012505
Iteration 226/1000 | Loss: 0.00006285
Iteration 227/1000 | Loss: 0.00006183
Iteration 228/1000 | Loss: 0.00006130
Iteration 229/1000 | Loss: 0.00006094
Iteration 230/1000 | Loss: 0.00006055
Iteration 231/1000 | Loss: 0.00006042
Iteration 232/1000 | Loss: 0.00006040
Iteration 233/1000 | Loss: 0.00006027
Iteration 234/1000 | Loss: 0.00006023
Iteration 235/1000 | Loss: 0.00006023
Iteration 236/1000 | Loss: 0.00006020
Iteration 237/1000 | Loss: 0.00006020
Iteration 238/1000 | Loss: 0.00006019
Iteration 239/1000 | Loss: 0.00006016
Iteration 240/1000 | Loss: 0.00006015
Iteration 241/1000 | Loss: 0.00006014
Iteration 242/1000 | Loss: 0.00006014
Iteration 243/1000 | Loss: 0.00006014
Iteration 244/1000 | Loss: 0.00006013
Iteration 245/1000 | Loss: 0.00006013
Iteration 246/1000 | Loss: 0.00006013
Iteration 247/1000 | Loss: 0.00006013
Iteration 248/1000 | Loss: 0.00006013
Iteration 249/1000 | Loss: 0.00006013
Iteration 250/1000 | Loss: 0.00006013
Iteration 251/1000 | Loss: 0.00006012
Iteration 252/1000 | Loss: 0.00006012
Iteration 253/1000 | Loss: 0.00006012
Iteration 254/1000 | Loss: 0.00006011
Iteration 255/1000 | Loss: 0.00006011
Iteration 256/1000 | Loss: 0.00006011
Iteration 257/1000 | Loss: 0.00006010
Iteration 258/1000 | Loss: 0.00006010
Iteration 259/1000 | Loss: 0.00006010
Iteration 260/1000 | Loss: 0.00006009
Iteration 261/1000 | Loss: 0.00006008
Iteration 262/1000 | Loss: 0.00006007
Iteration 263/1000 | Loss: 0.00006007
Iteration 264/1000 | Loss: 0.00006007
Iteration 265/1000 | Loss: 0.00006007
Iteration 266/1000 | Loss: 0.00006007
Iteration 267/1000 | Loss: 0.00006007
Iteration 268/1000 | Loss: 0.00006007
Iteration 269/1000 | Loss: 0.00006007
Iteration 270/1000 | Loss: 0.00006007
Iteration 271/1000 | Loss: 0.00006007
Iteration 272/1000 | Loss: 0.00006007
Iteration 273/1000 | Loss: 0.00006007
Iteration 274/1000 | Loss: 0.00006007
Iteration 275/1000 | Loss: 0.00006006
Iteration 276/1000 | Loss: 0.00006006
Iteration 277/1000 | Loss: 0.00006006
Iteration 278/1000 | Loss: 0.00006006
Iteration 279/1000 | Loss: 0.00006005
Iteration 280/1000 | Loss: 0.00006005
Iteration 281/1000 | Loss: 0.00006005
Iteration 282/1000 | Loss: 0.00006003
Iteration 283/1000 | Loss: 0.00006003
Iteration 284/1000 | Loss: 0.00006003
Iteration 285/1000 | Loss: 0.00006002
Iteration 286/1000 | Loss: 0.00006001
Iteration 287/1000 | Loss: 0.00005999
Iteration 288/1000 | Loss: 0.00005999
Iteration 289/1000 | Loss: 0.00005999
Iteration 290/1000 | Loss: 0.00005998
Iteration 291/1000 | Loss: 0.00005998
Iteration 292/1000 | Loss: 0.00005998
Iteration 293/1000 | Loss: 0.00005998
Iteration 294/1000 | Loss: 0.00005998
Iteration 295/1000 | Loss: 0.00005998
Iteration 296/1000 | Loss: 0.00005998
Iteration 297/1000 | Loss: 0.00005998
Iteration 298/1000 | Loss: 0.00005998
Iteration 299/1000 | Loss: 0.00005997
Iteration 300/1000 | Loss: 0.00005997
Iteration 301/1000 | Loss: 0.00005997
Iteration 302/1000 | Loss: 0.00005997
Iteration 303/1000 | Loss: 0.00005997
Iteration 304/1000 | Loss: 0.00005997
Iteration 305/1000 | Loss: 0.00005997
Iteration 306/1000 | Loss: 0.00005997
Iteration 307/1000 | Loss: 0.00005997
Iteration 308/1000 | Loss: 0.00005997
Iteration 309/1000 | Loss: 0.00005997
Iteration 310/1000 | Loss: 0.00005996
Iteration 311/1000 | Loss: 0.00005996
Iteration 312/1000 | Loss: 0.00005995
Iteration 313/1000 | Loss: 0.00005994
Iteration 314/1000 | Loss: 0.00005994
Iteration 315/1000 | Loss: 0.00005994
Iteration 316/1000 | Loss: 0.00005994
Iteration 317/1000 | Loss: 0.00005994
Iteration 318/1000 | Loss: 0.00005994
Iteration 319/1000 | Loss: 0.00005994
Iteration 320/1000 | Loss: 0.00005994
Iteration 321/1000 | Loss: 0.00005990
Iteration 322/1000 | Loss: 0.00005973
Iteration 323/1000 | Loss: 0.00005947
Iteration 324/1000 | Loss: 0.00005923
Iteration 325/1000 | Loss: 0.00005884
Iteration 326/1000 | Loss: 0.00005814
Iteration 327/1000 | Loss: 0.00005729
Iteration 328/1000 | Loss: 0.00048375
Iteration 329/1000 | Loss: 0.00097146
Iteration 330/1000 | Loss: 0.00049044
Iteration 331/1000 | Loss: 0.00006100
Iteration 332/1000 | Loss: 0.00005647
Iteration 333/1000 | Loss: 0.00005375
Iteration 334/1000 | Loss: 0.00005064
Iteration 335/1000 | Loss: 0.00004860
Iteration 336/1000 | Loss: 0.00004725
Iteration 337/1000 | Loss: 0.00004638
Iteration 338/1000 | Loss: 0.00004584
Iteration 339/1000 | Loss: 0.00004534
Iteration 340/1000 | Loss: 0.00004477
Iteration 341/1000 | Loss: 0.00004434
Iteration 342/1000 | Loss: 0.00004398
Iteration 343/1000 | Loss: 0.00004355
Iteration 344/1000 | Loss: 0.00004311
Iteration 345/1000 | Loss: 0.00004283
Iteration 346/1000 | Loss: 0.00004245
Iteration 347/1000 | Loss: 0.00004221
Iteration 348/1000 | Loss: 0.00004180
Iteration 349/1000 | Loss: 0.00004121
Iteration 350/1000 | Loss: 0.00013596
Iteration 351/1000 | Loss: 0.00007157
Iteration 352/1000 | Loss: 0.00013199
Iteration 353/1000 | Loss: 0.00079724
Iteration 354/1000 | Loss: 0.00074653
Iteration 355/1000 | Loss: 0.00068907
Iteration 356/1000 | Loss: 0.00026938
Iteration 357/1000 | Loss: 0.00024461
Iteration 358/1000 | Loss: 0.00008052
Iteration 359/1000 | Loss: 0.00047127
Iteration 360/1000 | Loss: 0.00012369
Iteration 361/1000 | Loss: 0.00007905
Iteration 362/1000 | Loss: 0.00051237
Iteration 363/1000 | Loss: 0.00007651
Iteration 364/1000 | Loss: 0.00017689
Iteration 365/1000 | Loss: 0.00008372
Iteration 366/1000 | Loss: 0.00063905
Iteration 367/1000 | Loss: 0.00025500
Iteration 368/1000 | Loss: 0.00014862
Iteration 369/1000 | Loss: 0.00010919
Iteration 370/1000 | Loss: 0.00012575
Iteration 371/1000 | Loss: 0.00063643
Iteration 372/1000 | Loss: 0.00023243
Iteration 373/1000 | Loss: 0.00009569
Iteration 374/1000 | Loss: 0.00013508
Iteration 375/1000 | Loss: 0.00006211
Iteration 376/1000 | Loss: 0.00003341
Iteration 377/1000 | Loss: 0.00011796
Iteration 378/1000 | Loss: 0.00010888
Iteration 379/1000 | Loss: 0.00005608
Iteration 380/1000 | Loss: 0.00005070
Iteration 381/1000 | Loss: 0.00009895
Iteration 382/1000 | Loss: 0.00006907
Iteration 383/1000 | Loss: 0.00005743
Iteration 384/1000 | Loss: 0.00010683
Iteration 385/1000 | Loss: 0.00006283
Iteration 386/1000 | Loss: 0.00012735
Iteration 387/1000 | Loss: 0.00002550
Iteration 388/1000 | Loss: 0.00002184
Iteration 389/1000 | Loss: 0.00001994
Iteration 390/1000 | Loss: 0.00001938
Iteration 391/1000 | Loss: 0.00001890
Iteration 392/1000 | Loss: 0.00001841
Iteration 393/1000 | Loss: 0.00001810
Iteration 394/1000 | Loss: 0.00001782
Iteration 395/1000 | Loss: 0.00001766
Iteration 396/1000 | Loss: 0.00001755
Iteration 397/1000 | Loss: 0.00001749
Iteration 398/1000 | Loss: 0.00001744
Iteration 399/1000 | Loss: 0.00001740
Iteration 400/1000 | Loss: 0.00001734
Iteration 401/1000 | Loss: 0.00001727
Iteration 402/1000 | Loss: 0.00001727
Iteration 403/1000 | Loss: 0.00001726
Iteration 404/1000 | Loss: 0.00001726
Iteration 405/1000 | Loss: 0.00001726
Iteration 406/1000 | Loss: 0.00001726
Iteration 407/1000 | Loss: 0.00001726
Iteration 408/1000 | Loss: 0.00001726
Iteration 409/1000 | Loss: 0.00001726
Iteration 410/1000 | Loss: 0.00001724
Iteration 411/1000 | Loss: 0.00001723
Iteration 412/1000 | Loss: 0.00001722
Iteration 413/1000 | Loss: 0.00001722
Iteration 414/1000 | Loss: 0.00001722
Iteration 415/1000 | Loss: 0.00001721
Iteration 416/1000 | Loss: 0.00001718
Iteration 417/1000 | Loss: 0.00001717
Iteration 418/1000 | Loss: 0.00001717
Iteration 419/1000 | Loss: 0.00001717
Iteration 420/1000 | Loss: 0.00001716
Iteration 421/1000 | Loss: 0.00001715
Iteration 422/1000 | Loss: 0.00001715
Iteration 423/1000 | Loss: 0.00001715
Iteration 424/1000 | Loss: 0.00001714
Iteration 425/1000 | Loss: 0.00001714
Iteration 426/1000 | Loss: 0.00001714
Iteration 427/1000 | Loss: 0.00001713
Iteration 428/1000 | Loss: 0.00001713
Iteration 429/1000 | Loss: 0.00001713
Iteration 430/1000 | Loss: 0.00001713
Iteration 431/1000 | Loss: 0.00001713
Iteration 432/1000 | Loss: 0.00001713
Iteration 433/1000 | Loss: 0.00001712
Iteration 434/1000 | Loss: 0.00001712
Iteration 435/1000 | Loss: 0.00001712
Iteration 436/1000 | Loss: 0.00001712
Iteration 437/1000 | Loss: 0.00001712
Iteration 438/1000 | Loss: 0.00001712
Iteration 439/1000 | Loss: 0.00001712
Iteration 440/1000 | Loss: 0.00001712
Iteration 441/1000 | Loss: 0.00001712
Iteration 442/1000 | Loss: 0.00001711
Iteration 443/1000 | Loss: 0.00001711
Iteration 444/1000 | Loss: 0.00001711
Iteration 445/1000 | Loss: 0.00001711
Iteration 446/1000 | Loss: 0.00001710
Iteration 447/1000 | Loss: 0.00001710
Iteration 448/1000 | Loss: 0.00001710
Iteration 449/1000 | Loss: 0.00001709
Iteration 450/1000 | Loss: 0.00001709
Iteration 451/1000 | Loss: 0.00001709
Iteration 452/1000 | Loss: 0.00001708
Iteration 453/1000 | Loss: 0.00001708
Iteration 454/1000 | Loss: 0.00001708
Iteration 455/1000 | Loss: 0.00001708
Iteration 456/1000 | Loss: 0.00001707
Iteration 457/1000 | Loss: 0.00001707
Iteration 458/1000 | Loss: 0.00001707
Iteration 459/1000 | Loss: 0.00001707
Iteration 460/1000 | Loss: 0.00001707
Iteration 461/1000 | Loss: 0.00001707
Iteration 462/1000 | Loss: 0.00001707
Iteration 463/1000 | Loss: 0.00001707
Iteration 464/1000 | Loss: 0.00001707
Iteration 465/1000 | Loss: 0.00001707
Iteration 466/1000 | Loss: 0.00001707
Iteration 467/1000 | Loss: 0.00001707
Iteration 468/1000 | Loss: 0.00001707
Iteration 469/1000 | Loss: 0.00001707
Iteration 470/1000 | Loss: 0.00001707
Iteration 471/1000 | Loss: 0.00001707
Iteration 472/1000 | Loss: 0.00001707
Iteration 473/1000 | Loss: 0.00001707
Iteration 474/1000 | Loss: 0.00001707
Iteration 475/1000 | Loss: 0.00001707
Iteration 476/1000 | Loss: 0.00001707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 476. Stopping optimization.
Last 5 losses: [1.707202500256244e-05, 1.707202500256244e-05, 1.707202500256244e-05, 1.707202500256244e-05, 1.707202500256244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.707202500256244e-05

Optimization complete. Final v2v error: 3.435382127761841 mm

Highest mean error: 4.346482753753662 mm for frame 217

Lowest mean error: 3.2960405349731445 mm for frame 187

Saving results

Total time: 282.66785073280334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824140
Iteration 2/25 | Loss: 0.00151449
Iteration 3/25 | Loss: 0.00137729
Iteration 4/25 | Loss: 0.00134987
Iteration 5/25 | Loss: 0.00134252
Iteration 6/25 | Loss: 0.00134153
Iteration 7/25 | Loss: 0.00134153
Iteration 8/25 | Loss: 0.00134153
Iteration 9/25 | Loss: 0.00134153
Iteration 10/25 | Loss: 0.00134153
Iteration 11/25 | Loss: 0.00134153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013415312860161066, 0.0013415312860161066, 0.0013415312860161066, 0.0013415312860161066, 0.0013415312860161066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013415312860161066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93602502
Iteration 2/25 | Loss: 0.00092419
Iteration 3/25 | Loss: 0.00092417
Iteration 4/25 | Loss: 0.00092417
Iteration 5/25 | Loss: 0.00092417
Iteration 6/25 | Loss: 0.00092417
Iteration 7/25 | Loss: 0.00092417
Iteration 8/25 | Loss: 0.00092417
Iteration 9/25 | Loss: 0.00092416
Iteration 10/25 | Loss: 0.00092416
Iteration 11/25 | Loss: 0.00092416
Iteration 12/25 | Loss: 0.00092416
Iteration 13/25 | Loss: 0.00092416
Iteration 14/25 | Loss: 0.00092416
Iteration 15/25 | Loss: 0.00092416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009241645457223058, 0.0009241645457223058, 0.0009241645457223058, 0.0009241645457223058, 0.0009241645457223058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009241645457223058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092416
Iteration 2/1000 | Loss: 0.00005129
Iteration 3/1000 | Loss: 0.00003703
Iteration 4/1000 | Loss: 0.00003159
Iteration 5/1000 | Loss: 0.00002928
Iteration 6/1000 | Loss: 0.00002827
Iteration 7/1000 | Loss: 0.00002748
Iteration 8/1000 | Loss: 0.00002685
Iteration 9/1000 | Loss: 0.00002624
Iteration 10/1000 | Loss: 0.00002572
Iteration 11/1000 | Loss: 0.00002544
Iteration 12/1000 | Loss: 0.00002516
Iteration 13/1000 | Loss: 0.00002496
Iteration 14/1000 | Loss: 0.00002474
Iteration 15/1000 | Loss: 0.00002455
Iteration 16/1000 | Loss: 0.00002439
Iteration 17/1000 | Loss: 0.00002435
Iteration 18/1000 | Loss: 0.00002435
Iteration 19/1000 | Loss: 0.00002433
Iteration 20/1000 | Loss: 0.00002431
Iteration 21/1000 | Loss: 0.00002430
Iteration 22/1000 | Loss: 0.00002429
Iteration 23/1000 | Loss: 0.00002429
Iteration 24/1000 | Loss: 0.00002428
Iteration 25/1000 | Loss: 0.00002428
Iteration 26/1000 | Loss: 0.00002427
Iteration 27/1000 | Loss: 0.00002425
Iteration 28/1000 | Loss: 0.00002425
Iteration 29/1000 | Loss: 0.00002424
Iteration 30/1000 | Loss: 0.00002423
Iteration 31/1000 | Loss: 0.00002423
Iteration 32/1000 | Loss: 0.00002422
Iteration 33/1000 | Loss: 0.00002421
Iteration 34/1000 | Loss: 0.00002420
Iteration 35/1000 | Loss: 0.00002420
Iteration 36/1000 | Loss: 0.00002420
Iteration 37/1000 | Loss: 0.00002420
Iteration 38/1000 | Loss: 0.00002420
Iteration 39/1000 | Loss: 0.00002420
Iteration 40/1000 | Loss: 0.00002419
Iteration 41/1000 | Loss: 0.00002418
Iteration 42/1000 | Loss: 0.00002418
Iteration 43/1000 | Loss: 0.00002418
Iteration 44/1000 | Loss: 0.00002417
Iteration 45/1000 | Loss: 0.00002417
Iteration 46/1000 | Loss: 0.00002417
Iteration 47/1000 | Loss: 0.00002417
Iteration 48/1000 | Loss: 0.00002416
Iteration 49/1000 | Loss: 0.00002416
Iteration 50/1000 | Loss: 0.00002414
Iteration 51/1000 | Loss: 0.00002414
Iteration 52/1000 | Loss: 0.00002414
Iteration 53/1000 | Loss: 0.00002414
Iteration 54/1000 | Loss: 0.00002414
Iteration 55/1000 | Loss: 0.00002414
Iteration 56/1000 | Loss: 0.00002414
Iteration 57/1000 | Loss: 0.00002414
Iteration 58/1000 | Loss: 0.00002414
Iteration 59/1000 | Loss: 0.00002414
Iteration 60/1000 | Loss: 0.00002413
Iteration 61/1000 | Loss: 0.00002413
Iteration 62/1000 | Loss: 0.00002413
Iteration 63/1000 | Loss: 0.00002412
Iteration 64/1000 | Loss: 0.00002412
Iteration 65/1000 | Loss: 0.00002412
Iteration 66/1000 | Loss: 0.00002411
Iteration 67/1000 | Loss: 0.00002411
Iteration 68/1000 | Loss: 0.00002411
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002410
Iteration 71/1000 | Loss: 0.00002410
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002410
Iteration 76/1000 | Loss: 0.00002410
Iteration 77/1000 | Loss: 0.00002410
Iteration 78/1000 | Loss: 0.00002410
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002409
Iteration 82/1000 | Loss: 0.00002409
Iteration 83/1000 | Loss: 0.00002408
Iteration 84/1000 | Loss: 0.00002408
Iteration 85/1000 | Loss: 0.00002408
Iteration 86/1000 | Loss: 0.00002408
Iteration 87/1000 | Loss: 0.00002408
Iteration 88/1000 | Loss: 0.00002407
Iteration 89/1000 | Loss: 0.00002407
Iteration 90/1000 | Loss: 0.00002407
Iteration 91/1000 | Loss: 0.00002407
Iteration 92/1000 | Loss: 0.00002406
Iteration 93/1000 | Loss: 0.00002406
Iteration 94/1000 | Loss: 0.00002406
Iteration 95/1000 | Loss: 0.00002405
Iteration 96/1000 | Loss: 0.00002405
Iteration 97/1000 | Loss: 0.00002405
Iteration 98/1000 | Loss: 0.00002404
Iteration 99/1000 | Loss: 0.00002404
Iteration 100/1000 | Loss: 0.00002404
Iteration 101/1000 | Loss: 0.00002404
Iteration 102/1000 | Loss: 0.00002404
Iteration 103/1000 | Loss: 0.00002404
Iteration 104/1000 | Loss: 0.00002404
Iteration 105/1000 | Loss: 0.00002404
Iteration 106/1000 | Loss: 0.00002403
Iteration 107/1000 | Loss: 0.00002403
Iteration 108/1000 | Loss: 0.00002403
Iteration 109/1000 | Loss: 0.00002403
Iteration 110/1000 | Loss: 0.00002403
Iteration 111/1000 | Loss: 0.00002403
Iteration 112/1000 | Loss: 0.00002403
Iteration 113/1000 | Loss: 0.00002403
Iteration 114/1000 | Loss: 0.00002402
Iteration 115/1000 | Loss: 0.00002402
Iteration 116/1000 | Loss: 0.00002402
Iteration 117/1000 | Loss: 0.00002402
Iteration 118/1000 | Loss: 0.00002402
Iteration 119/1000 | Loss: 0.00002402
Iteration 120/1000 | Loss: 0.00002402
Iteration 121/1000 | Loss: 0.00002402
Iteration 122/1000 | Loss: 0.00002402
Iteration 123/1000 | Loss: 0.00002401
Iteration 124/1000 | Loss: 0.00002401
Iteration 125/1000 | Loss: 0.00002401
Iteration 126/1000 | Loss: 0.00002401
Iteration 127/1000 | Loss: 0.00002401
Iteration 128/1000 | Loss: 0.00002401
Iteration 129/1000 | Loss: 0.00002401
Iteration 130/1000 | Loss: 0.00002401
Iteration 131/1000 | Loss: 0.00002401
Iteration 132/1000 | Loss: 0.00002401
Iteration 133/1000 | Loss: 0.00002401
Iteration 134/1000 | Loss: 0.00002401
Iteration 135/1000 | Loss: 0.00002401
Iteration 136/1000 | Loss: 0.00002401
Iteration 137/1000 | Loss: 0.00002401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.4013044821913354e-05, 2.4013044821913354e-05, 2.4013044821913354e-05, 2.4013044821913354e-05, 2.4013044821913354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4013044821913354e-05

Optimization complete. Final v2v error: 4.099158763885498 mm

Highest mean error: 4.533958435058594 mm for frame 51

Lowest mean error: 3.8254384994506836 mm for frame 130

Saving results

Total time: 40.812156438827515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422100
Iteration 2/25 | Loss: 0.00161535
Iteration 3/25 | Loss: 0.00135463
Iteration 4/25 | Loss: 0.00131780
Iteration 5/25 | Loss: 0.00131144
Iteration 6/25 | Loss: 0.00130994
Iteration 7/25 | Loss: 0.00130982
Iteration 8/25 | Loss: 0.00130982
Iteration 9/25 | Loss: 0.00130982
Iteration 10/25 | Loss: 0.00130982
Iteration 11/25 | Loss: 0.00130982
Iteration 12/25 | Loss: 0.00130982
Iteration 13/25 | Loss: 0.00130982
Iteration 14/25 | Loss: 0.00130982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013098226627334952, 0.0013098226627334952, 0.0013098226627334952, 0.0013098226627334952, 0.0013098226627334952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013098226627334952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36826038
Iteration 2/25 | Loss: 0.00077830
Iteration 3/25 | Loss: 0.00077829
Iteration 4/25 | Loss: 0.00077829
Iteration 5/25 | Loss: 0.00077829
Iteration 6/25 | Loss: 0.00077829
Iteration 7/25 | Loss: 0.00077829
Iteration 8/25 | Loss: 0.00077829
Iteration 9/25 | Loss: 0.00077829
Iteration 10/25 | Loss: 0.00077829
Iteration 11/25 | Loss: 0.00077829
Iteration 12/25 | Loss: 0.00077829
Iteration 13/25 | Loss: 0.00077829
Iteration 14/25 | Loss: 0.00077829
Iteration 15/25 | Loss: 0.00077829
Iteration 16/25 | Loss: 0.00077829
Iteration 17/25 | Loss: 0.00077829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007782900356687605, 0.0007782900356687605, 0.0007782900356687605, 0.0007782900356687605, 0.0007782900356687605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007782900356687605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077829
Iteration 2/1000 | Loss: 0.00004142
Iteration 3/1000 | Loss: 0.00002876
Iteration 4/1000 | Loss: 0.00002278
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001964
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001853
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001780
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001775
Iteration 15/1000 | Loss: 0.00001771
Iteration 16/1000 | Loss: 0.00001771
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001770
Iteration 19/1000 | Loss: 0.00001768
Iteration 20/1000 | Loss: 0.00001767
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001759
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001757
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001756
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001751
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00001740
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001740
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001740
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001734
Iteration 76/1000 | Loss: 0.00001734
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001731
Iteration 92/1000 | Loss: 0.00001731
Iteration 93/1000 | Loss: 0.00001731
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001729
Iteration 97/1000 | Loss: 0.00001729
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001729
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001723
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001721
Iteration 119/1000 | Loss: 0.00001721
Iteration 120/1000 | Loss: 0.00001721
Iteration 121/1000 | Loss: 0.00001720
Iteration 122/1000 | Loss: 0.00001720
Iteration 123/1000 | Loss: 0.00001720
Iteration 124/1000 | Loss: 0.00001720
Iteration 125/1000 | Loss: 0.00001720
Iteration 126/1000 | Loss: 0.00001720
Iteration 127/1000 | Loss: 0.00001719
Iteration 128/1000 | Loss: 0.00001719
Iteration 129/1000 | Loss: 0.00001719
Iteration 130/1000 | Loss: 0.00001719
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001717
Iteration 136/1000 | Loss: 0.00001717
Iteration 137/1000 | Loss: 0.00001717
Iteration 138/1000 | Loss: 0.00001717
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001716
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001715
Iteration 147/1000 | Loss: 0.00001715
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001714
Iteration 150/1000 | Loss: 0.00001714
Iteration 151/1000 | Loss: 0.00001714
Iteration 152/1000 | Loss: 0.00001714
Iteration 153/1000 | Loss: 0.00001714
Iteration 154/1000 | Loss: 0.00001713
Iteration 155/1000 | Loss: 0.00001713
Iteration 156/1000 | Loss: 0.00001713
Iteration 157/1000 | Loss: 0.00001713
Iteration 158/1000 | Loss: 0.00001712
Iteration 159/1000 | Loss: 0.00001712
Iteration 160/1000 | Loss: 0.00001712
Iteration 161/1000 | Loss: 0.00001712
Iteration 162/1000 | Loss: 0.00001711
Iteration 163/1000 | Loss: 0.00001711
Iteration 164/1000 | Loss: 0.00001711
Iteration 165/1000 | Loss: 0.00001711
Iteration 166/1000 | Loss: 0.00001711
Iteration 167/1000 | Loss: 0.00001711
Iteration 168/1000 | Loss: 0.00001711
Iteration 169/1000 | Loss: 0.00001710
Iteration 170/1000 | Loss: 0.00001710
Iteration 171/1000 | Loss: 0.00001710
Iteration 172/1000 | Loss: 0.00001710
Iteration 173/1000 | Loss: 0.00001709
Iteration 174/1000 | Loss: 0.00001709
Iteration 175/1000 | Loss: 0.00001709
Iteration 176/1000 | Loss: 0.00001709
Iteration 177/1000 | Loss: 0.00001709
Iteration 178/1000 | Loss: 0.00001708
Iteration 179/1000 | Loss: 0.00001708
Iteration 180/1000 | Loss: 0.00001708
Iteration 181/1000 | Loss: 0.00001708
Iteration 182/1000 | Loss: 0.00001708
Iteration 183/1000 | Loss: 0.00001707
Iteration 184/1000 | Loss: 0.00001707
Iteration 185/1000 | Loss: 0.00001707
Iteration 186/1000 | Loss: 0.00001707
Iteration 187/1000 | Loss: 0.00001707
Iteration 188/1000 | Loss: 0.00001707
Iteration 189/1000 | Loss: 0.00001707
Iteration 190/1000 | Loss: 0.00001707
Iteration 191/1000 | Loss: 0.00001707
Iteration 192/1000 | Loss: 0.00001707
Iteration 193/1000 | Loss: 0.00001706
Iteration 194/1000 | Loss: 0.00001706
Iteration 195/1000 | Loss: 0.00001706
Iteration 196/1000 | Loss: 0.00001706
Iteration 197/1000 | Loss: 0.00001706
Iteration 198/1000 | Loss: 0.00001706
Iteration 199/1000 | Loss: 0.00001706
Iteration 200/1000 | Loss: 0.00001706
Iteration 201/1000 | Loss: 0.00001706
Iteration 202/1000 | Loss: 0.00001706
Iteration 203/1000 | Loss: 0.00001706
Iteration 204/1000 | Loss: 0.00001705
Iteration 205/1000 | Loss: 0.00001705
Iteration 206/1000 | Loss: 0.00001705
Iteration 207/1000 | Loss: 0.00001705
Iteration 208/1000 | Loss: 0.00001705
Iteration 209/1000 | Loss: 0.00001705
Iteration 210/1000 | Loss: 0.00001705
Iteration 211/1000 | Loss: 0.00001705
Iteration 212/1000 | Loss: 0.00001705
Iteration 213/1000 | Loss: 0.00001705
Iteration 214/1000 | Loss: 0.00001705
Iteration 215/1000 | Loss: 0.00001705
Iteration 216/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.7052912880899385e-05, 1.7052912880899385e-05, 1.7052912880899385e-05, 1.7052912880899385e-05, 1.7052912880899385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7052912880899385e-05

Optimization complete. Final v2v error: 3.4974205493927 mm

Highest mean error: 4.230388641357422 mm for frame 72

Lowest mean error: 3.021697759628296 mm for frame 13

Saving results

Total time: 43.41779661178589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951255
Iteration 2/25 | Loss: 0.00369082
Iteration 3/25 | Loss: 0.00239332
Iteration 4/25 | Loss: 0.00209948
Iteration 5/25 | Loss: 0.00192701
Iteration 6/25 | Loss: 0.00184020
Iteration 7/25 | Loss: 0.00184672
Iteration 8/25 | Loss: 0.00179053
Iteration 9/25 | Loss: 0.00168074
Iteration 10/25 | Loss: 0.00160304
Iteration 11/25 | Loss: 0.00157077
Iteration 12/25 | Loss: 0.00153419
Iteration 13/25 | Loss: 0.00152027
Iteration 14/25 | Loss: 0.00150748
Iteration 15/25 | Loss: 0.00151127
Iteration 16/25 | Loss: 0.00148529
Iteration 17/25 | Loss: 0.00148742
Iteration 18/25 | Loss: 0.00148427
Iteration 19/25 | Loss: 0.00147894
Iteration 20/25 | Loss: 0.00147519
Iteration 21/25 | Loss: 0.00146850
Iteration 22/25 | Loss: 0.00146107
Iteration 23/25 | Loss: 0.00146287
Iteration 24/25 | Loss: 0.00145676
Iteration 25/25 | Loss: 0.00145845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39589000
Iteration 2/25 | Loss: 0.00249488
Iteration 3/25 | Loss: 0.00237701
Iteration 4/25 | Loss: 0.00237701
Iteration 5/25 | Loss: 0.00237700
Iteration 6/25 | Loss: 0.00237700
Iteration 7/25 | Loss: 0.00237700
Iteration 8/25 | Loss: 0.00237700
Iteration 9/25 | Loss: 0.00237700
Iteration 10/25 | Loss: 0.00237700
Iteration 11/25 | Loss: 0.00237700
Iteration 12/25 | Loss: 0.00237700
Iteration 13/25 | Loss: 0.00237700
Iteration 14/25 | Loss: 0.00237700
Iteration 15/25 | Loss: 0.00237700
Iteration 16/25 | Loss: 0.00237700
Iteration 17/25 | Loss: 0.00237700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002377002499997616, 0.002377002499997616, 0.002377002499997616, 0.002377002499997616, 0.002377002499997616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002377002499997616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237700
Iteration 2/1000 | Loss: 0.00102457
Iteration 3/1000 | Loss: 0.00074432
Iteration 4/1000 | Loss: 0.00019376
Iteration 5/1000 | Loss: 0.00031305
Iteration 6/1000 | Loss: 0.00022648
Iteration 7/1000 | Loss: 0.00040009
Iteration 8/1000 | Loss: 0.00019212
Iteration 9/1000 | Loss: 0.00013705
Iteration 10/1000 | Loss: 0.00025084
Iteration 11/1000 | Loss: 0.00015835
Iteration 12/1000 | Loss: 0.00020927
Iteration 13/1000 | Loss: 0.00012730
Iteration 14/1000 | Loss: 0.00011991
Iteration 15/1000 | Loss: 0.00025373
Iteration 16/1000 | Loss: 0.00024497
Iteration 17/1000 | Loss: 0.00025680
Iteration 18/1000 | Loss: 0.00023409
Iteration 19/1000 | Loss: 0.00010915
Iteration 20/1000 | Loss: 0.00014517
Iteration 21/1000 | Loss: 0.00007705
Iteration 22/1000 | Loss: 0.00007551
Iteration 23/1000 | Loss: 0.00019121
Iteration 24/1000 | Loss: 0.00057000
Iteration 25/1000 | Loss: 0.00040097
Iteration 26/1000 | Loss: 0.00008433
Iteration 27/1000 | Loss: 0.00018292
Iteration 28/1000 | Loss: 0.00025815
Iteration 29/1000 | Loss: 0.00023987
Iteration 30/1000 | Loss: 0.00016168
Iteration 31/1000 | Loss: 0.00011828
Iteration 32/1000 | Loss: 0.00014679
Iteration 33/1000 | Loss: 0.00018446
Iteration 34/1000 | Loss: 0.00008490
Iteration 35/1000 | Loss: 0.00053791
Iteration 36/1000 | Loss: 0.00044905
Iteration 37/1000 | Loss: 0.00013997
Iteration 38/1000 | Loss: 0.00007927
Iteration 39/1000 | Loss: 0.00009406
Iteration 40/1000 | Loss: 0.00020692
Iteration 41/1000 | Loss: 0.00006292
Iteration 42/1000 | Loss: 0.00029049
Iteration 43/1000 | Loss: 0.00035958
Iteration 44/1000 | Loss: 0.00015048
Iteration 45/1000 | Loss: 0.00016278
Iteration 46/1000 | Loss: 0.00007857
Iteration 47/1000 | Loss: 0.00013302
Iteration 48/1000 | Loss: 0.00052503
Iteration 49/1000 | Loss: 0.00154176
Iteration 50/1000 | Loss: 0.00071613
Iteration 51/1000 | Loss: 0.00114217
Iteration 52/1000 | Loss: 0.00016470
Iteration 53/1000 | Loss: 0.00035429
Iteration 54/1000 | Loss: 0.00022160
Iteration 55/1000 | Loss: 0.00036740
Iteration 56/1000 | Loss: 0.00014880
Iteration 57/1000 | Loss: 0.00015264
Iteration 58/1000 | Loss: 0.00028937
Iteration 59/1000 | Loss: 0.00042608
Iteration 60/1000 | Loss: 0.00015358
Iteration 61/1000 | Loss: 0.00014479
Iteration 62/1000 | Loss: 0.00061431
Iteration 63/1000 | Loss: 0.00030572
Iteration 64/1000 | Loss: 0.00020539
Iteration 65/1000 | Loss: 0.00037370
Iteration 66/1000 | Loss: 0.00005343
Iteration 67/1000 | Loss: 0.00005084
Iteration 68/1000 | Loss: 0.00008024
Iteration 69/1000 | Loss: 0.00005132
Iteration 70/1000 | Loss: 0.00009863
Iteration 71/1000 | Loss: 0.00012272
Iteration 72/1000 | Loss: 0.00003247
Iteration 73/1000 | Loss: 0.00005737
Iteration 74/1000 | Loss: 0.00003123
Iteration 75/1000 | Loss: 0.00004066
Iteration 76/1000 | Loss: 0.00015964
Iteration 77/1000 | Loss: 0.00039905
Iteration 78/1000 | Loss: 0.00064048
Iteration 79/1000 | Loss: 0.00009129
Iteration 80/1000 | Loss: 0.00024894
Iteration 81/1000 | Loss: 0.00003540
Iteration 82/1000 | Loss: 0.00007477
Iteration 83/1000 | Loss: 0.00004644
Iteration 84/1000 | Loss: 0.00002657
Iteration 85/1000 | Loss: 0.00003201
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00003337
Iteration 88/1000 | Loss: 0.00001976
Iteration 89/1000 | Loss: 0.00007157
Iteration 90/1000 | Loss: 0.00040097
Iteration 91/1000 | Loss: 0.00011302
Iteration 92/1000 | Loss: 0.00002360
Iteration 93/1000 | Loss: 0.00002563
Iteration 94/1000 | Loss: 0.00002667
Iteration 95/1000 | Loss: 0.00002066
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00003918
Iteration 98/1000 | Loss: 0.00001859
Iteration 99/1000 | Loss: 0.00002201
Iteration 100/1000 | Loss: 0.00002939
Iteration 101/1000 | Loss: 0.00004139
Iteration 102/1000 | Loss: 0.00014943
Iteration 103/1000 | Loss: 0.00017517
Iteration 104/1000 | Loss: 0.00002915
Iteration 105/1000 | Loss: 0.00003480
Iteration 106/1000 | Loss: 0.00033504
Iteration 107/1000 | Loss: 0.00001746
Iteration 108/1000 | Loss: 0.00001776
Iteration 109/1000 | Loss: 0.00002101
Iteration 110/1000 | Loss: 0.00002224
Iteration 111/1000 | Loss: 0.00002716
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001865
Iteration 114/1000 | Loss: 0.00002080
Iteration 115/1000 | Loss: 0.00001896
Iteration 116/1000 | Loss: 0.00002937
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00002289
Iteration 120/1000 | Loss: 0.00002422
Iteration 121/1000 | Loss: 0.00003927
Iteration 122/1000 | Loss: 0.00001891
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001632
Iteration 127/1000 | Loss: 0.00001632
Iteration 128/1000 | Loss: 0.00001774
Iteration 129/1000 | Loss: 0.00003023
Iteration 130/1000 | Loss: 0.00003540
Iteration 131/1000 | Loss: 0.00019514
Iteration 132/1000 | Loss: 0.00019985
Iteration 133/1000 | Loss: 0.00017906
Iteration 134/1000 | Loss: 0.00029847
Iteration 135/1000 | Loss: 0.00008872
Iteration 136/1000 | Loss: 0.00005703
Iteration 137/1000 | Loss: 0.00004017
Iteration 138/1000 | Loss: 0.00004454
Iteration 139/1000 | Loss: 0.00003496
Iteration 140/1000 | Loss: 0.00003506
Iteration 141/1000 | Loss: 0.00001530
Iteration 142/1000 | Loss: 0.00002503
Iteration 143/1000 | Loss: 0.00002737
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001465
Iteration 146/1000 | Loss: 0.00001460
Iteration 147/1000 | Loss: 0.00001460
Iteration 148/1000 | Loss: 0.00002547
Iteration 149/1000 | Loss: 0.00001698
Iteration 150/1000 | Loss: 0.00001606
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00008136
Iteration 153/1000 | Loss: 0.00019338
Iteration 154/1000 | Loss: 0.00002231
Iteration 155/1000 | Loss: 0.00017058
Iteration 156/1000 | Loss: 0.00005003
Iteration 157/1000 | Loss: 0.00002221
Iteration 158/1000 | Loss: 0.00001441
Iteration 159/1000 | Loss: 0.00001441
Iteration 160/1000 | Loss: 0.00001884
Iteration 161/1000 | Loss: 0.00001576
Iteration 162/1000 | Loss: 0.00001437
Iteration 163/1000 | Loss: 0.00001437
Iteration 164/1000 | Loss: 0.00001437
Iteration 165/1000 | Loss: 0.00001437
Iteration 166/1000 | Loss: 0.00001437
Iteration 167/1000 | Loss: 0.00001437
Iteration 168/1000 | Loss: 0.00001437
Iteration 169/1000 | Loss: 0.00001437
Iteration 170/1000 | Loss: 0.00001437
Iteration 171/1000 | Loss: 0.00001437
Iteration 172/1000 | Loss: 0.00001436
Iteration 173/1000 | Loss: 0.00001436
Iteration 174/1000 | Loss: 0.00001455
Iteration 175/1000 | Loss: 0.00001472
Iteration 176/1000 | Loss: 0.00003013
Iteration 177/1000 | Loss: 0.00001465
Iteration 178/1000 | Loss: 0.00001435
Iteration 179/1000 | Loss: 0.00001434
Iteration 180/1000 | Loss: 0.00001434
Iteration 181/1000 | Loss: 0.00001434
Iteration 182/1000 | Loss: 0.00001434
Iteration 183/1000 | Loss: 0.00001434
Iteration 184/1000 | Loss: 0.00001434
Iteration 185/1000 | Loss: 0.00001434
Iteration 186/1000 | Loss: 0.00001434
Iteration 187/1000 | Loss: 0.00001434
Iteration 188/1000 | Loss: 0.00001434
Iteration 189/1000 | Loss: 0.00001434
Iteration 190/1000 | Loss: 0.00001434
Iteration 191/1000 | Loss: 0.00001434
Iteration 192/1000 | Loss: 0.00001433
Iteration 193/1000 | Loss: 0.00001433
Iteration 194/1000 | Loss: 0.00001433
Iteration 195/1000 | Loss: 0.00001433
Iteration 196/1000 | Loss: 0.00001433
Iteration 197/1000 | Loss: 0.00001433
Iteration 198/1000 | Loss: 0.00001433
Iteration 199/1000 | Loss: 0.00001531
Iteration 200/1000 | Loss: 0.00001608
Iteration 201/1000 | Loss: 0.00001434
Iteration 202/1000 | Loss: 0.00001434
Iteration 203/1000 | Loss: 0.00001434
Iteration 204/1000 | Loss: 0.00001434
Iteration 205/1000 | Loss: 0.00001434
Iteration 206/1000 | Loss: 0.00001433
Iteration 207/1000 | Loss: 0.00001433
Iteration 208/1000 | Loss: 0.00001433
Iteration 209/1000 | Loss: 0.00001433
Iteration 210/1000 | Loss: 0.00001433
Iteration 211/1000 | Loss: 0.00001433
Iteration 212/1000 | Loss: 0.00001433
Iteration 213/1000 | Loss: 0.00001433
Iteration 214/1000 | Loss: 0.00001433
Iteration 215/1000 | Loss: 0.00001433
Iteration 216/1000 | Loss: 0.00001433
Iteration 217/1000 | Loss: 0.00001433
Iteration 218/1000 | Loss: 0.00001432
Iteration 219/1000 | Loss: 0.00001432
Iteration 220/1000 | Loss: 0.00001432
Iteration 221/1000 | Loss: 0.00001432
Iteration 222/1000 | Loss: 0.00001432
Iteration 223/1000 | Loss: 0.00001432
Iteration 224/1000 | Loss: 0.00001432
Iteration 225/1000 | Loss: 0.00001432
Iteration 226/1000 | Loss: 0.00001432
Iteration 227/1000 | Loss: 0.00001432
Iteration 228/1000 | Loss: 0.00001432
Iteration 229/1000 | Loss: 0.00001432
Iteration 230/1000 | Loss: 0.00001432
Iteration 231/1000 | Loss: 0.00001432
Iteration 232/1000 | Loss: 0.00001432
Iteration 233/1000 | Loss: 0.00001432
Iteration 234/1000 | Loss: 0.00001432
Iteration 235/1000 | Loss: 0.00001432
Iteration 236/1000 | Loss: 0.00001432
Iteration 237/1000 | Loss: 0.00001432
Iteration 238/1000 | Loss: 0.00001432
Iteration 239/1000 | Loss: 0.00001432
Iteration 240/1000 | Loss: 0.00001432
Iteration 241/1000 | Loss: 0.00001432
Iteration 242/1000 | Loss: 0.00001432
Iteration 243/1000 | Loss: 0.00001432
Iteration 244/1000 | Loss: 0.00001432
Iteration 245/1000 | Loss: 0.00001432
Iteration 246/1000 | Loss: 0.00001432
Iteration 247/1000 | Loss: 0.00001432
Iteration 248/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.4317647583084181e-05, 1.4317647583084181e-05, 1.4317647583084181e-05, 1.4317647583084181e-05, 1.4317647583084181e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4317647583084181e-05

Optimization complete. Final v2v error: 3.1712839603424072 mm

Highest mean error: 5.860395908355713 mm for frame 11

Lowest mean error: 2.904703378677368 mm for frame 221

Saving results

Total time: 300.60053849220276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824379
Iteration 2/25 | Loss: 0.00141066
Iteration 3/25 | Loss: 0.00130195
Iteration 4/25 | Loss: 0.00127600
Iteration 5/25 | Loss: 0.00127491
Iteration 6/25 | Loss: 0.00127006
Iteration 7/25 | Loss: 0.00126893
Iteration 8/25 | Loss: 0.00126906
Iteration 9/25 | Loss: 0.00126866
Iteration 10/25 | Loss: 0.00126852
Iteration 11/25 | Loss: 0.00126851
Iteration 12/25 | Loss: 0.00126851
Iteration 13/25 | Loss: 0.00126851
Iteration 14/25 | Loss: 0.00126851
Iteration 15/25 | Loss: 0.00126851
Iteration 16/25 | Loss: 0.00126851
Iteration 17/25 | Loss: 0.00126851
Iteration 18/25 | Loss: 0.00126851
Iteration 19/25 | Loss: 0.00126851
Iteration 20/25 | Loss: 0.00126851
Iteration 21/25 | Loss: 0.00126851
Iteration 22/25 | Loss: 0.00126851
Iteration 23/25 | Loss: 0.00126851
Iteration 24/25 | Loss: 0.00126851
Iteration 25/25 | Loss: 0.00126851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76205897
Iteration 2/25 | Loss: 0.00091436
Iteration 3/25 | Loss: 0.00091214
Iteration 4/25 | Loss: 0.00091214
Iteration 5/25 | Loss: 0.00091214
Iteration 6/25 | Loss: 0.00091214
Iteration 7/25 | Loss: 0.00091214
Iteration 8/25 | Loss: 0.00091214
Iteration 9/25 | Loss: 0.00091214
Iteration 10/25 | Loss: 0.00091214
Iteration 11/25 | Loss: 0.00091214
Iteration 12/25 | Loss: 0.00091214
Iteration 13/25 | Loss: 0.00091214
Iteration 14/25 | Loss: 0.00091214
Iteration 15/25 | Loss: 0.00091214
Iteration 16/25 | Loss: 0.00091214
Iteration 17/25 | Loss: 0.00091214
Iteration 18/25 | Loss: 0.00091214
Iteration 19/25 | Loss: 0.00091214
Iteration 20/25 | Loss: 0.00091214
Iteration 21/25 | Loss: 0.00091213
Iteration 22/25 | Loss: 0.00091213
Iteration 23/25 | Loss: 0.00091213
Iteration 24/25 | Loss: 0.00091213
Iteration 25/25 | Loss: 0.00091213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091213
Iteration 2/1000 | Loss: 0.00003767
Iteration 3/1000 | Loss: 0.00001727
Iteration 4/1000 | Loss: 0.00002237
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001460
Iteration 7/1000 | Loss: 0.00001696
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001813
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00002253
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00001985
Iteration 17/1000 | Loss: 0.00001536
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001291
Iteration 21/1000 | Loss: 0.00001291
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00002050
Iteration 25/1000 | Loss: 0.00001283
Iteration 26/1000 | Loss: 0.00001845
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001268
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001268
Iteration 31/1000 | Loss: 0.00001268
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001268
Iteration 35/1000 | Loss: 0.00001268
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001264
Iteration 44/1000 | Loss: 0.00001263
Iteration 45/1000 | Loss: 0.00001263
Iteration 46/1000 | Loss: 0.00001263
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001263
Iteration 49/1000 | Loss: 0.00001262
Iteration 50/1000 | Loss: 0.00001261
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001254
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.2543750926852226e-05, 1.2543750926852226e-05, 1.2543750926852226e-05, 1.2543750926852226e-05, 1.2543750926852226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2543750926852226e-05

Optimization complete. Final v2v error: 3.0549354553222656 mm

Highest mean error: 3.3426480293273926 mm for frame 40

Lowest mean error: 2.7904443740844727 mm for frame 101

Saving results

Total time: 57.39646339416504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842607
Iteration 2/25 | Loss: 0.00138732
Iteration 3/25 | Loss: 0.00131072
Iteration 4/25 | Loss: 0.00129830
Iteration 5/25 | Loss: 0.00129510
Iteration 6/25 | Loss: 0.00129503
Iteration 7/25 | Loss: 0.00129503
Iteration 8/25 | Loss: 0.00129503
Iteration 9/25 | Loss: 0.00129503
Iteration 10/25 | Loss: 0.00129503
Iteration 11/25 | Loss: 0.00129503
Iteration 12/25 | Loss: 0.00129503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012950277887284756, 0.0012950277887284756, 0.0012950277887284756, 0.0012950277887284756, 0.0012950277887284756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012950277887284756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38164306
Iteration 2/25 | Loss: 0.00097224
Iteration 3/25 | Loss: 0.00097223
Iteration 4/25 | Loss: 0.00097223
Iteration 5/25 | Loss: 0.00097223
Iteration 6/25 | Loss: 0.00097223
Iteration 7/25 | Loss: 0.00097223
Iteration 8/25 | Loss: 0.00097223
Iteration 9/25 | Loss: 0.00097223
Iteration 10/25 | Loss: 0.00097223
Iteration 11/25 | Loss: 0.00097223
Iteration 12/25 | Loss: 0.00097223
Iteration 13/25 | Loss: 0.00097223
Iteration 14/25 | Loss: 0.00097223
Iteration 15/25 | Loss: 0.00097223
Iteration 16/25 | Loss: 0.00097223
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009722263203002512, 0.0009722263203002512, 0.0009722263203002512, 0.0009722263203002512, 0.0009722263203002512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009722263203002512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097223
Iteration 2/1000 | Loss: 0.00002835
Iteration 3/1000 | Loss: 0.00001803
Iteration 4/1000 | Loss: 0.00001616
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001482
Iteration 7/1000 | Loss: 0.00001443
Iteration 8/1000 | Loss: 0.00001421
Iteration 9/1000 | Loss: 0.00001406
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001366
Iteration 14/1000 | Loss: 0.00001365
Iteration 15/1000 | Loss: 0.00001362
Iteration 16/1000 | Loss: 0.00001361
Iteration 17/1000 | Loss: 0.00001360
Iteration 18/1000 | Loss: 0.00001357
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001351
Iteration 21/1000 | Loss: 0.00001344
Iteration 22/1000 | Loss: 0.00001344
Iteration 23/1000 | Loss: 0.00001344
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001343
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001341
Iteration 28/1000 | Loss: 0.00001340
Iteration 29/1000 | Loss: 0.00001340
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001338
Iteration 33/1000 | Loss: 0.00001337
Iteration 34/1000 | Loss: 0.00001337
Iteration 35/1000 | Loss: 0.00001336
Iteration 36/1000 | Loss: 0.00001336
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001332
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001327
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001326
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001323
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001320
Iteration 61/1000 | Loss: 0.00001319
Iteration 62/1000 | Loss: 0.00001319
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001318
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001317
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001316
Iteration 80/1000 | Loss: 0.00001316
Iteration 81/1000 | Loss: 0.00001316
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001316
Iteration 101/1000 | Loss: 0.00001316
Iteration 102/1000 | Loss: 0.00001316
Iteration 103/1000 | Loss: 0.00001316
Iteration 104/1000 | Loss: 0.00001316
Iteration 105/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.3155633496353403e-05, 1.3155633496353403e-05, 1.3155633496353403e-05, 1.3155633496353403e-05, 1.3155633496353403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3155633496353403e-05

Optimization complete. Final v2v error: 3.09913969039917 mm

Highest mean error: 3.4831910133361816 mm for frame 192

Lowest mean error: 2.7681479454040527 mm for frame 107

Saving results

Total time: 33.440690755844116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102892
Iteration 2/25 | Loss: 0.01102892
Iteration 3/25 | Loss: 0.00351191
Iteration 4/25 | Loss: 0.00225777
Iteration 5/25 | Loss: 0.00240042
Iteration 6/25 | Loss: 0.00168271
Iteration 7/25 | Loss: 0.00178332
Iteration 8/25 | Loss: 0.00165794
Iteration 9/25 | Loss: 0.00158001
Iteration 10/25 | Loss: 0.00152174
Iteration 11/25 | Loss: 0.00152056
Iteration 12/25 | Loss: 0.00150717
Iteration 13/25 | Loss: 0.00149640
Iteration 14/25 | Loss: 0.00150075
Iteration 15/25 | Loss: 0.00149500
Iteration 16/25 | Loss: 0.00149385
Iteration 17/25 | Loss: 0.00149440
Iteration 18/25 | Loss: 0.00148958
Iteration 19/25 | Loss: 0.00148931
Iteration 20/25 | Loss: 0.00148922
Iteration 21/25 | Loss: 0.00148922
Iteration 22/25 | Loss: 0.00148922
Iteration 23/25 | Loss: 0.00148922
Iteration 24/25 | Loss: 0.00148922
Iteration 25/25 | Loss: 0.00148922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66113603
Iteration 2/25 | Loss: 0.00405839
Iteration 3/25 | Loss: 0.00107971
Iteration 4/25 | Loss: 0.00107970
Iteration 5/25 | Loss: 0.00107970
Iteration 6/25 | Loss: 0.00107970
Iteration 7/25 | Loss: 0.00107970
Iteration 8/25 | Loss: 0.00107970
Iteration 9/25 | Loss: 0.00107969
Iteration 10/25 | Loss: 0.00107969
Iteration 11/25 | Loss: 0.00107969
Iteration 12/25 | Loss: 0.00107969
Iteration 13/25 | Loss: 0.00107969
Iteration 14/25 | Loss: 0.00107969
Iteration 15/25 | Loss: 0.00107969
Iteration 16/25 | Loss: 0.00107969
Iteration 17/25 | Loss: 0.00107969
Iteration 18/25 | Loss: 0.00107969
Iteration 19/25 | Loss: 0.00107969
Iteration 20/25 | Loss: 0.00107969
Iteration 21/25 | Loss: 0.00107969
Iteration 22/25 | Loss: 0.00107969
Iteration 23/25 | Loss: 0.00107969
Iteration 24/25 | Loss: 0.00107969
Iteration 25/25 | Loss: 0.00107969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107969
Iteration 2/1000 | Loss: 0.00005105
Iteration 3/1000 | Loss: 0.00003534
Iteration 4/1000 | Loss: 0.00003096
Iteration 5/1000 | Loss: 0.00002952
Iteration 6/1000 | Loss: 0.00002885
Iteration 7/1000 | Loss: 0.00002816
Iteration 8/1000 | Loss: 0.00002766
Iteration 9/1000 | Loss: 0.00002729
Iteration 10/1000 | Loss: 0.00002688
Iteration 11/1000 | Loss: 0.00002659
Iteration 12/1000 | Loss: 0.00002629
Iteration 13/1000 | Loss: 0.00011114
Iteration 14/1000 | Loss: 0.00007497
Iteration 15/1000 | Loss: 0.00010415
Iteration 16/1000 | Loss: 0.00008186
Iteration 17/1000 | Loss: 0.00002914
Iteration 18/1000 | Loss: 0.00015502
Iteration 19/1000 | Loss: 0.00003073
Iteration 20/1000 | Loss: 0.00002761
Iteration 21/1000 | Loss: 0.00002650
Iteration 22/1000 | Loss: 0.00002602
Iteration 23/1000 | Loss: 0.00002579
Iteration 24/1000 | Loss: 0.00002564
Iteration 25/1000 | Loss: 0.00002547
Iteration 26/1000 | Loss: 0.00002539
Iteration 27/1000 | Loss: 0.00002529
Iteration 28/1000 | Loss: 0.00002526
Iteration 29/1000 | Loss: 0.00002525
Iteration 30/1000 | Loss: 0.00002525
Iteration 31/1000 | Loss: 0.00002521
Iteration 32/1000 | Loss: 0.00002520
Iteration 33/1000 | Loss: 0.00002519
Iteration 34/1000 | Loss: 0.00002519
Iteration 35/1000 | Loss: 0.00002518
Iteration 36/1000 | Loss: 0.00002517
Iteration 37/1000 | Loss: 0.00002516
Iteration 38/1000 | Loss: 0.00002516
Iteration 39/1000 | Loss: 0.00002516
Iteration 40/1000 | Loss: 0.00002516
Iteration 41/1000 | Loss: 0.00002515
Iteration 42/1000 | Loss: 0.00002515
Iteration 43/1000 | Loss: 0.00002515
Iteration 44/1000 | Loss: 0.00002515
Iteration 45/1000 | Loss: 0.00002515
Iteration 46/1000 | Loss: 0.00002515
Iteration 47/1000 | Loss: 0.00002515
Iteration 48/1000 | Loss: 0.00002515
Iteration 49/1000 | Loss: 0.00002514
Iteration 50/1000 | Loss: 0.00002514
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002514
Iteration 54/1000 | Loss: 0.00002514
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002514
Iteration 57/1000 | Loss: 0.00002514
Iteration 58/1000 | Loss: 0.00002514
Iteration 59/1000 | Loss: 0.00002513
Iteration 60/1000 | Loss: 0.00002513
Iteration 61/1000 | Loss: 0.00002513
Iteration 62/1000 | Loss: 0.00002513
Iteration 63/1000 | Loss: 0.00002512
Iteration 64/1000 | Loss: 0.00002512
Iteration 65/1000 | Loss: 0.00002512
Iteration 66/1000 | Loss: 0.00002512
Iteration 67/1000 | Loss: 0.00002512
Iteration 68/1000 | Loss: 0.00002512
Iteration 69/1000 | Loss: 0.00002512
Iteration 70/1000 | Loss: 0.00002512
Iteration 71/1000 | Loss: 0.00002512
Iteration 72/1000 | Loss: 0.00002512
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002511
Iteration 76/1000 | Loss: 0.00002511
Iteration 77/1000 | Loss: 0.00002511
Iteration 78/1000 | Loss: 0.00002510
Iteration 79/1000 | Loss: 0.00002510
Iteration 80/1000 | Loss: 0.00002510
Iteration 81/1000 | Loss: 0.00002510
Iteration 82/1000 | Loss: 0.00002510
Iteration 83/1000 | Loss: 0.00002510
Iteration 84/1000 | Loss: 0.00002510
Iteration 85/1000 | Loss: 0.00002510
Iteration 86/1000 | Loss: 0.00002510
Iteration 87/1000 | Loss: 0.00002510
Iteration 88/1000 | Loss: 0.00002509
Iteration 89/1000 | Loss: 0.00002509
Iteration 90/1000 | Loss: 0.00002509
Iteration 91/1000 | Loss: 0.00002509
Iteration 92/1000 | Loss: 0.00002509
Iteration 93/1000 | Loss: 0.00002508
Iteration 94/1000 | Loss: 0.00002508
Iteration 95/1000 | Loss: 0.00002508
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002508
Iteration 98/1000 | Loss: 0.00002508
Iteration 99/1000 | Loss: 0.00002508
Iteration 100/1000 | Loss: 0.00002508
Iteration 101/1000 | Loss: 0.00002508
Iteration 102/1000 | Loss: 0.00002508
Iteration 103/1000 | Loss: 0.00002508
Iteration 104/1000 | Loss: 0.00002508
Iteration 105/1000 | Loss: 0.00002508
Iteration 106/1000 | Loss: 0.00002508
Iteration 107/1000 | Loss: 0.00002508
Iteration 108/1000 | Loss: 0.00002507
Iteration 109/1000 | Loss: 0.00002507
Iteration 110/1000 | Loss: 0.00002507
Iteration 111/1000 | Loss: 0.00002507
Iteration 112/1000 | Loss: 0.00002507
Iteration 113/1000 | Loss: 0.00002507
Iteration 114/1000 | Loss: 0.00002506
Iteration 115/1000 | Loss: 0.00002506
Iteration 116/1000 | Loss: 0.00002506
Iteration 117/1000 | Loss: 0.00002506
Iteration 118/1000 | Loss: 0.00002505
Iteration 119/1000 | Loss: 0.00002505
Iteration 120/1000 | Loss: 0.00002505
Iteration 121/1000 | Loss: 0.00002505
Iteration 122/1000 | Loss: 0.00002505
Iteration 123/1000 | Loss: 0.00002505
Iteration 124/1000 | Loss: 0.00002504
Iteration 125/1000 | Loss: 0.00002504
Iteration 126/1000 | Loss: 0.00002504
Iteration 127/1000 | Loss: 0.00002504
Iteration 128/1000 | Loss: 0.00002504
Iteration 129/1000 | Loss: 0.00002504
Iteration 130/1000 | Loss: 0.00002504
Iteration 131/1000 | Loss: 0.00002504
Iteration 132/1000 | Loss: 0.00002504
Iteration 133/1000 | Loss: 0.00002503
Iteration 134/1000 | Loss: 0.00002503
Iteration 135/1000 | Loss: 0.00002503
Iteration 136/1000 | Loss: 0.00002503
Iteration 137/1000 | Loss: 0.00002503
Iteration 138/1000 | Loss: 0.00002503
Iteration 139/1000 | Loss: 0.00002503
Iteration 140/1000 | Loss: 0.00002503
Iteration 141/1000 | Loss: 0.00002503
Iteration 142/1000 | Loss: 0.00002503
Iteration 143/1000 | Loss: 0.00002503
Iteration 144/1000 | Loss: 0.00002503
Iteration 145/1000 | Loss: 0.00002502
Iteration 146/1000 | Loss: 0.00002502
Iteration 147/1000 | Loss: 0.00002502
Iteration 148/1000 | Loss: 0.00002502
Iteration 149/1000 | Loss: 0.00002502
Iteration 150/1000 | Loss: 0.00002502
Iteration 151/1000 | Loss: 0.00002502
Iteration 152/1000 | Loss: 0.00002502
Iteration 153/1000 | Loss: 0.00002502
Iteration 154/1000 | Loss: 0.00002502
Iteration 155/1000 | Loss: 0.00002501
Iteration 156/1000 | Loss: 0.00002501
Iteration 157/1000 | Loss: 0.00002501
Iteration 158/1000 | Loss: 0.00002501
Iteration 159/1000 | Loss: 0.00002501
Iteration 160/1000 | Loss: 0.00002501
Iteration 161/1000 | Loss: 0.00002501
Iteration 162/1000 | Loss: 0.00002501
Iteration 163/1000 | Loss: 0.00002501
Iteration 164/1000 | Loss: 0.00002501
Iteration 165/1000 | Loss: 0.00002501
Iteration 166/1000 | Loss: 0.00002501
Iteration 167/1000 | Loss: 0.00002501
Iteration 168/1000 | Loss: 0.00002501
Iteration 169/1000 | Loss: 0.00002501
Iteration 170/1000 | Loss: 0.00002501
Iteration 171/1000 | Loss: 0.00002501
Iteration 172/1000 | Loss: 0.00002500
Iteration 173/1000 | Loss: 0.00002500
Iteration 174/1000 | Loss: 0.00002500
Iteration 175/1000 | Loss: 0.00002500
Iteration 176/1000 | Loss: 0.00002500
Iteration 177/1000 | Loss: 0.00002500
Iteration 178/1000 | Loss: 0.00002500
Iteration 179/1000 | Loss: 0.00002500
Iteration 180/1000 | Loss: 0.00002500
Iteration 181/1000 | Loss: 0.00002500
Iteration 182/1000 | Loss: 0.00002500
Iteration 183/1000 | Loss: 0.00002500
Iteration 184/1000 | Loss: 0.00002500
Iteration 185/1000 | Loss: 0.00002500
Iteration 186/1000 | Loss: 0.00002500
Iteration 187/1000 | Loss: 0.00002500
Iteration 188/1000 | Loss: 0.00002500
Iteration 189/1000 | Loss: 0.00002500
Iteration 190/1000 | Loss: 0.00002500
Iteration 191/1000 | Loss: 0.00002500
Iteration 192/1000 | Loss: 0.00002500
Iteration 193/1000 | Loss: 0.00002500
Iteration 194/1000 | Loss: 0.00002500
Iteration 195/1000 | Loss: 0.00002500
Iteration 196/1000 | Loss: 0.00002500
Iteration 197/1000 | Loss: 0.00002500
Iteration 198/1000 | Loss: 0.00002500
Iteration 199/1000 | Loss: 0.00002500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.5002787879202515e-05, 2.5002787879202515e-05, 2.5002787879202515e-05, 2.5002787879202515e-05, 2.5002787879202515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5002787879202515e-05

Optimization complete. Final v2v error: 3.9720964431762695 mm

Highest mean error: 7.281475067138672 mm for frame 11

Lowest mean error: 3.520378351211548 mm for frame 4

Saving results

Total time: 80.77022647857666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_012/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_012/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00751084
Iteration 2/25 | Loss: 0.00155680
Iteration 3/25 | Loss: 0.00129153
Iteration 4/25 | Loss: 0.00128322
Iteration 5/25 | Loss: 0.00128075
Iteration 6/25 | Loss: 0.00128070
Iteration 7/25 | Loss: 0.00128070
Iteration 8/25 | Loss: 0.00128070
Iteration 9/25 | Loss: 0.00128070
Iteration 10/25 | Loss: 0.00128070
Iteration 11/25 | Loss: 0.00128070
Iteration 12/25 | Loss: 0.00128070
Iteration 13/25 | Loss: 0.00128070
Iteration 14/25 | Loss: 0.00128070
Iteration 15/25 | Loss: 0.00128070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012806958984583616, 0.0012806958984583616, 0.0012806958984583616, 0.0012806958984583616, 0.0012806958984583616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012806958984583616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38928521
Iteration 2/25 | Loss: 0.00066081
Iteration 3/25 | Loss: 0.00066080
Iteration 4/25 | Loss: 0.00066080
Iteration 5/25 | Loss: 0.00066080
Iteration 6/25 | Loss: 0.00066080
Iteration 7/25 | Loss: 0.00066080
Iteration 8/25 | Loss: 0.00066080
Iteration 9/25 | Loss: 0.00066080
Iteration 10/25 | Loss: 0.00066080
Iteration 11/25 | Loss: 0.00066080
Iteration 12/25 | Loss: 0.00066080
Iteration 13/25 | Loss: 0.00066080
Iteration 14/25 | Loss: 0.00066080
Iteration 15/25 | Loss: 0.00066080
Iteration 16/25 | Loss: 0.00066080
Iteration 17/25 | Loss: 0.00066080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006607988616451621, 0.0006607988616451621, 0.0006607988616451621, 0.0006607988616451621, 0.0006607988616451621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006607988616451621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066080
Iteration 2/1000 | Loss: 0.00003838
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002192
Iteration 5/1000 | Loss: 0.00002054
Iteration 6/1000 | Loss: 0.00001936
Iteration 7/1000 | Loss: 0.00001875
Iteration 8/1000 | Loss: 0.00001813
Iteration 9/1000 | Loss: 0.00001776
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001696
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001664
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001640
Iteration 17/1000 | Loss: 0.00001635
Iteration 18/1000 | Loss: 0.00001635
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001605
Iteration 26/1000 | Loss: 0.00001601
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001594
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001589
Iteration 34/1000 | Loss: 0.00001589
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001586
Iteration 40/1000 | Loss: 0.00001586
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001585
Iteration 43/1000 | Loss: 0.00001585
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001584
Iteration 46/1000 | Loss: 0.00001584
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001583
Iteration 49/1000 | Loss: 0.00001582
Iteration 50/1000 | Loss: 0.00001582
Iteration 51/1000 | Loss: 0.00001582
Iteration 52/1000 | Loss: 0.00001581
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001580
Iteration 55/1000 | Loss: 0.00001580
Iteration 56/1000 | Loss: 0.00001579
Iteration 57/1000 | Loss: 0.00001578
Iteration 58/1000 | Loss: 0.00001578
Iteration 59/1000 | Loss: 0.00001578
Iteration 60/1000 | Loss: 0.00001577
Iteration 61/1000 | Loss: 0.00001577
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001569
Iteration 86/1000 | Loss: 0.00001567
Iteration 87/1000 | Loss: 0.00001567
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001562
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001560
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001556
Iteration 123/1000 | Loss: 0.00001556
Iteration 124/1000 | Loss: 0.00001555
Iteration 125/1000 | Loss: 0.00001555
Iteration 126/1000 | Loss: 0.00001555
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001554
Iteration 132/1000 | Loss: 0.00001554
Iteration 133/1000 | Loss: 0.00001554
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001554
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001553
Iteration 140/1000 | Loss: 0.00001553
Iteration 141/1000 | Loss: 0.00001553
Iteration 142/1000 | Loss: 0.00001553
Iteration 143/1000 | Loss: 0.00001552
Iteration 144/1000 | Loss: 0.00001552
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001551
Iteration 148/1000 | Loss: 0.00001551
Iteration 149/1000 | Loss: 0.00001551
Iteration 150/1000 | Loss: 0.00001551
Iteration 151/1000 | Loss: 0.00001551
Iteration 152/1000 | Loss: 0.00001551
Iteration 153/1000 | Loss: 0.00001551
Iteration 154/1000 | Loss: 0.00001550
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001550
Iteration 158/1000 | Loss: 0.00001550
Iteration 159/1000 | Loss: 0.00001550
Iteration 160/1000 | Loss: 0.00001550
Iteration 161/1000 | Loss: 0.00001550
Iteration 162/1000 | Loss: 0.00001550
Iteration 163/1000 | Loss: 0.00001550
Iteration 164/1000 | Loss: 0.00001549
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001549
Iteration 167/1000 | Loss: 0.00001549
Iteration 168/1000 | Loss: 0.00001549
Iteration 169/1000 | Loss: 0.00001549
Iteration 170/1000 | Loss: 0.00001549
Iteration 171/1000 | Loss: 0.00001549
Iteration 172/1000 | Loss: 0.00001549
Iteration 173/1000 | Loss: 0.00001549
Iteration 174/1000 | Loss: 0.00001549
Iteration 175/1000 | Loss: 0.00001549
Iteration 176/1000 | Loss: 0.00001549
Iteration 177/1000 | Loss: 0.00001548
Iteration 178/1000 | Loss: 0.00001548
Iteration 179/1000 | Loss: 0.00001548
Iteration 180/1000 | Loss: 0.00001548
Iteration 181/1000 | Loss: 0.00001548
Iteration 182/1000 | Loss: 0.00001548
Iteration 183/1000 | Loss: 0.00001548
Iteration 184/1000 | Loss: 0.00001548
Iteration 185/1000 | Loss: 0.00001547
Iteration 186/1000 | Loss: 0.00001547
Iteration 187/1000 | Loss: 0.00001547
Iteration 188/1000 | Loss: 0.00001547
Iteration 189/1000 | Loss: 0.00001547
Iteration 190/1000 | Loss: 0.00001547
Iteration 191/1000 | Loss: 0.00001547
Iteration 192/1000 | Loss: 0.00001547
Iteration 193/1000 | Loss: 0.00001547
Iteration 194/1000 | Loss: 0.00001547
Iteration 195/1000 | Loss: 0.00001547
Iteration 196/1000 | Loss: 0.00001547
Iteration 197/1000 | Loss: 0.00001547
Iteration 198/1000 | Loss: 0.00001547
Iteration 199/1000 | Loss: 0.00001547
Iteration 200/1000 | Loss: 0.00001547
Iteration 201/1000 | Loss: 0.00001547
Iteration 202/1000 | Loss: 0.00001546
Iteration 203/1000 | Loss: 0.00001546
Iteration 204/1000 | Loss: 0.00001546
Iteration 205/1000 | Loss: 0.00001546
Iteration 206/1000 | Loss: 0.00001546
Iteration 207/1000 | Loss: 0.00001546
Iteration 208/1000 | Loss: 0.00001546
Iteration 209/1000 | Loss: 0.00001546
Iteration 210/1000 | Loss: 0.00001546
Iteration 211/1000 | Loss: 0.00001546
Iteration 212/1000 | Loss: 0.00001546
Iteration 213/1000 | Loss: 0.00001546
Iteration 214/1000 | Loss: 0.00001546
Iteration 215/1000 | Loss: 0.00001546
Iteration 216/1000 | Loss: 0.00001546
Iteration 217/1000 | Loss: 0.00001546
Iteration 218/1000 | Loss: 0.00001546
Iteration 219/1000 | Loss: 0.00001546
Iteration 220/1000 | Loss: 0.00001546
Iteration 221/1000 | Loss: 0.00001546
Iteration 222/1000 | Loss: 0.00001545
Iteration 223/1000 | Loss: 0.00001545
Iteration 224/1000 | Loss: 0.00001545
Iteration 225/1000 | Loss: 0.00001545
Iteration 226/1000 | Loss: 0.00001545
Iteration 227/1000 | Loss: 0.00001545
Iteration 228/1000 | Loss: 0.00001545
Iteration 229/1000 | Loss: 0.00001545
Iteration 230/1000 | Loss: 0.00001545
Iteration 231/1000 | Loss: 0.00001545
Iteration 232/1000 | Loss: 0.00001545
Iteration 233/1000 | Loss: 0.00001545
Iteration 234/1000 | Loss: 0.00001545
Iteration 235/1000 | Loss: 0.00001545
Iteration 236/1000 | Loss: 0.00001545
Iteration 237/1000 | Loss: 0.00001545
Iteration 238/1000 | Loss: 0.00001545
Iteration 239/1000 | Loss: 0.00001545
Iteration 240/1000 | Loss: 0.00001545
Iteration 241/1000 | Loss: 0.00001545
Iteration 242/1000 | Loss: 0.00001545
Iteration 243/1000 | Loss: 0.00001545
Iteration 244/1000 | Loss: 0.00001545
Iteration 245/1000 | Loss: 0.00001545
Iteration 246/1000 | Loss: 0.00001544
Iteration 247/1000 | Loss: 0.00001544
Iteration 248/1000 | Loss: 0.00001544
Iteration 249/1000 | Loss: 0.00001544
Iteration 250/1000 | Loss: 0.00001544
Iteration 251/1000 | Loss: 0.00001544
Iteration 252/1000 | Loss: 0.00001544
Iteration 253/1000 | Loss: 0.00001544
Iteration 254/1000 | Loss: 0.00001544
Iteration 255/1000 | Loss: 0.00001544
Iteration 256/1000 | Loss: 0.00001544
Iteration 257/1000 | Loss: 0.00001544
Iteration 258/1000 | Loss: 0.00001544
Iteration 259/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.544236693007406e-05, 1.544236693007406e-05, 1.544236693007406e-05, 1.544236693007406e-05, 1.544236693007406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.544236693007406e-05

Optimization complete. Final v2v error: 3.283989667892456 mm

Highest mean error: 5.38054895401001 mm for frame 0

Lowest mean error: 2.880601644515991 mm for frame 154

Saving results

Total time: 55.37967252731323
