Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=36, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/.
Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2016-2071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401220
Iteration 2/25 | Loss: 0.00074097
Iteration 3/25 | Loss: 0.00060868
Iteration 4/25 | Loss: 0.00058769
Iteration 5/25 | Loss: 0.00058378
Iteration 6/25 | Loss: 0.00058339
Iteration 7/25 | Loss: 0.00058339
Iteration 8/25 | Loss: 0.00058339
Iteration 9/25 | Loss: 0.00058339
Iteration 10/25 | Loss: 0.00058339
Iteration 11/25 | Loss: 0.00058339
Iteration 12/25 | Loss: 0.00058339
Iteration 13/25 | Loss: 0.00058339
Iteration 14/25 | Loss: 0.00058339
Iteration 15/25 | Loss: 0.00058339
Iteration 16/25 | Loss: 0.00058339
Iteration 17/25 | Loss: 0.00058339
Iteration 18/25 | Loss: 0.00058339
Iteration 19/25 | Loss: 0.00058339
Iteration 20/25 | Loss: 0.00058339
Iteration 21/25 | Loss: 0.00058339
Iteration 22/25 | Loss: 0.00058339
Iteration 23/25 | Loss: 0.00058339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005833893083035946, 0.0005833893083035946, 0.0005833893083035946, 0.0005833893083035946, 0.0005833893083035946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005833893083035946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.47666311
Iteration 2/25 | Loss: 0.00031510
Iteration 3/25 | Loss: 0.00031509
Iteration 4/25 | Loss: 0.00031509
Iteration 5/25 | Loss: 0.00031509
Iteration 6/25 | Loss: 0.00031509
Iteration 7/25 | Loss: 0.00031509
Iteration 8/25 | Loss: 0.00031509
Iteration 9/25 | Loss: 0.00031509
Iteration 10/25 | Loss: 0.00031509
Iteration 11/25 | Loss: 0.00031509
Iteration 12/25 | Loss: 0.00031509
Iteration 13/25 | Loss: 0.00031509
Iteration 14/25 | Loss: 0.00031509
Iteration 15/25 | Loss: 0.00031509
Iteration 16/25 | Loss: 0.00031509
Iteration 17/25 | Loss: 0.00031509
Iteration 18/25 | Loss: 0.00031509
Iteration 19/25 | Loss: 0.00031509
Iteration 20/25 | Loss: 0.00031509
Iteration 21/25 | Loss: 0.00031509
Iteration 22/25 | Loss: 0.00031509
Iteration 23/25 | Loss: 0.00031509
Iteration 24/25 | Loss: 0.00031509
Iteration 25/25 | Loss: 0.00031509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031509
Iteration 2/1000 | Loss: 0.00004104
Iteration 3/1000 | Loss: 0.00002296
Iteration 4/1000 | Loss: 0.00002042
Iteration 5/1000 | Loss: 0.00001891
Iteration 6/1000 | Loss: 0.00001834
Iteration 7/1000 | Loss: 0.00001799
Iteration 8/1000 | Loss: 0.00001762
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001735
Iteration 11/1000 | Loss: 0.00001729
Iteration 12/1000 | Loss: 0.00001726
Iteration 13/1000 | Loss: 0.00001725
Iteration 14/1000 | Loss: 0.00001720
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001717
Iteration 18/1000 | Loss: 0.00001717
Iteration 19/1000 | Loss: 0.00001716
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001714
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001714
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001713
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001710
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001709
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001708
Iteration 46/1000 | Loss: 0.00001708
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001700
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001698
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001697
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001695
Iteration 97/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.6948561096796766e-05, 1.6948561096796766e-05, 1.6948561096796766e-05, 1.6948561096796766e-05, 1.6948561096796766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6948561096796766e-05

Optimization complete. Final v2v error: 3.5338222980499268 mm

Highest mean error: 4.174777507781982 mm for frame 52

Lowest mean error: 3.1300182342529297 mm for frame 143

Saving results

Total time: 31.066828727722168
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502391
Iteration 2/25 | Loss: 0.00084630
Iteration 3/25 | Loss: 0.00065581
Iteration 4/25 | Loss: 0.00063681
Iteration 5/25 | Loss: 0.00063315
Iteration 6/25 | Loss: 0.00063293
Iteration 7/25 | Loss: 0.00063293
Iteration 8/25 | Loss: 0.00063293
Iteration 9/25 | Loss: 0.00063293
Iteration 10/25 | Loss: 0.00063293
Iteration 11/25 | Loss: 0.00063293
Iteration 12/25 | Loss: 0.00063293
Iteration 13/25 | Loss: 0.00063293
Iteration 14/25 | Loss: 0.00063293
Iteration 15/25 | Loss: 0.00063293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006329279276542366, 0.0006329279276542366, 0.0006329279276542366, 0.0006329279276542366, 0.0006329279276542366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006329279276542366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87580943
Iteration 2/25 | Loss: 0.00031271
Iteration 3/25 | Loss: 0.00031270
Iteration 4/25 | Loss: 0.00031270
Iteration 5/25 | Loss: 0.00031270
Iteration 6/25 | Loss: 0.00031270
Iteration 7/25 | Loss: 0.00031270
Iteration 8/25 | Loss: 0.00031270
Iteration 9/25 | Loss: 0.00031270
Iteration 10/25 | Loss: 0.00031270
Iteration 11/25 | Loss: 0.00031270
Iteration 12/25 | Loss: 0.00031270
Iteration 13/25 | Loss: 0.00031270
Iteration 14/25 | Loss: 0.00031270
Iteration 15/25 | Loss: 0.00031270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00031269638566300273, 0.00031269638566300273, 0.00031269638566300273, 0.00031269638566300273, 0.00031269638566300273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031269638566300273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031270
Iteration 2/1000 | Loss: 0.00002676
Iteration 3/1000 | Loss: 0.00002148
Iteration 4/1000 | Loss: 0.00002014
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001889
Iteration 8/1000 | Loss: 0.00001866
Iteration 9/1000 | Loss: 0.00001858
Iteration 10/1000 | Loss: 0.00001845
Iteration 11/1000 | Loss: 0.00001841
Iteration 12/1000 | Loss: 0.00001837
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001833
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001826
Iteration 19/1000 | Loss: 0.00001826
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001826
Iteration 22/1000 | Loss: 0.00001825
Iteration 23/1000 | Loss: 0.00001825
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001822
Iteration 27/1000 | Loss: 0.00001822
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001821
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001820
Iteration 37/1000 | Loss: 0.00001819
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00001818
Iteration 45/1000 | Loss: 0.00001818
Iteration 46/1000 | Loss: 0.00001818
Iteration 47/1000 | Loss: 0.00001818
Iteration 48/1000 | Loss: 0.00001818
Iteration 49/1000 | Loss: 0.00001818
Iteration 50/1000 | Loss: 0.00001818
Iteration 51/1000 | Loss: 0.00001818
Iteration 52/1000 | Loss: 0.00001817
Iteration 53/1000 | Loss: 0.00001815
Iteration 54/1000 | Loss: 0.00001815
Iteration 55/1000 | Loss: 0.00001814
Iteration 56/1000 | Loss: 0.00001814
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001812
Iteration 59/1000 | Loss: 0.00001812
Iteration 60/1000 | Loss: 0.00001812
Iteration 61/1000 | Loss: 0.00001812
Iteration 62/1000 | Loss: 0.00001811
Iteration 63/1000 | Loss: 0.00001811
Iteration 64/1000 | Loss: 0.00001811
Iteration 65/1000 | Loss: 0.00001811
Iteration 66/1000 | Loss: 0.00001811
Iteration 67/1000 | Loss: 0.00001811
Iteration 68/1000 | Loss: 0.00001811
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001808
Iteration 74/1000 | Loss: 0.00001808
Iteration 75/1000 | Loss: 0.00001808
Iteration 76/1000 | Loss: 0.00001808
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001807
Iteration 83/1000 | Loss: 0.00001807
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001806
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001806
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001805
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001805
Iteration 103/1000 | Loss: 0.00001805
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001804
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001802
Iteration 111/1000 | Loss: 0.00001802
Iteration 112/1000 | Loss: 0.00001802
Iteration 113/1000 | Loss: 0.00001802
Iteration 114/1000 | Loss: 0.00001802
Iteration 115/1000 | Loss: 0.00001802
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001800
Iteration 127/1000 | Loss: 0.00001800
Iteration 128/1000 | Loss: 0.00001800
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001800
Iteration 131/1000 | Loss: 0.00001800
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001799
Iteration 134/1000 | Loss: 0.00001799
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001798
Iteration 145/1000 | Loss: 0.00001798
Iteration 146/1000 | Loss: 0.00001798
Iteration 147/1000 | Loss: 0.00001798
Iteration 148/1000 | Loss: 0.00001798
Iteration 149/1000 | Loss: 0.00001798
Iteration 150/1000 | Loss: 0.00001798
Iteration 151/1000 | Loss: 0.00001798
Iteration 152/1000 | Loss: 0.00001798
Iteration 153/1000 | Loss: 0.00001797
Iteration 154/1000 | Loss: 0.00001797
Iteration 155/1000 | Loss: 0.00001797
Iteration 156/1000 | Loss: 0.00001797
Iteration 157/1000 | Loss: 0.00001797
Iteration 158/1000 | Loss: 0.00001797
Iteration 159/1000 | Loss: 0.00001797
Iteration 160/1000 | Loss: 0.00001797
Iteration 161/1000 | Loss: 0.00001797
Iteration 162/1000 | Loss: 0.00001797
Iteration 163/1000 | Loss: 0.00001797
Iteration 164/1000 | Loss: 0.00001797
Iteration 165/1000 | Loss: 0.00001797
Iteration 166/1000 | Loss: 0.00001797
Iteration 167/1000 | Loss: 0.00001797
Iteration 168/1000 | Loss: 0.00001797
Iteration 169/1000 | Loss: 0.00001797
Iteration 170/1000 | Loss: 0.00001797
Iteration 171/1000 | Loss: 0.00001797
Iteration 172/1000 | Loss: 0.00001797
Iteration 173/1000 | Loss: 0.00001797
Iteration 174/1000 | Loss: 0.00001797
Iteration 175/1000 | Loss: 0.00001797
Iteration 176/1000 | Loss: 0.00001797
Iteration 177/1000 | Loss: 0.00001797
Iteration 178/1000 | Loss: 0.00001797
Iteration 179/1000 | Loss: 0.00001797
Iteration 180/1000 | Loss: 0.00001797
Iteration 181/1000 | Loss: 0.00001797
Iteration 182/1000 | Loss: 0.00001797
Iteration 183/1000 | Loss: 0.00001797
Iteration 184/1000 | Loss: 0.00001797
Iteration 185/1000 | Loss: 0.00001797
Iteration 186/1000 | Loss: 0.00001797
Iteration 187/1000 | Loss: 0.00001797
Iteration 188/1000 | Loss: 0.00001797
Iteration 189/1000 | Loss: 0.00001797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.7970614862861112e-05, 1.7970614862861112e-05, 1.7970614862861112e-05, 1.7970614862861112e-05, 1.7970614862861112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7970614862861112e-05

Optimization complete. Final v2v error: 3.6567959785461426 mm

Highest mean error: 4.0842108726501465 mm for frame 256

Lowest mean error: 3.3770179748535156 mm for frame 153

Saving results

Total time: 41.51768898963928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831147
Iteration 2/25 | Loss: 0.00072865
Iteration 3/25 | Loss: 0.00055912
Iteration 4/25 | Loss: 0.00053629
Iteration 5/25 | Loss: 0.00052930
Iteration 6/25 | Loss: 0.00052772
Iteration 7/25 | Loss: 0.00052745
Iteration 8/25 | Loss: 0.00052745
Iteration 9/25 | Loss: 0.00052745
Iteration 10/25 | Loss: 0.00052745
Iteration 11/25 | Loss: 0.00052745
Iteration 12/25 | Loss: 0.00052745
Iteration 13/25 | Loss: 0.00052745
Iteration 14/25 | Loss: 0.00052745
Iteration 15/25 | Loss: 0.00052745
Iteration 16/25 | Loss: 0.00052745
Iteration 17/25 | Loss: 0.00052745
Iteration 18/25 | Loss: 0.00052745
Iteration 19/25 | Loss: 0.00052745
Iteration 20/25 | Loss: 0.00052745
Iteration 21/25 | Loss: 0.00052745
Iteration 22/25 | Loss: 0.00052745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005274512805044651, 0.0005274512805044651, 0.0005274512805044651, 0.0005274512805044651, 0.0005274512805044651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005274512805044651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49081469
Iteration 2/25 | Loss: 0.00035331
Iteration 3/25 | Loss: 0.00035331
Iteration 4/25 | Loss: 0.00035331
Iteration 5/25 | Loss: 0.00035331
Iteration 6/25 | Loss: 0.00035331
Iteration 7/25 | Loss: 0.00035331
Iteration 8/25 | Loss: 0.00035330
Iteration 9/25 | Loss: 0.00035330
Iteration 10/25 | Loss: 0.00035330
Iteration 11/25 | Loss: 0.00035330
Iteration 12/25 | Loss: 0.00035330
Iteration 13/25 | Loss: 0.00035330
Iteration 14/25 | Loss: 0.00035330
Iteration 15/25 | Loss: 0.00035330
Iteration 16/25 | Loss: 0.00035330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00035330449463799596, 0.00035330449463799596, 0.00035330449463799596, 0.00035330449463799596, 0.00035330449463799596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035330449463799596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035330
Iteration 2/1000 | Loss: 0.00002370
Iteration 3/1000 | Loss: 0.00001354
Iteration 4/1000 | Loss: 0.00001161
Iteration 5/1000 | Loss: 0.00001067
Iteration 6/1000 | Loss: 0.00001046
Iteration 7/1000 | Loss: 0.00001025
Iteration 8/1000 | Loss: 0.00001022
Iteration 9/1000 | Loss: 0.00001018
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00001016
Iteration 12/1000 | Loss: 0.00001007
Iteration 13/1000 | Loss: 0.00001007
Iteration 14/1000 | Loss: 0.00001006
Iteration 15/1000 | Loss: 0.00001006
Iteration 16/1000 | Loss: 0.00001006
Iteration 17/1000 | Loss: 0.00001006
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001004
Iteration 23/1000 | Loss: 0.00001004
Iteration 24/1000 | Loss: 0.00001004
Iteration 25/1000 | Loss: 0.00001004
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001002
Iteration 29/1000 | Loss: 0.00001002
Iteration 30/1000 | Loss: 0.00001002
Iteration 31/1000 | Loss: 0.00001001
Iteration 32/1000 | Loss: 0.00001001
Iteration 33/1000 | Loss: 0.00001001
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00001000
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000997
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000995
Iteration 45/1000 | Loss: 0.00000995
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000994
Iteration 49/1000 | Loss: 0.00000994
Iteration 50/1000 | Loss: 0.00000993
Iteration 51/1000 | Loss: 0.00000993
Iteration 52/1000 | Loss: 0.00000993
Iteration 53/1000 | Loss: 0.00000993
Iteration 54/1000 | Loss: 0.00000993
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000993
Iteration 58/1000 | Loss: 0.00000992
Iteration 59/1000 | Loss: 0.00000992
Iteration 60/1000 | Loss: 0.00000992
Iteration 61/1000 | Loss: 0.00000992
Iteration 62/1000 | Loss: 0.00000992
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000992
Iteration 67/1000 | Loss: 0.00000992
Iteration 68/1000 | Loss: 0.00000992
Iteration 69/1000 | Loss: 0.00000992
Iteration 70/1000 | Loss: 0.00000992
Iteration 71/1000 | Loss: 0.00000991
Iteration 72/1000 | Loss: 0.00000991
Iteration 73/1000 | Loss: 0.00000991
Iteration 74/1000 | Loss: 0.00000991
Iteration 75/1000 | Loss: 0.00000991
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000991
Iteration 79/1000 | Loss: 0.00000990
Iteration 80/1000 | Loss: 0.00000990
Iteration 81/1000 | Loss: 0.00000990
Iteration 82/1000 | Loss: 0.00000990
Iteration 83/1000 | Loss: 0.00000990
Iteration 84/1000 | Loss: 0.00000990
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000990
Iteration 88/1000 | Loss: 0.00000990
Iteration 89/1000 | Loss: 0.00000990
Iteration 90/1000 | Loss: 0.00000990
Iteration 91/1000 | Loss: 0.00000990
Iteration 92/1000 | Loss: 0.00000990
Iteration 93/1000 | Loss: 0.00000990
Iteration 94/1000 | Loss: 0.00000990
Iteration 95/1000 | Loss: 0.00000990
Iteration 96/1000 | Loss: 0.00000989
Iteration 97/1000 | Loss: 0.00000989
Iteration 98/1000 | Loss: 0.00000989
Iteration 99/1000 | Loss: 0.00000989
Iteration 100/1000 | Loss: 0.00000989
Iteration 101/1000 | Loss: 0.00000989
Iteration 102/1000 | Loss: 0.00000989
Iteration 103/1000 | Loss: 0.00000988
Iteration 104/1000 | Loss: 0.00000988
Iteration 105/1000 | Loss: 0.00000988
Iteration 106/1000 | Loss: 0.00000988
Iteration 107/1000 | Loss: 0.00000988
Iteration 108/1000 | Loss: 0.00000988
Iteration 109/1000 | Loss: 0.00000988
Iteration 110/1000 | Loss: 0.00000988
Iteration 111/1000 | Loss: 0.00000988
Iteration 112/1000 | Loss: 0.00000988
Iteration 113/1000 | Loss: 0.00000988
Iteration 114/1000 | Loss: 0.00000988
Iteration 115/1000 | Loss: 0.00000988
Iteration 116/1000 | Loss: 0.00000988
Iteration 117/1000 | Loss: 0.00000987
Iteration 118/1000 | Loss: 0.00000987
Iteration 119/1000 | Loss: 0.00000987
Iteration 120/1000 | Loss: 0.00000987
Iteration 121/1000 | Loss: 0.00000987
Iteration 122/1000 | Loss: 0.00000987
Iteration 123/1000 | Loss: 0.00000987
Iteration 124/1000 | Loss: 0.00000987
Iteration 125/1000 | Loss: 0.00000987
Iteration 126/1000 | Loss: 0.00000986
Iteration 127/1000 | Loss: 0.00000986
Iteration 128/1000 | Loss: 0.00000986
Iteration 129/1000 | Loss: 0.00000986
Iteration 130/1000 | Loss: 0.00000986
Iteration 131/1000 | Loss: 0.00000986
Iteration 132/1000 | Loss: 0.00000986
Iteration 133/1000 | Loss: 0.00000986
Iteration 134/1000 | Loss: 0.00000986
Iteration 135/1000 | Loss: 0.00000986
Iteration 136/1000 | Loss: 0.00000986
Iteration 137/1000 | Loss: 0.00000985
Iteration 138/1000 | Loss: 0.00000985
Iteration 139/1000 | Loss: 0.00000985
Iteration 140/1000 | Loss: 0.00000985
Iteration 141/1000 | Loss: 0.00000985
Iteration 142/1000 | Loss: 0.00000984
Iteration 143/1000 | Loss: 0.00000984
Iteration 144/1000 | Loss: 0.00000984
Iteration 145/1000 | Loss: 0.00000984
Iteration 146/1000 | Loss: 0.00000984
Iteration 147/1000 | Loss: 0.00000984
Iteration 148/1000 | Loss: 0.00000984
Iteration 149/1000 | Loss: 0.00000984
Iteration 150/1000 | Loss: 0.00000983
Iteration 151/1000 | Loss: 0.00000983
Iteration 152/1000 | Loss: 0.00000983
Iteration 153/1000 | Loss: 0.00000983
Iteration 154/1000 | Loss: 0.00000983
Iteration 155/1000 | Loss: 0.00000983
Iteration 156/1000 | Loss: 0.00000983
Iteration 157/1000 | Loss: 0.00000983
Iteration 158/1000 | Loss: 0.00000983
Iteration 159/1000 | Loss: 0.00000983
Iteration 160/1000 | Loss: 0.00000983
Iteration 161/1000 | Loss: 0.00000983
Iteration 162/1000 | Loss: 0.00000983
Iteration 163/1000 | Loss: 0.00000983
Iteration 164/1000 | Loss: 0.00000983
Iteration 165/1000 | Loss: 0.00000983
Iteration 166/1000 | Loss: 0.00000983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [9.83203608484473e-06, 9.83203608484473e-06, 9.83203608484473e-06, 9.83203608484473e-06, 9.83203608484473e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.83203608484473e-06

Optimization complete. Final v2v error: 2.701101541519165 mm

Highest mean error: 2.933105230331421 mm for frame 104

Lowest mean error: 2.4588935375213623 mm for frame 137

Saving results

Total time: 33.20740079879761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420158
Iteration 2/25 | Loss: 0.00084587
Iteration 3/25 | Loss: 0.00057202
Iteration 4/25 | Loss: 0.00055655
Iteration 5/25 | Loss: 0.00055372
Iteration 6/25 | Loss: 0.00055296
Iteration 7/25 | Loss: 0.00055296
Iteration 8/25 | Loss: 0.00055296
Iteration 9/25 | Loss: 0.00055296
Iteration 10/25 | Loss: 0.00055296
Iteration 11/25 | Loss: 0.00055296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005529590416699648, 0.0005529590416699648, 0.0005529590416699648, 0.0005529590416699648, 0.0005529590416699648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005529590416699648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50673246
Iteration 2/25 | Loss: 0.00038396
Iteration 3/25 | Loss: 0.00038396
Iteration 4/25 | Loss: 0.00038396
Iteration 5/25 | Loss: 0.00038396
Iteration 6/25 | Loss: 0.00038396
Iteration 7/25 | Loss: 0.00038396
Iteration 8/25 | Loss: 0.00038396
Iteration 9/25 | Loss: 0.00038396
Iteration 10/25 | Loss: 0.00038396
Iteration 11/25 | Loss: 0.00038396
Iteration 12/25 | Loss: 0.00038396
Iteration 13/25 | Loss: 0.00038396
Iteration 14/25 | Loss: 0.00038396
Iteration 15/25 | Loss: 0.00038396
Iteration 16/25 | Loss: 0.00038396
Iteration 17/25 | Loss: 0.00038396
Iteration 18/25 | Loss: 0.00038396
Iteration 19/25 | Loss: 0.00038396
Iteration 20/25 | Loss: 0.00038396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003839594719465822, 0.0003839594719465822, 0.0003839594719465822, 0.0003839594719465822, 0.0003839594719465822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003839594719465822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038396
Iteration 2/1000 | Loss: 0.00002420
Iteration 3/1000 | Loss: 0.00001485
Iteration 4/1000 | Loss: 0.00001296
Iteration 5/1000 | Loss: 0.00001211
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001164
Iteration 8/1000 | Loss: 0.00001163
Iteration 9/1000 | Loss: 0.00001137
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001104
Iteration 15/1000 | Loss: 0.00001104
Iteration 16/1000 | Loss: 0.00001104
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001103
Iteration 22/1000 | Loss: 0.00001102
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001091
Iteration 49/1000 | Loss: 0.00001091
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001090
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001090
Iteration 66/1000 | Loss: 0.00001090
Iteration 67/1000 | Loss: 0.00001090
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001089
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001089
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001088
Iteration 77/1000 | Loss: 0.00001088
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001088
Iteration 81/1000 | Loss: 0.00001088
Iteration 82/1000 | Loss: 0.00001088
Iteration 83/1000 | Loss: 0.00001088
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001088
Iteration 86/1000 | Loss: 0.00001088
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00001088
Iteration 90/1000 | Loss: 0.00001088
Iteration 91/1000 | Loss: 0.00001088
Iteration 92/1000 | Loss: 0.00001088
Iteration 93/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.0877926797547843e-05, 1.0877926797547843e-05, 1.0877926797547843e-05, 1.0877926797547843e-05, 1.0877926797547843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0877926797547843e-05

Optimization complete. Final v2v error: 2.8253982067108154 mm

Highest mean error: 3.0136606693267822 mm for frame 145

Lowest mean error: 2.6552138328552246 mm for frame 36

Saving results

Total time: 27.905680179595947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382690
Iteration 2/25 | Loss: 0.00076437
Iteration 3/25 | Loss: 0.00060645
Iteration 4/25 | Loss: 0.00057793
Iteration 5/25 | Loss: 0.00057090
Iteration 6/25 | Loss: 0.00056949
Iteration 7/25 | Loss: 0.00056949
Iteration 8/25 | Loss: 0.00056949
Iteration 9/25 | Loss: 0.00056949
Iteration 10/25 | Loss: 0.00056949
Iteration 11/25 | Loss: 0.00056949
Iteration 12/25 | Loss: 0.00056949
Iteration 13/25 | Loss: 0.00056949
Iteration 14/25 | Loss: 0.00056949
Iteration 15/25 | Loss: 0.00056949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005694872816093266, 0.0005694872816093266, 0.0005694872816093266, 0.0005694872816093266, 0.0005694872816093266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005694872816093266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48420990
Iteration 2/25 | Loss: 0.00039603
Iteration 3/25 | Loss: 0.00039603
Iteration 4/25 | Loss: 0.00039603
Iteration 5/25 | Loss: 0.00039603
Iteration 6/25 | Loss: 0.00039602
Iteration 7/25 | Loss: 0.00039602
Iteration 8/25 | Loss: 0.00039602
Iteration 9/25 | Loss: 0.00039602
Iteration 10/25 | Loss: 0.00039602
Iteration 11/25 | Loss: 0.00039602
Iteration 12/25 | Loss: 0.00039602
Iteration 13/25 | Loss: 0.00039602
Iteration 14/25 | Loss: 0.00039602
Iteration 15/25 | Loss: 0.00039602
Iteration 16/25 | Loss: 0.00039602
Iteration 17/25 | Loss: 0.00039602
Iteration 18/25 | Loss: 0.00039602
Iteration 19/25 | Loss: 0.00039602
Iteration 20/25 | Loss: 0.00039602
Iteration 21/25 | Loss: 0.00039602
Iteration 22/25 | Loss: 0.00039602
Iteration 23/25 | Loss: 0.00039602
Iteration 24/25 | Loss: 0.00039602
Iteration 25/25 | Loss: 0.00039602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039602
Iteration 2/1000 | Loss: 0.00003034
Iteration 3/1000 | Loss: 0.00001824
Iteration 4/1000 | Loss: 0.00001601
Iteration 5/1000 | Loss: 0.00001506
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001426
Iteration 8/1000 | Loss: 0.00001410
Iteration 9/1000 | Loss: 0.00001409
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001405
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001402
Iteration 16/1000 | Loss: 0.00001402
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001397
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001394
Iteration 24/1000 | Loss: 0.00001393
Iteration 25/1000 | Loss: 0.00001393
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001392
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001388
Iteration 32/1000 | Loss: 0.00001388
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001384
Iteration 41/1000 | Loss: 0.00001383
Iteration 42/1000 | Loss: 0.00001380
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001374
Iteration 49/1000 | Loss: 0.00001374
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001372
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001370
Iteration 58/1000 | Loss: 0.00001370
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001369
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001368
Iteration 63/1000 | Loss: 0.00001368
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001367
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001366
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001364
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001363
Iteration 97/1000 | Loss: 0.00001363
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001362
Iteration 101/1000 | Loss: 0.00001362
Iteration 102/1000 | Loss: 0.00001362
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001362
Iteration 118/1000 | Loss: 0.00001362
Iteration 119/1000 | Loss: 0.00001362
Iteration 120/1000 | Loss: 0.00001362
Iteration 121/1000 | Loss: 0.00001362
Iteration 122/1000 | Loss: 0.00001362
Iteration 123/1000 | Loss: 0.00001362
Iteration 124/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.3617967852042057e-05, 1.3617967852042057e-05, 1.3617967852042057e-05, 1.3617967852042057e-05, 1.3617967852042057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3617967852042057e-05

Optimization complete. Final v2v error: 3.1027626991271973 mm

Highest mean error: 3.4962053298950195 mm for frame 52

Lowest mean error: 2.5630314350128174 mm for frame 23

Saving results

Total time: 34.60645890235901
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962307
Iteration 2/25 | Loss: 0.00098355
Iteration 3/25 | Loss: 0.00077874
Iteration 4/25 | Loss: 0.00072975
Iteration 5/25 | Loss: 0.00070604
Iteration 6/25 | Loss: 0.00069921
Iteration 7/25 | Loss: 0.00069734
Iteration 8/25 | Loss: 0.00069682
Iteration 9/25 | Loss: 0.00069682
Iteration 10/25 | Loss: 0.00069682
Iteration 11/25 | Loss: 0.00069682
Iteration 12/25 | Loss: 0.00069682
Iteration 13/25 | Loss: 0.00069682
Iteration 14/25 | Loss: 0.00069682
Iteration 15/25 | Loss: 0.00069682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006968151428736746, 0.0006968151428736746, 0.0006968151428736746, 0.0006968151428736746, 0.0006968151428736746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006968151428736746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47005713
Iteration 2/25 | Loss: 0.00036985
Iteration 3/25 | Loss: 0.00036983
Iteration 4/25 | Loss: 0.00036983
Iteration 5/25 | Loss: 0.00036983
Iteration 6/25 | Loss: 0.00036983
Iteration 7/25 | Loss: 0.00036983
Iteration 8/25 | Loss: 0.00036983
Iteration 9/25 | Loss: 0.00036983
Iteration 10/25 | Loss: 0.00036983
Iteration 11/25 | Loss: 0.00036983
Iteration 12/25 | Loss: 0.00036983
Iteration 13/25 | Loss: 0.00036983
Iteration 14/25 | Loss: 0.00036983
Iteration 15/25 | Loss: 0.00036983
Iteration 16/25 | Loss: 0.00036983
Iteration 17/25 | Loss: 0.00036983
Iteration 18/25 | Loss: 0.00036983
Iteration 19/25 | Loss: 0.00036983
Iteration 20/25 | Loss: 0.00036983
Iteration 21/25 | Loss: 0.00036983
Iteration 22/25 | Loss: 0.00036983
Iteration 23/25 | Loss: 0.00036983
Iteration 24/25 | Loss: 0.00036983
Iteration 25/25 | Loss: 0.00036983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00036982831079512835, 0.00036982831079512835, 0.00036982831079512835, 0.00036982831079512835, 0.00036982831079512835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036982831079512835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036983
Iteration 2/1000 | Loss: 0.00006205
Iteration 3/1000 | Loss: 0.00004162
Iteration 4/1000 | Loss: 0.00003558
Iteration 5/1000 | Loss: 0.00003283
Iteration 6/1000 | Loss: 0.00003141
Iteration 7/1000 | Loss: 0.00003052
Iteration 8/1000 | Loss: 0.00002982
Iteration 9/1000 | Loss: 0.00002911
Iteration 10/1000 | Loss: 0.00002865
Iteration 11/1000 | Loss: 0.00002847
Iteration 12/1000 | Loss: 0.00002843
Iteration 13/1000 | Loss: 0.00002825
Iteration 14/1000 | Loss: 0.00002810
Iteration 15/1000 | Loss: 0.00002808
Iteration 16/1000 | Loss: 0.00002806
Iteration 17/1000 | Loss: 0.00002804
Iteration 18/1000 | Loss: 0.00002804
Iteration 19/1000 | Loss: 0.00002802
Iteration 20/1000 | Loss: 0.00002799
Iteration 21/1000 | Loss: 0.00002799
Iteration 22/1000 | Loss: 0.00002797
Iteration 23/1000 | Loss: 0.00002797
Iteration 24/1000 | Loss: 0.00002796
Iteration 25/1000 | Loss: 0.00002796
Iteration 26/1000 | Loss: 0.00002795
Iteration 27/1000 | Loss: 0.00002795
Iteration 28/1000 | Loss: 0.00002795
Iteration 29/1000 | Loss: 0.00002794
Iteration 30/1000 | Loss: 0.00002794
Iteration 31/1000 | Loss: 0.00002793
Iteration 32/1000 | Loss: 0.00002793
Iteration 33/1000 | Loss: 0.00002793
Iteration 34/1000 | Loss: 0.00002792
Iteration 35/1000 | Loss: 0.00002792
Iteration 36/1000 | Loss: 0.00002792
Iteration 37/1000 | Loss: 0.00002789
Iteration 38/1000 | Loss: 0.00002789
Iteration 39/1000 | Loss: 0.00002789
Iteration 40/1000 | Loss: 0.00002789
Iteration 41/1000 | Loss: 0.00002789
Iteration 42/1000 | Loss: 0.00002787
Iteration 43/1000 | Loss: 0.00002787
Iteration 44/1000 | Loss: 0.00002786
Iteration 45/1000 | Loss: 0.00002786
Iteration 46/1000 | Loss: 0.00002785
Iteration 47/1000 | Loss: 0.00002785
Iteration 48/1000 | Loss: 0.00002784
Iteration 49/1000 | Loss: 0.00002784
Iteration 50/1000 | Loss: 0.00002784
Iteration 51/1000 | Loss: 0.00002783
Iteration 52/1000 | Loss: 0.00002783
Iteration 53/1000 | Loss: 0.00002782
Iteration 54/1000 | Loss: 0.00002782
Iteration 55/1000 | Loss: 0.00002782
Iteration 56/1000 | Loss: 0.00002781
Iteration 57/1000 | Loss: 0.00002781
Iteration 58/1000 | Loss: 0.00002780
Iteration 59/1000 | Loss: 0.00002780
Iteration 60/1000 | Loss: 0.00002780
Iteration 61/1000 | Loss: 0.00002780
Iteration 62/1000 | Loss: 0.00002780
Iteration 63/1000 | Loss: 0.00002780
Iteration 64/1000 | Loss: 0.00002780
Iteration 65/1000 | Loss: 0.00002778
Iteration 66/1000 | Loss: 0.00002777
Iteration 67/1000 | Loss: 0.00002777
Iteration 68/1000 | Loss: 0.00002777
Iteration 69/1000 | Loss: 0.00002776
Iteration 70/1000 | Loss: 0.00002776
Iteration 71/1000 | Loss: 0.00002776
Iteration 72/1000 | Loss: 0.00002775
Iteration 73/1000 | Loss: 0.00002775
Iteration 74/1000 | Loss: 0.00002775
Iteration 75/1000 | Loss: 0.00002774
Iteration 76/1000 | Loss: 0.00002774
Iteration 77/1000 | Loss: 0.00002774
Iteration 78/1000 | Loss: 0.00002774
Iteration 79/1000 | Loss: 0.00002773
Iteration 80/1000 | Loss: 0.00002773
Iteration 81/1000 | Loss: 0.00002773
Iteration 82/1000 | Loss: 0.00002773
Iteration 83/1000 | Loss: 0.00002773
Iteration 84/1000 | Loss: 0.00002773
Iteration 85/1000 | Loss: 0.00002773
Iteration 86/1000 | Loss: 0.00002772
Iteration 87/1000 | Loss: 0.00002772
Iteration 88/1000 | Loss: 0.00002772
Iteration 89/1000 | Loss: 0.00002772
Iteration 90/1000 | Loss: 0.00002772
Iteration 91/1000 | Loss: 0.00002770
Iteration 92/1000 | Loss: 0.00002770
Iteration 93/1000 | Loss: 0.00002770
Iteration 94/1000 | Loss: 0.00002769
Iteration 95/1000 | Loss: 0.00002769
Iteration 96/1000 | Loss: 0.00002769
Iteration 97/1000 | Loss: 0.00002769
Iteration 98/1000 | Loss: 0.00002769
Iteration 99/1000 | Loss: 0.00002769
Iteration 100/1000 | Loss: 0.00002769
Iteration 101/1000 | Loss: 0.00002769
Iteration 102/1000 | Loss: 0.00002769
Iteration 103/1000 | Loss: 0.00002769
Iteration 104/1000 | Loss: 0.00002769
Iteration 105/1000 | Loss: 0.00002769
Iteration 106/1000 | Loss: 0.00002769
Iteration 107/1000 | Loss: 0.00002768
Iteration 108/1000 | Loss: 0.00002768
Iteration 109/1000 | Loss: 0.00002767
Iteration 110/1000 | Loss: 0.00002767
Iteration 111/1000 | Loss: 0.00002767
Iteration 112/1000 | Loss: 0.00002767
Iteration 113/1000 | Loss: 0.00002767
Iteration 114/1000 | Loss: 0.00002767
Iteration 115/1000 | Loss: 0.00002766
Iteration 116/1000 | Loss: 0.00002766
Iteration 117/1000 | Loss: 0.00002766
Iteration 118/1000 | Loss: 0.00002766
Iteration 119/1000 | Loss: 0.00002766
Iteration 120/1000 | Loss: 0.00002766
Iteration 121/1000 | Loss: 0.00002766
Iteration 122/1000 | Loss: 0.00002766
Iteration 123/1000 | Loss: 0.00002766
Iteration 124/1000 | Loss: 0.00002766
Iteration 125/1000 | Loss: 0.00002765
Iteration 126/1000 | Loss: 0.00002765
Iteration 127/1000 | Loss: 0.00002765
Iteration 128/1000 | Loss: 0.00002765
Iteration 129/1000 | Loss: 0.00002765
Iteration 130/1000 | Loss: 0.00002765
Iteration 131/1000 | Loss: 0.00002765
Iteration 132/1000 | Loss: 0.00002765
Iteration 133/1000 | Loss: 0.00002765
Iteration 134/1000 | Loss: 0.00002764
Iteration 135/1000 | Loss: 0.00002764
Iteration 136/1000 | Loss: 0.00002764
Iteration 137/1000 | Loss: 0.00002764
Iteration 138/1000 | Loss: 0.00002764
Iteration 139/1000 | Loss: 0.00002764
Iteration 140/1000 | Loss: 0.00002764
Iteration 141/1000 | Loss: 0.00002764
Iteration 142/1000 | Loss: 0.00002764
Iteration 143/1000 | Loss: 0.00002764
Iteration 144/1000 | Loss: 0.00002764
Iteration 145/1000 | Loss: 0.00002764
Iteration 146/1000 | Loss: 0.00002764
Iteration 147/1000 | Loss: 0.00002764
Iteration 148/1000 | Loss: 0.00002763
Iteration 149/1000 | Loss: 0.00002763
Iteration 150/1000 | Loss: 0.00002763
Iteration 151/1000 | Loss: 0.00002763
Iteration 152/1000 | Loss: 0.00002763
Iteration 153/1000 | Loss: 0.00002763
Iteration 154/1000 | Loss: 0.00002763
Iteration 155/1000 | Loss: 0.00002763
Iteration 156/1000 | Loss: 0.00002763
Iteration 157/1000 | Loss: 0.00002763
Iteration 158/1000 | Loss: 0.00002763
Iteration 159/1000 | Loss: 0.00002763
Iteration 160/1000 | Loss: 0.00002762
Iteration 161/1000 | Loss: 0.00002762
Iteration 162/1000 | Loss: 0.00002762
Iteration 163/1000 | Loss: 0.00002762
Iteration 164/1000 | Loss: 0.00002762
Iteration 165/1000 | Loss: 0.00002762
Iteration 166/1000 | Loss: 0.00002762
Iteration 167/1000 | Loss: 0.00002762
Iteration 168/1000 | Loss: 0.00002762
Iteration 169/1000 | Loss: 0.00002762
Iteration 170/1000 | Loss: 0.00002761
Iteration 171/1000 | Loss: 0.00002761
Iteration 172/1000 | Loss: 0.00002761
Iteration 173/1000 | Loss: 0.00002761
Iteration 174/1000 | Loss: 0.00002761
Iteration 175/1000 | Loss: 0.00002761
Iteration 176/1000 | Loss: 0.00002761
Iteration 177/1000 | Loss: 0.00002761
Iteration 178/1000 | Loss: 0.00002761
Iteration 179/1000 | Loss: 0.00002761
Iteration 180/1000 | Loss: 0.00002761
Iteration 181/1000 | Loss: 0.00002761
Iteration 182/1000 | Loss: 0.00002761
Iteration 183/1000 | Loss: 0.00002761
Iteration 184/1000 | Loss: 0.00002761
Iteration 185/1000 | Loss: 0.00002761
Iteration 186/1000 | Loss: 0.00002761
Iteration 187/1000 | Loss: 0.00002761
Iteration 188/1000 | Loss: 0.00002760
Iteration 189/1000 | Loss: 0.00002760
Iteration 190/1000 | Loss: 0.00002760
Iteration 191/1000 | Loss: 0.00002760
Iteration 192/1000 | Loss: 0.00002760
Iteration 193/1000 | Loss: 0.00002760
Iteration 194/1000 | Loss: 0.00002760
Iteration 195/1000 | Loss: 0.00002760
Iteration 196/1000 | Loss: 0.00002760
Iteration 197/1000 | Loss: 0.00002760
Iteration 198/1000 | Loss: 0.00002760
Iteration 199/1000 | Loss: 0.00002760
Iteration 200/1000 | Loss: 0.00002760
Iteration 201/1000 | Loss: 0.00002760
Iteration 202/1000 | Loss: 0.00002760
Iteration 203/1000 | Loss: 0.00002760
Iteration 204/1000 | Loss: 0.00002759
Iteration 205/1000 | Loss: 0.00002759
Iteration 206/1000 | Loss: 0.00002759
Iteration 207/1000 | Loss: 0.00002759
Iteration 208/1000 | Loss: 0.00002759
Iteration 209/1000 | Loss: 0.00002759
Iteration 210/1000 | Loss: 0.00002759
Iteration 211/1000 | Loss: 0.00002759
Iteration 212/1000 | Loss: 0.00002759
Iteration 213/1000 | Loss: 0.00002759
Iteration 214/1000 | Loss: 0.00002759
Iteration 215/1000 | Loss: 0.00002759
Iteration 216/1000 | Loss: 0.00002759
Iteration 217/1000 | Loss: 0.00002759
Iteration 218/1000 | Loss: 0.00002759
Iteration 219/1000 | Loss: 0.00002759
Iteration 220/1000 | Loss: 0.00002759
Iteration 221/1000 | Loss: 0.00002759
Iteration 222/1000 | Loss: 0.00002759
Iteration 223/1000 | Loss: 0.00002758
Iteration 224/1000 | Loss: 0.00002758
Iteration 225/1000 | Loss: 0.00002758
Iteration 226/1000 | Loss: 0.00002758
Iteration 227/1000 | Loss: 0.00002758
Iteration 228/1000 | Loss: 0.00002758
Iteration 229/1000 | Loss: 0.00002758
Iteration 230/1000 | Loss: 0.00002758
Iteration 231/1000 | Loss: 0.00002758
Iteration 232/1000 | Loss: 0.00002758
Iteration 233/1000 | Loss: 0.00002758
Iteration 234/1000 | Loss: 0.00002758
Iteration 235/1000 | Loss: 0.00002758
Iteration 236/1000 | Loss: 0.00002758
Iteration 237/1000 | Loss: 0.00002758
Iteration 238/1000 | Loss: 0.00002758
Iteration 239/1000 | Loss: 0.00002758
Iteration 240/1000 | Loss: 0.00002758
Iteration 241/1000 | Loss: 0.00002758
Iteration 242/1000 | Loss: 0.00002758
Iteration 243/1000 | Loss: 0.00002758
Iteration 244/1000 | Loss: 0.00002758
Iteration 245/1000 | Loss: 0.00002758
Iteration 246/1000 | Loss: 0.00002758
Iteration 247/1000 | Loss: 0.00002758
Iteration 248/1000 | Loss: 0.00002758
Iteration 249/1000 | Loss: 0.00002758
Iteration 250/1000 | Loss: 0.00002758
Iteration 251/1000 | Loss: 0.00002758
Iteration 252/1000 | Loss: 0.00002758
Iteration 253/1000 | Loss: 0.00002758
Iteration 254/1000 | Loss: 0.00002758
Iteration 255/1000 | Loss: 0.00002758
Iteration 256/1000 | Loss: 0.00002758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [2.7580055757425725e-05, 2.7580055757425725e-05, 2.7580055757425725e-05, 2.7580055757425725e-05, 2.7580055757425725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7580055757425725e-05

Optimization complete. Final v2v error: 4.277126312255859 mm

Highest mean error: 6.491662979125977 mm for frame 70

Lowest mean error: 3.5854275226593018 mm for frame 95

Saving results

Total time: 46.9507954120636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461268
Iteration 2/25 | Loss: 0.00092459
Iteration 3/25 | Loss: 0.00076046
Iteration 4/25 | Loss: 0.00071671
Iteration 5/25 | Loss: 0.00070050
Iteration 6/25 | Loss: 0.00069747
Iteration 7/25 | Loss: 0.00069704
Iteration 8/25 | Loss: 0.00069704
Iteration 9/25 | Loss: 0.00069704
Iteration 10/25 | Loss: 0.00069704
Iteration 11/25 | Loss: 0.00069704
Iteration 12/25 | Loss: 0.00069704
Iteration 13/25 | Loss: 0.00069704
Iteration 14/25 | Loss: 0.00069704
Iteration 15/25 | Loss: 0.00069704
Iteration 16/25 | Loss: 0.00069704
Iteration 17/25 | Loss: 0.00069704
Iteration 18/25 | Loss: 0.00069704
Iteration 19/25 | Loss: 0.00069704
Iteration 20/25 | Loss: 0.00069704
Iteration 21/25 | Loss: 0.00069704
Iteration 22/25 | Loss: 0.00069704
Iteration 23/25 | Loss: 0.00069704
Iteration 24/25 | Loss: 0.00069704
Iteration 25/25 | Loss: 0.00069704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50957668
Iteration 2/25 | Loss: 0.00038523
Iteration 3/25 | Loss: 0.00038523
Iteration 4/25 | Loss: 0.00038523
Iteration 5/25 | Loss: 0.00038523
Iteration 6/25 | Loss: 0.00038523
Iteration 7/25 | Loss: 0.00038523
Iteration 8/25 | Loss: 0.00038523
Iteration 9/25 | Loss: 0.00038523
Iteration 10/25 | Loss: 0.00038523
Iteration 11/25 | Loss: 0.00038523
Iteration 12/25 | Loss: 0.00038523
Iteration 13/25 | Loss: 0.00038523
Iteration 14/25 | Loss: 0.00038523
Iteration 15/25 | Loss: 0.00038523
Iteration 16/25 | Loss: 0.00038523
Iteration 17/25 | Loss: 0.00038523
Iteration 18/25 | Loss: 0.00038523
Iteration 19/25 | Loss: 0.00038523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003852279915008694, 0.0003852279915008694, 0.0003852279915008694, 0.0003852279915008694, 0.0003852279915008694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003852279915008694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038523
Iteration 2/1000 | Loss: 0.00007085
Iteration 3/1000 | Loss: 0.00004280
Iteration 4/1000 | Loss: 0.00003888
Iteration 5/1000 | Loss: 0.00003592
Iteration 6/1000 | Loss: 0.00003469
Iteration 7/1000 | Loss: 0.00003359
Iteration 8/1000 | Loss: 0.00003280
Iteration 9/1000 | Loss: 0.00003237
Iteration 10/1000 | Loss: 0.00003196
Iteration 11/1000 | Loss: 0.00003171
Iteration 12/1000 | Loss: 0.00003156
Iteration 13/1000 | Loss: 0.00003148
Iteration 14/1000 | Loss: 0.00003148
Iteration 15/1000 | Loss: 0.00003144
Iteration 16/1000 | Loss: 0.00003144
Iteration 17/1000 | Loss: 0.00003142
Iteration 18/1000 | Loss: 0.00003142
Iteration 19/1000 | Loss: 0.00003141
Iteration 20/1000 | Loss: 0.00003139
Iteration 21/1000 | Loss: 0.00003138
Iteration 22/1000 | Loss: 0.00003137
Iteration 23/1000 | Loss: 0.00003137
Iteration 24/1000 | Loss: 0.00003137
Iteration 25/1000 | Loss: 0.00003131
Iteration 26/1000 | Loss: 0.00003131
Iteration 27/1000 | Loss: 0.00003126
Iteration 28/1000 | Loss: 0.00003126
Iteration 29/1000 | Loss: 0.00003122
Iteration 30/1000 | Loss: 0.00003121
Iteration 31/1000 | Loss: 0.00003120
Iteration 32/1000 | Loss: 0.00003120
Iteration 33/1000 | Loss: 0.00003119
Iteration 34/1000 | Loss: 0.00003119
Iteration 35/1000 | Loss: 0.00003117
Iteration 36/1000 | Loss: 0.00003117
Iteration 37/1000 | Loss: 0.00003117
Iteration 38/1000 | Loss: 0.00003116
Iteration 39/1000 | Loss: 0.00003116
Iteration 40/1000 | Loss: 0.00003116
Iteration 41/1000 | Loss: 0.00003115
Iteration 42/1000 | Loss: 0.00003115
Iteration 43/1000 | Loss: 0.00003114
Iteration 44/1000 | Loss: 0.00003114
Iteration 45/1000 | Loss: 0.00003113
Iteration 46/1000 | Loss: 0.00003113
Iteration 47/1000 | Loss: 0.00003113
Iteration 48/1000 | Loss: 0.00003113
Iteration 49/1000 | Loss: 0.00003112
Iteration 50/1000 | Loss: 0.00003112
Iteration 51/1000 | Loss: 0.00003112
Iteration 52/1000 | Loss: 0.00003112
Iteration 53/1000 | Loss: 0.00003111
Iteration 54/1000 | Loss: 0.00003111
Iteration 55/1000 | Loss: 0.00003111
Iteration 56/1000 | Loss: 0.00003111
Iteration 57/1000 | Loss: 0.00003111
Iteration 58/1000 | Loss: 0.00003111
Iteration 59/1000 | Loss: 0.00003109
Iteration 60/1000 | Loss: 0.00003109
Iteration 61/1000 | Loss: 0.00003109
Iteration 62/1000 | Loss: 0.00003108
Iteration 63/1000 | Loss: 0.00003108
Iteration 64/1000 | Loss: 0.00003108
Iteration 65/1000 | Loss: 0.00003108
Iteration 66/1000 | Loss: 0.00003108
Iteration 67/1000 | Loss: 0.00003108
Iteration 68/1000 | Loss: 0.00003108
Iteration 69/1000 | Loss: 0.00003108
Iteration 70/1000 | Loss: 0.00003108
Iteration 71/1000 | Loss: 0.00003108
Iteration 72/1000 | Loss: 0.00003108
Iteration 73/1000 | Loss: 0.00003107
Iteration 74/1000 | Loss: 0.00003107
Iteration 75/1000 | Loss: 0.00003107
Iteration 76/1000 | Loss: 0.00003107
Iteration 77/1000 | Loss: 0.00003107
Iteration 78/1000 | Loss: 0.00003107
Iteration 79/1000 | Loss: 0.00003107
Iteration 80/1000 | Loss: 0.00003106
Iteration 81/1000 | Loss: 0.00003106
Iteration 82/1000 | Loss: 0.00003106
Iteration 83/1000 | Loss: 0.00003106
Iteration 84/1000 | Loss: 0.00003106
Iteration 85/1000 | Loss: 0.00003106
Iteration 86/1000 | Loss: 0.00003106
Iteration 87/1000 | Loss: 0.00003106
Iteration 88/1000 | Loss: 0.00003105
Iteration 89/1000 | Loss: 0.00003105
Iteration 90/1000 | Loss: 0.00003105
Iteration 91/1000 | Loss: 0.00003105
Iteration 92/1000 | Loss: 0.00003105
Iteration 93/1000 | Loss: 0.00003105
Iteration 94/1000 | Loss: 0.00003105
Iteration 95/1000 | Loss: 0.00003104
Iteration 96/1000 | Loss: 0.00003104
Iteration 97/1000 | Loss: 0.00003104
Iteration 98/1000 | Loss: 0.00003104
Iteration 99/1000 | Loss: 0.00003104
Iteration 100/1000 | Loss: 0.00003104
Iteration 101/1000 | Loss: 0.00003104
Iteration 102/1000 | Loss: 0.00003104
Iteration 103/1000 | Loss: 0.00003104
Iteration 104/1000 | Loss: 0.00003104
Iteration 105/1000 | Loss: 0.00003104
Iteration 106/1000 | Loss: 0.00003104
Iteration 107/1000 | Loss: 0.00003104
Iteration 108/1000 | Loss: 0.00003104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [3.104198913206346e-05, 3.104198913206346e-05, 3.104198913206346e-05, 3.104198913206346e-05, 3.104198913206346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.104198913206346e-05

Optimization complete. Final v2v error: 4.6168060302734375 mm

Highest mean error: 4.85324239730835 mm for frame 105

Lowest mean error: 4.191107749938965 mm for frame 0

Saving results

Total time: 38.112802267074585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357216
Iteration 2/25 | Loss: 0.00071722
Iteration 3/25 | Loss: 0.00055304
Iteration 4/25 | Loss: 0.00053257
Iteration 5/25 | Loss: 0.00052764
Iteration 6/25 | Loss: 0.00052659
Iteration 7/25 | Loss: 0.00052645
Iteration 8/25 | Loss: 0.00052645
Iteration 9/25 | Loss: 0.00052645
Iteration 10/25 | Loss: 0.00052645
Iteration 11/25 | Loss: 0.00052645
Iteration 12/25 | Loss: 0.00052645
Iteration 13/25 | Loss: 0.00052645
Iteration 14/25 | Loss: 0.00052645
Iteration 15/25 | Loss: 0.00052645
Iteration 16/25 | Loss: 0.00052645
Iteration 17/25 | Loss: 0.00052645
Iteration 18/25 | Loss: 0.00052645
Iteration 19/25 | Loss: 0.00052645
Iteration 20/25 | Loss: 0.00052645
Iteration 21/25 | Loss: 0.00052645
Iteration 22/25 | Loss: 0.00052645
Iteration 23/25 | Loss: 0.00052645
Iteration 24/25 | Loss: 0.00052645
Iteration 25/25 | Loss: 0.00052645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92501104
Iteration 2/25 | Loss: 0.00035430
Iteration 3/25 | Loss: 0.00035430
Iteration 4/25 | Loss: 0.00035430
Iteration 5/25 | Loss: 0.00035430
Iteration 6/25 | Loss: 0.00035430
Iteration 7/25 | Loss: 0.00035430
Iteration 8/25 | Loss: 0.00035430
Iteration 9/25 | Loss: 0.00035430
Iteration 10/25 | Loss: 0.00035430
Iteration 11/25 | Loss: 0.00035430
Iteration 12/25 | Loss: 0.00035430
Iteration 13/25 | Loss: 0.00035430
Iteration 14/25 | Loss: 0.00035430
Iteration 15/25 | Loss: 0.00035430
Iteration 16/25 | Loss: 0.00035430
Iteration 17/25 | Loss: 0.00035430
Iteration 18/25 | Loss: 0.00035430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003542975755408406, 0.0003542975755408406, 0.0003542975755408406, 0.0003542975755408406, 0.0003542975755408406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003542975755408406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035430
Iteration 2/1000 | Loss: 0.00003096
Iteration 3/1000 | Loss: 0.00001379
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001085
Iteration 6/1000 | Loss: 0.00001067
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001037
Iteration 9/1000 | Loss: 0.00001031
Iteration 10/1000 | Loss: 0.00001026
Iteration 11/1000 | Loss: 0.00001023
Iteration 12/1000 | Loss: 0.00001023
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001022
Iteration 15/1000 | Loss: 0.00001022
Iteration 16/1000 | Loss: 0.00001021
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001020
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001019
Iteration 21/1000 | Loss: 0.00001019
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001019
Iteration 24/1000 | Loss: 0.00001018
Iteration 25/1000 | Loss: 0.00001018
Iteration 26/1000 | Loss: 0.00001017
Iteration 27/1000 | Loss: 0.00001012
Iteration 28/1000 | Loss: 0.00001011
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001009
Iteration 32/1000 | Loss: 0.00001008
Iteration 33/1000 | Loss: 0.00001008
Iteration 34/1000 | Loss: 0.00001008
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001007
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001006
Iteration 40/1000 | Loss: 0.00001006
Iteration 41/1000 | Loss: 0.00001006
Iteration 42/1000 | Loss: 0.00001005
Iteration 43/1000 | Loss: 0.00001005
Iteration 44/1000 | Loss: 0.00001005
Iteration 45/1000 | Loss: 0.00001005
Iteration 46/1000 | Loss: 0.00001005
Iteration 47/1000 | Loss: 0.00001005
Iteration 48/1000 | Loss: 0.00001005
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001004
Iteration 51/1000 | Loss: 0.00001004
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001004
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001004
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001004
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001003
Iteration 63/1000 | Loss: 0.00001003
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001003
Iteration 66/1000 | Loss: 0.00001003
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001002
Iteration 69/1000 | Loss: 0.00001002
Iteration 70/1000 | Loss: 0.00001002
Iteration 71/1000 | Loss: 0.00001002
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001002
Iteration 74/1000 | Loss: 0.00001002
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001001
Iteration 78/1000 | Loss: 0.00001001
Iteration 79/1000 | Loss: 0.00001001
Iteration 80/1000 | Loss: 0.00001001
Iteration 81/1000 | Loss: 0.00001001
Iteration 82/1000 | Loss: 0.00001001
Iteration 83/1000 | Loss: 0.00001001
Iteration 84/1000 | Loss: 0.00001001
Iteration 85/1000 | Loss: 0.00001001
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00001000
Iteration 92/1000 | Loss: 0.00001000
Iteration 93/1000 | Loss: 0.00001000
Iteration 94/1000 | Loss: 0.00001000
Iteration 95/1000 | Loss: 0.00001000
Iteration 96/1000 | Loss: 0.00001000
Iteration 97/1000 | Loss: 0.00001000
Iteration 98/1000 | Loss: 0.00001000
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000999
Iteration 101/1000 | Loss: 0.00000999
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000999
Iteration 104/1000 | Loss: 0.00000998
Iteration 105/1000 | Loss: 0.00000998
Iteration 106/1000 | Loss: 0.00000998
Iteration 107/1000 | Loss: 0.00000998
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000996
Iteration 115/1000 | Loss: 0.00000996
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000996
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000995
Iteration 137/1000 | Loss: 0.00000995
Iteration 138/1000 | Loss: 0.00000995
Iteration 139/1000 | Loss: 0.00000995
Iteration 140/1000 | Loss: 0.00000995
Iteration 141/1000 | Loss: 0.00000995
Iteration 142/1000 | Loss: 0.00000995
Iteration 143/1000 | Loss: 0.00000995
Iteration 144/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [9.954770575859584e-06, 9.954770575859584e-06, 9.954770575859584e-06, 9.954770575859584e-06, 9.954770575859584e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.954770575859584e-06

Optimization complete. Final v2v error: 2.741743326187134 mm

Highest mean error: 3.301187515258789 mm for frame 74

Lowest mean error: 2.4825661182403564 mm for frame 190

Saving results

Total time: 32.409300565719604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_27_us_1570/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_27_us_1570/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870068
Iteration 2/25 | Loss: 0.00159225
Iteration 3/25 | Loss: 0.00099134
Iteration 4/25 | Loss: 0.00096170
Iteration 5/25 | Loss: 0.00095450
Iteration 6/25 | Loss: 0.00095223
Iteration 7/25 | Loss: 0.00095208
Iteration 8/25 | Loss: 0.00095208
Iteration 9/25 | Loss: 0.00095208
Iteration 10/25 | Loss: 0.00095208
Iteration 11/25 | Loss: 0.00095208
Iteration 12/25 | Loss: 0.00095208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009520830935798585, 0.0009520830935798585, 0.0009520830935798585, 0.0009520830935798585, 0.0009520830935798585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009520830935798585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.38367274
Iteration 2/25 | Loss: 0.00031581
Iteration 3/25 | Loss: 0.00031581
Iteration 4/25 | Loss: 0.00031581
Iteration 5/25 | Loss: 0.00031580
Iteration 6/25 | Loss: 0.00031580
Iteration 7/25 | Loss: 0.00031580
Iteration 8/25 | Loss: 0.00031580
Iteration 9/25 | Loss: 0.00031580
Iteration 10/25 | Loss: 0.00031580
Iteration 11/25 | Loss: 0.00031580
Iteration 12/25 | Loss: 0.00031580
Iteration 13/25 | Loss: 0.00031580
Iteration 14/25 | Loss: 0.00031580
Iteration 15/25 | Loss: 0.00031580
Iteration 16/25 | Loss: 0.00031580
Iteration 17/25 | Loss: 0.00031580
Iteration 18/25 | Loss: 0.00031580
Iteration 19/25 | Loss: 0.00031580
Iteration 20/25 | Loss: 0.00031580
Iteration 21/25 | Loss: 0.00031580
Iteration 22/25 | Loss: 0.00031580
Iteration 23/25 | Loss: 0.00031580
Iteration 24/25 | Loss: 0.00031580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00031580289942212403, 0.00031580289942212403, 0.00031580289942212403, 0.00031580289942212403, 0.00031580289942212403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031580289942212403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031580
Iteration 2/1000 | Loss: 0.00007409
Iteration 3/1000 | Loss: 0.00006120
Iteration 4/1000 | Loss: 0.00005468
Iteration 5/1000 | Loss: 0.00005229
Iteration 6/1000 | Loss: 0.00005090
Iteration 7/1000 | Loss: 0.00004962
Iteration 8/1000 | Loss: 0.00004865
Iteration 9/1000 | Loss: 0.00004811
Iteration 10/1000 | Loss: 0.00004777
Iteration 11/1000 | Loss: 0.00004736
Iteration 12/1000 | Loss: 0.00004677
Iteration 13/1000 | Loss: 0.00004629
Iteration 14/1000 | Loss: 0.00004603
Iteration 15/1000 | Loss: 0.00004583
Iteration 16/1000 | Loss: 0.00004562
Iteration 17/1000 | Loss: 0.00004545
Iteration 18/1000 | Loss: 0.00004541
Iteration 19/1000 | Loss: 0.00004534
Iteration 20/1000 | Loss: 0.00004534
Iteration 21/1000 | Loss: 0.00004530
Iteration 22/1000 | Loss: 0.00004530
Iteration 23/1000 | Loss: 0.00004529
Iteration 24/1000 | Loss: 0.00004529
Iteration 25/1000 | Loss: 0.00004529
Iteration 26/1000 | Loss: 0.00004529
Iteration 27/1000 | Loss: 0.00004529
Iteration 28/1000 | Loss: 0.00004529
Iteration 29/1000 | Loss: 0.00004529
Iteration 30/1000 | Loss: 0.00004529
Iteration 31/1000 | Loss: 0.00004529
Iteration 32/1000 | Loss: 0.00004528
Iteration 33/1000 | Loss: 0.00004527
Iteration 34/1000 | Loss: 0.00004527
Iteration 35/1000 | Loss: 0.00004527
Iteration 36/1000 | Loss: 0.00004527
Iteration 37/1000 | Loss: 0.00004527
Iteration 38/1000 | Loss: 0.00004527
Iteration 39/1000 | Loss: 0.00004527
Iteration 40/1000 | Loss: 0.00004527
Iteration 41/1000 | Loss: 0.00004527
Iteration 42/1000 | Loss: 0.00004527
Iteration 43/1000 | Loss: 0.00004527
Iteration 44/1000 | Loss: 0.00004527
Iteration 45/1000 | Loss: 0.00004526
Iteration 46/1000 | Loss: 0.00004525
Iteration 47/1000 | Loss: 0.00004524
Iteration 48/1000 | Loss: 0.00004524
Iteration 49/1000 | Loss: 0.00004523
Iteration 50/1000 | Loss: 0.00004523
Iteration 51/1000 | Loss: 0.00004523
Iteration 52/1000 | Loss: 0.00004523
Iteration 53/1000 | Loss: 0.00004523
Iteration 54/1000 | Loss: 0.00004522
Iteration 55/1000 | Loss: 0.00004522
Iteration 56/1000 | Loss: 0.00004522
Iteration 57/1000 | Loss: 0.00004522
Iteration 58/1000 | Loss: 0.00004522
Iteration 59/1000 | Loss: 0.00004522
Iteration 60/1000 | Loss: 0.00004522
Iteration 61/1000 | Loss: 0.00004522
Iteration 62/1000 | Loss: 0.00004521
Iteration 63/1000 | Loss: 0.00004521
Iteration 64/1000 | Loss: 0.00004521
Iteration 65/1000 | Loss: 0.00004521
Iteration 66/1000 | Loss: 0.00004521
Iteration 67/1000 | Loss: 0.00004521
Iteration 68/1000 | Loss: 0.00004521
Iteration 69/1000 | Loss: 0.00004521
Iteration 70/1000 | Loss: 0.00004521
Iteration 71/1000 | Loss: 0.00004521
Iteration 72/1000 | Loss: 0.00004520
Iteration 73/1000 | Loss: 0.00004520
Iteration 74/1000 | Loss: 0.00004520
Iteration 75/1000 | Loss: 0.00004520
Iteration 76/1000 | Loss: 0.00004520
Iteration 77/1000 | Loss: 0.00004520
Iteration 78/1000 | Loss: 0.00004520
Iteration 79/1000 | Loss: 0.00004520
Iteration 80/1000 | Loss: 0.00004520
Iteration 81/1000 | Loss: 0.00004520
Iteration 82/1000 | Loss: 0.00004520
Iteration 83/1000 | Loss: 0.00004520
Iteration 84/1000 | Loss: 0.00004520
Iteration 85/1000 | Loss: 0.00004520
Iteration 86/1000 | Loss: 0.00004519
Iteration 87/1000 | Loss: 0.00004519
Iteration 88/1000 | Loss: 0.00004519
Iteration 89/1000 | Loss: 0.00004519
Iteration 90/1000 | Loss: 0.00004519
Iteration 91/1000 | Loss: 0.00004519
Iteration 92/1000 | Loss: 0.00004519
Iteration 93/1000 | Loss: 0.00004519
Iteration 94/1000 | Loss: 0.00004519
Iteration 95/1000 | Loss: 0.00004519
Iteration 96/1000 | Loss: 0.00004518
Iteration 97/1000 | Loss: 0.00004518
Iteration 98/1000 | Loss: 0.00004518
Iteration 99/1000 | Loss: 0.00004518
Iteration 100/1000 | Loss: 0.00004518
Iteration 101/1000 | Loss: 0.00004517
Iteration 102/1000 | Loss: 0.00004517
Iteration 103/1000 | Loss: 0.00004517
Iteration 104/1000 | Loss: 0.00004517
Iteration 105/1000 | Loss: 0.00004517
Iteration 106/1000 | Loss: 0.00004517
Iteration 107/1000 | Loss: 0.00004517
Iteration 108/1000 | Loss: 0.00004517
Iteration 109/1000 | Loss: 0.00004517
Iteration 110/1000 | Loss: 0.00004517
Iteration 111/1000 | Loss: 0.00004517
Iteration 112/1000 | Loss: 0.00004516
Iteration 113/1000 | Loss: 0.00004516
Iteration 114/1000 | Loss: 0.00004516
Iteration 115/1000 | Loss: 0.00004516
Iteration 116/1000 | Loss: 0.00004516
Iteration 117/1000 | Loss: 0.00004516
Iteration 118/1000 | Loss: 0.00004516
Iteration 119/1000 | Loss: 0.00004516
Iteration 120/1000 | Loss: 0.00004516
Iteration 121/1000 | Loss: 0.00004516
Iteration 122/1000 | Loss: 0.00004516
Iteration 123/1000 | Loss: 0.00004516
Iteration 124/1000 | Loss: 0.00004515
Iteration 125/1000 | Loss: 0.00004515
Iteration 126/1000 | Loss: 0.00004515
Iteration 127/1000 | Loss: 0.00004515
Iteration 128/1000 | Loss: 0.00004514
Iteration 129/1000 | Loss: 0.00004514
Iteration 130/1000 | Loss: 0.00004514
Iteration 131/1000 | Loss: 0.00004514
Iteration 132/1000 | Loss: 0.00004513
Iteration 133/1000 | Loss: 0.00004513
Iteration 134/1000 | Loss: 0.00004513
Iteration 135/1000 | Loss: 0.00004513
Iteration 136/1000 | Loss: 0.00004512
Iteration 137/1000 | Loss: 0.00004512
Iteration 138/1000 | Loss: 0.00004512
Iteration 139/1000 | Loss: 0.00004512
Iteration 140/1000 | Loss: 0.00004512
Iteration 141/1000 | Loss: 0.00004512
Iteration 142/1000 | Loss: 0.00004512
Iteration 143/1000 | Loss: 0.00004512
Iteration 144/1000 | Loss: 0.00004512
Iteration 145/1000 | Loss: 0.00004511
Iteration 146/1000 | Loss: 0.00004511
Iteration 147/1000 | Loss: 0.00004511
Iteration 148/1000 | Loss: 0.00004511
Iteration 149/1000 | Loss: 0.00004511
Iteration 150/1000 | Loss: 0.00004511
Iteration 151/1000 | Loss: 0.00004511
Iteration 152/1000 | Loss: 0.00004510
Iteration 153/1000 | Loss: 0.00004510
Iteration 154/1000 | Loss: 0.00004510
Iteration 155/1000 | Loss: 0.00004510
Iteration 156/1000 | Loss: 0.00004510
Iteration 157/1000 | Loss: 0.00004510
Iteration 158/1000 | Loss: 0.00004510
Iteration 159/1000 | Loss: 0.00004509
Iteration 160/1000 | Loss: 0.00004509
Iteration 161/1000 | Loss: 0.00004509
Iteration 162/1000 | Loss: 0.00004509
Iteration 163/1000 | Loss: 0.00004509
Iteration 164/1000 | Loss: 0.00004509
Iteration 165/1000 | Loss: 0.00004508
Iteration 166/1000 | Loss: 0.00004508
Iteration 167/1000 | Loss: 0.00004508
Iteration 168/1000 | Loss: 0.00004508
Iteration 169/1000 | Loss: 0.00004507
Iteration 170/1000 | Loss: 0.00004506
Iteration 171/1000 | Loss: 0.00004506
Iteration 172/1000 | Loss: 0.00004506
Iteration 173/1000 | Loss: 0.00004506
Iteration 174/1000 | Loss: 0.00004506
Iteration 175/1000 | Loss: 0.00004506
Iteration 176/1000 | Loss: 0.00004506
Iteration 177/1000 | Loss: 0.00004506
Iteration 178/1000 | Loss: 0.00004506
Iteration 179/1000 | Loss: 0.00004506
Iteration 180/1000 | Loss: 0.00004506
Iteration 181/1000 | Loss: 0.00004506
Iteration 182/1000 | Loss: 0.00004505
Iteration 183/1000 | Loss: 0.00004505
Iteration 184/1000 | Loss: 0.00004505
Iteration 185/1000 | Loss: 0.00004505
Iteration 186/1000 | Loss: 0.00004504
Iteration 187/1000 | Loss: 0.00004504
Iteration 188/1000 | Loss: 0.00004504
Iteration 189/1000 | Loss: 0.00004504
Iteration 190/1000 | Loss: 0.00004504
Iteration 191/1000 | Loss: 0.00004504
Iteration 192/1000 | Loss: 0.00004504
Iteration 193/1000 | Loss: 0.00004504
Iteration 194/1000 | Loss: 0.00004504
Iteration 195/1000 | Loss: 0.00004504
Iteration 196/1000 | Loss: 0.00004504
Iteration 197/1000 | Loss: 0.00004504
Iteration 198/1000 | Loss: 0.00004504
Iteration 199/1000 | Loss: 0.00004504
Iteration 200/1000 | Loss: 0.00004503
Iteration 201/1000 | Loss: 0.00004503
Iteration 202/1000 | Loss: 0.00004503
Iteration 203/1000 | Loss: 0.00004503
Iteration 204/1000 | Loss: 0.00004503
Iteration 205/1000 | Loss: 0.00004503
Iteration 206/1000 | Loss: 0.00004503
Iteration 207/1000 | Loss: 0.00004503
Iteration 208/1000 | Loss: 0.00004502
Iteration 209/1000 | Loss: 0.00004502
Iteration 210/1000 | Loss: 0.00004502
Iteration 211/1000 | Loss: 0.00004502
Iteration 212/1000 | Loss: 0.00004502
Iteration 213/1000 | Loss: 0.00004502
Iteration 214/1000 | Loss: 0.00004502
Iteration 215/1000 | Loss: 0.00004502
Iteration 216/1000 | Loss: 0.00004502
Iteration 217/1000 | Loss: 0.00004502
Iteration 218/1000 | Loss: 0.00004502
Iteration 219/1000 | Loss: 0.00004502
Iteration 220/1000 | Loss: 0.00004502
Iteration 221/1000 | Loss: 0.00004502
Iteration 222/1000 | Loss: 0.00004502
Iteration 223/1000 | Loss: 0.00004502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [4.502241426962428e-05, 4.502241426962428e-05, 4.502241426962428e-05, 4.502241426962428e-05, 4.502241426962428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.502241426962428e-05

Optimization complete. Final v2v error: 5.153714656829834 mm

Highest mean error: 5.286842346191406 mm for frame 25

Lowest mean error: 4.942091941833496 mm for frame 131

Saving results

Total time: 47.750405073165894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900008
Iteration 2/25 | Loss: 0.00257973
Iteration 3/25 | Loss: 0.00194448
Iteration 4/25 | Loss: 0.00184014
Iteration 5/25 | Loss: 0.00179683
Iteration 6/25 | Loss: 0.00175269
Iteration 7/25 | Loss: 0.00166461
Iteration 8/25 | Loss: 0.00161998
Iteration 9/25 | Loss: 0.00159850
Iteration 10/25 | Loss: 0.00158110
Iteration 11/25 | Loss: 0.00155444
Iteration 12/25 | Loss: 0.00154906
Iteration 13/25 | Loss: 0.00154150
Iteration 14/25 | Loss: 0.00152850
Iteration 15/25 | Loss: 0.00152246
Iteration 16/25 | Loss: 0.00151681
Iteration 17/25 | Loss: 0.00151509
Iteration 18/25 | Loss: 0.00151361
Iteration 19/25 | Loss: 0.00151080
Iteration 20/25 | Loss: 0.00151399
Iteration 21/25 | Loss: 0.00151262
Iteration 22/25 | Loss: 0.00150736
Iteration 23/25 | Loss: 0.00150684
Iteration 24/25 | Loss: 0.00150593
Iteration 25/25 | Loss: 0.00150575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81703162
Iteration 2/25 | Loss: 0.00305785
Iteration 3/25 | Loss: 0.00299144
Iteration 4/25 | Loss: 0.00299120
Iteration 5/25 | Loss: 0.00299120
Iteration 6/25 | Loss: 0.00299119
Iteration 7/25 | Loss: 0.00299119
Iteration 8/25 | Loss: 0.00299119
Iteration 9/25 | Loss: 0.00299119
Iteration 10/25 | Loss: 0.00299119
Iteration 11/25 | Loss: 0.00299119
Iteration 12/25 | Loss: 0.00299119
Iteration 13/25 | Loss: 0.00299119
Iteration 14/25 | Loss: 0.00299119
Iteration 15/25 | Loss: 0.00299119
Iteration 16/25 | Loss: 0.00299119
Iteration 17/25 | Loss: 0.00299119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002991192974150181, 0.002991192974150181, 0.002991192974150181, 0.002991192974150181, 0.002991192974150181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002991192974150181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00299119
Iteration 2/1000 | Loss: 0.00031635
Iteration 3/1000 | Loss: 0.00026144
Iteration 4/1000 | Loss: 0.00026385
Iteration 5/1000 | Loss: 0.00033275
Iteration 6/1000 | Loss: 0.00013196
Iteration 7/1000 | Loss: 0.00018518
Iteration 8/1000 | Loss: 0.00011527
Iteration 9/1000 | Loss: 0.00010206
Iteration 10/1000 | Loss: 0.00007706
Iteration 11/1000 | Loss: 0.00007094
Iteration 12/1000 | Loss: 0.00007423
Iteration 13/1000 | Loss: 0.00006958
Iteration 14/1000 | Loss: 0.00030348
Iteration 15/1000 | Loss: 0.00007215
Iteration 16/1000 | Loss: 0.00006494
Iteration 17/1000 | Loss: 0.00007651
Iteration 18/1000 | Loss: 0.00006474
Iteration 19/1000 | Loss: 0.00006348
Iteration 20/1000 | Loss: 0.00008004
Iteration 21/1000 | Loss: 0.00007591
Iteration 22/1000 | Loss: 0.00007949
Iteration 23/1000 | Loss: 0.00007259
Iteration 24/1000 | Loss: 0.00007587
Iteration 25/1000 | Loss: 0.00006232
Iteration 26/1000 | Loss: 0.00044966
Iteration 27/1000 | Loss: 0.00019350
Iteration 28/1000 | Loss: 0.00007089
Iteration 29/1000 | Loss: 0.00006205
Iteration 30/1000 | Loss: 0.00007040
Iteration 31/1000 | Loss: 0.00006747
Iteration 32/1000 | Loss: 0.00006878
Iteration 33/1000 | Loss: 0.00006783
Iteration 34/1000 | Loss: 0.00006831
Iteration 35/1000 | Loss: 0.00050904
Iteration 36/1000 | Loss: 0.00017859
Iteration 37/1000 | Loss: 0.00007123
Iteration 38/1000 | Loss: 0.00040259
Iteration 39/1000 | Loss: 0.00010915
Iteration 40/1000 | Loss: 0.00006710
Iteration 41/1000 | Loss: 0.00006648
Iteration 42/1000 | Loss: 0.00007176
Iteration 43/1000 | Loss: 0.00006526
Iteration 44/1000 | Loss: 0.00006979
Iteration 45/1000 | Loss: 0.00006666
Iteration 46/1000 | Loss: 0.00007709
Iteration 47/1000 | Loss: 0.00006918
Iteration 48/1000 | Loss: 0.00007345
Iteration 49/1000 | Loss: 0.00007015
Iteration 50/1000 | Loss: 0.00007199
Iteration 51/1000 | Loss: 0.00007014
Iteration 52/1000 | Loss: 0.00007093
Iteration 53/1000 | Loss: 0.00006783
Iteration 54/1000 | Loss: 0.00006356
Iteration 55/1000 | Loss: 0.00006721
Iteration 56/1000 | Loss: 0.00007638
Iteration 57/1000 | Loss: 0.00005700
Iteration 58/1000 | Loss: 0.00005506
Iteration 59/1000 | Loss: 0.00005431
Iteration 60/1000 | Loss: 0.00005411
Iteration 61/1000 | Loss: 0.00005405
Iteration 62/1000 | Loss: 0.00005392
Iteration 63/1000 | Loss: 0.00005378
Iteration 64/1000 | Loss: 0.00005377
Iteration 65/1000 | Loss: 0.00005360
Iteration 66/1000 | Loss: 0.00005353
Iteration 67/1000 | Loss: 0.00005351
Iteration 68/1000 | Loss: 0.00005331
Iteration 69/1000 | Loss: 0.00005308
Iteration 70/1000 | Loss: 0.00028121
Iteration 71/1000 | Loss: 0.00049598
Iteration 72/1000 | Loss: 0.00107398
Iteration 73/1000 | Loss: 0.00022870
Iteration 74/1000 | Loss: 0.00014930
Iteration 75/1000 | Loss: 0.00006911
Iteration 76/1000 | Loss: 0.00005611
Iteration 77/1000 | Loss: 0.00005428
Iteration 78/1000 | Loss: 0.00005313
Iteration 79/1000 | Loss: 0.00005230
Iteration 80/1000 | Loss: 0.00005160
Iteration 81/1000 | Loss: 0.00005125
Iteration 82/1000 | Loss: 0.00005097
Iteration 83/1000 | Loss: 0.00005088
Iteration 84/1000 | Loss: 0.00005066
Iteration 85/1000 | Loss: 0.00023409
Iteration 86/1000 | Loss: 0.00005550
Iteration 87/1000 | Loss: 0.00005032
Iteration 88/1000 | Loss: 0.00004940
Iteration 89/1000 | Loss: 0.00004897
Iteration 90/1000 | Loss: 0.00004882
Iteration 91/1000 | Loss: 0.00004875
Iteration 92/1000 | Loss: 0.00004859
Iteration 93/1000 | Loss: 0.00004840
Iteration 94/1000 | Loss: 0.00004826
Iteration 95/1000 | Loss: 0.00025679
Iteration 96/1000 | Loss: 0.00010296
Iteration 97/1000 | Loss: 0.00006602
Iteration 98/1000 | Loss: 0.00005290
Iteration 99/1000 | Loss: 0.00004906
Iteration 100/1000 | Loss: 0.00004844
Iteration 101/1000 | Loss: 0.00004818
Iteration 102/1000 | Loss: 0.00043848
Iteration 103/1000 | Loss: 0.00017350
Iteration 104/1000 | Loss: 0.00005863
Iteration 105/1000 | Loss: 0.00005032
Iteration 106/1000 | Loss: 0.00004917
Iteration 107/1000 | Loss: 0.00004865
Iteration 108/1000 | Loss: 0.00004828
Iteration 109/1000 | Loss: 0.00004813
Iteration 110/1000 | Loss: 0.00004812
Iteration 111/1000 | Loss: 0.00044437
Iteration 112/1000 | Loss: 0.00032612
Iteration 113/1000 | Loss: 0.00004991
Iteration 114/1000 | Loss: 0.00004622
Iteration 115/1000 | Loss: 0.00004470
Iteration 116/1000 | Loss: 0.00004416
Iteration 117/1000 | Loss: 0.00004375
Iteration 118/1000 | Loss: 0.00004344
Iteration 119/1000 | Loss: 0.00004313
Iteration 120/1000 | Loss: 0.00004298
Iteration 121/1000 | Loss: 0.00004293
Iteration 122/1000 | Loss: 0.00004282
Iteration 123/1000 | Loss: 0.00004279
Iteration 124/1000 | Loss: 0.00004277
Iteration 125/1000 | Loss: 0.00004276
Iteration 126/1000 | Loss: 0.00004275
Iteration 127/1000 | Loss: 0.00004272
Iteration 128/1000 | Loss: 0.00004272
Iteration 129/1000 | Loss: 0.00004264
Iteration 130/1000 | Loss: 0.00004261
Iteration 131/1000 | Loss: 0.00004260
Iteration 132/1000 | Loss: 0.00004260
Iteration 133/1000 | Loss: 0.00004259
Iteration 134/1000 | Loss: 0.00004259
Iteration 135/1000 | Loss: 0.00004259
Iteration 136/1000 | Loss: 0.00004258
Iteration 137/1000 | Loss: 0.00004258
Iteration 138/1000 | Loss: 0.00004258
Iteration 139/1000 | Loss: 0.00004257
Iteration 140/1000 | Loss: 0.00004256
Iteration 141/1000 | Loss: 0.00004256
Iteration 142/1000 | Loss: 0.00004256
Iteration 143/1000 | Loss: 0.00004255
Iteration 144/1000 | Loss: 0.00004255
Iteration 145/1000 | Loss: 0.00004255
Iteration 146/1000 | Loss: 0.00004255
Iteration 147/1000 | Loss: 0.00004254
Iteration 148/1000 | Loss: 0.00004254
Iteration 149/1000 | Loss: 0.00004252
Iteration 150/1000 | Loss: 0.00004252
Iteration 151/1000 | Loss: 0.00004252
Iteration 152/1000 | Loss: 0.00004252
Iteration 153/1000 | Loss: 0.00004252
Iteration 154/1000 | Loss: 0.00004252
Iteration 155/1000 | Loss: 0.00004252
Iteration 156/1000 | Loss: 0.00004252
Iteration 157/1000 | Loss: 0.00004251
Iteration 158/1000 | Loss: 0.00004251
Iteration 159/1000 | Loss: 0.00004251
Iteration 160/1000 | Loss: 0.00004251
Iteration 161/1000 | Loss: 0.00004251
Iteration 162/1000 | Loss: 0.00004251
Iteration 163/1000 | Loss: 0.00004251
Iteration 164/1000 | Loss: 0.00004251
Iteration 165/1000 | Loss: 0.00004251
Iteration 166/1000 | Loss: 0.00004251
Iteration 167/1000 | Loss: 0.00004251
Iteration 168/1000 | Loss: 0.00004250
Iteration 169/1000 | Loss: 0.00004250
Iteration 170/1000 | Loss: 0.00004250
Iteration 171/1000 | Loss: 0.00004250
Iteration 172/1000 | Loss: 0.00004250
Iteration 173/1000 | Loss: 0.00004249
Iteration 174/1000 | Loss: 0.00004248
Iteration 175/1000 | Loss: 0.00004248
Iteration 176/1000 | Loss: 0.00004248
Iteration 177/1000 | Loss: 0.00004248
Iteration 178/1000 | Loss: 0.00004248
Iteration 179/1000 | Loss: 0.00004247
Iteration 180/1000 | Loss: 0.00004247
Iteration 181/1000 | Loss: 0.00004247
Iteration 182/1000 | Loss: 0.00004246
Iteration 183/1000 | Loss: 0.00004246
Iteration 184/1000 | Loss: 0.00004246
Iteration 185/1000 | Loss: 0.00004245
Iteration 186/1000 | Loss: 0.00004245
Iteration 187/1000 | Loss: 0.00004244
Iteration 188/1000 | Loss: 0.00004244
Iteration 189/1000 | Loss: 0.00004244
Iteration 190/1000 | Loss: 0.00004243
Iteration 191/1000 | Loss: 0.00004243
Iteration 192/1000 | Loss: 0.00004242
Iteration 193/1000 | Loss: 0.00004242
Iteration 194/1000 | Loss: 0.00004242
Iteration 195/1000 | Loss: 0.00004242
Iteration 196/1000 | Loss: 0.00004242
Iteration 197/1000 | Loss: 0.00004242
Iteration 198/1000 | Loss: 0.00004242
Iteration 199/1000 | Loss: 0.00004242
Iteration 200/1000 | Loss: 0.00004242
Iteration 201/1000 | Loss: 0.00004242
Iteration 202/1000 | Loss: 0.00004241
Iteration 203/1000 | Loss: 0.00004241
Iteration 204/1000 | Loss: 0.00004241
Iteration 205/1000 | Loss: 0.00004241
Iteration 206/1000 | Loss: 0.00004241
Iteration 207/1000 | Loss: 0.00004241
Iteration 208/1000 | Loss: 0.00004240
Iteration 209/1000 | Loss: 0.00004239
Iteration 210/1000 | Loss: 0.00004239
Iteration 211/1000 | Loss: 0.00004238
Iteration 212/1000 | Loss: 0.00004238
Iteration 213/1000 | Loss: 0.00004238
Iteration 214/1000 | Loss: 0.00004237
Iteration 215/1000 | Loss: 0.00004236
Iteration 216/1000 | Loss: 0.00004236
Iteration 217/1000 | Loss: 0.00004235
Iteration 218/1000 | Loss: 0.00004235
Iteration 219/1000 | Loss: 0.00004234
Iteration 220/1000 | Loss: 0.00004234
Iteration 221/1000 | Loss: 0.00004234
Iteration 222/1000 | Loss: 0.00004233
Iteration 223/1000 | Loss: 0.00004233
Iteration 224/1000 | Loss: 0.00004233
Iteration 225/1000 | Loss: 0.00004233
Iteration 226/1000 | Loss: 0.00004233
Iteration 227/1000 | Loss: 0.00004232
Iteration 228/1000 | Loss: 0.00004232
Iteration 229/1000 | Loss: 0.00004232
Iteration 230/1000 | Loss: 0.00004231
Iteration 231/1000 | Loss: 0.00004231
Iteration 232/1000 | Loss: 0.00004231
Iteration 233/1000 | Loss: 0.00004231
Iteration 234/1000 | Loss: 0.00004231
Iteration 235/1000 | Loss: 0.00004230
Iteration 236/1000 | Loss: 0.00004230
Iteration 237/1000 | Loss: 0.00004230
Iteration 238/1000 | Loss: 0.00004230
Iteration 239/1000 | Loss: 0.00004230
Iteration 240/1000 | Loss: 0.00004230
Iteration 241/1000 | Loss: 0.00004229
Iteration 242/1000 | Loss: 0.00004229
Iteration 243/1000 | Loss: 0.00004229
Iteration 244/1000 | Loss: 0.00004228
Iteration 245/1000 | Loss: 0.00004228
Iteration 246/1000 | Loss: 0.00004228
Iteration 247/1000 | Loss: 0.00004227
Iteration 248/1000 | Loss: 0.00004227
Iteration 249/1000 | Loss: 0.00004227
Iteration 250/1000 | Loss: 0.00004225
Iteration 251/1000 | Loss: 0.00004225
Iteration 252/1000 | Loss: 0.00004224
Iteration 253/1000 | Loss: 0.00004224
Iteration 254/1000 | Loss: 0.00004224
Iteration 255/1000 | Loss: 0.00004223
Iteration 256/1000 | Loss: 0.00004223
Iteration 257/1000 | Loss: 0.00004222
Iteration 258/1000 | Loss: 0.00004221
Iteration 259/1000 | Loss: 0.00004221
Iteration 260/1000 | Loss: 0.00004221
Iteration 261/1000 | Loss: 0.00004221
Iteration 262/1000 | Loss: 0.00004220
Iteration 263/1000 | Loss: 0.00004220
Iteration 264/1000 | Loss: 0.00004220
Iteration 265/1000 | Loss: 0.00004220
Iteration 266/1000 | Loss: 0.00004220
Iteration 267/1000 | Loss: 0.00004220
Iteration 268/1000 | Loss: 0.00021756
Iteration 269/1000 | Loss: 0.00007505
Iteration 270/1000 | Loss: 0.00004425
Iteration 271/1000 | Loss: 0.00004271
Iteration 272/1000 | Loss: 0.00004235
Iteration 273/1000 | Loss: 0.00004225
Iteration 274/1000 | Loss: 0.00028019
Iteration 275/1000 | Loss: 0.00011766
Iteration 276/1000 | Loss: 0.00004277
Iteration 277/1000 | Loss: 0.00004227
Iteration 278/1000 | Loss: 0.00004221
Iteration 279/1000 | Loss: 0.00004220
Iteration 280/1000 | Loss: 0.00022375
Iteration 281/1000 | Loss: 0.00008662
Iteration 282/1000 | Loss: 0.00004272
Iteration 283/1000 | Loss: 0.00004224
Iteration 284/1000 | Loss: 0.00004217
Iteration 285/1000 | Loss: 0.00004217
Iteration 286/1000 | Loss: 0.00004217
Iteration 287/1000 | Loss: 0.00004217
Iteration 288/1000 | Loss: 0.00004215
Iteration 289/1000 | Loss: 0.00004215
Iteration 290/1000 | Loss: 0.00024167
Iteration 291/1000 | Loss: 0.00009119
Iteration 292/1000 | Loss: 0.00008460
Iteration 293/1000 | Loss: 0.00004278
Iteration 294/1000 | Loss: 0.00004248
Iteration 295/1000 | Loss: 0.00004232
Iteration 296/1000 | Loss: 0.00004225
Iteration 297/1000 | Loss: 0.00004224
Iteration 298/1000 | Loss: 0.00004223
Iteration 299/1000 | Loss: 0.00004223
Iteration 300/1000 | Loss: 0.00004223
Iteration 301/1000 | Loss: 0.00004222
Iteration 302/1000 | Loss: 0.00004222
Iteration 303/1000 | Loss: 0.00004222
Iteration 304/1000 | Loss: 0.00004222
Iteration 305/1000 | Loss: 0.00004222
Iteration 306/1000 | Loss: 0.00004222
Iteration 307/1000 | Loss: 0.00004222
Iteration 308/1000 | Loss: 0.00004222
Iteration 309/1000 | Loss: 0.00018147
Iteration 310/1000 | Loss: 0.00010213
Iteration 311/1000 | Loss: 0.00005625
Iteration 312/1000 | Loss: 0.00004238
Iteration 313/1000 | Loss: 0.00004225
Iteration 314/1000 | Loss: 0.00004224
Iteration 315/1000 | Loss: 0.00004224
Iteration 316/1000 | Loss: 0.00004224
Iteration 317/1000 | Loss: 0.00004224
Iteration 318/1000 | Loss: 0.00004224
Iteration 319/1000 | Loss: 0.00004224
Iteration 320/1000 | Loss: 0.00004224
Iteration 321/1000 | Loss: 0.00004224
Iteration 322/1000 | Loss: 0.00004223
Iteration 323/1000 | Loss: 0.00017226
Iteration 324/1000 | Loss: 0.00006527
Iteration 325/1000 | Loss: 0.00004595
Iteration 326/1000 | Loss: 0.00004277
Iteration 327/1000 | Loss: 0.00004237
Iteration 328/1000 | Loss: 0.00036310
Iteration 329/1000 | Loss: 0.00013986
Iteration 330/1000 | Loss: 0.00007288
Iteration 331/1000 | Loss: 0.00005008
Iteration 332/1000 | Loss: 0.00004551
Iteration 333/1000 | Loss: 0.00034036
Iteration 334/1000 | Loss: 0.00009367
Iteration 335/1000 | Loss: 0.00005036
Iteration 336/1000 | Loss: 0.00004744
Iteration 337/1000 | Loss: 0.00004523
Iteration 338/1000 | Loss: 0.00004379
Iteration 339/1000 | Loss: 0.00034557
Iteration 340/1000 | Loss: 0.00010037
Iteration 341/1000 | Loss: 0.00024620
Iteration 342/1000 | Loss: 0.00031263
Iteration 343/1000 | Loss: 0.00017919
Iteration 344/1000 | Loss: 0.00005330
Iteration 345/1000 | Loss: 0.00004210
Iteration 346/1000 | Loss: 0.00004092
Iteration 347/1000 | Loss: 0.00004037
Iteration 348/1000 | Loss: 0.00004008
Iteration 349/1000 | Loss: 0.00003989
Iteration 350/1000 | Loss: 0.00003986
Iteration 351/1000 | Loss: 0.00003981
Iteration 352/1000 | Loss: 0.00003968
Iteration 353/1000 | Loss: 0.00003968
Iteration 354/1000 | Loss: 0.00026226
Iteration 355/1000 | Loss: 0.00013796
Iteration 356/1000 | Loss: 0.00005276
Iteration 357/1000 | Loss: 0.00004297
Iteration 358/1000 | Loss: 0.00004034
Iteration 359/1000 | Loss: 0.00003967
Iteration 360/1000 | Loss: 0.00003962
Iteration 361/1000 | Loss: 0.00003956
Iteration 362/1000 | Loss: 0.00003940
Iteration 363/1000 | Loss: 0.00003923
Iteration 364/1000 | Loss: 0.00003919
Iteration 365/1000 | Loss: 0.00003911
Iteration 366/1000 | Loss: 0.00003911
Iteration 367/1000 | Loss: 0.00003911
Iteration 368/1000 | Loss: 0.00003910
Iteration 369/1000 | Loss: 0.00003910
Iteration 370/1000 | Loss: 0.00003910
Iteration 371/1000 | Loss: 0.00003910
Iteration 372/1000 | Loss: 0.00003910
Iteration 373/1000 | Loss: 0.00003910
Iteration 374/1000 | Loss: 0.00003910
Iteration 375/1000 | Loss: 0.00003910
Iteration 376/1000 | Loss: 0.00003910
Iteration 377/1000 | Loss: 0.00003910
Iteration 378/1000 | Loss: 0.00003909
Iteration 379/1000 | Loss: 0.00003907
Iteration 380/1000 | Loss: 0.00003907
Iteration 381/1000 | Loss: 0.00003907
Iteration 382/1000 | Loss: 0.00003907
Iteration 383/1000 | Loss: 0.00003907
Iteration 384/1000 | Loss: 0.00003906
Iteration 385/1000 | Loss: 0.00003906
Iteration 386/1000 | Loss: 0.00003906
Iteration 387/1000 | Loss: 0.00003905
Iteration 388/1000 | Loss: 0.00003905
Iteration 389/1000 | Loss: 0.00003905
Iteration 390/1000 | Loss: 0.00003905
Iteration 391/1000 | Loss: 0.00003904
Iteration 392/1000 | Loss: 0.00003904
Iteration 393/1000 | Loss: 0.00003904
Iteration 394/1000 | Loss: 0.00003904
Iteration 395/1000 | Loss: 0.00003904
Iteration 396/1000 | Loss: 0.00003902
Iteration 397/1000 | Loss: 0.00003902
Iteration 398/1000 | Loss: 0.00003901
Iteration 399/1000 | Loss: 0.00003901
Iteration 400/1000 | Loss: 0.00003900
Iteration 401/1000 | Loss: 0.00003900
Iteration 402/1000 | Loss: 0.00003900
Iteration 403/1000 | Loss: 0.00003899
Iteration 404/1000 | Loss: 0.00003899
Iteration 405/1000 | Loss: 0.00003899
Iteration 406/1000 | Loss: 0.00003899
Iteration 407/1000 | Loss: 0.00003898
Iteration 408/1000 | Loss: 0.00003898
Iteration 409/1000 | Loss: 0.00003898
Iteration 410/1000 | Loss: 0.00003898
Iteration 411/1000 | Loss: 0.00003898
Iteration 412/1000 | Loss: 0.00003898
Iteration 413/1000 | Loss: 0.00003898
Iteration 414/1000 | Loss: 0.00003898
Iteration 415/1000 | Loss: 0.00003898
Iteration 416/1000 | Loss: 0.00003898
Iteration 417/1000 | Loss: 0.00003898
Iteration 418/1000 | Loss: 0.00003898
Iteration 419/1000 | Loss: 0.00003898
Iteration 420/1000 | Loss: 0.00003897
Iteration 421/1000 | Loss: 0.00003897
Iteration 422/1000 | Loss: 0.00003897
Iteration 423/1000 | Loss: 0.00003897
Iteration 424/1000 | Loss: 0.00003897
Iteration 425/1000 | Loss: 0.00003897
Iteration 426/1000 | Loss: 0.00003897
Iteration 427/1000 | Loss: 0.00003897
Iteration 428/1000 | Loss: 0.00003897
Iteration 429/1000 | Loss: 0.00003897
Iteration 430/1000 | Loss: 0.00003897
Iteration 431/1000 | Loss: 0.00003897
Iteration 432/1000 | Loss: 0.00003897
Iteration 433/1000 | Loss: 0.00003896
Iteration 434/1000 | Loss: 0.00003896
Iteration 435/1000 | Loss: 0.00003896
Iteration 436/1000 | Loss: 0.00003896
Iteration 437/1000 | Loss: 0.00003896
Iteration 438/1000 | Loss: 0.00003896
Iteration 439/1000 | Loss: 0.00003896
Iteration 440/1000 | Loss: 0.00003896
Iteration 441/1000 | Loss: 0.00003896
Iteration 442/1000 | Loss: 0.00003896
Iteration 443/1000 | Loss: 0.00003896
Iteration 444/1000 | Loss: 0.00003896
Iteration 445/1000 | Loss: 0.00003896
Iteration 446/1000 | Loss: 0.00003896
Iteration 447/1000 | Loss: 0.00003896
Iteration 448/1000 | Loss: 0.00003896
Iteration 449/1000 | Loss: 0.00003895
Iteration 450/1000 | Loss: 0.00003895
Iteration 451/1000 | Loss: 0.00003895
Iteration 452/1000 | Loss: 0.00003895
Iteration 453/1000 | Loss: 0.00003895
Iteration 454/1000 | Loss: 0.00003895
Iteration 455/1000 | Loss: 0.00003895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 455. Stopping optimization.
Last 5 losses: [3.895479312632233e-05, 3.895479312632233e-05, 3.895479312632233e-05, 3.895479312632233e-05, 3.895479312632233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.895479312632233e-05

Optimization complete. Final v2v error: 4.523813247680664 mm

Highest mean error: 11.783108711242676 mm for frame 18

Lowest mean error: 3.5263831615448 mm for frame 88

Saving results

Total time: 361.41649556159973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592102
Iteration 2/25 | Loss: 0.00169628
Iteration 3/25 | Loss: 0.00138322
Iteration 4/25 | Loss: 0.00134936
Iteration 5/25 | Loss: 0.00134488
Iteration 6/25 | Loss: 0.00134358
Iteration 7/25 | Loss: 0.00134327
Iteration 8/25 | Loss: 0.00134327
Iteration 9/25 | Loss: 0.00134327
Iteration 10/25 | Loss: 0.00134327
Iteration 11/25 | Loss: 0.00134327
Iteration 12/25 | Loss: 0.00134327
Iteration 13/25 | Loss: 0.00134327
Iteration 14/25 | Loss: 0.00134327
Iteration 15/25 | Loss: 0.00134327
Iteration 16/25 | Loss: 0.00134327
Iteration 17/25 | Loss: 0.00134327
Iteration 18/25 | Loss: 0.00134327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013432743726298213, 0.0013432743726298213, 0.0013432743726298213, 0.0013432743726298213, 0.0013432743726298213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013432743726298213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17145395
Iteration 2/25 | Loss: 0.00092085
Iteration 3/25 | Loss: 0.00092085
Iteration 4/25 | Loss: 0.00092085
Iteration 5/25 | Loss: 0.00092085
Iteration 6/25 | Loss: 0.00092085
Iteration 7/25 | Loss: 0.00092085
Iteration 8/25 | Loss: 0.00092085
Iteration 9/25 | Loss: 0.00092085
Iteration 10/25 | Loss: 0.00092085
Iteration 11/25 | Loss: 0.00092085
Iteration 12/25 | Loss: 0.00092085
Iteration 13/25 | Loss: 0.00092085
Iteration 14/25 | Loss: 0.00092085
Iteration 15/25 | Loss: 0.00092085
Iteration 16/25 | Loss: 0.00092085
Iteration 17/25 | Loss: 0.00092085
Iteration 18/25 | Loss: 0.00092085
Iteration 19/25 | Loss: 0.00092085
Iteration 20/25 | Loss: 0.00092085
Iteration 21/25 | Loss: 0.00092085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009208482806570828, 0.0009208482806570828, 0.0009208482806570828, 0.0009208482806570828, 0.0009208482806570828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009208482806570828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092085
Iteration 2/1000 | Loss: 0.00005402
Iteration 3/1000 | Loss: 0.00003224
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002303
Iteration 6/1000 | Loss: 0.00002212
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002104
Iteration 9/1000 | Loss: 0.00002060
Iteration 10/1000 | Loss: 0.00002032
Iteration 11/1000 | Loss: 0.00002007
Iteration 12/1000 | Loss: 0.00001990
Iteration 13/1000 | Loss: 0.00001989
Iteration 14/1000 | Loss: 0.00001982
Iteration 15/1000 | Loss: 0.00001981
Iteration 16/1000 | Loss: 0.00001980
Iteration 17/1000 | Loss: 0.00001979
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001977
Iteration 21/1000 | Loss: 0.00001977
Iteration 22/1000 | Loss: 0.00001976
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001971
Iteration 25/1000 | Loss: 0.00001964
Iteration 26/1000 | Loss: 0.00001955
Iteration 27/1000 | Loss: 0.00001954
Iteration 28/1000 | Loss: 0.00001954
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001952
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001951
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001950
Iteration 35/1000 | Loss: 0.00001949
Iteration 36/1000 | Loss: 0.00001949
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001944
Iteration 40/1000 | Loss: 0.00001942
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001941
Iteration 45/1000 | Loss: 0.00001940
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001940
Iteration 48/1000 | Loss: 0.00001939
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001936
Iteration 56/1000 | Loss: 0.00001934
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001933
Iteration 60/1000 | Loss: 0.00001933
Iteration 61/1000 | Loss: 0.00001933
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001932
Iteration 64/1000 | Loss: 0.00001932
Iteration 65/1000 | Loss: 0.00001932
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001931
Iteration 68/1000 | Loss: 0.00001931
Iteration 69/1000 | Loss: 0.00001931
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001930
Iteration 75/1000 | Loss: 0.00001930
Iteration 76/1000 | Loss: 0.00001930
Iteration 77/1000 | Loss: 0.00001930
Iteration 78/1000 | Loss: 0.00001930
Iteration 79/1000 | Loss: 0.00001930
Iteration 80/1000 | Loss: 0.00001929
Iteration 81/1000 | Loss: 0.00001929
Iteration 82/1000 | Loss: 0.00001929
Iteration 83/1000 | Loss: 0.00001929
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001928
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00001928
Iteration 93/1000 | Loss: 0.00001928
Iteration 94/1000 | Loss: 0.00001928
Iteration 95/1000 | Loss: 0.00001928
Iteration 96/1000 | Loss: 0.00001928
Iteration 97/1000 | Loss: 0.00001928
Iteration 98/1000 | Loss: 0.00001928
Iteration 99/1000 | Loss: 0.00001927
Iteration 100/1000 | Loss: 0.00001927
Iteration 101/1000 | Loss: 0.00001927
Iteration 102/1000 | Loss: 0.00001927
Iteration 103/1000 | Loss: 0.00001927
Iteration 104/1000 | Loss: 0.00001927
Iteration 105/1000 | Loss: 0.00001927
Iteration 106/1000 | Loss: 0.00001927
Iteration 107/1000 | Loss: 0.00001927
Iteration 108/1000 | Loss: 0.00001927
Iteration 109/1000 | Loss: 0.00001927
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001926
Iteration 112/1000 | Loss: 0.00001926
Iteration 113/1000 | Loss: 0.00001926
Iteration 114/1000 | Loss: 0.00001926
Iteration 115/1000 | Loss: 0.00001926
Iteration 116/1000 | Loss: 0.00001926
Iteration 117/1000 | Loss: 0.00001926
Iteration 118/1000 | Loss: 0.00001926
Iteration 119/1000 | Loss: 0.00001925
Iteration 120/1000 | Loss: 0.00001925
Iteration 121/1000 | Loss: 0.00001925
Iteration 122/1000 | Loss: 0.00001924
Iteration 123/1000 | Loss: 0.00001924
Iteration 124/1000 | Loss: 0.00001924
Iteration 125/1000 | Loss: 0.00001924
Iteration 126/1000 | Loss: 0.00001924
Iteration 127/1000 | Loss: 0.00001924
Iteration 128/1000 | Loss: 0.00001924
Iteration 129/1000 | Loss: 0.00001924
Iteration 130/1000 | Loss: 0.00001924
Iteration 131/1000 | Loss: 0.00001924
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001923
Iteration 135/1000 | Loss: 0.00001923
Iteration 136/1000 | Loss: 0.00001923
Iteration 137/1000 | Loss: 0.00001923
Iteration 138/1000 | Loss: 0.00001923
Iteration 139/1000 | Loss: 0.00001923
Iteration 140/1000 | Loss: 0.00001923
Iteration 141/1000 | Loss: 0.00001923
Iteration 142/1000 | Loss: 0.00001923
Iteration 143/1000 | Loss: 0.00001923
Iteration 144/1000 | Loss: 0.00001923
Iteration 145/1000 | Loss: 0.00001923
Iteration 146/1000 | Loss: 0.00001923
Iteration 147/1000 | Loss: 0.00001922
Iteration 148/1000 | Loss: 0.00001922
Iteration 149/1000 | Loss: 0.00001922
Iteration 150/1000 | Loss: 0.00001922
Iteration 151/1000 | Loss: 0.00001922
Iteration 152/1000 | Loss: 0.00001921
Iteration 153/1000 | Loss: 0.00001921
Iteration 154/1000 | Loss: 0.00001921
Iteration 155/1000 | Loss: 0.00001921
Iteration 156/1000 | Loss: 0.00001921
Iteration 157/1000 | Loss: 0.00001921
Iteration 158/1000 | Loss: 0.00001921
Iteration 159/1000 | Loss: 0.00001921
Iteration 160/1000 | Loss: 0.00001921
Iteration 161/1000 | Loss: 0.00001921
Iteration 162/1000 | Loss: 0.00001921
Iteration 163/1000 | Loss: 0.00001920
Iteration 164/1000 | Loss: 0.00001920
Iteration 165/1000 | Loss: 0.00001920
Iteration 166/1000 | Loss: 0.00001920
Iteration 167/1000 | Loss: 0.00001920
Iteration 168/1000 | Loss: 0.00001920
Iteration 169/1000 | Loss: 0.00001920
Iteration 170/1000 | Loss: 0.00001920
Iteration 171/1000 | Loss: 0.00001920
Iteration 172/1000 | Loss: 0.00001920
Iteration 173/1000 | Loss: 0.00001920
Iteration 174/1000 | Loss: 0.00001920
Iteration 175/1000 | Loss: 0.00001920
Iteration 176/1000 | Loss: 0.00001919
Iteration 177/1000 | Loss: 0.00001919
Iteration 178/1000 | Loss: 0.00001919
Iteration 179/1000 | Loss: 0.00001919
Iteration 180/1000 | Loss: 0.00001919
Iteration 181/1000 | Loss: 0.00001919
Iteration 182/1000 | Loss: 0.00001919
Iteration 183/1000 | Loss: 0.00001919
Iteration 184/1000 | Loss: 0.00001919
Iteration 185/1000 | Loss: 0.00001919
Iteration 186/1000 | Loss: 0.00001919
Iteration 187/1000 | Loss: 0.00001919
Iteration 188/1000 | Loss: 0.00001919
Iteration 189/1000 | Loss: 0.00001919
Iteration 190/1000 | Loss: 0.00001919
Iteration 191/1000 | Loss: 0.00001918
Iteration 192/1000 | Loss: 0.00001918
Iteration 193/1000 | Loss: 0.00001918
Iteration 194/1000 | Loss: 0.00001918
Iteration 195/1000 | Loss: 0.00001918
Iteration 196/1000 | Loss: 0.00001918
Iteration 197/1000 | Loss: 0.00001918
Iteration 198/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.918439011205919e-05, 1.918439011205919e-05, 1.918439011205919e-05, 1.918439011205919e-05, 1.918439011205919e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.918439011205919e-05

Optimization complete. Final v2v error: 3.61653995513916 mm

Highest mean error: 5.299199104309082 mm for frame 58

Lowest mean error: 3.182621479034424 mm for frame 134

Saving results

Total time: 42.631505489349365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993820
Iteration 2/25 | Loss: 0.00183976
Iteration 3/25 | Loss: 0.00153795
Iteration 4/25 | Loss: 0.00152293
Iteration 5/25 | Loss: 0.00151728
Iteration 6/25 | Loss: 0.00151697
Iteration 7/25 | Loss: 0.00151697
Iteration 8/25 | Loss: 0.00151697
Iteration 9/25 | Loss: 0.00151697
Iteration 10/25 | Loss: 0.00151697
Iteration 11/25 | Loss: 0.00151697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001516972086392343, 0.001516972086392343, 0.001516972086392343, 0.001516972086392343, 0.001516972086392343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001516972086392343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52268791
Iteration 2/25 | Loss: 0.00133509
Iteration 3/25 | Loss: 0.00133509
Iteration 4/25 | Loss: 0.00133509
Iteration 5/25 | Loss: 0.00133509
Iteration 6/25 | Loss: 0.00133509
Iteration 7/25 | Loss: 0.00133509
Iteration 8/25 | Loss: 0.00133509
Iteration 9/25 | Loss: 0.00133509
Iteration 10/25 | Loss: 0.00133509
Iteration 11/25 | Loss: 0.00133509
Iteration 12/25 | Loss: 0.00133509
Iteration 13/25 | Loss: 0.00133509
Iteration 14/25 | Loss: 0.00133509
Iteration 15/25 | Loss: 0.00133509
Iteration 16/25 | Loss: 0.00133509
Iteration 17/25 | Loss: 0.00133509
Iteration 18/25 | Loss: 0.00133509
Iteration 19/25 | Loss: 0.00133509
Iteration 20/25 | Loss: 0.00133509
Iteration 21/25 | Loss: 0.00133509
Iteration 22/25 | Loss: 0.00133509
Iteration 23/25 | Loss: 0.00133509
Iteration 24/25 | Loss: 0.00133509
Iteration 25/25 | Loss: 0.00133509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133509
Iteration 2/1000 | Loss: 0.00009424
Iteration 3/1000 | Loss: 0.00005953
Iteration 4/1000 | Loss: 0.00004544
Iteration 5/1000 | Loss: 0.00004115
Iteration 6/1000 | Loss: 0.00003946
Iteration 7/1000 | Loss: 0.00003871
Iteration 8/1000 | Loss: 0.00003797
Iteration 9/1000 | Loss: 0.00003740
Iteration 10/1000 | Loss: 0.00003687
Iteration 11/1000 | Loss: 0.00003639
Iteration 12/1000 | Loss: 0.00003598
Iteration 13/1000 | Loss: 0.00003563
Iteration 14/1000 | Loss: 0.00003522
Iteration 15/1000 | Loss: 0.00003491
Iteration 16/1000 | Loss: 0.00003462
Iteration 17/1000 | Loss: 0.00003431
Iteration 18/1000 | Loss: 0.00003401
Iteration 19/1000 | Loss: 0.00003384
Iteration 20/1000 | Loss: 0.00003374
Iteration 21/1000 | Loss: 0.00003362
Iteration 22/1000 | Loss: 0.00003356
Iteration 23/1000 | Loss: 0.00003355
Iteration 24/1000 | Loss: 0.00003354
Iteration 25/1000 | Loss: 0.00003352
Iteration 26/1000 | Loss: 0.00003340
Iteration 27/1000 | Loss: 0.00003337
Iteration 28/1000 | Loss: 0.00003333
Iteration 29/1000 | Loss: 0.00003333
Iteration 30/1000 | Loss: 0.00003332
Iteration 31/1000 | Loss: 0.00003332
Iteration 32/1000 | Loss: 0.00003331
Iteration 33/1000 | Loss: 0.00003330
Iteration 34/1000 | Loss: 0.00003330
Iteration 35/1000 | Loss: 0.00003328
Iteration 36/1000 | Loss: 0.00003328
Iteration 37/1000 | Loss: 0.00003328
Iteration 38/1000 | Loss: 0.00003327
Iteration 39/1000 | Loss: 0.00003327
Iteration 40/1000 | Loss: 0.00003327
Iteration 41/1000 | Loss: 0.00003326
Iteration 42/1000 | Loss: 0.00003326
Iteration 43/1000 | Loss: 0.00003326
Iteration 44/1000 | Loss: 0.00003326
Iteration 45/1000 | Loss: 0.00003325
Iteration 46/1000 | Loss: 0.00003325
Iteration 47/1000 | Loss: 0.00003325
Iteration 48/1000 | Loss: 0.00003325
Iteration 49/1000 | Loss: 0.00003325
Iteration 50/1000 | Loss: 0.00003325
Iteration 51/1000 | Loss: 0.00003325
Iteration 52/1000 | Loss: 0.00003325
Iteration 53/1000 | Loss: 0.00003325
Iteration 54/1000 | Loss: 0.00003325
Iteration 55/1000 | Loss: 0.00003324
Iteration 56/1000 | Loss: 0.00003323
Iteration 57/1000 | Loss: 0.00003323
Iteration 58/1000 | Loss: 0.00003323
Iteration 59/1000 | Loss: 0.00003323
Iteration 60/1000 | Loss: 0.00003323
Iteration 61/1000 | Loss: 0.00003323
Iteration 62/1000 | Loss: 0.00003323
Iteration 63/1000 | Loss: 0.00003323
Iteration 64/1000 | Loss: 0.00003323
Iteration 65/1000 | Loss: 0.00003321
Iteration 66/1000 | Loss: 0.00003320
Iteration 67/1000 | Loss: 0.00003320
Iteration 68/1000 | Loss: 0.00003320
Iteration 69/1000 | Loss: 0.00003320
Iteration 70/1000 | Loss: 0.00003320
Iteration 71/1000 | Loss: 0.00003320
Iteration 72/1000 | Loss: 0.00003320
Iteration 73/1000 | Loss: 0.00003320
Iteration 74/1000 | Loss: 0.00003319
Iteration 75/1000 | Loss: 0.00003319
Iteration 76/1000 | Loss: 0.00003319
Iteration 77/1000 | Loss: 0.00003319
Iteration 78/1000 | Loss: 0.00003319
Iteration 79/1000 | Loss: 0.00003318
Iteration 80/1000 | Loss: 0.00003317
Iteration 81/1000 | Loss: 0.00003317
Iteration 82/1000 | Loss: 0.00003317
Iteration 83/1000 | Loss: 0.00003317
Iteration 84/1000 | Loss: 0.00003317
Iteration 85/1000 | Loss: 0.00003316
Iteration 86/1000 | Loss: 0.00003316
Iteration 87/1000 | Loss: 0.00003316
Iteration 88/1000 | Loss: 0.00003316
Iteration 89/1000 | Loss: 0.00003316
Iteration 90/1000 | Loss: 0.00003316
Iteration 91/1000 | Loss: 0.00003315
Iteration 92/1000 | Loss: 0.00003315
Iteration 93/1000 | Loss: 0.00003315
Iteration 94/1000 | Loss: 0.00003315
Iteration 95/1000 | Loss: 0.00003315
Iteration 96/1000 | Loss: 0.00003315
Iteration 97/1000 | Loss: 0.00003314
Iteration 98/1000 | Loss: 0.00003314
Iteration 99/1000 | Loss: 0.00003314
Iteration 100/1000 | Loss: 0.00003314
Iteration 101/1000 | Loss: 0.00003313
Iteration 102/1000 | Loss: 0.00003313
Iteration 103/1000 | Loss: 0.00003313
Iteration 104/1000 | Loss: 0.00003313
Iteration 105/1000 | Loss: 0.00003313
Iteration 106/1000 | Loss: 0.00003313
Iteration 107/1000 | Loss: 0.00003313
Iteration 108/1000 | Loss: 0.00003313
Iteration 109/1000 | Loss: 0.00003313
Iteration 110/1000 | Loss: 0.00003312
Iteration 111/1000 | Loss: 0.00003312
Iteration 112/1000 | Loss: 0.00003312
Iteration 113/1000 | Loss: 0.00003312
Iteration 114/1000 | Loss: 0.00003312
Iteration 115/1000 | Loss: 0.00003312
Iteration 116/1000 | Loss: 0.00003311
Iteration 117/1000 | Loss: 0.00003311
Iteration 118/1000 | Loss: 0.00003311
Iteration 119/1000 | Loss: 0.00003311
Iteration 120/1000 | Loss: 0.00003311
Iteration 121/1000 | Loss: 0.00003311
Iteration 122/1000 | Loss: 0.00003311
Iteration 123/1000 | Loss: 0.00003311
Iteration 124/1000 | Loss: 0.00003311
Iteration 125/1000 | Loss: 0.00003310
Iteration 126/1000 | Loss: 0.00003310
Iteration 127/1000 | Loss: 0.00003310
Iteration 128/1000 | Loss: 0.00003310
Iteration 129/1000 | Loss: 0.00003310
Iteration 130/1000 | Loss: 0.00003310
Iteration 131/1000 | Loss: 0.00003310
Iteration 132/1000 | Loss: 0.00003310
Iteration 133/1000 | Loss: 0.00003309
Iteration 134/1000 | Loss: 0.00003309
Iteration 135/1000 | Loss: 0.00003309
Iteration 136/1000 | Loss: 0.00003309
Iteration 137/1000 | Loss: 0.00003309
Iteration 138/1000 | Loss: 0.00003309
Iteration 139/1000 | Loss: 0.00003309
Iteration 140/1000 | Loss: 0.00003309
Iteration 141/1000 | Loss: 0.00003309
Iteration 142/1000 | Loss: 0.00003309
Iteration 143/1000 | Loss: 0.00003309
Iteration 144/1000 | Loss: 0.00003309
Iteration 145/1000 | Loss: 0.00003309
Iteration 146/1000 | Loss: 0.00003308
Iteration 147/1000 | Loss: 0.00003308
Iteration 148/1000 | Loss: 0.00003308
Iteration 149/1000 | Loss: 0.00003308
Iteration 150/1000 | Loss: 0.00003308
Iteration 151/1000 | Loss: 0.00003308
Iteration 152/1000 | Loss: 0.00003308
Iteration 153/1000 | Loss: 0.00003307
Iteration 154/1000 | Loss: 0.00003307
Iteration 155/1000 | Loss: 0.00003307
Iteration 156/1000 | Loss: 0.00003307
Iteration 157/1000 | Loss: 0.00003307
Iteration 158/1000 | Loss: 0.00003306
Iteration 159/1000 | Loss: 0.00003306
Iteration 160/1000 | Loss: 0.00003306
Iteration 161/1000 | Loss: 0.00003306
Iteration 162/1000 | Loss: 0.00003306
Iteration 163/1000 | Loss: 0.00003306
Iteration 164/1000 | Loss: 0.00003306
Iteration 165/1000 | Loss: 0.00003305
Iteration 166/1000 | Loss: 0.00003305
Iteration 167/1000 | Loss: 0.00003305
Iteration 168/1000 | Loss: 0.00003305
Iteration 169/1000 | Loss: 0.00003304
Iteration 170/1000 | Loss: 0.00003304
Iteration 171/1000 | Loss: 0.00003304
Iteration 172/1000 | Loss: 0.00003304
Iteration 173/1000 | Loss: 0.00003304
Iteration 174/1000 | Loss: 0.00003304
Iteration 175/1000 | Loss: 0.00003304
Iteration 176/1000 | Loss: 0.00003304
Iteration 177/1000 | Loss: 0.00003303
Iteration 178/1000 | Loss: 0.00003303
Iteration 179/1000 | Loss: 0.00003303
Iteration 180/1000 | Loss: 0.00003303
Iteration 181/1000 | Loss: 0.00003303
Iteration 182/1000 | Loss: 0.00003303
Iteration 183/1000 | Loss: 0.00003303
Iteration 184/1000 | Loss: 0.00003303
Iteration 185/1000 | Loss: 0.00003303
Iteration 186/1000 | Loss: 0.00003303
Iteration 187/1000 | Loss: 0.00003302
Iteration 188/1000 | Loss: 0.00003302
Iteration 189/1000 | Loss: 0.00003302
Iteration 190/1000 | Loss: 0.00003302
Iteration 191/1000 | Loss: 0.00003302
Iteration 192/1000 | Loss: 0.00003302
Iteration 193/1000 | Loss: 0.00003302
Iteration 194/1000 | Loss: 0.00003302
Iteration 195/1000 | Loss: 0.00003302
Iteration 196/1000 | Loss: 0.00003302
Iteration 197/1000 | Loss: 0.00003302
Iteration 198/1000 | Loss: 0.00003302
Iteration 199/1000 | Loss: 0.00003302
Iteration 200/1000 | Loss: 0.00003302
Iteration 201/1000 | Loss: 0.00003302
Iteration 202/1000 | Loss: 0.00003302
Iteration 203/1000 | Loss: 0.00003301
Iteration 204/1000 | Loss: 0.00003301
Iteration 205/1000 | Loss: 0.00003301
Iteration 206/1000 | Loss: 0.00003301
Iteration 207/1000 | Loss: 0.00003301
Iteration 208/1000 | Loss: 0.00003301
Iteration 209/1000 | Loss: 0.00003301
Iteration 210/1000 | Loss: 0.00003301
Iteration 211/1000 | Loss: 0.00003301
Iteration 212/1000 | Loss: 0.00003301
Iteration 213/1000 | Loss: 0.00003301
Iteration 214/1000 | Loss: 0.00003301
Iteration 215/1000 | Loss: 0.00003301
Iteration 216/1000 | Loss: 0.00003301
Iteration 217/1000 | Loss: 0.00003301
Iteration 218/1000 | Loss: 0.00003301
Iteration 219/1000 | Loss: 0.00003301
Iteration 220/1000 | Loss: 0.00003300
Iteration 221/1000 | Loss: 0.00003300
Iteration 222/1000 | Loss: 0.00003300
Iteration 223/1000 | Loss: 0.00003300
Iteration 224/1000 | Loss: 0.00003300
Iteration 225/1000 | Loss: 0.00003300
Iteration 226/1000 | Loss: 0.00003300
Iteration 227/1000 | Loss: 0.00003300
Iteration 228/1000 | Loss: 0.00003300
Iteration 229/1000 | Loss: 0.00003300
Iteration 230/1000 | Loss: 0.00003300
Iteration 231/1000 | Loss: 0.00003300
Iteration 232/1000 | Loss: 0.00003300
Iteration 233/1000 | Loss: 0.00003300
Iteration 234/1000 | Loss: 0.00003300
Iteration 235/1000 | Loss: 0.00003300
Iteration 236/1000 | Loss: 0.00003300
Iteration 237/1000 | Loss: 0.00003300
Iteration 238/1000 | Loss: 0.00003299
Iteration 239/1000 | Loss: 0.00003299
Iteration 240/1000 | Loss: 0.00003299
Iteration 241/1000 | Loss: 0.00003299
Iteration 242/1000 | Loss: 0.00003299
Iteration 243/1000 | Loss: 0.00003299
Iteration 244/1000 | Loss: 0.00003299
Iteration 245/1000 | Loss: 0.00003299
Iteration 246/1000 | Loss: 0.00003299
Iteration 247/1000 | Loss: 0.00003299
Iteration 248/1000 | Loss: 0.00003299
Iteration 249/1000 | Loss: 0.00003299
Iteration 250/1000 | Loss: 0.00003299
Iteration 251/1000 | Loss: 0.00003299
Iteration 252/1000 | Loss: 0.00003299
Iteration 253/1000 | Loss: 0.00003299
Iteration 254/1000 | Loss: 0.00003299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [3.2986248697852716e-05, 3.2986248697852716e-05, 3.2986248697852716e-05, 3.2986248697852716e-05, 3.2986248697852716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2986248697852716e-05

Optimization complete. Final v2v error: 4.72552490234375 mm

Highest mean error: 5.728191375732422 mm for frame 40

Lowest mean error: 4.21607780456543 mm for frame 61

Saving results

Total time: 56.98606085777283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821604
Iteration 2/25 | Loss: 0.00128492
Iteration 3/25 | Loss: 0.00123746
Iteration 4/25 | Loss: 0.00123155
Iteration 5/25 | Loss: 0.00122996
Iteration 6/25 | Loss: 0.00122985
Iteration 7/25 | Loss: 0.00122985
Iteration 8/25 | Loss: 0.00122985
Iteration 9/25 | Loss: 0.00122985
Iteration 10/25 | Loss: 0.00122985
Iteration 11/25 | Loss: 0.00122985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012298496440052986, 0.0012298496440052986, 0.0012298496440052986, 0.0012298496440052986, 0.0012298496440052986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012298496440052986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55592287
Iteration 2/25 | Loss: 0.00108188
Iteration 3/25 | Loss: 0.00108188
Iteration 4/25 | Loss: 0.00108188
Iteration 5/25 | Loss: 0.00108188
Iteration 6/25 | Loss: 0.00108188
Iteration 7/25 | Loss: 0.00108188
Iteration 8/25 | Loss: 0.00108188
Iteration 9/25 | Loss: 0.00108188
Iteration 10/25 | Loss: 0.00108188
Iteration 11/25 | Loss: 0.00108188
Iteration 12/25 | Loss: 0.00108188
Iteration 13/25 | Loss: 0.00108188
Iteration 14/25 | Loss: 0.00108188
Iteration 15/25 | Loss: 0.00108188
Iteration 16/25 | Loss: 0.00108188
Iteration 17/25 | Loss: 0.00108188
Iteration 18/25 | Loss: 0.00108188
Iteration 19/25 | Loss: 0.00108187
Iteration 20/25 | Loss: 0.00108187
Iteration 21/25 | Loss: 0.00108187
Iteration 22/25 | Loss: 0.00108187
Iteration 23/25 | Loss: 0.00108187
Iteration 24/25 | Loss: 0.00108187
Iteration 25/25 | Loss: 0.00108187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108187
Iteration 2/1000 | Loss: 0.00002072
Iteration 3/1000 | Loss: 0.00001469
Iteration 4/1000 | Loss: 0.00001301
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001190
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001124
Iteration 9/1000 | Loss: 0.00001105
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001074
Iteration 12/1000 | Loss: 0.00001061
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001039
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001037
Iteration 20/1000 | Loss: 0.00001036
Iteration 21/1000 | Loss: 0.00001035
Iteration 22/1000 | Loss: 0.00001035
Iteration 23/1000 | Loss: 0.00001033
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001032
Iteration 26/1000 | Loss: 0.00001031
Iteration 27/1000 | Loss: 0.00001031
Iteration 28/1000 | Loss: 0.00001031
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001030
Iteration 31/1000 | Loss: 0.00001026
Iteration 32/1000 | Loss: 0.00001026
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001026
Iteration 35/1000 | Loss: 0.00001025
Iteration 36/1000 | Loss: 0.00001025
Iteration 37/1000 | Loss: 0.00001025
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001025
Iteration 42/1000 | Loss: 0.00001024
Iteration 43/1000 | Loss: 0.00001023
Iteration 44/1000 | Loss: 0.00001022
Iteration 45/1000 | Loss: 0.00001022
Iteration 46/1000 | Loss: 0.00001022
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001021
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001021
Iteration 51/1000 | Loss: 0.00001021
Iteration 52/1000 | Loss: 0.00001021
Iteration 53/1000 | Loss: 0.00001020
Iteration 54/1000 | Loss: 0.00001020
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001017
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001016
Iteration 64/1000 | Loss: 0.00001016
Iteration 65/1000 | Loss: 0.00001016
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001015
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001014
Iteration 70/1000 | Loss: 0.00001014
Iteration 71/1000 | Loss: 0.00001014
Iteration 72/1000 | Loss: 0.00001014
Iteration 73/1000 | Loss: 0.00001013
Iteration 74/1000 | Loss: 0.00001013
Iteration 75/1000 | Loss: 0.00001012
Iteration 76/1000 | Loss: 0.00001012
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001011
Iteration 79/1000 | Loss: 0.00001008
Iteration 80/1000 | Loss: 0.00001008
Iteration 81/1000 | Loss: 0.00001008
Iteration 82/1000 | Loss: 0.00001008
Iteration 83/1000 | Loss: 0.00001007
Iteration 84/1000 | Loss: 0.00001007
Iteration 85/1000 | Loss: 0.00001007
Iteration 86/1000 | Loss: 0.00001007
Iteration 87/1000 | Loss: 0.00001007
Iteration 88/1000 | Loss: 0.00001007
Iteration 89/1000 | Loss: 0.00001007
Iteration 90/1000 | Loss: 0.00001007
Iteration 91/1000 | Loss: 0.00001007
Iteration 92/1000 | Loss: 0.00001007
Iteration 93/1000 | Loss: 0.00001007
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001006
Iteration 96/1000 | Loss: 0.00001006
Iteration 97/1000 | Loss: 0.00001006
Iteration 98/1000 | Loss: 0.00001006
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001006
Iteration 101/1000 | Loss: 0.00001004
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001002
Iteration 107/1000 | Loss: 0.00001002
Iteration 108/1000 | Loss: 0.00001002
Iteration 109/1000 | Loss: 0.00001002
Iteration 110/1000 | Loss: 0.00001001
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001001
Iteration 113/1000 | Loss: 0.00001001
Iteration 114/1000 | Loss: 0.00001001
Iteration 115/1000 | Loss: 0.00001000
Iteration 116/1000 | Loss: 0.00001000
Iteration 117/1000 | Loss: 0.00001000
Iteration 118/1000 | Loss: 0.00001000
Iteration 119/1000 | Loss: 0.00001000
Iteration 120/1000 | Loss: 0.00001000
Iteration 121/1000 | Loss: 0.00001000
Iteration 122/1000 | Loss: 0.00001000
Iteration 123/1000 | Loss: 0.00001000
Iteration 124/1000 | Loss: 0.00000999
Iteration 125/1000 | Loss: 0.00000999
Iteration 126/1000 | Loss: 0.00000999
Iteration 127/1000 | Loss: 0.00000999
Iteration 128/1000 | Loss: 0.00000998
Iteration 129/1000 | Loss: 0.00000998
Iteration 130/1000 | Loss: 0.00000998
Iteration 131/1000 | Loss: 0.00000998
Iteration 132/1000 | Loss: 0.00000998
Iteration 133/1000 | Loss: 0.00000998
Iteration 134/1000 | Loss: 0.00000998
Iteration 135/1000 | Loss: 0.00000998
Iteration 136/1000 | Loss: 0.00000998
Iteration 137/1000 | Loss: 0.00000998
Iteration 138/1000 | Loss: 0.00000998
Iteration 139/1000 | Loss: 0.00000998
Iteration 140/1000 | Loss: 0.00000998
Iteration 141/1000 | Loss: 0.00000998
Iteration 142/1000 | Loss: 0.00000998
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000997
Iteration 150/1000 | Loss: 0.00000997
Iteration 151/1000 | Loss: 0.00000997
Iteration 152/1000 | Loss: 0.00000997
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000997
Iteration 155/1000 | Loss: 0.00000997
Iteration 156/1000 | Loss: 0.00000997
Iteration 157/1000 | Loss: 0.00000997
Iteration 158/1000 | Loss: 0.00000997
Iteration 159/1000 | Loss: 0.00000997
Iteration 160/1000 | Loss: 0.00000997
Iteration 161/1000 | Loss: 0.00000997
Iteration 162/1000 | Loss: 0.00000997
Iteration 163/1000 | Loss: 0.00000997
Iteration 164/1000 | Loss: 0.00000997
Iteration 165/1000 | Loss: 0.00000997
Iteration 166/1000 | Loss: 0.00000997
Iteration 167/1000 | Loss: 0.00000997
Iteration 168/1000 | Loss: 0.00000997
Iteration 169/1000 | Loss: 0.00000997
Iteration 170/1000 | Loss: 0.00000997
Iteration 171/1000 | Loss: 0.00000997
Iteration 172/1000 | Loss: 0.00000997
Iteration 173/1000 | Loss: 0.00000997
Iteration 174/1000 | Loss: 0.00000997
Iteration 175/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [9.969345228455495e-06, 9.969345228455495e-06, 9.969345228455495e-06, 9.969345228455495e-06, 9.969345228455495e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.969345228455495e-06

Optimization complete. Final v2v error: 2.731297254562378 mm

Highest mean error: 3.0867788791656494 mm for frame 51

Lowest mean error: 2.5970191955566406 mm for frame 108

Saving results

Total time: 35.833813428878784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763318
Iteration 2/25 | Loss: 0.00164163
Iteration 3/25 | Loss: 0.00143618
Iteration 4/25 | Loss: 0.00139614
Iteration 5/25 | Loss: 0.00138865
Iteration 6/25 | Loss: 0.00137050
Iteration 7/25 | Loss: 0.00139021
Iteration 8/25 | Loss: 0.00133834
Iteration 9/25 | Loss: 0.00133150
Iteration 10/25 | Loss: 0.00133109
Iteration 11/25 | Loss: 0.00133102
Iteration 12/25 | Loss: 0.00133102
Iteration 13/25 | Loss: 0.00133101
Iteration 14/25 | Loss: 0.00133101
Iteration 15/25 | Loss: 0.00133101
Iteration 16/25 | Loss: 0.00133101
Iteration 17/25 | Loss: 0.00133101
Iteration 18/25 | Loss: 0.00133101
Iteration 19/25 | Loss: 0.00133101
Iteration 20/25 | Loss: 0.00133101
Iteration 21/25 | Loss: 0.00133101
Iteration 22/25 | Loss: 0.00133101
Iteration 23/25 | Loss: 0.00133100
Iteration 24/25 | Loss: 0.00133100
Iteration 25/25 | Loss: 0.00133100

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30034840
Iteration 2/25 | Loss: 0.00090407
Iteration 3/25 | Loss: 0.00090404
Iteration 4/25 | Loss: 0.00090403
Iteration 5/25 | Loss: 0.00090403
Iteration 6/25 | Loss: 0.00090403
Iteration 7/25 | Loss: 0.00090403
Iteration 8/25 | Loss: 0.00090403
Iteration 9/25 | Loss: 0.00090403
Iteration 10/25 | Loss: 0.00090403
Iteration 11/25 | Loss: 0.00090403
Iteration 12/25 | Loss: 0.00090403
Iteration 13/25 | Loss: 0.00090403
Iteration 14/25 | Loss: 0.00090403
Iteration 15/25 | Loss: 0.00090403
Iteration 16/25 | Loss: 0.00090403
Iteration 17/25 | Loss: 0.00090403
Iteration 18/25 | Loss: 0.00090403
Iteration 19/25 | Loss: 0.00090403
Iteration 20/25 | Loss: 0.00090403
Iteration 21/25 | Loss: 0.00090403
Iteration 22/25 | Loss: 0.00090403
Iteration 23/25 | Loss: 0.00090403
Iteration 24/25 | Loss: 0.00090403
Iteration 25/25 | Loss: 0.00090403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090403
Iteration 2/1000 | Loss: 0.00005581
Iteration 3/1000 | Loss: 0.00003454
Iteration 4/1000 | Loss: 0.00002915
Iteration 5/1000 | Loss: 0.00002639
Iteration 6/1000 | Loss: 0.00002535
Iteration 7/1000 | Loss: 0.00002457
Iteration 8/1000 | Loss: 0.00002402
Iteration 9/1000 | Loss: 0.00002352
Iteration 10/1000 | Loss: 0.00002323
Iteration 11/1000 | Loss: 0.00002293
Iteration 12/1000 | Loss: 0.00002269
Iteration 13/1000 | Loss: 0.00002250
Iteration 14/1000 | Loss: 0.00002245
Iteration 15/1000 | Loss: 0.00002230
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002209
Iteration 18/1000 | Loss: 0.00002208
Iteration 19/1000 | Loss: 0.00002205
Iteration 20/1000 | Loss: 0.00002204
Iteration 21/1000 | Loss: 0.00002203
Iteration 22/1000 | Loss: 0.00002202
Iteration 23/1000 | Loss: 0.00002201
Iteration 24/1000 | Loss: 0.00002201
Iteration 25/1000 | Loss: 0.00002200
Iteration 26/1000 | Loss: 0.00002200
Iteration 27/1000 | Loss: 0.00002199
Iteration 28/1000 | Loss: 0.00002199
Iteration 29/1000 | Loss: 0.00002199
Iteration 30/1000 | Loss: 0.00002198
Iteration 31/1000 | Loss: 0.00002197
Iteration 32/1000 | Loss: 0.00002197
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00002191
Iteration 35/1000 | Loss: 0.00002191
Iteration 36/1000 | Loss: 0.00002190
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002190
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002189
Iteration 41/1000 | Loss: 0.00002189
Iteration 42/1000 | Loss: 0.00002188
Iteration 43/1000 | Loss: 0.00002188
Iteration 44/1000 | Loss: 0.00002188
Iteration 45/1000 | Loss: 0.00002188
Iteration 46/1000 | Loss: 0.00002188
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002187
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002187
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002186
Iteration 53/1000 | Loss: 0.00002186
Iteration 54/1000 | Loss: 0.00002186
Iteration 55/1000 | Loss: 0.00002186
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002185
Iteration 58/1000 | Loss: 0.00002184
Iteration 59/1000 | Loss: 0.00002184
Iteration 60/1000 | Loss: 0.00002183
Iteration 61/1000 | Loss: 0.00002183
Iteration 62/1000 | Loss: 0.00002182
Iteration 63/1000 | Loss: 0.00002182
Iteration 64/1000 | Loss: 0.00002182
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002181
Iteration 68/1000 | Loss: 0.00002181
Iteration 69/1000 | Loss: 0.00002181
Iteration 70/1000 | Loss: 0.00002181
Iteration 71/1000 | Loss: 0.00002181
Iteration 72/1000 | Loss: 0.00002181
Iteration 73/1000 | Loss: 0.00002181
Iteration 74/1000 | Loss: 0.00002181
Iteration 75/1000 | Loss: 0.00002180
Iteration 76/1000 | Loss: 0.00002180
Iteration 77/1000 | Loss: 0.00002180
Iteration 78/1000 | Loss: 0.00002179
Iteration 79/1000 | Loss: 0.00002179
Iteration 80/1000 | Loss: 0.00002179
Iteration 81/1000 | Loss: 0.00002179
Iteration 82/1000 | Loss: 0.00002179
Iteration 83/1000 | Loss: 0.00002179
Iteration 84/1000 | Loss: 0.00002178
Iteration 85/1000 | Loss: 0.00002178
Iteration 86/1000 | Loss: 0.00002178
Iteration 87/1000 | Loss: 0.00002178
Iteration 88/1000 | Loss: 0.00002178
Iteration 89/1000 | Loss: 0.00002178
Iteration 90/1000 | Loss: 0.00002178
Iteration 91/1000 | Loss: 0.00002178
Iteration 92/1000 | Loss: 0.00002178
Iteration 93/1000 | Loss: 0.00002178
Iteration 94/1000 | Loss: 0.00002178
Iteration 95/1000 | Loss: 0.00002177
Iteration 96/1000 | Loss: 0.00002177
Iteration 97/1000 | Loss: 0.00002176
Iteration 98/1000 | Loss: 0.00002176
Iteration 99/1000 | Loss: 0.00002176
Iteration 100/1000 | Loss: 0.00002176
Iteration 101/1000 | Loss: 0.00002176
Iteration 102/1000 | Loss: 0.00002176
Iteration 103/1000 | Loss: 0.00002176
Iteration 104/1000 | Loss: 0.00002176
Iteration 105/1000 | Loss: 0.00002176
Iteration 106/1000 | Loss: 0.00002175
Iteration 107/1000 | Loss: 0.00002175
Iteration 108/1000 | Loss: 0.00002175
Iteration 109/1000 | Loss: 0.00002175
Iteration 110/1000 | Loss: 0.00002175
Iteration 111/1000 | Loss: 0.00002174
Iteration 112/1000 | Loss: 0.00002174
Iteration 113/1000 | Loss: 0.00002174
Iteration 114/1000 | Loss: 0.00002174
Iteration 115/1000 | Loss: 0.00002174
Iteration 116/1000 | Loss: 0.00002174
Iteration 117/1000 | Loss: 0.00002173
Iteration 118/1000 | Loss: 0.00002173
Iteration 119/1000 | Loss: 0.00002173
Iteration 120/1000 | Loss: 0.00002173
Iteration 121/1000 | Loss: 0.00002173
Iteration 122/1000 | Loss: 0.00002173
Iteration 123/1000 | Loss: 0.00002173
Iteration 124/1000 | Loss: 0.00002173
Iteration 125/1000 | Loss: 0.00002173
Iteration 126/1000 | Loss: 0.00002172
Iteration 127/1000 | Loss: 0.00002172
Iteration 128/1000 | Loss: 0.00002172
Iteration 129/1000 | Loss: 0.00002172
Iteration 130/1000 | Loss: 0.00002172
Iteration 131/1000 | Loss: 0.00002172
Iteration 132/1000 | Loss: 0.00002172
Iteration 133/1000 | Loss: 0.00002172
Iteration 134/1000 | Loss: 0.00002172
Iteration 135/1000 | Loss: 0.00002171
Iteration 136/1000 | Loss: 0.00002171
Iteration 137/1000 | Loss: 0.00002171
Iteration 138/1000 | Loss: 0.00002171
Iteration 139/1000 | Loss: 0.00002171
Iteration 140/1000 | Loss: 0.00002171
Iteration 141/1000 | Loss: 0.00002171
Iteration 142/1000 | Loss: 0.00002171
Iteration 143/1000 | Loss: 0.00002171
Iteration 144/1000 | Loss: 0.00002171
Iteration 145/1000 | Loss: 0.00002171
Iteration 146/1000 | Loss: 0.00002171
Iteration 147/1000 | Loss: 0.00002170
Iteration 148/1000 | Loss: 0.00002170
Iteration 149/1000 | Loss: 0.00002170
Iteration 150/1000 | Loss: 0.00002170
Iteration 151/1000 | Loss: 0.00002170
Iteration 152/1000 | Loss: 0.00002170
Iteration 153/1000 | Loss: 0.00002170
Iteration 154/1000 | Loss: 0.00002170
Iteration 155/1000 | Loss: 0.00002170
Iteration 156/1000 | Loss: 0.00002170
Iteration 157/1000 | Loss: 0.00002170
Iteration 158/1000 | Loss: 0.00002170
Iteration 159/1000 | Loss: 0.00002170
Iteration 160/1000 | Loss: 0.00002170
Iteration 161/1000 | Loss: 0.00002170
Iteration 162/1000 | Loss: 0.00002170
Iteration 163/1000 | Loss: 0.00002170
Iteration 164/1000 | Loss: 0.00002170
Iteration 165/1000 | Loss: 0.00002170
Iteration 166/1000 | Loss: 0.00002170
Iteration 167/1000 | Loss: 0.00002170
Iteration 168/1000 | Loss: 0.00002170
Iteration 169/1000 | Loss: 0.00002170
Iteration 170/1000 | Loss: 0.00002170
Iteration 171/1000 | Loss: 0.00002170
Iteration 172/1000 | Loss: 0.00002170
Iteration 173/1000 | Loss: 0.00002170
Iteration 174/1000 | Loss: 0.00002170
Iteration 175/1000 | Loss: 0.00002170
Iteration 176/1000 | Loss: 0.00002170
Iteration 177/1000 | Loss: 0.00002170
Iteration 178/1000 | Loss: 0.00002170
Iteration 179/1000 | Loss: 0.00002170
Iteration 180/1000 | Loss: 0.00002170
Iteration 181/1000 | Loss: 0.00002170
Iteration 182/1000 | Loss: 0.00002170
Iteration 183/1000 | Loss: 0.00002170
Iteration 184/1000 | Loss: 0.00002170
Iteration 185/1000 | Loss: 0.00002170
Iteration 186/1000 | Loss: 0.00002170
Iteration 187/1000 | Loss: 0.00002170
Iteration 188/1000 | Loss: 0.00002170
Iteration 189/1000 | Loss: 0.00002170
Iteration 190/1000 | Loss: 0.00002170
Iteration 191/1000 | Loss: 0.00002170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.170230982301291e-05, 2.170230982301291e-05, 2.170230982301291e-05, 2.170230982301291e-05, 2.170230982301291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.170230982301291e-05

Optimization complete. Final v2v error: 3.8107240200042725 mm

Highest mean error: 4.531998157501221 mm for frame 127

Lowest mean error: 3.219398021697998 mm for frame 12

Saving results

Total time: 51.696953535079956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834854
Iteration 2/25 | Loss: 0.00166411
Iteration 3/25 | Loss: 0.00141409
Iteration 4/25 | Loss: 0.00137756
Iteration 5/25 | Loss: 0.00136483
Iteration 6/25 | Loss: 0.00135917
Iteration 7/25 | Loss: 0.00135341
Iteration 8/25 | Loss: 0.00136417
Iteration 9/25 | Loss: 0.00133044
Iteration 10/25 | Loss: 0.00131776
Iteration 11/25 | Loss: 0.00131926
Iteration 12/25 | Loss: 0.00131832
Iteration 13/25 | Loss: 0.00130679
Iteration 14/25 | Loss: 0.00129979
Iteration 15/25 | Loss: 0.00129763
Iteration 16/25 | Loss: 0.00130131
Iteration 17/25 | Loss: 0.00129526
Iteration 18/25 | Loss: 0.00129439
Iteration 19/25 | Loss: 0.00129431
Iteration 20/25 | Loss: 0.00129431
Iteration 21/25 | Loss: 0.00129431
Iteration 22/25 | Loss: 0.00129431
Iteration 23/25 | Loss: 0.00129431
Iteration 24/25 | Loss: 0.00129431
Iteration 25/25 | Loss: 0.00129430

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92007744
Iteration 2/25 | Loss: 0.00068492
Iteration 3/25 | Loss: 0.00068491
Iteration 4/25 | Loss: 0.00068491
Iteration 5/25 | Loss: 0.00068491
Iteration 6/25 | Loss: 0.00068491
Iteration 7/25 | Loss: 0.00068491
Iteration 8/25 | Loss: 0.00068491
Iteration 9/25 | Loss: 0.00068491
Iteration 10/25 | Loss: 0.00068491
Iteration 11/25 | Loss: 0.00068491
Iteration 12/25 | Loss: 0.00068491
Iteration 13/25 | Loss: 0.00068491
Iteration 14/25 | Loss: 0.00068491
Iteration 15/25 | Loss: 0.00068491
Iteration 16/25 | Loss: 0.00068491
Iteration 17/25 | Loss: 0.00068491
Iteration 18/25 | Loss: 0.00068491
Iteration 19/25 | Loss: 0.00068491
Iteration 20/25 | Loss: 0.00068491
Iteration 21/25 | Loss: 0.00068491
Iteration 22/25 | Loss: 0.00068491
Iteration 23/25 | Loss: 0.00068491
Iteration 24/25 | Loss: 0.00068491
Iteration 25/25 | Loss: 0.00068491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068491
Iteration 2/1000 | Loss: 0.00004149
Iteration 3/1000 | Loss: 0.00003116
Iteration 4/1000 | Loss: 0.00002757
Iteration 5/1000 | Loss: 0.00002642
Iteration 6/1000 | Loss: 0.00002539
Iteration 7/1000 | Loss: 0.00002472
Iteration 8/1000 | Loss: 0.00002417
Iteration 9/1000 | Loss: 0.00002384
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002291
Iteration 12/1000 | Loss: 0.00040455
Iteration 13/1000 | Loss: 0.00002916
Iteration 14/1000 | Loss: 0.00002443
Iteration 15/1000 | Loss: 0.00002247
Iteration 16/1000 | Loss: 0.00002110
Iteration 17/1000 | Loss: 0.00002054
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002015
Iteration 20/1000 | Loss: 0.00002007
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00002006
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00002005
Iteration 26/1000 | Loss: 0.00002004
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00002001
Iteration 29/1000 | Loss: 0.00002001
Iteration 30/1000 | Loss: 0.00002000
Iteration 31/1000 | Loss: 0.00001999
Iteration 32/1000 | Loss: 0.00001998
Iteration 33/1000 | Loss: 0.00001998
Iteration 34/1000 | Loss: 0.00001997
Iteration 35/1000 | Loss: 0.00001997
Iteration 36/1000 | Loss: 0.00001996
Iteration 37/1000 | Loss: 0.00001995
Iteration 38/1000 | Loss: 0.00001995
Iteration 39/1000 | Loss: 0.00001995
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001994
Iteration 43/1000 | Loss: 0.00001994
Iteration 44/1000 | Loss: 0.00001994
Iteration 45/1000 | Loss: 0.00001994
Iteration 46/1000 | Loss: 0.00001994
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001992
Iteration 50/1000 | Loss: 0.00001992
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001990
Iteration 55/1000 | Loss: 0.00001990
Iteration 56/1000 | Loss: 0.00001990
Iteration 57/1000 | Loss: 0.00001989
Iteration 58/1000 | Loss: 0.00001989
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001988
Iteration 61/1000 | Loss: 0.00001988
Iteration 62/1000 | Loss: 0.00001988
Iteration 63/1000 | Loss: 0.00001988
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001985
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001984
Iteration 83/1000 | Loss: 0.00001983
Iteration 84/1000 | Loss: 0.00001983
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001982
Iteration 94/1000 | Loss: 0.00001982
Iteration 95/1000 | Loss: 0.00001982
Iteration 96/1000 | Loss: 0.00001982
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001981
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001981
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001981
Iteration 107/1000 | Loss: 0.00001981
Iteration 108/1000 | Loss: 0.00001981
Iteration 109/1000 | Loss: 0.00001981
Iteration 110/1000 | Loss: 0.00001980
Iteration 111/1000 | Loss: 0.00001980
Iteration 112/1000 | Loss: 0.00001980
Iteration 113/1000 | Loss: 0.00001980
Iteration 114/1000 | Loss: 0.00001980
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001980
Iteration 117/1000 | Loss: 0.00001980
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001979
Iteration 125/1000 | Loss: 0.00001979
Iteration 126/1000 | Loss: 0.00001979
Iteration 127/1000 | Loss: 0.00001979
Iteration 128/1000 | Loss: 0.00001979
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001979
Iteration 132/1000 | Loss: 0.00001979
Iteration 133/1000 | Loss: 0.00001979
Iteration 134/1000 | Loss: 0.00001979
Iteration 135/1000 | Loss: 0.00001979
Iteration 136/1000 | Loss: 0.00001979
Iteration 137/1000 | Loss: 0.00001979
Iteration 138/1000 | Loss: 0.00001979
Iteration 139/1000 | Loss: 0.00001979
Iteration 140/1000 | Loss: 0.00001978
Iteration 141/1000 | Loss: 0.00001978
Iteration 142/1000 | Loss: 0.00001978
Iteration 143/1000 | Loss: 0.00001978
Iteration 144/1000 | Loss: 0.00001978
Iteration 145/1000 | Loss: 0.00001978
Iteration 146/1000 | Loss: 0.00001978
Iteration 147/1000 | Loss: 0.00001978
Iteration 148/1000 | Loss: 0.00001978
Iteration 149/1000 | Loss: 0.00001978
Iteration 150/1000 | Loss: 0.00001978
Iteration 151/1000 | Loss: 0.00001978
Iteration 152/1000 | Loss: 0.00001978
Iteration 153/1000 | Loss: 0.00001978
Iteration 154/1000 | Loss: 0.00001978
Iteration 155/1000 | Loss: 0.00001978
Iteration 156/1000 | Loss: 0.00001978
Iteration 157/1000 | Loss: 0.00001978
Iteration 158/1000 | Loss: 0.00001978
Iteration 159/1000 | Loss: 0.00001978
Iteration 160/1000 | Loss: 0.00001978
Iteration 161/1000 | Loss: 0.00001978
Iteration 162/1000 | Loss: 0.00001978
Iteration 163/1000 | Loss: 0.00001978
Iteration 164/1000 | Loss: 0.00001978
Iteration 165/1000 | Loss: 0.00001978
Iteration 166/1000 | Loss: 0.00001978
Iteration 167/1000 | Loss: 0.00001978
Iteration 168/1000 | Loss: 0.00001978
Iteration 169/1000 | Loss: 0.00001978
Iteration 170/1000 | Loss: 0.00001978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.9780436559813097e-05, 1.9780436559813097e-05, 1.9780436559813097e-05, 1.9780436559813097e-05, 1.9780436559813097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9780436559813097e-05

Optimization complete. Final v2v error: 3.7584526538848877 mm

Highest mean error: 4.176248550415039 mm for frame 137

Lowest mean error: 3.616640090942383 mm for frame 4

Saving results

Total time: 69.64452838897705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844610
Iteration 2/25 | Loss: 0.00185435
Iteration 3/25 | Loss: 0.00164191
Iteration 4/25 | Loss: 0.00163129
Iteration 5/25 | Loss: 0.00162849
Iteration 6/25 | Loss: 0.00162823
Iteration 7/25 | Loss: 0.00162823
Iteration 8/25 | Loss: 0.00162823
Iteration 9/25 | Loss: 0.00162823
Iteration 10/25 | Loss: 0.00162823
Iteration 11/25 | Loss: 0.00162823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001628231955692172, 0.001628231955692172, 0.001628231955692172, 0.001628231955692172, 0.001628231955692172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001628231955692172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.48486382
Iteration 2/25 | Loss: 0.00126812
Iteration 3/25 | Loss: 0.00126812
Iteration 4/25 | Loss: 0.00126812
Iteration 5/25 | Loss: 0.00126812
Iteration 6/25 | Loss: 0.00126811
Iteration 7/25 | Loss: 0.00126811
Iteration 8/25 | Loss: 0.00126811
Iteration 9/25 | Loss: 0.00126811
Iteration 10/25 | Loss: 0.00126811
Iteration 11/25 | Loss: 0.00126811
Iteration 12/25 | Loss: 0.00126811
Iteration 13/25 | Loss: 0.00126811
Iteration 14/25 | Loss: 0.00126811
Iteration 15/25 | Loss: 0.00126811
Iteration 16/25 | Loss: 0.00126811
Iteration 17/25 | Loss: 0.00126811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012681136140599847, 0.0012681136140599847, 0.0012681136140599847, 0.0012681136140599847, 0.0012681136140599847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012681136140599847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126811
Iteration 2/1000 | Loss: 0.00009775
Iteration 3/1000 | Loss: 0.00006602
Iteration 4/1000 | Loss: 0.00005383
Iteration 5/1000 | Loss: 0.00005002
Iteration 6/1000 | Loss: 0.00004815
Iteration 7/1000 | Loss: 0.00004712
Iteration 8/1000 | Loss: 0.00004629
Iteration 9/1000 | Loss: 0.00004533
Iteration 10/1000 | Loss: 0.00004454
Iteration 11/1000 | Loss: 0.00004383
Iteration 12/1000 | Loss: 0.00004323
Iteration 13/1000 | Loss: 0.00004272
Iteration 14/1000 | Loss: 0.00004222
Iteration 15/1000 | Loss: 0.00004187
Iteration 16/1000 | Loss: 0.00004148
Iteration 17/1000 | Loss: 0.00004118
Iteration 18/1000 | Loss: 0.00004098
Iteration 19/1000 | Loss: 0.00004075
Iteration 20/1000 | Loss: 0.00004066
Iteration 21/1000 | Loss: 0.00004056
Iteration 22/1000 | Loss: 0.00004052
Iteration 23/1000 | Loss: 0.00004046
Iteration 24/1000 | Loss: 0.00004045
Iteration 25/1000 | Loss: 0.00004043
Iteration 26/1000 | Loss: 0.00004042
Iteration 27/1000 | Loss: 0.00004042
Iteration 28/1000 | Loss: 0.00004039
Iteration 29/1000 | Loss: 0.00004029
Iteration 30/1000 | Loss: 0.00004027
Iteration 31/1000 | Loss: 0.00004025
Iteration 32/1000 | Loss: 0.00004021
Iteration 33/1000 | Loss: 0.00004017
Iteration 34/1000 | Loss: 0.00004017
Iteration 35/1000 | Loss: 0.00004015
Iteration 36/1000 | Loss: 0.00004015
Iteration 37/1000 | Loss: 0.00004015
Iteration 38/1000 | Loss: 0.00004015
Iteration 39/1000 | Loss: 0.00004014
Iteration 40/1000 | Loss: 0.00004013
Iteration 41/1000 | Loss: 0.00004013
Iteration 42/1000 | Loss: 0.00004010
Iteration 43/1000 | Loss: 0.00004010
Iteration 44/1000 | Loss: 0.00004009
Iteration 45/1000 | Loss: 0.00004008
Iteration 46/1000 | Loss: 0.00004008
Iteration 47/1000 | Loss: 0.00004007
Iteration 48/1000 | Loss: 0.00004007
Iteration 49/1000 | Loss: 0.00004007
Iteration 50/1000 | Loss: 0.00004006
Iteration 51/1000 | Loss: 0.00004006
Iteration 52/1000 | Loss: 0.00004005
Iteration 53/1000 | Loss: 0.00004005
Iteration 54/1000 | Loss: 0.00004005
Iteration 55/1000 | Loss: 0.00004005
Iteration 56/1000 | Loss: 0.00004005
Iteration 57/1000 | Loss: 0.00004005
Iteration 58/1000 | Loss: 0.00004005
Iteration 59/1000 | Loss: 0.00004005
Iteration 60/1000 | Loss: 0.00004005
Iteration 61/1000 | Loss: 0.00004005
Iteration 62/1000 | Loss: 0.00004005
Iteration 63/1000 | Loss: 0.00004005
Iteration 64/1000 | Loss: 0.00004005
Iteration 65/1000 | Loss: 0.00004005
Iteration 66/1000 | Loss: 0.00004004
Iteration 67/1000 | Loss: 0.00004004
Iteration 68/1000 | Loss: 0.00004004
Iteration 69/1000 | Loss: 0.00004004
Iteration 70/1000 | Loss: 0.00004004
Iteration 71/1000 | Loss: 0.00004003
Iteration 72/1000 | Loss: 0.00004003
Iteration 73/1000 | Loss: 0.00004003
Iteration 74/1000 | Loss: 0.00004003
Iteration 75/1000 | Loss: 0.00004003
Iteration 76/1000 | Loss: 0.00004003
Iteration 77/1000 | Loss: 0.00004002
Iteration 78/1000 | Loss: 0.00004002
Iteration 79/1000 | Loss: 0.00004002
Iteration 80/1000 | Loss: 0.00004002
Iteration 81/1000 | Loss: 0.00004002
Iteration 82/1000 | Loss: 0.00004002
Iteration 83/1000 | Loss: 0.00004001
Iteration 84/1000 | Loss: 0.00004001
Iteration 85/1000 | Loss: 0.00004001
Iteration 86/1000 | Loss: 0.00004001
Iteration 87/1000 | Loss: 0.00004001
Iteration 88/1000 | Loss: 0.00004000
Iteration 89/1000 | Loss: 0.00004000
Iteration 90/1000 | Loss: 0.00004000
Iteration 91/1000 | Loss: 0.00004000
Iteration 92/1000 | Loss: 0.00004000
Iteration 93/1000 | Loss: 0.00004000
Iteration 94/1000 | Loss: 0.00004000
Iteration 95/1000 | Loss: 0.00003999
Iteration 96/1000 | Loss: 0.00003999
Iteration 97/1000 | Loss: 0.00003999
Iteration 98/1000 | Loss: 0.00003999
Iteration 99/1000 | Loss: 0.00003999
Iteration 100/1000 | Loss: 0.00003999
Iteration 101/1000 | Loss: 0.00003998
Iteration 102/1000 | Loss: 0.00003998
Iteration 103/1000 | Loss: 0.00003998
Iteration 104/1000 | Loss: 0.00003998
Iteration 105/1000 | Loss: 0.00003998
Iteration 106/1000 | Loss: 0.00003998
Iteration 107/1000 | Loss: 0.00003998
Iteration 108/1000 | Loss: 0.00003998
Iteration 109/1000 | Loss: 0.00003998
Iteration 110/1000 | Loss: 0.00003998
Iteration 111/1000 | Loss: 0.00003998
Iteration 112/1000 | Loss: 0.00003998
Iteration 113/1000 | Loss: 0.00003998
Iteration 114/1000 | Loss: 0.00003998
Iteration 115/1000 | Loss: 0.00003997
Iteration 116/1000 | Loss: 0.00003997
Iteration 117/1000 | Loss: 0.00003997
Iteration 118/1000 | Loss: 0.00003997
Iteration 119/1000 | Loss: 0.00003997
Iteration 120/1000 | Loss: 0.00003997
Iteration 121/1000 | Loss: 0.00003997
Iteration 122/1000 | Loss: 0.00003997
Iteration 123/1000 | Loss: 0.00003997
Iteration 124/1000 | Loss: 0.00003997
Iteration 125/1000 | Loss: 0.00003997
Iteration 126/1000 | Loss: 0.00003997
Iteration 127/1000 | Loss: 0.00003997
Iteration 128/1000 | Loss: 0.00003997
Iteration 129/1000 | Loss: 0.00003997
Iteration 130/1000 | Loss: 0.00003997
Iteration 131/1000 | Loss: 0.00003997
Iteration 132/1000 | Loss: 0.00003997
Iteration 133/1000 | Loss: 0.00003997
Iteration 134/1000 | Loss: 0.00003997
Iteration 135/1000 | Loss: 0.00003997
Iteration 136/1000 | Loss: 0.00003997
Iteration 137/1000 | Loss: 0.00003997
Iteration 138/1000 | Loss: 0.00003997
Iteration 139/1000 | Loss: 0.00003997
Iteration 140/1000 | Loss: 0.00003997
Iteration 141/1000 | Loss: 0.00003997
Iteration 142/1000 | Loss: 0.00003997
Iteration 143/1000 | Loss: 0.00003997
Iteration 144/1000 | Loss: 0.00003997
Iteration 145/1000 | Loss: 0.00003997
Iteration 146/1000 | Loss: 0.00003997
Iteration 147/1000 | Loss: 0.00003997
Iteration 148/1000 | Loss: 0.00003997
Iteration 149/1000 | Loss: 0.00003997
Iteration 150/1000 | Loss: 0.00003997
Iteration 151/1000 | Loss: 0.00003997
Iteration 152/1000 | Loss: 0.00003997
Iteration 153/1000 | Loss: 0.00003997
Iteration 154/1000 | Loss: 0.00003997
Iteration 155/1000 | Loss: 0.00003997
Iteration 156/1000 | Loss: 0.00003997
Iteration 157/1000 | Loss: 0.00003997
Iteration 158/1000 | Loss: 0.00003997
Iteration 159/1000 | Loss: 0.00003997
Iteration 160/1000 | Loss: 0.00003997
Iteration 161/1000 | Loss: 0.00003997
Iteration 162/1000 | Loss: 0.00003997
Iteration 163/1000 | Loss: 0.00003997
Iteration 164/1000 | Loss: 0.00003997
Iteration 165/1000 | Loss: 0.00003997
Iteration 166/1000 | Loss: 0.00003997
Iteration 167/1000 | Loss: 0.00003997
Iteration 168/1000 | Loss: 0.00003997
Iteration 169/1000 | Loss: 0.00003997
Iteration 170/1000 | Loss: 0.00003997
Iteration 171/1000 | Loss: 0.00003997
Iteration 172/1000 | Loss: 0.00003997
Iteration 173/1000 | Loss: 0.00003997
Iteration 174/1000 | Loss: 0.00003997
Iteration 175/1000 | Loss: 0.00003997
Iteration 176/1000 | Loss: 0.00003997
Iteration 177/1000 | Loss: 0.00003997
Iteration 178/1000 | Loss: 0.00003997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.9971579099074006e-05, 3.9971579099074006e-05, 3.9971579099074006e-05, 3.9971579099074006e-05, 3.9971579099074006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9971579099074006e-05

Optimization complete. Final v2v error: 5.214385509490967 mm

Highest mean error: 6.234071254730225 mm for frame 143

Lowest mean error: 4.905121326446533 mm for frame 37

Saving results

Total time: 49.627856731414795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034940
Iteration 2/25 | Loss: 0.00160373
Iteration 3/25 | Loss: 0.00159493
Iteration 4/25 | Loss: 0.00128715
Iteration 5/25 | Loss: 0.00127638
Iteration 6/25 | Loss: 0.00126985
Iteration 7/25 | Loss: 0.00126235
Iteration 8/25 | Loss: 0.00125844
Iteration 9/25 | Loss: 0.00125666
Iteration 10/25 | Loss: 0.00125558
Iteration 11/25 | Loss: 0.00125918
Iteration 12/25 | Loss: 0.00125245
Iteration 13/25 | Loss: 0.00125111
Iteration 14/25 | Loss: 0.00125052
Iteration 15/25 | Loss: 0.00125046
Iteration 16/25 | Loss: 0.00125046
Iteration 17/25 | Loss: 0.00125045
Iteration 18/25 | Loss: 0.00125045
Iteration 19/25 | Loss: 0.00125045
Iteration 20/25 | Loss: 0.00125045
Iteration 21/25 | Loss: 0.00125045
Iteration 22/25 | Loss: 0.00125044
Iteration 23/25 | Loss: 0.00125044
Iteration 24/25 | Loss: 0.00125044
Iteration 25/25 | Loss: 0.00125044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.57214355
Iteration 2/25 | Loss: 0.00132304
Iteration 3/25 | Loss: 0.00132304
Iteration 4/25 | Loss: 0.00108397
Iteration 5/25 | Loss: 0.00108397
Iteration 6/25 | Loss: 0.00108397
Iteration 7/25 | Loss: 0.00108397
Iteration 8/25 | Loss: 0.00108396
Iteration 9/25 | Loss: 0.00108396
Iteration 10/25 | Loss: 0.00108396
Iteration 11/25 | Loss: 0.00108396
Iteration 12/25 | Loss: 0.00108396
Iteration 13/25 | Loss: 0.00108396
Iteration 14/25 | Loss: 0.00108396
Iteration 15/25 | Loss: 0.00108396
Iteration 16/25 | Loss: 0.00108396
Iteration 17/25 | Loss: 0.00108396
Iteration 18/25 | Loss: 0.00108396
Iteration 19/25 | Loss: 0.00108396
Iteration 20/25 | Loss: 0.00108396
Iteration 21/25 | Loss: 0.00108396
Iteration 22/25 | Loss: 0.00108396
Iteration 23/25 | Loss: 0.00108396
Iteration 24/25 | Loss: 0.00108396
Iteration 25/25 | Loss: 0.00108396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108396
Iteration 2/1000 | Loss: 0.00026473
Iteration 3/1000 | Loss: 0.00010599
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00005890
Iteration 6/1000 | Loss: 0.00022252
Iteration 7/1000 | Loss: 0.00001698
Iteration 8/1000 | Loss: 0.00009479
Iteration 9/1000 | Loss: 0.00044531
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00002573
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001513
Iteration 14/1000 | Loss: 0.00005627
Iteration 15/1000 | Loss: 0.00017222
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001451
Iteration 18/1000 | Loss: 0.00001429
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00009799
Iteration 22/1000 | Loss: 0.00008817
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001356
Iteration 26/1000 | Loss: 0.00004839
Iteration 27/1000 | Loss: 0.00009951
Iteration 28/1000 | Loss: 0.00002162
Iteration 29/1000 | Loss: 0.00004543
Iteration 30/1000 | Loss: 0.00003141
Iteration 31/1000 | Loss: 0.00003143
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001342
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00008269
Iteration 55/1000 | Loss: 0.00004183
Iteration 56/1000 | Loss: 0.00005488
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00003129
Iteration 59/1000 | Loss: 0.00001850
Iteration 60/1000 | Loss: 0.00003672
Iteration 61/1000 | Loss: 0.00001333
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001325
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001322
Iteration 73/1000 | Loss: 0.00002303
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001313
Iteration 86/1000 | Loss: 0.00001313
Iteration 87/1000 | Loss: 0.00001313
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001312
Iteration 91/1000 | Loss: 0.00001312
Iteration 92/1000 | Loss: 0.00001312
Iteration 93/1000 | Loss: 0.00001312
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001311
Iteration 103/1000 | Loss: 0.00001311
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001310
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001309
Iteration 126/1000 | Loss: 0.00001309
Iteration 127/1000 | Loss: 0.00001309
Iteration 128/1000 | Loss: 0.00001309
Iteration 129/1000 | Loss: 0.00001309
Iteration 130/1000 | Loss: 0.00001309
Iteration 131/1000 | Loss: 0.00001309
Iteration 132/1000 | Loss: 0.00001309
Iteration 133/1000 | Loss: 0.00001309
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00005212
Iteration 136/1000 | Loss: 0.00001312
Iteration 137/1000 | Loss: 0.00001312
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001308
Iteration 140/1000 | Loss: 0.00001308
Iteration 141/1000 | Loss: 0.00001307
Iteration 142/1000 | Loss: 0.00001307
Iteration 143/1000 | Loss: 0.00001307
Iteration 144/1000 | Loss: 0.00001307
Iteration 145/1000 | Loss: 0.00001307
Iteration 146/1000 | Loss: 0.00001307
Iteration 147/1000 | Loss: 0.00001307
Iteration 148/1000 | Loss: 0.00001306
Iteration 149/1000 | Loss: 0.00001306
Iteration 150/1000 | Loss: 0.00001306
Iteration 151/1000 | Loss: 0.00001306
Iteration 152/1000 | Loss: 0.00001306
Iteration 153/1000 | Loss: 0.00001306
Iteration 154/1000 | Loss: 0.00001306
Iteration 155/1000 | Loss: 0.00001306
Iteration 156/1000 | Loss: 0.00001306
Iteration 157/1000 | Loss: 0.00001305
Iteration 158/1000 | Loss: 0.00001305
Iteration 159/1000 | Loss: 0.00001305
Iteration 160/1000 | Loss: 0.00001305
Iteration 161/1000 | Loss: 0.00001305
Iteration 162/1000 | Loss: 0.00001305
Iteration 163/1000 | Loss: 0.00001305
Iteration 164/1000 | Loss: 0.00001305
Iteration 165/1000 | Loss: 0.00001305
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001305
Iteration 169/1000 | Loss: 0.00001305
Iteration 170/1000 | Loss: 0.00001304
Iteration 171/1000 | Loss: 0.00001304
Iteration 172/1000 | Loss: 0.00001304
Iteration 173/1000 | Loss: 0.00001304
Iteration 174/1000 | Loss: 0.00001304
Iteration 175/1000 | Loss: 0.00001304
Iteration 176/1000 | Loss: 0.00001304
Iteration 177/1000 | Loss: 0.00001304
Iteration 178/1000 | Loss: 0.00001304
Iteration 179/1000 | Loss: 0.00001304
Iteration 180/1000 | Loss: 0.00001304
Iteration 181/1000 | Loss: 0.00001304
Iteration 182/1000 | Loss: 0.00001304
Iteration 183/1000 | Loss: 0.00001304
Iteration 184/1000 | Loss: 0.00001304
Iteration 185/1000 | Loss: 0.00001304
Iteration 186/1000 | Loss: 0.00001304
Iteration 187/1000 | Loss: 0.00001304
Iteration 188/1000 | Loss: 0.00001304
Iteration 189/1000 | Loss: 0.00001304
Iteration 190/1000 | Loss: 0.00001304
Iteration 191/1000 | Loss: 0.00001304
Iteration 192/1000 | Loss: 0.00001304
Iteration 193/1000 | Loss: 0.00001304
Iteration 194/1000 | Loss: 0.00001304
Iteration 195/1000 | Loss: 0.00001304
Iteration 196/1000 | Loss: 0.00001304
Iteration 197/1000 | Loss: 0.00001304
Iteration 198/1000 | Loss: 0.00001304
Iteration 199/1000 | Loss: 0.00001304
Iteration 200/1000 | Loss: 0.00001304
Iteration 201/1000 | Loss: 0.00001304
Iteration 202/1000 | Loss: 0.00001304
Iteration 203/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.304302531934809e-05, 1.304302531934809e-05, 1.304302531934809e-05, 1.304302531934809e-05, 1.304302531934809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.304302531934809e-05

Optimization complete. Final v2v error: 3.0938808917999268 mm

Highest mean error: 3.407317638397217 mm for frame 37

Lowest mean error: 2.8030617237091064 mm for frame 63

Saving results

Total time: 91.9551956653595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090376
Iteration 2/25 | Loss: 0.00350344
Iteration 3/25 | Loss: 0.00299092
Iteration 4/25 | Loss: 0.00214665
Iteration 5/25 | Loss: 0.00220674
Iteration 6/25 | Loss: 0.00202859
Iteration 7/25 | Loss: 0.00194109
Iteration 8/25 | Loss: 0.00191241
Iteration 9/25 | Loss: 0.00189512
Iteration 10/25 | Loss: 0.00186879
Iteration 11/25 | Loss: 0.00188094
Iteration 12/25 | Loss: 0.00188967
Iteration 13/25 | Loss: 0.00179403
Iteration 14/25 | Loss: 0.00177773
Iteration 15/25 | Loss: 0.00177400
Iteration 16/25 | Loss: 0.00177260
Iteration 17/25 | Loss: 0.00177227
Iteration 18/25 | Loss: 0.00178077
Iteration 19/25 | Loss: 0.00176662
Iteration 20/25 | Loss: 0.00176334
Iteration 21/25 | Loss: 0.00176278
Iteration 22/25 | Loss: 0.00176265
Iteration 23/25 | Loss: 0.00176265
Iteration 24/25 | Loss: 0.00176265
Iteration 25/25 | Loss: 0.00176265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.41975492
Iteration 2/25 | Loss: 0.00697925
Iteration 3/25 | Loss: 0.00188287
Iteration 4/25 | Loss: 0.00188287
Iteration 5/25 | Loss: 0.00188287
Iteration 6/25 | Loss: 0.00188287
Iteration 7/25 | Loss: 0.00188287
Iteration 8/25 | Loss: 0.00188287
Iteration 9/25 | Loss: 0.00188287
Iteration 10/25 | Loss: 0.00188287
Iteration 11/25 | Loss: 0.00188287
Iteration 12/25 | Loss: 0.00188287
Iteration 13/25 | Loss: 0.00188287
Iteration 14/25 | Loss: 0.00188287
Iteration 15/25 | Loss: 0.00188287
Iteration 16/25 | Loss: 0.00188287
Iteration 17/25 | Loss: 0.00188287
Iteration 18/25 | Loss: 0.00188287
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018828670727089047, 0.0018828670727089047, 0.0018828670727089047, 0.0018828670727089047, 0.0018828670727089047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018828670727089047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188287
Iteration 2/1000 | Loss: 0.00248164
Iteration 3/1000 | Loss: 0.00260773
Iteration 4/1000 | Loss: 0.00361673
Iteration 5/1000 | Loss: 0.00208517
Iteration 6/1000 | Loss: 0.00283652
Iteration 7/1000 | Loss: 0.00309166
Iteration 8/1000 | Loss: 0.00394695
Iteration 9/1000 | Loss: 0.00275267
Iteration 10/1000 | Loss: 0.00246702
Iteration 11/1000 | Loss: 0.00133243
Iteration 12/1000 | Loss: 0.00141058
Iteration 13/1000 | Loss: 0.00131234
Iteration 14/1000 | Loss: 0.00112320
Iteration 15/1000 | Loss: 0.00079101
Iteration 16/1000 | Loss: 0.00093609
Iteration 17/1000 | Loss: 0.00067330
Iteration 18/1000 | Loss: 0.00071046
Iteration 19/1000 | Loss: 0.00059663
Iteration 20/1000 | Loss: 0.00053654
Iteration 21/1000 | Loss: 0.00042207
Iteration 22/1000 | Loss: 0.00037471
Iteration 23/1000 | Loss: 0.00022193
Iteration 24/1000 | Loss: 0.00022093
Iteration 25/1000 | Loss: 0.00018423
Iteration 26/1000 | Loss: 0.00056935
Iteration 27/1000 | Loss: 0.00015653
Iteration 28/1000 | Loss: 0.00014018
Iteration 29/1000 | Loss: 0.00012279
Iteration 30/1000 | Loss: 0.00011096
Iteration 31/1000 | Loss: 0.00009877
Iteration 32/1000 | Loss: 0.00008914
Iteration 33/1000 | Loss: 0.00008003
Iteration 34/1000 | Loss: 0.00007303
Iteration 35/1000 | Loss: 0.00006901
Iteration 36/1000 | Loss: 0.00006556
Iteration 37/1000 | Loss: 0.00006325
Iteration 38/1000 | Loss: 0.00006124
Iteration 39/1000 | Loss: 0.00005975
Iteration 40/1000 | Loss: 0.00005863
Iteration 41/1000 | Loss: 0.00005737
Iteration 42/1000 | Loss: 0.00005643
Iteration 43/1000 | Loss: 0.00005552
Iteration 44/1000 | Loss: 0.00005488
Iteration 45/1000 | Loss: 0.00005435
Iteration 46/1000 | Loss: 0.00005401
Iteration 47/1000 | Loss: 0.00005377
Iteration 48/1000 | Loss: 0.00005364
Iteration 49/1000 | Loss: 0.00005351
Iteration 50/1000 | Loss: 0.00005346
Iteration 51/1000 | Loss: 0.00005332
Iteration 52/1000 | Loss: 0.00005319
Iteration 53/1000 | Loss: 0.00005313
Iteration 54/1000 | Loss: 0.00005312
Iteration 55/1000 | Loss: 0.00005310
Iteration 56/1000 | Loss: 0.00005309
Iteration 57/1000 | Loss: 0.00005308
Iteration 58/1000 | Loss: 0.00005307
Iteration 59/1000 | Loss: 0.00005307
Iteration 60/1000 | Loss: 0.00005307
Iteration 61/1000 | Loss: 0.00005307
Iteration 62/1000 | Loss: 0.00005307
Iteration 63/1000 | Loss: 0.00005307
Iteration 64/1000 | Loss: 0.00005306
Iteration 65/1000 | Loss: 0.00005306
Iteration 66/1000 | Loss: 0.00005306
Iteration 67/1000 | Loss: 0.00005305
Iteration 68/1000 | Loss: 0.00005305
Iteration 69/1000 | Loss: 0.00005304
Iteration 70/1000 | Loss: 0.00005304
Iteration 71/1000 | Loss: 0.00005304
Iteration 72/1000 | Loss: 0.00005303
Iteration 73/1000 | Loss: 0.00005303
Iteration 74/1000 | Loss: 0.00005302
Iteration 75/1000 | Loss: 0.00005301
Iteration 76/1000 | Loss: 0.00005300
Iteration 77/1000 | Loss: 0.00005298
Iteration 78/1000 | Loss: 0.00005297
Iteration 79/1000 | Loss: 0.00005297
Iteration 80/1000 | Loss: 0.00005297
Iteration 81/1000 | Loss: 0.00005296
Iteration 82/1000 | Loss: 0.00005296
Iteration 83/1000 | Loss: 0.00005296
Iteration 84/1000 | Loss: 0.00005296
Iteration 85/1000 | Loss: 0.00005296
Iteration 86/1000 | Loss: 0.00005296
Iteration 87/1000 | Loss: 0.00005296
Iteration 88/1000 | Loss: 0.00005296
Iteration 89/1000 | Loss: 0.00005296
Iteration 90/1000 | Loss: 0.00005296
Iteration 91/1000 | Loss: 0.00005296
Iteration 92/1000 | Loss: 0.00005296
Iteration 93/1000 | Loss: 0.00005295
Iteration 94/1000 | Loss: 0.00005295
Iteration 95/1000 | Loss: 0.00005294
Iteration 96/1000 | Loss: 0.00005293
Iteration 97/1000 | Loss: 0.00005293
Iteration 98/1000 | Loss: 0.00005292
Iteration 99/1000 | Loss: 0.00005291
Iteration 100/1000 | Loss: 0.00005291
Iteration 101/1000 | Loss: 0.00005291
Iteration 102/1000 | Loss: 0.00005291
Iteration 103/1000 | Loss: 0.00005291
Iteration 104/1000 | Loss: 0.00005291
Iteration 105/1000 | Loss: 0.00005290
Iteration 106/1000 | Loss: 0.00005289
Iteration 107/1000 | Loss: 0.00005289
Iteration 108/1000 | Loss: 0.00005289
Iteration 109/1000 | Loss: 0.00005289
Iteration 110/1000 | Loss: 0.00005289
Iteration 111/1000 | Loss: 0.00005288
Iteration 112/1000 | Loss: 0.00005288
Iteration 113/1000 | Loss: 0.00005288
Iteration 114/1000 | Loss: 0.00005288
Iteration 115/1000 | Loss: 0.00005288
Iteration 116/1000 | Loss: 0.00005288
Iteration 117/1000 | Loss: 0.00005288
Iteration 118/1000 | Loss: 0.00005288
Iteration 119/1000 | Loss: 0.00005288
Iteration 120/1000 | Loss: 0.00005287
Iteration 121/1000 | Loss: 0.00005287
Iteration 122/1000 | Loss: 0.00005287
Iteration 123/1000 | Loss: 0.00005287
Iteration 124/1000 | Loss: 0.00005287
Iteration 125/1000 | Loss: 0.00005287
Iteration 126/1000 | Loss: 0.00005287
Iteration 127/1000 | Loss: 0.00005287
Iteration 128/1000 | Loss: 0.00005286
Iteration 129/1000 | Loss: 0.00005286
Iteration 130/1000 | Loss: 0.00005286
Iteration 131/1000 | Loss: 0.00005286
Iteration 132/1000 | Loss: 0.00005286
Iteration 133/1000 | Loss: 0.00005286
Iteration 134/1000 | Loss: 0.00005286
Iteration 135/1000 | Loss: 0.00005286
Iteration 136/1000 | Loss: 0.00005286
Iteration 137/1000 | Loss: 0.00005286
Iteration 138/1000 | Loss: 0.00005286
Iteration 139/1000 | Loss: 0.00005286
Iteration 140/1000 | Loss: 0.00005286
Iteration 141/1000 | Loss: 0.00005286
Iteration 142/1000 | Loss: 0.00005286
Iteration 143/1000 | Loss: 0.00005285
Iteration 144/1000 | Loss: 0.00005285
Iteration 145/1000 | Loss: 0.00005285
Iteration 146/1000 | Loss: 0.00005285
Iteration 147/1000 | Loss: 0.00005285
Iteration 148/1000 | Loss: 0.00005285
Iteration 149/1000 | Loss: 0.00005285
Iteration 150/1000 | Loss: 0.00005285
Iteration 151/1000 | Loss: 0.00005284
Iteration 152/1000 | Loss: 0.00005284
Iteration 153/1000 | Loss: 0.00005284
Iteration 154/1000 | Loss: 0.00005284
Iteration 155/1000 | Loss: 0.00005284
Iteration 156/1000 | Loss: 0.00005284
Iteration 157/1000 | Loss: 0.00005283
Iteration 158/1000 | Loss: 0.00005283
Iteration 159/1000 | Loss: 0.00005283
Iteration 160/1000 | Loss: 0.00005282
Iteration 161/1000 | Loss: 0.00005282
Iteration 162/1000 | Loss: 0.00005282
Iteration 163/1000 | Loss: 0.00005281
Iteration 164/1000 | Loss: 0.00005281
Iteration 165/1000 | Loss: 0.00005281
Iteration 166/1000 | Loss: 0.00005281
Iteration 167/1000 | Loss: 0.00005281
Iteration 168/1000 | Loss: 0.00005281
Iteration 169/1000 | Loss: 0.00005281
Iteration 170/1000 | Loss: 0.00005281
Iteration 171/1000 | Loss: 0.00005281
Iteration 172/1000 | Loss: 0.00005281
Iteration 173/1000 | Loss: 0.00005281
Iteration 174/1000 | Loss: 0.00005280
Iteration 175/1000 | Loss: 0.00005280
Iteration 176/1000 | Loss: 0.00005280
Iteration 177/1000 | Loss: 0.00005280
Iteration 178/1000 | Loss: 0.00005280
Iteration 179/1000 | Loss: 0.00005279
Iteration 180/1000 | Loss: 0.00005279
Iteration 181/1000 | Loss: 0.00005279
Iteration 182/1000 | Loss: 0.00005279
Iteration 183/1000 | Loss: 0.00005279
Iteration 184/1000 | Loss: 0.00005279
Iteration 185/1000 | Loss: 0.00005279
Iteration 186/1000 | Loss: 0.00005279
Iteration 187/1000 | Loss: 0.00005279
Iteration 188/1000 | Loss: 0.00005279
Iteration 189/1000 | Loss: 0.00005279
Iteration 190/1000 | Loss: 0.00005279
Iteration 191/1000 | Loss: 0.00005279
Iteration 192/1000 | Loss: 0.00005279
Iteration 193/1000 | Loss: 0.00005279
Iteration 194/1000 | Loss: 0.00005279
Iteration 195/1000 | Loss: 0.00005279
Iteration 196/1000 | Loss: 0.00005279
Iteration 197/1000 | Loss: 0.00005279
Iteration 198/1000 | Loss: 0.00005279
Iteration 199/1000 | Loss: 0.00005278
Iteration 200/1000 | Loss: 0.00005278
Iteration 201/1000 | Loss: 0.00005278
Iteration 202/1000 | Loss: 0.00005278
Iteration 203/1000 | Loss: 0.00005278
Iteration 204/1000 | Loss: 0.00005278
Iteration 205/1000 | Loss: 0.00005278
Iteration 206/1000 | Loss: 0.00005278
Iteration 207/1000 | Loss: 0.00005278
Iteration 208/1000 | Loss: 0.00005278
Iteration 209/1000 | Loss: 0.00005278
Iteration 210/1000 | Loss: 0.00005278
Iteration 211/1000 | Loss: 0.00005278
Iteration 212/1000 | Loss: 0.00005278
Iteration 213/1000 | Loss: 0.00005278
Iteration 214/1000 | Loss: 0.00005278
Iteration 215/1000 | Loss: 0.00005278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [5.278269964037463e-05, 5.278269964037463e-05, 5.278269964037463e-05, 5.278269964037463e-05, 5.278269964037463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.278269964037463e-05

Optimization complete. Final v2v error: 5.853005409240723 mm

Highest mean error: 7.343651294708252 mm for frame 58

Lowest mean error: 4.827154636383057 mm for frame 121

Saving results

Total time: 121.98660039901733
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040703
Iteration 2/25 | Loss: 0.00150900
Iteration 3/25 | Loss: 0.00130485
Iteration 4/25 | Loss: 0.00128301
Iteration 5/25 | Loss: 0.00128030
Iteration 6/25 | Loss: 0.00128356
Iteration 7/25 | Loss: 0.00127463
Iteration 8/25 | Loss: 0.00128168
Iteration 9/25 | Loss: 0.00128481
Iteration 10/25 | Loss: 0.00127898
Iteration 11/25 | Loss: 0.00128372
Iteration 12/25 | Loss: 0.00128350
Iteration 13/25 | Loss: 0.00128168
Iteration 14/25 | Loss: 0.00128224
Iteration 15/25 | Loss: 0.00128062
Iteration 16/25 | Loss: 0.00128324
Iteration 17/25 | Loss: 0.00128127
Iteration 18/25 | Loss: 0.00127983
Iteration 19/25 | Loss: 0.00128088
Iteration 20/25 | Loss: 0.00128347
Iteration 21/25 | Loss: 0.00128225
Iteration 22/25 | Loss: 0.00127789
Iteration 23/25 | Loss: 0.00127386
Iteration 24/25 | Loss: 0.00127192
Iteration 25/25 | Loss: 0.00127224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24884748
Iteration 2/25 | Loss: 0.00117693
Iteration 3/25 | Loss: 0.00117693
Iteration 4/25 | Loss: 0.00117693
Iteration 5/25 | Loss: 0.00117693
Iteration 6/25 | Loss: 0.00117693
Iteration 7/25 | Loss: 0.00117693
Iteration 8/25 | Loss: 0.00117693
Iteration 9/25 | Loss: 0.00117693
Iteration 10/25 | Loss: 0.00117693
Iteration 11/25 | Loss: 0.00117693
Iteration 12/25 | Loss: 0.00117693
Iteration 13/25 | Loss: 0.00117693
Iteration 14/25 | Loss: 0.00117693
Iteration 15/25 | Loss: 0.00117693
Iteration 16/25 | Loss: 0.00117693
Iteration 17/25 | Loss: 0.00117693
Iteration 18/25 | Loss: 0.00117693
Iteration 19/25 | Loss: 0.00117693
Iteration 20/25 | Loss: 0.00117693
Iteration 21/25 | Loss: 0.00117693
Iteration 22/25 | Loss: 0.00117693
Iteration 23/25 | Loss: 0.00117693
Iteration 24/25 | Loss: 0.00117693
Iteration 25/25 | Loss: 0.00117693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117693
Iteration 2/1000 | Loss: 0.00035589
Iteration 3/1000 | Loss: 0.00024226
Iteration 4/1000 | Loss: 0.00020265
Iteration 5/1000 | Loss: 0.00032349
Iteration 6/1000 | Loss: 0.00022458
Iteration 7/1000 | Loss: 0.00030812
Iteration 8/1000 | Loss: 0.00017454
Iteration 9/1000 | Loss: 0.00010444
Iteration 10/1000 | Loss: 0.00014850
Iteration 11/1000 | Loss: 0.00008670
Iteration 12/1000 | Loss: 0.00013551
Iteration 13/1000 | Loss: 0.00031058
Iteration 14/1000 | Loss: 0.00020608
Iteration 15/1000 | Loss: 0.00025186
Iteration 16/1000 | Loss: 0.00019955
Iteration 17/1000 | Loss: 0.00023340
Iteration 18/1000 | Loss: 0.00016779
Iteration 19/1000 | Loss: 0.00015178
Iteration 20/1000 | Loss: 0.00006149
Iteration 21/1000 | Loss: 0.00005199
Iteration 22/1000 | Loss: 0.00008316
Iteration 23/1000 | Loss: 0.00010580
Iteration 24/1000 | Loss: 0.00008427
Iteration 25/1000 | Loss: 0.00008464
Iteration 26/1000 | Loss: 0.00007614
Iteration 27/1000 | Loss: 0.00010534
Iteration 28/1000 | Loss: 0.00010849
Iteration 29/1000 | Loss: 0.00007375
Iteration 30/1000 | Loss: 0.00013293
Iteration 31/1000 | Loss: 0.00012172
Iteration 32/1000 | Loss: 0.00010568
Iteration 33/1000 | Loss: 0.00009306
Iteration 34/1000 | Loss: 0.00008798
Iteration 35/1000 | Loss: 0.00007079
Iteration 36/1000 | Loss: 0.00004197
Iteration 37/1000 | Loss: 0.00017329
Iteration 38/1000 | Loss: 0.00020463
Iteration 39/1000 | Loss: 0.00013330
Iteration 40/1000 | Loss: 0.00011504
Iteration 41/1000 | Loss: 0.00006635
Iteration 42/1000 | Loss: 0.00009675
Iteration 43/1000 | Loss: 0.00016001
Iteration 44/1000 | Loss: 0.00014102
Iteration 45/1000 | Loss: 0.00019311
Iteration 46/1000 | Loss: 0.00021224
Iteration 47/1000 | Loss: 0.00019469
Iteration 48/1000 | Loss: 0.00011430
Iteration 49/1000 | Loss: 0.00011484
Iteration 50/1000 | Loss: 0.00015932
Iteration 51/1000 | Loss: 0.00018193
Iteration 52/1000 | Loss: 0.00014284
Iteration 53/1000 | Loss: 0.00015960
Iteration 54/1000 | Loss: 0.00015356
Iteration 55/1000 | Loss: 0.00018310
Iteration 56/1000 | Loss: 0.00017755
Iteration 57/1000 | Loss: 0.00012452
Iteration 58/1000 | Loss: 0.00007641
Iteration 59/1000 | Loss: 0.00009427
Iteration 60/1000 | Loss: 0.00010915
Iteration 61/1000 | Loss: 0.00013251
Iteration 62/1000 | Loss: 0.00017247
Iteration 63/1000 | Loss: 0.00015362
Iteration 64/1000 | Loss: 0.00015626
Iteration 65/1000 | Loss: 0.00014017
Iteration 66/1000 | Loss: 0.00013700
Iteration 67/1000 | Loss: 0.00021188
Iteration 68/1000 | Loss: 0.00028473
Iteration 69/1000 | Loss: 0.00023846
Iteration 70/1000 | Loss: 0.00027199
Iteration 71/1000 | Loss: 0.00021128
Iteration 72/1000 | Loss: 0.00028090
Iteration 73/1000 | Loss: 0.00031974
Iteration 74/1000 | Loss: 0.00039706
Iteration 75/1000 | Loss: 0.00015434
Iteration 76/1000 | Loss: 0.00023379
Iteration 77/1000 | Loss: 0.00018578
Iteration 78/1000 | Loss: 0.00020104
Iteration 79/1000 | Loss: 0.00031981
Iteration 80/1000 | Loss: 0.00022992
Iteration 81/1000 | Loss: 0.00024404
Iteration 82/1000 | Loss: 0.00024182
Iteration 83/1000 | Loss: 0.00028276
Iteration 84/1000 | Loss: 0.00024981
Iteration 85/1000 | Loss: 0.00018034
Iteration 86/1000 | Loss: 0.00026755
Iteration 87/1000 | Loss: 0.00016967
Iteration 88/1000 | Loss: 0.00030839
Iteration 89/1000 | Loss: 0.00019876
Iteration 90/1000 | Loss: 0.00027379
Iteration 91/1000 | Loss: 0.00018995
Iteration 92/1000 | Loss: 0.00028390
Iteration 93/1000 | Loss: 0.00059348
Iteration 94/1000 | Loss: 0.00027988
Iteration 95/1000 | Loss: 0.00024763
Iteration 96/1000 | Loss: 0.00020177
Iteration 97/1000 | Loss: 0.00021445
Iteration 98/1000 | Loss: 0.00018097
Iteration 99/1000 | Loss: 0.00019341
Iteration 100/1000 | Loss: 0.00024661
Iteration 101/1000 | Loss: 0.00020071
Iteration 102/1000 | Loss: 0.00025250
Iteration 103/1000 | Loss: 0.00023669
Iteration 104/1000 | Loss: 0.00021674
Iteration 105/1000 | Loss: 0.00026418
Iteration 106/1000 | Loss: 0.00023555
Iteration 107/1000 | Loss: 0.00021611
Iteration 108/1000 | Loss: 0.00025890
Iteration 109/1000 | Loss: 0.00014898
Iteration 110/1000 | Loss: 0.00008596
Iteration 111/1000 | Loss: 0.00006383
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00014546
Iteration 114/1000 | Loss: 0.00011197
Iteration 115/1000 | Loss: 0.00006336
Iteration 116/1000 | Loss: 0.00011556
Iteration 117/1000 | Loss: 0.00004320
Iteration 118/1000 | Loss: 0.00005618
Iteration 119/1000 | Loss: 0.00013773
Iteration 120/1000 | Loss: 0.00012792
Iteration 121/1000 | Loss: 0.00012531
Iteration 122/1000 | Loss: 0.00011815
Iteration 123/1000 | Loss: 0.00002737
Iteration 124/1000 | Loss: 0.00007159
Iteration 125/1000 | Loss: 0.00003462
Iteration 126/1000 | Loss: 0.00004047
Iteration 127/1000 | Loss: 0.00003035
Iteration 128/1000 | Loss: 0.00007976
Iteration 129/1000 | Loss: 0.00010530
Iteration 130/1000 | Loss: 0.00010487
Iteration 131/1000 | Loss: 0.00013864
Iteration 132/1000 | Loss: 0.00010045
Iteration 133/1000 | Loss: 0.00013978
Iteration 134/1000 | Loss: 0.00005470
Iteration 135/1000 | Loss: 0.00012464
Iteration 136/1000 | Loss: 0.00015240
Iteration 137/1000 | Loss: 0.00013789
Iteration 138/1000 | Loss: 0.00010852
Iteration 139/1000 | Loss: 0.00010651
Iteration 140/1000 | Loss: 0.00012523
Iteration 141/1000 | Loss: 0.00010533
Iteration 142/1000 | Loss: 0.00011034
Iteration 143/1000 | Loss: 0.00007322
Iteration 144/1000 | Loss: 0.00005919
Iteration 145/1000 | Loss: 0.00005992
Iteration 146/1000 | Loss: 0.00006085
Iteration 147/1000 | Loss: 0.00011253
Iteration 148/1000 | Loss: 0.00010689
Iteration 149/1000 | Loss: 0.00007400
Iteration 150/1000 | Loss: 0.00006972
Iteration 151/1000 | Loss: 0.00011183
Iteration 152/1000 | Loss: 0.00008204
Iteration 153/1000 | Loss: 0.00007067
Iteration 154/1000 | Loss: 0.00044265
Iteration 155/1000 | Loss: 0.00004250
Iteration 156/1000 | Loss: 0.00007962
Iteration 157/1000 | Loss: 0.00009808
Iteration 158/1000 | Loss: 0.00007676
Iteration 159/1000 | Loss: 0.00011429
Iteration 160/1000 | Loss: 0.00008815
Iteration 161/1000 | Loss: 0.00009047
Iteration 162/1000 | Loss: 0.00010582
Iteration 163/1000 | Loss: 0.00005170
Iteration 164/1000 | Loss: 0.00009213
Iteration 165/1000 | Loss: 0.00005844
Iteration 166/1000 | Loss: 0.00010706
Iteration 167/1000 | Loss: 0.00011575
Iteration 168/1000 | Loss: 0.00011156
Iteration 169/1000 | Loss: 0.00006894
Iteration 170/1000 | Loss: 0.00001867
Iteration 171/1000 | Loss: 0.00001497
Iteration 172/1000 | Loss: 0.00001377
Iteration 173/1000 | Loss: 0.00001317
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001274
Iteration 176/1000 | Loss: 0.00001259
Iteration 177/1000 | Loss: 0.00001254
Iteration 178/1000 | Loss: 0.00001241
Iteration 179/1000 | Loss: 0.00001232
Iteration 180/1000 | Loss: 0.00001231
Iteration 181/1000 | Loss: 0.00001229
Iteration 182/1000 | Loss: 0.00001222
Iteration 183/1000 | Loss: 0.00001217
Iteration 184/1000 | Loss: 0.00001217
Iteration 185/1000 | Loss: 0.00001217
Iteration 186/1000 | Loss: 0.00001217
Iteration 187/1000 | Loss: 0.00001217
Iteration 188/1000 | Loss: 0.00001217
Iteration 189/1000 | Loss: 0.00001217
Iteration 190/1000 | Loss: 0.00001217
Iteration 191/1000 | Loss: 0.00001217
Iteration 192/1000 | Loss: 0.00001217
Iteration 193/1000 | Loss: 0.00001217
Iteration 194/1000 | Loss: 0.00001216
Iteration 195/1000 | Loss: 0.00001216
Iteration 196/1000 | Loss: 0.00001215
Iteration 197/1000 | Loss: 0.00001214
Iteration 198/1000 | Loss: 0.00001214
Iteration 199/1000 | Loss: 0.00001213
Iteration 200/1000 | Loss: 0.00001212
Iteration 201/1000 | Loss: 0.00001212
Iteration 202/1000 | Loss: 0.00001211
Iteration 203/1000 | Loss: 0.00001209
Iteration 204/1000 | Loss: 0.00001209
Iteration 205/1000 | Loss: 0.00001208
Iteration 206/1000 | Loss: 0.00001208
Iteration 207/1000 | Loss: 0.00001207
Iteration 208/1000 | Loss: 0.00001206
Iteration 209/1000 | Loss: 0.00001205
Iteration 210/1000 | Loss: 0.00001205
Iteration 211/1000 | Loss: 0.00001205
Iteration 212/1000 | Loss: 0.00001204
Iteration 213/1000 | Loss: 0.00001204
Iteration 214/1000 | Loss: 0.00001203
Iteration 215/1000 | Loss: 0.00001201
Iteration 216/1000 | Loss: 0.00001200
Iteration 217/1000 | Loss: 0.00001200
Iteration 218/1000 | Loss: 0.00001200
Iteration 219/1000 | Loss: 0.00001199
Iteration 220/1000 | Loss: 0.00001199
Iteration 221/1000 | Loss: 0.00001199
Iteration 222/1000 | Loss: 0.00001197
Iteration 223/1000 | Loss: 0.00001197
Iteration 224/1000 | Loss: 0.00001197
Iteration 225/1000 | Loss: 0.00001196
Iteration 226/1000 | Loss: 0.00001195
Iteration 227/1000 | Loss: 0.00001194
Iteration 228/1000 | Loss: 0.00001194
Iteration 229/1000 | Loss: 0.00001194
Iteration 230/1000 | Loss: 0.00001193
Iteration 231/1000 | Loss: 0.00001193
Iteration 232/1000 | Loss: 0.00001193
Iteration 233/1000 | Loss: 0.00001193
Iteration 234/1000 | Loss: 0.00001191
Iteration 235/1000 | Loss: 0.00001191
Iteration 236/1000 | Loss: 0.00001191
Iteration 237/1000 | Loss: 0.00001191
Iteration 238/1000 | Loss: 0.00001191
Iteration 239/1000 | Loss: 0.00001191
Iteration 240/1000 | Loss: 0.00001191
Iteration 241/1000 | Loss: 0.00001191
Iteration 242/1000 | Loss: 0.00001191
Iteration 243/1000 | Loss: 0.00001191
Iteration 244/1000 | Loss: 0.00001191
Iteration 245/1000 | Loss: 0.00001191
Iteration 246/1000 | Loss: 0.00001191
Iteration 247/1000 | Loss: 0.00001190
Iteration 248/1000 | Loss: 0.00001190
Iteration 249/1000 | Loss: 0.00001190
Iteration 250/1000 | Loss: 0.00001189
Iteration 251/1000 | Loss: 0.00001189
Iteration 252/1000 | Loss: 0.00001189
Iteration 253/1000 | Loss: 0.00001188
Iteration 254/1000 | Loss: 0.00001188
Iteration 255/1000 | Loss: 0.00001188
Iteration 256/1000 | Loss: 0.00001188
Iteration 257/1000 | Loss: 0.00001187
Iteration 258/1000 | Loss: 0.00001187
Iteration 259/1000 | Loss: 0.00001187
Iteration 260/1000 | Loss: 0.00001187
Iteration 261/1000 | Loss: 0.00001186
Iteration 262/1000 | Loss: 0.00001186
Iteration 263/1000 | Loss: 0.00001186
Iteration 264/1000 | Loss: 0.00001186
Iteration 265/1000 | Loss: 0.00001186
Iteration 266/1000 | Loss: 0.00001186
Iteration 267/1000 | Loss: 0.00001186
Iteration 268/1000 | Loss: 0.00001186
Iteration 269/1000 | Loss: 0.00001186
Iteration 270/1000 | Loss: 0.00001186
Iteration 271/1000 | Loss: 0.00001186
Iteration 272/1000 | Loss: 0.00001186
Iteration 273/1000 | Loss: 0.00001186
Iteration 274/1000 | Loss: 0.00001185
Iteration 275/1000 | Loss: 0.00001185
Iteration 276/1000 | Loss: 0.00001185
Iteration 277/1000 | Loss: 0.00001185
Iteration 278/1000 | Loss: 0.00001185
Iteration 279/1000 | Loss: 0.00001185
Iteration 280/1000 | Loss: 0.00001185
Iteration 281/1000 | Loss: 0.00001185
Iteration 282/1000 | Loss: 0.00001185
Iteration 283/1000 | Loss: 0.00001184
Iteration 284/1000 | Loss: 0.00001184
Iteration 285/1000 | Loss: 0.00001184
Iteration 286/1000 | Loss: 0.00001184
Iteration 287/1000 | Loss: 0.00001184
Iteration 288/1000 | Loss: 0.00001184
Iteration 289/1000 | Loss: 0.00001184
Iteration 290/1000 | Loss: 0.00001184
Iteration 291/1000 | Loss: 0.00001184
Iteration 292/1000 | Loss: 0.00001184
Iteration 293/1000 | Loss: 0.00001184
Iteration 294/1000 | Loss: 0.00001184
Iteration 295/1000 | Loss: 0.00001183
Iteration 296/1000 | Loss: 0.00001183
Iteration 297/1000 | Loss: 0.00001183
Iteration 298/1000 | Loss: 0.00001183
Iteration 299/1000 | Loss: 0.00001183
Iteration 300/1000 | Loss: 0.00001183
Iteration 301/1000 | Loss: 0.00001183
Iteration 302/1000 | Loss: 0.00001183
Iteration 303/1000 | Loss: 0.00001183
Iteration 304/1000 | Loss: 0.00001183
Iteration 305/1000 | Loss: 0.00001183
Iteration 306/1000 | Loss: 0.00001183
Iteration 307/1000 | Loss: 0.00001183
Iteration 308/1000 | Loss: 0.00001183
Iteration 309/1000 | Loss: 0.00001183
Iteration 310/1000 | Loss: 0.00001182
Iteration 311/1000 | Loss: 0.00001182
Iteration 312/1000 | Loss: 0.00001182
Iteration 313/1000 | Loss: 0.00001182
Iteration 314/1000 | Loss: 0.00001182
Iteration 315/1000 | Loss: 0.00001182
Iteration 316/1000 | Loss: 0.00001182
Iteration 317/1000 | Loss: 0.00001182
Iteration 318/1000 | Loss: 0.00001182
Iteration 319/1000 | Loss: 0.00001182
Iteration 320/1000 | Loss: 0.00001182
Iteration 321/1000 | Loss: 0.00001182
Iteration 322/1000 | Loss: 0.00001182
Iteration 323/1000 | Loss: 0.00001182
Iteration 324/1000 | Loss: 0.00001182
Iteration 325/1000 | Loss: 0.00001182
Iteration 326/1000 | Loss: 0.00001182
Iteration 327/1000 | Loss: 0.00001181
Iteration 328/1000 | Loss: 0.00001181
Iteration 329/1000 | Loss: 0.00001181
Iteration 330/1000 | Loss: 0.00001181
Iteration 331/1000 | Loss: 0.00001181
Iteration 332/1000 | Loss: 0.00001181
Iteration 333/1000 | Loss: 0.00001181
Iteration 334/1000 | Loss: 0.00001181
Iteration 335/1000 | Loss: 0.00001181
Iteration 336/1000 | Loss: 0.00001181
Iteration 337/1000 | Loss: 0.00001181
Iteration 338/1000 | Loss: 0.00001181
Iteration 339/1000 | Loss: 0.00001181
Iteration 340/1000 | Loss: 0.00001181
Iteration 341/1000 | Loss: 0.00001181
Iteration 342/1000 | Loss: 0.00001181
Iteration 343/1000 | Loss: 0.00001181
Iteration 344/1000 | Loss: 0.00001181
Iteration 345/1000 | Loss: 0.00001181
Iteration 346/1000 | Loss: 0.00001181
Iteration 347/1000 | Loss: 0.00001181
Iteration 348/1000 | Loss: 0.00001181
Iteration 349/1000 | Loss: 0.00001181
Iteration 350/1000 | Loss: 0.00001181
Iteration 351/1000 | Loss: 0.00001181
Iteration 352/1000 | Loss: 0.00001181
Iteration 353/1000 | Loss: 0.00001181
Iteration 354/1000 | Loss: 0.00001181
Iteration 355/1000 | Loss: 0.00001181
Iteration 356/1000 | Loss: 0.00001181
Iteration 357/1000 | Loss: 0.00001181
Iteration 358/1000 | Loss: 0.00001181
Iteration 359/1000 | Loss: 0.00001181
Iteration 360/1000 | Loss: 0.00001181
Iteration 361/1000 | Loss: 0.00001181
Iteration 362/1000 | Loss: 0.00001181
Iteration 363/1000 | Loss: 0.00001181
Iteration 364/1000 | Loss: 0.00001181
Iteration 365/1000 | Loss: 0.00001181
Iteration 366/1000 | Loss: 0.00001181
Iteration 367/1000 | Loss: 0.00001181
Iteration 368/1000 | Loss: 0.00001181
Iteration 369/1000 | Loss: 0.00001181
Iteration 370/1000 | Loss: 0.00001181
Iteration 371/1000 | Loss: 0.00001181
Iteration 372/1000 | Loss: 0.00001181
Iteration 373/1000 | Loss: 0.00001181
Iteration 374/1000 | Loss: 0.00001181
Iteration 375/1000 | Loss: 0.00001181
Iteration 376/1000 | Loss: 0.00001181
Iteration 377/1000 | Loss: 0.00001181
Iteration 378/1000 | Loss: 0.00001181
Iteration 379/1000 | Loss: 0.00001181
Iteration 380/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 380. Stopping optimization.
Last 5 losses: [1.1813277524197474e-05, 1.1813277524197474e-05, 1.1813277524197474e-05, 1.1813277524197474e-05, 1.1813277524197474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1813277524197474e-05

Optimization complete. Final v2v error: 2.9440698623657227 mm

Highest mean error: 4.090341091156006 mm for frame 49

Lowest mean error: 2.8142871856689453 mm for frame 193

Saving results

Total time: 356.912633895874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392902
Iteration 2/25 | Loss: 0.00130949
Iteration 3/25 | Loss: 0.00123673
Iteration 4/25 | Loss: 0.00122970
Iteration 5/25 | Loss: 0.00122852
Iteration 6/25 | Loss: 0.00122852
Iteration 7/25 | Loss: 0.00122852
Iteration 8/25 | Loss: 0.00122852
Iteration 9/25 | Loss: 0.00122852
Iteration 10/25 | Loss: 0.00122852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012285173870623112, 0.0012285173870623112, 0.0012285173870623112, 0.0012285173870623112, 0.0012285173870623112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012285173870623112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34168088
Iteration 2/25 | Loss: 0.00085756
Iteration 3/25 | Loss: 0.00085755
Iteration 4/25 | Loss: 0.00085755
Iteration 5/25 | Loss: 0.00085755
Iteration 6/25 | Loss: 0.00085755
Iteration 7/25 | Loss: 0.00085755
Iteration 8/25 | Loss: 0.00085755
Iteration 9/25 | Loss: 0.00085755
Iteration 10/25 | Loss: 0.00085755
Iteration 11/25 | Loss: 0.00085755
Iteration 12/25 | Loss: 0.00085755
Iteration 13/25 | Loss: 0.00085755
Iteration 14/25 | Loss: 0.00085755
Iteration 15/25 | Loss: 0.00085755
Iteration 16/25 | Loss: 0.00085755
Iteration 17/25 | Loss: 0.00085755
Iteration 18/25 | Loss: 0.00085755
Iteration 19/25 | Loss: 0.00085755
Iteration 20/25 | Loss: 0.00085755
Iteration 21/25 | Loss: 0.00085755
Iteration 22/25 | Loss: 0.00085755
Iteration 23/25 | Loss: 0.00085755
Iteration 24/25 | Loss: 0.00085755
Iteration 25/25 | Loss: 0.00085755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085754
Iteration 2/1000 | Loss: 0.00002178
Iteration 3/1000 | Loss: 0.00001607
Iteration 4/1000 | Loss: 0.00001510
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001313
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001243
Iteration 10/1000 | Loss: 0.00001235
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001222
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001221
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001208
Iteration 18/1000 | Loss: 0.00001203
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001201
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001185
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001180
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001159
Iteration 60/1000 | Loss: 0.00001159
Iteration 61/1000 | Loss: 0.00001159
Iteration 62/1000 | Loss: 0.00001158
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001157
Iteration 65/1000 | Loss: 0.00001156
Iteration 66/1000 | Loss: 0.00001156
Iteration 67/1000 | Loss: 0.00001156
Iteration 68/1000 | Loss: 0.00001156
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001155
Iteration 73/1000 | Loss: 0.00001155
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001147
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001146
Iteration 88/1000 | Loss: 0.00001146
Iteration 89/1000 | Loss: 0.00001145
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001144
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001143
Iteration 97/1000 | Loss: 0.00001143
Iteration 98/1000 | Loss: 0.00001143
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001141
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001140
Iteration 111/1000 | Loss: 0.00001140
Iteration 112/1000 | Loss: 0.00001140
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001140
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Iteration 126/1000 | Loss: 0.00001139
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001138
Iteration 132/1000 | Loss: 0.00001138
Iteration 133/1000 | Loss: 0.00001138
Iteration 134/1000 | Loss: 0.00001138
Iteration 135/1000 | Loss: 0.00001138
Iteration 136/1000 | Loss: 0.00001138
Iteration 137/1000 | Loss: 0.00001138
Iteration 138/1000 | Loss: 0.00001138
Iteration 139/1000 | Loss: 0.00001137
Iteration 140/1000 | Loss: 0.00001137
Iteration 141/1000 | Loss: 0.00001137
Iteration 142/1000 | Loss: 0.00001137
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.1366911166987848e-05, 1.1366911166987848e-05, 1.1366911166987848e-05, 1.1366911166987848e-05, 1.1366911166987848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1366911166987848e-05

Optimization complete. Final v2v error: 2.909611225128174 mm

Highest mean error: 2.961919069290161 mm for frame 64

Lowest mean error: 2.7827134132385254 mm for frame 235

Saving results

Total time: 42.50345754623413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791695
Iteration 2/25 | Loss: 0.00156149
Iteration 3/25 | Loss: 0.00133580
Iteration 4/25 | Loss: 0.00130111
Iteration 5/25 | Loss: 0.00129831
Iteration 6/25 | Loss: 0.00129772
Iteration 7/25 | Loss: 0.00129658
Iteration 8/25 | Loss: 0.00129518
Iteration 9/25 | Loss: 0.00129471
Iteration 10/25 | Loss: 0.00129402
Iteration 11/25 | Loss: 0.00129351
Iteration 12/25 | Loss: 0.00129331
Iteration 13/25 | Loss: 0.00129324
Iteration 14/25 | Loss: 0.00129320
Iteration 15/25 | Loss: 0.00129320
Iteration 16/25 | Loss: 0.00129319
Iteration 17/25 | Loss: 0.00129319
Iteration 18/25 | Loss: 0.00129319
Iteration 19/25 | Loss: 0.00129319
Iteration 20/25 | Loss: 0.00129319
Iteration 21/25 | Loss: 0.00129318
Iteration 22/25 | Loss: 0.00129318
Iteration 23/25 | Loss: 0.00129318
Iteration 24/25 | Loss: 0.00129318
Iteration 25/25 | Loss: 0.00129318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12427187
Iteration 2/25 | Loss: 0.00123239
Iteration 3/25 | Loss: 0.00120360
Iteration 4/25 | Loss: 0.00120359
Iteration 5/25 | Loss: 0.00120359
Iteration 6/25 | Loss: 0.00120359
Iteration 7/25 | Loss: 0.00120359
Iteration 8/25 | Loss: 0.00120359
Iteration 9/25 | Loss: 0.00120359
Iteration 10/25 | Loss: 0.00120359
Iteration 11/25 | Loss: 0.00120359
Iteration 12/25 | Loss: 0.00120359
Iteration 13/25 | Loss: 0.00120359
Iteration 14/25 | Loss: 0.00120359
Iteration 15/25 | Loss: 0.00120359
Iteration 16/25 | Loss: 0.00120359
Iteration 17/25 | Loss: 0.00120359
Iteration 18/25 | Loss: 0.00120359
Iteration 19/25 | Loss: 0.00120359
Iteration 20/25 | Loss: 0.00120359
Iteration 21/25 | Loss: 0.00120359
Iteration 22/25 | Loss: 0.00120359
Iteration 23/25 | Loss: 0.00120359
Iteration 24/25 | Loss: 0.00120359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012035912368446589, 0.0012035912368446589, 0.0012035912368446589, 0.0012035912368446589, 0.0012035912368446589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012035912368446589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120359
Iteration 2/1000 | Loss: 0.00003977
Iteration 3/1000 | Loss: 0.00003272
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00002977
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00002802
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00002806
Iteration 10/1000 | Loss: 0.00019763
Iteration 11/1000 | Loss: 0.00002908
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001617
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00002413
Iteration 17/1000 | Loss: 0.00001599
Iteration 18/1000 | Loss: 0.00001588
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001587
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001587
Iteration 24/1000 | Loss: 0.00001587
Iteration 25/1000 | Loss: 0.00001587
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001568
Iteration 33/1000 | Loss: 0.00001568
Iteration 34/1000 | Loss: 0.00001567
Iteration 35/1000 | Loss: 0.00001566
Iteration 36/1000 | Loss: 0.00001566
Iteration 37/1000 | Loss: 0.00001566
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001565
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001563
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001559
Iteration 52/1000 | Loss: 0.00001559
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001559
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001555
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001553
Iteration 79/1000 | Loss: 0.00001553
Iteration 80/1000 | Loss: 0.00001553
Iteration 81/1000 | Loss: 0.00001553
Iteration 82/1000 | Loss: 0.00001553
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001552
Iteration 85/1000 | Loss: 0.00001552
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001550
Iteration 90/1000 | Loss: 0.00001550
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001547
Iteration 110/1000 | Loss: 0.00001546
Iteration 111/1000 | Loss: 0.00001546
Iteration 112/1000 | Loss: 0.00001546
Iteration 113/1000 | Loss: 0.00001546
Iteration 114/1000 | Loss: 0.00001546
Iteration 115/1000 | Loss: 0.00001546
Iteration 116/1000 | Loss: 0.00001545
Iteration 117/1000 | Loss: 0.00001545
Iteration 118/1000 | Loss: 0.00001545
Iteration 119/1000 | Loss: 0.00001545
Iteration 120/1000 | Loss: 0.00001544
Iteration 121/1000 | Loss: 0.00001544
Iteration 122/1000 | Loss: 0.00001544
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001542
Iteration 128/1000 | Loss: 0.00001542
Iteration 129/1000 | Loss: 0.00001542
Iteration 130/1000 | Loss: 0.00001542
Iteration 131/1000 | Loss: 0.00001542
Iteration 132/1000 | Loss: 0.00001542
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001541
Iteration 137/1000 | Loss: 0.00001541
Iteration 138/1000 | Loss: 0.00001541
Iteration 139/1000 | Loss: 0.00001541
Iteration 140/1000 | Loss: 0.00001541
Iteration 141/1000 | Loss: 0.00001541
Iteration 142/1000 | Loss: 0.00001541
Iteration 143/1000 | Loss: 0.00001541
Iteration 144/1000 | Loss: 0.00001541
Iteration 145/1000 | Loss: 0.00001541
Iteration 146/1000 | Loss: 0.00001541
Iteration 147/1000 | Loss: 0.00001541
Iteration 148/1000 | Loss: 0.00001540
Iteration 149/1000 | Loss: 0.00001540
Iteration 150/1000 | Loss: 0.00001540
Iteration 151/1000 | Loss: 0.00001540
Iteration 152/1000 | Loss: 0.00001540
Iteration 153/1000 | Loss: 0.00001540
Iteration 154/1000 | Loss: 0.00001539
Iteration 155/1000 | Loss: 0.00001539
Iteration 156/1000 | Loss: 0.00001539
Iteration 157/1000 | Loss: 0.00001539
Iteration 158/1000 | Loss: 0.00001539
Iteration 159/1000 | Loss: 0.00001539
Iteration 160/1000 | Loss: 0.00001539
Iteration 161/1000 | Loss: 0.00001538
Iteration 162/1000 | Loss: 0.00001538
Iteration 163/1000 | Loss: 0.00001538
Iteration 164/1000 | Loss: 0.00001538
Iteration 165/1000 | Loss: 0.00001538
Iteration 166/1000 | Loss: 0.00001538
Iteration 167/1000 | Loss: 0.00001537
Iteration 168/1000 | Loss: 0.00001537
Iteration 169/1000 | Loss: 0.00001537
Iteration 170/1000 | Loss: 0.00001537
Iteration 171/1000 | Loss: 0.00001537
Iteration 172/1000 | Loss: 0.00001537
Iteration 173/1000 | Loss: 0.00001537
Iteration 174/1000 | Loss: 0.00001537
Iteration 175/1000 | Loss: 0.00001537
Iteration 176/1000 | Loss: 0.00001537
Iteration 177/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.5372586858575232e-05, 1.5372586858575232e-05, 1.5372586858575232e-05, 1.5372586858575232e-05, 1.5372586858575232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5372586858575232e-05

Optimization complete. Final v2v error: 3.2746963500976562 mm

Highest mean error: 3.6691417694091797 mm for frame 206

Lowest mean error: 2.9132590293884277 mm for frame 143

Saving results

Total time: 68.26307225227356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795206
Iteration 2/25 | Loss: 0.00174932
Iteration 3/25 | Loss: 0.00158375
Iteration 4/25 | Loss: 0.00155330
Iteration 5/25 | Loss: 0.00154954
Iteration 6/25 | Loss: 0.00156986
Iteration 7/25 | Loss: 0.00152415
Iteration 8/25 | Loss: 0.00150968
Iteration 9/25 | Loss: 0.00150335
Iteration 10/25 | Loss: 0.00148838
Iteration 11/25 | Loss: 0.00147498
Iteration 12/25 | Loss: 0.00146952
Iteration 13/25 | Loss: 0.00146834
Iteration 14/25 | Loss: 0.00146800
Iteration 15/25 | Loss: 0.00146783
Iteration 16/25 | Loss: 0.00146767
Iteration 17/25 | Loss: 0.00147084
Iteration 18/25 | Loss: 0.00146999
Iteration 19/25 | Loss: 0.00146179
Iteration 20/25 | Loss: 0.00145941
Iteration 21/25 | Loss: 0.00145739
Iteration 22/25 | Loss: 0.00145574
Iteration 23/25 | Loss: 0.00145541
Iteration 24/25 | Loss: 0.00145517
Iteration 25/25 | Loss: 0.00146150

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28766036
Iteration 2/25 | Loss: 0.00386503
Iteration 3/25 | Loss: 0.00386500
Iteration 4/25 | Loss: 0.00386500
Iteration 5/25 | Loss: 0.00386500
Iteration 6/25 | Loss: 0.00386500
Iteration 7/25 | Loss: 0.00386500
Iteration 8/25 | Loss: 0.00386500
Iteration 9/25 | Loss: 0.00386500
Iteration 10/25 | Loss: 0.00386500
Iteration 11/25 | Loss: 0.00386500
Iteration 12/25 | Loss: 0.00386500
Iteration 13/25 | Loss: 0.00386500
Iteration 14/25 | Loss: 0.00386500
Iteration 15/25 | Loss: 0.00386500
Iteration 16/25 | Loss: 0.00386500
Iteration 17/25 | Loss: 0.00386500
Iteration 18/25 | Loss: 0.00386500
Iteration 19/25 | Loss: 0.00386500
Iteration 20/25 | Loss: 0.00386500
Iteration 21/25 | Loss: 0.00386500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003865000093355775, 0.003865000093355775, 0.003865000093355775, 0.003865000093355775, 0.003865000093355775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003865000093355775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00386500
Iteration 2/1000 | Loss: 0.00035795
Iteration 3/1000 | Loss: 0.00020795
Iteration 4/1000 | Loss: 0.00016293
Iteration 5/1000 | Loss: 0.00014298
Iteration 6/1000 | Loss: 0.00013384
Iteration 7/1000 | Loss: 0.00012529
Iteration 8/1000 | Loss: 0.00012055
Iteration 9/1000 | Loss: 0.00011483
Iteration 10/1000 | Loss: 0.00011130
Iteration 11/1000 | Loss: 0.00030417
Iteration 12/1000 | Loss: 0.00197637
Iteration 13/1000 | Loss: 0.00262455
Iteration 14/1000 | Loss: 0.00097687
Iteration 15/1000 | Loss: 0.00064222
Iteration 16/1000 | Loss: 0.00041993
Iteration 17/1000 | Loss: 0.00039229
Iteration 18/1000 | Loss: 0.00011506
Iteration 19/1000 | Loss: 0.00009883
Iteration 20/1000 | Loss: 0.00008664
Iteration 21/1000 | Loss: 0.00038285
Iteration 22/1000 | Loss: 0.00008895
Iteration 23/1000 | Loss: 0.00007830
Iteration 24/1000 | Loss: 0.00007379
Iteration 25/1000 | Loss: 0.00007070
Iteration 26/1000 | Loss: 0.00007536
Iteration 27/1000 | Loss: 0.00006876
Iteration 28/1000 | Loss: 0.00006659
Iteration 29/1000 | Loss: 0.00082757
Iteration 30/1000 | Loss: 0.00007592
Iteration 31/1000 | Loss: 0.00007009
Iteration 32/1000 | Loss: 0.00029544
Iteration 33/1000 | Loss: 0.00007052
Iteration 34/1000 | Loss: 0.00006789
Iteration 35/1000 | Loss: 0.00088207
Iteration 36/1000 | Loss: 0.00027755
Iteration 37/1000 | Loss: 0.00057825
Iteration 38/1000 | Loss: 0.00031706
Iteration 39/1000 | Loss: 0.00024207
Iteration 40/1000 | Loss: 0.00010062
Iteration 41/1000 | Loss: 0.00007368
Iteration 42/1000 | Loss: 0.00008664
Iteration 43/1000 | Loss: 0.00007875
Iteration 44/1000 | Loss: 0.00063466
Iteration 45/1000 | Loss: 0.00006951
Iteration 46/1000 | Loss: 0.00052778
Iteration 47/1000 | Loss: 0.00006471
Iteration 48/1000 | Loss: 0.00006028
Iteration 49/1000 | Loss: 0.00005810
Iteration 50/1000 | Loss: 0.00058361
Iteration 51/1000 | Loss: 0.00006028
Iteration 52/1000 | Loss: 0.00065329
Iteration 53/1000 | Loss: 0.00005698
Iteration 54/1000 | Loss: 0.00005369
Iteration 55/1000 | Loss: 0.00056770
Iteration 56/1000 | Loss: 0.00061889
Iteration 57/1000 | Loss: 0.00011664
Iteration 58/1000 | Loss: 0.00005987
Iteration 59/1000 | Loss: 0.00020733
Iteration 60/1000 | Loss: 0.00007044
Iteration 61/1000 | Loss: 0.00005603
Iteration 62/1000 | Loss: 0.00004842
Iteration 63/1000 | Loss: 0.00010703
Iteration 64/1000 | Loss: 0.00117597
Iteration 65/1000 | Loss: 0.00005142
Iteration 66/1000 | Loss: 0.00004549
Iteration 67/1000 | Loss: 0.00004235
Iteration 68/1000 | Loss: 0.00003894
Iteration 69/1000 | Loss: 0.00003697
Iteration 70/1000 | Loss: 0.00062479
Iteration 71/1000 | Loss: 0.00003777
Iteration 72/1000 | Loss: 0.00003518
Iteration 73/1000 | Loss: 0.00003393
Iteration 74/1000 | Loss: 0.00003244
Iteration 75/1000 | Loss: 0.00003143
Iteration 76/1000 | Loss: 0.00066173
Iteration 77/1000 | Loss: 0.00055013
Iteration 78/1000 | Loss: 0.00064795
Iteration 79/1000 | Loss: 0.00105917
Iteration 80/1000 | Loss: 0.00056273
Iteration 81/1000 | Loss: 0.00004081
Iteration 82/1000 | Loss: 0.00003337
Iteration 83/1000 | Loss: 0.00003052
Iteration 84/1000 | Loss: 0.00002911
Iteration 85/1000 | Loss: 0.00002828
Iteration 86/1000 | Loss: 0.00002736
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00065417
Iteration 89/1000 | Loss: 0.00003190
Iteration 90/1000 | Loss: 0.00002686
Iteration 91/1000 | Loss: 0.00002521
Iteration 92/1000 | Loss: 0.00002336
Iteration 93/1000 | Loss: 0.00002240
Iteration 94/1000 | Loss: 0.00002129
Iteration 95/1000 | Loss: 0.00002088
Iteration 96/1000 | Loss: 0.00002075
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002040
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002020
Iteration 101/1000 | Loss: 0.00002016
Iteration 102/1000 | Loss: 0.00002016
Iteration 103/1000 | Loss: 0.00002014
Iteration 104/1000 | Loss: 0.00002012
Iteration 105/1000 | Loss: 0.00002007
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002002
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001998
Iteration 110/1000 | Loss: 0.00001998
Iteration 111/1000 | Loss: 0.00001998
Iteration 112/1000 | Loss: 0.00001998
Iteration 113/1000 | Loss: 0.00001998
Iteration 114/1000 | Loss: 0.00001997
Iteration 115/1000 | Loss: 0.00001997
Iteration 116/1000 | Loss: 0.00001996
Iteration 117/1000 | Loss: 0.00001995
Iteration 118/1000 | Loss: 0.00001995
Iteration 119/1000 | Loss: 0.00001995
Iteration 120/1000 | Loss: 0.00001994
Iteration 121/1000 | Loss: 0.00001994
Iteration 122/1000 | Loss: 0.00001994
Iteration 123/1000 | Loss: 0.00001994
Iteration 124/1000 | Loss: 0.00001993
Iteration 125/1000 | Loss: 0.00001992
Iteration 126/1000 | Loss: 0.00001992
Iteration 127/1000 | Loss: 0.00001990
Iteration 128/1000 | Loss: 0.00001990
Iteration 129/1000 | Loss: 0.00001990
Iteration 130/1000 | Loss: 0.00001990
Iteration 131/1000 | Loss: 0.00001990
Iteration 132/1000 | Loss: 0.00001990
Iteration 133/1000 | Loss: 0.00001990
Iteration 134/1000 | Loss: 0.00001990
Iteration 135/1000 | Loss: 0.00001990
Iteration 136/1000 | Loss: 0.00001990
Iteration 137/1000 | Loss: 0.00001990
Iteration 138/1000 | Loss: 0.00001989
Iteration 139/1000 | Loss: 0.00001989
Iteration 140/1000 | Loss: 0.00001989
Iteration 141/1000 | Loss: 0.00001989
Iteration 142/1000 | Loss: 0.00001989
Iteration 143/1000 | Loss: 0.00001989
Iteration 144/1000 | Loss: 0.00001989
Iteration 145/1000 | Loss: 0.00001989
Iteration 146/1000 | Loss: 0.00001989
Iteration 147/1000 | Loss: 0.00001988
Iteration 148/1000 | Loss: 0.00001988
Iteration 149/1000 | Loss: 0.00001988
Iteration 150/1000 | Loss: 0.00001988
Iteration 151/1000 | Loss: 0.00001988
Iteration 152/1000 | Loss: 0.00001988
Iteration 153/1000 | Loss: 0.00001988
Iteration 154/1000 | Loss: 0.00001987
Iteration 155/1000 | Loss: 0.00001987
Iteration 156/1000 | Loss: 0.00001987
Iteration 157/1000 | Loss: 0.00001987
Iteration 158/1000 | Loss: 0.00001987
Iteration 159/1000 | Loss: 0.00001986
Iteration 160/1000 | Loss: 0.00001986
Iteration 161/1000 | Loss: 0.00001986
Iteration 162/1000 | Loss: 0.00001986
Iteration 163/1000 | Loss: 0.00001986
Iteration 164/1000 | Loss: 0.00001986
Iteration 165/1000 | Loss: 0.00001986
Iteration 166/1000 | Loss: 0.00001986
Iteration 167/1000 | Loss: 0.00001985
Iteration 168/1000 | Loss: 0.00001985
Iteration 169/1000 | Loss: 0.00001985
Iteration 170/1000 | Loss: 0.00001985
Iteration 171/1000 | Loss: 0.00001985
Iteration 172/1000 | Loss: 0.00001985
Iteration 173/1000 | Loss: 0.00001985
Iteration 174/1000 | Loss: 0.00001985
Iteration 175/1000 | Loss: 0.00001985
Iteration 176/1000 | Loss: 0.00001985
Iteration 177/1000 | Loss: 0.00001985
Iteration 178/1000 | Loss: 0.00001985
Iteration 179/1000 | Loss: 0.00001984
Iteration 180/1000 | Loss: 0.00001984
Iteration 181/1000 | Loss: 0.00001984
Iteration 182/1000 | Loss: 0.00001984
Iteration 183/1000 | Loss: 0.00001984
Iteration 184/1000 | Loss: 0.00001983
Iteration 185/1000 | Loss: 0.00001983
Iteration 186/1000 | Loss: 0.00001983
Iteration 187/1000 | Loss: 0.00001983
Iteration 188/1000 | Loss: 0.00001983
Iteration 189/1000 | Loss: 0.00001983
Iteration 190/1000 | Loss: 0.00001983
Iteration 191/1000 | Loss: 0.00051764
Iteration 192/1000 | Loss: 0.00051764
Iteration 193/1000 | Loss: 0.00051764
Iteration 194/1000 | Loss: 0.00038884
Iteration 195/1000 | Loss: 0.00045112
Iteration 196/1000 | Loss: 0.00002557
Iteration 197/1000 | Loss: 0.00002041
Iteration 198/1000 | Loss: 0.00001947
Iteration 199/1000 | Loss: 0.00001876
Iteration 200/1000 | Loss: 0.00001834
Iteration 201/1000 | Loss: 0.00001788
Iteration 202/1000 | Loss: 0.00001746
Iteration 203/1000 | Loss: 0.00001720
Iteration 204/1000 | Loss: 0.00001699
Iteration 205/1000 | Loss: 0.00001686
Iteration 206/1000 | Loss: 0.00001681
Iteration 207/1000 | Loss: 0.00001680
Iteration 208/1000 | Loss: 0.00001676
Iteration 209/1000 | Loss: 0.00001676
Iteration 210/1000 | Loss: 0.00001676
Iteration 211/1000 | Loss: 0.00001676
Iteration 212/1000 | Loss: 0.00001676
Iteration 213/1000 | Loss: 0.00001675
Iteration 214/1000 | Loss: 0.00001675
Iteration 215/1000 | Loss: 0.00001675
Iteration 216/1000 | Loss: 0.00001674
Iteration 217/1000 | Loss: 0.00001673
Iteration 218/1000 | Loss: 0.00001673
Iteration 219/1000 | Loss: 0.00001673
Iteration 220/1000 | Loss: 0.00001673
Iteration 221/1000 | Loss: 0.00001672
Iteration 222/1000 | Loss: 0.00001671
Iteration 223/1000 | Loss: 0.00001670
Iteration 224/1000 | Loss: 0.00001670
Iteration 225/1000 | Loss: 0.00001669
Iteration 226/1000 | Loss: 0.00001669
Iteration 227/1000 | Loss: 0.00001668
Iteration 228/1000 | Loss: 0.00001668
Iteration 229/1000 | Loss: 0.00001668
Iteration 230/1000 | Loss: 0.00001668
Iteration 231/1000 | Loss: 0.00001668
Iteration 232/1000 | Loss: 0.00001668
Iteration 233/1000 | Loss: 0.00001668
Iteration 234/1000 | Loss: 0.00001668
Iteration 235/1000 | Loss: 0.00001667
Iteration 236/1000 | Loss: 0.00001667
Iteration 237/1000 | Loss: 0.00001667
Iteration 238/1000 | Loss: 0.00001667
Iteration 239/1000 | Loss: 0.00001667
Iteration 240/1000 | Loss: 0.00001667
Iteration 241/1000 | Loss: 0.00001667
Iteration 242/1000 | Loss: 0.00001667
Iteration 243/1000 | Loss: 0.00001667
Iteration 244/1000 | Loss: 0.00001667
Iteration 245/1000 | Loss: 0.00001667
Iteration 246/1000 | Loss: 0.00001667
Iteration 247/1000 | Loss: 0.00001667
Iteration 248/1000 | Loss: 0.00001667
Iteration 249/1000 | Loss: 0.00001666
Iteration 250/1000 | Loss: 0.00001666
Iteration 251/1000 | Loss: 0.00001666
Iteration 252/1000 | Loss: 0.00001666
Iteration 253/1000 | Loss: 0.00001666
Iteration 254/1000 | Loss: 0.00001666
Iteration 255/1000 | Loss: 0.00001666
Iteration 256/1000 | Loss: 0.00001666
Iteration 257/1000 | Loss: 0.00001666
Iteration 258/1000 | Loss: 0.00001666
Iteration 259/1000 | Loss: 0.00001666
Iteration 260/1000 | Loss: 0.00001666
Iteration 261/1000 | Loss: 0.00001666
Iteration 262/1000 | Loss: 0.00001666
Iteration 263/1000 | Loss: 0.00001666
Iteration 264/1000 | Loss: 0.00001666
Iteration 265/1000 | Loss: 0.00001666
Iteration 266/1000 | Loss: 0.00001666
Iteration 267/1000 | Loss: 0.00001666
Iteration 268/1000 | Loss: 0.00001666
Iteration 269/1000 | Loss: 0.00001666
Iteration 270/1000 | Loss: 0.00001666
Iteration 271/1000 | Loss: 0.00001666
Iteration 272/1000 | Loss: 0.00001666
Iteration 273/1000 | Loss: 0.00001666
Iteration 274/1000 | Loss: 0.00001666
Iteration 275/1000 | Loss: 0.00001666
Iteration 276/1000 | Loss: 0.00001666
Iteration 277/1000 | Loss: 0.00001666
Iteration 278/1000 | Loss: 0.00001666
Iteration 279/1000 | Loss: 0.00001666
Iteration 280/1000 | Loss: 0.00001666
Iteration 281/1000 | Loss: 0.00001666
Iteration 282/1000 | Loss: 0.00001666
Iteration 283/1000 | Loss: 0.00001666
Iteration 284/1000 | Loss: 0.00001666
Iteration 285/1000 | Loss: 0.00001666
Iteration 286/1000 | Loss: 0.00001666
Iteration 287/1000 | Loss: 0.00001666
Iteration 288/1000 | Loss: 0.00001666
Iteration 289/1000 | Loss: 0.00001666
Iteration 290/1000 | Loss: 0.00001666
Iteration 291/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.6660682376823388e-05, 1.6660682376823388e-05, 1.6660682376823388e-05, 1.6660682376823388e-05, 1.6660682376823388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6660682376823388e-05

Optimization complete. Final v2v error: 3.398618221282959 mm

Highest mean error: 5.15509557723999 mm for frame 52

Lowest mean error: 2.8757784366607666 mm for frame 3

Saving results

Total time: 209.8478171825409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593819
Iteration 2/25 | Loss: 0.00158757
Iteration 3/25 | Loss: 0.00136149
Iteration 4/25 | Loss: 0.00133693
Iteration 5/25 | Loss: 0.00133294
Iteration 6/25 | Loss: 0.00133146
Iteration 7/25 | Loss: 0.00133135
Iteration 8/25 | Loss: 0.00133135
Iteration 9/25 | Loss: 0.00133135
Iteration 10/25 | Loss: 0.00133135
Iteration 11/25 | Loss: 0.00133135
Iteration 12/25 | Loss: 0.00133135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013313469244167209, 0.0013313469244167209, 0.0013313469244167209, 0.0013313469244167209, 0.0013313469244167209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013313469244167209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15367949
Iteration 2/25 | Loss: 0.00094863
Iteration 3/25 | Loss: 0.00094863
Iteration 4/25 | Loss: 0.00094863
Iteration 5/25 | Loss: 0.00094863
Iteration 6/25 | Loss: 0.00094863
Iteration 7/25 | Loss: 0.00094863
Iteration 8/25 | Loss: 0.00094863
Iteration 9/25 | Loss: 0.00094863
Iteration 10/25 | Loss: 0.00094863
Iteration 11/25 | Loss: 0.00094863
Iteration 12/25 | Loss: 0.00094863
Iteration 13/25 | Loss: 0.00094863
Iteration 14/25 | Loss: 0.00094863
Iteration 15/25 | Loss: 0.00094863
Iteration 16/25 | Loss: 0.00094863
Iteration 17/25 | Loss: 0.00094863
Iteration 18/25 | Loss: 0.00094863
Iteration 19/25 | Loss: 0.00094863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000948627304751426, 0.000948627304751426, 0.000948627304751426, 0.000948627304751426, 0.000948627304751426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000948627304751426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094863
Iteration 2/1000 | Loss: 0.00004356
Iteration 3/1000 | Loss: 0.00002978
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002015
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001845
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001792
Iteration 11/1000 | Loss: 0.00001770
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001752
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001751
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001747
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001742
Iteration 25/1000 | Loss: 0.00001741
Iteration 26/1000 | Loss: 0.00001741
Iteration 27/1000 | Loss: 0.00001741
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001741
Iteration 30/1000 | Loss: 0.00001741
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001740
Iteration 34/1000 | Loss: 0.00001740
Iteration 35/1000 | Loss: 0.00001740
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001735
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001733
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001732
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001729
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001729
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001729
Iteration 58/1000 | Loss: 0.00001729
Iteration 59/1000 | Loss: 0.00001729
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001728
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001727
Iteration 66/1000 | Loss: 0.00001727
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001725
Iteration 71/1000 | Loss: 0.00001725
Iteration 72/1000 | Loss: 0.00001725
Iteration 73/1000 | Loss: 0.00001725
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001722
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001720
Iteration 83/1000 | Loss: 0.00001720
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001719
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001715
Iteration 112/1000 | Loss: 0.00001715
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001714
Iteration 120/1000 | Loss: 0.00001714
Iteration 121/1000 | Loss: 0.00001714
Iteration 122/1000 | Loss: 0.00001714
Iteration 123/1000 | Loss: 0.00001714
Iteration 124/1000 | Loss: 0.00001713
Iteration 125/1000 | Loss: 0.00001713
Iteration 126/1000 | Loss: 0.00001713
Iteration 127/1000 | Loss: 0.00001713
Iteration 128/1000 | Loss: 0.00001713
Iteration 129/1000 | Loss: 0.00001713
Iteration 130/1000 | Loss: 0.00001713
Iteration 131/1000 | Loss: 0.00001713
Iteration 132/1000 | Loss: 0.00001713
Iteration 133/1000 | Loss: 0.00001713
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001713
Iteration 148/1000 | Loss: 0.00001713
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00001713
Iteration 151/1000 | Loss: 0.00001713
Iteration 152/1000 | Loss: 0.00001713
Iteration 153/1000 | Loss: 0.00001713
Iteration 154/1000 | Loss: 0.00001713
Iteration 155/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7127846149378456e-05, 1.7127846149378456e-05, 1.7127846149378456e-05, 1.7127846149378456e-05, 1.7127846149378456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7127846149378456e-05

Optimization complete. Final v2v error: 3.3653697967529297 mm

Highest mean error: 4.886507987976074 mm for frame 54

Lowest mean error: 3.0205142498016357 mm for frame 134

Saving results

Total time: 36.45946955680847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430662
Iteration 2/25 | Loss: 0.00134356
Iteration 3/25 | Loss: 0.00126385
Iteration 4/25 | Loss: 0.00124924
Iteration 5/25 | Loss: 0.00124428
Iteration 6/25 | Loss: 0.00124318
Iteration 7/25 | Loss: 0.00124311
Iteration 8/25 | Loss: 0.00124311
Iteration 9/25 | Loss: 0.00124311
Iteration 10/25 | Loss: 0.00124311
Iteration 11/25 | Loss: 0.00124311
Iteration 12/25 | Loss: 0.00124311
Iteration 13/25 | Loss: 0.00124311
Iteration 14/25 | Loss: 0.00124311
Iteration 15/25 | Loss: 0.00124311
Iteration 16/25 | Loss: 0.00124311
Iteration 17/25 | Loss: 0.00124311
Iteration 18/25 | Loss: 0.00124311
Iteration 19/25 | Loss: 0.00124311
Iteration 20/25 | Loss: 0.00124311
Iteration 21/25 | Loss: 0.00124311
Iteration 22/25 | Loss: 0.00124311
Iteration 23/25 | Loss: 0.00124311
Iteration 24/25 | Loss: 0.00124311
Iteration 25/25 | Loss: 0.00124311

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56528544
Iteration 2/25 | Loss: 0.00107173
Iteration 3/25 | Loss: 0.00107173
Iteration 4/25 | Loss: 0.00107173
Iteration 5/25 | Loss: 0.00107173
Iteration 6/25 | Loss: 0.00107173
Iteration 7/25 | Loss: 0.00107173
Iteration 8/25 | Loss: 0.00107173
Iteration 9/25 | Loss: 0.00107173
Iteration 10/25 | Loss: 0.00107173
Iteration 11/25 | Loss: 0.00107172
Iteration 12/25 | Loss: 0.00107172
Iteration 13/25 | Loss: 0.00107172
Iteration 14/25 | Loss: 0.00107172
Iteration 15/25 | Loss: 0.00107172
Iteration 16/25 | Loss: 0.00107172
Iteration 17/25 | Loss: 0.00107172
Iteration 18/25 | Loss: 0.00107172
Iteration 19/25 | Loss: 0.00107172
Iteration 20/25 | Loss: 0.00107172
Iteration 21/25 | Loss: 0.00107172
Iteration 22/25 | Loss: 0.00107172
Iteration 23/25 | Loss: 0.00107172
Iteration 24/25 | Loss: 0.00107172
Iteration 25/25 | Loss: 0.00107172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107172
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001605
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001444
Iteration 9/1000 | Loss: 0.00001414
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001334
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001324
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001318
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001317
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001302
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001300
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001295
Iteration 38/1000 | Loss: 0.00001295
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001288
Iteration 44/1000 | Loss: 0.00001288
Iteration 45/1000 | Loss: 0.00001288
Iteration 46/1000 | Loss: 0.00001288
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001285
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001283
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001278
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001275
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001274
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001272
Iteration 89/1000 | Loss: 0.00001272
Iteration 90/1000 | Loss: 0.00001272
Iteration 91/1000 | Loss: 0.00001272
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001270
Iteration 97/1000 | Loss: 0.00001269
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001269
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001269
Iteration 103/1000 | Loss: 0.00001269
Iteration 104/1000 | Loss: 0.00001269
Iteration 105/1000 | Loss: 0.00001268
Iteration 106/1000 | Loss: 0.00001268
Iteration 107/1000 | Loss: 0.00001268
Iteration 108/1000 | Loss: 0.00001268
Iteration 109/1000 | Loss: 0.00001268
Iteration 110/1000 | Loss: 0.00001267
Iteration 111/1000 | Loss: 0.00001267
Iteration 112/1000 | Loss: 0.00001267
Iteration 113/1000 | Loss: 0.00001267
Iteration 114/1000 | Loss: 0.00001267
Iteration 115/1000 | Loss: 0.00001267
Iteration 116/1000 | Loss: 0.00001266
Iteration 117/1000 | Loss: 0.00001266
Iteration 118/1000 | Loss: 0.00001266
Iteration 119/1000 | Loss: 0.00001266
Iteration 120/1000 | Loss: 0.00001266
Iteration 121/1000 | Loss: 0.00001266
Iteration 122/1000 | Loss: 0.00001266
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Iteration 129/1000 | Loss: 0.00001265
Iteration 130/1000 | Loss: 0.00001265
Iteration 131/1000 | Loss: 0.00001265
Iteration 132/1000 | Loss: 0.00001265
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001264
Iteration 135/1000 | Loss: 0.00001264
Iteration 136/1000 | Loss: 0.00001264
Iteration 137/1000 | Loss: 0.00001263
Iteration 138/1000 | Loss: 0.00001263
Iteration 139/1000 | Loss: 0.00001263
Iteration 140/1000 | Loss: 0.00001263
Iteration 141/1000 | Loss: 0.00001263
Iteration 142/1000 | Loss: 0.00001263
Iteration 143/1000 | Loss: 0.00001263
Iteration 144/1000 | Loss: 0.00001262
Iteration 145/1000 | Loss: 0.00001262
Iteration 146/1000 | Loss: 0.00001262
Iteration 147/1000 | Loss: 0.00001262
Iteration 148/1000 | Loss: 0.00001262
Iteration 149/1000 | Loss: 0.00001262
Iteration 150/1000 | Loss: 0.00001261
Iteration 151/1000 | Loss: 0.00001261
Iteration 152/1000 | Loss: 0.00001261
Iteration 153/1000 | Loss: 0.00001261
Iteration 154/1000 | Loss: 0.00001261
Iteration 155/1000 | Loss: 0.00001261
Iteration 156/1000 | Loss: 0.00001261
Iteration 157/1000 | Loss: 0.00001261
Iteration 158/1000 | Loss: 0.00001261
Iteration 159/1000 | Loss: 0.00001261
Iteration 160/1000 | Loss: 0.00001261
Iteration 161/1000 | Loss: 0.00001260
Iteration 162/1000 | Loss: 0.00001260
Iteration 163/1000 | Loss: 0.00001260
Iteration 164/1000 | Loss: 0.00001260
Iteration 165/1000 | Loss: 0.00001260
Iteration 166/1000 | Loss: 0.00001260
Iteration 167/1000 | Loss: 0.00001260
Iteration 168/1000 | Loss: 0.00001260
Iteration 169/1000 | Loss: 0.00001260
Iteration 170/1000 | Loss: 0.00001260
Iteration 171/1000 | Loss: 0.00001260
Iteration 172/1000 | Loss: 0.00001259
Iteration 173/1000 | Loss: 0.00001259
Iteration 174/1000 | Loss: 0.00001259
Iteration 175/1000 | Loss: 0.00001259
Iteration 176/1000 | Loss: 0.00001259
Iteration 177/1000 | Loss: 0.00001259
Iteration 178/1000 | Loss: 0.00001259
Iteration 179/1000 | Loss: 0.00001259
Iteration 180/1000 | Loss: 0.00001259
Iteration 181/1000 | Loss: 0.00001258
Iteration 182/1000 | Loss: 0.00001258
Iteration 183/1000 | Loss: 0.00001258
Iteration 184/1000 | Loss: 0.00001258
Iteration 185/1000 | Loss: 0.00001258
Iteration 186/1000 | Loss: 0.00001258
Iteration 187/1000 | Loss: 0.00001258
Iteration 188/1000 | Loss: 0.00001258
Iteration 189/1000 | Loss: 0.00001258
Iteration 190/1000 | Loss: 0.00001258
Iteration 191/1000 | Loss: 0.00001258
Iteration 192/1000 | Loss: 0.00001258
Iteration 193/1000 | Loss: 0.00001258
Iteration 194/1000 | Loss: 0.00001258
Iteration 195/1000 | Loss: 0.00001258
Iteration 196/1000 | Loss: 0.00001258
Iteration 197/1000 | Loss: 0.00001258
Iteration 198/1000 | Loss: 0.00001258
Iteration 199/1000 | Loss: 0.00001258
Iteration 200/1000 | Loss: 0.00001258
Iteration 201/1000 | Loss: 0.00001258
Iteration 202/1000 | Loss: 0.00001258
Iteration 203/1000 | Loss: 0.00001258
Iteration 204/1000 | Loss: 0.00001258
Iteration 205/1000 | Loss: 0.00001258
Iteration 206/1000 | Loss: 0.00001258
Iteration 207/1000 | Loss: 0.00001258
Iteration 208/1000 | Loss: 0.00001258
Iteration 209/1000 | Loss: 0.00001258
Iteration 210/1000 | Loss: 0.00001258
Iteration 211/1000 | Loss: 0.00001258
Iteration 212/1000 | Loss: 0.00001258
Iteration 213/1000 | Loss: 0.00001258
Iteration 214/1000 | Loss: 0.00001258
Iteration 215/1000 | Loss: 0.00001258
Iteration 216/1000 | Loss: 0.00001258
Iteration 217/1000 | Loss: 0.00001258
Iteration 218/1000 | Loss: 0.00001258
Iteration 219/1000 | Loss: 0.00001258
Iteration 220/1000 | Loss: 0.00001258
Iteration 221/1000 | Loss: 0.00001258
Iteration 222/1000 | Loss: 0.00001258
Iteration 223/1000 | Loss: 0.00001258
Iteration 224/1000 | Loss: 0.00001258
Iteration 225/1000 | Loss: 0.00001258
Iteration 226/1000 | Loss: 0.00001258
Iteration 227/1000 | Loss: 0.00001258
Iteration 228/1000 | Loss: 0.00001258
Iteration 229/1000 | Loss: 0.00001258
Iteration 230/1000 | Loss: 0.00001258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.2576801964314654e-05, 1.2576801964314654e-05, 1.2576801964314654e-05, 1.2576801964314654e-05, 1.2576801964314654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2576801964314654e-05

Optimization complete. Final v2v error: 3.051011562347412 mm

Highest mean error: 3.390716552734375 mm for frame 84

Lowest mean error: 2.7637407779693604 mm for frame 110

Saving results

Total time: 44.33499264717102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835132
Iteration 2/25 | Loss: 0.00156683
Iteration 3/25 | Loss: 0.00132837
Iteration 4/25 | Loss: 0.00130545
Iteration 5/25 | Loss: 0.00130228
Iteration 6/25 | Loss: 0.00130222
Iteration 7/25 | Loss: 0.00130222
Iteration 8/25 | Loss: 0.00130222
Iteration 9/25 | Loss: 0.00130222
Iteration 10/25 | Loss: 0.00130222
Iteration 11/25 | Loss: 0.00130222
Iteration 12/25 | Loss: 0.00130222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013022193452343345, 0.0013022193452343345, 0.0013022193452343345, 0.0013022193452343345, 0.0013022193452343345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013022193452343345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31338286
Iteration 2/25 | Loss: 0.00093913
Iteration 3/25 | Loss: 0.00093911
Iteration 4/25 | Loss: 0.00093911
Iteration 5/25 | Loss: 0.00093911
Iteration 6/25 | Loss: 0.00093911
Iteration 7/25 | Loss: 0.00093911
Iteration 8/25 | Loss: 0.00093911
Iteration 9/25 | Loss: 0.00093911
Iteration 10/25 | Loss: 0.00093911
Iteration 11/25 | Loss: 0.00093911
Iteration 12/25 | Loss: 0.00093911
Iteration 13/25 | Loss: 0.00093911
Iteration 14/25 | Loss: 0.00093911
Iteration 15/25 | Loss: 0.00093911
Iteration 16/25 | Loss: 0.00093911
Iteration 17/25 | Loss: 0.00093911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009391102357767522, 0.0009391102357767522, 0.0009391102357767522, 0.0009391102357767522, 0.0009391102357767522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009391102357767522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093911
Iteration 2/1000 | Loss: 0.00003824
Iteration 3/1000 | Loss: 0.00002320
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001835
Iteration 6/1000 | Loss: 0.00001746
Iteration 7/1000 | Loss: 0.00001695
Iteration 8/1000 | Loss: 0.00001653
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001578
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001543
Iteration 13/1000 | Loss: 0.00001538
Iteration 14/1000 | Loss: 0.00001535
Iteration 15/1000 | Loss: 0.00001531
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001517
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001487
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001466
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001457
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001455
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001452
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001446
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001441
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001440
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001435
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001434
Iteration 120/1000 | Loss: 0.00001434
Iteration 121/1000 | Loss: 0.00001434
Iteration 122/1000 | Loss: 0.00001434
Iteration 123/1000 | Loss: 0.00001434
Iteration 124/1000 | Loss: 0.00001433
Iteration 125/1000 | Loss: 0.00001433
Iteration 126/1000 | Loss: 0.00001433
Iteration 127/1000 | Loss: 0.00001433
Iteration 128/1000 | Loss: 0.00001433
Iteration 129/1000 | Loss: 0.00001433
Iteration 130/1000 | Loss: 0.00001433
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.433316538168583e-05, 1.433316538168583e-05, 1.433316538168583e-05, 1.433316538168583e-05, 1.433316538168583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.433316538168583e-05

Optimization complete. Final v2v error: 3.219193696975708 mm

Highest mean error: 3.802523136138916 mm for frame 1

Lowest mean error: 2.876558542251587 mm for frame 238

Saving results

Total time: 45.05147695541382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036063
Iteration 2/25 | Loss: 0.01036063
Iteration 3/25 | Loss: 0.01036063
Iteration 4/25 | Loss: 0.01036063
Iteration 5/25 | Loss: 0.01036063
Iteration 6/25 | Loss: 0.01036063
Iteration 7/25 | Loss: 0.01036063
Iteration 8/25 | Loss: 0.01036063
Iteration 9/25 | Loss: 0.01036063
Iteration 10/25 | Loss: 0.01036063
Iteration 11/25 | Loss: 0.01036063
Iteration 12/25 | Loss: 0.01036063
Iteration 13/25 | Loss: 0.01036062
Iteration 14/25 | Loss: 0.01036062
Iteration 15/25 | Loss: 0.01036062
Iteration 16/25 | Loss: 0.01036062
Iteration 17/25 | Loss: 0.01036062
Iteration 18/25 | Loss: 0.01036062
Iteration 19/25 | Loss: 0.01036062
Iteration 20/25 | Loss: 0.01036062
Iteration 21/25 | Loss: 0.01036062
Iteration 22/25 | Loss: 0.01036062
Iteration 23/25 | Loss: 0.01036062
Iteration 24/25 | Loss: 0.01036062
Iteration 25/25 | Loss: 0.01036062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82159436
Iteration 2/25 | Loss: 0.06878985
Iteration 3/25 | Loss: 0.06878974
Iteration 4/25 | Loss: 0.06878974
Iteration 5/25 | Loss: 0.06878974
Iteration 6/25 | Loss: 0.06878974
Iteration 7/25 | Loss: 0.06878972
Iteration 8/25 | Loss: 0.06878972
Iteration 9/25 | Loss: 0.06878972
Iteration 10/25 | Loss: 0.06878972
Iteration 11/25 | Loss: 0.06878972
Iteration 12/25 | Loss: 0.06878972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.06878972053527832, 0.06878972053527832, 0.06878972053527832, 0.06878972053527832, 0.06878972053527832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06878972053527832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06878972
Iteration 2/1000 | Loss: 0.00563686
Iteration 3/1000 | Loss: 0.00140085
Iteration 4/1000 | Loss: 0.00057739
Iteration 5/1000 | Loss: 0.01011373
Iteration 6/1000 | Loss: 0.00326104
Iteration 7/1000 | Loss: 0.01236058
Iteration 8/1000 | Loss: 0.00145059
Iteration 9/1000 | Loss: 0.00100864
Iteration 10/1000 | Loss: 0.00047615
Iteration 11/1000 | Loss: 0.00019011
Iteration 12/1000 | Loss: 0.00154373
Iteration 13/1000 | Loss: 0.00008148
Iteration 14/1000 | Loss: 0.00011148
Iteration 15/1000 | Loss: 0.00010678
Iteration 16/1000 | Loss: 0.00006780
Iteration 17/1000 | Loss: 0.00016044
Iteration 18/1000 | Loss: 0.00011402
Iteration 19/1000 | Loss: 0.00006372
Iteration 20/1000 | Loss: 0.00013294
Iteration 21/1000 | Loss: 0.00042336
Iteration 22/1000 | Loss: 0.00009565
Iteration 23/1000 | Loss: 0.00029602
Iteration 24/1000 | Loss: 0.00121263
Iteration 25/1000 | Loss: 0.00009365
Iteration 26/1000 | Loss: 0.00005470
Iteration 27/1000 | Loss: 0.00010265
Iteration 28/1000 | Loss: 0.00007960
Iteration 29/1000 | Loss: 0.00027786
Iteration 30/1000 | Loss: 0.00030997
Iteration 31/1000 | Loss: 0.00041313
Iteration 32/1000 | Loss: 0.00048977
Iteration 33/1000 | Loss: 0.00003885
Iteration 34/1000 | Loss: 0.00023523
Iteration 35/1000 | Loss: 0.00003578
Iteration 36/1000 | Loss: 0.00021846
Iteration 37/1000 | Loss: 0.00530359
Iteration 38/1000 | Loss: 0.00047775
Iteration 39/1000 | Loss: 0.00025516
Iteration 40/1000 | Loss: 0.00009856
Iteration 41/1000 | Loss: 0.00003376
Iteration 42/1000 | Loss: 0.00012312
Iteration 43/1000 | Loss: 0.00039521
Iteration 44/1000 | Loss: 0.00003415
Iteration 45/1000 | Loss: 0.00003063
Iteration 46/1000 | Loss: 0.00038637
Iteration 47/1000 | Loss: 0.00124981
Iteration 48/1000 | Loss: 0.00005534
Iteration 49/1000 | Loss: 0.00003013
Iteration 50/1000 | Loss: 0.00003234
Iteration 51/1000 | Loss: 0.00002832
Iteration 52/1000 | Loss: 0.00006287
Iteration 53/1000 | Loss: 0.00017241
Iteration 54/1000 | Loss: 0.00003101
Iteration 55/1000 | Loss: 0.00002743
Iteration 56/1000 | Loss: 0.00009556
Iteration 57/1000 | Loss: 0.00010457
Iteration 58/1000 | Loss: 0.00007095
Iteration 59/1000 | Loss: 0.00003328
Iteration 60/1000 | Loss: 0.00016935
Iteration 61/1000 | Loss: 0.00235162
Iteration 62/1000 | Loss: 0.00005542
Iteration 63/1000 | Loss: 0.00041463
Iteration 64/1000 | Loss: 0.00005378
Iteration 65/1000 | Loss: 0.00008927
Iteration 66/1000 | Loss: 0.00003571
Iteration 67/1000 | Loss: 0.00008107
Iteration 68/1000 | Loss: 0.00002585
Iteration 69/1000 | Loss: 0.00005883
Iteration 70/1000 | Loss: 0.00002537
Iteration 71/1000 | Loss: 0.00015545
Iteration 72/1000 | Loss: 0.00010859
Iteration 73/1000 | Loss: 0.00005560
Iteration 74/1000 | Loss: 0.00003798
Iteration 75/1000 | Loss: 0.00002689
Iteration 76/1000 | Loss: 0.00006339
Iteration 77/1000 | Loss: 0.00002615
Iteration 78/1000 | Loss: 0.00006214
Iteration 79/1000 | Loss: 0.00004840
Iteration 80/1000 | Loss: 0.00002866
Iteration 81/1000 | Loss: 0.00005836
Iteration 82/1000 | Loss: 0.00013975
Iteration 83/1000 | Loss: 0.00002750
Iteration 84/1000 | Loss: 0.00002436
Iteration 85/1000 | Loss: 0.00002565
Iteration 86/1000 | Loss: 0.00002416
Iteration 87/1000 | Loss: 0.00002416
Iteration 88/1000 | Loss: 0.00002416
Iteration 89/1000 | Loss: 0.00002416
Iteration 90/1000 | Loss: 0.00002416
Iteration 91/1000 | Loss: 0.00002415
Iteration 92/1000 | Loss: 0.00002564
Iteration 93/1000 | Loss: 0.00011085
Iteration 94/1000 | Loss: 0.00016203
Iteration 95/1000 | Loss: 0.00003995
Iteration 96/1000 | Loss: 0.00002846
Iteration 97/1000 | Loss: 0.00002584
Iteration 98/1000 | Loss: 0.00003152
Iteration 99/1000 | Loss: 0.00002410
Iteration 100/1000 | Loss: 0.00002398
Iteration 101/1000 | Loss: 0.00002398
Iteration 102/1000 | Loss: 0.00002397
Iteration 103/1000 | Loss: 0.00002397
Iteration 104/1000 | Loss: 0.00002397
Iteration 105/1000 | Loss: 0.00002397
Iteration 106/1000 | Loss: 0.00002397
Iteration 107/1000 | Loss: 0.00002397
Iteration 108/1000 | Loss: 0.00002397
Iteration 109/1000 | Loss: 0.00002397
Iteration 110/1000 | Loss: 0.00002396
Iteration 111/1000 | Loss: 0.00002396
Iteration 112/1000 | Loss: 0.00002396
Iteration 113/1000 | Loss: 0.00002396
Iteration 114/1000 | Loss: 0.00002396
Iteration 115/1000 | Loss: 0.00002396
Iteration 116/1000 | Loss: 0.00002396
Iteration 117/1000 | Loss: 0.00002396
Iteration 118/1000 | Loss: 0.00013083
Iteration 119/1000 | Loss: 0.00003909
Iteration 120/1000 | Loss: 0.00005082
Iteration 121/1000 | Loss: 0.00002622
Iteration 122/1000 | Loss: 0.00006940
Iteration 123/1000 | Loss: 0.00006769
Iteration 124/1000 | Loss: 0.00003520
Iteration 125/1000 | Loss: 0.00002409
Iteration 126/1000 | Loss: 0.00002409
Iteration 127/1000 | Loss: 0.00002377
Iteration 128/1000 | Loss: 0.00006658
Iteration 129/1000 | Loss: 0.00019308
Iteration 130/1000 | Loss: 0.00009628
Iteration 131/1000 | Loss: 0.00003703
Iteration 132/1000 | Loss: 0.00006525
Iteration 133/1000 | Loss: 0.00018236
Iteration 134/1000 | Loss: 0.00004456
Iteration 135/1000 | Loss: 0.00015701
Iteration 136/1000 | Loss: 0.00041386
Iteration 137/1000 | Loss: 0.00048225
Iteration 138/1000 | Loss: 0.00002843
Iteration 139/1000 | Loss: 0.00015523
Iteration 140/1000 | Loss: 0.00003712
Iteration 141/1000 | Loss: 0.00002394
Iteration 142/1000 | Loss: 0.00005859
Iteration 143/1000 | Loss: 0.00004555
Iteration 144/1000 | Loss: 0.00004571
Iteration 145/1000 | Loss: 0.00003072
Iteration 146/1000 | Loss: 0.00002373
Iteration 147/1000 | Loss: 0.00002373
Iteration 148/1000 | Loss: 0.00002372
Iteration 149/1000 | Loss: 0.00002372
Iteration 150/1000 | Loss: 0.00002372
Iteration 151/1000 | Loss: 0.00002372
Iteration 152/1000 | Loss: 0.00002372
Iteration 153/1000 | Loss: 0.00002372
Iteration 154/1000 | Loss: 0.00002372
Iteration 155/1000 | Loss: 0.00002372
Iteration 156/1000 | Loss: 0.00002372
Iteration 157/1000 | Loss: 0.00002372
Iteration 158/1000 | Loss: 0.00002371
Iteration 159/1000 | Loss: 0.00002371
Iteration 160/1000 | Loss: 0.00002371
Iteration 161/1000 | Loss: 0.00002370
Iteration 162/1000 | Loss: 0.00002370
Iteration 163/1000 | Loss: 0.00003061
Iteration 164/1000 | Loss: 0.00002369
Iteration 165/1000 | Loss: 0.00002369
Iteration 166/1000 | Loss: 0.00002369
Iteration 167/1000 | Loss: 0.00002369
Iteration 168/1000 | Loss: 0.00002368
Iteration 169/1000 | Loss: 0.00002368
Iteration 170/1000 | Loss: 0.00002368
Iteration 171/1000 | Loss: 0.00002368
Iteration 172/1000 | Loss: 0.00002368
Iteration 173/1000 | Loss: 0.00002368
Iteration 174/1000 | Loss: 0.00002368
Iteration 175/1000 | Loss: 0.00002368
Iteration 176/1000 | Loss: 0.00002368
Iteration 177/1000 | Loss: 0.00002368
Iteration 178/1000 | Loss: 0.00002963
Iteration 179/1000 | Loss: 0.00003068
Iteration 180/1000 | Loss: 0.00002370
Iteration 181/1000 | Loss: 0.00002367
Iteration 182/1000 | Loss: 0.00002367
Iteration 183/1000 | Loss: 0.00002367
Iteration 184/1000 | Loss: 0.00002367
Iteration 185/1000 | Loss: 0.00002367
Iteration 186/1000 | Loss: 0.00002367
Iteration 187/1000 | Loss: 0.00002367
Iteration 188/1000 | Loss: 0.00002367
Iteration 189/1000 | Loss: 0.00002367
Iteration 190/1000 | Loss: 0.00002366
Iteration 191/1000 | Loss: 0.00002366
Iteration 192/1000 | Loss: 0.00002366
Iteration 193/1000 | Loss: 0.00002366
Iteration 194/1000 | Loss: 0.00002366
Iteration 195/1000 | Loss: 0.00002366
Iteration 196/1000 | Loss: 0.00002366
Iteration 197/1000 | Loss: 0.00002366
Iteration 198/1000 | Loss: 0.00002366
Iteration 199/1000 | Loss: 0.00002366
Iteration 200/1000 | Loss: 0.00002366
Iteration 201/1000 | Loss: 0.00002366
Iteration 202/1000 | Loss: 0.00002366
Iteration 203/1000 | Loss: 0.00002366
Iteration 204/1000 | Loss: 0.00002365
Iteration 205/1000 | Loss: 0.00002365
Iteration 206/1000 | Loss: 0.00002365
Iteration 207/1000 | Loss: 0.00002365
Iteration 208/1000 | Loss: 0.00002365
Iteration 209/1000 | Loss: 0.00002365
Iteration 210/1000 | Loss: 0.00002365
Iteration 211/1000 | Loss: 0.00003921
Iteration 212/1000 | Loss: 0.00002365
Iteration 213/1000 | Loss: 0.00002365
Iteration 214/1000 | Loss: 0.00002365
Iteration 215/1000 | Loss: 0.00002365
Iteration 216/1000 | Loss: 0.00002365
Iteration 217/1000 | Loss: 0.00002365
Iteration 218/1000 | Loss: 0.00002365
Iteration 219/1000 | Loss: 0.00002365
Iteration 220/1000 | Loss: 0.00002365
Iteration 221/1000 | Loss: 0.00002365
Iteration 222/1000 | Loss: 0.00002365
Iteration 223/1000 | Loss: 0.00002365
Iteration 224/1000 | Loss: 0.00002364
Iteration 225/1000 | Loss: 0.00002364
Iteration 226/1000 | Loss: 0.00002364
Iteration 227/1000 | Loss: 0.00002364
Iteration 228/1000 | Loss: 0.00002363
Iteration 229/1000 | Loss: 0.00002363
Iteration 230/1000 | Loss: 0.00002363
Iteration 231/1000 | Loss: 0.00002363
Iteration 232/1000 | Loss: 0.00002363
Iteration 233/1000 | Loss: 0.00002363
Iteration 234/1000 | Loss: 0.00002362
Iteration 235/1000 | Loss: 0.00002362
Iteration 236/1000 | Loss: 0.00002362
Iteration 237/1000 | Loss: 0.00002362
Iteration 238/1000 | Loss: 0.00002362
Iteration 239/1000 | Loss: 0.00002362
Iteration 240/1000 | Loss: 0.00002362
Iteration 241/1000 | Loss: 0.00002362
Iteration 242/1000 | Loss: 0.00002362
Iteration 243/1000 | Loss: 0.00002362
Iteration 244/1000 | Loss: 0.00002362
Iteration 245/1000 | Loss: 0.00002362
Iteration 246/1000 | Loss: 0.00002362
Iteration 247/1000 | Loss: 0.00002362
Iteration 248/1000 | Loss: 0.00002362
Iteration 249/1000 | Loss: 0.00002362
Iteration 250/1000 | Loss: 0.00002362
Iteration 251/1000 | Loss: 0.00002362
Iteration 252/1000 | Loss: 0.00002362
Iteration 253/1000 | Loss: 0.00002362
Iteration 254/1000 | Loss: 0.00002362
Iteration 255/1000 | Loss: 0.00002362
Iteration 256/1000 | Loss: 0.00002362
Iteration 257/1000 | Loss: 0.00002362
Iteration 258/1000 | Loss: 0.00002362
Iteration 259/1000 | Loss: 0.00002362
Iteration 260/1000 | Loss: 0.00002362
Iteration 261/1000 | Loss: 0.00002362
Iteration 262/1000 | Loss: 0.00002362
Iteration 263/1000 | Loss: 0.00002362
Iteration 264/1000 | Loss: 0.00002362
Iteration 265/1000 | Loss: 0.00002362
Iteration 266/1000 | Loss: 0.00002362
Iteration 267/1000 | Loss: 0.00002362
Iteration 268/1000 | Loss: 0.00002362
Iteration 269/1000 | Loss: 0.00002362
Iteration 270/1000 | Loss: 0.00002362
Iteration 271/1000 | Loss: 0.00002362
Iteration 272/1000 | Loss: 0.00002362
Iteration 273/1000 | Loss: 0.00002362
Iteration 274/1000 | Loss: 0.00002362
Iteration 275/1000 | Loss: 0.00002362
Iteration 276/1000 | Loss: 0.00002362
Iteration 277/1000 | Loss: 0.00002362
Iteration 278/1000 | Loss: 0.00002362
Iteration 279/1000 | Loss: 0.00002362
Iteration 280/1000 | Loss: 0.00002362
Iteration 281/1000 | Loss: 0.00002362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.3616748876520433e-05, 2.3616748876520433e-05, 2.3616748876520433e-05, 2.3616748876520433e-05, 2.3616748876520433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3616748876520433e-05

Optimization complete. Final v2v error: 3.832603693008423 mm

Highest mean error: 17.428993225097656 mm for frame 236

Lowest mean error: 3.051576614379883 mm for frame 92

Saving results

Total time: 209.71439242362976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873335
Iteration 2/25 | Loss: 0.00178165
Iteration 3/25 | Loss: 0.00147050
Iteration 4/25 | Loss: 0.00148748
Iteration 5/25 | Loss: 0.00142858
Iteration 6/25 | Loss: 0.00140773
Iteration 7/25 | Loss: 0.00140094
Iteration 8/25 | Loss: 0.00139390
Iteration 9/25 | Loss: 0.00139468
Iteration 10/25 | Loss: 0.00139326
Iteration 11/25 | Loss: 0.00138022
Iteration 12/25 | Loss: 0.00136630
Iteration 13/25 | Loss: 0.00137330
Iteration 14/25 | Loss: 0.00137205
Iteration 15/25 | Loss: 0.00136859
Iteration 16/25 | Loss: 0.00136487
Iteration 17/25 | Loss: 0.00136210
Iteration 18/25 | Loss: 0.00136748
Iteration 19/25 | Loss: 0.00136280
Iteration 20/25 | Loss: 0.00136345
Iteration 21/25 | Loss: 0.00136185
Iteration 22/25 | Loss: 0.00135924
Iteration 23/25 | Loss: 0.00135648
Iteration 24/25 | Loss: 0.00135745
Iteration 25/25 | Loss: 0.00135764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.64776087
Iteration 2/25 | Loss: 0.00231132
Iteration 3/25 | Loss: 0.00231131
Iteration 4/25 | Loss: 0.00231131
Iteration 5/25 | Loss: 0.00198229
Iteration 6/25 | Loss: 0.00198227
Iteration 7/25 | Loss: 0.00198227
Iteration 8/25 | Loss: 0.00198227
Iteration 9/25 | Loss: 0.00198227
Iteration 10/25 | Loss: 0.00198227
Iteration 11/25 | Loss: 0.00198227
Iteration 12/25 | Loss: 0.00198227
Iteration 13/25 | Loss: 0.00198227
Iteration 14/25 | Loss: 0.00198227
Iteration 15/25 | Loss: 0.00198227
Iteration 16/25 | Loss: 0.00198227
Iteration 17/25 | Loss: 0.00198227
Iteration 18/25 | Loss: 0.00198227
Iteration 19/25 | Loss: 0.00198227
Iteration 20/25 | Loss: 0.00198227
Iteration 21/25 | Loss: 0.00198227
Iteration 22/25 | Loss: 0.00198227
Iteration 23/25 | Loss: 0.00198227
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019822686444967985, 0.0019822686444967985, 0.0019822686444967985, 0.0019822686444967985, 0.0019822686444967985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019822686444967985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198227
Iteration 2/1000 | Loss: 0.00053611
Iteration 3/1000 | Loss: 0.00374926
Iteration 4/1000 | Loss: 0.00257428
Iteration 5/1000 | Loss: 0.00013442
Iteration 6/1000 | Loss: 0.00074261
Iteration 7/1000 | Loss: 0.00026100
Iteration 8/1000 | Loss: 0.00045962
Iteration 9/1000 | Loss: 0.00046329
Iteration 10/1000 | Loss: 0.00039968
Iteration 11/1000 | Loss: 0.00046276
Iteration 12/1000 | Loss: 0.00039430
Iteration 13/1000 | Loss: 0.00109385
Iteration 14/1000 | Loss: 0.00040749
Iteration 15/1000 | Loss: 0.00031195
Iteration 16/1000 | Loss: 0.00021362
Iteration 17/1000 | Loss: 0.00026176
Iteration 18/1000 | Loss: 0.00046633
Iteration 19/1000 | Loss: 0.00149335
Iteration 20/1000 | Loss: 0.00031954
Iteration 21/1000 | Loss: 0.00037055
Iteration 22/1000 | Loss: 0.00091411
Iteration 23/1000 | Loss: 0.00024526
Iteration 24/1000 | Loss: 0.00006973
Iteration 25/1000 | Loss: 0.00005397
Iteration 26/1000 | Loss: 0.00004830
Iteration 27/1000 | Loss: 0.00040743
Iteration 28/1000 | Loss: 0.00054892
Iteration 29/1000 | Loss: 0.00009356
Iteration 30/1000 | Loss: 0.00005550
Iteration 31/1000 | Loss: 0.00049051
Iteration 32/1000 | Loss: 0.00029420
Iteration 33/1000 | Loss: 0.00008160
Iteration 34/1000 | Loss: 0.00006790
Iteration 35/1000 | Loss: 0.00005988
Iteration 36/1000 | Loss: 0.00004574
Iteration 37/1000 | Loss: 0.00005488
Iteration 38/1000 | Loss: 0.00004496
Iteration 39/1000 | Loss: 0.00006931
Iteration 40/1000 | Loss: 0.00004274
Iteration 41/1000 | Loss: 0.00004628
Iteration 42/1000 | Loss: 0.00004626
Iteration 43/1000 | Loss: 0.00004536
Iteration 44/1000 | Loss: 0.00004068
Iteration 45/1000 | Loss: 0.00003714
Iteration 46/1000 | Loss: 0.00003727
Iteration 47/1000 | Loss: 0.00003653
Iteration 48/1000 | Loss: 0.00004209
Iteration 49/1000 | Loss: 0.00005104
Iteration 50/1000 | Loss: 0.00004614
Iteration 51/1000 | Loss: 0.00003961
Iteration 52/1000 | Loss: 0.00011199
Iteration 53/1000 | Loss: 0.00004147
Iteration 54/1000 | Loss: 0.00003808
Iteration 55/1000 | Loss: 0.00003661
Iteration 56/1000 | Loss: 0.00003689
Iteration 57/1000 | Loss: 0.00004533
Iteration 58/1000 | Loss: 0.00004527
Iteration 59/1000 | Loss: 0.00004455
Iteration 60/1000 | Loss: 0.00004158
Iteration 61/1000 | Loss: 0.00004331
Iteration 62/1000 | Loss: 0.00004256
Iteration 63/1000 | Loss: 0.00004210
Iteration 64/1000 | Loss: 0.00004499
Iteration 65/1000 | Loss: 0.00004122
Iteration 66/1000 | Loss: 0.00004555
Iteration 67/1000 | Loss: 0.00004211
Iteration 68/1000 | Loss: 0.00004664
Iteration 69/1000 | Loss: 0.00004107
Iteration 70/1000 | Loss: 0.00005863
Iteration 71/1000 | Loss: 0.00003903
Iteration 72/1000 | Loss: 0.00047771
Iteration 73/1000 | Loss: 0.00060072
Iteration 74/1000 | Loss: 0.00007956
Iteration 75/1000 | Loss: 0.00026188
Iteration 76/1000 | Loss: 0.00097429
Iteration 77/1000 | Loss: 0.00009316
Iteration 78/1000 | Loss: 0.00007154
Iteration 79/1000 | Loss: 0.00013556
Iteration 80/1000 | Loss: 0.00004914
Iteration 81/1000 | Loss: 0.00004143
Iteration 82/1000 | Loss: 0.00004487
Iteration 83/1000 | Loss: 0.00004752
Iteration 84/1000 | Loss: 0.00003809
Iteration 85/1000 | Loss: 0.00004460
Iteration 86/1000 | Loss: 0.00004544
Iteration 87/1000 | Loss: 0.00004580
Iteration 88/1000 | Loss: 0.00004168
Iteration 89/1000 | Loss: 0.00004543
Iteration 90/1000 | Loss: 0.00004147
Iteration 91/1000 | Loss: 0.00004557
Iteration 92/1000 | Loss: 0.00004351
Iteration 93/1000 | Loss: 0.00004450
Iteration 94/1000 | Loss: 0.00004301
Iteration 95/1000 | Loss: 0.00004558
Iteration 96/1000 | Loss: 0.00004257
Iteration 97/1000 | Loss: 0.00004580
Iteration 98/1000 | Loss: 0.00004237
Iteration 99/1000 | Loss: 0.00005043
Iteration 100/1000 | Loss: 0.00004551
Iteration 101/1000 | Loss: 0.00004898
Iteration 102/1000 | Loss: 0.00004421
Iteration 103/1000 | Loss: 0.00005073
Iteration 104/1000 | Loss: 0.00004957
Iteration 105/1000 | Loss: 0.00003731
Iteration 106/1000 | Loss: 0.00004035
Iteration 107/1000 | Loss: 0.00004140
Iteration 108/1000 | Loss: 0.00004230
Iteration 109/1000 | Loss: 0.00004612
Iteration 110/1000 | Loss: 0.00004304
Iteration 111/1000 | Loss: 0.00005058
Iteration 112/1000 | Loss: 0.00004304
Iteration 113/1000 | Loss: 0.00004628
Iteration 114/1000 | Loss: 0.00004287
Iteration 115/1000 | Loss: 0.00005013
Iteration 116/1000 | Loss: 0.00004284
Iteration 117/1000 | Loss: 0.00027371
Iteration 118/1000 | Loss: 0.00056510
Iteration 119/1000 | Loss: 0.00010629
Iteration 120/1000 | Loss: 0.00003649
Iteration 121/1000 | Loss: 0.00016242
Iteration 122/1000 | Loss: 0.00004113
Iteration 123/1000 | Loss: 0.00003677
Iteration 124/1000 | Loss: 0.00003352
Iteration 125/1000 | Loss: 0.00018506
Iteration 126/1000 | Loss: 0.00003819
Iteration 127/1000 | Loss: 0.00003309
Iteration 128/1000 | Loss: 0.00003178
Iteration 129/1000 | Loss: 0.00003129
Iteration 130/1000 | Loss: 0.00003104
Iteration 131/1000 | Loss: 0.00019083
Iteration 132/1000 | Loss: 0.00011339
Iteration 133/1000 | Loss: 0.00003045
Iteration 134/1000 | Loss: 0.00003020
Iteration 135/1000 | Loss: 0.00002996
Iteration 136/1000 | Loss: 0.00002979
Iteration 137/1000 | Loss: 0.00002974
Iteration 138/1000 | Loss: 0.00002974
Iteration 139/1000 | Loss: 0.00002974
Iteration 140/1000 | Loss: 0.00002974
Iteration 141/1000 | Loss: 0.00002971
Iteration 142/1000 | Loss: 0.00002971
Iteration 143/1000 | Loss: 0.00002970
Iteration 144/1000 | Loss: 0.00002968
Iteration 145/1000 | Loss: 0.00002967
Iteration 146/1000 | Loss: 0.00002966
Iteration 147/1000 | Loss: 0.00002966
Iteration 148/1000 | Loss: 0.00002966
Iteration 149/1000 | Loss: 0.00002965
Iteration 150/1000 | Loss: 0.00002965
Iteration 151/1000 | Loss: 0.00002965
Iteration 152/1000 | Loss: 0.00002964
Iteration 153/1000 | Loss: 0.00002964
Iteration 154/1000 | Loss: 0.00002964
Iteration 155/1000 | Loss: 0.00002964
Iteration 156/1000 | Loss: 0.00002964
Iteration 157/1000 | Loss: 0.00002963
Iteration 158/1000 | Loss: 0.00002963
Iteration 159/1000 | Loss: 0.00002963
Iteration 160/1000 | Loss: 0.00002963
Iteration 161/1000 | Loss: 0.00002962
Iteration 162/1000 | Loss: 0.00002962
Iteration 163/1000 | Loss: 0.00002962
Iteration 164/1000 | Loss: 0.00002961
Iteration 165/1000 | Loss: 0.00002960
Iteration 166/1000 | Loss: 0.00002959
Iteration 167/1000 | Loss: 0.00002959
Iteration 168/1000 | Loss: 0.00002958
Iteration 169/1000 | Loss: 0.00002958
Iteration 170/1000 | Loss: 0.00002957
Iteration 171/1000 | Loss: 0.00002957
Iteration 172/1000 | Loss: 0.00002957
Iteration 173/1000 | Loss: 0.00002957
Iteration 174/1000 | Loss: 0.00002957
Iteration 175/1000 | Loss: 0.00002957
Iteration 176/1000 | Loss: 0.00002957
Iteration 177/1000 | Loss: 0.00002956
Iteration 178/1000 | Loss: 0.00002956
Iteration 179/1000 | Loss: 0.00002956
Iteration 180/1000 | Loss: 0.00002956
Iteration 181/1000 | Loss: 0.00002956
Iteration 182/1000 | Loss: 0.00002956
Iteration 183/1000 | Loss: 0.00002956
Iteration 184/1000 | Loss: 0.00002955
Iteration 185/1000 | Loss: 0.00002955
Iteration 186/1000 | Loss: 0.00002955
Iteration 187/1000 | Loss: 0.00002955
Iteration 188/1000 | Loss: 0.00002954
Iteration 189/1000 | Loss: 0.00002954
Iteration 190/1000 | Loss: 0.00002954
Iteration 191/1000 | Loss: 0.00002953
Iteration 192/1000 | Loss: 0.00002953
Iteration 193/1000 | Loss: 0.00002952
Iteration 194/1000 | Loss: 0.00002952
Iteration 195/1000 | Loss: 0.00002952
Iteration 196/1000 | Loss: 0.00002952
Iteration 197/1000 | Loss: 0.00002951
Iteration 198/1000 | Loss: 0.00002951
Iteration 199/1000 | Loss: 0.00002951
Iteration 200/1000 | Loss: 0.00002950
Iteration 201/1000 | Loss: 0.00002950
Iteration 202/1000 | Loss: 0.00002950
Iteration 203/1000 | Loss: 0.00002949
Iteration 204/1000 | Loss: 0.00002949
Iteration 205/1000 | Loss: 0.00002949
Iteration 206/1000 | Loss: 0.00002949
Iteration 207/1000 | Loss: 0.00002948
Iteration 208/1000 | Loss: 0.00002948
Iteration 209/1000 | Loss: 0.00002948
Iteration 210/1000 | Loss: 0.00002948
Iteration 211/1000 | Loss: 0.00002948
Iteration 212/1000 | Loss: 0.00002947
Iteration 213/1000 | Loss: 0.00002947
Iteration 214/1000 | Loss: 0.00002947
Iteration 215/1000 | Loss: 0.00002946
Iteration 216/1000 | Loss: 0.00002946
Iteration 217/1000 | Loss: 0.00002946
Iteration 218/1000 | Loss: 0.00002945
Iteration 219/1000 | Loss: 0.00002945
Iteration 220/1000 | Loss: 0.00002945
Iteration 221/1000 | Loss: 0.00002944
Iteration 222/1000 | Loss: 0.00002944
Iteration 223/1000 | Loss: 0.00002944
Iteration 224/1000 | Loss: 0.00002944
Iteration 225/1000 | Loss: 0.00002944
Iteration 226/1000 | Loss: 0.00002944
Iteration 227/1000 | Loss: 0.00002944
Iteration 228/1000 | Loss: 0.00002944
Iteration 229/1000 | Loss: 0.00002944
Iteration 230/1000 | Loss: 0.00002944
Iteration 231/1000 | Loss: 0.00002944
Iteration 232/1000 | Loss: 0.00002944
Iteration 233/1000 | Loss: 0.00002944
Iteration 234/1000 | Loss: 0.00002944
Iteration 235/1000 | Loss: 0.00002944
Iteration 236/1000 | Loss: 0.00002943
Iteration 237/1000 | Loss: 0.00002943
Iteration 238/1000 | Loss: 0.00002943
Iteration 239/1000 | Loss: 0.00002943
Iteration 240/1000 | Loss: 0.00002943
Iteration 241/1000 | Loss: 0.00002943
Iteration 242/1000 | Loss: 0.00002943
Iteration 243/1000 | Loss: 0.00002943
Iteration 244/1000 | Loss: 0.00002943
Iteration 245/1000 | Loss: 0.00002943
Iteration 246/1000 | Loss: 0.00002942
Iteration 247/1000 | Loss: 0.00002942
Iteration 248/1000 | Loss: 0.00002942
Iteration 249/1000 | Loss: 0.00002942
Iteration 250/1000 | Loss: 0.00002942
Iteration 251/1000 | Loss: 0.00002942
Iteration 252/1000 | Loss: 0.00002942
Iteration 253/1000 | Loss: 0.00002942
Iteration 254/1000 | Loss: 0.00002942
Iteration 255/1000 | Loss: 0.00002942
Iteration 256/1000 | Loss: 0.00002942
Iteration 257/1000 | Loss: 0.00002942
Iteration 258/1000 | Loss: 0.00002942
Iteration 259/1000 | Loss: 0.00002941
Iteration 260/1000 | Loss: 0.00002941
Iteration 261/1000 | Loss: 0.00002941
Iteration 262/1000 | Loss: 0.00002941
Iteration 263/1000 | Loss: 0.00002941
Iteration 264/1000 | Loss: 0.00002941
Iteration 265/1000 | Loss: 0.00002941
Iteration 266/1000 | Loss: 0.00002941
Iteration 267/1000 | Loss: 0.00002941
Iteration 268/1000 | Loss: 0.00002941
Iteration 269/1000 | Loss: 0.00002941
Iteration 270/1000 | Loss: 0.00002941
Iteration 271/1000 | Loss: 0.00002941
Iteration 272/1000 | Loss: 0.00002941
Iteration 273/1000 | Loss: 0.00002941
Iteration 274/1000 | Loss: 0.00002941
Iteration 275/1000 | Loss: 0.00002941
Iteration 276/1000 | Loss: 0.00002941
Iteration 277/1000 | Loss: 0.00002941
Iteration 278/1000 | Loss: 0.00002941
Iteration 279/1000 | Loss: 0.00002941
Iteration 280/1000 | Loss: 0.00002941
Iteration 281/1000 | Loss: 0.00002941
Iteration 282/1000 | Loss: 0.00002941
Iteration 283/1000 | Loss: 0.00002941
Iteration 284/1000 | Loss: 0.00002941
Iteration 285/1000 | Loss: 0.00002941
Iteration 286/1000 | Loss: 0.00002941
Iteration 287/1000 | Loss: 0.00002941
Iteration 288/1000 | Loss: 0.00002941
Iteration 289/1000 | Loss: 0.00002941
Iteration 290/1000 | Loss: 0.00002941
Iteration 291/1000 | Loss: 0.00002941
Iteration 292/1000 | Loss: 0.00002941
Iteration 293/1000 | Loss: 0.00002941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [2.9408281989162788e-05, 2.9408281989162788e-05, 2.9408281989162788e-05, 2.9408281989162788e-05, 2.9408281989162788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9408281989162788e-05

Optimization complete. Final v2v error: 4.460599422454834 mm

Highest mean error: 6.977337837219238 mm for frame 43

Lowest mean error: 3.1898679733276367 mm for frame 137

Saving results

Total time: 281.19812512397766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01161942
Iteration 2/25 | Loss: 0.00383382
Iteration 3/25 | Loss: 0.00248657
Iteration 4/25 | Loss: 0.00227671
Iteration 5/25 | Loss: 0.00195271
Iteration 6/25 | Loss: 0.00176284
Iteration 7/25 | Loss: 0.00165407
Iteration 8/25 | Loss: 0.00160030
Iteration 9/25 | Loss: 0.00158867
Iteration 10/25 | Loss: 0.00156782
Iteration 11/25 | Loss: 0.00156175
Iteration 12/25 | Loss: 0.00156441
Iteration 13/25 | Loss: 0.00155774
Iteration 14/25 | Loss: 0.00155461
Iteration 15/25 | Loss: 0.00155430
Iteration 16/25 | Loss: 0.00155429
Iteration 17/25 | Loss: 0.00155429
Iteration 18/25 | Loss: 0.00155429
Iteration 19/25 | Loss: 0.00155429
Iteration 20/25 | Loss: 0.00155429
Iteration 21/25 | Loss: 0.00155429
Iteration 22/25 | Loss: 0.00155429
Iteration 23/25 | Loss: 0.00155429
Iteration 24/25 | Loss: 0.00155429
Iteration 25/25 | Loss: 0.00155429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64540809
Iteration 2/25 | Loss: 0.00152220
Iteration 3/25 | Loss: 0.00152220
Iteration 4/25 | Loss: 0.00152220
Iteration 5/25 | Loss: 0.00152220
Iteration 6/25 | Loss: 0.00152220
Iteration 7/25 | Loss: 0.00152220
Iteration 8/25 | Loss: 0.00152220
Iteration 9/25 | Loss: 0.00152220
Iteration 10/25 | Loss: 0.00152220
Iteration 11/25 | Loss: 0.00152220
Iteration 12/25 | Loss: 0.00152220
Iteration 13/25 | Loss: 0.00152220
Iteration 14/25 | Loss: 0.00152220
Iteration 15/25 | Loss: 0.00152220
Iteration 16/25 | Loss: 0.00152220
Iteration 17/25 | Loss: 0.00152220
Iteration 18/25 | Loss: 0.00152220
Iteration 19/25 | Loss: 0.00152220
Iteration 20/25 | Loss: 0.00152220
Iteration 21/25 | Loss: 0.00152220
Iteration 22/25 | Loss: 0.00152220
Iteration 23/25 | Loss: 0.00152220
Iteration 24/25 | Loss: 0.00152220
Iteration 25/25 | Loss: 0.00152220

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152220
Iteration 2/1000 | Loss: 0.00011049
Iteration 3/1000 | Loss: 0.00006879
Iteration 4/1000 | Loss: 0.00005727
Iteration 5/1000 | Loss: 0.00005127
Iteration 6/1000 | Loss: 0.00004897
Iteration 7/1000 | Loss: 0.00004678
Iteration 8/1000 | Loss: 0.00004518
Iteration 9/1000 | Loss: 0.00004439
Iteration 10/1000 | Loss: 0.00004376
Iteration 11/1000 | Loss: 0.00004336
Iteration 12/1000 | Loss: 0.00004295
Iteration 13/1000 | Loss: 0.00004246
Iteration 14/1000 | Loss: 0.00004223
Iteration 15/1000 | Loss: 0.00004201
Iteration 16/1000 | Loss: 0.00004187
Iteration 17/1000 | Loss: 0.00004182
Iteration 18/1000 | Loss: 0.00004169
Iteration 19/1000 | Loss: 0.00004165
Iteration 20/1000 | Loss: 0.00004165
Iteration 21/1000 | Loss: 0.00004162
Iteration 22/1000 | Loss: 0.00004162
Iteration 23/1000 | Loss: 0.00004162
Iteration 24/1000 | Loss: 0.00004161
Iteration 25/1000 | Loss: 0.00004161
Iteration 26/1000 | Loss: 0.00004160
Iteration 27/1000 | Loss: 0.00004158
Iteration 28/1000 | Loss: 0.00004153
Iteration 29/1000 | Loss: 0.00004150
Iteration 30/1000 | Loss: 0.00004150
Iteration 31/1000 | Loss: 0.00004150
Iteration 32/1000 | Loss: 0.00004150
Iteration 33/1000 | Loss: 0.00004150
Iteration 34/1000 | Loss: 0.00004150
Iteration 35/1000 | Loss: 0.00004150
Iteration 36/1000 | Loss: 0.00004150
Iteration 37/1000 | Loss: 0.00004149
Iteration 38/1000 | Loss: 0.00004149
Iteration 39/1000 | Loss: 0.00004149
Iteration 40/1000 | Loss: 0.00004149
Iteration 41/1000 | Loss: 0.00004149
Iteration 42/1000 | Loss: 0.00004148
Iteration 43/1000 | Loss: 0.00004148
Iteration 44/1000 | Loss: 0.00004147
Iteration 45/1000 | Loss: 0.00004147
Iteration 46/1000 | Loss: 0.00004147
Iteration 47/1000 | Loss: 0.00004147
Iteration 48/1000 | Loss: 0.00004147
Iteration 49/1000 | Loss: 0.00004147
Iteration 50/1000 | Loss: 0.00004147
Iteration 51/1000 | Loss: 0.00004146
Iteration 52/1000 | Loss: 0.00004146
Iteration 53/1000 | Loss: 0.00004146
Iteration 54/1000 | Loss: 0.00004145
Iteration 55/1000 | Loss: 0.00004145
Iteration 56/1000 | Loss: 0.00004145
Iteration 57/1000 | Loss: 0.00004145
Iteration 58/1000 | Loss: 0.00004145
Iteration 59/1000 | Loss: 0.00004145
Iteration 60/1000 | Loss: 0.00004145
Iteration 61/1000 | Loss: 0.00004144
Iteration 62/1000 | Loss: 0.00004144
Iteration 63/1000 | Loss: 0.00004143
Iteration 64/1000 | Loss: 0.00004143
Iteration 65/1000 | Loss: 0.00004142
Iteration 66/1000 | Loss: 0.00004142
Iteration 67/1000 | Loss: 0.00004142
Iteration 68/1000 | Loss: 0.00004142
Iteration 69/1000 | Loss: 0.00004142
Iteration 70/1000 | Loss: 0.00004142
Iteration 71/1000 | Loss: 0.00004142
Iteration 72/1000 | Loss: 0.00004142
Iteration 73/1000 | Loss: 0.00004141
Iteration 74/1000 | Loss: 0.00004141
Iteration 75/1000 | Loss: 0.00004140
Iteration 76/1000 | Loss: 0.00004140
Iteration 77/1000 | Loss: 0.00004140
Iteration 78/1000 | Loss: 0.00004139
Iteration 79/1000 | Loss: 0.00004139
Iteration 80/1000 | Loss: 0.00004139
Iteration 81/1000 | Loss: 0.00004138
Iteration 82/1000 | Loss: 0.00004138
Iteration 83/1000 | Loss: 0.00004138
Iteration 84/1000 | Loss: 0.00004138
Iteration 85/1000 | Loss: 0.00004138
Iteration 86/1000 | Loss: 0.00004137
Iteration 87/1000 | Loss: 0.00004137
Iteration 88/1000 | Loss: 0.00004137
Iteration 89/1000 | Loss: 0.00004137
Iteration 90/1000 | Loss: 0.00004137
Iteration 91/1000 | Loss: 0.00004137
Iteration 92/1000 | Loss: 0.00004137
Iteration 93/1000 | Loss: 0.00004137
Iteration 94/1000 | Loss: 0.00004137
Iteration 95/1000 | Loss: 0.00004136
Iteration 96/1000 | Loss: 0.00004136
Iteration 97/1000 | Loss: 0.00004135
Iteration 98/1000 | Loss: 0.00004135
Iteration 99/1000 | Loss: 0.00004135
Iteration 100/1000 | Loss: 0.00004134
Iteration 101/1000 | Loss: 0.00004134
Iteration 102/1000 | Loss: 0.00004134
Iteration 103/1000 | Loss: 0.00004134
Iteration 104/1000 | Loss: 0.00004134
Iteration 105/1000 | Loss: 0.00004134
Iteration 106/1000 | Loss: 0.00004133
Iteration 107/1000 | Loss: 0.00004133
Iteration 108/1000 | Loss: 0.00004133
Iteration 109/1000 | Loss: 0.00004133
Iteration 110/1000 | Loss: 0.00004133
Iteration 111/1000 | Loss: 0.00004133
Iteration 112/1000 | Loss: 0.00004133
Iteration 113/1000 | Loss: 0.00004133
Iteration 114/1000 | Loss: 0.00004133
Iteration 115/1000 | Loss: 0.00004133
Iteration 116/1000 | Loss: 0.00004133
Iteration 117/1000 | Loss: 0.00004133
Iteration 118/1000 | Loss: 0.00004133
Iteration 119/1000 | Loss: 0.00004132
Iteration 120/1000 | Loss: 0.00004132
Iteration 121/1000 | Loss: 0.00004132
Iteration 122/1000 | Loss: 0.00004132
Iteration 123/1000 | Loss: 0.00004132
Iteration 124/1000 | Loss: 0.00004131
Iteration 125/1000 | Loss: 0.00004131
Iteration 126/1000 | Loss: 0.00004131
Iteration 127/1000 | Loss: 0.00004131
Iteration 128/1000 | Loss: 0.00004131
Iteration 129/1000 | Loss: 0.00004131
Iteration 130/1000 | Loss: 0.00004131
Iteration 131/1000 | Loss: 0.00004131
Iteration 132/1000 | Loss: 0.00004131
Iteration 133/1000 | Loss: 0.00004131
Iteration 134/1000 | Loss: 0.00004131
Iteration 135/1000 | Loss: 0.00004131
Iteration 136/1000 | Loss: 0.00004131
Iteration 137/1000 | Loss: 0.00004131
Iteration 138/1000 | Loss: 0.00004130
Iteration 139/1000 | Loss: 0.00004130
Iteration 140/1000 | Loss: 0.00004130
Iteration 141/1000 | Loss: 0.00004130
Iteration 142/1000 | Loss: 0.00004129
Iteration 143/1000 | Loss: 0.00004129
Iteration 144/1000 | Loss: 0.00004128
Iteration 145/1000 | Loss: 0.00004128
Iteration 146/1000 | Loss: 0.00004128
Iteration 147/1000 | Loss: 0.00004128
Iteration 148/1000 | Loss: 0.00004128
Iteration 149/1000 | Loss: 0.00004128
Iteration 150/1000 | Loss: 0.00004128
Iteration 151/1000 | Loss: 0.00004128
Iteration 152/1000 | Loss: 0.00004128
Iteration 153/1000 | Loss: 0.00004128
Iteration 154/1000 | Loss: 0.00004128
Iteration 155/1000 | Loss: 0.00004128
Iteration 156/1000 | Loss: 0.00004127
Iteration 157/1000 | Loss: 0.00004127
Iteration 158/1000 | Loss: 0.00004127
Iteration 159/1000 | Loss: 0.00004127
Iteration 160/1000 | Loss: 0.00004126
Iteration 161/1000 | Loss: 0.00004126
Iteration 162/1000 | Loss: 0.00004126
Iteration 163/1000 | Loss: 0.00004126
Iteration 164/1000 | Loss: 0.00004126
Iteration 165/1000 | Loss: 0.00004125
Iteration 166/1000 | Loss: 0.00004125
Iteration 167/1000 | Loss: 0.00004125
Iteration 168/1000 | Loss: 0.00004125
Iteration 169/1000 | Loss: 0.00004125
Iteration 170/1000 | Loss: 0.00004125
Iteration 171/1000 | Loss: 0.00004125
Iteration 172/1000 | Loss: 0.00004124
Iteration 173/1000 | Loss: 0.00004124
Iteration 174/1000 | Loss: 0.00004124
Iteration 175/1000 | Loss: 0.00004124
Iteration 176/1000 | Loss: 0.00004124
Iteration 177/1000 | Loss: 0.00004124
Iteration 178/1000 | Loss: 0.00004124
Iteration 179/1000 | Loss: 0.00004123
Iteration 180/1000 | Loss: 0.00004123
Iteration 181/1000 | Loss: 0.00004123
Iteration 182/1000 | Loss: 0.00004123
Iteration 183/1000 | Loss: 0.00004123
Iteration 184/1000 | Loss: 0.00004123
Iteration 185/1000 | Loss: 0.00004123
Iteration 186/1000 | Loss: 0.00004123
Iteration 187/1000 | Loss: 0.00004123
Iteration 188/1000 | Loss: 0.00004123
Iteration 189/1000 | Loss: 0.00004122
Iteration 190/1000 | Loss: 0.00004122
Iteration 191/1000 | Loss: 0.00004122
Iteration 192/1000 | Loss: 0.00004122
Iteration 193/1000 | Loss: 0.00004122
Iteration 194/1000 | Loss: 0.00004122
Iteration 195/1000 | Loss: 0.00004122
Iteration 196/1000 | Loss: 0.00004122
Iteration 197/1000 | Loss: 0.00004122
Iteration 198/1000 | Loss: 0.00004122
Iteration 199/1000 | Loss: 0.00004122
Iteration 200/1000 | Loss: 0.00004121
Iteration 201/1000 | Loss: 0.00004121
Iteration 202/1000 | Loss: 0.00004121
Iteration 203/1000 | Loss: 0.00004121
Iteration 204/1000 | Loss: 0.00004121
Iteration 205/1000 | Loss: 0.00004121
Iteration 206/1000 | Loss: 0.00004120
Iteration 207/1000 | Loss: 0.00004120
Iteration 208/1000 | Loss: 0.00004120
Iteration 209/1000 | Loss: 0.00004120
Iteration 210/1000 | Loss: 0.00004120
Iteration 211/1000 | Loss: 0.00004120
Iteration 212/1000 | Loss: 0.00004120
Iteration 213/1000 | Loss: 0.00004120
Iteration 214/1000 | Loss: 0.00004120
Iteration 215/1000 | Loss: 0.00004120
Iteration 216/1000 | Loss: 0.00004120
Iteration 217/1000 | Loss: 0.00004120
Iteration 218/1000 | Loss: 0.00004120
Iteration 219/1000 | Loss: 0.00004120
Iteration 220/1000 | Loss: 0.00004120
Iteration 221/1000 | Loss: 0.00004119
Iteration 222/1000 | Loss: 0.00004119
Iteration 223/1000 | Loss: 0.00004119
Iteration 224/1000 | Loss: 0.00004119
Iteration 225/1000 | Loss: 0.00004119
Iteration 226/1000 | Loss: 0.00004119
Iteration 227/1000 | Loss: 0.00004119
Iteration 228/1000 | Loss: 0.00004119
Iteration 229/1000 | Loss: 0.00004119
Iteration 230/1000 | Loss: 0.00004119
Iteration 231/1000 | Loss: 0.00004119
Iteration 232/1000 | Loss: 0.00004119
Iteration 233/1000 | Loss: 0.00004118
Iteration 234/1000 | Loss: 0.00004118
Iteration 235/1000 | Loss: 0.00004118
Iteration 236/1000 | Loss: 0.00004118
Iteration 237/1000 | Loss: 0.00004118
Iteration 238/1000 | Loss: 0.00004118
Iteration 239/1000 | Loss: 0.00004118
Iteration 240/1000 | Loss: 0.00004118
Iteration 241/1000 | Loss: 0.00004118
Iteration 242/1000 | Loss: 0.00004118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [4.1182953282259405e-05, 4.1182953282259405e-05, 4.1182953282259405e-05, 4.1182953282259405e-05, 4.1182953282259405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1182953282259405e-05

Optimization complete. Final v2v error: 5.398100852966309 mm

Highest mean error: 5.861772537231445 mm for frame 4

Lowest mean error: 4.724921226501465 mm for frame 20

Saving results

Total time: 66.64192152023315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409764
Iteration 2/25 | Loss: 0.00134316
Iteration 3/25 | Loss: 0.00128787
Iteration 4/25 | Loss: 0.00128464
Iteration 5/25 | Loss: 0.00128464
Iteration 6/25 | Loss: 0.00128464
Iteration 7/25 | Loss: 0.00128464
Iteration 8/25 | Loss: 0.00128464
Iteration 9/25 | Loss: 0.00128464
Iteration 10/25 | Loss: 0.00128464
Iteration 11/25 | Loss: 0.00128464
Iteration 12/25 | Loss: 0.00128464
Iteration 13/25 | Loss: 0.00128464
Iteration 14/25 | Loss: 0.00128464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012846403988078237, 0.0012846403988078237, 0.0012846403988078237, 0.0012846403988078237, 0.0012846403988078237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012846403988078237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34472024
Iteration 2/25 | Loss: 0.00103197
Iteration 3/25 | Loss: 0.00103197
Iteration 4/25 | Loss: 0.00103197
Iteration 5/25 | Loss: 0.00103197
Iteration 6/25 | Loss: 0.00103197
Iteration 7/25 | Loss: 0.00103197
Iteration 8/25 | Loss: 0.00103197
Iteration 9/25 | Loss: 0.00103197
Iteration 10/25 | Loss: 0.00103197
Iteration 11/25 | Loss: 0.00103197
Iteration 12/25 | Loss: 0.00103197
Iteration 13/25 | Loss: 0.00103197
Iteration 14/25 | Loss: 0.00103197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010319699067622423, 0.0010319699067622423, 0.0010319699067622423, 0.0010319699067622423, 0.0010319699067622423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010319699067622423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103197
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001900
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001531
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001466
Iteration 10/1000 | Loss: 0.00001449
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001435
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001430
Iteration 16/1000 | Loss: 0.00001430
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001411
Iteration 19/1000 | Loss: 0.00001410
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001406
Iteration 22/1000 | Loss: 0.00001405
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001404
Iteration 25/1000 | Loss: 0.00001404
Iteration 26/1000 | Loss: 0.00001403
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001396
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001393
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001393
Iteration 38/1000 | Loss: 0.00001392
Iteration 39/1000 | Loss: 0.00001391
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001384
Iteration 43/1000 | Loss: 0.00001382
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001381
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001381
Iteration 48/1000 | Loss: 0.00001380
Iteration 49/1000 | Loss: 0.00001379
Iteration 50/1000 | Loss: 0.00001379
Iteration 51/1000 | Loss: 0.00001378
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001378
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001378
Iteration 57/1000 | Loss: 0.00001378
Iteration 58/1000 | Loss: 0.00001378
Iteration 59/1000 | Loss: 0.00001378
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001375
Iteration 66/1000 | Loss: 0.00001375
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001374
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001373
Iteration 71/1000 | Loss: 0.00001373
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001372
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001371
Iteration 77/1000 | Loss: 0.00001371
Iteration 78/1000 | Loss: 0.00001370
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001365
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001364
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001362
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001361
Iteration 108/1000 | Loss: 0.00001361
Iteration 109/1000 | Loss: 0.00001361
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001360
Iteration 115/1000 | Loss: 0.00001360
Iteration 116/1000 | Loss: 0.00001360
Iteration 117/1000 | Loss: 0.00001360
Iteration 118/1000 | Loss: 0.00001360
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001357
Iteration 127/1000 | Loss: 0.00001357
Iteration 128/1000 | Loss: 0.00001357
Iteration 129/1000 | Loss: 0.00001357
Iteration 130/1000 | Loss: 0.00001356
Iteration 131/1000 | Loss: 0.00001356
Iteration 132/1000 | Loss: 0.00001356
Iteration 133/1000 | Loss: 0.00001356
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001354
Iteration 137/1000 | Loss: 0.00001354
Iteration 138/1000 | Loss: 0.00001353
Iteration 139/1000 | Loss: 0.00001353
Iteration 140/1000 | Loss: 0.00001352
Iteration 141/1000 | Loss: 0.00001352
Iteration 142/1000 | Loss: 0.00001352
Iteration 143/1000 | Loss: 0.00001352
Iteration 144/1000 | Loss: 0.00001352
Iteration 145/1000 | Loss: 0.00001352
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001352
Iteration 149/1000 | Loss: 0.00001351
Iteration 150/1000 | Loss: 0.00001351
Iteration 151/1000 | Loss: 0.00001351
Iteration 152/1000 | Loss: 0.00001351
Iteration 153/1000 | Loss: 0.00001351
Iteration 154/1000 | Loss: 0.00001351
Iteration 155/1000 | Loss: 0.00001351
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001350
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001350
Iteration 165/1000 | Loss: 0.00001350
Iteration 166/1000 | Loss: 0.00001350
Iteration 167/1000 | Loss: 0.00001350
Iteration 168/1000 | Loss: 0.00001350
Iteration 169/1000 | Loss: 0.00001350
Iteration 170/1000 | Loss: 0.00001350
Iteration 171/1000 | Loss: 0.00001350
Iteration 172/1000 | Loss: 0.00001350
Iteration 173/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.3496330211637542e-05, 1.3496330211637542e-05, 1.3496330211637542e-05, 1.3496330211637542e-05, 1.3496330211637542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3496330211637542e-05

Optimization complete. Final v2v error: 3.119586229324341 mm

Highest mean error: 3.425823211669922 mm for frame 47

Lowest mean error: 2.9913647174835205 mm for frame 72

Saving results

Total time: 37.99464726448059
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778459
Iteration 2/25 | Loss: 0.00193783
Iteration 3/25 | Loss: 0.00140689
Iteration 4/25 | Loss: 0.00135327
Iteration 5/25 | Loss: 0.00134958
Iteration 6/25 | Loss: 0.00134900
Iteration 7/25 | Loss: 0.00134900
Iteration 8/25 | Loss: 0.00134895
Iteration 9/25 | Loss: 0.00134895
Iteration 10/25 | Loss: 0.00134895
Iteration 11/25 | Loss: 0.00134895
Iteration 12/25 | Loss: 0.00134895
Iteration 13/25 | Loss: 0.00134895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013489482225850224, 0.0013489482225850224, 0.0013489482225850224, 0.0013489482225850224, 0.0013489482225850224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013489482225850224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35565472
Iteration 2/25 | Loss: 0.00103147
Iteration 3/25 | Loss: 0.00103145
Iteration 4/25 | Loss: 0.00103145
Iteration 5/25 | Loss: 0.00103145
Iteration 6/25 | Loss: 0.00103145
Iteration 7/25 | Loss: 0.00103145
Iteration 8/25 | Loss: 0.00103145
Iteration 9/25 | Loss: 0.00103145
Iteration 10/25 | Loss: 0.00103145
Iteration 11/25 | Loss: 0.00103145
Iteration 12/25 | Loss: 0.00103145
Iteration 13/25 | Loss: 0.00103145
Iteration 14/25 | Loss: 0.00103145
Iteration 15/25 | Loss: 0.00103145
Iteration 16/25 | Loss: 0.00103145
Iteration 17/25 | Loss: 0.00103145
Iteration 18/25 | Loss: 0.00103145
Iteration 19/25 | Loss: 0.00103145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010314473183825612, 0.0010314473183825612, 0.0010314473183825612, 0.0010314473183825612, 0.0010314473183825612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010314473183825612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103145
Iteration 2/1000 | Loss: 0.00003806
Iteration 3/1000 | Loss: 0.00002782
Iteration 4/1000 | Loss: 0.00002434
Iteration 5/1000 | Loss: 0.00002337
Iteration 6/1000 | Loss: 0.00002278
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002224
Iteration 9/1000 | Loss: 0.00002199
Iteration 10/1000 | Loss: 0.00002181
Iteration 11/1000 | Loss: 0.00002174
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00002154
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002151
Iteration 16/1000 | Loss: 0.00002151
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00002144
Iteration 19/1000 | Loss: 0.00002143
Iteration 20/1000 | Loss: 0.00002143
Iteration 21/1000 | Loss: 0.00002142
Iteration 22/1000 | Loss: 0.00002137
Iteration 23/1000 | Loss: 0.00002136
Iteration 24/1000 | Loss: 0.00002134
Iteration 25/1000 | Loss: 0.00002134
Iteration 26/1000 | Loss: 0.00002134
Iteration 27/1000 | Loss: 0.00002134
Iteration 28/1000 | Loss: 0.00002133
Iteration 29/1000 | Loss: 0.00002133
Iteration 30/1000 | Loss: 0.00002133
Iteration 31/1000 | Loss: 0.00002132
Iteration 32/1000 | Loss: 0.00002131
Iteration 33/1000 | Loss: 0.00002131
Iteration 34/1000 | Loss: 0.00002131
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002130
Iteration 37/1000 | Loss: 0.00002130
Iteration 38/1000 | Loss: 0.00002130
Iteration 39/1000 | Loss: 0.00002129
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002128
Iteration 44/1000 | Loss: 0.00002128
Iteration 45/1000 | Loss: 0.00002127
Iteration 46/1000 | Loss: 0.00002127
Iteration 47/1000 | Loss: 0.00002125
Iteration 48/1000 | Loss: 0.00002123
Iteration 49/1000 | Loss: 0.00002123
Iteration 50/1000 | Loss: 0.00002123
Iteration 51/1000 | Loss: 0.00002123
Iteration 52/1000 | Loss: 0.00002123
Iteration 53/1000 | Loss: 0.00002122
Iteration 54/1000 | Loss: 0.00002122
Iteration 55/1000 | Loss: 0.00002122
Iteration 56/1000 | Loss: 0.00002122
Iteration 57/1000 | Loss: 0.00002122
Iteration 58/1000 | Loss: 0.00002122
Iteration 59/1000 | Loss: 0.00002121
Iteration 60/1000 | Loss: 0.00002121
Iteration 61/1000 | Loss: 0.00002121
Iteration 62/1000 | Loss: 0.00002121
Iteration 63/1000 | Loss: 0.00002120
Iteration 64/1000 | Loss: 0.00002120
Iteration 65/1000 | Loss: 0.00002120
Iteration 66/1000 | Loss: 0.00002119
Iteration 67/1000 | Loss: 0.00002119
Iteration 68/1000 | Loss: 0.00002119
Iteration 69/1000 | Loss: 0.00002119
Iteration 70/1000 | Loss: 0.00002119
Iteration 71/1000 | Loss: 0.00002119
Iteration 72/1000 | Loss: 0.00002119
Iteration 73/1000 | Loss: 0.00002119
Iteration 74/1000 | Loss: 0.00002119
Iteration 75/1000 | Loss: 0.00002118
Iteration 76/1000 | Loss: 0.00002117
Iteration 77/1000 | Loss: 0.00002117
Iteration 78/1000 | Loss: 0.00002117
Iteration 79/1000 | Loss: 0.00002117
Iteration 80/1000 | Loss: 0.00002117
Iteration 81/1000 | Loss: 0.00002117
Iteration 82/1000 | Loss: 0.00002117
Iteration 83/1000 | Loss: 0.00002117
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00002116
Iteration 86/1000 | Loss: 0.00002116
Iteration 87/1000 | Loss: 0.00002115
Iteration 88/1000 | Loss: 0.00002115
Iteration 89/1000 | Loss: 0.00002115
Iteration 90/1000 | Loss: 0.00002115
Iteration 91/1000 | Loss: 0.00002115
Iteration 92/1000 | Loss: 0.00002115
Iteration 93/1000 | Loss: 0.00002115
Iteration 94/1000 | Loss: 0.00002115
Iteration 95/1000 | Loss: 0.00002114
Iteration 96/1000 | Loss: 0.00002114
Iteration 97/1000 | Loss: 0.00002114
Iteration 98/1000 | Loss: 0.00002114
Iteration 99/1000 | Loss: 0.00002114
Iteration 100/1000 | Loss: 0.00002114
Iteration 101/1000 | Loss: 0.00002113
Iteration 102/1000 | Loss: 0.00002113
Iteration 103/1000 | Loss: 0.00002113
Iteration 104/1000 | Loss: 0.00002113
Iteration 105/1000 | Loss: 0.00002113
Iteration 106/1000 | Loss: 0.00002113
Iteration 107/1000 | Loss: 0.00002113
Iteration 108/1000 | Loss: 0.00002113
Iteration 109/1000 | Loss: 0.00002113
Iteration 110/1000 | Loss: 0.00002113
Iteration 111/1000 | Loss: 0.00002113
Iteration 112/1000 | Loss: 0.00002113
Iteration 113/1000 | Loss: 0.00002113
Iteration 114/1000 | Loss: 0.00002113
Iteration 115/1000 | Loss: 0.00002113
Iteration 116/1000 | Loss: 0.00002113
Iteration 117/1000 | Loss: 0.00002113
Iteration 118/1000 | Loss: 0.00002113
Iteration 119/1000 | Loss: 0.00002113
Iteration 120/1000 | Loss: 0.00002113
Iteration 121/1000 | Loss: 0.00002112
Iteration 122/1000 | Loss: 0.00002112
Iteration 123/1000 | Loss: 0.00002112
Iteration 124/1000 | Loss: 0.00002112
Iteration 125/1000 | Loss: 0.00002112
Iteration 126/1000 | Loss: 0.00002112
Iteration 127/1000 | Loss: 0.00002112
Iteration 128/1000 | Loss: 0.00002112
Iteration 129/1000 | Loss: 0.00002112
Iteration 130/1000 | Loss: 0.00002112
Iteration 131/1000 | Loss: 0.00002112
Iteration 132/1000 | Loss: 0.00002112
Iteration 133/1000 | Loss: 0.00002112
Iteration 134/1000 | Loss: 0.00002112
Iteration 135/1000 | Loss: 0.00002112
Iteration 136/1000 | Loss: 0.00002112
Iteration 137/1000 | Loss: 0.00002112
Iteration 138/1000 | Loss: 0.00002112
Iteration 139/1000 | Loss: 0.00002112
Iteration 140/1000 | Loss: 0.00002112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.1124120394233614e-05, 2.1124120394233614e-05, 2.1124120394233614e-05, 2.1124120394233614e-05, 2.1124120394233614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1124120394233614e-05

Optimization complete. Final v2v error: 3.879047393798828 mm

Highest mean error: 4.417355060577393 mm for frame 135

Lowest mean error: 3.618877649307251 mm for frame 110

Saving results

Total time: 33.82248520851135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971953
Iteration 2/25 | Loss: 0.00356928
Iteration 3/25 | Loss: 0.00223219
Iteration 4/25 | Loss: 0.00206479
Iteration 5/25 | Loss: 0.00184036
Iteration 6/25 | Loss: 0.00179091
Iteration 7/25 | Loss: 0.00171316
Iteration 8/25 | Loss: 0.00165672
Iteration 9/25 | Loss: 0.00162680
Iteration 10/25 | Loss: 0.00163044
Iteration 11/25 | Loss: 0.00158831
Iteration 12/25 | Loss: 0.00157839
Iteration 13/25 | Loss: 0.00154703
Iteration 14/25 | Loss: 0.00153650
Iteration 15/25 | Loss: 0.00152456
Iteration 16/25 | Loss: 0.00151041
Iteration 17/25 | Loss: 0.00151061
Iteration 18/25 | Loss: 0.00150720
Iteration 19/25 | Loss: 0.00148722
Iteration 20/25 | Loss: 0.00147725
Iteration 21/25 | Loss: 0.00147058
Iteration 22/25 | Loss: 0.00146765
Iteration 23/25 | Loss: 0.00146503
Iteration 24/25 | Loss: 0.00146929
Iteration 25/25 | Loss: 0.00146004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38728881
Iteration 2/25 | Loss: 0.01099501
Iteration 3/25 | Loss: 0.04662865
Iteration 4/25 | Loss: 0.01093842
Iteration 5/25 | Loss: 0.00259913
Iteration 6/25 | Loss: 0.00259912
Iteration 7/25 | Loss: 0.00259912
Iteration 8/25 | Loss: 0.00259912
Iteration 9/25 | Loss: 0.00259912
Iteration 10/25 | Loss: 0.00259912
Iteration 11/25 | Loss: 0.00259912
Iteration 12/25 | Loss: 0.00259912
Iteration 13/25 | Loss: 0.00259912
Iteration 14/25 | Loss: 0.00259912
Iteration 15/25 | Loss: 0.00259912
Iteration 16/25 | Loss: 0.00259912
Iteration 17/25 | Loss: 0.00259912
Iteration 18/25 | Loss: 0.00259912
Iteration 19/25 | Loss: 0.00259912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002599116414785385, 0.002599116414785385, 0.002599116414785385, 0.002599116414785385, 0.002599116414785385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002599116414785385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259912
Iteration 2/1000 | Loss: 0.00062484
Iteration 3/1000 | Loss: 0.00301904
Iteration 4/1000 | Loss: 0.00074763
Iteration 5/1000 | Loss: 0.00041914
Iteration 6/1000 | Loss: 0.00069308
Iteration 7/1000 | Loss: 0.00117675
Iteration 8/1000 | Loss: 0.00074740
Iteration 9/1000 | Loss: 0.00027047
Iteration 10/1000 | Loss: 0.00022369
Iteration 11/1000 | Loss: 0.00037277
Iteration 12/1000 | Loss: 0.00050920
Iteration 13/1000 | Loss: 0.00111477
Iteration 14/1000 | Loss: 0.00030097
Iteration 15/1000 | Loss: 0.00033983
Iteration 16/1000 | Loss: 0.00036713
Iteration 17/1000 | Loss: 0.00013235
Iteration 18/1000 | Loss: 0.00055145
Iteration 19/1000 | Loss: 0.00027649
Iteration 20/1000 | Loss: 0.00024429
Iteration 21/1000 | Loss: 0.00038954
Iteration 22/1000 | Loss: 0.00014034
Iteration 23/1000 | Loss: 0.00029970
Iteration 24/1000 | Loss: 0.00011630
Iteration 25/1000 | Loss: 0.00043582
Iteration 26/1000 | Loss: 0.00050299
Iteration 27/1000 | Loss: 0.00011035
Iteration 28/1000 | Loss: 0.00019269
Iteration 29/1000 | Loss: 0.00020490
Iteration 30/1000 | Loss: 0.00203685
Iteration 31/1000 | Loss: 0.00135320
Iteration 32/1000 | Loss: 0.00019194
Iteration 33/1000 | Loss: 0.00083291
Iteration 34/1000 | Loss: 0.00145719
Iteration 35/1000 | Loss: 0.00111259
Iteration 36/1000 | Loss: 0.00015207
Iteration 37/1000 | Loss: 0.00016317
Iteration 38/1000 | Loss: 0.00194010
Iteration 39/1000 | Loss: 0.00246033
Iteration 40/1000 | Loss: 0.00114209
Iteration 41/1000 | Loss: 0.00055534
Iteration 42/1000 | Loss: 0.00043998
Iteration 43/1000 | Loss: 0.00385989
Iteration 44/1000 | Loss: 0.00120129
Iteration 45/1000 | Loss: 0.00013023
Iteration 46/1000 | Loss: 0.00011259
Iteration 47/1000 | Loss: 0.00068286
Iteration 48/1000 | Loss: 0.00009966
Iteration 49/1000 | Loss: 0.00009582
Iteration 50/1000 | Loss: 0.00009318
Iteration 51/1000 | Loss: 0.00028001
Iteration 52/1000 | Loss: 0.00229658
Iteration 53/1000 | Loss: 0.00318053
Iteration 54/1000 | Loss: 0.00049681
Iteration 55/1000 | Loss: 0.00296688
Iteration 56/1000 | Loss: 0.00129560
Iteration 57/1000 | Loss: 0.00057061
Iteration 58/1000 | Loss: 0.00016444
Iteration 59/1000 | Loss: 0.00075752
Iteration 60/1000 | Loss: 0.00080811
Iteration 61/1000 | Loss: 0.00158776
Iteration 62/1000 | Loss: 0.00074419
Iteration 63/1000 | Loss: 0.00178660
Iteration 64/1000 | Loss: 0.00096726
Iteration 65/1000 | Loss: 0.00061336
Iteration 66/1000 | Loss: 0.00103272
Iteration 67/1000 | Loss: 0.00016765
Iteration 68/1000 | Loss: 0.00120722
Iteration 69/1000 | Loss: 0.00022991
Iteration 70/1000 | Loss: 0.00011075
Iteration 71/1000 | Loss: 0.00007770
Iteration 72/1000 | Loss: 0.00015637
Iteration 73/1000 | Loss: 0.00037069
Iteration 74/1000 | Loss: 0.00075720
Iteration 75/1000 | Loss: 0.00021990
Iteration 76/1000 | Loss: 0.00011117
Iteration 77/1000 | Loss: 0.00006667
Iteration 78/1000 | Loss: 0.00075359
Iteration 79/1000 | Loss: 0.00013926
Iteration 80/1000 | Loss: 0.00019267
Iteration 81/1000 | Loss: 0.00013194
Iteration 82/1000 | Loss: 0.00005729
Iteration 83/1000 | Loss: 0.00014842
Iteration 84/1000 | Loss: 0.00005330
Iteration 85/1000 | Loss: 0.00038150
Iteration 86/1000 | Loss: 0.00020670
Iteration 87/1000 | Loss: 0.00005178
Iteration 88/1000 | Loss: 0.00026051
Iteration 89/1000 | Loss: 0.00004846
Iteration 90/1000 | Loss: 0.00128645
Iteration 91/1000 | Loss: 0.00137567
Iteration 92/1000 | Loss: 0.00130459
Iteration 93/1000 | Loss: 0.00126253
Iteration 94/1000 | Loss: 0.00031055
Iteration 95/1000 | Loss: 0.00018410
Iteration 96/1000 | Loss: 0.00004921
Iteration 97/1000 | Loss: 0.00004755
Iteration 98/1000 | Loss: 0.00026900
Iteration 99/1000 | Loss: 0.00095879
Iteration 100/1000 | Loss: 0.00045428
Iteration 101/1000 | Loss: 0.00064539
Iteration 102/1000 | Loss: 0.00044213
Iteration 103/1000 | Loss: 0.00008635
Iteration 104/1000 | Loss: 0.00006277
Iteration 105/1000 | Loss: 0.00005239
Iteration 106/1000 | Loss: 0.00020295
Iteration 107/1000 | Loss: 0.00022311
Iteration 108/1000 | Loss: 0.00004800
Iteration 109/1000 | Loss: 0.00004624
Iteration 110/1000 | Loss: 0.00016976
Iteration 111/1000 | Loss: 0.00004554
Iteration 112/1000 | Loss: 0.00027900
Iteration 113/1000 | Loss: 0.00004537
Iteration 114/1000 | Loss: 0.00004502
Iteration 115/1000 | Loss: 0.00004469
Iteration 116/1000 | Loss: 0.00072738
Iteration 117/1000 | Loss: 0.00028050
Iteration 118/1000 | Loss: 0.00047864
Iteration 119/1000 | Loss: 0.00005286
Iteration 120/1000 | Loss: 0.00014693
Iteration 121/1000 | Loss: 0.00004502
Iteration 122/1000 | Loss: 0.00004354
Iteration 123/1000 | Loss: 0.00004295
Iteration 124/1000 | Loss: 0.00004254
Iteration 125/1000 | Loss: 0.00004218
Iteration 126/1000 | Loss: 0.00004199
Iteration 127/1000 | Loss: 0.00066810
Iteration 128/1000 | Loss: 0.00047940
Iteration 129/1000 | Loss: 0.00010924
Iteration 130/1000 | Loss: 0.00004496
Iteration 131/1000 | Loss: 0.00008181
Iteration 132/1000 | Loss: 0.00038386
Iteration 133/1000 | Loss: 0.00004173
Iteration 134/1000 | Loss: 0.00026904
Iteration 135/1000 | Loss: 0.00004919
Iteration 136/1000 | Loss: 0.00010874
Iteration 137/1000 | Loss: 0.00013545
Iteration 138/1000 | Loss: 0.00010147
Iteration 139/1000 | Loss: 0.00005940
Iteration 140/1000 | Loss: 0.00004289
Iteration 141/1000 | Loss: 0.00004004
Iteration 142/1000 | Loss: 0.00003990
Iteration 143/1000 | Loss: 0.00003986
Iteration 144/1000 | Loss: 0.00003983
Iteration 145/1000 | Loss: 0.00008818
Iteration 146/1000 | Loss: 0.00051456
Iteration 147/1000 | Loss: 0.00006218
Iteration 148/1000 | Loss: 0.00003973
Iteration 149/1000 | Loss: 0.00007807
Iteration 150/1000 | Loss: 0.00004343
Iteration 151/1000 | Loss: 0.00004100
Iteration 152/1000 | Loss: 0.00003976
Iteration 153/1000 | Loss: 0.00003962
Iteration 154/1000 | Loss: 0.00003960
Iteration 155/1000 | Loss: 0.00003960
Iteration 156/1000 | Loss: 0.00003959
Iteration 157/1000 | Loss: 0.00003959
Iteration 158/1000 | Loss: 0.00003959
Iteration 159/1000 | Loss: 0.00003959
Iteration 160/1000 | Loss: 0.00003959
Iteration 161/1000 | Loss: 0.00003959
Iteration 162/1000 | Loss: 0.00003959
Iteration 163/1000 | Loss: 0.00003959
Iteration 164/1000 | Loss: 0.00003958
Iteration 165/1000 | Loss: 0.00003958
Iteration 166/1000 | Loss: 0.00003958
Iteration 167/1000 | Loss: 0.00003958
Iteration 168/1000 | Loss: 0.00003958
Iteration 169/1000 | Loss: 0.00003958
Iteration 170/1000 | Loss: 0.00003958
Iteration 171/1000 | Loss: 0.00003958
Iteration 172/1000 | Loss: 0.00003958
Iteration 173/1000 | Loss: 0.00003958
Iteration 174/1000 | Loss: 0.00003958
Iteration 175/1000 | Loss: 0.00003958
Iteration 176/1000 | Loss: 0.00003958
Iteration 177/1000 | Loss: 0.00003958
Iteration 178/1000 | Loss: 0.00003958
Iteration 179/1000 | Loss: 0.00003957
Iteration 180/1000 | Loss: 0.00003957
Iteration 181/1000 | Loss: 0.00005702
Iteration 182/1000 | Loss: 0.00005702
Iteration 183/1000 | Loss: 0.00006977
Iteration 184/1000 | Loss: 0.00033670
Iteration 185/1000 | Loss: 0.00004984
Iteration 186/1000 | Loss: 0.00003966
Iteration 187/1000 | Loss: 0.00010814
Iteration 188/1000 | Loss: 0.00004584
Iteration 189/1000 | Loss: 0.00003965
Iteration 190/1000 | Loss: 0.00005325
Iteration 191/1000 | Loss: 0.00004169
Iteration 192/1000 | Loss: 0.00003963
Iteration 193/1000 | Loss: 0.00003963
Iteration 194/1000 | Loss: 0.00003962
Iteration 195/1000 | Loss: 0.00004459
Iteration 196/1000 | Loss: 0.00003958
Iteration 197/1000 | Loss: 0.00005553
Iteration 198/1000 | Loss: 0.00007713
Iteration 199/1000 | Loss: 0.00004393
Iteration 200/1000 | Loss: 0.00003955
Iteration 201/1000 | Loss: 0.00003950
Iteration 202/1000 | Loss: 0.00003949
Iteration 203/1000 | Loss: 0.00006751
Iteration 204/1000 | Loss: 0.00004344
Iteration 205/1000 | Loss: 0.00003950
Iteration 206/1000 | Loss: 0.00003949
Iteration 207/1000 | Loss: 0.00004593
Iteration 208/1000 | Loss: 0.00004809
Iteration 209/1000 | Loss: 0.00003941
Iteration 210/1000 | Loss: 0.00003941
Iteration 211/1000 | Loss: 0.00003939
Iteration 212/1000 | Loss: 0.00003939
Iteration 213/1000 | Loss: 0.00004272
Iteration 214/1000 | Loss: 0.00003937
Iteration 215/1000 | Loss: 0.00003935
Iteration 216/1000 | Loss: 0.00003935
Iteration 217/1000 | Loss: 0.00003935
Iteration 218/1000 | Loss: 0.00003934
Iteration 219/1000 | Loss: 0.00003934
Iteration 220/1000 | Loss: 0.00003933
Iteration 221/1000 | Loss: 0.00003933
Iteration 222/1000 | Loss: 0.00003929
Iteration 223/1000 | Loss: 0.00003928
Iteration 224/1000 | Loss: 0.00003928
Iteration 225/1000 | Loss: 0.00003927
Iteration 226/1000 | Loss: 0.00003925
Iteration 227/1000 | Loss: 0.00020218
Iteration 228/1000 | Loss: 0.00003970
Iteration 229/1000 | Loss: 0.00003869
Iteration 230/1000 | Loss: 0.00003830
Iteration 231/1000 | Loss: 0.00003795
Iteration 232/1000 | Loss: 0.00003766
Iteration 233/1000 | Loss: 0.00003762
Iteration 234/1000 | Loss: 0.00003746
Iteration 235/1000 | Loss: 0.00003743
Iteration 236/1000 | Loss: 0.00003743
Iteration 237/1000 | Loss: 0.00003743
Iteration 238/1000 | Loss: 0.00003743
Iteration 239/1000 | Loss: 0.00003743
Iteration 240/1000 | Loss: 0.00003743
Iteration 241/1000 | Loss: 0.00003743
Iteration 242/1000 | Loss: 0.00003743
Iteration 243/1000 | Loss: 0.00003743
Iteration 244/1000 | Loss: 0.00003743
Iteration 245/1000 | Loss: 0.00003742
Iteration 246/1000 | Loss: 0.00003742
Iteration 247/1000 | Loss: 0.00003741
Iteration 248/1000 | Loss: 0.00003741
Iteration 249/1000 | Loss: 0.00003741
Iteration 250/1000 | Loss: 0.00003740
Iteration 251/1000 | Loss: 0.00003740
Iteration 252/1000 | Loss: 0.00003739
Iteration 253/1000 | Loss: 0.00003739
Iteration 254/1000 | Loss: 0.00003738
Iteration 255/1000 | Loss: 0.00003738
Iteration 256/1000 | Loss: 0.00003738
Iteration 257/1000 | Loss: 0.00003737
Iteration 258/1000 | Loss: 0.00003737
Iteration 259/1000 | Loss: 0.00003736
Iteration 260/1000 | Loss: 0.00003736
Iteration 261/1000 | Loss: 0.00003736
Iteration 262/1000 | Loss: 0.00003735
Iteration 263/1000 | Loss: 0.00003735
Iteration 264/1000 | Loss: 0.00003734
Iteration 265/1000 | Loss: 0.00003734
Iteration 266/1000 | Loss: 0.00003734
Iteration 267/1000 | Loss: 0.00003734
Iteration 268/1000 | Loss: 0.00003733
Iteration 269/1000 | Loss: 0.00003733
Iteration 270/1000 | Loss: 0.00003733
Iteration 271/1000 | Loss: 0.00003733
Iteration 272/1000 | Loss: 0.00003733
Iteration 273/1000 | Loss: 0.00003732
Iteration 274/1000 | Loss: 0.00003732
Iteration 275/1000 | Loss: 0.00003732
Iteration 276/1000 | Loss: 0.00003732
Iteration 277/1000 | Loss: 0.00003731
Iteration 278/1000 | Loss: 0.00003731
Iteration 279/1000 | Loss: 0.00003731
Iteration 280/1000 | Loss: 0.00003731
Iteration 281/1000 | Loss: 0.00003730
Iteration 282/1000 | Loss: 0.00003730
Iteration 283/1000 | Loss: 0.00003730
Iteration 284/1000 | Loss: 0.00003730
Iteration 285/1000 | Loss: 0.00003730
Iteration 286/1000 | Loss: 0.00003729
Iteration 287/1000 | Loss: 0.00003729
Iteration 288/1000 | Loss: 0.00003729
Iteration 289/1000 | Loss: 0.00003729
Iteration 290/1000 | Loss: 0.00003729
Iteration 291/1000 | Loss: 0.00003729
Iteration 292/1000 | Loss: 0.00003729
Iteration 293/1000 | Loss: 0.00003729
Iteration 294/1000 | Loss: 0.00003729
Iteration 295/1000 | Loss: 0.00003729
Iteration 296/1000 | Loss: 0.00003729
Iteration 297/1000 | Loss: 0.00003728
Iteration 298/1000 | Loss: 0.00003728
Iteration 299/1000 | Loss: 0.00003728
Iteration 300/1000 | Loss: 0.00003728
Iteration 301/1000 | Loss: 0.00003728
Iteration 302/1000 | Loss: 0.00003727
Iteration 303/1000 | Loss: 0.00003727
Iteration 304/1000 | Loss: 0.00003727
Iteration 305/1000 | Loss: 0.00003727
Iteration 306/1000 | Loss: 0.00003727
Iteration 307/1000 | Loss: 0.00003727
Iteration 308/1000 | Loss: 0.00003727
Iteration 309/1000 | Loss: 0.00003727
Iteration 310/1000 | Loss: 0.00003727
Iteration 311/1000 | Loss: 0.00003727
Iteration 312/1000 | Loss: 0.00003727
Iteration 313/1000 | Loss: 0.00003727
Iteration 314/1000 | Loss: 0.00003727
Iteration 315/1000 | Loss: 0.00003727
Iteration 316/1000 | Loss: 0.00003727
Iteration 317/1000 | Loss: 0.00003727
Iteration 318/1000 | Loss: 0.00003727
Iteration 319/1000 | Loss: 0.00003727
Iteration 320/1000 | Loss: 0.00003727
Iteration 321/1000 | Loss: 0.00003727
Iteration 322/1000 | Loss: 0.00003727
Iteration 323/1000 | Loss: 0.00003727
Iteration 324/1000 | Loss: 0.00003727
Iteration 325/1000 | Loss: 0.00003727
Iteration 326/1000 | Loss: 0.00003727
Iteration 327/1000 | Loss: 0.00003727
Iteration 328/1000 | Loss: 0.00003727
Iteration 329/1000 | Loss: 0.00003727
Iteration 330/1000 | Loss: 0.00003727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [3.726757859112695e-05, 3.726757859112695e-05, 3.726757859112695e-05, 3.726757859112695e-05, 3.726757859112695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.726757859112695e-05

Optimization complete. Final v2v error: 4.836211681365967 mm

Highest mean error: 12.092192649841309 mm for frame 126

Lowest mean error: 3.3960819244384766 mm for frame 110

Saving results

Total time: 351.0315318107605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961823
Iteration 2/25 | Loss: 0.00289565
Iteration 3/25 | Loss: 0.00189949
Iteration 4/25 | Loss: 0.00179727
Iteration 5/25 | Loss: 0.00170363
Iteration 6/25 | Loss: 0.00153149
Iteration 7/25 | Loss: 0.00149689
Iteration 8/25 | Loss: 0.00143265
Iteration 9/25 | Loss: 0.00138735
Iteration 10/25 | Loss: 0.00137591
Iteration 11/25 | Loss: 0.00137388
Iteration 12/25 | Loss: 0.00137207
Iteration 13/25 | Loss: 0.00137579
Iteration 14/25 | Loss: 0.00137903
Iteration 15/25 | Loss: 0.00137880
Iteration 16/25 | Loss: 0.00138483
Iteration 17/25 | Loss: 0.00138128
Iteration 18/25 | Loss: 0.00137756
Iteration 19/25 | Loss: 0.00137669
Iteration 20/25 | Loss: 0.00137327
Iteration 21/25 | Loss: 0.00137883
Iteration 22/25 | Loss: 0.00137481
Iteration 23/25 | Loss: 0.00137231
Iteration 24/25 | Loss: 0.00136536
Iteration 25/25 | Loss: 0.00136516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31288123
Iteration 2/25 | Loss: 0.00139333
Iteration 3/25 | Loss: 0.00137975
Iteration 4/25 | Loss: 0.00137975
Iteration 5/25 | Loss: 0.00137975
Iteration 6/25 | Loss: 0.00137975
Iteration 7/25 | Loss: 0.00137975
Iteration 8/25 | Loss: 0.00137975
Iteration 9/25 | Loss: 0.00137975
Iteration 10/25 | Loss: 0.00137975
Iteration 11/25 | Loss: 0.00137975
Iteration 12/25 | Loss: 0.00137975
Iteration 13/25 | Loss: 0.00137975
Iteration 14/25 | Loss: 0.00137975
Iteration 15/25 | Loss: 0.00137975
Iteration 16/25 | Loss: 0.00137975
Iteration 17/25 | Loss: 0.00137975
Iteration 18/25 | Loss: 0.00137975
Iteration 19/25 | Loss: 0.00137975
Iteration 20/25 | Loss: 0.00137975
Iteration 21/25 | Loss: 0.00137975
Iteration 22/25 | Loss: 0.00137975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013797517167404294, 0.0013797517167404294, 0.0013797517167404294, 0.0013797517167404294, 0.0013797517167404294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013797517167404294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137975
Iteration 2/1000 | Loss: 0.00013885
Iteration 3/1000 | Loss: 0.00007621
Iteration 4/1000 | Loss: 0.00006517
Iteration 5/1000 | Loss: 0.00026621
Iteration 6/1000 | Loss: 0.00007442
Iteration 7/1000 | Loss: 0.00006502
Iteration 8/1000 | Loss: 0.00006680
Iteration 9/1000 | Loss: 0.00006696
Iteration 10/1000 | Loss: 0.00028017
Iteration 11/1000 | Loss: 0.00025669
Iteration 12/1000 | Loss: 0.00006807
Iteration 13/1000 | Loss: 0.00005398
Iteration 14/1000 | Loss: 0.00005181
Iteration 15/1000 | Loss: 0.00005649
Iteration 16/1000 | Loss: 0.00006897
Iteration 17/1000 | Loss: 0.00014169
Iteration 18/1000 | Loss: 0.00005568
Iteration 19/1000 | Loss: 0.00004855
Iteration 20/1000 | Loss: 0.00004525
Iteration 21/1000 | Loss: 0.00004347
Iteration 22/1000 | Loss: 0.00004201
Iteration 23/1000 | Loss: 0.00004115
Iteration 24/1000 | Loss: 0.00004064
Iteration 25/1000 | Loss: 0.00004020
Iteration 26/1000 | Loss: 0.00003984
Iteration 27/1000 | Loss: 0.00005593
Iteration 28/1000 | Loss: 0.00004215
Iteration 29/1000 | Loss: 0.00004028
Iteration 30/1000 | Loss: 0.00003946
Iteration 31/1000 | Loss: 0.00003861
Iteration 32/1000 | Loss: 0.00070923
Iteration 33/1000 | Loss: 0.00004145
Iteration 34/1000 | Loss: 0.00065043
Iteration 35/1000 | Loss: 0.00004077
Iteration 36/1000 | Loss: 0.00060078
Iteration 37/1000 | Loss: 0.00055207
Iteration 38/1000 | Loss: 0.00056220
Iteration 39/1000 | Loss: 0.00059687
Iteration 40/1000 | Loss: 0.00074800
Iteration 41/1000 | Loss: 0.00006655
Iteration 42/1000 | Loss: 0.00031224
Iteration 43/1000 | Loss: 0.00043775
Iteration 44/1000 | Loss: 0.00004041
Iteration 45/1000 | Loss: 0.00003708
Iteration 46/1000 | Loss: 0.00003366
Iteration 47/1000 | Loss: 0.00003124
Iteration 48/1000 | Loss: 0.00002947
Iteration 49/1000 | Loss: 0.00002844
Iteration 50/1000 | Loss: 0.00002773
Iteration 51/1000 | Loss: 0.00002731
Iteration 52/1000 | Loss: 0.00002864
Iteration 53/1000 | Loss: 0.00002767
Iteration 54/1000 | Loss: 0.00002668
Iteration 55/1000 | Loss: 0.00002651
Iteration 56/1000 | Loss: 0.00002643
Iteration 57/1000 | Loss: 0.00002636
Iteration 58/1000 | Loss: 0.00002625
Iteration 59/1000 | Loss: 0.00002621
Iteration 60/1000 | Loss: 0.00002621
Iteration 61/1000 | Loss: 0.00002620
Iteration 62/1000 | Loss: 0.00002613
Iteration 63/1000 | Loss: 0.00002612
Iteration 64/1000 | Loss: 0.00002611
Iteration 65/1000 | Loss: 0.00002610
Iteration 66/1000 | Loss: 0.00002609
Iteration 67/1000 | Loss: 0.00002609
Iteration 68/1000 | Loss: 0.00002609
Iteration 69/1000 | Loss: 0.00002609
Iteration 70/1000 | Loss: 0.00002609
Iteration 71/1000 | Loss: 0.00002609
Iteration 72/1000 | Loss: 0.00002608
Iteration 73/1000 | Loss: 0.00002608
Iteration 74/1000 | Loss: 0.00002608
Iteration 75/1000 | Loss: 0.00002608
Iteration 76/1000 | Loss: 0.00002608
Iteration 77/1000 | Loss: 0.00002607
Iteration 78/1000 | Loss: 0.00002607
Iteration 79/1000 | Loss: 0.00002607
Iteration 80/1000 | Loss: 0.00002606
Iteration 81/1000 | Loss: 0.00002606
Iteration 82/1000 | Loss: 0.00002606
Iteration 83/1000 | Loss: 0.00002606
Iteration 84/1000 | Loss: 0.00002606
Iteration 85/1000 | Loss: 0.00002606
Iteration 86/1000 | Loss: 0.00002605
Iteration 87/1000 | Loss: 0.00002605
Iteration 88/1000 | Loss: 0.00002604
Iteration 89/1000 | Loss: 0.00002604
Iteration 90/1000 | Loss: 0.00002604
Iteration 91/1000 | Loss: 0.00002603
Iteration 92/1000 | Loss: 0.00002603
Iteration 93/1000 | Loss: 0.00002602
Iteration 94/1000 | Loss: 0.00002602
Iteration 95/1000 | Loss: 0.00002600
Iteration 96/1000 | Loss: 0.00002599
Iteration 97/1000 | Loss: 0.00002598
Iteration 98/1000 | Loss: 0.00002597
Iteration 99/1000 | Loss: 0.00002597
Iteration 100/1000 | Loss: 0.00002597
Iteration 101/1000 | Loss: 0.00002597
Iteration 102/1000 | Loss: 0.00002597
Iteration 103/1000 | Loss: 0.00002597
Iteration 104/1000 | Loss: 0.00002596
Iteration 105/1000 | Loss: 0.00002596
Iteration 106/1000 | Loss: 0.00002596
Iteration 107/1000 | Loss: 0.00002595
Iteration 108/1000 | Loss: 0.00002595
Iteration 109/1000 | Loss: 0.00002595
Iteration 110/1000 | Loss: 0.00002595
Iteration 111/1000 | Loss: 0.00002594
Iteration 112/1000 | Loss: 0.00002594
Iteration 113/1000 | Loss: 0.00002594
Iteration 114/1000 | Loss: 0.00002593
Iteration 115/1000 | Loss: 0.00002593
Iteration 116/1000 | Loss: 0.00002593
Iteration 117/1000 | Loss: 0.00002593
Iteration 118/1000 | Loss: 0.00002593
Iteration 119/1000 | Loss: 0.00002593
Iteration 120/1000 | Loss: 0.00002593
Iteration 121/1000 | Loss: 0.00002593
Iteration 122/1000 | Loss: 0.00002592
Iteration 123/1000 | Loss: 0.00002592
Iteration 124/1000 | Loss: 0.00002592
Iteration 125/1000 | Loss: 0.00002592
Iteration 126/1000 | Loss: 0.00002592
Iteration 127/1000 | Loss: 0.00002592
Iteration 128/1000 | Loss: 0.00002592
Iteration 129/1000 | Loss: 0.00002592
Iteration 130/1000 | Loss: 0.00002592
Iteration 131/1000 | Loss: 0.00002592
Iteration 132/1000 | Loss: 0.00002592
Iteration 133/1000 | Loss: 0.00002592
Iteration 134/1000 | Loss: 0.00002592
Iteration 135/1000 | Loss: 0.00002592
Iteration 136/1000 | Loss: 0.00002591
Iteration 137/1000 | Loss: 0.00002591
Iteration 138/1000 | Loss: 0.00002591
Iteration 139/1000 | Loss: 0.00002591
Iteration 140/1000 | Loss: 0.00002591
Iteration 141/1000 | Loss: 0.00002591
Iteration 142/1000 | Loss: 0.00002591
Iteration 143/1000 | Loss: 0.00002591
Iteration 144/1000 | Loss: 0.00002591
Iteration 145/1000 | Loss: 0.00002591
Iteration 146/1000 | Loss: 0.00002591
Iteration 147/1000 | Loss: 0.00002591
Iteration 148/1000 | Loss: 0.00002591
Iteration 149/1000 | Loss: 0.00002590
Iteration 150/1000 | Loss: 0.00002590
Iteration 151/1000 | Loss: 0.00002590
Iteration 152/1000 | Loss: 0.00002590
Iteration 153/1000 | Loss: 0.00002590
Iteration 154/1000 | Loss: 0.00002590
Iteration 155/1000 | Loss: 0.00002590
Iteration 156/1000 | Loss: 0.00002589
Iteration 157/1000 | Loss: 0.00002589
Iteration 158/1000 | Loss: 0.00002589
Iteration 159/1000 | Loss: 0.00002589
Iteration 160/1000 | Loss: 0.00002589
Iteration 161/1000 | Loss: 0.00002589
Iteration 162/1000 | Loss: 0.00002588
Iteration 163/1000 | Loss: 0.00002588
Iteration 164/1000 | Loss: 0.00002588
Iteration 165/1000 | Loss: 0.00002588
Iteration 166/1000 | Loss: 0.00002587
Iteration 167/1000 | Loss: 0.00002587
Iteration 168/1000 | Loss: 0.00002587
Iteration 169/1000 | Loss: 0.00002587
Iteration 170/1000 | Loss: 0.00002587
Iteration 171/1000 | Loss: 0.00002587
Iteration 172/1000 | Loss: 0.00002586
Iteration 173/1000 | Loss: 0.00002586
Iteration 174/1000 | Loss: 0.00002586
Iteration 175/1000 | Loss: 0.00002586
Iteration 176/1000 | Loss: 0.00002586
Iteration 177/1000 | Loss: 0.00002586
Iteration 178/1000 | Loss: 0.00002586
Iteration 179/1000 | Loss: 0.00002586
Iteration 180/1000 | Loss: 0.00002586
Iteration 181/1000 | Loss: 0.00002585
Iteration 182/1000 | Loss: 0.00002585
Iteration 183/1000 | Loss: 0.00002585
Iteration 184/1000 | Loss: 0.00002585
Iteration 185/1000 | Loss: 0.00002585
Iteration 186/1000 | Loss: 0.00002585
Iteration 187/1000 | Loss: 0.00002585
Iteration 188/1000 | Loss: 0.00002585
Iteration 189/1000 | Loss: 0.00002585
Iteration 190/1000 | Loss: 0.00002585
Iteration 191/1000 | Loss: 0.00002585
Iteration 192/1000 | Loss: 0.00002585
Iteration 193/1000 | Loss: 0.00002585
Iteration 194/1000 | Loss: 0.00002585
Iteration 195/1000 | Loss: 0.00002585
Iteration 196/1000 | Loss: 0.00002585
Iteration 197/1000 | Loss: 0.00002585
Iteration 198/1000 | Loss: 0.00002585
Iteration 199/1000 | Loss: 0.00002585
Iteration 200/1000 | Loss: 0.00002584
Iteration 201/1000 | Loss: 0.00002584
Iteration 202/1000 | Loss: 0.00002584
Iteration 203/1000 | Loss: 0.00002584
Iteration 204/1000 | Loss: 0.00002584
Iteration 205/1000 | Loss: 0.00002584
Iteration 206/1000 | Loss: 0.00002584
Iteration 207/1000 | Loss: 0.00002584
Iteration 208/1000 | Loss: 0.00002583
Iteration 209/1000 | Loss: 0.00002583
Iteration 210/1000 | Loss: 0.00002583
Iteration 211/1000 | Loss: 0.00002583
Iteration 212/1000 | Loss: 0.00002583
Iteration 213/1000 | Loss: 0.00002583
Iteration 214/1000 | Loss: 0.00002583
Iteration 215/1000 | Loss: 0.00002583
Iteration 216/1000 | Loss: 0.00002583
Iteration 217/1000 | Loss: 0.00002583
Iteration 218/1000 | Loss: 0.00002583
Iteration 219/1000 | Loss: 0.00002583
Iteration 220/1000 | Loss: 0.00002583
Iteration 221/1000 | Loss: 0.00002583
Iteration 222/1000 | Loss: 0.00002583
Iteration 223/1000 | Loss: 0.00002583
Iteration 224/1000 | Loss: 0.00002583
Iteration 225/1000 | Loss: 0.00002583
Iteration 226/1000 | Loss: 0.00002583
Iteration 227/1000 | Loss: 0.00002583
Iteration 228/1000 | Loss: 0.00002583
Iteration 229/1000 | Loss: 0.00002583
Iteration 230/1000 | Loss: 0.00002583
Iteration 231/1000 | Loss: 0.00002583
Iteration 232/1000 | Loss: 0.00002583
Iteration 233/1000 | Loss: 0.00002583
Iteration 234/1000 | Loss: 0.00002583
Iteration 235/1000 | Loss: 0.00002583
Iteration 236/1000 | Loss: 0.00002583
Iteration 237/1000 | Loss: 0.00002583
Iteration 238/1000 | Loss: 0.00002583
Iteration 239/1000 | Loss: 0.00002583
Iteration 240/1000 | Loss: 0.00002583
Iteration 241/1000 | Loss: 0.00002583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.583079185569659e-05, 2.583079185569659e-05, 2.583079185569659e-05, 2.583079185569659e-05, 2.583079185569659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.583079185569659e-05

Optimization complete. Final v2v error: 4.06151008605957 mm

Highest mean error: 5.923210620880127 mm for frame 73

Lowest mean error: 2.7271647453308105 mm for frame 7

Saving results

Total time: 135.55353832244873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033355
Iteration 2/25 | Loss: 0.00220182
Iteration 3/25 | Loss: 0.00179981
Iteration 4/25 | Loss: 0.00151558
Iteration 5/25 | Loss: 0.00171834
Iteration 6/25 | Loss: 0.00139841
Iteration 7/25 | Loss: 0.00140451
Iteration 8/25 | Loss: 0.00134698
Iteration 9/25 | Loss: 0.00136493
Iteration 10/25 | Loss: 0.00134009
Iteration 11/25 | Loss: 0.00134310
Iteration 12/25 | Loss: 0.00132244
Iteration 13/25 | Loss: 0.00132308
Iteration 14/25 | Loss: 0.00130898
Iteration 15/25 | Loss: 0.00131762
Iteration 16/25 | Loss: 0.00131110
Iteration 17/25 | Loss: 0.00130921
Iteration 18/25 | Loss: 0.00130833
Iteration 19/25 | Loss: 0.00130832
Iteration 20/25 | Loss: 0.00130832
Iteration 21/25 | Loss: 0.00130832
Iteration 22/25 | Loss: 0.00130832
Iteration 23/25 | Loss: 0.00130832
Iteration 24/25 | Loss: 0.00130832
Iteration 25/25 | Loss: 0.00130832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33150935
Iteration 2/25 | Loss: 0.00352069
Iteration 3/25 | Loss: 0.00117900
Iteration 4/25 | Loss: 0.00117900
Iteration 5/25 | Loss: 0.00117900
Iteration 6/25 | Loss: 0.00117900
Iteration 7/25 | Loss: 0.00117900
Iteration 8/25 | Loss: 0.00117900
Iteration 9/25 | Loss: 0.00117900
Iteration 10/25 | Loss: 0.00117900
Iteration 11/25 | Loss: 0.00117900
Iteration 12/25 | Loss: 0.00117900
Iteration 13/25 | Loss: 0.00117900
Iteration 14/25 | Loss: 0.00117900
Iteration 15/25 | Loss: 0.00117900
Iteration 16/25 | Loss: 0.00117900
Iteration 17/25 | Loss: 0.00117900
Iteration 18/25 | Loss: 0.00117900
Iteration 19/25 | Loss: 0.00117900
Iteration 20/25 | Loss: 0.00117900
Iteration 21/25 | Loss: 0.00117900
Iteration 22/25 | Loss: 0.00117900
Iteration 23/25 | Loss: 0.00117900
Iteration 24/25 | Loss: 0.00117900
Iteration 25/25 | Loss: 0.00117900

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117900
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002235
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002105
Iteration 7/1000 | Loss: 0.00002067
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00002009
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001960
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001927
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001918
Iteration 17/1000 | Loss: 0.00001918
Iteration 18/1000 | Loss: 0.00001917
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00001906
Iteration 21/1000 | Loss: 0.00001906
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001906
Iteration 26/1000 | Loss: 0.00001906
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001905
Iteration 30/1000 | Loss: 0.00001905
Iteration 31/1000 | Loss: 0.00001901
Iteration 32/1000 | Loss: 0.00001900
Iteration 33/1000 | Loss: 0.00001900
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001898
Iteration 38/1000 | Loss: 0.00001898
Iteration 39/1000 | Loss: 0.00001895
Iteration 40/1000 | Loss: 0.00001895
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001891
Iteration 46/1000 | Loss: 0.00001891
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001890
Iteration 50/1000 | Loss: 0.00001890
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001888
Iteration 57/1000 | Loss: 0.00001888
Iteration 58/1000 | Loss: 0.00001888
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001888
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001884
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001884
Iteration 75/1000 | Loss: 0.00001884
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001883
Iteration 79/1000 | Loss: 0.00001883
Iteration 80/1000 | Loss: 0.00001881
Iteration 81/1000 | Loss: 0.00001881
Iteration 82/1000 | Loss: 0.00001881
Iteration 83/1000 | Loss: 0.00001881
Iteration 84/1000 | Loss: 0.00001881
Iteration 85/1000 | Loss: 0.00001881
Iteration 86/1000 | Loss: 0.00001881
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001880
Iteration 91/1000 | Loss: 0.00001880
Iteration 92/1000 | Loss: 0.00001880
Iteration 93/1000 | Loss: 0.00001880
Iteration 94/1000 | Loss: 0.00001880
Iteration 95/1000 | Loss: 0.00001879
Iteration 96/1000 | Loss: 0.00001879
Iteration 97/1000 | Loss: 0.00001878
Iteration 98/1000 | Loss: 0.00001878
Iteration 99/1000 | Loss: 0.00001877
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001877
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001877
Iteration 104/1000 | Loss: 0.00001877
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001876
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001876
Iteration 116/1000 | Loss: 0.00001876
Iteration 117/1000 | Loss: 0.00001876
Iteration 118/1000 | Loss: 0.00001876
Iteration 119/1000 | Loss: 0.00001876
Iteration 120/1000 | Loss: 0.00001876
Iteration 121/1000 | Loss: 0.00001876
Iteration 122/1000 | Loss: 0.00001876
Iteration 123/1000 | Loss: 0.00001876
Iteration 124/1000 | Loss: 0.00001876
Iteration 125/1000 | Loss: 0.00001876
Iteration 126/1000 | Loss: 0.00001876
Iteration 127/1000 | Loss: 0.00001876
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001876
Iteration 130/1000 | Loss: 0.00001876
Iteration 131/1000 | Loss: 0.00001876
Iteration 132/1000 | Loss: 0.00001876
Iteration 133/1000 | Loss: 0.00001876
Iteration 134/1000 | Loss: 0.00001876
Iteration 135/1000 | Loss: 0.00001876
Iteration 136/1000 | Loss: 0.00001876
Iteration 137/1000 | Loss: 0.00001876
Iteration 138/1000 | Loss: 0.00001876
Iteration 139/1000 | Loss: 0.00001876
Iteration 140/1000 | Loss: 0.00001876
Iteration 141/1000 | Loss: 0.00001876
Iteration 142/1000 | Loss: 0.00001876
Iteration 143/1000 | Loss: 0.00001876
Iteration 144/1000 | Loss: 0.00001876
Iteration 145/1000 | Loss: 0.00001876
Iteration 146/1000 | Loss: 0.00001876
Iteration 147/1000 | Loss: 0.00001876
Iteration 148/1000 | Loss: 0.00001876
Iteration 149/1000 | Loss: 0.00001876
Iteration 150/1000 | Loss: 0.00001876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.8757691577775404e-05, 1.8757691577775404e-05, 1.8757691577775404e-05, 1.8757691577775404e-05, 1.8757691577775404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8757691577775404e-05

Optimization complete. Final v2v error: 3.703139543533325 mm

Highest mean error: 3.9301984310150146 mm for frame 238

Lowest mean error: 3.5855512619018555 mm for frame 172

Saving results

Total time: 71.25846552848816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008285
Iteration 2/25 | Loss: 0.00182967
Iteration 3/25 | Loss: 0.00150813
Iteration 4/25 | Loss: 0.00146850
Iteration 5/25 | Loss: 0.00151357
Iteration 6/25 | Loss: 0.00136171
Iteration 7/25 | Loss: 0.00133549
Iteration 8/25 | Loss: 0.00132714
Iteration 9/25 | Loss: 0.00130717
Iteration 10/25 | Loss: 0.00130491
Iteration 11/25 | Loss: 0.00130004
Iteration 12/25 | Loss: 0.00130818
Iteration 13/25 | Loss: 0.00130624
Iteration 14/25 | Loss: 0.00129804
Iteration 15/25 | Loss: 0.00128937
Iteration 16/25 | Loss: 0.00128707
Iteration 17/25 | Loss: 0.00128718
Iteration 18/25 | Loss: 0.00128476
Iteration 19/25 | Loss: 0.00128300
Iteration 20/25 | Loss: 0.00128308
Iteration 21/25 | Loss: 0.00128209
Iteration 22/25 | Loss: 0.00128340
Iteration 23/25 | Loss: 0.00127911
Iteration 24/25 | Loss: 0.00128291
Iteration 25/25 | Loss: 0.00128535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43671167
Iteration 2/25 | Loss: 0.00130625
Iteration 3/25 | Loss: 0.00130625
Iteration 4/25 | Loss: 0.00130625
Iteration 5/25 | Loss: 0.00130625
Iteration 6/25 | Loss: 0.00130625
Iteration 7/25 | Loss: 0.00130625
Iteration 8/25 | Loss: 0.00130625
Iteration 9/25 | Loss: 0.00130625
Iteration 10/25 | Loss: 0.00130624
Iteration 11/25 | Loss: 0.00130624
Iteration 12/25 | Loss: 0.00130624
Iteration 13/25 | Loss: 0.00130624
Iteration 14/25 | Loss: 0.00130624
Iteration 15/25 | Loss: 0.00130624
Iteration 16/25 | Loss: 0.00130624
Iteration 17/25 | Loss: 0.00130624
Iteration 18/25 | Loss: 0.00130624
Iteration 19/25 | Loss: 0.00130624
Iteration 20/25 | Loss: 0.00130624
Iteration 21/25 | Loss: 0.00130624
Iteration 22/25 | Loss: 0.00130624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013062439393252134, 0.0013062439393252134, 0.0013062439393252134, 0.0013062439393252134, 0.0013062439393252134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013062439393252134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130624
Iteration 2/1000 | Loss: 0.00039554
Iteration 3/1000 | Loss: 0.00016398
Iteration 4/1000 | Loss: 0.00004271
Iteration 5/1000 | Loss: 0.00004887
Iteration 6/1000 | Loss: 0.00021042
Iteration 7/1000 | Loss: 0.00023760
Iteration 8/1000 | Loss: 0.00016147
Iteration 9/1000 | Loss: 0.00055673
Iteration 10/1000 | Loss: 0.00056739
Iteration 11/1000 | Loss: 0.00041959
Iteration 12/1000 | Loss: 0.00014305
Iteration 13/1000 | Loss: 0.00029409
Iteration 14/1000 | Loss: 0.00061016
Iteration 15/1000 | Loss: 0.00063294
Iteration 16/1000 | Loss: 0.00059681
Iteration 17/1000 | Loss: 0.00071478
Iteration 18/1000 | Loss: 0.00040706
Iteration 19/1000 | Loss: 0.00074019
Iteration 20/1000 | Loss: 0.00004039
Iteration 21/1000 | Loss: 0.00036850
Iteration 22/1000 | Loss: 0.00003486
Iteration 23/1000 | Loss: 0.00003689
Iteration 24/1000 | Loss: 0.00007350
Iteration 25/1000 | Loss: 0.00002930
Iteration 26/1000 | Loss: 0.00016463
Iteration 27/1000 | Loss: 0.00025888
Iteration 28/1000 | Loss: 0.00017625
Iteration 29/1000 | Loss: 0.00024683
Iteration 30/1000 | Loss: 0.00099914
Iteration 31/1000 | Loss: 0.00005721
Iteration 32/1000 | Loss: 0.00036767
Iteration 33/1000 | Loss: 0.00013143
Iteration 34/1000 | Loss: 0.00028211
Iteration 35/1000 | Loss: 0.00007145
Iteration 36/1000 | Loss: 0.00023362
Iteration 37/1000 | Loss: 0.00014461
Iteration 38/1000 | Loss: 0.00002588
Iteration 39/1000 | Loss: 0.00002281
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001682
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001588
Iteration 45/1000 | Loss: 0.00012385
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00001812
Iteration 48/1000 | Loss: 0.00001666
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001554
Iteration 51/1000 | Loss: 0.00001530
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00027681
Iteration 54/1000 | Loss: 0.00007943
Iteration 55/1000 | Loss: 0.00008386
Iteration 56/1000 | Loss: 0.00007984
Iteration 57/1000 | Loss: 0.00001593
Iteration 58/1000 | Loss: 0.00024508
Iteration 59/1000 | Loss: 0.00022619
Iteration 60/1000 | Loss: 0.00012958
Iteration 61/1000 | Loss: 0.00018188
Iteration 62/1000 | Loss: 0.00014359
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00023600
Iteration 67/1000 | Loss: 0.00013846
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00025231
Iteration 71/1000 | Loss: 0.00015785
Iteration 72/1000 | Loss: 0.00002001
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00020807
Iteration 75/1000 | Loss: 0.00011784
Iteration 76/1000 | Loss: 0.00017884
Iteration 77/1000 | Loss: 0.00012532
Iteration 78/1000 | Loss: 0.00017677
Iteration 79/1000 | Loss: 0.00002468
Iteration 80/1000 | Loss: 0.00025829
Iteration 81/1000 | Loss: 0.00010511
Iteration 82/1000 | Loss: 0.00011947
Iteration 83/1000 | Loss: 0.00005171
Iteration 84/1000 | Loss: 0.00014800
Iteration 85/1000 | Loss: 0.00010919
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001504
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001496
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001495
Iteration 94/1000 | Loss: 0.00001495
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001494
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001493
Iteration 99/1000 | Loss: 0.00001492
Iteration 100/1000 | Loss: 0.00001492
Iteration 101/1000 | Loss: 0.00001492
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001491
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001490
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001486
Iteration 108/1000 | Loss: 0.00001486
Iteration 109/1000 | Loss: 0.00001486
Iteration 110/1000 | Loss: 0.00001485
Iteration 111/1000 | Loss: 0.00001485
Iteration 112/1000 | Loss: 0.00001485
Iteration 113/1000 | Loss: 0.00001484
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001482
Iteration 119/1000 | Loss: 0.00001482
Iteration 120/1000 | Loss: 0.00001482
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00001482
Iteration 123/1000 | Loss: 0.00022107
Iteration 124/1000 | Loss: 0.00022932
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001483
Iteration 127/1000 | Loss: 0.00001459
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001446
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001445
Iteration 134/1000 | Loss: 0.00001445
Iteration 135/1000 | Loss: 0.00001444
Iteration 136/1000 | Loss: 0.00001443
Iteration 137/1000 | Loss: 0.00001443
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001442
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001441
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001439
Iteration 148/1000 | Loss: 0.00001439
Iteration 149/1000 | Loss: 0.00001439
Iteration 150/1000 | Loss: 0.00001438
Iteration 151/1000 | Loss: 0.00001438
Iteration 152/1000 | Loss: 0.00001438
Iteration 153/1000 | Loss: 0.00001437
Iteration 154/1000 | Loss: 0.00001437
Iteration 155/1000 | Loss: 0.00001437
Iteration 156/1000 | Loss: 0.00001437
Iteration 157/1000 | Loss: 0.00001436
Iteration 158/1000 | Loss: 0.00001436
Iteration 159/1000 | Loss: 0.00001436
Iteration 160/1000 | Loss: 0.00001435
Iteration 161/1000 | Loss: 0.00001435
Iteration 162/1000 | Loss: 0.00001435
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001432
Iteration 165/1000 | Loss: 0.00001432
Iteration 166/1000 | Loss: 0.00001432
Iteration 167/1000 | Loss: 0.00001428
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001427
Iteration 171/1000 | Loss: 0.00001427
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001426
Iteration 182/1000 | Loss: 0.00001425
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001425
Iteration 188/1000 | Loss: 0.00001424
Iteration 189/1000 | Loss: 0.00001424
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001424
Iteration 192/1000 | Loss: 0.00001424
Iteration 193/1000 | Loss: 0.00001424
Iteration 194/1000 | Loss: 0.00001424
Iteration 195/1000 | Loss: 0.00001424
Iteration 196/1000 | Loss: 0.00001424
Iteration 197/1000 | Loss: 0.00001424
Iteration 198/1000 | Loss: 0.00001424
Iteration 199/1000 | Loss: 0.00001424
Iteration 200/1000 | Loss: 0.00001424
Iteration 201/1000 | Loss: 0.00001424
Iteration 202/1000 | Loss: 0.00001424
Iteration 203/1000 | Loss: 0.00001424
Iteration 204/1000 | Loss: 0.00001424
Iteration 205/1000 | Loss: 0.00001424
Iteration 206/1000 | Loss: 0.00001424
Iteration 207/1000 | Loss: 0.00001424
Iteration 208/1000 | Loss: 0.00001424
Iteration 209/1000 | Loss: 0.00001424
Iteration 210/1000 | Loss: 0.00001424
Iteration 211/1000 | Loss: 0.00001424
Iteration 212/1000 | Loss: 0.00001424
Iteration 213/1000 | Loss: 0.00001424
Iteration 214/1000 | Loss: 0.00001424
Iteration 215/1000 | Loss: 0.00001424
Iteration 216/1000 | Loss: 0.00001424
Iteration 217/1000 | Loss: 0.00001424
Iteration 218/1000 | Loss: 0.00001424
Iteration 219/1000 | Loss: 0.00001424
Iteration 220/1000 | Loss: 0.00001424
Iteration 221/1000 | Loss: 0.00001424
Iteration 222/1000 | Loss: 0.00001424
Iteration 223/1000 | Loss: 0.00001424
Iteration 224/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.4240633390727453e-05, 1.4240633390727453e-05, 1.4240633390727453e-05, 1.4240633390727453e-05, 1.4240633390727453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4240633390727453e-05

Optimization complete. Final v2v error: 3.1076672077178955 mm

Highest mean error: 6.282230377197266 mm for frame 107

Lowest mean error: 2.8070566654205322 mm for frame 23

Saving results

Total time: 183.88138842582703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779313
Iteration 2/25 | Loss: 0.00153302
Iteration 3/25 | Loss: 0.00131502
Iteration 4/25 | Loss: 0.00130523
Iteration 5/25 | Loss: 0.00130354
Iteration 6/25 | Loss: 0.00130354
Iteration 7/25 | Loss: 0.00130354
Iteration 8/25 | Loss: 0.00130354
Iteration 9/25 | Loss: 0.00130354
Iteration 10/25 | Loss: 0.00130354
Iteration 11/25 | Loss: 0.00130354
Iteration 12/25 | Loss: 0.00130354
Iteration 13/25 | Loss: 0.00130354
Iteration 14/25 | Loss: 0.00130354
Iteration 15/25 | Loss: 0.00130354
Iteration 16/25 | Loss: 0.00130354
Iteration 17/25 | Loss: 0.00130354
Iteration 18/25 | Loss: 0.00130354
Iteration 19/25 | Loss: 0.00130354
Iteration 20/25 | Loss: 0.00130354
Iteration 21/25 | Loss: 0.00130354
Iteration 22/25 | Loss: 0.00130354
Iteration 23/25 | Loss: 0.00130354
Iteration 24/25 | Loss: 0.00130354
Iteration 25/25 | Loss: 0.00130354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37335408
Iteration 2/25 | Loss: 0.00100841
Iteration 3/25 | Loss: 0.00100841
Iteration 4/25 | Loss: 0.00100841
Iteration 5/25 | Loss: 0.00100841
Iteration 6/25 | Loss: 0.00100841
Iteration 7/25 | Loss: 0.00100841
Iteration 8/25 | Loss: 0.00100841
Iteration 9/25 | Loss: 0.00100841
Iteration 10/25 | Loss: 0.00100841
Iteration 11/25 | Loss: 0.00100841
Iteration 12/25 | Loss: 0.00100841
Iteration 13/25 | Loss: 0.00100841
Iteration 14/25 | Loss: 0.00100841
Iteration 15/25 | Loss: 0.00100841
Iteration 16/25 | Loss: 0.00100841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010084104724228382, 0.0010084104724228382, 0.0010084104724228382, 0.0010084104724228382, 0.0010084104724228382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010084104724228382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100841
Iteration 2/1000 | Loss: 0.00003270
Iteration 3/1000 | Loss: 0.00002322
Iteration 4/1000 | Loss: 0.00002094
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001897
Iteration 7/1000 | Loss: 0.00001840
Iteration 8/1000 | Loss: 0.00001794
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001709
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001685
Iteration 13/1000 | Loss: 0.00001680
Iteration 14/1000 | Loss: 0.00001672
Iteration 15/1000 | Loss: 0.00001670
Iteration 16/1000 | Loss: 0.00001669
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001660
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001659
Iteration 22/1000 | Loss: 0.00001658
Iteration 23/1000 | Loss: 0.00001657
Iteration 24/1000 | Loss: 0.00001656
Iteration 25/1000 | Loss: 0.00001655
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001653
Iteration 28/1000 | Loss: 0.00001653
Iteration 29/1000 | Loss: 0.00001653
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001652
Iteration 32/1000 | Loss: 0.00001651
Iteration 33/1000 | Loss: 0.00001650
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001649
Iteration 37/1000 | Loss: 0.00001648
Iteration 38/1000 | Loss: 0.00001647
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001643
Iteration 47/1000 | Loss: 0.00001643
Iteration 48/1000 | Loss: 0.00001642
Iteration 49/1000 | Loss: 0.00001642
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001638
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001635
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001633
Iteration 71/1000 | Loss: 0.00001633
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001629
Iteration 91/1000 | Loss: 0.00001629
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001628
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001626
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001624
Iteration 117/1000 | Loss: 0.00001624
Iteration 118/1000 | Loss: 0.00001624
Iteration 119/1000 | Loss: 0.00001624
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.623990283405874e-05, 1.623990283405874e-05, 1.623990283405874e-05, 1.623990283405874e-05, 1.623990283405874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.623990283405874e-05

Optimization complete. Final v2v error: 3.418668508529663 mm

Highest mean error: 3.7374491691589355 mm for frame 168

Lowest mean error: 3.115385055541992 mm for frame 219

Saving results

Total time: 39.05621957778931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843558
Iteration 2/25 | Loss: 0.00136611
Iteration 3/25 | Loss: 0.00127509
Iteration 4/25 | Loss: 0.00125802
Iteration 5/25 | Loss: 0.00125171
Iteration 6/25 | Loss: 0.00125095
Iteration 7/25 | Loss: 0.00125095
Iteration 8/25 | Loss: 0.00125095
Iteration 9/25 | Loss: 0.00125095
Iteration 10/25 | Loss: 0.00125095
Iteration 11/25 | Loss: 0.00125095
Iteration 12/25 | Loss: 0.00125095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001250950270332396, 0.001250950270332396, 0.001250950270332396, 0.001250950270332396, 0.001250950270332396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001250950270332396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33422410
Iteration 2/25 | Loss: 0.00150534
Iteration 3/25 | Loss: 0.00150534
Iteration 4/25 | Loss: 0.00150533
Iteration 5/25 | Loss: 0.00150533
Iteration 6/25 | Loss: 0.00150533
Iteration 7/25 | Loss: 0.00150533
Iteration 8/25 | Loss: 0.00150533
Iteration 9/25 | Loss: 0.00150533
Iteration 10/25 | Loss: 0.00150533
Iteration 11/25 | Loss: 0.00150533
Iteration 12/25 | Loss: 0.00150533
Iteration 13/25 | Loss: 0.00150533
Iteration 14/25 | Loss: 0.00150533
Iteration 15/25 | Loss: 0.00150533
Iteration 16/25 | Loss: 0.00150533
Iteration 17/25 | Loss: 0.00150533
Iteration 18/25 | Loss: 0.00150533
Iteration 19/25 | Loss: 0.00150533
Iteration 20/25 | Loss: 0.00150533
Iteration 21/25 | Loss: 0.00150533
Iteration 22/25 | Loss: 0.00150533
Iteration 23/25 | Loss: 0.00150533
Iteration 24/25 | Loss: 0.00150533
Iteration 25/25 | Loss: 0.00150533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150533
Iteration 2/1000 | Loss: 0.00003287
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00002152
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001866
Iteration 8/1000 | Loss: 0.00001815
Iteration 9/1000 | Loss: 0.00001786
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001704
Iteration 13/1000 | Loss: 0.00001693
Iteration 14/1000 | Loss: 0.00001684
Iteration 15/1000 | Loss: 0.00001679
Iteration 16/1000 | Loss: 0.00001674
Iteration 17/1000 | Loss: 0.00001669
Iteration 18/1000 | Loss: 0.00001668
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001661
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001655
Iteration 27/1000 | Loss: 0.00001655
Iteration 28/1000 | Loss: 0.00001654
Iteration 29/1000 | Loss: 0.00001652
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001652
Iteration 32/1000 | Loss: 0.00001652
Iteration 33/1000 | Loss: 0.00001651
Iteration 34/1000 | Loss: 0.00001651
Iteration 35/1000 | Loss: 0.00001651
Iteration 36/1000 | Loss: 0.00001651
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001648
Iteration 48/1000 | Loss: 0.00001648
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001647
Iteration 53/1000 | Loss: 0.00001647
Iteration 54/1000 | Loss: 0.00001647
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001646
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001646
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001645
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001644
Iteration 63/1000 | Loss: 0.00001644
Iteration 64/1000 | Loss: 0.00001644
Iteration 65/1000 | Loss: 0.00001644
Iteration 66/1000 | Loss: 0.00001644
Iteration 67/1000 | Loss: 0.00001643
Iteration 68/1000 | Loss: 0.00001643
Iteration 69/1000 | Loss: 0.00001642
Iteration 70/1000 | Loss: 0.00001642
Iteration 71/1000 | Loss: 0.00001642
Iteration 72/1000 | Loss: 0.00001642
Iteration 73/1000 | Loss: 0.00001641
Iteration 74/1000 | Loss: 0.00001641
Iteration 75/1000 | Loss: 0.00001641
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001640
Iteration 78/1000 | Loss: 0.00001640
Iteration 79/1000 | Loss: 0.00001640
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001639
Iteration 82/1000 | Loss: 0.00001638
Iteration 83/1000 | Loss: 0.00001638
Iteration 84/1000 | Loss: 0.00001638
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001636
Iteration 90/1000 | Loss: 0.00001636
Iteration 91/1000 | Loss: 0.00001636
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001635
Iteration 96/1000 | Loss: 0.00001635
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001634
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001633
Iteration 108/1000 | Loss: 0.00001633
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001633
Iteration 111/1000 | Loss: 0.00001633
Iteration 112/1000 | Loss: 0.00001633
Iteration 113/1000 | Loss: 0.00001633
Iteration 114/1000 | Loss: 0.00001633
Iteration 115/1000 | Loss: 0.00001633
Iteration 116/1000 | Loss: 0.00001632
Iteration 117/1000 | Loss: 0.00001632
Iteration 118/1000 | Loss: 0.00001632
Iteration 119/1000 | Loss: 0.00001632
Iteration 120/1000 | Loss: 0.00001632
Iteration 121/1000 | Loss: 0.00001632
Iteration 122/1000 | Loss: 0.00001632
Iteration 123/1000 | Loss: 0.00001632
Iteration 124/1000 | Loss: 0.00001632
Iteration 125/1000 | Loss: 0.00001631
Iteration 126/1000 | Loss: 0.00001631
Iteration 127/1000 | Loss: 0.00001631
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001630
Iteration 130/1000 | Loss: 0.00001630
Iteration 131/1000 | Loss: 0.00001630
Iteration 132/1000 | Loss: 0.00001630
Iteration 133/1000 | Loss: 0.00001629
Iteration 134/1000 | Loss: 0.00001629
Iteration 135/1000 | Loss: 0.00001629
Iteration 136/1000 | Loss: 0.00001628
Iteration 137/1000 | Loss: 0.00001628
Iteration 138/1000 | Loss: 0.00001628
Iteration 139/1000 | Loss: 0.00001628
Iteration 140/1000 | Loss: 0.00001628
Iteration 141/1000 | Loss: 0.00001628
Iteration 142/1000 | Loss: 0.00001628
Iteration 143/1000 | Loss: 0.00001628
Iteration 144/1000 | Loss: 0.00001627
Iteration 145/1000 | Loss: 0.00001627
Iteration 146/1000 | Loss: 0.00001627
Iteration 147/1000 | Loss: 0.00001627
Iteration 148/1000 | Loss: 0.00001627
Iteration 149/1000 | Loss: 0.00001627
Iteration 150/1000 | Loss: 0.00001627
Iteration 151/1000 | Loss: 0.00001627
Iteration 152/1000 | Loss: 0.00001626
Iteration 153/1000 | Loss: 0.00001626
Iteration 154/1000 | Loss: 0.00001626
Iteration 155/1000 | Loss: 0.00001626
Iteration 156/1000 | Loss: 0.00001626
Iteration 157/1000 | Loss: 0.00001626
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001625
Iteration 160/1000 | Loss: 0.00001625
Iteration 161/1000 | Loss: 0.00001625
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00001625
Iteration 166/1000 | Loss: 0.00001625
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001624
Iteration 169/1000 | Loss: 0.00001624
Iteration 170/1000 | Loss: 0.00001624
Iteration 171/1000 | Loss: 0.00001623
Iteration 172/1000 | Loss: 0.00001623
Iteration 173/1000 | Loss: 0.00001623
Iteration 174/1000 | Loss: 0.00001623
Iteration 175/1000 | Loss: 0.00001623
Iteration 176/1000 | Loss: 0.00001623
Iteration 177/1000 | Loss: 0.00001623
Iteration 178/1000 | Loss: 0.00001623
Iteration 179/1000 | Loss: 0.00001623
Iteration 180/1000 | Loss: 0.00001623
Iteration 181/1000 | Loss: 0.00001623
Iteration 182/1000 | Loss: 0.00001623
Iteration 183/1000 | Loss: 0.00001623
Iteration 184/1000 | Loss: 0.00001623
Iteration 185/1000 | Loss: 0.00001622
Iteration 186/1000 | Loss: 0.00001622
Iteration 187/1000 | Loss: 0.00001622
Iteration 188/1000 | Loss: 0.00001622
Iteration 189/1000 | Loss: 0.00001622
Iteration 190/1000 | Loss: 0.00001622
Iteration 191/1000 | Loss: 0.00001622
Iteration 192/1000 | Loss: 0.00001622
Iteration 193/1000 | Loss: 0.00001622
Iteration 194/1000 | Loss: 0.00001622
Iteration 195/1000 | Loss: 0.00001621
Iteration 196/1000 | Loss: 0.00001621
Iteration 197/1000 | Loss: 0.00001621
Iteration 198/1000 | Loss: 0.00001621
Iteration 199/1000 | Loss: 0.00001621
Iteration 200/1000 | Loss: 0.00001621
Iteration 201/1000 | Loss: 0.00001621
Iteration 202/1000 | Loss: 0.00001621
Iteration 203/1000 | Loss: 0.00001621
Iteration 204/1000 | Loss: 0.00001621
Iteration 205/1000 | Loss: 0.00001621
Iteration 206/1000 | Loss: 0.00001621
Iteration 207/1000 | Loss: 0.00001621
Iteration 208/1000 | Loss: 0.00001621
Iteration 209/1000 | Loss: 0.00001621
Iteration 210/1000 | Loss: 0.00001621
Iteration 211/1000 | Loss: 0.00001621
Iteration 212/1000 | Loss: 0.00001621
Iteration 213/1000 | Loss: 0.00001621
Iteration 214/1000 | Loss: 0.00001620
Iteration 215/1000 | Loss: 0.00001620
Iteration 216/1000 | Loss: 0.00001620
Iteration 217/1000 | Loss: 0.00001620
Iteration 218/1000 | Loss: 0.00001620
Iteration 219/1000 | Loss: 0.00001620
Iteration 220/1000 | Loss: 0.00001620
Iteration 221/1000 | Loss: 0.00001620
Iteration 222/1000 | Loss: 0.00001620
Iteration 223/1000 | Loss: 0.00001620
Iteration 224/1000 | Loss: 0.00001620
Iteration 225/1000 | Loss: 0.00001620
Iteration 226/1000 | Loss: 0.00001620
Iteration 227/1000 | Loss: 0.00001620
Iteration 228/1000 | Loss: 0.00001620
Iteration 229/1000 | Loss: 0.00001620
Iteration 230/1000 | Loss: 0.00001620
Iteration 231/1000 | Loss: 0.00001620
Iteration 232/1000 | Loss: 0.00001620
Iteration 233/1000 | Loss: 0.00001620
Iteration 234/1000 | Loss: 0.00001620
Iteration 235/1000 | Loss: 0.00001620
Iteration 236/1000 | Loss: 0.00001620
Iteration 237/1000 | Loss: 0.00001620
Iteration 238/1000 | Loss: 0.00001620
Iteration 239/1000 | Loss: 0.00001620
Iteration 240/1000 | Loss: 0.00001620
Iteration 241/1000 | Loss: 0.00001620
Iteration 242/1000 | Loss: 0.00001620
Iteration 243/1000 | Loss: 0.00001620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.6196467186091468e-05, 1.6196467186091468e-05, 1.6196467186091468e-05, 1.6196467186091468e-05, 1.6196467186091468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6196467186091468e-05

Optimization complete. Final v2v error: 3.4261868000030518 mm

Highest mean error: 3.769090175628662 mm for frame 151

Lowest mean error: 3.1603903770446777 mm for frame 72

Saving results

Total time: 45.40033507347107
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973842
Iteration 2/25 | Loss: 0.00375059
Iteration 3/25 | Loss: 0.00273882
Iteration 4/25 | Loss: 0.00244492
Iteration 5/25 | Loss: 0.00217842
Iteration 6/25 | Loss: 0.00205784
Iteration 7/25 | Loss: 0.00197757
Iteration 8/25 | Loss: 0.00192963
Iteration 9/25 | Loss: 0.00195542
Iteration 10/25 | Loss: 0.00187753
Iteration 11/25 | Loss: 0.00183697
Iteration 12/25 | Loss: 0.00183493
Iteration 13/25 | Loss: 0.00178339
Iteration 14/25 | Loss: 0.00178146
Iteration 15/25 | Loss: 0.00177838
Iteration 16/25 | Loss: 0.00175396
Iteration 17/25 | Loss: 0.00176108
Iteration 18/25 | Loss: 0.00174858
Iteration 19/25 | Loss: 0.00174923
Iteration 20/25 | Loss: 0.00174143
Iteration 21/25 | Loss: 0.00172851
Iteration 22/25 | Loss: 0.00171518
Iteration 23/25 | Loss: 0.00171844
Iteration 24/25 | Loss: 0.00171371
Iteration 25/25 | Loss: 0.00171305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32151544
Iteration 2/25 | Loss: 0.00673938
Iteration 3/25 | Loss: 0.00435901
Iteration 4/25 | Loss: 0.00435899
Iteration 5/25 | Loss: 0.00435899
Iteration 6/25 | Loss: 0.00435899
Iteration 7/25 | Loss: 0.00435899
Iteration 8/25 | Loss: 0.00435899
Iteration 9/25 | Loss: 0.00435899
Iteration 10/25 | Loss: 0.00435899
Iteration 11/25 | Loss: 0.00435899
Iteration 12/25 | Loss: 0.00435899
Iteration 13/25 | Loss: 0.00435899
Iteration 14/25 | Loss: 0.00435899
Iteration 15/25 | Loss: 0.00435899
Iteration 16/25 | Loss: 0.00435899
Iteration 17/25 | Loss: 0.00435899
Iteration 18/25 | Loss: 0.00435899
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004358987789601088, 0.004358987789601088, 0.004358987789601088, 0.004358987789601088, 0.004358987789601088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004358987789601088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00435899
Iteration 2/1000 | Loss: 0.00370773
Iteration 3/1000 | Loss: 0.00086675
Iteration 4/1000 | Loss: 0.00156267
Iteration 5/1000 | Loss: 0.00079306
Iteration 6/1000 | Loss: 0.00041372
Iteration 7/1000 | Loss: 0.00044401
Iteration 8/1000 | Loss: 0.00034640
Iteration 9/1000 | Loss: 0.00134600
Iteration 10/1000 | Loss: 0.00185368
Iteration 11/1000 | Loss: 0.00283069
Iteration 12/1000 | Loss: 0.00038532
Iteration 13/1000 | Loss: 0.00079051
Iteration 14/1000 | Loss: 0.00047697
Iteration 15/1000 | Loss: 0.00064473
Iteration 16/1000 | Loss: 0.00104248
Iteration 17/1000 | Loss: 0.00034410
Iteration 18/1000 | Loss: 0.00068897
Iteration 19/1000 | Loss: 0.00065772
Iteration 20/1000 | Loss: 0.00290926
Iteration 21/1000 | Loss: 0.00107988
Iteration 22/1000 | Loss: 0.00161890
Iteration 23/1000 | Loss: 0.00120404
Iteration 24/1000 | Loss: 0.00111325
Iteration 25/1000 | Loss: 0.00190107
Iteration 26/1000 | Loss: 0.00028372
Iteration 27/1000 | Loss: 0.00018505
Iteration 28/1000 | Loss: 0.00105041
Iteration 29/1000 | Loss: 0.00372237
Iteration 30/1000 | Loss: 0.00026620
Iteration 31/1000 | Loss: 0.00040962
Iteration 32/1000 | Loss: 0.00066607
Iteration 33/1000 | Loss: 0.00022136
Iteration 34/1000 | Loss: 0.00111680
Iteration 35/1000 | Loss: 0.00021303
Iteration 36/1000 | Loss: 0.00014792
Iteration 37/1000 | Loss: 0.00037600
Iteration 38/1000 | Loss: 0.00048617
Iteration 39/1000 | Loss: 0.00056706
Iteration 40/1000 | Loss: 0.00029831
Iteration 41/1000 | Loss: 0.00069309
Iteration 42/1000 | Loss: 0.00015574
Iteration 43/1000 | Loss: 0.00029264
Iteration 44/1000 | Loss: 0.00065897
Iteration 45/1000 | Loss: 0.00028696
Iteration 46/1000 | Loss: 0.00024345
Iteration 47/1000 | Loss: 0.00014942
Iteration 48/1000 | Loss: 0.00011677
Iteration 49/1000 | Loss: 0.00062822
Iteration 50/1000 | Loss: 0.00024879
Iteration 51/1000 | Loss: 0.00017918
Iteration 52/1000 | Loss: 0.00071187
Iteration 53/1000 | Loss: 0.00013347
Iteration 54/1000 | Loss: 0.00012528
Iteration 55/1000 | Loss: 0.00011548
Iteration 56/1000 | Loss: 0.00017824
Iteration 57/1000 | Loss: 0.00010609
Iteration 58/1000 | Loss: 0.00012206
Iteration 59/1000 | Loss: 0.00016790
Iteration 60/1000 | Loss: 0.00013044
Iteration 61/1000 | Loss: 0.00010641
Iteration 62/1000 | Loss: 0.00011111
Iteration 63/1000 | Loss: 0.00009613
Iteration 64/1000 | Loss: 0.00009269
Iteration 65/1000 | Loss: 0.00014179
Iteration 66/1000 | Loss: 0.00010253
Iteration 67/1000 | Loss: 0.00035016
Iteration 68/1000 | Loss: 0.00037093
Iteration 69/1000 | Loss: 0.00075291
Iteration 70/1000 | Loss: 0.00036554
Iteration 71/1000 | Loss: 0.00018821
Iteration 72/1000 | Loss: 0.00012174
Iteration 73/1000 | Loss: 0.00021679
Iteration 74/1000 | Loss: 0.00013579
Iteration 75/1000 | Loss: 0.00010025
Iteration 76/1000 | Loss: 0.00062748
Iteration 77/1000 | Loss: 0.00516141
Iteration 78/1000 | Loss: 0.00354437
Iteration 79/1000 | Loss: 0.00177507
Iteration 80/1000 | Loss: 0.00120581
Iteration 81/1000 | Loss: 0.00053232
Iteration 82/1000 | Loss: 0.00021438
Iteration 83/1000 | Loss: 0.00041563
Iteration 84/1000 | Loss: 0.00032223
Iteration 85/1000 | Loss: 0.00054012
Iteration 86/1000 | Loss: 0.00099340
Iteration 87/1000 | Loss: 0.00022441
Iteration 88/1000 | Loss: 0.00021489
Iteration 89/1000 | Loss: 0.00074560
Iteration 90/1000 | Loss: 0.00031951
Iteration 91/1000 | Loss: 0.00028522
Iteration 92/1000 | Loss: 0.00044162
Iteration 93/1000 | Loss: 0.00011758
Iteration 94/1000 | Loss: 0.00005777
Iteration 95/1000 | Loss: 0.00005393
Iteration 96/1000 | Loss: 0.00081082
Iteration 97/1000 | Loss: 0.00008275
Iteration 98/1000 | Loss: 0.00013563
Iteration 99/1000 | Loss: 0.00029053
Iteration 100/1000 | Loss: 0.00060022
Iteration 101/1000 | Loss: 0.00070158
Iteration 102/1000 | Loss: 0.00006693
Iteration 103/1000 | Loss: 0.00003603
Iteration 104/1000 | Loss: 0.00009491
Iteration 105/1000 | Loss: 0.00003344
Iteration 106/1000 | Loss: 0.00012571
Iteration 107/1000 | Loss: 0.00009607
Iteration 108/1000 | Loss: 0.00003189
Iteration 109/1000 | Loss: 0.00012788
Iteration 110/1000 | Loss: 0.00003027
Iteration 111/1000 | Loss: 0.00002964
Iteration 112/1000 | Loss: 0.00002895
Iteration 113/1000 | Loss: 0.00009212
Iteration 114/1000 | Loss: 0.00002852
Iteration 115/1000 | Loss: 0.00002822
Iteration 116/1000 | Loss: 0.00006287
Iteration 117/1000 | Loss: 0.00004758
Iteration 118/1000 | Loss: 0.00012822
Iteration 119/1000 | Loss: 0.00009750
Iteration 120/1000 | Loss: 0.00003591
Iteration 121/1000 | Loss: 0.00027820
Iteration 122/1000 | Loss: 0.00025012
Iteration 123/1000 | Loss: 0.00034412
Iteration 124/1000 | Loss: 0.00005129
Iteration 125/1000 | Loss: 0.00010711
Iteration 126/1000 | Loss: 0.00003327
Iteration 127/1000 | Loss: 0.00007745
Iteration 128/1000 | Loss: 0.00003198
Iteration 129/1000 | Loss: 0.00002879
Iteration 130/1000 | Loss: 0.00002733
Iteration 131/1000 | Loss: 0.00005521
Iteration 132/1000 | Loss: 0.00002581
Iteration 133/1000 | Loss: 0.00002522
Iteration 134/1000 | Loss: 0.00007428
Iteration 135/1000 | Loss: 0.00002485
Iteration 136/1000 | Loss: 0.00002461
Iteration 137/1000 | Loss: 0.00011791
Iteration 138/1000 | Loss: 0.00009523
Iteration 139/1000 | Loss: 0.00007602
Iteration 140/1000 | Loss: 0.00007121
Iteration 141/1000 | Loss: 0.00002845
Iteration 142/1000 | Loss: 0.00002634
Iteration 143/1000 | Loss: 0.00002540
Iteration 144/1000 | Loss: 0.00002484
Iteration 145/1000 | Loss: 0.00003059
Iteration 146/1000 | Loss: 0.00023571
Iteration 147/1000 | Loss: 0.00035097
Iteration 148/1000 | Loss: 0.00043816
Iteration 149/1000 | Loss: 0.00005566
Iteration 150/1000 | Loss: 0.00002701
Iteration 151/1000 | Loss: 0.00002561
Iteration 152/1000 | Loss: 0.00002482
Iteration 153/1000 | Loss: 0.00002427
Iteration 154/1000 | Loss: 0.00002380
Iteration 155/1000 | Loss: 0.00022983
Iteration 156/1000 | Loss: 0.00003213
Iteration 157/1000 | Loss: 0.00020321
Iteration 158/1000 | Loss: 0.00022273
Iteration 159/1000 | Loss: 0.00015920
Iteration 160/1000 | Loss: 0.00023114
Iteration 161/1000 | Loss: 0.00002518
Iteration 162/1000 | Loss: 0.00002371
Iteration 163/1000 | Loss: 0.00002304
Iteration 164/1000 | Loss: 0.00002270
Iteration 165/1000 | Loss: 0.00002256
Iteration 166/1000 | Loss: 0.00002246
Iteration 167/1000 | Loss: 0.00002246
Iteration 168/1000 | Loss: 0.00002245
Iteration 169/1000 | Loss: 0.00002241
Iteration 170/1000 | Loss: 0.00002239
Iteration 171/1000 | Loss: 0.00002239
Iteration 172/1000 | Loss: 0.00002238
Iteration 173/1000 | Loss: 0.00002238
Iteration 174/1000 | Loss: 0.00002238
Iteration 175/1000 | Loss: 0.00002237
Iteration 176/1000 | Loss: 0.00002237
Iteration 177/1000 | Loss: 0.00002234
Iteration 178/1000 | Loss: 0.00002234
Iteration 179/1000 | Loss: 0.00002234
Iteration 180/1000 | Loss: 0.00002233
Iteration 181/1000 | Loss: 0.00002233
Iteration 182/1000 | Loss: 0.00002233
Iteration 183/1000 | Loss: 0.00002233
Iteration 184/1000 | Loss: 0.00002230
Iteration 185/1000 | Loss: 0.00002229
Iteration 186/1000 | Loss: 0.00002229
Iteration 187/1000 | Loss: 0.00002227
Iteration 188/1000 | Loss: 0.00002227
Iteration 189/1000 | Loss: 0.00002224
Iteration 190/1000 | Loss: 0.00002224
Iteration 191/1000 | Loss: 0.00002223
Iteration 192/1000 | Loss: 0.00002223
Iteration 193/1000 | Loss: 0.00002222
Iteration 194/1000 | Loss: 0.00002220
Iteration 195/1000 | Loss: 0.00002220
Iteration 196/1000 | Loss: 0.00002219
Iteration 197/1000 | Loss: 0.00002218
Iteration 198/1000 | Loss: 0.00002218
Iteration 199/1000 | Loss: 0.00002216
Iteration 200/1000 | Loss: 0.00002216
Iteration 201/1000 | Loss: 0.00002215
Iteration 202/1000 | Loss: 0.00002215
Iteration 203/1000 | Loss: 0.00002215
Iteration 204/1000 | Loss: 0.00002214
Iteration 205/1000 | Loss: 0.00002214
Iteration 206/1000 | Loss: 0.00002214
Iteration 207/1000 | Loss: 0.00002213
Iteration 208/1000 | Loss: 0.00002213
Iteration 209/1000 | Loss: 0.00002213
Iteration 210/1000 | Loss: 0.00002213
Iteration 211/1000 | Loss: 0.00002212
Iteration 212/1000 | Loss: 0.00002212
Iteration 213/1000 | Loss: 0.00002212
Iteration 214/1000 | Loss: 0.00002212
Iteration 215/1000 | Loss: 0.00002212
Iteration 216/1000 | Loss: 0.00002212
Iteration 217/1000 | Loss: 0.00002211
Iteration 218/1000 | Loss: 0.00002211
Iteration 219/1000 | Loss: 0.00002210
Iteration 220/1000 | Loss: 0.00002210
Iteration 221/1000 | Loss: 0.00002210
Iteration 222/1000 | Loss: 0.00002210
Iteration 223/1000 | Loss: 0.00002210
Iteration 224/1000 | Loss: 0.00002210
Iteration 225/1000 | Loss: 0.00002209
Iteration 226/1000 | Loss: 0.00002209
Iteration 227/1000 | Loss: 0.00002209
Iteration 228/1000 | Loss: 0.00002209
Iteration 229/1000 | Loss: 0.00002209
Iteration 230/1000 | Loss: 0.00002209
Iteration 231/1000 | Loss: 0.00002209
Iteration 232/1000 | Loss: 0.00002209
Iteration 233/1000 | Loss: 0.00002209
Iteration 234/1000 | Loss: 0.00002208
Iteration 235/1000 | Loss: 0.00002208
Iteration 236/1000 | Loss: 0.00027313
Iteration 237/1000 | Loss: 0.00002890
Iteration 238/1000 | Loss: 0.00002462
Iteration 239/1000 | Loss: 0.00002278
Iteration 240/1000 | Loss: 0.00002157
Iteration 241/1000 | Loss: 0.00002094
Iteration 242/1000 | Loss: 0.00002046
Iteration 243/1000 | Loss: 0.00002020
Iteration 244/1000 | Loss: 0.00002011
Iteration 245/1000 | Loss: 0.00001999
Iteration 246/1000 | Loss: 0.00001996
Iteration 247/1000 | Loss: 0.00001992
Iteration 248/1000 | Loss: 0.00001992
Iteration 249/1000 | Loss: 0.00001984
Iteration 250/1000 | Loss: 0.00001980
Iteration 251/1000 | Loss: 0.00001980
Iteration 252/1000 | Loss: 0.00001979
Iteration 253/1000 | Loss: 0.00001979
Iteration 254/1000 | Loss: 0.00001979
Iteration 255/1000 | Loss: 0.00001979
Iteration 256/1000 | Loss: 0.00001977
Iteration 257/1000 | Loss: 0.00001976
Iteration 258/1000 | Loss: 0.00001976
Iteration 259/1000 | Loss: 0.00001975
Iteration 260/1000 | Loss: 0.00001974
Iteration 261/1000 | Loss: 0.00001974
Iteration 262/1000 | Loss: 0.00001973
Iteration 263/1000 | Loss: 0.00001973
Iteration 264/1000 | Loss: 0.00001972
Iteration 265/1000 | Loss: 0.00001972
Iteration 266/1000 | Loss: 0.00001972
Iteration 267/1000 | Loss: 0.00001971
Iteration 268/1000 | Loss: 0.00001971
Iteration 269/1000 | Loss: 0.00001971
Iteration 270/1000 | Loss: 0.00001971
Iteration 271/1000 | Loss: 0.00001970
Iteration 272/1000 | Loss: 0.00001970
Iteration 273/1000 | Loss: 0.00001970
Iteration 274/1000 | Loss: 0.00001969
Iteration 275/1000 | Loss: 0.00001969
Iteration 276/1000 | Loss: 0.00001969
Iteration 277/1000 | Loss: 0.00001969
Iteration 278/1000 | Loss: 0.00001968
Iteration 279/1000 | Loss: 0.00001968
Iteration 280/1000 | Loss: 0.00001968
Iteration 281/1000 | Loss: 0.00001968
Iteration 282/1000 | Loss: 0.00001968
Iteration 283/1000 | Loss: 0.00001968
Iteration 284/1000 | Loss: 0.00001968
Iteration 285/1000 | Loss: 0.00001968
Iteration 286/1000 | Loss: 0.00001968
Iteration 287/1000 | Loss: 0.00001968
Iteration 288/1000 | Loss: 0.00001968
Iteration 289/1000 | Loss: 0.00001968
Iteration 290/1000 | Loss: 0.00001968
Iteration 291/1000 | Loss: 0.00001968
Iteration 292/1000 | Loss: 0.00001968
Iteration 293/1000 | Loss: 0.00001968
Iteration 294/1000 | Loss: 0.00001968
Iteration 295/1000 | Loss: 0.00001968
Iteration 296/1000 | Loss: 0.00001968
Iteration 297/1000 | Loss: 0.00001967
Iteration 298/1000 | Loss: 0.00001967
Iteration 299/1000 | Loss: 0.00001967
Iteration 300/1000 | Loss: 0.00001967
Iteration 301/1000 | Loss: 0.00001967
Iteration 302/1000 | Loss: 0.00001967
Iteration 303/1000 | Loss: 0.00001967
Iteration 304/1000 | Loss: 0.00001967
Iteration 305/1000 | Loss: 0.00001967
Iteration 306/1000 | Loss: 0.00001967
Iteration 307/1000 | Loss: 0.00001967
Iteration 308/1000 | Loss: 0.00001967
Iteration 309/1000 | Loss: 0.00001967
Iteration 310/1000 | Loss: 0.00001967
Iteration 311/1000 | Loss: 0.00001967
Iteration 312/1000 | Loss: 0.00001967
Iteration 313/1000 | Loss: 0.00001967
Iteration 314/1000 | Loss: 0.00001966
Iteration 315/1000 | Loss: 0.00001966
Iteration 316/1000 | Loss: 0.00001966
Iteration 317/1000 | Loss: 0.00001966
Iteration 318/1000 | Loss: 0.00001966
Iteration 319/1000 | Loss: 0.00001966
Iteration 320/1000 | Loss: 0.00001966
Iteration 321/1000 | Loss: 0.00001966
Iteration 322/1000 | Loss: 0.00001966
Iteration 323/1000 | Loss: 0.00001966
Iteration 324/1000 | Loss: 0.00001966
Iteration 325/1000 | Loss: 0.00001966
Iteration 326/1000 | Loss: 0.00001966
Iteration 327/1000 | Loss: 0.00001966
Iteration 328/1000 | Loss: 0.00001966
Iteration 329/1000 | Loss: 0.00001966
Iteration 330/1000 | Loss: 0.00001965
Iteration 331/1000 | Loss: 0.00001965
Iteration 332/1000 | Loss: 0.00001965
Iteration 333/1000 | Loss: 0.00001965
Iteration 334/1000 | Loss: 0.00001965
Iteration 335/1000 | Loss: 0.00001965
Iteration 336/1000 | Loss: 0.00001965
Iteration 337/1000 | Loss: 0.00001965
Iteration 338/1000 | Loss: 0.00001965
Iteration 339/1000 | Loss: 0.00001965
Iteration 340/1000 | Loss: 0.00001965
Iteration 341/1000 | Loss: 0.00001965
Iteration 342/1000 | Loss: 0.00001965
Iteration 343/1000 | Loss: 0.00001965
Iteration 344/1000 | Loss: 0.00001965
Iteration 345/1000 | Loss: 0.00001965
Iteration 346/1000 | Loss: 0.00001965
Iteration 347/1000 | Loss: 0.00001965
Iteration 348/1000 | Loss: 0.00001965
Iteration 349/1000 | Loss: 0.00001964
Iteration 350/1000 | Loss: 0.00001964
Iteration 351/1000 | Loss: 0.00001964
Iteration 352/1000 | Loss: 0.00001964
Iteration 353/1000 | Loss: 0.00001964
Iteration 354/1000 | Loss: 0.00001964
Iteration 355/1000 | Loss: 0.00010341
Iteration 356/1000 | Loss: 0.00002447
Iteration 357/1000 | Loss: 0.00006933
Iteration 358/1000 | Loss: 0.00001975
Iteration 359/1000 | Loss: 0.00009478
Iteration 360/1000 | Loss: 0.00002250
Iteration 361/1000 | Loss: 0.00002108
Iteration 362/1000 | Loss: 0.00002056
Iteration 363/1000 | Loss: 0.00002032
Iteration 364/1000 | Loss: 0.00002030
Iteration 365/1000 | Loss: 0.00002026
Iteration 366/1000 | Loss: 0.00002025
Iteration 367/1000 | Loss: 0.00002025
Iteration 368/1000 | Loss: 0.00002023
Iteration 369/1000 | Loss: 0.00002022
Iteration 370/1000 | Loss: 0.00002020
Iteration 371/1000 | Loss: 0.00002019
Iteration 372/1000 | Loss: 0.00002019
Iteration 373/1000 | Loss: 0.00002018
Iteration 374/1000 | Loss: 0.00002018
Iteration 375/1000 | Loss: 0.00002018
Iteration 376/1000 | Loss: 0.00002018
Iteration 377/1000 | Loss: 0.00002017
Iteration 378/1000 | Loss: 0.00002017
Iteration 379/1000 | Loss: 0.00002017
Iteration 380/1000 | Loss: 0.00002017
Iteration 381/1000 | Loss: 0.00002016
Iteration 382/1000 | Loss: 0.00002016
Iteration 383/1000 | Loss: 0.00002016
Iteration 384/1000 | Loss: 0.00002016
Iteration 385/1000 | Loss: 0.00002016
Iteration 386/1000 | Loss: 0.00002016
Iteration 387/1000 | Loss: 0.00002016
Iteration 388/1000 | Loss: 0.00002016
Iteration 389/1000 | Loss: 0.00002016
Iteration 390/1000 | Loss: 0.00002016
Iteration 391/1000 | Loss: 0.00002015
Iteration 392/1000 | Loss: 0.00002015
Iteration 393/1000 | Loss: 0.00002015
Iteration 394/1000 | Loss: 0.00002015
Iteration 395/1000 | Loss: 0.00002015
Iteration 396/1000 | Loss: 0.00002015
Iteration 397/1000 | Loss: 0.00002015
Iteration 398/1000 | Loss: 0.00002015
Iteration 399/1000 | Loss: 0.00002015
Iteration 400/1000 | Loss: 0.00002015
Iteration 401/1000 | Loss: 0.00002015
Iteration 402/1000 | Loss: 0.00002015
Iteration 403/1000 | Loss: 0.00002015
Iteration 404/1000 | Loss: 0.00002015
Iteration 405/1000 | Loss: 0.00002015
Iteration 406/1000 | Loss: 0.00002015
Iteration 407/1000 | Loss: 0.00002015
Iteration 408/1000 | Loss: 0.00002015
Iteration 409/1000 | Loss: 0.00002015
Iteration 410/1000 | Loss: 0.00002015
Iteration 411/1000 | Loss: 0.00002015
Iteration 412/1000 | Loss: 0.00002015
Iteration 413/1000 | Loss: 0.00002015
Iteration 414/1000 | Loss: 0.00002014
Iteration 415/1000 | Loss: 0.00002014
Iteration 416/1000 | Loss: 0.00002014
Iteration 417/1000 | Loss: 0.00002014
Iteration 418/1000 | Loss: 0.00002014
Iteration 419/1000 | Loss: 0.00002014
Iteration 420/1000 | Loss: 0.00002014
Iteration 421/1000 | Loss: 0.00002014
Iteration 422/1000 | Loss: 0.00002014
Iteration 423/1000 | Loss: 0.00002014
Iteration 424/1000 | Loss: 0.00002014
Iteration 425/1000 | Loss: 0.00002014
Iteration 426/1000 | Loss: 0.00002014
Iteration 427/1000 | Loss: 0.00002013
Iteration 428/1000 | Loss: 0.00002013
Iteration 429/1000 | Loss: 0.00002013
Iteration 430/1000 | Loss: 0.00002013
Iteration 431/1000 | Loss: 0.00002013
Iteration 432/1000 | Loss: 0.00002013
Iteration 433/1000 | Loss: 0.00002013
Iteration 434/1000 | Loss: 0.00002013
Iteration 435/1000 | Loss: 0.00002013
Iteration 436/1000 | Loss: 0.00002013
Iteration 437/1000 | Loss: 0.00002012
Iteration 438/1000 | Loss: 0.00002012
Iteration 439/1000 | Loss: 0.00002012
Iteration 440/1000 | Loss: 0.00002012
Iteration 441/1000 | Loss: 0.00002012
Iteration 442/1000 | Loss: 0.00002012
Iteration 443/1000 | Loss: 0.00002012
Iteration 444/1000 | Loss: 0.00002012
Iteration 445/1000 | Loss: 0.00002012
Iteration 446/1000 | Loss: 0.00002012
Iteration 447/1000 | Loss: 0.00002012
Iteration 448/1000 | Loss: 0.00002012
Iteration 449/1000 | Loss: 0.00002012
Iteration 450/1000 | Loss: 0.00002012
Iteration 451/1000 | Loss: 0.00002011
Iteration 452/1000 | Loss: 0.00002011
Iteration 453/1000 | Loss: 0.00002011
Iteration 454/1000 | Loss: 0.00002011
Iteration 455/1000 | Loss: 0.00002011
Iteration 456/1000 | Loss: 0.00002011
Iteration 457/1000 | Loss: 0.00002011
Iteration 458/1000 | Loss: 0.00002011
Iteration 459/1000 | Loss: 0.00002011
Iteration 460/1000 | Loss: 0.00002011
Iteration 461/1000 | Loss: 0.00002011
Iteration 462/1000 | Loss: 0.00002011
Iteration 463/1000 | Loss: 0.00002011
Iteration 464/1000 | Loss: 0.00002011
Iteration 465/1000 | Loss: 0.00002010
Iteration 466/1000 | Loss: 0.00002010
Iteration 467/1000 | Loss: 0.00002010
Iteration 468/1000 | Loss: 0.00002010
Iteration 469/1000 | Loss: 0.00002010
Iteration 470/1000 | Loss: 0.00002010
Iteration 471/1000 | Loss: 0.00002010
Iteration 472/1000 | Loss: 0.00002009
Iteration 473/1000 | Loss: 0.00002009
Iteration 474/1000 | Loss: 0.00002009
Iteration 475/1000 | Loss: 0.00002009
Iteration 476/1000 | Loss: 0.00002009
Iteration 477/1000 | Loss: 0.00002009
Iteration 478/1000 | Loss: 0.00002009
Iteration 479/1000 | Loss: 0.00002009
Iteration 480/1000 | Loss: 0.00002009
Iteration 481/1000 | Loss: 0.00002009
Iteration 482/1000 | Loss: 0.00023408
Iteration 483/1000 | Loss: 0.00032994
Iteration 484/1000 | Loss: 0.00038189
Iteration 485/1000 | Loss: 0.00018374
Iteration 486/1000 | Loss: 0.00002703
Iteration 487/1000 | Loss: 0.00005299
Iteration 488/1000 | Loss: 0.00002323
Iteration 489/1000 | Loss: 0.00002265
Iteration 490/1000 | Loss: 0.00002210
Iteration 491/1000 | Loss: 0.00029770
Iteration 492/1000 | Loss: 0.00017081
Iteration 493/1000 | Loss: 0.00022012
Iteration 494/1000 | Loss: 0.00022502
Iteration 495/1000 | Loss: 0.00088387
Iteration 496/1000 | Loss: 0.00010669
Iteration 497/1000 | Loss: 0.00004455
Iteration 498/1000 | Loss: 0.00003470
Iteration 499/1000 | Loss: 0.00002310
Iteration 500/1000 | Loss: 0.00005688
Iteration 501/1000 | Loss: 0.00002329
Iteration 502/1000 | Loss: 0.00002029
Iteration 503/1000 | Loss: 0.00002168
Iteration 504/1000 | Loss: 0.00001976
Iteration 505/1000 | Loss: 0.00002597
Iteration 506/1000 | Loss: 0.00001956
Iteration 507/1000 | Loss: 0.00001953
Iteration 508/1000 | Loss: 0.00001952
Iteration 509/1000 | Loss: 0.00001951
Iteration 510/1000 | Loss: 0.00001943
Iteration 511/1000 | Loss: 0.00001942
Iteration 512/1000 | Loss: 0.00001942
Iteration 513/1000 | Loss: 0.00001941
Iteration 514/1000 | Loss: 0.00001941
Iteration 515/1000 | Loss: 0.00001940
Iteration 516/1000 | Loss: 0.00001940
Iteration 517/1000 | Loss: 0.00001940
Iteration 518/1000 | Loss: 0.00001939
Iteration 519/1000 | Loss: 0.00001939
Iteration 520/1000 | Loss: 0.00001939
Iteration 521/1000 | Loss: 0.00001938
Iteration 522/1000 | Loss: 0.00001937
Iteration 523/1000 | Loss: 0.00001937
Iteration 524/1000 | Loss: 0.00001935
Iteration 525/1000 | Loss: 0.00001934
Iteration 526/1000 | Loss: 0.00001933
Iteration 527/1000 | Loss: 0.00001933
Iteration 528/1000 | Loss: 0.00001933
Iteration 529/1000 | Loss: 0.00001932
Iteration 530/1000 | Loss: 0.00007172
Iteration 531/1000 | Loss: 0.00002010
Iteration 532/1000 | Loss: 0.00001932
Iteration 533/1000 | Loss: 0.00001931
Iteration 534/1000 | Loss: 0.00001931
Iteration 535/1000 | Loss: 0.00001931
Iteration 536/1000 | Loss: 0.00001931
Iteration 537/1000 | Loss: 0.00001930
Iteration 538/1000 | Loss: 0.00001930
Iteration 539/1000 | Loss: 0.00001929
Iteration 540/1000 | Loss: 0.00001928
Iteration 541/1000 | Loss: 0.00005393
Iteration 542/1000 | Loss: 0.00002368
Iteration 543/1000 | Loss: 0.00003054
Iteration 544/1000 | Loss: 0.00002420
Iteration 545/1000 | Loss: 0.00001933
Iteration 546/1000 | Loss: 0.00001931
Iteration 547/1000 | Loss: 0.00001931
Iteration 548/1000 | Loss: 0.00001931
Iteration 549/1000 | Loss: 0.00001931
Iteration 550/1000 | Loss: 0.00001931
Iteration 551/1000 | Loss: 0.00001930
Iteration 552/1000 | Loss: 0.00001930
Iteration 553/1000 | Loss: 0.00001930
Iteration 554/1000 | Loss: 0.00001930
Iteration 555/1000 | Loss: 0.00001930
Iteration 556/1000 | Loss: 0.00001930
Iteration 557/1000 | Loss: 0.00001930
Iteration 558/1000 | Loss: 0.00001930
Iteration 559/1000 | Loss: 0.00001930
Iteration 560/1000 | Loss: 0.00001930
Iteration 561/1000 | Loss: 0.00001929
Iteration 562/1000 | Loss: 0.00001929
Iteration 563/1000 | Loss: 0.00001929
Iteration 564/1000 | Loss: 0.00001929
Iteration 565/1000 | Loss: 0.00001929
Iteration 566/1000 | Loss: 0.00001928
Iteration 567/1000 | Loss: 0.00001928
Iteration 568/1000 | Loss: 0.00001928
Iteration 569/1000 | Loss: 0.00001928
Iteration 570/1000 | Loss: 0.00001928
Iteration 571/1000 | Loss: 0.00001928
Iteration 572/1000 | Loss: 0.00001928
Iteration 573/1000 | Loss: 0.00002239
Iteration 574/1000 | Loss: 0.00001930
Iteration 575/1000 | Loss: 0.00001930
Iteration 576/1000 | Loss: 0.00001930
Iteration 577/1000 | Loss: 0.00001929
Iteration 578/1000 | Loss: 0.00001929
Iteration 579/1000 | Loss: 0.00001928
Iteration 580/1000 | Loss: 0.00001928
Iteration 581/1000 | Loss: 0.00001928
Iteration 582/1000 | Loss: 0.00001928
Iteration 583/1000 | Loss: 0.00001928
Iteration 584/1000 | Loss: 0.00001928
Iteration 585/1000 | Loss: 0.00001928
Iteration 586/1000 | Loss: 0.00001928
Iteration 587/1000 | Loss: 0.00001928
Iteration 588/1000 | Loss: 0.00001928
Iteration 589/1000 | Loss: 0.00001928
Iteration 590/1000 | Loss: 0.00001927
Iteration 591/1000 | Loss: 0.00001927
Iteration 592/1000 | Loss: 0.00001927
Iteration 593/1000 | Loss: 0.00001926
Iteration 594/1000 | Loss: 0.00001926
Iteration 595/1000 | Loss: 0.00001926
Iteration 596/1000 | Loss: 0.00001926
Iteration 597/1000 | Loss: 0.00001925
Iteration 598/1000 | Loss: 0.00001925
Iteration 599/1000 | Loss: 0.00001925
Iteration 600/1000 | Loss: 0.00001925
Iteration 601/1000 | Loss: 0.00001925
Iteration 602/1000 | Loss: 0.00001925
Iteration 603/1000 | Loss: 0.00001924
Iteration 604/1000 | Loss: 0.00001924
Iteration 605/1000 | Loss: 0.00001924
Iteration 606/1000 | Loss: 0.00001924
Iteration 607/1000 | Loss: 0.00001924
Iteration 608/1000 | Loss: 0.00001923
Iteration 609/1000 | Loss: 0.00001923
Iteration 610/1000 | Loss: 0.00001923
Iteration 611/1000 | Loss: 0.00001922
Iteration 612/1000 | Loss: 0.00001922
Iteration 613/1000 | Loss: 0.00001922
Iteration 614/1000 | Loss: 0.00001922
Iteration 615/1000 | Loss: 0.00001922
Iteration 616/1000 | Loss: 0.00001922
Iteration 617/1000 | Loss: 0.00001922
Iteration 618/1000 | Loss: 0.00001922
Iteration 619/1000 | Loss: 0.00001922
Iteration 620/1000 | Loss: 0.00001922
Iteration 621/1000 | Loss: 0.00001922
Iteration 622/1000 | Loss: 0.00001922
Iteration 623/1000 | Loss: 0.00001922
Iteration 624/1000 | Loss: 0.00001922
Iteration 625/1000 | Loss: 0.00001921
Iteration 626/1000 | Loss: 0.00001920
Iteration 627/1000 | Loss: 0.00001919
Iteration 628/1000 | Loss: 0.00001919
Iteration 629/1000 | Loss: 0.00001919
Iteration 630/1000 | Loss: 0.00001919
Iteration 631/1000 | Loss: 0.00001919
Iteration 632/1000 | Loss: 0.00001919
Iteration 633/1000 | Loss: 0.00001919
Iteration 634/1000 | Loss: 0.00001919
Iteration 635/1000 | Loss: 0.00001919
Iteration 636/1000 | Loss: 0.00001919
Iteration 637/1000 | Loss: 0.00001919
Iteration 638/1000 | Loss: 0.00001919
Iteration 639/1000 | Loss: 0.00001919
Iteration 640/1000 | Loss: 0.00001919
Iteration 641/1000 | Loss: 0.00001919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 641. Stopping optimization.
Last 5 losses: [1.9188722944818437e-05, 1.9188722944818437e-05, 1.9188722944818437e-05, 1.9188722944818437e-05, 1.9188722944818437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9188722944818437e-05

Optimization complete. Final v2v error: 3.7272071838378906 mm

Highest mean error: 4.873270034790039 mm for frame 148

Lowest mean error: 3.445427894592285 mm for frame 98

Saving results

Total time: 376.7597596645355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356199
Iteration 2/25 | Loss: 0.00139464
Iteration 3/25 | Loss: 0.00125968
Iteration 4/25 | Loss: 0.00124672
Iteration 5/25 | Loss: 0.00124350
Iteration 6/25 | Loss: 0.00124350
Iteration 7/25 | Loss: 0.00124350
Iteration 8/25 | Loss: 0.00124350
Iteration 9/25 | Loss: 0.00124350
Iteration 10/25 | Loss: 0.00124350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012434987584128976, 0.0012434987584128976, 0.0012434987584128976, 0.0012434987584128976, 0.0012434987584128976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012434987584128976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32292438
Iteration 2/25 | Loss: 0.00105501
Iteration 3/25 | Loss: 0.00105501
Iteration 4/25 | Loss: 0.00105501
Iteration 5/25 | Loss: 0.00105501
Iteration 6/25 | Loss: 0.00105501
Iteration 7/25 | Loss: 0.00105501
Iteration 8/25 | Loss: 0.00105501
Iteration 9/25 | Loss: 0.00105501
Iteration 10/25 | Loss: 0.00105501
Iteration 11/25 | Loss: 0.00105501
Iteration 12/25 | Loss: 0.00105501
Iteration 13/25 | Loss: 0.00105501
Iteration 14/25 | Loss: 0.00105501
Iteration 15/25 | Loss: 0.00105501
Iteration 16/25 | Loss: 0.00105501
Iteration 17/25 | Loss: 0.00105501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010550059378147125, 0.0010550059378147125, 0.0010550059378147125, 0.0010550059378147125, 0.0010550059378147125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010550059378147125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105501
Iteration 2/1000 | Loss: 0.00003682
Iteration 3/1000 | Loss: 0.00002437
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001699
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001508
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001472
Iteration 13/1000 | Loss: 0.00001464
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001441
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001435
Iteration 19/1000 | Loss: 0.00001434
Iteration 20/1000 | Loss: 0.00001433
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001430
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001416
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001405
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001401
Iteration 34/1000 | Loss: 0.00001399
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001395
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001394
Iteration 42/1000 | Loss: 0.00001393
Iteration 43/1000 | Loss: 0.00001393
Iteration 44/1000 | Loss: 0.00001393
Iteration 45/1000 | Loss: 0.00001393
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001392
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001392
Iteration 50/1000 | Loss: 0.00001392
Iteration 51/1000 | Loss: 0.00001392
Iteration 52/1000 | Loss: 0.00001392
Iteration 53/1000 | Loss: 0.00001392
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001391
Iteration 56/1000 | Loss: 0.00001391
Iteration 57/1000 | Loss: 0.00001391
Iteration 58/1000 | Loss: 0.00001390
Iteration 59/1000 | Loss: 0.00001390
Iteration 60/1000 | Loss: 0.00001390
Iteration 61/1000 | Loss: 0.00001390
Iteration 62/1000 | Loss: 0.00001389
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001383
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001382
Iteration 87/1000 | Loss: 0.00001382
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001381
Iteration 93/1000 | Loss: 0.00001381
Iteration 94/1000 | Loss: 0.00001381
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001380
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001377
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001376
Iteration 118/1000 | Loss: 0.00001376
Iteration 119/1000 | Loss: 0.00001376
Iteration 120/1000 | Loss: 0.00001376
Iteration 121/1000 | Loss: 0.00001375
Iteration 122/1000 | Loss: 0.00001375
Iteration 123/1000 | Loss: 0.00001375
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001375
Iteration 129/1000 | Loss: 0.00001375
Iteration 130/1000 | Loss: 0.00001375
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001374
Iteration 133/1000 | Loss: 0.00001374
Iteration 134/1000 | Loss: 0.00001374
Iteration 135/1000 | Loss: 0.00001374
Iteration 136/1000 | Loss: 0.00001374
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001374
Iteration 139/1000 | Loss: 0.00001374
Iteration 140/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3739957466896158e-05, 1.3739957466896158e-05, 1.3739957466896158e-05, 1.3739957466896158e-05, 1.3739957466896158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3739957466896158e-05

Optimization complete. Final v2v error: 3.1449246406555176 mm

Highest mean error: 3.5710158348083496 mm for frame 202

Lowest mean error: 2.822371006011963 mm for frame 150

Saving results

Total time: 44.91917896270752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856468
Iteration 2/25 | Loss: 0.00184894
Iteration 3/25 | Loss: 0.00154613
Iteration 4/25 | Loss: 0.00150164
Iteration 5/25 | Loss: 0.00149113
Iteration 6/25 | Loss: 0.00146730
Iteration 7/25 | Loss: 0.00138026
Iteration 8/25 | Loss: 0.00137614
Iteration 9/25 | Loss: 0.00136296
Iteration 10/25 | Loss: 0.00135471
Iteration 11/25 | Loss: 0.00136541
Iteration 12/25 | Loss: 0.00137371
Iteration 13/25 | Loss: 0.00137877
Iteration 14/25 | Loss: 0.00136186
Iteration 15/25 | Loss: 0.00133688
Iteration 16/25 | Loss: 0.00133117
Iteration 17/25 | Loss: 0.00133040
Iteration 18/25 | Loss: 0.00133021
Iteration 19/25 | Loss: 0.00133021
Iteration 20/25 | Loss: 0.00133020
Iteration 21/25 | Loss: 0.00133020
Iteration 22/25 | Loss: 0.00133020
Iteration 23/25 | Loss: 0.00133020
Iteration 24/25 | Loss: 0.00133020
Iteration 25/25 | Loss: 0.00133020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013302020961418748, 0.0013302020961418748, 0.0013302020961418748, 0.0013302020961418748, 0.0013302020961418748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013302020961418748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.51807261
Iteration 2/25 | Loss: 0.00106812
Iteration 3/25 | Loss: 0.00106811
Iteration 4/25 | Loss: 0.00106811
Iteration 5/25 | Loss: 0.00106811
Iteration 6/25 | Loss: 0.00106811
Iteration 7/25 | Loss: 0.00106811
Iteration 8/25 | Loss: 0.00106811
Iteration 9/25 | Loss: 0.00106811
Iteration 10/25 | Loss: 0.00106811
Iteration 11/25 | Loss: 0.00106811
Iteration 12/25 | Loss: 0.00106811
Iteration 13/25 | Loss: 0.00106811
Iteration 14/25 | Loss: 0.00106811
Iteration 15/25 | Loss: 0.00106811
Iteration 16/25 | Loss: 0.00106811
Iteration 17/25 | Loss: 0.00106811
Iteration 18/25 | Loss: 0.00106811
Iteration 19/25 | Loss: 0.00106811
Iteration 20/25 | Loss: 0.00106811
Iteration 21/25 | Loss: 0.00106811
Iteration 22/25 | Loss: 0.00106811
Iteration 23/25 | Loss: 0.00106811
Iteration 24/25 | Loss: 0.00106811
Iteration 25/25 | Loss: 0.00106811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106811
Iteration 2/1000 | Loss: 0.00003545
Iteration 3/1000 | Loss: 0.00015739
Iteration 4/1000 | Loss: 0.00004406
Iteration 5/1000 | Loss: 0.00002776
Iteration 6/1000 | Loss: 0.00002191
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00001987
Iteration 10/1000 | Loss: 0.00001954
Iteration 11/1000 | Loss: 0.00001929
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001878
Iteration 14/1000 | Loss: 0.00001869
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001855
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001846
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001837
Iteration 22/1000 | Loss: 0.00001837
Iteration 23/1000 | Loss: 0.00001833
Iteration 24/1000 | Loss: 0.00001833
Iteration 25/1000 | Loss: 0.00001831
Iteration 26/1000 | Loss: 0.00001830
Iteration 27/1000 | Loss: 0.00001830
Iteration 28/1000 | Loss: 0.00001829
Iteration 29/1000 | Loss: 0.00001829
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001825
Iteration 32/1000 | Loss: 0.00001824
Iteration 33/1000 | Loss: 0.00001824
Iteration 34/1000 | Loss: 0.00001824
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001823
Iteration 37/1000 | Loss: 0.00001822
Iteration 38/1000 | Loss: 0.00001822
Iteration 39/1000 | Loss: 0.00001821
Iteration 40/1000 | Loss: 0.00001820
Iteration 41/1000 | Loss: 0.00001820
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00001819
Iteration 45/1000 | Loss: 0.00001819
Iteration 46/1000 | Loss: 0.00001819
Iteration 47/1000 | Loss: 0.00001819
Iteration 48/1000 | Loss: 0.00001819
Iteration 49/1000 | Loss: 0.00001819
Iteration 50/1000 | Loss: 0.00001819
Iteration 51/1000 | Loss: 0.00001819
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001818
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00001818
Iteration 56/1000 | Loss: 0.00001818
Iteration 57/1000 | Loss: 0.00001818
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001815
Iteration 60/1000 | Loss: 0.00001815
Iteration 61/1000 | Loss: 0.00001814
Iteration 62/1000 | Loss: 0.00001814
Iteration 63/1000 | Loss: 0.00001813
Iteration 64/1000 | Loss: 0.00001813
Iteration 65/1000 | Loss: 0.00001813
Iteration 66/1000 | Loss: 0.00001812
Iteration 67/1000 | Loss: 0.00001812
Iteration 68/1000 | Loss: 0.00001811
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001809
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001804
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001803
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001801
Iteration 96/1000 | Loss: 0.00001801
Iteration 97/1000 | Loss: 0.00001800
Iteration 98/1000 | Loss: 0.00001800
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001797
Iteration 102/1000 | Loss: 0.00001797
Iteration 103/1000 | Loss: 0.00001796
Iteration 104/1000 | Loss: 0.00001796
Iteration 105/1000 | Loss: 0.00001796
Iteration 106/1000 | Loss: 0.00001796
Iteration 107/1000 | Loss: 0.00001796
Iteration 108/1000 | Loss: 0.00001796
Iteration 109/1000 | Loss: 0.00001796
Iteration 110/1000 | Loss: 0.00001796
Iteration 111/1000 | Loss: 0.00001796
Iteration 112/1000 | Loss: 0.00001796
Iteration 113/1000 | Loss: 0.00001796
Iteration 114/1000 | Loss: 0.00001796
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001795
Iteration 117/1000 | Loss: 0.00001795
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001793
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001793
Iteration 125/1000 | Loss: 0.00001793
Iteration 126/1000 | Loss: 0.00001793
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001793
Iteration 129/1000 | Loss: 0.00001793
Iteration 130/1000 | Loss: 0.00001792
Iteration 131/1000 | Loss: 0.00001792
Iteration 132/1000 | Loss: 0.00001792
Iteration 133/1000 | Loss: 0.00001792
Iteration 134/1000 | Loss: 0.00001792
Iteration 135/1000 | Loss: 0.00001792
Iteration 136/1000 | Loss: 0.00001792
Iteration 137/1000 | Loss: 0.00001792
Iteration 138/1000 | Loss: 0.00001792
Iteration 139/1000 | Loss: 0.00001792
Iteration 140/1000 | Loss: 0.00001791
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001791
Iteration 143/1000 | Loss: 0.00001790
Iteration 144/1000 | Loss: 0.00001790
Iteration 145/1000 | Loss: 0.00001790
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001790
Iteration 149/1000 | Loss: 0.00001790
Iteration 150/1000 | Loss: 0.00001789
Iteration 151/1000 | Loss: 0.00001789
Iteration 152/1000 | Loss: 0.00001789
Iteration 153/1000 | Loss: 0.00001789
Iteration 154/1000 | Loss: 0.00001789
Iteration 155/1000 | Loss: 0.00001788
Iteration 156/1000 | Loss: 0.00001788
Iteration 157/1000 | Loss: 0.00001788
Iteration 158/1000 | Loss: 0.00001788
Iteration 159/1000 | Loss: 0.00001788
Iteration 160/1000 | Loss: 0.00001788
Iteration 161/1000 | Loss: 0.00001788
Iteration 162/1000 | Loss: 0.00001787
Iteration 163/1000 | Loss: 0.00001787
Iteration 164/1000 | Loss: 0.00001787
Iteration 165/1000 | Loss: 0.00001787
Iteration 166/1000 | Loss: 0.00001787
Iteration 167/1000 | Loss: 0.00001787
Iteration 168/1000 | Loss: 0.00001787
Iteration 169/1000 | Loss: 0.00001787
Iteration 170/1000 | Loss: 0.00001787
Iteration 171/1000 | Loss: 0.00001787
Iteration 172/1000 | Loss: 0.00001787
Iteration 173/1000 | Loss: 0.00001787
Iteration 174/1000 | Loss: 0.00001787
Iteration 175/1000 | Loss: 0.00001787
Iteration 176/1000 | Loss: 0.00001786
Iteration 177/1000 | Loss: 0.00001786
Iteration 178/1000 | Loss: 0.00001786
Iteration 179/1000 | Loss: 0.00001786
Iteration 180/1000 | Loss: 0.00001786
Iteration 181/1000 | Loss: 0.00001786
Iteration 182/1000 | Loss: 0.00001785
Iteration 183/1000 | Loss: 0.00001785
Iteration 184/1000 | Loss: 0.00001785
Iteration 185/1000 | Loss: 0.00001785
Iteration 186/1000 | Loss: 0.00001785
Iteration 187/1000 | Loss: 0.00001785
Iteration 188/1000 | Loss: 0.00001785
Iteration 189/1000 | Loss: 0.00001784
Iteration 190/1000 | Loss: 0.00001784
Iteration 191/1000 | Loss: 0.00001784
Iteration 192/1000 | Loss: 0.00001784
Iteration 193/1000 | Loss: 0.00001784
Iteration 194/1000 | Loss: 0.00001784
Iteration 195/1000 | Loss: 0.00001784
Iteration 196/1000 | Loss: 0.00001784
Iteration 197/1000 | Loss: 0.00001784
Iteration 198/1000 | Loss: 0.00001783
Iteration 199/1000 | Loss: 0.00001783
Iteration 200/1000 | Loss: 0.00001783
Iteration 201/1000 | Loss: 0.00001783
Iteration 202/1000 | Loss: 0.00001783
Iteration 203/1000 | Loss: 0.00001783
Iteration 204/1000 | Loss: 0.00001783
Iteration 205/1000 | Loss: 0.00001783
Iteration 206/1000 | Loss: 0.00001783
Iteration 207/1000 | Loss: 0.00001783
Iteration 208/1000 | Loss: 0.00001783
Iteration 209/1000 | Loss: 0.00001783
Iteration 210/1000 | Loss: 0.00001783
Iteration 211/1000 | Loss: 0.00001783
Iteration 212/1000 | Loss: 0.00001782
Iteration 213/1000 | Loss: 0.00001782
Iteration 214/1000 | Loss: 0.00001782
Iteration 215/1000 | Loss: 0.00001782
Iteration 216/1000 | Loss: 0.00001782
Iteration 217/1000 | Loss: 0.00001782
Iteration 218/1000 | Loss: 0.00001782
Iteration 219/1000 | Loss: 0.00001782
Iteration 220/1000 | Loss: 0.00001782
Iteration 221/1000 | Loss: 0.00001782
Iteration 222/1000 | Loss: 0.00001782
Iteration 223/1000 | Loss: 0.00001782
Iteration 224/1000 | Loss: 0.00001782
Iteration 225/1000 | Loss: 0.00001782
Iteration 226/1000 | Loss: 0.00001782
Iteration 227/1000 | Loss: 0.00001782
Iteration 228/1000 | Loss: 0.00001782
Iteration 229/1000 | Loss: 0.00001782
Iteration 230/1000 | Loss: 0.00001782
Iteration 231/1000 | Loss: 0.00001782
Iteration 232/1000 | Loss: 0.00001782
Iteration 233/1000 | Loss: 0.00001782
Iteration 234/1000 | Loss: 0.00001782
Iteration 235/1000 | Loss: 0.00001782
Iteration 236/1000 | Loss: 0.00001782
Iteration 237/1000 | Loss: 0.00001782
Iteration 238/1000 | Loss: 0.00001782
Iteration 239/1000 | Loss: 0.00001782
Iteration 240/1000 | Loss: 0.00001782
Iteration 241/1000 | Loss: 0.00001782
Iteration 242/1000 | Loss: 0.00001782
Iteration 243/1000 | Loss: 0.00001782
Iteration 244/1000 | Loss: 0.00001782
Iteration 245/1000 | Loss: 0.00001782
Iteration 246/1000 | Loss: 0.00001782
Iteration 247/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.7816064428188838e-05, 1.7816064428188838e-05, 1.7816064428188838e-05, 1.7816064428188838e-05, 1.7816064428188838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7816064428188838e-05

Optimization complete. Final v2v error: 3.4844512939453125 mm

Highest mean error: 5.370601654052734 mm for frame 95

Lowest mean error: 3.0496902465820312 mm for frame 52

Saving results

Total time: 73.72900938987732
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448840
Iteration 2/25 | Loss: 0.00137749
Iteration 3/25 | Loss: 0.00130703
Iteration 4/25 | Loss: 0.00129608
Iteration 5/25 | Loss: 0.00129175
Iteration 6/25 | Loss: 0.00129175
Iteration 7/25 | Loss: 0.00129175
Iteration 8/25 | Loss: 0.00129175
Iteration 9/25 | Loss: 0.00129175
Iteration 10/25 | Loss: 0.00129175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001291746972128749, 0.001291746972128749, 0.001291746972128749, 0.001291746972128749, 0.001291746972128749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001291746972128749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45999146
Iteration 2/25 | Loss: 0.00104566
Iteration 3/25 | Loss: 0.00104565
Iteration 4/25 | Loss: 0.00104565
Iteration 5/25 | Loss: 0.00104565
Iteration 6/25 | Loss: 0.00104565
Iteration 7/25 | Loss: 0.00104565
Iteration 8/25 | Loss: 0.00104565
Iteration 9/25 | Loss: 0.00104565
Iteration 10/25 | Loss: 0.00104565
Iteration 11/25 | Loss: 0.00104564
Iteration 12/25 | Loss: 0.00104564
Iteration 13/25 | Loss: 0.00104564
Iteration 14/25 | Loss: 0.00104564
Iteration 15/25 | Loss: 0.00104564
Iteration 16/25 | Loss: 0.00104564
Iteration 17/25 | Loss: 0.00104564
Iteration 18/25 | Loss: 0.00104564
Iteration 19/25 | Loss: 0.00104564
Iteration 20/25 | Loss: 0.00104564
Iteration 21/25 | Loss: 0.00104564
Iteration 22/25 | Loss: 0.00104564
Iteration 23/25 | Loss: 0.00104564
Iteration 24/25 | Loss: 0.00104564
Iteration 25/25 | Loss: 0.00104564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104564
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001769
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001628
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001556
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001514
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001511
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001492
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001476
Iteration 20/1000 | Loss: 0.00001472
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001471
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001463
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001450
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001447
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001436
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001428
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001427
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001425
Iteration 56/1000 | Loss: 0.00001425
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001424
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00001424
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001424
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001421
Iteration 71/1000 | Loss: 0.00001421
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001421
Iteration 75/1000 | Loss: 0.00001421
Iteration 76/1000 | Loss: 0.00001421
Iteration 77/1000 | Loss: 0.00001421
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001419
Iteration 82/1000 | Loss: 0.00001419
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001418
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001418
Iteration 89/1000 | Loss: 0.00001418
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001417
Iteration 94/1000 | Loss: 0.00001417
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001416
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00001415
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001414
Iteration 102/1000 | Loss: 0.00001414
Iteration 103/1000 | Loss: 0.00001414
Iteration 104/1000 | Loss: 0.00001413
Iteration 105/1000 | Loss: 0.00001413
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001413
Iteration 111/1000 | Loss: 0.00001412
Iteration 112/1000 | Loss: 0.00001412
Iteration 113/1000 | Loss: 0.00001412
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001411
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001410
Iteration 119/1000 | Loss: 0.00001410
Iteration 120/1000 | Loss: 0.00001410
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001410
Iteration 125/1000 | Loss: 0.00001410
Iteration 126/1000 | Loss: 0.00001410
Iteration 127/1000 | Loss: 0.00001410
Iteration 128/1000 | Loss: 0.00001410
Iteration 129/1000 | Loss: 0.00001410
Iteration 130/1000 | Loss: 0.00001410
Iteration 131/1000 | Loss: 0.00001410
Iteration 132/1000 | Loss: 0.00001410
Iteration 133/1000 | Loss: 0.00001410
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001410
Iteration 136/1000 | Loss: 0.00001410
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001410
Iteration 140/1000 | Loss: 0.00001410
Iteration 141/1000 | Loss: 0.00001410
Iteration 142/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.4101475244387984e-05, 1.4101475244387984e-05, 1.4101475244387984e-05, 1.4101475244387984e-05, 1.4101475244387984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4101475244387984e-05

Optimization complete. Final v2v error: 3.1894912719726562 mm

Highest mean error: 3.4768192768096924 mm for frame 176

Lowest mean error: 2.8843255043029785 mm for frame 98

Saving results

Total time: 42.18948936462402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820619
Iteration 2/25 | Loss: 0.00146573
Iteration 3/25 | Loss: 0.00136087
Iteration 4/25 | Loss: 0.00135093
Iteration 5/25 | Loss: 0.00134911
Iteration 6/25 | Loss: 0.00134911
Iteration 7/25 | Loss: 0.00134911
Iteration 8/25 | Loss: 0.00134911
Iteration 9/25 | Loss: 0.00134911
Iteration 10/25 | Loss: 0.00134911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013491149293258786, 0.0013491149293258786, 0.0013491149293258786, 0.0013491149293258786, 0.0013491149293258786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013491149293258786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30576563
Iteration 2/25 | Loss: 0.00092189
Iteration 3/25 | Loss: 0.00092182
Iteration 4/25 | Loss: 0.00092182
Iteration 5/25 | Loss: 0.00092182
Iteration 6/25 | Loss: 0.00092182
Iteration 7/25 | Loss: 0.00092182
Iteration 8/25 | Loss: 0.00092182
Iteration 9/25 | Loss: 0.00092182
Iteration 10/25 | Loss: 0.00092182
Iteration 11/25 | Loss: 0.00092182
Iteration 12/25 | Loss: 0.00092182
Iteration 13/25 | Loss: 0.00092182
Iteration 14/25 | Loss: 0.00092182
Iteration 15/25 | Loss: 0.00092182
Iteration 16/25 | Loss: 0.00092182
Iteration 17/25 | Loss: 0.00092182
Iteration 18/25 | Loss: 0.00092182
Iteration 19/25 | Loss: 0.00092182
Iteration 20/25 | Loss: 0.00092182
Iteration 21/25 | Loss: 0.00092182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009218180784955621, 0.0009218180784955621, 0.0009218180784955621, 0.0009218180784955621, 0.0009218180784955621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009218180784955621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092182
Iteration 2/1000 | Loss: 0.00004484
Iteration 3/1000 | Loss: 0.00002652
Iteration 4/1000 | Loss: 0.00002385
Iteration 5/1000 | Loss: 0.00002284
Iteration 6/1000 | Loss: 0.00002181
Iteration 7/1000 | Loss: 0.00002134
Iteration 8/1000 | Loss: 0.00002098
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00002032
Iteration 11/1000 | Loss: 0.00002006
Iteration 12/1000 | Loss: 0.00002001
Iteration 13/1000 | Loss: 0.00001989
Iteration 14/1000 | Loss: 0.00001963
Iteration 15/1000 | Loss: 0.00001946
Iteration 16/1000 | Loss: 0.00001929
Iteration 17/1000 | Loss: 0.00001925
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001915
Iteration 21/1000 | Loss: 0.00001914
Iteration 22/1000 | Loss: 0.00001914
Iteration 23/1000 | Loss: 0.00001912
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001910
Iteration 27/1000 | Loss: 0.00001908
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001905
Iteration 30/1000 | Loss: 0.00001902
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001899
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001889
Iteration 47/1000 | Loss: 0.00001886
Iteration 48/1000 | Loss: 0.00001886
Iteration 49/1000 | Loss: 0.00001886
Iteration 50/1000 | Loss: 0.00001885
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001883
Iteration 53/1000 | Loss: 0.00001883
Iteration 54/1000 | Loss: 0.00001883
Iteration 55/1000 | Loss: 0.00001883
Iteration 56/1000 | Loss: 0.00001882
Iteration 57/1000 | Loss: 0.00001882
Iteration 58/1000 | Loss: 0.00001882
Iteration 59/1000 | Loss: 0.00001881
Iteration 60/1000 | Loss: 0.00001881
Iteration 61/1000 | Loss: 0.00001881
Iteration 62/1000 | Loss: 0.00001880
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001877
Iteration 72/1000 | Loss: 0.00001877
Iteration 73/1000 | Loss: 0.00001877
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001877
Iteration 79/1000 | Loss: 0.00001877
Iteration 80/1000 | Loss: 0.00001877
Iteration 81/1000 | Loss: 0.00001877
Iteration 82/1000 | Loss: 0.00001877
Iteration 83/1000 | Loss: 0.00001877
Iteration 84/1000 | Loss: 0.00001877
Iteration 85/1000 | Loss: 0.00001877
Iteration 86/1000 | Loss: 0.00001877
Iteration 87/1000 | Loss: 0.00001877
Iteration 88/1000 | Loss: 0.00001877
Iteration 89/1000 | Loss: 0.00001877
Iteration 90/1000 | Loss: 0.00001877
Iteration 91/1000 | Loss: 0.00001877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.8767474102787673e-05, 1.8767474102787673e-05, 1.8767474102787673e-05, 1.8767474102787673e-05, 1.8767474102787673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8767474102787673e-05

Optimization complete. Final v2v error: 3.638329029083252 mm

Highest mean error: 4.378548622131348 mm for frame 174

Lowest mean error: 3.0413894653320312 mm for frame 23

Saving results

Total time: 41.09268283843994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449281
Iteration 2/25 | Loss: 0.00139417
Iteration 3/25 | Loss: 0.00130280
Iteration 4/25 | Loss: 0.00128789
Iteration 5/25 | Loss: 0.00128285
Iteration 6/25 | Loss: 0.00128232
Iteration 7/25 | Loss: 0.00128232
Iteration 8/25 | Loss: 0.00128232
Iteration 9/25 | Loss: 0.00128232
Iteration 10/25 | Loss: 0.00128232
Iteration 11/25 | Loss: 0.00128232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012823172146454453, 0.0012823172146454453, 0.0012823172146454453, 0.0012823172146454453, 0.0012823172146454453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012823172146454453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34575701
Iteration 2/25 | Loss: 0.00105137
Iteration 3/25 | Loss: 0.00105137
Iteration 4/25 | Loss: 0.00105137
Iteration 5/25 | Loss: 0.00105137
Iteration 6/25 | Loss: 0.00105136
Iteration 7/25 | Loss: 0.00105136
Iteration 8/25 | Loss: 0.00105136
Iteration 9/25 | Loss: 0.00105136
Iteration 10/25 | Loss: 0.00105136
Iteration 11/25 | Loss: 0.00105136
Iteration 12/25 | Loss: 0.00105136
Iteration 13/25 | Loss: 0.00105136
Iteration 14/25 | Loss: 0.00105136
Iteration 15/25 | Loss: 0.00105136
Iteration 16/25 | Loss: 0.00105136
Iteration 17/25 | Loss: 0.00105136
Iteration 18/25 | Loss: 0.00105136
Iteration 19/25 | Loss: 0.00105136
Iteration 20/25 | Loss: 0.00105136
Iteration 21/25 | Loss: 0.00105136
Iteration 22/25 | Loss: 0.00105136
Iteration 23/25 | Loss: 0.00105136
Iteration 24/25 | Loss: 0.00105136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010513628367334604, 0.0010513628367334604, 0.0010513628367334604, 0.0010513628367334604, 0.0010513628367334604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010513628367334604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105136
Iteration 2/1000 | Loss: 0.00003368
Iteration 3/1000 | Loss: 0.00002510
Iteration 4/1000 | Loss: 0.00002332
Iteration 5/1000 | Loss: 0.00002236
Iteration 6/1000 | Loss: 0.00002166
Iteration 7/1000 | Loss: 0.00002116
Iteration 8/1000 | Loss: 0.00002070
Iteration 9/1000 | Loss: 0.00002041
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00001984
Iteration 12/1000 | Loss: 0.00001962
Iteration 13/1000 | Loss: 0.00001942
Iteration 14/1000 | Loss: 0.00001938
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00001935
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001928
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001928
Iteration 22/1000 | Loss: 0.00001925
Iteration 23/1000 | Loss: 0.00001921
Iteration 24/1000 | Loss: 0.00001920
Iteration 25/1000 | Loss: 0.00001919
Iteration 26/1000 | Loss: 0.00001915
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001907
Iteration 31/1000 | Loss: 0.00001905
Iteration 32/1000 | Loss: 0.00001905
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001901
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001893
Iteration 37/1000 | Loss: 0.00001892
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001889
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001887
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001886
Iteration 47/1000 | Loss: 0.00001886
Iteration 48/1000 | Loss: 0.00001885
Iteration 49/1000 | Loss: 0.00001885
Iteration 50/1000 | Loss: 0.00001885
Iteration 51/1000 | Loss: 0.00001884
Iteration 52/1000 | Loss: 0.00001884
Iteration 53/1000 | Loss: 0.00001884
Iteration 54/1000 | Loss: 0.00001884
Iteration 55/1000 | Loss: 0.00001883
Iteration 56/1000 | Loss: 0.00001883
Iteration 57/1000 | Loss: 0.00001883
Iteration 58/1000 | Loss: 0.00001883
Iteration 59/1000 | Loss: 0.00001883
Iteration 60/1000 | Loss: 0.00001883
Iteration 61/1000 | Loss: 0.00001882
Iteration 62/1000 | Loss: 0.00001882
Iteration 63/1000 | Loss: 0.00001882
Iteration 64/1000 | Loss: 0.00001881
Iteration 65/1000 | Loss: 0.00001881
Iteration 66/1000 | Loss: 0.00001881
Iteration 67/1000 | Loss: 0.00001881
Iteration 68/1000 | Loss: 0.00001881
Iteration 69/1000 | Loss: 0.00001880
Iteration 70/1000 | Loss: 0.00001880
Iteration 71/1000 | Loss: 0.00001880
Iteration 72/1000 | Loss: 0.00001880
Iteration 73/1000 | Loss: 0.00001880
Iteration 74/1000 | Loss: 0.00001880
Iteration 75/1000 | Loss: 0.00001880
Iteration 76/1000 | Loss: 0.00001880
Iteration 77/1000 | Loss: 0.00001879
Iteration 78/1000 | Loss: 0.00001879
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00001878
Iteration 81/1000 | Loss: 0.00001878
Iteration 82/1000 | Loss: 0.00001878
Iteration 83/1000 | Loss: 0.00001878
Iteration 84/1000 | Loss: 0.00001878
Iteration 85/1000 | Loss: 0.00001878
Iteration 86/1000 | Loss: 0.00001878
Iteration 87/1000 | Loss: 0.00001877
Iteration 88/1000 | Loss: 0.00001877
Iteration 89/1000 | Loss: 0.00001877
Iteration 90/1000 | Loss: 0.00001876
Iteration 91/1000 | Loss: 0.00001876
Iteration 92/1000 | Loss: 0.00001876
Iteration 93/1000 | Loss: 0.00001876
Iteration 94/1000 | Loss: 0.00001876
Iteration 95/1000 | Loss: 0.00001876
Iteration 96/1000 | Loss: 0.00001876
Iteration 97/1000 | Loss: 0.00001876
Iteration 98/1000 | Loss: 0.00001876
Iteration 99/1000 | Loss: 0.00001876
Iteration 100/1000 | Loss: 0.00001875
Iteration 101/1000 | Loss: 0.00001875
Iteration 102/1000 | Loss: 0.00001875
Iteration 103/1000 | Loss: 0.00001875
Iteration 104/1000 | Loss: 0.00001875
Iteration 105/1000 | Loss: 0.00001875
Iteration 106/1000 | Loss: 0.00001875
Iteration 107/1000 | Loss: 0.00001875
Iteration 108/1000 | Loss: 0.00001875
Iteration 109/1000 | Loss: 0.00001875
Iteration 110/1000 | Loss: 0.00001874
Iteration 111/1000 | Loss: 0.00001874
Iteration 112/1000 | Loss: 0.00001874
Iteration 113/1000 | Loss: 0.00001874
Iteration 114/1000 | Loss: 0.00001874
Iteration 115/1000 | Loss: 0.00001874
Iteration 116/1000 | Loss: 0.00001874
Iteration 117/1000 | Loss: 0.00001874
Iteration 118/1000 | Loss: 0.00001874
Iteration 119/1000 | Loss: 0.00001874
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001874
Iteration 123/1000 | Loss: 0.00001874
Iteration 124/1000 | Loss: 0.00001874
Iteration 125/1000 | Loss: 0.00001874
Iteration 126/1000 | Loss: 0.00001874
Iteration 127/1000 | Loss: 0.00001874
Iteration 128/1000 | Loss: 0.00001874
Iteration 129/1000 | Loss: 0.00001874
Iteration 130/1000 | Loss: 0.00001874
Iteration 131/1000 | Loss: 0.00001874
Iteration 132/1000 | Loss: 0.00001874
Iteration 133/1000 | Loss: 0.00001874
Iteration 134/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.873746805358678e-05, 1.873746805358678e-05, 1.873746805358678e-05, 1.873746805358678e-05, 1.873746805358678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.873746805358678e-05

Optimization complete. Final v2v error: 3.6876473426818848 mm

Highest mean error: 4.390392780303955 mm for frame 29

Lowest mean error: 3.525362253189087 mm for frame 3

Saving results

Total time: 38.29336142539978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424035
Iteration 2/25 | Loss: 0.00138756
Iteration 3/25 | Loss: 0.00130011
Iteration 4/25 | Loss: 0.00128600
Iteration 5/25 | Loss: 0.00128146
Iteration 6/25 | Loss: 0.00128093
Iteration 7/25 | Loss: 0.00128093
Iteration 8/25 | Loss: 0.00128093
Iteration 9/25 | Loss: 0.00128093
Iteration 10/25 | Loss: 0.00128093
Iteration 11/25 | Loss: 0.00128093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00128092837985605, 0.00128092837985605, 0.00128092837985605, 0.00128092837985605, 0.00128092837985605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00128092837985605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39933956
Iteration 2/25 | Loss: 0.00106884
Iteration 3/25 | Loss: 0.00106884
Iteration 4/25 | Loss: 0.00106884
Iteration 5/25 | Loss: 0.00106884
Iteration 6/25 | Loss: 0.00106884
Iteration 7/25 | Loss: 0.00106884
Iteration 8/25 | Loss: 0.00106884
Iteration 9/25 | Loss: 0.00106884
Iteration 10/25 | Loss: 0.00106884
Iteration 11/25 | Loss: 0.00106884
Iteration 12/25 | Loss: 0.00106884
Iteration 13/25 | Loss: 0.00106884
Iteration 14/25 | Loss: 0.00106884
Iteration 15/25 | Loss: 0.00106884
Iteration 16/25 | Loss: 0.00106884
Iteration 17/25 | Loss: 0.00106884
Iteration 18/25 | Loss: 0.00106884
Iteration 19/25 | Loss: 0.00106884
Iteration 20/25 | Loss: 0.00106884
Iteration 21/25 | Loss: 0.00106884
Iteration 22/25 | Loss: 0.00106884
Iteration 23/25 | Loss: 0.00106884
Iteration 24/25 | Loss: 0.00106884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010688379406929016, 0.0010688379406929016, 0.0010688379406929016, 0.0010688379406929016, 0.0010688379406929016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010688379406929016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106884
Iteration 2/1000 | Loss: 0.00003358
Iteration 3/1000 | Loss: 0.00002528
Iteration 4/1000 | Loss: 0.00002257
Iteration 5/1000 | Loss: 0.00002123
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001898
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001802
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001771
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001756
Iteration 21/1000 | Loss: 0.00001754
Iteration 22/1000 | Loss: 0.00001753
Iteration 23/1000 | Loss: 0.00001753
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001744
Iteration 32/1000 | Loss: 0.00001742
Iteration 33/1000 | Loss: 0.00001740
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001731
Iteration 43/1000 | Loss: 0.00001731
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001729
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001729
Iteration 51/1000 | Loss: 0.00001728
Iteration 52/1000 | Loss: 0.00001728
Iteration 53/1000 | Loss: 0.00001727
Iteration 54/1000 | Loss: 0.00001727
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001724
Iteration 57/1000 | Loss: 0.00001724
Iteration 58/1000 | Loss: 0.00001724
Iteration 59/1000 | Loss: 0.00001724
Iteration 60/1000 | Loss: 0.00001724
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001720
Iteration 74/1000 | Loss: 0.00001720
Iteration 75/1000 | Loss: 0.00001720
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001720
Iteration 78/1000 | Loss: 0.00001720
Iteration 79/1000 | Loss: 0.00001720
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001719
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001719
Iteration 85/1000 | Loss: 0.00001719
Iteration 86/1000 | Loss: 0.00001719
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001718
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001717
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00001717
Iteration 101/1000 | Loss: 0.00001717
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001715
Iteration 112/1000 | Loss: 0.00001715
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001715
Iteration 120/1000 | Loss: 0.00001715
Iteration 121/1000 | Loss: 0.00001715
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001714
Iteration 126/1000 | Loss: 0.00001714
Iteration 127/1000 | Loss: 0.00001714
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Iteration 132/1000 | Loss: 0.00001714
Iteration 133/1000 | Loss: 0.00001714
Iteration 134/1000 | Loss: 0.00001714
Iteration 135/1000 | Loss: 0.00001714
Iteration 136/1000 | Loss: 0.00001714
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001712
Iteration 146/1000 | Loss: 0.00001712
Iteration 147/1000 | Loss: 0.00001712
Iteration 148/1000 | Loss: 0.00001712
Iteration 149/1000 | Loss: 0.00001712
Iteration 150/1000 | Loss: 0.00001712
Iteration 151/1000 | Loss: 0.00001712
Iteration 152/1000 | Loss: 0.00001712
Iteration 153/1000 | Loss: 0.00001712
Iteration 154/1000 | Loss: 0.00001712
Iteration 155/1000 | Loss: 0.00001712
Iteration 156/1000 | Loss: 0.00001712
Iteration 157/1000 | Loss: 0.00001711
Iteration 158/1000 | Loss: 0.00001711
Iteration 159/1000 | Loss: 0.00001711
Iteration 160/1000 | Loss: 0.00001711
Iteration 161/1000 | Loss: 0.00001711
Iteration 162/1000 | Loss: 0.00001711
Iteration 163/1000 | Loss: 0.00001711
Iteration 164/1000 | Loss: 0.00001711
Iteration 165/1000 | Loss: 0.00001711
Iteration 166/1000 | Loss: 0.00001711
Iteration 167/1000 | Loss: 0.00001711
Iteration 168/1000 | Loss: 0.00001711
Iteration 169/1000 | Loss: 0.00001711
Iteration 170/1000 | Loss: 0.00001711
Iteration 171/1000 | Loss: 0.00001711
Iteration 172/1000 | Loss: 0.00001711
Iteration 173/1000 | Loss: 0.00001711
Iteration 174/1000 | Loss: 0.00001711
Iteration 175/1000 | Loss: 0.00001711
Iteration 176/1000 | Loss: 0.00001711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.711002187221311e-05, 1.711002187221311e-05, 1.711002187221311e-05, 1.711002187221311e-05, 1.711002187221311e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.711002187221311e-05

Optimization complete. Final v2v error: 3.53426456451416 mm

Highest mean error: 4.238450050354004 mm for frame 20

Lowest mean error: 3.3317344188690186 mm for frame 8

Saving results

Total time: 41.592026233673096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842840
Iteration 2/25 | Loss: 0.00193400
Iteration 3/25 | Loss: 0.00149306
Iteration 4/25 | Loss: 0.00143288
Iteration 5/25 | Loss: 0.00142799
Iteration 6/25 | Loss: 0.00142751
Iteration 7/25 | Loss: 0.00142751
Iteration 8/25 | Loss: 0.00142751
Iteration 9/25 | Loss: 0.00142751
Iteration 10/25 | Loss: 0.00142751
Iteration 11/25 | Loss: 0.00142751
Iteration 12/25 | Loss: 0.00142751
Iteration 13/25 | Loss: 0.00142751
Iteration 14/25 | Loss: 0.00142751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014275121502578259, 0.0014275121502578259, 0.0014275121502578259, 0.0014275121502578259, 0.0014275121502578259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014275121502578259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17744780
Iteration 2/25 | Loss: 0.00089926
Iteration 3/25 | Loss: 0.00089926
Iteration 4/25 | Loss: 0.00089926
Iteration 5/25 | Loss: 0.00089926
Iteration 6/25 | Loss: 0.00089926
Iteration 7/25 | Loss: 0.00089926
Iteration 8/25 | Loss: 0.00089926
Iteration 9/25 | Loss: 0.00089926
Iteration 10/25 | Loss: 0.00089926
Iteration 11/25 | Loss: 0.00089926
Iteration 12/25 | Loss: 0.00089926
Iteration 13/25 | Loss: 0.00089926
Iteration 14/25 | Loss: 0.00089926
Iteration 15/25 | Loss: 0.00089926
Iteration 16/25 | Loss: 0.00089926
Iteration 17/25 | Loss: 0.00089926
Iteration 18/25 | Loss: 0.00089926
Iteration 19/25 | Loss: 0.00089926
Iteration 20/25 | Loss: 0.00089926
Iteration 21/25 | Loss: 0.00089926
Iteration 22/25 | Loss: 0.00089926
Iteration 23/25 | Loss: 0.00089926
Iteration 24/25 | Loss: 0.00089926
Iteration 25/25 | Loss: 0.00089926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089926
Iteration 2/1000 | Loss: 0.00005434
Iteration 3/1000 | Loss: 0.00003431
Iteration 4/1000 | Loss: 0.00002801
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002457
Iteration 7/1000 | Loss: 0.00002399
Iteration 8/1000 | Loss: 0.00002350
Iteration 9/1000 | Loss: 0.00002304
Iteration 10/1000 | Loss: 0.00002286
Iteration 11/1000 | Loss: 0.00002268
Iteration 12/1000 | Loss: 0.00002264
Iteration 13/1000 | Loss: 0.00002238
Iteration 14/1000 | Loss: 0.00002215
Iteration 15/1000 | Loss: 0.00002214
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002208
Iteration 18/1000 | Loss: 0.00002204
Iteration 19/1000 | Loss: 0.00002204
Iteration 20/1000 | Loss: 0.00002187
Iteration 21/1000 | Loss: 0.00002184
Iteration 22/1000 | Loss: 0.00002181
Iteration 23/1000 | Loss: 0.00002181
Iteration 24/1000 | Loss: 0.00002181
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00002181
Iteration 27/1000 | Loss: 0.00002181
Iteration 28/1000 | Loss: 0.00002179
Iteration 29/1000 | Loss: 0.00002178
Iteration 30/1000 | Loss: 0.00002178
Iteration 31/1000 | Loss: 0.00002176
Iteration 32/1000 | Loss: 0.00002175
Iteration 33/1000 | Loss: 0.00002174
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002173
Iteration 36/1000 | Loss: 0.00002172
Iteration 37/1000 | Loss: 0.00002172
Iteration 38/1000 | Loss: 0.00002171
Iteration 39/1000 | Loss: 0.00002170
Iteration 40/1000 | Loss: 0.00002170
Iteration 41/1000 | Loss: 0.00002170
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002168
Iteration 44/1000 | Loss: 0.00002167
Iteration 45/1000 | Loss: 0.00002167
Iteration 46/1000 | Loss: 0.00002166
Iteration 47/1000 | Loss: 0.00002166
Iteration 48/1000 | Loss: 0.00002166
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002166
Iteration 52/1000 | Loss: 0.00002166
Iteration 53/1000 | Loss: 0.00002166
Iteration 54/1000 | Loss: 0.00002166
Iteration 55/1000 | Loss: 0.00002166
Iteration 56/1000 | Loss: 0.00002165
Iteration 57/1000 | Loss: 0.00002165
Iteration 58/1000 | Loss: 0.00002164
Iteration 59/1000 | Loss: 0.00002163
Iteration 60/1000 | Loss: 0.00002163
Iteration 61/1000 | Loss: 0.00002163
Iteration 62/1000 | Loss: 0.00002162
Iteration 63/1000 | Loss: 0.00002162
Iteration 64/1000 | Loss: 0.00002162
Iteration 65/1000 | Loss: 0.00002162
Iteration 66/1000 | Loss: 0.00002162
Iteration 67/1000 | Loss: 0.00002162
Iteration 68/1000 | Loss: 0.00002162
Iteration 69/1000 | Loss: 0.00002162
Iteration 70/1000 | Loss: 0.00002162
Iteration 71/1000 | Loss: 0.00002162
Iteration 72/1000 | Loss: 0.00002162
Iteration 73/1000 | Loss: 0.00002162
Iteration 74/1000 | Loss: 0.00002162
Iteration 75/1000 | Loss: 0.00002162
Iteration 76/1000 | Loss: 0.00002162
Iteration 77/1000 | Loss: 0.00002162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.161795418942347e-05, 2.161795418942347e-05, 2.161795418942347e-05, 2.161795418942347e-05, 2.161795418942347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.161795418942347e-05

Optimization complete. Final v2v error: 3.900367259979248 mm

Highest mean error: 4.099290370941162 mm for frame 67

Lowest mean error: 3.676560163497925 mm for frame 35

Saving results

Total time: 33.22251653671265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510638
Iteration 2/25 | Loss: 0.00154266
Iteration 3/25 | Loss: 0.00134468
Iteration 4/25 | Loss: 0.00132804
Iteration 5/25 | Loss: 0.00132447
Iteration 6/25 | Loss: 0.00132280
Iteration 7/25 | Loss: 0.00132280
Iteration 8/25 | Loss: 0.00132280
Iteration 9/25 | Loss: 0.00132280
Iteration 10/25 | Loss: 0.00132280
Iteration 11/25 | Loss: 0.00132280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013228025054559112, 0.0013228025054559112, 0.0013228025054559112, 0.0013228025054559112, 0.0013228025054559112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013228025054559112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19082212
Iteration 2/25 | Loss: 0.00102434
Iteration 3/25 | Loss: 0.00102431
Iteration 4/25 | Loss: 0.00102430
Iteration 5/25 | Loss: 0.00102430
Iteration 6/25 | Loss: 0.00102430
Iteration 7/25 | Loss: 0.00102430
Iteration 8/25 | Loss: 0.00102430
Iteration 9/25 | Loss: 0.00102430
Iteration 10/25 | Loss: 0.00102430
Iteration 11/25 | Loss: 0.00102430
Iteration 12/25 | Loss: 0.00102430
Iteration 13/25 | Loss: 0.00102430
Iteration 14/25 | Loss: 0.00102430
Iteration 15/25 | Loss: 0.00102430
Iteration 16/25 | Loss: 0.00102430
Iteration 17/25 | Loss: 0.00102430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010243019787594676, 0.0010243019787594676, 0.0010243019787594676, 0.0010243019787594676, 0.0010243019787594676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010243019787594676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102430
Iteration 2/1000 | Loss: 0.00005684
Iteration 3/1000 | Loss: 0.00003645
Iteration 4/1000 | Loss: 0.00002772
Iteration 5/1000 | Loss: 0.00002528
Iteration 6/1000 | Loss: 0.00002404
Iteration 7/1000 | Loss: 0.00002342
Iteration 8/1000 | Loss: 0.00002283
Iteration 9/1000 | Loss: 0.00002243
Iteration 10/1000 | Loss: 0.00002208
Iteration 11/1000 | Loss: 0.00002177
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002117
Iteration 14/1000 | Loss: 0.00002094
Iteration 15/1000 | Loss: 0.00002072
Iteration 16/1000 | Loss: 0.00002053
Iteration 17/1000 | Loss: 0.00002038
Iteration 18/1000 | Loss: 0.00002035
Iteration 19/1000 | Loss: 0.00002029
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002024
Iteration 22/1000 | Loss: 0.00002019
Iteration 23/1000 | Loss: 0.00002016
Iteration 24/1000 | Loss: 0.00002008
Iteration 25/1000 | Loss: 0.00002007
Iteration 26/1000 | Loss: 0.00002003
Iteration 27/1000 | Loss: 0.00002003
Iteration 28/1000 | Loss: 0.00002003
Iteration 29/1000 | Loss: 0.00002003
Iteration 30/1000 | Loss: 0.00002003
Iteration 31/1000 | Loss: 0.00002003
Iteration 32/1000 | Loss: 0.00002003
Iteration 33/1000 | Loss: 0.00002003
Iteration 34/1000 | Loss: 0.00002003
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00002001
Iteration 40/1000 | Loss: 0.00002000
Iteration 41/1000 | Loss: 0.00001999
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001998
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001995
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001994
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001993
Iteration 64/1000 | Loss: 0.00001993
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001993
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001992
Iteration 73/1000 | Loss: 0.00001992
Iteration 74/1000 | Loss: 0.00001992
Iteration 75/1000 | Loss: 0.00001992
Iteration 76/1000 | Loss: 0.00001992
Iteration 77/1000 | Loss: 0.00001992
Iteration 78/1000 | Loss: 0.00001992
Iteration 79/1000 | Loss: 0.00001991
Iteration 80/1000 | Loss: 0.00001991
Iteration 81/1000 | Loss: 0.00001991
Iteration 82/1000 | Loss: 0.00001991
Iteration 83/1000 | Loss: 0.00001991
Iteration 84/1000 | Loss: 0.00001991
Iteration 85/1000 | Loss: 0.00001991
Iteration 86/1000 | Loss: 0.00001989
Iteration 87/1000 | Loss: 0.00001989
Iteration 88/1000 | Loss: 0.00001989
Iteration 89/1000 | Loss: 0.00001989
Iteration 90/1000 | Loss: 0.00001989
Iteration 91/1000 | Loss: 0.00001989
Iteration 92/1000 | Loss: 0.00001989
Iteration 93/1000 | Loss: 0.00001989
Iteration 94/1000 | Loss: 0.00001989
Iteration 95/1000 | Loss: 0.00001989
Iteration 96/1000 | Loss: 0.00001989
Iteration 97/1000 | Loss: 0.00001989
Iteration 98/1000 | Loss: 0.00001988
Iteration 99/1000 | Loss: 0.00001988
Iteration 100/1000 | Loss: 0.00001988
Iteration 101/1000 | Loss: 0.00001988
Iteration 102/1000 | Loss: 0.00001988
Iteration 103/1000 | Loss: 0.00001987
Iteration 104/1000 | Loss: 0.00001987
Iteration 105/1000 | Loss: 0.00001987
Iteration 106/1000 | Loss: 0.00001987
Iteration 107/1000 | Loss: 0.00001987
Iteration 108/1000 | Loss: 0.00001987
Iteration 109/1000 | Loss: 0.00001986
Iteration 110/1000 | Loss: 0.00001986
Iteration 111/1000 | Loss: 0.00001986
Iteration 112/1000 | Loss: 0.00001986
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001985
Iteration 115/1000 | Loss: 0.00001985
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001985
Iteration 120/1000 | Loss: 0.00001985
Iteration 121/1000 | Loss: 0.00001985
Iteration 122/1000 | Loss: 0.00001985
Iteration 123/1000 | Loss: 0.00001985
Iteration 124/1000 | Loss: 0.00001985
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001984
Iteration 128/1000 | Loss: 0.00001984
Iteration 129/1000 | Loss: 0.00001984
Iteration 130/1000 | Loss: 0.00001984
Iteration 131/1000 | Loss: 0.00001984
Iteration 132/1000 | Loss: 0.00001984
Iteration 133/1000 | Loss: 0.00001983
Iteration 134/1000 | Loss: 0.00001983
Iteration 135/1000 | Loss: 0.00001983
Iteration 136/1000 | Loss: 0.00001983
Iteration 137/1000 | Loss: 0.00001983
Iteration 138/1000 | Loss: 0.00001982
Iteration 139/1000 | Loss: 0.00001982
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001982
Iteration 142/1000 | Loss: 0.00001982
Iteration 143/1000 | Loss: 0.00001982
Iteration 144/1000 | Loss: 0.00001982
Iteration 145/1000 | Loss: 0.00001982
Iteration 146/1000 | Loss: 0.00001982
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001981
Iteration 149/1000 | Loss: 0.00001981
Iteration 150/1000 | Loss: 0.00001981
Iteration 151/1000 | Loss: 0.00001981
Iteration 152/1000 | Loss: 0.00001981
Iteration 153/1000 | Loss: 0.00001981
Iteration 154/1000 | Loss: 0.00001981
Iteration 155/1000 | Loss: 0.00001981
Iteration 156/1000 | Loss: 0.00001981
Iteration 157/1000 | Loss: 0.00001981
Iteration 158/1000 | Loss: 0.00001981
Iteration 159/1000 | Loss: 0.00001981
Iteration 160/1000 | Loss: 0.00001981
Iteration 161/1000 | Loss: 0.00001980
Iteration 162/1000 | Loss: 0.00001980
Iteration 163/1000 | Loss: 0.00001980
Iteration 164/1000 | Loss: 0.00001980
Iteration 165/1000 | Loss: 0.00001980
Iteration 166/1000 | Loss: 0.00001980
Iteration 167/1000 | Loss: 0.00001980
Iteration 168/1000 | Loss: 0.00001980
Iteration 169/1000 | Loss: 0.00001979
Iteration 170/1000 | Loss: 0.00001979
Iteration 171/1000 | Loss: 0.00001979
Iteration 172/1000 | Loss: 0.00001979
Iteration 173/1000 | Loss: 0.00001979
Iteration 174/1000 | Loss: 0.00001979
Iteration 175/1000 | Loss: 0.00001979
Iteration 176/1000 | Loss: 0.00001979
Iteration 177/1000 | Loss: 0.00001979
Iteration 178/1000 | Loss: 0.00001979
Iteration 179/1000 | Loss: 0.00001979
Iteration 180/1000 | Loss: 0.00001979
Iteration 181/1000 | Loss: 0.00001979
Iteration 182/1000 | Loss: 0.00001979
Iteration 183/1000 | Loss: 0.00001979
Iteration 184/1000 | Loss: 0.00001979
Iteration 185/1000 | Loss: 0.00001979
Iteration 186/1000 | Loss: 0.00001979
Iteration 187/1000 | Loss: 0.00001979
Iteration 188/1000 | Loss: 0.00001979
Iteration 189/1000 | Loss: 0.00001979
Iteration 190/1000 | Loss: 0.00001979
Iteration 191/1000 | Loss: 0.00001979
Iteration 192/1000 | Loss: 0.00001979
Iteration 193/1000 | Loss: 0.00001979
Iteration 194/1000 | Loss: 0.00001979
Iteration 195/1000 | Loss: 0.00001979
Iteration 196/1000 | Loss: 0.00001979
Iteration 197/1000 | Loss: 0.00001979
Iteration 198/1000 | Loss: 0.00001979
Iteration 199/1000 | Loss: 0.00001979
Iteration 200/1000 | Loss: 0.00001979
Iteration 201/1000 | Loss: 0.00001979
Iteration 202/1000 | Loss: 0.00001979
Iteration 203/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.9787657947745174e-05, 1.9787657947745174e-05, 1.9787657947745174e-05, 1.9787657947745174e-05, 1.9787657947745174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9787657947745174e-05

Optimization complete. Final v2v error: 3.5272939205169678 mm

Highest mean error: 5.574662685394287 mm for frame 80

Lowest mean error: 2.809399366378784 mm for frame 130

Saving results

Total time: 50.75500273704529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441085
Iteration 2/25 | Loss: 0.00133734
Iteration 3/25 | Loss: 0.00128638
Iteration 4/25 | Loss: 0.00127610
Iteration 5/25 | Loss: 0.00127269
Iteration 6/25 | Loss: 0.00127269
Iteration 7/25 | Loss: 0.00127269
Iteration 8/25 | Loss: 0.00127269
Iteration 9/25 | Loss: 0.00127269
Iteration 10/25 | Loss: 0.00127269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012726896675303578, 0.0012726896675303578, 0.0012726896675303578, 0.0012726896675303578, 0.0012726896675303578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012726896675303578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34397852
Iteration 2/25 | Loss: 0.00127031
Iteration 3/25 | Loss: 0.00127031
Iteration 4/25 | Loss: 0.00127031
Iteration 5/25 | Loss: 0.00127031
Iteration 6/25 | Loss: 0.00127031
Iteration 7/25 | Loss: 0.00127031
Iteration 8/25 | Loss: 0.00127031
Iteration 9/25 | Loss: 0.00127031
Iteration 10/25 | Loss: 0.00127031
Iteration 11/25 | Loss: 0.00127031
Iteration 12/25 | Loss: 0.00127031
Iteration 13/25 | Loss: 0.00127031
Iteration 14/25 | Loss: 0.00127031
Iteration 15/25 | Loss: 0.00127031
Iteration 16/25 | Loss: 0.00127031
Iteration 17/25 | Loss: 0.00127031
Iteration 18/25 | Loss: 0.00127031
Iteration 19/25 | Loss: 0.00127031
Iteration 20/25 | Loss: 0.00127031
Iteration 21/25 | Loss: 0.00127031
Iteration 22/25 | Loss: 0.00127031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012703054817393422, 0.0012703054817393422, 0.0012703054817393422, 0.0012703054817393422, 0.0012703054817393422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012703054817393422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127031
Iteration 2/1000 | Loss: 0.00002454
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001457
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001329
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001301
Iteration 17/1000 | Loss: 0.00001300
Iteration 18/1000 | Loss: 0.00001297
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001289
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001269
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001268
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001264
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001255
Iteration 46/1000 | Loss: 0.00001255
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001251
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001249
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001242
Iteration 85/1000 | Loss: 0.00001242
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001237
Iteration 99/1000 | Loss: 0.00001237
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001237
Iteration 103/1000 | Loss: 0.00001236
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001231
Iteration 126/1000 | Loss: 0.00001231
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001230
Iteration 135/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.230127236340195e-05, 1.230127236340195e-05, 1.230127236340195e-05, 1.230127236340195e-05, 1.230127236340195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.230127236340195e-05

Optimization complete. Final v2v error: 3.0147433280944824 mm

Highest mean error: 3.37693452835083 mm for frame 47

Lowest mean error: 2.7990574836730957 mm for frame 5

Saving results

Total time: 42.39936661720276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556019
Iteration 2/25 | Loss: 0.00168319
Iteration 3/25 | Loss: 0.00142731
Iteration 4/25 | Loss: 0.00141828
Iteration 5/25 | Loss: 0.00141642
Iteration 6/25 | Loss: 0.00141642
Iteration 7/25 | Loss: 0.00141642
Iteration 8/25 | Loss: 0.00141642
Iteration 9/25 | Loss: 0.00141642
Iteration 10/25 | Loss: 0.00141642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014164169551804662, 0.0014164169551804662, 0.0014164169551804662, 0.0014164169551804662, 0.0014164169551804662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014164169551804662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99342215
Iteration 2/25 | Loss: 0.00115448
Iteration 3/25 | Loss: 0.00115446
Iteration 4/25 | Loss: 0.00115446
Iteration 5/25 | Loss: 0.00115446
Iteration 6/25 | Loss: 0.00115446
Iteration 7/25 | Loss: 0.00115446
Iteration 8/25 | Loss: 0.00115446
Iteration 9/25 | Loss: 0.00115446
Iteration 10/25 | Loss: 0.00115446
Iteration 11/25 | Loss: 0.00115446
Iteration 12/25 | Loss: 0.00115446
Iteration 13/25 | Loss: 0.00115446
Iteration 14/25 | Loss: 0.00115446
Iteration 15/25 | Loss: 0.00115446
Iteration 16/25 | Loss: 0.00115446
Iteration 17/25 | Loss: 0.00115446
Iteration 18/25 | Loss: 0.00115446
Iteration 19/25 | Loss: 0.00115446
Iteration 20/25 | Loss: 0.00115446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00115445745177567, 0.00115445745177567, 0.00115445745177567, 0.00115445745177567, 0.00115445745177567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00115445745177567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115446
Iteration 2/1000 | Loss: 0.00004574
Iteration 3/1000 | Loss: 0.00003097
Iteration 4/1000 | Loss: 0.00002760
Iteration 5/1000 | Loss: 0.00002639
Iteration 6/1000 | Loss: 0.00002545
Iteration 7/1000 | Loss: 0.00002487
Iteration 8/1000 | Loss: 0.00002431
Iteration 9/1000 | Loss: 0.00002396
Iteration 10/1000 | Loss: 0.00002360
Iteration 11/1000 | Loss: 0.00002331
Iteration 12/1000 | Loss: 0.00002304
Iteration 13/1000 | Loss: 0.00002281
Iteration 14/1000 | Loss: 0.00002260
Iteration 15/1000 | Loss: 0.00002244
Iteration 16/1000 | Loss: 0.00002230
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00002210
Iteration 19/1000 | Loss: 0.00002210
Iteration 20/1000 | Loss: 0.00002207
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002203
Iteration 25/1000 | Loss: 0.00002203
Iteration 26/1000 | Loss: 0.00002200
Iteration 27/1000 | Loss: 0.00002199
Iteration 28/1000 | Loss: 0.00002198
Iteration 29/1000 | Loss: 0.00002198
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002196
Iteration 32/1000 | Loss: 0.00002195
Iteration 33/1000 | Loss: 0.00002191
Iteration 34/1000 | Loss: 0.00002186
Iteration 35/1000 | Loss: 0.00002186
Iteration 36/1000 | Loss: 0.00002186
Iteration 37/1000 | Loss: 0.00002186
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002185
Iteration 40/1000 | Loss: 0.00002185
Iteration 41/1000 | Loss: 0.00002184
Iteration 42/1000 | Loss: 0.00002183
Iteration 43/1000 | Loss: 0.00002183
Iteration 44/1000 | Loss: 0.00002183
Iteration 45/1000 | Loss: 0.00002183
Iteration 46/1000 | Loss: 0.00002183
Iteration 47/1000 | Loss: 0.00002183
Iteration 48/1000 | Loss: 0.00002183
Iteration 49/1000 | Loss: 0.00002183
Iteration 50/1000 | Loss: 0.00002183
Iteration 51/1000 | Loss: 0.00002183
Iteration 52/1000 | Loss: 0.00002183
Iteration 53/1000 | Loss: 0.00002182
Iteration 54/1000 | Loss: 0.00002182
Iteration 55/1000 | Loss: 0.00002182
Iteration 56/1000 | Loss: 0.00002182
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002180
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002179
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002176
Iteration 65/1000 | Loss: 0.00002176
Iteration 66/1000 | Loss: 0.00002176
Iteration 67/1000 | Loss: 0.00002176
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002174
Iteration 72/1000 | Loss: 0.00002174
Iteration 73/1000 | Loss: 0.00002174
Iteration 74/1000 | Loss: 0.00002174
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002173
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002172
Iteration 87/1000 | Loss: 0.00002172
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002172
Iteration 90/1000 | Loss: 0.00002172
Iteration 91/1000 | Loss: 0.00002172
Iteration 92/1000 | Loss: 0.00002172
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002171
Iteration 95/1000 | Loss: 0.00002171
Iteration 96/1000 | Loss: 0.00002171
Iteration 97/1000 | Loss: 0.00002171
Iteration 98/1000 | Loss: 0.00002171
Iteration 99/1000 | Loss: 0.00002171
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002171
Iteration 104/1000 | Loss: 0.00002171
Iteration 105/1000 | Loss: 0.00002171
Iteration 106/1000 | Loss: 0.00002171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.1710447981604375e-05, 2.1710447981604375e-05, 2.1710447981604375e-05, 2.1710447981604375e-05, 2.1710447981604375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1710447981604375e-05

Optimization complete. Final v2v error: 3.762791633605957 mm

Highest mean error: 4.750159740447998 mm for frame 59

Lowest mean error: 2.979804515838623 mm for frame 137

Saving results

Total time: 42.28078866004944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030809
Iteration 2/25 | Loss: 0.00260638
Iteration 3/25 | Loss: 0.00194415
Iteration 4/25 | Loss: 0.00170209
Iteration 5/25 | Loss: 0.00161871
Iteration 6/25 | Loss: 0.00163119
Iteration 7/25 | Loss: 0.00146964
Iteration 8/25 | Loss: 0.00135124
Iteration 9/25 | Loss: 0.00134453
Iteration 10/25 | Loss: 0.00128844
Iteration 11/25 | Loss: 0.00128725
Iteration 12/25 | Loss: 0.00126260
Iteration 13/25 | Loss: 0.00126065
Iteration 14/25 | Loss: 0.00125876
Iteration 15/25 | Loss: 0.00126313
Iteration 16/25 | Loss: 0.00126849
Iteration 17/25 | Loss: 0.00126227
Iteration 18/25 | Loss: 0.00126210
Iteration 19/25 | Loss: 0.00125945
Iteration 20/25 | Loss: 0.00126147
Iteration 21/25 | Loss: 0.00125839
Iteration 22/25 | Loss: 0.00125839
Iteration 23/25 | Loss: 0.00125838
Iteration 24/25 | Loss: 0.00125838
Iteration 25/25 | Loss: 0.00125838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.70132017
Iteration 2/25 | Loss: 0.00122879
Iteration 3/25 | Loss: 0.00122879
Iteration 4/25 | Loss: 0.00109059
Iteration 5/25 | Loss: 0.00109059
Iteration 6/25 | Loss: 0.00109058
Iteration 7/25 | Loss: 0.00109058
Iteration 8/25 | Loss: 0.00109058
Iteration 9/25 | Loss: 0.00109058
Iteration 10/25 | Loss: 0.00109058
Iteration 11/25 | Loss: 0.00109058
Iteration 12/25 | Loss: 0.00109058
Iteration 13/25 | Loss: 0.00109058
Iteration 14/25 | Loss: 0.00109058
Iteration 15/25 | Loss: 0.00109058
Iteration 16/25 | Loss: 0.00109058
Iteration 17/25 | Loss: 0.00109058
Iteration 18/25 | Loss: 0.00109058
Iteration 19/25 | Loss: 0.00109058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010905825765803456, 0.0010905825765803456, 0.0010905825765803456, 0.0010905825765803456, 0.0010905825765803456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010905825765803456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109058
Iteration 2/1000 | Loss: 0.00013297
Iteration 3/1000 | Loss: 0.00007212
Iteration 4/1000 | Loss: 0.00030800
Iteration 5/1000 | Loss: 0.00229835
Iteration 6/1000 | Loss: 0.00176345
Iteration 7/1000 | Loss: 0.00115849
Iteration 8/1000 | Loss: 0.00275220
Iteration 9/1000 | Loss: 0.00099668
Iteration 10/1000 | Loss: 0.00280900
Iteration 11/1000 | Loss: 0.00101521
Iteration 12/1000 | Loss: 0.00261913
Iteration 13/1000 | Loss: 0.00097041
Iteration 14/1000 | Loss: 0.00235500
Iteration 15/1000 | Loss: 0.00092666
Iteration 16/1000 | Loss: 0.00006150
Iteration 17/1000 | Loss: 0.00005476
Iteration 18/1000 | Loss: 0.00003792
Iteration 19/1000 | Loss: 0.00004276
Iteration 20/1000 | Loss: 0.00020978
Iteration 21/1000 | Loss: 0.00005796
Iteration 22/1000 | Loss: 0.00002759
Iteration 23/1000 | Loss: 0.00002343
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00004833
Iteration 26/1000 | Loss: 0.00010388
Iteration 27/1000 | Loss: 0.00002382
Iteration 28/1000 | Loss: 0.00002355
Iteration 29/1000 | Loss: 0.00002820
Iteration 30/1000 | Loss: 0.00004876
Iteration 31/1000 | Loss: 0.00003711
Iteration 32/1000 | Loss: 0.00002740
Iteration 33/1000 | Loss: 0.00002289
Iteration 34/1000 | Loss: 0.00002550
Iteration 35/1000 | Loss: 0.00039467
Iteration 36/1000 | Loss: 0.00091865
Iteration 37/1000 | Loss: 0.00014325
Iteration 38/1000 | Loss: 0.00014658
Iteration 39/1000 | Loss: 0.00002006
Iteration 40/1000 | Loss: 0.00005626
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00016508
Iteration 43/1000 | Loss: 0.00001999
Iteration 44/1000 | Loss: 0.00005128
Iteration 45/1000 | Loss: 0.00001821
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00001473
Iteration 48/1000 | Loss: 0.00003666
Iteration 49/1000 | Loss: 0.00003805
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00005416
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001360
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00003644
Iteration 59/1000 | Loss: 0.00007211
Iteration 60/1000 | Loss: 0.00003385
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001327
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001326
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001326
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001324
Iteration 83/1000 | Loss: 0.00001324
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001323
Iteration 88/1000 | Loss: 0.00003374
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00002888
Iteration 91/1000 | Loss: 0.00003233
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001321
Iteration 94/1000 | Loss: 0.00001320
Iteration 95/1000 | Loss: 0.00001320
Iteration 96/1000 | Loss: 0.00001320
Iteration 97/1000 | Loss: 0.00001320
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001319
Iteration 100/1000 | Loss: 0.00001319
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001318
Iteration 104/1000 | Loss: 0.00001318
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001318
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001317
Iteration 117/1000 | Loss: 0.00001317
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001317
Iteration 121/1000 | Loss: 0.00001317
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001316
Iteration 126/1000 | Loss: 0.00001316
Iteration 127/1000 | Loss: 0.00001316
Iteration 128/1000 | Loss: 0.00001316
Iteration 129/1000 | Loss: 0.00001316
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001316
Iteration 144/1000 | Loss: 0.00001316
Iteration 145/1000 | Loss: 0.00001315
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001315
Iteration 150/1000 | Loss: 0.00001315
Iteration 151/1000 | Loss: 0.00001315
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001314
Iteration 157/1000 | Loss: 0.00001314
Iteration 158/1000 | Loss: 0.00001314
Iteration 159/1000 | Loss: 0.00001314
Iteration 160/1000 | Loss: 0.00001314
Iteration 161/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3143441719876137e-05, 1.3143441719876137e-05, 1.3143441719876137e-05, 1.3143441719876137e-05, 1.3143441719876137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3143441719876137e-05

Optimization complete. Final v2v error: 3.113344669342041 mm

Highest mean error: 3.618460178375244 mm for frame 132

Lowest mean error: 2.8082945346832275 mm for frame 18

Saving results

Total time: 138.36532425880432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964497
Iteration 2/25 | Loss: 0.00158804
Iteration 3/25 | Loss: 0.00139152
Iteration 4/25 | Loss: 0.00136887
Iteration 5/25 | Loss: 0.00136782
Iteration 6/25 | Loss: 0.00136009
Iteration 7/25 | Loss: 0.00135285
Iteration 8/25 | Loss: 0.00134199
Iteration 9/25 | Loss: 0.00134096
Iteration 10/25 | Loss: 0.00133776
Iteration 11/25 | Loss: 0.00133671
Iteration 12/25 | Loss: 0.00133652
Iteration 13/25 | Loss: 0.00133649
Iteration 14/25 | Loss: 0.00133649
Iteration 15/25 | Loss: 0.00133649
Iteration 16/25 | Loss: 0.00133649
Iteration 17/25 | Loss: 0.00133649
Iteration 18/25 | Loss: 0.00133649
Iteration 19/25 | Loss: 0.00133648
Iteration 20/25 | Loss: 0.00133648
Iteration 21/25 | Loss: 0.00133648
Iteration 22/25 | Loss: 0.00133648
Iteration 23/25 | Loss: 0.00133648
Iteration 24/25 | Loss: 0.00133648
Iteration 25/25 | Loss: 0.00133648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.55501366
Iteration 2/25 | Loss: 0.00158460
Iteration 3/25 | Loss: 0.00158454
Iteration 4/25 | Loss: 0.00158453
Iteration 5/25 | Loss: 0.00158453
Iteration 6/25 | Loss: 0.00158453
Iteration 7/25 | Loss: 0.00158453
Iteration 8/25 | Loss: 0.00158453
Iteration 9/25 | Loss: 0.00158453
Iteration 10/25 | Loss: 0.00158453
Iteration 11/25 | Loss: 0.00158453
Iteration 12/25 | Loss: 0.00158453
Iteration 13/25 | Loss: 0.00158453
Iteration 14/25 | Loss: 0.00158453
Iteration 15/25 | Loss: 0.00158453
Iteration 16/25 | Loss: 0.00158453
Iteration 17/25 | Loss: 0.00158453
Iteration 18/25 | Loss: 0.00158453
Iteration 19/25 | Loss: 0.00158453
Iteration 20/25 | Loss: 0.00158453
Iteration 21/25 | Loss: 0.00158453
Iteration 22/25 | Loss: 0.00158453
Iteration 23/25 | Loss: 0.00158453
Iteration 24/25 | Loss: 0.00158453
Iteration 25/25 | Loss: 0.00158453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158453
Iteration 2/1000 | Loss: 0.00042616
Iteration 3/1000 | Loss: 0.00011737
Iteration 4/1000 | Loss: 0.00012083
Iteration 5/1000 | Loss: 0.00005555
Iteration 6/1000 | Loss: 0.00008057
Iteration 7/1000 | Loss: 0.00003866
Iteration 8/1000 | Loss: 0.00010891
Iteration 9/1000 | Loss: 0.00007808
Iteration 10/1000 | Loss: 0.00008829
Iteration 11/1000 | Loss: 0.00006868
Iteration 12/1000 | Loss: 0.00005745
Iteration 13/1000 | Loss: 0.00005628
Iteration 14/1000 | Loss: 0.00008078
Iteration 15/1000 | Loss: 0.00006101
Iteration 16/1000 | Loss: 0.00005740
Iteration 17/1000 | Loss: 0.00005710
Iteration 18/1000 | Loss: 0.00006825
Iteration 19/1000 | Loss: 0.00004439
Iteration 20/1000 | Loss: 0.00005263
Iteration 21/1000 | Loss: 0.00005854
Iteration 22/1000 | Loss: 0.00005455
Iteration 23/1000 | Loss: 0.00005731
Iteration 24/1000 | Loss: 0.00031826
Iteration 25/1000 | Loss: 0.00009843
Iteration 26/1000 | Loss: 0.00005328
Iteration 27/1000 | Loss: 0.00004951
Iteration 28/1000 | Loss: 0.00006734
Iteration 29/1000 | Loss: 0.00006441
Iteration 30/1000 | Loss: 0.00004312
Iteration 31/1000 | Loss: 0.00003623
Iteration 32/1000 | Loss: 0.00005082
Iteration 33/1000 | Loss: 0.00003130
Iteration 34/1000 | Loss: 0.00004742
Iteration 35/1000 | Loss: 0.00004100
Iteration 36/1000 | Loss: 0.00005364
Iteration 37/1000 | Loss: 0.00002806
Iteration 38/1000 | Loss: 0.00003073
Iteration 39/1000 | Loss: 0.00003343
Iteration 40/1000 | Loss: 0.00003968
Iteration 41/1000 | Loss: 0.00004387
Iteration 42/1000 | Loss: 0.00006116
Iteration 43/1000 | Loss: 0.00004303
Iteration 44/1000 | Loss: 0.00005638
Iteration 45/1000 | Loss: 0.00004239
Iteration 46/1000 | Loss: 0.00003611
Iteration 47/1000 | Loss: 0.00005062
Iteration 48/1000 | Loss: 0.00004994
Iteration 49/1000 | Loss: 0.00006871
Iteration 50/1000 | Loss: 0.00004463
Iteration 51/1000 | Loss: 0.00004202
Iteration 52/1000 | Loss: 0.00004015
Iteration 53/1000 | Loss: 0.00003314
Iteration 54/1000 | Loss: 0.00004403
Iteration 55/1000 | Loss: 0.00004212
Iteration 56/1000 | Loss: 0.00002327
Iteration 57/1000 | Loss: 0.00002092
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001850
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001751
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001699
Iteration 81/1000 | Loss: 0.00001699
Iteration 82/1000 | Loss: 0.00001699
Iteration 83/1000 | Loss: 0.00001699
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001695
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001691
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001689
Iteration 103/1000 | Loss: 0.00001689
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001688
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001684
Iteration 114/1000 | Loss: 0.00001683
Iteration 115/1000 | Loss: 0.00001683
Iteration 116/1000 | Loss: 0.00001682
Iteration 117/1000 | Loss: 0.00001682
Iteration 118/1000 | Loss: 0.00001682
Iteration 119/1000 | Loss: 0.00001681
Iteration 120/1000 | Loss: 0.00001681
Iteration 121/1000 | Loss: 0.00001681
Iteration 122/1000 | Loss: 0.00001681
Iteration 123/1000 | Loss: 0.00001680
Iteration 124/1000 | Loss: 0.00001680
Iteration 125/1000 | Loss: 0.00001680
Iteration 126/1000 | Loss: 0.00001680
Iteration 127/1000 | Loss: 0.00001680
Iteration 128/1000 | Loss: 0.00001680
Iteration 129/1000 | Loss: 0.00001679
Iteration 130/1000 | Loss: 0.00001679
Iteration 131/1000 | Loss: 0.00001679
Iteration 132/1000 | Loss: 0.00001679
Iteration 133/1000 | Loss: 0.00001679
Iteration 134/1000 | Loss: 0.00001679
Iteration 135/1000 | Loss: 0.00001679
Iteration 136/1000 | Loss: 0.00001678
Iteration 137/1000 | Loss: 0.00001678
Iteration 138/1000 | Loss: 0.00001678
Iteration 139/1000 | Loss: 0.00001677
Iteration 140/1000 | Loss: 0.00001677
Iteration 141/1000 | Loss: 0.00001677
Iteration 142/1000 | Loss: 0.00001677
Iteration 143/1000 | Loss: 0.00001676
Iteration 144/1000 | Loss: 0.00001676
Iteration 145/1000 | Loss: 0.00001676
Iteration 146/1000 | Loss: 0.00001676
Iteration 147/1000 | Loss: 0.00001676
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001674
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001674
Iteration 155/1000 | Loss: 0.00001674
Iteration 156/1000 | Loss: 0.00001674
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001674
Iteration 159/1000 | Loss: 0.00001674
Iteration 160/1000 | Loss: 0.00001674
Iteration 161/1000 | Loss: 0.00001673
Iteration 162/1000 | Loss: 0.00001673
Iteration 163/1000 | Loss: 0.00001673
Iteration 164/1000 | Loss: 0.00001673
Iteration 165/1000 | Loss: 0.00001673
Iteration 166/1000 | Loss: 0.00001673
Iteration 167/1000 | Loss: 0.00001673
Iteration 168/1000 | Loss: 0.00001673
Iteration 169/1000 | Loss: 0.00001673
Iteration 170/1000 | Loss: 0.00001672
Iteration 171/1000 | Loss: 0.00001672
Iteration 172/1000 | Loss: 0.00001672
Iteration 173/1000 | Loss: 0.00001672
Iteration 174/1000 | Loss: 0.00001672
Iteration 175/1000 | Loss: 0.00001672
Iteration 176/1000 | Loss: 0.00001672
Iteration 177/1000 | Loss: 0.00001672
Iteration 178/1000 | Loss: 0.00001672
Iteration 179/1000 | Loss: 0.00001671
Iteration 180/1000 | Loss: 0.00001671
Iteration 181/1000 | Loss: 0.00001671
Iteration 182/1000 | Loss: 0.00001671
Iteration 183/1000 | Loss: 0.00001671
Iteration 184/1000 | Loss: 0.00001671
Iteration 185/1000 | Loss: 0.00001671
Iteration 186/1000 | Loss: 0.00001671
Iteration 187/1000 | Loss: 0.00001671
Iteration 188/1000 | Loss: 0.00001671
Iteration 189/1000 | Loss: 0.00001671
Iteration 190/1000 | Loss: 0.00001671
Iteration 191/1000 | Loss: 0.00001671
Iteration 192/1000 | Loss: 0.00001671
Iteration 193/1000 | Loss: 0.00001670
Iteration 194/1000 | Loss: 0.00001670
Iteration 195/1000 | Loss: 0.00001670
Iteration 196/1000 | Loss: 0.00001670
Iteration 197/1000 | Loss: 0.00001670
Iteration 198/1000 | Loss: 0.00001670
Iteration 199/1000 | Loss: 0.00001670
Iteration 200/1000 | Loss: 0.00001670
Iteration 201/1000 | Loss: 0.00001670
Iteration 202/1000 | Loss: 0.00001670
Iteration 203/1000 | Loss: 0.00001670
Iteration 204/1000 | Loss: 0.00001670
Iteration 205/1000 | Loss: 0.00001670
Iteration 206/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.670420533628203e-05, 1.670420533628203e-05, 1.670420533628203e-05, 1.670420533628203e-05, 1.670420533628203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.670420533628203e-05

Optimization complete. Final v2v error: 3.3365888595581055 mm

Highest mean error: 5.813571929931641 mm for frame 41

Lowest mean error: 2.7626073360443115 mm for frame 71

Saving results

Total time: 126.7616982460022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824364
Iteration 2/25 | Loss: 0.00137651
Iteration 3/25 | Loss: 0.00129154
Iteration 4/25 | Loss: 0.00128064
Iteration 5/25 | Loss: 0.00127740
Iteration 6/25 | Loss: 0.00127685
Iteration 7/25 | Loss: 0.00127685
Iteration 8/25 | Loss: 0.00127685
Iteration 9/25 | Loss: 0.00127685
Iteration 10/25 | Loss: 0.00127685
Iteration 11/25 | Loss: 0.00127685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012768538435921073, 0.0012768538435921073, 0.0012768538435921073, 0.0012768538435921073, 0.0012768538435921073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012768538435921073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.21130085
Iteration 2/25 | Loss: 0.00113918
Iteration 3/25 | Loss: 0.00113918
Iteration 4/25 | Loss: 0.00113918
Iteration 5/25 | Loss: 0.00113918
Iteration 6/25 | Loss: 0.00113918
Iteration 7/25 | Loss: 0.00113918
Iteration 8/25 | Loss: 0.00113918
Iteration 9/25 | Loss: 0.00113918
Iteration 10/25 | Loss: 0.00113918
Iteration 11/25 | Loss: 0.00113918
Iteration 12/25 | Loss: 0.00113918
Iteration 13/25 | Loss: 0.00113918
Iteration 14/25 | Loss: 0.00113918
Iteration 15/25 | Loss: 0.00113918
Iteration 16/25 | Loss: 0.00113918
Iteration 17/25 | Loss: 0.00113918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011391759617254138, 0.0011391759617254138, 0.0011391759617254138, 0.0011391759617254138, 0.0011391759617254138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011391759617254138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113918
Iteration 2/1000 | Loss: 0.00002920
Iteration 3/1000 | Loss: 0.00002130
Iteration 4/1000 | Loss: 0.00001807
Iteration 5/1000 | Loss: 0.00001714
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001609
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001482
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001475
Iteration 14/1000 | Loss: 0.00001471
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001444
Iteration 20/1000 | Loss: 0.00001443
Iteration 21/1000 | Loss: 0.00001443
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001439
Iteration 24/1000 | Loss: 0.00001439
Iteration 25/1000 | Loss: 0.00001438
Iteration 26/1000 | Loss: 0.00001438
Iteration 27/1000 | Loss: 0.00001437
Iteration 28/1000 | Loss: 0.00001436
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001435
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001426
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001425
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001424
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001420
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001419
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001418
Iteration 45/1000 | Loss: 0.00001416
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001413
Iteration 49/1000 | Loss: 0.00001413
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001407
Iteration 56/1000 | Loss: 0.00001407
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001398
Iteration 82/1000 | Loss: 0.00001398
Iteration 83/1000 | Loss: 0.00001398
Iteration 84/1000 | Loss: 0.00001397
Iteration 85/1000 | Loss: 0.00001397
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001396
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001396
Iteration 98/1000 | Loss: 0.00001395
Iteration 99/1000 | Loss: 0.00001395
Iteration 100/1000 | Loss: 0.00001395
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001395
Iteration 105/1000 | Loss: 0.00001395
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001394
Iteration 108/1000 | Loss: 0.00001394
Iteration 109/1000 | Loss: 0.00001394
Iteration 110/1000 | Loss: 0.00001394
Iteration 111/1000 | Loss: 0.00001394
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001394
Iteration 116/1000 | Loss: 0.00001393
Iteration 117/1000 | Loss: 0.00001393
Iteration 118/1000 | Loss: 0.00001393
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001393
Iteration 124/1000 | Loss: 0.00001392
Iteration 125/1000 | Loss: 0.00001392
Iteration 126/1000 | Loss: 0.00001392
Iteration 127/1000 | Loss: 0.00001392
Iteration 128/1000 | Loss: 0.00001392
Iteration 129/1000 | Loss: 0.00001392
Iteration 130/1000 | Loss: 0.00001392
Iteration 131/1000 | Loss: 0.00001392
Iteration 132/1000 | Loss: 0.00001392
Iteration 133/1000 | Loss: 0.00001392
Iteration 134/1000 | Loss: 0.00001392
Iteration 135/1000 | Loss: 0.00001392
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001391
Iteration 139/1000 | Loss: 0.00001391
Iteration 140/1000 | Loss: 0.00001391
Iteration 141/1000 | Loss: 0.00001391
Iteration 142/1000 | Loss: 0.00001391
Iteration 143/1000 | Loss: 0.00001391
Iteration 144/1000 | Loss: 0.00001391
Iteration 145/1000 | Loss: 0.00001391
Iteration 146/1000 | Loss: 0.00001391
Iteration 147/1000 | Loss: 0.00001391
Iteration 148/1000 | Loss: 0.00001390
Iteration 149/1000 | Loss: 0.00001390
Iteration 150/1000 | Loss: 0.00001390
Iteration 151/1000 | Loss: 0.00001390
Iteration 152/1000 | Loss: 0.00001390
Iteration 153/1000 | Loss: 0.00001390
Iteration 154/1000 | Loss: 0.00001390
Iteration 155/1000 | Loss: 0.00001390
Iteration 156/1000 | Loss: 0.00001390
Iteration 157/1000 | Loss: 0.00001390
Iteration 158/1000 | Loss: 0.00001390
Iteration 159/1000 | Loss: 0.00001390
Iteration 160/1000 | Loss: 0.00001390
Iteration 161/1000 | Loss: 0.00001390
Iteration 162/1000 | Loss: 0.00001390
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001389
Iteration 166/1000 | Loss: 0.00001389
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001389
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001389
Iteration 172/1000 | Loss: 0.00001389
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001389
Iteration 177/1000 | Loss: 0.00001388
Iteration 178/1000 | Loss: 0.00001388
Iteration 179/1000 | Loss: 0.00001388
Iteration 180/1000 | Loss: 0.00001388
Iteration 181/1000 | Loss: 0.00001388
Iteration 182/1000 | Loss: 0.00001388
Iteration 183/1000 | Loss: 0.00001388
Iteration 184/1000 | Loss: 0.00001388
Iteration 185/1000 | Loss: 0.00001388
Iteration 186/1000 | Loss: 0.00001388
Iteration 187/1000 | Loss: 0.00001388
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001388
Iteration 192/1000 | Loss: 0.00001388
Iteration 193/1000 | Loss: 0.00001388
Iteration 194/1000 | Loss: 0.00001388
Iteration 195/1000 | Loss: 0.00001388
Iteration 196/1000 | Loss: 0.00001388
Iteration 197/1000 | Loss: 0.00001388
Iteration 198/1000 | Loss: 0.00001388
Iteration 199/1000 | Loss: 0.00001388
Iteration 200/1000 | Loss: 0.00001388
Iteration 201/1000 | Loss: 0.00001388
Iteration 202/1000 | Loss: 0.00001388
Iteration 203/1000 | Loss: 0.00001388
Iteration 204/1000 | Loss: 0.00001388
Iteration 205/1000 | Loss: 0.00001388
Iteration 206/1000 | Loss: 0.00001388
Iteration 207/1000 | Loss: 0.00001388
Iteration 208/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.3879542166250758e-05, 1.3879542166250758e-05, 1.3879542166250758e-05, 1.3879542166250758e-05, 1.3879542166250758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3879542166250758e-05

Optimization complete. Final v2v error: 3.149860143661499 mm

Highest mean error: 3.8691208362579346 mm for frame 90

Lowest mean error: 2.869203567504883 mm for frame 13

Saving results

Total time: 42.74084401130676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592120
Iteration 2/25 | Loss: 0.00164304
Iteration 3/25 | Loss: 0.00137337
Iteration 4/25 | Loss: 0.00134846
Iteration 5/25 | Loss: 0.00134457
Iteration 6/25 | Loss: 0.00134348
Iteration 7/25 | Loss: 0.00134327
Iteration 8/25 | Loss: 0.00134327
Iteration 9/25 | Loss: 0.00134327
Iteration 10/25 | Loss: 0.00134327
Iteration 11/25 | Loss: 0.00134327
Iteration 12/25 | Loss: 0.00134327
Iteration 13/25 | Loss: 0.00134327
Iteration 14/25 | Loss: 0.00134327
Iteration 15/25 | Loss: 0.00134327
Iteration 16/25 | Loss: 0.00134327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001343272509984672, 0.001343272509984672, 0.001343272509984672, 0.001343272509984672, 0.001343272509984672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001343272509984672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17147374
Iteration 2/25 | Loss: 0.00091917
Iteration 3/25 | Loss: 0.00091917
Iteration 4/25 | Loss: 0.00091916
Iteration 5/25 | Loss: 0.00091916
Iteration 6/25 | Loss: 0.00091916
Iteration 7/25 | Loss: 0.00091916
Iteration 8/25 | Loss: 0.00091916
Iteration 9/25 | Loss: 0.00091916
Iteration 10/25 | Loss: 0.00091916
Iteration 11/25 | Loss: 0.00091916
Iteration 12/25 | Loss: 0.00091916
Iteration 13/25 | Loss: 0.00091916
Iteration 14/25 | Loss: 0.00091916
Iteration 15/25 | Loss: 0.00091916
Iteration 16/25 | Loss: 0.00091916
Iteration 17/25 | Loss: 0.00091916
Iteration 18/25 | Loss: 0.00091916
Iteration 19/25 | Loss: 0.00091916
Iteration 20/25 | Loss: 0.00091916
Iteration 21/25 | Loss: 0.00091916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009191617136821151, 0.0009191617136821151, 0.0009191617136821151, 0.0009191617136821151, 0.0009191617136821151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009191617136821151

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091916
Iteration 2/1000 | Loss: 0.00004679
Iteration 3/1000 | Loss: 0.00003002
Iteration 4/1000 | Loss: 0.00002478
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002217
Iteration 7/1000 | Loss: 0.00002154
Iteration 8/1000 | Loss: 0.00002103
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00002029
Iteration 11/1000 | Loss: 0.00002005
Iteration 12/1000 | Loss: 0.00001989
Iteration 13/1000 | Loss: 0.00001988
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001986
Iteration 16/1000 | Loss: 0.00001983
Iteration 17/1000 | Loss: 0.00001982
Iteration 18/1000 | Loss: 0.00001981
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001976
Iteration 21/1000 | Loss: 0.00001966
Iteration 22/1000 | Loss: 0.00001958
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001954
Iteration 25/1000 | Loss: 0.00001954
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001953
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001952
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001943
Iteration 38/1000 | Loss: 0.00001943
Iteration 39/1000 | Loss: 0.00001942
Iteration 40/1000 | Loss: 0.00001942
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001940
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001939
Iteration 47/1000 | Loss: 0.00001939
Iteration 48/1000 | Loss: 0.00001939
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001939
Iteration 52/1000 | Loss: 0.00001939
Iteration 53/1000 | Loss: 0.00001939
Iteration 54/1000 | Loss: 0.00001939
Iteration 55/1000 | Loss: 0.00001939
Iteration 56/1000 | Loss: 0.00001938
Iteration 57/1000 | Loss: 0.00001938
Iteration 58/1000 | Loss: 0.00001937
Iteration 59/1000 | Loss: 0.00001937
Iteration 60/1000 | Loss: 0.00001937
Iteration 61/1000 | Loss: 0.00001937
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001936
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001935
Iteration 70/1000 | Loss: 0.00001935
Iteration 71/1000 | Loss: 0.00001935
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001933
Iteration 81/1000 | Loss: 0.00001933
Iteration 82/1000 | Loss: 0.00001933
Iteration 83/1000 | Loss: 0.00001932
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001931
Iteration 91/1000 | Loss: 0.00001931
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001930
Iteration 95/1000 | Loss: 0.00001930
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001930
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001929
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001929
Iteration 102/1000 | Loss: 0.00001929
Iteration 103/1000 | Loss: 0.00001929
Iteration 104/1000 | Loss: 0.00001929
Iteration 105/1000 | Loss: 0.00001929
Iteration 106/1000 | Loss: 0.00001929
Iteration 107/1000 | Loss: 0.00001929
Iteration 108/1000 | Loss: 0.00001929
Iteration 109/1000 | Loss: 0.00001929
Iteration 110/1000 | Loss: 0.00001929
Iteration 111/1000 | Loss: 0.00001929
Iteration 112/1000 | Loss: 0.00001929
Iteration 113/1000 | Loss: 0.00001928
Iteration 114/1000 | Loss: 0.00001928
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001928
Iteration 119/1000 | Loss: 0.00001928
Iteration 120/1000 | Loss: 0.00001928
Iteration 121/1000 | Loss: 0.00001928
Iteration 122/1000 | Loss: 0.00001928
Iteration 123/1000 | Loss: 0.00001928
Iteration 124/1000 | Loss: 0.00001928
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001927
Iteration 127/1000 | Loss: 0.00001927
Iteration 128/1000 | Loss: 0.00001927
Iteration 129/1000 | Loss: 0.00001927
Iteration 130/1000 | Loss: 0.00001927
Iteration 131/1000 | Loss: 0.00001927
Iteration 132/1000 | Loss: 0.00001927
Iteration 133/1000 | Loss: 0.00001926
Iteration 134/1000 | Loss: 0.00001926
Iteration 135/1000 | Loss: 0.00001926
Iteration 136/1000 | Loss: 0.00001926
Iteration 137/1000 | Loss: 0.00001926
Iteration 138/1000 | Loss: 0.00001925
Iteration 139/1000 | Loss: 0.00001925
Iteration 140/1000 | Loss: 0.00001925
Iteration 141/1000 | Loss: 0.00001925
Iteration 142/1000 | Loss: 0.00001925
Iteration 143/1000 | Loss: 0.00001925
Iteration 144/1000 | Loss: 0.00001924
Iteration 145/1000 | Loss: 0.00001924
Iteration 146/1000 | Loss: 0.00001924
Iteration 147/1000 | Loss: 0.00001924
Iteration 148/1000 | Loss: 0.00001924
Iteration 149/1000 | Loss: 0.00001924
Iteration 150/1000 | Loss: 0.00001924
Iteration 151/1000 | Loss: 0.00001924
Iteration 152/1000 | Loss: 0.00001923
Iteration 153/1000 | Loss: 0.00001923
Iteration 154/1000 | Loss: 0.00001923
Iteration 155/1000 | Loss: 0.00001923
Iteration 156/1000 | Loss: 0.00001923
Iteration 157/1000 | Loss: 0.00001923
Iteration 158/1000 | Loss: 0.00001923
Iteration 159/1000 | Loss: 0.00001923
Iteration 160/1000 | Loss: 0.00001923
Iteration 161/1000 | Loss: 0.00001923
Iteration 162/1000 | Loss: 0.00001923
Iteration 163/1000 | Loss: 0.00001923
Iteration 164/1000 | Loss: 0.00001923
Iteration 165/1000 | Loss: 0.00001923
Iteration 166/1000 | Loss: 0.00001923
Iteration 167/1000 | Loss: 0.00001923
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001923
Iteration 173/1000 | Loss: 0.00001923
Iteration 174/1000 | Loss: 0.00001923
Iteration 175/1000 | Loss: 0.00001923
Iteration 176/1000 | Loss: 0.00001923
Iteration 177/1000 | Loss: 0.00001923
Iteration 178/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.9226836229790933e-05, 1.9226836229790933e-05, 1.9226836229790933e-05, 1.9226836229790933e-05, 1.9226836229790933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9226836229790933e-05

Optimization complete. Final v2v error: 3.621107816696167 mm

Highest mean error: 5.31098747253418 mm for frame 58

Lowest mean error: 3.186504602432251 mm for frame 134

Saving results

Total time: 42.22246813774109
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736777
Iteration 2/25 | Loss: 0.00161698
Iteration 3/25 | Loss: 0.00136776
Iteration 4/25 | Loss: 0.00134455
Iteration 5/25 | Loss: 0.00133975
Iteration 6/25 | Loss: 0.00134508
Iteration 7/25 | Loss: 0.00131721
Iteration 8/25 | Loss: 0.00131161
Iteration 9/25 | Loss: 0.00131066
Iteration 10/25 | Loss: 0.00131048
Iteration 11/25 | Loss: 0.00131034
Iteration 12/25 | Loss: 0.00131033
Iteration 13/25 | Loss: 0.00131033
Iteration 14/25 | Loss: 0.00131033
Iteration 15/25 | Loss: 0.00131033
Iteration 16/25 | Loss: 0.00131033
Iteration 17/25 | Loss: 0.00131033
Iteration 18/25 | Loss: 0.00131033
Iteration 19/25 | Loss: 0.00131033
Iteration 20/25 | Loss: 0.00131033
Iteration 21/25 | Loss: 0.00131033
Iteration 22/25 | Loss: 0.00131033
Iteration 23/25 | Loss: 0.00131033
Iteration 24/25 | Loss: 0.00131033
Iteration 25/25 | Loss: 0.00131033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.48613834
Iteration 2/25 | Loss: 0.00101299
Iteration 3/25 | Loss: 0.00101285
Iteration 4/25 | Loss: 0.00101285
Iteration 5/25 | Loss: 0.00101285
Iteration 6/25 | Loss: 0.00101285
Iteration 7/25 | Loss: 0.00101285
Iteration 8/25 | Loss: 0.00101285
Iteration 9/25 | Loss: 0.00101285
Iteration 10/25 | Loss: 0.00101285
Iteration 11/25 | Loss: 0.00101285
Iteration 12/25 | Loss: 0.00101285
Iteration 13/25 | Loss: 0.00101285
Iteration 14/25 | Loss: 0.00101285
Iteration 15/25 | Loss: 0.00101285
Iteration 16/25 | Loss: 0.00101285
Iteration 17/25 | Loss: 0.00101285
Iteration 18/25 | Loss: 0.00101285
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010128460125997663, 0.0010128460125997663, 0.0010128460125997663, 0.0010128460125997663, 0.0010128460125997663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010128460125997663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101285
Iteration 2/1000 | Loss: 0.00003131
Iteration 3/1000 | Loss: 0.00002204
Iteration 4/1000 | Loss: 0.00001951
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001751
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001650
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001590
Iteration 11/1000 | Loss: 0.00001570
Iteration 12/1000 | Loss: 0.00001552
Iteration 13/1000 | Loss: 0.00001551
Iteration 14/1000 | Loss: 0.00001550
Iteration 15/1000 | Loss: 0.00001548
Iteration 16/1000 | Loss: 0.00001547
Iteration 17/1000 | Loss: 0.00001546
Iteration 18/1000 | Loss: 0.00001545
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001543
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001537
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001529
Iteration 30/1000 | Loss: 0.00001528
Iteration 31/1000 | Loss: 0.00001528
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001523
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001520
Iteration 40/1000 | Loss: 0.00001520
Iteration 41/1000 | Loss: 0.00001519
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001519
Iteration 44/1000 | Loss: 0.00001519
Iteration 45/1000 | Loss: 0.00001519
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001515
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001512
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001511
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001510
Iteration 79/1000 | Loss: 0.00001510
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001505
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001504
Iteration 90/1000 | Loss: 0.00001504
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001503
Iteration 94/1000 | Loss: 0.00001503
Iteration 95/1000 | Loss: 0.00001503
Iteration 96/1000 | Loss: 0.00001503
Iteration 97/1000 | Loss: 0.00001503
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001502
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001500
Iteration 108/1000 | Loss: 0.00001500
Iteration 109/1000 | Loss: 0.00001500
Iteration 110/1000 | Loss: 0.00001500
Iteration 111/1000 | Loss: 0.00001499
Iteration 112/1000 | Loss: 0.00001499
Iteration 113/1000 | Loss: 0.00001499
Iteration 114/1000 | Loss: 0.00001499
Iteration 115/1000 | Loss: 0.00001499
Iteration 116/1000 | Loss: 0.00001499
Iteration 117/1000 | Loss: 0.00001499
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001498
Iteration 121/1000 | Loss: 0.00001498
Iteration 122/1000 | Loss: 0.00001498
Iteration 123/1000 | Loss: 0.00001498
Iteration 124/1000 | Loss: 0.00001498
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001497
Iteration 130/1000 | Loss: 0.00001497
Iteration 131/1000 | Loss: 0.00001497
Iteration 132/1000 | Loss: 0.00001497
Iteration 133/1000 | Loss: 0.00001497
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001495
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001494
Iteration 142/1000 | Loss: 0.00001494
Iteration 143/1000 | Loss: 0.00001494
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001493
Iteration 146/1000 | Loss: 0.00001493
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001493
Iteration 149/1000 | Loss: 0.00001492
Iteration 150/1000 | Loss: 0.00001492
Iteration 151/1000 | Loss: 0.00001492
Iteration 152/1000 | Loss: 0.00001491
Iteration 153/1000 | Loss: 0.00001491
Iteration 154/1000 | Loss: 0.00001491
Iteration 155/1000 | Loss: 0.00001491
Iteration 156/1000 | Loss: 0.00001491
Iteration 157/1000 | Loss: 0.00001491
Iteration 158/1000 | Loss: 0.00001491
Iteration 159/1000 | Loss: 0.00001490
Iteration 160/1000 | Loss: 0.00001490
Iteration 161/1000 | Loss: 0.00001490
Iteration 162/1000 | Loss: 0.00001490
Iteration 163/1000 | Loss: 0.00001490
Iteration 164/1000 | Loss: 0.00001490
Iteration 165/1000 | Loss: 0.00001490
Iteration 166/1000 | Loss: 0.00001490
Iteration 167/1000 | Loss: 0.00001489
Iteration 168/1000 | Loss: 0.00001489
Iteration 169/1000 | Loss: 0.00001489
Iteration 170/1000 | Loss: 0.00001489
Iteration 171/1000 | Loss: 0.00001489
Iteration 172/1000 | Loss: 0.00001489
Iteration 173/1000 | Loss: 0.00001489
Iteration 174/1000 | Loss: 0.00001489
Iteration 175/1000 | Loss: 0.00001489
Iteration 176/1000 | Loss: 0.00001489
Iteration 177/1000 | Loss: 0.00001489
Iteration 178/1000 | Loss: 0.00001489
Iteration 179/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.4892154467815999e-05, 1.4892154467815999e-05, 1.4892154467815999e-05, 1.4892154467815999e-05, 1.4892154467815999e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4892154467815999e-05

Optimization complete. Final v2v error: 3.299703359603882 mm

Highest mean error: 4.014509201049805 mm for frame 27

Lowest mean error: 2.9068262577056885 mm for frame 10

Saving results

Total time: 58.21991419792175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801935
Iteration 2/25 | Loss: 0.00150667
Iteration 3/25 | Loss: 0.00128460
Iteration 4/25 | Loss: 0.00126823
Iteration 5/25 | Loss: 0.00126640
Iteration 6/25 | Loss: 0.00126587
Iteration 7/25 | Loss: 0.00126587
Iteration 8/25 | Loss: 0.00126587
Iteration 9/25 | Loss: 0.00126587
Iteration 10/25 | Loss: 0.00126587
Iteration 11/25 | Loss: 0.00126587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001265869359485805, 0.001265869359485805, 0.001265869359485805, 0.001265869359485805, 0.001265869359485805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001265869359485805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34989035
Iteration 2/25 | Loss: 0.00103470
Iteration 3/25 | Loss: 0.00103470
Iteration 4/25 | Loss: 0.00103470
Iteration 5/25 | Loss: 0.00103470
Iteration 6/25 | Loss: 0.00103469
Iteration 7/25 | Loss: 0.00103469
Iteration 8/25 | Loss: 0.00103469
Iteration 9/25 | Loss: 0.00103469
Iteration 10/25 | Loss: 0.00103469
Iteration 11/25 | Loss: 0.00103469
Iteration 12/25 | Loss: 0.00103469
Iteration 13/25 | Loss: 0.00103469
Iteration 14/25 | Loss: 0.00103469
Iteration 15/25 | Loss: 0.00103469
Iteration 16/25 | Loss: 0.00103469
Iteration 17/25 | Loss: 0.00103469
Iteration 18/25 | Loss: 0.00103469
Iteration 19/25 | Loss: 0.00103469
Iteration 20/25 | Loss: 0.00103469
Iteration 21/25 | Loss: 0.00103469
Iteration 22/25 | Loss: 0.00103469
Iteration 23/25 | Loss: 0.00103469
Iteration 24/25 | Loss: 0.00103469
Iteration 25/25 | Loss: 0.00103469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103469
Iteration 2/1000 | Loss: 0.00002591
Iteration 3/1000 | Loss: 0.00001844
Iteration 4/1000 | Loss: 0.00001608
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001416
Iteration 7/1000 | Loss: 0.00001359
Iteration 8/1000 | Loss: 0.00001320
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001268
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001249
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001234
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001233
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001232
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001231
Iteration 24/1000 | Loss: 0.00001231
Iteration 25/1000 | Loss: 0.00001231
Iteration 26/1000 | Loss: 0.00001230
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001226
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001225
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001215
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001215
Iteration 51/1000 | Loss: 0.00001215
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001215
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001214
Iteration 62/1000 | Loss: 0.00001214
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.2142978448537178e-05, 1.2142978448537178e-05, 1.2142978448537178e-05, 1.2142978448537178e-05, 1.2142978448537178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2142978448537178e-05

Optimization complete. Final v2v error: 3.0037333965301514 mm

Highest mean error: 3.298114776611328 mm for frame 58

Lowest mean error: 2.8614203929901123 mm for frame 2

Saving results

Total time: 30.713014364242554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567179
Iteration 2/25 | Loss: 0.00135751
Iteration 3/25 | Loss: 0.00128038
Iteration 4/25 | Loss: 0.00126231
Iteration 5/25 | Loss: 0.00125666
Iteration 6/25 | Loss: 0.00125532
Iteration 7/25 | Loss: 0.00125532
Iteration 8/25 | Loss: 0.00125532
Iteration 9/25 | Loss: 0.00125532
Iteration 10/25 | Loss: 0.00125532
Iteration 11/25 | Loss: 0.00125532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012553201522678137, 0.0012553201522678137, 0.0012553201522678137, 0.0012553201522678137, 0.0012553201522678137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012553201522678137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99193746
Iteration 2/25 | Loss: 0.00070953
Iteration 3/25 | Loss: 0.00070953
Iteration 4/25 | Loss: 0.00070953
Iteration 5/25 | Loss: 0.00070953
Iteration 6/25 | Loss: 0.00070953
Iteration 7/25 | Loss: 0.00070953
Iteration 8/25 | Loss: 0.00070953
Iteration 9/25 | Loss: 0.00070953
Iteration 10/25 | Loss: 0.00070953
Iteration 11/25 | Loss: 0.00070953
Iteration 12/25 | Loss: 0.00070953
Iteration 13/25 | Loss: 0.00070953
Iteration 14/25 | Loss: 0.00070953
Iteration 15/25 | Loss: 0.00070953
Iteration 16/25 | Loss: 0.00070953
Iteration 17/25 | Loss: 0.00070953
Iteration 18/25 | Loss: 0.00070953
Iteration 19/25 | Loss: 0.00070953
Iteration 20/25 | Loss: 0.00070953
Iteration 21/25 | Loss: 0.00070953
Iteration 22/25 | Loss: 0.00070953
Iteration 23/25 | Loss: 0.00070953
Iteration 24/25 | Loss: 0.00070953
Iteration 25/25 | Loss: 0.00070953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070953
Iteration 2/1000 | Loss: 0.00003839
Iteration 3/1000 | Loss: 0.00003364
Iteration 4/1000 | Loss: 0.00003128
Iteration 5/1000 | Loss: 0.00002960
Iteration 6/1000 | Loss: 0.00002857
Iteration 7/1000 | Loss: 0.00002792
Iteration 8/1000 | Loss: 0.00002748
Iteration 9/1000 | Loss: 0.00002707
Iteration 10/1000 | Loss: 0.00002694
Iteration 11/1000 | Loss: 0.00002694
Iteration 12/1000 | Loss: 0.00002693
Iteration 13/1000 | Loss: 0.00002669
Iteration 14/1000 | Loss: 0.00002662
Iteration 15/1000 | Loss: 0.00002647
Iteration 16/1000 | Loss: 0.00002622
Iteration 17/1000 | Loss: 0.00002606
Iteration 18/1000 | Loss: 0.00002605
Iteration 19/1000 | Loss: 0.00002604
Iteration 20/1000 | Loss: 0.00002604
Iteration 21/1000 | Loss: 0.00002603
Iteration 22/1000 | Loss: 0.00002603
Iteration 23/1000 | Loss: 0.00002602
Iteration 24/1000 | Loss: 0.00002602
Iteration 25/1000 | Loss: 0.00002602
Iteration 26/1000 | Loss: 0.00002602
Iteration 27/1000 | Loss: 0.00002602
Iteration 28/1000 | Loss: 0.00002602
Iteration 29/1000 | Loss: 0.00002602
Iteration 30/1000 | Loss: 0.00002602
Iteration 31/1000 | Loss: 0.00002602
Iteration 32/1000 | Loss: 0.00002601
Iteration 33/1000 | Loss: 0.00002601
Iteration 34/1000 | Loss: 0.00002600
Iteration 35/1000 | Loss: 0.00002599
Iteration 36/1000 | Loss: 0.00002599
Iteration 37/1000 | Loss: 0.00002599
Iteration 38/1000 | Loss: 0.00002599
Iteration 39/1000 | Loss: 0.00002599
Iteration 40/1000 | Loss: 0.00002599
Iteration 41/1000 | Loss: 0.00002599
Iteration 42/1000 | Loss: 0.00002599
Iteration 43/1000 | Loss: 0.00002599
Iteration 44/1000 | Loss: 0.00002599
Iteration 45/1000 | Loss: 0.00002599
Iteration 46/1000 | Loss: 0.00002599
Iteration 47/1000 | Loss: 0.00002599
Iteration 48/1000 | Loss: 0.00002599
Iteration 49/1000 | Loss: 0.00002598
Iteration 50/1000 | Loss: 0.00002598
Iteration 51/1000 | Loss: 0.00002598
Iteration 52/1000 | Loss: 0.00002597
Iteration 53/1000 | Loss: 0.00002597
Iteration 54/1000 | Loss: 0.00002597
Iteration 55/1000 | Loss: 0.00002595
Iteration 56/1000 | Loss: 0.00002594
Iteration 57/1000 | Loss: 0.00002594
Iteration 58/1000 | Loss: 0.00002594
Iteration 59/1000 | Loss: 0.00002593
Iteration 60/1000 | Loss: 0.00002593
Iteration 61/1000 | Loss: 0.00002593
Iteration 62/1000 | Loss: 0.00002593
Iteration 63/1000 | Loss: 0.00002593
Iteration 64/1000 | Loss: 0.00002593
Iteration 65/1000 | Loss: 0.00002593
Iteration 66/1000 | Loss: 0.00002592
Iteration 67/1000 | Loss: 0.00002592
Iteration 68/1000 | Loss: 0.00002592
Iteration 69/1000 | Loss: 0.00002592
Iteration 70/1000 | Loss: 0.00002592
Iteration 71/1000 | Loss: 0.00002592
Iteration 72/1000 | Loss: 0.00002592
Iteration 73/1000 | Loss: 0.00002592
Iteration 74/1000 | Loss: 0.00002592
Iteration 75/1000 | Loss: 0.00002592
Iteration 76/1000 | Loss: 0.00002592
Iteration 77/1000 | Loss: 0.00002592
Iteration 78/1000 | Loss: 0.00002592
Iteration 79/1000 | Loss: 0.00002591
Iteration 80/1000 | Loss: 0.00002591
Iteration 81/1000 | Loss: 0.00002591
Iteration 82/1000 | Loss: 0.00002591
Iteration 83/1000 | Loss: 0.00002591
Iteration 84/1000 | Loss: 0.00002591
Iteration 85/1000 | Loss: 0.00002589
Iteration 86/1000 | Loss: 0.00002588
Iteration 87/1000 | Loss: 0.00002588
Iteration 88/1000 | Loss: 0.00002588
Iteration 89/1000 | Loss: 0.00002587
Iteration 90/1000 | Loss: 0.00002586
Iteration 91/1000 | Loss: 0.00002585
Iteration 92/1000 | Loss: 0.00002585
Iteration 93/1000 | Loss: 0.00002585
Iteration 94/1000 | Loss: 0.00002584
Iteration 95/1000 | Loss: 0.00002584
Iteration 96/1000 | Loss: 0.00002584
Iteration 97/1000 | Loss: 0.00002583
Iteration 98/1000 | Loss: 0.00002583
Iteration 99/1000 | Loss: 0.00002583
Iteration 100/1000 | Loss: 0.00002583
Iteration 101/1000 | Loss: 0.00002583
Iteration 102/1000 | Loss: 0.00002583
Iteration 103/1000 | Loss: 0.00002583
Iteration 104/1000 | Loss: 0.00002582
Iteration 105/1000 | Loss: 0.00002582
Iteration 106/1000 | Loss: 0.00002582
Iteration 107/1000 | Loss: 0.00002582
Iteration 108/1000 | Loss: 0.00002581
Iteration 109/1000 | Loss: 0.00002581
Iteration 110/1000 | Loss: 0.00002580
Iteration 111/1000 | Loss: 0.00002580
Iteration 112/1000 | Loss: 0.00002580
Iteration 113/1000 | Loss: 0.00002580
Iteration 114/1000 | Loss: 0.00002578
Iteration 115/1000 | Loss: 0.00002578
Iteration 116/1000 | Loss: 0.00002577
Iteration 117/1000 | Loss: 0.00002577
Iteration 118/1000 | Loss: 0.00002577
Iteration 119/1000 | Loss: 0.00002576
Iteration 120/1000 | Loss: 0.00002576
Iteration 121/1000 | Loss: 0.00002576
Iteration 122/1000 | Loss: 0.00002575
Iteration 123/1000 | Loss: 0.00002575
Iteration 124/1000 | Loss: 0.00002575
Iteration 125/1000 | Loss: 0.00002575
Iteration 126/1000 | Loss: 0.00002575
Iteration 127/1000 | Loss: 0.00002574
Iteration 128/1000 | Loss: 0.00002574
Iteration 129/1000 | Loss: 0.00002574
Iteration 130/1000 | Loss: 0.00002574
Iteration 131/1000 | Loss: 0.00002574
Iteration 132/1000 | Loss: 0.00002574
Iteration 133/1000 | Loss: 0.00002574
Iteration 134/1000 | Loss: 0.00002574
Iteration 135/1000 | Loss: 0.00002574
Iteration 136/1000 | Loss: 0.00002574
Iteration 137/1000 | Loss: 0.00002574
Iteration 138/1000 | Loss: 0.00002574
Iteration 139/1000 | Loss: 0.00002574
Iteration 140/1000 | Loss: 0.00002574
Iteration 141/1000 | Loss: 0.00002574
Iteration 142/1000 | Loss: 0.00002574
Iteration 143/1000 | Loss: 0.00002574
Iteration 144/1000 | Loss: 0.00002574
Iteration 145/1000 | Loss: 0.00002574
Iteration 146/1000 | Loss: 0.00002574
Iteration 147/1000 | Loss: 0.00002574
Iteration 148/1000 | Loss: 0.00002574
Iteration 149/1000 | Loss: 0.00002574
Iteration 150/1000 | Loss: 0.00002574
Iteration 151/1000 | Loss: 0.00002574
Iteration 152/1000 | Loss: 0.00002574
Iteration 153/1000 | Loss: 0.00002574
Iteration 154/1000 | Loss: 0.00002574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.5743036530911922e-05, 2.5743036530911922e-05, 2.5743036530911922e-05, 2.5743036530911922e-05, 2.5743036530911922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5743036530911922e-05

Optimization complete. Final v2v error: 4.37738037109375 mm

Highest mean error: 4.485342979431152 mm for frame 16

Lowest mean error: 4.235636234283447 mm for frame 143

Saving results

Total time: 37.27758574485779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068490
Iteration 2/25 | Loss: 0.01068490
Iteration 3/25 | Loss: 0.01068490
Iteration 4/25 | Loss: 0.00332769
Iteration 5/25 | Loss: 0.00191247
Iteration 6/25 | Loss: 0.00181252
Iteration 7/25 | Loss: 0.00189069
Iteration 8/25 | Loss: 0.00177645
Iteration 9/25 | Loss: 0.00168969
Iteration 10/25 | Loss: 0.00160855
Iteration 11/25 | Loss: 0.00164676
Iteration 12/25 | Loss: 0.00163838
Iteration 13/25 | Loss: 0.00144687
Iteration 14/25 | Loss: 0.00138680
Iteration 15/25 | Loss: 0.00137565
Iteration 16/25 | Loss: 0.00137187
Iteration 17/25 | Loss: 0.00136309
Iteration 18/25 | Loss: 0.00136291
Iteration 19/25 | Loss: 0.00136258
Iteration 20/25 | Loss: 0.00136258
Iteration 21/25 | Loss: 0.00136296
Iteration 22/25 | Loss: 0.00136251
Iteration 23/25 | Loss: 0.00136251
Iteration 24/25 | Loss: 0.00136251
Iteration 25/25 | Loss: 0.00136251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53600448
Iteration 2/25 | Loss: 0.00084573
Iteration 3/25 | Loss: 0.00083528
Iteration 4/25 | Loss: 0.00083528
Iteration 5/25 | Loss: 0.00083528
Iteration 6/25 | Loss: 0.00083528
Iteration 7/25 | Loss: 0.00083528
Iteration 8/25 | Loss: 0.00083528
Iteration 9/25 | Loss: 0.00083528
Iteration 10/25 | Loss: 0.00083528
Iteration 11/25 | Loss: 0.00083528
Iteration 12/25 | Loss: 0.00083528
Iteration 13/25 | Loss: 0.00083528
Iteration 14/25 | Loss: 0.00083528
Iteration 15/25 | Loss: 0.00083528
Iteration 16/25 | Loss: 0.00083528
Iteration 17/25 | Loss: 0.00083528
Iteration 18/25 | Loss: 0.00083528
Iteration 19/25 | Loss: 0.00083528
Iteration 20/25 | Loss: 0.00083528
Iteration 21/25 | Loss: 0.00083528
Iteration 22/25 | Loss: 0.00083528
Iteration 23/25 | Loss: 0.00083528
Iteration 24/25 | Loss: 0.00083528
Iteration 25/25 | Loss: 0.00083528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083528
Iteration 2/1000 | Loss: 0.00011350
Iteration 3/1000 | Loss: 0.00021096
Iteration 4/1000 | Loss: 0.00012981
Iteration 5/1000 | Loss: 0.00151851
Iteration 6/1000 | Loss: 0.00008855
Iteration 7/1000 | Loss: 0.00008584
Iteration 8/1000 | Loss: 0.00002977
Iteration 9/1000 | Loss: 0.00003388
Iteration 10/1000 | Loss: 0.00002334
Iteration 11/1000 | Loss: 0.00003387
Iteration 12/1000 | Loss: 0.00002655
Iteration 13/1000 | Loss: 0.00002694
Iteration 14/1000 | Loss: 0.00002061
Iteration 15/1000 | Loss: 0.00001847
Iteration 16/1000 | Loss: 0.00003356
Iteration 17/1000 | Loss: 0.00001953
Iteration 18/1000 | Loss: 0.00002298
Iteration 19/1000 | Loss: 0.00001810
Iteration 20/1000 | Loss: 0.00002180
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001983
Iteration 24/1000 | Loss: 0.00001983
Iteration 25/1000 | Loss: 0.00003316
Iteration 26/1000 | Loss: 0.00003226
Iteration 27/1000 | Loss: 0.00002288
Iteration 28/1000 | Loss: 0.00002171
Iteration 29/1000 | Loss: 0.00002695
Iteration 30/1000 | Loss: 0.00007951
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001920
Iteration 33/1000 | Loss: 0.00001773
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001942
Iteration 36/1000 | Loss: 0.00001810
Iteration 37/1000 | Loss: 0.00001818
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001794
Iteration 42/1000 | Loss: 0.00001794
Iteration 43/1000 | Loss: 0.00001794
Iteration 44/1000 | Loss: 0.00001793
Iteration 45/1000 | Loss: 0.00001841
Iteration 46/1000 | Loss: 0.00001757
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001756
Iteration 54/1000 | Loss: 0.00001756
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001756
Iteration 59/1000 | Loss: 0.00001756
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.755755511112511e-05, 1.755755511112511e-05, 1.755755511112511e-05, 1.755755511112511e-05, 1.755755511112511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.755755511112511e-05

Optimization complete. Final v2v error: 3.60052490234375 mm

Highest mean error: 3.6561214923858643 mm for frame 104

Lowest mean error: 3.5073914527893066 mm for frame 132

Saving results

Total time: 97.6014974117279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elizabeth_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elizabeth_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039696
Iteration 2/25 | Loss: 0.01039696
Iteration 3/25 | Loss: 0.01039696
Iteration 4/25 | Loss: 0.01039696
Iteration 5/25 | Loss: 0.01039696
Iteration 6/25 | Loss: 0.01039696
Iteration 7/25 | Loss: 0.01039696
Iteration 8/25 | Loss: 0.01039696
Iteration 9/25 | Loss: 0.01039696
Iteration 10/25 | Loss: 0.01039695
Iteration 11/25 | Loss: 0.01039695
Iteration 12/25 | Loss: 0.01039695
Iteration 13/25 | Loss: 0.01039695
Iteration 14/25 | Loss: 0.01039695
Iteration 15/25 | Loss: 0.01039695
Iteration 16/25 | Loss: 0.01039695
Iteration 17/25 | Loss: 0.01039695
Iteration 18/25 | Loss: 0.01039695
Iteration 19/25 | Loss: 0.01039694
Iteration 20/25 | Loss: 0.01039694
Iteration 21/25 | Loss: 0.01039694
Iteration 22/25 | Loss: 0.01039694
Iteration 23/25 | Loss: 0.01039694
Iteration 24/25 | Loss: 0.01039694
Iteration 25/25 | Loss: 0.01039694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67717230
Iteration 2/25 | Loss: 0.08546315
Iteration 3/25 | Loss: 0.08541969
Iteration 4/25 | Loss: 0.08541968
Iteration 5/25 | Loss: 0.08541968
Iteration 6/25 | Loss: 0.08541968
Iteration 7/25 | Loss: 0.08541967
Iteration 8/25 | Loss: 0.08541967
Iteration 9/25 | Loss: 0.08541967
Iteration 10/25 | Loss: 0.08541967
Iteration 11/25 | Loss: 0.08541967
Iteration 12/25 | Loss: 0.08541967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0854196697473526, 0.0854196697473526, 0.0854196697473526, 0.0854196697473526, 0.0854196697473526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0854196697473526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08541967
Iteration 2/1000 | Loss: 0.00183120
Iteration 3/1000 | Loss: 0.00057300
Iteration 4/1000 | Loss: 0.00125480
Iteration 5/1000 | Loss: 0.00013918
Iteration 6/1000 | Loss: 0.00009102
Iteration 7/1000 | Loss: 0.00013077
Iteration 8/1000 | Loss: 0.00006567
Iteration 9/1000 | Loss: 0.00004782
Iteration 10/1000 | Loss: 0.00012760
Iteration 11/1000 | Loss: 0.00005189
Iteration 12/1000 | Loss: 0.00003134
Iteration 13/1000 | Loss: 0.00002885
Iteration 14/1000 | Loss: 0.00005299
Iteration 15/1000 | Loss: 0.00003031
Iteration 16/1000 | Loss: 0.00004431
Iteration 17/1000 | Loss: 0.00002473
Iteration 18/1000 | Loss: 0.00004286
Iteration 19/1000 | Loss: 0.00002346
Iteration 20/1000 | Loss: 0.00002234
Iteration 21/1000 | Loss: 0.00005295
Iteration 22/1000 | Loss: 0.00002099
Iteration 23/1000 | Loss: 0.00003249
Iteration 24/1000 | Loss: 0.00009859
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00004458
Iteration 27/1000 | Loss: 0.00002214
Iteration 28/1000 | Loss: 0.00004647
Iteration 29/1000 | Loss: 0.00006903
Iteration 30/1000 | Loss: 0.00010821
Iteration 31/1000 | Loss: 0.00003653
Iteration 32/1000 | Loss: 0.00001860
Iteration 33/1000 | Loss: 0.00003123
Iteration 34/1000 | Loss: 0.00030104
Iteration 35/1000 | Loss: 0.00004431
Iteration 36/1000 | Loss: 0.00002040
Iteration 37/1000 | Loss: 0.00011073
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001917
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00013338
Iteration 42/1000 | Loss: 0.00002218
Iteration 43/1000 | Loss: 0.00003103
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001776
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001771
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001814
Iteration 52/1000 | Loss: 0.00001767
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001765
Iteration 56/1000 | Loss: 0.00001764
Iteration 57/1000 | Loss: 0.00001764
Iteration 58/1000 | Loss: 0.00001764
Iteration 59/1000 | Loss: 0.00001763
Iteration 60/1000 | Loss: 0.00001763
Iteration 61/1000 | Loss: 0.00001762
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001760
Iteration 66/1000 | Loss: 0.00001760
Iteration 67/1000 | Loss: 0.00001760
Iteration 68/1000 | Loss: 0.00002660
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001925
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001752
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001758
Iteration 75/1000 | Loss: 0.00005798
Iteration 76/1000 | Loss: 0.00012267
Iteration 77/1000 | Loss: 0.00002566
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001747
Iteration 88/1000 | Loss: 0.00001747
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001748
Iteration 95/1000 | Loss: 0.00001748
Iteration 96/1000 | Loss: 0.00001748
Iteration 97/1000 | Loss: 0.00001868
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001746
Iteration 102/1000 | Loss: 0.00001746
Iteration 103/1000 | Loss: 0.00001745
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001744
Iteration 109/1000 | Loss: 0.00001744
Iteration 110/1000 | Loss: 0.00001744
Iteration 111/1000 | Loss: 0.00001744
Iteration 112/1000 | Loss: 0.00001744
Iteration 113/1000 | Loss: 0.00001744
Iteration 114/1000 | Loss: 0.00001744
Iteration 115/1000 | Loss: 0.00001744
Iteration 116/1000 | Loss: 0.00001744
Iteration 117/1000 | Loss: 0.00001744
Iteration 118/1000 | Loss: 0.00001744
Iteration 119/1000 | Loss: 0.00001744
Iteration 120/1000 | Loss: 0.00001744
Iteration 121/1000 | Loss: 0.00001744
Iteration 122/1000 | Loss: 0.00001744
Iteration 123/1000 | Loss: 0.00001744
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001744
Iteration 126/1000 | Loss: 0.00001744
Iteration 127/1000 | Loss: 0.00001744
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001744
Iteration 132/1000 | Loss: 0.00001744
Iteration 133/1000 | Loss: 0.00001744
Iteration 134/1000 | Loss: 0.00001744
Iteration 135/1000 | Loss: 0.00001744
Iteration 136/1000 | Loss: 0.00001744
Iteration 137/1000 | Loss: 0.00001744
Iteration 138/1000 | Loss: 0.00001744
Iteration 139/1000 | Loss: 0.00001744
Iteration 140/1000 | Loss: 0.00001744
Iteration 141/1000 | Loss: 0.00001744
Iteration 142/1000 | Loss: 0.00001744
Iteration 143/1000 | Loss: 0.00001744
Iteration 144/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.7439973817090504e-05, 1.7439973817090504e-05, 1.7439973817090504e-05, 1.7439973817090504e-05, 1.7439973817090504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7439973817090504e-05

Optimization complete. Final v2v error: 3.6009867191314697 mm

Highest mean error: 3.9863879680633545 mm for frame 7

Lowest mean error: 3.306067943572998 mm for frame 12

Saving results

Total time: 89.36749267578125
