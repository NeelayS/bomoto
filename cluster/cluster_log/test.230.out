Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=230, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12880-12935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01168832
Iteration 2/25 | Loss: 0.00280813
Iteration 3/25 | Loss: 0.00217112
Iteration 4/25 | Loss: 0.00205813
Iteration 5/25 | Loss: 0.00194779
Iteration 6/25 | Loss: 0.00188493
Iteration 7/25 | Loss: 0.00183600
Iteration 8/25 | Loss: 0.00175876
Iteration 9/25 | Loss: 0.00167155
Iteration 10/25 | Loss: 0.00164514
Iteration 11/25 | Loss: 0.00164111
Iteration 12/25 | Loss: 0.00162415
Iteration 13/25 | Loss: 0.00161345
Iteration 14/25 | Loss: 0.00161008
Iteration 15/25 | Loss: 0.00160289
Iteration 16/25 | Loss: 0.00160752
Iteration 17/25 | Loss: 0.00160125
Iteration 18/25 | Loss: 0.00160009
Iteration 19/25 | Loss: 0.00160077
Iteration 20/25 | Loss: 0.00160117
Iteration 21/25 | Loss: 0.00160072
Iteration 22/25 | Loss: 0.00159865
Iteration 23/25 | Loss: 0.00160250
Iteration 24/25 | Loss: 0.00160129
Iteration 25/25 | Loss: 0.00159836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38030398
Iteration 2/25 | Loss: 0.00645290
Iteration 3/25 | Loss: 0.00454559
Iteration 4/25 | Loss: 0.00454559
Iteration 5/25 | Loss: 0.00454559
Iteration 6/25 | Loss: 0.00454559
Iteration 7/25 | Loss: 0.00454559
Iteration 8/25 | Loss: 0.00454559
Iteration 9/25 | Loss: 0.00454559
Iteration 10/25 | Loss: 0.00454559
Iteration 11/25 | Loss: 0.00454559
Iteration 12/25 | Loss: 0.00454559
Iteration 13/25 | Loss: 0.00454559
Iteration 14/25 | Loss: 0.00454559
Iteration 15/25 | Loss: 0.00454559
Iteration 16/25 | Loss: 0.00454559
Iteration 17/25 | Loss: 0.00454559
Iteration 18/25 | Loss: 0.00454559
Iteration 19/25 | Loss: 0.00454559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0045455885119736195, 0.0045455885119736195, 0.0045455885119736195, 0.0045455885119736195, 0.0045455885119736195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045455885119736195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00454559
Iteration 2/1000 | Loss: 0.00200700
Iteration 3/1000 | Loss: 0.00305471
Iteration 4/1000 | Loss: 0.00326594
Iteration 5/1000 | Loss: 0.00226206
Iteration 6/1000 | Loss: 0.00261967
Iteration 7/1000 | Loss: 0.00217742
Iteration 8/1000 | Loss: 0.00091137
Iteration 9/1000 | Loss: 0.00414985
Iteration 10/1000 | Loss: 0.00277857
Iteration 11/1000 | Loss: 0.00517627
Iteration 12/1000 | Loss: 0.01188963
Iteration 13/1000 | Loss: 0.00407455
Iteration 14/1000 | Loss: 0.00046632
Iteration 15/1000 | Loss: 0.00047084
Iteration 16/1000 | Loss: 0.00383967
Iteration 17/1000 | Loss: 0.00256057
Iteration 18/1000 | Loss: 0.00185203
Iteration 19/1000 | Loss: 0.00148537
Iteration 20/1000 | Loss: 0.00259012
Iteration 21/1000 | Loss: 0.00112686
Iteration 22/1000 | Loss: 0.00463870
Iteration 23/1000 | Loss: 0.00201121
Iteration 24/1000 | Loss: 0.00333349
Iteration 25/1000 | Loss: 0.00165027
Iteration 26/1000 | Loss: 0.00280694
Iteration 27/1000 | Loss: 0.00295457
Iteration 28/1000 | Loss: 0.00399032
Iteration 29/1000 | Loss: 0.00276786
Iteration 30/1000 | Loss: 0.00500653
Iteration 31/1000 | Loss: 0.00135179
Iteration 32/1000 | Loss: 0.00146186
Iteration 33/1000 | Loss: 0.00109024
Iteration 34/1000 | Loss: 0.00215533
Iteration 35/1000 | Loss: 0.00451943
Iteration 36/1000 | Loss: 0.00367666
Iteration 37/1000 | Loss: 0.00358333
Iteration 38/1000 | Loss: 0.00244030
Iteration 39/1000 | Loss: 0.00213460
Iteration 40/1000 | Loss: 0.00124585
Iteration 41/1000 | Loss: 0.00104939
Iteration 42/1000 | Loss: 0.00074193
Iteration 43/1000 | Loss: 0.00026797
Iteration 44/1000 | Loss: 0.00242663
Iteration 45/1000 | Loss: 0.00312623
Iteration 46/1000 | Loss: 0.00075658
Iteration 47/1000 | Loss: 0.00146399
Iteration 48/1000 | Loss: 0.00130647
Iteration 49/1000 | Loss: 0.00032966
Iteration 50/1000 | Loss: 0.00037698
Iteration 51/1000 | Loss: 0.00285487
Iteration 52/1000 | Loss: 0.00214166
Iteration 53/1000 | Loss: 0.00080053
Iteration 54/1000 | Loss: 0.00061104
Iteration 55/1000 | Loss: 0.00025641
Iteration 56/1000 | Loss: 0.00020294
Iteration 57/1000 | Loss: 0.00141899
Iteration 58/1000 | Loss: 0.00049614
Iteration 59/1000 | Loss: 0.00025896
Iteration 60/1000 | Loss: 0.00118102
Iteration 61/1000 | Loss: 0.00067373
Iteration 62/1000 | Loss: 0.00035107
Iteration 63/1000 | Loss: 0.00143196
Iteration 64/1000 | Loss: 0.00037713
Iteration 65/1000 | Loss: 0.00024653
Iteration 66/1000 | Loss: 0.00014144
Iteration 67/1000 | Loss: 0.00119277
Iteration 68/1000 | Loss: 0.00041866
Iteration 69/1000 | Loss: 0.00098116
Iteration 70/1000 | Loss: 0.00063040
Iteration 71/1000 | Loss: 0.00050462
Iteration 72/1000 | Loss: 0.00037071
Iteration 73/1000 | Loss: 0.00033964
Iteration 74/1000 | Loss: 0.00033058
Iteration 75/1000 | Loss: 0.00015954
Iteration 76/1000 | Loss: 0.00013769
Iteration 77/1000 | Loss: 0.00013101
Iteration 78/1000 | Loss: 0.00035797
Iteration 79/1000 | Loss: 0.00044724
Iteration 80/1000 | Loss: 0.00080244
Iteration 81/1000 | Loss: 0.00079268
Iteration 82/1000 | Loss: 0.00025140
Iteration 83/1000 | Loss: 0.00042502
Iteration 84/1000 | Loss: 0.00052536
Iteration 85/1000 | Loss: 0.00017814
Iteration 86/1000 | Loss: 0.00014010
Iteration 87/1000 | Loss: 0.00012757
Iteration 88/1000 | Loss: 0.00013475
Iteration 89/1000 | Loss: 0.00013237
Iteration 90/1000 | Loss: 0.00021245
Iteration 91/1000 | Loss: 0.00011724
Iteration 92/1000 | Loss: 0.00011703
Iteration 93/1000 | Loss: 0.00013034
Iteration 94/1000 | Loss: 0.00012281
Iteration 95/1000 | Loss: 0.00020053
Iteration 96/1000 | Loss: 0.00013116
Iteration 97/1000 | Loss: 0.00013136
Iteration 98/1000 | Loss: 0.00011224
Iteration 99/1000 | Loss: 0.00159511
Iteration 100/1000 | Loss: 0.00020770
Iteration 101/1000 | Loss: 0.00014984
Iteration 102/1000 | Loss: 0.00012309
Iteration 103/1000 | Loss: 0.00015002
Iteration 104/1000 | Loss: 0.00013370
Iteration 105/1000 | Loss: 0.00011015
Iteration 106/1000 | Loss: 0.00011861
Iteration 107/1000 | Loss: 0.00011914
Iteration 108/1000 | Loss: 0.00012621
Iteration 109/1000 | Loss: 0.00011743
Iteration 110/1000 | Loss: 0.00012541
Iteration 111/1000 | Loss: 0.00011601
Iteration 112/1000 | Loss: 0.00016518
Iteration 113/1000 | Loss: 0.00011712
Iteration 114/1000 | Loss: 0.00015602
Iteration 115/1000 | Loss: 0.00011768
Iteration 116/1000 | Loss: 0.00011474
Iteration 117/1000 | Loss: 0.00011874
Iteration 118/1000 | Loss: 0.00012792
Iteration 119/1000 | Loss: 0.00010446
Iteration 120/1000 | Loss: 0.00012169
Iteration 121/1000 | Loss: 0.00012962
Iteration 122/1000 | Loss: 0.00011906
Iteration 123/1000 | Loss: 0.00018306
Iteration 124/1000 | Loss: 0.00010925
Iteration 125/1000 | Loss: 0.00010971
Iteration 126/1000 | Loss: 0.00012192
Iteration 127/1000 | Loss: 0.00012213
Iteration 128/1000 | Loss: 0.00011876
Iteration 129/1000 | Loss: 0.00012266
Iteration 130/1000 | Loss: 0.00230548
Iteration 131/1000 | Loss: 0.00074744
Iteration 132/1000 | Loss: 0.00083393
Iteration 133/1000 | Loss: 0.00080105
Iteration 134/1000 | Loss: 0.00044923
Iteration 135/1000 | Loss: 0.00127049
Iteration 136/1000 | Loss: 0.00015840
Iteration 137/1000 | Loss: 0.00115685
Iteration 138/1000 | Loss: 0.00101997
Iteration 139/1000 | Loss: 0.00033627
Iteration 140/1000 | Loss: 0.00113066
Iteration 141/1000 | Loss: 0.00036069
Iteration 142/1000 | Loss: 0.00113744
Iteration 143/1000 | Loss: 0.00093698
Iteration 144/1000 | Loss: 0.00116578
Iteration 145/1000 | Loss: 0.00077385
Iteration 146/1000 | Loss: 0.00031551
Iteration 147/1000 | Loss: 0.00010445
Iteration 148/1000 | Loss: 0.00109158
Iteration 149/1000 | Loss: 0.00062484
Iteration 150/1000 | Loss: 0.00129628
Iteration 151/1000 | Loss: 0.00108244
Iteration 152/1000 | Loss: 0.00113942
Iteration 153/1000 | Loss: 0.00121175
Iteration 154/1000 | Loss: 0.00209348
Iteration 155/1000 | Loss: 0.00076272
Iteration 156/1000 | Loss: 0.00132060
Iteration 157/1000 | Loss: 0.00038307
Iteration 158/1000 | Loss: 0.00047148
Iteration 159/1000 | Loss: 0.00085486
Iteration 160/1000 | Loss: 0.00014932
Iteration 161/1000 | Loss: 0.00019027
Iteration 162/1000 | Loss: 0.00021641
Iteration 163/1000 | Loss: 0.00025137
Iteration 164/1000 | Loss: 0.00011218
Iteration 165/1000 | Loss: 0.00037746
Iteration 166/1000 | Loss: 0.00010912
Iteration 167/1000 | Loss: 0.00018091
Iteration 168/1000 | Loss: 0.00058118
Iteration 169/1000 | Loss: 0.00034648
Iteration 170/1000 | Loss: 0.00048990
Iteration 171/1000 | Loss: 0.00047984
Iteration 172/1000 | Loss: 0.00047800
Iteration 173/1000 | Loss: 0.00052859
Iteration 174/1000 | Loss: 0.00037404
Iteration 175/1000 | Loss: 0.00061744
Iteration 176/1000 | Loss: 0.00052375
Iteration 177/1000 | Loss: 0.00020757
Iteration 178/1000 | Loss: 0.00009819
Iteration 179/1000 | Loss: 0.00009693
Iteration 180/1000 | Loss: 0.00009836
Iteration 181/1000 | Loss: 0.00009541
Iteration 182/1000 | Loss: 0.00009465
Iteration 183/1000 | Loss: 0.00009408
Iteration 184/1000 | Loss: 0.00009353
Iteration 185/1000 | Loss: 0.00009317
Iteration 186/1000 | Loss: 0.00009297
Iteration 187/1000 | Loss: 0.00011070
Iteration 188/1000 | Loss: 0.00009282
Iteration 189/1000 | Loss: 0.00009681
Iteration 190/1000 | Loss: 0.00009280
Iteration 191/1000 | Loss: 0.00009280
Iteration 192/1000 | Loss: 0.00009279
Iteration 193/1000 | Loss: 0.00009279
Iteration 194/1000 | Loss: 0.00009278
Iteration 195/1000 | Loss: 0.00009274
Iteration 196/1000 | Loss: 0.00009274
Iteration 197/1000 | Loss: 0.00009274
Iteration 198/1000 | Loss: 0.00009274
Iteration 199/1000 | Loss: 0.00009274
Iteration 200/1000 | Loss: 0.00010458
Iteration 201/1000 | Loss: 0.00010458
Iteration 202/1000 | Loss: 0.00010344
Iteration 203/1000 | Loss: 0.00009533
Iteration 204/1000 | Loss: 0.00009312
Iteration 205/1000 | Loss: 0.00009267
Iteration 206/1000 | Loss: 0.00009267
Iteration 207/1000 | Loss: 0.00009267
Iteration 208/1000 | Loss: 0.00009266
Iteration 209/1000 | Loss: 0.00009266
Iteration 210/1000 | Loss: 0.00009266
Iteration 211/1000 | Loss: 0.00009266
Iteration 212/1000 | Loss: 0.00009266
Iteration 213/1000 | Loss: 0.00009266
Iteration 214/1000 | Loss: 0.00009266
Iteration 215/1000 | Loss: 0.00009266
Iteration 216/1000 | Loss: 0.00009266
Iteration 217/1000 | Loss: 0.00009266
Iteration 218/1000 | Loss: 0.00009266
Iteration 219/1000 | Loss: 0.00009266
Iteration 220/1000 | Loss: 0.00009266
Iteration 221/1000 | Loss: 0.00009266
Iteration 222/1000 | Loss: 0.00009266
Iteration 223/1000 | Loss: 0.00009266
Iteration 224/1000 | Loss: 0.00009266
Iteration 225/1000 | Loss: 0.00009265
Iteration 226/1000 | Loss: 0.00009265
Iteration 227/1000 | Loss: 0.00009265
Iteration 228/1000 | Loss: 0.00009265
Iteration 229/1000 | Loss: 0.00009265
Iteration 230/1000 | Loss: 0.00009265
Iteration 231/1000 | Loss: 0.00009265
Iteration 232/1000 | Loss: 0.00009265
Iteration 233/1000 | Loss: 0.00009265
Iteration 234/1000 | Loss: 0.00009265
Iteration 235/1000 | Loss: 0.00009265
Iteration 236/1000 | Loss: 0.00009265
Iteration 237/1000 | Loss: 0.00009265
Iteration 238/1000 | Loss: 0.00009264
Iteration 239/1000 | Loss: 0.00009264
Iteration 240/1000 | Loss: 0.00009264
Iteration 241/1000 | Loss: 0.00009264
Iteration 242/1000 | Loss: 0.00009264
Iteration 243/1000 | Loss: 0.00009264
Iteration 244/1000 | Loss: 0.00009263
Iteration 245/1000 | Loss: 0.00009263
Iteration 246/1000 | Loss: 0.00009263
Iteration 247/1000 | Loss: 0.00077353
Iteration 248/1000 | Loss: 0.00011448
Iteration 249/1000 | Loss: 0.00010781
Iteration 250/1000 | Loss: 0.00009635
Iteration 251/1000 | Loss: 0.00009452
Iteration 252/1000 | Loss: 0.00009160
Iteration 253/1000 | Loss: 0.00009056
Iteration 254/1000 | Loss: 0.00009018
Iteration 255/1000 | Loss: 0.00008997
Iteration 256/1000 | Loss: 0.00008991
Iteration 257/1000 | Loss: 0.00008985
Iteration 258/1000 | Loss: 0.00008982
Iteration 259/1000 | Loss: 0.00008982
Iteration 260/1000 | Loss: 0.00008982
Iteration 261/1000 | Loss: 0.00008981
Iteration 262/1000 | Loss: 0.00008981
Iteration 263/1000 | Loss: 0.00008981
Iteration 264/1000 | Loss: 0.00008981
Iteration 265/1000 | Loss: 0.00008980
Iteration 266/1000 | Loss: 0.00008980
Iteration 267/1000 | Loss: 0.00008980
Iteration 268/1000 | Loss: 0.00008980
Iteration 269/1000 | Loss: 0.00008980
Iteration 270/1000 | Loss: 0.00008979
Iteration 271/1000 | Loss: 0.00008979
Iteration 272/1000 | Loss: 0.00008979
Iteration 273/1000 | Loss: 0.00008979
Iteration 274/1000 | Loss: 0.00008979
Iteration 275/1000 | Loss: 0.00008979
Iteration 276/1000 | Loss: 0.00008979
Iteration 277/1000 | Loss: 0.00008979
Iteration 278/1000 | Loss: 0.00008978
Iteration 279/1000 | Loss: 0.00008978
Iteration 280/1000 | Loss: 0.00008978
Iteration 281/1000 | Loss: 0.00008978
Iteration 282/1000 | Loss: 0.00008978
Iteration 283/1000 | Loss: 0.00008978
Iteration 284/1000 | Loss: 0.00008978
Iteration 285/1000 | Loss: 0.00008978
Iteration 286/1000 | Loss: 0.00008978
Iteration 287/1000 | Loss: 0.00008978
Iteration 288/1000 | Loss: 0.00008978
Iteration 289/1000 | Loss: 0.00008978
Iteration 290/1000 | Loss: 0.00008978
Iteration 291/1000 | Loss: 0.00008978
Iteration 292/1000 | Loss: 0.00008978
Iteration 293/1000 | Loss: 0.00008978
Iteration 294/1000 | Loss: 0.00008978
Iteration 295/1000 | Loss: 0.00008978
Iteration 296/1000 | Loss: 0.00008978
Iteration 297/1000 | Loss: 0.00008978
Iteration 298/1000 | Loss: 0.00008978
Iteration 299/1000 | Loss: 0.00008978
Iteration 300/1000 | Loss: 0.00008978
Iteration 301/1000 | Loss: 0.00008978
Iteration 302/1000 | Loss: 0.00008978
Iteration 303/1000 | Loss: 0.00008978
Iteration 304/1000 | Loss: 0.00008978
Iteration 305/1000 | Loss: 0.00008978
Iteration 306/1000 | Loss: 0.00008978
Iteration 307/1000 | Loss: 0.00008978
Iteration 308/1000 | Loss: 0.00008978
Iteration 309/1000 | Loss: 0.00008978
Iteration 310/1000 | Loss: 0.00008978
Iteration 311/1000 | Loss: 0.00008978
Iteration 312/1000 | Loss: 0.00008978
Iteration 313/1000 | Loss: 0.00008978
Iteration 314/1000 | Loss: 0.00008978
Iteration 315/1000 | Loss: 0.00008978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [8.978400001069531e-05, 8.978400001069531e-05, 8.978400001069531e-05, 8.978400001069531e-05, 8.978400001069531e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.978400001069531e-05

Optimization complete. Final v2v error: 5.8170485496521 mm

Highest mean error: 13.841019630432129 mm for frame 36

Lowest mean error: 4.215886116027832 mm for frame 73

Saving results

Total time: 357.5573935508728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00645387
Iteration 2/25 | Loss: 0.00163274
Iteration 3/25 | Loss: 0.00146927
Iteration 4/25 | Loss: 0.00142516
Iteration 5/25 | Loss: 0.00140647
Iteration 6/25 | Loss: 0.00140325
Iteration 7/25 | Loss: 0.00140222
Iteration 8/25 | Loss: 0.00140165
Iteration 9/25 | Loss: 0.00140087
Iteration 10/25 | Loss: 0.00140029
Iteration 11/25 | Loss: 0.00140009
Iteration 12/25 | Loss: 0.00140000
Iteration 13/25 | Loss: 0.00139992
Iteration 14/25 | Loss: 0.00139985
Iteration 15/25 | Loss: 0.00139985
Iteration 16/25 | Loss: 0.00139984
Iteration 17/25 | Loss: 0.00139982
Iteration 18/25 | Loss: 0.00139975
Iteration 19/25 | Loss: 0.00139972
Iteration 20/25 | Loss: 0.00140690
Iteration 21/25 | Loss: 0.00139783
Iteration 22/25 | Loss: 0.00139588
Iteration 23/25 | Loss: 0.00139544
Iteration 24/25 | Loss: 0.00139535
Iteration 25/25 | Loss: 0.00139534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15869284
Iteration 2/25 | Loss: 0.00126210
Iteration 3/25 | Loss: 0.00126205
Iteration 4/25 | Loss: 0.00126205
Iteration 5/25 | Loss: 0.00126205
Iteration 6/25 | Loss: 0.00126205
Iteration 7/25 | Loss: 0.00126205
Iteration 8/25 | Loss: 0.00126205
Iteration 9/25 | Loss: 0.00126205
Iteration 10/25 | Loss: 0.00126204
Iteration 11/25 | Loss: 0.00126204
Iteration 12/25 | Loss: 0.00126205
Iteration 13/25 | Loss: 0.00126205
Iteration 14/25 | Loss: 0.00126205
Iteration 15/25 | Loss: 0.00126205
Iteration 16/25 | Loss: 0.00126205
Iteration 17/25 | Loss: 0.00126205
Iteration 18/25 | Loss: 0.00126205
Iteration 19/25 | Loss: 0.00126205
Iteration 20/25 | Loss: 0.00126205
Iteration 21/25 | Loss: 0.00126205
Iteration 22/25 | Loss: 0.00126205
Iteration 23/25 | Loss: 0.00126205
Iteration 24/25 | Loss: 0.00126205
Iteration 25/25 | Loss: 0.00126205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126205
Iteration 2/1000 | Loss: 0.00006741
Iteration 3/1000 | Loss: 0.00005404
Iteration 4/1000 | Loss: 0.00005019
Iteration 5/1000 | Loss: 0.00004809
Iteration 6/1000 | Loss: 0.00004629
Iteration 7/1000 | Loss: 0.00004518
Iteration 8/1000 | Loss: 0.00004428
Iteration 9/1000 | Loss: 0.00004377
Iteration 10/1000 | Loss: 0.00004347
Iteration 11/1000 | Loss: 0.00004317
Iteration 12/1000 | Loss: 0.00004309
Iteration 13/1000 | Loss: 0.00004307
Iteration 14/1000 | Loss: 0.00004307
Iteration 15/1000 | Loss: 0.00004303
Iteration 16/1000 | Loss: 0.00004303
Iteration 17/1000 | Loss: 0.00004303
Iteration 18/1000 | Loss: 0.00004302
Iteration 19/1000 | Loss: 0.00004302
Iteration 20/1000 | Loss: 0.00004300
Iteration 21/1000 | Loss: 0.00004299
Iteration 22/1000 | Loss: 0.00004299
Iteration 23/1000 | Loss: 0.00004299
Iteration 24/1000 | Loss: 0.00004298
Iteration 25/1000 | Loss: 0.00004298
Iteration 26/1000 | Loss: 0.00004297
Iteration 27/1000 | Loss: 0.00004296
Iteration 28/1000 | Loss: 0.00004295
Iteration 29/1000 | Loss: 0.00004295
Iteration 30/1000 | Loss: 0.00004295
Iteration 31/1000 | Loss: 0.00004294
Iteration 32/1000 | Loss: 0.00004294
Iteration 33/1000 | Loss: 0.00004294
Iteration 34/1000 | Loss: 0.00004294
Iteration 35/1000 | Loss: 0.00004294
Iteration 36/1000 | Loss: 0.00004294
Iteration 37/1000 | Loss: 0.00004294
Iteration 38/1000 | Loss: 0.00004292
Iteration 39/1000 | Loss: 0.00004291
Iteration 40/1000 | Loss: 0.00004291
Iteration 41/1000 | Loss: 0.00004291
Iteration 42/1000 | Loss: 0.00004291
Iteration 43/1000 | Loss: 0.00004291
Iteration 44/1000 | Loss: 0.00004291
Iteration 45/1000 | Loss: 0.00004291
Iteration 46/1000 | Loss: 0.00004290
Iteration 47/1000 | Loss: 0.00004290
Iteration 48/1000 | Loss: 0.00004290
Iteration 49/1000 | Loss: 0.00004290
Iteration 50/1000 | Loss: 0.00004290
Iteration 51/1000 | Loss: 0.00004290
Iteration 52/1000 | Loss: 0.00004290
Iteration 53/1000 | Loss: 0.00004289
Iteration 54/1000 | Loss: 0.00004288
Iteration 55/1000 | Loss: 0.00004288
Iteration 56/1000 | Loss: 0.00004287
Iteration 57/1000 | Loss: 0.00004287
Iteration 58/1000 | Loss: 0.00004287
Iteration 59/1000 | Loss: 0.00004287
Iteration 60/1000 | Loss: 0.00004286
Iteration 61/1000 | Loss: 0.00004286
Iteration 62/1000 | Loss: 0.00004286
Iteration 63/1000 | Loss: 0.00004285
Iteration 64/1000 | Loss: 0.00004285
Iteration 65/1000 | Loss: 0.00004285
Iteration 66/1000 | Loss: 0.00004284
Iteration 67/1000 | Loss: 0.00004284
Iteration 68/1000 | Loss: 0.00004284
Iteration 69/1000 | Loss: 0.00004284
Iteration 70/1000 | Loss: 0.00004283
Iteration 71/1000 | Loss: 0.00004283
Iteration 72/1000 | Loss: 0.00004283
Iteration 73/1000 | Loss: 0.00004283
Iteration 74/1000 | Loss: 0.00004283
Iteration 75/1000 | Loss: 0.00004283
Iteration 76/1000 | Loss: 0.00004283
Iteration 77/1000 | Loss: 0.00004282
Iteration 78/1000 | Loss: 0.00004282
Iteration 79/1000 | Loss: 0.00004282
Iteration 80/1000 | Loss: 0.00004281
Iteration 81/1000 | Loss: 0.00004281
Iteration 82/1000 | Loss: 0.00004281
Iteration 83/1000 | Loss: 0.00004281
Iteration 84/1000 | Loss: 0.00004281
Iteration 85/1000 | Loss: 0.00004281
Iteration 86/1000 | Loss: 0.00004280
Iteration 87/1000 | Loss: 0.00004280
Iteration 88/1000 | Loss: 0.00004280
Iteration 89/1000 | Loss: 0.00004280
Iteration 90/1000 | Loss: 0.00004280
Iteration 91/1000 | Loss: 0.00004280
Iteration 92/1000 | Loss: 0.00004279
Iteration 93/1000 | Loss: 0.00004279
Iteration 94/1000 | Loss: 0.00004279
Iteration 95/1000 | Loss: 0.00004279
Iteration 96/1000 | Loss: 0.00004278
Iteration 97/1000 | Loss: 0.00004278
Iteration 98/1000 | Loss: 0.00004278
Iteration 99/1000 | Loss: 0.00004278
Iteration 100/1000 | Loss: 0.00004278
Iteration 101/1000 | Loss: 0.00004278
Iteration 102/1000 | Loss: 0.00004278
Iteration 103/1000 | Loss: 0.00004278
Iteration 104/1000 | Loss: 0.00004278
Iteration 105/1000 | Loss: 0.00004278
Iteration 106/1000 | Loss: 0.00004278
Iteration 107/1000 | Loss: 0.00004277
Iteration 108/1000 | Loss: 0.00004277
Iteration 109/1000 | Loss: 0.00004277
Iteration 110/1000 | Loss: 0.00004277
Iteration 111/1000 | Loss: 0.00004277
Iteration 112/1000 | Loss: 0.00004276
Iteration 113/1000 | Loss: 0.00004276
Iteration 114/1000 | Loss: 0.00004276
Iteration 115/1000 | Loss: 0.00004276
Iteration 116/1000 | Loss: 0.00004276
Iteration 117/1000 | Loss: 0.00004276
Iteration 118/1000 | Loss: 0.00004275
Iteration 119/1000 | Loss: 0.00004275
Iteration 120/1000 | Loss: 0.00004275
Iteration 121/1000 | Loss: 0.00004274
Iteration 122/1000 | Loss: 0.00004274
Iteration 123/1000 | Loss: 0.00004274
Iteration 124/1000 | Loss: 0.00004274
Iteration 125/1000 | Loss: 0.00004274
Iteration 126/1000 | Loss: 0.00004274
Iteration 127/1000 | Loss: 0.00004274
Iteration 128/1000 | Loss: 0.00004274
Iteration 129/1000 | Loss: 0.00004274
Iteration 130/1000 | Loss: 0.00004274
Iteration 131/1000 | Loss: 0.00004273
Iteration 132/1000 | Loss: 0.00004273
Iteration 133/1000 | Loss: 0.00004273
Iteration 134/1000 | Loss: 0.00004273
Iteration 135/1000 | Loss: 0.00004273
Iteration 136/1000 | Loss: 0.00004273
Iteration 137/1000 | Loss: 0.00004273
Iteration 138/1000 | Loss: 0.00004272
Iteration 139/1000 | Loss: 0.00004272
Iteration 140/1000 | Loss: 0.00004272
Iteration 141/1000 | Loss: 0.00004272
Iteration 142/1000 | Loss: 0.00004272
Iteration 143/1000 | Loss: 0.00004272
Iteration 144/1000 | Loss: 0.00004271
Iteration 145/1000 | Loss: 0.00004271
Iteration 146/1000 | Loss: 0.00004271
Iteration 147/1000 | Loss: 0.00004271
Iteration 148/1000 | Loss: 0.00004271
Iteration 149/1000 | Loss: 0.00004271
Iteration 150/1000 | Loss: 0.00004271
Iteration 151/1000 | Loss: 0.00004271
Iteration 152/1000 | Loss: 0.00004271
Iteration 153/1000 | Loss: 0.00004271
Iteration 154/1000 | Loss: 0.00004271
Iteration 155/1000 | Loss: 0.00004271
Iteration 156/1000 | Loss: 0.00004271
Iteration 157/1000 | Loss: 0.00004271
Iteration 158/1000 | Loss: 0.00004271
Iteration 159/1000 | Loss: 0.00004271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [4.2712428694358096e-05, 4.2712428694358096e-05, 4.2712428694358096e-05, 4.2712428694358096e-05, 4.2712428694358096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2712428694358096e-05

Optimization complete. Final v2v error: 5.592382431030273 mm

Highest mean error: 7.244476795196533 mm for frame 115

Lowest mean error: 4.8190999031066895 mm for frame 0

Saving results

Total time: 64.51018762588501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01144725
Iteration 2/25 | Loss: 0.01144725
Iteration 3/25 | Loss: 0.01144725
Iteration 4/25 | Loss: 0.01144725
Iteration 5/25 | Loss: 0.01144725
Iteration 6/25 | Loss: 0.01144725
Iteration 7/25 | Loss: 0.01144724
Iteration 8/25 | Loss: 0.01144724
Iteration 9/25 | Loss: 0.01144724
Iteration 10/25 | Loss: 0.01144724
Iteration 11/25 | Loss: 0.01144724
Iteration 12/25 | Loss: 0.01144724
Iteration 13/25 | Loss: 0.01144724
Iteration 14/25 | Loss: 0.01144724
Iteration 15/25 | Loss: 0.01144724
Iteration 16/25 | Loss: 0.01144724
Iteration 17/25 | Loss: 0.01144724
Iteration 18/25 | Loss: 0.01144724
Iteration 19/25 | Loss: 0.01144724
Iteration 20/25 | Loss: 0.01144724
Iteration 21/25 | Loss: 0.01144724
Iteration 22/25 | Loss: 0.01144724
Iteration 23/25 | Loss: 0.01144724
Iteration 24/25 | Loss: 0.01144724
Iteration 25/25 | Loss: 0.01144724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23732901
Iteration 2/25 | Loss: 0.00396135
Iteration 3/25 | Loss: 0.00395795
Iteration 4/25 | Loss: 0.00395795
Iteration 5/25 | Loss: 0.00395795
Iteration 6/25 | Loss: 0.00395795
Iteration 7/25 | Loss: 0.00395795
Iteration 8/25 | Loss: 0.00395795
Iteration 9/25 | Loss: 0.00395795
Iteration 10/25 | Loss: 0.00395795
Iteration 11/25 | Loss: 0.00395795
Iteration 12/25 | Loss: 0.00395795
Iteration 13/25 | Loss: 0.00395795
Iteration 14/25 | Loss: 0.00395795
Iteration 15/25 | Loss: 0.00395795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0039579495787620544, 0.0039579495787620544, 0.0039579495787620544, 0.0039579495787620544, 0.0039579495787620544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0039579495787620544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395795
Iteration 2/1000 | Loss: 0.00259331
Iteration 3/1000 | Loss: 0.00060998
Iteration 4/1000 | Loss: 0.00093086
Iteration 5/1000 | Loss: 0.00091772
Iteration 6/1000 | Loss: 0.00161507
Iteration 7/1000 | Loss: 0.00011112
Iteration 8/1000 | Loss: 0.00030208
Iteration 9/1000 | Loss: 0.00010840
Iteration 10/1000 | Loss: 0.00007022
Iteration 11/1000 | Loss: 0.00022423
Iteration 12/1000 | Loss: 0.00010342
Iteration 13/1000 | Loss: 0.00098522
Iteration 14/1000 | Loss: 0.00044127
Iteration 15/1000 | Loss: 0.00004722
Iteration 16/1000 | Loss: 0.00031252
Iteration 17/1000 | Loss: 0.00044110
Iteration 18/1000 | Loss: 0.00020401
Iteration 19/1000 | Loss: 0.00011556
Iteration 20/1000 | Loss: 0.00114572
Iteration 21/1000 | Loss: 0.00009693
Iteration 22/1000 | Loss: 0.00021004
Iteration 23/1000 | Loss: 0.00004447
Iteration 24/1000 | Loss: 0.00007540
Iteration 25/1000 | Loss: 0.00012525
Iteration 26/1000 | Loss: 0.00003532
Iteration 27/1000 | Loss: 0.00024229
Iteration 28/1000 | Loss: 0.00017768
Iteration 29/1000 | Loss: 0.00003350
Iteration 30/1000 | Loss: 0.00016647
Iteration 31/1000 | Loss: 0.00009856
Iteration 32/1000 | Loss: 0.00010857
Iteration 33/1000 | Loss: 0.00020729
Iteration 34/1000 | Loss: 0.00027387
Iteration 35/1000 | Loss: 0.00025851
Iteration 36/1000 | Loss: 0.00012017
Iteration 37/1000 | Loss: 0.00003094
Iteration 38/1000 | Loss: 0.00019001
Iteration 39/1000 | Loss: 0.00013694
Iteration 40/1000 | Loss: 0.00061142
Iteration 41/1000 | Loss: 0.00005635
Iteration 42/1000 | Loss: 0.00006910
Iteration 43/1000 | Loss: 0.00010841
Iteration 44/1000 | Loss: 0.00002989
Iteration 45/1000 | Loss: 0.00011140
Iteration 46/1000 | Loss: 0.00020023
Iteration 47/1000 | Loss: 0.00018465
Iteration 48/1000 | Loss: 0.00025421
Iteration 49/1000 | Loss: 0.00003932
Iteration 50/1000 | Loss: 0.00005978
Iteration 51/1000 | Loss: 0.00012172
Iteration 52/1000 | Loss: 0.00063969
Iteration 53/1000 | Loss: 0.00005991
Iteration 54/1000 | Loss: 0.00005976
Iteration 55/1000 | Loss: 0.00003524
Iteration 56/1000 | Loss: 0.00002915
Iteration 57/1000 | Loss: 0.00020540
Iteration 58/1000 | Loss: 0.00004064
Iteration 59/1000 | Loss: 0.00021963
Iteration 60/1000 | Loss: 0.00012836
Iteration 61/1000 | Loss: 0.00005082
Iteration 62/1000 | Loss: 0.00005824
Iteration 63/1000 | Loss: 0.00006880
Iteration 64/1000 | Loss: 0.00023412
Iteration 65/1000 | Loss: 0.00002882
Iteration 66/1000 | Loss: 0.00002878
Iteration 67/1000 | Loss: 0.00002876
Iteration 68/1000 | Loss: 0.00002875
Iteration 69/1000 | Loss: 0.00002872
Iteration 70/1000 | Loss: 0.00002871
Iteration 71/1000 | Loss: 0.00002869
Iteration 72/1000 | Loss: 0.00005085
Iteration 73/1000 | Loss: 0.00003775
Iteration 74/1000 | Loss: 0.00002861
Iteration 75/1000 | Loss: 0.00004238
Iteration 76/1000 | Loss: 0.00004101
Iteration 77/1000 | Loss: 0.00002855
Iteration 78/1000 | Loss: 0.00002855
Iteration 79/1000 | Loss: 0.00003119
Iteration 80/1000 | Loss: 0.00003152
Iteration 81/1000 | Loss: 0.00003133
Iteration 82/1000 | Loss: 0.00003009
Iteration 83/1000 | Loss: 0.00014501
Iteration 84/1000 | Loss: 0.00016007
Iteration 85/1000 | Loss: 0.00009928
Iteration 86/1000 | Loss: 0.00037237
Iteration 87/1000 | Loss: 0.00031991
Iteration 88/1000 | Loss: 0.00004246
Iteration 89/1000 | Loss: 0.00027211
Iteration 90/1000 | Loss: 0.00005343
Iteration 91/1000 | Loss: 0.00003624
Iteration 92/1000 | Loss: 0.00006131
Iteration 93/1000 | Loss: 0.00011944
Iteration 94/1000 | Loss: 0.00019982
Iteration 95/1000 | Loss: 0.00008580
Iteration 96/1000 | Loss: 0.00005043
Iteration 97/1000 | Loss: 0.00060883
Iteration 98/1000 | Loss: 0.00009714
Iteration 99/1000 | Loss: 0.00003281
Iteration 100/1000 | Loss: 0.00005908
Iteration 101/1000 | Loss: 0.00002880
Iteration 102/1000 | Loss: 0.00007686
Iteration 103/1000 | Loss: 0.00002885
Iteration 104/1000 | Loss: 0.00002855
Iteration 105/1000 | Loss: 0.00011051
Iteration 106/1000 | Loss: 0.00012423
Iteration 107/1000 | Loss: 0.00031327
Iteration 108/1000 | Loss: 0.00022713
Iteration 109/1000 | Loss: 0.00003119
Iteration 110/1000 | Loss: 0.00003759
Iteration 111/1000 | Loss: 0.00003743
Iteration 112/1000 | Loss: 0.00008709
Iteration 113/1000 | Loss: 0.00003478
Iteration 114/1000 | Loss: 0.00009413
Iteration 115/1000 | Loss: 0.00009992
Iteration 116/1000 | Loss: 0.00005200
Iteration 117/1000 | Loss: 0.00003970
Iteration 118/1000 | Loss: 0.00006610
Iteration 119/1000 | Loss: 0.00006461
Iteration 120/1000 | Loss: 0.00007473
Iteration 121/1000 | Loss: 0.00003309
Iteration 122/1000 | Loss: 0.00002959
Iteration 123/1000 | Loss: 0.00002854
Iteration 124/1000 | Loss: 0.00002854
Iteration 125/1000 | Loss: 0.00002854
Iteration 126/1000 | Loss: 0.00002854
Iteration 127/1000 | Loss: 0.00002854
Iteration 128/1000 | Loss: 0.00002854
Iteration 129/1000 | Loss: 0.00002854
Iteration 130/1000 | Loss: 0.00002854
Iteration 131/1000 | Loss: 0.00002854
Iteration 132/1000 | Loss: 0.00002854
Iteration 133/1000 | Loss: 0.00002853
Iteration 134/1000 | Loss: 0.00002853
Iteration 135/1000 | Loss: 0.00002852
Iteration 136/1000 | Loss: 0.00002852
Iteration 137/1000 | Loss: 0.00007405
Iteration 138/1000 | Loss: 0.00002853
Iteration 139/1000 | Loss: 0.00007881
Iteration 140/1000 | Loss: 0.00003917
Iteration 141/1000 | Loss: 0.00011551
Iteration 142/1000 | Loss: 0.00004794
Iteration 143/1000 | Loss: 0.00003443
Iteration 144/1000 | Loss: 0.00004361
Iteration 145/1000 | Loss: 0.00003952
Iteration 146/1000 | Loss: 0.00004739
Iteration 147/1000 | Loss: 0.00006268
Iteration 148/1000 | Loss: 0.00004500
Iteration 149/1000 | Loss: 0.00004115
Iteration 150/1000 | Loss: 0.00002853
Iteration 151/1000 | Loss: 0.00008396
Iteration 152/1000 | Loss: 0.00002855
Iteration 153/1000 | Loss: 0.00006285
Iteration 154/1000 | Loss: 0.00002851
Iteration 155/1000 | Loss: 0.00002841
Iteration 156/1000 | Loss: 0.00002841
Iteration 157/1000 | Loss: 0.00002840
Iteration 158/1000 | Loss: 0.00002840
Iteration 159/1000 | Loss: 0.00002840
Iteration 160/1000 | Loss: 0.00002840
Iteration 161/1000 | Loss: 0.00002840
Iteration 162/1000 | Loss: 0.00002840
Iteration 163/1000 | Loss: 0.00009967
Iteration 164/1000 | Loss: 0.00003074
Iteration 165/1000 | Loss: 0.00003873
Iteration 166/1000 | Loss: 0.00002846
Iteration 167/1000 | Loss: 0.00007130
Iteration 168/1000 | Loss: 0.00004061
Iteration 169/1000 | Loss: 0.00004061
Iteration 170/1000 | Loss: 0.00002879
Iteration 171/1000 | Loss: 0.00009348
Iteration 172/1000 | Loss: 0.00002939
Iteration 173/1000 | Loss: 0.00002939
Iteration 174/1000 | Loss: 0.00013721
Iteration 175/1000 | Loss: 0.00003535
Iteration 176/1000 | Loss: 0.00003665
Iteration 177/1000 | Loss: 0.00002863
Iteration 178/1000 | Loss: 0.00004378
Iteration 179/1000 | Loss: 0.00003477
Iteration 180/1000 | Loss: 0.00002851
Iteration 181/1000 | Loss: 0.00005469
Iteration 182/1000 | Loss: 0.00008074
Iteration 183/1000 | Loss: 0.00003511
Iteration 184/1000 | Loss: 0.00004603
Iteration 185/1000 | Loss: 0.00005569
Iteration 186/1000 | Loss: 0.00005254
Iteration 187/1000 | Loss: 0.00002852
Iteration 188/1000 | Loss: 0.00005902
Iteration 189/1000 | Loss: 0.00003478
Iteration 190/1000 | Loss: 0.00004320
Iteration 191/1000 | Loss: 0.00002849
Iteration 192/1000 | Loss: 0.00002846
Iteration 193/1000 | Loss: 0.00006035
Iteration 194/1000 | Loss: 0.00003670
Iteration 195/1000 | Loss: 0.00002848
Iteration 196/1000 | Loss: 0.00002848
Iteration 197/1000 | Loss: 0.00002848
Iteration 198/1000 | Loss: 0.00002848
Iteration 199/1000 | Loss: 0.00002848
Iteration 200/1000 | Loss: 0.00002848
Iteration 201/1000 | Loss: 0.00002848
Iteration 202/1000 | Loss: 0.00002848
Iteration 203/1000 | Loss: 0.00002845
Iteration 204/1000 | Loss: 0.00002845
Iteration 205/1000 | Loss: 0.00002843
Iteration 206/1000 | Loss: 0.00002843
Iteration 207/1000 | Loss: 0.00002843
Iteration 208/1000 | Loss: 0.00002843
Iteration 209/1000 | Loss: 0.00002843
Iteration 210/1000 | Loss: 0.00002843
Iteration 211/1000 | Loss: 0.00002843
Iteration 212/1000 | Loss: 0.00002843
Iteration 213/1000 | Loss: 0.00002843
Iteration 214/1000 | Loss: 0.00002843
Iteration 215/1000 | Loss: 0.00002843
Iteration 216/1000 | Loss: 0.00002843
Iteration 217/1000 | Loss: 0.00002842
Iteration 218/1000 | Loss: 0.00002842
Iteration 219/1000 | Loss: 0.00002842
Iteration 220/1000 | Loss: 0.00002841
Iteration 221/1000 | Loss: 0.00002841
Iteration 222/1000 | Loss: 0.00002841
Iteration 223/1000 | Loss: 0.00002841
Iteration 224/1000 | Loss: 0.00002841
Iteration 225/1000 | Loss: 0.00002841
Iteration 226/1000 | Loss: 0.00002841
Iteration 227/1000 | Loss: 0.00002841
Iteration 228/1000 | Loss: 0.00002840
Iteration 229/1000 | Loss: 0.00005983
Iteration 230/1000 | Loss: 0.00003391
Iteration 231/1000 | Loss: 0.00002845
Iteration 232/1000 | Loss: 0.00002844
Iteration 233/1000 | Loss: 0.00002843
Iteration 234/1000 | Loss: 0.00002843
Iteration 235/1000 | Loss: 0.00004222
Iteration 236/1000 | Loss: 0.00014628
Iteration 237/1000 | Loss: 0.00003045
Iteration 238/1000 | Loss: 0.00002939
Iteration 239/1000 | Loss: 0.00003728
Iteration 240/1000 | Loss: 0.00002852
Iteration 241/1000 | Loss: 0.00005202
Iteration 242/1000 | Loss: 0.00003449
Iteration 243/1000 | Loss: 0.00004798
Iteration 244/1000 | Loss: 0.00002854
Iteration 245/1000 | Loss: 0.00002850
Iteration 246/1000 | Loss: 0.00007771
Iteration 247/1000 | Loss: 0.00003187
Iteration 248/1000 | Loss: 0.00002853
Iteration 249/1000 | Loss: 0.00003296
Iteration 250/1000 | Loss: 0.00005322
Iteration 251/1000 | Loss: 0.00002848
Iteration 252/1000 | Loss: 0.00002846
Iteration 253/1000 | Loss: 0.00005979
Iteration 254/1000 | Loss: 0.00003512
Iteration 255/1000 | Loss: 0.00003748
Iteration 256/1000 | Loss: 0.00007656
Iteration 257/1000 | Loss: 0.00004580
Iteration 258/1000 | Loss: 0.00003923
Iteration 259/1000 | Loss: 0.00003167
Iteration 260/1000 | Loss: 0.00003160
Iteration 261/1000 | Loss: 0.00002847
Iteration 262/1000 | Loss: 0.00002845
Iteration 263/1000 | Loss: 0.00002844
Iteration 264/1000 | Loss: 0.00002844
Iteration 265/1000 | Loss: 0.00002844
Iteration 266/1000 | Loss: 0.00002844
Iteration 267/1000 | Loss: 0.00002844
Iteration 268/1000 | Loss: 0.00002843
Iteration 269/1000 | Loss: 0.00002843
Iteration 270/1000 | Loss: 0.00002843
Iteration 271/1000 | Loss: 0.00002843
Iteration 272/1000 | Loss: 0.00002843
Iteration 273/1000 | Loss: 0.00002842
Iteration 274/1000 | Loss: 0.00002841
Iteration 275/1000 | Loss: 0.00004412
Iteration 276/1000 | Loss: 0.00002841
Iteration 277/1000 | Loss: 0.00002841
Iteration 278/1000 | Loss: 0.00009388
Iteration 279/1000 | Loss: 0.00003268
Iteration 280/1000 | Loss: 0.00002849
Iteration 281/1000 | Loss: 0.00002848
Iteration 282/1000 | Loss: 0.00013249
Iteration 283/1000 | Loss: 0.00002879
Iteration 284/1000 | Loss: 0.00002858
Iteration 285/1000 | Loss: 0.00005278
Iteration 286/1000 | Loss: 0.00002847
Iteration 287/1000 | Loss: 0.00002845
Iteration 288/1000 | Loss: 0.00005684
Iteration 289/1000 | Loss: 0.00003811
Iteration 290/1000 | Loss: 0.00004078
Iteration 291/1000 | Loss: 0.00002841
Iteration 292/1000 | Loss: 0.00002840
Iteration 293/1000 | Loss: 0.00002840
Iteration 294/1000 | Loss: 0.00002840
Iteration 295/1000 | Loss: 0.00002840
Iteration 296/1000 | Loss: 0.00002840
Iteration 297/1000 | Loss: 0.00002840
Iteration 298/1000 | Loss: 0.00002840
Iteration 299/1000 | Loss: 0.00002840
Iteration 300/1000 | Loss: 0.00002839
Iteration 301/1000 | Loss: 0.00002839
Iteration 302/1000 | Loss: 0.00002839
Iteration 303/1000 | Loss: 0.00006543
Iteration 304/1000 | Loss: 0.00002847
Iteration 305/1000 | Loss: 0.00002838
Iteration 306/1000 | Loss: 0.00002838
Iteration 307/1000 | Loss: 0.00002838
Iteration 308/1000 | Loss: 0.00002838
Iteration 309/1000 | Loss: 0.00002838
Iteration 310/1000 | Loss: 0.00002838
Iteration 311/1000 | Loss: 0.00002838
Iteration 312/1000 | Loss: 0.00002838
Iteration 313/1000 | Loss: 0.00002838
Iteration 314/1000 | Loss: 0.00002838
Iteration 315/1000 | Loss: 0.00002838
Iteration 316/1000 | Loss: 0.00002837
Iteration 317/1000 | Loss: 0.00002837
Iteration 318/1000 | Loss: 0.00002837
Iteration 319/1000 | Loss: 0.00002837
Iteration 320/1000 | Loss: 0.00002837
Iteration 321/1000 | Loss: 0.00002837
Iteration 322/1000 | Loss: 0.00005386
Iteration 323/1000 | Loss: 0.00003280
Iteration 324/1000 | Loss: 0.00002839
Iteration 325/1000 | Loss: 0.00002839
Iteration 326/1000 | Loss: 0.00002839
Iteration 327/1000 | Loss: 0.00002839
Iteration 328/1000 | Loss: 0.00002839
Iteration 329/1000 | Loss: 0.00002839
Iteration 330/1000 | Loss: 0.00002839
Iteration 331/1000 | Loss: 0.00002838
Iteration 332/1000 | Loss: 0.00002838
Iteration 333/1000 | Loss: 0.00002838
Iteration 334/1000 | Loss: 0.00002838
Iteration 335/1000 | Loss: 0.00002838
Iteration 336/1000 | Loss: 0.00002838
Iteration 337/1000 | Loss: 0.00002838
Iteration 338/1000 | Loss: 0.00002838
Iteration 339/1000 | Loss: 0.00002838
Iteration 340/1000 | Loss: 0.00002838
Iteration 341/1000 | Loss: 0.00002838
Iteration 342/1000 | Loss: 0.00002838
Iteration 343/1000 | Loss: 0.00002838
Iteration 344/1000 | Loss: 0.00002838
Iteration 345/1000 | Loss: 0.00002838
Iteration 346/1000 | Loss: 0.00002838
Iteration 347/1000 | Loss: 0.00002838
Iteration 348/1000 | Loss: 0.00002837
Iteration 349/1000 | Loss: 0.00003884
Iteration 350/1000 | Loss: 0.00003151
Iteration 351/1000 | Loss: 0.00002839
Iteration 352/1000 | Loss: 0.00002839
Iteration 353/1000 | Loss: 0.00002839
Iteration 354/1000 | Loss: 0.00002839
Iteration 355/1000 | Loss: 0.00002839
Iteration 356/1000 | Loss: 0.00002839
Iteration 357/1000 | Loss: 0.00002839
Iteration 358/1000 | Loss: 0.00002839
Iteration 359/1000 | Loss: 0.00002839
Iteration 360/1000 | Loss: 0.00002839
Iteration 361/1000 | Loss: 0.00002838
Iteration 362/1000 | Loss: 0.00002838
Iteration 363/1000 | Loss: 0.00002838
Iteration 364/1000 | Loss: 0.00002838
Iteration 365/1000 | Loss: 0.00002838
Iteration 366/1000 | Loss: 0.00003459
Iteration 367/1000 | Loss: 0.00002839
Iteration 368/1000 | Loss: 0.00002839
Iteration 369/1000 | Loss: 0.00002839
Iteration 370/1000 | Loss: 0.00002839
Iteration 371/1000 | Loss: 0.00002839
Iteration 372/1000 | Loss: 0.00002839
Iteration 373/1000 | Loss: 0.00002839
Iteration 374/1000 | Loss: 0.00002838
Iteration 375/1000 | Loss: 0.00002838
Iteration 376/1000 | Loss: 0.00002838
Iteration 377/1000 | Loss: 0.00002838
Iteration 378/1000 | Loss: 0.00002838
Iteration 379/1000 | Loss: 0.00002838
Iteration 380/1000 | Loss: 0.00002838
Iteration 381/1000 | Loss: 0.00002838
Iteration 382/1000 | Loss: 0.00002838
Iteration 383/1000 | Loss: 0.00002838
Iteration 384/1000 | Loss: 0.00002838
Iteration 385/1000 | Loss: 0.00002838
Iteration 386/1000 | Loss: 0.00002838
Iteration 387/1000 | Loss: 0.00002838
Iteration 388/1000 | Loss: 0.00002837
Iteration 389/1000 | Loss: 0.00002837
Iteration 390/1000 | Loss: 0.00002837
Iteration 391/1000 | Loss: 0.00002837
Iteration 392/1000 | Loss: 0.00002837
Iteration 393/1000 | Loss: 0.00002837
Iteration 394/1000 | Loss: 0.00002837
Iteration 395/1000 | Loss: 0.00002837
Iteration 396/1000 | Loss: 0.00002837
Iteration 397/1000 | Loss: 0.00002837
Iteration 398/1000 | Loss: 0.00002837
Iteration 399/1000 | Loss: 0.00002837
Iteration 400/1000 | Loss: 0.00002836
Iteration 401/1000 | Loss: 0.00002836
Iteration 402/1000 | Loss: 0.00002836
Iteration 403/1000 | Loss: 0.00002836
Iteration 404/1000 | Loss: 0.00002835
Iteration 405/1000 | Loss: 0.00002835
Iteration 406/1000 | Loss: 0.00002835
Iteration 407/1000 | Loss: 0.00002835
Iteration 408/1000 | Loss: 0.00002835
Iteration 409/1000 | Loss: 0.00002835
Iteration 410/1000 | Loss: 0.00002835
Iteration 411/1000 | Loss: 0.00002835
Iteration 412/1000 | Loss: 0.00002835
Iteration 413/1000 | Loss: 0.00002835
Iteration 414/1000 | Loss: 0.00002835
Iteration 415/1000 | Loss: 0.00002835
Iteration 416/1000 | Loss: 0.00002835
Iteration 417/1000 | Loss: 0.00002835
Iteration 418/1000 | Loss: 0.00002835
Iteration 419/1000 | Loss: 0.00002835
Iteration 420/1000 | Loss: 0.00002835
Iteration 421/1000 | Loss: 0.00002835
Iteration 422/1000 | Loss: 0.00002835
Iteration 423/1000 | Loss: 0.00002835
Iteration 424/1000 | Loss: 0.00002835
Iteration 425/1000 | Loss: 0.00002835
Iteration 426/1000 | Loss: 0.00002835
Iteration 427/1000 | Loss: 0.00002835
Iteration 428/1000 | Loss: 0.00002835
Iteration 429/1000 | Loss: 0.00002835
Iteration 430/1000 | Loss: 0.00002835
Iteration 431/1000 | Loss: 0.00002835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 431. Stopping optimization.
Last 5 losses: [2.83479894278571e-05, 2.83479894278571e-05, 2.83479894278571e-05, 2.83479894278571e-05, 2.83479894278571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.83479894278571e-05

Optimization complete. Final v2v error: 4.544366359710693 mm

Highest mean error: 5.0531110763549805 mm for frame 0

Lowest mean error: 4.15941047668457 mm for frame 147

Saving results

Total time: 286.7821321487427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935647
Iteration 2/25 | Loss: 0.00180976
Iteration 3/25 | Loss: 0.00132477
Iteration 4/25 | Loss: 0.00127921
Iteration 5/25 | Loss: 0.00127185
Iteration 6/25 | Loss: 0.00127021
Iteration 7/25 | Loss: 0.00127021
Iteration 8/25 | Loss: 0.00127021
Iteration 9/25 | Loss: 0.00127021
Iteration 10/25 | Loss: 0.00127021
Iteration 11/25 | Loss: 0.00127021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012702055973932147, 0.0012702055973932147, 0.0012702055973932147, 0.0012702055973932147, 0.0012702055973932147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012702055973932147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26695955
Iteration 2/25 | Loss: 0.00112523
Iteration 3/25 | Loss: 0.00112523
Iteration 4/25 | Loss: 0.00112522
Iteration 5/25 | Loss: 0.00112522
Iteration 6/25 | Loss: 0.00112522
Iteration 7/25 | Loss: 0.00112522
Iteration 8/25 | Loss: 0.00112522
Iteration 9/25 | Loss: 0.00112522
Iteration 10/25 | Loss: 0.00112522
Iteration 11/25 | Loss: 0.00112522
Iteration 12/25 | Loss: 0.00112522
Iteration 13/25 | Loss: 0.00112522
Iteration 14/25 | Loss: 0.00112522
Iteration 15/25 | Loss: 0.00112522
Iteration 16/25 | Loss: 0.00112522
Iteration 17/25 | Loss: 0.00112522
Iteration 18/25 | Loss: 0.00112522
Iteration 19/25 | Loss: 0.00112522
Iteration 20/25 | Loss: 0.00112522
Iteration 21/25 | Loss: 0.00112522
Iteration 22/25 | Loss: 0.00112522
Iteration 23/25 | Loss: 0.00112522
Iteration 24/25 | Loss: 0.00112522
Iteration 25/25 | Loss: 0.00112522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112522
Iteration 2/1000 | Loss: 0.00003706
Iteration 3/1000 | Loss: 0.00003081
Iteration 4/1000 | Loss: 0.00002822
Iteration 5/1000 | Loss: 0.00002675
Iteration 6/1000 | Loss: 0.00002609
Iteration 7/1000 | Loss: 0.00002564
Iteration 8/1000 | Loss: 0.00002529
Iteration 9/1000 | Loss: 0.00002514
Iteration 10/1000 | Loss: 0.00002492
Iteration 11/1000 | Loss: 0.00002472
Iteration 12/1000 | Loss: 0.00002460
Iteration 13/1000 | Loss: 0.00002455
Iteration 14/1000 | Loss: 0.00002455
Iteration 15/1000 | Loss: 0.00002454
Iteration 16/1000 | Loss: 0.00002454
Iteration 17/1000 | Loss: 0.00002454
Iteration 18/1000 | Loss: 0.00002453
Iteration 19/1000 | Loss: 0.00002452
Iteration 20/1000 | Loss: 0.00002452
Iteration 21/1000 | Loss: 0.00002451
Iteration 22/1000 | Loss: 0.00002451
Iteration 23/1000 | Loss: 0.00002451
Iteration 24/1000 | Loss: 0.00002450
Iteration 25/1000 | Loss: 0.00002450
Iteration 26/1000 | Loss: 0.00002449
Iteration 27/1000 | Loss: 0.00002449
Iteration 28/1000 | Loss: 0.00002448
Iteration 29/1000 | Loss: 0.00002448
Iteration 30/1000 | Loss: 0.00002448
Iteration 31/1000 | Loss: 0.00002447
Iteration 32/1000 | Loss: 0.00002447
Iteration 33/1000 | Loss: 0.00002447
Iteration 34/1000 | Loss: 0.00002446
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002444
Iteration 37/1000 | Loss: 0.00002444
Iteration 38/1000 | Loss: 0.00002444
Iteration 39/1000 | Loss: 0.00002443
Iteration 40/1000 | Loss: 0.00002443
Iteration 41/1000 | Loss: 0.00002443
Iteration 42/1000 | Loss: 0.00002443
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00002442
Iteration 45/1000 | Loss: 0.00002442
Iteration 46/1000 | Loss: 0.00002442
Iteration 47/1000 | Loss: 0.00002442
Iteration 48/1000 | Loss: 0.00002441
Iteration 49/1000 | Loss: 0.00002441
Iteration 50/1000 | Loss: 0.00002441
Iteration 51/1000 | Loss: 0.00002440
Iteration 52/1000 | Loss: 0.00002440
Iteration 53/1000 | Loss: 0.00002440
Iteration 54/1000 | Loss: 0.00002440
Iteration 55/1000 | Loss: 0.00002440
Iteration 56/1000 | Loss: 0.00002440
Iteration 57/1000 | Loss: 0.00002440
Iteration 58/1000 | Loss: 0.00002440
Iteration 59/1000 | Loss: 0.00002440
Iteration 60/1000 | Loss: 0.00002440
Iteration 61/1000 | Loss: 0.00002440
Iteration 62/1000 | Loss: 0.00002440
Iteration 63/1000 | Loss: 0.00002439
Iteration 64/1000 | Loss: 0.00002439
Iteration 65/1000 | Loss: 0.00002439
Iteration 66/1000 | Loss: 0.00002439
Iteration 67/1000 | Loss: 0.00002438
Iteration 68/1000 | Loss: 0.00002438
Iteration 69/1000 | Loss: 0.00002438
Iteration 70/1000 | Loss: 0.00002437
Iteration 71/1000 | Loss: 0.00002437
Iteration 72/1000 | Loss: 0.00002437
Iteration 73/1000 | Loss: 0.00002437
Iteration 74/1000 | Loss: 0.00002436
Iteration 75/1000 | Loss: 0.00002436
Iteration 76/1000 | Loss: 0.00002436
Iteration 77/1000 | Loss: 0.00002436
Iteration 78/1000 | Loss: 0.00002436
Iteration 79/1000 | Loss: 0.00002436
Iteration 80/1000 | Loss: 0.00002435
Iteration 81/1000 | Loss: 0.00002435
Iteration 82/1000 | Loss: 0.00002435
Iteration 83/1000 | Loss: 0.00002435
Iteration 84/1000 | Loss: 0.00002435
Iteration 85/1000 | Loss: 0.00002435
Iteration 86/1000 | Loss: 0.00002435
Iteration 87/1000 | Loss: 0.00002435
Iteration 88/1000 | Loss: 0.00002435
Iteration 89/1000 | Loss: 0.00002435
Iteration 90/1000 | Loss: 0.00002435
Iteration 91/1000 | Loss: 0.00002435
Iteration 92/1000 | Loss: 0.00002435
Iteration 93/1000 | Loss: 0.00002435
Iteration 94/1000 | Loss: 0.00002434
Iteration 95/1000 | Loss: 0.00002434
Iteration 96/1000 | Loss: 0.00002434
Iteration 97/1000 | Loss: 0.00002434
Iteration 98/1000 | Loss: 0.00002434
Iteration 99/1000 | Loss: 0.00002434
Iteration 100/1000 | Loss: 0.00002434
Iteration 101/1000 | Loss: 0.00002434
Iteration 102/1000 | Loss: 0.00002434
Iteration 103/1000 | Loss: 0.00002434
Iteration 104/1000 | Loss: 0.00002434
Iteration 105/1000 | Loss: 0.00002434
Iteration 106/1000 | Loss: 0.00002433
Iteration 107/1000 | Loss: 0.00002433
Iteration 108/1000 | Loss: 0.00002433
Iteration 109/1000 | Loss: 0.00002433
Iteration 110/1000 | Loss: 0.00002433
Iteration 111/1000 | Loss: 0.00002433
Iteration 112/1000 | Loss: 0.00002433
Iteration 113/1000 | Loss: 0.00002433
Iteration 114/1000 | Loss: 0.00002433
Iteration 115/1000 | Loss: 0.00002433
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002433
Iteration 118/1000 | Loss: 0.00002433
Iteration 119/1000 | Loss: 0.00002433
Iteration 120/1000 | Loss: 0.00002433
Iteration 121/1000 | Loss: 0.00002433
Iteration 122/1000 | Loss: 0.00002433
Iteration 123/1000 | Loss: 0.00002433
Iteration 124/1000 | Loss: 0.00002433
Iteration 125/1000 | Loss: 0.00002433
Iteration 126/1000 | Loss: 0.00002433
Iteration 127/1000 | Loss: 0.00002433
Iteration 128/1000 | Loss: 0.00002433
Iteration 129/1000 | Loss: 0.00002433
Iteration 130/1000 | Loss: 0.00002433
Iteration 131/1000 | Loss: 0.00002433
Iteration 132/1000 | Loss: 0.00002433
Iteration 133/1000 | Loss: 0.00002433
Iteration 134/1000 | Loss: 0.00002433
Iteration 135/1000 | Loss: 0.00002433
Iteration 136/1000 | Loss: 0.00002433
Iteration 137/1000 | Loss: 0.00002433
Iteration 138/1000 | Loss: 0.00002433
Iteration 139/1000 | Loss: 0.00002433
Iteration 140/1000 | Loss: 0.00002433
Iteration 141/1000 | Loss: 0.00002433
Iteration 142/1000 | Loss: 0.00002433
Iteration 143/1000 | Loss: 0.00002433
Iteration 144/1000 | Loss: 0.00002433
Iteration 145/1000 | Loss: 0.00002433
Iteration 146/1000 | Loss: 0.00002433
Iteration 147/1000 | Loss: 0.00002433
Iteration 148/1000 | Loss: 0.00002433
Iteration 149/1000 | Loss: 0.00002433
Iteration 150/1000 | Loss: 0.00002433
Iteration 151/1000 | Loss: 0.00002433
Iteration 152/1000 | Loss: 0.00002433
Iteration 153/1000 | Loss: 0.00002433
Iteration 154/1000 | Loss: 0.00002433
Iteration 155/1000 | Loss: 0.00002433
Iteration 156/1000 | Loss: 0.00002433
Iteration 157/1000 | Loss: 0.00002433
Iteration 158/1000 | Loss: 0.00002433
Iteration 159/1000 | Loss: 0.00002433
Iteration 160/1000 | Loss: 0.00002433
Iteration 161/1000 | Loss: 0.00002433
Iteration 162/1000 | Loss: 0.00002433
Iteration 163/1000 | Loss: 0.00002433
Iteration 164/1000 | Loss: 0.00002433
Iteration 165/1000 | Loss: 0.00002433
Iteration 166/1000 | Loss: 0.00002433
Iteration 167/1000 | Loss: 0.00002433
Iteration 168/1000 | Loss: 0.00002433
Iteration 169/1000 | Loss: 0.00002433
Iteration 170/1000 | Loss: 0.00002433
Iteration 171/1000 | Loss: 0.00002433
Iteration 172/1000 | Loss: 0.00002433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.4325585400220007e-05, 2.4325585400220007e-05, 2.4325585400220007e-05, 2.4325585400220007e-05, 2.4325585400220007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4325585400220007e-05

Optimization complete. Final v2v error: 4.348860740661621 mm

Highest mean error: 5.0970540046691895 mm for frame 107

Lowest mean error: 3.9282338619232178 mm for frame 47

Saving results

Total time: 39.45677042007446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410842
Iteration 2/25 | Loss: 0.00132985
Iteration 3/25 | Loss: 0.00126081
Iteration 4/25 | Loss: 0.00124961
Iteration 5/25 | Loss: 0.00124626
Iteration 6/25 | Loss: 0.00124500
Iteration 7/25 | Loss: 0.00124496
Iteration 8/25 | Loss: 0.00124496
Iteration 9/25 | Loss: 0.00124496
Iteration 10/25 | Loss: 0.00124496
Iteration 11/25 | Loss: 0.00124496
Iteration 12/25 | Loss: 0.00124496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012449633795768023, 0.0012449633795768023, 0.0012449633795768023, 0.0012449633795768023, 0.0012449633795768023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012449633795768023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51159072
Iteration 2/25 | Loss: 0.00096306
Iteration 3/25 | Loss: 0.00096306
Iteration 4/25 | Loss: 0.00096306
Iteration 5/25 | Loss: 0.00096306
Iteration 6/25 | Loss: 0.00096306
Iteration 7/25 | Loss: 0.00096306
Iteration 8/25 | Loss: 0.00096306
Iteration 9/25 | Loss: 0.00096306
Iteration 10/25 | Loss: 0.00096306
Iteration 11/25 | Loss: 0.00096306
Iteration 12/25 | Loss: 0.00096306
Iteration 13/25 | Loss: 0.00096306
Iteration 14/25 | Loss: 0.00096306
Iteration 15/25 | Loss: 0.00096306
Iteration 16/25 | Loss: 0.00096306
Iteration 17/25 | Loss: 0.00096306
Iteration 18/25 | Loss: 0.00096306
Iteration 19/25 | Loss: 0.00096306
Iteration 20/25 | Loss: 0.00096306
Iteration 21/25 | Loss: 0.00096306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009630562271922827, 0.0009630562271922827, 0.0009630562271922827, 0.0009630562271922827, 0.0009630562271922827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009630562271922827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096306
Iteration 2/1000 | Loss: 0.00002795
Iteration 3/1000 | Loss: 0.00002316
Iteration 4/1000 | Loss: 0.00002187
Iteration 5/1000 | Loss: 0.00002116
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00002000
Iteration 11/1000 | Loss: 0.00001994
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00001990
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001985
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001975
Iteration 18/1000 | Loss: 0.00001975
Iteration 19/1000 | Loss: 0.00001975
Iteration 20/1000 | Loss: 0.00001975
Iteration 21/1000 | Loss: 0.00001975
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001974
Iteration 26/1000 | Loss: 0.00001974
Iteration 27/1000 | Loss: 0.00001974
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001971
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001971
Iteration 36/1000 | Loss: 0.00001971
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001971
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001970
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001968
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001965
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001964
Iteration 63/1000 | Loss: 0.00001964
Iteration 64/1000 | Loss: 0.00001964
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00001964
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001964
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001963
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001961
Iteration 103/1000 | Loss: 0.00001961
Iteration 104/1000 | Loss: 0.00001961
Iteration 105/1000 | Loss: 0.00001961
Iteration 106/1000 | Loss: 0.00001961
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001961
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001960
Iteration 114/1000 | Loss: 0.00001960
Iteration 115/1000 | Loss: 0.00001960
Iteration 116/1000 | Loss: 0.00001960
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001960
Iteration 121/1000 | Loss: 0.00001960
Iteration 122/1000 | Loss: 0.00001960
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00001960
Iteration 125/1000 | Loss: 0.00001960
Iteration 126/1000 | Loss: 0.00001960
Iteration 127/1000 | Loss: 0.00001960
Iteration 128/1000 | Loss: 0.00001960
Iteration 129/1000 | Loss: 0.00001959
Iteration 130/1000 | Loss: 0.00001959
Iteration 131/1000 | Loss: 0.00001959
Iteration 132/1000 | Loss: 0.00001959
Iteration 133/1000 | Loss: 0.00001959
Iteration 134/1000 | Loss: 0.00001959
Iteration 135/1000 | Loss: 0.00001959
Iteration 136/1000 | Loss: 0.00001959
Iteration 137/1000 | Loss: 0.00001959
Iteration 138/1000 | Loss: 0.00001959
Iteration 139/1000 | Loss: 0.00001959
Iteration 140/1000 | Loss: 0.00001959
Iteration 141/1000 | Loss: 0.00001959
Iteration 142/1000 | Loss: 0.00001959
Iteration 143/1000 | Loss: 0.00001959
Iteration 144/1000 | Loss: 0.00001959
Iteration 145/1000 | Loss: 0.00001959
Iteration 146/1000 | Loss: 0.00001959
Iteration 147/1000 | Loss: 0.00001959
Iteration 148/1000 | Loss: 0.00001959
Iteration 149/1000 | Loss: 0.00001959
Iteration 150/1000 | Loss: 0.00001959
Iteration 151/1000 | Loss: 0.00001959
Iteration 152/1000 | Loss: 0.00001959
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001959
Iteration 155/1000 | Loss: 0.00001959
Iteration 156/1000 | Loss: 0.00001959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.959005749085918e-05, 1.959005749085918e-05, 1.959005749085918e-05, 1.959005749085918e-05, 1.959005749085918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.959005749085918e-05

Optimization complete. Final v2v error: 3.8779985904693604 mm

Highest mean error: 4.1485700607299805 mm for frame 30

Lowest mean error: 3.6344871520996094 mm for frame 161

Saving results

Total time: 32.63546276092529
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01150746
Iteration 2/25 | Loss: 0.00177910
Iteration 3/25 | Loss: 0.00145551
Iteration 4/25 | Loss: 0.00130379
Iteration 5/25 | Loss: 0.00126124
Iteration 6/25 | Loss: 0.00125355
Iteration 7/25 | Loss: 0.00126327
Iteration 8/25 | Loss: 0.00125417
Iteration 9/25 | Loss: 0.00125776
Iteration 10/25 | Loss: 0.00125054
Iteration 11/25 | Loss: 0.00124872
Iteration 12/25 | Loss: 0.00125203
Iteration 13/25 | Loss: 0.00124919
Iteration 14/25 | Loss: 0.00124112
Iteration 15/25 | Loss: 0.00123979
Iteration 16/25 | Loss: 0.00123944
Iteration 17/25 | Loss: 0.00123918
Iteration 18/25 | Loss: 0.00123905
Iteration 19/25 | Loss: 0.00123901
Iteration 20/25 | Loss: 0.00123899
Iteration 21/25 | Loss: 0.00123899
Iteration 22/25 | Loss: 0.00123898
Iteration 23/25 | Loss: 0.00123898
Iteration 24/25 | Loss: 0.00123898
Iteration 25/25 | Loss: 0.00123898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49734426
Iteration 2/25 | Loss: 0.00092517
Iteration 3/25 | Loss: 0.00092517
Iteration 4/25 | Loss: 0.00092517
Iteration 5/25 | Loss: 0.00092517
Iteration 6/25 | Loss: 0.00092517
Iteration 7/25 | Loss: 0.00092516
Iteration 8/25 | Loss: 0.00092516
Iteration 9/25 | Loss: 0.00092516
Iteration 10/25 | Loss: 0.00092516
Iteration 11/25 | Loss: 0.00092516
Iteration 12/25 | Loss: 0.00092516
Iteration 13/25 | Loss: 0.00092516
Iteration 14/25 | Loss: 0.00092516
Iteration 15/25 | Loss: 0.00092516
Iteration 16/25 | Loss: 0.00092516
Iteration 17/25 | Loss: 0.00092516
Iteration 18/25 | Loss: 0.00092516
Iteration 19/25 | Loss: 0.00092516
Iteration 20/25 | Loss: 0.00092516
Iteration 21/25 | Loss: 0.00092516
Iteration 22/25 | Loss: 0.00092516
Iteration 23/25 | Loss: 0.00092516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009251643787138164, 0.0009251643787138164, 0.0009251643787138164, 0.0009251643787138164, 0.0009251643787138164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009251643787138164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092516
Iteration 2/1000 | Loss: 0.00003245
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002333
Iteration 5/1000 | Loss: 0.00002238
Iteration 6/1000 | Loss: 0.00002166
Iteration 7/1000 | Loss: 0.00002112
Iteration 8/1000 | Loss: 0.00002082
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002062
Iteration 11/1000 | Loss: 0.00002061
Iteration 12/1000 | Loss: 0.00002061
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00002055
Iteration 15/1000 | Loss: 0.00002054
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00002047
Iteration 18/1000 | Loss: 0.00002034
Iteration 19/1000 | Loss: 0.00002033
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002031
Iteration 22/1000 | Loss: 0.00002030
Iteration 23/1000 | Loss: 0.00002030
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00002029
Iteration 26/1000 | Loss: 0.00002028
Iteration 27/1000 | Loss: 0.00002025
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002025
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00002020
Iteration 34/1000 | Loss: 0.00002020
Iteration 35/1000 | Loss: 0.00002020
Iteration 36/1000 | Loss: 0.00002020
Iteration 37/1000 | Loss: 0.00002019
Iteration 38/1000 | Loss: 0.00002019
Iteration 39/1000 | Loss: 0.00002019
Iteration 40/1000 | Loss: 0.00002019
Iteration 41/1000 | Loss: 0.00002019
Iteration 42/1000 | Loss: 0.00002018
Iteration 43/1000 | Loss: 0.00002018
Iteration 44/1000 | Loss: 0.00002018
Iteration 45/1000 | Loss: 0.00002016
Iteration 46/1000 | Loss: 0.00002016
Iteration 47/1000 | Loss: 0.00002015
Iteration 48/1000 | Loss: 0.00002015
Iteration 49/1000 | Loss: 0.00002015
Iteration 50/1000 | Loss: 0.00002014
Iteration 51/1000 | Loss: 0.00002014
Iteration 52/1000 | Loss: 0.00002014
Iteration 53/1000 | Loss: 0.00002014
Iteration 54/1000 | Loss: 0.00002013
Iteration 55/1000 | Loss: 0.00002013
Iteration 56/1000 | Loss: 0.00002013
Iteration 57/1000 | Loss: 0.00002012
Iteration 58/1000 | Loss: 0.00002012
Iteration 59/1000 | Loss: 0.00002012
Iteration 60/1000 | Loss: 0.00002012
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002012
Iteration 63/1000 | Loss: 0.00002011
Iteration 64/1000 | Loss: 0.00002011
Iteration 65/1000 | Loss: 0.00002011
Iteration 66/1000 | Loss: 0.00002011
Iteration 67/1000 | Loss: 0.00002011
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002010
Iteration 71/1000 | Loss: 0.00002010
Iteration 72/1000 | Loss: 0.00002010
Iteration 73/1000 | Loss: 0.00002010
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002009
Iteration 79/1000 | Loss: 0.00002008
Iteration 80/1000 | Loss: 0.00002008
Iteration 81/1000 | Loss: 0.00002008
Iteration 82/1000 | Loss: 0.00002008
Iteration 83/1000 | Loss: 0.00002008
Iteration 84/1000 | Loss: 0.00002008
Iteration 85/1000 | Loss: 0.00002007
Iteration 86/1000 | Loss: 0.00002007
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002007
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00002007
Iteration 93/1000 | Loss: 0.00002007
Iteration 94/1000 | Loss: 0.00002007
Iteration 95/1000 | Loss: 0.00002007
Iteration 96/1000 | Loss: 0.00002006
Iteration 97/1000 | Loss: 0.00002006
Iteration 98/1000 | Loss: 0.00002006
Iteration 99/1000 | Loss: 0.00002006
Iteration 100/1000 | Loss: 0.00002006
Iteration 101/1000 | Loss: 0.00002006
Iteration 102/1000 | Loss: 0.00002006
Iteration 103/1000 | Loss: 0.00002006
Iteration 104/1000 | Loss: 0.00002006
Iteration 105/1000 | Loss: 0.00002006
Iteration 106/1000 | Loss: 0.00002006
Iteration 107/1000 | Loss: 0.00002006
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002006
Iteration 115/1000 | Loss: 0.00002006
Iteration 116/1000 | Loss: 0.00002006
Iteration 117/1000 | Loss: 0.00002006
Iteration 118/1000 | Loss: 0.00002006
Iteration 119/1000 | Loss: 0.00002006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.0058743757545017e-05, 2.0058743757545017e-05, 2.0058743757545017e-05, 2.0058743757545017e-05, 2.0058743757545017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0058743757545017e-05

Optimization complete. Final v2v error: 3.9028468132019043 mm

Highest mean error: 4.418204307556152 mm for frame 77

Lowest mean error: 3.5821168422698975 mm for frame 120

Saving results

Total time: 55.88225078582764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01194401
Iteration 2/25 | Loss: 0.00183637
Iteration 3/25 | Loss: 0.00160598
Iteration 4/25 | Loss: 0.00126601
Iteration 5/25 | Loss: 0.00122419
Iteration 6/25 | Loss: 0.00121651
Iteration 7/25 | Loss: 0.00120559
Iteration 8/25 | Loss: 0.00120422
Iteration 9/25 | Loss: 0.00120576
Iteration 10/25 | Loss: 0.00119865
Iteration 11/25 | Loss: 0.00119807
Iteration 12/25 | Loss: 0.00119933
Iteration 13/25 | Loss: 0.00119799
Iteration 14/25 | Loss: 0.00119925
Iteration 15/25 | Loss: 0.00119859
Iteration 16/25 | Loss: 0.00119936
Iteration 17/25 | Loss: 0.00119777
Iteration 18/25 | Loss: 0.00119752
Iteration 19/25 | Loss: 0.00119738
Iteration 20/25 | Loss: 0.00119726
Iteration 21/25 | Loss: 0.00119724
Iteration 22/25 | Loss: 0.00119724
Iteration 23/25 | Loss: 0.00119724
Iteration 24/25 | Loss: 0.00119724
Iteration 25/25 | Loss: 0.00119724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.41412354
Iteration 2/25 | Loss: 0.00092754
Iteration 3/25 | Loss: 0.00092754
Iteration 4/25 | Loss: 0.00092754
Iteration 5/25 | Loss: 0.00092754
Iteration 6/25 | Loss: 0.00092754
Iteration 7/25 | Loss: 0.00092754
Iteration 8/25 | Loss: 0.00092754
Iteration 9/25 | Loss: 0.00092754
Iteration 10/25 | Loss: 0.00092754
Iteration 11/25 | Loss: 0.00092754
Iteration 12/25 | Loss: 0.00092754
Iteration 13/25 | Loss: 0.00092754
Iteration 14/25 | Loss: 0.00092754
Iteration 15/25 | Loss: 0.00092754
Iteration 16/25 | Loss: 0.00092754
Iteration 17/25 | Loss: 0.00092754
Iteration 18/25 | Loss: 0.00092754
Iteration 19/25 | Loss: 0.00092754
Iteration 20/25 | Loss: 0.00092754
Iteration 21/25 | Loss: 0.00092754
Iteration 22/25 | Loss: 0.00092754
Iteration 23/25 | Loss: 0.00092754
Iteration 24/25 | Loss: 0.00092754
Iteration 25/25 | Loss: 0.00092754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092754
Iteration 2/1000 | Loss: 0.00003333
Iteration 3/1000 | Loss: 0.00002831
Iteration 4/1000 | Loss: 0.00002672
Iteration 5/1000 | Loss: 0.00002593
Iteration 6/1000 | Loss: 0.00002546
Iteration 7/1000 | Loss: 0.00002524
Iteration 8/1000 | Loss: 0.00002500
Iteration 9/1000 | Loss: 0.00002498
Iteration 10/1000 | Loss: 0.00002480
Iteration 11/1000 | Loss: 0.00002480
Iteration 12/1000 | Loss: 0.00002479
Iteration 13/1000 | Loss: 0.00002478
Iteration 14/1000 | Loss: 0.00002474
Iteration 15/1000 | Loss: 0.00002474
Iteration 16/1000 | Loss: 0.00002474
Iteration 17/1000 | Loss: 0.00002473
Iteration 18/1000 | Loss: 0.00002473
Iteration 19/1000 | Loss: 0.00002473
Iteration 20/1000 | Loss: 0.00002473
Iteration 21/1000 | Loss: 0.00002473
Iteration 22/1000 | Loss: 0.00002472
Iteration 23/1000 | Loss: 0.00002470
Iteration 24/1000 | Loss: 0.00002470
Iteration 25/1000 | Loss: 0.00002469
Iteration 26/1000 | Loss: 0.00002469
Iteration 27/1000 | Loss: 0.00002469
Iteration 28/1000 | Loss: 0.00002469
Iteration 29/1000 | Loss: 0.00002469
Iteration 30/1000 | Loss: 0.00002468
Iteration 31/1000 | Loss: 0.00002468
Iteration 32/1000 | Loss: 0.00002468
Iteration 33/1000 | Loss: 0.00002467
Iteration 34/1000 | Loss: 0.00002467
Iteration 35/1000 | Loss: 0.00002466
Iteration 36/1000 | Loss: 0.00002466
Iteration 37/1000 | Loss: 0.00002466
Iteration 38/1000 | Loss: 0.00002465
Iteration 39/1000 | Loss: 0.00002465
Iteration 40/1000 | Loss: 0.00002465
Iteration 41/1000 | Loss: 0.00002463
Iteration 42/1000 | Loss: 0.00002462
Iteration 43/1000 | Loss: 0.00002462
Iteration 44/1000 | Loss: 0.00002462
Iteration 45/1000 | Loss: 0.00002461
Iteration 46/1000 | Loss: 0.00002461
Iteration 47/1000 | Loss: 0.00002461
Iteration 48/1000 | Loss: 0.00002461
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002460
Iteration 52/1000 | Loss: 0.00002460
Iteration 53/1000 | Loss: 0.00002460
Iteration 54/1000 | Loss: 0.00002460
Iteration 55/1000 | Loss: 0.00002460
Iteration 56/1000 | Loss: 0.00002460
Iteration 57/1000 | Loss: 0.00002460
Iteration 58/1000 | Loss: 0.00002459
Iteration 59/1000 | Loss: 0.00002459
Iteration 60/1000 | Loss: 0.00002459
Iteration 61/1000 | Loss: 0.00002459
Iteration 62/1000 | Loss: 0.00002459
Iteration 63/1000 | Loss: 0.00002459
Iteration 64/1000 | Loss: 0.00002459
Iteration 65/1000 | Loss: 0.00002458
Iteration 66/1000 | Loss: 0.00002458
Iteration 67/1000 | Loss: 0.00002458
Iteration 68/1000 | Loss: 0.00002458
Iteration 69/1000 | Loss: 0.00002458
Iteration 70/1000 | Loss: 0.00002458
Iteration 71/1000 | Loss: 0.00002458
Iteration 72/1000 | Loss: 0.00002458
Iteration 73/1000 | Loss: 0.00002458
Iteration 74/1000 | Loss: 0.00002458
Iteration 75/1000 | Loss: 0.00002458
Iteration 76/1000 | Loss: 0.00002457
Iteration 77/1000 | Loss: 0.00002457
Iteration 78/1000 | Loss: 0.00002457
Iteration 79/1000 | Loss: 0.00002457
Iteration 80/1000 | Loss: 0.00002456
Iteration 81/1000 | Loss: 0.00002456
Iteration 82/1000 | Loss: 0.00002456
Iteration 83/1000 | Loss: 0.00002456
Iteration 84/1000 | Loss: 0.00002456
Iteration 85/1000 | Loss: 0.00002456
Iteration 86/1000 | Loss: 0.00002456
Iteration 87/1000 | Loss: 0.00002456
Iteration 88/1000 | Loss: 0.00002456
Iteration 89/1000 | Loss: 0.00002456
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002456
Iteration 92/1000 | Loss: 0.00002456
Iteration 93/1000 | Loss: 0.00002456
Iteration 94/1000 | Loss: 0.00002456
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002455
Iteration 98/1000 | Loss: 0.00002455
Iteration 99/1000 | Loss: 0.00002455
Iteration 100/1000 | Loss: 0.00002455
Iteration 101/1000 | Loss: 0.00002455
Iteration 102/1000 | Loss: 0.00002455
Iteration 103/1000 | Loss: 0.00002455
Iteration 104/1000 | Loss: 0.00002455
Iteration 105/1000 | Loss: 0.00002455
Iteration 106/1000 | Loss: 0.00002455
Iteration 107/1000 | Loss: 0.00002455
Iteration 108/1000 | Loss: 0.00002455
Iteration 109/1000 | Loss: 0.00002455
Iteration 110/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.4548606234020554e-05, 2.4548606234020554e-05, 2.4548606234020554e-05, 2.4548606234020554e-05, 2.4548606234020554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4548606234020554e-05

Optimization complete. Final v2v error: 4.335096836090088 mm

Highest mean error: 10.186736106872559 mm for frame 177

Lowest mean error: 4.0922369956970215 mm for frame 86

Saving results

Total time: 60.23467993736267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424911
Iteration 2/25 | Loss: 0.00140356
Iteration 3/25 | Loss: 0.00133899
Iteration 4/25 | Loss: 0.00132704
Iteration 5/25 | Loss: 0.00132251
Iteration 6/25 | Loss: 0.00132210
Iteration 7/25 | Loss: 0.00132210
Iteration 8/25 | Loss: 0.00132210
Iteration 9/25 | Loss: 0.00132210
Iteration 10/25 | Loss: 0.00132210
Iteration 11/25 | Loss: 0.00132210
Iteration 12/25 | Loss: 0.00132210
Iteration 13/25 | Loss: 0.00132210
Iteration 14/25 | Loss: 0.00132210
Iteration 15/25 | Loss: 0.00132210
Iteration 16/25 | Loss: 0.00132210
Iteration 17/25 | Loss: 0.00132210
Iteration 18/25 | Loss: 0.00132210
Iteration 19/25 | Loss: 0.00132210
Iteration 20/25 | Loss: 0.00132210
Iteration 21/25 | Loss: 0.00132210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013221001718193293, 0.0013221001718193293, 0.0013221001718193293, 0.0013221001718193293, 0.0013221001718193293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013221001718193293

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64351070
Iteration 2/25 | Loss: 0.00131531
Iteration 3/25 | Loss: 0.00131530
Iteration 4/25 | Loss: 0.00131530
Iteration 5/25 | Loss: 0.00131530
Iteration 6/25 | Loss: 0.00131530
Iteration 7/25 | Loss: 0.00131530
Iteration 8/25 | Loss: 0.00131530
Iteration 9/25 | Loss: 0.00131530
Iteration 10/25 | Loss: 0.00131530
Iteration 11/25 | Loss: 0.00131530
Iteration 12/25 | Loss: 0.00131530
Iteration 13/25 | Loss: 0.00131530
Iteration 14/25 | Loss: 0.00131530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013153025647625327, 0.0013153025647625327, 0.0013153025647625327, 0.0013153025647625327, 0.0013153025647625327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013153025647625327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131530
Iteration 2/1000 | Loss: 0.00003961
Iteration 3/1000 | Loss: 0.00003040
Iteration 4/1000 | Loss: 0.00002694
Iteration 5/1000 | Loss: 0.00002593
Iteration 6/1000 | Loss: 0.00002476
Iteration 7/1000 | Loss: 0.00002423
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002341
Iteration 10/1000 | Loss: 0.00002320
Iteration 11/1000 | Loss: 0.00002307
Iteration 12/1000 | Loss: 0.00002305
Iteration 13/1000 | Loss: 0.00002285
Iteration 14/1000 | Loss: 0.00002281
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002271
Iteration 17/1000 | Loss: 0.00002271
Iteration 18/1000 | Loss: 0.00002270
Iteration 19/1000 | Loss: 0.00002270
Iteration 20/1000 | Loss: 0.00002269
Iteration 21/1000 | Loss: 0.00002268
Iteration 22/1000 | Loss: 0.00002268
Iteration 23/1000 | Loss: 0.00002268
Iteration 24/1000 | Loss: 0.00002268
Iteration 25/1000 | Loss: 0.00002267
Iteration 26/1000 | Loss: 0.00002267
Iteration 27/1000 | Loss: 0.00002267
Iteration 28/1000 | Loss: 0.00002266
Iteration 29/1000 | Loss: 0.00002266
Iteration 30/1000 | Loss: 0.00002265
Iteration 31/1000 | Loss: 0.00002265
Iteration 32/1000 | Loss: 0.00002264
Iteration 33/1000 | Loss: 0.00002263
Iteration 34/1000 | Loss: 0.00002263
Iteration 35/1000 | Loss: 0.00002263
Iteration 36/1000 | Loss: 0.00002262
Iteration 37/1000 | Loss: 0.00002262
Iteration 38/1000 | Loss: 0.00002262
Iteration 39/1000 | Loss: 0.00002262
Iteration 40/1000 | Loss: 0.00002262
Iteration 41/1000 | Loss: 0.00002262
Iteration 42/1000 | Loss: 0.00002262
Iteration 43/1000 | Loss: 0.00002262
Iteration 44/1000 | Loss: 0.00002262
Iteration 45/1000 | Loss: 0.00002262
Iteration 46/1000 | Loss: 0.00002262
Iteration 47/1000 | Loss: 0.00002261
Iteration 48/1000 | Loss: 0.00002261
Iteration 49/1000 | Loss: 0.00002261
Iteration 50/1000 | Loss: 0.00002260
Iteration 51/1000 | Loss: 0.00002260
Iteration 52/1000 | Loss: 0.00002259
Iteration 53/1000 | Loss: 0.00002259
Iteration 54/1000 | Loss: 0.00002259
Iteration 55/1000 | Loss: 0.00002259
Iteration 56/1000 | Loss: 0.00002259
Iteration 57/1000 | Loss: 0.00002259
Iteration 58/1000 | Loss: 0.00002259
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002259
Iteration 61/1000 | Loss: 0.00002258
Iteration 62/1000 | Loss: 0.00002258
Iteration 63/1000 | Loss: 0.00002258
Iteration 64/1000 | Loss: 0.00002257
Iteration 65/1000 | Loss: 0.00002257
Iteration 66/1000 | Loss: 0.00002257
Iteration 67/1000 | Loss: 0.00002257
Iteration 68/1000 | Loss: 0.00002257
Iteration 69/1000 | Loss: 0.00002257
Iteration 70/1000 | Loss: 0.00002256
Iteration 71/1000 | Loss: 0.00002256
Iteration 72/1000 | Loss: 0.00002256
Iteration 73/1000 | Loss: 0.00002256
Iteration 74/1000 | Loss: 0.00002256
Iteration 75/1000 | Loss: 0.00002256
Iteration 76/1000 | Loss: 0.00002256
Iteration 77/1000 | Loss: 0.00002256
Iteration 78/1000 | Loss: 0.00002256
Iteration 79/1000 | Loss: 0.00002256
Iteration 80/1000 | Loss: 0.00002256
Iteration 81/1000 | Loss: 0.00002255
Iteration 82/1000 | Loss: 0.00002255
Iteration 83/1000 | Loss: 0.00002255
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002255
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002254
Iteration 90/1000 | Loss: 0.00002254
Iteration 91/1000 | Loss: 0.00002254
Iteration 92/1000 | Loss: 0.00002254
Iteration 93/1000 | Loss: 0.00002254
Iteration 94/1000 | Loss: 0.00002254
Iteration 95/1000 | Loss: 0.00002254
Iteration 96/1000 | Loss: 0.00002254
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00002253
Iteration 99/1000 | Loss: 0.00002253
Iteration 100/1000 | Loss: 0.00002253
Iteration 101/1000 | Loss: 0.00002253
Iteration 102/1000 | Loss: 0.00002253
Iteration 103/1000 | Loss: 0.00002253
Iteration 104/1000 | Loss: 0.00002253
Iteration 105/1000 | Loss: 0.00002253
Iteration 106/1000 | Loss: 0.00002253
Iteration 107/1000 | Loss: 0.00002253
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002252
Iteration 111/1000 | Loss: 0.00002252
Iteration 112/1000 | Loss: 0.00002252
Iteration 113/1000 | Loss: 0.00002252
Iteration 114/1000 | Loss: 0.00002252
Iteration 115/1000 | Loss: 0.00002252
Iteration 116/1000 | Loss: 0.00002252
Iteration 117/1000 | Loss: 0.00002252
Iteration 118/1000 | Loss: 0.00002252
Iteration 119/1000 | Loss: 0.00002252
Iteration 120/1000 | Loss: 0.00002252
Iteration 121/1000 | Loss: 0.00002252
Iteration 122/1000 | Loss: 0.00002252
Iteration 123/1000 | Loss: 0.00002252
Iteration 124/1000 | Loss: 0.00002252
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002252
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002252
Iteration 130/1000 | Loss: 0.00002252
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002252
Iteration 133/1000 | Loss: 0.00002252
Iteration 134/1000 | Loss: 0.00002252
Iteration 135/1000 | Loss: 0.00002252
Iteration 136/1000 | Loss: 0.00002252
Iteration 137/1000 | Loss: 0.00002252
Iteration 138/1000 | Loss: 0.00002252
Iteration 139/1000 | Loss: 0.00002252
Iteration 140/1000 | Loss: 0.00002252
Iteration 141/1000 | Loss: 0.00002252
Iteration 142/1000 | Loss: 0.00002252
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.251814294140786e-05, 2.251814294140786e-05, 2.251814294140786e-05, 2.251814294140786e-05, 2.251814294140786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.251814294140786e-05

Optimization complete. Final v2v error: 4.21091365814209 mm

Highest mean error: 4.5186238288879395 mm for frame 243

Lowest mean error: 3.9025321006774902 mm for frame 156

Saving results

Total time: 39.14159274101257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096741
Iteration 2/25 | Loss: 0.00385679
Iteration 3/25 | Loss: 0.00262855
Iteration 4/25 | Loss: 0.00234900
Iteration 5/25 | Loss: 0.00211657
Iteration 6/25 | Loss: 0.00203070
Iteration 7/25 | Loss: 0.00200184
Iteration 8/25 | Loss: 0.00201948
Iteration 9/25 | Loss: 0.00198256
Iteration 10/25 | Loss: 0.00195697
Iteration 11/25 | Loss: 0.00189424
Iteration 12/25 | Loss: 0.00185932
Iteration 13/25 | Loss: 0.00187492
Iteration 14/25 | Loss: 0.00186701
Iteration 15/25 | Loss: 0.00185567
Iteration 16/25 | Loss: 0.00184260
Iteration 17/25 | Loss: 0.00183400
Iteration 18/25 | Loss: 0.00182282
Iteration 19/25 | Loss: 0.00181670
Iteration 20/25 | Loss: 0.00181148
Iteration 21/25 | Loss: 0.00181234
Iteration 22/25 | Loss: 0.00181218
Iteration 23/25 | Loss: 0.00180352
Iteration 24/25 | Loss: 0.00180671
Iteration 25/25 | Loss: 0.00180485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41878653
Iteration 2/25 | Loss: 0.00646319
Iteration 3/25 | Loss: 0.00591291
Iteration 4/25 | Loss: 0.00591291
Iteration 5/25 | Loss: 0.00591291
Iteration 6/25 | Loss: 0.00591291
Iteration 7/25 | Loss: 0.00591291
Iteration 8/25 | Loss: 0.00591291
Iteration 9/25 | Loss: 0.00591291
Iteration 10/25 | Loss: 0.00591291
Iteration 11/25 | Loss: 0.00591291
Iteration 12/25 | Loss: 0.00591291
Iteration 13/25 | Loss: 0.00591291
Iteration 14/25 | Loss: 0.00591291
Iteration 15/25 | Loss: 0.00591291
Iteration 16/25 | Loss: 0.00591291
Iteration 17/25 | Loss: 0.00591291
Iteration 18/25 | Loss: 0.00591291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005912913475185633, 0.005912913475185633, 0.005912913475185633, 0.005912913475185633, 0.005912913475185633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005912913475185633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00591291
Iteration 2/1000 | Loss: 0.00179030
Iteration 3/1000 | Loss: 0.00112135
Iteration 4/1000 | Loss: 0.00122307
Iteration 5/1000 | Loss: 0.00098901
Iteration 6/1000 | Loss: 0.00468436
Iteration 7/1000 | Loss: 0.00078962
Iteration 8/1000 | Loss: 0.00117566
Iteration 9/1000 | Loss: 0.00074913
Iteration 10/1000 | Loss: 0.00056940
Iteration 11/1000 | Loss: 0.00047068
Iteration 12/1000 | Loss: 0.00050557
Iteration 13/1000 | Loss: 0.00045135
Iteration 14/1000 | Loss: 0.00039833
Iteration 15/1000 | Loss: 0.00052925
Iteration 16/1000 | Loss: 0.00036344
Iteration 17/1000 | Loss: 0.00063516
Iteration 18/1000 | Loss: 0.00050792
Iteration 19/1000 | Loss: 0.00046147
Iteration 20/1000 | Loss: 0.00110145
Iteration 21/1000 | Loss: 0.00034620
Iteration 22/1000 | Loss: 0.00035935
Iteration 23/1000 | Loss: 0.00043480
Iteration 24/1000 | Loss: 0.00140216
Iteration 25/1000 | Loss: 0.00048281
Iteration 26/1000 | Loss: 0.00075696
Iteration 27/1000 | Loss: 0.00060757
Iteration 28/1000 | Loss: 0.00089117
Iteration 29/1000 | Loss: 0.00066814
Iteration 30/1000 | Loss: 0.00160246
Iteration 31/1000 | Loss: 0.00082191
Iteration 32/1000 | Loss: 0.00092447
Iteration 33/1000 | Loss: 0.00130843
Iteration 34/1000 | Loss: 0.00070077
Iteration 35/1000 | Loss: 0.00070313
Iteration 36/1000 | Loss: 0.00036411
Iteration 37/1000 | Loss: 0.00087337
Iteration 38/1000 | Loss: 0.00055135
Iteration 39/1000 | Loss: 0.00038612
Iteration 40/1000 | Loss: 0.00038580
Iteration 41/1000 | Loss: 0.00042292
Iteration 42/1000 | Loss: 0.00090825
Iteration 43/1000 | Loss: 0.00189718
Iteration 44/1000 | Loss: 0.00058465
Iteration 45/1000 | Loss: 0.00040168
Iteration 46/1000 | Loss: 0.00058401
Iteration 47/1000 | Loss: 0.00086715
Iteration 48/1000 | Loss: 0.00076402
Iteration 49/1000 | Loss: 0.00062594
Iteration 50/1000 | Loss: 0.00035255
Iteration 51/1000 | Loss: 0.00033969
Iteration 52/1000 | Loss: 0.00032333
Iteration 53/1000 | Loss: 0.00035076
Iteration 54/1000 | Loss: 0.00033344
Iteration 55/1000 | Loss: 0.00030646
Iteration 56/1000 | Loss: 0.00031571
Iteration 57/1000 | Loss: 0.00031370
Iteration 58/1000 | Loss: 0.00032355
Iteration 59/1000 | Loss: 0.00056103
Iteration 60/1000 | Loss: 0.00033569
Iteration 61/1000 | Loss: 0.00032338
Iteration 62/1000 | Loss: 0.00031617
Iteration 63/1000 | Loss: 0.00030288
Iteration 64/1000 | Loss: 0.00033248
Iteration 65/1000 | Loss: 0.00030099
Iteration 66/1000 | Loss: 0.00031572
Iteration 67/1000 | Loss: 0.00031539
Iteration 68/1000 | Loss: 0.00031344
Iteration 69/1000 | Loss: 0.00031779
Iteration 70/1000 | Loss: 0.00031547
Iteration 71/1000 | Loss: 0.00029961
Iteration 72/1000 | Loss: 0.00031154
Iteration 73/1000 | Loss: 0.00031553
Iteration 74/1000 | Loss: 0.00031673
Iteration 75/1000 | Loss: 0.00030303
Iteration 76/1000 | Loss: 0.00065045
Iteration 77/1000 | Loss: 0.00032346
Iteration 78/1000 | Loss: 0.00031581
Iteration 79/1000 | Loss: 0.00037863
Iteration 80/1000 | Loss: 0.00030632
Iteration 81/1000 | Loss: 0.00059461
Iteration 82/1000 | Loss: 0.00029318
Iteration 83/1000 | Loss: 0.00030888
Iteration 84/1000 | Loss: 0.00032770
Iteration 85/1000 | Loss: 0.00044961
Iteration 86/1000 | Loss: 0.00034154
Iteration 87/1000 | Loss: 0.00031364
Iteration 88/1000 | Loss: 0.00031087
Iteration 89/1000 | Loss: 0.00037745
Iteration 90/1000 | Loss: 0.00029469
Iteration 91/1000 | Loss: 0.00029026
Iteration 92/1000 | Loss: 0.00028764
Iteration 93/1000 | Loss: 0.00028656
Iteration 94/1000 | Loss: 0.00028607
Iteration 95/1000 | Loss: 0.00028576
Iteration 96/1000 | Loss: 0.00028560
Iteration 97/1000 | Loss: 0.00028540
Iteration 98/1000 | Loss: 0.00028525
Iteration 99/1000 | Loss: 0.00028520
Iteration 100/1000 | Loss: 0.00028516
Iteration 101/1000 | Loss: 0.00028514
Iteration 102/1000 | Loss: 0.00028513
Iteration 103/1000 | Loss: 0.00028512
Iteration 104/1000 | Loss: 0.00028511
Iteration 105/1000 | Loss: 0.00028510
Iteration 106/1000 | Loss: 0.00028503
Iteration 107/1000 | Loss: 0.00028500
Iteration 108/1000 | Loss: 0.00028500
Iteration 109/1000 | Loss: 0.00028500
Iteration 110/1000 | Loss: 0.00028499
Iteration 111/1000 | Loss: 0.00028498
Iteration 112/1000 | Loss: 0.00028498
Iteration 113/1000 | Loss: 0.00028498
Iteration 114/1000 | Loss: 0.00028498
Iteration 115/1000 | Loss: 0.00028498
Iteration 116/1000 | Loss: 0.00028498
Iteration 117/1000 | Loss: 0.00028498
Iteration 118/1000 | Loss: 0.00028498
Iteration 119/1000 | Loss: 0.00028498
Iteration 120/1000 | Loss: 0.00028497
Iteration 121/1000 | Loss: 0.00028497
Iteration 122/1000 | Loss: 0.00028497
Iteration 123/1000 | Loss: 0.00028496
Iteration 124/1000 | Loss: 0.00028495
Iteration 125/1000 | Loss: 0.00028494
Iteration 126/1000 | Loss: 0.00028494
Iteration 127/1000 | Loss: 0.00028494
Iteration 128/1000 | Loss: 0.00028494
Iteration 129/1000 | Loss: 0.00028494
Iteration 130/1000 | Loss: 0.00028494
Iteration 131/1000 | Loss: 0.00028494
Iteration 132/1000 | Loss: 0.00028494
Iteration 133/1000 | Loss: 0.00028494
Iteration 134/1000 | Loss: 0.00028494
Iteration 135/1000 | Loss: 0.00028494
Iteration 136/1000 | Loss: 0.00028494
Iteration 137/1000 | Loss: 0.00028493
Iteration 138/1000 | Loss: 0.00028493
Iteration 139/1000 | Loss: 0.00028493
Iteration 140/1000 | Loss: 0.00028493
Iteration 141/1000 | Loss: 0.00028493
Iteration 142/1000 | Loss: 0.00028493
Iteration 143/1000 | Loss: 0.00028493
Iteration 144/1000 | Loss: 0.00028493
Iteration 145/1000 | Loss: 0.00028493
Iteration 146/1000 | Loss: 0.00028492
Iteration 147/1000 | Loss: 0.00028492
Iteration 148/1000 | Loss: 0.00028492
Iteration 149/1000 | Loss: 0.00028492
Iteration 150/1000 | Loss: 0.00028492
Iteration 151/1000 | Loss: 0.00028491
Iteration 152/1000 | Loss: 0.00028491
Iteration 153/1000 | Loss: 0.00028491
Iteration 154/1000 | Loss: 0.00028491
Iteration 155/1000 | Loss: 0.00028491
Iteration 156/1000 | Loss: 0.00028491
Iteration 157/1000 | Loss: 0.00028491
Iteration 158/1000 | Loss: 0.00028491
Iteration 159/1000 | Loss: 0.00028490
Iteration 160/1000 | Loss: 0.00028490
Iteration 161/1000 | Loss: 0.00028490
Iteration 162/1000 | Loss: 0.00028490
Iteration 163/1000 | Loss: 0.00028490
Iteration 164/1000 | Loss: 0.00028490
Iteration 165/1000 | Loss: 0.00028490
Iteration 166/1000 | Loss: 0.00028489
Iteration 167/1000 | Loss: 0.00028489
Iteration 168/1000 | Loss: 0.00028489
Iteration 169/1000 | Loss: 0.00028489
Iteration 170/1000 | Loss: 0.00028489
Iteration 171/1000 | Loss: 0.00028489
Iteration 172/1000 | Loss: 0.00028489
Iteration 173/1000 | Loss: 0.00028489
Iteration 174/1000 | Loss: 0.00028489
Iteration 175/1000 | Loss: 0.00028489
Iteration 176/1000 | Loss: 0.00028489
Iteration 177/1000 | Loss: 0.00028488
Iteration 178/1000 | Loss: 0.00028488
Iteration 179/1000 | Loss: 0.00028488
Iteration 180/1000 | Loss: 0.00028488
Iteration 181/1000 | Loss: 0.00028488
Iteration 182/1000 | Loss: 0.00028488
Iteration 183/1000 | Loss: 0.00028488
Iteration 184/1000 | Loss: 0.00028488
Iteration 185/1000 | Loss: 0.00028488
Iteration 186/1000 | Loss: 0.00028488
Iteration 187/1000 | Loss: 0.00028488
Iteration 188/1000 | Loss: 0.00028488
Iteration 189/1000 | Loss: 0.00028488
Iteration 190/1000 | Loss: 0.00028487
Iteration 191/1000 | Loss: 0.00028487
Iteration 192/1000 | Loss: 0.00028487
Iteration 193/1000 | Loss: 0.00028487
Iteration 194/1000 | Loss: 0.00028487
Iteration 195/1000 | Loss: 0.00028487
Iteration 196/1000 | Loss: 0.00028487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [0.00028487356030382216, 0.00028487356030382216, 0.00028487356030382216, 0.00028487356030382216, 0.00028487356030382216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028487356030382216

Optimization complete. Final v2v error: 9.142242431640625 mm

Highest mean error: 14.65857219696045 mm for frame 38

Lowest mean error: 5.586792945861816 mm for frame 132

Saving results

Total time: 197.05511450767517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596012
Iteration 2/25 | Loss: 0.00158348
Iteration 3/25 | Loss: 0.00143900
Iteration 4/25 | Loss: 0.00141781
Iteration 5/25 | Loss: 0.00141268
Iteration 6/25 | Loss: 0.00141152
Iteration 7/25 | Loss: 0.00141152
Iteration 8/25 | Loss: 0.00141152
Iteration 9/25 | Loss: 0.00141152
Iteration 10/25 | Loss: 0.00141152
Iteration 11/25 | Loss: 0.00141152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014115181984379888, 0.0014115181984379888, 0.0014115181984379888, 0.0014115181984379888, 0.0014115181984379888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014115181984379888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55582321
Iteration 2/25 | Loss: 0.00121999
Iteration 3/25 | Loss: 0.00121998
Iteration 4/25 | Loss: 0.00121998
Iteration 5/25 | Loss: 0.00121998
Iteration 6/25 | Loss: 0.00121998
Iteration 7/25 | Loss: 0.00121998
Iteration 8/25 | Loss: 0.00121998
Iteration 9/25 | Loss: 0.00121998
Iteration 10/25 | Loss: 0.00121998
Iteration 11/25 | Loss: 0.00121998
Iteration 12/25 | Loss: 0.00121998
Iteration 13/25 | Loss: 0.00121998
Iteration 14/25 | Loss: 0.00121998
Iteration 15/25 | Loss: 0.00121998
Iteration 16/25 | Loss: 0.00121998
Iteration 17/25 | Loss: 0.00121998
Iteration 18/25 | Loss: 0.00121998
Iteration 19/25 | Loss: 0.00121998
Iteration 20/25 | Loss: 0.00121998
Iteration 21/25 | Loss: 0.00121998
Iteration 22/25 | Loss: 0.00121998
Iteration 23/25 | Loss: 0.00121998
Iteration 24/25 | Loss: 0.00121998
Iteration 25/25 | Loss: 0.00121998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121998
Iteration 2/1000 | Loss: 0.00006928
Iteration 3/1000 | Loss: 0.00005117
Iteration 4/1000 | Loss: 0.00004492
Iteration 5/1000 | Loss: 0.00004227
Iteration 6/1000 | Loss: 0.00004060
Iteration 7/1000 | Loss: 0.00003905
Iteration 8/1000 | Loss: 0.00003797
Iteration 9/1000 | Loss: 0.00003746
Iteration 10/1000 | Loss: 0.00003698
Iteration 11/1000 | Loss: 0.00003666
Iteration 12/1000 | Loss: 0.00003649
Iteration 13/1000 | Loss: 0.00003644
Iteration 14/1000 | Loss: 0.00003643
Iteration 15/1000 | Loss: 0.00003642
Iteration 16/1000 | Loss: 0.00003642
Iteration 17/1000 | Loss: 0.00003641
Iteration 18/1000 | Loss: 0.00003641
Iteration 19/1000 | Loss: 0.00003640
Iteration 20/1000 | Loss: 0.00003640
Iteration 21/1000 | Loss: 0.00003639
Iteration 22/1000 | Loss: 0.00003638
Iteration 23/1000 | Loss: 0.00003638
Iteration 24/1000 | Loss: 0.00003638
Iteration 25/1000 | Loss: 0.00003637
Iteration 26/1000 | Loss: 0.00003637
Iteration 27/1000 | Loss: 0.00003637
Iteration 28/1000 | Loss: 0.00003637
Iteration 29/1000 | Loss: 0.00003637
Iteration 30/1000 | Loss: 0.00003636
Iteration 31/1000 | Loss: 0.00003635
Iteration 32/1000 | Loss: 0.00003635
Iteration 33/1000 | Loss: 0.00003635
Iteration 34/1000 | Loss: 0.00003635
Iteration 35/1000 | Loss: 0.00003635
Iteration 36/1000 | Loss: 0.00003635
Iteration 37/1000 | Loss: 0.00003634
Iteration 38/1000 | Loss: 0.00003634
Iteration 39/1000 | Loss: 0.00003634
Iteration 40/1000 | Loss: 0.00003634
Iteration 41/1000 | Loss: 0.00003633
Iteration 42/1000 | Loss: 0.00003633
Iteration 43/1000 | Loss: 0.00003632
Iteration 44/1000 | Loss: 0.00003632
Iteration 45/1000 | Loss: 0.00003632
Iteration 46/1000 | Loss: 0.00003631
Iteration 47/1000 | Loss: 0.00003631
Iteration 48/1000 | Loss: 0.00003631
Iteration 49/1000 | Loss: 0.00003630
Iteration 50/1000 | Loss: 0.00003630
Iteration 51/1000 | Loss: 0.00003629
Iteration 52/1000 | Loss: 0.00003629
Iteration 53/1000 | Loss: 0.00003629
Iteration 54/1000 | Loss: 0.00003629
Iteration 55/1000 | Loss: 0.00003628
Iteration 56/1000 | Loss: 0.00003628
Iteration 57/1000 | Loss: 0.00003628
Iteration 58/1000 | Loss: 0.00003628
Iteration 59/1000 | Loss: 0.00003627
Iteration 60/1000 | Loss: 0.00003627
Iteration 61/1000 | Loss: 0.00003627
Iteration 62/1000 | Loss: 0.00003626
Iteration 63/1000 | Loss: 0.00003624
Iteration 64/1000 | Loss: 0.00003624
Iteration 65/1000 | Loss: 0.00003624
Iteration 66/1000 | Loss: 0.00003624
Iteration 67/1000 | Loss: 0.00003624
Iteration 68/1000 | Loss: 0.00003624
Iteration 69/1000 | Loss: 0.00003624
Iteration 70/1000 | Loss: 0.00003624
Iteration 71/1000 | Loss: 0.00003624
Iteration 72/1000 | Loss: 0.00003624
Iteration 73/1000 | Loss: 0.00003623
Iteration 74/1000 | Loss: 0.00003621
Iteration 75/1000 | Loss: 0.00003621
Iteration 76/1000 | Loss: 0.00003621
Iteration 77/1000 | Loss: 0.00003621
Iteration 78/1000 | Loss: 0.00003621
Iteration 79/1000 | Loss: 0.00003621
Iteration 80/1000 | Loss: 0.00003621
Iteration 81/1000 | Loss: 0.00003621
Iteration 82/1000 | Loss: 0.00003621
Iteration 83/1000 | Loss: 0.00003620
Iteration 84/1000 | Loss: 0.00003620
Iteration 85/1000 | Loss: 0.00003620
Iteration 86/1000 | Loss: 0.00003620
Iteration 87/1000 | Loss: 0.00003620
Iteration 88/1000 | Loss: 0.00003620
Iteration 89/1000 | Loss: 0.00003619
Iteration 90/1000 | Loss: 0.00003619
Iteration 91/1000 | Loss: 0.00003619
Iteration 92/1000 | Loss: 0.00003619
Iteration 93/1000 | Loss: 0.00003619
Iteration 94/1000 | Loss: 0.00003618
Iteration 95/1000 | Loss: 0.00003618
Iteration 96/1000 | Loss: 0.00003618
Iteration 97/1000 | Loss: 0.00003618
Iteration 98/1000 | Loss: 0.00003618
Iteration 99/1000 | Loss: 0.00003617
Iteration 100/1000 | Loss: 0.00003617
Iteration 101/1000 | Loss: 0.00003617
Iteration 102/1000 | Loss: 0.00003617
Iteration 103/1000 | Loss: 0.00003617
Iteration 104/1000 | Loss: 0.00003616
Iteration 105/1000 | Loss: 0.00003616
Iteration 106/1000 | Loss: 0.00003616
Iteration 107/1000 | Loss: 0.00003616
Iteration 108/1000 | Loss: 0.00003616
Iteration 109/1000 | Loss: 0.00003616
Iteration 110/1000 | Loss: 0.00003615
Iteration 111/1000 | Loss: 0.00003615
Iteration 112/1000 | Loss: 0.00003615
Iteration 113/1000 | Loss: 0.00003615
Iteration 114/1000 | Loss: 0.00003615
Iteration 115/1000 | Loss: 0.00003614
Iteration 116/1000 | Loss: 0.00003614
Iteration 117/1000 | Loss: 0.00003614
Iteration 118/1000 | Loss: 0.00003614
Iteration 119/1000 | Loss: 0.00003614
Iteration 120/1000 | Loss: 0.00003614
Iteration 121/1000 | Loss: 0.00003614
Iteration 122/1000 | Loss: 0.00003614
Iteration 123/1000 | Loss: 0.00003613
Iteration 124/1000 | Loss: 0.00003613
Iteration 125/1000 | Loss: 0.00003613
Iteration 126/1000 | Loss: 0.00003613
Iteration 127/1000 | Loss: 0.00003613
Iteration 128/1000 | Loss: 0.00003613
Iteration 129/1000 | Loss: 0.00003612
Iteration 130/1000 | Loss: 0.00003612
Iteration 131/1000 | Loss: 0.00003612
Iteration 132/1000 | Loss: 0.00003612
Iteration 133/1000 | Loss: 0.00003612
Iteration 134/1000 | Loss: 0.00003612
Iteration 135/1000 | Loss: 0.00003612
Iteration 136/1000 | Loss: 0.00003612
Iteration 137/1000 | Loss: 0.00003612
Iteration 138/1000 | Loss: 0.00003612
Iteration 139/1000 | Loss: 0.00003612
Iteration 140/1000 | Loss: 0.00003612
Iteration 141/1000 | Loss: 0.00003612
Iteration 142/1000 | Loss: 0.00003612
Iteration 143/1000 | Loss: 0.00003612
Iteration 144/1000 | Loss: 0.00003612
Iteration 145/1000 | Loss: 0.00003612
Iteration 146/1000 | Loss: 0.00003612
Iteration 147/1000 | Loss: 0.00003612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [3.611557622207329e-05, 3.611557622207329e-05, 3.611557622207329e-05, 3.611557622207329e-05, 3.611557622207329e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.611557622207329e-05

Optimization complete. Final v2v error: 5.091802597045898 mm

Highest mean error: 5.43284797668457 mm for frame 128

Lowest mean error: 4.524219512939453 mm for frame 71

Saving results

Total time: 35.94480228424072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01170294
Iteration 2/25 | Loss: 0.00216341
Iteration 3/25 | Loss: 0.00150485
Iteration 4/25 | Loss: 0.00138345
Iteration 5/25 | Loss: 0.00135917
Iteration 6/25 | Loss: 0.00135204
Iteration 7/25 | Loss: 0.00134966
Iteration 8/25 | Loss: 0.00134921
Iteration 9/25 | Loss: 0.00134921
Iteration 10/25 | Loss: 0.00134921
Iteration 11/25 | Loss: 0.00134921
Iteration 12/25 | Loss: 0.00134921
Iteration 13/25 | Loss: 0.00134921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013492085272446275, 0.0013492085272446275, 0.0013492085272446275, 0.0013492085272446275, 0.0013492085272446275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013492085272446275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72277963
Iteration 2/25 | Loss: 0.00133437
Iteration 3/25 | Loss: 0.00133432
Iteration 4/25 | Loss: 0.00133432
Iteration 5/25 | Loss: 0.00133432
Iteration 6/25 | Loss: 0.00133432
Iteration 7/25 | Loss: 0.00133432
Iteration 8/25 | Loss: 0.00133432
Iteration 9/25 | Loss: 0.00133432
Iteration 10/25 | Loss: 0.00133432
Iteration 11/25 | Loss: 0.00133432
Iteration 12/25 | Loss: 0.00133432
Iteration 13/25 | Loss: 0.00133432
Iteration 14/25 | Loss: 0.00133432
Iteration 15/25 | Loss: 0.00133432
Iteration 16/25 | Loss: 0.00133432
Iteration 17/25 | Loss: 0.00133432
Iteration 18/25 | Loss: 0.00133432
Iteration 19/25 | Loss: 0.00133432
Iteration 20/25 | Loss: 0.00133432
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013343221507966518, 0.0013343221507966518, 0.0013343221507966518, 0.0013343221507966518, 0.0013343221507966518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013343221507966518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133432
Iteration 2/1000 | Loss: 0.00005496
Iteration 3/1000 | Loss: 0.00004242
Iteration 4/1000 | Loss: 0.00003884
Iteration 5/1000 | Loss: 0.00003686
Iteration 6/1000 | Loss: 0.00003576
Iteration 7/1000 | Loss: 0.00003485
Iteration 8/1000 | Loss: 0.00003428
Iteration 9/1000 | Loss: 0.00003382
Iteration 10/1000 | Loss: 0.00003344
Iteration 11/1000 | Loss: 0.00003319
Iteration 12/1000 | Loss: 0.00003312
Iteration 13/1000 | Loss: 0.00003291
Iteration 14/1000 | Loss: 0.00003278
Iteration 15/1000 | Loss: 0.00003277
Iteration 16/1000 | Loss: 0.00003273
Iteration 17/1000 | Loss: 0.00003270
Iteration 18/1000 | Loss: 0.00003266
Iteration 19/1000 | Loss: 0.00003264
Iteration 20/1000 | Loss: 0.00003259
Iteration 21/1000 | Loss: 0.00003259
Iteration 22/1000 | Loss: 0.00003259
Iteration 23/1000 | Loss: 0.00003259
Iteration 24/1000 | Loss: 0.00003259
Iteration 25/1000 | Loss: 0.00003259
Iteration 26/1000 | Loss: 0.00003259
Iteration 27/1000 | Loss: 0.00003259
Iteration 28/1000 | Loss: 0.00003258
Iteration 29/1000 | Loss: 0.00003256
Iteration 30/1000 | Loss: 0.00003255
Iteration 31/1000 | Loss: 0.00003255
Iteration 32/1000 | Loss: 0.00003254
Iteration 33/1000 | Loss: 0.00003254
Iteration 34/1000 | Loss: 0.00003253
Iteration 35/1000 | Loss: 0.00003253
Iteration 36/1000 | Loss: 0.00003252
Iteration 37/1000 | Loss: 0.00003252
Iteration 38/1000 | Loss: 0.00003252
Iteration 39/1000 | Loss: 0.00003251
Iteration 40/1000 | Loss: 0.00003251
Iteration 41/1000 | Loss: 0.00003251
Iteration 42/1000 | Loss: 0.00003250
Iteration 43/1000 | Loss: 0.00003249
Iteration 44/1000 | Loss: 0.00003248
Iteration 45/1000 | Loss: 0.00003248
Iteration 46/1000 | Loss: 0.00003248
Iteration 47/1000 | Loss: 0.00003248
Iteration 48/1000 | Loss: 0.00003248
Iteration 49/1000 | Loss: 0.00003247
Iteration 50/1000 | Loss: 0.00003247
Iteration 51/1000 | Loss: 0.00003247
Iteration 52/1000 | Loss: 0.00003247
Iteration 53/1000 | Loss: 0.00003247
Iteration 54/1000 | Loss: 0.00003247
Iteration 55/1000 | Loss: 0.00003246
Iteration 56/1000 | Loss: 0.00003246
Iteration 57/1000 | Loss: 0.00003246
Iteration 58/1000 | Loss: 0.00003245
Iteration 59/1000 | Loss: 0.00003245
Iteration 60/1000 | Loss: 0.00003245
Iteration 61/1000 | Loss: 0.00003245
Iteration 62/1000 | Loss: 0.00003245
Iteration 63/1000 | Loss: 0.00003244
Iteration 64/1000 | Loss: 0.00003244
Iteration 65/1000 | Loss: 0.00003244
Iteration 66/1000 | Loss: 0.00003244
Iteration 67/1000 | Loss: 0.00003244
Iteration 68/1000 | Loss: 0.00003244
Iteration 69/1000 | Loss: 0.00003244
Iteration 70/1000 | Loss: 0.00003244
Iteration 71/1000 | Loss: 0.00003243
Iteration 72/1000 | Loss: 0.00003243
Iteration 73/1000 | Loss: 0.00003243
Iteration 74/1000 | Loss: 0.00003243
Iteration 75/1000 | Loss: 0.00003243
Iteration 76/1000 | Loss: 0.00003243
Iteration 77/1000 | Loss: 0.00003243
Iteration 78/1000 | Loss: 0.00003243
Iteration 79/1000 | Loss: 0.00003243
Iteration 80/1000 | Loss: 0.00003243
Iteration 81/1000 | Loss: 0.00003243
Iteration 82/1000 | Loss: 0.00003243
Iteration 83/1000 | Loss: 0.00003243
Iteration 84/1000 | Loss: 0.00003243
Iteration 85/1000 | Loss: 0.00003243
Iteration 86/1000 | Loss: 0.00003243
Iteration 87/1000 | Loss: 0.00003243
Iteration 88/1000 | Loss: 0.00003243
Iteration 89/1000 | Loss: 0.00003243
Iteration 90/1000 | Loss: 0.00003243
Iteration 91/1000 | Loss: 0.00003243
Iteration 92/1000 | Loss: 0.00003243
Iteration 93/1000 | Loss: 0.00003243
Iteration 94/1000 | Loss: 0.00003243
Iteration 95/1000 | Loss: 0.00003243
Iteration 96/1000 | Loss: 0.00003243
Iteration 97/1000 | Loss: 0.00003243
Iteration 98/1000 | Loss: 0.00003243
Iteration 99/1000 | Loss: 0.00003243
Iteration 100/1000 | Loss: 0.00003243
Iteration 101/1000 | Loss: 0.00003243
Iteration 102/1000 | Loss: 0.00003243
Iteration 103/1000 | Loss: 0.00003243
Iteration 104/1000 | Loss: 0.00003243
Iteration 105/1000 | Loss: 0.00003243
Iteration 106/1000 | Loss: 0.00003243
Iteration 107/1000 | Loss: 0.00003243
Iteration 108/1000 | Loss: 0.00003243
Iteration 109/1000 | Loss: 0.00003243
Iteration 110/1000 | Loss: 0.00003243
Iteration 111/1000 | Loss: 0.00003243
Iteration 112/1000 | Loss: 0.00003243
Iteration 113/1000 | Loss: 0.00003243
Iteration 114/1000 | Loss: 0.00003243
Iteration 115/1000 | Loss: 0.00003243
Iteration 116/1000 | Loss: 0.00003243
Iteration 117/1000 | Loss: 0.00003243
Iteration 118/1000 | Loss: 0.00003243
Iteration 119/1000 | Loss: 0.00003243
Iteration 120/1000 | Loss: 0.00003243
Iteration 121/1000 | Loss: 0.00003243
Iteration 122/1000 | Loss: 0.00003243
Iteration 123/1000 | Loss: 0.00003243
Iteration 124/1000 | Loss: 0.00003243
Iteration 125/1000 | Loss: 0.00003243
Iteration 126/1000 | Loss: 0.00003243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [3.2429546990897506e-05, 3.2429546990897506e-05, 3.2429546990897506e-05, 3.2429546990897506e-05, 3.2429546990897506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2429546990897506e-05

Optimization complete. Final v2v error: 4.85409688949585 mm

Highest mean error: 6.084496021270752 mm for frame 212

Lowest mean error: 4.061500549316406 mm for frame 69

Saving results

Total time: 44.16909456253052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429772
Iteration 2/25 | Loss: 0.00137167
Iteration 3/25 | Loss: 0.00125634
Iteration 4/25 | Loss: 0.00124199
Iteration 5/25 | Loss: 0.00123815
Iteration 6/25 | Loss: 0.00123708
Iteration 7/25 | Loss: 0.00123671
Iteration 8/25 | Loss: 0.00123671
Iteration 9/25 | Loss: 0.00123671
Iteration 10/25 | Loss: 0.00123671
Iteration 11/25 | Loss: 0.00123671
Iteration 12/25 | Loss: 0.00123671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012367082526907325, 0.0012367082526907325, 0.0012367082526907325, 0.0012367082526907325, 0.0012367082526907325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012367082526907325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56202638
Iteration 2/25 | Loss: 0.00100241
Iteration 3/25 | Loss: 0.00100241
Iteration 4/25 | Loss: 0.00100241
Iteration 5/25 | Loss: 0.00100241
Iteration 6/25 | Loss: 0.00100241
Iteration 7/25 | Loss: 0.00100241
Iteration 8/25 | Loss: 0.00100241
Iteration 9/25 | Loss: 0.00100241
Iteration 10/25 | Loss: 0.00100241
Iteration 11/25 | Loss: 0.00100241
Iteration 12/25 | Loss: 0.00100241
Iteration 13/25 | Loss: 0.00100241
Iteration 14/25 | Loss: 0.00100241
Iteration 15/25 | Loss: 0.00100241
Iteration 16/25 | Loss: 0.00100241
Iteration 17/25 | Loss: 0.00100241
Iteration 18/25 | Loss: 0.00100241
Iteration 19/25 | Loss: 0.00100241
Iteration 20/25 | Loss: 0.00100241
Iteration 21/25 | Loss: 0.00100241
Iteration 22/25 | Loss: 0.00100241
Iteration 23/25 | Loss: 0.00100241
Iteration 24/25 | Loss: 0.00100241
Iteration 25/25 | Loss: 0.00100241

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100241
Iteration 2/1000 | Loss: 0.00003050
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00002108
Iteration 5/1000 | Loss: 0.00002004
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001922
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001870
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001860
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001854
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001849
Iteration 16/1000 | Loss: 0.00001844
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001839
Iteration 19/1000 | Loss: 0.00001839
Iteration 20/1000 | Loss: 0.00001838
Iteration 21/1000 | Loss: 0.00001834
Iteration 22/1000 | Loss: 0.00001834
Iteration 23/1000 | Loss: 0.00001832
Iteration 24/1000 | Loss: 0.00001832
Iteration 25/1000 | Loss: 0.00001831
Iteration 26/1000 | Loss: 0.00001831
Iteration 27/1000 | Loss: 0.00001831
Iteration 28/1000 | Loss: 0.00001830
Iteration 29/1000 | Loss: 0.00001830
Iteration 30/1000 | Loss: 0.00001829
Iteration 31/1000 | Loss: 0.00001829
Iteration 32/1000 | Loss: 0.00001829
Iteration 33/1000 | Loss: 0.00001829
Iteration 34/1000 | Loss: 0.00001829
Iteration 35/1000 | Loss: 0.00001828
Iteration 36/1000 | Loss: 0.00001828
Iteration 37/1000 | Loss: 0.00001827
Iteration 38/1000 | Loss: 0.00001827
Iteration 39/1000 | Loss: 0.00001827
Iteration 40/1000 | Loss: 0.00001827
Iteration 41/1000 | Loss: 0.00001826
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001826
Iteration 44/1000 | Loss: 0.00001826
Iteration 45/1000 | Loss: 0.00001826
Iteration 46/1000 | Loss: 0.00001826
Iteration 47/1000 | Loss: 0.00001826
Iteration 48/1000 | Loss: 0.00001826
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001825
Iteration 51/1000 | Loss: 0.00001825
Iteration 52/1000 | Loss: 0.00001825
Iteration 53/1000 | Loss: 0.00001825
Iteration 54/1000 | Loss: 0.00001824
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001823
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001822
Iteration 65/1000 | Loss: 0.00001821
Iteration 66/1000 | Loss: 0.00001821
Iteration 67/1000 | Loss: 0.00001821
Iteration 68/1000 | Loss: 0.00001821
Iteration 69/1000 | Loss: 0.00001821
Iteration 70/1000 | Loss: 0.00001821
Iteration 71/1000 | Loss: 0.00001821
Iteration 72/1000 | Loss: 0.00001821
Iteration 73/1000 | Loss: 0.00001821
Iteration 74/1000 | Loss: 0.00001821
Iteration 75/1000 | Loss: 0.00001821
Iteration 76/1000 | Loss: 0.00001821
Iteration 77/1000 | Loss: 0.00001820
Iteration 78/1000 | Loss: 0.00001820
Iteration 79/1000 | Loss: 0.00001820
Iteration 80/1000 | Loss: 0.00001820
Iteration 81/1000 | Loss: 0.00001820
Iteration 82/1000 | Loss: 0.00001820
Iteration 83/1000 | Loss: 0.00001820
Iteration 84/1000 | Loss: 0.00001820
Iteration 85/1000 | Loss: 0.00001820
Iteration 86/1000 | Loss: 0.00001820
Iteration 87/1000 | Loss: 0.00001820
Iteration 88/1000 | Loss: 0.00001820
Iteration 89/1000 | Loss: 0.00001820
Iteration 90/1000 | Loss: 0.00001820
Iteration 91/1000 | Loss: 0.00001820
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.8197421013610438e-05, 1.8197421013610438e-05, 1.8197421013610438e-05, 1.8197421013610438e-05, 1.8197421013610438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8197421013610438e-05

Optimization complete. Final v2v error: 3.7211318016052246 mm

Highest mean error: 4.430679798126221 mm for frame 46

Lowest mean error: 3.278911590576172 mm for frame 32

Saving results

Total time: 30.7249014377594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087492
Iteration 2/25 | Loss: 0.01087492
Iteration 3/25 | Loss: 0.01087492
Iteration 4/25 | Loss: 0.01087491
Iteration 5/25 | Loss: 0.00342111
Iteration 6/25 | Loss: 0.00254817
Iteration 7/25 | Loss: 0.00238425
Iteration 8/25 | Loss: 0.00225579
Iteration 9/25 | Loss: 0.00219680
Iteration 10/25 | Loss: 0.00216632
Iteration 11/25 | Loss: 0.00206747
Iteration 12/25 | Loss: 0.00206145
Iteration 13/25 | Loss: 0.00206259
Iteration 14/25 | Loss: 0.00204103
Iteration 15/25 | Loss: 0.00203871
Iteration 16/25 | Loss: 0.00203526
Iteration 17/25 | Loss: 0.00203547
Iteration 18/25 | Loss: 0.00203447
Iteration 19/25 | Loss: 0.00203511
Iteration 20/25 | Loss: 0.00203266
Iteration 21/25 | Loss: 0.00203026
Iteration 22/25 | Loss: 0.00202931
Iteration 23/25 | Loss: 0.00202859
Iteration 24/25 | Loss: 0.00202793
Iteration 25/25 | Loss: 0.00202574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40306270
Iteration 2/25 | Loss: 0.01588194
Iteration 3/25 | Loss: 0.00743660
Iteration 4/25 | Loss: 0.00743656
Iteration 5/25 | Loss: 0.00743656
Iteration 6/25 | Loss: 0.00743656
Iteration 7/25 | Loss: 0.00743655
Iteration 8/25 | Loss: 0.00743655
Iteration 9/25 | Loss: 0.00743655
Iteration 10/25 | Loss: 0.00743655
Iteration 11/25 | Loss: 0.00743655
Iteration 12/25 | Loss: 0.00743655
Iteration 13/25 | Loss: 0.00743655
Iteration 14/25 | Loss: 0.00743655
Iteration 15/25 | Loss: 0.00743655
Iteration 16/25 | Loss: 0.00743655
Iteration 17/25 | Loss: 0.00743655
Iteration 18/25 | Loss: 0.00743655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.007436553481966257, 0.007436553481966257, 0.007436553481966257, 0.007436553481966257, 0.007436553481966257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007436553481966257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00743655
Iteration 2/1000 | Loss: 0.00869649
Iteration 3/1000 | Loss: 0.00922838
Iteration 4/1000 | Loss: 0.00247809
Iteration 5/1000 | Loss: 0.00369625
Iteration 6/1000 | Loss: 0.00104290
Iteration 7/1000 | Loss: 0.00068613
Iteration 8/1000 | Loss: 0.00103231
Iteration 9/1000 | Loss: 0.00485335
Iteration 10/1000 | Loss: 0.00069006
Iteration 11/1000 | Loss: 0.00414884
Iteration 12/1000 | Loss: 0.00368853
Iteration 13/1000 | Loss: 0.00498870
Iteration 14/1000 | Loss: 0.02311015
Iteration 15/1000 | Loss: 0.00187430
Iteration 16/1000 | Loss: 0.00055399
Iteration 17/1000 | Loss: 0.00159156
Iteration 18/1000 | Loss: 0.00412561
Iteration 19/1000 | Loss: 0.00019703
Iteration 20/1000 | Loss: 0.00082864
Iteration 21/1000 | Loss: 0.00018380
Iteration 22/1000 | Loss: 0.00069500
Iteration 23/1000 | Loss: 0.00038223
Iteration 24/1000 | Loss: 0.00034495
Iteration 25/1000 | Loss: 0.00053550
Iteration 26/1000 | Loss: 0.00004905
Iteration 27/1000 | Loss: 0.00004343
Iteration 28/1000 | Loss: 0.00099441
Iteration 29/1000 | Loss: 0.00032465
Iteration 30/1000 | Loss: 0.00277237
Iteration 31/1000 | Loss: 0.00702811
Iteration 32/1000 | Loss: 0.00311498
Iteration 33/1000 | Loss: 0.00199489
Iteration 34/1000 | Loss: 0.00073634
Iteration 35/1000 | Loss: 0.00328431
Iteration 36/1000 | Loss: 0.00027670
Iteration 37/1000 | Loss: 0.00020565
Iteration 38/1000 | Loss: 0.00016765
Iteration 39/1000 | Loss: 0.00003556
Iteration 40/1000 | Loss: 0.00003408
Iteration 41/1000 | Loss: 0.00003349
Iteration 42/1000 | Loss: 0.00003335
Iteration 43/1000 | Loss: 0.00003289
Iteration 44/1000 | Loss: 0.00009672
Iteration 45/1000 | Loss: 0.00003266
Iteration 46/1000 | Loss: 0.00003247
Iteration 47/1000 | Loss: 0.00003247
Iteration 48/1000 | Loss: 0.00003240
Iteration 49/1000 | Loss: 0.00003237
Iteration 50/1000 | Loss: 0.00003237
Iteration 51/1000 | Loss: 0.00003237
Iteration 52/1000 | Loss: 0.00003237
Iteration 53/1000 | Loss: 0.00003237
Iteration 54/1000 | Loss: 0.00003237
Iteration 55/1000 | Loss: 0.00003237
Iteration 56/1000 | Loss: 0.00003237
Iteration 57/1000 | Loss: 0.00003237
Iteration 58/1000 | Loss: 0.00003237
Iteration 59/1000 | Loss: 0.00003237
Iteration 60/1000 | Loss: 0.00003251
Iteration 61/1000 | Loss: 0.00003240
Iteration 62/1000 | Loss: 0.00003238
Iteration 63/1000 | Loss: 0.00003237
Iteration 64/1000 | Loss: 0.00003237
Iteration 65/1000 | Loss: 0.00003237
Iteration 66/1000 | Loss: 0.00003236
Iteration 67/1000 | Loss: 0.00003236
Iteration 68/1000 | Loss: 0.00003235
Iteration 69/1000 | Loss: 0.00003235
Iteration 70/1000 | Loss: 0.00003235
Iteration 71/1000 | Loss: 0.00003235
Iteration 72/1000 | Loss: 0.00003235
Iteration 73/1000 | Loss: 0.00003235
Iteration 74/1000 | Loss: 0.00003235
Iteration 75/1000 | Loss: 0.00003235
Iteration 76/1000 | Loss: 0.00003234
Iteration 77/1000 | Loss: 0.00003234
Iteration 78/1000 | Loss: 0.00003234
Iteration 79/1000 | Loss: 0.00003234
Iteration 80/1000 | Loss: 0.00003234
Iteration 81/1000 | Loss: 0.00003242
Iteration 82/1000 | Loss: 0.00003241
Iteration 83/1000 | Loss: 0.00003241
Iteration 84/1000 | Loss: 0.00003226
Iteration 85/1000 | Loss: 0.00003226
Iteration 86/1000 | Loss: 0.00003226
Iteration 87/1000 | Loss: 0.00003226
Iteration 88/1000 | Loss: 0.00003226
Iteration 89/1000 | Loss: 0.00003226
Iteration 90/1000 | Loss: 0.00003225
Iteration 91/1000 | Loss: 0.00003225
Iteration 92/1000 | Loss: 0.00003225
Iteration 93/1000 | Loss: 0.00003225
Iteration 94/1000 | Loss: 0.00003224
Iteration 95/1000 | Loss: 0.00003224
Iteration 96/1000 | Loss: 0.00003224
Iteration 97/1000 | Loss: 0.00003224
Iteration 98/1000 | Loss: 0.00003224
Iteration 99/1000 | Loss: 0.00003224
Iteration 100/1000 | Loss: 0.00003224
Iteration 101/1000 | Loss: 0.00003224
Iteration 102/1000 | Loss: 0.00003224
Iteration 103/1000 | Loss: 0.00003224
Iteration 104/1000 | Loss: 0.00003224
Iteration 105/1000 | Loss: 0.00003224
Iteration 106/1000 | Loss: 0.00003224
Iteration 107/1000 | Loss: 0.00003224
Iteration 108/1000 | Loss: 0.00003224
Iteration 109/1000 | Loss: 0.00003224
Iteration 110/1000 | Loss: 0.00003224
Iteration 111/1000 | Loss: 0.00003224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [3.223949170205742e-05, 3.223949170205742e-05, 3.223949170205742e-05, 3.223949170205742e-05, 3.223949170205742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.223949170205742e-05

Optimization complete. Final v2v error: 4.887004852294922 mm

Highest mean error: 10.67203140258789 mm for frame 5

Lowest mean error: 4.498841285705566 mm for frame 47

Saving results

Total time: 128.79350566864014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509296
Iteration 2/25 | Loss: 0.00145307
Iteration 3/25 | Loss: 0.00132048
Iteration 4/25 | Loss: 0.00130335
Iteration 5/25 | Loss: 0.00129597
Iteration 6/25 | Loss: 0.00129446
Iteration 7/25 | Loss: 0.00129446
Iteration 8/25 | Loss: 0.00129446
Iteration 9/25 | Loss: 0.00129446
Iteration 10/25 | Loss: 0.00129446
Iteration 11/25 | Loss: 0.00129446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001294459099881351, 0.001294459099881351, 0.001294459099881351, 0.001294459099881351, 0.001294459099881351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001294459099881351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02070808
Iteration 2/25 | Loss: 0.00120843
Iteration 3/25 | Loss: 0.00120841
Iteration 4/25 | Loss: 0.00120841
Iteration 5/25 | Loss: 0.00120841
Iteration 6/25 | Loss: 0.00120841
Iteration 7/25 | Loss: 0.00120841
Iteration 8/25 | Loss: 0.00120841
Iteration 9/25 | Loss: 0.00120841
Iteration 10/25 | Loss: 0.00120841
Iteration 11/25 | Loss: 0.00120841
Iteration 12/25 | Loss: 0.00120841
Iteration 13/25 | Loss: 0.00120841
Iteration 14/25 | Loss: 0.00120841
Iteration 15/25 | Loss: 0.00120841
Iteration 16/25 | Loss: 0.00120841
Iteration 17/25 | Loss: 0.00120841
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012084122281521559, 0.0012084122281521559, 0.0012084122281521559, 0.0012084122281521559, 0.0012084122281521559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012084122281521559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120841
Iteration 2/1000 | Loss: 0.00003656
Iteration 3/1000 | Loss: 0.00003005
Iteration 4/1000 | Loss: 0.00002833
Iteration 5/1000 | Loss: 0.00002751
Iteration 6/1000 | Loss: 0.00002684
Iteration 7/1000 | Loss: 0.00002638
Iteration 8/1000 | Loss: 0.00002599
Iteration 9/1000 | Loss: 0.00002571
Iteration 10/1000 | Loss: 0.00002555
Iteration 11/1000 | Loss: 0.00002546
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002543
Iteration 14/1000 | Loss: 0.00002542
Iteration 15/1000 | Loss: 0.00002536
Iteration 16/1000 | Loss: 0.00002534
Iteration 17/1000 | Loss: 0.00002533
Iteration 18/1000 | Loss: 0.00002533
Iteration 19/1000 | Loss: 0.00002532
Iteration 20/1000 | Loss: 0.00002526
Iteration 21/1000 | Loss: 0.00002525
Iteration 22/1000 | Loss: 0.00002525
Iteration 23/1000 | Loss: 0.00002524
Iteration 24/1000 | Loss: 0.00002524
Iteration 25/1000 | Loss: 0.00002521
Iteration 26/1000 | Loss: 0.00002520
Iteration 27/1000 | Loss: 0.00002520
Iteration 28/1000 | Loss: 0.00002520
Iteration 29/1000 | Loss: 0.00002519
Iteration 30/1000 | Loss: 0.00002519
Iteration 31/1000 | Loss: 0.00002519
Iteration 32/1000 | Loss: 0.00002518
Iteration 33/1000 | Loss: 0.00002514
Iteration 34/1000 | Loss: 0.00002514
Iteration 35/1000 | Loss: 0.00002514
Iteration 36/1000 | Loss: 0.00002514
Iteration 37/1000 | Loss: 0.00002513
Iteration 38/1000 | Loss: 0.00002513
Iteration 39/1000 | Loss: 0.00002513
Iteration 40/1000 | Loss: 0.00002511
Iteration 41/1000 | Loss: 0.00002511
Iteration 42/1000 | Loss: 0.00002511
Iteration 43/1000 | Loss: 0.00002511
Iteration 44/1000 | Loss: 0.00002511
Iteration 45/1000 | Loss: 0.00002511
Iteration 46/1000 | Loss: 0.00002511
Iteration 47/1000 | Loss: 0.00002511
Iteration 48/1000 | Loss: 0.00002511
Iteration 49/1000 | Loss: 0.00002511
Iteration 50/1000 | Loss: 0.00002510
Iteration 51/1000 | Loss: 0.00002510
Iteration 52/1000 | Loss: 0.00002510
Iteration 53/1000 | Loss: 0.00002510
Iteration 54/1000 | Loss: 0.00002510
Iteration 55/1000 | Loss: 0.00002510
Iteration 56/1000 | Loss: 0.00002510
Iteration 57/1000 | Loss: 0.00002509
Iteration 58/1000 | Loss: 0.00002509
Iteration 59/1000 | Loss: 0.00002509
Iteration 60/1000 | Loss: 0.00002508
Iteration 61/1000 | Loss: 0.00002508
Iteration 62/1000 | Loss: 0.00002508
Iteration 63/1000 | Loss: 0.00002508
Iteration 64/1000 | Loss: 0.00002507
Iteration 65/1000 | Loss: 0.00002507
Iteration 66/1000 | Loss: 0.00002507
Iteration 67/1000 | Loss: 0.00002507
Iteration 68/1000 | Loss: 0.00002506
Iteration 69/1000 | Loss: 0.00002506
Iteration 70/1000 | Loss: 0.00002506
Iteration 71/1000 | Loss: 0.00002506
Iteration 72/1000 | Loss: 0.00002506
Iteration 73/1000 | Loss: 0.00002506
Iteration 74/1000 | Loss: 0.00002506
Iteration 75/1000 | Loss: 0.00002506
Iteration 76/1000 | Loss: 0.00002505
Iteration 77/1000 | Loss: 0.00002505
Iteration 78/1000 | Loss: 0.00002505
Iteration 79/1000 | Loss: 0.00002505
Iteration 80/1000 | Loss: 0.00002505
Iteration 81/1000 | Loss: 0.00002504
Iteration 82/1000 | Loss: 0.00002504
Iteration 83/1000 | Loss: 0.00002503
Iteration 84/1000 | Loss: 0.00002503
Iteration 85/1000 | Loss: 0.00002503
Iteration 86/1000 | Loss: 0.00002502
Iteration 87/1000 | Loss: 0.00002502
Iteration 88/1000 | Loss: 0.00002502
Iteration 89/1000 | Loss: 0.00002502
Iteration 90/1000 | Loss: 0.00002502
Iteration 91/1000 | Loss: 0.00002502
Iteration 92/1000 | Loss: 0.00002502
Iteration 93/1000 | Loss: 0.00002501
Iteration 94/1000 | Loss: 0.00002501
Iteration 95/1000 | Loss: 0.00002501
Iteration 96/1000 | Loss: 0.00002501
Iteration 97/1000 | Loss: 0.00002501
Iteration 98/1000 | Loss: 0.00002501
Iteration 99/1000 | Loss: 0.00002500
Iteration 100/1000 | Loss: 0.00002500
Iteration 101/1000 | Loss: 0.00002500
Iteration 102/1000 | Loss: 0.00002500
Iteration 103/1000 | Loss: 0.00002500
Iteration 104/1000 | Loss: 0.00002500
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00002500
Iteration 107/1000 | Loss: 0.00002500
Iteration 108/1000 | Loss: 0.00002499
Iteration 109/1000 | Loss: 0.00002499
Iteration 110/1000 | Loss: 0.00002499
Iteration 111/1000 | Loss: 0.00002499
Iteration 112/1000 | Loss: 0.00002499
Iteration 113/1000 | Loss: 0.00002499
Iteration 114/1000 | Loss: 0.00002499
Iteration 115/1000 | Loss: 0.00002499
Iteration 116/1000 | Loss: 0.00002499
Iteration 117/1000 | Loss: 0.00002499
Iteration 118/1000 | Loss: 0.00002499
Iteration 119/1000 | Loss: 0.00002499
Iteration 120/1000 | Loss: 0.00002499
Iteration 121/1000 | Loss: 0.00002499
Iteration 122/1000 | Loss: 0.00002499
Iteration 123/1000 | Loss: 0.00002498
Iteration 124/1000 | Loss: 0.00002498
Iteration 125/1000 | Loss: 0.00002498
Iteration 126/1000 | Loss: 0.00002498
Iteration 127/1000 | Loss: 0.00002498
Iteration 128/1000 | Loss: 0.00002498
Iteration 129/1000 | Loss: 0.00002498
Iteration 130/1000 | Loss: 0.00002498
Iteration 131/1000 | Loss: 0.00002498
Iteration 132/1000 | Loss: 0.00002498
Iteration 133/1000 | Loss: 0.00002497
Iteration 134/1000 | Loss: 0.00002497
Iteration 135/1000 | Loss: 0.00002497
Iteration 136/1000 | Loss: 0.00002497
Iteration 137/1000 | Loss: 0.00002497
Iteration 138/1000 | Loss: 0.00002497
Iteration 139/1000 | Loss: 0.00002497
Iteration 140/1000 | Loss: 0.00002497
Iteration 141/1000 | Loss: 0.00002497
Iteration 142/1000 | Loss: 0.00002497
Iteration 143/1000 | Loss: 0.00002497
Iteration 144/1000 | Loss: 0.00002497
Iteration 145/1000 | Loss: 0.00002497
Iteration 146/1000 | Loss: 0.00002497
Iteration 147/1000 | Loss: 0.00002497
Iteration 148/1000 | Loss: 0.00002497
Iteration 149/1000 | Loss: 0.00002497
Iteration 150/1000 | Loss: 0.00002497
Iteration 151/1000 | Loss: 0.00002497
Iteration 152/1000 | Loss: 0.00002497
Iteration 153/1000 | Loss: 0.00002497
Iteration 154/1000 | Loss: 0.00002497
Iteration 155/1000 | Loss: 0.00002497
Iteration 156/1000 | Loss: 0.00002497
Iteration 157/1000 | Loss: 0.00002497
Iteration 158/1000 | Loss: 0.00002497
Iteration 159/1000 | Loss: 0.00002497
Iteration 160/1000 | Loss: 0.00002497
Iteration 161/1000 | Loss: 0.00002497
Iteration 162/1000 | Loss: 0.00002497
Iteration 163/1000 | Loss: 0.00002497
Iteration 164/1000 | Loss: 0.00002497
Iteration 165/1000 | Loss: 0.00002497
Iteration 166/1000 | Loss: 0.00002497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.4965251213870943e-05, 2.4965251213870943e-05, 2.4965251213870943e-05, 2.4965251213870943e-05, 2.4965251213870943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4965251213870943e-05

Optimization complete. Final v2v error: 4.395721912384033 mm

Highest mean error: 4.896975040435791 mm for frame 87

Lowest mean error: 3.7503182888031006 mm for frame 239

Saving results

Total time: 41.72242188453674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085754
Iteration 2/25 | Loss: 0.01085753
Iteration 3/25 | Loss: 0.01085753
Iteration 4/25 | Loss: 0.01085753
Iteration 5/25 | Loss: 0.01085753
Iteration 6/25 | Loss: 0.01085753
Iteration 7/25 | Loss: 0.00305883
Iteration 8/25 | Loss: 0.00236340
Iteration 9/25 | Loss: 0.00205056
Iteration 10/25 | Loss: 0.00200711
Iteration 11/25 | Loss: 0.00207668
Iteration 12/25 | Loss: 0.00203074
Iteration 13/25 | Loss: 0.00193632
Iteration 14/25 | Loss: 0.00183488
Iteration 15/25 | Loss: 0.00182002
Iteration 16/25 | Loss: 0.00180078
Iteration 17/25 | Loss: 0.00177514
Iteration 18/25 | Loss: 0.00174505
Iteration 19/25 | Loss: 0.00172461
Iteration 20/25 | Loss: 0.00171994
Iteration 21/25 | Loss: 0.00171183
Iteration 22/25 | Loss: 0.00171765
Iteration 23/25 | Loss: 0.00170894
Iteration 24/25 | Loss: 0.00170706
Iteration 25/25 | Loss: 0.00170686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51949489
Iteration 2/25 | Loss: 0.00536673
Iteration 3/25 | Loss: 0.00493523
Iteration 4/25 | Loss: 0.00493523
Iteration 5/25 | Loss: 0.00493523
Iteration 6/25 | Loss: 0.00493523
Iteration 7/25 | Loss: 0.00493523
Iteration 8/25 | Loss: 0.00493523
Iteration 9/25 | Loss: 0.00493523
Iteration 10/25 | Loss: 0.00493523
Iteration 11/25 | Loss: 0.00493523
Iteration 12/25 | Loss: 0.00493523
Iteration 13/25 | Loss: 0.00493523
Iteration 14/25 | Loss: 0.00493523
Iteration 15/25 | Loss: 0.00493523
Iteration 16/25 | Loss: 0.00493523
Iteration 17/25 | Loss: 0.00493523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004935229197144508, 0.004935229197144508, 0.004935229197144508, 0.004935229197144508, 0.004935229197144508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004935229197144508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00493523
Iteration 2/1000 | Loss: 0.00261114
Iteration 3/1000 | Loss: 0.00175109
Iteration 4/1000 | Loss: 0.00078518
Iteration 5/1000 | Loss: 0.00208659
Iteration 6/1000 | Loss: 0.00088580
Iteration 7/1000 | Loss: 0.00191621
Iteration 8/1000 | Loss: 0.00283500
Iteration 9/1000 | Loss: 0.00111052
Iteration 10/1000 | Loss: 0.00201087
Iteration 11/1000 | Loss: 0.00197437
Iteration 12/1000 | Loss: 0.00145411
Iteration 13/1000 | Loss: 0.00071641
Iteration 14/1000 | Loss: 0.00110222
Iteration 15/1000 | Loss: 0.00111463
Iteration 16/1000 | Loss: 0.00139059
Iteration 17/1000 | Loss: 0.00093734
Iteration 18/1000 | Loss: 0.00119755
Iteration 19/1000 | Loss: 0.00264998
Iteration 20/1000 | Loss: 0.00121604
Iteration 21/1000 | Loss: 0.00104123
Iteration 22/1000 | Loss: 0.00132949
Iteration 23/1000 | Loss: 0.00070504
Iteration 24/1000 | Loss: 0.00085643
Iteration 25/1000 | Loss: 0.00079411
Iteration 26/1000 | Loss: 0.00149145
Iteration 27/1000 | Loss: 0.00078072
Iteration 28/1000 | Loss: 0.00101695
Iteration 29/1000 | Loss: 0.00073689
Iteration 30/1000 | Loss: 0.00089515
Iteration 31/1000 | Loss: 0.00075242
Iteration 32/1000 | Loss: 0.00063646
Iteration 33/1000 | Loss: 0.00085109
Iteration 34/1000 | Loss: 0.00088252
Iteration 35/1000 | Loss: 0.00133003
Iteration 36/1000 | Loss: 0.00116409
Iteration 37/1000 | Loss: 0.00120152
Iteration 38/1000 | Loss: 0.00101356
Iteration 39/1000 | Loss: 0.00064236
Iteration 40/1000 | Loss: 0.00064303
Iteration 41/1000 | Loss: 0.00060518
Iteration 42/1000 | Loss: 0.00047630
Iteration 43/1000 | Loss: 0.00059082
Iteration 44/1000 | Loss: 0.00060678
Iteration 45/1000 | Loss: 0.00052680
Iteration 46/1000 | Loss: 0.00066407
Iteration 47/1000 | Loss: 0.00197544
Iteration 48/1000 | Loss: 0.00060391
Iteration 49/1000 | Loss: 0.00040946
Iteration 50/1000 | Loss: 0.00043500
Iteration 51/1000 | Loss: 0.00086613
Iteration 52/1000 | Loss: 0.00070023
Iteration 53/1000 | Loss: 0.00066944
Iteration 54/1000 | Loss: 0.00125593
Iteration 55/1000 | Loss: 0.00090152
Iteration 56/1000 | Loss: 0.00065233
Iteration 57/1000 | Loss: 0.00071231
Iteration 58/1000 | Loss: 0.00068283
Iteration 59/1000 | Loss: 0.00119835
Iteration 60/1000 | Loss: 0.00091663
Iteration 61/1000 | Loss: 0.00084968
Iteration 62/1000 | Loss: 0.00086229
Iteration 63/1000 | Loss: 0.00045460
Iteration 64/1000 | Loss: 0.00130658
Iteration 65/1000 | Loss: 0.00094799
Iteration 66/1000 | Loss: 0.00077705
Iteration 67/1000 | Loss: 0.00087485
Iteration 68/1000 | Loss: 0.00080755
Iteration 69/1000 | Loss: 0.00064114
Iteration 70/1000 | Loss: 0.00065109
Iteration 71/1000 | Loss: 0.00059815
Iteration 72/1000 | Loss: 0.00082785
Iteration 73/1000 | Loss: 0.00088602
Iteration 74/1000 | Loss: 0.00079150
Iteration 75/1000 | Loss: 0.00106255
Iteration 76/1000 | Loss: 0.00051537
Iteration 77/1000 | Loss: 0.00128479
Iteration 78/1000 | Loss: 0.00107911
Iteration 79/1000 | Loss: 0.00070750
Iteration 80/1000 | Loss: 0.00113195
Iteration 81/1000 | Loss: 0.00169271
Iteration 82/1000 | Loss: 0.00103334
Iteration 83/1000 | Loss: 0.00060458
Iteration 84/1000 | Loss: 0.00081669
Iteration 85/1000 | Loss: 0.00079567
Iteration 86/1000 | Loss: 0.00061465
Iteration 87/1000 | Loss: 0.00046945
Iteration 88/1000 | Loss: 0.00107189
Iteration 89/1000 | Loss: 0.00056895
Iteration 90/1000 | Loss: 0.00091559
Iteration 91/1000 | Loss: 0.00102416
Iteration 92/1000 | Loss: 0.00068948
Iteration 93/1000 | Loss: 0.00073096
Iteration 94/1000 | Loss: 0.00081335
Iteration 95/1000 | Loss: 0.00135770
Iteration 96/1000 | Loss: 0.00210667
Iteration 97/1000 | Loss: 0.00148680
Iteration 98/1000 | Loss: 0.00115231
Iteration 99/1000 | Loss: 0.00049982
Iteration 100/1000 | Loss: 0.00051081
Iteration 101/1000 | Loss: 0.00044847
Iteration 102/1000 | Loss: 0.00062712
Iteration 103/1000 | Loss: 0.00045135
Iteration 104/1000 | Loss: 0.00118745
Iteration 105/1000 | Loss: 0.00060011
Iteration 106/1000 | Loss: 0.00122017
Iteration 107/1000 | Loss: 0.00162744
Iteration 108/1000 | Loss: 0.00225591
Iteration 109/1000 | Loss: 0.00071848
Iteration 110/1000 | Loss: 0.00062453
Iteration 111/1000 | Loss: 0.00054471
Iteration 112/1000 | Loss: 0.00070889
Iteration 113/1000 | Loss: 0.00047780
Iteration 114/1000 | Loss: 0.00248765
Iteration 115/1000 | Loss: 0.00212761
Iteration 116/1000 | Loss: 0.00138178
Iteration 117/1000 | Loss: 0.00083813
Iteration 118/1000 | Loss: 0.00062011
Iteration 119/1000 | Loss: 0.00038567
Iteration 120/1000 | Loss: 0.00063207
Iteration 121/1000 | Loss: 0.00050446
Iteration 122/1000 | Loss: 0.00048469
Iteration 123/1000 | Loss: 0.00073728
Iteration 124/1000 | Loss: 0.00059739
Iteration 125/1000 | Loss: 0.00103469
Iteration 126/1000 | Loss: 0.00134749
Iteration 127/1000 | Loss: 0.00053424
Iteration 128/1000 | Loss: 0.00039358
Iteration 129/1000 | Loss: 0.00053952
Iteration 130/1000 | Loss: 0.00030589
Iteration 131/1000 | Loss: 0.00031189
Iteration 132/1000 | Loss: 0.00029562
Iteration 133/1000 | Loss: 0.00028687
Iteration 134/1000 | Loss: 0.00044254
Iteration 135/1000 | Loss: 0.00047548
Iteration 136/1000 | Loss: 0.00046072
Iteration 137/1000 | Loss: 0.00072526
Iteration 138/1000 | Loss: 0.00046392
Iteration 139/1000 | Loss: 0.00036661
Iteration 140/1000 | Loss: 0.00040672
Iteration 141/1000 | Loss: 0.00047525
Iteration 142/1000 | Loss: 0.00051571
Iteration 143/1000 | Loss: 0.00051573
Iteration 144/1000 | Loss: 0.00038197
Iteration 145/1000 | Loss: 0.00032427
Iteration 146/1000 | Loss: 0.00033456
Iteration 147/1000 | Loss: 0.00033614
Iteration 148/1000 | Loss: 0.00028385
Iteration 149/1000 | Loss: 0.00032015
Iteration 150/1000 | Loss: 0.00038078
Iteration 151/1000 | Loss: 0.00037169
Iteration 152/1000 | Loss: 0.00038643
Iteration 153/1000 | Loss: 0.00039090
Iteration 154/1000 | Loss: 0.00031266
Iteration 155/1000 | Loss: 0.00044173
Iteration 156/1000 | Loss: 0.00031334
Iteration 157/1000 | Loss: 0.00055614
Iteration 158/1000 | Loss: 0.00047608
Iteration 159/1000 | Loss: 0.00038304
Iteration 160/1000 | Loss: 0.00037422
Iteration 161/1000 | Loss: 0.00034848
Iteration 162/1000 | Loss: 0.00036360
Iteration 163/1000 | Loss: 0.00037193
Iteration 164/1000 | Loss: 0.00037708
Iteration 165/1000 | Loss: 0.00036327
Iteration 166/1000 | Loss: 0.00035995
Iteration 167/1000 | Loss: 0.00035498
Iteration 168/1000 | Loss: 0.00034993
Iteration 169/1000 | Loss: 0.00036709
Iteration 170/1000 | Loss: 0.00035743
Iteration 171/1000 | Loss: 0.00033847
Iteration 172/1000 | Loss: 0.00035805
Iteration 173/1000 | Loss: 0.00117186
Iteration 174/1000 | Loss: 0.00114002
Iteration 175/1000 | Loss: 0.00108485
Iteration 176/1000 | Loss: 0.00066438
Iteration 177/1000 | Loss: 0.00048658
Iteration 178/1000 | Loss: 0.00056651
Iteration 179/1000 | Loss: 0.00083899
Iteration 180/1000 | Loss: 0.00066489
Iteration 181/1000 | Loss: 0.00061237
Iteration 182/1000 | Loss: 0.00044494
Iteration 183/1000 | Loss: 0.00048903
Iteration 184/1000 | Loss: 0.00069252
Iteration 185/1000 | Loss: 0.00038437
Iteration 186/1000 | Loss: 0.00046454
Iteration 187/1000 | Loss: 0.00047468
Iteration 188/1000 | Loss: 0.00044034
Iteration 189/1000 | Loss: 0.00044759
Iteration 190/1000 | Loss: 0.00070593
Iteration 191/1000 | Loss: 0.00048396
Iteration 192/1000 | Loss: 0.00060716
Iteration 193/1000 | Loss: 0.00048549
Iteration 194/1000 | Loss: 0.00045634
Iteration 195/1000 | Loss: 0.00044236
Iteration 196/1000 | Loss: 0.00046564
Iteration 197/1000 | Loss: 0.00045687
Iteration 198/1000 | Loss: 0.00048678
Iteration 199/1000 | Loss: 0.00039699
Iteration 200/1000 | Loss: 0.00043738
Iteration 201/1000 | Loss: 0.00046276
Iteration 202/1000 | Loss: 0.00045381
Iteration 203/1000 | Loss: 0.00089540
Iteration 204/1000 | Loss: 0.00049484
Iteration 205/1000 | Loss: 0.00043766
Iteration 206/1000 | Loss: 0.00051553
Iteration 207/1000 | Loss: 0.00055367
Iteration 208/1000 | Loss: 0.00048962
Iteration 209/1000 | Loss: 0.00040595
Iteration 210/1000 | Loss: 0.00043116
Iteration 211/1000 | Loss: 0.00047157
Iteration 212/1000 | Loss: 0.00045340
Iteration 213/1000 | Loss: 0.00032851
Iteration 214/1000 | Loss: 0.00051205
Iteration 215/1000 | Loss: 0.00051911
Iteration 216/1000 | Loss: 0.00028379
Iteration 217/1000 | Loss: 0.00036640
Iteration 218/1000 | Loss: 0.00037709
Iteration 219/1000 | Loss: 0.00036896
Iteration 220/1000 | Loss: 0.00038248
Iteration 221/1000 | Loss: 0.00047272
Iteration 222/1000 | Loss: 0.00051259
Iteration 223/1000 | Loss: 0.00041266
Iteration 224/1000 | Loss: 0.00042364
Iteration 225/1000 | Loss: 0.00035908
Iteration 226/1000 | Loss: 0.00033026
Iteration 227/1000 | Loss: 0.00037210
Iteration 228/1000 | Loss: 0.00052208
Iteration 229/1000 | Loss: 0.00082497
Iteration 230/1000 | Loss: 0.00041559
Iteration 231/1000 | Loss: 0.00054182
Iteration 232/1000 | Loss: 0.00051258
Iteration 233/1000 | Loss: 0.00042292
Iteration 234/1000 | Loss: 0.00043781
Iteration 235/1000 | Loss: 0.00036731
Iteration 236/1000 | Loss: 0.00054185
Iteration 237/1000 | Loss: 0.00036166
Iteration 238/1000 | Loss: 0.00047464
Iteration 239/1000 | Loss: 0.00059193
Iteration 240/1000 | Loss: 0.00046303
Iteration 241/1000 | Loss: 0.00046602
Iteration 242/1000 | Loss: 0.00030282
Iteration 243/1000 | Loss: 0.00029607
Iteration 244/1000 | Loss: 0.00027035
Iteration 245/1000 | Loss: 0.00026133
Iteration 246/1000 | Loss: 0.00027653
Iteration 247/1000 | Loss: 0.00027982
Iteration 248/1000 | Loss: 0.00027197
Iteration 249/1000 | Loss: 0.00029842
Iteration 250/1000 | Loss: 0.00027257
Iteration 251/1000 | Loss: 0.00029084
Iteration 252/1000 | Loss: 0.00027057
Iteration 253/1000 | Loss: 0.00030650
Iteration 254/1000 | Loss: 0.00027160
Iteration 255/1000 | Loss: 0.00026684
Iteration 256/1000 | Loss: 0.00027718
Iteration 257/1000 | Loss: 0.00028072
Iteration 258/1000 | Loss: 0.00027077
Iteration 259/1000 | Loss: 0.00057472
Iteration 260/1000 | Loss: 0.00029771
Iteration 261/1000 | Loss: 0.00027876
Iteration 262/1000 | Loss: 0.00029008
Iteration 263/1000 | Loss: 0.00052591
Iteration 264/1000 | Loss: 0.00030116
Iteration 265/1000 | Loss: 0.00032746
Iteration 266/1000 | Loss: 0.00027398
Iteration 267/1000 | Loss: 0.00030406
Iteration 268/1000 | Loss: 0.00029560
Iteration 269/1000 | Loss: 0.00029947
Iteration 270/1000 | Loss: 0.00029129
Iteration 271/1000 | Loss: 0.00030359
Iteration 272/1000 | Loss: 0.00029200
Iteration 273/1000 | Loss: 0.00029629
Iteration 274/1000 | Loss: 0.00041440
Iteration 275/1000 | Loss: 0.00031155
Iteration 276/1000 | Loss: 0.00029938
Iteration 277/1000 | Loss: 0.00029327
Iteration 278/1000 | Loss: 0.00041995
Iteration 279/1000 | Loss: 0.00030880
Iteration 280/1000 | Loss: 0.00034958
Iteration 281/1000 | Loss: 0.00030990
Iteration 282/1000 | Loss: 0.00047291
Iteration 283/1000 | Loss: 0.00030079
Iteration 284/1000 | Loss: 0.00027266
Iteration 285/1000 | Loss: 0.00027643
Iteration 286/1000 | Loss: 0.00028513
Iteration 287/1000 | Loss: 0.00027965
Iteration 288/1000 | Loss: 0.00029249
Iteration 289/1000 | Loss: 0.00054662
Iteration 290/1000 | Loss: 0.00048421
Iteration 291/1000 | Loss: 0.00028149
Iteration 292/1000 | Loss: 0.00030605
Iteration 293/1000 | Loss: 0.00037362
Iteration 294/1000 | Loss: 0.00025494
Iteration 295/1000 | Loss: 0.00026828
Iteration 296/1000 | Loss: 0.00025928
Iteration 297/1000 | Loss: 0.00025540
Iteration 298/1000 | Loss: 0.00057513
Iteration 299/1000 | Loss: 0.00025931
Iteration 300/1000 | Loss: 0.00025714
Iteration 301/1000 | Loss: 0.00024662
Iteration 302/1000 | Loss: 0.00025413
Iteration 303/1000 | Loss: 0.00024579
Iteration 304/1000 | Loss: 0.00024641
Iteration 305/1000 | Loss: 0.00045376
Iteration 306/1000 | Loss: 0.00040913
Iteration 307/1000 | Loss: 0.00033358
Iteration 308/1000 | Loss: 0.00042370
Iteration 309/1000 | Loss: 0.00024411
Iteration 310/1000 | Loss: 0.00024322
Iteration 311/1000 | Loss: 0.00024200
Iteration 312/1000 | Loss: 0.00025144
Iteration 313/1000 | Loss: 0.00024752
Iteration 314/1000 | Loss: 0.00025552
Iteration 315/1000 | Loss: 0.00025373
Iteration 316/1000 | Loss: 0.00025964
Iteration 317/1000 | Loss: 0.00026056
Iteration 318/1000 | Loss: 0.00025855
Iteration 319/1000 | Loss: 0.00048416
Iteration 320/1000 | Loss: 0.00048832
Iteration 321/1000 | Loss: 0.00028298
Iteration 322/1000 | Loss: 0.00026025
Iteration 323/1000 | Loss: 0.00026022
Iteration 324/1000 | Loss: 0.00024090
Iteration 325/1000 | Loss: 0.00024025
Iteration 326/1000 | Loss: 0.00040336
Iteration 327/1000 | Loss: 0.00031023
Iteration 328/1000 | Loss: 0.00053485
Iteration 329/1000 | Loss: 0.00039185
Iteration 330/1000 | Loss: 0.00029406
Iteration 331/1000 | Loss: 0.00024513
Iteration 332/1000 | Loss: 0.00025436
Iteration 333/1000 | Loss: 0.00024135
Iteration 334/1000 | Loss: 0.00023967
Iteration 335/1000 | Loss: 0.00023895
Iteration 336/1000 | Loss: 0.00023867
Iteration 337/1000 | Loss: 0.00023866
Iteration 338/1000 | Loss: 0.00039577
Iteration 339/1000 | Loss: 0.00054062
Iteration 340/1000 | Loss: 0.00043493
Iteration 341/1000 | Loss: 0.00024946
Iteration 342/1000 | Loss: 0.00024202
Iteration 343/1000 | Loss: 0.00023942
Iteration 344/1000 | Loss: 0.00024751
Iteration 345/1000 | Loss: 0.00024245
Iteration 346/1000 | Loss: 0.00023785
Iteration 347/1000 | Loss: 0.00023739
Iteration 348/1000 | Loss: 0.00024673
Iteration 349/1000 | Loss: 0.00024483
Iteration 350/1000 | Loss: 0.00036487
Iteration 351/1000 | Loss: 0.00029424
Iteration 352/1000 | Loss: 0.00024231
Iteration 353/1000 | Loss: 0.00024115
Iteration 354/1000 | Loss: 0.00023979
Iteration 355/1000 | Loss: 0.00023764
Iteration 356/1000 | Loss: 0.00023616
Iteration 357/1000 | Loss: 0.00023564
Iteration 358/1000 | Loss: 0.00023559
Iteration 359/1000 | Loss: 0.00024487
Iteration 360/1000 | Loss: 0.00024358
Iteration 361/1000 | Loss: 0.00024221
Iteration 362/1000 | Loss: 0.00024019
Iteration 363/1000 | Loss: 0.00023883
Iteration 364/1000 | Loss: 0.00023635
Iteration 365/1000 | Loss: 0.00023550
Iteration 366/1000 | Loss: 0.00023516
Iteration 367/1000 | Loss: 0.00023514
Iteration 368/1000 | Loss: 0.00023514
Iteration 369/1000 | Loss: 0.00023513
Iteration 370/1000 | Loss: 0.00023512
Iteration 371/1000 | Loss: 0.00023512
Iteration 372/1000 | Loss: 0.00023512
Iteration 373/1000 | Loss: 0.00023512
Iteration 374/1000 | Loss: 0.00023512
Iteration 375/1000 | Loss: 0.00023512
Iteration 376/1000 | Loss: 0.00023512
Iteration 377/1000 | Loss: 0.00023512
Iteration 378/1000 | Loss: 0.00023512
Iteration 379/1000 | Loss: 0.00023512
Iteration 380/1000 | Loss: 0.00023511
Iteration 381/1000 | Loss: 0.00023511
Iteration 382/1000 | Loss: 0.00023511
Iteration 383/1000 | Loss: 0.00023511
Iteration 384/1000 | Loss: 0.00023511
Iteration 385/1000 | Loss: 0.00023510
Iteration 386/1000 | Loss: 0.00023510
Iteration 387/1000 | Loss: 0.00023510
Iteration 388/1000 | Loss: 0.00023510
Iteration 389/1000 | Loss: 0.00023510
Iteration 390/1000 | Loss: 0.00023509
Iteration 391/1000 | Loss: 0.00023509
Iteration 392/1000 | Loss: 0.00023509
Iteration 393/1000 | Loss: 0.00023509
Iteration 394/1000 | Loss: 0.00023509
Iteration 395/1000 | Loss: 0.00023508
Iteration 396/1000 | Loss: 0.00023508
Iteration 397/1000 | Loss: 0.00023508
Iteration 398/1000 | Loss: 0.00023508
Iteration 399/1000 | Loss: 0.00023508
Iteration 400/1000 | Loss: 0.00023508
Iteration 401/1000 | Loss: 0.00023508
Iteration 402/1000 | Loss: 0.00023508
Iteration 403/1000 | Loss: 0.00023508
Iteration 404/1000 | Loss: 0.00023508
Iteration 405/1000 | Loss: 0.00023508
Iteration 406/1000 | Loss: 0.00023507
Iteration 407/1000 | Loss: 0.00023507
Iteration 408/1000 | Loss: 0.00023507
Iteration 409/1000 | Loss: 0.00023506
Iteration 410/1000 | Loss: 0.00023506
Iteration 411/1000 | Loss: 0.00023506
Iteration 412/1000 | Loss: 0.00023506
Iteration 413/1000 | Loss: 0.00023506
Iteration 414/1000 | Loss: 0.00023506
Iteration 415/1000 | Loss: 0.00023506
Iteration 416/1000 | Loss: 0.00023506
Iteration 417/1000 | Loss: 0.00023506
Iteration 418/1000 | Loss: 0.00023506
Iteration 419/1000 | Loss: 0.00023506
Iteration 420/1000 | Loss: 0.00023506
Iteration 421/1000 | Loss: 0.00023506
Iteration 422/1000 | Loss: 0.00023506
Iteration 423/1000 | Loss: 0.00023506
Iteration 424/1000 | Loss: 0.00023506
Iteration 425/1000 | Loss: 0.00023506
Iteration 426/1000 | Loss: 0.00023506
Iteration 427/1000 | Loss: 0.00023506
Iteration 428/1000 | Loss: 0.00023506
Iteration 429/1000 | Loss: 0.00023506
Iteration 430/1000 | Loss: 0.00023506
Iteration 431/1000 | Loss: 0.00023506
Iteration 432/1000 | Loss: 0.00023506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 432. Stopping optimization.
Last 5 losses: [0.00023506379511673003, 0.00023506379511673003, 0.00023506379511673003, 0.00023506379511673003, 0.00023506379511673003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023506379511673003

Optimization complete. Final v2v error: 8.956881523132324 mm

Highest mean error: 14.701166152954102 mm for frame 93

Lowest mean error: 5.443370342254639 mm for frame 9

Saving results

Total time: 631.2223417758942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00614016
Iteration 2/25 | Loss: 0.00150902
Iteration 3/25 | Loss: 0.00133121
Iteration 4/25 | Loss: 0.00131270
Iteration 5/25 | Loss: 0.00130818
Iteration 6/25 | Loss: 0.00130674
Iteration 7/25 | Loss: 0.00130674
Iteration 8/25 | Loss: 0.00130674
Iteration 9/25 | Loss: 0.00130674
Iteration 10/25 | Loss: 0.00130674
Iteration 11/25 | Loss: 0.00130674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013067356776446104, 0.0013067356776446104, 0.0013067356776446104, 0.0013067356776446104, 0.0013067356776446104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013067356776446104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78543472
Iteration 2/25 | Loss: 0.00122115
Iteration 3/25 | Loss: 0.00122112
Iteration 4/25 | Loss: 0.00122112
Iteration 5/25 | Loss: 0.00122112
Iteration 6/25 | Loss: 0.00122112
Iteration 7/25 | Loss: 0.00122112
Iteration 8/25 | Loss: 0.00122112
Iteration 9/25 | Loss: 0.00122112
Iteration 10/25 | Loss: 0.00122112
Iteration 11/25 | Loss: 0.00122112
Iteration 12/25 | Loss: 0.00122112
Iteration 13/25 | Loss: 0.00122112
Iteration 14/25 | Loss: 0.00122112
Iteration 15/25 | Loss: 0.00122112
Iteration 16/25 | Loss: 0.00122112
Iteration 17/25 | Loss: 0.00122112
Iteration 18/25 | Loss: 0.00122112
Iteration 19/25 | Loss: 0.00122112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012211173307150602, 0.0012211173307150602, 0.0012211173307150602, 0.0012211173307150602, 0.0012211173307150602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012211173307150602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122112
Iteration 2/1000 | Loss: 0.00004653
Iteration 3/1000 | Loss: 0.00003458
Iteration 4/1000 | Loss: 0.00003100
Iteration 5/1000 | Loss: 0.00002966
Iteration 6/1000 | Loss: 0.00002858
Iteration 7/1000 | Loss: 0.00002784
Iteration 8/1000 | Loss: 0.00002729
Iteration 9/1000 | Loss: 0.00002683
Iteration 10/1000 | Loss: 0.00002650
Iteration 11/1000 | Loss: 0.00002640
Iteration 12/1000 | Loss: 0.00002632
Iteration 13/1000 | Loss: 0.00002618
Iteration 14/1000 | Loss: 0.00002616
Iteration 15/1000 | Loss: 0.00002602
Iteration 16/1000 | Loss: 0.00002596
Iteration 17/1000 | Loss: 0.00002596
Iteration 18/1000 | Loss: 0.00002593
Iteration 19/1000 | Loss: 0.00002593
Iteration 20/1000 | Loss: 0.00002592
Iteration 21/1000 | Loss: 0.00002592
Iteration 22/1000 | Loss: 0.00002591
Iteration 23/1000 | Loss: 0.00002591
Iteration 24/1000 | Loss: 0.00002590
Iteration 25/1000 | Loss: 0.00002590
Iteration 26/1000 | Loss: 0.00002589
Iteration 27/1000 | Loss: 0.00002589
Iteration 28/1000 | Loss: 0.00002589
Iteration 29/1000 | Loss: 0.00002588
Iteration 30/1000 | Loss: 0.00002587
Iteration 31/1000 | Loss: 0.00002587
Iteration 32/1000 | Loss: 0.00002586
Iteration 33/1000 | Loss: 0.00002586
Iteration 34/1000 | Loss: 0.00002586
Iteration 35/1000 | Loss: 0.00002585
Iteration 36/1000 | Loss: 0.00002585
Iteration 37/1000 | Loss: 0.00002585
Iteration 38/1000 | Loss: 0.00002584
Iteration 39/1000 | Loss: 0.00002584
Iteration 40/1000 | Loss: 0.00002583
Iteration 41/1000 | Loss: 0.00002583
Iteration 42/1000 | Loss: 0.00002583
Iteration 43/1000 | Loss: 0.00002582
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002581
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002580
Iteration 48/1000 | Loss: 0.00002580
Iteration 49/1000 | Loss: 0.00002580
Iteration 50/1000 | Loss: 0.00002579
Iteration 51/1000 | Loss: 0.00002579
Iteration 52/1000 | Loss: 0.00002579
Iteration 53/1000 | Loss: 0.00002579
Iteration 54/1000 | Loss: 0.00002578
Iteration 55/1000 | Loss: 0.00002578
Iteration 56/1000 | Loss: 0.00002578
Iteration 57/1000 | Loss: 0.00002578
Iteration 58/1000 | Loss: 0.00002577
Iteration 59/1000 | Loss: 0.00002577
Iteration 60/1000 | Loss: 0.00002577
Iteration 61/1000 | Loss: 0.00002576
Iteration 62/1000 | Loss: 0.00002576
Iteration 63/1000 | Loss: 0.00002576
Iteration 64/1000 | Loss: 0.00002575
Iteration 65/1000 | Loss: 0.00002575
Iteration 66/1000 | Loss: 0.00002575
Iteration 67/1000 | Loss: 0.00002575
Iteration 68/1000 | Loss: 0.00002575
Iteration 69/1000 | Loss: 0.00002574
Iteration 70/1000 | Loss: 0.00002574
Iteration 71/1000 | Loss: 0.00002574
Iteration 72/1000 | Loss: 0.00002573
Iteration 73/1000 | Loss: 0.00002573
Iteration 74/1000 | Loss: 0.00002573
Iteration 75/1000 | Loss: 0.00002572
Iteration 76/1000 | Loss: 0.00002572
Iteration 77/1000 | Loss: 0.00002572
Iteration 78/1000 | Loss: 0.00002572
Iteration 79/1000 | Loss: 0.00002571
Iteration 80/1000 | Loss: 0.00002571
Iteration 81/1000 | Loss: 0.00002571
Iteration 82/1000 | Loss: 0.00002571
Iteration 83/1000 | Loss: 0.00002571
Iteration 84/1000 | Loss: 0.00002570
Iteration 85/1000 | Loss: 0.00002570
Iteration 86/1000 | Loss: 0.00002570
Iteration 87/1000 | Loss: 0.00002570
Iteration 88/1000 | Loss: 0.00002569
Iteration 89/1000 | Loss: 0.00002569
Iteration 90/1000 | Loss: 0.00002569
Iteration 91/1000 | Loss: 0.00002569
Iteration 92/1000 | Loss: 0.00002569
Iteration 93/1000 | Loss: 0.00002569
Iteration 94/1000 | Loss: 0.00002569
Iteration 95/1000 | Loss: 0.00002569
Iteration 96/1000 | Loss: 0.00002569
Iteration 97/1000 | Loss: 0.00002569
Iteration 98/1000 | Loss: 0.00002569
Iteration 99/1000 | Loss: 0.00002569
Iteration 100/1000 | Loss: 0.00002569
Iteration 101/1000 | Loss: 0.00002568
Iteration 102/1000 | Loss: 0.00002568
Iteration 103/1000 | Loss: 0.00002568
Iteration 104/1000 | Loss: 0.00002568
Iteration 105/1000 | Loss: 0.00002568
Iteration 106/1000 | Loss: 0.00002568
Iteration 107/1000 | Loss: 0.00002568
Iteration 108/1000 | Loss: 0.00002568
Iteration 109/1000 | Loss: 0.00002568
Iteration 110/1000 | Loss: 0.00002568
Iteration 111/1000 | Loss: 0.00002568
Iteration 112/1000 | Loss: 0.00002568
Iteration 113/1000 | Loss: 0.00002568
Iteration 114/1000 | Loss: 0.00002568
Iteration 115/1000 | Loss: 0.00002568
Iteration 116/1000 | Loss: 0.00002568
Iteration 117/1000 | Loss: 0.00002568
Iteration 118/1000 | Loss: 0.00002568
Iteration 119/1000 | Loss: 0.00002568
Iteration 120/1000 | Loss: 0.00002568
Iteration 121/1000 | Loss: 0.00002568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.568060881458223e-05, 2.568060881458223e-05, 2.568060881458223e-05, 2.568060881458223e-05, 2.568060881458223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.568060881458223e-05

Optimization complete. Final v2v error: 4.328775882720947 mm

Highest mean error: 5.075538158416748 mm for frame 145

Lowest mean error: 3.6431498527526855 mm for frame 198

Saving results

Total time: 40.69628572463989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01181823
Iteration 2/25 | Loss: 0.01181823
Iteration 3/25 | Loss: 0.01181823
Iteration 4/25 | Loss: 0.01181823
Iteration 5/25 | Loss: 0.01181823
Iteration 6/25 | Loss: 0.01181823
Iteration 7/25 | Loss: 0.01181823
Iteration 8/25 | Loss: 0.01181823
Iteration 9/25 | Loss: 0.01181823
Iteration 10/25 | Loss: 0.01181822
Iteration 11/25 | Loss: 0.01181822
Iteration 12/25 | Loss: 0.01181822
Iteration 13/25 | Loss: 0.01181822
Iteration 14/25 | Loss: 0.01181822
Iteration 15/25 | Loss: 0.01181822
Iteration 16/25 | Loss: 0.01181822
Iteration 17/25 | Loss: 0.01181822
Iteration 18/25 | Loss: 0.01181822
Iteration 19/25 | Loss: 0.01181822
Iteration 20/25 | Loss: 0.01181822
Iteration 21/25 | Loss: 0.01181821
Iteration 22/25 | Loss: 0.01181821
Iteration 23/25 | Loss: 0.01181821
Iteration 24/25 | Loss: 0.01181821
Iteration 25/25 | Loss: 0.01181821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68645251
Iteration 2/25 | Loss: 0.08806219
Iteration 3/25 | Loss: 0.08803634
Iteration 4/25 | Loss: 0.08803935
Iteration 5/25 | Loss: 0.08802982
Iteration 6/25 | Loss: 0.08802981
Iteration 7/25 | Loss: 0.08802979
Iteration 8/25 | Loss: 0.08802979
Iteration 9/25 | Loss: 0.08802979
Iteration 10/25 | Loss: 0.08802979
Iteration 11/25 | Loss: 0.08802979
Iteration 12/25 | Loss: 0.08802979
Iteration 13/25 | Loss: 0.08802979
Iteration 14/25 | Loss: 0.08802979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.08802978694438934, 0.08802978694438934, 0.08802978694438934, 0.08802978694438934, 0.08802978694438934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08802978694438934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08802979
Iteration 2/1000 | Loss: 0.00082687
Iteration 3/1000 | Loss: 0.00024268
Iteration 4/1000 | Loss: 0.00038019
Iteration 5/1000 | Loss: 0.00007129
Iteration 6/1000 | Loss: 0.00005730
Iteration 7/1000 | Loss: 0.00005152
Iteration 8/1000 | Loss: 0.00004500
Iteration 9/1000 | Loss: 0.00003981
Iteration 10/1000 | Loss: 0.00003578
Iteration 11/1000 | Loss: 0.00003447
Iteration 12/1000 | Loss: 0.00003215
Iteration 13/1000 | Loss: 0.00003058
Iteration 14/1000 | Loss: 0.00002908
Iteration 15/1000 | Loss: 0.00002792
Iteration 16/1000 | Loss: 0.00002681
Iteration 17/1000 | Loss: 0.00002623
Iteration 18/1000 | Loss: 0.00002546
Iteration 19/1000 | Loss: 0.00002499
Iteration 20/1000 | Loss: 0.00002457
Iteration 21/1000 | Loss: 0.00002419
Iteration 22/1000 | Loss: 0.00002396
Iteration 23/1000 | Loss: 0.00002377
Iteration 24/1000 | Loss: 0.00002361
Iteration 25/1000 | Loss: 0.00002351
Iteration 26/1000 | Loss: 0.00002345
Iteration 27/1000 | Loss: 0.00002345
Iteration 28/1000 | Loss: 0.00002342
Iteration 29/1000 | Loss: 0.00002601
Iteration 30/1000 | Loss: 0.00002601
Iteration 31/1000 | Loss: 0.00002377
Iteration 32/1000 | Loss: 0.00002333
Iteration 33/1000 | Loss: 0.00002332
Iteration 34/1000 | Loss: 0.00002332
Iteration 35/1000 | Loss: 0.00002332
Iteration 36/1000 | Loss: 0.00002332
Iteration 37/1000 | Loss: 0.00002332
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002332
Iteration 41/1000 | Loss: 0.00002332
Iteration 42/1000 | Loss: 0.00002549
Iteration 43/1000 | Loss: 0.00002373
Iteration 44/1000 | Loss: 0.00002327
Iteration 45/1000 | Loss: 0.00002437
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00002430
Iteration 48/1000 | Loss: 0.00002357
Iteration 49/1000 | Loss: 0.00002417
Iteration 50/1000 | Loss: 0.00002335
Iteration 51/1000 | Loss: 0.00002409
Iteration 52/1000 | Loss: 0.00002420
Iteration 53/1000 | Loss: 0.00002420
Iteration 54/1000 | Loss: 0.00002325
Iteration 55/1000 | Loss: 0.00002318
Iteration 56/1000 | Loss: 0.00002318
Iteration 57/1000 | Loss: 0.00002317
Iteration 58/1000 | Loss: 0.00002317
Iteration 59/1000 | Loss: 0.00002317
Iteration 60/1000 | Loss: 0.00002317
Iteration 61/1000 | Loss: 0.00002316
Iteration 62/1000 | Loss: 0.00002316
Iteration 63/1000 | Loss: 0.00002316
Iteration 64/1000 | Loss: 0.00002315
Iteration 65/1000 | Loss: 0.00002315
Iteration 66/1000 | Loss: 0.00002315
Iteration 67/1000 | Loss: 0.00002315
Iteration 68/1000 | Loss: 0.00002315
Iteration 69/1000 | Loss: 0.00002315
Iteration 70/1000 | Loss: 0.00002315
Iteration 71/1000 | Loss: 0.00002315
Iteration 72/1000 | Loss: 0.00002315
Iteration 73/1000 | Loss: 0.00002314
Iteration 74/1000 | Loss: 0.00002314
Iteration 75/1000 | Loss: 0.00002314
Iteration 76/1000 | Loss: 0.00002314
Iteration 77/1000 | Loss: 0.00002314
Iteration 78/1000 | Loss: 0.00002314
Iteration 79/1000 | Loss: 0.00002314
Iteration 80/1000 | Loss: 0.00002314
Iteration 81/1000 | Loss: 0.00002314
Iteration 82/1000 | Loss: 0.00002314
Iteration 83/1000 | Loss: 0.00002314
Iteration 84/1000 | Loss: 0.00002314
Iteration 85/1000 | Loss: 0.00002314
Iteration 86/1000 | Loss: 0.00002314
Iteration 87/1000 | Loss: 0.00002314
Iteration 88/1000 | Loss: 0.00002314
Iteration 89/1000 | Loss: 0.00002314
Iteration 90/1000 | Loss: 0.00002314
Iteration 91/1000 | Loss: 0.00002314
Iteration 92/1000 | Loss: 0.00002314
Iteration 93/1000 | Loss: 0.00002314
Iteration 94/1000 | Loss: 0.00002314
Iteration 95/1000 | Loss: 0.00002314
Iteration 96/1000 | Loss: 0.00002314
Iteration 97/1000 | Loss: 0.00002314
Iteration 98/1000 | Loss: 0.00002314
Iteration 99/1000 | Loss: 0.00002314
Iteration 100/1000 | Loss: 0.00002314
Iteration 101/1000 | Loss: 0.00002314
Iteration 102/1000 | Loss: 0.00002314
Iteration 103/1000 | Loss: 0.00002314
Iteration 104/1000 | Loss: 0.00002314
Iteration 105/1000 | Loss: 0.00002314
Iteration 106/1000 | Loss: 0.00002314
Iteration 107/1000 | Loss: 0.00002314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.3140597477322444e-05, 2.3140597477322444e-05, 2.3140597477322444e-05, 2.3140597477322444e-05, 2.3140597477322444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3140597477322444e-05

Optimization complete. Final v2v error: 4.186583995819092 mm

Highest mean error: 11.17322063446045 mm for frame 66

Lowest mean error: 3.6409313678741455 mm for frame 0

Saving results

Total time: 71.31297826766968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105892
Iteration 2/25 | Loss: 0.00181386
Iteration 3/25 | Loss: 0.00165219
Iteration 4/25 | Loss: 0.00145393
Iteration 5/25 | Loss: 0.00147475
Iteration 6/25 | Loss: 0.00144058
Iteration 7/25 | Loss: 0.00141938
Iteration 8/25 | Loss: 0.00142728
Iteration 9/25 | Loss: 0.00135463
Iteration 10/25 | Loss: 0.00135038
Iteration 11/25 | Loss: 0.00133746
Iteration 12/25 | Loss: 0.00133394
Iteration 13/25 | Loss: 0.00133330
Iteration 14/25 | Loss: 0.00133314
Iteration 15/25 | Loss: 0.00133307
Iteration 16/25 | Loss: 0.00133290
Iteration 17/25 | Loss: 0.00133272
Iteration 18/25 | Loss: 0.00133255
Iteration 19/25 | Loss: 0.00133240
Iteration 20/25 | Loss: 0.00133224
Iteration 21/25 | Loss: 0.00133204
Iteration 22/25 | Loss: 0.00133159
Iteration 23/25 | Loss: 0.00133655
Iteration 24/25 | Loss: 0.00133594
Iteration 25/25 | Loss: 0.00134353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.01931167
Iteration 2/25 | Loss: 0.00149002
Iteration 3/25 | Loss: 0.00149001
Iteration 4/25 | Loss: 0.00149001
Iteration 5/25 | Loss: 0.00149001
Iteration 6/25 | Loss: 0.00149001
Iteration 7/25 | Loss: 0.00149001
Iteration 8/25 | Loss: 0.00149001
Iteration 9/25 | Loss: 0.00149001
Iteration 10/25 | Loss: 0.00149001
Iteration 11/25 | Loss: 0.00149001
Iteration 12/25 | Loss: 0.00149001
Iteration 13/25 | Loss: 0.00149001
Iteration 14/25 | Loss: 0.00149001
Iteration 15/25 | Loss: 0.00149001
Iteration 16/25 | Loss: 0.00149001
Iteration 17/25 | Loss: 0.00149001
Iteration 18/25 | Loss: 0.00149001
Iteration 19/25 | Loss: 0.00149001
Iteration 20/25 | Loss: 0.00149001
Iteration 21/25 | Loss: 0.00149001
Iteration 22/25 | Loss: 0.00149001
Iteration 23/25 | Loss: 0.00149001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014900059904903173, 0.0014900059904903173, 0.0014900059904903173, 0.0014900059904903173, 0.0014900059904903173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014900059904903173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149001
Iteration 2/1000 | Loss: 0.00010727
Iteration 3/1000 | Loss: 0.00147623
Iteration 4/1000 | Loss: 0.00010784
Iteration 5/1000 | Loss: 0.00006672
Iteration 6/1000 | Loss: 0.00005566
Iteration 7/1000 | Loss: 0.00102617
Iteration 8/1000 | Loss: 0.00135339
Iteration 9/1000 | Loss: 0.00161415
Iteration 10/1000 | Loss: 0.00112908
Iteration 11/1000 | Loss: 0.00011333
Iteration 12/1000 | Loss: 0.00006821
Iteration 13/1000 | Loss: 0.00005437
Iteration 14/1000 | Loss: 0.00053306
Iteration 15/1000 | Loss: 0.00019882
Iteration 16/1000 | Loss: 0.00029150
Iteration 17/1000 | Loss: 0.00030697
Iteration 18/1000 | Loss: 0.00022631
Iteration 19/1000 | Loss: 0.00023748
Iteration 20/1000 | Loss: 0.00024123
Iteration 21/1000 | Loss: 0.00022789
Iteration 22/1000 | Loss: 0.00018426
Iteration 23/1000 | Loss: 0.00018931
Iteration 24/1000 | Loss: 0.00012380
Iteration 25/1000 | Loss: 0.00003951
Iteration 26/1000 | Loss: 0.00003584
Iteration 27/1000 | Loss: 0.00003368
Iteration 28/1000 | Loss: 0.00003228
Iteration 29/1000 | Loss: 0.00003125
Iteration 30/1000 | Loss: 0.00003065
Iteration 31/1000 | Loss: 0.00003015
Iteration 32/1000 | Loss: 0.00002978
Iteration 33/1000 | Loss: 0.00077699
Iteration 34/1000 | Loss: 0.00004072
Iteration 35/1000 | Loss: 0.00003088
Iteration 36/1000 | Loss: 0.00002826
Iteration 37/1000 | Loss: 0.00002683
Iteration 38/1000 | Loss: 0.00002595
Iteration 39/1000 | Loss: 0.00002549
Iteration 40/1000 | Loss: 0.00002518
Iteration 41/1000 | Loss: 0.00002497
Iteration 42/1000 | Loss: 0.00002490
Iteration 43/1000 | Loss: 0.00046337
Iteration 44/1000 | Loss: 0.00005508
Iteration 45/1000 | Loss: 0.00076764
Iteration 46/1000 | Loss: 0.00105892
Iteration 47/1000 | Loss: 0.00055444
Iteration 48/1000 | Loss: 0.00035270
Iteration 49/1000 | Loss: 0.00004403
Iteration 50/1000 | Loss: 0.00003750
Iteration 51/1000 | Loss: 0.00027023
Iteration 52/1000 | Loss: 0.00029537
Iteration 53/1000 | Loss: 0.00031694
Iteration 54/1000 | Loss: 0.00003903
Iteration 55/1000 | Loss: 0.00002925
Iteration 56/1000 | Loss: 0.00002662
Iteration 57/1000 | Loss: 0.00002472
Iteration 58/1000 | Loss: 0.00002392
Iteration 59/1000 | Loss: 0.00002331
Iteration 60/1000 | Loss: 0.00002300
Iteration 61/1000 | Loss: 0.00002262
Iteration 62/1000 | Loss: 0.00002241
Iteration 63/1000 | Loss: 0.00002240
Iteration 64/1000 | Loss: 0.00002239
Iteration 65/1000 | Loss: 0.00002239
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002237
Iteration 68/1000 | Loss: 0.00002236
Iteration 69/1000 | Loss: 0.00002231
Iteration 70/1000 | Loss: 0.00002230
Iteration 71/1000 | Loss: 0.00002229
Iteration 72/1000 | Loss: 0.00002229
Iteration 73/1000 | Loss: 0.00002228
Iteration 74/1000 | Loss: 0.00002228
Iteration 75/1000 | Loss: 0.00002228
Iteration 76/1000 | Loss: 0.00002227
Iteration 77/1000 | Loss: 0.00002227
Iteration 78/1000 | Loss: 0.00002227
Iteration 79/1000 | Loss: 0.00002227
Iteration 80/1000 | Loss: 0.00002227
Iteration 81/1000 | Loss: 0.00002227
Iteration 82/1000 | Loss: 0.00002226
Iteration 83/1000 | Loss: 0.00002226
Iteration 84/1000 | Loss: 0.00002226
Iteration 85/1000 | Loss: 0.00002226
Iteration 86/1000 | Loss: 0.00002226
Iteration 87/1000 | Loss: 0.00002226
Iteration 88/1000 | Loss: 0.00002226
Iteration 89/1000 | Loss: 0.00002225
Iteration 90/1000 | Loss: 0.00002225
Iteration 91/1000 | Loss: 0.00002224
Iteration 92/1000 | Loss: 0.00002224
Iteration 93/1000 | Loss: 0.00002224
Iteration 94/1000 | Loss: 0.00002223
Iteration 95/1000 | Loss: 0.00002223
Iteration 96/1000 | Loss: 0.00002223
Iteration 97/1000 | Loss: 0.00002223
Iteration 98/1000 | Loss: 0.00002223
Iteration 99/1000 | Loss: 0.00002223
Iteration 100/1000 | Loss: 0.00002223
Iteration 101/1000 | Loss: 0.00002222
Iteration 102/1000 | Loss: 0.00002222
Iteration 103/1000 | Loss: 0.00002222
Iteration 104/1000 | Loss: 0.00002222
Iteration 105/1000 | Loss: 0.00002222
Iteration 106/1000 | Loss: 0.00002222
Iteration 107/1000 | Loss: 0.00002222
Iteration 108/1000 | Loss: 0.00002222
Iteration 109/1000 | Loss: 0.00002222
Iteration 110/1000 | Loss: 0.00002222
Iteration 111/1000 | Loss: 0.00002222
Iteration 112/1000 | Loss: 0.00002222
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002221
Iteration 115/1000 | Loss: 0.00002221
Iteration 116/1000 | Loss: 0.00002221
Iteration 117/1000 | Loss: 0.00002221
Iteration 118/1000 | Loss: 0.00002221
Iteration 119/1000 | Loss: 0.00002221
Iteration 120/1000 | Loss: 0.00002221
Iteration 121/1000 | Loss: 0.00002220
Iteration 122/1000 | Loss: 0.00002220
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002220
Iteration 125/1000 | Loss: 0.00002220
Iteration 126/1000 | Loss: 0.00002220
Iteration 127/1000 | Loss: 0.00002220
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002220
Iteration 131/1000 | Loss: 0.00002219
Iteration 132/1000 | Loss: 0.00002219
Iteration 133/1000 | Loss: 0.00002219
Iteration 134/1000 | Loss: 0.00002218
Iteration 135/1000 | Loss: 0.00002218
Iteration 136/1000 | Loss: 0.00002218
Iteration 137/1000 | Loss: 0.00002217
Iteration 138/1000 | Loss: 0.00002217
Iteration 139/1000 | Loss: 0.00002217
Iteration 140/1000 | Loss: 0.00002217
Iteration 141/1000 | Loss: 0.00002216
Iteration 142/1000 | Loss: 0.00002216
Iteration 143/1000 | Loss: 0.00002216
Iteration 144/1000 | Loss: 0.00002216
Iteration 145/1000 | Loss: 0.00002216
Iteration 146/1000 | Loss: 0.00002216
Iteration 147/1000 | Loss: 0.00002216
Iteration 148/1000 | Loss: 0.00002215
Iteration 149/1000 | Loss: 0.00002215
Iteration 150/1000 | Loss: 0.00002215
Iteration 151/1000 | Loss: 0.00002215
Iteration 152/1000 | Loss: 0.00002215
Iteration 153/1000 | Loss: 0.00002214
Iteration 154/1000 | Loss: 0.00002214
Iteration 155/1000 | Loss: 0.00002214
Iteration 156/1000 | Loss: 0.00002214
Iteration 157/1000 | Loss: 0.00002214
Iteration 158/1000 | Loss: 0.00002214
Iteration 159/1000 | Loss: 0.00002214
Iteration 160/1000 | Loss: 0.00002214
Iteration 161/1000 | Loss: 0.00002214
Iteration 162/1000 | Loss: 0.00002214
Iteration 163/1000 | Loss: 0.00002214
Iteration 164/1000 | Loss: 0.00002214
Iteration 165/1000 | Loss: 0.00002214
Iteration 166/1000 | Loss: 0.00002214
Iteration 167/1000 | Loss: 0.00002213
Iteration 168/1000 | Loss: 0.00002213
Iteration 169/1000 | Loss: 0.00002213
Iteration 170/1000 | Loss: 0.00002213
Iteration 171/1000 | Loss: 0.00002213
Iteration 172/1000 | Loss: 0.00002213
Iteration 173/1000 | Loss: 0.00002213
Iteration 174/1000 | Loss: 0.00002213
Iteration 175/1000 | Loss: 0.00002213
Iteration 176/1000 | Loss: 0.00002213
Iteration 177/1000 | Loss: 0.00002213
Iteration 178/1000 | Loss: 0.00002213
Iteration 179/1000 | Loss: 0.00002213
Iteration 180/1000 | Loss: 0.00002213
Iteration 181/1000 | Loss: 0.00002213
Iteration 182/1000 | Loss: 0.00002213
Iteration 183/1000 | Loss: 0.00002213
Iteration 184/1000 | Loss: 0.00002213
Iteration 185/1000 | Loss: 0.00002213
Iteration 186/1000 | Loss: 0.00002213
Iteration 187/1000 | Loss: 0.00002213
Iteration 188/1000 | Loss: 0.00002213
Iteration 189/1000 | Loss: 0.00002213
Iteration 190/1000 | Loss: 0.00002213
Iteration 191/1000 | Loss: 0.00002213
Iteration 192/1000 | Loss: 0.00002213
Iteration 193/1000 | Loss: 0.00002213
Iteration 194/1000 | Loss: 0.00002213
Iteration 195/1000 | Loss: 0.00002213
Iteration 196/1000 | Loss: 0.00002213
Iteration 197/1000 | Loss: 0.00002213
Iteration 198/1000 | Loss: 0.00002213
Iteration 199/1000 | Loss: 0.00002213
Iteration 200/1000 | Loss: 0.00002213
Iteration 201/1000 | Loss: 0.00002213
Iteration 202/1000 | Loss: 0.00002213
Iteration 203/1000 | Loss: 0.00002213
Iteration 204/1000 | Loss: 0.00002213
Iteration 205/1000 | Loss: 0.00002213
Iteration 206/1000 | Loss: 0.00002213
Iteration 207/1000 | Loss: 0.00002213
Iteration 208/1000 | Loss: 0.00002213
Iteration 209/1000 | Loss: 0.00002213
Iteration 210/1000 | Loss: 0.00002213
Iteration 211/1000 | Loss: 0.00002213
Iteration 212/1000 | Loss: 0.00002213
Iteration 213/1000 | Loss: 0.00002213
Iteration 214/1000 | Loss: 0.00002213
Iteration 215/1000 | Loss: 0.00002213
Iteration 216/1000 | Loss: 0.00002213
Iteration 217/1000 | Loss: 0.00002213
Iteration 218/1000 | Loss: 0.00002213
Iteration 219/1000 | Loss: 0.00002213
Iteration 220/1000 | Loss: 0.00002213
Iteration 221/1000 | Loss: 0.00002213
Iteration 222/1000 | Loss: 0.00002213
Iteration 223/1000 | Loss: 0.00002213
Iteration 224/1000 | Loss: 0.00002213
Iteration 225/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.2126421754364856e-05, 2.2126421754364856e-05, 2.2126421754364856e-05, 2.2126421754364856e-05, 2.2126421754364856e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2126421754364856e-05

Optimization complete. Final v2v error: 4.108636379241943 mm

Highest mean error: 5.601221084594727 mm for frame 103

Lowest mean error: 3.6783623695373535 mm for frame 35

Saving results

Total time: 141.0487093925476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792806
Iteration 2/25 | Loss: 0.00154698
Iteration 3/25 | Loss: 0.00136356
Iteration 4/25 | Loss: 0.00133840
Iteration 5/25 | Loss: 0.00133523
Iteration 6/25 | Loss: 0.00133509
Iteration 7/25 | Loss: 0.00133509
Iteration 8/25 | Loss: 0.00133509
Iteration 9/25 | Loss: 0.00133509
Iteration 10/25 | Loss: 0.00133509
Iteration 11/25 | Loss: 0.00133509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001335090957581997, 0.001335090957581997, 0.001335090957581997, 0.001335090957581997, 0.001335090957581997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001335090957581997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.11049557
Iteration 2/25 | Loss: 0.00117328
Iteration 3/25 | Loss: 0.00117325
Iteration 4/25 | Loss: 0.00117325
Iteration 5/25 | Loss: 0.00117325
Iteration 6/25 | Loss: 0.00117325
Iteration 7/25 | Loss: 0.00117325
Iteration 8/25 | Loss: 0.00117325
Iteration 9/25 | Loss: 0.00117325
Iteration 10/25 | Loss: 0.00117325
Iteration 11/25 | Loss: 0.00117325
Iteration 12/25 | Loss: 0.00117325
Iteration 13/25 | Loss: 0.00117325
Iteration 14/25 | Loss: 0.00117325
Iteration 15/25 | Loss: 0.00117325
Iteration 16/25 | Loss: 0.00117325
Iteration 17/25 | Loss: 0.00117325
Iteration 18/25 | Loss: 0.00117325
Iteration 19/25 | Loss: 0.00117325
Iteration 20/25 | Loss: 0.00117325
Iteration 21/25 | Loss: 0.00117325
Iteration 22/25 | Loss: 0.00117325
Iteration 23/25 | Loss: 0.00117325
Iteration 24/25 | Loss: 0.00117325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001173250493593514, 0.001173250493593514, 0.001173250493593514, 0.001173250493593514, 0.001173250493593514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001173250493593514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117325
Iteration 2/1000 | Loss: 0.00004906
Iteration 3/1000 | Loss: 0.00003507
Iteration 4/1000 | Loss: 0.00003174
Iteration 5/1000 | Loss: 0.00003003
Iteration 6/1000 | Loss: 0.00002869
Iteration 7/1000 | Loss: 0.00002777
Iteration 8/1000 | Loss: 0.00002728
Iteration 9/1000 | Loss: 0.00002698
Iteration 10/1000 | Loss: 0.00002670
Iteration 11/1000 | Loss: 0.00002658
Iteration 12/1000 | Loss: 0.00002654
Iteration 13/1000 | Loss: 0.00002637
Iteration 14/1000 | Loss: 0.00002632
Iteration 15/1000 | Loss: 0.00002631
Iteration 16/1000 | Loss: 0.00002629
Iteration 17/1000 | Loss: 0.00002629
Iteration 18/1000 | Loss: 0.00002628
Iteration 19/1000 | Loss: 0.00002628
Iteration 20/1000 | Loss: 0.00002628
Iteration 21/1000 | Loss: 0.00002628
Iteration 22/1000 | Loss: 0.00002628
Iteration 23/1000 | Loss: 0.00002627
Iteration 24/1000 | Loss: 0.00002627
Iteration 25/1000 | Loss: 0.00002627
Iteration 26/1000 | Loss: 0.00002627
Iteration 27/1000 | Loss: 0.00002626
Iteration 28/1000 | Loss: 0.00002626
Iteration 29/1000 | Loss: 0.00002625
Iteration 30/1000 | Loss: 0.00002625
Iteration 31/1000 | Loss: 0.00002624
Iteration 32/1000 | Loss: 0.00002624
Iteration 33/1000 | Loss: 0.00002624
Iteration 34/1000 | Loss: 0.00002624
Iteration 35/1000 | Loss: 0.00002624
Iteration 36/1000 | Loss: 0.00002624
Iteration 37/1000 | Loss: 0.00002624
Iteration 38/1000 | Loss: 0.00002624
Iteration 39/1000 | Loss: 0.00002624
Iteration 40/1000 | Loss: 0.00002624
Iteration 41/1000 | Loss: 0.00002624
Iteration 42/1000 | Loss: 0.00002624
Iteration 43/1000 | Loss: 0.00002624
Iteration 44/1000 | Loss: 0.00002624
Iteration 45/1000 | Loss: 0.00002624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [2.6241121304337867e-05, 2.6241121304337867e-05, 2.6241121304337867e-05, 2.6241121304337867e-05, 2.6241121304337867e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6241121304337867e-05

Optimization complete. Final v2v error: 4.502341270446777 mm

Highest mean error: 5.011763572692871 mm for frame 142

Lowest mean error: 4.081422805786133 mm for frame 197

Saving results

Total time: 30.555694103240967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973969
Iteration 2/25 | Loss: 0.00176201
Iteration 3/25 | Loss: 0.00148316
Iteration 4/25 | Loss: 0.00144437
Iteration 5/25 | Loss: 0.00140559
Iteration 6/25 | Loss: 0.00140398
Iteration 7/25 | Loss: 0.00140162
Iteration 8/25 | Loss: 0.00140287
Iteration 9/25 | Loss: 0.00140054
Iteration 10/25 | Loss: 0.00140155
Iteration 11/25 | Loss: 0.00140182
Iteration 12/25 | Loss: 0.00140187
Iteration 13/25 | Loss: 0.00140217
Iteration 14/25 | Loss: 0.00140187
Iteration 15/25 | Loss: 0.00140269
Iteration 16/25 | Loss: 0.00140199
Iteration 17/25 | Loss: 0.00140220
Iteration 18/25 | Loss: 0.00140001
Iteration 19/25 | Loss: 0.00140134
Iteration 20/25 | Loss: 0.00140298
Iteration 21/25 | Loss: 0.00139935
Iteration 22/25 | Loss: 0.00139871
Iteration 23/25 | Loss: 0.00139754
Iteration 24/25 | Loss: 0.00139700
Iteration 25/25 | Loss: 0.00139696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37496722
Iteration 2/25 | Loss: 0.00124632
Iteration 3/25 | Loss: 0.00124632
Iteration 4/25 | Loss: 0.00124632
Iteration 5/25 | Loss: 0.00124632
Iteration 6/25 | Loss: 0.00124632
Iteration 7/25 | Loss: 0.00124632
Iteration 8/25 | Loss: 0.00124632
Iteration 9/25 | Loss: 0.00124632
Iteration 10/25 | Loss: 0.00124631
Iteration 11/25 | Loss: 0.00124631
Iteration 12/25 | Loss: 0.00124632
Iteration 13/25 | Loss: 0.00124632
Iteration 14/25 | Loss: 0.00124632
Iteration 15/25 | Loss: 0.00124632
Iteration 16/25 | Loss: 0.00124632
Iteration 17/25 | Loss: 0.00124632
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012463150778785348, 0.0012463150778785348, 0.0012463150778785348, 0.0012463150778785348, 0.0012463150778785348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012463150778785348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124632
Iteration 2/1000 | Loss: 0.00005199
Iteration 3/1000 | Loss: 0.00004228
Iteration 4/1000 | Loss: 0.00003911
Iteration 5/1000 | Loss: 0.00003727
Iteration 6/1000 | Loss: 0.00003595
Iteration 7/1000 | Loss: 0.00003506
Iteration 8/1000 | Loss: 0.00003436
Iteration 9/1000 | Loss: 0.00003372
Iteration 10/1000 | Loss: 0.00003332
Iteration 11/1000 | Loss: 0.00003302
Iteration 12/1000 | Loss: 0.00003278
Iteration 13/1000 | Loss: 0.00003263
Iteration 14/1000 | Loss: 0.00003244
Iteration 15/1000 | Loss: 0.00003243
Iteration 16/1000 | Loss: 0.00003242
Iteration 17/1000 | Loss: 0.00003238
Iteration 18/1000 | Loss: 0.00003233
Iteration 19/1000 | Loss: 0.00003231
Iteration 20/1000 | Loss: 0.00003230
Iteration 21/1000 | Loss: 0.00003229
Iteration 22/1000 | Loss: 0.00003229
Iteration 23/1000 | Loss: 0.00003229
Iteration 24/1000 | Loss: 0.00003229
Iteration 25/1000 | Loss: 0.00003228
Iteration 26/1000 | Loss: 0.00003228
Iteration 27/1000 | Loss: 0.00003228
Iteration 28/1000 | Loss: 0.00003228
Iteration 29/1000 | Loss: 0.00003228
Iteration 30/1000 | Loss: 0.00003228
Iteration 31/1000 | Loss: 0.00003228
Iteration 32/1000 | Loss: 0.00003228
Iteration 33/1000 | Loss: 0.00003228
Iteration 34/1000 | Loss: 0.00003228
Iteration 35/1000 | Loss: 0.00003228
Iteration 36/1000 | Loss: 0.00003227
Iteration 37/1000 | Loss: 0.00003226
Iteration 38/1000 | Loss: 0.00003226
Iteration 39/1000 | Loss: 0.00003226
Iteration 40/1000 | Loss: 0.00003226
Iteration 41/1000 | Loss: 0.00003225
Iteration 42/1000 | Loss: 0.00003225
Iteration 43/1000 | Loss: 0.00003225
Iteration 44/1000 | Loss: 0.00003225
Iteration 45/1000 | Loss: 0.00003225
Iteration 46/1000 | Loss: 0.00003225
Iteration 47/1000 | Loss: 0.00003225
Iteration 48/1000 | Loss: 0.00003225
Iteration 49/1000 | Loss: 0.00003225
Iteration 50/1000 | Loss: 0.00003225
Iteration 51/1000 | Loss: 0.00003224
Iteration 52/1000 | Loss: 0.00003224
Iteration 53/1000 | Loss: 0.00003224
Iteration 54/1000 | Loss: 0.00003223
Iteration 55/1000 | Loss: 0.00003223
Iteration 56/1000 | Loss: 0.00003222
Iteration 57/1000 | Loss: 0.00003222
Iteration 58/1000 | Loss: 0.00003221
Iteration 59/1000 | Loss: 0.00003221
Iteration 60/1000 | Loss: 0.00003221
Iteration 61/1000 | Loss: 0.00003220
Iteration 62/1000 | Loss: 0.00003220
Iteration 63/1000 | Loss: 0.00003220
Iteration 64/1000 | Loss: 0.00003219
Iteration 65/1000 | Loss: 0.00003219
Iteration 66/1000 | Loss: 0.00003219
Iteration 67/1000 | Loss: 0.00003218
Iteration 68/1000 | Loss: 0.00003218
Iteration 69/1000 | Loss: 0.00003218
Iteration 70/1000 | Loss: 0.00003218
Iteration 71/1000 | Loss: 0.00003217
Iteration 72/1000 | Loss: 0.00003217
Iteration 73/1000 | Loss: 0.00003216
Iteration 74/1000 | Loss: 0.00003216
Iteration 75/1000 | Loss: 0.00003216
Iteration 76/1000 | Loss: 0.00003216
Iteration 77/1000 | Loss: 0.00003216
Iteration 78/1000 | Loss: 0.00003216
Iteration 79/1000 | Loss: 0.00003215
Iteration 80/1000 | Loss: 0.00003215
Iteration 81/1000 | Loss: 0.00003215
Iteration 82/1000 | Loss: 0.00003215
Iteration 83/1000 | Loss: 0.00003214
Iteration 84/1000 | Loss: 0.00003214
Iteration 85/1000 | Loss: 0.00003214
Iteration 86/1000 | Loss: 0.00003213
Iteration 87/1000 | Loss: 0.00003213
Iteration 88/1000 | Loss: 0.00003213
Iteration 89/1000 | Loss: 0.00003213
Iteration 90/1000 | Loss: 0.00003213
Iteration 91/1000 | Loss: 0.00003212
Iteration 92/1000 | Loss: 0.00003212
Iteration 93/1000 | Loss: 0.00003212
Iteration 94/1000 | Loss: 0.00003212
Iteration 95/1000 | Loss: 0.00003211
Iteration 96/1000 | Loss: 0.00003211
Iteration 97/1000 | Loss: 0.00003211
Iteration 98/1000 | Loss: 0.00003211
Iteration 99/1000 | Loss: 0.00003211
Iteration 100/1000 | Loss: 0.00003211
Iteration 101/1000 | Loss: 0.00003211
Iteration 102/1000 | Loss: 0.00003210
Iteration 103/1000 | Loss: 0.00003210
Iteration 104/1000 | Loss: 0.00003210
Iteration 105/1000 | Loss: 0.00003210
Iteration 106/1000 | Loss: 0.00003210
Iteration 107/1000 | Loss: 0.00003210
Iteration 108/1000 | Loss: 0.00003209
Iteration 109/1000 | Loss: 0.00003209
Iteration 110/1000 | Loss: 0.00003209
Iteration 111/1000 | Loss: 0.00003208
Iteration 112/1000 | Loss: 0.00003208
Iteration 113/1000 | Loss: 0.00003208
Iteration 114/1000 | Loss: 0.00003208
Iteration 115/1000 | Loss: 0.00003208
Iteration 116/1000 | Loss: 0.00003208
Iteration 117/1000 | Loss: 0.00003208
Iteration 118/1000 | Loss: 0.00003208
Iteration 119/1000 | Loss: 0.00003208
Iteration 120/1000 | Loss: 0.00003208
Iteration 121/1000 | Loss: 0.00003208
Iteration 122/1000 | Loss: 0.00003208
Iteration 123/1000 | Loss: 0.00003208
Iteration 124/1000 | Loss: 0.00003208
Iteration 125/1000 | Loss: 0.00003208
Iteration 126/1000 | Loss: 0.00003208
Iteration 127/1000 | Loss: 0.00003208
Iteration 128/1000 | Loss: 0.00003208
Iteration 129/1000 | Loss: 0.00003208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [3.207523332093842e-05, 3.207523332093842e-05, 3.207523332093842e-05, 3.207523332093842e-05, 3.207523332093842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.207523332093842e-05

Optimization complete. Final v2v error: 4.930081367492676 mm

Highest mean error: 5.93969202041626 mm for frame 223

Lowest mean error: 4.395040512084961 mm for frame 103

Saving results

Total time: 81.21988129615784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452217
Iteration 2/25 | Loss: 0.00129631
Iteration 3/25 | Loss: 0.00117952
Iteration 4/25 | Loss: 0.00116437
Iteration 5/25 | Loss: 0.00116075
Iteration 6/25 | Loss: 0.00116024
Iteration 7/25 | Loss: 0.00116024
Iteration 8/25 | Loss: 0.00116024
Iteration 9/25 | Loss: 0.00116024
Iteration 10/25 | Loss: 0.00116024
Iteration 11/25 | Loss: 0.00116024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011602442245930433, 0.0011602442245930433, 0.0011602442245930433, 0.0011602442245930433, 0.0011602442245930433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011602442245930433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39563179
Iteration 2/25 | Loss: 0.00045094
Iteration 3/25 | Loss: 0.00045094
Iteration 4/25 | Loss: 0.00045094
Iteration 5/25 | Loss: 0.00045094
Iteration 6/25 | Loss: 0.00045094
Iteration 7/25 | Loss: 0.00045094
Iteration 8/25 | Loss: 0.00045094
Iteration 9/25 | Loss: 0.00045094
Iteration 10/25 | Loss: 0.00045094
Iteration 11/25 | Loss: 0.00045094
Iteration 12/25 | Loss: 0.00045094
Iteration 13/25 | Loss: 0.00045094
Iteration 14/25 | Loss: 0.00045094
Iteration 15/25 | Loss: 0.00045094
Iteration 16/25 | Loss: 0.00045094
Iteration 17/25 | Loss: 0.00045094
Iteration 18/25 | Loss: 0.00045094
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004509353602770716, 0.0004509353602770716, 0.0004509353602770716, 0.0004509353602770716, 0.0004509353602770716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004509353602770716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045094
Iteration 2/1000 | Loss: 0.00003649
Iteration 3/1000 | Loss: 0.00002542
Iteration 4/1000 | Loss: 0.00002336
Iteration 5/1000 | Loss: 0.00002218
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002082
Iteration 9/1000 | Loss: 0.00002064
Iteration 10/1000 | Loss: 0.00002060
Iteration 11/1000 | Loss: 0.00002052
Iteration 12/1000 | Loss: 0.00002052
Iteration 13/1000 | Loss: 0.00002051
Iteration 14/1000 | Loss: 0.00002050
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00002049
Iteration 17/1000 | Loss: 0.00002047
Iteration 18/1000 | Loss: 0.00002047
Iteration 19/1000 | Loss: 0.00002046
Iteration 20/1000 | Loss: 0.00002044
Iteration 21/1000 | Loss: 0.00002042
Iteration 22/1000 | Loss: 0.00002042
Iteration 23/1000 | Loss: 0.00002041
Iteration 24/1000 | Loss: 0.00002040
Iteration 25/1000 | Loss: 0.00002040
Iteration 26/1000 | Loss: 0.00002040
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00002039
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00002039
Iteration 33/1000 | Loss: 0.00002038
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002037
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002035
Iteration 40/1000 | Loss: 0.00002035
Iteration 41/1000 | Loss: 0.00002035
Iteration 42/1000 | Loss: 0.00002035
Iteration 43/1000 | Loss: 0.00002035
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002034
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00002034
Iteration 48/1000 | Loss: 0.00002033
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002032
Iteration 51/1000 | Loss: 0.00002032
Iteration 52/1000 | Loss: 0.00002032
Iteration 53/1000 | Loss: 0.00002032
Iteration 54/1000 | Loss: 0.00002032
Iteration 55/1000 | Loss: 0.00002032
Iteration 56/1000 | Loss: 0.00002031
Iteration 57/1000 | Loss: 0.00002031
Iteration 58/1000 | Loss: 0.00002031
Iteration 59/1000 | Loss: 0.00002031
Iteration 60/1000 | Loss: 0.00002031
Iteration 61/1000 | Loss: 0.00002031
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002030
Iteration 64/1000 | Loss: 0.00002030
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002030
Iteration 67/1000 | Loss: 0.00002030
Iteration 68/1000 | Loss: 0.00002030
Iteration 69/1000 | Loss: 0.00002030
Iteration 70/1000 | Loss: 0.00002030
Iteration 71/1000 | Loss: 0.00002030
Iteration 72/1000 | Loss: 0.00002029
Iteration 73/1000 | Loss: 0.00002029
Iteration 74/1000 | Loss: 0.00002029
Iteration 75/1000 | Loss: 0.00002029
Iteration 76/1000 | Loss: 0.00002029
Iteration 77/1000 | Loss: 0.00002029
Iteration 78/1000 | Loss: 0.00002029
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002028
Iteration 81/1000 | Loss: 0.00002028
Iteration 82/1000 | Loss: 0.00002028
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002028
Iteration 85/1000 | Loss: 0.00002028
Iteration 86/1000 | Loss: 0.00002028
Iteration 87/1000 | Loss: 0.00002027
Iteration 88/1000 | Loss: 0.00002027
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002027
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00002027
Iteration 95/1000 | Loss: 0.00002026
Iteration 96/1000 | Loss: 0.00002026
Iteration 97/1000 | Loss: 0.00002026
Iteration 98/1000 | Loss: 0.00002026
Iteration 99/1000 | Loss: 0.00002026
Iteration 100/1000 | Loss: 0.00002026
Iteration 101/1000 | Loss: 0.00002026
Iteration 102/1000 | Loss: 0.00002025
Iteration 103/1000 | Loss: 0.00002025
Iteration 104/1000 | Loss: 0.00002024
Iteration 105/1000 | Loss: 0.00002024
Iteration 106/1000 | Loss: 0.00002024
Iteration 107/1000 | Loss: 0.00002024
Iteration 108/1000 | Loss: 0.00002024
Iteration 109/1000 | Loss: 0.00002024
Iteration 110/1000 | Loss: 0.00002023
Iteration 111/1000 | Loss: 0.00002023
Iteration 112/1000 | Loss: 0.00002023
Iteration 113/1000 | Loss: 0.00002023
Iteration 114/1000 | Loss: 0.00002022
Iteration 115/1000 | Loss: 0.00002022
Iteration 116/1000 | Loss: 0.00002022
Iteration 117/1000 | Loss: 0.00002022
Iteration 118/1000 | Loss: 0.00002022
Iteration 119/1000 | Loss: 0.00002021
Iteration 120/1000 | Loss: 0.00002021
Iteration 121/1000 | Loss: 0.00002021
Iteration 122/1000 | Loss: 0.00002021
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002020
Iteration 125/1000 | Loss: 0.00002020
Iteration 126/1000 | Loss: 0.00002020
Iteration 127/1000 | Loss: 0.00002019
Iteration 128/1000 | Loss: 0.00002019
Iteration 129/1000 | Loss: 0.00002019
Iteration 130/1000 | Loss: 0.00002019
Iteration 131/1000 | Loss: 0.00002019
Iteration 132/1000 | Loss: 0.00002018
Iteration 133/1000 | Loss: 0.00002018
Iteration 134/1000 | Loss: 0.00002018
Iteration 135/1000 | Loss: 0.00002018
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002017
Iteration 138/1000 | Loss: 0.00002017
Iteration 139/1000 | Loss: 0.00002017
Iteration 140/1000 | Loss: 0.00002017
Iteration 141/1000 | Loss: 0.00002016
Iteration 142/1000 | Loss: 0.00002016
Iteration 143/1000 | Loss: 0.00002016
Iteration 144/1000 | Loss: 0.00002016
Iteration 145/1000 | Loss: 0.00002015
Iteration 146/1000 | Loss: 0.00002015
Iteration 147/1000 | Loss: 0.00002015
Iteration 148/1000 | Loss: 0.00002015
Iteration 149/1000 | Loss: 0.00002015
Iteration 150/1000 | Loss: 0.00002015
Iteration 151/1000 | Loss: 0.00002015
Iteration 152/1000 | Loss: 0.00002015
Iteration 153/1000 | Loss: 0.00002015
Iteration 154/1000 | Loss: 0.00002014
Iteration 155/1000 | Loss: 0.00002014
Iteration 156/1000 | Loss: 0.00002014
Iteration 157/1000 | Loss: 0.00002014
Iteration 158/1000 | Loss: 0.00002014
Iteration 159/1000 | Loss: 0.00002014
Iteration 160/1000 | Loss: 0.00002013
Iteration 161/1000 | Loss: 0.00002013
Iteration 162/1000 | Loss: 0.00002013
Iteration 163/1000 | Loss: 0.00002013
Iteration 164/1000 | Loss: 0.00002013
Iteration 165/1000 | Loss: 0.00002013
Iteration 166/1000 | Loss: 0.00002013
Iteration 167/1000 | Loss: 0.00002013
Iteration 168/1000 | Loss: 0.00002013
Iteration 169/1000 | Loss: 0.00002012
Iteration 170/1000 | Loss: 0.00002012
Iteration 171/1000 | Loss: 0.00002012
Iteration 172/1000 | Loss: 0.00002012
Iteration 173/1000 | Loss: 0.00002012
Iteration 174/1000 | Loss: 0.00002012
Iteration 175/1000 | Loss: 0.00002012
Iteration 176/1000 | Loss: 0.00002012
Iteration 177/1000 | Loss: 0.00002012
Iteration 178/1000 | Loss: 0.00002012
Iteration 179/1000 | Loss: 0.00002012
Iteration 180/1000 | Loss: 0.00002012
Iteration 181/1000 | Loss: 0.00002011
Iteration 182/1000 | Loss: 0.00002011
Iteration 183/1000 | Loss: 0.00002011
Iteration 184/1000 | Loss: 0.00002011
Iteration 185/1000 | Loss: 0.00002011
Iteration 186/1000 | Loss: 0.00002011
Iteration 187/1000 | Loss: 0.00002011
Iteration 188/1000 | Loss: 0.00002011
Iteration 189/1000 | Loss: 0.00002011
Iteration 190/1000 | Loss: 0.00002010
Iteration 191/1000 | Loss: 0.00002010
Iteration 192/1000 | Loss: 0.00002010
Iteration 193/1000 | Loss: 0.00002010
Iteration 194/1000 | Loss: 0.00002010
Iteration 195/1000 | Loss: 0.00002009
Iteration 196/1000 | Loss: 0.00002009
Iteration 197/1000 | Loss: 0.00002009
Iteration 198/1000 | Loss: 0.00002008
Iteration 199/1000 | Loss: 0.00002008
Iteration 200/1000 | Loss: 0.00002008
Iteration 201/1000 | Loss: 0.00002008
Iteration 202/1000 | Loss: 0.00002008
Iteration 203/1000 | Loss: 0.00002008
Iteration 204/1000 | Loss: 0.00002008
Iteration 205/1000 | Loss: 0.00002008
Iteration 206/1000 | Loss: 0.00002008
Iteration 207/1000 | Loss: 0.00002008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [2.007926013902761e-05, 2.007926013902761e-05, 2.007926013902761e-05, 2.007926013902761e-05, 2.007926013902761e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.007926013902761e-05

Optimization complete. Final v2v error: 3.740889310836792 mm

Highest mean error: 4.448347091674805 mm for frame 0

Lowest mean error: 3.0342040061950684 mm for frame 239

Saving results

Total time: 42.08958339691162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892089
Iteration 2/25 | Loss: 0.00153300
Iteration 3/25 | Loss: 0.00115977
Iteration 4/25 | Loss: 0.00112501
Iteration 5/25 | Loss: 0.00111996
Iteration 6/25 | Loss: 0.00111885
Iteration 7/25 | Loss: 0.00111873
Iteration 8/25 | Loss: 0.00111873
Iteration 9/25 | Loss: 0.00111873
Iteration 10/25 | Loss: 0.00111873
Iteration 11/25 | Loss: 0.00111873
Iteration 12/25 | Loss: 0.00111873
Iteration 13/25 | Loss: 0.00111873
Iteration 14/25 | Loss: 0.00111873
Iteration 15/25 | Loss: 0.00111873
Iteration 16/25 | Loss: 0.00111873
Iteration 17/25 | Loss: 0.00111873
Iteration 18/25 | Loss: 0.00111873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011187338968738914, 0.0011187338968738914, 0.0011187338968738914, 0.0011187338968738914, 0.0011187338968738914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011187338968738914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42358935
Iteration 2/25 | Loss: 0.00038433
Iteration 3/25 | Loss: 0.00038433
Iteration 4/25 | Loss: 0.00038433
Iteration 5/25 | Loss: 0.00038433
Iteration 6/25 | Loss: 0.00038433
Iteration 7/25 | Loss: 0.00038433
Iteration 8/25 | Loss: 0.00038433
Iteration 9/25 | Loss: 0.00038433
Iteration 10/25 | Loss: 0.00038433
Iteration 11/25 | Loss: 0.00038433
Iteration 12/25 | Loss: 0.00038433
Iteration 13/25 | Loss: 0.00038433
Iteration 14/25 | Loss: 0.00038433
Iteration 15/25 | Loss: 0.00038433
Iteration 16/25 | Loss: 0.00038433
Iteration 17/25 | Loss: 0.00038433
Iteration 18/25 | Loss: 0.00038433
Iteration 19/25 | Loss: 0.00038433
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00038432650035247207, 0.00038432650035247207, 0.00038432650035247207, 0.00038432650035247207, 0.00038432650035247207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038432650035247207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038433
Iteration 2/1000 | Loss: 0.00003387
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001403
Iteration 8/1000 | Loss: 0.00001383
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001374
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001370
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001367
Iteration 21/1000 | Loss: 0.00001367
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001366
Iteration 25/1000 | Loss: 0.00001365
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001363
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001356
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001353
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001351
Iteration 99/1000 | Loss: 0.00001351
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001351
Iteration 102/1000 | Loss: 0.00001350
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001349
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001349
Iteration 110/1000 | Loss: 0.00001349
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001348
Iteration 121/1000 | Loss: 0.00001348
Iteration 122/1000 | Loss: 0.00001348
Iteration 123/1000 | Loss: 0.00001348
Iteration 124/1000 | Loss: 0.00001348
Iteration 125/1000 | Loss: 0.00001348
Iteration 126/1000 | Loss: 0.00001348
Iteration 127/1000 | Loss: 0.00001348
Iteration 128/1000 | Loss: 0.00001348
Iteration 129/1000 | Loss: 0.00001348
Iteration 130/1000 | Loss: 0.00001348
Iteration 131/1000 | Loss: 0.00001348
Iteration 132/1000 | Loss: 0.00001348
Iteration 133/1000 | Loss: 0.00001348
Iteration 134/1000 | Loss: 0.00001348
Iteration 135/1000 | Loss: 0.00001348
Iteration 136/1000 | Loss: 0.00001348
Iteration 137/1000 | Loss: 0.00001348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3481297173711937e-05, 1.3481297173711937e-05, 1.3481297173711937e-05, 1.3481297173711937e-05, 1.3481297173711937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3481297173711937e-05

Optimization complete. Final v2v error: 3.099688768386841 mm

Highest mean error: 3.30364727973938 mm for frame 3

Lowest mean error: 2.940155267715454 mm for frame 39

Saving results

Total time: 30.14275860786438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406356
Iteration 2/25 | Loss: 0.00133481
Iteration 3/25 | Loss: 0.00121228
Iteration 4/25 | Loss: 0.00119227
Iteration 5/25 | Loss: 0.00118441
Iteration 6/25 | Loss: 0.00118199
Iteration 7/25 | Loss: 0.00118125
Iteration 8/25 | Loss: 0.00118109
Iteration 9/25 | Loss: 0.00118109
Iteration 10/25 | Loss: 0.00118109
Iteration 11/25 | Loss: 0.00118109
Iteration 12/25 | Loss: 0.00118109
Iteration 13/25 | Loss: 0.00118109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011810899013653398, 0.0011810899013653398, 0.0011810899013653398, 0.0011810899013653398, 0.0011810899013653398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011810899013653398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55803967
Iteration 2/25 | Loss: 0.00041699
Iteration 3/25 | Loss: 0.00041699
Iteration 4/25 | Loss: 0.00041699
Iteration 5/25 | Loss: 0.00041699
Iteration 6/25 | Loss: 0.00041699
Iteration 7/25 | Loss: 0.00041699
Iteration 8/25 | Loss: 0.00041699
Iteration 9/25 | Loss: 0.00041699
Iteration 10/25 | Loss: 0.00041699
Iteration 11/25 | Loss: 0.00041699
Iteration 12/25 | Loss: 0.00041699
Iteration 13/25 | Loss: 0.00041699
Iteration 14/25 | Loss: 0.00041699
Iteration 15/25 | Loss: 0.00041699
Iteration 16/25 | Loss: 0.00041699
Iteration 17/25 | Loss: 0.00041699
Iteration 18/25 | Loss: 0.00041699
Iteration 19/25 | Loss: 0.00041699
Iteration 20/25 | Loss: 0.00041699
Iteration 21/25 | Loss: 0.00041699
Iteration 22/25 | Loss: 0.00041699
Iteration 23/25 | Loss: 0.00041699
Iteration 24/25 | Loss: 0.00041699
Iteration 25/25 | Loss: 0.00041699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041699
Iteration 2/1000 | Loss: 0.00005005
Iteration 3/1000 | Loss: 0.00003355
Iteration 4/1000 | Loss: 0.00002789
Iteration 5/1000 | Loss: 0.00002603
Iteration 6/1000 | Loss: 0.00002504
Iteration 7/1000 | Loss: 0.00002423
Iteration 8/1000 | Loss: 0.00002364
Iteration 9/1000 | Loss: 0.00002333
Iteration 10/1000 | Loss: 0.00002300
Iteration 11/1000 | Loss: 0.00002286
Iteration 12/1000 | Loss: 0.00002267
Iteration 13/1000 | Loss: 0.00002263
Iteration 14/1000 | Loss: 0.00002249
Iteration 15/1000 | Loss: 0.00002249
Iteration 16/1000 | Loss: 0.00002242
Iteration 17/1000 | Loss: 0.00002240
Iteration 18/1000 | Loss: 0.00002239
Iteration 19/1000 | Loss: 0.00002238
Iteration 20/1000 | Loss: 0.00002238
Iteration 21/1000 | Loss: 0.00002236
Iteration 22/1000 | Loss: 0.00002234
Iteration 23/1000 | Loss: 0.00002231
Iteration 24/1000 | Loss: 0.00002231
Iteration 25/1000 | Loss: 0.00002229
Iteration 26/1000 | Loss: 0.00002229
Iteration 27/1000 | Loss: 0.00002228
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00002228
Iteration 30/1000 | Loss: 0.00002227
Iteration 31/1000 | Loss: 0.00002227
Iteration 32/1000 | Loss: 0.00002227
Iteration 33/1000 | Loss: 0.00002226
Iteration 34/1000 | Loss: 0.00002226
Iteration 35/1000 | Loss: 0.00002226
Iteration 36/1000 | Loss: 0.00002226
Iteration 37/1000 | Loss: 0.00002225
Iteration 38/1000 | Loss: 0.00002225
Iteration 39/1000 | Loss: 0.00002225
Iteration 40/1000 | Loss: 0.00002224
Iteration 41/1000 | Loss: 0.00002224
Iteration 42/1000 | Loss: 0.00002223
Iteration 43/1000 | Loss: 0.00002223
Iteration 44/1000 | Loss: 0.00002223
Iteration 45/1000 | Loss: 0.00002223
Iteration 46/1000 | Loss: 0.00002223
Iteration 47/1000 | Loss: 0.00002222
Iteration 48/1000 | Loss: 0.00002222
Iteration 49/1000 | Loss: 0.00002222
Iteration 50/1000 | Loss: 0.00002221
Iteration 51/1000 | Loss: 0.00002220
Iteration 52/1000 | Loss: 0.00002220
Iteration 53/1000 | Loss: 0.00002220
Iteration 54/1000 | Loss: 0.00002220
Iteration 55/1000 | Loss: 0.00002220
Iteration 56/1000 | Loss: 0.00002220
Iteration 57/1000 | Loss: 0.00002220
Iteration 58/1000 | Loss: 0.00002219
Iteration 59/1000 | Loss: 0.00002219
Iteration 60/1000 | Loss: 0.00002219
Iteration 61/1000 | Loss: 0.00002219
Iteration 62/1000 | Loss: 0.00002219
Iteration 63/1000 | Loss: 0.00002218
Iteration 64/1000 | Loss: 0.00002218
Iteration 65/1000 | Loss: 0.00002218
Iteration 66/1000 | Loss: 0.00002217
Iteration 67/1000 | Loss: 0.00002217
Iteration 68/1000 | Loss: 0.00002217
Iteration 69/1000 | Loss: 0.00002216
Iteration 70/1000 | Loss: 0.00002215
Iteration 71/1000 | Loss: 0.00002215
Iteration 72/1000 | Loss: 0.00002215
Iteration 73/1000 | Loss: 0.00002214
Iteration 74/1000 | Loss: 0.00002214
Iteration 75/1000 | Loss: 0.00002213
Iteration 76/1000 | Loss: 0.00002213
Iteration 77/1000 | Loss: 0.00002213
Iteration 78/1000 | Loss: 0.00002213
Iteration 79/1000 | Loss: 0.00002213
Iteration 80/1000 | Loss: 0.00002212
Iteration 81/1000 | Loss: 0.00002212
Iteration 82/1000 | Loss: 0.00002212
Iteration 83/1000 | Loss: 0.00002212
Iteration 84/1000 | Loss: 0.00002212
Iteration 85/1000 | Loss: 0.00002211
Iteration 86/1000 | Loss: 0.00002211
Iteration 87/1000 | Loss: 0.00002210
Iteration 88/1000 | Loss: 0.00002210
Iteration 89/1000 | Loss: 0.00002210
Iteration 90/1000 | Loss: 0.00002209
Iteration 91/1000 | Loss: 0.00002209
Iteration 92/1000 | Loss: 0.00002209
Iteration 93/1000 | Loss: 0.00002209
Iteration 94/1000 | Loss: 0.00002209
Iteration 95/1000 | Loss: 0.00002209
Iteration 96/1000 | Loss: 0.00002209
Iteration 97/1000 | Loss: 0.00002209
Iteration 98/1000 | Loss: 0.00002208
Iteration 99/1000 | Loss: 0.00002208
Iteration 100/1000 | Loss: 0.00002208
Iteration 101/1000 | Loss: 0.00002207
Iteration 102/1000 | Loss: 0.00002207
Iteration 103/1000 | Loss: 0.00002207
Iteration 104/1000 | Loss: 0.00002207
Iteration 105/1000 | Loss: 0.00002207
Iteration 106/1000 | Loss: 0.00002206
Iteration 107/1000 | Loss: 0.00002206
Iteration 108/1000 | Loss: 0.00002206
Iteration 109/1000 | Loss: 0.00002206
Iteration 110/1000 | Loss: 0.00002206
Iteration 111/1000 | Loss: 0.00002206
Iteration 112/1000 | Loss: 0.00002206
Iteration 113/1000 | Loss: 0.00002205
Iteration 114/1000 | Loss: 0.00002205
Iteration 115/1000 | Loss: 0.00002205
Iteration 116/1000 | Loss: 0.00002205
Iteration 117/1000 | Loss: 0.00002205
Iteration 118/1000 | Loss: 0.00002205
Iteration 119/1000 | Loss: 0.00002204
Iteration 120/1000 | Loss: 0.00002204
Iteration 121/1000 | Loss: 0.00002204
Iteration 122/1000 | Loss: 0.00002204
Iteration 123/1000 | Loss: 0.00002204
Iteration 124/1000 | Loss: 0.00002203
Iteration 125/1000 | Loss: 0.00002203
Iteration 126/1000 | Loss: 0.00002203
Iteration 127/1000 | Loss: 0.00002203
Iteration 128/1000 | Loss: 0.00002203
Iteration 129/1000 | Loss: 0.00002203
Iteration 130/1000 | Loss: 0.00002203
Iteration 131/1000 | Loss: 0.00002203
Iteration 132/1000 | Loss: 0.00002203
Iteration 133/1000 | Loss: 0.00002203
Iteration 134/1000 | Loss: 0.00002202
Iteration 135/1000 | Loss: 0.00002202
Iteration 136/1000 | Loss: 0.00002202
Iteration 137/1000 | Loss: 0.00002202
Iteration 138/1000 | Loss: 0.00002202
Iteration 139/1000 | Loss: 0.00002202
Iteration 140/1000 | Loss: 0.00002202
Iteration 141/1000 | Loss: 0.00002202
Iteration 142/1000 | Loss: 0.00002202
Iteration 143/1000 | Loss: 0.00002202
Iteration 144/1000 | Loss: 0.00002202
Iteration 145/1000 | Loss: 0.00002201
Iteration 146/1000 | Loss: 0.00002201
Iteration 147/1000 | Loss: 0.00002201
Iteration 148/1000 | Loss: 0.00002201
Iteration 149/1000 | Loss: 0.00002201
Iteration 150/1000 | Loss: 0.00002201
Iteration 151/1000 | Loss: 0.00002201
Iteration 152/1000 | Loss: 0.00002200
Iteration 153/1000 | Loss: 0.00002200
Iteration 154/1000 | Loss: 0.00002200
Iteration 155/1000 | Loss: 0.00002200
Iteration 156/1000 | Loss: 0.00002200
Iteration 157/1000 | Loss: 0.00002200
Iteration 158/1000 | Loss: 0.00002200
Iteration 159/1000 | Loss: 0.00002200
Iteration 160/1000 | Loss: 0.00002200
Iteration 161/1000 | Loss: 0.00002200
Iteration 162/1000 | Loss: 0.00002200
Iteration 163/1000 | Loss: 0.00002200
Iteration 164/1000 | Loss: 0.00002200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.2003523554303683e-05, 2.2003523554303683e-05, 2.2003523554303683e-05, 2.2003523554303683e-05, 2.2003523554303683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2003523554303683e-05

Optimization complete. Final v2v error: 3.8339428901672363 mm

Highest mean error: 5.153984546661377 mm for frame 45

Lowest mean error: 3.048365831375122 mm for frame 122

Saving results

Total time: 40.59407353401184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00603618
Iteration 2/25 | Loss: 0.00132996
Iteration 3/25 | Loss: 0.00117812
Iteration 4/25 | Loss: 0.00116763
Iteration 5/25 | Loss: 0.00116492
Iteration 6/25 | Loss: 0.00116457
Iteration 7/25 | Loss: 0.00116457
Iteration 8/25 | Loss: 0.00116457
Iteration 9/25 | Loss: 0.00116457
Iteration 10/25 | Loss: 0.00116457
Iteration 11/25 | Loss: 0.00116457
Iteration 12/25 | Loss: 0.00116457
Iteration 13/25 | Loss: 0.00116457
Iteration 14/25 | Loss: 0.00116457
Iteration 15/25 | Loss: 0.00116457
Iteration 16/25 | Loss: 0.00116457
Iteration 17/25 | Loss: 0.00116457
Iteration 18/25 | Loss: 0.00116457
Iteration 19/25 | Loss: 0.00116457
Iteration 20/25 | Loss: 0.00116457
Iteration 21/25 | Loss: 0.00116457
Iteration 22/25 | Loss: 0.00116457
Iteration 23/25 | Loss: 0.00116457
Iteration 24/25 | Loss: 0.00116457
Iteration 25/25 | Loss: 0.00116457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.98824501
Iteration 2/25 | Loss: 0.00039515
Iteration 3/25 | Loss: 0.00039514
Iteration 4/25 | Loss: 0.00039514
Iteration 5/25 | Loss: 0.00039514
Iteration 6/25 | Loss: 0.00039514
Iteration 7/25 | Loss: 0.00039514
Iteration 8/25 | Loss: 0.00039514
Iteration 9/25 | Loss: 0.00039514
Iteration 10/25 | Loss: 0.00039514
Iteration 11/25 | Loss: 0.00039514
Iteration 12/25 | Loss: 0.00039514
Iteration 13/25 | Loss: 0.00039514
Iteration 14/25 | Loss: 0.00039514
Iteration 15/25 | Loss: 0.00039514
Iteration 16/25 | Loss: 0.00039514
Iteration 17/25 | Loss: 0.00039514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00039513930096291006, 0.00039513930096291006, 0.00039513930096291006, 0.00039513930096291006, 0.00039513930096291006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039513930096291006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039514
Iteration 2/1000 | Loss: 0.00003826
Iteration 3/1000 | Loss: 0.00002150
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001813
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001692
Iteration 9/1000 | Loss: 0.00001686
Iteration 10/1000 | Loss: 0.00001682
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001681
Iteration 14/1000 | Loss: 0.00001680
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001675
Iteration 18/1000 | Loss: 0.00001675
Iteration 19/1000 | Loss: 0.00001672
Iteration 20/1000 | Loss: 0.00001671
Iteration 21/1000 | Loss: 0.00001670
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001662
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001653
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001653
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001649
Iteration 45/1000 | Loss: 0.00001649
Iteration 46/1000 | Loss: 0.00001649
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001649
Iteration 62/1000 | Loss: 0.00001649
Iteration 63/1000 | Loss: 0.00001649
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001649
Iteration 66/1000 | Loss: 0.00001649
Iteration 67/1000 | Loss: 0.00001649
Iteration 68/1000 | Loss: 0.00001649
Iteration 69/1000 | Loss: 0.00001649
Iteration 70/1000 | Loss: 0.00001649
Iteration 71/1000 | Loss: 0.00001649
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001649
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001647
Iteration 78/1000 | Loss: 0.00001646
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001646
Iteration 81/1000 | Loss: 0.00001646
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001645
Iteration 85/1000 | Loss: 0.00001645
Iteration 86/1000 | Loss: 0.00001645
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001639
Iteration 103/1000 | Loss: 0.00001639
Iteration 104/1000 | Loss: 0.00001639
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001639
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001638
Iteration 110/1000 | Loss: 0.00001638
Iteration 111/1000 | Loss: 0.00001637
Iteration 112/1000 | Loss: 0.00001637
Iteration 113/1000 | Loss: 0.00001637
Iteration 114/1000 | Loss: 0.00001637
Iteration 115/1000 | Loss: 0.00001637
Iteration 116/1000 | Loss: 0.00001637
Iteration 117/1000 | Loss: 0.00001637
Iteration 118/1000 | Loss: 0.00001637
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001636
Iteration 121/1000 | Loss: 0.00001636
Iteration 122/1000 | Loss: 0.00001636
Iteration 123/1000 | Loss: 0.00001636
Iteration 124/1000 | Loss: 0.00001636
Iteration 125/1000 | Loss: 0.00001636
Iteration 126/1000 | Loss: 0.00001636
Iteration 127/1000 | Loss: 0.00001636
Iteration 128/1000 | Loss: 0.00001636
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001636
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.636265187698882e-05, 1.636265187698882e-05, 1.636265187698882e-05, 1.636265187698882e-05, 1.636265187698882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.636265187698882e-05

Optimization complete. Final v2v error: 3.4180188179016113 mm

Highest mean error: 3.6992948055267334 mm for frame 53

Lowest mean error: 3.1919546127319336 mm for frame 126

Saving results

Total time: 30.271586894989014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498094
Iteration 2/25 | Loss: 0.00145340
Iteration 3/25 | Loss: 0.00121956
Iteration 4/25 | Loss: 0.00120064
Iteration 5/25 | Loss: 0.00119727
Iteration 6/25 | Loss: 0.00119671
Iteration 7/25 | Loss: 0.00119671
Iteration 8/25 | Loss: 0.00119671
Iteration 9/25 | Loss: 0.00119671
Iteration 10/25 | Loss: 0.00119671
Iteration 11/25 | Loss: 0.00119671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011967120226472616, 0.0011967120226472616, 0.0011967120226472616, 0.0011967120226472616, 0.0011967120226472616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011967120226472616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42845368
Iteration 2/25 | Loss: 0.00038408
Iteration 3/25 | Loss: 0.00038407
Iteration 4/25 | Loss: 0.00038406
Iteration 5/25 | Loss: 0.00038406
Iteration 6/25 | Loss: 0.00038406
Iteration 7/25 | Loss: 0.00038406
Iteration 8/25 | Loss: 0.00038406
Iteration 9/25 | Loss: 0.00038406
Iteration 10/25 | Loss: 0.00038406
Iteration 11/25 | Loss: 0.00038406
Iteration 12/25 | Loss: 0.00038406
Iteration 13/25 | Loss: 0.00038406
Iteration 14/25 | Loss: 0.00038406
Iteration 15/25 | Loss: 0.00038406
Iteration 16/25 | Loss: 0.00038406
Iteration 17/25 | Loss: 0.00038406
Iteration 18/25 | Loss: 0.00038406
Iteration 19/25 | Loss: 0.00038406
Iteration 20/25 | Loss: 0.00038406
Iteration 21/25 | Loss: 0.00038406
Iteration 22/25 | Loss: 0.00038406
Iteration 23/25 | Loss: 0.00038406
Iteration 24/25 | Loss: 0.00038406
Iteration 25/25 | Loss: 0.00038406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038406
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00002535
Iteration 4/1000 | Loss: 0.00002296
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00002097
Iteration 7/1000 | Loss: 0.00002043
Iteration 8/1000 | Loss: 0.00002012
Iteration 9/1000 | Loss: 0.00001991
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001980
Iteration 12/1000 | Loss: 0.00001974
Iteration 13/1000 | Loss: 0.00001970
Iteration 14/1000 | Loss: 0.00001968
Iteration 15/1000 | Loss: 0.00001968
Iteration 16/1000 | Loss: 0.00001967
Iteration 17/1000 | Loss: 0.00001966
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001965
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001964
Iteration 22/1000 | Loss: 0.00001964
Iteration 23/1000 | Loss: 0.00001963
Iteration 24/1000 | Loss: 0.00001962
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001960
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001956
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001951
Iteration 41/1000 | Loss: 0.00001951
Iteration 42/1000 | Loss: 0.00001950
Iteration 43/1000 | Loss: 0.00001950
Iteration 44/1000 | Loss: 0.00001950
Iteration 45/1000 | Loss: 0.00001950
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001949
Iteration 50/1000 | Loss: 0.00001949
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001948
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001945
Iteration 73/1000 | Loss: 0.00001945
Iteration 74/1000 | Loss: 0.00001945
Iteration 75/1000 | Loss: 0.00001945
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001945
Iteration 78/1000 | Loss: 0.00001945
Iteration 79/1000 | Loss: 0.00001945
Iteration 80/1000 | Loss: 0.00001944
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001944
Iteration 83/1000 | Loss: 0.00001944
Iteration 84/1000 | Loss: 0.00001944
Iteration 85/1000 | Loss: 0.00001943
Iteration 86/1000 | Loss: 0.00001943
Iteration 87/1000 | Loss: 0.00001943
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001943
Iteration 90/1000 | Loss: 0.00001943
Iteration 91/1000 | Loss: 0.00001942
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Iteration 94/1000 | Loss: 0.00001942
Iteration 95/1000 | Loss: 0.00001941
Iteration 96/1000 | Loss: 0.00001941
Iteration 97/1000 | Loss: 0.00001941
Iteration 98/1000 | Loss: 0.00001941
Iteration 99/1000 | Loss: 0.00001941
Iteration 100/1000 | Loss: 0.00001941
Iteration 101/1000 | Loss: 0.00001940
Iteration 102/1000 | Loss: 0.00001940
Iteration 103/1000 | Loss: 0.00001940
Iteration 104/1000 | Loss: 0.00001939
Iteration 105/1000 | Loss: 0.00001939
Iteration 106/1000 | Loss: 0.00001939
Iteration 107/1000 | Loss: 0.00001939
Iteration 108/1000 | Loss: 0.00001939
Iteration 109/1000 | Loss: 0.00001939
Iteration 110/1000 | Loss: 0.00001939
Iteration 111/1000 | Loss: 0.00001938
Iteration 112/1000 | Loss: 0.00001938
Iteration 113/1000 | Loss: 0.00001938
Iteration 114/1000 | Loss: 0.00001938
Iteration 115/1000 | Loss: 0.00001938
Iteration 116/1000 | Loss: 0.00001937
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001936
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001936
Iteration 131/1000 | Loss: 0.00001936
Iteration 132/1000 | Loss: 0.00001936
Iteration 133/1000 | Loss: 0.00001936
Iteration 134/1000 | Loss: 0.00001936
Iteration 135/1000 | Loss: 0.00001936
Iteration 136/1000 | Loss: 0.00001936
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001935
Iteration 140/1000 | Loss: 0.00001935
Iteration 141/1000 | Loss: 0.00001935
Iteration 142/1000 | Loss: 0.00001935
Iteration 143/1000 | Loss: 0.00001935
Iteration 144/1000 | Loss: 0.00001935
Iteration 145/1000 | Loss: 0.00001935
Iteration 146/1000 | Loss: 0.00001935
Iteration 147/1000 | Loss: 0.00001935
Iteration 148/1000 | Loss: 0.00001935
Iteration 149/1000 | Loss: 0.00001935
Iteration 150/1000 | Loss: 0.00001935
Iteration 151/1000 | Loss: 0.00001935
Iteration 152/1000 | Loss: 0.00001935
Iteration 153/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.9349363356013782e-05, 1.9349363356013782e-05, 1.9349363356013782e-05, 1.9349363356013782e-05, 1.9349363356013782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9349363356013782e-05

Optimization complete. Final v2v error: 3.661971092224121 mm

Highest mean error: 4.033915996551514 mm for frame 61

Lowest mean error: 3.0835459232330322 mm for frame 239

Saving results

Total time: 38.862213373184204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480374
Iteration 2/25 | Loss: 0.00154643
Iteration 3/25 | Loss: 0.00129045
Iteration 4/25 | Loss: 0.00126809
Iteration 5/25 | Loss: 0.00125837
Iteration 6/25 | Loss: 0.00125573
Iteration 7/25 | Loss: 0.00125478
Iteration 8/25 | Loss: 0.00125464
Iteration 9/25 | Loss: 0.00125464
Iteration 10/25 | Loss: 0.00125464
Iteration 11/25 | Loss: 0.00125464
Iteration 12/25 | Loss: 0.00125464
Iteration 13/25 | Loss: 0.00125464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012546413345262408, 0.0012546413345262408, 0.0012546413345262408, 0.0012546413345262408, 0.0012546413345262408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012546413345262408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23204505
Iteration 2/25 | Loss: 0.00057945
Iteration 3/25 | Loss: 0.00057943
Iteration 4/25 | Loss: 0.00057943
Iteration 5/25 | Loss: 0.00057943
Iteration 6/25 | Loss: 0.00057943
Iteration 7/25 | Loss: 0.00057943
Iteration 8/25 | Loss: 0.00057943
Iteration 9/25 | Loss: 0.00057942
Iteration 10/25 | Loss: 0.00057942
Iteration 11/25 | Loss: 0.00057942
Iteration 12/25 | Loss: 0.00057942
Iteration 13/25 | Loss: 0.00057942
Iteration 14/25 | Loss: 0.00057942
Iteration 15/25 | Loss: 0.00057942
Iteration 16/25 | Loss: 0.00057942
Iteration 17/25 | Loss: 0.00057942
Iteration 18/25 | Loss: 0.00057942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005794241442345083, 0.0005794241442345083, 0.0005794241442345083, 0.0005794241442345083, 0.0005794241442345083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005794241442345083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057942
Iteration 2/1000 | Loss: 0.00008467
Iteration 3/1000 | Loss: 0.00004535
Iteration 4/1000 | Loss: 0.00003939
Iteration 5/1000 | Loss: 0.00003760
Iteration 6/1000 | Loss: 0.00003645
Iteration 7/1000 | Loss: 0.00003522
Iteration 8/1000 | Loss: 0.00003423
Iteration 9/1000 | Loss: 0.00003379
Iteration 10/1000 | Loss: 0.00003341
Iteration 11/1000 | Loss: 0.00003308
Iteration 12/1000 | Loss: 0.00003285
Iteration 13/1000 | Loss: 0.00003265
Iteration 14/1000 | Loss: 0.00003246
Iteration 15/1000 | Loss: 0.00003232
Iteration 16/1000 | Loss: 0.00003223
Iteration 17/1000 | Loss: 0.00003222
Iteration 18/1000 | Loss: 0.00003222
Iteration 19/1000 | Loss: 0.00003221
Iteration 20/1000 | Loss: 0.00003215
Iteration 21/1000 | Loss: 0.00003207
Iteration 22/1000 | Loss: 0.00003204
Iteration 23/1000 | Loss: 0.00003203
Iteration 24/1000 | Loss: 0.00003203
Iteration 25/1000 | Loss: 0.00003203
Iteration 26/1000 | Loss: 0.00003203
Iteration 27/1000 | Loss: 0.00003203
Iteration 28/1000 | Loss: 0.00003202
Iteration 29/1000 | Loss: 0.00003202
Iteration 30/1000 | Loss: 0.00003202
Iteration 31/1000 | Loss: 0.00003202
Iteration 32/1000 | Loss: 0.00003202
Iteration 33/1000 | Loss: 0.00003200
Iteration 34/1000 | Loss: 0.00003200
Iteration 35/1000 | Loss: 0.00003200
Iteration 36/1000 | Loss: 0.00003199
Iteration 37/1000 | Loss: 0.00003197
Iteration 38/1000 | Loss: 0.00003194
Iteration 39/1000 | Loss: 0.00003194
Iteration 40/1000 | Loss: 0.00003193
Iteration 41/1000 | Loss: 0.00003192
Iteration 42/1000 | Loss: 0.00003192
Iteration 43/1000 | Loss: 0.00003191
Iteration 44/1000 | Loss: 0.00003191
Iteration 45/1000 | Loss: 0.00003191
Iteration 46/1000 | Loss: 0.00003190
Iteration 47/1000 | Loss: 0.00003190
Iteration 48/1000 | Loss: 0.00003190
Iteration 49/1000 | Loss: 0.00003190
Iteration 50/1000 | Loss: 0.00003190
Iteration 51/1000 | Loss: 0.00003190
Iteration 52/1000 | Loss: 0.00003189
Iteration 53/1000 | Loss: 0.00003189
Iteration 54/1000 | Loss: 0.00003189
Iteration 55/1000 | Loss: 0.00003188
Iteration 56/1000 | Loss: 0.00003188
Iteration 57/1000 | Loss: 0.00003187
Iteration 58/1000 | Loss: 0.00003187
Iteration 59/1000 | Loss: 0.00003187
Iteration 60/1000 | Loss: 0.00003184
Iteration 61/1000 | Loss: 0.00003184
Iteration 62/1000 | Loss: 0.00003183
Iteration 63/1000 | Loss: 0.00003183
Iteration 64/1000 | Loss: 0.00003183
Iteration 65/1000 | Loss: 0.00003183
Iteration 66/1000 | Loss: 0.00003180
Iteration 67/1000 | Loss: 0.00003180
Iteration 68/1000 | Loss: 0.00003180
Iteration 69/1000 | Loss: 0.00003180
Iteration 70/1000 | Loss: 0.00003180
Iteration 71/1000 | Loss: 0.00003180
Iteration 72/1000 | Loss: 0.00003180
Iteration 73/1000 | Loss: 0.00003180
Iteration 74/1000 | Loss: 0.00003179
Iteration 75/1000 | Loss: 0.00003179
Iteration 76/1000 | Loss: 0.00003179
Iteration 77/1000 | Loss: 0.00003178
Iteration 78/1000 | Loss: 0.00003178
Iteration 79/1000 | Loss: 0.00003177
Iteration 80/1000 | Loss: 0.00003177
Iteration 81/1000 | Loss: 0.00003177
Iteration 82/1000 | Loss: 0.00003177
Iteration 83/1000 | Loss: 0.00003177
Iteration 84/1000 | Loss: 0.00003177
Iteration 85/1000 | Loss: 0.00003177
Iteration 86/1000 | Loss: 0.00003177
Iteration 87/1000 | Loss: 0.00003177
Iteration 88/1000 | Loss: 0.00003177
Iteration 89/1000 | Loss: 0.00003177
Iteration 90/1000 | Loss: 0.00003176
Iteration 91/1000 | Loss: 0.00003176
Iteration 92/1000 | Loss: 0.00003176
Iteration 93/1000 | Loss: 0.00003176
Iteration 94/1000 | Loss: 0.00003175
Iteration 95/1000 | Loss: 0.00003175
Iteration 96/1000 | Loss: 0.00003175
Iteration 97/1000 | Loss: 0.00003175
Iteration 98/1000 | Loss: 0.00003175
Iteration 99/1000 | Loss: 0.00003174
Iteration 100/1000 | Loss: 0.00003174
Iteration 101/1000 | Loss: 0.00003174
Iteration 102/1000 | Loss: 0.00003174
Iteration 103/1000 | Loss: 0.00003174
Iteration 104/1000 | Loss: 0.00003174
Iteration 105/1000 | Loss: 0.00003173
Iteration 106/1000 | Loss: 0.00003173
Iteration 107/1000 | Loss: 0.00003173
Iteration 108/1000 | Loss: 0.00003172
Iteration 109/1000 | Loss: 0.00003172
Iteration 110/1000 | Loss: 0.00003172
Iteration 111/1000 | Loss: 0.00003172
Iteration 112/1000 | Loss: 0.00003171
Iteration 113/1000 | Loss: 0.00003171
Iteration 114/1000 | Loss: 0.00003171
Iteration 115/1000 | Loss: 0.00003171
Iteration 116/1000 | Loss: 0.00003171
Iteration 117/1000 | Loss: 0.00003171
Iteration 118/1000 | Loss: 0.00003170
Iteration 119/1000 | Loss: 0.00003170
Iteration 120/1000 | Loss: 0.00003170
Iteration 121/1000 | Loss: 0.00003170
Iteration 122/1000 | Loss: 0.00003170
Iteration 123/1000 | Loss: 0.00003170
Iteration 124/1000 | Loss: 0.00003168
Iteration 125/1000 | Loss: 0.00003168
Iteration 126/1000 | Loss: 0.00003167
Iteration 127/1000 | Loss: 0.00003167
Iteration 128/1000 | Loss: 0.00003166
Iteration 129/1000 | Loss: 0.00003165
Iteration 130/1000 | Loss: 0.00003165
Iteration 131/1000 | Loss: 0.00003165
Iteration 132/1000 | Loss: 0.00003165
Iteration 133/1000 | Loss: 0.00003165
Iteration 134/1000 | Loss: 0.00003164
Iteration 135/1000 | Loss: 0.00003164
Iteration 136/1000 | Loss: 0.00003164
Iteration 137/1000 | Loss: 0.00003164
Iteration 138/1000 | Loss: 0.00003164
Iteration 139/1000 | Loss: 0.00003164
Iteration 140/1000 | Loss: 0.00003164
Iteration 141/1000 | Loss: 0.00003164
Iteration 142/1000 | Loss: 0.00003164
Iteration 143/1000 | Loss: 0.00003162
Iteration 144/1000 | Loss: 0.00003162
Iteration 145/1000 | Loss: 0.00003162
Iteration 146/1000 | Loss: 0.00003161
Iteration 147/1000 | Loss: 0.00003161
Iteration 148/1000 | Loss: 0.00003160
Iteration 149/1000 | Loss: 0.00003160
Iteration 150/1000 | Loss: 0.00003160
Iteration 151/1000 | Loss: 0.00003160
Iteration 152/1000 | Loss: 0.00003159
Iteration 153/1000 | Loss: 0.00003159
Iteration 154/1000 | Loss: 0.00003159
Iteration 155/1000 | Loss: 0.00003159
Iteration 156/1000 | Loss: 0.00003159
Iteration 157/1000 | Loss: 0.00003158
Iteration 158/1000 | Loss: 0.00003158
Iteration 159/1000 | Loss: 0.00003158
Iteration 160/1000 | Loss: 0.00003158
Iteration 161/1000 | Loss: 0.00003158
Iteration 162/1000 | Loss: 0.00003158
Iteration 163/1000 | Loss: 0.00003158
Iteration 164/1000 | Loss: 0.00003158
Iteration 165/1000 | Loss: 0.00003157
Iteration 166/1000 | Loss: 0.00003157
Iteration 167/1000 | Loss: 0.00003157
Iteration 168/1000 | Loss: 0.00003157
Iteration 169/1000 | Loss: 0.00003157
Iteration 170/1000 | Loss: 0.00003157
Iteration 171/1000 | Loss: 0.00003156
Iteration 172/1000 | Loss: 0.00003156
Iteration 173/1000 | Loss: 0.00003156
Iteration 174/1000 | Loss: 0.00003156
Iteration 175/1000 | Loss: 0.00003156
Iteration 176/1000 | Loss: 0.00003155
Iteration 177/1000 | Loss: 0.00003155
Iteration 178/1000 | Loss: 0.00003155
Iteration 179/1000 | Loss: 0.00003155
Iteration 180/1000 | Loss: 0.00003155
Iteration 181/1000 | Loss: 0.00003154
Iteration 182/1000 | Loss: 0.00003154
Iteration 183/1000 | Loss: 0.00003153
Iteration 184/1000 | Loss: 0.00003153
Iteration 185/1000 | Loss: 0.00003153
Iteration 186/1000 | Loss: 0.00003153
Iteration 187/1000 | Loss: 0.00003153
Iteration 188/1000 | Loss: 0.00003153
Iteration 189/1000 | Loss: 0.00003152
Iteration 190/1000 | Loss: 0.00003152
Iteration 191/1000 | Loss: 0.00003152
Iteration 192/1000 | Loss: 0.00003152
Iteration 193/1000 | Loss: 0.00003152
Iteration 194/1000 | Loss: 0.00003151
Iteration 195/1000 | Loss: 0.00003151
Iteration 196/1000 | Loss: 0.00003151
Iteration 197/1000 | Loss: 0.00003151
Iteration 198/1000 | Loss: 0.00003151
Iteration 199/1000 | Loss: 0.00003151
Iteration 200/1000 | Loss: 0.00003150
Iteration 201/1000 | Loss: 0.00003150
Iteration 202/1000 | Loss: 0.00003150
Iteration 203/1000 | Loss: 0.00003150
Iteration 204/1000 | Loss: 0.00003150
Iteration 205/1000 | Loss: 0.00003150
Iteration 206/1000 | Loss: 0.00003149
Iteration 207/1000 | Loss: 0.00003149
Iteration 208/1000 | Loss: 0.00003149
Iteration 209/1000 | Loss: 0.00003149
Iteration 210/1000 | Loss: 0.00003149
Iteration 211/1000 | Loss: 0.00003149
Iteration 212/1000 | Loss: 0.00003149
Iteration 213/1000 | Loss: 0.00003149
Iteration 214/1000 | Loss: 0.00003149
Iteration 215/1000 | Loss: 0.00003149
Iteration 216/1000 | Loss: 0.00003149
Iteration 217/1000 | Loss: 0.00003149
Iteration 218/1000 | Loss: 0.00003149
Iteration 219/1000 | Loss: 0.00003149
Iteration 220/1000 | Loss: 0.00003148
Iteration 221/1000 | Loss: 0.00003148
Iteration 222/1000 | Loss: 0.00003148
Iteration 223/1000 | Loss: 0.00003148
Iteration 224/1000 | Loss: 0.00003148
Iteration 225/1000 | Loss: 0.00003148
Iteration 226/1000 | Loss: 0.00003148
Iteration 227/1000 | Loss: 0.00003148
Iteration 228/1000 | Loss: 0.00003148
Iteration 229/1000 | Loss: 0.00003148
Iteration 230/1000 | Loss: 0.00003148
Iteration 231/1000 | Loss: 0.00003148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [3.14843600790482e-05, 3.14843600790482e-05, 3.14843600790482e-05, 3.14843600790482e-05, 3.14843600790482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.14843600790482e-05

Optimization complete. Final v2v error: 4.503299236297607 mm

Highest mean error: 6.3507981300354 mm for frame 79

Lowest mean error: 3.5799059867858887 mm for frame 47

Saving results

Total time: 51.73422408103943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105132
Iteration 2/25 | Loss: 0.00310788
Iteration 3/25 | Loss: 0.00214988
Iteration 4/25 | Loss: 0.00179541
Iteration 5/25 | Loss: 0.00153895
Iteration 6/25 | Loss: 0.00148461
Iteration 7/25 | Loss: 0.00138098
Iteration 8/25 | Loss: 0.00135160
Iteration 9/25 | Loss: 0.00128953
Iteration 10/25 | Loss: 0.00126937
Iteration 11/25 | Loss: 0.00124554
Iteration 12/25 | Loss: 0.00124548
Iteration 13/25 | Loss: 0.00122720
Iteration 14/25 | Loss: 0.00121280
Iteration 15/25 | Loss: 0.00120944
Iteration 16/25 | Loss: 0.00120688
Iteration 17/25 | Loss: 0.00120598
Iteration 18/25 | Loss: 0.00120571
Iteration 19/25 | Loss: 0.00120590
Iteration 20/25 | Loss: 0.00120610
Iteration 21/25 | Loss: 0.00120632
Iteration 22/25 | Loss: 0.00120657
Iteration 23/25 | Loss: 0.00120649
Iteration 24/25 | Loss: 0.00120652
Iteration 25/25 | Loss: 0.00120615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03766012
Iteration 2/25 | Loss: 0.00061757
Iteration 3/25 | Loss: 0.00061757
Iteration 4/25 | Loss: 0.00061757
Iteration 5/25 | Loss: 0.00061757
Iteration 6/25 | Loss: 0.00061757
Iteration 7/25 | Loss: 0.00061757
Iteration 8/25 | Loss: 0.00061757
Iteration 9/25 | Loss: 0.00061757
Iteration 10/25 | Loss: 0.00061757
Iteration 11/25 | Loss: 0.00061757
Iteration 12/25 | Loss: 0.00061757
Iteration 13/25 | Loss: 0.00061757
Iteration 14/25 | Loss: 0.00061757
Iteration 15/25 | Loss: 0.00061757
Iteration 16/25 | Loss: 0.00061757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006175693706609309, 0.0006175693706609309, 0.0006175693706609309, 0.0006175693706609309, 0.0006175693706609309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006175693706609309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061757
Iteration 2/1000 | Loss: 0.00012871
Iteration 3/1000 | Loss: 0.00035643
Iteration 4/1000 | Loss: 0.00038323
Iteration 5/1000 | Loss: 0.00009334
Iteration 6/1000 | Loss: 0.00009465
Iteration 7/1000 | Loss: 0.00008271
Iteration 8/1000 | Loss: 0.00007340
Iteration 9/1000 | Loss: 0.00007062
Iteration 10/1000 | Loss: 0.00006853
Iteration 11/1000 | Loss: 0.00579158
Iteration 12/1000 | Loss: 0.00139870
Iteration 13/1000 | Loss: 0.00010278
Iteration 14/1000 | Loss: 0.00006947
Iteration 15/1000 | Loss: 0.00005480
Iteration 16/1000 | Loss: 0.00004303
Iteration 17/1000 | Loss: 0.00003822
Iteration 18/1000 | Loss: 0.00004423
Iteration 19/1000 | Loss: 0.00004068
Iteration 20/1000 | Loss: 0.00003306
Iteration 21/1000 | Loss: 0.00003004
Iteration 22/1000 | Loss: 0.00003916
Iteration 23/1000 | Loss: 0.00002826
Iteration 24/1000 | Loss: 0.00002738
Iteration 25/1000 | Loss: 0.00002667
Iteration 26/1000 | Loss: 0.00002579
Iteration 27/1000 | Loss: 0.00002519
Iteration 28/1000 | Loss: 0.00002471
Iteration 29/1000 | Loss: 0.00002438
Iteration 30/1000 | Loss: 0.00002415
Iteration 31/1000 | Loss: 0.00002410
Iteration 32/1000 | Loss: 0.00002408
Iteration 33/1000 | Loss: 0.00002404
Iteration 34/1000 | Loss: 0.00002398
Iteration 35/1000 | Loss: 0.00002394
Iteration 36/1000 | Loss: 0.00002393
Iteration 37/1000 | Loss: 0.00002393
Iteration 38/1000 | Loss: 0.00002392
Iteration 39/1000 | Loss: 0.00002392
Iteration 40/1000 | Loss: 0.00002391
Iteration 41/1000 | Loss: 0.00002391
Iteration 42/1000 | Loss: 0.00002390
Iteration 43/1000 | Loss: 0.00002390
Iteration 44/1000 | Loss: 0.00002389
Iteration 45/1000 | Loss: 0.00002389
Iteration 46/1000 | Loss: 0.00002389
Iteration 47/1000 | Loss: 0.00002388
Iteration 48/1000 | Loss: 0.00002388
Iteration 49/1000 | Loss: 0.00002388
Iteration 50/1000 | Loss: 0.00002387
Iteration 51/1000 | Loss: 0.00002387
Iteration 52/1000 | Loss: 0.00002387
Iteration 53/1000 | Loss: 0.00002387
Iteration 54/1000 | Loss: 0.00002386
Iteration 55/1000 | Loss: 0.00002386
Iteration 56/1000 | Loss: 0.00002386
Iteration 57/1000 | Loss: 0.00002386
Iteration 58/1000 | Loss: 0.00002386
Iteration 59/1000 | Loss: 0.00002386
Iteration 60/1000 | Loss: 0.00002385
Iteration 61/1000 | Loss: 0.00002385
Iteration 62/1000 | Loss: 0.00002385
Iteration 63/1000 | Loss: 0.00002385
Iteration 64/1000 | Loss: 0.00002384
Iteration 65/1000 | Loss: 0.00002384
Iteration 66/1000 | Loss: 0.00002383
Iteration 67/1000 | Loss: 0.00002383
Iteration 68/1000 | Loss: 0.00002383
Iteration 69/1000 | Loss: 0.00002383
Iteration 70/1000 | Loss: 0.00002383
Iteration 71/1000 | Loss: 0.00002382
Iteration 72/1000 | Loss: 0.00002381
Iteration 73/1000 | Loss: 0.00002381
Iteration 74/1000 | Loss: 0.00002381
Iteration 75/1000 | Loss: 0.00002381
Iteration 76/1000 | Loss: 0.00002381
Iteration 77/1000 | Loss: 0.00002381
Iteration 78/1000 | Loss: 0.00002381
Iteration 79/1000 | Loss: 0.00002398
Iteration 80/1000 | Loss: 0.00002398
Iteration 81/1000 | Loss: 0.00002398
Iteration 82/1000 | Loss: 0.00002398
Iteration 83/1000 | Loss: 0.00002398
Iteration 84/1000 | Loss: 0.00002398
Iteration 85/1000 | Loss: 0.00002398
Iteration 86/1000 | Loss: 0.00002398
Iteration 87/1000 | Loss: 0.00002398
Iteration 88/1000 | Loss: 0.00002397
Iteration 89/1000 | Loss: 0.00002397
Iteration 90/1000 | Loss: 0.00002396
Iteration 91/1000 | Loss: 0.00002380
Iteration 92/1000 | Loss: 0.00002380
Iteration 93/1000 | Loss: 0.00002379
Iteration 94/1000 | Loss: 0.00002379
Iteration 95/1000 | Loss: 0.00002379
Iteration 96/1000 | Loss: 0.00002379
Iteration 97/1000 | Loss: 0.00002379
Iteration 98/1000 | Loss: 0.00002379
Iteration 99/1000 | Loss: 0.00002379
Iteration 100/1000 | Loss: 0.00002379
Iteration 101/1000 | Loss: 0.00002379
Iteration 102/1000 | Loss: 0.00002379
Iteration 103/1000 | Loss: 0.00002379
Iteration 104/1000 | Loss: 0.00002378
Iteration 105/1000 | Loss: 0.00002378
Iteration 106/1000 | Loss: 0.00002378
Iteration 107/1000 | Loss: 0.00002378
Iteration 108/1000 | Loss: 0.00002377
Iteration 109/1000 | Loss: 0.00002377
Iteration 110/1000 | Loss: 0.00002376
Iteration 111/1000 | Loss: 0.00002376
Iteration 112/1000 | Loss: 0.00002376
Iteration 113/1000 | Loss: 0.00002376
Iteration 114/1000 | Loss: 0.00002376
Iteration 115/1000 | Loss: 0.00002376
Iteration 116/1000 | Loss: 0.00002376
Iteration 117/1000 | Loss: 0.00002376
Iteration 118/1000 | Loss: 0.00002376
Iteration 119/1000 | Loss: 0.00002375
Iteration 120/1000 | Loss: 0.00002375
Iteration 121/1000 | Loss: 0.00002375
Iteration 122/1000 | Loss: 0.00002375
Iteration 123/1000 | Loss: 0.00002375
Iteration 124/1000 | Loss: 0.00002375
Iteration 125/1000 | Loss: 0.00002375
Iteration 126/1000 | Loss: 0.00002375
Iteration 127/1000 | Loss: 0.00002375
Iteration 128/1000 | Loss: 0.00002375
Iteration 129/1000 | Loss: 0.00002375
Iteration 130/1000 | Loss: 0.00002375
Iteration 131/1000 | Loss: 0.00002375
Iteration 132/1000 | Loss: 0.00002375
Iteration 133/1000 | Loss: 0.00002375
Iteration 134/1000 | Loss: 0.00002375
Iteration 135/1000 | Loss: 0.00002374
Iteration 136/1000 | Loss: 0.00002374
Iteration 137/1000 | Loss: 0.00002374
Iteration 138/1000 | Loss: 0.00002374
Iteration 139/1000 | Loss: 0.00002374
Iteration 140/1000 | Loss: 0.00002374
Iteration 141/1000 | Loss: 0.00002374
Iteration 142/1000 | Loss: 0.00002374
Iteration 143/1000 | Loss: 0.00002374
Iteration 144/1000 | Loss: 0.00002374
Iteration 145/1000 | Loss: 0.00002374
Iteration 146/1000 | Loss: 0.00002374
Iteration 147/1000 | Loss: 0.00002374
Iteration 148/1000 | Loss: 0.00002373
Iteration 149/1000 | Loss: 0.00002373
Iteration 150/1000 | Loss: 0.00002373
Iteration 151/1000 | Loss: 0.00002373
Iteration 152/1000 | Loss: 0.00002373
Iteration 153/1000 | Loss: 0.00002373
Iteration 154/1000 | Loss: 0.00002373
Iteration 155/1000 | Loss: 0.00002373
Iteration 156/1000 | Loss: 0.00002373
Iteration 157/1000 | Loss: 0.00002373
Iteration 158/1000 | Loss: 0.00002373
Iteration 159/1000 | Loss: 0.00002373
Iteration 160/1000 | Loss: 0.00002372
Iteration 161/1000 | Loss: 0.00002372
Iteration 162/1000 | Loss: 0.00002372
Iteration 163/1000 | Loss: 0.00002372
Iteration 164/1000 | Loss: 0.00002372
Iteration 165/1000 | Loss: 0.00002372
Iteration 166/1000 | Loss: 0.00002372
Iteration 167/1000 | Loss: 0.00002372
Iteration 168/1000 | Loss: 0.00002372
Iteration 169/1000 | Loss: 0.00002372
Iteration 170/1000 | Loss: 0.00002372
Iteration 171/1000 | Loss: 0.00002372
Iteration 172/1000 | Loss: 0.00002372
Iteration 173/1000 | Loss: 0.00002372
Iteration 174/1000 | Loss: 0.00002371
Iteration 175/1000 | Loss: 0.00002371
Iteration 176/1000 | Loss: 0.00002371
Iteration 177/1000 | Loss: 0.00002371
Iteration 178/1000 | Loss: 0.00002371
Iteration 179/1000 | Loss: 0.00002371
Iteration 180/1000 | Loss: 0.00002371
Iteration 181/1000 | Loss: 0.00002371
Iteration 182/1000 | Loss: 0.00002371
Iteration 183/1000 | Loss: 0.00002371
Iteration 184/1000 | Loss: 0.00002371
Iteration 185/1000 | Loss: 0.00002371
Iteration 186/1000 | Loss: 0.00002371
Iteration 187/1000 | Loss: 0.00002371
Iteration 188/1000 | Loss: 0.00002371
Iteration 189/1000 | Loss: 0.00002371
Iteration 190/1000 | Loss: 0.00002371
Iteration 191/1000 | Loss: 0.00002371
Iteration 192/1000 | Loss: 0.00002371
Iteration 193/1000 | Loss: 0.00002371
Iteration 194/1000 | Loss: 0.00002371
Iteration 195/1000 | Loss: 0.00002371
Iteration 196/1000 | Loss: 0.00002371
Iteration 197/1000 | Loss: 0.00002370
Iteration 198/1000 | Loss: 0.00002370
Iteration 199/1000 | Loss: 0.00002370
Iteration 200/1000 | Loss: 0.00002370
Iteration 201/1000 | Loss: 0.00002370
Iteration 202/1000 | Loss: 0.00002370
Iteration 203/1000 | Loss: 0.00002370
Iteration 204/1000 | Loss: 0.00002370
Iteration 205/1000 | Loss: 0.00002370
Iteration 206/1000 | Loss: 0.00002370
Iteration 207/1000 | Loss: 0.00002370
Iteration 208/1000 | Loss: 0.00002370
Iteration 209/1000 | Loss: 0.00002370
Iteration 210/1000 | Loss: 0.00002370
Iteration 211/1000 | Loss: 0.00002370
Iteration 212/1000 | Loss: 0.00002370
Iteration 213/1000 | Loss: 0.00002370
Iteration 214/1000 | Loss: 0.00002370
Iteration 215/1000 | Loss: 0.00002370
Iteration 216/1000 | Loss: 0.00002370
Iteration 217/1000 | Loss: 0.00002370
Iteration 218/1000 | Loss: 0.00002370
Iteration 219/1000 | Loss: 0.00002370
Iteration 220/1000 | Loss: 0.00002370
Iteration 221/1000 | Loss: 0.00002370
Iteration 222/1000 | Loss: 0.00002370
Iteration 223/1000 | Loss: 0.00002370
Iteration 224/1000 | Loss: 0.00002370
Iteration 225/1000 | Loss: 0.00002370
Iteration 226/1000 | Loss: 0.00002370
Iteration 227/1000 | Loss: 0.00002370
Iteration 228/1000 | Loss: 0.00002370
Iteration 229/1000 | Loss: 0.00002370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.3699540179222822e-05, 2.3699540179222822e-05, 2.3699540179222822e-05, 2.3699540179222822e-05, 2.3699540179222822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3699540179222822e-05

Optimization complete. Final v2v error: 3.6826181411743164 mm

Highest mean error: 21.068313598632812 mm for frame 216

Lowest mean error: 3.068392515182495 mm for frame 133

Saving results

Total time: 115.30067753791809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051159
Iteration 2/25 | Loss: 0.00474763
Iteration 3/25 | Loss: 0.00336587
Iteration 4/25 | Loss: 0.00304171
Iteration 5/25 | Loss: 0.00261719
Iteration 6/25 | Loss: 0.00242905
Iteration 7/25 | Loss: 0.00224269
Iteration 8/25 | Loss: 0.00201167
Iteration 9/25 | Loss: 0.00191057
Iteration 10/25 | Loss: 0.00186994
Iteration 11/25 | Loss: 0.00184524
Iteration 12/25 | Loss: 0.00182893
Iteration 13/25 | Loss: 0.00183235
Iteration 14/25 | Loss: 0.00180838
Iteration 15/25 | Loss: 0.00179929
Iteration 16/25 | Loss: 0.00179950
Iteration 17/25 | Loss: 0.00179129
Iteration 18/25 | Loss: 0.00178474
Iteration 19/25 | Loss: 0.00177956
Iteration 20/25 | Loss: 0.00178015
Iteration 21/25 | Loss: 0.00177583
Iteration 22/25 | Loss: 0.00177366
Iteration 23/25 | Loss: 0.00177264
Iteration 24/25 | Loss: 0.00177611
Iteration 25/25 | Loss: 0.00177532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40805650
Iteration 2/25 | Loss: 0.00485970
Iteration 3/25 | Loss: 0.00484291
Iteration 4/25 | Loss: 0.00484290
Iteration 5/25 | Loss: 0.00484290
Iteration 6/25 | Loss: 0.00484290
Iteration 7/25 | Loss: 0.00484290
Iteration 8/25 | Loss: 0.00484290
Iteration 9/25 | Loss: 0.00484290
Iteration 10/25 | Loss: 0.00484290
Iteration 11/25 | Loss: 0.00484290
Iteration 12/25 | Loss: 0.00484290
Iteration 13/25 | Loss: 0.00484290
Iteration 14/25 | Loss: 0.00484290
Iteration 15/25 | Loss: 0.00484290
Iteration 16/25 | Loss: 0.00484290
Iteration 17/25 | Loss: 0.00484290
Iteration 18/25 | Loss: 0.00484290
Iteration 19/25 | Loss: 0.00484290
Iteration 20/25 | Loss: 0.00484290
Iteration 21/25 | Loss: 0.00484290
Iteration 22/25 | Loss: 0.00484290
Iteration 23/25 | Loss: 0.00484290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004842896945774555, 0.004842896945774555, 0.004842896945774555, 0.004842896945774555, 0.004842896945774555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004842896945774555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00484290
Iteration 2/1000 | Loss: 0.00083780
Iteration 3/1000 | Loss: 0.00062677
Iteration 4/1000 | Loss: 0.00052131
Iteration 5/1000 | Loss: 0.00046027
Iteration 6/1000 | Loss: 0.00041978
Iteration 7/1000 | Loss: 0.00039330
Iteration 8/1000 | Loss: 0.00069451
Iteration 9/1000 | Loss: 0.00037317
Iteration 10/1000 | Loss: 0.00527780
Iteration 11/1000 | Loss: 0.02394704
Iteration 12/1000 | Loss: 0.00128146
Iteration 13/1000 | Loss: 0.00108256
Iteration 14/1000 | Loss: 0.00049967
Iteration 15/1000 | Loss: 0.00036554
Iteration 16/1000 | Loss: 0.00026913
Iteration 17/1000 | Loss: 0.00088329
Iteration 18/1000 | Loss: 0.00061650
Iteration 19/1000 | Loss: 0.00016760
Iteration 20/1000 | Loss: 0.00055460
Iteration 21/1000 | Loss: 0.00080771
Iteration 22/1000 | Loss: 0.00058423
Iteration 23/1000 | Loss: 0.00065610
Iteration 24/1000 | Loss: 0.00062667
Iteration 25/1000 | Loss: 0.00066956
Iteration 26/1000 | Loss: 0.00058806
Iteration 27/1000 | Loss: 0.00075711
Iteration 28/1000 | Loss: 0.00068100
Iteration 29/1000 | Loss: 0.00081535
Iteration 30/1000 | Loss: 0.00013603
Iteration 31/1000 | Loss: 0.00037846
Iteration 32/1000 | Loss: 0.00014320
Iteration 33/1000 | Loss: 0.00012768
Iteration 34/1000 | Loss: 0.00039758
Iteration 35/1000 | Loss: 0.00039007
Iteration 36/1000 | Loss: 0.00049340
Iteration 37/1000 | Loss: 0.00059546
Iteration 38/1000 | Loss: 0.00131852
Iteration 39/1000 | Loss: 0.00078892
Iteration 40/1000 | Loss: 0.00086244
Iteration 41/1000 | Loss: 0.00064373
Iteration 42/1000 | Loss: 0.00080995
Iteration 43/1000 | Loss: 0.00068382
Iteration 44/1000 | Loss: 0.00083066
Iteration 45/1000 | Loss: 0.00050164
Iteration 46/1000 | Loss: 0.00052182
Iteration 47/1000 | Loss: 0.00085251
Iteration 48/1000 | Loss: 0.00069912
Iteration 49/1000 | Loss: 0.00055262
Iteration 50/1000 | Loss: 0.00051748
Iteration 51/1000 | Loss: 0.00038041
Iteration 52/1000 | Loss: 0.00034580
Iteration 53/1000 | Loss: 0.00012160
Iteration 54/1000 | Loss: 0.00016009
Iteration 55/1000 | Loss: 0.00011971
Iteration 56/1000 | Loss: 0.00011452
Iteration 57/1000 | Loss: 0.00038031
Iteration 58/1000 | Loss: 0.00052814
Iteration 59/1000 | Loss: 0.00056596
Iteration 60/1000 | Loss: 0.00054363
Iteration 61/1000 | Loss: 0.00071678
Iteration 62/1000 | Loss: 0.00013654
Iteration 63/1000 | Loss: 0.00104702
Iteration 64/1000 | Loss: 0.00011559
Iteration 65/1000 | Loss: 0.00011016
Iteration 66/1000 | Loss: 0.00010718
Iteration 67/1000 | Loss: 0.00010587
Iteration 68/1000 | Loss: 0.00010532
Iteration 69/1000 | Loss: 0.00036682
Iteration 70/1000 | Loss: 0.00010672
Iteration 71/1000 | Loss: 0.00010460
Iteration 72/1000 | Loss: 0.00010330
Iteration 73/1000 | Loss: 0.00010252
Iteration 74/1000 | Loss: 0.00010183
Iteration 75/1000 | Loss: 0.00010158
Iteration 76/1000 | Loss: 0.00010147
Iteration 77/1000 | Loss: 0.00010133
Iteration 78/1000 | Loss: 0.00010133
Iteration 79/1000 | Loss: 0.00010130
Iteration 80/1000 | Loss: 0.00010129
Iteration 81/1000 | Loss: 0.00010128
Iteration 82/1000 | Loss: 0.00010128
Iteration 83/1000 | Loss: 0.00010128
Iteration 84/1000 | Loss: 0.00010126
Iteration 85/1000 | Loss: 0.00010125
Iteration 86/1000 | Loss: 0.00010125
Iteration 87/1000 | Loss: 0.00010125
Iteration 88/1000 | Loss: 0.00010125
Iteration 89/1000 | Loss: 0.00010124
Iteration 90/1000 | Loss: 0.00010124
Iteration 91/1000 | Loss: 0.00010123
Iteration 92/1000 | Loss: 0.00010121
Iteration 93/1000 | Loss: 0.00010121
Iteration 94/1000 | Loss: 0.00010121
Iteration 95/1000 | Loss: 0.00010121
Iteration 96/1000 | Loss: 0.00010121
Iteration 97/1000 | Loss: 0.00010121
Iteration 98/1000 | Loss: 0.00010121
Iteration 99/1000 | Loss: 0.00010121
Iteration 100/1000 | Loss: 0.00010120
Iteration 101/1000 | Loss: 0.00010120
Iteration 102/1000 | Loss: 0.00010120
Iteration 103/1000 | Loss: 0.00010120
Iteration 104/1000 | Loss: 0.00010119
Iteration 105/1000 | Loss: 0.00010119
Iteration 106/1000 | Loss: 0.00010119
Iteration 107/1000 | Loss: 0.00010118
Iteration 108/1000 | Loss: 0.00010118
Iteration 109/1000 | Loss: 0.00010118
Iteration 110/1000 | Loss: 0.00010118
Iteration 111/1000 | Loss: 0.00010118
Iteration 112/1000 | Loss: 0.00010117
Iteration 113/1000 | Loss: 0.00010117
Iteration 114/1000 | Loss: 0.00010116
Iteration 115/1000 | Loss: 0.00010116
Iteration 116/1000 | Loss: 0.00010116
Iteration 117/1000 | Loss: 0.00010116
Iteration 118/1000 | Loss: 0.00010115
Iteration 119/1000 | Loss: 0.00010115
Iteration 120/1000 | Loss: 0.00010115
Iteration 121/1000 | Loss: 0.00010115
Iteration 122/1000 | Loss: 0.00010115
Iteration 123/1000 | Loss: 0.00010114
Iteration 124/1000 | Loss: 0.00010114
Iteration 125/1000 | Loss: 0.00010114
Iteration 126/1000 | Loss: 0.00010114
Iteration 127/1000 | Loss: 0.00010114
Iteration 128/1000 | Loss: 0.00010113
Iteration 129/1000 | Loss: 0.00010113
Iteration 130/1000 | Loss: 0.00010113
Iteration 131/1000 | Loss: 0.00010113
Iteration 132/1000 | Loss: 0.00010113
Iteration 133/1000 | Loss: 0.00010113
Iteration 134/1000 | Loss: 0.00010113
Iteration 135/1000 | Loss: 0.00010113
Iteration 136/1000 | Loss: 0.00010112
Iteration 137/1000 | Loss: 0.00010112
Iteration 138/1000 | Loss: 0.00010112
Iteration 139/1000 | Loss: 0.00010112
Iteration 140/1000 | Loss: 0.00010112
Iteration 141/1000 | Loss: 0.00010112
Iteration 142/1000 | Loss: 0.00010112
Iteration 143/1000 | Loss: 0.00010111
Iteration 144/1000 | Loss: 0.00010111
Iteration 145/1000 | Loss: 0.00010111
Iteration 146/1000 | Loss: 0.00010111
Iteration 147/1000 | Loss: 0.00010111
Iteration 148/1000 | Loss: 0.00010110
Iteration 149/1000 | Loss: 0.00010110
Iteration 150/1000 | Loss: 0.00010110
Iteration 151/1000 | Loss: 0.00010110
Iteration 152/1000 | Loss: 0.00010110
Iteration 153/1000 | Loss: 0.00010110
Iteration 154/1000 | Loss: 0.00010110
Iteration 155/1000 | Loss: 0.00010110
Iteration 156/1000 | Loss: 0.00010110
Iteration 157/1000 | Loss: 0.00010110
Iteration 158/1000 | Loss: 0.00010109
Iteration 159/1000 | Loss: 0.00010109
Iteration 160/1000 | Loss: 0.00010109
Iteration 161/1000 | Loss: 0.00010109
Iteration 162/1000 | Loss: 0.00010109
Iteration 163/1000 | Loss: 0.00010109
Iteration 164/1000 | Loss: 0.00010109
Iteration 165/1000 | Loss: 0.00010109
Iteration 166/1000 | Loss: 0.00010108
Iteration 167/1000 | Loss: 0.00010108
Iteration 168/1000 | Loss: 0.00010108
Iteration 169/1000 | Loss: 0.00010108
Iteration 170/1000 | Loss: 0.00010108
Iteration 171/1000 | Loss: 0.00010108
Iteration 172/1000 | Loss: 0.00010107
Iteration 173/1000 | Loss: 0.00010107
Iteration 174/1000 | Loss: 0.00010107
Iteration 175/1000 | Loss: 0.00010107
Iteration 176/1000 | Loss: 0.00010107
Iteration 177/1000 | Loss: 0.00010107
Iteration 178/1000 | Loss: 0.00010107
Iteration 179/1000 | Loss: 0.00010107
Iteration 180/1000 | Loss: 0.00010107
Iteration 181/1000 | Loss: 0.00010107
Iteration 182/1000 | Loss: 0.00010107
Iteration 183/1000 | Loss: 0.00010107
Iteration 184/1000 | Loss: 0.00010107
Iteration 185/1000 | Loss: 0.00010106
Iteration 186/1000 | Loss: 0.00010106
Iteration 187/1000 | Loss: 0.00010106
Iteration 188/1000 | Loss: 0.00010106
Iteration 189/1000 | Loss: 0.00010106
Iteration 190/1000 | Loss: 0.00010106
Iteration 191/1000 | Loss: 0.00010106
Iteration 192/1000 | Loss: 0.00010106
Iteration 193/1000 | Loss: 0.00010106
Iteration 194/1000 | Loss: 0.00010106
Iteration 195/1000 | Loss: 0.00010106
Iteration 196/1000 | Loss: 0.00010106
Iteration 197/1000 | Loss: 0.00010106
Iteration 198/1000 | Loss: 0.00010106
Iteration 199/1000 | Loss: 0.00010106
Iteration 200/1000 | Loss: 0.00010106
Iteration 201/1000 | Loss: 0.00010106
Iteration 202/1000 | Loss: 0.00010106
Iteration 203/1000 | Loss: 0.00010106
Iteration 204/1000 | Loss: 0.00010106
Iteration 205/1000 | Loss: 0.00010106
Iteration 206/1000 | Loss: 0.00010106
Iteration 207/1000 | Loss: 0.00010106
Iteration 208/1000 | Loss: 0.00010106
Iteration 209/1000 | Loss: 0.00010106
Iteration 210/1000 | Loss: 0.00010106
Iteration 211/1000 | Loss: 0.00010106
Iteration 212/1000 | Loss: 0.00010106
Iteration 213/1000 | Loss: 0.00010106
Iteration 214/1000 | Loss: 0.00010106
Iteration 215/1000 | Loss: 0.00010106
Iteration 216/1000 | Loss: 0.00010106
Iteration 217/1000 | Loss: 0.00010106
Iteration 218/1000 | Loss: 0.00010106
Iteration 219/1000 | Loss: 0.00010106
Iteration 220/1000 | Loss: 0.00010106
Iteration 221/1000 | Loss: 0.00010106
Iteration 222/1000 | Loss: 0.00010106
Iteration 223/1000 | Loss: 0.00010106
Iteration 224/1000 | Loss: 0.00010106
Iteration 225/1000 | Loss: 0.00010106
Iteration 226/1000 | Loss: 0.00010106
Iteration 227/1000 | Loss: 0.00010106
Iteration 228/1000 | Loss: 0.00010106
Iteration 229/1000 | Loss: 0.00010106
Iteration 230/1000 | Loss: 0.00010106
Iteration 231/1000 | Loss: 0.00010106
Iteration 232/1000 | Loss: 0.00010106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [0.00010105600085807964, 0.00010105600085807964, 0.00010105600085807964, 0.00010105600085807964, 0.00010105600085807964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010105600085807964

Optimization complete. Final v2v error: 4.826638221740723 mm

Highest mean error: 12.937599182128906 mm for frame 131

Lowest mean error: 3.3619651794433594 mm for frame 227

Saving results

Total time: 186.2340748310089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959938
Iteration 2/25 | Loss: 0.00128549
Iteration 3/25 | Loss: 0.00115834
Iteration 4/25 | Loss: 0.00114513
Iteration 5/25 | Loss: 0.00114175
Iteration 6/25 | Loss: 0.00114084
Iteration 7/25 | Loss: 0.00114076
Iteration 8/25 | Loss: 0.00114076
Iteration 9/25 | Loss: 0.00114076
Iteration 10/25 | Loss: 0.00114076
Iteration 11/25 | Loss: 0.00114076
Iteration 12/25 | Loss: 0.00114076
Iteration 13/25 | Loss: 0.00114076
Iteration 14/25 | Loss: 0.00114076
Iteration 15/25 | Loss: 0.00114076
Iteration 16/25 | Loss: 0.00114076
Iteration 17/25 | Loss: 0.00114076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001140758628025651, 0.001140758628025651, 0.001140758628025651, 0.001140758628025651, 0.001140758628025651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001140758628025651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96653199
Iteration 2/25 | Loss: 0.00044279
Iteration 3/25 | Loss: 0.00044278
Iteration 4/25 | Loss: 0.00044278
Iteration 5/25 | Loss: 0.00044278
Iteration 6/25 | Loss: 0.00044278
Iteration 7/25 | Loss: 0.00044278
Iteration 8/25 | Loss: 0.00044278
Iteration 9/25 | Loss: 0.00044278
Iteration 10/25 | Loss: 0.00044278
Iteration 11/25 | Loss: 0.00044278
Iteration 12/25 | Loss: 0.00044278
Iteration 13/25 | Loss: 0.00044278
Iteration 14/25 | Loss: 0.00044278
Iteration 15/25 | Loss: 0.00044278
Iteration 16/25 | Loss: 0.00044278
Iteration 17/25 | Loss: 0.00044278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004427789826877415, 0.0004427789826877415, 0.0004427789826877415, 0.0004427789826877415, 0.0004427789826877415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004427789826877415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044278
Iteration 2/1000 | Loss: 0.00002981
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001872
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001640
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001584
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001577
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001576
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001573
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001572
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001569
Iteration 38/1000 | Loss: 0.00001568
Iteration 39/1000 | Loss: 0.00001568
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001567
Iteration 42/1000 | Loss: 0.00001567
Iteration 43/1000 | Loss: 0.00001566
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001558
Iteration 52/1000 | Loss: 0.00001558
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001554
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001553
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001553
Iteration 76/1000 | Loss: 0.00001553
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001550
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001547
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001546
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001545
Iteration 113/1000 | Loss: 0.00001545
Iteration 114/1000 | Loss: 0.00001545
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001544
Iteration 120/1000 | Loss: 0.00001544
Iteration 121/1000 | Loss: 0.00001544
Iteration 122/1000 | Loss: 0.00001544
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001543
Iteration 128/1000 | Loss: 0.00001543
Iteration 129/1000 | Loss: 0.00001543
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001543
Iteration 132/1000 | Loss: 0.00001543
Iteration 133/1000 | Loss: 0.00001543
Iteration 134/1000 | Loss: 0.00001543
Iteration 135/1000 | Loss: 0.00001543
Iteration 136/1000 | Loss: 0.00001543
Iteration 137/1000 | Loss: 0.00001543
Iteration 138/1000 | Loss: 0.00001543
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001542
Iteration 143/1000 | Loss: 0.00001542
Iteration 144/1000 | Loss: 0.00001542
Iteration 145/1000 | Loss: 0.00001542
Iteration 146/1000 | Loss: 0.00001542
Iteration 147/1000 | Loss: 0.00001542
Iteration 148/1000 | Loss: 0.00001542
Iteration 149/1000 | Loss: 0.00001542
Iteration 150/1000 | Loss: 0.00001542
Iteration 151/1000 | Loss: 0.00001541
Iteration 152/1000 | Loss: 0.00001541
Iteration 153/1000 | Loss: 0.00001541
Iteration 154/1000 | Loss: 0.00001541
Iteration 155/1000 | Loss: 0.00001541
Iteration 156/1000 | Loss: 0.00001541
Iteration 157/1000 | Loss: 0.00001541
Iteration 158/1000 | Loss: 0.00001541
Iteration 159/1000 | Loss: 0.00001541
Iteration 160/1000 | Loss: 0.00001541
Iteration 161/1000 | Loss: 0.00001541
Iteration 162/1000 | Loss: 0.00001541
Iteration 163/1000 | Loss: 0.00001541
Iteration 164/1000 | Loss: 0.00001541
Iteration 165/1000 | Loss: 0.00001541
Iteration 166/1000 | Loss: 0.00001541
Iteration 167/1000 | Loss: 0.00001541
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001540
Iteration 178/1000 | Loss: 0.00001540
Iteration 179/1000 | Loss: 0.00001540
Iteration 180/1000 | Loss: 0.00001540
Iteration 181/1000 | Loss: 0.00001540
Iteration 182/1000 | Loss: 0.00001540
Iteration 183/1000 | Loss: 0.00001540
Iteration 184/1000 | Loss: 0.00001540
Iteration 185/1000 | Loss: 0.00001540
Iteration 186/1000 | Loss: 0.00001540
Iteration 187/1000 | Loss: 0.00001540
Iteration 188/1000 | Loss: 0.00001540
Iteration 189/1000 | Loss: 0.00001540
Iteration 190/1000 | Loss: 0.00001540
Iteration 191/1000 | Loss: 0.00001540
Iteration 192/1000 | Loss: 0.00001539
Iteration 193/1000 | Loss: 0.00001539
Iteration 194/1000 | Loss: 0.00001539
Iteration 195/1000 | Loss: 0.00001539
Iteration 196/1000 | Loss: 0.00001539
Iteration 197/1000 | Loss: 0.00001539
Iteration 198/1000 | Loss: 0.00001539
Iteration 199/1000 | Loss: 0.00001539
Iteration 200/1000 | Loss: 0.00001539
Iteration 201/1000 | Loss: 0.00001539
Iteration 202/1000 | Loss: 0.00001539
Iteration 203/1000 | Loss: 0.00001539
Iteration 204/1000 | Loss: 0.00001539
Iteration 205/1000 | Loss: 0.00001539
Iteration 206/1000 | Loss: 0.00001538
Iteration 207/1000 | Loss: 0.00001538
Iteration 208/1000 | Loss: 0.00001538
Iteration 209/1000 | Loss: 0.00001538
Iteration 210/1000 | Loss: 0.00001538
Iteration 211/1000 | Loss: 0.00001538
Iteration 212/1000 | Loss: 0.00001538
Iteration 213/1000 | Loss: 0.00001538
Iteration 214/1000 | Loss: 0.00001538
Iteration 215/1000 | Loss: 0.00001538
Iteration 216/1000 | Loss: 0.00001538
Iteration 217/1000 | Loss: 0.00001538
Iteration 218/1000 | Loss: 0.00001538
Iteration 219/1000 | Loss: 0.00001538
Iteration 220/1000 | Loss: 0.00001538
Iteration 221/1000 | Loss: 0.00001537
Iteration 222/1000 | Loss: 0.00001537
Iteration 223/1000 | Loss: 0.00001537
Iteration 224/1000 | Loss: 0.00001537
Iteration 225/1000 | Loss: 0.00001537
Iteration 226/1000 | Loss: 0.00001537
Iteration 227/1000 | Loss: 0.00001537
Iteration 228/1000 | Loss: 0.00001537
Iteration 229/1000 | Loss: 0.00001537
Iteration 230/1000 | Loss: 0.00001537
Iteration 231/1000 | Loss: 0.00001537
Iteration 232/1000 | Loss: 0.00001537
Iteration 233/1000 | Loss: 0.00001537
Iteration 234/1000 | Loss: 0.00001537
Iteration 235/1000 | Loss: 0.00001537
Iteration 236/1000 | Loss: 0.00001537
Iteration 237/1000 | Loss: 0.00001537
Iteration 238/1000 | Loss: 0.00001537
Iteration 239/1000 | Loss: 0.00001537
Iteration 240/1000 | Loss: 0.00001537
Iteration 241/1000 | Loss: 0.00001537
Iteration 242/1000 | Loss: 0.00001537
Iteration 243/1000 | Loss: 0.00001537
Iteration 244/1000 | Loss: 0.00001537
Iteration 245/1000 | Loss: 0.00001537
Iteration 246/1000 | Loss: 0.00001537
Iteration 247/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.5367017113021575e-05, 1.5367017113021575e-05, 1.5367017113021575e-05, 1.5367017113021575e-05, 1.5367017113021575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5367017113021575e-05

Optimization complete. Final v2v error: 3.315903663635254 mm

Highest mean error: 3.647428274154663 mm for frame 132

Lowest mean error: 3.124202251434326 mm for frame 124

Saving results

Total time: 36.23840403556824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924189
Iteration 2/25 | Loss: 0.00359742
Iteration 3/25 | Loss: 0.00237257
Iteration 4/25 | Loss: 0.00176420
Iteration 5/25 | Loss: 0.00161696
Iteration 6/25 | Loss: 0.00156484
Iteration 7/25 | Loss: 0.00156011
Iteration 8/25 | Loss: 0.00151481
Iteration 9/25 | Loss: 0.00140525
Iteration 10/25 | Loss: 0.00139267
Iteration 11/25 | Loss: 0.00137156
Iteration 12/25 | Loss: 0.00136262
Iteration 13/25 | Loss: 0.00135814
Iteration 14/25 | Loss: 0.00135418
Iteration 15/25 | Loss: 0.00136611
Iteration 16/25 | Loss: 0.00135569
Iteration 17/25 | Loss: 0.00136408
Iteration 18/25 | Loss: 0.00135192
Iteration 19/25 | Loss: 0.00138020
Iteration 20/25 | Loss: 0.00134615
Iteration 21/25 | Loss: 0.00134769
Iteration 22/25 | Loss: 0.00136424
Iteration 23/25 | Loss: 0.00134277
Iteration 24/25 | Loss: 0.00136423
Iteration 25/25 | Loss: 0.00135996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.40687561
Iteration 2/25 | Loss: 0.00586943
Iteration 3/25 | Loss: 0.00559313
Iteration 4/25 | Loss: 0.00538441
Iteration 5/25 | Loss: 0.00326939
Iteration 6/25 | Loss: 0.00326774
Iteration 7/25 | Loss: 0.00326774
Iteration 8/25 | Loss: 0.00326774
Iteration 9/25 | Loss: 0.00326774
Iteration 10/25 | Loss: 0.00326774
Iteration 11/25 | Loss: 0.00326774
Iteration 12/25 | Loss: 0.00326774
Iteration 13/25 | Loss: 0.00326774
Iteration 14/25 | Loss: 0.00326774
Iteration 15/25 | Loss: 0.00326774
Iteration 16/25 | Loss: 0.00326774
Iteration 17/25 | Loss: 0.00326774
Iteration 18/25 | Loss: 0.00326774
Iteration 19/25 | Loss: 0.00326774
Iteration 20/25 | Loss: 0.00326774
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003267739899456501, 0.003267739899456501, 0.003267739899456501, 0.003267739899456501, 0.003267739899456501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003267739899456501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00326774
Iteration 2/1000 | Loss: 0.00289763
Iteration 3/1000 | Loss: 0.00085100
Iteration 4/1000 | Loss: 0.00192998
Iteration 5/1000 | Loss: 0.00169506
Iteration 6/1000 | Loss: 0.00373658
Iteration 7/1000 | Loss: 0.00459160
Iteration 8/1000 | Loss: 0.00080553
Iteration 9/1000 | Loss: 0.00171317
Iteration 10/1000 | Loss: 0.00185052
Iteration 11/1000 | Loss: 0.00304384
Iteration 12/1000 | Loss: 0.00265174
Iteration 13/1000 | Loss: 0.00390700
Iteration 14/1000 | Loss: 0.00425541
Iteration 15/1000 | Loss: 0.00372182
Iteration 16/1000 | Loss: 0.00035306
Iteration 17/1000 | Loss: 0.00057811
Iteration 18/1000 | Loss: 0.00032416
Iteration 19/1000 | Loss: 0.00113040
Iteration 20/1000 | Loss: 0.00037671
Iteration 21/1000 | Loss: 0.00047559
Iteration 22/1000 | Loss: 0.00041646
Iteration 23/1000 | Loss: 0.00101853
Iteration 24/1000 | Loss: 0.00285733
Iteration 25/1000 | Loss: 0.00127431
Iteration 26/1000 | Loss: 0.00432737
Iteration 27/1000 | Loss: 0.00626402
Iteration 28/1000 | Loss: 0.00427893
Iteration 29/1000 | Loss: 0.00602434
Iteration 30/1000 | Loss: 0.00487543
Iteration 31/1000 | Loss: 0.00514357
Iteration 32/1000 | Loss: 0.00312184
Iteration 33/1000 | Loss: 0.00505749
Iteration 34/1000 | Loss: 0.00207945
Iteration 35/1000 | Loss: 0.00322413
Iteration 36/1000 | Loss: 0.00143681
Iteration 37/1000 | Loss: 0.00201816
Iteration 38/1000 | Loss: 0.00152398
Iteration 39/1000 | Loss: 0.00156114
Iteration 40/1000 | Loss: 0.00041285
Iteration 41/1000 | Loss: 0.00018032
Iteration 42/1000 | Loss: 0.00013966
Iteration 43/1000 | Loss: 0.00010662
Iteration 44/1000 | Loss: 0.00083451
Iteration 45/1000 | Loss: 0.00010155
Iteration 46/1000 | Loss: 0.00030088
Iteration 47/1000 | Loss: 0.00008943
Iteration 48/1000 | Loss: 0.00008118
Iteration 49/1000 | Loss: 0.00028779
Iteration 50/1000 | Loss: 0.00045025
Iteration 51/1000 | Loss: 0.00034767
Iteration 52/1000 | Loss: 0.00166135
Iteration 53/1000 | Loss: 0.00072360
Iteration 54/1000 | Loss: 0.00159975
Iteration 55/1000 | Loss: 0.00209120
Iteration 56/1000 | Loss: 0.00065936
Iteration 57/1000 | Loss: 0.00054512
Iteration 58/1000 | Loss: 0.00034584
Iteration 59/1000 | Loss: 0.00006669
Iteration 60/1000 | Loss: 0.00008783
Iteration 61/1000 | Loss: 0.00005183
Iteration 62/1000 | Loss: 0.00004724
Iteration 63/1000 | Loss: 0.00004416
Iteration 64/1000 | Loss: 0.00004163
Iteration 65/1000 | Loss: 0.00065237
Iteration 66/1000 | Loss: 0.00062341
Iteration 67/1000 | Loss: 0.00063832
Iteration 68/1000 | Loss: 0.00023955
Iteration 69/1000 | Loss: 0.00042035
Iteration 70/1000 | Loss: 0.00016378
Iteration 71/1000 | Loss: 0.00004824
Iteration 72/1000 | Loss: 0.00021974
Iteration 73/1000 | Loss: 0.00004268
Iteration 74/1000 | Loss: 0.00016878
Iteration 75/1000 | Loss: 0.00004074
Iteration 76/1000 | Loss: 0.00013278
Iteration 77/1000 | Loss: 0.00003794
Iteration 78/1000 | Loss: 0.00040013
Iteration 79/1000 | Loss: 0.00017662
Iteration 80/1000 | Loss: 0.00030759
Iteration 81/1000 | Loss: 0.00071002
Iteration 82/1000 | Loss: 0.00027919
Iteration 83/1000 | Loss: 0.00003769
Iteration 84/1000 | Loss: 0.00003561
Iteration 85/1000 | Loss: 0.00003451
Iteration 86/1000 | Loss: 0.00003366
Iteration 87/1000 | Loss: 0.00003306
Iteration 88/1000 | Loss: 0.00003262
Iteration 89/1000 | Loss: 0.00077530
Iteration 90/1000 | Loss: 0.00023510
Iteration 91/1000 | Loss: 0.00004948
Iteration 92/1000 | Loss: 0.00035363
Iteration 93/1000 | Loss: 0.00039162
Iteration 94/1000 | Loss: 0.00004498
Iteration 95/1000 | Loss: 0.00009505
Iteration 96/1000 | Loss: 0.00005770
Iteration 97/1000 | Loss: 0.00003525
Iteration 98/1000 | Loss: 0.00083894
Iteration 99/1000 | Loss: 0.00017737
Iteration 100/1000 | Loss: 0.00007603
Iteration 101/1000 | Loss: 0.00004157
Iteration 102/1000 | Loss: 0.00007424
Iteration 103/1000 | Loss: 0.00003420
Iteration 104/1000 | Loss: 0.00140493
Iteration 105/1000 | Loss: 0.00072350
Iteration 106/1000 | Loss: 0.00084802
Iteration 107/1000 | Loss: 0.00045065
Iteration 108/1000 | Loss: 0.00017947
Iteration 109/1000 | Loss: 0.00018961
Iteration 110/1000 | Loss: 0.00009379
Iteration 111/1000 | Loss: 0.00079667
Iteration 112/1000 | Loss: 0.00051976
Iteration 113/1000 | Loss: 0.00085124
Iteration 114/1000 | Loss: 0.00117741
Iteration 115/1000 | Loss: 0.00020677
Iteration 116/1000 | Loss: 0.00005368
Iteration 117/1000 | Loss: 0.00016262
Iteration 118/1000 | Loss: 0.00010649
Iteration 119/1000 | Loss: 0.00004029
Iteration 120/1000 | Loss: 0.00018934
Iteration 121/1000 | Loss: 0.00014407
Iteration 122/1000 | Loss: 0.00010569
Iteration 123/1000 | Loss: 0.00071818
Iteration 124/1000 | Loss: 0.00016471
Iteration 125/1000 | Loss: 0.00003646
Iteration 126/1000 | Loss: 0.00003405
Iteration 127/1000 | Loss: 0.00006189
Iteration 128/1000 | Loss: 0.00005407
Iteration 129/1000 | Loss: 0.00005241
Iteration 130/1000 | Loss: 0.00004204
Iteration 131/1000 | Loss: 0.00005597
Iteration 132/1000 | Loss: 0.00008336
Iteration 133/1000 | Loss: 0.00007660
Iteration 134/1000 | Loss: 0.00004523
Iteration 135/1000 | Loss: 0.00005911
Iteration 136/1000 | Loss: 0.00004101
Iteration 137/1000 | Loss: 0.00002900
Iteration 138/1000 | Loss: 0.00002765
Iteration 139/1000 | Loss: 0.00002688
Iteration 140/1000 | Loss: 0.00002656
Iteration 141/1000 | Loss: 0.00008494
Iteration 142/1000 | Loss: 0.00005241
Iteration 143/1000 | Loss: 0.00007950
Iteration 144/1000 | Loss: 0.00002994
Iteration 145/1000 | Loss: 0.00004172
Iteration 146/1000 | Loss: 0.00079649
Iteration 147/1000 | Loss: 0.00006705
Iteration 148/1000 | Loss: 0.00019595
Iteration 149/1000 | Loss: 0.00017516
Iteration 150/1000 | Loss: 0.00033278
Iteration 151/1000 | Loss: 0.00004221
Iteration 152/1000 | Loss: 0.00008329
Iteration 153/1000 | Loss: 0.00004357
Iteration 154/1000 | Loss: 0.00003210
Iteration 155/1000 | Loss: 0.00028956
Iteration 156/1000 | Loss: 0.00005537
Iteration 157/1000 | Loss: 0.00003532
Iteration 158/1000 | Loss: 0.00003801
Iteration 159/1000 | Loss: 0.00004502
Iteration 160/1000 | Loss: 0.00002788
Iteration 161/1000 | Loss: 0.00002750
Iteration 162/1000 | Loss: 0.00002688
Iteration 163/1000 | Loss: 0.00002650
Iteration 164/1000 | Loss: 0.00002629
Iteration 165/1000 | Loss: 0.00070767
Iteration 166/1000 | Loss: 0.00048512
Iteration 167/1000 | Loss: 0.00005731
Iteration 168/1000 | Loss: 0.00002692
Iteration 169/1000 | Loss: 0.00072465
Iteration 170/1000 | Loss: 0.00021700
Iteration 171/1000 | Loss: 0.00003571
Iteration 172/1000 | Loss: 0.00002682
Iteration 173/1000 | Loss: 0.00002597
Iteration 174/1000 | Loss: 0.00002585
Iteration 175/1000 | Loss: 0.00002573
Iteration 176/1000 | Loss: 0.00002588
Iteration 177/1000 | Loss: 0.00002572
Iteration 178/1000 | Loss: 0.00002558
Iteration 179/1000 | Loss: 0.00002558
Iteration 180/1000 | Loss: 0.00071230
Iteration 181/1000 | Loss: 0.00054688
Iteration 182/1000 | Loss: 0.00069328
Iteration 183/1000 | Loss: 0.00056606
Iteration 184/1000 | Loss: 0.00064561
Iteration 185/1000 | Loss: 0.00028339
Iteration 186/1000 | Loss: 0.00016441
Iteration 187/1000 | Loss: 0.00041040
Iteration 188/1000 | Loss: 0.00029699
Iteration 189/1000 | Loss: 0.00017062
Iteration 190/1000 | Loss: 0.00018000
Iteration 191/1000 | Loss: 0.00049397
Iteration 192/1000 | Loss: 0.00026271
Iteration 193/1000 | Loss: 0.00011603
Iteration 194/1000 | Loss: 0.00036391
Iteration 195/1000 | Loss: 0.00108370
Iteration 196/1000 | Loss: 0.00132549
Iteration 197/1000 | Loss: 0.00044281
Iteration 198/1000 | Loss: 0.00090184
Iteration 199/1000 | Loss: 0.00018014
Iteration 200/1000 | Loss: 0.00013909
Iteration 201/1000 | Loss: 0.00010028
Iteration 202/1000 | Loss: 0.00003229
Iteration 203/1000 | Loss: 0.00002841
Iteration 204/1000 | Loss: 0.00002642
Iteration 205/1000 | Loss: 0.00002503
Iteration 206/1000 | Loss: 0.00002401
Iteration 207/1000 | Loss: 0.00002344
Iteration 208/1000 | Loss: 0.00002308
Iteration 209/1000 | Loss: 0.00002281
Iteration 210/1000 | Loss: 0.00002269
Iteration 211/1000 | Loss: 0.00002262
Iteration 212/1000 | Loss: 0.00002262
Iteration 213/1000 | Loss: 0.00002259
Iteration 214/1000 | Loss: 0.00002255
Iteration 215/1000 | Loss: 0.00002255
Iteration 216/1000 | Loss: 0.00002253
Iteration 217/1000 | Loss: 0.00002253
Iteration 218/1000 | Loss: 0.00002252
Iteration 219/1000 | Loss: 0.00002252
Iteration 220/1000 | Loss: 0.00002252
Iteration 221/1000 | Loss: 0.00002252
Iteration 222/1000 | Loss: 0.00002252
Iteration 223/1000 | Loss: 0.00002252
Iteration 224/1000 | Loss: 0.00002252
Iteration 225/1000 | Loss: 0.00002252
Iteration 226/1000 | Loss: 0.00002252
Iteration 227/1000 | Loss: 0.00002252
Iteration 228/1000 | Loss: 0.00002252
Iteration 229/1000 | Loss: 0.00002251
Iteration 230/1000 | Loss: 0.00002251
Iteration 231/1000 | Loss: 0.00002251
Iteration 232/1000 | Loss: 0.00002251
Iteration 233/1000 | Loss: 0.00002251
Iteration 234/1000 | Loss: 0.00002251
Iteration 235/1000 | Loss: 0.00002251
Iteration 236/1000 | Loss: 0.00002251
Iteration 237/1000 | Loss: 0.00002251
Iteration 238/1000 | Loss: 0.00002251
Iteration 239/1000 | Loss: 0.00002251
Iteration 240/1000 | Loss: 0.00002250
Iteration 241/1000 | Loss: 0.00002250
Iteration 242/1000 | Loss: 0.00002250
Iteration 243/1000 | Loss: 0.00002250
Iteration 244/1000 | Loss: 0.00002250
Iteration 245/1000 | Loss: 0.00002250
Iteration 246/1000 | Loss: 0.00002249
Iteration 247/1000 | Loss: 0.00002249
Iteration 248/1000 | Loss: 0.00002249
Iteration 249/1000 | Loss: 0.00002249
Iteration 250/1000 | Loss: 0.00002249
Iteration 251/1000 | Loss: 0.00002249
Iteration 252/1000 | Loss: 0.00002249
Iteration 253/1000 | Loss: 0.00002249
Iteration 254/1000 | Loss: 0.00002249
Iteration 255/1000 | Loss: 0.00002249
Iteration 256/1000 | Loss: 0.00002249
Iteration 257/1000 | Loss: 0.00002249
Iteration 258/1000 | Loss: 0.00002249
Iteration 259/1000 | Loss: 0.00002249
Iteration 260/1000 | Loss: 0.00002249
Iteration 261/1000 | Loss: 0.00002249
Iteration 262/1000 | Loss: 0.00002248
Iteration 263/1000 | Loss: 0.00002248
Iteration 264/1000 | Loss: 0.00002248
Iteration 265/1000 | Loss: 0.00002248
Iteration 266/1000 | Loss: 0.00002248
Iteration 267/1000 | Loss: 0.00002248
Iteration 268/1000 | Loss: 0.00002248
Iteration 269/1000 | Loss: 0.00002248
Iteration 270/1000 | Loss: 0.00002248
Iteration 271/1000 | Loss: 0.00002248
Iteration 272/1000 | Loss: 0.00002248
Iteration 273/1000 | Loss: 0.00002248
Iteration 274/1000 | Loss: 0.00002248
Iteration 275/1000 | Loss: 0.00002248
Iteration 276/1000 | Loss: 0.00002248
Iteration 277/1000 | Loss: 0.00002248
Iteration 278/1000 | Loss: 0.00002247
Iteration 279/1000 | Loss: 0.00002247
Iteration 280/1000 | Loss: 0.00002247
Iteration 281/1000 | Loss: 0.00002247
Iteration 282/1000 | Loss: 0.00002247
Iteration 283/1000 | Loss: 0.00002247
Iteration 284/1000 | Loss: 0.00002247
Iteration 285/1000 | Loss: 0.00002247
Iteration 286/1000 | Loss: 0.00002247
Iteration 287/1000 | Loss: 0.00002247
Iteration 288/1000 | Loss: 0.00002247
Iteration 289/1000 | Loss: 0.00002247
Iteration 290/1000 | Loss: 0.00002247
Iteration 291/1000 | Loss: 0.00002247
Iteration 292/1000 | Loss: 0.00002247
Iteration 293/1000 | Loss: 0.00002246
Iteration 294/1000 | Loss: 0.00002246
Iteration 295/1000 | Loss: 0.00002246
Iteration 296/1000 | Loss: 0.00002246
Iteration 297/1000 | Loss: 0.00002246
Iteration 298/1000 | Loss: 0.00002246
Iteration 299/1000 | Loss: 0.00002246
Iteration 300/1000 | Loss: 0.00002246
Iteration 301/1000 | Loss: 0.00002246
Iteration 302/1000 | Loss: 0.00002246
Iteration 303/1000 | Loss: 0.00002246
Iteration 304/1000 | Loss: 0.00002246
Iteration 305/1000 | Loss: 0.00002246
Iteration 306/1000 | Loss: 0.00002245
Iteration 307/1000 | Loss: 0.00002245
Iteration 308/1000 | Loss: 0.00002245
Iteration 309/1000 | Loss: 0.00002245
Iteration 310/1000 | Loss: 0.00002245
Iteration 311/1000 | Loss: 0.00002245
Iteration 312/1000 | Loss: 0.00002245
Iteration 313/1000 | Loss: 0.00002245
Iteration 314/1000 | Loss: 0.00002245
Iteration 315/1000 | Loss: 0.00071806
Iteration 316/1000 | Loss: 0.00055811
Iteration 317/1000 | Loss: 0.00005791
Iteration 318/1000 | Loss: 0.00002378
Iteration 319/1000 | Loss: 0.00002280
Iteration 320/1000 | Loss: 0.00002255
Iteration 321/1000 | Loss: 0.00002255
Iteration 322/1000 | Loss: 0.00002255
Iteration 323/1000 | Loss: 0.00002254
Iteration 324/1000 | Loss: 0.00002253
Iteration 325/1000 | Loss: 0.00002253
Iteration 326/1000 | Loss: 0.00002252
Iteration 327/1000 | Loss: 0.00002252
Iteration 328/1000 | Loss: 0.00002252
Iteration 329/1000 | Loss: 0.00002251
Iteration 330/1000 | Loss: 0.00002251
Iteration 331/1000 | Loss: 0.00002251
Iteration 332/1000 | Loss: 0.00002250
Iteration 333/1000 | Loss: 0.00002250
Iteration 334/1000 | Loss: 0.00002249
Iteration 335/1000 | Loss: 0.00002249
Iteration 336/1000 | Loss: 0.00002248
Iteration 337/1000 | Loss: 0.00002248
Iteration 338/1000 | Loss: 0.00002248
Iteration 339/1000 | Loss: 0.00002248
Iteration 340/1000 | Loss: 0.00002248
Iteration 341/1000 | Loss: 0.00002247
Iteration 342/1000 | Loss: 0.00002247
Iteration 343/1000 | Loss: 0.00002247
Iteration 344/1000 | Loss: 0.00002247
Iteration 345/1000 | Loss: 0.00002247
Iteration 346/1000 | Loss: 0.00002247
Iteration 347/1000 | Loss: 0.00002247
Iteration 348/1000 | Loss: 0.00002247
Iteration 349/1000 | Loss: 0.00002247
Iteration 350/1000 | Loss: 0.00002247
Iteration 351/1000 | Loss: 0.00002247
Iteration 352/1000 | Loss: 0.00002246
Iteration 353/1000 | Loss: 0.00002245
Iteration 354/1000 | Loss: 0.00002245
Iteration 355/1000 | Loss: 0.00002245
Iteration 356/1000 | Loss: 0.00002245
Iteration 357/1000 | Loss: 0.00002245
Iteration 358/1000 | Loss: 0.00002245
Iteration 359/1000 | Loss: 0.00002245
Iteration 360/1000 | Loss: 0.00002245
Iteration 361/1000 | Loss: 0.00002245
Iteration 362/1000 | Loss: 0.00002244
Iteration 363/1000 | Loss: 0.00002244
Iteration 364/1000 | Loss: 0.00002244
Iteration 365/1000 | Loss: 0.00002244
Iteration 366/1000 | Loss: 0.00002244
Iteration 367/1000 | Loss: 0.00002244
Iteration 368/1000 | Loss: 0.00002243
Iteration 369/1000 | Loss: 0.00002243
Iteration 370/1000 | Loss: 0.00002243
Iteration 371/1000 | Loss: 0.00002243
Iteration 372/1000 | Loss: 0.00002243
Iteration 373/1000 | Loss: 0.00002243
Iteration 374/1000 | Loss: 0.00002243
Iteration 375/1000 | Loss: 0.00002243
Iteration 376/1000 | Loss: 0.00002243
Iteration 377/1000 | Loss: 0.00002242
Iteration 378/1000 | Loss: 0.00002242
Iteration 379/1000 | Loss: 0.00002242
Iteration 380/1000 | Loss: 0.00002242
Iteration 381/1000 | Loss: 0.00002242
Iteration 382/1000 | Loss: 0.00002242
Iteration 383/1000 | Loss: 0.00002242
Iteration 384/1000 | Loss: 0.00002242
Iteration 385/1000 | Loss: 0.00002242
Iteration 386/1000 | Loss: 0.00002242
Iteration 387/1000 | Loss: 0.00002241
Iteration 388/1000 | Loss: 0.00002241
Iteration 389/1000 | Loss: 0.00002241
Iteration 390/1000 | Loss: 0.00002241
Iteration 391/1000 | Loss: 0.00002241
Iteration 392/1000 | Loss: 0.00002240
Iteration 393/1000 | Loss: 0.00002240
Iteration 394/1000 | Loss: 0.00002240
Iteration 395/1000 | Loss: 0.00002240
Iteration 396/1000 | Loss: 0.00002240
Iteration 397/1000 | Loss: 0.00002239
Iteration 398/1000 | Loss: 0.00002239
Iteration 399/1000 | Loss: 0.00002239
Iteration 400/1000 | Loss: 0.00002239
Iteration 401/1000 | Loss: 0.00002239
Iteration 402/1000 | Loss: 0.00002239
Iteration 403/1000 | Loss: 0.00002239
Iteration 404/1000 | Loss: 0.00002239
Iteration 405/1000 | Loss: 0.00002238
Iteration 406/1000 | Loss: 0.00002238
Iteration 407/1000 | Loss: 0.00002238
Iteration 408/1000 | Loss: 0.00002238
Iteration 409/1000 | Loss: 0.00002238
Iteration 410/1000 | Loss: 0.00002238
Iteration 411/1000 | Loss: 0.00002238
Iteration 412/1000 | Loss: 0.00002238
Iteration 413/1000 | Loss: 0.00002238
Iteration 414/1000 | Loss: 0.00002238
Iteration 415/1000 | Loss: 0.00002238
Iteration 416/1000 | Loss: 0.00002238
Iteration 417/1000 | Loss: 0.00002238
Iteration 418/1000 | Loss: 0.00002238
Iteration 419/1000 | Loss: 0.00002238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 419. Stopping optimization.
Last 5 losses: [2.23782717512222e-05, 2.23782717512222e-05, 2.23782717512222e-05, 2.23782717512222e-05, 2.23782717512222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.23782717512222e-05

Optimization complete. Final v2v error: 3.599276304244995 mm

Highest mean error: 13.583070755004883 mm for frame 107

Lowest mean error: 2.677220344543457 mm for frame 209

Saving results

Total time: 411.2180697917938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596246
Iteration 2/25 | Loss: 0.00136190
Iteration 3/25 | Loss: 0.00121928
Iteration 4/25 | Loss: 0.00119684
Iteration 5/25 | Loss: 0.00119072
Iteration 6/25 | Loss: 0.00118955
Iteration 7/25 | Loss: 0.00118955
Iteration 8/25 | Loss: 0.00118955
Iteration 9/25 | Loss: 0.00118955
Iteration 10/25 | Loss: 0.00118955
Iteration 11/25 | Loss: 0.00118955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001189547125250101, 0.001189547125250101, 0.001189547125250101, 0.001189547125250101, 0.001189547125250101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001189547125250101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34900153
Iteration 2/25 | Loss: 0.00047929
Iteration 3/25 | Loss: 0.00047929
Iteration 4/25 | Loss: 0.00047929
Iteration 5/25 | Loss: 0.00047929
Iteration 6/25 | Loss: 0.00047929
Iteration 7/25 | Loss: 0.00047929
Iteration 8/25 | Loss: 0.00047929
Iteration 9/25 | Loss: 0.00047929
Iteration 10/25 | Loss: 0.00047929
Iteration 11/25 | Loss: 0.00047929
Iteration 12/25 | Loss: 0.00047929
Iteration 13/25 | Loss: 0.00047929
Iteration 14/25 | Loss: 0.00047929
Iteration 15/25 | Loss: 0.00047929
Iteration 16/25 | Loss: 0.00047929
Iteration 17/25 | Loss: 0.00047929
Iteration 18/25 | Loss: 0.00047929
Iteration 19/25 | Loss: 0.00047929
Iteration 20/25 | Loss: 0.00047929
Iteration 21/25 | Loss: 0.00047929
Iteration 22/25 | Loss: 0.00047929
Iteration 23/25 | Loss: 0.00047929
Iteration 24/25 | Loss: 0.00047929
Iteration 25/25 | Loss: 0.00047929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047929
Iteration 2/1000 | Loss: 0.00004792
Iteration 3/1000 | Loss: 0.00002892
Iteration 4/1000 | Loss: 0.00002508
Iteration 5/1000 | Loss: 0.00002364
Iteration 6/1000 | Loss: 0.00002282
Iteration 7/1000 | Loss: 0.00002221
Iteration 8/1000 | Loss: 0.00002180
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00002147
Iteration 11/1000 | Loss: 0.00002140
Iteration 12/1000 | Loss: 0.00002136
Iteration 13/1000 | Loss: 0.00002135
Iteration 14/1000 | Loss: 0.00002135
Iteration 15/1000 | Loss: 0.00002134
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002118
Iteration 18/1000 | Loss: 0.00002116
Iteration 19/1000 | Loss: 0.00002113
Iteration 20/1000 | Loss: 0.00002113
Iteration 21/1000 | Loss: 0.00002112
Iteration 22/1000 | Loss: 0.00002112
Iteration 23/1000 | Loss: 0.00002109
Iteration 24/1000 | Loss: 0.00002106
Iteration 25/1000 | Loss: 0.00002103
Iteration 26/1000 | Loss: 0.00002102
Iteration 27/1000 | Loss: 0.00002102
Iteration 28/1000 | Loss: 0.00002101
Iteration 29/1000 | Loss: 0.00002100
Iteration 30/1000 | Loss: 0.00002100
Iteration 31/1000 | Loss: 0.00002098
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002095
Iteration 34/1000 | Loss: 0.00002095
Iteration 35/1000 | Loss: 0.00002094
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00002092
Iteration 38/1000 | Loss: 0.00002092
Iteration 39/1000 | Loss: 0.00002091
Iteration 40/1000 | Loss: 0.00002090
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002087
Iteration 45/1000 | Loss: 0.00002087
Iteration 46/1000 | Loss: 0.00002087
Iteration 47/1000 | Loss: 0.00002086
Iteration 48/1000 | Loss: 0.00002086
Iteration 49/1000 | Loss: 0.00002085
Iteration 50/1000 | Loss: 0.00002085
Iteration 51/1000 | Loss: 0.00002085
Iteration 52/1000 | Loss: 0.00002084
Iteration 53/1000 | Loss: 0.00002084
Iteration 54/1000 | Loss: 0.00002084
Iteration 55/1000 | Loss: 0.00002084
Iteration 56/1000 | Loss: 0.00002084
Iteration 57/1000 | Loss: 0.00002084
Iteration 58/1000 | Loss: 0.00002084
Iteration 59/1000 | Loss: 0.00002084
Iteration 60/1000 | Loss: 0.00002084
Iteration 61/1000 | Loss: 0.00002084
Iteration 62/1000 | Loss: 0.00002084
Iteration 63/1000 | Loss: 0.00002084
Iteration 64/1000 | Loss: 0.00002084
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00002084
Iteration 67/1000 | Loss: 0.00002084
Iteration 68/1000 | Loss: 0.00002084
Iteration 69/1000 | Loss: 0.00002084
Iteration 70/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.083577601297293e-05, 2.083577601297293e-05, 2.083577601297293e-05, 2.083577601297293e-05, 2.083577601297293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.083577601297293e-05

Optimization complete. Final v2v error: 3.7713913917541504 mm

Highest mean error: 4.564418315887451 mm for frame 143

Lowest mean error: 3.195301055908203 mm for frame 98

Saving results

Total time: 35.23986887931824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932786
Iteration 2/25 | Loss: 0.00129637
Iteration 3/25 | Loss: 0.00118857
Iteration 4/25 | Loss: 0.00117398
Iteration 5/25 | Loss: 0.00117021
Iteration 6/25 | Loss: 0.00116878
Iteration 7/25 | Loss: 0.00116878
Iteration 8/25 | Loss: 0.00116878
Iteration 9/25 | Loss: 0.00116878
Iteration 10/25 | Loss: 0.00116878
Iteration 11/25 | Loss: 0.00116878
Iteration 12/25 | Loss: 0.00116878
Iteration 13/25 | Loss: 0.00116878
Iteration 14/25 | Loss: 0.00116878
Iteration 15/25 | Loss: 0.00116878
Iteration 16/25 | Loss: 0.00116878
Iteration 17/25 | Loss: 0.00116878
Iteration 18/25 | Loss: 0.00116878
Iteration 19/25 | Loss: 0.00116878
Iteration 20/25 | Loss: 0.00116878
Iteration 21/25 | Loss: 0.00116878
Iteration 22/25 | Loss: 0.00116878
Iteration 23/25 | Loss: 0.00116878
Iteration 24/25 | Loss: 0.00116878
Iteration 25/25 | Loss: 0.00116878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40084922
Iteration 2/25 | Loss: 0.00050266
Iteration 3/25 | Loss: 0.00050257
Iteration 4/25 | Loss: 0.00050257
Iteration 5/25 | Loss: 0.00050257
Iteration 6/25 | Loss: 0.00050257
Iteration 7/25 | Loss: 0.00050257
Iteration 8/25 | Loss: 0.00050257
Iteration 9/25 | Loss: 0.00050257
Iteration 10/25 | Loss: 0.00050257
Iteration 11/25 | Loss: 0.00050257
Iteration 12/25 | Loss: 0.00050257
Iteration 13/25 | Loss: 0.00050257
Iteration 14/25 | Loss: 0.00050257
Iteration 15/25 | Loss: 0.00050257
Iteration 16/25 | Loss: 0.00050257
Iteration 17/25 | Loss: 0.00050257
Iteration 18/25 | Loss: 0.00050257
Iteration 19/25 | Loss: 0.00050257
Iteration 20/25 | Loss: 0.00050257
Iteration 21/25 | Loss: 0.00050257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005025701830163598, 0.0005025701830163598, 0.0005025701830163598, 0.0005025701830163598, 0.0005025701830163598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005025701830163598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050257
Iteration 2/1000 | Loss: 0.00002671
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001865
Iteration 5/1000 | Loss: 0.00001772
Iteration 6/1000 | Loss: 0.00001716
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001678
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001648
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001641
Iteration 15/1000 | Loss: 0.00001637
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001632
Iteration 19/1000 | Loss: 0.00001630
Iteration 20/1000 | Loss: 0.00001629
Iteration 21/1000 | Loss: 0.00001628
Iteration 22/1000 | Loss: 0.00001628
Iteration 23/1000 | Loss: 0.00001627
Iteration 24/1000 | Loss: 0.00001627
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001617
Iteration 35/1000 | Loss: 0.00001617
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001613
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001611
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001610
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001609
Iteration 53/1000 | Loss: 0.00001609
Iteration 54/1000 | Loss: 0.00001609
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001608
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001607
Iteration 65/1000 | Loss: 0.00001607
Iteration 66/1000 | Loss: 0.00001607
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001604
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001602
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00001602
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001597
Iteration 97/1000 | Loss: 0.00001597
Iteration 98/1000 | Loss: 0.00001597
Iteration 99/1000 | Loss: 0.00001596
Iteration 100/1000 | Loss: 0.00001596
Iteration 101/1000 | Loss: 0.00001596
Iteration 102/1000 | Loss: 0.00001596
Iteration 103/1000 | Loss: 0.00001596
Iteration 104/1000 | Loss: 0.00001596
Iteration 105/1000 | Loss: 0.00001596
Iteration 106/1000 | Loss: 0.00001596
Iteration 107/1000 | Loss: 0.00001596
Iteration 108/1000 | Loss: 0.00001596
Iteration 109/1000 | Loss: 0.00001596
Iteration 110/1000 | Loss: 0.00001595
Iteration 111/1000 | Loss: 0.00001595
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001594
Iteration 119/1000 | Loss: 0.00001594
Iteration 120/1000 | Loss: 0.00001594
Iteration 121/1000 | Loss: 0.00001593
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001592
Iteration 124/1000 | Loss: 0.00001592
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001590
Iteration 128/1000 | Loss: 0.00001590
Iteration 129/1000 | Loss: 0.00001590
Iteration 130/1000 | Loss: 0.00001590
Iteration 131/1000 | Loss: 0.00001590
Iteration 132/1000 | Loss: 0.00001590
Iteration 133/1000 | Loss: 0.00001590
Iteration 134/1000 | Loss: 0.00001590
Iteration 135/1000 | Loss: 0.00001590
Iteration 136/1000 | Loss: 0.00001590
Iteration 137/1000 | Loss: 0.00001590
Iteration 138/1000 | Loss: 0.00001590
Iteration 139/1000 | Loss: 0.00001590
Iteration 140/1000 | Loss: 0.00001590
Iteration 141/1000 | Loss: 0.00001590
Iteration 142/1000 | Loss: 0.00001590
Iteration 143/1000 | Loss: 0.00001590
Iteration 144/1000 | Loss: 0.00001590
Iteration 145/1000 | Loss: 0.00001590
Iteration 146/1000 | Loss: 0.00001590
Iteration 147/1000 | Loss: 0.00001590
Iteration 148/1000 | Loss: 0.00001590
Iteration 149/1000 | Loss: 0.00001590
Iteration 150/1000 | Loss: 0.00001590
Iteration 151/1000 | Loss: 0.00001590
Iteration 152/1000 | Loss: 0.00001590
Iteration 153/1000 | Loss: 0.00001590
Iteration 154/1000 | Loss: 0.00001590
Iteration 155/1000 | Loss: 0.00001590
Iteration 156/1000 | Loss: 0.00001590
Iteration 157/1000 | Loss: 0.00001590
Iteration 158/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.5900763173704036e-05, 1.5900763173704036e-05, 1.5900763173704036e-05, 1.5900763173704036e-05, 1.5900763173704036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5900763173704036e-05

Optimization complete. Final v2v error: 3.3892557621002197 mm

Highest mean error: 3.5723812580108643 mm for frame 99

Lowest mean error: 3.1860275268554688 mm for frame 13

Saving results

Total time: 41.1932258605957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046327
Iteration 2/25 | Loss: 0.00200574
Iteration 3/25 | Loss: 0.00144224
Iteration 4/25 | Loss: 0.00141513
Iteration 5/25 | Loss: 0.00140515
Iteration 6/25 | Loss: 0.00140180
Iteration 7/25 | Loss: 0.00140144
Iteration 8/25 | Loss: 0.00140144
Iteration 9/25 | Loss: 0.00140144
Iteration 10/25 | Loss: 0.00140144
Iteration 11/25 | Loss: 0.00140144
Iteration 12/25 | Loss: 0.00140144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014014432672411203, 0.0014014432672411203, 0.0014014432672411203, 0.0014014432672411203, 0.0014014432672411203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014014432672411203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81657118
Iteration 2/25 | Loss: 0.00069025
Iteration 3/25 | Loss: 0.00069024
Iteration 4/25 | Loss: 0.00069024
Iteration 5/25 | Loss: 0.00069024
Iteration 6/25 | Loss: 0.00069024
Iteration 7/25 | Loss: 0.00069024
Iteration 8/25 | Loss: 0.00069024
Iteration 9/25 | Loss: 0.00069024
Iteration 10/25 | Loss: 0.00069024
Iteration 11/25 | Loss: 0.00069024
Iteration 12/25 | Loss: 0.00069024
Iteration 13/25 | Loss: 0.00069024
Iteration 14/25 | Loss: 0.00069024
Iteration 15/25 | Loss: 0.00069024
Iteration 16/25 | Loss: 0.00069024
Iteration 17/25 | Loss: 0.00069024
Iteration 18/25 | Loss: 0.00069024
Iteration 19/25 | Loss: 0.00069024
Iteration 20/25 | Loss: 0.00069024
Iteration 21/25 | Loss: 0.00069024
Iteration 22/25 | Loss: 0.00069024
Iteration 23/25 | Loss: 0.00069024
Iteration 24/25 | Loss: 0.00069024
Iteration 25/25 | Loss: 0.00069024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069024
Iteration 2/1000 | Loss: 0.00009860
Iteration 3/1000 | Loss: 0.00007054
Iteration 4/1000 | Loss: 0.00006504
Iteration 5/1000 | Loss: 0.00006255
Iteration 6/1000 | Loss: 0.00006068
Iteration 7/1000 | Loss: 0.00005937
Iteration 8/1000 | Loss: 0.00005824
Iteration 9/1000 | Loss: 0.00005746
Iteration 10/1000 | Loss: 0.00005686
Iteration 11/1000 | Loss: 0.00005628
Iteration 12/1000 | Loss: 0.00005581
Iteration 13/1000 | Loss: 0.00005539
Iteration 14/1000 | Loss: 0.00005503
Iteration 15/1000 | Loss: 0.00005459
Iteration 16/1000 | Loss: 0.00005425
Iteration 17/1000 | Loss: 0.00005397
Iteration 18/1000 | Loss: 0.00005371
Iteration 19/1000 | Loss: 0.00005342
Iteration 20/1000 | Loss: 0.00005320
Iteration 21/1000 | Loss: 0.00005302
Iteration 22/1000 | Loss: 0.00005293
Iteration 23/1000 | Loss: 0.00005284
Iteration 24/1000 | Loss: 0.00005282
Iteration 25/1000 | Loss: 0.00005282
Iteration 26/1000 | Loss: 0.00005281
Iteration 27/1000 | Loss: 0.00005280
Iteration 28/1000 | Loss: 0.00005280
Iteration 29/1000 | Loss: 0.00005280
Iteration 30/1000 | Loss: 0.00005279
Iteration 31/1000 | Loss: 0.00005279
Iteration 32/1000 | Loss: 0.00005279
Iteration 33/1000 | Loss: 0.00005278
Iteration 34/1000 | Loss: 0.00005278
Iteration 35/1000 | Loss: 0.00005276
Iteration 36/1000 | Loss: 0.00005276
Iteration 37/1000 | Loss: 0.00005273
Iteration 38/1000 | Loss: 0.00005271
Iteration 39/1000 | Loss: 0.00005271
Iteration 40/1000 | Loss: 0.00005270
Iteration 41/1000 | Loss: 0.00005269
Iteration 42/1000 | Loss: 0.00005268
Iteration 43/1000 | Loss: 0.00005267
Iteration 44/1000 | Loss: 0.00005267
Iteration 45/1000 | Loss: 0.00005266
Iteration 46/1000 | Loss: 0.00005266
Iteration 47/1000 | Loss: 0.00005266
Iteration 48/1000 | Loss: 0.00005264
Iteration 49/1000 | Loss: 0.00005263
Iteration 50/1000 | Loss: 0.00005263
Iteration 51/1000 | Loss: 0.00005263
Iteration 52/1000 | Loss: 0.00005263
Iteration 53/1000 | Loss: 0.00005263
Iteration 54/1000 | Loss: 0.00005263
Iteration 55/1000 | Loss: 0.00005263
Iteration 56/1000 | Loss: 0.00005263
Iteration 57/1000 | Loss: 0.00005263
Iteration 58/1000 | Loss: 0.00005263
Iteration 59/1000 | Loss: 0.00005263
Iteration 60/1000 | Loss: 0.00005263
Iteration 61/1000 | Loss: 0.00005262
Iteration 62/1000 | Loss: 0.00005262
Iteration 63/1000 | Loss: 0.00005262
Iteration 64/1000 | Loss: 0.00005262
Iteration 65/1000 | Loss: 0.00005262
Iteration 66/1000 | Loss: 0.00005261
Iteration 67/1000 | Loss: 0.00005261
Iteration 68/1000 | Loss: 0.00005261
Iteration 69/1000 | Loss: 0.00005261
Iteration 70/1000 | Loss: 0.00005261
Iteration 71/1000 | Loss: 0.00005261
Iteration 72/1000 | Loss: 0.00005261
Iteration 73/1000 | Loss: 0.00005261
Iteration 74/1000 | Loss: 0.00005261
Iteration 75/1000 | Loss: 0.00005261
Iteration 76/1000 | Loss: 0.00005261
Iteration 77/1000 | Loss: 0.00005261
Iteration 78/1000 | Loss: 0.00005261
Iteration 79/1000 | Loss: 0.00005261
Iteration 80/1000 | Loss: 0.00005261
Iteration 81/1000 | Loss: 0.00005261
Iteration 82/1000 | Loss: 0.00005261
Iteration 83/1000 | Loss: 0.00005260
Iteration 84/1000 | Loss: 0.00005260
Iteration 85/1000 | Loss: 0.00005260
Iteration 86/1000 | Loss: 0.00005260
Iteration 87/1000 | Loss: 0.00005260
Iteration 88/1000 | Loss: 0.00005259
Iteration 89/1000 | Loss: 0.00005259
Iteration 90/1000 | Loss: 0.00005259
Iteration 91/1000 | Loss: 0.00005259
Iteration 92/1000 | Loss: 0.00005259
Iteration 93/1000 | Loss: 0.00005259
Iteration 94/1000 | Loss: 0.00005259
Iteration 95/1000 | Loss: 0.00005258
Iteration 96/1000 | Loss: 0.00005258
Iteration 97/1000 | Loss: 0.00005258
Iteration 98/1000 | Loss: 0.00005258
Iteration 99/1000 | Loss: 0.00005258
Iteration 100/1000 | Loss: 0.00005258
Iteration 101/1000 | Loss: 0.00005257
Iteration 102/1000 | Loss: 0.00005257
Iteration 103/1000 | Loss: 0.00005257
Iteration 104/1000 | Loss: 0.00005257
Iteration 105/1000 | Loss: 0.00005257
Iteration 106/1000 | Loss: 0.00005257
Iteration 107/1000 | Loss: 0.00005257
Iteration 108/1000 | Loss: 0.00005257
Iteration 109/1000 | Loss: 0.00005256
Iteration 110/1000 | Loss: 0.00005256
Iteration 111/1000 | Loss: 0.00005256
Iteration 112/1000 | Loss: 0.00005256
Iteration 113/1000 | Loss: 0.00005256
Iteration 114/1000 | Loss: 0.00005256
Iteration 115/1000 | Loss: 0.00005256
Iteration 116/1000 | Loss: 0.00005256
Iteration 117/1000 | Loss: 0.00005256
Iteration 118/1000 | Loss: 0.00005256
Iteration 119/1000 | Loss: 0.00005256
Iteration 120/1000 | Loss: 0.00005256
Iteration 121/1000 | Loss: 0.00005255
Iteration 122/1000 | Loss: 0.00005255
Iteration 123/1000 | Loss: 0.00005255
Iteration 124/1000 | Loss: 0.00005255
Iteration 125/1000 | Loss: 0.00005255
Iteration 126/1000 | Loss: 0.00005255
Iteration 127/1000 | Loss: 0.00005255
Iteration 128/1000 | Loss: 0.00005255
Iteration 129/1000 | Loss: 0.00005255
Iteration 130/1000 | Loss: 0.00005255
Iteration 131/1000 | Loss: 0.00005255
Iteration 132/1000 | Loss: 0.00005255
Iteration 133/1000 | Loss: 0.00005255
Iteration 134/1000 | Loss: 0.00005255
Iteration 135/1000 | Loss: 0.00005255
Iteration 136/1000 | Loss: 0.00005255
Iteration 137/1000 | Loss: 0.00005255
Iteration 138/1000 | Loss: 0.00005254
Iteration 139/1000 | Loss: 0.00005254
Iteration 140/1000 | Loss: 0.00005254
Iteration 141/1000 | Loss: 0.00005254
Iteration 142/1000 | Loss: 0.00005254
Iteration 143/1000 | Loss: 0.00005254
Iteration 144/1000 | Loss: 0.00005254
Iteration 145/1000 | Loss: 0.00005254
Iteration 146/1000 | Loss: 0.00005254
Iteration 147/1000 | Loss: 0.00005254
Iteration 148/1000 | Loss: 0.00005254
Iteration 149/1000 | Loss: 0.00005254
Iteration 150/1000 | Loss: 0.00005254
Iteration 151/1000 | Loss: 0.00005254
Iteration 152/1000 | Loss: 0.00005254
Iteration 153/1000 | Loss: 0.00005254
Iteration 154/1000 | Loss: 0.00005254
Iteration 155/1000 | Loss: 0.00005254
Iteration 156/1000 | Loss: 0.00005254
Iteration 157/1000 | Loss: 0.00005254
Iteration 158/1000 | Loss: 0.00005254
Iteration 159/1000 | Loss: 0.00005254
Iteration 160/1000 | Loss: 0.00005254
Iteration 161/1000 | Loss: 0.00005254
Iteration 162/1000 | Loss: 0.00005254
Iteration 163/1000 | Loss: 0.00005254
Iteration 164/1000 | Loss: 0.00005254
Iteration 165/1000 | Loss: 0.00005254
Iteration 166/1000 | Loss: 0.00005254
Iteration 167/1000 | Loss: 0.00005254
Iteration 168/1000 | Loss: 0.00005254
Iteration 169/1000 | Loss: 0.00005254
Iteration 170/1000 | Loss: 0.00005254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [5.253885319689289e-05, 5.253885319689289e-05, 5.253885319689289e-05, 5.253885319689289e-05, 5.253885319689289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.253885319689289e-05

Optimization complete. Final v2v error: 5.707395076751709 mm

Highest mean error: 6.928943157196045 mm for frame 99

Lowest mean error: 4.326277256011963 mm for frame 32

Saving results

Total time: 55.21945667266846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00644856
Iteration 2/25 | Loss: 0.00140740
Iteration 3/25 | Loss: 0.00127392
Iteration 4/25 | Loss: 0.00124561
Iteration 5/25 | Loss: 0.00123987
Iteration 6/25 | Loss: 0.00123801
Iteration 7/25 | Loss: 0.00123801
Iteration 8/25 | Loss: 0.00123801
Iteration 9/25 | Loss: 0.00123801
Iteration 10/25 | Loss: 0.00123801
Iteration 11/25 | Loss: 0.00123801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012380086118355393, 0.0012380086118355393, 0.0012380086118355393, 0.0012380086118355393, 0.0012380086118355393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012380086118355393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60713243
Iteration 2/25 | Loss: 0.00041443
Iteration 3/25 | Loss: 0.00041442
Iteration 4/25 | Loss: 0.00041442
Iteration 5/25 | Loss: 0.00041442
Iteration 6/25 | Loss: 0.00041442
Iteration 7/25 | Loss: 0.00041442
Iteration 8/25 | Loss: 0.00041442
Iteration 9/25 | Loss: 0.00041442
Iteration 10/25 | Loss: 0.00041442
Iteration 11/25 | Loss: 0.00041442
Iteration 12/25 | Loss: 0.00041442
Iteration 13/25 | Loss: 0.00041442
Iteration 14/25 | Loss: 0.00041442
Iteration 15/25 | Loss: 0.00041442
Iteration 16/25 | Loss: 0.00041442
Iteration 17/25 | Loss: 0.00041442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00041441619396209717, 0.00041441619396209717, 0.00041441619396209717, 0.00041441619396209717, 0.00041441619396209717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041441619396209717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041442
Iteration 2/1000 | Loss: 0.00005177
Iteration 3/1000 | Loss: 0.00002820
Iteration 4/1000 | Loss: 0.00002451
Iteration 5/1000 | Loss: 0.00002306
Iteration 6/1000 | Loss: 0.00002188
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002075
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00002035
Iteration 11/1000 | Loss: 0.00002009
Iteration 12/1000 | Loss: 0.00002000
Iteration 13/1000 | Loss: 0.00001993
Iteration 14/1000 | Loss: 0.00001989
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001977
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001975
Iteration 20/1000 | Loss: 0.00001973
Iteration 21/1000 | Loss: 0.00001972
Iteration 22/1000 | Loss: 0.00001972
Iteration 23/1000 | Loss: 0.00001971
Iteration 24/1000 | Loss: 0.00001971
Iteration 25/1000 | Loss: 0.00001968
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001967
Iteration 28/1000 | Loss: 0.00001967
Iteration 29/1000 | Loss: 0.00001967
Iteration 30/1000 | Loss: 0.00001967
Iteration 31/1000 | Loss: 0.00001967
Iteration 32/1000 | Loss: 0.00001967
Iteration 33/1000 | Loss: 0.00001967
Iteration 34/1000 | Loss: 0.00001967
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00001966
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001965
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001964
Iteration 46/1000 | Loss: 0.00001964
Iteration 47/1000 | Loss: 0.00001964
Iteration 48/1000 | Loss: 0.00001964
Iteration 49/1000 | Loss: 0.00001964
Iteration 50/1000 | Loss: 0.00001964
Iteration 51/1000 | Loss: 0.00001964
Iteration 52/1000 | Loss: 0.00001964
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001963
Iteration 57/1000 | Loss: 0.00001963
Iteration 58/1000 | Loss: 0.00001963
Iteration 59/1000 | Loss: 0.00001962
Iteration 60/1000 | Loss: 0.00001962
Iteration 61/1000 | Loss: 0.00001962
Iteration 62/1000 | Loss: 0.00001962
Iteration 63/1000 | Loss: 0.00001962
Iteration 64/1000 | Loss: 0.00001962
Iteration 65/1000 | Loss: 0.00001962
Iteration 66/1000 | Loss: 0.00001961
Iteration 67/1000 | Loss: 0.00001961
Iteration 68/1000 | Loss: 0.00001961
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001959
Iteration 72/1000 | Loss: 0.00001959
Iteration 73/1000 | Loss: 0.00001959
Iteration 74/1000 | Loss: 0.00001959
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001959
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001957
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00001954
Iteration 91/1000 | Loss: 0.00001954
Iteration 92/1000 | Loss: 0.00001953
Iteration 93/1000 | Loss: 0.00001953
Iteration 94/1000 | Loss: 0.00001953
Iteration 95/1000 | Loss: 0.00001953
Iteration 96/1000 | Loss: 0.00001953
Iteration 97/1000 | Loss: 0.00001953
Iteration 98/1000 | Loss: 0.00001953
Iteration 99/1000 | Loss: 0.00001953
Iteration 100/1000 | Loss: 0.00001953
Iteration 101/1000 | Loss: 0.00001953
Iteration 102/1000 | Loss: 0.00001952
Iteration 103/1000 | Loss: 0.00001952
Iteration 104/1000 | Loss: 0.00001952
Iteration 105/1000 | Loss: 0.00001952
Iteration 106/1000 | Loss: 0.00001951
Iteration 107/1000 | Loss: 0.00001951
Iteration 108/1000 | Loss: 0.00001951
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001950
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001950
Iteration 113/1000 | Loss: 0.00001950
Iteration 114/1000 | Loss: 0.00001950
Iteration 115/1000 | Loss: 0.00001950
Iteration 116/1000 | Loss: 0.00001950
Iteration 117/1000 | Loss: 0.00001950
Iteration 118/1000 | Loss: 0.00001950
Iteration 119/1000 | Loss: 0.00001950
Iteration 120/1000 | Loss: 0.00001950
Iteration 121/1000 | Loss: 0.00001949
Iteration 122/1000 | Loss: 0.00001949
Iteration 123/1000 | Loss: 0.00001949
Iteration 124/1000 | Loss: 0.00001949
Iteration 125/1000 | Loss: 0.00001949
Iteration 126/1000 | Loss: 0.00001949
Iteration 127/1000 | Loss: 0.00001949
Iteration 128/1000 | Loss: 0.00001949
Iteration 129/1000 | Loss: 0.00001949
Iteration 130/1000 | Loss: 0.00001949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.9494680600473657e-05, 1.9494680600473657e-05, 1.9494680600473657e-05, 1.9494680600473657e-05, 1.9494680600473657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9494680600473657e-05

Optimization complete. Final v2v error: 3.841294765472412 mm

Highest mean error: 4.099714279174805 mm for frame 178

Lowest mean error: 3.4833085536956787 mm for frame 138

Saving results

Total time: 40.48016691207886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589994
Iteration 2/25 | Loss: 0.00141079
Iteration 3/25 | Loss: 0.00119861
Iteration 4/25 | Loss: 0.00116020
Iteration 5/25 | Loss: 0.00112487
Iteration 6/25 | Loss: 0.00111308
Iteration 7/25 | Loss: 0.00109827
Iteration 8/25 | Loss: 0.00109281
Iteration 9/25 | Loss: 0.00109453
Iteration 10/25 | Loss: 0.00109071
Iteration 11/25 | Loss: 0.00108973
Iteration 12/25 | Loss: 0.00108947
Iteration 13/25 | Loss: 0.00108934
Iteration 14/25 | Loss: 0.00108928
Iteration 15/25 | Loss: 0.00108928
Iteration 16/25 | Loss: 0.00108928
Iteration 17/25 | Loss: 0.00108928
Iteration 18/25 | Loss: 0.00108928
Iteration 19/25 | Loss: 0.00108928
Iteration 20/25 | Loss: 0.00108928
Iteration 21/25 | Loss: 0.00108928
Iteration 22/25 | Loss: 0.00108927
Iteration 23/25 | Loss: 0.00108927
Iteration 24/25 | Loss: 0.00108927
Iteration 25/25 | Loss: 0.00108927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67819786
Iteration 2/25 | Loss: 0.00036897
Iteration 3/25 | Loss: 0.00036897
Iteration 4/25 | Loss: 0.00036897
Iteration 5/25 | Loss: 0.00036897
Iteration 6/25 | Loss: 0.00036897
Iteration 7/25 | Loss: 0.00036897
Iteration 8/25 | Loss: 0.00036897
Iteration 9/25 | Loss: 0.00036897
Iteration 10/25 | Loss: 0.00036897
Iteration 11/25 | Loss: 0.00036897
Iteration 12/25 | Loss: 0.00036897
Iteration 13/25 | Loss: 0.00036897
Iteration 14/25 | Loss: 0.00036897
Iteration 15/25 | Loss: 0.00036897
Iteration 16/25 | Loss: 0.00036897
Iteration 17/25 | Loss: 0.00036897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000368966517271474, 0.000368966517271474, 0.000368966517271474, 0.000368966517271474, 0.000368966517271474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000368966517271474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036897
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00003760
Iteration 4/1000 | Loss: 0.00045242
Iteration 5/1000 | Loss: 0.00022772
Iteration 6/1000 | Loss: 0.00028650
Iteration 7/1000 | Loss: 0.00002542
Iteration 8/1000 | Loss: 0.00001906
Iteration 9/1000 | Loss: 0.00001750
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00002671
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001616
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001615
Iteration 16/1000 | Loss: 0.00001609
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001594
Iteration 20/1000 | Loss: 0.00001585
Iteration 21/1000 | Loss: 0.00002459
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001556
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001555
Iteration 27/1000 | Loss: 0.00001554
Iteration 28/1000 | Loss: 0.00001554
Iteration 29/1000 | Loss: 0.00001553
Iteration 30/1000 | Loss: 0.00001553
Iteration 31/1000 | Loss: 0.00001553
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001552
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001551
Iteration 37/1000 | Loss: 0.00001551
Iteration 38/1000 | Loss: 0.00001550
Iteration 39/1000 | Loss: 0.00001550
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001546
Iteration 42/1000 | Loss: 0.00001546
Iteration 43/1000 | Loss: 0.00001546
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001545
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001542
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001541
Iteration 51/1000 | Loss: 0.00001541
Iteration 52/1000 | Loss: 0.00001540
Iteration 53/1000 | Loss: 0.00001540
Iteration 54/1000 | Loss: 0.00001540
Iteration 55/1000 | Loss: 0.00001540
Iteration 56/1000 | Loss: 0.00001539
Iteration 57/1000 | Loss: 0.00001539
Iteration 58/1000 | Loss: 0.00001537
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001533
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001529
Iteration 69/1000 | Loss: 0.00001529
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001526
Iteration 75/1000 | Loss: 0.00001526
Iteration 76/1000 | Loss: 0.00001526
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001520
Iteration 86/1000 | Loss: 0.00001519
Iteration 87/1000 | Loss: 0.00001519
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001518
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001516
Iteration 92/1000 | Loss: 0.00001516
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001515
Iteration 95/1000 | Loss: 0.00001515
Iteration 96/1000 | Loss: 0.00001513
Iteration 97/1000 | Loss: 0.00001513
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001511
Iteration 102/1000 | Loss: 0.00001511
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001511
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001510
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001507
Iteration 111/1000 | Loss: 0.00001507
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001504
Iteration 121/1000 | Loss: 0.00002267
Iteration 122/1000 | Loss: 0.00002350
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001541
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001506
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001505
Iteration 130/1000 | Loss: 0.00001503
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001501
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001498
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001496
Iteration 138/1000 | Loss: 0.00001496
Iteration 139/1000 | Loss: 0.00001496
Iteration 140/1000 | Loss: 0.00001496
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001494
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001494
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001493
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001493
Iteration 151/1000 | Loss: 0.00001493
Iteration 152/1000 | Loss: 0.00001493
Iteration 153/1000 | Loss: 0.00001493
Iteration 154/1000 | Loss: 0.00001493
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001492
Iteration 158/1000 | Loss: 0.00001492
Iteration 159/1000 | Loss: 0.00001492
Iteration 160/1000 | Loss: 0.00001492
Iteration 161/1000 | Loss: 0.00001492
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001492
Iteration 164/1000 | Loss: 0.00001492
Iteration 165/1000 | Loss: 0.00001492
Iteration 166/1000 | Loss: 0.00001492
Iteration 167/1000 | Loss: 0.00001492
Iteration 168/1000 | Loss: 0.00001492
Iteration 169/1000 | Loss: 0.00001492
Iteration 170/1000 | Loss: 0.00001492
Iteration 171/1000 | Loss: 0.00001492
Iteration 172/1000 | Loss: 0.00001492
Iteration 173/1000 | Loss: 0.00001491
Iteration 174/1000 | Loss: 0.00001491
Iteration 175/1000 | Loss: 0.00001491
Iteration 176/1000 | Loss: 0.00001491
Iteration 177/1000 | Loss: 0.00001491
Iteration 178/1000 | Loss: 0.00001491
Iteration 179/1000 | Loss: 0.00001491
Iteration 180/1000 | Loss: 0.00001491
Iteration 181/1000 | Loss: 0.00001491
Iteration 182/1000 | Loss: 0.00001491
Iteration 183/1000 | Loss: 0.00001491
Iteration 184/1000 | Loss: 0.00001491
Iteration 185/1000 | Loss: 0.00001491
Iteration 186/1000 | Loss: 0.00001491
Iteration 187/1000 | Loss: 0.00001491
Iteration 188/1000 | Loss: 0.00001491
Iteration 189/1000 | Loss: 0.00001491
Iteration 190/1000 | Loss: 0.00001490
Iteration 191/1000 | Loss: 0.00001490
Iteration 192/1000 | Loss: 0.00001490
Iteration 193/1000 | Loss: 0.00001490
Iteration 194/1000 | Loss: 0.00001490
Iteration 195/1000 | Loss: 0.00001490
Iteration 196/1000 | Loss: 0.00001490
Iteration 197/1000 | Loss: 0.00001490
Iteration 198/1000 | Loss: 0.00001490
Iteration 199/1000 | Loss: 0.00001490
Iteration 200/1000 | Loss: 0.00001490
Iteration 201/1000 | Loss: 0.00001490
Iteration 202/1000 | Loss: 0.00001490
Iteration 203/1000 | Loss: 0.00001490
Iteration 204/1000 | Loss: 0.00001490
Iteration 205/1000 | Loss: 0.00001489
Iteration 206/1000 | Loss: 0.00001489
Iteration 207/1000 | Loss: 0.00001489
Iteration 208/1000 | Loss: 0.00001489
Iteration 209/1000 | Loss: 0.00001489
Iteration 210/1000 | Loss: 0.00001489
Iteration 211/1000 | Loss: 0.00001489
Iteration 212/1000 | Loss: 0.00001489
Iteration 213/1000 | Loss: 0.00001489
Iteration 214/1000 | Loss: 0.00001489
Iteration 215/1000 | Loss: 0.00001489
Iteration 216/1000 | Loss: 0.00001489
Iteration 217/1000 | Loss: 0.00001489
Iteration 218/1000 | Loss: 0.00001489
Iteration 219/1000 | Loss: 0.00001489
Iteration 220/1000 | Loss: 0.00001489
Iteration 221/1000 | Loss: 0.00001489
Iteration 222/1000 | Loss: 0.00001489
Iteration 223/1000 | Loss: 0.00001489
Iteration 224/1000 | Loss: 0.00001489
Iteration 225/1000 | Loss: 0.00001489
Iteration 226/1000 | Loss: 0.00001489
Iteration 227/1000 | Loss: 0.00001489
Iteration 228/1000 | Loss: 0.00001488
Iteration 229/1000 | Loss: 0.00001488
Iteration 230/1000 | Loss: 0.00001488
Iteration 231/1000 | Loss: 0.00001488
Iteration 232/1000 | Loss: 0.00001488
Iteration 233/1000 | Loss: 0.00001488
Iteration 234/1000 | Loss: 0.00001488
Iteration 235/1000 | Loss: 0.00001488
Iteration 236/1000 | Loss: 0.00001488
Iteration 237/1000 | Loss: 0.00001488
Iteration 238/1000 | Loss: 0.00001488
Iteration 239/1000 | Loss: 0.00001488
Iteration 240/1000 | Loss: 0.00001488
Iteration 241/1000 | Loss: 0.00001488
Iteration 242/1000 | Loss: 0.00001488
Iteration 243/1000 | Loss: 0.00001488
Iteration 244/1000 | Loss: 0.00001488
Iteration 245/1000 | Loss: 0.00001488
Iteration 246/1000 | Loss: 0.00001488
Iteration 247/1000 | Loss: 0.00001488
Iteration 248/1000 | Loss: 0.00001488
Iteration 249/1000 | Loss: 0.00001488
Iteration 250/1000 | Loss: 0.00001488
Iteration 251/1000 | Loss: 0.00001488
Iteration 252/1000 | Loss: 0.00001488
Iteration 253/1000 | Loss: 0.00001488
Iteration 254/1000 | Loss: 0.00001488
Iteration 255/1000 | Loss: 0.00001488
Iteration 256/1000 | Loss: 0.00001488
Iteration 257/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.4875335182296112e-05, 1.4875335182296112e-05, 1.4875335182296112e-05, 1.4875335182296112e-05, 1.4875335182296112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4875335182296112e-05

Optimization complete. Final v2v error: 3.2101058959960938 mm

Highest mean error: 9.220541954040527 mm for frame 239

Lowest mean error: 2.896569013595581 mm for frame 86

Saving results

Total time: 82.83037257194519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01143770
Iteration 2/25 | Loss: 0.00597142
Iteration 3/25 | Loss: 0.00513103
Iteration 4/25 | Loss: 0.00287003
Iteration 5/25 | Loss: 0.00223465
Iteration 6/25 | Loss: 0.00227547
Iteration 7/25 | Loss: 0.00241514
Iteration 8/25 | Loss: 0.00219948
Iteration 9/25 | Loss: 0.00197966
Iteration 10/25 | Loss: 0.00199754
Iteration 11/25 | Loss: 0.00190550
Iteration 12/25 | Loss: 0.00173109
Iteration 13/25 | Loss: 0.00157959
Iteration 14/25 | Loss: 0.00154341
Iteration 15/25 | Loss: 0.00150812
Iteration 16/25 | Loss: 0.00148536
Iteration 17/25 | Loss: 0.00148046
Iteration 18/25 | Loss: 0.00146465
Iteration 19/25 | Loss: 0.00147071
Iteration 20/25 | Loss: 0.00146180
Iteration 21/25 | Loss: 0.00145757
Iteration 22/25 | Loss: 0.00145523
Iteration 23/25 | Loss: 0.00145547
Iteration 24/25 | Loss: 0.00145247
Iteration 25/25 | Loss: 0.00145302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.46052986
Iteration 2/25 | Loss: 0.00084178
Iteration 3/25 | Loss: 0.00079773
Iteration 4/25 | Loss: 0.00079773
Iteration 5/25 | Loss: 0.00079772
Iteration 6/25 | Loss: 0.00079772
Iteration 7/25 | Loss: 0.00079772
Iteration 8/25 | Loss: 0.00079772
Iteration 9/25 | Loss: 0.00079772
Iteration 10/25 | Loss: 0.00079772
Iteration 11/25 | Loss: 0.00079772
Iteration 12/25 | Loss: 0.00079772
Iteration 13/25 | Loss: 0.00079772
Iteration 14/25 | Loss: 0.00079772
Iteration 15/25 | Loss: 0.00079772
Iteration 16/25 | Loss: 0.00079772
Iteration 17/25 | Loss: 0.00079772
Iteration 18/25 | Loss: 0.00079772
Iteration 19/25 | Loss: 0.00079772
Iteration 20/25 | Loss: 0.00079772
Iteration 21/25 | Loss: 0.00079772
Iteration 22/25 | Loss: 0.00079772
Iteration 23/25 | Loss: 0.00079772
Iteration 24/25 | Loss: 0.00079772
Iteration 25/25 | Loss: 0.00079772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079772
Iteration 2/1000 | Loss: 0.00036012
Iteration 3/1000 | Loss: 0.00013776
Iteration 4/1000 | Loss: 0.00012451
Iteration 5/1000 | Loss: 0.00011881
Iteration 6/1000 | Loss: 0.00010922
Iteration 7/1000 | Loss: 0.00009627
Iteration 8/1000 | Loss: 0.00212681
Iteration 9/1000 | Loss: 0.00367059
Iteration 10/1000 | Loss: 0.00026045
Iteration 11/1000 | Loss: 0.00035347
Iteration 12/1000 | Loss: 0.00041368
Iteration 13/1000 | Loss: 0.00035754
Iteration 14/1000 | Loss: 0.00035469
Iteration 15/1000 | Loss: 0.00037540
Iteration 16/1000 | Loss: 0.00030027
Iteration 17/1000 | Loss: 0.00016423
Iteration 18/1000 | Loss: 0.00021113
Iteration 19/1000 | Loss: 0.00020187
Iteration 20/1000 | Loss: 0.00087044
Iteration 21/1000 | Loss: 0.00064547
Iteration 22/1000 | Loss: 0.00006797
Iteration 23/1000 | Loss: 0.00006177
Iteration 24/1000 | Loss: 0.00006660
Iteration 25/1000 | Loss: 0.00005387
Iteration 26/1000 | Loss: 0.00006334
Iteration 27/1000 | Loss: 0.00005099
Iteration 28/1000 | Loss: 0.00005067
Iteration 29/1000 | Loss: 0.00004908
Iteration 30/1000 | Loss: 0.00004920
Iteration 31/1000 | Loss: 0.00004750
Iteration 32/1000 | Loss: 0.00006907
Iteration 33/1000 | Loss: 0.00005129
Iteration 34/1000 | Loss: 0.00004867
Iteration 35/1000 | Loss: 0.00004645
Iteration 36/1000 | Loss: 0.00005008
Iteration 37/1000 | Loss: 0.00004592
Iteration 38/1000 | Loss: 0.00004577
Iteration 39/1000 | Loss: 0.00004557
Iteration 40/1000 | Loss: 0.00004536
Iteration 41/1000 | Loss: 0.00004513
Iteration 42/1000 | Loss: 0.00004493
Iteration 43/1000 | Loss: 0.00004473
Iteration 44/1000 | Loss: 0.00004454
Iteration 45/1000 | Loss: 0.00004504
Iteration 46/1000 | Loss: 0.00004432
Iteration 47/1000 | Loss: 0.00004432
Iteration 48/1000 | Loss: 0.00004420
Iteration 49/1000 | Loss: 0.00004410
Iteration 50/1000 | Loss: 0.00004604
Iteration 51/1000 | Loss: 0.00004448
Iteration 52/1000 | Loss: 0.00004416
Iteration 53/1000 | Loss: 0.00004507
Iteration 54/1000 | Loss: 0.00004386
Iteration 55/1000 | Loss: 0.00004375
Iteration 56/1000 | Loss: 0.00004374
Iteration 57/1000 | Loss: 0.00004374
Iteration 58/1000 | Loss: 0.00004374
Iteration 59/1000 | Loss: 0.00004374
Iteration 60/1000 | Loss: 0.00004374
Iteration 61/1000 | Loss: 0.00004374
Iteration 62/1000 | Loss: 0.00004374
Iteration 63/1000 | Loss: 0.00004374
Iteration 64/1000 | Loss: 0.00004374
Iteration 65/1000 | Loss: 0.00004374
Iteration 66/1000 | Loss: 0.00004373
Iteration 67/1000 | Loss: 0.00004373
Iteration 68/1000 | Loss: 0.00004373
Iteration 69/1000 | Loss: 0.00004372
Iteration 70/1000 | Loss: 0.00004372
Iteration 71/1000 | Loss: 0.00004372
Iteration 72/1000 | Loss: 0.00004372
Iteration 73/1000 | Loss: 0.00004372
Iteration 74/1000 | Loss: 0.00004371
Iteration 75/1000 | Loss: 0.00004371
Iteration 76/1000 | Loss: 0.00004370
Iteration 77/1000 | Loss: 0.00004370
Iteration 78/1000 | Loss: 0.00004367
Iteration 79/1000 | Loss: 0.00004367
Iteration 80/1000 | Loss: 0.00004367
Iteration 81/1000 | Loss: 0.00004367
Iteration 82/1000 | Loss: 0.00004366
Iteration 83/1000 | Loss: 0.00004366
Iteration 84/1000 | Loss: 0.00004366
Iteration 85/1000 | Loss: 0.00004366
Iteration 86/1000 | Loss: 0.00004365
Iteration 87/1000 | Loss: 0.00004364
Iteration 88/1000 | Loss: 0.00004364
Iteration 89/1000 | Loss: 0.00004364
Iteration 90/1000 | Loss: 0.00004364
Iteration 91/1000 | Loss: 0.00004363
Iteration 92/1000 | Loss: 0.00004363
Iteration 93/1000 | Loss: 0.00004363
Iteration 94/1000 | Loss: 0.00004363
Iteration 95/1000 | Loss: 0.00004363
Iteration 96/1000 | Loss: 0.00004363
Iteration 97/1000 | Loss: 0.00004363
Iteration 98/1000 | Loss: 0.00004363
Iteration 99/1000 | Loss: 0.00004363
Iteration 100/1000 | Loss: 0.00004363
Iteration 101/1000 | Loss: 0.00004363
Iteration 102/1000 | Loss: 0.00004363
Iteration 103/1000 | Loss: 0.00004363
Iteration 104/1000 | Loss: 0.00004363
Iteration 105/1000 | Loss: 0.00004363
Iteration 106/1000 | Loss: 0.00004363
Iteration 107/1000 | Loss: 0.00004363
Iteration 108/1000 | Loss: 0.00004363
Iteration 109/1000 | Loss: 0.00004363
Iteration 110/1000 | Loss: 0.00004363
Iteration 111/1000 | Loss: 0.00004363
Iteration 112/1000 | Loss: 0.00004363
Iteration 113/1000 | Loss: 0.00004363
Iteration 114/1000 | Loss: 0.00004363
Iteration 115/1000 | Loss: 0.00004363
Iteration 116/1000 | Loss: 0.00004363
Iteration 117/1000 | Loss: 0.00004363
Iteration 118/1000 | Loss: 0.00004363
Iteration 119/1000 | Loss: 0.00004363
Iteration 120/1000 | Loss: 0.00004363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [4.363194966572337e-05, 4.363194966572337e-05, 4.363194966572337e-05, 4.363194966572337e-05, 4.363194966572337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.363194966572337e-05

Optimization complete. Final v2v error: 5.172783374786377 mm

Highest mean error: 10.352035522460938 mm for frame 19

Lowest mean error: 4.436793327331543 mm for frame 6

Saving results

Total time: 126.95188045501709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01159558
Iteration 2/25 | Loss: 0.00165568
Iteration 3/25 | Loss: 0.00130654
Iteration 4/25 | Loss: 0.00125564
Iteration 5/25 | Loss: 0.00128055
Iteration 6/25 | Loss: 0.00131324
Iteration 7/25 | Loss: 0.00126131
Iteration 8/25 | Loss: 0.00122786
Iteration 9/25 | Loss: 0.00122724
Iteration 10/25 | Loss: 0.00122637
Iteration 11/25 | Loss: 0.00122104
Iteration 12/25 | Loss: 0.00121801
Iteration 13/25 | Loss: 0.00121349
Iteration 14/25 | Loss: 0.00121069
Iteration 15/25 | Loss: 0.00121505
Iteration 16/25 | Loss: 0.00121304
Iteration 17/25 | Loss: 0.00122097
Iteration 18/25 | Loss: 0.00121691
Iteration 19/25 | Loss: 0.00121203
Iteration 20/25 | Loss: 0.00120666
Iteration 21/25 | Loss: 0.00120458
Iteration 22/25 | Loss: 0.00120047
Iteration 23/25 | Loss: 0.00119965
Iteration 24/25 | Loss: 0.00119850
Iteration 25/25 | Loss: 0.00120364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45019078
Iteration 2/25 | Loss: 0.00098629
Iteration 3/25 | Loss: 0.00098629
Iteration 4/25 | Loss: 0.00098629
Iteration 5/25 | Loss: 0.00098629
Iteration 6/25 | Loss: 0.00098629
Iteration 7/25 | Loss: 0.00098629
Iteration 8/25 | Loss: 0.00098629
Iteration 9/25 | Loss: 0.00098629
Iteration 10/25 | Loss: 0.00098629
Iteration 11/25 | Loss: 0.00098629
Iteration 12/25 | Loss: 0.00098629
Iteration 13/25 | Loss: 0.00098629
Iteration 14/25 | Loss: 0.00098629
Iteration 15/25 | Loss: 0.00098629
Iteration 16/25 | Loss: 0.00098629
Iteration 17/25 | Loss: 0.00098629
Iteration 18/25 | Loss: 0.00098629
Iteration 19/25 | Loss: 0.00098629
Iteration 20/25 | Loss: 0.00098629
Iteration 21/25 | Loss: 0.00098629
Iteration 22/25 | Loss: 0.00098629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009862857405096292, 0.0009862857405096292, 0.0009862857405096292, 0.0009862857405096292, 0.0009862857405096292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009862857405096292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098629
Iteration 2/1000 | Loss: 0.00027196
Iteration 3/1000 | Loss: 0.00092154
Iteration 4/1000 | Loss: 0.00008042
Iteration 5/1000 | Loss: 0.00005478
Iteration 6/1000 | Loss: 0.00022637
Iteration 7/1000 | Loss: 0.00022110
Iteration 8/1000 | Loss: 0.00006709
Iteration 9/1000 | Loss: 0.00005022
Iteration 10/1000 | Loss: 0.00006605
Iteration 11/1000 | Loss: 0.00004570
Iteration 12/1000 | Loss: 0.00020316
Iteration 13/1000 | Loss: 0.00014734
Iteration 14/1000 | Loss: 0.00004920
Iteration 15/1000 | Loss: 0.00020208
Iteration 16/1000 | Loss: 0.00023972
Iteration 17/1000 | Loss: 0.00019216
Iteration 18/1000 | Loss: 0.00008621
Iteration 19/1000 | Loss: 0.00016996
Iteration 20/1000 | Loss: 0.00009512
Iteration 21/1000 | Loss: 0.00019272
Iteration 22/1000 | Loss: 0.00010361
Iteration 23/1000 | Loss: 0.00011211
Iteration 24/1000 | Loss: 0.00016233
Iteration 25/1000 | Loss: 0.00010298
Iteration 26/1000 | Loss: 0.00008070
Iteration 27/1000 | Loss: 0.00010685
Iteration 28/1000 | Loss: 0.00005983
Iteration 29/1000 | Loss: 0.00003146
Iteration 30/1000 | Loss: 0.00009780
Iteration 31/1000 | Loss: 0.00076254
Iteration 32/1000 | Loss: 0.00040518
Iteration 33/1000 | Loss: 0.00033441
Iteration 34/1000 | Loss: 0.00024645
Iteration 35/1000 | Loss: 0.00003230
Iteration 36/1000 | Loss: 0.00003450
Iteration 37/1000 | Loss: 0.00003879
Iteration 38/1000 | Loss: 0.00002833
Iteration 39/1000 | Loss: 0.00049569
Iteration 40/1000 | Loss: 0.00072133
Iteration 41/1000 | Loss: 0.00056424
Iteration 42/1000 | Loss: 0.00062086
Iteration 43/1000 | Loss: 0.00057267
Iteration 44/1000 | Loss: 0.00043184
Iteration 45/1000 | Loss: 0.00006470
Iteration 46/1000 | Loss: 0.00046429
Iteration 47/1000 | Loss: 0.00058800
Iteration 48/1000 | Loss: 0.00023361
Iteration 49/1000 | Loss: 0.00004119
Iteration 50/1000 | Loss: 0.00003301
Iteration 51/1000 | Loss: 0.00002850
Iteration 52/1000 | Loss: 0.00002562
Iteration 53/1000 | Loss: 0.00002444
Iteration 54/1000 | Loss: 0.00002383
Iteration 55/1000 | Loss: 0.00002331
Iteration 56/1000 | Loss: 0.00081511
Iteration 57/1000 | Loss: 0.00037266
Iteration 58/1000 | Loss: 0.00011491
Iteration 59/1000 | Loss: 0.00042585
Iteration 60/1000 | Loss: 0.00027730
Iteration 61/1000 | Loss: 0.00036219
Iteration 62/1000 | Loss: 0.00022196
Iteration 63/1000 | Loss: 0.00002467
Iteration 64/1000 | Loss: 0.00036634
Iteration 65/1000 | Loss: 0.00067950
Iteration 66/1000 | Loss: 0.00004708
Iteration 67/1000 | Loss: 0.00058045
Iteration 68/1000 | Loss: 0.00029873
Iteration 69/1000 | Loss: 0.00011956
Iteration 70/1000 | Loss: 0.00013703
Iteration 71/1000 | Loss: 0.00015462
Iteration 72/1000 | Loss: 0.00020926
Iteration 73/1000 | Loss: 0.00017618
Iteration 74/1000 | Loss: 0.00012865
Iteration 75/1000 | Loss: 0.00002778
Iteration 76/1000 | Loss: 0.00002581
Iteration 77/1000 | Loss: 0.00002481
Iteration 78/1000 | Loss: 0.00024412
Iteration 79/1000 | Loss: 0.00017558
Iteration 80/1000 | Loss: 0.00025038
Iteration 81/1000 | Loss: 0.00020972
Iteration 82/1000 | Loss: 0.00038527
Iteration 83/1000 | Loss: 0.00016306
Iteration 84/1000 | Loss: 0.00016161
Iteration 85/1000 | Loss: 0.00022077
Iteration 86/1000 | Loss: 0.00027105
Iteration 87/1000 | Loss: 0.00015601
Iteration 88/1000 | Loss: 0.00019469
Iteration 89/1000 | Loss: 0.00004963
Iteration 90/1000 | Loss: 0.00063320
Iteration 91/1000 | Loss: 0.00004354
Iteration 92/1000 | Loss: 0.00003487
Iteration 93/1000 | Loss: 0.00003063
Iteration 94/1000 | Loss: 0.00002623
Iteration 95/1000 | Loss: 0.00002468
Iteration 96/1000 | Loss: 0.00002384
Iteration 97/1000 | Loss: 0.00002317
Iteration 98/1000 | Loss: 0.00002254
Iteration 99/1000 | Loss: 0.00002213
Iteration 100/1000 | Loss: 0.00002186
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002176
Iteration 103/1000 | Loss: 0.00002172
Iteration 104/1000 | Loss: 0.00002171
Iteration 105/1000 | Loss: 0.00002170
Iteration 106/1000 | Loss: 0.00002170
Iteration 107/1000 | Loss: 0.00002168
Iteration 108/1000 | Loss: 0.00002166
Iteration 109/1000 | Loss: 0.00002166
Iteration 110/1000 | Loss: 0.00002166
Iteration 111/1000 | Loss: 0.00002165
Iteration 112/1000 | Loss: 0.00002165
Iteration 113/1000 | Loss: 0.00002165
Iteration 114/1000 | Loss: 0.00002165
Iteration 115/1000 | Loss: 0.00002164
Iteration 116/1000 | Loss: 0.00002163
Iteration 117/1000 | Loss: 0.00002163
Iteration 118/1000 | Loss: 0.00002162
Iteration 119/1000 | Loss: 0.00002161
Iteration 120/1000 | Loss: 0.00002160
Iteration 121/1000 | Loss: 0.00002160
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002159
Iteration 124/1000 | Loss: 0.00002159
Iteration 125/1000 | Loss: 0.00002157
Iteration 126/1000 | Loss: 0.00002157
Iteration 127/1000 | Loss: 0.00002157
Iteration 128/1000 | Loss: 0.00002157
Iteration 129/1000 | Loss: 0.00002156
Iteration 130/1000 | Loss: 0.00002155
Iteration 131/1000 | Loss: 0.00002155
Iteration 132/1000 | Loss: 0.00002155
Iteration 133/1000 | Loss: 0.00002154
Iteration 134/1000 | Loss: 0.00002154
Iteration 135/1000 | Loss: 0.00002154
Iteration 136/1000 | Loss: 0.00002154
Iteration 137/1000 | Loss: 0.00002154
Iteration 138/1000 | Loss: 0.00002153
Iteration 139/1000 | Loss: 0.00002153
Iteration 140/1000 | Loss: 0.00002153
Iteration 141/1000 | Loss: 0.00002152
Iteration 142/1000 | Loss: 0.00002152
Iteration 143/1000 | Loss: 0.00002152
Iteration 144/1000 | Loss: 0.00002152
Iteration 145/1000 | Loss: 0.00002152
Iteration 146/1000 | Loss: 0.00002152
Iteration 147/1000 | Loss: 0.00002152
Iteration 148/1000 | Loss: 0.00002151
Iteration 149/1000 | Loss: 0.00002150
Iteration 150/1000 | Loss: 0.00002150
Iteration 151/1000 | Loss: 0.00002150
Iteration 152/1000 | Loss: 0.00002150
Iteration 153/1000 | Loss: 0.00002150
Iteration 154/1000 | Loss: 0.00002149
Iteration 155/1000 | Loss: 0.00002149
Iteration 156/1000 | Loss: 0.00002149
Iteration 157/1000 | Loss: 0.00002149
Iteration 158/1000 | Loss: 0.00002149
Iteration 159/1000 | Loss: 0.00002149
Iteration 160/1000 | Loss: 0.00002149
Iteration 161/1000 | Loss: 0.00002149
Iteration 162/1000 | Loss: 0.00002148
Iteration 163/1000 | Loss: 0.00002148
Iteration 164/1000 | Loss: 0.00002148
Iteration 165/1000 | Loss: 0.00002148
Iteration 166/1000 | Loss: 0.00002148
Iteration 167/1000 | Loss: 0.00002148
Iteration 168/1000 | Loss: 0.00002148
Iteration 169/1000 | Loss: 0.00002147
Iteration 170/1000 | Loss: 0.00002147
Iteration 171/1000 | Loss: 0.00002147
Iteration 172/1000 | Loss: 0.00002147
Iteration 173/1000 | Loss: 0.00002147
Iteration 174/1000 | Loss: 0.00002146
Iteration 175/1000 | Loss: 0.00002145
Iteration 176/1000 | Loss: 0.00002145
Iteration 177/1000 | Loss: 0.00002145
Iteration 178/1000 | Loss: 0.00002144
Iteration 179/1000 | Loss: 0.00002143
Iteration 180/1000 | Loss: 0.00002143
Iteration 181/1000 | Loss: 0.00002143
Iteration 182/1000 | Loss: 0.00002143
Iteration 183/1000 | Loss: 0.00002142
Iteration 184/1000 | Loss: 0.00002142
Iteration 185/1000 | Loss: 0.00002141
Iteration 186/1000 | Loss: 0.00002141
Iteration 187/1000 | Loss: 0.00002139
Iteration 188/1000 | Loss: 0.00002138
Iteration 189/1000 | Loss: 0.00002138
Iteration 190/1000 | Loss: 0.00002137
Iteration 191/1000 | Loss: 0.00002137
Iteration 192/1000 | Loss: 0.00002136
Iteration 193/1000 | Loss: 0.00002136
Iteration 194/1000 | Loss: 0.00002136
Iteration 195/1000 | Loss: 0.00002136
Iteration 196/1000 | Loss: 0.00002135
Iteration 197/1000 | Loss: 0.00002135
Iteration 198/1000 | Loss: 0.00002135
Iteration 199/1000 | Loss: 0.00002135
Iteration 200/1000 | Loss: 0.00002135
Iteration 201/1000 | Loss: 0.00002134
Iteration 202/1000 | Loss: 0.00002134
Iteration 203/1000 | Loss: 0.00002134
Iteration 204/1000 | Loss: 0.00002134
Iteration 205/1000 | Loss: 0.00002134
Iteration 206/1000 | Loss: 0.00002134
Iteration 207/1000 | Loss: 0.00002133
Iteration 208/1000 | Loss: 0.00002133
Iteration 209/1000 | Loss: 0.00002131
Iteration 210/1000 | Loss: 0.00002124
Iteration 211/1000 | Loss: 0.00002122
Iteration 212/1000 | Loss: 0.00003210
Iteration 213/1000 | Loss: 0.00002525
Iteration 214/1000 | Loss: 0.00003354
Iteration 215/1000 | Loss: 0.00002117
Iteration 216/1000 | Loss: 0.00002070
Iteration 217/1000 | Loss: 0.00002047
Iteration 218/1000 | Loss: 0.00002034
Iteration 219/1000 | Loss: 0.00002034
Iteration 220/1000 | Loss: 0.00002032
Iteration 221/1000 | Loss: 0.00002031
Iteration 222/1000 | Loss: 0.00002031
Iteration 223/1000 | Loss: 0.00002031
Iteration 224/1000 | Loss: 0.00002030
Iteration 225/1000 | Loss: 0.00002030
Iteration 226/1000 | Loss: 0.00002030
Iteration 227/1000 | Loss: 0.00002030
Iteration 228/1000 | Loss: 0.00002030
Iteration 229/1000 | Loss: 0.00002030
Iteration 230/1000 | Loss: 0.00002030
Iteration 231/1000 | Loss: 0.00002029
Iteration 232/1000 | Loss: 0.00002029
Iteration 233/1000 | Loss: 0.00002029
Iteration 234/1000 | Loss: 0.00002029
Iteration 235/1000 | Loss: 0.00002029
Iteration 236/1000 | Loss: 0.00002028
Iteration 237/1000 | Loss: 0.00002028
Iteration 238/1000 | Loss: 0.00002028
Iteration 239/1000 | Loss: 0.00002028
Iteration 240/1000 | Loss: 0.00002027
Iteration 241/1000 | Loss: 0.00002027
Iteration 242/1000 | Loss: 0.00002027
Iteration 243/1000 | Loss: 0.00002027
Iteration 244/1000 | Loss: 0.00002027
Iteration 245/1000 | Loss: 0.00002027
Iteration 246/1000 | Loss: 0.00002027
Iteration 247/1000 | Loss: 0.00002027
Iteration 248/1000 | Loss: 0.00002027
Iteration 249/1000 | Loss: 0.00002026
Iteration 250/1000 | Loss: 0.00002026
Iteration 251/1000 | Loss: 0.00002026
Iteration 252/1000 | Loss: 0.00002026
Iteration 253/1000 | Loss: 0.00002025
Iteration 254/1000 | Loss: 0.00002025
Iteration 255/1000 | Loss: 0.00002025
Iteration 256/1000 | Loss: 0.00002025
Iteration 257/1000 | Loss: 0.00002025
Iteration 258/1000 | Loss: 0.00002025
Iteration 259/1000 | Loss: 0.00002025
Iteration 260/1000 | Loss: 0.00002025
Iteration 261/1000 | Loss: 0.00002025
Iteration 262/1000 | Loss: 0.00002024
Iteration 263/1000 | Loss: 0.00002024
Iteration 264/1000 | Loss: 0.00002024
Iteration 265/1000 | Loss: 0.00002024
Iteration 266/1000 | Loss: 0.00002024
Iteration 267/1000 | Loss: 0.00002024
Iteration 268/1000 | Loss: 0.00002024
Iteration 269/1000 | Loss: 0.00002024
Iteration 270/1000 | Loss: 0.00002024
Iteration 271/1000 | Loss: 0.00002024
Iteration 272/1000 | Loss: 0.00002024
Iteration 273/1000 | Loss: 0.00002024
Iteration 274/1000 | Loss: 0.00002024
Iteration 275/1000 | Loss: 0.00002024
Iteration 276/1000 | Loss: 0.00002023
Iteration 277/1000 | Loss: 0.00002023
Iteration 278/1000 | Loss: 0.00002023
Iteration 279/1000 | Loss: 0.00002023
Iteration 280/1000 | Loss: 0.00002023
Iteration 281/1000 | Loss: 0.00002023
Iteration 282/1000 | Loss: 0.00002023
Iteration 283/1000 | Loss: 0.00002022
Iteration 284/1000 | Loss: 0.00002022
Iteration 285/1000 | Loss: 0.00002022
Iteration 286/1000 | Loss: 0.00002022
Iteration 287/1000 | Loss: 0.00002022
Iteration 288/1000 | Loss: 0.00002022
Iteration 289/1000 | Loss: 0.00002021
Iteration 290/1000 | Loss: 0.00002021
Iteration 291/1000 | Loss: 0.00002021
Iteration 292/1000 | Loss: 0.00002021
Iteration 293/1000 | Loss: 0.00002021
Iteration 294/1000 | Loss: 0.00002021
Iteration 295/1000 | Loss: 0.00002021
Iteration 296/1000 | Loss: 0.00002021
Iteration 297/1000 | Loss: 0.00002021
Iteration 298/1000 | Loss: 0.00002021
Iteration 299/1000 | Loss: 0.00002021
Iteration 300/1000 | Loss: 0.00002021
Iteration 301/1000 | Loss: 0.00002021
Iteration 302/1000 | Loss: 0.00002021
Iteration 303/1000 | Loss: 0.00002021
Iteration 304/1000 | Loss: 0.00002021
Iteration 305/1000 | Loss: 0.00002021
Iteration 306/1000 | Loss: 0.00002021
Iteration 307/1000 | Loss: 0.00002021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [2.0210627553751692e-05, 2.0210627553751692e-05, 2.0210627553751692e-05, 2.0210627553751692e-05, 2.0210627553751692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0210627553751692e-05

Optimization complete. Final v2v error: 3.700655698776245 mm

Highest mean error: 4.6584062576293945 mm for frame 8

Lowest mean error: 3.236361026763916 mm for frame 52

Saving results

Total time: 214.51025557518005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915852
Iteration 2/25 | Loss: 0.00127889
Iteration 3/25 | Loss: 0.00111545
Iteration 4/25 | Loss: 0.00109663
Iteration 5/25 | Loss: 0.00109240
Iteration 6/25 | Loss: 0.00109154
Iteration 7/25 | Loss: 0.00109154
Iteration 8/25 | Loss: 0.00109154
Iteration 9/25 | Loss: 0.00109153
Iteration 10/25 | Loss: 0.00109152
Iteration 11/25 | Loss: 0.00109152
Iteration 12/25 | Loss: 0.00109152
Iteration 13/25 | Loss: 0.00109152
Iteration 14/25 | Loss: 0.00109152
Iteration 15/25 | Loss: 0.00109152
Iteration 16/25 | Loss: 0.00109152
Iteration 17/25 | Loss: 0.00109152
Iteration 18/25 | Loss: 0.00109152
Iteration 19/25 | Loss: 0.00109152
Iteration 20/25 | Loss: 0.00109152
Iteration 21/25 | Loss: 0.00109152
Iteration 22/25 | Loss: 0.00109152
Iteration 23/25 | Loss: 0.00109152
Iteration 24/25 | Loss: 0.00109152
Iteration 25/25 | Loss: 0.00109152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42593491
Iteration 2/25 | Loss: 0.00034139
Iteration 3/25 | Loss: 0.00034137
Iteration 4/25 | Loss: 0.00034137
Iteration 5/25 | Loss: 0.00034137
Iteration 6/25 | Loss: 0.00034137
Iteration 7/25 | Loss: 0.00034136
Iteration 8/25 | Loss: 0.00034136
Iteration 9/25 | Loss: 0.00034136
Iteration 10/25 | Loss: 0.00034136
Iteration 11/25 | Loss: 0.00034136
Iteration 12/25 | Loss: 0.00034136
Iteration 13/25 | Loss: 0.00034136
Iteration 14/25 | Loss: 0.00034136
Iteration 15/25 | Loss: 0.00034136
Iteration 16/25 | Loss: 0.00034136
Iteration 17/25 | Loss: 0.00034136
Iteration 18/25 | Loss: 0.00034136
Iteration 19/25 | Loss: 0.00034136
Iteration 20/25 | Loss: 0.00034136
Iteration 21/25 | Loss: 0.00034136
Iteration 22/25 | Loss: 0.00034136
Iteration 23/25 | Loss: 0.00034136
Iteration 24/25 | Loss: 0.00034136
Iteration 25/25 | Loss: 0.00034136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034136
Iteration 2/1000 | Loss: 0.00002470
Iteration 3/1000 | Loss: 0.00001906
Iteration 4/1000 | Loss: 0.00001624
Iteration 5/1000 | Loss: 0.00001482
Iteration 6/1000 | Loss: 0.00001408
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001358
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001357
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001353
Iteration 16/1000 | Loss: 0.00001352
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001343
Iteration 21/1000 | Loss: 0.00001342
Iteration 22/1000 | Loss: 0.00001341
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001341
Iteration 25/1000 | Loss: 0.00001340
Iteration 26/1000 | Loss: 0.00001340
Iteration 27/1000 | Loss: 0.00001338
Iteration 28/1000 | Loss: 0.00001336
Iteration 29/1000 | Loss: 0.00001336
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001336
Iteration 35/1000 | Loss: 0.00001336
Iteration 36/1000 | Loss: 0.00001336
Iteration 37/1000 | Loss: 0.00001336
Iteration 38/1000 | Loss: 0.00001335
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001333
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001332
Iteration 47/1000 | Loss: 0.00001332
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001324
Iteration 57/1000 | Loss: 0.00001324
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001320
Iteration 64/1000 | Loss: 0.00001319
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001318
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001315
Iteration 72/1000 | Loss: 0.00001315
Iteration 73/1000 | Loss: 0.00001315
Iteration 74/1000 | Loss: 0.00001314
Iteration 75/1000 | Loss: 0.00001314
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001312
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001312
Iteration 89/1000 | Loss: 0.00001312
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001309
Iteration 110/1000 | Loss: 0.00001309
Iteration 111/1000 | Loss: 0.00001309
Iteration 112/1000 | Loss: 0.00001309
Iteration 113/1000 | Loss: 0.00001309
Iteration 114/1000 | Loss: 0.00001308
Iteration 115/1000 | Loss: 0.00001308
Iteration 116/1000 | Loss: 0.00001308
Iteration 117/1000 | Loss: 0.00001308
Iteration 118/1000 | Loss: 0.00001308
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001308
Iteration 121/1000 | Loss: 0.00001308
Iteration 122/1000 | Loss: 0.00001308
Iteration 123/1000 | Loss: 0.00001308
Iteration 124/1000 | Loss: 0.00001308
Iteration 125/1000 | Loss: 0.00001308
Iteration 126/1000 | Loss: 0.00001308
Iteration 127/1000 | Loss: 0.00001308
Iteration 128/1000 | Loss: 0.00001308
Iteration 129/1000 | Loss: 0.00001308
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001308
Iteration 133/1000 | Loss: 0.00001308
Iteration 134/1000 | Loss: 0.00001308
Iteration 135/1000 | Loss: 0.00001308
Iteration 136/1000 | Loss: 0.00001308
Iteration 137/1000 | Loss: 0.00001308
Iteration 138/1000 | Loss: 0.00001308
Iteration 139/1000 | Loss: 0.00001308
Iteration 140/1000 | Loss: 0.00001308
Iteration 141/1000 | Loss: 0.00001308
Iteration 142/1000 | Loss: 0.00001308
Iteration 143/1000 | Loss: 0.00001308
Iteration 144/1000 | Loss: 0.00001308
Iteration 145/1000 | Loss: 0.00001308
Iteration 146/1000 | Loss: 0.00001308
Iteration 147/1000 | Loss: 0.00001308
Iteration 148/1000 | Loss: 0.00001308
Iteration 149/1000 | Loss: 0.00001308
Iteration 150/1000 | Loss: 0.00001308
Iteration 151/1000 | Loss: 0.00001308
Iteration 152/1000 | Loss: 0.00001308
Iteration 153/1000 | Loss: 0.00001308
Iteration 154/1000 | Loss: 0.00001308
Iteration 155/1000 | Loss: 0.00001308
Iteration 156/1000 | Loss: 0.00001308
Iteration 157/1000 | Loss: 0.00001308
Iteration 158/1000 | Loss: 0.00001308
Iteration 159/1000 | Loss: 0.00001308
Iteration 160/1000 | Loss: 0.00001308
Iteration 161/1000 | Loss: 0.00001308
Iteration 162/1000 | Loss: 0.00001308
Iteration 163/1000 | Loss: 0.00001308
Iteration 164/1000 | Loss: 0.00001308
Iteration 165/1000 | Loss: 0.00001308
Iteration 166/1000 | Loss: 0.00001308
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001308
Iteration 169/1000 | Loss: 0.00001308
Iteration 170/1000 | Loss: 0.00001308
Iteration 171/1000 | Loss: 0.00001308
Iteration 172/1000 | Loss: 0.00001308
Iteration 173/1000 | Loss: 0.00001308
Iteration 174/1000 | Loss: 0.00001308
Iteration 175/1000 | Loss: 0.00001308
Iteration 176/1000 | Loss: 0.00001308
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.3081928045721725e-05, 1.3081928045721725e-05, 1.3081928045721725e-05, 1.3081928045721725e-05, 1.3081928045721725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3081928045721725e-05

Optimization complete. Final v2v error: 3.0760319232940674 mm

Highest mean error: 3.3994197845458984 mm for frame 32

Lowest mean error: 2.721456289291382 mm for frame 163

Saving results

Total time: 32.732107162475586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798339
Iteration 2/25 | Loss: 0.00155125
Iteration 3/25 | Loss: 0.00129281
Iteration 4/25 | Loss: 0.00124967
Iteration 5/25 | Loss: 0.00122975
Iteration 6/25 | Loss: 0.00123078
Iteration 7/25 | Loss: 0.00122485
Iteration 8/25 | Loss: 0.00122446
Iteration 9/25 | Loss: 0.00122434
Iteration 10/25 | Loss: 0.00122432
Iteration 11/25 | Loss: 0.00122432
Iteration 12/25 | Loss: 0.00122432
Iteration 13/25 | Loss: 0.00122432
Iteration 14/25 | Loss: 0.00122432
Iteration 15/25 | Loss: 0.00122432
Iteration 16/25 | Loss: 0.00122431
Iteration 17/25 | Loss: 0.00122431
Iteration 18/25 | Loss: 0.00122431
Iteration 19/25 | Loss: 0.00122431
Iteration 20/25 | Loss: 0.00122431
Iteration 21/25 | Loss: 0.00122431
Iteration 22/25 | Loss: 0.00122431
Iteration 23/25 | Loss: 0.00122431
Iteration 24/25 | Loss: 0.00122431
Iteration 25/25 | Loss: 0.00122431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.35698342
Iteration 2/25 | Loss: 0.00049916
Iteration 3/25 | Loss: 0.00044657
Iteration 4/25 | Loss: 0.00044656
Iteration 5/25 | Loss: 0.00044656
Iteration 6/25 | Loss: 0.00044656
Iteration 7/25 | Loss: 0.00044656
Iteration 8/25 | Loss: 0.00044656
Iteration 9/25 | Loss: 0.00044656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.00044656451791524887, 0.00044656451791524887, 0.00044656451791524887, 0.00044656451791524887, 0.00044656451791524887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00044656451791524887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044656
Iteration 2/1000 | Loss: 0.00003479
Iteration 3/1000 | Loss: 0.00002747
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002340
Iteration 6/1000 | Loss: 0.00002290
Iteration 7/1000 | Loss: 0.00002262
Iteration 8/1000 | Loss: 0.00002260
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002247
Iteration 11/1000 | Loss: 0.00002234
Iteration 12/1000 | Loss: 0.00002234
Iteration 13/1000 | Loss: 0.00002229
Iteration 14/1000 | Loss: 0.00002229
Iteration 15/1000 | Loss: 0.00002228
Iteration 16/1000 | Loss: 0.00002228
Iteration 17/1000 | Loss: 0.00002228
Iteration 18/1000 | Loss: 0.00002228
Iteration 19/1000 | Loss: 0.00002228
Iteration 20/1000 | Loss: 0.00002228
Iteration 21/1000 | Loss: 0.00002228
Iteration 22/1000 | Loss: 0.00002228
Iteration 23/1000 | Loss: 0.00002228
Iteration 24/1000 | Loss: 0.00002224
Iteration 25/1000 | Loss: 0.00002224
Iteration 26/1000 | Loss: 0.00002224
Iteration 27/1000 | Loss: 0.00002224
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002222
Iteration 30/1000 | Loss: 0.00002222
Iteration 31/1000 | Loss: 0.00002221
Iteration 32/1000 | Loss: 0.00002221
Iteration 33/1000 | Loss: 0.00002220
Iteration 34/1000 | Loss: 0.00002220
Iteration 35/1000 | Loss: 0.00002219
Iteration 36/1000 | Loss: 0.00002219
Iteration 37/1000 | Loss: 0.00002216
Iteration 38/1000 | Loss: 0.00002215
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002214
Iteration 42/1000 | Loss: 0.00002212
Iteration 43/1000 | Loss: 0.00002211
Iteration 44/1000 | Loss: 0.00002211
Iteration 45/1000 | Loss: 0.00002210
Iteration 46/1000 | Loss: 0.00002210
Iteration 47/1000 | Loss: 0.00002210
Iteration 48/1000 | Loss: 0.00002210
Iteration 49/1000 | Loss: 0.00002210
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002210
Iteration 52/1000 | Loss: 0.00002210
Iteration 53/1000 | Loss: 0.00002210
Iteration 54/1000 | Loss: 0.00002210
Iteration 55/1000 | Loss: 0.00002209
Iteration 56/1000 | Loss: 0.00002209
Iteration 57/1000 | Loss: 0.00002209
Iteration 58/1000 | Loss: 0.00002208
Iteration 59/1000 | Loss: 0.00002208
Iteration 60/1000 | Loss: 0.00002207
Iteration 61/1000 | Loss: 0.00002207
Iteration 62/1000 | Loss: 0.00002207
Iteration 63/1000 | Loss: 0.00002207
Iteration 64/1000 | Loss: 0.00002207
Iteration 65/1000 | Loss: 0.00002206
Iteration 66/1000 | Loss: 0.00002206
Iteration 67/1000 | Loss: 0.00002206
Iteration 68/1000 | Loss: 0.00002205
Iteration 69/1000 | Loss: 0.00002205
Iteration 70/1000 | Loss: 0.00002205
Iteration 71/1000 | Loss: 0.00002205
Iteration 72/1000 | Loss: 0.00002205
Iteration 73/1000 | Loss: 0.00002205
Iteration 74/1000 | Loss: 0.00002205
Iteration 75/1000 | Loss: 0.00002205
Iteration 76/1000 | Loss: 0.00002204
Iteration 77/1000 | Loss: 0.00002204
Iteration 78/1000 | Loss: 0.00002204
Iteration 79/1000 | Loss: 0.00002204
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002204
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002204
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002204
Iteration 87/1000 | Loss: 0.00002204
Iteration 88/1000 | Loss: 0.00002204
Iteration 89/1000 | Loss: 0.00002204
Iteration 90/1000 | Loss: 0.00002204
Iteration 91/1000 | Loss: 0.00002204
Iteration 92/1000 | Loss: 0.00002204
Iteration 93/1000 | Loss: 0.00002204
Iteration 94/1000 | Loss: 0.00002204
Iteration 95/1000 | Loss: 0.00002204
Iteration 96/1000 | Loss: 0.00002204
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002203
Iteration 99/1000 | Loss: 0.00002203
Iteration 100/1000 | Loss: 0.00002203
Iteration 101/1000 | Loss: 0.00002203
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00002203
Iteration 104/1000 | Loss: 0.00002203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.203466101491358e-05, 2.203466101491358e-05, 2.203466101491358e-05, 2.203466101491358e-05, 2.203466101491358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.203466101491358e-05

Optimization complete. Final v2v error: 3.951017379760742 mm

Highest mean error: 4.344942092895508 mm for frame 232

Lowest mean error: 3.66228985786438 mm for frame 144

Saving results

Total time: 39.9283401966095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794492
Iteration 2/25 | Loss: 0.00223837
Iteration 3/25 | Loss: 0.00169773
Iteration 4/25 | Loss: 0.00152013
Iteration 5/25 | Loss: 0.00152220
Iteration 6/25 | Loss: 0.00129851
Iteration 7/25 | Loss: 0.00127906
Iteration 8/25 | Loss: 0.00127339
Iteration 9/25 | Loss: 0.00127194
Iteration 10/25 | Loss: 0.00126944
Iteration 11/25 | Loss: 0.00126253
Iteration 12/25 | Loss: 0.00125466
Iteration 13/25 | Loss: 0.00125026
Iteration 14/25 | Loss: 0.00124745
Iteration 15/25 | Loss: 0.00124655
Iteration 16/25 | Loss: 0.00124669
Iteration 17/25 | Loss: 0.00124664
Iteration 18/25 | Loss: 0.00124615
Iteration 19/25 | Loss: 0.00124524
Iteration 20/25 | Loss: 0.00124485
Iteration 21/25 | Loss: 0.00124692
Iteration 22/25 | Loss: 0.00124482
Iteration 23/25 | Loss: 0.00124475
Iteration 24/25 | Loss: 0.00124726
Iteration 25/25 | Loss: 0.00124574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42143047
Iteration 2/25 | Loss: 0.00055573
Iteration 3/25 | Loss: 0.00055571
Iteration 4/25 | Loss: 0.00055570
Iteration 5/25 | Loss: 0.00055570
Iteration 6/25 | Loss: 0.00055570
Iteration 7/25 | Loss: 0.00055570
Iteration 8/25 | Loss: 0.00055570
Iteration 9/25 | Loss: 0.00055570
Iteration 10/25 | Loss: 0.00055570
Iteration 11/25 | Loss: 0.00055570
Iteration 12/25 | Loss: 0.00055570
Iteration 13/25 | Loss: 0.00055570
Iteration 14/25 | Loss: 0.00055570
Iteration 15/25 | Loss: 0.00055570
Iteration 16/25 | Loss: 0.00055570
Iteration 17/25 | Loss: 0.00055570
Iteration 18/25 | Loss: 0.00055570
Iteration 19/25 | Loss: 0.00055570
Iteration 20/25 | Loss: 0.00055570
Iteration 21/25 | Loss: 0.00055570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000555702019482851, 0.000555702019482851, 0.000555702019482851, 0.000555702019482851, 0.000555702019482851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000555702019482851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055570
Iteration 2/1000 | Loss: 0.00006949
Iteration 3/1000 | Loss: 0.00010989
Iteration 4/1000 | Loss: 0.00011107
Iteration 5/1000 | Loss: 0.00009857
Iteration 6/1000 | Loss: 0.00009491
Iteration 7/1000 | Loss: 0.00012583
Iteration 8/1000 | Loss: 0.00014629
Iteration 9/1000 | Loss: 0.00015755
Iteration 10/1000 | Loss: 0.00012858
Iteration 11/1000 | Loss: 0.00008891
Iteration 12/1000 | Loss: 0.00006561
Iteration 13/1000 | Loss: 0.00009965
Iteration 14/1000 | Loss: 0.00010428
Iteration 15/1000 | Loss: 0.00008823
Iteration 16/1000 | Loss: 0.00013009
Iteration 17/1000 | Loss: 0.00012538
Iteration 18/1000 | Loss: 0.00009883
Iteration 19/1000 | Loss: 0.00011331
Iteration 20/1000 | Loss: 0.00012688
Iteration 21/1000 | Loss: 0.00011409
Iteration 22/1000 | Loss: 0.00011204
Iteration 23/1000 | Loss: 0.00010184
Iteration 24/1000 | Loss: 0.00010647
Iteration 25/1000 | Loss: 0.00007142
Iteration 26/1000 | Loss: 0.00007452
Iteration 27/1000 | Loss: 0.00007172
Iteration 28/1000 | Loss: 0.00007907
Iteration 29/1000 | Loss: 0.00005342
Iteration 30/1000 | Loss: 0.00006658
Iteration 31/1000 | Loss: 0.00005270
Iteration 32/1000 | Loss: 0.00005887
Iteration 33/1000 | Loss: 0.00006239
Iteration 34/1000 | Loss: 0.00005015
Iteration 35/1000 | Loss: 0.00005161
Iteration 36/1000 | Loss: 0.00007001
Iteration 37/1000 | Loss: 0.00006371
Iteration 38/1000 | Loss: 0.00005447
Iteration 39/1000 | Loss: 0.00003765
Iteration 40/1000 | Loss: 0.00003327
Iteration 41/1000 | Loss: 0.00003599
Iteration 42/1000 | Loss: 0.00004786
Iteration 43/1000 | Loss: 0.00006197
Iteration 44/1000 | Loss: 0.00005485
Iteration 45/1000 | Loss: 0.00006934
Iteration 46/1000 | Loss: 0.00007032
Iteration 47/1000 | Loss: 0.00002652
Iteration 48/1000 | Loss: 0.00003724
Iteration 49/1000 | Loss: 0.00003233
Iteration 50/1000 | Loss: 0.00004076
Iteration 51/1000 | Loss: 0.00004150
Iteration 52/1000 | Loss: 0.00004905
Iteration 53/1000 | Loss: 0.00004719
Iteration 54/1000 | Loss: 0.00002850
Iteration 55/1000 | Loss: 0.00003874
Iteration 56/1000 | Loss: 0.00004849
Iteration 57/1000 | Loss: 0.00003007
Iteration 58/1000 | Loss: 0.00004330
Iteration 59/1000 | Loss: 0.00003502
Iteration 60/1000 | Loss: 0.00005985
Iteration 61/1000 | Loss: 0.00004910
Iteration 62/1000 | Loss: 0.00004348
Iteration 63/1000 | Loss: 0.00007808
Iteration 64/1000 | Loss: 0.00004456
Iteration 65/1000 | Loss: 0.00003541
Iteration 66/1000 | Loss: 0.00004074
Iteration 67/1000 | Loss: 0.00002932
Iteration 68/1000 | Loss: 0.00004002
Iteration 69/1000 | Loss: 0.00003939
Iteration 70/1000 | Loss: 0.00003442
Iteration 71/1000 | Loss: 0.00003543
Iteration 72/1000 | Loss: 0.00003098
Iteration 73/1000 | Loss: 0.00003081
Iteration 74/1000 | Loss: 0.00003204
Iteration 75/1000 | Loss: 0.00003080
Iteration 76/1000 | Loss: 0.00003867
Iteration 77/1000 | Loss: 0.00003413
Iteration 78/1000 | Loss: 0.00004009
Iteration 79/1000 | Loss: 0.00002523
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002040
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001961
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001930
Iteration 86/1000 | Loss: 0.00001927
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001915
Iteration 90/1000 | Loss: 0.00001915
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001910
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001908
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001908
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001904
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001901
Iteration 111/1000 | Loss: 0.00001899
Iteration 112/1000 | Loss: 0.00001898
Iteration 113/1000 | Loss: 0.00001898
Iteration 114/1000 | Loss: 0.00001898
Iteration 115/1000 | Loss: 0.00001897
Iteration 116/1000 | Loss: 0.00001897
Iteration 117/1000 | Loss: 0.00001896
Iteration 118/1000 | Loss: 0.00001896
Iteration 119/1000 | Loss: 0.00001895
Iteration 120/1000 | Loss: 0.00001895
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001893
Iteration 124/1000 | Loss: 0.00001893
Iteration 125/1000 | Loss: 0.00001892
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00001892
Iteration 128/1000 | Loss: 0.00001891
Iteration 129/1000 | Loss: 0.00001891
Iteration 130/1000 | Loss: 0.00001891
Iteration 131/1000 | Loss: 0.00001891
Iteration 132/1000 | Loss: 0.00001891
Iteration 133/1000 | Loss: 0.00001891
Iteration 134/1000 | Loss: 0.00001890
Iteration 135/1000 | Loss: 0.00001890
Iteration 136/1000 | Loss: 0.00001890
Iteration 137/1000 | Loss: 0.00001890
Iteration 138/1000 | Loss: 0.00001890
Iteration 139/1000 | Loss: 0.00001890
Iteration 140/1000 | Loss: 0.00001890
Iteration 141/1000 | Loss: 0.00001890
Iteration 142/1000 | Loss: 0.00001889
Iteration 143/1000 | Loss: 0.00001889
Iteration 144/1000 | Loss: 0.00001889
Iteration 145/1000 | Loss: 0.00001888
Iteration 146/1000 | Loss: 0.00001888
Iteration 147/1000 | Loss: 0.00001888
Iteration 148/1000 | Loss: 0.00001888
Iteration 149/1000 | Loss: 0.00001888
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001888
Iteration 153/1000 | Loss: 0.00001887
Iteration 154/1000 | Loss: 0.00001887
Iteration 155/1000 | Loss: 0.00001886
Iteration 156/1000 | Loss: 0.00001886
Iteration 157/1000 | Loss: 0.00001886
Iteration 158/1000 | Loss: 0.00001886
Iteration 159/1000 | Loss: 0.00001886
Iteration 160/1000 | Loss: 0.00001886
Iteration 161/1000 | Loss: 0.00001885
Iteration 162/1000 | Loss: 0.00001885
Iteration 163/1000 | Loss: 0.00001885
Iteration 164/1000 | Loss: 0.00001885
Iteration 165/1000 | Loss: 0.00001885
Iteration 166/1000 | Loss: 0.00001884
Iteration 167/1000 | Loss: 0.00001884
Iteration 168/1000 | Loss: 0.00001884
Iteration 169/1000 | Loss: 0.00001884
Iteration 170/1000 | Loss: 0.00001884
Iteration 171/1000 | Loss: 0.00001884
Iteration 172/1000 | Loss: 0.00001884
Iteration 173/1000 | Loss: 0.00001884
Iteration 174/1000 | Loss: 0.00001884
Iteration 175/1000 | Loss: 0.00001882
Iteration 176/1000 | Loss: 0.00001882
Iteration 177/1000 | Loss: 0.00001882
Iteration 178/1000 | Loss: 0.00001882
Iteration 179/1000 | Loss: 0.00001882
Iteration 180/1000 | Loss: 0.00001882
Iteration 181/1000 | Loss: 0.00001882
Iteration 182/1000 | Loss: 0.00001881
Iteration 183/1000 | Loss: 0.00001881
Iteration 184/1000 | Loss: 0.00001881
Iteration 185/1000 | Loss: 0.00001881
Iteration 186/1000 | Loss: 0.00001881
Iteration 187/1000 | Loss: 0.00001880
Iteration 188/1000 | Loss: 0.00001880
Iteration 189/1000 | Loss: 0.00001880
Iteration 190/1000 | Loss: 0.00001879
Iteration 191/1000 | Loss: 0.00001879
Iteration 192/1000 | Loss: 0.00001879
Iteration 193/1000 | Loss: 0.00001878
Iteration 194/1000 | Loss: 0.00001878
Iteration 195/1000 | Loss: 0.00001878
Iteration 196/1000 | Loss: 0.00001878
Iteration 197/1000 | Loss: 0.00001878
Iteration 198/1000 | Loss: 0.00001878
Iteration 199/1000 | Loss: 0.00001878
Iteration 200/1000 | Loss: 0.00001878
Iteration 201/1000 | Loss: 0.00001878
Iteration 202/1000 | Loss: 0.00001878
Iteration 203/1000 | Loss: 0.00001878
Iteration 204/1000 | Loss: 0.00001877
Iteration 205/1000 | Loss: 0.00001877
Iteration 206/1000 | Loss: 0.00001877
Iteration 207/1000 | Loss: 0.00001877
Iteration 208/1000 | Loss: 0.00001877
Iteration 209/1000 | Loss: 0.00001877
Iteration 210/1000 | Loss: 0.00001876
Iteration 211/1000 | Loss: 0.00001876
Iteration 212/1000 | Loss: 0.00001876
Iteration 213/1000 | Loss: 0.00001876
Iteration 214/1000 | Loss: 0.00001876
Iteration 215/1000 | Loss: 0.00001876
Iteration 216/1000 | Loss: 0.00001876
Iteration 217/1000 | Loss: 0.00001876
Iteration 218/1000 | Loss: 0.00001876
Iteration 219/1000 | Loss: 0.00001876
Iteration 220/1000 | Loss: 0.00001876
Iteration 221/1000 | Loss: 0.00001876
Iteration 222/1000 | Loss: 0.00001876
Iteration 223/1000 | Loss: 0.00001876
Iteration 224/1000 | Loss: 0.00001876
Iteration 225/1000 | Loss: 0.00001875
Iteration 226/1000 | Loss: 0.00001875
Iteration 227/1000 | Loss: 0.00001875
Iteration 228/1000 | Loss: 0.00001875
Iteration 229/1000 | Loss: 0.00001875
Iteration 230/1000 | Loss: 0.00001875
Iteration 231/1000 | Loss: 0.00001875
Iteration 232/1000 | Loss: 0.00001875
Iteration 233/1000 | Loss: 0.00001875
Iteration 234/1000 | Loss: 0.00001875
Iteration 235/1000 | Loss: 0.00001875
Iteration 236/1000 | Loss: 0.00001874
Iteration 237/1000 | Loss: 0.00001874
Iteration 238/1000 | Loss: 0.00001874
Iteration 239/1000 | Loss: 0.00001874
Iteration 240/1000 | Loss: 0.00001874
Iteration 241/1000 | Loss: 0.00001874
Iteration 242/1000 | Loss: 0.00001873
Iteration 243/1000 | Loss: 0.00001873
Iteration 244/1000 | Loss: 0.00001873
Iteration 245/1000 | Loss: 0.00001873
Iteration 246/1000 | Loss: 0.00001873
Iteration 247/1000 | Loss: 0.00001873
Iteration 248/1000 | Loss: 0.00001873
Iteration 249/1000 | Loss: 0.00001873
Iteration 250/1000 | Loss: 0.00001873
Iteration 251/1000 | Loss: 0.00001873
Iteration 252/1000 | Loss: 0.00001873
Iteration 253/1000 | Loss: 0.00001873
Iteration 254/1000 | Loss: 0.00001873
Iteration 255/1000 | Loss: 0.00001873
Iteration 256/1000 | Loss: 0.00001873
Iteration 257/1000 | Loss: 0.00001873
Iteration 258/1000 | Loss: 0.00001873
Iteration 259/1000 | Loss: 0.00001873
Iteration 260/1000 | Loss: 0.00001873
Iteration 261/1000 | Loss: 0.00001872
Iteration 262/1000 | Loss: 0.00001872
Iteration 263/1000 | Loss: 0.00001872
Iteration 264/1000 | Loss: 0.00001872
Iteration 265/1000 | Loss: 0.00001872
Iteration 266/1000 | Loss: 0.00001872
Iteration 267/1000 | Loss: 0.00001872
Iteration 268/1000 | Loss: 0.00001872
Iteration 269/1000 | Loss: 0.00001872
Iteration 270/1000 | Loss: 0.00001872
Iteration 271/1000 | Loss: 0.00001872
Iteration 272/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.8723056200542487e-05, 1.8723056200542487e-05, 1.8723056200542487e-05, 1.8723056200542487e-05, 1.8723056200542487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8723056200542487e-05

Optimization complete. Final v2v error: 3.7236216068267822 mm

Highest mean error: 4.273800849914551 mm for frame 234

Lowest mean error: 3.4050111770629883 mm for frame 188

Saving results

Total time: 205.33259224891663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825730
Iteration 2/25 | Loss: 0.00229644
Iteration 3/25 | Loss: 0.00169618
Iteration 4/25 | Loss: 0.00164327
Iteration 5/25 | Loss: 0.00154516
Iteration 6/25 | Loss: 0.00141953
Iteration 7/25 | Loss: 0.00136657
Iteration 8/25 | Loss: 0.00134982
Iteration 9/25 | Loss: 0.00133430
Iteration 10/25 | Loss: 0.00132505
Iteration 11/25 | Loss: 0.00131674
Iteration 12/25 | Loss: 0.00131144
Iteration 13/25 | Loss: 0.00130945
Iteration 14/25 | Loss: 0.00130697
Iteration 15/25 | Loss: 0.00130522
Iteration 16/25 | Loss: 0.00130395
Iteration 17/25 | Loss: 0.00130307
Iteration 18/25 | Loss: 0.00130057
Iteration 19/25 | Loss: 0.00130309
Iteration 20/25 | Loss: 0.00130147
Iteration 21/25 | Loss: 0.00130325
Iteration 22/25 | Loss: 0.00130156
Iteration 23/25 | Loss: 0.00130110
Iteration 24/25 | Loss: 0.00129971
Iteration 25/25 | Loss: 0.00129729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31929421
Iteration 2/25 | Loss: 0.00072781
Iteration 3/25 | Loss: 0.00072781
Iteration 4/25 | Loss: 0.00072781
Iteration 5/25 | Loss: 0.00072781
Iteration 6/25 | Loss: 0.00072781
Iteration 7/25 | Loss: 0.00072781
Iteration 8/25 | Loss: 0.00072781
Iteration 9/25 | Loss: 0.00072781
Iteration 10/25 | Loss: 0.00072781
Iteration 11/25 | Loss: 0.00072781
Iteration 12/25 | Loss: 0.00072781
Iteration 13/25 | Loss: 0.00072781
Iteration 14/25 | Loss: 0.00072781
Iteration 15/25 | Loss: 0.00072781
Iteration 16/25 | Loss: 0.00072781
Iteration 17/25 | Loss: 0.00072781
Iteration 18/25 | Loss: 0.00072781
Iteration 19/25 | Loss: 0.00072781
Iteration 20/25 | Loss: 0.00072781
Iteration 21/25 | Loss: 0.00072781
Iteration 22/25 | Loss: 0.00072781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007278064149431884, 0.0007278064149431884, 0.0007278064149431884, 0.0007278064149431884, 0.0007278064149431884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007278064149431884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072781
Iteration 2/1000 | Loss: 0.00083701
Iteration 3/1000 | Loss: 0.00014755
Iteration 4/1000 | Loss: 0.00007003
Iteration 5/1000 | Loss: 0.00005307
Iteration 6/1000 | Loss: 0.00003850
Iteration 7/1000 | Loss: 0.00003443
Iteration 8/1000 | Loss: 0.00003260
Iteration 9/1000 | Loss: 0.00003143
Iteration 10/1000 | Loss: 0.00003041
Iteration 11/1000 | Loss: 0.00061253
Iteration 12/1000 | Loss: 0.00060942
Iteration 13/1000 | Loss: 0.00004475
Iteration 14/1000 | Loss: 0.00003565
Iteration 15/1000 | Loss: 0.00002935
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00002336
Iteration 18/1000 | Loss: 0.00002257
Iteration 19/1000 | Loss: 0.00002212
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002143
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002108
Iteration 24/1000 | Loss: 0.00002103
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002099
Iteration 27/1000 | Loss: 0.00002098
Iteration 28/1000 | Loss: 0.00002096
Iteration 29/1000 | Loss: 0.00002095
Iteration 30/1000 | Loss: 0.00002094
Iteration 31/1000 | Loss: 0.00002092
Iteration 32/1000 | Loss: 0.00002092
Iteration 33/1000 | Loss: 0.00002091
Iteration 34/1000 | Loss: 0.00002090
Iteration 35/1000 | Loss: 0.00002090
Iteration 36/1000 | Loss: 0.00002089
Iteration 37/1000 | Loss: 0.00002089
Iteration 38/1000 | Loss: 0.00002089
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00002087
Iteration 41/1000 | Loss: 0.00002087
Iteration 42/1000 | Loss: 0.00002086
Iteration 43/1000 | Loss: 0.00002085
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002084
Iteration 46/1000 | Loss: 0.00002084
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002081
Iteration 50/1000 | Loss: 0.00002081
Iteration 51/1000 | Loss: 0.00002081
Iteration 52/1000 | Loss: 0.00002081
Iteration 53/1000 | Loss: 0.00002081
Iteration 54/1000 | Loss: 0.00002080
Iteration 55/1000 | Loss: 0.00002080
Iteration 56/1000 | Loss: 0.00002080
Iteration 57/1000 | Loss: 0.00002080
Iteration 58/1000 | Loss: 0.00002080
Iteration 59/1000 | Loss: 0.00002080
Iteration 60/1000 | Loss: 0.00002080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [2.0803008737857454e-05, 2.0803008737857454e-05, 2.0803008737857454e-05, 2.0803008737857454e-05, 2.0803008737857454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0803008737857454e-05

Optimization complete. Final v2v error: 3.8195104598999023 mm

Highest mean error: 4.197468280792236 mm for frame 91

Lowest mean error: 3.435608386993408 mm for frame 65

Saving results

Total time: 92.89217209815979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473531
Iteration 2/25 | Loss: 0.00127309
Iteration 3/25 | Loss: 0.00117504
Iteration 4/25 | Loss: 0.00114582
Iteration 5/25 | Loss: 0.00114152
Iteration 6/25 | Loss: 0.00114020
Iteration 7/25 | Loss: 0.00113977
Iteration 8/25 | Loss: 0.00113977
Iteration 9/25 | Loss: 0.00113977
Iteration 10/25 | Loss: 0.00113977
Iteration 11/25 | Loss: 0.00113977
Iteration 12/25 | Loss: 0.00113977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011397714260965586, 0.0011397714260965586, 0.0011397714260965586, 0.0011397714260965586, 0.0011397714260965586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011397714260965586

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47083759
Iteration 2/25 | Loss: 0.00039395
Iteration 3/25 | Loss: 0.00039395
Iteration 4/25 | Loss: 0.00039395
Iteration 5/25 | Loss: 0.00039395
Iteration 6/25 | Loss: 0.00039395
Iteration 7/25 | Loss: 0.00039394
Iteration 8/25 | Loss: 0.00039394
Iteration 9/25 | Loss: 0.00039394
Iteration 10/25 | Loss: 0.00039394
Iteration 11/25 | Loss: 0.00039394
Iteration 12/25 | Loss: 0.00039394
Iteration 13/25 | Loss: 0.00039394
Iteration 14/25 | Loss: 0.00039394
Iteration 15/25 | Loss: 0.00039394
Iteration 16/25 | Loss: 0.00039394
Iteration 17/25 | Loss: 0.00039394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003939442103728652, 0.0003939442103728652, 0.0003939442103728652, 0.0003939442103728652, 0.0003939442103728652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003939442103728652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039394
Iteration 2/1000 | Loss: 0.00003064
Iteration 3/1000 | Loss: 0.00002239
Iteration 4/1000 | Loss: 0.00002035
Iteration 5/1000 | Loss: 0.00001931
Iteration 6/1000 | Loss: 0.00001885
Iteration 7/1000 | Loss: 0.00001854
Iteration 8/1000 | Loss: 0.00001844
Iteration 9/1000 | Loss: 0.00001839
Iteration 10/1000 | Loss: 0.00001838
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001830
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00001828
Iteration 17/1000 | Loss: 0.00001828
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001825
Iteration 22/1000 | Loss: 0.00001824
Iteration 23/1000 | Loss: 0.00001824
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001823
Iteration 26/1000 | Loss: 0.00001823
Iteration 27/1000 | Loss: 0.00001823
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001822
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001821
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001820
Iteration 35/1000 | Loss: 0.00001820
Iteration 36/1000 | Loss: 0.00001819
Iteration 37/1000 | Loss: 0.00001819
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001818
Iteration 40/1000 | Loss: 0.00001818
Iteration 41/1000 | Loss: 0.00001817
Iteration 42/1000 | Loss: 0.00001817
Iteration 43/1000 | Loss: 0.00001816
Iteration 44/1000 | Loss: 0.00001816
Iteration 45/1000 | Loss: 0.00001816
Iteration 46/1000 | Loss: 0.00001815
Iteration 47/1000 | Loss: 0.00001815
Iteration 48/1000 | Loss: 0.00001815
Iteration 49/1000 | Loss: 0.00001815
Iteration 50/1000 | Loss: 0.00001815
Iteration 51/1000 | Loss: 0.00001814
Iteration 52/1000 | Loss: 0.00001814
Iteration 53/1000 | Loss: 0.00001814
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001813
Iteration 56/1000 | Loss: 0.00001813
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001812
Iteration 59/1000 | Loss: 0.00001812
Iteration 60/1000 | Loss: 0.00001811
Iteration 61/1000 | Loss: 0.00001811
Iteration 62/1000 | Loss: 0.00001811
Iteration 63/1000 | Loss: 0.00001811
Iteration 64/1000 | Loss: 0.00001810
Iteration 65/1000 | Loss: 0.00001810
Iteration 66/1000 | Loss: 0.00001810
Iteration 67/1000 | Loss: 0.00001810
Iteration 68/1000 | Loss: 0.00001810
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00001810
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001808
Iteration 74/1000 | Loss: 0.00001808
Iteration 75/1000 | Loss: 0.00001808
Iteration 76/1000 | Loss: 0.00001808
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001807
Iteration 88/1000 | Loss: 0.00001807
Iteration 89/1000 | Loss: 0.00001807
Iteration 90/1000 | Loss: 0.00001807
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001807
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001807
Iteration 95/1000 | Loss: 0.00001807
Iteration 96/1000 | Loss: 0.00001807
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001807
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001807
Iteration 104/1000 | Loss: 0.00001807
Iteration 105/1000 | Loss: 0.00001807
Iteration 106/1000 | Loss: 0.00001807
Iteration 107/1000 | Loss: 0.00001807
Iteration 108/1000 | Loss: 0.00001807
Iteration 109/1000 | Loss: 0.00001807
Iteration 110/1000 | Loss: 0.00001807
Iteration 111/1000 | Loss: 0.00001807
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.8073187675327063e-05, 1.8073187675327063e-05, 1.8073187675327063e-05, 1.8073187675327063e-05, 1.8073187675327063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8073187675327063e-05

Optimization complete. Final v2v error: 3.589001178741455 mm

Highest mean error: 4.007338047027588 mm for frame 79

Lowest mean error: 3.4139363765716553 mm for frame 63

Saving results

Total time: 29.09172224998474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451229
Iteration 2/25 | Loss: 0.00132744
Iteration 3/25 | Loss: 0.00116736
Iteration 4/25 | Loss: 0.00114575
Iteration 5/25 | Loss: 0.00114180
Iteration 6/25 | Loss: 0.00114103
Iteration 7/25 | Loss: 0.00114096
Iteration 8/25 | Loss: 0.00114096
Iteration 9/25 | Loss: 0.00114096
Iteration 10/25 | Loss: 0.00114096
Iteration 11/25 | Loss: 0.00114096
Iteration 12/25 | Loss: 0.00114096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011409639846533537, 0.0011409639846533537, 0.0011409639846533537, 0.0011409639846533537, 0.0011409639846533537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011409639846533537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47267222
Iteration 2/25 | Loss: 0.00042164
Iteration 3/25 | Loss: 0.00042164
Iteration 4/25 | Loss: 0.00042164
Iteration 5/25 | Loss: 0.00042164
Iteration 6/25 | Loss: 0.00042164
Iteration 7/25 | Loss: 0.00042164
Iteration 8/25 | Loss: 0.00042164
Iteration 9/25 | Loss: 0.00042164
Iteration 10/25 | Loss: 0.00042164
Iteration 11/25 | Loss: 0.00042164
Iteration 12/25 | Loss: 0.00042164
Iteration 13/25 | Loss: 0.00042164
Iteration 14/25 | Loss: 0.00042164
Iteration 15/25 | Loss: 0.00042164
Iteration 16/25 | Loss: 0.00042164
Iteration 17/25 | Loss: 0.00042164
Iteration 18/25 | Loss: 0.00042164
Iteration 19/25 | Loss: 0.00042164
Iteration 20/25 | Loss: 0.00042164
Iteration 21/25 | Loss: 0.00042164
Iteration 22/25 | Loss: 0.00042164
Iteration 23/25 | Loss: 0.00042164
Iteration 24/25 | Loss: 0.00042164
Iteration 25/25 | Loss: 0.00042164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042164
Iteration 2/1000 | Loss: 0.00003489
Iteration 3/1000 | Loss: 0.00002071
Iteration 4/1000 | Loss: 0.00001851
Iteration 5/1000 | Loss: 0.00001696
Iteration 6/1000 | Loss: 0.00001612
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001534
Iteration 10/1000 | Loss: 0.00001532
Iteration 11/1000 | Loss: 0.00001525
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001519
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001509
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001504
Iteration 21/1000 | Loss: 0.00001503
Iteration 22/1000 | Loss: 0.00001501
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001500
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001499
Iteration 27/1000 | Loss: 0.00001498
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001495
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001494
Iteration 36/1000 | Loss: 0.00001494
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001490
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001489
Iteration 53/1000 | Loss: 0.00001489
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001486
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001484
Iteration 69/1000 | Loss: 0.00001483
Iteration 70/1000 | Loss: 0.00001483
Iteration 71/1000 | Loss: 0.00001483
Iteration 72/1000 | Loss: 0.00001483
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001482
Iteration 79/1000 | Loss: 0.00001482
Iteration 80/1000 | Loss: 0.00001482
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001482
Iteration 86/1000 | Loss: 0.00001481
Iteration 87/1000 | Loss: 0.00001481
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001479
Iteration 94/1000 | Loss: 0.00001479
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001479
Iteration 99/1000 | Loss: 0.00001479
Iteration 100/1000 | Loss: 0.00001479
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001478
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001477
Iteration 108/1000 | Loss: 0.00001477
Iteration 109/1000 | Loss: 0.00001477
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001477
Iteration 112/1000 | Loss: 0.00001477
Iteration 113/1000 | Loss: 0.00001477
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001476
Iteration 117/1000 | Loss: 0.00001476
Iteration 118/1000 | Loss: 0.00001476
Iteration 119/1000 | Loss: 0.00001476
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001475
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001472
Iteration 145/1000 | Loss: 0.00001472
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001472
Iteration 152/1000 | Loss: 0.00001472
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001471
Iteration 155/1000 | Loss: 0.00001471
Iteration 156/1000 | Loss: 0.00001471
Iteration 157/1000 | Loss: 0.00001471
Iteration 158/1000 | Loss: 0.00001471
Iteration 159/1000 | Loss: 0.00001471
Iteration 160/1000 | Loss: 0.00001471
Iteration 161/1000 | Loss: 0.00001471
Iteration 162/1000 | Loss: 0.00001471
Iteration 163/1000 | Loss: 0.00001471
Iteration 164/1000 | Loss: 0.00001471
Iteration 165/1000 | Loss: 0.00001471
Iteration 166/1000 | Loss: 0.00001470
Iteration 167/1000 | Loss: 0.00001470
Iteration 168/1000 | Loss: 0.00001470
Iteration 169/1000 | Loss: 0.00001470
Iteration 170/1000 | Loss: 0.00001470
Iteration 171/1000 | Loss: 0.00001470
Iteration 172/1000 | Loss: 0.00001470
Iteration 173/1000 | Loss: 0.00001470
Iteration 174/1000 | Loss: 0.00001470
Iteration 175/1000 | Loss: 0.00001470
Iteration 176/1000 | Loss: 0.00001470
Iteration 177/1000 | Loss: 0.00001470
Iteration 178/1000 | Loss: 0.00001470
Iteration 179/1000 | Loss: 0.00001470
Iteration 180/1000 | Loss: 0.00001469
Iteration 181/1000 | Loss: 0.00001469
Iteration 182/1000 | Loss: 0.00001469
Iteration 183/1000 | Loss: 0.00001469
Iteration 184/1000 | Loss: 0.00001469
Iteration 185/1000 | Loss: 0.00001469
Iteration 186/1000 | Loss: 0.00001469
Iteration 187/1000 | Loss: 0.00001469
Iteration 188/1000 | Loss: 0.00001468
Iteration 189/1000 | Loss: 0.00001468
Iteration 190/1000 | Loss: 0.00001468
Iteration 191/1000 | Loss: 0.00001468
Iteration 192/1000 | Loss: 0.00001468
Iteration 193/1000 | Loss: 0.00001468
Iteration 194/1000 | Loss: 0.00001468
Iteration 195/1000 | Loss: 0.00001468
Iteration 196/1000 | Loss: 0.00001468
Iteration 197/1000 | Loss: 0.00001468
Iteration 198/1000 | Loss: 0.00001468
Iteration 199/1000 | Loss: 0.00001468
Iteration 200/1000 | Loss: 0.00001467
Iteration 201/1000 | Loss: 0.00001467
Iteration 202/1000 | Loss: 0.00001467
Iteration 203/1000 | Loss: 0.00001467
Iteration 204/1000 | Loss: 0.00001467
Iteration 205/1000 | Loss: 0.00001467
Iteration 206/1000 | Loss: 0.00001467
Iteration 207/1000 | Loss: 0.00001467
Iteration 208/1000 | Loss: 0.00001467
Iteration 209/1000 | Loss: 0.00001467
Iteration 210/1000 | Loss: 0.00001467
Iteration 211/1000 | Loss: 0.00001467
Iteration 212/1000 | Loss: 0.00001467
Iteration 213/1000 | Loss: 0.00001467
Iteration 214/1000 | Loss: 0.00001467
Iteration 215/1000 | Loss: 0.00001467
Iteration 216/1000 | Loss: 0.00001467
Iteration 217/1000 | Loss: 0.00001466
Iteration 218/1000 | Loss: 0.00001466
Iteration 219/1000 | Loss: 0.00001466
Iteration 220/1000 | Loss: 0.00001466
Iteration 221/1000 | Loss: 0.00001466
Iteration 222/1000 | Loss: 0.00001466
Iteration 223/1000 | Loss: 0.00001466
Iteration 224/1000 | Loss: 0.00001466
Iteration 225/1000 | Loss: 0.00001466
Iteration 226/1000 | Loss: 0.00001466
Iteration 227/1000 | Loss: 0.00001466
Iteration 228/1000 | Loss: 0.00001466
Iteration 229/1000 | Loss: 0.00001466
Iteration 230/1000 | Loss: 0.00001465
Iteration 231/1000 | Loss: 0.00001465
Iteration 232/1000 | Loss: 0.00001465
Iteration 233/1000 | Loss: 0.00001465
Iteration 234/1000 | Loss: 0.00001465
Iteration 235/1000 | Loss: 0.00001465
Iteration 236/1000 | Loss: 0.00001465
Iteration 237/1000 | Loss: 0.00001465
Iteration 238/1000 | Loss: 0.00001465
Iteration 239/1000 | Loss: 0.00001465
Iteration 240/1000 | Loss: 0.00001465
Iteration 241/1000 | Loss: 0.00001465
Iteration 242/1000 | Loss: 0.00001465
Iteration 243/1000 | Loss: 0.00001465
Iteration 244/1000 | Loss: 0.00001465
Iteration 245/1000 | Loss: 0.00001465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.464987508370541e-05, 1.464987508370541e-05, 1.464987508370541e-05, 1.464987508370541e-05, 1.464987508370541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.464987508370541e-05

Optimization complete. Final v2v error: 3.247748851776123 mm

Highest mean error: 4.21202278137207 mm for frame 62

Lowest mean error: 2.9264562129974365 mm for frame 4

Saving results

Total time: 39.148842334747314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756149
Iteration 2/25 | Loss: 0.00162354
Iteration 3/25 | Loss: 0.00122900
Iteration 4/25 | Loss: 0.00117756
Iteration 5/25 | Loss: 0.00116769
Iteration 6/25 | Loss: 0.00118184
Iteration 7/25 | Loss: 0.00114705
Iteration 8/25 | Loss: 0.00113855
Iteration 9/25 | Loss: 0.00113701
Iteration 10/25 | Loss: 0.00113946
Iteration 11/25 | Loss: 0.00113681
Iteration 12/25 | Loss: 0.00113507
Iteration 13/25 | Loss: 0.00113472
Iteration 14/25 | Loss: 0.00113466
Iteration 15/25 | Loss: 0.00113466
Iteration 16/25 | Loss: 0.00113466
Iteration 17/25 | Loss: 0.00113466
Iteration 18/25 | Loss: 0.00113465
Iteration 19/25 | Loss: 0.00113465
Iteration 20/25 | Loss: 0.00113465
Iteration 21/25 | Loss: 0.00113465
Iteration 22/25 | Loss: 0.00113465
Iteration 23/25 | Loss: 0.00113465
Iteration 24/25 | Loss: 0.00113465
Iteration 25/25 | Loss: 0.00113465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37853217
Iteration 2/25 | Loss: 0.00095824
Iteration 3/25 | Loss: 0.00095824
Iteration 4/25 | Loss: 0.00095824
Iteration 5/25 | Loss: 0.00095824
Iteration 6/25 | Loss: 0.00095824
Iteration 7/25 | Loss: 0.00095824
Iteration 8/25 | Loss: 0.00095823
Iteration 9/25 | Loss: 0.00095823
Iteration 10/25 | Loss: 0.00095823
Iteration 11/25 | Loss: 0.00095823
Iteration 12/25 | Loss: 0.00095823
Iteration 13/25 | Loss: 0.00095823
Iteration 14/25 | Loss: 0.00095823
Iteration 15/25 | Loss: 0.00095823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000958234362769872, 0.000958234362769872, 0.000958234362769872, 0.000958234362769872, 0.000958234362769872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000958234362769872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095823
Iteration 2/1000 | Loss: 0.00015725
Iteration 3/1000 | Loss: 0.00007900
Iteration 4/1000 | Loss: 0.00006047
Iteration 5/1000 | Loss: 0.00005267
Iteration 6/1000 | Loss: 0.00003460
Iteration 7/1000 | Loss: 0.00002986
Iteration 8/1000 | Loss: 0.00003911
Iteration 9/1000 | Loss: 0.00018153
Iteration 10/1000 | Loss: 0.00002965
Iteration 11/1000 | Loss: 0.00002656
Iteration 12/1000 | Loss: 0.00003437
Iteration 13/1000 | Loss: 0.00003090
Iteration 14/1000 | Loss: 0.00003416
Iteration 15/1000 | Loss: 0.00002804
Iteration 16/1000 | Loss: 0.00002920
Iteration 17/1000 | Loss: 0.00003874
Iteration 18/1000 | Loss: 0.00002930
Iteration 19/1000 | Loss: 0.00002937
Iteration 20/1000 | Loss: 0.00003305
Iteration 21/1000 | Loss: 0.00003185
Iteration 22/1000 | Loss: 0.00005418
Iteration 23/1000 | Loss: 0.00016568
Iteration 24/1000 | Loss: 0.00003912
Iteration 25/1000 | Loss: 0.00002645
Iteration 26/1000 | Loss: 0.00002243
Iteration 27/1000 | Loss: 0.00004159
Iteration 28/1000 | Loss: 0.00002062
Iteration 29/1000 | Loss: 0.00001918
Iteration 30/1000 | Loss: 0.00001836
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001720
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00001681
Iteration 36/1000 | Loss: 0.00001680
Iteration 37/1000 | Loss: 0.00001679
Iteration 38/1000 | Loss: 0.00001678
Iteration 39/1000 | Loss: 0.00001670
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001653
Iteration 44/1000 | Loss: 0.00001652
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001648
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001643
Iteration 56/1000 | Loss: 0.00001643
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001630
Iteration 61/1000 | Loss: 0.00001630
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001624
Iteration 77/1000 | Loss: 0.00001624
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001621
Iteration 103/1000 | Loss: 0.00001621
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001620
Iteration 107/1000 | Loss: 0.00001620
Iteration 108/1000 | Loss: 0.00001620
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001620
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00001620
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001619
Iteration 126/1000 | Loss: 0.00001619
Iteration 127/1000 | Loss: 0.00001619
Iteration 128/1000 | Loss: 0.00001619
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001619
Iteration 132/1000 | Loss: 0.00001619
Iteration 133/1000 | Loss: 0.00001619
Iteration 134/1000 | Loss: 0.00001619
Iteration 135/1000 | Loss: 0.00001619
Iteration 136/1000 | Loss: 0.00001619
Iteration 137/1000 | Loss: 0.00001619
Iteration 138/1000 | Loss: 0.00001619
Iteration 139/1000 | Loss: 0.00001619
Iteration 140/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.6190884707611986e-05, 1.6190884707611986e-05, 1.6190884707611986e-05, 1.6190884707611986e-05, 1.6190884707611986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6190884707611986e-05

Optimization complete. Final v2v error: 3.431478500366211 mm

Highest mean error: 4.371123313903809 mm for frame 120

Lowest mean error: 3.1065356731414795 mm for frame 154

Saving results

Total time: 93.89236259460449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_nl_2375/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_nl_2375/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119701
Iteration 2/25 | Loss: 0.01119700
Iteration 3/25 | Loss: 0.01119700
Iteration 4/25 | Loss: 0.00420276
Iteration 5/25 | Loss: 0.00227100
Iteration 6/25 | Loss: 0.00197143
Iteration 7/25 | Loss: 0.00196210
Iteration 8/25 | Loss: 0.00173100
Iteration 9/25 | Loss: 0.00169709
Iteration 10/25 | Loss: 0.00162320
Iteration 11/25 | Loss: 0.00148708
Iteration 12/25 | Loss: 0.00145045
Iteration 13/25 | Loss: 0.00143793
Iteration 14/25 | Loss: 0.00144139
Iteration 15/25 | Loss: 0.00140517
Iteration 16/25 | Loss: 0.00139875
Iteration 17/25 | Loss: 0.00138727
Iteration 18/25 | Loss: 0.00138504
Iteration 19/25 | Loss: 0.00137837
Iteration 20/25 | Loss: 0.00137960
Iteration 21/25 | Loss: 0.00137260
Iteration 22/25 | Loss: 0.00137927
Iteration 23/25 | Loss: 0.00137358
Iteration 24/25 | Loss: 0.00137453
Iteration 25/25 | Loss: 0.00137279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39699578
Iteration 2/25 | Loss: 0.00284500
Iteration 3/25 | Loss: 0.00284499
Iteration 4/25 | Loss: 0.00284499
Iteration 5/25 | Loss: 0.00284499
Iteration 6/25 | Loss: 0.00284499
Iteration 7/25 | Loss: 0.00284499
Iteration 8/25 | Loss: 0.00284499
Iteration 9/25 | Loss: 0.00284499
Iteration 10/25 | Loss: 0.00284499
Iteration 11/25 | Loss: 0.00284499
Iteration 12/25 | Loss: 0.00284499
Iteration 13/25 | Loss: 0.00284499
Iteration 14/25 | Loss: 0.00284499
Iteration 15/25 | Loss: 0.00284499
Iteration 16/25 | Loss: 0.00284499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0028449895326048136, 0.0028449895326048136, 0.0028449895326048136, 0.0028449895326048136, 0.0028449895326048136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028449895326048136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284499
Iteration 2/1000 | Loss: 0.00113090
Iteration 3/1000 | Loss: 0.00144384
Iteration 4/1000 | Loss: 0.00092079
Iteration 5/1000 | Loss: 0.00082913
Iteration 6/1000 | Loss: 0.00240499
Iteration 7/1000 | Loss: 0.00491565
Iteration 8/1000 | Loss: 0.00705326
Iteration 9/1000 | Loss: 0.00415672
Iteration 10/1000 | Loss: 0.00084845
Iteration 11/1000 | Loss: 0.00048911
Iteration 12/1000 | Loss: 0.00029795
Iteration 13/1000 | Loss: 0.00094093
Iteration 14/1000 | Loss: 0.00123651
Iteration 15/1000 | Loss: 0.00054663
Iteration 16/1000 | Loss: 0.00033128
Iteration 17/1000 | Loss: 0.00035142
Iteration 18/1000 | Loss: 0.00076837
Iteration 19/1000 | Loss: 0.00044889
Iteration 20/1000 | Loss: 0.00049182
Iteration 21/1000 | Loss: 0.00019903
Iteration 22/1000 | Loss: 0.00030112
Iteration 23/1000 | Loss: 0.00296798
Iteration 24/1000 | Loss: 0.00243192
Iteration 25/1000 | Loss: 0.00235405
Iteration 26/1000 | Loss: 0.00317062
Iteration 27/1000 | Loss: 0.00637051
Iteration 28/1000 | Loss: 0.00206424
Iteration 29/1000 | Loss: 0.00279343
Iteration 30/1000 | Loss: 0.00163309
Iteration 31/1000 | Loss: 0.00094820
Iteration 32/1000 | Loss: 0.00069940
Iteration 33/1000 | Loss: 0.00056125
Iteration 34/1000 | Loss: 0.00046784
Iteration 35/1000 | Loss: 0.00125754
Iteration 36/1000 | Loss: 0.00180776
Iteration 37/1000 | Loss: 0.00070017
Iteration 38/1000 | Loss: 0.00151809
Iteration 39/1000 | Loss: 0.00030153
Iteration 40/1000 | Loss: 0.00067512
Iteration 41/1000 | Loss: 0.00042302
Iteration 42/1000 | Loss: 0.00068225
Iteration 43/1000 | Loss: 0.00072596
Iteration 44/1000 | Loss: 0.00076181
Iteration 45/1000 | Loss: 0.00082108
Iteration 46/1000 | Loss: 0.00020576
Iteration 47/1000 | Loss: 0.00029177
Iteration 48/1000 | Loss: 0.00014096
Iteration 49/1000 | Loss: 0.00014073
Iteration 50/1000 | Loss: 0.00033360
Iteration 51/1000 | Loss: 0.00131214
Iteration 52/1000 | Loss: 0.00089142
Iteration 53/1000 | Loss: 0.00013662
Iteration 54/1000 | Loss: 0.00041451
Iteration 55/1000 | Loss: 0.00011279
Iteration 56/1000 | Loss: 0.00035794
Iteration 57/1000 | Loss: 0.00104953
Iteration 58/1000 | Loss: 0.00028875
Iteration 59/1000 | Loss: 0.00015058
Iteration 60/1000 | Loss: 0.00018129
Iteration 61/1000 | Loss: 0.00009680
Iteration 62/1000 | Loss: 0.00010926
Iteration 63/1000 | Loss: 0.00036562
Iteration 64/1000 | Loss: 0.00019498
Iteration 65/1000 | Loss: 0.00030889
Iteration 66/1000 | Loss: 0.00013148
Iteration 67/1000 | Loss: 0.00009300
Iteration 68/1000 | Loss: 0.00035306
Iteration 69/1000 | Loss: 0.00065329
Iteration 70/1000 | Loss: 0.00074790
Iteration 71/1000 | Loss: 0.00058093
Iteration 72/1000 | Loss: 0.00014297
Iteration 73/1000 | Loss: 0.00082673
Iteration 74/1000 | Loss: 0.00028862
Iteration 75/1000 | Loss: 0.00067596
Iteration 76/1000 | Loss: 0.00038586
Iteration 77/1000 | Loss: 0.00111672
Iteration 78/1000 | Loss: 0.00016844
Iteration 79/1000 | Loss: 0.00011052
Iteration 80/1000 | Loss: 0.00028799
Iteration 81/1000 | Loss: 0.00008551
Iteration 82/1000 | Loss: 0.00027081
Iteration 83/1000 | Loss: 0.00025118
Iteration 84/1000 | Loss: 0.00022386
Iteration 85/1000 | Loss: 0.00021616
Iteration 86/1000 | Loss: 0.00018873
Iteration 87/1000 | Loss: 0.00016478
Iteration 88/1000 | Loss: 0.00012216
Iteration 89/1000 | Loss: 0.00009304
Iteration 90/1000 | Loss: 0.00009144
Iteration 91/1000 | Loss: 0.00068645
Iteration 92/1000 | Loss: 0.00009968
Iteration 93/1000 | Loss: 0.00009477
Iteration 94/1000 | Loss: 0.00047694
Iteration 95/1000 | Loss: 0.00009846
Iteration 96/1000 | Loss: 0.00027272
Iteration 97/1000 | Loss: 0.00070115
Iteration 98/1000 | Loss: 0.00024880
Iteration 99/1000 | Loss: 0.00025911
Iteration 100/1000 | Loss: 0.00022954
Iteration 101/1000 | Loss: 0.00020691
Iteration 102/1000 | Loss: 0.00085950
Iteration 103/1000 | Loss: 0.00030450
Iteration 104/1000 | Loss: 0.00045527
Iteration 105/1000 | Loss: 0.00015845
Iteration 106/1000 | Loss: 0.00076924
Iteration 107/1000 | Loss: 0.00015935
Iteration 108/1000 | Loss: 0.00008296
Iteration 109/1000 | Loss: 0.00044750
Iteration 110/1000 | Loss: 0.00030623
Iteration 111/1000 | Loss: 0.00029834
Iteration 112/1000 | Loss: 0.00012177
Iteration 113/1000 | Loss: 0.00011560
Iteration 114/1000 | Loss: 0.00052927
Iteration 115/1000 | Loss: 0.00053158
Iteration 116/1000 | Loss: 0.00008647
Iteration 117/1000 | Loss: 0.00112951
Iteration 118/1000 | Loss: 0.00053483
Iteration 119/1000 | Loss: 0.00047776
Iteration 120/1000 | Loss: 0.00026564
Iteration 121/1000 | Loss: 0.00051998
Iteration 122/1000 | Loss: 0.00038473
Iteration 123/1000 | Loss: 0.00034139
Iteration 124/1000 | Loss: 0.00035962
Iteration 125/1000 | Loss: 0.00034376
Iteration 126/1000 | Loss: 0.00047260
Iteration 127/1000 | Loss: 0.00008380
Iteration 128/1000 | Loss: 0.00008055
Iteration 129/1000 | Loss: 0.00006988
Iteration 130/1000 | Loss: 0.00008223
Iteration 131/1000 | Loss: 0.00007530
Iteration 132/1000 | Loss: 0.00007933
Iteration 133/1000 | Loss: 0.00010902
Iteration 134/1000 | Loss: 0.00007176
Iteration 135/1000 | Loss: 0.00007299
Iteration 136/1000 | Loss: 0.00007666
Iteration 137/1000 | Loss: 0.00007828
Iteration 138/1000 | Loss: 0.00007148
Iteration 139/1000 | Loss: 0.00043182
Iteration 140/1000 | Loss: 0.00054923
Iteration 141/1000 | Loss: 0.00043634
Iteration 142/1000 | Loss: 0.00008164
Iteration 143/1000 | Loss: 0.00057181
Iteration 144/1000 | Loss: 0.00008518
Iteration 145/1000 | Loss: 0.00015267
Iteration 146/1000 | Loss: 0.00007671
Iteration 147/1000 | Loss: 0.00006775
Iteration 148/1000 | Loss: 0.00006217
Iteration 149/1000 | Loss: 0.00006729
Iteration 150/1000 | Loss: 0.00005768
Iteration 151/1000 | Loss: 0.00006324
Iteration 152/1000 | Loss: 0.00005760
Iteration 153/1000 | Loss: 0.00005678
Iteration 154/1000 | Loss: 0.00005690
Iteration 155/1000 | Loss: 0.00020405
Iteration 156/1000 | Loss: 0.00009237
Iteration 157/1000 | Loss: 0.00013195
Iteration 158/1000 | Loss: 0.00016856
Iteration 159/1000 | Loss: 0.00008068
Iteration 160/1000 | Loss: 0.00042068
Iteration 161/1000 | Loss: 0.00010513
Iteration 162/1000 | Loss: 0.00035245
Iteration 163/1000 | Loss: 0.00015475
Iteration 164/1000 | Loss: 0.00028392
Iteration 165/1000 | Loss: 0.00010423
Iteration 166/1000 | Loss: 0.00009464
Iteration 167/1000 | Loss: 0.00026837
Iteration 168/1000 | Loss: 0.00035997
Iteration 169/1000 | Loss: 0.00012188
Iteration 170/1000 | Loss: 0.00061835
Iteration 171/1000 | Loss: 0.00035250
Iteration 172/1000 | Loss: 0.00046952
Iteration 173/1000 | Loss: 0.00041441
Iteration 174/1000 | Loss: 0.00031742
Iteration 175/1000 | Loss: 0.00014282
Iteration 176/1000 | Loss: 0.00035274
Iteration 177/1000 | Loss: 0.00029710
Iteration 178/1000 | Loss: 0.00041881
Iteration 179/1000 | Loss: 0.00020676
Iteration 180/1000 | Loss: 0.00095620
Iteration 181/1000 | Loss: 0.00043016
Iteration 182/1000 | Loss: 0.00021343
Iteration 183/1000 | Loss: 0.00005626
Iteration 184/1000 | Loss: 0.00005669
Iteration 185/1000 | Loss: 0.00005570
Iteration 186/1000 | Loss: 0.00005717
Iteration 187/1000 | Loss: 0.00005517
Iteration 188/1000 | Loss: 0.00005517
Iteration 189/1000 | Loss: 0.00005517
Iteration 190/1000 | Loss: 0.00005517
Iteration 191/1000 | Loss: 0.00005517
Iteration 192/1000 | Loss: 0.00005517
Iteration 193/1000 | Loss: 0.00005517
Iteration 194/1000 | Loss: 0.00005517
Iteration 195/1000 | Loss: 0.00005517
Iteration 196/1000 | Loss: 0.00005516
Iteration 197/1000 | Loss: 0.00005516
Iteration 198/1000 | Loss: 0.00005732
Iteration 199/1000 | Loss: 0.00005513
Iteration 200/1000 | Loss: 0.00005513
Iteration 201/1000 | Loss: 0.00005513
Iteration 202/1000 | Loss: 0.00005513
Iteration 203/1000 | Loss: 0.00005513
Iteration 204/1000 | Loss: 0.00005513
Iteration 205/1000 | Loss: 0.00005513
Iteration 206/1000 | Loss: 0.00005512
Iteration 207/1000 | Loss: 0.00005512
Iteration 208/1000 | Loss: 0.00005512
Iteration 209/1000 | Loss: 0.00005603
Iteration 210/1000 | Loss: 0.00006146
Iteration 211/1000 | Loss: 0.00005844
Iteration 212/1000 | Loss: 0.00005842
Iteration 213/1000 | Loss: 0.00017386
Iteration 214/1000 | Loss: 0.00007100
Iteration 215/1000 | Loss: 0.00005588
Iteration 216/1000 | Loss: 0.00005513
Iteration 217/1000 | Loss: 0.00005504
Iteration 218/1000 | Loss: 0.00005504
Iteration 219/1000 | Loss: 0.00005504
Iteration 220/1000 | Loss: 0.00005504
Iteration 221/1000 | Loss: 0.00005504
Iteration 222/1000 | Loss: 0.00005504
Iteration 223/1000 | Loss: 0.00005503
Iteration 224/1000 | Loss: 0.00005503
Iteration 225/1000 | Loss: 0.00005503
Iteration 226/1000 | Loss: 0.00005503
Iteration 227/1000 | Loss: 0.00005503
Iteration 228/1000 | Loss: 0.00005503
Iteration 229/1000 | Loss: 0.00005503
Iteration 230/1000 | Loss: 0.00005503
Iteration 231/1000 | Loss: 0.00005503
Iteration 232/1000 | Loss: 0.00005503
Iteration 233/1000 | Loss: 0.00005502
Iteration 234/1000 | Loss: 0.00005502
Iteration 235/1000 | Loss: 0.00005502
Iteration 236/1000 | Loss: 0.00005502
Iteration 237/1000 | Loss: 0.00005501
Iteration 238/1000 | Loss: 0.00005501
Iteration 239/1000 | Loss: 0.00005501
Iteration 240/1000 | Loss: 0.00005501
Iteration 241/1000 | Loss: 0.00005501
Iteration 242/1000 | Loss: 0.00005501
Iteration 243/1000 | Loss: 0.00005501
Iteration 244/1000 | Loss: 0.00005501
Iteration 245/1000 | Loss: 0.00005501
Iteration 246/1000 | Loss: 0.00005501
Iteration 247/1000 | Loss: 0.00005501
Iteration 248/1000 | Loss: 0.00005501
Iteration 249/1000 | Loss: 0.00005501
Iteration 250/1000 | Loss: 0.00005501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [5.501231498783454e-05, 5.501231498783454e-05, 5.501231498783454e-05, 5.501231498783454e-05, 5.501231498783454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.501231498783454e-05

Optimization complete. Final v2v error: 4.085526466369629 mm

Highest mean error: 21.498817443847656 mm for frame 107

Lowest mean error: 2.6275851726531982 mm for frame 10

Saving results

Total time: 356.9244360923767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01112409
Iteration 2/25 | Loss: 0.00355843
Iteration 3/25 | Loss: 0.00183499
Iteration 4/25 | Loss: 0.00164451
Iteration 5/25 | Loss: 0.00157247
Iteration 6/25 | Loss: 0.00156963
Iteration 7/25 | Loss: 0.00163554
Iteration 8/25 | Loss: 0.00130172
Iteration 9/25 | Loss: 0.00125059
Iteration 10/25 | Loss: 0.00116937
Iteration 11/25 | Loss: 0.00115777
Iteration 12/25 | Loss: 0.00114876
Iteration 13/25 | Loss: 0.00114084
Iteration 14/25 | Loss: 0.00113654
Iteration 15/25 | Loss: 0.00113554
Iteration 16/25 | Loss: 0.00112937
Iteration 17/25 | Loss: 0.00113038
Iteration 18/25 | Loss: 0.00112879
Iteration 19/25 | Loss: 0.00113018
Iteration 20/25 | Loss: 0.00113083
Iteration 21/25 | Loss: 0.00112850
Iteration 22/25 | Loss: 0.00112850
Iteration 23/25 | Loss: 0.00112850
Iteration 24/25 | Loss: 0.00112849
Iteration 25/25 | Loss: 0.00112849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44602537
Iteration 2/25 | Loss: 0.00108385
Iteration 3/25 | Loss: 0.00094782
Iteration 4/25 | Loss: 0.00094727
Iteration 5/25 | Loss: 0.00093128
Iteration 6/25 | Loss: 0.00093128
Iteration 7/25 | Loss: 0.00093128
Iteration 8/25 | Loss: 0.00093128
Iteration 9/25 | Loss: 0.00093128
Iteration 10/25 | Loss: 0.00093128
Iteration 11/25 | Loss: 0.00093128
Iteration 12/25 | Loss: 0.00093128
Iteration 13/25 | Loss: 0.00093128
Iteration 14/25 | Loss: 0.00093128
Iteration 15/25 | Loss: 0.00093128
Iteration 16/25 | Loss: 0.00093128
Iteration 17/25 | Loss: 0.00093128
Iteration 18/25 | Loss: 0.00093128
Iteration 19/25 | Loss: 0.00093128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009312776383012533, 0.0009312776383012533, 0.0009312776383012533, 0.0009312776383012533, 0.0009312776383012533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009312776383012533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093128
Iteration 2/1000 | Loss: 0.00018190
Iteration 3/1000 | Loss: 0.00027647
Iteration 4/1000 | Loss: 0.00066764
Iteration 5/1000 | Loss: 0.00149443
Iteration 6/1000 | Loss: 0.00061859
Iteration 7/1000 | Loss: 0.00009926
Iteration 8/1000 | Loss: 0.00007014
Iteration 9/1000 | Loss: 0.00075233
Iteration 10/1000 | Loss: 0.00060012
Iteration 11/1000 | Loss: 0.00006706
Iteration 12/1000 | Loss: 0.00144574
Iteration 13/1000 | Loss: 0.00151427
Iteration 14/1000 | Loss: 0.00218174
Iteration 15/1000 | Loss: 0.00234763
Iteration 16/1000 | Loss: 0.00222779
Iteration 17/1000 | Loss: 0.00173008
Iteration 18/1000 | Loss: 0.00064970
Iteration 19/1000 | Loss: 0.00006608
Iteration 20/1000 | Loss: 0.00005501
Iteration 21/1000 | Loss: 0.00004750
Iteration 22/1000 | Loss: 0.00008615
Iteration 23/1000 | Loss: 0.00005972
Iteration 24/1000 | Loss: 0.00004725
Iteration 25/1000 | Loss: 0.00004304
Iteration 26/1000 | Loss: 0.00004061
Iteration 27/1000 | Loss: 0.00003317
Iteration 28/1000 | Loss: 0.00004532
Iteration 29/1000 | Loss: 0.00005378
Iteration 30/1000 | Loss: 0.00003199
Iteration 31/1000 | Loss: 0.00003166
Iteration 32/1000 | Loss: 0.00003132
Iteration 33/1000 | Loss: 0.00003116
Iteration 34/1000 | Loss: 0.00003108
Iteration 35/1000 | Loss: 0.00003103
Iteration 36/1000 | Loss: 0.00003101
Iteration 37/1000 | Loss: 0.00003100
Iteration 38/1000 | Loss: 0.00003100
Iteration 39/1000 | Loss: 0.00003099
Iteration 40/1000 | Loss: 0.00003093
Iteration 41/1000 | Loss: 0.00005136
Iteration 42/1000 | Loss: 0.00003088
Iteration 43/1000 | Loss: 0.00003088
Iteration 44/1000 | Loss: 0.00003087
Iteration 45/1000 | Loss: 0.00003087
Iteration 46/1000 | Loss: 0.00003087
Iteration 47/1000 | Loss: 0.00003087
Iteration 48/1000 | Loss: 0.00003086
Iteration 49/1000 | Loss: 0.00003086
Iteration 50/1000 | Loss: 0.00003086
Iteration 51/1000 | Loss: 0.00003086
Iteration 52/1000 | Loss: 0.00003086
Iteration 53/1000 | Loss: 0.00003086
Iteration 54/1000 | Loss: 0.00003085
Iteration 55/1000 | Loss: 0.00003085
Iteration 56/1000 | Loss: 0.00003085
Iteration 57/1000 | Loss: 0.00003084
Iteration 58/1000 | Loss: 0.00003084
Iteration 59/1000 | Loss: 0.00003084
Iteration 60/1000 | Loss: 0.00003084
Iteration 61/1000 | Loss: 0.00003084
Iteration 62/1000 | Loss: 0.00003084
Iteration 63/1000 | Loss: 0.00003084
Iteration 64/1000 | Loss: 0.00003084
Iteration 65/1000 | Loss: 0.00003084
Iteration 66/1000 | Loss: 0.00003084
Iteration 67/1000 | Loss: 0.00003083
Iteration 68/1000 | Loss: 0.00003083
Iteration 69/1000 | Loss: 0.00003083
Iteration 70/1000 | Loss: 0.00003083
Iteration 71/1000 | Loss: 0.00003083
Iteration 72/1000 | Loss: 0.00003083
Iteration 73/1000 | Loss: 0.00003082
Iteration 74/1000 | Loss: 0.00003082
Iteration 75/1000 | Loss: 0.00003082
Iteration 76/1000 | Loss: 0.00003082
Iteration 77/1000 | Loss: 0.00003082
Iteration 78/1000 | Loss: 0.00003082
Iteration 79/1000 | Loss: 0.00003082
Iteration 80/1000 | Loss: 0.00003082
Iteration 81/1000 | Loss: 0.00003082
Iteration 82/1000 | Loss: 0.00003082
Iteration 83/1000 | Loss: 0.00003082
Iteration 84/1000 | Loss: 0.00003081
Iteration 85/1000 | Loss: 0.00003081
Iteration 86/1000 | Loss: 0.00003081
Iteration 87/1000 | Loss: 0.00003081
Iteration 88/1000 | Loss: 0.00003081
Iteration 89/1000 | Loss: 0.00003081
Iteration 90/1000 | Loss: 0.00003081
Iteration 91/1000 | Loss: 0.00003081
Iteration 92/1000 | Loss: 0.00003081
Iteration 93/1000 | Loss: 0.00003081
Iteration 94/1000 | Loss: 0.00003081
Iteration 95/1000 | Loss: 0.00003081
Iteration 96/1000 | Loss: 0.00003081
Iteration 97/1000 | Loss: 0.00003081
Iteration 98/1000 | Loss: 0.00003081
Iteration 99/1000 | Loss: 0.00003080
Iteration 100/1000 | Loss: 0.00003080
Iteration 101/1000 | Loss: 0.00003080
Iteration 102/1000 | Loss: 0.00003080
Iteration 103/1000 | Loss: 0.00003079
Iteration 104/1000 | Loss: 0.00003079
Iteration 105/1000 | Loss: 0.00003079
Iteration 106/1000 | Loss: 0.00003079
Iteration 107/1000 | Loss: 0.00003079
Iteration 108/1000 | Loss: 0.00003079
Iteration 109/1000 | Loss: 0.00003079
Iteration 110/1000 | Loss: 0.00003079
Iteration 111/1000 | Loss: 0.00003079
Iteration 112/1000 | Loss: 0.00003079
Iteration 113/1000 | Loss: 0.00003079
Iteration 114/1000 | Loss: 0.00003079
Iteration 115/1000 | Loss: 0.00003079
Iteration 116/1000 | Loss: 0.00003079
Iteration 117/1000 | Loss: 0.00003079
Iteration 118/1000 | Loss: 0.00003078
Iteration 119/1000 | Loss: 0.00003078
Iteration 120/1000 | Loss: 0.00003078
Iteration 121/1000 | Loss: 0.00003078
Iteration 122/1000 | Loss: 0.00003078
Iteration 123/1000 | Loss: 0.00003078
Iteration 124/1000 | Loss: 0.00003078
Iteration 125/1000 | Loss: 0.00003078
Iteration 126/1000 | Loss: 0.00003078
Iteration 127/1000 | Loss: 0.00003078
Iteration 128/1000 | Loss: 0.00003078
Iteration 129/1000 | Loss: 0.00003078
Iteration 130/1000 | Loss: 0.00003078
Iteration 131/1000 | Loss: 0.00003078
Iteration 132/1000 | Loss: 0.00003078
Iteration 133/1000 | Loss: 0.00003078
Iteration 134/1000 | Loss: 0.00003078
Iteration 135/1000 | Loss: 0.00003078
Iteration 136/1000 | Loss: 0.00003078
Iteration 137/1000 | Loss: 0.00003078
Iteration 138/1000 | Loss: 0.00003078
Iteration 139/1000 | Loss: 0.00003078
Iteration 140/1000 | Loss: 0.00003077
Iteration 141/1000 | Loss: 0.00003077
Iteration 142/1000 | Loss: 0.00004752
Iteration 143/1000 | Loss: 0.00003441
Iteration 144/1000 | Loss: 0.00003855
Iteration 145/1000 | Loss: 0.00003245
Iteration 146/1000 | Loss: 0.00003076
Iteration 147/1000 | Loss: 0.00003076
Iteration 148/1000 | Loss: 0.00003076
Iteration 149/1000 | Loss: 0.00003076
Iteration 150/1000 | Loss: 0.00003076
Iteration 151/1000 | Loss: 0.00003076
Iteration 152/1000 | Loss: 0.00003076
Iteration 153/1000 | Loss: 0.00003076
Iteration 154/1000 | Loss: 0.00003076
Iteration 155/1000 | Loss: 0.00003076
Iteration 156/1000 | Loss: 0.00003076
Iteration 157/1000 | Loss: 0.00003076
Iteration 158/1000 | Loss: 0.00003076
Iteration 159/1000 | Loss: 0.00003076
Iteration 160/1000 | Loss: 0.00003076
Iteration 161/1000 | Loss: 0.00003076
Iteration 162/1000 | Loss: 0.00003076
Iteration 163/1000 | Loss: 0.00003076
Iteration 164/1000 | Loss: 0.00003076
Iteration 165/1000 | Loss: 0.00003076
Iteration 166/1000 | Loss: 0.00003076
Iteration 167/1000 | Loss: 0.00003076
Iteration 168/1000 | Loss: 0.00003076
Iteration 169/1000 | Loss: 0.00003076
Iteration 170/1000 | Loss: 0.00003076
Iteration 171/1000 | Loss: 0.00003076
Iteration 172/1000 | Loss: 0.00003076
Iteration 173/1000 | Loss: 0.00003076
Iteration 174/1000 | Loss: 0.00003076
Iteration 175/1000 | Loss: 0.00003076
Iteration 176/1000 | Loss: 0.00003076
Iteration 177/1000 | Loss: 0.00003076
Iteration 178/1000 | Loss: 0.00003076
Iteration 179/1000 | Loss: 0.00003076
Iteration 180/1000 | Loss: 0.00003076
Iteration 181/1000 | Loss: 0.00003076
Iteration 182/1000 | Loss: 0.00003076
Iteration 183/1000 | Loss: 0.00003076
Iteration 184/1000 | Loss: 0.00003076
Iteration 185/1000 | Loss: 0.00003076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [3.075543281738646e-05, 3.075543281738646e-05, 3.075543281738646e-05, 3.075543281738646e-05, 3.075543281738646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.075543281738646e-05

Optimization complete. Final v2v error: 4.4160237312316895 mm

Highest mean error: 11.100872039794922 mm for frame 59

Lowest mean error: 3.3540139198303223 mm for frame 2

Saving results

Total time: 111.91293597221375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130576
Iteration 2/25 | Loss: 0.00214675
Iteration 3/25 | Loss: 0.00160403
Iteration 4/25 | Loss: 0.00146960
Iteration 5/25 | Loss: 0.00134859
Iteration 6/25 | Loss: 0.00134215
Iteration 7/25 | Loss: 0.00132420
Iteration 8/25 | Loss: 0.00132639
Iteration 9/25 | Loss: 0.00131697
Iteration 10/25 | Loss: 0.00131125
Iteration 11/25 | Loss: 0.00130778
Iteration 12/25 | Loss: 0.00131295
Iteration 13/25 | Loss: 0.00130615
Iteration 14/25 | Loss: 0.00130804
Iteration 15/25 | Loss: 0.00130846
Iteration 16/25 | Loss: 0.00130969
Iteration 17/25 | Loss: 0.00130839
Iteration 18/25 | Loss: 0.00130546
Iteration 19/25 | Loss: 0.00130335
Iteration 20/25 | Loss: 0.00130846
Iteration 21/25 | Loss: 0.00130454
Iteration 22/25 | Loss: 0.00129661
Iteration 23/25 | Loss: 0.00129882
Iteration 24/25 | Loss: 0.00128300
Iteration 25/25 | Loss: 0.00128566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47771680
Iteration 2/25 | Loss: 0.00207231
Iteration 3/25 | Loss: 0.00141205
Iteration 4/25 | Loss: 0.00141205
Iteration 5/25 | Loss: 0.00141205
Iteration 6/25 | Loss: 0.00141204
Iteration 7/25 | Loss: 0.00141204
Iteration 8/25 | Loss: 0.00141204
Iteration 9/25 | Loss: 0.00141204
Iteration 10/25 | Loss: 0.00141204
Iteration 11/25 | Loss: 0.00141204
Iteration 12/25 | Loss: 0.00141204
Iteration 13/25 | Loss: 0.00141204
Iteration 14/25 | Loss: 0.00141204
Iteration 15/25 | Loss: 0.00141204
Iteration 16/25 | Loss: 0.00141204
Iteration 17/25 | Loss: 0.00141204
Iteration 18/25 | Loss: 0.00141204
Iteration 19/25 | Loss: 0.00141204
Iteration 20/25 | Loss: 0.00141204
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014120439300313592, 0.0014120439300313592, 0.0014120439300313592, 0.0014120439300313592, 0.0014120439300313592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014120439300313592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141204
Iteration 2/1000 | Loss: 0.00075601
Iteration 3/1000 | Loss: 0.00024196
Iteration 4/1000 | Loss: 0.00007213
Iteration 5/1000 | Loss: 0.00005496
Iteration 6/1000 | Loss: 0.00004453
Iteration 7/1000 | Loss: 0.00054976
Iteration 8/1000 | Loss: 0.00005504
Iteration 9/1000 | Loss: 0.00004261
Iteration 10/1000 | Loss: 0.00003726
Iteration 11/1000 | Loss: 0.00003819
Iteration 12/1000 | Loss: 0.00003531
Iteration 13/1000 | Loss: 0.00003303
Iteration 14/1000 | Loss: 0.00004851
Iteration 15/1000 | Loss: 0.00003378
Iteration 16/1000 | Loss: 0.00003357
Iteration 17/1000 | Loss: 0.00036466
Iteration 18/1000 | Loss: 0.00004122
Iteration 19/1000 | Loss: 0.00008890
Iteration 20/1000 | Loss: 0.00019942
Iteration 21/1000 | Loss: 0.00023786
Iteration 22/1000 | Loss: 0.00017190
Iteration 23/1000 | Loss: 0.00021810
Iteration 24/1000 | Loss: 0.00022092
Iteration 25/1000 | Loss: 0.00007442
Iteration 26/1000 | Loss: 0.00003805
Iteration 27/1000 | Loss: 0.00017779
Iteration 28/1000 | Loss: 0.00021470
Iteration 29/1000 | Loss: 0.00015739
Iteration 30/1000 | Loss: 0.00019336
Iteration 31/1000 | Loss: 0.00004541
Iteration 32/1000 | Loss: 0.00029260
Iteration 33/1000 | Loss: 0.00005680
Iteration 34/1000 | Loss: 0.00003357
Iteration 35/1000 | Loss: 0.00003425
Iteration 36/1000 | Loss: 0.00003057
Iteration 37/1000 | Loss: 0.00003155
Iteration 38/1000 | Loss: 0.00002957
Iteration 39/1000 | Loss: 0.00003464
Iteration 40/1000 | Loss: 0.00002995
Iteration 41/1000 | Loss: 0.00002909
Iteration 42/1000 | Loss: 0.00003080
Iteration 43/1000 | Loss: 0.00003291
Iteration 44/1000 | Loss: 0.00002913
Iteration 45/1000 | Loss: 0.00003568
Iteration 46/1000 | Loss: 0.00004414
Iteration 47/1000 | Loss: 0.00002902
Iteration 48/1000 | Loss: 0.00003418
Iteration 49/1000 | Loss: 0.00005558
Iteration 50/1000 | Loss: 0.00002964
Iteration 51/1000 | Loss: 0.00002947
Iteration 52/1000 | Loss: 0.00002939
Iteration 53/1000 | Loss: 0.00003288
Iteration 54/1000 | Loss: 0.00002933
Iteration 55/1000 | Loss: 0.00003113
Iteration 56/1000 | Loss: 0.00002928
Iteration 57/1000 | Loss: 0.00002920
Iteration 58/1000 | Loss: 0.00002978
Iteration 59/1000 | Loss: 0.00002903
Iteration 60/1000 | Loss: 0.00002972
Iteration 61/1000 | Loss: 0.00002838
Iteration 62/1000 | Loss: 0.00002904
Iteration 63/1000 | Loss: 0.00002915
Iteration 64/1000 | Loss: 0.00004163
Iteration 65/1000 | Loss: 0.00002886
Iteration 66/1000 | Loss: 0.00002912
Iteration 67/1000 | Loss: 0.00003331
Iteration 68/1000 | Loss: 0.00004329
Iteration 69/1000 | Loss: 0.00003178
Iteration 70/1000 | Loss: 0.00002950
Iteration 71/1000 | Loss: 0.00002900
Iteration 72/1000 | Loss: 0.00002931
Iteration 73/1000 | Loss: 0.00003649
Iteration 74/1000 | Loss: 0.00002928
Iteration 75/1000 | Loss: 0.00004368
Iteration 76/1000 | Loss: 0.00002917
Iteration 77/1000 | Loss: 0.00002886
Iteration 78/1000 | Loss: 0.00002900
Iteration 79/1000 | Loss: 0.00002880
Iteration 80/1000 | Loss: 0.00002894
Iteration 81/1000 | Loss: 0.00002883
Iteration 82/1000 | Loss: 0.00002909
Iteration 83/1000 | Loss: 0.00002908
Iteration 84/1000 | Loss: 0.00005393
Iteration 85/1000 | Loss: 0.00002892
Iteration 86/1000 | Loss: 0.00003186
Iteration 87/1000 | Loss: 0.00002917
Iteration 88/1000 | Loss: 0.00003055
Iteration 89/1000 | Loss: 0.00003407
Iteration 90/1000 | Loss: 0.00003268
Iteration 91/1000 | Loss: 0.00003565
Iteration 92/1000 | Loss: 0.00002913
Iteration 93/1000 | Loss: 0.00002912
Iteration 94/1000 | Loss: 0.00003224
Iteration 95/1000 | Loss: 0.00002897
Iteration 96/1000 | Loss: 0.00004768
Iteration 97/1000 | Loss: 0.00002897
Iteration 98/1000 | Loss: 0.00002897
Iteration 99/1000 | Loss: 0.00002906
Iteration 100/1000 | Loss: 0.00002925
Iteration 101/1000 | Loss: 0.00002807
Iteration 102/1000 | Loss: 0.00002805
Iteration 103/1000 | Loss: 0.00002805
Iteration 104/1000 | Loss: 0.00002805
Iteration 105/1000 | Loss: 0.00002805
Iteration 106/1000 | Loss: 0.00002805
Iteration 107/1000 | Loss: 0.00002805
Iteration 108/1000 | Loss: 0.00002805
Iteration 109/1000 | Loss: 0.00002805
Iteration 110/1000 | Loss: 0.00002805
Iteration 111/1000 | Loss: 0.00002805
Iteration 112/1000 | Loss: 0.00002805
Iteration 113/1000 | Loss: 0.00002805
Iteration 114/1000 | Loss: 0.00002805
Iteration 115/1000 | Loss: 0.00002805
Iteration 116/1000 | Loss: 0.00002805
Iteration 117/1000 | Loss: 0.00002805
Iteration 118/1000 | Loss: 0.00002804
Iteration 119/1000 | Loss: 0.00002804
Iteration 120/1000 | Loss: 0.00002804
Iteration 121/1000 | Loss: 0.00002803
Iteration 122/1000 | Loss: 0.00002803
Iteration 123/1000 | Loss: 0.00002803
Iteration 124/1000 | Loss: 0.00002803
Iteration 125/1000 | Loss: 0.00002803
Iteration 126/1000 | Loss: 0.00002803
Iteration 127/1000 | Loss: 0.00002803
Iteration 128/1000 | Loss: 0.00002803
Iteration 129/1000 | Loss: 0.00002802
Iteration 130/1000 | Loss: 0.00002802
Iteration 131/1000 | Loss: 0.00002802
Iteration 132/1000 | Loss: 0.00002802
Iteration 133/1000 | Loss: 0.00002802
Iteration 134/1000 | Loss: 0.00002802
Iteration 135/1000 | Loss: 0.00002802
Iteration 136/1000 | Loss: 0.00003173
Iteration 137/1000 | Loss: 0.00002811
Iteration 138/1000 | Loss: 0.00002811
Iteration 139/1000 | Loss: 0.00002811
Iteration 140/1000 | Loss: 0.00002802
Iteration 141/1000 | Loss: 0.00002802
Iteration 142/1000 | Loss: 0.00002802
Iteration 143/1000 | Loss: 0.00002820
Iteration 144/1000 | Loss: 0.00002801
Iteration 145/1000 | Loss: 0.00002814
Iteration 146/1000 | Loss: 0.00002801
Iteration 147/1000 | Loss: 0.00002801
Iteration 148/1000 | Loss: 0.00002801
Iteration 149/1000 | Loss: 0.00002801
Iteration 150/1000 | Loss: 0.00002801
Iteration 151/1000 | Loss: 0.00002801
Iteration 152/1000 | Loss: 0.00002801
Iteration 153/1000 | Loss: 0.00002801
Iteration 154/1000 | Loss: 0.00002801
Iteration 155/1000 | Loss: 0.00002801
Iteration 156/1000 | Loss: 0.00002801
Iteration 157/1000 | Loss: 0.00002801
Iteration 158/1000 | Loss: 0.00002801
Iteration 159/1000 | Loss: 0.00002801
Iteration 160/1000 | Loss: 0.00002801
Iteration 161/1000 | Loss: 0.00002801
Iteration 162/1000 | Loss: 0.00002801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.800812762870919e-05, 2.800812762870919e-05, 2.800812762870919e-05, 2.800812762870919e-05, 2.800812762870919e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.800812762870919e-05

Optimization complete. Final v2v error: 4.333395004272461 mm

Highest mean error: 15.17410945892334 mm for frame 136

Lowest mean error: 3.8662772178649902 mm for frame 210

Saving results

Total time: 195.93417835235596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096910
Iteration 2/25 | Loss: 0.01096910
Iteration 3/25 | Loss: 0.00312499
Iteration 4/25 | Loss: 0.00183867
Iteration 5/25 | Loss: 0.00171340
Iteration 6/25 | Loss: 0.00182244
Iteration 7/25 | Loss: 0.00150975
Iteration 8/25 | Loss: 0.00138813
Iteration 9/25 | Loss: 0.00140214
Iteration 10/25 | Loss: 0.00127608
Iteration 11/25 | Loss: 0.00124535
Iteration 12/25 | Loss: 0.00123474
Iteration 13/25 | Loss: 0.00121467
Iteration 14/25 | Loss: 0.00119964
Iteration 15/25 | Loss: 0.00119797
Iteration 16/25 | Loss: 0.00119751
Iteration 17/25 | Loss: 0.00119727
Iteration 18/25 | Loss: 0.00119798
Iteration 19/25 | Loss: 0.00119711
Iteration 20/25 | Loss: 0.00119739
Iteration 21/25 | Loss: 0.00119716
Iteration 22/25 | Loss: 0.00119694
Iteration 23/25 | Loss: 0.00119655
Iteration 24/25 | Loss: 0.00119593
Iteration 25/25 | Loss: 0.00119544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48387349
Iteration 2/25 | Loss: 0.00105614
Iteration 3/25 | Loss: 0.00105614
Iteration 4/25 | Loss: 0.00105613
Iteration 5/25 | Loss: 0.00105613
Iteration 6/25 | Loss: 0.00105613
Iteration 7/25 | Loss: 0.00105613
Iteration 8/25 | Loss: 0.00105613
Iteration 9/25 | Loss: 0.00105613
Iteration 10/25 | Loss: 0.00105613
Iteration 11/25 | Loss: 0.00105613
Iteration 12/25 | Loss: 0.00105613
Iteration 13/25 | Loss: 0.00105613
Iteration 14/25 | Loss: 0.00105613
Iteration 15/25 | Loss: 0.00105613
Iteration 16/25 | Loss: 0.00105613
Iteration 17/25 | Loss: 0.00105613
Iteration 18/25 | Loss: 0.00105613
Iteration 19/25 | Loss: 0.00105613
Iteration 20/25 | Loss: 0.00105613
Iteration 21/25 | Loss: 0.00105613
Iteration 22/25 | Loss: 0.00105613
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010561342351138592, 0.0010561342351138592, 0.0010561342351138592, 0.0010561342351138592, 0.0010561342351138592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010561342351138592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105613
Iteration 2/1000 | Loss: 0.00004763
Iteration 3/1000 | Loss: 0.00003164
Iteration 4/1000 | Loss: 0.00002863
Iteration 5/1000 | Loss: 0.00002683
Iteration 6/1000 | Loss: 0.00002576
Iteration 7/1000 | Loss: 0.00002517
Iteration 8/1000 | Loss: 0.00002478
Iteration 9/1000 | Loss: 0.00002458
Iteration 10/1000 | Loss: 0.00002446
Iteration 11/1000 | Loss: 0.00002429
Iteration 12/1000 | Loss: 0.00002424
Iteration 13/1000 | Loss: 0.00002424
Iteration 14/1000 | Loss: 0.00002417
Iteration 15/1000 | Loss: 0.00002415
Iteration 16/1000 | Loss: 0.00002414
Iteration 17/1000 | Loss: 0.00002414
Iteration 18/1000 | Loss: 0.00002413
Iteration 19/1000 | Loss: 0.00002413
Iteration 20/1000 | Loss: 0.00002412
Iteration 21/1000 | Loss: 0.00002412
Iteration 22/1000 | Loss: 0.00002411
Iteration 23/1000 | Loss: 0.00002411
Iteration 24/1000 | Loss: 0.00002411
Iteration 25/1000 | Loss: 0.00002410
Iteration 26/1000 | Loss: 0.00002410
Iteration 27/1000 | Loss: 0.00002409
Iteration 28/1000 | Loss: 0.00002409
Iteration 29/1000 | Loss: 0.00002407
Iteration 30/1000 | Loss: 0.00002407
Iteration 31/1000 | Loss: 0.00002405
Iteration 32/1000 | Loss: 0.00002405
Iteration 33/1000 | Loss: 0.00002404
Iteration 34/1000 | Loss: 0.00002404
Iteration 35/1000 | Loss: 0.00002404
Iteration 36/1000 | Loss: 0.00002403
Iteration 37/1000 | Loss: 0.00002403
Iteration 38/1000 | Loss: 0.00002403
Iteration 39/1000 | Loss: 0.00002403
Iteration 40/1000 | Loss: 0.00002403
Iteration 41/1000 | Loss: 0.00002402
Iteration 42/1000 | Loss: 0.00002402
Iteration 43/1000 | Loss: 0.00002402
Iteration 44/1000 | Loss: 0.00002402
Iteration 45/1000 | Loss: 0.00002402
Iteration 46/1000 | Loss: 0.00002402
Iteration 47/1000 | Loss: 0.00002402
Iteration 48/1000 | Loss: 0.00002402
Iteration 49/1000 | Loss: 0.00002402
Iteration 50/1000 | Loss: 0.00002402
Iteration 51/1000 | Loss: 0.00002401
Iteration 52/1000 | Loss: 0.00002401
Iteration 53/1000 | Loss: 0.00002401
Iteration 54/1000 | Loss: 0.00002401
Iteration 55/1000 | Loss: 0.00002401
Iteration 56/1000 | Loss: 0.00002401
Iteration 57/1000 | Loss: 0.00002401
Iteration 58/1000 | Loss: 0.00002401
Iteration 59/1000 | Loss: 0.00002401
Iteration 60/1000 | Loss: 0.00002401
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002401
Iteration 64/1000 | Loss: 0.00002401
Iteration 65/1000 | Loss: 0.00002401
Iteration 66/1000 | Loss: 0.00002401
Iteration 67/1000 | Loss: 0.00002401
Iteration 68/1000 | Loss: 0.00002401
Iteration 69/1000 | Loss: 0.00002401
Iteration 70/1000 | Loss: 0.00002401
Iteration 71/1000 | Loss: 0.00002401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [2.4014014343265444e-05, 2.4014014343265444e-05, 2.4014014343265444e-05, 2.4014014343265444e-05, 2.4014014343265444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4014014343265444e-05

Optimization complete. Final v2v error: 4.094332695007324 mm

Highest mean error: 11.946062088012695 mm for frame 28

Lowest mean error: 3.3139405250549316 mm for frame 5

Saving results

Total time: 64.0275444984436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464948
Iteration 2/25 | Loss: 0.00145035
Iteration 3/25 | Loss: 0.00137790
Iteration 4/25 | Loss: 0.00136171
Iteration 5/25 | Loss: 0.00135492
Iteration 6/25 | Loss: 0.00135305
Iteration 7/25 | Loss: 0.00135255
Iteration 8/25 | Loss: 0.00135255
Iteration 9/25 | Loss: 0.00135255
Iteration 10/25 | Loss: 0.00135255
Iteration 11/25 | Loss: 0.00135255
Iteration 12/25 | Loss: 0.00135255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013525534886866808, 0.0013525534886866808, 0.0013525534886866808, 0.0013525534886866808, 0.0013525534886866808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013525534886866808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01514173
Iteration 2/25 | Loss: 0.00112490
Iteration 3/25 | Loss: 0.00112490
Iteration 4/25 | Loss: 0.00112490
Iteration 5/25 | Loss: 0.00112490
Iteration 6/25 | Loss: 0.00112490
Iteration 7/25 | Loss: 0.00112490
Iteration 8/25 | Loss: 0.00112490
Iteration 9/25 | Loss: 0.00112490
Iteration 10/25 | Loss: 0.00112490
Iteration 11/25 | Loss: 0.00112490
Iteration 12/25 | Loss: 0.00112490
Iteration 13/25 | Loss: 0.00112490
Iteration 14/25 | Loss: 0.00112490
Iteration 15/25 | Loss: 0.00112490
Iteration 16/25 | Loss: 0.00112490
Iteration 17/25 | Loss: 0.00112490
Iteration 18/25 | Loss: 0.00112490
Iteration 19/25 | Loss: 0.00112490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011249001836404204, 0.0011249001836404204, 0.0011249001836404204, 0.0011249001836404204, 0.0011249001836404204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011249001836404204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112490
Iteration 2/1000 | Loss: 0.00006029
Iteration 3/1000 | Loss: 0.00003698
Iteration 4/1000 | Loss: 0.00003064
Iteration 5/1000 | Loss: 0.00002893
Iteration 6/1000 | Loss: 0.00002760
Iteration 7/1000 | Loss: 0.00002702
Iteration 8/1000 | Loss: 0.00002645
Iteration 9/1000 | Loss: 0.00002616
Iteration 10/1000 | Loss: 0.00002590
Iteration 11/1000 | Loss: 0.00002581
Iteration 12/1000 | Loss: 0.00002579
Iteration 13/1000 | Loss: 0.00002569
Iteration 14/1000 | Loss: 0.00002565
Iteration 15/1000 | Loss: 0.00002552
Iteration 16/1000 | Loss: 0.00002550
Iteration 17/1000 | Loss: 0.00002546
Iteration 18/1000 | Loss: 0.00002546
Iteration 19/1000 | Loss: 0.00002546
Iteration 20/1000 | Loss: 0.00002545
Iteration 21/1000 | Loss: 0.00002541
Iteration 22/1000 | Loss: 0.00002541
Iteration 23/1000 | Loss: 0.00002541
Iteration 24/1000 | Loss: 0.00002541
Iteration 25/1000 | Loss: 0.00002540
Iteration 26/1000 | Loss: 0.00002540
Iteration 27/1000 | Loss: 0.00002540
Iteration 28/1000 | Loss: 0.00002540
Iteration 29/1000 | Loss: 0.00002540
Iteration 30/1000 | Loss: 0.00002540
Iteration 31/1000 | Loss: 0.00002540
Iteration 32/1000 | Loss: 0.00002540
Iteration 33/1000 | Loss: 0.00002540
Iteration 34/1000 | Loss: 0.00002540
Iteration 35/1000 | Loss: 0.00002540
Iteration 36/1000 | Loss: 0.00002539
Iteration 37/1000 | Loss: 0.00002539
Iteration 38/1000 | Loss: 0.00002538
Iteration 39/1000 | Loss: 0.00002538
Iteration 40/1000 | Loss: 0.00002538
Iteration 41/1000 | Loss: 0.00002538
Iteration 42/1000 | Loss: 0.00002538
Iteration 43/1000 | Loss: 0.00002537
Iteration 44/1000 | Loss: 0.00002537
Iteration 45/1000 | Loss: 0.00002537
Iteration 46/1000 | Loss: 0.00002537
Iteration 47/1000 | Loss: 0.00002537
Iteration 48/1000 | Loss: 0.00002537
Iteration 49/1000 | Loss: 0.00002537
Iteration 50/1000 | Loss: 0.00002537
Iteration 51/1000 | Loss: 0.00002537
Iteration 52/1000 | Loss: 0.00002537
Iteration 53/1000 | Loss: 0.00002537
Iteration 54/1000 | Loss: 0.00002537
Iteration 55/1000 | Loss: 0.00002537
Iteration 56/1000 | Loss: 0.00002537
Iteration 57/1000 | Loss: 0.00002537
Iteration 58/1000 | Loss: 0.00002536
Iteration 59/1000 | Loss: 0.00002536
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002535
Iteration 62/1000 | Loss: 0.00002535
Iteration 63/1000 | Loss: 0.00002535
Iteration 64/1000 | Loss: 0.00002534
Iteration 65/1000 | Loss: 0.00002534
Iteration 66/1000 | Loss: 0.00002534
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002534
Iteration 70/1000 | Loss: 0.00002534
Iteration 71/1000 | Loss: 0.00002534
Iteration 72/1000 | Loss: 0.00002534
Iteration 73/1000 | Loss: 0.00002534
Iteration 74/1000 | Loss: 0.00002534
Iteration 75/1000 | Loss: 0.00002534
Iteration 76/1000 | Loss: 0.00002533
Iteration 77/1000 | Loss: 0.00002533
Iteration 78/1000 | Loss: 0.00002533
Iteration 79/1000 | Loss: 0.00002533
Iteration 80/1000 | Loss: 0.00002533
Iteration 81/1000 | Loss: 0.00002533
Iteration 82/1000 | Loss: 0.00002533
Iteration 83/1000 | Loss: 0.00002533
Iteration 84/1000 | Loss: 0.00002533
Iteration 85/1000 | Loss: 0.00002533
Iteration 86/1000 | Loss: 0.00002533
Iteration 87/1000 | Loss: 0.00002532
Iteration 88/1000 | Loss: 0.00002531
Iteration 89/1000 | Loss: 0.00002531
Iteration 90/1000 | Loss: 0.00002531
Iteration 91/1000 | Loss: 0.00002530
Iteration 92/1000 | Loss: 0.00002530
Iteration 93/1000 | Loss: 0.00002530
Iteration 94/1000 | Loss: 0.00002530
Iteration 95/1000 | Loss: 0.00002530
Iteration 96/1000 | Loss: 0.00002530
Iteration 97/1000 | Loss: 0.00002529
Iteration 98/1000 | Loss: 0.00002529
Iteration 99/1000 | Loss: 0.00002529
Iteration 100/1000 | Loss: 0.00002529
Iteration 101/1000 | Loss: 0.00002529
Iteration 102/1000 | Loss: 0.00002529
Iteration 103/1000 | Loss: 0.00002529
Iteration 104/1000 | Loss: 0.00002529
Iteration 105/1000 | Loss: 0.00002529
Iteration 106/1000 | Loss: 0.00002528
Iteration 107/1000 | Loss: 0.00002528
Iteration 108/1000 | Loss: 0.00002528
Iteration 109/1000 | Loss: 0.00002528
Iteration 110/1000 | Loss: 0.00002527
Iteration 111/1000 | Loss: 0.00002527
Iteration 112/1000 | Loss: 0.00002527
Iteration 113/1000 | Loss: 0.00002527
Iteration 114/1000 | Loss: 0.00002527
Iteration 115/1000 | Loss: 0.00002526
Iteration 116/1000 | Loss: 0.00002526
Iteration 117/1000 | Loss: 0.00002526
Iteration 118/1000 | Loss: 0.00002526
Iteration 119/1000 | Loss: 0.00002526
Iteration 120/1000 | Loss: 0.00002526
Iteration 121/1000 | Loss: 0.00002526
Iteration 122/1000 | Loss: 0.00002526
Iteration 123/1000 | Loss: 0.00002526
Iteration 124/1000 | Loss: 0.00002526
Iteration 125/1000 | Loss: 0.00002525
Iteration 126/1000 | Loss: 0.00002525
Iteration 127/1000 | Loss: 0.00002525
Iteration 128/1000 | Loss: 0.00002525
Iteration 129/1000 | Loss: 0.00002525
Iteration 130/1000 | Loss: 0.00002524
Iteration 131/1000 | Loss: 0.00002524
Iteration 132/1000 | Loss: 0.00002524
Iteration 133/1000 | Loss: 0.00002524
Iteration 134/1000 | Loss: 0.00002524
Iteration 135/1000 | Loss: 0.00002524
Iteration 136/1000 | Loss: 0.00002524
Iteration 137/1000 | Loss: 0.00002524
Iteration 138/1000 | Loss: 0.00002524
Iteration 139/1000 | Loss: 0.00002524
Iteration 140/1000 | Loss: 0.00002524
Iteration 141/1000 | Loss: 0.00002524
Iteration 142/1000 | Loss: 0.00002523
Iteration 143/1000 | Loss: 0.00002523
Iteration 144/1000 | Loss: 0.00002523
Iteration 145/1000 | Loss: 0.00002523
Iteration 146/1000 | Loss: 0.00002523
Iteration 147/1000 | Loss: 0.00002523
Iteration 148/1000 | Loss: 0.00002523
Iteration 149/1000 | Loss: 0.00002523
Iteration 150/1000 | Loss: 0.00002522
Iteration 151/1000 | Loss: 0.00002522
Iteration 152/1000 | Loss: 0.00002522
Iteration 153/1000 | Loss: 0.00002522
Iteration 154/1000 | Loss: 0.00002522
Iteration 155/1000 | Loss: 0.00002522
Iteration 156/1000 | Loss: 0.00002522
Iteration 157/1000 | Loss: 0.00002522
Iteration 158/1000 | Loss: 0.00002522
Iteration 159/1000 | Loss: 0.00002522
Iteration 160/1000 | Loss: 0.00002522
Iteration 161/1000 | Loss: 0.00002522
Iteration 162/1000 | Loss: 0.00002522
Iteration 163/1000 | Loss: 0.00002522
Iteration 164/1000 | Loss: 0.00002522
Iteration 165/1000 | Loss: 0.00002521
Iteration 166/1000 | Loss: 0.00002521
Iteration 167/1000 | Loss: 0.00002521
Iteration 168/1000 | Loss: 0.00002521
Iteration 169/1000 | Loss: 0.00002521
Iteration 170/1000 | Loss: 0.00002521
Iteration 171/1000 | Loss: 0.00002521
Iteration 172/1000 | Loss: 0.00002521
Iteration 173/1000 | Loss: 0.00002521
Iteration 174/1000 | Loss: 0.00002521
Iteration 175/1000 | Loss: 0.00002520
Iteration 176/1000 | Loss: 0.00002520
Iteration 177/1000 | Loss: 0.00002520
Iteration 178/1000 | Loss: 0.00002520
Iteration 179/1000 | Loss: 0.00002520
Iteration 180/1000 | Loss: 0.00002520
Iteration 181/1000 | Loss: 0.00002520
Iteration 182/1000 | Loss: 0.00002520
Iteration 183/1000 | Loss: 0.00002520
Iteration 184/1000 | Loss: 0.00002520
Iteration 185/1000 | Loss: 0.00002520
Iteration 186/1000 | Loss: 0.00002520
Iteration 187/1000 | Loss: 0.00002520
Iteration 188/1000 | Loss: 0.00002520
Iteration 189/1000 | Loss: 0.00002520
Iteration 190/1000 | Loss: 0.00002520
Iteration 191/1000 | Loss: 0.00002520
Iteration 192/1000 | Loss: 0.00002520
Iteration 193/1000 | Loss: 0.00002520
Iteration 194/1000 | Loss: 0.00002520
Iteration 195/1000 | Loss: 0.00002520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.5201285097864456e-05, 2.5201285097864456e-05, 2.5201285097864456e-05, 2.5201285097864456e-05, 2.5201285097864456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5201285097864456e-05

Optimization complete. Final v2v error: 4.3167595863342285 mm

Highest mean error: 5.052553653717041 mm for frame 61

Lowest mean error: 3.9680330753326416 mm for frame 48

Saving results

Total time: 38.2369601726532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512906
Iteration 2/25 | Loss: 0.00163584
Iteration 3/25 | Loss: 0.00138366
Iteration 4/25 | Loss: 0.00135517
Iteration 5/25 | Loss: 0.00134681
Iteration 6/25 | Loss: 0.00134433
Iteration 7/25 | Loss: 0.00134371
Iteration 8/25 | Loss: 0.00134371
Iteration 9/25 | Loss: 0.00134371
Iteration 10/25 | Loss: 0.00134371
Iteration 11/25 | Loss: 0.00134371
Iteration 12/25 | Loss: 0.00134371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013437120942398906, 0.0013437120942398906, 0.0013437120942398906, 0.0013437120942398906, 0.0013437120942398906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013437120942398906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45425975
Iteration 2/25 | Loss: 0.00122630
Iteration 3/25 | Loss: 0.00122630
Iteration 4/25 | Loss: 0.00122630
Iteration 5/25 | Loss: 0.00122630
Iteration 6/25 | Loss: 0.00122629
Iteration 7/25 | Loss: 0.00122629
Iteration 8/25 | Loss: 0.00122629
Iteration 9/25 | Loss: 0.00122629
Iteration 10/25 | Loss: 0.00122629
Iteration 11/25 | Loss: 0.00122629
Iteration 12/25 | Loss: 0.00122629
Iteration 13/25 | Loss: 0.00122629
Iteration 14/25 | Loss: 0.00122629
Iteration 15/25 | Loss: 0.00122629
Iteration 16/25 | Loss: 0.00122629
Iteration 17/25 | Loss: 0.00122629
Iteration 18/25 | Loss: 0.00122629
Iteration 19/25 | Loss: 0.00122629
Iteration 20/25 | Loss: 0.00122629
Iteration 21/25 | Loss: 0.00122629
Iteration 22/25 | Loss: 0.00122629
Iteration 23/25 | Loss: 0.00122629
Iteration 24/25 | Loss: 0.00122629
Iteration 25/25 | Loss: 0.00122629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122629
Iteration 2/1000 | Loss: 0.00006267
Iteration 3/1000 | Loss: 0.00003769
Iteration 4/1000 | Loss: 0.00003400
Iteration 5/1000 | Loss: 0.00003225
Iteration 6/1000 | Loss: 0.00003127
Iteration 7/1000 | Loss: 0.00003057
Iteration 8/1000 | Loss: 0.00002998
Iteration 9/1000 | Loss: 0.00002958
Iteration 10/1000 | Loss: 0.00002933
Iteration 11/1000 | Loss: 0.00002914
Iteration 12/1000 | Loss: 0.00002912
Iteration 13/1000 | Loss: 0.00002902
Iteration 14/1000 | Loss: 0.00002902
Iteration 15/1000 | Loss: 0.00002896
Iteration 16/1000 | Loss: 0.00002883
Iteration 17/1000 | Loss: 0.00002881
Iteration 18/1000 | Loss: 0.00002872
Iteration 19/1000 | Loss: 0.00002871
Iteration 20/1000 | Loss: 0.00002871
Iteration 21/1000 | Loss: 0.00002867
Iteration 22/1000 | Loss: 0.00002867
Iteration 23/1000 | Loss: 0.00002867
Iteration 24/1000 | Loss: 0.00002867
Iteration 25/1000 | Loss: 0.00002867
Iteration 26/1000 | Loss: 0.00002866
Iteration 27/1000 | Loss: 0.00002866
Iteration 28/1000 | Loss: 0.00002866
Iteration 29/1000 | Loss: 0.00002865
Iteration 30/1000 | Loss: 0.00002865
Iteration 31/1000 | Loss: 0.00002865
Iteration 32/1000 | Loss: 0.00002864
Iteration 33/1000 | Loss: 0.00002864
Iteration 34/1000 | Loss: 0.00002864
Iteration 35/1000 | Loss: 0.00002863
Iteration 36/1000 | Loss: 0.00002863
Iteration 37/1000 | Loss: 0.00002863
Iteration 38/1000 | Loss: 0.00002862
Iteration 39/1000 | Loss: 0.00002862
Iteration 40/1000 | Loss: 0.00002861
Iteration 41/1000 | Loss: 0.00002861
Iteration 42/1000 | Loss: 0.00002861
Iteration 43/1000 | Loss: 0.00002861
Iteration 44/1000 | Loss: 0.00002861
Iteration 45/1000 | Loss: 0.00002860
Iteration 46/1000 | Loss: 0.00002860
Iteration 47/1000 | Loss: 0.00002860
Iteration 48/1000 | Loss: 0.00002860
Iteration 49/1000 | Loss: 0.00002860
Iteration 50/1000 | Loss: 0.00002860
Iteration 51/1000 | Loss: 0.00002859
Iteration 52/1000 | Loss: 0.00002859
Iteration 53/1000 | Loss: 0.00002859
Iteration 54/1000 | Loss: 0.00002859
Iteration 55/1000 | Loss: 0.00002858
Iteration 56/1000 | Loss: 0.00002858
Iteration 57/1000 | Loss: 0.00002858
Iteration 58/1000 | Loss: 0.00002858
Iteration 59/1000 | Loss: 0.00002858
Iteration 60/1000 | Loss: 0.00002858
Iteration 61/1000 | Loss: 0.00002857
Iteration 62/1000 | Loss: 0.00002857
Iteration 63/1000 | Loss: 0.00002856
Iteration 64/1000 | Loss: 0.00002856
Iteration 65/1000 | Loss: 0.00002856
Iteration 66/1000 | Loss: 0.00002856
Iteration 67/1000 | Loss: 0.00002855
Iteration 68/1000 | Loss: 0.00002855
Iteration 69/1000 | Loss: 0.00002855
Iteration 70/1000 | Loss: 0.00002855
Iteration 71/1000 | Loss: 0.00002854
Iteration 72/1000 | Loss: 0.00002854
Iteration 73/1000 | Loss: 0.00002854
Iteration 74/1000 | Loss: 0.00002853
Iteration 75/1000 | Loss: 0.00002853
Iteration 76/1000 | Loss: 0.00002853
Iteration 77/1000 | Loss: 0.00002852
Iteration 78/1000 | Loss: 0.00002852
Iteration 79/1000 | Loss: 0.00002852
Iteration 80/1000 | Loss: 0.00002852
Iteration 81/1000 | Loss: 0.00002852
Iteration 82/1000 | Loss: 0.00002851
Iteration 83/1000 | Loss: 0.00002851
Iteration 84/1000 | Loss: 0.00002850
Iteration 85/1000 | Loss: 0.00002850
Iteration 86/1000 | Loss: 0.00002849
Iteration 87/1000 | Loss: 0.00002849
Iteration 88/1000 | Loss: 0.00002849
Iteration 89/1000 | Loss: 0.00002849
Iteration 90/1000 | Loss: 0.00002849
Iteration 91/1000 | Loss: 0.00002849
Iteration 92/1000 | Loss: 0.00002849
Iteration 93/1000 | Loss: 0.00002849
Iteration 94/1000 | Loss: 0.00002848
Iteration 95/1000 | Loss: 0.00002848
Iteration 96/1000 | Loss: 0.00002848
Iteration 97/1000 | Loss: 0.00002848
Iteration 98/1000 | Loss: 0.00002847
Iteration 99/1000 | Loss: 0.00002847
Iteration 100/1000 | Loss: 0.00002847
Iteration 101/1000 | Loss: 0.00002847
Iteration 102/1000 | Loss: 0.00002847
Iteration 103/1000 | Loss: 0.00002847
Iteration 104/1000 | Loss: 0.00002847
Iteration 105/1000 | Loss: 0.00002846
Iteration 106/1000 | Loss: 0.00002846
Iteration 107/1000 | Loss: 0.00002846
Iteration 108/1000 | Loss: 0.00002845
Iteration 109/1000 | Loss: 0.00002845
Iteration 110/1000 | Loss: 0.00002845
Iteration 111/1000 | Loss: 0.00002845
Iteration 112/1000 | Loss: 0.00002845
Iteration 113/1000 | Loss: 0.00002845
Iteration 114/1000 | Loss: 0.00002844
Iteration 115/1000 | Loss: 0.00002844
Iteration 116/1000 | Loss: 0.00002844
Iteration 117/1000 | Loss: 0.00002844
Iteration 118/1000 | Loss: 0.00002844
Iteration 119/1000 | Loss: 0.00002844
Iteration 120/1000 | Loss: 0.00002844
Iteration 121/1000 | Loss: 0.00002843
Iteration 122/1000 | Loss: 0.00002843
Iteration 123/1000 | Loss: 0.00002843
Iteration 124/1000 | Loss: 0.00002843
Iteration 125/1000 | Loss: 0.00002843
Iteration 126/1000 | Loss: 0.00002843
Iteration 127/1000 | Loss: 0.00002843
Iteration 128/1000 | Loss: 0.00002843
Iteration 129/1000 | Loss: 0.00002843
Iteration 130/1000 | Loss: 0.00002843
Iteration 131/1000 | Loss: 0.00002843
Iteration 132/1000 | Loss: 0.00002843
Iteration 133/1000 | Loss: 0.00002843
Iteration 134/1000 | Loss: 0.00002843
Iteration 135/1000 | Loss: 0.00002843
Iteration 136/1000 | Loss: 0.00002843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.8430829843273386e-05, 2.8430829843273386e-05, 2.8430829843273386e-05, 2.8430829843273386e-05, 2.8430829843273386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8430829843273386e-05

Optimization complete. Final v2v error: 4.502135753631592 mm

Highest mean error: 5.059598445892334 mm for frame 85

Lowest mean error: 4.021895885467529 mm for frame 193

Saving results

Total time: 44.5838725566864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886244
Iteration 2/25 | Loss: 0.00197576
Iteration 3/25 | Loss: 0.00157348
Iteration 4/25 | Loss: 0.00151270
Iteration 5/25 | Loss: 0.00149726
Iteration 6/25 | Loss: 0.00149461
Iteration 7/25 | Loss: 0.00149360
Iteration 8/25 | Loss: 0.00149400
Iteration 9/25 | Loss: 0.00149375
Iteration 10/25 | Loss: 0.00149260
Iteration 11/25 | Loss: 0.00148915
Iteration 12/25 | Loss: 0.00148783
Iteration 13/25 | Loss: 0.00148733
Iteration 14/25 | Loss: 0.00148726
Iteration 15/25 | Loss: 0.00148726
Iteration 16/25 | Loss: 0.00148726
Iteration 17/25 | Loss: 0.00148726
Iteration 18/25 | Loss: 0.00148726
Iteration 19/25 | Loss: 0.00148726
Iteration 20/25 | Loss: 0.00148726
Iteration 21/25 | Loss: 0.00148726
Iteration 22/25 | Loss: 0.00148726
Iteration 23/25 | Loss: 0.00148726
Iteration 24/25 | Loss: 0.00148725
Iteration 25/25 | Loss: 0.00148725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37851477
Iteration 2/25 | Loss: 0.00165582
Iteration 3/25 | Loss: 0.00165582
Iteration 4/25 | Loss: 0.00165582
Iteration 5/25 | Loss: 0.00165582
Iteration 6/25 | Loss: 0.00165582
Iteration 7/25 | Loss: 0.00165582
Iteration 8/25 | Loss: 0.00165582
Iteration 9/25 | Loss: 0.00165582
Iteration 10/25 | Loss: 0.00165582
Iteration 11/25 | Loss: 0.00165582
Iteration 12/25 | Loss: 0.00165582
Iteration 13/25 | Loss: 0.00165582
Iteration 14/25 | Loss: 0.00165582
Iteration 15/25 | Loss: 0.00165582
Iteration 16/25 | Loss: 0.00165582
Iteration 17/25 | Loss: 0.00165582
Iteration 18/25 | Loss: 0.00165582
Iteration 19/25 | Loss: 0.00165582
Iteration 20/25 | Loss: 0.00165582
Iteration 21/25 | Loss: 0.00165582
Iteration 22/25 | Loss: 0.00165582
Iteration 23/25 | Loss: 0.00165582
Iteration 24/25 | Loss: 0.00165582
Iteration 25/25 | Loss: 0.00165582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165582
Iteration 2/1000 | Loss: 0.00007405
Iteration 3/1000 | Loss: 0.00005038
Iteration 4/1000 | Loss: 0.00004313
Iteration 5/1000 | Loss: 0.00004096
Iteration 6/1000 | Loss: 0.00003911
Iteration 7/1000 | Loss: 0.00003808
Iteration 8/1000 | Loss: 0.00003727
Iteration 9/1000 | Loss: 0.00003679
Iteration 10/1000 | Loss: 0.00003632
Iteration 11/1000 | Loss: 0.00003603
Iteration 12/1000 | Loss: 0.00003578
Iteration 13/1000 | Loss: 0.00003561
Iteration 14/1000 | Loss: 0.00003556
Iteration 15/1000 | Loss: 0.00003555
Iteration 16/1000 | Loss: 0.00003551
Iteration 17/1000 | Loss: 0.00003546
Iteration 18/1000 | Loss: 0.00003546
Iteration 19/1000 | Loss: 0.00003545
Iteration 20/1000 | Loss: 0.00003545
Iteration 21/1000 | Loss: 0.00003544
Iteration 22/1000 | Loss: 0.00003544
Iteration 23/1000 | Loss: 0.00003544
Iteration 24/1000 | Loss: 0.00003543
Iteration 25/1000 | Loss: 0.00003543
Iteration 26/1000 | Loss: 0.00003543
Iteration 27/1000 | Loss: 0.00003542
Iteration 28/1000 | Loss: 0.00003542
Iteration 29/1000 | Loss: 0.00003542
Iteration 30/1000 | Loss: 0.00003541
Iteration 31/1000 | Loss: 0.00003541
Iteration 32/1000 | Loss: 0.00003541
Iteration 33/1000 | Loss: 0.00003540
Iteration 34/1000 | Loss: 0.00003540
Iteration 35/1000 | Loss: 0.00003540
Iteration 36/1000 | Loss: 0.00003539
Iteration 37/1000 | Loss: 0.00003539
Iteration 38/1000 | Loss: 0.00003538
Iteration 39/1000 | Loss: 0.00003538
Iteration 40/1000 | Loss: 0.00003538
Iteration 41/1000 | Loss: 0.00003537
Iteration 42/1000 | Loss: 0.00003537
Iteration 43/1000 | Loss: 0.00003537
Iteration 44/1000 | Loss: 0.00003537
Iteration 45/1000 | Loss: 0.00003537
Iteration 46/1000 | Loss: 0.00003537
Iteration 47/1000 | Loss: 0.00003537
Iteration 48/1000 | Loss: 0.00003536
Iteration 49/1000 | Loss: 0.00003536
Iteration 50/1000 | Loss: 0.00003536
Iteration 51/1000 | Loss: 0.00003536
Iteration 52/1000 | Loss: 0.00003536
Iteration 53/1000 | Loss: 0.00003536
Iteration 54/1000 | Loss: 0.00003536
Iteration 55/1000 | Loss: 0.00003536
Iteration 56/1000 | Loss: 0.00003535
Iteration 57/1000 | Loss: 0.00003535
Iteration 58/1000 | Loss: 0.00003535
Iteration 59/1000 | Loss: 0.00003535
Iteration 60/1000 | Loss: 0.00003535
Iteration 61/1000 | Loss: 0.00003535
Iteration 62/1000 | Loss: 0.00003535
Iteration 63/1000 | Loss: 0.00003535
Iteration 64/1000 | Loss: 0.00003534
Iteration 65/1000 | Loss: 0.00003534
Iteration 66/1000 | Loss: 0.00003534
Iteration 67/1000 | Loss: 0.00003534
Iteration 68/1000 | Loss: 0.00003534
Iteration 69/1000 | Loss: 0.00003534
Iteration 70/1000 | Loss: 0.00003534
Iteration 71/1000 | Loss: 0.00003534
Iteration 72/1000 | Loss: 0.00003534
Iteration 73/1000 | Loss: 0.00003534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [3.53422001353465e-05, 3.53422001353465e-05, 3.53422001353465e-05, 3.53422001353465e-05, 3.53422001353465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.53422001353465e-05

Optimization complete. Final v2v error: 5.175747394561768 mm

Highest mean error: 5.9624128341674805 mm for frame 76

Lowest mean error: 4.6511688232421875 mm for frame 54

Saving results

Total time: 53.08350205421448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512516
Iteration 2/25 | Loss: 0.00158098
Iteration 3/25 | Loss: 0.00139292
Iteration 4/25 | Loss: 0.00137391
Iteration 5/25 | Loss: 0.00136723
Iteration 6/25 | Loss: 0.00136583
Iteration 7/25 | Loss: 0.00136581
Iteration 8/25 | Loss: 0.00136581
Iteration 9/25 | Loss: 0.00136581
Iteration 10/25 | Loss: 0.00136581
Iteration 11/25 | Loss: 0.00136581
Iteration 12/25 | Loss: 0.00136581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001365811680443585, 0.001365811680443585, 0.001365811680443585, 0.001365811680443585, 0.001365811680443585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001365811680443585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02737832
Iteration 2/25 | Loss: 0.00136459
Iteration 3/25 | Loss: 0.00136458
Iteration 4/25 | Loss: 0.00136458
Iteration 5/25 | Loss: 0.00136458
Iteration 6/25 | Loss: 0.00136458
Iteration 7/25 | Loss: 0.00136458
Iteration 8/25 | Loss: 0.00136458
Iteration 9/25 | Loss: 0.00136458
Iteration 10/25 | Loss: 0.00136458
Iteration 11/25 | Loss: 0.00136458
Iteration 12/25 | Loss: 0.00136458
Iteration 13/25 | Loss: 0.00136458
Iteration 14/25 | Loss: 0.00136458
Iteration 15/25 | Loss: 0.00136458
Iteration 16/25 | Loss: 0.00136458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013645753497257829, 0.0013645753497257829, 0.0013645753497257829, 0.0013645753497257829, 0.0013645753497257829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013645753497257829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136458
Iteration 2/1000 | Loss: 0.00005757
Iteration 3/1000 | Loss: 0.00003597
Iteration 4/1000 | Loss: 0.00003065
Iteration 5/1000 | Loss: 0.00002934
Iteration 6/1000 | Loss: 0.00002840
Iteration 7/1000 | Loss: 0.00002788
Iteration 8/1000 | Loss: 0.00002735
Iteration 9/1000 | Loss: 0.00002690
Iteration 10/1000 | Loss: 0.00002666
Iteration 11/1000 | Loss: 0.00002653
Iteration 12/1000 | Loss: 0.00002635
Iteration 13/1000 | Loss: 0.00002629
Iteration 14/1000 | Loss: 0.00002619
Iteration 15/1000 | Loss: 0.00002618
Iteration 16/1000 | Loss: 0.00002613
Iteration 17/1000 | Loss: 0.00002613
Iteration 18/1000 | Loss: 0.00002610
Iteration 19/1000 | Loss: 0.00002610
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00002609
Iteration 22/1000 | Loss: 0.00002609
Iteration 23/1000 | Loss: 0.00002608
Iteration 24/1000 | Loss: 0.00002608
Iteration 25/1000 | Loss: 0.00002608
Iteration 26/1000 | Loss: 0.00002608
Iteration 27/1000 | Loss: 0.00002608
Iteration 28/1000 | Loss: 0.00002608
Iteration 29/1000 | Loss: 0.00002608
Iteration 30/1000 | Loss: 0.00002608
Iteration 31/1000 | Loss: 0.00002608
Iteration 32/1000 | Loss: 0.00002607
Iteration 33/1000 | Loss: 0.00002607
Iteration 34/1000 | Loss: 0.00002607
Iteration 35/1000 | Loss: 0.00002607
Iteration 36/1000 | Loss: 0.00002607
Iteration 37/1000 | Loss: 0.00002606
Iteration 38/1000 | Loss: 0.00002606
Iteration 39/1000 | Loss: 0.00002605
Iteration 40/1000 | Loss: 0.00002605
Iteration 41/1000 | Loss: 0.00002605
Iteration 42/1000 | Loss: 0.00002605
Iteration 43/1000 | Loss: 0.00002605
Iteration 44/1000 | Loss: 0.00002604
Iteration 45/1000 | Loss: 0.00002604
Iteration 46/1000 | Loss: 0.00002604
Iteration 47/1000 | Loss: 0.00002604
Iteration 48/1000 | Loss: 0.00002604
Iteration 49/1000 | Loss: 0.00002603
Iteration 50/1000 | Loss: 0.00002603
Iteration 51/1000 | Loss: 0.00002601
Iteration 52/1000 | Loss: 0.00002601
Iteration 53/1000 | Loss: 0.00002601
Iteration 54/1000 | Loss: 0.00002601
Iteration 55/1000 | Loss: 0.00002601
Iteration 56/1000 | Loss: 0.00002600
Iteration 57/1000 | Loss: 0.00002600
Iteration 58/1000 | Loss: 0.00002600
Iteration 59/1000 | Loss: 0.00002599
Iteration 60/1000 | Loss: 0.00002599
Iteration 61/1000 | Loss: 0.00002599
Iteration 62/1000 | Loss: 0.00002599
Iteration 63/1000 | Loss: 0.00002599
Iteration 64/1000 | Loss: 0.00002598
Iteration 65/1000 | Loss: 0.00002598
Iteration 66/1000 | Loss: 0.00002598
Iteration 67/1000 | Loss: 0.00002598
Iteration 68/1000 | Loss: 0.00002597
Iteration 69/1000 | Loss: 0.00002597
Iteration 70/1000 | Loss: 0.00002597
Iteration 71/1000 | Loss: 0.00002597
Iteration 72/1000 | Loss: 0.00002597
Iteration 73/1000 | Loss: 0.00002597
Iteration 74/1000 | Loss: 0.00002597
Iteration 75/1000 | Loss: 0.00002597
Iteration 76/1000 | Loss: 0.00002597
Iteration 77/1000 | Loss: 0.00002597
Iteration 78/1000 | Loss: 0.00002597
Iteration 79/1000 | Loss: 0.00002597
Iteration 80/1000 | Loss: 0.00002597
Iteration 81/1000 | Loss: 0.00002597
Iteration 82/1000 | Loss: 0.00002597
Iteration 83/1000 | Loss: 0.00002597
Iteration 84/1000 | Loss: 0.00002597
Iteration 85/1000 | Loss: 0.00002597
Iteration 86/1000 | Loss: 0.00002597
Iteration 87/1000 | Loss: 0.00002597
Iteration 88/1000 | Loss: 0.00002597
Iteration 89/1000 | Loss: 0.00002597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.5967103283619508e-05, 2.5967103283619508e-05, 2.5967103283619508e-05, 2.5967103283619508e-05, 2.5967103283619508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5967103283619508e-05

Optimization complete. Final v2v error: 4.474238395690918 mm

Highest mean error: 4.9310479164123535 mm for frame 86

Lowest mean error: 3.7778091430664062 mm for frame 11

Saving results

Total time: 38.17115831375122
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432200
Iteration 2/25 | Loss: 0.00145037
Iteration 3/25 | Loss: 0.00136416
Iteration 4/25 | Loss: 0.00135065
Iteration 5/25 | Loss: 0.00134418
Iteration 6/25 | Loss: 0.00134291
Iteration 7/25 | Loss: 0.00134291
Iteration 8/25 | Loss: 0.00134291
Iteration 9/25 | Loss: 0.00134291
Iteration 10/25 | Loss: 0.00134291
Iteration 11/25 | Loss: 0.00134291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013429132523015141, 0.0013429132523015141, 0.0013429132523015141, 0.0013429132523015141, 0.0013429132523015141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013429132523015141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51686943
Iteration 2/25 | Loss: 0.00117011
Iteration 3/25 | Loss: 0.00117011
Iteration 4/25 | Loss: 0.00117011
Iteration 5/25 | Loss: 0.00117011
Iteration 6/25 | Loss: 0.00117011
Iteration 7/25 | Loss: 0.00117011
Iteration 8/25 | Loss: 0.00117011
Iteration 9/25 | Loss: 0.00117011
Iteration 10/25 | Loss: 0.00117011
Iteration 11/25 | Loss: 0.00117011
Iteration 12/25 | Loss: 0.00117011
Iteration 13/25 | Loss: 0.00117011
Iteration 14/25 | Loss: 0.00117011
Iteration 15/25 | Loss: 0.00117011
Iteration 16/25 | Loss: 0.00117011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011701100738719106, 0.0011701100738719106, 0.0011701100738719106, 0.0011701100738719106, 0.0011701100738719106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011701100738719106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117011
Iteration 2/1000 | Loss: 0.00005115
Iteration 3/1000 | Loss: 0.00002944
Iteration 4/1000 | Loss: 0.00002705
Iteration 5/1000 | Loss: 0.00002580
Iteration 6/1000 | Loss: 0.00002526
Iteration 7/1000 | Loss: 0.00002476
Iteration 8/1000 | Loss: 0.00002434
Iteration 9/1000 | Loss: 0.00002425
Iteration 10/1000 | Loss: 0.00002421
Iteration 11/1000 | Loss: 0.00002421
Iteration 12/1000 | Loss: 0.00002414
Iteration 13/1000 | Loss: 0.00002401
Iteration 14/1000 | Loss: 0.00002399
Iteration 15/1000 | Loss: 0.00002392
Iteration 16/1000 | Loss: 0.00002392
Iteration 17/1000 | Loss: 0.00002391
Iteration 18/1000 | Loss: 0.00002390
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002389
Iteration 21/1000 | Loss: 0.00002389
Iteration 22/1000 | Loss: 0.00002389
Iteration 23/1000 | Loss: 0.00002389
Iteration 24/1000 | Loss: 0.00002388
Iteration 25/1000 | Loss: 0.00002388
Iteration 26/1000 | Loss: 0.00002388
Iteration 27/1000 | Loss: 0.00002387
Iteration 28/1000 | Loss: 0.00002386
Iteration 29/1000 | Loss: 0.00002386
Iteration 30/1000 | Loss: 0.00002386
Iteration 31/1000 | Loss: 0.00002386
Iteration 32/1000 | Loss: 0.00002386
Iteration 33/1000 | Loss: 0.00002386
Iteration 34/1000 | Loss: 0.00002386
Iteration 35/1000 | Loss: 0.00002386
Iteration 36/1000 | Loss: 0.00002386
Iteration 37/1000 | Loss: 0.00002385
Iteration 38/1000 | Loss: 0.00002385
Iteration 39/1000 | Loss: 0.00002385
Iteration 40/1000 | Loss: 0.00002385
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002385
Iteration 43/1000 | Loss: 0.00002385
Iteration 44/1000 | Loss: 0.00002385
Iteration 45/1000 | Loss: 0.00002385
Iteration 46/1000 | Loss: 0.00002385
Iteration 47/1000 | Loss: 0.00002385
Iteration 48/1000 | Loss: 0.00002385
Iteration 49/1000 | Loss: 0.00002385
Iteration 50/1000 | Loss: 0.00002384
Iteration 51/1000 | Loss: 0.00002384
Iteration 52/1000 | Loss: 0.00002384
Iteration 53/1000 | Loss: 0.00002384
Iteration 54/1000 | Loss: 0.00002384
Iteration 55/1000 | Loss: 0.00002384
Iteration 56/1000 | Loss: 0.00002384
Iteration 57/1000 | Loss: 0.00002383
Iteration 58/1000 | Loss: 0.00002382
Iteration 59/1000 | Loss: 0.00002382
Iteration 60/1000 | Loss: 0.00002381
Iteration 61/1000 | Loss: 0.00002381
Iteration 62/1000 | Loss: 0.00002381
Iteration 63/1000 | Loss: 0.00002381
Iteration 64/1000 | Loss: 0.00002381
Iteration 65/1000 | Loss: 0.00002381
Iteration 66/1000 | Loss: 0.00002381
Iteration 67/1000 | Loss: 0.00002381
Iteration 68/1000 | Loss: 0.00002380
Iteration 69/1000 | Loss: 0.00002380
Iteration 70/1000 | Loss: 0.00002379
Iteration 71/1000 | Loss: 0.00002379
Iteration 72/1000 | Loss: 0.00002379
Iteration 73/1000 | Loss: 0.00002379
Iteration 74/1000 | Loss: 0.00002379
Iteration 75/1000 | Loss: 0.00002379
Iteration 76/1000 | Loss: 0.00002378
Iteration 77/1000 | Loss: 0.00002378
Iteration 78/1000 | Loss: 0.00002378
Iteration 79/1000 | Loss: 0.00002378
Iteration 80/1000 | Loss: 0.00002377
Iteration 81/1000 | Loss: 0.00002377
Iteration 82/1000 | Loss: 0.00002377
Iteration 83/1000 | Loss: 0.00002376
Iteration 84/1000 | Loss: 0.00002376
Iteration 85/1000 | Loss: 0.00002376
Iteration 86/1000 | Loss: 0.00002376
Iteration 87/1000 | Loss: 0.00002376
Iteration 88/1000 | Loss: 0.00002375
Iteration 89/1000 | Loss: 0.00002375
Iteration 90/1000 | Loss: 0.00002375
Iteration 91/1000 | Loss: 0.00002375
Iteration 92/1000 | Loss: 0.00002374
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002374
Iteration 95/1000 | Loss: 0.00002374
Iteration 96/1000 | Loss: 0.00002374
Iteration 97/1000 | Loss: 0.00002374
Iteration 98/1000 | Loss: 0.00002374
Iteration 99/1000 | Loss: 0.00002374
Iteration 100/1000 | Loss: 0.00002374
Iteration 101/1000 | Loss: 0.00002374
Iteration 102/1000 | Loss: 0.00002374
Iteration 103/1000 | Loss: 0.00002374
Iteration 104/1000 | Loss: 0.00002374
Iteration 105/1000 | Loss: 0.00002374
Iteration 106/1000 | Loss: 0.00002374
Iteration 107/1000 | Loss: 0.00002374
Iteration 108/1000 | Loss: 0.00002374
Iteration 109/1000 | Loss: 0.00002374
Iteration 110/1000 | Loss: 0.00002374
Iteration 111/1000 | Loss: 0.00002374
Iteration 112/1000 | Loss: 0.00002374
Iteration 113/1000 | Loss: 0.00002374
Iteration 114/1000 | Loss: 0.00002374
Iteration 115/1000 | Loss: 0.00002374
Iteration 116/1000 | Loss: 0.00002374
Iteration 117/1000 | Loss: 0.00002374
Iteration 118/1000 | Loss: 0.00002374
Iteration 119/1000 | Loss: 0.00002374
Iteration 120/1000 | Loss: 0.00002374
Iteration 121/1000 | Loss: 0.00002374
Iteration 122/1000 | Loss: 0.00002374
Iteration 123/1000 | Loss: 0.00002374
Iteration 124/1000 | Loss: 0.00002374
Iteration 125/1000 | Loss: 0.00002374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.373608367634006e-05, 2.373608367634006e-05, 2.373608367634006e-05, 2.373608367634006e-05, 2.373608367634006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.373608367634006e-05

Optimization complete. Final v2v error: 4.325778484344482 mm

Highest mean error: 4.732235908508301 mm for frame 173

Lowest mean error: 3.930541753768921 mm for frame 72

Saving results

Total time: 32.01005744934082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957018
Iteration 2/25 | Loss: 0.00169317
Iteration 3/25 | Loss: 0.00144077
Iteration 4/25 | Loss: 0.00140987
Iteration 5/25 | Loss: 0.00139431
Iteration 6/25 | Loss: 0.00138976
Iteration 7/25 | Loss: 0.00138808
Iteration 8/25 | Loss: 0.00138775
Iteration 9/25 | Loss: 0.00138775
Iteration 10/25 | Loss: 0.00138775
Iteration 11/25 | Loss: 0.00138775
Iteration 12/25 | Loss: 0.00138775
Iteration 13/25 | Loss: 0.00138775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013877529418095946, 0.0013877529418095946, 0.0013877529418095946, 0.0013877529418095946, 0.0013877529418095946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013877529418095946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48048377
Iteration 2/25 | Loss: 0.00124562
Iteration 3/25 | Loss: 0.00124562
Iteration 4/25 | Loss: 0.00124562
Iteration 5/25 | Loss: 0.00124562
Iteration 6/25 | Loss: 0.00124562
Iteration 7/25 | Loss: 0.00124562
Iteration 8/25 | Loss: 0.00124562
Iteration 9/25 | Loss: 0.00124562
Iteration 10/25 | Loss: 0.00124562
Iteration 11/25 | Loss: 0.00124562
Iteration 12/25 | Loss: 0.00124562
Iteration 13/25 | Loss: 0.00124562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012456208933144808, 0.0012456208933144808, 0.0012456208933144808, 0.0012456208933144808, 0.0012456208933144808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012456208933144808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124562
Iteration 2/1000 | Loss: 0.00007402
Iteration 3/1000 | Loss: 0.00004681
Iteration 4/1000 | Loss: 0.00003626
Iteration 5/1000 | Loss: 0.00003279
Iteration 6/1000 | Loss: 0.00003106
Iteration 7/1000 | Loss: 0.00003016
Iteration 8/1000 | Loss: 0.00002953
Iteration 9/1000 | Loss: 0.00002899
Iteration 10/1000 | Loss: 0.00002852
Iteration 11/1000 | Loss: 0.00002810
Iteration 12/1000 | Loss: 0.00002779
Iteration 13/1000 | Loss: 0.00002759
Iteration 14/1000 | Loss: 0.00002753
Iteration 15/1000 | Loss: 0.00002733
Iteration 16/1000 | Loss: 0.00002727
Iteration 17/1000 | Loss: 0.00002724
Iteration 18/1000 | Loss: 0.00002721
Iteration 19/1000 | Loss: 0.00002720
Iteration 20/1000 | Loss: 0.00002716
Iteration 21/1000 | Loss: 0.00002716
Iteration 22/1000 | Loss: 0.00002714
Iteration 23/1000 | Loss: 0.00002713
Iteration 24/1000 | Loss: 0.00002713
Iteration 25/1000 | Loss: 0.00002711
Iteration 26/1000 | Loss: 0.00002711
Iteration 27/1000 | Loss: 0.00002711
Iteration 28/1000 | Loss: 0.00002711
Iteration 29/1000 | Loss: 0.00002710
Iteration 30/1000 | Loss: 0.00002710
Iteration 31/1000 | Loss: 0.00002710
Iteration 32/1000 | Loss: 0.00002709
Iteration 33/1000 | Loss: 0.00002709
Iteration 34/1000 | Loss: 0.00002708
Iteration 35/1000 | Loss: 0.00002708
Iteration 36/1000 | Loss: 0.00002708
Iteration 37/1000 | Loss: 0.00002708
Iteration 38/1000 | Loss: 0.00002707
Iteration 39/1000 | Loss: 0.00002707
Iteration 40/1000 | Loss: 0.00002707
Iteration 41/1000 | Loss: 0.00002706
Iteration 42/1000 | Loss: 0.00002706
Iteration 43/1000 | Loss: 0.00002706
Iteration 44/1000 | Loss: 0.00002705
Iteration 45/1000 | Loss: 0.00002705
Iteration 46/1000 | Loss: 0.00002705
Iteration 47/1000 | Loss: 0.00002705
Iteration 48/1000 | Loss: 0.00002704
Iteration 49/1000 | Loss: 0.00002704
Iteration 50/1000 | Loss: 0.00002704
Iteration 51/1000 | Loss: 0.00002704
Iteration 52/1000 | Loss: 0.00002703
Iteration 53/1000 | Loss: 0.00002703
Iteration 54/1000 | Loss: 0.00002702
Iteration 55/1000 | Loss: 0.00002702
Iteration 56/1000 | Loss: 0.00002702
Iteration 57/1000 | Loss: 0.00002701
Iteration 58/1000 | Loss: 0.00002701
Iteration 59/1000 | Loss: 0.00002701
Iteration 60/1000 | Loss: 0.00002701
Iteration 61/1000 | Loss: 0.00002701
Iteration 62/1000 | Loss: 0.00002701
Iteration 63/1000 | Loss: 0.00002701
Iteration 64/1000 | Loss: 0.00002701
Iteration 65/1000 | Loss: 0.00002701
Iteration 66/1000 | Loss: 0.00002701
Iteration 67/1000 | Loss: 0.00002700
Iteration 68/1000 | Loss: 0.00002700
Iteration 69/1000 | Loss: 0.00002700
Iteration 70/1000 | Loss: 0.00002700
Iteration 71/1000 | Loss: 0.00002700
Iteration 72/1000 | Loss: 0.00002700
Iteration 73/1000 | Loss: 0.00002700
Iteration 74/1000 | Loss: 0.00002699
Iteration 75/1000 | Loss: 0.00002699
Iteration 76/1000 | Loss: 0.00002699
Iteration 77/1000 | Loss: 0.00002698
Iteration 78/1000 | Loss: 0.00002698
Iteration 79/1000 | Loss: 0.00002698
Iteration 80/1000 | Loss: 0.00002697
Iteration 81/1000 | Loss: 0.00002697
Iteration 82/1000 | Loss: 0.00002697
Iteration 83/1000 | Loss: 0.00002697
Iteration 84/1000 | Loss: 0.00002697
Iteration 85/1000 | Loss: 0.00002697
Iteration 86/1000 | Loss: 0.00002697
Iteration 87/1000 | Loss: 0.00002696
Iteration 88/1000 | Loss: 0.00002696
Iteration 89/1000 | Loss: 0.00002696
Iteration 90/1000 | Loss: 0.00002696
Iteration 91/1000 | Loss: 0.00002696
Iteration 92/1000 | Loss: 0.00002696
Iteration 93/1000 | Loss: 0.00002695
Iteration 94/1000 | Loss: 0.00002695
Iteration 95/1000 | Loss: 0.00002695
Iteration 96/1000 | Loss: 0.00002694
Iteration 97/1000 | Loss: 0.00002694
Iteration 98/1000 | Loss: 0.00002694
Iteration 99/1000 | Loss: 0.00002693
Iteration 100/1000 | Loss: 0.00002693
Iteration 101/1000 | Loss: 0.00002693
Iteration 102/1000 | Loss: 0.00002693
Iteration 103/1000 | Loss: 0.00002693
Iteration 104/1000 | Loss: 0.00002693
Iteration 105/1000 | Loss: 0.00002693
Iteration 106/1000 | Loss: 0.00002693
Iteration 107/1000 | Loss: 0.00002693
Iteration 108/1000 | Loss: 0.00002693
Iteration 109/1000 | Loss: 0.00002693
Iteration 110/1000 | Loss: 0.00002692
Iteration 111/1000 | Loss: 0.00002692
Iteration 112/1000 | Loss: 0.00002692
Iteration 113/1000 | Loss: 0.00002692
Iteration 114/1000 | Loss: 0.00002692
Iteration 115/1000 | Loss: 0.00002692
Iteration 116/1000 | Loss: 0.00002692
Iteration 117/1000 | Loss: 0.00002692
Iteration 118/1000 | Loss: 0.00002692
Iteration 119/1000 | Loss: 0.00002692
Iteration 120/1000 | Loss: 0.00002692
Iteration 121/1000 | Loss: 0.00002692
Iteration 122/1000 | Loss: 0.00002692
Iteration 123/1000 | Loss: 0.00002692
Iteration 124/1000 | Loss: 0.00002691
Iteration 125/1000 | Loss: 0.00002691
Iteration 126/1000 | Loss: 0.00002691
Iteration 127/1000 | Loss: 0.00002691
Iteration 128/1000 | Loss: 0.00002691
Iteration 129/1000 | Loss: 0.00002691
Iteration 130/1000 | Loss: 0.00002690
Iteration 131/1000 | Loss: 0.00002690
Iteration 132/1000 | Loss: 0.00002690
Iteration 133/1000 | Loss: 0.00002690
Iteration 134/1000 | Loss: 0.00002690
Iteration 135/1000 | Loss: 0.00002690
Iteration 136/1000 | Loss: 0.00002690
Iteration 137/1000 | Loss: 0.00002690
Iteration 138/1000 | Loss: 0.00002690
Iteration 139/1000 | Loss: 0.00002690
Iteration 140/1000 | Loss: 0.00002690
Iteration 141/1000 | Loss: 0.00002690
Iteration 142/1000 | Loss: 0.00002690
Iteration 143/1000 | Loss: 0.00002690
Iteration 144/1000 | Loss: 0.00002690
Iteration 145/1000 | Loss: 0.00002690
Iteration 146/1000 | Loss: 0.00002689
Iteration 147/1000 | Loss: 0.00002689
Iteration 148/1000 | Loss: 0.00002689
Iteration 149/1000 | Loss: 0.00002689
Iteration 150/1000 | Loss: 0.00002689
Iteration 151/1000 | Loss: 0.00002689
Iteration 152/1000 | Loss: 0.00002689
Iteration 153/1000 | Loss: 0.00002689
Iteration 154/1000 | Loss: 0.00002689
Iteration 155/1000 | Loss: 0.00002689
Iteration 156/1000 | Loss: 0.00002689
Iteration 157/1000 | Loss: 0.00002689
Iteration 158/1000 | Loss: 0.00002689
Iteration 159/1000 | Loss: 0.00002688
Iteration 160/1000 | Loss: 0.00002688
Iteration 161/1000 | Loss: 0.00002688
Iteration 162/1000 | Loss: 0.00002688
Iteration 163/1000 | Loss: 0.00002688
Iteration 164/1000 | Loss: 0.00002688
Iteration 165/1000 | Loss: 0.00002688
Iteration 166/1000 | Loss: 0.00002688
Iteration 167/1000 | Loss: 0.00002688
Iteration 168/1000 | Loss: 0.00002688
Iteration 169/1000 | Loss: 0.00002688
Iteration 170/1000 | Loss: 0.00002688
Iteration 171/1000 | Loss: 0.00002688
Iteration 172/1000 | Loss: 0.00002688
Iteration 173/1000 | Loss: 0.00002688
Iteration 174/1000 | Loss: 0.00002687
Iteration 175/1000 | Loss: 0.00002687
Iteration 176/1000 | Loss: 0.00002687
Iteration 177/1000 | Loss: 0.00002687
Iteration 178/1000 | Loss: 0.00002687
Iteration 179/1000 | Loss: 0.00002687
Iteration 180/1000 | Loss: 0.00002687
Iteration 181/1000 | Loss: 0.00002687
Iteration 182/1000 | Loss: 0.00002687
Iteration 183/1000 | Loss: 0.00002687
Iteration 184/1000 | Loss: 0.00002687
Iteration 185/1000 | Loss: 0.00002687
Iteration 186/1000 | Loss: 0.00002686
Iteration 187/1000 | Loss: 0.00002686
Iteration 188/1000 | Loss: 0.00002686
Iteration 189/1000 | Loss: 0.00002686
Iteration 190/1000 | Loss: 0.00002686
Iteration 191/1000 | Loss: 0.00002686
Iteration 192/1000 | Loss: 0.00002686
Iteration 193/1000 | Loss: 0.00002686
Iteration 194/1000 | Loss: 0.00002686
Iteration 195/1000 | Loss: 0.00002686
Iteration 196/1000 | Loss: 0.00002686
Iteration 197/1000 | Loss: 0.00002686
Iteration 198/1000 | Loss: 0.00002686
Iteration 199/1000 | Loss: 0.00002686
Iteration 200/1000 | Loss: 0.00002686
Iteration 201/1000 | Loss: 0.00002685
Iteration 202/1000 | Loss: 0.00002685
Iteration 203/1000 | Loss: 0.00002685
Iteration 204/1000 | Loss: 0.00002685
Iteration 205/1000 | Loss: 0.00002685
Iteration 206/1000 | Loss: 0.00002685
Iteration 207/1000 | Loss: 0.00002685
Iteration 208/1000 | Loss: 0.00002685
Iteration 209/1000 | Loss: 0.00002685
Iteration 210/1000 | Loss: 0.00002685
Iteration 211/1000 | Loss: 0.00002685
Iteration 212/1000 | Loss: 0.00002685
Iteration 213/1000 | Loss: 0.00002685
Iteration 214/1000 | Loss: 0.00002685
Iteration 215/1000 | Loss: 0.00002685
Iteration 216/1000 | Loss: 0.00002685
Iteration 217/1000 | Loss: 0.00002685
Iteration 218/1000 | Loss: 0.00002685
Iteration 219/1000 | Loss: 0.00002685
Iteration 220/1000 | Loss: 0.00002685
Iteration 221/1000 | Loss: 0.00002685
Iteration 222/1000 | Loss: 0.00002685
Iteration 223/1000 | Loss: 0.00002685
Iteration 224/1000 | Loss: 0.00002685
Iteration 225/1000 | Loss: 0.00002685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.685259642021265e-05, 2.685259642021265e-05, 2.685259642021265e-05, 2.685259642021265e-05, 2.685259642021265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.685259642021265e-05

Optimization complete. Final v2v error: 4.378712177276611 mm

Highest mean error: 6.089544296264648 mm for frame 71

Lowest mean error: 3.6624200344085693 mm for frame 36

Saving results

Total time: 45.554749488830566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872638
Iteration 2/25 | Loss: 0.00173909
Iteration 3/25 | Loss: 0.00141903
Iteration 4/25 | Loss: 0.00137641
Iteration 5/25 | Loss: 0.00136179
Iteration 6/25 | Loss: 0.00135507
Iteration 7/25 | Loss: 0.00134787
Iteration 8/25 | Loss: 0.00134117
Iteration 9/25 | Loss: 0.00133783
Iteration 10/25 | Loss: 0.00133554
Iteration 11/25 | Loss: 0.00133523
Iteration 12/25 | Loss: 0.00133841
Iteration 13/25 | Loss: 0.00133812
Iteration 14/25 | Loss: 0.00134300
Iteration 15/25 | Loss: 0.00134049
Iteration 16/25 | Loss: 0.00133469
Iteration 17/25 | Loss: 0.00133307
Iteration 18/25 | Loss: 0.00133240
Iteration 19/25 | Loss: 0.00133382
Iteration 20/25 | Loss: 0.00133138
Iteration 21/25 | Loss: 0.00133104
Iteration 22/25 | Loss: 0.00133096
Iteration 23/25 | Loss: 0.00133096
Iteration 24/25 | Loss: 0.00133096
Iteration 25/25 | Loss: 0.00133096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98738694
Iteration 2/25 | Loss: 0.00110026
Iteration 3/25 | Loss: 0.00109035
Iteration 4/25 | Loss: 0.00109034
Iteration 5/25 | Loss: 0.00109034
Iteration 6/25 | Loss: 0.00109034
Iteration 7/25 | Loss: 0.00109034
Iteration 8/25 | Loss: 0.00109034
Iteration 9/25 | Loss: 0.00109034
Iteration 10/25 | Loss: 0.00109034
Iteration 11/25 | Loss: 0.00109034
Iteration 12/25 | Loss: 0.00109034
Iteration 13/25 | Loss: 0.00109034
Iteration 14/25 | Loss: 0.00109034
Iteration 15/25 | Loss: 0.00109034
Iteration 16/25 | Loss: 0.00109034
Iteration 17/25 | Loss: 0.00109034
Iteration 18/25 | Loss: 0.00109034
Iteration 19/25 | Loss: 0.00109034
Iteration 20/25 | Loss: 0.00109034
Iteration 21/25 | Loss: 0.00109034
Iteration 22/25 | Loss: 0.00109034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010903411312028766, 0.0010903411312028766, 0.0010903411312028766, 0.0010903411312028766, 0.0010903411312028766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010903411312028766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109034
Iteration 2/1000 | Loss: 0.00006247
Iteration 3/1000 | Loss: 0.00003848
Iteration 4/1000 | Loss: 0.00003414
Iteration 5/1000 | Loss: 0.00003250
Iteration 6/1000 | Loss: 0.00003159
Iteration 7/1000 | Loss: 0.00003103
Iteration 8/1000 | Loss: 0.00003054
Iteration 9/1000 | Loss: 0.00003018
Iteration 10/1000 | Loss: 0.00002995
Iteration 11/1000 | Loss: 0.00002979
Iteration 12/1000 | Loss: 0.00002975
Iteration 13/1000 | Loss: 0.00002974
Iteration 14/1000 | Loss: 0.00002973
Iteration 15/1000 | Loss: 0.00002969
Iteration 16/1000 | Loss: 0.00002968
Iteration 17/1000 | Loss: 0.00002968
Iteration 18/1000 | Loss: 0.00002968
Iteration 19/1000 | Loss: 0.00002967
Iteration 20/1000 | Loss: 0.00002967
Iteration 21/1000 | Loss: 0.00002967
Iteration 22/1000 | Loss: 0.00002966
Iteration 23/1000 | Loss: 0.00002966
Iteration 24/1000 | Loss: 0.00002964
Iteration 25/1000 | Loss: 0.00002964
Iteration 26/1000 | Loss: 0.00002964
Iteration 27/1000 | Loss: 0.00002964
Iteration 28/1000 | Loss: 0.00002963
Iteration 29/1000 | Loss: 0.00002963
Iteration 30/1000 | Loss: 0.00002963
Iteration 31/1000 | Loss: 0.00002963
Iteration 32/1000 | Loss: 0.00002962
Iteration 33/1000 | Loss: 0.00002962
Iteration 34/1000 | Loss: 0.00002961
Iteration 35/1000 | Loss: 0.00002961
Iteration 36/1000 | Loss: 0.00002960
Iteration 37/1000 | Loss: 0.00002960
Iteration 38/1000 | Loss: 0.00002960
Iteration 39/1000 | Loss: 0.00002960
Iteration 40/1000 | Loss: 0.00002960
Iteration 41/1000 | Loss: 0.00002959
Iteration 42/1000 | Loss: 0.00002959
Iteration 43/1000 | Loss: 0.00002959
Iteration 44/1000 | Loss: 0.00002958
Iteration 45/1000 | Loss: 0.00002958
Iteration 46/1000 | Loss: 0.00002958
Iteration 47/1000 | Loss: 0.00002958
Iteration 48/1000 | Loss: 0.00002957
Iteration 49/1000 | Loss: 0.00002957
Iteration 50/1000 | Loss: 0.00002957
Iteration 51/1000 | Loss: 0.00002957
Iteration 52/1000 | Loss: 0.00002957
Iteration 53/1000 | Loss: 0.00002956
Iteration 54/1000 | Loss: 0.00002956
Iteration 55/1000 | Loss: 0.00002956
Iteration 56/1000 | Loss: 0.00002956
Iteration 57/1000 | Loss: 0.00002956
Iteration 58/1000 | Loss: 0.00002956
Iteration 59/1000 | Loss: 0.00002955
Iteration 60/1000 | Loss: 0.00002955
Iteration 61/1000 | Loss: 0.00002955
Iteration 62/1000 | Loss: 0.00002955
Iteration 63/1000 | Loss: 0.00002954
Iteration 64/1000 | Loss: 0.00002954
Iteration 65/1000 | Loss: 0.00002954
Iteration 66/1000 | Loss: 0.00002953
Iteration 67/1000 | Loss: 0.00002953
Iteration 68/1000 | Loss: 0.00002953
Iteration 69/1000 | Loss: 0.00002953
Iteration 70/1000 | Loss: 0.00002953
Iteration 71/1000 | Loss: 0.00002952
Iteration 72/1000 | Loss: 0.00002952
Iteration 73/1000 | Loss: 0.00002952
Iteration 74/1000 | Loss: 0.00002952
Iteration 75/1000 | Loss: 0.00002952
Iteration 76/1000 | Loss: 0.00002952
Iteration 77/1000 | Loss: 0.00002952
Iteration 78/1000 | Loss: 0.00002952
Iteration 79/1000 | Loss: 0.00002952
Iteration 80/1000 | Loss: 0.00002952
Iteration 81/1000 | Loss: 0.00002952
Iteration 82/1000 | Loss: 0.00002952
Iteration 83/1000 | Loss: 0.00002952
Iteration 84/1000 | Loss: 0.00002952
Iteration 85/1000 | Loss: 0.00002951
Iteration 86/1000 | Loss: 0.00002951
Iteration 87/1000 | Loss: 0.00002951
Iteration 88/1000 | Loss: 0.00002951
Iteration 89/1000 | Loss: 0.00002951
Iteration 90/1000 | Loss: 0.00002951
Iteration 91/1000 | Loss: 0.00002951
Iteration 92/1000 | Loss: 0.00002951
Iteration 93/1000 | Loss: 0.00002951
Iteration 94/1000 | Loss: 0.00002951
Iteration 95/1000 | Loss: 0.00002951
Iteration 96/1000 | Loss: 0.00002951
Iteration 97/1000 | Loss: 0.00002951
Iteration 98/1000 | Loss: 0.00002951
Iteration 99/1000 | Loss: 0.00002951
Iteration 100/1000 | Loss: 0.00002951
Iteration 101/1000 | Loss: 0.00002951
Iteration 102/1000 | Loss: 0.00002951
Iteration 103/1000 | Loss: 0.00002951
Iteration 104/1000 | Loss: 0.00002951
Iteration 105/1000 | Loss: 0.00002951
Iteration 106/1000 | Loss: 0.00002951
Iteration 107/1000 | Loss: 0.00002951
Iteration 108/1000 | Loss: 0.00002951
Iteration 109/1000 | Loss: 0.00002951
Iteration 110/1000 | Loss: 0.00002951
Iteration 111/1000 | Loss: 0.00002951
Iteration 112/1000 | Loss: 0.00002951
Iteration 113/1000 | Loss: 0.00002951
Iteration 114/1000 | Loss: 0.00002951
Iteration 115/1000 | Loss: 0.00002951
Iteration 116/1000 | Loss: 0.00002951
Iteration 117/1000 | Loss: 0.00002951
Iteration 118/1000 | Loss: 0.00002951
Iteration 119/1000 | Loss: 0.00002951
Iteration 120/1000 | Loss: 0.00002951
Iteration 121/1000 | Loss: 0.00002951
Iteration 122/1000 | Loss: 0.00002951
Iteration 123/1000 | Loss: 0.00002951
Iteration 124/1000 | Loss: 0.00002951
Iteration 125/1000 | Loss: 0.00002951
Iteration 126/1000 | Loss: 0.00002951
Iteration 127/1000 | Loss: 0.00002951
Iteration 128/1000 | Loss: 0.00002951
Iteration 129/1000 | Loss: 0.00002951
Iteration 130/1000 | Loss: 0.00002951
Iteration 131/1000 | Loss: 0.00002951
Iteration 132/1000 | Loss: 0.00002951
Iteration 133/1000 | Loss: 0.00002951
Iteration 134/1000 | Loss: 0.00002951
Iteration 135/1000 | Loss: 0.00002951
Iteration 136/1000 | Loss: 0.00002951
Iteration 137/1000 | Loss: 0.00002951
Iteration 138/1000 | Loss: 0.00002951
Iteration 139/1000 | Loss: 0.00002951
Iteration 140/1000 | Loss: 0.00002951
Iteration 141/1000 | Loss: 0.00002951
Iteration 142/1000 | Loss: 0.00002951
Iteration 143/1000 | Loss: 0.00002951
Iteration 144/1000 | Loss: 0.00002951
Iteration 145/1000 | Loss: 0.00002951
Iteration 146/1000 | Loss: 0.00002951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.9511813409044407e-05, 2.9511813409044407e-05, 2.9511813409044407e-05, 2.9511813409044407e-05, 2.9511813409044407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9511813409044407e-05

Optimization complete. Final v2v error: 4.694376468658447 mm

Highest mean error: 10.739352226257324 mm for frame 36

Lowest mean error: 4.113553524017334 mm for frame 46

Saving results

Total time: 69.44192433357239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_2582/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_2582/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061282
Iteration 2/25 | Loss: 0.00219532
Iteration 3/25 | Loss: 0.00184035
Iteration 4/25 | Loss: 0.00176752
Iteration 5/25 | Loss: 0.00174309
Iteration 6/25 | Loss: 0.00173742
Iteration 7/25 | Loss: 0.00173465
Iteration 8/25 | Loss: 0.00173282
Iteration 9/25 | Loss: 0.00173201
Iteration 10/25 | Loss: 0.00173193
Iteration 11/25 | Loss: 0.00173193
Iteration 12/25 | Loss: 0.00173193
Iteration 13/25 | Loss: 0.00173193
Iteration 14/25 | Loss: 0.00173193
Iteration 15/25 | Loss: 0.00173193
Iteration 16/25 | Loss: 0.00173193
Iteration 17/25 | Loss: 0.00173193
Iteration 18/25 | Loss: 0.00173193
Iteration 19/25 | Loss: 0.00173193
Iteration 20/25 | Loss: 0.00173193
Iteration 21/25 | Loss: 0.00173193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017319266917183995, 0.0017319266917183995, 0.0017319266917183995, 0.0017319266917183995, 0.0017319266917183995]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017319266917183995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53350759
Iteration 2/25 | Loss: 0.00446838
Iteration 3/25 | Loss: 0.00446837
Iteration 4/25 | Loss: 0.00446837
Iteration 5/25 | Loss: 0.00446837
Iteration 6/25 | Loss: 0.00446837
Iteration 7/25 | Loss: 0.00446837
Iteration 8/25 | Loss: 0.00446837
Iteration 9/25 | Loss: 0.00446837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.004468374419957399, 0.004468374419957399, 0.004468374419957399, 0.004468374419957399, 0.004468374419957399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004468374419957399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00446837
Iteration 2/1000 | Loss: 0.00052283
Iteration 3/1000 | Loss: 0.00032679
Iteration 4/1000 | Loss: 0.00025392
Iteration 5/1000 | Loss: 0.00022952
Iteration 6/1000 | Loss: 0.00021443
Iteration 7/1000 | Loss: 0.00020262
Iteration 8/1000 | Loss: 0.00019524
Iteration 9/1000 | Loss: 0.00727098
Iteration 10/1000 | Loss: 0.02364339
Iteration 11/1000 | Loss: 0.00119469
Iteration 12/1000 | Loss: 0.00035312
Iteration 13/1000 | Loss: 0.00017306
Iteration 14/1000 | Loss: 0.00012415
Iteration 15/1000 | Loss: 0.00007972
Iteration 16/1000 | Loss: 0.00005523
Iteration 17/1000 | Loss: 0.00004901
Iteration 18/1000 | Loss: 0.00004441
Iteration 19/1000 | Loss: 0.00004001
Iteration 20/1000 | Loss: 0.00003733
Iteration 21/1000 | Loss: 0.00003530
Iteration 22/1000 | Loss: 0.00003398
Iteration 23/1000 | Loss: 0.00003269
Iteration 24/1000 | Loss: 0.00003146
Iteration 25/1000 | Loss: 0.00003066
Iteration 26/1000 | Loss: 0.00003008
Iteration 27/1000 | Loss: 0.00002968
Iteration 28/1000 | Loss: 0.00002945
Iteration 29/1000 | Loss: 0.00002939
Iteration 30/1000 | Loss: 0.00002918
Iteration 31/1000 | Loss: 0.00002914
Iteration 32/1000 | Loss: 0.00002900
Iteration 33/1000 | Loss: 0.00002894
Iteration 34/1000 | Loss: 0.00002885
Iteration 35/1000 | Loss: 0.00002873
Iteration 36/1000 | Loss: 0.00002868
Iteration 37/1000 | Loss: 0.00002868
Iteration 38/1000 | Loss: 0.00002867
Iteration 39/1000 | Loss: 0.00002867
Iteration 40/1000 | Loss: 0.00002862
Iteration 41/1000 | Loss: 0.00002858
Iteration 42/1000 | Loss: 0.00002856
Iteration 43/1000 | Loss: 0.00002855
Iteration 44/1000 | Loss: 0.00002854
Iteration 45/1000 | Loss: 0.00002853
Iteration 46/1000 | Loss: 0.00002852
Iteration 47/1000 | Loss: 0.00002847
Iteration 48/1000 | Loss: 0.00002843
Iteration 49/1000 | Loss: 0.00002839
Iteration 50/1000 | Loss: 0.00002838
Iteration 51/1000 | Loss: 0.00002837
Iteration 52/1000 | Loss: 0.00002836
Iteration 53/1000 | Loss: 0.00002836
Iteration 54/1000 | Loss: 0.00002835
Iteration 55/1000 | Loss: 0.00002835
Iteration 56/1000 | Loss: 0.00002835
Iteration 57/1000 | Loss: 0.00002835
Iteration 58/1000 | Loss: 0.00002835
Iteration 59/1000 | Loss: 0.00002835
Iteration 60/1000 | Loss: 0.00002835
Iteration 61/1000 | Loss: 0.00002834
Iteration 62/1000 | Loss: 0.00002831
Iteration 63/1000 | Loss: 0.00002831
Iteration 64/1000 | Loss: 0.00002831
Iteration 65/1000 | Loss: 0.00002831
Iteration 66/1000 | Loss: 0.00002831
Iteration 67/1000 | Loss: 0.00002831
Iteration 68/1000 | Loss: 0.00002831
Iteration 69/1000 | Loss: 0.00002831
Iteration 70/1000 | Loss: 0.00002831
Iteration 71/1000 | Loss: 0.00002831
Iteration 72/1000 | Loss: 0.00002831
Iteration 73/1000 | Loss: 0.00002831
Iteration 74/1000 | Loss: 0.00002831
Iteration 75/1000 | Loss: 0.00002831
Iteration 76/1000 | Loss: 0.00002831
Iteration 77/1000 | Loss: 0.00002831
Iteration 78/1000 | Loss: 0.00002831
Iteration 79/1000 | Loss: 0.00002831
Iteration 80/1000 | Loss: 0.00002831
Iteration 81/1000 | Loss: 0.00002831
Iteration 82/1000 | Loss: 0.00002831
Iteration 83/1000 | Loss: 0.00002831
Iteration 84/1000 | Loss: 0.00002831
Iteration 85/1000 | Loss: 0.00002831
Iteration 86/1000 | Loss: 0.00002831
Iteration 87/1000 | Loss: 0.00002831
Iteration 88/1000 | Loss: 0.00002830
Iteration 89/1000 | Loss: 0.00002830
Iteration 90/1000 | Loss: 0.00002830
Iteration 91/1000 | Loss: 0.00002830
Iteration 92/1000 | Loss: 0.00002830
Iteration 93/1000 | Loss: 0.00002830
Iteration 94/1000 | Loss: 0.00002830
Iteration 95/1000 | Loss: 0.00002830
Iteration 96/1000 | Loss: 0.00002830
Iteration 97/1000 | Loss: 0.00002830
Iteration 98/1000 | Loss: 0.00002830
Iteration 99/1000 | Loss: 0.00002830
Iteration 100/1000 | Loss: 0.00002830
Iteration 101/1000 | Loss: 0.00002830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.8304879378993064e-05, 2.8304879378993064e-05, 2.8304879378993064e-05, 2.8304879378993064e-05, 2.8304879378993064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8304879378993064e-05

Optimization complete. Final v2v error: 4.500607013702393 mm

Highest mean error: 4.80268669128418 mm for frame 180

Lowest mean error: 4.204512596130371 mm for frame 100

Saving results

Total time: 69.57317757606506
