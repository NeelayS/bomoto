Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=19, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1064-1119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103986
Iteration 2/25 | Loss: 0.00221563
Iteration 3/25 | Loss: 0.00142527
Iteration 4/25 | Loss: 0.00139149
Iteration 5/25 | Loss: 0.00125230
Iteration 6/25 | Loss: 0.00134968
Iteration 7/25 | Loss: 0.00122891
Iteration 8/25 | Loss: 0.00113446
Iteration 9/25 | Loss: 0.00108622
Iteration 10/25 | Loss: 0.00103769
Iteration 11/25 | Loss: 0.00096950
Iteration 12/25 | Loss: 0.00100145
Iteration 13/25 | Loss: 0.00097002
Iteration 14/25 | Loss: 0.00091046
Iteration 15/25 | Loss: 0.00089154
Iteration 16/25 | Loss: 0.00087942
Iteration 17/25 | Loss: 0.00088269
Iteration 18/25 | Loss: 0.00088101
Iteration 19/25 | Loss: 0.00087139
Iteration 20/25 | Loss: 0.00086084
Iteration 21/25 | Loss: 0.00086004
Iteration 22/25 | Loss: 0.00085529
Iteration 23/25 | Loss: 0.00085578
Iteration 24/25 | Loss: 0.00085237
Iteration 25/25 | Loss: 0.00085151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69980657
Iteration 2/25 | Loss: 0.00238111
Iteration 3/25 | Loss: 0.00238111
Iteration 4/25 | Loss: 0.00238110
Iteration 5/25 | Loss: 0.00238110
Iteration 6/25 | Loss: 0.00235125
Iteration 7/25 | Loss: 0.00235124
Iteration 8/25 | Loss: 0.00235124
Iteration 9/25 | Loss: 0.00235124
Iteration 10/25 | Loss: 0.00235123
Iteration 11/25 | Loss: 0.00235123
Iteration 12/25 | Loss: 0.00235123
Iteration 13/25 | Loss: 0.00235123
Iteration 14/25 | Loss: 0.00235123
Iteration 15/25 | Loss: 0.00235123
Iteration 16/25 | Loss: 0.00235123
Iteration 17/25 | Loss: 0.00235123
Iteration 18/25 | Loss: 0.00235123
Iteration 19/25 | Loss: 0.00235123
Iteration 20/25 | Loss: 0.00235123
Iteration 21/25 | Loss: 0.00235123
Iteration 22/25 | Loss: 0.00235123
Iteration 23/25 | Loss: 0.00235123
Iteration 24/25 | Loss: 0.00235123
Iteration 25/25 | Loss: 0.00235123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235123
Iteration 2/1000 | Loss: 0.00012187
Iteration 3/1000 | Loss: 0.00008818
Iteration 4/1000 | Loss: 0.00034358
Iteration 5/1000 | Loss: 0.00170884
Iteration 6/1000 | Loss: 0.00015825
Iteration 7/1000 | Loss: 0.00008091
Iteration 8/1000 | Loss: 0.00006290
Iteration 9/1000 | Loss: 0.00004142
Iteration 10/1000 | Loss: 0.00021464
Iteration 11/1000 | Loss: 0.00015240
Iteration 12/1000 | Loss: 0.00016838
Iteration 13/1000 | Loss: 0.00003358
Iteration 14/1000 | Loss: 0.00004265
Iteration 15/1000 | Loss: 0.00003609
Iteration 16/1000 | Loss: 0.00002855
Iteration 17/1000 | Loss: 0.00002758
Iteration 18/1000 | Loss: 0.00002701
Iteration 19/1000 | Loss: 0.00092857
Iteration 20/1000 | Loss: 0.00027222
Iteration 21/1000 | Loss: 0.00052377
Iteration 22/1000 | Loss: 0.00003297
Iteration 23/1000 | Loss: 0.00003552
Iteration 24/1000 | Loss: 0.00002939
Iteration 25/1000 | Loss: 0.00002490
Iteration 26/1000 | Loss: 0.00002383
Iteration 27/1000 | Loss: 0.00002334
Iteration 28/1000 | Loss: 0.00002312
Iteration 29/1000 | Loss: 0.00002301
Iteration 30/1000 | Loss: 0.00002288
Iteration 31/1000 | Loss: 0.00002283
Iteration 32/1000 | Loss: 0.00002283
Iteration 33/1000 | Loss: 0.00002282
Iteration 34/1000 | Loss: 0.00002282
Iteration 35/1000 | Loss: 0.00002281
Iteration 36/1000 | Loss: 0.00002281
Iteration 37/1000 | Loss: 0.00002280
Iteration 38/1000 | Loss: 0.00002278
Iteration 39/1000 | Loss: 0.00002277
Iteration 40/1000 | Loss: 0.00002276
Iteration 41/1000 | Loss: 0.00002276
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00002272
Iteration 44/1000 | Loss: 0.00002272
Iteration 45/1000 | Loss: 0.00002271
Iteration 46/1000 | Loss: 0.00002271
Iteration 47/1000 | Loss: 0.00002270
Iteration 48/1000 | Loss: 0.00002269
Iteration 49/1000 | Loss: 0.00002269
Iteration 50/1000 | Loss: 0.00002268
Iteration 51/1000 | Loss: 0.00002268
Iteration 52/1000 | Loss: 0.00002267
Iteration 53/1000 | Loss: 0.00002267
Iteration 54/1000 | Loss: 0.00002267
Iteration 55/1000 | Loss: 0.00003247
Iteration 56/1000 | Loss: 0.00002266
Iteration 57/1000 | Loss: 0.00002265
Iteration 58/1000 | Loss: 0.00002264
Iteration 59/1000 | Loss: 0.00002264
Iteration 60/1000 | Loss: 0.00002264
Iteration 61/1000 | Loss: 0.00002264
Iteration 62/1000 | Loss: 0.00002263
Iteration 63/1000 | Loss: 0.00002263
Iteration 64/1000 | Loss: 0.00002263
Iteration 65/1000 | Loss: 0.00002263
Iteration 66/1000 | Loss: 0.00002263
Iteration 67/1000 | Loss: 0.00002263
Iteration 68/1000 | Loss: 0.00002262
Iteration 69/1000 | Loss: 0.00002846
Iteration 70/1000 | Loss: 0.00002525
Iteration 71/1000 | Loss: 0.00002471
Iteration 72/1000 | Loss: 0.00002261
Iteration 73/1000 | Loss: 0.00002260
Iteration 74/1000 | Loss: 0.00002260
Iteration 75/1000 | Loss: 0.00002260
Iteration 76/1000 | Loss: 0.00002260
Iteration 77/1000 | Loss: 0.00002260
Iteration 78/1000 | Loss: 0.00002260
Iteration 79/1000 | Loss: 0.00002260
Iteration 80/1000 | Loss: 0.00002260
Iteration 81/1000 | Loss: 0.00002260
Iteration 82/1000 | Loss: 0.00002260
Iteration 83/1000 | Loss: 0.00002260
Iteration 84/1000 | Loss: 0.00002260
Iteration 85/1000 | Loss: 0.00002260
Iteration 86/1000 | Loss: 0.00002260
Iteration 87/1000 | Loss: 0.00002260
Iteration 88/1000 | Loss: 0.00002260
Iteration 89/1000 | Loss: 0.00002259
Iteration 90/1000 | Loss: 0.00002259
Iteration 91/1000 | Loss: 0.00002259
Iteration 92/1000 | Loss: 0.00002259
Iteration 93/1000 | Loss: 0.00002259
Iteration 94/1000 | Loss: 0.00002259
Iteration 95/1000 | Loss: 0.00002259
Iteration 96/1000 | Loss: 0.00002259
Iteration 97/1000 | Loss: 0.00002259
Iteration 98/1000 | Loss: 0.00002259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.259467146359384e-05, 2.259467146359384e-05, 2.259467146359384e-05, 2.259467146359384e-05, 2.259467146359384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.259467146359384e-05

Optimization complete. Final v2v error: 3.693439483642578 mm

Highest mean error: 11.644248962402344 mm for frame 95

Lowest mean error: 3.105759859085083 mm for frame 85

Saving results

Total time: 102.6233422756195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957942
Iteration 2/25 | Loss: 0.00144501
Iteration 3/25 | Loss: 0.00110703
Iteration 4/25 | Loss: 0.00106798
Iteration 5/25 | Loss: 0.00105707
Iteration 6/25 | Loss: 0.00105526
Iteration 7/25 | Loss: 0.00105509
Iteration 8/25 | Loss: 0.00105509
Iteration 9/25 | Loss: 0.00105509
Iteration 10/25 | Loss: 0.00105509
Iteration 11/25 | Loss: 0.00105509
Iteration 12/25 | Loss: 0.00105509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001055094413459301, 0.001055094413459301, 0.001055094413459301, 0.001055094413459301, 0.001055094413459301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001055094413459301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09739256
Iteration 2/25 | Loss: 0.00177466
Iteration 3/25 | Loss: 0.00177466
Iteration 4/25 | Loss: 0.00177466
Iteration 5/25 | Loss: 0.00177465
Iteration 6/25 | Loss: 0.00177465
Iteration 7/25 | Loss: 0.00177465
Iteration 8/25 | Loss: 0.00177465
Iteration 9/25 | Loss: 0.00177465
Iteration 10/25 | Loss: 0.00177465
Iteration 11/25 | Loss: 0.00177465
Iteration 12/25 | Loss: 0.00177465
Iteration 13/25 | Loss: 0.00177465
Iteration 14/25 | Loss: 0.00177465
Iteration 15/25 | Loss: 0.00177465
Iteration 16/25 | Loss: 0.00177465
Iteration 17/25 | Loss: 0.00177465
Iteration 18/25 | Loss: 0.00177465
Iteration 19/25 | Loss: 0.00177465
Iteration 20/25 | Loss: 0.00177465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017746532103046775, 0.0017746532103046775, 0.0017746532103046775, 0.0017746532103046775, 0.0017746532103046775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017746532103046775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177465
Iteration 2/1000 | Loss: 0.00004513
Iteration 3/1000 | Loss: 0.00003753
Iteration 4/1000 | Loss: 0.00003399
Iteration 5/1000 | Loss: 0.00003247
Iteration 6/1000 | Loss: 0.00003059
Iteration 7/1000 | Loss: 0.00002973
Iteration 8/1000 | Loss: 0.00002918
Iteration 9/1000 | Loss: 0.00002880
Iteration 10/1000 | Loss: 0.00002861
Iteration 11/1000 | Loss: 0.00002843
Iteration 12/1000 | Loss: 0.00002839
Iteration 13/1000 | Loss: 0.00002831
Iteration 14/1000 | Loss: 0.00002827
Iteration 15/1000 | Loss: 0.00002827
Iteration 16/1000 | Loss: 0.00002822
Iteration 17/1000 | Loss: 0.00002822
Iteration 18/1000 | Loss: 0.00002822
Iteration 19/1000 | Loss: 0.00002821
Iteration 20/1000 | Loss: 0.00002821
Iteration 21/1000 | Loss: 0.00002820
Iteration 22/1000 | Loss: 0.00002820
Iteration 23/1000 | Loss: 0.00002820
Iteration 24/1000 | Loss: 0.00002820
Iteration 25/1000 | Loss: 0.00002820
Iteration 26/1000 | Loss: 0.00002820
Iteration 27/1000 | Loss: 0.00002820
Iteration 28/1000 | Loss: 0.00002819
Iteration 29/1000 | Loss: 0.00002819
Iteration 30/1000 | Loss: 0.00002819
Iteration 31/1000 | Loss: 0.00002819
Iteration 32/1000 | Loss: 0.00002819
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002818
Iteration 35/1000 | Loss: 0.00002816
Iteration 36/1000 | Loss: 0.00002816
Iteration 37/1000 | Loss: 0.00002814
Iteration 38/1000 | Loss: 0.00002814
Iteration 39/1000 | Loss: 0.00002814
Iteration 40/1000 | Loss: 0.00002814
Iteration 41/1000 | Loss: 0.00002814
Iteration 42/1000 | Loss: 0.00002814
Iteration 43/1000 | Loss: 0.00002814
Iteration 44/1000 | Loss: 0.00002814
Iteration 45/1000 | Loss: 0.00002814
Iteration 46/1000 | Loss: 0.00002814
Iteration 47/1000 | Loss: 0.00002814
Iteration 48/1000 | Loss: 0.00002814
Iteration 49/1000 | Loss: 0.00002814
Iteration 50/1000 | Loss: 0.00002814
Iteration 51/1000 | Loss: 0.00002814
Iteration 52/1000 | Loss: 0.00002814
Iteration 53/1000 | Loss: 0.00002814
Iteration 54/1000 | Loss: 0.00002814
Iteration 55/1000 | Loss: 0.00002814
Iteration 56/1000 | Loss: 0.00002814
Iteration 57/1000 | Loss: 0.00002814
Iteration 58/1000 | Loss: 0.00002814
Iteration 59/1000 | Loss: 0.00002814
Iteration 60/1000 | Loss: 0.00002814
Iteration 61/1000 | Loss: 0.00002814
Iteration 62/1000 | Loss: 0.00002814
Iteration 63/1000 | Loss: 0.00002814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [2.813576975313481e-05, 2.813576975313481e-05, 2.813576975313481e-05, 2.813576975313481e-05, 2.813576975313481e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.813576975313481e-05

Optimization complete. Final v2v error: 4.4853129386901855 mm

Highest mean error: 4.714644432067871 mm for frame 8

Lowest mean error: 4.159468173980713 mm for frame 100

Saving results

Total time: 30.382349014282227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567039
Iteration 2/25 | Loss: 0.00134238
Iteration 3/25 | Loss: 0.00110224
Iteration 4/25 | Loss: 0.00106971
Iteration 5/25 | Loss: 0.00106231
Iteration 6/25 | Loss: 0.00106044
Iteration 7/25 | Loss: 0.00105997
Iteration 8/25 | Loss: 0.00105997
Iteration 9/25 | Loss: 0.00105997
Iteration 10/25 | Loss: 0.00105997
Iteration 11/25 | Loss: 0.00105997
Iteration 12/25 | Loss: 0.00105997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010599703527987003, 0.0010599703527987003, 0.0010599703527987003, 0.0010599703527987003, 0.0010599703527987003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010599703527987003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73032629
Iteration 2/25 | Loss: 0.00173280
Iteration 3/25 | Loss: 0.00173279
Iteration 4/25 | Loss: 0.00173279
Iteration 5/25 | Loss: 0.00173278
Iteration 6/25 | Loss: 0.00173278
Iteration 7/25 | Loss: 0.00173278
Iteration 8/25 | Loss: 0.00173278
Iteration 9/25 | Loss: 0.00173278
Iteration 10/25 | Loss: 0.00173278
Iteration 11/25 | Loss: 0.00173278
Iteration 12/25 | Loss: 0.00173278
Iteration 13/25 | Loss: 0.00173278
Iteration 14/25 | Loss: 0.00173278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017327836249023676, 0.0017327836249023676, 0.0017327836249023676, 0.0017327836249023676, 0.0017327836249023676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017327836249023676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173278
Iteration 2/1000 | Loss: 0.00008128
Iteration 3/1000 | Loss: 0.00006305
Iteration 4/1000 | Loss: 0.00005017
Iteration 5/1000 | Loss: 0.00004576
Iteration 6/1000 | Loss: 0.00004363
Iteration 7/1000 | Loss: 0.00004221
Iteration 8/1000 | Loss: 0.00004103
Iteration 9/1000 | Loss: 0.00004034
Iteration 10/1000 | Loss: 0.00003984
Iteration 11/1000 | Loss: 0.00003948
Iteration 12/1000 | Loss: 0.00003923
Iteration 13/1000 | Loss: 0.00003907
Iteration 14/1000 | Loss: 0.00003897
Iteration 15/1000 | Loss: 0.00003892
Iteration 16/1000 | Loss: 0.00003888
Iteration 17/1000 | Loss: 0.00003888
Iteration 18/1000 | Loss: 0.00003887
Iteration 19/1000 | Loss: 0.00003885
Iteration 20/1000 | Loss: 0.00003885
Iteration 21/1000 | Loss: 0.00003885
Iteration 22/1000 | Loss: 0.00003884
Iteration 23/1000 | Loss: 0.00003884
Iteration 24/1000 | Loss: 0.00003882
Iteration 25/1000 | Loss: 0.00003881
Iteration 26/1000 | Loss: 0.00003881
Iteration 27/1000 | Loss: 0.00003881
Iteration 28/1000 | Loss: 0.00003879
Iteration 29/1000 | Loss: 0.00003878
Iteration 30/1000 | Loss: 0.00003877
Iteration 31/1000 | Loss: 0.00003876
Iteration 32/1000 | Loss: 0.00003876
Iteration 33/1000 | Loss: 0.00003876
Iteration 34/1000 | Loss: 0.00003876
Iteration 35/1000 | Loss: 0.00003876
Iteration 36/1000 | Loss: 0.00003876
Iteration 37/1000 | Loss: 0.00003876
Iteration 38/1000 | Loss: 0.00003875
Iteration 39/1000 | Loss: 0.00003875
Iteration 40/1000 | Loss: 0.00003875
Iteration 41/1000 | Loss: 0.00003874
Iteration 42/1000 | Loss: 0.00003874
Iteration 43/1000 | Loss: 0.00003874
Iteration 44/1000 | Loss: 0.00003874
Iteration 45/1000 | Loss: 0.00003873
Iteration 46/1000 | Loss: 0.00003873
Iteration 47/1000 | Loss: 0.00003873
Iteration 48/1000 | Loss: 0.00003873
Iteration 49/1000 | Loss: 0.00003872
Iteration 50/1000 | Loss: 0.00003872
Iteration 51/1000 | Loss: 0.00003872
Iteration 52/1000 | Loss: 0.00003872
Iteration 53/1000 | Loss: 0.00003872
Iteration 54/1000 | Loss: 0.00003872
Iteration 55/1000 | Loss: 0.00003872
Iteration 56/1000 | Loss: 0.00003872
Iteration 57/1000 | Loss: 0.00003871
Iteration 58/1000 | Loss: 0.00003871
Iteration 59/1000 | Loss: 0.00003871
Iteration 60/1000 | Loss: 0.00003871
Iteration 61/1000 | Loss: 0.00003871
Iteration 62/1000 | Loss: 0.00003871
Iteration 63/1000 | Loss: 0.00003871
Iteration 64/1000 | Loss: 0.00003869
Iteration 65/1000 | Loss: 0.00003869
Iteration 66/1000 | Loss: 0.00003869
Iteration 67/1000 | Loss: 0.00003869
Iteration 68/1000 | Loss: 0.00003869
Iteration 69/1000 | Loss: 0.00003868
Iteration 70/1000 | Loss: 0.00003868
Iteration 71/1000 | Loss: 0.00003868
Iteration 72/1000 | Loss: 0.00003868
Iteration 73/1000 | Loss: 0.00003867
Iteration 74/1000 | Loss: 0.00003867
Iteration 75/1000 | Loss: 0.00003866
Iteration 76/1000 | Loss: 0.00003866
Iteration 77/1000 | Loss: 0.00003866
Iteration 78/1000 | Loss: 0.00003865
Iteration 79/1000 | Loss: 0.00003865
Iteration 80/1000 | Loss: 0.00003865
Iteration 81/1000 | Loss: 0.00003865
Iteration 82/1000 | Loss: 0.00003865
Iteration 83/1000 | Loss: 0.00003865
Iteration 84/1000 | Loss: 0.00003864
Iteration 85/1000 | Loss: 0.00003864
Iteration 86/1000 | Loss: 0.00003864
Iteration 87/1000 | Loss: 0.00003864
Iteration 88/1000 | Loss: 0.00003863
Iteration 89/1000 | Loss: 0.00003863
Iteration 90/1000 | Loss: 0.00003863
Iteration 91/1000 | Loss: 0.00003863
Iteration 92/1000 | Loss: 0.00003863
Iteration 93/1000 | Loss: 0.00003863
Iteration 94/1000 | Loss: 0.00003863
Iteration 95/1000 | Loss: 0.00003863
Iteration 96/1000 | Loss: 0.00003863
Iteration 97/1000 | Loss: 0.00003863
Iteration 98/1000 | Loss: 0.00003863
Iteration 99/1000 | Loss: 0.00003862
Iteration 100/1000 | Loss: 0.00003862
Iteration 101/1000 | Loss: 0.00003862
Iteration 102/1000 | Loss: 0.00003862
Iteration 103/1000 | Loss: 0.00003862
Iteration 104/1000 | Loss: 0.00003862
Iteration 105/1000 | Loss: 0.00003862
Iteration 106/1000 | Loss: 0.00003862
Iteration 107/1000 | Loss: 0.00003862
Iteration 108/1000 | Loss: 0.00003862
Iteration 109/1000 | Loss: 0.00003862
Iteration 110/1000 | Loss: 0.00003861
Iteration 111/1000 | Loss: 0.00003861
Iteration 112/1000 | Loss: 0.00003861
Iteration 113/1000 | Loss: 0.00003861
Iteration 114/1000 | Loss: 0.00003861
Iteration 115/1000 | Loss: 0.00003861
Iteration 116/1000 | Loss: 0.00003861
Iteration 117/1000 | Loss: 0.00003861
Iteration 118/1000 | Loss: 0.00003861
Iteration 119/1000 | Loss: 0.00003861
Iteration 120/1000 | Loss: 0.00003861
Iteration 121/1000 | Loss: 0.00003861
Iteration 122/1000 | Loss: 0.00003861
Iteration 123/1000 | Loss: 0.00003861
Iteration 124/1000 | Loss: 0.00003861
Iteration 125/1000 | Loss: 0.00003861
Iteration 126/1000 | Loss: 0.00003861
Iteration 127/1000 | Loss: 0.00003860
Iteration 128/1000 | Loss: 0.00003860
Iteration 129/1000 | Loss: 0.00003860
Iteration 130/1000 | Loss: 0.00003860
Iteration 131/1000 | Loss: 0.00003860
Iteration 132/1000 | Loss: 0.00003860
Iteration 133/1000 | Loss: 0.00003860
Iteration 134/1000 | Loss: 0.00003860
Iteration 135/1000 | Loss: 0.00003860
Iteration 136/1000 | Loss: 0.00003860
Iteration 137/1000 | Loss: 0.00003860
Iteration 138/1000 | Loss: 0.00003860
Iteration 139/1000 | Loss: 0.00003860
Iteration 140/1000 | Loss: 0.00003860
Iteration 141/1000 | Loss: 0.00003860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.859925709548406e-05, 3.859925709548406e-05, 3.859925709548406e-05, 3.859925709548406e-05, 3.859925709548406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.859925709548406e-05

Optimization complete. Final v2v error: 5.270843505859375 mm

Highest mean error: 5.638925552368164 mm for frame 107

Lowest mean error: 4.787693023681641 mm for frame 71

Saving results

Total time: 40.37267065048218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875332
Iteration 2/25 | Loss: 0.00176335
Iteration 3/25 | Loss: 0.00113092
Iteration 4/25 | Loss: 0.00102640
Iteration 5/25 | Loss: 0.00101310
Iteration 6/25 | Loss: 0.00101105
Iteration 7/25 | Loss: 0.00101067
Iteration 8/25 | Loss: 0.00101067
Iteration 9/25 | Loss: 0.00101067
Iteration 10/25 | Loss: 0.00101067
Iteration 11/25 | Loss: 0.00101067
Iteration 12/25 | Loss: 0.00101067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010106662521138787, 0.0010106662521138787, 0.0010106662521138787, 0.0010106662521138787, 0.0010106662521138787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010106662521138787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53927946
Iteration 2/25 | Loss: 0.00187114
Iteration 3/25 | Loss: 0.00187114
Iteration 4/25 | Loss: 0.00187114
Iteration 5/25 | Loss: 0.00187114
Iteration 6/25 | Loss: 0.00187114
Iteration 7/25 | Loss: 0.00187114
Iteration 8/25 | Loss: 0.00187114
Iteration 9/25 | Loss: 0.00187114
Iteration 10/25 | Loss: 0.00187114
Iteration 11/25 | Loss: 0.00187114
Iteration 12/25 | Loss: 0.00187114
Iteration 13/25 | Loss: 0.00187114
Iteration 14/25 | Loss: 0.00187114
Iteration 15/25 | Loss: 0.00187114
Iteration 16/25 | Loss: 0.00187114
Iteration 17/25 | Loss: 0.00187114
Iteration 18/25 | Loss: 0.00187114
Iteration 19/25 | Loss: 0.00187114
Iteration 20/25 | Loss: 0.00187114
Iteration 21/25 | Loss: 0.00187114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018711407901719213, 0.0018711407901719213, 0.0018711407901719213, 0.0018711407901719213, 0.0018711407901719213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018711407901719213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187114
Iteration 2/1000 | Loss: 0.00004817
Iteration 3/1000 | Loss: 0.00003508
Iteration 4/1000 | Loss: 0.00002967
Iteration 5/1000 | Loss: 0.00002717
Iteration 6/1000 | Loss: 0.00002538
Iteration 7/1000 | Loss: 0.00002461
Iteration 8/1000 | Loss: 0.00002409
Iteration 9/1000 | Loss: 0.00002361
Iteration 10/1000 | Loss: 0.00002332
Iteration 11/1000 | Loss: 0.00002308
Iteration 12/1000 | Loss: 0.00002293
Iteration 13/1000 | Loss: 0.00002290
Iteration 14/1000 | Loss: 0.00002284
Iteration 15/1000 | Loss: 0.00002283
Iteration 16/1000 | Loss: 0.00002283
Iteration 17/1000 | Loss: 0.00002279
Iteration 18/1000 | Loss: 0.00002279
Iteration 19/1000 | Loss: 0.00002276
Iteration 20/1000 | Loss: 0.00002275
Iteration 21/1000 | Loss: 0.00002275
Iteration 22/1000 | Loss: 0.00002275
Iteration 23/1000 | Loss: 0.00002272
Iteration 24/1000 | Loss: 0.00002267
Iteration 25/1000 | Loss: 0.00002267
Iteration 26/1000 | Loss: 0.00002265
Iteration 27/1000 | Loss: 0.00002263
Iteration 28/1000 | Loss: 0.00002263
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002261
Iteration 33/1000 | Loss: 0.00002261
Iteration 34/1000 | Loss: 0.00002260
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002258
Iteration 39/1000 | Loss: 0.00002257
Iteration 40/1000 | Loss: 0.00002257
Iteration 41/1000 | Loss: 0.00002256
Iteration 42/1000 | Loss: 0.00002256
Iteration 43/1000 | Loss: 0.00002256
Iteration 44/1000 | Loss: 0.00002256
Iteration 45/1000 | Loss: 0.00002256
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002256
Iteration 49/1000 | Loss: 0.00002256
Iteration 50/1000 | Loss: 0.00002256
Iteration 51/1000 | Loss: 0.00002255
Iteration 52/1000 | Loss: 0.00002255
Iteration 53/1000 | Loss: 0.00002255
Iteration 54/1000 | Loss: 0.00002255
Iteration 55/1000 | Loss: 0.00002255
Iteration 56/1000 | Loss: 0.00002255
Iteration 57/1000 | Loss: 0.00002255
Iteration 58/1000 | Loss: 0.00002255
Iteration 59/1000 | Loss: 0.00002254
Iteration 60/1000 | Loss: 0.00002254
Iteration 61/1000 | Loss: 0.00002254
Iteration 62/1000 | Loss: 0.00002254
Iteration 63/1000 | Loss: 0.00002254
Iteration 64/1000 | Loss: 0.00002254
Iteration 65/1000 | Loss: 0.00002254
Iteration 66/1000 | Loss: 0.00002254
Iteration 67/1000 | Loss: 0.00002254
Iteration 68/1000 | Loss: 0.00002254
Iteration 69/1000 | Loss: 0.00002254
Iteration 70/1000 | Loss: 0.00002254
Iteration 71/1000 | Loss: 0.00002253
Iteration 72/1000 | Loss: 0.00002253
Iteration 73/1000 | Loss: 0.00002253
Iteration 74/1000 | Loss: 0.00002253
Iteration 75/1000 | Loss: 0.00002253
Iteration 76/1000 | Loss: 0.00002253
Iteration 77/1000 | Loss: 0.00002253
Iteration 78/1000 | Loss: 0.00002253
Iteration 79/1000 | Loss: 0.00002253
Iteration 80/1000 | Loss: 0.00002253
Iteration 81/1000 | Loss: 0.00002253
Iteration 82/1000 | Loss: 0.00002253
Iteration 83/1000 | Loss: 0.00002252
Iteration 84/1000 | Loss: 0.00002252
Iteration 85/1000 | Loss: 0.00002252
Iteration 86/1000 | Loss: 0.00002252
Iteration 87/1000 | Loss: 0.00002252
Iteration 88/1000 | Loss: 0.00002252
Iteration 89/1000 | Loss: 0.00002252
Iteration 90/1000 | Loss: 0.00002252
Iteration 91/1000 | Loss: 0.00002252
Iteration 92/1000 | Loss: 0.00002252
Iteration 93/1000 | Loss: 0.00002252
Iteration 94/1000 | Loss: 0.00002252
Iteration 95/1000 | Loss: 0.00002251
Iteration 96/1000 | Loss: 0.00002251
Iteration 97/1000 | Loss: 0.00002251
Iteration 98/1000 | Loss: 0.00002251
Iteration 99/1000 | Loss: 0.00002251
Iteration 100/1000 | Loss: 0.00002251
Iteration 101/1000 | Loss: 0.00002251
Iteration 102/1000 | Loss: 0.00002250
Iteration 103/1000 | Loss: 0.00002250
Iteration 104/1000 | Loss: 0.00002250
Iteration 105/1000 | Loss: 0.00002250
Iteration 106/1000 | Loss: 0.00002250
Iteration 107/1000 | Loss: 0.00002250
Iteration 108/1000 | Loss: 0.00002250
Iteration 109/1000 | Loss: 0.00002249
Iteration 110/1000 | Loss: 0.00002249
Iteration 111/1000 | Loss: 0.00002249
Iteration 112/1000 | Loss: 0.00002249
Iteration 113/1000 | Loss: 0.00002249
Iteration 114/1000 | Loss: 0.00002249
Iteration 115/1000 | Loss: 0.00002249
Iteration 116/1000 | Loss: 0.00002249
Iteration 117/1000 | Loss: 0.00002249
Iteration 118/1000 | Loss: 0.00002249
Iteration 119/1000 | Loss: 0.00002249
Iteration 120/1000 | Loss: 0.00002249
Iteration 121/1000 | Loss: 0.00002249
Iteration 122/1000 | Loss: 0.00002249
Iteration 123/1000 | Loss: 0.00002249
Iteration 124/1000 | Loss: 0.00002249
Iteration 125/1000 | Loss: 0.00002249
Iteration 126/1000 | Loss: 0.00002249
Iteration 127/1000 | Loss: 0.00002249
Iteration 128/1000 | Loss: 0.00002249
Iteration 129/1000 | Loss: 0.00002249
Iteration 130/1000 | Loss: 0.00002249
Iteration 131/1000 | Loss: 0.00002249
Iteration 132/1000 | Loss: 0.00002249
Iteration 133/1000 | Loss: 0.00002249
Iteration 134/1000 | Loss: 0.00002249
Iteration 135/1000 | Loss: 0.00002249
Iteration 136/1000 | Loss: 0.00002249
Iteration 137/1000 | Loss: 0.00002249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.249049794045277e-05, 2.249049794045277e-05, 2.249049794045277e-05, 2.249049794045277e-05, 2.249049794045277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.249049794045277e-05

Optimization complete. Final v2v error: 4.079931735992432 mm

Highest mean error: 4.400510311126709 mm for frame 7

Lowest mean error: 3.656917095184326 mm for frame 80

Saving results

Total time: 42.50544238090515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139151
Iteration 2/25 | Loss: 0.01139151
Iteration 3/25 | Loss: 0.01139151
Iteration 4/25 | Loss: 0.01139150
Iteration 5/25 | Loss: 0.01139150
Iteration 6/25 | Loss: 0.01139150
Iteration 7/25 | Loss: 0.01139150
Iteration 8/25 | Loss: 0.01139150
Iteration 9/25 | Loss: 0.01139150
Iteration 10/25 | Loss: 0.01139150
Iteration 11/25 | Loss: 0.01139150
Iteration 12/25 | Loss: 0.01139150
Iteration 13/25 | Loss: 0.01139150
Iteration 14/25 | Loss: 0.01139150
Iteration 15/25 | Loss: 0.01139150
Iteration 16/25 | Loss: 0.01139150
Iteration 17/25 | Loss: 0.01139150
Iteration 18/25 | Loss: 0.01139150
Iteration 19/25 | Loss: 0.01139150
Iteration 20/25 | Loss: 0.01139150
Iteration 21/25 | Loss: 0.01139150
Iteration 22/25 | Loss: 0.01139150
Iteration 23/25 | Loss: 0.01139150
Iteration 24/25 | Loss: 0.01139149
Iteration 25/25 | Loss: 0.01139149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14959908
Iteration 2/25 | Loss: 0.06803357
Iteration 3/25 | Loss: 0.06774459
Iteration 4/25 | Loss: 0.06761620
Iteration 5/25 | Loss: 0.06761620
Iteration 6/25 | Loss: 0.06761620
Iteration 7/25 | Loss: 0.06761619
Iteration 8/25 | Loss: 0.06761619
Iteration 9/25 | Loss: 0.06761619
Iteration 10/25 | Loss: 0.06761620
Iteration 11/25 | Loss: 0.06761620
Iteration 12/25 | Loss: 0.06761619
Iteration 13/25 | Loss: 0.06761619
Iteration 14/25 | Loss: 0.06761619
Iteration 15/25 | Loss: 0.06761619
Iteration 16/25 | Loss: 0.06761619
Iteration 17/25 | Loss: 0.06761619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.06761619448661804, 0.06761619448661804, 0.06761619448661804, 0.06761619448661804, 0.06761619448661804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06761619448661804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06761619
Iteration 2/1000 | Loss: 0.02037669
Iteration 3/1000 | Loss: 0.00197819
Iteration 4/1000 | Loss: 0.00201729
Iteration 5/1000 | Loss: 0.00170891
Iteration 6/1000 | Loss: 0.00086145
Iteration 7/1000 | Loss: 0.00061246
Iteration 8/1000 | Loss: 0.00157683
Iteration 9/1000 | Loss: 0.00035753
Iteration 10/1000 | Loss: 0.00016068
Iteration 11/1000 | Loss: 0.00011374
Iteration 12/1000 | Loss: 0.00046039
Iteration 13/1000 | Loss: 0.00031211
Iteration 14/1000 | Loss: 0.00068792
Iteration 15/1000 | Loss: 0.00008971
Iteration 16/1000 | Loss: 0.00006678
Iteration 17/1000 | Loss: 0.00007785
Iteration 18/1000 | Loss: 0.00024011
Iteration 19/1000 | Loss: 0.00016027
Iteration 20/1000 | Loss: 0.00006910
Iteration 21/1000 | Loss: 0.00019718
Iteration 22/1000 | Loss: 0.00021170
Iteration 23/1000 | Loss: 0.00030789
Iteration 24/1000 | Loss: 0.00017376
Iteration 25/1000 | Loss: 0.00016169
Iteration 26/1000 | Loss: 0.00026661
Iteration 27/1000 | Loss: 0.00024303
Iteration 28/1000 | Loss: 0.00048611
Iteration 29/1000 | Loss: 0.00005778
Iteration 30/1000 | Loss: 0.00037135
Iteration 31/1000 | Loss: 0.00004645
Iteration 32/1000 | Loss: 0.00023606
Iteration 33/1000 | Loss: 0.00003802
Iteration 34/1000 | Loss: 0.00003628
Iteration 35/1000 | Loss: 0.00025614
Iteration 36/1000 | Loss: 0.00003497
Iteration 37/1000 | Loss: 0.00005583
Iteration 38/1000 | Loss: 0.00004603
Iteration 39/1000 | Loss: 0.00003520
Iteration 40/1000 | Loss: 0.00003159
Iteration 41/1000 | Loss: 0.00003014
Iteration 42/1000 | Loss: 0.00024432
Iteration 43/1000 | Loss: 0.00023308
Iteration 44/1000 | Loss: 0.00022663
Iteration 45/1000 | Loss: 0.00004759
Iteration 46/1000 | Loss: 0.00003432
Iteration 47/1000 | Loss: 0.00045193
Iteration 48/1000 | Loss: 0.00003451
Iteration 49/1000 | Loss: 0.00005691
Iteration 50/1000 | Loss: 0.00006320
Iteration 51/1000 | Loss: 0.00006525
Iteration 52/1000 | Loss: 0.00027429
Iteration 53/1000 | Loss: 0.00087867
Iteration 54/1000 | Loss: 0.00214030
Iteration 55/1000 | Loss: 0.00003999
Iteration 56/1000 | Loss: 0.00023253
Iteration 57/1000 | Loss: 0.00014728
Iteration 58/1000 | Loss: 0.00022248
Iteration 59/1000 | Loss: 0.00005389
Iteration 60/1000 | Loss: 0.00003571
Iteration 61/1000 | Loss: 0.00005657
Iteration 62/1000 | Loss: 0.00004296
Iteration 63/1000 | Loss: 0.00004227
Iteration 64/1000 | Loss: 0.00003053
Iteration 65/1000 | Loss: 0.00009851
Iteration 66/1000 | Loss: 0.00002878
Iteration 67/1000 | Loss: 0.00002820
Iteration 68/1000 | Loss: 0.00008615
Iteration 69/1000 | Loss: 0.00010942
Iteration 70/1000 | Loss: 0.00010193
Iteration 71/1000 | Loss: 0.00011296
Iteration 72/1000 | Loss: 0.00009176
Iteration 73/1000 | Loss: 0.00009451
Iteration 74/1000 | Loss: 0.00005312
Iteration 75/1000 | Loss: 0.00003273
Iteration 76/1000 | Loss: 0.00003941
Iteration 77/1000 | Loss: 0.00003353
Iteration 78/1000 | Loss: 0.00005187
Iteration 79/1000 | Loss: 0.00010156
Iteration 80/1000 | Loss: 0.00008863
Iteration 81/1000 | Loss: 0.00008765
Iteration 82/1000 | Loss: 0.00009194
Iteration 83/1000 | Loss: 0.00005321
Iteration 84/1000 | Loss: 0.00003630
Iteration 85/1000 | Loss: 0.00004160
Iteration 86/1000 | Loss: 0.00004991
Iteration 87/1000 | Loss: 0.00003601
Iteration 88/1000 | Loss: 0.00009844
Iteration 89/1000 | Loss: 0.00004651
Iteration 90/1000 | Loss: 0.00003761
Iteration 91/1000 | Loss: 0.00003086
Iteration 92/1000 | Loss: 0.00002943
Iteration 93/1000 | Loss: 0.00004199
Iteration 94/1000 | Loss: 0.00006973
Iteration 95/1000 | Loss: 0.00002782
Iteration 96/1000 | Loss: 0.00019999
Iteration 97/1000 | Loss: 0.00021619
Iteration 98/1000 | Loss: 0.00022065
Iteration 99/1000 | Loss: 0.00023779
Iteration 100/1000 | Loss: 0.00003639
Iteration 101/1000 | Loss: 0.00005276
Iteration 102/1000 | Loss: 0.00009157
Iteration 103/1000 | Loss: 0.00003228
Iteration 104/1000 | Loss: 0.00004824
Iteration 105/1000 | Loss: 0.00002941
Iteration 106/1000 | Loss: 0.00002886
Iteration 107/1000 | Loss: 0.00002911
Iteration 108/1000 | Loss: 0.00004944
Iteration 109/1000 | Loss: 0.00005100
Iteration 110/1000 | Loss: 0.00023308
Iteration 111/1000 | Loss: 0.00042636
Iteration 112/1000 | Loss: 0.00016364
Iteration 113/1000 | Loss: 0.00004495
Iteration 114/1000 | Loss: 0.00003385
Iteration 115/1000 | Loss: 0.00002972
Iteration 116/1000 | Loss: 0.00005111
Iteration 117/1000 | Loss: 0.00003159
Iteration 118/1000 | Loss: 0.00003038
Iteration 119/1000 | Loss: 0.00006128
Iteration 120/1000 | Loss: 0.00002881
Iteration 121/1000 | Loss: 0.00002801
Iteration 122/1000 | Loss: 0.00002786
Iteration 123/1000 | Loss: 0.00021754
Iteration 124/1000 | Loss: 0.00051673
Iteration 125/1000 | Loss: 0.00013136
Iteration 126/1000 | Loss: 0.00005485
Iteration 127/1000 | Loss: 0.00005850
Iteration 128/1000 | Loss: 0.00003212
Iteration 129/1000 | Loss: 0.00016561
Iteration 130/1000 | Loss: 0.00006496
Iteration 131/1000 | Loss: 0.00020058
Iteration 132/1000 | Loss: 0.00014825
Iteration 133/1000 | Loss: 0.00002937
Iteration 134/1000 | Loss: 0.00002813
Iteration 135/1000 | Loss: 0.00011702
Iteration 136/1000 | Loss: 0.00004228
Iteration 137/1000 | Loss: 0.00005208
Iteration 138/1000 | Loss: 0.00003732
Iteration 139/1000 | Loss: 0.00003885
Iteration 140/1000 | Loss: 0.00003101
Iteration 141/1000 | Loss: 0.00014307
Iteration 142/1000 | Loss: 0.00009745
Iteration 143/1000 | Loss: 0.00030554
Iteration 144/1000 | Loss: 0.00003898
Iteration 145/1000 | Loss: 0.00003733
Iteration 146/1000 | Loss: 0.00003456
Iteration 147/1000 | Loss: 0.00002886
Iteration 148/1000 | Loss: 0.00002787
Iteration 149/1000 | Loss: 0.00002661
Iteration 150/1000 | Loss: 0.00002565
Iteration 151/1000 | Loss: 0.00004199
Iteration 152/1000 | Loss: 0.00002612
Iteration 153/1000 | Loss: 0.00002536
Iteration 154/1000 | Loss: 0.00002520
Iteration 155/1000 | Loss: 0.00002519
Iteration 156/1000 | Loss: 0.00002519
Iteration 157/1000 | Loss: 0.00002518
Iteration 158/1000 | Loss: 0.00002518
Iteration 159/1000 | Loss: 0.00002518
Iteration 160/1000 | Loss: 0.00002518
Iteration 161/1000 | Loss: 0.00002518
Iteration 162/1000 | Loss: 0.00002518
Iteration 163/1000 | Loss: 0.00002518
Iteration 164/1000 | Loss: 0.00002518
Iteration 165/1000 | Loss: 0.00002518
Iteration 166/1000 | Loss: 0.00002518
Iteration 167/1000 | Loss: 0.00002518
Iteration 168/1000 | Loss: 0.00002518
Iteration 169/1000 | Loss: 0.00002518
Iteration 170/1000 | Loss: 0.00002518
Iteration 171/1000 | Loss: 0.00002518
Iteration 172/1000 | Loss: 0.00002518
Iteration 173/1000 | Loss: 0.00002518
Iteration 174/1000 | Loss: 0.00002518
Iteration 175/1000 | Loss: 0.00002518
Iteration 176/1000 | Loss: 0.00002517
Iteration 177/1000 | Loss: 0.00002517
Iteration 178/1000 | Loss: 0.00002532
Iteration 179/1000 | Loss: 0.00002530
Iteration 180/1000 | Loss: 0.00002529
Iteration 181/1000 | Loss: 0.00002514
Iteration 182/1000 | Loss: 0.00002513
Iteration 183/1000 | Loss: 0.00002512
Iteration 184/1000 | Loss: 0.00002510
Iteration 185/1000 | Loss: 0.00002509
Iteration 186/1000 | Loss: 0.00002509
Iteration 187/1000 | Loss: 0.00002508
Iteration 188/1000 | Loss: 0.00002559
Iteration 189/1000 | Loss: 0.00002559
Iteration 190/1000 | Loss: 0.00002601
Iteration 191/1000 | Loss: 0.00002523
Iteration 192/1000 | Loss: 0.00002499
Iteration 193/1000 | Loss: 0.00002498
Iteration 194/1000 | Loss: 0.00002498
Iteration 195/1000 | Loss: 0.00002498
Iteration 196/1000 | Loss: 0.00002498
Iteration 197/1000 | Loss: 0.00002498
Iteration 198/1000 | Loss: 0.00002498
Iteration 199/1000 | Loss: 0.00002564
Iteration 200/1000 | Loss: 0.00002517
Iteration 201/1000 | Loss: 0.00002555
Iteration 202/1000 | Loss: 0.00002590
Iteration 203/1000 | Loss: 0.00004689
Iteration 204/1000 | Loss: 0.00002547
Iteration 205/1000 | Loss: 0.00003853
Iteration 206/1000 | Loss: 0.00002538
Iteration 207/1000 | Loss: 0.00002543
Iteration 208/1000 | Loss: 0.00003906
Iteration 209/1000 | Loss: 0.00003789
Iteration 210/1000 | Loss: 0.00005348
Iteration 211/1000 | Loss: 0.00002947
Iteration 212/1000 | Loss: 0.00004972
Iteration 213/1000 | Loss: 0.00003233
Iteration 214/1000 | Loss: 0.00012529
Iteration 215/1000 | Loss: 0.00004379
Iteration 216/1000 | Loss: 0.00004829
Iteration 217/1000 | Loss: 0.00002919
Iteration 218/1000 | Loss: 0.00002658
Iteration 219/1000 | Loss: 0.00003418
Iteration 220/1000 | Loss: 0.00004901
Iteration 221/1000 | Loss: 0.00004903
Iteration 222/1000 | Loss: 0.00009558
Iteration 223/1000 | Loss: 0.00003874
Iteration 224/1000 | Loss: 0.00006843
Iteration 225/1000 | Loss: 0.00003573
Iteration 226/1000 | Loss: 0.00004869
Iteration 227/1000 | Loss: 0.00003676
Iteration 228/1000 | Loss: 0.00004910
Iteration 229/1000 | Loss: 0.00004045
Iteration 230/1000 | Loss: 0.00002323
Iteration 231/1000 | Loss: 0.00004946
Iteration 232/1000 | Loss: 0.00003986
Iteration 233/1000 | Loss: 0.00004931
Iteration 234/1000 | Loss: 0.00004277
Iteration 235/1000 | Loss: 0.00002301
Iteration 236/1000 | Loss: 0.00005224
Iteration 237/1000 | Loss: 0.00003320
Iteration 238/1000 | Loss: 0.00005139
Iteration 239/1000 | Loss: 0.00005741
Iteration 240/1000 | Loss: 0.00007018
Iteration 241/1000 | Loss: 0.00002563
Iteration 242/1000 | Loss: 0.00003706
Iteration 243/1000 | Loss: 0.00003091
Iteration 244/1000 | Loss: 0.00002353
Iteration 245/1000 | Loss: 0.00002565
Iteration 246/1000 | Loss: 0.00002339
Iteration 247/1000 | Loss: 0.00002338
Iteration 248/1000 | Loss: 0.00002338
Iteration 249/1000 | Loss: 0.00002337
Iteration 250/1000 | Loss: 0.00002337
Iteration 251/1000 | Loss: 0.00002341
Iteration 252/1000 | Loss: 0.00002340
Iteration 253/1000 | Loss: 0.00002374
Iteration 254/1000 | Loss: 0.00002361
Iteration 255/1000 | Loss: 0.00002333
Iteration 256/1000 | Loss: 0.00002333
Iteration 257/1000 | Loss: 0.00002299
Iteration 258/1000 | Loss: 0.00002297
Iteration 259/1000 | Loss: 0.00002296
Iteration 260/1000 | Loss: 0.00002296
Iteration 261/1000 | Loss: 0.00002296
Iteration 262/1000 | Loss: 0.00002295
Iteration 263/1000 | Loss: 0.00002295
Iteration 264/1000 | Loss: 0.00002293
Iteration 265/1000 | Loss: 0.00002293
Iteration 266/1000 | Loss: 0.00002293
Iteration 267/1000 | Loss: 0.00002293
Iteration 268/1000 | Loss: 0.00002293
Iteration 269/1000 | Loss: 0.00002293
Iteration 270/1000 | Loss: 0.00002292
Iteration 271/1000 | Loss: 0.00002292
Iteration 272/1000 | Loss: 0.00002292
Iteration 273/1000 | Loss: 0.00002292
Iteration 274/1000 | Loss: 0.00002292
Iteration 275/1000 | Loss: 0.00002292
Iteration 276/1000 | Loss: 0.00002292
Iteration 277/1000 | Loss: 0.00002292
Iteration 278/1000 | Loss: 0.00002291
Iteration 279/1000 | Loss: 0.00002291
Iteration 280/1000 | Loss: 0.00002291
Iteration 281/1000 | Loss: 0.00002291
Iteration 282/1000 | Loss: 0.00002291
Iteration 283/1000 | Loss: 0.00002290
Iteration 284/1000 | Loss: 0.00002290
Iteration 285/1000 | Loss: 0.00002290
Iteration 286/1000 | Loss: 0.00002290
Iteration 287/1000 | Loss: 0.00002290
Iteration 288/1000 | Loss: 0.00002290
Iteration 289/1000 | Loss: 0.00002290
Iteration 290/1000 | Loss: 0.00002290
Iteration 291/1000 | Loss: 0.00002289
Iteration 292/1000 | Loss: 0.00002289
Iteration 293/1000 | Loss: 0.00002289
Iteration 294/1000 | Loss: 0.00002289
Iteration 295/1000 | Loss: 0.00002289
Iteration 296/1000 | Loss: 0.00002289
Iteration 297/1000 | Loss: 0.00002289
Iteration 298/1000 | Loss: 0.00002289
Iteration 299/1000 | Loss: 0.00002289
Iteration 300/1000 | Loss: 0.00002289
Iteration 301/1000 | Loss: 0.00002289
Iteration 302/1000 | Loss: 0.00002328
Iteration 303/1000 | Loss: 0.00002309
Iteration 304/1000 | Loss: 0.00002362
Iteration 305/1000 | Loss: 0.00002337
Iteration 306/1000 | Loss: 0.00002363
Iteration 307/1000 | Loss: 0.00002320
Iteration 308/1000 | Loss: 0.00002285
Iteration 309/1000 | Loss: 0.00002297
Iteration 310/1000 | Loss: 0.00002359
Iteration 311/1000 | Loss: 0.00002329
Iteration 312/1000 | Loss: 0.00002284
Iteration 313/1000 | Loss: 0.00002367
Iteration 314/1000 | Loss: 0.00002321
Iteration 315/1000 | Loss: 0.00002319
Iteration 316/1000 | Loss: 0.00002319
Iteration 317/1000 | Loss: 0.00005268
Iteration 318/1000 | Loss: 0.00002315
Iteration 319/1000 | Loss: 0.00002392
Iteration 320/1000 | Loss: 0.00002392
Iteration 321/1000 | Loss: 0.00002335
Iteration 322/1000 | Loss: 0.00002361
Iteration 323/1000 | Loss: 0.00002965
Iteration 324/1000 | Loss: 0.00002706
Iteration 325/1000 | Loss: 0.00002368
Iteration 326/1000 | Loss: 0.00003190
Iteration 327/1000 | Loss: 0.00002448
Iteration 328/1000 | Loss: 0.00002448
Iteration 329/1000 | Loss: 0.00002354
Iteration 330/1000 | Loss: 0.00002729
Iteration 331/1000 | Loss: 0.00002295
Iteration 332/1000 | Loss: 0.00002293
Iteration 333/1000 | Loss: 0.00002292
Iteration 334/1000 | Loss: 0.00002292
Iteration 335/1000 | Loss: 0.00002292
Iteration 336/1000 | Loss: 0.00002292
Iteration 337/1000 | Loss: 0.00002292
Iteration 338/1000 | Loss: 0.00002360
Iteration 339/1000 | Loss: 0.00002861
Iteration 340/1000 | Loss: 0.00002418
Iteration 341/1000 | Loss: 0.00002898
Iteration 342/1000 | Loss: 0.00002375
Iteration 343/1000 | Loss: 0.00002345
Iteration 344/1000 | Loss: 0.00002733
Iteration 345/1000 | Loss: 0.00002344
Iteration 346/1000 | Loss: 0.00002406
Iteration 347/1000 | Loss: 0.00002373
Iteration 348/1000 | Loss: 0.00002491
Iteration 349/1000 | Loss: 0.00005574
Iteration 350/1000 | Loss: 0.00002681
Iteration 351/1000 | Loss: 0.00004080
Iteration 352/1000 | Loss: 0.00002457
Iteration 353/1000 | Loss: 0.00002396
Iteration 354/1000 | Loss: 0.00002410
Iteration 355/1000 | Loss: 0.00002389
Iteration 356/1000 | Loss: 0.00002389
Iteration 357/1000 | Loss: 0.00005167
Iteration 358/1000 | Loss: 0.00002545
Iteration 359/1000 | Loss: 0.00002542
Iteration 360/1000 | Loss: 0.00002570
Iteration 361/1000 | Loss: 0.00002392
Iteration 362/1000 | Loss: 0.00002287
Iteration 363/1000 | Loss: 0.00002345
Iteration 364/1000 | Loss: 0.00002345
Iteration 365/1000 | Loss: 0.00002324
Iteration 366/1000 | Loss: 0.00002281
Iteration 367/1000 | Loss: 0.00002280
Iteration 368/1000 | Loss: 0.00002280
Iteration 369/1000 | Loss: 0.00002280
Iteration 370/1000 | Loss: 0.00002288
Iteration 371/1000 | Loss: 0.00002288
Iteration 372/1000 | Loss: 0.00002288
Iteration 373/1000 | Loss: 0.00002287
Iteration 374/1000 | Loss: 0.00002287
Iteration 375/1000 | Loss: 0.00002286
Iteration 376/1000 | Loss: 0.00002321
Iteration 377/1000 | Loss: 0.00004337
Iteration 378/1000 | Loss: 0.00002825
Iteration 379/1000 | Loss: 0.00003341
Iteration 380/1000 | Loss: 0.00002396
Iteration 381/1000 | Loss: 0.00002892
Iteration 382/1000 | Loss: 0.00002331
Iteration 383/1000 | Loss: 0.00002436
Iteration 384/1000 | Loss: 0.00002600
Iteration 385/1000 | Loss: 0.00002384
Iteration 386/1000 | Loss: 0.00003219
Iteration 387/1000 | Loss: 0.00002470
Iteration 388/1000 | Loss: 0.00002331
Iteration 389/1000 | Loss: 0.00002583
Iteration 390/1000 | Loss: 0.00002383
Iteration 391/1000 | Loss: 0.00002434
Iteration 392/1000 | Loss: 0.00002454
Iteration 393/1000 | Loss: 0.00002386
Iteration 394/1000 | Loss: 0.00002512
Iteration 395/1000 | Loss: 0.00002354
Iteration 396/1000 | Loss: 0.00002343
Iteration 397/1000 | Loss: 0.00002755
Iteration 398/1000 | Loss: 0.00002378
Iteration 399/1000 | Loss: 0.00002535
Iteration 400/1000 | Loss: 0.00002395
Iteration 401/1000 | Loss: 0.00002634
Iteration 402/1000 | Loss: 0.00002368
Iteration 403/1000 | Loss: 0.00002342
Iteration 404/1000 | Loss: 0.00002334
Iteration 405/1000 | Loss: 0.00003013
Iteration 406/1000 | Loss: 0.00002441
Iteration 407/1000 | Loss: 0.00002678
Iteration 408/1000 | Loss: 0.00002791
Iteration 409/1000 | Loss: 0.00002563
Iteration 410/1000 | Loss: 0.00002399
Iteration 411/1000 | Loss: 0.00002677
Iteration 412/1000 | Loss: 0.00002851
Iteration 413/1000 | Loss: 0.00002418
Iteration 414/1000 | Loss: 0.00002284
Iteration 415/1000 | Loss: 0.00002344
Iteration 416/1000 | Loss: 0.00002354
Iteration 417/1000 | Loss: 0.00002286
Iteration 418/1000 | Loss: 0.00002286
Iteration 419/1000 | Loss: 0.00002286
Iteration 420/1000 | Loss: 0.00002286
Iteration 421/1000 | Loss: 0.00002286
Iteration 422/1000 | Loss: 0.00002285
Iteration 423/1000 | Loss: 0.00002285
Iteration 424/1000 | Loss: 0.00002279
Iteration 425/1000 | Loss: 0.00002279
Iteration 426/1000 | Loss: 0.00002279
Iteration 427/1000 | Loss: 0.00002279
Iteration 428/1000 | Loss: 0.00002279
Iteration 429/1000 | Loss: 0.00002278
Iteration 430/1000 | Loss: 0.00002278
Iteration 431/1000 | Loss: 0.00002278
Iteration 432/1000 | Loss: 0.00002277
Iteration 433/1000 | Loss: 0.00002285
Iteration 434/1000 | Loss: 0.00002285
Iteration 435/1000 | Loss: 0.00002285
Iteration 436/1000 | Loss: 0.00002285
Iteration 437/1000 | Loss: 0.00002285
Iteration 438/1000 | Loss: 0.00002285
Iteration 439/1000 | Loss: 0.00002285
Iteration 440/1000 | Loss: 0.00002285
Iteration 441/1000 | Loss: 0.00002285
Iteration 442/1000 | Loss: 0.00002285
Iteration 443/1000 | Loss: 0.00002285
Iteration 444/1000 | Loss: 0.00002285
Iteration 445/1000 | Loss: 0.00002285
Iteration 446/1000 | Loss: 0.00002285
Iteration 447/1000 | Loss: 0.00002285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 447. Stopping optimization.
Last 5 losses: [2.2845870262244716e-05, 2.2845870262244716e-05, 2.2845870262244716e-05, 2.2845870262244716e-05, 2.2845870262244716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2845870262244716e-05

Optimization complete. Final v2v error: 3.8124430179595947 mm

Highest mean error: 12.728434562683105 mm for frame 150

Lowest mean error: 3.332364559173584 mm for frame 5

Saving results

Total time: 480.74381375312805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063109
Iteration 2/25 | Loss: 0.01063109
Iteration 3/25 | Loss: 0.01063109
Iteration 4/25 | Loss: 0.01063109
Iteration 5/25 | Loss: 0.01063108
Iteration 6/25 | Loss: 0.00287508
Iteration 7/25 | Loss: 0.00208181
Iteration 8/25 | Loss: 0.00189374
Iteration 9/25 | Loss: 0.00172720
Iteration 10/25 | Loss: 0.00178535
Iteration 11/25 | Loss: 0.00175334
Iteration 12/25 | Loss: 0.00167945
Iteration 13/25 | Loss: 0.00164467
Iteration 14/25 | Loss: 0.00158942
Iteration 15/25 | Loss: 0.00156639
Iteration 16/25 | Loss: 0.00155771
Iteration 17/25 | Loss: 0.00153817
Iteration 18/25 | Loss: 0.00148698
Iteration 19/25 | Loss: 0.00146058
Iteration 20/25 | Loss: 0.00144639
Iteration 21/25 | Loss: 0.00144553
Iteration 22/25 | Loss: 0.00143440
Iteration 23/25 | Loss: 0.00141421
Iteration 24/25 | Loss: 0.00141501
Iteration 25/25 | Loss: 0.00141065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70751929
Iteration 2/25 | Loss: 0.00642027
Iteration 3/25 | Loss: 0.00696476
Iteration 4/25 | Loss: 0.00501113
Iteration 5/25 | Loss: 0.00472487
Iteration 6/25 | Loss: 0.00472487
Iteration 7/25 | Loss: 0.00472487
Iteration 8/25 | Loss: 0.00472487
Iteration 9/25 | Loss: 0.00472487
Iteration 10/25 | Loss: 0.00472487
Iteration 11/25 | Loss: 0.00472487
Iteration 12/25 | Loss: 0.00472487
Iteration 13/25 | Loss: 0.00472487
Iteration 14/25 | Loss: 0.00472487
Iteration 15/25 | Loss: 0.00472487
Iteration 16/25 | Loss: 0.00472487
Iteration 17/25 | Loss: 0.00472487
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004724869970232248, 0.004724869970232248, 0.004724869970232248, 0.004724869970232248, 0.004724869970232248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004724869970232248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00472487
Iteration 2/1000 | Loss: 0.00281364
Iteration 3/1000 | Loss: 0.00150465
Iteration 4/1000 | Loss: 0.00388487
Iteration 5/1000 | Loss: 0.00138103
Iteration 6/1000 | Loss: 0.00116007
Iteration 7/1000 | Loss: 0.00128873
Iteration 8/1000 | Loss: 0.00098256
Iteration 9/1000 | Loss: 0.00326202
Iteration 10/1000 | Loss: 0.00291817
Iteration 11/1000 | Loss: 0.00188023
Iteration 12/1000 | Loss: 0.00179668
Iteration 13/1000 | Loss: 0.00318235
Iteration 14/1000 | Loss: 0.00057865
Iteration 15/1000 | Loss: 0.00125695
Iteration 16/1000 | Loss: 0.00099495
Iteration 17/1000 | Loss: 0.00162097
Iteration 18/1000 | Loss: 0.00126892
Iteration 19/1000 | Loss: 0.00080063
Iteration 20/1000 | Loss: 0.00129776
Iteration 21/1000 | Loss: 0.00110525
Iteration 22/1000 | Loss: 0.00092436
Iteration 23/1000 | Loss: 0.00103308
Iteration 24/1000 | Loss: 0.00178192
Iteration 25/1000 | Loss: 0.00092067
Iteration 26/1000 | Loss: 0.00058848
Iteration 27/1000 | Loss: 0.00080880
Iteration 28/1000 | Loss: 0.00119566
Iteration 29/1000 | Loss: 0.00116707
Iteration 30/1000 | Loss: 0.00101785
Iteration 31/1000 | Loss: 0.00088910
Iteration 32/1000 | Loss: 0.00065517
Iteration 33/1000 | Loss: 0.00078900
Iteration 34/1000 | Loss: 0.00065983
Iteration 35/1000 | Loss: 0.00079592
Iteration 36/1000 | Loss: 0.00084067
Iteration 37/1000 | Loss: 0.00060401
Iteration 38/1000 | Loss: 0.00122827
Iteration 39/1000 | Loss: 0.00109890
Iteration 40/1000 | Loss: 0.00049071
Iteration 41/1000 | Loss: 0.00065987
Iteration 42/1000 | Loss: 0.00043336
Iteration 43/1000 | Loss: 0.00042943
Iteration 44/1000 | Loss: 0.00118775
Iteration 45/1000 | Loss: 0.00139938
Iteration 46/1000 | Loss: 0.00084915
Iteration 47/1000 | Loss: 0.00094637
Iteration 48/1000 | Loss: 0.00140822
Iteration 49/1000 | Loss: 0.00199966
Iteration 50/1000 | Loss: 0.00080357
Iteration 51/1000 | Loss: 0.00119348
Iteration 52/1000 | Loss: 0.00074340
Iteration 53/1000 | Loss: 0.00030912
Iteration 54/1000 | Loss: 0.00042820
Iteration 55/1000 | Loss: 0.00041975
Iteration 56/1000 | Loss: 0.00073259
Iteration 57/1000 | Loss: 0.00093283
Iteration 58/1000 | Loss: 0.00067351
Iteration 59/1000 | Loss: 0.00042163
Iteration 60/1000 | Loss: 0.00096676
Iteration 61/1000 | Loss: 0.00050640
Iteration 62/1000 | Loss: 0.00027336
Iteration 63/1000 | Loss: 0.00049707
Iteration 64/1000 | Loss: 0.00030628
Iteration 65/1000 | Loss: 0.00097057
Iteration 66/1000 | Loss: 0.00255235
Iteration 67/1000 | Loss: 0.00242773
Iteration 68/1000 | Loss: 0.00275531
Iteration 69/1000 | Loss: 0.00167553
Iteration 70/1000 | Loss: 0.00190520
Iteration 71/1000 | Loss: 0.00158459
Iteration 72/1000 | Loss: 0.00031557
Iteration 73/1000 | Loss: 0.00044126
Iteration 74/1000 | Loss: 0.00068358
Iteration 75/1000 | Loss: 0.00034687
Iteration 76/1000 | Loss: 0.00090113
Iteration 77/1000 | Loss: 0.00040045
Iteration 78/1000 | Loss: 0.00049995
Iteration 79/1000 | Loss: 0.00036895
Iteration 80/1000 | Loss: 0.00076172
Iteration 81/1000 | Loss: 0.00154957
Iteration 82/1000 | Loss: 0.00091339
Iteration 83/1000 | Loss: 0.00067826
Iteration 84/1000 | Loss: 0.00053333
Iteration 85/1000 | Loss: 0.00053627
Iteration 86/1000 | Loss: 0.00068896
Iteration 87/1000 | Loss: 0.00053124
Iteration 88/1000 | Loss: 0.00051646
Iteration 89/1000 | Loss: 0.00037809
Iteration 90/1000 | Loss: 0.00043449
Iteration 91/1000 | Loss: 0.00035154
Iteration 92/1000 | Loss: 0.00047469
Iteration 93/1000 | Loss: 0.00041342
Iteration 94/1000 | Loss: 0.00048566
Iteration 95/1000 | Loss: 0.00037775
Iteration 96/1000 | Loss: 0.00068861
Iteration 97/1000 | Loss: 0.00036385
Iteration 98/1000 | Loss: 0.00024719
Iteration 99/1000 | Loss: 0.00029445
Iteration 100/1000 | Loss: 0.00030118
Iteration 101/1000 | Loss: 0.00030667
Iteration 102/1000 | Loss: 0.00031942
Iteration 103/1000 | Loss: 0.00035986
Iteration 104/1000 | Loss: 0.00080290
Iteration 105/1000 | Loss: 0.00035253
Iteration 106/1000 | Loss: 0.00031703
Iteration 107/1000 | Loss: 0.00032381
Iteration 108/1000 | Loss: 0.00036789
Iteration 109/1000 | Loss: 0.00032877
Iteration 110/1000 | Loss: 0.00030691
Iteration 111/1000 | Loss: 0.00068299
Iteration 112/1000 | Loss: 0.00065743
Iteration 113/1000 | Loss: 0.00069288
Iteration 114/1000 | Loss: 0.00062569
Iteration 115/1000 | Loss: 0.00034862
Iteration 116/1000 | Loss: 0.00061411
Iteration 117/1000 | Loss: 0.00061398
Iteration 118/1000 | Loss: 0.00058002
Iteration 119/1000 | Loss: 0.00073195
Iteration 120/1000 | Loss: 0.00037031
Iteration 121/1000 | Loss: 0.00026237
Iteration 122/1000 | Loss: 0.00023127
Iteration 123/1000 | Loss: 0.00039782
Iteration 124/1000 | Loss: 0.00037136
Iteration 125/1000 | Loss: 0.00038513
Iteration 126/1000 | Loss: 0.00073798
Iteration 127/1000 | Loss: 0.00039138
Iteration 128/1000 | Loss: 0.00024640
Iteration 129/1000 | Loss: 0.00023858
Iteration 130/1000 | Loss: 0.00071674
Iteration 131/1000 | Loss: 0.00090129
Iteration 132/1000 | Loss: 0.00029691
Iteration 133/1000 | Loss: 0.00050684
Iteration 134/1000 | Loss: 0.00031674
Iteration 135/1000 | Loss: 0.00042201
Iteration 136/1000 | Loss: 0.00031288
Iteration 137/1000 | Loss: 0.00038491
Iteration 138/1000 | Loss: 0.00044358
Iteration 139/1000 | Loss: 0.00043186
Iteration 140/1000 | Loss: 0.00041376
Iteration 141/1000 | Loss: 0.00060166
Iteration 142/1000 | Loss: 0.00045092
Iteration 143/1000 | Loss: 0.00045743
Iteration 144/1000 | Loss: 0.00033374
Iteration 145/1000 | Loss: 0.00024390
Iteration 146/1000 | Loss: 0.00027129
Iteration 147/1000 | Loss: 0.00042893
Iteration 148/1000 | Loss: 0.00150270
Iteration 149/1000 | Loss: 0.00057409
Iteration 150/1000 | Loss: 0.00077495
Iteration 151/1000 | Loss: 0.00091934
Iteration 152/1000 | Loss: 0.00037101
Iteration 153/1000 | Loss: 0.00035202
Iteration 154/1000 | Loss: 0.00037401
Iteration 155/1000 | Loss: 0.00097009
Iteration 156/1000 | Loss: 0.00050698
Iteration 157/1000 | Loss: 0.00033560
Iteration 158/1000 | Loss: 0.00025162
Iteration 159/1000 | Loss: 0.00024279
Iteration 160/1000 | Loss: 0.00036542
Iteration 161/1000 | Loss: 0.00036739
Iteration 162/1000 | Loss: 0.00031075
Iteration 163/1000 | Loss: 0.00044026
Iteration 164/1000 | Loss: 0.00049415
Iteration 165/1000 | Loss: 0.00039258
Iteration 166/1000 | Loss: 0.00055867
Iteration 167/1000 | Loss: 0.00056538
Iteration 168/1000 | Loss: 0.00031888
Iteration 169/1000 | Loss: 0.00039723
Iteration 170/1000 | Loss: 0.00056427
Iteration 171/1000 | Loss: 0.00030323
Iteration 172/1000 | Loss: 0.00024666
Iteration 173/1000 | Loss: 0.00041706
Iteration 174/1000 | Loss: 0.00044015
Iteration 175/1000 | Loss: 0.00041197
Iteration 176/1000 | Loss: 0.00044863
Iteration 177/1000 | Loss: 0.00040464
Iteration 178/1000 | Loss: 0.00038949
Iteration 179/1000 | Loss: 0.00038278
Iteration 180/1000 | Loss: 0.00042799
Iteration 181/1000 | Loss: 0.00034012
Iteration 182/1000 | Loss: 0.00054315
Iteration 183/1000 | Loss: 0.00032079
Iteration 184/1000 | Loss: 0.00036314
Iteration 185/1000 | Loss: 0.00036108
Iteration 186/1000 | Loss: 0.00045238
Iteration 187/1000 | Loss: 0.00057390
Iteration 188/1000 | Loss: 0.00041562
Iteration 189/1000 | Loss: 0.00043466
Iteration 190/1000 | Loss: 0.00041922
Iteration 191/1000 | Loss: 0.00044669
Iteration 192/1000 | Loss: 0.00073313
Iteration 193/1000 | Loss: 0.00077680
Iteration 194/1000 | Loss: 0.00166529
Iteration 195/1000 | Loss: 0.00051510
Iteration 196/1000 | Loss: 0.00024607
Iteration 197/1000 | Loss: 0.00037778
Iteration 198/1000 | Loss: 0.00023353
Iteration 199/1000 | Loss: 0.00042805
Iteration 200/1000 | Loss: 0.00068220
Iteration 201/1000 | Loss: 0.00023157
Iteration 202/1000 | Loss: 0.00040670
Iteration 203/1000 | Loss: 0.00024468
Iteration 204/1000 | Loss: 0.00022472
Iteration 205/1000 | Loss: 0.00023283
Iteration 206/1000 | Loss: 0.00023552
Iteration 207/1000 | Loss: 0.00023892
Iteration 208/1000 | Loss: 0.00049003
Iteration 209/1000 | Loss: 0.00037207
Iteration 210/1000 | Loss: 0.00037730
Iteration 211/1000 | Loss: 0.00032355
Iteration 212/1000 | Loss: 0.00035032
Iteration 213/1000 | Loss: 0.00024238
Iteration 214/1000 | Loss: 0.00036746
Iteration 215/1000 | Loss: 0.00039199
Iteration 216/1000 | Loss: 0.00032439
Iteration 217/1000 | Loss: 0.00030768
Iteration 218/1000 | Loss: 0.00034231
Iteration 219/1000 | Loss: 0.00042084
Iteration 220/1000 | Loss: 0.00026408
Iteration 221/1000 | Loss: 0.00039748
Iteration 222/1000 | Loss: 0.00032283
Iteration 223/1000 | Loss: 0.00033152
Iteration 224/1000 | Loss: 0.00032473
Iteration 225/1000 | Loss: 0.00033738
Iteration 226/1000 | Loss: 0.00032492
Iteration 227/1000 | Loss: 0.00034117
Iteration 228/1000 | Loss: 0.00032619
Iteration 229/1000 | Loss: 0.00034037
Iteration 230/1000 | Loss: 0.00045787
Iteration 231/1000 | Loss: 0.00036097
Iteration 232/1000 | Loss: 0.00036950
Iteration 233/1000 | Loss: 0.00035234
Iteration 234/1000 | Loss: 0.00069501
Iteration 235/1000 | Loss: 0.00038135
Iteration 236/1000 | Loss: 0.00035820
Iteration 237/1000 | Loss: 0.00034947
Iteration 238/1000 | Loss: 0.00031295
Iteration 239/1000 | Loss: 0.00029314
Iteration 240/1000 | Loss: 0.00028891
Iteration 241/1000 | Loss: 0.00022768
Iteration 242/1000 | Loss: 0.00022819
Iteration 243/1000 | Loss: 0.00022730
Iteration 244/1000 | Loss: 0.00026912
Iteration 245/1000 | Loss: 0.00025024
Iteration 246/1000 | Loss: 0.00026899
Iteration 247/1000 | Loss: 0.00023650
Iteration 248/1000 | Loss: 0.00045294
Iteration 249/1000 | Loss: 0.00030572
Iteration 250/1000 | Loss: 0.00028742
Iteration 251/1000 | Loss: 0.00028084
Iteration 252/1000 | Loss: 0.00025815
Iteration 253/1000 | Loss: 0.00036734
Iteration 254/1000 | Loss: 0.00026105
Iteration 255/1000 | Loss: 0.00025365
Iteration 256/1000 | Loss: 0.00025794
Iteration 257/1000 | Loss: 0.00028817
Iteration 258/1000 | Loss: 0.00024629
Iteration 259/1000 | Loss: 0.00031830
Iteration 260/1000 | Loss: 0.00029133
Iteration 261/1000 | Loss: 0.00039336
Iteration 262/1000 | Loss: 0.00031104
Iteration 263/1000 | Loss: 0.00031717
Iteration 264/1000 | Loss: 0.00030209
Iteration 265/1000 | Loss: 0.00030724
Iteration 266/1000 | Loss: 0.00031957
Iteration 267/1000 | Loss: 0.00046766
Iteration 268/1000 | Loss: 0.00033589
Iteration 269/1000 | Loss: 0.00035784
Iteration 270/1000 | Loss: 0.00023624
Iteration 271/1000 | Loss: 0.00022292
Iteration 272/1000 | Loss: 0.00029492
Iteration 273/1000 | Loss: 0.00022913
Iteration 274/1000 | Loss: 0.00023307
Iteration 275/1000 | Loss: 0.00022118
Iteration 276/1000 | Loss: 0.00035354
Iteration 277/1000 | Loss: 0.00022916
Iteration 278/1000 | Loss: 0.00022010
Iteration 279/1000 | Loss: 0.00031154
Iteration 280/1000 | Loss: 0.00048615
Iteration 281/1000 | Loss: 0.00064426
Iteration 282/1000 | Loss: 0.00026914
Iteration 283/1000 | Loss: 0.00034957
Iteration 284/1000 | Loss: 0.00023889
Iteration 285/1000 | Loss: 0.00022597
Iteration 286/1000 | Loss: 0.00024782
Iteration 287/1000 | Loss: 0.00022576
Iteration 288/1000 | Loss: 0.00026838
Iteration 289/1000 | Loss: 0.00027425
Iteration 290/1000 | Loss: 0.00046957
Iteration 291/1000 | Loss: 0.00021392
Iteration 292/1000 | Loss: 0.00024574
Iteration 293/1000 | Loss: 0.00022456
Iteration 294/1000 | Loss: 0.00025118
Iteration 295/1000 | Loss: 0.00022737
Iteration 296/1000 | Loss: 0.00023517
Iteration 297/1000 | Loss: 0.00021060
Iteration 298/1000 | Loss: 0.00045837
Iteration 299/1000 | Loss: 0.00028143
Iteration 300/1000 | Loss: 0.00021247
Iteration 301/1000 | Loss: 0.00031187
Iteration 302/1000 | Loss: 0.00021082
Iteration 303/1000 | Loss: 0.00023428
Iteration 304/1000 | Loss: 0.00020988
Iteration 305/1000 | Loss: 0.00019596
Iteration 306/1000 | Loss: 0.00027485
Iteration 307/1000 | Loss: 0.00023562
Iteration 308/1000 | Loss: 0.00022231
Iteration 309/1000 | Loss: 0.00020826
Iteration 310/1000 | Loss: 0.00019892
Iteration 311/1000 | Loss: 0.00020024
Iteration 312/1000 | Loss: 0.00019781
Iteration 313/1000 | Loss: 0.00019976
Iteration 314/1000 | Loss: 0.00020174
Iteration 315/1000 | Loss: 0.00019816
Iteration 316/1000 | Loss: 0.00020103
Iteration 317/1000 | Loss: 0.00019955
Iteration 318/1000 | Loss: 0.00020346
Iteration 319/1000 | Loss: 0.00021866
Iteration 320/1000 | Loss: 0.00020301
Iteration 321/1000 | Loss: 0.00020126
Iteration 322/1000 | Loss: 0.00019904
Iteration 323/1000 | Loss: 0.00019570
Iteration 324/1000 | Loss: 0.00019670
Iteration 325/1000 | Loss: 0.00019372
Iteration 326/1000 | Loss: 0.00019971
Iteration 327/1000 | Loss: 0.00020359
Iteration 328/1000 | Loss: 0.00054751
Iteration 329/1000 | Loss: 0.00021226
Iteration 330/1000 | Loss: 0.00025747
Iteration 331/1000 | Loss: 0.00021376
Iteration 332/1000 | Loss: 0.00020341
Iteration 333/1000 | Loss: 0.00019865
Iteration 334/1000 | Loss: 0.00051029
Iteration 335/1000 | Loss: 0.00021138
Iteration 336/1000 | Loss: 0.00018999
Iteration 337/1000 | Loss: 0.00018905
Iteration 338/1000 | Loss: 0.00018878
Iteration 339/1000 | Loss: 0.00018864
Iteration 340/1000 | Loss: 0.00018848
Iteration 341/1000 | Loss: 0.00018846
Iteration 342/1000 | Loss: 0.00018831
Iteration 343/1000 | Loss: 0.00018820
Iteration 344/1000 | Loss: 0.00018820
Iteration 345/1000 | Loss: 0.00018819
Iteration 346/1000 | Loss: 0.00018819
Iteration 347/1000 | Loss: 0.00018818
Iteration 348/1000 | Loss: 0.00018818
Iteration 349/1000 | Loss: 0.00018816
Iteration 350/1000 | Loss: 0.00018816
Iteration 351/1000 | Loss: 0.00018816
Iteration 352/1000 | Loss: 0.00018816
Iteration 353/1000 | Loss: 0.00018816
Iteration 354/1000 | Loss: 0.00018816
Iteration 355/1000 | Loss: 0.00018816
Iteration 356/1000 | Loss: 0.00018816
Iteration 357/1000 | Loss: 0.00018816
Iteration 358/1000 | Loss: 0.00018816
Iteration 359/1000 | Loss: 0.00018815
Iteration 360/1000 | Loss: 0.00018815
Iteration 361/1000 | Loss: 0.00018815
Iteration 362/1000 | Loss: 0.00018815
Iteration 363/1000 | Loss: 0.00018815
Iteration 364/1000 | Loss: 0.00018814
Iteration 365/1000 | Loss: 0.00018814
Iteration 366/1000 | Loss: 0.00018814
Iteration 367/1000 | Loss: 0.00018814
Iteration 368/1000 | Loss: 0.00018813
Iteration 369/1000 | Loss: 0.00018813
Iteration 370/1000 | Loss: 0.00018813
Iteration 371/1000 | Loss: 0.00018813
Iteration 372/1000 | Loss: 0.00018813
Iteration 373/1000 | Loss: 0.00018812
Iteration 374/1000 | Loss: 0.00018812
Iteration 375/1000 | Loss: 0.00018812
Iteration 376/1000 | Loss: 0.00018811
Iteration 377/1000 | Loss: 0.00018811
Iteration 378/1000 | Loss: 0.00018811
Iteration 379/1000 | Loss: 0.00018811
Iteration 380/1000 | Loss: 0.00018811
Iteration 381/1000 | Loss: 0.00018811
Iteration 382/1000 | Loss: 0.00018811
Iteration 383/1000 | Loss: 0.00018811
Iteration 384/1000 | Loss: 0.00018811
Iteration 385/1000 | Loss: 0.00018811
Iteration 386/1000 | Loss: 0.00018810
Iteration 387/1000 | Loss: 0.00018810
Iteration 388/1000 | Loss: 0.00018810
Iteration 389/1000 | Loss: 0.00018810
Iteration 390/1000 | Loss: 0.00018810
Iteration 391/1000 | Loss: 0.00018810
Iteration 392/1000 | Loss: 0.00018810
Iteration 393/1000 | Loss: 0.00018810
Iteration 394/1000 | Loss: 0.00018809
Iteration 395/1000 | Loss: 0.00018809
Iteration 396/1000 | Loss: 0.00018809
Iteration 397/1000 | Loss: 0.00018808
Iteration 398/1000 | Loss: 0.00018808
Iteration 399/1000 | Loss: 0.00018807
Iteration 400/1000 | Loss: 0.00018807
Iteration 401/1000 | Loss: 0.00019844
Iteration 402/1000 | Loss: 0.00054057
Iteration 403/1000 | Loss: 0.00023820
Iteration 404/1000 | Loss: 0.00020120
Iteration 405/1000 | Loss: 0.00019320
Iteration 406/1000 | Loss: 0.00019060
Iteration 407/1000 | Loss: 0.00019523
Iteration 408/1000 | Loss: 0.00019246
Iteration 409/1000 | Loss: 0.00019297
Iteration 410/1000 | Loss: 0.00020390
Iteration 411/1000 | Loss: 0.00019264
Iteration 412/1000 | Loss: 0.00019106
Iteration 413/1000 | Loss: 0.00019045
Iteration 414/1000 | Loss: 0.00020202
Iteration 415/1000 | Loss: 0.00019258
Iteration 416/1000 | Loss: 0.00019182
Iteration 417/1000 | Loss: 0.00019131
Iteration 418/1000 | Loss: 0.00019079
Iteration 419/1000 | Loss: 0.00019047
Iteration 420/1000 | Loss: 0.00019014
Iteration 421/1000 | Loss: 0.00018970
Iteration 422/1000 | Loss: 0.00018919
Iteration 423/1000 | Loss: 0.00018866
Iteration 424/1000 | Loss: 0.00018838
Iteration 425/1000 | Loss: 0.00018811
Iteration 426/1000 | Loss: 0.00018803
Iteration 427/1000 | Loss: 0.00018800
Iteration 428/1000 | Loss: 0.00018800
Iteration 429/1000 | Loss: 0.00018799
Iteration 430/1000 | Loss: 0.00018799
Iteration 431/1000 | Loss: 0.00018799
Iteration 432/1000 | Loss: 0.00018799
Iteration 433/1000 | Loss: 0.00018799
Iteration 434/1000 | Loss: 0.00018799
Iteration 435/1000 | Loss: 0.00018798
Iteration 436/1000 | Loss: 0.00018798
Iteration 437/1000 | Loss: 0.00018798
Iteration 438/1000 | Loss: 0.00018798
Iteration 439/1000 | Loss: 0.00018798
Iteration 440/1000 | Loss: 0.00018798
Iteration 441/1000 | Loss: 0.00018798
Iteration 442/1000 | Loss: 0.00018797
Iteration 443/1000 | Loss: 0.00018797
Iteration 444/1000 | Loss: 0.00018797
Iteration 445/1000 | Loss: 0.00018797
Iteration 446/1000 | Loss: 0.00018797
Iteration 447/1000 | Loss: 0.00018797
Iteration 448/1000 | Loss: 0.00018797
Iteration 449/1000 | Loss: 0.00018796
Iteration 450/1000 | Loss: 0.00018796
Iteration 451/1000 | Loss: 0.00018796
Iteration 452/1000 | Loss: 0.00018796
Iteration 453/1000 | Loss: 0.00018796
Iteration 454/1000 | Loss: 0.00018796
Iteration 455/1000 | Loss: 0.00018796
Iteration 456/1000 | Loss: 0.00018796
Iteration 457/1000 | Loss: 0.00018795
Iteration 458/1000 | Loss: 0.00018795
Iteration 459/1000 | Loss: 0.00018795
Iteration 460/1000 | Loss: 0.00018795
Iteration 461/1000 | Loss: 0.00018795
Iteration 462/1000 | Loss: 0.00018795
Iteration 463/1000 | Loss: 0.00018795
Iteration 464/1000 | Loss: 0.00018795
Iteration 465/1000 | Loss: 0.00018795
Iteration 466/1000 | Loss: 0.00018795
Iteration 467/1000 | Loss: 0.00018794
Iteration 468/1000 | Loss: 0.00018794
Iteration 469/1000 | Loss: 0.00018794
Iteration 470/1000 | Loss: 0.00018793
Iteration 471/1000 | Loss: 0.00018793
Iteration 472/1000 | Loss: 0.00018793
Iteration 473/1000 | Loss: 0.00018793
Iteration 474/1000 | Loss: 0.00018793
Iteration 475/1000 | Loss: 0.00018792
Iteration 476/1000 | Loss: 0.00018792
Iteration 477/1000 | Loss: 0.00018792
Iteration 478/1000 | Loss: 0.00018792
Iteration 479/1000 | Loss: 0.00018792
Iteration 480/1000 | Loss: 0.00018792
Iteration 481/1000 | Loss: 0.00018792
Iteration 482/1000 | Loss: 0.00018792
Iteration 483/1000 | Loss: 0.00018792
Iteration 484/1000 | Loss: 0.00018792
Iteration 485/1000 | Loss: 0.00018791
Iteration 486/1000 | Loss: 0.00018791
Iteration 487/1000 | Loss: 0.00018791
Iteration 488/1000 | Loss: 0.00018791
Iteration 489/1000 | Loss: 0.00018790
Iteration 490/1000 | Loss: 0.00018790
Iteration 491/1000 | Loss: 0.00018790
Iteration 492/1000 | Loss: 0.00018790
Iteration 493/1000 | Loss: 0.00018790
Iteration 494/1000 | Loss: 0.00018790
Iteration 495/1000 | Loss: 0.00018790
Iteration 496/1000 | Loss: 0.00018790
Iteration 497/1000 | Loss: 0.00018790
Iteration 498/1000 | Loss: 0.00018790
Iteration 499/1000 | Loss: 0.00018789
Iteration 500/1000 | Loss: 0.00018789
Iteration 501/1000 | Loss: 0.00018789
Iteration 502/1000 | Loss: 0.00018789
Iteration 503/1000 | Loss: 0.00018789
Iteration 504/1000 | Loss: 0.00018789
Iteration 505/1000 | Loss: 0.00018789
Iteration 506/1000 | Loss: 0.00018789
Iteration 507/1000 | Loss: 0.00018789
Iteration 508/1000 | Loss: 0.00018789
Iteration 509/1000 | Loss: 0.00018789
Iteration 510/1000 | Loss: 0.00018789
Iteration 511/1000 | Loss: 0.00018789
Iteration 512/1000 | Loss: 0.00018789
Iteration 513/1000 | Loss: 0.00018789
Iteration 514/1000 | Loss: 0.00018788
Iteration 515/1000 | Loss: 0.00018788
Iteration 516/1000 | Loss: 0.00018788
Iteration 517/1000 | Loss: 0.00018788
Iteration 518/1000 | Loss: 0.00018788
Iteration 519/1000 | Loss: 0.00018787
Iteration 520/1000 | Loss: 0.00018787
Iteration 521/1000 | Loss: 0.00018787
Iteration 522/1000 | Loss: 0.00018787
Iteration 523/1000 | Loss: 0.00018787
Iteration 524/1000 | Loss: 0.00018787
Iteration 525/1000 | Loss: 0.00018787
Iteration 526/1000 | Loss: 0.00018787
Iteration 527/1000 | Loss: 0.00018787
Iteration 528/1000 | Loss: 0.00018786
Iteration 529/1000 | Loss: 0.00018786
Iteration 530/1000 | Loss: 0.00018786
Iteration 531/1000 | Loss: 0.00018786
Iteration 532/1000 | Loss: 0.00018786
Iteration 533/1000 | Loss: 0.00018786
Iteration 534/1000 | Loss: 0.00018786
Iteration 535/1000 | Loss: 0.00018786
Iteration 536/1000 | Loss: 0.00018786
Iteration 537/1000 | Loss: 0.00018786
Iteration 538/1000 | Loss: 0.00018786
Iteration 539/1000 | Loss: 0.00018786
Iteration 540/1000 | Loss: 0.00018786
Iteration 541/1000 | Loss: 0.00018786
Iteration 542/1000 | Loss: 0.00018786
Iteration 543/1000 | Loss: 0.00018786
Iteration 544/1000 | Loss: 0.00018786
Iteration 545/1000 | Loss: 0.00018786
Iteration 546/1000 | Loss: 0.00018786
Iteration 547/1000 | Loss: 0.00018786
Iteration 548/1000 | Loss: 0.00018786
Iteration 549/1000 | Loss: 0.00018786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 549. Stopping optimization.
Last 5 losses: [0.00018785614520311356, 0.00018785614520311356, 0.00018785614520311356, 0.00018785614520311356, 0.00018785614520311356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018785614520311356

Optimization complete. Final v2v error: 7.494918346405029 mm

Highest mean error: 20.930564880371094 mm for frame 152

Lowest mean error: 4.496851921081543 mm for frame 117

Saving results

Total time: 652.8111715316772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875928
Iteration 2/25 | Loss: 0.00111442
Iteration 3/25 | Loss: 0.00101145
Iteration 4/25 | Loss: 0.00096794
Iteration 5/25 | Loss: 0.00095985
Iteration 6/25 | Loss: 0.00095806
Iteration 7/25 | Loss: 0.00095763
Iteration 8/25 | Loss: 0.00095763
Iteration 9/25 | Loss: 0.00095763
Iteration 10/25 | Loss: 0.00095763
Iteration 11/25 | Loss: 0.00095763
Iteration 12/25 | Loss: 0.00095763
Iteration 13/25 | Loss: 0.00095763
Iteration 14/25 | Loss: 0.00095763
Iteration 15/25 | Loss: 0.00095763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009576298180036247, 0.0009576298180036247, 0.0009576298180036247, 0.0009576298180036247, 0.0009576298180036247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009576298180036247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59175253
Iteration 2/25 | Loss: 0.00208726
Iteration 3/25 | Loss: 0.00208721
Iteration 4/25 | Loss: 0.00208721
Iteration 5/25 | Loss: 0.00208721
Iteration 6/25 | Loss: 0.00208721
Iteration 7/25 | Loss: 0.00208721
Iteration 8/25 | Loss: 0.00208721
Iteration 9/25 | Loss: 0.00208721
Iteration 10/25 | Loss: 0.00208721
Iteration 11/25 | Loss: 0.00208721
Iteration 12/25 | Loss: 0.00208721
Iteration 13/25 | Loss: 0.00208721
Iteration 14/25 | Loss: 0.00208721
Iteration 15/25 | Loss: 0.00208721
Iteration 16/25 | Loss: 0.00208721
Iteration 17/25 | Loss: 0.00208721
Iteration 18/25 | Loss: 0.00208721
Iteration 19/25 | Loss: 0.00208721
Iteration 20/25 | Loss: 0.00208721
Iteration 21/25 | Loss: 0.00208721
Iteration 22/25 | Loss: 0.00208721
Iteration 23/25 | Loss: 0.00208721
Iteration 24/25 | Loss: 0.00208721
Iteration 25/25 | Loss: 0.00208721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0020872054155915976, 0.0020872054155915976, 0.0020872054155915976, 0.0020872054155915976, 0.0020872054155915976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020872054155915976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208721
Iteration 2/1000 | Loss: 0.00005590
Iteration 3/1000 | Loss: 0.00003084
Iteration 4/1000 | Loss: 0.00002597
Iteration 5/1000 | Loss: 0.00002415
Iteration 6/1000 | Loss: 0.00002300
Iteration 7/1000 | Loss: 0.00002235
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002084
Iteration 12/1000 | Loss: 0.00002065
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00002055
Iteration 15/1000 | Loss: 0.00002054
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00002053
Iteration 18/1000 | Loss: 0.00002052
Iteration 19/1000 | Loss: 0.00002052
Iteration 20/1000 | Loss: 0.00002052
Iteration 21/1000 | Loss: 0.00002051
Iteration 22/1000 | Loss: 0.00002051
Iteration 23/1000 | Loss: 0.00002051
Iteration 24/1000 | Loss: 0.00002051
Iteration 25/1000 | Loss: 0.00002050
Iteration 26/1000 | Loss: 0.00002050
Iteration 27/1000 | Loss: 0.00002050
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002049
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002047
Iteration 33/1000 | Loss: 0.00002047
Iteration 34/1000 | Loss: 0.00002046
Iteration 35/1000 | Loss: 0.00002046
Iteration 36/1000 | Loss: 0.00002045
Iteration 37/1000 | Loss: 0.00002045
Iteration 38/1000 | Loss: 0.00002045
Iteration 39/1000 | Loss: 0.00002044
Iteration 40/1000 | Loss: 0.00002042
Iteration 41/1000 | Loss: 0.00002042
Iteration 42/1000 | Loss: 0.00002042
Iteration 43/1000 | Loss: 0.00002042
Iteration 44/1000 | Loss: 0.00002041
Iteration 45/1000 | Loss: 0.00002041
Iteration 46/1000 | Loss: 0.00002040
Iteration 47/1000 | Loss: 0.00002039
Iteration 48/1000 | Loss: 0.00002039
Iteration 49/1000 | Loss: 0.00002038
Iteration 50/1000 | Loss: 0.00002038
Iteration 51/1000 | Loss: 0.00002038
Iteration 52/1000 | Loss: 0.00002037
Iteration 53/1000 | Loss: 0.00002037
Iteration 54/1000 | Loss: 0.00002037
Iteration 55/1000 | Loss: 0.00002036
Iteration 56/1000 | Loss: 0.00002036
Iteration 57/1000 | Loss: 0.00002035
Iteration 58/1000 | Loss: 0.00002035
Iteration 59/1000 | Loss: 0.00002034
Iteration 60/1000 | Loss: 0.00002034
Iteration 61/1000 | Loss: 0.00002033
Iteration 62/1000 | Loss: 0.00002033
Iteration 63/1000 | Loss: 0.00002033
Iteration 64/1000 | Loss: 0.00002032
Iteration 65/1000 | Loss: 0.00002032
Iteration 66/1000 | Loss: 0.00002032
Iteration 67/1000 | Loss: 0.00002032
Iteration 68/1000 | Loss: 0.00002032
Iteration 69/1000 | Loss: 0.00002032
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002031
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002030
Iteration 77/1000 | Loss: 0.00002030
Iteration 78/1000 | Loss: 0.00002030
Iteration 79/1000 | Loss: 0.00002030
Iteration 80/1000 | Loss: 0.00002030
Iteration 81/1000 | Loss: 0.00002030
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002029
Iteration 84/1000 | Loss: 0.00002029
Iteration 85/1000 | Loss: 0.00002029
Iteration 86/1000 | Loss: 0.00002029
Iteration 87/1000 | Loss: 0.00002029
Iteration 88/1000 | Loss: 0.00002029
Iteration 89/1000 | Loss: 0.00002029
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002028
Iteration 93/1000 | Loss: 0.00002028
Iteration 94/1000 | Loss: 0.00002028
Iteration 95/1000 | Loss: 0.00002028
Iteration 96/1000 | Loss: 0.00002028
Iteration 97/1000 | Loss: 0.00002027
Iteration 98/1000 | Loss: 0.00002027
Iteration 99/1000 | Loss: 0.00002027
Iteration 100/1000 | Loss: 0.00002026
Iteration 101/1000 | Loss: 0.00002026
Iteration 102/1000 | Loss: 0.00002026
Iteration 103/1000 | Loss: 0.00002026
Iteration 104/1000 | Loss: 0.00002026
Iteration 105/1000 | Loss: 0.00002024
Iteration 106/1000 | Loss: 0.00002024
Iteration 107/1000 | Loss: 0.00002023
Iteration 108/1000 | Loss: 0.00002023
Iteration 109/1000 | Loss: 0.00002023
Iteration 110/1000 | Loss: 0.00002023
Iteration 111/1000 | Loss: 0.00002023
Iteration 112/1000 | Loss: 0.00002022
Iteration 113/1000 | Loss: 0.00002022
Iteration 114/1000 | Loss: 0.00002022
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00002021
Iteration 117/1000 | Loss: 0.00002021
Iteration 118/1000 | Loss: 0.00002021
Iteration 119/1000 | Loss: 0.00002021
Iteration 120/1000 | Loss: 0.00002021
Iteration 121/1000 | Loss: 0.00002021
Iteration 122/1000 | Loss: 0.00002021
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002020
Iteration 125/1000 | Loss: 0.00002020
Iteration 126/1000 | Loss: 0.00002020
Iteration 127/1000 | Loss: 0.00002020
Iteration 128/1000 | Loss: 0.00002020
Iteration 129/1000 | Loss: 0.00002020
Iteration 130/1000 | Loss: 0.00002020
Iteration 131/1000 | Loss: 0.00002020
Iteration 132/1000 | Loss: 0.00002020
Iteration 133/1000 | Loss: 0.00002020
Iteration 134/1000 | Loss: 0.00002020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.019553903664928e-05, 2.019553903664928e-05, 2.019553903664928e-05, 2.019553903664928e-05, 2.019553903664928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.019553903664928e-05

Optimization complete. Final v2v error: 3.7819180488586426 mm

Highest mean error: 4.5509114265441895 mm for frame 233

Lowest mean error: 3.3387858867645264 mm for frame 134

Saving results

Total time: 43.634117126464844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909562
Iteration 2/25 | Loss: 0.00121529
Iteration 3/25 | Loss: 0.00095321
Iteration 4/25 | Loss: 0.00089251
Iteration 5/25 | Loss: 0.00087726
Iteration 6/25 | Loss: 0.00087261
Iteration 7/25 | Loss: 0.00087023
Iteration 8/25 | Loss: 0.00086766
Iteration 9/25 | Loss: 0.00086942
Iteration 10/25 | Loss: 0.00086663
Iteration 11/25 | Loss: 0.00086786
Iteration 12/25 | Loss: 0.00086652
Iteration 13/25 | Loss: 0.00086651
Iteration 14/25 | Loss: 0.00086651
Iteration 15/25 | Loss: 0.00086651
Iteration 16/25 | Loss: 0.00086651
Iteration 17/25 | Loss: 0.00086651
Iteration 18/25 | Loss: 0.00086651
Iteration 19/25 | Loss: 0.00086651
Iteration 20/25 | Loss: 0.00086651
Iteration 21/25 | Loss: 0.00086651
Iteration 22/25 | Loss: 0.00086651
Iteration 23/25 | Loss: 0.00086651
Iteration 24/25 | Loss: 0.00086651
Iteration 25/25 | Loss: 0.00086650

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97139525
Iteration 2/25 | Loss: 0.00162680
Iteration 3/25 | Loss: 0.00162680
Iteration 4/25 | Loss: 0.00162680
Iteration 5/25 | Loss: 0.00162680
Iteration 6/25 | Loss: 0.00162680
Iteration 7/25 | Loss: 0.00162680
Iteration 8/25 | Loss: 0.00162680
Iteration 9/25 | Loss: 0.00162680
Iteration 10/25 | Loss: 0.00162680
Iteration 11/25 | Loss: 0.00162680
Iteration 12/25 | Loss: 0.00162680
Iteration 13/25 | Loss: 0.00162680
Iteration 14/25 | Loss: 0.00162680
Iteration 15/25 | Loss: 0.00162680
Iteration 16/25 | Loss: 0.00162680
Iteration 17/25 | Loss: 0.00162680
Iteration 18/25 | Loss: 0.00162680
Iteration 19/25 | Loss: 0.00162680
Iteration 20/25 | Loss: 0.00162680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001626799814403057, 0.001626799814403057, 0.001626799814403057, 0.001626799814403057, 0.001626799814403057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001626799814403057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162680
Iteration 2/1000 | Loss: 0.00003934
Iteration 3/1000 | Loss: 0.00003858
Iteration 4/1000 | Loss: 0.00002756
Iteration 5/1000 | Loss: 0.00002603
Iteration 6/1000 | Loss: 0.00002347
Iteration 7/1000 | Loss: 0.00002009
Iteration 8/1000 | Loss: 0.00001976
Iteration 9/1000 | Loss: 0.00001933
Iteration 10/1000 | Loss: 0.00001922
Iteration 11/1000 | Loss: 0.00001908
Iteration 12/1000 | Loss: 0.00003514
Iteration 13/1000 | Loss: 0.00002160
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001889
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001889
Iteration 19/1000 | Loss: 0.00001889
Iteration 20/1000 | Loss: 0.00001889
Iteration 21/1000 | Loss: 0.00001884
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001877
Iteration 29/1000 | Loss: 0.00001877
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001876
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001871
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001860
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001856
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001856
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001856
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001850
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001840
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001839
Iteration 60/1000 | Loss: 0.00001839
Iteration 61/1000 | Loss: 0.00001839
Iteration 62/1000 | Loss: 0.00001838
Iteration 63/1000 | Loss: 0.00001838
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001837
Iteration 66/1000 | Loss: 0.00001837
Iteration 67/1000 | Loss: 0.00001836
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001835
Iteration 70/1000 | Loss: 0.00001835
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001835
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001834
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001834
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001834
Iteration 85/1000 | Loss: 0.00001834
Iteration 86/1000 | Loss: 0.00001834
Iteration 87/1000 | Loss: 0.00001834
Iteration 88/1000 | Loss: 0.00001834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.8340837414143607e-05, 1.8340837414143607e-05, 1.8340837414143607e-05, 1.8340837414143607e-05, 1.8340837414143607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8340837414143607e-05

Optimization complete. Final v2v error: 3.639225721359253 mm

Highest mean error: 10.694743156433105 mm for frame 27

Lowest mean error: 3.3216042518615723 mm for frame 139

Saving results

Total time: 54.361324071884155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00933735
Iteration 2/25 | Loss: 0.00145913
Iteration 3/25 | Loss: 0.00107473
Iteration 4/25 | Loss: 0.00101435
Iteration 5/25 | Loss: 0.00099690
Iteration 6/25 | Loss: 0.00099194
Iteration 7/25 | Loss: 0.00098994
Iteration 8/25 | Loss: 0.00098927
Iteration 9/25 | Loss: 0.00098927
Iteration 10/25 | Loss: 0.00098927
Iteration 11/25 | Loss: 0.00098927
Iteration 12/25 | Loss: 0.00098927
Iteration 13/25 | Loss: 0.00098927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009892672533169389, 0.0009892672533169389, 0.0009892672533169389, 0.0009892672533169389, 0.0009892672533169389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009892672533169389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.42398119
Iteration 2/25 | Loss: 0.00167273
Iteration 3/25 | Loss: 0.00167273
Iteration 4/25 | Loss: 0.00167273
Iteration 5/25 | Loss: 0.00167273
Iteration 6/25 | Loss: 0.00167273
Iteration 7/25 | Loss: 0.00167273
Iteration 8/25 | Loss: 0.00167273
Iteration 9/25 | Loss: 0.00167273
Iteration 10/25 | Loss: 0.00167273
Iteration 11/25 | Loss: 0.00167273
Iteration 12/25 | Loss: 0.00167273
Iteration 13/25 | Loss: 0.00167273
Iteration 14/25 | Loss: 0.00167273
Iteration 15/25 | Loss: 0.00167273
Iteration 16/25 | Loss: 0.00167273
Iteration 17/25 | Loss: 0.00167273
Iteration 18/25 | Loss: 0.00167273
Iteration 19/25 | Loss: 0.00167273
Iteration 20/25 | Loss: 0.00167273
Iteration 21/25 | Loss: 0.00167273
Iteration 22/25 | Loss: 0.00167273
Iteration 23/25 | Loss: 0.00167273
Iteration 24/25 | Loss: 0.00167273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016727268230170012, 0.0016727268230170012, 0.0016727268230170012, 0.0016727268230170012, 0.0016727268230170012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016727268230170012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167273
Iteration 2/1000 | Loss: 0.00007919
Iteration 3/1000 | Loss: 0.00004357
Iteration 4/1000 | Loss: 0.00003640
Iteration 5/1000 | Loss: 0.00003373
Iteration 6/1000 | Loss: 0.00003165
Iteration 7/1000 | Loss: 0.00003045
Iteration 8/1000 | Loss: 0.00002961
Iteration 9/1000 | Loss: 0.00002904
Iteration 10/1000 | Loss: 0.00002854
Iteration 11/1000 | Loss: 0.00002826
Iteration 12/1000 | Loss: 0.00002799
Iteration 13/1000 | Loss: 0.00002773
Iteration 14/1000 | Loss: 0.00002753
Iteration 15/1000 | Loss: 0.00002736
Iteration 16/1000 | Loss: 0.00002730
Iteration 17/1000 | Loss: 0.00002722
Iteration 18/1000 | Loss: 0.00002718
Iteration 19/1000 | Loss: 0.00002717
Iteration 20/1000 | Loss: 0.00002715
Iteration 21/1000 | Loss: 0.00002710
Iteration 22/1000 | Loss: 0.00002709
Iteration 23/1000 | Loss: 0.00002708
Iteration 24/1000 | Loss: 0.00002706
Iteration 25/1000 | Loss: 0.00002706
Iteration 26/1000 | Loss: 0.00002705
Iteration 27/1000 | Loss: 0.00002705
Iteration 28/1000 | Loss: 0.00002704
Iteration 29/1000 | Loss: 0.00002704
Iteration 30/1000 | Loss: 0.00002704
Iteration 31/1000 | Loss: 0.00002703
Iteration 32/1000 | Loss: 0.00002702
Iteration 33/1000 | Loss: 0.00002701
Iteration 34/1000 | Loss: 0.00002701
Iteration 35/1000 | Loss: 0.00002701
Iteration 36/1000 | Loss: 0.00002700
Iteration 37/1000 | Loss: 0.00002700
Iteration 38/1000 | Loss: 0.00002700
Iteration 39/1000 | Loss: 0.00002699
Iteration 40/1000 | Loss: 0.00002699
Iteration 41/1000 | Loss: 0.00002699
Iteration 42/1000 | Loss: 0.00002698
Iteration 43/1000 | Loss: 0.00002698
Iteration 44/1000 | Loss: 0.00002698
Iteration 45/1000 | Loss: 0.00002697
Iteration 46/1000 | Loss: 0.00002697
Iteration 47/1000 | Loss: 0.00002697
Iteration 48/1000 | Loss: 0.00002696
Iteration 49/1000 | Loss: 0.00002696
Iteration 50/1000 | Loss: 0.00002696
Iteration 51/1000 | Loss: 0.00002695
Iteration 52/1000 | Loss: 0.00002695
Iteration 53/1000 | Loss: 0.00002695
Iteration 54/1000 | Loss: 0.00002694
Iteration 55/1000 | Loss: 0.00002694
Iteration 56/1000 | Loss: 0.00002694
Iteration 57/1000 | Loss: 0.00002692
Iteration 58/1000 | Loss: 0.00002691
Iteration 59/1000 | Loss: 0.00002691
Iteration 60/1000 | Loss: 0.00002689
Iteration 61/1000 | Loss: 0.00002689
Iteration 62/1000 | Loss: 0.00002689
Iteration 63/1000 | Loss: 0.00002689
Iteration 64/1000 | Loss: 0.00002689
Iteration 65/1000 | Loss: 0.00002689
Iteration 66/1000 | Loss: 0.00002689
Iteration 67/1000 | Loss: 0.00002689
Iteration 68/1000 | Loss: 0.00002689
Iteration 69/1000 | Loss: 0.00002689
Iteration 70/1000 | Loss: 0.00002688
Iteration 71/1000 | Loss: 0.00002688
Iteration 72/1000 | Loss: 0.00002688
Iteration 73/1000 | Loss: 0.00002688
Iteration 74/1000 | Loss: 0.00002688
Iteration 75/1000 | Loss: 0.00002688
Iteration 76/1000 | Loss: 0.00002688
Iteration 77/1000 | Loss: 0.00002688
Iteration 78/1000 | Loss: 0.00002688
Iteration 79/1000 | Loss: 0.00002688
Iteration 80/1000 | Loss: 0.00002687
Iteration 81/1000 | Loss: 0.00002687
Iteration 82/1000 | Loss: 0.00002687
Iteration 83/1000 | Loss: 0.00002686
Iteration 84/1000 | Loss: 0.00002685
Iteration 85/1000 | Loss: 0.00002685
Iteration 86/1000 | Loss: 0.00002685
Iteration 87/1000 | Loss: 0.00002684
Iteration 88/1000 | Loss: 0.00002684
Iteration 89/1000 | Loss: 0.00002684
Iteration 90/1000 | Loss: 0.00002682
Iteration 91/1000 | Loss: 0.00002682
Iteration 92/1000 | Loss: 0.00002682
Iteration 93/1000 | Loss: 0.00002680
Iteration 94/1000 | Loss: 0.00002679
Iteration 95/1000 | Loss: 0.00002678
Iteration 96/1000 | Loss: 0.00002678
Iteration 97/1000 | Loss: 0.00002677
Iteration 98/1000 | Loss: 0.00002677
Iteration 99/1000 | Loss: 0.00002677
Iteration 100/1000 | Loss: 0.00002677
Iteration 101/1000 | Loss: 0.00002676
Iteration 102/1000 | Loss: 0.00002674
Iteration 103/1000 | Loss: 0.00002674
Iteration 104/1000 | Loss: 0.00002674
Iteration 105/1000 | Loss: 0.00002673
Iteration 106/1000 | Loss: 0.00002673
Iteration 107/1000 | Loss: 0.00002672
Iteration 108/1000 | Loss: 0.00002672
Iteration 109/1000 | Loss: 0.00002672
Iteration 110/1000 | Loss: 0.00002671
Iteration 111/1000 | Loss: 0.00002671
Iteration 112/1000 | Loss: 0.00002670
Iteration 113/1000 | Loss: 0.00002670
Iteration 114/1000 | Loss: 0.00002670
Iteration 115/1000 | Loss: 0.00002669
Iteration 116/1000 | Loss: 0.00002669
Iteration 117/1000 | Loss: 0.00002669
Iteration 118/1000 | Loss: 0.00002668
Iteration 119/1000 | Loss: 0.00002668
Iteration 120/1000 | Loss: 0.00002668
Iteration 121/1000 | Loss: 0.00002667
Iteration 122/1000 | Loss: 0.00002667
Iteration 123/1000 | Loss: 0.00002666
Iteration 124/1000 | Loss: 0.00002666
Iteration 125/1000 | Loss: 0.00002666
Iteration 126/1000 | Loss: 0.00002666
Iteration 127/1000 | Loss: 0.00002665
Iteration 128/1000 | Loss: 0.00002665
Iteration 129/1000 | Loss: 0.00002665
Iteration 130/1000 | Loss: 0.00002665
Iteration 131/1000 | Loss: 0.00002665
Iteration 132/1000 | Loss: 0.00002665
Iteration 133/1000 | Loss: 0.00002665
Iteration 134/1000 | Loss: 0.00002665
Iteration 135/1000 | Loss: 0.00002665
Iteration 136/1000 | Loss: 0.00002664
Iteration 137/1000 | Loss: 0.00002664
Iteration 138/1000 | Loss: 0.00002663
Iteration 139/1000 | Loss: 0.00002663
Iteration 140/1000 | Loss: 0.00002662
Iteration 141/1000 | Loss: 0.00002662
Iteration 142/1000 | Loss: 0.00002662
Iteration 143/1000 | Loss: 0.00002662
Iteration 144/1000 | Loss: 0.00002661
Iteration 145/1000 | Loss: 0.00002661
Iteration 146/1000 | Loss: 0.00002661
Iteration 147/1000 | Loss: 0.00002660
Iteration 148/1000 | Loss: 0.00002660
Iteration 149/1000 | Loss: 0.00002660
Iteration 150/1000 | Loss: 0.00002660
Iteration 151/1000 | Loss: 0.00002659
Iteration 152/1000 | Loss: 0.00002659
Iteration 153/1000 | Loss: 0.00002659
Iteration 154/1000 | Loss: 0.00002658
Iteration 155/1000 | Loss: 0.00002658
Iteration 156/1000 | Loss: 0.00002658
Iteration 157/1000 | Loss: 0.00002658
Iteration 158/1000 | Loss: 0.00002657
Iteration 159/1000 | Loss: 0.00002657
Iteration 160/1000 | Loss: 0.00002657
Iteration 161/1000 | Loss: 0.00002656
Iteration 162/1000 | Loss: 0.00002656
Iteration 163/1000 | Loss: 0.00002656
Iteration 164/1000 | Loss: 0.00002656
Iteration 165/1000 | Loss: 0.00002656
Iteration 166/1000 | Loss: 0.00002656
Iteration 167/1000 | Loss: 0.00002656
Iteration 168/1000 | Loss: 0.00002656
Iteration 169/1000 | Loss: 0.00002656
Iteration 170/1000 | Loss: 0.00002655
Iteration 171/1000 | Loss: 0.00002655
Iteration 172/1000 | Loss: 0.00002655
Iteration 173/1000 | Loss: 0.00002655
Iteration 174/1000 | Loss: 0.00002654
Iteration 175/1000 | Loss: 0.00002654
Iteration 176/1000 | Loss: 0.00002654
Iteration 177/1000 | Loss: 0.00002654
Iteration 178/1000 | Loss: 0.00002654
Iteration 179/1000 | Loss: 0.00002654
Iteration 180/1000 | Loss: 0.00002654
Iteration 181/1000 | Loss: 0.00002653
Iteration 182/1000 | Loss: 0.00002653
Iteration 183/1000 | Loss: 0.00002653
Iteration 184/1000 | Loss: 0.00002653
Iteration 185/1000 | Loss: 0.00002653
Iteration 186/1000 | Loss: 0.00002653
Iteration 187/1000 | Loss: 0.00002653
Iteration 188/1000 | Loss: 0.00002653
Iteration 189/1000 | Loss: 0.00002653
Iteration 190/1000 | Loss: 0.00002653
Iteration 191/1000 | Loss: 0.00002652
Iteration 192/1000 | Loss: 0.00002652
Iteration 193/1000 | Loss: 0.00002652
Iteration 194/1000 | Loss: 0.00002652
Iteration 195/1000 | Loss: 0.00002652
Iteration 196/1000 | Loss: 0.00002651
Iteration 197/1000 | Loss: 0.00002651
Iteration 198/1000 | Loss: 0.00002651
Iteration 199/1000 | Loss: 0.00002651
Iteration 200/1000 | Loss: 0.00002651
Iteration 201/1000 | Loss: 0.00002651
Iteration 202/1000 | Loss: 0.00002651
Iteration 203/1000 | Loss: 0.00002651
Iteration 204/1000 | Loss: 0.00002650
Iteration 205/1000 | Loss: 0.00002650
Iteration 206/1000 | Loss: 0.00002650
Iteration 207/1000 | Loss: 0.00002649
Iteration 208/1000 | Loss: 0.00002649
Iteration 209/1000 | Loss: 0.00002649
Iteration 210/1000 | Loss: 0.00002649
Iteration 211/1000 | Loss: 0.00002649
Iteration 212/1000 | Loss: 0.00002649
Iteration 213/1000 | Loss: 0.00002649
Iteration 214/1000 | Loss: 0.00002649
Iteration 215/1000 | Loss: 0.00002649
Iteration 216/1000 | Loss: 0.00002649
Iteration 217/1000 | Loss: 0.00002649
Iteration 218/1000 | Loss: 0.00002648
Iteration 219/1000 | Loss: 0.00002648
Iteration 220/1000 | Loss: 0.00002648
Iteration 221/1000 | Loss: 0.00002648
Iteration 222/1000 | Loss: 0.00002648
Iteration 223/1000 | Loss: 0.00002648
Iteration 224/1000 | Loss: 0.00002648
Iteration 225/1000 | Loss: 0.00002648
Iteration 226/1000 | Loss: 0.00002648
Iteration 227/1000 | Loss: 0.00002648
Iteration 228/1000 | Loss: 0.00002647
Iteration 229/1000 | Loss: 0.00002647
Iteration 230/1000 | Loss: 0.00002647
Iteration 231/1000 | Loss: 0.00002647
Iteration 232/1000 | Loss: 0.00002647
Iteration 233/1000 | Loss: 0.00002647
Iteration 234/1000 | Loss: 0.00002647
Iteration 235/1000 | Loss: 0.00002647
Iteration 236/1000 | Loss: 0.00002647
Iteration 237/1000 | Loss: 0.00002647
Iteration 238/1000 | Loss: 0.00002646
Iteration 239/1000 | Loss: 0.00002646
Iteration 240/1000 | Loss: 0.00002646
Iteration 241/1000 | Loss: 0.00002646
Iteration 242/1000 | Loss: 0.00002646
Iteration 243/1000 | Loss: 0.00002646
Iteration 244/1000 | Loss: 0.00002646
Iteration 245/1000 | Loss: 0.00002646
Iteration 246/1000 | Loss: 0.00002646
Iteration 247/1000 | Loss: 0.00002646
Iteration 248/1000 | Loss: 0.00002646
Iteration 249/1000 | Loss: 0.00002646
Iteration 250/1000 | Loss: 0.00002646
Iteration 251/1000 | Loss: 0.00002646
Iteration 252/1000 | Loss: 0.00002646
Iteration 253/1000 | Loss: 0.00002646
Iteration 254/1000 | Loss: 0.00002646
Iteration 255/1000 | Loss: 0.00002646
Iteration 256/1000 | Loss: 0.00002646
Iteration 257/1000 | Loss: 0.00002646
Iteration 258/1000 | Loss: 0.00002646
Iteration 259/1000 | Loss: 0.00002646
Iteration 260/1000 | Loss: 0.00002646
Iteration 261/1000 | Loss: 0.00002646
Iteration 262/1000 | Loss: 0.00002646
Iteration 263/1000 | Loss: 0.00002646
Iteration 264/1000 | Loss: 0.00002646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [2.6462270398042165e-05, 2.6462270398042165e-05, 2.6462270398042165e-05, 2.6462270398042165e-05, 2.6462270398042165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6462270398042165e-05

Optimization complete. Final v2v error: 4.207812786102295 mm

Highest mean error: 6.339742183685303 mm for frame 91

Lowest mean error: 3.0586605072021484 mm for frame 111

Saving results

Total time: 55.71566581726074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00694783
Iteration 2/25 | Loss: 0.00137191
Iteration 3/25 | Loss: 0.00107795
Iteration 4/25 | Loss: 0.00102037
Iteration 5/25 | Loss: 0.00100733
Iteration 6/25 | Loss: 0.00101063
Iteration 7/25 | Loss: 0.00100278
Iteration 8/25 | Loss: 0.00099438
Iteration 9/25 | Loss: 0.00098594
Iteration 10/25 | Loss: 0.00097977
Iteration 11/25 | Loss: 0.00097604
Iteration 12/25 | Loss: 0.00097499
Iteration 13/25 | Loss: 0.00097484
Iteration 14/25 | Loss: 0.00097482
Iteration 15/25 | Loss: 0.00097482
Iteration 16/25 | Loss: 0.00097482
Iteration 17/25 | Loss: 0.00097482
Iteration 18/25 | Loss: 0.00097482
Iteration 19/25 | Loss: 0.00097482
Iteration 20/25 | Loss: 0.00097482
Iteration 21/25 | Loss: 0.00097482
Iteration 22/25 | Loss: 0.00097481
Iteration 23/25 | Loss: 0.00097481
Iteration 24/25 | Loss: 0.00097481
Iteration 25/25 | Loss: 0.00097481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25344110
Iteration 2/25 | Loss: 0.00197923
Iteration 3/25 | Loss: 0.00197904
Iteration 4/25 | Loss: 0.00197904
Iteration 5/25 | Loss: 0.00197904
Iteration 6/25 | Loss: 0.00197904
Iteration 7/25 | Loss: 0.00197904
Iteration 8/25 | Loss: 0.00197904
Iteration 9/25 | Loss: 0.00197904
Iteration 10/25 | Loss: 0.00197904
Iteration 11/25 | Loss: 0.00197904
Iteration 12/25 | Loss: 0.00197904
Iteration 13/25 | Loss: 0.00197904
Iteration 14/25 | Loss: 0.00197904
Iteration 15/25 | Loss: 0.00197904
Iteration 16/25 | Loss: 0.00197904
Iteration 17/25 | Loss: 0.00197904
Iteration 18/25 | Loss: 0.00197904
Iteration 19/25 | Loss: 0.00197904
Iteration 20/25 | Loss: 0.00197904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019790399819612503, 0.0019790399819612503, 0.0019790399819612503, 0.0019790399819612503, 0.0019790399819612503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019790399819612503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197904
Iteration 2/1000 | Loss: 0.00005663
Iteration 3/1000 | Loss: 0.00004145
Iteration 4/1000 | Loss: 0.00003636
Iteration 5/1000 | Loss: 0.00003424
Iteration 6/1000 | Loss: 0.00003216
Iteration 7/1000 | Loss: 0.00003071
Iteration 8/1000 | Loss: 0.00003008
Iteration 9/1000 | Loss: 0.00002952
Iteration 10/1000 | Loss: 0.00002899
Iteration 11/1000 | Loss: 0.00002869
Iteration 12/1000 | Loss: 0.00002846
Iteration 13/1000 | Loss: 0.00002823
Iteration 14/1000 | Loss: 0.00002807
Iteration 15/1000 | Loss: 0.00002797
Iteration 16/1000 | Loss: 0.00002793
Iteration 17/1000 | Loss: 0.00002790
Iteration 18/1000 | Loss: 0.00002787
Iteration 19/1000 | Loss: 0.00002787
Iteration 20/1000 | Loss: 0.00002787
Iteration 21/1000 | Loss: 0.00002783
Iteration 22/1000 | Loss: 0.00002782
Iteration 23/1000 | Loss: 0.00002782
Iteration 24/1000 | Loss: 0.00002782
Iteration 25/1000 | Loss: 0.00002781
Iteration 26/1000 | Loss: 0.00002781
Iteration 27/1000 | Loss: 0.00002780
Iteration 28/1000 | Loss: 0.00002776
Iteration 29/1000 | Loss: 0.00002776
Iteration 30/1000 | Loss: 0.00002774
Iteration 31/1000 | Loss: 0.00002774
Iteration 32/1000 | Loss: 0.00002769
Iteration 33/1000 | Loss: 0.00002769
Iteration 34/1000 | Loss: 0.00002768
Iteration 35/1000 | Loss: 0.00002768
Iteration 36/1000 | Loss: 0.00002767
Iteration 37/1000 | Loss: 0.00002767
Iteration 38/1000 | Loss: 0.00002767
Iteration 39/1000 | Loss: 0.00002766
Iteration 40/1000 | Loss: 0.00002765
Iteration 41/1000 | Loss: 0.00002765
Iteration 42/1000 | Loss: 0.00002764
Iteration 43/1000 | Loss: 0.00002764
Iteration 44/1000 | Loss: 0.00002764
Iteration 45/1000 | Loss: 0.00002763
Iteration 46/1000 | Loss: 0.00002763
Iteration 47/1000 | Loss: 0.00002763
Iteration 48/1000 | Loss: 0.00002762
Iteration 49/1000 | Loss: 0.00002762
Iteration 50/1000 | Loss: 0.00002762
Iteration 51/1000 | Loss: 0.00002761
Iteration 52/1000 | Loss: 0.00002761
Iteration 53/1000 | Loss: 0.00002761
Iteration 54/1000 | Loss: 0.00002761
Iteration 55/1000 | Loss: 0.00002760
Iteration 56/1000 | Loss: 0.00002760
Iteration 57/1000 | Loss: 0.00002759
Iteration 58/1000 | Loss: 0.00002758
Iteration 59/1000 | Loss: 0.00002758
Iteration 60/1000 | Loss: 0.00002758
Iteration 61/1000 | Loss: 0.00002758
Iteration 62/1000 | Loss: 0.00002758
Iteration 63/1000 | Loss: 0.00002757
Iteration 64/1000 | Loss: 0.00002757
Iteration 65/1000 | Loss: 0.00002757
Iteration 66/1000 | Loss: 0.00002756
Iteration 67/1000 | Loss: 0.00002756
Iteration 68/1000 | Loss: 0.00002755
Iteration 69/1000 | Loss: 0.00002755
Iteration 70/1000 | Loss: 0.00002755
Iteration 71/1000 | Loss: 0.00002754
Iteration 72/1000 | Loss: 0.00002754
Iteration 73/1000 | Loss: 0.00002753
Iteration 74/1000 | Loss: 0.00002753
Iteration 75/1000 | Loss: 0.00002753
Iteration 76/1000 | Loss: 0.00002753
Iteration 77/1000 | Loss: 0.00002752
Iteration 78/1000 | Loss: 0.00002752
Iteration 79/1000 | Loss: 0.00002752
Iteration 80/1000 | Loss: 0.00002751
Iteration 81/1000 | Loss: 0.00002751
Iteration 82/1000 | Loss: 0.00002749
Iteration 83/1000 | Loss: 0.00002749
Iteration 84/1000 | Loss: 0.00002749
Iteration 85/1000 | Loss: 0.00002749
Iteration 86/1000 | Loss: 0.00002749
Iteration 87/1000 | Loss: 0.00002749
Iteration 88/1000 | Loss: 0.00002749
Iteration 89/1000 | Loss: 0.00002749
Iteration 90/1000 | Loss: 0.00002749
Iteration 91/1000 | Loss: 0.00002748
Iteration 92/1000 | Loss: 0.00002748
Iteration 93/1000 | Loss: 0.00002748
Iteration 94/1000 | Loss: 0.00002747
Iteration 95/1000 | Loss: 0.00002747
Iteration 96/1000 | Loss: 0.00002747
Iteration 97/1000 | Loss: 0.00002746
Iteration 98/1000 | Loss: 0.00002745
Iteration 99/1000 | Loss: 0.00002745
Iteration 100/1000 | Loss: 0.00002745
Iteration 101/1000 | Loss: 0.00002744
Iteration 102/1000 | Loss: 0.00002744
Iteration 103/1000 | Loss: 0.00002744
Iteration 104/1000 | Loss: 0.00002744
Iteration 105/1000 | Loss: 0.00002743
Iteration 106/1000 | Loss: 0.00002743
Iteration 107/1000 | Loss: 0.00002743
Iteration 108/1000 | Loss: 0.00002743
Iteration 109/1000 | Loss: 0.00002742
Iteration 110/1000 | Loss: 0.00002742
Iteration 111/1000 | Loss: 0.00002742
Iteration 112/1000 | Loss: 0.00002742
Iteration 113/1000 | Loss: 0.00002741
Iteration 114/1000 | Loss: 0.00002741
Iteration 115/1000 | Loss: 0.00002741
Iteration 116/1000 | Loss: 0.00002740
Iteration 117/1000 | Loss: 0.00002740
Iteration 118/1000 | Loss: 0.00002740
Iteration 119/1000 | Loss: 0.00002740
Iteration 120/1000 | Loss: 0.00002740
Iteration 121/1000 | Loss: 0.00002740
Iteration 122/1000 | Loss: 0.00002740
Iteration 123/1000 | Loss: 0.00002740
Iteration 124/1000 | Loss: 0.00002740
Iteration 125/1000 | Loss: 0.00002740
Iteration 126/1000 | Loss: 0.00002740
Iteration 127/1000 | Loss: 0.00002740
Iteration 128/1000 | Loss: 0.00002740
Iteration 129/1000 | Loss: 0.00002739
Iteration 130/1000 | Loss: 0.00002739
Iteration 131/1000 | Loss: 0.00002739
Iteration 132/1000 | Loss: 0.00002739
Iteration 133/1000 | Loss: 0.00002739
Iteration 134/1000 | Loss: 0.00002739
Iteration 135/1000 | Loss: 0.00002739
Iteration 136/1000 | Loss: 0.00002738
Iteration 137/1000 | Loss: 0.00002738
Iteration 138/1000 | Loss: 0.00002738
Iteration 139/1000 | Loss: 0.00002738
Iteration 140/1000 | Loss: 0.00002738
Iteration 141/1000 | Loss: 0.00002738
Iteration 142/1000 | Loss: 0.00002738
Iteration 143/1000 | Loss: 0.00002738
Iteration 144/1000 | Loss: 0.00002738
Iteration 145/1000 | Loss: 0.00002737
Iteration 146/1000 | Loss: 0.00002737
Iteration 147/1000 | Loss: 0.00002737
Iteration 148/1000 | Loss: 0.00002737
Iteration 149/1000 | Loss: 0.00002737
Iteration 150/1000 | Loss: 0.00002737
Iteration 151/1000 | Loss: 0.00002737
Iteration 152/1000 | Loss: 0.00002737
Iteration 153/1000 | Loss: 0.00002736
Iteration 154/1000 | Loss: 0.00002736
Iteration 155/1000 | Loss: 0.00002736
Iteration 156/1000 | Loss: 0.00002736
Iteration 157/1000 | Loss: 0.00002736
Iteration 158/1000 | Loss: 0.00002736
Iteration 159/1000 | Loss: 0.00002736
Iteration 160/1000 | Loss: 0.00002736
Iteration 161/1000 | Loss: 0.00002736
Iteration 162/1000 | Loss: 0.00002736
Iteration 163/1000 | Loss: 0.00002736
Iteration 164/1000 | Loss: 0.00002736
Iteration 165/1000 | Loss: 0.00002736
Iteration 166/1000 | Loss: 0.00002736
Iteration 167/1000 | Loss: 0.00002735
Iteration 168/1000 | Loss: 0.00002735
Iteration 169/1000 | Loss: 0.00002735
Iteration 170/1000 | Loss: 0.00002735
Iteration 171/1000 | Loss: 0.00002735
Iteration 172/1000 | Loss: 0.00002735
Iteration 173/1000 | Loss: 0.00002735
Iteration 174/1000 | Loss: 0.00002735
Iteration 175/1000 | Loss: 0.00002735
Iteration 176/1000 | Loss: 0.00002735
Iteration 177/1000 | Loss: 0.00002735
Iteration 178/1000 | Loss: 0.00002735
Iteration 179/1000 | Loss: 0.00002735
Iteration 180/1000 | Loss: 0.00002734
Iteration 181/1000 | Loss: 0.00002734
Iteration 182/1000 | Loss: 0.00002734
Iteration 183/1000 | Loss: 0.00002734
Iteration 184/1000 | Loss: 0.00002734
Iteration 185/1000 | Loss: 0.00002734
Iteration 186/1000 | Loss: 0.00002734
Iteration 187/1000 | Loss: 0.00002734
Iteration 188/1000 | Loss: 0.00002733
Iteration 189/1000 | Loss: 0.00002733
Iteration 190/1000 | Loss: 0.00002733
Iteration 191/1000 | Loss: 0.00002733
Iteration 192/1000 | Loss: 0.00002733
Iteration 193/1000 | Loss: 0.00002733
Iteration 194/1000 | Loss: 0.00002733
Iteration 195/1000 | Loss: 0.00002733
Iteration 196/1000 | Loss: 0.00002733
Iteration 197/1000 | Loss: 0.00002732
Iteration 198/1000 | Loss: 0.00002732
Iteration 199/1000 | Loss: 0.00002732
Iteration 200/1000 | Loss: 0.00002732
Iteration 201/1000 | Loss: 0.00002732
Iteration 202/1000 | Loss: 0.00002732
Iteration 203/1000 | Loss: 0.00002732
Iteration 204/1000 | Loss: 0.00002732
Iteration 205/1000 | Loss: 0.00002732
Iteration 206/1000 | Loss: 0.00002732
Iteration 207/1000 | Loss: 0.00002732
Iteration 208/1000 | Loss: 0.00002732
Iteration 209/1000 | Loss: 0.00002732
Iteration 210/1000 | Loss: 0.00002732
Iteration 211/1000 | Loss: 0.00002732
Iteration 212/1000 | Loss: 0.00002732
Iteration 213/1000 | Loss: 0.00002731
Iteration 214/1000 | Loss: 0.00002731
Iteration 215/1000 | Loss: 0.00002731
Iteration 216/1000 | Loss: 0.00002731
Iteration 217/1000 | Loss: 0.00002731
Iteration 218/1000 | Loss: 0.00002731
Iteration 219/1000 | Loss: 0.00002731
Iteration 220/1000 | Loss: 0.00002731
Iteration 221/1000 | Loss: 0.00002731
Iteration 222/1000 | Loss: 0.00002731
Iteration 223/1000 | Loss: 0.00002731
Iteration 224/1000 | Loss: 0.00002731
Iteration 225/1000 | Loss: 0.00002731
Iteration 226/1000 | Loss: 0.00002731
Iteration 227/1000 | Loss: 0.00002731
Iteration 228/1000 | Loss: 0.00002731
Iteration 229/1000 | Loss: 0.00002731
Iteration 230/1000 | Loss: 0.00002731
Iteration 231/1000 | Loss: 0.00002731
Iteration 232/1000 | Loss: 0.00002730
Iteration 233/1000 | Loss: 0.00002730
Iteration 234/1000 | Loss: 0.00002730
Iteration 235/1000 | Loss: 0.00002730
Iteration 236/1000 | Loss: 0.00002730
Iteration 237/1000 | Loss: 0.00002730
Iteration 238/1000 | Loss: 0.00002730
Iteration 239/1000 | Loss: 0.00002730
Iteration 240/1000 | Loss: 0.00002730
Iteration 241/1000 | Loss: 0.00002730
Iteration 242/1000 | Loss: 0.00002730
Iteration 243/1000 | Loss: 0.00002730
Iteration 244/1000 | Loss: 0.00002730
Iteration 245/1000 | Loss: 0.00002730
Iteration 246/1000 | Loss: 0.00002729
Iteration 247/1000 | Loss: 0.00002729
Iteration 248/1000 | Loss: 0.00002729
Iteration 249/1000 | Loss: 0.00002729
Iteration 250/1000 | Loss: 0.00002729
Iteration 251/1000 | Loss: 0.00002729
Iteration 252/1000 | Loss: 0.00002729
Iteration 253/1000 | Loss: 0.00002729
Iteration 254/1000 | Loss: 0.00002729
Iteration 255/1000 | Loss: 0.00002729
Iteration 256/1000 | Loss: 0.00002729
Iteration 257/1000 | Loss: 0.00002729
Iteration 258/1000 | Loss: 0.00002729
Iteration 259/1000 | Loss: 0.00002729
Iteration 260/1000 | Loss: 0.00002729
Iteration 261/1000 | Loss: 0.00002728
Iteration 262/1000 | Loss: 0.00002728
Iteration 263/1000 | Loss: 0.00002728
Iteration 264/1000 | Loss: 0.00002728
Iteration 265/1000 | Loss: 0.00002728
Iteration 266/1000 | Loss: 0.00002728
Iteration 267/1000 | Loss: 0.00002728
Iteration 268/1000 | Loss: 0.00002728
Iteration 269/1000 | Loss: 0.00002728
Iteration 270/1000 | Loss: 0.00002728
Iteration 271/1000 | Loss: 0.00002728
Iteration 272/1000 | Loss: 0.00002728
Iteration 273/1000 | Loss: 0.00002728
Iteration 274/1000 | Loss: 0.00002728
Iteration 275/1000 | Loss: 0.00002728
Iteration 276/1000 | Loss: 0.00002728
Iteration 277/1000 | Loss: 0.00002728
Iteration 278/1000 | Loss: 0.00002728
Iteration 279/1000 | Loss: 0.00002727
Iteration 280/1000 | Loss: 0.00002727
Iteration 281/1000 | Loss: 0.00002727
Iteration 282/1000 | Loss: 0.00002727
Iteration 283/1000 | Loss: 0.00002727
Iteration 284/1000 | Loss: 0.00002727
Iteration 285/1000 | Loss: 0.00002727
Iteration 286/1000 | Loss: 0.00002727
Iteration 287/1000 | Loss: 0.00002727
Iteration 288/1000 | Loss: 0.00002727
Iteration 289/1000 | Loss: 0.00002727
Iteration 290/1000 | Loss: 0.00002727
Iteration 291/1000 | Loss: 0.00002727
Iteration 292/1000 | Loss: 0.00002727
Iteration 293/1000 | Loss: 0.00002727
Iteration 294/1000 | Loss: 0.00002727
Iteration 295/1000 | Loss: 0.00002727
Iteration 296/1000 | Loss: 0.00002727
Iteration 297/1000 | Loss: 0.00002727
Iteration 298/1000 | Loss: 0.00002727
Iteration 299/1000 | Loss: 0.00002727
Iteration 300/1000 | Loss: 0.00002727
Iteration 301/1000 | Loss: 0.00002727
Iteration 302/1000 | Loss: 0.00002727
Iteration 303/1000 | Loss: 0.00002727
Iteration 304/1000 | Loss: 0.00002727
Iteration 305/1000 | Loss: 0.00002727
Iteration 306/1000 | Loss: 0.00002727
Iteration 307/1000 | Loss: 0.00002727
Iteration 308/1000 | Loss: 0.00002727
Iteration 309/1000 | Loss: 0.00002727
Iteration 310/1000 | Loss: 0.00002727
Iteration 311/1000 | Loss: 0.00002727
Iteration 312/1000 | Loss: 0.00002727
Iteration 313/1000 | Loss: 0.00002727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 313. Stopping optimization.
Last 5 losses: [2.7266914912615903e-05, 2.7266914912615903e-05, 2.7266914912615903e-05, 2.7266914912615903e-05, 2.7266914912615903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7266914912615903e-05

Optimization complete. Final v2v error: 4.273188591003418 mm

Highest mean error: 6.333869457244873 mm for frame 119

Lowest mean error: 3.3421151638031006 mm for frame 160

Saving results

Total time: 75.51074290275574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_us_1560/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_us_1560/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058614
Iteration 2/25 | Loss: 0.00213794
Iteration 3/25 | Loss: 0.00131470
Iteration 4/25 | Loss: 0.00115052
Iteration 5/25 | Loss: 0.00116207
Iteration 6/25 | Loss: 0.00105976
Iteration 7/25 | Loss: 0.00098001
Iteration 8/25 | Loss: 0.00094440
Iteration 9/25 | Loss: 0.00093729
Iteration 10/25 | Loss: 0.00092758
Iteration 11/25 | Loss: 0.00092251
Iteration 12/25 | Loss: 0.00092125
Iteration 13/25 | Loss: 0.00092082
Iteration 14/25 | Loss: 0.00092071
Iteration 15/25 | Loss: 0.00092063
Iteration 16/25 | Loss: 0.00092063
Iteration 17/25 | Loss: 0.00092063
Iteration 18/25 | Loss: 0.00092063
Iteration 19/25 | Loss: 0.00092063
Iteration 20/25 | Loss: 0.00092063
Iteration 21/25 | Loss: 0.00092063
Iteration 22/25 | Loss: 0.00092063
Iteration 23/25 | Loss: 0.00092062
Iteration 24/25 | Loss: 0.00092062
Iteration 25/25 | Loss: 0.00092062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60489643
Iteration 2/25 | Loss: 0.00177009
Iteration 3/25 | Loss: 0.00169360
Iteration 4/25 | Loss: 0.00169360
Iteration 5/25 | Loss: 0.00169360
Iteration 6/25 | Loss: 0.00169360
Iteration 7/25 | Loss: 0.00169360
Iteration 8/25 | Loss: 0.00169360
Iteration 9/25 | Loss: 0.00169360
Iteration 10/25 | Loss: 0.00169360
Iteration 11/25 | Loss: 0.00169360
Iteration 12/25 | Loss: 0.00169360
Iteration 13/25 | Loss: 0.00169360
Iteration 14/25 | Loss: 0.00169360
Iteration 15/25 | Loss: 0.00169360
Iteration 16/25 | Loss: 0.00169360
Iteration 17/25 | Loss: 0.00169360
Iteration 18/25 | Loss: 0.00169360
Iteration 19/25 | Loss: 0.00169360
Iteration 20/25 | Loss: 0.00169360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016936007887125015, 0.0016936007887125015, 0.0016936007887125015, 0.0016936007887125015, 0.0016936007887125015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016936007887125015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169360
Iteration 2/1000 | Loss: 0.00004057
Iteration 3/1000 | Loss: 0.00003056
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002261
Iteration 6/1000 | Loss: 0.00002167
Iteration 7/1000 | Loss: 0.00019742
Iteration 8/1000 | Loss: 0.00002230
Iteration 9/1000 | Loss: 0.00002044
Iteration 10/1000 | Loss: 0.00001960
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001839
Iteration 13/1000 | Loss: 0.00001817
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001811
Iteration 16/1000 | Loss: 0.00001806
Iteration 17/1000 | Loss: 0.00001804
Iteration 18/1000 | Loss: 0.00001803
Iteration 19/1000 | Loss: 0.00001803
Iteration 20/1000 | Loss: 0.00010570
Iteration 21/1000 | Loss: 0.00001828
Iteration 22/1000 | Loss: 0.00001797
Iteration 23/1000 | Loss: 0.00001793
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001792
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001790
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001788
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001785
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001774
Iteration 52/1000 | Loss: 0.00001774
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001769
Iteration 65/1000 | Loss: 0.00001769
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001767
Iteration 70/1000 | Loss: 0.00001767
Iteration 71/1000 | Loss: 0.00001767
Iteration 72/1000 | Loss: 0.00001767
Iteration 73/1000 | Loss: 0.00001767
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001766
Iteration 76/1000 | Loss: 0.00001765
Iteration 77/1000 | Loss: 0.00001765
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001762
Iteration 83/1000 | Loss: 0.00001762
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001762
Iteration 87/1000 | Loss: 0.00001762
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001760
Iteration 93/1000 | Loss: 0.00001760
Iteration 94/1000 | Loss: 0.00001760
Iteration 95/1000 | Loss: 0.00001759
Iteration 96/1000 | Loss: 0.00001759
Iteration 97/1000 | Loss: 0.00001759
Iteration 98/1000 | Loss: 0.00001759
Iteration 99/1000 | Loss: 0.00001759
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001758
Iteration 103/1000 | Loss: 0.00001758
Iteration 104/1000 | Loss: 0.00001758
Iteration 105/1000 | Loss: 0.00001758
Iteration 106/1000 | Loss: 0.00001758
Iteration 107/1000 | Loss: 0.00001758
Iteration 108/1000 | Loss: 0.00001757
Iteration 109/1000 | Loss: 0.00001757
Iteration 110/1000 | Loss: 0.00001757
Iteration 111/1000 | Loss: 0.00001757
Iteration 112/1000 | Loss: 0.00001757
Iteration 113/1000 | Loss: 0.00001757
Iteration 114/1000 | Loss: 0.00001757
Iteration 115/1000 | Loss: 0.00001757
Iteration 116/1000 | Loss: 0.00001757
Iteration 117/1000 | Loss: 0.00001757
Iteration 118/1000 | Loss: 0.00001757
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001756
Iteration 122/1000 | Loss: 0.00001756
Iteration 123/1000 | Loss: 0.00001756
Iteration 124/1000 | Loss: 0.00001756
Iteration 125/1000 | Loss: 0.00001756
Iteration 126/1000 | Loss: 0.00001756
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00001756
Iteration 129/1000 | Loss: 0.00001756
Iteration 130/1000 | Loss: 0.00001756
Iteration 131/1000 | Loss: 0.00001756
Iteration 132/1000 | Loss: 0.00001756
Iteration 133/1000 | Loss: 0.00001756
Iteration 134/1000 | Loss: 0.00001756
Iteration 135/1000 | Loss: 0.00001755
Iteration 136/1000 | Loss: 0.00001755
Iteration 137/1000 | Loss: 0.00001755
Iteration 138/1000 | Loss: 0.00001755
Iteration 139/1000 | Loss: 0.00001755
Iteration 140/1000 | Loss: 0.00001755
Iteration 141/1000 | Loss: 0.00001755
Iteration 142/1000 | Loss: 0.00001755
Iteration 143/1000 | Loss: 0.00001755
Iteration 144/1000 | Loss: 0.00001755
Iteration 145/1000 | Loss: 0.00001755
Iteration 146/1000 | Loss: 0.00001755
Iteration 147/1000 | Loss: 0.00001755
Iteration 148/1000 | Loss: 0.00001755
Iteration 149/1000 | Loss: 0.00001755
Iteration 150/1000 | Loss: 0.00001755
Iteration 151/1000 | Loss: 0.00001755
Iteration 152/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.7552994904690422e-05, 1.7552994904690422e-05, 1.7552994904690422e-05, 1.7552994904690422e-05, 1.7552994904690422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7552994904690422e-05

Optimization complete. Final v2v error: 3.615051746368408 mm

Highest mean error: 4.8033671379089355 mm for frame 215

Lowest mean error: 3.183737277984619 mm for frame 94

Saving results

Total time: 71.09953141212463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020740
Iteration 2/25 | Loss: 0.00269548
Iteration 3/25 | Loss: 0.00196623
Iteration 4/25 | Loss: 0.00170925
Iteration 5/25 | Loss: 0.00161788
Iteration 6/25 | Loss: 0.00143959
Iteration 7/25 | Loss: 0.00148455
Iteration 8/25 | Loss: 0.00135110
Iteration 9/25 | Loss: 0.00127621
Iteration 10/25 | Loss: 0.00123282
Iteration 11/25 | Loss: 0.00120863
Iteration 12/25 | Loss: 0.00120783
Iteration 13/25 | Loss: 0.00120641
Iteration 14/25 | Loss: 0.00120532
Iteration 15/25 | Loss: 0.00120368
Iteration 16/25 | Loss: 0.00120327
Iteration 17/25 | Loss: 0.00120290
Iteration 18/25 | Loss: 0.00120507
Iteration 19/25 | Loss: 0.00120192
Iteration 20/25 | Loss: 0.00120084
Iteration 21/25 | Loss: 0.00120050
Iteration 22/25 | Loss: 0.00120316
Iteration 23/25 | Loss: 0.00120084
Iteration 24/25 | Loss: 0.00120057
Iteration 25/25 | Loss: 0.00119932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37446201
Iteration 2/25 | Loss: 0.00072535
Iteration 3/25 | Loss: 0.00072534
Iteration 4/25 | Loss: 0.00072534
Iteration 5/25 | Loss: 0.00072534
Iteration 6/25 | Loss: 0.00072534
Iteration 7/25 | Loss: 0.00072534
Iteration 8/25 | Loss: 0.00072534
Iteration 9/25 | Loss: 0.00072534
Iteration 10/25 | Loss: 0.00072534
Iteration 11/25 | Loss: 0.00072534
Iteration 12/25 | Loss: 0.00072534
Iteration 13/25 | Loss: 0.00072534
Iteration 14/25 | Loss: 0.00072534
Iteration 15/25 | Loss: 0.00072534
Iteration 16/25 | Loss: 0.00072534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007253402145579457, 0.0007253402145579457, 0.0007253402145579457, 0.0007253402145579457, 0.0007253402145579457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007253402145579457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072534
Iteration 2/1000 | Loss: 0.00004158
Iteration 3/1000 | Loss: 0.00002942
Iteration 4/1000 | Loss: 0.00002680
Iteration 5/1000 | Loss: 0.00002547
Iteration 6/1000 | Loss: 0.00002452
Iteration 7/1000 | Loss: 0.00002394
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00013049
Iteration 10/1000 | Loss: 0.00048288
Iteration 11/1000 | Loss: 0.00050422
Iteration 12/1000 | Loss: 0.00002632
Iteration 13/1000 | Loss: 0.00002346
Iteration 14/1000 | Loss: 0.00002296
Iteration 15/1000 | Loss: 0.00002269
Iteration 16/1000 | Loss: 0.00002261
Iteration 17/1000 | Loss: 0.00002260
Iteration 18/1000 | Loss: 0.00002260
Iteration 19/1000 | Loss: 0.00002259
Iteration 20/1000 | Loss: 0.00002258
Iteration 21/1000 | Loss: 0.00002257
Iteration 22/1000 | Loss: 0.00002257
Iteration 23/1000 | Loss: 0.00002257
Iteration 24/1000 | Loss: 0.00002257
Iteration 25/1000 | Loss: 0.00002257
Iteration 26/1000 | Loss: 0.00002257
Iteration 27/1000 | Loss: 0.00002256
Iteration 28/1000 | Loss: 0.00002256
Iteration 29/1000 | Loss: 0.00002255
Iteration 30/1000 | Loss: 0.00002253
Iteration 31/1000 | Loss: 0.00002253
Iteration 32/1000 | Loss: 0.00002252
Iteration 33/1000 | Loss: 0.00002252
Iteration 34/1000 | Loss: 0.00002251
Iteration 35/1000 | Loss: 0.00002251
Iteration 36/1000 | Loss: 0.00002251
Iteration 37/1000 | Loss: 0.00002250
Iteration 38/1000 | Loss: 0.00002249
Iteration 39/1000 | Loss: 0.00002248
Iteration 40/1000 | Loss: 0.00002247
Iteration 41/1000 | Loss: 0.00014112
Iteration 42/1000 | Loss: 0.00002815
Iteration 43/1000 | Loss: 0.00002601
Iteration 44/1000 | Loss: 0.00005169
Iteration 45/1000 | Loss: 0.00002251
Iteration 46/1000 | Loss: 0.00002240
Iteration 47/1000 | Loss: 0.00002237
Iteration 48/1000 | Loss: 0.00002234
Iteration 49/1000 | Loss: 0.00002234
Iteration 50/1000 | Loss: 0.00002234
Iteration 51/1000 | Loss: 0.00002233
Iteration 52/1000 | Loss: 0.00002233
Iteration 53/1000 | Loss: 0.00002233
Iteration 54/1000 | Loss: 0.00002233
Iteration 55/1000 | Loss: 0.00002233
Iteration 56/1000 | Loss: 0.00002233
Iteration 57/1000 | Loss: 0.00002233
Iteration 58/1000 | Loss: 0.00002233
Iteration 59/1000 | Loss: 0.00002233
Iteration 60/1000 | Loss: 0.00002233
Iteration 61/1000 | Loss: 0.00002232
Iteration 62/1000 | Loss: 0.00002232
Iteration 63/1000 | Loss: 0.00002232
Iteration 64/1000 | Loss: 0.00002232
Iteration 65/1000 | Loss: 0.00002232
Iteration 66/1000 | Loss: 0.00002231
Iteration 67/1000 | Loss: 0.00002231
Iteration 68/1000 | Loss: 0.00002231
Iteration 69/1000 | Loss: 0.00002231
Iteration 70/1000 | Loss: 0.00002230
Iteration 71/1000 | Loss: 0.00002230
Iteration 72/1000 | Loss: 0.00002230
Iteration 73/1000 | Loss: 0.00002230
Iteration 74/1000 | Loss: 0.00002229
Iteration 75/1000 | Loss: 0.00002229
Iteration 76/1000 | Loss: 0.00002229
Iteration 77/1000 | Loss: 0.00002229
Iteration 78/1000 | Loss: 0.00002229
Iteration 79/1000 | Loss: 0.00002229
Iteration 80/1000 | Loss: 0.00002229
Iteration 81/1000 | Loss: 0.00002229
Iteration 82/1000 | Loss: 0.00002228
Iteration 83/1000 | Loss: 0.00002228
Iteration 84/1000 | Loss: 0.00002228
Iteration 85/1000 | Loss: 0.00002228
Iteration 86/1000 | Loss: 0.00002228
Iteration 87/1000 | Loss: 0.00002228
Iteration 88/1000 | Loss: 0.00002228
Iteration 89/1000 | Loss: 0.00002228
Iteration 90/1000 | Loss: 0.00002227
Iteration 91/1000 | Loss: 0.00002227
Iteration 92/1000 | Loss: 0.00002227
Iteration 93/1000 | Loss: 0.00002227
Iteration 94/1000 | Loss: 0.00002226
Iteration 95/1000 | Loss: 0.00002226
Iteration 96/1000 | Loss: 0.00002226
Iteration 97/1000 | Loss: 0.00002226
Iteration 98/1000 | Loss: 0.00002226
Iteration 99/1000 | Loss: 0.00002226
Iteration 100/1000 | Loss: 0.00002226
Iteration 101/1000 | Loss: 0.00002226
Iteration 102/1000 | Loss: 0.00002225
Iteration 103/1000 | Loss: 0.00002225
Iteration 104/1000 | Loss: 0.00002225
Iteration 105/1000 | Loss: 0.00002225
Iteration 106/1000 | Loss: 0.00002225
Iteration 107/1000 | Loss: 0.00002224
Iteration 108/1000 | Loss: 0.00002224
Iteration 109/1000 | Loss: 0.00002224
Iteration 110/1000 | Loss: 0.00002224
Iteration 111/1000 | Loss: 0.00002224
Iteration 112/1000 | Loss: 0.00002224
Iteration 113/1000 | Loss: 0.00002224
Iteration 114/1000 | Loss: 0.00002224
Iteration 115/1000 | Loss: 0.00002224
Iteration 116/1000 | Loss: 0.00002224
Iteration 117/1000 | Loss: 0.00002224
Iteration 118/1000 | Loss: 0.00002224
Iteration 119/1000 | Loss: 0.00002224
Iteration 120/1000 | Loss: 0.00002224
Iteration 121/1000 | Loss: 0.00002224
Iteration 122/1000 | Loss: 0.00002224
Iteration 123/1000 | Loss: 0.00002223
Iteration 124/1000 | Loss: 0.00002223
Iteration 125/1000 | Loss: 0.00002223
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002223
Iteration 128/1000 | Loss: 0.00002223
Iteration 129/1000 | Loss: 0.00002223
Iteration 130/1000 | Loss: 0.00002223
Iteration 131/1000 | Loss: 0.00002223
Iteration 132/1000 | Loss: 0.00002223
Iteration 133/1000 | Loss: 0.00002223
Iteration 134/1000 | Loss: 0.00002223
Iteration 135/1000 | Loss: 0.00002223
Iteration 136/1000 | Loss: 0.00002223
Iteration 137/1000 | Loss: 0.00002223
Iteration 138/1000 | Loss: 0.00002223
Iteration 139/1000 | Loss: 0.00002223
Iteration 140/1000 | Loss: 0.00002223
Iteration 141/1000 | Loss: 0.00002223
Iteration 142/1000 | Loss: 0.00002223
Iteration 143/1000 | Loss: 0.00002223
Iteration 144/1000 | Loss: 0.00002223
Iteration 145/1000 | Loss: 0.00002223
Iteration 146/1000 | Loss: 0.00002223
Iteration 147/1000 | Loss: 0.00002223
Iteration 148/1000 | Loss: 0.00002223
Iteration 149/1000 | Loss: 0.00002223
Iteration 150/1000 | Loss: 0.00002223
Iteration 151/1000 | Loss: 0.00002223
Iteration 152/1000 | Loss: 0.00002223
Iteration 153/1000 | Loss: 0.00002223
Iteration 154/1000 | Loss: 0.00002223
Iteration 155/1000 | Loss: 0.00002223
Iteration 156/1000 | Loss: 0.00002223
Iteration 157/1000 | Loss: 0.00002223
Iteration 158/1000 | Loss: 0.00002223
Iteration 159/1000 | Loss: 0.00002223
Iteration 160/1000 | Loss: 0.00002223
Iteration 161/1000 | Loss: 0.00002223
Iteration 162/1000 | Loss: 0.00002223
Iteration 163/1000 | Loss: 0.00002223
Iteration 164/1000 | Loss: 0.00002223
Iteration 165/1000 | Loss: 0.00002223
Iteration 166/1000 | Loss: 0.00002223
Iteration 167/1000 | Loss: 0.00002223
Iteration 168/1000 | Loss: 0.00002223
Iteration 169/1000 | Loss: 0.00002223
Iteration 170/1000 | Loss: 0.00002223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.2225018255994655e-05, 2.2225018255994655e-05, 2.2225018255994655e-05, 2.2225018255994655e-05, 2.2225018255994655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2225018255994655e-05

Optimization complete. Final v2v error: 3.9572222232818604 mm

Highest mean error: 4.3141937255859375 mm for frame 147

Lowest mean error: 3.5305919647216797 mm for frame 58

Saving results

Total time: 97.00946927070618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436782
Iteration 2/25 | Loss: 0.00125885
Iteration 3/25 | Loss: 0.00115546
Iteration 4/25 | Loss: 0.00113890
Iteration 5/25 | Loss: 0.00113494
Iteration 6/25 | Loss: 0.00113483
Iteration 7/25 | Loss: 0.00113483
Iteration 8/25 | Loss: 0.00113483
Iteration 9/25 | Loss: 0.00113483
Iteration 10/25 | Loss: 0.00113483
Iteration 11/25 | Loss: 0.00113483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011348348343744874, 0.0011348348343744874, 0.0011348348343744874, 0.0011348348343744874, 0.0011348348343744874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011348348343744874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35067666
Iteration 2/25 | Loss: 0.00068605
Iteration 3/25 | Loss: 0.00068605
Iteration 4/25 | Loss: 0.00068605
Iteration 5/25 | Loss: 0.00068605
Iteration 6/25 | Loss: 0.00068605
Iteration 7/25 | Loss: 0.00068605
Iteration 8/25 | Loss: 0.00068605
Iteration 9/25 | Loss: 0.00068605
Iteration 10/25 | Loss: 0.00068605
Iteration 11/25 | Loss: 0.00068605
Iteration 12/25 | Loss: 0.00068605
Iteration 13/25 | Loss: 0.00068604
Iteration 14/25 | Loss: 0.00068605
Iteration 15/25 | Loss: 0.00068604
Iteration 16/25 | Loss: 0.00068604
Iteration 17/25 | Loss: 0.00068604
Iteration 18/25 | Loss: 0.00068605
Iteration 19/25 | Loss: 0.00068605
Iteration 20/25 | Loss: 0.00068605
Iteration 21/25 | Loss: 0.00068604
Iteration 22/25 | Loss: 0.00068604
Iteration 23/25 | Loss: 0.00068605
Iteration 24/25 | Loss: 0.00068605
Iteration 25/25 | Loss: 0.00068605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068605
Iteration 2/1000 | Loss: 0.00002622
Iteration 3/1000 | Loss: 0.00002039
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001694
Iteration 8/1000 | Loss: 0.00001658
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00001605
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001566
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001559
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001554
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001554
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001553
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001551
Iteration 31/1000 | Loss: 0.00001551
Iteration 32/1000 | Loss: 0.00001550
Iteration 33/1000 | Loss: 0.00001550
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001546
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001545
Iteration 46/1000 | Loss: 0.00001545
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001543
Iteration 51/1000 | Loss: 0.00001543
Iteration 52/1000 | Loss: 0.00001542
Iteration 53/1000 | Loss: 0.00001542
Iteration 54/1000 | Loss: 0.00001542
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001541
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001540
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001540
Iteration 68/1000 | Loss: 0.00001540
Iteration 69/1000 | Loss: 0.00001540
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001539
Iteration 77/1000 | Loss: 0.00001539
Iteration 78/1000 | Loss: 0.00001539
Iteration 79/1000 | Loss: 0.00001539
Iteration 80/1000 | Loss: 0.00001539
Iteration 81/1000 | Loss: 0.00001539
Iteration 82/1000 | Loss: 0.00001539
Iteration 83/1000 | Loss: 0.00001538
Iteration 84/1000 | Loss: 0.00001538
Iteration 85/1000 | Loss: 0.00001538
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001537
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001537
Iteration 93/1000 | Loss: 0.00001536
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001536
Iteration 101/1000 | Loss: 0.00001536
Iteration 102/1000 | Loss: 0.00001536
Iteration 103/1000 | Loss: 0.00001536
Iteration 104/1000 | Loss: 0.00001536
Iteration 105/1000 | Loss: 0.00001536
Iteration 106/1000 | Loss: 0.00001536
Iteration 107/1000 | Loss: 0.00001536
Iteration 108/1000 | Loss: 0.00001536
Iteration 109/1000 | Loss: 0.00001536
Iteration 110/1000 | Loss: 0.00001535
Iteration 111/1000 | Loss: 0.00001535
Iteration 112/1000 | Loss: 0.00001535
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001534
Iteration 115/1000 | Loss: 0.00001534
Iteration 116/1000 | Loss: 0.00001534
Iteration 117/1000 | Loss: 0.00001534
Iteration 118/1000 | Loss: 0.00001534
Iteration 119/1000 | Loss: 0.00001534
Iteration 120/1000 | Loss: 0.00001534
Iteration 121/1000 | Loss: 0.00001534
Iteration 122/1000 | Loss: 0.00001534
Iteration 123/1000 | Loss: 0.00001534
Iteration 124/1000 | Loss: 0.00001534
Iteration 125/1000 | Loss: 0.00001533
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001533
Iteration 130/1000 | Loss: 0.00001533
Iteration 131/1000 | Loss: 0.00001533
Iteration 132/1000 | Loss: 0.00001533
Iteration 133/1000 | Loss: 0.00001533
Iteration 134/1000 | Loss: 0.00001533
Iteration 135/1000 | Loss: 0.00001533
Iteration 136/1000 | Loss: 0.00001533
Iteration 137/1000 | Loss: 0.00001533
Iteration 138/1000 | Loss: 0.00001533
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.5329142115660943e-05, 1.5329142115660943e-05, 1.5329142115660943e-05, 1.5329142115660943e-05, 1.5329142115660943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5329142115660943e-05

Optimization complete. Final v2v error: 3.295436382293701 mm

Highest mean error: 4.358944892883301 mm for frame 230

Lowest mean error: 2.7484328746795654 mm for frame 89

Saving results

Total time: 39.51471495628357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813115
Iteration 2/25 | Loss: 0.00170305
Iteration 3/25 | Loss: 0.00122007
Iteration 4/25 | Loss: 0.00116951
Iteration 5/25 | Loss: 0.00116547
Iteration 6/25 | Loss: 0.00116547
Iteration 7/25 | Loss: 0.00116547
Iteration 8/25 | Loss: 0.00116547
Iteration 9/25 | Loss: 0.00116547
Iteration 10/25 | Loss: 0.00116547
Iteration 11/25 | Loss: 0.00116547
Iteration 12/25 | Loss: 0.00116547
Iteration 13/25 | Loss: 0.00116547
Iteration 14/25 | Loss: 0.00116547
Iteration 15/25 | Loss: 0.00116547
Iteration 16/25 | Loss: 0.00116547
Iteration 17/25 | Loss: 0.00116547
Iteration 18/25 | Loss: 0.00116547
Iteration 19/25 | Loss: 0.00116547
Iteration 20/25 | Loss: 0.00116547
Iteration 21/25 | Loss: 0.00116547
Iteration 22/25 | Loss: 0.00116547
Iteration 23/25 | Loss: 0.00116547
Iteration 24/25 | Loss: 0.00116547
Iteration 25/25 | Loss: 0.00116547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34186018
Iteration 2/25 | Loss: 0.00063499
Iteration 3/25 | Loss: 0.00063499
Iteration 4/25 | Loss: 0.00063499
Iteration 5/25 | Loss: 0.00063499
Iteration 6/25 | Loss: 0.00063499
Iteration 7/25 | Loss: 0.00063499
Iteration 8/25 | Loss: 0.00063499
Iteration 9/25 | Loss: 0.00063499
Iteration 10/25 | Loss: 0.00063499
Iteration 11/25 | Loss: 0.00063499
Iteration 12/25 | Loss: 0.00063499
Iteration 13/25 | Loss: 0.00063499
Iteration 14/25 | Loss: 0.00063499
Iteration 15/25 | Loss: 0.00063499
Iteration 16/25 | Loss: 0.00063499
Iteration 17/25 | Loss: 0.00063499
Iteration 18/25 | Loss: 0.00063499
Iteration 19/25 | Loss: 0.00063499
Iteration 20/25 | Loss: 0.00063499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006349862087517977, 0.0006349862087517977, 0.0006349862087517977, 0.0006349862087517977, 0.0006349862087517977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006349862087517977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063499
Iteration 2/1000 | Loss: 0.00002957
Iteration 3/1000 | Loss: 0.00002194
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001951
Iteration 7/1000 | Loss: 0.00001914
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001857
Iteration 11/1000 | Loss: 0.00001836
Iteration 12/1000 | Loss: 0.00001825
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001807
Iteration 18/1000 | Loss: 0.00001806
Iteration 19/1000 | Loss: 0.00001806
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001803
Iteration 24/1000 | Loss: 0.00001802
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001797
Iteration 30/1000 | Loss: 0.00001795
Iteration 31/1000 | Loss: 0.00001795
Iteration 32/1000 | Loss: 0.00001794
Iteration 33/1000 | Loss: 0.00001794
Iteration 34/1000 | Loss: 0.00001793
Iteration 35/1000 | Loss: 0.00001792
Iteration 36/1000 | Loss: 0.00001792
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00001791
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001789
Iteration 41/1000 | Loss: 0.00001789
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001786
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001784
Iteration 55/1000 | Loss: 0.00001784
Iteration 56/1000 | Loss: 0.00001783
Iteration 57/1000 | Loss: 0.00001783
Iteration 58/1000 | Loss: 0.00001783
Iteration 59/1000 | Loss: 0.00001783
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001782
Iteration 66/1000 | Loss: 0.00001782
Iteration 67/1000 | Loss: 0.00001782
Iteration 68/1000 | Loss: 0.00001782
Iteration 69/1000 | Loss: 0.00001781
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00001780
Iteration 75/1000 | Loss: 0.00001780
Iteration 76/1000 | Loss: 0.00001780
Iteration 77/1000 | Loss: 0.00001780
Iteration 78/1000 | Loss: 0.00001780
Iteration 79/1000 | Loss: 0.00001780
Iteration 80/1000 | Loss: 0.00001780
Iteration 81/1000 | Loss: 0.00001780
Iteration 82/1000 | Loss: 0.00001780
Iteration 83/1000 | Loss: 0.00001780
Iteration 84/1000 | Loss: 0.00001779
Iteration 85/1000 | Loss: 0.00001779
Iteration 86/1000 | Loss: 0.00001779
Iteration 87/1000 | Loss: 0.00001779
Iteration 88/1000 | Loss: 0.00001779
Iteration 89/1000 | Loss: 0.00001779
Iteration 90/1000 | Loss: 0.00001779
Iteration 91/1000 | Loss: 0.00001779
Iteration 92/1000 | Loss: 0.00001779
Iteration 93/1000 | Loss: 0.00001779
Iteration 94/1000 | Loss: 0.00001779
Iteration 95/1000 | Loss: 0.00001779
Iteration 96/1000 | Loss: 0.00001779
Iteration 97/1000 | Loss: 0.00001779
Iteration 98/1000 | Loss: 0.00001778
Iteration 99/1000 | Loss: 0.00001778
Iteration 100/1000 | Loss: 0.00001778
Iteration 101/1000 | Loss: 0.00001778
Iteration 102/1000 | Loss: 0.00001778
Iteration 103/1000 | Loss: 0.00001778
Iteration 104/1000 | Loss: 0.00001778
Iteration 105/1000 | Loss: 0.00001778
Iteration 106/1000 | Loss: 0.00001778
Iteration 107/1000 | Loss: 0.00001778
Iteration 108/1000 | Loss: 0.00001778
Iteration 109/1000 | Loss: 0.00001778
Iteration 110/1000 | Loss: 0.00001778
Iteration 111/1000 | Loss: 0.00001777
Iteration 112/1000 | Loss: 0.00001777
Iteration 113/1000 | Loss: 0.00001777
Iteration 114/1000 | Loss: 0.00001777
Iteration 115/1000 | Loss: 0.00001777
Iteration 116/1000 | Loss: 0.00001776
Iteration 117/1000 | Loss: 0.00001776
Iteration 118/1000 | Loss: 0.00001776
Iteration 119/1000 | Loss: 0.00001776
Iteration 120/1000 | Loss: 0.00001775
Iteration 121/1000 | Loss: 0.00001775
Iteration 122/1000 | Loss: 0.00001775
Iteration 123/1000 | Loss: 0.00001775
Iteration 124/1000 | Loss: 0.00001775
Iteration 125/1000 | Loss: 0.00001774
Iteration 126/1000 | Loss: 0.00001774
Iteration 127/1000 | Loss: 0.00001774
Iteration 128/1000 | Loss: 0.00001774
Iteration 129/1000 | Loss: 0.00001774
Iteration 130/1000 | Loss: 0.00001774
Iteration 131/1000 | Loss: 0.00001774
Iteration 132/1000 | Loss: 0.00001774
Iteration 133/1000 | Loss: 0.00001774
Iteration 134/1000 | Loss: 0.00001774
Iteration 135/1000 | Loss: 0.00001774
Iteration 136/1000 | Loss: 0.00001774
Iteration 137/1000 | Loss: 0.00001774
Iteration 138/1000 | Loss: 0.00001773
Iteration 139/1000 | Loss: 0.00001773
Iteration 140/1000 | Loss: 0.00001773
Iteration 141/1000 | Loss: 0.00001773
Iteration 142/1000 | Loss: 0.00001773
Iteration 143/1000 | Loss: 0.00001773
Iteration 144/1000 | Loss: 0.00001773
Iteration 145/1000 | Loss: 0.00001773
Iteration 146/1000 | Loss: 0.00001773
Iteration 147/1000 | Loss: 0.00001773
Iteration 148/1000 | Loss: 0.00001773
Iteration 149/1000 | Loss: 0.00001773
Iteration 150/1000 | Loss: 0.00001773
Iteration 151/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.773256553860847e-05, 1.773256553860847e-05, 1.773256553860847e-05, 1.773256553860847e-05, 1.773256553860847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.773256553860847e-05

Optimization complete. Final v2v error: 3.50469970703125 mm

Highest mean error: 3.5838253498077393 mm for frame 69

Lowest mean error: 3.128901481628418 mm for frame 4

Saving results

Total time: 40.38512420654297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798106
Iteration 2/25 | Loss: 0.00154823
Iteration 3/25 | Loss: 0.00132124
Iteration 4/25 | Loss: 0.00117700
Iteration 5/25 | Loss: 0.00116399
Iteration 6/25 | Loss: 0.00115011
Iteration 7/25 | Loss: 0.00114987
Iteration 8/25 | Loss: 0.00113841
Iteration 9/25 | Loss: 0.00113536
Iteration 10/25 | Loss: 0.00113660
Iteration 11/25 | Loss: 0.00113767
Iteration 12/25 | Loss: 0.00113945
Iteration 13/25 | Loss: 0.00114127
Iteration 14/25 | Loss: 0.00113022
Iteration 15/25 | Loss: 0.00113182
Iteration 16/25 | Loss: 0.00113012
Iteration 17/25 | Loss: 0.00112935
Iteration 18/25 | Loss: 0.00112935
Iteration 19/25 | Loss: 0.00112935
Iteration 20/25 | Loss: 0.00112935
Iteration 21/25 | Loss: 0.00112935
Iteration 22/25 | Loss: 0.00112935
Iteration 23/25 | Loss: 0.00112935
Iteration 24/25 | Loss: 0.00112935
Iteration 25/25 | Loss: 0.00112935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82965171
Iteration 2/25 | Loss: 0.00126283
Iteration 3/25 | Loss: 0.00092479
Iteration 4/25 | Loss: 0.00092479
Iteration 5/25 | Loss: 0.00092479
Iteration 6/25 | Loss: 0.00092479
Iteration 7/25 | Loss: 0.00092479
Iteration 8/25 | Loss: 0.00092479
Iteration 9/25 | Loss: 0.00092479
Iteration 10/25 | Loss: 0.00092479
Iteration 11/25 | Loss: 0.00092479
Iteration 12/25 | Loss: 0.00092479
Iteration 13/25 | Loss: 0.00092479
Iteration 14/25 | Loss: 0.00092479
Iteration 15/25 | Loss: 0.00092479
Iteration 16/25 | Loss: 0.00092479
Iteration 17/25 | Loss: 0.00092479
Iteration 18/25 | Loss: 0.00092479
Iteration 19/25 | Loss: 0.00092479
Iteration 20/25 | Loss: 0.00092479
Iteration 21/25 | Loss: 0.00092479
Iteration 22/25 | Loss: 0.00092479
Iteration 23/25 | Loss: 0.00092479
Iteration 24/25 | Loss: 0.00092479
Iteration 25/25 | Loss: 0.00092479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092479
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00039391
Iteration 4/1000 | Loss: 0.00011481
Iteration 5/1000 | Loss: 0.00004950
Iteration 6/1000 | Loss: 0.00006305
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001907
Iteration 10/1000 | Loss: 0.00001856
Iteration 11/1000 | Loss: 0.00001813
Iteration 12/1000 | Loss: 0.00001771
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001718
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00006234
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00003723
Iteration 19/1000 | Loss: 0.00001683
Iteration 20/1000 | Loss: 0.00001672
Iteration 21/1000 | Loss: 0.00001670
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001667
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001663
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001651
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001649
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001648
Iteration 33/1000 | Loss: 0.00001647
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001644
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001643
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001643
Iteration 44/1000 | Loss: 0.00001643
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001642
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001642
Iteration 49/1000 | Loss: 0.00001642
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001641
Iteration 52/1000 | Loss: 0.00001640
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001640
Iteration 56/1000 | Loss: 0.00001640
Iteration 57/1000 | Loss: 0.00001640
Iteration 58/1000 | Loss: 0.00001640
Iteration 59/1000 | Loss: 0.00001639
Iteration 60/1000 | Loss: 0.00001639
Iteration 61/1000 | Loss: 0.00001639
Iteration 62/1000 | Loss: 0.00001639
Iteration 63/1000 | Loss: 0.00001639
Iteration 64/1000 | Loss: 0.00001639
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001638
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001637
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001635
Iteration 72/1000 | Loss: 0.00001634
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001631
Iteration 88/1000 | Loss: 0.00001631
Iteration 89/1000 | Loss: 0.00001631
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001628
Iteration 102/1000 | Loss: 0.00001628
Iteration 103/1000 | Loss: 0.00001628
Iteration 104/1000 | Loss: 0.00001628
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001627
Iteration 109/1000 | Loss: 0.00001627
Iteration 110/1000 | Loss: 0.00001627
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001626
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001625
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001624
Iteration 127/1000 | Loss: 0.00001624
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001623
Iteration 133/1000 | Loss: 0.00001623
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001621
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001620
Iteration 154/1000 | Loss: 0.00001620
Iteration 155/1000 | Loss: 0.00001620
Iteration 156/1000 | Loss: 0.00001620
Iteration 157/1000 | Loss: 0.00001620
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001620
Iteration 160/1000 | Loss: 0.00001620
Iteration 161/1000 | Loss: 0.00001620
Iteration 162/1000 | Loss: 0.00001620
Iteration 163/1000 | Loss: 0.00001620
Iteration 164/1000 | Loss: 0.00001620
Iteration 165/1000 | Loss: 0.00001620
Iteration 166/1000 | Loss: 0.00001620
Iteration 167/1000 | Loss: 0.00001620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.6201338439714164e-05, 1.6201338439714164e-05, 1.6201338439714164e-05, 1.6201338439714164e-05, 1.6201338439714164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6201338439714164e-05

Optimization complete. Final v2v error: 3.4105770587921143 mm

Highest mean error: 3.828530788421631 mm for frame 104

Lowest mean error: 3.0811829566955566 mm for frame 135

Saving results

Total time: 69.7794897556305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062353
Iteration 2/25 | Loss: 0.00360407
Iteration 3/25 | Loss: 0.00237334
Iteration 4/25 | Loss: 0.00182234
Iteration 5/25 | Loss: 0.00169404
Iteration 6/25 | Loss: 0.00135595
Iteration 7/25 | Loss: 0.00130036
Iteration 8/25 | Loss: 0.00125341
Iteration 9/25 | Loss: 0.00115128
Iteration 10/25 | Loss: 0.00111783
Iteration 11/25 | Loss: 0.00110910
Iteration 12/25 | Loss: 0.00110208
Iteration 13/25 | Loss: 0.00109932
Iteration 14/25 | Loss: 0.00109859
Iteration 15/25 | Loss: 0.00109835
Iteration 16/25 | Loss: 0.00109834
Iteration 17/25 | Loss: 0.00109834
Iteration 18/25 | Loss: 0.00109833
Iteration 19/25 | Loss: 0.00109833
Iteration 20/25 | Loss: 0.00109832
Iteration 21/25 | Loss: 0.00109832
Iteration 22/25 | Loss: 0.00109832
Iteration 23/25 | Loss: 0.00109832
Iteration 24/25 | Loss: 0.00109831
Iteration 25/25 | Loss: 0.00109831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31335151
Iteration 2/25 | Loss: 0.00089559
Iteration 3/25 | Loss: 0.00089559
Iteration 4/25 | Loss: 0.00089559
Iteration 5/25 | Loss: 0.00089559
Iteration 6/25 | Loss: 0.00089559
Iteration 7/25 | Loss: 0.00089559
Iteration 8/25 | Loss: 0.00089559
Iteration 9/25 | Loss: 0.00089559
Iteration 10/25 | Loss: 0.00089559
Iteration 11/25 | Loss: 0.00089559
Iteration 12/25 | Loss: 0.00089559
Iteration 13/25 | Loss: 0.00089559
Iteration 14/25 | Loss: 0.00089559
Iteration 15/25 | Loss: 0.00089559
Iteration 16/25 | Loss: 0.00089559
Iteration 17/25 | Loss: 0.00089559
Iteration 18/25 | Loss: 0.00089559
Iteration 19/25 | Loss: 0.00089559
Iteration 20/25 | Loss: 0.00089559
Iteration 21/25 | Loss: 0.00089559
Iteration 22/25 | Loss: 0.00089559
Iteration 23/25 | Loss: 0.00089559
Iteration 24/25 | Loss: 0.00089559
Iteration 25/25 | Loss: 0.00089559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089559
Iteration 2/1000 | Loss: 0.00003261
Iteration 3/1000 | Loss: 0.00021419
Iteration 4/1000 | Loss: 0.00003343
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00001866
Iteration 7/1000 | Loss: 0.00001733
Iteration 8/1000 | Loss: 0.00001670
Iteration 9/1000 | Loss: 0.00001615
Iteration 10/1000 | Loss: 0.00018638
Iteration 11/1000 | Loss: 0.00001587
Iteration 12/1000 | Loss: 0.00001534
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001480
Iteration 15/1000 | Loss: 0.00001467
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001429
Iteration 19/1000 | Loss: 0.00001426
Iteration 20/1000 | Loss: 0.00001426
Iteration 21/1000 | Loss: 0.00001425
Iteration 22/1000 | Loss: 0.00001423
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001412
Iteration 26/1000 | Loss: 0.00001411
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001408
Iteration 30/1000 | Loss: 0.00001408
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00001408
Iteration 33/1000 | Loss: 0.00001408
Iteration 34/1000 | Loss: 0.00001407
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001406
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00022660
Iteration 45/1000 | Loss: 0.00022660
Iteration 46/1000 | Loss: 0.00002737
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001427
Iteration 49/1000 | Loss: 0.00001404
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001401
Iteration 52/1000 | Loss: 0.00001399
Iteration 53/1000 | Loss: 0.00001398
Iteration 54/1000 | Loss: 0.00001398
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001397
Iteration 62/1000 | Loss: 0.00001397
Iteration 63/1000 | Loss: 0.00001397
Iteration 64/1000 | Loss: 0.00001397
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001393
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001390
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001389
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001388
Iteration 88/1000 | Loss: 0.00001388
Iteration 89/1000 | Loss: 0.00001388
Iteration 90/1000 | Loss: 0.00001387
Iteration 91/1000 | Loss: 0.00001387
Iteration 92/1000 | Loss: 0.00001386
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001384
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001383
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001380
Iteration 111/1000 | Loss: 0.00001380
Iteration 112/1000 | Loss: 0.00001380
Iteration 113/1000 | Loss: 0.00001380
Iteration 114/1000 | Loss: 0.00001380
Iteration 115/1000 | Loss: 0.00001380
Iteration 116/1000 | Loss: 0.00001380
Iteration 117/1000 | Loss: 0.00001380
Iteration 118/1000 | Loss: 0.00001380
Iteration 119/1000 | Loss: 0.00001380
Iteration 120/1000 | Loss: 0.00001380
Iteration 121/1000 | Loss: 0.00001380
Iteration 122/1000 | Loss: 0.00001380
Iteration 123/1000 | Loss: 0.00001380
Iteration 124/1000 | Loss: 0.00001380
Iteration 125/1000 | Loss: 0.00001380
Iteration 126/1000 | Loss: 0.00001380
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.3801181012240704e-05, 1.3801181012240704e-05, 1.3801181012240704e-05, 1.3801181012240704e-05, 1.3801181012240704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3801181012240704e-05

Optimization complete. Final v2v error: 3.1367502212524414 mm

Highest mean error: 3.696469306945801 mm for frame 76

Lowest mean error: 2.886946439743042 mm for frame 158

Saving results

Total time: 68.78283381462097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013725
Iteration 2/25 | Loss: 0.01013725
Iteration 3/25 | Loss: 0.01013725
Iteration 4/25 | Loss: 0.01013725
Iteration 5/25 | Loss: 0.01013725
Iteration 6/25 | Loss: 0.01013725
Iteration 7/25 | Loss: 0.01013724
Iteration 8/25 | Loss: 0.01013724
Iteration 9/25 | Loss: 0.01013724
Iteration 10/25 | Loss: 0.01013724
Iteration 11/25 | Loss: 0.01013724
Iteration 12/25 | Loss: 0.01013723
Iteration 13/25 | Loss: 0.01013723
Iteration 14/25 | Loss: 0.01013723
Iteration 15/25 | Loss: 0.01013723
Iteration 16/25 | Loss: 0.01013723
Iteration 17/25 | Loss: 0.01013723
Iteration 18/25 | Loss: 0.01013723
Iteration 19/25 | Loss: 0.01013723
Iteration 20/25 | Loss: 0.01013723
Iteration 21/25 | Loss: 0.01013722
Iteration 22/25 | Loss: 0.01013722
Iteration 23/25 | Loss: 0.01013722
Iteration 24/25 | Loss: 0.01013722
Iteration 25/25 | Loss: 0.01013722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51858759
Iteration 2/25 | Loss: 0.13409907
Iteration 3/25 | Loss: 0.13323450
Iteration 4/25 | Loss: 0.13670553
Iteration 5/25 | Loss: 0.13416605
Iteration 6/25 | Loss: 0.13653260
Iteration 7/25 | Loss: 0.13287431
Iteration 8/25 | Loss: 0.13338383
Iteration 9/25 | Loss: 0.13288851
Iteration 10/25 | Loss: 0.13261352
Iteration 11/25 | Loss: 0.13264105
Iteration 12/25 | Loss: 0.13264100
Iteration 13/25 | Loss: 0.13262784
Iteration 14/25 | Loss: 0.13262220
Iteration 15/25 | Loss: 0.13255820
Iteration 16/25 | Loss: 0.13254252
Iteration 17/25 | Loss: 0.13254252
Iteration 18/25 | Loss: 0.13254251
Iteration 19/25 | Loss: 0.13254248
Iteration 20/25 | Loss: 0.13254248
Iteration 21/25 | Loss: 0.13254248
Iteration 22/25 | Loss: 0.13254248
Iteration 23/25 | Loss: 0.13254248
Iteration 24/25 | Loss: 0.13254248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.1325424760580063, 0.1325424760580063, 0.1325424760580063, 0.1325424760580063, 0.1325424760580063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1325424760580063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.13254248
Iteration 2/1000 | Loss: 0.00213581
Iteration 3/1000 | Loss: 0.00185510
Iteration 4/1000 | Loss: 0.00109973
Iteration 5/1000 | Loss: 0.00028545
Iteration 6/1000 | Loss: 0.00022597
Iteration 7/1000 | Loss: 0.00025493
Iteration 8/1000 | Loss: 0.00005427
Iteration 9/1000 | Loss: 0.00003821
Iteration 10/1000 | Loss: 0.00010941
Iteration 11/1000 | Loss: 0.00004916
Iteration 12/1000 | Loss: 0.00002670
Iteration 13/1000 | Loss: 0.00002425
Iteration 14/1000 | Loss: 0.00002259
Iteration 15/1000 | Loss: 0.00002106
Iteration 16/1000 | Loss: 0.00002015
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001796
Iteration 20/1000 | Loss: 0.00017184
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001525
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00014210
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00001754
Iteration 30/1000 | Loss: 0.00001459
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00004576
Iteration 33/1000 | Loss: 0.00001439
Iteration 34/1000 | Loss: 0.00003484
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001374
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001370
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001370
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001370
Iteration 48/1000 | Loss: 0.00001370
Iteration 49/1000 | Loss: 0.00001370
Iteration 50/1000 | Loss: 0.00001370
Iteration 51/1000 | Loss: 0.00001370
Iteration 52/1000 | Loss: 0.00001370
Iteration 53/1000 | Loss: 0.00001370
Iteration 54/1000 | Loss: 0.00001369
Iteration 55/1000 | Loss: 0.00001369
Iteration 56/1000 | Loss: 0.00001369
Iteration 57/1000 | Loss: 0.00001368
Iteration 58/1000 | Loss: 0.00001367
Iteration 59/1000 | Loss: 0.00001367
Iteration 60/1000 | Loss: 0.00001367
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001366
Iteration 63/1000 | Loss: 0.00001366
Iteration 64/1000 | Loss: 0.00001366
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001365
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001359
Iteration 87/1000 | Loss: 0.00001359
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001358
Iteration 92/1000 | Loss: 0.00001358
Iteration 93/1000 | Loss: 0.00001358
Iteration 94/1000 | Loss: 0.00001358
Iteration 95/1000 | Loss: 0.00001358
Iteration 96/1000 | Loss: 0.00001358
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001358
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001358
Iteration 101/1000 | Loss: 0.00001357
Iteration 102/1000 | Loss: 0.00001357
Iteration 103/1000 | Loss: 0.00001357
Iteration 104/1000 | Loss: 0.00001357
Iteration 105/1000 | Loss: 0.00001356
Iteration 106/1000 | Loss: 0.00001356
Iteration 107/1000 | Loss: 0.00001356
Iteration 108/1000 | Loss: 0.00001356
Iteration 109/1000 | Loss: 0.00001356
Iteration 110/1000 | Loss: 0.00001355
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001355
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001354
Iteration 117/1000 | Loss: 0.00001354
Iteration 118/1000 | Loss: 0.00001354
Iteration 119/1000 | Loss: 0.00001354
Iteration 120/1000 | Loss: 0.00001353
Iteration 121/1000 | Loss: 0.00001353
Iteration 122/1000 | Loss: 0.00001353
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001352
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001352
Iteration 129/1000 | Loss: 0.00001352
Iteration 130/1000 | Loss: 0.00001351
Iteration 131/1000 | Loss: 0.00001351
Iteration 132/1000 | Loss: 0.00001351
Iteration 133/1000 | Loss: 0.00001351
Iteration 134/1000 | Loss: 0.00001351
Iteration 135/1000 | Loss: 0.00001351
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001350
Iteration 139/1000 | Loss: 0.00001349
Iteration 140/1000 | Loss: 0.00001349
Iteration 141/1000 | Loss: 0.00001349
Iteration 142/1000 | Loss: 0.00001349
Iteration 143/1000 | Loss: 0.00001348
Iteration 144/1000 | Loss: 0.00001348
Iteration 145/1000 | Loss: 0.00001348
Iteration 146/1000 | Loss: 0.00001348
Iteration 147/1000 | Loss: 0.00001348
Iteration 148/1000 | Loss: 0.00001348
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001347
Iteration 154/1000 | Loss: 0.00001347
Iteration 155/1000 | Loss: 0.00001347
Iteration 156/1000 | Loss: 0.00001347
Iteration 157/1000 | Loss: 0.00001347
Iteration 158/1000 | Loss: 0.00001347
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001346
Iteration 161/1000 | Loss: 0.00001346
Iteration 162/1000 | Loss: 0.00001346
Iteration 163/1000 | Loss: 0.00001346
Iteration 164/1000 | Loss: 0.00001346
Iteration 165/1000 | Loss: 0.00001346
Iteration 166/1000 | Loss: 0.00001346
Iteration 167/1000 | Loss: 0.00001346
Iteration 168/1000 | Loss: 0.00001346
Iteration 169/1000 | Loss: 0.00001346
Iteration 170/1000 | Loss: 0.00001346
Iteration 171/1000 | Loss: 0.00001346
Iteration 172/1000 | Loss: 0.00001346
Iteration 173/1000 | Loss: 0.00001346
Iteration 174/1000 | Loss: 0.00001346
Iteration 175/1000 | Loss: 0.00001345
Iteration 176/1000 | Loss: 0.00001345
Iteration 177/1000 | Loss: 0.00001345
Iteration 178/1000 | Loss: 0.00001345
Iteration 179/1000 | Loss: 0.00001345
Iteration 180/1000 | Loss: 0.00001345
Iteration 181/1000 | Loss: 0.00001345
Iteration 182/1000 | Loss: 0.00001345
Iteration 183/1000 | Loss: 0.00001345
Iteration 184/1000 | Loss: 0.00001345
Iteration 185/1000 | Loss: 0.00001345
Iteration 186/1000 | Loss: 0.00001345
Iteration 187/1000 | Loss: 0.00001344
Iteration 188/1000 | Loss: 0.00001344
Iteration 189/1000 | Loss: 0.00001344
Iteration 190/1000 | Loss: 0.00001344
Iteration 191/1000 | Loss: 0.00001344
Iteration 192/1000 | Loss: 0.00001344
Iteration 193/1000 | Loss: 0.00001344
Iteration 194/1000 | Loss: 0.00001344
Iteration 195/1000 | Loss: 0.00001344
Iteration 196/1000 | Loss: 0.00001344
Iteration 197/1000 | Loss: 0.00001344
Iteration 198/1000 | Loss: 0.00001344
Iteration 199/1000 | Loss: 0.00001344
Iteration 200/1000 | Loss: 0.00001344
Iteration 201/1000 | Loss: 0.00001344
Iteration 202/1000 | Loss: 0.00001344
Iteration 203/1000 | Loss: 0.00001344
Iteration 204/1000 | Loss: 0.00001344
Iteration 205/1000 | Loss: 0.00001344
Iteration 206/1000 | Loss: 0.00001344
Iteration 207/1000 | Loss: 0.00001344
Iteration 208/1000 | Loss: 0.00001344
Iteration 209/1000 | Loss: 0.00001344
Iteration 210/1000 | Loss: 0.00001344
Iteration 211/1000 | Loss: 0.00001344
Iteration 212/1000 | Loss: 0.00001344
Iteration 213/1000 | Loss: 0.00001344
Iteration 214/1000 | Loss: 0.00001344
Iteration 215/1000 | Loss: 0.00001344
Iteration 216/1000 | Loss: 0.00001344
Iteration 217/1000 | Loss: 0.00001344
Iteration 218/1000 | Loss: 0.00001344
Iteration 219/1000 | Loss: 0.00001344
Iteration 220/1000 | Loss: 0.00001344
Iteration 221/1000 | Loss: 0.00001344
Iteration 222/1000 | Loss: 0.00001344
Iteration 223/1000 | Loss: 0.00001344
Iteration 224/1000 | Loss: 0.00001344
Iteration 225/1000 | Loss: 0.00001344
Iteration 226/1000 | Loss: 0.00001344
Iteration 227/1000 | Loss: 0.00001344
Iteration 228/1000 | Loss: 0.00001344
Iteration 229/1000 | Loss: 0.00001344
Iteration 230/1000 | Loss: 0.00001344
Iteration 231/1000 | Loss: 0.00001344
Iteration 232/1000 | Loss: 0.00001344
Iteration 233/1000 | Loss: 0.00001344
Iteration 234/1000 | Loss: 0.00001344
Iteration 235/1000 | Loss: 0.00001344
Iteration 236/1000 | Loss: 0.00001344
Iteration 237/1000 | Loss: 0.00001344
Iteration 238/1000 | Loss: 0.00001344
Iteration 239/1000 | Loss: 0.00001344
Iteration 240/1000 | Loss: 0.00001344
Iteration 241/1000 | Loss: 0.00001344
Iteration 242/1000 | Loss: 0.00001344
Iteration 243/1000 | Loss: 0.00001344
Iteration 244/1000 | Loss: 0.00001344
Iteration 245/1000 | Loss: 0.00001344
Iteration 246/1000 | Loss: 0.00001344
Iteration 247/1000 | Loss: 0.00001344
Iteration 248/1000 | Loss: 0.00001344
Iteration 249/1000 | Loss: 0.00001344
Iteration 250/1000 | Loss: 0.00001344
Iteration 251/1000 | Loss: 0.00001344
Iteration 252/1000 | Loss: 0.00001344
Iteration 253/1000 | Loss: 0.00001344
Iteration 254/1000 | Loss: 0.00001344
Iteration 255/1000 | Loss: 0.00001344
Iteration 256/1000 | Loss: 0.00001344
Iteration 257/1000 | Loss: 0.00001344
Iteration 258/1000 | Loss: 0.00001344
Iteration 259/1000 | Loss: 0.00001344
Iteration 260/1000 | Loss: 0.00001344
Iteration 261/1000 | Loss: 0.00001344
Iteration 262/1000 | Loss: 0.00001344
Iteration 263/1000 | Loss: 0.00001344
Iteration 264/1000 | Loss: 0.00001344
Iteration 265/1000 | Loss: 0.00001344
Iteration 266/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.3438811038213316e-05, 1.3438811038213316e-05, 1.3438811038213316e-05, 1.3438811038213316e-05, 1.3438811038213316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3438811038213316e-05

Optimization complete. Final v2v error: 3.136510133743286 mm

Highest mean error: 3.597201347351074 mm for frame 239

Lowest mean error: 2.751605272293091 mm for frame 58

Saving results

Total time: 95.97217011451721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455401
Iteration 2/25 | Loss: 0.00117212
Iteration 3/25 | Loss: 0.00110992
Iteration 4/25 | Loss: 0.00110201
Iteration 5/25 | Loss: 0.00109972
Iteration 6/25 | Loss: 0.00109972
Iteration 7/25 | Loss: 0.00109972
Iteration 8/25 | Loss: 0.00109972
Iteration 9/25 | Loss: 0.00109972
Iteration 10/25 | Loss: 0.00109972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010997249046340585, 0.0010997249046340585, 0.0010997249046340585, 0.0010997249046340585, 0.0010997249046340585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010997249046340585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36633670
Iteration 2/25 | Loss: 0.00090419
Iteration 3/25 | Loss: 0.00090419
Iteration 4/25 | Loss: 0.00090419
Iteration 5/25 | Loss: 0.00090419
Iteration 6/25 | Loss: 0.00090419
Iteration 7/25 | Loss: 0.00090419
Iteration 8/25 | Loss: 0.00090419
Iteration 9/25 | Loss: 0.00090419
Iteration 10/25 | Loss: 0.00090419
Iteration 11/25 | Loss: 0.00090419
Iteration 12/25 | Loss: 0.00090419
Iteration 13/25 | Loss: 0.00090419
Iteration 14/25 | Loss: 0.00090419
Iteration 15/25 | Loss: 0.00090419
Iteration 16/25 | Loss: 0.00090419
Iteration 17/25 | Loss: 0.00090419
Iteration 18/25 | Loss: 0.00090419
Iteration 19/25 | Loss: 0.00090419
Iteration 20/25 | Loss: 0.00090419
Iteration 21/25 | Loss: 0.00090419
Iteration 22/25 | Loss: 0.00090419
Iteration 23/25 | Loss: 0.00090419
Iteration 24/25 | Loss: 0.00090419
Iteration 25/25 | Loss: 0.00090419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090419
Iteration 2/1000 | Loss: 0.00002198
Iteration 3/1000 | Loss: 0.00001631
Iteration 4/1000 | Loss: 0.00001512
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001393
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001341
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001304
Iteration 13/1000 | Loss: 0.00001302
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001287
Iteration 19/1000 | Loss: 0.00001283
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001277
Iteration 23/1000 | Loss: 0.00001275
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001271
Iteration 27/1000 | Loss: 0.00001271
Iteration 28/1000 | Loss: 0.00001271
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001271
Iteration 31/1000 | Loss: 0.00001271
Iteration 32/1000 | Loss: 0.00001271
Iteration 33/1000 | Loss: 0.00001271
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001266
Iteration 38/1000 | Loss: 0.00001265
Iteration 39/1000 | Loss: 0.00001265
Iteration 40/1000 | Loss: 0.00001265
Iteration 41/1000 | Loss: 0.00001265
Iteration 42/1000 | Loss: 0.00001254
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001233
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001231
Iteration 63/1000 | Loss: 0.00001231
Iteration 64/1000 | Loss: 0.00001226
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001224
Iteration 67/1000 | Loss: 0.00001224
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001211
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001207
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001202
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001202
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001201
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001200
Iteration 119/1000 | Loss: 0.00001200
Iteration 120/1000 | Loss: 0.00001200
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001200
Iteration 123/1000 | Loss: 0.00001200
Iteration 124/1000 | Loss: 0.00001200
Iteration 125/1000 | Loss: 0.00001200
Iteration 126/1000 | Loss: 0.00001200
Iteration 127/1000 | Loss: 0.00001200
Iteration 128/1000 | Loss: 0.00001200
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001200
Iteration 138/1000 | Loss: 0.00001200
Iteration 139/1000 | Loss: 0.00001200
Iteration 140/1000 | Loss: 0.00001200
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1996933608315885e-05, 1.1996933608315885e-05, 1.1996933608315885e-05, 1.1996933608315885e-05, 1.1996933608315885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1996933608315885e-05

Optimization complete. Final v2v error: 2.9411675930023193 mm

Highest mean error: 3.149949073791504 mm for frame 48

Lowest mean error: 2.5658328533172607 mm for frame 7

Saving results

Total time: 42.473944664001465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395207
Iteration 2/25 | Loss: 0.00122188
Iteration 3/25 | Loss: 0.00113155
Iteration 4/25 | Loss: 0.00112114
Iteration 5/25 | Loss: 0.00111874
Iteration 6/25 | Loss: 0.00111815
Iteration 7/25 | Loss: 0.00111815
Iteration 8/25 | Loss: 0.00111815
Iteration 9/25 | Loss: 0.00111815
Iteration 10/25 | Loss: 0.00111815
Iteration 11/25 | Loss: 0.00111815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011181493755429983, 0.0011181493755429983, 0.0011181493755429983, 0.0011181493755429983, 0.0011181493755429983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011181493755429983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35614133
Iteration 2/25 | Loss: 0.00085250
Iteration 3/25 | Loss: 0.00085250
Iteration 4/25 | Loss: 0.00085250
Iteration 5/25 | Loss: 0.00085249
Iteration 6/25 | Loss: 0.00085249
Iteration 7/25 | Loss: 0.00085249
Iteration 8/25 | Loss: 0.00085249
Iteration 9/25 | Loss: 0.00085249
Iteration 10/25 | Loss: 0.00085249
Iteration 11/25 | Loss: 0.00085249
Iteration 12/25 | Loss: 0.00085249
Iteration 13/25 | Loss: 0.00085249
Iteration 14/25 | Loss: 0.00085249
Iteration 15/25 | Loss: 0.00085249
Iteration 16/25 | Loss: 0.00085249
Iteration 17/25 | Loss: 0.00085249
Iteration 18/25 | Loss: 0.00085249
Iteration 19/25 | Loss: 0.00085249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008524917648173869, 0.0008524917648173869, 0.0008524917648173869, 0.0008524917648173869, 0.0008524917648173869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008524917648173869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085249
Iteration 2/1000 | Loss: 0.00003312
Iteration 3/1000 | Loss: 0.00002200
Iteration 4/1000 | Loss: 0.00001714
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001513
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001449
Iteration 9/1000 | Loss: 0.00001424
Iteration 10/1000 | Loss: 0.00001404
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001362
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001345
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001339
Iteration 34/1000 | Loss: 0.00001338
Iteration 35/1000 | Loss: 0.00001338
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001336
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001336
Iteration 40/1000 | Loss: 0.00001336
Iteration 41/1000 | Loss: 0.00001335
Iteration 42/1000 | Loss: 0.00001335
Iteration 43/1000 | Loss: 0.00001334
Iteration 44/1000 | Loss: 0.00001334
Iteration 45/1000 | Loss: 0.00001334
Iteration 46/1000 | Loss: 0.00001333
Iteration 47/1000 | Loss: 0.00001333
Iteration 48/1000 | Loss: 0.00001333
Iteration 49/1000 | Loss: 0.00001333
Iteration 50/1000 | Loss: 0.00001332
Iteration 51/1000 | Loss: 0.00001332
Iteration 52/1000 | Loss: 0.00001331
Iteration 53/1000 | Loss: 0.00001331
Iteration 54/1000 | Loss: 0.00001331
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001330
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001330
Iteration 60/1000 | Loss: 0.00001330
Iteration 61/1000 | Loss: 0.00001330
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001329
Iteration 64/1000 | Loss: 0.00001329
Iteration 65/1000 | Loss: 0.00001329
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001328
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001323
Iteration 91/1000 | Loss: 0.00001323
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001322
Iteration 95/1000 | Loss: 0.00001322
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001321
Iteration 100/1000 | Loss: 0.00001321
Iteration 101/1000 | Loss: 0.00001321
Iteration 102/1000 | Loss: 0.00001321
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001320
Iteration 106/1000 | Loss: 0.00001320
Iteration 107/1000 | Loss: 0.00001320
Iteration 108/1000 | Loss: 0.00001319
Iteration 109/1000 | Loss: 0.00001319
Iteration 110/1000 | Loss: 0.00001319
Iteration 111/1000 | Loss: 0.00001318
Iteration 112/1000 | Loss: 0.00001318
Iteration 113/1000 | Loss: 0.00001318
Iteration 114/1000 | Loss: 0.00001318
Iteration 115/1000 | Loss: 0.00001318
Iteration 116/1000 | Loss: 0.00001318
Iteration 117/1000 | Loss: 0.00001318
Iteration 118/1000 | Loss: 0.00001318
Iteration 119/1000 | Loss: 0.00001318
Iteration 120/1000 | Loss: 0.00001318
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001317
Iteration 123/1000 | Loss: 0.00001317
Iteration 124/1000 | Loss: 0.00001317
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001315
Iteration 136/1000 | Loss: 0.00001315
Iteration 137/1000 | Loss: 0.00001314
Iteration 138/1000 | Loss: 0.00001314
Iteration 139/1000 | Loss: 0.00001314
Iteration 140/1000 | Loss: 0.00001313
Iteration 141/1000 | Loss: 0.00001313
Iteration 142/1000 | Loss: 0.00001313
Iteration 143/1000 | Loss: 0.00001313
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001312
Iteration 146/1000 | Loss: 0.00001312
Iteration 147/1000 | Loss: 0.00001312
Iteration 148/1000 | Loss: 0.00001312
Iteration 149/1000 | Loss: 0.00001312
Iteration 150/1000 | Loss: 0.00001312
Iteration 151/1000 | Loss: 0.00001311
Iteration 152/1000 | Loss: 0.00001311
Iteration 153/1000 | Loss: 0.00001311
Iteration 154/1000 | Loss: 0.00001311
Iteration 155/1000 | Loss: 0.00001311
Iteration 156/1000 | Loss: 0.00001311
Iteration 157/1000 | Loss: 0.00001310
Iteration 158/1000 | Loss: 0.00001310
Iteration 159/1000 | Loss: 0.00001310
Iteration 160/1000 | Loss: 0.00001310
Iteration 161/1000 | Loss: 0.00001310
Iteration 162/1000 | Loss: 0.00001310
Iteration 163/1000 | Loss: 0.00001310
Iteration 164/1000 | Loss: 0.00001310
Iteration 165/1000 | Loss: 0.00001310
Iteration 166/1000 | Loss: 0.00001310
Iteration 167/1000 | Loss: 0.00001310
Iteration 168/1000 | Loss: 0.00001310
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001309
Iteration 171/1000 | Loss: 0.00001309
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001309
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001308
Iteration 186/1000 | Loss: 0.00001308
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001308
Iteration 190/1000 | Loss: 0.00001308
Iteration 191/1000 | Loss: 0.00001308
Iteration 192/1000 | Loss: 0.00001308
Iteration 193/1000 | Loss: 0.00001308
Iteration 194/1000 | Loss: 0.00001308
Iteration 195/1000 | Loss: 0.00001308
Iteration 196/1000 | Loss: 0.00001308
Iteration 197/1000 | Loss: 0.00001308
Iteration 198/1000 | Loss: 0.00001308
Iteration 199/1000 | Loss: 0.00001308
Iteration 200/1000 | Loss: 0.00001308
Iteration 201/1000 | Loss: 0.00001308
Iteration 202/1000 | Loss: 0.00001308
Iteration 203/1000 | Loss: 0.00001308
Iteration 204/1000 | Loss: 0.00001308
Iteration 205/1000 | Loss: 0.00001308
Iteration 206/1000 | Loss: 0.00001308
Iteration 207/1000 | Loss: 0.00001308
Iteration 208/1000 | Loss: 0.00001308
Iteration 209/1000 | Loss: 0.00001308
Iteration 210/1000 | Loss: 0.00001308
Iteration 211/1000 | Loss: 0.00001308
Iteration 212/1000 | Loss: 0.00001308
Iteration 213/1000 | Loss: 0.00001308
Iteration 214/1000 | Loss: 0.00001308
Iteration 215/1000 | Loss: 0.00001308
Iteration 216/1000 | Loss: 0.00001308
Iteration 217/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.3083435078442562e-05, 1.3083435078442562e-05, 1.3083435078442562e-05, 1.3083435078442562e-05, 1.3083435078442562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3083435078442562e-05

Optimization complete. Final v2v error: 2.9903454780578613 mm

Highest mean error: 3.2541465759277344 mm for frame 20

Lowest mean error: 2.631237030029297 mm for frame 65

Saving results

Total time: 41.0085027217865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433261
Iteration 2/25 | Loss: 0.00118072
Iteration 3/25 | Loss: 0.00112308
Iteration 4/25 | Loss: 0.00111444
Iteration 5/25 | Loss: 0.00111233
Iteration 6/25 | Loss: 0.00111233
Iteration 7/25 | Loss: 0.00111233
Iteration 8/25 | Loss: 0.00111233
Iteration 9/25 | Loss: 0.00111233
Iteration 10/25 | Loss: 0.00111233
Iteration 11/25 | Loss: 0.00111233
Iteration 12/25 | Loss: 0.00111233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011123333824798465, 0.0011123333824798465, 0.0011123333824798465, 0.0011123333824798465, 0.0011123333824798465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011123333824798465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36900818
Iteration 2/25 | Loss: 0.00075501
Iteration 3/25 | Loss: 0.00075501
Iteration 4/25 | Loss: 0.00075501
Iteration 5/25 | Loss: 0.00075501
Iteration 6/25 | Loss: 0.00075501
Iteration 7/25 | Loss: 0.00075501
Iteration 8/25 | Loss: 0.00075501
Iteration 9/25 | Loss: 0.00075501
Iteration 10/25 | Loss: 0.00075501
Iteration 11/25 | Loss: 0.00075501
Iteration 12/25 | Loss: 0.00075501
Iteration 13/25 | Loss: 0.00075501
Iteration 14/25 | Loss: 0.00075501
Iteration 15/25 | Loss: 0.00075501
Iteration 16/25 | Loss: 0.00075501
Iteration 17/25 | Loss: 0.00075501
Iteration 18/25 | Loss: 0.00075501
Iteration 19/25 | Loss: 0.00075501
Iteration 20/25 | Loss: 0.00075501
Iteration 21/25 | Loss: 0.00075501
Iteration 22/25 | Loss: 0.00075501
Iteration 23/25 | Loss: 0.00075501
Iteration 24/25 | Loss: 0.00075501
Iteration 25/25 | Loss: 0.00075501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075501
Iteration 2/1000 | Loss: 0.00001977
Iteration 3/1000 | Loss: 0.00001537
Iteration 4/1000 | Loss: 0.00001422
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001303
Iteration 7/1000 | Loss: 0.00001266
Iteration 8/1000 | Loss: 0.00001240
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001205
Iteration 15/1000 | Loss: 0.00001198
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001187
Iteration 18/1000 | Loss: 0.00001183
Iteration 19/1000 | Loss: 0.00001182
Iteration 20/1000 | Loss: 0.00001182
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001176
Iteration 23/1000 | Loss: 0.00001176
Iteration 24/1000 | Loss: 0.00001175
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001171
Iteration 28/1000 | Loss: 0.00001171
Iteration 29/1000 | Loss: 0.00001170
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001165
Iteration 33/1000 | Loss: 0.00001164
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001160
Iteration 36/1000 | Loss: 0.00001159
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001159
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001159
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001154
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001153
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00001153
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001151
Iteration 64/1000 | Loss: 0.00001151
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001147
Iteration 75/1000 | Loss: 0.00001147
Iteration 76/1000 | Loss: 0.00001147
Iteration 77/1000 | Loss: 0.00001146
Iteration 78/1000 | Loss: 0.00001146
Iteration 79/1000 | Loss: 0.00001146
Iteration 80/1000 | Loss: 0.00001146
Iteration 81/1000 | Loss: 0.00001146
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001145
Iteration 84/1000 | Loss: 0.00001145
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001144
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001142
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001141
Iteration 96/1000 | Loss: 0.00001141
Iteration 97/1000 | Loss: 0.00001141
Iteration 98/1000 | Loss: 0.00001141
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001140
Iteration 101/1000 | Loss: 0.00001138
Iteration 102/1000 | Loss: 0.00001138
Iteration 103/1000 | Loss: 0.00001138
Iteration 104/1000 | Loss: 0.00001138
Iteration 105/1000 | Loss: 0.00001138
Iteration 106/1000 | Loss: 0.00001138
Iteration 107/1000 | Loss: 0.00001138
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001136
Iteration 122/1000 | Loss: 0.00001136
Iteration 123/1000 | Loss: 0.00001135
Iteration 124/1000 | Loss: 0.00001135
Iteration 125/1000 | Loss: 0.00001135
Iteration 126/1000 | Loss: 0.00001135
Iteration 127/1000 | Loss: 0.00001135
Iteration 128/1000 | Loss: 0.00001135
Iteration 129/1000 | Loss: 0.00001134
Iteration 130/1000 | Loss: 0.00001134
Iteration 131/1000 | Loss: 0.00001133
Iteration 132/1000 | Loss: 0.00001133
Iteration 133/1000 | Loss: 0.00001132
Iteration 134/1000 | Loss: 0.00001132
Iteration 135/1000 | Loss: 0.00001132
Iteration 136/1000 | Loss: 0.00001132
Iteration 137/1000 | Loss: 0.00001132
Iteration 138/1000 | Loss: 0.00001132
Iteration 139/1000 | Loss: 0.00001131
Iteration 140/1000 | Loss: 0.00001131
Iteration 141/1000 | Loss: 0.00001131
Iteration 142/1000 | Loss: 0.00001131
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001128
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001128
Iteration 162/1000 | Loss: 0.00001128
Iteration 163/1000 | Loss: 0.00001128
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1278063539066352e-05, 1.1278063539066352e-05, 1.1278063539066352e-05, 1.1278063539066352e-05, 1.1278063539066352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1278063539066352e-05

Optimization complete. Final v2v error: 2.868476390838623 mm

Highest mean error: 3.030048131942749 mm for frame 167

Lowest mean error: 2.7217884063720703 mm for frame 0

Saving results

Total time: 41.1463725566864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590662
Iteration 2/25 | Loss: 0.00163822
Iteration 3/25 | Loss: 0.00138700
Iteration 4/25 | Loss: 0.00139918
Iteration 5/25 | Loss: 0.00132904
Iteration 6/25 | Loss: 0.00128797
Iteration 7/25 | Loss: 0.00125749
Iteration 8/25 | Loss: 0.00123648
Iteration 9/25 | Loss: 0.00122813
Iteration 10/25 | Loss: 0.00123772
Iteration 11/25 | Loss: 0.00123779
Iteration 12/25 | Loss: 0.00122837
Iteration 13/25 | Loss: 0.00122344
Iteration 14/25 | Loss: 0.00121921
Iteration 15/25 | Loss: 0.00121865
Iteration 16/25 | Loss: 0.00121834
Iteration 17/25 | Loss: 0.00121820
Iteration 18/25 | Loss: 0.00121825
Iteration 19/25 | Loss: 0.00121737
Iteration 20/25 | Loss: 0.00121683
Iteration 21/25 | Loss: 0.00121666
Iteration 22/25 | Loss: 0.00121663
Iteration 23/25 | Loss: 0.00121663
Iteration 24/25 | Loss: 0.00121663
Iteration 25/25 | Loss: 0.00121662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83869982
Iteration 2/25 | Loss: 0.00115784
Iteration 3/25 | Loss: 0.00106884
Iteration 4/25 | Loss: 0.00106884
Iteration 5/25 | Loss: 0.00106884
Iteration 6/25 | Loss: 0.00106884
Iteration 7/25 | Loss: 0.00106884
Iteration 8/25 | Loss: 0.00106884
Iteration 9/25 | Loss: 0.00106884
Iteration 10/25 | Loss: 0.00106884
Iteration 11/25 | Loss: 0.00106884
Iteration 12/25 | Loss: 0.00106884
Iteration 13/25 | Loss: 0.00106884
Iteration 14/25 | Loss: 0.00106884
Iteration 15/25 | Loss: 0.00106884
Iteration 16/25 | Loss: 0.00106884
Iteration 17/25 | Loss: 0.00106884
Iteration 18/25 | Loss: 0.00106884
Iteration 19/25 | Loss: 0.00106884
Iteration 20/25 | Loss: 0.00106884
Iteration 21/25 | Loss: 0.00106884
Iteration 22/25 | Loss: 0.00106884
Iteration 23/25 | Loss: 0.00106884
Iteration 24/25 | Loss: 0.00106884
Iteration 25/25 | Loss: 0.00106884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106884
Iteration 2/1000 | Loss: 0.00025595
Iteration 3/1000 | Loss: 0.00012742
Iteration 4/1000 | Loss: 0.00004949
Iteration 5/1000 | Loss: 0.00014258
Iteration 6/1000 | Loss: 0.00050910
Iteration 7/1000 | Loss: 0.00008067
Iteration 8/1000 | Loss: 0.00005426
Iteration 9/1000 | Loss: 0.00012374
Iteration 10/1000 | Loss: 0.00004056
Iteration 11/1000 | Loss: 0.00003929
Iteration 12/1000 | Loss: 0.00018593
Iteration 13/1000 | Loss: 0.00003771
Iteration 14/1000 | Loss: 0.00003704
Iteration 15/1000 | Loss: 0.00003625
Iteration 16/1000 | Loss: 0.00003559
Iteration 17/1000 | Loss: 0.00008685
Iteration 18/1000 | Loss: 0.00003921
Iteration 19/1000 | Loss: 0.00003645
Iteration 20/1000 | Loss: 0.00003495
Iteration 21/1000 | Loss: 0.00006950
Iteration 22/1000 | Loss: 0.00003449
Iteration 23/1000 | Loss: 0.00003446
Iteration 24/1000 | Loss: 0.00003420
Iteration 25/1000 | Loss: 0.00003398
Iteration 26/1000 | Loss: 0.00003396
Iteration 27/1000 | Loss: 0.00003388
Iteration 28/1000 | Loss: 0.00003383
Iteration 29/1000 | Loss: 0.00003373
Iteration 30/1000 | Loss: 0.00003370
Iteration 31/1000 | Loss: 0.00003369
Iteration 32/1000 | Loss: 0.00003369
Iteration 33/1000 | Loss: 0.00003368
Iteration 34/1000 | Loss: 0.00003366
Iteration 35/1000 | Loss: 0.00014715
Iteration 36/1000 | Loss: 0.00003394
Iteration 37/1000 | Loss: 0.00034066
Iteration 38/1000 | Loss: 0.00003753
Iteration 39/1000 | Loss: 0.00003489
Iteration 40/1000 | Loss: 0.00003386
Iteration 41/1000 | Loss: 0.00003304
Iteration 42/1000 | Loss: 0.00003244
Iteration 43/1000 | Loss: 0.00003204
Iteration 44/1000 | Loss: 0.00003180
Iteration 45/1000 | Loss: 0.00003176
Iteration 46/1000 | Loss: 0.00003166
Iteration 47/1000 | Loss: 0.00003163
Iteration 48/1000 | Loss: 0.00003162
Iteration 49/1000 | Loss: 0.00003162
Iteration 50/1000 | Loss: 0.00003161
Iteration 51/1000 | Loss: 0.00003161
Iteration 52/1000 | Loss: 0.00003160
Iteration 53/1000 | Loss: 0.00003159
Iteration 54/1000 | Loss: 0.00003157
Iteration 55/1000 | Loss: 0.00003156
Iteration 56/1000 | Loss: 0.00003155
Iteration 57/1000 | Loss: 0.00003154
Iteration 58/1000 | Loss: 0.00003153
Iteration 59/1000 | Loss: 0.00003153
Iteration 60/1000 | Loss: 0.00003152
Iteration 61/1000 | Loss: 0.00003152
Iteration 62/1000 | Loss: 0.00003150
Iteration 63/1000 | Loss: 0.00003149
Iteration 64/1000 | Loss: 0.00003148
Iteration 65/1000 | Loss: 0.00003148
Iteration 66/1000 | Loss: 0.00003147
Iteration 67/1000 | Loss: 0.00003147
Iteration 68/1000 | Loss: 0.00003146
Iteration 69/1000 | Loss: 0.00003145
Iteration 70/1000 | Loss: 0.00003144
Iteration 71/1000 | Loss: 0.00003144
Iteration 72/1000 | Loss: 0.00003143
Iteration 73/1000 | Loss: 0.00003143
Iteration 74/1000 | Loss: 0.00003142
Iteration 75/1000 | Loss: 0.00003141
Iteration 76/1000 | Loss: 0.00003140
Iteration 77/1000 | Loss: 0.00003140
Iteration 78/1000 | Loss: 0.00003139
Iteration 79/1000 | Loss: 0.00003139
Iteration 80/1000 | Loss: 0.00003138
Iteration 81/1000 | Loss: 0.00003138
Iteration 82/1000 | Loss: 0.00009978
Iteration 83/1000 | Loss: 0.00003153
Iteration 84/1000 | Loss: 0.00003132
Iteration 85/1000 | Loss: 0.00003131
Iteration 86/1000 | Loss: 0.00003131
Iteration 87/1000 | Loss: 0.00003130
Iteration 88/1000 | Loss: 0.00003130
Iteration 89/1000 | Loss: 0.00003130
Iteration 90/1000 | Loss: 0.00003130
Iteration 91/1000 | Loss: 0.00003130
Iteration 92/1000 | Loss: 0.00003130
Iteration 93/1000 | Loss: 0.00003130
Iteration 94/1000 | Loss: 0.00003130
Iteration 95/1000 | Loss: 0.00003129
Iteration 96/1000 | Loss: 0.00003129
Iteration 97/1000 | Loss: 0.00003128
Iteration 98/1000 | Loss: 0.00003128
Iteration 99/1000 | Loss: 0.00003128
Iteration 100/1000 | Loss: 0.00003128
Iteration 101/1000 | Loss: 0.00003128
Iteration 102/1000 | Loss: 0.00003128
Iteration 103/1000 | Loss: 0.00003128
Iteration 104/1000 | Loss: 0.00003127
Iteration 105/1000 | Loss: 0.00003127
Iteration 106/1000 | Loss: 0.00003127
Iteration 107/1000 | Loss: 0.00003127
Iteration 108/1000 | Loss: 0.00003127
Iteration 109/1000 | Loss: 0.00003127
Iteration 110/1000 | Loss: 0.00003127
Iteration 111/1000 | Loss: 0.00003126
Iteration 112/1000 | Loss: 0.00003126
Iteration 113/1000 | Loss: 0.00003125
Iteration 114/1000 | Loss: 0.00003119
Iteration 115/1000 | Loss: 0.00003116
Iteration 116/1000 | Loss: 0.00003116
Iteration 117/1000 | Loss: 0.00003116
Iteration 118/1000 | Loss: 0.00003116
Iteration 119/1000 | Loss: 0.00003116
Iteration 120/1000 | Loss: 0.00003116
Iteration 121/1000 | Loss: 0.00003115
Iteration 122/1000 | Loss: 0.00003115
Iteration 123/1000 | Loss: 0.00003115
Iteration 124/1000 | Loss: 0.00003115
Iteration 125/1000 | Loss: 0.00003115
Iteration 126/1000 | Loss: 0.00003115
Iteration 127/1000 | Loss: 0.00003115
Iteration 128/1000 | Loss: 0.00003115
Iteration 129/1000 | Loss: 0.00003115
Iteration 130/1000 | Loss: 0.00003115
Iteration 131/1000 | Loss: 0.00003114
Iteration 132/1000 | Loss: 0.00003114
Iteration 133/1000 | Loss: 0.00003114
Iteration 134/1000 | Loss: 0.00003114
Iteration 135/1000 | Loss: 0.00003113
Iteration 136/1000 | Loss: 0.00003113
Iteration 137/1000 | Loss: 0.00003112
Iteration 138/1000 | Loss: 0.00003112
Iteration 139/1000 | Loss: 0.00003112
Iteration 140/1000 | Loss: 0.00003112
Iteration 141/1000 | Loss: 0.00003112
Iteration 142/1000 | Loss: 0.00003112
Iteration 143/1000 | Loss: 0.00003112
Iteration 144/1000 | Loss: 0.00003112
Iteration 145/1000 | Loss: 0.00003111
Iteration 146/1000 | Loss: 0.00003111
Iteration 147/1000 | Loss: 0.00003111
Iteration 148/1000 | Loss: 0.00003111
Iteration 149/1000 | Loss: 0.00003111
Iteration 150/1000 | Loss: 0.00003111
Iteration 151/1000 | Loss: 0.00003111
Iteration 152/1000 | Loss: 0.00003111
Iteration 153/1000 | Loss: 0.00003111
Iteration 154/1000 | Loss: 0.00003111
Iteration 155/1000 | Loss: 0.00003110
Iteration 156/1000 | Loss: 0.00003110
Iteration 157/1000 | Loss: 0.00003110
Iteration 158/1000 | Loss: 0.00003110
Iteration 159/1000 | Loss: 0.00003110
Iteration 160/1000 | Loss: 0.00003110
Iteration 161/1000 | Loss: 0.00003110
Iteration 162/1000 | Loss: 0.00003110
Iteration 163/1000 | Loss: 0.00003110
Iteration 164/1000 | Loss: 0.00003110
Iteration 165/1000 | Loss: 0.00003110
Iteration 166/1000 | Loss: 0.00003109
Iteration 167/1000 | Loss: 0.00003109
Iteration 168/1000 | Loss: 0.00003109
Iteration 169/1000 | Loss: 0.00003109
Iteration 170/1000 | Loss: 0.00003109
Iteration 171/1000 | Loss: 0.00003109
Iteration 172/1000 | Loss: 0.00003109
Iteration 173/1000 | Loss: 0.00003108
Iteration 174/1000 | Loss: 0.00003108
Iteration 175/1000 | Loss: 0.00003108
Iteration 176/1000 | Loss: 0.00003108
Iteration 177/1000 | Loss: 0.00003107
Iteration 178/1000 | Loss: 0.00003107
Iteration 179/1000 | Loss: 0.00003107
Iteration 180/1000 | Loss: 0.00003107
Iteration 181/1000 | Loss: 0.00003106
Iteration 182/1000 | Loss: 0.00003106
Iteration 183/1000 | Loss: 0.00003106
Iteration 184/1000 | Loss: 0.00003105
Iteration 185/1000 | Loss: 0.00003105
Iteration 186/1000 | Loss: 0.00003105
Iteration 187/1000 | Loss: 0.00003104
Iteration 188/1000 | Loss: 0.00003104
Iteration 189/1000 | Loss: 0.00003104
Iteration 190/1000 | Loss: 0.00003104
Iteration 191/1000 | Loss: 0.00003104
Iteration 192/1000 | Loss: 0.00003103
Iteration 193/1000 | Loss: 0.00003103
Iteration 194/1000 | Loss: 0.00003103
Iteration 195/1000 | Loss: 0.00003103
Iteration 196/1000 | Loss: 0.00003103
Iteration 197/1000 | Loss: 0.00003103
Iteration 198/1000 | Loss: 0.00003103
Iteration 199/1000 | Loss: 0.00003103
Iteration 200/1000 | Loss: 0.00003103
Iteration 201/1000 | Loss: 0.00003103
Iteration 202/1000 | Loss: 0.00003103
Iteration 203/1000 | Loss: 0.00003103
Iteration 204/1000 | Loss: 0.00003102
Iteration 205/1000 | Loss: 0.00003102
Iteration 206/1000 | Loss: 0.00003102
Iteration 207/1000 | Loss: 0.00003102
Iteration 208/1000 | Loss: 0.00003102
Iteration 209/1000 | Loss: 0.00003102
Iteration 210/1000 | Loss: 0.00003102
Iteration 211/1000 | Loss: 0.00003102
Iteration 212/1000 | Loss: 0.00003102
Iteration 213/1000 | Loss: 0.00003102
Iteration 214/1000 | Loss: 0.00003102
Iteration 215/1000 | Loss: 0.00003102
Iteration 216/1000 | Loss: 0.00003101
Iteration 217/1000 | Loss: 0.00003101
Iteration 218/1000 | Loss: 0.00003101
Iteration 219/1000 | Loss: 0.00003101
Iteration 220/1000 | Loss: 0.00003101
Iteration 221/1000 | Loss: 0.00003101
Iteration 222/1000 | Loss: 0.00003101
Iteration 223/1000 | Loss: 0.00003101
Iteration 224/1000 | Loss: 0.00003101
Iteration 225/1000 | Loss: 0.00003101
Iteration 226/1000 | Loss: 0.00003101
Iteration 227/1000 | Loss: 0.00003101
Iteration 228/1000 | Loss: 0.00003101
Iteration 229/1000 | Loss: 0.00003100
Iteration 230/1000 | Loss: 0.00003100
Iteration 231/1000 | Loss: 0.00003100
Iteration 232/1000 | Loss: 0.00003100
Iteration 233/1000 | Loss: 0.00003100
Iteration 234/1000 | Loss: 0.00003100
Iteration 235/1000 | Loss: 0.00003100
Iteration 236/1000 | Loss: 0.00003100
Iteration 237/1000 | Loss: 0.00003100
Iteration 238/1000 | Loss: 0.00003100
Iteration 239/1000 | Loss: 0.00003100
Iteration 240/1000 | Loss: 0.00003099
Iteration 241/1000 | Loss: 0.00003099
Iteration 242/1000 | Loss: 0.00003099
Iteration 243/1000 | Loss: 0.00003099
Iteration 244/1000 | Loss: 0.00003098
Iteration 245/1000 | Loss: 0.00003098
Iteration 246/1000 | Loss: 0.00003098
Iteration 247/1000 | Loss: 0.00003098
Iteration 248/1000 | Loss: 0.00003098
Iteration 249/1000 | Loss: 0.00003098
Iteration 250/1000 | Loss: 0.00003098
Iteration 251/1000 | Loss: 0.00003098
Iteration 252/1000 | Loss: 0.00003098
Iteration 253/1000 | Loss: 0.00003098
Iteration 254/1000 | Loss: 0.00003098
Iteration 255/1000 | Loss: 0.00003098
Iteration 256/1000 | Loss: 0.00003097
Iteration 257/1000 | Loss: 0.00003097
Iteration 258/1000 | Loss: 0.00003096
Iteration 259/1000 | Loss: 0.00003096
Iteration 260/1000 | Loss: 0.00003096
Iteration 261/1000 | Loss: 0.00003096
Iteration 262/1000 | Loss: 0.00003096
Iteration 263/1000 | Loss: 0.00003096
Iteration 264/1000 | Loss: 0.00003096
Iteration 265/1000 | Loss: 0.00003096
Iteration 266/1000 | Loss: 0.00003095
Iteration 267/1000 | Loss: 0.00003095
Iteration 268/1000 | Loss: 0.00003095
Iteration 269/1000 | Loss: 0.00003095
Iteration 270/1000 | Loss: 0.00003095
Iteration 271/1000 | Loss: 0.00003095
Iteration 272/1000 | Loss: 0.00003094
Iteration 273/1000 | Loss: 0.00003094
Iteration 274/1000 | Loss: 0.00003094
Iteration 275/1000 | Loss: 0.00003094
Iteration 276/1000 | Loss: 0.00003094
Iteration 277/1000 | Loss: 0.00003093
Iteration 278/1000 | Loss: 0.00003093
Iteration 279/1000 | Loss: 0.00003093
Iteration 280/1000 | Loss: 0.00003093
Iteration 281/1000 | Loss: 0.00003092
Iteration 282/1000 | Loss: 0.00003092
Iteration 283/1000 | Loss: 0.00003092
Iteration 284/1000 | Loss: 0.00003092
Iteration 285/1000 | Loss: 0.00003092
Iteration 286/1000 | Loss: 0.00003092
Iteration 287/1000 | Loss: 0.00003092
Iteration 288/1000 | Loss: 0.00003091
Iteration 289/1000 | Loss: 0.00003091
Iteration 290/1000 | Loss: 0.00003091
Iteration 291/1000 | Loss: 0.00003090
Iteration 292/1000 | Loss: 0.00003090
Iteration 293/1000 | Loss: 0.00003090
Iteration 294/1000 | Loss: 0.00003089
Iteration 295/1000 | Loss: 0.00003089
Iteration 296/1000 | Loss: 0.00003089
Iteration 297/1000 | Loss: 0.00003088
Iteration 298/1000 | Loss: 0.00003087
Iteration 299/1000 | Loss: 0.00003086
Iteration 300/1000 | Loss: 0.00003086
Iteration 301/1000 | Loss: 0.00003086
Iteration 302/1000 | Loss: 0.00003085
Iteration 303/1000 | Loss: 0.00003080
Iteration 304/1000 | Loss: 0.00003077
Iteration 305/1000 | Loss: 0.00003075
Iteration 306/1000 | Loss: 0.00003075
Iteration 307/1000 | Loss: 0.00003060
Iteration 308/1000 | Loss: 0.00003058
Iteration 309/1000 | Loss: 0.00003055
Iteration 310/1000 | Loss: 0.00003055
Iteration 311/1000 | Loss: 0.00003054
Iteration 312/1000 | Loss: 0.00003041
Iteration 313/1000 | Loss: 0.00003040
Iteration 314/1000 | Loss: 0.00003032
Iteration 315/1000 | Loss: 0.00003030
Iteration 316/1000 | Loss: 0.00003027
Iteration 317/1000 | Loss: 0.00003024
Iteration 318/1000 | Loss: 0.00003024
Iteration 319/1000 | Loss: 0.00003023
Iteration 320/1000 | Loss: 0.00003020
Iteration 321/1000 | Loss: 0.00003016
Iteration 322/1000 | Loss: 0.00002998
Iteration 323/1000 | Loss: 0.00014904
Iteration 324/1000 | Loss: 0.00003128
Iteration 325/1000 | Loss: 0.00003044
Iteration 326/1000 | Loss: 0.00002990
Iteration 327/1000 | Loss: 0.00002968
Iteration 328/1000 | Loss: 0.00002958
Iteration 329/1000 | Loss: 0.00013641
Iteration 330/1000 | Loss: 0.00003614
Iteration 331/1000 | Loss: 0.00003096
Iteration 332/1000 | Loss: 0.00002975
Iteration 333/1000 | Loss: 0.00002947
Iteration 334/1000 | Loss: 0.00002927
Iteration 335/1000 | Loss: 0.00002925
Iteration 336/1000 | Loss: 0.00002907
Iteration 337/1000 | Loss: 0.00002886
Iteration 338/1000 | Loss: 0.00002884
Iteration 339/1000 | Loss: 0.00002878
Iteration 340/1000 | Loss: 0.00002876
Iteration 341/1000 | Loss: 0.00002871
Iteration 342/1000 | Loss: 0.00002869
Iteration 343/1000 | Loss: 0.00002861
Iteration 344/1000 | Loss: 0.00002860
Iteration 345/1000 | Loss: 0.00002859
Iteration 346/1000 | Loss: 0.00002859
Iteration 347/1000 | Loss: 0.00002858
Iteration 348/1000 | Loss: 0.00002857
Iteration 349/1000 | Loss: 0.00002856
Iteration 350/1000 | Loss: 0.00002856
Iteration 351/1000 | Loss: 0.00002855
Iteration 352/1000 | Loss: 0.00002855
Iteration 353/1000 | Loss: 0.00002854
Iteration 354/1000 | Loss: 0.00002853
Iteration 355/1000 | Loss: 0.00002852
Iteration 356/1000 | Loss: 0.00002851
Iteration 357/1000 | Loss: 0.00002851
Iteration 358/1000 | Loss: 0.00002850
Iteration 359/1000 | Loss: 0.00002850
Iteration 360/1000 | Loss: 0.00002850
Iteration 361/1000 | Loss: 0.00002849
Iteration 362/1000 | Loss: 0.00002849
Iteration 363/1000 | Loss: 0.00002849
Iteration 364/1000 | Loss: 0.00002849
Iteration 365/1000 | Loss: 0.00002848
Iteration 366/1000 | Loss: 0.00002848
Iteration 367/1000 | Loss: 0.00002848
Iteration 368/1000 | Loss: 0.00002848
Iteration 369/1000 | Loss: 0.00002848
Iteration 370/1000 | Loss: 0.00002848
Iteration 371/1000 | Loss: 0.00002848
Iteration 372/1000 | Loss: 0.00002847
Iteration 373/1000 | Loss: 0.00002847
Iteration 374/1000 | Loss: 0.00002846
Iteration 375/1000 | Loss: 0.00002846
Iteration 376/1000 | Loss: 0.00002846
Iteration 377/1000 | Loss: 0.00002846
Iteration 378/1000 | Loss: 0.00002846
Iteration 379/1000 | Loss: 0.00002845
Iteration 380/1000 | Loss: 0.00002845
Iteration 381/1000 | Loss: 0.00002845
Iteration 382/1000 | Loss: 0.00002845
Iteration 383/1000 | Loss: 0.00002844
Iteration 384/1000 | Loss: 0.00002844
Iteration 385/1000 | Loss: 0.00002844
Iteration 386/1000 | Loss: 0.00002843
Iteration 387/1000 | Loss: 0.00002843
Iteration 388/1000 | Loss: 0.00002843
Iteration 389/1000 | Loss: 0.00002843
Iteration 390/1000 | Loss: 0.00002842
Iteration 391/1000 | Loss: 0.00002842
Iteration 392/1000 | Loss: 0.00002842
Iteration 393/1000 | Loss: 0.00002842
Iteration 394/1000 | Loss: 0.00002842
Iteration 395/1000 | Loss: 0.00002842
Iteration 396/1000 | Loss: 0.00002842
Iteration 397/1000 | Loss: 0.00002842
Iteration 398/1000 | Loss: 0.00002841
Iteration 399/1000 | Loss: 0.00002841
Iteration 400/1000 | Loss: 0.00002841
Iteration 401/1000 | Loss: 0.00002841
Iteration 402/1000 | Loss: 0.00002841
Iteration 403/1000 | Loss: 0.00002841
Iteration 404/1000 | Loss: 0.00002841
Iteration 405/1000 | Loss: 0.00002841
Iteration 406/1000 | Loss: 0.00002841
Iteration 407/1000 | Loss: 0.00002841
Iteration 408/1000 | Loss: 0.00002841
Iteration 409/1000 | Loss: 0.00002840
Iteration 410/1000 | Loss: 0.00002840
Iteration 411/1000 | Loss: 0.00002840
Iteration 412/1000 | Loss: 0.00002840
Iteration 413/1000 | Loss: 0.00002840
Iteration 414/1000 | Loss: 0.00002840
Iteration 415/1000 | Loss: 0.00002839
Iteration 416/1000 | Loss: 0.00002839
Iteration 417/1000 | Loss: 0.00002839
Iteration 418/1000 | Loss: 0.00002839
Iteration 419/1000 | Loss: 0.00002838
Iteration 420/1000 | Loss: 0.00002838
Iteration 421/1000 | Loss: 0.00002838
Iteration 422/1000 | Loss: 0.00002838
Iteration 423/1000 | Loss: 0.00002838
Iteration 424/1000 | Loss: 0.00002838
Iteration 425/1000 | Loss: 0.00002838
Iteration 426/1000 | Loss: 0.00002838
Iteration 427/1000 | Loss: 0.00002838
Iteration 428/1000 | Loss: 0.00002837
Iteration 429/1000 | Loss: 0.00002837
Iteration 430/1000 | Loss: 0.00002837
Iteration 431/1000 | Loss: 0.00002837
Iteration 432/1000 | Loss: 0.00002837
Iteration 433/1000 | Loss: 0.00002837
Iteration 434/1000 | Loss: 0.00002837
Iteration 435/1000 | Loss: 0.00002837
Iteration 436/1000 | Loss: 0.00002837
Iteration 437/1000 | Loss: 0.00002837
Iteration 438/1000 | Loss: 0.00002837
Iteration 439/1000 | Loss: 0.00002837
Iteration 440/1000 | Loss: 0.00002837
Iteration 441/1000 | Loss: 0.00002836
Iteration 442/1000 | Loss: 0.00002836
Iteration 443/1000 | Loss: 0.00002836
Iteration 444/1000 | Loss: 0.00002836
Iteration 445/1000 | Loss: 0.00002836
Iteration 446/1000 | Loss: 0.00002836
Iteration 447/1000 | Loss: 0.00002836
Iteration 448/1000 | Loss: 0.00002836
Iteration 449/1000 | Loss: 0.00002836
Iteration 450/1000 | Loss: 0.00002836
Iteration 451/1000 | Loss: 0.00002836
Iteration 452/1000 | Loss: 0.00002836
Iteration 453/1000 | Loss: 0.00002836
Iteration 454/1000 | Loss: 0.00002836
Iteration 455/1000 | Loss: 0.00002836
Iteration 456/1000 | Loss: 0.00002836
Iteration 457/1000 | Loss: 0.00002835
Iteration 458/1000 | Loss: 0.00002835
Iteration 459/1000 | Loss: 0.00002835
Iteration 460/1000 | Loss: 0.00002835
Iteration 461/1000 | Loss: 0.00002835
Iteration 462/1000 | Loss: 0.00002835
Iteration 463/1000 | Loss: 0.00002835
Iteration 464/1000 | Loss: 0.00002835
Iteration 465/1000 | Loss: 0.00002835
Iteration 466/1000 | Loss: 0.00002835
Iteration 467/1000 | Loss: 0.00002835
Iteration 468/1000 | Loss: 0.00002835
Iteration 469/1000 | Loss: 0.00002835
Iteration 470/1000 | Loss: 0.00002835
Iteration 471/1000 | Loss: 0.00002835
Iteration 472/1000 | Loss: 0.00002835
Iteration 473/1000 | Loss: 0.00002835
Iteration 474/1000 | Loss: 0.00002835
Iteration 475/1000 | Loss: 0.00002835
Iteration 476/1000 | Loss: 0.00002835
Iteration 477/1000 | Loss: 0.00002835
Iteration 478/1000 | Loss: 0.00002835
Iteration 479/1000 | Loss: 0.00002835
Iteration 480/1000 | Loss: 0.00002835
Iteration 481/1000 | Loss: 0.00002835
Iteration 482/1000 | Loss: 0.00002835
Iteration 483/1000 | Loss: 0.00002835
Iteration 484/1000 | Loss: 0.00002835
Iteration 485/1000 | Loss: 0.00002835
Iteration 486/1000 | Loss: 0.00002835
Iteration 487/1000 | Loss: 0.00002835
Iteration 488/1000 | Loss: 0.00002835
Iteration 489/1000 | Loss: 0.00002835
Iteration 490/1000 | Loss: 0.00002835
Iteration 491/1000 | Loss: 0.00002835
Iteration 492/1000 | Loss: 0.00002835
Iteration 493/1000 | Loss: 0.00002835
Iteration 494/1000 | Loss: 0.00002835
Iteration 495/1000 | Loss: 0.00002835
Iteration 496/1000 | Loss: 0.00002835
Iteration 497/1000 | Loss: 0.00002835
Iteration 498/1000 | Loss: 0.00002835
Iteration 499/1000 | Loss: 0.00002835
Iteration 500/1000 | Loss: 0.00002835
Iteration 501/1000 | Loss: 0.00002835
Iteration 502/1000 | Loss: 0.00002835
Iteration 503/1000 | Loss: 0.00002835
Iteration 504/1000 | Loss: 0.00002835
Iteration 505/1000 | Loss: 0.00002835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 505. Stopping optimization.
Last 5 losses: [2.834935912687797e-05, 2.834935912687797e-05, 2.834935912687797e-05, 2.834935912687797e-05, 2.834935912687797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.834935912687797e-05

Optimization complete. Final v2v error: 3.751627206802368 mm

Highest mean error: 11.67813777923584 mm for frame 105

Lowest mean error: 2.7517101764678955 mm for frame 173

Saving results

Total time: 177.33655762672424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843984
Iteration 2/25 | Loss: 0.00134454
Iteration 3/25 | Loss: 0.00118279
Iteration 4/25 | Loss: 0.00110326
Iteration 5/25 | Loss: 0.00110545
Iteration 6/25 | Loss: 0.00110171
Iteration 7/25 | Loss: 0.00109780
Iteration 8/25 | Loss: 0.00109767
Iteration 9/25 | Loss: 0.00109591
Iteration 10/25 | Loss: 0.00109588
Iteration 11/25 | Loss: 0.00109588
Iteration 12/25 | Loss: 0.00109588
Iteration 13/25 | Loss: 0.00109588
Iteration 14/25 | Loss: 0.00109588
Iteration 15/25 | Loss: 0.00109588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010958826169371605, 0.0010958826169371605, 0.0010958826169371605, 0.0010958826169371605, 0.0010958826169371605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010958826169371605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.46834183
Iteration 2/25 | Loss: 0.00090601
Iteration 3/25 | Loss: 0.00090600
Iteration 4/25 | Loss: 0.00090599
Iteration 5/25 | Loss: 0.00090599
Iteration 6/25 | Loss: 0.00090599
Iteration 7/25 | Loss: 0.00090599
Iteration 8/25 | Loss: 0.00090599
Iteration 9/25 | Loss: 0.00090599
Iteration 10/25 | Loss: 0.00090599
Iteration 11/25 | Loss: 0.00090599
Iteration 12/25 | Loss: 0.00090599
Iteration 13/25 | Loss: 0.00090599
Iteration 14/25 | Loss: 0.00090599
Iteration 15/25 | Loss: 0.00090599
Iteration 16/25 | Loss: 0.00090599
Iteration 17/25 | Loss: 0.00090599
Iteration 18/25 | Loss: 0.00090599
Iteration 19/25 | Loss: 0.00090599
Iteration 20/25 | Loss: 0.00090599
Iteration 21/25 | Loss: 0.00090599
Iteration 22/25 | Loss: 0.00090599
Iteration 23/25 | Loss: 0.00090599
Iteration 24/25 | Loss: 0.00090599
Iteration 25/25 | Loss: 0.00090599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090599
Iteration 2/1000 | Loss: 0.00001679
Iteration 3/1000 | Loss: 0.00001233
Iteration 4/1000 | Loss: 0.00001144
Iteration 5/1000 | Loss: 0.00001082
Iteration 6/1000 | Loss: 0.00001051
Iteration 7/1000 | Loss: 0.00001039
Iteration 8/1000 | Loss: 0.00001038
Iteration 9/1000 | Loss: 0.00001014
Iteration 10/1000 | Loss: 0.00000995
Iteration 11/1000 | Loss: 0.00000980
Iteration 12/1000 | Loss: 0.00000975
Iteration 13/1000 | Loss: 0.00000973
Iteration 14/1000 | Loss: 0.00000972
Iteration 15/1000 | Loss: 0.00000970
Iteration 16/1000 | Loss: 0.00000969
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000961
Iteration 19/1000 | Loss: 0.00000961
Iteration 20/1000 | Loss: 0.00000959
Iteration 21/1000 | Loss: 0.00000959
Iteration 22/1000 | Loss: 0.00000956
Iteration 23/1000 | Loss: 0.00000956
Iteration 24/1000 | Loss: 0.00000955
Iteration 25/1000 | Loss: 0.00000955
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000955
Iteration 28/1000 | Loss: 0.00000955
Iteration 29/1000 | Loss: 0.00000954
Iteration 30/1000 | Loss: 0.00000954
Iteration 31/1000 | Loss: 0.00000951
Iteration 32/1000 | Loss: 0.00000951
Iteration 33/1000 | Loss: 0.00000950
Iteration 34/1000 | Loss: 0.00000950
Iteration 35/1000 | Loss: 0.00000950
Iteration 36/1000 | Loss: 0.00000950
Iteration 37/1000 | Loss: 0.00000950
Iteration 38/1000 | Loss: 0.00000949
Iteration 39/1000 | Loss: 0.00000948
Iteration 40/1000 | Loss: 0.00000948
Iteration 41/1000 | Loss: 0.00000947
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000947
Iteration 44/1000 | Loss: 0.00000947
Iteration 45/1000 | Loss: 0.00000947
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000946
Iteration 48/1000 | Loss: 0.00000946
Iteration 49/1000 | Loss: 0.00000946
Iteration 50/1000 | Loss: 0.00000946
Iteration 51/1000 | Loss: 0.00000943
Iteration 52/1000 | Loss: 0.00000943
Iteration 53/1000 | Loss: 0.00000943
Iteration 54/1000 | Loss: 0.00000943
Iteration 55/1000 | Loss: 0.00000943
Iteration 56/1000 | Loss: 0.00000943
Iteration 57/1000 | Loss: 0.00000943
Iteration 58/1000 | Loss: 0.00000943
Iteration 59/1000 | Loss: 0.00000942
Iteration 60/1000 | Loss: 0.00000942
Iteration 61/1000 | Loss: 0.00000942
Iteration 62/1000 | Loss: 0.00000942
Iteration 63/1000 | Loss: 0.00000942
Iteration 64/1000 | Loss: 0.00000942
Iteration 65/1000 | Loss: 0.00000942
Iteration 66/1000 | Loss: 0.00000942
Iteration 67/1000 | Loss: 0.00000942
Iteration 68/1000 | Loss: 0.00000940
Iteration 69/1000 | Loss: 0.00000939
Iteration 70/1000 | Loss: 0.00000939
Iteration 71/1000 | Loss: 0.00000938
Iteration 72/1000 | Loss: 0.00000938
Iteration 73/1000 | Loss: 0.00000938
Iteration 74/1000 | Loss: 0.00000937
Iteration 75/1000 | Loss: 0.00000937
Iteration 76/1000 | Loss: 0.00000937
Iteration 77/1000 | Loss: 0.00000937
Iteration 78/1000 | Loss: 0.00000936
Iteration 79/1000 | Loss: 0.00000936
Iteration 80/1000 | Loss: 0.00000935
Iteration 81/1000 | Loss: 0.00000935
Iteration 82/1000 | Loss: 0.00000935
Iteration 83/1000 | Loss: 0.00000934
Iteration 84/1000 | Loss: 0.00000934
Iteration 85/1000 | Loss: 0.00000934
Iteration 86/1000 | Loss: 0.00000934
Iteration 87/1000 | Loss: 0.00000934
Iteration 88/1000 | Loss: 0.00000934
Iteration 89/1000 | Loss: 0.00000934
Iteration 90/1000 | Loss: 0.00000933
Iteration 91/1000 | Loss: 0.00000933
Iteration 92/1000 | Loss: 0.00000933
Iteration 93/1000 | Loss: 0.00000933
Iteration 94/1000 | Loss: 0.00000932
Iteration 95/1000 | Loss: 0.00000932
Iteration 96/1000 | Loss: 0.00000932
Iteration 97/1000 | Loss: 0.00000932
Iteration 98/1000 | Loss: 0.00000932
Iteration 99/1000 | Loss: 0.00000931
Iteration 100/1000 | Loss: 0.00000931
Iteration 101/1000 | Loss: 0.00000931
Iteration 102/1000 | Loss: 0.00000931
Iteration 103/1000 | Loss: 0.00000931
Iteration 104/1000 | Loss: 0.00000930
Iteration 105/1000 | Loss: 0.00000930
Iteration 106/1000 | Loss: 0.00000930
Iteration 107/1000 | Loss: 0.00000929
Iteration 108/1000 | Loss: 0.00000929
Iteration 109/1000 | Loss: 0.00000929
Iteration 110/1000 | Loss: 0.00000929
Iteration 111/1000 | Loss: 0.00000929
Iteration 112/1000 | Loss: 0.00000929
Iteration 113/1000 | Loss: 0.00000929
Iteration 114/1000 | Loss: 0.00000929
Iteration 115/1000 | Loss: 0.00000929
Iteration 116/1000 | Loss: 0.00000929
Iteration 117/1000 | Loss: 0.00000929
Iteration 118/1000 | Loss: 0.00000929
Iteration 119/1000 | Loss: 0.00000928
Iteration 120/1000 | Loss: 0.00000928
Iteration 121/1000 | Loss: 0.00000928
Iteration 122/1000 | Loss: 0.00000928
Iteration 123/1000 | Loss: 0.00000927
Iteration 124/1000 | Loss: 0.00000927
Iteration 125/1000 | Loss: 0.00000927
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000927
Iteration 130/1000 | Loss: 0.00000927
Iteration 131/1000 | Loss: 0.00000927
Iteration 132/1000 | Loss: 0.00000927
Iteration 133/1000 | Loss: 0.00000927
Iteration 134/1000 | Loss: 0.00000927
Iteration 135/1000 | Loss: 0.00000926
Iteration 136/1000 | Loss: 0.00000926
Iteration 137/1000 | Loss: 0.00000926
Iteration 138/1000 | Loss: 0.00000926
Iteration 139/1000 | Loss: 0.00000926
Iteration 140/1000 | Loss: 0.00000926
Iteration 141/1000 | Loss: 0.00000926
Iteration 142/1000 | Loss: 0.00000926
Iteration 143/1000 | Loss: 0.00000926
Iteration 144/1000 | Loss: 0.00000926
Iteration 145/1000 | Loss: 0.00000926
Iteration 146/1000 | Loss: 0.00000926
Iteration 147/1000 | Loss: 0.00000926
Iteration 148/1000 | Loss: 0.00000926
Iteration 149/1000 | Loss: 0.00000926
Iteration 150/1000 | Loss: 0.00000926
Iteration 151/1000 | Loss: 0.00000926
Iteration 152/1000 | Loss: 0.00000926
Iteration 153/1000 | Loss: 0.00000925
Iteration 154/1000 | Loss: 0.00000925
Iteration 155/1000 | Loss: 0.00000925
Iteration 156/1000 | Loss: 0.00000925
Iteration 157/1000 | Loss: 0.00000925
Iteration 158/1000 | Loss: 0.00000925
Iteration 159/1000 | Loss: 0.00000925
Iteration 160/1000 | Loss: 0.00000925
Iteration 161/1000 | Loss: 0.00000925
Iteration 162/1000 | Loss: 0.00000925
Iteration 163/1000 | Loss: 0.00000925
Iteration 164/1000 | Loss: 0.00000925
Iteration 165/1000 | Loss: 0.00000925
Iteration 166/1000 | Loss: 0.00000925
Iteration 167/1000 | Loss: 0.00000925
Iteration 168/1000 | Loss: 0.00000925
Iteration 169/1000 | Loss: 0.00000925
Iteration 170/1000 | Loss: 0.00000924
Iteration 171/1000 | Loss: 0.00000924
Iteration 172/1000 | Loss: 0.00000924
Iteration 173/1000 | Loss: 0.00000924
Iteration 174/1000 | Loss: 0.00000924
Iteration 175/1000 | Loss: 0.00000924
Iteration 176/1000 | Loss: 0.00000924
Iteration 177/1000 | Loss: 0.00000924
Iteration 178/1000 | Loss: 0.00000924
Iteration 179/1000 | Loss: 0.00000924
Iteration 180/1000 | Loss: 0.00000924
Iteration 181/1000 | Loss: 0.00000924
Iteration 182/1000 | Loss: 0.00000924
Iteration 183/1000 | Loss: 0.00000924
Iteration 184/1000 | Loss: 0.00000924
Iteration 185/1000 | Loss: 0.00000923
Iteration 186/1000 | Loss: 0.00000923
Iteration 187/1000 | Loss: 0.00000923
Iteration 188/1000 | Loss: 0.00000923
Iteration 189/1000 | Loss: 0.00000923
Iteration 190/1000 | Loss: 0.00000923
Iteration 191/1000 | Loss: 0.00000923
Iteration 192/1000 | Loss: 0.00000923
Iteration 193/1000 | Loss: 0.00000923
Iteration 194/1000 | Loss: 0.00000923
Iteration 195/1000 | Loss: 0.00000923
Iteration 196/1000 | Loss: 0.00000923
Iteration 197/1000 | Loss: 0.00000923
Iteration 198/1000 | Loss: 0.00000923
Iteration 199/1000 | Loss: 0.00000922
Iteration 200/1000 | Loss: 0.00000922
Iteration 201/1000 | Loss: 0.00000922
Iteration 202/1000 | Loss: 0.00000922
Iteration 203/1000 | Loss: 0.00000922
Iteration 204/1000 | Loss: 0.00000922
Iteration 205/1000 | Loss: 0.00000922
Iteration 206/1000 | Loss: 0.00000922
Iteration 207/1000 | Loss: 0.00000922
Iteration 208/1000 | Loss: 0.00000921
Iteration 209/1000 | Loss: 0.00000921
Iteration 210/1000 | Loss: 0.00000921
Iteration 211/1000 | Loss: 0.00000921
Iteration 212/1000 | Loss: 0.00000921
Iteration 213/1000 | Loss: 0.00000921
Iteration 214/1000 | Loss: 0.00000921
Iteration 215/1000 | Loss: 0.00000921
Iteration 216/1000 | Loss: 0.00000921
Iteration 217/1000 | Loss: 0.00000921
Iteration 218/1000 | Loss: 0.00000921
Iteration 219/1000 | Loss: 0.00000921
Iteration 220/1000 | Loss: 0.00000921
Iteration 221/1000 | Loss: 0.00000921
Iteration 222/1000 | Loss: 0.00000921
Iteration 223/1000 | Loss: 0.00000921
Iteration 224/1000 | Loss: 0.00000921
Iteration 225/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [9.212016266246792e-06, 9.212016266246792e-06, 9.212016266246792e-06, 9.212016266246792e-06, 9.212016266246792e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.212016266246792e-06

Optimization complete. Final v2v error: 2.5880210399627686 mm

Highest mean error: 2.901482343673706 mm for frame 237

Lowest mean error: 2.3817138671875 mm for frame 81

Saving results

Total time: 52.49439525604248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966409
Iteration 2/25 | Loss: 0.00966408
Iteration 3/25 | Loss: 0.00966408
Iteration 4/25 | Loss: 0.00966408
Iteration 5/25 | Loss: 0.00966408
Iteration 6/25 | Loss: 0.00966408
Iteration 7/25 | Loss: 0.00966408
Iteration 8/25 | Loss: 0.00966407
Iteration 9/25 | Loss: 0.00966407
Iteration 10/25 | Loss: 0.00966407
Iteration 11/25 | Loss: 0.00966407
Iteration 12/25 | Loss: 0.00966407
Iteration 13/25 | Loss: 0.00966407
Iteration 14/25 | Loss: 0.00966406
Iteration 15/25 | Loss: 0.00966406
Iteration 16/25 | Loss: 0.00966406
Iteration 17/25 | Loss: 0.00966406
Iteration 18/25 | Loss: 0.00966405
Iteration 19/25 | Loss: 0.00966405
Iteration 20/25 | Loss: 0.00966405
Iteration 21/25 | Loss: 0.00966405
Iteration 22/25 | Loss: 0.00966404
Iteration 23/25 | Loss: 0.00966404
Iteration 24/25 | Loss: 0.00966404
Iteration 25/25 | Loss: 0.00966404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63560915
Iteration 2/25 | Loss: 0.19795680
Iteration 3/25 | Loss: 0.19794858
Iteration 4/25 | Loss: 0.19794856
Iteration 5/25 | Loss: 0.19794856
Iteration 6/25 | Loss: 0.19794855
Iteration 7/25 | Loss: 0.19794855
Iteration 8/25 | Loss: 0.19794855
Iteration 9/25 | Loss: 0.19794856
Iteration 10/25 | Loss: 0.19794853
Iteration 11/25 | Loss: 0.19794853
Iteration 12/25 | Loss: 0.19794853
Iteration 13/25 | Loss: 0.19794853
Iteration 14/25 | Loss: 0.19794853
Iteration 15/25 | Loss: 0.19794853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.19794853031635284, 0.19794853031635284, 0.19794853031635284, 0.19794853031635284, 0.19794853031635284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.19794853031635284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.19794853
Iteration 2/1000 | Loss: 0.00429246
Iteration 3/1000 | Loss: 0.00176774
Iteration 4/1000 | Loss: 0.00196922
Iteration 5/1000 | Loss: 0.00040165
Iteration 6/1000 | Loss: 0.00025342
Iteration 7/1000 | Loss: 0.00017619
Iteration 8/1000 | Loss: 0.00053932
Iteration 9/1000 | Loss: 0.00015563
Iteration 10/1000 | Loss: 0.00012312
Iteration 11/1000 | Loss: 0.00008150
Iteration 12/1000 | Loss: 0.00005520
Iteration 13/1000 | Loss: 0.00025058
Iteration 14/1000 | Loss: 0.00004121
Iteration 15/1000 | Loss: 0.00003633
Iteration 16/1000 | Loss: 0.00009053
Iteration 17/1000 | Loss: 0.00002946
Iteration 18/1000 | Loss: 0.00002673
Iteration 19/1000 | Loss: 0.00002419
Iteration 20/1000 | Loss: 0.00021290
Iteration 21/1000 | Loss: 0.00002158
Iteration 22/1000 | Loss: 0.00002011
Iteration 23/1000 | Loss: 0.00012292
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001815
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001739
Iteration 28/1000 | Loss: 0.00001705
Iteration 29/1000 | Loss: 0.00001677
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001563
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001560
Iteration 40/1000 | Loss: 0.00001560
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001557
Iteration 43/1000 | Loss: 0.00001557
Iteration 44/1000 | Loss: 0.00001555
Iteration 45/1000 | Loss: 0.00001555
Iteration 46/1000 | Loss: 0.00001554
Iteration 47/1000 | Loss: 0.00001554
Iteration 48/1000 | Loss: 0.00001554
Iteration 49/1000 | Loss: 0.00001554
Iteration 50/1000 | Loss: 0.00001553
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001551
Iteration 55/1000 | Loss: 0.00001551
Iteration 56/1000 | Loss: 0.00001551
Iteration 57/1000 | Loss: 0.00001551
Iteration 58/1000 | Loss: 0.00001551
Iteration 59/1000 | Loss: 0.00001551
Iteration 60/1000 | Loss: 0.00001550
Iteration 61/1000 | Loss: 0.00001550
Iteration 62/1000 | Loss: 0.00001550
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001549
Iteration 67/1000 | Loss: 0.00001549
Iteration 68/1000 | Loss: 0.00001549
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001549
Iteration 72/1000 | Loss: 0.00001548
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001548
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001548
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001544
Iteration 97/1000 | Loss: 0.00001544
Iteration 98/1000 | Loss: 0.00001544
Iteration 99/1000 | Loss: 0.00001544
Iteration 100/1000 | Loss: 0.00001544
Iteration 101/1000 | Loss: 0.00001543
Iteration 102/1000 | Loss: 0.00001543
Iteration 103/1000 | Loss: 0.00001543
Iteration 104/1000 | Loss: 0.00001543
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001542
Iteration 110/1000 | Loss: 0.00001542
Iteration 111/1000 | Loss: 0.00001542
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001541
Iteration 123/1000 | Loss: 0.00001541
Iteration 124/1000 | Loss: 0.00001541
Iteration 125/1000 | Loss: 0.00001540
Iteration 126/1000 | Loss: 0.00001540
Iteration 127/1000 | Loss: 0.00001540
Iteration 128/1000 | Loss: 0.00001540
Iteration 129/1000 | Loss: 0.00001540
Iteration 130/1000 | Loss: 0.00001540
Iteration 131/1000 | Loss: 0.00001540
Iteration 132/1000 | Loss: 0.00001540
Iteration 133/1000 | Loss: 0.00001540
Iteration 134/1000 | Loss: 0.00001540
Iteration 135/1000 | Loss: 0.00001540
Iteration 136/1000 | Loss: 0.00001540
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5403378711198457e-05, 1.5403378711198457e-05, 1.5403378711198457e-05, 1.5403378711198457e-05, 1.5403378711198457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5403378711198457e-05

Optimization complete. Final v2v error: 3.394718647003174 mm

Highest mean error: 3.587972640991211 mm for frame 238

Lowest mean error: 3.201141119003296 mm for frame 123

Saving results

Total time: 70.59559178352356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042291
Iteration 2/25 | Loss: 0.01042290
Iteration 3/25 | Loss: 0.01042290
Iteration 4/25 | Loss: 0.01042290
Iteration 5/25 | Loss: 0.01042290
Iteration 6/25 | Loss: 0.00255527
Iteration 7/25 | Loss: 0.00172659
Iteration 8/25 | Loss: 0.00173948
Iteration 9/25 | Loss: 0.00149027
Iteration 10/25 | Loss: 0.00148063
Iteration 11/25 | Loss: 0.00145389
Iteration 12/25 | Loss: 0.00142689
Iteration 13/25 | Loss: 0.00141567
Iteration 14/25 | Loss: 0.00141287
Iteration 15/25 | Loss: 0.00140434
Iteration 16/25 | Loss: 0.00139264
Iteration 17/25 | Loss: 0.00138789
Iteration 18/25 | Loss: 0.00138626
Iteration 19/25 | Loss: 0.00138458
Iteration 20/25 | Loss: 0.00138751
Iteration 21/25 | Loss: 0.00137649
Iteration 22/25 | Loss: 0.00137787
Iteration 23/25 | Loss: 0.00137294
Iteration 24/25 | Loss: 0.00137208
Iteration 25/25 | Loss: 0.00137099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31092584
Iteration 2/25 | Loss: 0.00316909
Iteration 3/25 | Loss: 0.00309582
Iteration 4/25 | Loss: 0.00309582
Iteration 5/25 | Loss: 0.00309582
Iteration 6/25 | Loss: 0.00309582
Iteration 7/25 | Loss: 0.00309582
Iteration 8/25 | Loss: 0.00309582
Iteration 9/25 | Loss: 0.00309582
Iteration 10/25 | Loss: 0.00309582
Iteration 11/25 | Loss: 0.00309582
Iteration 12/25 | Loss: 0.00309582
Iteration 13/25 | Loss: 0.00309582
Iteration 14/25 | Loss: 0.00309582
Iteration 15/25 | Loss: 0.00309582
Iteration 16/25 | Loss: 0.00309582
Iteration 17/25 | Loss: 0.00309582
Iteration 18/25 | Loss: 0.00309582
Iteration 19/25 | Loss: 0.00309582
Iteration 20/25 | Loss: 0.00309582
Iteration 21/25 | Loss: 0.00309582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003095818916335702, 0.003095818916335702, 0.003095818916335702, 0.003095818916335702, 0.003095818916335702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003095818916335702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00309582
Iteration 2/1000 | Loss: 0.00314262
Iteration 3/1000 | Loss: 0.00413388
Iteration 4/1000 | Loss: 0.00521414
Iteration 5/1000 | Loss: 0.00293795
Iteration 6/1000 | Loss: 0.00107267
Iteration 7/1000 | Loss: 0.00213581
Iteration 8/1000 | Loss: 0.00116219
Iteration 9/1000 | Loss: 0.00202305
Iteration 10/1000 | Loss: 0.00183847
Iteration 11/1000 | Loss: 0.00041821
Iteration 12/1000 | Loss: 0.00132382
Iteration 13/1000 | Loss: 0.00120063
Iteration 14/1000 | Loss: 0.00045819
Iteration 15/1000 | Loss: 0.00090353
Iteration 16/1000 | Loss: 0.00139025
Iteration 17/1000 | Loss: 0.00026125
Iteration 18/1000 | Loss: 0.00032999
Iteration 19/1000 | Loss: 0.00016252
Iteration 20/1000 | Loss: 0.00053020
Iteration 21/1000 | Loss: 0.00028822
Iteration 22/1000 | Loss: 0.00023586
Iteration 23/1000 | Loss: 0.00035643
Iteration 24/1000 | Loss: 0.00018260
Iteration 25/1000 | Loss: 0.00015914
Iteration 26/1000 | Loss: 0.00175311
Iteration 27/1000 | Loss: 0.00181989
Iteration 28/1000 | Loss: 0.00057585
Iteration 29/1000 | Loss: 0.00054070
Iteration 30/1000 | Loss: 0.00486303
Iteration 31/1000 | Loss: 0.00261120
Iteration 32/1000 | Loss: 0.00358417
Iteration 33/1000 | Loss: 0.00175995
Iteration 34/1000 | Loss: 0.00132348
Iteration 35/1000 | Loss: 0.00320522
Iteration 36/1000 | Loss: 0.00536311
Iteration 37/1000 | Loss: 0.00317041
Iteration 38/1000 | Loss: 0.00244742
Iteration 39/1000 | Loss: 0.00324876
Iteration 40/1000 | Loss: 0.00270143
Iteration 41/1000 | Loss: 0.00246022
Iteration 42/1000 | Loss: 0.00176448
Iteration 43/1000 | Loss: 0.00201325
Iteration 44/1000 | Loss: 0.00213364
Iteration 45/1000 | Loss: 0.00115459
Iteration 46/1000 | Loss: 0.00122606
Iteration 47/1000 | Loss: 0.00081090
Iteration 48/1000 | Loss: 0.00106126
Iteration 49/1000 | Loss: 0.00096331
Iteration 50/1000 | Loss: 0.00081336
Iteration 51/1000 | Loss: 0.00240247
Iteration 52/1000 | Loss: 0.00188677
Iteration 53/1000 | Loss: 0.00078041
Iteration 54/1000 | Loss: 0.00030665
Iteration 55/1000 | Loss: 0.00075189
Iteration 56/1000 | Loss: 0.00093058
Iteration 57/1000 | Loss: 0.00045112
Iteration 58/1000 | Loss: 0.00014608
Iteration 59/1000 | Loss: 0.00052208
Iteration 60/1000 | Loss: 0.00077214
Iteration 61/1000 | Loss: 0.00061605
Iteration 62/1000 | Loss: 0.00072535
Iteration 63/1000 | Loss: 0.00063453
Iteration 64/1000 | Loss: 0.00018259
Iteration 65/1000 | Loss: 0.00098557
Iteration 66/1000 | Loss: 0.00103117
Iteration 67/1000 | Loss: 0.00248892
Iteration 68/1000 | Loss: 0.00066372
Iteration 69/1000 | Loss: 0.00027365
Iteration 70/1000 | Loss: 0.00034566
Iteration 71/1000 | Loss: 0.00019513
Iteration 72/1000 | Loss: 0.00122774
Iteration 73/1000 | Loss: 0.00082365
Iteration 74/1000 | Loss: 0.00052170
Iteration 75/1000 | Loss: 0.00162740
Iteration 76/1000 | Loss: 0.00007749
Iteration 77/1000 | Loss: 0.00031235
Iteration 78/1000 | Loss: 0.00104090
Iteration 79/1000 | Loss: 0.00091570
Iteration 80/1000 | Loss: 0.00173357
Iteration 81/1000 | Loss: 0.00057424
Iteration 82/1000 | Loss: 0.00105734
Iteration 83/1000 | Loss: 0.00030646
Iteration 84/1000 | Loss: 0.00026517
Iteration 85/1000 | Loss: 0.00024475
Iteration 86/1000 | Loss: 0.00046721
Iteration 87/1000 | Loss: 0.00027967
Iteration 88/1000 | Loss: 0.00040720
Iteration 89/1000 | Loss: 0.00058102
Iteration 90/1000 | Loss: 0.00038256
Iteration 91/1000 | Loss: 0.00090511
Iteration 92/1000 | Loss: 0.00069214
Iteration 93/1000 | Loss: 0.00006740
Iteration 94/1000 | Loss: 0.00010636
Iteration 95/1000 | Loss: 0.00005058
Iteration 96/1000 | Loss: 0.00013220
Iteration 97/1000 | Loss: 0.00009819
Iteration 98/1000 | Loss: 0.00009633
Iteration 99/1000 | Loss: 0.00008736
Iteration 100/1000 | Loss: 0.00056841
Iteration 101/1000 | Loss: 0.00067604
Iteration 102/1000 | Loss: 0.00054500
Iteration 103/1000 | Loss: 0.00043736
Iteration 104/1000 | Loss: 0.00039224
Iteration 105/1000 | Loss: 0.00019084
Iteration 106/1000 | Loss: 0.00070797
Iteration 107/1000 | Loss: 0.00028268
Iteration 108/1000 | Loss: 0.00012950
Iteration 109/1000 | Loss: 0.00024857
Iteration 110/1000 | Loss: 0.00022803
Iteration 111/1000 | Loss: 0.00020838
Iteration 112/1000 | Loss: 0.00037240
Iteration 113/1000 | Loss: 0.00022679
Iteration 114/1000 | Loss: 0.00027696
Iteration 115/1000 | Loss: 0.00019006
Iteration 116/1000 | Loss: 0.00029696
Iteration 117/1000 | Loss: 0.00036978
Iteration 118/1000 | Loss: 0.00023919
Iteration 119/1000 | Loss: 0.00022560
Iteration 120/1000 | Loss: 0.00025458
Iteration 121/1000 | Loss: 0.00026922
Iteration 122/1000 | Loss: 0.00036748
Iteration 123/1000 | Loss: 0.00057646
Iteration 124/1000 | Loss: 0.00047736
Iteration 125/1000 | Loss: 0.00168878
Iteration 126/1000 | Loss: 0.00096731
Iteration 127/1000 | Loss: 0.00079064
Iteration 128/1000 | Loss: 0.00032460
Iteration 129/1000 | Loss: 0.00034039
Iteration 130/1000 | Loss: 0.00019880
Iteration 131/1000 | Loss: 0.00015203
Iteration 132/1000 | Loss: 0.00004880
Iteration 133/1000 | Loss: 0.00034533
Iteration 134/1000 | Loss: 0.00023416
Iteration 135/1000 | Loss: 0.00025485
Iteration 136/1000 | Loss: 0.00009369
Iteration 137/1000 | Loss: 0.00014224
Iteration 138/1000 | Loss: 0.00046935
Iteration 139/1000 | Loss: 0.00013686
Iteration 140/1000 | Loss: 0.00012419
Iteration 141/1000 | Loss: 0.00011138
Iteration 142/1000 | Loss: 0.00044576
Iteration 143/1000 | Loss: 0.00103197
Iteration 144/1000 | Loss: 0.00035230
Iteration 145/1000 | Loss: 0.00023605
Iteration 146/1000 | Loss: 0.00050856
Iteration 147/1000 | Loss: 0.00005187
Iteration 148/1000 | Loss: 0.00067452
Iteration 149/1000 | Loss: 0.00042139
Iteration 150/1000 | Loss: 0.00020177
Iteration 151/1000 | Loss: 0.00011004
Iteration 152/1000 | Loss: 0.00055572
Iteration 153/1000 | Loss: 0.00034880
Iteration 154/1000 | Loss: 0.00059405
Iteration 155/1000 | Loss: 0.00045913
Iteration 156/1000 | Loss: 0.00039046
Iteration 157/1000 | Loss: 0.00045330
Iteration 158/1000 | Loss: 0.00034784
Iteration 159/1000 | Loss: 0.00022837
Iteration 160/1000 | Loss: 0.00054696
Iteration 161/1000 | Loss: 0.00008388
Iteration 162/1000 | Loss: 0.00008221
Iteration 163/1000 | Loss: 0.00016802
Iteration 164/1000 | Loss: 0.00020965
Iteration 165/1000 | Loss: 0.00013252
Iteration 166/1000 | Loss: 0.00010423
Iteration 167/1000 | Loss: 0.00004320
Iteration 168/1000 | Loss: 0.00003921
Iteration 169/1000 | Loss: 0.00004283
Iteration 170/1000 | Loss: 0.00003100
Iteration 171/1000 | Loss: 0.00003640
Iteration 172/1000 | Loss: 0.00003518
Iteration 173/1000 | Loss: 0.00011121
Iteration 174/1000 | Loss: 0.00005359
Iteration 175/1000 | Loss: 0.00003535
Iteration 176/1000 | Loss: 0.00003371
Iteration 177/1000 | Loss: 0.00031595
Iteration 178/1000 | Loss: 0.00026791
Iteration 179/1000 | Loss: 0.00079452
Iteration 180/1000 | Loss: 0.00017330
Iteration 181/1000 | Loss: 0.00009586
Iteration 182/1000 | Loss: 0.00007588
Iteration 183/1000 | Loss: 0.00003493
Iteration 184/1000 | Loss: 0.00004486
Iteration 185/1000 | Loss: 0.00008680
Iteration 186/1000 | Loss: 0.00007379
Iteration 187/1000 | Loss: 0.00004192
Iteration 188/1000 | Loss: 0.00005577
Iteration 189/1000 | Loss: 0.00004892
Iteration 190/1000 | Loss: 0.00004023
Iteration 191/1000 | Loss: 0.00013476
Iteration 192/1000 | Loss: 0.00003730
Iteration 193/1000 | Loss: 0.00003420
Iteration 194/1000 | Loss: 0.00003282
Iteration 195/1000 | Loss: 0.00031761
Iteration 196/1000 | Loss: 0.00062679
Iteration 197/1000 | Loss: 0.00068049
Iteration 198/1000 | Loss: 0.00005347
Iteration 199/1000 | Loss: 0.00003894
Iteration 200/1000 | Loss: 0.00023678
Iteration 201/1000 | Loss: 0.00004392
Iteration 202/1000 | Loss: 0.00005459
Iteration 203/1000 | Loss: 0.00043671
Iteration 204/1000 | Loss: 0.00020302
Iteration 205/1000 | Loss: 0.00012938
Iteration 206/1000 | Loss: 0.00002497
Iteration 207/1000 | Loss: 0.00002419
Iteration 208/1000 | Loss: 0.00007671
Iteration 209/1000 | Loss: 0.00002168
Iteration 210/1000 | Loss: 0.00002066
Iteration 211/1000 | Loss: 0.00004086
Iteration 212/1000 | Loss: 0.00007534
Iteration 213/1000 | Loss: 0.00002695
Iteration 214/1000 | Loss: 0.00005497
Iteration 215/1000 | Loss: 0.00001980
Iteration 216/1000 | Loss: 0.00002826
Iteration 217/1000 | Loss: 0.00001902
Iteration 218/1000 | Loss: 0.00001877
Iteration 219/1000 | Loss: 0.00002621
Iteration 220/1000 | Loss: 0.00003627
Iteration 221/1000 | Loss: 0.00001996
Iteration 222/1000 | Loss: 0.00001828
Iteration 223/1000 | Loss: 0.00001827
Iteration 224/1000 | Loss: 0.00001827
Iteration 225/1000 | Loss: 0.00002001
Iteration 226/1000 | Loss: 0.00002156
Iteration 227/1000 | Loss: 0.00002387
Iteration 228/1000 | Loss: 0.00001929
Iteration 229/1000 | Loss: 0.00001783
Iteration 230/1000 | Loss: 0.00001783
Iteration 231/1000 | Loss: 0.00001999
Iteration 232/1000 | Loss: 0.00001779
Iteration 233/1000 | Loss: 0.00001779
Iteration 234/1000 | Loss: 0.00001779
Iteration 235/1000 | Loss: 0.00001779
Iteration 236/1000 | Loss: 0.00001779
Iteration 237/1000 | Loss: 0.00001779
Iteration 238/1000 | Loss: 0.00001779
Iteration 239/1000 | Loss: 0.00001779
Iteration 240/1000 | Loss: 0.00001779
Iteration 241/1000 | Loss: 0.00001779
Iteration 242/1000 | Loss: 0.00001778
Iteration 243/1000 | Loss: 0.00001778
Iteration 244/1000 | Loss: 0.00001778
Iteration 245/1000 | Loss: 0.00001778
Iteration 246/1000 | Loss: 0.00001777
Iteration 247/1000 | Loss: 0.00001777
Iteration 248/1000 | Loss: 0.00001837
Iteration 249/1000 | Loss: 0.00001843
Iteration 250/1000 | Loss: 0.00001772
Iteration 251/1000 | Loss: 0.00001772
Iteration 252/1000 | Loss: 0.00001772
Iteration 253/1000 | Loss: 0.00001772
Iteration 254/1000 | Loss: 0.00001772
Iteration 255/1000 | Loss: 0.00001772
Iteration 256/1000 | Loss: 0.00001772
Iteration 257/1000 | Loss: 0.00001772
Iteration 258/1000 | Loss: 0.00001772
Iteration 259/1000 | Loss: 0.00001772
Iteration 260/1000 | Loss: 0.00001772
Iteration 261/1000 | Loss: 0.00001772
Iteration 262/1000 | Loss: 0.00001772
Iteration 263/1000 | Loss: 0.00001772
Iteration 264/1000 | Loss: 0.00001772
Iteration 265/1000 | Loss: 0.00001772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.7716854927130044e-05, 1.7716854927130044e-05, 1.7716854927130044e-05, 1.7716854927130044e-05, 1.7716854927130044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7716854927130044e-05

Optimization complete. Final v2v error: 3.1395249366760254 mm

Highest mean error: 12.027770042419434 mm for frame 48

Lowest mean error: 2.562364101409912 mm for frame 75

Saving results

Total time: 407.88831901550293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455217
Iteration 2/25 | Loss: 0.00121541
Iteration 3/25 | Loss: 0.00114400
Iteration 4/25 | Loss: 0.00113589
Iteration 5/25 | Loss: 0.00113257
Iteration 6/25 | Loss: 0.00113227
Iteration 7/25 | Loss: 0.00113227
Iteration 8/25 | Loss: 0.00113227
Iteration 9/25 | Loss: 0.00113227
Iteration 10/25 | Loss: 0.00113227
Iteration 11/25 | Loss: 0.00113227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011322747450321913, 0.0011322747450321913, 0.0011322747450321913, 0.0011322747450321913, 0.0011322747450321913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011322747450321913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34963596
Iteration 2/25 | Loss: 0.00081394
Iteration 3/25 | Loss: 0.00081394
Iteration 4/25 | Loss: 0.00081394
Iteration 5/25 | Loss: 0.00081393
Iteration 6/25 | Loss: 0.00081393
Iteration 7/25 | Loss: 0.00081393
Iteration 8/25 | Loss: 0.00081393
Iteration 9/25 | Loss: 0.00081393
Iteration 10/25 | Loss: 0.00081393
Iteration 11/25 | Loss: 0.00081393
Iteration 12/25 | Loss: 0.00081393
Iteration 13/25 | Loss: 0.00081393
Iteration 14/25 | Loss: 0.00081393
Iteration 15/25 | Loss: 0.00081393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008139336132444441, 0.0008139336132444441, 0.0008139336132444441, 0.0008139336132444441, 0.0008139336132444441]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008139336132444441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081393
Iteration 2/1000 | Loss: 0.00002341
Iteration 3/1000 | Loss: 0.00001600
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001423
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001316
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001288
Iteration 14/1000 | Loss: 0.00001283
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001281
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001280
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001280
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001279
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001276
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001275
Iteration 36/1000 | Loss: 0.00001275
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001272
Iteration 42/1000 | Loss: 0.00001272
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001270
Iteration 46/1000 | Loss: 0.00001270
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001270
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001261
Iteration 79/1000 | Loss: 0.00001260
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001257
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001256
Iteration 92/1000 | Loss: 0.00001256
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.2558822163555305e-05, 1.2558822163555305e-05, 1.2558822163555305e-05, 1.2558822163555305e-05, 1.2558822163555305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2558822163555305e-05

Optimization complete. Final v2v error: 2.974738836288452 mm

Highest mean error: 3.267069101333618 mm for frame 177

Lowest mean error: 2.7782959938049316 mm for frame 0

Saving results

Total time: 34.58552169799805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785386
Iteration 2/25 | Loss: 0.00135789
Iteration 3/25 | Loss: 0.00122600
Iteration 4/25 | Loss: 0.00121638
Iteration 5/25 | Loss: 0.00121496
Iteration 6/25 | Loss: 0.00121496
Iteration 7/25 | Loss: 0.00121496
Iteration 8/25 | Loss: 0.00121496
Iteration 9/25 | Loss: 0.00121496
Iteration 10/25 | Loss: 0.00121496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012149588437750936, 0.0012149588437750936, 0.0012149588437750936, 0.0012149588437750936, 0.0012149588437750936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012149588437750936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94136226
Iteration 2/25 | Loss: 0.00074468
Iteration 3/25 | Loss: 0.00074465
Iteration 4/25 | Loss: 0.00074465
Iteration 5/25 | Loss: 0.00074465
Iteration 6/25 | Loss: 0.00074465
Iteration 7/25 | Loss: 0.00074465
Iteration 8/25 | Loss: 0.00074465
Iteration 9/25 | Loss: 0.00074465
Iteration 10/25 | Loss: 0.00074465
Iteration 11/25 | Loss: 0.00074465
Iteration 12/25 | Loss: 0.00074465
Iteration 13/25 | Loss: 0.00074465
Iteration 14/25 | Loss: 0.00074465
Iteration 15/25 | Loss: 0.00074465
Iteration 16/25 | Loss: 0.00074465
Iteration 17/25 | Loss: 0.00074465
Iteration 18/25 | Loss: 0.00074465
Iteration 19/25 | Loss: 0.00074465
Iteration 20/25 | Loss: 0.00074465
Iteration 21/25 | Loss: 0.00074465
Iteration 22/25 | Loss: 0.00074465
Iteration 23/25 | Loss: 0.00074465
Iteration 24/25 | Loss: 0.00074465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007446460076607764, 0.0007446460076607764, 0.0007446460076607764, 0.0007446460076607764, 0.0007446460076607764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007446460076607764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074465
Iteration 2/1000 | Loss: 0.00003095
Iteration 3/1000 | Loss: 0.00002110
Iteration 4/1000 | Loss: 0.00001935
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001793
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001748
Iteration 9/1000 | Loss: 0.00001721
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001668
Iteration 14/1000 | Loss: 0.00001662
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001647
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001645
Iteration 38/1000 | Loss: 0.00001645
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001642
Iteration 42/1000 | Loss: 0.00001642
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001640
Iteration 45/1000 | Loss: 0.00001639
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001639
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001638
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001630
Iteration 61/1000 | Loss: 0.00001630
Iteration 62/1000 | Loss: 0.00001630
Iteration 63/1000 | Loss: 0.00001630
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001622
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001621
Iteration 83/1000 | Loss: 0.00001621
Iteration 84/1000 | Loss: 0.00001621
Iteration 85/1000 | Loss: 0.00001619
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001618
Iteration 91/1000 | Loss: 0.00001618
Iteration 92/1000 | Loss: 0.00001618
Iteration 93/1000 | Loss: 0.00001618
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001615
Iteration 100/1000 | Loss: 0.00001615
Iteration 101/1000 | Loss: 0.00001615
Iteration 102/1000 | Loss: 0.00001615
Iteration 103/1000 | Loss: 0.00001615
Iteration 104/1000 | Loss: 0.00001615
Iteration 105/1000 | Loss: 0.00001615
Iteration 106/1000 | Loss: 0.00001615
Iteration 107/1000 | Loss: 0.00001615
Iteration 108/1000 | Loss: 0.00001614
Iteration 109/1000 | Loss: 0.00001614
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001610
Iteration 116/1000 | Loss: 0.00001610
Iteration 117/1000 | Loss: 0.00001610
Iteration 118/1000 | Loss: 0.00001610
Iteration 119/1000 | Loss: 0.00001610
Iteration 120/1000 | Loss: 0.00001610
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001609
Iteration 124/1000 | Loss: 0.00001609
Iteration 125/1000 | Loss: 0.00001609
Iteration 126/1000 | Loss: 0.00001609
Iteration 127/1000 | Loss: 0.00001609
Iteration 128/1000 | Loss: 0.00001609
Iteration 129/1000 | Loss: 0.00001609
Iteration 130/1000 | Loss: 0.00001608
Iteration 131/1000 | Loss: 0.00001608
Iteration 132/1000 | Loss: 0.00001608
Iteration 133/1000 | Loss: 0.00001608
Iteration 134/1000 | Loss: 0.00001608
Iteration 135/1000 | Loss: 0.00001608
Iteration 136/1000 | Loss: 0.00001608
Iteration 137/1000 | Loss: 0.00001608
Iteration 138/1000 | Loss: 0.00001608
Iteration 139/1000 | Loss: 0.00001608
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001608
Iteration 142/1000 | Loss: 0.00001608
Iteration 143/1000 | Loss: 0.00001608
Iteration 144/1000 | Loss: 0.00001608
Iteration 145/1000 | Loss: 0.00001608
Iteration 146/1000 | Loss: 0.00001608
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001608
Iteration 150/1000 | Loss: 0.00001608
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001608
Iteration 153/1000 | Loss: 0.00001608
Iteration 154/1000 | Loss: 0.00001608
Iteration 155/1000 | Loss: 0.00001608
Iteration 156/1000 | Loss: 0.00001608
Iteration 157/1000 | Loss: 0.00001608
Iteration 158/1000 | Loss: 0.00001608
Iteration 159/1000 | Loss: 0.00001608
Iteration 160/1000 | Loss: 0.00001608
Iteration 161/1000 | Loss: 0.00001608
Iteration 162/1000 | Loss: 0.00001608
Iteration 163/1000 | Loss: 0.00001608
Iteration 164/1000 | Loss: 0.00001608
Iteration 165/1000 | Loss: 0.00001608
Iteration 166/1000 | Loss: 0.00001608
Iteration 167/1000 | Loss: 0.00001608
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001608
Iteration 170/1000 | Loss: 0.00001608
Iteration 171/1000 | Loss: 0.00001608
Iteration 172/1000 | Loss: 0.00001608
Iteration 173/1000 | Loss: 0.00001608
Iteration 174/1000 | Loss: 0.00001608
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001608
Iteration 178/1000 | Loss: 0.00001608
Iteration 179/1000 | Loss: 0.00001608
Iteration 180/1000 | Loss: 0.00001608
Iteration 181/1000 | Loss: 0.00001608
Iteration 182/1000 | Loss: 0.00001608
Iteration 183/1000 | Loss: 0.00001608
Iteration 184/1000 | Loss: 0.00001608
Iteration 185/1000 | Loss: 0.00001608
Iteration 186/1000 | Loss: 0.00001608
Iteration 187/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.6084777598734945e-05, 1.6084777598734945e-05, 1.6084777598734945e-05, 1.6084777598734945e-05, 1.6084777598734945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6084777598734945e-05

Optimization complete. Final v2v error: 3.316304922103882 mm

Highest mean error: 3.803215265274048 mm for frame 172

Lowest mean error: 3.0397253036499023 mm for frame 51

Saving results

Total time: 41.860169887542725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044911
Iteration 2/25 | Loss: 0.00206053
Iteration 3/25 | Loss: 0.00157672
Iteration 4/25 | Loss: 0.00135397
Iteration 5/25 | Loss: 0.00141573
Iteration 6/25 | Loss: 0.00135859
Iteration 7/25 | Loss: 0.00127204
Iteration 8/25 | Loss: 0.00118802
Iteration 9/25 | Loss: 0.00118747
Iteration 10/25 | Loss: 0.00120999
Iteration 11/25 | Loss: 0.00117046
Iteration 12/25 | Loss: 0.00116051
Iteration 13/25 | Loss: 0.00115487
Iteration 14/25 | Loss: 0.00115380
Iteration 15/25 | Loss: 0.00115377
Iteration 16/25 | Loss: 0.00115377
Iteration 17/25 | Loss: 0.00115377
Iteration 18/25 | Loss: 0.00115377
Iteration 19/25 | Loss: 0.00115377
Iteration 20/25 | Loss: 0.00115377
Iteration 21/25 | Loss: 0.00115377
Iteration 22/25 | Loss: 0.00115377
Iteration 23/25 | Loss: 0.00115377
Iteration 24/25 | Loss: 0.00115376
Iteration 25/25 | Loss: 0.00115376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33012819
Iteration 2/25 | Loss: 0.00197120
Iteration 3/25 | Loss: 0.00094337
Iteration 4/25 | Loss: 0.00094337
Iteration 5/25 | Loss: 0.00094337
Iteration 6/25 | Loss: 0.00094337
Iteration 7/25 | Loss: 0.00094337
Iteration 8/25 | Loss: 0.00094337
Iteration 9/25 | Loss: 0.00094337
Iteration 10/25 | Loss: 0.00094337
Iteration 11/25 | Loss: 0.00094337
Iteration 12/25 | Loss: 0.00094337
Iteration 13/25 | Loss: 0.00094337
Iteration 14/25 | Loss: 0.00094337
Iteration 15/25 | Loss: 0.00094337
Iteration 16/25 | Loss: 0.00094337
Iteration 17/25 | Loss: 0.00094337
Iteration 18/25 | Loss: 0.00094337
Iteration 19/25 | Loss: 0.00094337
Iteration 20/25 | Loss: 0.00094337
Iteration 21/25 | Loss: 0.00094337
Iteration 22/25 | Loss: 0.00094337
Iteration 23/25 | Loss: 0.00094337
Iteration 24/25 | Loss: 0.00094337
Iteration 25/25 | Loss: 0.00094337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094337
Iteration 2/1000 | Loss: 0.00002749
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00002035
Iteration 5/1000 | Loss: 0.00001967
Iteration 6/1000 | Loss: 0.00001919
Iteration 7/1000 | Loss: 0.00001874
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001821
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001750
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001748
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001734
Iteration 27/1000 | Loss: 0.00001733
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001730
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001730
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001726
Iteration 47/1000 | Loss: 0.00001726
Iteration 48/1000 | Loss: 0.00001726
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001725
Iteration 60/1000 | Loss: 0.00001725
Iteration 61/1000 | Loss: 0.00001725
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001723
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001722
Iteration 72/1000 | Loss: 0.00001722
Iteration 73/1000 | Loss: 0.00001722
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001722
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001721
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001720
Iteration 93/1000 | Loss: 0.00001720
Iteration 94/1000 | Loss: 0.00001720
Iteration 95/1000 | Loss: 0.00001720
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001718
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001717
Iteration 113/1000 | Loss: 0.00001716
Iteration 114/1000 | Loss: 0.00001716
Iteration 115/1000 | Loss: 0.00001716
Iteration 116/1000 | Loss: 0.00001716
Iteration 117/1000 | Loss: 0.00001716
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001716
Iteration 120/1000 | Loss: 0.00001716
Iteration 121/1000 | Loss: 0.00001716
Iteration 122/1000 | Loss: 0.00001716
Iteration 123/1000 | Loss: 0.00001716
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001715
Iteration 129/1000 | Loss: 0.00001715
Iteration 130/1000 | Loss: 0.00001715
Iteration 131/1000 | Loss: 0.00001715
Iteration 132/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.7153628505184315e-05, 1.7153628505184315e-05, 1.7153628505184315e-05, 1.7153628505184315e-05, 1.7153628505184315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7153628505184315e-05

Optimization complete. Final v2v error: 3.519097089767456 mm

Highest mean error: 3.807758331298828 mm for frame 239

Lowest mean error: 3.3829116821289062 mm for frame 167

Saving results

Total time: 61.46405816078186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380492
Iteration 2/25 | Loss: 0.00118907
Iteration 3/25 | Loss: 0.00110202
Iteration 4/25 | Loss: 0.00109369
Iteration 5/25 | Loss: 0.00109151
Iteration 6/25 | Loss: 0.00109121
Iteration 7/25 | Loss: 0.00109121
Iteration 8/25 | Loss: 0.00109121
Iteration 9/25 | Loss: 0.00109121
Iteration 10/25 | Loss: 0.00109121
Iteration 11/25 | Loss: 0.00109121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010912071447819471, 0.0010912071447819471, 0.0010912071447819471, 0.0010912071447819471, 0.0010912071447819471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010912071447819471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37835109
Iteration 2/25 | Loss: 0.00084043
Iteration 3/25 | Loss: 0.00084043
Iteration 4/25 | Loss: 0.00084043
Iteration 5/25 | Loss: 0.00084043
Iteration 6/25 | Loss: 0.00084043
Iteration 7/25 | Loss: 0.00084043
Iteration 8/25 | Loss: 0.00084043
Iteration 9/25 | Loss: 0.00084043
Iteration 10/25 | Loss: 0.00084043
Iteration 11/25 | Loss: 0.00084043
Iteration 12/25 | Loss: 0.00084043
Iteration 13/25 | Loss: 0.00084043
Iteration 14/25 | Loss: 0.00084043
Iteration 15/25 | Loss: 0.00084043
Iteration 16/25 | Loss: 0.00084043
Iteration 17/25 | Loss: 0.00084043
Iteration 18/25 | Loss: 0.00084043
Iteration 19/25 | Loss: 0.00084043
Iteration 20/25 | Loss: 0.00084043
Iteration 21/25 | Loss: 0.00084043
Iteration 22/25 | Loss: 0.00084043
Iteration 23/25 | Loss: 0.00084043
Iteration 24/25 | Loss: 0.00084043
Iteration 25/25 | Loss: 0.00084043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084043
Iteration 2/1000 | Loss: 0.00002053
Iteration 3/1000 | Loss: 0.00001403
Iteration 4/1000 | Loss: 0.00001277
Iteration 5/1000 | Loss: 0.00001218
Iteration 6/1000 | Loss: 0.00001168
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001103
Iteration 10/1000 | Loss: 0.00001093
Iteration 11/1000 | Loss: 0.00001084
Iteration 12/1000 | Loss: 0.00001076
Iteration 13/1000 | Loss: 0.00001071
Iteration 14/1000 | Loss: 0.00001068
Iteration 15/1000 | Loss: 0.00001067
Iteration 16/1000 | Loss: 0.00001066
Iteration 17/1000 | Loss: 0.00001066
Iteration 18/1000 | Loss: 0.00001065
Iteration 19/1000 | Loss: 0.00001064
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001052
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001050
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001049
Iteration 32/1000 | Loss: 0.00001049
Iteration 33/1000 | Loss: 0.00001049
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001040
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001038
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001035
Iteration 54/1000 | Loss: 0.00001035
Iteration 55/1000 | Loss: 0.00001034
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001034
Iteration 59/1000 | Loss: 0.00001034
Iteration 60/1000 | Loss: 0.00001034
Iteration 61/1000 | Loss: 0.00001034
Iteration 62/1000 | Loss: 0.00001033
Iteration 63/1000 | Loss: 0.00001033
Iteration 64/1000 | Loss: 0.00001033
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001031
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001031
Iteration 71/1000 | Loss: 0.00001031
Iteration 72/1000 | Loss: 0.00001031
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001030
Iteration 76/1000 | Loss: 0.00001030
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001030
Iteration 80/1000 | Loss: 0.00001030
Iteration 81/1000 | Loss: 0.00001030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.0304132047167514e-05, 1.0304132047167514e-05, 1.0304132047167514e-05, 1.0304132047167514e-05, 1.0304132047167514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0304132047167514e-05

Optimization complete. Final v2v error: 2.742654323577881 mm

Highest mean error: 2.9486782550811768 mm for frame 29

Lowest mean error: 2.5928828716278076 mm for frame 70

Saving results

Total time: 29.093857288360596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527779
Iteration 2/25 | Loss: 0.00117626
Iteration 3/25 | Loss: 0.00110373
Iteration 4/25 | Loss: 0.00109379
Iteration 5/25 | Loss: 0.00109064
Iteration 6/25 | Loss: 0.00108996
Iteration 7/25 | Loss: 0.00108996
Iteration 8/25 | Loss: 0.00108996
Iteration 9/25 | Loss: 0.00108996
Iteration 10/25 | Loss: 0.00108996
Iteration 11/25 | Loss: 0.00108996
Iteration 12/25 | Loss: 0.00108996
Iteration 13/25 | Loss: 0.00108996
Iteration 14/25 | Loss: 0.00108996
Iteration 15/25 | Loss: 0.00108996
Iteration 16/25 | Loss: 0.00108996
Iteration 17/25 | Loss: 0.00108996
Iteration 18/25 | Loss: 0.00108996
Iteration 19/25 | Loss: 0.00108996
Iteration 20/25 | Loss: 0.00108996
Iteration 21/25 | Loss: 0.00108996
Iteration 22/25 | Loss: 0.00108996
Iteration 23/25 | Loss: 0.00108996
Iteration 24/25 | Loss: 0.00108996
Iteration 25/25 | Loss: 0.00108996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.71082306
Iteration 2/25 | Loss: 0.00079009
Iteration 3/25 | Loss: 0.00079008
Iteration 4/25 | Loss: 0.00079008
Iteration 5/25 | Loss: 0.00079008
Iteration 6/25 | Loss: 0.00079008
Iteration 7/25 | Loss: 0.00079008
Iteration 8/25 | Loss: 0.00079008
Iteration 9/25 | Loss: 0.00079008
Iteration 10/25 | Loss: 0.00079007
Iteration 11/25 | Loss: 0.00079007
Iteration 12/25 | Loss: 0.00079007
Iteration 13/25 | Loss: 0.00079007
Iteration 14/25 | Loss: 0.00079007
Iteration 15/25 | Loss: 0.00079007
Iteration 16/25 | Loss: 0.00079007
Iteration 17/25 | Loss: 0.00079007
Iteration 18/25 | Loss: 0.00079007
Iteration 19/25 | Loss: 0.00079007
Iteration 20/25 | Loss: 0.00079007
Iteration 21/25 | Loss: 0.00079007
Iteration 22/25 | Loss: 0.00079007
Iteration 23/25 | Loss: 0.00079007
Iteration 24/25 | Loss: 0.00079007
Iteration 25/25 | Loss: 0.00079007

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079007
Iteration 2/1000 | Loss: 0.00002393
Iteration 3/1000 | Loss: 0.00001573
Iteration 4/1000 | Loss: 0.00001395
Iteration 5/1000 | Loss: 0.00001310
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001210
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001152
Iteration 10/1000 | Loss: 0.00001128
Iteration 11/1000 | Loss: 0.00001114
Iteration 12/1000 | Loss: 0.00001106
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001104
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001100
Iteration 18/1000 | Loss: 0.00001099
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001098
Iteration 22/1000 | Loss: 0.00001097
Iteration 23/1000 | Loss: 0.00001096
Iteration 24/1000 | Loss: 0.00001095
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001090
Iteration 30/1000 | Loss: 0.00001089
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001088
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001087
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001085
Iteration 39/1000 | Loss: 0.00001085
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001084
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001080
Iteration 45/1000 | Loss: 0.00001080
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001079
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001077
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001075
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001075
Iteration 67/1000 | Loss: 0.00001075
Iteration 68/1000 | Loss: 0.00001075
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001072
Iteration 76/1000 | Loss: 0.00001072
Iteration 77/1000 | Loss: 0.00001072
Iteration 78/1000 | Loss: 0.00001071
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001070
Iteration 81/1000 | Loss: 0.00001069
Iteration 82/1000 | Loss: 0.00001069
Iteration 83/1000 | Loss: 0.00001068
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001067
Iteration 87/1000 | Loss: 0.00001067
Iteration 88/1000 | Loss: 0.00001066
Iteration 89/1000 | Loss: 0.00001066
Iteration 90/1000 | Loss: 0.00001066
Iteration 91/1000 | Loss: 0.00001066
Iteration 92/1000 | Loss: 0.00001066
Iteration 93/1000 | Loss: 0.00001066
Iteration 94/1000 | Loss: 0.00001066
Iteration 95/1000 | Loss: 0.00001066
Iteration 96/1000 | Loss: 0.00001066
Iteration 97/1000 | Loss: 0.00001066
Iteration 98/1000 | Loss: 0.00001066
Iteration 99/1000 | Loss: 0.00001065
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001063
Iteration 104/1000 | Loss: 0.00001063
Iteration 105/1000 | Loss: 0.00001063
Iteration 106/1000 | Loss: 0.00001063
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001063
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001063
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001063
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001062
Iteration 119/1000 | Loss: 0.00001062
Iteration 120/1000 | Loss: 0.00001062
Iteration 121/1000 | Loss: 0.00001061
Iteration 122/1000 | Loss: 0.00001060
Iteration 123/1000 | Loss: 0.00001060
Iteration 124/1000 | Loss: 0.00001060
Iteration 125/1000 | Loss: 0.00001060
Iteration 126/1000 | Loss: 0.00001059
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001058
Iteration 129/1000 | Loss: 0.00001058
Iteration 130/1000 | Loss: 0.00001058
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001056
Iteration 135/1000 | Loss: 0.00001056
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001055
Iteration 141/1000 | Loss: 0.00001055
Iteration 142/1000 | Loss: 0.00001055
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001054
Iteration 145/1000 | Loss: 0.00001054
Iteration 146/1000 | Loss: 0.00001054
Iteration 147/1000 | Loss: 0.00001053
Iteration 148/1000 | Loss: 0.00001053
Iteration 149/1000 | Loss: 0.00001052
Iteration 150/1000 | Loss: 0.00001052
Iteration 151/1000 | Loss: 0.00001052
Iteration 152/1000 | Loss: 0.00001051
Iteration 153/1000 | Loss: 0.00001051
Iteration 154/1000 | Loss: 0.00001051
Iteration 155/1000 | Loss: 0.00001051
Iteration 156/1000 | Loss: 0.00001051
Iteration 157/1000 | Loss: 0.00001051
Iteration 158/1000 | Loss: 0.00001051
Iteration 159/1000 | Loss: 0.00001051
Iteration 160/1000 | Loss: 0.00001051
Iteration 161/1000 | Loss: 0.00001051
Iteration 162/1000 | Loss: 0.00001051
Iteration 163/1000 | Loss: 0.00001051
Iteration 164/1000 | Loss: 0.00001050
Iteration 165/1000 | Loss: 0.00001050
Iteration 166/1000 | Loss: 0.00001050
Iteration 167/1000 | Loss: 0.00001050
Iteration 168/1000 | Loss: 0.00001050
Iteration 169/1000 | Loss: 0.00001050
Iteration 170/1000 | Loss: 0.00001050
Iteration 171/1000 | Loss: 0.00001050
Iteration 172/1000 | Loss: 0.00001050
Iteration 173/1000 | Loss: 0.00001050
Iteration 174/1000 | Loss: 0.00001050
Iteration 175/1000 | Loss: 0.00001050
Iteration 176/1000 | Loss: 0.00001049
Iteration 177/1000 | Loss: 0.00001049
Iteration 178/1000 | Loss: 0.00001049
Iteration 179/1000 | Loss: 0.00001049
Iteration 180/1000 | Loss: 0.00001049
Iteration 181/1000 | Loss: 0.00001049
Iteration 182/1000 | Loss: 0.00001049
Iteration 183/1000 | Loss: 0.00001049
Iteration 184/1000 | Loss: 0.00001049
Iteration 185/1000 | Loss: 0.00001049
Iteration 186/1000 | Loss: 0.00001049
Iteration 187/1000 | Loss: 0.00001049
Iteration 188/1000 | Loss: 0.00001049
Iteration 189/1000 | Loss: 0.00001049
Iteration 190/1000 | Loss: 0.00001049
Iteration 191/1000 | Loss: 0.00001049
Iteration 192/1000 | Loss: 0.00001049
Iteration 193/1000 | Loss: 0.00001049
Iteration 194/1000 | Loss: 0.00001049
Iteration 195/1000 | Loss: 0.00001049
Iteration 196/1000 | Loss: 0.00001049
Iteration 197/1000 | Loss: 0.00001049
Iteration 198/1000 | Loss: 0.00001049
Iteration 199/1000 | Loss: 0.00001049
Iteration 200/1000 | Loss: 0.00001049
Iteration 201/1000 | Loss: 0.00001049
Iteration 202/1000 | Loss: 0.00001049
Iteration 203/1000 | Loss: 0.00001049
Iteration 204/1000 | Loss: 0.00001049
Iteration 205/1000 | Loss: 0.00001049
Iteration 206/1000 | Loss: 0.00001049
Iteration 207/1000 | Loss: 0.00001049
Iteration 208/1000 | Loss: 0.00001049
Iteration 209/1000 | Loss: 0.00001049
Iteration 210/1000 | Loss: 0.00001049
Iteration 211/1000 | Loss: 0.00001049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.048523608915275e-05, 1.048523608915275e-05, 1.048523608915275e-05, 1.048523608915275e-05, 1.048523608915275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.048523608915275e-05

Optimization complete. Final v2v error: 2.7615973949432373 mm

Highest mean error: 3.5432403087615967 mm for frame 70

Lowest mean error: 2.441668748855591 mm for frame 24

Saving results

Total time: 40.40155386924744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025674
Iteration 2/25 | Loss: 0.00428694
Iteration 3/25 | Loss: 0.00385625
Iteration 4/25 | Loss: 0.00215039
Iteration 5/25 | Loss: 0.00170026
Iteration 6/25 | Loss: 0.00159170
Iteration 7/25 | Loss: 0.00148992
Iteration 8/25 | Loss: 0.00140815
Iteration 9/25 | Loss: 0.00129308
Iteration 10/25 | Loss: 0.00124308
Iteration 11/25 | Loss: 0.00122493
Iteration 12/25 | Loss: 0.00121257
Iteration 13/25 | Loss: 0.00119560
Iteration 14/25 | Loss: 0.00119402
Iteration 15/25 | Loss: 0.00119353
Iteration 16/25 | Loss: 0.00119331
Iteration 17/25 | Loss: 0.00119329
Iteration 18/25 | Loss: 0.00119329
Iteration 19/25 | Loss: 0.00119329
Iteration 20/25 | Loss: 0.00119329
Iteration 21/25 | Loss: 0.00119329
Iteration 22/25 | Loss: 0.00119329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011932851048186421, 0.0011932851048186421, 0.0011932851048186421, 0.0011932851048186421, 0.0011932851048186421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011932851048186421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33822072
Iteration 2/25 | Loss: 0.00089928
Iteration 3/25 | Loss: 0.00089928
Iteration 4/25 | Loss: 0.00089927
Iteration 5/25 | Loss: 0.00079905
Iteration 6/25 | Loss: 0.00079905
Iteration 7/25 | Loss: 0.00079905
Iteration 8/25 | Loss: 0.00079905
Iteration 9/25 | Loss: 0.00079905
Iteration 10/25 | Loss: 0.00079905
Iteration 11/25 | Loss: 0.00079905
Iteration 12/25 | Loss: 0.00079905
Iteration 13/25 | Loss: 0.00079905
Iteration 14/25 | Loss: 0.00079905
Iteration 15/25 | Loss: 0.00079905
Iteration 16/25 | Loss: 0.00079905
Iteration 17/25 | Loss: 0.00079905
Iteration 18/25 | Loss: 0.00079905
Iteration 19/25 | Loss: 0.00079905
Iteration 20/25 | Loss: 0.00079905
Iteration 21/25 | Loss: 0.00079905
Iteration 22/25 | Loss: 0.00079905
Iteration 23/25 | Loss: 0.00079905
Iteration 24/25 | Loss: 0.00079905
Iteration 25/25 | Loss: 0.00079905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079905
Iteration 2/1000 | Loss: 0.00049499
Iteration 3/1000 | Loss: 0.00009360
Iteration 4/1000 | Loss: 0.00010756
Iteration 5/1000 | Loss: 0.00012951
Iteration 6/1000 | Loss: 0.00002946
Iteration 7/1000 | Loss: 0.00002564
Iteration 8/1000 | Loss: 0.00018807
Iteration 9/1000 | Loss: 0.00015077
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002192
Iteration 12/1000 | Loss: 0.00002148
Iteration 13/1000 | Loss: 0.00002090
Iteration 14/1000 | Loss: 0.00002050
Iteration 15/1000 | Loss: 0.00002011
Iteration 16/1000 | Loss: 0.00001975
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001964
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001956
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001937
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001929
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001918
Iteration 28/1000 | Loss: 0.00001917
Iteration 29/1000 | Loss: 0.00001917
Iteration 30/1000 | Loss: 0.00001917
Iteration 31/1000 | Loss: 0.00001917
Iteration 32/1000 | Loss: 0.00001917
Iteration 33/1000 | Loss: 0.00001917
Iteration 34/1000 | Loss: 0.00001917
Iteration 35/1000 | Loss: 0.00001916
Iteration 36/1000 | Loss: 0.00001916
Iteration 37/1000 | Loss: 0.00001914
Iteration 38/1000 | Loss: 0.00001913
Iteration 39/1000 | Loss: 0.00001913
Iteration 40/1000 | Loss: 0.00001913
Iteration 41/1000 | Loss: 0.00001913
Iteration 42/1000 | Loss: 0.00001913
Iteration 43/1000 | Loss: 0.00001913
Iteration 44/1000 | Loss: 0.00001913
Iteration 45/1000 | Loss: 0.00001913
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001908
Iteration 50/1000 | Loss: 0.00001908
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001905
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001904
Iteration 61/1000 | Loss: 0.00001904
Iteration 62/1000 | Loss: 0.00001904
Iteration 63/1000 | Loss: 0.00001904
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001903
Iteration 68/1000 | Loss: 0.00001903
Iteration 69/1000 | Loss: 0.00001902
Iteration 70/1000 | Loss: 0.00001901
Iteration 71/1000 | Loss: 0.00001901
Iteration 72/1000 | Loss: 0.00001901
Iteration 73/1000 | Loss: 0.00001901
Iteration 74/1000 | Loss: 0.00001901
Iteration 75/1000 | Loss: 0.00001901
Iteration 76/1000 | Loss: 0.00001900
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001899
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001898
Iteration 85/1000 | Loss: 0.00001898
Iteration 86/1000 | Loss: 0.00001898
Iteration 87/1000 | Loss: 0.00001898
Iteration 88/1000 | Loss: 0.00001897
Iteration 89/1000 | Loss: 0.00001897
Iteration 90/1000 | Loss: 0.00001897
Iteration 91/1000 | Loss: 0.00001897
Iteration 92/1000 | Loss: 0.00001896
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001896
Iteration 96/1000 | Loss: 0.00001896
Iteration 97/1000 | Loss: 0.00001896
Iteration 98/1000 | Loss: 0.00001896
Iteration 99/1000 | Loss: 0.00001896
Iteration 100/1000 | Loss: 0.00001895
Iteration 101/1000 | Loss: 0.00001895
Iteration 102/1000 | Loss: 0.00001895
Iteration 103/1000 | Loss: 0.00001895
Iteration 104/1000 | Loss: 0.00001895
Iteration 105/1000 | Loss: 0.00001895
Iteration 106/1000 | Loss: 0.00001895
Iteration 107/1000 | Loss: 0.00001894
Iteration 108/1000 | Loss: 0.00001894
Iteration 109/1000 | Loss: 0.00001894
Iteration 110/1000 | Loss: 0.00001894
Iteration 111/1000 | Loss: 0.00001894
Iteration 112/1000 | Loss: 0.00001894
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001893
Iteration 115/1000 | Loss: 0.00001893
Iteration 116/1000 | Loss: 0.00001893
Iteration 117/1000 | Loss: 0.00001893
Iteration 118/1000 | Loss: 0.00001893
Iteration 119/1000 | Loss: 0.00001893
Iteration 120/1000 | Loss: 0.00001893
Iteration 121/1000 | Loss: 0.00001892
Iteration 122/1000 | Loss: 0.00001892
Iteration 123/1000 | Loss: 0.00001892
Iteration 124/1000 | Loss: 0.00001892
Iteration 125/1000 | Loss: 0.00001892
Iteration 126/1000 | Loss: 0.00001892
Iteration 127/1000 | Loss: 0.00001892
Iteration 128/1000 | Loss: 0.00001892
Iteration 129/1000 | Loss: 0.00001891
Iteration 130/1000 | Loss: 0.00001891
Iteration 131/1000 | Loss: 0.00001891
Iteration 132/1000 | Loss: 0.00001891
Iteration 133/1000 | Loss: 0.00001891
Iteration 134/1000 | Loss: 0.00001891
Iteration 135/1000 | Loss: 0.00001891
Iteration 136/1000 | Loss: 0.00001890
Iteration 137/1000 | Loss: 0.00001890
Iteration 138/1000 | Loss: 0.00001890
Iteration 139/1000 | Loss: 0.00001890
Iteration 140/1000 | Loss: 0.00001890
Iteration 141/1000 | Loss: 0.00001890
Iteration 142/1000 | Loss: 0.00001890
Iteration 143/1000 | Loss: 0.00001890
Iteration 144/1000 | Loss: 0.00001890
Iteration 145/1000 | Loss: 0.00001890
Iteration 146/1000 | Loss: 0.00001890
Iteration 147/1000 | Loss: 0.00001890
Iteration 148/1000 | Loss: 0.00001890
Iteration 149/1000 | Loss: 0.00001890
Iteration 150/1000 | Loss: 0.00001890
Iteration 151/1000 | Loss: 0.00001890
Iteration 152/1000 | Loss: 0.00001890
Iteration 153/1000 | Loss: 0.00001890
Iteration 154/1000 | Loss: 0.00001890
Iteration 155/1000 | Loss: 0.00001890
Iteration 156/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.8896967958426103e-05, 1.8896967958426103e-05, 1.8896967958426103e-05, 1.8896967958426103e-05, 1.8896967958426103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8896967958426103e-05

Optimization complete. Final v2v error: 3.723015785217285 mm

Highest mean error: 4.034533977508545 mm for frame 100

Lowest mean error: 3.5583181381225586 mm for frame 58

Saving results

Total time: 74.06102275848389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905105
Iteration 2/25 | Loss: 0.00138831
Iteration 3/25 | Loss: 0.00119448
Iteration 4/25 | Loss: 0.00117010
Iteration 5/25 | Loss: 0.00116273
Iteration 6/25 | Loss: 0.00116072
Iteration 7/25 | Loss: 0.00116060
Iteration 8/25 | Loss: 0.00116060
Iteration 9/25 | Loss: 0.00116060
Iteration 10/25 | Loss: 0.00116060
Iteration 11/25 | Loss: 0.00116060
Iteration 12/25 | Loss: 0.00116060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011605971958488226, 0.0011605971958488226, 0.0011605971958488226, 0.0011605971958488226, 0.0011605971958488226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011605971958488226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31473732
Iteration 2/25 | Loss: 0.00071314
Iteration 3/25 | Loss: 0.00071311
Iteration 4/25 | Loss: 0.00071311
Iteration 5/25 | Loss: 0.00071311
Iteration 6/25 | Loss: 0.00071311
Iteration 7/25 | Loss: 0.00071311
Iteration 8/25 | Loss: 0.00071311
Iteration 9/25 | Loss: 0.00071311
Iteration 10/25 | Loss: 0.00071311
Iteration 11/25 | Loss: 0.00071311
Iteration 12/25 | Loss: 0.00071311
Iteration 13/25 | Loss: 0.00071311
Iteration 14/25 | Loss: 0.00071311
Iteration 15/25 | Loss: 0.00071311
Iteration 16/25 | Loss: 0.00071311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007131085731089115, 0.0007131085731089115, 0.0007131085731089115, 0.0007131085731089115, 0.0007131085731089115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007131085731089115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071311
Iteration 2/1000 | Loss: 0.00004576
Iteration 3/1000 | Loss: 0.00002819
Iteration 4/1000 | Loss: 0.00002328
Iteration 5/1000 | Loss: 0.00002157
Iteration 6/1000 | Loss: 0.00002077
Iteration 7/1000 | Loss: 0.00002005
Iteration 8/1000 | Loss: 0.00001941
Iteration 9/1000 | Loss: 0.00001889
Iteration 10/1000 | Loss: 0.00001862
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001813
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001800
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001796
Iteration 19/1000 | Loss: 0.00001796
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001795
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001792
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001790
Iteration 31/1000 | Loss: 0.00001790
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001786
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001775
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001775
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001770
Iteration 108/1000 | Loss: 0.00001770
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001770
Iteration 111/1000 | Loss: 0.00001770
Iteration 112/1000 | Loss: 0.00001769
Iteration 113/1000 | Loss: 0.00001769
Iteration 114/1000 | Loss: 0.00001769
Iteration 115/1000 | Loss: 0.00001769
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001765
Iteration 133/1000 | Loss: 0.00001765
Iteration 134/1000 | Loss: 0.00001765
Iteration 135/1000 | Loss: 0.00001765
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001765
Iteration 141/1000 | Loss: 0.00001765
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Iteration 154/1000 | Loss: 0.00001764
Iteration 155/1000 | Loss: 0.00001764
Iteration 156/1000 | Loss: 0.00001764
Iteration 157/1000 | Loss: 0.00001763
Iteration 158/1000 | Loss: 0.00001763
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001763
Iteration 165/1000 | Loss: 0.00001763
Iteration 166/1000 | Loss: 0.00001762
Iteration 167/1000 | Loss: 0.00001762
Iteration 168/1000 | Loss: 0.00001762
Iteration 169/1000 | Loss: 0.00001762
Iteration 170/1000 | Loss: 0.00001762
Iteration 171/1000 | Loss: 0.00001762
Iteration 172/1000 | Loss: 0.00001762
Iteration 173/1000 | Loss: 0.00001762
Iteration 174/1000 | Loss: 0.00001762
Iteration 175/1000 | Loss: 0.00001762
Iteration 176/1000 | Loss: 0.00001762
Iteration 177/1000 | Loss: 0.00001762
Iteration 178/1000 | Loss: 0.00001761
Iteration 179/1000 | Loss: 0.00001761
Iteration 180/1000 | Loss: 0.00001761
Iteration 181/1000 | Loss: 0.00001761
Iteration 182/1000 | Loss: 0.00001761
Iteration 183/1000 | Loss: 0.00001761
Iteration 184/1000 | Loss: 0.00001761
Iteration 185/1000 | Loss: 0.00001761
Iteration 186/1000 | Loss: 0.00001761
Iteration 187/1000 | Loss: 0.00001761
Iteration 188/1000 | Loss: 0.00001761
Iteration 189/1000 | Loss: 0.00001761
Iteration 190/1000 | Loss: 0.00001761
Iteration 191/1000 | Loss: 0.00001760
Iteration 192/1000 | Loss: 0.00001760
Iteration 193/1000 | Loss: 0.00001760
Iteration 194/1000 | Loss: 0.00001760
Iteration 195/1000 | Loss: 0.00001760
Iteration 196/1000 | Loss: 0.00001760
Iteration 197/1000 | Loss: 0.00001760
Iteration 198/1000 | Loss: 0.00001760
Iteration 199/1000 | Loss: 0.00001760
Iteration 200/1000 | Loss: 0.00001760
Iteration 201/1000 | Loss: 0.00001760
Iteration 202/1000 | Loss: 0.00001760
Iteration 203/1000 | Loss: 0.00001760
Iteration 204/1000 | Loss: 0.00001760
Iteration 205/1000 | Loss: 0.00001760
Iteration 206/1000 | Loss: 0.00001760
Iteration 207/1000 | Loss: 0.00001760
Iteration 208/1000 | Loss: 0.00001760
Iteration 209/1000 | Loss: 0.00001760
Iteration 210/1000 | Loss: 0.00001760
Iteration 211/1000 | Loss: 0.00001760
Iteration 212/1000 | Loss: 0.00001760
Iteration 213/1000 | Loss: 0.00001760
Iteration 214/1000 | Loss: 0.00001760
Iteration 215/1000 | Loss: 0.00001760
Iteration 216/1000 | Loss: 0.00001760
Iteration 217/1000 | Loss: 0.00001760
Iteration 218/1000 | Loss: 0.00001760
Iteration 219/1000 | Loss: 0.00001760
Iteration 220/1000 | Loss: 0.00001760
Iteration 221/1000 | Loss: 0.00001760
Iteration 222/1000 | Loss: 0.00001760
Iteration 223/1000 | Loss: 0.00001760
Iteration 224/1000 | Loss: 0.00001760
Iteration 225/1000 | Loss: 0.00001760
Iteration 226/1000 | Loss: 0.00001760
Iteration 227/1000 | Loss: 0.00001760
Iteration 228/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.759609585860744e-05, 1.759609585860744e-05, 1.759609585860744e-05, 1.759609585860744e-05, 1.759609585860744e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.759609585860744e-05

Optimization complete. Final v2v error: 3.535163640975952 mm

Highest mean error: 4.632544994354248 mm for frame 68

Lowest mean error: 2.9298696517944336 mm for frame 46

Saving results

Total time: 42.983556509017944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996569
Iteration 2/25 | Loss: 0.00225092
Iteration 3/25 | Loss: 0.00171553
Iteration 4/25 | Loss: 0.00163152
Iteration 5/25 | Loss: 0.00162304
Iteration 6/25 | Loss: 0.00161191
Iteration 7/25 | Loss: 0.00150993
Iteration 8/25 | Loss: 0.00139872
Iteration 9/25 | Loss: 0.00136388
Iteration 10/25 | Loss: 0.00133713
Iteration 11/25 | Loss: 0.00132438
Iteration 12/25 | Loss: 0.00131721
Iteration 13/25 | Loss: 0.00130634
Iteration 14/25 | Loss: 0.00129927
Iteration 15/25 | Loss: 0.00129641
Iteration 16/25 | Loss: 0.00128512
Iteration 17/25 | Loss: 0.00127543
Iteration 18/25 | Loss: 0.00127034
Iteration 19/25 | Loss: 0.00126810
Iteration 20/25 | Loss: 0.00126811
Iteration 21/25 | Loss: 0.00126794
Iteration 22/25 | Loss: 0.00126818
Iteration 23/25 | Loss: 0.00126715
Iteration 24/25 | Loss: 0.00126723
Iteration 25/25 | Loss: 0.00126733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36123943
Iteration 2/25 | Loss: 0.00218722
Iteration 3/25 | Loss: 0.00205920
Iteration 4/25 | Loss: 0.00205909
Iteration 5/25 | Loss: 0.00205855
Iteration 6/25 | Loss: 0.00205855
Iteration 7/25 | Loss: 0.00205855
Iteration 8/25 | Loss: 0.00205855
Iteration 9/25 | Loss: 0.00205855
Iteration 10/25 | Loss: 0.00205855
Iteration 11/25 | Loss: 0.00205855
Iteration 12/25 | Loss: 0.00205855
Iteration 13/25 | Loss: 0.00205855
Iteration 14/25 | Loss: 0.00205855
Iteration 15/25 | Loss: 0.00205855
Iteration 16/25 | Loss: 0.00205855
Iteration 17/25 | Loss: 0.00205855
Iteration 18/25 | Loss: 0.00205855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020585493184626102, 0.0020585493184626102, 0.0020585493184626102, 0.0020585493184626102, 0.0020585493184626102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020585493184626102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205855
Iteration 2/1000 | Loss: 0.00092528
Iteration 3/1000 | Loss: 0.00099524
Iteration 4/1000 | Loss: 0.00038465
Iteration 5/1000 | Loss: 0.00025178
Iteration 6/1000 | Loss: 0.00032079
Iteration 7/1000 | Loss: 0.00096677
Iteration 8/1000 | Loss: 0.00081999
Iteration 9/1000 | Loss: 0.00019777
Iteration 10/1000 | Loss: 0.00086679
Iteration 11/1000 | Loss: 0.00103024
Iteration 12/1000 | Loss: 0.00115153
Iteration 13/1000 | Loss: 0.00072727
Iteration 14/1000 | Loss: 0.00063493
Iteration 15/1000 | Loss: 0.00062675
Iteration 16/1000 | Loss: 0.00065203
Iteration 17/1000 | Loss: 0.00023931
Iteration 18/1000 | Loss: 0.00061451
Iteration 19/1000 | Loss: 0.00011284
Iteration 20/1000 | Loss: 0.00013735
Iteration 21/1000 | Loss: 0.00009604
Iteration 22/1000 | Loss: 0.00018613
Iteration 23/1000 | Loss: 0.00008629
Iteration 24/1000 | Loss: 0.00056066
Iteration 25/1000 | Loss: 0.00187324
Iteration 26/1000 | Loss: 0.00123777
Iteration 27/1000 | Loss: 0.00148655
Iteration 28/1000 | Loss: 0.00099410
Iteration 29/1000 | Loss: 0.00092481
Iteration 30/1000 | Loss: 0.00031248
Iteration 31/1000 | Loss: 0.00037795
Iteration 32/1000 | Loss: 0.00028145
Iteration 33/1000 | Loss: 0.00018620
Iteration 34/1000 | Loss: 0.00015697
Iteration 35/1000 | Loss: 0.00007147
Iteration 36/1000 | Loss: 0.00008217
Iteration 37/1000 | Loss: 0.00059799
Iteration 38/1000 | Loss: 0.00006766
Iteration 39/1000 | Loss: 0.00006659
Iteration 40/1000 | Loss: 0.00004735
Iteration 41/1000 | Loss: 0.00005441
Iteration 42/1000 | Loss: 0.00006171
Iteration 43/1000 | Loss: 0.00008809
Iteration 44/1000 | Loss: 0.00004840
Iteration 45/1000 | Loss: 0.00006941
Iteration 46/1000 | Loss: 0.00004831
Iteration 47/1000 | Loss: 0.00004845
Iteration 48/1000 | Loss: 0.00005778
Iteration 49/1000 | Loss: 0.00004696
Iteration 50/1000 | Loss: 0.00100879
Iteration 51/1000 | Loss: 0.00056318
Iteration 52/1000 | Loss: 0.00009987
Iteration 53/1000 | Loss: 0.00005996
Iteration 54/1000 | Loss: 0.00007860
Iteration 55/1000 | Loss: 0.00004949
Iteration 56/1000 | Loss: 0.00004698
Iteration 57/1000 | Loss: 0.00003764
Iteration 58/1000 | Loss: 0.00003486
Iteration 59/1000 | Loss: 0.00005466
Iteration 60/1000 | Loss: 0.00003988
Iteration 61/1000 | Loss: 0.00032368
Iteration 62/1000 | Loss: 0.00046347
Iteration 63/1000 | Loss: 0.00056545
Iteration 64/1000 | Loss: 0.00069674
Iteration 65/1000 | Loss: 0.00026529
Iteration 66/1000 | Loss: 0.00015365
Iteration 67/1000 | Loss: 0.00021904
Iteration 68/1000 | Loss: 0.00018790
Iteration 69/1000 | Loss: 0.00022726
Iteration 70/1000 | Loss: 0.00017762
Iteration 71/1000 | Loss: 0.00003009
Iteration 72/1000 | Loss: 0.00002663
Iteration 73/1000 | Loss: 0.00002615
Iteration 74/1000 | Loss: 0.00002319
Iteration 75/1000 | Loss: 0.00002064
Iteration 76/1000 | Loss: 0.00001966
Iteration 77/1000 | Loss: 0.00001882
Iteration 78/1000 | Loss: 0.00004674
Iteration 79/1000 | Loss: 0.00001832
Iteration 80/1000 | Loss: 0.00001801
Iteration 81/1000 | Loss: 0.00030930
Iteration 82/1000 | Loss: 0.00002614
Iteration 83/1000 | Loss: 0.00046116
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00017810
Iteration 87/1000 | Loss: 0.00017440
Iteration 88/1000 | Loss: 0.00014461
Iteration 89/1000 | Loss: 0.00002613
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001551
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001361
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001252
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001312
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001123
Iteration 102/1000 | Loss: 0.00002401
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001106
Iteration 107/1000 | Loss: 0.00001106
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001103
Iteration 112/1000 | Loss: 0.00001103
Iteration 113/1000 | Loss: 0.00001103
Iteration 114/1000 | Loss: 0.00001102
Iteration 115/1000 | Loss: 0.00001100
Iteration 116/1000 | Loss: 0.00001100
Iteration 117/1000 | Loss: 0.00001100
Iteration 118/1000 | Loss: 0.00001100
Iteration 119/1000 | Loss: 0.00001100
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001098
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001093
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001092
Iteration 133/1000 | Loss: 0.00001091
Iteration 134/1000 | Loss: 0.00001091
Iteration 135/1000 | Loss: 0.00001091
Iteration 136/1000 | Loss: 0.00001091
Iteration 137/1000 | Loss: 0.00001090
Iteration 138/1000 | Loss: 0.00001090
Iteration 139/1000 | Loss: 0.00001090
Iteration 140/1000 | Loss: 0.00001090
Iteration 141/1000 | Loss: 0.00001090
Iteration 142/1000 | Loss: 0.00001090
Iteration 143/1000 | Loss: 0.00001090
Iteration 144/1000 | Loss: 0.00001090
Iteration 145/1000 | Loss: 0.00001089
Iteration 146/1000 | Loss: 0.00001089
Iteration 147/1000 | Loss: 0.00001089
Iteration 148/1000 | Loss: 0.00001089
Iteration 149/1000 | Loss: 0.00001089
Iteration 150/1000 | Loss: 0.00001089
Iteration 151/1000 | Loss: 0.00001089
Iteration 152/1000 | Loss: 0.00001089
Iteration 153/1000 | Loss: 0.00001088
Iteration 154/1000 | Loss: 0.00001088
Iteration 155/1000 | Loss: 0.00001088
Iteration 156/1000 | Loss: 0.00001088
Iteration 157/1000 | Loss: 0.00001088
Iteration 158/1000 | Loss: 0.00001088
Iteration 159/1000 | Loss: 0.00001088
Iteration 160/1000 | Loss: 0.00001087
Iteration 161/1000 | Loss: 0.00001087
Iteration 162/1000 | Loss: 0.00001087
Iteration 163/1000 | Loss: 0.00001087
Iteration 164/1000 | Loss: 0.00001087
Iteration 165/1000 | Loss: 0.00001087
Iteration 166/1000 | Loss: 0.00001087
Iteration 167/1000 | Loss: 0.00001087
Iteration 168/1000 | Loss: 0.00001087
Iteration 169/1000 | Loss: 0.00001087
Iteration 170/1000 | Loss: 0.00001086
Iteration 171/1000 | Loss: 0.00001086
Iteration 172/1000 | Loss: 0.00001086
Iteration 173/1000 | Loss: 0.00001086
Iteration 174/1000 | Loss: 0.00001086
Iteration 175/1000 | Loss: 0.00001086
Iteration 176/1000 | Loss: 0.00001086
Iteration 177/1000 | Loss: 0.00001086
Iteration 178/1000 | Loss: 0.00001086
Iteration 179/1000 | Loss: 0.00001086
Iteration 180/1000 | Loss: 0.00001086
Iteration 181/1000 | Loss: 0.00001086
Iteration 182/1000 | Loss: 0.00001086
Iteration 183/1000 | Loss: 0.00001086
Iteration 184/1000 | Loss: 0.00001086
Iteration 185/1000 | Loss: 0.00001086
Iteration 186/1000 | Loss: 0.00001086
Iteration 187/1000 | Loss: 0.00001086
Iteration 188/1000 | Loss: 0.00001085
Iteration 189/1000 | Loss: 0.00001085
Iteration 190/1000 | Loss: 0.00001085
Iteration 191/1000 | Loss: 0.00001085
Iteration 192/1000 | Loss: 0.00001085
Iteration 193/1000 | Loss: 0.00001085
Iteration 194/1000 | Loss: 0.00001085
Iteration 195/1000 | Loss: 0.00001085
Iteration 196/1000 | Loss: 0.00001085
Iteration 197/1000 | Loss: 0.00001085
Iteration 198/1000 | Loss: 0.00001085
Iteration 199/1000 | Loss: 0.00001085
Iteration 200/1000 | Loss: 0.00001085
Iteration 201/1000 | Loss: 0.00001085
Iteration 202/1000 | Loss: 0.00001085
Iteration 203/1000 | Loss: 0.00001085
Iteration 204/1000 | Loss: 0.00001085
Iteration 205/1000 | Loss: 0.00001085
Iteration 206/1000 | Loss: 0.00001085
Iteration 207/1000 | Loss: 0.00001085
Iteration 208/1000 | Loss: 0.00001084
Iteration 209/1000 | Loss: 0.00001084
Iteration 210/1000 | Loss: 0.00001084
Iteration 211/1000 | Loss: 0.00001084
Iteration 212/1000 | Loss: 0.00001084
Iteration 213/1000 | Loss: 0.00001084
Iteration 214/1000 | Loss: 0.00001084
Iteration 215/1000 | Loss: 0.00001084
Iteration 216/1000 | Loss: 0.00001084
Iteration 217/1000 | Loss: 0.00001084
Iteration 218/1000 | Loss: 0.00001084
Iteration 219/1000 | Loss: 0.00001084
Iteration 220/1000 | Loss: 0.00001084
Iteration 221/1000 | Loss: 0.00001084
Iteration 222/1000 | Loss: 0.00001084
Iteration 223/1000 | Loss: 0.00001083
Iteration 224/1000 | Loss: 0.00001083
Iteration 225/1000 | Loss: 0.00001083
Iteration 226/1000 | Loss: 0.00001083
Iteration 227/1000 | Loss: 0.00001083
Iteration 228/1000 | Loss: 0.00001083
Iteration 229/1000 | Loss: 0.00001083
Iteration 230/1000 | Loss: 0.00001082
Iteration 231/1000 | Loss: 0.00001082
Iteration 232/1000 | Loss: 0.00001082
Iteration 233/1000 | Loss: 0.00001082
Iteration 234/1000 | Loss: 0.00001082
Iteration 235/1000 | Loss: 0.00001082
Iteration 236/1000 | Loss: 0.00001082
Iteration 237/1000 | Loss: 0.00001082
Iteration 238/1000 | Loss: 0.00001082
Iteration 239/1000 | Loss: 0.00001082
Iteration 240/1000 | Loss: 0.00001082
Iteration 241/1000 | Loss: 0.00001082
Iteration 242/1000 | Loss: 0.00001082
Iteration 243/1000 | Loss: 0.00001082
Iteration 244/1000 | Loss: 0.00001082
Iteration 245/1000 | Loss: 0.00001081
Iteration 246/1000 | Loss: 0.00001081
Iteration 247/1000 | Loss: 0.00001081
Iteration 248/1000 | Loss: 0.00001081
Iteration 249/1000 | Loss: 0.00001081
Iteration 250/1000 | Loss: 0.00001081
Iteration 251/1000 | Loss: 0.00001081
Iteration 252/1000 | Loss: 0.00001081
Iteration 253/1000 | Loss: 0.00001081
Iteration 254/1000 | Loss: 0.00001081
Iteration 255/1000 | Loss: 0.00001081
Iteration 256/1000 | Loss: 0.00001081
Iteration 257/1000 | Loss: 0.00001081
Iteration 258/1000 | Loss: 0.00001081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.081063146557426e-05, 1.081063146557426e-05, 1.081063146557426e-05, 1.081063146557426e-05, 1.081063146557426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.081063146557426e-05

Optimization complete. Final v2v error: 2.77437424659729 mm

Highest mean error: 4.197829723358154 mm for frame 155

Lowest mean error: 2.3145618438720703 mm for frame 68

Saving results

Total time: 226.46908402442932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031708
Iteration 2/25 | Loss: 0.00259328
Iteration 3/25 | Loss: 0.00241372
Iteration 4/25 | Loss: 0.00215086
Iteration 5/25 | Loss: 0.00182448
Iteration 6/25 | Loss: 0.00171432
Iteration 7/25 | Loss: 0.00157110
Iteration 8/25 | Loss: 0.00140126
Iteration 9/25 | Loss: 0.00134186
Iteration 10/25 | Loss: 0.00129450
Iteration 11/25 | Loss: 0.00130453
Iteration 12/25 | Loss: 0.00131693
Iteration 13/25 | Loss: 0.00129305
Iteration 14/25 | Loss: 0.00125755
Iteration 15/25 | Loss: 0.00124311
Iteration 16/25 | Loss: 0.00122966
Iteration 17/25 | Loss: 0.00122331
Iteration 18/25 | Loss: 0.00122561
Iteration 19/25 | Loss: 0.00121887
Iteration 20/25 | Loss: 0.00121846
Iteration 21/25 | Loss: 0.00121831
Iteration 22/25 | Loss: 0.00121826
Iteration 23/25 | Loss: 0.00121826
Iteration 24/25 | Loss: 0.00121826
Iteration 25/25 | Loss: 0.00121826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33741891
Iteration 2/25 | Loss: 0.00154363
Iteration 3/25 | Loss: 0.00104032
Iteration 4/25 | Loss: 0.00104032
Iteration 5/25 | Loss: 0.00104032
Iteration 6/25 | Loss: 0.00104032
Iteration 7/25 | Loss: 0.00104032
Iteration 8/25 | Loss: 0.00104032
Iteration 9/25 | Loss: 0.00104032
Iteration 10/25 | Loss: 0.00104032
Iteration 11/25 | Loss: 0.00104032
Iteration 12/25 | Loss: 0.00104032
Iteration 13/25 | Loss: 0.00104032
Iteration 14/25 | Loss: 0.00104032
Iteration 15/25 | Loss: 0.00104032
Iteration 16/25 | Loss: 0.00104032
Iteration 17/25 | Loss: 0.00104032
Iteration 18/25 | Loss: 0.00104032
Iteration 19/25 | Loss: 0.00104032
Iteration 20/25 | Loss: 0.00104032
Iteration 21/25 | Loss: 0.00104032
Iteration 22/25 | Loss: 0.00104032
Iteration 23/25 | Loss: 0.00104032
Iteration 24/25 | Loss: 0.00104032
Iteration 25/25 | Loss: 0.00104032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104032
Iteration 2/1000 | Loss: 0.00057559
Iteration 3/1000 | Loss: 0.00106626
Iteration 4/1000 | Loss: 0.00007226
Iteration 5/1000 | Loss: 0.00009211
Iteration 6/1000 | Loss: 0.00026168
Iteration 7/1000 | Loss: 0.00007544
Iteration 8/1000 | Loss: 0.00006145
Iteration 9/1000 | Loss: 0.00003969
Iteration 10/1000 | Loss: 0.00012229
Iteration 11/1000 | Loss: 0.00003786
Iteration 12/1000 | Loss: 0.00003719
Iteration 13/1000 | Loss: 0.00003646
Iteration 14/1000 | Loss: 0.00008082
Iteration 15/1000 | Loss: 0.00004915
Iteration 16/1000 | Loss: 0.00009722
Iteration 17/1000 | Loss: 0.00013010
Iteration 18/1000 | Loss: 0.00004845
Iteration 19/1000 | Loss: 0.00005806
Iteration 20/1000 | Loss: 0.00003318
Iteration 21/1000 | Loss: 0.00004938
Iteration 22/1000 | Loss: 0.00003190
Iteration 23/1000 | Loss: 0.00003740
Iteration 24/1000 | Loss: 0.00003137
Iteration 25/1000 | Loss: 0.00003099
Iteration 26/1000 | Loss: 0.00003061
Iteration 27/1000 | Loss: 0.00003028
Iteration 28/1000 | Loss: 0.00002999
Iteration 29/1000 | Loss: 0.00010116
Iteration 30/1000 | Loss: 0.00002995
Iteration 31/1000 | Loss: 0.00002977
Iteration 32/1000 | Loss: 0.00002976
Iteration 33/1000 | Loss: 0.00002973
Iteration 34/1000 | Loss: 0.00002972
Iteration 35/1000 | Loss: 0.00002971
Iteration 36/1000 | Loss: 0.00002971
Iteration 37/1000 | Loss: 0.00002971
Iteration 38/1000 | Loss: 0.00002970
Iteration 39/1000 | Loss: 0.00002969
Iteration 40/1000 | Loss: 0.00002968
Iteration 41/1000 | Loss: 0.00002967
Iteration 42/1000 | Loss: 0.00002966
Iteration 43/1000 | Loss: 0.00002964
Iteration 44/1000 | Loss: 0.00002963
Iteration 45/1000 | Loss: 0.00002963
Iteration 46/1000 | Loss: 0.00002963
Iteration 47/1000 | Loss: 0.00002963
Iteration 48/1000 | Loss: 0.00002963
Iteration 49/1000 | Loss: 0.00002963
Iteration 50/1000 | Loss: 0.00002963
Iteration 51/1000 | Loss: 0.00002962
Iteration 52/1000 | Loss: 0.00002962
Iteration 53/1000 | Loss: 0.00002962
Iteration 54/1000 | Loss: 0.00002962
Iteration 55/1000 | Loss: 0.00002961
Iteration 56/1000 | Loss: 0.00002961
Iteration 57/1000 | Loss: 0.00002961
Iteration 58/1000 | Loss: 0.00002960
Iteration 59/1000 | Loss: 0.00002960
Iteration 60/1000 | Loss: 0.00002959
Iteration 61/1000 | Loss: 0.00002959
Iteration 62/1000 | Loss: 0.00002959
Iteration 63/1000 | Loss: 0.00002959
Iteration 64/1000 | Loss: 0.00002959
Iteration 65/1000 | Loss: 0.00002959
Iteration 66/1000 | Loss: 0.00007067
Iteration 67/1000 | Loss: 0.00005280
Iteration 68/1000 | Loss: 0.00014695
Iteration 69/1000 | Loss: 0.00005318
Iteration 70/1000 | Loss: 0.00008508
Iteration 71/1000 | Loss: 0.00002997
Iteration 72/1000 | Loss: 0.00002959
Iteration 73/1000 | Loss: 0.00002957
Iteration 74/1000 | Loss: 0.00002957
Iteration 75/1000 | Loss: 0.00002957
Iteration 76/1000 | Loss: 0.00002955
Iteration 77/1000 | Loss: 0.00002954
Iteration 78/1000 | Loss: 0.00002954
Iteration 79/1000 | Loss: 0.00002953
Iteration 80/1000 | Loss: 0.00002953
Iteration 81/1000 | Loss: 0.00002952
Iteration 82/1000 | Loss: 0.00002952
Iteration 83/1000 | Loss: 0.00002952
Iteration 84/1000 | Loss: 0.00002952
Iteration 85/1000 | Loss: 0.00002952
Iteration 86/1000 | Loss: 0.00002952
Iteration 87/1000 | Loss: 0.00002952
Iteration 88/1000 | Loss: 0.00002952
Iteration 89/1000 | Loss: 0.00002951
Iteration 90/1000 | Loss: 0.00002951
Iteration 91/1000 | Loss: 0.00002951
Iteration 92/1000 | Loss: 0.00002951
Iteration 93/1000 | Loss: 0.00002950
Iteration 94/1000 | Loss: 0.00002950
Iteration 95/1000 | Loss: 0.00002950
Iteration 96/1000 | Loss: 0.00002949
Iteration 97/1000 | Loss: 0.00002949
Iteration 98/1000 | Loss: 0.00002949
Iteration 99/1000 | Loss: 0.00002949
Iteration 100/1000 | Loss: 0.00002949
Iteration 101/1000 | Loss: 0.00002949
Iteration 102/1000 | Loss: 0.00002949
Iteration 103/1000 | Loss: 0.00002949
Iteration 104/1000 | Loss: 0.00002949
Iteration 105/1000 | Loss: 0.00002948
Iteration 106/1000 | Loss: 0.00002948
Iteration 107/1000 | Loss: 0.00002948
Iteration 108/1000 | Loss: 0.00002947
Iteration 109/1000 | Loss: 0.00002947
Iteration 110/1000 | Loss: 0.00002947
Iteration 111/1000 | Loss: 0.00002947
Iteration 112/1000 | Loss: 0.00002947
Iteration 113/1000 | Loss: 0.00002947
Iteration 114/1000 | Loss: 0.00002947
Iteration 115/1000 | Loss: 0.00002946
Iteration 116/1000 | Loss: 0.00002946
Iteration 117/1000 | Loss: 0.00002946
Iteration 118/1000 | Loss: 0.00002946
Iteration 119/1000 | Loss: 0.00002946
Iteration 120/1000 | Loss: 0.00002946
Iteration 121/1000 | Loss: 0.00002946
Iteration 122/1000 | Loss: 0.00002946
Iteration 123/1000 | Loss: 0.00002946
Iteration 124/1000 | Loss: 0.00002946
Iteration 125/1000 | Loss: 0.00002946
Iteration 126/1000 | Loss: 0.00002946
Iteration 127/1000 | Loss: 0.00002946
Iteration 128/1000 | Loss: 0.00002946
Iteration 129/1000 | Loss: 0.00002946
Iteration 130/1000 | Loss: 0.00002946
Iteration 131/1000 | Loss: 0.00002945
Iteration 132/1000 | Loss: 0.00002945
Iteration 133/1000 | Loss: 0.00002945
Iteration 134/1000 | Loss: 0.00002945
Iteration 135/1000 | Loss: 0.00002945
Iteration 136/1000 | Loss: 0.00002945
Iteration 137/1000 | Loss: 0.00002945
Iteration 138/1000 | Loss: 0.00002945
Iteration 139/1000 | Loss: 0.00002945
Iteration 140/1000 | Loss: 0.00002944
Iteration 141/1000 | Loss: 0.00002944
Iteration 142/1000 | Loss: 0.00002944
Iteration 143/1000 | Loss: 0.00002944
Iteration 144/1000 | Loss: 0.00002944
Iteration 145/1000 | Loss: 0.00002944
Iteration 146/1000 | Loss: 0.00002944
Iteration 147/1000 | Loss: 0.00002944
Iteration 148/1000 | Loss: 0.00002944
Iteration 149/1000 | Loss: 0.00002944
Iteration 150/1000 | Loss: 0.00002943
Iteration 151/1000 | Loss: 0.00002943
Iteration 152/1000 | Loss: 0.00002943
Iteration 153/1000 | Loss: 0.00002943
Iteration 154/1000 | Loss: 0.00002943
Iteration 155/1000 | Loss: 0.00002943
Iteration 156/1000 | Loss: 0.00002943
Iteration 157/1000 | Loss: 0.00002943
Iteration 158/1000 | Loss: 0.00002942
Iteration 159/1000 | Loss: 0.00002942
Iteration 160/1000 | Loss: 0.00002942
Iteration 161/1000 | Loss: 0.00002942
Iteration 162/1000 | Loss: 0.00002942
Iteration 163/1000 | Loss: 0.00002942
Iteration 164/1000 | Loss: 0.00002942
Iteration 165/1000 | Loss: 0.00002941
Iteration 166/1000 | Loss: 0.00002941
Iteration 167/1000 | Loss: 0.00002941
Iteration 168/1000 | Loss: 0.00002941
Iteration 169/1000 | Loss: 0.00002941
Iteration 170/1000 | Loss: 0.00002941
Iteration 171/1000 | Loss: 0.00002941
Iteration 172/1000 | Loss: 0.00002941
Iteration 173/1000 | Loss: 0.00002941
Iteration 174/1000 | Loss: 0.00002940
Iteration 175/1000 | Loss: 0.00002940
Iteration 176/1000 | Loss: 0.00002940
Iteration 177/1000 | Loss: 0.00002940
Iteration 178/1000 | Loss: 0.00002940
Iteration 179/1000 | Loss: 0.00002940
Iteration 180/1000 | Loss: 0.00002940
Iteration 181/1000 | Loss: 0.00002940
Iteration 182/1000 | Loss: 0.00002939
Iteration 183/1000 | Loss: 0.00002939
Iteration 184/1000 | Loss: 0.00002939
Iteration 185/1000 | Loss: 0.00002938
Iteration 186/1000 | Loss: 0.00002938
Iteration 187/1000 | Loss: 0.00002938
Iteration 188/1000 | Loss: 0.00002938
Iteration 189/1000 | Loss: 0.00002937
Iteration 190/1000 | Loss: 0.00002937
Iteration 191/1000 | Loss: 0.00002937
Iteration 192/1000 | Loss: 0.00002937
Iteration 193/1000 | Loss: 0.00002937
Iteration 194/1000 | Loss: 0.00002936
Iteration 195/1000 | Loss: 0.00002936
Iteration 196/1000 | Loss: 0.00002936
Iteration 197/1000 | Loss: 0.00002936
Iteration 198/1000 | Loss: 0.00002936
Iteration 199/1000 | Loss: 0.00002936
Iteration 200/1000 | Loss: 0.00002936
Iteration 201/1000 | Loss: 0.00002936
Iteration 202/1000 | Loss: 0.00002936
Iteration 203/1000 | Loss: 0.00002936
Iteration 204/1000 | Loss: 0.00002935
Iteration 205/1000 | Loss: 0.00002935
Iteration 206/1000 | Loss: 0.00002935
Iteration 207/1000 | Loss: 0.00002935
Iteration 208/1000 | Loss: 0.00002935
Iteration 209/1000 | Loss: 0.00002935
Iteration 210/1000 | Loss: 0.00002935
Iteration 211/1000 | Loss: 0.00002935
Iteration 212/1000 | Loss: 0.00002935
Iteration 213/1000 | Loss: 0.00002935
Iteration 214/1000 | Loss: 0.00002935
Iteration 215/1000 | Loss: 0.00002935
Iteration 216/1000 | Loss: 0.00002935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.934734038717579e-05, 2.934734038717579e-05, 2.934734038717579e-05, 2.934734038717579e-05, 2.934734038717579e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.934734038717579e-05

Optimization complete. Final v2v error: 3.753525733947754 mm

Highest mean error: 11.828681945800781 mm for frame 212

Lowest mean error: 2.82542085647583 mm for frame 152

Saving results

Total time: 116.95616602897644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768402
Iteration 2/25 | Loss: 0.00153263
Iteration 3/25 | Loss: 0.00128621
Iteration 4/25 | Loss: 0.00126269
Iteration 5/25 | Loss: 0.00125387
Iteration 6/25 | Loss: 0.00124892
Iteration 7/25 | Loss: 0.00124674
Iteration 8/25 | Loss: 0.00124825
Iteration 9/25 | Loss: 0.00124548
Iteration 10/25 | Loss: 0.00124479
Iteration 11/25 | Loss: 0.00124432
Iteration 12/25 | Loss: 0.00124390
Iteration 13/25 | Loss: 0.00124321
Iteration 14/25 | Loss: 0.00124270
Iteration 15/25 | Loss: 0.00124378
Iteration 16/25 | Loss: 0.00124443
Iteration 17/25 | Loss: 0.00124165
Iteration 18/25 | Loss: 0.00124135
Iteration 19/25 | Loss: 0.00124116
Iteration 20/25 | Loss: 0.00124113
Iteration 21/25 | Loss: 0.00124113
Iteration 22/25 | Loss: 0.00124113
Iteration 23/25 | Loss: 0.00124113
Iteration 24/25 | Loss: 0.00124113
Iteration 25/25 | Loss: 0.00124113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35152125
Iteration 2/25 | Loss: 0.00075456
Iteration 3/25 | Loss: 0.00075453
Iteration 4/25 | Loss: 0.00075453
Iteration 5/25 | Loss: 0.00075453
Iteration 6/25 | Loss: 0.00075453
Iteration 7/25 | Loss: 0.00075453
Iteration 8/25 | Loss: 0.00075453
Iteration 9/25 | Loss: 0.00075453
Iteration 10/25 | Loss: 0.00075453
Iteration 11/25 | Loss: 0.00075453
Iteration 12/25 | Loss: 0.00075453
Iteration 13/25 | Loss: 0.00075453
Iteration 14/25 | Loss: 0.00075453
Iteration 15/25 | Loss: 0.00075453
Iteration 16/25 | Loss: 0.00075453
Iteration 17/25 | Loss: 0.00075453
Iteration 18/25 | Loss: 0.00075453
Iteration 19/25 | Loss: 0.00075453
Iteration 20/25 | Loss: 0.00075453
Iteration 21/25 | Loss: 0.00075453
Iteration 22/25 | Loss: 0.00075453
Iteration 23/25 | Loss: 0.00075453
Iteration 24/25 | Loss: 0.00075453
Iteration 25/25 | Loss: 0.00075453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075453
Iteration 2/1000 | Loss: 0.00003680
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002350
Iteration 5/1000 | Loss: 0.00002211
Iteration 6/1000 | Loss: 0.00002136
Iteration 7/1000 | Loss: 0.00002089
Iteration 8/1000 | Loss: 0.00002047
Iteration 9/1000 | Loss: 0.00002017
Iteration 10/1000 | Loss: 0.00001999
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001989
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001968
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00001962
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001955
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001955
Iteration 24/1000 | Loss: 0.00001955
Iteration 25/1000 | Loss: 0.00001955
Iteration 26/1000 | Loss: 0.00001955
Iteration 27/1000 | Loss: 0.00001955
Iteration 28/1000 | Loss: 0.00001953
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001953
Iteration 31/1000 | Loss: 0.00001952
Iteration 32/1000 | Loss: 0.00001952
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001952
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001950
Iteration 38/1000 | Loss: 0.00001950
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00001948
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001948
Iteration 48/1000 | Loss: 0.00001948
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00001948
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.9476474335533567e-05, 1.9476474335533567e-05, 1.9476474335533567e-05, 1.9476474335533567e-05, 1.9476474335533567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9476474335533567e-05

Optimization complete. Final v2v error: 3.6059107780456543 mm

Highest mean error: 5.565439224243164 mm for frame 65

Lowest mean error: 3.1720781326293945 mm for frame 194

Saving results

Total time: 62.82952690124512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825763
Iteration 2/25 | Loss: 0.00125558
Iteration 3/25 | Loss: 0.00113941
Iteration 4/25 | Loss: 0.00111316
Iteration 5/25 | Loss: 0.00110950
Iteration 6/25 | Loss: 0.00111011
Iteration 7/25 | Loss: 0.00110906
Iteration 8/25 | Loss: 0.00110785
Iteration 9/25 | Loss: 0.00110719
Iteration 10/25 | Loss: 0.00110684
Iteration 11/25 | Loss: 0.00110665
Iteration 12/25 | Loss: 0.00110657
Iteration 13/25 | Loss: 0.00110653
Iteration 14/25 | Loss: 0.00110653
Iteration 15/25 | Loss: 0.00110653
Iteration 16/25 | Loss: 0.00110653
Iteration 17/25 | Loss: 0.00110653
Iteration 18/25 | Loss: 0.00110653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001106534618884325, 0.001106534618884325, 0.001106534618884325, 0.001106534618884325, 0.001106534618884325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001106534618884325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.64587951
Iteration 2/25 | Loss: 0.00080971
Iteration 3/25 | Loss: 0.00080970
Iteration 4/25 | Loss: 0.00080970
Iteration 5/25 | Loss: 0.00080970
Iteration 6/25 | Loss: 0.00080970
Iteration 7/25 | Loss: 0.00080970
Iteration 8/25 | Loss: 0.00080970
Iteration 9/25 | Loss: 0.00080970
Iteration 10/25 | Loss: 0.00080970
Iteration 11/25 | Loss: 0.00080969
Iteration 12/25 | Loss: 0.00080969
Iteration 13/25 | Loss: 0.00080969
Iteration 14/25 | Loss: 0.00080969
Iteration 15/25 | Loss: 0.00080969
Iteration 16/25 | Loss: 0.00080969
Iteration 17/25 | Loss: 0.00080969
Iteration 18/25 | Loss: 0.00080969
Iteration 19/25 | Loss: 0.00080969
Iteration 20/25 | Loss: 0.00080969
Iteration 21/25 | Loss: 0.00080969
Iteration 22/25 | Loss: 0.00080969
Iteration 23/25 | Loss: 0.00080969
Iteration 24/25 | Loss: 0.00080969
Iteration 25/25 | Loss: 0.00080969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000809694523923099, 0.000809694523923099, 0.000809694523923099, 0.000809694523923099, 0.000809694523923099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000809694523923099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080969
Iteration 2/1000 | Loss: 0.00001995
Iteration 3/1000 | Loss: 0.00001503
Iteration 4/1000 | Loss: 0.00001371
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001119
Iteration 12/1000 | Loss: 0.00001112
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001103
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001098
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001094
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001093
Iteration 26/1000 | Loss: 0.00001092
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001086
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001085
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001084
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001080
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001075
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001072
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001070
Iteration 77/1000 | Loss: 0.00001070
Iteration 78/1000 | Loss: 0.00001070
Iteration 79/1000 | Loss: 0.00001070
Iteration 80/1000 | Loss: 0.00001070
Iteration 81/1000 | Loss: 0.00001069
Iteration 82/1000 | Loss: 0.00001069
Iteration 83/1000 | Loss: 0.00001069
Iteration 84/1000 | Loss: 0.00001068
Iteration 85/1000 | Loss: 0.00001068
Iteration 86/1000 | Loss: 0.00001068
Iteration 87/1000 | Loss: 0.00001068
Iteration 88/1000 | Loss: 0.00001068
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001068
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001068
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001068
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001067
Iteration 104/1000 | Loss: 0.00001067
Iteration 105/1000 | Loss: 0.00001067
Iteration 106/1000 | Loss: 0.00001067
Iteration 107/1000 | Loss: 0.00001067
Iteration 108/1000 | Loss: 0.00001067
Iteration 109/1000 | Loss: 0.00001066
Iteration 110/1000 | Loss: 0.00001066
Iteration 111/1000 | Loss: 0.00001066
Iteration 112/1000 | Loss: 0.00001066
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001065
Iteration 117/1000 | Loss: 0.00001065
Iteration 118/1000 | Loss: 0.00001065
Iteration 119/1000 | Loss: 0.00001065
Iteration 120/1000 | Loss: 0.00001065
Iteration 121/1000 | Loss: 0.00001065
Iteration 122/1000 | Loss: 0.00001065
Iteration 123/1000 | Loss: 0.00001064
Iteration 124/1000 | Loss: 0.00001064
Iteration 125/1000 | Loss: 0.00001064
Iteration 126/1000 | Loss: 0.00001064
Iteration 127/1000 | Loss: 0.00001064
Iteration 128/1000 | Loss: 0.00001063
Iteration 129/1000 | Loss: 0.00001063
Iteration 130/1000 | Loss: 0.00001063
Iteration 131/1000 | Loss: 0.00001063
Iteration 132/1000 | Loss: 0.00001062
Iteration 133/1000 | Loss: 0.00001062
Iteration 134/1000 | Loss: 0.00001062
Iteration 135/1000 | Loss: 0.00001062
Iteration 136/1000 | Loss: 0.00001062
Iteration 137/1000 | Loss: 0.00001062
Iteration 138/1000 | Loss: 0.00001062
Iteration 139/1000 | Loss: 0.00001062
Iteration 140/1000 | Loss: 0.00001062
Iteration 141/1000 | Loss: 0.00001062
Iteration 142/1000 | Loss: 0.00001061
Iteration 143/1000 | Loss: 0.00001061
Iteration 144/1000 | Loss: 0.00001061
Iteration 145/1000 | Loss: 0.00001061
Iteration 146/1000 | Loss: 0.00001061
Iteration 147/1000 | Loss: 0.00001061
Iteration 148/1000 | Loss: 0.00001060
Iteration 149/1000 | Loss: 0.00001060
Iteration 150/1000 | Loss: 0.00001060
Iteration 151/1000 | Loss: 0.00001060
Iteration 152/1000 | Loss: 0.00001060
Iteration 153/1000 | Loss: 0.00001060
Iteration 154/1000 | Loss: 0.00001060
Iteration 155/1000 | Loss: 0.00001060
Iteration 156/1000 | Loss: 0.00001060
Iteration 157/1000 | Loss: 0.00001060
Iteration 158/1000 | Loss: 0.00001060
Iteration 159/1000 | Loss: 0.00001060
Iteration 160/1000 | Loss: 0.00001060
Iteration 161/1000 | Loss: 0.00001060
Iteration 162/1000 | Loss: 0.00001060
Iteration 163/1000 | Loss: 0.00001060
Iteration 164/1000 | Loss: 0.00001060
Iteration 165/1000 | Loss: 0.00001060
Iteration 166/1000 | Loss: 0.00001060
Iteration 167/1000 | Loss: 0.00001060
Iteration 168/1000 | Loss: 0.00001060
Iteration 169/1000 | Loss: 0.00001060
Iteration 170/1000 | Loss: 0.00001060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.0603658665786497e-05, 1.0603658665786497e-05, 1.0603658665786497e-05, 1.0603658665786497e-05, 1.0603658665786497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0603658665786497e-05

Optimization complete. Final v2v error: 2.7962486743927 mm

Highest mean error: 3.1147782802581787 mm for frame 239

Lowest mean error: 2.5313096046447754 mm for frame 38

Saving results

Total time: 57.066304445266724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843492
Iteration 2/25 | Loss: 0.00119081
Iteration 3/25 | Loss: 0.00110050
Iteration 4/25 | Loss: 0.00108959
Iteration 5/25 | Loss: 0.00108608
Iteration 6/25 | Loss: 0.00108550
Iteration 7/25 | Loss: 0.00108550
Iteration 8/25 | Loss: 0.00108550
Iteration 9/25 | Loss: 0.00108535
Iteration 10/25 | Loss: 0.00108535
Iteration 11/25 | Loss: 0.00108535
Iteration 12/25 | Loss: 0.00108535
Iteration 13/25 | Loss: 0.00108535
Iteration 14/25 | Loss: 0.00108535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001085346913896501, 0.001085346913896501, 0.001085346913896501, 0.001085346913896501, 0.001085346913896501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001085346913896501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74404383
Iteration 2/25 | Loss: 0.00091546
Iteration 3/25 | Loss: 0.00091546
Iteration 4/25 | Loss: 0.00091546
Iteration 5/25 | Loss: 0.00091546
Iteration 6/25 | Loss: 0.00091545
Iteration 7/25 | Loss: 0.00091545
Iteration 8/25 | Loss: 0.00091545
Iteration 9/25 | Loss: 0.00091545
Iteration 10/25 | Loss: 0.00091545
Iteration 11/25 | Loss: 0.00091545
Iteration 12/25 | Loss: 0.00091545
Iteration 13/25 | Loss: 0.00091545
Iteration 14/25 | Loss: 0.00091545
Iteration 15/25 | Loss: 0.00091545
Iteration 16/25 | Loss: 0.00091545
Iteration 17/25 | Loss: 0.00091545
Iteration 18/25 | Loss: 0.00091545
Iteration 19/25 | Loss: 0.00091545
Iteration 20/25 | Loss: 0.00091545
Iteration 21/25 | Loss: 0.00091545
Iteration 22/25 | Loss: 0.00091545
Iteration 23/25 | Loss: 0.00091545
Iteration 24/25 | Loss: 0.00091545
Iteration 25/25 | Loss: 0.00091545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091545
Iteration 2/1000 | Loss: 0.00002392
Iteration 3/1000 | Loss: 0.00001517
Iteration 4/1000 | Loss: 0.00001239
Iteration 5/1000 | Loss: 0.00001177
Iteration 6/1000 | Loss: 0.00001135
Iteration 7/1000 | Loss: 0.00001101
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001051
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001040
Iteration 13/1000 | Loss: 0.00001037
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001002
Iteration 16/1000 | Loss: 0.00000998
Iteration 17/1000 | Loss: 0.00000997
Iteration 18/1000 | Loss: 0.00000996
Iteration 19/1000 | Loss: 0.00000993
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000991
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000990
Iteration 25/1000 | Loss: 0.00000990
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000989
Iteration 28/1000 | Loss: 0.00000989
Iteration 29/1000 | Loss: 0.00000988
Iteration 30/1000 | Loss: 0.00000988
Iteration 31/1000 | Loss: 0.00000987
Iteration 32/1000 | Loss: 0.00000986
Iteration 33/1000 | Loss: 0.00000986
Iteration 34/1000 | Loss: 0.00000986
Iteration 35/1000 | Loss: 0.00000985
Iteration 36/1000 | Loss: 0.00000985
Iteration 37/1000 | Loss: 0.00000984
Iteration 38/1000 | Loss: 0.00000984
Iteration 39/1000 | Loss: 0.00000984
Iteration 40/1000 | Loss: 0.00000984
Iteration 41/1000 | Loss: 0.00000983
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000975
Iteration 46/1000 | Loss: 0.00000974
Iteration 47/1000 | Loss: 0.00000974
Iteration 48/1000 | Loss: 0.00000974
Iteration 49/1000 | Loss: 0.00000973
Iteration 50/1000 | Loss: 0.00000973
Iteration 51/1000 | Loss: 0.00000972
Iteration 52/1000 | Loss: 0.00000972
Iteration 53/1000 | Loss: 0.00000972
Iteration 54/1000 | Loss: 0.00000972
Iteration 55/1000 | Loss: 0.00000971
Iteration 56/1000 | Loss: 0.00000971
Iteration 57/1000 | Loss: 0.00000971
Iteration 58/1000 | Loss: 0.00000971
Iteration 59/1000 | Loss: 0.00000970
Iteration 60/1000 | Loss: 0.00000970
Iteration 61/1000 | Loss: 0.00000970
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000969
Iteration 64/1000 | Loss: 0.00000969
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000969
Iteration 68/1000 | Loss: 0.00000969
Iteration 69/1000 | Loss: 0.00000969
Iteration 70/1000 | Loss: 0.00000969
Iteration 71/1000 | Loss: 0.00000969
Iteration 72/1000 | Loss: 0.00000969
Iteration 73/1000 | Loss: 0.00000968
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000967
Iteration 76/1000 | Loss: 0.00000967
Iteration 77/1000 | Loss: 0.00000967
Iteration 78/1000 | Loss: 0.00000966
Iteration 79/1000 | Loss: 0.00000966
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000964
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000963
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000963
Iteration 88/1000 | Loss: 0.00000963
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000962
Iteration 93/1000 | Loss: 0.00000961
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000960
Iteration 99/1000 | Loss: 0.00000960
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000960
Iteration 105/1000 | Loss: 0.00000960
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000958
Iteration 111/1000 | Loss: 0.00000958
Iteration 112/1000 | Loss: 0.00000958
Iteration 113/1000 | Loss: 0.00000958
Iteration 114/1000 | Loss: 0.00000958
Iteration 115/1000 | Loss: 0.00000957
Iteration 116/1000 | Loss: 0.00000957
Iteration 117/1000 | Loss: 0.00000957
Iteration 118/1000 | Loss: 0.00000957
Iteration 119/1000 | Loss: 0.00000957
Iteration 120/1000 | Loss: 0.00000957
Iteration 121/1000 | Loss: 0.00000957
Iteration 122/1000 | Loss: 0.00000957
Iteration 123/1000 | Loss: 0.00000957
Iteration 124/1000 | Loss: 0.00000957
Iteration 125/1000 | Loss: 0.00000957
Iteration 126/1000 | Loss: 0.00000957
Iteration 127/1000 | Loss: 0.00000957
Iteration 128/1000 | Loss: 0.00000957
Iteration 129/1000 | Loss: 0.00000957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [9.572886483510956e-06, 9.572886483510956e-06, 9.572886483510956e-06, 9.572886483510956e-06, 9.572886483510956e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.572886483510956e-06

Optimization complete. Final v2v error: 2.6554183959960938 mm

Highest mean error: 3.215155601501465 mm for frame 55

Lowest mean error: 2.4501678943634033 mm for frame 108

Saving results

Total time: 33.13432312011719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379009
Iteration 2/25 | Loss: 0.00132761
Iteration 3/25 | Loss: 0.00113461
Iteration 4/25 | Loss: 0.00110564
Iteration 5/25 | Loss: 0.00110267
Iteration 6/25 | Loss: 0.00110264
Iteration 7/25 | Loss: 0.00110264
Iteration 8/25 | Loss: 0.00110264
Iteration 9/25 | Loss: 0.00110264
Iteration 10/25 | Loss: 0.00110264
Iteration 11/25 | Loss: 0.00110264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011026441352441907, 0.0011026441352441907, 0.0011026441352441907, 0.0011026441352441907, 0.0011026441352441907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011026441352441907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33201528
Iteration 2/25 | Loss: 0.00076075
Iteration 3/25 | Loss: 0.00076075
Iteration 4/25 | Loss: 0.00076075
Iteration 5/25 | Loss: 0.00076074
Iteration 6/25 | Loss: 0.00076074
Iteration 7/25 | Loss: 0.00076074
Iteration 8/25 | Loss: 0.00076074
Iteration 9/25 | Loss: 0.00076074
Iteration 10/25 | Loss: 0.00076074
Iteration 11/25 | Loss: 0.00076074
Iteration 12/25 | Loss: 0.00076074
Iteration 13/25 | Loss: 0.00076074
Iteration 14/25 | Loss: 0.00076074
Iteration 15/25 | Loss: 0.00076074
Iteration 16/25 | Loss: 0.00076074
Iteration 17/25 | Loss: 0.00076074
Iteration 18/25 | Loss: 0.00076074
Iteration 19/25 | Loss: 0.00076074
Iteration 20/25 | Loss: 0.00076074
Iteration 21/25 | Loss: 0.00076074
Iteration 22/25 | Loss: 0.00076074
Iteration 23/25 | Loss: 0.00076074
Iteration 24/25 | Loss: 0.00076074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007607415900565684, 0.0007607415900565684, 0.0007607415900565684, 0.0007607415900565684, 0.0007607415900565684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007607415900565684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076074
Iteration 2/1000 | Loss: 0.00002590
Iteration 3/1000 | Loss: 0.00001722
Iteration 4/1000 | Loss: 0.00001558
Iteration 5/1000 | Loss: 0.00001477
Iteration 6/1000 | Loss: 0.00001427
Iteration 7/1000 | Loss: 0.00001392
Iteration 8/1000 | Loss: 0.00001368
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001366
Iteration 11/1000 | Loss: 0.00001366
Iteration 12/1000 | Loss: 0.00001340
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001313
Iteration 15/1000 | Loss: 0.00001299
Iteration 16/1000 | Loss: 0.00001294
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001293
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001285
Iteration 22/1000 | Loss: 0.00001284
Iteration 23/1000 | Loss: 0.00001279
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001276
Iteration 31/1000 | Loss: 0.00001276
Iteration 32/1000 | Loss: 0.00001276
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001275
Iteration 35/1000 | Loss: 0.00001275
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001274
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001274
Iteration 41/1000 | Loss: 0.00001274
Iteration 42/1000 | Loss: 0.00001274
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001273
Iteration 45/1000 | Loss: 0.00001273
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001272
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001271
Iteration 51/1000 | Loss: 0.00001271
Iteration 52/1000 | Loss: 0.00001271
Iteration 53/1000 | Loss: 0.00001270
Iteration 54/1000 | Loss: 0.00001270
Iteration 55/1000 | Loss: 0.00001269
Iteration 56/1000 | Loss: 0.00001269
Iteration 57/1000 | Loss: 0.00001269
Iteration 58/1000 | Loss: 0.00001269
Iteration 59/1000 | Loss: 0.00001269
Iteration 60/1000 | Loss: 0.00001268
Iteration 61/1000 | Loss: 0.00001268
Iteration 62/1000 | Loss: 0.00001268
Iteration 63/1000 | Loss: 0.00001268
Iteration 64/1000 | Loss: 0.00001268
Iteration 65/1000 | Loss: 0.00001267
Iteration 66/1000 | Loss: 0.00001267
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001266
Iteration 69/1000 | Loss: 0.00001266
Iteration 70/1000 | Loss: 0.00001266
Iteration 71/1000 | Loss: 0.00001266
Iteration 72/1000 | Loss: 0.00001265
Iteration 73/1000 | Loss: 0.00001265
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001263
Iteration 81/1000 | Loss: 0.00001263
Iteration 82/1000 | Loss: 0.00001263
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001260
Iteration 90/1000 | Loss: 0.00001260
Iteration 91/1000 | Loss: 0.00001260
Iteration 92/1000 | Loss: 0.00001260
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001259
Iteration 95/1000 | Loss: 0.00001259
Iteration 96/1000 | Loss: 0.00001259
Iteration 97/1000 | Loss: 0.00001259
Iteration 98/1000 | Loss: 0.00001259
Iteration 99/1000 | Loss: 0.00001259
Iteration 100/1000 | Loss: 0.00001259
Iteration 101/1000 | Loss: 0.00001258
Iteration 102/1000 | Loss: 0.00001258
Iteration 103/1000 | Loss: 0.00001258
Iteration 104/1000 | Loss: 0.00001258
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001257
Iteration 109/1000 | Loss: 0.00001257
Iteration 110/1000 | Loss: 0.00001257
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001256
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001256
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001255
Iteration 128/1000 | Loss: 0.00001255
Iteration 129/1000 | Loss: 0.00001255
Iteration 130/1000 | Loss: 0.00001255
Iteration 131/1000 | Loss: 0.00001255
Iteration 132/1000 | Loss: 0.00001255
Iteration 133/1000 | Loss: 0.00001255
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001255
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001254
Iteration 138/1000 | Loss: 0.00001254
Iteration 139/1000 | Loss: 0.00001254
Iteration 140/1000 | Loss: 0.00001254
Iteration 141/1000 | Loss: 0.00001254
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001253
Iteration 145/1000 | Loss: 0.00001253
Iteration 146/1000 | Loss: 0.00001253
Iteration 147/1000 | Loss: 0.00001253
Iteration 148/1000 | Loss: 0.00001253
Iteration 149/1000 | Loss: 0.00001253
Iteration 150/1000 | Loss: 0.00001253
Iteration 151/1000 | Loss: 0.00001253
Iteration 152/1000 | Loss: 0.00001253
Iteration 153/1000 | Loss: 0.00001253
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.252969650522573e-05, 1.252969650522573e-05, 1.252969650522573e-05, 1.252969650522573e-05, 1.252969650522573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.252969650522573e-05

Optimization complete. Final v2v error: 2.9682538509368896 mm

Highest mean error: 3.2112159729003906 mm for frame 0

Lowest mean error: 2.8065261840820312 mm for frame 150

Saving results

Total time: 35.83641767501831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960313
Iteration 2/25 | Loss: 0.00960313
Iteration 3/25 | Loss: 0.00400320
Iteration 4/25 | Loss: 0.00219475
Iteration 5/25 | Loss: 0.00202625
Iteration 6/25 | Loss: 0.00187135
Iteration 7/25 | Loss: 0.00174650
Iteration 8/25 | Loss: 0.00172187
Iteration 9/25 | Loss: 0.00175456
Iteration 10/25 | Loss: 0.00172570
Iteration 11/25 | Loss: 0.00156445
Iteration 12/25 | Loss: 0.00148642
Iteration 13/25 | Loss: 0.00144772
Iteration 14/25 | Loss: 0.00143464
Iteration 15/25 | Loss: 0.00141571
Iteration 16/25 | Loss: 0.00138998
Iteration 17/25 | Loss: 0.00137682
Iteration 18/25 | Loss: 0.00137230
Iteration 19/25 | Loss: 0.00136778
Iteration 20/25 | Loss: 0.00136239
Iteration 21/25 | Loss: 0.00135807
Iteration 22/25 | Loss: 0.00135630
Iteration 23/25 | Loss: 0.00135452
Iteration 24/25 | Loss: 0.00135280
Iteration 25/25 | Loss: 0.00135267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33942866
Iteration 2/25 | Loss: 0.00441114
Iteration 3/25 | Loss: 0.00392228
Iteration 4/25 | Loss: 0.00392226
Iteration 5/25 | Loss: 0.00392226
Iteration 6/25 | Loss: 0.00392226
Iteration 7/25 | Loss: 0.00392225
Iteration 8/25 | Loss: 0.00392225
Iteration 9/25 | Loss: 0.00392225
Iteration 10/25 | Loss: 0.00392225
Iteration 11/25 | Loss: 0.00392225
Iteration 12/25 | Loss: 0.00392225
Iteration 13/25 | Loss: 0.00392225
Iteration 14/25 | Loss: 0.00392225
Iteration 15/25 | Loss: 0.00392225
Iteration 16/25 | Loss: 0.00392225
Iteration 17/25 | Loss: 0.00392225
Iteration 18/25 | Loss: 0.00392225
Iteration 19/25 | Loss: 0.00392225
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0039222524501383305, 0.0039222524501383305, 0.0039222524501383305, 0.0039222524501383305, 0.0039222524501383305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0039222524501383305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00392225
Iteration 2/1000 | Loss: 0.00157633
Iteration 3/1000 | Loss: 0.00141275
Iteration 4/1000 | Loss: 0.00157748
Iteration 5/1000 | Loss: 0.00105753
Iteration 6/1000 | Loss: 0.00053052
Iteration 7/1000 | Loss: 0.00061803
Iteration 8/1000 | Loss: 0.00054302
Iteration 9/1000 | Loss: 0.00030349
Iteration 10/1000 | Loss: 0.00008502
Iteration 11/1000 | Loss: 0.00006695
Iteration 12/1000 | Loss: 0.00033053
Iteration 13/1000 | Loss: 0.00007526
Iteration 14/1000 | Loss: 0.00016587
Iteration 15/1000 | Loss: 0.00009366
Iteration 16/1000 | Loss: 0.00006631
Iteration 17/1000 | Loss: 0.00008396
Iteration 18/1000 | Loss: 0.00007731
Iteration 19/1000 | Loss: 0.00006052
Iteration 20/1000 | Loss: 0.00008941
Iteration 21/1000 | Loss: 0.00003612
Iteration 22/1000 | Loss: 0.00008874
Iteration 23/1000 | Loss: 0.00004810
Iteration 24/1000 | Loss: 0.00011344
Iteration 25/1000 | Loss: 0.00009199
Iteration 26/1000 | Loss: 0.00007792
Iteration 27/1000 | Loss: 0.00013956
Iteration 28/1000 | Loss: 0.00002829
Iteration 29/1000 | Loss: 0.00003968
Iteration 30/1000 | Loss: 0.00002476
Iteration 31/1000 | Loss: 0.00015310
Iteration 32/1000 | Loss: 0.00034968
Iteration 33/1000 | Loss: 0.00007231
Iteration 34/1000 | Loss: 0.00033916
Iteration 35/1000 | Loss: 0.00020042
Iteration 36/1000 | Loss: 0.00041520
Iteration 37/1000 | Loss: 0.00025715
Iteration 38/1000 | Loss: 0.00006264
Iteration 39/1000 | Loss: 0.00006188
Iteration 40/1000 | Loss: 0.00005479
Iteration 41/1000 | Loss: 0.00007132
Iteration 42/1000 | Loss: 0.00004297
Iteration 43/1000 | Loss: 0.00008106
Iteration 44/1000 | Loss: 0.00007622
Iteration 45/1000 | Loss: 0.00006345
Iteration 46/1000 | Loss: 0.00002507
Iteration 47/1000 | Loss: 0.00002886
Iteration 48/1000 | Loss: 0.00002737
Iteration 49/1000 | Loss: 0.00002315
Iteration 50/1000 | Loss: 0.00003340
Iteration 51/1000 | Loss: 0.00002498
Iteration 52/1000 | Loss: 0.00003270
Iteration 53/1000 | Loss: 0.00003147
Iteration 54/1000 | Loss: 0.00002260
Iteration 55/1000 | Loss: 0.00002417
Iteration 56/1000 | Loss: 0.00011552
Iteration 57/1000 | Loss: 0.00024805
Iteration 58/1000 | Loss: 0.00003082
Iteration 59/1000 | Loss: 0.00009463
Iteration 60/1000 | Loss: 0.00019583
Iteration 61/1000 | Loss: 0.00033652
Iteration 62/1000 | Loss: 0.00014584
Iteration 63/1000 | Loss: 0.00002706
Iteration 64/1000 | Loss: 0.00006444
Iteration 65/1000 | Loss: 0.00007473
Iteration 66/1000 | Loss: 0.00026166
Iteration 67/1000 | Loss: 0.00010975
Iteration 68/1000 | Loss: 0.00052974
Iteration 69/1000 | Loss: 0.00012098
Iteration 70/1000 | Loss: 0.00007907
Iteration 71/1000 | Loss: 0.00002804
Iteration 72/1000 | Loss: 0.00003908
Iteration 73/1000 | Loss: 0.00002633
Iteration 74/1000 | Loss: 0.00003340
Iteration 75/1000 | Loss: 0.00003207
Iteration 76/1000 | Loss: 0.00004756
Iteration 77/1000 | Loss: 0.00002254
Iteration 78/1000 | Loss: 0.00002892
Iteration 79/1000 | Loss: 0.00003106
Iteration 80/1000 | Loss: 0.00002090
Iteration 81/1000 | Loss: 0.00004595
Iteration 82/1000 | Loss: 0.00002846
Iteration 83/1000 | Loss: 0.00002030
Iteration 84/1000 | Loss: 0.00002288
Iteration 85/1000 | Loss: 0.00005091
Iteration 86/1000 | Loss: 0.00002010
Iteration 87/1000 | Loss: 0.00001990
Iteration 88/1000 | Loss: 0.00001989
Iteration 89/1000 | Loss: 0.00001989
Iteration 90/1000 | Loss: 0.00001988
Iteration 91/1000 | Loss: 0.00001987
Iteration 92/1000 | Loss: 0.00002596
Iteration 93/1000 | Loss: 0.00001968
Iteration 94/1000 | Loss: 0.00001967
Iteration 95/1000 | Loss: 0.00001967
Iteration 96/1000 | Loss: 0.00001967
Iteration 97/1000 | Loss: 0.00001967
Iteration 98/1000 | Loss: 0.00001967
Iteration 99/1000 | Loss: 0.00001967
Iteration 100/1000 | Loss: 0.00001967
Iteration 101/1000 | Loss: 0.00001967
Iteration 102/1000 | Loss: 0.00001967
Iteration 103/1000 | Loss: 0.00001966
Iteration 104/1000 | Loss: 0.00001966
Iteration 105/1000 | Loss: 0.00001966
Iteration 106/1000 | Loss: 0.00001966
Iteration 107/1000 | Loss: 0.00001966
Iteration 108/1000 | Loss: 0.00001966
Iteration 109/1000 | Loss: 0.00001965
Iteration 110/1000 | Loss: 0.00001965
Iteration 111/1000 | Loss: 0.00001965
Iteration 112/1000 | Loss: 0.00001964
Iteration 113/1000 | Loss: 0.00001964
Iteration 114/1000 | Loss: 0.00001964
Iteration 115/1000 | Loss: 0.00001964
Iteration 116/1000 | Loss: 0.00001964
Iteration 117/1000 | Loss: 0.00001964
Iteration 118/1000 | Loss: 0.00001964
Iteration 119/1000 | Loss: 0.00001964
Iteration 120/1000 | Loss: 0.00001964
Iteration 121/1000 | Loss: 0.00001964
Iteration 122/1000 | Loss: 0.00001964
Iteration 123/1000 | Loss: 0.00001964
Iteration 124/1000 | Loss: 0.00001963
Iteration 125/1000 | Loss: 0.00003145
Iteration 126/1000 | Loss: 0.00002707
Iteration 127/1000 | Loss: 0.00002661
Iteration 128/1000 | Loss: 0.00001959
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001958
Iteration 132/1000 | Loss: 0.00001958
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00002100
Iteration 135/1000 | Loss: 0.00001954
Iteration 136/1000 | Loss: 0.00001954
Iteration 137/1000 | Loss: 0.00002430
Iteration 138/1000 | Loss: 0.00002289
Iteration 139/1000 | Loss: 0.00002223
Iteration 140/1000 | Loss: 0.00001952
Iteration 141/1000 | Loss: 0.00001952
Iteration 142/1000 | Loss: 0.00001952
Iteration 143/1000 | Loss: 0.00001951
Iteration 144/1000 | Loss: 0.00001951
Iteration 145/1000 | Loss: 0.00001951
Iteration 146/1000 | Loss: 0.00001951
Iteration 147/1000 | Loss: 0.00001951
Iteration 148/1000 | Loss: 0.00001951
Iteration 149/1000 | Loss: 0.00001951
Iteration 150/1000 | Loss: 0.00001951
Iteration 151/1000 | Loss: 0.00001951
Iteration 152/1000 | Loss: 0.00001951
Iteration 153/1000 | Loss: 0.00001951
Iteration 154/1000 | Loss: 0.00001951
Iteration 155/1000 | Loss: 0.00001951
Iteration 156/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.95116317627253e-05, 1.95116317627253e-05, 1.95116317627253e-05, 1.95116317627253e-05, 1.95116317627253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.95116317627253e-05

Optimization complete. Final v2v error: 3.105417251586914 mm

Highest mean error: 11.059633255004883 mm for frame 9

Lowest mean error: 2.611145496368408 mm for frame 95

Saving results

Total time: 200.4680051803589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966195
Iteration 2/25 | Loss: 0.00966195
Iteration 3/25 | Loss: 0.00966194
Iteration 4/25 | Loss: 0.00966194
Iteration 5/25 | Loss: 0.00351994
Iteration 6/25 | Loss: 0.00273608
Iteration 7/25 | Loss: 0.00228055
Iteration 8/25 | Loss: 0.00221202
Iteration 9/25 | Loss: 0.00216140
Iteration 10/25 | Loss: 0.00194210
Iteration 11/25 | Loss: 0.00185764
Iteration 12/25 | Loss: 0.00176725
Iteration 13/25 | Loss: 0.00174982
Iteration 14/25 | Loss: 0.00169520
Iteration 15/25 | Loss: 0.00168006
Iteration 16/25 | Loss: 0.00168045
Iteration 17/25 | Loss: 0.00164470
Iteration 18/25 | Loss: 0.00164571
Iteration 19/25 | Loss: 0.00163016
Iteration 20/25 | Loss: 0.00162307
Iteration 21/25 | Loss: 0.00160712
Iteration 22/25 | Loss: 0.00160096
Iteration 23/25 | Loss: 0.00158845
Iteration 24/25 | Loss: 0.00158413
Iteration 25/25 | Loss: 0.00159608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42156982
Iteration 2/25 | Loss: 0.00831727
Iteration 3/25 | Loss: 0.00509627
Iteration 4/25 | Loss: 0.00509627
Iteration 5/25 | Loss: 0.00509627
Iteration 6/25 | Loss: 0.00509627
Iteration 7/25 | Loss: 0.00509627
Iteration 8/25 | Loss: 0.00509627
Iteration 9/25 | Loss: 0.00509627
Iteration 10/25 | Loss: 0.00509626
Iteration 11/25 | Loss: 0.00509626
Iteration 12/25 | Loss: 0.00509626
Iteration 13/25 | Loss: 0.00509626
Iteration 14/25 | Loss: 0.00509626
Iteration 15/25 | Loss: 0.00509626
Iteration 16/25 | Loss: 0.00509626
Iteration 17/25 | Loss: 0.00509626
Iteration 18/25 | Loss: 0.00509626
Iteration 19/25 | Loss: 0.00509626
Iteration 20/25 | Loss: 0.00509626
Iteration 21/25 | Loss: 0.00509626
Iteration 22/25 | Loss: 0.00509626
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.005096264649182558, 0.005096264649182558, 0.005096264649182558, 0.005096264649182558, 0.005096264649182558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005096264649182558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00509626
Iteration 2/1000 | Loss: 0.00389143
Iteration 3/1000 | Loss: 0.00357975
Iteration 4/1000 | Loss: 0.00224673
Iteration 5/1000 | Loss: 0.00424698
Iteration 6/1000 | Loss: 0.00087873
Iteration 7/1000 | Loss: 0.00119557
Iteration 8/1000 | Loss: 0.00222804
Iteration 9/1000 | Loss: 0.00053166
Iteration 10/1000 | Loss: 0.00164527
Iteration 11/1000 | Loss: 0.00164985
Iteration 12/1000 | Loss: 0.00584767
Iteration 13/1000 | Loss: 0.00305576
Iteration 14/1000 | Loss: 0.00398826
Iteration 15/1000 | Loss: 0.00396533
Iteration 16/1000 | Loss: 0.00364939
Iteration 17/1000 | Loss: 0.00125997
Iteration 18/1000 | Loss: 0.00412640
Iteration 19/1000 | Loss: 0.00405355
Iteration 20/1000 | Loss: 0.00081926
Iteration 21/1000 | Loss: 0.00140989
Iteration 22/1000 | Loss: 0.00044024
Iteration 23/1000 | Loss: 0.00044213
Iteration 24/1000 | Loss: 0.00109591
Iteration 25/1000 | Loss: 0.00032550
Iteration 26/1000 | Loss: 0.00063548
Iteration 27/1000 | Loss: 0.00029925
Iteration 28/1000 | Loss: 0.00173210
Iteration 29/1000 | Loss: 0.00084856
Iteration 30/1000 | Loss: 0.00064814
Iteration 31/1000 | Loss: 0.00108516
Iteration 32/1000 | Loss: 0.00051139
Iteration 33/1000 | Loss: 0.00043630
Iteration 34/1000 | Loss: 0.00087164
Iteration 35/1000 | Loss: 0.00069855
Iteration 36/1000 | Loss: 0.00223519
Iteration 37/1000 | Loss: 0.00076191
Iteration 38/1000 | Loss: 0.00038522
Iteration 39/1000 | Loss: 0.00020685
Iteration 40/1000 | Loss: 0.00040471
Iteration 41/1000 | Loss: 0.00022225
Iteration 42/1000 | Loss: 0.00078323
Iteration 43/1000 | Loss: 0.00032679
Iteration 44/1000 | Loss: 0.00024751
Iteration 45/1000 | Loss: 0.00037931
Iteration 46/1000 | Loss: 0.00029536
Iteration 47/1000 | Loss: 0.00024643
Iteration 48/1000 | Loss: 0.00053910
Iteration 49/1000 | Loss: 0.00028427
Iteration 50/1000 | Loss: 0.00032503
Iteration 51/1000 | Loss: 0.00030579
Iteration 52/1000 | Loss: 0.00047082
Iteration 53/1000 | Loss: 0.00064970
Iteration 54/1000 | Loss: 0.00033557
Iteration 55/1000 | Loss: 0.00034027
Iteration 56/1000 | Loss: 0.00012271
Iteration 57/1000 | Loss: 0.00014913
Iteration 58/1000 | Loss: 0.00018700
Iteration 59/1000 | Loss: 0.00152468
Iteration 60/1000 | Loss: 0.00032261
Iteration 61/1000 | Loss: 0.00030286
Iteration 62/1000 | Loss: 0.00024214
Iteration 63/1000 | Loss: 0.00027687
Iteration 64/1000 | Loss: 0.00013167
Iteration 65/1000 | Loss: 0.00012217
Iteration 66/1000 | Loss: 0.00010733
Iteration 67/1000 | Loss: 0.00049908
Iteration 68/1000 | Loss: 0.00147512
Iteration 69/1000 | Loss: 0.00018980
Iteration 70/1000 | Loss: 0.00014395
Iteration 71/1000 | Loss: 0.00020004
Iteration 72/1000 | Loss: 0.00019631
Iteration 73/1000 | Loss: 0.00012393
Iteration 74/1000 | Loss: 0.00013258
Iteration 75/1000 | Loss: 0.00025808
Iteration 76/1000 | Loss: 0.00019356
Iteration 77/1000 | Loss: 0.00021103
Iteration 78/1000 | Loss: 0.00043526
Iteration 79/1000 | Loss: 0.00023960
Iteration 80/1000 | Loss: 0.00011850
Iteration 81/1000 | Loss: 0.00008708
Iteration 82/1000 | Loss: 0.00021857
Iteration 83/1000 | Loss: 0.00009558
Iteration 84/1000 | Loss: 0.00012861
Iteration 85/1000 | Loss: 0.00035113
Iteration 86/1000 | Loss: 0.00065992
Iteration 87/1000 | Loss: 0.00066694
Iteration 88/1000 | Loss: 0.00010451
Iteration 89/1000 | Loss: 0.00020229
Iteration 90/1000 | Loss: 0.00029060
Iteration 91/1000 | Loss: 0.00038321
Iteration 92/1000 | Loss: 0.00028103
Iteration 93/1000 | Loss: 0.00102754
Iteration 94/1000 | Loss: 0.00036398
Iteration 95/1000 | Loss: 0.00059887
Iteration 96/1000 | Loss: 0.00011339
Iteration 97/1000 | Loss: 0.00028877
Iteration 98/1000 | Loss: 0.00010532
Iteration 99/1000 | Loss: 0.00015981
Iteration 100/1000 | Loss: 0.00009530
Iteration 101/1000 | Loss: 0.00009532
Iteration 102/1000 | Loss: 0.00015385
Iteration 103/1000 | Loss: 0.00008633
Iteration 104/1000 | Loss: 0.00013189
Iteration 105/1000 | Loss: 0.00008138
Iteration 106/1000 | Loss: 0.00031749
Iteration 107/1000 | Loss: 0.00009982
Iteration 108/1000 | Loss: 0.00017482
Iteration 109/1000 | Loss: 0.00008125
Iteration 110/1000 | Loss: 0.00012656
Iteration 111/1000 | Loss: 0.00021802
Iteration 112/1000 | Loss: 0.00008414
Iteration 113/1000 | Loss: 0.00032586
Iteration 114/1000 | Loss: 0.00070966
Iteration 115/1000 | Loss: 0.00063613
Iteration 116/1000 | Loss: 0.00011375
Iteration 117/1000 | Loss: 0.00012813
Iteration 118/1000 | Loss: 0.00010763
Iteration 119/1000 | Loss: 0.00012122
Iteration 120/1000 | Loss: 0.00007978
Iteration 121/1000 | Loss: 0.00027901
Iteration 122/1000 | Loss: 0.00026563
Iteration 123/1000 | Loss: 0.00030516
Iteration 124/1000 | Loss: 0.00020848
Iteration 125/1000 | Loss: 0.00031206
Iteration 126/1000 | Loss: 0.00051702
Iteration 127/1000 | Loss: 0.00051339
Iteration 128/1000 | Loss: 0.00098290
Iteration 129/1000 | Loss: 0.00103813
Iteration 130/1000 | Loss: 0.00056690
Iteration 131/1000 | Loss: 0.00043194
Iteration 132/1000 | Loss: 0.00055055
Iteration 133/1000 | Loss: 0.00028021
Iteration 134/1000 | Loss: 0.00012075
Iteration 135/1000 | Loss: 0.00014862
Iteration 136/1000 | Loss: 0.00013192
Iteration 137/1000 | Loss: 0.00015300
Iteration 138/1000 | Loss: 0.00018586
Iteration 139/1000 | Loss: 0.00008230
Iteration 140/1000 | Loss: 0.00007911
Iteration 141/1000 | Loss: 0.00007491
Iteration 142/1000 | Loss: 0.00007949
Iteration 143/1000 | Loss: 0.00007139
Iteration 144/1000 | Loss: 0.00020520
Iteration 145/1000 | Loss: 0.00016117
Iteration 146/1000 | Loss: 0.00031043
Iteration 147/1000 | Loss: 0.00043976
Iteration 148/1000 | Loss: 0.00021463
Iteration 149/1000 | Loss: 0.00019315
Iteration 150/1000 | Loss: 0.00017538
Iteration 151/1000 | Loss: 0.00011745
Iteration 152/1000 | Loss: 0.00008882
Iteration 153/1000 | Loss: 0.00007819
Iteration 154/1000 | Loss: 0.00007972
Iteration 155/1000 | Loss: 0.00024510
Iteration 156/1000 | Loss: 0.00044402
Iteration 157/1000 | Loss: 0.00015346
Iteration 158/1000 | Loss: 0.00006881
Iteration 159/1000 | Loss: 0.00006750
Iteration 160/1000 | Loss: 0.00021966
Iteration 161/1000 | Loss: 0.00010577
Iteration 162/1000 | Loss: 0.00014206
Iteration 163/1000 | Loss: 0.00010664
Iteration 164/1000 | Loss: 0.00008559
Iteration 165/1000 | Loss: 0.00008136
Iteration 166/1000 | Loss: 0.00006793
Iteration 167/1000 | Loss: 0.00007528
Iteration 168/1000 | Loss: 0.00007767
Iteration 169/1000 | Loss: 0.00069858
Iteration 170/1000 | Loss: 0.00024658
Iteration 171/1000 | Loss: 0.00020103
Iteration 172/1000 | Loss: 0.00011288
Iteration 173/1000 | Loss: 0.00006934
Iteration 174/1000 | Loss: 0.00068754
Iteration 175/1000 | Loss: 0.00274004
Iteration 176/1000 | Loss: 0.00086690
Iteration 177/1000 | Loss: 0.00054776
Iteration 178/1000 | Loss: 0.00045690
Iteration 179/1000 | Loss: 0.00025121
Iteration 180/1000 | Loss: 0.00013560
Iteration 181/1000 | Loss: 0.00007410
Iteration 182/1000 | Loss: 0.00023380
Iteration 183/1000 | Loss: 0.00052984
Iteration 184/1000 | Loss: 0.00012914
Iteration 185/1000 | Loss: 0.00011237
Iteration 186/1000 | Loss: 0.00013634
Iteration 187/1000 | Loss: 0.00012009
Iteration 188/1000 | Loss: 0.00010847
Iteration 189/1000 | Loss: 0.00006249
Iteration 190/1000 | Loss: 0.00005583
Iteration 191/1000 | Loss: 0.00005483
Iteration 192/1000 | Loss: 0.00005412
Iteration 193/1000 | Loss: 0.00005331
Iteration 194/1000 | Loss: 0.00005273
Iteration 195/1000 | Loss: 0.00005232
Iteration 196/1000 | Loss: 0.00024030
Iteration 197/1000 | Loss: 0.00005199
Iteration 198/1000 | Loss: 0.00005166
Iteration 199/1000 | Loss: 0.00005144
Iteration 200/1000 | Loss: 0.00005118
Iteration 201/1000 | Loss: 0.00005107
Iteration 202/1000 | Loss: 0.00005101
Iteration 203/1000 | Loss: 0.00005088
Iteration 204/1000 | Loss: 0.00005088
Iteration 205/1000 | Loss: 0.00005086
Iteration 206/1000 | Loss: 0.00005084
Iteration 207/1000 | Loss: 0.00005083
Iteration 208/1000 | Loss: 0.00005079
Iteration 209/1000 | Loss: 0.00005079
Iteration 210/1000 | Loss: 0.00005079
Iteration 211/1000 | Loss: 0.00005078
Iteration 212/1000 | Loss: 0.00005077
Iteration 213/1000 | Loss: 0.00005077
Iteration 214/1000 | Loss: 0.00005073
Iteration 215/1000 | Loss: 0.00005073
Iteration 216/1000 | Loss: 0.00005072
Iteration 217/1000 | Loss: 0.00005072
Iteration 218/1000 | Loss: 0.00005072
Iteration 219/1000 | Loss: 0.00005072
Iteration 220/1000 | Loss: 0.00005072
Iteration 221/1000 | Loss: 0.00005072
Iteration 222/1000 | Loss: 0.00005072
Iteration 223/1000 | Loss: 0.00005072
Iteration 224/1000 | Loss: 0.00005072
Iteration 225/1000 | Loss: 0.00005071
Iteration 226/1000 | Loss: 0.00005071
Iteration 227/1000 | Loss: 0.00005071
Iteration 228/1000 | Loss: 0.00005071
Iteration 229/1000 | Loss: 0.00005070
Iteration 230/1000 | Loss: 0.00005070
Iteration 231/1000 | Loss: 0.00005070
Iteration 232/1000 | Loss: 0.00005069
Iteration 233/1000 | Loss: 0.00005069
Iteration 234/1000 | Loss: 0.00005069
Iteration 235/1000 | Loss: 0.00005069
Iteration 236/1000 | Loss: 0.00005069
Iteration 237/1000 | Loss: 0.00005068
Iteration 238/1000 | Loss: 0.00005068
Iteration 239/1000 | Loss: 0.00005068
Iteration 240/1000 | Loss: 0.00005068
Iteration 241/1000 | Loss: 0.00005068
Iteration 242/1000 | Loss: 0.00005068
Iteration 243/1000 | Loss: 0.00005068
Iteration 244/1000 | Loss: 0.00005067
Iteration 245/1000 | Loss: 0.00005067
Iteration 246/1000 | Loss: 0.00005067
Iteration 247/1000 | Loss: 0.00005067
Iteration 248/1000 | Loss: 0.00005067
Iteration 249/1000 | Loss: 0.00005067
Iteration 250/1000 | Loss: 0.00005066
Iteration 251/1000 | Loss: 0.00005066
Iteration 252/1000 | Loss: 0.00005066
Iteration 253/1000 | Loss: 0.00005066
Iteration 254/1000 | Loss: 0.00005066
Iteration 255/1000 | Loss: 0.00005066
Iteration 256/1000 | Loss: 0.00005066
Iteration 257/1000 | Loss: 0.00005066
Iteration 258/1000 | Loss: 0.00005066
Iteration 259/1000 | Loss: 0.00005065
Iteration 260/1000 | Loss: 0.00005065
Iteration 261/1000 | Loss: 0.00005065
Iteration 262/1000 | Loss: 0.00005065
Iteration 263/1000 | Loss: 0.00005065
Iteration 264/1000 | Loss: 0.00005065
Iteration 265/1000 | Loss: 0.00005065
Iteration 266/1000 | Loss: 0.00005064
Iteration 267/1000 | Loss: 0.00005064
Iteration 268/1000 | Loss: 0.00005064
Iteration 269/1000 | Loss: 0.00005064
Iteration 270/1000 | Loss: 0.00005064
Iteration 271/1000 | Loss: 0.00005064
Iteration 272/1000 | Loss: 0.00005064
Iteration 273/1000 | Loss: 0.00005064
Iteration 274/1000 | Loss: 0.00005064
Iteration 275/1000 | Loss: 0.00005063
Iteration 276/1000 | Loss: 0.00005063
Iteration 277/1000 | Loss: 0.00005063
Iteration 278/1000 | Loss: 0.00005062
Iteration 279/1000 | Loss: 0.00005062
Iteration 280/1000 | Loss: 0.00005061
Iteration 281/1000 | Loss: 0.00005061
Iteration 282/1000 | Loss: 0.00005061
Iteration 283/1000 | Loss: 0.00005060
Iteration 284/1000 | Loss: 0.00005060
Iteration 285/1000 | Loss: 0.00005060
Iteration 286/1000 | Loss: 0.00005060
Iteration 287/1000 | Loss: 0.00005059
Iteration 288/1000 | Loss: 0.00005059
Iteration 289/1000 | Loss: 0.00005059
Iteration 290/1000 | Loss: 0.00005059
Iteration 291/1000 | Loss: 0.00005058
Iteration 292/1000 | Loss: 0.00005058
Iteration 293/1000 | Loss: 0.00005058
Iteration 294/1000 | Loss: 0.00005058
Iteration 295/1000 | Loss: 0.00005058
Iteration 296/1000 | Loss: 0.00005058
Iteration 297/1000 | Loss: 0.00005058
Iteration 298/1000 | Loss: 0.00005058
Iteration 299/1000 | Loss: 0.00005058
Iteration 300/1000 | Loss: 0.00005057
Iteration 301/1000 | Loss: 0.00005057
Iteration 302/1000 | Loss: 0.00005057
Iteration 303/1000 | Loss: 0.00005057
Iteration 304/1000 | Loss: 0.00005057
Iteration 305/1000 | Loss: 0.00005057
Iteration 306/1000 | Loss: 0.00005057
Iteration 307/1000 | Loss: 0.00005057
Iteration 308/1000 | Loss: 0.00005057
Iteration 309/1000 | Loss: 0.00005057
Iteration 310/1000 | Loss: 0.00005056
Iteration 311/1000 | Loss: 0.00005056
Iteration 312/1000 | Loss: 0.00005056
Iteration 313/1000 | Loss: 0.00005056
Iteration 314/1000 | Loss: 0.00005056
Iteration 315/1000 | Loss: 0.00005056
Iteration 316/1000 | Loss: 0.00005056
Iteration 317/1000 | Loss: 0.00005056
Iteration 318/1000 | Loss: 0.00005056
Iteration 319/1000 | Loss: 0.00005056
Iteration 320/1000 | Loss: 0.00005056
Iteration 321/1000 | Loss: 0.00005056
Iteration 322/1000 | Loss: 0.00005055
Iteration 323/1000 | Loss: 0.00005055
Iteration 324/1000 | Loss: 0.00005055
Iteration 325/1000 | Loss: 0.00005055
Iteration 326/1000 | Loss: 0.00005055
Iteration 327/1000 | Loss: 0.00005055
Iteration 328/1000 | Loss: 0.00005055
Iteration 329/1000 | Loss: 0.00005055
Iteration 330/1000 | Loss: 0.00005055
Iteration 331/1000 | Loss: 0.00005054
Iteration 332/1000 | Loss: 0.00005054
Iteration 333/1000 | Loss: 0.00005054
Iteration 334/1000 | Loss: 0.00005054
Iteration 335/1000 | Loss: 0.00005054
Iteration 336/1000 | Loss: 0.00005054
Iteration 337/1000 | Loss: 0.00005054
Iteration 338/1000 | Loss: 0.00005054
Iteration 339/1000 | Loss: 0.00005054
Iteration 340/1000 | Loss: 0.00005054
Iteration 341/1000 | Loss: 0.00005054
Iteration 342/1000 | Loss: 0.00005053
Iteration 343/1000 | Loss: 0.00005053
Iteration 344/1000 | Loss: 0.00005053
Iteration 345/1000 | Loss: 0.00005053
Iteration 346/1000 | Loss: 0.00005053
Iteration 347/1000 | Loss: 0.00005053
Iteration 348/1000 | Loss: 0.00005053
Iteration 349/1000 | Loss: 0.00005053
Iteration 350/1000 | Loss: 0.00005053
Iteration 351/1000 | Loss: 0.00005053
Iteration 352/1000 | Loss: 0.00005053
Iteration 353/1000 | Loss: 0.00005053
Iteration 354/1000 | Loss: 0.00005053
Iteration 355/1000 | Loss: 0.00005053
Iteration 356/1000 | Loss: 0.00005053
Iteration 357/1000 | Loss: 0.00005052
Iteration 358/1000 | Loss: 0.00005052
Iteration 359/1000 | Loss: 0.00005052
Iteration 360/1000 | Loss: 0.00005052
Iteration 361/1000 | Loss: 0.00005052
Iteration 362/1000 | Loss: 0.00005052
Iteration 363/1000 | Loss: 0.00005052
Iteration 364/1000 | Loss: 0.00005052
Iteration 365/1000 | Loss: 0.00005052
Iteration 366/1000 | Loss: 0.00005052
Iteration 367/1000 | Loss: 0.00005052
Iteration 368/1000 | Loss: 0.00005052
Iteration 369/1000 | Loss: 0.00005052
Iteration 370/1000 | Loss: 0.00005052
Iteration 371/1000 | Loss: 0.00005052
Iteration 372/1000 | Loss: 0.00005052
Iteration 373/1000 | Loss: 0.00005052
Iteration 374/1000 | Loss: 0.00005052
Iteration 375/1000 | Loss: 0.00005052
Iteration 376/1000 | Loss: 0.00005051
Iteration 377/1000 | Loss: 0.00005051
Iteration 378/1000 | Loss: 0.00005051
Iteration 379/1000 | Loss: 0.00005051
Iteration 380/1000 | Loss: 0.00005051
Iteration 381/1000 | Loss: 0.00005051
Iteration 382/1000 | Loss: 0.00005051
Iteration 383/1000 | Loss: 0.00005051
Iteration 384/1000 | Loss: 0.00005051
Iteration 385/1000 | Loss: 0.00005051
Iteration 386/1000 | Loss: 0.00005051
Iteration 387/1000 | Loss: 0.00005051
Iteration 388/1000 | Loss: 0.00005051
Iteration 389/1000 | Loss: 0.00005051
Iteration 390/1000 | Loss: 0.00005051
Iteration 391/1000 | Loss: 0.00005051
Iteration 392/1000 | Loss: 0.00005051
Iteration 393/1000 | Loss: 0.00005051
Iteration 394/1000 | Loss: 0.00005050
Iteration 395/1000 | Loss: 0.00005050
Iteration 396/1000 | Loss: 0.00005050
Iteration 397/1000 | Loss: 0.00005050
Iteration 398/1000 | Loss: 0.00005050
Iteration 399/1000 | Loss: 0.00005050
Iteration 400/1000 | Loss: 0.00005050
Iteration 401/1000 | Loss: 0.00005050
Iteration 402/1000 | Loss: 0.00005050
Iteration 403/1000 | Loss: 0.00005050
Iteration 404/1000 | Loss: 0.00005050
Iteration 405/1000 | Loss: 0.00005050
Iteration 406/1000 | Loss: 0.00005050
Iteration 407/1000 | Loss: 0.00005050
Iteration 408/1000 | Loss: 0.00005050
Iteration 409/1000 | Loss: 0.00005050
Iteration 410/1000 | Loss: 0.00005050
Iteration 411/1000 | Loss: 0.00005050
Iteration 412/1000 | Loss: 0.00005050
Iteration 413/1000 | Loss: 0.00005050
Iteration 414/1000 | Loss: 0.00005050
Iteration 415/1000 | Loss: 0.00005050
Iteration 416/1000 | Loss: 0.00005050
Iteration 417/1000 | Loss: 0.00005050
Iteration 418/1000 | Loss: 0.00005050
Iteration 419/1000 | Loss: 0.00005050
Iteration 420/1000 | Loss: 0.00005050
Iteration 421/1000 | Loss: 0.00005050
Iteration 422/1000 | Loss: 0.00005050
Iteration 423/1000 | Loss: 0.00005050
Iteration 424/1000 | Loss: 0.00005050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 424. Stopping optimization.
Last 5 losses: [5.050353502156213e-05, 5.050353502156213e-05, 5.050353502156213e-05, 5.050353502156213e-05, 5.050353502156213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.050353502156213e-05

Optimization complete. Final v2v error: 4.048093318939209 mm

Highest mean error: 11.900428771972656 mm for frame 3

Lowest mean error: 2.9895381927490234 mm for frame 173

Saving results

Total time: 385.57324409484863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433909
Iteration 2/25 | Loss: 0.00116020
Iteration 3/25 | Loss: 0.00108287
Iteration 4/25 | Loss: 0.00107028
Iteration 5/25 | Loss: 0.00106718
Iteration 6/25 | Loss: 0.00106718
Iteration 7/25 | Loss: 0.00106718
Iteration 8/25 | Loss: 0.00106718
Iteration 9/25 | Loss: 0.00106718
Iteration 10/25 | Loss: 0.00106718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010671793716028333, 0.0010671793716028333, 0.0010671793716028333, 0.0010671793716028333, 0.0010671793716028333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010671793716028333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.60511541
Iteration 2/25 | Loss: 0.00080082
Iteration 3/25 | Loss: 0.00080082
Iteration 4/25 | Loss: 0.00080082
Iteration 5/25 | Loss: 0.00080082
Iteration 6/25 | Loss: 0.00080082
Iteration 7/25 | Loss: 0.00080082
Iteration 8/25 | Loss: 0.00080082
Iteration 9/25 | Loss: 0.00080082
Iteration 10/25 | Loss: 0.00080082
Iteration 11/25 | Loss: 0.00080082
Iteration 12/25 | Loss: 0.00080082
Iteration 13/25 | Loss: 0.00080082
Iteration 14/25 | Loss: 0.00080082
Iteration 15/25 | Loss: 0.00080082
Iteration 16/25 | Loss: 0.00080082
Iteration 17/25 | Loss: 0.00080082
Iteration 18/25 | Loss: 0.00080082
Iteration 19/25 | Loss: 0.00080082
Iteration 20/25 | Loss: 0.00080082
Iteration 21/25 | Loss: 0.00080082
Iteration 22/25 | Loss: 0.00080082
Iteration 23/25 | Loss: 0.00080082
Iteration 24/25 | Loss: 0.00080082
Iteration 25/25 | Loss: 0.00080082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080082
Iteration 2/1000 | Loss: 0.00001654
Iteration 3/1000 | Loss: 0.00001336
Iteration 4/1000 | Loss: 0.00001238
Iteration 5/1000 | Loss: 0.00001169
Iteration 6/1000 | Loss: 0.00001134
Iteration 7/1000 | Loss: 0.00001123
Iteration 8/1000 | Loss: 0.00001079
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001056
Iteration 11/1000 | Loss: 0.00001052
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001034
Iteration 14/1000 | Loss: 0.00001027
Iteration 15/1000 | Loss: 0.00001026
Iteration 16/1000 | Loss: 0.00001008
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001008
Iteration 19/1000 | Loss: 0.00001008
Iteration 20/1000 | Loss: 0.00001007
Iteration 21/1000 | Loss: 0.00001007
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00000999
Iteration 26/1000 | Loss: 0.00000990
Iteration 27/1000 | Loss: 0.00000981
Iteration 28/1000 | Loss: 0.00000975
Iteration 29/1000 | Loss: 0.00000972
Iteration 30/1000 | Loss: 0.00000972
Iteration 31/1000 | Loss: 0.00000971
Iteration 32/1000 | Loss: 0.00000970
Iteration 33/1000 | Loss: 0.00000969
Iteration 34/1000 | Loss: 0.00000969
Iteration 35/1000 | Loss: 0.00000968
Iteration 36/1000 | Loss: 0.00000968
Iteration 37/1000 | Loss: 0.00000968
Iteration 38/1000 | Loss: 0.00000967
Iteration 39/1000 | Loss: 0.00000967
Iteration 40/1000 | Loss: 0.00000967
Iteration 41/1000 | Loss: 0.00000967
Iteration 42/1000 | Loss: 0.00000967
Iteration 43/1000 | Loss: 0.00000966
Iteration 44/1000 | Loss: 0.00000966
Iteration 45/1000 | Loss: 0.00000966
Iteration 46/1000 | Loss: 0.00000966
Iteration 47/1000 | Loss: 0.00000961
Iteration 48/1000 | Loss: 0.00000961
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000955
Iteration 56/1000 | Loss: 0.00000954
Iteration 57/1000 | Loss: 0.00000953
Iteration 58/1000 | Loss: 0.00000952
Iteration 59/1000 | Loss: 0.00000952
Iteration 60/1000 | Loss: 0.00000951
Iteration 61/1000 | Loss: 0.00000950
Iteration 62/1000 | Loss: 0.00000949
Iteration 63/1000 | Loss: 0.00000947
Iteration 64/1000 | Loss: 0.00000942
Iteration 65/1000 | Loss: 0.00000942
Iteration 66/1000 | Loss: 0.00000942
Iteration 67/1000 | Loss: 0.00000942
Iteration 68/1000 | Loss: 0.00000941
Iteration 69/1000 | Loss: 0.00000941
Iteration 70/1000 | Loss: 0.00000941
Iteration 71/1000 | Loss: 0.00000941
Iteration 72/1000 | Loss: 0.00000941
Iteration 73/1000 | Loss: 0.00000941
Iteration 74/1000 | Loss: 0.00000941
Iteration 75/1000 | Loss: 0.00000940
Iteration 76/1000 | Loss: 0.00000940
Iteration 77/1000 | Loss: 0.00000939
Iteration 78/1000 | Loss: 0.00000939
Iteration 79/1000 | Loss: 0.00000939
Iteration 80/1000 | Loss: 0.00000939
Iteration 81/1000 | Loss: 0.00000939
Iteration 82/1000 | Loss: 0.00000938
Iteration 83/1000 | Loss: 0.00000938
Iteration 84/1000 | Loss: 0.00000938
Iteration 85/1000 | Loss: 0.00000938
Iteration 86/1000 | Loss: 0.00000937
Iteration 87/1000 | Loss: 0.00000937
Iteration 88/1000 | Loss: 0.00000937
Iteration 89/1000 | Loss: 0.00000937
Iteration 90/1000 | Loss: 0.00000936
Iteration 91/1000 | Loss: 0.00000936
Iteration 92/1000 | Loss: 0.00000936
Iteration 93/1000 | Loss: 0.00000936
Iteration 94/1000 | Loss: 0.00000935
Iteration 95/1000 | Loss: 0.00000935
Iteration 96/1000 | Loss: 0.00000935
Iteration 97/1000 | Loss: 0.00000935
Iteration 98/1000 | Loss: 0.00000935
Iteration 99/1000 | Loss: 0.00000935
Iteration 100/1000 | Loss: 0.00000935
Iteration 101/1000 | Loss: 0.00000934
Iteration 102/1000 | Loss: 0.00000934
Iteration 103/1000 | Loss: 0.00000933
Iteration 104/1000 | Loss: 0.00000933
Iteration 105/1000 | Loss: 0.00000933
Iteration 106/1000 | Loss: 0.00000933
Iteration 107/1000 | Loss: 0.00000933
Iteration 108/1000 | Loss: 0.00000933
Iteration 109/1000 | Loss: 0.00000933
Iteration 110/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [9.33417322812602e-06, 9.33417322812602e-06, 9.33417322812602e-06, 9.33417322812602e-06, 9.33417322812602e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.33417322812602e-06

Optimization complete. Final v2v error: 2.6576101779937744 mm

Highest mean error: 2.9901247024536133 mm for frame 43

Lowest mean error: 2.4357457160949707 mm for frame 26

Saving results

Total time: 40.40198612213135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869123
Iteration 2/25 | Loss: 0.00153719
Iteration 3/25 | Loss: 0.00124108
Iteration 4/25 | Loss: 0.00120727
Iteration 5/25 | Loss: 0.00120331
Iteration 6/25 | Loss: 0.00120017
Iteration 7/25 | Loss: 0.00119076
Iteration 8/25 | Loss: 0.00118222
Iteration 9/25 | Loss: 0.00117499
Iteration 10/25 | Loss: 0.00117382
Iteration 11/25 | Loss: 0.00116998
Iteration 12/25 | Loss: 0.00116936
Iteration 13/25 | Loss: 0.00116921
Iteration 14/25 | Loss: 0.00116912
Iteration 15/25 | Loss: 0.00116910
Iteration 16/25 | Loss: 0.00116910
Iteration 17/25 | Loss: 0.00116909
Iteration 18/25 | Loss: 0.00116906
Iteration 19/25 | Loss: 0.00116905
Iteration 20/25 | Loss: 0.00116905
Iteration 21/25 | Loss: 0.00116896
Iteration 22/25 | Loss: 0.00117077
Iteration 23/25 | Loss: 0.00116671
Iteration 24/25 | Loss: 0.00116603
Iteration 25/25 | Loss: 0.00116591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91024804
Iteration 2/25 | Loss: 0.00044149
Iteration 3/25 | Loss: 0.00044149
Iteration 4/25 | Loss: 0.00044149
Iteration 5/25 | Loss: 0.00044148
Iteration 6/25 | Loss: 0.00044148
Iteration 7/25 | Loss: 0.00044148
Iteration 8/25 | Loss: 0.00044148
Iteration 9/25 | Loss: 0.00044148
Iteration 10/25 | Loss: 0.00044148
Iteration 11/25 | Loss: 0.00044148
Iteration 12/25 | Loss: 0.00044148
Iteration 13/25 | Loss: 0.00044148
Iteration 14/25 | Loss: 0.00044148
Iteration 15/25 | Loss: 0.00044148
Iteration 16/25 | Loss: 0.00044148
Iteration 17/25 | Loss: 0.00044148
Iteration 18/25 | Loss: 0.00044148
Iteration 19/25 | Loss: 0.00044148
Iteration 20/25 | Loss: 0.00044148
Iteration 21/25 | Loss: 0.00044148
Iteration 22/25 | Loss: 0.00044148
Iteration 23/25 | Loss: 0.00044148
Iteration 24/25 | Loss: 0.00044148
Iteration 25/25 | Loss: 0.00044148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044148
Iteration 2/1000 | Loss: 0.00003863
Iteration 3/1000 | Loss: 0.00003023
Iteration 4/1000 | Loss: 0.00002809
Iteration 5/1000 | Loss: 0.00002698
Iteration 6/1000 | Loss: 0.00002633
Iteration 7/1000 | Loss: 0.00002562
Iteration 8/1000 | Loss: 0.00002525
Iteration 9/1000 | Loss: 0.00002491
Iteration 10/1000 | Loss: 0.00002469
Iteration 11/1000 | Loss: 0.00002453
Iteration 12/1000 | Loss: 0.00002452
Iteration 13/1000 | Loss: 0.00002438
Iteration 14/1000 | Loss: 0.00002438
Iteration 15/1000 | Loss: 0.00002437
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002428
Iteration 18/1000 | Loss: 0.00002428
Iteration 19/1000 | Loss: 0.00002428
Iteration 20/1000 | Loss: 0.00002427
Iteration 21/1000 | Loss: 0.00002427
Iteration 22/1000 | Loss: 0.00002427
Iteration 23/1000 | Loss: 0.00002427
Iteration 24/1000 | Loss: 0.00002426
Iteration 25/1000 | Loss: 0.00002424
Iteration 26/1000 | Loss: 0.00002423
Iteration 27/1000 | Loss: 0.00002422
Iteration 28/1000 | Loss: 0.00002422
Iteration 29/1000 | Loss: 0.00002421
Iteration 30/1000 | Loss: 0.00002421
Iteration 31/1000 | Loss: 0.00002421
Iteration 32/1000 | Loss: 0.00002421
Iteration 33/1000 | Loss: 0.00002421
Iteration 34/1000 | Loss: 0.00002421
Iteration 35/1000 | Loss: 0.00002421
Iteration 36/1000 | Loss: 0.00002421
Iteration 37/1000 | Loss: 0.00002419
Iteration 38/1000 | Loss: 0.00002419
Iteration 39/1000 | Loss: 0.00002419
Iteration 40/1000 | Loss: 0.00002419
Iteration 41/1000 | Loss: 0.00002419
Iteration 42/1000 | Loss: 0.00002419
Iteration 43/1000 | Loss: 0.00002419
Iteration 44/1000 | Loss: 0.00002419
Iteration 45/1000 | Loss: 0.00002419
Iteration 46/1000 | Loss: 0.00002418
Iteration 47/1000 | Loss: 0.00002418
Iteration 48/1000 | Loss: 0.00002418
Iteration 49/1000 | Loss: 0.00002418
Iteration 50/1000 | Loss: 0.00002418
Iteration 51/1000 | Loss: 0.00002418
Iteration 52/1000 | Loss: 0.00002418
Iteration 53/1000 | Loss: 0.00002418
Iteration 54/1000 | Loss: 0.00002418
Iteration 55/1000 | Loss: 0.00002418
Iteration 56/1000 | Loss: 0.00002418
Iteration 57/1000 | Loss: 0.00002417
Iteration 58/1000 | Loss: 0.00002417
Iteration 59/1000 | Loss: 0.00002417
Iteration 60/1000 | Loss: 0.00002417
Iteration 61/1000 | Loss: 0.00002417
Iteration 62/1000 | Loss: 0.00002417
Iteration 63/1000 | Loss: 0.00002417
Iteration 64/1000 | Loss: 0.00002416
Iteration 65/1000 | Loss: 0.00002416
Iteration 66/1000 | Loss: 0.00002416
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002416
Iteration 69/1000 | Loss: 0.00002416
Iteration 70/1000 | Loss: 0.00002416
Iteration 71/1000 | Loss: 0.00002416
Iteration 72/1000 | Loss: 0.00002416
Iteration 73/1000 | Loss: 0.00002415
Iteration 74/1000 | Loss: 0.00002415
Iteration 75/1000 | Loss: 0.00002415
Iteration 76/1000 | Loss: 0.00002415
Iteration 77/1000 | Loss: 0.00002415
Iteration 78/1000 | Loss: 0.00002415
Iteration 79/1000 | Loss: 0.00002415
Iteration 80/1000 | Loss: 0.00002415
Iteration 81/1000 | Loss: 0.00002415
Iteration 82/1000 | Loss: 0.00002415
Iteration 83/1000 | Loss: 0.00002415
Iteration 84/1000 | Loss: 0.00002415
Iteration 85/1000 | Loss: 0.00002415
Iteration 86/1000 | Loss: 0.00002415
Iteration 87/1000 | Loss: 0.00002415
Iteration 88/1000 | Loss: 0.00002415
Iteration 89/1000 | Loss: 0.00002415
Iteration 90/1000 | Loss: 0.00002415
Iteration 91/1000 | Loss: 0.00002415
Iteration 92/1000 | Loss: 0.00002415
Iteration 93/1000 | Loss: 0.00002415
Iteration 94/1000 | Loss: 0.00002415
Iteration 95/1000 | Loss: 0.00002415
Iteration 96/1000 | Loss: 0.00002415
Iteration 97/1000 | Loss: 0.00002415
Iteration 98/1000 | Loss: 0.00002415
Iteration 99/1000 | Loss: 0.00002415
Iteration 100/1000 | Loss: 0.00002415
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Iteration 103/1000 | Loss: 0.00002415
Iteration 104/1000 | Loss: 0.00002415
Iteration 105/1000 | Loss: 0.00002415
Iteration 106/1000 | Loss: 0.00002415
Iteration 107/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.4150582248694263e-05, 2.4150582248694263e-05, 2.4150582248694263e-05, 2.4150582248694263e-05, 2.4150582248694263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4150582248694263e-05

Optimization complete. Final v2v error: 4.114045143127441 mm

Highest mean error: 4.217286109924316 mm for frame 42

Lowest mean error: 4.037445068359375 mm for frame 15

Saving results

Total time: 57.23518705368042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839132
Iteration 2/25 | Loss: 0.00118678
Iteration 3/25 | Loss: 0.00109563
Iteration 4/25 | Loss: 0.00108889
Iteration 5/25 | Loss: 0.00108733
Iteration 6/25 | Loss: 0.00108733
Iteration 7/25 | Loss: 0.00108733
Iteration 8/25 | Loss: 0.00108733
Iteration 9/25 | Loss: 0.00108733
Iteration 10/25 | Loss: 0.00108733
Iteration 11/25 | Loss: 0.00108733
Iteration 12/25 | Loss: 0.00108733
Iteration 13/25 | Loss: 0.00108733
Iteration 14/25 | Loss: 0.00108733
Iteration 15/25 | Loss: 0.00108733
Iteration 16/25 | Loss: 0.00108733
Iteration 17/25 | Loss: 0.00108733
Iteration 18/25 | Loss: 0.00108733
Iteration 19/25 | Loss: 0.00108733
Iteration 20/25 | Loss: 0.00108733
Iteration 21/25 | Loss: 0.00108733
Iteration 22/25 | Loss: 0.00108733
Iteration 23/25 | Loss: 0.00108733
Iteration 24/25 | Loss: 0.00108733
Iteration 25/25 | Loss: 0.00108733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.97183657
Iteration 2/25 | Loss: 0.00085034
Iteration 3/25 | Loss: 0.00085034
Iteration 4/25 | Loss: 0.00085034
Iteration 5/25 | Loss: 0.00085034
Iteration 6/25 | Loss: 0.00085034
Iteration 7/25 | Loss: 0.00085034
Iteration 8/25 | Loss: 0.00085034
Iteration 9/25 | Loss: 0.00085034
Iteration 10/25 | Loss: 0.00085034
Iteration 11/25 | Loss: 0.00085034
Iteration 12/25 | Loss: 0.00085034
Iteration 13/25 | Loss: 0.00085034
Iteration 14/25 | Loss: 0.00085034
Iteration 15/25 | Loss: 0.00085034
Iteration 16/25 | Loss: 0.00085034
Iteration 17/25 | Loss: 0.00085034
Iteration 18/25 | Loss: 0.00085034
Iteration 19/25 | Loss: 0.00085034
Iteration 20/25 | Loss: 0.00085034
Iteration 21/25 | Loss: 0.00085034
Iteration 22/25 | Loss: 0.00085034
Iteration 23/25 | Loss: 0.00085034
Iteration 24/25 | Loss: 0.00085034
Iteration 25/25 | Loss: 0.00085034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085034
Iteration 2/1000 | Loss: 0.00001758
Iteration 3/1000 | Loss: 0.00001233
Iteration 4/1000 | Loss: 0.00001107
Iteration 5/1000 | Loss: 0.00001037
Iteration 6/1000 | Loss: 0.00000996
Iteration 7/1000 | Loss: 0.00000982
Iteration 8/1000 | Loss: 0.00000957
Iteration 9/1000 | Loss: 0.00000948
Iteration 10/1000 | Loss: 0.00000932
Iteration 11/1000 | Loss: 0.00000919
Iteration 12/1000 | Loss: 0.00000918
Iteration 13/1000 | Loss: 0.00000916
Iteration 14/1000 | Loss: 0.00000915
Iteration 15/1000 | Loss: 0.00000912
Iteration 16/1000 | Loss: 0.00000912
Iteration 17/1000 | Loss: 0.00000911
Iteration 18/1000 | Loss: 0.00000911
Iteration 19/1000 | Loss: 0.00000911
Iteration 20/1000 | Loss: 0.00000911
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000904
Iteration 23/1000 | Loss: 0.00000903
Iteration 24/1000 | Loss: 0.00000902
Iteration 25/1000 | Loss: 0.00000901
Iteration 26/1000 | Loss: 0.00000901
Iteration 27/1000 | Loss: 0.00000901
Iteration 28/1000 | Loss: 0.00000900
Iteration 29/1000 | Loss: 0.00000900
Iteration 30/1000 | Loss: 0.00000899
Iteration 31/1000 | Loss: 0.00000899
Iteration 32/1000 | Loss: 0.00000899
Iteration 33/1000 | Loss: 0.00000898
Iteration 34/1000 | Loss: 0.00000898
Iteration 35/1000 | Loss: 0.00000896
Iteration 36/1000 | Loss: 0.00000896
Iteration 37/1000 | Loss: 0.00000895
Iteration 38/1000 | Loss: 0.00000895
Iteration 39/1000 | Loss: 0.00000895
Iteration 40/1000 | Loss: 0.00000895
Iteration 41/1000 | Loss: 0.00000892
Iteration 42/1000 | Loss: 0.00000890
Iteration 43/1000 | Loss: 0.00000889
Iteration 44/1000 | Loss: 0.00000889
Iteration 45/1000 | Loss: 0.00000889
Iteration 46/1000 | Loss: 0.00000888
Iteration 47/1000 | Loss: 0.00000887
Iteration 48/1000 | Loss: 0.00000886
Iteration 49/1000 | Loss: 0.00000885
Iteration 50/1000 | Loss: 0.00000884
Iteration 51/1000 | Loss: 0.00000884
Iteration 52/1000 | Loss: 0.00000883
Iteration 53/1000 | Loss: 0.00000882
Iteration 54/1000 | Loss: 0.00000879
Iteration 55/1000 | Loss: 0.00000879
Iteration 56/1000 | Loss: 0.00000879
Iteration 57/1000 | Loss: 0.00000878
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000877
Iteration 63/1000 | Loss: 0.00000876
Iteration 64/1000 | Loss: 0.00000876
Iteration 65/1000 | Loss: 0.00000876
Iteration 66/1000 | Loss: 0.00000876
Iteration 67/1000 | Loss: 0.00000876
Iteration 68/1000 | Loss: 0.00000875
Iteration 69/1000 | Loss: 0.00000875
Iteration 70/1000 | Loss: 0.00000875
Iteration 71/1000 | Loss: 0.00000875
Iteration 72/1000 | Loss: 0.00000874
Iteration 73/1000 | Loss: 0.00000874
Iteration 74/1000 | Loss: 0.00000873
Iteration 75/1000 | Loss: 0.00000873
Iteration 76/1000 | Loss: 0.00000872
Iteration 77/1000 | Loss: 0.00000872
Iteration 78/1000 | Loss: 0.00000872
Iteration 79/1000 | Loss: 0.00000872
Iteration 80/1000 | Loss: 0.00000872
Iteration 81/1000 | Loss: 0.00000872
Iteration 82/1000 | Loss: 0.00000872
Iteration 83/1000 | Loss: 0.00000871
Iteration 84/1000 | Loss: 0.00000871
Iteration 85/1000 | Loss: 0.00000871
Iteration 86/1000 | Loss: 0.00000871
Iteration 87/1000 | Loss: 0.00000871
Iteration 88/1000 | Loss: 0.00000870
Iteration 89/1000 | Loss: 0.00000870
Iteration 90/1000 | Loss: 0.00000870
Iteration 91/1000 | Loss: 0.00000870
Iteration 92/1000 | Loss: 0.00000869
Iteration 93/1000 | Loss: 0.00000869
Iteration 94/1000 | Loss: 0.00000869
Iteration 95/1000 | Loss: 0.00000868
Iteration 96/1000 | Loss: 0.00000868
Iteration 97/1000 | Loss: 0.00000868
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000867
Iteration 103/1000 | Loss: 0.00000867
Iteration 104/1000 | Loss: 0.00000867
Iteration 105/1000 | Loss: 0.00000867
Iteration 106/1000 | Loss: 0.00000867
Iteration 107/1000 | Loss: 0.00000866
Iteration 108/1000 | Loss: 0.00000865
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000864
Iteration 111/1000 | Loss: 0.00000864
Iteration 112/1000 | Loss: 0.00000864
Iteration 113/1000 | Loss: 0.00000863
Iteration 114/1000 | Loss: 0.00000863
Iteration 115/1000 | Loss: 0.00000863
Iteration 116/1000 | Loss: 0.00000863
Iteration 117/1000 | Loss: 0.00000862
Iteration 118/1000 | Loss: 0.00000862
Iteration 119/1000 | Loss: 0.00000862
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000861
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000860
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000860
Iteration 131/1000 | Loss: 0.00000860
Iteration 132/1000 | Loss: 0.00000860
Iteration 133/1000 | Loss: 0.00000859
Iteration 134/1000 | Loss: 0.00000859
Iteration 135/1000 | Loss: 0.00000859
Iteration 136/1000 | Loss: 0.00000859
Iteration 137/1000 | Loss: 0.00000859
Iteration 138/1000 | Loss: 0.00000859
Iteration 139/1000 | Loss: 0.00000859
Iteration 140/1000 | Loss: 0.00000858
Iteration 141/1000 | Loss: 0.00000858
Iteration 142/1000 | Loss: 0.00000858
Iteration 143/1000 | Loss: 0.00000858
Iteration 144/1000 | Loss: 0.00000858
Iteration 145/1000 | Loss: 0.00000857
Iteration 146/1000 | Loss: 0.00000857
Iteration 147/1000 | Loss: 0.00000857
Iteration 148/1000 | Loss: 0.00000857
Iteration 149/1000 | Loss: 0.00000857
Iteration 150/1000 | Loss: 0.00000857
Iteration 151/1000 | Loss: 0.00000857
Iteration 152/1000 | Loss: 0.00000857
Iteration 153/1000 | Loss: 0.00000857
Iteration 154/1000 | Loss: 0.00000857
Iteration 155/1000 | Loss: 0.00000857
Iteration 156/1000 | Loss: 0.00000857
Iteration 157/1000 | Loss: 0.00000857
Iteration 158/1000 | Loss: 0.00000857
Iteration 159/1000 | Loss: 0.00000857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [8.571139915147796e-06, 8.571139915147796e-06, 8.571139915147796e-06, 8.571139915147796e-06, 8.571139915147796e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.571139915147796e-06

Optimization complete. Final v2v error: 2.5334441661834717 mm

Highest mean error: 2.762822389602661 mm for frame 239

Lowest mean error: 2.310883045196533 mm for frame 169

Saving results

Total time: 40.621623516082764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055088
Iteration 2/25 | Loss: 0.01055088
Iteration 3/25 | Loss: 0.01055088
Iteration 4/25 | Loss: 0.01055088
Iteration 5/25 | Loss: 0.01055088
Iteration 6/25 | Loss: 0.01055088
Iteration 7/25 | Loss: 0.01055087
Iteration 8/25 | Loss: 0.01055087
Iteration 9/25 | Loss: 0.01055087
Iteration 10/25 | Loss: 0.01055087
Iteration 11/25 | Loss: 0.00522391
Iteration 12/25 | Loss: 0.00386473
Iteration 13/25 | Loss: 0.00287686
Iteration 14/25 | Loss: 0.00249848
Iteration 15/25 | Loss: 0.00228889
Iteration 16/25 | Loss: 0.00222552
Iteration 17/25 | Loss: 0.00220921
Iteration 18/25 | Loss: 0.00214485
Iteration 19/25 | Loss: 0.00189272
Iteration 20/25 | Loss: 0.00174279
Iteration 21/25 | Loss: 0.00165882
Iteration 22/25 | Loss: 0.00157456
Iteration 23/25 | Loss: 0.00154326
Iteration 24/25 | Loss: 0.00153842
Iteration 25/25 | Loss: 0.00154634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66211069
Iteration 2/25 | Loss: 0.00185229
Iteration 3/25 | Loss: 0.00185229
Iteration 4/25 | Loss: 0.00185229
Iteration 5/25 | Loss: 0.00185229
Iteration 6/25 | Loss: 0.00185229
Iteration 7/25 | Loss: 0.00185229
Iteration 8/25 | Loss: 0.00185229
Iteration 9/25 | Loss: 0.00185229
Iteration 10/25 | Loss: 0.00185229
Iteration 11/25 | Loss: 0.00185229
Iteration 12/25 | Loss: 0.00185229
Iteration 13/25 | Loss: 0.00185229
Iteration 14/25 | Loss: 0.00185229
Iteration 15/25 | Loss: 0.00185229
Iteration 16/25 | Loss: 0.00185229
Iteration 17/25 | Loss: 0.00185229
Iteration 18/25 | Loss: 0.00185229
Iteration 19/25 | Loss: 0.00185229
Iteration 20/25 | Loss: 0.00185229
Iteration 21/25 | Loss: 0.00185229
Iteration 22/25 | Loss: 0.00185229
Iteration 23/25 | Loss: 0.00185229
Iteration 24/25 | Loss: 0.00185229
Iteration 25/25 | Loss: 0.00185229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185229
Iteration 2/1000 | Loss: 0.00087076
Iteration 3/1000 | Loss: 0.00027619
Iteration 4/1000 | Loss: 0.00043469
Iteration 5/1000 | Loss: 0.00060266
Iteration 6/1000 | Loss: 0.00079700
Iteration 7/1000 | Loss: 0.00067239
Iteration 8/1000 | Loss: 0.00036927
Iteration 9/1000 | Loss: 0.00030697
Iteration 10/1000 | Loss: 0.00032786
Iteration 11/1000 | Loss: 0.00035210
Iteration 12/1000 | Loss: 0.00019582
Iteration 13/1000 | Loss: 0.00016732
Iteration 14/1000 | Loss: 0.00017971
Iteration 15/1000 | Loss: 0.00032874
Iteration 16/1000 | Loss: 0.00104453
Iteration 17/1000 | Loss: 0.00015606
Iteration 18/1000 | Loss: 0.00018470
Iteration 19/1000 | Loss: 0.00018113
Iteration 20/1000 | Loss: 0.00015368
Iteration 21/1000 | Loss: 0.00015928
Iteration 22/1000 | Loss: 0.00014557
Iteration 23/1000 | Loss: 0.00017421
Iteration 24/1000 | Loss: 0.00015223
Iteration 25/1000 | Loss: 0.00019487
Iteration 26/1000 | Loss: 0.00016907
Iteration 27/1000 | Loss: 0.00021362
Iteration 28/1000 | Loss: 0.00011942
Iteration 29/1000 | Loss: 0.00013540
Iteration 30/1000 | Loss: 0.00013529
Iteration 31/1000 | Loss: 0.00010828
Iteration 32/1000 | Loss: 0.00011771
Iteration 33/1000 | Loss: 0.00010994
Iteration 34/1000 | Loss: 0.00012950
Iteration 35/1000 | Loss: 0.00011564
Iteration 36/1000 | Loss: 0.00012375
Iteration 37/1000 | Loss: 0.00011339
Iteration 38/1000 | Loss: 0.00020915
Iteration 39/1000 | Loss: 0.00011582
Iteration 40/1000 | Loss: 0.00010072
Iteration 41/1000 | Loss: 0.00010178
Iteration 42/1000 | Loss: 0.00009919
Iteration 43/1000 | Loss: 0.00023287
Iteration 44/1000 | Loss: 0.00138340
Iteration 45/1000 | Loss: 0.00033339
Iteration 46/1000 | Loss: 0.00013835
Iteration 47/1000 | Loss: 0.00010581
Iteration 48/1000 | Loss: 0.00025094
Iteration 49/1000 | Loss: 0.00011042
Iteration 50/1000 | Loss: 0.00010873
Iteration 51/1000 | Loss: 0.00009391
Iteration 52/1000 | Loss: 0.00006274
Iteration 53/1000 | Loss: 0.00006209
Iteration 54/1000 | Loss: 0.00005209
Iteration 55/1000 | Loss: 0.00005776
Iteration 56/1000 | Loss: 0.00005713
Iteration 57/1000 | Loss: 0.00006060
Iteration 58/1000 | Loss: 0.00005253
Iteration 59/1000 | Loss: 0.00005831
Iteration 60/1000 | Loss: 0.00020337
Iteration 61/1000 | Loss: 0.00108779
Iteration 62/1000 | Loss: 0.00008444
Iteration 63/1000 | Loss: 0.00006803
Iteration 64/1000 | Loss: 0.00009501
Iteration 65/1000 | Loss: 0.00009523
Iteration 66/1000 | Loss: 0.00013570
Iteration 67/1000 | Loss: 0.00008700
Iteration 68/1000 | Loss: 0.00011528
Iteration 69/1000 | Loss: 0.00005994
Iteration 70/1000 | Loss: 0.00009314
Iteration 71/1000 | Loss: 0.00004931
Iteration 72/1000 | Loss: 0.00009198
Iteration 73/1000 | Loss: 0.00006345
Iteration 74/1000 | Loss: 0.00005927
Iteration 75/1000 | Loss: 0.00009519
Iteration 76/1000 | Loss: 0.00008359
Iteration 77/1000 | Loss: 0.00005663
Iteration 78/1000 | Loss: 0.00005487
Iteration 79/1000 | Loss: 0.00006932
Iteration 80/1000 | Loss: 0.00006148
Iteration 81/1000 | Loss: 0.00006739
Iteration 82/1000 | Loss: 0.00009331
Iteration 83/1000 | Loss: 0.00079038
Iteration 84/1000 | Loss: 0.00160701
Iteration 85/1000 | Loss: 0.00011706
Iteration 86/1000 | Loss: 0.00004557
Iteration 87/1000 | Loss: 0.00004027
Iteration 88/1000 | Loss: 0.00003778
Iteration 89/1000 | Loss: 0.00003600
Iteration 90/1000 | Loss: 0.00003499
Iteration 91/1000 | Loss: 0.00003414
Iteration 92/1000 | Loss: 0.00003366
Iteration 93/1000 | Loss: 0.00003320
Iteration 94/1000 | Loss: 0.00003294
Iteration 95/1000 | Loss: 0.00003289
Iteration 96/1000 | Loss: 0.00003288
Iteration 97/1000 | Loss: 0.00003288
Iteration 98/1000 | Loss: 0.00003269
Iteration 99/1000 | Loss: 0.00003255
Iteration 100/1000 | Loss: 0.00003252
Iteration 101/1000 | Loss: 0.00003251
Iteration 102/1000 | Loss: 0.00003242
Iteration 103/1000 | Loss: 0.00003241
Iteration 104/1000 | Loss: 0.00003236
Iteration 105/1000 | Loss: 0.00003235
Iteration 106/1000 | Loss: 0.00003235
Iteration 107/1000 | Loss: 0.00003235
Iteration 108/1000 | Loss: 0.00003235
Iteration 109/1000 | Loss: 0.00003235
Iteration 110/1000 | Loss: 0.00003235
Iteration 111/1000 | Loss: 0.00003235
Iteration 112/1000 | Loss: 0.00003235
Iteration 113/1000 | Loss: 0.00003235
Iteration 114/1000 | Loss: 0.00003235
Iteration 115/1000 | Loss: 0.00003231
Iteration 116/1000 | Loss: 0.00003231
Iteration 117/1000 | Loss: 0.00003231
Iteration 118/1000 | Loss: 0.00003230
Iteration 119/1000 | Loss: 0.00003229
Iteration 120/1000 | Loss: 0.00003229
Iteration 121/1000 | Loss: 0.00003228
Iteration 122/1000 | Loss: 0.00003228
Iteration 123/1000 | Loss: 0.00003228
Iteration 124/1000 | Loss: 0.00003228
Iteration 125/1000 | Loss: 0.00003227
Iteration 126/1000 | Loss: 0.00003227
Iteration 127/1000 | Loss: 0.00003227
Iteration 128/1000 | Loss: 0.00003227
Iteration 129/1000 | Loss: 0.00003227
Iteration 130/1000 | Loss: 0.00003226
Iteration 131/1000 | Loss: 0.00003226
Iteration 132/1000 | Loss: 0.00003226
Iteration 133/1000 | Loss: 0.00003225
Iteration 134/1000 | Loss: 0.00003225
Iteration 135/1000 | Loss: 0.00003224
Iteration 136/1000 | Loss: 0.00003224
Iteration 137/1000 | Loss: 0.00003224
Iteration 138/1000 | Loss: 0.00003223
Iteration 139/1000 | Loss: 0.00003222
Iteration 140/1000 | Loss: 0.00003222
Iteration 141/1000 | Loss: 0.00003222
Iteration 142/1000 | Loss: 0.00003222
Iteration 143/1000 | Loss: 0.00003222
Iteration 144/1000 | Loss: 0.00003222
Iteration 145/1000 | Loss: 0.00003222
Iteration 146/1000 | Loss: 0.00003222
Iteration 147/1000 | Loss: 0.00003221
Iteration 148/1000 | Loss: 0.00003221
Iteration 149/1000 | Loss: 0.00003221
Iteration 150/1000 | Loss: 0.00003221
Iteration 151/1000 | Loss: 0.00003221
Iteration 152/1000 | Loss: 0.00003221
Iteration 153/1000 | Loss: 0.00003221
Iteration 154/1000 | Loss: 0.00003221
Iteration 155/1000 | Loss: 0.00003221
Iteration 156/1000 | Loss: 0.00003221
Iteration 157/1000 | Loss: 0.00003220
Iteration 158/1000 | Loss: 0.00003220
Iteration 159/1000 | Loss: 0.00003220
Iteration 160/1000 | Loss: 0.00003220
Iteration 161/1000 | Loss: 0.00003220
Iteration 162/1000 | Loss: 0.00003220
Iteration 163/1000 | Loss: 0.00003220
Iteration 164/1000 | Loss: 0.00003220
Iteration 165/1000 | Loss: 0.00003220
Iteration 166/1000 | Loss: 0.00003220
Iteration 167/1000 | Loss: 0.00003220
Iteration 168/1000 | Loss: 0.00003220
Iteration 169/1000 | Loss: 0.00003220
Iteration 170/1000 | Loss: 0.00003220
Iteration 171/1000 | Loss: 0.00003220
Iteration 172/1000 | Loss: 0.00003220
Iteration 173/1000 | Loss: 0.00003220
Iteration 174/1000 | Loss: 0.00003220
Iteration 175/1000 | Loss: 0.00003220
Iteration 176/1000 | Loss: 0.00003220
Iteration 177/1000 | Loss: 0.00003220
Iteration 178/1000 | Loss: 0.00003220
Iteration 179/1000 | Loss: 0.00003220
Iteration 180/1000 | Loss: 0.00003220
Iteration 181/1000 | Loss: 0.00003220
Iteration 182/1000 | Loss: 0.00003220
Iteration 183/1000 | Loss: 0.00003220
Iteration 184/1000 | Loss: 0.00003220
Iteration 185/1000 | Loss: 0.00003220
Iteration 186/1000 | Loss: 0.00003220
Iteration 187/1000 | Loss: 0.00003220
Iteration 188/1000 | Loss: 0.00003220
Iteration 189/1000 | Loss: 0.00003220
Iteration 190/1000 | Loss: 0.00003220
Iteration 191/1000 | Loss: 0.00003220
Iteration 192/1000 | Loss: 0.00003220
Iteration 193/1000 | Loss: 0.00003220
Iteration 194/1000 | Loss: 0.00003220
Iteration 195/1000 | Loss: 0.00003220
Iteration 196/1000 | Loss: 0.00003220
Iteration 197/1000 | Loss: 0.00003220
Iteration 198/1000 | Loss: 0.00003220
Iteration 199/1000 | Loss: 0.00003220
Iteration 200/1000 | Loss: 0.00003220
Iteration 201/1000 | Loss: 0.00003220
Iteration 202/1000 | Loss: 0.00003220
Iteration 203/1000 | Loss: 0.00003220
Iteration 204/1000 | Loss: 0.00003220
Iteration 205/1000 | Loss: 0.00003220
Iteration 206/1000 | Loss: 0.00003220
Iteration 207/1000 | Loss: 0.00003220
Iteration 208/1000 | Loss: 0.00003220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [3.2198044209508225e-05, 3.2198044209508225e-05, 3.2198044209508225e-05, 3.2198044209508225e-05, 3.2198044209508225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2198044209508225e-05

Optimization complete. Final v2v error: 4.004719257354736 mm

Highest mean error: 10.81751537322998 mm for frame 31

Lowest mean error: 3.5563175678253174 mm for frame 47

Saving results

Total time: 195.90558290481567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012207
Iteration 2/25 | Loss: 0.00245388
Iteration 3/25 | Loss: 0.00192323
Iteration 4/25 | Loss: 0.00184344
Iteration 5/25 | Loss: 0.00166817
Iteration 6/25 | Loss: 0.00151176
Iteration 7/25 | Loss: 0.00132810
Iteration 8/25 | Loss: 0.00127747
Iteration 9/25 | Loss: 0.00126317
Iteration 10/25 | Loss: 0.00127911
Iteration 11/25 | Loss: 0.00125530
Iteration 12/25 | Loss: 0.00127868
Iteration 13/25 | Loss: 0.00125537
Iteration 14/25 | Loss: 0.00125008
Iteration 15/25 | Loss: 0.00125950
Iteration 16/25 | Loss: 0.00125600
Iteration 17/25 | Loss: 0.00125505
Iteration 18/25 | Loss: 0.00125476
Iteration 19/25 | Loss: 0.00125759
Iteration 20/25 | Loss: 0.00124632
Iteration 21/25 | Loss: 0.00124009
Iteration 22/25 | Loss: 0.00123990
Iteration 23/25 | Loss: 0.00123340
Iteration 24/25 | Loss: 0.00123255
Iteration 25/25 | Loss: 0.00123223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30449271
Iteration 2/25 | Loss: 0.00098304
Iteration 3/25 | Loss: 0.00098304
Iteration 4/25 | Loss: 0.00098304
Iteration 5/25 | Loss: 0.00098304
Iteration 6/25 | Loss: 0.00098304
Iteration 7/25 | Loss: 0.00098304
Iteration 8/25 | Loss: 0.00098304
Iteration 9/25 | Loss: 0.00098304
Iteration 10/25 | Loss: 0.00098304
Iteration 11/25 | Loss: 0.00098304
Iteration 12/25 | Loss: 0.00098304
Iteration 13/25 | Loss: 0.00098304
Iteration 14/25 | Loss: 0.00098304
Iteration 15/25 | Loss: 0.00098304
Iteration 16/25 | Loss: 0.00098304
Iteration 17/25 | Loss: 0.00098304
Iteration 18/25 | Loss: 0.00098304
Iteration 19/25 | Loss: 0.00098304
Iteration 20/25 | Loss: 0.00098304
Iteration 21/25 | Loss: 0.00098304
Iteration 22/25 | Loss: 0.00098304
Iteration 23/25 | Loss: 0.00098304
Iteration 24/25 | Loss: 0.00098304
Iteration 25/25 | Loss: 0.00098304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098304
Iteration 2/1000 | Loss: 0.00041446
Iteration 3/1000 | Loss: 0.00027426
Iteration 4/1000 | Loss: 0.00019174
Iteration 5/1000 | Loss: 0.00019914
Iteration 6/1000 | Loss: 0.00037479
Iteration 7/1000 | Loss: 0.00031490
Iteration 8/1000 | Loss: 0.00039093
Iteration 9/1000 | Loss: 0.00030841
Iteration 10/1000 | Loss: 0.00018027
Iteration 11/1000 | Loss: 0.00022204
Iteration 12/1000 | Loss: 0.00005118
Iteration 13/1000 | Loss: 0.00031022
Iteration 14/1000 | Loss: 0.00032332
Iteration 15/1000 | Loss: 0.00018824
Iteration 16/1000 | Loss: 0.00026013
Iteration 17/1000 | Loss: 0.00020492
Iteration 18/1000 | Loss: 0.00010565
Iteration 19/1000 | Loss: 0.00012031
Iteration 20/1000 | Loss: 0.00006888
Iteration 21/1000 | Loss: 0.00012606
Iteration 22/1000 | Loss: 0.00009906
Iteration 23/1000 | Loss: 0.00012024
Iteration 24/1000 | Loss: 0.00013872
Iteration 25/1000 | Loss: 0.00009904
Iteration 26/1000 | Loss: 0.00008229
Iteration 27/1000 | Loss: 0.00012446
Iteration 28/1000 | Loss: 0.00028572
Iteration 29/1000 | Loss: 0.00020749
Iteration 30/1000 | Loss: 0.00030576
Iteration 31/1000 | Loss: 0.00013295
Iteration 32/1000 | Loss: 0.00010033
Iteration 33/1000 | Loss: 0.00008893
Iteration 34/1000 | Loss: 0.00007032
Iteration 35/1000 | Loss: 0.00003754
Iteration 36/1000 | Loss: 0.00002689
Iteration 37/1000 | Loss: 0.00002486
Iteration 38/1000 | Loss: 0.00002372
Iteration 39/1000 | Loss: 0.00002246
Iteration 40/1000 | Loss: 0.00002174
Iteration 41/1000 | Loss: 0.00002106
Iteration 42/1000 | Loss: 0.00002059
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002020
Iteration 46/1000 | Loss: 0.00002003
Iteration 47/1000 | Loss: 0.00002001
Iteration 48/1000 | Loss: 0.00002001
Iteration 49/1000 | Loss: 0.00001998
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001995
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001993
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001992
Iteration 60/1000 | Loss: 0.00001992
Iteration 61/1000 | Loss: 0.00001992
Iteration 62/1000 | Loss: 0.00001991
Iteration 63/1000 | Loss: 0.00001991
Iteration 64/1000 | Loss: 0.00001991
Iteration 65/1000 | Loss: 0.00001991
Iteration 66/1000 | Loss: 0.00001991
Iteration 67/1000 | Loss: 0.00001990
Iteration 68/1000 | Loss: 0.00001990
Iteration 69/1000 | Loss: 0.00001990
Iteration 70/1000 | Loss: 0.00001990
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001990
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001988
Iteration 76/1000 | Loss: 0.00001988
Iteration 77/1000 | Loss: 0.00001988
Iteration 78/1000 | Loss: 0.00001988
Iteration 79/1000 | Loss: 0.00001988
Iteration 80/1000 | Loss: 0.00001988
Iteration 81/1000 | Loss: 0.00001987
Iteration 82/1000 | Loss: 0.00001985
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001984
Iteration 86/1000 | Loss: 0.00001984
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001981
Iteration 90/1000 | Loss: 0.00001981
Iteration 91/1000 | Loss: 0.00001980
Iteration 92/1000 | Loss: 0.00001980
Iteration 93/1000 | Loss: 0.00001980
Iteration 94/1000 | Loss: 0.00001980
Iteration 95/1000 | Loss: 0.00001980
Iteration 96/1000 | Loss: 0.00001979
Iteration 97/1000 | Loss: 0.00001979
Iteration 98/1000 | Loss: 0.00001979
Iteration 99/1000 | Loss: 0.00001978
Iteration 100/1000 | Loss: 0.00001978
Iteration 101/1000 | Loss: 0.00001978
Iteration 102/1000 | Loss: 0.00001977
Iteration 103/1000 | Loss: 0.00001977
Iteration 104/1000 | Loss: 0.00001977
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001976
Iteration 108/1000 | Loss: 0.00001976
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001975
Iteration 115/1000 | Loss: 0.00001975
Iteration 116/1000 | Loss: 0.00001975
Iteration 117/1000 | Loss: 0.00001975
Iteration 118/1000 | Loss: 0.00001975
Iteration 119/1000 | Loss: 0.00001975
Iteration 120/1000 | Loss: 0.00001975
Iteration 121/1000 | Loss: 0.00001975
Iteration 122/1000 | Loss: 0.00001975
Iteration 123/1000 | Loss: 0.00001975
Iteration 124/1000 | Loss: 0.00001975
Iteration 125/1000 | Loss: 0.00001975
Iteration 126/1000 | Loss: 0.00001975
Iteration 127/1000 | Loss: 0.00001975
Iteration 128/1000 | Loss: 0.00001975
Iteration 129/1000 | Loss: 0.00001975
Iteration 130/1000 | Loss: 0.00001975
Iteration 131/1000 | Loss: 0.00001975
Iteration 132/1000 | Loss: 0.00001974
Iteration 133/1000 | Loss: 0.00001974
Iteration 134/1000 | Loss: 0.00001974
Iteration 135/1000 | Loss: 0.00001974
Iteration 136/1000 | Loss: 0.00001974
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001974
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Iteration 141/1000 | Loss: 0.00001974
Iteration 142/1000 | Loss: 0.00001974
Iteration 143/1000 | Loss: 0.00001974
Iteration 144/1000 | Loss: 0.00001974
Iteration 145/1000 | Loss: 0.00001974
Iteration 146/1000 | Loss: 0.00001974
Iteration 147/1000 | Loss: 0.00001974
Iteration 148/1000 | Loss: 0.00001974
Iteration 149/1000 | Loss: 0.00001974
Iteration 150/1000 | Loss: 0.00001974
Iteration 151/1000 | Loss: 0.00001974
Iteration 152/1000 | Loss: 0.00001974
Iteration 153/1000 | Loss: 0.00001974
Iteration 154/1000 | Loss: 0.00001974
Iteration 155/1000 | Loss: 0.00001974
Iteration 156/1000 | Loss: 0.00001974
Iteration 157/1000 | Loss: 0.00001974
Iteration 158/1000 | Loss: 0.00001974
Iteration 159/1000 | Loss: 0.00001974
Iteration 160/1000 | Loss: 0.00001974
Iteration 161/1000 | Loss: 0.00001974
Iteration 162/1000 | Loss: 0.00001974
Iteration 163/1000 | Loss: 0.00001974
Iteration 164/1000 | Loss: 0.00001974
Iteration 165/1000 | Loss: 0.00001974
Iteration 166/1000 | Loss: 0.00001974
Iteration 167/1000 | Loss: 0.00001974
Iteration 168/1000 | Loss: 0.00001974
Iteration 169/1000 | Loss: 0.00001974
Iteration 170/1000 | Loss: 0.00001974
Iteration 171/1000 | Loss: 0.00001974
Iteration 172/1000 | Loss: 0.00001974
Iteration 173/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.973849066416733e-05, 1.973849066416733e-05, 1.973849066416733e-05, 1.973849066416733e-05, 1.973849066416733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.973849066416733e-05

Optimization complete. Final v2v error: 3.6384074687957764 mm

Highest mean error: 11.726866722106934 mm for frame 7

Lowest mean error: 3.476980209350586 mm for frame 12

Saving results

Total time: 131.08875823020935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882661
Iteration 2/25 | Loss: 0.00155431
Iteration 3/25 | Loss: 0.00129620
Iteration 4/25 | Loss: 0.00125321
Iteration 5/25 | Loss: 0.00124813
Iteration 6/25 | Loss: 0.00125091
Iteration 7/25 | Loss: 0.00122843
Iteration 8/25 | Loss: 0.00120744
Iteration 9/25 | Loss: 0.00119456
Iteration 10/25 | Loss: 0.00119026
Iteration 11/25 | Loss: 0.00118546
Iteration 12/25 | Loss: 0.00118739
Iteration 13/25 | Loss: 0.00118700
Iteration 14/25 | Loss: 0.00120458
Iteration 15/25 | Loss: 0.00119949
Iteration 16/25 | Loss: 0.00119186
Iteration 17/25 | Loss: 0.00117733
Iteration 18/25 | Loss: 0.00117160
Iteration 19/25 | Loss: 0.00117024
Iteration 20/25 | Loss: 0.00116990
Iteration 21/25 | Loss: 0.00116980
Iteration 22/25 | Loss: 0.00116980
Iteration 23/25 | Loss: 0.00116979
Iteration 24/25 | Loss: 0.00116979
Iteration 25/25 | Loss: 0.00116979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24834585
Iteration 2/25 | Loss: 0.00104168
Iteration 3/25 | Loss: 0.00104167
Iteration 4/25 | Loss: 0.00104167
Iteration 5/25 | Loss: 0.00104167
Iteration 6/25 | Loss: 0.00104167
Iteration 7/25 | Loss: 0.00104167
Iteration 8/25 | Loss: 0.00104167
Iteration 9/25 | Loss: 0.00104167
Iteration 10/25 | Loss: 0.00104167
Iteration 11/25 | Loss: 0.00104167
Iteration 12/25 | Loss: 0.00104167
Iteration 13/25 | Loss: 0.00104167
Iteration 14/25 | Loss: 0.00104167
Iteration 15/25 | Loss: 0.00104167
Iteration 16/25 | Loss: 0.00104167
Iteration 17/25 | Loss: 0.00104167
Iteration 18/25 | Loss: 0.00104167
Iteration 19/25 | Loss: 0.00104167
Iteration 20/25 | Loss: 0.00104167
Iteration 21/25 | Loss: 0.00104167
Iteration 22/25 | Loss: 0.00104167
Iteration 23/25 | Loss: 0.00104167
Iteration 24/25 | Loss: 0.00104167
Iteration 25/25 | Loss: 0.00104167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001041665324009955, 0.001041665324009955, 0.001041665324009955, 0.001041665324009955, 0.001041665324009955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001041665324009955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104167
Iteration 2/1000 | Loss: 0.00010581
Iteration 3/1000 | Loss: 0.00017478
Iteration 4/1000 | Loss: 0.00003686
Iteration 5/1000 | Loss: 0.00002818
Iteration 6/1000 | Loss: 0.00002434
Iteration 7/1000 | Loss: 0.00002329
Iteration 8/1000 | Loss: 0.00002254
Iteration 9/1000 | Loss: 0.00002170
Iteration 10/1000 | Loss: 0.00002114
Iteration 11/1000 | Loss: 0.00002080
Iteration 12/1000 | Loss: 0.00002057
Iteration 13/1000 | Loss: 0.00002031
Iteration 14/1000 | Loss: 0.00002015
Iteration 15/1000 | Loss: 0.00002013
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001988
Iteration 18/1000 | Loss: 0.00001985
Iteration 19/1000 | Loss: 0.00001972
Iteration 20/1000 | Loss: 0.00001969
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001966
Iteration 23/1000 | Loss: 0.00001966
Iteration 24/1000 | Loss: 0.00001966
Iteration 25/1000 | Loss: 0.00001965
Iteration 26/1000 | Loss: 0.00001965
Iteration 27/1000 | Loss: 0.00001965
Iteration 28/1000 | Loss: 0.00001964
Iteration 29/1000 | Loss: 0.00001964
Iteration 30/1000 | Loss: 0.00001964
Iteration 31/1000 | Loss: 0.00001963
Iteration 32/1000 | Loss: 0.00001963
Iteration 33/1000 | Loss: 0.00001963
Iteration 34/1000 | Loss: 0.00001963
Iteration 35/1000 | Loss: 0.00001963
Iteration 36/1000 | Loss: 0.00001962
Iteration 37/1000 | Loss: 0.00001962
Iteration 38/1000 | Loss: 0.00001961
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001959
Iteration 42/1000 | Loss: 0.00001959
Iteration 43/1000 | Loss: 0.00001959
Iteration 44/1000 | Loss: 0.00001959
Iteration 45/1000 | Loss: 0.00001958
Iteration 46/1000 | Loss: 0.00001958
Iteration 47/1000 | Loss: 0.00001958
Iteration 48/1000 | Loss: 0.00001958
Iteration 49/1000 | Loss: 0.00001958
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001958
Iteration 53/1000 | Loss: 0.00001957
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001955
Iteration 57/1000 | Loss: 0.00001955
Iteration 58/1000 | Loss: 0.00001954
Iteration 59/1000 | Loss: 0.00001954
Iteration 60/1000 | Loss: 0.00001953
Iteration 61/1000 | Loss: 0.00001953
Iteration 62/1000 | Loss: 0.00001953
Iteration 63/1000 | Loss: 0.00001952
Iteration 64/1000 | Loss: 0.00001951
Iteration 65/1000 | Loss: 0.00001951
Iteration 66/1000 | Loss: 0.00001951
Iteration 67/1000 | Loss: 0.00001951
Iteration 68/1000 | Loss: 0.00001951
Iteration 69/1000 | Loss: 0.00001951
Iteration 70/1000 | Loss: 0.00001951
Iteration 71/1000 | Loss: 0.00001950
Iteration 72/1000 | Loss: 0.00001950
Iteration 73/1000 | Loss: 0.00001950
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001949
Iteration 76/1000 | Loss: 0.00001949
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001949
Iteration 87/1000 | Loss: 0.00001949
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001948
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001947
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001946
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001946
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001946
Iteration 103/1000 | Loss: 0.00001946
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001944
Iteration 107/1000 | Loss: 0.00001944
Iteration 108/1000 | Loss: 0.00001944
Iteration 109/1000 | Loss: 0.00001944
Iteration 110/1000 | Loss: 0.00001943
Iteration 111/1000 | Loss: 0.00001943
Iteration 112/1000 | Loss: 0.00001943
Iteration 113/1000 | Loss: 0.00001943
Iteration 114/1000 | Loss: 0.00001943
Iteration 115/1000 | Loss: 0.00001943
Iteration 116/1000 | Loss: 0.00001943
Iteration 117/1000 | Loss: 0.00001942
Iteration 118/1000 | Loss: 0.00001942
Iteration 119/1000 | Loss: 0.00001942
Iteration 120/1000 | Loss: 0.00001942
Iteration 121/1000 | Loss: 0.00001942
Iteration 122/1000 | Loss: 0.00001942
Iteration 123/1000 | Loss: 0.00001941
Iteration 124/1000 | Loss: 0.00001941
Iteration 125/1000 | Loss: 0.00001941
Iteration 126/1000 | Loss: 0.00001941
Iteration 127/1000 | Loss: 0.00001941
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001940
Iteration 130/1000 | Loss: 0.00001940
Iteration 131/1000 | Loss: 0.00001940
Iteration 132/1000 | Loss: 0.00001940
Iteration 133/1000 | Loss: 0.00001939
Iteration 134/1000 | Loss: 0.00001939
Iteration 135/1000 | Loss: 0.00001939
Iteration 136/1000 | Loss: 0.00001939
Iteration 137/1000 | Loss: 0.00001939
Iteration 138/1000 | Loss: 0.00001938
Iteration 139/1000 | Loss: 0.00001938
Iteration 140/1000 | Loss: 0.00001938
Iteration 141/1000 | Loss: 0.00001938
Iteration 142/1000 | Loss: 0.00001938
Iteration 143/1000 | Loss: 0.00001938
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00001938
Iteration 146/1000 | Loss: 0.00001938
Iteration 147/1000 | Loss: 0.00001938
Iteration 148/1000 | Loss: 0.00001938
Iteration 149/1000 | Loss: 0.00001937
Iteration 150/1000 | Loss: 0.00001937
Iteration 151/1000 | Loss: 0.00001937
Iteration 152/1000 | Loss: 0.00001937
Iteration 153/1000 | Loss: 0.00001937
Iteration 154/1000 | Loss: 0.00001937
Iteration 155/1000 | Loss: 0.00001937
Iteration 156/1000 | Loss: 0.00001936
Iteration 157/1000 | Loss: 0.00001936
Iteration 158/1000 | Loss: 0.00001936
Iteration 159/1000 | Loss: 0.00001936
Iteration 160/1000 | Loss: 0.00001936
Iteration 161/1000 | Loss: 0.00001936
Iteration 162/1000 | Loss: 0.00001936
Iteration 163/1000 | Loss: 0.00001936
Iteration 164/1000 | Loss: 0.00001936
Iteration 165/1000 | Loss: 0.00001935
Iteration 166/1000 | Loss: 0.00001935
Iteration 167/1000 | Loss: 0.00001935
Iteration 168/1000 | Loss: 0.00001935
Iteration 169/1000 | Loss: 0.00001935
Iteration 170/1000 | Loss: 0.00001934
Iteration 171/1000 | Loss: 0.00001934
Iteration 172/1000 | Loss: 0.00001934
Iteration 173/1000 | Loss: 0.00001934
Iteration 174/1000 | Loss: 0.00001933
Iteration 175/1000 | Loss: 0.00001933
Iteration 176/1000 | Loss: 0.00001933
Iteration 177/1000 | Loss: 0.00001932
Iteration 178/1000 | Loss: 0.00001932
Iteration 179/1000 | Loss: 0.00001932
Iteration 180/1000 | Loss: 0.00001932
Iteration 181/1000 | Loss: 0.00001931
Iteration 182/1000 | Loss: 0.00001931
Iteration 183/1000 | Loss: 0.00001931
Iteration 184/1000 | Loss: 0.00001931
Iteration 185/1000 | Loss: 0.00001930
Iteration 186/1000 | Loss: 0.00001930
Iteration 187/1000 | Loss: 0.00001930
Iteration 188/1000 | Loss: 0.00001930
Iteration 189/1000 | Loss: 0.00001930
Iteration 190/1000 | Loss: 0.00001930
Iteration 191/1000 | Loss: 0.00001930
Iteration 192/1000 | Loss: 0.00001929
Iteration 193/1000 | Loss: 0.00001929
Iteration 194/1000 | Loss: 0.00001929
Iteration 195/1000 | Loss: 0.00001929
Iteration 196/1000 | Loss: 0.00001929
Iteration 197/1000 | Loss: 0.00001929
Iteration 198/1000 | Loss: 0.00001929
Iteration 199/1000 | Loss: 0.00001929
Iteration 200/1000 | Loss: 0.00001929
Iteration 201/1000 | Loss: 0.00001929
Iteration 202/1000 | Loss: 0.00001929
Iteration 203/1000 | Loss: 0.00001929
Iteration 204/1000 | Loss: 0.00001929
Iteration 205/1000 | Loss: 0.00001929
Iteration 206/1000 | Loss: 0.00001929
Iteration 207/1000 | Loss: 0.00001929
Iteration 208/1000 | Loss: 0.00001928
Iteration 209/1000 | Loss: 0.00001928
Iteration 210/1000 | Loss: 0.00001928
Iteration 211/1000 | Loss: 0.00001928
Iteration 212/1000 | Loss: 0.00001928
Iteration 213/1000 | Loss: 0.00001928
Iteration 214/1000 | Loss: 0.00001928
Iteration 215/1000 | Loss: 0.00001928
Iteration 216/1000 | Loss: 0.00001928
Iteration 217/1000 | Loss: 0.00001928
Iteration 218/1000 | Loss: 0.00001928
Iteration 219/1000 | Loss: 0.00001928
Iteration 220/1000 | Loss: 0.00001928
Iteration 221/1000 | Loss: 0.00001928
Iteration 222/1000 | Loss: 0.00001927
Iteration 223/1000 | Loss: 0.00001927
Iteration 224/1000 | Loss: 0.00001927
Iteration 225/1000 | Loss: 0.00001927
Iteration 226/1000 | Loss: 0.00001927
Iteration 227/1000 | Loss: 0.00001927
Iteration 228/1000 | Loss: 0.00001927
Iteration 229/1000 | Loss: 0.00001927
Iteration 230/1000 | Loss: 0.00001927
Iteration 231/1000 | Loss: 0.00001926
Iteration 232/1000 | Loss: 0.00001926
Iteration 233/1000 | Loss: 0.00001926
Iteration 234/1000 | Loss: 0.00001926
Iteration 235/1000 | Loss: 0.00001926
Iteration 236/1000 | Loss: 0.00001926
Iteration 237/1000 | Loss: 0.00001926
Iteration 238/1000 | Loss: 0.00001926
Iteration 239/1000 | Loss: 0.00001926
Iteration 240/1000 | Loss: 0.00001926
Iteration 241/1000 | Loss: 0.00001926
Iteration 242/1000 | Loss: 0.00001926
Iteration 243/1000 | Loss: 0.00001926
Iteration 244/1000 | Loss: 0.00001926
Iteration 245/1000 | Loss: 0.00001926
Iteration 246/1000 | Loss: 0.00001926
Iteration 247/1000 | Loss: 0.00001925
Iteration 248/1000 | Loss: 0.00001925
Iteration 249/1000 | Loss: 0.00001925
Iteration 250/1000 | Loss: 0.00001925
Iteration 251/1000 | Loss: 0.00001925
Iteration 252/1000 | Loss: 0.00001925
Iteration 253/1000 | Loss: 0.00001925
Iteration 254/1000 | Loss: 0.00001925
Iteration 255/1000 | Loss: 0.00001925
Iteration 256/1000 | Loss: 0.00001925
Iteration 257/1000 | Loss: 0.00001925
Iteration 258/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.9253739083069377e-05, 1.9253739083069377e-05, 1.9253739083069377e-05, 1.9253739083069377e-05, 1.9253739083069377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9253739083069377e-05

Optimization complete. Final v2v error: 3.649670124053955 mm

Highest mean error: 5.647907733917236 mm for frame 106

Lowest mean error: 3.0941965579986572 mm for frame 70

Saving results

Total time: 79.85150933265686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971121
Iteration 2/25 | Loss: 0.00157626
Iteration 3/25 | Loss: 0.00125536
Iteration 4/25 | Loss: 0.00117622
Iteration 5/25 | Loss: 0.00118060
Iteration 6/25 | Loss: 0.00117440
Iteration 7/25 | Loss: 0.00117103
Iteration 8/25 | Loss: 0.00115629
Iteration 9/25 | Loss: 0.00115387
Iteration 10/25 | Loss: 0.00115330
Iteration 11/25 | Loss: 0.00115299
Iteration 12/25 | Loss: 0.00115599
Iteration 13/25 | Loss: 0.00115494
Iteration 14/25 | Loss: 0.00115456
Iteration 15/25 | Loss: 0.00115306
Iteration 16/25 | Loss: 0.00115294
Iteration 17/25 | Loss: 0.00115283
Iteration 18/25 | Loss: 0.00115908
Iteration 19/25 | Loss: 0.00115401
Iteration 20/25 | Loss: 0.00115008
Iteration 21/25 | Loss: 0.00114882
Iteration 22/25 | Loss: 0.00114820
Iteration 23/25 | Loss: 0.00114816
Iteration 24/25 | Loss: 0.00114816
Iteration 25/25 | Loss: 0.00114816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44787192
Iteration 2/25 | Loss: 0.00118064
Iteration 3/25 | Loss: 0.00118063
Iteration 4/25 | Loss: 0.00118062
Iteration 5/25 | Loss: 0.00118062
Iteration 6/25 | Loss: 0.00118062
Iteration 7/25 | Loss: 0.00118062
Iteration 8/25 | Loss: 0.00118062
Iteration 9/25 | Loss: 0.00118062
Iteration 10/25 | Loss: 0.00118062
Iteration 11/25 | Loss: 0.00118062
Iteration 12/25 | Loss: 0.00118062
Iteration 13/25 | Loss: 0.00118062
Iteration 14/25 | Loss: 0.00118062
Iteration 15/25 | Loss: 0.00118062
Iteration 16/25 | Loss: 0.00118062
Iteration 17/25 | Loss: 0.00118062
Iteration 18/25 | Loss: 0.00118062
Iteration 19/25 | Loss: 0.00118062
Iteration 20/25 | Loss: 0.00118062
Iteration 21/25 | Loss: 0.00118062
Iteration 22/25 | Loss: 0.00118062
Iteration 23/25 | Loss: 0.00118062
Iteration 24/25 | Loss: 0.00118062
Iteration 25/25 | Loss: 0.00118062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118062
Iteration 2/1000 | Loss: 0.00002650
Iteration 3/1000 | Loss: 0.00001760
Iteration 4/1000 | Loss: 0.00001576
Iteration 5/1000 | Loss: 0.00001497
Iteration 6/1000 | Loss: 0.00001438
Iteration 7/1000 | Loss: 0.00001404
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001311
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001305
Iteration 16/1000 | Loss: 0.00001304
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001298
Iteration 23/1000 | Loss: 0.00001298
Iteration 24/1000 | Loss: 0.00001297
Iteration 25/1000 | Loss: 0.00001295
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001294
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001293
Iteration 34/1000 | Loss: 0.00001293
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001291
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001290
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001288
Iteration 58/1000 | Loss: 0.00001288
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001286
Iteration 62/1000 | Loss: 0.00001286
Iteration 63/1000 | Loss: 0.00001286
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001286
Iteration 68/1000 | Loss: 0.00001286
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.2851494830101728e-05, 1.2851494830101728e-05, 1.2851494830101728e-05, 1.2851494830101728e-05, 1.2851494830101728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2851494830101728e-05

Optimization complete. Final v2v error: 3.057126760482788 mm

Highest mean error: 3.6468892097473145 mm for frame 179

Lowest mean error: 2.7637972831726074 mm for frame 95

Saving results

Total time: 68.11032700538635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447291
Iteration 2/25 | Loss: 0.00134940
Iteration 3/25 | Loss: 0.00117033
Iteration 4/25 | Loss: 0.00113071
Iteration 5/25 | Loss: 0.00112279
Iteration 6/25 | Loss: 0.00112389
Iteration 7/25 | Loss: 0.00112590
Iteration 8/25 | Loss: 0.00112369
Iteration 9/25 | Loss: 0.00111615
Iteration 10/25 | Loss: 0.00111456
Iteration 11/25 | Loss: 0.00111360
Iteration 12/25 | Loss: 0.00111340
Iteration 13/25 | Loss: 0.00111334
Iteration 14/25 | Loss: 0.00111333
Iteration 15/25 | Loss: 0.00111333
Iteration 16/25 | Loss: 0.00111331
Iteration 17/25 | Loss: 0.00111331
Iteration 18/25 | Loss: 0.00111331
Iteration 19/25 | Loss: 0.00111330
Iteration 20/25 | Loss: 0.00111330
Iteration 21/25 | Loss: 0.00111330
Iteration 22/25 | Loss: 0.00111330
Iteration 23/25 | Loss: 0.00111330
Iteration 24/25 | Loss: 0.00111330
Iteration 25/25 | Loss: 0.00111330

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34066236
Iteration 2/25 | Loss: 0.00106555
Iteration 3/25 | Loss: 0.00106554
Iteration 4/25 | Loss: 0.00106554
Iteration 5/25 | Loss: 0.00106554
Iteration 6/25 | Loss: 0.00106554
Iteration 7/25 | Loss: 0.00106554
Iteration 8/25 | Loss: 0.00106554
Iteration 9/25 | Loss: 0.00106554
Iteration 10/25 | Loss: 0.00106554
Iteration 11/25 | Loss: 0.00106554
Iteration 12/25 | Loss: 0.00106554
Iteration 13/25 | Loss: 0.00106554
Iteration 14/25 | Loss: 0.00106554
Iteration 15/25 | Loss: 0.00106554
Iteration 16/25 | Loss: 0.00106554
Iteration 17/25 | Loss: 0.00106554
Iteration 18/25 | Loss: 0.00106554
Iteration 19/25 | Loss: 0.00106554
Iteration 20/25 | Loss: 0.00106554
Iteration 21/25 | Loss: 0.00106554
Iteration 22/25 | Loss: 0.00106554
Iteration 23/25 | Loss: 0.00106554
Iteration 24/25 | Loss: 0.00106554
Iteration 25/25 | Loss: 0.00106554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106554
Iteration 2/1000 | Loss: 0.00003726
Iteration 3/1000 | Loss: 0.00002375
Iteration 4/1000 | Loss: 0.00001953
Iteration 5/1000 | Loss: 0.00001808
Iteration 6/1000 | Loss: 0.00001703
Iteration 7/1000 | Loss: 0.00001616
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001531
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001476
Iteration 12/1000 | Loss: 0.00001463
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001436
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001429
Iteration 20/1000 | Loss: 0.00001427
Iteration 21/1000 | Loss: 0.00001426
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001416
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001412
Iteration 28/1000 | Loss: 0.00001412
Iteration 29/1000 | Loss: 0.00001411
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001409
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001408
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001408
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001407
Iteration 44/1000 | Loss: 0.00001407
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001405
Iteration 47/1000 | Loss: 0.00001405
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001405
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001403
Iteration 55/1000 | Loss: 0.00001400
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001400
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001396
Iteration 73/1000 | Loss: 0.00001396
Iteration 74/1000 | Loss: 0.00001396
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001395
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001393
Iteration 86/1000 | Loss: 0.00001393
Iteration 87/1000 | Loss: 0.00001393
Iteration 88/1000 | Loss: 0.00001393
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001392
Iteration 91/1000 | Loss: 0.00001392
Iteration 92/1000 | Loss: 0.00001392
Iteration 93/1000 | Loss: 0.00001392
Iteration 94/1000 | Loss: 0.00001392
Iteration 95/1000 | Loss: 0.00001392
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001391
Iteration 103/1000 | Loss: 0.00001391
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001390
Iteration 108/1000 | Loss: 0.00001390
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001390
Iteration 113/1000 | Loss: 0.00001390
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00001387
Iteration 130/1000 | Loss: 0.00001387
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001386
Iteration 134/1000 | Loss: 0.00001386
Iteration 135/1000 | Loss: 0.00001386
Iteration 136/1000 | Loss: 0.00001386
Iteration 137/1000 | Loss: 0.00001386
Iteration 138/1000 | Loss: 0.00001386
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001385
Iteration 147/1000 | Loss: 0.00001385
Iteration 148/1000 | Loss: 0.00001385
Iteration 149/1000 | Loss: 0.00001385
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001384
Iteration 152/1000 | Loss: 0.00001384
Iteration 153/1000 | Loss: 0.00001383
Iteration 154/1000 | Loss: 0.00001383
Iteration 155/1000 | Loss: 0.00001383
Iteration 156/1000 | Loss: 0.00001383
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001382
Iteration 161/1000 | Loss: 0.00001382
Iteration 162/1000 | Loss: 0.00001382
Iteration 163/1000 | Loss: 0.00001382
Iteration 164/1000 | Loss: 0.00001382
Iteration 165/1000 | Loss: 0.00001382
Iteration 166/1000 | Loss: 0.00001381
Iteration 167/1000 | Loss: 0.00001381
Iteration 168/1000 | Loss: 0.00001381
Iteration 169/1000 | Loss: 0.00001381
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001381
Iteration 172/1000 | Loss: 0.00001381
Iteration 173/1000 | Loss: 0.00001380
Iteration 174/1000 | Loss: 0.00001380
Iteration 175/1000 | Loss: 0.00001380
Iteration 176/1000 | Loss: 0.00001380
Iteration 177/1000 | Loss: 0.00001379
Iteration 178/1000 | Loss: 0.00001379
Iteration 179/1000 | Loss: 0.00001379
Iteration 180/1000 | Loss: 0.00001379
Iteration 181/1000 | Loss: 0.00001379
Iteration 182/1000 | Loss: 0.00001379
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001376
Iteration 198/1000 | Loss: 0.00001376
Iteration 199/1000 | Loss: 0.00001376
Iteration 200/1000 | Loss: 0.00001376
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001376
Iteration 207/1000 | Loss: 0.00001376
Iteration 208/1000 | Loss: 0.00001376
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Iteration 213/1000 | Loss: 0.00001375
Iteration 214/1000 | Loss: 0.00001375
Iteration 215/1000 | Loss: 0.00001374
Iteration 216/1000 | Loss: 0.00001374
Iteration 217/1000 | Loss: 0.00001374
Iteration 218/1000 | Loss: 0.00001374
Iteration 219/1000 | Loss: 0.00001374
Iteration 220/1000 | Loss: 0.00001374
Iteration 221/1000 | Loss: 0.00001374
Iteration 222/1000 | Loss: 0.00001374
Iteration 223/1000 | Loss: 0.00001374
Iteration 224/1000 | Loss: 0.00001374
Iteration 225/1000 | Loss: 0.00001374
Iteration 226/1000 | Loss: 0.00001374
Iteration 227/1000 | Loss: 0.00001374
Iteration 228/1000 | Loss: 0.00001374
Iteration 229/1000 | Loss: 0.00001374
Iteration 230/1000 | Loss: 0.00001373
Iteration 231/1000 | Loss: 0.00001373
Iteration 232/1000 | Loss: 0.00001373
Iteration 233/1000 | Loss: 0.00001373
Iteration 234/1000 | Loss: 0.00001373
Iteration 235/1000 | Loss: 0.00001373
Iteration 236/1000 | Loss: 0.00001373
Iteration 237/1000 | Loss: 0.00001373
Iteration 238/1000 | Loss: 0.00001373
Iteration 239/1000 | Loss: 0.00001373
Iteration 240/1000 | Loss: 0.00001373
Iteration 241/1000 | Loss: 0.00001373
Iteration 242/1000 | Loss: 0.00001372
Iteration 243/1000 | Loss: 0.00001372
Iteration 244/1000 | Loss: 0.00001372
Iteration 245/1000 | Loss: 0.00001372
Iteration 246/1000 | Loss: 0.00001372
Iteration 247/1000 | Loss: 0.00001372
Iteration 248/1000 | Loss: 0.00001372
Iteration 249/1000 | Loss: 0.00001372
Iteration 250/1000 | Loss: 0.00001372
Iteration 251/1000 | Loss: 0.00001372
Iteration 252/1000 | Loss: 0.00001372
Iteration 253/1000 | Loss: 0.00001372
Iteration 254/1000 | Loss: 0.00001372
Iteration 255/1000 | Loss: 0.00001372
Iteration 256/1000 | Loss: 0.00001372
Iteration 257/1000 | Loss: 0.00001372
Iteration 258/1000 | Loss: 0.00001372
Iteration 259/1000 | Loss: 0.00001372
Iteration 260/1000 | Loss: 0.00001372
Iteration 261/1000 | Loss: 0.00001372
Iteration 262/1000 | Loss: 0.00001372
Iteration 263/1000 | Loss: 0.00001372
Iteration 264/1000 | Loss: 0.00001372
Iteration 265/1000 | Loss: 0.00001372
Iteration 266/1000 | Loss: 0.00001372
Iteration 267/1000 | Loss: 0.00001372
Iteration 268/1000 | Loss: 0.00001372
Iteration 269/1000 | Loss: 0.00001372
Iteration 270/1000 | Loss: 0.00001372
Iteration 271/1000 | Loss: 0.00001372
Iteration 272/1000 | Loss: 0.00001372
Iteration 273/1000 | Loss: 0.00001372
Iteration 274/1000 | Loss: 0.00001372
Iteration 275/1000 | Loss: 0.00001372
Iteration 276/1000 | Loss: 0.00001372
Iteration 277/1000 | Loss: 0.00001372
Iteration 278/1000 | Loss: 0.00001372
Iteration 279/1000 | Loss: 0.00001372
Iteration 280/1000 | Loss: 0.00001372
Iteration 281/1000 | Loss: 0.00001372
Iteration 282/1000 | Loss: 0.00001372
Iteration 283/1000 | Loss: 0.00001372
Iteration 284/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.3717423826165032e-05, 1.3717423826165032e-05, 1.3717423826165032e-05, 1.3717423826165032e-05, 1.3717423826165032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3717423826165032e-05

Optimization complete. Final v2v error: 3.081305742263794 mm

Highest mean error: 4.135990619659424 mm for frame 110

Lowest mean error: 2.5643703937530518 mm for frame 0

Saving results

Total time: 62.54855227470398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462375
Iteration 2/25 | Loss: 0.00141941
Iteration 3/25 | Loss: 0.00122354
Iteration 4/25 | Loss: 0.00120196
Iteration 5/25 | Loss: 0.00119743
Iteration 6/25 | Loss: 0.00119709
Iteration 7/25 | Loss: 0.00119709
Iteration 8/25 | Loss: 0.00119709
Iteration 9/25 | Loss: 0.00119709
Iteration 10/25 | Loss: 0.00119709
Iteration 11/25 | Loss: 0.00119709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011970855994150043, 0.0011970855994150043, 0.0011970855994150043, 0.0011970855994150043, 0.0011970855994150043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011970855994150043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33284259
Iteration 2/25 | Loss: 0.00093590
Iteration 3/25 | Loss: 0.00093590
Iteration 4/25 | Loss: 0.00093590
Iteration 5/25 | Loss: 0.00093590
Iteration 6/25 | Loss: 0.00093590
Iteration 7/25 | Loss: 0.00093590
Iteration 8/25 | Loss: 0.00093590
Iteration 9/25 | Loss: 0.00093590
Iteration 10/25 | Loss: 0.00093590
Iteration 11/25 | Loss: 0.00093590
Iteration 12/25 | Loss: 0.00093590
Iteration 13/25 | Loss: 0.00093590
Iteration 14/25 | Loss: 0.00093590
Iteration 15/25 | Loss: 0.00093590
Iteration 16/25 | Loss: 0.00093590
Iteration 17/25 | Loss: 0.00093590
Iteration 18/25 | Loss: 0.00093590
Iteration 19/25 | Loss: 0.00093590
Iteration 20/25 | Loss: 0.00093590
Iteration 21/25 | Loss: 0.00093590
Iteration 22/25 | Loss: 0.00093590
Iteration 23/25 | Loss: 0.00093590
Iteration 24/25 | Loss: 0.00093590
Iteration 25/25 | Loss: 0.00093590

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093590
Iteration 2/1000 | Loss: 0.00004795
Iteration 3/1000 | Loss: 0.00003441
Iteration 4/1000 | Loss: 0.00003181
Iteration 5/1000 | Loss: 0.00003025
Iteration 6/1000 | Loss: 0.00002918
Iteration 7/1000 | Loss: 0.00002813
Iteration 8/1000 | Loss: 0.00002746
Iteration 9/1000 | Loss: 0.00002704
Iteration 10/1000 | Loss: 0.00002671
Iteration 11/1000 | Loss: 0.00002654
Iteration 12/1000 | Loss: 0.00002639
Iteration 13/1000 | Loss: 0.00002624
Iteration 14/1000 | Loss: 0.00002608
Iteration 15/1000 | Loss: 0.00002604
Iteration 16/1000 | Loss: 0.00002600
Iteration 17/1000 | Loss: 0.00002599
Iteration 18/1000 | Loss: 0.00002597
Iteration 19/1000 | Loss: 0.00002588
Iteration 20/1000 | Loss: 0.00002584
Iteration 21/1000 | Loss: 0.00002582
Iteration 22/1000 | Loss: 0.00002576
Iteration 23/1000 | Loss: 0.00002566
Iteration 24/1000 | Loss: 0.00002563
Iteration 25/1000 | Loss: 0.00002562
Iteration 26/1000 | Loss: 0.00002562
Iteration 27/1000 | Loss: 0.00002562
Iteration 28/1000 | Loss: 0.00002562
Iteration 29/1000 | Loss: 0.00002562
Iteration 30/1000 | Loss: 0.00002562
Iteration 31/1000 | Loss: 0.00002562
Iteration 32/1000 | Loss: 0.00002561
Iteration 33/1000 | Loss: 0.00002561
Iteration 34/1000 | Loss: 0.00002559
Iteration 35/1000 | Loss: 0.00002559
Iteration 36/1000 | Loss: 0.00002559
Iteration 37/1000 | Loss: 0.00002559
Iteration 38/1000 | Loss: 0.00002559
Iteration 39/1000 | Loss: 0.00002559
Iteration 40/1000 | Loss: 0.00002559
Iteration 41/1000 | Loss: 0.00002559
Iteration 42/1000 | Loss: 0.00002558
Iteration 43/1000 | Loss: 0.00002558
Iteration 44/1000 | Loss: 0.00002558
Iteration 45/1000 | Loss: 0.00002558
Iteration 46/1000 | Loss: 0.00002558
Iteration 47/1000 | Loss: 0.00002558
Iteration 48/1000 | Loss: 0.00002558
Iteration 49/1000 | Loss: 0.00002558
Iteration 50/1000 | Loss: 0.00002558
Iteration 51/1000 | Loss: 0.00002558
Iteration 52/1000 | Loss: 0.00002558
Iteration 53/1000 | Loss: 0.00002558
Iteration 54/1000 | Loss: 0.00002558
Iteration 55/1000 | Loss: 0.00002557
Iteration 56/1000 | Loss: 0.00002557
Iteration 57/1000 | Loss: 0.00002557
Iteration 58/1000 | Loss: 0.00002557
Iteration 59/1000 | Loss: 0.00002556
Iteration 60/1000 | Loss: 0.00002556
Iteration 61/1000 | Loss: 0.00002555
Iteration 62/1000 | Loss: 0.00002555
Iteration 63/1000 | Loss: 0.00002555
Iteration 64/1000 | Loss: 0.00002554
Iteration 65/1000 | Loss: 0.00002554
Iteration 66/1000 | Loss: 0.00002554
Iteration 67/1000 | Loss: 0.00002553
Iteration 68/1000 | Loss: 0.00002553
Iteration 69/1000 | Loss: 0.00002553
Iteration 70/1000 | Loss: 0.00002552
Iteration 71/1000 | Loss: 0.00002552
Iteration 72/1000 | Loss: 0.00002552
Iteration 73/1000 | Loss: 0.00002552
Iteration 74/1000 | Loss: 0.00002552
Iteration 75/1000 | Loss: 0.00002552
Iteration 76/1000 | Loss: 0.00002551
Iteration 77/1000 | Loss: 0.00002551
Iteration 78/1000 | Loss: 0.00002551
Iteration 79/1000 | Loss: 0.00002551
Iteration 80/1000 | Loss: 0.00002551
Iteration 81/1000 | Loss: 0.00002551
Iteration 82/1000 | Loss: 0.00002551
Iteration 83/1000 | Loss: 0.00002551
Iteration 84/1000 | Loss: 0.00002551
Iteration 85/1000 | Loss: 0.00002551
Iteration 86/1000 | Loss: 0.00002551
Iteration 87/1000 | Loss: 0.00002550
Iteration 88/1000 | Loss: 0.00002550
Iteration 89/1000 | Loss: 0.00002550
Iteration 90/1000 | Loss: 0.00002550
Iteration 91/1000 | Loss: 0.00002550
Iteration 92/1000 | Loss: 0.00002550
Iteration 93/1000 | Loss: 0.00002550
Iteration 94/1000 | Loss: 0.00002550
Iteration 95/1000 | Loss: 0.00002550
Iteration 96/1000 | Loss: 0.00002550
Iteration 97/1000 | Loss: 0.00002549
Iteration 98/1000 | Loss: 0.00002549
Iteration 99/1000 | Loss: 0.00002549
Iteration 100/1000 | Loss: 0.00002549
Iteration 101/1000 | Loss: 0.00002549
Iteration 102/1000 | Loss: 0.00002548
Iteration 103/1000 | Loss: 0.00002548
Iteration 104/1000 | Loss: 0.00002548
Iteration 105/1000 | Loss: 0.00002548
Iteration 106/1000 | Loss: 0.00002548
Iteration 107/1000 | Loss: 0.00002548
Iteration 108/1000 | Loss: 0.00002548
Iteration 109/1000 | Loss: 0.00002548
Iteration 110/1000 | Loss: 0.00002547
Iteration 111/1000 | Loss: 0.00002547
Iteration 112/1000 | Loss: 0.00002547
Iteration 113/1000 | Loss: 0.00002547
Iteration 114/1000 | Loss: 0.00002547
Iteration 115/1000 | Loss: 0.00002547
Iteration 116/1000 | Loss: 0.00002547
Iteration 117/1000 | Loss: 0.00002547
Iteration 118/1000 | Loss: 0.00002547
Iteration 119/1000 | Loss: 0.00002547
Iteration 120/1000 | Loss: 0.00002547
Iteration 121/1000 | Loss: 0.00002547
Iteration 122/1000 | Loss: 0.00002547
Iteration 123/1000 | Loss: 0.00002547
Iteration 124/1000 | Loss: 0.00002547
Iteration 125/1000 | Loss: 0.00002547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.5467561499681324e-05, 2.5467561499681324e-05, 2.5467561499681324e-05, 2.5467561499681324e-05, 2.5467561499681324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5467561499681324e-05

Optimization complete. Final v2v error: 4.052715301513672 mm

Highest mean error: 4.462740898132324 mm for frame 160

Lowest mean error: 3.3423423767089844 mm for frame 119

Saving results

Total time: 45.74110412597656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00221688
Iteration 2/25 | Loss: 0.00116723
Iteration 3/25 | Loss: 0.00108589
Iteration 4/25 | Loss: 0.00106719
Iteration 5/25 | Loss: 0.00105909
Iteration 6/25 | Loss: 0.00105635
Iteration 7/25 | Loss: 0.00105534
Iteration 8/25 | Loss: 0.00105534
Iteration 9/25 | Loss: 0.00105534
Iteration 10/25 | Loss: 0.00105534
Iteration 11/25 | Loss: 0.00105534
Iteration 12/25 | Loss: 0.00105534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001055337954312563, 0.001055337954312563, 0.001055337954312563, 0.001055337954312563, 0.001055337954312563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001055337954312563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30637383
Iteration 2/25 | Loss: 0.00135059
Iteration 3/25 | Loss: 0.00135059
Iteration 4/25 | Loss: 0.00135059
Iteration 5/25 | Loss: 0.00135059
Iteration 6/25 | Loss: 0.00135059
Iteration 7/25 | Loss: 0.00135059
Iteration 8/25 | Loss: 0.00135059
Iteration 9/25 | Loss: 0.00135059
Iteration 10/25 | Loss: 0.00135059
Iteration 11/25 | Loss: 0.00135059
Iteration 12/25 | Loss: 0.00135059
Iteration 13/25 | Loss: 0.00135059
Iteration 14/25 | Loss: 0.00135059
Iteration 15/25 | Loss: 0.00135059
Iteration 16/25 | Loss: 0.00135059
Iteration 17/25 | Loss: 0.00135059
Iteration 18/25 | Loss: 0.00135059
Iteration 19/25 | Loss: 0.00135059
Iteration 20/25 | Loss: 0.00135059
Iteration 21/25 | Loss: 0.00135059
Iteration 22/25 | Loss: 0.00135059
Iteration 23/25 | Loss: 0.00135059
Iteration 24/25 | Loss: 0.00135059
Iteration 25/25 | Loss: 0.00135059
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001350587117485702, 0.001350587117485702, 0.001350587117485702, 0.001350587117485702, 0.001350587117485702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001350587117485702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135059
Iteration 2/1000 | Loss: 0.00002964
Iteration 3/1000 | Loss: 0.00001716
Iteration 4/1000 | Loss: 0.00001519
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001296
Iteration 8/1000 | Loss: 0.00001257
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001218
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001206
Iteration 13/1000 | Loss: 0.00001200
Iteration 14/1000 | Loss: 0.00001196
Iteration 15/1000 | Loss: 0.00001195
Iteration 16/1000 | Loss: 0.00001193
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001188
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001182
Iteration 27/1000 | Loss: 0.00001182
Iteration 28/1000 | Loss: 0.00001182
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001177
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001175
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001172
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001170
Iteration 52/1000 | Loss: 0.00001170
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001161
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001159
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001158
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001157
Iteration 105/1000 | Loss: 0.00001157
Iteration 106/1000 | Loss: 0.00001157
Iteration 107/1000 | Loss: 0.00001157
Iteration 108/1000 | Loss: 0.00001157
Iteration 109/1000 | Loss: 0.00001157
Iteration 110/1000 | Loss: 0.00001157
Iteration 111/1000 | Loss: 0.00001157
Iteration 112/1000 | Loss: 0.00001157
Iteration 113/1000 | Loss: 0.00001157
Iteration 114/1000 | Loss: 0.00001157
Iteration 115/1000 | Loss: 0.00001157
Iteration 116/1000 | Loss: 0.00001157
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001156
Iteration 120/1000 | Loss: 0.00001156
Iteration 121/1000 | Loss: 0.00001156
Iteration 122/1000 | Loss: 0.00001156
Iteration 123/1000 | Loss: 0.00001156
Iteration 124/1000 | Loss: 0.00001156
Iteration 125/1000 | Loss: 0.00001155
Iteration 126/1000 | Loss: 0.00001155
Iteration 127/1000 | Loss: 0.00001155
Iteration 128/1000 | Loss: 0.00001155
Iteration 129/1000 | Loss: 0.00001155
Iteration 130/1000 | Loss: 0.00001155
Iteration 131/1000 | Loss: 0.00001154
Iteration 132/1000 | Loss: 0.00001154
Iteration 133/1000 | Loss: 0.00001154
Iteration 134/1000 | Loss: 0.00001154
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001152
Iteration 140/1000 | Loss: 0.00001152
Iteration 141/1000 | Loss: 0.00001152
Iteration 142/1000 | Loss: 0.00001152
Iteration 143/1000 | Loss: 0.00001152
Iteration 144/1000 | Loss: 0.00001152
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001151
Iteration 148/1000 | Loss: 0.00001151
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001151
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001151
Iteration 153/1000 | Loss: 0.00001151
Iteration 154/1000 | Loss: 0.00001151
Iteration 155/1000 | Loss: 0.00001150
Iteration 156/1000 | Loss: 0.00001150
Iteration 157/1000 | Loss: 0.00001149
Iteration 158/1000 | Loss: 0.00001149
Iteration 159/1000 | Loss: 0.00001149
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001148
Iteration 163/1000 | Loss: 0.00001148
Iteration 164/1000 | Loss: 0.00001147
Iteration 165/1000 | Loss: 0.00001147
Iteration 166/1000 | Loss: 0.00001147
Iteration 167/1000 | Loss: 0.00001147
Iteration 168/1000 | Loss: 0.00001147
Iteration 169/1000 | Loss: 0.00001147
Iteration 170/1000 | Loss: 0.00001147
Iteration 171/1000 | Loss: 0.00001147
Iteration 172/1000 | Loss: 0.00001146
Iteration 173/1000 | Loss: 0.00001146
Iteration 174/1000 | Loss: 0.00001146
Iteration 175/1000 | Loss: 0.00001146
Iteration 176/1000 | Loss: 0.00001146
Iteration 177/1000 | Loss: 0.00001146
Iteration 178/1000 | Loss: 0.00001146
Iteration 179/1000 | Loss: 0.00001146
Iteration 180/1000 | Loss: 0.00001146
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001146
Iteration 183/1000 | Loss: 0.00001146
Iteration 184/1000 | Loss: 0.00001145
Iteration 185/1000 | Loss: 0.00001145
Iteration 186/1000 | Loss: 0.00001145
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001145
Iteration 189/1000 | Loss: 0.00001145
Iteration 190/1000 | Loss: 0.00001145
Iteration 191/1000 | Loss: 0.00001145
Iteration 192/1000 | Loss: 0.00001145
Iteration 193/1000 | Loss: 0.00001145
Iteration 194/1000 | Loss: 0.00001145
Iteration 195/1000 | Loss: 0.00001145
Iteration 196/1000 | Loss: 0.00001144
Iteration 197/1000 | Loss: 0.00001144
Iteration 198/1000 | Loss: 0.00001144
Iteration 199/1000 | Loss: 0.00001144
Iteration 200/1000 | Loss: 0.00001144
Iteration 201/1000 | Loss: 0.00001144
Iteration 202/1000 | Loss: 0.00001144
Iteration 203/1000 | Loss: 0.00001144
Iteration 204/1000 | Loss: 0.00001144
Iteration 205/1000 | Loss: 0.00001144
Iteration 206/1000 | Loss: 0.00001144
Iteration 207/1000 | Loss: 0.00001144
Iteration 208/1000 | Loss: 0.00001143
Iteration 209/1000 | Loss: 0.00001143
Iteration 210/1000 | Loss: 0.00001143
Iteration 211/1000 | Loss: 0.00001143
Iteration 212/1000 | Loss: 0.00001143
Iteration 213/1000 | Loss: 0.00001143
Iteration 214/1000 | Loss: 0.00001143
Iteration 215/1000 | Loss: 0.00001143
Iteration 216/1000 | Loss: 0.00001143
Iteration 217/1000 | Loss: 0.00001143
Iteration 218/1000 | Loss: 0.00001143
Iteration 219/1000 | Loss: 0.00001143
Iteration 220/1000 | Loss: 0.00001143
Iteration 221/1000 | Loss: 0.00001143
Iteration 222/1000 | Loss: 0.00001143
Iteration 223/1000 | Loss: 0.00001143
Iteration 224/1000 | Loss: 0.00001143
Iteration 225/1000 | Loss: 0.00001143
Iteration 226/1000 | Loss: 0.00001143
Iteration 227/1000 | Loss: 0.00001143
Iteration 228/1000 | Loss: 0.00001143
Iteration 229/1000 | Loss: 0.00001143
Iteration 230/1000 | Loss: 0.00001143
Iteration 231/1000 | Loss: 0.00001143
Iteration 232/1000 | Loss: 0.00001143
Iteration 233/1000 | Loss: 0.00001143
Iteration 234/1000 | Loss: 0.00001143
Iteration 235/1000 | Loss: 0.00001143
Iteration 236/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.1429971891629975e-05, 1.1429971891629975e-05, 1.1429971891629975e-05, 1.1429971891629975e-05, 1.1429971891629975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1429971891629975e-05

Optimization complete. Final v2v error: 2.914379596710205 mm

Highest mean error: 3.333822727203369 mm for frame 43

Lowest mean error: 2.6701722145080566 mm for frame 187

Saving results

Total time: 45.97328162193298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922518
Iteration 2/25 | Loss: 0.00147231
Iteration 3/25 | Loss: 0.00127814
Iteration 4/25 | Loss: 0.00126711
Iteration 5/25 | Loss: 0.00126529
Iteration 6/25 | Loss: 0.00126525
Iteration 7/25 | Loss: 0.00126525
Iteration 8/25 | Loss: 0.00126525
Iteration 9/25 | Loss: 0.00126525
Iteration 10/25 | Loss: 0.00126525
Iteration 11/25 | Loss: 0.00126525
Iteration 12/25 | Loss: 0.00126525
Iteration 13/25 | Loss: 0.00126525
Iteration 14/25 | Loss: 0.00126525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001265249797143042, 0.001265249797143042, 0.001265249797143042, 0.001265249797143042, 0.001265249797143042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001265249797143042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64533412
Iteration 2/25 | Loss: 0.00081645
Iteration 3/25 | Loss: 0.00081644
Iteration 4/25 | Loss: 0.00081644
Iteration 5/25 | Loss: 0.00081644
Iteration 6/25 | Loss: 0.00081644
Iteration 7/25 | Loss: 0.00081644
Iteration 8/25 | Loss: 0.00081644
Iteration 9/25 | Loss: 0.00081644
Iteration 10/25 | Loss: 0.00081644
Iteration 11/25 | Loss: 0.00081644
Iteration 12/25 | Loss: 0.00081643
Iteration 13/25 | Loss: 0.00081643
Iteration 14/25 | Loss: 0.00081643
Iteration 15/25 | Loss: 0.00081643
Iteration 16/25 | Loss: 0.00081643
Iteration 17/25 | Loss: 0.00081643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008164349710568786, 0.0008164349710568786, 0.0008164349710568786, 0.0008164349710568786, 0.0008164349710568786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008164349710568786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081643
Iteration 2/1000 | Loss: 0.00004408
Iteration 3/1000 | Loss: 0.00002860
Iteration 4/1000 | Loss: 0.00002536
Iteration 5/1000 | Loss: 0.00002328
Iteration 6/1000 | Loss: 0.00002237
Iteration 7/1000 | Loss: 0.00002185
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002118
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002073
Iteration 12/1000 | Loss: 0.00002052
Iteration 13/1000 | Loss: 0.00002045
Iteration 14/1000 | Loss: 0.00002042
Iteration 15/1000 | Loss: 0.00002030
Iteration 16/1000 | Loss: 0.00002028
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00002018
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002015
Iteration 22/1000 | Loss: 0.00002015
Iteration 23/1000 | Loss: 0.00002015
Iteration 24/1000 | Loss: 0.00002015
Iteration 25/1000 | Loss: 0.00002014
Iteration 26/1000 | Loss: 0.00002014
Iteration 27/1000 | Loss: 0.00002014
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002007
Iteration 34/1000 | Loss: 0.00002007
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00001999
Iteration 40/1000 | Loss: 0.00001999
Iteration 41/1000 | Loss: 0.00001998
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001993
Iteration 44/1000 | Loss: 0.00001992
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001992
Iteration 47/1000 | Loss: 0.00001992
Iteration 48/1000 | Loss: 0.00001990
Iteration 49/1000 | Loss: 0.00001990
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001989
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001986
Iteration 61/1000 | Loss: 0.00001986
Iteration 62/1000 | Loss: 0.00001985
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001985
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001985
Iteration 69/1000 | Loss: 0.00001985
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001985
Iteration 72/1000 | Loss: 0.00001985
Iteration 73/1000 | Loss: 0.00001985
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001984
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001984
Iteration 81/1000 | Loss: 0.00001983
Iteration 82/1000 | Loss: 0.00001983
Iteration 83/1000 | Loss: 0.00001983
Iteration 84/1000 | Loss: 0.00001983
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001982
Iteration 91/1000 | Loss: 0.00001982
Iteration 92/1000 | Loss: 0.00001982
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001981
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00001981
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001980
Iteration 101/1000 | Loss: 0.00001980
Iteration 102/1000 | Loss: 0.00001980
Iteration 103/1000 | Loss: 0.00001980
Iteration 104/1000 | Loss: 0.00001980
Iteration 105/1000 | Loss: 0.00001980
Iteration 106/1000 | Loss: 0.00001980
Iteration 107/1000 | Loss: 0.00001979
Iteration 108/1000 | Loss: 0.00001979
Iteration 109/1000 | Loss: 0.00001979
Iteration 110/1000 | Loss: 0.00001979
Iteration 111/1000 | Loss: 0.00001979
Iteration 112/1000 | Loss: 0.00001979
Iteration 113/1000 | Loss: 0.00001979
Iteration 114/1000 | Loss: 0.00001978
Iteration 115/1000 | Loss: 0.00001978
Iteration 116/1000 | Loss: 0.00001978
Iteration 117/1000 | Loss: 0.00001978
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001977
Iteration 122/1000 | Loss: 0.00001977
Iteration 123/1000 | Loss: 0.00001977
Iteration 124/1000 | Loss: 0.00001977
Iteration 125/1000 | Loss: 0.00001977
Iteration 126/1000 | Loss: 0.00001976
Iteration 127/1000 | Loss: 0.00001976
Iteration 128/1000 | Loss: 0.00001976
Iteration 129/1000 | Loss: 0.00001976
Iteration 130/1000 | Loss: 0.00001976
Iteration 131/1000 | Loss: 0.00001975
Iteration 132/1000 | Loss: 0.00001975
Iteration 133/1000 | Loss: 0.00001975
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001975
Iteration 136/1000 | Loss: 0.00001974
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001974
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Iteration 141/1000 | Loss: 0.00001973
Iteration 142/1000 | Loss: 0.00001973
Iteration 143/1000 | Loss: 0.00001973
Iteration 144/1000 | Loss: 0.00001973
Iteration 145/1000 | Loss: 0.00001973
Iteration 146/1000 | Loss: 0.00001973
Iteration 147/1000 | Loss: 0.00001972
Iteration 148/1000 | Loss: 0.00001972
Iteration 149/1000 | Loss: 0.00001972
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001971
Iteration 154/1000 | Loss: 0.00001971
Iteration 155/1000 | Loss: 0.00001971
Iteration 156/1000 | Loss: 0.00001971
Iteration 157/1000 | Loss: 0.00001971
Iteration 158/1000 | Loss: 0.00001970
Iteration 159/1000 | Loss: 0.00001970
Iteration 160/1000 | Loss: 0.00001970
Iteration 161/1000 | Loss: 0.00001970
Iteration 162/1000 | Loss: 0.00001970
Iteration 163/1000 | Loss: 0.00001970
Iteration 164/1000 | Loss: 0.00001970
Iteration 165/1000 | Loss: 0.00001970
Iteration 166/1000 | Loss: 0.00001970
Iteration 167/1000 | Loss: 0.00001970
Iteration 168/1000 | Loss: 0.00001970
Iteration 169/1000 | Loss: 0.00001970
Iteration 170/1000 | Loss: 0.00001970
Iteration 171/1000 | Loss: 0.00001970
Iteration 172/1000 | Loss: 0.00001970
Iteration 173/1000 | Loss: 0.00001970
Iteration 174/1000 | Loss: 0.00001970
Iteration 175/1000 | Loss: 0.00001970
Iteration 176/1000 | Loss: 0.00001970
Iteration 177/1000 | Loss: 0.00001970
Iteration 178/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.9700441043823957e-05, 1.9700441043823957e-05, 1.9700441043823957e-05, 1.9700441043823957e-05, 1.9700441043823957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9700441043823957e-05

Optimization complete. Final v2v error: 3.638986349105835 mm

Highest mean error: 5.091342926025391 mm for frame 5

Lowest mean error: 2.9147043228149414 mm for frame 225

Saving results

Total time: 47.19481587409973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897195
Iteration 2/25 | Loss: 0.00134887
Iteration 3/25 | Loss: 0.00120228
Iteration 4/25 | Loss: 0.00117772
Iteration 5/25 | Loss: 0.00116933
Iteration 6/25 | Loss: 0.00116750
Iteration 7/25 | Loss: 0.00116750
Iteration 8/25 | Loss: 0.00116750
Iteration 9/25 | Loss: 0.00116750
Iteration 10/25 | Loss: 0.00116750
Iteration 11/25 | Loss: 0.00116750
Iteration 12/25 | Loss: 0.00116750
Iteration 13/25 | Loss: 0.00116750
Iteration 14/25 | Loss: 0.00116750
Iteration 15/25 | Loss: 0.00116750
Iteration 16/25 | Loss: 0.00116750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011675006244331598, 0.0011675006244331598, 0.0011675006244331598, 0.0011675006244331598, 0.0011675006244331598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011675006244331598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33149886
Iteration 2/25 | Loss: 0.00105717
Iteration 3/25 | Loss: 0.00105704
Iteration 4/25 | Loss: 0.00105704
Iteration 5/25 | Loss: 0.00105703
Iteration 6/25 | Loss: 0.00105703
Iteration 7/25 | Loss: 0.00105703
Iteration 8/25 | Loss: 0.00105703
Iteration 9/25 | Loss: 0.00105703
Iteration 10/25 | Loss: 0.00105703
Iteration 11/25 | Loss: 0.00105703
Iteration 12/25 | Loss: 0.00105703
Iteration 13/25 | Loss: 0.00105703
Iteration 14/25 | Loss: 0.00105703
Iteration 15/25 | Loss: 0.00105703
Iteration 16/25 | Loss: 0.00105703
Iteration 17/25 | Loss: 0.00105703
Iteration 18/25 | Loss: 0.00105703
Iteration 19/25 | Loss: 0.00105703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001057032379321754, 0.001057032379321754, 0.001057032379321754, 0.001057032379321754, 0.001057032379321754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001057032379321754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105703
Iteration 2/1000 | Loss: 0.00005848
Iteration 3/1000 | Loss: 0.00003701
Iteration 4/1000 | Loss: 0.00002878
Iteration 5/1000 | Loss: 0.00002695
Iteration 6/1000 | Loss: 0.00002593
Iteration 7/1000 | Loss: 0.00002514
Iteration 8/1000 | Loss: 0.00002467
Iteration 9/1000 | Loss: 0.00002415
Iteration 10/1000 | Loss: 0.00002386
Iteration 11/1000 | Loss: 0.00002367
Iteration 12/1000 | Loss: 0.00002349
Iteration 13/1000 | Loss: 0.00002345
Iteration 14/1000 | Loss: 0.00002341
Iteration 15/1000 | Loss: 0.00002337
Iteration 16/1000 | Loss: 0.00002336
Iteration 17/1000 | Loss: 0.00002326
Iteration 18/1000 | Loss: 0.00002324
Iteration 19/1000 | Loss: 0.00002316
Iteration 20/1000 | Loss: 0.00002316
Iteration 21/1000 | Loss: 0.00002315
Iteration 22/1000 | Loss: 0.00002314
Iteration 23/1000 | Loss: 0.00002314
Iteration 24/1000 | Loss: 0.00002313
Iteration 25/1000 | Loss: 0.00002313
Iteration 26/1000 | Loss: 0.00002313
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002311
Iteration 29/1000 | Loss: 0.00002310
Iteration 30/1000 | Loss: 0.00002309
Iteration 31/1000 | Loss: 0.00002309
Iteration 32/1000 | Loss: 0.00002308
Iteration 33/1000 | Loss: 0.00002308
Iteration 34/1000 | Loss: 0.00002307
Iteration 35/1000 | Loss: 0.00002304
Iteration 36/1000 | Loss: 0.00002302
Iteration 37/1000 | Loss: 0.00002302
Iteration 38/1000 | Loss: 0.00002301
Iteration 39/1000 | Loss: 0.00002301
Iteration 40/1000 | Loss: 0.00002300
Iteration 41/1000 | Loss: 0.00002297
Iteration 42/1000 | Loss: 0.00002297
Iteration 43/1000 | Loss: 0.00002296
Iteration 44/1000 | Loss: 0.00002296
Iteration 45/1000 | Loss: 0.00002295
Iteration 46/1000 | Loss: 0.00002294
Iteration 47/1000 | Loss: 0.00002294
Iteration 48/1000 | Loss: 0.00002293
Iteration 49/1000 | Loss: 0.00002293
Iteration 50/1000 | Loss: 0.00002290
Iteration 51/1000 | Loss: 0.00002290
Iteration 52/1000 | Loss: 0.00002289
Iteration 53/1000 | Loss: 0.00002287
Iteration 54/1000 | Loss: 0.00002287
Iteration 55/1000 | Loss: 0.00002286
Iteration 56/1000 | Loss: 0.00002286
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002286
Iteration 59/1000 | Loss: 0.00002286
Iteration 60/1000 | Loss: 0.00002286
Iteration 61/1000 | Loss: 0.00002286
Iteration 62/1000 | Loss: 0.00002286
Iteration 63/1000 | Loss: 0.00002285
Iteration 64/1000 | Loss: 0.00002285
Iteration 65/1000 | Loss: 0.00002285
Iteration 66/1000 | Loss: 0.00002285
Iteration 67/1000 | Loss: 0.00002285
Iteration 68/1000 | Loss: 0.00002285
Iteration 69/1000 | Loss: 0.00002285
Iteration 70/1000 | Loss: 0.00002284
Iteration 71/1000 | Loss: 0.00002284
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002283
Iteration 74/1000 | Loss: 0.00002283
Iteration 75/1000 | Loss: 0.00002283
Iteration 76/1000 | Loss: 0.00002283
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002281
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002280
Iteration 91/1000 | Loss: 0.00002280
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002279
Iteration 94/1000 | Loss: 0.00002279
Iteration 95/1000 | Loss: 0.00002279
Iteration 96/1000 | Loss: 0.00002279
Iteration 97/1000 | Loss: 0.00002279
Iteration 98/1000 | Loss: 0.00002278
Iteration 99/1000 | Loss: 0.00002278
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002278
Iteration 102/1000 | Loss: 0.00002278
Iteration 103/1000 | Loss: 0.00002278
Iteration 104/1000 | Loss: 0.00002278
Iteration 105/1000 | Loss: 0.00002278
Iteration 106/1000 | Loss: 0.00002278
Iteration 107/1000 | Loss: 0.00002278
Iteration 108/1000 | Loss: 0.00002278
Iteration 109/1000 | Loss: 0.00002277
Iteration 110/1000 | Loss: 0.00002277
Iteration 111/1000 | Loss: 0.00002277
Iteration 112/1000 | Loss: 0.00002276
Iteration 113/1000 | Loss: 0.00002276
Iteration 114/1000 | Loss: 0.00002276
Iteration 115/1000 | Loss: 0.00002276
Iteration 116/1000 | Loss: 0.00002275
Iteration 117/1000 | Loss: 0.00002275
Iteration 118/1000 | Loss: 0.00002275
Iteration 119/1000 | Loss: 0.00002275
Iteration 120/1000 | Loss: 0.00002275
Iteration 121/1000 | Loss: 0.00002275
Iteration 122/1000 | Loss: 0.00002275
Iteration 123/1000 | Loss: 0.00002275
Iteration 124/1000 | Loss: 0.00002275
Iteration 125/1000 | Loss: 0.00002275
Iteration 126/1000 | Loss: 0.00002275
Iteration 127/1000 | Loss: 0.00002275
Iteration 128/1000 | Loss: 0.00002274
Iteration 129/1000 | Loss: 0.00002274
Iteration 130/1000 | Loss: 0.00002274
Iteration 131/1000 | Loss: 0.00002274
Iteration 132/1000 | Loss: 0.00002273
Iteration 133/1000 | Loss: 0.00002273
Iteration 134/1000 | Loss: 0.00002273
Iteration 135/1000 | Loss: 0.00002273
Iteration 136/1000 | Loss: 0.00002273
Iteration 137/1000 | Loss: 0.00002273
Iteration 138/1000 | Loss: 0.00002273
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002273
Iteration 141/1000 | Loss: 0.00002273
Iteration 142/1000 | Loss: 0.00002273
Iteration 143/1000 | Loss: 0.00002273
Iteration 144/1000 | Loss: 0.00002273
Iteration 145/1000 | Loss: 0.00002273
Iteration 146/1000 | Loss: 0.00002272
Iteration 147/1000 | Loss: 0.00002272
Iteration 148/1000 | Loss: 0.00002272
Iteration 149/1000 | Loss: 0.00002272
Iteration 150/1000 | Loss: 0.00002272
Iteration 151/1000 | Loss: 0.00002272
Iteration 152/1000 | Loss: 0.00002272
Iteration 153/1000 | Loss: 0.00002272
Iteration 154/1000 | Loss: 0.00002272
Iteration 155/1000 | Loss: 0.00002272
Iteration 156/1000 | Loss: 0.00002271
Iteration 157/1000 | Loss: 0.00002271
Iteration 158/1000 | Loss: 0.00002271
Iteration 159/1000 | Loss: 0.00002271
Iteration 160/1000 | Loss: 0.00002271
Iteration 161/1000 | Loss: 0.00002271
Iteration 162/1000 | Loss: 0.00002271
Iteration 163/1000 | Loss: 0.00002271
Iteration 164/1000 | Loss: 0.00002270
Iteration 165/1000 | Loss: 0.00002270
Iteration 166/1000 | Loss: 0.00002270
Iteration 167/1000 | Loss: 0.00002270
Iteration 168/1000 | Loss: 0.00002270
Iteration 169/1000 | Loss: 0.00002270
Iteration 170/1000 | Loss: 0.00002270
Iteration 171/1000 | Loss: 0.00002270
Iteration 172/1000 | Loss: 0.00002269
Iteration 173/1000 | Loss: 0.00002269
Iteration 174/1000 | Loss: 0.00002269
Iteration 175/1000 | Loss: 0.00002269
Iteration 176/1000 | Loss: 0.00002269
Iteration 177/1000 | Loss: 0.00002269
Iteration 178/1000 | Loss: 0.00002269
Iteration 179/1000 | Loss: 0.00002269
Iteration 180/1000 | Loss: 0.00002269
Iteration 181/1000 | Loss: 0.00002269
Iteration 182/1000 | Loss: 0.00002269
Iteration 183/1000 | Loss: 0.00002269
Iteration 184/1000 | Loss: 0.00002269
Iteration 185/1000 | Loss: 0.00002269
Iteration 186/1000 | Loss: 0.00002269
Iteration 187/1000 | Loss: 0.00002269
Iteration 188/1000 | Loss: 0.00002269
Iteration 189/1000 | Loss: 0.00002268
Iteration 190/1000 | Loss: 0.00002268
Iteration 191/1000 | Loss: 0.00002268
Iteration 192/1000 | Loss: 0.00002268
Iteration 193/1000 | Loss: 0.00002268
Iteration 194/1000 | Loss: 0.00002268
Iteration 195/1000 | Loss: 0.00002268
Iteration 196/1000 | Loss: 0.00002268
Iteration 197/1000 | Loss: 0.00002268
Iteration 198/1000 | Loss: 0.00002268
Iteration 199/1000 | Loss: 0.00002268
Iteration 200/1000 | Loss: 0.00002268
Iteration 201/1000 | Loss: 0.00002268
Iteration 202/1000 | Loss: 0.00002268
Iteration 203/1000 | Loss: 0.00002268
Iteration 204/1000 | Loss: 0.00002268
Iteration 205/1000 | Loss: 0.00002268
Iteration 206/1000 | Loss: 0.00002267
Iteration 207/1000 | Loss: 0.00002267
Iteration 208/1000 | Loss: 0.00002267
Iteration 209/1000 | Loss: 0.00002267
Iteration 210/1000 | Loss: 0.00002267
Iteration 211/1000 | Loss: 0.00002267
Iteration 212/1000 | Loss: 0.00002267
Iteration 213/1000 | Loss: 0.00002267
Iteration 214/1000 | Loss: 0.00002267
Iteration 215/1000 | Loss: 0.00002267
Iteration 216/1000 | Loss: 0.00002267
Iteration 217/1000 | Loss: 0.00002267
Iteration 218/1000 | Loss: 0.00002266
Iteration 219/1000 | Loss: 0.00002266
Iteration 220/1000 | Loss: 0.00002266
Iteration 221/1000 | Loss: 0.00002266
Iteration 222/1000 | Loss: 0.00002266
Iteration 223/1000 | Loss: 0.00002266
Iteration 224/1000 | Loss: 0.00002266
Iteration 225/1000 | Loss: 0.00002266
Iteration 226/1000 | Loss: 0.00002266
Iteration 227/1000 | Loss: 0.00002266
Iteration 228/1000 | Loss: 0.00002266
Iteration 229/1000 | Loss: 0.00002266
Iteration 230/1000 | Loss: 0.00002266
Iteration 231/1000 | Loss: 0.00002266
Iteration 232/1000 | Loss: 0.00002266
Iteration 233/1000 | Loss: 0.00002266
Iteration 234/1000 | Loss: 0.00002266
Iteration 235/1000 | Loss: 0.00002266
Iteration 236/1000 | Loss: 0.00002266
Iteration 237/1000 | Loss: 0.00002266
Iteration 238/1000 | Loss: 0.00002265
Iteration 239/1000 | Loss: 0.00002265
Iteration 240/1000 | Loss: 0.00002265
Iteration 241/1000 | Loss: 0.00002265
Iteration 242/1000 | Loss: 0.00002265
Iteration 243/1000 | Loss: 0.00002265
Iteration 244/1000 | Loss: 0.00002265
Iteration 245/1000 | Loss: 0.00002265
Iteration 246/1000 | Loss: 0.00002265
Iteration 247/1000 | Loss: 0.00002265
Iteration 248/1000 | Loss: 0.00002265
Iteration 249/1000 | Loss: 0.00002265
Iteration 250/1000 | Loss: 0.00002265
Iteration 251/1000 | Loss: 0.00002265
Iteration 252/1000 | Loss: 0.00002265
Iteration 253/1000 | Loss: 0.00002265
Iteration 254/1000 | Loss: 0.00002265
Iteration 255/1000 | Loss: 0.00002265
Iteration 256/1000 | Loss: 0.00002265
Iteration 257/1000 | Loss: 0.00002265
Iteration 258/1000 | Loss: 0.00002265
Iteration 259/1000 | Loss: 0.00002265
Iteration 260/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.2645623175776564e-05, 2.2645623175776564e-05, 2.2645623175776564e-05, 2.2645623175776564e-05, 2.2645623175776564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2645623175776564e-05

Optimization complete. Final v2v error: 3.505307674407959 mm

Highest mean error: 5.569672584533691 mm for frame 86

Lowest mean error: 2.395347833633423 mm for frame 49

Saving results

Total time: 46.94044780731201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093177
Iteration 2/25 | Loss: 0.00205688
Iteration 3/25 | Loss: 0.00147489
Iteration 4/25 | Loss: 0.00140461
Iteration 5/25 | Loss: 0.00144008
Iteration 6/25 | Loss: 0.00144739
Iteration 7/25 | Loss: 0.00140133
Iteration 8/25 | Loss: 0.00136257
Iteration 9/25 | Loss: 0.00135276
Iteration 10/25 | Loss: 0.00134295
Iteration 11/25 | Loss: 0.00134277
Iteration 12/25 | Loss: 0.00133826
Iteration 13/25 | Loss: 0.00132636
Iteration 14/25 | Loss: 0.00132387
Iteration 15/25 | Loss: 0.00131668
Iteration 16/25 | Loss: 0.00131751
Iteration 17/25 | Loss: 0.00132013
Iteration 18/25 | Loss: 0.00131768
Iteration 19/25 | Loss: 0.00131903
Iteration 20/25 | Loss: 0.00131655
Iteration 21/25 | Loss: 0.00131410
Iteration 22/25 | Loss: 0.00131485
Iteration 23/25 | Loss: 0.00131728
Iteration 24/25 | Loss: 0.00131564
Iteration 25/25 | Loss: 0.00131959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15495372
Iteration 2/25 | Loss: 0.00152937
Iteration 3/25 | Loss: 0.00152934
Iteration 4/25 | Loss: 0.00152934
Iteration 5/25 | Loss: 0.00152934
Iteration 6/25 | Loss: 0.00152934
Iteration 7/25 | Loss: 0.00152934
Iteration 8/25 | Loss: 0.00152934
Iteration 9/25 | Loss: 0.00152934
Iteration 10/25 | Loss: 0.00152934
Iteration 11/25 | Loss: 0.00152934
Iteration 12/25 | Loss: 0.00152934
Iteration 13/25 | Loss: 0.00152934
Iteration 14/25 | Loss: 0.00152934
Iteration 15/25 | Loss: 0.00152934
Iteration 16/25 | Loss: 0.00152934
Iteration 17/25 | Loss: 0.00152934
Iteration 18/25 | Loss: 0.00152934
Iteration 19/25 | Loss: 0.00152934
Iteration 20/25 | Loss: 0.00152934
Iteration 21/25 | Loss: 0.00152934
Iteration 22/25 | Loss: 0.00152934
Iteration 23/25 | Loss: 0.00152934
Iteration 24/25 | Loss: 0.00152934
Iteration 25/25 | Loss: 0.00152934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152934
Iteration 2/1000 | Loss: 0.00038178
Iteration 3/1000 | Loss: 0.00044207
Iteration 4/1000 | Loss: 0.00033149
Iteration 5/1000 | Loss: 0.00039722
Iteration 6/1000 | Loss: 0.00048807
Iteration 7/1000 | Loss: 0.00041810
Iteration 8/1000 | Loss: 0.00038869
Iteration 9/1000 | Loss: 0.00041499
Iteration 10/1000 | Loss: 0.00053799
Iteration 11/1000 | Loss: 0.00045812
Iteration 12/1000 | Loss: 0.00035047
Iteration 13/1000 | Loss: 0.00048296
Iteration 14/1000 | Loss: 0.00040223
Iteration 15/1000 | Loss: 0.00033063
Iteration 16/1000 | Loss: 0.00056996
Iteration 17/1000 | Loss: 0.00037529
Iteration 18/1000 | Loss: 0.00056450
Iteration 19/1000 | Loss: 0.00038105
Iteration 20/1000 | Loss: 0.00043579
Iteration 21/1000 | Loss: 0.00038872
Iteration 22/1000 | Loss: 0.00037413
Iteration 23/1000 | Loss: 0.00038341
Iteration 24/1000 | Loss: 0.00045089
Iteration 25/1000 | Loss: 0.00045025
Iteration 26/1000 | Loss: 0.00043553
Iteration 27/1000 | Loss: 0.00054230
Iteration 28/1000 | Loss: 0.00045218
Iteration 29/1000 | Loss: 0.00019969
Iteration 30/1000 | Loss: 0.00032821
Iteration 31/1000 | Loss: 0.00054119
Iteration 32/1000 | Loss: 0.00035763
Iteration 33/1000 | Loss: 0.00025500
Iteration 34/1000 | Loss: 0.00017321
Iteration 35/1000 | Loss: 0.00048990
Iteration 36/1000 | Loss: 0.00023489
Iteration 37/1000 | Loss: 0.00021825
Iteration 38/1000 | Loss: 0.00041117
Iteration 39/1000 | Loss: 0.00042767
Iteration 40/1000 | Loss: 0.00032466
Iteration 41/1000 | Loss: 0.00030817
Iteration 42/1000 | Loss: 0.00024162
Iteration 43/1000 | Loss: 0.00037867
Iteration 44/1000 | Loss: 0.00048310
Iteration 45/1000 | Loss: 0.00042526
Iteration 46/1000 | Loss: 0.00022379
Iteration 47/1000 | Loss: 0.00022783
Iteration 48/1000 | Loss: 0.00025483
Iteration 49/1000 | Loss: 0.00044286
Iteration 50/1000 | Loss: 0.00036021
Iteration 51/1000 | Loss: 0.00026251
Iteration 52/1000 | Loss: 0.00027621
Iteration 53/1000 | Loss: 0.00038570
Iteration 54/1000 | Loss: 0.00040011
Iteration 55/1000 | Loss: 0.00026753
Iteration 56/1000 | Loss: 0.00035221
Iteration 57/1000 | Loss: 0.00040899
Iteration 58/1000 | Loss: 0.00028598
Iteration 59/1000 | Loss: 0.00029704
Iteration 60/1000 | Loss: 0.00024877
Iteration 61/1000 | Loss: 0.00043149
Iteration 62/1000 | Loss: 0.00038775
Iteration 63/1000 | Loss: 0.00049669
Iteration 64/1000 | Loss: 0.00035434
Iteration 65/1000 | Loss: 0.00036191
Iteration 66/1000 | Loss: 0.00036200
Iteration 67/1000 | Loss: 0.00035189
Iteration 68/1000 | Loss: 0.00036814
Iteration 69/1000 | Loss: 0.00037730
Iteration 70/1000 | Loss: 0.00042760
Iteration 71/1000 | Loss: 0.00034176
Iteration 72/1000 | Loss: 0.00043336
Iteration 73/1000 | Loss: 0.00039045
Iteration 74/1000 | Loss: 0.00052994
Iteration 75/1000 | Loss: 0.00026730
Iteration 76/1000 | Loss: 0.00037890
Iteration 77/1000 | Loss: 0.00038500
Iteration 78/1000 | Loss: 0.00037714
Iteration 79/1000 | Loss: 0.00026707
Iteration 80/1000 | Loss: 0.00031440
Iteration 81/1000 | Loss: 0.00035020
Iteration 82/1000 | Loss: 0.00031653
Iteration 83/1000 | Loss: 0.00038382
Iteration 84/1000 | Loss: 0.00035156
Iteration 85/1000 | Loss: 0.00032195
Iteration 86/1000 | Loss: 0.00035365
Iteration 87/1000 | Loss: 0.00035887
Iteration 88/1000 | Loss: 0.00035147
Iteration 89/1000 | Loss: 0.00039805
Iteration 90/1000 | Loss: 0.00050675
Iteration 91/1000 | Loss: 0.00037539
Iteration 92/1000 | Loss: 0.00026724
Iteration 93/1000 | Loss: 0.00037010
Iteration 94/1000 | Loss: 0.00038981
Iteration 95/1000 | Loss: 0.00035498
Iteration 96/1000 | Loss: 0.00030852
Iteration 97/1000 | Loss: 0.00029862
Iteration 98/1000 | Loss: 0.00027810
Iteration 99/1000 | Loss: 0.00035885
Iteration 100/1000 | Loss: 0.00031381
Iteration 101/1000 | Loss: 0.00034600
Iteration 102/1000 | Loss: 0.00073546
Iteration 103/1000 | Loss: 0.00050845
Iteration 104/1000 | Loss: 0.00021281
Iteration 105/1000 | Loss: 0.00056312
Iteration 106/1000 | Loss: 0.00062294
Iteration 107/1000 | Loss: 0.00037991
Iteration 108/1000 | Loss: 0.00035455
Iteration 109/1000 | Loss: 0.00075175
Iteration 110/1000 | Loss: 0.00094269
Iteration 111/1000 | Loss: 0.00035694
Iteration 112/1000 | Loss: 0.00066694
Iteration 113/1000 | Loss: 0.00010010
Iteration 114/1000 | Loss: 0.00029075
Iteration 115/1000 | Loss: 0.00029357
Iteration 116/1000 | Loss: 0.00022339
Iteration 117/1000 | Loss: 0.00025732
Iteration 118/1000 | Loss: 0.00050166
Iteration 119/1000 | Loss: 0.00059325
Iteration 120/1000 | Loss: 0.00066647
Iteration 121/1000 | Loss: 0.00041674
Iteration 122/1000 | Loss: 0.00010050
Iteration 123/1000 | Loss: 0.00021829
Iteration 124/1000 | Loss: 0.00017822
Iteration 125/1000 | Loss: 0.00020351
Iteration 126/1000 | Loss: 0.00061261
Iteration 127/1000 | Loss: 0.00029568
Iteration 128/1000 | Loss: 0.00021342
Iteration 129/1000 | Loss: 0.00016970
Iteration 130/1000 | Loss: 0.00021504
Iteration 131/1000 | Loss: 0.00018359
Iteration 132/1000 | Loss: 0.00011925
Iteration 133/1000 | Loss: 0.00025648
Iteration 134/1000 | Loss: 0.00025605
Iteration 135/1000 | Loss: 0.00025320
Iteration 136/1000 | Loss: 0.00018069
Iteration 137/1000 | Loss: 0.00030630
Iteration 138/1000 | Loss: 0.00014566
Iteration 139/1000 | Loss: 0.00017567
Iteration 140/1000 | Loss: 0.00021058
Iteration 141/1000 | Loss: 0.00023168
Iteration 142/1000 | Loss: 0.00020761
Iteration 143/1000 | Loss: 0.00021872
Iteration 144/1000 | Loss: 0.00026610
Iteration 145/1000 | Loss: 0.00020461
Iteration 146/1000 | Loss: 0.00014100
Iteration 147/1000 | Loss: 0.00025015
Iteration 148/1000 | Loss: 0.00020761
Iteration 149/1000 | Loss: 0.00021002
Iteration 150/1000 | Loss: 0.00022430
Iteration 151/1000 | Loss: 0.00019179
Iteration 152/1000 | Loss: 0.00029706
Iteration 153/1000 | Loss: 0.00016151
Iteration 154/1000 | Loss: 0.00061022
Iteration 155/1000 | Loss: 0.00020921
Iteration 156/1000 | Loss: 0.00005920
Iteration 157/1000 | Loss: 0.00013070
Iteration 158/1000 | Loss: 0.00005150
Iteration 159/1000 | Loss: 0.00011817
Iteration 160/1000 | Loss: 0.00011197
Iteration 161/1000 | Loss: 0.00013146
Iteration 162/1000 | Loss: 0.00005460
Iteration 163/1000 | Loss: 0.00011806
Iteration 164/1000 | Loss: 0.00011907
Iteration 165/1000 | Loss: 0.00015215
Iteration 166/1000 | Loss: 0.00012463
Iteration 167/1000 | Loss: 0.00011741
Iteration 168/1000 | Loss: 0.00012784
Iteration 169/1000 | Loss: 0.00010813
Iteration 170/1000 | Loss: 0.00012539
Iteration 171/1000 | Loss: 0.00012767
Iteration 172/1000 | Loss: 0.00014645
Iteration 173/1000 | Loss: 0.00015037
Iteration 174/1000 | Loss: 0.00021030
Iteration 175/1000 | Loss: 0.00016788
Iteration 176/1000 | Loss: 0.00019469
Iteration 177/1000 | Loss: 0.00005481
Iteration 178/1000 | Loss: 0.00016650
Iteration 179/1000 | Loss: 0.00005893
Iteration 180/1000 | Loss: 0.00014181
Iteration 181/1000 | Loss: 0.00011469
Iteration 182/1000 | Loss: 0.00011039
Iteration 183/1000 | Loss: 0.00011372
Iteration 184/1000 | Loss: 0.00017295
Iteration 185/1000 | Loss: 0.00012562
Iteration 186/1000 | Loss: 0.00009159
Iteration 187/1000 | Loss: 0.00012064
Iteration 188/1000 | Loss: 0.00013502
Iteration 189/1000 | Loss: 0.00011413
Iteration 190/1000 | Loss: 0.00013396
Iteration 191/1000 | Loss: 0.00011506
Iteration 192/1000 | Loss: 0.00008183
Iteration 193/1000 | Loss: 0.00011794
Iteration 194/1000 | Loss: 0.00011701
Iteration 195/1000 | Loss: 0.00012589
Iteration 196/1000 | Loss: 0.00013787
Iteration 197/1000 | Loss: 0.00010625
Iteration 198/1000 | Loss: 0.00006668
Iteration 199/1000 | Loss: 0.00007299
Iteration 200/1000 | Loss: 0.00015396
Iteration 201/1000 | Loss: 0.00015305
Iteration 202/1000 | Loss: 0.00015192
Iteration 203/1000 | Loss: 0.00013536
Iteration 204/1000 | Loss: 0.00004124
Iteration 205/1000 | Loss: 0.00016165
Iteration 206/1000 | Loss: 0.00004055
Iteration 207/1000 | Loss: 0.00004405
Iteration 208/1000 | Loss: 0.00004088
Iteration 209/1000 | Loss: 0.00004357
Iteration 210/1000 | Loss: 0.00004050
Iteration 211/1000 | Loss: 0.00003131
Iteration 212/1000 | Loss: 0.00003419
Iteration 213/1000 | Loss: 0.00003181
Iteration 214/1000 | Loss: 0.00004124
Iteration 215/1000 | Loss: 0.00004053
Iteration 216/1000 | Loss: 0.00004268
Iteration 217/1000 | Loss: 0.00003904
Iteration 218/1000 | Loss: 0.00004202
Iteration 219/1000 | Loss: 0.00003956
Iteration 220/1000 | Loss: 0.00004151
Iteration 221/1000 | Loss: 0.00003918
Iteration 222/1000 | Loss: 0.00003674
Iteration 223/1000 | Loss: 0.00003463
Iteration 224/1000 | Loss: 0.00004108
Iteration 225/1000 | Loss: 0.00003846
Iteration 226/1000 | Loss: 0.00003353
Iteration 227/1000 | Loss: 0.00003917
Iteration 228/1000 | Loss: 0.00003276
Iteration 229/1000 | Loss: 0.00003334
Iteration 230/1000 | Loss: 0.00003134
Iteration 231/1000 | Loss: 0.00003185
Iteration 232/1000 | Loss: 0.00004045
Iteration 233/1000 | Loss: 0.00003989
Iteration 234/1000 | Loss: 0.00004131
Iteration 235/1000 | Loss: 0.00003977
Iteration 236/1000 | Loss: 0.00045788
Iteration 237/1000 | Loss: 0.00035176
Iteration 238/1000 | Loss: 0.00046535
Iteration 239/1000 | Loss: 0.00004808
Iteration 240/1000 | Loss: 0.00003304
Iteration 241/1000 | Loss: 0.00002896
Iteration 242/1000 | Loss: 0.00002756
Iteration 243/1000 | Loss: 0.00002683
Iteration 244/1000 | Loss: 0.00002643
Iteration 245/1000 | Loss: 0.00002611
Iteration 246/1000 | Loss: 0.00002601
Iteration 247/1000 | Loss: 0.00002597
Iteration 248/1000 | Loss: 0.00002583
Iteration 249/1000 | Loss: 0.00002582
Iteration 250/1000 | Loss: 0.00002582
Iteration 251/1000 | Loss: 0.00002581
Iteration 252/1000 | Loss: 0.00002580
Iteration 253/1000 | Loss: 0.00002580
Iteration 254/1000 | Loss: 0.00002580
Iteration 255/1000 | Loss: 0.00002580
Iteration 256/1000 | Loss: 0.00002579
Iteration 257/1000 | Loss: 0.00002579
Iteration 258/1000 | Loss: 0.00002579
Iteration 259/1000 | Loss: 0.00002579
Iteration 260/1000 | Loss: 0.00002579
Iteration 261/1000 | Loss: 0.00002578
Iteration 262/1000 | Loss: 0.00002578
Iteration 263/1000 | Loss: 0.00002578
Iteration 264/1000 | Loss: 0.00002578
Iteration 265/1000 | Loss: 0.00002578
Iteration 266/1000 | Loss: 0.00002570
Iteration 267/1000 | Loss: 0.00002569
Iteration 268/1000 | Loss: 0.00002569
Iteration 269/1000 | Loss: 0.00002569
Iteration 270/1000 | Loss: 0.00002568
Iteration 271/1000 | Loss: 0.00002568
Iteration 272/1000 | Loss: 0.00002567
Iteration 273/1000 | Loss: 0.00002567
Iteration 274/1000 | Loss: 0.00002567
Iteration 275/1000 | Loss: 0.00002566
Iteration 276/1000 | Loss: 0.00002566
Iteration 277/1000 | Loss: 0.00002566
Iteration 278/1000 | Loss: 0.00002565
Iteration 279/1000 | Loss: 0.00002565
Iteration 280/1000 | Loss: 0.00002564
Iteration 281/1000 | Loss: 0.00002564
Iteration 282/1000 | Loss: 0.00002563
Iteration 283/1000 | Loss: 0.00002562
Iteration 284/1000 | Loss: 0.00002561
Iteration 285/1000 | Loss: 0.00002561
Iteration 286/1000 | Loss: 0.00002561
Iteration 287/1000 | Loss: 0.00002559
Iteration 288/1000 | Loss: 0.00002558
Iteration 289/1000 | Loss: 0.00002558
Iteration 290/1000 | Loss: 0.00002557
Iteration 291/1000 | Loss: 0.00002557
Iteration 292/1000 | Loss: 0.00002557
Iteration 293/1000 | Loss: 0.00002557
Iteration 294/1000 | Loss: 0.00002557
Iteration 295/1000 | Loss: 0.00002556
Iteration 296/1000 | Loss: 0.00002556
Iteration 297/1000 | Loss: 0.00002556
Iteration 298/1000 | Loss: 0.00002556
Iteration 299/1000 | Loss: 0.00002556
Iteration 300/1000 | Loss: 0.00002556
Iteration 301/1000 | Loss: 0.00002555
Iteration 302/1000 | Loss: 0.00002555
Iteration 303/1000 | Loss: 0.00002555
Iteration 304/1000 | Loss: 0.00002555
Iteration 305/1000 | Loss: 0.00002555
Iteration 306/1000 | Loss: 0.00002555
Iteration 307/1000 | Loss: 0.00002554
Iteration 308/1000 | Loss: 0.00002554
Iteration 309/1000 | Loss: 0.00002554
Iteration 310/1000 | Loss: 0.00002554
Iteration 311/1000 | Loss: 0.00002554
Iteration 312/1000 | Loss: 0.00002554
Iteration 313/1000 | Loss: 0.00002554
Iteration 314/1000 | Loss: 0.00002554
Iteration 315/1000 | Loss: 0.00002553
Iteration 316/1000 | Loss: 0.00002553
Iteration 317/1000 | Loss: 0.00002553
Iteration 318/1000 | Loss: 0.00002553
Iteration 319/1000 | Loss: 0.00002553
Iteration 320/1000 | Loss: 0.00002552
Iteration 321/1000 | Loss: 0.00002552
Iteration 322/1000 | Loss: 0.00002552
Iteration 323/1000 | Loss: 0.00002552
Iteration 324/1000 | Loss: 0.00002552
Iteration 325/1000 | Loss: 0.00002552
Iteration 326/1000 | Loss: 0.00002551
Iteration 327/1000 | Loss: 0.00002551
Iteration 328/1000 | Loss: 0.00002551
Iteration 329/1000 | Loss: 0.00002551
Iteration 330/1000 | Loss: 0.00002551
Iteration 331/1000 | Loss: 0.00002550
Iteration 332/1000 | Loss: 0.00002550
Iteration 333/1000 | Loss: 0.00002549
Iteration 334/1000 | Loss: 0.00002549
Iteration 335/1000 | Loss: 0.00002549
Iteration 336/1000 | Loss: 0.00002549
Iteration 337/1000 | Loss: 0.00002549
Iteration 338/1000 | Loss: 0.00002549
Iteration 339/1000 | Loss: 0.00002549
Iteration 340/1000 | Loss: 0.00002549
Iteration 341/1000 | Loss: 0.00002549
Iteration 342/1000 | Loss: 0.00002548
Iteration 343/1000 | Loss: 0.00002548
Iteration 344/1000 | Loss: 0.00002548
Iteration 345/1000 | Loss: 0.00002548
Iteration 346/1000 | Loss: 0.00002548
Iteration 347/1000 | Loss: 0.00002547
Iteration 348/1000 | Loss: 0.00002547
Iteration 349/1000 | Loss: 0.00002547
Iteration 350/1000 | Loss: 0.00002547
Iteration 351/1000 | Loss: 0.00002547
Iteration 352/1000 | Loss: 0.00002547
Iteration 353/1000 | Loss: 0.00002547
Iteration 354/1000 | Loss: 0.00002547
Iteration 355/1000 | Loss: 0.00002547
Iteration 356/1000 | Loss: 0.00002547
Iteration 357/1000 | Loss: 0.00002547
Iteration 358/1000 | Loss: 0.00002547
Iteration 359/1000 | Loss: 0.00002546
Iteration 360/1000 | Loss: 0.00002546
Iteration 361/1000 | Loss: 0.00002545
Iteration 362/1000 | Loss: 0.00002545
Iteration 363/1000 | Loss: 0.00002545
Iteration 364/1000 | Loss: 0.00002545
Iteration 365/1000 | Loss: 0.00002545
Iteration 366/1000 | Loss: 0.00002544
Iteration 367/1000 | Loss: 0.00002544
Iteration 368/1000 | Loss: 0.00002544
Iteration 369/1000 | Loss: 0.00002544
Iteration 370/1000 | Loss: 0.00002544
Iteration 371/1000 | Loss: 0.00002544
Iteration 372/1000 | Loss: 0.00002544
Iteration 373/1000 | Loss: 0.00002543
Iteration 374/1000 | Loss: 0.00002543
Iteration 375/1000 | Loss: 0.00002543
Iteration 376/1000 | Loss: 0.00002543
Iteration 377/1000 | Loss: 0.00002542
Iteration 378/1000 | Loss: 0.00002542
Iteration 379/1000 | Loss: 0.00002542
Iteration 380/1000 | Loss: 0.00002542
Iteration 381/1000 | Loss: 0.00002542
Iteration 382/1000 | Loss: 0.00002541
Iteration 383/1000 | Loss: 0.00002541
Iteration 384/1000 | Loss: 0.00002541
Iteration 385/1000 | Loss: 0.00002541
Iteration 386/1000 | Loss: 0.00002541
Iteration 387/1000 | Loss: 0.00002541
Iteration 388/1000 | Loss: 0.00002541
Iteration 389/1000 | Loss: 0.00002541
Iteration 390/1000 | Loss: 0.00002541
Iteration 391/1000 | Loss: 0.00002541
Iteration 392/1000 | Loss: 0.00002541
Iteration 393/1000 | Loss: 0.00002541
Iteration 394/1000 | Loss: 0.00002540
Iteration 395/1000 | Loss: 0.00002540
Iteration 396/1000 | Loss: 0.00002540
Iteration 397/1000 | Loss: 0.00002540
Iteration 398/1000 | Loss: 0.00002540
Iteration 399/1000 | Loss: 0.00002540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 399. Stopping optimization.
Last 5 losses: [2.5404886400792748e-05, 2.5404886400792748e-05, 2.5404886400792748e-05, 2.5404886400792748e-05, 2.5404886400792748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5404886400792748e-05

Optimization complete. Final v2v error: 4.059459209442139 mm

Highest mean error: 5.847576141357422 mm for frame 181

Lowest mean error: 3.2460920810699463 mm for frame 54

Saving results

Total time: 455.76794266700745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771469
Iteration 2/25 | Loss: 0.00163826
Iteration 3/25 | Loss: 0.00133339
Iteration 4/25 | Loss: 0.00129685
Iteration 5/25 | Loss: 0.00127540
Iteration 6/25 | Loss: 0.00127178
Iteration 7/25 | Loss: 0.00127736
Iteration 8/25 | Loss: 0.00128031
Iteration 9/25 | Loss: 0.00126790
Iteration 10/25 | Loss: 0.00126592
Iteration 11/25 | Loss: 0.00126309
Iteration 12/25 | Loss: 0.00124580
Iteration 13/25 | Loss: 0.00123959
Iteration 14/25 | Loss: 0.00123295
Iteration 15/25 | Loss: 0.00123589
Iteration 16/25 | Loss: 0.00123472
Iteration 17/25 | Loss: 0.00123886
Iteration 18/25 | Loss: 0.00123116
Iteration 19/25 | Loss: 0.00122928
Iteration 20/25 | Loss: 0.00122601
Iteration 21/25 | Loss: 0.00122403
Iteration 22/25 | Loss: 0.00122378
Iteration 23/25 | Loss: 0.00122386
Iteration 24/25 | Loss: 0.00122352
Iteration 25/25 | Loss: 0.00122827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33336568
Iteration 2/25 | Loss: 0.00075699
Iteration 3/25 | Loss: 0.00075699
Iteration 4/25 | Loss: 0.00075699
Iteration 5/25 | Loss: 0.00075699
Iteration 6/25 | Loss: 0.00075699
Iteration 7/25 | Loss: 0.00075699
Iteration 8/25 | Loss: 0.00075699
Iteration 9/25 | Loss: 0.00075699
Iteration 10/25 | Loss: 0.00075699
Iteration 11/25 | Loss: 0.00075699
Iteration 12/25 | Loss: 0.00075699
Iteration 13/25 | Loss: 0.00075699
Iteration 14/25 | Loss: 0.00075699
Iteration 15/25 | Loss: 0.00075699
Iteration 16/25 | Loss: 0.00075699
Iteration 17/25 | Loss: 0.00075699
Iteration 18/25 | Loss: 0.00075699
Iteration 19/25 | Loss: 0.00075699
Iteration 20/25 | Loss: 0.00075699
Iteration 21/25 | Loss: 0.00075699
Iteration 22/25 | Loss: 0.00075699
Iteration 23/25 | Loss: 0.00075699
Iteration 24/25 | Loss: 0.00075699
Iteration 25/25 | Loss: 0.00075699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075699
Iteration 2/1000 | Loss: 0.00004921
Iteration 3/1000 | Loss: 0.00003167
Iteration 4/1000 | Loss: 0.00002912
Iteration 5/1000 | Loss: 0.00002751
Iteration 6/1000 | Loss: 0.00002648
Iteration 7/1000 | Loss: 0.00002584
Iteration 8/1000 | Loss: 0.00002535
Iteration 9/1000 | Loss: 0.00002493
Iteration 10/1000 | Loss: 0.00002449
Iteration 11/1000 | Loss: 0.00002445
Iteration 12/1000 | Loss: 0.00002418
Iteration 13/1000 | Loss: 0.00002396
Iteration 14/1000 | Loss: 0.00002374
Iteration 15/1000 | Loss: 0.00002369
Iteration 16/1000 | Loss: 0.00002349
Iteration 17/1000 | Loss: 0.00002345
Iteration 18/1000 | Loss: 0.00002335
Iteration 19/1000 | Loss: 0.00002331
Iteration 20/1000 | Loss: 0.00002318
Iteration 21/1000 | Loss: 0.00002314
Iteration 22/1000 | Loss: 0.00002314
Iteration 23/1000 | Loss: 0.00002313
Iteration 24/1000 | Loss: 0.00002313
Iteration 25/1000 | Loss: 0.00002311
Iteration 26/1000 | Loss: 0.00019451
Iteration 27/1000 | Loss: 0.00012450
Iteration 28/1000 | Loss: 0.00019014
Iteration 29/1000 | Loss: 0.00002986
Iteration 30/1000 | Loss: 0.00002659
Iteration 31/1000 | Loss: 0.00002493
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002406
Iteration 34/1000 | Loss: 0.00002402
Iteration 35/1000 | Loss: 0.00002401
Iteration 36/1000 | Loss: 0.00002401
Iteration 37/1000 | Loss: 0.00002400
Iteration 38/1000 | Loss: 0.00002398
Iteration 39/1000 | Loss: 0.00002392
Iteration 40/1000 | Loss: 0.00002391
Iteration 41/1000 | Loss: 0.00002389
Iteration 42/1000 | Loss: 0.00002389
Iteration 43/1000 | Loss: 0.00002389
Iteration 44/1000 | Loss: 0.00002388
Iteration 45/1000 | Loss: 0.00002388
Iteration 46/1000 | Loss: 0.00002388
Iteration 47/1000 | Loss: 0.00002388
Iteration 48/1000 | Loss: 0.00002388
Iteration 49/1000 | Loss: 0.00002388
Iteration 50/1000 | Loss: 0.00002388
Iteration 51/1000 | Loss: 0.00002388
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002388
Iteration 55/1000 | Loss: 0.00002387
Iteration 56/1000 | Loss: 0.00002387
Iteration 57/1000 | Loss: 0.00002387
Iteration 58/1000 | Loss: 0.00002387
Iteration 59/1000 | Loss: 0.00002387
Iteration 60/1000 | Loss: 0.00002387
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002386
Iteration 64/1000 | Loss: 0.00002385
Iteration 65/1000 | Loss: 0.00002384
Iteration 66/1000 | Loss: 0.00002384
Iteration 67/1000 | Loss: 0.00002384
Iteration 68/1000 | Loss: 0.00002384
Iteration 69/1000 | Loss: 0.00002383
Iteration 70/1000 | Loss: 0.00002383
Iteration 71/1000 | Loss: 0.00002383
Iteration 72/1000 | Loss: 0.00002382
Iteration 73/1000 | Loss: 0.00002382
Iteration 74/1000 | Loss: 0.00002377
Iteration 75/1000 | Loss: 0.00002371
Iteration 76/1000 | Loss: 0.00002368
Iteration 77/1000 | Loss: 0.00002367
Iteration 78/1000 | Loss: 0.00002366
Iteration 79/1000 | Loss: 0.00002366
Iteration 80/1000 | Loss: 0.00002366
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002365
Iteration 83/1000 | Loss: 0.00002365
Iteration 84/1000 | Loss: 0.00002365
Iteration 85/1000 | Loss: 0.00002364
Iteration 86/1000 | Loss: 0.00002364
Iteration 87/1000 | Loss: 0.00002364
Iteration 88/1000 | Loss: 0.00002363
Iteration 89/1000 | Loss: 0.00002363
Iteration 90/1000 | Loss: 0.00002363
Iteration 91/1000 | Loss: 0.00002362
Iteration 92/1000 | Loss: 0.00002362
Iteration 93/1000 | Loss: 0.00002362
Iteration 94/1000 | Loss: 0.00002361
Iteration 95/1000 | Loss: 0.00002361
Iteration 96/1000 | Loss: 0.00002361
Iteration 97/1000 | Loss: 0.00002360
Iteration 98/1000 | Loss: 0.00002360
Iteration 99/1000 | Loss: 0.00002360
Iteration 100/1000 | Loss: 0.00002359
Iteration 101/1000 | Loss: 0.00002359
Iteration 102/1000 | Loss: 0.00002359
Iteration 103/1000 | Loss: 0.00002358
Iteration 104/1000 | Loss: 0.00002358
Iteration 105/1000 | Loss: 0.00002358
Iteration 106/1000 | Loss: 0.00002358
Iteration 107/1000 | Loss: 0.00002358
Iteration 108/1000 | Loss: 0.00002358
Iteration 109/1000 | Loss: 0.00002358
Iteration 110/1000 | Loss: 0.00002358
Iteration 111/1000 | Loss: 0.00002357
Iteration 112/1000 | Loss: 0.00002357
Iteration 113/1000 | Loss: 0.00002357
Iteration 114/1000 | Loss: 0.00002357
Iteration 115/1000 | Loss: 0.00002357
Iteration 116/1000 | Loss: 0.00002357
Iteration 117/1000 | Loss: 0.00002357
Iteration 118/1000 | Loss: 0.00002357
Iteration 119/1000 | Loss: 0.00002357
Iteration 120/1000 | Loss: 0.00002357
Iteration 121/1000 | Loss: 0.00002357
Iteration 122/1000 | Loss: 0.00002357
Iteration 123/1000 | Loss: 0.00002357
Iteration 124/1000 | Loss: 0.00002357
Iteration 125/1000 | Loss: 0.00002356
Iteration 126/1000 | Loss: 0.00002356
Iteration 127/1000 | Loss: 0.00002356
Iteration 128/1000 | Loss: 0.00002356
Iteration 129/1000 | Loss: 0.00002356
Iteration 130/1000 | Loss: 0.00002356
Iteration 131/1000 | Loss: 0.00002356
Iteration 132/1000 | Loss: 0.00002356
Iteration 133/1000 | Loss: 0.00002356
Iteration 134/1000 | Loss: 0.00002356
Iteration 135/1000 | Loss: 0.00002356
Iteration 136/1000 | Loss: 0.00002356
Iteration 137/1000 | Loss: 0.00002356
Iteration 138/1000 | Loss: 0.00002356
Iteration 139/1000 | Loss: 0.00002355
Iteration 140/1000 | Loss: 0.00002355
Iteration 141/1000 | Loss: 0.00002355
Iteration 142/1000 | Loss: 0.00002355
Iteration 143/1000 | Loss: 0.00002355
Iteration 144/1000 | Loss: 0.00002355
Iteration 145/1000 | Loss: 0.00002355
Iteration 146/1000 | Loss: 0.00002355
Iteration 147/1000 | Loss: 0.00002355
Iteration 148/1000 | Loss: 0.00002355
Iteration 149/1000 | Loss: 0.00002355
Iteration 150/1000 | Loss: 0.00002354
Iteration 151/1000 | Loss: 0.00002354
Iteration 152/1000 | Loss: 0.00002354
Iteration 153/1000 | Loss: 0.00002354
Iteration 154/1000 | Loss: 0.00002354
Iteration 155/1000 | Loss: 0.00002354
Iteration 156/1000 | Loss: 0.00002354
Iteration 157/1000 | Loss: 0.00002354
Iteration 158/1000 | Loss: 0.00002354
Iteration 159/1000 | Loss: 0.00002354
Iteration 160/1000 | Loss: 0.00002354
Iteration 161/1000 | Loss: 0.00002354
Iteration 162/1000 | Loss: 0.00002353
Iteration 163/1000 | Loss: 0.00002353
Iteration 164/1000 | Loss: 0.00002353
Iteration 165/1000 | Loss: 0.00002353
Iteration 166/1000 | Loss: 0.00002353
Iteration 167/1000 | Loss: 0.00002353
Iteration 168/1000 | Loss: 0.00002353
Iteration 169/1000 | Loss: 0.00002353
Iteration 170/1000 | Loss: 0.00002353
Iteration 171/1000 | Loss: 0.00002353
Iteration 172/1000 | Loss: 0.00002353
Iteration 173/1000 | Loss: 0.00002353
Iteration 174/1000 | Loss: 0.00002353
Iteration 175/1000 | Loss: 0.00002353
Iteration 176/1000 | Loss: 0.00002353
Iteration 177/1000 | Loss: 0.00002353
Iteration 178/1000 | Loss: 0.00002352
Iteration 179/1000 | Loss: 0.00002352
Iteration 180/1000 | Loss: 0.00002352
Iteration 181/1000 | Loss: 0.00002352
Iteration 182/1000 | Loss: 0.00002352
Iteration 183/1000 | Loss: 0.00002352
Iteration 184/1000 | Loss: 0.00002352
Iteration 185/1000 | Loss: 0.00002352
Iteration 186/1000 | Loss: 0.00002352
Iteration 187/1000 | Loss: 0.00002352
Iteration 188/1000 | Loss: 0.00002352
Iteration 189/1000 | Loss: 0.00002352
Iteration 190/1000 | Loss: 0.00002352
Iteration 191/1000 | Loss: 0.00002352
Iteration 192/1000 | Loss: 0.00002352
Iteration 193/1000 | Loss: 0.00002352
Iteration 194/1000 | Loss: 0.00002352
Iteration 195/1000 | Loss: 0.00002351
Iteration 196/1000 | Loss: 0.00002351
Iteration 197/1000 | Loss: 0.00002351
Iteration 198/1000 | Loss: 0.00002351
Iteration 199/1000 | Loss: 0.00002351
Iteration 200/1000 | Loss: 0.00002351
Iteration 201/1000 | Loss: 0.00002351
Iteration 202/1000 | Loss: 0.00002350
Iteration 203/1000 | Loss: 0.00002350
Iteration 204/1000 | Loss: 0.00002350
Iteration 205/1000 | Loss: 0.00002350
Iteration 206/1000 | Loss: 0.00002350
Iteration 207/1000 | Loss: 0.00002350
Iteration 208/1000 | Loss: 0.00002350
Iteration 209/1000 | Loss: 0.00002350
Iteration 210/1000 | Loss: 0.00002350
Iteration 211/1000 | Loss: 0.00002350
Iteration 212/1000 | Loss: 0.00002350
Iteration 213/1000 | Loss: 0.00002350
Iteration 214/1000 | Loss: 0.00002350
Iteration 215/1000 | Loss: 0.00002350
Iteration 216/1000 | Loss: 0.00002350
Iteration 217/1000 | Loss: 0.00002350
Iteration 218/1000 | Loss: 0.00002350
Iteration 219/1000 | Loss: 0.00002350
Iteration 220/1000 | Loss: 0.00002350
Iteration 221/1000 | Loss: 0.00002350
Iteration 222/1000 | Loss: 0.00002350
Iteration 223/1000 | Loss: 0.00002350
Iteration 224/1000 | Loss: 0.00002350
Iteration 225/1000 | Loss: 0.00002350
Iteration 226/1000 | Loss: 0.00002350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.3500891984440386e-05, 2.3500891984440386e-05, 2.3500891984440386e-05, 2.3500891984440386e-05, 2.3500891984440386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3500891984440386e-05

Optimization complete. Final v2v error: 3.851696252822876 mm

Highest mean error: 10.412137985229492 mm for frame 94

Lowest mean error: 3.1366279125213623 mm for frame 188

Saving results

Total time: 109.31505584716797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740557
Iteration 2/25 | Loss: 0.00122804
Iteration 3/25 | Loss: 0.00114544
Iteration 4/25 | Loss: 0.00113428
Iteration 5/25 | Loss: 0.00113137
Iteration 6/25 | Loss: 0.00113081
Iteration 7/25 | Loss: 0.00113081
Iteration 8/25 | Loss: 0.00113081
Iteration 9/25 | Loss: 0.00113081
Iteration 10/25 | Loss: 0.00113081
Iteration 11/25 | Loss: 0.00113081
Iteration 12/25 | Loss: 0.00113081
Iteration 13/25 | Loss: 0.00113081
Iteration 14/25 | Loss: 0.00113081
Iteration 15/25 | Loss: 0.00113081
Iteration 16/25 | Loss: 0.00113081
Iteration 17/25 | Loss: 0.00113081
Iteration 18/25 | Loss: 0.00113081
Iteration 19/25 | Loss: 0.00113081
Iteration 20/25 | Loss: 0.00113081
Iteration 21/25 | Loss: 0.00113081
Iteration 22/25 | Loss: 0.00113081
Iteration 23/25 | Loss: 0.00113081
Iteration 24/25 | Loss: 0.00113081
Iteration 25/25 | Loss: 0.00113081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34113812
Iteration 2/25 | Loss: 0.00069909
Iteration 3/25 | Loss: 0.00069901
Iteration 4/25 | Loss: 0.00069901
Iteration 5/25 | Loss: 0.00069901
Iteration 6/25 | Loss: 0.00069901
Iteration 7/25 | Loss: 0.00069901
Iteration 8/25 | Loss: 0.00069901
Iteration 9/25 | Loss: 0.00069901
Iteration 10/25 | Loss: 0.00069901
Iteration 11/25 | Loss: 0.00069901
Iteration 12/25 | Loss: 0.00069901
Iteration 13/25 | Loss: 0.00069901
Iteration 14/25 | Loss: 0.00069901
Iteration 15/25 | Loss: 0.00069901
Iteration 16/25 | Loss: 0.00069901
Iteration 17/25 | Loss: 0.00069901
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006990104448050261, 0.0006990104448050261, 0.0006990104448050261, 0.0006990104448050261, 0.0006990104448050261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006990104448050261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069901
Iteration 2/1000 | Loss: 0.00003254
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002084
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001932
Iteration 8/1000 | Loss: 0.00001877
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001807
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001783
Iteration 16/1000 | Loss: 0.00001782
Iteration 17/1000 | Loss: 0.00001759
Iteration 18/1000 | Loss: 0.00001742
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001719
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001708
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001707
Iteration 28/1000 | Loss: 0.00001707
Iteration 29/1000 | Loss: 0.00001706
Iteration 30/1000 | Loss: 0.00001706
Iteration 31/1000 | Loss: 0.00001705
Iteration 32/1000 | Loss: 0.00001705
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001704
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001702
Iteration 43/1000 | Loss: 0.00001701
Iteration 44/1000 | Loss: 0.00001700
Iteration 45/1000 | Loss: 0.00001700
Iteration 46/1000 | Loss: 0.00001700
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001699
Iteration 49/1000 | Loss: 0.00001699
Iteration 50/1000 | Loss: 0.00001699
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001698
Iteration 54/1000 | Loss: 0.00001698
Iteration 55/1000 | Loss: 0.00001697
Iteration 56/1000 | Loss: 0.00001697
Iteration 57/1000 | Loss: 0.00001697
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001693
Iteration 67/1000 | Loss: 0.00001693
Iteration 68/1000 | Loss: 0.00001693
Iteration 69/1000 | Loss: 0.00001693
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001690
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001690
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001689
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001689
Iteration 96/1000 | Loss: 0.00001689
Iteration 97/1000 | Loss: 0.00001689
Iteration 98/1000 | Loss: 0.00001689
Iteration 99/1000 | Loss: 0.00001689
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001687
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001685
Iteration 110/1000 | Loss: 0.00001685
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001683
Iteration 122/1000 | Loss: 0.00001683
Iteration 123/1000 | Loss: 0.00001683
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001681
Iteration 127/1000 | Loss: 0.00001681
Iteration 128/1000 | Loss: 0.00001681
Iteration 129/1000 | Loss: 0.00001681
Iteration 130/1000 | Loss: 0.00001681
Iteration 131/1000 | Loss: 0.00001681
Iteration 132/1000 | Loss: 0.00001681
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001679
Iteration 138/1000 | Loss: 0.00001679
Iteration 139/1000 | Loss: 0.00001679
Iteration 140/1000 | Loss: 0.00001679
Iteration 141/1000 | Loss: 0.00001678
Iteration 142/1000 | Loss: 0.00001678
Iteration 143/1000 | Loss: 0.00001678
Iteration 144/1000 | Loss: 0.00001678
Iteration 145/1000 | Loss: 0.00001678
Iteration 146/1000 | Loss: 0.00001678
Iteration 147/1000 | Loss: 0.00001678
Iteration 148/1000 | Loss: 0.00001678
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001677
Iteration 151/1000 | Loss: 0.00001677
Iteration 152/1000 | Loss: 0.00001677
Iteration 153/1000 | Loss: 0.00001677
Iteration 154/1000 | Loss: 0.00001677
Iteration 155/1000 | Loss: 0.00001677
Iteration 156/1000 | Loss: 0.00001677
Iteration 157/1000 | Loss: 0.00001677
Iteration 158/1000 | Loss: 0.00001677
Iteration 159/1000 | Loss: 0.00001677
Iteration 160/1000 | Loss: 0.00001677
Iteration 161/1000 | Loss: 0.00001677
Iteration 162/1000 | Loss: 0.00001677
Iteration 163/1000 | Loss: 0.00001676
Iteration 164/1000 | Loss: 0.00001676
Iteration 165/1000 | Loss: 0.00001676
Iteration 166/1000 | Loss: 0.00001676
Iteration 167/1000 | Loss: 0.00001676
Iteration 168/1000 | Loss: 0.00001676
Iteration 169/1000 | Loss: 0.00001676
Iteration 170/1000 | Loss: 0.00001676
Iteration 171/1000 | Loss: 0.00001676
Iteration 172/1000 | Loss: 0.00001676
Iteration 173/1000 | Loss: 0.00001676
Iteration 174/1000 | Loss: 0.00001676
Iteration 175/1000 | Loss: 0.00001676
Iteration 176/1000 | Loss: 0.00001676
Iteration 177/1000 | Loss: 0.00001676
Iteration 178/1000 | Loss: 0.00001676
Iteration 179/1000 | Loss: 0.00001676
Iteration 180/1000 | Loss: 0.00001676
Iteration 181/1000 | Loss: 0.00001676
Iteration 182/1000 | Loss: 0.00001676
Iteration 183/1000 | Loss: 0.00001676
Iteration 184/1000 | Loss: 0.00001675
Iteration 185/1000 | Loss: 0.00001675
Iteration 186/1000 | Loss: 0.00001675
Iteration 187/1000 | Loss: 0.00001675
Iteration 188/1000 | Loss: 0.00001675
Iteration 189/1000 | Loss: 0.00001675
Iteration 190/1000 | Loss: 0.00001675
Iteration 191/1000 | Loss: 0.00001675
Iteration 192/1000 | Loss: 0.00001675
Iteration 193/1000 | Loss: 0.00001675
Iteration 194/1000 | Loss: 0.00001674
Iteration 195/1000 | Loss: 0.00001674
Iteration 196/1000 | Loss: 0.00001674
Iteration 197/1000 | Loss: 0.00001674
Iteration 198/1000 | Loss: 0.00001674
Iteration 199/1000 | Loss: 0.00001674
Iteration 200/1000 | Loss: 0.00001674
Iteration 201/1000 | Loss: 0.00001673
Iteration 202/1000 | Loss: 0.00001673
Iteration 203/1000 | Loss: 0.00001673
Iteration 204/1000 | Loss: 0.00001673
Iteration 205/1000 | Loss: 0.00001673
Iteration 206/1000 | Loss: 0.00001673
Iteration 207/1000 | Loss: 0.00001673
Iteration 208/1000 | Loss: 0.00001673
Iteration 209/1000 | Loss: 0.00001673
Iteration 210/1000 | Loss: 0.00001673
Iteration 211/1000 | Loss: 0.00001673
Iteration 212/1000 | Loss: 0.00001673
Iteration 213/1000 | Loss: 0.00001673
Iteration 214/1000 | Loss: 0.00001673
Iteration 215/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.6731184587115422e-05, 1.6731184587115422e-05, 1.6731184587115422e-05, 1.6731184587115422e-05, 1.6731184587115422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6731184587115422e-05

Optimization complete. Final v2v error: 3.425079107284546 mm

Highest mean error: 3.6697616577148438 mm for frame 42

Lowest mean error: 3.279465436935425 mm for frame 9

Saving results

Total time: 44.256651878356934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030007
Iteration 2/25 | Loss: 0.00158536
Iteration 3/25 | Loss: 0.00140610
Iteration 4/25 | Loss: 0.00128965
Iteration 5/25 | Loss: 0.00128771
Iteration 6/25 | Loss: 0.00123766
Iteration 7/25 | Loss: 0.00119408
Iteration 8/25 | Loss: 0.00117281
Iteration 9/25 | Loss: 0.00115351
Iteration 10/25 | Loss: 0.00114449
Iteration 11/25 | Loss: 0.00114708
Iteration 12/25 | Loss: 0.00114038
Iteration 13/25 | Loss: 0.00113103
Iteration 14/25 | Loss: 0.00113303
Iteration 15/25 | Loss: 0.00113358
Iteration 16/25 | Loss: 0.00113399
Iteration 17/25 | Loss: 0.00113380
Iteration 18/25 | Loss: 0.00113388
Iteration 19/25 | Loss: 0.00113613
Iteration 20/25 | Loss: 0.00113216
Iteration 21/25 | Loss: 0.00112909
Iteration 22/25 | Loss: 0.00114132
Iteration 23/25 | Loss: 0.00113502
Iteration 24/25 | Loss: 0.00112511
Iteration 25/25 | Loss: 0.00112103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38001561
Iteration 2/25 | Loss: 0.00152640
Iteration 3/25 | Loss: 0.00116396
Iteration 4/25 | Loss: 0.00116396
Iteration 5/25 | Loss: 0.00116396
Iteration 6/25 | Loss: 0.00116396
Iteration 7/25 | Loss: 0.00116396
Iteration 8/25 | Loss: 0.00116396
Iteration 9/25 | Loss: 0.00116396
Iteration 10/25 | Loss: 0.00116396
Iteration 11/25 | Loss: 0.00116396
Iteration 12/25 | Loss: 0.00116396
Iteration 13/25 | Loss: 0.00116396
Iteration 14/25 | Loss: 0.00116396
Iteration 15/25 | Loss: 0.00116396
Iteration 16/25 | Loss: 0.00116396
Iteration 17/25 | Loss: 0.00116396
Iteration 18/25 | Loss: 0.00116396
Iteration 19/25 | Loss: 0.00116396
Iteration 20/25 | Loss: 0.00116396
Iteration 21/25 | Loss: 0.00116396
Iteration 22/25 | Loss: 0.00116396
Iteration 23/25 | Loss: 0.00116396
Iteration 24/25 | Loss: 0.00116396
Iteration 25/25 | Loss: 0.00116396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116396
Iteration 2/1000 | Loss: 0.00029060
Iteration 3/1000 | Loss: 0.00017191
Iteration 4/1000 | Loss: 0.00016703
Iteration 5/1000 | Loss: 0.00022031
Iteration 6/1000 | Loss: 0.00008859
Iteration 7/1000 | Loss: 0.00017960
Iteration 8/1000 | Loss: 0.00027310
Iteration 9/1000 | Loss: 0.00023107
Iteration 10/1000 | Loss: 0.00016434
Iteration 11/1000 | Loss: 0.00021451
Iteration 12/1000 | Loss: 0.00019081
Iteration 13/1000 | Loss: 0.00020197
Iteration 14/1000 | Loss: 0.00014996
Iteration 15/1000 | Loss: 0.00030939
Iteration 16/1000 | Loss: 0.00004109
Iteration 17/1000 | Loss: 0.00027092
Iteration 18/1000 | Loss: 0.00005163
Iteration 19/1000 | Loss: 0.00003893
Iteration 20/1000 | Loss: 0.00003638
Iteration 21/1000 | Loss: 0.00004746
Iteration 22/1000 | Loss: 0.00004571
Iteration 23/1000 | Loss: 0.00003374
Iteration 24/1000 | Loss: 0.00004254
Iteration 25/1000 | Loss: 0.00004258
Iteration 26/1000 | Loss: 0.00004143
Iteration 27/1000 | Loss: 0.00003753
Iteration 28/1000 | Loss: 0.00004598
Iteration 29/1000 | Loss: 0.00004002
Iteration 30/1000 | Loss: 0.00003697
Iteration 31/1000 | Loss: 0.00004810
Iteration 32/1000 | Loss: 0.00005001
Iteration 33/1000 | Loss: 0.00005311
Iteration 34/1000 | Loss: 0.00004782
Iteration 35/1000 | Loss: 0.00005160
Iteration 36/1000 | Loss: 0.00027731
Iteration 37/1000 | Loss: 0.00009040
Iteration 38/1000 | Loss: 0.00109905
Iteration 39/1000 | Loss: 0.00051283
Iteration 40/1000 | Loss: 0.00077612
Iteration 41/1000 | Loss: 0.00092568
Iteration 42/1000 | Loss: 0.00025466
Iteration 43/1000 | Loss: 0.00045113
Iteration 44/1000 | Loss: 0.00035064
Iteration 45/1000 | Loss: 0.00045600
Iteration 46/1000 | Loss: 0.00042116
Iteration 47/1000 | Loss: 0.00045889
Iteration 48/1000 | Loss: 0.00053559
Iteration 49/1000 | Loss: 0.00022777
Iteration 50/1000 | Loss: 0.00018002
Iteration 51/1000 | Loss: 0.00011804
Iteration 52/1000 | Loss: 0.00018542
Iteration 53/1000 | Loss: 0.00016584
Iteration 54/1000 | Loss: 0.00004395
Iteration 55/1000 | Loss: 0.00003643
Iteration 56/1000 | Loss: 0.00005942
Iteration 57/1000 | Loss: 0.00003343
Iteration 58/1000 | Loss: 0.00004637
Iteration 59/1000 | Loss: 0.00004648
Iteration 60/1000 | Loss: 0.00004182
Iteration 61/1000 | Loss: 0.00003063
Iteration 62/1000 | Loss: 0.00002668
Iteration 63/1000 | Loss: 0.00003999
Iteration 64/1000 | Loss: 0.00004149
Iteration 65/1000 | Loss: 0.00004167
Iteration 66/1000 | Loss: 0.00003162
Iteration 67/1000 | Loss: 0.00011275
Iteration 68/1000 | Loss: 0.00013507
Iteration 69/1000 | Loss: 0.00049505
Iteration 70/1000 | Loss: 0.00004718
Iteration 71/1000 | Loss: 0.00041653
Iteration 72/1000 | Loss: 0.00028019
Iteration 73/1000 | Loss: 0.00021783
Iteration 74/1000 | Loss: 0.00009621
Iteration 75/1000 | Loss: 0.00007531
Iteration 76/1000 | Loss: 0.00004627
Iteration 77/1000 | Loss: 0.00009077
Iteration 78/1000 | Loss: 0.00051434
Iteration 79/1000 | Loss: 0.00021471
Iteration 80/1000 | Loss: 0.00035999
Iteration 81/1000 | Loss: 0.00005944
Iteration 82/1000 | Loss: 0.00004345
Iteration 83/1000 | Loss: 0.00038881
Iteration 84/1000 | Loss: 0.00052301
Iteration 85/1000 | Loss: 0.00028981
Iteration 86/1000 | Loss: 0.00011500
Iteration 87/1000 | Loss: 0.00003095
Iteration 88/1000 | Loss: 0.00002246
Iteration 89/1000 | Loss: 0.00001941
Iteration 90/1000 | Loss: 0.00002402
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001513
Iteration 96/1000 | Loss: 0.00001452
Iteration 97/1000 | Loss: 0.00019401
Iteration 98/1000 | Loss: 0.00001639
Iteration 99/1000 | Loss: 0.00002738
Iteration 100/1000 | Loss: 0.00002037
Iteration 101/1000 | Loss: 0.00001778
Iteration 102/1000 | Loss: 0.00001463
Iteration 103/1000 | Loss: 0.00001162
Iteration 104/1000 | Loss: 0.00001090
Iteration 105/1000 | Loss: 0.00001027
Iteration 106/1000 | Loss: 0.00001000
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000987
Iteration 109/1000 | Loss: 0.00000984
Iteration 110/1000 | Loss: 0.00000983
Iteration 111/1000 | Loss: 0.00000983
Iteration 112/1000 | Loss: 0.00000983
Iteration 113/1000 | Loss: 0.00000982
Iteration 114/1000 | Loss: 0.00000982
Iteration 115/1000 | Loss: 0.00000982
Iteration 116/1000 | Loss: 0.00000981
Iteration 117/1000 | Loss: 0.00000981
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000980
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000978
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000978
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000975
Iteration 128/1000 | Loss: 0.00000974
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000973
Iteration 131/1000 | Loss: 0.00000972
Iteration 132/1000 | Loss: 0.00000972
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000970
Iteration 136/1000 | Loss: 0.00000969
Iteration 137/1000 | Loss: 0.00000969
Iteration 138/1000 | Loss: 0.00000969
Iteration 139/1000 | Loss: 0.00000969
Iteration 140/1000 | Loss: 0.00000968
Iteration 141/1000 | Loss: 0.00000968
Iteration 142/1000 | Loss: 0.00000968
Iteration 143/1000 | Loss: 0.00000968
Iteration 144/1000 | Loss: 0.00000968
Iteration 145/1000 | Loss: 0.00000968
Iteration 146/1000 | Loss: 0.00000967
Iteration 147/1000 | Loss: 0.00000967
Iteration 148/1000 | Loss: 0.00000967
Iteration 149/1000 | Loss: 0.00000967
Iteration 150/1000 | Loss: 0.00000967
Iteration 151/1000 | Loss: 0.00000966
Iteration 152/1000 | Loss: 0.00000966
Iteration 153/1000 | Loss: 0.00000966
Iteration 154/1000 | Loss: 0.00000966
Iteration 155/1000 | Loss: 0.00000966
Iteration 156/1000 | Loss: 0.00000966
Iteration 157/1000 | Loss: 0.00000966
Iteration 158/1000 | Loss: 0.00000966
Iteration 159/1000 | Loss: 0.00000965
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000965
Iteration 162/1000 | Loss: 0.00000965
Iteration 163/1000 | Loss: 0.00000965
Iteration 164/1000 | Loss: 0.00000964
Iteration 165/1000 | Loss: 0.00000964
Iteration 166/1000 | Loss: 0.00000963
Iteration 167/1000 | Loss: 0.00000963
Iteration 168/1000 | Loss: 0.00000963
Iteration 169/1000 | Loss: 0.00000963
Iteration 170/1000 | Loss: 0.00000962
Iteration 171/1000 | Loss: 0.00000962
Iteration 172/1000 | Loss: 0.00000961
Iteration 173/1000 | Loss: 0.00000961
Iteration 174/1000 | Loss: 0.00000961
Iteration 175/1000 | Loss: 0.00000960
Iteration 176/1000 | Loss: 0.00000960
Iteration 177/1000 | Loss: 0.00000960
Iteration 178/1000 | Loss: 0.00000960
Iteration 179/1000 | Loss: 0.00000960
Iteration 180/1000 | Loss: 0.00000960
Iteration 181/1000 | Loss: 0.00000959
Iteration 182/1000 | Loss: 0.00000959
Iteration 183/1000 | Loss: 0.00000959
Iteration 184/1000 | Loss: 0.00000958
Iteration 185/1000 | Loss: 0.00000958
Iteration 186/1000 | Loss: 0.00000958
Iteration 187/1000 | Loss: 0.00000958
Iteration 188/1000 | Loss: 0.00000958
Iteration 189/1000 | Loss: 0.00000958
Iteration 190/1000 | Loss: 0.00000958
Iteration 191/1000 | Loss: 0.00000957
Iteration 192/1000 | Loss: 0.00000957
Iteration 193/1000 | Loss: 0.00000957
Iteration 194/1000 | Loss: 0.00000957
Iteration 195/1000 | Loss: 0.00000957
Iteration 196/1000 | Loss: 0.00000957
Iteration 197/1000 | Loss: 0.00000957
Iteration 198/1000 | Loss: 0.00000956
Iteration 199/1000 | Loss: 0.00000956
Iteration 200/1000 | Loss: 0.00000956
Iteration 201/1000 | Loss: 0.00000956
Iteration 202/1000 | Loss: 0.00000956
Iteration 203/1000 | Loss: 0.00000956
Iteration 204/1000 | Loss: 0.00000955
Iteration 205/1000 | Loss: 0.00000955
Iteration 206/1000 | Loss: 0.00000955
Iteration 207/1000 | Loss: 0.00000955
Iteration 208/1000 | Loss: 0.00000955
Iteration 209/1000 | Loss: 0.00000955
Iteration 210/1000 | Loss: 0.00000954
Iteration 211/1000 | Loss: 0.00000954
Iteration 212/1000 | Loss: 0.00000954
Iteration 213/1000 | Loss: 0.00000954
Iteration 214/1000 | Loss: 0.00000954
Iteration 215/1000 | Loss: 0.00000954
Iteration 216/1000 | Loss: 0.00000953
Iteration 217/1000 | Loss: 0.00000953
Iteration 218/1000 | Loss: 0.00000953
Iteration 219/1000 | Loss: 0.00000953
Iteration 220/1000 | Loss: 0.00000953
Iteration 221/1000 | Loss: 0.00000953
Iteration 222/1000 | Loss: 0.00000953
Iteration 223/1000 | Loss: 0.00000953
Iteration 224/1000 | Loss: 0.00000953
Iteration 225/1000 | Loss: 0.00000953
Iteration 226/1000 | Loss: 0.00000953
Iteration 227/1000 | Loss: 0.00000953
Iteration 228/1000 | Loss: 0.00000953
Iteration 229/1000 | Loss: 0.00000953
Iteration 230/1000 | Loss: 0.00000953
Iteration 231/1000 | Loss: 0.00000953
Iteration 232/1000 | Loss: 0.00000953
Iteration 233/1000 | Loss: 0.00000953
Iteration 234/1000 | Loss: 0.00000953
Iteration 235/1000 | Loss: 0.00000952
Iteration 236/1000 | Loss: 0.00000952
Iteration 237/1000 | Loss: 0.00000952
Iteration 238/1000 | Loss: 0.00000952
Iteration 239/1000 | Loss: 0.00000952
Iteration 240/1000 | Loss: 0.00000952
Iteration 241/1000 | Loss: 0.00000952
Iteration 242/1000 | Loss: 0.00000952
Iteration 243/1000 | Loss: 0.00000952
Iteration 244/1000 | Loss: 0.00000951
Iteration 245/1000 | Loss: 0.00000951
Iteration 246/1000 | Loss: 0.00000951
Iteration 247/1000 | Loss: 0.00000951
Iteration 248/1000 | Loss: 0.00000951
Iteration 249/1000 | Loss: 0.00000951
Iteration 250/1000 | Loss: 0.00000951
Iteration 251/1000 | Loss: 0.00000951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [9.513837539998349e-06, 9.513837539998349e-06, 9.513837539998349e-06, 9.513837539998349e-06, 9.513837539998349e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.513837539998349e-06

Optimization complete. Final v2v error: 2.620889186859131 mm

Highest mean error: 3.6917736530303955 mm for frame 67

Lowest mean error: 2.4145326614379883 mm for frame 39

Saving results

Total time: 205.5526852607727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_014/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_014/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018628
Iteration 2/25 | Loss: 0.00187349
Iteration 3/25 | Loss: 0.00144235
Iteration 4/25 | Loss: 0.00134924
Iteration 5/25 | Loss: 0.00136948
Iteration 6/25 | Loss: 0.00135141
Iteration 7/25 | Loss: 0.00134522
Iteration 8/25 | Loss: 0.00128895
Iteration 9/25 | Loss: 0.00129029
Iteration 10/25 | Loss: 0.00124005
Iteration 11/25 | Loss: 0.00122437
Iteration 12/25 | Loss: 0.00121478
Iteration 13/25 | Loss: 0.00120604
Iteration 14/25 | Loss: 0.00121140
Iteration 15/25 | Loss: 0.00117946
Iteration 16/25 | Loss: 0.00116934
Iteration 17/25 | Loss: 0.00115863
Iteration 18/25 | Loss: 0.00114771
Iteration 19/25 | Loss: 0.00114293
Iteration 20/25 | Loss: 0.00114649
Iteration 21/25 | Loss: 0.00114723
Iteration 22/25 | Loss: 0.00114403
Iteration 23/25 | Loss: 0.00114027
Iteration 24/25 | Loss: 0.00114135
Iteration 25/25 | Loss: 0.00113789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40977514
Iteration 2/25 | Loss: 0.00124278
Iteration 3/25 | Loss: 0.00119975
Iteration 4/25 | Loss: 0.00119975
Iteration 5/25 | Loss: 0.00119975
Iteration 6/25 | Loss: 0.00119975
Iteration 7/25 | Loss: 0.00119975
Iteration 8/25 | Loss: 0.00119975
Iteration 9/25 | Loss: 0.00119975
Iteration 10/25 | Loss: 0.00119974
Iteration 11/25 | Loss: 0.00119974
Iteration 12/25 | Loss: 0.00119974
Iteration 13/25 | Loss: 0.00119974
Iteration 14/25 | Loss: 0.00119974
Iteration 15/25 | Loss: 0.00119974
Iteration 16/25 | Loss: 0.00119974
Iteration 17/25 | Loss: 0.00119974
Iteration 18/25 | Loss: 0.00119974
Iteration 19/25 | Loss: 0.00119974
Iteration 20/25 | Loss: 0.00119974
Iteration 21/25 | Loss: 0.00119974
Iteration 22/25 | Loss: 0.00119974
Iteration 23/25 | Loss: 0.00119974
Iteration 24/25 | Loss: 0.00119974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011997442925348878, 0.0011997442925348878, 0.0011997442925348878, 0.0011997442925348878, 0.0011997442925348878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011997442925348878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119974
Iteration 2/1000 | Loss: 0.00068298
Iteration 3/1000 | Loss: 0.00042915
Iteration 4/1000 | Loss: 0.00009388
Iteration 5/1000 | Loss: 0.00033261
Iteration 6/1000 | Loss: 0.00037598
Iteration 7/1000 | Loss: 0.00014215
Iteration 8/1000 | Loss: 0.00033415
Iteration 9/1000 | Loss: 0.00032853
Iteration 10/1000 | Loss: 0.00022822
Iteration 11/1000 | Loss: 0.00021009
Iteration 12/1000 | Loss: 0.00032235
Iteration 13/1000 | Loss: 0.00012839
Iteration 14/1000 | Loss: 0.00002904
Iteration 15/1000 | Loss: 0.00014848
Iteration 16/1000 | Loss: 0.00048166
Iteration 17/1000 | Loss: 0.00050083
Iteration 18/1000 | Loss: 0.00004856
Iteration 19/1000 | Loss: 0.00004462
Iteration 20/1000 | Loss: 0.00006034
Iteration 21/1000 | Loss: 0.00005063
Iteration 22/1000 | Loss: 0.00005262
Iteration 23/1000 | Loss: 0.00024118
Iteration 24/1000 | Loss: 0.00016367
Iteration 25/1000 | Loss: 0.00024477
Iteration 26/1000 | Loss: 0.00028130
Iteration 27/1000 | Loss: 0.00019379
Iteration 28/1000 | Loss: 0.00003934
Iteration 29/1000 | Loss: 0.00003507
Iteration 30/1000 | Loss: 0.00001976
Iteration 31/1000 | Loss: 0.00003144
Iteration 32/1000 | Loss: 0.00002469
Iteration 33/1000 | Loss: 0.00001884
Iteration 34/1000 | Loss: 0.00010554
Iteration 35/1000 | Loss: 0.00013710
Iteration 36/1000 | Loss: 0.00029821
Iteration 37/1000 | Loss: 0.00027643
Iteration 38/1000 | Loss: 0.00026295
Iteration 39/1000 | Loss: 0.00018808
Iteration 40/1000 | Loss: 0.00021890
Iteration 41/1000 | Loss: 0.00030952
Iteration 42/1000 | Loss: 0.00028974
Iteration 43/1000 | Loss: 0.00021409
Iteration 44/1000 | Loss: 0.00032595
Iteration 45/1000 | Loss: 0.00017385
Iteration 46/1000 | Loss: 0.00006160
Iteration 47/1000 | Loss: 0.00006585
Iteration 48/1000 | Loss: 0.00002111
Iteration 49/1000 | Loss: 0.00003226
Iteration 50/1000 | Loss: 0.00003283
Iteration 51/1000 | Loss: 0.00003090
Iteration 52/1000 | Loss: 0.00002646
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00003203
Iteration 56/1000 | Loss: 0.00002965
Iteration 57/1000 | Loss: 0.00003132
Iteration 58/1000 | Loss: 0.00008138
Iteration 59/1000 | Loss: 0.00004439
Iteration 60/1000 | Loss: 0.00002938
Iteration 61/1000 | Loss: 0.00003214
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00003883
Iteration 64/1000 | Loss: 0.00003237
Iteration 65/1000 | Loss: 0.00003302
Iteration 66/1000 | Loss: 0.00003147
Iteration 67/1000 | Loss: 0.00003438
Iteration 68/1000 | Loss: 0.00003178
Iteration 69/1000 | Loss: 0.00003089
Iteration 70/1000 | Loss: 0.00003147
Iteration 71/1000 | Loss: 0.00003064
Iteration 72/1000 | Loss: 0.00003131
Iteration 73/1000 | Loss: 0.00003397
Iteration 74/1000 | Loss: 0.00003060
Iteration 75/1000 | Loss: 0.00003164
Iteration 76/1000 | Loss: 0.00003163
Iteration 77/1000 | Loss: 0.00002886
Iteration 78/1000 | Loss: 0.00003260
Iteration 79/1000 | Loss: 0.00002756
Iteration 80/1000 | Loss: 0.00009683
Iteration 81/1000 | Loss: 0.00002403
Iteration 82/1000 | Loss: 0.00002920
Iteration 83/1000 | Loss: 0.00002289
Iteration 84/1000 | Loss: 0.00003023
Iteration 85/1000 | Loss: 0.00003106
Iteration 86/1000 | Loss: 0.00005092
Iteration 87/1000 | Loss: 0.00003112
Iteration 88/1000 | Loss: 0.00004317
Iteration 89/1000 | Loss: 0.00003376
Iteration 90/1000 | Loss: 0.00003300
Iteration 91/1000 | Loss: 0.00003769
Iteration 92/1000 | Loss: 0.00002393
Iteration 93/1000 | Loss: 0.00003650
Iteration 94/1000 | Loss: 0.00003358
Iteration 95/1000 | Loss: 0.00003158
Iteration 96/1000 | Loss: 0.00002272
Iteration 97/1000 | Loss: 0.00003309
Iteration 98/1000 | Loss: 0.00002827
Iteration 99/1000 | Loss: 0.00003344
Iteration 100/1000 | Loss: 0.00002816
Iteration 101/1000 | Loss: 0.00003318
Iteration 102/1000 | Loss: 0.00008644
Iteration 103/1000 | Loss: 0.00003750
Iteration 104/1000 | Loss: 0.00002737
Iteration 105/1000 | Loss: 0.00002788
Iteration 106/1000 | Loss: 0.00002975
Iteration 107/1000 | Loss: 0.00003020
Iteration 108/1000 | Loss: 0.00002714
Iteration 109/1000 | Loss: 0.00004572
Iteration 110/1000 | Loss: 0.00003046
Iteration 111/1000 | Loss: 0.00002280
Iteration 112/1000 | Loss: 0.00003208
Iteration 113/1000 | Loss: 0.00003984
Iteration 114/1000 | Loss: 0.00003270
Iteration 115/1000 | Loss: 0.00003069
Iteration 116/1000 | Loss: 0.00003668
Iteration 117/1000 | Loss: 0.00003095
Iteration 118/1000 | Loss: 0.00003365
Iteration 119/1000 | Loss: 0.00003142
Iteration 120/1000 | Loss: 0.00003365
Iteration 121/1000 | Loss: 0.00003104
Iteration 122/1000 | Loss: 0.00003751
Iteration 123/1000 | Loss: 0.00002970
Iteration 124/1000 | Loss: 0.00003289
Iteration 125/1000 | Loss: 0.00003306
Iteration 126/1000 | Loss: 0.00003253
Iteration 127/1000 | Loss: 0.00003223
Iteration 128/1000 | Loss: 0.00003238
Iteration 129/1000 | Loss: 0.00006479
Iteration 130/1000 | Loss: 0.00025872
Iteration 131/1000 | Loss: 0.00002782
Iteration 132/1000 | Loss: 0.00002619
Iteration 133/1000 | Loss: 0.00002385
Iteration 134/1000 | Loss: 0.00003004
Iteration 135/1000 | Loss: 0.00007289
Iteration 136/1000 | Loss: 0.00003843
Iteration 137/1000 | Loss: 0.00003118
Iteration 138/1000 | Loss: 0.00003185
Iteration 139/1000 | Loss: 0.00003372
Iteration 140/1000 | Loss: 0.00003441
Iteration 141/1000 | Loss: 0.00002420
Iteration 142/1000 | Loss: 0.00002072
Iteration 143/1000 | Loss: 0.00002134
Iteration 144/1000 | Loss: 0.00004314
Iteration 145/1000 | Loss: 0.00002893
Iteration 146/1000 | Loss: 0.00002980
Iteration 147/1000 | Loss: 0.00007137
Iteration 148/1000 | Loss: 0.00003604
Iteration 149/1000 | Loss: 0.00002685
Iteration 150/1000 | Loss: 0.00003742
Iteration 151/1000 | Loss: 0.00003123
Iteration 152/1000 | Loss: 0.00003106
Iteration 153/1000 | Loss: 0.00005673
Iteration 154/1000 | Loss: 0.00003200
Iteration 155/1000 | Loss: 0.00002958
Iteration 156/1000 | Loss: 0.00003288
Iteration 157/1000 | Loss: 0.00003066
Iteration 158/1000 | Loss: 0.00003245
Iteration 159/1000 | Loss: 0.00004646
Iteration 160/1000 | Loss: 0.00002685
Iteration 161/1000 | Loss: 0.00002593
Iteration 162/1000 | Loss: 0.00003451
Iteration 163/1000 | Loss: 0.00003051
Iteration 164/1000 | Loss: 0.00003239
Iteration 165/1000 | Loss: 0.00003098
Iteration 166/1000 | Loss: 0.00003811
Iteration 167/1000 | Loss: 0.00004299
Iteration 168/1000 | Loss: 0.00003851
Iteration 169/1000 | Loss: 0.00003689
Iteration 170/1000 | Loss: 0.00006066
Iteration 171/1000 | Loss: 0.00003568
Iteration 172/1000 | Loss: 0.00005309
Iteration 173/1000 | Loss: 0.00005707
Iteration 174/1000 | Loss: 0.00005094
Iteration 175/1000 | Loss: 0.00004142
Iteration 176/1000 | Loss: 0.00004150
Iteration 177/1000 | Loss: 0.00002545
Iteration 178/1000 | Loss: 0.00002307
Iteration 179/1000 | Loss: 0.00004955
Iteration 180/1000 | Loss: 0.00017663
Iteration 181/1000 | Loss: 0.00017843
Iteration 182/1000 | Loss: 0.00001596
Iteration 183/1000 | Loss: 0.00003415
Iteration 184/1000 | Loss: 0.00002183
Iteration 185/1000 | Loss: 0.00001258
Iteration 186/1000 | Loss: 0.00001200
Iteration 187/1000 | Loss: 0.00002078
Iteration 188/1000 | Loss: 0.00001154
Iteration 189/1000 | Loss: 0.00001150
Iteration 190/1000 | Loss: 0.00001146
Iteration 191/1000 | Loss: 0.00001144
Iteration 192/1000 | Loss: 0.00001130
Iteration 193/1000 | Loss: 0.00001129
Iteration 194/1000 | Loss: 0.00001127
Iteration 195/1000 | Loss: 0.00001127
Iteration 196/1000 | Loss: 0.00001126
Iteration 197/1000 | Loss: 0.00001126
Iteration 198/1000 | Loss: 0.00001125
Iteration 199/1000 | Loss: 0.00001125
Iteration 200/1000 | Loss: 0.00001125
Iteration 201/1000 | Loss: 0.00001124
Iteration 202/1000 | Loss: 0.00001124
Iteration 203/1000 | Loss: 0.00001124
Iteration 204/1000 | Loss: 0.00001123
Iteration 205/1000 | Loss: 0.00001123
Iteration 206/1000 | Loss: 0.00001122
Iteration 207/1000 | Loss: 0.00001122
Iteration 208/1000 | Loss: 0.00001122
Iteration 209/1000 | Loss: 0.00001121
Iteration 210/1000 | Loss: 0.00001121
Iteration 211/1000 | Loss: 0.00001121
Iteration 212/1000 | Loss: 0.00001121
Iteration 213/1000 | Loss: 0.00001121
Iteration 214/1000 | Loss: 0.00001121
Iteration 215/1000 | Loss: 0.00001121
Iteration 216/1000 | Loss: 0.00001120
Iteration 217/1000 | Loss: 0.00001120
Iteration 218/1000 | Loss: 0.00001120
Iteration 219/1000 | Loss: 0.00001120
Iteration 220/1000 | Loss: 0.00001120
Iteration 221/1000 | Loss: 0.00001119
Iteration 222/1000 | Loss: 0.00001119
Iteration 223/1000 | Loss: 0.00001119
Iteration 224/1000 | Loss: 0.00001119
Iteration 225/1000 | Loss: 0.00001119
Iteration 226/1000 | Loss: 0.00001119
Iteration 227/1000 | Loss: 0.00001118
Iteration 228/1000 | Loss: 0.00001118
Iteration 229/1000 | Loss: 0.00001118
Iteration 230/1000 | Loss: 0.00001118
Iteration 231/1000 | Loss: 0.00001118
Iteration 232/1000 | Loss: 0.00001118
Iteration 233/1000 | Loss: 0.00001118
Iteration 234/1000 | Loss: 0.00001118
Iteration 235/1000 | Loss: 0.00001118
Iteration 236/1000 | Loss: 0.00001118
Iteration 237/1000 | Loss: 0.00001117
Iteration 238/1000 | Loss: 0.00001117
Iteration 239/1000 | Loss: 0.00001117
Iteration 240/1000 | Loss: 0.00001117
Iteration 241/1000 | Loss: 0.00001117
Iteration 242/1000 | Loss: 0.00007826
Iteration 243/1000 | Loss: 0.00001203
Iteration 244/1000 | Loss: 0.00001120
Iteration 245/1000 | Loss: 0.00001109
Iteration 246/1000 | Loss: 0.00001109
Iteration 247/1000 | Loss: 0.00001108
Iteration 248/1000 | Loss: 0.00001107
Iteration 249/1000 | Loss: 0.00001107
Iteration 250/1000 | Loss: 0.00001107
Iteration 251/1000 | Loss: 0.00001107
Iteration 252/1000 | Loss: 0.00001106
Iteration 253/1000 | Loss: 0.00001106
Iteration 254/1000 | Loss: 0.00001106
Iteration 255/1000 | Loss: 0.00001106
Iteration 256/1000 | Loss: 0.00001105
Iteration 257/1000 | Loss: 0.00001105
Iteration 258/1000 | Loss: 0.00001105
Iteration 259/1000 | Loss: 0.00001105
Iteration 260/1000 | Loss: 0.00001105
Iteration 261/1000 | Loss: 0.00001105
Iteration 262/1000 | Loss: 0.00001105
Iteration 263/1000 | Loss: 0.00001105
Iteration 264/1000 | Loss: 0.00001104
Iteration 265/1000 | Loss: 0.00001104
Iteration 266/1000 | Loss: 0.00001104
Iteration 267/1000 | Loss: 0.00001104
Iteration 268/1000 | Loss: 0.00001104
Iteration 269/1000 | Loss: 0.00001104
Iteration 270/1000 | Loss: 0.00001104
Iteration 271/1000 | Loss: 0.00001104
Iteration 272/1000 | Loss: 0.00001104
Iteration 273/1000 | Loss: 0.00001104
Iteration 274/1000 | Loss: 0.00001104
Iteration 275/1000 | Loss: 0.00001104
Iteration 276/1000 | Loss: 0.00001104
Iteration 277/1000 | Loss: 0.00001104
Iteration 278/1000 | Loss: 0.00001104
Iteration 279/1000 | Loss: 0.00001104
Iteration 280/1000 | Loss: 0.00001104
Iteration 281/1000 | Loss: 0.00001104
Iteration 282/1000 | Loss: 0.00001104
Iteration 283/1000 | Loss: 0.00001104
Iteration 284/1000 | Loss: 0.00001104
Iteration 285/1000 | Loss: 0.00001104
Iteration 286/1000 | Loss: 0.00001104
Iteration 287/1000 | Loss: 0.00001104
Iteration 288/1000 | Loss: 0.00001104
Iteration 289/1000 | Loss: 0.00001104
Iteration 290/1000 | Loss: 0.00001104
Iteration 291/1000 | Loss: 0.00001104
Iteration 292/1000 | Loss: 0.00001104
Iteration 293/1000 | Loss: 0.00001104
Iteration 294/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.1041891411878169e-05, 1.1041891411878169e-05, 1.1041891411878169e-05, 1.1041891411878169e-05, 1.1041891411878169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1041891411878169e-05

Optimization complete. Final v2v error: 2.7808659076690674 mm

Highest mean error: 3.9590744972229004 mm for frame 64

Lowest mean error: 2.389921188354492 mm for frame 149

Saving results

Total time: 317.14159321784973
