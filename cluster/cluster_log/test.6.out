Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=6, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 336-391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804891
Iteration 2/25 | Loss: 0.00126185
Iteration 3/25 | Loss: 0.00117459
Iteration 4/25 | Loss: 0.00117012
Iteration 5/25 | Loss: 0.00116956
Iteration 6/25 | Loss: 0.00116956
Iteration 7/25 | Loss: 0.00116956
Iteration 8/25 | Loss: 0.00116956
Iteration 9/25 | Loss: 0.00116956
Iteration 10/25 | Loss: 0.00116956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011695644352585077, 0.0011695644352585077, 0.0011695644352585077, 0.0011695644352585077, 0.0011695644352585077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011695644352585077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29703617
Iteration 2/25 | Loss: 0.00129569
Iteration 3/25 | Loss: 0.00129569
Iteration 4/25 | Loss: 0.00129569
Iteration 5/25 | Loss: 0.00129569
Iteration 6/25 | Loss: 0.00129569
Iteration 7/25 | Loss: 0.00129569
Iteration 8/25 | Loss: 0.00129569
Iteration 9/25 | Loss: 0.00129569
Iteration 10/25 | Loss: 0.00129569
Iteration 11/25 | Loss: 0.00129569
Iteration 12/25 | Loss: 0.00129569
Iteration 13/25 | Loss: 0.00129569
Iteration 14/25 | Loss: 0.00129568
Iteration 15/25 | Loss: 0.00129568
Iteration 16/25 | Loss: 0.00129568
Iteration 17/25 | Loss: 0.00129568
Iteration 18/25 | Loss: 0.00129568
Iteration 19/25 | Loss: 0.00129568
Iteration 20/25 | Loss: 0.00129568
Iteration 21/25 | Loss: 0.00129568
Iteration 22/25 | Loss: 0.00129568
Iteration 23/25 | Loss: 0.00129568
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001295684720389545, 0.001295684720389545, 0.001295684720389545, 0.001295684720389545, 0.001295684720389545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001295684720389545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129568
Iteration 2/1000 | Loss: 0.00002120
Iteration 3/1000 | Loss: 0.00001414
Iteration 4/1000 | Loss: 0.00001235
Iteration 5/1000 | Loss: 0.00001159
Iteration 6/1000 | Loss: 0.00001103
Iteration 7/1000 | Loss: 0.00001061
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001026
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00000991
Iteration 12/1000 | Loss: 0.00000974
Iteration 13/1000 | Loss: 0.00000969
Iteration 14/1000 | Loss: 0.00000967
Iteration 15/1000 | Loss: 0.00000966
Iteration 16/1000 | Loss: 0.00000964
Iteration 17/1000 | Loss: 0.00000961
Iteration 18/1000 | Loss: 0.00000960
Iteration 19/1000 | Loss: 0.00000959
Iteration 20/1000 | Loss: 0.00000957
Iteration 21/1000 | Loss: 0.00000956
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000951
Iteration 24/1000 | Loss: 0.00000950
Iteration 25/1000 | Loss: 0.00000950
Iteration 26/1000 | Loss: 0.00000949
Iteration 27/1000 | Loss: 0.00000946
Iteration 28/1000 | Loss: 0.00000937
Iteration 29/1000 | Loss: 0.00000935
Iteration 30/1000 | Loss: 0.00000934
Iteration 31/1000 | Loss: 0.00000932
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000926
Iteration 34/1000 | Loss: 0.00000924
Iteration 35/1000 | Loss: 0.00000923
Iteration 36/1000 | Loss: 0.00000922
Iteration 37/1000 | Loss: 0.00000916
Iteration 38/1000 | Loss: 0.00000916
Iteration 39/1000 | Loss: 0.00000916
Iteration 40/1000 | Loss: 0.00000916
Iteration 41/1000 | Loss: 0.00000916
Iteration 42/1000 | Loss: 0.00000915
Iteration 43/1000 | Loss: 0.00000915
Iteration 44/1000 | Loss: 0.00000914
Iteration 45/1000 | Loss: 0.00000914
Iteration 46/1000 | Loss: 0.00000914
Iteration 47/1000 | Loss: 0.00000913
Iteration 48/1000 | Loss: 0.00000912
Iteration 49/1000 | Loss: 0.00000912
Iteration 50/1000 | Loss: 0.00000911
Iteration 51/1000 | Loss: 0.00000911
Iteration 52/1000 | Loss: 0.00000911
Iteration 53/1000 | Loss: 0.00000910
Iteration 54/1000 | Loss: 0.00000909
Iteration 55/1000 | Loss: 0.00000909
Iteration 56/1000 | Loss: 0.00000908
Iteration 57/1000 | Loss: 0.00000908
Iteration 58/1000 | Loss: 0.00000908
Iteration 59/1000 | Loss: 0.00000907
Iteration 60/1000 | Loss: 0.00000907
Iteration 61/1000 | Loss: 0.00000906
Iteration 62/1000 | Loss: 0.00000906
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000905
Iteration 65/1000 | Loss: 0.00000905
Iteration 66/1000 | Loss: 0.00000904
Iteration 67/1000 | Loss: 0.00000904
Iteration 68/1000 | Loss: 0.00000904
Iteration 69/1000 | Loss: 0.00000903
Iteration 70/1000 | Loss: 0.00000902
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000901
Iteration 73/1000 | Loss: 0.00000901
Iteration 74/1000 | Loss: 0.00000901
Iteration 75/1000 | Loss: 0.00000900
Iteration 76/1000 | Loss: 0.00000900
Iteration 77/1000 | Loss: 0.00000900
Iteration 78/1000 | Loss: 0.00000900
Iteration 79/1000 | Loss: 0.00000900
Iteration 80/1000 | Loss: 0.00000899
Iteration 81/1000 | Loss: 0.00000899
Iteration 82/1000 | Loss: 0.00000899
Iteration 83/1000 | Loss: 0.00000898
Iteration 84/1000 | Loss: 0.00000898
Iteration 85/1000 | Loss: 0.00000898
Iteration 86/1000 | Loss: 0.00000897
Iteration 87/1000 | Loss: 0.00000897
Iteration 88/1000 | Loss: 0.00000897
Iteration 89/1000 | Loss: 0.00000896
Iteration 90/1000 | Loss: 0.00000896
Iteration 91/1000 | Loss: 0.00000896
Iteration 92/1000 | Loss: 0.00000896
Iteration 93/1000 | Loss: 0.00000896
Iteration 94/1000 | Loss: 0.00000896
Iteration 95/1000 | Loss: 0.00000896
Iteration 96/1000 | Loss: 0.00000896
Iteration 97/1000 | Loss: 0.00000895
Iteration 98/1000 | Loss: 0.00000895
Iteration 99/1000 | Loss: 0.00000895
Iteration 100/1000 | Loss: 0.00000895
Iteration 101/1000 | Loss: 0.00000894
Iteration 102/1000 | Loss: 0.00000894
Iteration 103/1000 | Loss: 0.00000894
Iteration 104/1000 | Loss: 0.00000894
Iteration 105/1000 | Loss: 0.00000893
Iteration 106/1000 | Loss: 0.00000893
Iteration 107/1000 | Loss: 0.00000893
Iteration 108/1000 | Loss: 0.00000893
Iteration 109/1000 | Loss: 0.00000893
Iteration 110/1000 | Loss: 0.00000893
Iteration 111/1000 | Loss: 0.00000892
Iteration 112/1000 | Loss: 0.00000892
Iteration 113/1000 | Loss: 0.00000892
Iteration 114/1000 | Loss: 0.00000892
Iteration 115/1000 | Loss: 0.00000892
Iteration 116/1000 | Loss: 0.00000892
Iteration 117/1000 | Loss: 0.00000891
Iteration 118/1000 | Loss: 0.00000891
Iteration 119/1000 | Loss: 0.00000891
Iteration 120/1000 | Loss: 0.00000891
Iteration 121/1000 | Loss: 0.00000891
Iteration 122/1000 | Loss: 0.00000891
Iteration 123/1000 | Loss: 0.00000891
Iteration 124/1000 | Loss: 0.00000891
Iteration 125/1000 | Loss: 0.00000891
Iteration 126/1000 | Loss: 0.00000891
Iteration 127/1000 | Loss: 0.00000891
Iteration 128/1000 | Loss: 0.00000891
Iteration 129/1000 | Loss: 0.00000890
Iteration 130/1000 | Loss: 0.00000890
Iteration 131/1000 | Loss: 0.00000890
Iteration 132/1000 | Loss: 0.00000890
Iteration 133/1000 | Loss: 0.00000890
Iteration 134/1000 | Loss: 0.00000890
Iteration 135/1000 | Loss: 0.00000890
Iteration 136/1000 | Loss: 0.00000890
Iteration 137/1000 | Loss: 0.00000890
Iteration 138/1000 | Loss: 0.00000890
Iteration 139/1000 | Loss: 0.00000890
Iteration 140/1000 | Loss: 0.00000890
Iteration 141/1000 | Loss: 0.00000890
Iteration 142/1000 | Loss: 0.00000889
Iteration 143/1000 | Loss: 0.00000889
Iteration 144/1000 | Loss: 0.00000889
Iteration 145/1000 | Loss: 0.00000889
Iteration 146/1000 | Loss: 0.00000889
Iteration 147/1000 | Loss: 0.00000889
Iteration 148/1000 | Loss: 0.00000889
Iteration 149/1000 | Loss: 0.00000889
Iteration 150/1000 | Loss: 0.00000889
Iteration 151/1000 | Loss: 0.00000889
Iteration 152/1000 | Loss: 0.00000889
Iteration 153/1000 | Loss: 0.00000889
Iteration 154/1000 | Loss: 0.00000889
Iteration 155/1000 | Loss: 0.00000889
Iteration 156/1000 | Loss: 0.00000889
Iteration 157/1000 | Loss: 0.00000888
Iteration 158/1000 | Loss: 0.00000888
Iteration 159/1000 | Loss: 0.00000888
Iteration 160/1000 | Loss: 0.00000888
Iteration 161/1000 | Loss: 0.00000888
Iteration 162/1000 | Loss: 0.00000888
Iteration 163/1000 | Loss: 0.00000888
Iteration 164/1000 | Loss: 0.00000888
Iteration 165/1000 | Loss: 0.00000888
Iteration 166/1000 | Loss: 0.00000888
Iteration 167/1000 | Loss: 0.00000887
Iteration 168/1000 | Loss: 0.00000887
Iteration 169/1000 | Loss: 0.00000887
Iteration 170/1000 | Loss: 0.00000887
Iteration 171/1000 | Loss: 0.00000887
Iteration 172/1000 | Loss: 0.00000886
Iteration 173/1000 | Loss: 0.00000886
Iteration 174/1000 | Loss: 0.00000886
Iteration 175/1000 | Loss: 0.00000886
Iteration 176/1000 | Loss: 0.00000886
Iteration 177/1000 | Loss: 0.00000886
Iteration 178/1000 | Loss: 0.00000886
Iteration 179/1000 | Loss: 0.00000886
Iteration 180/1000 | Loss: 0.00000886
Iteration 181/1000 | Loss: 0.00000886
Iteration 182/1000 | Loss: 0.00000885
Iteration 183/1000 | Loss: 0.00000885
Iteration 184/1000 | Loss: 0.00000885
Iteration 185/1000 | Loss: 0.00000885
Iteration 186/1000 | Loss: 0.00000885
Iteration 187/1000 | Loss: 0.00000885
Iteration 188/1000 | Loss: 0.00000884
Iteration 189/1000 | Loss: 0.00000884
Iteration 190/1000 | Loss: 0.00000883
Iteration 191/1000 | Loss: 0.00000883
Iteration 192/1000 | Loss: 0.00000883
Iteration 193/1000 | Loss: 0.00000883
Iteration 194/1000 | Loss: 0.00000883
Iteration 195/1000 | Loss: 0.00000883
Iteration 196/1000 | Loss: 0.00000883
Iteration 197/1000 | Loss: 0.00000883
Iteration 198/1000 | Loss: 0.00000883
Iteration 199/1000 | Loss: 0.00000883
Iteration 200/1000 | Loss: 0.00000883
Iteration 201/1000 | Loss: 0.00000882
Iteration 202/1000 | Loss: 0.00000882
Iteration 203/1000 | Loss: 0.00000882
Iteration 204/1000 | Loss: 0.00000882
Iteration 205/1000 | Loss: 0.00000882
Iteration 206/1000 | Loss: 0.00000882
Iteration 207/1000 | Loss: 0.00000882
Iteration 208/1000 | Loss: 0.00000882
Iteration 209/1000 | Loss: 0.00000882
Iteration 210/1000 | Loss: 0.00000882
Iteration 211/1000 | Loss: 0.00000882
Iteration 212/1000 | Loss: 0.00000881
Iteration 213/1000 | Loss: 0.00000881
Iteration 214/1000 | Loss: 0.00000881
Iteration 215/1000 | Loss: 0.00000881
Iteration 216/1000 | Loss: 0.00000881
Iteration 217/1000 | Loss: 0.00000881
Iteration 218/1000 | Loss: 0.00000881
Iteration 219/1000 | Loss: 0.00000881
Iteration 220/1000 | Loss: 0.00000881
Iteration 221/1000 | Loss: 0.00000881
Iteration 222/1000 | Loss: 0.00000881
Iteration 223/1000 | Loss: 0.00000881
Iteration 224/1000 | Loss: 0.00000881
Iteration 225/1000 | Loss: 0.00000881
Iteration 226/1000 | Loss: 0.00000880
Iteration 227/1000 | Loss: 0.00000880
Iteration 228/1000 | Loss: 0.00000880
Iteration 229/1000 | Loss: 0.00000880
Iteration 230/1000 | Loss: 0.00000880
Iteration 231/1000 | Loss: 0.00000880
Iteration 232/1000 | Loss: 0.00000880
Iteration 233/1000 | Loss: 0.00000880
Iteration 234/1000 | Loss: 0.00000880
Iteration 235/1000 | Loss: 0.00000880
Iteration 236/1000 | Loss: 0.00000880
Iteration 237/1000 | Loss: 0.00000880
Iteration 238/1000 | Loss: 0.00000880
Iteration 239/1000 | Loss: 0.00000880
Iteration 240/1000 | Loss: 0.00000880
Iteration 241/1000 | Loss: 0.00000880
Iteration 242/1000 | Loss: 0.00000880
Iteration 243/1000 | Loss: 0.00000880
Iteration 244/1000 | Loss: 0.00000880
Iteration 245/1000 | Loss: 0.00000880
Iteration 246/1000 | Loss: 0.00000880
Iteration 247/1000 | Loss: 0.00000880
Iteration 248/1000 | Loss: 0.00000880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [8.802976481092628e-06, 8.802976481092628e-06, 8.802976481092628e-06, 8.802976481092628e-06, 8.802976481092628e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.802976481092628e-06

Optimization complete. Final v2v error: 2.533763885498047 mm

Highest mean error: 2.6719157695770264 mm for frame 55

Lowest mean error: 2.4127302169799805 mm for frame 28

Saving results

Total time: 43.86045217514038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023037
Iteration 2/25 | Loss: 0.00235267
Iteration 3/25 | Loss: 0.00173353
Iteration 4/25 | Loss: 0.00167770
Iteration 5/25 | Loss: 0.00159933
Iteration 6/25 | Loss: 0.00155444
Iteration 7/25 | Loss: 0.00147345
Iteration 8/25 | Loss: 0.00134013
Iteration 9/25 | Loss: 0.00132843
Iteration 10/25 | Loss: 0.00127083
Iteration 11/25 | Loss: 0.00124976
Iteration 12/25 | Loss: 0.00124994
Iteration 13/25 | Loss: 0.00124471
Iteration 14/25 | Loss: 0.00123861
Iteration 15/25 | Loss: 0.00123773
Iteration 16/25 | Loss: 0.00123669
Iteration 17/25 | Loss: 0.00123601
Iteration 18/25 | Loss: 0.00123662
Iteration 19/25 | Loss: 0.00123493
Iteration 20/25 | Loss: 0.00123631
Iteration 21/25 | Loss: 0.00123400
Iteration 22/25 | Loss: 0.00123691
Iteration 23/25 | Loss: 0.00123075
Iteration 24/25 | Loss: 0.00123290
Iteration 25/25 | Loss: 0.00123219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51127958
Iteration 2/25 | Loss: 0.00149863
Iteration 3/25 | Loss: 0.00149863
Iteration 4/25 | Loss: 0.00149863
Iteration 5/25 | Loss: 0.00149863
Iteration 6/25 | Loss: 0.00149862
Iteration 7/25 | Loss: 0.00149862
Iteration 8/25 | Loss: 0.00149862
Iteration 9/25 | Loss: 0.00149862
Iteration 10/25 | Loss: 0.00149862
Iteration 11/25 | Loss: 0.00149862
Iteration 12/25 | Loss: 0.00149862
Iteration 13/25 | Loss: 0.00149862
Iteration 14/25 | Loss: 0.00149862
Iteration 15/25 | Loss: 0.00149862
Iteration 16/25 | Loss: 0.00149862
Iteration 17/25 | Loss: 0.00149862
Iteration 18/25 | Loss: 0.00149862
Iteration 19/25 | Loss: 0.00149862
Iteration 20/25 | Loss: 0.00149862
Iteration 21/25 | Loss: 0.00149862
Iteration 22/25 | Loss: 0.00149862
Iteration 23/25 | Loss: 0.00149862
Iteration 24/25 | Loss: 0.00149862
Iteration 25/25 | Loss: 0.00149862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149862
Iteration 2/1000 | Loss: 0.00007794
Iteration 3/1000 | Loss: 0.00021399
Iteration 4/1000 | Loss: 0.00004503
Iteration 5/1000 | Loss: 0.00003235
Iteration 6/1000 | Loss: 0.00004221
Iteration 7/1000 | Loss: 0.00002961
Iteration 8/1000 | Loss: 0.00002961
Iteration 9/1000 | Loss: 0.00022599
Iteration 10/1000 | Loss: 0.00040655
Iteration 11/1000 | Loss: 0.00004742
Iteration 12/1000 | Loss: 0.00004597
Iteration 13/1000 | Loss: 0.00003278
Iteration 14/1000 | Loss: 0.00004050
Iteration 15/1000 | Loss: 0.00002809
Iteration 16/1000 | Loss: 0.00003719
Iteration 17/1000 | Loss: 0.00004243
Iteration 18/1000 | Loss: 0.00004143
Iteration 19/1000 | Loss: 0.00004602
Iteration 20/1000 | Loss: 0.00004640
Iteration 21/1000 | Loss: 0.00003573
Iteration 22/1000 | Loss: 0.00004759
Iteration 23/1000 | Loss: 0.00004561
Iteration 24/1000 | Loss: 0.00023739
Iteration 25/1000 | Loss: 0.00005809
Iteration 26/1000 | Loss: 0.00004691
Iteration 27/1000 | Loss: 0.00005762
Iteration 28/1000 | Loss: 0.00004645
Iteration 29/1000 | Loss: 0.00009704
Iteration 30/1000 | Loss: 0.00004411
Iteration 31/1000 | Loss: 0.00003916
Iteration 32/1000 | Loss: 0.00004216
Iteration 33/1000 | Loss: 0.00003593
Iteration 34/1000 | Loss: 0.00005207
Iteration 35/1000 | Loss: 0.00009323
Iteration 36/1000 | Loss: 0.00004972
Iteration 37/1000 | Loss: 0.00002708
Iteration 38/1000 | Loss: 0.00004822
Iteration 39/1000 | Loss: 0.00003487
Iteration 40/1000 | Loss: 0.00004926
Iteration 41/1000 | Loss: 0.00007370
Iteration 42/1000 | Loss: 0.00004470
Iteration 43/1000 | Loss: 0.00003980
Iteration 44/1000 | Loss: 0.00005675
Iteration 45/1000 | Loss: 0.00010679
Iteration 46/1000 | Loss: 0.00004232
Iteration 47/1000 | Loss: 0.00003574
Iteration 48/1000 | Loss: 0.00005868
Iteration 49/1000 | Loss: 0.00004944
Iteration 50/1000 | Loss: 0.00004446
Iteration 51/1000 | Loss: 0.00003821
Iteration 52/1000 | Loss: 0.00004783
Iteration 53/1000 | Loss: 0.00005368
Iteration 54/1000 | Loss: 0.00004566
Iteration 55/1000 | Loss: 0.00005100
Iteration 56/1000 | Loss: 0.00075851
Iteration 57/1000 | Loss: 0.00082305
Iteration 58/1000 | Loss: 0.00005140
Iteration 59/1000 | Loss: 0.00003357
Iteration 60/1000 | Loss: 0.00025316
Iteration 61/1000 | Loss: 0.00002279
Iteration 62/1000 | Loss: 0.00001883
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00017055
Iteration 65/1000 | Loss: 0.00040705
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00015124
Iteration 68/1000 | Loss: 0.00002874
Iteration 69/1000 | Loss: 0.00003311
Iteration 70/1000 | Loss: 0.00001404
Iteration 71/1000 | Loss: 0.00001358
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001226
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001217
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001214
Iteration 90/1000 | Loss: 0.00001214
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001212
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001200
Iteration 140/1000 | Loss: 0.00001200
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001196
Iteration 157/1000 | Loss: 0.00001196
Iteration 158/1000 | Loss: 0.00001196
Iteration 159/1000 | Loss: 0.00001196
Iteration 160/1000 | Loss: 0.00001196
Iteration 161/1000 | Loss: 0.00001196
Iteration 162/1000 | Loss: 0.00001195
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001195
Iteration 165/1000 | Loss: 0.00001195
Iteration 166/1000 | Loss: 0.00001195
Iteration 167/1000 | Loss: 0.00001195
Iteration 168/1000 | Loss: 0.00001195
Iteration 169/1000 | Loss: 0.00001195
Iteration 170/1000 | Loss: 0.00001195
Iteration 171/1000 | Loss: 0.00001194
Iteration 172/1000 | Loss: 0.00001194
Iteration 173/1000 | Loss: 0.00001194
Iteration 174/1000 | Loss: 0.00001194
Iteration 175/1000 | Loss: 0.00001194
Iteration 176/1000 | Loss: 0.00001194
Iteration 177/1000 | Loss: 0.00001194
Iteration 178/1000 | Loss: 0.00001194
Iteration 179/1000 | Loss: 0.00001194
Iteration 180/1000 | Loss: 0.00001194
Iteration 181/1000 | Loss: 0.00001194
Iteration 182/1000 | Loss: 0.00001194
Iteration 183/1000 | Loss: 0.00001194
Iteration 184/1000 | Loss: 0.00001194
Iteration 185/1000 | Loss: 0.00001194
Iteration 186/1000 | Loss: 0.00001194
Iteration 187/1000 | Loss: 0.00001194
Iteration 188/1000 | Loss: 0.00001194
Iteration 189/1000 | Loss: 0.00001194
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001194
Iteration 192/1000 | Loss: 0.00001194
Iteration 193/1000 | Loss: 0.00001194
Iteration 194/1000 | Loss: 0.00001194
Iteration 195/1000 | Loss: 0.00001194
Iteration 196/1000 | Loss: 0.00001194
Iteration 197/1000 | Loss: 0.00001194
Iteration 198/1000 | Loss: 0.00001194
Iteration 199/1000 | Loss: 0.00001194
Iteration 200/1000 | Loss: 0.00001194
Iteration 201/1000 | Loss: 0.00001194
Iteration 202/1000 | Loss: 0.00001194
Iteration 203/1000 | Loss: 0.00001194
Iteration 204/1000 | Loss: 0.00001194
Iteration 205/1000 | Loss: 0.00001194
Iteration 206/1000 | Loss: 0.00001194
Iteration 207/1000 | Loss: 0.00001194
Iteration 208/1000 | Loss: 0.00001194
Iteration 209/1000 | Loss: 0.00001194
Iteration 210/1000 | Loss: 0.00001194
Iteration 211/1000 | Loss: 0.00001194
Iteration 212/1000 | Loss: 0.00001194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.1942987839574926e-05, 1.1942987839574926e-05, 1.1942987839574926e-05, 1.1942987839574926e-05, 1.1942987839574926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1942987839574926e-05

Optimization complete. Final v2v error: 2.9721121788024902 mm

Highest mean error: 3.4645233154296875 mm for frame 49

Lowest mean error: 2.7019810676574707 mm for frame 86

Saving results

Total time: 155.67592573165894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996937
Iteration 2/25 | Loss: 0.00200016
Iteration 3/25 | Loss: 0.00152879
Iteration 4/25 | Loss: 0.00142612
Iteration 5/25 | Loss: 0.00142754
Iteration 6/25 | Loss: 0.00135978
Iteration 7/25 | Loss: 0.00133662
Iteration 8/25 | Loss: 0.00133190
Iteration 9/25 | Loss: 0.00135365
Iteration 10/25 | Loss: 0.00130848
Iteration 11/25 | Loss: 0.00129033
Iteration 12/25 | Loss: 0.00128671
Iteration 13/25 | Loss: 0.00128721
Iteration 14/25 | Loss: 0.00129041
Iteration 15/25 | Loss: 0.00128541
Iteration 16/25 | Loss: 0.00128287
Iteration 17/25 | Loss: 0.00128121
Iteration 18/25 | Loss: 0.00128047
Iteration 19/25 | Loss: 0.00128022
Iteration 20/25 | Loss: 0.00128013
Iteration 21/25 | Loss: 0.00128013
Iteration 22/25 | Loss: 0.00128013
Iteration 23/25 | Loss: 0.00128013
Iteration 24/25 | Loss: 0.00128013
Iteration 25/25 | Loss: 0.00128012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46513641
Iteration 2/25 | Loss: 0.00167333
Iteration 3/25 | Loss: 0.00167333
Iteration 4/25 | Loss: 0.00167333
Iteration 5/25 | Loss: 0.00167333
Iteration 6/25 | Loss: 0.00167333
Iteration 7/25 | Loss: 0.00167333
Iteration 8/25 | Loss: 0.00167333
Iteration 9/25 | Loss: 0.00167333
Iteration 10/25 | Loss: 0.00167333
Iteration 11/25 | Loss: 0.00167333
Iteration 12/25 | Loss: 0.00167333
Iteration 13/25 | Loss: 0.00167333
Iteration 14/25 | Loss: 0.00167333
Iteration 15/25 | Loss: 0.00167333
Iteration 16/25 | Loss: 0.00167333
Iteration 17/25 | Loss: 0.00167333
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016733251977711916, 0.0016733251977711916, 0.0016733251977711916, 0.0016733251977711916, 0.0016733251977711916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016733251977711916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167333
Iteration 2/1000 | Loss: 0.00076516
Iteration 3/1000 | Loss: 0.00004946
Iteration 4/1000 | Loss: 0.00003874
Iteration 5/1000 | Loss: 0.00003375
Iteration 6/1000 | Loss: 0.00003179
Iteration 7/1000 | Loss: 0.00003031
Iteration 8/1000 | Loss: 0.00116639
Iteration 9/1000 | Loss: 0.00011226
Iteration 10/1000 | Loss: 0.00002756
Iteration 11/1000 | Loss: 0.00002470
Iteration 12/1000 | Loss: 0.00002365
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002202
Iteration 15/1000 | Loss: 0.00002144
Iteration 16/1000 | Loss: 0.00034048
Iteration 17/1000 | Loss: 0.00002107
Iteration 18/1000 | Loss: 0.00001950
Iteration 19/1000 | Loss: 0.00001867
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001705
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001688
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001678
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001676
Iteration 40/1000 | Loss: 0.00001676
Iteration 41/1000 | Loss: 0.00001676
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001676
Iteration 44/1000 | Loss: 0.00001675
Iteration 45/1000 | Loss: 0.00001674
Iteration 46/1000 | Loss: 0.00001674
Iteration 47/1000 | Loss: 0.00001673
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001669
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001666
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001666
Iteration 58/1000 | Loss: 0.00001666
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001664
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001663
Iteration 70/1000 | Loss: 0.00001662
Iteration 71/1000 | Loss: 0.00001662
Iteration 72/1000 | Loss: 0.00001662
Iteration 73/1000 | Loss: 0.00001661
Iteration 74/1000 | Loss: 0.00001661
Iteration 75/1000 | Loss: 0.00001661
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001660
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001660
Iteration 81/1000 | Loss: 0.00001660
Iteration 82/1000 | Loss: 0.00001660
Iteration 83/1000 | Loss: 0.00001660
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001660
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001660
Iteration 91/1000 | Loss: 0.00001660
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001660
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001660
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001660
Iteration 105/1000 | Loss: 0.00001660
Iteration 106/1000 | Loss: 0.00001660
Iteration 107/1000 | Loss: 0.00001660
Iteration 108/1000 | Loss: 0.00001660
Iteration 109/1000 | Loss: 0.00001660
Iteration 110/1000 | Loss: 0.00001660
Iteration 111/1000 | Loss: 0.00001660
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001660
Iteration 122/1000 | Loss: 0.00001660
Iteration 123/1000 | Loss: 0.00001660
Iteration 124/1000 | Loss: 0.00001660
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001660
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001660
Iteration 136/1000 | Loss: 0.00001660
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001660
Iteration 139/1000 | Loss: 0.00001660
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001660
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Iteration 144/1000 | Loss: 0.00001660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.6598023648839444e-05, 1.6598023648839444e-05, 1.6598023648839444e-05, 1.6598023648839444e-05, 1.6598023648839444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6598023648839444e-05

Optimization complete. Final v2v error: 3.240647554397583 mm

Highest mean error: 6.093549728393555 mm for frame 50

Lowest mean error: 2.5047097206115723 mm for frame 0

Saving results

Total time: 72.96234941482544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387379
Iteration 2/25 | Loss: 0.00134174
Iteration 3/25 | Loss: 0.00125068
Iteration 4/25 | Loss: 0.00123966
Iteration 5/25 | Loss: 0.00123518
Iteration 6/25 | Loss: 0.00123509
Iteration 7/25 | Loss: 0.00123509
Iteration 8/25 | Loss: 0.00123509
Iteration 9/25 | Loss: 0.00123509
Iteration 10/25 | Loss: 0.00123509
Iteration 11/25 | Loss: 0.00123509
Iteration 12/25 | Loss: 0.00123509
Iteration 13/25 | Loss: 0.00123509
Iteration 14/25 | Loss: 0.00123509
Iteration 15/25 | Loss: 0.00123509
Iteration 16/25 | Loss: 0.00123509
Iteration 17/25 | Loss: 0.00123509
Iteration 18/25 | Loss: 0.00123509
Iteration 19/25 | Loss: 0.00123509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001235094154253602, 0.001235094154253602, 0.001235094154253602, 0.001235094154253602, 0.001235094154253602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001235094154253602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76908404
Iteration 2/25 | Loss: 0.00171363
Iteration 3/25 | Loss: 0.00171363
Iteration 4/25 | Loss: 0.00171363
Iteration 5/25 | Loss: 0.00171363
Iteration 6/25 | Loss: 0.00171363
Iteration 7/25 | Loss: 0.00171363
Iteration 8/25 | Loss: 0.00171363
Iteration 9/25 | Loss: 0.00171363
Iteration 10/25 | Loss: 0.00171363
Iteration 11/25 | Loss: 0.00171363
Iteration 12/25 | Loss: 0.00171363
Iteration 13/25 | Loss: 0.00171363
Iteration 14/25 | Loss: 0.00171363
Iteration 15/25 | Loss: 0.00171363
Iteration 16/25 | Loss: 0.00171363
Iteration 17/25 | Loss: 0.00171363
Iteration 18/25 | Loss: 0.00171363
Iteration 19/25 | Loss: 0.00171363
Iteration 20/25 | Loss: 0.00171363
Iteration 21/25 | Loss: 0.00171363
Iteration 22/25 | Loss: 0.00171363
Iteration 23/25 | Loss: 0.00171363
Iteration 24/25 | Loss: 0.00171363
Iteration 25/25 | Loss: 0.00171363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171363
Iteration 2/1000 | Loss: 0.00003619
Iteration 3/1000 | Loss: 0.00001945
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001546
Iteration 6/1000 | Loss: 0.00001454
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001328
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001294
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00001262
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001260
Iteration 16/1000 | Loss: 0.00001258
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001256
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001254
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001253
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001252
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001251
Iteration 33/1000 | Loss: 0.00001251
Iteration 34/1000 | Loss: 0.00001251
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001247
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001239
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001237
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001230
Iteration 91/1000 | Loss: 0.00001230
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001228
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001226
Iteration 107/1000 | Loss: 0.00001226
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001225
Iteration 111/1000 | Loss: 0.00001225
Iteration 112/1000 | Loss: 0.00001225
Iteration 113/1000 | Loss: 0.00001225
Iteration 114/1000 | Loss: 0.00001224
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001224
Iteration 118/1000 | Loss: 0.00001224
Iteration 119/1000 | Loss: 0.00001223
Iteration 120/1000 | Loss: 0.00001223
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001223
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001223
Iteration 126/1000 | Loss: 0.00001223
Iteration 127/1000 | Loss: 0.00001223
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001221
Iteration 131/1000 | Loss: 0.00001221
Iteration 132/1000 | Loss: 0.00001221
Iteration 133/1000 | Loss: 0.00001221
Iteration 134/1000 | Loss: 0.00001221
Iteration 135/1000 | Loss: 0.00001221
Iteration 136/1000 | Loss: 0.00001221
Iteration 137/1000 | Loss: 0.00001220
Iteration 138/1000 | Loss: 0.00001220
Iteration 139/1000 | Loss: 0.00001220
Iteration 140/1000 | Loss: 0.00001220
Iteration 141/1000 | Loss: 0.00001220
Iteration 142/1000 | Loss: 0.00001220
Iteration 143/1000 | Loss: 0.00001220
Iteration 144/1000 | Loss: 0.00001220
Iteration 145/1000 | Loss: 0.00001220
Iteration 146/1000 | Loss: 0.00001220
Iteration 147/1000 | Loss: 0.00001220
Iteration 148/1000 | Loss: 0.00001220
Iteration 149/1000 | Loss: 0.00001220
Iteration 150/1000 | Loss: 0.00001220
Iteration 151/1000 | Loss: 0.00001220
Iteration 152/1000 | Loss: 0.00001220
Iteration 153/1000 | Loss: 0.00001220
Iteration 154/1000 | Loss: 0.00001220
Iteration 155/1000 | Loss: 0.00001220
Iteration 156/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2196785064588767e-05, 1.2196785064588767e-05, 1.2196785064588767e-05, 1.2196785064588767e-05, 1.2196785064588767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2196785064588767e-05

Optimization complete. Final v2v error: 2.965179204940796 mm

Highest mean error: 3.1408069133758545 mm for frame 200

Lowest mean error: 2.813498020172119 mm for frame 182

Saving results

Total time: 42.78890514373779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753596
Iteration 2/25 | Loss: 0.00153830
Iteration 3/25 | Loss: 0.00125293
Iteration 4/25 | Loss: 0.00123668
Iteration 5/25 | Loss: 0.00123596
Iteration 6/25 | Loss: 0.00123596
Iteration 7/25 | Loss: 0.00123596
Iteration 8/25 | Loss: 0.00123596
Iteration 9/25 | Loss: 0.00123596
Iteration 10/25 | Loss: 0.00123596
Iteration 11/25 | Loss: 0.00123596
Iteration 12/25 | Loss: 0.00123596
Iteration 13/25 | Loss: 0.00123596
Iteration 14/25 | Loss: 0.00123596
Iteration 15/25 | Loss: 0.00123596
Iteration 16/25 | Loss: 0.00123596
Iteration 17/25 | Loss: 0.00123596
Iteration 18/25 | Loss: 0.00123596
Iteration 19/25 | Loss: 0.00123596
Iteration 20/25 | Loss: 0.00123596
Iteration 21/25 | Loss: 0.00123596
Iteration 22/25 | Loss: 0.00123596
Iteration 23/25 | Loss: 0.00123596
Iteration 24/25 | Loss: 0.00123596
Iteration 25/25 | Loss: 0.00123596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25242615
Iteration 2/25 | Loss: 0.00102463
Iteration 3/25 | Loss: 0.00102463
Iteration 4/25 | Loss: 0.00102463
Iteration 5/25 | Loss: 0.00102463
Iteration 6/25 | Loss: 0.00102463
Iteration 7/25 | Loss: 0.00102463
Iteration 8/25 | Loss: 0.00102463
Iteration 9/25 | Loss: 0.00102463
Iteration 10/25 | Loss: 0.00102463
Iteration 11/25 | Loss: 0.00102463
Iteration 12/25 | Loss: 0.00102463
Iteration 13/25 | Loss: 0.00102463
Iteration 14/25 | Loss: 0.00102463
Iteration 15/25 | Loss: 0.00102463
Iteration 16/25 | Loss: 0.00102463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010246307356283069, 0.0010246307356283069, 0.0010246307356283069, 0.0010246307356283069, 0.0010246307356283069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010246307356283069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102463
Iteration 2/1000 | Loss: 0.00002489
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001495
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001248
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001185
Iteration 21/1000 | Loss: 0.00001184
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001176
Iteration 24/1000 | Loss: 0.00001176
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001173
Iteration 28/1000 | Loss: 0.00001173
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001172
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001171
Iteration 34/1000 | Loss: 0.00001170
Iteration 35/1000 | Loss: 0.00001168
Iteration 36/1000 | Loss: 0.00001166
Iteration 37/1000 | Loss: 0.00001165
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001161
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001159
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001149
Iteration 65/1000 | Loss: 0.00001149
Iteration 66/1000 | Loss: 0.00001147
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001144
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001144
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001144
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001143
Iteration 96/1000 | Loss: 0.00001143
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001141
Iteration 102/1000 | Loss: 0.00001141
Iteration 103/1000 | Loss: 0.00001140
Iteration 104/1000 | Loss: 0.00001140
Iteration 105/1000 | Loss: 0.00001140
Iteration 106/1000 | Loss: 0.00001139
Iteration 107/1000 | Loss: 0.00001139
Iteration 108/1000 | Loss: 0.00001139
Iteration 109/1000 | Loss: 0.00001139
Iteration 110/1000 | Loss: 0.00001139
Iteration 111/1000 | Loss: 0.00001139
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001138
Iteration 114/1000 | Loss: 0.00001137
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001137
Iteration 117/1000 | Loss: 0.00001137
Iteration 118/1000 | Loss: 0.00001137
Iteration 119/1000 | Loss: 0.00001137
Iteration 120/1000 | Loss: 0.00001137
Iteration 121/1000 | Loss: 0.00001137
Iteration 122/1000 | Loss: 0.00001137
Iteration 123/1000 | Loss: 0.00001137
Iteration 124/1000 | Loss: 0.00001137
Iteration 125/1000 | Loss: 0.00001137
Iteration 126/1000 | Loss: 0.00001137
Iteration 127/1000 | Loss: 0.00001137
Iteration 128/1000 | Loss: 0.00001137
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001137
Iteration 131/1000 | Loss: 0.00001137
Iteration 132/1000 | Loss: 0.00001137
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001137
Iteration 135/1000 | Loss: 0.00001137
Iteration 136/1000 | Loss: 0.00001137
Iteration 137/1000 | Loss: 0.00001137
Iteration 138/1000 | Loss: 0.00001137
Iteration 139/1000 | Loss: 0.00001137
Iteration 140/1000 | Loss: 0.00001137
Iteration 141/1000 | Loss: 0.00001137
Iteration 142/1000 | Loss: 0.00001137
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.13672458610381e-05, 1.13672458610381e-05, 1.13672458610381e-05, 1.13672458610381e-05, 1.13672458610381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.13672458610381e-05

Optimization complete. Final v2v error: 2.869778633117676 mm

Highest mean error: 3.0788934230804443 mm for frame 143

Lowest mean error: 2.716738224029541 mm for frame 181

Saving results

Total time: 40.607304096221924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821148
Iteration 2/25 | Loss: 0.00130303
Iteration 3/25 | Loss: 0.00121132
Iteration 4/25 | Loss: 0.00120324
Iteration 5/25 | Loss: 0.00120079
Iteration 6/25 | Loss: 0.00120072
Iteration 7/25 | Loss: 0.00120072
Iteration 8/25 | Loss: 0.00120072
Iteration 9/25 | Loss: 0.00120072
Iteration 10/25 | Loss: 0.00120072
Iteration 11/25 | Loss: 0.00120072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012007217155769467, 0.0012007217155769467, 0.0012007217155769467, 0.0012007217155769467, 0.0012007217155769467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012007217155769467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67194879
Iteration 2/25 | Loss: 0.00139275
Iteration 3/25 | Loss: 0.00139275
Iteration 4/25 | Loss: 0.00139275
Iteration 5/25 | Loss: 0.00139275
Iteration 6/25 | Loss: 0.00139275
Iteration 7/25 | Loss: 0.00139275
Iteration 8/25 | Loss: 0.00139275
Iteration 9/25 | Loss: 0.00139275
Iteration 10/25 | Loss: 0.00139275
Iteration 11/25 | Loss: 0.00139274
Iteration 12/25 | Loss: 0.00139274
Iteration 13/25 | Loss: 0.00139274
Iteration 14/25 | Loss: 0.00139274
Iteration 15/25 | Loss: 0.00139274
Iteration 16/25 | Loss: 0.00139274
Iteration 17/25 | Loss: 0.00139274
Iteration 18/25 | Loss: 0.00139274
Iteration 19/25 | Loss: 0.00139274
Iteration 20/25 | Loss: 0.00139274
Iteration 21/25 | Loss: 0.00139274
Iteration 22/25 | Loss: 0.00139274
Iteration 23/25 | Loss: 0.00139274
Iteration 24/25 | Loss: 0.00139274
Iteration 25/25 | Loss: 0.00139274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001392743899486959, 0.001392743899486959, 0.001392743899486959, 0.001392743899486959, 0.001392743899486959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001392743899486959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139274
Iteration 2/1000 | Loss: 0.00002069
Iteration 3/1000 | Loss: 0.00001621
Iteration 4/1000 | Loss: 0.00001465
Iteration 5/1000 | Loss: 0.00001387
Iteration 6/1000 | Loss: 0.00001314
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001186
Iteration 11/1000 | Loss: 0.00001166
Iteration 12/1000 | Loss: 0.00001157
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001138
Iteration 15/1000 | Loss: 0.00001130
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001115
Iteration 19/1000 | Loss: 0.00001115
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001101
Iteration 27/1000 | Loss: 0.00001101
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001097
Iteration 36/1000 | Loss: 0.00001097
Iteration 37/1000 | Loss: 0.00001097
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001096
Iteration 40/1000 | Loss: 0.00001095
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001093
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001092
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001090
Iteration 56/1000 | Loss: 0.00001089
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001088
Iteration 60/1000 | Loss: 0.00001088
Iteration 61/1000 | Loss: 0.00001088
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001087
Iteration 64/1000 | Loss: 0.00001087
Iteration 65/1000 | Loss: 0.00001087
Iteration 66/1000 | Loss: 0.00001086
Iteration 67/1000 | Loss: 0.00001086
Iteration 68/1000 | Loss: 0.00001086
Iteration 69/1000 | Loss: 0.00001085
Iteration 70/1000 | Loss: 0.00001085
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001084
Iteration 75/1000 | Loss: 0.00001084
Iteration 76/1000 | Loss: 0.00001084
Iteration 77/1000 | Loss: 0.00001084
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001083
Iteration 81/1000 | Loss: 0.00001083
Iteration 82/1000 | Loss: 0.00001083
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001082
Iteration 92/1000 | Loss: 0.00001082
Iteration 93/1000 | Loss: 0.00001081
Iteration 94/1000 | Loss: 0.00001081
Iteration 95/1000 | Loss: 0.00001081
Iteration 96/1000 | Loss: 0.00001081
Iteration 97/1000 | Loss: 0.00001081
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001080
Iteration 100/1000 | Loss: 0.00001080
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001079
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001079
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001078
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001078
Iteration 114/1000 | Loss: 0.00001078
Iteration 115/1000 | Loss: 0.00001077
Iteration 116/1000 | Loss: 0.00001077
Iteration 117/1000 | Loss: 0.00001077
Iteration 118/1000 | Loss: 0.00001077
Iteration 119/1000 | Loss: 0.00001077
Iteration 120/1000 | Loss: 0.00001077
Iteration 121/1000 | Loss: 0.00001077
Iteration 122/1000 | Loss: 0.00001077
Iteration 123/1000 | Loss: 0.00001077
Iteration 124/1000 | Loss: 0.00001077
Iteration 125/1000 | Loss: 0.00001077
Iteration 126/1000 | Loss: 0.00001077
Iteration 127/1000 | Loss: 0.00001077
Iteration 128/1000 | Loss: 0.00001077
Iteration 129/1000 | Loss: 0.00001076
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001076
Iteration 132/1000 | Loss: 0.00001076
Iteration 133/1000 | Loss: 0.00001076
Iteration 134/1000 | Loss: 0.00001076
Iteration 135/1000 | Loss: 0.00001076
Iteration 136/1000 | Loss: 0.00001076
Iteration 137/1000 | Loss: 0.00001076
Iteration 138/1000 | Loss: 0.00001076
Iteration 139/1000 | Loss: 0.00001076
Iteration 140/1000 | Loss: 0.00001076
Iteration 141/1000 | Loss: 0.00001076
Iteration 142/1000 | Loss: 0.00001076
Iteration 143/1000 | Loss: 0.00001076
Iteration 144/1000 | Loss: 0.00001076
Iteration 145/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.0756383744592313e-05, 1.0756383744592313e-05, 1.0756383744592313e-05, 1.0756383744592313e-05, 1.0756383744592313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0756383744592313e-05

Optimization complete. Final v2v error: 2.8245882987976074 mm

Highest mean error: 3.5161807537078857 mm for frame 115

Lowest mean error: 2.5997488498687744 mm for frame 71

Saving results

Total time: 38.70331358909607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020414
Iteration 2/25 | Loss: 0.01020414
Iteration 3/25 | Loss: 0.00196956
Iteration 4/25 | Loss: 0.00200333
Iteration 5/25 | Loss: 0.00125554
Iteration 6/25 | Loss: 0.00122934
Iteration 7/25 | Loss: 0.00121842
Iteration 8/25 | Loss: 0.00121193
Iteration 9/25 | Loss: 0.00121957
Iteration 10/25 | Loss: 0.00121294
Iteration 11/25 | Loss: 0.00121147
Iteration 12/25 | Loss: 0.00121202
Iteration 13/25 | Loss: 0.00120750
Iteration 14/25 | Loss: 0.00120700
Iteration 15/25 | Loss: 0.00120675
Iteration 16/25 | Loss: 0.00120659
Iteration 17/25 | Loss: 0.00120657
Iteration 18/25 | Loss: 0.00120657
Iteration 19/25 | Loss: 0.00120656
Iteration 20/25 | Loss: 0.00120656
Iteration 21/25 | Loss: 0.00120656
Iteration 22/25 | Loss: 0.00120656
Iteration 23/25 | Loss: 0.00120656
Iteration 24/25 | Loss: 0.00120656
Iteration 25/25 | Loss: 0.00120656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23722899
Iteration 2/25 | Loss: 0.00180624
Iteration 3/25 | Loss: 0.00180624
Iteration 4/25 | Loss: 0.00180624
Iteration 5/25 | Loss: 0.00180624
Iteration 6/25 | Loss: 0.00180624
Iteration 7/25 | Loss: 0.00180624
Iteration 8/25 | Loss: 0.00180624
Iteration 9/25 | Loss: 0.00180624
Iteration 10/25 | Loss: 0.00180624
Iteration 11/25 | Loss: 0.00180624
Iteration 12/25 | Loss: 0.00180623
Iteration 13/25 | Loss: 0.00180624
Iteration 14/25 | Loss: 0.00180623
Iteration 15/25 | Loss: 0.00180623
Iteration 16/25 | Loss: 0.00180623
Iteration 17/25 | Loss: 0.00180624
Iteration 18/25 | Loss: 0.00180624
Iteration 19/25 | Loss: 0.00180623
Iteration 20/25 | Loss: 0.00180623
Iteration 21/25 | Loss: 0.00180623
Iteration 22/25 | Loss: 0.00180623
Iteration 23/25 | Loss: 0.00180623
Iteration 24/25 | Loss: 0.00180623
Iteration 25/25 | Loss: 0.00180623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180623
Iteration 2/1000 | Loss: 0.00022248
Iteration 3/1000 | Loss: 0.00013089
Iteration 4/1000 | Loss: 0.00010131
Iteration 5/1000 | Loss: 0.00002404
Iteration 6/1000 | Loss: 0.00007946
Iteration 7/1000 | Loss: 0.00001998
Iteration 8/1000 | Loss: 0.00001929
Iteration 9/1000 | Loss: 0.00011513
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00008415
Iteration 12/1000 | Loss: 0.00001682
Iteration 13/1000 | Loss: 0.00008822
Iteration 14/1000 | Loss: 0.00031586
Iteration 15/1000 | Loss: 0.00010494
Iteration 16/1000 | Loss: 0.00001729
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00002899
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00003028
Iteration 21/1000 | Loss: 0.00008209
Iteration 22/1000 | Loss: 0.00001536
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001515
Iteration 25/1000 | Loss: 0.00001512
Iteration 26/1000 | Loss: 0.00009239
Iteration 27/1000 | Loss: 0.00001755
Iteration 28/1000 | Loss: 0.00001566
Iteration 29/1000 | Loss: 0.00003376
Iteration 30/1000 | Loss: 0.00002169
Iteration 31/1000 | Loss: 0.00002493
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001496
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00003270
Iteration 41/1000 | Loss: 0.00001828
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001476
Iteration 44/1000 | Loss: 0.00001476
Iteration 45/1000 | Loss: 0.00001476
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001475
Iteration 48/1000 | Loss: 0.00001475
Iteration 49/1000 | Loss: 0.00001475
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001475
Iteration 55/1000 | Loss: 0.00001475
Iteration 56/1000 | Loss: 0.00001475
Iteration 57/1000 | Loss: 0.00001475
Iteration 58/1000 | Loss: 0.00001475
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001475
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001475
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.4752884453628212e-05, 1.4752884453628212e-05, 1.4752884453628212e-05, 1.4752884453628212e-05, 1.4752884453628212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4752884453628212e-05

Optimization complete. Final v2v error: 3.2889645099639893 mm

Highest mean error: 3.726768732070923 mm for frame 58

Lowest mean error: 2.966909885406494 mm for frame 175

Saving results

Total time: 77.69786477088928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818204
Iteration 2/25 | Loss: 0.00123533
Iteration 3/25 | Loss: 0.00118246
Iteration 4/25 | Loss: 0.00117658
Iteration 5/25 | Loss: 0.00117468
Iteration 6/25 | Loss: 0.00117468
Iteration 7/25 | Loss: 0.00117468
Iteration 8/25 | Loss: 0.00117468
Iteration 9/25 | Loss: 0.00117468
Iteration 10/25 | Loss: 0.00117468
Iteration 11/25 | Loss: 0.00117468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001174675184302032, 0.001174675184302032, 0.001174675184302032, 0.001174675184302032, 0.001174675184302032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001174675184302032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.17764282
Iteration 2/25 | Loss: 0.00139294
Iteration 3/25 | Loss: 0.00139293
Iteration 4/25 | Loss: 0.00139293
Iteration 5/25 | Loss: 0.00139293
Iteration 6/25 | Loss: 0.00139293
Iteration 7/25 | Loss: 0.00139293
Iteration 8/25 | Loss: 0.00139293
Iteration 9/25 | Loss: 0.00139293
Iteration 10/25 | Loss: 0.00139293
Iteration 11/25 | Loss: 0.00139293
Iteration 12/25 | Loss: 0.00139293
Iteration 13/25 | Loss: 0.00139293
Iteration 14/25 | Loss: 0.00139293
Iteration 15/25 | Loss: 0.00139293
Iteration 16/25 | Loss: 0.00139293
Iteration 17/25 | Loss: 0.00139293
Iteration 18/25 | Loss: 0.00139293
Iteration 19/25 | Loss: 0.00139293
Iteration 20/25 | Loss: 0.00139293
Iteration 21/25 | Loss: 0.00139293
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013929309789091349, 0.0013929309789091349, 0.0013929309789091349, 0.0013929309789091349, 0.0013929309789091349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013929309789091349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139293
Iteration 2/1000 | Loss: 0.00002299
Iteration 3/1000 | Loss: 0.00001636
Iteration 4/1000 | Loss: 0.00001479
Iteration 5/1000 | Loss: 0.00001390
Iteration 6/1000 | Loss: 0.00001341
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001231
Iteration 10/1000 | Loss: 0.00001208
Iteration 11/1000 | Loss: 0.00001195
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001194
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001194
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001188
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001169
Iteration 36/1000 | Loss: 0.00001165
Iteration 37/1000 | Loss: 0.00001165
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001164
Iteration 40/1000 | Loss: 0.00001158
Iteration 41/1000 | Loss: 0.00001158
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001150
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001144
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001143
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001120
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001118
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001117
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001117
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001114
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001113
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001111
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001110
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001108
Iteration 134/1000 | Loss: 0.00001108
Iteration 135/1000 | Loss: 0.00001108
Iteration 136/1000 | Loss: 0.00001108
Iteration 137/1000 | Loss: 0.00001108
Iteration 138/1000 | Loss: 0.00001108
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001108
Iteration 141/1000 | Loss: 0.00001108
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001107
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001107
Iteration 158/1000 | Loss: 0.00001107
Iteration 159/1000 | Loss: 0.00001107
Iteration 160/1000 | Loss: 0.00001107
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Iteration 168/1000 | Loss: 0.00001106
Iteration 169/1000 | Loss: 0.00001106
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001106
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001105
Iteration 180/1000 | Loss: 0.00001105
Iteration 181/1000 | Loss: 0.00001105
Iteration 182/1000 | Loss: 0.00001105
Iteration 183/1000 | Loss: 0.00001105
Iteration 184/1000 | Loss: 0.00001105
Iteration 185/1000 | Loss: 0.00001105
Iteration 186/1000 | Loss: 0.00001105
Iteration 187/1000 | Loss: 0.00001105
Iteration 188/1000 | Loss: 0.00001105
Iteration 189/1000 | Loss: 0.00001105
Iteration 190/1000 | Loss: 0.00001105
Iteration 191/1000 | Loss: 0.00001105
Iteration 192/1000 | Loss: 0.00001105
Iteration 193/1000 | Loss: 0.00001105
Iteration 194/1000 | Loss: 0.00001105
Iteration 195/1000 | Loss: 0.00001104
Iteration 196/1000 | Loss: 0.00001104
Iteration 197/1000 | Loss: 0.00001104
Iteration 198/1000 | Loss: 0.00001104
Iteration 199/1000 | Loss: 0.00001104
Iteration 200/1000 | Loss: 0.00001104
Iteration 201/1000 | Loss: 0.00001104
Iteration 202/1000 | Loss: 0.00001104
Iteration 203/1000 | Loss: 0.00001104
Iteration 204/1000 | Loss: 0.00001104
Iteration 205/1000 | Loss: 0.00001104
Iteration 206/1000 | Loss: 0.00001104
Iteration 207/1000 | Loss: 0.00001104
Iteration 208/1000 | Loss: 0.00001104
Iteration 209/1000 | Loss: 0.00001103
Iteration 210/1000 | Loss: 0.00001103
Iteration 211/1000 | Loss: 0.00001103
Iteration 212/1000 | Loss: 0.00001103
Iteration 213/1000 | Loss: 0.00001103
Iteration 214/1000 | Loss: 0.00001103
Iteration 215/1000 | Loss: 0.00001103
Iteration 216/1000 | Loss: 0.00001103
Iteration 217/1000 | Loss: 0.00001103
Iteration 218/1000 | Loss: 0.00001103
Iteration 219/1000 | Loss: 0.00001103
Iteration 220/1000 | Loss: 0.00001103
Iteration 221/1000 | Loss: 0.00001103
Iteration 222/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.1030763744201977e-05, 1.1030763744201977e-05, 1.1030763744201977e-05, 1.1030763744201977e-05, 1.1030763744201977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1030763744201977e-05

Optimization complete. Final v2v error: 2.8648288249969482 mm

Highest mean error: 3.169379472732544 mm for frame 113

Lowest mean error: 2.5594348907470703 mm for frame 138

Saving results

Total time: 41.30121660232544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825702
Iteration 2/25 | Loss: 0.00141108
Iteration 3/25 | Loss: 0.00131431
Iteration 4/25 | Loss: 0.00129790
Iteration 5/25 | Loss: 0.00129248
Iteration 6/25 | Loss: 0.00129194
Iteration 7/25 | Loss: 0.00129194
Iteration 8/25 | Loss: 0.00129194
Iteration 9/25 | Loss: 0.00129194
Iteration 10/25 | Loss: 0.00129194
Iteration 11/25 | Loss: 0.00129194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012919401051476598, 0.0012919401051476598, 0.0012919401051476598, 0.0012919401051476598, 0.0012919401051476598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012919401051476598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11715341
Iteration 2/25 | Loss: 0.00113836
Iteration 3/25 | Loss: 0.00113835
Iteration 4/25 | Loss: 0.00113835
Iteration 5/25 | Loss: 0.00113835
Iteration 6/25 | Loss: 0.00113835
Iteration 7/25 | Loss: 0.00113835
Iteration 8/25 | Loss: 0.00113835
Iteration 9/25 | Loss: 0.00113835
Iteration 10/25 | Loss: 0.00113835
Iteration 11/25 | Loss: 0.00113835
Iteration 12/25 | Loss: 0.00113835
Iteration 13/25 | Loss: 0.00113835
Iteration 14/25 | Loss: 0.00113835
Iteration 15/25 | Loss: 0.00113835
Iteration 16/25 | Loss: 0.00113835
Iteration 17/25 | Loss: 0.00113835
Iteration 18/25 | Loss: 0.00113835
Iteration 19/25 | Loss: 0.00113835
Iteration 20/25 | Loss: 0.00113835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011383519740775228, 0.0011383519740775228, 0.0011383519740775228, 0.0011383519740775228, 0.0011383519740775228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011383519740775228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113835
Iteration 2/1000 | Loss: 0.00004965
Iteration 3/1000 | Loss: 0.00003397
Iteration 4/1000 | Loss: 0.00003016
Iteration 5/1000 | Loss: 0.00002815
Iteration 6/1000 | Loss: 0.00002708
Iteration 7/1000 | Loss: 0.00002616
Iteration 8/1000 | Loss: 0.00002564
Iteration 9/1000 | Loss: 0.00002512
Iteration 10/1000 | Loss: 0.00002477
Iteration 11/1000 | Loss: 0.00002452
Iteration 12/1000 | Loss: 0.00002430
Iteration 13/1000 | Loss: 0.00002413
Iteration 14/1000 | Loss: 0.00002400
Iteration 15/1000 | Loss: 0.00002396
Iteration 16/1000 | Loss: 0.00002392
Iteration 17/1000 | Loss: 0.00002386
Iteration 18/1000 | Loss: 0.00002384
Iteration 19/1000 | Loss: 0.00002383
Iteration 20/1000 | Loss: 0.00002383
Iteration 21/1000 | Loss: 0.00002382
Iteration 22/1000 | Loss: 0.00002378
Iteration 23/1000 | Loss: 0.00002378
Iteration 24/1000 | Loss: 0.00002374
Iteration 25/1000 | Loss: 0.00002373
Iteration 26/1000 | Loss: 0.00002373
Iteration 27/1000 | Loss: 0.00002373
Iteration 28/1000 | Loss: 0.00002372
Iteration 29/1000 | Loss: 0.00002372
Iteration 30/1000 | Loss: 0.00002372
Iteration 31/1000 | Loss: 0.00002371
Iteration 32/1000 | Loss: 0.00002370
Iteration 33/1000 | Loss: 0.00002368
Iteration 34/1000 | Loss: 0.00002368
Iteration 35/1000 | Loss: 0.00002367
Iteration 36/1000 | Loss: 0.00002367
Iteration 37/1000 | Loss: 0.00002366
Iteration 38/1000 | Loss: 0.00002365
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002364
Iteration 42/1000 | Loss: 0.00002363
Iteration 43/1000 | Loss: 0.00002363
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002362
Iteration 46/1000 | Loss: 0.00002360
Iteration 47/1000 | Loss: 0.00002357
Iteration 48/1000 | Loss: 0.00002357
Iteration 49/1000 | Loss: 0.00002357
Iteration 50/1000 | Loss: 0.00002357
Iteration 51/1000 | Loss: 0.00002357
Iteration 52/1000 | Loss: 0.00002356
Iteration 53/1000 | Loss: 0.00002355
Iteration 54/1000 | Loss: 0.00002354
Iteration 55/1000 | Loss: 0.00002354
Iteration 56/1000 | Loss: 0.00002354
Iteration 57/1000 | Loss: 0.00002353
Iteration 58/1000 | Loss: 0.00002353
Iteration 59/1000 | Loss: 0.00002352
Iteration 60/1000 | Loss: 0.00002352
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002351
Iteration 63/1000 | Loss: 0.00002350
Iteration 64/1000 | Loss: 0.00002350
Iteration 65/1000 | Loss: 0.00002350
Iteration 66/1000 | Loss: 0.00002349
Iteration 67/1000 | Loss: 0.00002349
Iteration 68/1000 | Loss: 0.00002349
Iteration 69/1000 | Loss: 0.00002349
Iteration 70/1000 | Loss: 0.00002349
Iteration 71/1000 | Loss: 0.00002349
Iteration 72/1000 | Loss: 0.00002348
Iteration 73/1000 | Loss: 0.00002348
Iteration 74/1000 | Loss: 0.00002348
Iteration 75/1000 | Loss: 0.00002348
Iteration 76/1000 | Loss: 0.00002348
Iteration 77/1000 | Loss: 0.00002347
Iteration 78/1000 | Loss: 0.00002347
Iteration 79/1000 | Loss: 0.00002347
Iteration 80/1000 | Loss: 0.00002347
Iteration 81/1000 | Loss: 0.00002346
Iteration 82/1000 | Loss: 0.00002346
Iteration 83/1000 | Loss: 0.00002346
Iteration 84/1000 | Loss: 0.00002346
Iteration 85/1000 | Loss: 0.00002346
Iteration 86/1000 | Loss: 0.00002346
Iteration 87/1000 | Loss: 0.00002346
Iteration 88/1000 | Loss: 0.00002346
Iteration 89/1000 | Loss: 0.00002346
Iteration 90/1000 | Loss: 0.00002346
Iteration 91/1000 | Loss: 0.00002346
Iteration 92/1000 | Loss: 0.00002346
Iteration 93/1000 | Loss: 0.00002346
Iteration 94/1000 | Loss: 0.00002346
Iteration 95/1000 | Loss: 0.00002346
Iteration 96/1000 | Loss: 0.00002346
Iteration 97/1000 | Loss: 0.00002346
Iteration 98/1000 | Loss: 0.00002346
Iteration 99/1000 | Loss: 0.00002346
Iteration 100/1000 | Loss: 0.00002346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.3456128474208526e-05, 2.3456128474208526e-05, 2.3456128474208526e-05, 2.3456128474208526e-05, 2.3456128474208526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3456128474208526e-05

Optimization complete. Final v2v error: 3.9944169521331787 mm

Highest mean error: 5.283483028411865 mm for frame 127

Lowest mean error: 3.0482497215270996 mm for frame 51

Saving results

Total time: 42.34068202972412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814832
Iteration 2/25 | Loss: 0.00145368
Iteration 3/25 | Loss: 0.00125293
Iteration 4/25 | Loss: 0.00123527
Iteration 5/25 | Loss: 0.00123364
Iteration 6/25 | Loss: 0.00123364
Iteration 7/25 | Loss: 0.00123364
Iteration 8/25 | Loss: 0.00123364
Iteration 9/25 | Loss: 0.00123364
Iteration 10/25 | Loss: 0.00123364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012336415238678455, 0.0012336415238678455, 0.0012336415238678455, 0.0012336415238678455, 0.0012336415238678455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012336415238678455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92618340
Iteration 2/25 | Loss: 0.00075380
Iteration 3/25 | Loss: 0.00075380
Iteration 4/25 | Loss: 0.00075380
Iteration 5/25 | Loss: 0.00075380
Iteration 6/25 | Loss: 0.00075380
Iteration 7/25 | Loss: 0.00075379
Iteration 8/25 | Loss: 0.00075379
Iteration 9/25 | Loss: 0.00075379
Iteration 10/25 | Loss: 0.00075379
Iteration 11/25 | Loss: 0.00075379
Iteration 12/25 | Loss: 0.00075379
Iteration 13/25 | Loss: 0.00075379
Iteration 14/25 | Loss: 0.00075379
Iteration 15/25 | Loss: 0.00075379
Iteration 16/25 | Loss: 0.00075379
Iteration 17/25 | Loss: 0.00075379
Iteration 18/25 | Loss: 0.00075379
Iteration 19/25 | Loss: 0.00075379
Iteration 20/25 | Loss: 0.00075379
Iteration 21/25 | Loss: 0.00075379
Iteration 22/25 | Loss: 0.00075379
Iteration 23/25 | Loss: 0.00075379
Iteration 24/25 | Loss: 0.00075379
Iteration 25/25 | Loss: 0.00075379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075379
Iteration 2/1000 | Loss: 0.00003088
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00002191
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00002029
Iteration 9/1000 | Loss: 0.00002004
Iteration 10/1000 | Loss: 0.00002002
Iteration 11/1000 | Loss: 0.00001979
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001953
Iteration 14/1000 | Loss: 0.00001944
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001927
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001926
Iteration 19/1000 | Loss: 0.00001924
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001911
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001908
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001907
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001904
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001903
Iteration 35/1000 | Loss: 0.00001903
Iteration 36/1000 | Loss: 0.00001903
Iteration 37/1000 | Loss: 0.00001903
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001903
Iteration 40/1000 | Loss: 0.00001903
Iteration 41/1000 | Loss: 0.00001903
Iteration 42/1000 | Loss: 0.00001903
Iteration 43/1000 | Loss: 0.00001903
Iteration 44/1000 | Loss: 0.00001902
Iteration 45/1000 | Loss: 0.00001902
Iteration 46/1000 | Loss: 0.00001901
Iteration 47/1000 | Loss: 0.00001901
Iteration 48/1000 | Loss: 0.00001901
Iteration 49/1000 | Loss: 0.00001901
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001892
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001889
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001889
Iteration 66/1000 | Loss: 0.00001889
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001888
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001884
Iteration 74/1000 | Loss: 0.00001882
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00001881
Iteration 77/1000 | Loss: 0.00001881
Iteration 78/1000 | Loss: 0.00001881
Iteration 79/1000 | Loss: 0.00001879
Iteration 80/1000 | Loss: 0.00001879
Iteration 81/1000 | Loss: 0.00001879
Iteration 82/1000 | Loss: 0.00001879
Iteration 83/1000 | Loss: 0.00001879
Iteration 84/1000 | Loss: 0.00001879
Iteration 85/1000 | Loss: 0.00001879
Iteration 86/1000 | Loss: 0.00001879
Iteration 87/1000 | Loss: 0.00001878
Iteration 88/1000 | Loss: 0.00001878
Iteration 89/1000 | Loss: 0.00001878
Iteration 90/1000 | Loss: 0.00001878
Iteration 91/1000 | Loss: 0.00001878
Iteration 92/1000 | Loss: 0.00001878
Iteration 93/1000 | Loss: 0.00001878
Iteration 94/1000 | Loss: 0.00001878
Iteration 95/1000 | Loss: 0.00001877
Iteration 96/1000 | Loss: 0.00001877
Iteration 97/1000 | Loss: 0.00001877
Iteration 98/1000 | Loss: 0.00001877
Iteration 99/1000 | Loss: 0.00001877
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001876
Iteration 102/1000 | Loss: 0.00001876
Iteration 103/1000 | Loss: 0.00001875
Iteration 104/1000 | Loss: 0.00001875
Iteration 105/1000 | Loss: 0.00001875
Iteration 106/1000 | Loss: 0.00001875
Iteration 107/1000 | Loss: 0.00001875
Iteration 108/1000 | Loss: 0.00001874
Iteration 109/1000 | Loss: 0.00001874
Iteration 110/1000 | Loss: 0.00001874
Iteration 111/1000 | Loss: 0.00001874
Iteration 112/1000 | Loss: 0.00001874
Iteration 113/1000 | Loss: 0.00001874
Iteration 114/1000 | Loss: 0.00001873
Iteration 115/1000 | Loss: 0.00001873
Iteration 116/1000 | Loss: 0.00001873
Iteration 117/1000 | Loss: 0.00001873
Iteration 118/1000 | Loss: 0.00001873
Iteration 119/1000 | Loss: 0.00001873
Iteration 120/1000 | Loss: 0.00001873
Iteration 121/1000 | Loss: 0.00001873
Iteration 122/1000 | Loss: 0.00001873
Iteration 123/1000 | Loss: 0.00001873
Iteration 124/1000 | Loss: 0.00001873
Iteration 125/1000 | Loss: 0.00001873
Iteration 126/1000 | Loss: 0.00001873
Iteration 127/1000 | Loss: 0.00001872
Iteration 128/1000 | Loss: 0.00001872
Iteration 129/1000 | Loss: 0.00001872
Iteration 130/1000 | Loss: 0.00001872
Iteration 131/1000 | Loss: 0.00001872
Iteration 132/1000 | Loss: 0.00001872
Iteration 133/1000 | Loss: 0.00001872
Iteration 134/1000 | Loss: 0.00001872
Iteration 135/1000 | Loss: 0.00001872
Iteration 136/1000 | Loss: 0.00001872
Iteration 137/1000 | Loss: 0.00001872
Iteration 138/1000 | Loss: 0.00001872
Iteration 139/1000 | Loss: 0.00001872
Iteration 140/1000 | Loss: 0.00001872
Iteration 141/1000 | Loss: 0.00001872
Iteration 142/1000 | Loss: 0.00001872
Iteration 143/1000 | Loss: 0.00001872
Iteration 144/1000 | Loss: 0.00001872
Iteration 145/1000 | Loss: 0.00001872
Iteration 146/1000 | Loss: 0.00001872
Iteration 147/1000 | Loss: 0.00001872
Iteration 148/1000 | Loss: 0.00001872
Iteration 149/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.8721511878538877e-05, 1.8721511878538877e-05, 1.8721511878538877e-05, 1.8721511878538877e-05, 1.8721511878538877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8721511878538877e-05

Optimization complete. Final v2v error: 3.6021108627319336 mm

Highest mean error: 3.712217092514038 mm for frame 107

Lowest mean error: 3.5058562755584717 mm for frame 35

Saving results

Total time: 37.86296272277832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928968
Iteration 2/25 | Loss: 0.00928968
Iteration 3/25 | Loss: 0.00928968
Iteration 4/25 | Loss: 0.00310354
Iteration 5/25 | Loss: 0.00212190
Iteration 6/25 | Loss: 0.00183459
Iteration 7/25 | Loss: 0.00154098
Iteration 8/25 | Loss: 0.00144657
Iteration 9/25 | Loss: 0.00139120
Iteration 10/25 | Loss: 0.00134376
Iteration 11/25 | Loss: 0.00132131
Iteration 12/25 | Loss: 0.00131657
Iteration 13/25 | Loss: 0.00131688
Iteration 14/25 | Loss: 0.00130631
Iteration 15/25 | Loss: 0.00130453
Iteration 16/25 | Loss: 0.00130371
Iteration 17/25 | Loss: 0.00130256
Iteration 18/25 | Loss: 0.00130170
Iteration 19/25 | Loss: 0.00130153
Iteration 20/25 | Loss: 0.00130134
Iteration 21/25 | Loss: 0.00130114
Iteration 22/25 | Loss: 0.00130097
Iteration 23/25 | Loss: 0.00130151
Iteration 24/25 | Loss: 0.00130085
Iteration 25/25 | Loss: 0.00130080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27651966
Iteration 2/25 | Loss: 0.00159037
Iteration 3/25 | Loss: 0.00148734
Iteration 4/25 | Loss: 0.00148734
Iteration 5/25 | Loss: 0.00148734
Iteration 6/25 | Loss: 0.00148734
Iteration 7/25 | Loss: 0.00148734
Iteration 8/25 | Loss: 0.00148734
Iteration 9/25 | Loss: 0.00148734
Iteration 10/25 | Loss: 0.00148734
Iteration 11/25 | Loss: 0.00148734
Iteration 12/25 | Loss: 0.00148734
Iteration 13/25 | Loss: 0.00148734
Iteration 14/25 | Loss: 0.00148734
Iteration 15/25 | Loss: 0.00148734
Iteration 16/25 | Loss: 0.00148734
Iteration 17/25 | Loss: 0.00148734
Iteration 18/25 | Loss: 0.00148734
Iteration 19/25 | Loss: 0.00148734
Iteration 20/25 | Loss: 0.00148734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014873372856527567, 0.0014873372856527567, 0.0014873372856527567, 0.0014873372856527567, 0.0014873372856527567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014873372856527567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148734
Iteration 2/1000 | Loss: 0.00024434
Iteration 3/1000 | Loss: 0.00026752
Iteration 4/1000 | Loss: 0.00015806
Iteration 5/1000 | Loss: 0.00005729
Iteration 6/1000 | Loss: 0.00010684
Iteration 7/1000 | Loss: 0.00009235
Iteration 8/1000 | Loss: 0.00004529
Iteration 9/1000 | Loss: 0.00026270
Iteration 10/1000 | Loss: 0.00003941
Iteration 11/1000 | Loss: 0.00003682
Iteration 12/1000 | Loss: 0.00003423
Iteration 13/1000 | Loss: 0.00018418
Iteration 14/1000 | Loss: 0.00047782
Iteration 15/1000 | Loss: 0.00008436
Iteration 16/1000 | Loss: 0.00004194
Iteration 17/1000 | Loss: 0.00003421
Iteration 18/1000 | Loss: 0.00002802
Iteration 19/1000 | Loss: 0.00002336
Iteration 20/1000 | Loss: 0.00002260
Iteration 21/1000 | Loss: 0.00003134
Iteration 22/1000 | Loss: 0.00002019
Iteration 23/1000 | Loss: 0.00007629
Iteration 24/1000 | Loss: 0.00002887
Iteration 25/1000 | Loss: 0.00001752
Iteration 26/1000 | Loss: 0.00001695
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001586
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00003839
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00001564
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001506
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001505
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001505
Iteration 42/1000 | Loss: 0.00001505
Iteration 43/1000 | Loss: 0.00001505
Iteration 44/1000 | Loss: 0.00001505
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001503
Iteration 47/1000 | Loss: 0.00001503
Iteration 48/1000 | Loss: 0.00001503
Iteration 49/1000 | Loss: 0.00001503
Iteration 50/1000 | Loss: 0.00001503
Iteration 51/1000 | Loss: 0.00001502
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001498
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001498
Iteration 58/1000 | Loss: 0.00001498
Iteration 59/1000 | Loss: 0.00001497
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001496
Iteration 64/1000 | Loss: 0.00001496
Iteration 65/1000 | Loss: 0.00001495
Iteration 66/1000 | Loss: 0.00001494
Iteration 67/1000 | Loss: 0.00001494
Iteration 68/1000 | Loss: 0.00001494
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001493
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001492
Iteration 75/1000 | Loss: 0.00001492
Iteration 76/1000 | Loss: 0.00001492
Iteration 77/1000 | Loss: 0.00001492
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001491
Iteration 81/1000 | Loss: 0.00001491
Iteration 82/1000 | Loss: 0.00001491
Iteration 83/1000 | Loss: 0.00001491
Iteration 84/1000 | Loss: 0.00001491
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001491
Iteration 87/1000 | Loss: 0.00001491
Iteration 88/1000 | Loss: 0.00001491
Iteration 89/1000 | Loss: 0.00001491
Iteration 90/1000 | Loss: 0.00001491
Iteration 91/1000 | Loss: 0.00001491
Iteration 92/1000 | Loss: 0.00001491
Iteration 93/1000 | Loss: 0.00001491
Iteration 94/1000 | Loss: 0.00001491
Iteration 95/1000 | Loss: 0.00001491
Iteration 96/1000 | Loss: 0.00001491
Iteration 97/1000 | Loss: 0.00001491
Iteration 98/1000 | Loss: 0.00001491
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001490
Iteration 101/1000 | Loss: 0.00001490
Iteration 102/1000 | Loss: 0.00001490
Iteration 103/1000 | Loss: 0.00001490
Iteration 104/1000 | Loss: 0.00001490
Iteration 105/1000 | Loss: 0.00001490
Iteration 106/1000 | Loss: 0.00001490
Iteration 107/1000 | Loss: 0.00001490
Iteration 108/1000 | Loss: 0.00001490
Iteration 109/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.4904921044944786e-05, 1.4904921044944786e-05, 1.4904921044944786e-05, 1.4904921044944786e-05, 1.4904921044944786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4904921044944786e-05

Optimization complete. Final v2v error: 3.276487350463867 mm

Highest mean error: 3.778209686279297 mm for frame 78

Lowest mean error: 2.902069091796875 mm for frame 120

Saving results

Total time: 106.8896849155426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825873
Iteration 2/25 | Loss: 0.00129971
Iteration 3/25 | Loss: 0.00119012
Iteration 4/25 | Loss: 0.00117906
Iteration 5/25 | Loss: 0.00117804
Iteration 6/25 | Loss: 0.00117804
Iteration 7/25 | Loss: 0.00117804
Iteration 8/25 | Loss: 0.00117804
Iteration 9/25 | Loss: 0.00117804
Iteration 10/25 | Loss: 0.00117804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011780404020100832, 0.0011780404020100832, 0.0011780404020100832, 0.0011780404020100832, 0.0011780404020100832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011780404020100832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28487825
Iteration 2/25 | Loss: 0.00126806
Iteration 3/25 | Loss: 0.00126806
Iteration 4/25 | Loss: 0.00126806
Iteration 5/25 | Loss: 0.00126806
Iteration 6/25 | Loss: 0.00126806
Iteration 7/25 | Loss: 0.00126806
Iteration 8/25 | Loss: 0.00126806
Iteration 9/25 | Loss: 0.00126806
Iteration 10/25 | Loss: 0.00126806
Iteration 11/25 | Loss: 0.00126805
Iteration 12/25 | Loss: 0.00126805
Iteration 13/25 | Loss: 0.00126805
Iteration 14/25 | Loss: 0.00126805
Iteration 15/25 | Loss: 0.00126805
Iteration 16/25 | Loss: 0.00126805
Iteration 17/25 | Loss: 0.00126805
Iteration 18/25 | Loss: 0.00126805
Iteration 19/25 | Loss: 0.00126805
Iteration 20/25 | Loss: 0.00126805
Iteration 21/25 | Loss: 0.00126805
Iteration 22/25 | Loss: 0.00126805
Iteration 23/25 | Loss: 0.00126805
Iteration 24/25 | Loss: 0.00126805
Iteration 25/25 | Loss: 0.00126805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126805
Iteration 2/1000 | Loss: 0.00002155
Iteration 3/1000 | Loss: 0.00001474
Iteration 4/1000 | Loss: 0.00001347
Iteration 5/1000 | Loss: 0.00001264
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001142
Iteration 8/1000 | Loss: 0.00001120
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001062
Iteration 11/1000 | Loss: 0.00001048
Iteration 12/1000 | Loss: 0.00001040
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001033
Iteration 15/1000 | Loss: 0.00001018
Iteration 16/1000 | Loss: 0.00001015
Iteration 17/1000 | Loss: 0.00001013
Iteration 18/1000 | Loss: 0.00001010
Iteration 19/1000 | Loss: 0.00001000
Iteration 20/1000 | Loss: 0.00000995
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000985
Iteration 24/1000 | Loss: 0.00000981
Iteration 25/1000 | Loss: 0.00000981
Iteration 26/1000 | Loss: 0.00000980
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000979
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000976
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000975
Iteration 33/1000 | Loss: 0.00000975
Iteration 34/1000 | Loss: 0.00000975
Iteration 35/1000 | Loss: 0.00000974
Iteration 36/1000 | Loss: 0.00000974
Iteration 37/1000 | Loss: 0.00000974
Iteration 38/1000 | Loss: 0.00000971
Iteration 39/1000 | Loss: 0.00000969
Iteration 40/1000 | Loss: 0.00000969
Iteration 41/1000 | Loss: 0.00000968
Iteration 42/1000 | Loss: 0.00000967
Iteration 43/1000 | Loss: 0.00000966
Iteration 44/1000 | Loss: 0.00000965
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000964
Iteration 47/1000 | Loss: 0.00000964
Iteration 48/1000 | Loss: 0.00000963
Iteration 49/1000 | Loss: 0.00000963
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000960
Iteration 52/1000 | Loss: 0.00000960
Iteration 53/1000 | Loss: 0.00000959
Iteration 54/1000 | Loss: 0.00000959
Iteration 55/1000 | Loss: 0.00000959
Iteration 56/1000 | Loss: 0.00000959
Iteration 57/1000 | Loss: 0.00000958
Iteration 58/1000 | Loss: 0.00000957
Iteration 59/1000 | Loss: 0.00000957
Iteration 60/1000 | Loss: 0.00000957
Iteration 61/1000 | Loss: 0.00000956
Iteration 62/1000 | Loss: 0.00000956
Iteration 63/1000 | Loss: 0.00000955
Iteration 64/1000 | Loss: 0.00000955
Iteration 65/1000 | Loss: 0.00000955
Iteration 66/1000 | Loss: 0.00000954
Iteration 67/1000 | Loss: 0.00000953
Iteration 68/1000 | Loss: 0.00000953
Iteration 69/1000 | Loss: 0.00000953
Iteration 70/1000 | Loss: 0.00000953
Iteration 71/1000 | Loss: 0.00000953
Iteration 72/1000 | Loss: 0.00000952
Iteration 73/1000 | Loss: 0.00000952
Iteration 74/1000 | Loss: 0.00000952
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000951
Iteration 77/1000 | Loss: 0.00000950
Iteration 78/1000 | Loss: 0.00000949
Iteration 79/1000 | Loss: 0.00000949
Iteration 80/1000 | Loss: 0.00000949
Iteration 81/1000 | Loss: 0.00000947
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000945
Iteration 85/1000 | Loss: 0.00000944
Iteration 86/1000 | Loss: 0.00000944
Iteration 87/1000 | Loss: 0.00000944
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000943
Iteration 90/1000 | Loss: 0.00000943
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000943
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000941
Iteration 99/1000 | Loss: 0.00000941
Iteration 100/1000 | Loss: 0.00000941
Iteration 101/1000 | Loss: 0.00000941
Iteration 102/1000 | Loss: 0.00000940
Iteration 103/1000 | Loss: 0.00000940
Iteration 104/1000 | Loss: 0.00000940
Iteration 105/1000 | Loss: 0.00000939
Iteration 106/1000 | Loss: 0.00000939
Iteration 107/1000 | Loss: 0.00000939
Iteration 108/1000 | Loss: 0.00000939
Iteration 109/1000 | Loss: 0.00000939
Iteration 110/1000 | Loss: 0.00000939
Iteration 111/1000 | Loss: 0.00000939
Iteration 112/1000 | Loss: 0.00000938
Iteration 113/1000 | Loss: 0.00000938
Iteration 114/1000 | Loss: 0.00000937
Iteration 115/1000 | Loss: 0.00000937
Iteration 116/1000 | Loss: 0.00000937
Iteration 117/1000 | Loss: 0.00000937
Iteration 118/1000 | Loss: 0.00000937
Iteration 119/1000 | Loss: 0.00000937
Iteration 120/1000 | Loss: 0.00000937
Iteration 121/1000 | Loss: 0.00000936
Iteration 122/1000 | Loss: 0.00000936
Iteration 123/1000 | Loss: 0.00000936
Iteration 124/1000 | Loss: 0.00000936
Iteration 125/1000 | Loss: 0.00000936
Iteration 126/1000 | Loss: 0.00000936
Iteration 127/1000 | Loss: 0.00000936
Iteration 128/1000 | Loss: 0.00000936
Iteration 129/1000 | Loss: 0.00000936
Iteration 130/1000 | Loss: 0.00000936
Iteration 131/1000 | Loss: 0.00000936
Iteration 132/1000 | Loss: 0.00000936
Iteration 133/1000 | Loss: 0.00000936
Iteration 134/1000 | Loss: 0.00000936
Iteration 135/1000 | Loss: 0.00000936
Iteration 136/1000 | Loss: 0.00000936
Iteration 137/1000 | Loss: 0.00000936
Iteration 138/1000 | Loss: 0.00000936
Iteration 139/1000 | Loss: 0.00000936
Iteration 140/1000 | Loss: 0.00000936
Iteration 141/1000 | Loss: 0.00000936
Iteration 142/1000 | Loss: 0.00000936
Iteration 143/1000 | Loss: 0.00000936
Iteration 144/1000 | Loss: 0.00000936
Iteration 145/1000 | Loss: 0.00000936
Iteration 146/1000 | Loss: 0.00000936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [9.357087037642486e-06, 9.357087037642486e-06, 9.357087037642486e-06, 9.357087037642486e-06, 9.357087037642486e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.357087037642486e-06

Optimization complete. Final v2v error: 2.623297929763794 mm

Highest mean error: 2.7689359188079834 mm for frame 118

Lowest mean error: 2.438905715942383 mm for frame 241

Saving results

Total time: 45.244452714920044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031779
Iteration 2/25 | Loss: 0.00148275
Iteration 3/25 | Loss: 0.00126721
Iteration 4/25 | Loss: 0.00125764
Iteration 5/25 | Loss: 0.00123741
Iteration 6/25 | Loss: 0.00121999
Iteration 7/25 | Loss: 0.00121613
Iteration 8/25 | Loss: 0.00121046
Iteration 9/25 | Loss: 0.00121149
Iteration 10/25 | Loss: 0.00121328
Iteration 11/25 | Loss: 0.00120964
Iteration 12/25 | Loss: 0.00120357
Iteration 13/25 | Loss: 0.00120162
Iteration 14/25 | Loss: 0.00120095
Iteration 15/25 | Loss: 0.00120453
Iteration 16/25 | Loss: 0.00120133
Iteration 17/25 | Loss: 0.00120101
Iteration 18/25 | Loss: 0.00119928
Iteration 19/25 | Loss: 0.00119874
Iteration 20/25 | Loss: 0.00119845
Iteration 21/25 | Loss: 0.00119825
Iteration 22/25 | Loss: 0.00119810
Iteration 23/25 | Loss: 0.00119803
Iteration 24/25 | Loss: 0.00119803
Iteration 25/25 | Loss: 0.00119802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50397420
Iteration 2/25 | Loss: 0.00146101
Iteration 3/25 | Loss: 0.00146101
Iteration 4/25 | Loss: 0.00146100
Iteration 5/25 | Loss: 0.00146100
Iteration 6/25 | Loss: 0.00146100
Iteration 7/25 | Loss: 0.00146100
Iteration 8/25 | Loss: 0.00146100
Iteration 9/25 | Loss: 0.00146100
Iteration 10/25 | Loss: 0.00146100
Iteration 11/25 | Loss: 0.00146100
Iteration 12/25 | Loss: 0.00146100
Iteration 13/25 | Loss: 0.00146100
Iteration 14/25 | Loss: 0.00146100
Iteration 15/25 | Loss: 0.00146100
Iteration 16/25 | Loss: 0.00146100
Iteration 17/25 | Loss: 0.00146100
Iteration 18/25 | Loss: 0.00146100
Iteration 19/25 | Loss: 0.00146100
Iteration 20/25 | Loss: 0.00146100
Iteration 21/25 | Loss: 0.00146100
Iteration 22/25 | Loss: 0.00146100
Iteration 23/25 | Loss: 0.00146100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001461002859286964, 0.001461002859286964, 0.001461002859286964, 0.001461002859286964, 0.001461002859286964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001461002859286964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146100
Iteration 2/1000 | Loss: 0.00002353
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001571
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001398
Iteration 7/1000 | Loss: 0.00001360
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001237
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001218
Iteration 16/1000 | Loss: 0.00014125
Iteration 17/1000 | Loss: 0.00009952
Iteration 18/1000 | Loss: 0.00012292
Iteration 19/1000 | Loss: 0.00006584
Iteration 20/1000 | Loss: 0.00005489
Iteration 21/1000 | Loss: 0.00012910
Iteration 22/1000 | Loss: 0.00003124
Iteration 23/1000 | Loss: 0.00002004
Iteration 24/1000 | Loss: 0.00001523
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001171
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001164
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001161
Iteration 41/1000 | Loss: 0.00001160
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001156
Iteration 47/1000 | Loss: 0.00001155
Iteration 48/1000 | Loss: 0.00001155
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001154
Iteration 53/1000 | Loss: 0.00001154
Iteration 54/1000 | Loss: 0.00001154
Iteration 55/1000 | Loss: 0.00001154
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001153
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001153
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00001153
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001152
Iteration 65/1000 | Loss: 0.00001152
Iteration 66/1000 | Loss: 0.00001152
Iteration 67/1000 | Loss: 0.00001152
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001151
Iteration 72/1000 | Loss: 0.00001151
Iteration 73/1000 | Loss: 0.00001151
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001150
Iteration 76/1000 | Loss: 0.00001150
Iteration 77/1000 | Loss: 0.00001150
Iteration 78/1000 | Loss: 0.00001150
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001148
Iteration 91/1000 | Loss: 0.00001148
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001146
Iteration 99/1000 | Loss: 0.00001146
Iteration 100/1000 | Loss: 0.00001145
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001145
Iteration 103/1000 | Loss: 0.00001145
Iteration 104/1000 | Loss: 0.00001145
Iteration 105/1000 | Loss: 0.00001145
Iteration 106/1000 | Loss: 0.00001145
Iteration 107/1000 | Loss: 0.00001145
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001144
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001141
Iteration 142/1000 | Loss: 0.00001141
Iteration 143/1000 | Loss: 0.00001141
Iteration 144/1000 | Loss: 0.00001141
Iteration 145/1000 | Loss: 0.00001141
Iteration 146/1000 | Loss: 0.00001141
Iteration 147/1000 | Loss: 0.00001140
Iteration 148/1000 | Loss: 0.00001140
Iteration 149/1000 | Loss: 0.00001140
Iteration 150/1000 | Loss: 0.00001140
Iteration 151/1000 | Loss: 0.00001140
Iteration 152/1000 | Loss: 0.00001140
Iteration 153/1000 | Loss: 0.00001140
Iteration 154/1000 | Loss: 0.00001140
Iteration 155/1000 | Loss: 0.00001140
Iteration 156/1000 | Loss: 0.00001140
Iteration 157/1000 | Loss: 0.00001140
Iteration 158/1000 | Loss: 0.00001140
Iteration 159/1000 | Loss: 0.00001140
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001139
Iteration 162/1000 | Loss: 0.00001139
Iteration 163/1000 | Loss: 0.00001139
Iteration 164/1000 | Loss: 0.00001139
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001139
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001139
Iteration 172/1000 | Loss: 0.00001139
Iteration 173/1000 | Loss: 0.00001139
Iteration 174/1000 | Loss: 0.00001139
Iteration 175/1000 | Loss: 0.00001139
Iteration 176/1000 | Loss: 0.00001139
Iteration 177/1000 | Loss: 0.00001139
Iteration 178/1000 | Loss: 0.00001139
Iteration 179/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1388311577320565e-05, 1.1388311577320565e-05, 1.1388311577320565e-05, 1.1388311577320565e-05, 1.1388311577320565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1388311577320565e-05

Optimization complete. Final v2v error: 2.854295015335083 mm

Highest mean error: 5.049068927764893 mm for frame 1

Lowest mean error: 2.5549235343933105 mm for frame 205

Saving results

Total time: 100.12744951248169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820358
Iteration 2/25 | Loss: 0.00208855
Iteration 3/25 | Loss: 0.00152554
Iteration 4/25 | Loss: 0.00143922
Iteration 5/25 | Loss: 0.00142640
Iteration 6/25 | Loss: 0.00142319
Iteration 7/25 | Loss: 0.00141433
Iteration 8/25 | Loss: 0.00140912
Iteration 9/25 | Loss: 0.00140772
Iteration 10/25 | Loss: 0.00140406
Iteration 11/25 | Loss: 0.00140204
Iteration 12/25 | Loss: 0.00140095
Iteration 13/25 | Loss: 0.00139994
Iteration 14/25 | Loss: 0.00139942
Iteration 15/25 | Loss: 0.00139889
Iteration 16/25 | Loss: 0.00139879
Iteration 17/25 | Loss: 0.00139878
Iteration 18/25 | Loss: 0.00139878
Iteration 19/25 | Loss: 0.00139878
Iteration 20/25 | Loss: 0.00139878
Iteration 21/25 | Loss: 0.00139878
Iteration 22/25 | Loss: 0.00139878
Iteration 23/25 | Loss: 0.00139878
Iteration 24/25 | Loss: 0.00139878
Iteration 25/25 | Loss: 0.00139878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13887036
Iteration 2/25 | Loss: 0.00113198
Iteration 3/25 | Loss: 0.00113195
Iteration 4/25 | Loss: 0.00113195
Iteration 5/25 | Loss: 0.00113195
Iteration 6/25 | Loss: 0.00113195
Iteration 7/25 | Loss: 0.00113195
Iteration 8/25 | Loss: 0.00113195
Iteration 9/25 | Loss: 0.00113195
Iteration 10/25 | Loss: 0.00113195
Iteration 11/25 | Loss: 0.00113195
Iteration 12/25 | Loss: 0.00113195
Iteration 13/25 | Loss: 0.00113195
Iteration 14/25 | Loss: 0.00113195
Iteration 15/25 | Loss: 0.00113195
Iteration 16/25 | Loss: 0.00113195
Iteration 17/25 | Loss: 0.00113195
Iteration 18/25 | Loss: 0.00113195
Iteration 19/25 | Loss: 0.00113195
Iteration 20/25 | Loss: 0.00113195
Iteration 21/25 | Loss: 0.00113195
Iteration 22/25 | Loss: 0.00113195
Iteration 23/25 | Loss: 0.00113195
Iteration 24/25 | Loss: 0.00113195
Iteration 25/25 | Loss: 0.00113195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113195
Iteration 2/1000 | Loss: 0.00004779
Iteration 3/1000 | Loss: 0.00003131
Iteration 4/1000 | Loss: 0.00002876
Iteration 5/1000 | Loss: 0.00002776
Iteration 6/1000 | Loss: 0.00002693
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00002624
Iteration 9/1000 | Loss: 0.00002600
Iteration 10/1000 | Loss: 0.00002593
Iteration 11/1000 | Loss: 0.00002585
Iteration 12/1000 | Loss: 0.00002582
Iteration 13/1000 | Loss: 0.00002581
Iteration 14/1000 | Loss: 0.00002576
Iteration 15/1000 | Loss: 0.00002573
Iteration 16/1000 | Loss: 0.00002571
Iteration 17/1000 | Loss: 0.00002569
Iteration 18/1000 | Loss: 0.00002569
Iteration 19/1000 | Loss: 0.00002568
Iteration 20/1000 | Loss: 0.00002568
Iteration 21/1000 | Loss: 0.00002568
Iteration 22/1000 | Loss: 0.00002567
Iteration 23/1000 | Loss: 0.00002567
Iteration 24/1000 | Loss: 0.00002563
Iteration 25/1000 | Loss: 0.00002563
Iteration 26/1000 | Loss: 0.00002562
Iteration 27/1000 | Loss: 0.00002560
Iteration 28/1000 | Loss: 0.00002557
Iteration 29/1000 | Loss: 0.00002552
Iteration 30/1000 | Loss: 0.00002552
Iteration 31/1000 | Loss: 0.00002552
Iteration 32/1000 | Loss: 0.00002550
Iteration 33/1000 | Loss: 0.00002550
Iteration 34/1000 | Loss: 0.00002549
Iteration 35/1000 | Loss: 0.00002549
Iteration 36/1000 | Loss: 0.00002549
Iteration 37/1000 | Loss: 0.00002549
Iteration 38/1000 | Loss: 0.00002549
Iteration 39/1000 | Loss: 0.00002548
Iteration 40/1000 | Loss: 0.00002548
Iteration 41/1000 | Loss: 0.00002547
Iteration 42/1000 | Loss: 0.00002547
Iteration 43/1000 | Loss: 0.00002547
Iteration 44/1000 | Loss: 0.00002547
Iteration 45/1000 | Loss: 0.00002546
Iteration 46/1000 | Loss: 0.00002546
Iteration 47/1000 | Loss: 0.00002546
Iteration 48/1000 | Loss: 0.00002546
Iteration 49/1000 | Loss: 0.00002546
Iteration 50/1000 | Loss: 0.00002545
Iteration 51/1000 | Loss: 0.00002545
Iteration 52/1000 | Loss: 0.00002545
Iteration 53/1000 | Loss: 0.00002545
Iteration 54/1000 | Loss: 0.00002545
Iteration 55/1000 | Loss: 0.00002545
Iteration 56/1000 | Loss: 0.00002545
Iteration 57/1000 | Loss: 0.00002545
Iteration 58/1000 | Loss: 0.00002545
Iteration 59/1000 | Loss: 0.00002544
Iteration 60/1000 | Loss: 0.00002544
Iteration 61/1000 | Loss: 0.00002544
Iteration 62/1000 | Loss: 0.00002543
Iteration 63/1000 | Loss: 0.00002542
Iteration 64/1000 | Loss: 0.00002542
Iteration 65/1000 | Loss: 0.00002542
Iteration 66/1000 | Loss: 0.00002542
Iteration 67/1000 | Loss: 0.00002542
Iteration 68/1000 | Loss: 0.00002542
Iteration 69/1000 | Loss: 0.00002542
Iteration 70/1000 | Loss: 0.00002541
Iteration 71/1000 | Loss: 0.00002541
Iteration 72/1000 | Loss: 0.00002541
Iteration 73/1000 | Loss: 0.00002541
Iteration 74/1000 | Loss: 0.00002540
Iteration 75/1000 | Loss: 0.00002540
Iteration 76/1000 | Loss: 0.00002539
Iteration 77/1000 | Loss: 0.00002539
Iteration 78/1000 | Loss: 0.00002539
Iteration 79/1000 | Loss: 0.00002539
Iteration 80/1000 | Loss: 0.00002539
Iteration 81/1000 | Loss: 0.00002539
Iteration 82/1000 | Loss: 0.00002539
Iteration 83/1000 | Loss: 0.00002539
Iteration 84/1000 | Loss: 0.00002538
Iteration 85/1000 | Loss: 0.00002538
Iteration 86/1000 | Loss: 0.00002538
Iteration 87/1000 | Loss: 0.00002538
Iteration 88/1000 | Loss: 0.00002538
Iteration 89/1000 | Loss: 0.00002538
Iteration 90/1000 | Loss: 0.00002538
Iteration 91/1000 | Loss: 0.00002538
Iteration 92/1000 | Loss: 0.00002538
Iteration 93/1000 | Loss: 0.00002538
Iteration 94/1000 | Loss: 0.00002538
Iteration 95/1000 | Loss: 0.00002537
Iteration 96/1000 | Loss: 0.00002537
Iteration 97/1000 | Loss: 0.00002537
Iteration 98/1000 | Loss: 0.00002537
Iteration 99/1000 | Loss: 0.00002537
Iteration 100/1000 | Loss: 0.00002537
Iteration 101/1000 | Loss: 0.00002537
Iteration 102/1000 | Loss: 0.00002537
Iteration 103/1000 | Loss: 0.00002537
Iteration 104/1000 | Loss: 0.00002537
Iteration 105/1000 | Loss: 0.00002537
Iteration 106/1000 | Loss: 0.00002537
Iteration 107/1000 | Loss: 0.00002537
Iteration 108/1000 | Loss: 0.00002536
Iteration 109/1000 | Loss: 0.00002536
Iteration 110/1000 | Loss: 0.00002536
Iteration 111/1000 | Loss: 0.00002536
Iteration 112/1000 | Loss: 0.00002536
Iteration 113/1000 | Loss: 0.00002536
Iteration 114/1000 | Loss: 0.00002536
Iteration 115/1000 | Loss: 0.00002536
Iteration 116/1000 | Loss: 0.00002536
Iteration 117/1000 | Loss: 0.00002536
Iteration 118/1000 | Loss: 0.00002536
Iteration 119/1000 | Loss: 0.00002536
Iteration 120/1000 | Loss: 0.00002536
Iteration 121/1000 | Loss: 0.00002536
Iteration 122/1000 | Loss: 0.00002536
Iteration 123/1000 | Loss: 0.00002536
Iteration 124/1000 | Loss: 0.00002536
Iteration 125/1000 | Loss: 0.00002536
Iteration 126/1000 | Loss: 0.00002536
Iteration 127/1000 | Loss: 0.00002536
Iteration 128/1000 | Loss: 0.00002536
Iteration 129/1000 | Loss: 0.00002536
Iteration 130/1000 | Loss: 0.00002536
Iteration 131/1000 | Loss: 0.00002536
Iteration 132/1000 | Loss: 0.00002536
Iteration 133/1000 | Loss: 0.00002536
Iteration 134/1000 | Loss: 0.00002536
Iteration 135/1000 | Loss: 0.00002536
Iteration 136/1000 | Loss: 0.00002536
Iteration 137/1000 | Loss: 0.00002536
Iteration 138/1000 | Loss: 0.00002536
Iteration 139/1000 | Loss: 0.00002536
Iteration 140/1000 | Loss: 0.00002536
Iteration 141/1000 | Loss: 0.00002536
Iteration 142/1000 | Loss: 0.00002536
Iteration 143/1000 | Loss: 0.00002536
Iteration 144/1000 | Loss: 0.00002536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.5363300665048882e-05, 2.5363300665048882e-05, 2.5363300665048882e-05, 2.5363300665048882e-05, 2.5363300665048882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5363300665048882e-05

Optimization complete. Final v2v error: 4.137465000152588 mm

Highest mean error: 4.353182792663574 mm for frame 148

Lowest mean error: 4.006862640380859 mm for frame 45

Saving results

Total time: 58.741488456726074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401593
Iteration 2/25 | Loss: 0.00092747
Iteration 3/25 | Loss: 0.00071314
Iteration 4/25 | Loss: 0.00068605
Iteration 5/25 | Loss: 0.00067664
Iteration 6/25 | Loss: 0.00067389
Iteration 7/25 | Loss: 0.00067320
Iteration 8/25 | Loss: 0.00067312
Iteration 9/25 | Loss: 0.00067312
Iteration 10/25 | Loss: 0.00067312
Iteration 11/25 | Loss: 0.00067312
Iteration 12/25 | Loss: 0.00067312
Iteration 13/25 | Loss: 0.00067312
Iteration 14/25 | Loss: 0.00067312
Iteration 15/25 | Loss: 0.00067312
Iteration 16/25 | Loss: 0.00067312
Iteration 17/25 | Loss: 0.00067312
Iteration 18/25 | Loss: 0.00067312
Iteration 19/25 | Loss: 0.00067312
Iteration 20/25 | Loss: 0.00067312
Iteration 21/25 | Loss: 0.00067312
Iteration 22/25 | Loss: 0.00067312
Iteration 23/25 | Loss: 0.00067312
Iteration 24/25 | Loss: 0.00067312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006731165340170264, 0.0006731165340170264, 0.0006731165340170264, 0.0006731165340170264, 0.0006731165340170264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006731165340170264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50109935
Iteration 2/25 | Loss: 0.00024990
Iteration 3/25 | Loss: 0.00024989
Iteration 4/25 | Loss: 0.00024989
Iteration 5/25 | Loss: 0.00024989
Iteration 6/25 | Loss: 0.00024989
Iteration 7/25 | Loss: 0.00024989
Iteration 8/25 | Loss: 0.00024989
Iteration 9/25 | Loss: 0.00024989
Iteration 10/25 | Loss: 0.00024989
Iteration 11/25 | Loss: 0.00024989
Iteration 12/25 | Loss: 0.00024989
Iteration 13/25 | Loss: 0.00024989
Iteration 14/25 | Loss: 0.00024989
Iteration 15/25 | Loss: 0.00024989
Iteration 16/25 | Loss: 0.00024989
Iteration 17/25 | Loss: 0.00024989
Iteration 18/25 | Loss: 0.00024989
Iteration 19/25 | Loss: 0.00024989
Iteration 20/25 | Loss: 0.00024989
Iteration 21/25 | Loss: 0.00024989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00024989157100208104, 0.00024989157100208104, 0.00024989157100208104, 0.00024989157100208104, 0.00024989157100208104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024989157100208104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024989
Iteration 2/1000 | Loss: 0.00005584
Iteration 3/1000 | Loss: 0.00002329
Iteration 4/1000 | Loss: 0.00001816
Iteration 5/1000 | Loss: 0.00001666
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001492
Iteration 8/1000 | Loss: 0.00001456
Iteration 9/1000 | Loss: 0.00001423
Iteration 10/1000 | Loss: 0.00001402
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001365
Iteration 16/1000 | Loss: 0.00001360
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001353
Iteration 19/1000 | Loss: 0.00001353
Iteration 20/1000 | Loss: 0.00001352
Iteration 21/1000 | Loss: 0.00001351
Iteration 22/1000 | Loss: 0.00001350
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001348
Iteration 25/1000 | Loss: 0.00001348
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001338
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001337
Iteration 43/1000 | Loss: 0.00001337
Iteration 44/1000 | Loss: 0.00001336
Iteration 45/1000 | Loss: 0.00001336
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001333
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001327
Iteration 149/1000 | Loss: 0.00001327
Iteration 150/1000 | Loss: 0.00001327
Iteration 151/1000 | Loss: 0.00001327
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Iteration 158/1000 | Loss: 0.00001327
Iteration 159/1000 | Loss: 0.00001327
Iteration 160/1000 | Loss: 0.00001327
Iteration 161/1000 | Loss: 0.00001327
Iteration 162/1000 | Loss: 0.00001327
Iteration 163/1000 | Loss: 0.00001327
Iteration 164/1000 | Loss: 0.00001327
Iteration 165/1000 | Loss: 0.00001327
Iteration 166/1000 | Loss: 0.00001327
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Iteration 172/1000 | Loss: 0.00001327
Iteration 173/1000 | Loss: 0.00001327
Iteration 174/1000 | Loss: 0.00001327
Iteration 175/1000 | Loss: 0.00001327
Iteration 176/1000 | Loss: 0.00001327
Iteration 177/1000 | Loss: 0.00001327
Iteration 178/1000 | Loss: 0.00001327
Iteration 179/1000 | Loss: 0.00001327
Iteration 180/1000 | Loss: 0.00001327
Iteration 181/1000 | Loss: 0.00001327
Iteration 182/1000 | Loss: 0.00001327
Iteration 183/1000 | Loss: 0.00001327
Iteration 184/1000 | Loss: 0.00001327
Iteration 185/1000 | Loss: 0.00001327
Iteration 186/1000 | Loss: 0.00001327
Iteration 187/1000 | Loss: 0.00001327
Iteration 188/1000 | Loss: 0.00001327
Iteration 189/1000 | Loss: 0.00001327
Iteration 190/1000 | Loss: 0.00001327
Iteration 191/1000 | Loss: 0.00001327
Iteration 192/1000 | Loss: 0.00001327
Iteration 193/1000 | Loss: 0.00001327
Iteration 194/1000 | Loss: 0.00001327
Iteration 195/1000 | Loss: 0.00001327
Iteration 196/1000 | Loss: 0.00001327
Iteration 197/1000 | Loss: 0.00001327
Iteration 198/1000 | Loss: 0.00001327
Iteration 199/1000 | Loss: 0.00001327
Iteration 200/1000 | Loss: 0.00001327
Iteration 201/1000 | Loss: 0.00001327
Iteration 202/1000 | Loss: 0.00001327
Iteration 203/1000 | Loss: 0.00001327
Iteration 204/1000 | Loss: 0.00001327
Iteration 205/1000 | Loss: 0.00001327
Iteration 206/1000 | Loss: 0.00001327
Iteration 207/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.3265243978821672e-05, 1.3265243978821672e-05, 1.3265243978821672e-05, 1.3265243978821672e-05, 1.3265243978821672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3265243978821672e-05

Optimization complete. Final v2v error: 3.1079816818237305 mm

Highest mean error: 4.115713119506836 mm for frame 57

Lowest mean error: 2.719639301300049 mm for frame 1

Saving results

Total time: 39.56946587562561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053081
Iteration 2/25 | Loss: 0.01053080
Iteration 3/25 | Loss: 0.01053080
Iteration 4/25 | Loss: 0.00339166
Iteration 5/25 | Loss: 0.00193173
Iteration 6/25 | Loss: 0.00203818
Iteration 7/25 | Loss: 0.00177134
Iteration 8/25 | Loss: 0.00212064
Iteration 9/25 | Loss: 0.00167241
Iteration 10/25 | Loss: 0.00163378
Iteration 11/25 | Loss: 0.00149376
Iteration 12/25 | Loss: 0.00146473
Iteration 13/25 | Loss: 0.00142602
Iteration 14/25 | Loss: 0.00135831
Iteration 15/25 | Loss: 0.00130498
Iteration 16/25 | Loss: 0.00123856
Iteration 17/25 | Loss: 0.00127007
Iteration 18/25 | Loss: 0.00123804
Iteration 19/25 | Loss: 0.00122603
Iteration 20/25 | Loss: 0.00120108
Iteration 21/25 | Loss: 0.00119009
Iteration 22/25 | Loss: 0.00119716
Iteration 23/25 | Loss: 0.00117234
Iteration 24/25 | Loss: 0.00117792
Iteration 25/25 | Loss: 0.00116782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44602323
Iteration 2/25 | Loss: 0.00601862
Iteration 3/25 | Loss: 0.00443786
Iteration 4/25 | Loss: 0.00443785
Iteration 5/25 | Loss: 0.00443784
Iteration 6/25 | Loss: 0.00443784
Iteration 7/25 | Loss: 0.00443784
Iteration 8/25 | Loss: 0.00443784
Iteration 9/25 | Loss: 0.00443784
Iteration 10/25 | Loss: 0.00443784
Iteration 11/25 | Loss: 0.00443784
Iteration 12/25 | Loss: 0.00443784
Iteration 13/25 | Loss: 0.00443784
Iteration 14/25 | Loss: 0.00443784
Iteration 15/25 | Loss: 0.00443784
Iteration 16/25 | Loss: 0.00443784
Iteration 17/25 | Loss: 0.00443784
Iteration 18/25 | Loss: 0.00443784
Iteration 19/25 | Loss: 0.00443784
Iteration 20/25 | Loss: 0.00443784
Iteration 21/25 | Loss: 0.00443784
Iteration 22/25 | Loss: 0.00443784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.004437840078026056, 0.004437840078026056, 0.004437840078026056, 0.004437840078026056, 0.004437840078026056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004437840078026056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00443784
Iteration 2/1000 | Loss: 0.00218625
Iteration 3/1000 | Loss: 0.00806078
Iteration 4/1000 | Loss: 0.01019695
Iteration 5/1000 | Loss: 0.00395687
Iteration 6/1000 | Loss: 0.00305097
Iteration 7/1000 | Loss: 0.00566995
Iteration 8/1000 | Loss: 0.00520204
Iteration 9/1000 | Loss: 0.00521772
Iteration 10/1000 | Loss: 0.00116892
Iteration 11/1000 | Loss: 0.00277812
Iteration 12/1000 | Loss: 0.00191032
Iteration 13/1000 | Loss: 0.00140585
Iteration 14/1000 | Loss: 0.00245581
Iteration 15/1000 | Loss: 0.00142534
Iteration 16/1000 | Loss: 0.00098667
Iteration 17/1000 | Loss: 0.00105807
Iteration 18/1000 | Loss: 0.00087690
Iteration 19/1000 | Loss: 0.00131130
Iteration 20/1000 | Loss: 0.00047099
Iteration 21/1000 | Loss: 0.00247478
Iteration 22/1000 | Loss: 0.00597883
Iteration 23/1000 | Loss: 0.00260419
Iteration 24/1000 | Loss: 0.00366209
Iteration 25/1000 | Loss: 0.00193982
Iteration 26/1000 | Loss: 0.00141949
Iteration 27/1000 | Loss: 0.00179447
Iteration 28/1000 | Loss: 0.00173377
Iteration 29/1000 | Loss: 0.00148328
Iteration 30/1000 | Loss: 0.00206855
Iteration 31/1000 | Loss: 0.00322423
Iteration 32/1000 | Loss: 0.00163714
Iteration 33/1000 | Loss: 0.00300203
Iteration 34/1000 | Loss: 0.00066384
Iteration 35/1000 | Loss: 0.00093310
Iteration 36/1000 | Loss: 0.00166985
Iteration 37/1000 | Loss: 0.00105783
Iteration 38/1000 | Loss: 0.00135810
Iteration 39/1000 | Loss: 0.00100119
Iteration 40/1000 | Loss: 0.00096293
Iteration 41/1000 | Loss: 0.00099009
Iteration 42/1000 | Loss: 0.00066639
Iteration 43/1000 | Loss: 0.00131147
Iteration 44/1000 | Loss: 0.00183247
Iteration 45/1000 | Loss: 0.00033362
Iteration 46/1000 | Loss: 0.00085236
Iteration 47/1000 | Loss: 0.00073130
Iteration 48/1000 | Loss: 0.00309273
Iteration 49/1000 | Loss: 0.00168107
Iteration 50/1000 | Loss: 0.00167098
Iteration 51/1000 | Loss: 0.00239628
Iteration 52/1000 | Loss: 0.00411524
Iteration 53/1000 | Loss: 0.00750987
Iteration 54/1000 | Loss: 0.00161823
Iteration 55/1000 | Loss: 0.00300317
Iteration 56/1000 | Loss: 0.00081747
Iteration 57/1000 | Loss: 0.00180410
Iteration 58/1000 | Loss: 0.00095681
Iteration 59/1000 | Loss: 0.00195958
Iteration 60/1000 | Loss: 0.00317629
Iteration 61/1000 | Loss: 0.00132685
Iteration 62/1000 | Loss: 0.00102495
Iteration 63/1000 | Loss: 0.00194527
Iteration 64/1000 | Loss: 0.00095203
Iteration 65/1000 | Loss: 0.00055052
Iteration 66/1000 | Loss: 0.00056217
Iteration 67/1000 | Loss: 0.00094758
Iteration 68/1000 | Loss: 0.00303449
Iteration 69/1000 | Loss: 0.00066802
Iteration 70/1000 | Loss: 0.00059863
Iteration 71/1000 | Loss: 0.00125080
Iteration 72/1000 | Loss: 0.00312055
Iteration 73/1000 | Loss: 0.00220980
Iteration 74/1000 | Loss: 0.00254012
Iteration 75/1000 | Loss: 0.00117981
Iteration 76/1000 | Loss: 0.00091675
Iteration 77/1000 | Loss: 0.00147802
Iteration 78/1000 | Loss: 0.00079668
Iteration 79/1000 | Loss: 0.00093924
Iteration 80/1000 | Loss: 0.00094198
Iteration 81/1000 | Loss: 0.00146564
Iteration 82/1000 | Loss: 0.00145812
Iteration 83/1000 | Loss: 0.00449036
Iteration 84/1000 | Loss: 0.00072608
Iteration 85/1000 | Loss: 0.00147543
Iteration 86/1000 | Loss: 0.00095507
Iteration 87/1000 | Loss: 0.00038396
Iteration 88/1000 | Loss: 0.00147757
Iteration 89/1000 | Loss: 0.00176563
Iteration 90/1000 | Loss: 0.00056267
Iteration 91/1000 | Loss: 0.00068318
Iteration 92/1000 | Loss: 0.00047185
Iteration 93/1000 | Loss: 0.00048099
Iteration 94/1000 | Loss: 0.00054124
Iteration 95/1000 | Loss: 0.00051238
Iteration 96/1000 | Loss: 0.00118004
Iteration 97/1000 | Loss: 0.00057659
Iteration 98/1000 | Loss: 0.00128127
Iteration 99/1000 | Loss: 0.00016242
Iteration 100/1000 | Loss: 0.00037940
Iteration 101/1000 | Loss: 0.00109181
Iteration 102/1000 | Loss: 0.00095100
Iteration 103/1000 | Loss: 0.00049234
Iteration 104/1000 | Loss: 0.00049558
Iteration 105/1000 | Loss: 0.00106026
Iteration 106/1000 | Loss: 0.00062945
Iteration 107/1000 | Loss: 0.00066645
Iteration 108/1000 | Loss: 0.00121934
Iteration 109/1000 | Loss: 0.00031493
Iteration 110/1000 | Loss: 0.00026092
Iteration 111/1000 | Loss: 0.00021697
Iteration 112/1000 | Loss: 0.00038423
Iteration 113/1000 | Loss: 0.00017612
Iteration 114/1000 | Loss: 0.00016460
Iteration 115/1000 | Loss: 0.00011790
Iteration 116/1000 | Loss: 0.00017384
Iteration 117/1000 | Loss: 0.00012454
Iteration 118/1000 | Loss: 0.00064570
Iteration 119/1000 | Loss: 0.00052559
Iteration 120/1000 | Loss: 0.00024975
Iteration 121/1000 | Loss: 0.00059418
Iteration 122/1000 | Loss: 0.00043654
Iteration 123/1000 | Loss: 0.00014671
Iteration 124/1000 | Loss: 0.00021063
Iteration 125/1000 | Loss: 0.00044004
Iteration 126/1000 | Loss: 0.00016071
Iteration 127/1000 | Loss: 0.00017574
Iteration 128/1000 | Loss: 0.00025153
Iteration 129/1000 | Loss: 0.00043373
Iteration 130/1000 | Loss: 0.00013784
Iteration 131/1000 | Loss: 0.00019230
Iteration 132/1000 | Loss: 0.00042789
Iteration 133/1000 | Loss: 0.00033667
Iteration 134/1000 | Loss: 0.00066191
Iteration 135/1000 | Loss: 0.00019739
Iteration 136/1000 | Loss: 0.00041502
Iteration 137/1000 | Loss: 0.00050455
Iteration 138/1000 | Loss: 0.00035500
Iteration 139/1000 | Loss: 0.00042990
Iteration 140/1000 | Loss: 0.00036428
Iteration 141/1000 | Loss: 0.00057523
Iteration 142/1000 | Loss: 0.00029421
Iteration 143/1000 | Loss: 0.00023786
Iteration 144/1000 | Loss: 0.00036362
Iteration 145/1000 | Loss: 0.00042238
Iteration 146/1000 | Loss: 0.00040903
Iteration 147/1000 | Loss: 0.00039050
Iteration 148/1000 | Loss: 0.00020328
Iteration 149/1000 | Loss: 0.00050301
Iteration 150/1000 | Loss: 0.00082370
Iteration 151/1000 | Loss: 0.00039208
Iteration 152/1000 | Loss: 0.00121534
Iteration 153/1000 | Loss: 0.00171650
Iteration 154/1000 | Loss: 0.00292620
Iteration 155/1000 | Loss: 0.00028551
Iteration 156/1000 | Loss: 0.00030024
Iteration 157/1000 | Loss: 0.00080057
Iteration 158/1000 | Loss: 0.00092348
Iteration 159/1000 | Loss: 0.00037528
Iteration 160/1000 | Loss: 0.00031522
Iteration 161/1000 | Loss: 0.00066739
Iteration 162/1000 | Loss: 0.00069070
Iteration 163/1000 | Loss: 0.00046288
Iteration 164/1000 | Loss: 0.00067330
Iteration 165/1000 | Loss: 0.00060540
Iteration 166/1000 | Loss: 0.00067460
Iteration 167/1000 | Loss: 0.00055352
Iteration 168/1000 | Loss: 0.00014341
Iteration 169/1000 | Loss: 0.00024634
Iteration 170/1000 | Loss: 0.00024782
Iteration 171/1000 | Loss: 0.00012915
Iteration 172/1000 | Loss: 0.00022449
Iteration 173/1000 | Loss: 0.00019907
Iteration 174/1000 | Loss: 0.00071042
Iteration 175/1000 | Loss: 0.00050471
Iteration 176/1000 | Loss: 0.00015035
Iteration 177/1000 | Loss: 0.00031131
Iteration 178/1000 | Loss: 0.00014628
Iteration 179/1000 | Loss: 0.00017609
Iteration 180/1000 | Loss: 0.00016615
Iteration 181/1000 | Loss: 0.00074571
Iteration 182/1000 | Loss: 0.00008850
Iteration 183/1000 | Loss: 0.00016476
Iteration 184/1000 | Loss: 0.00039547
Iteration 185/1000 | Loss: 0.00018790
Iteration 186/1000 | Loss: 0.00022949
Iteration 187/1000 | Loss: 0.00016531
Iteration 188/1000 | Loss: 0.00016288
Iteration 189/1000 | Loss: 0.00020968
Iteration 190/1000 | Loss: 0.00016611
Iteration 191/1000 | Loss: 0.00013116
Iteration 192/1000 | Loss: 0.00033818
Iteration 193/1000 | Loss: 0.00020085
Iteration 194/1000 | Loss: 0.00048299
Iteration 195/1000 | Loss: 0.00028868
Iteration 196/1000 | Loss: 0.00019114
Iteration 197/1000 | Loss: 0.00024881
Iteration 198/1000 | Loss: 0.00021664
Iteration 199/1000 | Loss: 0.00025374
Iteration 200/1000 | Loss: 0.00042571
Iteration 201/1000 | Loss: 0.00014235
Iteration 202/1000 | Loss: 0.00051865
Iteration 203/1000 | Loss: 0.00041495
Iteration 204/1000 | Loss: 0.00021892
Iteration 205/1000 | Loss: 0.00011403
Iteration 206/1000 | Loss: 0.00084916
Iteration 207/1000 | Loss: 0.00041862
Iteration 208/1000 | Loss: 0.00026667
Iteration 209/1000 | Loss: 0.00009933
Iteration 210/1000 | Loss: 0.00006115
Iteration 211/1000 | Loss: 0.00009443
Iteration 212/1000 | Loss: 0.00003459
Iteration 213/1000 | Loss: 0.00014451
Iteration 214/1000 | Loss: 0.00006080
Iteration 215/1000 | Loss: 0.00004318
Iteration 216/1000 | Loss: 0.00005454
Iteration 217/1000 | Loss: 0.00009624
Iteration 218/1000 | Loss: 0.00017088
Iteration 219/1000 | Loss: 0.00003005
Iteration 220/1000 | Loss: 0.00009882
Iteration 221/1000 | Loss: 0.00009393
Iteration 222/1000 | Loss: 0.00004513
Iteration 223/1000 | Loss: 0.00003030
Iteration 224/1000 | Loss: 0.00004548
Iteration 225/1000 | Loss: 0.00003588
Iteration 226/1000 | Loss: 0.00014180
Iteration 227/1000 | Loss: 0.00014488
Iteration 228/1000 | Loss: 0.00048852
Iteration 229/1000 | Loss: 0.00036808
Iteration 230/1000 | Loss: 0.00035411
Iteration 231/1000 | Loss: 0.00005378
Iteration 232/1000 | Loss: 0.00011066
Iteration 233/1000 | Loss: 0.00004841
Iteration 234/1000 | Loss: 0.00002984
Iteration 235/1000 | Loss: 0.00003563
Iteration 236/1000 | Loss: 0.00052425
Iteration 237/1000 | Loss: 0.00075791
Iteration 238/1000 | Loss: 0.00068533
Iteration 239/1000 | Loss: 0.00048434
Iteration 240/1000 | Loss: 0.00029002
Iteration 241/1000 | Loss: 0.00018226
Iteration 242/1000 | Loss: 0.00004514
Iteration 243/1000 | Loss: 0.00010179
Iteration 244/1000 | Loss: 0.00002554
Iteration 245/1000 | Loss: 0.00016832
Iteration 246/1000 | Loss: 0.00004768
Iteration 247/1000 | Loss: 0.00011411
Iteration 248/1000 | Loss: 0.00012644
Iteration 249/1000 | Loss: 0.00003663
Iteration 250/1000 | Loss: 0.00004667
Iteration 251/1000 | Loss: 0.00002204
Iteration 252/1000 | Loss: 0.00001909
Iteration 253/1000 | Loss: 0.00001769
Iteration 254/1000 | Loss: 0.00021472
Iteration 255/1000 | Loss: 0.00006433
Iteration 256/1000 | Loss: 0.00002510
Iteration 257/1000 | Loss: 0.00002064
Iteration 258/1000 | Loss: 0.00015442
Iteration 259/1000 | Loss: 0.00004819
Iteration 260/1000 | Loss: 0.00004582
Iteration 261/1000 | Loss: 0.00006104
Iteration 262/1000 | Loss: 0.00007057
Iteration 263/1000 | Loss: 0.00005071
Iteration 264/1000 | Loss: 0.00010747
Iteration 265/1000 | Loss: 0.00003024
Iteration 266/1000 | Loss: 0.00004021
Iteration 267/1000 | Loss: 0.00007006
Iteration 268/1000 | Loss: 0.00014868
Iteration 269/1000 | Loss: 0.00001645
Iteration 270/1000 | Loss: 0.00001625
Iteration 271/1000 | Loss: 0.00001604
Iteration 272/1000 | Loss: 0.00001599
Iteration 273/1000 | Loss: 0.00001598
Iteration 274/1000 | Loss: 0.00001597
Iteration 275/1000 | Loss: 0.00001597
Iteration 276/1000 | Loss: 0.00001597
Iteration 277/1000 | Loss: 0.00001596
Iteration 278/1000 | Loss: 0.00001596
Iteration 279/1000 | Loss: 0.00001596
Iteration 280/1000 | Loss: 0.00001596
Iteration 281/1000 | Loss: 0.00001595
Iteration 282/1000 | Loss: 0.00001595
Iteration 283/1000 | Loss: 0.00001595
Iteration 284/1000 | Loss: 0.00001594
Iteration 285/1000 | Loss: 0.00001594
Iteration 286/1000 | Loss: 0.00001593
Iteration 287/1000 | Loss: 0.00001593
Iteration 288/1000 | Loss: 0.00001593
Iteration 289/1000 | Loss: 0.00001593
Iteration 290/1000 | Loss: 0.00001593
Iteration 291/1000 | Loss: 0.00001592
Iteration 292/1000 | Loss: 0.00001592
Iteration 293/1000 | Loss: 0.00001591
Iteration 294/1000 | Loss: 0.00001591
Iteration 295/1000 | Loss: 0.00001590
Iteration 296/1000 | Loss: 0.00001589
Iteration 297/1000 | Loss: 0.00001589
Iteration 298/1000 | Loss: 0.00001589
Iteration 299/1000 | Loss: 0.00001588
Iteration 300/1000 | Loss: 0.00001588
Iteration 301/1000 | Loss: 0.00001587
Iteration 302/1000 | Loss: 0.00001587
Iteration 303/1000 | Loss: 0.00001586
Iteration 304/1000 | Loss: 0.00001586
Iteration 305/1000 | Loss: 0.00005540
Iteration 306/1000 | Loss: 0.00008163
Iteration 307/1000 | Loss: 0.00001850
Iteration 308/1000 | Loss: 0.00009217
Iteration 309/1000 | Loss: 0.00003163
Iteration 310/1000 | Loss: 0.00001980
Iteration 311/1000 | Loss: 0.00002858
Iteration 312/1000 | Loss: 0.00001824
Iteration 313/1000 | Loss: 0.00001937
Iteration 314/1000 | Loss: 0.00001581
Iteration 315/1000 | Loss: 0.00001581
Iteration 316/1000 | Loss: 0.00001581
Iteration 317/1000 | Loss: 0.00001580
Iteration 318/1000 | Loss: 0.00001580
Iteration 319/1000 | Loss: 0.00001580
Iteration 320/1000 | Loss: 0.00001579
Iteration 321/1000 | Loss: 0.00001579
Iteration 322/1000 | Loss: 0.00001579
Iteration 323/1000 | Loss: 0.00001578
Iteration 324/1000 | Loss: 0.00001578
Iteration 325/1000 | Loss: 0.00001577
Iteration 326/1000 | Loss: 0.00001577
Iteration 327/1000 | Loss: 0.00001576
Iteration 328/1000 | Loss: 0.00001576
Iteration 329/1000 | Loss: 0.00001576
Iteration 330/1000 | Loss: 0.00001576
Iteration 331/1000 | Loss: 0.00001575
Iteration 332/1000 | Loss: 0.00001575
Iteration 333/1000 | Loss: 0.00001574
Iteration 334/1000 | Loss: 0.00001574
Iteration 335/1000 | Loss: 0.00001574
Iteration 336/1000 | Loss: 0.00001573
Iteration 337/1000 | Loss: 0.00001573
Iteration 338/1000 | Loss: 0.00001573
Iteration 339/1000 | Loss: 0.00001573
Iteration 340/1000 | Loss: 0.00001573
Iteration 341/1000 | Loss: 0.00001572
Iteration 342/1000 | Loss: 0.00001572
Iteration 343/1000 | Loss: 0.00001571
Iteration 344/1000 | Loss: 0.00001571
Iteration 345/1000 | Loss: 0.00001571
Iteration 346/1000 | Loss: 0.00004643
Iteration 347/1000 | Loss: 0.00003888
Iteration 348/1000 | Loss: 0.00001571
Iteration 349/1000 | Loss: 0.00001571
Iteration 350/1000 | Loss: 0.00001571
Iteration 351/1000 | Loss: 0.00001571
Iteration 352/1000 | Loss: 0.00001571
Iteration 353/1000 | Loss: 0.00001571
Iteration 354/1000 | Loss: 0.00001571
Iteration 355/1000 | Loss: 0.00001571
Iteration 356/1000 | Loss: 0.00001571
Iteration 357/1000 | Loss: 0.00001571
Iteration 358/1000 | Loss: 0.00001571
Iteration 359/1000 | Loss: 0.00001570
Iteration 360/1000 | Loss: 0.00001570
Iteration 361/1000 | Loss: 0.00001570
Iteration 362/1000 | Loss: 0.00001570
Iteration 363/1000 | Loss: 0.00001570
Iteration 364/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 364. Stopping optimization.
Last 5 losses: [1.570496169733815e-05, 1.570496169733815e-05, 1.570496169733815e-05, 1.570496169733815e-05, 1.570496169733815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.570496169733815e-05

Optimization complete. Final v2v error: 3.313976287841797 mm

Highest mean error: 7.952978134155273 mm for frame 65

Lowest mean error: 2.764744520187378 mm for frame 238

Saving results

Total time: 506.9751799106598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398566
Iteration 2/25 | Loss: 0.00097697
Iteration 3/25 | Loss: 0.00077193
Iteration 4/25 | Loss: 0.00072472
Iteration 5/25 | Loss: 0.00070737
Iteration 6/25 | Loss: 0.00070062
Iteration 7/25 | Loss: 0.00069604
Iteration 8/25 | Loss: 0.00070039
Iteration 9/25 | Loss: 0.00068706
Iteration 10/25 | Loss: 0.00066491
Iteration 11/25 | Loss: 0.00065366
Iteration 12/25 | Loss: 0.00065083
Iteration 13/25 | Loss: 0.00065020
Iteration 14/25 | Loss: 0.00065005
Iteration 15/25 | Loss: 0.00065005
Iteration 16/25 | Loss: 0.00065005
Iteration 17/25 | Loss: 0.00065005
Iteration 18/25 | Loss: 0.00065005
Iteration 19/25 | Loss: 0.00065005
Iteration 20/25 | Loss: 0.00065005
Iteration 21/25 | Loss: 0.00065005
Iteration 22/25 | Loss: 0.00065005
Iteration 23/25 | Loss: 0.00065005
Iteration 24/25 | Loss: 0.00065004
Iteration 25/25 | Loss: 0.00065004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41761827
Iteration 2/25 | Loss: 0.00019617
Iteration 3/25 | Loss: 0.00019616
Iteration 4/25 | Loss: 0.00019616
Iteration 5/25 | Loss: 0.00019616
Iteration 6/25 | Loss: 0.00019616
Iteration 7/25 | Loss: 0.00019616
Iteration 8/25 | Loss: 0.00019616
Iteration 9/25 | Loss: 0.00019616
Iteration 10/25 | Loss: 0.00019616
Iteration 11/25 | Loss: 0.00019616
Iteration 12/25 | Loss: 0.00019616
Iteration 13/25 | Loss: 0.00019616
Iteration 14/25 | Loss: 0.00019616
Iteration 15/25 | Loss: 0.00019616
Iteration 16/25 | Loss: 0.00019616
Iteration 17/25 | Loss: 0.00019616
Iteration 18/25 | Loss: 0.00019616
Iteration 19/25 | Loss: 0.00019616
Iteration 20/25 | Loss: 0.00019616
Iteration 21/25 | Loss: 0.00019616
Iteration 22/25 | Loss: 0.00019616
Iteration 23/25 | Loss: 0.00019616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00019616202916949987, 0.00019616202916949987, 0.00019616202916949987, 0.00019616202916949987, 0.00019616202916949987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00019616202916949987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019616
Iteration 2/1000 | Loss: 0.00001900
Iteration 3/1000 | Loss: 0.00001447
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001293
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001208
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001205
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001204
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001202
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001198
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001198
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001198
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001198
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001187
Iteration 92/1000 | Loss: 0.00001187
Iteration 93/1000 | Loss: 0.00001187
Iteration 94/1000 | Loss: 0.00001187
Iteration 95/1000 | Loss: 0.00001187
Iteration 96/1000 | Loss: 0.00001187
Iteration 97/1000 | Loss: 0.00001187
Iteration 98/1000 | Loss: 0.00001187
Iteration 99/1000 | Loss: 0.00001187
Iteration 100/1000 | Loss: 0.00001187
Iteration 101/1000 | Loss: 0.00001187
Iteration 102/1000 | Loss: 0.00001187
Iteration 103/1000 | Loss: 0.00001187
Iteration 104/1000 | Loss: 0.00001187
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.1868892215716187e-05, 1.1868892215716187e-05, 1.1868892215716187e-05, 1.1868892215716187e-05, 1.1868892215716187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1868892215716187e-05

Optimization complete. Final v2v error: 2.873525619506836 mm

Highest mean error: 3.2034831047058105 mm for frame 197

Lowest mean error: 2.6250698566436768 mm for frame 211

Saving results

Total time: 48.88981080055237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00903564
Iteration 2/25 | Loss: 0.00101963
Iteration 3/25 | Loss: 0.00086867
Iteration 4/25 | Loss: 0.00080544
Iteration 5/25 | Loss: 0.00078392
Iteration 6/25 | Loss: 0.00077963
Iteration 7/25 | Loss: 0.00077770
Iteration 8/25 | Loss: 0.00077747
Iteration 9/25 | Loss: 0.00077747
Iteration 10/25 | Loss: 0.00077747
Iteration 11/25 | Loss: 0.00077747
Iteration 12/25 | Loss: 0.00077747
Iteration 13/25 | Loss: 0.00077747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00077747309114784, 0.00077747309114784, 0.00077747309114784, 0.00077747309114784, 0.00077747309114784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00077747309114784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41331482
Iteration 2/25 | Loss: 0.00033144
Iteration 3/25 | Loss: 0.00033144
Iteration 4/25 | Loss: 0.00033144
Iteration 5/25 | Loss: 0.00033144
Iteration 6/25 | Loss: 0.00033144
Iteration 7/25 | Loss: 0.00033144
Iteration 8/25 | Loss: 0.00033144
Iteration 9/25 | Loss: 0.00033144
Iteration 10/25 | Loss: 0.00033143
Iteration 11/25 | Loss: 0.00033143
Iteration 12/25 | Loss: 0.00033143
Iteration 13/25 | Loss: 0.00033143
Iteration 14/25 | Loss: 0.00033143
Iteration 15/25 | Loss: 0.00033143
Iteration 16/25 | Loss: 0.00033143
Iteration 17/25 | Loss: 0.00033143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00033143474138341844, 0.00033143474138341844, 0.00033143474138341844, 0.00033143474138341844, 0.00033143474138341844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033143474138341844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033143
Iteration 2/1000 | Loss: 0.00005636
Iteration 3/1000 | Loss: 0.00003588
Iteration 4/1000 | Loss: 0.00003190
Iteration 5/1000 | Loss: 0.00003042
Iteration 6/1000 | Loss: 0.00002924
Iteration 7/1000 | Loss: 0.00002848
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002725
Iteration 10/1000 | Loss: 0.00002689
Iteration 11/1000 | Loss: 0.00002662
Iteration 12/1000 | Loss: 0.00002662
Iteration 13/1000 | Loss: 0.00002655
Iteration 14/1000 | Loss: 0.00002637
Iteration 15/1000 | Loss: 0.00002635
Iteration 16/1000 | Loss: 0.00002633
Iteration 17/1000 | Loss: 0.00002626
Iteration 18/1000 | Loss: 0.00002626
Iteration 19/1000 | Loss: 0.00002625
Iteration 20/1000 | Loss: 0.00002623
Iteration 21/1000 | Loss: 0.00002622
Iteration 22/1000 | Loss: 0.00002622
Iteration 23/1000 | Loss: 0.00002619
Iteration 24/1000 | Loss: 0.00002619
Iteration 25/1000 | Loss: 0.00002618
Iteration 26/1000 | Loss: 0.00002618
Iteration 27/1000 | Loss: 0.00002617
Iteration 28/1000 | Loss: 0.00002617
Iteration 29/1000 | Loss: 0.00002617
Iteration 30/1000 | Loss: 0.00002616
Iteration 31/1000 | Loss: 0.00002616
Iteration 32/1000 | Loss: 0.00002615
Iteration 33/1000 | Loss: 0.00002615
Iteration 34/1000 | Loss: 0.00002615
Iteration 35/1000 | Loss: 0.00002615
Iteration 36/1000 | Loss: 0.00002615
Iteration 37/1000 | Loss: 0.00002614
Iteration 38/1000 | Loss: 0.00002614
Iteration 39/1000 | Loss: 0.00002614
Iteration 40/1000 | Loss: 0.00002614
Iteration 41/1000 | Loss: 0.00002613
Iteration 42/1000 | Loss: 0.00002613
Iteration 43/1000 | Loss: 0.00002613
Iteration 44/1000 | Loss: 0.00002612
Iteration 45/1000 | Loss: 0.00002612
Iteration 46/1000 | Loss: 0.00002611
Iteration 47/1000 | Loss: 0.00002610
Iteration 48/1000 | Loss: 0.00002610
Iteration 49/1000 | Loss: 0.00002610
Iteration 50/1000 | Loss: 0.00002609
Iteration 51/1000 | Loss: 0.00002609
Iteration 52/1000 | Loss: 0.00002609
Iteration 53/1000 | Loss: 0.00002608
Iteration 54/1000 | Loss: 0.00002608
Iteration 55/1000 | Loss: 0.00002608
Iteration 56/1000 | Loss: 0.00002607
Iteration 57/1000 | Loss: 0.00002607
Iteration 58/1000 | Loss: 0.00002607
Iteration 59/1000 | Loss: 0.00002607
Iteration 60/1000 | Loss: 0.00002607
Iteration 61/1000 | Loss: 0.00002606
Iteration 62/1000 | Loss: 0.00002606
Iteration 63/1000 | Loss: 0.00002606
Iteration 64/1000 | Loss: 0.00002606
Iteration 65/1000 | Loss: 0.00002606
Iteration 66/1000 | Loss: 0.00002605
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002604
Iteration 69/1000 | Loss: 0.00002604
Iteration 70/1000 | Loss: 0.00002604
Iteration 71/1000 | Loss: 0.00002604
Iteration 72/1000 | Loss: 0.00002604
Iteration 73/1000 | Loss: 0.00002604
Iteration 74/1000 | Loss: 0.00002604
Iteration 75/1000 | Loss: 0.00002603
Iteration 76/1000 | Loss: 0.00002603
Iteration 77/1000 | Loss: 0.00002603
Iteration 78/1000 | Loss: 0.00002603
Iteration 79/1000 | Loss: 0.00002603
Iteration 80/1000 | Loss: 0.00002603
Iteration 81/1000 | Loss: 0.00002602
Iteration 82/1000 | Loss: 0.00002602
Iteration 83/1000 | Loss: 0.00002602
Iteration 84/1000 | Loss: 0.00002602
Iteration 85/1000 | Loss: 0.00002602
Iteration 86/1000 | Loss: 0.00002601
Iteration 87/1000 | Loss: 0.00002601
Iteration 88/1000 | Loss: 0.00002601
Iteration 89/1000 | Loss: 0.00002601
Iteration 90/1000 | Loss: 0.00002601
Iteration 91/1000 | Loss: 0.00002601
Iteration 92/1000 | Loss: 0.00002601
Iteration 93/1000 | Loss: 0.00002601
Iteration 94/1000 | Loss: 0.00002601
Iteration 95/1000 | Loss: 0.00002601
Iteration 96/1000 | Loss: 0.00002601
Iteration 97/1000 | Loss: 0.00002601
Iteration 98/1000 | Loss: 0.00002601
Iteration 99/1000 | Loss: 0.00002600
Iteration 100/1000 | Loss: 0.00002600
Iteration 101/1000 | Loss: 0.00002600
Iteration 102/1000 | Loss: 0.00002599
Iteration 103/1000 | Loss: 0.00002599
Iteration 104/1000 | Loss: 0.00002599
Iteration 105/1000 | Loss: 0.00002599
Iteration 106/1000 | Loss: 0.00002599
Iteration 107/1000 | Loss: 0.00002598
Iteration 108/1000 | Loss: 0.00002598
Iteration 109/1000 | Loss: 0.00002598
Iteration 110/1000 | Loss: 0.00002598
Iteration 111/1000 | Loss: 0.00002598
Iteration 112/1000 | Loss: 0.00002598
Iteration 113/1000 | Loss: 0.00002597
Iteration 114/1000 | Loss: 0.00002597
Iteration 115/1000 | Loss: 0.00002597
Iteration 116/1000 | Loss: 0.00002597
Iteration 117/1000 | Loss: 0.00002597
Iteration 118/1000 | Loss: 0.00002596
Iteration 119/1000 | Loss: 0.00002596
Iteration 120/1000 | Loss: 0.00002596
Iteration 121/1000 | Loss: 0.00002596
Iteration 122/1000 | Loss: 0.00002596
Iteration 123/1000 | Loss: 0.00002596
Iteration 124/1000 | Loss: 0.00002596
Iteration 125/1000 | Loss: 0.00002596
Iteration 126/1000 | Loss: 0.00002596
Iteration 127/1000 | Loss: 0.00002595
Iteration 128/1000 | Loss: 0.00002595
Iteration 129/1000 | Loss: 0.00002595
Iteration 130/1000 | Loss: 0.00002595
Iteration 131/1000 | Loss: 0.00002595
Iteration 132/1000 | Loss: 0.00002595
Iteration 133/1000 | Loss: 0.00002595
Iteration 134/1000 | Loss: 0.00002595
Iteration 135/1000 | Loss: 0.00002595
Iteration 136/1000 | Loss: 0.00002595
Iteration 137/1000 | Loss: 0.00002595
Iteration 138/1000 | Loss: 0.00002595
Iteration 139/1000 | Loss: 0.00002595
Iteration 140/1000 | Loss: 0.00002595
Iteration 141/1000 | Loss: 0.00002595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.5953824660973623e-05, 2.5953824660973623e-05, 2.5953824660973623e-05, 2.5953824660973623e-05, 2.5953824660973623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5953824660973623e-05

Optimization complete. Final v2v error: 4.18161153793335 mm

Highest mean error: 4.6070876121521 mm for frame 123

Lowest mean error: 3.7592720985412598 mm for frame 12

Saving results

Total time: 45.24996829032898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396535
Iteration 2/25 | Loss: 0.00083591
Iteration 3/25 | Loss: 0.00068212
Iteration 4/25 | Loss: 0.00066093
Iteration 5/25 | Loss: 0.00065300
Iteration 6/25 | Loss: 0.00065180
Iteration 7/25 | Loss: 0.00065147
Iteration 8/25 | Loss: 0.00065147
Iteration 9/25 | Loss: 0.00065147
Iteration 10/25 | Loss: 0.00065147
Iteration 11/25 | Loss: 0.00065147
Iteration 12/25 | Loss: 0.00065147
Iteration 13/25 | Loss: 0.00065147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006514725973829627, 0.0006514725973829627, 0.0006514725973829627, 0.0006514725973829627, 0.0006514725973829627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006514725973829627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48012400
Iteration 2/25 | Loss: 0.00019181
Iteration 3/25 | Loss: 0.00019181
Iteration 4/25 | Loss: 0.00019181
Iteration 5/25 | Loss: 0.00019181
Iteration 6/25 | Loss: 0.00019181
Iteration 7/25 | Loss: 0.00019181
Iteration 8/25 | Loss: 0.00019181
Iteration 9/25 | Loss: 0.00019181
Iteration 10/25 | Loss: 0.00019181
Iteration 11/25 | Loss: 0.00019181
Iteration 12/25 | Loss: 0.00019181
Iteration 13/25 | Loss: 0.00019181
Iteration 14/25 | Loss: 0.00019181
Iteration 15/25 | Loss: 0.00019181
Iteration 16/25 | Loss: 0.00019181
Iteration 17/25 | Loss: 0.00019181
Iteration 18/25 | Loss: 0.00019181
Iteration 19/25 | Loss: 0.00019181
Iteration 20/25 | Loss: 0.00019181
Iteration 21/25 | Loss: 0.00019181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00019180514209438115, 0.00019180514209438115, 0.00019180514209438115, 0.00019180514209438115, 0.00019180514209438115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00019180514209438115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019181
Iteration 2/1000 | Loss: 0.00003226
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001704
Iteration 5/1000 | Loss: 0.00001610
Iteration 6/1000 | Loss: 0.00001555
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001493
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001465
Iteration 11/1000 | Loss: 0.00001459
Iteration 12/1000 | Loss: 0.00001459
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001457
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001451
Iteration 18/1000 | Loss: 0.00001449
Iteration 19/1000 | Loss: 0.00001446
Iteration 20/1000 | Loss: 0.00001445
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001439
Iteration 24/1000 | Loss: 0.00001439
Iteration 25/1000 | Loss: 0.00001433
Iteration 26/1000 | Loss: 0.00001433
Iteration 27/1000 | Loss: 0.00001433
Iteration 28/1000 | Loss: 0.00001432
Iteration 29/1000 | Loss: 0.00001432
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001429
Iteration 33/1000 | Loss: 0.00001429
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001428
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001428
Iteration 38/1000 | Loss: 0.00001428
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001422
Iteration 54/1000 | Loss: 0.00001422
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001419
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001419
Iteration 74/1000 | Loss: 0.00001419
Iteration 75/1000 | Loss: 0.00001419
Iteration 76/1000 | Loss: 0.00001419
Iteration 77/1000 | Loss: 0.00001419
Iteration 78/1000 | Loss: 0.00001419
Iteration 79/1000 | Loss: 0.00001419
Iteration 80/1000 | Loss: 0.00001419
Iteration 81/1000 | Loss: 0.00001419
Iteration 82/1000 | Loss: 0.00001418
Iteration 83/1000 | Loss: 0.00001418
Iteration 84/1000 | Loss: 0.00001418
Iteration 85/1000 | Loss: 0.00001418
Iteration 86/1000 | Loss: 0.00001418
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001418
Iteration 89/1000 | Loss: 0.00001418
Iteration 90/1000 | Loss: 0.00001418
Iteration 91/1000 | Loss: 0.00001418
Iteration 92/1000 | Loss: 0.00001418
Iteration 93/1000 | Loss: 0.00001418
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001418
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001417
Iteration 98/1000 | Loss: 0.00001417
Iteration 99/1000 | Loss: 0.00001417
Iteration 100/1000 | Loss: 0.00001417
Iteration 101/1000 | Loss: 0.00001417
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001417
Iteration 104/1000 | Loss: 0.00001417
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001416
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001416
Iteration 118/1000 | Loss: 0.00001416
Iteration 119/1000 | Loss: 0.00001416
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001416
Iteration 123/1000 | Loss: 0.00001416
Iteration 124/1000 | Loss: 0.00001416
Iteration 125/1000 | Loss: 0.00001416
Iteration 126/1000 | Loss: 0.00001416
Iteration 127/1000 | Loss: 0.00001416
Iteration 128/1000 | Loss: 0.00001416
Iteration 129/1000 | Loss: 0.00001416
Iteration 130/1000 | Loss: 0.00001416
Iteration 131/1000 | Loss: 0.00001416
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001416
Iteration 138/1000 | Loss: 0.00001416
Iteration 139/1000 | Loss: 0.00001416
Iteration 140/1000 | Loss: 0.00001416
Iteration 141/1000 | Loss: 0.00001416
Iteration 142/1000 | Loss: 0.00001416
Iteration 143/1000 | Loss: 0.00001416
Iteration 144/1000 | Loss: 0.00001416
Iteration 145/1000 | Loss: 0.00001416
Iteration 146/1000 | Loss: 0.00001416
Iteration 147/1000 | Loss: 0.00001416
Iteration 148/1000 | Loss: 0.00001416
Iteration 149/1000 | Loss: 0.00001416
Iteration 150/1000 | Loss: 0.00001416
Iteration 151/1000 | Loss: 0.00001416
Iteration 152/1000 | Loss: 0.00001416
Iteration 153/1000 | Loss: 0.00001416
Iteration 154/1000 | Loss: 0.00001416
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001416
Iteration 157/1000 | Loss: 0.00001416
Iteration 158/1000 | Loss: 0.00001416
Iteration 159/1000 | Loss: 0.00001416
Iteration 160/1000 | Loss: 0.00001416
Iteration 161/1000 | Loss: 0.00001416
Iteration 162/1000 | Loss: 0.00001416
Iteration 163/1000 | Loss: 0.00001416
Iteration 164/1000 | Loss: 0.00001416
Iteration 165/1000 | Loss: 0.00001416
Iteration 166/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.4155618373479228e-05, 1.4155618373479228e-05, 1.4155618373479228e-05, 1.4155618373479228e-05, 1.4155618373479228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4155618373479228e-05

Optimization complete. Final v2v error: 3.2501513957977295 mm

Highest mean error: 4.172131538391113 mm for frame 34

Lowest mean error: 2.842038154602051 mm for frame 121

Saving results

Total time: 33.84266257286072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407314
Iteration 2/25 | Loss: 0.00089376
Iteration 3/25 | Loss: 0.00070376
Iteration 4/25 | Loss: 0.00067751
Iteration 5/25 | Loss: 0.00066884
Iteration 6/25 | Loss: 0.00066779
Iteration 7/25 | Loss: 0.00066779
Iteration 8/25 | Loss: 0.00066779
Iteration 9/25 | Loss: 0.00066779
Iteration 10/25 | Loss: 0.00066779
Iteration 11/25 | Loss: 0.00066779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000667787913698703, 0.000667787913698703, 0.000667787913698703, 0.000667787913698703, 0.000667787913698703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000667787913698703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67384672
Iteration 2/25 | Loss: 0.00022014
Iteration 3/25 | Loss: 0.00022012
Iteration 4/25 | Loss: 0.00022012
Iteration 5/25 | Loss: 0.00022012
Iteration 6/25 | Loss: 0.00022012
Iteration 7/25 | Loss: 0.00022012
Iteration 8/25 | Loss: 0.00022012
Iteration 9/25 | Loss: 0.00022012
Iteration 10/25 | Loss: 0.00022012
Iteration 11/25 | Loss: 0.00022012
Iteration 12/25 | Loss: 0.00022012
Iteration 13/25 | Loss: 0.00022012
Iteration 14/25 | Loss: 0.00022012
Iteration 15/25 | Loss: 0.00022012
Iteration 16/25 | Loss: 0.00022012
Iteration 17/25 | Loss: 0.00022012
Iteration 18/25 | Loss: 0.00022012
Iteration 19/25 | Loss: 0.00022012
Iteration 20/25 | Loss: 0.00022012
Iteration 21/25 | Loss: 0.00022012
Iteration 22/25 | Loss: 0.00022012
Iteration 23/25 | Loss: 0.00022012
Iteration 24/25 | Loss: 0.00022012
Iteration 25/25 | Loss: 0.00022012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022012
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00001631
Iteration 4/1000 | Loss: 0.00001454
Iteration 5/1000 | Loss: 0.00001393
Iteration 6/1000 | Loss: 0.00001359
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001283
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001272
Iteration 13/1000 | Loss: 0.00001270
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001261
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001251
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001247
Iteration 45/1000 | Loss: 0.00001247
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001239
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001238
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001237
Iteration 84/1000 | Loss: 0.00001237
Iteration 85/1000 | Loss: 0.00001237
Iteration 86/1000 | Loss: 0.00001237
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001234
Iteration 108/1000 | Loss: 0.00001234
Iteration 109/1000 | Loss: 0.00001234
Iteration 110/1000 | Loss: 0.00001234
Iteration 111/1000 | Loss: 0.00001234
Iteration 112/1000 | Loss: 0.00001234
Iteration 113/1000 | Loss: 0.00001234
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001234
Iteration 116/1000 | Loss: 0.00001234
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001234
Iteration 121/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.2340869034233037e-05, 1.2340869034233037e-05, 1.2340869034233037e-05, 1.2340869034233037e-05, 1.2340869034233037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2340869034233037e-05

Optimization complete. Final v2v error: 3.025383710861206 mm

Highest mean error: 3.441096544265747 mm for frame 43

Lowest mean error: 2.770883083343506 mm for frame 201

Saving results

Total time: 38.20861554145813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872846
Iteration 2/25 | Loss: 0.00106425
Iteration 3/25 | Loss: 0.00078589
Iteration 4/25 | Loss: 0.00075315
Iteration 5/25 | Loss: 0.00074188
Iteration 6/25 | Loss: 0.00074022
Iteration 7/25 | Loss: 0.00074007
Iteration 8/25 | Loss: 0.00074007
Iteration 9/25 | Loss: 0.00074007
Iteration 10/25 | Loss: 0.00074007
Iteration 11/25 | Loss: 0.00074007
Iteration 12/25 | Loss: 0.00074007
Iteration 13/25 | Loss: 0.00074007
Iteration 14/25 | Loss: 0.00074007
Iteration 15/25 | Loss: 0.00074007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007400686736218631, 0.0007400686736218631, 0.0007400686736218631, 0.0007400686736218631, 0.0007400686736218631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007400686736218631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.85355186
Iteration 2/25 | Loss: 0.00024602
Iteration 3/25 | Loss: 0.00024599
Iteration 4/25 | Loss: 0.00024599
Iteration 5/25 | Loss: 0.00024599
Iteration 6/25 | Loss: 0.00024599
Iteration 7/25 | Loss: 0.00024599
Iteration 8/25 | Loss: 0.00024599
Iteration 9/25 | Loss: 0.00024599
Iteration 10/25 | Loss: 0.00024599
Iteration 11/25 | Loss: 0.00024599
Iteration 12/25 | Loss: 0.00024599
Iteration 13/25 | Loss: 0.00024599
Iteration 14/25 | Loss: 0.00024599
Iteration 15/25 | Loss: 0.00024599
Iteration 16/25 | Loss: 0.00024599
Iteration 17/25 | Loss: 0.00024599
Iteration 18/25 | Loss: 0.00024599
Iteration 19/25 | Loss: 0.00024599
Iteration 20/25 | Loss: 0.00024599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002459915995132178, 0.0002459915995132178, 0.0002459915995132178, 0.0002459915995132178, 0.0002459915995132178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002459915995132178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024599
Iteration 2/1000 | Loss: 0.00004021
Iteration 3/1000 | Loss: 0.00002710
Iteration 4/1000 | Loss: 0.00002557
Iteration 5/1000 | Loss: 0.00002441
Iteration 6/1000 | Loss: 0.00002377
Iteration 7/1000 | Loss: 0.00002322
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002243
Iteration 11/1000 | Loss: 0.00002242
Iteration 12/1000 | Loss: 0.00002237
Iteration 13/1000 | Loss: 0.00002234
Iteration 14/1000 | Loss: 0.00002230
Iteration 15/1000 | Loss: 0.00002230
Iteration 16/1000 | Loss: 0.00002228
Iteration 17/1000 | Loss: 0.00002228
Iteration 18/1000 | Loss: 0.00002227
Iteration 19/1000 | Loss: 0.00002227
Iteration 20/1000 | Loss: 0.00002227
Iteration 21/1000 | Loss: 0.00002227
Iteration 22/1000 | Loss: 0.00002227
Iteration 23/1000 | Loss: 0.00002227
Iteration 24/1000 | Loss: 0.00002227
Iteration 25/1000 | Loss: 0.00002227
Iteration 26/1000 | Loss: 0.00002227
Iteration 27/1000 | Loss: 0.00002227
Iteration 28/1000 | Loss: 0.00002226
Iteration 29/1000 | Loss: 0.00002226
Iteration 30/1000 | Loss: 0.00002226
Iteration 31/1000 | Loss: 0.00002225
Iteration 32/1000 | Loss: 0.00002225
Iteration 33/1000 | Loss: 0.00002225
Iteration 34/1000 | Loss: 0.00002224
Iteration 35/1000 | Loss: 0.00002224
Iteration 36/1000 | Loss: 0.00002224
Iteration 37/1000 | Loss: 0.00002224
Iteration 38/1000 | Loss: 0.00002224
Iteration 39/1000 | Loss: 0.00002224
Iteration 40/1000 | Loss: 0.00002224
Iteration 41/1000 | Loss: 0.00002224
Iteration 42/1000 | Loss: 0.00002224
Iteration 43/1000 | Loss: 0.00002224
Iteration 44/1000 | Loss: 0.00002224
Iteration 45/1000 | Loss: 0.00002224
Iteration 46/1000 | Loss: 0.00002224
Iteration 47/1000 | Loss: 0.00002224
Iteration 48/1000 | Loss: 0.00002224
Iteration 49/1000 | Loss: 0.00002224
Iteration 50/1000 | Loss: 0.00002224
Iteration 51/1000 | Loss: 0.00002224
Iteration 52/1000 | Loss: 0.00002224
Iteration 53/1000 | Loss: 0.00002224
Iteration 54/1000 | Loss: 0.00002224
Iteration 55/1000 | Loss: 0.00002224
Iteration 56/1000 | Loss: 0.00002224
Iteration 57/1000 | Loss: 0.00002224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [2.223736373707652e-05, 2.223736373707652e-05, 2.223736373707652e-05, 2.223736373707652e-05, 2.223736373707652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.223736373707652e-05

Optimization complete. Final v2v error: 3.9456160068511963 mm

Highest mean error: 4.681333541870117 mm for frame 117

Lowest mean error: 3.304506540298462 mm for frame 12

Saving results

Total time: 31.436246395111084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074210
Iteration 2/25 | Loss: 0.00278109
Iteration 3/25 | Loss: 0.00208247
Iteration 4/25 | Loss: 0.00158751
Iteration 5/25 | Loss: 0.00148343
Iteration 6/25 | Loss: 0.00144729
Iteration 7/25 | Loss: 0.00138338
Iteration 8/25 | Loss: 0.00135412
Iteration 9/25 | Loss: 0.00127482
Iteration 10/25 | Loss: 0.00125683
Iteration 11/25 | Loss: 0.00117465
Iteration 12/25 | Loss: 0.00115646
Iteration 13/25 | Loss: 0.00111551
Iteration 14/25 | Loss: 0.00110954
Iteration 15/25 | Loss: 0.00111314
Iteration 16/25 | Loss: 0.00108425
Iteration 17/25 | Loss: 0.00104236
Iteration 18/25 | Loss: 0.00102885
Iteration 19/25 | Loss: 0.00100593
Iteration 20/25 | Loss: 0.00098879
Iteration 21/25 | Loss: 0.00098459
Iteration 22/25 | Loss: 0.00097915
Iteration 23/25 | Loss: 0.00096708
Iteration 24/25 | Loss: 0.00096051
Iteration 25/25 | Loss: 0.00094508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69725621
Iteration 2/25 | Loss: 0.00302266
Iteration 3/25 | Loss: 0.00132209
Iteration 4/25 | Loss: 0.00132208
Iteration 5/25 | Loss: 0.00132208
Iteration 6/25 | Loss: 0.00132208
Iteration 7/25 | Loss: 0.00132208
Iteration 8/25 | Loss: 0.00132208
Iteration 9/25 | Loss: 0.00132208
Iteration 10/25 | Loss: 0.00132208
Iteration 11/25 | Loss: 0.00132208
Iteration 12/25 | Loss: 0.00132208
Iteration 13/25 | Loss: 0.00132208
Iteration 14/25 | Loss: 0.00132208
Iteration 15/25 | Loss: 0.00132208
Iteration 16/25 | Loss: 0.00132208
Iteration 17/25 | Loss: 0.00132208
Iteration 18/25 | Loss: 0.00132208
Iteration 19/25 | Loss: 0.00132208
Iteration 20/25 | Loss: 0.00132208
Iteration 21/25 | Loss: 0.00132208
Iteration 22/25 | Loss: 0.00132208
Iteration 23/25 | Loss: 0.00132208
Iteration 24/25 | Loss: 0.00132208
Iteration 25/25 | Loss: 0.00132208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013220765395089984, 0.0013220765395089984, 0.0013220765395089984, 0.0013220765395089984, 0.0013220765395089984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013220765395089984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132208
Iteration 2/1000 | Loss: 0.00113047
Iteration 3/1000 | Loss: 0.00066642
Iteration 4/1000 | Loss: 0.00028333
Iteration 5/1000 | Loss: 0.00062256
Iteration 6/1000 | Loss: 0.00017896
Iteration 7/1000 | Loss: 0.00049811
Iteration 8/1000 | Loss: 0.00068178
Iteration 9/1000 | Loss: 0.00099210
Iteration 10/1000 | Loss: 0.00185311
Iteration 11/1000 | Loss: 0.00011505
Iteration 12/1000 | Loss: 0.00022097
Iteration 13/1000 | Loss: 0.00027258
Iteration 14/1000 | Loss: 0.00017880
Iteration 15/1000 | Loss: 0.00015761
Iteration 16/1000 | Loss: 0.00011822
Iteration 17/1000 | Loss: 0.00010756
Iteration 18/1000 | Loss: 0.00011519
Iteration 19/1000 | Loss: 0.00071095
Iteration 20/1000 | Loss: 0.00128471
Iteration 21/1000 | Loss: 0.00092383
Iteration 22/1000 | Loss: 0.00046080
Iteration 23/1000 | Loss: 0.00040830
Iteration 24/1000 | Loss: 0.00020444
Iteration 25/1000 | Loss: 0.00010426
Iteration 26/1000 | Loss: 0.00039263
Iteration 27/1000 | Loss: 0.00149265
Iteration 28/1000 | Loss: 0.00078970
Iteration 29/1000 | Loss: 0.00080783
Iteration 30/1000 | Loss: 0.00040807
Iteration 31/1000 | Loss: 0.00036142
Iteration 32/1000 | Loss: 0.00040336
Iteration 33/1000 | Loss: 0.00025446
Iteration 34/1000 | Loss: 0.00019975
Iteration 35/1000 | Loss: 0.00019357
Iteration 36/1000 | Loss: 0.00017839
Iteration 37/1000 | Loss: 0.00057061
Iteration 38/1000 | Loss: 0.00020581
Iteration 39/1000 | Loss: 0.00028976
Iteration 40/1000 | Loss: 0.00016623
Iteration 41/1000 | Loss: 0.00011652
Iteration 42/1000 | Loss: 0.00014303
Iteration 43/1000 | Loss: 0.00017833
Iteration 44/1000 | Loss: 0.00018546
Iteration 45/1000 | Loss: 0.00018285
Iteration 46/1000 | Loss: 0.00052085
Iteration 47/1000 | Loss: 0.00034520
Iteration 48/1000 | Loss: 0.00024489
Iteration 49/1000 | Loss: 0.00022290
Iteration 50/1000 | Loss: 0.00027798
Iteration 51/1000 | Loss: 0.00028141
Iteration 52/1000 | Loss: 0.00030673
Iteration 53/1000 | Loss: 0.00026983
Iteration 54/1000 | Loss: 0.00020043
Iteration 55/1000 | Loss: 0.00028307
Iteration 56/1000 | Loss: 0.00060915
Iteration 57/1000 | Loss: 0.00026127
Iteration 58/1000 | Loss: 0.00071781
Iteration 59/1000 | Loss: 0.00035202
Iteration 60/1000 | Loss: 0.00012330
Iteration 61/1000 | Loss: 0.00025037
Iteration 62/1000 | Loss: 0.00020708
Iteration 63/1000 | Loss: 0.00023747
Iteration 64/1000 | Loss: 0.00021978
Iteration 65/1000 | Loss: 0.00022478
Iteration 66/1000 | Loss: 0.00048422
Iteration 67/1000 | Loss: 0.00024231
Iteration 68/1000 | Loss: 0.00040719
Iteration 69/1000 | Loss: 0.00032775
Iteration 70/1000 | Loss: 0.00029990
Iteration 71/1000 | Loss: 0.00029412
Iteration 72/1000 | Loss: 0.00140960
Iteration 73/1000 | Loss: 0.00031859
Iteration 74/1000 | Loss: 0.00027498
Iteration 75/1000 | Loss: 0.00010397
Iteration 76/1000 | Loss: 0.00022174
Iteration 77/1000 | Loss: 0.00015538
Iteration 78/1000 | Loss: 0.00018853
Iteration 79/1000 | Loss: 0.00025086
Iteration 80/1000 | Loss: 0.00010074
Iteration 81/1000 | Loss: 0.00012854
Iteration 82/1000 | Loss: 0.00060675
Iteration 83/1000 | Loss: 0.00023844
Iteration 84/1000 | Loss: 0.00017946
Iteration 85/1000 | Loss: 0.00013393
Iteration 86/1000 | Loss: 0.00091960
Iteration 87/1000 | Loss: 0.00051118
Iteration 88/1000 | Loss: 0.00097792
Iteration 89/1000 | Loss: 0.00044617
Iteration 90/1000 | Loss: 0.00043991
Iteration 91/1000 | Loss: 0.00012453
Iteration 92/1000 | Loss: 0.00011316
Iteration 93/1000 | Loss: 0.00013197
Iteration 94/1000 | Loss: 0.00023875
Iteration 95/1000 | Loss: 0.00017445
Iteration 96/1000 | Loss: 0.00037555
Iteration 97/1000 | Loss: 0.00024123
Iteration 98/1000 | Loss: 0.00037564
Iteration 99/1000 | Loss: 0.00010010
Iteration 100/1000 | Loss: 0.00009383
Iteration 101/1000 | Loss: 0.00026628
Iteration 102/1000 | Loss: 0.00008653
Iteration 103/1000 | Loss: 0.00008101
Iteration 104/1000 | Loss: 0.00021938
Iteration 105/1000 | Loss: 0.00009701
Iteration 106/1000 | Loss: 0.00007737
Iteration 107/1000 | Loss: 0.00024940
Iteration 108/1000 | Loss: 0.00021426
Iteration 109/1000 | Loss: 0.00068623
Iteration 110/1000 | Loss: 0.00104157
Iteration 111/1000 | Loss: 0.00088860
Iteration 112/1000 | Loss: 0.00013605
Iteration 113/1000 | Loss: 0.00036682
Iteration 114/1000 | Loss: 0.00020337
Iteration 115/1000 | Loss: 0.00026652
Iteration 116/1000 | Loss: 0.00009555
Iteration 117/1000 | Loss: 0.00026539
Iteration 118/1000 | Loss: 0.00009149
Iteration 119/1000 | Loss: 0.00008768
Iteration 120/1000 | Loss: 0.00008858
Iteration 121/1000 | Loss: 0.00025446
Iteration 122/1000 | Loss: 0.00020195
Iteration 123/1000 | Loss: 0.00024890
Iteration 124/1000 | Loss: 0.00022683
Iteration 125/1000 | Loss: 0.00026012
Iteration 126/1000 | Loss: 0.00017540
Iteration 127/1000 | Loss: 0.00009065
Iteration 128/1000 | Loss: 0.00025225
Iteration 129/1000 | Loss: 0.00008735
Iteration 130/1000 | Loss: 0.00008700
Iteration 131/1000 | Loss: 0.00008115
Iteration 132/1000 | Loss: 0.00020089
Iteration 133/1000 | Loss: 0.00008844
Iteration 134/1000 | Loss: 0.00017576
Iteration 135/1000 | Loss: 0.00017304
Iteration 136/1000 | Loss: 0.00020392
Iteration 137/1000 | Loss: 0.00016214
Iteration 138/1000 | Loss: 0.00020354
Iteration 139/1000 | Loss: 0.00025273
Iteration 140/1000 | Loss: 0.00013839
Iteration 141/1000 | Loss: 0.00007735
Iteration 142/1000 | Loss: 0.00009013
Iteration 143/1000 | Loss: 0.00009053
Iteration 144/1000 | Loss: 0.00013031
Iteration 145/1000 | Loss: 0.00008720
Iteration 146/1000 | Loss: 0.00008712
Iteration 147/1000 | Loss: 0.00008671
Iteration 148/1000 | Loss: 0.00022268
Iteration 149/1000 | Loss: 0.00008757
Iteration 150/1000 | Loss: 0.00016345
Iteration 151/1000 | Loss: 0.00009456
Iteration 152/1000 | Loss: 0.00011273
Iteration 153/1000 | Loss: 0.00021532
Iteration 154/1000 | Loss: 0.00016970
Iteration 155/1000 | Loss: 0.00018135
Iteration 156/1000 | Loss: 0.00010768
Iteration 157/1000 | Loss: 0.00015391
Iteration 158/1000 | Loss: 0.00013969
Iteration 159/1000 | Loss: 0.00016266
Iteration 160/1000 | Loss: 0.00015850
Iteration 161/1000 | Loss: 0.00009783
Iteration 162/1000 | Loss: 0.00009165
Iteration 163/1000 | Loss: 0.00010248
Iteration 164/1000 | Loss: 0.00009391
Iteration 165/1000 | Loss: 0.00008089
Iteration 166/1000 | Loss: 0.00009957
Iteration 167/1000 | Loss: 0.00007762
Iteration 168/1000 | Loss: 0.00007832
Iteration 169/1000 | Loss: 0.00007166
Iteration 170/1000 | Loss: 0.00007011
Iteration 171/1000 | Loss: 0.00007869
Iteration 172/1000 | Loss: 0.00006991
Iteration 173/1000 | Loss: 0.00022163
Iteration 174/1000 | Loss: 0.00007307
Iteration 175/1000 | Loss: 0.00049978
Iteration 176/1000 | Loss: 0.00089593
Iteration 177/1000 | Loss: 0.00044714
Iteration 178/1000 | Loss: 0.00044841
Iteration 179/1000 | Loss: 0.00040350
Iteration 180/1000 | Loss: 0.00036006
Iteration 181/1000 | Loss: 0.00015354
Iteration 182/1000 | Loss: 0.00022755
Iteration 183/1000 | Loss: 0.00008709
Iteration 184/1000 | Loss: 0.00008762
Iteration 185/1000 | Loss: 0.00006686
Iteration 186/1000 | Loss: 0.00044620
Iteration 187/1000 | Loss: 0.00028910
Iteration 188/1000 | Loss: 0.00015154
Iteration 189/1000 | Loss: 0.00007214
Iteration 190/1000 | Loss: 0.00006816
Iteration 191/1000 | Loss: 0.00006677
Iteration 192/1000 | Loss: 0.00039979
Iteration 193/1000 | Loss: 0.00009323
Iteration 194/1000 | Loss: 0.00007161
Iteration 195/1000 | Loss: 0.00006928
Iteration 196/1000 | Loss: 0.00048372
Iteration 197/1000 | Loss: 0.00026873
Iteration 198/1000 | Loss: 0.00006868
Iteration 199/1000 | Loss: 0.00039180
Iteration 200/1000 | Loss: 0.00074554
Iteration 201/1000 | Loss: 0.00010247
Iteration 202/1000 | Loss: 0.00008520
Iteration 203/1000 | Loss: 0.00008813
Iteration 204/1000 | Loss: 0.00006785
Iteration 205/1000 | Loss: 0.00022089
Iteration 206/1000 | Loss: 0.00029107
Iteration 207/1000 | Loss: 0.00006678
Iteration 208/1000 | Loss: 0.00017825
Iteration 209/1000 | Loss: 0.00007665
Iteration 210/1000 | Loss: 0.00008301
Iteration 211/1000 | Loss: 0.00006574
Iteration 212/1000 | Loss: 0.00006511
Iteration 213/1000 | Loss: 0.00006462
Iteration 214/1000 | Loss: 0.00006429
Iteration 215/1000 | Loss: 0.00006404
Iteration 216/1000 | Loss: 0.00006388
Iteration 217/1000 | Loss: 0.00006387
Iteration 218/1000 | Loss: 0.00042854
Iteration 219/1000 | Loss: 0.00043923
Iteration 220/1000 | Loss: 0.00016864
Iteration 221/1000 | Loss: 0.00047722
Iteration 222/1000 | Loss: 0.00024651
Iteration 223/1000 | Loss: 0.00021809
Iteration 224/1000 | Loss: 0.00007167
Iteration 225/1000 | Loss: 0.00006947
Iteration 226/1000 | Loss: 0.00025626
Iteration 227/1000 | Loss: 0.00071816
Iteration 228/1000 | Loss: 0.00010798
Iteration 229/1000 | Loss: 0.00008447
Iteration 230/1000 | Loss: 0.00027490
Iteration 231/1000 | Loss: 0.00046621
Iteration 232/1000 | Loss: 0.00025906
Iteration 233/1000 | Loss: 0.00063409
Iteration 234/1000 | Loss: 0.00011422
Iteration 235/1000 | Loss: 0.00011918
Iteration 236/1000 | Loss: 0.00010738
Iteration 237/1000 | Loss: 0.00006722
Iteration 238/1000 | Loss: 0.00016858
Iteration 239/1000 | Loss: 0.00006485
Iteration 240/1000 | Loss: 0.00011600
Iteration 241/1000 | Loss: 0.00006441
Iteration 242/1000 | Loss: 0.00006385
Iteration 243/1000 | Loss: 0.00006362
Iteration 244/1000 | Loss: 0.00006358
Iteration 245/1000 | Loss: 0.00006344
Iteration 246/1000 | Loss: 0.00006343
Iteration 247/1000 | Loss: 0.00006343
Iteration 248/1000 | Loss: 0.00006342
Iteration 249/1000 | Loss: 0.00006341
Iteration 250/1000 | Loss: 0.00006341
Iteration 251/1000 | Loss: 0.00006340
Iteration 252/1000 | Loss: 0.00006339
Iteration 253/1000 | Loss: 0.00008662
Iteration 254/1000 | Loss: 0.00014101
Iteration 255/1000 | Loss: 0.00007694
Iteration 256/1000 | Loss: 0.00008516
Iteration 257/1000 | Loss: 0.00006388
Iteration 258/1000 | Loss: 0.00006337
Iteration 259/1000 | Loss: 0.00008703
Iteration 260/1000 | Loss: 0.00006766
Iteration 261/1000 | Loss: 0.00006561
Iteration 262/1000 | Loss: 0.00006440
Iteration 263/1000 | Loss: 0.00006389
Iteration 264/1000 | Loss: 0.00006369
Iteration 265/1000 | Loss: 0.00006344
Iteration 266/1000 | Loss: 0.00006321
Iteration 267/1000 | Loss: 0.00006299
Iteration 268/1000 | Loss: 0.00006297
Iteration 269/1000 | Loss: 0.00006293
Iteration 270/1000 | Loss: 0.00006292
Iteration 271/1000 | Loss: 0.00006290
Iteration 272/1000 | Loss: 0.00006289
Iteration 273/1000 | Loss: 0.00006289
Iteration 274/1000 | Loss: 0.00006289
Iteration 275/1000 | Loss: 0.00006287
Iteration 276/1000 | Loss: 0.00006287
Iteration 277/1000 | Loss: 0.00006287
Iteration 278/1000 | Loss: 0.00006286
Iteration 279/1000 | Loss: 0.00006286
Iteration 280/1000 | Loss: 0.00006286
Iteration 281/1000 | Loss: 0.00006286
Iteration 282/1000 | Loss: 0.00006286
Iteration 283/1000 | Loss: 0.00006286
Iteration 284/1000 | Loss: 0.00006286
Iteration 285/1000 | Loss: 0.00006286
Iteration 286/1000 | Loss: 0.00006285
Iteration 287/1000 | Loss: 0.00006285
Iteration 288/1000 | Loss: 0.00006285
Iteration 289/1000 | Loss: 0.00006285
Iteration 290/1000 | Loss: 0.00006285
Iteration 291/1000 | Loss: 0.00006285
Iteration 292/1000 | Loss: 0.00006284
Iteration 293/1000 | Loss: 0.00006284
Iteration 294/1000 | Loss: 0.00006284
Iteration 295/1000 | Loss: 0.00006284
Iteration 296/1000 | Loss: 0.00006284
Iteration 297/1000 | Loss: 0.00006284
Iteration 298/1000 | Loss: 0.00006284
Iteration 299/1000 | Loss: 0.00006284
Iteration 300/1000 | Loss: 0.00006284
Iteration 301/1000 | Loss: 0.00006284
Iteration 302/1000 | Loss: 0.00006284
Iteration 303/1000 | Loss: 0.00006284
Iteration 304/1000 | Loss: 0.00006284
Iteration 305/1000 | Loss: 0.00006284
Iteration 306/1000 | Loss: 0.00006284
Iteration 307/1000 | Loss: 0.00006284
Iteration 308/1000 | Loss: 0.00006284
Iteration 309/1000 | Loss: 0.00006284
Iteration 310/1000 | Loss: 0.00006284
Iteration 311/1000 | Loss: 0.00006284
Iteration 312/1000 | Loss: 0.00006284
Iteration 313/1000 | Loss: 0.00006283
Iteration 314/1000 | Loss: 0.00006283
Iteration 315/1000 | Loss: 0.00006283
Iteration 316/1000 | Loss: 0.00006283
Iteration 317/1000 | Loss: 0.00006283
Iteration 318/1000 | Loss: 0.00006283
Iteration 319/1000 | Loss: 0.00006283
Iteration 320/1000 | Loss: 0.00006283
Iteration 321/1000 | Loss: 0.00006283
Iteration 322/1000 | Loss: 0.00006283
Iteration 323/1000 | Loss: 0.00006283
Iteration 324/1000 | Loss: 0.00006283
Iteration 325/1000 | Loss: 0.00006283
Iteration 326/1000 | Loss: 0.00006283
Iteration 327/1000 | Loss: 0.00006283
Iteration 328/1000 | Loss: 0.00006283
Iteration 329/1000 | Loss: 0.00006283
Iteration 330/1000 | Loss: 0.00006283
Iteration 331/1000 | Loss: 0.00006283
Iteration 332/1000 | Loss: 0.00006283
Iteration 333/1000 | Loss: 0.00006283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [6.282830872805789e-05, 6.282830872805789e-05, 6.282830872805789e-05, 6.282830872805789e-05, 6.282830872805789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.282830872805789e-05

Optimization complete. Final v2v error: 4.8914875984191895 mm

Highest mean error: 13.030274391174316 mm for frame 140

Lowest mean error: 3.3905258178710938 mm for frame 225

Saving results

Total time: 470.2547676563263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134035
Iteration 2/25 | Loss: 0.00173636
Iteration 3/25 | Loss: 0.00116779
Iteration 4/25 | Loss: 0.00108519
Iteration 5/25 | Loss: 0.00099636
Iteration 6/25 | Loss: 0.00097370
Iteration 7/25 | Loss: 0.00096010
Iteration 8/25 | Loss: 0.00096053
Iteration 9/25 | Loss: 0.00093147
Iteration 10/25 | Loss: 0.00091421
Iteration 11/25 | Loss: 0.00090771
Iteration 12/25 | Loss: 0.00090376
Iteration 13/25 | Loss: 0.00090834
Iteration 14/25 | Loss: 0.00091428
Iteration 15/25 | Loss: 0.00091666
Iteration 16/25 | Loss: 0.00090834
Iteration 17/25 | Loss: 0.00090372
Iteration 18/25 | Loss: 0.00089812
Iteration 19/25 | Loss: 0.00089547
Iteration 20/25 | Loss: 0.00089249
Iteration 21/25 | Loss: 0.00089279
Iteration 22/25 | Loss: 0.00089169
Iteration 23/25 | Loss: 0.00089236
Iteration 24/25 | Loss: 0.00089301
Iteration 25/25 | Loss: 0.00089235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28812420
Iteration 2/25 | Loss: 0.00043260
Iteration 3/25 | Loss: 0.00043257
Iteration 4/25 | Loss: 0.00043257
Iteration 5/25 | Loss: 0.00043256
Iteration 6/25 | Loss: 0.00043256
Iteration 7/25 | Loss: 0.00043256
Iteration 8/25 | Loss: 0.00043256
Iteration 9/25 | Loss: 0.00043256
Iteration 10/25 | Loss: 0.00043256
Iteration 11/25 | Loss: 0.00043256
Iteration 12/25 | Loss: 0.00043256
Iteration 13/25 | Loss: 0.00043256
Iteration 14/25 | Loss: 0.00043256
Iteration 15/25 | Loss: 0.00043256
Iteration 16/25 | Loss: 0.00043256
Iteration 17/25 | Loss: 0.00043256
Iteration 18/25 | Loss: 0.00043256
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000432563538197428, 0.000432563538197428, 0.000432563538197428, 0.000432563538197428, 0.000432563538197428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000432563538197428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043256
Iteration 2/1000 | Loss: 0.00008112
Iteration 3/1000 | Loss: 0.00005099
Iteration 4/1000 | Loss: 0.00005590
Iteration 5/1000 | Loss: 0.00004893
Iteration 6/1000 | Loss: 0.00239006
Iteration 7/1000 | Loss: 0.00015102
Iteration 8/1000 | Loss: 0.00005809
Iteration 9/1000 | Loss: 0.00004374
Iteration 10/1000 | Loss: 0.00003737
Iteration 11/1000 | Loss: 0.00003851
Iteration 12/1000 | Loss: 0.00003553
Iteration 13/1000 | Loss: 0.00003637
Iteration 14/1000 | Loss: 0.00003419
Iteration 15/1000 | Loss: 0.00003255
Iteration 16/1000 | Loss: 0.00003181
Iteration 17/1000 | Loss: 0.00003841
Iteration 18/1000 | Loss: 0.00003547
Iteration 19/1000 | Loss: 0.00003432
Iteration 20/1000 | Loss: 0.00003740
Iteration 21/1000 | Loss: 0.00003691
Iteration 22/1000 | Loss: 0.00003708
Iteration 23/1000 | Loss: 0.00003465
Iteration 24/1000 | Loss: 0.00003401
Iteration 25/1000 | Loss: 0.00003368
Iteration 26/1000 | Loss: 0.00003455
Iteration 27/1000 | Loss: 0.00003650
Iteration 28/1000 | Loss: 0.00003589
Iteration 29/1000 | Loss: 0.00003564
Iteration 30/1000 | Loss: 0.00003895
Iteration 31/1000 | Loss: 0.00003769
Iteration 32/1000 | Loss: 0.00003553
Iteration 33/1000 | Loss: 0.00003704
Iteration 34/1000 | Loss: 0.00003291
Iteration 35/1000 | Loss: 0.00003032
Iteration 36/1000 | Loss: 0.00002944
Iteration 37/1000 | Loss: 0.00002903
Iteration 38/1000 | Loss: 0.00002891
Iteration 39/1000 | Loss: 0.00002889
Iteration 40/1000 | Loss: 0.00002889
Iteration 41/1000 | Loss: 0.00002888
Iteration 42/1000 | Loss: 0.00002887
Iteration 43/1000 | Loss: 0.00002887
Iteration 44/1000 | Loss: 0.00002886
Iteration 45/1000 | Loss: 0.00002883
Iteration 46/1000 | Loss: 0.00002883
Iteration 47/1000 | Loss: 0.00002883
Iteration 48/1000 | Loss: 0.00002882
Iteration 49/1000 | Loss: 0.00002882
Iteration 50/1000 | Loss: 0.00002882
Iteration 51/1000 | Loss: 0.00002881
Iteration 52/1000 | Loss: 0.00002878
Iteration 53/1000 | Loss: 0.00002878
Iteration 54/1000 | Loss: 0.00002877
Iteration 55/1000 | Loss: 0.00002877
Iteration 56/1000 | Loss: 0.00002876
Iteration 57/1000 | Loss: 0.00002875
Iteration 58/1000 | Loss: 0.00002874
Iteration 59/1000 | Loss: 0.00002874
Iteration 60/1000 | Loss: 0.00002873
Iteration 61/1000 | Loss: 0.00002873
Iteration 62/1000 | Loss: 0.00002873
Iteration 63/1000 | Loss: 0.00002873
Iteration 64/1000 | Loss: 0.00002873
Iteration 65/1000 | Loss: 0.00002873
Iteration 66/1000 | Loss: 0.00002873
Iteration 67/1000 | Loss: 0.00002873
Iteration 68/1000 | Loss: 0.00002873
Iteration 69/1000 | Loss: 0.00002873
Iteration 70/1000 | Loss: 0.00002872
Iteration 71/1000 | Loss: 0.00002872
Iteration 72/1000 | Loss: 0.00002872
Iteration 73/1000 | Loss: 0.00002872
Iteration 74/1000 | Loss: 0.00002872
Iteration 75/1000 | Loss: 0.00002872
Iteration 76/1000 | Loss: 0.00002872
Iteration 77/1000 | Loss: 0.00002872
Iteration 78/1000 | Loss: 0.00002872
Iteration 79/1000 | Loss: 0.00002871
Iteration 80/1000 | Loss: 0.00002871
Iteration 81/1000 | Loss: 0.00002871
Iteration 82/1000 | Loss: 0.00002871
Iteration 83/1000 | Loss: 0.00002871
Iteration 84/1000 | Loss: 0.00002871
Iteration 85/1000 | Loss: 0.00002871
Iteration 86/1000 | Loss: 0.00002871
Iteration 87/1000 | Loss: 0.00002870
Iteration 88/1000 | Loss: 0.00002870
Iteration 89/1000 | Loss: 0.00002870
Iteration 90/1000 | Loss: 0.00002870
Iteration 91/1000 | Loss: 0.00002870
Iteration 92/1000 | Loss: 0.00002870
Iteration 93/1000 | Loss: 0.00002870
Iteration 94/1000 | Loss: 0.00002870
Iteration 95/1000 | Loss: 0.00002869
Iteration 96/1000 | Loss: 0.00002869
Iteration 97/1000 | Loss: 0.00002869
Iteration 98/1000 | Loss: 0.00002868
Iteration 99/1000 | Loss: 0.00002868
Iteration 100/1000 | Loss: 0.00002868
Iteration 101/1000 | Loss: 0.00002868
Iteration 102/1000 | Loss: 0.00002868
Iteration 103/1000 | Loss: 0.00002868
Iteration 104/1000 | Loss: 0.00002868
Iteration 105/1000 | Loss: 0.00002868
Iteration 106/1000 | Loss: 0.00002868
Iteration 107/1000 | Loss: 0.00002868
Iteration 108/1000 | Loss: 0.00002868
Iteration 109/1000 | Loss: 0.00002867
Iteration 110/1000 | Loss: 0.00002867
Iteration 111/1000 | Loss: 0.00002867
Iteration 112/1000 | Loss: 0.00002867
Iteration 113/1000 | Loss: 0.00002867
Iteration 114/1000 | Loss: 0.00002867
Iteration 115/1000 | Loss: 0.00002867
Iteration 116/1000 | Loss: 0.00002866
Iteration 117/1000 | Loss: 0.00002866
Iteration 118/1000 | Loss: 0.00002866
Iteration 119/1000 | Loss: 0.00002866
Iteration 120/1000 | Loss: 0.00002866
Iteration 121/1000 | Loss: 0.00002866
Iteration 122/1000 | Loss: 0.00002866
Iteration 123/1000 | Loss: 0.00002866
Iteration 124/1000 | Loss: 0.00002866
Iteration 125/1000 | Loss: 0.00002866
Iteration 126/1000 | Loss: 0.00002866
Iteration 127/1000 | Loss: 0.00002866
Iteration 128/1000 | Loss: 0.00002866
Iteration 129/1000 | Loss: 0.00002866
Iteration 130/1000 | Loss: 0.00002866
Iteration 131/1000 | Loss: 0.00002866
Iteration 132/1000 | Loss: 0.00002866
Iteration 133/1000 | Loss: 0.00002866
Iteration 134/1000 | Loss: 0.00002866
Iteration 135/1000 | Loss: 0.00002866
Iteration 136/1000 | Loss: 0.00002866
Iteration 137/1000 | Loss: 0.00002866
Iteration 138/1000 | Loss: 0.00002866
Iteration 139/1000 | Loss: 0.00002866
Iteration 140/1000 | Loss: 0.00002866
Iteration 141/1000 | Loss: 0.00002866
Iteration 142/1000 | Loss: 0.00002866
Iteration 143/1000 | Loss: 0.00002866
Iteration 144/1000 | Loss: 0.00002866
Iteration 145/1000 | Loss: 0.00002866
Iteration 146/1000 | Loss: 0.00002866
Iteration 147/1000 | Loss: 0.00002866
Iteration 148/1000 | Loss: 0.00002866
Iteration 149/1000 | Loss: 0.00002866
Iteration 150/1000 | Loss: 0.00002866
Iteration 151/1000 | Loss: 0.00002866
Iteration 152/1000 | Loss: 0.00002866
Iteration 153/1000 | Loss: 0.00002866
Iteration 154/1000 | Loss: 0.00002866
Iteration 155/1000 | Loss: 0.00002866
Iteration 156/1000 | Loss: 0.00002866
Iteration 157/1000 | Loss: 0.00002866
Iteration 158/1000 | Loss: 0.00002866
Iteration 159/1000 | Loss: 0.00002866
Iteration 160/1000 | Loss: 0.00002866
Iteration 161/1000 | Loss: 0.00002866
Iteration 162/1000 | Loss: 0.00002866
Iteration 163/1000 | Loss: 0.00002866
Iteration 164/1000 | Loss: 0.00002866
Iteration 165/1000 | Loss: 0.00002866
Iteration 166/1000 | Loss: 0.00002866
Iteration 167/1000 | Loss: 0.00002866
Iteration 168/1000 | Loss: 0.00002866
Iteration 169/1000 | Loss: 0.00002866
Iteration 170/1000 | Loss: 0.00002866
Iteration 171/1000 | Loss: 0.00002866
Iteration 172/1000 | Loss: 0.00002866
Iteration 173/1000 | Loss: 0.00002866
Iteration 174/1000 | Loss: 0.00002866
Iteration 175/1000 | Loss: 0.00002866
Iteration 176/1000 | Loss: 0.00002866
Iteration 177/1000 | Loss: 0.00002866
Iteration 178/1000 | Loss: 0.00002866
Iteration 179/1000 | Loss: 0.00002866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.8656322683673352e-05, 2.8656322683673352e-05, 2.8656322683673352e-05, 2.8656322683673352e-05, 2.8656322683673352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8656322683673352e-05

Optimization complete. Final v2v error: 4.357321262359619 mm

Highest mean error: 5.898108005523682 mm for frame 228

Lowest mean error: 3.780592441558838 mm for frame 55

Saving results

Total time: 121.88761806488037
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905238
Iteration 2/25 | Loss: 0.00124308
Iteration 3/25 | Loss: 0.00080076
Iteration 4/25 | Loss: 0.00076244
Iteration 5/25 | Loss: 0.00075810
Iteration 6/25 | Loss: 0.00075780
Iteration 7/25 | Loss: 0.00075780
Iteration 8/25 | Loss: 0.00075780
Iteration 9/25 | Loss: 0.00075780
Iteration 10/25 | Loss: 0.00075780
Iteration 11/25 | Loss: 0.00075780
Iteration 12/25 | Loss: 0.00075780
Iteration 13/25 | Loss: 0.00075780
Iteration 14/25 | Loss: 0.00075780
Iteration 15/25 | Loss: 0.00075780
Iteration 16/25 | Loss: 0.00075780
Iteration 17/25 | Loss: 0.00075780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007578041404485703, 0.0007578041404485703, 0.0007578041404485703, 0.0007578041404485703, 0.0007578041404485703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007578041404485703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.03499508
Iteration 2/25 | Loss: 0.00026206
Iteration 3/25 | Loss: 0.00026204
Iteration 4/25 | Loss: 0.00026204
Iteration 5/25 | Loss: 0.00026204
Iteration 6/25 | Loss: 0.00026204
Iteration 7/25 | Loss: 0.00026204
Iteration 8/25 | Loss: 0.00026204
Iteration 9/25 | Loss: 0.00026204
Iteration 10/25 | Loss: 0.00026204
Iteration 11/25 | Loss: 0.00026204
Iteration 12/25 | Loss: 0.00026204
Iteration 13/25 | Loss: 0.00026204
Iteration 14/25 | Loss: 0.00026204
Iteration 15/25 | Loss: 0.00026204
Iteration 16/25 | Loss: 0.00026204
Iteration 17/25 | Loss: 0.00026204
Iteration 18/25 | Loss: 0.00026204
Iteration 19/25 | Loss: 0.00026204
Iteration 20/25 | Loss: 0.00026204
Iteration 21/25 | Loss: 0.00026204
Iteration 22/25 | Loss: 0.00026204
Iteration 23/25 | Loss: 0.00026204
Iteration 24/25 | Loss: 0.00026204
Iteration 25/25 | Loss: 0.00026204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026204
Iteration 2/1000 | Loss: 0.00003292
Iteration 3/1000 | Loss: 0.00002161
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001720
Iteration 7/1000 | Loss: 0.00001679
Iteration 8/1000 | Loss: 0.00001632
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00001613
Iteration 11/1000 | Loss: 0.00001594
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001583
Iteration 14/1000 | Loss: 0.00001583
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00001576
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001574
Iteration 20/1000 | Loss: 0.00001572
Iteration 21/1000 | Loss: 0.00001572
Iteration 22/1000 | Loss: 0.00001571
Iteration 23/1000 | Loss: 0.00001569
Iteration 24/1000 | Loss: 0.00001569
Iteration 25/1000 | Loss: 0.00001568
Iteration 26/1000 | Loss: 0.00001568
Iteration 27/1000 | Loss: 0.00001567
Iteration 28/1000 | Loss: 0.00001567
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001567
Iteration 32/1000 | Loss: 0.00001562
Iteration 33/1000 | Loss: 0.00001562
Iteration 34/1000 | Loss: 0.00001562
Iteration 35/1000 | Loss: 0.00001562
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001561
Iteration 45/1000 | Loss: 0.00001561
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001556
Iteration 51/1000 | Loss: 0.00001556
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001556
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001555
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001551
Iteration 65/1000 | Loss: 0.00001551
Iteration 66/1000 | Loss: 0.00001551
Iteration 67/1000 | Loss: 0.00001551
Iteration 68/1000 | Loss: 0.00001551
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001550
Iteration 76/1000 | Loss: 0.00001550
Iteration 77/1000 | Loss: 0.00001550
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001549
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001548
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001548
Iteration 107/1000 | Loss: 0.00001547
Iteration 108/1000 | Loss: 0.00001547
Iteration 109/1000 | Loss: 0.00001547
Iteration 110/1000 | Loss: 0.00001547
Iteration 111/1000 | Loss: 0.00001547
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001547
Iteration 116/1000 | Loss: 0.00001546
Iteration 117/1000 | Loss: 0.00001546
Iteration 118/1000 | Loss: 0.00001546
Iteration 119/1000 | Loss: 0.00001546
Iteration 120/1000 | Loss: 0.00001546
Iteration 121/1000 | Loss: 0.00001546
Iteration 122/1000 | Loss: 0.00001546
Iteration 123/1000 | Loss: 0.00001546
Iteration 124/1000 | Loss: 0.00001546
Iteration 125/1000 | Loss: 0.00001546
Iteration 126/1000 | Loss: 0.00001546
Iteration 127/1000 | Loss: 0.00001546
Iteration 128/1000 | Loss: 0.00001546
Iteration 129/1000 | Loss: 0.00001546
Iteration 130/1000 | Loss: 0.00001546
Iteration 131/1000 | Loss: 0.00001546
Iteration 132/1000 | Loss: 0.00001546
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001546
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.5461069779121317e-05, 1.5461069779121317e-05, 1.5461069779121317e-05, 1.5461069779121317e-05, 1.5461069779121317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5461069779121317e-05

Optimization complete. Final v2v error: 3.4383132457733154 mm

Highest mean error: 3.916456460952759 mm for frame 0

Lowest mean error: 3.2177517414093018 mm for frame 230

Saving results

Total time: 38.11746597290039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624985
Iteration 2/25 | Loss: 0.00115403
Iteration 3/25 | Loss: 0.00086900
Iteration 4/25 | Loss: 0.00081907
Iteration 5/25 | Loss: 0.00079861
Iteration 6/25 | Loss: 0.00079402
Iteration 7/25 | Loss: 0.00079272
Iteration 8/25 | Loss: 0.00079245
Iteration 9/25 | Loss: 0.00079245
Iteration 10/25 | Loss: 0.00079245
Iteration 11/25 | Loss: 0.00079245
Iteration 12/25 | Loss: 0.00079245
Iteration 13/25 | Loss: 0.00079245
Iteration 14/25 | Loss: 0.00079245
Iteration 15/25 | Loss: 0.00079245
Iteration 16/25 | Loss: 0.00079245
Iteration 17/25 | Loss: 0.00079245
Iteration 18/25 | Loss: 0.00079245
Iteration 19/25 | Loss: 0.00079245
Iteration 20/25 | Loss: 0.00079245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007924509700387716, 0.0007924509700387716, 0.0007924509700387716, 0.0007924509700387716, 0.0007924509700387716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007924509700387716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54853678
Iteration 2/25 | Loss: 0.00027376
Iteration 3/25 | Loss: 0.00027375
Iteration 4/25 | Loss: 0.00027375
Iteration 5/25 | Loss: 0.00027375
Iteration 6/25 | Loss: 0.00027375
Iteration 7/25 | Loss: 0.00027375
Iteration 8/25 | Loss: 0.00027375
Iteration 9/25 | Loss: 0.00027375
Iteration 10/25 | Loss: 0.00027375
Iteration 11/25 | Loss: 0.00027375
Iteration 12/25 | Loss: 0.00027375
Iteration 13/25 | Loss: 0.00027375
Iteration 14/25 | Loss: 0.00027375
Iteration 15/25 | Loss: 0.00027375
Iteration 16/25 | Loss: 0.00027375
Iteration 17/25 | Loss: 0.00027375
Iteration 18/25 | Loss: 0.00027375
Iteration 19/25 | Loss: 0.00027375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002737480099312961, 0.0002737480099312961, 0.0002737480099312961, 0.0002737480099312961, 0.0002737480099312961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002737480099312961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027375
Iteration 2/1000 | Loss: 0.00005384
Iteration 3/1000 | Loss: 0.00003456
Iteration 4/1000 | Loss: 0.00003000
Iteration 5/1000 | Loss: 0.00002827
Iteration 6/1000 | Loss: 0.00002704
Iteration 7/1000 | Loss: 0.00002633
Iteration 8/1000 | Loss: 0.00002591
Iteration 9/1000 | Loss: 0.00002542
Iteration 10/1000 | Loss: 0.00002505
Iteration 11/1000 | Loss: 0.00002480
Iteration 12/1000 | Loss: 0.00002462
Iteration 13/1000 | Loss: 0.00002450
Iteration 14/1000 | Loss: 0.00002443
Iteration 15/1000 | Loss: 0.00002436
Iteration 16/1000 | Loss: 0.00002434
Iteration 17/1000 | Loss: 0.00002432
Iteration 18/1000 | Loss: 0.00002432
Iteration 19/1000 | Loss: 0.00002432
Iteration 20/1000 | Loss: 0.00002431
Iteration 21/1000 | Loss: 0.00002431
Iteration 22/1000 | Loss: 0.00002430
Iteration 23/1000 | Loss: 0.00002430
Iteration 24/1000 | Loss: 0.00002427
Iteration 25/1000 | Loss: 0.00002424
Iteration 26/1000 | Loss: 0.00002423
Iteration 27/1000 | Loss: 0.00002422
Iteration 28/1000 | Loss: 0.00002422
Iteration 29/1000 | Loss: 0.00002421
Iteration 30/1000 | Loss: 0.00002421
Iteration 31/1000 | Loss: 0.00002418
Iteration 32/1000 | Loss: 0.00002418
Iteration 33/1000 | Loss: 0.00002418
Iteration 34/1000 | Loss: 0.00002415
Iteration 35/1000 | Loss: 0.00002415
Iteration 36/1000 | Loss: 0.00002415
Iteration 37/1000 | Loss: 0.00002414
Iteration 38/1000 | Loss: 0.00002413
Iteration 39/1000 | Loss: 0.00002413
Iteration 40/1000 | Loss: 0.00002410
Iteration 41/1000 | Loss: 0.00002410
Iteration 42/1000 | Loss: 0.00002410
Iteration 43/1000 | Loss: 0.00002409
Iteration 44/1000 | Loss: 0.00002409
Iteration 45/1000 | Loss: 0.00002409
Iteration 46/1000 | Loss: 0.00002409
Iteration 47/1000 | Loss: 0.00002408
Iteration 48/1000 | Loss: 0.00002408
Iteration 49/1000 | Loss: 0.00002408
Iteration 50/1000 | Loss: 0.00002407
Iteration 51/1000 | Loss: 0.00002407
Iteration 52/1000 | Loss: 0.00002407
Iteration 53/1000 | Loss: 0.00002407
Iteration 54/1000 | Loss: 0.00002406
Iteration 55/1000 | Loss: 0.00002406
Iteration 56/1000 | Loss: 0.00002406
Iteration 57/1000 | Loss: 0.00002406
Iteration 58/1000 | Loss: 0.00002406
Iteration 59/1000 | Loss: 0.00002406
Iteration 60/1000 | Loss: 0.00002406
Iteration 61/1000 | Loss: 0.00002406
Iteration 62/1000 | Loss: 0.00002406
Iteration 63/1000 | Loss: 0.00002405
Iteration 64/1000 | Loss: 0.00002405
Iteration 65/1000 | Loss: 0.00002405
Iteration 66/1000 | Loss: 0.00002405
Iteration 67/1000 | Loss: 0.00002404
Iteration 68/1000 | Loss: 0.00002404
Iteration 69/1000 | Loss: 0.00002404
Iteration 70/1000 | Loss: 0.00002404
Iteration 71/1000 | Loss: 0.00002404
Iteration 72/1000 | Loss: 0.00002403
Iteration 73/1000 | Loss: 0.00002403
Iteration 74/1000 | Loss: 0.00002403
Iteration 75/1000 | Loss: 0.00002403
Iteration 76/1000 | Loss: 0.00002403
Iteration 77/1000 | Loss: 0.00002403
Iteration 78/1000 | Loss: 0.00002402
Iteration 79/1000 | Loss: 0.00002402
Iteration 80/1000 | Loss: 0.00002402
Iteration 81/1000 | Loss: 0.00002402
Iteration 82/1000 | Loss: 0.00002402
Iteration 83/1000 | Loss: 0.00002402
Iteration 84/1000 | Loss: 0.00002402
Iteration 85/1000 | Loss: 0.00002402
Iteration 86/1000 | Loss: 0.00002402
Iteration 87/1000 | Loss: 0.00002402
Iteration 88/1000 | Loss: 0.00002401
Iteration 89/1000 | Loss: 0.00002401
Iteration 90/1000 | Loss: 0.00002401
Iteration 91/1000 | Loss: 0.00002401
Iteration 92/1000 | Loss: 0.00002401
Iteration 93/1000 | Loss: 0.00002401
Iteration 94/1000 | Loss: 0.00002401
Iteration 95/1000 | Loss: 0.00002400
Iteration 96/1000 | Loss: 0.00002400
Iteration 97/1000 | Loss: 0.00002400
Iteration 98/1000 | Loss: 0.00002400
Iteration 99/1000 | Loss: 0.00002400
Iteration 100/1000 | Loss: 0.00002400
Iteration 101/1000 | Loss: 0.00002400
Iteration 102/1000 | Loss: 0.00002400
Iteration 103/1000 | Loss: 0.00002400
Iteration 104/1000 | Loss: 0.00002399
Iteration 105/1000 | Loss: 0.00002399
Iteration 106/1000 | Loss: 0.00002399
Iteration 107/1000 | Loss: 0.00002399
Iteration 108/1000 | Loss: 0.00002399
Iteration 109/1000 | Loss: 0.00002399
Iteration 110/1000 | Loss: 0.00002399
Iteration 111/1000 | Loss: 0.00002399
Iteration 112/1000 | Loss: 0.00002399
Iteration 113/1000 | Loss: 0.00002399
Iteration 114/1000 | Loss: 0.00002399
Iteration 115/1000 | Loss: 0.00002399
Iteration 116/1000 | Loss: 0.00002399
Iteration 117/1000 | Loss: 0.00002398
Iteration 118/1000 | Loss: 0.00002398
Iteration 119/1000 | Loss: 0.00002398
Iteration 120/1000 | Loss: 0.00002398
Iteration 121/1000 | Loss: 0.00002398
Iteration 122/1000 | Loss: 0.00002398
Iteration 123/1000 | Loss: 0.00002398
Iteration 124/1000 | Loss: 0.00002398
Iteration 125/1000 | Loss: 0.00002398
Iteration 126/1000 | Loss: 0.00002398
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002398
Iteration 129/1000 | Loss: 0.00002398
Iteration 130/1000 | Loss: 0.00002398
Iteration 131/1000 | Loss: 0.00002398
Iteration 132/1000 | Loss: 0.00002398
Iteration 133/1000 | Loss: 0.00002398
Iteration 134/1000 | Loss: 0.00002398
Iteration 135/1000 | Loss: 0.00002398
Iteration 136/1000 | Loss: 0.00002398
Iteration 137/1000 | Loss: 0.00002398
Iteration 138/1000 | Loss: 0.00002398
Iteration 139/1000 | Loss: 0.00002398
Iteration 140/1000 | Loss: 0.00002398
Iteration 141/1000 | Loss: 0.00002398
Iteration 142/1000 | Loss: 0.00002398
Iteration 143/1000 | Loss: 0.00002398
Iteration 144/1000 | Loss: 0.00002398
Iteration 145/1000 | Loss: 0.00002398
Iteration 146/1000 | Loss: 0.00002398
Iteration 147/1000 | Loss: 0.00002398
Iteration 148/1000 | Loss: 0.00002398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.3978403987712227e-05, 2.3978403987712227e-05, 2.3978403987712227e-05, 2.3978403987712227e-05, 2.3978403987712227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3978403987712227e-05

Optimization complete. Final v2v error: 4.126709938049316 mm

Highest mean error: 5.09490966796875 mm for frame 104

Lowest mean error: 3.2095932960510254 mm for frame 221

Saving results

Total time: 46.753116846084595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409821
Iteration 2/25 | Loss: 0.00089203
Iteration 3/25 | Loss: 0.00070057
Iteration 4/25 | Loss: 0.00067885
Iteration 5/25 | Loss: 0.00066959
Iteration 6/25 | Loss: 0.00066621
Iteration 7/25 | Loss: 0.00066531
Iteration 8/25 | Loss: 0.00066513
Iteration 9/25 | Loss: 0.00066513
Iteration 10/25 | Loss: 0.00066512
Iteration 11/25 | Loss: 0.00066512
Iteration 12/25 | Loss: 0.00066512
Iteration 13/25 | Loss: 0.00066512
Iteration 14/25 | Loss: 0.00066512
Iteration 15/25 | Loss: 0.00066512
Iteration 16/25 | Loss: 0.00066512
Iteration 17/25 | Loss: 0.00066512
Iteration 18/25 | Loss: 0.00066512
Iteration 19/25 | Loss: 0.00066510
Iteration 20/25 | Loss: 0.00066510
Iteration 21/25 | Loss: 0.00066510
Iteration 22/25 | Loss: 0.00066510
Iteration 23/25 | Loss: 0.00066510
Iteration 24/25 | Loss: 0.00066510
Iteration 25/25 | Loss: 0.00066510

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34813643
Iteration 2/25 | Loss: 0.00016016
Iteration 3/25 | Loss: 0.00016013
Iteration 4/25 | Loss: 0.00016013
Iteration 5/25 | Loss: 0.00016013
Iteration 6/25 | Loss: 0.00016013
Iteration 7/25 | Loss: 0.00016013
Iteration 8/25 | Loss: 0.00016013
Iteration 9/25 | Loss: 0.00016013
Iteration 10/25 | Loss: 0.00016013
Iteration 11/25 | Loss: 0.00016013
Iteration 12/25 | Loss: 0.00016013
Iteration 13/25 | Loss: 0.00016013
Iteration 14/25 | Loss: 0.00016013
Iteration 15/25 | Loss: 0.00016013
Iteration 16/25 | Loss: 0.00016013
Iteration 17/25 | Loss: 0.00016013
Iteration 18/25 | Loss: 0.00016013
Iteration 19/25 | Loss: 0.00016013
Iteration 20/25 | Loss: 0.00016013
Iteration 21/25 | Loss: 0.00016013
Iteration 22/25 | Loss: 0.00016013
Iteration 23/25 | Loss: 0.00016013
Iteration 24/25 | Loss: 0.00016013
Iteration 25/25 | Loss: 0.00016013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016013
Iteration 2/1000 | Loss: 0.00003997
Iteration 3/1000 | Loss: 0.00002065
Iteration 4/1000 | Loss: 0.00001685
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001431
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001348
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001332
Iteration 16/1000 | Loss: 0.00001331
Iteration 17/1000 | Loss: 0.00001330
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001321
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001319
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001318
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001314
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001301
Iteration 37/1000 | Loss: 0.00001300
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001297
Iteration 41/1000 | Loss: 0.00001297
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001295
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001294
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001287
Iteration 61/1000 | Loss: 0.00001287
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001286
Iteration 68/1000 | Loss: 0.00001286
Iteration 69/1000 | Loss: 0.00001286
Iteration 70/1000 | Loss: 0.00001286
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001280
Iteration 90/1000 | Loss: 0.00001280
Iteration 91/1000 | Loss: 0.00001280
Iteration 92/1000 | Loss: 0.00001280
Iteration 93/1000 | Loss: 0.00001279
Iteration 94/1000 | Loss: 0.00001279
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001278
Iteration 98/1000 | Loss: 0.00001278
Iteration 99/1000 | Loss: 0.00001278
Iteration 100/1000 | Loss: 0.00001278
Iteration 101/1000 | Loss: 0.00001278
Iteration 102/1000 | Loss: 0.00001278
Iteration 103/1000 | Loss: 0.00001278
Iteration 104/1000 | Loss: 0.00001278
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001278
Iteration 110/1000 | Loss: 0.00001278
Iteration 111/1000 | Loss: 0.00001278
Iteration 112/1000 | Loss: 0.00001278
Iteration 113/1000 | Loss: 0.00001278
Iteration 114/1000 | Loss: 0.00001278
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001278
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001277
Iteration 124/1000 | Loss: 0.00001277
Iteration 125/1000 | Loss: 0.00001277
Iteration 126/1000 | Loss: 0.00001277
Iteration 127/1000 | Loss: 0.00001277
Iteration 128/1000 | Loss: 0.00001277
Iteration 129/1000 | Loss: 0.00001277
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001277
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001277
Iteration 138/1000 | Loss: 0.00001277
Iteration 139/1000 | Loss: 0.00001277
Iteration 140/1000 | Loss: 0.00001277
Iteration 141/1000 | Loss: 0.00001277
Iteration 142/1000 | Loss: 0.00001277
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001277
Iteration 149/1000 | Loss: 0.00001277
Iteration 150/1000 | Loss: 0.00001277
Iteration 151/1000 | Loss: 0.00001277
Iteration 152/1000 | Loss: 0.00001277
Iteration 153/1000 | Loss: 0.00001277
Iteration 154/1000 | Loss: 0.00001277
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001277
Iteration 163/1000 | Loss: 0.00001277
Iteration 164/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.2767331099894363e-05, 1.2767331099894363e-05, 1.2767331099894363e-05, 1.2767331099894363e-05, 1.2767331099894363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2767331099894363e-05

Optimization complete. Final v2v error: 2.820086717605591 mm

Highest mean error: 4.9426116943359375 mm for frame 83

Lowest mean error: 2.187627077102661 mm for frame 119

Saving results

Total time: 41.026665449142456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051199
Iteration 2/25 | Loss: 0.00232603
Iteration 3/25 | Loss: 0.00164125
Iteration 4/25 | Loss: 0.00136030
Iteration 5/25 | Loss: 0.00127313
Iteration 6/25 | Loss: 0.00120895
Iteration 7/25 | Loss: 0.00113015
Iteration 8/25 | Loss: 0.00111356
Iteration 9/25 | Loss: 0.00111349
Iteration 10/25 | Loss: 0.00110010
Iteration 11/25 | Loss: 0.00102654
Iteration 12/25 | Loss: 0.00097434
Iteration 13/25 | Loss: 0.00090996
Iteration 14/25 | Loss: 0.00087754
Iteration 15/25 | Loss: 0.00086069
Iteration 16/25 | Loss: 0.00084743
Iteration 17/25 | Loss: 0.00084300
Iteration 18/25 | Loss: 0.00082487
Iteration 19/25 | Loss: 0.00081720
Iteration 20/25 | Loss: 0.00081469
Iteration 21/25 | Loss: 0.00081365
Iteration 22/25 | Loss: 0.00081340
Iteration 23/25 | Loss: 0.00081335
Iteration 24/25 | Loss: 0.00081335
Iteration 25/25 | Loss: 0.00081334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42916358
Iteration 2/25 | Loss: 0.00116057
Iteration 3/25 | Loss: 0.00095886
Iteration 4/25 | Loss: 0.00095886
Iteration 5/25 | Loss: 0.00095886
Iteration 6/25 | Loss: 0.00095886
Iteration 7/25 | Loss: 0.00095886
Iteration 8/25 | Loss: 0.00095886
Iteration 9/25 | Loss: 0.00095886
Iteration 10/25 | Loss: 0.00095886
Iteration 11/25 | Loss: 0.00095886
Iteration 12/25 | Loss: 0.00095886
Iteration 13/25 | Loss: 0.00095886
Iteration 14/25 | Loss: 0.00095886
Iteration 15/25 | Loss: 0.00095886
Iteration 16/25 | Loss: 0.00095886
Iteration 17/25 | Loss: 0.00095886
Iteration 18/25 | Loss: 0.00095886
Iteration 19/25 | Loss: 0.00095886
Iteration 20/25 | Loss: 0.00095886
Iteration 21/25 | Loss: 0.00095886
Iteration 22/25 | Loss: 0.00095886
Iteration 23/25 | Loss: 0.00095886
Iteration 24/25 | Loss: 0.00095886
Iteration 25/25 | Loss: 0.00095886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095886
Iteration 2/1000 | Loss: 0.00046376
Iteration 3/1000 | Loss: 0.00152428
Iteration 4/1000 | Loss: 0.00011388
Iteration 5/1000 | Loss: 0.00060894
Iteration 6/1000 | Loss: 0.00007419
Iteration 7/1000 | Loss: 0.00006461
Iteration 8/1000 | Loss: 0.00005965
Iteration 9/1000 | Loss: 0.00012683
Iteration 10/1000 | Loss: 0.00011155
Iteration 11/1000 | Loss: 0.00005237
Iteration 12/1000 | Loss: 0.00008050
Iteration 13/1000 | Loss: 0.00005033
Iteration 14/1000 | Loss: 0.00028871
Iteration 15/1000 | Loss: 0.00139648
Iteration 16/1000 | Loss: 0.00033068
Iteration 17/1000 | Loss: 0.00032555
Iteration 18/1000 | Loss: 0.00012270
Iteration 19/1000 | Loss: 0.00015566
Iteration 20/1000 | Loss: 0.00024584
Iteration 21/1000 | Loss: 0.00012779
Iteration 22/1000 | Loss: 0.00031659
Iteration 23/1000 | Loss: 0.00093165
Iteration 24/1000 | Loss: 0.00040309
Iteration 25/1000 | Loss: 0.00022229
Iteration 26/1000 | Loss: 0.00006233
Iteration 27/1000 | Loss: 0.00009709
Iteration 28/1000 | Loss: 0.00008525
Iteration 29/1000 | Loss: 0.00033605
Iteration 30/1000 | Loss: 0.00004567
Iteration 31/1000 | Loss: 0.00004267
Iteration 32/1000 | Loss: 0.00004067
Iteration 33/1000 | Loss: 0.00035548
Iteration 34/1000 | Loss: 0.00020371
Iteration 35/1000 | Loss: 0.00004894
Iteration 36/1000 | Loss: 0.00004326
Iteration 37/1000 | Loss: 0.00004046
Iteration 38/1000 | Loss: 0.00003919
Iteration 39/1000 | Loss: 0.00003807
Iteration 40/1000 | Loss: 0.00003752
Iteration 41/1000 | Loss: 0.00003708
Iteration 42/1000 | Loss: 0.00003686
Iteration 43/1000 | Loss: 0.00003664
Iteration 44/1000 | Loss: 0.00003649
Iteration 45/1000 | Loss: 0.00003631
Iteration 46/1000 | Loss: 0.00003628
Iteration 47/1000 | Loss: 0.00003621
Iteration 48/1000 | Loss: 0.00003618
Iteration 49/1000 | Loss: 0.00003615
Iteration 50/1000 | Loss: 0.00003612
Iteration 51/1000 | Loss: 0.00003612
Iteration 52/1000 | Loss: 0.00003611
Iteration 53/1000 | Loss: 0.00003611
Iteration 54/1000 | Loss: 0.00003611
Iteration 55/1000 | Loss: 0.00003611
Iteration 56/1000 | Loss: 0.00003610
Iteration 57/1000 | Loss: 0.00003610
Iteration 58/1000 | Loss: 0.00003610
Iteration 59/1000 | Loss: 0.00003609
Iteration 60/1000 | Loss: 0.00003609
Iteration 61/1000 | Loss: 0.00003608
Iteration 62/1000 | Loss: 0.00003608
Iteration 63/1000 | Loss: 0.00003608
Iteration 64/1000 | Loss: 0.00003608
Iteration 65/1000 | Loss: 0.00003608
Iteration 66/1000 | Loss: 0.00003607
Iteration 67/1000 | Loss: 0.00003607
Iteration 68/1000 | Loss: 0.00003605
Iteration 69/1000 | Loss: 0.00003605
Iteration 70/1000 | Loss: 0.00003605
Iteration 71/1000 | Loss: 0.00003605
Iteration 72/1000 | Loss: 0.00003605
Iteration 73/1000 | Loss: 0.00003605
Iteration 74/1000 | Loss: 0.00003605
Iteration 75/1000 | Loss: 0.00003605
Iteration 76/1000 | Loss: 0.00003604
Iteration 77/1000 | Loss: 0.00003604
Iteration 78/1000 | Loss: 0.00003604
Iteration 79/1000 | Loss: 0.00003604
Iteration 80/1000 | Loss: 0.00003604
Iteration 81/1000 | Loss: 0.00003604
Iteration 82/1000 | Loss: 0.00003604
Iteration 83/1000 | Loss: 0.00003604
Iteration 84/1000 | Loss: 0.00003604
Iteration 85/1000 | Loss: 0.00003604
Iteration 86/1000 | Loss: 0.00003604
Iteration 87/1000 | Loss: 0.00003603
Iteration 88/1000 | Loss: 0.00003603
Iteration 89/1000 | Loss: 0.00003603
Iteration 90/1000 | Loss: 0.00003603
Iteration 91/1000 | Loss: 0.00003603
Iteration 92/1000 | Loss: 0.00003603
Iteration 93/1000 | Loss: 0.00003603
Iteration 94/1000 | Loss: 0.00003602
Iteration 95/1000 | Loss: 0.00003602
Iteration 96/1000 | Loss: 0.00003602
Iteration 97/1000 | Loss: 0.00003602
Iteration 98/1000 | Loss: 0.00003602
Iteration 99/1000 | Loss: 0.00003602
Iteration 100/1000 | Loss: 0.00003602
Iteration 101/1000 | Loss: 0.00003602
Iteration 102/1000 | Loss: 0.00003602
Iteration 103/1000 | Loss: 0.00003602
Iteration 104/1000 | Loss: 0.00003601
Iteration 105/1000 | Loss: 0.00003601
Iteration 106/1000 | Loss: 0.00003601
Iteration 107/1000 | Loss: 0.00003601
Iteration 108/1000 | Loss: 0.00003601
Iteration 109/1000 | Loss: 0.00003600
Iteration 110/1000 | Loss: 0.00003600
Iteration 111/1000 | Loss: 0.00003600
Iteration 112/1000 | Loss: 0.00003600
Iteration 113/1000 | Loss: 0.00003600
Iteration 114/1000 | Loss: 0.00003600
Iteration 115/1000 | Loss: 0.00003600
Iteration 116/1000 | Loss: 0.00003600
Iteration 117/1000 | Loss: 0.00003600
Iteration 118/1000 | Loss: 0.00003600
Iteration 119/1000 | Loss: 0.00003600
Iteration 120/1000 | Loss: 0.00003600
Iteration 121/1000 | Loss: 0.00003599
Iteration 122/1000 | Loss: 0.00003599
Iteration 123/1000 | Loss: 0.00003599
Iteration 124/1000 | Loss: 0.00003599
Iteration 125/1000 | Loss: 0.00003599
Iteration 126/1000 | Loss: 0.00003599
Iteration 127/1000 | Loss: 0.00003599
Iteration 128/1000 | Loss: 0.00003599
Iteration 129/1000 | Loss: 0.00003599
Iteration 130/1000 | Loss: 0.00003599
Iteration 131/1000 | Loss: 0.00003598
Iteration 132/1000 | Loss: 0.00003598
Iteration 133/1000 | Loss: 0.00003598
Iteration 134/1000 | Loss: 0.00003598
Iteration 135/1000 | Loss: 0.00003598
Iteration 136/1000 | Loss: 0.00003598
Iteration 137/1000 | Loss: 0.00003598
Iteration 138/1000 | Loss: 0.00003598
Iteration 139/1000 | Loss: 0.00003598
Iteration 140/1000 | Loss: 0.00003597
Iteration 141/1000 | Loss: 0.00003597
Iteration 142/1000 | Loss: 0.00003597
Iteration 143/1000 | Loss: 0.00003596
Iteration 144/1000 | Loss: 0.00003596
Iteration 145/1000 | Loss: 0.00003596
Iteration 146/1000 | Loss: 0.00003596
Iteration 147/1000 | Loss: 0.00003596
Iteration 148/1000 | Loss: 0.00003596
Iteration 149/1000 | Loss: 0.00003596
Iteration 150/1000 | Loss: 0.00003596
Iteration 151/1000 | Loss: 0.00003596
Iteration 152/1000 | Loss: 0.00003596
Iteration 153/1000 | Loss: 0.00003595
Iteration 154/1000 | Loss: 0.00003595
Iteration 155/1000 | Loss: 0.00003595
Iteration 156/1000 | Loss: 0.00003595
Iteration 157/1000 | Loss: 0.00003595
Iteration 158/1000 | Loss: 0.00003594
Iteration 159/1000 | Loss: 0.00003594
Iteration 160/1000 | Loss: 0.00003594
Iteration 161/1000 | Loss: 0.00003594
Iteration 162/1000 | Loss: 0.00003594
Iteration 163/1000 | Loss: 0.00003594
Iteration 164/1000 | Loss: 0.00003594
Iteration 165/1000 | Loss: 0.00003594
Iteration 166/1000 | Loss: 0.00003594
Iteration 167/1000 | Loss: 0.00003594
Iteration 168/1000 | Loss: 0.00003594
Iteration 169/1000 | Loss: 0.00003594
Iteration 170/1000 | Loss: 0.00003594
Iteration 171/1000 | Loss: 0.00003594
Iteration 172/1000 | Loss: 0.00003594
Iteration 173/1000 | Loss: 0.00003594
Iteration 174/1000 | Loss: 0.00003594
Iteration 175/1000 | Loss: 0.00003594
Iteration 176/1000 | Loss: 0.00003593
Iteration 177/1000 | Loss: 0.00003593
Iteration 178/1000 | Loss: 0.00003593
Iteration 179/1000 | Loss: 0.00003593
Iteration 180/1000 | Loss: 0.00003593
Iteration 181/1000 | Loss: 0.00003593
Iteration 182/1000 | Loss: 0.00003593
Iteration 183/1000 | Loss: 0.00003593
Iteration 184/1000 | Loss: 0.00003593
Iteration 185/1000 | Loss: 0.00003593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [3.593360452214256e-05, 3.593360452214256e-05, 3.593360452214256e-05, 3.593360452214256e-05, 3.593360452214256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.593360452214256e-05

Optimization complete. Final v2v error: 4.215340614318848 mm

Highest mean error: 10.579421043395996 mm for frame 123

Lowest mean error: 3.2090208530426025 mm for frame 1

Saving results

Total time: 112.9587128162384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835412
Iteration 2/25 | Loss: 0.00103594
Iteration 3/25 | Loss: 0.00071231
Iteration 4/25 | Loss: 0.00068087
Iteration 5/25 | Loss: 0.00067385
Iteration 6/25 | Loss: 0.00067305
Iteration 7/25 | Loss: 0.00067305
Iteration 8/25 | Loss: 0.00067305
Iteration 9/25 | Loss: 0.00067305
Iteration 10/25 | Loss: 0.00067305
Iteration 11/25 | Loss: 0.00067305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006730543100275099, 0.0006730543100275099, 0.0006730543100275099, 0.0006730543100275099, 0.0006730543100275099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006730543100275099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40736592
Iteration 2/25 | Loss: 0.00026833
Iteration 3/25 | Loss: 0.00026833
Iteration 4/25 | Loss: 0.00026833
Iteration 5/25 | Loss: 0.00026833
Iteration 6/25 | Loss: 0.00026833
Iteration 7/25 | Loss: 0.00026833
Iteration 8/25 | Loss: 0.00026833
Iteration 9/25 | Loss: 0.00026833
Iteration 10/25 | Loss: 0.00026833
Iteration 11/25 | Loss: 0.00026833
Iteration 12/25 | Loss: 0.00026833
Iteration 13/25 | Loss: 0.00026833
Iteration 14/25 | Loss: 0.00026833
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0002683285274542868, 0.0002683285274542868, 0.0002683285274542868, 0.0002683285274542868, 0.0002683285274542868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002683285274542868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026833
Iteration 2/1000 | Loss: 0.00002597
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001699
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001616
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001556
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001555
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001549
Iteration 16/1000 | Loss: 0.00001548
Iteration 17/1000 | Loss: 0.00001547
Iteration 18/1000 | Loss: 0.00001547
Iteration 19/1000 | Loss: 0.00001547
Iteration 20/1000 | Loss: 0.00001546
Iteration 21/1000 | Loss: 0.00001546
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001545
Iteration 25/1000 | Loss: 0.00001545
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001545
Iteration 29/1000 | Loss: 0.00001545
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001544
Iteration 33/1000 | Loss: 0.00001543
Iteration 34/1000 | Loss: 0.00001543
Iteration 35/1000 | Loss: 0.00001543
Iteration 36/1000 | Loss: 0.00001542
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001541
Iteration 39/1000 | Loss: 0.00001541
Iteration 40/1000 | Loss: 0.00001540
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001540
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001540
Iteration 45/1000 | Loss: 0.00001540
Iteration 46/1000 | Loss: 0.00001540
Iteration 47/1000 | Loss: 0.00001540
Iteration 48/1000 | Loss: 0.00001540
Iteration 49/1000 | Loss: 0.00001540
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001538
Iteration 56/1000 | Loss: 0.00001537
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001537
Iteration 59/1000 | Loss: 0.00001537
Iteration 60/1000 | Loss: 0.00001537
Iteration 61/1000 | Loss: 0.00001537
Iteration 62/1000 | Loss: 0.00001537
Iteration 63/1000 | Loss: 0.00001536
Iteration 64/1000 | Loss: 0.00001536
Iteration 65/1000 | Loss: 0.00001535
Iteration 66/1000 | Loss: 0.00001535
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001535
Iteration 69/1000 | Loss: 0.00001535
Iteration 70/1000 | Loss: 0.00001535
Iteration 71/1000 | Loss: 0.00001535
Iteration 72/1000 | Loss: 0.00001535
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001534
Iteration 83/1000 | Loss: 0.00001534
Iteration 84/1000 | Loss: 0.00001534
Iteration 85/1000 | Loss: 0.00001534
Iteration 86/1000 | Loss: 0.00001534
Iteration 87/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.5340958270826377e-05, 1.5340958270826377e-05, 1.5340958270826377e-05, 1.5340958270826377e-05, 1.5340958270826377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5340958270826377e-05

Optimization complete. Final v2v error: 3.288280725479126 mm

Highest mean error: 3.6202738285064697 mm for frame 206

Lowest mean error: 2.964693546295166 mm for frame 89

Saving results

Total time: 29.001885414123535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460003
Iteration 2/25 | Loss: 0.00130188
Iteration 3/25 | Loss: 0.00093566
Iteration 4/25 | Loss: 0.00086866
Iteration 5/25 | Loss: 0.00084642
Iteration 6/25 | Loss: 0.00084230
Iteration 7/25 | Loss: 0.00084091
Iteration 8/25 | Loss: 0.00084064
Iteration 9/25 | Loss: 0.00084064
Iteration 10/25 | Loss: 0.00084064
Iteration 11/25 | Loss: 0.00084064
Iteration 12/25 | Loss: 0.00084064
Iteration 13/25 | Loss: 0.00084064
Iteration 14/25 | Loss: 0.00084064
Iteration 15/25 | Loss: 0.00084064
Iteration 16/25 | Loss: 0.00084064
Iteration 17/25 | Loss: 0.00084064
Iteration 18/25 | Loss: 0.00084064
Iteration 19/25 | Loss: 0.00084064
Iteration 20/25 | Loss: 0.00084064
Iteration 21/25 | Loss: 0.00084064
Iteration 22/25 | Loss: 0.00084064
Iteration 23/25 | Loss: 0.00084064
Iteration 24/25 | Loss: 0.00084064
Iteration 25/25 | Loss: 0.00084064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19695187
Iteration 2/25 | Loss: 0.00022230
Iteration 3/25 | Loss: 0.00022229
Iteration 4/25 | Loss: 0.00022229
Iteration 5/25 | Loss: 0.00022229
Iteration 6/25 | Loss: 0.00022229
Iteration 7/25 | Loss: 0.00022229
Iteration 8/25 | Loss: 0.00022229
Iteration 9/25 | Loss: 0.00022228
Iteration 10/25 | Loss: 0.00022228
Iteration 11/25 | Loss: 0.00022228
Iteration 12/25 | Loss: 0.00022228
Iteration 13/25 | Loss: 0.00022228
Iteration 14/25 | Loss: 0.00022228
Iteration 15/25 | Loss: 0.00022228
Iteration 16/25 | Loss: 0.00022228
Iteration 17/25 | Loss: 0.00022228
Iteration 18/25 | Loss: 0.00022228
Iteration 19/25 | Loss: 0.00022228
Iteration 20/25 | Loss: 0.00022228
Iteration 21/25 | Loss: 0.00022228
Iteration 22/25 | Loss: 0.00022228
Iteration 23/25 | Loss: 0.00022228
Iteration 24/25 | Loss: 0.00022228
Iteration 25/25 | Loss: 0.00022228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022228
Iteration 2/1000 | Loss: 0.00008786
Iteration 3/1000 | Loss: 0.00005682
Iteration 4/1000 | Loss: 0.00004744
Iteration 5/1000 | Loss: 0.00004346
Iteration 6/1000 | Loss: 0.00004210
Iteration 7/1000 | Loss: 0.00004051
Iteration 8/1000 | Loss: 0.00003970
Iteration 9/1000 | Loss: 0.00003887
Iteration 10/1000 | Loss: 0.00003832
Iteration 11/1000 | Loss: 0.00003789
Iteration 12/1000 | Loss: 0.00003746
Iteration 13/1000 | Loss: 0.00003717
Iteration 14/1000 | Loss: 0.00003693
Iteration 15/1000 | Loss: 0.00003675
Iteration 16/1000 | Loss: 0.00003664
Iteration 17/1000 | Loss: 0.00003661
Iteration 18/1000 | Loss: 0.00003656
Iteration 19/1000 | Loss: 0.00003656
Iteration 20/1000 | Loss: 0.00003655
Iteration 21/1000 | Loss: 0.00003655
Iteration 22/1000 | Loss: 0.00003653
Iteration 23/1000 | Loss: 0.00003653
Iteration 24/1000 | Loss: 0.00003652
Iteration 25/1000 | Loss: 0.00003652
Iteration 26/1000 | Loss: 0.00003652
Iteration 27/1000 | Loss: 0.00003651
Iteration 28/1000 | Loss: 0.00003651
Iteration 29/1000 | Loss: 0.00003651
Iteration 30/1000 | Loss: 0.00003651
Iteration 31/1000 | Loss: 0.00003651
Iteration 32/1000 | Loss: 0.00003651
Iteration 33/1000 | Loss: 0.00003651
Iteration 34/1000 | Loss: 0.00003651
Iteration 35/1000 | Loss: 0.00003651
Iteration 36/1000 | Loss: 0.00003650
Iteration 37/1000 | Loss: 0.00003650
Iteration 38/1000 | Loss: 0.00003650
Iteration 39/1000 | Loss: 0.00003648
Iteration 40/1000 | Loss: 0.00003647
Iteration 41/1000 | Loss: 0.00003647
Iteration 42/1000 | Loss: 0.00003646
Iteration 43/1000 | Loss: 0.00003646
Iteration 44/1000 | Loss: 0.00003646
Iteration 45/1000 | Loss: 0.00003646
Iteration 46/1000 | Loss: 0.00003645
Iteration 47/1000 | Loss: 0.00003645
Iteration 48/1000 | Loss: 0.00003645
Iteration 49/1000 | Loss: 0.00003645
Iteration 50/1000 | Loss: 0.00003645
Iteration 51/1000 | Loss: 0.00003645
Iteration 52/1000 | Loss: 0.00003645
Iteration 53/1000 | Loss: 0.00003644
Iteration 54/1000 | Loss: 0.00003644
Iteration 55/1000 | Loss: 0.00003644
Iteration 56/1000 | Loss: 0.00003644
Iteration 57/1000 | Loss: 0.00003643
Iteration 58/1000 | Loss: 0.00003643
Iteration 59/1000 | Loss: 0.00003643
Iteration 60/1000 | Loss: 0.00003643
Iteration 61/1000 | Loss: 0.00003643
Iteration 62/1000 | Loss: 0.00003643
Iteration 63/1000 | Loss: 0.00003643
Iteration 64/1000 | Loss: 0.00003643
Iteration 65/1000 | Loss: 0.00003643
Iteration 66/1000 | Loss: 0.00003643
Iteration 67/1000 | Loss: 0.00003643
Iteration 68/1000 | Loss: 0.00003643
Iteration 69/1000 | Loss: 0.00003643
Iteration 70/1000 | Loss: 0.00003643
Iteration 71/1000 | Loss: 0.00003642
Iteration 72/1000 | Loss: 0.00003642
Iteration 73/1000 | Loss: 0.00003641
Iteration 74/1000 | Loss: 0.00003641
Iteration 75/1000 | Loss: 0.00003641
Iteration 76/1000 | Loss: 0.00003641
Iteration 77/1000 | Loss: 0.00003640
Iteration 78/1000 | Loss: 0.00003640
Iteration 79/1000 | Loss: 0.00003640
Iteration 80/1000 | Loss: 0.00003640
Iteration 81/1000 | Loss: 0.00003640
Iteration 82/1000 | Loss: 0.00003640
Iteration 83/1000 | Loss: 0.00003640
Iteration 84/1000 | Loss: 0.00003640
Iteration 85/1000 | Loss: 0.00003640
Iteration 86/1000 | Loss: 0.00003639
Iteration 87/1000 | Loss: 0.00003639
Iteration 88/1000 | Loss: 0.00003638
Iteration 89/1000 | Loss: 0.00003638
Iteration 90/1000 | Loss: 0.00003638
Iteration 91/1000 | Loss: 0.00003638
Iteration 92/1000 | Loss: 0.00003637
Iteration 93/1000 | Loss: 0.00003637
Iteration 94/1000 | Loss: 0.00003637
Iteration 95/1000 | Loss: 0.00003637
Iteration 96/1000 | Loss: 0.00003637
Iteration 97/1000 | Loss: 0.00003637
Iteration 98/1000 | Loss: 0.00003637
Iteration 99/1000 | Loss: 0.00003637
Iteration 100/1000 | Loss: 0.00003636
Iteration 101/1000 | Loss: 0.00003636
Iteration 102/1000 | Loss: 0.00003636
Iteration 103/1000 | Loss: 0.00003635
Iteration 104/1000 | Loss: 0.00003635
Iteration 105/1000 | Loss: 0.00003635
Iteration 106/1000 | Loss: 0.00003635
Iteration 107/1000 | Loss: 0.00003635
Iteration 108/1000 | Loss: 0.00003635
Iteration 109/1000 | Loss: 0.00003635
Iteration 110/1000 | Loss: 0.00003634
Iteration 111/1000 | Loss: 0.00003634
Iteration 112/1000 | Loss: 0.00003633
Iteration 113/1000 | Loss: 0.00003633
Iteration 114/1000 | Loss: 0.00003633
Iteration 115/1000 | Loss: 0.00003633
Iteration 116/1000 | Loss: 0.00003633
Iteration 117/1000 | Loss: 0.00003633
Iteration 118/1000 | Loss: 0.00003633
Iteration 119/1000 | Loss: 0.00003633
Iteration 120/1000 | Loss: 0.00003632
Iteration 121/1000 | Loss: 0.00003632
Iteration 122/1000 | Loss: 0.00003632
Iteration 123/1000 | Loss: 0.00003632
Iteration 124/1000 | Loss: 0.00003632
Iteration 125/1000 | Loss: 0.00003632
Iteration 126/1000 | Loss: 0.00003632
Iteration 127/1000 | Loss: 0.00003632
Iteration 128/1000 | Loss: 0.00003632
Iteration 129/1000 | Loss: 0.00003632
Iteration 130/1000 | Loss: 0.00003631
Iteration 131/1000 | Loss: 0.00003631
Iteration 132/1000 | Loss: 0.00003631
Iteration 133/1000 | Loss: 0.00003631
Iteration 134/1000 | Loss: 0.00003631
Iteration 135/1000 | Loss: 0.00003630
Iteration 136/1000 | Loss: 0.00003630
Iteration 137/1000 | Loss: 0.00003630
Iteration 138/1000 | Loss: 0.00003630
Iteration 139/1000 | Loss: 0.00003630
Iteration 140/1000 | Loss: 0.00003630
Iteration 141/1000 | Loss: 0.00003630
Iteration 142/1000 | Loss: 0.00003630
Iteration 143/1000 | Loss: 0.00003629
Iteration 144/1000 | Loss: 0.00003629
Iteration 145/1000 | Loss: 0.00003629
Iteration 146/1000 | Loss: 0.00003628
Iteration 147/1000 | Loss: 0.00003628
Iteration 148/1000 | Loss: 0.00003627
Iteration 149/1000 | Loss: 0.00003627
Iteration 150/1000 | Loss: 0.00003626
Iteration 151/1000 | Loss: 0.00003626
Iteration 152/1000 | Loss: 0.00003626
Iteration 153/1000 | Loss: 0.00003626
Iteration 154/1000 | Loss: 0.00003625
Iteration 155/1000 | Loss: 0.00003625
Iteration 156/1000 | Loss: 0.00003625
Iteration 157/1000 | Loss: 0.00003624
Iteration 158/1000 | Loss: 0.00003623
Iteration 159/1000 | Loss: 0.00003623
Iteration 160/1000 | Loss: 0.00003623
Iteration 161/1000 | Loss: 0.00003622
Iteration 162/1000 | Loss: 0.00003622
Iteration 163/1000 | Loss: 0.00003622
Iteration 164/1000 | Loss: 0.00003622
Iteration 165/1000 | Loss: 0.00003622
Iteration 166/1000 | Loss: 0.00003621
Iteration 167/1000 | Loss: 0.00003621
Iteration 168/1000 | Loss: 0.00003621
Iteration 169/1000 | Loss: 0.00003621
Iteration 170/1000 | Loss: 0.00003621
Iteration 171/1000 | Loss: 0.00003621
Iteration 172/1000 | Loss: 0.00003621
Iteration 173/1000 | Loss: 0.00003621
Iteration 174/1000 | Loss: 0.00003621
Iteration 175/1000 | Loss: 0.00003621
Iteration 176/1000 | Loss: 0.00003620
Iteration 177/1000 | Loss: 0.00003620
Iteration 178/1000 | Loss: 0.00003620
Iteration 179/1000 | Loss: 0.00003620
Iteration 180/1000 | Loss: 0.00003620
Iteration 181/1000 | Loss: 0.00003620
Iteration 182/1000 | Loss: 0.00003620
Iteration 183/1000 | Loss: 0.00003619
Iteration 184/1000 | Loss: 0.00003619
Iteration 185/1000 | Loss: 0.00003619
Iteration 186/1000 | Loss: 0.00003619
Iteration 187/1000 | Loss: 0.00003619
Iteration 188/1000 | Loss: 0.00003619
Iteration 189/1000 | Loss: 0.00003618
Iteration 190/1000 | Loss: 0.00003618
Iteration 191/1000 | Loss: 0.00003618
Iteration 192/1000 | Loss: 0.00003617
Iteration 193/1000 | Loss: 0.00003617
Iteration 194/1000 | Loss: 0.00003617
Iteration 195/1000 | Loss: 0.00003617
Iteration 196/1000 | Loss: 0.00003617
Iteration 197/1000 | Loss: 0.00003617
Iteration 198/1000 | Loss: 0.00003616
Iteration 199/1000 | Loss: 0.00003616
Iteration 200/1000 | Loss: 0.00003616
Iteration 201/1000 | Loss: 0.00003615
Iteration 202/1000 | Loss: 0.00003615
Iteration 203/1000 | Loss: 0.00003615
Iteration 204/1000 | Loss: 0.00003615
Iteration 205/1000 | Loss: 0.00003615
Iteration 206/1000 | Loss: 0.00003615
Iteration 207/1000 | Loss: 0.00003615
Iteration 208/1000 | Loss: 0.00003615
Iteration 209/1000 | Loss: 0.00003615
Iteration 210/1000 | Loss: 0.00003614
Iteration 211/1000 | Loss: 0.00003614
Iteration 212/1000 | Loss: 0.00003614
Iteration 213/1000 | Loss: 0.00003614
Iteration 214/1000 | Loss: 0.00003614
Iteration 215/1000 | Loss: 0.00003614
Iteration 216/1000 | Loss: 0.00003614
Iteration 217/1000 | Loss: 0.00003613
Iteration 218/1000 | Loss: 0.00003613
Iteration 219/1000 | Loss: 0.00003613
Iteration 220/1000 | Loss: 0.00003613
Iteration 221/1000 | Loss: 0.00003613
Iteration 222/1000 | Loss: 0.00003613
Iteration 223/1000 | Loss: 0.00003613
Iteration 224/1000 | Loss: 0.00003613
Iteration 225/1000 | Loss: 0.00003613
Iteration 226/1000 | Loss: 0.00003613
Iteration 227/1000 | Loss: 0.00003612
Iteration 228/1000 | Loss: 0.00003612
Iteration 229/1000 | Loss: 0.00003612
Iteration 230/1000 | Loss: 0.00003612
Iteration 231/1000 | Loss: 0.00003612
Iteration 232/1000 | Loss: 0.00003612
Iteration 233/1000 | Loss: 0.00003612
Iteration 234/1000 | Loss: 0.00003611
Iteration 235/1000 | Loss: 0.00003611
Iteration 236/1000 | Loss: 0.00003611
Iteration 237/1000 | Loss: 0.00003611
Iteration 238/1000 | Loss: 0.00003611
Iteration 239/1000 | Loss: 0.00003611
Iteration 240/1000 | Loss: 0.00003611
Iteration 241/1000 | Loss: 0.00003611
Iteration 242/1000 | Loss: 0.00003610
Iteration 243/1000 | Loss: 0.00003610
Iteration 244/1000 | Loss: 0.00003610
Iteration 245/1000 | Loss: 0.00003610
Iteration 246/1000 | Loss: 0.00003610
Iteration 247/1000 | Loss: 0.00003610
Iteration 248/1000 | Loss: 0.00003610
Iteration 249/1000 | Loss: 0.00003610
Iteration 250/1000 | Loss: 0.00003610
Iteration 251/1000 | Loss: 0.00003610
Iteration 252/1000 | Loss: 0.00003610
Iteration 253/1000 | Loss: 0.00003610
Iteration 254/1000 | Loss: 0.00003609
Iteration 255/1000 | Loss: 0.00003609
Iteration 256/1000 | Loss: 0.00003609
Iteration 257/1000 | Loss: 0.00003609
Iteration 258/1000 | Loss: 0.00003609
Iteration 259/1000 | Loss: 0.00003609
Iteration 260/1000 | Loss: 0.00003608
Iteration 261/1000 | Loss: 0.00003608
Iteration 262/1000 | Loss: 0.00003608
Iteration 263/1000 | Loss: 0.00003608
Iteration 264/1000 | Loss: 0.00003608
Iteration 265/1000 | Loss: 0.00003608
Iteration 266/1000 | Loss: 0.00003608
Iteration 267/1000 | Loss: 0.00003608
Iteration 268/1000 | Loss: 0.00003608
Iteration 269/1000 | Loss: 0.00003608
Iteration 270/1000 | Loss: 0.00003608
Iteration 271/1000 | Loss: 0.00003608
Iteration 272/1000 | Loss: 0.00003608
Iteration 273/1000 | Loss: 0.00003608
Iteration 274/1000 | Loss: 0.00003608
Iteration 275/1000 | Loss: 0.00003608
Iteration 276/1000 | Loss: 0.00003608
Iteration 277/1000 | Loss: 0.00003608
Iteration 278/1000 | Loss: 0.00003608
Iteration 279/1000 | Loss: 0.00003608
Iteration 280/1000 | Loss: 0.00003608
Iteration 281/1000 | Loss: 0.00003608
Iteration 282/1000 | Loss: 0.00003608
Iteration 283/1000 | Loss: 0.00003608
Iteration 284/1000 | Loss: 0.00003608
Iteration 285/1000 | Loss: 0.00003608
Iteration 286/1000 | Loss: 0.00003608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [3.6079414712730795e-05, 3.6079414712730795e-05, 3.6079414712730795e-05, 3.6079414712730795e-05, 3.6079414712730795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6079414712730795e-05

Optimization complete. Final v2v error: 4.8587775230407715 mm

Highest mean error: 6.6486687660217285 mm for frame 84

Lowest mean error: 3.7335171699523926 mm for frame 50

Saving results

Total time: 52.382131814956665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941057
Iteration 2/25 | Loss: 0.00086544
Iteration 3/25 | Loss: 0.00072818
Iteration 4/25 | Loss: 0.00070570
Iteration 5/25 | Loss: 0.00069787
Iteration 6/25 | Loss: 0.00069637
Iteration 7/25 | Loss: 0.00069637
Iteration 8/25 | Loss: 0.00069637
Iteration 9/25 | Loss: 0.00069637
Iteration 10/25 | Loss: 0.00069637
Iteration 11/25 | Loss: 0.00069637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006963710184209049, 0.0006963710184209049, 0.0006963710184209049, 0.0006963710184209049, 0.0006963710184209049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006963710184209049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35886669
Iteration 2/25 | Loss: 0.00035454
Iteration 3/25 | Loss: 0.00035454
Iteration 4/25 | Loss: 0.00035454
Iteration 5/25 | Loss: 0.00035454
Iteration 6/25 | Loss: 0.00035454
Iteration 7/25 | Loss: 0.00035454
Iteration 8/25 | Loss: 0.00035454
Iteration 9/25 | Loss: 0.00035454
Iteration 10/25 | Loss: 0.00035454
Iteration 11/25 | Loss: 0.00035454
Iteration 12/25 | Loss: 0.00035454
Iteration 13/25 | Loss: 0.00035454
Iteration 14/25 | Loss: 0.00035454
Iteration 15/25 | Loss: 0.00035454
Iteration 16/25 | Loss: 0.00035454
Iteration 17/25 | Loss: 0.00035454
Iteration 18/25 | Loss: 0.00035454
Iteration 19/25 | Loss: 0.00035454
Iteration 20/25 | Loss: 0.00035454
Iteration 21/25 | Loss: 0.00035454
Iteration 22/25 | Loss: 0.00035454
Iteration 23/25 | Loss: 0.00035454
Iteration 24/25 | Loss: 0.00035454
Iteration 25/25 | Loss: 0.00035454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035454
Iteration 2/1000 | Loss: 0.00003236
Iteration 3/1000 | Loss: 0.00001700
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001449
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001429
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001405
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001404
Iteration 17/1000 | Loss: 0.00001404
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001399
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001397
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001396
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001393
Iteration 32/1000 | Loss: 0.00001392
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001391
Iteration 35/1000 | Loss: 0.00001391
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001389
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001386
Iteration 52/1000 | Loss: 0.00001385
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001385
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001385
Iteration 60/1000 | Loss: 0.00001385
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.3846805813955143e-05, 1.3846805813955143e-05, 1.3846805813955143e-05, 1.3846805813955143e-05, 1.3846805813955143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3846805813955143e-05

Optimization complete. Final v2v error: 3.2514326572418213 mm

Highest mean error: 3.467655897140503 mm for frame 16

Lowest mean error: 2.9792428016662598 mm for frame 83

Saving results

Total time: 26.520820140838623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896524
Iteration 2/25 | Loss: 0.00147452
Iteration 3/25 | Loss: 0.00091336
Iteration 4/25 | Loss: 0.00085921
Iteration 5/25 | Loss: 0.00084582
Iteration 6/25 | Loss: 0.00084269
Iteration 7/25 | Loss: 0.00084145
Iteration 8/25 | Loss: 0.00084133
Iteration 9/25 | Loss: 0.00084133
Iteration 10/25 | Loss: 0.00084133
Iteration 11/25 | Loss: 0.00084133
Iteration 12/25 | Loss: 0.00084133
Iteration 13/25 | Loss: 0.00084133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008413295727223158, 0.0008413295727223158, 0.0008413295727223158, 0.0008413295727223158, 0.0008413295727223158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008413295727223158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55255938
Iteration 2/25 | Loss: 0.00032485
Iteration 3/25 | Loss: 0.00032484
Iteration 4/25 | Loss: 0.00032484
Iteration 5/25 | Loss: 0.00032484
Iteration 6/25 | Loss: 0.00032484
Iteration 7/25 | Loss: 0.00032484
Iteration 8/25 | Loss: 0.00032484
Iteration 9/25 | Loss: 0.00032484
Iteration 10/25 | Loss: 0.00032484
Iteration 11/25 | Loss: 0.00032484
Iteration 12/25 | Loss: 0.00032484
Iteration 13/25 | Loss: 0.00032484
Iteration 14/25 | Loss: 0.00032484
Iteration 15/25 | Loss: 0.00032484
Iteration 16/25 | Loss: 0.00032484
Iteration 17/25 | Loss: 0.00032484
Iteration 18/25 | Loss: 0.00032484
Iteration 19/25 | Loss: 0.00032484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000324840861139819, 0.000324840861139819, 0.000324840861139819, 0.000324840861139819, 0.000324840861139819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000324840861139819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032484
Iteration 2/1000 | Loss: 0.00007278
Iteration 3/1000 | Loss: 0.00004509
Iteration 4/1000 | Loss: 0.00003975
Iteration 5/1000 | Loss: 0.00003824
Iteration 6/1000 | Loss: 0.00003690
Iteration 7/1000 | Loss: 0.00003621
Iteration 8/1000 | Loss: 0.00003577
Iteration 9/1000 | Loss: 0.00003539
Iteration 10/1000 | Loss: 0.00003514
Iteration 11/1000 | Loss: 0.00003494
Iteration 12/1000 | Loss: 0.00003485
Iteration 13/1000 | Loss: 0.00003484
Iteration 14/1000 | Loss: 0.00003474
Iteration 15/1000 | Loss: 0.00003470
Iteration 16/1000 | Loss: 0.00003466
Iteration 17/1000 | Loss: 0.00003463
Iteration 18/1000 | Loss: 0.00003458
Iteration 19/1000 | Loss: 0.00003454
Iteration 20/1000 | Loss: 0.00003454
Iteration 21/1000 | Loss: 0.00003450
Iteration 22/1000 | Loss: 0.00003447
Iteration 23/1000 | Loss: 0.00003439
Iteration 24/1000 | Loss: 0.00003434
Iteration 25/1000 | Loss: 0.00003421
Iteration 26/1000 | Loss: 0.00003418
Iteration 27/1000 | Loss: 0.00003417
Iteration 28/1000 | Loss: 0.00003417
Iteration 29/1000 | Loss: 0.00003416
Iteration 30/1000 | Loss: 0.00003415
Iteration 31/1000 | Loss: 0.00003415
Iteration 32/1000 | Loss: 0.00003414
Iteration 33/1000 | Loss: 0.00003414
Iteration 34/1000 | Loss: 0.00003414
Iteration 35/1000 | Loss: 0.00003413
Iteration 36/1000 | Loss: 0.00003412
Iteration 37/1000 | Loss: 0.00003411
Iteration 38/1000 | Loss: 0.00003411
Iteration 39/1000 | Loss: 0.00003411
Iteration 40/1000 | Loss: 0.00003410
Iteration 41/1000 | Loss: 0.00003410
Iteration 42/1000 | Loss: 0.00003410
Iteration 43/1000 | Loss: 0.00003410
Iteration 44/1000 | Loss: 0.00003405
Iteration 45/1000 | Loss: 0.00003405
Iteration 46/1000 | Loss: 0.00003405
Iteration 47/1000 | Loss: 0.00003403
Iteration 48/1000 | Loss: 0.00003402
Iteration 49/1000 | Loss: 0.00003402
Iteration 50/1000 | Loss: 0.00003402
Iteration 51/1000 | Loss: 0.00003402
Iteration 52/1000 | Loss: 0.00003401
Iteration 53/1000 | Loss: 0.00003401
Iteration 54/1000 | Loss: 0.00003401
Iteration 55/1000 | Loss: 0.00003400
Iteration 56/1000 | Loss: 0.00003399
Iteration 57/1000 | Loss: 0.00003399
Iteration 58/1000 | Loss: 0.00003399
Iteration 59/1000 | Loss: 0.00003399
Iteration 60/1000 | Loss: 0.00003399
Iteration 61/1000 | Loss: 0.00003399
Iteration 62/1000 | Loss: 0.00003399
Iteration 63/1000 | Loss: 0.00003399
Iteration 64/1000 | Loss: 0.00003399
Iteration 65/1000 | Loss: 0.00003399
Iteration 66/1000 | Loss: 0.00003399
Iteration 67/1000 | Loss: 0.00003398
Iteration 68/1000 | Loss: 0.00003398
Iteration 69/1000 | Loss: 0.00003397
Iteration 70/1000 | Loss: 0.00003396
Iteration 71/1000 | Loss: 0.00003395
Iteration 72/1000 | Loss: 0.00003395
Iteration 73/1000 | Loss: 0.00003395
Iteration 74/1000 | Loss: 0.00003395
Iteration 75/1000 | Loss: 0.00003395
Iteration 76/1000 | Loss: 0.00003395
Iteration 77/1000 | Loss: 0.00003395
Iteration 78/1000 | Loss: 0.00003395
Iteration 79/1000 | Loss: 0.00003395
Iteration 80/1000 | Loss: 0.00003395
Iteration 81/1000 | Loss: 0.00003395
Iteration 82/1000 | Loss: 0.00003394
Iteration 83/1000 | Loss: 0.00003392
Iteration 84/1000 | Loss: 0.00003392
Iteration 85/1000 | Loss: 0.00003392
Iteration 86/1000 | Loss: 0.00003392
Iteration 87/1000 | Loss: 0.00003392
Iteration 88/1000 | Loss: 0.00003391
Iteration 89/1000 | Loss: 0.00003391
Iteration 90/1000 | Loss: 0.00003391
Iteration 91/1000 | Loss: 0.00003391
Iteration 92/1000 | Loss: 0.00003391
Iteration 93/1000 | Loss: 0.00003390
Iteration 94/1000 | Loss: 0.00003390
Iteration 95/1000 | Loss: 0.00003390
Iteration 96/1000 | Loss: 0.00003390
Iteration 97/1000 | Loss: 0.00003390
Iteration 98/1000 | Loss: 0.00003389
Iteration 99/1000 | Loss: 0.00003388
Iteration 100/1000 | Loss: 0.00003388
Iteration 101/1000 | Loss: 0.00003387
Iteration 102/1000 | Loss: 0.00003387
Iteration 103/1000 | Loss: 0.00003387
Iteration 104/1000 | Loss: 0.00003387
Iteration 105/1000 | Loss: 0.00003387
Iteration 106/1000 | Loss: 0.00003387
Iteration 107/1000 | Loss: 0.00003387
Iteration 108/1000 | Loss: 0.00003386
Iteration 109/1000 | Loss: 0.00003384
Iteration 110/1000 | Loss: 0.00003383
Iteration 111/1000 | Loss: 0.00003383
Iteration 112/1000 | Loss: 0.00003382
Iteration 113/1000 | Loss: 0.00003382
Iteration 114/1000 | Loss: 0.00003378
Iteration 115/1000 | Loss: 0.00003378
Iteration 116/1000 | Loss: 0.00003378
Iteration 117/1000 | Loss: 0.00003377
Iteration 118/1000 | Loss: 0.00003377
Iteration 119/1000 | Loss: 0.00003377
Iteration 120/1000 | Loss: 0.00003375
Iteration 121/1000 | Loss: 0.00003375
Iteration 122/1000 | Loss: 0.00003375
Iteration 123/1000 | Loss: 0.00003375
Iteration 124/1000 | Loss: 0.00003375
Iteration 125/1000 | Loss: 0.00003375
Iteration 126/1000 | Loss: 0.00003375
Iteration 127/1000 | Loss: 0.00003375
Iteration 128/1000 | Loss: 0.00003375
Iteration 129/1000 | Loss: 0.00003375
Iteration 130/1000 | Loss: 0.00003375
Iteration 131/1000 | Loss: 0.00003375
Iteration 132/1000 | Loss: 0.00003375
Iteration 133/1000 | Loss: 0.00003375
Iteration 134/1000 | Loss: 0.00003375
Iteration 135/1000 | Loss: 0.00003374
Iteration 136/1000 | Loss: 0.00003374
Iteration 137/1000 | Loss: 0.00003374
Iteration 138/1000 | Loss: 0.00003374
Iteration 139/1000 | Loss: 0.00003374
Iteration 140/1000 | Loss: 0.00003374
Iteration 141/1000 | Loss: 0.00003373
Iteration 142/1000 | Loss: 0.00003373
Iteration 143/1000 | Loss: 0.00003373
Iteration 144/1000 | Loss: 0.00003373
Iteration 145/1000 | Loss: 0.00003373
Iteration 146/1000 | Loss: 0.00003372
Iteration 147/1000 | Loss: 0.00003372
Iteration 148/1000 | Loss: 0.00003372
Iteration 149/1000 | Loss: 0.00003372
Iteration 150/1000 | Loss: 0.00003371
Iteration 151/1000 | Loss: 0.00003371
Iteration 152/1000 | Loss: 0.00003371
Iteration 153/1000 | Loss: 0.00003371
Iteration 154/1000 | Loss: 0.00003371
Iteration 155/1000 | Loss: 0.00003371
Iteration 156/1000 | Loss: 0.00003370
Iteration 157/1000 | Loss: 0.00003370
Iteration 158/1000 | Loss: 0.00003370
Iteration 159/1000 | Loss: 0.00003370
Iteration 160/1000 | Loss: 0.00003370
Iteration 161/1000 | Loss: 0.00003370
Iteration 162/1000 | Loss: 0.00003369
Iteration 163/1000 | Loss: 0.00003369
Iteration 164/1000 | Loss: 0.00003369
Iteration 165/1000 | Loss: 0.00003369
Iteration 166/1000 | Loss: 0.00003369
Iteration 167/1000 | Loss: 0.00003369
Iteration 168/1000 | Loss: 0.00003369
Iteration 169/1000 | Loss: 0.00003369
Iteration 170/1000 | Loss: 0.00003369
Iteration 171/1000 | Loss: 0.00003369
Iteration 172/1000 | Loss: 0.00003369
Iteration 173/1000 | Loss: 0.00003368
Iteration 174/1000 | Loss: 0.00003368
Iteration 175/1000 | Loss: 0.00003368
Iteration 176/1000 | Loss: 0.00003368
Iteration 177/1000 | Loss: 0.00003368
Iteration 178/1000 | Loss: 0.00003368
Iteration 179/1000 | Loss: 0.00003368
Iteration 180/1000 | Loss: 0.00003368
Iteration 181/1000 | Loss: 0.00003367
Iteration 182/1000 | Loss: 0.00003367
Iteration 183/1000 | Loss: 0.00003367
Iteration 184/1000 | Loss: 0.00003367
Iteration 185/1000 | Loss: 0.00003367
Iteration 186/1000 | Loss: 0.00003367
Iteration 187/1000 | Loss: 0.00003367
Iteration 188/1000 | Loss: 0.00003367
Iteration 189/1000 | Loss: 0.00003367
Iteration 190/1000 | Loss: 0.00003367
Iteration 191/1000 | Loss: 0.00003367
Iteration 192/1000 | Loss: 0.00003367
Iteration 193/1000 | Loss: 0.00003367
Iteration 194/1000 | Loss: 0.00003367
Iteration 195/1000 | Loss: 0.00003367
Iteration 196/1000 | Loss: 0.00003367
Iteration 197/1000 | Loss: 0.00003367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [3.3666139643173665e-05, 3.3666139643173665e-05, 3.3666139643173665e-05, 3.3666139643173665e-05, 3.3666139643173665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3666139643173665e-05

Optimization complete. Final v2v error: 4.771417617797852 mm

Highest mean error: 6.199212551116943 mm for frame 0

Lowest mean error: 3.839838981628418 mm for frame 14

Saving results

Total time: 50.97396445274353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00650372
Iteration 2/25 | Loss: 0.00084029
Iteration 3/25 | Loss: 0.00071622
Iteration 4/25 | Loss: 0.00068119
Iteration 5/25 | Loss: 0.00066857
Iteration 6/25 | Loss: 0.00066633
Iteration 7/25 | Loss: 0.00066603
Iteration 8/25 | Loss: 0.00066603
Iteration 9/25 | Loss: 0.00066603
Iteration 10/25 | Loss: 0.00066603
Iteration 11/25 | Loss: 0.00066603
Iteration 12/25 | Loss: 0.00066603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006660276558250189, 0.0006660276558250189, 0.0006660276558250189, 0.0006660276558250189, 0.0006660276558250189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006660276558250189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71914983
Iteration 2/25 | Loss: 0.00018366
Iteration 3/25 | Loss: 0.00018366
Iteration 4/25 | Loss: 0.00018366
Iteration 5/25 | Loss: 0.00018366
Iteration 6/25 | Loss: 0.00018366
Iteration 7/25 | Loss: 0.00018366
Iteration 8/25 | Loss: 0.00018366
Iteration 9/25 | Loss: 0.00018366
Iteration 10/25 | Loss: 0.00018366
Iteration 11/25 | Loss: 0.00018366
Iteration 12/25 | Loss: 0.00018366
Iteration 13/25 | Loss: 0.00018366
Iteration 14/25 | Loss: 0.00018366
Iteration 15/25 | Loss: 0.00018366
Iteration 16/25 | Loss: 0.00018366
Iteration 17/25 | Loss: 0.00018366
Iteration 18/25 | Loss: 0.00018366
Iteration 19/25 | Loss: 0.00018366
Iteration 20/25 | Loss: 0.00018366
Iteration 21/25 | Loss: 0.00018366
Iteration 22/25 | Loss: 0.00018366
Iteration 23/25 | Loss: 0.00018366
Iteration 24/25 | Loss: 0.00018366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00018365746655035764, 0.00018365746655035764, 0.00018365746655035764, 0.00018365746655035764, 0.00018365746655035764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018365746655035764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018366
Iteration 2/1000 | Loss: 0.00003929
Iteration 3/1000 | Loss: 0.00002072
Iteration 4/1000 | Loss: 0.00001757
Iteration 5/1000 | Loss: 0.00001634
Iteration 6/1000 | Loss: 0.00001584
Iteration 7/1000 | Loss: 0.00001543
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001485
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001482
Iteration 18/1000 | Loss: 0.00001479
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001472
Iteration 29/1000 | Loss: 0.00001472
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001470
Iteration 32/1000 | Loss: 0.00001469
Iteration 33/1000 | Loss: 0.00001469
Iteration 34/1000 | Loss: 0.00001468
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001465
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00001462
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001459
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001457
Iteration 88/1000 | Loss: 0.00001457
Iteration 89/1000 | Loss: 0.00001457
Iteration 90/1000 | Loss: 0.00001457
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001455
Iteration 100/1000 | Loss: 0.00001455
Iteration 101/1000 | Loss: 0.00001455
Iteration 102/1000 | Loss: 0.00001455
Iteration 103/1000 | Loss: 0.00001454
Iteration 104/1000 | Loss: 0.00001454
Iteration 105/1000 | Loss: 0.00001454
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001454
Iteration 113/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.4535260561387986e-05, 1.4535260561387986e-05, 1.4535260561387986e-05, 1.4535260561387986e-05, 1.4535260561387986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4535260561387986e-05

Optimization complete. Final v2v error: 3.227348804473877 mm

Highest mean error: 3.4832606315612793 mm for frame 92

Lowest mean error: 3.041567087173462 mm for frame 121

Saving results

Total time: 31.63658094406128
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01151302
Iteration 2/25 | Loss: 0.01151302
Iteration 3/25 | Loss: 0.01151301
Iteration 4/25 | Loss: 0.00284233
Iteration 5/25 | Loss: 0.00203787
Iteration 6/25 | Loss: 0.00202617
Iteration 7/25 | Loss: 0.00168103
Iteration 8/25 | Loss: 0.00129557
Iteration 9/25 | Loss: 0.00114428
Iteration 10/25 | Loss: 0.00106598
Iteration 11/25 | Loss: 0.00099693
Iteration 12/25 | Loss: 0.00093660
Iteration 13/25 | Loss: 0.00089064
Iteration 14/25 | Loss: 0.00085249
Iteration 15/25 | Loss: 0.00083432
Iteration 16/25 | Loss: 0.00083050
Iteration 17/25 | Loss: 0.00083436
Iteration 18/25 | Loss: 0.00081900
Iteration 19/25 | Loss: 0.00080757
Iteration 20/25 | Loss: 0.00080482
Iteration 21/25 | Loss: 0.00080142
Iteration 22/25 | Loss: 0.00079892
Iteration 23/25 | Loss: 0.00079975
Iteration 24/25 | Loss: 0.00079850
Iteration 25/25 | Loss: 0.00079662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39530039
Iteration 2/25 | Loss: 0.00040889
Iteration 3/25 | Loss: 0.00040889
Iteration 4/25 | Loss: 0.00040889
Iteration 5/25 | Loss: 0.00040889
Iteration 6/25 | Loss: 0.00040889
Iteration 7/25 | Loss: 0.00040889
Iteration 8/25 | Loss: 0.00040889
Iteration 9/25 | Loss: 0.00040889
Iteration 10/25 | Loss: 0.00040889
Iteration 11/25 | Loss: 0.00040889
Iteration 12/25 | Loss: 0.00040889
Iteration 13/25 | Loss: 0.00040889
Iteration 14/25 | Loss: 0.00040889
Iteration 15/25 | Loss: 0.00040889
Iteration 16/25 | Loss: 0.00040889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004088877758476883, 0.0004088877758476883, 0.0004088877758476883, 0.0004088877758476883, 0.0004088877758476883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004088877758476883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040889
Iteration 2/1000 | Loss: 0.00014926
Iteration 3/1000 | Loss: 0.00014034
Iteration 4/1000 | Loss: 0.00016329
Iteration 5/1000 | Loss: 0.00022217
Iteration 6/1000 | Loss: 0.00007663
Iteration 7/1000 | Loss: 0.00026577
Iteration 8/1000 | Loss: 0.00018353
Iteration 9/1000 | Loss: 0.00013721
Iteration 10/1000 | Loss: 0.00010894
Iteration 11/1000 | Loss: 0.00006036
Iteration 12/1000 | Loss: 0.00003880
Iteration 13/1000 | Loss: 0.00007520
Iteration 14/1000 | Loss: 0.00003209
Iteration 15/1000 | Loss: 0.00023589
Iteration 16/1000 | Loss: 0.00005719
Iteration 17/1000 | Loss: 0.00014092
Iteration 18/1000 | Loss: 0.00005270
Iteration 19/1000 | Loss: 0.00010831
Iteration 20/1000 | Loss: 0.00004439
Iteration 21/1000 | Loss: 0.00011758
Iteration 22/1000 | Loss: 0.00009526
Iteration 23/1000 | Loss: 0.00004601
Iteration 24/1000 | Loss: 0.00004760
Iteration 25/1000 | Loss: 0.00004200
Iteration 26/1000 | Loss: 0.00006566
Iteration 27/1000 | Loss: 0.00010198
Iteration 28/1000 | Loss: 0.00006309
Iteration 29/1000 | Loss: 0.00019141
Iteration 30/1000 | Loss: 0.00005373
Iteration 31/1000 | Loss: 0.00006183
Iteration 32/1000 | Loss: 0.00046980
Iteration 33/1000 | Loss: 0.00017803
Iteration 34/1000 | Loss: 0.00054812
Iteration 35/1000 | Loss: 0.00033359
Iteration 36/1000 | Loss: 0.00020317
Iteration 37/1000 | Loss: 0.00005724
Iteration 38/1000 | Loss: 0.00007106
Iteration 39/1000 | Loss: 0.00004425
Iteration 40/1000 | Loss: 0.00014644
Iteration 41/1000 | Loss: 0.00005617
Iteration 42/1000 | Loss: 0.00004396
Iteration 43/1000 | Loss: 0.00003424
Iteration 44/1000 | Loss: 0.00003013
Iteration 45/1000 | Loss: 0.00004517
Iteration 46/1000 | Loss: 0.00004232
Iteration 47/1000 | Loss: 0.00006190
Iteration 48/1000 | Loss: 0.00005387
Iteration 49/1000 | Loss: 0.00003388
Iteration 50/1000 | Loss: 0.00003586
Iteration 51/1000 | Loss: 0.00003168
Iteration 52/1000 | Loss: 0.00011311
Iteration 53/1000 | Loss: 0.00011941
Iteration 54/1000 | Loss: 0.00010036
Iteration 55/1000 | Loss: 0.00012169
Iteration 56/1000 | Loss: 0.00010639
Iteration 57/1000 | Loss: 0.00009692
Iteration 58/1000 | Loss: 0.00004556
Iteration 59/1000 | Loss: 0.00007970
Iteration 60/1000 | Loss: 0.00005840
Iteration 61/1000 | Loss: 0.00004211
Iteration 62/1000 | Loss: 0.00008845
Iteration 63/1000 | Loss: 0.00005872
Iteration 64/1000 | Loss: 0.00004406
Iteration 65/1000 | Loss: 0.00004093
Iteration 66/1000 | Loss: 0.00003042
Iteration 67/1000 | Loss: 0.00003011
Iteration 68/1000 | Loss: 0.00004019
Iteration 69/1000 | Loss: 0.00011447
Iteration 70/1000 | Loss: 0.00023119
Iteration 71/1000 | Loss: 0.00080303
Iteration 72/1000 | Loss: 0.00025211
Iteration 73/1000 | Loss: 0.00009843
Iteration 74/1000 | Loss: 0.00008942
Iteration 75/1000 | Loss: 0.00011527
Iteration 76/1000 | Loss: 0.00010273
Iteration 77/1000 | Loss: 0.00019228
Iteration 78/1000 | Loss: 0.00014403
Iteration 79/1000 | Loss: 0.00004295
Iteration 80/1000 | Loss: 0.00014914
Iteration 81/1000 | Loss: 0.00014446
Iteration 82/1000 | Loss: 0.00009490
Iteration 83/1000 | Loss: 0.00011232
Iteration 84/1000 | Loss: 0.00007738
Iteration 85/1000 | Loss: 0.00009696
Iteration 86/1000 | Loss: 0.00008114
Iteration 87/1000 | Loss: 0.00008838
Iteration 88/1000 | Loss: 0.00007030
Iteration 89/1000 | Loss: 0.00011271
Iteration 90/1000 | Loss: 0.00011691
Iteration 91/1000 | Loss: 0.00009416
Iteration 92/1000 | Loss: 0.00007946
Iteration 93/1000 | Loss: 0.00010487
Iteration 94/1000 | Loss: 0.00007025
Iteration 95/1000 | Loss: 0.00009644
Iteration 96/1000 | Loss: 0.00009055
Iteration 97/1000 | Loss: 0.00008818
Iteration 98/1000 | Loss: 0.00007246
Iteration 99/1000 | Loss: 0.00012261
Iteration 100/1000 | Loss: 0.00003732
Iteration 101/1000 | Loss: 0.00014041
Iteration 102/1000 | Loss: 0.00016058
Iteration 103/1000 | Loss: 0.00003505
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002414
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002174
Iteration 108/1000 | Loss: 0.00002132
Iteration 109/1000 | Loss: 0.00002103
Iteration 110/1000 | Loss: 0.00002097
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002081
Iteration 113/1000 | Loss: 0.00002066
Iteration 114/1000 | Loss: 0.00002065
Iteration 115/1000 | Loss: 0.00002065
Iteration 116/1000 | Loss: 0.00002064
Iteration 117/1000 | Loss: 0.00002064
Iteration 118/1000 | Loss: 0.00002064
Iteration 119/1000 | Loss: 0.00002063
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002051
Iteration 122/1000 | Loss: 0.00002048
Iteration 123/1000 | Loss: 0.00002048
Iteration 124/1000 | Loss: 0.00002047
Iteration 125/1000 | Loss: 0.00002047
Iteration 126/1000 | Loss: 0.00002047
Iteration 127/1000 | Loss: 0.00002047
Iteration 128/1000 | Loss: 0.00002046
Iteration 129/1000 | Loss: 0.00002046
Iteration 130/1000 | Loss: 0.00002045
Iteration 131/1000 | Loss: 0.00002045
Iteration 132/1000 | Loss: 0.00002044
Iteration 133/1000 | Loss: 0.00002044
Iteration 134/1000 | Loss: 0.00002044
Iteration 135/1000 | Loss: 0.00002043
Iteration 136/1000 | Loss: 0.00002043
Iteration 137/1000 | Loss: 0.00002043
Iteration 138/1000 | Loss: 0.00002043
Iteration 139/1000 | Loss: 0.00002043
Iteration 140/1000 | Loss: 0.00002043
Iteration 141/1000 | Loss: 0.00002043
Iteration 142/1000 | Loss: 0.00002043
Iteration 143/1000 | Loss: 0.00002043
Iteration 144/1000 | Loss: 0.00002043
Iteration 145/1000 | Loss: 0.00002042
Iteration 146/1000 | Loss: 0.00002042
Iteration 147/1000 | Loss: 0.00002042
Iteration 148/1000 | Loss: 0.00002042
Iteration 149/1000 | Loss: 0.00002042
Iteration 150/1000 | Loss: 0.00002042
Iteration 151/1000 | Loss: 0.00002041
Iteration 152/1000 | Loss: 0.00002041
Iteration 153/1000 | Loss: 0.00002041
Iteration 154/1000 | Loss: 0.00002041
Iteration 155/1000 | Loss: 0.00002041
Iteration 156/1000 | Loss: 0.00002041
Iteration 157/1000 | Loss: 0.00002041
Iteration 158/1000 | Loss: 0.00002041
Iteration 159/1000 | Loss: 0.00002040
Iteration 160/1000 | Loss: 0.00002040
Iteration 161/1000 | Loss: 0.00002040
Iteration 162/1000 | Loss: 0.00002040
Iteration 163/1000 | Loss: 0.00002040
Iteration 164/1000 | Loss: 0.00002039
Iteration 165/1000 | Loss: 0.00002039
Iteration 166/1000 | Loss: 0.00002039
Iteration 167/1000 | Loss: 0.00002038
Iteration 168/1000 | Loss: 0.00002038
Iteration 169/1000 | Loss: 0.00002038
Iteration 170/1000 | Loss: 0.00002037
Iteration 171/1000 | Loss: 0.00002037
Iteration 172/1000 | Loss: 0.00002037
Iteration 173/1000 | Loss: 0.00002037
Iteration 174/1000 | Loss: 0.00002037
Iteration 175/1000 | Loss: 0.00002036
Iteration 176/1000 | Loss: 0.00002036
Iteration 177/1000 | Loss: 0.00002036
Iteration 178/1000 | Loss: 0.00002036
Iteration 179/1000 | Loss: 0.00002036
Iteration 180/1000 | Loss: 0.00002036
Iteration 181/1000 | Loss: 0.00002036
Iteration 182/1000 | Loss: 0.00002035
Iteration 183/1000 | Loss: 0.00002035
Iteration 184/1000 | Loss: 0.00002035
Iteration 185/1000 | Loss: 0.00002035
Iteration 186/1000 | Loss: 0.00002035
Iteration 187/1000 | Loss: 0.00002035
Iteration 188/1000 | Loss: 0.00002035
Iteration 189/1000 | Loss: 0.00002035
Iteration 190/1000 | Loss: 0.00002035
Iteration 191/1000 | Loss: 0.00002035
Iteration 192/1000 | Loss: 0.00002035
Iteration 193/1000 | Loss: 0.00002035
Iteration 194/1000 | Loss: 0.00002035
Iteration 195/1000 | Loss: 0.00002035
Iteration 196/1000 | Loss: 0.00002035
Iteration 197/1000 | Loss: 0.00002035
Iteration 198/1000 | Loss: 0.00002035
Iteration 199/1000 | Loss: 0.00002035
Iteration 200/1000 | Loss: 0.00002035
Iteration 201/1000 | Loss: 0.00002035
Iteration 202/1000 | Loss: 0.00002035
Iteration 203/1000 | Loss: 0.00002035
Iteration 204/1000 | Loss: 0.00002035
Iteration 205/1000 | Loss: 0.00002035
Iteration 206/1000 | Loss: 0.00002035
Iteration 207/1000 | Loss: 0.00002035
Iteration 208/1000 | Loss: 0.00002035
Iteration 209/1000 | Loss: 0.00002035
Iteration 210/1000 | Loss: 0.00002035
Iteration 211/1000 | Loss: 0.00002035
Iteration 212/1000 | Loss: 0.00002035
Iteration 213/1000 | Loss: 0.00002035
Iteration 214/1000 | Loss: 0.00002035
Iteration 215/1000 | Loss: 0.00002035
Iteration 216/1000 | Loss: 0.00002035
Iteration 217/1000 | Loss: 0.00002035
Iteration 218/1000 | Loss: 0.00002035
Iteration 219/1000 | Loss: 0.00002035
Iteration 220/1000 | Loss: 0.00002035
Iteration 221/1000 | Loss: 0.00002035
Iteration 222/1000 | Loss: 0.00002035
Iteration 223/1000 | Loss: 0.00002035
Iteration 224/1000 | Loss: 0.00002035
Iteration 225/1000 | Loss: 0.00002035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.0349843907752074e-05, 2.0349843907752074e-05, 2.0349843907752074e-05, 2.0349843907752074e-05, 2.0349843907752074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0349843907752074e-05

Optimization complete. Final v2v error: 3.7403578758239746 mm

Highest mean error: 4.825479030609131 mm for frame 53

Lowest mean error: 3.1617050170898438 mm for frame 192

Saving results

Total time: 232.23272395133972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801301
Iteration 2/25 | Loss: 0.00174026
Iteration 3/25 | Loss: 0.00105659
Iteration 4/25 | Loss: 0.00087936
Iteration 5/25 | Loss: 0.00085570
Iteration 6/25 | Loss: 0.00085368
Iteration 7/25 | Loss: 0.00085364
Iteration 8/25 | Loss: 0.00085364
Iteration 9/25 | Loss: 0.00085364
Iteration 10/25 | Loss: 0.00085364
Iteration 11/25 | Loss: 0.00085364
Iteration 12/25 | Loss: 0.00085364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008536378154531121, 0.0008536378154531121, 0.0008536378154531121, 0.0008536378154531121, 0.0008536378154531121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008536378154531121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46115744
Iteration 2/25 | Loss: 0.00029790
Iteration 3/25 | Loss: 0.00029788
Iteration 4/25 | Loss: 0.00029788
Iteration 5/25 | Loss: 0.00029788
Iteration 6/25 | Loss: 0.00029788
Iteration 7/25 | Loss: 0.00029788
Iteration 8/25 | Loss: 0.00029788
Iteration 9/25 | Loss: 0.00029788
Iteration 10/25 | Loss: 0.00029788
Iteration 11/25 | Loss: 0.00029788
Iteration 12/25 | Loss: 0.00029788
Iteration 13/25 | Loss: 0.00029788
Iteration 14/25 | Loss: 0.00029788
Iteration 15/25 | Loss: 0.00029788
Iteration 16/25 | Loss: 0.00029788
Iteration 17/25 | Loss: 0.00029788
Iteration 18/25 | Loss: 0.00029788
Iteration 19/25 | Loss: 0.00029788
Iteration 20/25 | Loss: 0.00029788
Iteration 21/25 | Loss: 0.00029788
Iteration 22/25 | Loss: 0.00029788
Iteration 23/25 | Loss: 0.00029788
Iteration 24/25 | Loss: 0.00029788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002978752600029111, 0.0002978752600029111, 0.0002978752600029111, 0.0002978752600029111, 0.0002978752600029111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002978752600029111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029788
Iteration 2/1000 | Loss: 0.00004001
Iteration 3/1000 | Loss: 0.00003003
Iteration 4/1000 | Loss: 0.00002782
Iteration 5/1000 | Loss: 0.00002703
Iteration 6/1000 | Loss: 0.00002630
Iteration 7/1000 | Loss: 0.00002590
Iteration 8/1000 | Loss: 0.00002559
Iteration 9/1000 | Loss: 0.00002525
Iteration 10/1000 | Loss: 0.00002503
Iteration 11/1000 | Loss: 0.00002498
Iteration 12/1000 | Loss: 0.00002498
Iteration 13/1000 | Loss: 0.00002496
Iteration 14/1000 | Loss: 0.00002486
Iteration 15/1000 | Loss: 0.00002477
Iteration 16/1000 | Loss: 0.00002477
Iteration 17/1000 | Loss: 0.00002476
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002472
Iteration 21/1000 | Loss: 0.00002466
Iteration 22/1000 | Loss: 0.00002465
Iteration 23/1000 | Loss: 0.00002465
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002463
Iteration 26/1000 | Loss: 0.00002462
Iteration 27/1000 | Loss: 0.00002462
Iteration 28/1000 | Loss: 0.00002462
Iteration 29/1000 | Loss: 0.00002461
Iteration 30/1000 | Loss: 0.00002461
Iteration 31/1000 | Loss: 0.00002461
Iteration 32/1000 | Loss: 0.00002460
Iteration 33/1000 | Loss: 0.00002460
Iteration 34/1000 | Loss: 0.00002459
Iteration 35/1000 | Loss: 0.00002459
Iteration 36/1000 | Loss: 0.00002459
Iteration 37/1000 | Loss: 0.00002459
Iteration 38/1000 | Loss: 0.00002459
Iteration 39/1000 | Loss: 0.00002459
Iteration 40/1000 | Loss: 0.00002459
Iteration 41/1000 | Loss: 0.00002459
Iteration 42/1000 | Loss: 0.00002459
Iteration 43/1000 | Loss: 0.00002459
Iteration 44/1000 | Loss: 0.00002459
Iteration 45/1000 | Loss: 0.00002459
Iteration 46/1000 | Loss: 0.00002459
Iteration 47/1000 | Loss: 0.00002459
Iteration 48/1000 | Loss: 0.00002459
Iteration 49/1000 | Loss: 0.00002458
Iteration 50/1000 | Loss: 0.00002458
Iteration 51/1000 | Loss: 0.00002458
Iteration 52/1000 | Loss: 0.00002458
Iteration 53/1000 | Loss: 0.00002458
Iteration 54/1000 | Loss: 0.00002458
Iteration 55/1000 | Loss: 0.00002458
Iteration 56/1000 | Loss: 0.00002458
Iteration 57/1000 | Loss: 0.00002458
Iteration 58/1000 | Loss: 0.00002458
Iteration 59/1000 | Loss: 0.00002458
Iteration 60/1000 | Loss: 0.00002458
Iteration 61/1000 | Loss: 0.00002458
Iteration 62/1000 | Loss: 0.00002458
Iteration 63/1000 | Loss: 0.00002458
Iteration 64/1000 | Loss: 0.00002458
Iteration 65/1000 | Loss: 0.00002458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.4577073418186046e-05, 2.4577073418186046e-05, 2.4577073418186046e-05, 2.4577073418186046e-05, 2.4577073418186046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4577073418186046e-05

Optimization complete. Final v2v error: 4.263741493225098 mm

Highest mean error: 4.605655193328857 mm for frame 205

Lowest mean error: 3.984300374984741 mm for frame 153

Saving results

Total time: 34.51061773300171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084537
Iteration 2/25 | Loss: 0.00410606
Iteration 3/25 | Loss: 0.00296523
Iteration 4/25 | Loss: 0.00212612
Iteration 5/25 | Loss: 0.00203670
Iteration 6/25 | Loss: 0.00137113
Iteration 7/25 | Loss: 0.00117624
Iteration 8/25 | Loss: 0.00110991
Iteration 9/25 | Loss: 0.00108955
Iteration 10/25 | Loss: 0.00103279
Iteration 11/25 | Loss: 0.00100049
Iteration 12/25 | Loss: 0.00099440
Iteration 13/25 | Loss: 0.00097620
Iteration 14/25 | Loss: 0.00097306
Iteration 15/25 | Loss: 0.00097207
Iteration 16/25 | Loss: 0.00097171
Iteration 17/25 | Loss: 0.00097152
Iteration 18/25 | Loss: 0.00097141
Iteration 19/25 | Loss: 0.00097133
Iteration 20/25 | Loss: 0.00097125
Iteration 21/25 | Loss: 0.00097103
Iteration 22/25 | Loss: 0.00097007
Iteration 23/25 | Loss: 0.00096868
Iteration 24/25 | Loss: 0.00096822
Iteration 25/25 | Loss: 0.00096804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40839624
Iteration 2/25 | Loss: 0.00222805
Iteration 3/25 | Loss: 0.00222805
Iteration 4/25 | Loss: 0.00222805
Iteration 5/25 | Loss: 0.00222805
Iteration 6/25 | Loss: 0.00222805
Iteration 7/25 | Loss: 0.00222805
Iteration 8/25 | Loss: 0.00222805
Iteration 9/25 | Loss: 0.00222805
Iteration 10/25 | Loss: 0.00222804
Iteration 11/25 | Loss: 0.00222804
Iteration 12/25 | Loss: 0.00222804
Iteration 13/25 | Loss: 0.00222804
Iteration 14/25 | Loss: 0.00222804
Iteration 15/25 | Loss: 0.00222804
Iteration 16/25 | Loss: 0.00222804
Iteration 17/25 | Loss: 0.00222804
Iteration 18/25 | Loss: 0.00222804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0022280446719378233, 0.0022280446719378233, 0.0022280446719378233, 0.0022280446719378233, 0.0022280446719378233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022280446719378233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222804
Iteration 2/1000 | Loss: 0.00043195
Iteration 3/1000 | Loss: 0.00027855
Iteration 4/1000 | Loss: 0.00022518
Iteration 5/1000 | Loss: 0.00274560
Iteration 6/1000 | Loss: 0.00017842
Iteration 7/1000 | Loss: 0.00178790
Iteration 8/1000 | Loss: 0.00015719
Iteration 9/1000 | Loss: 0.00014954
Iteration 10/1000 | Loss: 0.00958574
Iteration 11/1000 | Loss: 0.00021700
Iteration 12/1000 | Loss: 0.00010627
Iteration 13/1000 | Loss: 0.00007075
Iteration 14/1000 | Loss: 0.00005293
Iteration 15/1000 | Loss: 0.00003898
Iteration 16/1000 | Loss: 0.00003285
Iteration 17/1000 | Loss: 0.00002802
Iteration 18/1000 | Loss: 0.00002566
Iteration 19/1000 | Loss: 0.00002374
Iteration 20/1000 | Loss: 0.00002245
Iteration 21/1000 | Loss: 0.00002151
Iteration 22/1000 | Loss: 0.00002084
Iteration 23/1000 | Loss: 0.00002030
Iteration 24/1000 | Loss: 0.00001984
Iteration 25/1000 | Loss: 0.00001965
Iteration 26/1000 | Loss: 0.00001955
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001949
Iteration 29/1000 | Loss: 0.00001949
Iteration 30/1000 | Loss: 0.00001949
Iteration 31/1000 | Loss: 0.00001949
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001948
Iteration 34/1000 | Loss: 0.00001948
Iteration 35/1000 | Loss: 0.00001948
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001945
Iteration 47/1000 | Loss: 0.00001945
Iteration 48/1000 | Loss: 0.00001945
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001944
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001944
Iteration 58/1000 | Loss: 0.00001944
Iteration 59/1000 | Loss: 0.00001944
Iteration 60/1000 | Loss: 0.00001943
Iteration 61/1000 | Loss: 0.00001942
Iteration 62/1000 | Loss: 0.00001942
Iteration 63/1000 | Loss: 0.00001941
Iteration 64/1000 | Loss: 0.00001941
Iteration 65/1000 | Loss: 0.00001941
Iteration 66/1000 | Loss: 0.00001941
Iteration 67/1000 | Loss: 0.00001940
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001939
Iteration 76/1000 | Loss: 0.00001939
Iteration 77/1000 | Loss: 0.00001939
Iteration 78/1000 | Loss: 0.00001938
Iteration 79/1000 | Loss: 0.00001938
Iteration 80/1000 | Loss: 0.00001938
Iteration 81/1000 | Loss: 0.00001938
Iteration 82/1000 | Loss: 0.00001938
Iteration 83/1000 | Loss: 0.00001938
Iteration 84/1000 | Loss: 0.00001938
Iteration 85/1000 | Loss: 0.00001938
Iteration 86/1000 | Loss: 0.00001938
Iteration 87/1000 | Loss: 0.00001938
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001936
Iteration 96/1000 | Loss: 0.00001936
Iteration 97/1000 | Loss: 0.00001936
Iteration 98/1000 | Loss: 0.00001936
Iteration 99/1000 | Loss: 0.00001936
Iteration 100/1000 | Loss: 0.00001936
Iteration 101/1000 | Loss: 0.00001936
Iteration 102/1000 | Loss: 0.00001936
Iteration 103/1000 | Loss: 0.00001936
Iteration 104/1000 | Loss: 0.00001936
Iteration 105/1000 | Loss: 0.00001936
Iteration 106/1000 | Loss: 0.00001936
Iteration 107/1000 | Loss: 0.00001936
Iteration 108/1000 | Loss: 0.00001935
Iteration 109/1000 | Loss: 0.00001935
Iteration 110/1000 | Loss: 0.00001935
Iteration 111/1000 | Loss: 0.00001935
Iteration 112/1000 | Loss: 0.00001935
Iteration 113/1000 | Loss: 0.00001935
Iteration 114/1000 | Loss: 0.00001935
Iteration 115/1000 | Loss: 0.00001934
Iteration 116/1000 | Loss: 0.00001934
Iteration 117/1000 | Loss: 0.00001934
Iteration 118/1000 | Loss: 0.00001934
Iteration 119/1000 | Loss: 0.00001934
Iteration 120/1000 | Loss: 0.00001934
Iteration 121/1000 | Loss: 0.00001934
Iteration 122/1000 | Loss: 0.00001934
Iteration 123/1000 | Loss: 0.00001934
Iteration 124/1000 | Loss: 0.00001934
Iteration 125/1000 | Loss: 0.00001934
Iteration 126/1000 | Loss: 0.00001933
Iteration 127/1000 | Loss: 0.00001933
Iteration 128/1000 | Loss: 0.00001933
Iteration 129/1000 | Loss: 0.00001933
Iteration 130/1000 | Loss: 0.00001933
Iteration 131/1000 | Loss: 0.00001933
Iteration 132/1000 | Loss: 0.00001933
Iteration 133/1000 | Loss: 0.00001933
Iteration 134/1000 | Loss: 0.00001933
Iteration 135/1000 | Loss: 0.00001933
Iteration 136/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.9331493604113348e-05, 1.9331493604113348e-05, 1.9331493604113348e-05, 1.9331493604113348e-05, 1.9331493604113348e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9331493604113348e-05

Optimization complete. Final v2v error: 3.7179903984069824 mm

Highest mean error: 8.889556884765625 mm for frame 6

Lowest mean error: 3.3359858989715576 mm for frame 37

Saving results

Total time: 85.76106476783752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081285
Iteration 2/25 | Loss: 0.00377368
Iteration 3/25 | Loss: 0.00215332
Iteration 4/25 | Loss: 0.00181361
Iteration 5/25 | Loss: 0.00173592
Iteration 6/25 | Loss: 0.00159902
Iteration 7/25 | Loss: 0.00138546
Iteration 8/25 | Loss: 0.00126811
Iteration 9/25 | Loss: 0.00119104
Iteration 10/25 | Loss: 0.00115449
Iteration 11/25 | Loss: 0.00112599
Iteration 12/25 | Loss: 0.00109839
Iteration 13/25 | Loss: 0.00107803
Iteration 14/25 | Loss: 0.00107134
Iteration 15/25 | Loss: 0.00106578
Iteration 16/25 | Loss: 0.00105751
Iteration 17/25 | Loss: 0.00106138
Iteration 18/25 | Loss: 0.00106262
Iteration 19/25 | Loss: 0.00106317
Iteration 20/25 | Loss: 0.00104962
Iteration 21/25 | Loss: 0.00104768
Iteration 22/25 | Loss: 0.00104494
Iteration 23/25 | Loss: 0.00104408
Iteration 24/25 | Loss: 0.00104344
Iteration 25/25 | Loss: 0.00104314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41533709
Iteration 2/25 | Loss: 0.00366891
Iteration 3/25 | Loss: 0.00366890
Iteration 4/25 | Loss: 0.00329236
Iteration 5/25 | Loss: 0.00329236
Iteration 6/25 | Loss: 0.00329235
Iteration 7/25 | Loss: 0.00329235
Iteration 8/25 | Loss: 0.00329235
Iteration 9/25 | Loss: 0.00329235
Iteration 10/25 | Loss: 0.00329235
Iteration 11/25 | Loss: 0.00329235
Iteration 12/25 | Loss: 0.00329235
Iteration 13/25 | Loss: 0.00329235
Iteration 14/25 | Loss: 0.00329235
Iteration 15/25 | Loss: 0.00329235
Iteration 16/25 | Loss: 0.00329235
Iteration 17/25 | Loss: 0.00329235
Iteration 18/25 | Loss: 0.00329235
Iteration 19/25 | Loss: 0.00329235
Iteration 20/25 | Loss: 0.00329235
Iteration 21/25 | Loss: 0.00329235
Iteration 22/25 | Loss: 0.00329235
Iteration 23/25 | Loss: 0.00329235
Iteration 24/25 | Loss: 0.00329235
Iteration 25/25 | Loss: 0.00329235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329235
Iteration 2/1000 | Loss: 0.00088124
Iteration 3/1000 | Loss: 0.00105787
Iteration 4/1000 | Loss: 0.00589171
Iteration 5/1000 | Loss: 0.00067682
Iteration 6/1000 | Loss: 0.00262804
Iteration 7/1000 | Loss: 0.00103874
Iteration 8/1000 | Loss: 0.00051522
Iteration 9/1000 | Loss: 0.00049864
Iteration 10/1000 | Loss: 0.00025093
Iteration 11/1000 | Loss: 0.00061304
Iteration 12/1000 | Loss: 0.00153558
Iteration 13/1000 | Loss: 0.01295462
Iteration 14/1000 | Loss: 0.01119343
Iteration 15/1000 | Loss: 0.00609750
Iteration 16/1000 | Loss: 0.00697496
Iteration 17/1000 | Loss: 0.00493716
Iteration 18/1000 | Loss: 0.00452168
Iteration 19/1000 | Loss: 0.00563845
Iteration 20/1000 | Loss: 0.00355488
Iteration 21/1000 | Loss: 0.00101586
Iteration 22/1000 | Loss: 0.00237024
Iteration 23/1000 | Loss: 0.00246773
Iteration 24/1000 | Loss: 0.00210891
Iteration 25/1000 | Loss: 0.00612060
Iteration 26/1000 | Loss: 0.00376723
Iteration 27/1000 | Loss: 0.00280854
Iteration 28/1000 | Loss: 0.00448526
Iteration 29/1000 | Loss: 0.00373225
Iteration 30/1000 | Loss: 0.00178447
Iteration 31/1000 | Loss: 0.00413403
Iteration 32/1000 | Loss: 0.00383062
Iteration 33/1000 | Loss: 0.00343363
Iteration 34/1000 | Loss: 0.00323071
Iteration 35/1000 | Loss: 0.00473298
Iteration 36/1000 | Loss: 0.00512438
Iteration 37/1000 | Loss: 0.00268945
Iteration 38/1000 | Loss: 0.00278615
Iteration 39/1000 | Loss: 0.00072262
Iteration 40/1000 | Loss: 0.00114305
Iteration 41/1000 | Loss: 0.00113363
Iteration 42/1000 | Loss: 0.00136078
Iteration 43/1000 | Loss: 0.00108973
Iteration 44/1000 | Loss: 0.00097335
Iteration 45/1000 | Loss: 0.00119682
Iteration 46/1000 | Loss: 0.00118928
Iteration 47/1000 | Loss: 0.00116646
Iteration 48/1000 | Loss: 0.00205856
Iteration 49/1000 | Loss: 0.00226815
Iteration 50/1000 | Loss: 0.00241511
Iteration 51/1000 | Loss: 0.00130390
Iteration 52/1000 | Loss: 0.00061544
Iteration 53/1000 | Loss: 0.00067540
Iteration 54/1000 | Loss: 0.00070611
Iteration 55/1000 | Loss: 0.00014062
Iteration 56/1000 | Loss: 0.00016004
Iteration 57/1000 | Loss: 0.00018135
Iteration 58/1000 | Loss: 0.00032995
Iteration 59/1000 | Loss: 0.00022981
Iteration 60/1000 | Loss: 0.00015420
Iteration 61/1000 | Loss: 0.00030776
Iteration 62/1000 | Loss: 0.00024829
Iteration 63/1000 | Loss: 0.00015907
Iteration 64/1000 | Loss: 0.00017862
Iteration 65/1000 | Loss: 0.00024383
Iteration 66/1000 | Loss: 0.00059867
Iteration 67/1000 | Loss: 0.00044211
Iteration 68/1000 | Loss: 0.00024611
Iteration 69/1000 | Loss: 0.00045770
Iteration 70/1000 | Loss: 0.00036731
Iteration 71/1000 | Loss: 0.00050611
Iteration 72/1000 | Loss: 0.00055697
Iteration 73/1000 | Loss: 0.00040468
Iteration 74/1000 | Loss: 0.00048426
Iteration 75/1000 | Loss: 0.00128289
Iteration 76/1000 | Loss: 0.00058869
Iteration 77/1000 | Loss: 0.00063517
Iteration 78/1000 | Loss: 0.00051609
Iteration 79/1000 | Loss: 0.00104974
Iteration 80/1000 | Loss: 0.00047461
Iteration 81/1000 | Loss: 0.00009965
Iteration 82/1000 | Loss: 0.00010416
Iteration 83/1000 | Loss: 0.00026081
Iteration 84/1000 | Loss: 0.00008271
Iteration 85/1000 | Loss: 0.00010841
Iteration 86/1000 | Loss: 0.00011346
Iteration 87/1000 | Loss: 0.00009222
Iteration 88/1000 | Loss: 0.00005220
Iteration 89/1000 | Loss: 0.00015417
Iteration 90/1000 | Loss: 0.00037623
Iteration 91/1000 | Loss: 0.00028105
Iteration 92/1000 | Loss: 0.00004407
Iteration 93/1000 | Loss: 0.00032461
Iteration 94/1000 | Loss: 0.00055456
Iteration 95/1000 | Loss: 0.00112022
Iteration 96/1000 | Loss: 0.00080384
Iteration 97/1000 | Loss: 0.00076913
Iteration 98/1000 | Loss: 0.00066835
Iteration 99/1000 | Loss: 0.00056361
Iteration 100/1000 | Loss: 0.00007321
Iteration 101/1000 | Loss: 0.00004949
Iteration 102/1000 | Loss: 0.00003939
Iteration 103/1000 | Loss: 0.00003618
Iteration 104/1000 | Loss: 0.00003416
Iteration 105/1000 | Loss: 0.00067861
Iteration 106/1000 | Loss: 0.00012879
Iteration 107/1000 | Loss: 0.00010030
Iteration 108/1000 | Loss: 0.00003965
Iteration 109/1000 | Loss: 0.00056799
Iteration 110/1000 | Loss: 0.00042623
Iteration 111/1000 | Loss: 0.00056200
Iteration 112/1000 | Loss: 0.00004611
Iteration 113/1000 | Loss: 0.00003927
Iteration 114/1000 | Loss: 0.00003181
Iteration 115/1000 | Loss: 0.00003913
Iteration 116/1000 | Loss: 0.00014215
Iteration 117/1000 | Loss: 0.00011913
Iteration 118/1000 | Loss: 0.00003011
Iteration 119/1000 | Loss: 0.00015892
Iteration 120/1000 | Loss: 0.00012749
Iteration 121/1000 | Loss: 0.00014883
Iteration 122/1000 | Loss: 0.00014607
Iteration 123/1000 | Loss: 0.00003523
Iteration 124/1000 | Loss: 0.00003230
Iteration 125/1000 | Loss: 0.00003038
Iteration 126/1000 | Loss: 0.00002900
Iteration 127/1000 | Loss: 0.00003017
Iteration 128/1000 | Loss: 0.00054021
Iteration 129/1000 | Loss: 0.00024271
Iteration 130/1000 | Loss: 0.00004617
Iteration 131/1000 | Loss: 0.00005115
Iteration 132/1000 | Loss: 0.00004377
Iteration 133/1000 | Loss: 0.00041150
Iteration 134/1000 | Loss: 0.00004691
Iteration 135/1000 | Loss: 0.00003296
Iteration 136/1000 | Loss: 0.00067003
Iteration 137/1000 | Loss: 0.00008838
Iteration 138/1000 | Loss: 0.00002772
Iteration 139/1000 | Loss: 0.00010830
Iteration 140/1000 | Loss: 0.00012042
Iteration 141/1000 | Loss: 0.00006494
Iteration 142/1000 | Loss: 0.00004963
Iteration 143/1000 | Loss: 0.00003840
Iteration 144/1000 | Loss: 0.00003116
Iteration 145/1000 | Loss: 0.00004184
Iteration 146/1000 | Loss: 0.00002657
Iteration 147/1000 | Loss: 0.00002583
Iteration 148/1000 | Loss: 0.00003262
Iteration 149/1000 | Loss: 0.00002649
Iteration 150/1000 | Loss: 0.00002546
Iteration 151/1000 | Loss: 0.00002502
Iteration 152/1000 | Loss: 0.00002446
Iteration 153/1000 | Loss: 0.00004200
Iteration 154/1000 | Loss: 0.00003626
Iteration 155/1000 | Loss: 0.00021632
Iteration 156/1000 | Loss: 0.00009335
Iteration 157/1000 | Loss: 0.00008921
Iteration 158/1000 | Loss: 0.00008549
Iteration 159/1000 | Loss: 0.00002705
Iteration 160/1000 | Loss: 0.00018243
Iteration 161/1000 | Loss: 0.00011109
Iteration 162/1000 | Loss: 0.00011302
Iteration 163/1000 | Loss: 0.00013794
Iteration 164/1000 | Loss: 0.00005237
Iteration 165/1000 | Loss: 0.00021100
Iteration 166/1000 | Loss: 0.00004289
Iteration 167/1000 | Loss: 0.00004557
Iteration 168/1000 | Loss: 0.00002603
Iteration 169/1000 | Loss: 0.00003392
Iteration 170/1000 | Loss: 0.00005698
Iteration 171/1000 | Loss: 0.00004179
Iteration 172/1000 | Loss: 0.00034458
Iteration 173/1000 | Loss: 0.00018695
Iteration 174/1000 | Loss: 0.00029152
Iteration 175/1000 | Loss: 0.00014172
Iteration 176/1000 | Loss: 0.00005430
Iteration 177/1000 | Loss: 0.00012470
Iteration 178/1000 | Loss: 0.00012688
Iteration 179/1000 | Loss: 0.00010065
Iteration 180/1000 | Loss: 0.00005793
Iteration 181/1000 | Loss: 0.00008107
Iteration 182/1000 | Loss: 0.00012052
Iteration 183/1000 | Loss: 0.00003706
Iteration 184/1000 | Loss: 0.00005855
Iteration 185/1000 | Loss: 0.00018435
Iteration 186/1000 | Loss: 0.00013279
Iteration 187/1000 | Loss: 0.00010545
Iteration 188/1000 | Loss: 0.00004733
Iteration 189/1000 | Loss: 0.00005054
Iteration 190/1000 | Loss: 0.00005012
Iteration 191/1000 | Loss: 0.00004079
Iteration 192/1000 | Loss: 0.00007034
Iteration 193/1000 | Loss: 0.00006114
Iteration 194/1000 | Loss: 0.00006330
Iteration 195/1000 | Loss: 0.00006036
Iteration 196/1000 | Loss: 0.00004426
Iteration 197/1000 | Loss: 0.00011275
Iteration 198/1000 | Loss: 0.00010436
Iteration 199/1000 | Loss: 0.00006338
Iteration 200/1000 | Loss: 0.00003661
Iteration 201/1000 | Loss: 0.00010946
Iteration 202/1000 | Loss: 0.00013432
Iteration 203/1000 | Loss: 0.00010897
Iteration 204/1000 | Loss: 0.00011043
Iteration 205/1000 | Loss: 0.00003510
Iteration 206/1000 | Loss: 0.00028309
Iteration 207/1000 | Loss: 0.00014862
Iteration 208/1000 | Loss: 0.00025337
Iteration 209/1000 | Loss: 0.00013093
Iteration 210/1000 | Loss: 0.00021603
Iteration 211/1000 | Loss: 0.00003600
Iteration 212/1000 | Loss: 0.00003714
Iteration 213/1000 | Loss: 0.00002650
Iteration 214/1000 | Loss: 0.00004066
Iteration 215/1000 | Loss: 0.00004167
Iteration 216/1000 | Loss: 0.00004008
Iteration 217/1000 | Loss: 0.00003395
Iteration 218/1000 | Loss: 0.00002727
Iteration 219/1000 | Loss: 0.00003908
Iteration 220/1000 | Loss: 0.00004118
Iteration 221/1000 | Loss: 0.00003902
Iteration 222/1000 | Loss: 0.00003418
Iteration 223/1000 | Loss: 0.00002965
Iteration 224/1000 | Loss: 0.00002407
Iteration 225/1000 | Loss: 0.00002769
Iteration 226/1000 | Loss: 0.00003120
Iteration 227/1000 | Loss: 0.00003329
Iteration 228/1000 | Loss: 0.00003205
Iteration 229/1000 | Loss: 0.00003759
Iteration 230/1000 | Loss: 0.00003620
Iteration 231/1000 | Loss: 0.00017938
Iteration 232/1000 | Loss: 0.00009175
Iteration 233/1000 | Loss: 0.00043154
Iteration 234/1000 | Loss: 0.00009591
Iteration 235/1000 | Loss: 0.00036018
Iteration 236/1000 | Loss: 0.00007306
Iteration 237/1000 | Loss: 0.00003401
Iteration 238/1000 | Loss: 0.00003345
Iteration 239/1000 | Loss: 0.00002758
Iteration 240/1000 | Loss: 0.00002535
Iteration 241/1000 | Loss: 0.00003204
Iteration 242/1000 | Loss: 0.00003195
Iteration 243/1000 | Loss: 0.00003197
Iteration 244/1000 | Loss: 0.00003098
Iteration 245/1000 | Loss: 0.00003151
Iteration 246/1000 | Loss: 0.00002524
Iteration 247/1000 | Loss: 0.00002959
Iteration 248/1000 | Loss: 0.00003227
Iteration 249/1000 | Loss: 0.00003047
Iteration 250/1000 | Loss: 0.00030852
Iteration 251/1000 | Loss: 0.00023290
Iteration 252/1000 | Loss: 0.00018679
Iteration 253/1000 | Loss: 0.00035395
Iteration 254/1000 | Loss: 0.00031551
Iteration 255/1000 | Loss: 0.00030583
Iteration 256/1000 | Loss: 0.00008739
Iteration 257/1000 | Loss: 0.00018809
Iteration 258/1000 | Loss: 0.00014252
Iteration 259/1000 | Loss: 0.00040294
Iteration 260/1000 | Loss: 0.00028716
Iteration 261/1000 | Loss: 0.00016259
Iteration 262/1000 | Loss: 0.00004594
Iteration 263/1000 | Loss: 0.00003493
Iteration 264/1000 | Loss: 0.00003128
Iteration 265/1000 | Loss: 0.00002828
Iteration 266/1000 | Loss: 0.00002624
Iteration 267/1000 | Loss: 0.00003185
Iteration 268/1000 | Loss: 0.00002323
Iteration 269/1000 | Loss: 0.00003950
Iteration 270/1000 | Loss: 0.00003509
Iteration 271/1000 | Loss: 0.00003215
Iteration 272/1000 | Loss: 0.00002975
Iteration 273/1000 | Loss: 0.00002932
Iteration 274/1000 | Loss: 0.00003429
Iteration 275/1000 | Loss: 0.00004484
Iteration 276/1000 | Loss: 0.00003616
Iteration 277/1000 | Loss: 0.00003977
Iteration 278/1000 | Loss: 0.00003507
Iteration 279/1000 | Loss: 0.00002876
Iteration 280/1000 | Loss: 0.00004087
Iteration 281/1000 | Loss: 0.00003836
Iteration 282/1000 | Loss: 0.00003924
Iteration 283/1000 | Loss: 0.00003866
Iteration 284/1000 | Loss: 0.00003584
Iteration 285/1000 | Loss: 0.00005834
Iteration 286/1000 | Loss: 0.00002751
Iteration 287/1000 | Loss: 0.00003786
Iteration 288/1000 | Loss: 0.00003649
Iteration 289/1000 | Loss: 0.00003826
Iteration 290/1000 | Loss: 0.00003565
Iteration 291/1000 | Loss: 0.00003841
Iteration 292/1000 | Loss: 0.00004287
Iteration 293/1000 | Loss: 0.00003993
Iteration 294/1000 | Loss: 0.00003630
Iteration 295/1000 | Loss: 0.00003943
Iteration 296/1000 | Loss: 0.00003788
Iteration 297/1000 | Loss: 0.00003889
Iteration 298/1000 | Loss: 0.00003741
Iteration 299/1000 | Loss: 0.00003556
Iteration 300/1000 | Loss: 0.00004073
Iteration 301/1000 | Loss: 0.00003922
Iteration 302/1000 | Loss: 0.00002656
Iteration 303/1000 | Loss: 0.00002594
Iteration 304/1000 | Loss: 0.00003855
Iteration 305/1000 | Loss: 0.00003721
Iteration 306/1000 | Loss: 0.00002690
Iteration 307/1000 | Loss: 0.00003418
Iteration 308/1000 | Loss: 0.00002469
Iteration 309/1000 | Loss: 0.00002810
Iteration 310/1000 | Loss: 0.00004330
Iteration 311/1000 | Loss: 0.00003648
Iteration 312/1000 | Loss: 0.00003882
Iteration 313/1000 | Loss: 0.00003545
Iteration 314/1000 | Loss: 0.00002902
Iteration 315/1000 | Loss: 0.00003024
Iteration 316/1000 | Loss: 0.00002384
Iteration 317/1000 | Loss: 0.00002604
Iteration 318/1000 | Loss: 0.00003150
Iteration 319/1000 | Loss: 0.00002630
Iteration 320/1000 | Loss: 0.00003113
Iteration 321/1000 | Loss: 0.00002597
Iteration 322/1000 | Loss: 0.00002388
Iteration 323/1000 | Loss: 0.00002522
Iteration 324/1000 | Loss: 0.00002825
Iteration 325/1000 | Loss: 0.00002729
Iteration 326/1000 | Loss: 0.00003110
Iteration 327/1000 | Loss: 0.00003157
Iteration 328/1000 | Loss: 0.00003387
Iteration 329/1000 | Loss: 0.00002993
Iteration 330/1000 | Loss: 0.00002729
Iteration 331/1000 | Loss: 0.00003232
Iteration 332/1000 | Loss: 0.00002726
Iteration 333/1000 | Loss: 0.00003345
Iteration 334/1000 | Loss: 0.00002737
Iteration 335/1000 | Loss: 0.00003301
Iteration 336/1000 | Loss: 0.00002815
Iteration 337/1000 | Loss: 0.00002963
Iteration 338/1000 | Loss: 0.00002278
Iteration 339/1000 | Loss: 0.00002587
Iteration 340/1000 | Loss: 0.00002723
Iteration 341/1000 | Loss: 0.00003062
Iteration 342/1000 | Loss: 0.00003432
Iteration 343/1000 | Loss: 0.00003051
Iteration 344/1000 | Loss: 0.00002655
Iteration 345/1000 | Loss: 0.00003210
Iteration 346/1000 | Loss: 0.00002813
Iteration 347/1000 | Loss: 0.00003213
Iteration 348/1000 | Loss: 0.00002855
Iteration 349/1000 | Loss: 0.00003044
Iteration 350/1000 | Loss: 0.00002802
Iteration 351/1000 | Loss: 0.00004391
Iteration 352/1000 | Loss: 0.00002805
Iteration 353/1000 | Loss: 0.00005283
Iteration 354/1000 | Loss: 0.00002785
Iteration 355/1000 | Loss: 0.00003111
Iteration 356/1000 | Loss: 0.00002774
Iteration 357/1000 | Loss: 0.00003038
Iteration 358/1000 | Loss: 0.00002790
Iteration 359/1000 | Loss: 0.00003673
Iteration 360/1000 | Loss: 0.00002813
Iteration 361/1000 | Loss: 0.00003267
Iteration 362/1000 | Loss: 0.00002797
Iteration 363/1000 | Loss: 0.00003136
Iteration 364/1000 | Loss: 0.00002932
Iteration 365/1000 | Loss: 0.00002933
Iteration 366/1000 | Loss: 0.00003037
Iteration 367/1000 | Loss: 0.00002905
Iteration 368/1000 | Loss: 0.00002260
Iteration 369/1000 | Loss: 0.00002549
Iteration 370/1000 | Loss: 0.00003864
Iteration 371/1000 | Loss: 0.00003515
Iteration 372/1000 | Loss: 0.00003255
Iteration 373/1000 | Loss: 0.00003721
Iteration 374/1000 | Loss: 0.00003138
Iteration 375/1000 | Loss: 0.00003011
Iteration 376/1000 | Loss: 0.00002827
Iteration 377/1000 | Loss: 0.00003226
Iteration 378/1000 | Loss: 0.00002852
Iteration 379/1000 | Loss: 0.00002899
Iteration 380/1000 | Loss: 0.00003909
Iteration 381/1000 | Loss: 0.00003041
Iteration 382/1000 | Loss: 0.00002934
Iteration 383/1000 | Loss: 0.00002944
Iteration 384/1000 | Loss: 0.00002984
Iteration 385/1000 | Loss: 0.00002883
Iteration 386/1000 | Loss: 0.00003091
Iteration 387/1000 | Loss: 0.00002987
Iteration 388/1000 | Loss: 0.00002786
Iteration 389/1000 | Loss: 0.00002909
Iteration 390/1000 | Loss: 0.00003090
Iteration 391/1000 | Loss: 0.00003373
Iteration 392/1000 | Loss: 0.00004890
Iteration 393/1000 | Loss: 0.00003435
Iteration 394/1000 | Loss: 0.00003039
Iteration 395/1000 | Loss: 0.00002606
Iteration 396/1000 | Loss: 0.00003272
Iteration 397/1000 | Loss: 0.00002821
Iteration 398/1000 | Loss: 0.00004107
Iteration 399/1000 | Loss: 0.00002849
Iteration 400/1000 | Loss: 0.00003013
Iteration 401/1000 | Loss: 0.00003834
Iteration 402/1000 | Loss: 0.00002705
Iteration 403/1000 | Loss: 0.00003074
Iteration 404/1000 | Loss: 0.00002835
Iteration 405/1000 | Loss: 0.00003010
Iteration 406/1000 | Loss: 0.00002915
Iteration 407/1000 | Loss: 0.00002845
Iteration 408/1000 | Loss: 0.00002841
Iteration 409/1000 | Loss: 0.00003014
Iteration 410/1000 | Loss: 0.00002771
Iteration 411/1000 | Loss: 0.00003231
Iteration 412/1000 | Loss: 0.00002776
Iteration 413/1000 | Loss: 0.00003809
Iteration 414/1000 | Loss: 0.00003550
Iteration 415/1000 | Loss: 0.00002923
Iteration 416/1000 | Loss: 0.00002902
Iteration 417/1000 | Loss: 0.00003414
Iteration 418/1000 | Loss: 0.00002957
Iteration 419/1000 | Loss: 0.00003416
Iteration 420/1000 | Loss: 0.00002981
Iteration 421/1000 | Loss: 0.00002974
Iteration 422/1000 | Loss: 0.00002952
Iteration 423/1000 | Loss: 0.00002982
Iteration 424/1000 | Loss: 0.00002849
Iteration 425/1000 | Loss: 0.00003166
Iteration 426/1000 | Loss: 0.00003978
Iteration 427/1000 | Loss: 0.00003524
Iteration 428/1000 | Loss: 0.00003679
Iteration 429/1000 | Loss: 0.00003001
Iteration 430/1000 | Loss: 0.00004377
Iteration 431/1000 | Loss: 0.00003114
Iteration 432/1000 | Loss: 0.00003135
Iteration 433/1000 | Loss: 0.00002939
Iteration 434/1000 | Loss: 0.00002622
Iteration 435/1000 | Loss: 0.00002648
Iteration 436/1000 | Loss: 0.00003671
Iteration 437/1000 | Loss: 0.00003928
Iteration 438/1000 | Loss: 0.00002308
Iteration 439/1000 | Loss: 0.00003054
Iteration 440/1000 | Loss: 0.00003380
Iteration 441/1000 | Loss: 0.00007058
Iteration 442/1000 | Loss: 0.00005457
Iteration 443/1000 | Loss: 0.00003014
Iteration 444/1000 | Loss: 0.00002286
Iteration 445/1000 | Loss: 0.00002404
Iteration 446/1000 | Loss: 0.00002415
Iteration 447/1000 | Loss: 0.00002126
Iteration 448/1000 | Loss: 0.00002116
Iteration 449/1000 | Loss: 0.00002534
Iteration 450/1000 | Loss: 0.00002104
Iteration 451/1000 | Loss: 0.00003946
Iteration 452/1000 | Loss: 0.00002794
Iteration 453/1000 | Loss: 0.00002497
Iteration 454/1000 | Loss: 0.00004532
Iteration 455/1000 | Loss: 0.00002288
Iteration 456/1000 | Loss: 0.00004310
Iteration 457/1000 | Loss: 0.00004609
Iteration 458/1000 | Loss: 0.00004194
Iteration 459/1000 | Loss: 0.00004533
Iteration 460/1000 | Loss: 0.00004159
Iteration 461/1000 | Loss: 0.00002374
Iteration 462/1000 | Loss: 0.00004471
Iteration 463/1000 | Loss: 0.00002696
Iteration 464/1000 | Loss: 0.00002224
Iteration 465/1000 | Loss: 0.00002161
Iteration 466/1000 | Loss: 0.00002291
Iteration 467/1000 | Loss: 0.00002087
Iteration 468/1000 | Loss: 0.00002265
Iteration 469/1000 | Loss: 0.00002049
Iteration 470/1000 | Loss: 0.00002204
Iteration 471/1000 | Loss: 0.00002056
Iteration 472/1000 | Loss: 0.00002038
Iteration 473/1000 | Loss: 0.00002038
Iteration 474/1000 | Loss: 0.00002038
Iteration 475/1000 | Loss: 0.00002038
Iteration 476/1000 | Loss: 0.00002038
Iteration 477/1000 | Loss: 0.00002038
Iteration 478/1000 | Loss: 0.00002038
Iteration 479/1000 | Loss: 0.00002038
Iteration 480/1000 | Loss: 0.00002038
Iteration 481/1000 | Loss: 0.00002038
Iteration 482/1000 | Loss: 0.00002038
Iteration 483/1000 | Loss: 0.00002038
Iteration 484/1000 | Loss: 0.00002037
Iteration 485/1000 | Loss: 0.00002037
Iteration 486/1000 | Loss: 0.00002034
Iteration 487/1000 | Loss: 0.00002034
Iteration 488/1000 | Loss: 0.00002034
Iteration 489/1000 | Loss: 0.00002034
Iteration 490/1000 | Loss: 0.00002034
Iteration 491/1000 | Loss: 0.00002034
Iteration 492/1000 | Loss: 0.00002033
Iteration 493/1000 | Loss: 0.00002033
Iteration 494/1000 | Loss: 0.00002033
Iteration 495/1000 | Loss: 0.00002033
Iteration 496/1000 | Loss: 0.00002033
Iteration 497/1000 | Loss: 0.00002033
Iteration 498/1000 | Loss: 0.00002032
Iteration 499/1000 | Loss: 0.00002032
Iteration 500/1000 | Loss: 0.00002032
Iteration 501/1000 | Loss: 0.00002032
Iteration 502/1000 | Loss: 0.00002039
Iteration 503/1000 | Loss: 0.00002032
Iteration 504/1000 | Loss: 0.00002032
Iteration 505/1000 | Loss: 0.00002032
Iteration 506/1000 | Loss: 0.00002032
Iteration 507/1000 | Loss: 0.00002031
Iteration 508/1000 | Loss: 0.00002031
Iteration 509/1000 | Loss: 0.00002031
Iteration 510/1000 | Loss: 0.00002031
Iteration 511/1000 | Loss: 0.00002031
Iteration 512/1000 | Loss: 0.00002031
Iteration 513/1000 | Loss: 0.00002031
Iteration 514/1000 | Loss: 0.00002031
Iteration 515/1000 | Loss: 0.00002031
Iteration 516/1000 | Loss: 0.00002031
Iteration 517/1000 | Loss: 0.00002031
Iteration 518/1000 | Loss: 0.00002031
Iteration 519/1000 | Loss: 0.00002031
Iteration 520/1000 | Loss: 0.00002030
Iteration 521/1000 | Loss: 0.00002030
Iteration 522/1000 | Loss: 0.00002030
Iteration 523/1000 | Loss: 0.00002030
Iteration 524/1000 | Loss: 0.00002030
Iteration 525/1000 | Loss: 0.00002030
Iteration 526/1000 | Loss: 0.00002030
Iteration 527/1000 | Loss: 0.00002030
Iteration 528/1000 | Loss: 0.00002030
Iteration 529/1000 | Loss: 0.00002030
Iteration 530/1000 | Loss: 0.00002030
Iteration 531/1000 | Loss: 0.00002030
Iteration 532/1000 | Loss: 0.00002030
Iteration 533/1000 | Loss: 0.00002030
Iteration 534/1000 | Loss: 0.00002030
Iteration 535/1000 | Loss: 0.00002030
Iteration 536/1000 | Loss: 0.00002030
Iteration 537/1000 | Loss: 0.00002029
Iteration 538/1000 | Loss: 0.00002029
Iteration 539/1000 | Loss: 0.00002029
Iteration 540/1000 | Loss: 0.00002029
Iteration 541/1000 | Loss: 0.00002029
Iteration 542/1000 | Loss: 0.00002029
Iteration 543/1000 | Loss: 0.00002029
Iteration 544/1000 | Loss: 0.00002029
Iteration 545/1000 | Loss: 0.00002029
Iteration 546/1000 | Loss: 0.00002029
Iteration 547/1000 | Loss: 0.00002029
Iteration 548/1000 | Loss: 0.00002029
Iteration 549/1000 | Loss: 0.00002029
Iteration 550/1000 | Loss: 0.00002028
Iteration 551/1000 | Loss: 0.00002028
Iteration 552/1000 | Loss: 0.00002028
Iteration 553/1000 | Loss: 0.00002028
Iteration 554/1000 | Loss: 0.00002028
Iteration 555/1000 | Loss: 0.00002028
Iteration 556/1000 | Loss: 0.00002028
Iteration 557/1000 | Loss: 0.00002028
Iteration 558/1000 | Loss: 0.00002028
Iteration 559/1000 | Loss: 0.00002028
Iteration 560/1000 | Loss: 0.00002028
Iteration 561/1000 | Loss: 0.00002028
Iteration 562/1000 | Loss: 0.00002028
Iteration 563/1000 | Loss: 0.00002028
Iteration 564/1000 | Loss: 0.00002028
Iteration 565/1000 | Loss: 0.00002028
Iteration 566/1000 | Loss: 0.00002028
Iteration 567/1000 | Loss: 0.00002028
Iteration 568/1000 | Loss: 0.00002028
Iteration 569/1000 | Loss: 0.00002028
Iteration 570/1000 | Loss: 0.00002028
Iteration 571/1000 | Loss: 0.00002028
Iteration 572/1000 | Loss: 0.00002028
Iteration 573/1000 | Loss: 0.00002028
Iteration 574/1000 | Loss: 0.00002028
Iteration 575/1000 | Loss: 0.00002028
Iteration 576/1000 | Loss: 0.00002028
Iteration 577/1000 | Loss: 0.00002028
Iteration 578/1000 | Loss: 0.00002028
Iteration 579/1000 | Loss: 0.00002028
Iteration 580/1000 | Loss: 0.00002028
Iteration 581/1000 | Loss: 0.00002028
Iteration 582/1000 | Loss: 0.00002028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 582. Stopping optimization.
Last 5 losses: [2.027939808613155e-05, 2.027939808613155e-05, 2.027939808613155e-05, 2.027939808613155e-05, 2.027939808613155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.027939808613155e-05

Optimization complete. Final v2v error: 3.6592724323272705 mm

Highest mean error: 9.5897798538208 mm for frame 209

Lowest mean error: 2.7217814922332764 mm for frame 122

Saving results

Total time: 793.1999657154083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807454
Iteration 2/25 | Loss: 0.00214710
Iteration 3/25 | Loss: 0.00100377
Iteration 4/25 | Loss: 0.00088000
Iteration 5/25 | Loss: 0.00076382
Iteration 6/25 | Loss: 0.00075140
Iteration 7/25 | Loss: 0.00076187
Iteration 8/25 | Loss: 0.00067337
Iteration 9/25 | Loss: 0.00068805
Iteration 10/25 | Loss: 0.00064167
Iteration 11/25 | Loss: 0.00063738
Iteration 12/25 | Loss: 0.00063063
Iteration 13/25 | Loss: 0.00063087
Iteration 14/25 | Loss: 0.00062379
Iteration 15/25 | Loss: 0.00062507
Iteration 16/25 | Loss: 0.00062023
Iteration 17/25 | Loss: 0.00062009
Iteration 18/25 | Loss: 0.00061988
Iteration 19/25 | Loss: 0.00061976
Iteration 20/25 | Loss: 0.00061973
Iteration 21/25 | Loss: 0.00061973
Iteration 22/25 | Loss: 0.00061973
Iteration 23/25 | Loss: 0.00061973
Iteration 24/25 | Loss: 0.00061973
Iteration 25/25 | Loss: 0.00061973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09372115
Iteration 2/25 | Loss: 0.00019134
Iteration 3/25 | Loss: 0.00020731
Iteration 4/25 | Loss: 0.00020731
Iteration 5/25 | Loss: 0.00020731
Iteration 6/25 | Loss: 0.00020731
Iteration 7/25 | Loss: 0.00020731
Iteration 8/25 | Loss: 0.00020731
Iteration 9/25 | Loss: 0.00020731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.00020730617688968778, 0.00020730617688968778, 0.00020730617688968778, 0.00020730617688968778, 0.00020730617688968778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020730617688968778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020731
Iteration 2/1000 | Loss: 0.00002694
Iteration 3/1000 | Loss: 0.00001977
Iteration 4/1000 | Loss: 0.00001802
Iteration 5/1000 | Loss: 0.00003372
Iteration 6/1000 | Loss: 0.00015384
Iteration 7/1000 | Loss: 0.00002326
Iteration 8/1000 | Loss: 0.00001836
Iteration 9/1000 | Loss: 0.00002110
Iteration 10/1000 | Loss: 0.00001817
Iteration 11/1000 | Loss: 0.00002909
Iteration 12/1000 | Loss: 0.00003218
Iteration 13/1000 | Loss: 0.00004326
Iteration 14/1000 | Loss: 0.00001547
Iteration 15/1000 | Loss: 0.00001732
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001534
Iteration 18/1000 | Loss: 0.00001575
Iteration 19/1000 | Loss: 0.00001530
Iteration 20/1000 | Loss: 0.00001529
Iteration 21/1000 | Loss: 0.00001529
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001528
Iteration 24/1000 | Loss: 0.00001528
Iteration 25/1000 | Loss: 0.00001537
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001527
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001527
Iteration 36/1000 | Loss: 0.00001527
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [1.52712036651792e-05, 1.52712036651792e-05, 1.52712036651792e-05, 1.52712036651792e-05, 1.52712036651792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.52712036651792e-05

Optimization complete. Final v2v error: 3.2087526321411133 mm

Highest mean error: 9.656885147094727 mm for frame 131

Lowest mean error: 2.88804292678833 mm for frame 144

Saving results

Total time: 58.467326641082764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386956
Iteration 2/25 | Loss: 0.00078689
Iteration 3/25 | Loss: 0.00067948
Iteration 4/25 | Loss: 0.00065986
Iteration 5/25 | Loss: 0.00065159
Iteration 6/25 | Loss: 0.00065026
Iteration 7/25 | Loss: 0.00065004
Iteration 8/25 | Loss: 0.00065004
Iteration 9/25 | Loss: 0.00065004
Iteration 10/25 | Loss: 0.00065004
Iteration 11/25 | Loss: 0.00065004
Iteration 12/25 | Loss: 0.00065004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006500413292087615, 0.0006500413292087615, 0.0006500413292087615, 0.0006500413292087615, 0.0006500413292087615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006500413292087615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.64280748
Iteration 2/25 | Loss: 0.00016557
Iteration 3/25 | Loss: 0.00016557
Iteration 4/25 | Loss: 0.00016557
Iteration 5/25 | Loss: 0.00016557
Iteration 6/25 | Loss: 0.00016557
Iteration 7/25 | Loss: 0.00016556
Iteration 8/25 | Loss: 0.00016556
Iteration 9/25 | Loss: 0.00016556
Iteration 10/25 | Loss: 0.00016556
Iteration 11/25 | Loss: 0.00016556
Iteration 12/25 | Loss: 0.00016556
Iteration 13/25 | Loss: 0.00016556
Iteration 14/25 | Loss: 0.00016556
Iteration 15/25 | Loss: 0.00016556
Iteration 16/25 | Loss: 0.00016556
Iteration 17/25 | Loss: 0.00016556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00016556394984945655, 0.00016556394984945655, 0.00016556394984945655, 0.00016556394984945655, 0.00016556394984945655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00016556394984945655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016556
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00001892
Iteration 4/1000 | Loss: 0.00001738
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001607
Iteration 7/1000 | Loss: 0.00001577
Iteration 8/1000 | Loss: 0.00001556
Iteration 9/1000 | Loss: 0.00001553
Iteration 10/1000 | Loss: 0.00001553
Iteration 11/1000 | Loss: 0.00001549
Iteration 12/1000 | Loss: 0.00001542
Iteration 13/1000 | Loss: 0.00001538
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001533
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001529
Iteration 18/1000 | Loss: 0.00001526
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001524
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001524
Iteration 30/1000 | Loss: 0.00001523
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001520
Iteration 35/1000 | Loss: 0.00001520
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001519
Iteration 38/1000 | Loss: 0.00001519
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00001518
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001517
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001516
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001516
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001514
Iteration 58/1000 | Loss: 0.00001514
Iteration 59/1000 | Loss: 0.00001514
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001513
Iteration 62/1000 | Loss: 0.00001513
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001512
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001512
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001512
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001511
Iteration 88/1000 | Loss: 0.00001511
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001510
Iteration 93/1000 | Loss: 0.00001510
Iteration 94/1000 | Loss: 0.00001510
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001508
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001507
Iteration 129/1000 | Loss: 0.00001507
Iteration 130/1000 | Loss: 0.00001507
Iteration 131/1000 | Loss: 0.00001507
Iteration 132/1000 | Loss: 0.00001507
Iteration 133/1000 | Loss: 0.00001507
Iteration 134/1000 | Loss: 0.00001507
Iteration 135/1000 | Loss: 0.00001507
Iteration 136/1000 | Loss: 0.00001507
Iteration 137/1000 | Loss: 0.00001507
Iteration 138/1000 | Loss: 0.00001507
Iteration 139/1000 | Loss: 0.00001507
Iteration 140/1000 | Loss: 0.00001507
Iteration 141/1000 | Loss: 0.00001507
Iteration 142/1000 | Loss: 0.00001507
Iteration 143/1000 | Loss: 0.00001507
Iteration 144/1000 | Loss: 0.00001507
Iteration 145/1000 | Loss: 0.00001507
Iteration 146/1000 | Loss: 0.00001507
Iteration 147/1000 | Loss: 0.00001507
Iteration 148/1000 | Loss: 0.00001507
Iteration 149/1000 | Loss: 0.00001507
Iteration 150/1000 | Loss: 0.00001507
Iteration 151/1000 | Loss: 0.00001507
Iteration 152/1000 | Loss: 0.00001507
Iteration 153/1000 | Loss: 0.00001507
Iteration 154/1000 | Loss: 0.00001507
Iteration 155/1000 | Loss: 0.00001507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.5067778804223053e-05, 1.5067778804223053e-05, 1.5067778804223053e-05, 1.5067778804223053e-05, 1.5067778804223053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5067778804223053e-05

Optimization complete. Final v2v error: 3.3091413974761963 mm

Highest mean error: 3.5552186965942383 mm for frame 20

Lowest mean error: 2.942715644836426 mm for frame 40

Saving results

Total time: 32.06145668029785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_it_4611/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_it_4611/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387973
Iteration 2/25 | Loss: 0.00092251
Iteration 3/25 | Loss: 0.00078028
Iteration 4/25 | Loss: 0.00074708
Iteration 5/25 | Loss: 0.00073480
Iteration 6/25 | Loss: 0.00073122
Iteration 7/25 | Loss: 0.00073015
Iteration 8/25 | Loss: 0.00072976
Iteration 9/25 | Loss: 0.00072976
Iteration 10/25 | Loss: 0.00072976
Iteration 11/25 | Loss: 0.00072976
Iteration 12/25 | Loss: 0.00072976
Iteration 13/25 | Loss: 0.00072976
Iteration 14/25 | Loss: 0.00072976
Iteration 15/25 | Loss: 0.00072976
Iteration 16/25 | Loss: 0.00072976
Iteration 17/25 | Loss: 0.00072976
Iteration 18/25 | Loss: 0.00072976
Iteration 19/25 | Loss: 0.00072976
Iteration 20/25 | Loss: 0.00072976
Iteration 21/25 | Loss: 0.00072976
Iteration 22/25 | Loss: 0.00072976
Iteration 23/25 | Loss: 0.00072976
Iteration 24/25 | Loss: 0.00072976
Iteration 25/25 | Loss: 0.00072976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007297631236724555, 0.0007297631236724555, 0.0007297631236724555, 0.0007297631236724555, 0.0007297631236724555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007297631236724555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39890957
Iteration 2/25 | Loss: 0.00028937
Iteration 3/25 | Loss: 0.00028937
Iteration 4/25 | Loss: 0.00028937
Iteration 5/25 | Loss: 0.00028937
Iteration 6/25 | Loss: 0.00028937
Iteration 7/25 | Loss: 0.00028937
Iteration 8/25 | Loss: 0.00028937
Iteration 9/25 | Loss: 0.00028937
Iteration 10/25 | Loss: 0.00028937
Iteration 11/25 | Loss: 0.00028937
Iteration 12/25 | Loss: 0.00028937
Iteration 13/25 | Loss: 0.00028937
Iteration 14/25 | Loss: 0.00028937
Iteration 15/25 | Loss: 0.00028937
Iteration 16/25 | Loss: 0.00028937
Iteration 17/25 | Loss: 0.00028937
Iteration 18/25 | Loss: 0.00028937
Iteration 19/25 | Loss: 0.00028937
Iteration 20/25 | Loss: 0.00028937
Iteration 21/25 | Loss: 0.00028937
Iteration 22/25 | Loss: 0.00028937
Iteration 23/25 | Loss: 0.00028937
Iteration 24/25 | Loss: 0.00028937
Iteration 25/25 | Loss: 0.00028937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028937
Iteration 2/1000 | Loss: 0.00005589
Iteration 3/1000 | Loss: 0.00003872
Iteration 4/1000 | Loss: 0.00003221
Iteration 5/1000 | Loss: 0.00003001
Iteration 6/1000 | Loss: 0.00002828
Iteration 7/1000 | Loss: 0.00002734
Iteration 8/1000 | Loss: 0.00002681
Iteration 9/1000 | Loss: 0.00002621
Iteration 10/1000 | Loss: 0.00002574
Iteration 11/1000 | Loss: 0.00002544
Iteration 12/1000 | Loss: 0.00002520
Iteration 13/1000 | Loss: 0.00002501
Iteration 14/1000 | Loss: 0.00002500
Iteration 15/1000 | Loss: 0.00002487
Iteration 16/1000 | Loss: 0.00002486
Iteration 17/1000 | Loss: 0.00002485
Iteration 18/1000 | Loss: 0.00002484
Iteration 19/1000 | Loss: 0.00002484
Iteration 20/1000 | Loss: 0.00002483
Iteration 21/1000 | Loss: 0.00002483
Iteration 22/1000 | Loss: 0.00002482
Iteration 23/1000 | Loss: 0.00002482
Iteration 24/1000 | Loss: 0.00002482
Iteration 25/1000 | Loss: 0.00002481
Iteration 26/1000 | Loss: 0.00002481
Iteration 27/1000 | Loss: 0.00002480
Iteration 28/1000 | Loss: 0.00002480
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002479
Iteration 31/1000 | Loss: 0.00002478
Iteration 32/1000 | Loss: 0.00002478
Iteration 33/1000 | Loss: 0.00002476
Iteration 34/1000 | Loss: 0.00002475
Iteration 35/1000 | Loss: 0.00002475
Iteration 36/1000 | Loss: 0.00002473
Iteration 37/1000 | Loss: 0.00002473
Iteration 38/1000 | Loss: 0.00002472
Iteration 39/1000 | Loss: 0.00002472
Iteration 40/1000 | Loss: 0.00002472
Iteration 41/1000 | Loss: 0.00002471
Iteration 42/1000 | Loss: 0.00002471
Iteration 43/1000 | Loss: 0.00002471
Iteration 44/1000 | Loss: 0.00002470
Iteration 45/1000 | Loss: 0.00002470
Iteration 46/1000 | Loss: 0.00002470
Iteration 47/1000 | Loss: 0.00002470
Iteration 48/1000 | Loss: 0.00002469
Iteration 49/1000 | Loss: 0.00002469
Iteration 50/1000 | Loss: 0.00002469
Iteration 51/1000 | Loss: 0.00002469
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00002468
Iteration 54/1000 | Loss: 0.00002468
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002467
Iteration 57/1000 | Loss: 0.00002467
Iteration 58/1000 | Loss: 0.00002467
Iteration 59/1000 | Loss: 0.00002467
Iteration 60/1000 | Loss: 0.00002467
Iteration 61/1000 | Loss: 0.00002467
Iteration 62/1000 | Loss: 0.00002467
Iteration 63/1000 | Loss: 0.00002467
Iteration 64/1000 | Loss: 0.00002466
Iteration 65/1000 | Loss: 0.00002466
Iteration 66/1000 | Loss: 0.00002466
Iteration 67/1000 | Loss: 0.00002465
Iteration 68/1000 | Loss: 0.00002465
Iteration 69/1000 | Loss: 0.00002464
Iteration 70/1000 | Loss: 0.00002464
Iteration 71/1000 | Loss: 0.00002464
Iteration 72/1000 | Loss: 0.00002464
Iteration 73/1000 | Loss: 0.00002464
Iteration 74/1000 | Loss: 0.00002463
Iteration 75/1000 | Loss: 0.00002463
Iteration 76/1000 | Loss: 0.00002463
Iteration 77/1000 | Loss: 0.00002462
Iteration 78/1000 | Loss: 0.00002462
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00002461
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002461
Iteration 83/1000 | Loss: 0.00002461
Iteration 84/1000 | Loss: 0.00002461
Iteration 85/1000 | Loss: 0.00002460
Iteration 86/1000 | Loss: 0.00002460
Iteration 87/1000 | Loss: 0.00002460
Iteration 88/1000 | Loss: 0.00002460
Iteration 89/1000 | Loss: 0.00002460
Iteration 90/1000 | Loss: 0.00002460
Iteration 91/1000 | Loss: 0.00002460
Iteration 92/1000 | Loss: 0.00002459
Iteration 93/1000 | Loss: 0.00002459
Iteration 94/1000 | Loss: 0.00002459
Iteration 95/1000 | Loss: 0.00002459
Iteration 96/1000 | Loss: 0.00002459
Iteration 97/1000 | Loss: 0.00002459
Iteration 98/1000 | Loss: 0.00002459
Iteration 99/1000 | Loss: 0.00002459
Iteration 100/1000 | Loss: 0.00002459
Iteration 101/1000 | Loss: 0.00002459
Iteration 102/1000 | Loss: 0.00002459
Iteration 103/1000 | Loss: 0.00002458
Iteration 104/1000 | Loss: 0.00002458
Iteration 105/1000 | Loss: 0.00002458
Iteration 106/1000 | Loss: 0.00002457
Iteration 107/1000 | Loss: 0.00002457
Iteration 108/1000 | Loss: 0.00002457
Iteration 109/1000 | Loss: 0.00002456
Iteration 110/1000 | Loss: 0.00002456
Iteration 111/1000 | Loss: 0.00002456
Iteration 112/1000 | Loss: 0.00002456
Iteration 113/1000 | Loss: 0.00002455
Iteration 114/1000 | Loss: 0.00002455
Iteration 115/1000 | Loss: 0.00002455
Iteration 116/1000 | Loss: 0.00002455
Iteration 117/1000 | Loss: 0.00002454
Iteration 118/1000 | Loss: 0.00002454
Iteration 119/1000 | Loss: 0.00002454
Iteration 120/1000 | Loss: 0.00002454
Iteration 121/1000 | Loss: 0.00002454
Iteration 122/1000 | Loss: 0.00002454
Iteration 123/1000 | Loss: 0.00002454
Iteration 124/1000 | Loss: 0.00002454
Iteration 125/1000 | Loss: 0.00002454
Iteration 126/1000 | Loss: 0.00002454
Iteration 127/1000 | Loss: 0.00002453
Iteration 128/1000 | Loss: 0.00002453
Iteration 129/1000 | Loss: 0.00002453
Iteration 130/1000 | Loss: 0.00002453
Iteration 131/1000 | Loss: 0.00002453
Iteration 132/1000 | Loss: 0.00002453
Iteration 133/1000 | Loss: 0.00002453
Iteration 134/1000 | Loss: 0.00002453
Iteration 135/1000 | Loss: 0.00002453
Iteration 136/1000 | Loss: 0.00002452
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00002452
Iteration 139/1000 | Loss: 0.00002452
Iteration 140/1000 | Loss: 0.00002452
Iteration 141/1000 | Loss: 0.00002452
Iteration 142/1000 | Loss: 0.00002452
Iteration 143/1000 | Loss: 0.00002451
Iteration 144/1000 | Loss: 0.00002451
Iteration 145/1000 | Loss: 0.00002451
Iteration 146/1000 | Loss: 0.00002451
Iteration 147/1000 | Loss: 0.00002451
Iteration 148/1000 | Loss: 0.00002451
Iteration 149/1000 | Loss: 0.00002450
Iteration 150/1000 | Loss: 0.00002450
Iteration 151/1000 | Loss: 0.00002450
Iteration 152/1000 | Loss: 0.00002450
Iteration 153/1000 | Loss: 0.00002450
Iteration 154/1000 | Loss: 0.00002450
Iteration 155/1000 | Loss: 0.00002450
Iteration 156/1000 | Loss: 0.00002450
Iteration 157/1000 | Loss: 0.00002449
Iteration 158/1000 | Loss: 0.00002449
Iteration 159/1000 | Loss: 0.00002449
Iteration 160/1000 | Loss: 0.00002449
Iteration 161/1000 | Loss: 0.00002449
Iteration 162/1000 | Loss: 0.00002449
Iteration 163/1000 | Loss: 0.00002449
Iteration 164/1000 | Loss: 0.00002449
Iteration 165/1000 | Loss: 0.00002449
Iteration 166/1000 | Loss: 0.00002449
Iteration 167/1000 | Loss: 0.00002449
Iteration 168/1000 | Loss: 0.00002449
Iteration 169/1000 | Loss: 0.00002448
Iteration 170/1000 | Loss: 0.00002448
Iteration 171/1000 | Loss: 0.00002448
Iteration 172/1000 | Loss: 0.00002448
Iteration 173/1000 | Loss: 0.00002448
Iteration 174/1000 | Loss: 0.00002448
Iteration 175/1000 | Loss: 0.00002448
Iteration 176/1000 | Loss: 0.00002448
Iteration 177/1000 | Loss: 0.00002448
Iteration 178/1000 | Loss: 0.00002448
Iteration 179/1000 | Loss: 0.00002448
Iteration 180/1000 | Loss: 0.00002448
Iteration 181/1000 | Loss: 0.00002448
Iteration 182/1000 | Loss: 0.00002448
Iteration 183/1000 | Loss: 0.00002448
Iteration 184/1000 | Loss: 0.00002448
Iteration 185/1000 | Loss: 0.00002448
Iteration 186/1000 | Loss: 0.00002448
Iteration 187/1000 | Loss: 0.00002448
Iteration 188/1000 | Loss: 0.00002448
Iteration 189/1000 | Loss: 0.00002448
Iteration 190/1000 | Loss: 0.00002448
Iteration 191/1000 | Loss: 0.00002448
Iteration 192/1000 | Loss: 0.00002448
Iteration 193/1000 | Loss: 0.00002448
Iteration 194/1000 | Loss: 0.00002448
Iteration 195/1000 | Loss: 0.00002448
Iteration 196/1000 | Loss: 0.00002448
Iteration 197/1000 | Loss: 0.00002448
Iteration 198/1000 | Loss: 0.00002448
Iteration 199/1000 | Loss: 0.00002448
Iteration 200/1000 | Loss: 0.00002448
Iteration 201/1000 | Loss: 0.00002448
Iteration 202/1000 | Loss: 0.00002448
Iteration 203/1000 | Loss: 0.00002448
Iteration 204/1000 | Loss: 0.00002448
Iteration 205/1000 | Loss: 0.00002448
Iteration 206/1000 | Loss: 0.00002448
Iteration 207/1000 | Loss: 0.00002448
Iteration 208/1000 | Loss: 0.00002448
Iteration 209/1000 | Loss: 0.00002448
Iteration 210/1000 | Loss: 0.00002448
Iteration 211/1000 | Loss: 0.00002448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.4475290047121234e-05, 2.4475290047121234e-05, 2.4475290047121234e-05, 2.4475290047121234e-05, 2.4475290047121234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4475290047121234e-05

Optimization complete. Final v2v error: 4.073423862457275 mm

Highest mean error: 4.513727188110352 mm for frame 104

Lowest mean error: 3.313389778137207 mm for frame 7

Saving results

Total time: 46.39249229431152
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461316
Iteration 2/25 | Loss: 0.00158485
Iteration 3/25 | Loss: 0.00151501
Iteration 4/25 | Loss: 0.00150471
Iteration 5/25 | Loss: 0.00149815
Iteration 6/25 | Loss: 0.00149664
Iteration 7/25 | Loss: 0.00149650
Iteration 8/25 | Loss: 0.00149650
Iteration 9/25 | Loss: 0.00149650
Iteration 10/25 | Loss: 0.00149650
Iteration 11/25 | Loss: 0.00149650
Iteration 12/25 | Loss: 0.00149650
Iteration 13/25 | Loss: 0.00149650
Iteration 14/25 | Loss: 0.00149650
Iteration 15/25 | Loss: 0.00149650
Iteration 16/25 | Loss: 0.00149650
Iteration 17/25 | Loss: 0.00149650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001496504875831306, 0.001496504875831306, 0.001496504875831306, 0.001496504875831306, 0.001496504875831306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001496504875831306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92915475
Iteration 2/25 | Loss: 0.00369231
Iteration 3/25 | Loss: 0.00369231
Iteration 4/25 | Loss: 0.00369231
Iteration 5/25 | Loss: 0.00369231
Iteration 6/25 | Loss: 0.00369231
Iteration 7/25 | Loss: 0.00369231
Iteration 8/25 | Loss: 0.00369231
Iteration 9/25 | Loss: 0.00369231
Iteration 10/25 | Loss: 0.00369231
Iteration 11/25 | Loss: 0.00369231
Iteration 12/25 | Loss: 0.00369231
Iteration 13/25 | Loss: 0.00369231
Iteration 14/25 | Loss: 0.00369231
Iteration 15/25 | Loss: 0.00369231
Iteration 16/25 | Loss: 0.00369231
Iteration 17/25 | Loss: 0.00369231
Iteration 18/25 | Loss: 0.00369231
Iteration 19/25 | Loss: 0.00369231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003692307509481907, 0.003692307509481907, 0.003692307509481907, 0.003692307509481907, 0.003692307509481907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003692307509481907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00369231
Iteration 2/1000 | Loss: 0.00008230
Iteration 3/1000 | Loss: 0.00004230
Iteration 4/1000 | Loss: 0.00003740
Iteration 5/1000 | Loss: 0.00003279
Iteration 6/1000 | Loss: 0.00003080
Iteration 7/1000 | Loss: 0.00002926
Iteration 8/1000 | Loss: 0.00002832
Iteration 9/1000 | Loss: 0.00002786
Iteration 10/1000 | Loss: 0.00002748
Iteration 11/1000 | Loss: 0.00002709
Iteration 12/1000 | Loss: 0.00002667
Iteration 13/1000 | Loss: 0.00002642
Iteration 14/1000 | Loss: 0.00002623
Iteration 15/1000 | Loss: 0.00002623
Iteration 16/1000 | Loss: 0.00002620
Iteration 17/1000 | Loss: 0.00002613
Iteration 18/1000 | Loss: 0.00002608
Iteration 19/1000 | Loss: 0.00002607
Iteration 20/1000 | Loss: 0.00002607
Iteration 21/1000 | Loss: 0.00002607
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002606
Iteration 24/1000 | Loss: 0.00002604
Iteration 25/1000 | Loss: 0.00002603
Iteration 26/1000 | Loss: 0.00002603
Iteration 27/1000 | Loss: 0.00002602
Iteration 28/1000 | Loss: 0.00002602
Iteration 29/1000 | Loss: 0.00002602
Iteration 30/1000 | Loss: 0.00002602
Iteration 31/1000 | Loss: 0.00002601
Iteration 32/1000 | Loss: 0.00002601
Iteration 33/1000 | Loss: 0.00002599
Iteration 34/1000 | Loss: 0.00002599
Iteration 35/1000 | Loss: 0.00002598
Iteration 36/1000 | Loss: 0.00002598
Iteration 37/1000 | Loss: 0.00002598
Iteration 38/1000 | Loss: 0.00002597
Iteration 39/1000 | Loss: 0.00002597
Iteration 40/1000 | Loss: 0.00002597
Iteration 41/1000 | Loss: 0.00002596
Iteration 42/1000 | Loss: 0.00002596
Iteration 43/1000 | Loss: 0.00002596
Iteration 44/1000 | Loss: 0.00002596
Iteration 45/1000 | Loss: 0.00002596
Iteration 46/1000 | Loss: 0.00002596
Iteration 47/1000 | Loss: 0.00002595
Iteration 48/1000 | Loss: 0.00002595
Iteration 49/1000 | Loss: 0.00002595
Iteration 50/1000 | Loss: 0.00002595
Iteration 51/1000 | Loss: 0.00002595
Iteration 52/1000 | Loss: 0.00002595
Iteration 53/1000 | Loss: 0.00002595
Iteration 54/1000 | Loss: 0.00002594
Iteration 55/1000 | Loss: 0.00002594
Iteration 56/1000 | Loss: 0.00002594
Iteration 57/1000 | Loss: 0.00002593
Iteration 58/1000 | Loss: 0.00002593
Iteration 59/1000 | Loss: 0.00002593
Iteration 60/1000 | Loss: 0.00002593
Iteration 61/1000 | Loss: 0.00002593
Iteration 62/1000 | Loss: 0.00002593
Iteration 63/1000 | Loss: 0.00002593
Iteration 64/1000 | Loss: 0.00002592
Iteration 65/1000 | Loss: 0.00002592
Iteration 66/1000 | Loss: 0.00002592
Iteration 67/1000 | Loss: 0.00002592
Iteration 68/1000 | Loss: 0.00002592
Iteration 69/1000 | Loss: 0.00002592
Iteration 70/1000 | Loss: 0.00002591
Iteration 71/1000 | Loss: 0.00002591
Iteration 72/1000 | Loss: 0.00002591
Iteration 73/1000 | Loss: 0.00002591
Iteration 74/1000 | Loss: 0.00002591
Iteration 75/1000 | Loss: 0.00002591
Iteration 76/1000 | Loss: 0.00002591
Iteration 77/1000 | Loss: 0.00002591
Iteration 78/1000 | Loss: 0.00002591
Iteration 79/1000 | Loss: 0.00002591
Iteration 80/1000 | Loss: 0.00002591
Iteration 81/1000 | Loss: 0.00002590
Iteration 82/1000 | Loss: 0.00002590
Iteration 83/1000 | Loss: 0.00002590
Iteration 84/1000 | Loss: 0.00002590
Iteration 85/1000 | Loss: 0.00002589
Iteration 86/1000 | Loss: 0.00002589
Iteration 87/1000 | Loss: 0.00002589
Iteration 88/1000 | Loss: 0.00002589
Iteration 89/1000 | Loss: 0.00002589
Iteration 90/1000 | Loss: 0.00002589
Iteration 91/1000 | Loss: 0.00002589
Iteration 92/1000 | Loss: 0.00002588
Iteration 93/1000 | Loss: 0.00002588
Iteration 94/1000 | Loss: 0.00002588
Iteration 95/1000 | Loss: 0.00002587
Iteration 96/1000 | Loss: 0.00002587
Iteration 97/1000 | Loss: 0.00002587
Iteration 98/1000 | Loss: 0.00002587
Iteration 99/1000 | Loss: 0.00002587
Iteration 100/1000 | Loss: 0.00002587
Iteration 101/1000 | Loss: 0.00002586
Iteration 102/1000 | Loss: 0.00002586
Iteration 103/1000 | Loss: 0.00002586
Iteration 104/1000 | Loss: 0.00002586
Iteration 105/1000 | Loss: 0.00002586
Iteration 106/1000 | Loss: 0.00002585
Iteration 107/1000 | Loss: 0.00002585
Iteration 108/1000 | Loss: 0.00002585
Iteration 109/1000 | Loss: 0.00002585
Iteration 110/1000 | Loss: 0.00002585
Iteration 111/1000 | Loss: 0.00002584
Iteration 112/1000 | Loss: 0.00002584
Iteration 113/1000 | Loss: 0.00002584
Iteration 114/1000 | Loss: 0.00002584
Iteration 115/1000 | Loss: 0.00002583
Iteration 116/1000 | Loss: 0.00002583
Iteration 117/1000 | Loss: 0.00002583
Iteration 118/1000 | Loss: 0.00002583
Iteration 119/1000 | Loss: 0.00002583
Iteration 120/1000 | Loss: 0.00002583
Iteration 121/1000 | Loss: 0.00002583
Iteration 122/1000 | Loss: 0.00002583
Iteration 123/1000 | Loss: 0.00002583
Iteration 124/1000 | Loss: 0.00002583
Iteration 125/1000 | Loss: 0.00002583
Iteration 126/1000 | Loss: 0.00002583
Iteration 127/1000 | Loss: 0.00002583
Iteration 128/1000 | Loss: 0.00002583
Iteration 129/1000 | Loss: 0.00002582
Iteration 130/1000 | Loss: 0.00002582
Iteration 131/1000 | Loss: 0.00002582
Iteration 132/1000 | Loss: 0.00002582
Iteration 133/1000 | Loss: 0.00002582
Iteration 134/1000 | Loss: 0.00002582
Iteration 135/1000 | Loss: 0.00002582
Iteration 136/1000 | Loss: 0.00002582
Iteration 137/1000 | Loss: 0.00002582
Iteration 138/1000 | Loss: 0.00002582
Iteration 139/1000 | Loss: 0.00002582
Iteration 140/1000 | Loss: 0.00002582
Iteration 141/1000 | Loss: 0.00002582
Iteration 142/1000 | Loss: 0.00002582
Iteration 143/1000 | Loss: 0.00002582
Iteration 144/1000 | Loss: 0.00002582
Iteration 145/1000 | Loss: 0.00002582
Iteration 146/1000 | Loss: 0.00002582
Iteration 147/1000 | Loss: 0.00002582
Iteration 148/1000 | Loss: 0.00002582
Iteration 149/1000 | Loss: 0.00002582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.581820808700286e-05, 2.581820808700286e-05, 2.581820808700286e-05, 2.581820808700286e-05, 2.581820808700286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.581820808700286e-05

Optimization complete. Final v2v error: 4.5630269050598145 mm

Highest mean error: 4.854370594024658 mm for frame 40

Lowest mean error: 4.318136692047119 mm for frame 70

Saving results

Total time: 38.66399335861206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961020
Iteration 2/25 | Loss: 0.00169981
Iteration 3/25 | Loss: 0.00157290
Iteration 4/25 | Loss: 0.00155570
Iteration 5/25 | Loss: 0.00155050
Iteration 6/25 | Loss: 0.00155225
Iteration 7/25 | Loss: 0.00155084
Iteration 8/25 | Loss: 0.00154714
Iteration 9/25 | Loss: 0.00154947
Iteration 10/25 | Loss: 0.00154742
Iteration 11/25 | Loss: 0.00154596
Iteration 12/25 | Loss: 0.00154524
Iteration 13/25 | Loss: 0.00154469
Iteration 14/25 | Loss: 0.00154447
Iteration 15/25 | Loss: 0.00154433
Iteration 16/25 | Loss: 0.00154430
Iteration 17/25 | Loss: 0.00154429
Iteration 18/25 | Loss: 0.00154429
Iteration 19/25 | Loss: 0.00154429
Iteration 20/25 | Loss: 0.00154429
Iteration 21/25 | Loss: 0.00154429
Iteration 22/25 | Loss: 0.00154429
Iteration 23/25 | Loss: 0.00154429
Iteration 24/25 | Loss: 0.00154429
Iteration 25/25 | Loss: 0.00154429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65241659
Iteration 2/25 | Loss: 0.00372957
Iteration 3/25 | Loss: 0.00372956
Iteration 4/25 | Loss: 0.00372956
Iteration 5/25 | Loss: 0.00372956
Iteration 6/25 | Loss: 0.00372956
Iteration 7/25 | Loss: 0.00372956
Iteration 8/25 | Loss: 0.00372956
Iteration 9/25 | Loss: 0.00372956
Iteration 10/25 | Loss: 0.00372956
Iteration 11/25 | Loss: 0.00372956
Iteration 12/25 | Loss: 0.00372956
Iteration 13/25 | Loss: 0.00372956
Iteration 14/25 | Loss: 0.00372956
Iteration 15/25 | Loss: 0.00372956
Iteration 16/25 | Loss: 0.00372956
Iteration 17/25 | Loss: 0.00372956
Iteration 18/25 | Loss: 0.00372956
Iteration 19/25 | Loss: 0.00372956
Iteration 20/25 | Loss: 0.00372956
Iteration 21/25 | Loss: 0.00372956
Iteration 22/25 | Loss: 0.00372956
Iteration 23/25 | Loss: 0.00372956
Iteration 24/25 | Loss: 0.00372956
Iteration 25/25 | Loss: 0.00372956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00372956
Iteration 2/1000 | Loss: 0.00015738
Iteration 3/1000 | Loss: 0.00012426
Iteration 4/1000 | Loss: 0.00010829
Iteration 5/1000 | Loss: 0.00009790
Iteration 6/1000 | Loss: 0.00022857
Iteration 7/1000 | Loss: 0.00032547
Iteration 8/1000 | Loss: 0.00346614
Iteration 9/1000 | Loss: 0.00063875
Iteration 10/1000 | Loss: 0.00061075
Iteration 11/1000 | Loss: 0.00048656
Iteration 12/1000 | Loss: 0.00009875
Iteration 13/1000 | Loss: 0.00008256
Iteration 14/1000 | Loss: 0.00134844
Iteration 15/1000 | Loss: 0.00038230
Iteration 16/1000 | Loss: 0.00131720
Iteration 17/1000 | Loss: 0.00068646
Iteration 18/1000 | Loss: 0.00008507
Iteration 19/1000 | Loss: 0.00055340
Iteration 20/1000 | Loss: 0.00053008
Iteration 21/1000 | Loss: 0.00067131
Iteration 22/1000 | Loss: 0.00122563
Iteration 23/1000 | Loss: 0.00060527
Iteration 24/1000 | Loss: 0.00019065
Iteration 25/1000 | Loss: 0.00019473
Iteration 26/1000 | Loss: 0.00015123
Iteration 27/1000 | Loss: 0.00106559
Iteration 28/1000 | Loss: 0.00139678
Iteration 29/1000 | Loss: 0.00039677
Iteration 30/1000 | Loss: 0.00031414
Iteration 31/1000 | Loss: 0.00030909
Iteration 32/1000 | Loss: 0.00024590
Iteration 33/1000 | Loss: 0.00031692
Iteration 34/1000 | Loss: 0.00026853
Iteration 35/1000 | Loss: 0.00026288
Iteration 36/1000 | Loss: 0.00024532
Iteration 37/1000 | Loss: 0.00021294
Iteration 38/1000 | Loss: 0.00018854
Iteration 39/1000 | Loss: 0.00020413
Iteration 40/1000 | Loss: 0.00048041
Iteration 41/1000 | Loss: 0.00044205
Iteration 42/1000 | Loss: 0.00047600
Iteration 43/1000 | Loss: 0.00022053
Iteration 44/1000 | Loss: 0.00013317
Iteration 45/1000 | Loss: 0.00005756
Iteration 46/1000 | Loss: 0.00065823
Iteration 47/1000 | Loss: 0.00040325
Iteration 48/1000 | Loss: 0.00016372
Iteration 49/1000 | Loss: 0.00021035
Iteration 50/1000 | Loss: 0.00028988
Iteration 51/1000 | Loss: 0.00047562
Iteration 52/1000 | Loss: 0.00031436
Iteration 53/1000 | Loss: 0.00056056
Iteration 54/1000 | Loss: 0.00066468
Iteration 55/1000 | Loss: 0.00006619
Iteration 56/1000 | Loss: 0.00017093
Iteration 57/1000 | Loss: 0.00017451
Iteration 58/1000 | Loss: 0.00017126
Iteration 59/1000 | Loss: 0.00016222
Iteration 60/1000 | Loss: 0.00015493
Iteration 61/1000 | Loss: 0.00015426
Iteration 62/1000 | Loss: 0.00015879
Iteration 63/1000 | Loss: 0.00013969
Iteration 64/1000 | Loss: 0.00015431
Iteration 65/1000 | Loss: 0.00011277
Iteration 66/1000 | Loss: 0.00010489
Iteration 67/1000 | Loss: 0.00005812
Iteration 68/1000 | Loss: 0.00004450
Iteration 69/1000 | Loss: 0.00013883
Iteration 70/1000 | Loss: 0.00026692
Iteration 71/1000 | Loss: 0.00019509
Iteration 72/1000 | Loss: 0.00016155
Iteration 73/1000 | Loss: 0.00021698
Iteration 74/1000 | Loss: 0.00016185
Iteration 75/1000 | Loss: 0.00017795
Iteration 76/1000 | Loss: 0.00004350
Iteration 77/1000 | Loss: 0.00024247
Iteration 78/1000 | Loss: 0.00004171
Iteration 79/1000 | Loss: 0.00003927
Iteration 80/1000 | Loss: 0.00003818
Iteration 81/1000 | Loss: 0.00003753
Iteration 82/1000 | Loss: 0.00003678
Iteration 83/1000 | Loss: 0.00003577
Iteration 84/1000 | Loss: 0.00003493
Iteration 85/1000 | Loss: 0.00003418
Iteration 86/1000 | Loss: 0.00003385
Iteration 87/1000 | Loss: 0.00003355
Iteration 88/1000 | Loss: 0.00003348
Iteration 89/1000 | Loss: 0.00003340
Iteration 90/1000 | Loss: 0.00003335
Iteration 91/1000 | Loss: 0.00003334
Iteration 92/1000 | Loss: 0.00003333
Iteration 93/1000 | Loss: 0.00003329
Iteration 94/1000 | Loss: 0.00003328
Iteration 95/1000 | Loss: 0.00003324
Iteration 96/1000 | Loss: 0.00003324
Iteration 97/1000 | Loss: 0.00003324
Iteration 98/1000 | Loss: 0.00003323
Iteration 99/1000 | Loss: 0.00003321
Iteration 100/1000 | Loss: 0.00003318
Iteration 101/1000 | Loss: 0.00003317
Iteration 102/1000 | Loss: 0.00003315
Iteration 103/1000 | Loss: 0.00003314
Iteration 104/1000 | Loss: 0.00003314
Iteration 105/1000 | Loss: 0.00003314
Iteration 106/1000 | Loss: 0.00003314
Iteration 107/1000 | Loss: 0.00003313
Iteration 108/1000 | Loss: 0.00003313
Iteration 109/1000 | Loss: 0.00003313
Iteration 110/1000 | Loss: 0.00003313
Iteration 111/1000 | Loss: 0.00003312
Iteration 112/1000 | Loss: 0.00003312
Iteration 113/1000 | Loss: 0.00003312
Iteration 114/1000 | Loss: 0.00003312
Iteration 115/1000 | Loss: 0.00003312
Iteration 116/1000 | Loss: 0.00003311
Iteration 117/1000 | Loss: 0.00003311
Iteration 118/1000 | Loss: 0.00003311
Iteration 119/1000 | Loss: 0.00003310
Iteration 120/1000 | Loss: 0.00003310
Iteration 121/1000 | Loss: 0.00003310
Iteration 122/1000 | Loss: 0.00003310
Iteration 123/1000 | Loss: 0.00003309
Iteration 124/1000 | Loss: 0.00003309
Iteration 125/1000 | Loss: 0.00003309
Iteration 126/1000 | Loss: 0.00003309
Iteration 127/1000 | Loss: 0.00003309
Iteration 128/1000 | Loss: 0.00003309
Iteration 129/1000 | Loss: 0.00003309
Iteration 130/1000 | Loss: 0.00003309
Iteration 131/1000 | Loss: 0.00003308
Iteration 132/1000 | Loss: 0.00003308
Iteration 133/1000 | Loss: 0.00003308
Iteration 134/1000 | Loss: 0.00003308
Iteration 135/1000 | Loss: 0.00003307
Iteration 136/1000 | Loss: 0.00003307
Iteration 137/1000 | Loss: 0.00003307
Iteration 138/1000 | Loss: 0.00003307
Iteration 139/1000 | Loss: 0.00003307
Iteration 140/1000 | Loss: 0.00003307
Iteration 141/1000 | Loss: 0.00003307
Iteration 142/1000 | Loss: 0.00003307
Iteration 143/1000 | Loss: 0.00003306
Iteration 144/1000 | Loss: 0.00003306
Iteration 145/1000 | Loss: 0.00003306
Iteration 146/1000 | Loss: 0.00003306
Iteration 147/1000 | Loss: 0.00003305
Iteration 148/1000 | Loss: 0.00003305
Iteration 149/1000 | Loss: 0.00003305
Iteration 150/1000 | Loss: 0.00003305
Iteration 151/1000 | Loss: 0.00003305
Iteration 152/1000 | Loss: 0.00003305
Iteration 153/1000 | Loss: 0.00003304
Iteration 154/1000 | Loss: 0.00003304
Iteration 155/1000 | Loss: 0.00003304
Iteration 156/1000 | Loss: 0.00003304
Iteration 157/1000 | Loss: 0.00003304
Iteration 158/1000 | Loss: 0.00003304
Iteration 159/1000 | Loss: 0.00003303
Iteration 160/1000 | Loss: 0.00003303
Iteration 161/1000 | Loss: 0.00003303
Iteration 162/1000 | Loss: 0.00003303
Iteration 163/1000 | Loss: 0.00003303
Iteration 164/1000 | Loss: 0.00003303
Iteration 165/1000 | Loss: 0.00003303
Iteration 166/1000 | Loss: 0.00003303
Iteration 167/1000 | Loss: 0.00003303
Iteration 168/1000 | Loss: 0.00003303
Iteration 169/1000 | Loss: 0.00003303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.3032920327968895e-05, 3.3032920327968895e-05, 3.3032920327968895e-05, 3.3032920327968895e-05, 3.3032920327968895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3032920327968895e-05

Optimization complete. Final v2v error: 4.652493953704834 mm

Highest mean error: 14.377058029174805 mm for frame 115

Lowest mean error: 4.219720840454102 mm for frame 254

Saving results

Total time: 183.46300530433655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01194195
Iteration 2/25 | Loss: 0.01194195
Iteration 3/25 | Loss: 0.00273966
Iteration 4/25 | Loss: 0.00208080
Iteration 5/25 | Loss: 0.00197424
Iteration 6/25 | Loss: 0.00171305
Iteration 7/25 | Loss: 0.00158969
Iteration 8/25 | Loss: 0.00149820
Iteration 9/25 | Loss: 0.00145117
Iteration 10/25 | Loss: 0.00144046
Iteration 11/25 | Loss: 0.00143736
Iteration 12/25 | Loss: 0.00143592
Iteration 13/25 | Loss: 0.00143552
Iteration 14/25 | Loss: 0.00143541
Iteration 15/25 | Loss: 0.00143533
Iteration 16/25 | Loss: 0.00143524
Iteration 17/25 | Loss: 0.00143512
Iteration 18/25 | Loss: 0.00143998
Iteration 19/25 | Loss: 0.00143225
Iteration 20/25 | Loss: 0.00143044
Iteration 21/25 | Loss: 0.00143002
Iteration 22/25 | Loss: 0.00142980
Iteration 23/25 | Loss: 0.00142969
Iteration 24/25 | Loss: 0.00142969
Iteration 25/25 | Loss: 0.00142968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58135593
Iteration 2/25 | Loss: 0.00276732
Iteration 3/25 | Loss: 0.00276732
Iteration 4/25 | Loss: 0.00276732
Iteration 5/25 | Loss: 0.00276732
Iteration 6/25 | Loss: 0.00276732
Iteration 7/25 | Loss: 0.00276732
Iteration 8/25 | Loss: 0.00276732
Iteration 9/25 | Loss: 0.00276732
Iteration 10/25 | Loss: 0.00276732
Iteration 11/25 | Loss: 0.00276732
Iteration 12/25 | Loss: 0.00276732
Iteration 13/25 | Loss: 0.00276732
Iteration 14/25 | Loss: 0.00276732
Iteration 15/25 | Loss: 0.00276732
Iteration 16/25 | Loss: 0.00276732
Iteration 17/25 | Loss: 0.00276732
Iteration 18/25 | Loss: 0.00276732
Iteration 19/25 | Loss: 0.00276732
Iteration 20/25 | Loss: 0.00276732
Iteration 21/25 | Loss: 0.00276732
Iteration 22/25 | Loss: 0.00276732
Iteration 23/25 | Loss: 0.00276732
Iteration 24/25 | Loss: 0.00276732
Iteration 25/25 | Loss: 0.00276732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276732
Iteration 2/1000 | Loss: 0.00006923
Iteration 3/1000 | Loss: 0.00005141
Iteration 4/1000 | Loss: 0.00004561
Iteration 5/1000 | Loss: 0.00004268
Iteration 6/1000 | Loss: 0.00004113
Iteration 7/1000 | Loss: 0.00004021
Iteration 8/1000 | Loss: 0.00136731
Iteration 9/1000 | Loss: 0.00004496
Iteration 10/1000 | Loss: 0.00004008
Iteration 11/1000 | Loss: 0.00003642
Iteration 12/1000 | Loss: 0.00003507
Iteration 13/1000 | Loss: 0.00003414
Iteration 14/1000 | Loss: 0.00003373
Iteration 15/1000 | Loss: 0.00003358
Iteration 16/1000 | Loss: 0.00003352
Iteration 17/1000 | Loss: 0.00003335
Iteration 18/1000 | Loss: 0.00003325
Iteration 19/1000 | Loss: 0.00003309
Iteration 20/1000 | Loss: 0.00003309
Iteration 21/1000 | Loss: 0.00003306
Iteration 22/1000 | Loss: 0.00003304
Iteration 23/1000 | Loss: 0.00003303
Iteration 24/1000 | Loss: 0.00003302
Iteration 25/1000 | Loss: 0.00003301
Iteration 26/1000 | Loss: 0.00003294
Iteration 27/1000 | Loss: 0.00003294
Iteration 28/1000 | Loss: 0.00003290
Iteration 29/1000 | Loss: 0.00003288
Iteration 30/1000 | Loss: 0.00003288
Iteration 31/1000 | Loss: 0.00003283
Iteration 32/1000 | Loss: 0.00003282
Iteration 33/1000 | Loss: 0.00003282
Iteration 34/1000 | Loss: 0.00003281
Iteration 35/1000 | Loss: 0.00003281
Iteration 36/1000 | Loss: 0.00003281
Iteration 37/1000 | Loss: 0.00003280
Iteration 38/1000 | Loss: 0.00003280
Iteration 39/1000 | Loss: 0.00003279
Iteration 40/1000 | Loss: 0.00003279
Iteration 41/1000 | Loss: 0.00003279
Iteration 42/1000 | Loss: 0.00003279
Iteration 43/1000 | Loss: 0.00003278
Iteration 44/1000 | Loss: 0.00003278
Iteration 45/1000 | Loss: 0.00003278
Iteration 46/1000 | Loss: 0.00003277
Iteration 47/1000 | Loss: 0.00003277
Iteration 48/1000 | Loss: 0.00003277
Iteration 49/1000 | Loss: 0.00003276
Iteration 50/1000 | Loss: 0.00003276
Iteration 51/1000 | Loss: 0.00003276
Iteration 52/1000 | Loss: 0.00003276
Iteration 53/1000 | Loss: 0.00003276
Iteration 54/1000 | Loss: 0.00003276
Iteration 55/1000 | Loss: 0.00003276
Iteration 56/1000 | Loss: 0.00003276
Iteration 57/1000 | Loss: 0.00003276
Iteration 58/1000 | Loss: 0.00003276
Iteration 59/1000 | Loss: 0.00003275
Iteration 60/1000 | Loss: 0.00003275
Iteration 61/1000 | Loss: 0.00003274
Iteration 62/1000 | Loss: 0.00003274
Iteration 63/1000 | Loss: 0.00003274
Iteration 64/1000 | Loss: 0.00003274
Iteration 65/1000 | Loss: 0.00003274
Iteration 66/1000 | Loss: 0.00003274
Iteration 67/1000 | Loss: 0.00003274
Iteration 68/1000 | Loss: 0.00003274
Iteration 69/1000 | Loss: 0.00003274
Iteration 70/1000 | Loss: 0.00003274
Iteration 71/1000 | Loss: 0.00003274
Iteration 72/1000 | Loss: 0.00003273
Iteration 73/1000 | Loss: 0.00003273
Iteration 74/1000 | Loss: 0.00003273
Iteration 75/1000 | Loss: 0.00003273
Iteration 76/1000 | Loss: 0.00003273
Iteration 77/1000 | Loss: 0.00003273
Iteration 78/1000 | Loss: 0.00003272
Iteration 79/1000 | Loss: 0.00003272
Iteration 80/1000 | Loss: 0.00003272
Iteration 81/1000 | Loss: 0.00003272
Iteration 82/1000 | Loss: 0.00003272
Iteration 83/1000 | Loss: 0.00003272
Iteration 84/1000 | Loss: 0.00003272
Iteration 85/1000 | Loss: 0.00003272
Iteration 86/1000 | Loss: 0.00003272
Iteration 87/1000 | Loss: 0.00003272
Iteration 88/1000 | Loss: 0.00003272
Iteration 89/1000 | Loss: 0.00003272
Iteration 90/1000 | Loss: 0.00003272
Iteration 91/1000 | Loss: 0.00003272
Iteration 92/1000 | Loss: 0.00003272
Iteration 93/1000 | Loss: 0.00003272
Iteration 94/1000 | Loss: 0.00003272
Iteration 95/1000 | Loss: 0.00003272
Iteration 96/1000 | Loss: 0.00003272
Iteration 97/1000 | Loss: 0.00003272
Iteration 98/1000 | Loss: 0.00003272
Iteration 99/1000 | Loss: 0.00003272
Iteration 100/1000 | Loss: 0.00003272
Iteration 101/1000 | Loss: 0.00003272
Iteration 102/1000 | Loss: 0.00003272
Iteration 103/1000 | Loss: 0.00003272
Iteration 104/1000 | Loss: 0.00003272
Iteration 105/1000 | Loss: 0.00003272
Iteration 106/1000 | Loss: 0.00003272
Iteration 107/1000 | Loss: 0.00003272
Iteration 108/1000 | Loss: 0.00003272
Iteration 109/1000 | Loss: 0.00003272
Iteration 110/1000 | Loss: 0.00003272
Iteration 111/1000 | Loss: 0.00003272
Iteration 112/1000 | Loss: 0.00003272
Iteration 113/1000 | Loss: 0.00003272
Iteration 114/1000 | Loss: 0.00003272
Iteration 115/1000 | Loss: 0.00003272
Iteration 116/1000 | Loss: 0.00003272
Iteration 117/1000 | Loss: 0.00003272
Iteration 118/1000 | Loss: 0.00003272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.2720872695790604e-05, 3.2720872695790604e-05, 3.2720872695790604e-05, 3.2720872695790604e-05, 3.2720872695790604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2720872695790604e-05

Optimization complete. Final v2v error: 4.8947906494140625 mm

Highest mean error: 10.969696044921875 mm for frame 97

Lowest mean error: 4.459768772125244 mm for frame 47

Saving results

Total time: 70.78732228279114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890526
Iteration 2/25 | Loss: 0.00171506
Iteration 3/25 | Loss: 0.00156833
Iteration 4/25 | Loss: 0.00154774
Iteration 5/25 | Loss: 0.00154016
Iteration 6/25 | Loss: 0.00153769
Iteration 7/25 | Loss: 0.00153764
Iteration 8/25 | Loss: 0.00153764
Iteration 9/25 | Loss: 0.00153764
Iteration 10/25 | Loss: 0.00153764
Iteration 11/25 | Loss: 0.00153764
Iteration 12/25 | Loss: 0.00153764
Iteration 13/25 | Loss: 0.00153764
Iteration 14/25 | Loss: 0.00153764
Iteration 15/25 | Loss: 0.00153764
Iteration 16/25 | Loss: 0.00153764
Iteration 17/25 | Loss: 0.00153764
Iteration 18/25 | Loss: 0.00153764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015376415103673935, 0.0015376415103673935, 0.0015376415103673935, 0.0015376415103673935, 0.0015376415103673935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015376415103673935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60479605
Iteration 2/25 | Loss: 0.00363490
Iteration 3/25 | Loss: 0.00363489
Iteration 4/25 | Loss: 0.00363489
Iteration 5/25 | Loss: 0.00363489
Iteration 6/25 | Loss: 0.00363489
Iteration 7/25 | Loss: 0.00363489
Iteration 8/25 | Loss: 0.00363489
Iteration 9/25 | Loss: 0.00363489
Iteration 10/25 | Loss: 0.00363489
Iteration 11/25 | Loss: 0.00363489
Iteration 12/25 | Loss: 0.00363489
Iteration 13/25 | Loss: 0.00363489
Iteration 14/25 | Loss: 0.00363489
Iteration 15/25 | Loss: 0.00363489
Iteration 16/25 | Loss: 0.00363489
Iteration 17/25 | Loss: 0.00363489
Iteration 18/25 | Loss: 0.00363489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0036348889116197824, 0.0036348889116197824, 0.0036348889116197824, 0.0036348889116197824, 0.0036348889116197824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036348889116197824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00363489
Iteration 2/1000 | Loss: 0.00006497
Iteration 3/1000 | Loss: 0.00004532
Iteration 4/1000 | Loss: 0.00003936
Iteration 5/1000 | Loss: 0.00003597
Iteration 6/1000 | Loss: 0.00003392
Iteration 7/1000 | Loss: 0.00003290
Iteration 8/1000 | Loss: 0.00003235
Iteration 9/1000 | Loss: 0.00003198
Iteration 10/1000 | Loss: 0.00003163
Iteration 11/1000 | Loss: 0.00003134
Iteration 12/1000 | Loss: 0.00003109
Iteration 13/1000 | Loss: 0.00003094
Iteration 14/1000 | Loss: 0.00003088
Iteration 15/1000 | Loss: 0.00003087
Iteration 16/1000 | Loss: 0.00003087
Iteration 17/1000 | Loss: 0.00003085
Iteration 18/1000 | Loss: 0.00003083
Iteration 19/1000 | Loss: 0.00003082
Iteration 20/1000 | Loss: 0.00003081
Iteration 21/1000 | Loss: 0.00003080
Iteration 22/1000 | Loss: 0.00003080
Iteration 23/1000 | Loss: 0.00003075
Iteration 24/1000 | Loss: 0.00003067
Iteration 25/1000 | Loss: 0.00003064
Iteration 26/1000 | Loss: 0.00003061
Iteration 27/1000 | Loss: 0.00003061
Iteration 28/1000 | Loss: 0.00003060
Iteration 29/1000 | Loss: 0.00003060
Iteration 30/1000 | Loss: 0.00003057
Iteration 31/1000 | Loss: 0.00003057
Iteration 32/1000 | Loss: 0.00003056
Iteration 33/1000 | Loss: 0.00003056
Iteration 34/1000 | Loss: 0.00003055
Iteration 35/1000 | Loss: 0.00003055
Iteration 36/1000 | Loss: 0.00003055
Iteration 37/1000 | Loss: 0.00003054
Iteration 38/1000 | Loss: 0.00003053
Iteration 39/1000 | Loss: 0.00003053
Iteration 40/1000 | Loss: 0.00003052
Iteration 41/1000 | Loss: 0.00003052
Iteration 42/1000 | Loss: 0.00003052
Iteration 43/1000 | Loss: 0.00003052
Iteration 44/1000 | Loss: 0.00003051
Iteration 45/1000 | Loss: 0.00003051
Iteration 46/1000 | Loss: 0.00003051
Iteration 47/1000 | Loss: 0.00003050
Iteration 48/1000 | Loss: 0.00003050
Iteration 49/1000 | Loss: 0.00003050
Iteration 50/1000 | Loss: 0.00003050
Iteration 51/1000 | Loss: 0.00003050
Iteration 52/1000 | Loss: 0.00003050
Iteration 53/1000 | Loss: 0.00003049
Iteration 54/1000 | Loss: 0.00003049
Iteration 55/1000 | Loss: 0.00003049
Iteration 56/1000 | Loss: 0.00003049
Iteration 57/1000 | Loss: 0.00003049
Iteration 58/1000 | Loss: 0.00003049
Iteration 59/1000 | Loss: 0.00003049
Iteration 60/1000 | Loss: 0.00003049
Iteration 61/1000 | Loss: 0.00003049
Iteration 62/1000 | Loss: 0.00003049
Iteration 63/1000 | Loss: 0.00003048
Iteration 64/1000 | Loss: 0.00003048
Iteration 65/1000 | Loss: 0.00003048
Iteration 66/1000 | Loss: 0.00003048
Iteration 67/1000 | Loss: 0.00003048
Iteration 68/1000 | Loss: 0.00003048
Iteration 69/1000 | Loss: 0.00003047
Iteration 70/1000 | Loss: 0.00003047
Iteration 71/1000 | Loss: 0.00003047
Iteration 72/1000 | Loss: 0.00003047
Iteration 73/1000 | Loss: 0.00003047
Iteration 74/1000 | Loss: 0.00003046
Iteration 75/1000 | Loss: 0.00003046
Iteration 76/1000 | Loss: 0.00003046
Iteration 77/1000 | Loss: 0.00003046
Iteration 78/1000 | Loss: 0.00003046
Iteration 79/1000 | Loss: 0.00003045
Iteration 80/1000 | Loss: 0.00003045
Iteration 81/1000 | Loss: 0.00003045
Iteration 82/1000 | Loss: 0.00003045
Iteration 83/1000 | Loss: 0.00003045
Iteration 84/1000 | Loss: 0.00003045
Iteration 85/1000 | Loss: 0.00003045
Iteration 86/1000 | Loss: 0.00003045
Iteration 87/1000 | Loss: 0.00003045
Iteration 88/1000 | Loss: 0.00003044
Iteration 89/1000 | Loss: 0.00003044
Iteration 90/1000 | Loss: 0.00003044
Iteration 91/1000 | Loss: 0.00003044
Iteration 92/1000 | Loss: 0.00003044
Iteration 93/1000 | Loss: 0.00003044
Iteration 94/1000 | Loss: 0.00003044
Iteration 95/1000 | Loss: 0.00003044
Iteration 96/1000 | Loss: 0.00003044
Iteration 97/1000 | Loss: 0.00003044
Iteration 98/1000 | Loss: 0.00003044
Iteration 99/1000 | Loss: 0.00003043
Iteration 100/1000 | Loss: 0.00003043
Iteration 101/1000 | Loss: 0.00003043
Iteration 102/1000 | Loss: 0.00003043
Iteration 103/1000 | Loss: 0.00003043
Iteration 104/1000 | Loss: 0.00003043
Iteration 105/1000 | Loss: 0.00003042
Iteration 106/1000 | Loss: 0.00003042
Iteration 107/1000 | Loss: 0.00003042
Iteration 108/1000 | Loss: 0.00003042
Iteration 109/1000 | Loss: 0.00003042
Iteration 110/1000 | Loss: 0.00003041
Iteration 111/1000 | Loss: 0.00003041
Iteration 112/1000 | Loss: 0.00003041
Iteration 113/1000 | Loss: 0.00003041
Iteration 114/1000 | Loss: 0.00003041
Iteration 115/1000 | Loss: 0.00003041
Iteration 116/1000 | Loss: 0.00003041
Iteration 117/1000 | Loss: 0.00003041
Iteration 118/1000 | Loss: 0.00003041
Iteration 119/1000 | Loss: 0.00003041
Iteration 120/1000 | Loss: 0.00003040
Iteration 121/1000 | Loss: 0.00003040
Iteration 122/1000 | Loss: 0.00003040
Iteration 123/1000 | Loss: 0.00003040
Iteration 124/1000 | Loss: 0.00003040
Iteration 125/1000 | Loss: 0.00003040
Iteration 126/1000 | Loss: 0.00003040
Iteration 127/1000 | Loss: 0.00003040
Iteration 128/1000 | Loss: 0.00003040
Iteration 129/1000 | Loss: 0.00003040
Iteration 130/1000 | Loss: 0.00003040
Iteration 131/1000 | Loss: 0.00003039
Iteration 132/1000 | Loss: 0.00003039
Iteration 133/1000 | Loss: 0.00003039
Iteration 134/1000 | Loss: 0.00003039
Iteration 135/1000 | Loss: 0.00003038
Iteration 136/1000 | Loss: 0.00003038
Iteration 137/1000 | Loss: 0.00003038
Iteration 138/1000 | Loss: 0.00003038
Iteration 139/1000 | Loss: 0.00003038
Iteration 140/1000 | Loss: 0.00003038
Iteration 141/1000 | Loss: 0.00003038
Iteration 142/1000 | Loss: 0.00003038
Iteration 143/1000 | Loss: 0.00003038
Iteration 144/1000 | Loss: 0.00003038
Iteration 145/1000 | Loss: 0.00003038
Iteration 146/1000 | Loss: 0.00003038
Iteration 147/1000 | Loss: 0.00003038
Iteration 148/1000 | Loss: 0.00003038
Iteration 149/1000 | Loss: 0.00003038
Iteration 150/1000 | Loss: 0.00003038
Iteration 151/1000 | Loss: 0.00003038
Iteration 152/1000 | Loss: 0.00003038
Iteration 153/1000 | Loss: 0.00003038
Iteration 154/1000 | Loss: 0.00003038
Iteration 155/1000 | Loss: 0.00003038
Iteration 156/1000 | Loss: 0.00003038
Iteration 157/1000 | Loss: 0.00003038
Iteration 158/1000 | Loss: 0.00003038
Iteration 159/1000 | Loss: 0.00003038
Iteration 160/1000 | Loss: 0.00003038
Iteration 161/1000 | Loss: 0.00003038
Iteration 162/1000 | Loss: 0.00003038
Iteration 163/1000 | Loss: 0.00003038
Iteration 164/1000 | Loss: 0.00003038
Iteration 165/1000 | Loss: 0.00003038
Iteration 166/1000 | Loss: 0.00003038
Iteration 167/1000 | Loss: 0.00003038
Iteration 168/1000 | Loss: 0.00003038
Iteration 169/1000 | Loss: 0.00003038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.0375204005395062e-05, 3.0375204005395062e-05, 3.0375204005395062e-05, 3.0375204005395062e-05, 3.0375204005395062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0375204005395062e-05

Optimization complete. Final v2v error: 4.81909704208374 mm

Highest mean error: 5.237032413482666 mm for frame 188

Lowest mean error: 4.494289398193359 mm for frame 163

Saving results

Total time: 45.104135274887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474602
Iteration 2/25 | Loss: 0.00162484
Iteration 3/25 | Loss: 0.00155360
Iteration 4/25 | Loss: 0.00153933
Iteration 5/25 | Loss: 0.00153524
Iteration 6/25 | Loss: 0.00153417
Iteration 7/25 | Loss: 0.00153417
Iteration 8/25 | Loss: 0.00153417
Iteration 9/25 | Loss: 0.00153417
Iteration 10/25 | Loss: 0.00153417
Iteration 11/25 | Loss: 0.00153417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001534170238301158, 0.001534170238301158, 0.001534170238301158, 0.001534170238301158, 0.001534170238301158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001534170238301158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02064347
Iteration 2/25 | Loss: 0.00322155
Iteration 3/25 | Loss: 0.00322155
Iteration 4/25 | Loss: 0.00322155
Iteration 5/25 | Loss: 0.00322155
Iteration 6/25 | Loss: 0.00322155
Iteration 7/25 | Loss: 0.00322155
Iteration 8/25 | Loss: 0.00322155
Iteration 9/25 | Loss: 0.00322155
Iteration 10/25 | Loss: 0.00322155
Iteration 11/25 | Loss: 0.00322155
Iteration 12/25 | Loss: 0.00322155
Iteration 13/25 | Loss: 0.00322155
Iteration 14/25 | Loss: 0.00322155
Iteration 15/25 | Loss: 0.00322155
Iteration 16/25 | Loss: 0.00322155
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003221548395231366, 0.003221548395231366, 0.003221548395231366, 0.003221548395231366, 0.003221548395231366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003221548395231366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322155
Iteration 2/1000 | Loss: 0.00004577
Iteration 3/1000 | Loss: 0.00003616
Iteration 4/1000 | Loss: 0.00003274
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00002994
Iteration 7/1000 | Loss: 0.00002953
Iteration 8/1000 | Loss: 0.00002916
Iteration 9/1000 | Loss: 0.00002889
Iteration 10/1000 | Loss: 0.00002868
Iteration 11/1000 | Loss: 0.00002868
Iteration 12/1000 | Loss: 0.00002866
Iteration 13/1000 | Loss: 0.00002862
Iteration 14/1000 | Loss: 0.00002858
Iteration 15/1000 | Loss: 0.00002854
Iteration 16/1000 | Loss: 0.00002854
Iteration 17/1000 | Loss: 0.00002851
Iteration 18/1000 | Loss: 0.00002849
Iteration 19/1000 | Loss: 0.00002849
Iteration 20/1000 | Loss: 0.00002849
Iteration 21/1000 | Loss: 0.00002849
Iteration 22/1000 | Loss: 0.00002848
Iteration 23/1000 | Loss: 0.00002848
Iteration 24/1000 | Loss: 0.00002846
Iteration 25/1000 | Loss: 0.00002845
Iteration 26/1000 | Loss: 0.00002845
Iteration 27/1000 | Loss: 0.00002845
Iteration 28/1000 | Loss: 0.00002845
Iteration 29/1000 | Loss: 0.00002845
Iteration 30/1000 | Loss: 0.00002845
Iteration 31/1000 | Loss: 0.00002845
Iteration 32/1000 | Loss: 0.00002845
Iteration 33/1000 | Loss: 0.00002845
Iteration 34/1000 | Loss: 0.00002845
Iteration 35/1000 | Loss: 0.00002844
Iteration 36/1000 | Loss: 0.00002844
Iteration 37/1000 | Loss: 0.00002844
Iteration 38/1000 | Loss: 0.00002844
Iteration 39/1000 | Loss: 0.00002844
Iteration 40/1000 | Loss: 0.00002844
Iteration 41/1000 | Loss: 0.00002844
Iteration 42/1000 | Loss: 0.00002844
Iteration 43/1000 | Loss: 0.00002844
Iteration 44/1000 | Loss: 0.00002843
Iteration 45/1000 | Loss: 0.00002843
Iteration 46/1000 | Loss: 0.00002843
Iteration 47/1000 | Loss: 0.00002843
Iteration 48/1000 | Loss: 0.00002843
Iteration 49/1000 | Loss: 0.00002843
Iteration 50/1000 | Loss: 0.00002842
Iteration 51/1000 | Loss: 0.00002842
Iteration 52/1000 | Loss: 0.00002842
Iteration 53/1000 | Loss: 0.00002842
Iteration 54/1000 | Loss: 0.00002841
Iteration 55/1000 | Loss: 0.00002841
Iteration 56/1000 | Loss: 0.00002841
Iteration 57/1000 | Loss: 0.00002841
Iteration 58/1000 | Loss: 0.00002841
Iteration 59/1000 | Loss: 0.00002840
Iteration 60/1000 | Loss: 0.00002840
Iteration 61/1000 | Loss: 0.00002839
Iteration 62/1000 | Loss: 0.00002839
Iteration 63/1000 | Loss: 0.00002839
Iteration 64/1000 | Loss: 0.00002839
Iteration 65/1000 | Loss: 0.00002839
Iteration 66/1000 | Loss: 0.00002839
Iteration 67/1000 | Loss: 0.00002839
Iteration 68/1000 | Loss: 0.00002839
Iteration 69/1000 | Loss: 0.00002839
Iteration 70/1000 | Loss: 0.00002839
Iteration 71/1000 | Loss: 0.00002839
Iteration 72/1000 | Loss: 0.00002839
Iteration 73/1000 | Loss: 0.00002838
Iteration 74/1000 | Loss: 0.00002838
Iteration 75/1000 | Loss: 0.00002838
Iteration 76/1000 | Loss: 0.00002837
Iteration 77/1000 | Loss: 0.00002837
Iteration 78/1000 | Loss: 0.00002837
Iteration 79/1000 | Loss: 0.00002837
Iteration 80/1000 | Loss: 0.00002837
Iteration 81/1000 | Loss: 0.00002837
Iteration 82/1000 | Loss: 0.00002837
Iteration 83/1000 | Loss: 0.00002836
Iteration 84/1000 | Loss: 0.00002836
Iteration 85/1000 | Loss: 0.00002835
Iteration 86/1000 | Loss: 0.00002835
Iteration 87/1000 | Loss: 0.00002835
Iteration 88/1000 | Loss: 0.00002835
Iteration 89/1000 | Loss: 0.00002835
Iteration 90/1000 | Loss: 0.00002835
Iteration 91/1000 | Loss: 0.00002834
Iteration 92/1000 | Loss: 0.00002834
Iteration 93/1000 | Loss: 0.00002833
Iteration 94/1000 | Loss: 0.00002833
Iteration 95/1000 | Loss: 0.00002833
Iteration 96/1000 | Loss: 0.00002833
Iteration 97/1000 | Loss: 0.00002832
Iteration 98/1000 | Loss: 0.00002832
Iteration 99/1000 | Loss: 0.00002832
Iteration 100/1000 | Loss: 0.00002832
Iteration 101/1000 | Loss: 0.00002832
Iteration 102/1000 | Loss: 0.00002831
Iteration 103/1000 | Loss: 0.00002831
Iteration 104/1000 | Loss: 0.00002831
Iteration 105/1000 | Loss: 0.00002830
Iteration 106/1000 | Loss: 0.00002830
Iteration 107/1000 | Loss: 0.00002830
Iteration 108/1000 | Loss: 0.00002830
Iteration 109/1000 | Loss: 0.00002830
Iteration 110/1000 | Loss: 0.00002830
Iteration 111/1000 | Loss: 0.00002830
Iteration 112/1000 | Loss: 0.00002830
Iteration 113/1000 | Loss: 0.00002830
Iteration 114/1000 | Loss: 0.00002830
Iteration 115/1000 | Loss: 0.00002830
Iteration 116/1000 | Loss: 0.00002830
Iteration 117/1000 | Loss: 0.00002829
Iteration 118/1000 | Loss: 0.00002829
Iteration 119/1000 | Loss: 0.00002829
Iteration 120/1000 | Loss: 0.00002829
Iteration 121/1000 | Loss: 0.00002829
Iteration 122/1000 | Loss: 0.00002829
Iteration 123/1000 | Loss: 0.00002829
Iteration 124/1000 | Loss: 0.00002829
Iteration 125/1000 | Loss: 0.00002829
Iteration 126/1000 | Loss: 0.00002829
Iteration 127/1000 | Loss: 0.00002829
Iteration 128/1000 | Loss: 0.00002829
Iteration 129/1000 | Loss: 0.00002829
Iteration 130/1000 | Loss: 0.00002829
Iteration 131/1000 | Loss: 0.00002829
Iteration 132/1000 | Loss: 0.00002829
Iteration 133/1000 | Loss: 0.00002829
Iteration 134/1000 | Loss: 0.00002829
Iteration 135/1000 | Loss: 0.00002829
Iteration 136/1000 | Loss: 0.00002829
Iteration 137/1000 | Loss: 0.00002829
Iteration 138/1000 | Loss: 0.00002829
Iteration 139/1000 | Loss: 0.00002829
Iteration 140/1000 | Loss: 0.00002828
Iteration 141/1000 | Loss: 0.00002828
Iteration 142/1000 | Loss: 0.00002828
Iteration 143/1000 | Loss: 0.00002828
Iteration 144/1000 | Loss: 0.00002828
Iteration 145/1000 | Loss: 0.00002828
Iteration 146/1000 | Loss: 0.00002828
Iteration 147/1000 | Loss: 0.00002828
Iteration 148/1000 | Loss: 0.00002828
Iteration 149/1000 | Loss: 0.00002828
Iteration 150/1000 | Loss: 0.00002828
Iteration 151/1000 | Loss: 0.00002828
Iteration 152/1000 | Loss: 0.00002828
Iteration 153/1000 | Loss: 0.00002828
Iteration 154/1000 | Loss: 0.00002828
Iteration 155/1000 | Loss: 0.00002828
Iteration 156/1000 | Loss: 0.00002828
Iteration 157/1000 | Loss: 0.00002828
Iteration 158/1000 | Loss: 0.00002828
Iteration 159/1000 | Loss: 0.00002828
Iteration 160/1000 | Loss: 0.00002828
Iteration 161/1000 | Loss: 0.00002828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.8282758648856543e-05, 2.8282758648856543e-05, 2.8282758648856543e-05, 2.8282758648856543e-05, 2.8282758648856543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8282758648856543e-05

Optimization complete. Final v2v error: 4.655220031738281 mm

Highest mean error: 4.925259113311768 mm for frame 84

Lowest mean error: 4.412642955780029 mm for frame 181

Saving results

Total time: 34.52116775512695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897067
Iteration 2/25 | Loss: 0.00179943
Iteration 3/25 | Loss: 0.00153479
Iteration 4/25 | Loss: 0.00150132
Iteration 5/25 | Loss: 0.00149167
Iteration 6/25 | Loss: 0.00148556
Iteration 7/25 | Loss: 0.00148161
Iteration 8/25 | Loss: 0.00148566
Iteration 9/25 | Loss: 0.00148165
Iteration 10/25 | Loss: 0.00148229
Iteration 11/25 | Loss: 0.00148421
Iteration 12/25 | Loss: 0.00148307
Iteration 13/25 | Loss: 0.00148488
Iteration 14/25 | Loss: 0.00148323
Iteration 15/25 | Loss: 0.00148145
Iteration 16/25 | Loss: 0.00148403
Iteration 17/25 | Loss: 0.00148311
Iteration 18/25 | Loss: 0.00148347
Iteration 19/25 | Loss: 0.00148082
Iteration 20/25 | Loss: 0.00148081
Iteration 21/25 | Loss: 0.00148081
Iteration 22/25 | Loss: 0.00148389
Iteration 23/25 | Loss: 0.00148389
Iteration 24/25 | Loss: 0.00148389
Iteration 25/25 | Loss: 0.00148389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44260216
Iteration 2/25 | Loss: 0.00331252
Iteration 3/25 | Loss: 0.00331252
Iteration 4/25 | Loss: 0.00331251
Iteration 5/25 | Loss: 0.00331252
Iteration 6/25 | Loss: 0.00331251
Iteration 7/25 | Loss: 0.00331251
Iteration 8/25 | Loss: 0.00331251
Iteration 9/25 | Loss: 0.00331251
Iteration 10/25 | Loss: 0.00331251
Iteration 11/25 | Loss: 0.00331251
Iteration 12/25 | Loss: 0.00331251
Iteration 13/25 | Loss: 0.00331251
Iteration 14/25 | Loss: 0.00331251
Iteration 15/25 | Loss: 0.00331251
Iteration 16/25 | Loss: 0.00331251
Iteration 17/25 | Loss: 0.00331251
Iteration 18/25 | Loss: 0.00331251
Iteration 19/25 | Loss: 0.00331251
Iteration 20/25 | Loss: 0.00331251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003312513930723071, 0.003312513930723071, 0.003312513930723071, 0.003312513930723071, 0.003312513930723071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003312513930723071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00331251
Iteration 2/1000 | Loss: 0.00016239
Iteration 3/1000 | Loss: 0.00025022
Iteration 4/1000 | Loss: 0.00020325
Iteration 5/1000 | Loss: 0.00005243
Iteration 6/1000 | Loss: 0.00005422
Iteration 7/1000 | Loss: 0.00004607
Iteration 8/1000 | Loss: 0.00004445
Iteration 9/1000 | Loss: 0.00004359
Iteration 10/1000 | Loss: 0.00004677
Iteration 11/1000 | Loss: 0.00004121
Iteration 12/1000 | Loss: 0.00004763
Iteration 13/1000 | Loss: 0.00004242
Iteration 14/1000 | Loss: 0.00005195
Iteration 15/1000 | Loss: 0.00004182
Iteration 16/1000 | Loss: 0.00004955
Iteration 17/1000 | Loss: 0.00004096
Iteration 18/1000 | Loss: 0.00005097
Iteration 19/1000 | Loss: 0.00004217
Iteration 20/1000 | Loss: 0.00005259
Iteration 21/1000 | Loss: 0.00004130
Iteration 22/1000 | Loss: 0.00004557
Iteration 23/1000 | Loss: 0.00003523
Iteration 24/1000 | Loss: 0.00004407
Iteration 25/1000 | Loss: 0.00004403
Iteration 26/1000 | Loss: 0.00004806
Iteration 27/1000 | Loss: 0.00004795
Iteration 28/1000 | Loss: 0.00004785
Iteration 29/1000 | Loss: 0.00004371
Iteration 30/1000 | Loss: 0.00005369
Iteration 31/1000 | Loss: 0.00004152
Iteration 32/1000 | Loss: 0.00006219
Iteration 33/1000 | Loss: 0.00004155
Iteration 34/1000 | Loss: 0.00004846
Iteration 35/1000 | Loss: 0.00004059
Iteration 36/1000 | Loss: 0.00005401
Iteration 37/1000 | Loss: 0.00004583
Iteration 38/1000 | Loss: 0.00004604
Iteration 39/1000 | Loss: 0.00004438
Iteration 40/1000 | Loss: 0.00004449
Iteration 41/1000 | Loss: 0.00003902
Iteration 42/1000 | Loss: 0.00005249
Iteration 43/1000 | Loss: 0.00003441
Iteration 44/1000 | Loss: 0.00004032
Iteration 45/1000 | Loss: 0.00003148
Iteration 46/1000 | Loss: 0.00004235
Iteration 47/1000 | Loss: 0.00003771
Iteration 48/1000 | Loss: 0.00004117
Iteration 49/1000 | Loss: 0.00005431
Iteration 50/1000 | Loss: 0.00005046
Iteration 51/1000 | Loss: 0.00004554
Iteration 52/1000 | Loss: 0.00004675
Iteration 53/1000 | Loss: 0.00003864
Iteration 54/1000 | Loss: 0.00004364
Iteration 55/1000 | Loss: 0.00005545
Iteration 56/1000 | Loss: 0.00004633
Iteration 57/1000 | Loss: 0.00005236
Iteration 58/1000 | Loss: 0.00003471
Iteration 59/1000 | Loss: 0.00004829
Iteration 60/1000 | Loss: 0.00004827
Iteration 61/1000 | Loss: 0.00004477
Iteration 62/1000 | Loss: 0.00004738
Iteration 63/1000 | Loss: 0.00004679
Iteration 64/1000 | Loss: 0.00004594
Iteration 65/1000 | Loss: 0.00004515
Iteration 66/1000 | Loss: 0.00003096
Iteration 67/1000 | Loss: 0.00003377
Iteration 68/1000 | Loss: 0.00005087
Iteration 69/1000 | Loss: 0.00004445
Iteration 70/1000 | Loss: 0.00004520
Iteration 71/1000 | Loss: 0.00004149
Iteration 72/1000 | Loss: 0.00004705
Iteration 73/1000 | Loss: 0.00003329
Iteration 74/1000 | Loss: 0.00004778
Iteration 75/1000 | Loss: 0.00004478
Iteration 76/1000 | Loss: 0.00004821
Iteration 77/1000 | Loss: 0.00004391
Iteration 78/1000 | Loss: 0.00003963
Iteration 79/1000 | Loss: 0.00003380
Iteration 80/1000 | Loss: 0.00004770
Iteration 81/1000 | Loss: 0.00004447
Iteration 82/1000 | Loss: 0.00005054
Iteration 83/1000 | Loss: 0.00004251
Iteration 84/1000 | Loss: 0.00004279
Iteration 85/1000 | Loss: 0.00003637
Iteration 86/1000 | Loss: 0.00005027
Iteration 87/1000 | Loss: 0.00004260
Iteration 88/1000 | Loss: 0.00004706
Iteration 89/1000 | Loss: 0.00004532
Iteration 90/1000 | Loss: 0.00004546
Iteration 91/1000 | Loss: 0.00006456
Iteration 92/1000 | Loss: 0.00004255
Iteration 93/1000 | Loss: 0.00004255
Iteration 94/1000 | Loss: 0.00004550
Iteration 95/1000 | Loss: 0.00004188
Iteration 96/1000 | Loss: 0.00004494
Iteration 97/1000 | Loss: 0.00004073
Iteration 98/1000 | Loss: 0.00004345
Iteration 99/1000 | Loss: 0.00004501
Iteration 100/1000 | Loss: 0.00004545
Iteration 101/1000 | Loss: 0.00005298
Iteration 102/1000 | Loss: 0.00003107
Iteration 103/1000 | Loss: 0.00002966
Iteration 104/1000 | Loss: 0.00002901
Iteration 105/1000 | Loss: 0.00002877
Iteration 106/1000 | Loss: 0.00002875
Iteration 107/1000 | Loss: 0.00002873
Iteration 108/1000 | Loss: 0.00002873
Iteration 109/1000 | Loss: 0.00002872
Iteration 110/1000 | Loss: 0.00002872
Iteration 111/1000 | Loss: 0.00002871
Iteration 112/1000 | Loss: 0.00002870
Iteration 113/1000 | Loss: 0.00002869
Iteration 114/1000 | Loss: 0.00002859
Iteration 115/1000 | Loss: 0.00002844
Iteration 116/1000 | Loss: 0.00002840
Iteration 117/1000 | Loss: 0.00002840
Iteration 118/1000 | Loss: 0.00002838
Iteration 119/1000 | Loss: 0.00002837
Iteration 120/1000 | Loss: 0.00002836
Iteration 121/1000 | Loss: 0.00002836
Iteration 122/1000 | Loss: 0.00002835
Iteration 123/1000 | Loss: 0.00002835
Iteration 124/1000 | Loss: 0.00002835
Iteration 125/1000 | Loss: 0.00002834
Iteration 126/1000 | Loss: 0.00002834
Iteration 127/1000 | Loss: 0.00002834
Iteration 128/1000 | Loss: 0.00002833
Iteration 129/1000 | Loss: 0.00002833
Iteration 130/1000 | Loss: 0.00002833
Iteration 131/1000 | Loss: 0.00002832
Iteration 132/1000 | Loss: 0.00002832
Iteration 133/1000 | Loss: 0.00002832
Iteration 134/1000 | Loss: 0.00002830
Iteration 135/1000 | Loss: 0.00002830
Iteration 136/1000 | Loss: 0.00002830
Iteration 137/1000 | Loss: 0.00002829
Iteration 138/1000 | Loss: 0.00002827
Iteration 139/1000 | Loss: 0.00002827
Iteration 140/1000 | Loss: 0.00002825
Iteration 141/1000 | Loss: 0.00002825
Iteration 142/1000 | Loss: 0.00002825
Iteration 143/1000 | Loss: 0.00002824
Iteration 144/1000 | Loss: 0.00002824
Iteration 145/1000 | Loss: 0.00002824
Iteration 146/1000 | Loss: 0.00002823
Iteration 147/1000 | Loss: 0.00002823
Iteration 148/1000 | Loss: 0.00002823
Iteration 149/1000 | Loss: 0.00002823
Iteration 150/1000 | Loss: 0.00002823
Iteration 151/1000 | Loss: 0.00002822
Iteration 152/1000 | Loss: 0.00002822
Iteration 153/1000 | Loss: 0.00002822
Iteration 154/1000 | Loss: 0.00002821
Iteration 155/1000 | Loss: 0.00002821
Iteration 156/1000 | Loss: 0.00002821
Iteration 157/1000 | Loss: 0.00002821
Iteration 158/1000 | Loss: 0.00002821
Iteration 159/1000 | Loss: 0.00002820
Iteration 160/1000 | Loss: 0.00002820
Iteration 161/1000 | Loss: 0.00002820
Iteration 162/1000 | Loss: 0.00002820
Iteration 163/1000 | Loss: 0.00002819
Iteration 164/1000 | Loss: 0.00002819
Iteration 165/1000 | Loss: 0.00002819
Iteration 166/1000 | Loss: 0.00002819
Iteration 167/1000 | Loss: 0.00002818
Iteration 168/1000 | Loss: 0.00002818
Iteration 169/1000 | Loss: 0.00002818
Iteration 170/1000 | Loss: 0.00002818
Iteration 171/1000 | Loss: 0.00002817
Iteration 172/1000 | Loss: 0.00002817
Iteration 173/1000 | Loss: 0.00002817
Iteration 174/1000 | Loss: 0.00002817
Iteration 175/1000 | Loss: 0.00002817
Iteration 176/1000 | Loss: 0.00002817
Iteration 177/1000 | Loss: 0.00002817
Iteration 178/1000 | Loss: 0.00002817
Iteration 179/1000 | Loss: 0.00002817
Iteration 180/1000 | Loss: 0.00002816
Iteration 181/1000 | Loss: 0.00002816
Iteration 182/1000 | Loss: 0.00002816
Iteration 183/1000 | Loss: 0.00002816
Iteration 184/1000 | Loss: 0.00002816
Iteration 185/1000 | Loss: 0.00002816
Iteration 186/1000 | Loss: 0.00002815
Iteration 187/1000 | Loss: 0.00002815
Iteration 188/1000 | Loss: 0.00002815
Iteration 189/1000 | Loss: 0.00002815
Iteration 190/1000 | Loss: 0.00002815
Iteration 191/1000 | Loss: 0.00002815
Iteration 192/1000 | Loss: 0.00002814
Iteration 193/1000 | Loss: 0.00002814
Iteration 194/1000 | Loss: 0.00002814
Iteration 195/1000 | Loss: 0.00002814
Iteration 196/1000 | Loss: 0.00002813
Iteration 197/1000 | Loss: 0.00002813
Iteration 198/1000 | Loss: 0.00002813
Iteration 199/1000 | Loss: 0.00002813
Iteration 200/1000 | Loss: 0.00002813
Iteration 201/1000 | Loss: 0.00002813
Iteration 202/1000 | Loss: 0.00002813
Iteration 203/1000 | Loss: 0.00002813
Iteration 204/1000 | Loss: 0.00002813
Iteration 205/1000 | Loss: 0.00002813
Iteration 206/1000 | Loss: 0.00002813
Iteration 207/1000 | Loss: 0.00002813
Iteration 208/1000 | Loss: 0.00002813
Iteration 209/1000 | Loss: 0.00002813
Iteration 210/1000 | Loss: 0.00002813
Iteration 211/1000 | Loss: 0.00002813
Iteration 212/1000 | Loss: 0.00002813
Iteration 213/1000 | Loss: 0.00002813
Iteration 214/1000 | Loss: 0.00002813
Iteration 215/1000 | Loss: 0.00002813
Iteration 216/1000 | Loss: 0.00002813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.8131938961450942e-05, 2.8131938961450942e-05, 2.8131938961450942e-05, 2.8131938961450942e-05, 2.8131938961450942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8131938961450942e-05

Optimization complete. Final v2v error: 4.565679550170898 mm

Highest mean error: 11.660604476928711 mm for frame 43

Lowest mean error: 4.110628128051758 mm for frame 25

Saving results

Total time: 219.62294912338257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156199
Iteration 2/25 | Loss: 0.00227023
Iteration 3/25 | Loss: 0.00190101
Iteration 4/25 | Loss: 0.00170694
Iteration 5/25 | Loss: 0.00164519
Iteration 6/25 | Loss: 0.00164545
Iteration 7/25 | Loss: 0.00160620
Iteration 8/25 | Loss: 0.00159959
Iteration 9/25 | Loss: 0.00159283
Iteration 10/25 | Loss: 0.00158234
Iteration 11/25 | Loss: 0.00159482
Iteration 12/25 | Loss: 0.00159443
Iteration 13/25 | Loss: 0.00158384
Iteration 14/25 | Loss: 0.00158892
Iteration 15/25 | Loss: 0.00157647
Iteration 16/25 | Loss: 0.00160545
Iteration 17/25 | Loss: 0.00157433
Iteration 18/25 | Loss: 0.00155925
Iteration 19/25 | Loss: 0.00155550
Iteration 20/25 | Loss: 0.00155834
Iteration 21/25 | Loss: 0.00155046
Iteration 22/25 | Loss: 0.00154913
Iteration 23/25 | Loss: 0.00154831
Iteration 24/25 | Loss: 0.00154812
Iteration 25/25 | Loss: 0.00154811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81485462
Iteration 2/25 | Loss: 0.00385726
Iteration 3/25 | Loss: 0.00383968
Iteration 4/25 | Loss: 0.00383968
Iteration 5/25 | Loss: 0.00383968
Iteration 6/25 | Loss: 0.00383968
Iteration 7/25 | Loss: 0.00383968
Iteration 8/25 | Loss: 0.00383968
Iteration 9/25 | Loss: 0.00383968
Iteration 10/25 | Loss: 0.00383968
Iteration 11/25 | Loss: 0.00383968
Iteration 12/25 | Loss: 0.00383968
Iteration 13/25 | Loss: 0.00383968
Iteration 14/25 | Loss: 0.00383968
Iteration 15/25 | Loss: 0.00383968
Iteration 16/25 | Loss: 0.00383968
Iteration 17/25 | Loss: 0.00383968
Iteration 18/25 | Loss: 0.00383968
Iteration 19/25 | Loss: 0.00383968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0038396769668906927, 0.0038396769668906927, 0.0038396769668906927, 0.0038396769668906927, 0.0038396769668906927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038396769668906927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00383968
Iteration 2/1000 | Loss: 0.00022493
Iteration 3/1000 | Loss: 0.00023041
Iteration 4/1000 | Loss: 0.00076069
Iteration 5/1000 | Loss: 0.00010798
Iteration 6/1000 | Loss: 0.00006493
Iteration 7/1000 | Loss: 0.00014027
Iteration 8/1000 | Loss: 0.00096427
Iteration 9/1000 | Loss: 0.00011468
Iteration 10/1000 | Loss: 0.00156142
Iteration 11/1000 | Loss: 0.00006395
Iteration 12/1000 | Loss: 0.00005122
Iteration 13/1000 | Loss: 0.00004591
Iteration 14/1000 | Loss: 0.00005100
Iteration 15/1000 | Loss: 0.00004476
Iteration 16/1000 | Loss: 0.00004312
Iteration 17/1000 | Loss: 0.00037206
Iteration 18/1000 | Loss: 0.00006455
Iteration 19/1000 | Loss: 0.00007804
Iteration 20/1000 | Loss: 0.00005544
Iteration 21/1000 | Loss: 0.00004277
Iteration 22/1000 | Loss: 0.00004858
Iteration 23/1000 | Loss: 0.00004409
Iteration 24/1000 | Loss: 0.00003890
Iteration 25/1000 | Loss: 0.00003845
Iteration 26/1000 | Loss: 0.00004416
Iteration 27/1000 | Loss: 0.00003824
Iteration 28/1000 | Loss: 0.00004844
Iteration 29/1000 | Loss: 0.00003803
Iteration 30/1000 | Loss: 0.00004867
Iteration 31/1000 | Loss: 0.00003780
Iteration 32/1000 | Loss: 0.00181534
Iteration 33/1000 | Loss: 0.00004839
Iteration 34/1000 | Loss: 0.00003448
Iteration 35/1000 | Loss: 0.00004266
Iteration 36/1000 | Loss: 0.00003567
Iteration 37/1000 | Loss: 0.00003119
Iteration 38/1000 | Loss: 0.00003095
Iteration 39/1000 | Loss: 0.00003088
Iteration 40/1000 | Loss: 0.00003088
Iteration 41/1000 | Loss: 0.00003087
Iteration 42/1000 | Loss: 0.00003086
Iteration 43/1000 | Loss: 0.00003079
Iteration 44/1000 | Loss: 0.00003077
Iteration 45/1000 | Loss: 0.00003077
Iteration 46/1000 | Loss: 0.00003076
Iteration 47/1000 | Loss: 0.00003076
Iteration 48/1000 | Loss: 0.00003075
Iteration 49/1000 | Loss: 0.00003075
Iteration 50/1000 | Loss: 0.00003075
Iteration 51/1000 | Loss: 0.00003074
Iteration 52/1000 | Loss: 0.00003074
Iteration 53/1000 | Loss: 0.00003074
Iteration 54/1000 | Loss: 0.00003073
Iteration 55/1000 | Loss: 0.00003073
Iteration 56/1000 | Loss: 0.00003073
Iteration 57/1000 | Loss: 0.00003072
Iteration 58/1000 | Loss: 0.00003072
Iteration 59/1000 | Loss: 0.00003072
Iteration 60/1000 | Loss: 0.00003072
Iteration 61/1000 | Loss: 0.00003072
Iteration 62/1000 | Loss: 0.00003072
Iteration 63/1000 | Loss: 0.00003072
Iteration 64/1000 | Loss: 0.00003072
Iteration 65/1000 | Loss: 0.00003072
Iteration 66/1000 | Loss: 0.00003071
Iteration 67/1000 | Loss: 0.00003071
Iteration 68/1000 | Loss: 0.00003071
Iteration 69/1000 | Loss: 0.00003071
Iteration 70/1000 | Loss: 0.00003071
Iteration 71/1000 | Loss: 0.00003071
Iteration 72/1000 | Loss: 0.00003071
Iteration 73/1000 | Loss: 0.00003070
Iteration 74/1000 | Loss: 0.00003070
Iteration 75/1000 | Loss: 0.00003070
Iteration 76/1000 | Loss: 0.00003070
Iteration 77/1000 | Loss: 0.00003069
Iteration 78/1000 | Loss: 0.00003069
Iteration 79/1000 | Loss: 0.00003069
Iteration 80/1000 | Loss: 0.00003069
Iteration 81/1000 | Loss: 0.00003068
Iteration 82/1000 | Loss: 0.00003068
Iteration 83/1000 | Loss: 0.00003068
Iteration 84/1000 | Loss: 0.00003068
Iteration 85/1000 | Loss: 0.00003068
Iteration 86/1000 | Loss: 0.00003068
Iteration 87/1000 | Loss: 0.00003067
Iteration 88/1000 | Loss: 0.00003067
Iteration 89/1000 | Loss: 0.00003067
Iteration 90/1000 | Loss: 0.00003067
Iteration 91/1000 | Loss: 0.00003067
Iteration 92/1000 | Loss: 0.00003067
Iteration 93/1000 | Loss: 0.00003067
Iteration 94/1000 | Loss: 0.00003067
Iteration 95/1000 | Loss: 0.00003067
Iteration 96/1000 | Loss: 0.00003067
Iteration 97/1000 | Loss: 0.00003067
Iteration 98/1000 | Loss: 0.00003067
Iteration 99/1000 | Loss: 0.00003067
Iteration 100/1000 | Loss: 0.00003067
Iteration 101/1000 | Loss: 0.00003067
Iteration 102/1000 | Loss: 0.00003066
Iteration 103/1000 | Loss: 0.00003066
Iteration 104/1000 | Loss: 0.00003066
Iteration 105/1000 | Loss: 0.00003066
Iteration 106/1000 | Loss: 0.00003066
Iteration 107/1000 | Loss: 0.00003066
Iteration 108/1000 | Loss: 0.00003066
Iteration 109/1000 | Loss: 0.00003066
Iteration 110/1000 | Loss: 0.00003066
Iteration 111/1000 | Loss: 0.00003066
Iteration 112/1000 | Loss: 0.00003066
Iteration 113/1000 | Loss: 0.00003066
Iteration 114/1000 | Loss: 0.00003066
Iteration 115/1000 | Loss: 0.00003066
Iteration 116/1000 | Loss: 0.00003066
Iteration 117/1000 | Loss: 0.00003066
Iteration 118/1000 | Loss: 0.00003066
Iteration 119/1000 | Loss: 0.00003066
Iteration 120/1000 | Loss: 0.00003066
Iteration 121/1000 | Loss: 0.00003066
Iteration 122/1000 | Loss: 0.00003065
Iteration 123/1000 | Loss: 0.00003065
Iteration 124/1000 | Loss: 0.00003065
Iteration 125/1000 | Loss: 0.00003065
Iteration 126/1000 | Loss: 0.00003065
Iteration 127/1000 | Loss: 0.00003065
Iteration 128/1000 | Loss: 0.00003065
Iteration 129/1000 | Loss: 0.00003065
Iteration 130/1000 | Loss: 0.00003065
Iteration 131/1000 | Loss: 0.00003065
Iteration 132/1000 | Loss: 0.00003065
Iteration 133/1000 | Loss: 0.00003065
Iteration 134/1000 | Loss: 0.00003065
Iteration 135/1000 | Loss: 0.00003065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [3.065448981942609e-05, 3.065448981942609e-05, 3.065448981942609e-05, 3.065448981942609e-05, 3.065448981942609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.065448981942609e-05

Optimization complete. Final v2v error: 4.726521015167236 mm

Highest mean error: 7.376764297485352 mm for frame 50

Lowest mean error: 4.023959159851074 mm for frame 98

Saving results

Total time: 101.08562874794006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401112
Iteration 2/25 | Loss: 0.00168832
Iteration 3/25 | Loss: 0.00155810
Iteration 4/25 | Loss: 0.00153617
Iteration 5/25 | Loss: 0.00152928
Iteration 6/25 | Loss: 0.00152810
Iteration 7/25 | Loss: 0.00152810
Iteration 8/25 | Loss: 0.00152810
Iteration 9/25 | Loss: 0.00152810
Iteration 10/25 | Loss: 0.00152810
Iteration 11/25 | Loss: 0.00152810
Iteration 12/25 | Loss: 0.00152810
Iteration 13/25 | Loss: 0.00152810
Iteration 14/25 | Loss: 0.00152810
Iteration 15/25 | Loss: 0.00152810
Iteration 16/25 | Loss: 0.00152810
Iteration 17/25 | Loss: 0.00152810
Iteration 18/25 | Loss: 0.00152810
Iteration 19/25 | Loss: 0.00152810
Iteration 20/25 | Loss: 0.00152810
Iteration 21/25 | Loss: 0.00152810
Iteration 22/25 | Loss: 0.00152810
Iteration 23/25 | Loss: 0.00152810
Iteration 24/25 | Loss: 0.00152810
Iteration 25/25 | Loss: 0.00152810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05706263
Iteration 2/25 | Loss: 0.00363657
Iteration 3/25 | Loss: 0.00363657
Iteration 4/25 | Loss: 0.00363656
Iteration 5/25 | Loss: 0.00363656
Iteration 6/25 | Loss: 0.00363656
Iteration 7/25 | Loss: 0.00363656
Iteration 8/25 | Loss: 0.00363656
Iteration 9/25 | Loss: 0.00363656
Iteration 10/25 | Loss: 0.00363656
Iteration 11/25 | Loss: 0.00363656
Iteration 12/25 | Loss: 0.00363656
Iteration 13/25 | Loss: 0.00363656
Iteration 14/25 | Loss: 0.00363656
Iteration 15/25 | Loss: 0.00363656
Iteration 16/25 | Loss: 0.00363656
Iteration 17/25 | Loss: 0.00363656
Iteration 18/25 | Loss: 0.00363656
Iteration 19/25 | Loss: 0.00363656
Iteration 20/25 | Loss: 0.00363656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0036365624982863665, 0.0036365624982863665, 0.0036365624982863665, 0.0036365624982863665, 0.0036365624982863665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036365624982863665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00363656
Iteration 2/1000 | Loss: 0.00006003
Iteration 3/1000 | Loss: 0.00004211
Iteration 4/1000 | Loss: 0.00003547
Iteration 5/1000 | Loss: 0.00003164
Iteration 6/1000 | Loss: 0.00002968
Iteration 7/1000 | Loss: 0.00002857
Iteration 8/1000 | Loss: 0.00002809
Iteration 9/1000 | Loss: 0.00002776
Iteration 10/1000 | Loss: 0.00002743
Iteration 11/1000 | Loss: 0.00002723
Iteration 12/1000 | Loss: 0.00002713
Iteration 13/1000 | Loss: 0.00002711
Iteration 14/1000 | Loss: 0.00002711
Iteration 15/1000 | Loss: 0.00002697
Iteration 16/1000 | Loss: 0.00002695
Iteration 17/1000 | Loss: 0.00002695
Iteration 18/1000 | Loss: 0.00002693
Iteration 19/1000 | Loss: 0.00002692
Iteration 20/1000 | Loss: 0.00002692
Iteration 21/1000 | Loss: 0.00002691
Iteration 22/1000 | Loss: 0.00002691
Iteration 23/1000 | Loss: 0.00002691
Iteration 24/1000 | Loss: 0.00002691
Iteration 25/1000 | Loss: 0.00002690
Iteration 26/1000 | Loss: 0.00002690
Iteration 27/1000 | Loss: 0.00002690
Iteration 28/1000 | Loss: 0.00002689
Iteration 29/1000 | Loss: 0.00002689
Iteration 30/1000 | Loss: 0.00002689
Iteration 31/1000 | Loss: 0.00002688
Iteration 32/1000 | Loss: 0.00002688
Iteration 33/1000 | Loss: 0.00002688
Iteration 34/1000 | Loss: 0.00002687
Iteration 35/1000 | Loss: 0.00002687
Iteration 36/1000 | Loss: 0.00002687
Iteration 37/1000 | Loss: 0.00002686
Iteration 38/1000 | Loss: 0.00002686
Iteration 39/1000 | Loss: 0.00002685
Iteration 40/1000 | Loss: 0.00002685
Iteration 41/1000 | Loss: 0.00002685
Iteration 42/1000 | Loss: 0.00002684
Iteration 43/1000 | Loss: 0.00002684
Iteration 44/1000 | Loss: 0.00002684
Iteration 45/1000 | Loss: 0.00002683
Iteration 46/1000 | Loss: 0.00002683
Iteration 47/1000 | Loss: 0.00002683
Iteration 48/1000 | Loss: 0.00002683
Iteration 49/1000 | Loss: 0.00002682
Iteration 50/1000 | Loss: 0.00002682
Iteration 51/1000 | Loss: 0.00002682
Iteration 52/1000 | Loss: 0.00002681
Iteration 53/1000 | Loss: 0.00002681
Iteration 54/1000 | Loss: 0.00002681
Iteration 55/1000 | Loss: 0.00002680
Iteration 56/1000 | Loss: 0.00002680
Iteration 57/1000 | Loss: 0.00002680
Iteration 58/1000 | Loss: 0.00002680
Iteration 59/1000 | Loss: 0.00002679
Iteration 60/1000 | Loss: 0.00002679
Iteration 61/1000 | Loss: 0.00002679
Iteration 62/1000 | Loss: 0.00002678
Iteration 63/1000 | Loss: 0.00002678
Iteration 64/1000 | Loss: 0.00002678
Iteration 65/1000 | Loss: 0.00002678
Iteration 66/1000 | Loss: 0.00002677
Iteration 67/1000 | Loss: 0.00002677
Iteration 68/1000 | Loss: 0.00002677
Iteration 69/1000 | Loss: 0.00002677
Iteration 70/1000 | Loss: 0.00002676
Iteration 71/1000 | Loss: 0.00002676
Iteration 72/1000 | Loss: 0.00002676
Iteration 73/1000 | Loss: 0.00002676
Iteration 74/1000 | Loss: 0.00002675
Iteration 75/1000 | Loss: 0.00002675
Iteration 76/1000 | Loss: 0.00002675
Iteration 77/1000 | Loss: 0.00002675
Iteration 78/1000 | Loss: 0.00002675
Iteration 79/1000 | Loss: 0.00002674
Iteration 80/1000 | Loss: 0.00002674
Iteration 81/1000 | Loss: 0.00002674
Iteration 82/1000 | Loss: 0.00002674
Iteration 83/1000 | Loss: 0.00002673
Iteration 84/1000 | Loss: 0.00002673
Iteration 85/1000 | Loss: 0.00002673
Iteration 86/1000 | Loss: 0.00002673
Iteration 87/1000 | Loss: 0.00002672
Iteration 88/1000 | Loss: 0.00002672
Iteration 89/1000 | Loss: 0.00002672
Iteration 90/1000 | Loss: 0.00002672
Iteration 91/1000 | Loss: 0.00002672
Iteration 92/1000 | Loss: 0.00002672
Iteration 93/1000 | Loss: 0.00002671
Iteration 94/1000 | Loss: 0.00002671
Iteration 95/1000 | Loss: 0.00002671
Iteration 96/1000 | Loss: 0.00002671
Iteration 97/1000 | Loss: 0.00002671
Iteration 98/1000 | Loss: 0.00002671
Iteration 99/1000 | Loss: 0.00002671
Iteration 100/1000 | Loss: 0.00002671
Iteration 101/1000 | Loss: 0.00002671
Iteration 102/1000 | Loss: 0.00002670
Iteration 103/1000 | Loss: 0.00002670
Iteration 104/1000 | Loss: 0.00002670
Iteration 105/1000 | Loss: 0.00002670
Iteration 106/1000 | Loss: 0.00002670
Iteration 107/1000 | Loss: 0.00002670
Iteration 108/1000 | Loss: 0.00002670
Iteration 109/1000 | Loss: 0.00002670
Iteration 110/1000 | Loss: 0.00002670
Iteration 111/1000 | Loss: 0.00002670
Iteration 112/1000 | Loss: 0.00002670
Iteration 113/1000 | Loss: 0.00002670
Iteration 114/1000 | Loss: 0.00002670
Iteration 115/1000 | Loss: 0.00002670
Iteration 116/1000 | Loss: 0.00002670
Iteration 117/1000 | Loss: 0.00002670
Iteration 118/1000 | Loss: 0.00002670
Iteration 119/1000 | Loss: 0.00002670
Iteration 120/1000 | Loss: 0.00002670
Iteration 121/1000 | Loss: 0.00002670
Iteration 122/1000 | Loss: 0.00002670
Iteration 123/1000 | Loss: 0.00002670
Iteration 124/1000 | Loss: 0.00002670
Iteration 125/1000 | Loss: 0.00002670
Iteration 126/1000 | Loss: 0.00002670
Iteration 127/1000 | Loss: 0.00002670
Iteration 128/1000 | Loss: 0.00002670
Iteration 129/1000 | Loss: 0.00002670
Iteration 130/1000 | Loss: 0.00002670
Iteration 131/1000 | Loss: 0.00002670
Iteration 132/1000 | Loss: 0.00002670
Iteration 133/1000 | Loss: 0.00002670
Iteration 134/1000 | Loss: 0.00002670
Iteration 135/1000 | Loss: 0.00002670
Iteration 136/1000 | Loss: 0.00002670
Iteration 137/1000 | Loss: 0.00002670
Iteration 138/1000 | Loss: 0.00002670
Iteration 139/1000 | Loss: 0.00002670
Iteration 140/1000 | Loss: 0.00002670
Iteration 141/1000 | Loss: 0.00002670
Iteration 142/1000 | Loss: 0.00002670
Iteration 143/1000 | Loss: 0.00002670
Iteration 144/1000 | Loss: 0.00002670
Iteration 145/1000 | Loss: 0.00002670
Iteration 146/1000 | Loss: 0.00002670
Iteration 147/1000 | Loss: 0.00002670
Iteration 148/1000 | Loss: 0.00002670
Iteration 149/1000 | Loss: 0.00002670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.670304456842132e-05, 2.670304456842132e-05, 2.670304456842132e-05, 2.670304456842132e-05, 2.670304456842132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.670304456842132e-05

Optimization complete. Final v2v error: 4.519874095916748 mm

Highest mean error: 5.004980564117432 mm for frame 4

Lowest mean error: 4.233370780944824 mm for frame 203

Saving results

Total time: 40.542370319366455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01246988
Iteration 2/25 | Loss: 0.00277990
Iteration 3/25 | Loss: 0.00209195
Iteration 4/25 | Loss: 0.00200531
Iteration 5/25 | Loss: 0.00208847
Iteration 6/25 | Loss: 0.00202413
Iteration 7/25 | Loss: 0.00195537
Iteration 8/25 | Loss: 0.00188484
Iteration 9/25 | Loss: 0.00183851
Iteration 10/25 | Loss: 0.00182288
Iteration 11/25 | Loss: 0.00181284
Iteration 12/25 | Loss: 0.00180048
Iteration 13/25 | Loss: 0.00180904
Iteration 14/25 | Loss: 0.00179937
Iteration 15/25 | Loss: 0.00180575
Iteration 16/25 | Loss: 0.00180419
Iteration 17/25 | Loss: 0.00180285
Iteration 18/25 | Loss: 0.00180689
Iteration 19/25 | Loss: 0.00179301
Iteration 20/25 | Loss: 0.00178531
Iteration 21/25 | Loss: 0.00178370
Iteration 22/25 | Loss: 0.00179465
Iteration 23/25 | Loss: 0.00178538
Iteration 24/25 | Loss: 0.00178053
Iteration 25/25 | Loss: 0.00178054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31958055
Iteration 2/25 | Loss: 0.00421309
Iteration 3/25 | Loss: 0.00421306
Iteration 4/25 | Loss: 0.00421306
Iteration 5/25 | Loss: 0.00421306
Iteration 6/25 | Loss: 0.00421306
Iteration 7/25 | Loss: 0.00421306
Iteration 8/25 | Loss: 0.00421306
Iteration 9/25 | Loss: 0.00421306
Iteration 10/25 | Loss: 0.00421306
Iteration 11/25 | Loss: 0.00421306
Iteration 12/25 | Loss: 0.00421306
Iteration 13/25 | Loss: 0.00421306
Iteration 14/25 | Loss: 0.00421306
Iteration 15/25 | Loss: 0.00421306
Iteration 16/25 | Loss: 0.00421306
Iteration 17/25 | Loss: 0.00421306
Iteration 18/25 | Loss: 0.00421306
Iteration 19/25 | Loss: 0.00421306
Iteration 20/25 | Loss: 0.00421306
Iteration 21/25 | Loss: 0.00421306
Iteration 22/25 | Loss: 0.00421306
Iteration 23/25 | Loss: 0.00421306
Iteration 24/25 | Loss: 0.00421306
Iteration 25/25 | Loss: 0.00421306
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0042130593210458755, 0.0042130593210458755, 0.0042130593210458755, 0.0042130593210458755, 0.0042130593210458755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0042130593210458755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00421306
Iteration 2/1000 | Loss: 0.00135446
Iteration 3/1000 | Loss: 0.00591311
Iteration 4/1000 | Loss: 0.00158480
Iteration 5/1000 | Loss: 0.00296063
Iteration 6/1000 | Loss: 0.00131898
Iteration 7/1000 | Loss: 0.00086015
Iteration 8/1000 | Loss: 0.00155035
Iteration 9/1000 | Loss: 0.00072762
Iteration 10/1000 | Loss: 0.00146589
Iteration 11/1000 | Loss: 0.00172894
Iteration 12/1000 | Loss: 0.00170498
Iteration 13/1000 | Loss: 0.00107424
Iteration 14/1000 | Loss: 0.00088761
Iteration 15/1000 | Loss: 0.00120226
Iteration 16/1000 | Loss: 0.00165295
Iteration 17/1000 | Loss: 0.00138756
Iteration 18/1000 | Loss: 0.00181850
Iteration 19/1000 | Loss: 0.00118134
Iteration 20/1000 | Loss: 0.00138795
Iteration 21/1000 | Loss: 0.00090148
Iteration 22/1000 | Loss: 0.00143795
Iteration 23/1000 | Loss: 0.00169706
Iteration 24/1000 | Loss: 0.00147765
Iteration 25/1000 | Loss: 0.00210243
Iteration 26/1000 | Loss: 0.00252748
Iteration 27/1000 | Loss: 0.00250269
Iteration 28/1000 | Loss: 0.00316763
Iteration 29/1000 | Loss: 0.00136348
Iteration 30/1000 | Loss: 0.00095428
Iteration 31/1000 | Loss: 0.00111035
Iteration 32/1000 | Loss: 0.00105362
Iteration 33/1000 | Loss: 0.00139903
Iteration 34/1000 | Loss: 0.00146460
Iteration 35/1000 | Loss: 0.00073087
Iteration 36/1000 | Loss: 0.00127026
Iteration 37/1000 | Loss: 0.00082539
Iteration 38/1000 | Loss: 0.00081340
Iteration 39/1000 | Loss: 0.00090109
Iteration 40/1000 | Loss: 0.00130537
Iteration 41/1000 | Loss: 0.00077383
Iteration 42/1000 | Loss: 0.00095503
Iteration 43/1000 | Loss: 0.00148967
Iteration 44/1000 | Loss: 0.00234963
Iteration 45/1000 | Loss: 0.00134230
Iteration 46/1000 | Loss: 0.00089235
Iteration 47/1000 | Loss: 0.00178345
Iteration 48/1000 | Loss: 0.00193097
Iteration 49/1000 | Loss: 0.00140430
Iteration 50/1000 | Loss: 0.00195845
Iteration 51/1000 | Loss: 0.00141023
Iteration 52/1000 | Loss: 0.00163353
Iteration 53/1000 | Loss: 0.00155984
Iteration 54/1000 | Loss: 0.00145782
Iteration 55/1000 | Loss: 0.00154846
Iteration 56/1000 | Loss: 0.00204509
Iteration 57/1000 | Loss: 0.00153369
Iteration 58/1000 | Loss: 0.00159512
Iteration 59/1000 | Loss: 0.00138366
Iteration 60/1000 | Loss: 0.00133332
Iteration 61/1000 | Loss: 0.00190478
Iteration 62/1000 | Loss: 0.00184827
Iteration 63/1000 | Loss: 0.00217366
Iteration 64/1000 | Loss: 0.00136380
Iteration 65/1000 | Loss: 0.00164284
Iteration 66/1000 | Loss: 0.00121773
Iteration 67/1000 | Loss: 0.00149422
Iteration 68/1000 | Loss: 0.00106536
Iteration 69/1000 | Loss: 0.00142823
Iteration 70/1000 | Loss: 0.00104426
Iteration 71/1000 | Loss: 0.00130865
Iteration 72/1000 | Loss: 0.00156874
Iteration 73/1000 | Loss: 0.00143544
Iteration 74/1000 | Loss: 0.00083960
Iteration 75/1000 | Loss: 0.00088218
Iteration 76/1000 | Loss: 0.00100327
Iteration 77/1000 | Loss: 0.00102528
Iteration 78/1000 | Loss: 0.00097531
Iteration 79/1000 | Loss: 0.00119911
Iteration 80/1000 | Loss: 0.00086860
Iteration 81/1000 | Loss: 0.00075055
Iteration 82/1000 | Loss: 0.00131186
Iteration 83/1000 | Loss: 0.00103710
Iteration 84/1000 | Loss: 0.00063930
Iteration 85/1000 | Loss: 0.00115306
Iteration 86/1000 | Loss: 0.00114256
Iteration 87/1000 | Loss: 0.00110543
Iteration 88/1000 | Loss: 0.00103161
Iteration 89/1000 | Loss: 0.00132241
Iteration 90/1000 | Loss: 0.00116459
Iteration 91/1000 | Loss: 0.00119624
Iteration 92/1000 | Loss: 0.00044550
Iteration 93/1000 | Loss: 0.00055409
Iteration 94/1000 | Loss: 0.00072182
Iteration 95/1000 | Loss: 0.00073231
Iteration 96/1000 | Loss: 0.00053174
Iteration 97/1000 | Loss: 0.00094787
Iteration 98/1000 | Loss: 0.00098438
Iteration 99/1000 | Loss: 0.00114094
Iteration 100/1000 | Loss: 0.00058466
Iteration 101/1000 | Loss: 0.00051263
Iteration 102/1000 | Loss: 0.00076662
Iteration 103/1000 | Loss: 0.00066647
Iteration 104/1000 | Loss: 0.00042499
Iteration 105/1000 | Loss: 0.00047961
Iteration 106/1000 | Loss: 0.00041097
Iteration 107/1000 | Loss: 0.00037809
Iteration 108/1000 | Loss: 0.00063734
Iteration 109/1000 | Loss: 0.00144566
Iteration 110/1000 | Loss: 0.00076400
Iteration 111/1000 | Loss: 0.00051035
Iteration 112/1000 | Loss: 0.00271144
Iteration 113/1000 | Loss: 0.00095048
Iteration 114/1000 | Loss: 0.00080734
Iteration 115/1000 | Loss: 0.00014118
Iteration 116/1000 | Loss: 0.00052428
Iteration 117/1000 | Loss: 0.00070033
Iteration 118/1000 | Loss: 0.00138464
Iteration 119/1000 | Loss: 0.00130568
Iteration 120/1000 | Loss: 0.00121562
Iteration 121/1000 | Loss: 0.00051405
Iteration 122/1000 | Loss: 0.00064694
Iteration 123/1000 | Loss: 0.00030935
Iteration 124/1000 | Loss: 0.00124141
Iteration 125/1000 | Loss: 0.00046002
Iteration 126/1000 | Loss: 0.00018518
Iteration 127/1000 | Loss: 0.00047774
Iteration 128/1000 | Loss: 0.00054133
Iteration 129/1000 | Loss: 0.00040388
Iteration 130/1000 | Loss: 0.00044227
Iteration 131/1000 | Loss: 0.00068933
Iteration 132/1000 | Loss: 0.00057858
Iteration 133/1000 | Loss: 0.00048569
Iteration 134/1000 | Loss: 0.00047888
Iteration 135/1000 | Loss: 0.00052435
Iteration 136/1000 | Loss: 0.00055700
Iteration 137/1000 | Loss: 0.00064055
Iteration 138/1000 | Loss: 0.00041967
Iteration 139/1000 | Loss: 0.00048122
Iteration 140/1000 | Loss: 0.00037909
Iteration 141/1000 | Loss: 0.00046632
Iteration 142/1000 | Loss: 0.00042036
Iteration 143/1000 | Loss: 0.00037358
Iteration 144/1000 | Loss: 0.00072102
Iteration 145/1000 | Loss: 0.00032388
Iteration 146/1000 | Loss: 0.00049672
Iteration 147/1000 | Loss: 0.00107383
Iteration 148/1000 | Loss: 0.00036443
Iteration 149/1000 | Loss: 0.00023768
Iteration 150/1000 | Loss: 0.00060434
Iteration 151/1000 | Loss: 0.00045175
Iteration 152/1000 | Loss: 0.00052867
Iteration 153/1000 | Loss: 0.00043098
Iteration 154/1000 | Loss: 0.00062223
Iteration 155/1000 | Loss: 0.00088183
Iteration 156/1000 | Loss: 0.00075610
Iteration 157/1000 | Loss: 0.00033234
Iteration 158/1000 | Loss: 0.00056972
Iteration 159/1000 | Loss: 0.00100427
Iteration 160/1000 | Loss: 0.00048448
Iteration 161/1000 | Loss: 0.00081752
Iteration 162/1000 | Loss: 0.00079785
Iteration 163/1000 | Loss: 0.00061949
Iteration 164/1000 | Loss: 0.00055950
Iteration 165/1000 | Loss: 0.00070407
Iteration 166/1000 | Loss: 0.00066748
Iteration 167/1000 | Loss: 0.00062391
Iteration 168/1000 | Loss: 0.00011947
Iteration 169/1000 | Loss: 0.00008873
Iteration 170/1000 | Loss: 0.00099138
Iteration 171/1000 | Loss: 0.00022476
Iteration 172/1000 | Loss: 0.00031720
Iteration 173/1000 | Loss: 0.00015167
Iteration 174/1000 | Loss: 0.00018462
Iteration 175/1000 | Loss: 0.00019159
Iteration 176/1000 | Loss: 0.00022964
Iteration 177/1000 | Loss: 0.00025657
Iteration 178/1000 | Loss: 0.00025067
Iteration 179/1000 | Loss: 0.00029671
Iteration 180/1000 | Loss: 0.00027114
Iteration 181/1000 | Loss: 0.00054362
Iteration 182/1000 | Loss: 0.00031128
Iteration 183/1000 | Loss: 0.00028917
Iteration 184/1000 | Loss: 0.00011729
Iteration 185/1000 | Loss: 0.00024848
Iteration 186/1000 | Loss: 0.00013637
Iteration 187/1000 | Loss: 0.00016320
Iteration 188/1000 | Loss: 0.00019546
Iteration 189/1000 | Loss: 0.00026913
Iteration 190/1000 | Loss: 0.00038275
Iteration 191/1000 | Loss: 0.00013443
Iteration 192/1000 | Loss: 0.00007381
Iteration 193/1000 | Loss: 0.00007992
Iteration 194/1000 | Loss: 0.00019497
Iteration 195/1000 | Loss: 0.00017617
Iteration 196/1000 | Loss: 0.00007082
Iteration 197/1000 | Loss: 0.00018488
Iteration 198/1000 | Loss: 0.00043565
Iteration 199/1000 | Loss: 0.00021871
Iteration 200/1000 | Loss: 0.00019938
Iteration 201/1000 | Loss: 0.00020271
Iteration 202/1000 | Loss: 0.00016885
Iteration 203/1000 | Loss: 0.00011161
Iteration 204/1000 | Loss: 0.00011609
Iteration 205/1000 | Loss: 0.00013438
Iteration 206/1000 | Loss: 0.00018918
Iteration 207/1000 | Loss: 0.00015890
Iteration 208/1000 | Loss: 0.00006359
Iteration 209/1000 | Loss: 0.00007801
Iteration 210/1000 | Loss: 0.00017086
Iteration 211/1000 | Loss: 0.00022112
Iteration 212/1000 | Loss: 0.00043051
Iteration 213/1000 | Loss: 0.00017357
Iteration 214/1000 | Loss: 0.00020350
Iteration 215/1000 | Loss: 0.00013322
Iteration 216/1000 | Loss: 0.00020671
Iteration 217/1000 | Loss: 0.00019890
Iteration 218/1000 | Loss: 0.00017608
Iteration 219/1000 | Loss: 0.00009474
Iteration 220/1000 | Loss: 0.00010137
Iteration 221/1000 | Loss: 0.00013131
Iteration 222/1000 | Loss: 0.00006693
Iteration 223/1000 | Loss: 0.00036642
Iteration 224/1000 | Loss: 0.00013347
Iteration 225/1000 | Loss: 0.00020413
Iteration 226/1000 | Loss: 0.00043896
Iteration 227/1000 | Loss: 0.00025589
Iteration 228/1000 | Loss: 0.00047119
Iteration 229/1000 | Loss: 0.00051403
Iteration 230/1000 | Loss: 0.00078707
Iteration 231/1000 | Loss: 0.00032644
Iteration 232/1000 | Loss: 0.00008944
Iteration 233/1000 | Loss: 0.00007569
Iteration 234/1000 | Loss: 0.00006919
Iteration 235/1000 | Loss: 0.00006430
Iteration 236/1000 | Loss: 0.00022157
Iteration 237/1000 | Loss: 0.00006508
Iteration 238/1000 | Loss: 0.00015191
Iteration 239/1000 | Loss: 0.00024778
Iteration 240/1000 | Loss: 0.00013560
Iteration 241/1000 | Loss: 0.00025128
Iteration 242/1000 | Loss: 0.00013943
Iteration 243/1000 | Loss: 0.00015814
Iteration 244/1000 | Loss: 0.00012079
Iteration 245/1000 | Loss: 0.00012780
Iteration 246/1000 | Loss: 0.00011073
Iteration 247/1000 | Loss: 0.00073253
Iteration 248/1000 | Loss: 0.00011186
Iteration 249/1000 | Loss: 0.00005929
Iteration 250/1000 | Loss: 0.00005570
Iteration 251/1000 | Loss: 0.00082458
Iteration 252/1000 | Loss: 0.00009966
Iteration 253/1000 | Loss: 0.00007217
Iteration 254/1000 | Loss: 0.00005466
Iteration 255/1000 | Loss: 0.00005984
Iteration 256/1000 | Loss: 0.00005482
Iteration 257/1000 | Loss: 0.00005134
Iteration 258/1000 | Loss: 0.00005002
Iteration 259/1000 | Loss: 0.00004903
Iteration 260/1000 | Loss: 0.00004855
Iteration 261/1000 | Loss: 0.00004805
Iteration 262/1000 | Loss: 0.00004759
Iteration 263/1000 | Loss: 0.00004719
Iteration 264/1000 | Loss: 0.00004701
Iteration 265/1000 | Loss: 0.00004692
Iteration 266/1000 | Loss: 0.00004691
Iteration 267/1000 | Loss: 0.00004691
Iteration 268/1000 | Loss: 0.00004684
Iteration 269/1000 | Loss: 0.00004679
Iteration 270/1000 | Loss: 0.00004674
Iteration 271/1000 | Loss: 0.00004674
Iteration 272/1000 | Loss: 0.00004674
Iteration 273/1000 | Loss: 0.00004674
Iteration 274/1000 | Loss: 0.00004673
Iteration 275/1000 | Loss: 0.00004673
Iteration 276/1000 | Loss: 0.00004672
Iteration 277/1000 | Loss: 0.00004672
Iteration 278/1000 | Loss: 0.00004671
Iteration 279/1000 | Loss: 0.00004671
Iteration 280/1000 | Loss: 0.00004671
Iteration 281/1000 | Loss: 0.00004670
Iteration 282/1000 | Loss: 0.00004670
Iteration 283/1000 | Loss: 0.00004670
Iteration 284/1000 | Loss: 0.00004668
Iteration 285/1000 | Loss: 0.00004667
Iteration 286/1000 | Loss: 0.00004666
Iteration 287/1000 | Loss: 0.00004665
Iteration 288/1000 | Loss: 0.00004665
Iteration 289/1000 | Loss: 0.00004665
Iteration 290/1000 | Loss: 0.00004665
Iteration 291/1000 | Loss: 0.00004665
Iteration 292/1000 | Loss: 0.00004665
Iteration 293/1000 | Loss: 0.00004664
Iteration 294/1000 | Loss: 0.00004664
Iteration 295/1000 | Loss: 0.00004664
Iteration 296/1000 | Loss: 0.00004664
Iteration 297/1000 | Loss: 0.00004663
Iteration 298/1000 | Loss: 0.00004663
Iteration 299/1000 | Loss: 0.00004663
Iteration 300/1000 | Loss: 0.00004663
Iteration 301/1000 | Loss: 0.00004663
Iteration 302/1000 | Loss: 0.00004662
Iteration 303/1000 | Loss: 0.00004662
Iteration 304/1000 | Loss: 0.00004662
Iteration 305/1000 | Loss: 0.00004662
Iteration 306/1000 | Loss: 0.00004661
Iteration 307/1000 | Loss: 0.00004661
Iteration 308/1000 | Loss: 0.00004660
Iteration 309/1000 | Loss: 0.00004660
Iteration 310/1000 | Loss: 0.00004660
Iteration 311/1000 | Loss: 0.00004660
Iteration 312/1000 | Loss: 0.00004659
Iteration 313/1000 | Loss: 0.00004659
Iteration 314/1000 | Loss: 0.00004659
Iteration 315/1000 | Loss: 0.00004659
Iteration 316/1000 | Loss: 0.00004659
Iteration 317/1000 | Loss: 0.00004659
Iteration 318/1000 | Loss: 0.00004659
Iteration 319/1000 | Loss: 0.00004659
Iteration 320/1000 | Loss: 0.00004659
Iteration 321/1000 | Loss: 0.00004659
Iteration 322/1000 | Loss: 0.00004658
Iteration 323/1000 | Loss: 0.00004658
Iteration 324/1000 | Loss: 0.00004658
Iteration 325/1000 | Loss: 0.00004658
Iteration 326/1000 | Loss: 0.00004658
Iteration 327/1000 | Loss: 0.00004658
Iteration 328/1000 | Loss: 0.00004658
Iteration 329/1000 | Loss: 0.00004658
Iteration 330/1000 | Loss: 0.00004658
Iteration 331/1000 | Loss: 0.00004658
Iteration 332/1000 | Loss: 0.00004658
Iteration 333/1000 | Loss: 0.00004658
Iteration 334/1000 | Loss: 0.00004658
Iteration 335/1000 | Loss: 0.00004658
Iteration 336/1000 | Loss: 0.00004658
Iteration 337/1000 | Loss: 0.00004658
Iteration 338/1000 | Loss: 0.00004658
Iteration 339/1000 | Loss: 0.00004658
Iteration 340/1000 | Loss: 0.00004658
Iteration 341/1000 | Loss: 0.00004658
Iteration 342/1000 | Loss: 0.00004658
Iteration 343/1000 | Loss: 0.00004658
Iteration 344/1000 | Loss: 0.00004658
Iteration 345/1000 | Loss: 0.00004658
Iteration 346/1000 | Loss: 0.00004658
Iteration 347/1000 | Loss: 0.00004658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 347. Stopping optimization.
Last 5 losses: [4.65750927105546e-05, 4.65750927105546e-05, 4.65750927105546e-05, 4.65750927105546e-05, 4.65750927105546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.65750927105546e-05

Optimization complete. Final v2v error: 5.639798641204834 mm

Highest mean error: 6.8233160972595215 mm for frame 77

Lowest mean error: 5.040854454040527 mm for frame 206

Saving results

Total time: 470.04043436050415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965147
Iteration 2/25 | Loss: 0.00161079
Iteration 3/25 | Loss: 0.00150955
Iteration 4/25 | Loss: 0.00149777
Iteration 5/25 | Loss: 0.00149445
Iteration 6/25 | Loss: 0.00149427
Iteration 7/25 | Loss: 0.00149427
Iteration 8/25 | Loss: 0.00149427
Iteration 9/25 | Loss: 0.00149427
Iteration 10/25 | Loss: 0.00149427
Iteration 11/25 | Loss: 0.00149427
Iteration 12/25 | Loss: 0.00149427
Iteration 13/25 | Loss: 0.00149427
Iteration 14/25 | Loss: 0.00149427
Iteration 15/25 | Loss: 0.00149427
Iteration 16/25 | Loss: 0.00149427
Iteration 17/25 | Loss: 0.00149427
Iteration 18/25 | Loss: 0.00149427
Iteration 19/25 | Loss: 0.00149427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014942705165594816, 0.0014942705165594816, 0.0014942705165594816, 0.0014942705165594816, 0.0014942705165594816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014942705165594816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.03027821
Iteration 2/25 | Loss: 0.00312037
Iteration 3/25 | Loss: 0.00312037
Iteration 4/25 | Loss: 0.00312037
Iteration 5/25 | Loss: 0.00312037
Iteration 6/25 | Loss: 0.00312037
Iteration 7/25 | Loss: 0.00312037
Iteration 8/25 | Loss: 0.00312037
Iteration 9/25 | Loss: 0.00312037
Iteration 10/25 | Loss: 0.00312037
Iteration 11/25 | Loss: 0.00312037
Iteration 12/25 | Loss: 0.00312037
Iteration 13/25 | Loss: 0.00312037
Iteration 14/25 | Loss: 0.00312037
Iteration 15/25 | Loss: 0.00312037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0031203669495880604, 0.0031203669495880604, 0.0031203669495880604, 0.0031203669495880604, 0.0031203669495880604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031203669495880604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00312037
Iteration 2/1000 | Loss: 0.00003577
Iteration 3/1000 | Loss: 0.00002838
Iteration 4/1000 | Loss: 0.00002579
Iteration 5/1000 | Loss: 0.00002448
Iteration 6/1000 | Loss: 0.00002399
Iteration 7/1000 | Loss: 0.00002372
Iteration 8/1000 | Loss: 0.00002343
Iteration 9/1000 | Loss: 0.00002325
Iteration 10/1000 | Loss: 0.00002322
Iteration 11/1000 | Loss: 0.00002312
Iteration 12/1000 | Loss: 0.00002307
Iteration 13/1000 | Loss: 0.00002306
Iteration 14/1000 | Loss: 0.00002303
Iteration 15/1000 | Loss: 0.00002303
Iteration 16/1000 | Loss: 0.00002302
Iteration 17/1000 | Loss: 0.00002302
Iteration 18/1000 | Loss: 0.00002301
Iteration 19/1000 | Loss: 0.00002301
Iteration 20/1000 | Loss: 0.00002295
Iteration 21/1000 | Loss: 0.00002295
Iteration 22/1000 | Loss: 0.00002293
Iteration 23/1000 | Loss: 0.00002293
Iteration 24/1000 | Loss: 0.00002293
Iteration 25/1000 | Loss: 0.00002293
Iteration 26/1000 | Loss: 0.00002293
Iteration 27/1000 | Loss: 0.00002292
Iteration 28/1000 | Loss: 0.00002292
Iteration 29/1000 | Loss: 0.00002292
Iteration 30/1000 | Loss: 0.00002291
Iteration 31/1000 | Loss: 0.00002291
Iteration 32/1000 | Loss: 0.00002290
Iteration 33/1000 | Loss: 0.00002290
Iteration 34/1000 | Loss: 0.00002289
Iteration 35/1000 | Loss: 0.00002289
Iteration 36/1000 | Loss: 0.00002288
Iteration 37/1000 | Loss: 0.00002288
Iteration 38/1000 | Loss: 0.00002288
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002287
Iteration 41/1000 | Loss: 0.00002285
Iteration 42/1000 | Loss: 0.00002285
Iteration 43/1000 | Loss: 0.00002285
Iteration 44/1000 | Loss: 0.00002285
Iteration 45/1000 | Loss: 0.00002284
Iteration 46/1000 | Loss: 0.00002284
Iteration 47/1000 | Loss: 0.00002284
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002284
Iteration 51/1000 | Loss: 0.00002284
Iteration 52/1000 | Loss: 0.00002283
Iteration 53/1000 | Loss: 0.00002283
Iteration 54/1000 | Loss: 0.00002283
Iteration 55/1000 | Loss: 0.00002283
Iteration 56/1000 | Loss: 0.00002283
Iteration 57/1000 | Loss: 0.00002283
Iteration 58/1000 | Loss: 0.00002282
Iteration 59/1000 | Loss: 0.00002282
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002280
Iteration 62/1000 | Loss: 0.00002279
Iteration 63/1000 | Loss: 0.00002279
Iteration 64/1000 | Loss: 0.00002279
Iteration 65/1000 | Loss: 0.00002279
Iteration 66/1000 | Loss: 0.00002278
Iteration 67/1000 | Loss: 0.00002278
Iteration 68/1000 | Loss: 0.00002278
Iteration 69/1000 | Loss: 0.00002278
Iteration 70/1000 | Loss: 0.00002278
Iteration 71/1000 | Loss: 0.00002278
Iteration 72/1000 | Loss: 0.00002277
Iteration 73/1000 | Loss: 0.00002277
Iteration 74/1000 | Loss: 0.00002276
Iteration 75/1000 | Loss: 0.00002276
Iteration 76/1000 | Loss: 0.00002276
Iteration 77/1000 | Loss: 0.00002276
Iteration 78/1000 | Loss: 0.00002276
Iteration 79/1000 | Loss: 0.00002276
Iteration 80/1000 | Loss: 0.00002276
Iteration 81/1000 | Loss: 0.00002276
Iteration 82/1000 | Loss: 0.00002276
Iteration 83/1000 | Loss: 0.00002276
Iteration 84/1000 | Loss: 0.00002276
Iteration 85/1000 | Loss: 0.00002276
Iteration 86/1000 | Loss: 0.00002276
Iteration 87/1000 | Loss: 0.00002276
Iteration 88/1000 | Loss: 0.00002275
Iteration 89/1000 | Loss: 0.00002275
Iteration 90/1000 | Loss: 0.00002275
Iteration 91/1000 | Loss: 0.00002275
Iteration 92/1000 | Loss: 0.00002275
Iteration 93/1000 | Loss: 0.00002275
Iteration 94/1000 | Loss: 0.00002275
Iteration 95/1000 | Loss: 0.00002275
Iteration 96/1000 | Loss: 0.00002275
Iteration 97/1000 | Loss: 0.00002275
Iteration 98/1000 | Loss: 0.00002275
Iteration 99/1000 | Loss: 0.00002275
Iteration 100/1000 | Loss: 0.00002275
Iteration 101/1000 | Loss: 0.00002275
Iteration 102/1000 | Loss: 0.00002275
Iteration 103/1000 | Loss: 0.00002275
Iteration 104/1000 | Loss: 0.00002275
Iteration 105/1000 | Loss: 0.00002275
Iteration 106/1000 | Loss: 0.00002275
Iteration 107/1000 | Loss: 0.00002274
Iteration 108/1000 | Loss: 0.00002274
Iteration 109/1000 | Loss: 0.00002274
Iteration 110/1000 | Loss: 0.00002274
Iteration 111/1000 | Loss: 0.00002274
Iteration 112/1000 | Loss: 0.00002274
Iteration 113/1000 | Loss: 0.00002274
Iteration 114/1000 | Loss: 0.00002274
Iteration 115/1000 | Loss: 0.00002274
Iteration 116/1000 | Loss: 0.00002274
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002274
Iteration 120/1000 | Loss: 0.00002274
Iteration 121/1000 | Loss: 0.00002274
Iteration 122/1000 | Loss: 0.00002274
Iteration 123/1000 | Loss: 0.00002274
Iteration 124/1000 | Loss: 0.00002274
Iteration 125/1000 | Loss: 0.00002274
Iteration 126/1000 | Loss: 0.00002274
Iteration 127/1000 | Loss: 0.00002274
Iteration 128/1000 | Loss: 0.00002274
Iteration 129/1000 | Loss: 0.00002274
Iteration 130/1000 | Loss: 0.00002274
Iteration 131/1000 | Loss: 0.00002274
Iteration 132/1000 | Loss: 0.00002274
Iteration 133/1000 | Loss: 0.00002274
Iteration 134/1000 | Loss: 0.00002274
Iteration 135/1000 | Loss: 0.00002274
Iteration 136/1000 | Loss: 0.00002274
Iteration 137/1000 | Loss: 0.00002274
Iteration 138/1000 | Loss: 0.00002274
Iteration 139/1000 | Loss: 0.00002274
Iteration 140/1000 | Loss: 0.00002274
Iteration 141/1000 | Loss: 0.00002274
Iteration 142/1000 | Loss: 0.00002274
Iteration 143/1000 | Loss: 0.00002274
Iteration 144/1000 | Loss: 0.00002274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.2737196559319273e-05, 2.2737196559319273e-05, 2.2737196559319273e-05, 2.2737196559319273e-05, 2.2737196559319273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2737196559319273e-05

Optimization complete. Final v2v error: 4.206120014190674 mm

Highest mean error: 4.521482467651367 mm for frame 220

Lowest mean error: 3.943084716796875 mm for frame 132

Saving results

Total time: 35.228875398635864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061848
Iteration 2/25 | Loss: 0.00211921
Iteration 3/25 | Loss: 0.00167104
Iteration 4/25 | Loss: 0.00164004
Iteration 5/25 | Loss: 0.00163094
Iteration 6/25 | Loss: 0.00162881
Iteration 7/25 | Loss: 0.00162872
Iteration 8/25 | Loss: 0.00162872
Iteration 9/25 | Loss: 0.00162872
Iteration 10/25 | Loss: 0.00162872
Iteration 11/25 | Loss: 0.00162872
Iteration 12/25 | Loss: 0.00162872
Iteration 13/25 | Loss: 0.00162872
Iteration 14/25 | Loss: 0.00162872
Iteration 15/25 | Loss: 0.00162872
Iteration 16/25 | Loss: 0.00162872
Iteration 17/25 | Loss: 0.00162872
Iteration 18/25 | Loss: 0.00162872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016287248581647873, 0.0016287248581647873, 0.0016287248581647873, 0.0016287248581647873, 0.0016287248581647873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016287248581647873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13118899
Iteration 2/25 | Loss: 0.00264918
Iteration 3/25 | Loss: 0.00264916
Iteration 4/25 | Loss: 0.00264916
Iteration 5/25 | Loss: 0.00264916
Iteration 6/25 | Loss: 0.00264916
Iteration 7/25 | Loss: 0.00264916
Iteration 8/25 | Loss: 0.00264916
Iteration 9/25 | Loss: 0.00264916
Iteration 10/25 | Loss: 0.00264916
Iteration 11/25 | Loss: 0.00264916
Iteration 12/25 | Loss: 0.00264916
Iteration 13/25 | Loss: 0.00264916
Iteration 14/25 | Loss: 0.00264916
Iteration 15/25 | Loss: 0.00264916
Iteration 16/25 | Loss: 0.00264916
Iteration 17/25 | Loss: 0.00264916
Iteration 18/25 | Loss: 0.00264916
Iteration 19/25 | Loss: 0.00264916
Iteration 20/25 | Loss: 0.00264916
Iteration 21/25 | Loss: 0.00264916
Iteration 22/25 | Loss: 0.00264916
Iteration 23/25 | Loss: 0.00264916
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0026491619646549225, 0.0026491619646549225, 0.0026491619646549225, 0.0026491619646549225, 0.0026491619646549225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026491619646549225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264916
Iteration 2/1000 | Loss: 0.00007408
Iteration 3/1000 | Loss: 0.00005462
Iteration 4/1000 | Loss: 0.00004713
Iteration 5/1000 | Loss: 0.00004374
Iteration 6/1000 | Loss: 0.00004148
Iteration 7/1000 | Loss: 0.00003995
Iteration 8/1000 | Loss: 0.00003925
Iteration 9/1000 | Loss: 0.00003875
Iteration 10/1000 | Loss: 0.00003838
Iteration 11/1000 | Loss: 0.00003808
Iteration 12/1000 | Loss: 0.00003788
Iteration 13/1000 | Loss: 0.00003782
Iteration 14/1000 | Loss: 0.00003777
Iteration 15/1000 | Loss: 0.00003771
Iteration 16/1000 | Loss: 0.00003770
Iteration 17/1000 | Loss: 0.00003770
Iteration 18/1000 | Loss: 0.00003770
Iteration 19/1000 | Loss: 0.00003768
Iteration 20/1000 | Loss: 0.00003768
Iteration 21/1000 | Loss: 0.00003766
Iteration 22/1000 | Loss: 0.00003765
Iteration 23/1000 | Loss: 0.00003765
Iteration 24/1000 | Loss: 0.00003764
Iteration 25/1000 | Loss: 0.00003764
Iteration 26/1000 | Loss: 0.00003764
Iteration 27/1000 | Loss: 0.00003763
Iteration 28/1000 | Loss: 0.00003763
Iteration 29/1000 | Loss: 0.00003762
Iteration 30/1000 | Loss: 0.00003762
Iteration 31/1000 | Loss: 0.00003762
Iteration 32/1000 | Loss: 0.00003762
Iteration 33/1000 | Loss: 0.00003761
Iteration 34/1000 | Loss: 0.00003761
Iteration 35/1000 | Loss: 0.00003761
Iteration 36/1000 | Loss: 0.00003761
Iteration 37/1000 | Loss: 0.00003761
Iteration 38/1000 | Loss: 0.00003760
Iteration 39/1000 | Loss: 0.00003760
Iteration 40/1000 | Loss: 0.00003759
Iteration 41/1000 | Loss: 0.00003759
Iteration 42/1000 | Loss: 0.00003759
Iteration 43/1000 | Loss: 0.00003758
Iteration 44/1000 | Loss: 0.00003758
Iteration 45/1000 | Loss: 0.00003758
Iteration 46/1000 | Loss: 0.00003758
Iteration 47/1000 | Loss: 0.00003757
Iteration 48/1000 | Loss: 0.00003757
Iteration 49/1000 | Loss: 0.00003757
Iteration 50/1000 | Loss: 0.00003757
Iteration 51/1000 | Loss: 0.00003757
Iteration 52/1000 | Loss: 0.00003757
Iteration 53/1000 | Loss: 0.00003756
Iteration 54/1000 | Loss: 0.00003756
Iteration 55/1000 | Loss: 0.00003756
Iteration 56/1000 | Loss: 0.00003756
Iteration 57/1000 | Loss: 0.00003756
Iteration 58/1000 | Loss: 0.00003755
Iteration 59/1000 | Loss: 0.00003755
Iteration 60/1000 | Loss: 0.00003755
Iteration 61/1000 | Loss: 0.00003755
Iteration 62/1000 | Loss: 0.00003754
Iteration 63/1000 | Loss: 0.00003754
Iteration 64/1000 | Loss: 0.00003754
Iteration 65/1000 | Loss: 0.00003754
Iteration 66/1000 | Loss: 0.00003754
Iteration 67/1000 | Loss: 0.00003753
Iteration 68/1000 | Loss: 0.00003753
Iteration 69/1000 | Loss: 0.00003753
Iteration 70/1000 | Loss: 0.00003753
Iteration 71/1000 | Loss: 0.00003753
Iteration 72/1000 | Loss: 0.00003753
Iteration 73/1000 | Loss: 0.00003753
Iteration 74/1000 | Loss: 0.00003753
Iteration 75/1000 | Loss: 0.00003752
Iteration 76/1000 | Loss: 0.00003752
Iteration 77/1000 | Loss: 0.00003752
Iteration 78/1000 | Loss: 0.00003752
Iteration 79/1000 | Loss: 0.00003751
Iteration 80/1000 | Loss: 0.00003751
Iteration 81/1000 | Loss: 0.00003751
Iteration 82/1000 | Loss: 0.00003751
Iteration 83/1000 | Loss: 0.00003750
Iteration 84/1000 | Loss: 0.00003750
Iteration 85/1000 | Loss: 0.00003750
Iteration 86/1000 | Loss: 0.00003750
Iteration 87/1000 | Loss: 0.00003750
Iteration 88/1000 | Loss: 0.00003749
Iteration 89/1000 | Loss: 0.00003749
Iteration 90/1000 | Loss: 0.00003749
Iteration 91/1000 | Loss: 0.00003749
Iteration 92/1000 | Loss: 0.00003749
Iteration 93/1000 | Loss: 0.00003748
Iteration 94/1000 | Loss: 0.00003748
Iteration 95/1000 | Loss: 0.00003748
Iteration 96/1000 | Loss: 0.00003748
Iteration 97/1000 | Loss: 0.00003748
Iteration 98/1000 | Loss: 0.00003748
Iteration 99/1000 | Loss: 0.00003748
Iteration 100/1000 | Loss: 0.00003748
Iteration 101/1000 | Loss: 0.00003748
Iteration 102/1000 | Loss: 0.00003748
Iteration 103/1000 | Loss: 0.00003747
Iteration 104/1000 | Loss: 0.00003747
Iteration 105/1000 | Loss: 0.00003747
Iteration 106/1000 | Loss: 0.00003747
Iteration 107/1000 | Loss: 0.00003747
Iteration 108/1000 | Loss: 0.00003747
Iteration 109/1000 | Loss: 0.00003747
Iteration 110/1000 | Loss: 0.00003747
Iteration 111/1000 | Loss: 0.00003746
Iteration 112/1000 | Loss: 0.00003746
Iteration 113/1000 | Loss: 0.00003746
Iteration 114/1000 | Loss: 0.00003746
Iteration 115/1000 | Loss: 0.00003745
Iteration 116/1000 | Loss: 0.00003745
Iteration 117/1000 | Loss: 0.00003745
Iteration 118/1000 | Loss: 0.00003745
Iteration 119/1000 | Loss: 0.00003745
Iteration 120/1000 | Loss: 0.00003745
Iteration 121/1000 | Loss: 0.00003745
Iteration 122/1000 | Loss: 0.00003745
Iteration 123/1000 | Loss: 0.00003745
Iteration 124/1000 | Loss: 0.00003745
Iteration 125/1000 | Loss: 0.00003744
Iteration 126/1000 | Loss: 0.00003744
Iteration 127/1000 | Loss: 0.00003744
Iteration 128/1000 | Loss: 0.00003744
Iteration 129/1000 | Loss: 0.00003744
Iteration 130/1000 | Loss: 0.00003744
Iteration 131/1000 | Loss: 0.00003744
Iteration 132/1000 | Loss: 0.00003744
Iteration 133/1000 | Loss: 0.00003744
Iteration 134/1000 | Loss: 0.00003743
Iteration 135/1000 | Loss: 0.00003743
Iteration 136/1000 | Loss: 0.00003743
Iteration 137/1000 | Loss: 0.00003743
Iteration 138/1000 | Loss: 0.00003743
Iteration 139/1000 | Loss: 0.00003743
Iteration 140/1000 | Loss: 0.00003743
Iteration 141/1000 | Loss: 0.00003743
Iteration 142/1000 | Loss: 0.00003743
Iteration 143/1000 | Loss: 0.00003743
Iteration 144/1000 | Loss: 0.00003743
Iteration 145/1000 | Loss: 0.00003743
Iteration 146/1000 | Loss: 0.00003743
Iteration 147/1000 | Loss: 0.00003742
Iteration 148/1000 | Loss: 0.00003742
Iteration 149/1000 | Loss: 0.00003742
Iteration 150/1000 | Loss: 0.00003742
Iteration 151/1000 | Loss: 0.00003742
Iteration 152/1000 | Loss: 0.00003742
Iteration 153/1000 | Loss: 0.00003742
Iteration 154/1000 | Loss: 0.00003742
Iteration 155/1000 | Loss: 0.00003742
Iteration 156/1000 | Loss: 0.00003742
Iteration 157/1000 | Loss: 0.00003742
Iteration 158/1000 | Loss: 0.00003742
Iteration 159/1000 | Loss: 0.00003742
Iteration 160/1000 | Loss: 0.00003742
Iteration 161/1000 | Loss: 0.00003742
Iteration 162/1000 | Loss: 0.00003742
Iteration 163/1000 | Loss: 0.00003742
Iteration 164/1000 | Loss: 0.00003742
Iteration 165/1000 | Loss: 0.00003742
Iteration 166/1000 | Loss: 0.00003742
Iteration 167/1000 | Loss: 0.00003742
Iteration 168/1000 | Loss: 0.00003742
Iteration 169/1000 | Loss: 0.00003742
Iteration 170/1000 | Loss: 0.00003742
Iteration 171/1000 | Loss: 0.00003742
Iteration 172/1000 | Loss: 0.00003742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [3.741537148016505e-05, 3.741537148016505e-05, 3.741537148016505e-05, 3.741537148016505e-05, 3.741537148016505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.741537148016505e-05

Optimization complete. Final v2v error: 5.185527801513672 mm

Highest mean error: 5.856168270111084 mm for frame 121

Lowest mean error: 4.529378890991211 mm for frame 57

Saving results

Total time: 38.22707462310791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01138607
Iteration 2/25 | Loss: 0.01138607
Iteration 3/25 | Loss: 0.01138606
Iteration 4/25 | Loss: 0.00348417
Iteration 5/25 | Loss: 0.00221896
Iteration 6/25 | Loss: 0.00205103
Iteration 7/25 | Loss: 0.00189622
Iteration 8/25 | Loss: 0.00183405
Iteration 9/25 | Loss: 0.00183965
Iteration 10/25 | Loss: 0.00176256
Iteration 11/25 | Loss: 0.00170824
Iteration 12/25 | Loss: 0.00168793
Iteration 13/25 | Loss: 0.00167216
Iteration 14/25 | Loss: 0.00166438
Iteration 15/25 | Loss: 0.00166265
Iteration 16/25 | Loss: 0.00165524
Iteration 17/25 | Loss: 0.00164687
Iteration 18/25 | Loss: 0.00164597
Iteration 19/25 | Loss: 0.00164575
Iteration 20/25 | Loss: 0.00164274
Iteration 21/25 | Loss: 0.00164243
Iteration 22/25 | Loss: 0.00164185
Iteration 23/25 | Loss: 0.00164139
Iteration 24/25 | Loss: 0.00164225
Iteration 25/25 | Loss: 0.00164133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67454314
Iteration 2/25 | Loss: 0.00483298
Iteration 3/25 | Loss: 0.00483297
Iteration 4/25 | Loss: 0.00483297
Iteration 5/25 | Loss: 0.00483297
Iteration 6/25 | Loss: 0.00483297
Iteration 7/25 | Loss: 0.00483297
Iteration 8/25 | Loss: 0.00483297
Iteration 9/25 | Loss: 0.00483297
Iteration 10/25 | Loss: 0.00483297
Iteration 11/25 | Loss: 0.00483297
Iteration 12/25 | Loss: 0.00483297
Iteration 13/25 | Loss: 0.00483297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0048329695127904415, 0.0048329695127904415, 0.0048329695127904415, 0.0048329695127904415, 0.0048329695127904415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0048329695127904415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00483297
Iteration 2/1000 | Loss: 0.00046530
Iteration 3/1000 | Loss: 0.00035565
Iteration 4/1000 | Loss: 0.00028747
Iteration 5/1000 | Loss: 0.00025580
Iteration 6/1000 | Loss: 0.00023983
Iteration 7/1000 | Loss: 0.00023089
Iteration 8/1000 | Loss: 0.00022115
Iteration 9/1000 | Loss: 0.00056713
Iteration 10/1000 | Loss: 0.00101028
Iteration 11/1000 | Loss: 0.00174231
Iteration 12/1000 | Loss: 0.00093353
Iteration 13/1000 | Loss: 0.00045063
Iteration 14/1000 | Loss: 0.00025639
Iteration 15/1000 | Loss: 0.00020292
Iteration 16/1000 | Loss: 0.00041184
Iteration 17/1000 | Loss: 0.00055364
Iteration 18/1000 | Loss: 0.00208764
Iteration 19/1000 | Loss: 0.00140348
Iteration 20/1000 | Loss: 0.00166297
Iteration 21/1000 | Loss: 0.00160668
Iteration 22/1000 | Loss: 0.00158678
Iteration 23/1000 | Loss: 0.00091613
Iteration 24/1000 | Loss: 0.00025166
Iteration 25/1000 | Loss: 0.00017429
Iteration 26/1000 | Loss: 0.00088377
Iteration 27/1000 | Loss: 0.00099341
Iteration 28/1000 | Loss: 0.00148635
Iteration 29/1000 | Loss: 0.00292297
Iteration 30/1000 | Loss: 0.00073466
Iteration 31/1000 | Loss: 0.00172605
Iteration 32/1000 | Loss: 0.00087914
Iteration 33/1000 | Loss: 0.00051678
Iteration 34/1000 | Loss: 0.00017020
Iteration 35/1000 | Loss: 0.00112017
Iteration 36/1000 | Loss: 0.00101909
Iteration 37/1000 | Loss: 0.00296745
Iteration 38/1000 | Loss: 0.00156108
Iteration 39/1000 | Loss: 0.00306077
Iteration 40/1000 | Loss: 0.00179247
Iteration 41/1000 | Loss: 0.00079094
Iteration 42/1000 | Loss: 0.00144564
Iteration 43/1000 | Loss: 0.00117527
Iteration 44/1000 | Loss: 0.00074214
Iteration 45/1000 | Loss: 0.00020872
Iteration 46/1000 | Loss: 0.00225088
Iteration 47/1000 | Loss: 0.00034686
Iteration 48/1000 | Loss: 0.00013442
Iteration 49/1000 | Loss: 0.00103035
Iteration 50/1000 | Loss: 0.00125377
Iteration 51/1000 | Loss: 0.00076398
Iteration 52/1000 | Loss: 0.00064571
Iteration 53/1000 | Loss: 0.00034054
Iteration 54/1000 | Loss: 0.00047495
Iteration 55/1000 | Loss: 0.00094530
Iteration 56/1000 | Loss: 0.00032242
Iteration 57/1000 | Loss: 0.00022075
Iteration 58/1000 | Loss: 0.00009681
Iteration 59/1000 | Loss: 0.00065944
Iteration 60/1000 | Loss: 0.00033029
Iteration 61/1000 | Loss: 0.00023023
Iteration 62/1000 | Loss: 0.00008969
Iteration 63/1000 | Loss: 0.00008033
Iteration 64/1000 | Loss: 0.00007484
Iteration 65/1000 | Loss: 0.00007124
Iteration 66/1000 | Loss: 0.00085024
Iteration 67/1000 | Loss: 0.00091982
Iteration 68/1000 | Loss: 0.00008503
Iteration 69/1000 | Loss: 0.00007051
Iteration 70/1000 | Loss: 0.00016861
Iteration 71/1000 | Loss: 0.00008264
Iteration 72/1000 | Loss: 0.00012898
Iteration 73/1000 | Loss: 0.00009143
Iteration 74/1000 | Loss: 0.00013649
Iteration 75/1000 | Loss: 0.00011570
Iteration 76/1000 | Loss: 0.00037232
Iteration 77/1000 | Loss: 0.00019891
Iteration 78/1000 | Loss: 0.00013350
Iteration 79/1000 | Loss: 0.00011901
Iteration 80/1000 | Loss: 0.00026767
Iteration 81/1000 | Loss: 0.00016601
Iteration 82/1000 | Loss: 0.00006973
Iteration 83/1000 | Loss: 0.00006469
Iteration 84/1000 | Loss: 0.00006265
Iteration 85/1000 | Loss: 0.00006216
Iteration 86/1000 | Loss: 0.00006138
Iteration 87/1000 | Loss: 0.00006019
Iteration 88/1000 | Loss: 0.00005838
Iteration 89/1000 | Loss: 0.00005751
Iteration 90/1000 | Loss: 0.00005689
Iteration 91/1000 | Loss: 0.00005652
Iteration 92/1000 | Loss: 0.00005708
Iteration 93/1000 | Loss: 0.00005631
Iteration 94/1000 | Loss: 0.00005613
Iteration 95/1000 | Loss: 0.00005612
Iteration 96/1000 | Loss: 0.00005606
Iteration 97/1000 | Loss: 0.00005615
Iteration 98/1000 | Loss: 0.00005595
Iteration 99/1000 | Loss: 0.00005588
Iteration 100/1000 | Loss: 0.00005587
Iteration 101/1000 | Loss: 0.00005587
Iteration 102/1000 | Loss: 0.00005587
Iteration 103/1000 | Loss: 0.00005586
Iteration 104/1000 | Loss: 0.00005586
Iteration 105/1000 | Loss: 0.00005586
Iteration 106/1000 | Loss: 0.00005586
Iteration 107/1000 | Loss: 0.00005586
Iteration 108/1000 | Loss: 0.00005585
Iteration 109/1000 | Loss: 0.00005585
Iteration 110/1000 | Loss: 0.00005585
Iteration 111/1000 | Loss: 0.00005585
Iteration 112/1000 | Loss: 0.00005585
Iteration 113/1000 | Loss: 0.00005585
Iteration 114/1000 | Loss: 0.00005585
Iteration 115/1000 | Loss: 0.00005584
Iteration 116/1000 | Loss: 0.00005584
Iteration 117/1000 | Loss: 0.00005584
Iteration 118/1000 | Loss: 0.00005584
Iteration 119/1000 | Loss: 0.00005584
Iteration 120/1000 | Loss: 0.00005584
Iteration 121/1000 | Loss: 0.00005583
Iteration 122/1000 | Loss: 0.00005583
Iteration 123/1000 | Loss: 0.00005583
Iteration 124/1000 | Loss: 0.00005583
Iteration 125/1000 | Loss: 0.00005583
Iteration 126/1000 | Loss: 0.00005582
Iteration 127/1000 | Loss: 0.00005582
Iteration 128/1000 | Loss: 0.00005582
Iteration 129/1000 | Loss: 0.00005582
Iteration 130/1000 | Loss: 0.00005582
Iteration 131/1000 | Loss: 0.00005581
Iteration 132/1000 | Loss: 0.00005580
Iteration 133/1000 | Loss: 0.00005580
Iteration 134/1000 | Loss: 0.00005580
Iteration 135/1000 | Loss: 0.00005580
Iteration 136/1000 | Loss: 0.00005580
Iteration 137/1000 | Loss: 0.00005580
Iteration 138/1000 | Loss: 0.00005580
Iteration 139/1000 | Loss: 0.00005580
Iteration 140/1000 | Loss: 0.00005579
Iteration 141/1000 | Loss: 0.00005579
Iteration 142/1000 | Loss: 0.00005579
Iteration 143/1000 | Loss: 0.00005579
Iteration 144/1000 | Loss: 0.00005579
Iteration 145/1000 | Loss: 0.00005579
Iteration 146/1000 | Loss: 0.00005579
Iteration 147/1000 | Loss: 0.00005579
Iteration 148/1000 | Loss: 0.00005579
Iteration 149/1000 | Loss: 0.00005579
Iteration 150/1000 | Loss: 0.00005579
Iteration 151/1000 | Loss: 0.00005578
Iteration 152/1000 | Loss: 0.00005578
Iteration 153/1000 | Loss: 0.00005578
Iteration 154/1000 | Loss: 0.00005578
Iteration 155/1000 | Loss: 0.00005578
Iteration 156/1000 | Loss: 0.00005578
Iteration 157/1000 | Loss: 0.00005578
Iteration 158/1000 | Loss: 0.00005577
Iteration 159/1000 | Loss: 0.00005678
Iteration 160/1000 | Loss: 0.00005599
Iteration 161/1000 | Loss: 0.00005577
Iteration 162/1000 | Loss: 0.00005577
Iteration 163/1000 | Loss: 0.00005577
Iteration 164/1000 | Loss: 0.00005678
Iteration 165/1000 | Loss: 0.00005609
Iteration 166/1000 | Loss: 0.00005576
Iteration 167/1000 | Loss: 0.00005576
Iteration 168/1000 | Loss: 0.00005576
Iteration 169/1000 | Loss: 0.00005576
Iteration 170/1000 | Loss: 0.00005576
Iteration 171/1000 | Loss: 0.00005576
Iteration 172/1000 | Loss: 0.00005576
Iteration 173/1000 | Loss: 0.00005682
Iteration 174/1000 | Loss: 0.00005598
Iteration 175/1000 | Loss: 0.00005681
Iteration 176/1000 | Loss: 0.00005604
Iteration 177/1000 | Loss: 0.00005646
Iteration 178/1000 | Loss: 0.00005592
Iteration 179/1000 | Loss: 0.00005639
Iteration 180/1000 | Loss: 0.00005589
Iteration 181/1000 | Loss: 0.00005640
Iteration 182/1000 | Loss: 0.00005592
Iteration 183/1000 | Loss: 0.00005630
Iteration 184/1000 | Loss: 0.00005587
Iteration 185/1000 | Loss: 0.00005633
Iteration 186/1000 | Loss: 0.00005594
Iteration 187/1000 | Loss: 0.00005621
Iteration 188/1000 | Loss: 0.00005587
Iteration 189/1000 | Loss: 0.00005614
Iteration 190/1000 | Loss: 0.00005588
Iteration 191/1000 | Loss: 0.00005588
Iteration 192/1000 | Loss: 0.00005588
Iteration 193/1000 | Loss: 0.00005588
Iteration 194/1000 | Loss: 0.00005587
Iteration 195/1000 | Loss: 0.00005586
Iteration 196/1000 | Loss: 0.00005579
Iteration 197/1000 | Loss: 0.00005579
Iteration 198/1000 | Loss: 0.00005578
Iteration 199/1000 | Loss: 0.00005578
Iteration 200/1000 | Loss: 0.00005578
Iteration 201/1000 | Loss: 0.00005578
Iteration 202/1000 | Loss: 0.00005578
Iteration 203/1000 | Loss: 0.00005578
Iteration 204/1000 | Loss: 0.00005578
Iteration 205/1000 | Loss: 0.00005577
Iteration 206/1000 | Loss: 0.00005577
Iteration 207/1000 | Loss: 0.00005577
Iteration 208/1000 | Loss: 0.00005576
Iteration 209/1000 | Loss: 0.00005576
Iteration 210/1000 | Loss: 0.00005576
Iteration 211/1000 | Loss: 0.00005576
Iteration 212/1000 | Loss: 0.00005576
Iteration 213/1000 | Loss: 0.00005576
Iteration 214/1000 | Loss: 0.00005576
Iteration 215/1000 | Loss: 0.00005576
Iteration 216/1000 | Loss: 0.00005576
Iteration 217/1000 | Loss: 0.00005576
Iteration 218/1000 | Loss: 0.00005576
Iteration 219/1000 | Loss: 0.00005576
Iteration 220/1000 | Loss: 0.00005576
Iteration 221/1000 | Loss: 0.00005587
Iteration 222/1000 | Loss: 0.00005587
Iteration 223/1000 | Loss: 0.00005575
Iteration 224/1000 | Loss: 0.00005575
Iteration 225/1000 | Loss: 0.00005575
Iteration 226/1000 | Loss: 0.00005575
Iteration 227/1000 | Loss: 0.00005575
Iteration 228/1000 | Loss: 0.00005575
Iteration 229/1000 | Loss: 0.00005575
Iteration 230/1000 | Loss: 0.00005575
Iteration 231/1000 | Loss: 0.00005575
Iteration 232/1000 | Loss: 0.00005575
Iteration 233/1000 | Loss: 0.00005575
Iteration 234/1000 | Loss: 0.00005575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [5.575381510425359e-05, 5.575381510425359e-05, 5.575381510425359e-05, 5.575381510425359e-05, 5.575381510425359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.575381510425359e-05

Optimization complete. Final v2v error: 5.726517200469971 mm

Highest mean error: 14.48130989074707 mm for frame 204

Lowest mean error: 4.852441787719727 mm for frame 4

Saving results

Total time: 229.2273244857788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094439
Iteration 2/25 | Loss: 0.00212347
Iteration 3/25 | Loss: 0.00168912
Iteration 4/25 | Loss: 0.00165627
Iteration 5/25 | Loss: 0.00164466
Iteration 6/25 | Loss: 0.00164111
Iteration 7/25 | Loss: 0.00163974
Iteration 8/25 | Loss: 0.00163974
Iteration 9/25 | Loss: 0.00163969
Iteration 10/25 | Loss: 0.00163969
Iteration 11/25 | Loss: 0.00163969
Iteration 12/25 | Loss: 0.00163969
Iteration 13/25 | Loss: 0.00163969
Iteration 14/25 | Loss: 0.00163969
Iteration 15/25 | Loss: 0.00163969
Iteration 16/25 | Loss: 0.00163969
Iteration 17/25 | Loss: 0.00163969
Iteration 18/25 | Loss: 0.00163969
Iteration 19/25 | Loss: 0.00163969
Iteration 20/25 | Loss: 0.00163969
Iteration 21/25 | Loss: 0.00163969
Iteration 22/25 | Loss: 0.00163969
Iteration 23/25 | Loss: 0.00163969
Iteration 24/25 | Loss: 0.00163969
Iteration 25/25 | Loss: 0.00163969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89665753
Iteration 2/25 | Loss: 0.00263280
Iteration 3/25 | Loss: 0.00263279
Iteration 4/25 | Loss: 0.00263279
Iteration 5/25 | Loss: 0.00263279
Iteration 6/25 | Loss: 0.00263279
Iteration 7/25 | Loss: 0.00263279
Iteration 8/25 | Loss: 0.00263279
Iteration 9/25 | Loss: 0.00263279
Iteration 10/25 | Loss: 0.00263279
Iteration 11/25 | Loss: 0.00263279
Iteration 12/25 | Loss: 0.00263279
Iteration 13/25 | Loss: 0.00263279
Iteration 14/25 | Loss: 0.00263279
Iteration 15/25 | Loss: 0.00263279
Iteration 16/25 | Loss: 0.00263279
Iteration 17/25 | Loss: 0.00263279
Iteration 18/25 | Loss: 0.00263279
Iteration 19/25 | Loss: 0.00263279
Iteration 20/25 | Loss: 0.00263279
Iteration 21/25 | Loss: 0.00263279
Iteration 22/25 | Loss: 0.00263279
Iteration 23/25 | Loss: 0.00263279
Iteration 24/25 | Loss: 0.00263279
Iteration 25/25 | Loss: 0.00263279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263279
Iteration 2/1000 | Loss: 0.00008520
Iteration 3/1000 | Loss: 0.00006885
Iteration 4/1000 | Loss: 0.00005973
Iteration 5/1000 | Loss: 0.00005397
Iteration 6/1000 | Loss: 0.00005156
Iteration 7/1000 | Loss: 0.00004987
Iteration 8/1000 | Loss: 0.00004917
Iteration 9/1000 | Loss: 0.00004860
Iteration 10/1000 | Loss: 0.00004813
Iteration 11/1000 | Loss: 0.00004776
Iteration 12/1000 | Loss: 0.00004751
Iteration 13/1000 | Loss: 0.00004741
Iteration 14/1000 | Loss: 0.00004732
Iteration 15/1000 | Loss: 0.00004731
Iteration 16/1000 | Loss: 0.00004731
Iteration 17/1000 | Loss: 0.00004730
Iteration 18/1000 | Loss: 0.00004730
Iteration 19/1000 | Loss: 0.00004730
Iteration 20/1000 | Loss: 0.00004727
Iteration 21/1000 | Loss: 0.00004723
Iteration 22/1000 | Loss: 0.00004723
Iteration 23/1000 | Loss: 0.00004719
Iteration 24/1000 | Loss: 0.00004719
Iteration 25/1000 | Loss: 0.00004719
Iteration 26/1000 | Loss: 0.00004716
Iteration 27/1000 | Loss: 0.00004716
Iteration 28/1000 | Loss: 0.00004714
Iteration 29/1000 | Loss: 0.00004714
Iteration 30/1000 | Loss: 0.00004712
Iteration 31/1000 | Loss: 0.00004712
Iteration 32/1000 | Loss: 0.00004712
Iteration 33/1000 | Loss: 0.00004711
Iteration 34/1000 | Loss: 0.00004711
Iteration 35/1000 | Loss: 0.00004711
Iteration 36/1000 | Loss: 0.00004711
Iteration 37/1000 | Loss: 0.00004710
Iteration 38/1000 | Loss: 0.00004709
Iteration 39/1000 | Loss: 0.00004709
Iteration 40/1000 | Loss: 0.00004709
Iteration 41/1000 | Loss: 0.00004707
Iteration 42/1000 | Loss: 0.00004706
Iteration 43/1000 | Loss: 0.00004706
Iteration 44/1000 | Loss: 0.00004706
Iteration 45/1000 | Loss: 0.00004705
Iteration 46/1000 | Loss: 0.00004705
Iteration 47/1000 | Loss: 0.00004705
Iteration 48/1000 | Loss: 0.00004705
Iteration 49/1000 | Loss: 0.00004705
Iteration 50/1000 | Loss: 0.00004705
Iteration 51/1000 | Loss: 0.00004704
Iteration 52/1000 | Loss: 0.00004704
Iteration 53/1000 | Loss: 0.00004704
Iteration 54/1000 | Loss: 0.00004704
Iteration 55/1000 | Loss: 0.00004704
Iteration 56/1000 | Loss: 0.00004704
Iteration 57/1000 | Loss: 0.00004704
Iteration 58/1000 | Loss: 0.00004704
Iteration 59/1000 | Loss: 0.00004704
Iteration 60/1000 | Loss: 0.00004704
Iteration 61/1000 | Loss: 0.00004704
Iteration 62/1000 | Loss: 0.00004704
Iteration 63/1000 | Loss: 0.00004703
Iteration 64/1000 | Loss: 0.00004703
Iteration 65/1000 | Loss: 0.00004703
Iteration 66/1000 | Loss: 0.00004702
Iteration 67/1000 | Loss: 0.00004702
Iteration 68/1000 | Loss: 0.00004701
Iteration 69/1000 | Loss: 0.00004701
Iteration 70/1000 | Loss: 0.00004701
Iteration 71/1000 | Loss: 0.00004701
Iteration 72/1000 | Loss: 0.00004701
Iteration 73/1000 | Loss: 0.00004701
Iteration 74/1000 | Loss: 0.00004701
Iteration 75/1000 | Loss: 0.00004701
Iteration 76/1000 | Loss: 0.00004701
Iteration 77/1000 | Loss: 0.00004701
Iteration 78/1000 | Loss: 0.00004700
Iteration 79/1000 | Loss: 0.00004700
Iteration 80/1000 | Loss: 0.00004700
Iteration 81/1000 | Loss: 0.00004700
Iteration 82/1000 | Loss: 0.00004699
Iteration 83/1000 | Loss: 0.00004699
Iteration 84/1000 | Loss: 0.00004699
Iteration 85/1000 | Loss: 0.00004698
Iteration 86/1000 | Loss: 0.00004698
Iteration 87/1000 | Loss: 0.00004698
Iteration 88/1000 | Loss: 0.00004698
Iteration 89/1000 | Loss: 0.00004697
Iteration 90/1000 | Loss: 0.00004697
Iteration 91/1000 | Loss: 0.00004697
Iteration 92/1000 | Loss: 0.00004697
Iteration 93/1000 | Loss: 0.00004696
Iteration 94/1000 | Loss: 0.00004696
Iteration 95/1000 | Loss: 0.00004696
Iteration 96/1000 | Loss: 0.00004696
Iteration 97/1000 | Loss: 0.00004696
Iteration 98/1000 | Loss: 0.00004696
Iteration 99/1000 | Loss: 0.00004696
Iteration 100/1000 | Loss: 0.00004696
Iteration 101/1000 | Loss: 0.00004696
Iteration 102/1000 | Loss: 0.00004696
Iteration 103/1000 | Loss: 0.00004695
Iteration 104/1000 | Loss: 0.00004695
Iteration 105/1000 | Loss: 0.00004695
Iteration 106/1000 | Loss: 0.00004695
Iteration 107/1000 | Loss: 0.00004695
Iteration 108/1000 | Loss: 0.00004694
Iteration 109/1000 | Loss: 0.00004694
Iteration 110/1000 | Loss: 0.00004694
Iteration 111/1000 | Loss: 0.00004694
Iteration 112/1000 | Loss: 0.00004694
Iteration 113/1000 | Loss: 0.00004694
Iteration 114/1000 | Loss: 0.00004693
Iteration 115/1000 | Loss: 0.00004693
Iteration 116/1000 | Loss: 0.00004693
Iteration 117/1000 | Loss: 0.00004693
Iteration 118/1000 | Loss: 0.00004693
Iteration 119/1000 | Loss: 0.00004693
Iteration 120/1000 | Loss: 0.00004693
Iteration 121/1000 | Loss: 0.00004693
Iteration 122/1000 | Loss: 0.00004693
Iteration 123/1000 | Loss: 0.00004693
Iteration 124/1000 | Loss: 0.00004692
Iteration 125/1000 | Loss: 0.00004692
Iteration 126/1000 | Loss: 0.00004692
Iteration 127/1000 | Loss: 0.00004692
Iteration 128/1000 | Loss: 0.00004692
Iteration 129/1000 | Loss: 0.00004692
Iteration 130/1000 | Loss: 0.00004692
Iteration 131/1000 | Loss: 0.00004692
Iteration 132/1000 | Loss: 0.00004692
Iteration 133/1000 | Loss: 0.00004692
Iteration 134/1000 | Loss: 0.00004692
Iteration 135/1000 | Loss: 0.00004692
Iteration 136/1000 | Loss: 0.00004692
Iteration 137/1000 | Loss: 0.00004692
Iteration 138/1000 | Loss: 0.00004692
Iteration 139/1000 | Loss: 0.00004692
Iteration 140/1000 | Loss: 0.00004692
Iteration 141/1000 | Loss: 0.00004691
Iteration 142/1000 | Loss: 0.00004691
Iteration 143/1000 | Loss: 0.00004691
Iteration 144/1000 | Loss: 0.00004691
Iteration 145/1000 | Loss: 0.00004691
Iteration 146/1000 | Loss: 0.00004691
Iteration 147/1000 | Loss: 0.00004691
Iteration 148/1000 | Loss: 0.00004691
Iteration 149/1000 | Loss: 0.00004691
Iteration 150/1000 | Loss: 0.00004691
Iteration 151/1000 | Loss: 0.00004691
Iteration 152/1000 | Loss: 0.00004691
Iteration 153/1000 | Loss: 0.00004691
Iteration 154/1000 | Loss: 0.00004691
Iteration 155/1000 | Loss: 0.00004691
Iteration 156/1000 | Loss: 0.00004691
Iteration 157/1000 | Loss: 0.00004691
Iteration 158/1000 | Loss: 0.00004691
Iteration 159/1000 | Loss: 0.00004691
Iteration 160/1000 | Loss: 0.00004691
Iteration 161/1000 | Loss: 0.00004691
Iteration 162/1000 | Loss: 0.00004691
Iteration 163/1000 | Loss: 0.00004691
Iteration 164/1000 | Loss: 0.00004691
Iteration 165/1000 | Loss: 0.00004691
Iteration 166/1000 | Loss: 0.00004691
Iteration 167/1000 | Loss: 0.00004691
Iteration 168/1000 | Loss: 0.00004691
Iteration 169/1000 | Loss: 0.00004691
Iteration 170/1000 | Loss: 0.00004691
Iteration 171/1000 | Loss: 0.00004691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [4.691051071858965e-05, 4.691051071858965e-05, 4.691051071858965e-05, 4.691051071858965e-05, 4.691051071858965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.691051071858965e-05

Optimization complete. Final v2v error: 5.891177654266357 mm

Highest mean error: 6.473371505737305 mm for frame 11

Lowest mean error: 5.226528167724609 mm for frame 47

Saving results

Total time: 43.92752957344055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01190245
Iteration 2/25 | Loss: 0.00206173
Iteration 3/25 | Loss: 0.00190062
Iteration 4/25 | Loss: 0.00167847
Iteration 5/25 | Loss: 0.00171220
Iteration 6/25 | Loss: 0.00161192
Iteration 7/25 | Loss: 0.00158108
Iteration 8/25 | Loss: 0.00152239
Iteration 9/25 | Loss: 0.00154423
Iteration 10/25 | Loss: 0.00151766
Iteration 11/25 | Loss: 0.00149843
Iteration 12/25 | Loss: 0.00152235
Iteration 13/25 | Loss: 0.00149182
Iteration 14/25 | Loss: 0.00148598
Iteration 15/25 | Loss: 0.00148065
Iteration 16/25 | Loss: 0.00146603
Iteration 17/25 | Loss: 0.00146880
Iteration 18/25 | Loss: 0.00146744
Iteration 19/25 | Loss: 0.00146817
Iteration 20/25 | Loss: 0.00147354
Iteration 21/25 | Loss: 0.00146738
Iteration 22/25 | Loss: 0.00146844
Iteration 23/25 | Loss: 0.00147028
Iteration 24/25 | Loss: 0.00147007
Iteration 25/25 | Loss: 0.00146819

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66243184
Iteration 2/25 | Loss: 0.00318979
Iteration 3/25 | Loss: 0.00318979
Iteration 4/25 | Loss: 0.00318979
Iteration 5/25 | Loss: 0.00318979
Iteration 6/25 | Loss: 0.00318979
Iteration 7/25 | Loss: 0.00318979
Iteration 8/25 | Loss: 0.00318979
Iteration 9/25 | Loss: 0.00318979
Iteration 10/25 | Loss: 0.00318979
Iteration 11/25 | Loss: 0.00318979
Iteration 12/25 | Loss: 0.00318979
Iteration 13/25 | Loss: 0.00318979
Iteration 14/25 | Loss: 0.00318979
Iteration 15/25 | Loss: 0.00318979
Iteration 16/25 | Loss: 0.00318979
Iteration 17/25 | Loss: 0.00318979
Iteration 18/25 | Loss: 0.00318979
Iteration 19/25 | Loss: 0.00318979
Iteration 20/25 | Loss: 0.00318979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0031897895969450474, 0.0031897895969450474, 0.0031897895969450474, 0.0031897895969450474, 0.0031897895969450474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031897895969450474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00318979
Iteration 2/1000 | Loss: 0.00005825
Iteration 3/1000 | Loss: 0.00007572
Iteration 4/1000 | Loss: 0.00003782
Iteration 5/1000 | Loss: 0.00008329
Iteration 6/1000 | Loss: 0.00003384
Iteration 7/1000 | Loss: 0.00003290
Iteration 8/1000 | Loss: 0.00003250
Iteration 9/1000 | Loss: 0.00029072
Iteration 10/1000 | Loss: 0.00025140
Iteration 11/1000 | Loss: 0.00004662
Iteration 12/1000 | Loss: 0.00004510
Iteration 13/1000 | Loss: 0.00004233
Iteration 14/1000 | Loss: 0.00003193
Iteration 15/1000 | Loss: 0.00003191
Iteration 16/1000 | Loss: 0.00026599
Iteration 17/1000 | Loss: 0.00009837
Iteration 18/1000 | Loss: 0.00009618
Iteration 19/1000 | Loss: 0.00009623
Iteration 20/1000 | Loss: 0.00003496
Iteration 21/1000 | Loss: 0.00010310
Iteration 22/1000 | Loss: 0.00003396
Iteration 23/1000 | Loss: 0.00008879
Iteration 24/1000 | Loss: 0.00020789
Iteration 25/1000 | Loss: 0.00030225
Iteration 26/1000 | Loss: 0.00015088
Iteration 27/1000 | Loss: 0.00020035
Iteration 28/1000 | Loss: 0.00012885
Iteration 29/1000 | Loss: 0.00022613
Iteration 30/1000 | Loss: 0.00038099
Iteration 31/1000 | Loss: 0.00051399
Iteration 32/1000 | Loss: 0.00008613
Iteration 33/1000 | Loss: 0.00004535
Iteration 34/1000 | Loss: 0.00003960
Iteration 35/1000 | Loss: 0.00013223
Iteration 36/1000 | Loss: 0.00004318
Iteration 37/1000 | Loss: 0.00034269
Iteration 38/1000 | Loss: 0.00011182
Iteration 39/1000 | Loss: 0.00041015
Iteration 40/1000 | Loss: 0.00022763
Iteration 41/1000 | Loss: 0.00030403
Iteration 42/1000 | Loss: 0.00025993
Iteration 43/1000 | Loss: 0.00020708
Iteration 44/1000 | Loss: 0.00007305
Iteration 45/1000 | Loss: 0.00009627
Iteration 46/1000 | Loss: 0.00003722
Iteration 47/1000 | Loss: 0.00006094
Iteration 48/1000 | Loss: 0.00004128
Iteration 49/1000 | Loss: 0.00003524
Iteration 50/1000 | Loss: 0.00003522
Iteration 51/1000 | Loss: 0.00008324
Iteration 52/1000 | Loss: 0.00003584
Iteration 53/1000 | Loss: 0.00006649
Iteration 54/1000 | Loss: 0.00003507
Iteration 55/1000 | Loss: 0.00003457
Iteration 56/1000 | Loss: 0.00065645
Iteration 57/1000 | Loss: 0.00015053
Iteration 58/1000 | Loss: 0.00003515
Iteration 59/1000 | Loss: 0.00038196
Iteration 60/1000 | Loss: 0.00010728
Iteration 61/1000 | Loss: 0.00026223
Iteration 62/1000 | Loss: 0.00005001
Iteration 63/1000 | Loss: 0.00006234
Iteration 64/1000 | Loss: 0.00003240
Iteration 65/1000 | Loss: 0.00003181
Iteration 66/1000 | Loss: 0.00003138
Iteration 67/1000 | Loss: 0.00005847
Iteration 68/1000 | Loss: 0.00003110
Iteration 69/1000 | Loss: 0.00003097
Iteration 70/1000 | Loss: 0.00003096
Iteration 71/1000 | Loss: 0.00003092
Iteration 72/1000 | Loss: 0.00003092
Iteration 73/1000 | Loss: 0.00003092
Iteration 74/1000 | Loss: 0.00003092
Iteration 75/1000 | Loss: 0.00003092
Iteration 76/1000 | Loss: 0.00003092
Iteration 77/1000 | Loss: 0.00003091
Iteration 78/1000 | Loss: 0.00003091
Iteration 79/1000 | Loss: 0.00003091
Iteration 80/1000 | Loss: 0.00003088
Iteration 81/1000 | Loss: 0.00003087
Iteration 82/1000 | Loss: 0.00003087
Iteration 83/1000 | Loss: 0.00003087
Iteration 84/1000 | Loss: 0.00003087
Iteration 85/1000 | Loss: 0.00003087
Iteration 86/1000 | Loss: 0.00003087
Iteration 87/1000 | Loss: 0.00003087
Iteration 88/1000 | Loss: 0.00003087
Iteration 89/1000 | Loss: 0.00003087
Iteration 90/1000 | Loss: 0.00003087
Iteration 91/1000 | Loss: 0.00003087
Iteration 92/1000 | Loss: 0.00003087
Iteration 93/1000 | Loss: 0.00003086
Iteration 94/1000 | Loss: 0.00003086
Iteration 95/1000 | Loss: 0.00003086
Iteration 96/1000 | Loss: 0.00003086
Iteration 97/1000 | Loss: 0.00003086
Iteration 98/1000 | Loss: 0.00003085
Iteration 99/1000 | Loss: 0.00003084
Iteration 100/1000 | Loss: 0.00003084
Iteration 101/1000 | Loss: 0.00003084
Iteration 102/1000 | Loss: 0.00003084
Iteration 103/1000 | Loss: 0.00003083
Iteration 104/1000 | Loss: 0.00008135
Iteration 105/1000 | Loss: 0.00003108
Iteration 106/1000 | Loss: 0.00006900
Iteration 107/1000 | Loss: 0.00003083
Iteration 108/1000 | Loss: 0.00003082
Iteration 109/1000 | Loss: 0.00003081
Iteration 110/1000 | Loss: 0.00003081
Iteration 111/1000 | Loss: 0.00003081
Iteration 112/1000 | Loss: 0.00003080
Iteration 113/1000 | Loss: 0.00003080
Iteration 114/1000 | Loss: 0.00003080
Iteration 115/1000 | Loss: 0.00003080
Iteration 116/1000 | Loss: 0.00003080
Iteration 117/1000 | Loss: 0.00003080
Iteration 118/1000 | Loss: 0.00003080
Iteration 119/1000 | Loss: 0.00003080
Iteration 120/1000 | Loss: 0.00003080
Iteration 121/1000 | Loss: 0.00003080
Iteration 122/1000 | Loss: 0.00003080
Iteration 123/1000 | Loss: 0.00003080
Iteration 124/1000 | Loss: 0.00003080
Iteration 125/1000 | Loss: 0.00003080
Iteration 126/1000 | Loss: 0.00003079
Iteration 127/1000 | Loss: 0.00003079
Iteration 128/1000 | Loss: 0.00003079
Iteration 129/1000 | Loss: 0.00003079
Iteration 130/1000 | Loss: 0.00003079
Iteration 131/1000 | Loss: 0.00003079
Iteration 132/1000 | Loss: 0.00003079
Iteration 133/1000 | Loss: 0.00003079
Iteration 134/1000 | Loss: 0.00003079
Iteration 135/1000 | Loss: 0.00003079
Iteration 136/1000 | Loss: 0.00003079
Iteration 137/1000 | Loss: 0.00003079
Iteration 138/1000 | Loss: 0.00003079
Iteration 139/1000 | Loss: 0.00003079
Iteration 140/1000 | Loss: 0.00003079
Iteration 141/1000 | Loss: 0.00003079
Iteration 142/1000 | Loss: 0.00003079
Iteration 143/1000 | Loss: 0.00003078
Iteration 144/1000 | Loss: 0.00003078
Iteration 145/1000 | Loss: 0.00003078
Iteration 146/1000 | Loss: 0.00003078
Iteration 147/1000 | Loss: 0.00003078
Iteration 148/1000 | Loss: 0.00003078
Iteration 149/1000 | Loss: 0.00003078
Iteration 150/1000 | Loss: 0.00003078
Iteration 151/1000 | Loss: 0.00003078
Iteration 152/1000 | Loss: 0.00003078
Iteration 153/1000 | Loss: 0.00003078
Iteration 154/1000 | Loss: 0.00003078
Iteration 155/1000 | Loss: 0.00003078
Iteration 156/1000 | Loss: 0.00003077
Iteration 157/1000 | Loss: 0.00003077
Iteration 158/1000 | Loss: 0.00003077
Iteration 159/1000 | Loss: 0.00003077
Iteration 160/1000 | Loss: 0.00003077
Iteration 161/1000 | Loss: 0.00003077
Iteration 162/1000 | Loss: 0.00003077
Iteration 163/1000 | Loss: 0.00003077
Iteration 164/1000 | Loss: 0.00003077
Iteration 165/1000 | Loss: 0.00003077
Iteration 166/1000 | Loss: 0.00003077
Iteration 167/1000 | Loss: 0.00003077
Iteration 168/1000 | Loss: 0.00003076
Iteration 169/1000 | Loss: 0.00003076
Iteration 170/1000 | Loss: 0.00003076
Iteration 171/1000 | Loss: 0.00003076
Iteration 172/1000 | Loss: 0.00003076
Iteration 173/1000 | Loss: 0.00003076
Iteration 174/1000 | Loss: 0.00003076
Iteration 175/1000 | Loss: 0.00003076
Iteration 176/1000 | Loss: 0.00003076
Iteration 177/1000 | Loss: 0.00003076
Iteration 178/1000 | Loss: 0.00003076
Iteration 179/1000 | Loss: 0.00003076
Iteration 180/1000 | Loss: 0.00003076
Iteration 181/1000 | Loss: 0.00003076
Iteration 182/1000 | Loss: 0.00003076
Iteration 183/1000 | Loss: 0.00003076
Iteration 184/1000 | Loss: 0.00003076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.0764662369620055e-05, 3.0764662369620055e-05, 3.0764662369620055e-05, 3.0764662369620055e-05, 3.0764662369620055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0764662369620055e-05

Optimization complete. Final v2v error: 4.79150915145874 mm

Highest mean error: 11.575064659118652 mm for frame 122

Lowest mean error: 4.408073902130127 mm for frame 75

Saving results

Total time: 147.88622903823853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00681505
Iteration 2/25 | Loss: 0.00158437
Iteration 3/25 | Loss: 0.00150911
Iteration 4/25 | Loss: 0.00149160
Iteration 5/25 | Loss: 0.00148835
Iteration 6/25 | Loss: 0.00148707
Iteration 7/25 | Loss: 0.00148704
Iteration 8/25 | Loss: 0.00148704
Iteration 9/25 | Loss: 0.00148704
Iteration 10/25 | Loss: 0.00148704
Iteration 11/25 | Loss: 0.00148704
Iteration 12/25 | Loss: 0.00148704
Iteration 13/25 | Loss: 0.00148704
Iteration 14/25 | Loss: 0.00148704
Iteration 15/25 | Loss: 0.00148704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014870387967675924, 0.0014870387967675924, 0.0014870387967675924, 0.0014870387967675924, 0.0014870387967675924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014870387967675924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.35675836
Iteration 2/25 | Loss: 0.00321750
Iteration 3/25 | Loss: 0.00321749
Iteration 4/25 | Loss: 0.00321749
Iteration 5/25 | Loss: 0.00321749
Iteration 6/25 | Loss: 0.00321749
Iteration 7/25 | Loss: 0.00321749
Iteration 8/25 | Loss: 0.00321749
Iteration 9/25 | Loss: 0.00321749
Iteration 10/25 | Loss: 0.00321749
Iteration 11/25 | Loss: 0.00321749
Iteration 12/25 | Loss: 0.00321749
Iteration 13/25 | Loss: 0.00321749
Iteration 14/25 | Loss: 0.00321749
Iteration 15/25 | Loss: 0.00321749
Iteration 16/25 | Loss: 0.00321749
Iteration 17/25 | Loss: 0.00321749
Iteration 18/25 | Loss: 0.00321749
Iteration 19/25 | Loss: 0.00321749
Iteration 20/25 | Loss: 0.00321749
Iteration 21/25 | Loss: 0.00321749
Iteration 22/25 | Loss: 0.00321749
Iteration 23/25 | Loss: 0.00321749
Iteration 24/25 | Loss: 0.00321749
Iteration 25/25 | Loss: 0.00321749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321749
Iteration 2/1000 | Loss: 0.00006089
Iteration 3/1000 | Loss: 0.00004045
Iteration 4/1000 | Loss: 0.00003469
Iteration 5/1000 | Loss: 0.00003199
Iteration 6/1000 | Loss: 0.00003034
Iteration 7/1000 | Loss: 0.00002969
Iteration 8/1000 | Loss: 0.00002940
Iteration 9/1000 | Loss: 0.00002917
Iteration 10/1000 | Loss: 0.00002907
Iteration 11/1000 | Loss: 0.00002896
Iteration 12/1000 | Loss: 0.00002879
Iteration 13/1000 | Loss: 0.00002877
Iteration 14/1000 | Loss: 0.00002872
Iteration 15/1000 | Loss: 0.00002871
Iteration 16/1000 | Loss: 0.00002868
Iteration 17/1000 | Loss: 0.00002868
Iteration 18/1000 | Loss: 0.00002868
Iteration 19/1000 | Loss: 0.00002868
Iteration 20/1000 | Loss: 0.00002867
Iteration 21/1000 | Loss: 0.00002867
Iteration 22/1000 | Loss: 0.00002867
Iteration 23/1000 | Loss: 0.00002867
Iteration 24/1000 | Loss: 0.00002866
Iteration 25/1000 | Loss: 0.00002866
Iteration 26/1000 | Loss: 0.00002865
Iteration 27/1000 | Loss: 0.00002865
Iteration 28/1000 | Loss: 0.00002865
Iteration 29/1000 | Loss: 0.00002865
Iteration 30/1000 | Loss: 0.00002865
Iteration 31/1000 | Loss: 0.00002864
Iteration 32/1000 | Loss: 0.00002864
Iteration 33/1000 | Loss: 0.00002864
Iteration 34/1000 | Loss: 0.00002864
Iteration 35/1000 | Loss: 0.00002864
Iteration 36/1000 | Loss: 0.00002864
Iteration 37/1000 | Loss: 0.00002864
Iteration 38/1000 | Loss: 0.00002863
Iteration 39/1000 | Loss: 0.00002863
Iteration 40/1000 | Loss: 0.00002863
Iteration 41/1000 | Loss: 0.00002863
Iteration 42/1000 | Loss: 0.00002863
Iteration 43/1000 | Loss: 0.00002863
Iteration 44/1000 | Loss: 0.00002863
Iteration 45/1000 | Loss: 0.00002863
Iteration 46/1000 | Loss: 0.00002863
Iteration 47/1000 | Loss: 0.00002863
Iteration 48/1000 | Loss: 0.00002863
Iteration 49/1000 | Loss: 0.00002863
Iteration 50/1000 | Loss: 0.00002863
Iteration 51/1000 | Loss: 0.00002862
Iteration 52/1000 | Loss: 0.00002862
Iteration 53/1000 | Loss: 0.00002862
Iteration 54/1000 | Loss: 0.00002862
Iteration 55/1000 | Loss: 0.00002861
Iteration 56/1000 | Loss: 0.00002861
Iteration 57/1000 | Loss: 0.00002861
Iteration 58/1000 | Loss: 0.00002861
Iteration 59/1000 | Loss: 0.00002861
Iteration 60/1000 | Loss: 0.00002861
Iteration 61/1000 | Loss: 0.00002861
Iteration 62/1000 | Loss: 0.00002861
Iteration 63/1000 | Loss: 0.00002861
Iteration 64/1000 | Loss: 0.00002861
Iteration 65/1000 | Loss: 0.00002861
Iteration 66/1000 | Loss: 0.00002861
Iteration 67/1000 | Loss: 0.00002860
Iteration 68/1000 | Loss: 0.00002860
Iteration 69/1000 | Loss: 0.00002860
Iteration 70/1000 | Loss: 0.00002860
Iteration 71/1000 | Loss: 0.00002860
Iteration 72/1000 | Loss: 0.00002860
Iteration 73/1000 | Loss: 0.00002859
Iteration 74/1000 | Loss: 0.00002859
Iteration 75/1000 | Loss: 0.00002859
Iteration 76/1000 | Loss: 0.00002859
Iteration 77/1000 | Loss: 0.00002859
Iteration 78/1000 | Loss: 0.00002859
Iteration 79/1000 | Loss: 0.00002858
Iteration 80/1000 | Loss: 0.00002858
Iteration 81/1000 | Loss: 0.00002858
Iteration 82/1000 | Loss: 0.00002858
Iteration 83/1000 | Loss: 0.00002858
Iteration 84/1000 | Loss: 0.00002858
Iteration 85/1000 | Loss: 0.00002858
Iteration 86/1000 | Loss: 0.00002858
Iteration 87/1000 | Loss: 0.00002858
Iteration 88/1000 | Loss: 0.00002858
Iteration 89/1000 | Loss: 0.00002858
Iteration 90/1000 | Loss: 0.00002857
Iteration 91/1000 | Loss: 0.00002857
Iteration 92/1000 | Loss: 0.00002857
Iteration 93/1000 | Loss: 0.00002857
Iteration 94/1000 | Loss: 0.00002857
Iteration 95/1000 | Loss: 0.00002857
Iteration 96/1000 | Loss: 0.00002857
Iteration 97/1000 | Loss: 0.00002857
Iteration 98/1000 | Loss: 0.00002857
Iteration 99/1000 | Loss: 0.00002857
Iteration 100/1000 | Loss: 0.00002857
Iteration 101/1000 | Loss: 0.00002857
Iteration 102/1000 | Loss: 0.00002856
Iteration 103/1000 | Loss: 0.00002856
Iteration 104/1000 | Loss: 0.00002856
Iteration 105/1000 | Loss: 0.00002856
Iteration 106/1000 | Loss: 0.00002856
Iteration 107/1000 | Loss: 0.00002856
Iteration 108/1000 | Loss: 0.00002856
Iteration 109/1000 | Loss: 0.00002856
Iteration 110/1000 | Loss: 0.00002856
Iteration 111/1000 | Loss: 0.00002856
Iteration 112/1000 | Loss: 0.00002856
Iteration 113/1000 | Loss: 0.00002856
Iteration 114/1000 | Loss: 0.00002856
Iteration 115/1000 | Loss: 0.00002856
Iteration 116/1000 | Loss: 0.00002856
Iteration 117/1000 | Loss: 0.00002856
Iteration 118/1000 | Loss: 0.00002856
Iteration 119/1000 | Loss: 0.00002856
Iteration 120/1000 | Loss: 0.00002856
Iteration 121/1000 | Loss: 0.00002856
Iteration 122/1000 | Loss: 0.00002856
Iteration 123/1000 | Loss: 0.00002856
Iteration 124/1000 | Loss: 0.00002856
Iteration 125/1000 | Loss: 0.00002856
Iteration 126/1000 | Loss: 0.00002856
Iteration 127/1000 | Loss: 0.00002856
Iteration 128/1000 | Loss: 0.00002856
Iteration 129/1000 | Loss: 0.00002856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.856217179214582e-05, 2.856217179214582e-05, 2.856217179214582e-05, 2.856217179214582e-05, 2.856217179214582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.856217179214582e-05

Optimization complete. Final v2v error: 4.691213607788086 mm

Highest mean error: 4.957529544830322 mm for frame 96

Lowest mean error: 4.486132621765137 mm for frame 78

Saving results

Total time: 31.605398893356323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004460
Iteration 2/25 | Loss: 0.00205658
Iteration 3/25 | Loss: 0.00183314
Iteration 4/25 | Loss: 0.00178188
Iteration 5/25 | Loss: 0.00176398
Iteration 6/25 | Loss: 0.00177191
Iteration 7/25 | Loss: 0.00177026
Iteration 8/25 | Loss: 0.00176586
Iteration 9/25 | Loss: 0.00176559
Iteration 10/25 | Loss: 0.00177301
Iteration 11/25 | Loss: 0.00176656
Iteration 12/25 | Loss: 0.00175864
Iteration 13/25 | Loss: 0.00175448
Iteration 14/25 | Loss: 0.00175235
Iteration 15/25 | Loss: 0.00175181
Iteration 16/25 | Loss: 0.00176511
Iteration 17/25 | Loss: 0.00176574
Iteration 18/25 | Loss: 0.00175633
Iteration 19/25 | Loss: 0.00174704
Iteration 20/25 | Loss: 0.00174886
Iteration 21/25 | Loss: 0.00174862
Iteration 22/25 | Loss: 0.00174348
Iteration 23/25 | Loss: 0.00174207
Iteration 24/25 | Loss: 0.00175112
Iteration 25/25 | Loss: 0.00173904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68531144
Iteration 2/25 | Loss: 0.00603072
Iteration 3/25 | Loss: 0.00603070
Iteration 4/25 | Loss: 0.00603070
Iteration 5/25 | Loss: 0.00603070
Iteration 6/25 | Loss: 0.00603070
Iteration 7/25 | Loss: 0.00603070
Iteration 8/25 | Loss: 0.00603070
Iteration 9/25 | Loss: 0.00603070
Iteration 10/25 | Loss: 0.00603070
Iteration 11/25 | Loss: 0.00603070
Iteration 12/25 | Loss: 0.00603070
Iteration 13/25 | Loss: 0.00603070
Iteration 14/25 | Loss: 0.00603070
Iteration 15/25 | Loss: 0.00603070
Iteration 16/25 | Loss: 0.00603070
Iteration 17/25 | Loss: 0.00603070
Iteration 18/25 | Loss: 0.00603070
Iteration 19/25 | Loss: 0.00603070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.006030702032148838, 0.006030702032148838, 0.006030702032148838, 0.006030702032148838, 0.006030702032148838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006030702032148838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00603070
Iteration 2/1000 | Loss: 0.00095388
Iteration 3/1000 | Loss: 0.00019172
Iteration 4/1000 | Loss: 0.00150554
Iteration 5/1000 | Loss: 0.00015467
Iteration 6/1000 | Loss: 0.00012278
Iteration 7/1000 | Loss: 0.00323603
Iteration 8/1000 | Loss: 0.00213317
Iteration 9/1000 | Loss: 0.00386813
Iteration 10/1000 | Loss: 0.00017273
Iteration 11/1000 | Loss: 0.00333763
Iteration 12/1000 | Loss: 0.01069964
Iteration 13/1000 | Loss: 0.00753067
Iteration 14/1000 | Loss: 0.00575564
Iteration 15/1000 | Loss: 0.00417366
Iteration 16/1000 | Loss: 0.00463471
Iteration 17/1000 | Loss: 0.00290668
Iteration 18/1000 | Loss: 0.00139867
Iteration 19/1000 | Loss: 0.00596422
Iteration 20/1000 | Loss: 0.00368877
Iteration 21/1000 | Loss: 0.00318318
Iteration 22/1000 | Loss: 0.00352619
Iteration 23/1000 | Loss: 0.00061321
Iteration 24/1000 | Loss: 0.00126579
Iteration 25/1000 | Loss: 0.00235760
Iteration 26/1000 | Loss: 0.00230259
Iteration 27/1000 | Loss: 0.00165255
Iteration 28/1000 | Loss: 0.00101370
Iteration 29/1000 | Loss: 0.00158453
Iteration 30/1000 | Loss: 0.00071858
Iteration 31/1000 | Loss: 0.00195712
Iteration 32/1000 | Loss: 0.00110099
Iteration 33/1000 | Loss: 0.00038470
Iteration 34/1000 | Loss: 0.00154058
Iteration 35/1000 | Loss: 0.00145774
Iteration 36/1000 | Loss: 0.00014654
Iteration 37/1000 | Loss: 0.00037106
Iteration 38/1000 | Loss: 0.00078399
Iteration 39/1000 | Loss: 0.00105268
Iteration 40/1000 | Loss: 0.00013490
Iteration 41/1000 | Loss: 0.00010653
Iteration 42/1000 | Loss: 0.00051508
Iteration 43/1000 | Loss: 0.00110960
Iteration 44/1000 | Loss: 0.00110902
Iteration 45/1000 | Loss: 0.00103427
Iteration 46/1000 | Loss: 0.00089031
Iteration 47/1000 | Loss: 0.00012044
Iteration 48/1000 | Loss: 0.00010016
Iteration 49/1000 | Loss: 0.00008851
Iteration 50/1000 | Loss: 0.00041927
Iteration 51/1000 | Loss: 0.00007740
Iteration 52/1000 | Loss: 0.00032852
Iteration 53/1000 | Loss: 0.00037378
Iteration 54/1000 | Loss: 0.00175353
Iteration 55/1000 | Loss: 0.00079428
Iteration 56/1000 | Loss: 0.00136842
Iteration 57/1000 | Loss: 0.00096136
Iteration 58/1000 | Loss: 0.00028048
Iteration 59/1000 | Loss: 0.00026068
Iteration 60/1000 | Loss: 0.00048401
Iteration 61/1000 | Loss: 0.00007690
Iteration 62/1000 | Loss: 0.00006531
Iteration 63/1000 | Loss: 0.00005910
Iteration 64/1000 | Loss: 0.00150424
Iteration 65/1000 | Loss: 0.00033041
Iteration 66/1000 | Loss: 0.00005393
Iteration 67/1000 | Loss: 0.00005088
Iteration 68/1000 | Loss: 0.00004963
Iteration 69/1000 | Loss: 0.00004840
Iteration 70/1000 | Loss: 0.00004735
Iteration 71/1000 | Loss: 0.00004679
Iteration 72/1000 | Loss: 0.00004619
Iteration 73/1000 | Loss: 0.00004547
Iteration 74/1000 | Loss: 0.00079448
Iteration 75/1000 | Loss: 0.00198259
Iteration 76/1000 | Loss: 0.00153009
Iteration 77/1000 | Loss: 0.00010560
Iteration 78/1000 | Loss: 0.00006529
Iteration 79/1000 | Loss: 0.00005643
Iteration 80/1000 | Loss: 0.00182713
Iteration 81/1000 | Loss: 0.00174344
Iteration 82/1000 | Loss: 0.00129005
Iteration 83/1000 | Loss: 0.00138753
Iteration 84/1000 | Loss: 0.00058024
Iteration 85/1000 | Loss: 0.00039295
Iteration 86/1000 | Loss: 0.00051542
Iteration 87/1000 | Loss: 0.00101685
Iteration 88/1000 | Loss: 0.00048846
Iteration 89/1000 | Loss: 0.00145725
Iteration 90/1000 | Loss: 0.00154977
Iteration 91/1000 | Loss: 0.00017270
Iteration 92/1000 | Loss: 0.00135937
Iteration 93/1000 | Loss: 0.00129034
Iteration 94/1000 | Loss: 0.00134684
Iteration 95/1000 | Loss: 0.00119757
Iteration 96/1000 | Loss: 0.00058557
Iteration 97/1000 | Loss: 0.00123490
Iteration 98/1000 | Loss: 0.00150737
Iteration 99/1000 | Loss: 0.00142374
Iteration 100/1000 | Loss: 0.00093720
Iteration 101/1000 | Loss: 0.00076396
Iteration 102/1000 | Loss: 0.00094359
Iteration 103/1000 | Loss: 0.00065086
Iteration 104/1000 | Loss: 0.00126384
Iteration 105/1000 | Loss: 0.00070775
Iteration 106/1000 | Loss: 0.00010533
Iteration 107/1000 | Loss: 0.00024989
Iteration 108/1000 | Loss: 0.00102005
Iteration 109/1000 | Loss: 0.00161880
Iteration 110/1000 | Loss: 0.00122896
Iteration 111/1000 | Loss: 0.00118399
Iteration 112/1000 | Loss: 0.00065187
Iteration 113/1000 | Loss: 0.00107111
Iteration 114/1000 | Loss: 0.00135096
Iteration 115/1000 | Loss: 0.00113284
Iteration 116/1000 | Loss: 0.00112231
Iteration 117/1000 | Loss: 0.00081387
Iteration 118/1000 | Loss: 0.00076086
Iteration 119/1000 | Loss: 0.00063379
Iteration 120/1000 | Loss: 0.00305276
Iteration 121/1000 | Loss: 0.00310872
Iteration 122/1000 | Loss: 0.00065776
Iteration 123/1000 | Loss: 0.00017133
Iteration 124/1000 | Loss: 0.00070615
Iteration 125/1000 | Loss: 0.00009908
Iteration 126/1000 | Loss: 0.00007431
Iteration 127/1000 | Loss: 0.00006436
Iteration 128/1000 | Loss: 0.00006053
Iteration 129/1000 | Loss: 0.00005673
Iteration 130/1000 | Loss: 0.00005385
Iteration 131/1000 | Loss: 0.00032583
Iteration 132/1000 | Loss: 0.00024018
Iteration 133/1000 | Loss: 0.00029230
Iteration 134/1000 | Loss: 0.00006587
Iteration 135/1000 | Loss: 0.00005507
Iteration 136/1000 | Loss: 0.00005186
Iteration 137/1000 | Loss: 0.00005026
Iteration 138/1000 | Loss: 0.00050542
Iteration 139/1000 | Loss: 0.00028740
Iteration 140/1000 | Loss: 0.00120163
Iteration 141/1000 | Loss: 0.00087914
Iteration 142/1000 | Loss: 0.00022379
Iteration 143/1000 | Loss: 0.00043669
Iteration 144/1000 | Loss: 0.00020664
Iteration 145/1000 | Loss: 0.00013337
Iteration 146/1000 | Loss: 0.00005382
Iteration 147/1000 | Loss: 0.00004833
Iteration 148/1000 | Loss: 0.00004551
Iteration 149/1000 | Loss: 0.00040063
Iteration 150/1000 | Loss: 0.00037607
Iteration 151/1000 | Loss: 0.00036058
Iteration 152/1000 | Loss: 0.00027329
Iteration 153/1000 | Loss: 0.00034167
Iteration 154/1000 | Loss: 0.00026544
Iteration 155/1000 | Loss: 0.00034838
Iteration 156/1000 | Loss: 0.00020572
Iteration 157/1000 | Loss: 0.00031343
Iteration 158/1000 | Loss: 0.00006899
Iteration 159/1000 | Loss: 0.00029271
Iteration 160/1000 | Loss: 0.00030101
Iteration 161/1000 | Loss: 0.00023949
Iteration 162/1000 | Loss: 0.00023260
Iteration 163/1000 | Loss: 0.00026090
Iteration 164/1000 | Loss: 0.00022860
Iteration 165/1000 | Loss: 0.00032521
Iteration 166/1000 | Loss: 0.00006555
Iteration 167/1000 | Loss: 0.00004988
Iteration 168/1000 | Loss: 0.00007158
Iteration 169/1000 | Loss: 0.00003979
Iteration 170/1000 | Loss: 0.00003823
Iteration 171/1000 | Loss: 0.00003693
Iteration 172/1000 | Loss: 0.00003640
Iteration 173/1000 | Loss: 0.00003582
Iteration 174/1000 | Loss: 0.00003532
Iteration 175/1000 | Loss: 0.00003501
Iteration 176/1000 | Loss: 0.00003472
Iteration 177/1000 | Loss: 0.00003461
Iteration 178/1000 | Loss: 0.00003458
Iteration 179/1000 | Loss: 0.00003457
Iteration 180/1000 | Loss: 0.00003456
Iteration 181/1000 | Loss: 0.00003452
Iteration 182/1000 | Loss: 0.00003452
Iteration 183/1000 | Loss: 0.00003452
Iteration 184/1000 | Loss: 0.00003451
Iteration 185/1000 | Loss: 0.00003451
Iteration 186/1000 | Loss: 0.00003449
Iteration 187/1000 | Loss: 0.00003449
Iteration 188/1000 | Loss: 0.00003448
Iteration 189/1000 | Loss: 0.00003448
Iteration 190/1000 | Loss: 0.00003447
Iteration 191/1000 | Loss: 0.00003447
Iteration 192/1000 | Loss: 0.00003447
Iteration 193/1000 | Loss: 0.00003447
Iteration 194/1000 | Loss: 0.00003447
Iteration 195/1000 | Loss: 0.00003446
Iteration 196/1000 | Loss: 0.00003446
Iteration 197/1000 | Loss: 0.00003446
Iteration 198/1000 | Loss: 0.00003446
Iteration 199/1000 | Loss: 0.00003446
Iteration 200/1000 | Loss: 0.00003446
Iteration 201/1000 | Loss: 0.00003446
Iteration 202/1000 | Loss: 0.00003445
Iteration 203/1000 | Loss: 0.00003445
Iteration 204/1000 | Loss: 0.00003444
Iteration 205/1000 | Loss: 0.00003444
Iteration 206/1000 | Loss: 0.00003444
Iteration 207/1000 | Loss: 0.00003444
Iteration 208/1000 | Loss: 0.00003443
Iteration 209/1000 | Loss: 0.00003443
Iteration 210/1000 | Loss: 0.00003442
Iteration 211/1000 | Loss: 0.00003442
Iteration 212/1000 | Loss: 0.00003442
Iteration 213/1000 | Loss: 0.00003442
Iteration 214/1000 | Loss: 0.00003442
Iteration 215/1000 | Loss: 0.00003442
Iteration 216/1000 | Loss: 0.00003442
Iteration 217/1000 | Loss: 0.00003442
Iteration 218/1000 | Loss: 0.00003441
Iteration 219/1000 | Loss: 0.00003441
Iteration 220/1000 | Loss: 0.00003441
Iteration 221/1000 | Loss: 0.00003441
Iteration 222/1000 | Loss: 0.00003440
Iteration 223/1000 | Loss: 0.00003440
Iteration 224/1000 | Loss: 0.00003440
Iteration 225/1000 | Loss: 0.00003440
Iteration 226/1000 | Loss: 0.00003440
Iteration 227/1000 | Loss: 0.00003439
Iteration 228/1000 | Loss: 0.00003439
Iteration 229/1000 | Loss: 0.00003439
Iteration 230/1000 | Loss: 0.00003439
Iteration 231/1000 | Loss: 0.00003438
Iteration 232/1000 | Loss: 0.00003438
Iteration 233/1000 | Loss: 0.00003438
Iteration 234/1000 | Loss: 0.00003437
Iteration 235/1000 | Loss: 0.00003437
Iteration 236/1000 | Loss: 0.00003437
Iteration 237/1000 | Loss: 0.00003437
Iteration 238/1000 | Loss: 0.00003436
Iteration 239/1000 | Loss: 0.00003436
Iteration 240/1000 | Loss: 0.00003436
Iteration 241/1000 | Loss: 0.00003436
Iteration 242/1000 | Loss: 0.00003436
Iteration 243/1000 | Loss: 0.00003436
Iteration 244/1000 | Loss: 0.00003436
Iteration 245/1000 | Loss: 0.00003436
Iteration 246/1000 | Loss: 0.00003436
Iteration 247/1000 | Loss: 0.00003436
Iteration 248/1000 | Loss: 0.00003436
Iteration 249/1000 | Loss: 0.00003435
Iteration 250/1000 | Loss: 0.00003435
Iteration 251/1000 | Loss: 0.00003435
Iteration 252/1000 | Loss: 0.00003435
Iteration 253/1000 | Loss: 0.00003435
Iteration 254/1000 | Loss: 0.00003435
Iteration 255/1000 | Loss: 0.00003435
Iteration 256/1000 | Loss: 0.00003435
Iteration 257/1000 | Loss: 0.00003435
Iteration 258/1000 | Loss: 0.00003435
Iteration 259/1000 | Loss: 0.00003435
Iteration 260/1000 | Loss: 0.00003435
Iteration 261/1000 | Loss: 0.00003435
Iteration 262/1000 | Loss: 0.00003434
Iteration 263/1000 | Loss: 0.00003434
Iteration 264/1000 | Loss: 0.00003434
Iteration 265/1000 | Loss: 0.00003434
Iteration 266/1000 | Loss: 0.00003434
Iteration 267/1000 | Loss: 0.00003434
Iteration 268/1000 | Loss: 0.00003434
Iteration 269/1000 | Loss: 0.00003434
Iteration 270/1000 | Loss: 0.00003433
Iteration 271/1000 | Loss: 0.00003433
Iteration 272/1000 | Loss: 0.00003433
Iteration 273/1000 | Loss: 0.00003433
Iteration 274/1000 | Loss: 0.00003433
Iteration 275/1000 | Loss: 0.00003433
Iteration 276/1000 | Loss: 0.00003433
Iteration 277/1000 | Loss: 0.00003433
Iteration 278/1000 | Loss: 0.00003433
Iteration 279/1000 | Loss: 0.00003433
Iteration 280/1000 | Loss: 0.00003433
Iteration 281/1000 | Loss: 0.00003433
Iteration 282/1000 | Loss: 0.00003432
Iteration 283/1000 | Loss: 0.00003432
Iteration 284/1000 | Loss: 0.00003432
Iteration 285/1000 | Loss: 0.00003432
Iteration 286/1000 | Loss: 0.00003432
Iteration 287/1000 | Loss: 0.00003432
Iteration 288/1000 | Loss: 0.00003432
Iteration 289/1000 | Loss: 0.00003432
Iteration 290/1000 | Loss: 0.00003431
Iteration 291/1000 | Loss: 0.00003431
Iteration 292/1000 | Loss: 0.00003431
Iteration 293/1000 | Loss: 0.00003431
Iteration 294/1000 | Loss: 0.00003431
Iteration 295/1000 | Loss: 0.00003431
Iteration 296/1000 | Loss: 0.00003431
Iteration 297/1000 | Loss: 0.00003431
Iteration 298/1000 | Loss: 0.00003431
Iteration 299/1000 | Loss: 0.00003430
Iteration 300/1000 | Loss: 0.00003430
Iteration 301/1000 | Loss: 0.00003430
Iteration 302/1000 | Loss: 0.00003430
Iteration 303/1000 | Loss: 0.00003430
Iteration 304/1000 | Loss: 0.00003430
Iteration 305/1000 | Loss: 0.00003430
Iteration 306/1000 | Loss: 0.00003430
Iteration 307/1000 | Loss: 0.00003430
Iteration 308/1000 | Loss: 0.00003430
Iteration 309/1000 | Loss: 0.00003430
Iteration 310/1000 | Loss: 0.00003430
Iteration 311/1000 | Loss: 0.00003430
Iteration 312/1000 | Loss: 0.00003430
Iteration 313/1000 | Loss: 0.00003429
Iteration 314/1000 | Loss: 0.00003429
Iteration 315/1000 | Loss: 0.00003429
Iteration 316/1000 | Loss: 0.00003429
Iteration 317/1000 | Loss: 0.00003429
Iteration 318/1000 | Loss: 0.00003429
Iteration 319/1000 | Loss: 0.00003429
Iteration 320/1000 | Loss: 0.00003429
Iteration 321/1000 | Loss: 0.00003429
Iteration 322/1000 | Loss: 0.00003429
Iteration 323/1000 | Loss: 0.00003429
Iteration 324/1000 | Loss: 0.00003429
Iteration 325/1000 | Loss: 0.00003429
Iteration 326/1000 | Loss: 0.00003429
Iteration 327/1000 | Loss: 0.00003429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 327. Stopping optimization.
Last 5 losses: [3.428927084314637e-05, 3.428927084314637e-05, 3.428927084314637e-05, 3.428927084314637e-05, 3.428927084314637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.428927084314637e-05

Optimization complete. Final v2v error: 4.735379695892334 mm

Highest mean error: 12.813211441040039 mm for frame 88

Lowest mean error: 4.1826701164245605 mm for frame 73

Saving results

Total time: 300.609778881073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6338/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6338/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073442
Iteration 2/25 | Loss: 0.00222049
Iteration 3/25 | Loss: 0.00192700
Iteration 4/25 | Loss: 0.00188249
Iteration 5/25 | Loss: 0.00184598
Iteration 6/25 | Loss: 0.00181158
Iteration 7/25 | Loss: 0.00179397
Iteration 8/25 | Loss: 0.00178812
Iteration 9/25 | Loss: 0.00178692
Iteration 10/25 | Loss: 0.00178945
Iteration 11/25 | Loss: 0.00178623
Iteration 12/25 | Loss: 0.00178609
Iteration 13/25 | Loss: 0.00179063
Iteration 14/25 | Loss: 0.00178754
Iteration 15/25 | Loss: 0.00178648
Iteration 16/25 | Loss: 0.00178588
Iteration 17/25 | Loss: 0.00178588
Iteration 18/25 | Loss: 0.00178588
Iteration 19/25 | Loss: 0.00178588
Iteration 20/25 | Loss: 0.00178588
Iteration 21/25 | Loss: 0.00178588
Iteration 22/25 | Loss: 0.00178588
Iteration 23/25 | Loss: 0.00178588
Iteration 24/25 | Loss: 0.00178587
Iteration 25/25 | Loss: 0.00178587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10467744
Iteration 2/25 | Loss: 0.00437596
Iteration 3/25 | Loss: 0.00433610
Iteration 4/25 | Loss: 0.00433610
Iteration 5/25 | Loss: 0.00433609
Iteration 6/25 | Loss: 0.00433609
Iteration 7/25 | Loss: 0.00433609
Iteration 8/25 | Loss: 0.00433609
Iteration 9/25 | Loss: 0.00433609
Iteration 10/25 | Loss: 0.00433609
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.004336090292781591, 0.004336090292781591, 0.004336090292781591, 0.004336090292781591, 0.004336090292781591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004336090292781591

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00433609
Iteration 2/1000 | Loss: 0.00048529
Iteration 3/1000 | Loss: 0.00164452
Iteration 4/1000 | Loss: 0.00554320
Iteration 5/1000 | Loss: 0.00292116
Iteration 6/1000 | Loss: 0.00137165
Iteration 7/1000 | Loss: 0.00126451
Iteration 8/1000 | Loss: 0.00061440
Iteration 9/1000 | Loss: 0.00334175
Iteration 10/1000 | Loss: 0.00149938
Iteration 11/1000 | Loss: 0.00043451
Iteration 12/1000 | Loss: 0.00074256
Iteration 13/1000 | Loss: 0.00034180
Iteration 14/1000 | Loss: 0.00007940
Iteration 15/1000 | Loss: 0.00006599
Iteration 16/1000 | Loss: 0.00011831
Iteration 17/1000 | Loss: 0.00005447
Iteration 18/1000 | Loss: 0.00010454
Iteration 19/1000 | Loss: 0.00011775
Iteration 20/1000 | Loss: 0.00004839
Iteration 21/1000 | Loss: 0.00011354
Iteration 22/1000 | Loss: 0.00011681
Iteration 23/1000 | Loss: 0.00029195
Iteration 24/1000 | Loss: 0.00004664
Iteration 25/1000 | Loss: 0.00004589
Iteration 26/1000 | Loss: 0.00004489
Iteration 27/1000 | Loss: 0.00004413
Iteration 28/1000 | Loss: 0.00004344
Iteration 29/1000 | Loss: 0.00004304
Iteration 30/1000 | Loss: 0.00004270
Iteration 31/1000 | Loss: 0.00014765
Iteration 32/1000 | Loss: 0.00004250
Iteration 33/1000 | Loss: 0.00004218
Iteration 34/1000 | Loss: 0.00004212
Iteration 35/1000 | Loss: 0.00004195
Iteration 36/1000 | Loss: 0.00004192
Iteration 37/1000 | Loss: 0.00004186
Iteration 38/1000 | Loss: 0.00004182
Iteration 39/1000 | Loss: 0.00004177
Iteration 40/1000 | Loss: 0.00004177
Iteration 41/1000 | Loss: 0.00004177
Iteration 42/1000 | Loss: 0.00004176
Iteration 43/1000 | Loss: 0.00004175
Iteration 44/1000 | Loss: 0.00004169
Iteration 45/1000 | Loss: 0.00004168
Iteration 46/1000 | Loss: 0.00004168
Iteration 47/1000 | Loss: 0.00004167
Iteration 48/1000 | Loss: 0.00004167
Iteration 49/1000 | Loss: 0.00004166
Iteration 50/1000 | Loss: 0.00004166
Iteration 51/1000 | Loss: 0.00004165
Iteration 52/1000 | Loss: 0.00004165
Iteration 53/1000 | Loss: 0.00004164
Iteration 54/1000 | Loss: 0.00004164
Iteration 55/1000 | Loss: 0.00004163
Iteration 56/1000 | Loss: 0.00004157
Iteration 57/1000 | Loss: 0.00004157
Iteration 58/1000 | Loss: 0.00004156
Iteration 59/1000 | Loss: 0.00004156
Iteration 60/1000 | Loss: 0.00004154
Iteration 61/1000 | Loss: 0.00004153
Iteration 62/1000 | Loss: 0.00004153
Iteration 63/1000 | Loss: 0.00004153
Iteration 64/1000 | Loss: 0.00004152
Iteration 65/1000 | Loss: 0.00004152
Iteration 66/1000 | Loss: 0.00004152
Iteration 67/1000 | Loss: 0.00004152
Iteration 68/1000 | Loss: 0.00004152
Iteration 69/1000 | Loss: 0.00004151
Iteration 70/1000 | Loss: 0.00004151
Iteration 71/1000 | Loss: 0.00004151
Iteration 72/1000 | Loss: 0.00004150
Iteration 73/1000 | Loss: 0.00004150
Iteration 74/1000 | Loss: 0.00004150
Iteration 75/1000 | Loss: 0.00004149
Iteration 76/1000 | Loss: 0.00004149
Iteration 77/1000 | Loss: 0.00004149
Iteration 78/1000 | Loss: 0.00004148
Iteration 79/1000 | Loss: 0.00004148
Iteration 80/1000 | Loss: 0.00004146
Iteration 81/1000 | Loss: 0.00004146
Iteration 82/1000 | Loss: 0.00004146
Iteration 83/1000 | Loss: 0.00004146
Iteration 84/1000 | Loss: 0.00004146
Iteration 85/1000 | Loss: 0.00004145
Iteration 86/1000 | Loss: 0.00004145
Iteration 87/1000 | Loss: 0.00004145
Iteration 88/1000 | Loss: 0.00004144
Iteration 89/1000 | Loss: 0.00004144
Iteration 90/1000 | Loss: 0.00004143
Iteration 91/1000 | Loss: 0.00004143
Iteration 92/1000 | Loss: 0.00004143
Iteration 93/1000 | Loss: 0.00004143
Iteration 94/1000 | Loss: 0.00004143
Iteration 95/1000 | Loss: 0.00004143
Iteration 96/1000 | Loss: 0.00004142
Iteration 97/1000 | Loss: 0.00004142
Iteration 98/1000 | Loss: 0.00004142
Iteration 99/1000 | Loss: 0.00004142
Iteration 100/1000 | Loss: 0.00004141
Iteration 101/1000 | Loss: 0.00004141
Iteration 102/1000 | Loss: 0.00004141
Iteration 103/1000 | Loss: 0.00004141
Iteration 104/1000 | Loss: 0.00004141
Iteration 105/1000 | Loss: 0.00004140
Iteration 106/1000 | Loss: 0.00004140
Iteration 107/1000 | Loss: 0.00004140
Iteration 108/1000 | Loss: 0.00004140
Iteration 109/1000 | Loss: 0.00004140
Iteration 110/1000 | Loss: 0.00004140
Iteration 111/1000 | Loss: 0.00004140
Iteration 112/1000 | Loss: 0.00004140
Iteration 113/1000 | Loss: 0.00004139
Iteration 114/1000 | Loss: 0.00004139
Iteration 115/1000 | Loss: 0.00004139
Iteration 116/1000 | Loss: 0.00004139
Iteration 117/1000 | Loss: 0.00004139
Iteration 118/1000 | Loss: 0.00004138
Iteration 119/1000 | Loss: 0.00004138
Iteration 120/1000 | Loss: 0.00004138
Iteration 121/1000 | Loss: 0.00004138
Iteration 122/1000 | Loss: 0.00004138
Iteration 123/1000 | Loss: 0.00004138
Iteration 124/1000 | Loss: 0.00004138
Iteration 125/1000 | Loss: 0.00004138
Iteration 126/1000 | Loss: 0.00004138
Iteration 127/1000 | Loss: 0.00004137
Iteration 128/1000 | Loss: 0.00004137
Iteration 129/1000 | Loss: 0.00004137
Iteration 130/1000 | Loss: 0.00004137
Iteration 131/1000 | Loss: 0.00004137
Iteration 132/1000 | Loss: 0.00004136
Iteration 133/1000 | Loss: 0.00004136
Iteration 134/1000 | Loss: 0.00004136
Iteration 135/1000 | Loss: 0.00004136
Iteration 136/1000 | Loss: 0.00004136
Iteration 137/1000 | Loss: 0.00004136
Iteration 138/1000 | Loss: 0.00004136
Iteration 139/1000 | Loss: 0.00004136
Iteration 140/1000 | Loss: 0.00004136
Iteration 141/1000 | Loss: 0.00004136
Iteration 142/1000 | Loss: 0.00004136
Iteration 143/1000 | Loss: 0.00004135
Iteration 144/1000 | Loss: 0.00004135
Iteration 145/1000 | Loss: 0.00004135
Iteration 146/1000 | Loss: 0.00004135
Iteration 147/1000 | Loss: 0.00004135
Iteration 148/1000 | Loss: 0.00004135
Iteration 149/1000 | Loss: 0.00004135
Iteration 150/1000 | Loss: 0.00004135
Iteration 151/1000 | Loss: 0.00004135
Iteration 152/1000 | Loss: 0.00004135
Iteration 153/1000 | Loss: 0.00004135
Iteration 154/1000 | Loss: 0.00004135
Iteration 155/1000 | Loss: 0.00004135
Iteration 156/1000 | Loss: 0.00004135
Iteration 157/1000 | Loss: 0.00004135
Iteration 158/1000 | Loss: 0.00004135
Iteration 159/1000 | Loss: 0.00004135
Iteration 160/1000 | Loss: 0.00004135
Iteration 161/1000 | Loss: 0.00004135
Iteration 162/1000 | Loss: 0.00004135
Iteration 163/1000 | Loss: 0.00004135
Iteration 164/1000 | Loss: 0.00004134
Iteration 165/1000 | Loss: 0.00004134
Iteration 166/1000 | Loss: 0.00004134
Iteration 167/1000 | Loss: 0.00004134
Iteration 168/1000 | Loss: 0.00004134
Iteration 169/1000 | Loss: 0.00004134
Iteration 170/1000 | Loss: 0.00004134
Iteration 171/1000 | Loss: 0.00004134
Iteration 172/1000 | Loss: 0.00004134
Iteration 173/1000 | Loss: 0.00004134
Iteration 174/1000 | Loss: 0.00004134
Iteration 175/1000 | Loss: 0.00004134
Iteration 176/1000 | Loss: 0.00004134
Iteration 177/1000 | Loss: 0.00004134
Iteration 178/1000 | Loss: 0.00004134
Iteration 179/1000 | Loss: 0.00004134
Iteration 180/1000 | Loss: 0.00004134
Iteration 181/1000 | Loss: 0.00004134
Iteration 182/1000 | Loss: 0.00004134
Iteration 183/1000 | Loss: 0.00004134
Iteration 184/1000 | Loss: 0.00004134
Iteration 185/1000 | Loss: 0.00004134
Iteration 186/1000 | Loss: 0.00004134
Iteration 187/1000 | Loss: 0.00004134
Iteration 188/1000 | Loss: 0.00004134
Iteration 189/1000 | Loss: 0.00004134
Iteration 190/1000 | Loss: 0.00004134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [4.1343704651808366e-05, 4.1343704651808366e-05, 4.1343704651808366e-05, 4.1343704651808366e-05, 4.1343704651808366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1343704651808366e-05

Optimization complete. Final v2v error: 5.520822525024414 mm

Highest mean error: 6.452259540557861 mm for frame 0

Lowest mean error: 4.545015335083008 mm for frame 204

Saving results

Total time: 101.4576027393341
