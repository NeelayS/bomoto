Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=94, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5264-5319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783145
Iteration 2/25 | Loss: 0.00124447
Iteration 3/25 | Loss: 0.00102501
Iteration 4/25 | Loss: 0.00099173
Iteration 5/25 | Loss: 0.00096788
Iteration 6/25 | Loss: 0.00096410
Iteration 7/25 | Loss: 0.00096144
Iteration 8/25 | Loss: 0.00096075
Iteration 9/25 | Loss: 0.00096039
Iteration 10/25 | Loss: 0.00096018
Iteration 11/25 | Loss: 0.00096016
Iteration 12/25 | Loss: 0.00096016
Iteration 13/25 | Loss: 0.00096016
Iteration 14/25 | Loss: 0.00096016
Iteration 15/25 | Loss: 0.00096016
Iteration 16/25 | Loss: 0.00096016
Iteration 17/25 | Loss: 0.00096016
Iteration 18/25 | Loss: 0.00096015
Iteration 19/25 | Loss: 0.00096015
Iteration 20/25 | Loss: 0.00096015
Iteration 21/25 | Loss: 0.00096015
Iteration 22/25 | Loss: 0.00096015
Iteration 23/25 | Loss: 0.00096015
Iteration 24/25 | Loss: 0.00096015
Iteration 25/25 | Loss: 0.00096015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.15266037
Iteration 2/25 | Loss: 0.00060988
Iteration 3/25 | Loss: 0.00060988
Iteration 4/25 | Loss: 0.00060988
Iteration 5/25 | Loss: 0.00060988
Iteration 6/25 | Loss: 0.00060988
Iteration 7/25 | Loss: 0.00060988
Iteration 8/25 | Loss: 0.00060988
Iteration 9/25 | Loss: 0.00060988
Iteration 10/25 | Loss: 0.00060988
Iteration 11/25 | Loss: 0.00060988
Iteration 12/25 | Loss: 0.00060988
Iteration 13/25 | Loss: 0.00060988
Iteration 14/25 | Loss: 0.00060988
Iteration 15/25 | Loss: 0.00060988
Iteration 16/25 | Loss: 0.00060988
Iteration 17/25 | Loss: 0.00060988
Iteration 18/25 | Loss: 0.00060988
Iteration 19/25 | Loss: 0.00060988
Iteration 20/25 | Loss: 0.00060988
Iteration 21/25 | Loss: 0.00060988
Iteration 22/25 | Loss: 0.00060988
Iteration 23/25 | Loss: 0.00060988
Iteration 24/25 | Loss: 0.00060988
Iteration 25/25 | Loss: 0.00060988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060988
Iteration 2/1000 | Loss: 0.00002839
Iteration 3/1000 | Loss: 0.00001940
Iteration 4/1000 | Loss: 0.00001782
Iteration 5/1000 | Loss: 0.00001729
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001684
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001655
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001644
Iteration 15/1000 | Loss: 0.00001643
Iteration 16/1000 | Loss: 0.00001642
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001641
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001638
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001638
Iteration 24/1000 | Loss: 0.00001637
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001635
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001633
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001632
Iteration 35/1000 | Loss: 0.00001632
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001631
Iteration 38/1000 | Loss: 0.00001631
Iteration 39/1000 | Loss: 0.00001631
Iteration 40/1000 | Loss: 0.00001630
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001628
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001625
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001620
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001618
Iteration 82/1000 | Loss: 0.00001618
Iteration 83/1000 | Loss: 0.00001618
Iteration 84/1000 | Loss: 0.00001618
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.6176070857909508e-05, 1.6176070857909508e-05, 1.6176070857909508e-05, 1.6176070857909508e-05, 1.6176070857909508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6176070857909508e-05

Optimization complete. Final v2v error: 3.3885250091552734 mm

Highest mean error: 3.7757484912872314 mm for frame 186

Lowest mean error: 3.2094192504882812 mm for frame 12

Saving results

Total time: 39.21689438819885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860208
Iteration 2/25 | Loss: 0.00152226
Iteration 3/25 | Loss: 0.00125807
Iteration 4/25 | Loss: 0.00124969
Iteration 5/25 | Loss: 0.00124693
Iteration 6/25 | Loss: 0.00124600
Iteration 7/25 | Loss: 0.00124593
Iteration 8/25 | Loss: 0.00124593
Iteration 9/25 | Loss: 0.00124593
Iteration 10/25 | Loss: 0.00124593
Iteration 11/25 | Loss: 0.00124593
Iteration 12/25 | Loss: 0.00124593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012459263671189547, 0.0012459263671189547, 0.0012459263671189547, 0.0012459263671189547, 0.0012459263671189547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012459263671189547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50532413
Iteration 2/25 | Loss: 0.00064591
Iteration 3/25 | Loss: 0.00064591
Iteration 4/25 | Loss: 0.00064591
Iteration 5/25 | Loss: 0.00064591
Iteration 6/25 | Loss: 0.00064591
Iteration 7/25 | Loss: 0.00064591
Iteration 8/25 | Loss: 0.00064591
Iteration 9/25 | Loss: 0.00064591
Iteration 10/25 | Loss: 0.00064591
Iteration 11/25 | Loss: 0.00064591
Iteration 12/25 | Loss: 0.00064591
Iteration 13/25 | Loss: 0.00064591
Iteration 14/25 | Loss: 0.00064591
Iteration 15/25 | Loss: 0.00064591
Iteration 16/25 | Loss: 0.00064591
Iteration 17/25 | Loss: 0.00064591
Iteration 18/25 | Loss: 0.00064591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006459085270762444, 0.0006459085270762444, 0.0006459085270762444, 0.0006459085270762444, 0.0006459085270762444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006459085270762444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064591
Iteration 2/1000 | Loss: 0.00009851
Iteration 3/1000 | Loss: 0.00006857
Iteration 4/1000 | Loss: 0.00005811
Iteration 5/1000 | Loss: 0.00005390
Iteration 6/1000 | Loss: 0.00005233
Iteration 7/1000 | Loss: 0.00005152
Iteration 8/1000 | Loss: 0.00005057
Iteration 9/1000 | Loss: 0.00004962
Iteration 10/1000 | Loss: 0.00004882
Iteration 11/1000 | Loss: 0.00004791
Iteration 12/1000 | Loss: 0.00004713
Iteration 13/1000 | Loss: 0.00004643
Iteration 14/1000 | Loss: 0.00004577
Iteration 15/1000 | Loss: 0.00004516
Iteration 16/1000 | Loss: 0.00004477
Iteration 17/1000 | Loss: 0.00004449
Iteration 18/1000 | Loss: 0.00004406
Iteration 19/1000 | Loss: 0.00004379
Iteration 20/1000 | Loss: 0.00004365
Iteration 21/1000 | Loss: 0.00004357
Iteration 22/1000 | Loss: 0.00004342
Iteration 23/1000 | Loss: 0.00004342
Iteration 24/1000 | Loss: 0.00004326
Iteration 25/1000 | Loss: 0.00004325
Iteration 26/1000 | Loss: 0.00004323
Iteration 27/1000 | Loss: 0.00004320
Iteration 28/1000 | Loss: 0.00004313
Iteration 29/1000 | Loss: 0.00004309
Iteration 30/1000 | Loss: 0.00004308
Iteration 31/1000 | Loss: 0.00004308
Iteration 32/1000 | Loss: 0.00004308
Iteration 33/1000 | Loss: 0.00004308
Iteration 34/1000 | Loss: 0.00004307
Iteration 35/1000 | Loss: 0.00004307
Iteration 36/1000 | Loss: 0.00004307
Iteration 37/1000 | Loss: 0.00004307
Iteration 38/1000 | Loss: 0.00004307
Iteration 39/1000 | Loss: 0.00004307
Iteration 40/1000 | Loss: 0.00004307
Iteration 41/1000 | Loss: 0.00004306
Iteration 42/1000 | Loss: 0.00004306
Iteration 43/1000 | Loss: 0.00004304
Iteration 44/1000 | Loss: 0.00004304
Iteration 45/1000 | Loss: 0.00004304
Iteration 46/1000 | Loss: 0.00004304
Iteration 47/1000 | Loss: 0.00004304
Iteration 48/1000 | Loss: 0.00004304
Iteration 49/1000 | Loss: 0.00004304
Iteration 50/1000 | Loss: 0.00004304
Iteration 51/1000 | Loss: 0.00004304
Iteration 52/1000 | Loss: 0.00004304
Iteration 53/1000 | Loss: 0.00004304
Iteration 54/1000 | Loss: 0.00004304
Iteration 55/1000 | Loss: 0.00004304
Iteration 56/1000 | Loss: 0.00004303
Iteration 57/1000 | Loss: 0.00004303
Iteration 58/1000 | Loss: 0.00004303
Iteration 59/1000 | Loss: 0.00004302
Iteration 60/1000 | Loss: 0.00004302
Iteration 61/1000 | Loss: 0.00004302
Iteration 62/1000 | Loss: 0.00004302
Iteration 63/1000 | Loss: 0.00004302
Iteration 64/1000 | Loss: 0.00004302
Iteration 65/1000 | Loss: 0.00004302
Iteration 66/1000 | Loss: 0.00004302
Iteration 67/1000 | Loss: 0.00004302
Iteration 68/1000 | Loss: 0.00004302
Iteration 69/1000 | Loss: 0.00004302
Iteration 70/1000 | Loss: 0.00004302
Iteration 71/1000 | Loss: 0.00004301
Iteration 72/1000 | Loss: 0.00004301
Iteration 73/1000 | Loss: 0.00004301
Iteration 74/1000 | Loss: 0.00004301
Iteration 75/1000 | Loss: 0.00004301
Iteration 76/1000 | Loss: 0.00004300
Iteration 77/1000 | Loss: 0.00004300
Iteration 78/1000 | Loss: 0.00004300
Iteration 79/1000 | Loss: 0.00004300
Iteration 80/1000 | Loss: 0.00004300
Iteration 81/1000 | Loss: 0.00004300
Iteration 82/1000 | Loss: 0.00004300
Iteration 83/1000 | Loss: 0.00004300
Iteration 84/1000 | Loss: 0.00004300
Iteration 85/1000 | Loss: 0.00004299
Iteration 86/1000 | Loss: 0.00004299
Iteration 87/1000 | Loss: 0.00004299
Iteration 88/1000 | Loss: 0.00004299
Iteration 89/1000 | Loss: 0.00004299
Iteration 90/1000 | Loss: 0.00004298
Iteration 91/1000 | Loss: 0.00004298
Iteration 92/1000 | Loss: 0.00004298
Iteration 93/1000 | Loss: 0.00004298
Iteration 94/1000 | Loss: 0.00004298
Iteration 95/1000 | Loss: 0.00004298
Iteration 96/1000 | Loss: 0.00004297
Iteration 97/1000 | Loss: 0.00004297
Iteration 98/1000 | Loss: 0.00004297
Iteration 99/1000 | Loss: 0.00004296
Iteration 100/1000 | Loss: 0.00004296
Iteration 101/1000 | Loss: 0.00004296
Iteration 102/1000 | Loss: 0.00004296
Iteration 103/1000 | Loss: 0.00004296
Iteration 104/1000 | Loss: 0.00004296
Iteration 105/1000 | Loss: 0.00004296
Iteration 106/1000 | Loss: 0.00004296
Iteration 107/1000 | Loss: 0.00004296
Iteration 108/1000 | Loss: 0.00004295
Iteration 109/1000 | Loss: 0.00004295
Iteration 110/1000 | Loss: 0.00004295
Iteration 111/1000 | Loss: 0.00004295
Iteration 112/1000 | Loss: 0.00004295
Iteration 113/1000 | Loss: 0.00004294
Iteration 114/1000 | Loss: 0.00004294
Iteration 115/1000 | Loss: 0.00004294
Iteration 116/1000 | Loss: 0.00004294
Iteration 117/1000 | Loss: 0.00004294
Iteration 118/1000 | Loss: 0.00004294
Iteration 119/1000 | Loss: 0.00004294
Iteration 120/1000 | Loss: 0.00004294
Iteration 121/1000 | Loss: 0.00004294
Iteration 122/1000 | Loss: 0.00004294
Iteration 123/1000 | Loss: 0.00004294
Iteration 124/1000 | Loss: 0.00004294
Iteration 125/1000 | Loss: 0.00004294
Iteration 126/1000 | Loss: 0.00004294
Iteration 127/1000 | Loss: 0.00004294
Iteration 128/1000 | Loss: 0.00004293
Iteration 129/1000 | Loss: 0.00004293
Iteration 130/1000 | Loss: 0.00004293
Iteration 131/1000 | Loss: 0.00004293
Iteration 132/1000 | Loss: 0.00004293
Iteration 133/1000 | Loss: 0.00004293
Iteration 134/1000 | Loss: 0.00004293
Iteration 135/1000 | Loss: 0.00004293
Iteration 136/1000 | Loss: 0.00004293
Iteration 137/1000 | Loss: 0.00004293
Iteration 138/1000 | Loss: 0.00004293
Iteration 139/1000 | Loss: 0.00004293
Iteration 140/1000 | Loss: 0.00004293
Iteration 141/1000 | Loss: 0.00004293
Iteration 142/1000 | Loss: 0.00004293
Iteration 143/1000 | Loss: 0.00004293
Iteration 144/1000 | Loss: 0.00004293
Iteration 145/1000 | Loss: 0.00004293
Iteration 146/1000 | Loss: 0.00004293
Iteration 147/1000 | Loss: 0.00004293
Iteration 148/1000 | Loss: 0.00004293
Iteration 149/1000 | Loss: 0.00004293
Iteration 150/1000 | Loss: 0.00004293
Iteration 151/1000 | Loss: 0.00004293
Iteration 152/1000 | Loss: 0.00004293
Iteration 153/1000 | Loss: 0.00004293
Iteration 154/1000 | Loss: 0.00004293
Iteration 155/1000 | Loss: 0.00004293
Iteration 156/1000 | Loss: 0.00004293
Iteration 157/1000 | Loss: 0.00004293
Iteration 158/1000 | Loss: 0.00004293
Iteration 159/1000 | Loss: 0.00004293
Iteration 160/1000 | Loss: 0.00004293
Iteration 161/1000 | Loss: 0.00004293
Iteration 162/1000 | Loss: 0.00004293
Iteration 163/1000 | Loss: 0.00004293
Iteration 164/1000 | Loss: 0.00004293
Iteration 165/1000 | Loss: 0.00004293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [4.292746234568767e-05, 4.292746234568767e-05, 4.292746234568767e-05, 4.292746234568767e-05, 4.292746234568767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.292746234568767e-05

Optimization complete. Final v2v error: 5.288141250610352 mm

Highest mean error: 6.502048969268799 mm for frame 143

Lowest mean error: 5.013302803039551 mm for frame 8

Saving results

Total time: 51.94342803955078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544423
Iteration 2/25 | Loss: 0.00114516
Iteration 3/25 | Loss: 0.00100397
Iteration 4/25 | Loss: 0.00099440
Iteration 5/25 | Loss: 0.00099108
Iteration 6/25 | Loss: 0.00099068
Iteration 7/25 | Loss: 0.00099068
Iteration 8/25 | Loss: 0.00099068
Iteration 9/25 | Loss: 0.00099068
Iteration 10/25 | Loss: 0.00099068
Iteration 11/25 | Loss: 0.00099068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000990681117400527, 0.000990681117400527, 0.000990681117400527, 0.000990681117400527, 0.000990681117400527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000990681117400527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90359187
Iteration 2/25 | Loss: 0.00053458
Iteration 3/25 | Loss: 0.00053456
Iteration 4/25 | Loss: 0.00053456
Iteration 5/25 | Loss: 0.00053456
Iteration 6/25 | Loss: 0.00053456
Iteration 7/25 | Loss: 0.00053456
Iteration 8/25 | Loss: 0.00053456
Iteration 9/25 | Loss: 0.00053456
Iteration 10/25 | Loss: 0.00053456
Iteration 11/25 | Loss: 0.00053456
Iteration 12/25 | Loss: 0.00053456
Iteration 13/25 | Loss: 0.00053456
Iteration 14/25 | Loss: 0.00053456
Iteration 15/25 | Loss: 0.00053456
Iteration 16/25 | Loss: 0.00053456
Iteration 17/25 | Loss: 0.00053456
Iteration 18/25 | Loss: 0.00053456
Iteration 19/25 | Loss: 0.00053456
Iteration 20/25 | Loss: 0.00053456
Iteration 21/25 | Loss: 0.00053456
Iteration 22/25 | Loss: 0.00053456
Iteration 23/25 | Loss: 0.00053456
Iteration 24/25 | Loss: 0.00053456
Iteration 25/25 | Loss: 0.00053456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053456
Iteration 2/1000 | Loss: 0.00003466
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00001991
Iteration 5/1000 | Loss: 0.00001887
Iteration 6/1000 | Loss: 0.00001811
Iteration 7/1000 | Loss: 0.00001768
Iteration 8/1000 | Loss: 0.00001743
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001703
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001700
Iteration 15/1000 | Loss: 0.00001691
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001687
Iteration 18/1000 | Loss: 0.00001686
Iteration 19/1000 | Loss: 0.00001686
Iteration 20/1000 | Loss: 0.00001684
Iteration 21/1000 | Loss: 0.00001683
Iteration 22/1000 | Loss: 0.00001682
Iteration 23/1000 | Loss: 0.00001682
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001682
Iteration 26/1000 | Loss: 0.00001682
Iteration 27/1000 | Loss: 0.00001682
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001682
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001681
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001679
Iteration 37/1000 | Loss: 0.00001679
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001678
Iteration 43/1000 | Loss: 0.00001677
Iteration 44/1000 | Loss: 0.00001677
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001677
Iteration 47/1000 | Loss: 0.00001677
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001674
Iteration 56/1000 | Loss: 0.00001673
Iteration 57/1000 | Loss: 0.00001672
Iteration 58/1000 | Loss: 0.00001672
Iteration 59/1000 | Loss: 0.00001671
Iteration 60/1000 | Loss: 0.00001671
Iteration 61/1000 | Loss: 0.00001671
Iteration 62/1000 | Loss: 0.00001671
Iteration 63/1000 | Loss: 0.00001670
Iteration 64/1000 | Loss: 0.00001670
Iteration 65/1000 | Loss: 0.00001670
Iteration 66/1000 | Loss: 0.00001670
Iteration 67/1000 | Loss: 0.00001669
Iteration 68/1000 | Loss: 0.00001669
Iteration 69/1000 | Loss: 0.00001668
Iteration 70/1000 | Loss: 0.00001668
Iteration 71/1000 | Loss: 0.00001668
Iteration 72/1000 | Loss: 0.00001668
Iteration 73/1000 | Loss: 0.00001668
Iteration 74/1000 | Loss: 0.00001668
Iteration 75/1000 | Loss: 0.00001668
Iteration 76/1000 | Loss: 0.00001668
Iteration 77/1000 | Loss: 0.00001668
Iteration 78/1000 | Loss: 0.00001668
Iteration 79/1000 | Loss: 0.00001667
Iteration 80/1000 | Loss: 0.00001667
Iteration 81/1000 | Loss: 0.00001667
Iteration 82/1000 | Loss: 0.00001667
Iteration 83/1000 | Loss: 0.00001667
Iteration 84/1000 | Loss: 0.00001666
Iteration 85/1000 | Loss: 0.00001666
Iteration 86/1000 | Loss: 0.00001666
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001665
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001665
Iteration 96/1000 | Loss: 0.00001665
Iteration 97/1000 | Loss: 0.00001665
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001664
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001664
Iteration 112/1000 | Loss: 0.00001664
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001663
Iteration 116/1000 | Loss: 0.00001663
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001662
Iteration 123/1000 | Loss: 0.00001662
Iteration 124/1000 | Loss: 0.00001662
Iteration 125/1000 | Loss: 0.00001662
Iteration 126/1000 | Loss: 0.00001662
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001661
Iteration 130/1000 | Loss: 0.00001661
Iteration 131/1000 | Loss: 0.00001661
Iteration 132/1000 | Loss: 0.00001661
Iteration 133/1000 | Loss: 0.00001661
Iteration 134/1000 | Loss: 0.00001661
Iteration 135/1000 | Loss: 0.00001661
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001661
Iteration 138/1000 | Loss: 0.00001661
Iteration 139/1000 | Loss: 0.00001660
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001660
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Iteration 144/1000 | Loss: 0.00001660
Iteration 145/1000 | Loss: 0.00001660
Iteration 146/1000 | Loss: 0.00001660
Iteration 147/1000 | Loss: 0.00001660
Iteration 148/1000 | Loss: 0.00001660
Iteration 149/1000 | Loss: 0.00001660
Iteration 150/1000 | Loss: 0.00001660
Iteration 151/1000 | Loss: 0.00001660
Iteration 152/1000 | Loss: 0.00001660
Iteration 153/1000 | Loss: 0.00001660
Iteration 154/1000 | Loss: 0.00001660
Iteration 155/1000 | Loss: 0.00001660
Iteration 156/1000 | Loss: 0.00001660
Iteration 157/1000 | Loss: 0.00001660
Iteration 158/1000 | Loss: 0.00001660
Iteration 159/1000 | Loss: 0.00001660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.6599680748186074e-05, 1.6599680748186074e-05, 1.6599680748186074e-05, 1.6599680748186074e-05, 1.6599680748186074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6599680748186074e-05

Optimization complete. Final v2v error: 3.344097137451172 mm

Highest mean error: 3.775689125061035 mm for frame 164

Lowest mean error: 3.0147805213928223 mm for frame 221

Saving results

Total time: 38.30056810379028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886333
Iteration 2/25 | Loss: 0.00092779
Iteration 3/25 | Loss: 0.00082724
Iteration 4/25 | Loss: 0.00080713
Iteration 5/25 | Loss: 0.00080007
Iteration 6/25 | Loss: 0.00079817
Iteration 7/25 | Loss: 0.00079748
Iteration 8/25 | Loss: 0.00079743
Iteration 9/25 | Loss: 0.00079743
Iteration 10/25 | Loss: 0.00079743
Iteration 11/25 | Loss: 0.00079743
Iteration 12/25 | Loss: 0.00079743
Iteration 13/25 | Loss: 0.00079743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007974326144903898, 0.0007974326144903898, 0.0007974326144903898, 0.0007974326144903898, 0.0007974326144903898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007974326144903898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35845184
Iteration 2/25 | Loss: 0.00050922
Iteration 3/25 | Loss: 0.00050921
Iteration 4/25 | Loss: 0.00050921
Iteration 5/25 | Loss: 0.00050921
Iteration 6/25 | Loss: 0.00050921
Iteration 7/25 | Loss: 0.00050921
Iteration 8/25 | Loss: 0.00050921
Iteration 9/25 | Loss: 0.00050921
Iteration 10/25 | Loss: 0.00050921
Iteration 11/25 | Loss: 0.00050921
Iteration 12/25 | Loss: 0.00050921
Iteration 13/25 | Loss: 0.00050921
Iteration 14/25 | Loss: 0.00050921
Iteration 15/25 | Loss: 0.00050921
Iteration 16/25 | Loss: 0.00050921
Iteration 17/25 | Loss: 0.00050921
Iteration 18/25 | Loss: 0.00050921
Iteration 19/25 | Loss: 0.00050921
Iteration 20/25 | Loss: 0.00050921
Iteration 21/25 | Loss: 0.00050921
Iteration 22/25 | Loss: 0.00050921
Iteration 23/25 | Loss: 0.00050921
Iteration 24/25 | Loss: 0.00050921
Iteration 25/25 | Loss: 0.00050921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050921
Iteration 2/1000 | Loss: 0.00003524
Iteration 3/1000 | Loss: 0.00002368
Iteration 4/1000 | Loss: 0.00001918
Iteration 5/1000 | Loss: 0.00001763
Iteration 6/1000 | Loss: 0.00001671
Iteration 7/1000 | Loss: 0.00001582
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001383
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001349
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001340
Iteration 35/1000 | Loss: 0.00001339
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001337
Iteration 38/1000 | Loss: 0.00001337
Iteration 39/1000 | Loss: 0.00001336
Iteration 40/1000 | Loss: 0.00001336
Iteration 41/1000 | Loss: 0.00001335
Iteration 42/1000 | Loss: 0.00001335
Iteration 43/1000 | Loss: 0.00001335
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00001335
Iteration 46/1000 | Loss: 0.00001335
Iteration 47/1000 | Loss: 0.00001334
Iteration 48/1000 | Loss: 0.00001334
Iteration 49/1000 | Loss: 0.00001334
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001333
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00001333
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001331
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001322
Iteration 123/1000 | Loss: 0.00001322
Iteration 124/1000 | Loss: 0.00001322
Iteration 125/1000 | Loss: 0.00001322
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.3222496818343643e-05, 1.3222496818343643e-05, 1.3222496818343643e-05, 1.3222496818343643e-05, 1.3222496818343643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3222496818343643e-05

Optimization complete. Final v2v error: 3.008800983428955 mm

Highest mean error: 3.4744789600372314 mm for frame 111

Lowest mean error: 2.383810520172119 mm for frame 69

Saving results

Total time: 41.329965353012085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374554
Iteration 2/25 | Loss: 0.00100286
Iteration 3/25 | Loss: 0.00086208
Iteration 4/25 | Loss: 0.00084389
Iteration 5/25 | Loss: 0.00083770
Iteration 6/25 | Loss: 0.00083623
Iteration 7/25 | Loss: 0.00083594
Iteration 8/25 | Loss: 0.00083594
Iteration 9/25 | Loss: 0.00083594
Iteration 10/25 | Loss: 0.00083594
Iteration 11/25 | Loss: 0.00083594
Iteration 12/25 | Loss: 0.00083594
Iteration 13/25 | Loss: 0.00083594
Iteration 14/25 | Loss: 0.00083594
Iteration 15/25 | Loss: 0.00083594
Iteration 16/25 | Loss: 0.00083594
Iteration 17/25 | Loss: 0.00083594
Iteration 18/25 | Loss: 0.00083594
Iteration 19/25 | Loss: 0.00083594
Iteration 20/25 | Loss: 0.00083594
Iteration 21/25 | Loss: 0.00083594
Iteration 22/25 | Loss: 0.00083594
Iteration 23/25 | Loss: 0.00083594
Iteration 24/25 | Loss: 0.00083594
Iteration 25/25 | Loss: 0.00083594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35273457
Iteration 2/25 | Loss: 0.00040230
Iteration 3/25 | Loss: 0.00040230
Iteration 4/25 | Loss: 0.00040230
Iteration 5/25 | Loss: 0.00040229
Iteration 6/25 | Loss: 0.00040229
Iteration 7/25 | Loss: 0.00040229
Iteration 8/25 | Loss: 0.00040229
Iteration 9/25 | Loss: 0.00040229
Iteration 10/25 | Loss: 0.00040229
Iteration 11/25 | Loss: 0.00040229
Iteration 12/25 | Loss: 0.00040229
Iteration 13/25 | Loss: 0.00040229
Iteration 14/25 | Loss: 0.00040229
Iteration 15/25 | Loss: 0.00040229
Iteration 16/25 | Loss: 0.00040229
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004022935754619539, 0.0004022935754619539, 0.0004022935754619539, 0.0004022935754619539, 0.0004022935754619539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004022935754619539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040229
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00002397
Iteration 4/1000 | Loss: 0.00001895
Iteration 5/1000 | Loss: 0.00001722
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001497
Iteration 8/1000 | Loss: 0.00001435
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001347
Iteration 14/1000 | Loss: 0.00001344
Iteration 15/1000 | Loss: 0.00001343
Iteration 16/1000 | Loss: 0.00001339
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001312
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001309
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001304
Iteration 32/1000 | Loss: 0.00001304
Iteration 33/1000 | Loss: 0.00001303
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001301
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001299
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001297
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001297
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001297
Iteration 77/1000 | Loss: 0.00001297
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001293
Iteration 110/1000 | Loss: 0.00001293
Iteration 111/1000 | Loss: 0.00001293
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001293
Iteration 117/1000 | Loss: 0.00001293
Iteration 118/1000 | Loss: 0.00001293
Iteration 119/1000 | Loss: 0.00001293
Iteration 120/1000 | Loss: 0.00001293
Iteration 121/1000 | Loss: 0.00001293
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001292
Iteration 125/1000 | Loss: 0.00001292
Iteration 126/1000 | Loss: 0.00001292
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001292
Iteration 131/1000 | Loss: 0.00001292
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001292
Iteration 134/1000 | Loss: 0.00001292
Iteration 135/1000 | Loss: 0.00001292
Iteration 136/1000 | Loss: 0.00001292
Iteration 137/1000 | Loss: 0.00001292
Iteration 138/1000 | Loss: 0.00001292
Iteration 139/1000 | Loss: 0.00001292
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001291
Iteration 145/1000 | Loss: 0.00001291
Iteration 146/1000 | Loss: 0.00001291
Iteration 147/1000 | Loss: 0.00001291
Iteration 148/1000 | Loss: 0.00001291
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001291
Iteration 151/1000 | Loss: 0.00001291
Iteration 152/1000 | Loss: 0.00001291
Iteration 153/1000 | Loss: 0.00001291
Iteration 154/1000 | Loss: 0.00001291
Iteration 155/1000 | Loss: 0.00001291
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001290
Iteration 158/1000 | Loss: 0.00001290
Iteration 159/1000 | Loss: 0.00001290
Iteration 160/1000 | Loss: 0.00001290
Iteration 161/1000 | Loss: 0.00001290
Iteration 162/1000 | Loss: 0.00001289
Iteration 163/1000 | Loss: 0.00001289
Iteration 164/1000 | Loss: 0.00001289
Iteration 165/1000 | Loss: 0.00001289
Iteration 166/1000 | Loss: 0.00001289
Iteration 167/1000 | Loss: 0.00001289
Iteration 168/1000 | Loss: 0.00001289
Iteration 169/1000 | Loss: 0.00001289
Iteration 170/1000 | Loss: 0.00001289
Iteration 171/1000 | Loss: 0.00001289
Iteration 172/1000 | Loss: 0.00001289
Iteration 173/1000 | Loss: 0.00001289
Iteration 174/1000 | Loss: 0.00001289
Iteration 175/1000 | Loss: 0.00001289
Iteration 176/1000 | Loss: 0.00001289
Iteration 177/1000 | Loss: 0.00001289
Iteration 178/1000 | Loss: 0.00001289
Iteration 179/1000 | Loss: 0.00001289
Iteration 180/1000 | Loss: 0.00001289
Iteration 181/1000 | Loss: 0.00001289
Iteration 182/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2890958714706358e-05, 1.2890958714706358e-05, 1.2890958714706358e-05, 1.2890958714706358e-05, 1.2890958714706358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2890958714706358e-05

Optimization complete. Final v2v error: 2.9916505813598633 mm

Highest mean error: 3.403933525085449 mm for frame 73

Lowest mean error: 2.3546085357666016 mm for frame 83

Saving results

Total time: 41.59322190284729
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881694
Iteration 2/25 | Loss: 0.00109395
Iteration 3/25 | Loss: 0.00099134
Iteration 4/25 | Loss: 0.00096664
Iteration 5/25 | Loss: 0.00095797
Iteration 6/25 | Loss: 0.00095646
Iteration 7/25 | Loss: 0.00095646
Iteration 8/25 | Loss: 0.00095646
Iteration 9/25 | Loss: 0.00095646
Iteration 10/25 | Loss: 0.00095646
Iteration 11/25 | Loss: 0.00095646
Iteration 12/25 | Loss: 0.00095646
Iteration 13/25 | Loss: 0.00095646
Iteration 14/25 | Loss: 0.00095646
Iteration 15/25 | Loss: 0.00095646
Iteration 16/25 | Loss: 0.00095646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009564553620293736, 0.0009564553620293736, 0.0009564553620293736, 0.0009564553620293736, 0.0009564553620293736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009564553620293736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31800926
Iteration 2/25 | Loss: 0.00060595
Iteration 3/25 | Loss: 0.00060587
Iteration 4/25 | Loss: 0.00060587
Iteration 5/25 | Loss: 0.00060587
Iteration 6/25 | Loss: 0.00060587
Iteration 7/25 | Loss: 0.00060587
Iteration 8/25 | Loss: 0.00060587
Iteration 9/25 | Loss: 0.00060587
Iteration 10/25 | Loss: 0.00060587
Iteration 11/25 | Loss: 0.00060587
Iteration 12/25 | Loss: 0.00060587
Iteration 13/25 | Loss: 0.00060587
Iteration 14/25 | Loss: 0.00060587
Iteration 15/25 | Loss: 0.00060587
Iteration 16/25 | Loss: 0.00060587
Iteration 17/25 | Loss: 0.00060587
Iteration 18/25 | Loss: 0.00060587
Iteration 19/25 | Loss: 0.00060587
Iteration 20/25 | Loss: 0.00060587
Iteration 21/25 | Loss: 0.00060587
Iteration 22/25 | Loss: 0.00060587
Iteration 23/25 | Loss: 0.00060587
Iteration 24/25 | Loss: 0.00060587
Iteration 25/25 | Loss: 0.00060587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060587
Iteration 2/1000 | Loss: 0.00005338
Iteration 3/1000 | Loss: 0.00003045
Iteration 4/1000 | Loss: 0.00002583
Iteration 5/1000 | Loss: 0.00002436
Iteration 6/1000 | Loss: 0.00002322
Iteration 7/1000 | Loss: 0.00002245
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002130
Iteration 10/1000 | Loss: 0.00002093
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00002063
Iteration 13/1000 | Loss: 0.00002045
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002037
Iteration 16/1000 | Loss: 0.00002032
Iteration 17/1000 | Loss: 0.00002031
Iteration 18/1000 | Loss: 0.00002028
Iteration 19/1000 | Loss: 0.00002027
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002025
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00002022
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002020
Iteration 28/1000 | Loss: 0.00002020
Iteration 29/1000 | Loss: 0.00002019
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002017
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002016
Iteration 34/1000 | Loss: 0.00002015
Iteration 35/1000 | Loss: 0.00002015
Iteration 36/1000 | Loss: 0.00002014
Iteration 37/1000 | Loss: 0.00002014
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002012
Iteration 40/1000 | Loss: 0.00002012
Iteration 41/1000 | Loss: 0.00002012
Iteration 42/1000 | Loss: 0.00002011
Iteration 43/1000 | Loss: 0.00002010
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00002009
Iteration 46/1000 | Loss: 0.00002009
Iteration 47/1000 | Loss: 0.00002009
Iteration 48/1000 | Loss: 0.00002008
Iteration 49/1000 | Loss: 0.00002008
Iteration 50/1000 | Loss: 0.00002008
Iteration 51/1000 | Loss: 0.00002008
Iteration 52/1000 | Loss: 0.00002007
Iteration 53/1000 | Loss: 0.00002007
Iteration 54/1000 | Loss: 0.00002007
Iteration 55/1000 | Loss: 0.00002007
Iteration 56/1000 | Loss: 0.00002006
Iteration 57/1000 | Loss: 0.00002006
Iteration 58/1000 | Loss: 0.00002006
Iteration 59/1000 | Loss: 0.00002006
Iteration 60/1000 | Loss: 0.00002006
Iteration 61/1000 | Loss: 0.00002006
Iteration 62/1000 | Loss: 0.00002006
Iteration 63/1000 | Loss: 0.00002005
Iteration 64/1000 | Loss: 0.00002005
Iteration 65/1000 | Loss: 0.00002005
Iteration 66/1000 | Loss: 0.00002005
Iteration 67/1000 | Loss: 0.00002004
Iteration 68/1000 | Loss: 0.00002004
Iteration 69/1000 | Loss: 0.00002004
Iteration 70/1000 | Loss: 0.00002004
Iteration 71/1000 | Loss: 0.00002004
Iteration 72/1000 | Loss: 0.00002003
Iteration 73/1000 | Loss: 0.00002003
Iteration 74/1000 | Loss: 0.00002002
Iteration 75/1000 | Loss: 0.00002002
Iteration 76/1000 | Loss: 0.00002002
Iteration 77/1000 | Loss: 0.00002001
Iteration 78/1000 | Loss: 0.00002001
Iteration 79/1000 | Loss: 0.00002000
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00002000
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001999
Iteration 84/1000 | Loss: 0.00001999
Iteration 85/1000 | Loss: 0.00001998
Iteration 86/1000 | Loss: 0.00001998
Iteration 87/1000 | Loss: 0.00001997
Iteration 88/1000 | Loss: 0.00001997
Iteration 89/1000 | Loss: 0.00001997
Iteration 90/1000 | Loss: 0.00001997
Iteration 91/1000 | Loss: 0.00001996
Iteration 92/1000 | Loss: 0.00001996
Iteration 93/1000 | Loss: 0.00001996
Iteration 94/1000 | Loss: 0.00001995
Iteration 95/1000 | Loss: 0.00001995
Iteration 96/1000 | Loss: 0.00001995
Iteration 97/1000 | Loss: 0.00001995
Iteration 98/1000 | Loss: 0.00001994
Iteration 99/1000 | Loss: 0.00001994
Iteration 100/1000 | Loss: 0.00001994
Iteration 101/1000 | Loss: 0.00001994
Iteration 102/1000 | Loss: 0.00001994
Iteration 103/1000 | Loss: 0.00001994
Iteration 104/1000 | Loss: 0.00001993
Iteration 105/1000 | Loss: 0.00001993
Iteration 106/1000 | Loss: 0.00001993
Iteration 107/1000 | Loss: 0.00001992
Iteration 108/1000 | Loss: 0.00001992
Iteration 109/1000 | Loss: 0.00001992
Iteration 110/1000 | Loss: 0.00001991
Iteration 111/1000 | Loss: 0.00001991
Iteration 112/1000 | Loss: 0.00001990
Iteration 113/1000 | Loss: 0.00001990
Iteration 114/1000 | Loss: 0.00001990
Iteration 115/1000 | Loss: 0.00001989
Iteration 116/1000 | Loss: 0.00001989
Iteration 117/1000 | Loss: 0.00001989
Iteration 118/1000 | Loss: 0.00001988
Iteration 119/1000 | Loss: 0.00001988
Iteration 120/1000 | Loss: 0.00001988
Iteration 121/1000 | Loss: 0.00001988
Iteration 122/1000 | Loss: 0.00001987
Iteration 123/1000 | Loss: 0.00001987
Iteration 124/1000 | Loss: 0.00001987
Iteration 125/1000 | Loss: 0.00001986
Iteration 126/1000 | Loss: 0.00001986
Iteration 127/1000 | Loss: 0.00001986
Iteration 128/1000 | Loss: 0.00001986
Iteration 129/1000 | Loss: 0.00001986
Iteration 130/1000 | Loss: 0.00001986
Iteration 131/1000 | Loss: 0.00001986
Iteration 132/1000 | Loss: 0.00001986
Iteration 133/1000 | Loss: 0.00001985
Iteration 134/1000 | Loss: 0.00001985
Iteration 135/1000 | Loss: 0.00001985
Iteration 136/1000 | Loss: 0.00001985
Iteration 137/1000 | Loss: 0.00001985
Iteration 138/1000 | Loss: 0.00001984
Iteration 139/1000 | Loss: 0.00001984
Iteration 140/1000 | Loss: 0.00001984
Iteration 141/1000 | Loss: 0.00001984
Iteration 142/1000 | Loss: 0.00001984
Iteration 143/1000 | Loss: 0.00001984
Iteration 144/1000 | Loss: 0.00001984
Iteration 145/1000 | Loss: 0.00001984
Iteration 146/1000 | Loss: 0.00001983
Iteration 147/1000 | Loss: 0.00001983
Iteration 148/1000 | Loss: 0.00001983
Iteration 149/1000 | Loss: 0.00001983
Iteration 150/1000 | Loss: 0.00001983
Iteration 151/1000 | Loss: 0.00001983
Iteration 152/1000 | Loss: 0.00001983
Iteration 153/1000 | Loss: 0.00001983
Iteration 154/1000 | Loss: 0.00001983
Iteration 155/1000 | Loss: 0.00001983
Iteration 156/1000 | Loss: 0.00001982
Iteration 157/1000 | Loss: 0.00001982
Iteration 158/1000 | Loss: 0.00001982
Iteration 159/1000 | Loss: 0.00001982
Iteration 160/1000 | Loss: 0.00001982
Iteration 161/1000 | Loss: 0.00001982
Iteration 162/1000 | Loss: 0.00001982
Iteration 163/1000 | Loss: 0.00001982
Iteration 164/1000 | Loss: 0.00001982
Iteration 165/1000 | Loss: 0.00001982
Iteration 166/1000 | Loss: 0.00001982
Iteration 167/1000 | Loss: 0.00001982
Iteration 168/1000 | Loss: 0.00001982
Iteration 169/1000 | Loss: 0.00001982
Iteration 170/1000 | Loss: 0.00001982
Iteration 171/1000 | Loss: 0.00001982
Iteration 172/1000 | Loss: 0.00001981
Iteration 173/1000 | Loss: 0.00001981
Iteration 174/1000 | Loss: 0.00001981
Iteration 175/1000 | Loss: 0.00001981
Iteration 176/1000 | Loss: 0.00001981
Iteration 177/1000 | Loss: 0.00001981
Iteration 178/1000 | Loss: 0.00001981
Iteration 179/1000 | Loss: 0.00001981
Iteration 180/1000 | Loss: 0.00001981
Iteration 181/1000 | Loss: 0.00001981
Iteration 182/1000 | Loss: 0.00001981
Iteration 183/1000 | Loss: 0.00001981
Iteration 184/1000 | Loss: 0.00001981
Iteration 185/1000 | Loss: 0.00001981
Iteration 186/1000 | Loss: 0.00001981
Iteration 187/1000 | Loss: 0.00001981
Iteration 188/1000 | Loss: 0.00001980
Iteration 189/1000 | Loss: 0.00001980
Iteration 190/1000 | Loss: 0.00001980
Iteration 191/1000 | Loss: 0.00001980
Iteration 192/1000 | Loss: 0.00001980
Iteration 193/1000 | Loss: 0.00001980
Iteration 194/1000 | Loss: 0.00001980
Iteration 195/1000 | Loss: 0.00001980
Iteration 196/1000 | Loss: 0.00001980
Iteration 197/1000 | Loss: 0.00001980
Iteration 198/1000 | Loss: 0.00001980
Iteration 199/1000 | Loss: 0.00001979
Iteration 200/1000 | Loss: 0.00001979
Iteration 201/1000 | Loss: 0.00001979
Iteration 202/1000 | Loss: 0.00001979
Iteration 203/1000 | Loss: 0.00001979
Iteration 204/1000 | Loss: 0.00001979
Iteration 205/1000 | Loss: 0.00001979
Iteration 206/1000 | Loss: 0.00001979
Iteration 207/1000 | Loss: 0.00001979
Iteration 208/1000 | Loss: 0.00001979
Iteration 209/1000 | Loss: 0.00001979
Iteration 210/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.9794983018073253e-05, 1.9794983018073253e-05, 1.9794983018073253e-05, 1.9794983018073253e-05, 1.9794983018073253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9794983018073253e-05

Optimization complete. Final v2v error: 3.7540743350982666 mm

Highest mean error: 4.106120586395264 mm for frame 113

Lowest mean error: 3.198558807373047 mm for frame 0

Saving results

Total time: 43.73039960861206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888023
Iteration 2/25 | Loss: 0.00124544
Iteration 3/25 | Loss: 0.00094213
Iteration 4/25 | Loss: 0.00090615
Iteration 5/25 | Loss: 0.00090353
Iteration 6/25 | Loss: 0.00090309
Iteration 7/25 | Loss: 0.00090309
Iteration 8/25 | Loss: 0.00090309
Iteration 9/25 | Loss: 0.00090309
Iteration 10/25 | Loss: 0.00090309
Iteration 11/25 | Loss: 0.00090309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009030912187881768, 0.0009030912187881768, 0.0009030912187881768, 0.0009030912187881768, 0.0009030912187881768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009030912187881768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93592399
Iteration 2/25 | Loss: 0.00024500
Iteration 3/25 | Loss: 0.00024499
Iteration 4/25 | Loss: 0.00024499
Iteration 5/25 | Loss: 0.00024499
Iteration 6/25 | Loss: 0.00024499
Iteration 7/25 | Loss: 0.00024499
Iteration 8/25 | Loss: 0.00024499
Iteration 9/25 | Loss: 0.00024499
Iteration 10/25 | Loss: 0.00024499
Iteration 11/25 | Loss: 0.00024499
Iteration 12/25 | Loss: 0.00024499
Iteration 13/25 | Loss: 0.00024499
Iteration 14/25 | Loss: 0.00024499
Iteration 15/25 | Loss: 0.00024499
Iteration 16/25 | Loss: 0.00024499
Iteration 17/25 | Loss: 0.00024499
Iteration 18/25 | Loss: 0.00024499
Iteration 19/25 | Loss: 0.00024499
Iteration 20/25 | Loss: 0.00024499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00024499159189872444, 0.00024499159189872444, 0.00024499159189872444, 0.00024499159189872444, 0.00024499159189872444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024499159189872444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024499
Iteration 2/1000 | Loss: 0.00003021
Iteration 3/1000 | Loss: 0.00002077
Iteration 4/1000 | Loss: 0.00001820
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001674
Iteration 7/1000 | Loss: 0.00001636
Iteration 8/1000 | Loss: 0.00001596
Iteration 9/1000 | Loss: 0.00001576
Iteration 10/1000 | Loss: 0.00001558
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001545
Iteration 13/1000 | Loss: 0.00001544
Iteration 14/1000 | Loss: 0.00001537
Iteration 15/1000 | Loss: 0.00001536
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001534
Iteration 18/1000 | Loss: 0.00001533
Iteration 19/1000 | Loss: 0.00001532
Iteration 20/1000 | Loss: 0.00001531
Iteration 21/1000 | Loss: 0.00001528
Iteration 22/1000 | Loss: 0.00001525
Iteration 23/1000 | Loss: 0.00001525
Iteration 24/1000 | Loss: 0.00001525
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001524
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001523
Iteration 30/1000 | Loss: 0.00001523
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001523
Iteration 34/1000 | Loss: 0.00001523
Iteration 35/1000 | Loss: 0.00001522
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001522
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001521
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001521
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001518
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001518
Iteration 53/1000 | Loss: 0.00001518
Iteration 54/1000 | Loss: 0.00001518
Iteration 55/1000 | Loss: 0.00001518
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001517
Iteration 58/1000 | Loss: 0.00001517
Iteration 59/1000 | Loss: 0.00001517
Iteration 60/1000 | Loss: 0.00001517
Iteration 61/1000 | Loss: 0.00001517
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001517
Iteration 68/1000 | Loss: 0.00001517
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001517
Iteration 71/1000 | Loss: 0.00001517
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.5167354831646662e-05, 1.5167354831646662e-05, 1.5167354831646662e-05, 1.5167354831646662e-05, 1.5167354831646662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5167354831646662e-05

Optimization complete. Final v2v error: 3.2673494815826416 mm

Highest mean error: 3.506870985031128 mm for frame 32

Lowest mean error: 3.1155829429626465 mm for frame 91

Saving results

Total time: 29.787827253341675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032814
Iteration 2/25 | Loss: 0.00191131
Iteration 3/25 | Loss: 0.00144055
Iteration 4/25 | Loss: 0.00126013
Iteration 5/25 | Loss: 0.00114424
Iteration 6/25 | Loss: 0.00105329
Iteration 7/25 | Loss: 0.00105216
Iteration 8/25 | Loss: 0.00103154
Iteration 9/25 | Loss: 0.00101528
Iteration 10/25 | Loss: 0.00096763
Iteration 11/25 | Loss: 0.00094302
Iteration 12/25 | Loss: 0.00094126
Iteration 13/25 | Loss: 0.00092624
Iteration 14/25 | Loss: 0.00092667
Iteration 15/25 | Loss: 0.00093010
Iteration 16/25 | Loss: 0.00092058
Iteration 17/25 | Loss: 0.00092626
Iteration 18/25 | Loss: 0.00092524
Iteration 19/25 | Loss: 0.00092388
Iteration 20/25 | Loss: 0.00091807
Iteration 21/25 | Loss: 0.00091649
Iteration 22/25 | Loss: 0.00092167
Iteration 23/25 | Loss: 0.00092210
Iteration 24/25 | Loss: 0.00092125
Iteration 25/25 | Loss: 0.00092249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47285688
Iteration 2/25 | Loss: 0.00057200
Iteration 3/25 | Loss: 0.00055120
Iteration 4/25 | Loss: 0.00055120
Iteration 5/25 | Loss: 0.00055120
Iteration 6/25 | Loss: 0.00055120
Iteration 7/25 | Loss: 0.00055120
Iteration 8/25 | Loss: 0.00055120
Iteration 9/25 | Loss: 0.00055120
Iteration 10/25 | Loss: 0.00055120
Iteration 11/25 | Loss: 0.00055120
Iteration 12/25 | Loss: 0.00055120
Iteration 13/25 | Loss: 0.00055120
Iteration 14/25 | Loss: 0.00055120
Iteration 15/25 | Loss: 0.00055120
Iteration 16/25 | Loss: 0.00055120
Iteration 17/25 | Loss: 0.00055120
Iteration 18/25 | Loss: 0.00055120
Iteration 19/25 | Loss: 0.00055120
Iteration 20/25 | Loss: 0.00055120
Iteration 21/25 | Loss: 0.00055120
Iteration 22/25 | Loss: 0.00055120
Iteration 23/25 | Loss: 0.00055120
Iteration 24/25 | Loss: 0.00055120
Iteration 25/25 | Loss: 0.00055120

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055120
Iteration 2/1000 | Loss: 0.00042224
Iteration 3/1000 | Loss: 0.00021020
Iteration 4/1000 | Loss: 0.00042967
Iteration 5/1000 | Loss: 0.00029969
Iteration 6/1000 | Loss: 0.00018369
Iteration 7/1000 | Loss: 0.00027812
Iteration 8/1000 | Loss: 0.00029043
Iteration 9/1000 | Loss: 0.00042939
Iteration 10/1000 | Loss: 0.00050838
Iteration 11/1000 | Loss: 0.00048714
Iteration 12/1000 | Loss: 0.00029961
Iteration 13/1000 | Loss: 0.00037709
Iteration 14/1000 | Loss: 0.00025386
Iteration 15/1000 | Loss: 0.00028272
Iteration 16/1000 | Loss: 0.00022535
Iteration 17/1000 | Loss: 0.00025072
Iteration 18/1000 | Loss: 0.00029253
Iteration 19/1000 | Loss: 0.00048073
Iteration 20/1000 | Loss: 0.00037204
Iteration 21/1000 | Loss: 0.00027799
Iteration 22/1000 | Loss: 0.00037798
Iteration 23/1000 | Loss: 0.00040667
Iteration 24/1000 | Loss: 0.00022350
Iteration 25/1000 | Loss: 0.00025554
Iteration 26/1000 | Loss: 0.00009883
Iteration 27/1000 | Loss: 0.00017393
Iteration 28/1000 | Loss: 0.00015529
Iteration 29/1000 | Loss: 0.00007352
Iteration 30/1000 | Loss: 0.00008201
Iteration 31/1000 | Loss: 0.00030986
Iteration 32/1000 | Loss: 0.00037219
Iteration 33/1000 | Loss: 0.00027721
Iteration 34/1000 | Loss: 0.00025968
Iteration 35/1000 | Loss: 0.00024719
Iteration 36/1000 | Loss: 0.00021632
Iteration 37/1000 | Loss: 0.00021820
Iteration 38/1000 | Loss: 0.00023852
Iteration 39/1000 | Loss: 0.00015951
Iteration 40/1000 | Loss: 0.00004700
Iteration 41/1000 | Loss: 0.00009929
Iteration 42/1000 | Loss: 0.00024267
Iteration 43/1000 | Loss: 0.00644616
Iteration 44/1000 | Loss: 0.00097249
Iteration 45/1000 | Loss: 0.00017384
Iteration 46/1000 | Loss: 0.00007982
Iteration 47/1000 | Loss: 0.00004342
Iteration 48/1000 | Loss: 0.00003348
Iteration 49/1000 | Loss: 0.00004212
Iteration 50/1000 | Loss: 0.00032119
Iteration 51/1000 | Loss: 0.00019465
Iteration 52/1000 | Loss: 0.00015041
Iteration 53/1000 | Loss: 0.00006000
Iteration 54/1000 | Loss: 0.00015564
Iteration 55/1000 | Loss: 0.00076203
Iteration 56/1000 | Loss: 0.00162449
Iteration 57/1000 | Loss: 0.00113820
Iteration 58/1000 | Loss: 0.00059575
Iteration 59/1000 | Loss: 0.00060450
Iteration 60/1000 | Loss: 0.00062966
Iteration 61/1000 | Loss: 0.00009859
Iteration 62/1000 | Loss: 0.00003552
Iteration 63/1000 | Loss: 0.00002864
Iteration 64/1000 | Loss: 0.00010250
Iteration 65/1000 | Loss: 0.00007895
Iteration 66/1000 | Loss: 0.00008867
Iteration 67/1000 | Loss: 0.00013905
Iteration 68/1000 | Loss: 0.00017820
Iteration 69/1000 | Loss: 0.00024395
Iteration 70/1000 | Loss: 0.00023911
Iteration 71/1000 | Loss: 0.00010765
Iteration 72/1000 | Loss: 0.00034132
Iteration 73/1000 | Loss: 0.00019534
Iteration 74/1000 | Loss: 0.00004481
Iteration 75/1000 | Loss: 0.00018903
Iteration 76/1000 | Loss: 0.00002509
Iteration 77/1000 | Loss: 0.00041870
Iteration 78/1000 | Loss: 0.00034014
Iteration 79/1000 | Loss: 0.00026955
Iteration 80/1000 | Loss: 0.00007925
Iteration 81/1000 | Loss: 0.00002731
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00008538
Iteration 85/1000 | Loss: 0.00003251
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00003240
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00004545
Iteration 91/1000 | Loss: 0.00001418
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001354
Iteration 97/1000 | Loss: 0.00001353
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001351
Iteration 102/1000 | Loss: 0.00005922
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001350
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001348
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001347
Iteration 111/1000 | Loss: 0.00001347
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001346
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001344
Iteration 125/1000 | Loss: 0.00001344
Iteration 126/1000 | Loss: 0.00001344
Iteration 127/1000 | Loss: 0.00001344
Iteration 128/1000 | Loss: 0.00001344
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001343
Iteration 133/1000 | Loss: 0.00001343
Iteration 134/1000 | Loss: 0.00001343
Iteration 135/1000 | Loss: 0.00001343
Iteration 136/1000 | Loss: 0.00001343
Iteration 137/1000 | Loss: 0.00001343
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Iteration 141/1000 | Loss: 0.00001343
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001343
Iteration 152/1000 | Loss: 0.00001343
Iteration 153/1000 | Loss: 0.00001343
Iteration 154/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.3427837075141724e-05, 1.3427837075141724e-05, 1.3427837075141724e-05, 1.3427837075141724e-05, 1.3427837075141724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3427837075141724e-05

Optimization complete. Final v2v error: 2.916480541229248 mm

Highest mean error: 5.779709339141846 mm for frame 129

Lowest mean error: 2.3322553634643555 mm for frame 31

Saving results

Total time: 179.4514124393463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016918
Iteration 2/25 | Loss: 0.00272213
Iteration 3/25 | Loss: 0.00190692
Iteration 4/25 | Loss: 0.00158271
Iteration 5/25 | Loss: 0.00156724
Iteration 6/25 | Loss: 0.00146160
Iteration 7/25 | Loss: 0.00140767
Iteration 8/25 | Loss: 0.00133658
Iteration 9/25 | Loss: 0.00132492
Iteration 10/25 | Loss: 0.00129375
Iteration 11/25 | Loss: 0.00132080
Iteration 12/25 | Loss: 0.00121307
Iteration 13/25 | Loss: 0.00112100
Iteration 14/25 | Loss: 0.00110078
Iteration 15/25 | Loss: 0.00109603
Iteration 16/25 | Loss: 0.00109145
Iteration 17/25 | Loss: 0.00108147
Iteration 18/25 | Loss: 0.00107275
Iteration 19/25 | Loss: 0.00107010
Iteration 20/25 | Loss: 0.00106882
Iteration 21/25 | Loss: 0.00106821
Iteration 22/25 | Loss: 0.00106734
Iteration 23/25 | Loss: 0.00107157
Iteration 24/25 | Loss: 0.00106668
Iteration 25/25 | Loss: 0.00106572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37720287
Iteration 2/25 | Loss: 0.00194209
Iteration 3/25 | Loss: 0.00123854
Iteration 4/25 | Loss: 0.00123853
Iteration 5/25 | Loss: 0.00123852
Iteration 6/25 | Loss: 0.00123852
Iteration 7/25 | Loss: 0.00123852
Iteration 8/25 | Loss: 0.00123852
Iteration 9/25 | Loss: 0.00123852
Iteration 10/25 | Loss: 0.00123852
Iteration 11/25 | Loss: 0.00123852
Iteration 12/25 | Loss: 0.00123852
Iteration 13/25 | Loss: 0.00123852
Iteration 14/25 | Loss: 0.00123852
Iteration 15/25 | Loss: 0.00123852
Iteration 16/25 | Loss: 0.00123852
Iteration 17/25 | Loss: 0.00123852
Iteration 18/25 | Loss: 0.00123852
Iteration 19/25 | Loss: 0.00123852
Iteration 20/25 | Loss: 0.00123852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012385222362354398, 0.0012385222362354398, 0.0012385222362354398, 0.0012385222362354398, 0.0012385222362354398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012385222362354398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123852
Iteration 2/1000 | Loss: 0.00281978
Iteration 3/1000 | Loss: 0.00216448
Iteration 4/1000 | Loss: 0.00236828
Iteration 5/1000 | Loss: 0.00140972
Iteration 6/1000 | Loss: 0.00159907
Iteration 7/1000 | Loss: 0.00104139
Iteration 8/1000 | Loss: 0.00017107
Iteration 9/1000 | Loss: 0.00045152
Iteration 10/1000 | Loss: 0.00009626
Iteration 11/1000 | Loss: 0.00009444
Iteration 12/1000 | Loss: 0.00007917
Iteration 13/1000 | Loss: 0.00033563
Iteration 14/1000 | Loss: 0.00008039
Iteration 15/1000 | Loss: 0.00007143
Iteration 16/1000 | Loss: 0.00006600
Iteration 17/1000 | Loss: 0.00006447
Iteration 18/1000 | Loss: 0.00020361
Iteration 19/1000 | Loss: 0.00152043
Iteration 20/1000 | Loss: 0.00095719
Iteration 21/1000 | Loss: 0.00042179
Iteration 22/1000 | Loss: 0.00030700
Iteration 23/1000 | Loss: 0.00038509
Iteration 24/1000 | Loss: 0.00055696
Iteration 25/1000 | Loss: 0.00050938
Iteration 26/1000 | Loss: 0.00046454
Iteration 27/1000 | Loss: 0.00043503
Iteration 28/1000 | Loss: 0.00012118
Iteration 29/1000 | Loss: 0.00024292
Iteration 30/1000 | Loss: 0.00007628
Iteration 31/1000 | Loss: 0.00020883
Iteration 32/1000 | Loss: 0.00006950
Iteration 33/1000 | Loss: 0.00006380
Iteration 34/1000 | Loss: 0.00008308
Iteration 35/1000 | Loss: 0.00034372
Iteration 36/1000 | Loss: 0.00032667
Iteration 37/1000 | Loss: 0.00023495
Iteration 38/1000 | Loss: 0.00007126
Iteration 39/1000 | Loss: 0.00006569
Iteration 40/1000 | Loss: 0.00013231
Iteration 41/1000 | Loss: 0.00029811
Iteration 42/1000 | Loss: 0.00007409
Iteration 43/1000 | Loss: 0.00006448
Iteration 44/1000 | Loss: 0.00006307
Iteration 45/1000 | Loss: 0.00034182
Iteration 46/1000 | Loss: 0.00043369
Iteration 47/1000 | Loss: 0.00029170
Iteration 48/1000 | Loss: 0.00032029
Iteration 49/1000 | Loss: 0.00030544
Iteration 50/1000 | Loss: 0.00025099
Iteration 51/1000 | Loss: 0.00006714
Iteration 52/1000 | Loss: 0.00005978
Iteration 53/1000 | Loss: 0.00005696
Iteration 54/1000 | Loss: 0.00005470
Iteration 55/1000 | Loss: 0.00033503
Iteration 56/1000 | Loss: 0.00032044
Iteration 57/1000 | Loss: 0.00022003
Iteration 58/1000 | Loss: 0.00006235
Iteration 59/1000 | Loss: 0.00031256
Iteration 60/1000 | Loss: 0.00036792
Iteration 61/1000 | Loss: 0.00040437
Iteration 62/1000 | Loss: 0.00051868
Iteration 63/1000 | Loss: 0.00077975
Iteration 64/1000 | Loss: 0.00033073
Iteration 65/1000 | Loss: 0.00032616
Iteration 66/1000 | Loss: 0.00038533
Iteration 67/1000 | Loss: 0.00019242
Iteration 68/1000 | Loss: 0.00017723
Iteration 69/1000 | Loss: 0.00034785
Iteration 70/1000 | Loss: 0.00033043
Iteration 71/1000 | Loss: 0.00005130
Iteration 72/1000 | Loss: 0.00006691
Iteration 73/1000 | Loss: 0.00004925
Iteration 74/1000 | Loss: 0.00023105
Iteration 75/1000 | Loss: 0.00021699
Iteration 76/1000 | Loss: 0.00004540
Iteration 77/1000 | Loss: 0.00004437
Iteration 78/1000 | Loss: 0.00004378
Iteration 79/1000 | Loss: 0.00004340
Iteration 80/1000 | Loss: 0.00004290
Iteration 81/1000 | Loss: 0.00017622
Iteration 82/1000 | Loss: 0.00012874
Iteration 83/1000 | Loss: 0.00011440
Iteration 84/1000 | Loss: 0.00017733
Iteration 85/1000 | Loss: 0.00012357
Iteration 86/1000 | Loss: 0.00018123
Iteration 87/1000 | Loss: 0.00018088
Iteration 88/1000 | Loss: 0.00019816
Iteration 89/1000 | Loss: 0.00007797
Iteration 90/1000 | Loss: 0.00017459
Iteration 91/1000 | Loss: 0.00041356
Iteration 92/1000 | Loss: 0.00034278
Iteration 93/1000 | Loss: 0.00006240
Iteration 94/1000 | Loss: 0.00004835
Iteration 95/1000 | Loss: 0.00004308
Iteration 96/1000 | Loss: 0.00004225
Iteration 97/1000 | Loss: 0.00004181
Iteration 98/1000 | Loss: 0.00004154
Iteration 99/1000 | Loss: 0.00004132
Iteration 100/1000 | Loss: 0.00004113
Iteration 101/1000 | Loss: 0.00004111
Iteration 102/1000 | Loss: 0.00004105
Iteration 103/1000 | Loss: 0.00004104
Iteration 104/1000 | Loss: 0.00004104
Iteration 105/1000 | Loss: 0.00004103
Iteration 106/1000 | Loss: 0.00004103
Iteration 107/1000 | Loss: 0.00004103
Iteration 108/1000 | Loss: 0.00004102
Iteration 109/1000 | Loss: 0.00004102
Iteration 110/1000 | Loss: 0.00004101
Iteration 111/1000 | Loss: 0.00004099
Iteration 112/1000 | Loss: 0.00004099
Iteration 113/1000 | Loss: 0.00004099
Iteration 114/1000 | Loss: 0.00004095
Iteration 115/1000 | Loss: 0.00004094
Iteration 116/1000 | Loss: 0.00004094
Iteration 117/1000 | Loss: 0.00004092
Iteration 118/1000 | Loss: 0.00004092
Iteration 119/1000 | Loss: 0.00004090
Iteration 120/1000 | Loss: 0.00004090
Iteration 121/1000 | Loss: 0.00004089
Iteration 122/1000 | Loss: 0.00004089
Iteration 123/1000 | Loss: 0.00004088
Iteration 124/1000 | Loss: 0.00004088
Iteration 125/1000 | Loss: 0.00004087
Iteration 126/1000 | Loss: 0.00004087
Iteration 127/1000 | Loss: 0.00004087
Iteration 128/1000 | Loss: 0.00004086
Iteration 129/1000 | Loss: 0.00004086
Iteration 130/1000 | Loss: 0.00004086
Iteration 131/1000 | Loss: 0.00004085
Iteration 132/1000 | Loss: 0.00004085
Iteration 133/1000 | Loss: 0.00004084
Iteration 134/1000 | Loss: 0.00004084
Iteration 135/1000 | Loss: 0.00004084
Iteration 136/1000 | Loss: 0.00004084
Iteration 137/1000 | Loss: 0.00004084
Iteration 138/1000 | Loss: 0.00004084
Iteration 139/1000 | Loss: 0.00004084
Iteration 140/1000 | Loss: 0.00004083
Iteration 141/1000 | Loss: 0.00004083
Iteration 142/1000 | Loss: 0.00004083
Iteration 143/1000 | Loss: 0.00004083
Iteration 144/1000 | Loss: 0.00004083
Iteration 145/1000 | Loss: 0.00004083
Iteration 146/1000 | Loss: 0.00004082
Iteration 147/1000 | Loss: 0.00004082
Iteration 148/1000 | Loss: 0.00004082
Iteration 149/1000 | Loss: 0.00004082
Iteration 150/1000 | Loss: 0.00004082
Iteration 151/1000 | Loss: 0.00004081
Iteration 152/1000 | Loss: 0.00004081
Iteration 153/1000 | Loss: 0.00004080
Iteration 154/1000 | Loss: 0.00004077
Iteration 155/1000 | Loss: 0.00004076
Iteration 156/1000 | Loss: 0.00004076
Iteration 157/1000 | Loss: 0.00004076
Iteration 158/1000 | Loss: 0.00004076
Iteration 159/1000 | Loss: 0.00004075
Iteration 160/1000 | Loss: 0.00004075
Iteration 161/1000 | Loss: 0.00004075
Iteration 162/1000 | Loss: 0.00004075
Iteration 163/1000 | Loss: 0.00004075
Iteration 164/1000 | Loss: 0.00004075
Iteration 165/1000 | Loss: 0.00004075
Iteration 166/1000 | Loss: 0.00004075
Iteration 167/1000 | Loss: 0.00004075
Iteration 168/1000 | Loss: 0.00004075
Iteration 169/1000 | Loss: 0.00004075
Iteration 170/1000 | Loss: 0.00004074
Iteration 171/1000 | Loss: 0.00004074
Iteration 172/1000 | Loss: 0.00004074
Iteration 173/1000 | Loss: 0.00004074
Iteration 174/1000 | Loss: 0.00004074
Iteration 175/1000 | Loss: 0.00004074
Iteration 176/1000 | Loss: 0.00004074
Iteration 177/1000 | Loss: 0.00004074
Iteration 178/1000 | Loss: 0.00004073
Iteration 179/1000 | Loss: 0.00004073
Iteration 180/1000 | Loss: 0.00004073
Iteration 181/1000 | Loss: 0.00004073
Iteration 182/1000 | Loss: 0.00004073
Iteration 183/1000 | Loss: 0.00004073
Iteration 184/1000 | Loss: 0.00004073
Iteration 185/1000 | Loss: 0.00004073
Iteration 186/1000 | Loss: 0.00004073
Iteration 187/1000 | Loss: 0.00004073
Iteration 188/1000 | Loss: 0.00004073
Iteration 189/1000 | Loss: 0.00004073
Iteration 190/1000 | Loss: 0.00004073
Iteration 191/1000 | Loss: 0.00004072
Iteration 192/1000 | Loss: 0.00004072
Iteration 193/1000 | Loss: 0.00004072
Iteration 194/1000 | Loss: 0.00004072
Iteration 195/1000 | Loss: 0.00004072
Iteration 196/1000 | Loss: 0.00004071
Iteration 197/1000 | Loss: 0.00004071
Iteration 198/1000 | Loss: 0.00004071
Iteration 199/1000 | Loss: 0.00004071
Iteration 200/1000 | Loss: 0.00004071
Iteration 201/1000 | Loss: 0.00004071
Iteration 202/1000 | Loss: 0.00004071
Iteration 203/1000 | Loss: 0.00004071
Iteration 204/1000 | Loss: 0.00004070
Iteration 205/1000 | Loss: 0.00004070
Iteration 206/1000 | Loss: 0.00004070
Iteration 207/1000 | Loss: 0.00004070
Iteration 208/1000 | Loss: 0.00004070
Iteration 209/1000 | Loss: 0.00004070
Iteration 210/1000 | Loss: 0.00004070
Iteration 211/1000 | Loss: 0.00004070
Iteration 212/1000 | Loss: 0.00004070
Iteration 213/1000 | Loss: 0.00004069
Iteration 214/1000 | Loss: 0.00004069
Iteration 215/1000 | Loss: 0.00004069
Iteration 216/1000 | Loss: 0.00004069
Iteration 217/1000 | Loss: 0.00004069
Iteration 218/1000 | Loss: 0.00004069
Iteration 219/1000 | Loss: 0.00004069
Iteration 220/1000 | Loss: 0.00004069
Iteration 221/1000 | Loss: 0.00004069
Iteration 222/1000 | Loss: 0.00004069
Iteration 223/1000 | Loss: 0.00004069
Iteration 224/1000 | Loss: 0.00004069
Iteration 225/1000 | Loss: 0.00004069
Iteration 226/1000 | Loss: 0.00004069
Iteration 227/1000 | Loss: 0.00004069
Iteration 228/1000 | Loss: 0.00004069
Iteration 229/1000 | Loss: 0.00004069
Iteration 230/1000 | Loss: 0.00004069
Iteration 231/1000 | Loss: 0.00004069
Iteration 232/1000 | Loss: 0.00004069
Iteration 233/1000 | Loss: 0.00004069
Iteration 234/1000 | Loss: 0.00004069
Iteration 235/1000 | Loss: 0.00004069
Iteration 236/1000 | Loss: 0.00004069
Iteration 237/1000 | Loss: 0.00004069
Iteration 238/1000 | Loss: 0.00004069
Iteration 239/1000 | Loss: 0.00004069
Iteration 240/1000 | Loss: 0.00004069
Iteration 241/1000 | Loss: 0.00004069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [4.068875205121003e-05, 4.068875205121003e-05, 4.068875205121003e-05, 4.068875205121003e-05, 4.068875205121003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.068875205121003e-05

Optimization complete. Final v2v error: 3.6862730979919434 mm

Highest mean error: 19.824426651000977 mm for frame 60

Lowest mean error: 2.414689302444458 mm for frame 150

Saving results

Total time: 194.453519821167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947760
Iteration 2/25 | Loss: 0.00257349
Iteration 3/25 | Loss: 0.00162125
Iteration 4/25 | Loss: 0.00141720
Iteration 5/25 | Loss: 0.00133395
Iteration 6/25 | Loss: 0.00124910
Iteration 7/25 | Loss: 0.00118838
Iteration 8/25 | Loss: 0.00115903
Iteration 9/25 | Loss: 0.00110329
Iteration 10/25 | Loss: 0.00107905
Iteration 11/25 | Loss: 0.00106304
Iteration 12/25 | Loss: 0.00104981
Iteration 13/25 | Loss: 0.00101444
Iteration 14/25 | Loss: 0.00099310
Iteration 15/25 | Loss: 0.00098238
Iteration 16/25 | Loss: 0.00098293
Iteration 17/25 | Loss: 0.00097956
Iteration 18/25 | Loss: 0.00098092
Iteration 19/25 | Loss: 0.00097453
Iteration 20/25 | Loss: 0.00096563
Iteration 21/25 | Loss: 0.00098468
Iteration 22/25 | Loss: 0.00097448
Iteration 23/25 | Loss: 0.00096111
Iteration 24/25 | Loss: 0.00095903
Iteration 25/25 | Loss: 0.00096455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35519445
Iteration 2/25 | Loss: 0.00074265
Iteration 3/25 | Loss: 0.00070467
Iteration 4/25 | Loss: 0.00070466
Iteration 5/25 | Loss: 0.00070466
Iteration 6/25 | Loss: 0.00070466
Iteration 7/25 | Loss: 0.00070466
Iteration 8/25 | Loss: 0.00070466
Iteration 9/25 | Loss: 0.00070466
Iteration 10/25 | Loss: 0.00070466
Iteration 11/25 | Loss: 0.00070466
Iteration 12/25 | Loss: 0.00070466
Iteration 13/25 | Loss: 0.00070466
Iteration 14/25 | Loss: 0.00070466
Iteration 15/25 | Loss: 0.00070466
Iteration 16/25 | Loss: 0.00070466
Iteration 17/25 | Loss: 0.00070466
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007046629907563329, 0.0007046629907563329, 0.0007046629907563329, 0.0007046629907563329, 0.0007046629907563329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007046629907563329

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070466
Iteration 2/1000 | Loss: 0.00191839
Iteration 3/1000 | Loss: 0.00072140
Iteration 4/1000 | Loss: 0.00070884
Iteration 5/1000 | Loss: 0.00032698
Iteration 6/1000 | Loss: 0.00037254
Iteration 7/1000 | Loss: 0.00027619
Iteration 8/1000 | Loss: 0.00025149
Iteration 9/1000 | Loss: 0.00147354
Iteration 10/1000 | Loss: 0.00141095
Iteration 11/1000 | Loss: 0.00145408
Iteration 12/1000 | Loss: 0.00050518
Iteration 13/1000 | Loss: 0.00044815
Iteration 14/1000 | Loss: 0.00013824
Iteration 15/1000 | Loss: 0.00054718
Iteration 16/1000 | Loss: 0.00013246
Iteration 17/1000 | Loss: 0.00012630
Iteration 18/1000 | Loss: 0.00041943
Iteration 19/1000 | Loss: 0.00028196
Iteration 20/1000 | Loss: 0.00084854
Iteration 21/1000 | Loss: 0.00012373
Iteration 22/1000 | Loss: 0.00008052
Iteration 23/1000 | Loss: 0.00020606
Iteration 24/1000 | Loss: 0.00007784
Iteration 25/1000 | Loss: 0.00022119
Iteration 26/1000 | Loss: 0.00039525
Iteration 27/1000 | Loss: 0.00024458
Iteration 28/1000 | Loss: 0.00007774
Iteration 29/1000 | Loss: 0.00031748
Iteration 30/1000 | Loss: 0.00062749
Iteration 31/1000 | Loss: 0.00006483
Iteration 32/1000 | Loss: 0.00035384
Iteration 33/1000 | Loss: 0.00006148
Iteration 34/1000 | Loss: 0.00015381
Iteration 35/1000 | Loss: 0.00014188
Iteration 36/1000 | Loss: 0.00005705
Iteration 37/1000 | Loss: 0.00005504
Iteration 38/1000 | Loss: 0.00031000
Iteration 39/1000 | Loss: 0.00165711
Iteration 40/1000 | Loss: 0.00599940
Iteration 41/1000 | Loss: 0.00057794
Iteration 42/1000 | Loss: 0.00037003
Iteration 43/1000 | Loss: 0.00006909
Iteration 44/1000 | Loss: 0.00016860
Iteration 45/1000 | Loss: 0.00004739
Iteration 46/1000 | Loss: 0.00072777
Iteration 47/1000 | Loss: 0.00004708
Iteration 48/1000 | Loss: 0.00020989
Iteration 49/1000 | Loss: 0.00023100
Iteration 50/1000 | Loss: 0.00080303
Iteration 51/1000 | Loss: 0.00005746
Iteration 52/1000 | Loss: 0.00015902
Iteration 53/1000 | Loss: 0.00004451
Iteration 54/1000 | Loss: 0.00003928
Iteration 55/1000 | Loss: 0.00003727
Iteration 56/1000 | Loss: 0.00007715
Iteration 57/1000 | Loss: 0.00029447
Iteration 58/1000 | Loss: 0.00004794
Iteration 59/1000 | Loss: 0.00003282
Iteration 60/1000 | Loss: 0.00003072
Iteration 61/1000 | Loss: 0.00002895
Iteration 62/1000 | Loss: 0.00081170
Iteration 63/1000 | Loss: 0.00026708
Iteration 64/1000 | Loss: 0.00022511
Iteration 65/1000 | Loss: 0.00005264
Iteration 66/1000 | Loss: 0.00033274
Iteration 67/1000 | Loss: 0.00097366
Iteration 68/1000 | Loss: 0.00100249
Iteration 69/1000 | Loss: 0.00006582
Iteration 70/1000 | Loss: 0.00047201
Iteration 71/1000 | Loss: 0.00004708
Iteration 72/1000 | Loss: 0.00024378
Iteration 73/1000 | Loss: 0.00040758
Iteration 74/1000 | Loss: 0.00003940
Iteration 75/1000 | Loss: 0.00032903
Iteration 76/1000 | Loss: 0.00126971
Iteration 77/1000 | Loss: 0.00004123
Iteration 78/1000 | Loss: 0.00003400
Iteration 79/1000 | Loss: 0.00003169
Iteration 80/1000 | Loss: 0.00005627
Iteration 81/1000 | Loss: 0.00003137
Iteration 82/1000 | Loss: 0.00002939
Iteration 83/1000 | Loss: 0.00002796
Iteration 84/1000 | Loss: 0.00002638
Iteration 85/1000 | Loss: 0.00002573
Iteration 86/1000 | Loss: 0.00025385
Iteration 87/1000 | Loss: 0.00002451
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002319
Iteration 90/1000 | Loss: 0.00002283
Iteration 91/1000 | Loss: 0.00002260
Iteration 92/1000 | Loss: 0.00002257
Iteration 93/1000 | Loss: 0.00002247
Iteration 94/1000 | Loss: 0.00002245
Iteration 95/1000 | Loss: 0.00002243
Iteration 96/1000 | Loss: 0.00002237
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002232
Iteration 99/1000 | Loss: 0.00002231
Iteration 100/1000 | Loss: 0.00002229
Iteration 101/1000 | Loss: 0.00002229
Iteration 102/1000 | Loss: 0.00002227
Iteration 103/1000 | Loss: 0.00002227
Iteration 104/1000 | Loss: 0.00002227
Iteration 105/1000 | Loss: 0.00002227
Iteration 106/1000 | Loss: 0.00002227
Iteration 107/1000 | Loss: 0.00002227
Iteration 108/1000 | Loss: 0.00002227
Iteration 109/1000 | Loss: 0.00002226
Iteration 110/1000 | Loss: 0.00002223
Iteration 111/1000 | Loss: 0.00002223
Iteration 112/1000 | Loss: 0.00002222
Iteration 113/1000 | Loss: 0.00002221
Iteration 114/1000 | Loss: 0.00002221
Iteration 115/1000 | Loss: 0.00002220
Iteration 116/1000 | Loss: 0.00002220
Iteration 117/1000 | Loss: 0.00002220
Iteration 118/1000 | Loss: 0.00002219
Iteration 119/1000 | Loss: 0.00002207
Iteration 120/1000 | Loss: 0.00002205
Iteration 121/1000 | Loss: 0.00002204
Iteration 122/1000 | Loss: 0.00002204
Iteration 123/1000 | Loss: 0.00002204
Iteration 124/1000 | Loss: 0.00002203
Iteration 125/1000 | Loss: 0.00002203
Iteration 126/1000 | Loss: 0.00002203
Iteration 127/1000 | Loss: 0.00002202
Iteration 128/1000 | Loss: 0.00002202
Iteration 129/1000 | Loss: 0.00002202
Iteration 130/1000 | Loss: 0.00002201
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002201
Iteration 138/1000 | Loss: 0.00002200
Iteration 139/1000 | Loss: 0.00002200
Iteration 140/1000 | Loss: 0.00002200
Iteration 141/1000 | Loss: 0.00002200
Iteration 142/1000 | Loss: 0.00002200
Iteration 143/1000 | Loss: 0.00002200
Iteration 144/1000 | Loss: 0.00002200
Iteration 145/1000 | Loss: 0.00002199
Iteration 146/1000 | Loss: 0.00002199
Iteration 147/1000 | Loss: 0.00002198
Iteration 148/1000 | Loss: 0.00002196
Iteration 149/1000 | Loss: 0.00002196
Iteration 150/1000 | Loss: 0.00002196
Iteration 151/1000 | Loss: 0.00002196
Iteration 152/1000 | Loss: 0.00002196
Iteration 153/1000 | Loss: 0.00002196
Iteration 154/1000 | Loss: 0.00002196
Iteration 155/1000 | Loss: 0.00002196
Iteration 156/1000 | Loss: 0.00002195
Iteration 157/1000 | Loss: 0.00002195
Iteration 158/1000 | Loss: 0.00002195
Iteration 159/1000 | Loss: 0.00002195
Iteration 160/1000 | Loss: 0.00002195
Iteration 161/1000 | Loss: 0.00002195
Iteration 162/1000 | Loss: 0.00002195
Iteration 163/1000 | Loss: 0.00002195
Iteration 164/1000 | Loss: 0.00002194
Iteration 165/1000 | Loss: 0.00002194
Iteration 166/1000 | Loss: 0.00002194
Iteration 167/1000 | Loss: 0.00002194
Iteration 168/1000 | Loss: 0.00002194
Iteration 169/1000 | Loss: 0.00002194
Iteration 170/1000 | Loss: 0.00002194
Iteration 171/1000 | Loss: 0.00002194
Iteration 172/1000 | Loss: 0.00002194
Iteration 173/1000 | Loss: 0.00002194
Iteration 174/1000 | Loss: 0.00002194
Iteration 175/1000 | Loss: 0.00002194
Iteration 176/1000 | Loss: 0.00002194
Iteration 177/1000 | Loss: 0.00002193
Iteration 178/1000 | Loss: 0.00002193
Iteration 179/1000 | Loss: 0.00002193
Iteration 180/1000 | Loss: 0.00002193
Iteration 181/1000 | Loss: 0.00002193
Iteration 182/1000 | Loss: 0.00002193
Iteration 183/1000 | Loss: 0.00002193
Iteration 184/1000 | Loss: 0.00002193
Iteration 185/1000 | Loss: 0.00002193
Iteration 186/1000 | Loss: 0.00002193
Iteration 187/1000 | Loss: 0.00002193
Iteration 188/1000 | Loss: 0.00002192
Iteration 189/1000 | Loss: 0.00002192
Iteration 190/1000 | Loss: 0.00002192
Iteration 191/1000 | Loss: 0.00002192
Iteration 192/1000 | Loss: 0.00002192
Iteration 193/1000 | Loss: 0.00002192
Iteration 194/1000 | Loss: 0.00002192
Iteration 195/1000 | Loss: 0.00002192
Iteration 196/1000 | Loss: 0.00002192
Iteration 197/1000 | Loss: 0.00002192
Iteration 198/1000 | Loss: 0.00002192
Iteration 199/1000 | Loss: 0.00002192
Iteration 200/1000 | Loss: 0.00002192
Iteration 201/1000 | Loss: 0.00002192
Iteration 202/1000 | Loss: 0.00002192
Iteration 203/1000 | Loss: 0.00002192
Iteration 204/1000 | Loss: 0.00002192
Iteration 205/1000 | Loss: 0.00002192
Iteration 206/1000 | Loss: 0.00002192
Iteration 207/1000 | Loss: 0.00002192
Iteration 208/1000 | Loss: 0.00002192
Iteration 209/1000 | Loss: 0.00002192
Iteration 210/1000 | Loss: 0.00002192
Iteration 211/1000 | Loss: 0.00002192
Iteration 212/1000 | Loss: 0.00002192
Iteration 213/1000 | Loss: 0.00002192
Iteration 214/1000 | Loss: 0.00002192
Iteration 215/1000 | Loss: 0.00002192
Iteration 216/1000 | Loss: 0.00002192
Iteration 217/1000 | Loss: 0.00002192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.1917243429925293e-05, 2.1917243429925293e-05, 2.1917243429925293e-05, 2.1917243429925293e-05, 2.1917243429925293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1917243429925293e-05

Optimization complete. Final v2v error: 3.175405979156494 mm

Highest mean error: 5.5666046142578125 mm for frame 100

Lowest mean error: 2.479316473007202 mm for frame 22

Saving results

Total time: 183.0218415260315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_caren_posed_008/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_caren_posed_008/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427645
Iteration 2/25 | Loss: 0.00094098
Iteration 3/25 | Loss: 0.00083952
Iteration 4/25 | Loss: 0.00082333
Iteration 5/25 | Loss: 0.00081772
Iteration 6/25 | Loss: 0.00081625
Iteration 7/25 | Loss: 0.00081625
Iteration 8/25 | Loss: 0.00081625
Iteration 9/25 | Loss: 0.00081625
Iteration 10/25 | Loss: 0.00081625
Iteration 11/25 | Loss: 0.00081625
Iteration 12/25 | Loss: 0.00081625
Iteration 13/25 | Loss: 0.00081625
Iteration 14/25 | Loss: 0.00081625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008162485901266336, 0.0008162485901266336, 0.0008162485901266336, 0.0008162485901266336, 0.0008162485901266336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008162485901266336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.62625313
Iteration 2/25 | Loss: 0.00048597
Iteration 3/25 | Loss: 0.00048596
Iteration 4/25 | Loss: 0.00048596
Iteration 5/25 | Loss: 0.00048596
Iteration 6/25 | Loss: 0.00048596
Iteration 7/25 | Loss: 0.00048596
Iteration 8/25 | Loss: 0.00048596
Iteration 9/25 | Loss: 0.00048596
Iteration 10/25 | Loss: 0.00048596
Iteration 11/25 | Loss: 0.00048596
Iteration 12/25 | Loss: 0.00048596
Iteration 13/25 | Loss: 0.00048596
Iteration 14/25 | Loss: 0.00048596
Iteration 15/25 | Loss: 0.00048596
Iteration 16/25 | Loss: 0.00048596
Iteration 17/25 | Loss: 0.00048596
Iteration 18/25 | Loss: 0.00048596
Iteration 19/25 | Loss: 0.00048596
Iteration 20/25 | Loss: 0.00048596
Iteration 21/25 | Loss: 0.00048596
Iteration 22/25 | Loss: 0.00048596
Iteration 23/25 | Loss: 0.00048596
Iteration 24/25 | Loss: 0.00048596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00048595506814308465, 0.00048595506814308465, 0.00048595506814308465, 0.00048595506814308465, 0.00048595506814308465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048595506814308465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048596
Iteration 2/1000 | Loss: 0.00002082
Iteration 3/1000 | Loss: 0.00001217
Iteration 4/1000 | Loss: 0.00001079
Iteration 5/1000 | Loss: 0.00001028
Iteration 6/1000 | Loss: 0.00000984
Iteration 7/1000 | Loss: 0.00000951
Iteration 8/1000 | Loss: 0.00000931
Iteration 9/1000 | Loss: 0.00000924
Iteration 10/1000 | Loss: 0.00000921
Iteration 11/1000 | Loss: 0.00000914
Iteration 12/1000 | Loss: 0.00000907
Iteration 13/1000 | Loss: 0.00000906
Iteration 14/1000 | Loss: 0.00000906
Iteration 15/1000 | Loss: 0.00000905
Iteration 16/1000 | Loss: 0.00000905
Iteration 17/1000 | Loss: 0.00000904
Iteration 18/1000 | Loss: 0.00000900
Iteration 19/1000 | Loss: 0.00000899
Iteration 20/1000 | Loss: 0.00000899
Iteration 21/1000 | Loss: 0.00000898
Iteration 22/1000 | Loss: 0.00000895
Iteration 23/1000 | Loss: 0.00000895
Iteration 24/1000 | Loss: 0.00000895
Iteration 25/1000 | Loss: 0.00000895
Iteration 26/1000 | Loss: 0.00000895
Iteration 27/1000 | Loss: 0.00000895
Iteration 28/1000 | Loss: 0.00000894
Iteration 29/1000 | Loss: 0.00000894
Iteration 30/1000 | Loss: 0.00000893
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000891
Iteration 33/1000 | Loss: 0.00000891
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000890
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000890
Iteration 38/1000 | Loss: 0.00000890
Iteration 39/1000 | Loss: 0.00000889
Iteration 40/1000 | Loss: 0.00000889
Iteration 41/1000 | Loss: 0.00000888
Iteration 42/1000 | Loss: 0.00000888
Iteration 43/1000 | Loss: 0.00000888
Iteration 44/1000 | Loss: 0.00000887
Iteration 45/1000 | Loss: 0.00000886
Iteration 46/1000 | Loss: 0.00000886
Iteration 47/1000 | Loss: 0.00000885
Iteration 48/1000 | Loss: 0.00000885
Iteration 49/1000 | Loss: 0.00000885
Iteration 50/1000 | Loss: 0.00000884
Iteration 51/1000 | Loss: 0.00000884
Iteration 52/1000 | Loss: 0.00000884
Iteration 53/1000 | Loss: 0.00000883
Iteration 54/1000 | Loss: 0.00000883
Iteration 55/1000 | Loss: 0.00000882
Iteration 56/1000 | Loss: 0.00000882
Iteration 57/1000 | Loss: 0.00000881
Iteration 58/1000 | Loss: 0.00000881
Iteration 59/1000 | Loss: 0.00000881
Iteration 60/1000 | Loss: 0.00000880
Iteration 61/1000 | Loss: 0.00000880
Iteration 62/1000 | Loss: 0.00000880
Iteration 63/1000 | Loss: 0.00000880
Iteration 64/1000 | Loss: 0.00000880
Iteration 65/1000 | Loss: 0.00000880
Iteration 66/1000 | Loss: 0.00000880
Iteration 67/1000 | Loss: 0.00000880
Iteration 68/1000 | Loss: 0.00000879
Iteration 69/1000 | Loss: 0.00000879
Iteration 70/1000 | Loss: 0.00000879
Iteration 71/1000 | Loss: 0.00000879
Iteration 72/1000 | Loss: 0.00000878
Iteration 73/1000 | Loss: 0.00000878
Iteration 74/1000 | Loss: 0.00000878
Iteration 75/1000 | Loss: 0.00000877
Iteration 76/1000 | Loss: 0.00000877
Iteration 77/1000 | Loss: 0.00000877
Iteration 78/1000 | Loss: 0.00000877
Iteration 79/1000 | Loss: 0.00000876
Iteration 80/1000 | Loss: 0.00000876
Iteration 81/1000 | Loss: 0.00000876
Iteration 82/1000 | Loss: 0.00000875
Iteration 83/1000 | Loss: 0.00000875
Iteration 84/1000 | Loss: 0.00000875
Iteration 85/1000 | Loss: 0.00000875
Iteration 86/1000 | Loss: 0.00000875
Iteration 87/1000 | Loss: 0.00000874
Iteration 88/1000 | Loss: 0.00000874
Iteration 89/1000 | Loss: 0.00000874
Iteration 90/1000 | Loss: 0.00000874
Iteration 91/1000 | Loss: 0.00000874
Iteration 92/1000 | Loss: 0.00000874
Iteration 93/1000 | Loss: 0.00000874
Iteration 94/1000 | Loss: 0.00000873
Iteration 95/1000 | Loss: 0.00000873
Iteration 96/1000 | Loss: 0.00000873
Iteration 97/1000 | Loss: 0.00000873
Iteration 98/1000 | Loss: 0.00000873
Iteration 99/1000 | Loss: 0.00000873
Iteration 100/1000 | Loss: 0.00000873
Iteration 101/1000 | Loss: 0.00000872
Iteration 102/1000 | Loss: 0.00000872
Iteration 103/1000 | Loss: 0.00000872
Iteration 104/1000 | Loss: 0.00000872
Iteration 105/1000 | Loss: 0.00000871
Iteration 106/1000 | Loss: 0.00000871
Iteration 107/1000 | Loss: 0.00000871
Iteration 108/1000 | Loss: 0.00000871
Iteration 109/1000 | Loss: 0.00000871
Iteration 110/1000 | Loss: 0.00000871
Iteration 111/1000 | Loss: 0.00000871
Iteration 112/1000 | Loss: 0.00000870
Iteration 113/1000 | Loss: 0.00000870
Iteration 114/1000 | Loss: 0.00000870
Iteration 115/1000 | Loss: 0.00000870
Iteration 116/1000 | Loss: 0.00000870
Iteration 117/1000 | Loss: 0.00000869
Iteration 118/1000 | Loss: 0.00000869
Iteration 119/1000 | Loss: 0.00000869
Iteration 120/1000 | Loss: 0.00000869
Iteration 121/1000 | Loss: 0.00000868
Iteration 122/1000 | Loss: 0.00000868
Iteration 123/1000 | Loss: 0.00000868
Iteration 124/1000 | Loss: 0.00000867
Iteration 125/1000 | Loss: 0.00000867
Iteration 126/1000 | Loss: 0.00000867
Iteration 127/1000 | Loss: 0.00000867
Iteration 128/1000 | Loss: 0.00000866
Iteration 129/1000 | Loss: 0.00000866
Iteration 130/1000 | Loss: 0.00000866
Iteration 131/1000 | Loss: 0.00000866
Iteration 132/1000 | Loss: 0.00000866
Iteration 133/1000 | Loss: 0.00000866
Iteration 134/1000 | Loss: 0.00000866
Iteration 135/1000 | Loss: 0.00000865
Iteration 136/1000 | Loss: 0.00000865
Iteration 137/1000 | Loss: 0.00000865
Iteration 138/1000 | Loss: 0.00000865
Iteration 139/1000 | Loss: 0.00000865
Iteration 140/1000 | Loss: 0.00000865
Iteration 141/1000 | Loss: 0.00000864
Iteration 142/1000 | Loss: 0.00000864
Iteration 143/1000 | Loss: 0.00000864
Iteration 144/1000 | Loss: 0.00000864
Iteration 145/1000 | Loss: 0.00000864
Iteration 146/1000 | Loss: 0.00000864
Iteration 147/1000 | Loss: 0.00000864
Iteration 148/1000 | Loss: 0.00000864
Iteration 149/1000 | Loss: 0.00000864
Iteration 150/1000 | Loss: 0.00000864
Iteration 151/1000 | Loss: 0.00000864
Iteration 152/1000 | Loss: 0.00000864
Iteration 153/1000 | Loss: 0.00000864
Iteration 154/1000 | Loss: 0.00000864
Iteration 155/1000 | Loss: 0.00000863
Iteration 156/1000 | Loss: 0.00000863
Iteration 157/1000 | Loss: 0.00000863
Iteration 158/1000 | Loss: 0.00000863
Iteration 159/1000 | Loss: 0.00000863
Iteration 160/1000 | Loss: 0.00000862
Iteration 161/1000 | Loss: 0.00000862
Iteration 162/1000 | Loss: 0.00000862
Iteration 163/1000 | Loss: 0.00000862
Iteration 164/1000 | Loss: 0.00000862
Iteration 165/1000 | Loss: 0.00000862
Iteration 166/1000 | Loss: 0.00000862
Iteration 167/1000 | Loss: 0.00000862
Iteration 168/1000 | Loss: 0.00000862
Iteration 169/1000 | Loss: 0.00000862
Iteration 170/1000 | Loss: 0.00000862
Iteration 171/1000 | Loss: 0.00000862
Iteration 172/1000 | Loss: 0.00000862
Iteration 173/1000 | Loss: 0.00000862
Iteration 174/1000 | Loss: 0.00000862
Iteration 175/1000 | Loss: 0.00000862
Iteration 176/1000 | Loss: 0.00000862
Iteration 177/1000 | Loss: 0.00000862
Iteration 178/1000 | Loss: 0.00000862
Iteration 179/1000 | Loss: 0.00000862
Iteration 180/1000 | Loss: 0.00000862
Iteration 181/1000 | Loss: 0.00000862
Iteration 182/1000 | Loss: 0.00000862
Iteration 183/1000 | Loss: 0.00000862
Iteration 184/1000 | Loss: 0.00000862
Iteration 185/1000 | Loss: 0.00000862
Iteration 186/1000 | Loss: 0.00000862
Iteration 187/1000 | Loss: 0.00000862
Iteration 188/1000 | Loss: 0.00000862
Iteration 189/1000 | Loss: 0.00000862
Iteration 190/1000 | Loss: 0.00000862
Iteration 191/1000 | Loss: 0.00000862
Iteration 192/1000 | Loss: 0.00000862
Iteration 193/1000 | Loss: 0.00000862
Iteration 194/1000 | Loss: 0.00000862
Iteration 195/1000 | Loss: 0.00000862
Iteration 196/1000 | Loss: 0.00000862
Iteration 197/1000 | Loss: 0.00000862
Iteration 198/1000 | Loss: 0.00000862
Iteration 199/1000 | Loss: 0.00000862
Iteration 200/1000 | Loss: 0.00000862
Iteration 201/1000 | Loss: 0.00000862
Iteration 202/1000 | Loss: 0.00000862
Iteration 203/1000 | Loss: 0.00000862
Iteration 204/1000 | Loss: 0.00000862
Iteration 205/1000 | Loss: 0.00000862
Iteration 206/1000 | Loss: 0.00000862
Iteration 207/1000 | Loss: 0.00000862
Iteration 208/1000 | Loss: 0.00000862
Iteration 209/1000 | Loss: 0.00000862
Iteration 210/1000 | Loss: 0.00000862
Iteration 211/1000 | Loss: 0.00000862
Iteration 212/1000 | Loss: 0.00000862
Iteration 213/1000 | Loss: 0.00000862
Iteration 214/1000 | Loss: 0.00000862
Iteration 215/1000 | Loss: 0.00000862
Iteration 216/1000 | Loss: 0.00000862
Iteration 217/1000 | Loss: 0.00000862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [8.615321348770522e-06, 8.615321348770522e-06, 8.615321348770522e-06, 8.615321348770522e-06, 8.615321348770522e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.615321348770522e-06

Optimization complete. Final v2v error: 2.49783992767334 mm

Highest mean error: 3.029707431793213 mm for frame 92

Lowest mean error: 2.1542110443115234 mm for frame 194

Saving results

Total time: 41.12890648841858
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605542
Iteration 2/25 | Loss: 0.00111510
Iteration 3/25 | Loss: 0.00097213
Iteration 4/25 | Loss: 0.00095974
Iteration 5/25 | Loss: 0.00095781
Iteration 6/25 | Loss: 0.00095781
Iteration 7/25 | Loss: 0.00095781
Iteration 8/25 | Loss: 0.00095781
Iteration 9/25 | Loss: 0.00095781
Iteration 10/25 | Loss: 0.00095781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.000957807176746428, 0.000957807176746428, 0.000957807176746428, 0.000957807176746428, 0.000957807176746428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000957807176746428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.55299711
Iteration 2/25 | Loss: 0.00050223
Iteration 3/25 | Loss: 0.00050222
Iteration 4/25 | Loss: 0.00050222
Iteration 5/25 | Loss: 0.00050222
Iteration 6/25 | Loss: 0.00050222
Iteration 7/25 | Loss: 0.00050222
Iteration 8/25 | Loss: 0.00050222
Iteration 9/25 | Loss: 0.00050222
Iteration 10/25 | Loss: 0.00050222
Iteration 11/25 | Loss: 0.00050222
Iteration 12/25 | Loss: 0.00050222
Iteration 13/25 | Loss: 0.00050222
Iteration 14/25 | Loss: 0.00050222
Iteration 15/25 | Loss: 0.00050222
Iteration 16/25 | Loss: 0.00050222
Iteration 17/25 | Loss: 0.00050222
Iteration 18/25 | Loss: 0.00050222
Iteration 19/25 | Loss: 0.00050222
Iteration 20/25 | Loss: 0.00050222
Iteration 21/25 | Loss: 0.00050222
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005022224504500628, 0.0005022224504500628, 0.0005022224504500628, 0.0005022224504500628, 0.0005022224504500628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005022224504500628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050222
Iteration 2/1000 | Loss: 0.00002331
Iteration 3/1000 | Loss: 0.00001768
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001452
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001315
Iteration 12/1000 | Loss: 0.00001308
Iteration 13/1000 | Loss: 0.00001305
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001304
Iteration 16/1000 | Loss: 0.00001304
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001302
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001300
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001297
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001295
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001294
Iteration 47/1000 | Loss: 0.00001294
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001293
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001288
Iteration 76/1000 | Loss: 0.00001288
Iteration 77/1000 | Loss: 0.00001288
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001287
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001287
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001287
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001286
Iteration 93/1000 | Loss: 0.00001286
Iteration 94/1000 | Loss: 0.00001286
Iteration 95/1000 | Loss: 0.00001286
Iteration 96/1000 | Loss: 0.00001286
Iteration 97/1000 | Loss: 0.00001286
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001285
Iteration 102/1000 | Loss: 0.00001285
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001282
Iteration 119/1000 | Loss: 0.00001282
Iteration 120/1000 | Loss: 0.00001282
Iteration 121/1000 | Loss: 0.00001282
Iteration 122/1000 | Loss: 0.00001282
Iteration 123/1000 | Loss: 0.00001282
Iteration 124/1000 | Loss: 0.00001282
Iteration 125/1000 | Loss: 0.00001281
Iteration 126/1000 | Loss: 0.00001281
Iteration 127/1000 | Loss: 0.00001281
Iteration 128/1000 | Loss: 0.00001281
Iteration 129/1000 | Loss: 0.00001280
Iteration 130/1000 | Loss: 0.00001280
Iteration 131/1000 | Loss: 0.00001280
Iteration 132/1000 | Loss: 0.00001280
Iteration 133/1000 | Loss: 0.00001280
Iteration 134/1000 | Loss: 0.00001280
Iteration 135/1000 | Loss: 0.00001280
Iteration 136/1000 | Loss: 0.00001280
Iteration 137/1000 | Loss: 0.00001279
Iteration 138/1000 | Loss: 0.00001279
Iteration 139/1000 | Loss: 0.00001279
Iteration 140/1000 | Loss: 0.00001279
Iteration 141/1000 | Loss: 0.00001279
Iteration 142/1000 | Loss: 0.00001279
Iteration 143/1000 | Loss: 0.00001278
Iteration 144/1000 | Loss: 0.00001278
Iteration 145/1000 | Loss: 0.00001278
Iteration 146/1000 | Loss: 0.00001278
Iteration 147/1000 | Loss: 0.00001278
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001278
Iteration 154/1000 | Loss: 0.00001278
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001277
Iteration 163/1000 | Loss: 0.00001277
Iteration 164/1000 | Loss: 0.00001277
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001276
Iteration 167/1000 | Loss: 0.00001276
Iteration 168/1000 | Loss: 0.00001276
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001275
Iteration 173/1000 | Loss: 0.00001275
Iteration 174/1000 | Loss: 0.00001275
Iteration 175/1000 | Loss: 0.00001275
Iteration 176/1000 | Loss: 0.00001275
Iteration 177/1000 | Loss: 0.00001275
Iteration 178/1000 | Loss: 0.00001275
Iteration 179/1000 | Loss: 0.00001275
Iteration 180/1000 | Loss: 0.00001275
Iteration 181/1000 | Loss: 0.00001275
Iteration 182/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2753855116898194e-05, 1.2753855116898194e-05, 1.2753855116898194e-05, 1.2753855116898194e-05, 1.2753855116898194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2753855116898194e-05

Optimization complete. Final v2v error: 2.9489147663116455 mm

Highest mean error: 3.265019178390503 mm for frame 69

Lowest mean error: 2.7263574600219727 mm for frame 52

Saving results

Total time: 36.170414686203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362981
Iteration 2/25 | Loss: 0.00117549
Iteration 3/25 | Loss: 0.00103911
Iteration 4/25 | Loss: 0.00101785
Iteration 5/25 | Loss: 0.00100984
Iteration 6/25 | Loss: 0.00100766
Iteration 7/25 | Loss: 0.00100689
Iteration 8/25 | Loss: 0.00100689
Iteration 9/25 | Loss: 0.00100689
Iteration 10/25 | Loss: 0.00100689
Iteration 11/25 | Loss: 0.00100689
Iteration 12/25 | Loss: 0.00100689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010068860137835145, 0.0010068860137835145, 0.0010068860137835145, 0.0010068860137835145, 0.0010068860137835145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010068860137835145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36547387
Iteration 2/25 | Loss: 0.00078071
Iteration 3/25 | Loss: 0.00078071
Iteration 4/25 | Loss: 0.00078071
Iteration 5/25 | Loss: 0.00078071
Iteration 6/25 | Loss: 0.00078071
Iteration 7/25 | Loss: 0.00078071
Iteration 8/25 | Loss: 0.00078071
Iteration 9/25 | Loss: 0.00078071
Iteration 10/25 | Loss: 0.00078071
Iteration 11/25 | Loss: 0.00078071
Iteration 12/25 | Loss: 0.00078071
Iteration 13/25 | Loss: 0.00078071
Iteration 14/25 | Loss: 0.00078071
Iteration 15/25 | Loss: 0.00078071
Iteration 16/25 | Loss: 0.00078071
Iteration 17/25 | Loss: 0.00078071
Iteration 18/25 | Loss: 0.00078071
Iteration 19/25 | Loss: 0.00078071
Iteration 20/25 | Loss: 0.00078071
Iteration 21/25 | Loss: 0.00078071
Iteration 22/25 | Loss: 0.00078071
Iteration 23/25 | Loss: 0.00078071
Iteration 24/25 | Loss: 0.00078071
Iteration 25/25 | Loss: 0.00078071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078071
Iteration 2/1000 | Loss: 0.00005499
Iteration 3/1000 | Loss: 0.00003046
Iteration 4/1000 | Loss: 0.00002128
Iteration 5/1000 | Loss: 0.00001907
Iteration 6/1000 | Loss: 0.00001795
Iteration 7/1000 | Loss: 0.00001719
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001600
Iteration 12/1000 | Loss: 0.00001598
Iteration 13/1000 | Loss: 0.00001588
Iteration 14/1000 | Loss: 0.00001585
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001584
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001580
Iteration 19/1000 | Loss: 0.00001580
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001577
Iteration 23/1000 | Loss: 0.00001577
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001574
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001565
Iteration 36/1000 | Loss: 0.00001565
Iteration 37/1000 | Loss: 0.00001564
Iteration 38/1000 | Loss: 0.00001563
Iteration 39/1000 | Loss: 0.00001563
Iteration 40/1000 | Loss: 0.00001562
Iteration 41/1000 | Loss: 0.00001562
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00001561
Iteration 44/1000 | Loss: 0.00001561
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001556
Iteration 53/1000 | Loss: 0.00001556
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001555
Iteration 56/1000 | Loss: 0.00001554
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001553
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001549
Iteration 79/1000 | Loss: 0.00001549
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001548
Iteration 83/1000 | Loss: 0.00001548
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001546
Iteration 96/1000 | Loss: 0.00001546
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001542
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001540
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001538
Iteration 134/1000 | Loss: 0.00001538
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001538
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001537
Iteration 139/1000 | Loss: 0.00001537
Iteration 140/1000 | Loss: 0.00001537
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001536
Iteration 146/1000 | Loss: 0.00001536
Iteration 147/1000 | Loss: 0.00001536
Iteration 148/1000 | Loss: 0.00001536
Iteration 149/1000 | Loss: 0.00001536
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001535
Iteration 155/1000 | Loss: 0.00001535
Iteration 156/1000 | Loss: 0.00001535
Iteration 157/1000 | Loss: 0.00001535
Iteration 158/1000 | Loss: 0.00001535
Iteration 159/1000 | Loss: 0.00001535
Iteration 160/1000 | Loss: 0.00001535
Iteration 161/1000 | Loss: 0.00001535
Iteration 162/1000 | Loss: 0.00001535
Iteration 163/1000 | Loss: 0.00001535
Iteration 164/1000 | Loss: 0.00001535
Iteration 165/1000 | Loss: 0.00001534
Iteration 166/1000 | Loss: 0.00001534
Iteration 167/1000 | Loss: 0.00001534
Iteration 168/1000 | Loss: 0.00001534
Iteration 169/1000 | Loss: 0.00001534
Iteration 170/1000 | Loss: 0.00001534
Iteration 171/1000 | Loss: 0.00001534
Iteration 172/1000 | Loss: 0.00001534
Iteration 173/1000 | Loss: 0.00001534
Iteration 174/1000 | Loss: 0.00001534
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001533
Iteration 177/1000 | Loss: 0.00001533
Iteration 178/1000 | Loss: 0.00001533
Iteration 179/1000 | Loss: 0.00001533
Iteration 180/1000 | Loss: 0.00001533
Iteration 181/1000 | Loss: 0.00001533
Iteration 182/1000 | Loss: 0.00001532
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001532
Iteration 185/1000 | Loss: 0.00001532
Iteration 186/1000 | Loss: 0.00001532
Iteration 187/1000 | Loss: 0.00001532
Iteration 188/1000 | Loss: 0.00001532
Iteration 189/1000 | Loss: 0.00001532
Iteration 190/1000 | Loss: 0.00001532
Iteration 191/1000 | Loss: 0.00001531
Iteration 192/1000 | Loss: 0.00001531
Iteration 193/1000 | Loss: 0.00001531
Iteration 194/1000 | Loss: 0.00001531
Iteration 195/1000 | Loss: 0.00001531
Iteration 196/1000 | Loss: 0.00001531
Iteration 197/1000 | Loss: 0.00001531
Iteration 198/1000 | Loss: 0.00001531
Iteration 199/1000 | Loss: 0.00001531
Iteration 200/1000 | Loss: 0.00001531
Iteration 201/1000 | Loss: 0.00001531
Iteration 202/1000 | Loss: 0.00001531
Iteration 203/1000 | Loss: 0.00001531
Iteration 204/1000 | Loss: 0.00001531
Iteration 205/1000 | Loss: 0.00001531
Iteration 206/1000 | Loss: 0.00001531
Iteration 207/1000 | Loss: 0.00001531
Iteration 208/1000 | Loss: 0.00001531
Iteration 209/1000 | Loss: 0.00001531
Iteration 210/1000 | Loss: 0.00001531
Iteration 211/1000 | Loss: 0.00001531
Iteration 212/1000 | Loss: 0.00001530
Iteration 213/1000 | Loss: 0.00001530
Iteration 214/1000 | Loss: 0.00001530
Iteration 215/1000 | Loss: 0.00001530
Iteration 216/1000 | Loss: 0.00001530
Iteration 217/1000 | Loss: 0.00001530
Iteration 218/1000 | Loss: 0.00001530
Iteration 219/1000 | Loss: 0.00001530
Iteration 220/1000 | Loss: 0.00001530
Iteration 221/1000 | Loss: 0.00001530
Iteration 222/1000 | Loss: 0.00001530
Iteration 223/1000 | Loss: 0.00001530
Iteration 224/1000 | Loss: 0.00001529
Iteration 225/1000 | Loss: 0.00001529
Iteration 226/1000 | Loss: 0.00001529
Iteration 227/1000 | Loss: 0.00001529
Iteration 228/1000 | Loss: 0.00001529
Iteration 229/1000 | Loss: 0.00001529
Iteration 230/1000 | Loss: 0.00001529
Iteration 231/1000 | Loss: 0.00001529
Iteration 232/1000 | Loss: 0.00001529
Iteration 233/1000 | Loss: 0.00001528
Iteration 234/1000 | Loss: 0.00001528
Iteration 235/1000 | Loss: 0.00001528
Iteration 236/1000 | Loss: 0.00001528
Iteration 237/1000 | Loss: 0.00001528
Iteration 238/1000 | Loss: 0.00001528
Iteration 239/1000 | Loss: 0.00001528
Iteration 240/1000 | Loss: 0.00001528
Iteration 241/1000 | Loss: 0.00001528
Iteration 242/1000 | Loss: 0.00001528
Iteration 243/1000 | Loss: 0.00001528
Iteration 244/1000 | Loss: 0.00001528
Iteration 245/1000 | Loss: 0.00001528
Iteration 246/1000 | Loss: 0.00001528
Iteration 247/1000 | Loss: 0.00001528
Iteration 248/1000 | Loss: 0.00001528
Iteration 249/1000 | Loss: 0.00001528
Iteration 250/1000 | Loss: 0.00001528
Iteration 251/1000 | Loss: 0.00001527
Iteration 252/1000 | Loss: 0.00001527
Iteration 253/1000 | Loss: 0.00001527
Iteration 254/1000 | Loss: 0.00001527
Iteration 255/1000 | Loss: 0.00001527
Iteration 256/1000 | Loss: 0.00001527
Iteration 257/1000 | Loss: 0.00001527
Iteration 258/1000 | Loss: 0.00001527
Iteration 259/1000 | Loss: 0.00001527
Iteration 260/1000 | Loss: 0.00001527
Iteration 261/1000 | Loss: 0.00001527
Iteration 262/1000 | Loss: 0.00001527
Iteration 263/1000 | Loss: 0.00001527
Iteration 264/1000 | Loss: 0.00001527
Iteration 265/1000 | Loss: 0.00001527
Iteration 266/1000 | Loss: 0.00001527
Iteration 267/1000 | Loss: 0.00001527
Iteration 268/1000 | Loss: 0.00001527
Iteration 269/1000 | Loss: 0.00001527
Iteration 270/1000 | Loss: 0.00001527
Iteration 271/1000 | Loss: 0.00001527
Iteration 272/1000 | Loss: 0.00001527
Iteration 273/1000 | Loss: 0.00001527
Iteration 274/1000 | Loss: 0.00001527
Iteration 275/1000 | Loss: 0.00001527
Iteration 276/1000 | Loss: 0.00001527
Iteration 277/1000 | Loss: 0.00001527
Iteration 278/1000 | Loss: 0.00001527
Iteration 279/1000 | Loss: 0.00001527
Iteration 280/1000 | Loss: 0.00001527
Iteration 281/1000 | Loss: 0.00001527
Iteration 282/1000 | Loss: 0.00001527
Iteration 283/1000 | Loss: 0.00001527
Iteration 284/1000 | Loss: 0.00001527
Iteration 285/1000 | Loss: 0.00001527
Iteration 286/1000 | Loss: 0.00001527
Iteration 287/1000 | Loss: 0.00001527
Iteration 288/1000 | Loss: 0.00001527
Iteration 289/1000 | Loss: 0.00001527
Iteration 290/1000 | Loss: 0.00001527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.5269968571374193e-05, 1.5269968571374193e-05, 1.5269968571374193e-05, 1.5269968571374193e-05, 1.5269968571374193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5269968571374193e-05

Optimization complete. Final v2v error: 3.206303119659424 mm

Highest mean error: 4.283106327056885 mm for frame 152

Lowest mean error: 2.5773379802703857 mm for frame 168

Saving results

Total time: 47.35723161697388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810170
Iteration 2/25 | Loss: 0.00199494
Iteration 3/25 | Loss: 0.00123364
Iteration 4/25 | Loss: 0.00111447
Iteration 5/25 | Loss: 0.00108998
Iteration 6/25 | Loss: 0.00108808
Iteration 7/25 | Loss: 0.00108707
Iteration 8/25 | Loss: 0.00108184
Iteration 9/25 | Loss: 0.00108205
Iteration 10/25 | Loss: 0.00107445
Iteration 11/25 | Loss: 0.00108022
Iteration 12/25 | Loss: 0.00107594
Iteration 13/25 | Loss: 0.00107276
Iteration 14/25 | Loss: 0.00106194
Iteration 15/25 | Loss: 0.00105967
Iteration 16/25 | Loss: 0.00105871
Iteration 17/25 | Loss: 0.00105818
Iteration 18/25 | Loss: 0.00105917
Iteration 19/25 | Loss: 0.00105717
Iteration 20/25 | Loss: 0.00105654
Iteration 21/25 | Loss: 0.00105497
Iteration 22/25 | Loss: 0.00105299
Iteration 23/25 | Loss: 0.00105258
Iteration 24/25 | Loss: 0.00105241
Iteration 25/25 | Loss: 0.00105239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36471057
Iteration 2/25 | Loss: 0.00038584
Iteration 3/25 | Loss: 0.00029130
Iteration 4/25 | Loss: 0.00029130
Iteration 5/25 | Loss: 0.00029130
Iteration 6/25 | Loss: 0.00029129
Iteration 7/25 | Loss: 0.00029129
Iteration 8/25 | Loss: 0.00029129
Iteration 9/25 | Loss: 0.00029129
Iteration 10/25 | Loss: 0.00029129
Iteration 11/25 | Loss: 0.00029129
Iteration 12/25 | Loss: 0.00029129
Iteration 13/25 | Loss: 0.00029129
Iteration 14/25 | Loss: 0.00029129
Iteration 15/25 | Loss: 0.00029129
Iteration 16/25 | Loss: 0.00029129
Iteration 17/25 | Loss: 0.00029129
Iteration 18/25 | Loss: 0.00029129
Iteration 19/25 | Loss: 0.00029129
Iteration 20/25 | Loss: 0.00029129
Iteration 21/25 | Loss: 0.00029129
Iteration 22/25 | Loss: 0.00029129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00029129322501830757, 0.00029129322501830757, 0.00029129322501830757, 0.00029129322501830757, 0.00029129322501830757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029129322501830757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029129
Iteration 2/1000 | Loss: 0.00009294
Iteration 3/1000 | Loss: 0.00003247
Iteration 4/1000 | Loss: 0.00002895
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002673
Iteration 7/1000 | Loss: 0.00002590
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002495
Iteration 10/1000 | Loss: 0.00002468
Iteration 11/1000 | Loss: 0.00002444
Iteration 12/1000 | Loss: 0.00002439
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002433
Iteration 15/1000 | Loss: 0.00002430
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002429
Iteration 18/1000 | Loss: 0.00002429
Iteration 19/1000 | Loss: 0.00002428
Iteration 20/1000 | Loss: 0.00002424
Iteration 21/1000 | Loss: 0.00002424
Iteration 22/1000 | Loss: 0.00002424
Iteration 23/1000 | Loss: 0.00002424
Iteration 24/1000 | Loss: 0.00002423
Iteration 25/1000 | Loss: 0.00002422
Iteration 26/1000 | Loss: 0.00002422
Iteration 27/1000 | Loss: 0.00002421
Iteration 28/1000 | Loss: 0.00002420
Iteration 29/1000 | Loss: 0.00002420
Iteration 30/1000 | Loss: 0.00002420
Iteration 31/1000 | Loss: 0.00002420
Iteration 32/1000 | Loss: 0.00002419
Iteration 33/1000 | Loss: 0.00002419
Iteration 34/1000 | Loss: 0.00018327
Iteration 35/1000 | Loss: 0.00022688
Iteration 36/1000 | Loss: 0.00027816
Iteration 37/1000 | Loss: 0.00004004
Iteration 38/1000 | Loss: 0.00019872
Iteration 39/1000 | Loss: 0.00004287
Iteration 40/1000 | Loss: 0.00003386
Iteration 41/1000 | Loss: 0.00007972
Iteration 42/1000 | Loss: 0.00002857
Iteration 43/1000 | Loss: 0.00009704
Iteration 44/1000 | Loss: 0.00002611
Iteration 45/1000 | Loss: 0.00002552
Iteration 46/1000 | Loss: 0.00008500
Iteration 47/1000 | Loss: 0.00002519
Iteration 48/1000 | Loss: 0.00002452
Iteration 49/1000 | Loss: 0.00002392
Iteration 50/1000 | Loss: 0.00007972
Iteration 51/1000 | Loss: 0.00002356
Iteration 52/1000 | Loss: 0.00002347
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002347
Iteration 56/1000 | Loss: 0.00002347
Iteration 57/1000 | Loss: 0.00002347
Iteration 58/1000 | Loss: 0.00002347
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002346
Iteration 62/1000 | Loss: 0.00002346
Iteration 63/1000 | Loss: 0.00002346
Iteration 64/1000 | Loss: 0.00002345
Iteration 65/1000 | Loss: 0.00002345
Iteration 66/1000 | Loss: 0.00002345
Iteration 67/1000 | Loss: 0.00002345
Iteration 68/1000 | Loss: 0.00002345
Iteration 69/1000 | Loss: 0.00002345
Iteration 70/1000 | Loss: 0.00002345
Iteration 71/1000 | Loss: 0.00002344
Iteration 72/1000 | Loss: 0.00002344
Iteration 73/1000 | Loss: 0.00002344
Iteration 74/1000 | Loss: 0.00002344
Iteration 75/1000 | Loss: 0.00002344
Iteration 76/1000 | Loss: 0.00002344
Iteration 77/1000 | Loss: 0.00002344
Iteration 78/1000 | Loss: 0.00002344
Iteration 79/1000 | Loss: 0.00002344
Iteration 80/1000 | Loss: 0.00002344
Iteration 81/1000 | Loss: 0.00002343
Iteration 82/1000 | Loss: 0.00002343
Iteration 83/1000 | Loss: 0.00002343
Iteration 84/1000 | Loss: 0.00002343
Iteration 85/1000 | Loss: 0.00002343
Iteration 86/1000 | Loss: 0.00002342
Iteration 87/1000 | Loss: 0.00002342
Iteration 88/1000 | Loss: 0.00002342
Iteration 89/1000 | Loss: 0.00002342
Iteration 90/1000 | Loss: 0.00002342
Iteration 91/1000 | Loss: 0.00002342
Iteration 92/1000 | Loss: 0.00002342
Iteration 93/1000 | Loss: 0.00002342
Iteration 94/1000 | Loss: 0.00002342
Iteration 95/1000 | Loss: 0.00002342
Iteration 96/1000 | Loss: 0.00002342
Iteration 97/1000 | Loss: 0.00002342
Iteration 98/1000 | Loss: 0.00002342
Iteration 99/1000 | Loss: 0.00002342
Iteration 100/1000 | Loss: 0.00002342
Iteration 101/1000 | Loss: 0.00002342
Iteration 102/1000 | Loss: 0.00002342
Iteration 103/1000 | Loss: 0.00002342
Iteration 104/1000 | Loss: 0.00002342
Iteration 105/1000 | Loss: 0.00002342
Iteration 106/1000 | Loss: 0.00002342
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002342
Iteration 112/1000 | Loss: 0.00002342
Iteration 113/1000 | Loss: 0.00002342
Iteration 114/1000 | Loss: 0.00002342
Iteration 115/1000 | Loss: 0.00002342
Iteration 116/1000 | Loss: 0.00002342
Iteration 117/1000 | Loss: 0.00002342
Iteration 118/1000 | Loss: 0.00002342
Iteration 119/1000 | Loss: 0.00002342
Iteration 120/1000 | Loss: 0.00002342
Iteration 121/1000 | Loss: 0.00002342
Iteration 122/1000 | Loss: 0.00002342
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002342
Iteration 125/1000 | Loss: 0.00002342
Iteration 126/1000 | Loss: 0.00002342
Iteration 127/1000 | Loss: 0.00002342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.3419299395754933e-05, 2.3419299395754933e-05, 2.3419299395754933e-05, 2.3419299395754933e-05, 2.3419299395754933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3419299395754933e-05

Optimization complete. Final v2v error: 3.9621469974517822 mm

Highest mean error: 5.087423324584961 mm for frame 96

Lowest mean error: 3.503899097442627 mm for frame 135

Saving results

Total time: 103.64220857620239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996578
Iteration 2/25 | Loss: 0.00246419
Iteration 3/25 | Loss: 0.00179643
Iteration 4/25 | Loss: 0.00156318
Iteration 5/25 | Loss: 0.00144039
Iteration 6/25 | Loss: 0.00137286
Iteration 7/25 | Loss: 0.00128008
Iteration 8/25 | Loss: 0.00123038
Iteration 9/25 | Loss: 0.00120034
Iteration 10/25 | Loss: 0.00118210
Iteration 11/25 | Loss: 0.00116904
Iteration 12/25 | Loss: 0.00114945
Iteration 13/25 | Loss: 0.00115068
Iteration 14/25 | Loss: 0.00114490
Iteration 15/25 | Loss: 0.00114455
Iteration 16/25 | Loss: 0.00114438
Iteration 17/25 | Loss: 0.00114434
Iteration 18/25 | Loss: 0.00114434
Iteration 19/25 | Loss: 0.00114434
Iteration 20/25 | Loss: 0.00114434
Iteration 21/25 | Loss: 0.00114434
Iteration 22/25 | Loss: 0.00114434
Iteration 23/25 | Loss: 0.00114434
Iteration 24/25 | Loss: 0.00114434
Iteration 25/25 | Loss: 0.00114433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36221230
Iteration 2/25 | Loss: 0.00127806
Iteration 3/25 | Loss: 0.00081204
Iteration 4/25 | Loss: 0.00081203
Iteration 5/25 | Loss: 0.00081203
Iteration 6/25 | Loss: 0.00081203
Iteration 7/25 | Loss: 0.00081203
Iteration 8/25 | Loss: 0.00081203
Iteration 9/25 | Loss: 0.00081203
Iteration 10/25 | Loss: 0.00081203
Iteration 11/25 | Loss: 0.00081203
Iteration 12/25 | Loss: 0.00081203
Iteration 13/25 | Loss: 0.00081203
Iteration 14/25 | Loss: 0.00081203
Iteration 15/25 | Loss: 0.00081203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008120330167002976, 0.0008120330167002976, 0.0008120330167002976, 0.0008120330167002976, 0.0008120330167002976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008120330167002976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081203
Iteration 2/1000 | Loss: 0.00199950
Iteration 3/1000 | Loss: 0.00181577
Iteration 4/1000 | Loss: 0.00465757
Iteration 5/1000 | Loss: 0.00137336
Iteration 6/1000 | Loss: 0.00711387
Iteration 7/1000 | Loss: 0.00063457
Iteration 8/1000 | Loss: 0.00193503
Iteration 9/1000 | Loss: 0.00357603
Iteration 10/1000 | Loss: 0.00140469
Iteration 11/1000 | Loss: 0.00037537
Iteration 12/1000 | Loss: 0.00031583
Iteration 13/1000 | Loss: 0.00063500
Iteration 14/1000 | Loss: 0.00025276
Iteration 15/1000 | Loss: 0.00063652
Iteration 16/1000 | Loss: 0.00012159
Iteration 17/1000 | Loss: 0.00021555
Iteration 18/1000 | Loss: 0.00058558
Iteration 19/1000 | Loss: 0.00222346
Iteration 20/1000 | Loss: 0.00046877
Iteration 21/1000 | Loss: 0.00056179
Iteration 22/1000 | Loss: 0.00016076
Iteration 23/1000 | Loss: 0.00012118
Iteration 24/1000 | Loss: 0.00007629
Iteration 25/1000 | Loss: 0.00007014
Iteration 26/1000 | Loss: 0.00006577
Iteration 27/1000 | Loss: 0.00008139
Iteration 28/1000 | Loss: 0.00017721
Iteration 29/1000 | Loss: 0.00004745
Iteration 30/1000 | Loss: 0.00006899
Iteration 31/1000 | Loss: 0.00016332
Iteration 32/1000 | Loss: 0.00004498
Iteration 33/1000 | Loss: 0.00045522
Iteration 34/1000 | Loss: 0.00039274
Iteration 35/1000 | Loss: 0.00016827
Iteration 36/1000 | Loss: 0.00005581
Iteration 37/1000 | Loss: 0.00004585
Iteration 38/1000 | Loss: 0.00016562
Iteration 39/1000 | Loss: 0.00025962
Iteration 40/1000 | Loss: 0.00005074
Iteration 41/1000 | Loss: 0.00005582
Iteration 42/1000 | Loss: 0.00003457
Iteration 43/1000 | Loss: 0.00002925
Iteration 44/1000 | Loss: 0.00003335
Iteration 45/1000 | Loss: 0.00009614
Iteration 46/1000 | Loss: 0.00002909
Iteration 47/1000 | Loss: 0.00002611
Iteration 48/1000 | Loss: 0.00014988
Iteration 49/1000 | Loss: 0.00032413
Iteration 50/1000 | Loss: 0.00012234
Iteration 51/1000 | Loss: 0.00005904
Iteration 52/1000 | Loss: 0.00009848
Iteration 53/1000 | Loss: 0.00002441
Iteration 54/1000 | Loss: 0.00005219
Iteration 55/1000 | Loss: 0.00012035
Iteration 56/1000 | Loss: 0.00002309
Iteration 57/1000 | Loss: 0.00002988
Iteration 58/1000 | Loss: 0.00002234
Iteration 59/1000 | Loss: 0.00008244
Iteration 60/1000 | Loss: 0.00002209
Iteration 61/1000 | Loss: 0.00002174
Iteration 62/1000 | Loss: 0.00002172
Iteration 63/1000 | Loss: 0.00002144
Iteration 64/1000 | Loss: 0.00005827
Iteration 65/1000 | Loss: 0.00007603
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00003601
Iteration 68/1000 | Loss: 0.00002123
Iteration 69/1000 | Loss: 0.00002121
Iteration 70/1000 | Loss: 0.00002121
Iteration 71/1000 | Loss: 0.00002121
Iteration 72/1000 | Loss: 0.00002121
Iteration 73/1000 | Loss: 0.00002121
Iteration 74/1000 | Loss: 0.00002121
Iteration 75/1000 | Loss: 0.00002121
Iteration 76/1000 | Loss: 0.00002120
Iteration 77/1000 | Loss: 0.00002120
Iteration 78/1000 | Loss: 0.00002120
Iteration 79/1000 | Loss: 0.00002120
Iteration 80/1000 | Loss: 0.00002118
Iteration 81/1000 | Loss: 0.00002117
Iteration 82/1000 | Loss: 0.00002117
Iteration 83/1000 | Loss: 0.00002116
Iteration 84/1000 | Loss: 0.00002116
Iteration 85/1000 | Loss: 0.00002116
Iteration 86/1000 | Loss: 0.00002116
Iteration 87/1000 | Loss: 0.00002114
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002108
Iteration 90/1000 | Loss: 0.00002108
Iteration 91/1000 | Loss: 0.00002108
Iteration 92/1000 | Loss: 0.00002107
Iteration 93/1000 | Loss: 0.00002106
Iteration 94/1000 | Loss: 0.00006154
Iteration 95/1000 | Loss: 0.00004790
Iteration 96/1000 | Loss: 0.00002116
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002099
Iteration 101/1000 | Loss: 0.00002099
Iteration 102/1000 | Loss: 0.00002099
Iteration 103/1000 | Loss: 0.00002099
Iteration 104/1000 | Loss: 0.00002099
Iteration 105/1000 | Loss: 0.00002099
Iteration 106/1000 | Loss: 0.00002098
Iteration 107/1000 | Loss: 0.00002098
Iteration 108/1000 | Loss: 0.00002098
Iteration 109/1000 | Loss: 0.00002098
Iteration 110/1000 | Loss: 0.00002098
Iteration 111/1000 | Loss: 0.00002098
Iteration 112/1000 | Loss: 0.00002098
Iteration 113/1000 | Loss: 0.00002098
Iteration 114/1000 | Loss: 0.00002097
Iteration 115/1000 | Loss: 0.00002097
Iteration 116/1000 | Loss: 0.00002097
Iteration 117/1000 | Loss: 0.00002097
Iteration 118/1000 | Loss: 0.00002097
Iteration 119/1000 | Loss: 0.00002097
Iteration 120/1000 | Loss: 0.00002097
Iteration 121/1000 | Loss: 0.00002097
Iteration 122/1000 | Loss: 0.00002097
Iteration 123/1000 | Loss: 0.00002097
Iteration 124/1000 | Loss: 0.00002097
Iteration 125/1000 | Loss: 0.00002097
Iteration 126/1000 | Loss: 0.00002097
Iteration 127/1000 | Loss: 0.00002097
Iteration 128/1000 | Loss: 0.00002097
Iteration 129/1000 | Loss: 0.00002097
Iteration 130/1000 | Loss: 0.00002097
Iteration 131/1000 | Loss: 0.00002096
Iteration 132/1000 | Loss: 0.00002096
Iteration 133/1000 | Loss: 0.00002096
Iteration 134/1000 | Loss: 0.00002096
Iteration 135/1000 | Loss: 0.00002096
Iteration 136/1000 | Loss: 0.00002096
Iteration 137/1000 | Loss: 0.00002096
Iteration 138/1000 | Loss: 0.00002096
Iteration 139/1000 | Loss: 0.00002096
Iteration 140/1000 | Loss: 0.00002096
Iteration 141/1000 | Loss: 0.00002096
Iteration 142/1000 | Loss: 0.00002096
Iteration 143/1000 | Loss: 0.00002096
Iteration 144/1000 | Loss: 0.00002096
Iteration 145/1000 | Loss: 0.00002096
Iteration 146/1000 | Loss: 0.00002096
Iteration 147/1000 | Loss: 0.00002096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.0962512280675583e-05, 2.0962512280675583e-05, 2.0962512280675583e-05, 2.0962512280675583e-05, 2.0962512280675583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0962512280675583e-05

Optimization complete. Final v2v error: 3.3310635089874268 mm

Highest mean error: 10.488102912902832 mm for frame 114

Lowest mean error: 2.438145160675049 mm for frame 2

Saving results

Total time: 139.7168436050415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773984
Iteration 2/25 | Loss: 0.00160098
Iteration 3/25 | Loss: 0.00118229
Iteration 4/25 | Loss: 0.00114418
Iteration 5/25 | Loss: 0.00113916
Iteration 6/25 | Loss: 0.00113757
Iteration 7/25 | Loss: 0.00113704
Iteration 8/25 | Loss: 0.00113696
Iteration 9/25 | Loss: 0.00113696
Iteration 10/25 | Loss: 0.00113696
Iteration 11/25 | Loss: 0.00113696
Iteration 12/25 | Loss: 0.00113696
Iteration 13/25 | Loss: 0.00113696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011369645362719893, 0.0011369645362719893, 0.0011369645362719893, 0.0011369645362719893, 0.0011369645362719893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011369645362719893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54165506
Iteration 2/25 | Loss: 0.00065634
Iteration 3/25 | Loss: 0.00065634
Iteration 4/25 | Loss: 0.00065633
Iteration 5/25 | Loss: 0.00065633
Iteration 6/25 | Loss: 0.00065633
Iteration 7/25 | Loss: 0.00065633
Iteration 8/25 | Loss: 0.00065633
Iteration 9/25 | Loss: 0.00065633
Iteration 10/25 | Loss: 0.00065633
Iteration 11/25 | Loss: 0.00065633
Iteration 12/25 | Loss: 0.00065633
Iteration 13/25 | Loss: 0.00065633
Iteration 14/25 | Loss: 0.00065633
Iteration 15/25 | Loss: 0.00065633
Iteration 16/25 | Loss: 0.00065633
Iteration 17/25 | Loss: 0.00065633
Iteration 18/25 | Loss: 0.00065633
Iteration 19/25 | Loss: 0.00065633
Iteration 20/25 | Loss: 0.00065633
Iteration 21/25 | Loss: 0.00065633
Iteration 22/25 | Loss: 0.00065633
Iteration 23/25 | Loss: 0.00065633
Iteration 24/25 | Loss: 0.00065633
Iteration 25/25 | Loss: 0.00065633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065633
Iteration 2/1000 | Loss: 0.00007365
Iteration 3/1000 | Loss: 0.00004900
Iteration 4/1000 | Loss: 0.00004013
Iteration 5/1000 | Loss: 0.00003663
Iteration 6/1000 | Loss: 0.00003446
Iteration 7/1000 | Loss: 0.00003309
Iteration 8/1000 | Loss: 0.00003208
Iteration 9/1000 | Loss: 0.00003139
Iteration 10/1000 | Loss: 0.00003102
Iteration 11/1000 | Loss: 0.00003077
Iteration 12/1000 | Loss: 0.00003051
Iteration 13/1000 | Loss: 0.00003027
Iteration 14/1000 | Loss: 0.00003003
Iteration 15/1000 | Loss: 0.00002988
Iteration 16/1000 | Loss: 0.00002972
Iteration 17/1000 | Loss: 0.00002960
Iteration 18/1000 | Loss: 0.00002952
Iteration 19/1000 | Loss: 0.00002946
Iteration 20/1000 | Loss: 0.00002935
Iteration 21/1000 | Loss: 0.00002934
Iteration 22/1000 | Loss: 0.00002933
Iteration 23/1000 | Loss: 0.00002933
Iteration 24/1000 | Loss: 0.00002933
Iteration 25/1000 | Loss: 0.00002932
Iteration 26/1000 | Loss: 0.00002926
Iteration 27/1000 | Loss: 0.00002926
Iteration 28/1000 | Loss: 0.00002926
Iteration 29/1000 | Loss: 0.00002925
Iteration 30/1000 | Loss: 0.00002923
Iteration 31/1000 | Loss: 0.00002923
Iteration 32/1000 | Loss: 0.00002923
Iteration 33/1000 | Loss: 0.00002923
Iteration 34/1000 | Loss: 0.00002922
Iteration 35/1000 | Loss: 0.00002922
Iteration 36/1000 | Loss: 0.00002922
Iteration 37/1000 | Loss: 0.00002922
Iteration 38/1000 | Loss: 0.00002922
Iteration 39/1000 | Loss: 0.00002922
Iteration 40/1000 | Loss: 0.00002921
Iteration 41/1000 | Loss: 0.00002921
Iteration 42/1000 | Loss: 0.00002921
Iteration 43/1000 | Loss: 0.00002917
Iteration 44/1000 | Loss: 0.00002917
Iteration 45/1000 | Loss: 0.00002916
Iteration 46/1000 | Loss: 0.00002916
Iteration 47/1000 | Loss: 0.00002914
Iteration 48/1000 | Loss: 0.00002914
Iteration 49/1000 | Loss: 0.00002914
Iteration 50/1000 | Loss: 0.00002912
Iteration 51/1000 | Loss: 0.00002912
Iteration 52/1000 | Loss: 0.00002912
Iteration 53/1000 | Loss: 0.00002912
Iteration 54/1000 | Loss: 0.00002912
Iteration 55/1000 | Loss: 0.00002912
Iteration 56/1000 | Loss: 0.00002912
Iteration 57/1000 | Loss: 0.00002911
Iteration 58/1000 | Loss: 0.00002911
Iteration 59/1000 | Loss: 0.00002911
Iteration 60/1000 | Loss: 0.00002911
Iteration 61/1000 | Loss: 0.00002911
Iteration 62/1000 | Loss: 0.00002910
Iteration 63/1000 | Loss: 0.00002909
Iteration 64/1000 | Loss: 0.00002909
Iteration 65/1000 | Loss: 0.00002908
Iteration 66/1000 | Loss: 0.00002908
Iteration 67/1000 | Loss: 0.00002908
Iteration 68/1000 | Loss: 0.00002907
Iteration 69/1000 | Loss: 0.00002907
Iteration 70/1000 | Loss: 0.00002907
Iteration 71/1000 | Loss: 0.00002907
Iteration 72/1000 | Loss: 0.00002906
Iteration 73/1000 | Loss: 0.00002906
Iteration 74/1000 | Loss: 0.00002906
Iteration 75/1000 | Loss: 0.00002906
Iteration 76/1000 | Loss: 0.00002906
Iteration 77/1000 | Loss: 0.00002906
Iteration 78/1000 | Loss: 0.00002905
Iteration 79/1000 | Loss: 0.00002905
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002904
Iteration 82/1000 | Loss: 0.00002904
Iteration 83/1000 | Loss: 0.00002904
Iteration 84/1000 | Loss: 0.00002903
Iteration 85/1000 | Loss: 0.00002903
Iteration 86/1000 | Loss: 0.00002903
Iteration 87/1000 | Loss: 0.00002903
Iteration 88/1000 | Loss: 0.00002903
Iteration 89/1000 | Loss: 0.00002903
Iteration 90/1000 | Loss: 0.00002902
Iteration 91/1000 | Loss: 0.00002902
Iteration 92/1000 | Loss: 0.00002902
Iteration 93/1000 | Loss: 0.00002902
Iteration 94/1000 | Loss: 0.00002902
Iteration 95/1000 | Loss: 0.00002901
Iteration 96/1000 | Loss: 0.00002901
Iteration 97/1000 | Loss: 0.00002901
Iteration 98/1000 | Loss: 0.00002901
Iteration 99/1000 | Loss: 0.00002901
Iteration 100/1000 | Loss: 0.00002901
Iteration 101/1000 | Loss: 0.00002901
Iteration 102/1000 | Loss: 0.00002900
Iteration 103/1000 | Loss: 0.00002900
Iteration 104/1000 | Loss: 0.00002900
Iteration 105/1000 | Loss: 0.00002900
Iteration 106/1000 | Loss: 0.00002899
Iteration 107/1000 | Loss: 0.00002899
Iteration 108/1000 | Loss: 0.00002899
Iteration 109/1000 | Loss: 0.00002899
Iteration 110/1000 | Loss: 0.00002899
Iteration 111/1000 | Loss: 0.00002899
Iteration 112/1000 | Loss: 0.00002899
Iteration 113/1000 | Loss: 0.00002899
Iteration 114/1000 | Loss: 0.00002899
Iteration 115/1000 | Loss: 0.00002899
Iteration 116/1000 | Loss: 0.00002899
Iteration 117/1000 | Loss: 0.00002898
Iteration 118/1000 | Loss: 0.00002898
Iteration 119/1000 | Loss: 0.00002898
Iteration 120/1000 | Loss: 0.00002898
Iteration 121/1000 | Loss: 0.00002898
Iteration 122/1000 | Loss: 0.00002898
Iteration 123/1000 | Loss: 0.00002897
Iteration 124/1000 | Loss: 0.00002897
Iteration 125/1000 | Loss: 0.00002897
Iteration 126/1000 | Loss: 0.00002897
Iteration 127/1000 | Loss: 0.00002897
Iteration 128/1000 | Loss: 0.00002897
Iteration 129/1000 | Loss: 0.00002896
Iteration 130/1000 | Loss: 0.00002896
Iteration 131/1000 | Loss: 0.00002896
Iteration 132/1000 | Loss: 0.00002896
Iteration 133/1000 | Loss: 0.00002896
Iteration 134/1000 | Loss: 0.00002895
Iteration 135/1000 | Loss: 0.00002895
Iteration 136/1000 | Loss: 0.00002895
Iteration 137/1000 | Loss: 0.00002895
Iteration 138/1000 | Loss: 0.00002895
Iteration 139/1000 | Loss: 0.00002895
Iteration 140/1000 | Loss: 0.00002895
Iteration 141/1000 | Loss: 0.00002895
Iteration 142/1000 | Loss: 0.00002895
Iteration 143/1000 | Loss: 0.00002894
Iteration 144/1000 | Loss: 0.00002894
Iteration 145/1000 | Loss: 0.00002894
Iteration 146/1000 | Loss: 0.00002894
Iteration 147/1000 | Loss: 0.00002894
Iteration 148/1000 | Loss: 0.00002894
Iteration 149/1000 | Loss: 0.00002894
Iteration 150/1000 | Loss: 0.00002894
Iteration 151/1000 | Loss: 0.00002894
Iteration 152/1000 | Loss: 0.00002893
Iteration 153/1000 | Loss: 0.00002893
Iteration 154/1000 | Loss: 0.00002893
Iteration 155/1000 | Loss: 0.00002893
Iteration 156/1000 | Loss: 0.00002893
Iteration 157/1000 | Loss: 0.00002893
Iteration 158/1000 | Loss: 0.00002893
Iteration 159/1000 | Loss: 0.00002893
Iteration 160/1000 | Loss: 0.00002892
Iteration 161/1000 | Loss: 0.00002892
Iteration 162/1000 | Loss: 0.00002892
Iteration 163/1000 | Loss: 0.00002892
Iteration 164/1000 | Loss: 0.00002892
Iteration 165/1000 | Loss: 0.00002892
Iteration 166/1000 | Loss: 0.00002891
Iteration 167/1000 | Loss: 0.00002891
Iteration 168/1000 | Loss: 0.00002891
Iteration 169/1000 | Loss: 0.00002891
Iteration 170/1000 | Loss: 0.00002891
Iteration 171/1000 | Loss: 0.00002890
Iteration 172/1000 | Loss: 0.00002890
Iteration 173/1000 | Loss: 0.00002890
Iteration 174/1000 | Loss: 0.00002890
Iteration 175/1000 | Loss: 0.00002890
Iteration 176/1000 | Loss: 0.00002890
Iteration 177/1000 | Loss: 0.00002890
Iteration 178/1000 | Loss: 0.00002890
Iteration 179/1000 | Loss: 0.00002890
Iteration 180/1000 | Loss: 0.00002889
Iteration 181/1000 | Loss: 0.00002889
Iteration 182/1000 | Loss: 0.00002889
Iteration 183/1000 | Loss: 0.00002889
Iteration 184/1000 | Loss: 0.00002889
Iteration 185/1000 | Loss: 0.00002889
Iteration 186/1000 | Loss: 0.00002889
Iteration 187/1000 | Loss: 0.00002889
Iteration 188/1000 | Loss: 0.00002889
Iteration 189/1000 | Loss: 0.00002889
Iteration 190/1000 | Loss: 0.00002889
Iteration 191/1000 | Loss: 0.00002889
Iteration 192/1000 | Loss: 0.00002889
Iteration 193/1000 | Loss: 0.00002888
Iteration 194/1000 | Loss: 0.00002888
Iteration 195/1000 | Loss: 0.00002888
Iteration 196/1000 | Loss: 0.00002888
Iteration 197/1000 | Loss: 0.00002888
Iteration 198/1000 | Loss: 0.00002887
Iteration 199/1000 | Loss: 0.00002887
Iteration 200/1000 | Loss: 0.00002887
Iteration 201/1000 | Loss: 0.00002887
Iteration 202/1000 | Loss: 0.00002887
Iteration 203/1000 | Loss: 0.00002887
Iteration 204/1000 | Loss: 0.00002887
Iteration 205/1000 | Loss: 0.00002887
Iteration 206/1000 | Loss: 0.00002887
Iteration 207/1000 | Loss: 0.00002887
Iteration 208/1000 | Loss: 0.00002887
Iteration 209/1000 | Loss: 0.00002887
Iteration 210/1000 | Loss: 0.00002887
Iteration 211/1000 | Loss: 0.00002887
Iteration 212/1000 | Loss: 0.00002887
Iteration 213/1000 | Loss: 0.00002886
Iteration 214/1000 | Loss: 0.00002886
Iteration 215/1000 | Loss: 0.00002886
Iteration 216/1000 | Loss: 0.00002886
Iteration 217/1000 | Loss: 0.00002886
Iteration 218/1000 | Loss: 0.00002886
Iteration 219/1000 | Loss: 0.00002886
Iteration 220/1000 | Loss: 0.00002886
Iteration 221/1000 | Loss: 0.00002886
Iteration 222/1000 | Loss: 0.00002886
Iteration 223/1000 | Loss: 0.00002886
Iteration 224/1000 | Loss: 0.00002886
Iteration 225/1000 | Loss: 0.00002886
Iteration 226/1000 | Loss: 0.00002886
Iteration 227/1000 | Loss: 0.00002886
Iteration 228/1000 | Loss: 0.00002886
Iteration 229/1000 | Loss: 0.00002885
Iteration 230/1000 | Loss: 0.00002885
Iteration 231/1000 | Loss: 0.00002885
Iteration 232/1000 | Loss: 0.00002885
Iteration 233/1000 | Loss: 0.00002885
Iteration 234/1000 | Loss: 0.00002885
Iteration 235/1000 | Loss: 0.00002885
Iteration 236/1000 | Loss: 0.00002885
Iteration 237/1000 | Loss: 0.00002885
Iteration 238/1000 | Loss: 0.00002885
Iteration 239/1000 | Loss: 0.00002885
Iteration 240/1000 | Loss: 0.00002885
Iteration 241/1000 | Loss: 0.00002885
Iteration 242/1000 | Loss: 0.00002885
Iteration 243/1000 | Loss: 0.00002885
Iteration 244/1000 | Loss: 0.00002884
Iteration 245/1000 | Loss: 0.00002884
Iteration 246/1000 | Loss: 0.00002884
Iteration 247/1000 | Loss: 0.00002884
Iteration 248/1000 | Loss: 0.00002884
Iteration 249/1000 | Loss: 0.00002884
Iteration 250/1000 | Loss: 0.00002884
Iteration 251/1000 | Loss: 0.00002884
Iteration 252/1000 | Loss: 0.00002884
Iteration 253/1000 | Loss: 0.00002884
Iteration 254/1000 | Loss: 0.00002884
Iteration 255/1000 | Loss: 0.00002884
Iteration 256/1000 | Loss: 0.00002884
Iteration 257/1000 | Loss: 0.00002884
Iteration 258/1000 | Loss: 0.00002884
Iteration 259/1000 | Loss: 0.00002884
Iteration 260/1000 | Loss: 0.00002884
Iteration 261/1000 | Loss: 0.00002884
Iteration 262/1000 | Loss: 0.00002884
Iteration 263/1000 | Loss: 0.00002884
Iteration 264/1000 | Loss: 0.00002884
Iteration 265/1000 | Loss: 0.00002884
Iteration 266/1000 | Loss: 0.00002884
Iteration 267/1000 | Loss: 0.00002884
Iteration 268/1000 | Loss: 0.00002884
Iteration 269/1000 | Loss: 0.00002884
Iteration 270/1000 | Loss: 0.00002884
Iteration 271/1000 | Loss: 0.00002884
Iteration 272/1000 | Loss: 0.00002884
Iteration 273/1000 | Loss: 0.00002884
Iteration 274/1000 | Loss: 0.00002884
Iteration 275/1000 | Loss: 0.00002884
Iteration 276/1000 | Loss: 0.00002884
Iteration 277/1000 | Loss: 0.00002884
Iteration 278/1000 | Loss: 0.00002884
Iteration 279/1000 | Loss: 0.00002884
Iteration 280/1000 | Loss: 0.00002884
Iteration 281/1000 | Loss: 0.00002884
Iteration 282/1000 | Loss: 0.00002884
Iteration 283/1000 | Loss: 0.00002884
Iteration 284/1000 | Loss: 0.00002884
Iteration 285/1000 | Loss: 0.00002884
Iteration 286/1000 | Loss: 0.00002884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [2.884057175833732e-05, 2.884057175833732e-05, 2.884057175833732e-05, 2.884057175833732e-05, 2.884057175833732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.884057175833732e-05

Optimization complete. Final v2v error: 4.331428050994873 mm

Highest mean error: 5.809314250946045 mm for frame 34

Lowest mean error: 3.327678918838501 mm for frame 1

Saving results

Total time: 57.14114308357239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792435
Iteration 2/25 | Loss: 0.00133710
Iteration 3/25 | Loss: 0.00130060
Iteration 4/25 | Loss: 0.00114836
Iteration 5/25 | Loss: 0.00108133
Iteration 6/25 | Loss: 0.00108001
Iteration 7/25 | Loss: 0.00106928
Iteration 8/25 | Loss: 0.00106708
Iteration 9/25 | Loss: 0.00106683
Iteration 10/25 | Loss: 0.00106679
Iteration 11/25 | Loss: 0.00106679
Iteration 12/25 | Loss: 0.00106679
Iteration 13/25 | Loss: 0.00106679
Iteration 14/25 | Loss: 0.00106679
Iteration 15/25 | Loss: 0.00106679
Iteration 16/25 | Loss: 0.00106679
Iteration 17/25 | Loss: 0.00106679
Iteration 18/25 | Loss: 0.00106679
Iteration 19/25 | Loss: 0.00106679
Iteration 20/25 | Loss: 0.00106679
Iteration 21/25 | Loss: 0.00106679
Iteration 22/25 | Loss: 0.00106679
Iteration 23/25 | Loss: 0.00106679
Iteration 24/25 | Loss: 0.00106679
Iteration 25/25 | Loss: 0.00106679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79935479
Iteration 2/25 | Loss: 0.00076521
Iteration 3/25 | Loss: 0.00076518
Iteration 4/25 | Loss: 0.00076518
Iteration 5/25 | Loss: 0.00076518
Iteration 6/25 | Loss: 0.00076518
Iteration 7/25 | Loss: 0.00076518
Iteration 8/25 | Loss: 0.00076518
Iteration 9/25 | Loss: 0.00076518
Iteration 10/25 | Loss: 0.00076518
Iteration 11/25 | Loss: 0.00076518
Iteration 12/25 | Loss: 0.00076518
Iteration 13/25 | Loss: 0.00076518
Iteration 14/25 | Loss: 0.00076518
Iteration 15/25 | Loss: 0.00076518
Iteration 16/25 | Loss: 0.00076518
Iteration 17/25 | Loss: 0.00076518
Iteration 18/25 | Loss: 0.00076518
Iteration 19/25 | Loss: 0.00076518
Iteration 20/25 | Loss: 0.00076518
Iteration 21/25 | Loss: 0.00076518
Iteration 22/25 | Loss: 0.00076518
Iteration 23/25 | Loss: 0.00076518
Iteration 24/25 | Loss: 0.00076518
Iteration 25/25 | Loss: 0.00076518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076518
Iteration 2/1000 | Loss: 0.00004712
Iteration 3/1000 | Loss: 0.00002948
Iteration 4/1000 | Loss: 0.00002387
Iteration 5/1000 | Loss: 0.00002210
Iteration 6/1000 | Loss: 0.00002100
Iteration 7/1000 | Loss: 0.00002049
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001981
Iteration 10/1000 | Loss: 0.00001971
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001953
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001927
Iteration 15/1000 | Loss: 0.00001926
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001917
Iteration 18/1000 | Loss: 0.00001917
Iteration 19/1000 | Loss: 0.00001916
Iteration 20/1000 | Loss: 0.00001915
Iteration 21/1000 | Loss: 0.00001914
Iteration 22/1000 | Loss: 0.00001914
Iteration 23/1000 | Loss: 0.00001913
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001912
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001911
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001909
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001909
Iteration 41/1000 | Loss: 0.00001909
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001906
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001905
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001905
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001904
Iteration 64/1000 | Loss: 0.00001904
Iteration 65/1000 | Loss: 0.00001904
Iteration 66/1000 | Loss: 0.00001904
Iteration 67/1000 | Loss: 0.00001904
Iteration 68/1000 | Loss: 0.00001904
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001903
Iteration 71/1000 | Loss: 0.00001903
Iteration 72/1000 | Loss: 0.00001903
Iteration 73/1000 | Loss: 0.00001903
Iteration 74/1000 | Loss: 0.00001903
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001903
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.902587064250838e-05, 1.902587064250838e-05, 1.902587064250838e-05, 1.902587064250838e-05, 1.902587064250838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.902587064250838e-05

Optimization complete. Final v2v error: 3.5517287254333496 mm

Highest mean error: 4.2585225105285645 mm for frame 106

Lowest mean error: 2.8983395099639893 mm for frame 144

Saving results

Total time: 45.43009853363037
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518058
Iteration 2/25 | Loss: 0.00134014
Iteration 3/25 | Loss: 0.00107763
Iteration 4/25 | Loss: 0.00106443
Iteration 5/25 | Loss: 0.00105990
Iteration 6/25 | Loss: 0.00105888
Iteration 7/25 | Loss: 0.00105888
Iteration 8/25 | Loss: 0.00105888
Iteration 9/25 | Loss: 0.00105888
Iteration 10/25 | Loss: 0.00105888
Iteration 11/25 | Loss: 0.00105888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010588777950033545, 0.0010588777950033545, 0.0010588777950033545, 0.0010588777950033545, 0.0010588777950033545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010588777950033545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08804893
Iteration 2/25 | Loss: 0.00064450
Iteration 3/25 | Loss: 0.00064450
Iteration 4/25 | Loss: 0.00064450
Iteration 5/25 | Loss: 0.00064450
Iteration 6/25 | Loss: 0.00064450
Iteration 7/25 | Loss: 0.00064450
Iteration 8/25 | Loss: 0.00064450
Iteration 9/25 | Loss: 0.00064450
Iteration 10/25 | Loss: 0.00064450
Iteration 11/25 | Loss: 0.00064450
Iteration 12/25 | Loss: 0.00064450
Iteration 13/25 | Loss: 0.00064450
Iteration 14/25 | Loss: 0.00064450
Iteration 15/25 | Loss: 0.00064450
Iteration 16/25 | Loss: 0.00064450
Iteration 17/25 | Loss: 0.00064450
Iteration 18/25 | Loss: 0.00064450
Iteration 19/25 | Loss: 0.00064450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006444969912990928, 0.0006444969912990928, 0.0006444969912990928, 0.0006444969912990928, 0.0006444969912990928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006444969912990928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064450
Iteration 2/1000 | Loss: 0.00007123
Iteration 3/1000 | Loss: 0.00004632
Iteration 4/1000 | Loss: 0.00003735
Iteration 5/1000 | Loss: 0.00003444
Iteration 6/1000 | Loss: 0.00003285
Iteration 7/1000 | Loss: 0.00003168
Iteration 8/1000 | Loss: 0.00003078
Iteration 9/1000 | Loss: 0.00002985
Iteration 10/1000 | Loss: 0.00002926
Iteration 11/1000 | Loss: 0.00002894
Iteration 12/1000 | Loss: 0.00002860
Iteration 13/1000 | Loss: 0.00002831
Iteration 14/1000 | Loss: 0.00002804
Iteration 15/1000 | Loss: 0.00002782
Iteration 16/1000 | Loss: 0.00002762
Iteration 17/1000 | Loss: 0.00002746
Iteration 18/1000 | Loss: 0.00002729
Iteration 19/1000 | Loss: 0.00002717
Iteration 20/1000 | Loss: 0.00002703
Iteration 21/1000 | Loss: 0.00002701
Iteration 22/1000 | Loss: 0.00002701
Iteration 23/1000 | Loss: 0.00002691
Iteration 24/1000 | Loss: 0.00002688
Iteration 25/1000 | Loss: 0.00002685
Iteration 26/1000 | Loss: 0.00002684
Iteration 27/1000 | Loss: 0.00002680
Iteration 28/1000 | Loss: 0.00002672
Iteration 29/1000 | Loss: 0.00002669
Iteration 30/1000 | Loss: 0.00002669
Iteration 31/1000 | Loss: 0.00002667
Iteration 32/1000 | Loss: 0.00002666
Iteration 33/1000 | Loss: 0.00002666
Iteration 34/1000 | Loss: 0.00002663
Iteration 35/1000 | Loss: 0.00002663
Iteration 36/1000 | Loss: 0.00002663
Iteration 37/1000 | Loss: 0.00002663
Iteration 38/1000 | Loss: 0.00002663
Iteration 39/1000 | Loss: 0.00002662
Iteration 40/1000 | Loss: 0.00002662
Iteration 41/1000 | Loss: 0.00002661
Iteration 42/1000 | Loss: 0.00002661
Iteration 43/1000 | Loss: 0.00002660
Iteration 44/1000 | Loss: 0.00002659
Iteration 45/1000 | Loss: 0.00002659
Iteration 46/1000 | Loss: 0.00002659
Iteration 47/1000 | Loss: 0.00002659
Iteration 48/1000 | Loss: 0.00002659
Iteration 49/1000 | Loss: 0.00002659
Iteration 50/1000 | Loss: 0.00002659
Iteration 51/1000 | Loss: 0.00002659
Iteration 52/1000 | Loss: 0.00002659
Iteration 53/1000 | Loss: 0.00002659
Iteration 54/1000 | Loss: 0.00002659
Iteration 55/1000 | Loss: 0.00002658
Iteration 56/1000 | Loss: 0.00002657
Iteration 57/1000 | Loss: 0.00002657
Iteration 58/1000 | Loss: 0.00002657
Iteration 59/1000 | Loss: 0.00002656
Iteration 60/1000 | Loss: 0.00002656
Iteration 61/1000 | Loss: 0.00002656
Iteration 62/1000 | Loss: 0.00002656
Iteration 63/1000 | Loss: 0.00002655
Iteration 64/1000 | Loss: 0.00002655
Iteration 65/1000 | Loss: 0.00002655
Iteration 66/1000 | Loss: 0.00002655
Iteration 67/1000 | Loss: 0.00002655
Iteration 68/1000 | Loss: 0.00002655
Iteration 69/1000 | Loss: 0.00002654
Iteration 70/1000 | Loss: 0.00002654
Iteration 71/1000 | Loss: 0.00002654
Iteration 72/1000 | Loss: 0.00002654
Iteration 73/1000 | Loss: 0.00002654
Iteration 74/1000 | Loss: 0.00002653
Iteration 75/1000 | Loss: 0.00002653
Iteration 76/1000 | Loss: 0.00002653
Iteration 77/1000 | Loss: 0.00002653
Iteration 78/1000 | Loss: 0.00002652
Iteration 79/1000 | Loss: 0.00002652
Iteration 80/1000 | Loss: 0.00002652
Iteration 81/1000 | Loss: 0.00002652
Iteration 82/1000 | Loss: 0.00002652
Iteration 83/1000 | Loss: 0.00002652
Iteration 84/1000 | Loss: 0.00002652
Iteration 85/1000 | Loss: 0.00002652
Iteration 86/1000 | Loss: 0.00002652
Iteration 87/1000 | Loss: 0.00002651
Iteration 88/1000 | Loss: 0.00002651
Iteration 89/1000 | Loss: 0.00002651
Iteration 90/1000 | Loss: 0.00002651
Iteration 91/1000 | Loss: 0.00002651
Iteration 92/1000 | Loss: 0.00002650
Iteration 93/1000 | Loss: 0.00002650
Iteration 94/1000 | Loss: 0.00002650
Iteration 95/1000 | Loss: 0.00002650
Iteration 96/1000 | Loss: 0.00002650
Iteration 97/1000 | Loss: 0.00002649
Iteration 98/1000 | Loss: 0.00002649
Iteration 99/1000 | Loss: 0.00002649
Iteration 100/1000 | Loss: 0.00002649
Iteration 101/1000 | Loss: 0.00002648
Iteration 102/1000 | Loss: 0.00002648
Iteration 103/1000 | Loss: 0.00002648
Iteration 104/1000 | Loss: 0.00002648
Iteration 105/1000 | Loss: 0.00002648
Iteration 106/1000 | Loss: 0.00002648
Iteration 107/1000 | Loss: 0.00002648
Iteration 108/1000 | Loss: 0.00002648
Iteration 109/1000 | Loss: 0.00002647
Iteration 110/1000 | Loss: 0.00002647
Iteration 111/1000 | Loss: 0.00002647
Iteration 112/1000 | Loss: 0.00002647
Iteration 113/1000 | Loss: 0.00002647
Iteration 114/1000 | Loss: 0.00002647
Iteration 115/1000 | Loss: 0.00002647
Iteration 116/1000 | Loss: 0.00002647
Iteration 117/1000 | Loss: 0.00002646
Iteration 118/1000 | Loss: 0.00002646
Iteration 119/1000 | Loss: 0.00002646
Iteration 120/1000 | Loss: 0.00002646
Iteration 121/1000 | Loss: 0.00002645
Iteration 122/1000 | Loss: 0.00002645
Iteration 123/1000 | Loss: 0.00002645
Iteration 124/1000 | Loss: 0.00002645
Iteration 125/1000 | Loss: 0.00002645
Iteration 126/1000 | Loss: 0.00002645
Iteration 127/1000 | Loss: 0.00002645
Iteration 128/1000 | Loss: 0.00002645
Iteration 129/1000 | Loss: 0.00002645
Iteration 130/1000 | Loss: 0.00002644
Iteration 131/1000 | Loss: 0.00002644
Iteration 132/1000 | Loss: 0.00002644
Iteration 133/1000 | Loss: 0.00002644
Iteration 134/1000 | Loss: 0.00002644
Iteration 135/1000 | Loss: 0.00002644
Iteration 136/1000 | Loss: 0.00002644
Iteration 137/1000 | Loss: 0.00002644
Iteration 138/1000 | Loss: 0.00002644
Iteration 139/1000 | Loss: 0.00002644
Iteration 140/1000 | Loss: 0.00002644
Iteration 141/1000 | Loss: 0.00002644
Iteration 142/1000 | Loss: 0.00002644
Iteration 143/1000 | Loss: 0.00002644
Iteration 144/1000 | Loss: 0.00002644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.6442590751685202e-05, 2.6442590751685202e-05, 2.6442590751685202e-05, 2.6442590751685202e-05, 2.6442590751685202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6442590751685202e-05

Optimization complete. Final v2v error: 4.068695068359375 mm

Highest mean error: 5.906681537628174 mm for frame 160

Lowest mean error: 2.8681211471557617 mm for frame 197

Saving results

Total time: 53.575661420822144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967242
Iteration 2/25 | Loss: 0.00216528
Iteration 3/25 | Loss: 0.00122075
Iteration 4/25 | Loss: 0.00119629
Iteration 5/25 | Loss: 0.00119037
Iteration 6/25 | Loss: 0.00118733
Iteration 7/25 | Loss: 0.00118733
Iteration 8/25 | Loss: 0.00118733
Iteration 9/25 | Loss: 0.00118733
Iteration 10/25 | Loss: 0.00118733
Iteration 11/25 | Loss: 0.00118733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011873282492160797, 0.0011873282492160797, 0.0011873282492160797, 0.0011873282492160797, 0.0011873282492160797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011873282492160797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52158982
Iteration 2/25 | Loss: 0.00064691
Iteration 3/25 | Loss: 0.00064691
Iteration 4/25 | Loss: 0.00064691
Iteration 5/25 | Loss: 0.00064691
Iteration 6/25 | Loss: 0.00064691
Iteration 7/25 | Loss: 0.00064691
Iteration 8/25 | Loss: 0.00064691
Iteration 9/25 | Loss: 0.00064691
Iteration 10/25 | Loss: 0.00064691
Iteration 11/25 | Loss: 0.00064691
Iteration 12/25 | Loss: 0.00064691
Iteration 13/25 | Loss: 0.00064691
Iteration 14/25 | Loss: 0.00064691
Iteration 15/25 | Loss: 0.00064691
Iteration 16/25 | Loss: 0.00064691
Iteration 17/25 | Loss: 0.00064691
Iteration 18/25 | Loss: 0.00064691
Iteration 19/25 | Loss: 0.00064691
Iteration 20/25 | Loss: 0.00064691
Iteration 21/25 | Loss: 0.00064691
Iteration 22/25 | Loss: 0.00064691
Iteration 23/25 | Loss: 0.00064691
Iteration 24/25 | Loss: 0.00064691
Iteration 25/25 | Loss: 0.00064691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064691
Iteration 2/1000 | Loss: 0.00009020
Iteration 3/1000 | Loss: 0.00007479
Iteration 4/1000 | Loss: 0.00006762
Iteration 5/1000 | Loss: 0.00006565
Iteration 6/1000 | Loss: 0.00006410
Iteration 7/1000 | Loss: 0.00006267
Iteration 8/1000 | Loss: 0.00006089
Iteration 9/1000 | Loss: 0.00005949
Iteration 10/1000 | Loss: 0.00005840
Iteration 11/1000 | Loss: 0.00005752
Iteration 12/1000 | Loss: 0.00005644
Iteration 13/1000 | Loss: 0.00005539
Iteration 14/1000 | Loss: 0.00005467
Iteration 15/1000 | Loss: 0.00005414
Iteration 16/1000 | Loss: 0.00005365
Iteration 17/1000 | Loss: 0.00005314
Iteration 18/1000 | Loss: 0.00005265
Iteration 19/1000 | Loss: 0.00005227
Iteration 20/1000 | Loss: 0.00005188
Iteration 21/1000 | Loss: 0.00005159
Iteration 22/1000 | Loss: 0.00005135
Iteration 23/1000 | Loss: 0.00005119
Iteration 24/1000 | Loss: 0.00005103
Iteration 25/1000 | Loss: 0.00005098
Iteration 26/1000 | Loss: 0.00005098
Iteration 27/1000 | Loss: 0.00005082
Iteration 28/1000 | Loss: 0.00005069
Iteration 29/1000 | Loss: 0.00005068
Iteration 30/1000 | Loss: 0.00005067
Iteration 31/1000 | Loss: 0.00005067
Iteration 32/1000 | Loss: 0.00005067
Iteration 33/1000 | Loss: 0.00005067
Iteration 34/1000 | Loss: 0.00005067
Iteration 35/1000 | Loss: 0.00005066
Iteration 36/1000 | Loss: 0.00005066
Iteration 37/1000 | Loss: 0.00005066
Iteration 38/1000 | Loss: 0.00005064
Iteration 39/1000 | Loss: 0.00005064
Iteration 40/1000 | Loss: 0.00005064
Iteration 41/1000 | Loss: 0.00005064
Iteration 42/1000 | Loss: 0.00005064
Iteration 43/1000 | Loss: 0.00005064
Iteration 44/1000 | Loss: 0.00005064
Iteration 45/1000 | Loss: 0.00005064
Iteration 46/1000 | Loss: 0.00005064
Iteration 47/1000 | Loss: 0.00005063
Iteration 48/1000 | Loss: 0.00005063
Iteration 49/1000 | Loss: 0.00005063
Iteration 50/1000 | Loss: 0.00005063
Iteration 51/1000 | Loss: 0.00005063
Iteration 52/1000 | Loss: 0.00005062
Iteration 53/1000 | Loss: 0.00005062
Iteration 54/1000 | Loss: 0.00005062
Iteration 55/1000 | Loss: 0.00005062
Iteration 56/1000 | Loss: 0.00005062
Iteration 57/1000 | Loss: 0.00005062
Iteration 58/1000 | Loss: 0.00005062
Iteration 59/1000 | Loss: 0.00005061
Iteration 60/1000 | Loss: 0.00005061
Iteration 61/1000 | Loss: 0.00005061
Iteration 62/1000 | Loss: 0.00005061
Iteration 63/1000 | Loss: 0.00005061
Iteration 64/1000 | Loss: 0.00005061
Iteration 65/1000 | Loss: 0.00005061
Iteration 66/1000 | Loss: 0.00005061
Iteration 67/1000 | Loss: 0.00005061
Iteration 68/1000 | Loss: 0.00005061
Iteration 69/1000 | Loss: 0.00005061
Iteration 70/1000 | Loss: 0.00005061
Iteration 71/1000 | Loss: 0.00005061
Iteration 72/1000 | Loss: 0.00005061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [5.061351475887932e-05, 5.061351475887932e-05, 5.061351475887932e-05, 5.061351475887932e-05, 5.061351475887932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.061351475887932e-05

Optimization complete. Final v2v error: 5.577600955963135 mm

Highest mean error: 6.0054802894592285 mm for frame 54

Lowest mean error: 5.191027641296387 mm for frame 128

Saving results

Total time: 58.00514054298401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00211182
Iteration 2/25 | Loss: 0.00109934
Iteration 3/25 | Loss: 0.00100926
Iteration 4/25 | Loss: 0.00098711
Iteration 5/25 | Loss: 0.00098050
Iteration 6/25 | Loss: 0.00097954
Iteration 7/25 | Loss: 0.00097954
Iteration 8/25 | Loss: 0.00097954
Iteration 9/25 | Loss: 0.00097954
Iteration 10/25 | Loss: 0.00097954
Iteration 11/25 | Loss: 0.00097954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009795364458113909, 0.0009795364458113909, 0.0009795364458113909, 0.0009795364458113909, 0.0009795364458113909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009795364458113909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33941257
Iteration 2/25 | Loss: 0.00103912
Iteration 3/25 | Loss: 0.00103912
Iteration 4/25 | Loss: 0.00103912
Iteration 5/25 | Loss: 0.00103912
Iteration 6/25 | Loss: 0.00103912
Iteration 7/25 | Loss: 0.00103912
Iteration 8/25 | Loss: 0.00103912
Iteration 9/25 | Loss: 0.00103912
Iteration 10/25 | Loss: 0.00103912
Iteration 11/25 | Loss: 0.00103912
Iteration 12/25 | Loss: 0.00103912
Iteration 13/25 | Loss: 0.00103912
Iteration 14/25 | Loss: 0.00103912
Iteration 15/25 | Loss: 0.00103912
Iteration 16/25 | Loss: 0.00103912
Iteration 17/25 | Loss: 0.00103912
Iteration 18/25 | Loss: 0.00103912
Iteration 19/25 | Loss: 0.00103912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010391166433691978, 0.0010391166433691978, 0.0010391166433691978, 0.0010391166433691978, 0.0010391166433691978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010391166433691978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103912
Iteration 2/1000 | Loss: 0.00004427
Iteration 3/1000 | Loss: 0.00002605
Iteration 4/1000 | Loss: 0.00001676
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001444
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001347
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001325
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001308
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001296
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001295
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001293
Iteration 20/1000 | Loss: 0.00001293
Iteration 21/1000 | Loss: 0.00001293
Iteration 22/1000 | Loss: 0.00001293
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001292
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001290
Iteration 31/1000 | Loss: 0.00001290
Iteration 32/1000 | Loss: 0.00001289
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001286
Iteration 37/1000 | Loss: 0.00001286
Iteration 38/1000 | Loss: 0.00001286
Iteration 39/1000 | Loss: 0.00001286
Iteration 40/1000 | Loss: 0.00001285
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001281
Iteration 43/1000 | Loss: 0.00001281
Iteration 44/1000 | Loss: 0.00001280
Iteration 45/1000 | Loss: 0.00001280
Iteration 46/1000 | Loss: 0.00001279
Iteration 47/1000 | Loss: 0.00001279
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001278
Iteration 53/1000 | Loss: 0.00001278
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001278
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001274
Iteration 72/1000 | Loss: 0.00001274
Iteration 73/1000 | Loss: 0.00001274
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001273
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001273
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001273
Iteration 84/1000 | Loss: 0.00001273
Iteration 85/1000 | Loss: 0.00001273
Iteration 86/1000 | Loss: 0.00001273
Iteration 87/1000 | Loss: 0.00001273
Iteration 88/1000 | Loss: 0.00001273
Iteration 89/1000 | Loss: 0.00001272
Iteration 90/1000 | Loss: 0.00001272
Iteration 91/1000 | Loss: 0.00001272
Iteration 92/1000 | Loss: 0.00001272
Iteration 93/1000 | Loss: 0.00001272
Iteration 94/1000 | Loss: 0.00001272
Iteration 95/1000 | Loss: 0.00001272
Iteration 96/1000 | Loss: 0.00001272
Iteration 97/1000 | Loss: 0.00001272
Iteration 98/1000 | Loss: 0.00001272
Iteration 99/1000 | Loss: 0.00001272
Iteration 100/1000 | Loss: 0.00001272
Iteration 101/1000 | Loss: 0.00001272
Iteration 102/1000 | Loss: 0.00001272
Iteration 103/1000 | Loss: 0.00001272
Iteration 104/1000 | Loss: 0.00001272
Iteration 105/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.2723615327558946e-05, 1.2723615327558946e-05, 1.2723615327558946e-05, 1.2723615327558946e-05, 1.2723615327558946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2723615327558946e-05

Optimization complete. Final v2v error: 3.0381438732147217 mm

Highest mean error: 3.383970260620117 mm for frame 180

Lowest mean error: 2.837874412536621 mm for frame 40

Saving results

Total time: 31.90288233757019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351776
Iteration 2/25 | Loss: 0.00099008
Iteration 3/25 | Loss: 0.00092111
Iteration 4/25 | Loss: 0.00091366
Iteration 5/25 | Loss: 0.00091119
Iteration 6/25 | Loss: 0.00091061
Iteration 7/25 | Loss: 0.00091061
Iteration 8/25 | Loss: 0.00091061
Iteration 9/25 | Loss: 0.00091061
Iteration 10/25 | Loss: 0.00091061
Iteration 11/25 | Loss: 0.00091061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009106065263040364, 0.0009106065263040364, 0.0009106065263040364, 0.0009106065263040364, 0.0009106065263040364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009106065263040364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37726438
Iteration 2/25 | Loss: 0.00074274
Iteration 3/25 | Loss: 0.00074274
Iteration 4/25 | Loss: 0.00074274
Iteration 5/25 | Loss: 0.00074274
Iteration 6/25 | Loss: 0.00074274
Iteration 7/25 | Loss: 0.00074274
Iteration 8/25 | Loss: 0.00074274
Iteration 9/25 | Loss: 0.00074274
Iteration 10/25 | Loss: 0.00074274
Iteration 11/25 | Loss: 0.00074274
Iteration 12/25 | Loss: 0.00074274
Iteration 13/25 | Loss: 0.00074274
Iteration 14/25 | Loss: 0.00074274
Iteration 15/25 | Loss: 0.00074274
Iteration 16/25 | Loss: 0.00074274
Iteration 17/25 | Loss: 0.00074274
Iteration 18/25 | Loss: 0.00074274
Iteration 19/25 | Loss: 0.00074274
Iteration 20/25 | Loss: 0.00074274
Iteration 21/25 | Loss: 0.00074273
Iteration 22/25 | Loss: 0.00074273
Iteration 23/25 | Loss: 0.00074273
Iteration 24/25 | Loss: 0.00074273
Iteration 25/25 | Loss: 0.00074273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074273
Iteration 2/1000 | Loss: 0.00001861
Iteration 3/1000 | Loss: 0.00001224
Iteration 4/1000 | Loss: 0.00001039
Iteration 5/1000 | Loss: 0.00000958
Iteration 6/1000 | Loss: 0.00000912
Iteration 7/1000 | Loss: 0.00000909
Iteration 8/1000 | Loss: 0.00000905
Iteration 9/1000 | Loss: 0.00000900
Iteration 10/1000 | Loss: 0.00000898
Iteration 11/1000 | Loss: 0.00000898
Iteration 12/1000 | Loss: 0.00000897
Iteration 13/1000 | Loss: 0.00000897
Iteration 14/1000 | Loss: 0.00000897
Iteration 15/1000 | Loss: 0.00000896
Iteration 16/1000 | Loss: 0.00000890
Iteration 17/1000 | Loss: 0.00000890
Iteration 18/1000 | Loss: 0.00000890
Iteration 19/1000 | Loss: 0.00000889
Iteration 20/1000 | Loss: 0.00000889
Iteration 21/1000 | Loss: 0.00000889
Iteration 22/1000 | Loss: 0.00000889
Iteration 23/1000 | Loss: 0.00000889
Iteration 24/1000 | Loss: 0.00000889
Iteration 25/1000 | Loss: 0.00000889
Iteration 26/1000 | Loss: 0.00000889
Iteration 27/1000 | Loss: 0.00000889
Iteration 28/1000 | Loss: 0.00000889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 28. Stopping optimization.
Last 5 losses: [8.89063721842831e-06, 8.89063721842831e-06, 8.89063721842831e-06, 8.89063721842831e-06, 8.89063721842831e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.89063721842831e-06

Optimization complete. Final v2v error: 2.501311779022217 mm

Highest mean error: 2.8870010375976562 mm for frame 61

Lowest mean error: 2.2796857357025146 mm for frame 2

Saving results

Total time: 18.793325424194336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001542
Iteration 2/25 | Loss: 0.01001542
Iteration 3/25 | Loss: 0.01001542
Iteration 4/25 | Loss: 0.00440297
Iteration 5/25 | Loss: 0.00316896
Iteration 6/25 | Loss: 0.00258786
Iteration 7/25 | Loss: 0.00221788
Iteration 8/25 | Loss: 0.00211925
Iteration 9/25 | Loss: 0.00199909
Iteration 10/25 | Loss: 0.00197981
Iteration 11/25 | Loss: 0.00189510
Iteration 12/25 | Loss: 0.00183651
Iteration 13/25 | Loss: 0.00168206
Iteration 14/25 | Loss: 0.00164179
Iteration 15/25 | Loss: 0.00162017
Iteration 16/25 | Loss: 0.00160595
Iteration 17/25 | Loss: 0.00159894
Iteration 18/25 | Loss: 0.00158855
Iteration 19/25 | Loss: 0.00158190
Iteration 20/25 | Loss: 0.00158037
Iteration 21/25 | Loss: 0.00157815
Iteration 22/25 | Loss: 0.00157042
Iteration 23/25 | Loss: 0.00156498
Iteration 24/25 | Loss: 0.00156015
Iteration 25/25 | Loss: 0.00155807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31575894
Iteration 2/25 | Loss: 0.00564191
Iteration 3/25 | Loss: 0.00522039
Iteration 4/25 | Loss: 0.00522038
Iteration 5/25 | Loss: 0.00522038
Iteration 6/25 | Loss: 0.00522038
Iteration 7/25 | Loss: 0.00522038
Iteration 8/25 | Loss: 0.00522038
Iteration 9/25 | Loss: 0.00522038
Iteration 10/25 | Loss: 0.00522038
Iteration 11/25 | Loss: 0.00522038
Iteration 12/25 | Loss: 0.00522038
Iteration 13/25 | Loss: 0.00522038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.005220382008701563, 0.005220382008701563, 0.005220382008701563, 0.005220382008701563, 0.005220382008701563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005220382008701563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00522038
Iteration 2/1000 | Loss: 0.00171477
Iteration 3/1000 | Loss: 0.00200591
Iteration 4/1000 | Loss: 0.00130600
Iteration 5/1000 | Loss: 0.00053950
Iteration 6/1000 | Loss: 0.00148827
Iteration 7/1000 | Loss: 0.00255586
Iteration 8/1000 | Loss: 0.00076488
Iteration 9/1000 | Loss: 0.00074355
Iteration 10/1000 | Loss: 0.00080799
Iteration 11/1000 | Loss: 0.00041971
Iteration 12/1000 | Loss: 0.00099987
Iteration 13/1000 | Loss: 0.00039739
Iteration 14/1000 | Loss: 0.00038825
Iteration 15/1000 | Loss: 0.00037047
Iteration 16/1000 | Loss: 0.00029035
Iteration 17/1000 | Loss: 0.00038018
Iteration 18/1000 | Loss: 0.00026268
Iteration 19/1000 | Loss: 0.00050079
Iteration 20/1000 | Loss: 0.00059880
Iteration 21/1000 | Loss: 0.00146965
Iteration 22/1000 | Loss: 0.00025422
Iteration 23/1000 | Loss: 0.00025957
Iteration 24/1000 | Loss: 0.00026550
Iteration 25/1000 | Loss: 0.00038150
Iteration 26/1000 | Loss: 0.00030020
Iteration 27/1000 | Loss: 0.00023898
Iteration 28/1000 | Loss: 0.00028422
Iteration 29/1000 | Loss: 0.00023916
Iteration 30/1000 | Loss: 0.00042983
Iteration 31/1000 | Loss: 0.00043600
Iteration 32/1000 | Loss: 0.00043240
Iteration 33/1000 | Loss: 0.00044843
Iteration 34/1000 | Loss: 0.00033971
Iteration 35/1000 | Loss: 0.00023981
Iteration 36/1000 | Loss: 0.00023419
Iteration 37/1000 | Loss: 0.00044632
Iteration 38/1000 | Loss: 0.00071793
Iteration 39/1000 | Loss: 0.00064096
Iteration 40/1000 | Loss: 0.00026137
Iteration 41/1000 | Loss: 0.00024345
Iteration 42/1000 | Loss: 0.00025601
Iteration 43/1000 | Loss: 0.00023792
Iteration 44/1000 | Loss: 0.00024975
Iteration 45/1000 | Loss: 0.00023478
Iteration 46/1000 | Loss: 0.00022757
Iteration 47/1000 | Loss: 0.00022941
Iteration 48/1000 | Loss: 0.00027315
Iteration 49/1000 | Loss: 0.00022552
Iteration 50/1000 | Loss: 0.00023100
Iteration 51/1000 | Loss: 0.00022485
Iteration 52/1000 | Loss: 0.00026959
Iteration 53/1000 | Loss: 0.00022821
Iteration 54/1000 | Loss: 0.00023515
Iteration 55/1000 | Loss: 0.00022402
Iteration 56/1000 | Loss: 0.00041772
Iteration 57/1000 | Loss: 0.00029308
Iteration 58/1000 | Loss: 0.00042839
Iteration 59/1000 | Loss: 0.00090120
Iteration 60/1000 | Loss: 0.00069457
Iteration 61/1000 | Loss: 0.00025269
Iteration 62/1000 | Loss: 0.00023750
Iteration 63/1000 | Loss: 0.00022943
Iteration 64/1000 | Loss: 0.00034252
Iteration 65/1000 | Loss: 0.00028572
Iteration 66/1000 | Loss: 0.00029859
Iteration 67/1000 | Loss: 0.00022415
Iteration 68/1000 | Loss: 0.00021466
Iteration 69/1000 | Loss: 0.00021712
Iteration 70/1000 | Loss: 0.00021276
Iteration 71/1000 | Loss: 0.00026112
Iteration 72/1000 | Loss: 0.00028168
Iteration 73/1000 | Loss: 0.00021409
Iteration 74/1000 | Loss: 0.00022307
Iteration 75/1000 | Loss: 0.00021461
Iteration 76/1000 | Loss: 0.00021221
Iteration 77/1000 | Loss: 0.00021067
Iteration 78/1000 | Loss: 0.00025893
Iteration 79/1000 | Loss: 0.00021067
Iteration 80/1000 | Loss: 0.00022633
Iteration 81/1000 | Loss: 0.00022612
Iteration 82/1000 | Loss: 0.00021016
Iteration 83/1000 | Loss: 0.00021779
Iteration 84/1000 | Loss: 0.00020997
Iteration 85/1000 | Loss: 0.00022563
Iteration 86/1000 | Loss: 0.00020974
Iteration 87/1000 | Loss: 0.00023064
Iteration 88/1000 | Loss: 0.00066135
Iteration 89/1000 | Loss: 0.00035901
Iteration 90/1000 | Loss: 0.00086397
Iteration 91/1000 | Loss: 0.00022632
Iteration 92/1000 | Loss: 0.00021598
Iteration 93/1000 | Loss: 0.00021026
Iteration 94/1000 | Loss: 0.00022378
Iteration 95/1000 | Loss: 0.00026489
Iteration 96/1000 | Loss: 0.00020253
Iteration 97/1000 | Loss: 0.00020169
Iteration 98/1000 | Loss: 0.00020088
Iteration 99/1000 | Loss: 0.00020026
Iteration 100/1000 | Loss: 0.00019972
Iteration 101/1000 | Loss: 0.00019936
Iteration 102/1000 | Loss: 0.00019908
Iteration 103/1000 | Loss: 0.00023604
Iteration 104/1000 | Loss: 0.00019887
Iteration 105/1000 | Loss: 0.00019873
Iteration 106/1000 | Loss: 0.00023518
Iteration 107/1000 | Loss: 0.00019873
Iteration 108/1000 | Loss: 0.00019852
Iteration 109/1000 | Loss: 0.00019852
Iteration 110/1000 | Loss: 0.00019850
Iteration 111/1000 | Loss: 0.00019850
Iteration 112/1000 | Loss: 0.00019850
Iteration 113/1000 | Loss: 0.00019850
Iteration 114/1000 | Loss: 0.00019850
Iteration 115/1000 | Loss: 0.00019850
Iteration 116/1000 | Loss: 0.00019850
Iteration 117/1000 | Loss: 0.00019850
Iteration 118/1000 | Loss: 0.00019850
Iteration 119/1000 | Loss: 0.00019849
Iteration 120/1000 | Loss: 0.00039690
Iteration 121/1000 | Loss: 0.00026809
Iteration 122/1000 | Loss: 0.00020113
Iteration 123/1000 | Loss: 0.00019941
Iteration 124/1000 | Loss: 0.00023965
Iteration 125/1000 | Loss: 0.00019874
Iteration 126/1000 | Loss: 0.00022118
Iteration 127/1000 | Loss: 0.00019703
Iteration 128/1000 | Loss: 0.00019672
Iteration 129/1000 | Loss: 0.00020497
Iteration 130/1000 | Loss: 0.00019660
Iteration 131/1000 | Loss: 0.00019634
Iteration 132/1000 | Loss: 0.00037966
Iteration 133/1000 | Loss: 0.00020550
Iteration 134/1000 | Loss: 0.00019898
Iteration 135/1000 | Loss: 0.00019739
Iteration 136/1000 | Loss: 0.00019601
Iteration 137/1000 | Loss: 0.00019519
Iteration 138/1000 | Loss: 0.00019445
Iteration 139/1000 | Loss: 0.00034408
Iteration 140/1000 | Loss: 0.00020020
Iteration 141/1000 | Loss: 0.00019627
Iteration 142/1000 | Loss: 0.00019475
Iteration 143/1000 | Loss: 0.00019335
Iteration 144/1000 | Loss: 0.00019255
Iteration 145/1000 | Loss: 0.00019202
Iteration 146/1000 | Loss: 0.00022843
Iteration 147/1000 | Loss: 0.00060216
Iteration 148/1000 | Loss: 0.00021058
Iteration 149/1000 | Loss: 0.00022172
Iteration 150/1000 | Loss: 0.00019228
Iteration 151/1000 | Loss: 0.00021778
Iteration 152/1000 | Loss: 0.00020239
Iteration 153/1000 | Loss: 0.00019017
Iteration 154/1000 | Loss: 0.00021583
Iteration 155/1000 | Loss: 0.00018933
Iteration 156/1000 | Loss: 0.00018912
Iteration 157/1000 | Loss: 0.00018908
Iteration 158/1000 | Loss: 0.00033662
Iteration 159/1000 | Loss: 0.00019770
Iteration 160/1000 | Loss: 0.00038622
Iteration 161/1000 | Loss: 0.00059063
Iteration 162/1000 | Loss: 0.00052701
Iteration 163/1000 | Loss: 0.00033860
Iteration 164/1000 | Loss: 0.00019288
Iteration 165/1000 | Loss: 0.00019036
Iteration 166/1000 | Loss: 0.00022298
Iteration 167/1000 | Loss: 0.00018765
Iteration 168/1000 | Loss: 0.00018677
Iteration 169/1000 | Loss: 0.00021759
Iteration 170/1000 | Loss: 0.00018848
Iteration 171/1000 | Loss: 0.00019485
Iteration 172/1000 | Loss: 0.00018580
Iteration 173/1000 | Loss: 0.00020653
Iteration 174/1000 | Loss: 0.00018549
Iteration 175/1000 | Loss: 0.00018543
Iteration 176/1000 | Loss: 0.00018542
Iteration 177/1000 | Loss: 0.00018528
Iteration 178/1000 | Loss: 0.00018518
Iteration 179/1000 | Loss: 0.00018504
Iteration 180/1000 | Loss: 0.00018502
Iteration 181/1000 | Loss: 0.00018486
Iteration 182/1000 | Loss: 0.00029310
Iteration 183/1000 | Loss: 0.00039189
Iteration 184/1000 | Loss: 0.00115960
Iteration 185/1000 | Loss: 0.00018855
Iteration 186/1000 | Loss: 0.00019637
Iteration 187/1000 | Loss: 0.00018478
Iteration 188/1000 | Loss: 0.00020590
Iteration 189/1000 | Loss: 0.00018484
Iteration 190/1000 | Loss: 0.00018239
Iteration 191/1000 | Loss: 0.00018164
Iteration 192/1000 | Loss: 0.00018130
Iteration 193/1000 | Loss: 0.00018094
Iteration 194/1000 | Loss: 0.00018067
Iteration 195/1000 | Loss: 0.00018047
Iteration 196/1000 | Loss: 0.00018026
Iteration 197/1000 | Loss: 0.00022307
Iteration 198/1000 | Loss: 0.00018551
Iteration 199/1000 | Loss: 0.00018849
Iteration 200/1000 | Loss: 0.00017990
Iteration 201/1000 | Loss: 0.00020178
Iteration 202/1000 | Loss: 0.00018999
Iteration 203/1000 | Loss: 0.00037132
Iteration 204/1000 | Loss: 0.00174940
Iteration 205/1000 | Loss: 0.00107860
Iteration 206/1000 | Loss: 0.00023919
Iteration 207/1000 | Loss: 0.00020288
Iteration 208/1000 | Loss: 0.00021967
Iteration 209/1000 | Loss: 0.00037655
Iteration 210/1000 | Loss: 0.00017771
Iteration 211/1000 | Loss: 0.00017130
Iteration 212/1000 | Loss: 0.00016761
Iteration 213/1000 | Loss: 0.00016556
Iteration 214/1000 | Loss: 0.00016443
Iteration 215/1000 | Loss: 0.00016337
Iteration 216/1000 | Loss: 0.00019716
Iteration 217/1000 | Loss: 0.00016206
Iteration 218/1000 | Loss: 0.00016152
Iteration 219/1000 | Loss: 0.00019153
Iteration 220/1000 | Loss: 0.00016091
Iteration 221/1000 | Loss: 0.00019282
Iteration 222/1000 | Loss: 0.00016066
Iteration 223/1000 | Loss: 0.00016039
Iteration 224/1000 | Loss: 0.00016029
Iteration 225/1000 | Loss: 0.00016021
Iteration 226/1000 | Loss: 0.00016005
Iteration 227/1000 | Loss: 0.00016005
Iteration 228/1000 | Loss: 0.00016003
Iteration 229/1000 | Loss: 0.00016003
Iteration 230/1000 | Loss: 0.00015998
Iteration 231/1000 | Loss: 0.00015991
Iteration 232/1000 | Loss: 0.00015991
Iteration 233/1000 | Loss: 0.00015991
Iteration 234/1000 | Loss: 0.00015991
Iteration 235/1000 | Loss: 0.00015990
Iteration 236/1000 | Loss: 0.00015990
Iteration 237/1000 | Loss: 0.00015987
Iteration 238/1000 | Loss: 0.00015983
Iteration 239/1000 | Loss: 0.00025622
Iteration 240/1000 | Loss: 0.00026094
Iteration 241/1000 | Loss: 0.00019544
Iteration 242/1000 | Loss: 0.00016261
Iteration 243/1000 | Loss: 0.00019175
Iteration 244/1000 | Loss: 0.00015938
Iteration 245/1000 | Loss: 0.00015867
Iteration 246/1000 | Loss: 0.00022840
Iteration 247/1000 | Loss: 0.00016037
Iteration 248/1000 | Loss: 0.00015805
Iteration 249/1000 | Loss: 0.00015790
Iteration 250/1000 | Loss: 0.00017621
Iteration 251/1000 | Loss: 0.00016136
Iteration 252/1000 | Loss: 0.00015781
Iteration 253/1000 | Loss: 0.00028869
Iteration 254/1000 | Loss: 0.00016359
Iteration 255/1000 | Loss: 0.00016016
Iteration 256/1000 | Loss: 0.00015887
Iteration 257/1000 | Loss: 0.00015764
Iteration 258/1000 | Loss: 0.00020689
Iteration 259/1000 | Loss: 0.00015832
Iteration 260/1000 | Loss: 0.00015737
Iteration 261/1000 | Loss: 0.00015647
Iteration 262/1000 | Loss: 0.00016172
Iteration 263/1000 | Loss: 0.00015690
Iteration 264/1000 | Loss: 0.00015624
Iteration 265/1000 | Loss: 0.00015607
Iteration 266/1000 | Loss: 0.00015599
Iteration 267/1000 | Loss: 0.00015588
Iteration 268/1000 | Loss: 0.00015585
Iteration 269/1000 | Loss: 0.00015585
Iteration 270/1000 | Loss: 0.00015585
Iteration 271/1000 | Loss: 0.00015584
Iteration 272/1000 | Loss: 0.00015581
Iteration 273/1000 | Loss: 0.00015574
Iteration 274/1000 | Loss: 0.00015566
Iteration 275/1000 | Loss: 0.00018232
Iteration 276/1000 | Loss: 0.00015568
Iteration 277/1000 | Loss: 0.00015543
Iteration 278/1000 | Loss: 0.00015540
Iteration 279/1000 | Loss: 0.00015540
Iteration 280/1000 | Loss: 0.00017046
Iteration 281/1000 | Loss: 0.00015530
Iteration 282/1000 | Loss: 0.00015521
Iteration 283/1000 | Loss: 0.00015517
Iteration 284/1000 | Loss: 0.00015516
Iteration 285/1000 | Loss: 0.00015515
Iteration 286/1000 | Loss: 0.00015514
Iteration 287/1000 | Loss: 0.00015514
Iteration 288/1000 | Loss: 0.00015513
Iteration 289/1000 | Loss: 0.00015513
Iteration 290/1000 | Loss: 0.00015513
Iteration 291/1000 | Loss: 0.00015512
Iteration 292/1000 | Loss: 0.00015512
Iteration 293/1000 | Loss: 0.00018280
Iteration 294/1000 | Loss: 0.00015513
Iteration 295/1000 | Loss: 0.00041763
Iteration 296/1000 | Loss: 0.00031465
Iteration 297/1000 | Loss: 0.00053532
Iteration 298/1000 | Loss: 0.00023415
Iteration 299/1000 | Loss: 0.00026974
Iteration 300/1000 | Loss: 0.00016836
Iteration 301/1000 | Loss: 0.00017753
Iteration 302/1000 | Loss: 0.00020666
Iteration 303/1000 | Loss: 0.00015381
Iteration 304/1000 | Loss: 0.00029607
Iteration 305/1000 | Loss: 0.00026775
Iteration 306/1000 | Loss: 0.00018690
Iteration 307/1000 | Loss: 0.00015060
Iteration 308/1000 | Loss: 0.00014890
Iteration 309/1000 | Loss: 0.00024118
Iteration 310/1000 | Loss: 0.00014747
Iteration 311/1000 | Loss: 0.00017185
Iteration 312/1000 | Loss: 0.00014630
Iteration 313/1000 | Loss: 0.00017992
Iteration 314/1000 | Loss: 0.00014606
Iteration 315/1000 | Loss: 0.00014566
Iteration 316/1000 | Loss: 0.00014547
Iteration 317/1000 | Loss: 0.00014526
Iteration 318/1000 | Loss: 0.00014525
Iteration 319/1000 | Loss: 0.00014524
Iteration 320/1000 | Loss: 0.00014524
Iteration 321/1000 | Loss: 0.00014524
Iteration 322/1000 | Loss: 0.00014523
Iteration 323/1000 | Loss: 0.00014522
Iteration 324/1000 | Loss: 0.00014521
Iteration 325/1000 | Loss: 0.00014521
Iteration 326/1000 | Loss: 0.00014520
Iteration 327/1000 | Loss: 0.00014518
Iteration 328/1000 | Loss: 0.00014518
Iteration 329/1000 | Loss: 0.00014517
Iteration 330/1000 | Loss: 0.00014517
Iteration 331/1000 | Loss: 0.00014517
Iteration 332/1000 | Loss: 0.00014516
Iteration 333/1000 | Loss: 0.00014515
Iteration 334/1000 | Loss: 0.00014515
Iteration 335/1000 | Loss: 0.00014514
Iteration 336/1000 | Loss: 0.00014514
Iteration 337/1000 | Loss: 0.00014514
Iteration 338/1000 | Loss: 0.00014511
Iteration 339/1000 | Loss: 0.00014511
Iteration 340/1000 | Loss: 0.00014510
Iteration 341/1000 | Loss: 0.00014509
Iteration 342/1000 | Loss: 0.00014509
Iteration 343/1000 | Loss: 0.00014508
Iteration 344/1000 | Loss: 0.00014508
Iteration 345/1000 | Loss: 0.00014508
Iteration 346/1000 | Loss: 0.00014508
Iteration 347/1000 | Loss: 0.00014507
Iteration 348/1000 | Loss: 0.00014507
Iteration 349/1000 | Loss: 0.00014507
Iteration 350/1000 | Loss: 0.00014507
Iteration 351/1000 | Loss: 0.00014507
Iteration 352/1000 | Loss: 0.00014507
Iteration 353/1000 | Loss: 0.00014507
Iteration 354/1000 | Loss: 0.00014507
Iteration 355/1000 | Loss: 0.00014507
Iteration 356/1000 | Loss: 0.00014507
Iteration 357/1000 | Loss: 0.00014507
Iteration 358/1000 | Loss: 0.00014507
Iteration 359/1000 | Loss: 0.00014507
Iteration 360/1000 | Loss: 0.00014506
Iteration 361/1000 | Loss: 0.00014506
Iteration 362/1000 | Loss: 0.00014506
Iteration 363/1000 | Loss: 0.00014506
Iteration 364/1000 | Loss: 0.00014506
Iteration 365/1000 | Loss: 0.00014506
Iteration 366/1000 | Loss: 0.00014506
Iteration 367/1000 | Loss: 0.00014506
Iteration 368/1000 | Loss: 0.00014506
Iteration 369/1000 | Loss: 0.00014506
Iteration 370/1000 | Loss: 0.00014505
Iteration 371/1000 | Loss: 0.00014505
Iteration 372/1000 | Loss: 0.00014505
Iteration 373/1000 | Loss: 0.00014505
Iteration 374/1000 | Loss: 0.00014505
Iteration 375/1000 | Loss: 0.00014505
Iteration 376/1000 | Loss: 0.00014505
Iteration 377/1000 | Loss: 0.00014505
Iteration 378/1000 | Loss: 0.00014505
Iteration 379/1000 | Loss: 0.00014505
Iteration 380/1000 | Loss: 0.00014505
Iteration 381/1000 | Loss: 0.00014505
Iteration 382/1000 | Loss: 0.00014505
Iteration 383/1000 | Loss: 0.00014505
Iteration 384/1000 | Loss: 0.00014505
Iteration 385/1000 | Loss: 0.00014504
Iteration 386/1000 | Loss: 0.00014504
Iteration 387/1000 | Loss: 0.00014504
Iteration 388/1000 | Loss: 0.00014504
Iteration 389/1000 | Loss: 0.00014504
Iteration 390/1000 | Loss: 0.00014504
Iteration 391/1000 | Loss: 0.00014504
Iteration 392/1000 | Loss: 0.00014504
Iteration 393/1000 | Loss: 0.00014504
Iteration 394/1000 | Loss: 0.00014504
Iteration 395/1000 | Loss: 0.00014504
Iteration 396/1000 | Loss: 0.00014504
Iteration 397/1000 | Loss: 0.00014504
Iteration 398/1000 | Loss: 0.00014504
Iteration 399/1000 | Loss: 0.00014503
Iteration 400/1000 | Loss: 0.00014503
Iteration 401/1000 | Loss: 0.00014503
Iteration 402/1000 | Loss: 0.00014503
Iteration 403/1000 | Loss: 0.00014503
Iteration 404/1000 | Loss: 0.00014503
Iteration 405/1000 | Loss: 0.00014503
Iteration 406/1000 | Loss: 0.00014502
Iteration 407/1000 | Loss: 0.00014502
Iteration 408/1000 | Loss: 0.00014502
Iteration 409/1000 | Loss: 0.00014502
Iteration 410/1000 | Loss: 0.00014502
Iteration 411/1000 | Loss: 0.00014502
Iteration 412/1000 | Loss: 0.00014502
Iteration 413/1000 | Loss: 0.00014502
Iteration 414/1000 | Loss: 0.00014502
Iteration 415/1000 | Loss: 0.00014502
Iteration 416/1000 | Loss: 0.00014502
Iteration 417/1000 | Loss: 0.00014502
Iteration 418/1000 | Loss: 0.00014502
Iteration 419/1000 | Loss: 0.00014502
Iteration 420/1000 | Loss: 0.00014502
Iteration 421/1000 | Loss: 0.00014502
Iteration 422/1000 | Loss: 0.00014502
Iteration 423/1000 | Loss: 0.00014501
Iteration 424/1000 | Loss: 0.00014501
Iteration 425/1000 | Loss: 0.00014501
Iteration 426/1000 | Loss: 0.00014501
Iteration 427/1000 | Loss: 0.00014501
Iteration 428/1000 | Loss: 0.00014501
Iteration 429/1000 | Loss: 0.00014501
Iteration 430/1000 | Loss: 0.00014501
Iteration 431/1000 | Loss: 0.00014501
Iteration 432/1000 | Loss: 0.00014501
Iteration 433/1000 | Loss: 0.00014501
Iteration 434/1000 | Loss: 0.00014501
Iteration 435/1000 | Loss: 0.00014501
Iteration 436/1000 | Loss: 0.00014501
Iteration 437/1000 | Loss: 0.00014501
Iteration 438/1000 | Loss: 0.00014501
Iteration 439/1000 | Loss: 0.00014501
Iteration 440/1000 | Loss: 0.00014501
Iteration 441/1000 | Loss: 0.00014501
Iteration 442/1000 | Loss: 0.00014501
Iteration 443/1000 | Loss: 0.00014501
Iteration 444/1000 | Loss: 0.00014501
Iteration 445/1000 | Loss: 0.00014501
Iteration 446/1000 | Loss: 0.00014501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 446. Stopping optimization.
Last 5 losses: [0.00014500551333185285, 0.00014500551333185285, 0.00014500551333185285, 0.00014500551333185285, 0.00014500551333185285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014500551333185285

Optimization complete. Final v2v error: 6.045367240905762 mm

Highest mean error: 19.349214553833008 mm for frame 192

Lowest mean error: 3.1247811317443848 mm for frame 13

Saving results

Total time: 494.61884474754333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885201
Iteration 2/25 | Loss: 0.00153448
Iteration 3/25 | Loss: 0.00113484
Iteration 4/25 | Loss: 0.00109936
Iteration 5/25 | Loss: 0.00108452
Iteration 6/25 | Loss: 0.00108421
Iteration 7/25 | Loss: 0.00109541
Iteration 8/25 | Loss: 0.00108309
Iteration 9/25 | Loss: 0.00107445
Iteration 10/25 | Loss: 0.00107104
Iteration 11/25 | Loss: 0.00106742
Iteration 12/25 | Loss: 0.00106771
Iteration 13/25 | Loss: 0.00106333
Iteration 14/25 | Loss: 0.00106314
Iteration 15/25 | Loss: 0.00106044
Iteration 16/25 | Loss: 0.00105936
Iteration 17/25 | Loss: 0.00106046
Iteration 18/25 | Loss: 0.00105658
Iteration 19/25 | Loss: 0.00105493
Iteration 20/25 | Loss: 0.00105474
Iteration 21/25 | Loss: 0.00105471
Iteration 22/25 | Loss: 0.00105471
Iteration 23/25 | Loss: 0.00105471
Iteration 24/25 | Loss: 0.00105471
Iteration 25/25 | Loss: 0.00105471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61752355
Iteration 2/25 | Loss: 0.00095816
Iteration 3/25 | Loss: 0.00076564
Iteration 4/25 | Loss: 0.00076563
Iteration 5/25 | Loss: 0.00076563
Iteration 6/25 | Loss: 0.00076563
Iteration 7/25 | Loss: 0.00076563
Iteration 8/25 | Loss: 0.00076563
Iteration 9/25 | Loss: 0.00076563
Iteration 10/25 | Loss: 0.00076563
Iteration 11/25 | Loss: 0.00076563
Iteration 12/25 | Loss: 0.00076563
Iteration 13/25 | Loss: 0.00076563
Iteration 14/25 | Loss: 0.00076563
Iteration 15/25 | Loss: 0.00076563
Iteration 16/25 | Loss: 0.00076563
Iteration 17/25 | Loss: 0.00076563
Iteration 18/25 | Loss: 0.00076563
Iteration 19/25 | Loss: 0.00076563
Iteration 20/25 | Loss: 0.00076563
Iteration 21/25 | Loss: 0.00076563
Iteration 22/25 | Loss: 0.00076563
Iteration 23/25 | Loss: 0.00076563
Iteration 24/25 | Loss: 0.00076563
Iteration 25/25 | Loss: 0.00076563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076563
Iteration 2/1000 | Loss: 0.00024209
Iteration 3/1000 | Loss: 0.00004305
Iteration 4/1000 | Loss: 0.00024619
Iteration 5/1000 | Loss: 0.00003428
Iteration 6/1000 | Loss: 0.00003169
Iteration 7/1000 | Loss: 0.00002971
Iteration 8/1000 | Loss: 0.00015483
Iteration 9/1000 | Loss: 0.00032788
Iteration 10/1000 | Loss: 0.00003754
Iteration 11/1000 | Loss: 0.00002888
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00022168
Iteration 14/1000 | Loss: 0.00002390
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002163
Iteration 17/1000 | Loss: 0.00002120
Iteration 18/1000 | Loss: 0.00002089
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002068
Iteration 21/1000 | Loss: 0.00002049
Iteration 22/1000 | Loss: 0.00002041
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002022
Iteration 25/1000 | Loss: 0.00002008
Iteration 26/1000 | Loss: 0.00002002
Iteration 27/1000 | Loss: 0.00001997
Iteration 28/1000 | Loss: 0.00001997
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001996
Iteration 31/1000 | Loss: 0.00001995
Iteration 32/1000 | Loss: 0.00001993
Iteration 33/1000 | Loss: 0.00001993
Iteration 34/1000 | Loss: 0.00001992
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001992
Iteration 37/1000 | Loss: 0.00001991
Iteration 38/1000 | Loss: 0.00001990
Iteration 39/1000 | Loss: 0.00001990
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001990
Iteration 43/1000 | Loss: 0.00001990
Iteration 44/1000 | Loss: 0.00001990
Iteration 45/1000 | Loss: 0.00001990
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001989
Iteration 48/1000 | Loss: 0.00001989
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001987
Iteration 55/1000 | Loss: 0.00001987
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001986
Iteration 60/1000 | Loss: 0.00001986
Iteration 61/1000 | Loss: 0.00001986
Iteration 62/1000 | Loss: 0.00001986
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001985
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001984
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00001984
Iteration 73/1000 | Loss: 0.00001984
Iteration 74/1000 | Loss: 0.00001984
Iteration 75/1000 | Loss: 0.00001984
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001983
Iteration 79/1000 | Loss: 0.00001982
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001982
Iteration 82/1000 | Loss: 0.00001982
Iteration 83/1000 | Loss: 0.00001982
Iteration 84/1000 | Loss: 0.00001981
Iteration 85/1000 | Loss: 0.00001981
Iteration 86/1000 | Loss: 0.00001981
Iteration 87/1000 | Loss: 0.00001981
Iteration 88/1000 | Loss: 0.00001981
Iteration 89/1000 | Loss: 0.00001980
Iteration 90/1000 | Loss: 0.00001980
Iteration 91/1000 | Loss: 0.00001980
Iteration 92/1000 | Loss: 0.00001980
Iteration 93/1000 | Loss: 0.00001980
Iteration 94/1000 | Loss: 0.00001980
Iteration 95/1000 | Loss: 0.00001980
Iteration 96/1000 | Loss: 0.00001979
Iteration 97/1000 | Loss: 0.00001979
Iteration 98/1000 | Loss: 0.00001979
Iteration 99/1000 | Loss: 0.00001979
Iteration 100/1000 | Loss: 0.00001978
Iteration 101/1000 | Loss: 0.00001978
Iteration 102/1000 | Loss: 0.00001978
Iteration 103/1000 | Loss: 0.00001978
Iteration 104/1000 | Loss: 0.00001978
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001977
Iteration 108/1000 | Loss: 0.00001977
Iteration 109/1000 | Loss: 0.00001977
Iteration 110/1000 | Loss: 0.00001977
Iteration 111/1000 | Loss: 0.00001977
Iteration 112/1000 | Loss: 0.00001977
Iteration 113/1000 | Loss: 0.00001977
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001976
Iteration 118/1000 | Loss: 0.00001976
Iteration 119/1000 | Loss: 0.00001976
Iteration 120/1000 | Loss: 0.00001976
Iteration 121/1000 | Loss: 0.00001976
Iteration 122/1000 | Loss: 0.00001976
Iteration 123/1000 | Loss: 0.00001976
Iteration 124/1000 | Loss: 0.00001976
Iteration 125/1000 | Loss: 0.00001976
Iteration 126/1000 | Loss: 0.00001975
Iteration 127/1000 | Loss: 0.00001975
Iteration 128/1000 | Loss: 0.00001975
Iteration 129/1000 | Loss: 0.00001975
Iteration 130/1000 | Loss: 0.00001975
Iteration 131/1000 | Loss: 0.00001975
Iteration 132/1000 | Loss: 0.00001975
Iteration 133/1000 | Loss: 0.00001975
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001975
Iteration 136/1000 | Loss: 0.00001974
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001974
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Iteration 141/1000 | Loss: 0.00001974
Iteration 142/1000 | Loss: 0.00001974
Iteration 143/1000 | Loss: 0.00001974
Iteration 144/1000 | Loss: 0.00001973
Iteration 145/1000 | Loss: 0.00001973
Iteration 146/1000 | Loss: 0.00001973
Iteration 147/1000 | Loss: 0.00001973
Iteration 148/1000 | Loss: 0.00001973
Iteration 149/1000 | Loss: 0.00001973
Iteration 150/1000 | Loss: 0.00001973
Iteration 151/1000 | Loss: 0.00001973
Iteration 152/1000 | Loss: 0.00001973
Iteration 153/1000 | Loss: 0.00001973
Iteration 154/1000 | Loss: 0.00001972
Iteration 155/1000 | Loss: 0.00001972
Iteration 156/1000 | Loss: 0.00001972
Iteration 157/1000 | Loss: 0.00001972
Iteration 158/1000 | Loss: 0.00001972
Iteration 159/1000 | Loss: 0.00001972
Iteration 160/1000 | Loss: 0.00001972
Iteration 161/1000 | Loss: 0.00001972
Iteration 162/1000 | Loss: 0.00001972
Iteration 163/1000 | Loss: 0.00001972
Iteration 164/1000 | Loss: 0.00001972
Iteration 165/1000 | Loss: 0.00001972
Iteration 166/1000 | Loss: 0.00001972
Iteration 167/1000 | Loss: 0.00001972
Iteration 168/1000 | Loss: 0.00001972
Iteration 169/1000 | Loss: 0.00001972
Iteration 170/1000 | Loss: 0.00001972
Iteration 171/1000 | Loss: 0.00001972
Iteration 172/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.971792516997084e-05, 1.971792516997084e-05, 1.971792516997084e-05, 1.971792516997084e-05, 1.971792516997084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.971792516997084e-05

Optimization complete. Final v2v error: 3.5545222759246826 mm

Highest mean error: 4.453879356384277 mm for frame 220

Lowest mean error: 2.8969004154205322 mm for frame 182

Saving results

Total time: 90.314044713974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404250
Iteration 2/25 | Loss: 0.00112921
Iteration 3/25 | Loss: 0.00102545
Iteration 4/25 | Loss: 0.00101830
Iteration 5/25 | Loss: 0.00101671
Iteration 6/25 | Loss: 0.00101612
Iteration 7/25 | Loss: 0.00101612
Iteration 8/25 | Loss: 0.00101612
Iteration 9/25 | Loss: 0.00101612
Iteration 10/25 | Loss: 0.00101612
Iteration 11/25 | Loss: 0.00101612
Iteration 12/25 | Loss: 0.00101612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010161158861592412, 0.0010161158861592412, 0.0010161158861592412, 0.0010161158861592412, 0.0010161158861592412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010161158861592412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37527454
Iteration 2/25 | Loss: 0.00081194
Iteration 3/25 | Loss: 0.00081193
Iteration 4/25 | Loss: 0.00081193
Iteration 5/25 | Loss: 0.00081193
Iteration 6/25 | Loss: 0.00081193
Iteration 7/25 | Loss: 0.00081193
Iteration 8/25 | Loss: 0.00081193
Iteration 9/25 | Loss: 0.00081193
Iteration 10/25 | Loss: 0.00081193
Iteration 11/25 | Loss: 0.00081193
Iteration 12/25 | Loss: 0.00081193
Iteration 13/25 | Loss: 0.00081193
Iteration 14/25 | Loss: 0.00081193
Iteration 15/25 | Loss: 0.00081193
Iteration 16/25 | Loss: 0.00081193
Iteration 17/25 | Loss: 0.00081193
Iteration 18/25 | Loss: 0.00081193
Iteration 19/25 | Loss: 0.00081193
Iteration 20/25 | Loss: 0.00081193
Iteration 21/25 | Loss: 0.00081193
Iteration 22/25 | Loss: 0.00081193
Iteration 23/25 | Loss: 0.00081193
Iteration 24/25 | Loss: 0.00081193
Iteration 25/25 | Loss: 0.00081193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081193
Iteration 2/1000 | Loss: 0.00003821
Iteration 3/1000 | Loss: 0.00002146
Iteration 4/1000 | Loss: 0.00001820
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001566
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001449
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001436
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001432
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001424
Iteration 24/1000 | Loss: 0.00001423
Iteration 25/1000 | Loss: 0.00001423
Iteration 26/1000 | Loss: 0.00001423
Iteration 27/1000 | Loss: 0.00001422
Iteration 28/1000 | Loss: 0.00001422
Iteration 29/1000 | Loss: 0.00001422
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001411
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001409
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001407
Iteration 39/1000 | Loss: 0.00001407
Iteration 40/1000 | Loss: 0.00001406
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001405
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001402
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001400
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001400
Iteration 64/1000 | Loss: 0.00001400
Iteration 65/1000 | Loss: 0.00001400
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001399
Iteration 73/1000 | Loss: 0.00001399
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001398
Iteration 78/1000 | Loss: 0.00001398
Iteration 79/1000 | Loss: 0.00001398
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001392
Iteration 95/1000 | Loss: 0.00001392
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001391
Iteration 103/1000 | Loss: 0.00001391
Iteration 104/1000 | Loss: 0.00001390
Iteration 105/1000 | Loss: 0.00001390
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001389
Iteration 108/1000 | Loss: 0.00001389
Iteration 109/1000 | Loss: 0.00001389
Iteration 110/1000 | Loss: 0.00001388
Iteration 111/1000 | Loss: 0.00001388
Iteration 112/1000 | Loss: 0.00001388
Iteration 113/1000 | Loss: 0.00001388
Iteration 114/1000 | Loss: 0.00001388
Iteration 115/1000 | Loss: 0.00001388
Iteration 116/1000 | Loss: 0.00001386
Iteration 117/1000 | Loss: 0.00001386
Iteration 118/1000 | Loss: 0.00001386
Iteration 119/1000 | Loss: 0.00001386
Iteration 120/1000 | Loss: 0.00001386
Iteration 121/1000 | Loss: 0.00001386
Iteration 122/1000 | Loss: 0.00001386
Iteration 123/1000 | Loss: 0.00001386
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001385
Iteration 126/1000 | Loss: 0.00001385
Iteration 127/1000 | Loss: 0.00001385
Iteration 128/1000 | Loss: 0.00001385
Iteration 129/1000 | Loss: 0.00001385
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001385
Iteration 147/1000 | Loss: 0.00001385
Iteration 148/1000 | Loss: 0.00001385
Iteration 149/1000 | Loss: 0.00001385
Iteration 150/1000 | Loss: 0.00001385
Iteration 151/1000 | Loss: 0.00001385
Iteration 152/1000 | Loss: 0.00001385
Iteration 153/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.3854535609425511e-05, 1.3854535609425511e-05, 1.3854535609425511e-05, 1.3854535609425511e-05, 1.3854535609425511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3854535609425511e-05

Optimization complete. Final v2v error: 3.1281416416168213 mm

Highest mean error: 4.227106094360352 mm for frame 61

Lowest mean error: 2.5103304386138916 mm for frame 135

Saving results

Total time: 33.737658739089966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924732
Iteration 2/25 | Loss: 0.00123136
Iteration 3/25 | Loss: 0.00101759
Iteration 4/25 | Loss: 0.00100147
Iteration 5/25 | Loss: 0.00099800
Iteration 6/25 | Loss: 0.00099676
Iteration 7/25 | Loss: 0.00099671
Iteration 8/25 | Loss: 0.00099671
Iteration 9/25 | Loss: 0.00099671
Iteration 10/25 | Loss: 0.00099671
Iteration 11/25 | Loss: 0.00099671
Iteration 12/25 | Loss: 0.00099671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009967086371034384, 0.0009967086371034384, 0.0009967086371034384, 0.0009967086371034384, 0.0009967086371034384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009967086371034384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44550323
Iteration 2/25 | Loss: 0.00066723
Iteration 3/25 | Loss: 0.00066723
Iteration 4/25 | Loss: 0.00066723
Iteration 5/25 | Loss: 0.00066723
Iteration 6/25 | Loss: 0.00066723
Iteration 7/25 | Loss: 0.00066723
Iteration 8/25 | Loss: 0.00066723
Iteration 9/25 | Loss: 0.00066723
Iteration 10/25 | Loss: 0.00066723
Iteration 11/25 | Loss: 0.00066723
Iteration 12/25 | Loss: 0.00066723
Iteration 13/25 | Loss: 0.00066723
Iteration 14/25 | Loss: 0.00066723
Iteration 15/25 | Loss: 0.00066723
Iteration 16/25 | Loss: 0.00066723
Iteration 17/25 | Loss: 0.00066723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006672271993011236, 0.0006672271993011236, 0.0006672271993011236, 0.0006672271993011236, 0.0006672271993011236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006672271993011236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066723
Iteration 2/1000 | Loss: 0.00003453
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002290
Iteration 5/1000 | Loss: 0.00002161
Iteration 6/1000 | Loss: 0.00002056
Iteration 7/1000 | Loss: 0.00001994
Iteration 8/1000 | Loss: 0.00001943
Iteration 9/1000 | Loss: 0.00001916
Iteration 10/1000 | Loss: 0.00001899
Iteration 11/1000 | Loss: 0.00001891
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001866
Iteration 14/1000 | Loss: 0.00001862
Iteration 15/1000 | Loss: 0.00001857
Iteration 16/1000 | Loss: 0.00001854
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001853
Iteration 19/1000 | Loss: 0.00001852
Iteration 20/1000 | Loss: 0.00001852
Iteration 21/1000 | Loss: 0.00001852
Iteration 22/1000 | Loss: 0.00001851
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001850
Iteration 26/1000 | Loss: 0.00001850
Iteration 27/1000 | Loss: 0.00001848
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001845
Iteration 40/1000 | Loss: 0.00001845
Iteration 41/1000 | Loss: 0.00001844
Iteration 42/1000 | Loss: 0.00001844
Iteration 43/1000 | Loss: 0.00001844
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001843
Iteration 46/1000 | Loss: 0.00001843
Iteration 47/1000 | Loss: 0.00001843
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001843
Iteration 51/1000 | Loss: 0.00001843
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001841
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001840
Iteration 60/1000 | Loss: 0.00001840
Iteration 61/1000 | Loss: 0.00001839
Iteration 62/1000 | Loss: 0.00001839
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001834
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001832
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001830
Iteration 82/1000 | Loss: 0.00001830
Iteration 83/1000 | Loss: 0.00001830
Iteration 84/1000 | Loss: 0.00001830
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001829
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001827
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001827
Iteration 108/1000 | Loss: 0.00001827
Iteration 109/1000 | Loss: 0.00001827
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001826
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001826
Iteration 119/1000 | Loss: 0.00001825
Iteration 120/1000 | Loss: 0.00001825
Iteration 121/1000 | Loss: 0.00001825
Iteration 122/1000 | Loss: 0.00001825
Iteration 123/1000 | Loss: 0.00001825
Iteration 124/1000 | Loss: 0.00001824
Iteration 125/1000 | Loss: 0.00001824
Iteration 126/1000 | Loss: 0.00001824
Iteration 127/1000 | Loss: 0.00001824
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001823
Iteration 130/1000 | Loss: 0.00001823
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001823
Iteration 133/1000 | Loss: 0.00001822
Iteration 134/1000 | Loss: 0.00001822
Iteration 135/1000 | Loss: 0.00001822
Iteration 136/1000 | Loss: 0.00001822
Iteration 137/1000 | Loss: 0.00001822
Iteration 138/1000 | Loss: 0.00001822
Iteration 139/1000 | Loss: 0.00001821
Iteration 140/1000 | Loss: 0.00001821
Iteration 141/1000 | Loss: 0.00001820
Iteration 142/1000 | Loss: 0.00001820
Iteration 143/1000 | Loss: 0.00001819
Iteration 144/1000 | Loss: 0.00001819
Iteration 145/1000 | Loss: 0.00001819
Iteration 146/1000 | Loss: 0.00001818
Iteration 147/1000 | Loss: 0.00001818
Iteration 148/1000 | Loss: 0.00001818
Iteration 149/1000 | Loss: 0.00001818
Iteration 150/1000 | Loss: 0.00001818
Iteration 151/1000 | Loss: 0.00001818
Iteration 152/1000 | Loss: 0.00001817
Iteration 153/1000 | Loss: 0.00001817
Iteration 154/1000 | Loss: 0.00001817
Iteration 155/1000 | Loss: 0.00001817
Iteration 156/1000 | Loss: 0.00001817
Iteration 157/1000 | Loss: 0.00001817
Iteration 158/1000 | Loss: 0.00001817
Iteration 159/1000 | Loss: 0.00001817
Iteration 160/1000 | Loss: 0.00001816
Iteration 161/1000 | Loss: 0.00001816
Iteration 162/1000 | Loss: 0.00001816
Iteration 163/1000 | Loss: 0.00001815
Iteration 164/1000 | Loss: 0.00001815
Iteration 165/1000 | Loss: 0.00001815
Iteration 166/1000 | Loss: 0.00001815
Iteration 167/1000 | Loss: 0.00001815
Iteration 168/1000 | Loss: 0.00001815
Iteration 169/1000 | Loss: 0.00001815
Iteration 170/1000 | Loss: 0.00001815
Iteration 171/1000 | Loss: 0.00001815
Iteration 172/1000 | Loss: 0.00001815
Iteration 173/1000 | Loss: 0.00001815
Iteration 174/1000 | Loss: 0.00001815
Iteration 175/1000 | Loss: 0.00001815
Iteration 176/1000 | Loss: 0.00001815
Iteration 177/1000 | Loss: 0.00001815
Iteration 178/1000 | Loss: 0.00001815
Iteration 179/1000 | Loss: 0.00001815
Iteration 180/1000 | Loss: 0.00001815
Iteration 181/1000 | Loss: 0.00001815
Iteration 182/1000 | Loss: 0.00001815
Iteration 183/1000 | Loss: 0.00001815
Iteration 184/1000 | Loss: 0.00001815
Iteration 185/1000 | Loss: 0.00001815
Iteration 186/1000 | Loss: 0.00001815
Iteration 187/1000 | Loss: 0.00001815
Iteration 188/1000 | Loss: 0.00001815
Iteration 189/1000 | Loss: 0.00001815
Iteration 190/1000 | Loss: 0.00001815
Iteration 191/1000 | Loss: 0.00001815
Iteration 192/1000 | Loss: 0.00001815
Iteration 193/1000 | Loss: 0.00001815
Iteration 194/1000 | Loss: 0.00001815
Iteration 195/1000 | Loss: 0.00001815
Iteration 196/1000 | Loss: 0.00001815
Iteration 197/1000 | Loss: 0.00001815
Iteration 198/1000 | Loss: 0.00001815
Iteration 199/1000 | Loss: 0.00001815
Iteration 200/1000 | Loss: 0.00001815
Iteration 201/1000 | Loss: 0.00001815
Iteration 202/1000 | Loss: 0.00001815
Iteration 203/1000 | Loss: 0.00001815
Iteration 204/1000 | Loss: 0.00001815
Iteration 205/1000 | Loss: 0.00001815
Iteration 206/1000 | Loss: 0.00001815
Iteration 207/1000 | Loss: 0.00001815
Iteration 208/1000 | Loss: 0.00001815
Iteration 209/1000 | Loss: 0.00001815
Iteration 210/1000 | Loss: 0.00001815
Iteration 211/1000 | Loss: 0.00001815
Iteration 212/1000 | Loss: 0.00001815
Iteration 213/1000 | Loss: 0.00001815
Iteration 214/1000 | Loss: 0.00001815
Iteration 215/1000 | Loss: 0.00001815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.815161704143975e-05, 1.815161704143975e-05, 1.815161704143975e-05, 1.815161704143975e-05, 1.815161704143975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.815161704143975e-05

Optimization complete. Final v2v error: 3.312098741531372 mm

Highest mean error: 5.129703998565674 mm for frame 57

Lowest mean error: 2.6644885540008545 mm for frame 120

Saving results

Total time: 41.73573422431946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431517
Iteration 2/25 | Loss: 0.00106588
Iteration 3/25 | Loss: 0.00097970
Iteration 4/25 | Loss: 0.00096559
Iteration 5/25 | Loss: 0.00096093
Iteration 6/25 | Loss: 0.00095952
Iteration 7/25 | Loss: 0.00095948
Iteration 8/25 | Loss: 0.00095948
Iteration 9/25 | Loss: 0.00095948
Iteration 10/25 | Loss: 0.00095948
Iteration 11/25 | Loss: 0.00095948
Iteration 12/25 | Loss: 0.00095948
Iteration 13/25 | Loss: 0.00095948
Iteration 14/25 | Loss: 0.00095948
Iteration 15/25 | Loss: 0.00095948
Iteration 16/25 | Loss: 0.00095948
Iteration 17/25 | Loss: 0.00095948
Iteration 18/25 | Loss: 0.00095948
Iteration 19/25 | Loss: 0.00095948
Iteration 20/25 | Loss: 0.00095948
Iteration 21/25 | Loss: 0.00095948
Iteration 22/25 | Loss: 0.00095948
Iteration 23/25 | Loss: 0.00095948
Iteration 24/25 | Loss: 0.00095948
Iteration 25/25 | Loss: 0.00095948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56614804
Iteration 2/25 | Loss: 0.00071385
Iteration 3/25 | Loss: 0.00071385
Iteration 4/25 | Loss: 0.00071385
Iteration 5/25 | Loss: 0.00071385
Iteration 6/25 | Loss: 0.00071385
Iteration 7/25 | Loss: 0.00071384
Iteration 8/25 | Loss: 0.00071384
Iteration 9/25 | Loss: 0.00071384
Iteration 10/25 | Loss: 0.00071384
Iteration 11/25 | Loss: 0.00071384
Iteration 12/25 | Loss: 0.00071384
Iteration 13/25 | Loss: 0.00071384
Iteration 14/25 | Loss: 0.00071384
Iteration 15/25 | Loss: 0.00071384
Iteration 16/25 | Loss: 0.00071384
Iteration 17/25 | Loss: 0.00071384
Iteration 18/25 | Loss: 0.00071384
Iteration 19/25 | Loss: 0.00071384
Iteration 20/25 | Loss: 0.00071384
Iteration 21/25 | Loss: 0.00071384
Iteration 22/25 | Loss: 0.00071384
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007138439686968923, 0.0007138439686968923, 0.0007138439686968923, 0.0007138439686968923, 0.0007138439686968923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007138439686968923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071384
Iteration 2/1000 | Loss: 0.00003343
Iteration 3/1000 | Loss: 0.00002019
Iteration 4/1000 | Loss: 0.00001720
Iteration 5/1000 | Loss: 0.00001587
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001426
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001382
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001375
Iteration 14/1000 | Loss: 0.00001374
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001368
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001364
Iteration 25/1000 | Loss: 0.00001364
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001360
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001358
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001354
Iteration 37/1000 | Loss: 0.00001353
Iteration 38/1000 | Loss: 0.00001353
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001339
Iteration 86/1000 | Loss: 0.00001339
Iteration 87/1000 | Loss: 0.00001339
Iteration 88/1000 | Loss: 0.00001339
Iteration 89/1000 | Loss: 0.00001339
Iteration 90/1000 | Loss: 0.00001339
Iteration 91/1000 | Loss: 0.00001339
Iteration 92/1000 | Loss: 0.00001339
Iteration 93/1000 | Loss: 0.00001339
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001338
Iteration 96/1000 | Loss: 0.00001338
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001338
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001338
Iteration 103/1000 | Loss: 0.00001338
Iteration 104/1000 | Loss: 0.00001338
Iteration 105/1000 | Loss: 0.00001338
Iteration 106/1000 | Loss: 0.00001338
Iteration 107/1000 | Loss: 0.00001338
Iteration 108/1000 | Loss: 0.00001338
Iteration 109/1000 | Loss: 0.00001337
Iteration 110/1000 | Loss: 0.00001337
Iteration 111/1000 | Loss: 0.00001337
Iteration 112/1000 | Loss: 0.00001337
Iteration 113/1000 | Loss: 0.00001337
Iteration 114/1000 | Loss: 0.00001337
Iteration 115/1000 | Loss: 0.00001337
Iteration 116/1000 | Loss: 0.00001337
Iteration 117/1000 | Loss: 0.00001337
Iteration 118/1000 | Loss: 0.00001337
Iteration 119/1000 | Loss: 0.00001337
Iteration 120/1000 | Loss: 0.00001337
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001337
Iteration 123/1000 | Loss: 0.00001337
Iteration 124/1000 | Loss: 0.00001337
Iteration 125/1000 | Loss: 0.00001337
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001337
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.3371615750656929e-05, 1.3371615750656929e-05, 1.3371615750656929e-05, 1.3371615750656929e-05, 1.3371615750656929e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3371615750656929e-05

Optimization complete. Final v2v error: 3.0764949321746826 mm

Highest mean error: 3.542288064956665 mm for frame 54

Lowest mean error: 2.59738826751709 mm for frame 102

Saving results

Total time: 32.298229932785034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853958
Iteration 2/25 | Loss: 0.00171408
Iteration 3/25 | Loss: 0.00138711
Iteration 4/25 | Loss: 0.00135162
Iteration 5/25 | Loss: 0.00134414
Iteration 6/25 | Loss: 0.00133520
Iteration 7/25 | Loss: 0.00132786
Iteration 8/25 | Loss: 0.00132588
Iteration 9/25 | Loss: 0.00132482
Iteration 10/25 | Loss: 0.00132414
Iteration 11/25 | Loss: 0.00132321
Iteration 12/25 | Loss: 0.00132230
Iteration 13/25 | Loss: 0.00132090
Iteration 14/25 | Loss: 0.00132014
Iteration 15/25 | Loss: 0.00131982
Iteration 16/25 | Loss: 0.00131969
Iteration 17/25 | Loss: 0.00131967
Iteration 18/25 | Loss: 0.00131967
Iteration 19/25 | Loss: 0.00131967
Iteration 20/25 | Loss: 0.00131967
Iteration 21/25 | Loss: 0.00131967
Iteration 22/25 | Loss: 0.00131967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013196681393310428, 0.0013196681393310428, 0.0013196681393310428, 0.0013196681393310428, 0.0013196681393310428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013196681393310428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44450474
Iteration 2/25 | Loss: 0.00233948
Iteration 3/25 | Loss: 0.00233948
Iteration 4/25 | Loss: 0.00233948
Iteration 5/25 | Loss: 0.00233948
Iteration 6/25 | Loss: 0.00233948
Iteration 7/25 | Loss: 0.00233948
Iteration 8/25 | Loss: 0.00233948
Iteration 9/25 | Loss: 0.00233948
Iteration 10/25 | Loss: 0.00233948
Iteration 11/25 | Loss: 0.00233948
Iteration 12/25 | Loss: 0.00233948
Iteration 13/25 | Loss: 0.00233948
Iteration 14/25 | Loss: 0.00233948
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0023394785821437836, 0.0023394785821437836, 0.0023394785821437836, 0.0023394785821437836, 0.0023394785821437836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023394785821437836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233948
Iteration 2/1000 | Loss: 0.00040453
Iteration 3/1000 | Loss: 0.00030959
Iteration 4/1000 | Loss: 0.00046777
Iteration 5/1000 | Loss: 0.00432356
Iteration 6/1000 | Loss: 0.00052986
Iteration 7/1000 | Loss: 0.00029288
Iteration 8/1000 | Loss: 0.00615465
Iteration 9/1000 | Loss: 0.00110526
Iteration 10/1000 | Loss: 0.00049569
Iteration 11/1000 | Loss: 0.00358449
Iteration 12/1000 | Loss: 0.00402171
Iteration 13/1000 | Loss: 0.00406203
Iteration 14/1000 | Loss: 0.00615942
Iteration 15/1000 | Loss: 0.00123538
Iteration 16/1000 | Loss: 0.00043992
Iteration 17/1000 | Loss: 0.00020885
Iteration 18/1000 | Loss: 0.00011076
Iteration 19/1000 | Loss: 0.00013478
Iteration 20/1000 | Loss: 0.00005637
Iteration 21/1000 | Loss: 0.00017124
Iteration 22/1000 | Loss: 0.00061605
Iteration 23/1000 | Loss: 0.00055793
Iteration 24/1000 | Loss: 0.00053569
Iteration 25/1000 | Loss: 0.00005284
Iteration 26/1000 | Loss: 0.00004347
Iteration 27/1000 | Loss: 0.00003594
Iteration 28/1000 | Loss: 0.00003309
Iteration 29/1000 | Loss: 0.00016678
Iteration 30/1000 | Loss: 0.00037584
Iteration 31/1000 | Loss: 0.00003691
Iteration 32/1000 | Loss: 0.00006001
Iteration 33/1000 | Loss: 0.00003668
Iteration 34/1000 | Loss: 0.00002676
Iteration 35/1000 | Loss: 0.00007046
Iteration 36/1000 | Loss: 0.00003370
Iteration 37/1000 | Loss: 0.00002446
Iteration 38/1000 | Loss: 0.00003876
Iteration 39/1000 | Loss: 0.00002305
Iteration 40/1000 | Loss: 0.00007466
Iteration 41/1000 | Loss: 0.00002310
Iteration 42/1000 | Loss: 0.00002175
Iteration 43/1000 | Loss: 0.00002122
Iteration 44/1000 | Loss: 0.00054360
Iteration 45/1000 | Loss: 0.00104055
Iteration 46/1000 | Loss: 0.00059856
Iteration 47/1000 | Loss: 0.00022162
Iteration 48/1000 | Loss: 0.00003069
Iteration 49/1000 | Loss: 0.00002438
Iteration 50/1000 | Loss: 0.00002240
Iteration 51/1000 | Loss: 0.00002128
Iteration 52/1000 | Loss: 0.00002066
Iteration 53/1000 | Loss: 0.00002042
Iteration 54/1000 | Loss: 0.00002023
Iteration 55/1000 | Loss: 0.00002009
Iteration 56/1000 | Loss: 0.00002009
Iteration 57/1000 | Loss: 0.00002007
Iteration 58/1000 | Loss: 0.00002006
Iteration 59/1000 | Loss: 0.00002003
Iteration 60/1000 | Loss: 0.00002002
Iteration 61/1000 | Loss: 0.00001994
Iteration 62/1000 | Loss: 0.00001986
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001983
Iteration 65/1000 | Loss: 0.00001982
Iteration 66/1000 | Loss: 0.00001982
Iteration 67/1000 | Loss: 0.00001981
Iteration 68/1000 | Loss: 0.00001981
Iteration 69/1000 | Loss: 0.00001981
Iteration 70/1000 | Loss: 0.00001980
Iteration 71/1000 | Loss: 0.00001980
Iteration 72/1000 | Loss: 0.00001979
Iteration 73/1000 | Loss: 0.00001979
Iteration 74/1000 | Loss: 0.00001979
Iteration 75/1000 | Loss: 0.00001978
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001976
Iteration 78/1000 | Loss: 0.00001975
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001973
Iteration 91/1000 | Loss: 0.00001972
Iteration 92/1000 | Loss: 0.00001972
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001971
Iteration 101/1000 | Loss: 0.00001971
Iteration 102/1000 | Loss: 0.00001971
Iteration 103/1000 | Loss: 0.00001971
Iteration 104/1000 | Loss: 0.00001971
Iteration 105/1000 | Loss: 0.00001971
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001970
Iteration 110/1000 | Loss: 0.00001970
Iteration 111/1000 | Loss: 0.00001969
Iteration 112/1000 | Loss: 0.00001969
Iteration 113/1000 | Loss: 0.00001968
Iteration 114/1000 | Loss: 0.00001968
Iteration 115/1000 | Loss: 0.00001968
Iteration 116/1000 | Loss: 0.00001967
Iteration 117/1000 | Loss: 0.00001967
Iteration 118/1000 | Loss: 0.00001966
Iteration 119/1000 | Loss: 0.00001966
Iteration 120/1000 | Loss: 0.00001966
Iteration 121/1000 | Loss: 0.00001964
Iteration 122/1000 | Loss: 0.00001964
Iteration 123/1000 | Loss: 0.00001964
Iteration 124/1000 | Loss: 0.00001964
Iteration 125/1000 | Loss: 0.00001964
Iteration 126/1000 | Loss: 0.00001964
Iteration 127/1000 | Loss: 0.00001963
Iteration 128/1000 | Loss: 0.00001963
Iteration 129/1000 | Loss: 0.00001963
Iteration 130/1000 | Loss: 0.00001962
Iteration 131/1000 | Loss: 0.00001962
Iteration 132/1000 | Loss: 0.00001962
Iteration 133/1000 | Loss: 0.00001962
Iteration 134/1000 | Loss: 0.00001962
Iteration 135/1000 | Loss: 0.00001962
Iteration 136/1000 | Loss: 0.00001962
Iteration 137/1000 | Loss: 0.00001961
Iteration 138/1000 | Loss: 0.00001961
Iteration 139/1000 | Loss: 0.00001961
Iteration 140/1000 | Loss: 0.00001960
Iteration 141/1000 | Loss: 0.00001960
Iteration 142/1000 | Loss: 0.00001960
Iteration 143/1000 | Loss: 0.00001960
Iteration 144/1000 | Loss: 0.00001960
Iteration 145/1000 | Loss: 0.00001960
Iteration 146/1000 | Loss: 0.00001960
Iteration 147/1000 | Loss: 0.00001960
Iteration 148/1000 | Loss: 0.00001960
Iteration 149/1000 | Loss: 0.00001959
Iteration 150/1000 | Loss: 0.00001959
Iteration 151/1000 | Loss: 0.00001959
Iteration 152/1000 | Loss: 0.00001959
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001958
Iteration 155/1000 | Loss: 0.00001958
Iteration 156/1000 | Loss: 0.00001958
Iteration 157/1000 | Loss: 0.00001958
Iteration 158/1000 | Loss: 0.00001958
Iteration 159/1000 | Loss: 0.00001958
Iteration 160/1000 | Loss: 0.00001958
Iteration 161/1000 | Loss: 0.00001958
Iteration 162/1000 | Loss: 0.00001957
Iteration 163/1000 | Loss: 0.00001957
Iteration 164/1000 | Loss: 0.00001957
Iteration 165/1000 | Loss: 0.00001957
Iteration 166/1000 | Loss: 0.00001957
Iteration 167/1000 | Loss: 0.00001957
Iteration 168/1000 | Loss: 0.00001957
Iteration 169/1000 | Loss: 0.00001957
Iteration 170/1000 | Loss: 0.00001957
Iteration 171/1000 | Loss: 0.00001957
Iteration 172/1000 | Loss: 0.00001957
Iteration 173/1000 | Loss: 0.00001957
Iteration 174/1000 | Loss: 0.00001957
Iteration 175/1000 | Loss: 0.00001957
Iteration 176/1000 | Loss: 0.00001957
Iteration 177/1000 | Loss: 0.00001957
Iteration 178/1000 | Loss: 0.00001957
Iteration 179/1000 | Loss: 0.00001957
Iteration 180/1000 | Loss: 0.00001957
Iteration 181/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.9574110410758294e-05, 1.9574110410758294e-05, 1.9574110410758294e-05, 1.9574110410758294e-05, 1.9574110410758294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9574110410758294e-05

Optimization complete. Final v2v error: 3.4646570682525635 mm

Highest mean error: 12.458382606506348 mm for frame 59

Lowest mean error: 2.969703197479248 mm for frame 56

Saving results

Total time: 132.3043656349182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00288018
Iteration 2/25 | Loss: 0.00121541
Iteration 3/25 | Loss: 0.00105398
Iteration 4/25 | Loss: 0.00102986
Iteration 5/25 | Loss: 0.00102261
Iteration 6/25 | Loss: 0.00102033
Iteration 7/25 | Loss: 0.00101996
Iteration 8/25 | Loss: 0.00101996
Iteration 9/25 | Loss: 0.00101996
Iteration 10/25 | Loss: 0.00101996
Iteration 11/25 | Loss: 0.00101996
Iteration 12/25 | Loss: 0.00101996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010199553798884153, 0.0010199553798884153, 0.0010199553798884153, 0.0010199553798884153, 0.0010199553798884153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010199553798884153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35195363
Iteration 2/25 | Loss: 0.00074426
Iteration 3/25 | Loss: 0.00074426
Iteration 4/25 | Loss: 0.00074426
Iteration 5/25 | Loss: 0.00074426
Iteration 6/25 | Loss: 0.00074426
Iteration 7/25 | Loss: 0.00074426
Iteration 8/25 | Loss: 0.00074426
Iteration 9/25 | Loss: 0.00074426
Iteration 10/25 | Loss: 0.00074426
Iteration 11/25 | Loss: 0.00074426
Iteration 12/25 | Loss: 0.00074426
Iteration 13/25 | Loss: 0.00074426
Iteration 14/25 | Loss: 0.00074426
Iteration 15/25 | Loss: 0.00074426
Iteration 16/25 | Loss: 0.00074426
Iteration 17/25 | Loss: 0.00074426
Iteration 18/25 | Loss: 0.00074426
Iteration 19/25 | Loss: 0.00074426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007442552014254034, 0.0007442552014254034, 0.0007442552014254034, 0.0007442552014254034, 0.0007442552014254034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007442552014254034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074426
Iteration 2/1000 | Loss: 0.00003649
Iteration 3/1000 | Loss: 0.00002010
Iteration 4/1000 | Loss: 0.00001753
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001579
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001498
Iteration 9/1000 | Loss: 0.00001470
Iteration 10/1000 | Loss: 0.00001468
Iteration 11/1000 | Loss: 0.00001443
Iteration 12/1000 | Loss: 0.00001432
Iteration 13/1000 | Loss: 0.00001430
Iteration 14/1000 | Loss: 0.00001428
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001412
Iteration 18/1000 | Loss: 0.00001408
Iteration 19/1000 | Loss: 0.00001408
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001407
Iteration 23/1000 | Loss: 0.00001406
Iteration 24/1000 | Loss: 0.00001406
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001404
Iteration 31/1000 | Loss: 0.00001404
Iteration 32/1000 | Loss: 0.00001404
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001403
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001398
Iteration 59/1000 | Loss: 0.00001398
Iteration 60/1000 | Loss: 0.00001398
Iteration 61/1000 | Loss: 0.00001398
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001397
Iteration 72/1000 | Loss: 0.00001397
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00001397
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001397
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001397
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001396
Iteration 86/1000 | Loss: 0.00001396
Iteration 87/1000 | Loss: 0.00001396
Iteration 88/1000 | Loss: 0.00001396
Iteration 89/1000 | Loss: 0.00001396
Iteration 90/1000 | Loss: 0.00001396
Iteration 91/1000 | Loss: 0.00001396
Iteration 92/1000 | Loss: 0.00001396
Iteration 93/1000 | Loss: 0.00001396
Iteration 94/1000 | Loss: 0.00001396
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001396
Iteration 98/1000 | Loss: 0.00001395
Iteration 99/1000 | Loss: 0.00001395
Iteration 100/1000 | Loss: 0.00001395
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001395
Iteration 105/1000 | Loss: 0.00001395
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001394
Iteration 108/1000 | Loss: 0.00001394
Iteration 109/1000 | Loss: 0.00001394
Iteration 110/1000 | Loss: 0.00001394
Iteration 111/1000 | Loss: 0.00001394
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.3942561054136604e-05, 1.3942561054136604e-05, 1.3942561054136604e-05, 1.3942561054136604e-05, 1.3942561054136604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3942561054136604e-05

Optimization complete. Final v2v error: 3.1465470790863037 mm

Highest mean error: 3.5854289531707764 mm for frame 208

Lowest mean error: 2.7364213466644287 mm for frame 4

Saving results

Total time: 39.26063632965088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069737
Iteration 2/25 | Loss: 0.00235365
Iteration 3/25 | Loss: 0.00152866
Iteration 4/25 | Loss: 0.00129695
Iteration 5/25 | Loss: 0.00123761
Iteration 6/25 | Loss: 0.00123895
Iteration 7/25 | Loss: 0.00116152
Iteration 8/25 | Loss: 0.00113640
Iteration 9/25 | Loss: 0.00112967
Iteration 10/25 | Loss: 0.00109836
Iteration 11/25 | Loss: 0.00107980
Iteration 12/25 | Loss: 0.00104967
Iteration 13/25 | Loss: 0.00103489
Iteration 14/25 | Loss: 0.00103801
Iteration 15/25 | Loss: 0.00104306
Iteration 16/25 | Loss: 0.00103865
Iteration 17/25 | Loss: 0.00102949
Iteration 18/25 | Loss: 0.00102608
Iteration 19/25 | Loss: 0.00102301
Iteration 20/25 | Loss: 0.00103058
Iteration 21/25 | Loss: 0.00101409
Iteration 22/25 | Loss: 0.00100930
Iteration 23/25 | Loss: 0.00100788
Iteration 24/25 | Loss: 0.00100739
Iteration 25/25 | Loss: 0.00100721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06342864
Iteration 2/25 | Loss: 0.00046756
Iteration 3/25 | Loss: 0.00046756
Iteration 4/25 | Loss: 0.00046756
Iteration 5/25 | Loss: 0.00046756
Iteration 6/25 | Loss: 0.00046756
Iteration 7/25 | Loss: 0.00046756
Iteration 8/25 | Loss: 0.00046755
Iteration 9/25 | Loss: 0.00046755
Iteration 10/25 | Loss: 0.00046755
Iteration 11/25 | Loss: 0.00046755
Iteration 12/25 | Loss: 0.00046755
Iteration 13/25 | Loss: 0.00046755
Iteration 14/25 | Loss: 0.00046755
Iteration 15/25 | Loss: 0.00046755
Iteration 16/25 | Loss: 0.00046755
Iteration 17/25 | Loss: 0.00046755
Iteration 18/25 | Loss: 0.00046755
Iteration 19/25 | Loss: 0.00046755
Iteration 20/25 | Loss: 0.00046755
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0004675544041674584, 0.0004675544041674584, 0.0004675544041674584, 0.0004675544041674584, 0.0004675544041674584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004675544041674584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046755
Iteration 2/1000 | Loss: 0.00023151
Iteration 3/1000 | Loss: 0.00003171
Iteration 4/1000 | Loss: 0.00018610
Iteration 5/1000 | Loss: 0.00012444
Iteration 6/1000 | Loss: 0.00003635
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001631
Iteration 10/1000 | Loss: 0.00001565
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00019380
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001487
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001470
Iteration 19/1000 | Loss: 0.00017044
Iteration 20/1000 | Loss: 0.00003042
Iteration 21/1000 | Loss: 0.00001483
Iteration 22/1000 | Loss: 0.00009756
Iteration 23/1000 | Loss: 0.00003755
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00010539
Iteration 26/1000 | Loss: 0.00002328
Iteration 27/1000 | Loss: 0.00002762
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001450
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001450
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001449
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001446
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001445
Iteration 44/1000 | Loss: 0.00001445
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001445
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001444
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001443
Iteration 69/1000 | Loss: 0.00001443
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001443
Iteration 76/1000 | Loss: 0.00001443
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001443
Iteration 85/1000 | Loss: 0.00001443
Iteration 86/1000 | Loss: 0.00001443
Iteration 87/1000 | Loss: 0.00001443
Iteration 88/1000 | Loss: 0.00001443
Iteration 89/1000 | Loss: 0.00001443
Iteration 90/1000 | Loss: 0.00001443
Iteration 91/1000 | Loss: 0.00001443
Iteration 92/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.4427630048885476e-05, 1.4427630048885476e-05, 1.4427630048885476e-05, 1.4427630048885476e-05, 1.4427630048885476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4427630048885476e-05

Optimization complete. Final v2v error: 3.096419334411621 mm

Highest mean error: 3.554550886154175 mm for frame 51

Lowest mean error: 2.824906349182129 mm for frame 71

Saving results

Total time: 83.31561040878296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076201
Iteration 2/25 | Loss: 0.00255484
Iteration 3/25 | Loss: 0.00191527
Iteration 4/25 | Loss: 0.00170273
Iteration 5/25 | Loss: 0.00163941
Iteration 6/25 | Loss: 0.00162607
Iteration 7/25 | Loss: 0.00146951
Iteration 8/25 | Loss: 0.00138597
Iteration 9/25 | Loss: 0.00131488
Iteration 10/25 | Loss: 0.00130544
Iteration 11/25 | Loss: 0.00126783
Iteration 12/25 | Loss: 0.00126403
Iteration 13/25 | Loss: 0.00124982
Iteration 14/25 | Loss: 0.00123753
Iteration 15/25 | Loss: 0.00123167
Iteration 16/25 | Loss: 0.00122458
Iteration 17/25 | Loss: 0.00122085
Iteration 18/25 | Loss: 0.00121471
Iteration 19/25 | Loss: 0.00120920
Iteration 20/25 | Loss: 0.00120692
Iteration 21/25 | Loss: 0.00120462
Iteration 22/25 | Loss: 0.00120287
Iteration 23/25 | Loss: 0.00120203
Iteration 24/25 | Loss: 0.00120753
Iteration 25/25 | Loss: 0.00120503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46946430
Iteration 2/25 | Loss: 0.00308893
Iteration 3/25 | Loss: 0.00231147
Iteration 4/25 | Loss: 0.00231147
Iteration 5/25 | Loss: 0.00231147
Iteration 6/25 | Loss: 0.00231147
Iteration 7/25 | Loss: 0.00231147
Iteration 8/25 | Loss: 0.00231147
Iteration 9/25 | Loss: 0.00231147
Iteration 10/25 | Loss: 0.00231147
Iteration 11/25 | Loss: 0.00231147
Iteration 12/25 | Loss: 0.00231147
Iteration 13/25 | Loss: 0.00231147
Iteration 14/25 | Loss: 0.00231147
Iteration 15/25 | Loss: 0.00231147
Iteration 16/25 | Loss: 0.00231147
Iteration 17/25 | Loss: 0.00231147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0023114709183573723, 0.0023114709183573723, 0.0023114709183573723, 0.0023114709183573723, 0.0023114709183573723]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023114709183573723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231147
Iteration 2/1000 | Loss: 0.00064993
Iteration 3/1000 | Loss: 0.00087849
Iteration 4/1000 | Loss: 0.00035230
Iteration 5/1000 | Loss: 0.00122454
Iteration 6/1000 | Loss: 0.00020393
Iteration 7/1000 | Loss: 0.00026355
Iteration 8/1000 | Loss: 0.00017248
Iteration 9/1000 | Loss: 0.00032326
Iteration 10/1000 | Loss: 0.00015730
Iteration 11/1000 | Loss: 0.00069629
Iteration 12/1000 | Loss: 0.00030389
Iteration 13/1000 | Loss: 0.00014668
Iteration 14/1000 | Loss: 0.00014084
Iteration 15/1000 | Loss: 0.00028167
Iteration 16/1000 | Loss: 0.00013544
Iteration 17/1000 | Loss: 0.00013366
Iteration 18/1000 | Loss: 0.00084169
Iteration 19/1000 | Loss: 0.00124759
Iteration 20/1000 | Loss: 0.00167139
Iteration 21/1000 | Loss: 0.00098879
Iteration 22/1000 | Loss: 0.00103570
Iteration 23/1000 | Loss: 0.00047244
Iteration 24/1000 | Loss: 0.00016312
Iteration 25/1000 | Loss: 0.00073254
Iteration 26/1000 | Loss: 0.00093115
Iteration 27/1000 | Loss: 0.00099997
Iteration 28/1000 | Loss: 0.00015444
Iteration 29/1000 | Loss: 0.00014371
Iteration 30/1000 | Loss: 0.00013917
Iteration 31/1000 | Loss: 0.00112227
Iteration 32/1000 | Loss: 0.00186605
Iteration 33/1000 | Loss: 0.00027054
Iteration 34/1000 | Loss: 0.00014447
Iteration 35/1000 | Loss: 0.00035663
Iteration 36/1000 | Loss: 0.00029794
Iteration 37/1000 | Loss: 0.00013426
Iteration 38/1000 | Loss: 0.00032807
Iteration 39/1000 | Loss: 0.00013014
Iteration 40/1000 | Loss: 0.00079927
Iteration 41/1000 | Loss: 0.00142042
Iteration 42/1000 | Loss: 0.00043482
Iteration 43/1000 | Loss: 0.00017069
Iteration 44/1000 | Loss: 0.00013392
Iteration 45/1000 | Loss: 0.00044773
Iteration 46/1000 | Loss: 0.00051503
Iteration 47/1000 | Loss: 0.00095399
Iteration 48/1000 | Loss: 0.00038524
Iteration 49/1000 | Loss: 0.00014470
Iteration 50/1000 | Loss: 0.00013582
Iteration 51/1000 | Loss: 0.00062393
Iteration 52/1000 | Loss: 0.00040653
Iteration 53/1000 | Loss: 0.00045555
Iteration 54/1000 | Loss: 0.00060039
Iteration 55/1000 | Loss: 0.00049952
Iteration 56/1000 | Loss: 0.00018874
Iteration 57/1000 | Loss: 0.00012982
Iteration 58/1000 | Loss: 0.00012747
Iteration 59/1000 | Loss: 0.00012612
Iteration 60/1000 | Loss: 0.00012527
Iteration 61/1000 | Loss: 0.00045958
Iteration 62/1000 | Loss: 0.00012479
Iteration 63/1000 | Loss: 0.00012362
Iteration 64/1000 | Loss: 0.00012300
Iteration 65/1000 | Loss: 0.00024783
Iteration 66/1000 | Loss: 0.00012548
Iteration 67/1000 | Loss: 0.00037042
Iteration 68/1000 | Loss: 0.00014300
Iteration 69/1000 | Loss: 0.00018084
Iteration 70/1000 | Loss: 0.00011954
Iteration 71/1000 | Loss: 0.00014075
Iteration 72/1000 | Loss: 0.00013644
Iteration 73/1000 | Loss: 0.00012411
Iteration 74/1000 | Loss: 0.00012169
Iteration 75/1000 | Loss: 0.00011848
Iteration 76/1000 | Loss: 0.00011803
Iteration 77/1000 | Loss: 0.00014364
Iteration 78/1000 | Loss: 0.00072960
Iteration 79/1000 | Loss: 0.00044520
Iteration 80/1000 | Loss: 0.00019212
Iteration 81/1000 | Loss: 0.00012301
Iteration 82/1000 | Loss: 0.00011875
Iteration 83/1000 | Loss: 0.00011689
Iteration 84/1000 | Loss: 0.00011541
Iteration 85/1000 | Loss: 0.00011461
Iteration 86/1000 | Loss: 0.00011413
Iteration 87/1000 | Loss: 0.00011368
Iteration 88/1000 | Loss: 0.00070513
Iteration 89/1000 | Loss: 0.00012557
Iteration 90/1000 | Loss: 0.00011536
Iteration 91/1000 | Loss: 0.00011300
Iteration 92/1000 | Loss: 0.00011131
Iteration 93/1000 | Loss: 0.00010987
Iteration 94/1000 | Loss: 0.00010898
Iteration 95/1000 | Loss: 0.00010824
Iteration 96/1000 | Loss: 0.00010744
Iteration 97/1000 | Loss: 0.00010623
Iteration 98/1000 | Loss: 0.00010545
Iteration 99/1000 | Loss: 0.00010460
Iteration 100/1000 | Loss: 0.00010370
Iteration 101/1000 | Loss: 0.00010258
Iteration 102/1000 | Loss: 0.00010192
Iteration 103/1000 | Loss: 0.00010109
Iteration 104/1000 | Loss: 0.00010062
Iteration 105/1000 | Loss: 0.00009987
Iteration 106/1000 | Loss: 0.00025721
Iteration 107/1000 | Loss: 0.00031494
Iteration 108/1000 | Loss: 0.00022423
Iteration 109/1000 | Loss: 0.00010076
Iteration 110/1000 | Loss: 0.00009889
Iteration 111/1000 | Loss: 0.00016776
Iteration 112/1000 | Loss: 0.00012900
Iteration 113/1000 | Loss: 0.00009573
Iteration 114/1000 | Loss: 0.00009526
Iteration 115/1000 | Loss: 0.00010884
Iteration 116/1000 | Loss: 0.00009482
Iteration 117/1000 | Loss: 0.00009448
Iteration 118/1000 | Loss: 0.00009424
Iteration 119/1000 | Loss: 0.00009406
Iteration 120/1000 | Loss: 0.00009388
Iteration 121/1000 | Loss: 0.00016861
Iteration 122/1000 | Loss: 0.00009383
Iteration 123/1000 | Loss: 0.00009357
Iteration 124/1000 | Loss: 0.00016101
Iteration 125/1000 | Loss: 0.00009346
Iteration 126/1000 | Loss: 0.00009331
Iteration 127/1000 | Loss: 0.00009306
Iteration 128/1000 | Loss: 0.00009275
Iteration 129/1000 | Loss: 0.00017787
Iteration 130/1000 | Loss: 0.00009254
Iteration 131/1000 | Loss: 0.00009223
Iteration 132/1000 | Loss: 0.00014104
Iteration 133/1000 | Loss: 0.00012372
Iteration 134/1000 | Loss: 0.00011663
Iteration 135/1000 | Loss: 0.00009192
Iteration 136/1000 | Loss: 0.00009186
Iteration 137/1000 | Loss: 0.00009184
Iteration 138/1000 | Loss: 0.00009182
Iteration 139/1000 | Loss: 0.00009181
Iteration 140/1000 | Loss: 0.00009168
Iteration 141/1000 | Loss: 0.00009167
Iteration 142/1000 | Loss: 0.00009167
Iteration 143/1000 | Loss: 0.00009167
Iteration 144/1000 | Loss: 0.00009166
Iteration 145/1000 | Loss: 0.00009166
Iteration 146/1000 | Loss: 0.00009165
Iteration 147/1000 | Loss: 0.00009165
Iteration 148/1000 | Loss: 0.00009164
Iteration 149/1000 | Loss: 0.00009164
Iteration 150/1000 | Loss: 0.00009163
Iteration 151/1000 | Loss: 0.00009163
Iteration 152/1000 | Loss: 0.00009163
Iteration 153/1000 | Loss: 0.00009163
Iteration 154/1000 | Loss: 0.00009163
Iteration 155/1000 | Loss: 0.00009163
Iteration 156/1000 | Loss: 0.00009163
Iteration 157/1000 | Loss: 0.00009163
Iteration 158/1000 | Loss: 0.00009163
Iteration 159/1000 | Loss: 0.00009163
Iteration 160/1000 | Loss: 0.00009162
Iteration 161/1000 | Loss: 0.00009160
Iteration 162/1000 | Loss: 0.00009160
Iteration 163/1000 | Loss: 0.00009160
Iteration 164/1000 | Loss: 0.00009160
Iteration 165/1000 | Loss: 0.00009159
Iteration 166/1000 | Loss: 0.00009159
Iteration 167/1000 | Loss: 0.00009159
Iteration 168/1000 | Loss: 0.00009159
Iteration 169/1000 | Loss: 0.00009159
Iteration 170/1000 | Loss: 0.00009159
Iteration 171/1000 | Loss: 0.00009159
Iteration 172/1000 | Loss: 0.00009158
Iteration 173/1000 | Loss: 0.00009158
Iteration 174/1000 | Loss: 0.00009158
Iteration 175/1000 | Loss: 0.00009158
Iteration 176/1000 | Loss: 0.00009158
Iteration 177/1000 | Loss: 0.00009157
Iteration 178/1000 | Loss: 0.00009157
Iteration 179/1000 | Loss: 0.00009157
Iteration 180/1000 | Loss: 0.00009157
Iteration 181/1000 | Loss: 0.00009157
Iteration 182/1000 | Loss: 0.00009157
Iteration 183/1000 | Loss: 0.00009157
Iteration 184/1000 | Loss: 0.00009157
Iteration 185/1000 | Loss: 0.00009157
Iteration 186/1000 | Loss: 0.00009157
Iteration 187/1000 | Loss: 0.00009157
Iteration 188/1000 | Loss: 0.00009157
Iteration 189/1000 | Loss: 0.00009157
Iteration 190/1000 | Loss: 0.00009157
Iteration 191/1000 | Loss: 0.00009156
Iteration 192/1000 | Loss: 0.00009156
Iteration 193/1000 | Loss: 0.00009156
Iteration 194/1000 | Loss: 0.00009156
Iteration 195/1000 | Loss: 0.00009156
Iteration 196/1000 | Loss: 0.00009156
Iteration 197/1000 | Loss: 0.00009156
Iteration 198/1000 | Loss: 0.00009156
Iteration 199/1000 | Loss: 0.00009156
Iteration 200/1000 | Loss: 0.00009156
Iteration 201/1000 | Loss: 0.00009155
Iteration 202/1000 | Loss: 0.00009155
Iteration 203/1000 | Loss: 0.00009155
Iteration 204/1000 | Loss: 0.00009155
Iteration 205/1000 | Loss: 0.00009155
Iteration 206/1000 | Loss: 0.00009155
Iteration 207/1000 | Loss: 0.00009155
Iteration 208/1000 | Loss: 0.00009155
Iteration 209/1000 | Loss: 0.00009155
Iteration 210/1000 | Loss: 0.00009155
Iteration 211/1000 | Loss: 0.00009155
Iteration 212/1000 | Loss: 0.00009155
Iteration 213/1000 | Loss: 0.00009155
Iteration 214/1000 | Loss: 0.00009154
Iteration 215/1000 | Loss: 0.00009154
Iteration 216/1000 | Loss: 0.00009154
Iteration 217/1000 | Loss: 0.00009154
Iteration 218/1000 | Loss: 0.00009154
Iteration 219/1000 | Loss: 0.00009154
Iteration 220/1000 | Loss: 0.00009154
Iteration 221/1000 | Loss: 0.00009154
Iteration 222/1000 | Loss: 0.00009154
Iteration 223/1000 | Loss: 0.00009153
Iteration 224/1000 | Loss: 0.00009153
Iteration 225/1000 | Loss: 0.00009153
Iteration 226/1000 | Loss: 0.00009153
Iteration 227/1000 | Loss: 0.00009153
Iteration 228/1000 | Loss: 0.00009153
Iteration 229/1000 | Loss: 0.00009153
Iteration 230/1000 | Loss: 0.00009153
Iteration 231/1000 | Loss: 0.00009153
Iteration 232/1000 | Loss: 0.00009153
Iteration 233/1000 | Loss: 0.00009152
Iteration 234/1000 | Loss: 0.00009152
Iteration 235/1000 | Loss: 0.00009152
Iteration 236/1000 | Loss: 0.00009152
Iteration 237/1000 | Loss: 0.00009152
Iteration 238/1000 | Loss: 0.00009152
Iteration 239/1000 | Loss: 0.00009152
Iteration 240/1000 | Loss: 0.00009152
Iteration 241/1000 | Loss: 0.00009152
Iteration 242/1000 | Loss: 0.00009152
Iteration 243/1000 | Loss: 0.00009152
Iteration 244/1000 | Loss: 0.00009151
Iteration 245/1000 | Loss: 0.00009151
Iteration 246/1000 | Loss: 0.00009151
Iteration 247/1000 | Loss: 0.00009150
Iteration 248/1000 | Loss: 0.00009150
Iteration 249/1000 | Loss: 0.00009150
Iteration 250/1000 | Loss: 0.00009150
Iteration 251/1000 | Loss: 0.00009150
Iteration 252/1000 | Loss: 0.00009150
Iteration 253/1000 | Loss: 0.00009149
Iteration 254/1000 | Loss: 0.00009149
Iteration 255/1000 | Loss: 0.00009149
Iteration 256/1000 | Loss: 0.00009149
Iteration 257/1000 | Loss: 0.00009148
Iteration 258/1000 | Loss: 0.00009148
Iteration 259/1000 | Loss: 0.00009148
Iteration 260/1000 | Loss: 0.00009148
Iteration 261/1000 | Loss: 0.00009148
Iteration 262/1000 | Loss: 0.00009148
Iteration 263/1000 | Loss: 0.00009148
Iteration 264/1000 | Loss: 0.00009148
Iteration 265/1000 | Loss: 0.00009148
Iteration 266/1000 | Loss: 0.00009148
Iteration 267/1000 | Loss: 0.00009148
Iteration 268/1000 | Loss: 0.00009148
Iteration 269/1000 | Loss: 0.00009148
Iteration 270/1000 | Loss: 0.00009148
Iteration 271/1000 | Loss: 0.00009148
Iteration 272/1000 | Loss: 0.00009148
Iteration 273/1000 | Loss: 0.00009148
Iteration 274/1000 | Loss: 0.00009148
Iteration 275/1000 | Loss: 0.00009148
Iteration 276/1000 | Loss: 0.00009148
Iteration 277/1000 | Loss: 0.00009148
Iteration 278/1000 | Loss: 0.00009148
Iteration 279/1000 | Loss: 0.00009148
Iteration 280/1000 | Loss: 0.00009148
Iteration 281/1000 | Loss: 0.00009148
Iteration 282/1000 | Loss: 0.00009148
Iteration 283/1000 | Loss: 0.00009148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [9.147622040472925e-05, 9.147622040472925e-05, 9.147622040472925e-05, 9.147622040472925e-05, 9.147622040472925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.147622040472925e-05

Optimization complete. Final v2v error: 4.94394063949585 mm

Highest mean error: 12.706151008605957 mm for frame 13

Lowest mean error: 3.1328961849212646 mm for frame 11

Saving results

Total time: 249.2619662284851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066612
Iteration 2/25 | Loss: 0.00182136
Iteration 3/25 | Loss: 0.00135889
Iteration 4/25 | Loss: 0.00124038
Iteration 5/25 | Loss: 0.00119308
Iteration 6/25 | Loss: 0.00115938
Iteration 7/25 | Loss: 0.00114787
Iteration 8/25 | Loss: 0.00114892
Iteration 9/25 | Loss: 0.00114632
Iteration 10/25 | Loss: 0.00114732
Iteration 11/25 | Loss: 0.00114289
Iteration 12/25 | Loss: 0.00114609
Iteration 13/25 | Loss: 0.00114259
Iteration 14/25 | Loss: 0.00114250
Iteration 15/25 | Loss: 0.00114262
Iteration 16/25 | Loss: 0.00114926
Iteration 17/25 | Loss: 0.00114224
Iteration 18/25 | Loss: 0.00114139
Iteration 19/25 | Loss: 0.00114308
Iteration 20/25 | Loss: 0.00114974
Iteration 21/25 | Loss: 0.00114285
Iteration 22/25 | Loss: 0.00114688
Iteration 23/25 | Loss: 0.00114364
Iteration 24/25 | Loss: 0.00114608
Iteration 25/25 | Loss: 0.00114187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.85371017
Iteration 2/25 | Loss: 0.00150627
Iteration 3/25 | Loss: 0.00149920
Iteration 4/25 | Loss: 0.00149920
Iteration 5/25 | Loss: 0.00149920
Iteration 6/25 | Loss: 0.00149920
Iteration 7/25 | Loss: 0.00149920
Iteration 8/25 | Loss: 0.00149920
Iteration 9/25 | Loss: 0.00149920
Iteration 10/25 | Loss: 0.00149920
Iteration 11/25 | Loss: 0.00149920
Iteration 12/25 | Loss: 0.00149920
Iteration 13/25 | Loss: 0.00149920
Iteration 14/25 | Loss: 0.00149920
Iteration 15/25 | Loss: 0.00149920
Iteration 16/25 | Loss: 0.00149920
Iteration 17/25 | Loss: 0.00149920
Iteration 18/25 | Loss: 0.00149920
Iteration 19/25 | Loss: 0.00149920
Iteration 20/25 | Loss: 0.00149920
Iteration 21/25 | Loss: 0.00149920
Iteration 22/25 | Loss: 0.00149920
Iteration 23/25 | Loss: 0.00149920
Iteration 24/25 | Loss: 0.00149920
Iteration 25/25 | Loss: 0.00149920

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149920
Iteration 2/1000 | Loss: 0.00930962
Iteration 3/1000 | Loss: 0.00397999
Iteration 4/1000 | Loss: 0.00371829
Iteration 5/1000 | Loss: 0.00153412
Iteration 6/1000 | Loss: 0.00142116
Iteration 7/1000 | Loss: 0.00125967
Iteration 8/1000 | Loss: 0.00141230
Iteration 9/1000 | Loss: 0.00130716
Iteration 10/1000 | Loss: 0.00136633
Iteration 11/1000 | Loss: 0.00154901
Iteration 12/1000 | Loss: 0.00180875
Iteration 13/1000 | Loss: 0.00134821
Iteration 14/1000 | Loss: 0.00175085
Iteration 15/1000 | Loss: 0.00102000
Iteration 16/1000 | Loss: 0.00112002
Iteration 17/1000 | Loss: 0.00176955
Iteration 18/1000 | Loss: 0.00064138
Iteration 19/1000 | Loss: 0.00007778
Iteration 20/1000 | Loss: 0.00007917
Iteration 21/1000 | Loss: 0.00005271
Iteration 22/1000 | Loss: 0.00004589
Iteration 23/1000 | Loss: 0.00004143
Iteration 24/1000 | Loss: 0.00003865
Iteration 25/1000 | Loss: 0.00003697
Iteration 26/1000 | Loss: 0.00003568
Iteration 27/1000 | Loss: 0.00003473
Iteration 28/1000 | Loss: 0.00003374
Iteration 29/1000 | Loss: 0.00003312
Iteration 30/1000 | Loss: 0.00003234
Iteration 31/1000 | Loss: 0.00004215
Iteration 32/1000 | Loss: 0.00003565
Iteration 33/1000 | Loss: 0.00003158
Iteration 34/1000 | Loss: 0.00003659
Iteration 35/1000 | Loss: 0.00003496
Iteration 36/1000 | Loss: 0.00003089
Iteration 37/1000 | Loss: 0.00003665
Iteration 38/1000 | Loss: 0.00003575
Iteration 39/1000 | Loss: 0.00003237
Iteration 40/1000 | Loss: 0.00004229
Iteration 41/1000 | Loss: 0.00003457
Iteration 42/1000 | Loss: 0.00004175
Iteration 43/1000 | Loss: 0.00003419
Iteration 44/1000 | Loss: 0.00003202
Iteration 45/1000 | Loss: 0.00003099
Iteration 46/1000 | Loss: 0.00003054
Iteration 47/1000 | Loss: 0.00003010
Iteration 48/1000 | Loss: 0.00002981
Iteration 49/1000 | Loss: 0.00002975
Iteration 50/1000 | Loss: 0.00002969
Iteration 51/1000 | Loss: 0.00002968
Iteration 52/1000 | Loss: 0.00002966
Iteration 53/1000 | Loss: 0.00002966
Iteration 54/1000 | Loss: 0.00002965
Iteration 55/1000 | Loss: 0.00002965
Iteration 56/1000 | Loss: 0.00002963
Iteration 57/1000 | Loss: 0.00002961
Iteration 58/1000 | Loss: 0.00002960
Iteration 59/1000 | Loss: 0.00002960
Iteration 60/1000 | Loss: 0.00002958
Iteration 61/1000 | Loss: 0.00002956
Iteration 62/1000 | Loss: 0.00002945
Iteration 63/1000 | Loss: 0.00002943
Iteration 64/1000 | Loss: 0.00002943
Iteration 65/1000 | Loss: 0.00002941
Iteration 66/1000 | Loss: 0.00002940
Iteration 67/1000 | Loss: 0.00002940
Iteration 68/1000 | Loss: 0.00002940
Iteration 69/1000 | Loss: 0.00002939
Iteration 70/1000 | Loss: 0.00002939
Iteration 71/1000 | Loss: 0.00002939
Iteration 72/1000 | Loss: 0.00002938
Iteration 73/1000 | Loss: 0.00002938
Iteration 74/1000 | Loss: 0.00002937
Iteration 75/1000 | Loss: 0.00002937
Iteration 76/1000 | Loss: 0.00002937
Iteration 77/1000 | Loss: 0.00002936
Iteration 78/1000 | Loss: 0.00002936
Iteration 79/1000 | Loss: 0.00002936
Iteration 80/1000 | Loss: 0.00002936
Iteration 81/1000 | Loss: 0.00002936
Iteration 82/1000 | Loss: 0.00002936
Iteration 83/1000 | Loss: 0.00002936
Iteration 84/1000 | Loss: 0.00002935
Iteration 85/1000 | Loss: 0.00002935
Iteration 86/1000 | Loss: 0.00002935
Iteration 87/1000 | Loss: 0.00002934
Iteration 88/1000 | Loss: 0.00002934
Iteration 89/1000 | Loss: 0.00002934
Iteration 90/1000 | Loss: 0.00002933
Iteration 91/1000 | Loss: 0.00002933
Iteration 92/1000 | Loss: 0.00002933
Iteration 93/1000 | Loss: 0.00002932
Iteration 94/1000 | Loss: 0.00002932
Iteration 95/1000 | Loss: 0.00002932
Iteration 96/1000 | Loss: 0.00002931
Iteration 97/1000 | Loss: 0.00002931
Iteration 98/1000 | Loss: 0.00002931
Iteration 99/1000 | Loss: 0.00002931
Iteration 100/1000 | Loss: 0.00002931
Iteration 101/1000 | Loss: 0.00002930
Iteration 102/1000 | Loss: 0.00002930
Iteration 103/1000 | Loss: 0.00002930
Iteration 104/1000 | Loss: 0.00002930
Iteration 105/1000 | Loss: 0.00002929
Iteration 106/1000 | Loss: 0.00002929
Iteration 107/1000 | Loss: 0.00002929
Iteration 108/1000 | Loss: 0.00002928
Iteration 109/1000 | Loss: 0.00002928
Iteration 110/1000 | Loss: 0.00002928
Iteration 111/1000 | Loss: 0.00002928
Iteration 112/1000 | Loss: 0.00002928
Iteration 113/1000 | Loss: 0.00002927
Iteration 114/1000 | Loss: 0.00002927
Iteration 115/1000 | Loss: 0.00002927
Iteration 116/1000 | Loss: 0.00002927
Iteration 117/1000 | Loss: 0.00002927
Iteration 118/1000 | Loss: 0.00002927
Iteration 119/1000 | Loss: 0.00002927
Iteration 120/1000 | Loss: 0.00002926
Iteration 121/1000 | Loss: 0.00002926
Iteration 122/1000 | Loss: 0.00002926
Iteration 123/1000 | Loss: 0.00002926
Iteration 124/1000 | Loss: 0.00002925
Iteration 125/1000 | Loss: 0.00002925
Iteration 126/1000 | Loss: 0.00002925
Iteration 127/1000 | Loss: 0.00002924
Iteration 128/1000 | Loss: 0.00002924
Iteration 129/1000 | Loss: 0.00002924
Iteration 130/1000 | Loss: 0.00002924
Iteration 131/1000 | Loss: 0.00002923
Iteration 132/1000 | Loss: 0.00002923
Iteration 133/1000 | Loss: 0.00002923
Iteration 134/1000 | Loss: 0.00002923
Iteration 135/1000 | Loss: 0.00002923
Iteration 136/1000 | Loss: 0.00002923
Iteration 137/1000 | Loss: 0.00002922
Iteration 138/1000 | Loss: 0.00002922
Iteration 139/1000 | Loss: 0.00002922
Iteration 140/1000 | Loss: 0.00002922
Iteration 141/1000 | Loss: 0.00002921
Iteration 142/1000 | Loss: 0.00002921
Iteration 143/1000 | Loss: 0.00002921
Iteration 144/1000 | Loss: 0.00002921
Iteration 145/1000 | Loss: 0.00002921
Iteration 146/1000 | Loss: 0.00002921
Iteration 147/1000 | Loss: 0.00002921
Iteration 148/1000 | Loss: 0.00002920
Iteration 149/1000 | Loss: 0.00002920
Iteration 150/1000 | Loss: 0.00002920
Iteration 151/1000 | Loss: 0.00002920
Iteration 152/1000 | Loss: 0.00002920
Iteration 153/1000 | Loss: 0.00002920
Iteration 154/1000 | Loss: 0.00002920
Iteration 155/1000 | Loss: 0.00002920
Iteration 156/1000 | Loss: 0.00002920
Iteration 157/1000 | Loss: 0.00002920
Iteration 158/1000 | Loss: 0.00002920
Iteration 159/1000 | Loss: 0.00002919
Iteration 160/1000 | Loss: 0.00002919
Iteration 161/1000 | Loss: 0.00002919
Iteration 162/1000 | Loss: 0.00002919
Iteration 163/1000 | Loss: 0.00002919
Iteration 164/1000 | Loss: 0.00002919
Iteration 165/1000 | Loss: 0.00002919
Iteration 166/1000 | Loss: 0.00002919
Iteration 167/1000 | Loss: 0.00002919
Iteration 168/1000 | Loss: 0.00002919
Iteration 169/1000 | Loss: 0.00002919
Iteration 170/1000 | Loss: 0.00002919
Iteration 171/1000 | Loss: 0.00002919
Iteration 172/1000 | Loss: 0.00002918
Iteration 173/1000 | Loss: 0.00002918
Iteration 174/1000 | Loss: 0.00002918
Iteration 175/1000 | Loss: 0.00002918
Iteration 176/1000 | Loss: 0.00002918
Iteration 177/1000 | Loss: 0.00002918
Iteration 178/1000 | Loss: 0.00002918
Iteration 179/1000 | Loss: 0.00002918
Iteration 180/1000 | Loss: 0.00002918
Iteration 181/1000 | Loss: 0.00002918
Iteration 182/1000 | Loss: 0.00002918
Iteration 183/1000 | Loss: 0.00002917
Iteration 184/1000 | Loss: 0.00002917
Iteration 185/1000 | Loss: 0.00002917
Iteration 186/1000 | Loss: 0.00002917
Iteration 187/1000 | Loss: 0.00002917
Iteration 188/1000 | Loss: 0.00002917
Iteration 189/1000 | Loss: 0.00002916
Iteration 190/1000 | Loss: 0.00002916
Iteration 191/1000 | Loss: 0.00002916
Iteration 192/1000 | Loss: 0.00002916
Iteration 193/1000 | Loss: 0.00002916
Iteration 194/1000 | Loss: 0.00002916
Iteration 195/1000 | Loss: 0.00002916
Iteration 196/1000 | Loss: 0.00002916
Iteration 197/1000 | Loss: 0.00002915
Iteration 198/1000 | Loss: 0.00002915
Iteration 199/1000 | Loss: 0.00002915
Iteration 200/1000 | Loss: 0.00002915
Iteration 201/1000 | Loss: 0.00002915
Iteration 202/1000 | Loss: 0.00002915
Iteration 203/1000 | Loss: 0.00002915
Iteration 204/1000 | Loss: 0.00002915
Iteration 205/1000 | Loss: 0.00002915
Iteration 206/1000 | Loss: 0.00002915
Iteration 207/1000 | Loss: 0.00002915
Iteration 208/1000 | Loss: 0.00002914
Iteration 209/1000 | Loss: 0.00002914
Iteration 210/1000 | Loss: 0.00002914
Iteration 211/1000 | Loss: 0.00002914
Iteration 212/1000 | Loss: 0.00002914
Iteration 213/1000 | Loss: 0.00002914
Iteration 214/1000 | Loss: 0.00002914
Iteration 215/1000 | Loss: 0.00002914
Iteration 216/1000 | Loss: 0.00002913
Iteration 217/1000 | Loss: 0.00002913
Iteration 218/1000 | Loss: 0.00002913
Iteration 219/1000 | Loss: 0.00002913
Iteration 220/1000 | Loss: 0.00002913
Iteration 221/1000 | Loss: 0.00002913
Iteration 222/1000 | Loss: 0.00002913
Iteration 223/1000 | Loss: 0.00002913
Iteration 224/1000 | Loss: 0.00002913
Iteration 225/1000 | Loss: 0.00002913
Iteration 226/1000 | Loss: 0.00002913
Iteration 227/1000 | Loss: 0.00002913
Iteration 228/1000 | Loss: 0.00002913
Iteration 229/1000 | Loss: 0.00002912
Iteration 230/1000 | Loss: 0.00002912
Iteration 231/1000 | Loss: 0.00002912
Iteration 232/1000 | Loss: 0.00002912
Iteration 233/1000 | Loss: 0.00002912
Iteration 234/1000 | Loss: 0.00002912
Iteration 235/1000 | Loss: 0.00002912
Iteration 236/1000 | Loss: 0.00002912
Iteration 237/1000 | Loss: 0.00002912
Iteration 238/1000 | Loss: 0.00002911
Iteration 239/1000 | Loss: 0.00002911
Iteration 240/1000 | Loss: 0.00002911
Iteration 241/1000 | Loss: 0.00002911
Iteration 242/1000 | Loss: 0.00002911
Iteration 243/1000 | Loss: 0.00002911
Iteration 244/1000 | Loss: 0.00002911
Iteration 245/1000 | Loss: 0.00002910
Iteration 246/1000 | Loss: 0.00002910
Iteration 247/1000 | Loss: 0.00002910
Iteration 248/1000 | Loss: 0.00002910
Iteration 249/1000 | Loss: 0.00002910
Iteration 250/1000 | Loss: 0.00002910
Iteration 251/1000 | Loss: 0.00002910
Iteration 252/1000 | Loss: 0.00002910
Iteration 253/1000 | Loss: 0.00002910
Iteration 254/1000 | Loss: 0.00002910
Iteration 255/1000 | Loss: 0.00002910
Iteration 256/1000 | Loss: 0.00002910
Iteration 257/1000 | Loss: 0.00002910
Iteration 258/1000 | Loss: 0.00002910
Iteration 259/1000 | Loss: 0.00002910
Iteration 260/1000 | Loss: 0.00002910
Iteration 261/1000 | Loss: 0.00002910
Iteration 262/1000 | Loss: 0.00002909
Iteration 263/1000 | Loss: 0.00002909
Iteration 264/1000 | Loss: 0.00002909
Iteration 265/1000 | Loss: 0.00002909
Iteration 266/1000 | Loss: 0.00002909
Iteration 267/1000 | Loss: 0.00002909
Iteration 268/1000 | Loss: 0.00002909
Iteration 269/1000 | Loss: 0.00002908
Iteration 270/1000 | Loss: 0.00002908
Iteration 271/1000 | Loss: 0.00002908
Iteration 272/1000 | Loss: 0.00002908
Iteration 273/1000 | Loss: 0.00002908
Iteration 274/1000 | Loss: 0.00002908
Iteration 275/1000 | Loss: 0.00002908
Iteration 276/1000 | Loss: 0.00002908
Iteration 277/1000 | Loss: 0.00002907
Iteration 278/1000 | Loss: 0.00002907
Iteration 279/1000 | Loss: 0.00002907
Iteration 280/1000 | Loss: 0.00002907
Iteration 281/1000 | Loss: 0.00002907
Iteration 282/1000 | Loss: 0.00002907
Iteration 283/1000 | Loss: 0.00002907
Iteration 284/1000 | Loss: 0.00002907
Iteration 285/1000 | Loss: 0.00002907
Iteration 286/1000 | Loss: 0.00002907
Iteration 287/1000 | Loss: 0.00002906
Iteration 288/1000 | Loss: 0.00002906
Iteration 289/1000 | Loss: 0.00002906
Iteration 290/1000 | Loss: 0.00002906
Iteration 291/1000 | Loss: 0.00002906
Iteration 292/1000 | Loss: 0.00002905
Iteration 293/1000 | Loss: 0.00002905
Iteration 294/1000 | Loss: 0.00002905
Iteration 295/1000 | Loss: 0.00002905
Iteration 296/1000 | Loss: 0.00002905
Iteration 297/1000 | Loss: 0.00002904
Iteration 298/1000 | Loss: 0.00002904
Iteration 299/1000 | Loss: 0.00002904
Iteration 300/1000 | Loss: 0.00002904
Iteration 301/1000 | Loss: 0.00002904
Iteration 302/1000 | Loss: 0.00002904
Iteration 303/1000 | Loss: 0.00002904
Iteration 304/1000 | Loss: 0.00002904
Iteration 305/1000 | Loss: 0.00002903
Iteration 306/1000 | Loss: 0.00002903
Iteration 307/1000 | Loss: 0.00002903
Iteration 308/1000 | Loss: 0.00002903
Iteration 309/1000 | Loss: 0.00002903
Iteration 310/1000 | Loss: 0.00002903
Iteration 311/1000 | Loss: 0.00002903
Iteration 312/1000 | Loss: 0.00002903
Iteration 313/1000 | Loss: 0.00002902
Iteration 314/1000 | Loss: 0.00002902
Iteration 315/1000 | Loss: 0.00002902
Iteration 316/1000 | Loss: 0.00002902
Iteration 317/1000 | Loss: 0.00002902
Iteration 318/1000 | Loss: 0.00002902
Iteration 319/1000 | Loss: 0.00002902
Iteration 320/1000 | Loss: 0.00002902
Iteration 321/1000 | Loss: 0.00002902
Iteration 322/1000 | Loss: 0.00002902
Iteration 323/1000 | Loss: 0.00002902
Iteration 324/1000 | Loss: 0.00002902
Iteration 325/1000 | Loss: 0.00002902
Iteration 326/1000 | Loss: 0.00002902
Iteration 327/1000 | Loss: 0.00002902
Iteration 328/1000 | Loss: 0.00002902
Iteration 329/1000 | Loss: 0.00002902
Iteration 330/1000 | Loss: 0.00002902
Iteration 331/1000 | Loss: 0.00002902
Iteration 332/1000 | Loss: 0.00002902
Iteration 333/1000 | Loss: 0.00002902
Iteration 334/1000 | Loss: 0.00002902
Iteration 335/1000 | Loss: 0.00002902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [2.901598418247886e-05, 2.901598418247886e-05, 2.901598418247886e-05, 2.901598418247886e-05, 2.901598418247886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.901598418247886e-05

Optimization complete. Final v2v error: 4.351663112640381 mm

Highest mean error: 6.180543422698975 mm for frame 123

Lowest mean error: 2.9745030403137207 mm for frame 0

Saving results

Total time: 144.26285600662231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882511
Iteration 2/25 | Loss: 0.00153699
Iteration 3/25 | Loss: 0.00118818
Iteration 4/25 | Loss: 0.00114488
Iteration 5/25 | Loss: 0.00113332
Iteration 6/25 | Loss: 0.00113641
Iteration 7/25 | Loss: 0.00111347
Iteration 8/25 | Loss: 0.00106607
Iteration 9/25 | Loss: 0.00105571
Iteration 10/25 | Loss: 0.00105421
Iteration 11/25 | Loss: 0.00105404
Iteration 12/25 | Loss: 0.00105400
Iteration 13/25 | Loss: 0.00105400
Iteration 14/25 | Loss: 0.00105400
Iteration 15/25 | Loss: 0.00105400
Iteration 16/25 | Loss: 0.00105400
Iteration 17/25 | Loss: 0.00105400
Iteration 18/25 | Loss: 0.00105400
Iteration 19/25 | Loss: 0.00105400
Iteration 20/25 | Loss: 0.00105400
Iteration 21/25 | Loss: 0.00105400
Iteration 22/25 | Loss: 0.00105400
Iteration 23/25 | Loss: 0.00105400
Iteration 24/25 | Loss: 0.00105399
Iteration 25/25 | Loss: 0.00105399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90840787
Iteration 2/25 | Loss: 0.00032156
Iteration 3/25 | Loss: 0.00032155
Iteration 4/25 | Loss: 0.00032155
Iteration 5/25 | Loss: 0.00032155
Iteration 6/25 | Loss: 0.00032155
Iteration 7/25 | Loss: 0.00032155
Iteration 8/25 | Loss: 0.00032155
Iteration 9/25 | Loss: 0.00032155
Iteration 10/25 | Loss: 0.00032155
Iteration 11/25 | Loss: 0.00032155
Iteration 12/25 | Loss: 0.00032155
Iteration 13/25 | Loss: 0.00032155
Iteration 14/25 | Loss: 0.00032155
Iteration 15/25 | Loss: 0.00032155
Iteration 16/25 | Loss: 0.00032155
Iteration 17/25 | Loss: 0.00032155
Iteration 18/25 | Loss: 0.00032155
Iteration 19/25 | Loss: 0.00032155
Iteration 20/25 | Loss: 0.00032155
Iteration 21/25 | Loss: 0.00032155
Iteration 22/25 | Loss: 0.00032155
Iteration 23/25 | Loss: 0.00032155
Iteration 24/25 | Loss: 0.00032155
Iteration 25/25 | Loss: 0.00032155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032155
Iteration 2/1000 | Loss: 0.00004688
Iteration 3/1000 | Loss: 0.00003505
Iteration 4/1000 | Loss: 0.00003054
Iteration 5/1000 | Loss: 0.00002860
Iteration 6/1000 | Loss: 0.00002713
Iteration 7/1000 | Loss: 0.00002638
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002552
Iteration 10/1000 | Loss: 0.00002532
Iteration 11/1000 | Loss: 0.00002508
Iteration 12/1000 | Loss: 0.00002501
Iteration 13/1000 | Loss: 0.00002501
Iteration 14/1000 | Loss: 0.00002500
Iteration 15/1000 | Loss: 0.00002494
Iteration 16/1000 | Loss: 0.00002484
Iteration 17/1000 | Loss: 0.00002479
Iteration 18/1000 | Loss: 0.00002479
Iteration 19/1000 | Loss: 0.00002479
Iteration 20/1000 | Loss: 0.00002479
Iteration 21/1000 | Loss: 0.00002479
Iteration 22/1000 | Loss: 0.00002479
Iteration 23/1000 | Loss: 0.00002479
Iteration 24/1000 | Loss: 0.00002478
Iteration 25/1000 | Loss: 0.00002478
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002478
Iteration 28/1000 | Loss: 0.00002477
Iteration 29/1000 | Loss: 0.00002477
Iteration 30/1000 | Loss: 0.00002477
Iteration 31/1000 | Loss: 0.00002477
Iteration 32/1000 | Loss: 0.00002477
Iteration 33/1000 | Loss: 0.00002477
Iteration 34/1000 | Loss: 0.00002477
Iteration 35/1000 | Loss: 0.00002477
Iteration 36/1000 | Loss: 0.00002477
Iteration 37/1000 | Loss: 0.00002477
Iteration 38/1000 | Loss: 0.00002477
Iteration 39/1000 | Loss: 0.00002477
Iteration 40/1000 | Loss: 0.00002476
Iteration 41/1000 | Loss: 0.00002476
Iteration 42/1000 | Loss: 0.00002476
Iteration 43/1000 | Loss: 0.00002476
Iteration 44/1000 | Loss: 0.00002476
Iteration 45/1000 | Loss: 0.00002476
Iteration 46/1000 | Loss: 0.00002475
Iteration 47/1000 | Loss: 0.00002475
Iteration 48/1000 | Loss: 0.00002475
Iteration 49/1000 | Loss: 0.00002474
Iteration 50/1000 | Loss: 0.00002474
Iteration 51/1000 | Loss: 0.00002474
Iteration 52/1000 | Loss: 0.00002474
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002473
Iteration 55/1000 | Loss: 0.00002473
Iteration 56/1000 | Loss: 0.00002473
Iteration 57/1000 | Loss: 0.00002473
Iteration 58/1000 | Loss: 0.00002473
Iteration 59/1000 | Loss: 0.00002473
Iteration 60/1000 | Loss: 0.00002473
Iteration 61/1000 | Loss: 0.00002473
Iteration 62/1000 | Loss: 0.00002473
Iteration 63/1000 | Loss: 0.00002473
Iteration 64/1000 | Loss: 0.00002473
Iteration 65/1000 | Loss: 0.00002473
Iteration 66/1000 | Loss: 0.00002473
Iteration 67/1000 | Loss: 0.00002473
Iteration 68/1000 | Loss: 0.00002473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.4732473320909776e-05, 2.4732473320909776e-05, 2.4732473320909776e-05, 2.4732473320909776e-05, 2.4732473320909776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4732473320909776e-05

Optimization complete. Final v2v error: 4.1546549797058105 mm

Highest mean error: 4.3088297843933105 mm for frame 84

Lowest mean error: 4.007884502410889 mm for frame 3

Saving results

Total time: 42.28710627555847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864102
Iteration 2/25 | Loss: 0.00126307
Iteration 3/25 | Loss: 0.00107315
Iteration 4/25 | Loss: 0.00105720
Iteration 5/25 | Loss: 0.00105371
Iteration 6/25 | Loss: 0.00105294
Iteration 7/25 | Loss: 0.00105282
Iteration 8/25 | Loss: 0.00105282
Iteration 9/25 | Loss: 0.00105282
Iteration 10/25 | Loss: 0.00105282
Iteration 11/25 | Loss: 0.00105282
Iteration 12/25 | Loss: 0.00105282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010528224520385265, 0.0010528224520385265, 0.0010528224520385265, 0.0010528224520385265, 0.0010528224520385265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010528224520385265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26877427
Iteration 2/25 | Loss: 0.00044946
Iteration 3/25 | Loss: 0.00044942
Iteration 4/25 | Loss: 0.00044942
Iteration 5/25 | Loss: 0.00044942
Iteration 6/25 | Loss: 0.00044942
Iteration 7/25 | Loss: 0.00044942
Iteration 8/25 | Loss: 0.00044942
Iteration 9/25 | Loss: 0.00044942
Iteration 10/25 | Loss: 0.00044942
Iteration 11/25 | Loss: 0.00044942
Iteration 12/25 | Loss: 0.00044942
Iteration 13/25 | Loss: 0.00044942
Iteration 14/25 | Loss: 0.00044942
Iteration 15/25 | Loss: 0.00044942
Iteration 16/25 | Loss: 0.00044942
Iteration 17/25 | Loss: 0.00044942
Iteration 18/25 | Loss: 0.00044942
Iteration 19/25 | Loss: 0.00044942
Iteration 20/25 | Loss: 0.00044942
Iteration 21/25 | Loss: 0.00044942
Iteration 22/25 | Loss: 0.00044942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004494197783060372, 0.0004494197783060372, 0.0004494197783060372, 0.0004494197783060372, 0.0004494197783060372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004494197783060372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044942
Iteration 2/1000 | Loss: 0.00004978
Iteration 3/1000 | Loss: 0.00003418
Iteration 4/1000 | Loss: 0.00002568
Iteration 5/1000 | Loss: 0.00002256
Iteration 6/1000 | Loss: 0.00002104
Iteration 7/1000 | Loss: 0.00002007
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001895
Iteration 10/1000 | Loss: 0.00001857
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001834
Iteration 13/1000 | Loss: 0.00001829
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001819
Iteration 17/1000 | Loss: 0.00001813
Iteration 18/1000 | Loss: 0.00001807
Iteration 19/1000 | Loss: 0.00001803
Iteration 20/1000 | Loss: 0.00001803
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001802
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001796
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001792
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001788
Iteration 30/1000 | Loss: 0.00001788
Iteration 31/1000 | Loss: 0.00001787
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001784
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001783
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001781
Iteration 48/1000 | Loss: 0.00001781
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001780
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001780
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.7793818187783472e-05, 1.7793818187783472e-05, 1.7793818187783472e-05, 1.7793818187783472e-05, 1.7793818187783472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7793818187783472e-05

Optimization complete. Final v2v error: 3.5453691482543945 mm

Highest mean error: 4.030388355255127 mm for frame 83

Lowest mean error: 2.9291515350341797 mm for frame 21

Saving results

Total time: 33.05970239639282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456592
Iteration 2/25 | Loss: 0.00114200
Iteration 3/25 | Loss: 0.00100278
Iteration 4/25 | Loss: 0.00099567
Iteration 5/25 | Loss: 0.00099347
Iteration 6/25 | Loss: 0.00099303
Iteration 7/25 | Loss: 0.00099303
Iteration 8/25 | Loss: 0.00099303
Iteration 9/25 | Loss: 0.00099303
Iteration 10/25 | Loss: 0.00099303
Iteration 11/25 | Loss: 0.00099303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000993029330857098, 0.000993029330857098, 0.000993029330857098, 0.000993029330857098, 0.000993029330857098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000993029330857098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36332405
Iteration 2/25 | Loss: 0.00065401
Iteration 3/25 | Loss: 0.00065401
Iteration 4/25 | Loss: 0.00065401
Iteration 5/25 | Loss: 0.00065401
Iteration 6/25 | Loss: 0.00065401
Iteration 7/25 | Loss: 0.00065401
Iteration 8/25 | Loss: 0.00065401
Iteration 9/25 | Loss: 0.00065401
Iteration 10/25 | Loss: 0.00065401
Iteration 11/25 | Loss: 0.00065401
Iteration 12/25 | Loss: 0.00065401
Iteration 13/25 | Loss: 0.00065401
Iteration 14/25 | Loss: 0.00065401
Iteration 15/25 | Loss: 0.00065401
Iteration 16/25 | Loss: 0.00065401
Iteration 17/25 | Loss: 0.00065401
Iteration 18/25 | Loss: 0.00065401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006540077738463879, 0.0006540077738463879, 0.0006540077738463879, 0.0006540077738463879, 0.0006540077738463879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006540077738463879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065401
Iteration 2/1000 | Loss: 0.00002197
Iteration 3/1000 | Loss: 0.00001557
Iteration 4/1000 | Loss: 0.00001389
Iteration 5/1000 | Loss: 0.00001331
Iteration 6/1000 | Loss: 0.00001296
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001261
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001260
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001251
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001244
Iteration 49/1000 | Loss: 0.00001244
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001243
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001242
Iteration 58/1000 | Loss: 0.00001242
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001239
Iteration 64/1000 | Loss: 0.00001239
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001234
Iteration 78/1000 | Loss: 0.00001234
Iteration 79/1000 | Loss: 0.00001234
Iteration 80/1000 | Loss: 0.00001234
Iteration 81/1000 | Loss: 0.00001233
Iteration 82/1000 | Loss: 0.00001233
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001232
Iteration 90/1000 | Loss: 0.00001232
Iteration 91/1000 | Loss: 0.00001232
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001231
Iteration 94/1000 | Loss: 0.00001231
Iteration 95/1000 | Loss: 0.00001231
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001229
Iteration 120/1000 | Loss: 0.00001229
Iteration 121/1000 | Loss: 0.00001229
Iteration 122/1000 | Loss: 0.00001229
Iteration 123/1000 | Loss: 0.00001229
Iteration 124/1000 | Loss: 0.00001229
Iteration 125/1000 | Loss: 0.00001229
Iteration 126/1000 | Loss: 0.00001229
Iteration 127/1000 | Loss: 0.00001229
Iteration 128/1000 | Loss: 0.00001229
Iteration 129/1000 | Loss: 0.00001229
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001229
Iteration 144/1000 | Loss: 0.00001229
Iteration 145/1000 | Loss: 0.00001229
Iteration 146/1000 | Loss: 0.00001229
Iteration 147/1000 | Loss: 0.00001229
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001229
Iteration 151/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.229046938533429e-05, 1.229046938533429e-05, 1.229046938533429e-05, 1.229046938533429e-05, 1.229046938533429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.229046938533429e-05

Optimization complete. Final v2v error: 2.8782875537872314 mm

Highest mean error: 3.1913840770721436 mm for frame 21

Lowest mean error: 2.477635383605957 mm for frame 4

Saving results

Total time: 29.281110763549805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011458
Iteration 2/25 | Loss: 0.00170515
Iteration 3/25 | Loss: 0.00123504
Iteration 4/25 | Loss: 0.00118688
Iteration 5/25 | Loss: 0.00118225
Iteration 6/25 | Loss: 0.00118203
Iteration 7/25 | Loss: 0.00118203
Iteration 8/25 | Loss: 0.00118203
Iteration 9/25 | Loss: 0.00118203
Iteration 10/25 | Loss: 0.00118203
Iteration 11/25 | Loss: 0.00118203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001182034146040678, 0.001182034146040678, 0.001182034146040678, 0.001182034146040678, 0.001182034146040678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001182034146040678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.85915518
Iteration 2/25 | Loss: 0.00051996
Iteration 3/25 | Loss: 0.00051991
Iteration 4/25 | Loss: 0.00051991
Iteration 5/25 | Loss: 0.00051991
Iteration 6/25 | Loss: 0.00051991
Iteration 7/25 | Loss: 0.00051991
Iteration 8/25 | Loss: 0.00051991
Iteration 9/25 | Loss: 0.00051991
Iteration 10/25 | Loss: 0.00051991
Iteration 11/25 | Loss: 0.00051991
Iteration 12/25 | Loss: 0.00051991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005199117586016655, 0.0005199117586016655, 0.0005199117586016655, 0.0005199117586016655, 0.0005199117586016655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005199117586016655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051991
Iteration 2/1000 | Loss: 0.00007882
Iteration 3/1000 | Loss: 0.00005378
Iteration 4/1000 | Loss: 0.00004722
Iteration 5/1000 | Loss: 0.00004530
Iteration 6/1000 | Loss: 0.00004446
Iteration 7/1000 | Loss: 0.00004380
Iteration 8/1000 | Loss: 0.00004316
Iteration 9/1000 | Loss: 0.00004259
Iteration 10/1000 | Loss: 0.00004223
Iteration 11/1000 | Loss: 0.00004185
Iteration 12/1000 | Loss: 0.00004137
Iteration 13/1000 | Loss: 0.00004095
Iteration 14/1000 | Loss: 0.00004048
Iteration 15/1000 | Loss: 0.00004007
Iteration 16/1000 | Loss: 0.00003974
Iteration 17/1000 | Loss: 0.00003940
Iteration 18/1000 | Loss: 0.00003910
Iteration 19/1000 | Loss: 0.00003889
Iteration 20/1000 | Loss: 0.00003875
Iteration 21/1000 | Loss: 0.00003865
Iteration 22/1000 | Loss: 0.00003862
Iteration 23/1000 | Loss: 0.00003858
Iteration 24/1000 | Loss: 0.00003856
Iteration 25/1000 | Loss: 0.00003855
Iteration 26/1000 | Loss: 0.00003853
Iteration 27/1000 | Loss: 0.00003852
Iteration 28/1000 | Loss: 0.00003850
Iteration 29/1000 | Loss: 0.00003849
Iteration 30/1000 | Loss: 0.00003846
Iteration 31/1000 | Loss: 0.00003846
Iteration 32/1000 | Loss: 0.00003846
Iteration 33/1000 | Loss: 0.00003846
Iteration 34/1000 | Loss: 0.00003846
Iteration 35/1000 | Loss: 0.00003846
Iteration 36/1000 | Loss: 0.00003846
Iteration 37/1000 | Loss: 0.00003844
Iteration 38/1000 | Loss: 0.00003838
Iteration 39/1000 | Loss: 0.00003837
Iteration 40/1000 | Loss: 0.00003836
Iteration 41/1000 | Loss: 0.00003834
Iteration 42/1000 | Loss: 0.00003834
Iteration 43/1000 | Loss: 0.00003834
Iteration 44/1000 | Loss: 0.00003833
Iteration 45/1000 | Loss: 0.00003833
Iteration 46/1000 | Loss: 0.00003833
Iteration 47/1000 | Loss: 0.00003833
Iteration 48/1000 | Loss: 0.00003833
Iteration 49/1000 | Loss: 0.00003833
Iteration 50/1000 | Loss: 0.00003833
Iteration 51/1000 | Loss: 0.00003833
Iteration 52/1000 | Loss: 0.00003833
Iteration 53/1000 | Loss: 0.00003833
Iteration 54/1000 | Loss: 0.00003833
Iteration 55/1000 | Loss: 0.00003833
Iteration 56/1000 | Loss: 0.00003833
Iteration 57/1000 | Loss: 0.00003833
Iteration 58/1000 | Loss: 0.00003833
Iteration 59/1000 | Loss: 0.00003833
Iteration 60/1000 | Loss: 0.00003833
Iteration 61/1000 | Loss: 0.00003833
Iteration 62/1000 | Loss: 0.00003833
Iteration 63/1000 | Loss: 0.00003833
Iteration 64/1000 | Loss: 0.00003833
Iteration 65/1000 | Loss: 0.00003833
Iteration 66/1000 | Loss: 0.00003833
Iteration 67/1000 | Loss: 0.00003833
Iteration 68/1000 | Loss: 0.00003833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [3.8330912502715364e-05, 3.8330912502715364e-05, 3.8330912502715364e-05, 3.8330912502715364e-05, 3.8330912502715364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8330912502715364e-05

Optimization complete. Final v2v error: 4.819623947143555 mm

Highest mean error: 6.010357856750488 mm for frame 0

Lowest mean error: 3.6454873085021973 mm for frame 20

Saving results

Total time: 50.091875314712524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_2234/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_2234/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833919
Iteration 2/25 | Loss: 0.00120897
Iteration 3/25 | Loss: 0.00106117
Iteration 4/25 | Loss: 0.00103764
Iteration 5/25 | Loss: 0.00102961
Iteration 6/25 | Loss: 0.00102686
Iteration 7/25 | Loss: 0.00102610
Iteration 8/25 | Loss: 0.00102610
Iteration 9/25 | Loss: 0.00102610
Iteration 10/25 | Loss: 0.00102610
Iteration 11/25 | Loss: 0.00102610
Iteration 12/25 | Loss: 0.00102610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010260973358526826, 0.0010260973358526826, 0.0010260973358526826, 0.0010260973358526826, 0.0010260973358526826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010260973358526826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34204853
Iteration 2/25 | Loss: 0.00086798
Iteration 3/25 | Loss: 0.00086798
Iteration 4/25 | Loss: 0.00086798
Iteration 5/25 | Loss: 0.00086798
Iteration 6/25 | Loss: 0.00086797
Iteration 7/25 | Loss: 0.00086797
Iteration 8/25 | Loss: 0.00086797
Iteration 9/25 | Loss: 0.00086797
Iteration 10/25 | Loss: 0.00086797
Iteration 11/25 | Loss: 0.00086797
Iteration 12/25 | Loss: 0.00086797
Iteration 13/25 | Loss: 0.00086797
Iteration 14/25 | Loss: 0.00086797
Iteration 15/25 | Loss: 0.00086797
Iteration 16/25 | Loss: 0.00086797
Iteration 17/25 | Loss: 0.00086797
Iteration 18/25 | Loss: 0.00086797
Iteration 19/25 | Loss: 0.00086797
Iteration 20/25 | Loss: 0.00086797
Iteration 21/25 | Loss: 0.00086797
Iteration 22/25 | Loss: 0.00086797
Iteration 23/25 | Loss: 0.00086797
Iteration 24/25 | Loss: 0.00086797
Iteration 25/25 | Loss: 0.00086797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086797
Iteration 2/1000 | Loss: 0.00004146
Iteration 3/1000 | Loss: 0.00002806
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002175
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002018
Iteration 8/1000 | Loss: 0.00001971
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001918
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001910
Iteration 13/1000 | Loss: 0.00001902
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001894
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001882
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001881
Iteration 24/1000 | Loss: 0.00001880
Iteration 25/1000 | Loss: 0.00001879
Iteration 26/1000 | Loss: 0.00001878
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001869
Iteration 37/1000 | Loss: 0.00001869
Iteration 38/1000 | Loss: 0.00001868
Iteration 39/1000 | Loss: 0.00001868
Iteration 40/1000 | Loss: 0.00001868
Iteration 41/1000 | Loss: 0.00001868
Iteration 42/1000 | Loss: 0.00001868
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001867
Iteration 53/1000 | Loss: 0.00001867
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001865
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001865
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001862
Iteration 72/1000 | Loss: 0.00001862
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001860
Iteration 86/1000 | Loss: 0.00001860
Iteration 87/1000 | Loss: 0.00001860
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001859
Iteration 90/1000 | Loss: 0.00001859
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001858
Iteration 93/1000 | Loss: 0.00001858
Iteration 94/1000 | Loss: 0.00001858
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001858
Iteration 100/1000 | Loss: 0.00001858
Iteration 101/1000 | Loss: 0.00001857
Iteration 102/1000 | Loss: 0.00001857
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001856
Iteration 105/1000 | Loss: 0.00001856
Iteration 106/1000 | Loss: 0.00001856
Iteration 107/1000 | Loss: 0.00001856
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001856
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001853
Iteration 126/1000 | Loss: 0.00001853
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001852
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001851
Iteration 140/1000 | Loss: 0.00001851
Iteration 141/1000 | Loss: 0.00001851
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001851
Iteration 144/1000 | Loss: 0.00001851
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001851
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001849
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001848
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001848
Iteration 172/1000 | Loss: 0.00001848
Iteration 173/1000 | Loss: 0.00001848
Iteration 174/1000 | Loss: 0.00001848
Iteration 175/1000 | Loss: 0.00001848
Iteration 176/1000 | Loss: 0.00001848
Iteration 177/1000 | Loss: 0.00001847
Iteration 178/1000 | Loss: 0.00001847
Iteration 179/1000 | Loss: 0.00001847
Iteration 180/1000 | Loss: 0.00001847
Iteration 181/1000 | Loss: 0.00001847
Iteration 182/1000 | Loss: 0.00001847
Iteration 183/1000 | Loss: 0.00001847
Iteration 184/1000 | Loss: 0.00001847
Iteration 185/1000 | Loss: 0.00001846
Iteration 186/1000 | Loss: 0.00001846
Iteration 187/1000 | Loss: 0.00001846
Iteration 188/1000 | Loss: 0.00001846
Iteration 189/1000 | Loss: 0.00001846
Iteration 190/1000 | Loss: 0.00001846
Iteration 191/1000 | Loss: 0.00001846
Iteration 192/1000 | Loss: 0.00001846
Iteration 193/1000 | Loss: 0.00001845
Iteration 194/1000 | Loss: 0.00001845
Iteration 195/1000 | Loss: 0.00001845
Iteration 196/1000 | Loss: 0.00001845
Iteration 197/1000 | Loss: 0.00001845
Iteration 198/1000 | Loss: 0.00001845
Iteration 199/1000 | Loss: 0.00001845
Iteration 200/1000 | Loss: 0.00001845
Iteration 201/1000 | Loss: 0.00001845
Iteration 202/1000 | Loss: 0.00001845
Iteration 203/1000 | Loss: 0.00001845
Iteration 204/1000 | Loss: 0.00001845
Iteration 205/1000 | Loss: 0.00001845
Iteration 206/1000 | Loss: 0.00001845
Iteration 207/1000 | Loss: 0.00001845
Iteration 208/1000 | Loss: 0.00001845
Iteration 209/1000 | Loss: 0.00001845
Iteration 210/1000 | Loss: 0.00001845
Iteration 211/1000 | Loss: 0.00001845
Iteration 212/1000 | Loss: 0.00001845
Iteration 213/1000 | Loss: 0.00001844
Iteration 214/1000 | Loss: 0.00001844
Iteration 215/1000 | Loss: 0.00001844
Iteration 216/1000 | Loss: 0.00001844
Iteration 217/1000 | Loss: 0.00001844
Iteration 218/1000 | Loss: 0.00001844
Iteration 219/1000 | Loss: 0.00001844
Iteration 220/1000 | Loss: 0.00001844
Iteration 221/1000 | Loss: 0.00001844
Iteration 222/1000 | Loss: 0.00001844
Iteration 223/1000 | Loss: 0.00001844
Iteration 224/1000 | Loss: 0.00001844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.8443422959535383e-05, 1.8443422959535383e-05, 1.8443422959535383e-05, 1.8443422959535383e-05, 1.8443422959535383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8443422959535383e-05

Optimization complete. Final v2v error: 3.496600866317749 mm

Highest mean error: 4.255312442779541 mm for frame 65

Lowest mean error: 2.760651111602783 mm for frame 2

Saving results

Total time: 43.97160887718201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985350
Iteration 2/25 | Loss: 0.00244818
Iteration 3/25 | Loss: 0.00164178
Iteration 4/25 | Loss: 0.00149748
Iteration 5/25 | Loss: 0.00150009
Iteration 6/25 | Loss: 0.00142955
Iteration 7/25 | Loss: 0.00139233
Iteration 8/25 | Loss: 0.00129582
Iteration 9/25 | Loss: 0.00124740
Iteration 10/25 | Loss: 0.00122714
Iteration 11/25 | Loss: 0.00121814
Iteration 12/25 | Loss: 0.00124085
Iteration 13/25 | Loss: 0.00123214
Iteration 14/25 | Loss: 0.00120296
Iteration 15/25 | Loss: 0.00120345
Iteration 16/25 | Loss: 0.00121586
Iteration 17/25 | Loss: 0.00119573
Iteration 18/25 | Loss: 0.00118036
Iteration 19/25 | Loss: 0.00118059
Iteration 20/25 | Loss: 0.00116763
Iteration 21/25 | Loss: 0.00115191
Iteration 22/25 | Loss: 0.00113866
Iteration 23/25 | Loss: 0.00112765
Iteration 24/25 | Loss: 0.00112346
Iteration 25/25 | Loss: 0.00112250

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09708321
Iteration 2/25 | Loss: 0.00193765
Iteration 3/25 | Loss: 0.00193764
Iteration 4/25 | Loss: 0.00193764
Iteration 5/25 | Loss: 0.00193764
Iteration 6/25 | Loss: 0.00193764
Iteration 7/25 | Loss: 0.00193764
Iteration 8/25 | Loss: 0.00193764
Iteration 9/25 | Loss: 0.00193764
Iteration 10/25 | Loss: 0.00193764
Iteration 11/25 | Loss: 0.00193764
Iteration 12/25 | Loss: 0.00193764
Iteration 13/25 | Loss: 0.00193764
Iteration 14/25 | Loss: 0.00193764
Iteration 15/25 | Loss: 0.00193764
Iteration 16/25 | Loss: 0.00193764
Iteration 17/25 | Loss: 0.00193764
Iteration 18/25 | Loss: 0.00193764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019376360578462481, 0.0019376360578462481, 0.0019376360578462481, 0.0019376360578462481, 0.0019376360578462481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019376360578462481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193764
Iteration 2/1000 | Loss: 0.00018517
Iteration 3/1000 | Loss: 0.00012666
Iteration 4/1000 | Loss: 0.00010423
Iteration 5/1000 | Loss: 0.00031365
Iteration 6/1000 | Loss: 0.00079473
Iteration 7/1000 | Loss: 0.00008614
Iteration 8/1000 | Loss: 0.00007547
Iteration 9/1000 | Loss: 0.00015750
Iteration 10/1000 | Loss: 0.00007891
Iteration 11/1000 | Loss: 0.00006736
Iteration 12/1000 | Loss: 0.00006564
Iteration 13/1000 | Loss: 0.00044820
Iteration 14/1000 | Loss: 0.00140634
Iteration 15/1000 | Loss: 0.00016603
Iteration 16/1000 | Loss: 0.00009382
Iteration 17/1000 | Loss: 0.00008259
Iteration 18/1000 | Loss: 0.00082401
Iteration 19/1000 | Loss: 0.00043703
Iteration 20/1000 | Loss: 0.00096098
Iteration 21/1000 | Loss: 0.00015982
Iteration 22/1000 | Loss: 0.00017598
Iteration 23/1000 | Loss: 0.00007405
Iteration 24/1000 | Loss: 0.00006873
Iteration 25/1000 | Loss: 0.00059409
Iteration 26/1000 | Loss: 0.00060193
Iteration 27/1000 | Loss: 0.00038261
Iteration 28/1000 | Loss: 0.00022093
Iteration 29/1000 | Loss: 0.00014174
Iteration 30/1000 | Loss: 0.00006396
Iteration 31/1000 | Loss: 0.00040365
Iteration 32/1000 | Loss: 0.00018582
Iteration 33/1000 | Loss: 0.00006746
Iteration 34/1000 | Loss: 0.00006196
Iteration 35/1000 | Loss: 0.00007406
Iteration 36/1000 | Loss: 0.00006165
Iteration 37/1000 | Loss: 0.00006294
Iteration 38/1000 | Loss: 0.00005678
Iteration 39/1000 | Loss: 0.00006499
Iteration 40/1000 | Loss: 0.00063269
Iteration 41/1000 | Loss: 0.00007626
Iteration 42/1000 | Loss: 0.00045227
Iteration 43/1000 | Loss: 0.00007952
Iteration 44/1000 | Loss: 0.00036210
Iteration 45/1000 | Loss: 0.00006984
Iteration 46/1000 | Loss: 0.00007977
Iteration 47/1000 | Loss: 0.00039589
Iteration 48/1000 | Loss: 0.00009425
Iteration 49/1000 | Loss: 0.00006560
Iteration 50/1000 | Loss: 0.00007988
Iteration 51/1000 | Loss: 0.00005732
Iteration 52/1000 | Loss: 0.00005547
Iteration 53/1000 | Loss: 0.00008036
Iteration 54/1000 | Loss: 0.00005617
Iteration 55/1000 | Loss: 0.00005355
Iteration 56/1000 | Loss: 0.00005264
Iteration 57/1000 | Loss: 0.00007201
Iteration 58/1000 | Loss: 0.00005203
Iteration 59/1000 | Loss: 0.00039617
Iteration 60/1000 | Loss: 0.00018361
Iteration 61/1000 | Loss: 0.00005367
Iteration 62/1000 | Loss: 0.00005263
Iteration 63/1000 | Loss: 0.00035658
Iteration 64/1000 | Loss: 0.00019646
Iteration 65/1000 | Loss: 0.00005262
Iteration 66/1000 | Loss: 0.00031217
Iteration 67/1000 | Loss: 0.00051574
Iteration 68/1000 | Loss: 0.00007065
Iteration 69/1000 | Loss: 0.00110978
Iteration 70/1000 | Loss: 0.00028729
Iteration 71/1000 | Loss: 0.00012420
Iteration 72/1000 | Loss: 0.00007292
Iteration 73/1000 | Loss: 0.00007458
Iteration 74/1000 | Loss: 0.00006080
Iteration 75/1000 | Loss: 0.00005565
Iteration 76/1000 | Loss: 0.00005200
Iteration 77/1000 | Loss: 0.00081118
Iteration 78/1000 | Loss: 0.00039198
Iteration 79/1000 | Loss: 0.00072645
Iteration 80/1000 | Loss: 0.00048242
Iteration 81/1000 | Loss: 0.00061578
Iteration 82/1000 | Loss: 0.00034847
Iteration 83/1000 | Loss: 0.00053392
Iteration 84/1000 | Loss: 0.00037189
Iteration 85/1000 | Loss: 0.00008278
Iteration 86/1000 | Loss: 0.00048539
Iteration 87/1000 | Loss: 0.00016878
Iteration 88/1000 | Loss: 0.00006724
Iteration 89/1000 | Loss: 0.00009601
Iteration 90/1000 | Loss: 0.00073955
Iteration 91/1000 | Loss: 0.00008823
Iteration 92/1000 | Loss: 0.00005131
Iteration 93/1000 | Loss: 0.00004668
Iteration 94/1000 | Loss: 0.00004468
Iteration 95/1000 | Loss: 0.00004347
Iteration 96/1000 | Loss: 0.00004266
Iteration 97/1000 | Loss: 0.00004214
Iteration 98/1000 | Loss: 0.00004185
Iteration 99/1000 | Loss: 0.00062775
Iteration 100/1000 | Loss: 0.00032246
Iteration 101/1000 | Loss: 0.00004173
Iteration 102/1000 | Loss: 0.00004130
Iteration 103/1000 | Loss: 0.00004128
Iteration 104/1000 | Loss: 0.00060439
Iteration 105/1000 | Loss: 0.00226002
Iteration 106/1000 | Loss: 0.00158325
Iteration 107/1000 | Loss: 0.00036971
Iteration 108/1000 | Loss: 0.00063491
Iteration 109/1000 | Loss: 0.00027970
Iteration 110/1000 | Loss: 0.00074543
Iteration 111/1000 | Loss: 0.00014373
Iteration 112/1000 | Loss: 0.00006964
Iteration 113/1000 | Loss: 0.00005736
Iteration 114/1000 | Loss: 0.00024533
Iteration 115/1000 | Loss: 0.00005084
Iteration 116/1000 | Loss: 0.00004581
Iteration 117/1000 | Loss: 0.00004264
Iteration 118/1000 | Loss: 0.00004111
Iteration 119/1000 | Loss: 0.00004034
Iteration 120/1000 | Loss: 0.00003971
Iteration 121/1000 | Loss: 0.00003901
Iteration 122/1000 | Loss: 0.00003836
Iteration 123/1000 | Loss: 0.00003763
Iteration 124/1000 | Loss: 0.00003715
Iteration 125/1000 | Loss: 0.00003684
Iteration 126/1000 | Loss: 0.00003674
Iteration 127/1000 | Loss: 0.00003651
Iteration 128/1000 | Loss: 0.00003625
Iteration 129/1000 | Loss: 0.00003590
Iteration 130/1000 | Loss: 0.00003567
Iteration 131/1000 | Loss: 0.00003549
Iteration 132/1000 | Loss: 0.00003549
Iteration 133/1000 | Loss: 0.00003546
Iteration 134/1000 | Loss: 0.00003540
Iteration 135/1000 | Loss: 0.00062222
Iteration 136/1000 | Loss: 0.00056049
Iteration 137/1000 | Loss: 0.00064361
Iteration 138/1000 | Loss: 0.00024205
Iteration 139/1000 | Loss: 0.00004213
Iteration 140/1000 | Loss: 0.00025992
Iteration 141/1000 | Loss: 0.00003560
Iteration 142/1000 | Loss: 0.00003457
Iteration 143/1000 | Loss: 0.00020774
Iteration 144/1000 | Loss: 0.00004105
Iteration 145/1000 | Loss: 0.00003568
Iteration 146/1000 | Loss: 0.00003283
Iteration 147/1000 | Loss: 0.00003149
Iteration 148/1000 | Loss: 0.00003067
Iteration 149/1000 | Loss: 0.00003022
Iteration 150/1000 | Loss: 0.00002996
Iteration 151/1000 | Loss: 0.00002969
Iteration 152/1000 | Loss: 0.00002956
Iteration 153/1000 | Loss: 0.00002956
Iteration 154/1000 | Loss: 0.00002956
Iteration 155/1000 | Loss: 0.00002941
Iteration 156/1000 | Loss: 0.00002939
Iteration 157/1000 | Loss: 0.00002936
Iteration 158/1000 | Loss: 0.00002935
Iteration 159/1000 | Loss: 0.00002935
Iteration 160/1000 | Loss: 0.00002935
Iteration 161/1000 | Loss: 0.00002935
Iteration 162/1000 | Loss: 0.00002935
Iteration 163/1000 | Loss: 0.00002935
Iteration 164/1000 | Loss: 0.00002935
Iteration 165/1000 | Loss: 0.00002935
Iteration 166/1000 | Loss: 0.00002935
Iteration 167/1000 | Loss: 0.00002935
Iteration 168/1000 | Loss: 0.00002934
Iteration 169/1000 | Loss: 0.00002934
Iteration 170/1000 | Loss: 0.00002934
Iteration 171/1000 | Loss: 0.00002934
Iteration 172/1000 | Loss: 0.00002934
Iteration 173/1000 | Loss: 0.00002934
Iteration 174/1000 | Loss: 0.00002933
Iteration 175/1000 | Loss: 0.00002933
Iteration 176/1000 | Loss: 0.00002933
Iteration 177/1000 | Loss: 0.00002933
Iteration 178/1000 | Loss: 0.00002933
Iteration 179/1000 | Loss: 0.00002933
Iteration 180/1000 | Loss: 0.00002933
Iteration 181/1000 | Loss: 0.00002932
Iteration 182/1000 | Loss: 0.00002932
Iteration 183/1000 | Loss: 0.00002932
Iteration 184/1000 | Loss: 0.00002932
Iteration 185/1000 | Loss: 0.00002931
Iteration 186/1000 | Loss: 0.00002931
Iteration 187/1000 | Loss: 0.00002931
Iteration 188/1000 | Loss: 0.00002931
Iteration 189/1000 | Loss: 0.00002931
Iteration 190/1000 | Loss: 0.00002931
Iteration 191/1000 | Loss: 0.00002931
Iteration 192/1000 | Loss: 0.00002931
Iteration 193/1000 | Loss: 0.00002931
Iteration 194/1000 | Loss: 0.00002930
Iteration 195/1000 | Loss: 0.00002930
Iteration 196/1000 | Loss: 0.00002930
Iteration 197/1000 | Loss: 0.00002930
Iteration 198/1000 | Loss: 0.00002929
Iteration 199/1000 | Loss: 0.00002929
Iteration 200/1000 | Loss: 0.00002929
Iteration 201/1000 | Loss: 0.00002929
Iteration 202/1000 | Loss: 0.00002929
Iteration 203/1000 | Loss: 0.00002929
Iteration 204/1000 | Loss: 0.00002928
Iteration 205/1000 | Loss: 0.00002928
Iteration 206/1000 | Loss: 0.00002928
Iteration 207/1000 | Loss: 0.00002928
Iteration 208/1000 | Loss: 0.00002928
Iteration 209/1000 | Loss: 0.00002928
Iteration 210/1000 | Loss: 0.00002928
Iteration 211/1000 | Loss: 0.00002928
Iteration 212/1000 | Loss: 0.00002928
Iteration 213/1000 | Loss: 0.00002928
Iteration 214/1000 | Loss: 0.00002927
Iteration 215/1000 | Loss: 0.00002927
Iteration 216/1000 | Loss: 0.00002927
Iteration 217/1000 | Loss: 0.00002927
Iteration 218/1000 | Loss: 0.00002927
Iteration 219/1000 | Loss: 0.00002927
Iteration 220/1000 | Loss: 0.00002927
Iteration 221/1000 | Loss: 0.00002927
Iteration 222/1000 | Loss: 0.00002927
Iteration 223/1000 | Loss: 0.00002927
Iteration 224/1000 | Loss: 0.00002927
Iteration 225/1000 | Loss: 0.00002927
Iteration 226/1000 | Loss: 0.00002927
Iteration 227/1000 | Loss: 0.00002927
Iteration 228/1000 | Loss: 0.00002927
Iteration 229/1000 | Loss: 0.00002927
Iteration 230/1000 | Loss: 0.00002927
Iteration 231/1000 | Loss: 0.00002927
Iteration 232/1000 | Loss: 0.00002927
Iteration 233/1000 | Loss: 0.00002927
Iteration 234/1000 | Loss: 0.00002927
Iteration 235/1000 | Loss: 0.00002927
Iteration 236/1000 | Loss: 0.00002927
Iteration 237/1000 | Loss: 0.00002927
Iteration 238/1000 | Loss: 0.00002927
Iteration 239/1000 | Loss: 0.00002927
Iteration 240/1000 | Loss: 0.00002927
Iteration 241/1000 | Loss: 0.00002927
Iteration 242/1000 | Loss: 0.00002927
Iteration 243/1000 | Loss: 0.00002927
Iteration 244/1000 | Loss: 0.00002927
Iteration 245/1000 | Loss: 0.00002927
Iteration 246/1000 | Loss: 0.00002927
Iteration 247/1000 | Loss: 0.00002927
Iteration 248/1000 | Loss: 0.00002927
Iteration 249/1000 | Loss: 0.00002927
Iteration 250/1000 | Loss: 0.00002927
Iteration 251/1000 | Loss: 0.00002927
Iteration 252/1000 | Loss: 0.00002927
Iteration 253/1000 | Loss: 0.00002927
Iteration 254/1000 | Loss: 0.00002927
Iteration 255/1000 | Loss: 0.00002927
Iteration 256/1000 | Loss: 0.00002927
Iteration 257/1000 | Loss: 0.00002927
Iteration 258/1000 | Loss: 0.00002927
Iteration 259/1000 | Loss: 0.00002927
Iteration 260/1000 | Loss: 0.00002927
Iteration 261/1000 | Loss: 0.00002927
Iteration 262/1000 | Loss: 0.00002927
Iteration 263/1000 | Loss: 0.00002927
Iteration 264/1000 | Loss: 0.00002927
Iteration 265/1000 | Loss: 0.00002927
Iteration 266/1000 | Loss: 0.00002927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [2.9265847842907533e-05, 2.9265847842907533e-05, 2.9265847842907533e-05, 2.9265847842907533e-05, 2.9265847842907533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9265847842907533e-05

Optimization complete. Final v2v error: 4.068987846374512 mm

Highest mean error: 12.471376419067383 mm for frame 20

Lowest mean error: 3.1321184635162354 mm for frame 60

Saving results

Total time: 268.7951669692993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941530
Iteration 2/25 | Loss: 0.00132155
Iteration 3/25 | Loss: 0.00100686
Iteration 4/25 | Loss: 0.00098316
Iteration 5/25 | Loss: 0.00097825
Iteration 6/25 | Loss: 0.00097656
Iteration 7/25 | Loss: 0.00097655
Iteration 8/25 | Loss: 0.00097655
Iteration 9/25 | Loss: 0.00097655
Iteration 10/25 | Loss: 0.00097655
Iteration 11/25 | Loss: 0.00097655
Iteration 12/25 | Loss: 0.00097655
Iteration 13/25 | Loss: 0.00097655
Iteration 14/25 | Loss: 0.00097655
Iteration 15/25 | Loss: 0.00097655
Iteration 16/25 | Loss: 0.00097655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009765508002601564, 0.0009765508002601564, 0.0009765508002601564, 0.0009765508002601564, 0.0009765508002601564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009765508002601564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67444736
Iteration 2/25 | Loss: 0.00081494
Iteration 3/25 | Loss: 0.00081494
Iteration 4/25 | Loss: 0.00081494
Iteration 5/25 | Loss: 0.00081494
Iteration 6/25 | Loss: 0.00081494
Iteration 7/25 | Loss: 0.00081494
Iteration 8/25 | Loss: 0.00081494
Iteration 9/25 | Loss: 0.00081494
Iteration 10/25 | Loss: 0.00081494
Iteration 11/25 | Loss: 0.00081494
Iteration 12/25 | Loss: 0.00081494
Iteration 13/25 | Loss: 0.00081494
Iteration 14/25 | Loss: 0.00081494
Iteration 15/25 | Loss: 0.00081494
Iteration 16/25 | Loss: 0.00081494
Iteration 17/25 | Loss: 0.00081494
Iteration 18/25 | Loss: 0.00081494
Iteration 19/25 | Loss: 0.00081494
Iteration 20/25 | Loss: 0.00081494
Iteration 21/25 | Loss: 0.00081494
Iteration 22/25 | Loss: 0.00081494
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008149417117238045, 0.0008149417117238045, 0.0008149417117238045, 0.0008149417117238045, 0.0008149417117238045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008149417117238045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081494
Iteration 2/1000 | Loss: 0.00005683
Iteration 3/1000 | Loss: 0.00004215
Iteration 4/1000 | Loss: 0.00003620
Iteration 5/1000 | Loss: 0.00003453
Iteration 6/1000 | Loss: 0.00003331
Iteration 7/1000 | Loss: 0.00003274
Iteration 8/1000 | Loss: 0.00003219
Iteration 9/1000 | Loss: 0.00003184
Iteration 10/1000 | Loss: 0.00003144
Iteration 11/1000 | Loss: 0.00003106
Iteration 12/1000 | Loss: 0.00003070
Iteration 13/1000 | Loss: 0.00003005
Iteration 14/1000 | Loss: 0.00002945
Iteration 15/1000 | Loss: 0.00002918
Iteration 16/1000 | Loss: 0.00002887
Iteration 17/1000 | Loss: 0.00002870
Iteration 18/1000 | Loss: 0.00002846
Iteration 19/1000 | Loss: 0.00002822
Iteration 20/1000 | Loss: 0.00002796
Iteration 21/1000 | Loss: 0.00002785
Iteration 22/1000 | Loss: 0.00002769
Iteration 23/1000 | Loss: 0.00002762
Iteration 24/1000 | Loss: 0.00002751
Iteration 25/1000 | Loss: 0.00002749
Iteration 26/1000 | Loss: 0.00002749
Iteration 27/1000 | Loss: 0.00002749
Iteration 28/1000 | Loss: 0.00002749
Iteration 29/1000 | Loss: 0.00002749
Iteration 30/1000 | Loss: 0.00002748
Iteration 31/1000 | Loss: 0.00002748
Iteration 32/1000 | Loss: 0.00002748
Iteration 33/1000 | Loss: 0.00002747
Iteration 34/1000 | Loss: 0.00002747
Iteration 35/1000 | Loss: 0.00002744
Iteration 36/1000 | Loss: 0.00002744
Iteration 37/1000 | Loss: 0.00002744
Iteration 38/1000 | Loss: 0.00002744
Iteration 39/1000 | Loss: 0.00002744
Iteration 40/1000 | Loss: 0.00002743
Iteration 41/1000 | Loss: 0.00002738
Iteration 42/1000 | Loss: 0.00002737
Iteration 43/1000 | Loss: 0.00002735
Iteration 44/1000 | Loss: 0.00002735
Iteration 45/1000 | Loss: 0.00002735
Iteration 46/1000 | Loss: 0.00002734
Iteration 47/1000 | Loss: 0.00002734
Iteration 48/1000 | Loss: 0.00002733
Iteration 49/1000 | Loss: 0.00002733
Iteration 50/1000 | Loss: 0.00002733
Iteration 51/1000 | Loss: 0.00002733
Iteration 52/1000 | Loss: 0.00002733
Iteration 53/1000 | Loss: 0.00002732
Iteration 54/1000 | Loss: 0.00002732
Iteration 55/1000 | Loss: 0.00002732
Iteration 56/1000 | Loss: 0.00002732
Iteration 57/1000 | Loss: 0.00002732
Iteration 58/1000 | Loss: 0.00002732
Iteration 59/1000 | Loss: 0.00002732
Iteration 60/1000 | Loss: 0.00002732
Iteration 61/1000 | Loss: 0.00002731
Iteration 62/1000 | Loss: 0.00002731
Iteration 63/1000 | Loss: 0.00002731
Iteration 64/1000 | Loss: 0.00002730
Iteration 65/1000 | Loss: 0.00002729
Iteration 66/1000 | Loss: 0.00002729
Iteration 67/1000 | Loss: 0.00002728
Iteration 68/1000 | Loss: 0.00002728
Iteration 69/1000 | Loss: 0.00002727
Iteration 70/1000 | Loss: 0.00002727
Iteration 71/1000 | Loss: 0.00002727
Iteration 72/1000 | Loss: 0.00002727
Iteration 73/1000 | Loss: 0.00002727
Iteration 74/1000 | Loss: 0.00002727
Iteration 75/1000 | Loss: 0.00002727
Iteration 76/1000 | Loss: 0.00002727
Iteration 77/1000 | Loss: 0.00002727
Iteration 78/1000 | Loss: 0.00002727
Iteration 79/1000 | Loss: 0.00002727
Iteration 80/1000 | Loss: 0.00002727
Iteration 81/1000 | Loss: 0.00002726
Iteration 82/1000 | Loss: 0.00002726
Iteration 83/1000 | Loss: 0.00002726
Iteration 84/1000 | Loss: 0.00002726
Iteration 85/1000 | Loss: 0.00002726
Iteration 86/1000 | Loss: 0.00002726
Iteration 87/1000 | Loss: 0.00002726
Iteration 88/1000 | Loss: 0.00002725
Iteration 89/1000 | Loss: 0.00002725
Iteration 90/1000 | Loss: 0.00002725
Iteration 91/1000 | Loss: 0.00002725
Iteration 92/1000 | Loss: 0.00002724
Iteration 93/1000 | Loss: 0.00002724
Iteration 94/1000 | Loss: 0.00002723
Iteration 95/1000 | Loss: 0.00002723
Iteration 96/1000 | Loss: 0.00002723
Iteration 97/1000 | Loss: 0.00002723
Iteration 98/1000 | Loss: 0.00002723
Iteration 99/1000 | Loss: 0.00002723
Iteration 100/1000 | Loss: 0.00002723
Iteration 101/1000 | Loss: 0.00002722
Iteration 102/1000 | Loss: 0.00002722
Iteration 103/1000 | Loss: 0.00002722
Iteration 104/1000 | Loss: 0.00002722
Iteration 105/1000 | Loss: 0.00002722
Iteration 106/1000 | Loss: 0.00002722
Iteration 107/1000 | Loss: 0.00002722
Iteration 108/1000 | Loss: 0.00002721
Iteration 109/1000 | Loss: 0.00002721
Iteration 110/1000 | Loss: 0.00002721
Iteration 111/1000 | Loss: 0.00002720
Iteration 112/1000 | Loss: 0.00002720
Iteration 113/1000 | Loss: 0.00002719
Iteration 114/1000 | Loss: 0.00002719
Iteration 115/1000 | Loss: 0.00002719
Iteration 116/1000 | Loss: 0.00002719
Iteration 117/1000 | Loss: 0.00002719
Iteration 118/1000 | Loss: 0.00002719
Iteration 119/1000 | Loss: 0.00002719
Iteration 120/1000 | Loss: 0.00002719
Iteration 121/1000 | Loss: 0.00002718
Iteration 122/1000 | Loss: 0.00002718
Iteration 123/1000 | Loss: 0.00002718
Iteration 124/1000 | Loss: 0.00002718
Iteration 125/1000 | Loss: 0.00002718
Iteration 126/1000 | Loss: 0.00002718
Iteration 127/1000 | Loss: 0.00002718
Iteration 128/1000 | Loss: 0.00002718
Iteration 129/1000 | Loss: 0.00002717
Iteration 130/1000 | Loss: 0.00002717
Iteration 131/1000 | Loss: 0.00002717
Iteration 132/1000 | Loss: 0.00002717
Iteration 133/1000 | Loss: 0.00002717
Iteration 134/1000 | Loss: 0.00002717
Iteration 135/1000 | Loss: 0.00002717
Iteration 136/1000 | Loss: 0.00002717
Iteration 137/1000 | Loss: 0.00002717
Iteration 138/1000 | Loss: 0.00002717
Iteration 139/1000 | Loss: 0.00002717
Iteration 140/1000 | Loss: 0.00002716
Iteration 141/1000 | Loss: 0.00002716
Iteration 142/1000 | Loss: 0.00002716
Iteration 143/1000 | Loss: 0.00002716
Iteration 144/1000 | Loss: 0.00002716
Iteration 145/1000 | Loss: 0.00002716
Iteration 146/1000 | Loss: 0.00002716
Iteration 147/1000 | Loss: 0.00002716
Iteration 148/1000 | Loss: 0.00002716
Iteration 149/1000 | Loss: 0.00002716
Iteration 150/1000 | Loss: 0.00002716
Iteration 151/1000 | Loss: 0.00002716
Iteration 152/1000 | Loss: 0.00002716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.716202834562864e-05, 2.716202834562864e-05, 2.716202834562864e-05, 2.716202834562864e-05, 2.716202834562864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.716202834562864e-05

Optimization complete. Final v2v error: 4.0507917404174805 mm

Highest mean error: 4.4047160148620605 mm for frame 58

Lowest mean error: 3.6771609783172607 mm for frame 159

Saving results

Total time: 53.49901270866394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949195
Iteration 2/25 | Loss: 0.00104934
Iteration 3/25 | Loss: 0.00096255
Iteration 4/25 | Loss: 0.00094558
Iteration 5/25 | Loss: 0.00094157
Iteration 6/25 | Loss: 0.00094111
Iteration 7/25 | Loss: 0.00094111
Iteration 8/25 | Loss: 0.00094111
Iteration 9/25 | Loss: 0.00094111
Iteration 10/25 | Loss: 0.00094111
Iteration 11/25 | Loss: 0.00094111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009411065257154405, 0.0009411065257154405, 0.0009411065257154405, 0.0009411065257154405, 0.0009411065257154405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009411065257154405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34930050
Iteration 2/25 | Loss: 0.00131469
Iteration 3/25 | Loss: 0.00131469
Iteration 4/25 | Loss: 0.00131469
Iteration 5/25 | Loss: 0.00131469
Iteration 6/25 | Loss: 0.00131469
Iteration 7/25 | Loss: 0.00131469
Iteration 8/25 | Loss: 0.00131469
Iteration 9/25 | Loss: 0.00131469
Iteration 10/25 | Loss: 0.00131469
Iteration 11/25 | Loss: 0.00131469
Iteration 12/25 | Loss: 0.00131469
Iteration 13/25 | Loss: 0.00131469
Iteration 14/25 | Loss: 0.00131469
Iteration 15/25 | Loss: 0.00131469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013146885903552175, 0.0013146885903552175, 0.0013146885903552175, 0.0013146885903552175, 0.0013146885903552175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013146885903552175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131469
Iteration 2/1000 | Loss: 0.00002886
Iteration 3/1000 | Loss: 0.00001831
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001275
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001267
Iteration 15/1000 | Loss: 0.00001265
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001255
Iteration 26/1000 | Loss: 0.00001254
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001250
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001250
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001250
Iteration 39/1000 | Loss: 0.00001250
Iteration 40/1000 | Loss: 0.00001249
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001240
Iteration 60/1000 | Loss: 0.00001240
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001237
Iteration 67/1000 | Loss: 0.00001237
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.232765043823747e-05, 1.232765043823747e-05, 1.232765043823747e-05, 1.232765043823747e-05, 1.232765043823747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.232765043823747e-05

Optimization complete. Final v2v error: 2.9580328464508057 mm

Highest mean error: 3.3704349994659424 mm for frame 7

Lowest mean error: 2.5947022438049316 mm for frame 135

Saving results

Total time: 35.73783040046692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491893
Iteration 2/25 | Loss: 0.00114419
Iteration 3/25 | Loss: 0.00101343
Iteration 4/25 | Loss: 0.00099351
Iteration 5/25 | Loss: 0.00098573
Iteration 6/25 | Loss: 0.00098347
Iteration 7/25 | Loss: 0.00098319
Iteration 8/25 | Loss: 0.00098319
Iteration 9/25 | Loss: 0.00098319
Iteration 10/25 | Loss: 0.00098319
Iteration 11/25 | Loss: 0.00098319
Iteration 12/25 | Loss: 0.00098319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009831879287958145, 0.0009831879287958145, 0.0009831879287958145, 0.0009831879287958145, 0.0009831879287958145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009831879287958145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.29754877
Iteration 2/25 | Loss: 0.00127382
Iteration 3/25 | Loss: 0.00127382
Iteration 4/25 | Loss: 0.00127382
Iteration 5/25 | Loss: 0.00127382
Iteration 6/25 | Loss: 0.00127382
Iteration 7/25 | Loss: 0.00127382
Iteration 8/25 | Loss: 0.00127382
Iteration 9/25 | Loss: 0.00127382
Iteration 10/25 | Loss: 0.00127382
Iteration 11/25 | Loss: 0.00127382
Iteration 12/25 | Loss: 0.00127382
Iteration 13/25 | Loss: 0.00127382
Iteration 14/25 | Loss: 0.00127382
Iteration 15/25 | Loss: 0.00127382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012738185469061136, 0.0012738185469061136, 0.0012738185469061136, 0.0012738185469061136, 0.0012738185469061136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012738185469061136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127382
Iteration 2/1000 | Loss: 0.00005435
Iteration 3/1000 | Loss: 0.00003393
Iteration 4/1000 | Loss: 0.00002426
Iteration 5/1000 | Loss: 0.00002188
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00001988
Iteration 8/1000 | Loss: 0.00001926
Iteration 9/1000 | Loss: 0.00001875
Iteration 10/1000 | Loss: 0.00001848
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001826
Iteration 13/1000 | Loss: 0.00001812
Iteration 14/1000 | Loss: 0.00001804
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001782
Iteration 18/1000 | Loss: 0.00001776
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001766
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001764
Iteration 25/1000 | Loss: 0.00001763
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001759
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00001757
Iteration 30/1000 | Loss: 0.00001757
Iteration 31/1000 | Loss: 0.00001756
Iteration 32/1000 | Loss: 0.00001755
Iteration 33/1000 | Loss: 0.00001755
Iteration 34/1000 | Loss: 0.00001755
Iteration 35/1000 | Loss: 0.00001754
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001753
Iteration 43/1000 | Loss: 0.00001753
Iteration 44/1000 | Loss: 0.00001753
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001753
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001752
Iteration 50/1000 | Loss: 0.00001752
Iteration 51/1000 | Loss: 0.00001752
Iteration 52/1000 | Loss: 0.00001751
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00001751
Iteration 55/1000 | Loss: 0.00001750
Iteration 56/1000 | Loss: 0.00001750
Iteration 57/1000 | Loss: 0.00001750
Iteration 58/1000 | Loss: 0.00001749
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001748
Iteration 61/1000 | Loss: 0.00001748
Iteration 62/1000 | Loss: 0.00001748
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001747
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001745
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001743
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001743
Iteration 84/1000 | Loss: 0.00001743
Iteration 85/1000 | Loss: 0.00001743
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00001742
Iteration 89/1000 | Loss: 0.00001742
Iteration 90/1000 | Loss: 0.00001742
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001741
Iteration 94/1000 | Loss: 0.00001741
Iteration 95/1000 | Loss: 0.00001741
Iteration 96/1000 | Loss: 0.00001741
Iteration 97/1000 | Loss: 0.00001741
Iteration 98/1000 | Loss: 0.00001741
Iteration 99/1000 | Loss: 0.00001741
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001740
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001739
Iteration 120/1000 | Loss: 0.00001739
Iteration 121/1000 | Loss: 0.00001739
Iteration 122/1000 | Loss: 0.00001739
Iteration 123/1000 | Loss: 0.00001739
Iteration 124/1000 | Loss: 0.00001739
Iteration 125/1000 | Loss: 0.00001738
Iteration 126/1000 | Loss: 0.00001738
Iteration 127/1000 | Loss: 0.00001738
Iteration 128/1000 | Loss: 0.00001738
Iteration 129/1000 | Loss: 0.00001738
Iteration 130/1000 | Loss: 0.00001738
Iteration 131/1000 | Loss: 0.00001738
Iteration 132/1000 | Loss: 0.00001738
Iteration 133/1000 | Loss: 0.00001738
Iteration 134/1000 | Loss: 0.00001738
Iteration 135/1000 | Loss: 0.00001738
Iteration 136/1000 | Loss: 0.00001738
Iteration 137/1000 | Loss: 0.00001738
Iteration 138/1000 | Loss: 0.00001738
Iteration 139/1000 | Loss: 0.00001738
Iteration 140/1000 | Loss: 0.00001738
Iteration 141/1000 | Loss: 0.00001738
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00001738
Iteration 145/1000 | Loss: 0.00001738
Iteration 146/1000 | Loss: 0.00001738
Iteration 147/1000 | Loss: 0.00001738
Iteration 148/1000 | Loss: 0.00001738
Iteration 149/1000 | Loss: 0.00001738
Iteration 150/1000 | Loss: 0.00001738
Iteration 151/1000 | Loss: 0.00001738
Iteration 152/1000 | Loss: 0.00001738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.7378148186253384e-05, 1.7378148186253384e-05, 1.7378148186253384e-05, 1.7378148186253384e-05, 1.7378148186253384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7378148186253384e-05

Optimization complete. Final v2v error: 3.5724339485168457 mm

Highest mean error: 4.271390914916992 mm for frame 0

Lowest mean error: 3.1318304538726807 mm for frame 131

Saving results

Total time: 39.75428056716919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569884
Iteration 2/25 | Loss: 0.00121263
Iteration 3/25 | Loss: 0.00101318
Iteration 4/25 | Loss: 0.00099309
Iteration 5/25 | Loss: 0.00098818
Iteration 6/25 | Loss: 0.00098745
Iteration 7/25 | Loss: 0.00098745
Iteration 8/25 | Loss: 0.00098745
Iteration 9/25 | Loss: 0.00098745
Iteration 10/25 | Loss: 0.00098745
Iteration 11/25 | Loss: 0.00098745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009874450042843819, 0.0009874450042843819, 0.0009874450042843819, 0.0009874450042843819, 0.0009874450042843819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009874450042843819

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18288660
Iteration 2/25 | Loss: 0.00115104
Iteration 3/25 | Loss: 0.00115104
Iteration 4/25 | Loss: 0.00115104
Iteration 5/25 | Loss: 0.00115104
Iteration 6/25 | Loss: 0.00115104
Iteration 7/25 | Loss: 0.00115104
Iteration 8/25 | Loss: 0.00115104
Iteration 9/25 | Loss: 0.00115104
Iteration 10/25 | Loss: 0.00115104
Iteration 11/25 | Loss: 0.00115104
Iteration 12/25 | Loss: 0.00115104
Iteration 13/25 | Loss: 0.00115104
Iteration 14/25 | Loss: 0.00115104
Iteration 15/25 | Loss: 0.00115104
Iteration 16/25 | Loss: 0.00115104
Iteration 17/25 | Loss: 0.00115104
Iteration 18/25 | Loss: 0.00115104
Iteration 19/25 | Loss: 0.00115104
Iteration 20/25 | Loss: 0.00115104
Iteration 21/25 | Loss: 0.00115104
Iteration 22/25 | Loss: 0.00115104
Iteration 23/25 | Loss: 0.00115104
Iteration 24/25 | Loss: 0.00115104
Iteration 25/25 | Loss: 0.00115104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115104
Iteration 2/1000 | Loss: 0.00005076
Iteration 3/1000 | Loss: 0.00003077
Iteration 4/1000 | Loss: 0.00002226
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001898
Iteration 7/1000 | Loss: 0.00001843
Iteration 8/1000 | Loss: 0.00001796
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001732
Iteration 11/1000 | Loss: 0.00001712
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001682
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001674
Iteration 19/1000 | Loss: 0.00001673
Iteration 20/1000 | Loss: 0.00001665
Iteration 21/1000 | Loss: 0.00001659
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001633
Iteration 28/1000 | Loss: 0.00001631
Iteration 29/1000 | Loss: 0.00001630
Iteration 30/1000 | Loss: 0.00001629
Iteration 31/1000 | Loss: 0.00001628
Iteration 32/1000 | Loss: 0.00001625
Iteration 33/1000 | Loss: 0.00001625
Iteration 34/1000 | Loss: 0.00001624
Iteration 35/1000 | Loss: 0.00001624
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001615
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001610
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001610
Iteration 47/1000 | Loss: 0.00001610
Iteration 48/1000 | Loss: 0.00001610
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001610
Iteration 56/1000 | Loss: 0.00001610
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001610
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001610
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001610
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001610
Iteration 71/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.6102130757644773e-05, 1.6102130757644773e-05, 1.6102130757644773e-05, 1.6102130757644773e-05, 1.6102130757644773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6102130757644773e-05

Optimization complete. Final v2v error: 3.317852735519409 mm

Highest mean error: 4.066997051239014 mm for frame 144

Lowest mean error: 2.7304413318634033 mm for frame 98

Saving results

Total time: 40.848477602005005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483320
Iteration 2/25 | Loss: 0.00108000
Iteration 3/25 | Loss: 0.00098062
Iteration 4/25 | Loss: 0.00096590
Iteration 5/25 | Loss: 0.00096317
Iteration 6/25 | Loss: 0.00096283
Iteration 7/25 | Loss: 0.00096283
Iteration 8/25 | Loss: 0.00096283
Iteration 9/25 | Loss: 0.00096283
Iteration 10/25 | Loss: 0.00096283
Iteration 11/25 | Loss: 0.00096283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009628287516534328, 0.0009628287516534328, 0.0009628287516534328, 0.0009628287516534328, 0.0009628287516534328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009628287516534328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32833815
Iteration 2/25 | Loss: 0.00118961
Iteration 3/25 | Loss: 0.00118960
Iteration 4/25 | Loss: 0.00118960
Iteration 5/25 | Loss: 0.00118960
Iteration 6/25 | Loss: 0.00118960
Iteration 7/25 | Loss: 0.00118960
Iteration 8/25 | Loss: 0.00118960
Iteration 9/25 | Loss: 0.00118960
Iteration 10/25 | Loss: 0.00118960
Iteration 11/25 | Loss: 0.00118960
Iteration 12/25 | Loss: 0.00118960
Iteration 13/25 | Loss: 0.00118960
Iteration 14/25 | Loss: 0.00118960
Iteration 15/25 | Loss: 0.00118960
Iteration 16/25 | Loss: 0.00118960
Iteration 17/25 | Loss: 0.00118960
Iteration 18/25 | Loss: 0.00118960
Iteration 19/25 | Loss: 0.00118960
Iteration 20/25 | Loss: 0.00118960
Iteration 21/25 | Loss: 0.00118960
Iteration 22/25 | Loss: 0.00118960
Iteration 23/25 | Loss: 0.00118960
Iteration 24/25 | Loss: 0.00118960
Iteration 25/25 | Loss: 0.00118960

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118960
Iteration 2/1000 | Loss: 0.00003319
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001677
Iteration 5/1000 | Loss: 0.00001576
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001434
Iteration 9/1000 | Loss: 0.00001407
Iteration 10/1000 | Loss: 0.00001402
Iteration 11/1000 | Loss: 0.00001398
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001372
Iteration 17/1000 | Loss: 0.00001364
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001358
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001357
Iteration 29/1000 | Loss: 0.00001357
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001343
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001334
Iteration 83/1000 | Loss: 0.00001334
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001334
Iteration 88/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.333556883764686e-05, 1.333556883764686e-05, 1.333556883764686e-05, 1.333556883764686e-05, 1.333556883764686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.333556883764686e-05

Optimization complete. Final v2v error: 3.126734733581543 mm

Highest mean error: 3.552741765975952 mm for frame 45

Lowest mean error: 2.7914180755615234 mm for frame 218

Saving results

Total time: 35.83787965774536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652351
Iteration 2/25 | Loss: 0.00158214
Iteration 3/25 | Loss: 0.00111552
Iteration 4/25 | Loss: 0.00101161
Iteration 5/25 | Loss: 0.00100862
Iteration 6/25 | Loss: 0.00100076
Iteration 7/25 | Loss: 0.00099829
Iteration 8/25 | Loss: 0.00099708
Iteration 9/25 | Loss: 0.00099558
Iteration 10/25 | Loss: 0.00099445
Iteration 11/25 | Loss: 0.00099767
Iteration 12/25 | Loss: 0.00100148
Iteration 13/25 | Loss: 0.00099536
Iteration 14/25 | Loss: 0.00099809
Iteration 15/25 | Loss: 0.00099815
Iteration 16/25 | Loss: 0.00099121
Iteration 17/25 | Loss: 0.00099430
Iteration 18/25 | Loss: 0.00099057
Iteration 19/25 | Loss: 0.00099050
Iteration 20/25 | Loss: 0.00099050
Iteration 21/25 | Loss: 0.00099050
Iteration 22/25 | Loss: 0.00099050
Iteration 23/25 | Loss: 0.00099050
Iteration 24/25 | Loss: 0.00099050
Iteration 25/25 | Loss: 0.00099050

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88690376
Iteration 2/25 | Loss: 0.00098730
Iteration 3/25 | Loss: 0.00098727
Iteration 4/25 | Loss: 0.00098727
Iteration 5/25 | Loss: 0.00098726
Iteration 6/25 | Loss: 0.00098726
Iteration 7/25 | Loss: 0.00098726
Iteration 8/25 | Loss: 0.00098726
Iteration 9/25 | Loss: 0.00098726
Iteration 10/25 | Loss: 0.00098726
Iteration 11/25 | Loss: 0.00098726
Iteration 12/25 | Loss: 0.00098726
Iteration 13/25 | Loss: 0.00098726
Iteration 14/25 | Loss: 0.00098726
Iteration 15/25 | Loss: 0.00098726
Iteration 16/25 | Loss: 0.00098726
Iteration 17/25 | Loss: 0.00098726
Iteration 18/25 | Loss: 0.00098726
Iteration 19/25 | Loss: 0.00098726
Iteration 20/25 | Loss: 0.00098726
Iteration 21/25 | Loss: 0.00098726
Iteration 22/25 | Loss: 0.00098726
Iteration 23/25 | Loss: 0.00098726
Iteration 24/25 | Loss: 0.00098726
Iteration 25/25 | Loss: 0.00098726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098726
Iteration 2/1000 | Loss: 0.00004428
Iteration 3/1000 | Loss: 0.00003071
Iteration 4/1000 | Loss: 0.00002773
Iteration 5/1000 | Loss: 0.00002588
Iteration 6/1000 | Loss: 0.00006798
Iteration 7/1000 | Loss: 0.00006793
Iteration 8/1000 | Loss: 0.00003424
Iteration 9/1000 | Loss: 0.00002654
Iteration 10/1000 | Loss: 0.00002443
Iteration 11/1000 | Loss: 0.00005842
Iteration 12/1000 | Loss: 0.00002389
Iteration 13/1000 | Loss: 0.00002350
Iteration 14/1000 | Loss: 0.00002319
Iteration 15/1000 | Loss: 0.00002293
Iteration 16/1000 | Loss: 0.00002266
Iteration 17/1000 | Loss: 0.00002241
Iteration 18/1000 | Loss: 0.00002216
Iteration 19/1000 | Loss: 0.00006570
Iteration 20/1000 | Loss: 0.00002179
Iteration 21/1000 | Loss: 0.00002155
Iteration 22/1000 | Loss: 0.00002131
Iteration 23/1000 | Loss: 0.00002108
Iteration 24/1000 | Loss: 0.00002107
Iteration 25/1000 | Loss: 0.00002107
Iteration 26/1000 | Loss: 0.00002107
Iteration 27/1000 | Loss: 0.00002096
Iteration 28/1000 | Loss: 0.00002095
Iteration 29/1000 | Loss: 0.00002085
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002082
Iteration 32/1000 | Loss: 0.00002080
Iteration 33/1000 | Loss: 0.00002079
Iteration 34/1000 | Loss: 0.00002079
Iteration 35/1000 | Loss: 0.00002078
Iteration 36/1000 | Loss: 0.00002078
Iteration 37/1000 | Loss: 0.00002078
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002078
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002077
Iteration 45/1000 | Loss: 0.00002076
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002075
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002072
Iteration 50/1000 | Loss: 0.00002071
Iteration 51/1000 | Loss: 0.00002071
Iteration 52/1000 | Loss: 0.00002070
Iteration 53/1000 | Loss: 0.00002070
Iteration 54/1000 | Loss: 0.00002070
Iteration 55/1000 | Loss: 0.00002070
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002069
Iteration 59/1000 | Loss: 0.00002069
Iteration 60/1000 | Loss: 0.00002069
Iteration 61/1000 | Loss: 0.00002069
Iteration 62/1000 | Loss: 0.00002069
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002068
Iteration 66/1000 | Loss: 0.00002068
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002066
Iteration 74/1000 | Loss: 0.00002066
Iteration 75/1000 | Loss: 0.00002065
Iteration 76/1000 | Loss: 0.00002064
Iteration 77/1000 | Loss: 0.00002064
Iteration 78/1000 | Loss: 0.00002064
Iteration 79/1000 | Loss: 0.00002063
Iteration 80/1000 | Loss: 0.00002063
Iteration 81/1000 | Loss: 0.00002063
Iteration 82/1000 | Loss: 0.00002063
Iteration 83/1000 | Loss: 0.00002063
Iteration 84/1000 | Loss: 0.00002062
Iteration 85/1000 | Loss: 0.00002062
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002062
Iteration 88/1000 | Loss: 0.00002062
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002061
Iteration 92/1000 | Loss: 0.00002061
Iteration 93/1000 | Loss: 0.00002061
Iteration 94/1000 | Loss: 0.00002061
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002059
Iteration 100/1000 | Loss: 0.00002059
Iteration 101/1000 | Loss: 0.00002059
Iteration 102/1000 | Loss: 0.00002059
Iteration 103/1000 | Loss: 0.00002059
Iteration 104/1000 | Loss: 0.00002059
Iteration 105/1000 | Loss: 0.00002059
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002057
Iteration 109/1000 | Loss: 0.00002057
Iteration 110/1000 | Loss: 0.00002055
Iteration 111/1000 | Loss: 0.00002055
Iteration 112/1000 | Loss: 0.00002055
Iteration 113/1000 | Loss: 0.00002055
Iteration 114/1000 | Loss: 0.00002055
Iteration 115/1000 | Loss: 0.00002055
Iteration 116/1000 | Loss: 0.00002055
Iteration 117/1000 | Loss: 0.00002055
Iteration 118/1000 | Loss: 0.00002055
Iteration 119/1000 | Loss: 0.00002055
Iteration 120/1000 | Loss: 0.00002055
Iteration 121/1000 | Loss: 0.00002055
Iteration 122/1000 | Loss: 0.00002054
Iteration 123/1000 | Loss: 0.00002054
Iteration 124/1000 | Loss: 0.00002054
Iteration 125/1000 | Loss: 0.00002054
Iteration 126/1000 | Loss: 0.00006588
Iteration 127/1000 | Loss: 0.00002104
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002053
Iteration 130/1000 | Loss: 0.00002052
Iteration 131/1000 | Loss: 0.00002052
Iteration 132/1000 | Loss: 0.00002052
Iteration 133/1000 | Loss: 0.00002051
Iteration 134/1000 | Loss: 0.00002051
Iteration 135/1000 | Loss: 0.00002051
Iteration 136/1000 | Loss: 0.00002051
Iteration 137/1000 | Loss: 0.00002051
Iteration 138/1000 | Loss: 0.00002050
Iteration 139/1000 | Loss: 0.00002050
Iteration 140/1000 | Loss: 0.00002050
Iteration 141/1000 | Loss: 0.00002050
Iteration 142/1000 | Loss: 0.00002050
Iteration 143/1000 | Loss: 0.00002050
Iteration 144/1000 | Loss: 0.00002050
Iteration 145/1000 | Loss: 0.00002050
Iteration 146/1000 | Loss: 0.00002050
Iteration 147/1000 | Loss: 0.00002049
Iteration 148/1000 | Loss: 0.00002049
Iteration 149/1000 | Loss: 0.00002049
Iteration 150/1000 | Loss: 0.00002049
Iteration 151/1000 | Loss: 0.00002049
Iteration 152/1000 | Loss: 0.00002049
Iteration 153/1000 | Loss: 0.00002049
Iteration 154/1000 | Loss: 0.00002049
Iteration 155/1000 | Loss: 0.00002049
Iteration 156/1000 | Loss: 0.00002049
Iteration 157/1000 | Loss: 0.00002049
Iteration 158/1000 | Loss: 0.00002048
Iteration 159/1000 | Loss: 0.00002048
Iteration 160/1000 | Loss: 0.00002048
Iteration 161/1000 | Loss: 0.00002048
Iteration 162/1000 | Loss: 0.00002048
Iteration 163/1000 | Loss: 0.00002048
Iteration 164/1000 | Loss: 0.00002048
Iteration 165/1000 | Loss: 0.00002047
Iteration 166/1000 | Loss: 0.00002047
Iteration 167/1000 | Loss: 0.00002047
Iteration 168/1000 | Loss: 0.00002047
Iteration 169/1000 | Loss: 0.00002047
Iteration 170/1000 | Loss: 0.00002047
Iteration 171/1000 | Loss: 0.00002047
Iteration 172/1000 | Loss: 0.00002047
Iteration 173/1000 | Loss: 0.00002047
Iteration 174/1000 | Loss: 0.00002047
Iteration 175/1000 | Loss: 0.00002047
Iteration 176/1000 | Loss: 0.00002047
Iteration 177/1000 | Loss: 0.00002047
Iteration 178/1000 | Loss: 0.00002047
Iteration 179/1000 | Loss: 0.00002047
Iteration 180/1000 | Loss: 0.00002047
Iteration 181/1000 | Loss: 0.00002047
Iteration 182/1000 | Loss: 0.00002047
Iteration 183/1000 | Loss: 0.00002047
Iteration 184/1000 | Loss: 0.00002047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.0472029063967057e-05, 2.0472029063967057e-05, 2.0472029063967057e-05, 2.0472029063967057e-05, 2.0472029063967057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0472029063967057e-05

Optimization complete. Final v2v error: 3.63556170463562 mm

Highest mean error: 4.5705437660217285 mm for frame 93

Lowest mean error: 2.9387450218200684 mm for frame 55

Saving results

Total time: 93.63169002532959
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932438
Iteration 2/25 | Loss: 0.00118172
Iteration 3/25 | Loss: 0.00102984
Iteration 4/25 | Loss: 0.00099983
Iteration 5/25 | Loss: 0.00098929
Iteration 6/25 | Loss: 0.00098634
Iteration 7/25 | Loss: 0.00098586
Iteration 8/25 | Loss: 0.00098586
Iteration 9/25 | Loss: 0.00098586
Iteration 10/25 | Loss: 0.00098586
Iteration 11/25 | Loss: 0.00098586
Iteration 12/25 | Loss: 0.00098586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009858591947704554, 0.0009858591947704554, 0.0009858591947704554, 0.0009858591947704554, 0.0009858591947704554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009858591947704554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31873608
Iteration 2/25 | Loss: 0.00119652
Iteration 3/25 | Loss: 0.00119652
Iteration 4/25 | Loss: 0.00119652
Iteration 5/25 | Loss: 0.00119652
Iteration 6/25 | Loss: 0.00119652
Iteration 7/25 | Loss: 0.00119652
Iteration 8/25 | Loss: 0.00119652
Iteration 9/25 | Loss: 0.00119652
Iteration 10/25 | Loss: 0.00119652
Iteration 11/25 | Loss: 0.00119652
Iteration 12/25 | Loss: 0.00119652
Iteration 13/25 | Loss: 0.00119652
Iteration 14/25 | Loss: 0.00119652
Iteration 15/25 | Loss: 0.00119652
Iteration 16/25 | Loss: 0.00119652
Iteration 17/25 | Loss: 0.00119652
Iteration 18/25 | Loss: 0.00119652
Iteration 19/25 | Loss: 0.00119652
Iteration 20/25 | Loss: 0.00119652
Iteration 21/25 | Loss: 0.00119652
Iteration 22/25 | Loss: 0.00119652
Iteration 23/25 | Loss: 0.00119652
Iteration 24/25 | Loss: 0.00119652
Iteration 25/25 | Loss: 0.00119652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119652
Iteration 2/1000 | Loss: 0.00006019
Iteration 3/1000 | Loss: 0.00003800
Iteration 4/1000 | Loss: 0.00002812
Iteration 5/1000 | Loss: 0.00002502
Iteration 6/1000 | Loss: 0.00002345
Iteration 7/1000 | Loss: 0.00002236
Iteration 8/1000 | Loss: 0.00002155
Iteration 9/1000 | Loss: 0.00002092
Iteration 10/1000 | Loss: 0.00002052
Iteration 11/1000 | Loss: 0.00002029
Iteration 12/1000 | Loss: 0.00002026
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002004
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001994
Iteration 18/1000 | Loss: 0.00001993
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001988
Iteration 23/1000 | Loss: 0.00001988
Iteration 24/1000 | Loss: 0.00001985
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001981
Iteration 28/1000 | Loss: 0.00001981
Iteration 29/1000 | Loss: 0.00001981
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001981
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001979
Iteration 39/1000 | Loss: 0.00001979
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001978
Iteration 43/1000 | Loss: 0.00001978
Iteration 44/1000 | Loss: 0.00001978
Iteration 45/1000 | Loss: 0.00001977
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001977
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001976
Iteration 53/1000 | Loss: 0.00001976
Iteration 54/1000 | Loss: 0.00001976
Iteration 55/1000 | Loss: 0.00001976
Iteration 56/1000 | Loss: 0.00001976
Iteration 57/1000 | Loss: 0.00001976
Iteration 58/1000 | Loss: 0.00001976
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001975
Iteration 61/1000 | Loss: 0.00001975
Iteration 62/1000 | Loss: 0.00001975
Iteration 63/1000 | Loss: 0.00001975
Iteration 64/1000 | Loss: 0.00001975
Iteration 65/1000 | Loss: 0.00001975
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001974
Iteration 68/1000 | Loss: 0.00001974
Iteration 69/1000 | Loss: 0.00001974
Iteration 70/1000 | Loss: 0.00001974
Iteration 71/1000 | Loss: 0.00001974
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001974
Iteration 75/1000 | Loss: 0.00001974
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001974
Iteration 86/1000 | Loss: 0.00001974
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.973526377696544e-05, 1.973526377696544e-05, 1.973526377696544e-05, 1.973526377696544e-05, 1.973526377696544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.973526377696544e-05

Optimization complete. Final v2v error: 3.7251970767974854 mm

Highest mean error: 5.322303295135498 mm for frame 69

Lowest mean error: 3.1891252994537354 mm for frame 140

Saving results

Total time: 34.09722948074341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080107
Iteration 2/25 | Loss: 0.00125557
Iteration 3/25 | Loss: 0.00104764
Iteration 4/25 | Loss: 0.00109513
Iteration 5/25 | Loss: 0.00099738
Iteration 6/25 | Loss: 0.00097491
Iteration 7/25 | Loss: 0.00096720
Iteration 8/25 | Loss: 0.00096298
Iteration 9/25 | Loss: 0.00096097
Iteration 10/25 | Loss: 0.00096015
Iteration 11/25 | Loss: 0.00095947
Iteration 12/25 | Loss: 0.00095750
Iteration 13/25 | Loss: 0.00095688
Iteration 14/25 | Loss: 0.00095668
Iteration 15/25 | Loss: 0.00095654
Iteration 16/25 | Loss: 0.00095644
Iteration 17/25 | Loss: 0.00095644
Iteration 18/25 | Loss: 0.00095643
Iteration 19/25 | Loss: 0.00095643
Iteration 20/25 | Loss: 0.00095643
Iteration 21/25 | Loss: 0.00095643
Iteration 22/25 | Loss: 0.00095643
Iteration 23/25 | Loss: 0.00095643
Iteration 24/25 | Loss: 0.00095643
Iteration 25/25 | Loss: 0.00095643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.09177589
Iteration 2/25 | Loss: 0.00102063
Iteration 3/25 | Loss: 0.00102063
Iteration 4/25 | Loss: 0.00102063
Iteration 5/25 | Loss: 0.00102063
Iteration 6/25 | Loss: 0.00102063
Iteration 7/25 | Loss: 0.00102063
Iteration 8/25 | Loss: 0.00102063
Iteration 9/25 | Loss: 0.00102063
Iteration 10/25 | Loss: 0.00102063
Iteration 11/25 | Loss: 0.00102063
Iteration 12/25 | Loss: 0.00102063
Iteration 13/25 | Loss: 0.00102063
Iteration 14/25 | Loss: 0.00102063
Iteration 15/25 | Loss: 0.00102063
Iteration 16/25 | Loss: 0.00102063
Iteration 17/25 | Loss: 0.00102063
Iteration 18/25 | Loss: 0.00102063
Iteration 19/25 | Loss: 0.00102063
Iteration 20/25 | Loss: 0.00102063
Iteration 21/25 | Loss: 0.00102063
Iteration 22/25 | Loss: 0.00102063
Iteration 23/25 | Loss: 0.00102063
Iteration 24/25 | Loss: 0.00102063
Iteration 25/25 | Loss: 0.00102063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102063
Iteration 2/1000 | Loss: 0.00003147
Iteration 3/1000 | Loss: 0.00001994
Iteration 4/1000 | Loss: 0.00001679
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001491
Iteration 7/1000 | Loss: 0.00001457
Iteration 8/1000 | Loss: 0.00001448
Iteration 9/1000 | Loss: 0.00001442
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001433
Iteration 13/1000 | Loss: 0.00001433
Iteration 14/1000 | Loss: 0.00001432
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00001432
Iteration 17/1000 | Loss: 0.00001431
Iteration 18/1000 | Loss: 0.00001426
Iteration 19/1000 | Loss: 0.00001426
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001425
Iteration 22/1000 | Loss: 0.00001425
Iteration 23/1000 | Loss: 0.00001424
Iteration 24/1000 | Loss: 0.00001424
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001421
Iteration 28/1000 | Loss: 0.00001421
Iteration 29/1000 | Loss: 0.00001421
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001421
Iteration 32/1000 | Loss: 0.00001420
Iteration 33/1000 | Loss: 0.00001420
Iteration 34/1000 | Loss: 0.00001419
Iteration 35/1000 | Loss: 0.00001419
Iteration 36/1000 | Loss: 0.00001418
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001418
Iteration 39/1000 | Loss: 0.00001417
Iteration 40/1000 | Loss: 0.00001417
Iteration 41/1000 | Loss: 0.00001417
Iteration 42/1000 | Loss: 0.00001417
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001416
Iteration 46/1000 | Loss: 0.00001416
Iteration 47/1000 | Loss: 0.00001415
Iteration 48/1000 | Loss: 0.00001415
Iteration 49/1000 | Loss: 0.00001414
Iteration 50/1000 | Loss: 0.00001414
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001413
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001409
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001408
Iteration 58/1000 | Loss: 0.00001407
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001405
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001404
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001404
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001398
Iteration 77/1000 | Loss: 0.00001398
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001396
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001390
Iteration 123/1000 | Loss: 0.00001390
Iteration 124/1000 | Loss: 0.00001390
Iteration 125/1000 | Loss: 0.00001389
Iteration 126/1000 | Loss: 0.00001389
Iteration 127/1000 | Loss: 0.00001389
Iteration 128/1000 | Loss: 0.00001389
Iteration 129/1000 | Loss: 0.00001388
Iteration 130/1000 | Loss: 0.00001388
Iteration 131/1000 | Loss: 0.00001388
Iteration 132/1000 | Loss: 0.00001388
Iteration 133/1000 | Loss: 0.00001388
Iteration 134/1000 | Loss: 0.00001388
Iteration 135/1000 | Loss: 0.00001388
Iteration 136/1000 | Loss: 0.00001388
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001387
Iteration 143/1000 | Loss: 0.00001387
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001387
Iteration 150/1000 | Loss: 0.00001387
Iteration 151/1000 | Loss: 0.00001387
Iteration 152/1000 | Loss: 0.00001387
Iteration 153/1000 | Loss: 0.00001386
Iteration 154/1000 | Loss: 0.00001386
Iteration 155/1000 | Loss: 0.00001386
Iteration 156/1000 | Loss: 0.00001386
Iteration 157/1000 | Loss: 0.00001386
Iteration 158/1000 | Loss: 0.00001386
Iteration 159/1000 | Loss: 0.00001386
Iteration 160/1000 | Loss: 0.00001386
Iteration 161/1000 | Loss: 0.00001386
Iteration 162/1000 | Loss: 0.00001386
Iteration 163/1000 | Loss: 0.00001386
Iteration 164/1000 | Loss: 0.00001386
Iteration 165/1000 | Loss: 0.00001386
Iteration 166/1000 | Loss: 0.00001386
Iteration 167/1000 | Loss: 0.00001386
Iteration 168/1000 | Loss: 0.00001386
Iteration 169/1000 | Loss: 0.00001386
Iteration 170/1000 | Loss: 0.00001386
Iteration 171/1000 | Loss: 0.00001386
Iteration 172/1000 | Loss: 0.00001386
Iteration 173/1000 | Loss: 0.00001386
Iteration 174/1000 | Loss: 0.00001386
Iteration 175/1000 | Loss: 0.00001386
Iteration 176/1000 | Loss: 0.00001386
Iteration 177/1000 | Loss: 0.00001386
Iteration 178/1000 | Loss: 0.00001386
Iteration 179/1000 | Loss: 0.00001386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.3858782040188089e-05, 1.3858782040188089e-05, 1.3858782040188089e-05, 1.3858782040188089e-05, 1.3858782040188089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3858782040188089e-05

Optimization complete. Final v2v error: 3.14674711227417 mm

Highest mean error: 3.3955605030059814 mm for frame 102

Lowest mean error: 2.942584276199341 mm for frame 21

Saving results

Total time: 50.006722927093506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00587353
Iteration 2/25 | Loss: 0.00127336
Iteration 3/25 | Loss: 0.00108360
Iteration 4/25 | Loss: 0.00103477
Iteration 5/25 | Loss: 0.00102286
Iteration 6/25 | Loss: 0.00102582
Iteration 7/25 | Loss: 0.00102155
Iteration 8/25 | Loss: 0.00101949
Iteration 9/25 | Loss: 0.00101931
Iteration 10/25 | Loss: 0.00101892
Iteration 11/25 | Loss: 0.00101824
Iteration 12/25 | Loss: 0.00101770
Iteration 13/25 | Loss: 0.00101751
Iteration 14/25 | Loss: 0.00101743
Iteration 15/25 | Loss: 0.00101742
Iteration 16/25 | Loss: 0.00101741
Iteration 17/25 | Loss: 0.00101741
Iteration 18/25 | Loss: 0.00101741
Iteration 19/25 | Loss: 0.00101741
Iteration 20/25 | Loss: 0.00101740
Iteration 21/25 | Loss: 0.00101740
Iteration 22/25 | Loss: 0.00101740
Iteration 23/25 | Loss: 0.00101740
Iteration 24/25 | Loss: 0.00101740
Iteration 25/25 | Loss: 0.00101740

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08410358
Iteration 2/25 | Loss: 0.00107538
Iteration 3/25 | Loss: 0.00107533
Iteration 4/25 | Loss: 0.00107532
Iteration 5/25 | Loss: 0.00107532
Iteration 6/25 | Loss: 0.00107532
Iteration 7/25 | Loss: 0.00107532
Iteration 8/25 | Loss: 0.00107532
Iteration 9/25 | Loss: 0.00107532
Iteration 10/25 | Loss: 0.00107532
Iteration 11/25 | Loss: 0.00107532
Iteration 12/25 | Loss: 0.00107532
Iteration 13/25 | Loss: 0.00107532
Iteration 14/25 | Loss: 0.00107532
Iteration 15/25 | Loss: 0.00107532
Iteration 16/25 | Loss: 0.00107532
Iteration 17/25 | Loss: 0.00107532
Iteration 18/25 | Loss: 0.00107532
Iteration 19/25 | Loss: 0.00107532
Iteration 20/25 | Loss: 0.00107532
Iteration 21/25 | Loss: 0.00107532
Iteration 22/25 | Loss: 0.00107532
Iteration 23/25 | Loss: 0.00107532
Iteration 24/25 | Loss: 0.00107532
Iteration 25/25 | Loss: 0.00107532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107532
Iteration 2/1000 | Loss: 0.00005841
Iteration 3/1000 | Loss: 0.00003787
Iteration 4/1000 | Loss: 0.00003281
Iteration 5/1000 | Loss: 0.00021735
Iteration 6/1000 | Loss: 0.00003109
Iteration 7/1000 | Loss: 0.00002921
Iteration 8/1000 | Loss: 0.00002773
Iteration 9/1000 | Loss: 0.00002682
Iteration 10/1000 | Loss: 0.00002604
Iteration 11/1000 | Loss: 0.00002560
Iteration 12/1000 | Loss: 0.00002530
Iteration 13/1000 | Loss: 0.00002497
Iteration 14/1000 | Loss: 0.00002471
Iteration 15/1000 | Loss: 0.00002449
Iteration 16/1000 | Loss: 0.00002435
Iteration 17/1000 | Loss: 0.00002431
Iteration 18/1000 | Loss: 0.00002429
Iteration 19/1000 | Loss: 0.00002425
Iteration 20/1000 | Loss: 0.00002425
Iteration 21/1000 | Loss: 0.00002422
Iteration 22/1000 | Loss: 0.00002422
Iteration 23/1000 | Loss: 0.00002420
Iteration 24/1000 | Loss: 0.00002419
Iteration 25/1000 | Loss: 0.00002419
Iteration 26/1000 | Loss: 0.00002419
Iteration 27/1000 | Loss: 0.00002419
Iteration 28/1000 | Loss: 0.00002418
Iteration 29/1000 | Loss: 0.00002418
Iteration 30/1000 | Loss: 0.00002417
Iteration 31/1000 | Loss: 0.00002417
Iteration 32/1000 | Loss: 0.00002417
Iteration 33/1000 | Loss: 0.00002417
Iteration 34/1000 | Loss: 0.00002417
Iteration 35/1000 | Loss: 0.00002417
Iteration 36/1000 | Loss: 0.00002417
Iteration 37/1000 | Loss: 0.00002417
Iteration 38/1000 | Loss: 0.00002417
Iteration 39/1000 | Loss: 0.00002417
Iteration 40/1000 | Loss: 0.00002417
Iteration 41/1000 | Loss: 0.00002417
Iteration 42/1000 | Loss: 0.00002416
Iteration 43/1000 | Loss: 0.00002416
Iteration 44/1000 | Loss: 0.00002416
Iteration 45/1000 | Loss: 0.00002416
Iteration 46/1000 | Loss: 0.00002415
Iteration 47/1000 | Loss: 0.00002415
Iteration 48/1000 | Loss: 0.00002415
Iteration 49/1000 | Loss: 0.00002414
Iteration 50/1000 | Loss: 0.00002414
Iteration 51/1000 | Loss: 0.00002414
Iteration 52/1000 | Loss: 0.00002413
Iteration 53/1000 | Loss: 0.00002413
Iteration 54/1000 | Loss: 0.00002413
Iteration 55/1000 | Loss: 0.00002413
Iteration 56/1000 | Loss: 0.00002413
Iteration 57/1000 | Loss: 0.00002413
Iteration 58/1000 | Loss: 0.00002413
Iteration 59/1000 | Loss: 0.00002413
Iteration 60/1000 | Loss: 0.00002412
Iteration 61/1000 | Loss: 0.00002412
Iteration 62/1000 | Loss: 0.00002412
Iteration 63/1000 | Loss: 0.00002412
Iteration 64/1000 | Loss: 0.00002412
Iteration 65/1000 | Loss: 0.00002412
Iteration 66/1000 | Loss: 0.00002412
Iteration 67/1000 | Loss: 0.00002412
Iteration 68/1000 | Loss: 0.00002411
Iteration 69/1000 | Loss: 0.00002411
Iteration 70/1000 | Loss: 0.00002411
Iteration 71/1000 | Loss: 0.00002410
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002410
Iteration 74/1000 | Loss: 0.00002410
Iteration 75/1000 | Loss: 0.00002409
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002409
Iteration 78/1000 | Loss: 0.00002409
Iteration 79/1000 | Loss: 0.00002409
Iteration 80/1000 | Loss: 0.00002409
Iteration 81/1000 | Loss: 0.00002408
Iteration 82/1000 | Loss: 0.00002408
Iteration 83/1000 | Loss: 0.00002408
Iteration 84/1000 | Loss: 0.00002408
Iteration 85/1000 | Loss: 0.00002407
Iteration 86/1000 | Loss: 0.00002407
Iteration 87/1000 | Loss: 0.00002407
Iteration 88/1000 | Loss: 0.00002407
Iteration 89/1000 | Loss: 0.00002407
Iteration 90/1000 | Loss: 0.00002407
Iteration 91/1000 | Loss: 0.00002407
Iteration 92/1000 | Loss: 0.00002407
Iteration 93/1000 | Loss: 0.00002406
Iteration 94/1000 | Loss: 0.00002406
Iteration 95/1000 | Loss: 0.00002406
Iteration 96/1000 | Loss: 0.00002406
Iteration 97/1000 | Loss: 0.00002406
Iteration 98/1000 | Loss: 0.00002406
Iteration 99/1000 | Loss: 0.00002406
Iteration 100/1000 | Loss: 0.00002406
Iteration 101/1000 | Loss: 0.00002405
Iteration 102/1000 | Loss: 0.00002405
Iteration 103/1000 | Loss: 0.00002405
Iteration 104/1000 | Loss: 0.00002405
Iteration 105/1000 | Loss: 0.00002405
Iteration 106/1000 | Loss: 0.00002405
Iteration 107/1000 | Loss: 0.00002405
Iteration 108/1000 | Loss: 0.00002405
Iteration 109/1000 | Loss: 0.00002404
Iteration 110/1000 | Loss: 0.00002404
Iteration 111/1000 | Loss: 0.00002404
Iteration 112/1000 | Loss: 0.00002403
Iteration 113/1000 | Loss: 0.00002403
Iteration 114/1000 | Loss: 0.00002403
Iteration 115/1000 | Loss: 0.00002403
Iteration 116/1000 | Loss: 0.00002403
Iteration 117/1000 | Loss: 0.00002403
Iteration 118/1000 | Loss: 0.00002403
Iteration 119/1000 | Loss: 0.00002403
Iteration 120/1000 | Loss: 0.00002403
Iteration 121/1000 | Loss: 0.00002403
Iteration 122/1000 | Loss: 0.00002402
Iteration 123/1000 | Loss: 0.00002402
Iteration 124/1000 | Loss: 0.00002402
Iteration 125/1000 | Loss: 0.00002402
Iteration 126/1000 | Loss: 0.00002402
Iteration 127/1000 | Loss: 0.00002401
Iteration 128/1000 | Loss: 0.00002401
Iteration 129/1000 | Loss: 0.00002401
Iteration 130/1000 | Loss: 0.00002400
Iteration 131/1000 | Loss: 0.00002400
Iteration 132/1000 | Loss: 0.00002400
Iteration 133/1000 | Loss: 0.00002400
Iteration 134/1000 | Loss: 0.00002400
Iteration 135/1000 | Loss: 0.00002400
Iteration 136/1000 | Loss: 0.00002400
Iteration 137/1000 | Loss: 0.00002400
Iteration 138/1000 | Loss: 0.00002400
Iteration 139/1000 | Loss: 0.00002400
Iteration 140/1000 | Loss: 0.00002400
Iteration 141/1000 | Loss: 0.00002400
Iteration 142/1000 | Loss: 0.00002400
Iteration 143/1000 | Loss: 0.00002400
Iteration 144/1000 | Loss: 0.00002400
Iteration 145/1000 | Loss: 0.00002400
Iteration 146/1000 | Loss: 0.00002400
Iteration 147/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.3997783500817604e-05, 2.3997783500817604e-05, 2.3997783500817604e-05, 2.3997783500817604e-05, 2.3997783500817604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3997783500817604e-05

Optimization complete. Final v2v error: 4.0065531730651855 mm

Highest mean error: 6.241883277893066 mm for frame 113

Lowest mean error: 3.3495216369628906 mm for frame 28

Saving results

Total time: 57.40667271614075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875952
Iteration 2/25 | Loss: 0.00132187
Iteration 3/25 | Loss: 0.00111684
Iteration 4/25 | Loss: 0.00109421
Iteration 5/25 | Loss: 0.00109145
Iteration 6/25 | Loss: 0.00109134
Iteration 7/25 | Loss: 0.00109134
Iteration 8/25 | Loss: 0.00109134
Iteration 9/25 | Loss: 0.00109134
Iteration 10/25 | Loss: 0.00109134
Iteration 11/25 | Loss: 0.00109134
Iteration 12/25 | Loss: 0.00109134
Iteration 13/25 | Loss: 0.00109134
Iteration 14/25 | Loss: 0.00109134
Iteration 15/25 | Loss: 0.00109134
Iteration 16/25 | Loss: 0.00109134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010913405567407608, 0.0010913405567407608, 0.0010913405567407608, 0.0010913405567407608, 0.0010913405567407608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010913405567407608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89196765
Iteration 2/25 | Loss: 0.00076062
Iteration 3/25 | Loss: 0.00076062
Iteration 4/25 | Loss: 0.00076062
Iteration 5/25 | Loss: 0.00076062
Iteration 6/25 | Loss: 0.00076062
Iteration 7/25 | Loss: 0.00076062
Iteration 8/25 | Loss: 0.00076062
Iteration 9/25 | Loss: 0.00076062
Iteration 10/25 | Loss: 0.00076062
Iteration 11/25 | Loss: 0.00076062
Iteration 12/25 | Loss: 0.00076062
Iteration 13/25 | Loss: 0.00076062
Iteration 14/25 | Loss: 0.00076062
Iteration 15/25 | Loss: 0.00076062
Iteration 16/25 | Loss: 0.00076062
Iteration 17/25 | Loss: 0.00076062
Iteration 18/25 | Loss: 0.00076062
Iteration 19/25 | Loss: 0.00076062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007606169092468917, 0.0007606169092468917, 0.0007606169092468917, 0.0007606169092468917, 0.0007606169092468917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007606169092468917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076062
Iteration 2/1000 | Loss: 0.00004914
Iteration 3/1000 | Loss: 0.00003590
Iteration 4/1000 | Loss: 0.00003048
Iteration 5/1000 | Loss: 0.00002815
Iteration 6/1000 | Loss: 0.00002734
Iteration 7/1000 | Loss: 0.00002679
Iteration 8/1000 | Loss: 0.00002647
Iteration 9/1000 | Loss: 0.00002639
Iteration 10/1000 | Loss: 0.00002613
Iteration 11/1000 | Loss: 0.00002594
Iteration 12/1000 | Loss: 0.00002591
Iteration 13/1000 | Loss: 0.00002577
Iteration 14/1000 | Loss: 0.00002560
Iteration 15/1000 | Loss: 0.00002558
Iteration 16/1000 | Loss: 0.00002558
Iteration 17/1000 | Loss: 0.00002557
Iteration 18/1000 | Loss: 0.00002554
Iteration 19/1000 | Loss: 0.00002548
Iteration 20/1000 | Loss: 0.00002547
Iteration 21/1000 | Loss: 0.00002544
Iteration 22/1000 | Loss: 0.00002544
Iteration 23/1000 | Loss: 0.00002544
Iteration 24/1000 | Loss: 0.00002543
Iteration 25/1000 | Loss: 0.00002543
Iteration 26/1000 | Loss: 0.00002543
Iteration 27/1000 | Loss: 0.00002542
Iteration 28/1000 | Loss: 0.00002542
Iteration 29/1000 | Loss: 0.00002542
Iteration 30/1000 | Loss: 0.00002542
Iteration 31/1000 | Loss: 0.00002541
Iteration 32/1000 | Loss: 0.00002541
Iteration 33/1000 | Loss: 0.00002540
Iteration 34/1000 | Loss: 0.00002540
Iteration 35/1000 | Loss: 0.00002537
Iteration 36/1000 | Loss: 0.00002537
Iteration 37/1000 | Loss: 0.00002537
Iteration 38/1000 | Loss: 0.00002537
Iteration 39/1000 | Loss: 0.00002537
Iteration 40/1000 | Loss: 0.00002537
Iteration 41/1000 | Loss: 0.00002537
Iteration 42/1000 | Loss: 0.00002536
Iteration 43/1000 | Loss: 0.00002536
Iteration 44/1000 | Loss: 0.00002536
Iteration 45/1000 | Loss: 0.00002536
Iteration 46/1000 | Loss: 0.00002536
Iteration 47/1000 | Loss: 0.00002535
Iteration 48/1000 | Loss: 0.00002535
Iteration 49/1000 | Loss: 0.00002532
Iteration 50/1000 | Loss: 0.00002532
Iteration 51/1000 | Loss: 0.00002531
Iteration 52/1000 | Loss: 0.00002531
Iteration 53/1000 | Loss: 0.00002531
Iteration 54/1000 | Loss: 0.00002531
Iteration 55/1000 | Loss: 0.00002531
Iteration 56/1000 | Loss: 0.00002531
Iteration 57/1000 | Loss: 0.00002530
Iteration 58/1000 | Loss: 0.00002528
Iteration 59/1000 | Loss: 0.00002527
Iteration 60/1000 | Loss: 0.00002527
Iteration 61/1000 | Loss: 0.00002524
Iteration 62/1000 | Loss: 0.00002522
Iteration 63/1000 | Loss: 0.00002516
Iteration 64/1000 | Loss: 0.00002515
Iteration 65/1000 | Loss: 0.00002511
Iteration 66/1000 | Loss: 0.00002502
Iteration 67/1000 | Loss: 0.00002502
Iteration 68/1000 | Loss: 0.00002501
Iteration 69/1000 | Loss: 0.00002500
Iteration 70/1000 | Loss: 0.00002500
Iteration 71/1000 | Loss: 0.00002500
Iteration 72/1000 | Loss: 0.00002499
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002498
Iteration 76/1000 | Loss: 0.00002498
Iteration 77/1000 | Loss: 0.00002497
Iteration 78/1000 | Loss: 0.00002497
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002495
Iteration 81/1000 | Loss: 0.00002495
Iteration 82/1000 | Loss: 0.00002495
Iteration 83/1000 | Loss: 0.00002495
Iteration 84/1000 | Loss: 0.00002495
Iteration 85/1000 | Loss: 0.00002495
Iteration 86/1000 | Loss: 0.00002495
Iteration 87/1000 | Loss: 0.00002495
Iteration 88/1000 | Loss: 0.00002495
Iteration 89/1000 | Loss: 0.00002495
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002494
Iteration 94/1000 | Loss: 0.00002494
Iteration 95/1000 | Loss: 0.00002494
Iteration 96/1000 | Loss: 0.00002493
Iteration 97/1000 | Loss: 0.00002493
Iteration 98/1000 | Loss: 0.00002492
Iteration 99/1000 | Loss: 0.00002492
Iteration 100/1000 | Loss: 0.00002492
Iteration 101/1000 | Loss: 0.00002492
Iteration 102/1000 | Loss: 0.00002492
Iteration 103/1000 | Loss: 0.00002492
Iteration 104/1000 | Loss: 0.00002492
Iteration 105/1000 | Loss: 0.00002492
Iteration 106/1000 | Loss: 0.00002492
Iteration 107/1000 | Loss: 0.00002491
Iteration 108/1000 | Loss: 0.00002491
Iteration 109/1000 | Loss: 0.00002491
Iteration 110/1000 | Loss: 0.00002491
Iteration 111/1000 | Loss: 0.00002491
Iteration 112/1000 | Loss: 0.00002491
Iteration 113/1000 | Loss: 0.00002491
Iteration 114/1000 | Loss: 0.00002491
Iteration 115/1000 | Loss: 0.00002491
Iteration 116/1000 | Loss: 0.00002491
Iteration 117/1000 | Loss: 0.00002491
Iteration 118/1000 | Loss: 0.00002490
Iteration 119/1000 | Loss: 0.00002490
Iteration 120/1000 | Loss: 0.00002490
Iteration 121/1000 | Loss: 0.00002490
Iteration 122/1000 | Loss: 0.00002490
Iteration 123/1000 | Loss: 0.00002490
Iteration 124/1000 | Loss: 0.00002490
Iteration 125/1000 | Loss: 0.00002489
Iteration 126/1000 | Loss: 0.00002489
Iteration 127/1000 | Loss: 0.00002489
Iteration 128/1000 | Loss: 0.00002489
Iteration 129/1000 | Loss: 0.00002489
Iteration 130/1000 | Loss: 0.00002489
Iteration 131/1000 | Loss: 0.00002489
Iteration 132/1000 | Loss: 0.00002489
Iteration 133/1000 | Loss: 0.00002489
Iteration 134/1000 | Loss: 0.00002489
Iteration 135/1000 | Loss: 0.00002489
Iteration 136/1000 | Loss: 0.00002488
Iteration 137/1000 | Loss: 0.00002488
Iteration 138/1000 | Loss: 0.00002486
Iteration 139/1000 | Loss: 0.00002485
Iteration 140/1000 | Loss: 0.00002485
Iteration 141/1000 | Loss: 0.00002485
Iteration 142/1000 | Loss: 0.00002485
Iteration 143/1000 | Loss: 0.00002484
Iteration 144/1000 | Loss: 0.00002484
Iteration 145/1000 | Loss: 0.00002484
Iteration 146/1000 | Loss: 0.00002484
Iteration 147/1000 | Loss: 0.00002484
Iteration 148/1000 | Loss: 0.00002484
Iteration 149/1000 | Loss: 0.00002484
Iteration 150/1000 | Loss: 0.00002484
Iteration 151/1000 | Loss: 0.00002484
Iteration 152/1000 | Loss: 0.00002483
Iteration 153/1000 | Loss: 0.00002482
Iteration 154/1000 | Loss: 0.00002482
Iteration 155/1000 | Loss: 0.00002482
Iteration 156/1000 | Loss: 0.00002482
Iteration 157/1000 | Loss: 0.00002482
Iteration 158/1000 | Loss: 0.00002482
Iteration 159/1000 | Loss: 0.00002482
Iteration 160/1000 | Loss: 0.00002482
Iteration 161/1000 | Loss: 0.00002482
Iteration 162/1000 | Loss: 0.00002482
Iteration 163/1000 | Loss: 0.00002482
Iteration 164/1000 | Loss: 0.00002482
Iteration 165/1000 | Loss: 0.00002482
Iteration 166/1000 | Loss: 0.00002482
Iteration 167/1000 | Loss: 0.00002481
Iteration 168/1000 | Loss: 0.00002480
Iteration 169/1000 | Loss: 0.00002479
Iteration 170/1000 | Loss: 0.00002479
Iteration 171/1000 | Loss: 0.00002479
Iteration 172/1000 | Loss: 0.00002479
Iteration 173/1000 | Loss: 0.00002479
Iteration 174/1000 | Loss: 0.00002479
Iteration 175/1000 | Loss: 0.00002479
Iteration 176/1000 | Loss: 0.00002478
Iteration 177/1000 | Loss: 0.00002478
Iteration 178/1000 | Loss: 0.00002477
Iteration 179/1000 | Loss: 0.00002477
Iteration 180/1000 | Loss: 0.00002477
Iteration 181/1000 | Loss: 0.00002477
Iteration 182/1000 | Loss: 0.00002477
Iteration 183/1000 | Loss: 0.00002477
Iteration 184/1000 | Loss: 0.00002476
Iteration 185/1000 | Loss: 0.00002476
Iteration 186/1000 | Loss: 0.00002476
Iteration 187/1000 | Loss: 0.00002476
Iteration 188/1000 | Loss: 0.00002476
Iteration 189/1000 | Loss: 0.00002476
Iteration 190/1000 | Loss: 0.00002475
Iteration 191/1000 | Loss: 0.00002474
Iteration 192/1000 | Loss: 0.00002474
Iteration 193/1000 | Loss: 0.00002474
Iteration 194/1000 | Loss: 0.00002474
Iteration 195/1000 | Loss: 0.00002474
Iteration 196/1000 | Loss: 0.00002474
Iteration 197/1000 | Loss: 0.00002474
Iteration 198/1000 | Loss: 0.00002473
Iteration 199/1000 | Loss: 0.00002473
Iteration 200/1000 | Loss: 0.00002473
Iteration 201/1000 | Loss: 0.00002473
Iteration 202/1000 | Loss: 0.00002473
Iteration 203/1000 | Loss: 0.00002473
Iteration 204/1000 | Loss: 0.00002473
Iteration 205/1000 | Loss: 0.00002473
Iteration 206/1000 | Loss: 0.00002473
Iteration 207/1000 | Loss: 0.00002473
Iteration 208/1000 | Loss: 0.00002473
Iteration 209/1000 | Loss: 0.00002473
Iteration 210/1000 | Loss: 0.00002472
Iteration 211/1000 | Loss: 0.00002472
Iteration 212/1000 | Loss: 0.00002472
Iteration 213/1000 | Loss: 0.00002472
Iteration 214/1000 | Loss: 0.00002472
Iteration 215/1000 | Loss: 0.00002472
Iteration 216/1000 | Loss: 0.00002472
Iteration 217/1000 | Loss: 0.00002472
Iteration 218/1000 | Loss: 0.00002472
Iteration 219/1000 | Loss: 0.00002472
Iteration 220/1000 | Loss: 0.00002472
Iteration 221/1000 | Loss: 0.00002471
Iteration 222/1000 | Loss: 0.00002471
Iteration 223/1000 | Loss: 0.00002471
Iteration 224/1000 | Loss: 0.00002471
Iteration 225/1000 | Loss: 0.00002471
Iteration 226/1000 | Loss: 0.00002471
Iteration 227/1000 | Loss: 0.00002471
Iteration 228/1000 | Loss: 0.00002471
Iteration 229/1000 | Loss: 0.00002471
Iteration 230/1000 | Loss: 0.00002471
Iteration 231/1000 | Loss: 0.00002471
Iteration 232/1000 | Loss: 0.00002471
Iteration 233/1000 | Loss: 0.00002471
Iteration 234/1000 | Loss: 0.00002471
Iteration 235/1000 | Loss: 0.00002471
Iteration 236/1000 | Loss: 0.00002471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.4714656319702044e-05, 2.4714656319702044e-05, 2.4714656319702044e-05, 2.4714656319702044e-05, 2.4714656319702044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4714656319702044e-05

Optimization complete. Final v2v error: 4.160775184631348 mm

Highest mean error: 4.215324878692627 mm for frame 20

Lowest mean error: 4.095303058624268 mm for frame 144

Saving results

Total time: 44.629539012908936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036745
Iteration 2/25 | Loss: 0.00171142
Iteration 3/25 | Loss: 0.00128154
Iteration 4/25 | Loss: 0.00114103
Iteration 5/25 | Loss: 0.00112618
Iteration 6/25 | Loss: 0.00111989
Iteration 7/25 | Loss: 0.00100272
Iteration 8/25 | Loss: 0.00098878
Iteration 9/25 | Loss: 0.00098545
Iteration 10/25 | Loss: 0.00098427
Iteration 11/25 | Loss: 0.00098672
Iteration 12/25 | Loss: 0.00097870
Iteration 13/25 | Loss: 0.00097818
Iteration 14/25 | Loss: 0.00097793
Iteration 15/25 | Loss: 0.00097782
Iteration 16/25 | Loss: 0.00097781
Iteration 17/25 | Loss: 0.00097781
Iteration 18/25 | Loss: 0.00097781
Iteration 19/25 | Loss: 0.00097781
Iteration 20/25 | Loss: 0.00097780
Iteration 21/25 | Loss: 0.00097780
Iteration 22/25 | Loss: 0.00097780
Iteration 23/25 | Loss: 0.00097779
Iteration 24/25 | Loss: 0.00097779
Iteration 25/25 | Loss: 0.00097779

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81193221
Iteration 2/25 | Loss: 0.00113215
Iteration 3/25 | Loss: 0.00113215
Iteration 4/25 | Loss: 0.00113215
Iteration 5/25 | Loss: 0.00113215
Iteration 6/25 | Loss: 0.00113215
Iteration 7/25 | Loss: 0.00113215
Iteration 8/25 | Loss: 0.00113215
Iteration 9/25 | Loss: 0.00113215
Iteration 10/25 | Loss: 0.00113215
Iteration 11/25 | Loss: 0.00113215
Iteration 12/25 | Loss: 0.00113215
Iteration 13/25 | Loss: 0.00113215
Iteration 14/25 | Loss: 0.00113215
Iteration 15/25 | Loss: 0.00113215
Iteration 16/25 | Loss: 0.00113215
Iteration 17/25 | Loss: 0.00113215
Iteration 18/25 | Loss: 0.00113215
Iteration 19/25 | Loss: 0.00113215
Iteration 20/25 | Loss: 0.00113215
Iteration 21/25 | Loss: 0.00113215
Iteration 22/25 | Loss: 0.00113215
Iteration 23/25 | Loss: 0.00113215
Iteration 24/25 | Loss: 0.00113215
Iteration 25/25 | Loss: 0.00113215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113215
Iteration 2/1000 | Loss: 0.00006773
Iteration 3/1000 | Loss: 0.00003790
Iteration 4/1000 | Loss: 0.00002981
Iteration 5/1000 | Loss: 0.00002546
Iteration 6/1000 | Loss: 0.00002299
Iteration 7/1000 | Loss: 0.00002118
Iteration 8/1000 | Loss: 0.00002042
Iteration 9/1000 | Loss: 0.00001970
Iteration 10/1000 | Loss: 0.00001934
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00031574
Iteration 13/1000 | Loss: 0.00032356
Iteration 14/1000 | Loss: 0.00026879
Iteration 15/1000 | Loss: 0.00026301
Iteration 16/1000 | Loss: 0.00026937
Iteration 17/1000 | Loss: 0.00016222
Iteration 18/1000 | Loss: 0.00002277
Iteration 19/1000 | Loss: 0.00020286
Iteration 20/1000 | Loss: 0.00002316
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00004062
Iteration 23/1000 | Loss: 0.00001720
Iteration 24/1000 | Loss: 0.00001663
Iteration 25/1000 | Loss: 0.00003662
Iteration 26/1000 | Loss: 0.00001604
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001569
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001533
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001505
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00029595
Iteration 45/1000 | Loss: 0.00001869
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00009054
Iteration 48/1000 | Loss: 0.00001540
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001467
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001432
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001429
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001427
Iteration 60/1000 | Loss: 0.00001427
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001423
Iteration 77/1000 | Loss: 0.00001423
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001423
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001421
Iteration 88/1000 | Loss: 0.00001421
Iteration 89/1000 | Loss: 0.00001421
Iteration 90/1000 | Loss: 0.00001421
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001418
Iteration 110/1000 | Loss: 0.00001418
Iteration 111/1000 | Loss: 0.00001418
Iteration 112/1000 | Loss: 0.00001418
Iteration 113/1000 | Loss: 0.00001418
Iteration 114/1000 | Loss: 0.00001418
Iteration 115/1000 | Loss: 0.00001418
Iteration 116/1000 | Loss: 0.00001418
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001417
Iteration 121/1000 | Loss: 0.00001417
Iteration 122/1000 | Loss: 0.00001417
Iteration 123/1000 | Loss: 0.00001417
Iteration 124/1000 | Loss: 0.00001417
Iteration 125/1000 | Loss: 0.00001417
Iteration 126/1000 | Loss: 0.00001417
Iteration 127/1000 | Loss: 0.00001417
Iteration 128/1000 | Loss: 0.00001417
Iteration 129/1000 | Loss: 0.00001417
Iteration 130/1000 | Loss: 0.00001417
Iteration 131/1000 | Loss: 0.00001417
Iteration 132/1000 | Loss: 0.00001417
Iteration 133/1000 | Loss: 0.00001417
Iteration 134/1000 | Loss: 0.00001417
Iteration 135/1000 | Loss: 0.00001417
Iteration 136/1000 | Loss: 0.00001417
Iteration 137/1000 | Loss: 0.00001417
Iteration 138/1000 | Loss: 0.00001417
Iteration 139/1000 | Loss: 0.00001417
Iteration 140/1000 | Loss: 0.00001417
Iteration 141/1000 | Loss: 0.00001417
Iteration 142/1000 | Loss: 0.00001417
Iteration 143/1000 | Loss: 0.00001417
Iteration 144/1000 | Loss: 0.00001417
Iteration 145/1000 | Loss: 0.00001417
Iteration 146/1000 | Loss: 0.00001417
Iteration 147/1000 | Loss: 0.00001417
Iteration 148/1000 | Loss: 0.00001417
Iteration 149/1000 | Loss: 0.00001417
Iteration 150/1000 | Loss: 0.00001417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.4169257156027015e-05, 1.4169257156027015e-05, 1.4169257156027015e-05, 1.4169257156027015e-05, 1.4169257156027015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4169257156027015e-05

Optimization complete. Final v2v error: 3.197777032852173 mm

Highest mean error: 3.714995861053467 mm for frame 84

Lowest mean error: 2.8813464641571045 mm for frame 106

Saving results

Total time: 87.07677245140076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541566
Iteration 2/25 | Loss: 0.00130986
Iteration 3/25 | Loss: 0.00112009
Iteration 4/25 | Loss: 0.00110406
Iteration 5/25 | Loss: 0.00110228
Iteration 6/25 | Loss: 0.00110191
Iteration 7/25 | Loss: 0.00110191
Iteration 8/25 | Loss: 0.00110191
Iteration 9/25 | Loss: 0.00110191
Iteration 10/25 | Loss: 0.00110191
Iteration 11/25 | Loss: 0.00110191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011019122321158648, 0.0011019122321158648, 0.0011019122321158648, 0.0011019122321158648, 0.0011019122321158648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011019122321158648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21908867
Iteration 2/25 | Loss: 0.00096987
Iteration 3/25 | Loss: 0.00096983
Iteration 4/25 | Loss: 0.00096983
Iteration 5/25 | Loss: 0.00096983
Iteration 6/25 | Loss: 0.00096983
Iteration 7/25 | Loss: 0.00096983
Iteration 8/25 | Loss: 0.00096983
Iteration 9/25 | Loss: 0.00096983
Iteration 10/25 | Loss: 0.00096983
Iteration 11/25 | Loss: 0.00096983
Iteration 12/25 | Loss: 0.00096983
Iteration 13/25 | Loss: 0.00096983
Iteration 14/25 | Loss: 0.00096983
Iteration 15/25 | Loss: 0.00096983
Iteration 16/25 | Loss: 0.00096983
Iteration 17/25 | Loss: 0.00096983
Iteration 18/25 | Loss: 0.00096983
Iteration 19/25 | Loss: 0.00096983
Iteration 20/25 | Loss: 0.00096983
Iteration 21/25 | Loss: 0.00096983
Iteration 22/25 | Loss: 0.00096983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009698271751403809, 0.0009698271751403809, 0.0009698271751403809, 0.0009698271751403809, 0.0009698271751403809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009698271751403809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096983
Iteration 2/1000 | Loss: 0.00004978
Iteration 3/1000 | Loss: 0.00003230
Iteration 4/1000 | Loss: 0.00002814
Iteration 5/1000 | Loss: 0.00002635
Iteration 6/1000 | Loss: 0.00002526
Iteration 7/1000 | Loss: 0.00002451
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002332
Iteration 10/1000 | Loss: 0.00002290
Iteration 11/1000 | Loss: 0.00002266
Iteration 12/1000 | Loss: 0.00002244
Iteration 13/1000 | Loss: 0.00002230
Iteration 14/1000 | Loss: 0.00002223
Iteration 15/1000 | Loss: 0.00002222
Iteration 16/1000 | Loss: 0.00002221
Iteration 17/1000 | Loss: 0.00002221
Iteration 18/1000 | Loss: 0.00002221
Iteration 19/1000 | Loss: 0.00002220
Iteration 20/1000 | Loss: 0.00002220
Iteration 21/1000 | Loss: 0.00002219
Iteration 22/1000 | Loss: 0.00002218
Iteration 23/1000 | Loss: 0.00002218
Iteration 24/1000 | Loss: 0.00002217
Iteration 25/1000 | Loss: 0.00002217
Iteration 26/1000 | Loss: 0.00002216
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002214
Iteration 29/1000 | Loss: 0.00002213
Iteration 30/1000 | Loss: 0.00002211
Iteration 31/1000 | Loss: 0.00002211
Iteration 32/1000 | Loss: 0.00002210
Iteration 33/1000 | Loss: 0.00002209
Iteration 34/1000 | Loss: 0.00002209
Iteration 35/1000 | Loss: 0.00002208
Iteration 36/1000 | Loss: 0.00002208
Iteration 37/1000 | Loss: 0.00002208
Iteration 38/1000 | Loss: 0.00002207
Iteration 39/1000 | Loss: 0.00002206
Iteration 40/1000 | Loss: 0.00002205
Iteration 41/1000 | Loss: 0.00002205
Iteration 42/1000 | Loss: 0.00002205
Iteration 43/1000 | Loss: 0.00002205
Iteration 44/1000 | Loss: 0.00002205
Iteration 45/1000 | Loss: 0.00002205
Iteration 46/1000 | Loss: 0.00002205
Iteration 47/1000 | Loss: 0.00002204
Iteration 48/1000 | Loss: 0.00002203
Iteration 49/1000 | Loss: 0.00002203
Iteration 50/1000 | Loss: 0.00002203
Iteration 51/1000 | Loss: 0.00002202
Iteration 52/1000 | Loss: 0.00002202
Iteration 53/1000 | Loss: 0.00002201
Iteration 54/1000 | Loss: 0.00002201
Iteration 55/1000 | Loss: 0.00002201
Iteration 56/1000 | Loss: 0.00002200
Iteration 57/1000 | Loss: 0.00002200
Iteration 58/1000 | Loss: 0.00002200
Iteration 59/1000 | Loss: 0.00002199
Iteration 60/1000 | Loss: 0.00002199
Iteration 61/1000 | Loss: 0.00002199
Iteration 62/1000 | Loss: 0.00002199
Iteration 63/1000 | Loss: 0.00002199
Iteration 64/1000 | Loss: 0.00002199
Iteration 65/1000 | Loss: 0.00002198
Iteration 66/1000 | Loss: 0.00002198
Iteration 67/1000 | Loss: 0.00002197
Iteration 68/1000 | Loss: 0.00002196
Iteration 69/1000 | Loss: 0.00002196
Iteration 70/1000 | Loss: 0.00002196
Iteration 71/1000 | Loss: 0.00002195
Iteration 72/1000 | Loss: 0.00002195
Iteration 73/1000 | Loss: 0.00002194
Iteration 74/1000 | Loss: 0.00002194
Iteration 75/1000 | Loss: 0.00002194
Iteration 76/1000 | Loss: 0.00002194
Iteration 77/1000 | Loss: 0.00002194
Iteration 78/1000 | Loss: 0.00002193
Iteration 79/1000 | Loss: 0.00002193
Iteration 80/1000 | Loss: 0.00002193
Iteration 81/1000 | Loss: 0.00002193
Iteration 82/1000 | Loss: 0.00002192
Iteration 83/1000 | Loss: 0.00002192
Iteration 84/1000 | Loss: 0.00002192
Iteration 85/1000 | Loss: 0.00002192
Iteration 86/1000 | Loss: 0.00002191
Iteration 87/1000 | Loss: 0.00002191
Iteration 88/1000 | Loss: 0.00002191
Iteration 89/1000 | Loss: 0.00002191
Iteration 90/1000 | Loss: 0.00002191
Iteration 91/1000 | Loss: 0.00002191
Iteration 92/1000 | Loss: 0.00002191
Iteration 93/1000 | Loss: 0.00002190
Iteration 94/1000 | Loss: 0.00002190
Iteration 95/1000 | Loss: 0.00002190
Iteration 96/1000 | Loss: 0.00002190
Iteration 97/1000 | Loss: 0.00002190
Iteration 98/1000 | Loss: 0.00002190
Iteration 99/1000 | Loss: 0.00002189
Iteration 100/1000 | Loss: 0.00002189
Iteration 101/1000 | Loss: 0.00002189
Iteration 102/1000 | Loss: 0.00002189
Iteration 103/1000 | Loss: 0.00002189
Iteration 104/1000 | Loss: 0.00002189
Iteration 105/1000 | Loss: 0.00002189
Iteration 106/1000 | Loss: 0.00002189
Iteration 107/1000 | Loss: 0.00002189
Iteration 108/1000 | Loss: 0.00002189
Iteration 109/1000 | Loss: 0.00002189
Iteration 110/1000 | Loss: 0.00002188
Iteration 111/1000 | Loss: 0.00002188
Iteration 112/1000 | Loss: 0.00002188
Iteration 113/1000 | Loss: 0.00002188
Iteration 114/1000 | Loss: 0.00002188
Iteration 115/1000 | Loss: 0.00002188
Iteration 116/1000 | Loss: 0.00002188
Iteration 117/1000 | Loss: 0.00002188
Iteration 118/1000 | Loss: 0.00002188
Iteration 119/1000 | Loss: 0.00002188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.1880545318708755e-05, 2.1880545318708755e-05, 2.1880545318708755e-05, 2.1880545318708755e-05, 2.1880545318708755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1880545318708755e-05

Optimization complete. Final v2v error: 4.006207466125488 mm

Highest mean error: 4.620445251464844 mm for frame 150

Lowest mean error: 3.347956657409668 mm for frame 18

Saving results

Total time: 36.59134483337402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957442
Iteration 2/25 | Loss: 0.00135104
Iteration 3/25 | Loss: 0.00108644
Iteration 4/25 | Loss: 0.00106026
Iteration 5/25 | Loss: 0.00105134
Iteration 6/25 | Loss: 0.00104972
Iteration 7/25 | Loss: 0.00104961
Iteration 8/25 | Loss: 0.00104961
Iteration 9/25 | Loss: 0.00104961
Iteration 10/25 | Loss: 0.00104961
Iteration 11/25 | Loss: 0.00104961
Iteration 12/25 | Loss: 0.00104961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010496111353859305, 0.0010496111353859305, 0.0010496111353859305, 0.0010496111353859305, 0.0010496111353859305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010496111353859305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93153036
Iteration 2/25 | Loss: 0.00107276
Iteration 3/25 | Loss: 0.00107276
Iteration 4/25 | Loss: 0.00107276
Iteration 5/25 | Loss: 0.00107276
Iteration 6/25 | Loss: 0.00107276
Iteration 7/25 | Loss: 0.00107276
Iteration 8/25 | Loss: 0.00107276
Iteration 9/25 | Loss: 0.00107276
Iteration 10/25 | Loss: 0.00107276
Iteration 11/25 | Loss: 0.00107276
Iteration 12/25 | Loss: 0.00107276
Iteration 13/25 | Loss: 0.00107276
Iteration 14/25 | Loss: 0.00107276
Iteration 15/25 | Loss: 0.00107276
Iteration 16/25 | Loss: 0.00107276
Iteration 17/25 | Loss: 0.00107276
Iteration 18/25 | Loss: 0.00107276
Iteration 19/25 | Loss: 0.00107276
Iteration 20/25 | Loss: 0.00107276
Iteration 21/25 | Loss: 0.00107276
Iteration 22/25 | Loss: 0.00107276
Iteration 23/25 | Loss: 0.00107276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010727564804255962, 0.0010727564804255962, 0.0010727564804255962, 0.0010727564804255962, 0.0010727564804255962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010727564804255962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107276
Iteration 2/1000 | Loss: 0.00007551
Iteration 3/1000 | Loss: 0.00004954
Iteration 4/1000 | Loss: 0.00003565
Iteration 5/1000 | Loss: 0.00003256
Iteration 6/1000 | Loss: 0.00003148
Iteration 7/1000 | Loss: 0.00003070
Iteration 8/1000 | Loss: 0.00003026
Iteration 9/1000 | Loss: 0.00002984
Iteration 10/1000 | Loss: 0.00002957
Iteration 11/1000 | Loss: 0.00002930
Iteration 12/1000 | Loss: 0.00002916
Iteration 13/1000 | Loss: 0.00002910
Iteration 14/1000 | Loss: 0.00002889
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00002852
Iteration 17/1000 | Loss: 0.00002834
Iteration 18/1000 | Loss: 0.00002814
Iteration 19/1000 | Loss: 0.00002797
Iteration 20/1000 | Loss: 0.00002784
Iteration 21/1000 | Loss: 0.00002768
Iteration 22/1000 | Loss: 0.00002763
Iteration 23/1000 | Loss: 0.00002755
Iteration 24/1000 | Loss: 0.00002754
Iteration 25/1000 | Loss: 0.00002750
Iteration 26/1000 | Loss: 0.00002750
Iteration 27/1000 | Loss: 0.00002748
Iteration 28/1000 | Loss: 0.00002747
Iteration 29/1000 | Loss: 0.00002747
Iteration 30/1000 | Loss: 0.00002746
Iteration 31/1000 | Loss: 0.00002746
Iteration 32/1000 | Loss: 0.00002745
Iteration 33/1000 | Loss: 0.00002744
Iteration 34/1000 | Loss: 0.00002744
Iteration 35/1000 | Loss: 0.00002743
Iteration 36/1000 | Loss: 0.00002742
Iteration 37/1000 | Loss: 0.00002741
Iteration 38/1000 | Loss: 0.00002741
Iteration 39/1000 | Loss: 0.00002739
Iteration 40/1000 | Loss: 0.00002739
Iteration 41/1000 | Loss: 0.00002738
Iteration 42/1000 | Loss: 0.00002738
Iteration 43/1000 | Loss: 0.00002738
Iteration 44/1000 | Loss: 0.00002737
Iteration 45/1000 | Loss: 0.00002736
Iteration 46/1000 | Loss: 0.00002736
Iteration 47/1000 | Loss: 0.00002736
Iteration 48/1000 | Loss: 0.00002736
Iteration 49/1000 | Loss: 0.00002736
Iteration 50/1000 | Loss: 0.00002736
Iteration 51/1000 | Loss: 0.00002736
Iteration 52/1000 | Loss: 0.00002735
Iteration 53/1000 | Loss: 0.00002735
Iteration 54/1000 | Loss: 0.00002735
Iteration 55/1000 | Loss: 0.00002735
Iteration 56/1000 | Loss: 0.00002735
Iteration 57/1000 | Loss: 0.00002735
Iteration 58/1000 | Loss: 0.00002735
Iteration 59/1000 | Loss: 0.00002735
Iteration 60/1000 | Loss: 0.00002735
Iteration 61/1000 | Loss: 0.00002734
Iteration 62/1000 | Loss: 0.00002734
Iteration 63/1000 | Loss: 0.00002733
Iteration 64/1000 | Loss: 0.00002733
Iteration 65/1000 | Loss: 0.00002733
Iteration 66/1000 | Loss: 0.00002733
Iteration 67/1000 | Loss: 0.00002733
Iteration 68/1000 | Loss: 0.00002733
Iteration 69/1000 | Loss: 0.00002733
Iteration 70/1000 | Loss: 0.00002732
Iteration 71/1000 | Loss: 0.00002732
Iteration 72/1000 | Loss: 0.00002732
Iteration 73/1000 | Loss: 0.00002732
Iteration 74/1000 | Loss: 0.00002731
Iteration 75/1000 | Loss: 0.00002731
Iteration 76/1000 | Loss: 0.00002731
Iteration 77/1000 | Loss: 0.00002731
Iteration 78/1000 | Loss: 0.00002731
Iteration 79/1000 | Loss: 0.00002731
Iteration 80/1000 | Loss: 0.00002731
Iteration 81/1000 | Loss: 0.00002731
Iteration 82/1000 | Loss: 0.00002731
Iteration 83/1000 | Loss: 0.00002731
Iteration 84/1000 | Loss: 0.00002730
Iteration 85/1000 | Loss: 0.00002730
Iteration 86/1000 | Loss: 0.00002729
Iteration 87/1000 | Loss: 0.00002729
Iteration 88/1000 | Loss: 0.00002729
Iteration 89/1000 | Loss: 0.00002728
Iteration 90/1000 | Loss: 0.00002728
Iteration 91/1000 | Loss: 0.00002727
Iteration 92/1000 | Loss: 0.00002727
Iteration 93/1000 | Loss: 0.00002726
Iteration 94/1000 | Loss: 0.00002726
Iteration 95/1000 | Loss: 0.00002726
Iteration 96/1000 | Loss: 0.00002726
Iteration 97/1000 | Loss: 0.00002726
Iteration 98/1000 | Loss: 0.00002725
Iteration 99/1000 | Loss: 0.00002725
Iteration 100/1000 | Loss: 0.00002724
Iteration 101/1000 | Loss: 0.00002724
Iteration 102/1000 | Loss: 0.00002724
Iteration 103/1000 | Loss: 0.00002724
Iteration 104/1000 | Loss: 0.00002723
Iteration 105/1000 | Loss: 0.00002723
Iteration 106/1000 | Loss: 0.00002723
Iteration 107/1000 | Loss: 0.00002723
Iteration 108/1000 | Loss: 0.00002723
Iteration 109/1000 | Loss: 0.00002723
Iteration 110/1000 | Loss: 0.00002723
Iteration 111/1000 | Loss: 0.00002722
Iteration 112/1000 | Loss: 0.00002722
Iteration 113/1000 | Loss: 0.00002722
Iteration 114/1000 | Loss: 0.00002722
Iteration 115/1000 | Loss: 0.00002721
Iteration 116/1000 | Loss: 0.00002721
Iteration 117/1000 | Loss: 0.00002721
Iteration 118/1000 | Loss: 0.00002721
Iteration 119/1000 | Loss: 0.00002721
Iteration 120/1000 | Loss: 0.00002720
Iteration 121/1000 | Loss: 0.00002720
Iteration 122/1000 | Loss: 0.00002720
Iteration 123/1000 | Loss: 0.00002719
Iteration 124/1000 | Loss: 0.00002719
Iteration 125/1000 | Loss: 0.00002719
Iteration 126/1000 | Loss: 0.00002719
Iteration 127/1000 | Loss: 0.00002719
Iteration 128/1000 | Loss: 0.00002719
Iteration 129/1000 | Loss: 0.00002719
Iteration 130/1000 | Loss: 0.00002719
Iteration 131/1000 | Loss: 0.00002719
Iteration 132/1000 | Loss: 0.00002719
Iteration 133/1000 | Loss: 0.00002719
Iteration 134/1000 | Loss: 0.00002718
Iteration 135/1000 | Loss: 0.00002718
Iteration 136/1000 | Loss: 0.00002718
Iteration 137/1000 | Loss: 0.00002718
Iteration 138/1000 | Loss: 0.00002718
Iteration 139/1000 | Loss: 0.00002718
Iteration 140/1000 | Loss: 0.00002718
Iteration 141/1000 | Loss: 0.00002718
Iteration 142/1000 | Loss: 0.00002718
Iteration 143/1000 | Loss: 0.00002718
Iteration 144/1000 | Loss: 0.00002718
Iteration 145/1000 | Loss: 0.00002718
Iteration 146/1000 | Loss: 0.00002718
Iteration 147/1000 | Loss: 0.00002718
Iteration 148/1000 | Loss: 0.00002718
Iteration 149/1000 | Loss: 0.00002718
Iteration 150/1000 | Loss: 0.00002718
Iteration 151/1000 | Loss: 0.00002717
Iteration 152/1000 | Loss: 0.00002717
Iteration 153/1000 | Loss: 0.00002717
Iteration 154/1000 | Loss: 0.00002717
Iteration 155/1000 | Loss: 0.00002717
Iteration 156/1000 | Loss: 0.00002717
Iteration 157/1000 | Loss: 0.00002717
Iteration 158/1000 | Loss: 0.00002717
Iteration 159/1000 | Loss: 0.00002717
Iteration 160/1000 | Loss: 0.00002716
Iteration 161/1000 | Loss: 0.00002716
Iteration 162/1000 | Loss: 0.00002716
Iteration 163/1000 | Loss: 0.00002716
Iteration 164/1000 | Loss: 0.00002716
Iteration 165/1000 | Loss: 0.00002716
Iteration 166/1000 | Loss: 0.00002716
Iteration 167/1000 | Loss: 0.00002716
Iteration 168/1000 | Loss: 0.00002716
Iteration 169/1000 | Loss: 0.00002715
Iteration 170/1000 | Loss: 0.00002715
Iteration 171/1000 | Loss: 0.00002715
Iteration 172/1000 | Loss: 0.00002715
Iteration 173/1000 | Loss: 0.00002715
Iteration 174/1000 | Loss: 0.00002715
Iteration 175/1000 | Loss: 0.00002715
Iteration 176/1000 | Loss: 0.00002715
Iteration 177/1000 | Loss: 0.00002715
Iteration 178/1000 | Loss: 0.00002715
Iteration 179/1000 | Loss: 0.00002715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.7151500034960918e-05, 2.7151500034960918e-05, 2.7151500034960918e-05, 2.7151500034960918e-05, 2.7151500034960918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7151500034960918e-05

Optimization complete. Final v2v error: 4.260003566741943 mm

Highest mean error: 4.979567050933838 mm for frame 79

Lowest mean error: 3.358766555786133 mm for frame 172

Saving results

Total time: 52.42078518867493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487097
Iteration 2/25 | Loss: 0.00116618
Iteration 3/25 | Loss: 0.00105648
Iteration 4/25 | Loss: 0.00102827
Iteration 5/25 | Loss: 0.00101651
Iteration 6/25 | Loss: 0.00101340
Iteration 7/25 | Loss: 0.00101300
Iteration 8/25 | Loss: 0.00101300
Iteration 9/25 | Loss: 0.00101300
Iteration 10/25 | Loss: 0.00101300
Iteration 11/25 | Loss: 0.00101300
Iteration 12/25 | Loss: 0.00101300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010130046866834164, 0.0010130046866834164, 0.0010130046866834164, 0.0010130046866834164, 0.0010130046866834164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010130046866834164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44380891
Iteration 2/25 | Loss: 0.00166803
Iteration 3/25 | Loss: 0.00166803
Iteration 4/25 | Loss: 0.00166803
Iteration 5/25 | Loss: 0.00166803
Iteration 6/25 | Loss: 0.00166803
Iteration 7/25 | Loss: 0.00166803
Iteration 8/25 | Loss: 0.00166803
Iteration 9/25 | Loss: 0.00166803
Iteration 10/25 | Loss: 0.00166803
Iteration 11/25 | Loss: 0.00166803
Iteration 12/25 | Loss: 0.00166803
Iteration 13/25 | Loss: 0.00166803
Iteration 14/25 | Loss: 0.00166803
Iteration 15/25 | Loss: 0.00166803
Iteration 16/25 | Loss: 0.00166803
Iteration 17/25 | Loss: 0.00166803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016680260887369514, 0.0016680260887369514, 0.0016680260887369514, 0.0016680260887369514, 0.0016680260887369514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016680260887369514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166803
Iteration 2/1000 | Loss: 0.00005733
Iteration 3/1000 | Loss: 0.00003750
Iteration 4/1000 | Loss: 0.00002762
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00002382
Iteration 7/1000 | Loss: 0.00002283
Iteration 8/1000 | Loss: 0.00002227
Iteration 9/1000 | Loss: 0.00002194
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002151
Iteration 13/1000 | Loss: 0.00002136
Iteration 14/1000 | Loss: 0.00002134
Iteration 15/1000 | Loss: 0.00002132
Iteration 16/1000 | Loss: 0.00002130
Iteration 17/1000 | Loss: 0.00002128
Iteration 18/1000 | Loss: 0.00002126
Iteration 19/1000 | Loss: 0.00002125
Iteration 20/1000 | Loss: 0.00002120
Iteration 21/1000 | Loss: 0.00002117
Iteration 22/1000 | Loss: 0.00002117
Iteration 23/1000 | Loss: 0.00002116
Iteration 24/1000 | Loss: 0.00002115
Iteration 25/1000 | Loss: 0.00002114
Iteration 26/1000 | Loss: 0.00002114
Iteration 27/1000 | Loss: 0.00002113
Iteration 28/1000 | Loss: 0.00002112
Iteration 29/1000 | Loss: 0.00002111
Iteration 30/1000 | Loss: 0.00002111
Iteration 31/1000 | Loss: 0.00002110
Iteration 32/1000 | Loss: 0.00002110
Iteration 33/1000 | Loss: 0.00002110
Iteration 34/1000 | Loss: 0.00002110
Iteration 35/1000 | Loss: 0.00002109
Iteration 36/1000 | Loss: 0.00002108
Iteration 37/1000 | Loss: 0.00002108
Iteration 38/1000 | Loss: 0.00002107
Iteration 39/1000 | Loss: 0.00002107
Iteration 40/1000 | Loss: 0.00002107
Iteration 41/1000 | Loss: 0.00002106
Iteration 42/1000 | Loss: 0.00002105
Iteration 43/1000 | Loss: 0.00002105
Iteration 44/1000 | Loss: 0.00002105
Iteration 45/1000 | Loss: 0.00002104
Iteration 46/1000 | Loss: 0.00002104
Iteration 47/1000 | Loss: 0.00002103
Iteration 48/1000 | Loss: 0.00002103
Iteration 49/1000 | Loss: 0.00002103
Iteration 50/1000 | Loss: 0.00002102
Iteration 51/1000 | Loss: 0.00002102
Iteration 52/1000 | Loss: 0.00002101
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002100
Iteration 56/1000 | Loss: 0.00002100
Iteration 57/1000 | Loss: 0.00002100
Iteration 58/1000 | Loss: 0.00002099
Iteration 59/1000 | Loss: 0.00002099
Iteration 60/1000 | Loss: 0.00002099
Iteration 61/1000 | Loss: 0.00002098
Iteration 62/1000 | Loss: 0.00002098
Iteration 63/1000 | Loss: 0.00002098
Iteration 64/1000 | Loss: 0.00002097
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002097
Iteration 67/1000 | Loss: 0.00002097
Iteration 68/1000 | Loss: 0.00002097
Iteration 69/1000 | Loss: 0.00002097
Iteration 70/1000 | Loss: 0.00002097
Iteration 71/1000 | Loss: 0.00002097
Iteration 72/1000 | Loss: 0.00002097
Iteration 73/1000 | Loss: 0.00002097
Iteration 74/1000 | Loss: 0.00002097
Iteration 75/1000 | Loss: 0.00002097
Iteration 76/1000 | Loss: 0.00002097
Iteration 77/1000 | Loss: 0.00002097
Iteration 78/1000 | Loss: 0.00002097
Iteration 79/1000 | Loss: 0.00002097
Iteration 80/1000 | Loss: 0.00002097
Iteration 81/1000 | Loss: 0.00002097
Iteration 82/1000 | Loss: 0.00002097
Iteration 83/1000 | Loss: 0.00002097
Iteration 84/1000 | Loss: 0.00002097
Iteration 85/1000 | Loss: 0.00002097
Iteration 86/1000 | Loss: 0.00002097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [2.097274955303874e-05, 2.097274955303874e-05, 2.097274955303874e-05, 2.097274955303874e-05, 2.097274955303874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.097274955303874e-05

Optimization complete. Final v2v error: 3.780674457550049 mm

Highest mean error: 4.5014519691467285 mm for frame 40

Lowest mean error: 3.249671697616577 mm for frame 147

Saving results

Total time: 38.72335958480835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371203
Iteration 2/25 | Loss: 0.00114839
Iteration 3/25 | Loss: 0.00099002
Iteration 4/25 | Loss: 0.00096070
Iteration 5/25 | Loss: 0.00095413
Iteration 6/25 | Loss: 0.00095149
Iteration 7/25 | Loss: 0.00095091
Iteration 8/25 | Loss: 0.00095091
Iteration 9/25 | Loss: 0.00095091
Iteration 10/25 | Loss: 0.00095091
Iteration 11/25 | Loss: 0.00095091
Iteration 12/25 | Loss: 0.00095091
Iteration 13/25 | Loss: 0.00095091
Iteration 14/25 | Loss: 0.00095091
Iteration 15/25 | Loss: 0.00095091
Iteration 16/25 | Loss: 0.00095091
Iteration 17/25 | Loss: 0.00095091
Iteration 18/25 | Loss: 0.00095091
Iteration 19/25 | Loss: 0.00095091
Iteration 20/25 | Loss: 0.00095091
Iteration 21/25 | Loss: 0.00095091
Iteration 22/25 | Loss: 0.00095091
Iteration 23/25 | Loss: 0.00095091
Iteration 24/25 | Loss: 0.00095091
Iteration 25/25 | Loss: 0.00095091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24050009
Iteration 2/25 | Loss: 0.00150198
Iteration 3/25 | Loss: 0.00150197
Iteration 4/25 | Loss: 0.00150197
Iteration 5/25 | Loss: 0.00150197
Iteration 6/25 | Loss: 0.00150197
Iteration 7/25 | Loss: 0.00150197
Iteration 8/25 | Loss: 0.00150197
Iteration 9/25 | Loss: 0.00150197
Iteration 10/25 | Loss: 0.00150197
Iteration 11/25 | Loss: 0.00150197
Iteration 12/25 | Loss: 0.00150197
Iteration 13/25 | Loss: 0.00150197
Iteration 14/25 | Loss: 0.00150197
Iteration 15/25 | Loss: 0.00150197
Iteration 16/25 | Loss: 0.00150197
Iteration 17/25 | Loss: 0.00150197
Iteration 18/25 | Loss: 0.00150197
Iteration 19/25 | Loss: 0.00150197
Iteration 20/25 | Loss: 0.00150197
Iteration 21/25 | Loss: 0.00150197
Iteration 22/25 | Loss: 0.00150197
Iteration 23/25 | Loss: 0.00150197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015019708080217242, 0.0015019708080217242, 0.0015019708080217242, 0.0015019708080217242, 0.0015019708080217242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015019708080217242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150197
Iteration 2/1000 | Loss: 0.00003145
Iteration 3/1000 | Loss: 0.00001972
Iteration 4/1000 | Loss: 0.00001646
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001403
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001275
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001250
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001243
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001238
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001236
Iteration 40/1000 | Loss: 0.00001236
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001235
Iteration 44/1000 | Loss: 0.00001235
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001235
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001234
Iteration 51/1000 | Loss: 0.00001234
Iteration 52/1000 | Loss: 0.00001234
Iteration 53/1000 | Loss: 0.00001234
Iteration 54/1000 | Loss: 0.00001233
Iteration 55/1000 | Loss: 0.00001233
Iteration 56/1000 | Loss: 0.00001232
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001232
Iteration 59/1000 | Loss: 0.00001232
Iteration 60/1000 | Loss: 0.00001232
Iteration 61/1000 | Loss: 0.00001232
Iteration 62/1000 | Loss: 0.00001232
Iteration 63/1000 | Loss: 0.00001232
Iteration 64/1000 | Loss: 0.00001232
Iteration 65/1000 | Loss: 0.00001232
Iteration 66/1000 | Loss: 0.00001232
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001231
Iteration 69/1000 | Loss: 0.00001231
Iteration 70/1000 | Loss: 0.00001231
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001228
Iteration 95/1000 | Loss: 0.00001228
Iteration 96/1000 | Loss: 0.00001228
Iteration 97/1000 | Loss: 0.00001228
Iteration 98/1000 | Loss: 0.00001228
Iteration 99/1000 | Loss: 0.00001228
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001227
Iteration 112/1000 | Loss: 0.00001227
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001225
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001225
Iteration 148/1000 | Loss: 0.00001225
Iteration 149/1000 | Loss: 0.00001225
Iteration 150/1000 | Loss: 0.00001225
Iteration 151/1000 | Loss: 0.00001225
Iteration 152/1000 | Loss: 0.00001225
Iteration 153/1000 | Loss: 0.00001225
Iteration 154/1000 | Loss: 0.00001225
Iteration 155/1000 | Loss: 0.00001225
Iteration 156/1000 | Loss: 0.00001225
Iteration 157/1000 | Loss: 0.00001225
Iteration 158/1000 | Loss: 0.00001225
Iteration 159/1000 | Loss: 0.00001225
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Iteration 163/1000 | Loss: 0.00001225
Iteration 164/1000 | Loss: 0.00001225
Iteration 165/1000 | Loss: 0.00001225
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001225
Iteration 169/1000 | Loss: 0.00001225
Iteration 170/1000 | Loss: 0.00001225
Iteration 171/1000 | Loss: 0.00001225
Iteration 172/1000 | Loss: 0.00001225
Iteration 173/1000 | Loss: 0.00001225
Iteration 174/1000 | Loss: 0.00001225
Iteration 175/1000 | Loss: 0.00001225
Iteration 176/1000 | Loss: 0.00001225
Iteration 177/1000 | Loss: 0.00001225
Iteration 178/1000 | Loss: 0.00001225
Iteration 179/1000 | Loss: 0.00001225
Iteration 180/1000 | Loss: 0.00001225
Iteration 181/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.2254557987034786e-05, 1.2254557987034786e-05, 1.2254557987034786e-05, 1.2254557987034786e-05, 1.2254557987034786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2254557987034786e-05

Optimization complete. Final v2v error: 2.916757822036743 mm

Highest mean error: 3.1447393894195557 mm for frame 7

Lowest mean error: 2.7265520095825195 mm for frame 92

Saving results

Total time: 37.52668881416321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477810
Iteration 2/25 | Loss: 0.00118783
Iteration 3/25 | Loss: 0.00102375
Iteration 4/25 | Loss: 0.00100705
Iteration 5/25 | Loss: 0.00099896
Iteration 6/25 | Loss: 0.00099682
Iteration 7/25 | Loss: 0.00099682
Iteration 8/25 | Loss: 0.00099682
Iteration 9/25 | Loss: 0.00099682
Iteration 10/25 | Loss: 0.00099682
Iteration 11/25 | Loss: 0.00099682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009968242375180125, 0.0009968242375180125, 0.0009968242375180125, 0.0009968242375180125, 0.0009968242375180125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009968242375180125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70183802
Iteration 2/25 | Loss: 0.00114708
Iteration 3/25 | Loss: 0.00114707
Iteration 4/25 | Loss: 0.00114707
Iteration 5/25 | Loss: 0.00114707
Iteration 6/25 | Loss: 0.00114707
Iteration 7/25 | Loss: 0.00114707
Iteration 8/25 | Loss: 0.00114707
Iteration 9/25 | Loss: 0.00114707
Iteration 10/25 | Loss: 0.00114707
Iteration 11/25 | Loss: 0.00114707
Iteration 12/25 | Loss: 0.00114707
Iteration 13/25 | Loss: 0.00114707
Iteration 14/25 | Loss: 0.00114707
Iteration 15/25 | Loss: 0.00114707
Iteration 16/25 | Loss: 0.00114707
Iteration 17/25 | Loss: 0.00114707
Iteration 18/25 | Loss: 0.00114707
Iteration 19/25 | Loss: 0.00114707
Iteration 20/25 | Loss: 0.00114707
Iteration 21/25 | Loss: 0.00114707
Iteration 22/25 | Loss: 0.00114707
Iteration 23/25 | Loss: 0.00114707
Iteration 24/25 | Loss: 0.00114707
Iteration 25/25 | Loss: 0.00114707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114707
Iteration 2/1000 | Loss: 0.00005782
Iteration 3/1000 | Loss: 0.00003992
Iteration 4/1000 | Loss: 0.00003393
Iteration 5/1000 | Loss: 0.00003201
Iteration 6/1000 | Loss: 0.00003092
Iteration 7/1000 | Loss: 0.00003020
Iteration 8/1000 | Loss: 0.00002960
Iteration 9/1000 | Loss: 0.00002898
Iteration 10/1000 | Loss: 0.00002853
Iteration 11/1000 | Loss: 0.00002824
Iteration 12/1000 | Loss: 0.00002805
Iteration 13/1000 | Loss: 0.00002787
Iteration 14/1000 | Loss: 0.00002767
Iteration 15/1000 | Loss: 0.00002746
Iteration 16/1000 | Loss: 0.00002726
Iteration 17/1000 | Loss: 0.00002709
Iteration 18/1000 | Loss: 0.00002691
Iteration 19/1000 | Loss: 0.00002687
Iteration 20/1000 | Loss: 0.00002667
Iteration 21/1000 | Loss: 0.00002648
Iteration 22/1000 | Loss: 0.00002625
Iteration 23/1000 | Loss: 0.00002611
Iteration 24/1000 | Loss: 0.00002604
Iteration 25/1000 | Loss: 0.00002596
Iteration 26/1000 | Loss: 0.00002588
Iteration 27/1000 | Loss: 0.00002585
Iteration 28/1000 | Loss: 0.00002585
Iteration 29/1000 | Loss: 0.00002585
Iteration 30/1000 | Loss: 0.00002584
Iteration 31/1000 | Loss: 0.00002584
Iteration 32/1000 | Loss: 0.00002583
Iteration 33/1000 | Loss: 0.00002583
Iteration 34/1000 | Loss: 0.00002582
Iteration 35/1000 | Loss: 0.00002581
Iteration 36/1000 | Loss: 0.00002577
Iteration 37/1000 | Loss: 0.00002570
Iteration 38/1000 | Loss: 0.00002570
Iteration 39/1000 | Loss: 0.00002569
Iteration 40/1000 | Loss: 0.00002568
Iteration 41/1000 | Loss: 0.00002568
Iteration 42/1000 | Loss: 0.00002567
Iteration 43/1000 | Loss: 0.00002567
Iteration 44/1000 | Loss: 0.00002566
Iteration 45/1000 | Loss: 0.00002566
Iteration 46/1000 | Loss: 0.00002566
Iteration 47/1000 | Loss: 0.00002566
Iteration 48/1000 | Loss: 0.00002565
Iteration 49/1000 | Loss: 0.00002565
Iteration 50/1000 | Loss: 0.00002565
Iteration 51/1000 | Loss: 0.00002565
Iteration 52/1000 | Loss: 0.00002565
Iteration 53/1000 | Loss: 0.00002564
Iteration 54/1000 | Loss: 0.00002564
Iteration 55/1000 | Loss: 0.00002564
Iteration 56/1000 | Loss: 0.00002564
Iteration 57/1000 | Loss: 0.00002564
Iteration 58/1000 | Loss: 0.00002563
Iteration 59/1000 | Loss: 0.00002563
Iteration 60/1000 | Loss: 0.00002563
Iteration 61/1000 | Loss: 0.00002562
Iteration 62/1000 | Loss: 0.00002562
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00002562
Iteration 65/1000 | Loss: 0.00002562
Iteration 66/1000 | Loss: 0.00002562
Iteration 67/1000 | Loss: 0.00002562
Iteration 68/1000 | Loss: 0.00002561
Iteration 69/1000 | Loss: 0.00002561
Iteration 70/1000 | Loss: 0.00002561
Iteration 71/1000 | Loss: 0.00002560
Iteration 72/1000 | Loss: 0.00002560
Iteration 73/1000 | Loss: 0.00002560
Iteration 74/1000 | Loss: 0.00002560
Iteration 75/1000 | Loss: 0.00002560
Iteration 76/1000 | Loss: 0.00002560
Iteration 77/1000 | Loss: 0.00002560
Iteration 78/1000 | Loss: 0.00002560
Iteration 79/1000 | Loss: 0.00002560
Iteration 80/1000 | Loss: 0.00002560
Iteration 81/1000 | Loss: 0.00002560
Iteration 82/1000 | Loss: 0.00002560
Iteration 83/1000 | Loss: 0.00002560
Iteration 84/1000 | Loss: 0.00002559
Iteration 85/1000 | Loss: 0.00002559
Iteration 86/1000 | Loss: 0.00002559
Iteration 87/1000 | Loss: 0.00002559
Iteration 88/1000 | Loss: 0.00002559
Iteration 89/1000 | Loss: 0.00002559
Iteration 90/1000 | Loss: 0.00002557
Iteration 91/1000 | Loss: 0.00002557
Iteration 92/1000 | Loss: 0.00002557
Iteration 93/1000 | Loss: 0.00002556
Iteration 94/1000 | Loss: 0.00002556
Iteration 95/1000 | Loss: 0.00002556
Iteration 96/1000 | Loss: 0.00002556
Iteration 97/1000 | Loss: 0.00002556
Iteration 98/1000 | Loss: 0.00002556
Iteration 99/1000 | Loss: 0.00002555
Iteration 100/1000 | Loss: 0.00002555
Iteration 101/1000 | Loss: 0.00002555
Iteration 102/1000 | Loss: 0.00002555
Iteration 103/1000 | Loss: 0.00002554
Iteration 104/1000 | Loss: 0.00002554
Iteration 105/1000 | Loss: 0.00002554
Iteration 106/1000 | Loss: 0.00002553
Iteration 107/1000 | Loss: 0.00002553
Iteration 108/1000 | Loss: 0.00002552
Iteration 109/1000 | Loss: 0.00002550
Iteration 110/1000 | Loss: 0.00002550
Iteration 111/1000 | Loss: 0.00002550
Iteration 112/1000 | Loss: 0.00002549
Iteration 113/1000 | Loss: 0.00002549
Iteration 114/1000 | Loss: 0.00002549
Iteration 115/1000 | Loss: 0.00002549
Iteration 116/1000 | Loss: 0.00002549
Iteration 117/1000 | Loss: 0.00002549
Iteration 118/1000 | Loss: 0.00002549
Iteration 119/1000 | Loss: 0.00002548
Iteration 120/1000 | Loss: 0.00002548
Iteration 121/1000 | Loss: 0.00002548
Iteration 122/1000 | Loss: 0.00002548
Iteration 123/1000 | Loss: 0.00002548
Iteration 124/1000 | Loss: 0.00002548
Iteration 125/1000 | Loss: 0.00002548
Iteration 126/1000 | Loss: 0.00002548
Iteration 127/1000 | Loss: 0.00002548
Iteration 128/1000 | Loss: 0.00002547
Iteration 129/1000 | Loss: 0.00002547
Iteration 130/1000 | Loss: 0.00002547
Iteration 131/1000 | Loss: 0.00002547
Iteration 132/1000 | Loss: 0.00002547
Iteration 133/1000 | Loss: 0.00002547
Iteration 134/1000 | Loss: 0.00002547
Iteration 135/1000 | Loss: 0.00002546
Iteration 136/1000 | Loss: 0.00002546
Iteration 137/1000 | Loss: 0.00002546
Iteration 138/1000 | Loss: 0.00002546
Iteration 139/1000 | Loss: 0.00002545
Iteration 140/1000 | Loss: 0.00002545
Iteration 141/1000 | Loss: 0.00002545
Iteration 142/1000 | Loss: 0.00002545
Iteration 143/1000 | Loss: 0.00002545
Iteration 144/1000 | Loss: 0.00002545
Iteration 145/1000 | Loss: 0.00002545
Iteration 146/1000 | Loss: 0.00002545
Iteration 147/1000 | Loss: 0.00002544
Iteration 148/1000 | Loss: 0.00002544
Iteration 149/1000 | Loss: 0.00002544
Iteration 150/1000 | Loss: 0.00002544
Iteration 151/1000 | Loss: 0.00002544
Iteration 152/1000 | Loss: 0.00002544
Iteration 153/1000 | Loss: 0.00002544
Iteration 154/1000 | Loss: 0.00002544
Iteration 155/1000 | Loss: 0.00002544
Iteration 156/1000 | Loss: 0.00002544
Iteration 157/1000 | Loss: 0.00002544
Iteration 158/1000 | Loss: 0.00002544
Iteration 159/1000 | Loss: 0.00002543
Iteration 160/1000 | Loss: 0.00002543
Iteration 161/1000 | Loss: 0.00002543
Iteration 162/1000 | Loss: 0.00002543
Iteration 163/1000 | Loss: 0.00002543
Iteration 164/1000 | Loss: 0.00002543
Iteration 165/1000 | Loss: 0.00002543
Iteration 166/1000 | Loss: 0.00002543
Iteration 167/1000 | Loss: 0.00002543
Iteration 168/1000 | Loss: 0.00002543
Iteration 169/1000 | Loss: 0.00002543
Iteration 170/1000 | Loss: 0.00002543
Iteration 171/1000 | Loss: 0.00002543
Iteration 172/1000 | Loss: 0.00002543
Iteration 173/1000 | Loss: 0.00002543
Iteration 174/1000 | Loss: 0.00002543
Iteration 175/1000 | Loss: 0.00002543
Iteration 176/1000 | Loss: 0.00002543
Iteration 177/1000 | Loss: 0.00002543
Iteration 178/1000 | Loss: 0.00002542
Iteration 179/1000 | Loss: 0.00002542
Iteration 180/1000 | Loss: 0.00002542
Iteration 181/1000 | Loss: 0.00002542
Iteration 182/1000 | Loss: 0.00002542
Iteration 183/1000 | Loss: 0.00002542
Iteration 184/1000 | Loss: 0.00002542
Iteration 185/1000 | Loss: 0.00002542
Iteration 186/1000 | Loss: 0.00002542
Iteration 187/1000 | Loss: 0.00002542
Iteration 188/1000 | Loss: 0.00002542
Iteration 189/1000 | Loss: 0.00002542
Iteration 190/1000 | Loss: 0.00002542
Iteration 191/1000 | Loss: 0.00002542
Iteration 192/1000 | Loss: 0.00002542
Iteration 193/1000 | Loss: 0.00002542
Iteration 194/1000 | Loss: 0.00002542
Iteration 195/1000 | Loss: 0.00002542
Iteration 196/1000 | Loss: 0.00002542
Iteration 197/1000 | Loss: 0.00002542
Iteration 198/1000 | Loss: 0.00002542
Iteration 199/1000 | Loss: 0.00002542
Iteration 200/1000 | Loss: 0.00002542
Iteration 201/1000 | Loss: 0.00002542
Iteration 202/1000 | Loss: 0.00002542
Iteration 203/1000 | Loss: 0.00002542
Iteration 204/1000 | Loss: 0.00002542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.5417741198907606e-05, 2.5417741198907606e-05, 2.5417741198907606e-05, 2.5417741198907606e-05, 2.5417741198907606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5417741198907606e-05

Optimization complete. Final v2v error: 4.165005207061768 mm

Highest mean error: 4.361337184906006 mm for frame 113

Lowest mean error: 3.8834121227264404 mm for frame 187

Saving results

Total time: 66.65879893302917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887814
Iteration 2/25 | Loss: 0.00121504
Iteration 3/25 | Loss: 0.00102774
Iteration 4/25 | Loss: 0.00101576
Iteration 5/25 | Loss: 0.00101263
Iteration 6/25 | Loss: 0.00101249
Iteration 7/25 | Loss: 0.00101246
Iteration 8/25 | Loss: 0.00101246
Iteration 9/25 | Loss: 0.00101246
Iteration 10/25 | Loss: 0.00101246
Iteration 11/25 | Loss: 0.00101246
Iteration 12/25 | Loss: 0.00101246
Iteration 13/25 | Loss: 0.00101246
Iteration 14/25 | Loss: 0.00101246
Iteration 15/25 | Loss: 0.00101246
Iteration 16/25 | Loss: 0.00101246
Iteration 17/25 | Loss: 0.00101246
Iteration 18/25 | Loss: 0.00101246
Iteration 19/25 | Loss: 0.00101246
Iteration 20/25 | Loss: 0.00101246
Iteration 21/25 | Loss: 0.00101246
Iteration 22/25 | Loss: 0.00101246
Iteration 23/25 | Loss: 0.00101246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001012455322779715, 0.001012455322779715, 0.001012455322779715, 0.001012455322779715, 0.001012455322779715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001012455322779715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23203218
Iteration 2/25 | Loss: 0.00112927
Iteration 3/25 | Loss: 0.00112927
Iteration 4/25 | Loss: 0.00112927
Iteration 5/25 | Loss: 0.00112927
Iteration 6/25 | Loss: 0.00112927
Iteration 7/25 | Loss: 0.00112927
Iteration 8/25 | Loss: 0.00112927
Iteration 9/25 | Loss: 0.00112927
Iteration 10/25 | Loss: 0.00112927
Iteration 11/25 | Loss: 0.00112927
Iteration 12/25 | Loss: 0.00112927
Iteration 13/25 | Loss: 0.00112927
Iteration 14/25 | Loss: 0.00112927
Iteration 15/25 | Loss: 0.00112927
Iteration 16/25 | Loss: 0.00112927
Iteration 17/25 | Loss: 0.00112927
Iteration 18/25 | Loss: 0.00112927
Iteration 19/25 | Loss: 0.00112927
Iteration 20/25 | Loss: 0.00112927
Iteration 21/25 | Loss: 0.00112927
Iteration 22/25 | Loss: 0.00112927
Iteration 23/25 | Loss: 0.00112927
Iteration 24/25 | Loss: 0.00112927
Iteration 25/25 | Loss: 0.00112927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112927
Iteration 2/1000 | Loss: 0.00004057
Iteration 3/1000 | Loss: 0.00002764
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002336
Iteration 6/1000 | Loss: 0.00002246
Iteration 7/1000 | Loss: 0.00002185
Iteration 8/1000 | Loss: 0.00002133
Iteration 9/1000 | Loss: 0.00002106
Iteration 10/1000 | Loss: 0.00002081
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002038
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001987
Iteration 17/1000 | Loss: 0.00001984
Iteration 18/1000 | Loss: 0.00001984
Iteration 19/1000 | Loss: 0.00001984
Iteration 20/1000 | Loss: 0.00001984
Iteration 21/1000 | Loss: 0.00001984
Iteration 22/1000 | Loss: 0.00001984
Iteration 23/1000 | Loss: 0.00001984
Iteration 24/1000 | Loss: 0.00001984
Iteration 25/1000 | Loss: 0.00001983
Iteration 26/1000 | Loss: 0.00001983
Iteration 27/1000 | Loss: 0.00001983
Iteration 28/1000 | Loss: 0.00001982
Iteration 29/1000 | Loss: 0.00001982
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001980
Iteration 34/1000 | Loss: 0.00001980
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001979
Iteration 39/1000 | Loss: 0.00001979
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001978
Iteration 43/1000 | Loss: 0.00001978
Iteration 44/1000 | Loss: 0.00001978
Iteration 45/1000 | Loss: 0.00001978
Iteration 46/1000 | Loss: 0.00001977
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001976
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001975
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001974
Iteration 60/1000 | Loss: 0.00001974
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001971
Iteration 71/1000 | Loss: 0.00001971
Iteration 72/1000 | Loss: 0.00001971
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00001970
Iteration 75/1000 | Loss: 0.00001969
Iteration 76/1000 | Loss: 0.00001969
Iteration 77/1000 | Loss: 0.00001969
Iteration 78/1000 | Loss: 0.00001969
Iteration 79/1000 | Loss: 0.00001969
Iteration 80/1000 | Loss: 0.00001968
Iteration 81/1000 | Loss: 0.00001968
Iteration 82/1000 | Loss: 0.00001967
Iteration 83/1000 | Loss: 0.00001966
Iteration 84/1000 | Loss: 0.00001966
Iteration 85/1000 | Loss: 0.00001966
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001965
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001964
Iteration 93/1000 | Loss: 0.00001964
Iteration 94/1000 | Loss: 0.00001964
Iteration 95/1000 | Loss: 0.00001964
Iteration 96/1000 | Loss: 0.00001964
Iteration 97/1000 | Loss: 0.00001964
Iteration 98/1000 | Loss: 0.00001964
Iteration 99/1000 | Loss: 0.00001964
Iteration 100/1000 | Loss: 0.00001963
Iteration 101/1000 | Loss: 0.00001963
Iteration 102/1000 | Loss: 0.00001963
Iteration 103/1000 | Loss: 0.00001963
Iteration 104/1000 | Loss: 0.00001963
Iteration 105/1000 | Loss: 0.00001962
Iteration 106/1000 | Loss: 0.00001962
Iteration 107/1000 | Loss: 0.00001962
Iteration 108/1000 | Loss: 0.00001962
Iteration 109/1000 | Loss: 0.00001962
Iteration 110/1000 | Loss: 0.00001962
Iteration 111/1000 | Loss: 0.00001962
Iteration 112/1000 | Loss: 0.00001962
Iteration 113/1000 | Loss: 0.00001962
Iteration 114/1000 | Loss: 0.00001961
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001961
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001960
Iteration 121/1000 | Loss: 0.00001960
Iteration 122/1000 | Loss: 0.00001960
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00001960
Iteration 125/1000 | Loss: 0.00001960
Iteration 126/1000 | Loss: 0.00001960
Iteration 127/1000 | Loss: 0.00001960
Iteration 128/1000 | Loss: 0.00001960
Iteration 129/1000 | Loss: 0.00001960
Iteration 130/1000 | Loss: 0.00001959
Iteration 131/1000 | Loss: 0.00001959
Iteration 132/1000 | Loss: 0.00001959
Iteration 133/1000 | Loss: 0.00001959
Iteration 134/1000 | Loss: 0.00001959
Iteration 135/1000 | Loss: 0.00001959
Iteration 136/1000 | Loss: 0.00001959
Iteration 137/1000 | Loss: 0.00001959
Iteration 138/1000 | Loss: 0.00001959
Iteration 139/1000 | Loss: 0.00001958
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001958
Iteration 143/1000 | Loss: 0.00001958
Iteration 144/1000 | Loss: 0.00001958
Iteration 145/1000 | Loss: 0.00001958
Iteration 146/1000 | Loss: 0.00001958
Iteration 147/1000 | Loss: 0.00001958
Iteration 148/1000 | Loss: 0.00001958
Iteration 149/1000 | Loss: 0.00001958
Iteration 150/1000 | Loss: 0.00001958
Iteration 151/1000 | Loss: 0.00001958
Iteration 152/1000 | Loss: 0.00001958
Iteration 153/1000 | Loss: 0.00001958
Iteration 154/1000 | Loss: 0.00001958
Iteration 155/1000 | Loss: 0.00001958
Iteration 156/1000 | Loss: 0.00001958
Iteration 157/1000 | Loss: 0.00001958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.958044958882965e-05, 1.958044958882965e-05, 1.958044958882965e-05, 1.958044958882965e-05, 1.958044958882965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.958044958882965e-05

Optimization complete. Final v2v error: 3.6232776641845703 mm

Highest mean error: 4.714286804199219 mm for frame 221

Lowest mean error: 2.913531541824341 mm for frame 1

Saving results

Total time: 45.098565101623535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017500
Iteration 2/25 | Loss: 0.01017500
Iteration 3/25 | Loss: 0.00321461
Iteration 4/25 | Loss: 0.00257883
Iteration 5/25 | Loss: 0.00216423
Iteration 6/25 | Loss: 0.00193173
Iteration 7/25 | Loss: 0.00187247
Iteration 8/25 | Loss: 0.00162959
Iteration 9/25 | Loss: 0.00150397
Iteration 10/25 | Loss: 0.00141402
Iteration 11/25 | Loss: 0.00138540
Iteration 12/25 | Loss: 0.00135891
Iteration 13/25 | Loss: 0.00131066
Iteration 14/25 | Loss: 0.00127450
Iteration 15/25 | Loss: 0.00124311
Iteration 16/25 | Loss: 0.00122115
Iteration 17/25 | Loss: 0.00121990
Iteration 18/25 | Loss: 0.00120629
Iteration 19/25 | Loss: 0.00119740
Iteration 20/25 | Loss: 0.00119302
Iteration 21/25 | Loss: 0.00119280
Iteration 22/25 | Loss: 0.00118960
Iteration 23/25 | Loss: 0.00118926
Iteration 24/25 | Loss: 0.00118698
Iteration 25/25 | Loss: 0.00118289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25917876
Iteration 2/25 | Loss: 0.00221296
Iteration 3/25 | Loss: 0.00221296
Iteration 4/25 | Loss: 0.00221296
Iteration 5/25 | Loss: 0.00221296
Iteration 6/25 | Loss: 0.00221296
Iteration 7/25 | Loss: 0.00221296
Iteration 8/25 | Loss: 0.00221296
Iteration 9/25 | Loss: 0.00221296
Iteration 10/25 | Loss: 0.00221296
Iteration 11/25 | Loss: 0.00221296
Iteration 12/25 | Loss: 0.00221296
Iteration 13/25 | Loss: 0.00221296
Iteration 14/25 | Loss: 0.00221296
Iteration 15/25 | Loss: 0.00221296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002212956314906478, 0.002212956314906478, 0.002212956314906478, 0.002212956314906478, 0.002212956314906478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002212956314906478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221296
Iteration 2/1000 | Loss: 0.00018796
Iteration 3/1000 | Loss: 0.00016975
Iteration 4/1000 | Loss: 0.00016394
Iteration 5/1000 | Loss: 0.00020138
Iteration 6/1000 | Loss: 0.00018375
Iteration 7/1000 | Loss: 0.00020187
Iteration 8/1000 | Loss: 0.00017658
Iteration 9/1000 | Loss: 0.00012335
Iteration 10/1000 | Loss: 0.00013975
Iteration 11/1000 | Loss: 0.00013127
Iteration 12/1000 | Loss: 0.00012688
Iteration 13/1000 | Loss: 0.00012653
Iteration 14/1000 | Loss: 0.00012826
Iteration 15/1000 | Loss: 0.00014876
Iteration 16/1000 | Loss: 0.00022245
Iteration 17/1000 | Loss: 0.00022289
Iteration 18/1000 | Loss: 0.00022617
Iteration 19/1000 | Loss: 0.00024161
Iteration 20/1000 | Loss: 0.00022647
Iteration 21/1000 | Loss: 0.00025693
Iteration 22/1000 | Loss: 0.00024757
Iteration 23/1000 | Loss: 0.00037776
Iteration 24/1000 | Loss: 0.00025634
Iteration 25/1000 | Loss: 0.00022404
Iteration 26/1000 | Loss: 0.00012971
Iteration 27/1000 | Loss: 0.00011998
Iteration 28/1000 | Loss: 0.00012875
Iteration 29/1000 | Loss: 0.00014171
Iteration 30/1000 | Loss: 0.00012414
Iteration 31/1000 | Loss: 0.00012837
Iteration 32/1000 | Loss: 0.00028459
Iteration 33/1000 | Loss: 0.00012197
Iteration 34/1000 | Loss: 0.00089684
Iteration 35/1000 | Loss: 0.00071201
Iteration 36/1000 | Loss: 0.00220603
Iteration 37/1000 | Loss: 0.00253386
Iteration 38/1000 | Loss: 0.00227961
Iteration 39/1000 | Loss: 0.00058755
Iteration 40/1000 | Loss: 0.00084967
Iteration 41/1000 | Loss: 0.00059083
Iteration 42/1000 | Loss: 0.00034867
Iteration 43/1000 | Loss: 0.00050150
Iteration 44/1000 | Loss: 0.00011992
Iteration 45/1000 | Loss: 0.00010484
Iteration 46/1000 | Loss: 0.00018083
Iteration 47/1000 | Loss: 0.00008775
Iteration 48/1000 | Loss: 0.00008067
Iteration 49/1000 | Loss: 0.00007487
Iteration 50/1000 | Loss: 0.00012411
Iteration 51/1000 | Loss: 0.00006411
Iteration 52/1000 | Loss: 0.00010834
Iteration 53/1000 | Loss: 0.00007723
Iteration 54/1000 | Loss: 0.00004878
Iteration 55/1000 | Loss: 0.00007616
Iteration 56/1000 | Loss: 0.00007324
Iteration 57/1000 | Loss: 0.00024433
Iteration 58/1000 | Loss: 0.00007785
Iteration 59/1000 | Loss: 0.00022695
Iteration 60/1000 | Loss: 0.00011748
Iteration 61/1000 | Loss: 0.00011899
Iteration 62/1000 | Loss: 0.00009222
Iteration 63/1000 | Loss: 0.00008035
Iteration 64/1000 | Loss: 0.00009126
Iteration 65/1000 | Loss: 0.00007725
Iteration 66/1000 | Loss: 0.00006908
Iteration 67/1000 | Loss: 0.00005793
Iteration 68/1000 | Loss: 0.00005794
Iteration 69/1000 | Loss: 0.00008412
Iteration 70/1000 | Loss: 0.00033895
Iteration 71/1000 | Loss: 0.00013147
Iteration 72/1000 | Loss: 0.00009315
Iteration 73/1000 | Loss: 0.00007100
Iteration 74/1000 | Loss: 0.00008522
Iteration 75/1000 | Loss: 0.00005974
Iteration 76/1000 | Loss: 0.00019985
Iteration 77/1000 | Loss: 0.00008151
Iteration 78/1000 | Loss: 0.00014722
Iteration 79/1000 | Loss: 0.00004481
Iteration 80/1000 | Loss: 0.00004466
Iteration 81/1000 | Loss: 0.00006395
Iteration 82/1000 | Loss: 0.00012155
Iteration 83/1000 | Loss: 0.00004851
Iteration 84/1000 | Loss: 0.00009355
Iteration 85/1000 | Loss: 0.00006388
Iteration 86/1000 | Loss: 0.00007594
Iteration 87/1000 | Loss: 0.00008863
Iteration 88/1000 | Loss: 0.00007119
Iteration 89/1000 | Loss: 0.00005616
Iteration 90/1000 | Loss: 0.00006103
Iteration 91/1000 | Loss: 0.00006276
Iteration 92/1000 | Loss: 0.00006998
Iteration 93/1000 | Loss: 0.00013224
Iteration 94/1000 | Loss: 0.00007028
Iteration 95/1000 | Loss: 0.00005879
Iteration 96/1000 | Loss: 0.00006324
Iteration 97/1000 | Loss: 0.00005498
Iteration 98/1000 | Loss: 0.00007048
Iteration 99/1000 | Loss: 0.00011184
Iteration 100/1000 | Loss: 0.00007747
Iteration 101/1000 | Loss: 0.00006798
Iteration 102/1000 | Loss: 0.00006958
Iteration 103/1000 | Loss: 0.00006442
Iteration 104/1000 | Loss: 0.00012850
Iteration 105/1000 | Loss: 0.00004834
Iteration 106/1000 | Loss: 0.00007876
Iteration 107/1000 | Loss: 0.00005831
Iteration 108/1000 | Loss: 0.00005173
Iteration 109/1000 | Loss: 0.00004151
Iteration 110/1000 | Loss: 0.00006174
Iteration 111/1000 | Loss: 0.00005960
Iteration 112/1000 | Loss: 0.00011595
Iteration 113/1000 | Loss: 0.00005669
Iteration 114/1000 | Loss: 0.00004424
Iteration 115/1000 | Loss: 0.00006504
Iteration 116/1000 | Loss: 0.00005910
Iteration 117/1000 | Loss: 0.00007140
Iteration 118/1000 | Loss: 0.00005644
Iteration 119/1000 | Loss: 0.00006857
Iteration 120/1000 | Loss: 0.00007108
Iteration 121/1000 | Loss: 0.00007305
Iteration 122/1000 | Loss: 0.00004594
Iteration 123/1000 | Loss: 0.00005948
Iteration 124/1000 | Loss: 0.00007822
Iteration 125/1000 | Loss: 0.00007239
Iteration 126/1000 | Loss: 0.00005685
Iteration 127/1000 | Loss: 0.00004935
Iteration 128/1000 | Loss: 0.00005511
Iteration 129/1000 | Loss: 0.00007003
Iteration 130/1000 | Loss: 0.00003796
Iteration 131/1000 | Loss: 0.00004583
Iteration 132/1000 | Loss: 0.00003877
Iteration 133/1000 | Loss: 0.00003701
Iteration 134/1000 | Loss: 0.00003917
Iteration 135/1000 | Loss: 0.00003470
Iteration 136/1000 | Loss: 0.00003091
Iteration 137/1000 | Loss: 0.00003572
Iteration 138/1000 | Loss: 0.00003618
Iteration 139/1000 | Loss: 0.00003140
Iteration 140/1000 | Loss: 0.00004668
Iteration 141/1000 | Loss: 0.00003536
Iteration 142/1000 | Loss: 0.00002957
Iteration 143/1000 | Loss: 0.00003869
Iteration 144/1000 | Loss: 0.00003531
Iteration 145/1000 | Loss: 0.00003956
Iteration 146/1000 | Loss: 0.00002890
Iteration 147/1000 | Loss: 0.00003531
Iteration 148/1000 | Loss: 0.00003912
Iteration 149/1000 | Loss: 0.00003477
Iteration 150/1000 | Loss: 0.00003846
Iteration 151/1000 | Loss: 0.00003464
Iteration 152/1000 | Loss: 0.00003779
Iteration 153/1000 | Loss: 0.00011795
Iteration 154/1000 | Loss: 0.00017287
Iteration 155/1000 | Loss: 0.00082070
Iteration 156/1000 | Loss: 0.00002920
Iteration 157/1000 | Loss: 0.00002728
Iteration 158/1000 | Loss: 0.00002796
Iteration 159/1000 | Loss: 0.00018399
Iteration 160/1000 | Loss: 0.00002529
Iteration 161/1000 | Loss: 0.00002492
Iteration 162/1000 | Loss: 0.00012074
Iteration 163/1000 | Loss: 0.00065659
Iteration 164/1000 | Loss: 0.00006044
Iteration 165/1000 | Loss: 0.00003797
Iteration 166/1000 | Loss: 0.00003134
Iteration 167/1000 | Loss: 0.00002476
Iteration 168/1000 | Loss: 0.00002435
Iteration 169/1000 | Loss: 0.00002412
Iteration 170/1000 | Loss: 0.00002412
Iteration 171/1000 | Loss: 0.00002412
Iteration 172/1000 | Loss: 0.00002411
Iteration 173/1000 | Loss: 0.00002411
Iteration 174/1000 | Loss: 0.00002411
Iteration 175/1000 | Loss: 0.00002410
Iteration 176/1000 | Loss: 0.00002403
Iteration 177/1000 | Loss: 0.00002397
Iteration 178/1000 | Loss: 0.00002397
Iteration 179/1000 | Loss: 0.00002397
Iteration 180/1000 | Loss: 0.00002396
Iteration 181/1000 | Loss: 0.00002396
Iteration 182/1000 | Loss: 0.00002396
Iteration 183/1000 | Loss: 0.00002396
Iteration 184/1000 | Loss: 0.00002396
Iteration 185/1000 | Loss: 0.00002396
Iteration 186/1000 | Loss: 0.00002394
Iteration 187/1000 | Loss: 0.00002393
Iteration 188/1000 | Loss: 0.00002393
Iteration 189/1000 | Loss: 0.00002391
Iteration 190/1000 | Loss: 0.00002391
Iteration 191/1000 | Loss: 0.00002381
Iteration 192/1000 | Loss: 0.00002372
Iteration 193/1000 | Loss: 0.00002369
Iteration 194/1000 | Loss: 0.00002369
Iteration 195/1000 | Loss: 0.00002367
Iteration 196/1000 | Loss: 0.00012065
Iteration 197/1000 | Loss: 0.00002394
Iteration 198/1000 | Loss: 0.00002362
Iteration 199/1000 | Loss: 0.00002353
Iteration 200/1000 | Loss: 0.00002353
Iteration 201/1000 | Loss: 0.00002353
Iteration 202/1000 | Loss: 0.00002353
Iteration 203/1000 | Loss: 0.00002353
Iteration 204/1000 | Loss: 0.00002353
Iteration 205/1000 | Loss: 0.00002352
Iteration 206/1000 | Loss: 0.00002352
Iteration 207/1000 | Loss: 0.00002351
Iteration 208/1000 | Loss: 0.00002351
Iteration 209/1000 | Loss: 0.00002351
Iteration 210/1000 | Loss: 0.00002350
Iteration 211/1000 | Loss: 0.00002350
Iteration 212/1000 | Loss: 0.00002350
Iteration 213/1000 | Loss: 0.00002350
Iteration 214/1000 | Loss: 0.00002350
Iteration 215/1000 | Loss: 0.00002350
Iteration 216/1000 | Loss: 0.00002349
Iteration 217/1000 | Loss: 0.00002349
Iteration 218/1000 | Loss: 0.00010126
Iteration 219/1000 | Loss: 0.00002567
Iteration 220/1000 | Loss: 0.00002420
Iteration 221/1000 | Loss: 0.00002351
Iteration 222/1000 | Loss: 0.00003586
Iteration 223/1000 | Loss: 0.00003287
Iteration 224/1000 | Loss: 0.00002340
Iteration 225/1000 | Loss: 0.00002339
Iteration 226/1000 | Loss: 0.00003518
Iteration 227/1000 | Loss: 0.00003157
Iteration 228/1000 | Loss: 0.00002347
Iteration 229/1000 | Loss: 0.00003293
Iteration 230/1000 | Loss: 0.00003293
Iteration 231/1000 | Loss: 0.00002463
Iteration 232/1000 | Loss: 0.00002357
Iteration 233/1000 | Loss: 0.00002337
Iteration 234/1000 | Loss: 0.00002322
Iteration 235/1000 | Loss: 0.00002319
Iteration 236/1000 | Loss: 0.00002319
Iteration 237/1000 | Loss: 0.00002313
Iteration 238/1000 | Loss: 0.00002312
Iteration 239/1000 | Loss: 0.00002311
Iteration 240/1000 | Loss: 0.00002311
Iteration 241/1000 | Loss: 0.00002311
Iteration 242/1000 | Loss: 0.00002311
Iteration 243/1000 | Loss: 0.00002311
Iteration 244/1000 | Loss: 0.00002310
Iteration 245/1000 | Loss: 0.00002310
Iteration 246/1000 | Loss: 0.00002309
Iteration 247/1000 | Loss: 0.00002309
Iteration 248/1000 | Loss: 0.00002309
Iteration 249/1000 | Loss: 0.00002309
Iteration 250/1000 | Loss: 0.00002309
Iteration 251/1000 | Loss: 0.00002309
Iteration 252/1000 | Loss: 0.00002309
Iteration 253/1000 | Loss: 0.00002308
Iteration 254/1000 | Loss: 0.00002308
Iteration 255/1000 | Loss: 0.00002308
Iteration 256/1000 | Loss: 0.00002308
Iteration 257/1000 | Loss: 0.00002308
Iteration 258/1000 | Loss: 0.00002308
Iteration 259/1000 | Loss: 0.00002307
Iteration 260/1000 | Loss: 0.00002307
Iteration 261/1000 | Loss: 0.00002307
Iteration 262/1000 | Loss: 0.00002307
Iteration 263/1000 | Loss: 0.00002306
Iteration 264/1000 | Loss: 0.00002306
Iteration 265/1000 | Loss: 0.00002306
Iteration 266/1000 | Loss: 0.00002305
Iteration 267/1000 | Loss: 0.00002305
Iteration 268/1000 | Loss: 0.00002305
Iteration 269/1000 | Loss: 0.00002304
Iteration 270/1000 | Loss: 0.00002304
Iteration 271/1000 | Loss: 0.00002304
Iteration 272/1000 | Loss: 0.00002303
Iteration 273/1000 | Loss: 0.00002303
Iteration 274/1000 | Loss: 0.00002303
Iteration 275/1000 | Loss: 0.00002302
Iteration 276/1000 | Loss: 0.00002302
Iteration 277/1000 | Loss: 0.00002302
Iteration 278/1000 | Loss: 0.00002302
Iteration 279/1000 | Loss: 0.00002302
Iteration 280/1000 | Loss: 0.00002302
Iteration 281/1000 | Loss: 0.00002302
Iteration 282/1000 | Loss: 0.00002302
Iteration 283/1000 | Loss: 0.00002302
Iteration 284/1000 | Loss: 0.00002302
Iteration 285/1000 | Loss: 0.00002302
Iteration 286/1000 | Loss: 0.00002302
Iteration 287/1000 | Loss: 0.00002302
Iteration 288/1000 | Loss: 0.00002302
Iteration 289/1000 | Loss: 0.00002302
Iteration 290/1000 | Loss: 0.00002302
Iteration 291/1000 | Loss: 0.00002302
Iteration 292/1000 | Loss: 0.00002302
Iteration 293/1000 | Loss: 0.00002302
Iteration 294/1000 | Loss: 0.00002302
Iteration 295/1000 | Loss: 0.00002302
Iteration 296/1000 | Loss: 0.00002302
Iteration 297/1000 | Loss: 0.00002302
Iteration 298/1000 | Loss: 0.00002302
Iteration 299/1000 | Loss: 0.00002301
Iteration 300/1000 | Loss: 0.00002301
Iteration 301/1000 | Loss: 0.00002301
Iteration 302/1000 | Loss: 0.00002301
Iteration 303/1000 | Loss: 0.00002301
Iteration 304/1000 | Loss: 0.00002301
Iteration 305/1000 | Loss: 0.00002301
Iteration 306/1000 | Loss: 0.00002301
Iteration 307/1000 | Loss: 0.00002301
Iteration 308/1000 | Loss: 0.00002301
Iteration 309/1000 | Loss: 0.00002301
Iteration 310/1000 | Loss: 0.00002301
Iteration 311/1000 | Loss: 0.00002301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [2.301464519405272e-05, 2.301464519405272e-05, 2.301464519405272e-05, 2.301464519405272e-05, 2.301464519405272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.301464519405272e-05

Optimization complete. Final v2v error: 3.7358462810516357 mm

Highest mean error: 11.358450889587402 mm for frame 199

Lowest mean error: 3.004483461380005 mm for frame 6

Saving results

Total time: 360.7575204372406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_us_2609/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_us_2609/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688464
Iteration 2/25 | Loss: 0.00150389
Iteration 3/25 | Loss: 0.00118717
Iteration 4/25 | Loss: 0.00117112
Iteration 5/25 | Loss: 0.00116975
Iteration 6/25 | Loss: 0.00116975
Iteration 7/25 | Loss: 0.00116975
Iteration 8/25 | Loss: 0.00116975
Iteration 9/25 | Loss: 0.00116975
Iteration 10/25 | Loss: 0.00116975
Iteration 11/25 | Loss: 0.00116975
Iteration 12/25 | Loss: 0.00116975
Iteration 13/25 | Loss: 0.00116975
Iteration 14/25 | Loss: 0.00116975
Iteration 15/25 | Loss: 0.00116975
Iteration 16/25 | Loss: 0.00116975
Iteration 17/25 | Loss: 0.00116975
Iteration 18/25 | Loss: 0.00116975
Iteration 19/25 | Loss: 0.00116975
Iteration 20/25 | Loss: 0.00116975
Iteration 21/25 | Loss: 0.00116975
Iteration 22/25 | Loss: 0.00116975
Iteration 23/25 | Loss: 0.00116975
Iteration 24/25 | Loss: 0.00116975
Iteration 25/25 | Loss: 0.00116975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.53292918
Iteration 2/25 | Loss: 0.00079187
Iteration 3/25 | Loss: 0.00079187
Iteration 4/25 | Loss: 0.00079187
Iteration 5/25 | Loss: 0.00079187
Iteration 6/25 | Loss: 0.00079187
Iteration 7/25 | Loss: 0.00079187
Iteration 8/25 | Loss: 0.00079187
Iteration 9/25 | Loss: 0.00079187
Iteration 10/25 | Loss: 0.00079187
Iteration 11/25 | Loss: 0.00079187
Iteration 12/25 | Loss: 0.00079187
Iteration 13/25 | Loss: 0.00079187
Iteration 14/25 | Loss: 0.00079187
Iteration 15/25 | Loss: 0.00079187
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007918650517240167, 0.0007918650517240167, 0.0007918650517240167, 0.0007918650517240167, 0.0007918650517240167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007918650517240167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079187
Iteration 2/1000 | Loss: 0.00009361
Iteration 3/1000 | Loss: 0.00006901
Iteration 4/1000 | Loss: 0.00006058
Iteration 5/1000 | Loss: 0.00005720
Iteration 6/1000 | Loss: 0.00005558
Iteration 7/1000 | Loss: 0.00005445
Iteration 8/1000 | Loss: 0.00005287
Iteration 9/1000 | Loss: 0.00005149
Iteration 10/1000 | Loss: 0.00005052
Iteration 11/1000 | Loss: 0.00004945
Iteration 12/1000 | Loss: 0.00004895
Iteration 13/1000 | Loss: 0.00004836
Iteration 14/1000 | Loss: 0.00004780
Iteration 15/1000 | Loss: 0.00004745
Iteration 16/1000 | Loss: 0.00004712
Iteration 17/1000 | Loss: 0.00004678
Iteration 18/1000 | Loss: 0.00004639
Iteration 19/1000 | Loss: 0.00004593
Iteration 20/1000 | Loss: 0.00004556
Iteration 21/1000 | Loss: 0.00004534
Iteration 22/1000 | Loss: 0.00004514
Iteration 23/1000 | Loss: 0.00004501
Iteration 24/1000 | Loss: 0.00004499
Iteration 25/1000 | Loss: 0.00004488
Iteration 26/1000 | Loss: 0.00004485
Iteration 27/1000 | Loss: 0.00004483
Iteration 28/1000 | Loss: 0.00004473
Iteration 29/1000 | Loss: 0.00004473
Iteration 30/1000 | Loss: 0.00004473
Iteration 31/1000 | Loss: 0.00004473
Iteration 32/1000 | Loss: 0.00004473
Iteration 33/1000 | Loss: 0.00004473
Iteration 34/1000 | Loss: 0.00004473
Iteration 35/1000 | Loss: 0.00004473
Iteration 36/1000 | Loss: 0.00004473
Iteration 37/1000 | Loss: 0.00004472
Iteration 38/1000 | Loss: 0.00004472
Iteration 39/1000 | Loss: 0.00004472
Iteration 40/1000 | Loss: 0.00004472
Iteration 41/1000 | Loss: 0.00004471
Iteration 42/1000 | Loss: 0.00004470
Iteration 43/1000 | Loss: 0.00004470
Iteration 44/1000 | Loss: 0.00004470
Iteration 45/1000 | Loss: 0.00004470
Iteration 46/1000 | Loss: 0.00004470
Iteration 47/1000 | Loss: 0.00004470
Iteration 48/1000 | Loss: 0.00004470
Iteration 49/1000 | Loss: 0.00004469
Iteration 50/1000 | Loss: 0.00004468
Iteration 51/1000 | Loss: 0.00004465
Iteration 52/1000 | Loss: 0.00004464
Iteration 53/1000 | Loss: 0.00004464
Iteration 54/1000 | Loss: 0.00004464
Iteration 55/1000 | Loss: 0.00004464
Iteration 56/1000 | Loss: 0.00004463
Iteration 57/1000 | Loss: 0.00004463
Iteration 58/1000 | Loss: 0.00004463
Iteration 59/1000 | Loss: 0.00004463
Iteration 60/1000 | Loss: 0.00004463
Iteration 61/1000 | Loss: 0.00004463
Iteration 62/1000 | Loss: 0.00004463
Iteration 63/1000 | Loss: 0.00004463
Iteration 64/1000 | Loss: 0.00004463
Iteration 65/1000 | Loss: 0.00004463
Iteration 66/1000 | Loss: 0.00004463
Iteration 67/1000 | Loss: 0.00004462
Iteration 68/1000 | Loss: 0.00004462
Iteration 69/1000 | Loss: 0.00004462
Iteration 70/1000 | Loss: 0.00004461
Iteration 71/1000 | Loss: 0.00004461
Iteration 72/1000 | Loss: 0.00004461
Iteration 73/1000 | Loss: 0.00004461
Iteration 74/1000 | Loss: 0.00004461
Iteration 75/1000 | Loss: 0.00004461
Iteration 76/1000 | Loss: 0.00004461
Iteration 77/1000 | Loss: 0.00004461
Iteration 78/1000 | Loss: 0.00004461
Iteration 79/1000 | Loss: 0.00004461
Iteration 80/1000 | Loss: 0.00004461
Iteration 81/1000 | Loss: 0.00004461
Iteration 82/1000 | Loss: 0.00004459
Iteration 83/1000 | Loss: 0.00004459
Iteration 84/1000 | Loss: 0.00004459
Iteration 85/1000 | Loss: 0.00004459
Iteration 86/1000 | Loss: 0.00004459
Iteration 87/1000 | Loss: 0.00004458
Iteration 88/1000 | Loss: 0.00004458
Iteration 89/1000 | Loss: 0.00004457
Iteration 90/1000 | Loss: 0.00004457
Iteration 91/1000 | Loss: 0.00004457
Iteration 92/1000 | Loss: 0.00004457
Iteration 93/1000 | Loss: 0.00004457
Iteration 94/1000 | Loss: 0.00004457
Iteration 95/1000 | Loss: 0.00004457
Iteration 96/1000 | Loss: 0.00004457
Iteration 97/1000 | Loss: 0.00004457
Iteration 98/1000 | Loss: 0.00004456
Iteration 99/1000 | Loss: 0.00004456
Iteration 100/1000 | Loss: 0.00004456
Iteration 101/1000 | Loss: 0.00004455
Iteration 102/1000 | Loss: 0.00004455
Iteration 103/1000 | Loss: 0.00004455
Iteration 104/1000 | Loss: 0.00004455
Iteration 105/1000 | Loss: 0.00004455
Iteration 106/1000 | Loss: 0.00004455
Iteration 107/1000 | Loss: 0.00004455
Iteration 108/1000 | Loss: 0.00004454
Iteration 109/1000 | Loss: 0.00004454
Iteration 110/1000 | Loss: 0.00004454
Iteration 111/1000 | Loss: 0.00004454
Iteration 112/1000 | Loss: 0.00004454
Iteration 113/1000 | Loss: 0.00004454
Iteration 114/1000 | Loss: 0.00004454
Iteration 115/1000 | Loss: 0.00004454
Iteration 116/1000 | Loss: 0.00004454
Iteration 117/1000 | Loss: 0.00004453
Iteration 118/1000 | Loss: 0.00004453
Iteration 119/1000 | Loss: 0.00004453
Iteration 120/1000 | Loss: 0.00004453
Iteration 121/1000 | Loss: 0.00004453
Iteration 122/1000 | Loss: 0.00004453
Iteration 123/1000 | Loss: 0.00004453
Iteration 124/1000 | Loss: 0.00004453
Iteration 125/1000 | Loss: 0.00004453
Iteration 126/1000 | Loss: 0.00004453
Iteration 127/1000 | Loss: 0.00004453
Iteration 128/1000 | Loss: 0.00004452
Iteration 129/1000 | Loss: 0.00004452
Iteration 130/1000 | Loss: 0.00004452
Iteration 131/1000 | Loss: 0.00004452
Iteration 132/1000 | Loss: 0.00004451
Iteration 133/1000 | Loss: 0.00004451
Iteration 134/1000 | Loss: 0.00004451
Iteration 135/1000 | Loss: 0.00004451
Iteration 136/1000 | Loss: 0.00004451
Iteration 137/1000 | Loss: 0.00004451
Iteration 138/1000 | Loss: 0.00004451
Iteration 139/1000 | Loss: 0.00004451
Iteration 140/1000 | Loss: 0.00004451
Iteration 141/1000 | Loss: 0.00004451
Iteration 142/1000 | Loss: 0.00004451
Iteration 143/1000 | Loss: 0.00004451
Iteration 144/1000 | Loss: 0.00004451
Iteration 145/1000 | Loss: 0.00004451
Iteration 146/1000 | Loss: 0.00004451
Iteration 147/1000 | Loss: 0.00004451
Iteration 148/1000 | Loss: 0.00004451
Iteration 149/1000 | Loss: 0.00004451
Iteration 150/1000 | Loss: 0.00004451
Iteration 151/1000 | Loss: 0.00004451
Iteration 152/1000 | Loss: 0.00004451
Iteration 153/1000 | Loss: 0.00004451
Iteration 154/1000 | Loss: 0.00004451
Iteration 155/1000 | Loss: 0.00004451
Iteration 156/1000 | Loss: 0.00004451
Iteration 157/1000 | Loss: 0.00004451
Iteration 158/1000 | Loss: 0.00004451
Iteration 159/1000 | Loss: 0.00004451
Iteration 160/1000 | Loss: 0.00004451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [4.450525375432335e-05, 4.450525375432335e-05, 4.450525375432335e-05, 4.450525375432335e-05, 4.450525375432335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.450525375432335e-05

Optimization complete. Final v2v error: 5.328610897064209 mm

Highest mean error: 5.942649841308594 mm for frame 14

Lowest mean error: 4.757391452789307 mm for frame 65

Saving results

Total time: 53.20623970031738
