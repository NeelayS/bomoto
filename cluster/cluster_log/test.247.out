Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=247, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13832-13887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914671
Iteration 2/25 | Loss: 0.00174874
Iteration 3/25 | Loss: 0.00149904
Iteration 4/25 | Loss: 0.00144154
Iteration 5/25 | Loss: 0.00142508
Iteration 6/25 | Loss: 0.00142250
Iteration 7/25 | Loss: 0.00142215
Iteration 8/25 | Loss: 0.00142215
Iteration 9/25 | Loss: 0.00142215
Iteration 10/25 | Loss: 0.00142215
Iteration 11/25 | Loss: 0.00142215
Iteration 12/25 | Loss: 0.00142215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014221527380868793, 0.0014221527380868793, 0.0014221527380868793, 0.0014221527380868793, 0.0014221527380868793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014221527380868793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42617667
Iteration 2/25 | Loss: 0.00104056
Iteration 3/25 | Loss: 0.00104056
Iteration 4/25 | Loss: 0.00104056
Iteration 5/25 | Loss: 0.00104056
Iteration 6/25 | Loss: 0.00104056
Iteration 7/25 | Loss: 0.00104056
Iteration 8/25 | Loss: 0.00104056
Iteration 9/25 | Loss: 0.00104056
Iteration 10/25 | Loss: 0.00104056
Iteration 11/25 | Loss: 0.00104056
Iteration 12/25 | Loss: 0.00104056
Iteration 13/25 | Loss: 0.00104056
Iteration 14/25 | Loss: 0.00104056
Iteration 15/25 | Loss: 0.00104056
Iteration 16/25 | Loss: 0.00104056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010405558859929442, 0.0010405558859929442, 0.0010405558859929442, 0.0010405558859929442, 0.0010405558859929442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010405558859929442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104056
Iteration 2/1000 | Loss: 0.00006269
Iteration 3/1000 | Loss: 0.00004548
Iteration 4/1000 | Loss: 0.00004164
Iteration 5/1000 | Loss: 0.00003964
Iteration 6/1000 | Loss: 0.00003841
Iteration 7/1000 | Loss: 0.00003768
Iteration 8/1000 | Loss: 0.00003714
Iteration 9/1000 | Loss: 0.00003676
Iteration 10/1000 | Loss: 0.00003647
Iteration 11/1000 | Loss: 0.00003627
Iteration 12/1000 | Loss: 0.00003623
Iteration 13/1000 | Loss: 0.00003614
Iteration 14/1000 | Loss: 0.00003614
Iteration 15/1000 | Loss: 0.00003614
Iteration 16/1000 | Loss: 0.00003612
Iteration 17/1000 | Loss: 0.00003612
Iteration 18/1000 | Loss: 0.00003612
Iteration 19/1000 | Loss: 0.00003612
Iteration 20/1000 | Loss: 0.00003612
Iteration 21/1000 | Loss: 0.00003612
Iteration 22/1000 | Loss: 0.00003612
Iteration 23/1000 | Loss: 0.00003612
Iteration 24/1000 | Loss: 0.00003612
Iteration 25/1000 | Loss: 0.00003612
Iteration 26/1000 | Loss: 0.00003611
Iteration 27/1000 | Loss: 0.00003611
Iteration 28/1000 | Loss: 0.00003611
Iteration 29/1000 | Loss: 0.00003611
Iteration 30/1000 | Loss: 0.00003611
Iteration 31/1000 | Loss: 0.00003610
Iteration 32/1000 | Loss: 0.00003610
Iteration 33/1000 | Loss: 0.00003610
Iteration 34/1000 | Loss: 0.00003610
Iteration 35/1000 | Loss: 0.00003609
Iteration 36/1000 | Loss: 0.00003609
Iteration 37/1000 | Loss: 0.00003609
Iteration 38/1000 | Loss: 0.00003608
Iteration 39/1000 | Loss: 0.00003608
Iteration 40/1000 | Loss: 0.00003608
Iteration 41/1000 | Loss: 0.00003607
Iteration 42/1000 | Loss: 0.00003607
Iteration 43/1000 | Loss: 0.00003607
Iteration 44/1000 | Loss: 0.00003607
Iteration 45/1000 | Loss: 0.00003606
Iteration 46/1000 | Loss: 0.00003606
Iteration 47/1000 | Loss: 0.00003606
Iteration 48/1000 | Loss: 0.00003606
Iteration 49/1000 | Loss: 0.00003605
Iteration 50/1000 | Loss: 0.00003605
Iteration 51/1000 | Loss: 0.00003605
Iteration 52/1000 | Loss: 0.00003605
Iteration 53/1000 | Loss: 0.00003605
Iteration 54/1000 | Loss: 0.00003605
Iteration 55/1000 | Loss: 0.00003605
Iteration 56/1000 | Loss: 0.00003605
Iteration 57/1000 | Loss: 0.00003605
Iteration 58/1000 | Loss: 0.00003604
Iteration 59/1000 | Loss: 0.00003604
Iteration 60/1000 | Loss: 0.00003603
Iteration 61/1000 | Loss: 0.00003603
Iteration 62/1000 | Loss: 0.00003602
Iteration 63/1000 | Loss: 0.00003602
Iteration 64/1000 | Loss: 0.00003602
Iteration 65/1000 | Loss: 0.00003602
Iteration 66/1000 | Loss: 0.00003602
Iteration 67/1000 | Loss: 0.00003602
Iteration 68/1000 | Loss: 0.00003602
Iteration 69/1000 | Loss: 0.00003602
Iteration 70/1000 | Loss: 0.00003602
Iteration 71/1000 | Loss: 0.00003602
Iteration 72/1000 | Loss: 0.00003602
Iteration 73/1000 | Loss: 0.00003601
Iteration 74/1000 | Loss: 0.00003601
Iteration 75/1000 | Loss: 0.00003601
Iteration 76/1000 | Loss: 0.00003601
Iteration 77/1000 | Loss: 0.00003600
Iteration 78/1000 | Loss: 0.00003600
Iteration 79/1000 | Loss: 0.00003600
Iteration 80/1000 | Loss: 0.00003599
Iteration 81/1000 | Loss: 0.00003599
Iteration 82/1000 | Loss: 0.00003599
Iteration 83/1000 | Loss: 0.00003599
Iteration 84/1000 | Loss: 0.00003598
Iteration 85/1000 | Loss: 0.00003598
Iteration 86/1000 | Loss: 0.00003598
Iteration 87/1000 | Loss: 0.00003597
Iteration 88/1000 | Loss: 0.00003597
Iteration 89/1000 | Loss: 0.00003596
Iteration 90/1000 | Loss: 0.00003596
Iteration 91/1000 | Loss: 0.00003596
Iteration 92/1000 | Loss: 0.00003596
Iteration 93/1000 | Loss: 0.00003596
Iteration 94/1000 | Loss: 0.00003596
Iteration 95/1000 | Loss: 0.00003595
Iteration 96/1000 | Loss: 0.00003595
Iteration 97/1000 | Loss: 0.00003595
Iteration 98/1000 | Loss: 0.00003594
Iteration 99/1000 | Loss: 0.00003594
Iteration 100/1000 | Loss: 0.00003594
Iteration 101/1000 | Loss: 0.00003594
Iteration 102/1000 | Loss: 0.00003594
Iteration 103/1000 | Loss: 0.00003594
Iteration 104/1000 | Loss: 0.00003593
Iteration 105/1000 | Loss: 0.00003593
Iteration 106/1000 | Loss: 0.00003591
Iteration 107/1000 | Loss: 0.00003591
Iteration 108/1000 | Loss: 0.00003591
Iteration 109/1000 | Loss: 0.00003591
Iteration 110/1000 | Loss: 0.00003591
Iteration 111/1000 | Loss: 0.00003591
Iteration 112/1000 | Loss: 0.00003590
Iteration 113/1000 | Loss: 0.00003590
Iteration 114/1000 | Loss: 0.00003590
Iteration 115/1000 | Loss: 0.00003590
Iteration 116/1000 | Loss: 0.00003590
Iteration 117/1000 | Loss: 0.00003589
Iteration 118/1000 | Loss: 0.00003588
Iteration 119/1000 | Loss: 0.00003588
Iteration 120/1000 | Loss: 0.00003588
Iteration 121/1000 | Loss: 0.00003588
Iteration 122/1000 | Loss: 0.00003588
Iteration 123/1000 | Loss: 0.00003587
Iteration 124/1000 | Loss: 0.00003587
Iteration 125/1000 | Loss: 0.00003587
Iteration 126/1000 | Loss: 0.00003587
Iteration 127/1000 | Loss: 0.00003587
Iteration 128/1000 | Loss: 0.00003587
Iteration 129/1000 | Loss: 0.00003586
Iteration 130/1000 | Loss: 0.00003586
Iteration 131/1000 | Loss: 0.00003586
Iteration 132/1000 | Loss: 0.00003585
Iteration 133/1000 | Loss: 0.00003585
Iteration 134/1000 | Loss: 0.00003585
Iteration 135/1000 | Loss: 0.00003585
Iteration 136/1000 | Loss: 0.00003585
Iteration 137/1000 | Loss: 0.00003584
Iteration 138/1000 | Loss: 0.00003584
Iteration 139/1000 | Loss: 0.00003584
Iteration 140/1000 | Loss: 0.00003584
Iteration 141/1000 | Loss: 0.00003584
Iteration 142/1000 | Loss: 0.00003584
Iteration 143/1000 | Loss: 0.00003584
Iteration 144/1000 | Loss: 0.00003584
Iteration 145/1000 | Loss: 0.00003584
Iteration 146/1000 | Loss: 0.00003584
Iteration 147/1000 | Loss: 0.00003584
Iteration 148/1000 | Loss: 0.00003584
Iteration 149/1000 | Loss: 0.00003583
Iteration 150/1000 | Loss: 0.00003583
Iteration 151/1000 | Loss: 0.00003583
Iteration 152/1000 | Loss: 0.00003583
Iteration 153/1000 | Loss: 0.00003583
Iteration 154/1000 | Loss: 0.00003583
Iteration 155/1000 | Loss: 0.00003583
Iteration 156/1000 | Loss: 0.00003583
Iteration 157/1000 | Loss: 0.00003583
Iteration 158/1000 | Loss: 0.00003583
Iteration 159/1000 | Loss: 0.00003583
Iteration 160/1000 | Loss: 0.00003583
Iteration 161/1000 | Loss: 0.00003583
Iteration 162/1000 | Loss: 0.00003583
Iteration 163/1000 | Loss: 0.00003583
Iteration 164/1000 | Loss: 0.00003583
Iteration 165/1000 | Loss: 0.00003583
Iteration 166/1000 | Loss: 0.00003583
Iteration 167/1000 | Loss: 0.00003583
Iteration 168/1000 | Loss: 0.00003583
Iteration 169/1000 | Loss: 0.00003583
Iteration 170/1000 | Loss: 0.00003583
Iteration 171/1000 | Loss: 0.00003583
Iteration 172/1000 | Loss: 0.00003583
Iteration 173/1000 | Loss: 0.00003583
Iteration 174/1000 | Loss: 0.00003583
Iteration 175/1000 | Loss: 0.00003583
Iteration 176/1000 | Loss: 0.00003583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.583037687349133e-05, 3.583037687349133e-05, 3.583037687349133e-05, 3.583037687349133e-05, 3.583037687349133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.583037687349133e-05

Optimization complete. Final v2v error: 5.105599403381348 mm

Highest mean error: 5.513851642608643 mm for frame 239

Lowest mean error: 4.6737799644470215 mm for frame 215

Saving results

Total time: 45.065913677215576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878573
Iteration 2/25 | Loss: 0.00175376
Iteration 3/25 | Loss: 0.00137875
Iteration 4/25 | Loss: 0.00133547
Iteration 5/25 | Loss: 0.00132895
Iteration 6/25 | Loss: 0.00132805
Iteration 7/25 | Loss: 0.00132805
Iteration 8/25 | Loss: 0.00132805
Iteration 9/25 | Loss: 0.00132805
Iteration 10/25 | Loss: 0.00132805
Iteration 11/25 | Loss: 0.00132805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013280498096719384, 0.0013280498096719384, 0.0013280498096719384, 0.0013280498096719384, 0.0013280498096719384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013280498096719384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58112121
Iteration 2/25 | Loss: 0.00087234
Iteration 3/25 | Loss: 0.00087232
Iteration 4/25 | Loss: 0.00087232
Iteration 5/25 | Loss: 0.00087232
Iteration 6/25 | Loss: 0.00087232
Iteration 7/25 | Loss: 0.00087232
Iteration 8/25 | Loss: 0.00087232
Iteration 9/25 | Loss: 0.00087232
Iteration 10/25 | Loss: 0.00087232
Iteration 11/25 | Loss: 0.00087232
Iteration 12/25 | Loss: 0.00087232
Iteration 13/25 | Loss: 0.00087232
Iteration 14/25 | Loss: 0.00087232
Iteration 15/25 | Loss: 0.00087232
Iteration 16/25 | Loss: 0.00087232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008723220671527088, 0.0008723220671527088, 0.0008723220671527088, 0.0008723220671527088, 0.0008723220671527088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008723220671527088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087232
Iteration 2/1000 | Loss: 0.00004439
Iteration 3/1000 | Loss: 0.00003236
Iteration 4/1000 | Loss: 0.00002929
Iteration 5/1000 | Loss: 0.00002799
Iteration 6/1000 | Loss: 0.00002750
Iteration 7/1000 | Loss: 0.00002710
Iteration 8/1000 | Loss: 0.00002685
Iteration 9/1000 | Loss: 0.00002684
Iteration 10/1000 | Loss: 0.00002666
Iteration 11/1000 | Loss: 0.00002651
Iteration 12/1000 | Loss: 0.00002639
Iteration 13/1000 | Loss: 0.00002635
Iteration 14/1000 | Loss: 0.00002634
Iteration 15/1000 | Loss: 0.00002634
Iteration 16/1000 | Loss: 0.00002633
Iteration 17/1000 | Loss: 0.00002629
Iteration 18/1000 | Loss: 0.00002629
Iteration 19/1000 | Loss: 0.00002627
Iteration 20/1000 | Loss: 0.00002627
Iteration 21/1000 | Loss: 0.00002626
Iteration 22/1000 | Loss: 0.00002626
Iteration 23/1000 | Loss: 0.00002626
Iteration 24/1000 | Loss: 0.00002626
Iteration 25/1000 | Loss: 0.00002626
Iteration 26/1000 | Loss: 0.00002626
Iteration 27/1000 | Loss: 0.00002626
Iteration 28/1000 | Loss: 0.00002626
Iteration 29/1000 | Loss: 0.00002626
Iteration 30/1000 | Loss: 0.00002626
Iteration 31/1000 | Loss: 0.00002626
Iteration 32/1000 | Loss: 0.00002624
Iteration 33/1000 | Loss: 0.00002624
Iteration 34/1000 | Loss: 0.00002623
Iteration 35/1000 | Loss: 0.00002623
Iteration 36/1000 | Loss: 0.00002623
Iteration 37/1000 | Loss: 0.00002623
Iteration 38/1000 | Loss: 0.00002623
Iteration 39/1000 | Loss: 0.00002623
Iteration 40/1000 | Loss: 0.00002622
Iteration 41/1000 | Loss: 0.00002622
Iteration 42/1000 | Loss: 0.00002622
Iteration 43/1000 | Loss: 0.00002622
Iteration 44/1000 | Loss: 0.00002622
Iteration 45/1000 | Loss: 0.00002622
Iteration 46/1000 | Loss: 0.00002621
Iteration 47/1000 | Loss: 0.00002621
Iteration 48/1000 | Loss: 0.00002621
Iteration 49/1000 | Loss: 0.00002621
Iteration 50/1000 | Loss: 0.00002620
Iteration 51/1000 | Loss: 0.00002620
Iteration 52/1000 | Loss: 0.00002620
Iteration 53/1000 | Loss: 0.00002620
Iteration 54/1000 | Loss: 0.00002620
Iteration 55/1000 | Loss: 0.00002620
Iteration 56/1000 | Loss: 0.00002620
Iteration 57/1000 | Loss: 0.00002620
Iteration 58/1000 | Loss: 0.00002620
Iteration 59/1000 | Loss: 0.00002620
Iteration 60/1000 | Loss: 0.00002620
Iteration 61/1000 | Loss: 0.00002620
Iteration 62/1000 | Loss: 0.00002620
Iteration 63/1000 | Loss: 0.00002620
Iteration 64/1000 | Loss: 0.00002620
Iteration 65/1000 | Loss: 0.00002620
Iteration 66/1000 | Loss: 0.00002620
Iteration 67/1000 | Loss: 0.00002620
Iteration 68/1000 | Loss: 0.00002620
Iteration 69/1000 | Loss: 0.00002620
Iteration 70/1000 | Loss: 0.00002620
Iteration 71/1000 | Loss: 0.00002620
Iteration 72/1000 | Loss: 0.00002620
Iteration 73/1000 | Loss: 0.00002620
Iteration 74/1000 | Loss: 0.00002620
Iteration 75/1000 | Loss: 0.00002620
Iteration 76/1000 | Loss: 0.00002620
Iteration 77/1000 | Loss: 0.00002620
Iteration 78/1000 | Loss: 0.00002620
Iteration 79/1000 | Loss: 0.00002620
Iteration 80/1000 | Loss: 0.00002620
Iteration 81/1000 | Loss: 0.00002620
Iteration 82/1000 | Loss: 0.00002620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [2.6200275897281244e-05, 2.6200275897281244e-05, 2.6200275897281244e-05, 2.6200275897281244e-05, 2.6200275897281244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6200275897281244e-05

Optimization complete. Final v2v error: 4.461105823516846 mm

Highest mean error: 4.772052764892578 mm for frame 221

Lowest mean error: 4.258668422698975 mm for frame 121

Saving results

Total time: 33.99186396598816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490224
Iteration 2/25 | Loss: 0.00143266
Iteration 3/25 | Loss: 0.00135015
Iteration 4/25 | Loss: 0.00133122
Iteration 5/25 | Loss: 0.00132558
Iteration 6/25 | Loss: 0.00132501
Iteration 7/25 | Loss: 0.00132501
Iteration 8/25 | Loss: 0.00132501
Iteration 9/25 | Loss: 0.00132501
Iteration 10/25 | Loss: 0.00132501
Iteration 11/25 | Loss: 0.00132501
Iteration 12/25 | Loss: 0.00132501
Iteration 13/25 | Loss: 0.00132501
Iteration 14/25 | Loss: 0.00132501
Iteration 15/25 | Loss: 0.00132501
Iteration 16/25 | Loss: 0.00132501
Iteration 17/25 | Loss: 0.00132501
Iteration 18/25 | Loss: 0.00132501
Iteration 19/25 | Loss: 0.00132501
Iteration 20/25 | Loss: 0.00132501
Iteration 21/25 | Loss: 0.00132501
Iteration 22/25 | Loss: 0.00132501
Iteration 23/25 | Loss: 0.00132501
Iteration 24/25 | Loss: 0.00132501
Iteration 25/25 | Loss: 0.00132501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44041979
Iteration 2/25 | Loss: 0.00087988
Iteration 3/25 | Loss: 0.00087988
Iteration 4/25 | Loss: 0.00087988
Iteration 5/25 | Loss: 0.00087988
Iteration 6/25 | Loss: 0.00087988
Iteration 7/25 | Loss: 0.00087988
Iteration 8/25 | Loss: 0.00087988
Iteration 9/25 | Loss: 0.00087988
Iteration 10/25 | Loss: 0.00087988
Iteration 11/25 | Loss: 0.00087988
Iteration 12/25 | Loss: 0.00087988
Iteration 13/25 | Loss: 0.00087988
Iteration 14/25 | Loss: 0.00087988
Iteration 15/25 | Loss: 0.00087988
Iteration 16/25 | Loss: 0.00087988
Iteration 17/25 | Loss: 0.00087988
Iteration 18/25 | Loss: 0.00087988
Iteration 19/25 | Loss: 0.00087988
Iteration 20/25 | Loss: 0.00087988
Iteration 21/25 | Loss: 0.00087988
Iteration 22/25 | Loss: 0.00087988
Iteration 23/25 | Loss: 0.00087988
Iteration 24/25 | Loss: 0.00087988
Iteration 25/25 | Loss: 0.00087988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087988
Iteration 2/1000 | Loss: 0.00004765
Iteration 3/1000 | Loss: 0.00003350
Iteration 4/1000 | Loss: 0.00003114
Iteration 5/1000 | Loss: 0.00003010
Iteration 6/1000 | Loss: 0.00002958
Iteration 7/1000 | Loss: 0.00002914
Iteration 8/1000 | Loss: 0.00002894
Iteration 9/1000 | Loss: 0.00002885
Iteration 10/1000 | Loss: 0.00002883
Iteration 11/1000 | Loss: 0.00002882
Iteration 12/1000 | Loss: 0.00002869
Iteration 13/1000 | Loss: 0.00002857
Iteration 14/1000 | Loss: 0.00002857
Iteration 15/1000 | Loss: 0.00002856
Iteration 16/1000 | Loss: 0.00002853
Iteration 17/1000 | Loss: 0.00002852
Iteration 18/1000 | Loss: 0.00002848
Iteration 19/1000 | Loss: 0.00002846
Iteration 20/1000 | Loss: 0.00002845
Iteration 21/1000 | Loss: 0.00002845
Iteration 22/1000 | Loss: 0.00002845
Iteration 23/1000 | Loss: 0.00002845
Iteration 24/1000 | Loss: 0.00002845
Iteration 25/1000 | Loss: 0.00002844
Iteration 26/1000 | Loss: 0.00002844
Iteration 27/1000 | Loss: 0.00002844
Iteration 28/1000 | Loss: 0.00002844
Iteration 29/1000 | Loss: 0.00002844
Iteration 30/1000 | Loss: 0.00002844
Iteration 31/1000 | Loss: 0.00002844
Iteration 32/1000 | Loss: 0.00002844
Iteration 33/1000 | Loss: 0.00002844
Iteration 34/1000 | Loss: 0.00002844
Iteration 35/1000 | Loss: 0.00002844
Iteration 36/1000 | Loss: 0.00002842
Iteration 37/1000 | Loss: 0.00002842
Iteration 38/1000 | Loss: 0.00002839
Iteration 39/1000 | Loss: 0.00002838
Iteration 40/1000 | Loss: 0.00002837
Iteration 41/1000 | Loss: 0.00002837
Iteration 42/1000 | Loss: 0.00002837
Iteration 43/1000 | Loss: 0.00002837
Iteration 44/1000 | Loss: 0.00002837
Iteration 45/1000 | Loss: 0.00002837
Iteration 46/1000 | Loss: 0.00002837
Iteration 47/1000 | Loss: 0.00002837
Iteration 48/1000 | Loss: 0.00002836
Iteration 49/1000 | Loss: 0.00002836
Iteration 50/1000 | Loss: 0.00002833
Iteration 51/1000 | Loss: 0.00002833
Iteration 52/1000 | Loss: 0.00002833
Iteration 53/1000 | Loss: 0.00002833
Iteration 54/1000 | Loss: 0.00002833
Iteration 55/1000 | Loss: 0.00002832
Iteration 56/1000 | Loss: 0.00002832
Iteration 57/1000 | Loss: 0.00002832
Iteration 58/1000 | Loss: 0.00002831
Iteration 59/1000 | Loss: 0.00002830
Iteration 60/1000 | Loss: 0.00002830
Iteration 61/1000 | Loss: 0.00002829
Iteration 62/1000 | Loss: 0.00002829
Iteration 63/1000 | Loss: 0.00002829
Iteration 64/1000 | Loss: 0.00002829
Iteration 65/1000 | Loss: 0.00002828
Iteration 66/1000 | Loss: 0.00002828
Iteration 67/1000 | Loss: 0.00002827
Iteration 68/1000 | Loss: 0.00002826
Iteration 69/1000 | Loss: 0.00002826
Iteration 70/1000 | Loss: 0.00002826
Iteration 71/1000 | Loss: 0.00002826
Iteration 72/1000 | Loss: 0.00002826
Iteration 73/1000 | Loss: 0.00002826
Iteration 74/1000 | Loss: 0.00002826
Iteration 75/1000 | Loss: 0.00002826
Iteration 76/1000 | Loss: 0.00002825
Iteration 77/1000 | Loss: 0.00002825
Iteration 78/1000 | Loss: 0.00002825
Iteration 79/1000 | Loss: 0.00002825
Iteration 80/1000 | Loss: 0.00002825
Iteration 81/1000 | Loss: 0.00002823
Iteration 82/1000 | Loss: 0.00002823
Iteration 83/1000 | Loss: 0.00002822
Iteration 84/1000 | Loss: 0.00002822
Iteration 85/1000 | Loss: 0.00002822
Iteration 86/1000 | Loss: 0.00002822
Iteration 87/1000 | Loss: 0.00002822
Iteration 88/1000 | Loss: 0.00002822
Iteration 89/1000 | Loss: 0.00002822
Iteration 90/1000 | Loss: 0.00002821
Iteration 91/1000 | Loss: 0.00002821
Iteration 92/1000 | Loss: 0.00002821
Iteration 93/1000 | Loss: 0.00002821
Iteration 94/1000 | Loss: 0.00002821
Iteration 95/1000 | Loss: 0.00002821
Iteration 96/1000 | Loss: 0.00002821
Iteration 97/1000 | Loss: 0.00002820
Iteration 98/1000 | Loss: 0.00002820
Iteration 99/1000 | Loss: 0.00002820
Iteration 100/1000 | Loss: 0.00002820
Iteration 101/1000 | Loss: 0.00002820
Iteration 102/1000 | Loss: 0.00002820
Iteration 103/1000 | Loss: 0.00002820
Iteration 104/1000 | Loss: 0.00002820
Iteration 105/1000 | Loss: 0.00002820
Iteration 106/1000 | Loss: 0.00002819
Iteration 107/1000 | Loss: 0.00002819
Iteration 108/1000 | Loss: 0.00002819
Iteration 109/1000 | Loss: 0.00002818
Iteration 110/1000 | Loss: 0.00002818
Iteration 111/1000 | Loss: 0.00002818
Iteration 112/1000 | Loss: 0.00002818
Iteration 113/1000 | Loss: 0.00002818
Iteration 114/1000 | Loss: 0.00002818
Iteration 115/1000 | Loss: 0.00002818
Iteration 116/1000 | Loss: 0.00002818
Iteration 117/1000 | Loss: 0.00002818
Iteration 118/1000 | Loss: 0.00002818
Iteration 119/1000 | Loss: 0.00002818
Iteration 120/1000 | Loss: 0.00002818
Iteration 121/1000 | Loss: 0.00002818
Iteration 122/1000 | Loss: 0.00002818
Iteration 123/1000 | Loss: 0.00002817
Iteration 124/1000 | Loss: 0.00002817
Iteration 125/1000 | Loss: 0.00002817
Iteration 126/1000 | Loss: 0.00002817
Iteration 127/1000 | Loss: 0.00002817
Iteration 128/1000 | Loss: 0.00002817
Iteration 129/1000 | Loss: 0.00002817
Iteration 130/1000 | Loss: 0.00002817
Iteration 131/1000 | Loss: 0.00002817
Iteration 132/1000 | Loss: 0.00002817
Iteration 133/1000 | Loss: 0.00002817
Iteration 134/1000 | Loss: 0.00002817
Iteration 135/1000 | Loss: 0.00002816
Iteration 136/1000 | Loss: 0.00002816
Iteration 137/1000 | Loss: 0.00002816
Iteration 138/1000 | Loss: 0.00002816
Iteration 139/1000 | Loss: 0.00002816
Iteration 140/1000 | Loss: 0.00002816
Iteration 141/1000 | Loss: 0.00002816
Iteration 142/1000 | Loss: 0.00002816
Iteration 143/1000 | Loss: 0.00002816
Iteration 144/1000 | Loss: 0.00002816
Iteration 145/1000 | Loss: 0.00002816
Iteration 146/1000 | Loss: 0.00002816
Iteration 147/1000 | Loss: 0.00002816
Iteration 148/1000 | Loss: 0.00002815
Iteration 149/1000 | Loss: 0.00002815
Iteration 150/1000 | Loss: 0.00002815
Iteration 151/1000 | Loss: 0.00002815
Iteration 152/1000 | Loss: 0.00002815
Iteration 153/1000 | Loss: 0.00002815
Iteration 154/1000 | Loss: 0.00002815
Iteration 155/1000 | Loss: 0.00002815
Iteration 156/1000 | Loss: 0.00002815
Iteration 157/1000 | Loss: 0.00002815
Iteration 158/1000 | Loss: 0.00002815
Iteration 159/1000 | Loss: 0.00002815
Iteration 160/1000 | Loss: 0.00002815
Iteration 161/1000 | Loss: 0.00002815
Iteration 162/1000 | Loss: 0.00002815
Iteration 163/1000 | Loss: 0.00002815
Iteration 164/1000 | Loss: 0.00002815
Iteration 165/1000 | Loss: 0.00002814
Iteration 166/1000 | Loss: 0.00002814
Iteration 167/1000 | Loss: 0.00002814
Iteration 168/1000 | Loss: 0.00002814
Iteration 169/1000 | Loss: 0.00002814
Iteration 170/1000 | Loss: 0.00002814
Iteration 171/1000 | Loss: 0.00002814
Iteration 172/1000 | Loss: 0.00002814
Iteration 173/1000 | Loss: 0.00002814
Iteration 174/1000 | Loss: 0.00002813
Iteration 175/1000 | Loss: 0.00002813
Iteration 176/1000 | Loss: 0.00002813
Iteration 177/1000 | Loss: 0.00002813
Iteration 178/1000 | Loss: 0.00002813
Iteration 179/1000 | Loss: 0.00002813
Iteration 180/1000 | Loss: 0.00002813
Iteration 181/1000 | Loss: 0.00002813
Iteration 182/1000 | Loss: 0.00002813
Iteration 183/1000 | Loss: 0.00002813
Iteration 184/1000 | Loss: 0.00002813
Iteration 185/1000 | Loss: 0.00002813
Iteration 186/1000 | Loss: 0.00002813
Iteration 187/1000 | Loss: 0.00002813
Iteration 188/1000 | Loss: 0.00002813
Iteration 189/1000 | Loss: 0.00002813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.813006540236529e-05, 2.813006540236529e-05, 2.813006540236529e-05, 2.813006540236529e-05, 2.813006540236529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.813006540236529e-05

Optimization complete. Final v2v error: 4.620786666870117 mm

Highest mean error: 5.017615795135498 mm for frame 71

Lowest mean error: 4.260005950927734 mm for frame 109

Saving results

Total time: 37.4476637840271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713385
Iteration 2/25 | Loss: 0.00180845
Iteration 3/25 | Loss: 0.00155283
Iteration 4/25 | Loss: 0.00151602
Iteration 5/25 | Loss: 0.00147154
Iteration 6/25 | Loss: 0.00146078
Iteration 7/25 | Loss: 0.00146863
Iteration 8/25 | Loss: 0.00145675
Iteration 9/25 | Loss: 0.00146743
Iteration 10/25 | Loss: 0.00146544
Iteration 11/25 | Loss: 0.00146010
Iteration 12/25 | Loss: 0.00145599
Iteration 13/25 | Loss: 0.00145587
Iteration 14/25 | Loss: 0.00145585
Iteration 15/25 | Loss: 0.00145585
Iteration 16/25 | Loss: 0.00145584
Iteration 17/25 | Loss: 0.00145584
Iteration 18/25 | Loss: 0.00145584
Iteration 19/25 | Loss: 0.00145584
Iteration 20/25 | Loss: 0.00145582
Iteration 21/25 | Loss: 0.00145582
Iteration 22/25 | Loss: 0.00145582
Iteration 23/25 | Loss: 0.00145582
Iteration 24/25 | Loss: 0.00145582
Iteration 25/25 | Loss: 0.00145582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75781083
Iteration 2/25 | Loss: 0.00131667
Iteration 3/25 | Loss: 0.00131667
Iteration 4/25 | Loss: 0.00131666
Iteration 5/25 | Loss: 0.00131666
Iteration 6/25 | Loss: 0.00131666
Iteration 7/25 | Loss: 0.00131666
Iteration 8/25 | Loss: 0.00131666
Iteration 9/25 | Loss: 0.00131666
Iteration 10/25 | Loss: 0.00131666
Iteration 11/25 | Loss: 0.00131666
Iteration 12/25 | Loss: 0.00131666
Iteration 13/25 | Loss: 0.00131666
Iteration 14/25 | Loss: 0.00131666
Iteration 15/25 | Loss: 0.00131666
Iteration 16/25 | Loss: 0.00131666
Iteration 17/25 | Loss: 0.00131666
Iteration 18/25 | Loss: 0.00131666
Iteration 19/25 | Loss: 0.00131666
Iteration 20/25 | Loss: 0.00131666
Iteration 21/25 | Loss: 0.00131666
Iteration 22/25 | Loss: 0.00131666
Iteration 23/25 | Loss: 0.00131666
Iteration 24/25 | Loss: 0.00131666
Iteration 25/25 | Loss: 0.00131666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131666
Iteration 2/1000 | Loss: 0.00009140
Iteration 3/1000 | Loss: 0.00006899
Iteration 4/1000 | Loss: 0.00005225
Iteration 5/1000 | Loss: 0.00004670
Iteration 6/1000 | Loss: 0.00004414
Iteration 7/1000 | Loss: 0.00004163
Iteration 8/1000 | Loss: 0.00066221
Iteration 9/1000 | Loss: 0.00005243
Iteration 10/1000 | Loss: 0.00004031
Iteration 11/1000 | Loss: 0.00003701
Iteration 12/1000 | Loss: 0.00003557
Iteration 13/1000 | Loss: 0.00003448
Iteration 14/1000 | Loss: 0.00003377
Iteration 15/1000 | Loss: 0.00003329
Iteration 16/1000 | Loss: 0.00003294
Iteration 17/1000 | Loss: 0.00003278
Iteration 18/1000 | Loss: 0.00003277
Iteration 19/1000 | Loss: 0.00003277
Iteration 20/1000 | Loss: 0.00003266
Iteration 21/1000 | Loss: 0.00003264
Iteration 22/1000 | Loss: 0.00003263
Iteration 23/1000 | Loss: 0.00003261
Iteration 24/1000 | Loss: 0.00003260
Iteration 25/1000 | Loss: 0.00003259
Iteration 26/1000 | Loss: 0.00003259
Iteration 27/1000 | Loss: 0.00003252
Iteration 28/1000 | Loss: 0.00003252
Iteration 29/1000 | Loss: 0.00003250
Iteration 30/1000 | Loss: 0.00003246
Iteration 31/1000 | Loss: 0.00003246
Iteration 32/1000 | Loss: 0.00003246
Iteration 33/1000 | Loss: 0.00003240
Iteration 34/1000 | Loss: 0.00003239
Iteration 35/1000 | Loss: 0.00003238
Iteration 36/1000 | Loss: 0.00003238
Iteration 37/1000 | Loss: 0.00003234
Iteration 38/1000 | Loss: 0.00003234
Iteration 39/1000 | Loss: 0.00003232
Iteration 40/1000 | Loss: 0.00003227
Iteration 41/1000 | Loss: 0.00003227
Iteration 42/1000 | Loss: 0.00003223
Iteration 43/1000 | Loss: 0.00003218
Iteration 44/1000 | Loss: 0.00003218
Iteration 45/1000 | Loss: 0.00003218
Iteration 46/1000 | Loss: 0.00003217
Iteration 47/1000 | Loss: 0.00003217
Iteration 48/1000 | Loss: 0.00003214
Iteration 49/1000 | Loss: 0.00003212
Iteration 50/1000 | Loss: 0.00003211
Iteration 51/1000 | Loss: 0.00003210
Iteration 52/1000 | Loss: 0.00003210
Iteration 53/1000 | Loss: 0.00003209
Iteration 54/1000 | Loss: 0.00003209
Iteration 55/1000 | Loss: 0.00003209
Iteration 56/1000 | Loss: 0.00003209
Iteration 57/1000 | Loss: 0.00003209
Iteration 58/1000 | Loss: 0.00003209
Iteration 59/1000 | Loss: 0.00003208
Iteration 60/1000 | Loss: 0.00003208
Iteration 61/1000 | Loss: 0.00003208
Iteration 62/1000 | Loss: 0.00003207
Iteration 63/1000 | Loss: 0.00003207
Iteration 64/1000 | Loss: 0.00003206
Iteration 65/1000 | Loss: 0.00003206
Iteration 66/1000 | Loss: 0.00003206
Iteration 67/1000 | Loss: 0.00003206
Iteration 68/1000 | Loss: 0.00003205
Iteration 69/1000 | Loss: 0.00003205
Iteration 70/1000 | Loss: 0.00003205
Iteration 71/1000 | Loss: 0.00003205
Iteration 72/1000 | Loss: 0.00003204
Iteration 73/1000 | Loss: 0.00003204
Iteration 74/1000 | Loss: 0.00003204
Iteration 75/1000 | Loss: 0.00003204
Iteration 76/1000 | Loss: 0.00003204
Iteration 77/1000 | Loss: 0.00003203
Iteration 78/1000 | Loss: 0.00003203
Iteration 79/1000 | Loss: 0.00003203
Iteration 80/1000 | Loss: 0.00003203
Iteration 81/1000 | Loss: 0.00003203
Iteration 82/1000 | Loss: 0.00003202
Iteration 83/1000 | Loss: 0.00003202
Iteration 84/1000 | Loss: 0.00003202
Iteration 85/1000 | Loss: 0.00003202
Iteration 86/1000 | Loss: 0.00003202
Iteration 87/1000 | Loss: 0.00003202
Iteration 88/1000 | Loss: 0.00003201
Iteration 89/1000 | Loss: 0.00003201
Iteration 90/1000 | Loss: 0.00003201
Iteration 91/1000 | Loss: 0.00003201
Iteration 92/1000 | Loss: 0.00003201
Iteration 93/1000 | Loss: 0.00003201
Iteration 94/1000 | Loss: 0.00003200
Iteration 95/1000 | Loss: 0.00003200
Iteration 96/1000 | Loss: 0.00003200
Iteration 97/1000 | Loss: 0.00003200
Iteration 98/1000 | Loss: 0.00003200
Iteration 99/1000 | Loss: 0.00003200
Iteration 100/1000 | Loss: 0.00003200
Iteration 101/1000 | Loss: 0.00003199
Iteration 102/1000 | Loss: 0.00003199
Iteration 103/1000 | Loss: 0.00003199
Iteration 104/1000 | Loss: 0.00003199
Iteration 105/1000 | Loss: 0.00003199
Iteration 106/1000 | Loss: 0.00003199
Iteration 107/1000 | Loss: 0.00003199
Iteration 108/1000 | Loss: 0.00003199
Iteration 109/1000 | Loss: 0.00003199
Iteration 110/1000 | Loss: 0.00003199
Iteration 111/1000 | Loss: 0.00003199
Iteration 112/1000 | Loss: 0.00003199
Iteration 113/1000 | Loss: 0.00003198
Iteration 114/1000 | Loss: 0.00003198
Iteration 115/1000 | Loss: 0.00003198
Iteration 116/1000 | Loss: 0.00003198
Iteration 117/1000 | Loss: 0.00003198
Iteration 118/1000 | Loss: 0.00003198
Iteration 119/1000 | Loss: 0.00003198
Iteration 120/1000 | Loss: 0.00003198
Iteration 121/1000 | Loss: 0.00003198
Iteration 122/1000 | Loss: 0.00003198
Iteration 123/1000 | Loss: 0.00003197
Iteration 124/1000 | Loss: 0.00003197
Iteration 125/1000 | Loss: 0.00003197
Iteration 126/1000 | Loss: 0.00003197
Iteration 127/1000 | Loss: 0.00003197
Iteration 128/1000 | Loss: 0.00003197
Iteration 129/1000 | Loss: 0.00003197
Iteration 130/1000 | Loss: 0.00003197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [3.197362457285635e-05, 3.197362457285635e-05, 3.197362457285635e-05, 3.197362457285635e-05, 3.197362457285635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.197362457285635e-05

Optimization complete. Final v2v error: 4.8655104637146 mm

Highest mean error: 6.240964889526367 mm for frame 54

Lowest mean error: 4.35142707824707 mm for frame 139

Saving results

Total time: 63.88142800331116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01147796
Iteration 2/25 | Loss: 0.01147796
Iteration 3/25 | Loss: 0.01147796
Iteration 4/25 | Loss: 0.01147796
Iteration 5/25 | Loss: 0.01147796
Iteration 6/25 | Loss: 0.01147796
Iteration 7/25 | Loss: 0.01147796
Iteration 8/25 | Loss: 0.01147796
Iteration 9/25 | Loss: 0.01147796
Iteration 10/25 | Loss: 0.01147796
Iteration 11/25 | Loss: 0.01147796
Iteration 12/25 | Loss: 0.01147796
Iteration 13/25 | Loss: 0.01147796
Iteration 14/25 | Loss: 0.01147796
Iteration 15/25 | Loss: 0.01147795
Iteration 16/25 | Loss: 0.01147795
Iteration 17/25 | Loss: 0.01147795
Iteration 18/25 | Loss: 0.01147795
Iteration 19/25 | Loss: 0.01147795
Iteration 20/25 | Loss: 0.01147795
Iteration 21/25 | Loss: 0.01147795
Iteration 22/25 | Loss: 0.01147795
Iteration 23/25 | Loss: 0.01147795
Iteration 24/25 | Loss: 0.01147795
Iteration 25/25 | Loss: 0.01147795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22166300
Iteration 2/25 | Loss: 0.00352014
Iteration 3/25 | Loss: 0.00352005
Iteration 4/25 | Loss: 0.00352005
Iteration 5/25 | Loss: 0.00352005
Iteration 6/25 | Loss: 0.00352005
Iteration 7/25 | Loss: 0.00352005
Iteration 8/25 | Loss: 0.00352005
Iteration 9/25 | Loss: 0.00352005
Iteration 10/25 | Loss: 0.00352005
Iteration 11/25 | Loss: 0.00352005
Iteration 12/25 | Loss: 0.00352005
Iteration 13/25 | Loss: 0.00352005
Iteration 14/25 | Loss: 0.00352005
Iteration 15/25 | Loss: 0.00352005
Iteration 16/25 | Loss: 0.00352005
Iteration 17/25 | Loss: 0.00352005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0035200456622987986, 0.0035200456622987986, 0.0035200456622987986, 0.0035200456622987986, 0.0035200456622987986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035200456622987986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00352005
Iteration 2/1000 | Loss: 0.00204284
Iteration 3/1000 | Loss: 0.00051036
Iteration 4/1000 | Loss: 0.00065239
Iteration 5/1000 | Loss: 0.00090217
Iteration 6/1000 | Loss: 0.00008235
Iteration 7/1000 | Loss: 0.00054652
Iteration 8/1000 | Loss: 0.00022883
Iteration 9/1000 | Loss: 0.00019475
Iteration 10/1000 | Loss: 0.00077456
Iteration 11/1000 | Loss: 0.00012164
Iteration 12/1000 | Loss: 0.00007253
Iteration 13/1000 | Loss: 0.00084159
Iteration 14/1000 | Loss: 0.00044615
Iteration 15/1000 | Loss: 0.00037684
Iteration 16/1000 | Loss: 0.00009300
Iteration 17/1000 | Loss: 0.00005634
Iteration 18/1000 | Loss: 0.00004298
Iteration 19/1000 | Loss: 0.00104138
Iteration 20/1000 | Loss: 0.00137061
Iteration 21/1000 | Loss: 0.00231894
Iteration 22/1000 | Loss: 0.00570491
Iteration 23/1000 | Loss: 0.00593858
Iteration 24/1000 | Loss: 0.00155778
Iteration 25/1000 | Loss: 0.00065036
Iteration 26/1000 | Loss: 0.00029768
Iteration 27/1000 | Loss: 0.00006187
Iteration 28/1000 | Loss: 0.00016933
Iteration 29/1000 | Loss: 0.00004750
Iteration 30/1000 | Loss: 0.00011904
Iteration 31/1000 | Loss: 0.00046290
Iteration 32/1000 | Loss: 0.00006946
Iteration 33/1000 | Loss: 0.00004696
Iteration 34/1000 | Loss: 0.00079494
Iteration 35/1000 | Loss: 0.00175815
Iteration 36/1000 | Loss: 0.00083533
Iteration 37/1000 | Loss: 0.00032407
Iteration 38/1000 | Loss: 0.00169742
Iteration 39/1000 | Loss: 0.00018277
Iteration 40/1000 | Loss: 0.00013762
Iteration 41/1000 | Loss: 0.00011189
Iteration 42/1000 | Loss: 0.00003688
Iteration 43/1000 | Loss: 0.00020893
Iteration 44/1000 | Loss: 0.00003671
Iteration 45/1000 | Loss: 0.00041958
Iteration 46/1000 | Loss: 0.00006568
Iteration 47/1000 | Loss: 0.00007628
Iteration 48/1000 | Loss: 0.00007032
Iteration 49/1000 | Loss: 0.00005844
Iteration 50/1000 | Loss: 0.00004284
Iteration 51/1000 | Loss: 0.00004398
Iteration 52/1000 | Loss: 0.00016135
Iteration 53/1000 | Loss: 0.00010587
Iteration 54/1000 | Loss: 0.00043784
Iteration 55/1000 | Loss: 0.00026845
Iteration 56/1000 | Loss: 0.00009729
Iteration 57/1000 | Loss: 0.00024513
Iteration 58/1000 | Loss: 0.00003425
Iteration 59/1000 | Loss: 0.00030727
Iteration 60/1000 | Loss: 0.00010561
Iteration 61/1000 | Loss: 0.00003387
Iteration 62/1000 | Loss: 0.00009590
Iteration 63/1000 | Loss: 0.00008448
Iteration 64/1000 | Loss: 0.00007905
Iteration 65/1000 | Loss: 0.00007930
Iteration 66/1000 | Loss: 0.00005786
Iteration 67/1000 | Loss: 0.00008300
Iteration 68/1000 | Loss: 0.00015893
Iteration 69/1000 | Loss: 0.00022735
Iteration 70/1000 | Loss: 0.00022967
Iteration 71/1000 | Loss: 0.00008798
Iteration 72/1000 | Loss: 0.00007086
Iteration 73/1000 | Loss: 0.00003302
Iteration 74/1000 | Loss: 0.00004546
Iteration 75/1000 | Loss: 0.00004313
Iteration 76/1000 | Loss: 0.00003284
Iteration 77/1000 | Loss: 0.00003284
Iteration 78/1000 | Loss: 0.00003284
Iteration 79/1000 | Loss: 0.00003282
Iteration 80/1000 | Loss: 0.00007987
Iteration 81/1000 | Loss: 0.00003660
Iteration 82/1000 | Loss: 0.00003272
Iteration 83/1000 | Loss: 0.00003271
Iteration 84/1000 | Loss: 0.00003271
Iteration 85/1000 | Loss: 0.00003268
Iteration 86/1000 | Loss: 0.00003265
Iteration 87/1000 | Loss: 0.00003265
Iteration 88/1000 | Loss: 0.00003264
Iteration 89/1000 | Loss: 0.00003264
Iteration 90/1000 | Loss: 0.00003262
Iteration 91/1000 | Loss: 0.00003262
Iteration 92/1000 | Loss: 0.00003262
Iteration 93/1000 | Loss: 0.00003262
Iteration 94/1000 | Loss: 0.00003261
Iteration 95/1000 | Loss: 0.00003261
Iteration 96/1000 | Loss: 0.00003261
Iteration 97/1000 | Loss: 0.00003261
Iteration 98/1000 | Loss: 0.00003261
Iteration 99/1000 | Loss: 0.00003261
Iteration 100/1000 | Loss: 0.00003260
Iteration 101/1000 | Loss: 0.00003260
Iteration 102/1000 | Loss: 0.00003260
Iteration 103/1000 | Loss: 0.00003260
Iteration 104/1000 | Loss: 0.00003260
Iteration 105/1000 | Loss: 0.00003259
Iteration 106/1000 | Loss: 0.00003259
Iteration 107/1000 | Loss: 0.00003259
Iteration 108/1000 | Loss: 0.00003259
Iteration 109/1000 | Loss: 0.00003258
Iteration 110/1000 | Loss: 0.00003258
Iteration 111/1000 | Loss: 0.00003258
Iteration 112/1000 | Loss: 0.00003258
Iteration 113/1000 | Loss: 0.00003258
Iteration 114/1000 | Loss: 0.00003258
Iteration 115/1000 | Loss: 0.00003258
Iteration 116/1000 | Loss: 0.00003258
Iteration 117/1000 | Loss: 0.00003258
Iteration 118/1000 | Loss: 0.00003258
Iteration 119/1000 | Loss: 0.00003258
Iteration 120/1000 | Loss: 0.00003258
Iteration 121/1000 | Loss: 0.00003257
Iteration 122/1000 | Loss: 0.00003257
Iteration 123/1000 | Loss: 0.00003257
Iteration 124/1000 | Loss: 0.00003257
Iteration 125/1000 | Loss: 0.00003257
Iteration 126/1000 | Loss: 0.00003257
Iteration 127/1000 | Loss: 0.00003257
Iteration 128/1000 | Loss: 0.00003257
Iteration 129/1000 | Loss: 0.00003257
Iteration 130/1000 | Loss: 0.00003257
Iteration 131/1000 | Loss: 0.00003256
Iteration 132/1000 | Loss: 0.00003256
Iteration 133/1000 | Loss: 0.00003256
Iteration 134/1000 | Loss: 0.00003256
Iteration 135/1000 | Loss: 0.00003256
Iteration 136/1000 | Loss: 0.00003256
Iteration 137/1000 | Loss: 0.00003255
Iteration 138/1000 | Loss: 0.00003255
Iteration 139/1000 | Loss: 0.00003255
Iteration 140/1000 | Loss: 0.00003255
Iteration 141/1000 | Loss: 0.00003255
Iteration 142/1000 | Loss: 0.00003255
Iteration 143/1000 | Loss: 0.00003255
Iteration 144/1000 | Loss: 0.00003255
Iteration 145/1000 | Loss: 0.00003254
Iteration 146/1000 | Loss: 0.00003254
Iteration 147/1000 | Loss: 0.00003254
Iteration 148/1000 | Loss: 0.00003254
Iteration 149/1000 | Loss: 0.00003254
Iteration 150/1000 | Loss: 0.00003254
Iteration 151/1000 | Loss: 0.00003253
Iteration 152/1000 | Loss: 0.00003253
Iteration 153/1000 | Loss: 0.00003253
Iteration 154/1000 | Loss: 0.00003252
Iteration 155/1000 | Loss: 0.00003252
Iteration 156/1000 | Loss: 0.00003252
Iteration 157/1000 | Loss: 0.00003252
Iteration 158/1000 | Loss: 0.00003251
Iteration 159/1000 | Loss: 0.00003251
Iteration 160/1000 | Loss: 0.00003251
Iteration 161/1000 | Loss: 0.00003251
Iteration 162/1000 | Loss: 0.00018087
Iteration 163/1000 | Loss: 0.00069829
Iteration 164/1000 | Loss: 0.00028694
Iteration 165/1000 | Loss: 0.00023621
Iteration 166/1000 | Loss: 0.00007937
Iteration 167/1000 | Loss: 0.00003858
Iteration 168/1000 | Loss: 0.00003270
Iteration 169/1000 | Loss: 0.00011698
Iteration 170/1000 | Loss: 0.00004323
Iteration 171/1000 | Loss: 0.00003257
Iteration 172/1000 | Loss: 0.00003256
Iteration 173/1000 | Loss: 0.00003252
Iteration 174/1000 | Loss: 0.00003251
Iteration 175/1000 | Loss: 0.00012543
Iteration 176/1000 | Loss: 0.00003367
Iteration 177/1000 | Loss: 0.00004731
Iteration 178/1000 | Loss: 0.00003695
Iteration 179/1000 | Loss: 0.00011589
Iteration 180/1000 | Loss: 0.00006020
Iteration 181/1000 | Loss: 0.00008081
Iteration 182/1000 | Loss: 0.00003258
Iteration 183/1000 | Loss: 0.00005292
Iteration 184/1000 | Loss: 0.00003262
Iteration 185/1000 | Loss: 0.00007204
Iteration 186/1000 | Loss: 0.00006328
Iteration 187/1000 | Loss: 0.00003261
Iteration 188/1000 | Loss: 0.00005154
Iteration 189/1000 | Loss: 0.00003259
Iteration 190/1000 | Loss: 0.00003254
Iteration 191/1000 | Loss: 0.00003254
Iteration 192/1000 | Loss: 0.00003253
Iteration 193/1000 | Loss: 0.00003251
Iteration 194/1000 | Loss: 0.00003251
Iteration 195/1000 | Loss: 0.00003250
Iteration 196/1000 | Loss: 0.00003250
Iteration 197/1000 | Loss: 0.00003250
Iteration 198/1000 | Loss: 0.00003250
Iteration 199/1000 | Loss: 0.00003250
Iteration 200/1000 | Loss: 0.00003250
Iteration 201/1000 | Loss: 0.00003250
Iteration 202/1000 | Loss: 0.00003250
Iteration 203/1000 | Loss: 0.00003250
Iteration 204/1000 | Loss: 0.00003250
Iteration 205/1000 | Loss: 0.00003250
Iteration 206/1000 | Loss: 0.00005805
Iteration 207/1000 | Loss: 0.00004044
Iteration 208/1000 | Loss: 0.00003251
Iteration 209/1000 | Loss: 0.00003250
Iteration 210/1000 | Loss: 0.00003248
Iteration 211/1000 | Loss: 0.00003248
Iteration 212/1000 | Loss: 0.00003248
Iteration 213/1000 | Loss: 0.00003248
Iteration 214/1000 | Loss: 0.00003248
Iteration 215/1000 | Loss: 0.00003248
Iteration 216/1000 | Loss: 0.00003247
Iteration 217/1000 | Loss: 0.00003247
Iteration 218/1000 | Loss: 0.00003247
Iteration 219/1000 | Loss: 0.00003247
Iteration 220/1000 | Loss: 0.00003247
Iteration 221/1000 | Loss: 0.00003247
Iteration 222/1000 | Loss: 0.00003247
Iteration 223/1000 | Loss: 0.00003247
Iteration 224/1000 | Loss: 0.00003247
Iteration 225/1000 | Loss: 0.00003247
Iteration 226/1000 | Loss: 0.00003246
Iteration 227/1000 | Loss: 0.00003246
Iteration 228/1000 | Loss: 0.00003246
Iteration 229/1000 | Loss: 0.00003245
Iteration 230/1000 | Loss: 0.00003245
Iteration 231/1000 | Loss: 0.00003244
Iteration 232/1000 | Loss: 0.00003244
Iteration 233/1000 | Loss: 0.00003244
Iteration 234/1000 | Loss: 0.00003244
Iteration 235/1000 | Loss: 0.00003244
Iteration 236/1000 | Loss: 0.00003244
Iteration 237/1000 | Loss: 0.00003244
Iteration 238/1000 | Loss: 0.00003244
Iteration 239/1000 | Loss: 0.00003244
Iteration 240/1000 | Loss: 0.00003244
Iteration 241/1000 | Loss: 0.00003244
Iteration 242/1000 | Loss: 0.00003244
Iteration 243/1000 | Loss: 0.00003244
Iteration 244/1000 | Loss: 0.00003244
Iteration 245/1000 | Loss: 0.00003244
Iteration 246/1000 | Loss: 0.00003244
Iteration 247/1000 | Loss: 0.00003244
Iteration 248/1000 | Loss: 0.00003244
Iteration 249/1000 | Loss: 0.00003244
Iteration 250/1000 | Loss: 0.00003243
Iteration 251/1000 | Loss: 0.00003243
Iteration 252/1000 | Loss: 0.00003243
Iteration 253/1000 | Loss: 0.00003243
Iteration 254/1000 | Loss: 0.00003243
Iteration 255/1000 | Loss: 0.00003243
Iteration 256/1000 | Loss: 0.00003243
Iteration 257/1000 | Loss: 0.00003243
Iteration 258/1000 | Loss: 0.00003243
Iteration 259/1000 | Loss: 0.00003243
Iteration 260/1000 | Loss: 0.00003243
Iteration 261/1000 | Loss: 0.00003243
Iteration 262/1000 | Loss: 0.00003243
Iteration 263/1000 | Loss: 0.00003243
Iteration 264/1000 | Loss: 0.00003243
Iteration 265/1000 | Loss: 0.00003243
Iteration 266/1000 | Loss: 0.00003243
Iteration 267/1000 | Loss: 0.00003243
Iteration 268/1000 | Loss: 0.00003243
Iteration 269/1000 | Loss: 0.00003243
Iteration 270/1000 | Loss: 0.00003242
Iteration 271/1000 | Loss: 0.00003242
Iteration 272/1000 | Loss: 0.00003242
Iteration 273/1000 | Loss: 0.00003242
Iteration 274/1000 | Loss: 0.00003242
Iteration 275/1000 | Loss: 0.00003242
Iteration 276/1000 | Loss: 0.00003242
Iteration 277/1000 | Loss: 0.00003242
Iteration 278/1000 | Loss: 0.00003242
Iteration 279/1000 | Loss: 0.00003242
Iteration 280/1000 | Loss: 0.00003242
Iteration 281/1000 | Loss: 0.00003242
Iteration 282/1000 | Loss: 0.00003242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [3.2422605727333575e-05, 3.2422605727333575e-05, 3.2422605727333575e-05, 3.2422605727333575e-05, 3.2422605727333575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2422605727333575e-05

Optimization complete. Final v2v error: 4.877213001251221 mm

Highest mean error: 5.443936347961426 mm for frame 2

Lowest mean error: 4.367018222808838 mm for frame 83

Saving results

Total time: 163.06758785247803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966782
Iteration 2/25 | Loss: 0.00153407
Iteration 3/25 | Loss: 0.00140668
Iteration 4/25 | Loss: 0.00138647
Iteration 5/25 | Loss: 0.00137884
Iteration 6/25 | Loss: 0.00137704
Iteration 7/25 | Loss: 0.00137704
Iteration 8/25 | Loss: 0.00137704
Iteration 9/25 | Loss: 0.00137704
Iteration 10/25 | Loss: 0.00137704
Iteration 11/25 | Loss: 0.00137704
Iteration 12/25 | Loss: 0.00137704
Iteration 13/25 | Loss: 0.00137704
Iteration 14/25 | Loss: 0.00137704
Iteration 15/25 | Loss: 0.00137704
Iteration 16/25 | Loss: 0.00137704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013770436635240912, 0.0013770436635240912, 0.0013770436635240912, 0.0013770436635240912, 0.0013770436635240912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013770436635240912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40634215
Iteration 2/25 | Loss: 0.00112120
Iteration 3/25 | Loss: 0.00112111
Iteration 4/25 | Loss: 0.00112111
Iteration 5/25 | Loss: 0.00112111
Iteration 6/25 | Loss: 0.00112111
Iteration 7/25 | Loss: 0.00112111
Iteration 8/25 | Loss: 0.00112111
Iteration 9/25 | Loss: 0.00112111
Iteration 10/25 | Loss: 0.00112111
Iteration 11/25 | Loss: 0.00112111
Iteration 12/25 | Loss: 0.00112111
Iteration 13/25 | Loss: 0.00112111
Iteration 14/25 | Loss: 0.00112111
Iteration 15/25 | Loss: 0.00112111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011211065575480461, 0.0011211065575480461, 0.0011211065575480461, 0.0011211065575480461, 0.0011211065575480461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011211065575480461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112111
Iteration 2/1000 | Loss: 0.00005026
Iteration 3/1000 | Loss: 0.00003287
Iteration 4/1000 | Loss: 0.00002817
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002536
Iteration 7/1000 | Loss: 0.00002474
Iteration 8/1000 | Loss: 0.00002431
Iteration 9/1000 | Loss: 0.00002402
Iteration 10/1000 | Loss: 0.00002391
Iteration 11/1000 | Loss: 0.00002383
Iteration 12/1000 | Loss: 0.00002382
Iteration 13/1000 | Loss: 0.00002381
Iteration 14/1000 | Loss: 0.00002380
Iteration 15/1000 | Loss: 0.00002375
Iteration 16/1000 | Loss: 0.00002363
Iteration 17/1000 | Loss: 0.00002354
Iteration 18/1000 | Loss: 0.00002354
Iteration 19/1000 | Loss: 0.00002353
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002352
Iteration 22/1000 | Loss: 0.00002352
Iteration 23/1000 | Loss: 0.00002352
Iteration 24/1000 | Loss: 0.00002352
Iteration 25/1000 | Loss: 0.00002352
Iteration 26/1000 | Loss: 0.00002351
Iteration 27/1000 | Loss: 0.00002351
Iteration 28/1000 | Loss: 0.00002351
Iteration 29/1000 | Loss: 0.00002351
Iteration 30/1000 | Loss: 0.00002351
Iteration 31/1000 | Loss: 0.00002350
Iteration 32/1000 | Loss: 0.00002349
Iteration 33/1000 | Loss: 0.00002349
Iteration 34/1000 | Loss: 0.00002349
Iteration 35/1000 | Loss: 0.00002349
Iteration 36/1000 | Loss: 0.00002348
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002348
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002347
Iteration 41/1000 | Loss: 0.00002346
Iteration 42/1000 | Loss: 0.00002346
Iteration 43/1000 | Loss: 0.00002346
Iteration 44/1000 | Loss: 0.00002345
Iteration 45/1000 | Loss: 0.00002345
Iteration 46/1000 | Loss: 0.00002345
Iteration 47/1000 | Loss: 0.00002345
Iteration 48/1000 | Loss: 0.00002345
Iteration 49/1000 | Loss: 0.00002345
Iteration 50/1000 | Loss: 0.00002345
Iteration 51/1000 | Loss: 0.00002344
Iteration 52/1000 | Loss: 0.00002343
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002342
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002341
Iteration 57/1000 | Loss: 0.00002341
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002341
Iteration 60/1000 | Loss: 0.00002341
Iteration 61/1000 | Loss: 0.00002341
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00002340
Iteration 64/1000 | Loss: 0.00002340
Iteration 65/1000 | Loss: 0.00002340
Iteration 66/1000 | Loss: 0.00002340
Iteration 67/1000 | Loss: 0.00002340
Iteration 68/1000 | Loss: 0.00002340
Iteration 69/1000 | Loss: 0.00002340
Iteration 70/1000 | Loss: 0.00002339
Iteration 71/1000 | Loss: 0.00002339
Iteration 72/1000 | Loss: 0.00002339
Iteration 73/1000 | Loss: 0.00002338
Iteration 74/1000 | Loss: 0.00002338
Iteration 75/1000 | Loss: 0.00002338
Iteration 76/1000 | Loss: 0.00002338
Iteration 77/1000 | Loss: 0.00002337
Iteration 78/1000 | Loss: 0.00002337
Iteration 79/1000 | Loss: 0.00002337
Iteration 80/1000 | Loss: 0.00002337
Iteration 81/1000 | Loss: 0.00002337
Iteration 82/1000 | Loss: 0.00002337
Iteration 83/1000 | Loss: 0.00002337
Iteration 84/1000 | Loss: 0.00002337
Iteration 85/1000 | Loss: 0.00002337
Iteration 86/1000 | Loss: 0.00002336
Iteration 87/1000 | Loss: 0.00002336
Iteration 88/1000 | Loss: 0.00002336
Iteration 89/1000 | Loss: 0.00002336
Iteration 90/1000 | Loss: 0.00002336
Iteration 91/1000 | Loss: 0.00002336
Iteration 92/1000 | Loss: 0.00002336
Iteration 93/1000 | Loss: 0.00002335
Iteration 94/1000 | Loss: 0.00002335
Iteration 95/1000 | Loss: 0.00002335
Iteration 96/1000 | Loss: 0.00002335
Iteration 97/1000 | Loss: 0.00002335
Iteration 98/1000 | Loss: 0.00002335
Iteration 99/1000 | Loss: 0.00002335
Iteration 100/1000 | Loss: 0.00002335
Iteration 101/1000 | Loss: 0.00002335
Iteration 102/1000 | Loss: 0.00002334
Iteration 103/1000 | Loss: 0.00002334
Iteration 104/1000 | Loss: 0.00002334
Iteration 105/1000 | Loss: 0.00002334
Iteration 106/1000 | Loss: 0.00002334
Iteration 107/1000 | Loss: 0.00002334
Iteration 108/1000 | Loss: 0.00002334
Iteration 109/1000 | Loss: 0.00002334
Iteration 110/1000 | Loss: 0.00002334
Iteration 111/1000 | Loss: 0.00002334
Iteration 112/1000 | Loss: 0.00002334
Iteration 113/1000 | Loss: 0.00002334
Iteration 114/1000 | Loss: 0.00002334
Iteration 115/1000 | Loss: 0.00002334
Iteration 116/1000 | Loss: 0.00002334
Iteration 117/1000 | Loss: 0.00002334
Iteration 118/1000 | Loss: 0.00002333
Iteration 119/1000 | Loss: 0.00002333
Iteration 120/1000 | Loss: 0.00002333
Iteration 121/1000 | Loss: 0.00002333
Iteration 122/1000 | Loss: 0.00002333
Iteration 123/1000 | Loss: 0.00002333
Iteration 124/1000 | Loss: 0.00002333
Iteration 125/1000 | Loss: 0.00002333
Iteration 126/1000 | Loss: 0.00002333
Iteration 127/1000 | Loss: 0.00002333
Iteration 128/1000 | Loss: 0.00002333
Iteration 129/1000 | Loss: 0.00002333
Iteration 130/1000 | Loss: 0.00002333
Iteration 131/1000 | Loss: 0.00002333
Iteration 132/1000 | Loss: 0.00002333
Iteration 133/1000 | Loss: 0.00002333
Iteration 134/1000 | Loss: 0.00002333
Iteration 135/1000 | Loss: 0.00002333
Iteration 136/1000 | Loss: 0.00002333
Iteration 137/1000 | Loss: 0.00002332
Iteration 138/1000 | Loss: 0.00002332
Iteration 139/1000 | Loss: 0.00002332
Iteration 140/1000 | Loss: 0.00002332
Iteration 141/1000 | Loss: 0.00002332
Iteration 142/1000 | Loss: 0.00002332
Iteration 143/1000 | Loss: 0.00002332
Iteration 144/1000 | Loss: 0.00002332
Iteration 145/1000 | Loss: 0.00002331
Iteration 146/1000 | Loss: 0.00002331
Iteration 147/1000 | Loss: 0.00002331
Iteration 148/1000 | Loss: 0.00002331
Iteration 149/1000 | Loss: 0.00002331
Iteration 150/1000 | Loss: 0.00002331
Iteration 151/1000 | Loss: 0.00002331
Iteration 152/1000 | Loss: 0.00002331
Iteration 153/1000 | Loss: 0.00002331
Iteration 154/1000 | Loss: 0.00002331
Iteration 155/1000 | Loss: 0.00002331
Iteration 156/1000 | Loss: 0.00002331
Iteration 157/1000 | Loss: 0.00002331
Iteration 158/1000 | Loss: 0.00002331
Iteration 159/1000 | Loss: 0.00002330
Iteration 160/1000 | Loss: 0.00002330
Iteration 161/1000 | Loss: 0.00002330
Iteration 162/1000 | Loss: 0.00002330
Iteration 163/1000 | Loss: 0.00002330
Iteration 164/1000 | Loss: 0.00002330
Iteration 165/1000 | Loss: 0.00002330
Iteration 166/1000 | Loss: 0.00002330
Iteration 167/1000 | Loss: 0.00002330
Iteration 168/1000 | Loss: 0.00002330
Iteration 169/1000 | Loss: 0.00002330
Iteration 170/1000 | Loss: 0.00002330
Iteration 171/1000 | Loss: 0.00002330
Iteration 172/1000 | Loss: 0.00002330
Iteration 173/1000 | Loss: 0.00002330
Iteration 174/1000 | Loss: 0.00002330
Iteration 175/1000 | Loss: 0.00002330
Iteration 176/1000 | Loss: 0.00002330
Iteration 177/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.33009868679801e-05, 2.33009868679801e-05, 2.33009868679801e-05, 2.33009868679801e-05, 2.33009868679801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.33009868679801e-05

Optimization complete. Final v2v error: 4.23785924911499 mm

Highest mean error: 4.620876789093018 mm for frame 209

Lowest mean error: 3.9291832447052 mm for frame 131

Saving results

Total time: 41.907402992248535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00668877
Iteration 2/25 | Loss: 0.00186194
Iteration 3/25 | Loss: 0.00145083
Iteration 4/25 | Loss: 0.00140950
Iteration 5/25 | Loss: 0.00139820
Iteration 6/25 | Loss: 0.00139392
Iteration 7/25 | Loss: 0.00139245
Iteration 8/25 | Loss: 0.00139231
Iteration 9/25 | Loss: 0.00139231
Iteration 10/25 | Loss: 0.00139231
Iteration 11/25 | Loss: 0.00139231
Iteration 12/25 | Loss: 0.00139231
Iteration 13/25 | Loss: 0.00139231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001392305945046246, 0.001392305945046246, 0.001392305945046246, 0.001392305945046246, 0.001392305945046246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001392305945046246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26094210
Iteration 2/25 | Loss: 0.00106982
Iteration 3/25 | Loss: 0.00106982
Iteration 4/25 | Loss: 0.00106982
Iteration 5/25 | Loss: 0.00106982
Iteration 6/25 | Loss: 0.00106982
Iteration 7/25 | Loss: 0.00106982
Iteration 8/25 | Loss: 0.00106982
Iteration 9/25 | Loss: 0.00106982
Iteration 10/25 | Loss: 0.00106982
Iteration 11/25 | Loss: 0.00106982
Iteration 12/25 | Loss: 0.00106982
Iteration 13/25 | Loss: 0.00106982
Iteration 14/25 | Loss: 0.00106982
Iteration 15/25 | Loss: 0.00106982
Iteration 16/25 | Loss: 0.00106982
Iteration 17/25 | Loss: 0.00106982
Iteration 18/25 | Loss: 0.00106982
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001069816411472857, 0.001069816411472857, 0.001069816411472857, 0.001069816411472857, 0.001069816411472857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001069816411472857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106982
Iteration 2/1000 | Loss: 0.00006033
Iteration 3/1000 | Loss: 0.00004493
Iteration 4/1000 | Loss: 0.00003722
Iteration 5/1000 | Loss: 0.00003397
Iteration 6/1000 | Loss: 0.00003271
Iteration 7/1000 | Loss: 0.00003177
Iteration 8/1000 | Loss: 0.00003125
Iteration 9/1000 | Loss: 0.00003089
Iteration 10/1000 | Loss: 0.00003052
Iteration 11/1000 | Loss: 0.00003025
Iteration 12/1000 | Loss: 0.00003024
Iteration 13/1000 | Loss: 0.00003009
Iteration 14/1000 | Loss: 0.00003008
Iteration 15/1000 | Loss: 0.00002994
Iteration 16/1000 | Loss: 0.00002991
Iteration 17/1000 | Loss: 0.00002988
Iteration 18/1000 | Loss: 0.00002988
Iteration 19/1000 | Loss: 0.00002988
Iteration 20/1000 | Loss: 0.00002987
Iteration 21/1000 | Loss: 0.00002987
Iteration 22/1000 | Loss: 0.00002987
Iteration 23/1000 | Loss: 0.00002987
Iteration 24/1000 | Loss: 0.00002987
Iteration 25/1000 | Loss: 0.00002986
Iteration 26/1000 | Loss: 0.00002986
Iteration 27/1000 | Loss: 0.00002983
Iteration 28/1000 | Loss: 0.00002982
Iteration 29/1000 | Loss: 0.00002982
Iteration 30/1000 | Loss: 0.00002982
Iteration 31/1000 | Loss: 0.00002981
Iteration 32/1000 | Loss: 0.00002980
Iteration 33/1000 | Loss: 0.00002978
Iteration 34/1000 | Loss: 0.00002978
Iteration 35/1000 | Loss: 0.00002978
Iteration 36/1000 | Loss: 0.00002978
Iteration 37/1000 | Loss: 0.00002978
Iteration 38/1000 | Loss: 0.00002978
Iteration 39/1000 | Loss: 0.00002978
Iteration 40/1000 | Loss: 0.00002977
Iteration 41/1000 | Loss: 0.00002977
Iteration 42/1000 | Loss: 0.00002977
Iteration 43/1000 | Loss: 0.00002975
Iteration 44/1000 | Loss: 0.00002975
Iteration 45/1000 | Loss: 0.00002975
Iteration 46/1000 | Loss: 0.00002974
Iteration 47/1000 | Loss: 0.00002974
Iteration 48/1000 | Loss: 0.00002974
Iteration 49/1000 | Loss: 0.00002973
Iteration 50/1000 | Loss: 0.00002973
Iteration 51/1000 | Loss: 0.00002972
Iteration 52/1000 | Loss: 0.00002972
Iteration 53/1000 | Loss: 0.00002972
Iteration 54/1000 | Loss: 0.00002972
Iteration 55/1000 | Loss: 0.00002971
Iteration 56/1000 | Loss: 0.00002971
Iteration 57/1000 | Loss: 0.00002971
Iteration 58/1000 | Loss: 0.00002971
Iteration 59/1000 | Loss: 0.00002970
Iteration 60/1000 | Loss: 0.00002970
Iteration 61/1000 | Loss: 0.00002970
Iteration 62/1000 | Loss: 0.00002970
Iteration 63/1000 | Loss: 0.00002969
Iteration 64/1000 | Loss: 0.00002969
Iteration 65/1000 | Loss: 0.00002969
Iteration 66/1000 | Loss: 0.00002968
Iteration 67/1000 | Loss: 0.00002968
Iteration 68/1000 | Loss: 0.00002968
Iteration 69/1000 | Loss: 0.00002968
Iteration 70/1000 | Loss: 0.00002967
Iteration 71/1000 | Loss: 0.00002967
Iteration 72/1000 | Loss: 0.00002967
Iteration 73/1000 | Loss: 0.00002966
Iteration 74/1000 | Loss: 0.00002966
Iteration 75/1000 | Loss: 0.00002966
Iteration 76/1000 | Loss: 0.00002966
Iteration 77/1000 | Loss: 0.00002966
Iteration 78/1000 | Loss: 0.00002966
Iteration 79/1000 | Loss: 0.00002966
Iteration 80/1000 | Loss: 0.00002966
Iteration 81/1000 | Loss: 0.00002966
Iteration 82/1000 | Loss: 0.00002965
Iteration 83/1000 | Loss: 0.00002965
Iteration 84/1000 | Loss: 0.00002965
Iteration 85/1000 | Loss: 0.00002965
Iteration 86/1000 | Loss: 0.00002965
Iteration 87/1000 | Loss: 0.00002965
Iteration 88/1000 | Loss: 0.00002965
Iteration 89/1000 | Loss: 0.00002965
Iteration 90/1000 | Loss: 0.00002965
Iteration 91/1000 | Loss: 0.00002965
Iteration 92/1000 | Loss: 0.00002964
Iteration 93/1000 | Loss: 0.00002964
Iteration 94/1000 | Loss: 0.00002964
Iteration 95/1000 | Loss: 0.00002964
Iteration 96/1000 | Loss: 0.00002964
Iteration 97/1000 | Loss: 0.00002963
Iteration 98/1000 | Loss: 0.00002963
Iteration 99/1000 | Loss: 0.00002963
Iteration 100/1000 | Loss: 0.00002963
Iteration 101/1000 | Loss: 0.00002963
Iteration 102/1000 | Loss: 0.00002963
Iteration 103/1000 | Loss: 0.00002963
Iteration 104/1000 | Loss: 0.00002963
Iteration 105/1000 | Loss: 0.00002963
Iteration 106/1000 | Loss: 0.00002963
Iteration 107/1000 | Loss: 0.00002962
Iteration 108/1000 | Loss: 0.00002962
Iteration 109/1000 | Loss: 0.00002962
Iteration 110/1000 | Loss: 0.00002962
Iteration 111/1000 | Loss: 0.00002962
Iteration 112/1000 | Loss: 0.00002962
Iteration 113/1000 | Loss: 0.00002962
Iteration 114/1000 | Loss: 0.00002962
Iteration 115/1000 | Loss: 0.00002961
Iteration 116/1000 | Loss: 0.00002961
Iteration 117/1000 | Loss: 0.00002961
Iteration 118/1000 | Loss: 0.00002961
Iteration 119/1000 | Loss: 0.00002961
Iteration 120/1000 | Loss: 0.00002960
Iteration 121/1000 | Loss: 0.00002960
Iteration 122/1000 | Loss: 0.00002960
Iteration 123/1000 | Loss: 0.00002960
Iteration 124/1000 | Loss: 0.00002959
Iteration 125/1000 | Loss: 0.00002959
Iteration 126/1000 | Loss: 0.00002959
Iteration 127/1000 | Loss: 0.00002959
Iteration 128/1000 | Loss: 0.00002959
Iteration 129/1000 | Loss: 0.00002959
Iteration 130/1000 | Loss: 0.00002959
Iteration 131/1000 | Loss: 0.00002959
Iteration 132/1000 | Loss: 0.00002959
Iteration 133/1000 | Loss: 0.00002959
Iteration 134/1000 | Loss: 0.00002958
Iteration 135/1000 | Loss: 0.00002958
Iteration 136/1000 | Loss: 0.00002958
Iteration 137/1000 | Loss: 0.00002958
Iteration 138/1000 | Loss: 0.00002958
Iteration 139/1000 | Loss: 0.00002958
Iteration 140/1000 | Loss: 0.00002958
Iteration 141/1000 | Loss: 0.00002958
Iteration 142/1000 | Loss: 0.00002958
Iteration 143/1000 | Loss: 0.00002957
Iteration 144/1000 | Loss: 0.00002957
Iteration 145/1000 | Loss: 0.00002957
Iteration 146/1000 | Loss: 0.00002957
Iteration 147/1000 | Loss: 0.00002957
Iteration 148/1000 | Loss: 0.00002957
Iteration 149/1000 | Loss: 0.00002957
Iteration 150/1000 | Loss: 0.00002957
Iteration 151/1000 | Loss: 0.00002957
Iteration 152/1000 | Loss: 0.00002957
Iteration 153/1000 | Loss: 0.00002957
Iteration 154/1000 | Loss: 0.00002957
Iteration 155/1000 | Loss: 0.00002957
Iteration 156/1000 | Loss: 0.00002957
Iteration 157/1000 | Loss: 0.00002956
Iteration 158/1000 | Loss: 0.00002956
Iteration 159/1000 | Loss: 0.00002956
Iteration 160/1000 | Loss: 0.00002956
Iteration 161/1000 | Loss: 0.00002956
Iteration 162/1000 | Loss: 0.00002956
Iteration 163/1000 | Loss: 0.00002956
Iteration 164/1000 | Loss: 0.00002956
Iteration 165/1000 | Loss: 0.00002956
Iteration 166/1000 | Loss: 0.00002956
Iteration 167/1000 | Loss: 0.00002956
Iteration 168/1000 | Loss: 0.00002955
Iteration 169/1000 | Loss: 0.00002955
Iteration 170/1000 | Loss: 0.00002955
Iteration 171/1000 | Loss: 0.00002955
Iteration 172/1000 | Loss: 0.00002955
Iteration 173/1000 | Loss: 0.00002954
Iteration 174/1000 | Loss: 0.00002954
Iteration 175/1000 | Loss: 0.00002954
Iteration 176/1000 | Loss: 0.00002954
Iteration 177/1000 | Loss: 0.00002954
Iteration 178/1000 | Loss: 0.00002954
Iteration 179/1000 | Loss: 0.00002954
Iteration 180/1000 | Loss: 0.00002954
Iteration 181/1000 | Loss: 0.00002954
Iteration 182/1000 | Loss: 0.00002954
Iteration 183/1000 | Loss: 0.00002953
Iteration 184/1000 | Loss: 0.00002953
Iteration 185/1000 | Loss: 0.00002953
Iteration 186/1000 | Loss: 0.00002953
Iteration 187/1000 | Loss: 0.00002953
Iteration 188/1000 | Loss: 0.00002953
Iteration 189/1000 | Loss: 0.00002953
Iteration 190/1000 | Loss: 0.00002953
Iteration 191/1000 | Loss: 0.00002953
Iteration 192/1000 | Loss: 0.00002953
Iteration 193/1000 | Loss: 0.00002953
Iteration 194/1000 | Loss: 0.00002953
Iteration 195/1000 | Loss: 0.00002953
Iteration 196/1000 | Loss: 0.00002953
Iteration 197/1000 | Loss: 0.00002953
Iteration 198/1000 | Loss: 0.00002953
Iteration 199/1000 | Loss: 0.00002953
Iteration 200/1000 | Loss: 0.00002953
Iteration 201/1000 | Loss: 0.00002953
Iteration 202/1000 | Loss: 0.00002953
Iteration 203/1000 | Loss: 0.00002953
Iteration 204/1000 | Loss: 0.00002953
Iteration 205/1000 | Loss: 0.00002953
Iteration 206/1000 | Loss: 0.00002953
Iteration 207/1000 | Loss: 0.00002953
Iteration 208/1000 | Loss: 0.00002953
Iteration 209/1000 | Loss: 0.00002953
Iteration 210/1000 | Loss: 0.00002953
Iteration 211/1000 | Loss: 0.00002953
Iteration 212/1000 | Loss: 0.00002953
Iteration 213/1000 | Loss: 0.00002953
Iteration 214/1000 | Loss: 0.00002953
Iteration 215/1000 | Loss: 0.00002953
Iteration 216/1000 | Loss: 0.00002953
Iteration 217/1000 | Loss: 0.00002953
Iteration 218/1000 | Loss: 0.00002953
Iteration 219/1000 | Loss: 0.00002953
Iteration 220/1000 | Loss: 0.00002953
Iteration 221/1000 | Loss: 0.00002953
Iteration 222/1000 | Loss: 0.00002953
Iteration 223/1000 | Loss: 0.00002953
Iteration 224/1000 | Loss: 0.00002953
Iteration 225/1000 | Loss: 0.00002953
Iteration 226/1000 | Loss: 0.00002953
Iteration 227/1000 | Loss: 0.00002953
Iteration 228/1000 | Loss: 0.00002953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.952710747194942e-05, 2.952710747194942e-05, 2.952710747194942e-05, 2.952710747194942e-05, 2.952710747194942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.952710747194942e-05

Optimization complete. Final v2v error: 4.636684894561768 mm

Highest mean error: 6.6283745765686035 mm for frame 58

Lowest mean error: 4.161210060119629 mm for frame 122

Saving results

Total time: 44.699925899505615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01142412
Iteration 2/25 | Loss: 0.00194133
Iteration 3/25 | Loss: 0.00160655
Iteration 4/25 | Loss: 0.00157435
Iteration 5/25 | Loss: 0.00155937
Iteration 6/25 | Loss: 0.00155616
Iteration 7/25 | Loss: 0.00155442
Iteration 8/25 | Loss: 0.00155436
Iteration 9/25 | Loss: 0.00155436
Iteration 10/25 | Loss: 0.00155436
Iteration 11/25 | Loss: 0.00155436
Iteration 12/25 | Loss: 0.00155436
Iteration 13/25 | Loss: 0.00155436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001554358284920454, 0.001554358284920454, 0.001554358284920454, 0.001554358284920454, 0.001554358284920454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001554358284920454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06137645
Iteration 2/25 | Loss: 0.00095919
Iteration 3/25 | Loss: 0.00095917
Iteration 4/25 | Loss: 0.00095917
Iteration 5/25 | Loss: 0.00095917
Iteration 6/25 | Loss: 0.00095917
Iteration 7/25 | Loss: 0.00095917
Iteration 8/25 | Loss: 0.00095917
Iteration 9/25 | Loss: 0.00095917
Iteration 10/25 | Loss: 0.00095917
Iteration 11/25 | Loss: 0.00095916
Iteration 12/25 | Loss: 0.00095916
Iteration 13/25 | Loss: 0.00095916
Iteration 14/25 | Loss: 0.00095916
Iteration 15/25 | Loss: 0.00095916
Iteration 16/25 | Loss: 0.00095917
Iteration 17/25 | Loss: 0.00095917
Iteration 18/25 | Loss: 0.00095917
Iteration 19/25 | Loss: 0.00095917
Iteration 20/25 | Loss: 0.00095917
Iteration 21/25 | Loss: 0.00095917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009591650450602174, 0.0009591650450602174, 0.0009591650450602174, 0.0009591650450602174, 0.0009591650450602174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009591650450602174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095917
Iteration 2/1000 | Loss: 0.00010945
Iteration 3/1000 | Loss: 0.00007546
Iteration 4/1000 | Loss: 0.00006737
Iteration 5/1000 | Loss: 0.00006420
Iteration 6/1000 | Loss: 0.00006146
Iteration 7/1000 | Loss: 0.00006004
Iteration 8/1000 | Loss: 0.00005872
Iteration 9/1000 | Loss: 0.00005780
Iteration 10/1000 | Loss: 0.00005729
Iteration 11/1000 | Loss: 0.00005679
Iteration 12/1000 | Loss: 0.00005646
Iteration 13/1000 | Loss: 0.00005627
Iteration 14/1000 | Loss: 0.00005611
Iteration 15/1000 | Loss: 0.00005607
Iteration 16/1000 | Loss: 0.00005600
Iteration 17/1000 | Loss: 0.00005595
Iteration 18/1000 | Loss: 0.00005595
Iteration 19/1000 | Loss: 0.00005589
Iteration 20/1000 | Loss: 0.00005589
Iteration 21/1000 | Loss: 0.00005586
Iteration 22/1000 | Loss: 0.00005585
Iteration 23/1000 | Loss: 0.00005581
Iteration 24/1000 | Loss: 0.00005581
Iteration 25/1000 | Loss: 0.00005579
Iteration 26/1000 | Loss: 0.00005578
Iteration 27/1000 | Loss: 0.00005578
Iteration 28/1000 | Loss: 0.00005578
Iteration 29/1000 | Loss: 0.00005577
Iteration 30/1000 | Loss: 0.00005577
Iteration 31/1000 | Loss: 0.00005574
Iteration 32/1000 | Loss: 0.00005573
Iteration 33/1000 | Loss: 0.00005573
Iteration 34/1000 | Loss: 0.00005572
Iteration 35/1000 | Loss: 0.00005571
Iteration 36/1000 | Loss: 0.00005570
Iteration 37/1000 | Loss: 0.00005569
Iteration 38/1000 | Loss: 0.00005569
Iteration 39/1000 | Loss: 0.00005569
Iteration 40/1000 | Loss: 0.00005568
Iteration 41/1000 | Loss: 0.00005568
Iteration 42/1000 | Loss: 0.00005567
Iteration 43/1000 | Loss: 0.00005567
Iteration 44/1000 | Loss: 0.00005566
Iteration 45/1000 | Loss: 0.00005565
Iteration 46/1000 | Loss: 0.00005565
Iteration 47/1000 | Loss: 0.00005564
Iteration 48/1000 | Loss: 0.00005564
Iteration 49/1000 | Loss: 0.00005564
Iteration 50/1000 | Loss: 0.00005562
Iteration 51/1000 | Loss: 0.00005562
Iteration 52/1000 | Loss: 0.00005560
Iteration 53/1000 | Loss: 0.00005559
Iteration 54/1000 | Loss: 0.00005559
Iteration 55/1000 | Loss: 0.00005559
Iteration 56/1000 | Loss: 0.00005559
Iteration 57/1000 | Loss: 0.00005559
Iteration 58/1000 | Loss: 0.00005558
Iteration 59/1000 | Loss: 0.00005558
Iteration 60/1000 | Loss: 0.00005558
Iteration 61/1000 | Loss: 0.00005558
Iteration 62/1000 | Loss: 0.00005558
Iteration 63/1000 | Loss: 0.00005558
Iteration 64/1000 | Loss: 0.00005558
Iteration 65/1000 | Loss: 0.00005558
Iteration 66/1000 | Loss: 0.00005557
Iteration 67/1000 | Loss: 0.00005556
Iteration 68/1000 | Loss: 0.00005556
Iteration 69/1000 | Loss: 0.00005556
Iteration 70/1000 | Loss: 0.00005556
Iteration 71/1000 | Loss: 0.00005556
Iteration 72/1000 | Loss: 0.00005555
Iteration 73/1000 | Loss: 0.00005555
Iteration 74/1000 | Loss: 0.00005555
Iteration 75/1000 | Loss: 0.00005555
Iteration 76/1000 | Loss: 0.00005555
Iteration 77/1000 | Loss: 0.00005554
Iteration 78/1000 | Loss: 0.00005554
Iteration 79/1000 | Loss: 0.00005554
Iteration 80/1000 | Loss: 0.00005554
Iteration 81/1000 | Loss: 0.00005554
Iteration 82/1000 | Loss: 0.00005553
Iteration 83/1000 | Loss: 0.00005553
Iteration 84/1000 | Loss: 0.00005553
Iteration 85/1000 | Loss: 0.00005553
Iteration 86/1000 | Loss: 0.00005552
Iteration 87/1000 | Loss: 0.00005552
Iteration 88/1000 | Loss: 0.00005552
Iteration 89/1000 | Loss: 0.00005552
Iteration 90/1000 | Loss: 0.00005552
Iteration 91/1000 | Loss: 0.00005552
Iteration 92/1000 | Loss: 0.00005552
Iteration 93/1000 | Loss: 0.00005552
Iteration 94/1000 | Loss: 0.00005552
Iteration 95/1000 | Loss: 0.00005551
Iteration 96/1000 | Loss: 0.00005551
Iteration 97/1000 | Loss: 0.00005551
Iteration 98/1000 | Loss: 0.00005550
Iteration 99/1000 | Loss: 0.00005550
Iteration 100/1000 | Loss: 0.00005550
Iteration 101/1000 | Loss: 0.00005549
Iteration 102/1000 | Loss: 0.00005549
Iteration 103/1000 | Loss: 0.00005549
Iteration 104/1000 | Loss: 0.00005549
Iteration 105/1000 | Loss: 0.00005549
Iteration 106/1000 | Loss: 0.00005549
Iteration 107/1000 | Loss: 0.00005549
Iteration 108/1000 | Loss: 0.00005549
Iteration 109/1000 | Loss: 0.00005549
Iteration 110/1000 | Loss: 0.00005549
Iteration 111/1000 | Loss: 0.00005549
Iteration 112/1000 | Loss: 0.00005549
Iteration 113/1000 | Loss: 0.00005549
Iteration 114/1000 | Loss: 0.00005549
Iteration 115/1000 | Loss: 0.00005549
Iteration 116/1000 | Loss: 0.00005549
Iteration 117/1000 | Loss: 0.00005548
Iteration 118/1000 | Loss: 0.00005548
Iteration 119/1000 | Loss: 0.00005548
Iteration 120/1000 | Loss: 0.00005547
Iteration 121/1000 | Loss: 0.00005547
Iteration 122/1000 | Loss: 0.00005547
Iteration 123/1000 | Loss: 0.00005547
Iteration 124/1000 | Loss: 0.00005547
Iteration 125/1000 | Loss: 0.00005547
Iteration 126/1000 | Loss: 0.00005547
Iteration 127/1000 | Loss: 0.00005546
Iteration 128/1000 | Loss: 0.00005546
Iteration 129/1000 | Loss: 0.00005546
Iteration 130/1000 | Loss: 0.00005546
Iteration 131/1000 | Loss: 0.00005546
Iteration 132/1000 | Loss: 0.00005545
Iteration 133/1000 | Loss: 0.00005545
Iteration 134/1000 | Loss: 0.00005544
Iteration 135/1000 | Loss: 0.00005544
Iteration 136/1000 | Loss: 0.00005544
Iteration 137/1000 | Loss: 0.00005543
Iteration 138/1000 | Loss: 0.00005543
Iteration 139/1000 | Loss: 0.00005543
Iteration 140/1000 | Loss: 0.00005543
Iteration 141/1000 | Loss: 0.00005542
Iteration 142/1000 | Loss: 0.00005542
Iteration 143/1000 | Loss: 0.00005541
Iteration 144/1000 | Loss: 0.00005541
Iteration 145/1000 | Loss: 0.00005540
Iteration 146/1000 | Loss: 0.00005540
Iteration 147/1000 | Loss: 0.00005540
Iteration 148/1000 | Loss: 0.00005540
Iteration 149/1000 | Loss: 0.00005540
Iteration 150/1000 | Loss: 0.00005540
Iteration 151/1000 | Loss: 0.00005540
Iteration 152/1000 | Loss: 0.00005540
Iteration 153/1000 | Loss: 0.00005540
Iteration 154/1000 | Loss: 0.00005540
Iteration 155/1000 | Loss: 0.00005540
Iteration 156/1000 | Loss: 0.00005540
Iteration 157/1000 | Loss: 0.00005540
Iteration 158/1000 | Loss: 0.00005539
Iteration 159/1000 | Loss: 0.00005539
Iteration 160/1000 | Loss: 0.00005539
Iteration 161/1000 | Loss: 0.00005539
Iteration 162/1000 | Loss: 0.00005539
Iteration 163/1000 | Loss: 0.00005538
Iteration 164/1000 | Loss: 0.00005538
Iteration 165/1000 | Loss: 0.00005538
Iteration 166/1000 | Loss: 0.00005538
Iteration 167/1000 | Loss: 0.00005538
Iteration 168/1000 | Loss: 0.00005538
Iteration 169/1000 | Loss: 0.00005538
Iteration 170/1000 | Loss: 0.00005538
Iteration 171/1000 | Loss: 0.00005537
Iteration 172/1000 | Loss: 0.00005537
Iteration 173/1000 | Loss: 0.00005537
Iteration 174/1000 | Loss: 0.00005537
Iteration 175/1000 | Loss: 0.00005537
Iteration 176/1000 | Loss: 0.00005537
Iteration 177/1000 | Loss: 0.00005537
Iteration 178/1000 | Loss: 0.00005537
Iteration 179/1000 | Loss: 0.00005537
Iteration 180/1000 | Loss: 0.00005537
Iteration 181/1000 | Loss: 0.00005537
Iteration 182/1000 | Loss: 0.00005537
Iteration 183/1000 | Loss: 0.00005537
Iteration 184/1000 | Loss: 0.00005537
Iteration 185/1000 | Loss: 0.00005537
Iteration 186/1000 | Loss: 0.00005536
Iteration 187/1000 | Loss: 0.00005536
Iteration 188/1000 | Loss: 0.00005536
Iteration 189/1000 | Loss: 0.00005536
Iteration 190/1000 | Loss: 0.00005536
Iteration 191/1000 | Loss: 0.00005536
Iteration 192/1000 | Loss: 0.00005536
Iteration 193/1000 | Loss: 0.00005536
Iteration 194/1000 | Loss: 0.00005536
Iteration 195/1000 | Loss: 0.00005536
Iteration 196/1000 | Loss: 0.00005536
Iteration 197/1000 | Loss: 0.00005536
Iteration 198/1000 | Loss: 0.00005536
Iteration 199/1000 | Loss: 0.00005536
Iteration 200/1000 | Loss: 0.00005536
Iteration 201/1000 | Loss: 0.00005536
Iteration 202/1000 | Loss: 0.00005536
Iteration 203/1000 | Loss: 0.00005536
Iteration 204/1000 | Loss: 0.00005536
Iteration 205/1000 | Loss: 0.00005536
Iteration 206/1000 | Loss: 0.00005536
Iteration 207/1000 | Loss: 0.00005536
Iteration 208/1000 | Loss: 0.00005536
Iteration 209/1000 | Loss: 0.00005536
Iteration 210/1000 | Loss: 0.00005536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [5.53588179172948e-05, 5.53588179172948e-05, 5.53588179172948e-05, 5.53588179172948e-05, 5.53588179172948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.53588179172948e-05

Optimization complete. Final v2v error: 6.151436805725098 mm

Highest mean error: 7.036683559417725 mm for frame 52

Lowest mean error: 5.610680103302002 mm for frame 162

Saving results

Total time: 50.81569290161133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909733
Iteration 2/25 | Loss: 0.00171074
Iteration 3/25 | Loss: 0.00146002
Iteration 4/25 | Loss: 0.00142632
Iteration 5/25 | Loss: 0.00141444
Iteration 6/25 | Loss: 0.00141064
Iteration 7/25 | Loss: 0.00140917
Iteration 8/25 | Loss: 0.00140917
Iteration 9/25 | Loss: 0.00140917
Iteration 10/25 | Loss: 0.00140917
Iteration 11/25 | Loss: 0.00140917
Iteration 12/25 | Loss: 0.00140917
Iteration 13/25 | Loss: 0.00140917
Iteration 14/25 | Loss: 0.00140917
Iteration 15/25 | Loss: 0.00140917
Iteration 16/25 | Loss: 0.00140917
Iteration 17/25 | Loss: 0.00140917
Iteration 18/25 | Loss: 0.00140917
Iteration 19/25 | Loss: 0.00140917
Iteration 20/25 | Loss: 0.00140917
Iteration 21/25 | Loss: 0.00140917
Iteration 22/25 | Loss: 0.00140917
Iteration 23/25 | Loss: 0.00140917
Iteration 24/25 | Loss: 0.00140917
Iteration 25/25 | Loss: 0.00140917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.17949867
Iteration 2/25 | Loss: 0.00118929
Iteration 3/25 | Loss: 0.00118925
Iteration 4/25 | Loss: 0.00118925
Iteration 5/25 | Loss: 0.00118925
Iteration 6/25 | Loss: 0.00118925
Iteration 7/25 | Loss: 0.00118925
Iteration 8/25 | Loss: 0.00118925
Iteration 9/25 | Loss: 0.00118925
Iteration 10/25 | Loss: 0.00118925
Iteration 11/25 | Loss: 0.00118925
Iteration 12/25 | Loss: 0.00118925
Iteration 13/25 | Loss: 0.00118925
Iteration 14/25 | Loss: 0.00118925
Iteration 15/25 | Loss: 0.00118925
Iteration 16/25 | Loss: 0.00118925
Iteration 17/25 | Loss: 0.00118925
Iteration 18/25 | Loss: 0.00118925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001189246540889144, 0.001189246540889144, 0.001189246540889144, 0.001189246540889144, 0.001189246540889144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001189246540889144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118925
Iteration 2/1000 | Loss: 0.00006183
Iteration 3/1000 | Loss: 0.00004347
Iteration 4/1000 | Loss: 0.00003616
Iteration 5/1000 | Loss: 0.00003414
Iteration 6/1000 | Loss: 0.00003252
Iteration 7/1000 | Loss: 0.00003113
Iteration 8/1000 | Loss: 0.00003031
Iteration 9/1000 | Loss: 0.00002975
Iteration 10/1000 | Loss: 0.00002931
Iteration 11/1000 | Loss: 0.00002898
Iteration 12/1000 | Loss: 0.00002875
Iteration 13/1000 | Loss: 0.00002848
Iteration 14/1000 | Loss: 0.00002830
Iteration 15/1000 | Loss: 0.00002824
Iteration 16/1000 | Loss: 0.00002814
Iteration 17/1000 | Loss: 0.00002808
Iteration 18/1000 | Loss: 0.00002800
Iteration 19/1000 | Loss: 0.00002797
Iteration 20/1000 | Loss: 0.00002797
Iteration 21/1000 | Loss: 0.00002796
Iteration 22/1000 | Loss: 0.00002796
Iteration 23/1000 | Loss: 0.00002795
Iteration 24/1000 | Loss: 0.00002794
Iteration 25/1000 | Loss: 0.00002794
Iteration 26/1000 | Loss: 0.00002793
Iteration 27/1000 | Loss: 0.00002792
Iteration 28/1000 | Loss: 0.00002792
Iteration 29/1000 | Loss: 0.00002792
Iteration 30/1000 | Loss: 0.00002789
Iteration 31/1000 | Loss: 0.00002786
Iteration 32/1000 | Loss: 0.00002785
Iteration 33/1000 | Loss: 0.00002785
Iteration 34/1000 | Loss: 0.00002783
Iteration 35/1000 | Loss: 0.00002782
Iteration 36/1000 | Loss: 0.00002782
Iteration 37/1000 | Loss: 0.00002781
Iteration 38/1000 | Loss: 0.00002781
Iteration 39/1000 | Loss: 0.00002780
Iteration 40/1000 | Loss: 0.00002780
Iteration 41/1000 | Loss: 0.00002779
Iteration 42/1000 | Loss: 0.00002779
Iteration 43/1000 | Loss: 0.00002778
Iteration 44/1000 | Loss: 0.00002777
Iteration 45/1000 | Loss: 0.00002777
Iteration 46/1000 | Loss: 0.00002776
Iteration 47/1000 | Loss: 0.00002776
Iteration 48/1000 | Loss: 0.00002776
Iteration 49/1000 | Loss: 0.00002775
Iteration 50/1000 | Loss: 0.00002775
Iteration 51/1000 | Loss: 0.00002775
Iteration 52/1000 | Loss: 0.00002775
Iteration 53/1000 | Loss: 0.00002775
Iteration 54/1000 | Loss: 0.00002774
Iteration 55/1000 | Loss: 0.00002774
Iteration 56/1000 | Loss: 0.00002774
Iteration 57/1000 | Loss: 0.00002774
Iteration 58/1000 | Loss: 0.00002774
Iteration 59/1000 | Loss: 0.00002773
Iteration 60/1000 | Loss: 0.00002773
Iteration 61/1000 | Loss: 0.00002773
Iteration 62/1000 | Loss: 0.00002773
Iteration 63/1000 | Loss: 0.00002773
Iteration 64/1000 | Loss: 0.00002773
Iteration 65/1000 | Loss: 0.00002773
Iteration 66/1000 | Loss: 0.00002773
Iteration 67/1000 | Loss: 0.00002772
Iteration 68/1000 | Loss: 0.00002772
Iteration 69/1000 | Loss: 0.00002772
Iteration 70/1000 | Loss: 0.00002771
Iteration 71/1000 | Loss: 0.00002771
Iteration 72/1000 | Loss: 0.00002771
Iteration 73/1000 | Loss: 0.00002770
Iteration 74/1000 | Loss: 0.00002770
Iteration 75/1000 | Loss: 0.00002770
Iteration 76/1000 | Loss: 0.00002769
Iteration 77/1000 | Loss: 0.00002769
Iteration 78/1000 | Loss: 0.00002769
Iteration 79/1000 | Loss: 0.00002769
Iteration 80/1000 | Loss: 0.00002769
Iteration 81/1000 | Loss: 0.00002769
Iteration 82/1000 | Loss: 0.00002769
Iteration 83/1000 | Loss: 0.00002768
Iteration 84/1000 | Loss: 0.00002768
Iteration 85/1000 | Loss: 0.00002768
Iteration 86/1000 | Loss: 0.00002768
Iteration 87/1000 | Loss: 0.00002767
Iteration 88/1000 | Loss: 0.00002767
Iteration 89/1000 | Loss: 0.00002767
Iteration 90/1000 | Loss: 0.00002767
Iteration 91/1000 | Loss: 0.00002767
Iteration 92/1000 | Loss: 0.00002767
Iteration 93/1000 | Loss: 0.00002767
Iteration 94/1000 | Loss: 0.00002766
Iteration 95/1000 | Loss: 0.00002766
Iteration 96/1000 | Loss: 0.00002766
Iteration 97/1000 | Loss: 0.00002765
Iteration 98/1000 | Loss: 0.00002765
Iteration 99/1000 | Loss: 0.00002765
Iteration 100/1000 | Loss: 0.00002764
Iteration 101/1000 | Loss: 0.00002764
Iteration 102/1000 | Loss: 0.00002764
Iteration 103/1000 | Loss: 0.00002764
Iteration 104/1000 | Loss: 0.00002764
Iteration 105/1000 | Loss: 0.00002763
Iteration 106/1000 | Loss: 0.00002763
Iteration 107/1000 | Loss: 0.00002763
Iteration 108/1000 | Loss: 0.00002763
Iteration 109/1000 | Loss: 0.00002763
Iteration 110/1000 | Loss: 0.00002762
Iteration 111/1000 | Loss: 0.00002762
Iteration 112/1000 | Loss: 0.00002762
Iteration 113/1000 | Loss: 0.00002761
Iteration 114/1000 | Loss: 0.00002761
Iteration 115/1000 | Loss: 0.00002761
Iteration 116/1000 | Loss: 0.00002761
Iteration 117/1000 | Loss: 0.00002761
Iteration 118/1000 | Loss: 0.00002761
Iteration 119/1000 | Loss: 0.00002760
Iteration 120/1000 | Loss: 0.00002760
Iteration 121/1000 | Loss: 0.00002760
Iteration 122/1000 | Loss: 0.00002760
Iteration 123/1000 | Loss: 0.00002760
Iteration 124/1000 | Loss: 0.00002760
Iteration 125/1000 | Loss: 0.00002760
Iteration 126/1000 | Loss: 0.00002759
Iteration 127/1000 | Loss: 0.00002759
Iteration 128/1000 | Loss: 0.00002759
Iteration 129/1000 | Loss: 0.00002759
Iteration 130/1000 | Loss: 0.00002759
Iteration 131/1000 | Loss: 0.00002759
Iteration 132/1000 | Loss: 0.00002759
Iteration 133/1000 | Loss: 0.00002759
Iteration 134/1000 | Loss: 0.00002759
Iteration 135/1000 | Loss: 0.00002759
Iteration 136/1000 | Loss: 0.00002759
Iteration 137/1000 | Loss: 0.00002759
Iteration 138/1000 | Loss: 0.00002759
Iteration 139/1000 | Loss: 0.00002759
Iteration 140/1000 | Loss: 0.00002759
Iteration 141/1000 | Loss: 0.00002759
Iteration 142/1000 | Loss: 0.00002759
Iteration 143/1000 | Loss: 0.00002759
Iteration 144/1000 | Loss: 0.00002759
Iteration 145/1000 | Loss: 0.00002759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.758986192930024e-05, 2.758986192930024e-05, 2.758986192930024e-05, 2.758986192930024e-05, 2.758986192930024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.758986192930024e-05

Optimization complete. Final v2v error: 4.4586615562438965 mm

Highest mean error: 5.340728759765625 mm for frame 25

Lowest mean error: 3.80364727973938 mm for frame 125

Saving results

Total time: 51.92978620529175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907271
Iteration 2/25 | Loss: 0.00211307
Iteration 3/25 | Loss: 0.00155484
Iteration 4/25 | Loss: 0.00147152
Iteration 5/25 | Loss: 0.00143692
Iteration 6/25 | Loss: 0.00140514
Iteration 7/25 | Loss: 0.00139568
Iteration 8/25 | Loss: 0.00139292
Iteration 9/25 | Loss: 0.00139178
Iteration 10/25 | Loss: 0.00139141
Iteration 11/25 | Loss: 0.00139136
Iteration 12/25 | Loss: 0.00139136
Iteration 13/25 | Loss: 0.00139136
Iteration 14/25 | Loss: 0.00139136
Iteration 15/25 | Loss: 0.00139136
Iteration 16/25 | Loss: 0.00139136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013913630973547697, 0.0013913630973547697, 0.0013913630973547697, 0.0013913630973547697, 0.0013913630973547697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013913630973547697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66359365
Iteration 2/25 | Loss: 0.00103789
Iteration 3/25 | Loss: 0.00103789
Iteration 4/25 | Loss: 0.00103789
Iteration 5/25 | Loss: 0.00103789
Iteration 6/25 | Loss: 0.00103789
Iteration 7/25 | Loss: 0.00103789
Iteration 8/25 | Loss: 0.00103789
Iteration 9/25 | Loss: 0.00103789
Iteration 10/25 | Loss: 0.00103789
Iteration 11/25 | Loss: 0.00103789
Iteration 12/25 | Loss: 0.00103789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010378860170021653, 0.0010378860170021653, 0.0010378860170021653, 0.0010378860170021653, 0.0010378860170021653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010378860170021653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103789
Iteration 2/1000 | Loss: 0.00006250
Iteration 3/1000 | Loss: 0.00004544
Iteration 4/1000 | Loss: 0.00003843
Iteration 5/1000 | Loss: 0.00003581
Iteration 6/1000 | Loss: 0.00003405
Iteration 7/1000 | Loss: 0.00003306
Iteration 8/1000 | Loss: 0.00003261
Iteration 9/1000 | Loss: 0.00003222
Iteration 10/1000 | Loss: 0.00003197
Iteration 11/1000 | Loss: 0.00003178
Iteration 12/1000 | Loss: 0.00003161
Iteration 13/1000 | Loss: 0.00003147
Iteration 14/1000 | Loss: 0.00003145
Iteration 15/1000 | Loss: 0.00003137
Iteration 16/1000 | Loss: 0.00003133
Iteration 17/1000 | Loss: 0.00003132
Iteration 18/1000 | Loss: 0.00003132
Iteration 19/1000 | Loss: 0.00003130
Iteration 20/1000 | Loss: 0.00003129
Iteration 21/1000 | Loss: 0.00003129
Iteration 22/1000 | Loss: 0.00003129
Iteration 23/1000 | Loss: 0.00003128
Iteration 24/1000 | Loss: 0.00003128
Iteration 25/1000 | Loss: 0.00003126
Iteration 26/1000 | Loss: 0.00003126
Iteration 27/1000 | Loss: 0.00003126
Iteration 28/1000 | Loss: 0.00003125
Iteration 29/1000 | Loss: 0.00003125
Iteration 30/1000 | Loss: 0.00003125
Iteration 31/1000 | Loss: 0.00003124
Iteration 32/1000 | Loss: 0.00003124
Iteration 33/1000 | Loss: 0.00003122
Iteration 34/1000 | Loss: 0.00003122
Iteration 35/1000 | Loss: 0.00003122
Iteration 36/1000 | Loss: 0.00003121
Iteration 37/1000 | Loss: 0.00003121
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00003120
Iteration 40/1000 | Loss: 0.00003119
Iteration 41/1000 | Loss: 0.00003119
Iteration 42/1000 | Loss: 0.00003119
Iteration 43/1000 | Loss: 0.00003118
Iteration 44/1000 | Loss: 0.00003118
Iteration 45/1000 | Loss: 0.00003118
Iteration 46/1000 | Loss: 0.00003118
Iteration 47/1000 | Loss: 0.00003117
Iteration 48/1000 | Loss: 0.00003117
Iteration 49/1000 | Loss: 0.00003117
Iteration 50/1000 | Loss: 0.00003115
Iteration 51/1000 | Loss: 0.00003115
Iteration 52/1000 | Loss: 0.00003115
Iteration 53/1000 | Loss: 0.00003115
Iteration 54/1000 | Loss: 0.00003115
Iteration 55/1000 | Loss: 0.00003114
Iteration 56/1000 | Loss: 0.00003114
Iteration 57/1000 | Loss: 0.00003114
Iteration 58/1000 | Loss: 0.00003113
Iteration 59/1000 | Loss: 0.00003113
Iteration 60/1000 | Loss: 0.00003112
Iteration 61/1000 | Loss: 0.00003112
Iteration 62/1000 | Loss: 0.00003112
Iteration 63/1000 | Loss: 0.00003112
Iteration 64/1000 | Loss: 0.00003112
Iteration 65/1000 | Loss: 0.00003112
Iteration 66/1000 | Loss: 0.00003112
Iteration 67/1000 | Loss: 0.00003112
Iteration 68/1000 | Loss: 0.00003112
Iteration 69/1000 | Loss: 0.00003112
Iteration 70/1000 | Loss: 0.00003112
Iteration 71/1000 | Loss: 0.00003112
Iteration 72/1000 | Loss: 0.00003111
Iteration 73/1000 | Loss: 0.00003111
Iteration 74/1000 | Loss: 0.00003111
Iteration 75/1000 | Loss: 0.00003111
Iteration 76/1000 | Loss: 0.00003111
Iteration 77/1000 | Loss: 0.00003111
Iteration 78/1000 | Loss: 0.00003111
Iteration 79/1000 | Loss: 0.00003111
Iteration 80/1000 | Loss: 0.00003111
Iteration 81/1000 | Loss: 0.00003111
Iteration 82/1000 | Loss: 0.00003110
Iteration 83/1000 | Loss: 0.00003110
Iteration 84/1000 | Loss: 0.00003110
Iteration 85/1000 | Loss: 0.00003110
Iteration 86/1000 | Loss: 0.00003110
Iteration 87/1000 | Loss: 0.00003110
Iteration 88/1000 | Loss: 0.00003110
Iteration 89/1000 | Loss: 0.00003110
Iteration 90/1000 | Loss: 0.00003110
Iteration 91/1000 | Loss: 0.00003110
Iteration 92/1000 | Loss: 0.00003110
Iteration 93/1000 | Loss: 0.00003110
Iteration 94/1000 | Loss: 0.00003110
Iteration 95/1000 | Loss: 0.00003110
Iteration 96/1000 | Loss: 0.00003110
Iteration 97/1000 | Loss: 0.00003110
Iteration 98/1000 | Loss: 0.00003110
Iteration 99/1000 | Loss: 0.00003110
Iteration 100/1000 | Loss: 0.00003110
Iteration 101/1000 | Loss: 0.00003110
Iteration 102/1000 | Loss: 0.00003110
Iteration 103/1000 | Loss: 0.00003110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [3.109749741270207e-05, 3.109749741270207e-05, 3.109749741270207e-05, 3.109749741270207e-05, 3.109749741270207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.109749741270207e-05

Optimization complete. Final v2v error: 4.754632472991943 mm

Highest mean error: 6.3495330810546875 mm for frame 79

Lowest mean error: 4.087459087371826 mm for frame 217

Saving results

Total time: 49.023733139038086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467717
Iteration 2/25 | Loss: 0.00167054
Iteration 3/25 | Loss: 0.00141945
Iteration 4/25 | Loss: 0.00138327
Iteration 5/25 | Loss: 0.00137757
Iteration 6/25 | Loss: 0.00137577
Iteration 7/25 | Loss: 0.00137527
Iteration 8/25 | Loss: 0.00137527
Iteration 9/25 | Loss: 0.00137527
Iteration 10/25 | Loss: 0.00137527
Iteration 11/25 | Loss: 0.00137527
Iteration 12/25 | Loss: 0.00137527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001375269377604127, 0.001375269377604127, 0.001375269377604127, 0.001375269377604127, 0.001375269377604127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001375269377604127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44686484
Iteration 2/25 | Loss: 0.00107160
Iteration 3/25 | Loss: 0.00107159
Iteration 4/25 | Loss: 0.00107159
Iteration 5/25 | Loss: 0.00107159
Iteration 6/25 | Loss: 0.00107159
Iteration 7/25 | Loss: 0.00107159
Iteration 8/25 | Loss: 0.00107159
Iteration 9/25 | Loss: 0.00107159
Iteration 10/25 | Loss: 0.00107159
Iteration 11/25 | Loss: 0.00107159
Iteration 12/25 | Loss: 0.00107159
Iteration 13/25 | Loss: 0.00107159
Iteration 14/25 | Loss: 0.00107159
Iteration 15/25 | Loss: 0.00107159
Iteration 16/25 | Loss: 0.00107159
Iteration 17/25 | Loss: 0.00107159
Iteration 18/25 | Loss: 0.00107159
Iteration 19/25 | Loss: 0.00107159
Iteration 20/25 | Loss: 0.00107159
Iteration 21/25 | Loss: 0.00107159
Iteration 22/25 | Loss: 0.00107159
Iteration 23/25 | Loss: 0.00107159
Iteration 24/25 | Loss: 0.00107159
Iteration 25/25 | Loss: 0.00107159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107159
Iteration 2/1000 | Loss: 0.00006436
Iteration 3/1000 | Loss: 0.00005121
Iteration 4/1000 | Loss: 0.00004699
Iteration 5/1000 | Loss: 0.00004525
Iteration 6/1000 | Loss: 0.00004393
Iteration 7/1000 | Loss: 0.00004303
Iteration 8/1000 | Loss: 0.00004241
Iteration 9/1000 | Loss: 0.00004207
Iteration 10/1000 | Loss: 0.00004185
Iteration 11/1000 | Loss: 0.00004170
Iteration 12/1000 | Loss: 0.00004167
Iteration 13/1000 | Loss: 0.00004166
Iteration 14/1000 | Loss: 0.00004154
Iteration 15/1000 | Loss: 0.00004151
Iteration 16/1000 | Loss: 0.00004148
Iteration 17/1000 | Loss: 0.00004146
Iteration 18/1000 | Loss: 0.00004145
Iteration 19/1000 | Loss: 0.00004142
Iteration 20/1000 | Loss: 0.00004139
Iteration 21/1000 | Loss: 0.00004139
Iteration 22/1000 | Loss: 0.00004136
Iteration 23/1000 | Loss: 0.00004134
Iteration 24/1000 | Loss: 0.00004133
Iteration 25/1000 | Loss: 0.00004133
Iteration 26/1000 | Loss: 0.00004133
Iteration 27/1000 | Loss: 0.00004133
Iteration 28/1000 | Loss: 0.00004132
Iteration 29/1000 | Loss: 0.00004130
Iteration 30/1000 | Loss: 0.00004127
Iteration 31/1000 | Loss: 0.00004124
Iteration 32/1000 | Loss: 0.00004123
Iteration 33/1000 | Loss: 0.00004119
Iteration 34/1000 | Loss: 0.00004119
Iteration 35/1000 | Loss: 0.00004119
Iteration 36/1000 | Loss: 0.00004118
Iteration 37/1000 | Loss: 0.00004118
Iteration 38/1000 | Loss: 0.00004118
Iteration 39/1000 | Loss: 0.00004117
Iteration 40/1000 | Loss: 0.00004117
Iteration 41/1000 | Loss: 0.00004117
Iteration 42/1000 | Loss: 0.00004117
Iteration 43/1000 | Loss: 0.00004116
Iteration 44/1000 | Loss: 0.00004116
Iteration 45/1000 | Loss: 0.00004116
Iteration 46/1000 | Loss: 0.00004115
Iteration 47/1000 | Loss: 0.00004115
Iteration 48/1000 | Loss: 0.00004114
Iteration 49/1000 | Loss: 0.00004114
Iteration 50/1000 | Loss: 0.00004114
Iteration 51/1000 | Loss: 0.00004114
Iteration 52/1000 | Loss: 0.00004114
Iteration 53/1000 | Loss: 0.00004114
Iteration 54/1000 | Loss: 0.00004113
Iteration 55/1000 | Loss: 0.00004113
Iteration 56/1000 | Loss: 0.00004113
Iteration 57/1000 | Loss: 0.00004112
Iteration 58/1000 | Loss: 0.00004112
Iteration 59/1000 | Loss: 0.00004112
Iteration 60/1000 | Loss: 0.00004112
Iteration 61/1000 | Loss: 0.00004111
Iteration 62/1000 | Loss: 0.00004111
Iteration 63/1000 | Loss: 0.00004111
Iteration 64/1000 | Loss: 0.00004110
Iteration 65/1000 | Loss: 0.00004110
Iteration 66/1000 | Loss: 0.00004110
Iteration 67/1000 | Loss: 0.00004110
Iteration 68/1000 | Loss: 0.00004110
Iteration 69/1000 | Loss: 0.00004109
Iteration 70/1000 | Loss: 0.00004109
Iteration 71/1000 | Loss: 0.00004109
Iteration 72/1000 | Loss: 0.00004109
Iteration 73/1000 | Loss: 0.00004109
Iteration 74/1000 | Loss: 0.00004109
Iteration 75/1000 | Loss: 0.00004109
Iteration 76/1000 | Loss: 0.00004109
Iteration 77/1000 | Loss: 0.00004109
Iteration 78/1000 | Loss: 0.00004109
Iteration 79/1000 | Loss: 0.00004109
Iteration 80/1000 | Loss: 0.00004108
Iteration 81/1000 | Loss: 0.00004108
Iteration 82/1000 | Loss: 0.00004108
Iteration 83/1000 | Loss: 0.00004108
Iteration 84/1000 | Loss: 0.00004107
Iteration 85/1000 | Loss: 0.00004107
Iteration 86/1000 | Loss: 0.00004107
Iteration 87/1000 | Loss: 0.00004107
Iteration 88/1000 | Loss: 0.00004107
Iteration 89/1000 | Loss: 0.00004107
Iteration 90/1000 | Loss: 0.00004107
Iteration 91/1000 | Loss: 0.00004107
Iteration 92/1000 | Loss: 0.00004107
Iteration 93/1000 | Loss: 0.00004107
Iteration 94/1000 | Loss: 0.00004106
Iteration 95/1000 | Loss: 0.00004106
Iteration 96/1000 | Loss: 0.00004106
Iteration 97/1000 | Loss: 0.00004106
Iteration 98/1000 | Loss: 0.00004106
Iteration 99/1000 | Loss: 0.00004106
Iteration 100/1000 | Loss: 0.00004106
Iteration 101/1000 | Loss: 0.00004106
Iteration 102/1000 | Loss: 0.00004106
Iteration 103/1000 | Loss: 0.00004105
Iteration 104/1000 | Loss: 0.00004105
Iteration 105/1000 | Loss: 0.00004105
Iteration 106/1000 | Loss: 0.00004105
Iteration 107/1000 | Loss: 0.00004105
Iteration 108/1000 | Loss: 0.00004105
Iteration 109/1000 | Loss: 0.00004105
Iteration 110/1000 | Loss: 0.00004105
Iteration 111/1000 | Loss: 0.00004105
Iteration 112/1000 | Loss: 0.00004105
Iteration 113/1000 | Loss: 0.00004105
Iteration 114/1000 | Loss: 0.00004105
Iteration 115/1000 | Loss: 0.00004105
Iteration 116/1000 | Loss: 0.00004105
Iteration 117/1000 | Loss: 0.00004105
Iteration 118/1000 | Loss: 0.00004105
Iteration 119/1000 | Loss: 0.00004105
Iteration 120/1000 | Loss: 0.00004105
Iteration 121/1000 | Loss: 0.00004105
Iteration 122/1000 | Loss: 0.00004105
Iteration 123/1000 | Loss: 0.00004105
Iteration 124/1000 | Loss: 0.00004105
Iteration 125/1000 | Loss: 0.00004105
Iteration 126/1000 | Loss: 0.00004105
Iteration 127/1000 | Loss: 0.00004105
Iteration 128/1000 | Loss: 0.00004105
Iteration 129/1000 | Loss: 0.00004105
Iteration 130/1000 | Loss: 0.00004105
Iteration 131/1000 | Loss: 0.00004105
Iteration 132/1000 | Loss: 0.00004105
Iteration 133/1000 | Loss: 0.00004105
Iteration 134/1000 | Loss: 0.00004105
Iteration 135/1000 | Loss: 0.00004105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [4.105253174202517e-05, 4.105253174202517e-05, 4.105253174202517e-05, 4.105253174202517e-05, 4.105253174202517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.105253174202517e-05

Optimization complete. Final v2v error: 5.49531888961792 mm

Highest mean error: 5.842319965362549 mm for frame 150

Lowest mean error: 5.098926544189453 mm for frame 73

Saving results

Total time: 39.72582650184631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454289
Iteration 2/25 | Loss: 0.00151732
Iteration 3/25 | Loss: 0.00136605
Iteration 4/25 | Loss: 0.00134796
Iteration 5/25 | Loss: 0.00134214
Iteration 6/25 | Loss: 0.00134039
Iteration 7/25 | Loss: 0.00134021
Iteration 8/25 | Loss: 0.00134021
Iteration 9/25 | Loss: 0.00134021
Iteration 10/25 | Loss: 0.00134021
Iteration 11/25 | Loss: 0.00134021
Iteration 12/25 | Loss: 0.00134021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013402086915448308, 0.0013402086915448308, 0.0013402086915448308, 0.0013402086915448308, 0.0013402086915448308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013402086915448308

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43078041
Iteration 2/25 | Loss: 0.00103985
Iteration 3/25 | Loss: 0.00103985
Iteration 4/25 | Loss: 0.00103985
Iteration 5/25 | Loss: 0.00103985
Iteration 6/25 | Loss: 0.00103985
Iteration 7/25 | Loss: 0.00103985
Iteration 8/25 | Loss: 0.00103985
Iteration 9/25 | Loss: 0.00103985
Iteration 10/25 | Loss: 0.00103985
Iteration 11/25 | Loss: 0.00103985
Iteration 12/25 | Loss: 0.00103984
Iteration 13/25 | Loss: 0.00103984
Iteration 14/25 | Loss: 0.00103984
Iteration 15/25 | Loss: 0.00103984
Iteration 16/25 | Loss: 0.00103984
Iteration 17/25 | Loss: 0.00103984
Iteration 18/25 | Loss: 0.00103984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010398447047919035, 0.0010398447047919035, 0.0010398447047919035, 0.0010398447047919035, 0.0010398447047919035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010398447047919035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103984
Iteration 2/1000 | Loss: 0.00004551
Iteration 3/1000 | Loss: 0.00003306
Iteration 4/1000 | Loss: 0.00003015
Iteration 5/1000 | Loss: 0.00002890
Iteration 6/1000 | Loss: 0.00002817
Iteration 7/1000 | Loss: 0.00002755
Iteration 8/1000 | Loss: 0.00002719
Iteration 9/1000 | Loss: 0.00002699
Iteration 10/1000 | Loss: 0.00002682
Iteration 11/1000 | Loss: 0.00002680
Iteration 12/1000 | Loss: 0.00002676
Iteration 13/1000 | Loss: 0.00002676
Iteration 14/1000 | Loss: 0.00002675
Iteration 15/1000 | Loss: 0.00002675
Iteration 16/1000 | Loss: 0.00002675
Iteration 17/1000 | Loss: 0.00002675
Iteration 18/1000 | Loss: 0.00002675
Iteration 19/1000 | Loss: 0.00002675
Iteration 20/1000 | Loss: 0.00002674
Iteration 21/1000 | Loss: 0.00002670
Iteration 22/1000 | Loss: 0.00002667
Iteration 23/1000 | Loss: 0.00002665
Iteration 24/1000 | Loss: 0.00002658
Iteration 25/1000 | Loss: 0.00002658
Iteration 26/1000 | Loss: 0.00002657
Iteration 27/1000 | Loss: 0.00002656
Iteration 28/1000 | Loss: 0.00002656
Iteration 29/1000 | Loss: 0.00002655
Iteration 30/1000 | Loss: 0.00002655
Iteration 31/1000 | Loss: 0.00002654
Iteration 32/1000 | Loss: 0.00002654
Iteration 33/1000 | Loss: 0.00002654
Iteration 34/1000 | Loss: 0.00002654
Iteration 35/1000 | Loss: 0.00002654
Iteration 36/1000 | Loss: 0.00002654
Iteration 37/1000 | Loss: 0.00002654
Iteration 38/1000 | Loss: 0.00002653
Iteration 39/1000 | Loss: 0.00002652
Iteration 40/1000 | Loss: 0.00002652
Iteration 41/1000 | Loss: 0.00002652
Iteration 42/1000 | Loss: 0.00002651
Iteration 43/1000 | Loss: 0.00002651
Iteration 44/1000 | Loss: 0.00002651
Iteration 45/1000 | Loss: 0.00002651
Iteration 46/1000 | Loss: 0.00002651
Iteration 47/1000 | Loss: 0.00002650
Iteration 48/1000 | Loss: 0.00002650
Iteration 49/1000 | Loss: 0.00002650
Iteration 50/1000 | Loss: 0.00002648
Iteration 51/1000 | Loss: 0.00002648
Iteration 52/1000 | Loss: 0.00002648
Iteration 53/1000 | Loss: 0.00002648
Iteration 54/1000 | Loss: 0.00002647
Iteration 55/1000 | Loss: 0.00002647
Iteration 56/1000 | Loss: 0.00002647
Iteration 57/1000 | Loss: 0.00002647
Iteration 58/1000 | Loss: 0.00002647
Iteration 59/1000 | Loss: 0.00002647
Iteration 60/1000 | Loss: 0.00002647
Iteration 61/1000 | Loss: 0.00002646
Iteration 62/1000 | Loss: 0.00002646
Iteration 63/1000 | Loss: 0.00002646
Iteration 64/1000 | Loss: 0.00002646
Iteration 65/1000 | Loss: 0.00002645
Iteration 66/1000 | Loss: 0.00002645
Iteration 67/1000 | Loss: 0.00002645
Iteration 68/1000 | Loss: 0.00002645
Iteration 69/1000 | Loss: 0.00002645
Iteration 70/1000 | Loss: 0.00002645
Iteration 71/1000 | Loss: 0.00002645
Iteration 72/1000 | Loss: 0.00002645
Iteration 73/1000 | Loss: 0.00002645
Iteration 74/1000 | Loss: 0.00002645
Iteration 75/1000 | Loss: 0.00002645
Iteration 76/1000 | Loss: 0.00002644
Iteration 77/1000 | Loss: 0.00002644
Iteration 78/1000 | Loss: 0.00002644
Iteration 79/1000 | Loss: 0.00002644
Iteration 80/1000 | Loss: 0.00002644
Iteration 81/1000 | Loss: 0.00002644
Iteration 82/1000 | Loss: 0.00002644
Iteration 83/1000 | Loss: 0.00002644
Iteration 84/1000 | Loss: 0.00002644
Iteration 85/1000 | Loss: 0.00002644
Iteration 86/1000 | Loss: 0.00002644
Iteration 87/1000 | Loss: 0.00002644
Iteration 88/1000 | Loss: 0.00002644
Iteration 89/1000 | Loss: 0.00002644
Iteration 90/1000 | Loss: 0.00002643
Iteration 91/1000 | Loss: 0.00002643
Iteration 92/1000 | Loss: 0.00002643
Iteration 93/1000 | Loss: 0.00002643
Iteration 94/1000 | Loss: 0.00002643
Iteration 95/1000 | Loss: 0.00002643
Iteration 96/1000 | Loss: 0.00002643
Iteration 97/1000 | Loss: 0.00002643
Iteration 98/1000 | Loss: 0.00002643
Iteration 99/1000 | Loss: 0.00002643
Iteration 100/1000 | Loss: 0.00002642
Iteration 101/1000 | Loss: 0.00002642
Iteration 102/1000 | Loss: 0.00002642
Iteration 103/1000 | Loss: 0.00002642
Iteration 104/1000 | Loss: 0.00002642
Iteration 105/1000 | Loss: 0.00002642
Iteration 106/1000 | Loss: 0.00002642
Iteration 107/1000 | Loss: 0.00002642
Iteration 108/1000 | Loss: 0.00002642
Iteration 109/1000 | Loss: 0.00002642
Iteration 110/1000 | Loss: 0.00002642
Iteration 111/1000 | Loss: 0.00002642
Iteration 112/1000 | Loss: 0.00002642
Iteration 113/1000 | Loss: 0.00002642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.6415633328724653e-05, 2.6415633328724653e-05, 2.6415633328724653e-05, 2.6415633328724653e-05, 2.6415633328724653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6415633328724653e-05

Optimization complete. Final v2v error: 4.461295127868652 mm

Highest mean error: 4.635362148284912 mm for frame 61

Lowest mean error: 4.266066074371338 mm for frame 0

Saving results

Total time: 34.24715709686279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01178425
Iteration 2/25 | Loss: 0.00458621
Iteration 3/25 | Loss: 0.00298431
Iteration 4/25 | Loss: 0.00265720
Iteration 5/25 | Loss: 0.00228465
Iteration 6/25 | Loss: 0.00223442
Iteration 7/25 | Loss: 0.00198340
Iteration 8/25 | Loss: 0.00187592
Iteration 9/25 | Loss: 0.00185202
Iteration 10/25 | Loss: 0.00179102
Iteration 11/25 | Loss: 0.00179546
Iteration 12/25 | Loss: 0.00178344
Iteration 13/25 | Loss: 0.00175030
Iteration 14/25 | Loss: 0.00175150
Iteration 15/25 | Loss: 0.00173501
Iteration 16/25 | Loss: 0.00173997
Iteration 17/25 | Loss: 0.00173300
Iteration 18/25 | Loss: 0.00172457
Iteration 19/25 | Loss: 0.00171699
Iteration 20/25 | Loss: 0.00171782
Iteration 21/25 | Loss: 0.00171743
Iteration 22/25 | Loss: 0.00171891
Iteration 23/25 | Loss: 0.00171965
Iteration 24/25 | Loss: 0.00171110
Iteration 25/25 | Loss: 0.00170892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28251290
Iteration 2/25 | Loss: 0.00478370
Iteration 3/25 | Loss: 0.00449871
Iteration 4/25 | Loss: 0.00449871
Iteration 5/25 | Loss: 0.00449871
Iteration 6/25 | Loss: 0.00449871
Iteration 7/25 | Loss: 0.00449871
Iteration 8/25 | Loss: 0.00449871
Iteration 9/25 | Loss: 0.00449871
Iteration 10/25 | Loss: 0.00449871
Iteration 11/25 | Loss: 0.00449871
Iteration 12/25 | Loss: 0.00449871
Iteration 13/25 | Loss: 0.00449871
Iteration 14/25 | Loss: 0.00449871
Iteration 15/25 | Loss: 0.00449871
Iteration 16/25 | Loss: 0.00449871
Iteration 17/25 | Loss: 0.00449871
Iteration 18/25 | Loss: 0.00449871
Iteration 19/25 | Loss: 0.00449871
Iteration 20/25 | Loss: 0.00449871
Iteration 21/25 | Loss: 0.00449871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004498709458857775, 0.004498709458857775, 0.004498709458857775, 0.004498709458857775, 0.004498709458857775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004498709458857775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00449871
Iteration 2/1000 | Loss: 0.00156593
Iteration 3/1000 | Loss: 0.00162705
Iteration 4/1000 | Loss: 0.00140831
Iteration 5/1000 | Loss: 0.00083239
Iteration 6/1000 | Loss: 0.00079942
Iteration 7/1000 | Loss: 0.00413653
Iteration 8/1000 | Loss: 0.00088175
Iteration 9/1000 | Loss: 0.00120980
Iteration 10/1000 | Loss: 0.00050947
Iteration 11/1000 | Loss: 0.00514417
Iteration 12/1000 | Loss: 0.00265522
Iteration 13/1000 | Loss: 0.00516044
Iteration 14/1000 | Loss: 0.00119373
Iteration 15/1000 | Loss: 0.00121274
Iteration 16/1000 | Loss: 0.00721328
Iteration 17/1000 | Loss: 0.00343868
Iteration 18/1000 | Loss: 0.00040142
Iteration 19/1000 | Loss: 0.00346635
Iteration 20/1000 | Loss: 0.02027192
Iteration 21/1000 | Loss: 0.00173858
Iteration 22/1000 | Loss: 0.00068560
Iteration 23/1000 | Loss: 0.01158491
Iteration 24/1000 | Loss: 0.00138802
Iteration 25/1000 | Loss: 0.00109418
Iteration 26/1000 | Loss: 0.00369638
Iteration 27/1000 | Loss: 0.01354199
Iteration 28/1000 | Loss: 0.01772673
Iteration 29/1000 | Loss: 0.01488893
Iteration 30/1000 | Loss: 0.00373061
Iteration 31/1000 | Loss: 0.00340786
Iteration 32/1000 | Loss: 0.00125302
Iteration 33/1000 | Loss: 0.00272663
Iteration 34/1000 | Loss: 0.00191686
Iteration 35/1000 | Loss: 0.00084544
Iteration 36/1000 | Loss: 0.00528526
Iteration 37/1000 | Loss: 0.00139457
Iteration 38/1000 | Loss: 0.00421454
Iteration 39/1000 | Loss: 0.00212855
Iteration 40/1000 | Loss: 0.00128569
Iteration 41/1000 | Loss: 0.00114058
Iteration 42/1000 | Loss: 0.00197840
Iteration 43/1000 | Loss: 0.00290343
Iteration 44/1000 | Loss: 0.00514685
Iteration 45/1000 | Loss: 0.00464972
Iteration 46/1000 | Loss: 0.00279788
Iteration 47/1000 | Loss: 0.00281305
Iteration 48/1000 | Loss: 0.00177441
Iteration 49/1000 | Loss: 0.00161912
Iteration 50/1000 | Loss: 0.00192863
Iteration 51/1000 | Loss: 0.00295346
Iteration 52/1000 | Loss: 0.00277859
Iteration 53/1000 | Loss: 0.00197136
Iteration 54/1000 | Loss: 0.00143576
Iteration 55/1000 | Loss: 0.00112833
Iteration 56/1000 | Loss: 0.00077706
Iteration 57/1000 | Loss: 0.00576844
Iteration 58/1000 | Loss: 0.00400672
Iteration 59/1000 | Loss: 0.00243205
Iteration 60/1000 | Loss: 0.00848017
Iteration 61/1000 | Loss: 0.00285126
Iteration 62/1000 | Loss: 0.00134239
Iteration 63/1000 | Loss: 0.00160319
Iteration 64/1000 | Loss: 0.00227195
Iteration 65/1000 | Loss: 0.00185029
Iteration 66/1000 | Loss: 0.00297056
Iteration 67/1000 | Loss: 0.00350983
Iteration 68/1000 | Loss: 0.00508677
Iteration 69/1000 | Loss: 0.00255718
Iteration 70/1000 | Loss: 0.00292040
Iteration 71/1000 | Loss: 0.00308036
Iteration 72/1000 | Loss: 0.00348624
Iteration 73/1000 | Loss: 0.00345985
Iteration 74/1000 | Loss: 0.00280488
Iteration 75/1000 | Loss: 0.00169867
Iteration 76/1000 | Loss: 0.00440562
Iteration 77/1000 | Loss: 0.00287854
Iteration 78/1000 | Loss: 0.00197149
Iteration 79/1000 | Loss: 0.00133989
Iteration 80/1000 | Loss: 0.00147970
Iteration 81/1000 | Loss: 0.00237718
Iteration 82/1000 | Loss: 0.00242907
Iteration 83/1000 | Loss: 0.00100818
Iteration 84/1000 | Loss: 0.00148866
Iteration 85/1000 | Loss: 0.00083214
Iteration 86/1000 | Loss: 0.00083945
Iteration 87/1000 | Loss: 0.00214242
Iteration 88/1000 | Loss: 0.00358956
Iteration 89/1000 | Loss: 0.00408684
Iteration 90/1000 | Loss: 0.00278549
Iteration 91/1000 | Loss: 0.00380045
Iteration 92/1000 | Loss: 0.00425350
Iteration 93/1000 | Loss: 0.00510815
Iteration 94/1000 | Loss: 0.00367291
Iteration 95/1000 | Loss: 0.00092592
Iteration 96/1000 | Loss: 0.00065780
Iteration 97/1000 | Loss: 0.00168185
Iteration 98/1000 | Loss: 0.00036354
Iteration 99/1000 | Loss: 0.00033824
Iteration 100/1000 | Loss: 0.00238059
Iteration 101/1000 | Loss: 0.00035044
Iteration 102/1000 | Loss: 0.00023892
Iteration 103/1000 | Loss: 0.00104803
Iteration 104/1000 | Loss: 0.00034884
Iteration 105/1000 | Loss: 0.00023315
Iteration 106/1000 | Loss: 0.00027201
Iteration 107/1000 | Loss: 0.00037788
Iteration 108/1000 | Loss: 0.00074413
Iteration 109/1000 | Loss: 0.00022763
Iteration 110/1000 | Loss: 0.00025306
Iteration 111/1000 | Loss: 0.00021113
Iteration 112/1000 | Loss: 0.00019208
Iteration 113/1000 | Loss: 0.00023493
Iteration 114/1000 | Loss: 0.00018792
Iteration 115/1000 | Loss: 0.00019091
Iteration 116/1000 | Loss: 0.00043772
Iteration 117/1000 | Loss: 0.00041434
Iteration 118/1000 | Loss: 0.00027622
Iteration 119/1000 | Loss: 0.00059383
Iteration 120/1000 | Loss: 0.00017883
Iteration 121/1000 | Loss: 0.00018205
Iteration 122/1000 | Loss: 0.00020322
Iteration 123/1000 | Loss: 0.00074874
Iteration 124/1000 | Loss: 0.00024729
Iteration 125/1000 | Loss: 0.00018258
Iteration 126/1000 | Loss: 0.00018294
Iteration 127/1000 | Loss: 0.00024323
Iteration 128/1000 | Loss: 0.00023553
Iteration 129/1000 | Loss: 0.00017985
Iteration 130/1000 | Loss: 0.00049353
Iteration 131/1000 | Loss: 0.00018840
Iteration 132/1000 | Loss: 0.00017449
Iteration 133/1000 | Loss: 0.00022026
Iteration 134/1000 | Loss: 0.00083926
Iteration 135/1000 | Loss: 0.00025040
Iteration 136/1000 | Loss: 0.00158461
Iteration 137/1000 | Loss: 0.00079574
Iteration 138/1000 | Loss: 0.00019131
Iteration 139/1000 | Loss: 0.00016738
Iteration 140/1000 | Loss: 0.00017197
Iteration 141/1000 | Loss: 0.00057151
Iteration 142/1000 | Loss: 0.00021635
Iteration 143/1000 | Loss: 0.00021084
Iteration 144/1000 | Loss: 0.00056972
Iteration 145/1000 | Loss: 0.00016817
Iteration 146/1000 | Loss: 0.00055695
Iteration 147/1000 | Loss: 0.00017528
Iteration 148/1000 | Loss: 0.00016530
Iteration 149/1000 | Loss: 0.00015711
Iteration 150/1000 | Loss: 0.00223478
Iteration 151/1000 | Loss: 0.00088790
Iteration 152/1000 | Loss: 0.00031240
Iteration 153/1000 | Loss: 0.00090078
Iteration 154/1000 | Loss: 0.00053722
Iteration 155/1000 | Loss: 0.00083202
Iteration 156/1000 | Loss: 0.00069325
Iteration 157/1000 | Loss: 0.00032264
Iteration 158/1000 | Loss: 0.00015708
Iteration 159/1000 | Loss: 0.00032775
Iteration 160/1000 | Loss: 0.00018026
Iteration 161/1000 | Loss: 0.00070909
Iteration 162/1000 | Loss: 0.00031371
Iteration 163/1000 | Loss: 0.00030455
Iteration 164/1000 | Loss: 0.00060531
Iteration 165/1000 | Loss: 0.00025822
Iteration 166/1000 | Loss: 0.00030586
Iteration 167/1000 | Loss: 0.00055116
Iteration 168/1000 | Loss: 0.00031300
Iteration 169/1000 | Loss: 0.00034888
Iteration 170/1000 | Loss: 0.00024697
Iteration 171/1000 | Loss: 0.00042657
Iteration 172/1000 | Loss: 0.00057810
Iteration 173/1000 | Loss: 0.00073920
Iteration 174/1000 | Loss: 0.00056082
Iteration 175/1000 | Loss: 0.00019519
Iteration 176/1000 | Loss: 0.00070836
Iteration 177/1000 | Loss: 0.00080733
Iteration 178/1000 | Loss: 0.00020569
Iteration 179/1000 | Loss: 0.00017625
Iteration 180/1000 | Loss: 0.00077097
Iteration 181/1000 | Loss: 0.00141466
Iteration 182/1000 | Loss: 0.00077542
Iteration 183/1000 | Loss: 0.00038091
Iteration 184/1000 | Loss: 0.00094106
Iteration 185/1000 | Loss: 0.00083775
Iteration 186/1000 | Loss: 0.00020718
Iteration 187/1000 | Loss: 0.00015152
Iteration 188/1000 | Loss: 0.00024006
Iteration 189/1000 | Loss: 0.00063075
Iteration 190/1000 | Loss: 0.00017369
Iteration 191/1000 | Loss: 0.00015492
Iteration 192/1000 | Loss: 0.00057888
Iteration 193/1000 | Loss: 0.00111117
Iteration 194/1000 | Loss: 0.00071776
Iteration 195/1000 | Loss: 0.00068721
Iteration 196/1000 | Loss: 0.00018691
Iteration 197/1000 | Loss: 0.00021340
Iteration 198/1000 | Loss: 0.00017208
Iteration 199/1000 | Loss: 0.00014834
Iteration 200/1000 | Loss: 0.00015438
Iteration 201/1000 | Loss: 0.00022667
Iteration 202/1000 | Loss: 0.00015495
Iteration 203/1000 | Loss: 0.00014440
Iteration 204/1000 | Loss: 0.00018299
Iteration 205/1000 | Loss: 0.00014030
Iteration 206/1000 | Loss: 0.00017714
Iteration 207/1000 | Loss: 0.00062105
Iteration 208/1000 | Loss: 0.00029674
Iteration 209/1000 | Loss: 0.00055729
Iteration 210/1000 | Loss: 0.00014041
Iteration 211/1000 | Loss: 0.00019299
Iteration 212/1000 | Loss: 0.00015729
Iteration 213/1000 | Loss: 0.00015145
Iteration 214/1000 | Loss: 0.00014444
Iteration 215/1000 | Loss: 0.00013893
Iteration 216/1000 | Loss: 0.00014856
Iteration 217/1000 | Loss: 0.00015801
Iteration 218/1000 | Loss: 0.00015060
Iteration 219/1000 | Loss: 0.00015422
Iteration 220/1000 | Loss: 0.00014640
Iteration 221/1000 | Loss: 0.00015130
Iteration 222/1000 | Loss: 0.00014734
Iteration 223/1000 | Loss: 0.00018365
Iteration 224/1000 | Loss: 0.00147145
Iteration 225/1000 | Loss: 0.00050749
Iteration 226/1000 | Loss: 0.00015199
Iteration 227/1000 | Loss: 0.00019367
Iteration 228/1000 | Loss: 0.00014799
Iteration 229/1000 | Loss: 0.00016194
Iteration 230/1000 | Loss: 0.00012963
Iteration 231/1000 | Loss: 0.00012584
Iteration 232/1000 | Loss: 0.00024422
Iteration 233/1000 | Loss: 0.00012360
Iteration 234/1000 | Loss: 0.00012310
Iteration 235/1000 | Loss: 0.00013436
Iteration 236/1000 | Loss: 0.00014151
Iteration 237/1000 | Loss: 0.00012374
Iteration 238/1000 | Loss: 0.00012253
Iteration 239/1000 | Loss: 0.00012249
Iteration 240/1000 | Loss: 0.00012248
Iteration 241/1000 | Loss: 0.00012248
Iteration 242/1000 | Loss: 0.00012247
Iteration 243/1000 | Loss: 0.00012247
Iteration 244/1000 | Loss: 0.00012247
Iteration 245/1000 | Loss: 0.00012246
Iteration 246/1000 | Loss: 0.00012246
Iteration 247/1000 | Loss: 0.00012245
Iteration 248/1000 | Loss: 0.00012245
Iteration 249/1000 | Loss: 0.00012245
Iteration 250/1000 | Loss: 0.00012245
Iteration 251/1000 | Loss: 0.00012245
Iteration 252/1000 | Loss: 0.00012245
Iteration 253/1000 | Loss: 0.00012245
Iteration 254/1000 | Loss: 0.00012245
Iteration 255/1000 | Loss: 0.00012245
Iteration 256/1000 | Loss: 0.00012245
Iteration 257/1000 | Loss: 0.00012245
Iteration 258/1000 | Loss: 0.00012244
Iteration 259/1000 | Loss: 0.00012244
Iteration 260/1000 | Loss: 0.00012243
Iteration 261/1000 | Loss: 0.00012243
Iteration 262/1000 | Loss: 0.00012243
Iteration 263/1000 | Loss: 0.00012243
Iteration 264/1000 | Loss: 0.00012243
Iteration 265/1000 | Loss: 0.00012242
Iteration 266/1000 | Loss: 0.00012242
Iteration 267/1000 | Loss: 0.00012242
Iteration 268/1000 | Loss: 0.00012242
Iteration 269/1000 | Loss: 0.00012241
Iteration 270/1000 | Loss: 0.00012241
Iteration 271/1000 | Loss: 0.00012241
Iteration 272/1000 | Loss: 0.00012241
Iteration 273/1000 | Loss: 0.00012240
Iteration 274/1000 | Loss: 0.00012240
Iteration 275/1000 | Loss: 0.00012240
Iteration 276/1000 | Loss: 0.00012240
Iteration 277/1000 | Loss: 0.00013669
Iteration 278/1000 | Loss: 0.00014193
Iteration 279/1000 | Loss: 0.00012361
Iteration 280/1000 | Loss: 0.00012565
Iteration 281/1000 | Loss: 0.00012238
Iteration 282/1000 | Loss: 0.00012569
Iteration 283/1000 | Loss: 0.00012234
Iteration 284/1000 | Loss: 0.00012234
Iteration 285/1000 | Loss: 0.00012234
Iteration 286/1000 | Loss: 0.00012234
Iteration 287/1000 | Loss: 0.00012234
Iteration 288/1000 | Loss: 0.00012234
Iteration 289/1000 | Loss: 0.00012233
Iteration 290/1000 | Loss: 0.00012233
Iteration 291/1000 | Loss: 0.00012233
Iteration 292/1000 | Loss: 0.00012233
Iteration 293/1000 | Loss: 0.00012233
Iteration 294/1000 | Loss: 0.00012233
Iteration 295/1000 | Loss: 0.00012233
Iteration 296/1000 | Loss: 0.00012233
Iteration 297/1000 | Loss: 0.00012233
Iteration 298/1000 | Loss: 0.00012233
Iteration 299/1000 | Loss: 0.00012233
Iteration 300/1000 | Loss: 0.00012233
Iteration 301/1000 | Loss: 0.00012233
Iteration 302/1000 | Loss: 0.00012233
Iteration 303/1000 | Loss: 0.00012232
Iteration 304/1000 | Loss: 0.00012232
Iteration 305/1000 | Loss: 0.00012232
Iteration 306/1000 | Loss: 0.00012232
Iteration 307/1000 | Loss: 0.00012232
Iteration 308/1000 | Loss: 0.00012232
Iteration 309/1000 | Loss: 0.00012232
Iteration 310/1000 | Loss: 0.00012232
Iteration 311/1000 | Loss: 0.00012232
Iteration 312/1000 | Loss: 0.00012232
Iteration 313/1000 | Loss: 0.00012232
Iteration 314/1000 | Loss: 0.00012231
Iteration 315/1000 | Loss: 0.00012231
Iteration 316/1000 | Loss: 0.00012231
Iteration 317/1000 | Loss: 0.00012231
Iteration 318/1000 | Loss: 0.00012230
Iteration 319/1000 | Loss: 0.00012230
Iteration 320/1000 | Loss: 0.00012230
Iteration 321/1000 | Loss: 0.00012230
Iteration 322/1000 | Loss: 0.00012230
Iteration 323/1000 | Loss: 0.00012230
Iteration 324/1000 | Loss: 0.00012230
Iteration 325/1000 | Loss: 0.00012230
Iteration 326/1000 | Loss: 0.00012229
Iteration 327/1000 | Loss: 0.00012229
Iteration 328/1000 | Loss: 0.00012229
Iteration 329/1000 | Loss: 0.00012842
Iteration 330/1000 | Loss: 0.00012226
Iteration 331/1000 | Loss: 0.00012225
Iteration 332/1000 | Loss: 0.00012225
Iteration 333/1000 | Loss: 0.00012225
Iteration 334/1000 | Loss: 0.00012225
Iteration 335/1000 | Loss: 0.00012225
Iteration 336/1000 | Loss: 0.00012225
Iteration 337/1000 | Loss: 0.00012224
Iteration 338/1000 | Loss: 0.00012224
Iteration 339/1000 | Loss: 0.00012224
Iteration 340/1000 | Loss: 0.00012224
Iteration 341/1000 | Loss: 0.00012224
Iteration 342/1000 | Loss: 0.00012224
Iteration 343/1000 | Loss: 0.00012224
Iteration 344/1000 | Loss: 0.00012224
Iteration 345/1000 | Loss: 0.00012224
Iteration 346/1000 | Loss: 0.00012224
Iteration 347/1000 | Loss: 0.00012224
Iteration 348/1000 | Loss: 0.00012223
Iteration 349/1000 | Loss: 0.00012223
Iteration 350/1000 | Loss: 0.00012223
Iteration 351/1000 | Loss: 0.00012223
Iteration 352/1000 | Loss: 0.00012223
Iteration 353/1000 | Loss: 0.00012223
Iteration 354/1000 | Loss: 0.00012223
Iteration 355/1000 | Loss: 0.00012222
Iteration 356/1000 | Loss: 0.00012222
Iteration 357/1000 | Loss: 0.00012222
Iteration 358/1000 | Loss: 0.00012222
Iteration 359/1000 | Loss: 0.00012222
Iteration 360/1000 | Loss: 0.00012222
Iteration 361/1000 | Loss: 0.00012222
Iteration 362/1000 | Loss: 0.00012222
Iteration 363/1000 | Loss: 0.00012221
Iteration 364/1000 | Loss: 0.00012221
Iteration 365/1000 | Loss: 0.00012221
Iteration 366/1000 | Loss: 0.00012221
Iteration 367/1000 | Loss: 0.00012221
Iteration 368/1000 | Loss: 0.00012221
Iteration 369/1000 | Loss: 0.00012221
Iteration 370/1000 | Loss: 0.00012221
Iteration 371/1000 | Loss: 0.00012221
Iteration 372/1000 | Loss: 0.00012221
Iteration 373/1000 | Loss: 0.00012221
Iteration 374/1000 | Loss: 0.00012221
Iteration 375/1000 | Loss: 0.00012221
Iteration 376/1000 | Loss: 0.00012221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 376. Stopping optimization.
Last 5 losses: [0.00012221386714372784, 0.00012221386714372784, 0.00012221386714372784, 0.00012221386714372784, 0.00012221386714372784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00012221386714372784

Optimization complete. Final v2v error: 6.671825408935547 mm

Highest mean error: 14.388854026794434 mm for frame 115

Lowest mean error: 4.453378200531006 mm for frame 95

Saving results

Total time: 424.4002139568329
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01160093
Iteration 2/25 | Loss: 0.00358893
Iteration 3/25 | Loss: 0.00278531
Iteration 4/25 | Loss: 0.00232205
Iteration 5/25 | Loss: 0.00222189
Iteration 6/25 | Loss: 0.00208157
Iteration 7/25 | Loss: 0.00194584
Iteration 8/25 | Loss: 0.00189261
Iteration 9/25 | Loss: 0.00184223
Iteration 10/25 | Loss: 0.00178479
Iteration 11/25 | Loss: 0.00173316
Iteration 12/25 | Loss: 0.00171378
Iteration 13/25 | Loss: 0.00170025
Iteration 14/25 | Loss: 0.00169133
Iteration 15/25 | Loss: 0.00169989
Iteration 16/25 | Loss: 0.00169598
Iteration 17/25 | Loss: 0.00167786
Iteration 18/25 | Loss: 0.00166669
Iteration 19/25 | Loss: 0.00166763
Iteration 20/25 | Loss: 0.00166835
Iteration 21/25 | Loss: 0.00166455
Iteration 22/25 | Loss: 0.00166154
Iteration 23/25 | Loss: 0.00166180
Iteration 24/25 | Loss: 0.00165760
Iteration 25/25 | Loss: 0.00166235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41713333
Iteration 2/25 | Loss: 0.00748533
Iteration 3/25 | Loss: 0.00541709
Iteration 4/25 | Loss: 0.00487229
Iteration 5/25 | Loss: 0.00487229
Iteration 6/25 | Loss: 0.00487229
Iteration 7/25 | Loss: 0.00487229
Iteration 8/25 | Loss: 0.00487229
Iteration 9/25 | Loss: 0.00487229
Iteration 10/25 | Loss: 0.00487229
Iteration 11/25 | Loss: 0.00487229
Iteration 12/25 | Loss: 0.00487229
Iteration 13/25 | Loss: 0.00487229
Iteration 14/25 | Loss: 0.00487229
Iteration 15/25 | Loss: 0.00487229
Iteration 16/25 | Loss: 0.00487229
Iteration 17/25 | Loss: 0.00487229
Iteration 18/25 | Loss: 0.00487229
Iteration 19/25 | Loss: 0.00487229
Iteration 20/25 | Loss: 0.00487229
Iteration 21/25 | Loss: 0.00487229
Iteration 22/25 | Loss: 0.00487229
Iteration 23/25 | Loss: 0.00487229
Iteration 24/25 | Loss: 0.00487229
Iteration 25/25 | Loss: 0.00487229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00487229
Iteration 2/1000 | Loss: 0.00142136
Iteration 3/1000 | Loss: 0.00653696
Iteration 4/1000 | Loss: 0.00105776
Iteration 5/1000 | Loss: 0.00163000
Iteration 6/1000 | Loss: 0.00103579
Iteration 7/1000 | Loss: 0.00084352
Iteration 8/1000 | Loss: 0.00200833
Iteration 9/1000 | Loss: 0.00194446
Iteration 10/1000 | Loss: 0.00068347
Iteration 11/1000 | Loss: 0.00102131
Iteration 12/1000 | Loss: 0.00032803
Iteration 13/1000 | Loss: 0.00096855
Iteration 14/1000 | Loss: 0.00032100
Iteration 15/1000 | Loss: 0.00063101
Iteration 16/1000 | Loss: 0.00066146
Iteration 17/1000 | Loss: 0.00069910
Iteration 18/1000 | Loss: 0.00043399
Iteration 19/1000 | Loss: 0.00058728
Iteration 20/1000 | Loss: 0.00045865
Iteration 21/1000 | Loss: 0.00026679
Iteration 22/1000 | Loss: 0.00046827
Iteration 23/1000 | Loss: 0.00074185
Iteration 24/1000 | Loss: 0.00044723
Iteration 25/1000 | Loss: 0.00058243
Iteration 26/1000 | Loss: 0.00080111
Iteration 27/1000 | Loss: 0.00047950
Iteration 28/1000 | Loss: 0.00041115
Iteration 29/1000 | Loss: 0.00120912
Iteration 30/1000 | Loss: 0.00103509
Iteration 31/1000 | Loss: 0.00093382
Iteration 32/1000 | Loss: 0.00067120
Iteration 33/1000 | Loss: 0.00043017
Iteration 34/1000 | Loss: 0.00075122
Iteration 35/1000 | Loss: 0.00024824
Iteration 36/1000 | Loss: 0.00024647
Iteration 37/1000 | Loss: 0.00023739
Iteration 38/1000 | Loss: 0.00051330
Iteration 39/1000 | Loss: 0.00048251
Iteration 40/1000 | Loss: 0.00025161
Iteration 41/1000 | Loss: 0.00023808
Iteration 42/1000 | Loss: 0.00023072
Iteration 43/1000 | Loss: 0.00050281
Iteration 44/1000 | Loss: 0.00044321
Iteration 45/1000 | Loss: 0.00024814
Iteration 46/1000 | Loss: 0.00023033
Iteration 47/1000 | Loss: 0.00022379
Iteration 48/1000 | Loss: 0.00021963
Iteration 49/1000 | Loss: 0.00021759
Iteration 50/1000 | Loss: 0.00021659
Iteration 51/1000 | Loss: 0.00021596
Iteration 52/1000 | Loss: 0.00021556
Iteration 53/1000 | Loss: 0.00021529
Iteration 54/1000 | Loss: 0.00084946
Iteration 55/1000 | Loss: 0.00094368
Iteration 56/1000 | Loss: 0.00055788
Iteration 57/1000 | Loss: 0.00077524
Iteration 58/1000 | Loss: 0.00021890
Iteration 59/1000 | Loss: 0.00021637
Iteration 60/1000 | Loss: 0.00021554
Iteration 61/1000 | Loss: 0.00021522
Iteration 62/1000 | Loss: 0.00021502
Iteration 63/1000 | Loss: 0.00097828
Iteration 64/1000 | Loss: 0.00111421
Iteration 65/1000 | Loss: 0.00085663
Iteration 66/1000 | Loss: 0.00110256
Iteration 67/1000 | Loss: 0.00044077
Iteration 68/1000 | Loss: 0.00097238
Iteration 69/1000 | Loss: 0.00044809
Iteration 70/1000 | Loss: 0.00087571
Iteration 71/1000 | Loss: 0.00038932
Iteration 72/1000 | Loss: 0.00047750
Iteration 73/1000 | Loss: 0.00021926
Iteration 74/1000 | Loss: 0.00021531
Iteration 75/1000 | Loss: 0.00021158
Iteration 76/1000 | Loss: 0.00020956
Iteration 77/1000 | Loss: 0.00020864
Iteration 78/1000 | Loss: 0.00020810
Iteration 79/1000 | Loss: 0.00020777
Iteration 80/1000 | Loss: 0.00020750
Iteration 81/1000 | Loss: 0.00020734
Iteration 82/1000 | Loss: 0.00020734
Iteration 83/1000 | Loss: 0.00020729
Iteration 84/1000 | Loss: 0.00020729
Iteration 85/1000 | Loss: 0.00020729
Iteration 86/1000 | Loss: 0.00020729
Iteration 87/1000 | Loss: 0.00020729
Iteration 88/1000 | Loss: 0.00020728
Iteration 89/1000 | Loss: 0.00020728
Iteration 90/1000 | Loss: 0.00020726
Iteration 91/1000 | Loss: 0.00020726
Iteration 92/1000 | Loss: 0.00020725
Iteration 93/1000 | Loss: 0.00020725
Iteration 94/1000 | Loss: 0.00020725
Iteration 95/1000 | Loss: 0.00020725
Iteration 96/1000 | Loss: 0.00020725
Iteration 97/1000 | Loss: 0.00020725
Iteration 98/1000 | Loss: 0.00020724
Iteration 99/1000 | Loss: 0.00020724
Iteration 100/1000 | Loss: 0.00020724
Iteration 101/1000 | Loss: 0.00020724
Iteration 102/1000 | Loss: 0.00020724
Iteration 103/1000 | Loss: 0.00020724
Iteration 104/1000 | Loss: 0.00020723
Iteration 105/1000 | Loss: 0.00020723
Iteration 106/1000 | Loss: 0.00020723
Iteration 107/1000 | Loss: 0.00020723
Iteration 108/1000 | Loss: 0.00020723
Iteration 109/1000 | Loss: 0.00020723
Iteration 110/1000 | Loss: 0.00020723
Iteration 111/1000 | Loss: 0.00020723
Iteration 112/1000 | Loss: 0.00020723
Iteration 113/1000 | Loss: 0.00020722
Iteration 114/1000 | Loss: 0.00020722
Iteration 115/1000 | Loss: 0.00020722
Iteration 116/1000 | Loss: 0.00020722
Iteration 117/1000 | Loss: 0.00020722
Iteration 118/1000 | Loss: 0.00020722
Iteration 119/1000 | Loss: 0.00020722
Iteration 120/1000 | Loss: 0.00020722
Iteration 121/1000 | Loss: 0.00020722
Iteration 122/1000 | Loss: 0.00020722
Iteration 123/1000 | Loss: 0.00020721
Iteration 124/1000 | Loss: 0.00020721
Iteration 125/1000 | Loss: 0.00020721
Iteration 126/1000 | Loss: 0.00020721
Iteration 127/1000 | Loss: 0.00020721
Iteration 128/1000 | Loss: 0.00020721
Iteration 129/1000 | Loss: 0.00020721
Iteration 130/1000 | Loss: 0.00020721
Iteration 131/1000 | Loss: 0.00020721
Iteration 132/1000 | Loss: 0.00020720
Iteration 133/1000 | Loss: 0.00020720
Iteration 134/1000 | Loss: 0.00020720
Iteration 135/1000 | Loss: 0.00020720
Iteration 136/1000 | Loss: 0.00020720
Iteration 137/1000 | Loss: 0.00020720
Iteration 138/1000 | Loss: 0.00020720
Iteration 139/1000 | Loss: 0.00020720
Iteration 140/1000 | Loss: 0.00020720
Iteration 141/1000 | Loss: 0.00020719
Iteration 142/1000 | Loss: 0.00020719
Iteration 143/1000 | Loss: 0.00020719
Iteration 144/1000 | Loss: 0.00020719
Iteration 145/1000 | Loss: 0.00020719
Iteration 146/1000 | Loss: 0.00020719
Iteration 147/1000 | Loss: 0.00020719
Iteration 148/1000 | Loss: 0.00020719
Iteration 149/1000 | Loss: 0.00020719
Iteration 150/1000 | Loss: 0.00020719
Iteration 151/1000 | Loss: 0.00020719
Iteration 152/1000 | Loss: 0.00020719
Iteration 153/1000 | Loss: 0.00020719
Iteration 154/1000 | Loss: 0.00020719
Iteration 155/1000 | Loss: 0.00020718
Iteration 156/1000 | Loss: 0.00020718
Iteration 157/1000 | Loss: 0.00020718
Iteration 158/1000 | Loss: 0.00020718
Iteration 159/1000 | Loss: 0.00020718
Iteration 160/1000 | Loss: 0.00020718
Iteration 161/1000 | Loss: 0.00020718
Iteration 162/1000 | Loss: 0.00020718
Iteration 163/1000 | Loss: 0.00020718
Iteration 164/1000 | Loss: 0.00020718
Iteration 165/1000 | Loss: 0.00020718
Iteration 166/1000 | Loss: 0.00020718
Iteration 167/1000 | Loss: 0.00020717
Iteration 168/1000 | Loss: 0.00020717
Iteration 169/1000 | Loss: 0.00020717
Iteration 170/1000 | Loss: 0.00020717
Iteration 171/1000 | Loss: 0.00020717
Iteration 172/1000 | Loss: 0.00020717
Iteration 173/1000 | Loss: 0.00020717
Iteration 174/1000 | Loss: 0.00020717
Iteration 175/1000 | Loss: 0.00020717
Iteration 176/1000 | Loss: 0.00020717
Iteration 177/1000 | Loss: 0.00020717
Iteration 178/1000 | Loss: 0.00020717
Iteration 179/1000 | Loss: 0.00020717
Iteration 180/1000 | Loss: 0.00020717
Iteration 181/1000 | Loss: 0.00020717
Iteration 182/1000 | Loss: 0.00020717
Iteration 183/1000 | Loss: 0.00020717
Iteration 184/1000 | Loss: 0.00020717
Iteration 185/1000 | Loss: 0.00020717
Iteration 186/1000 | Loss: 0.00020717
Iteration 187/1000 | Loss: 0.00020717
Iteration 188/1000 | Loss: 0.00020717
Iteration 189/1000 | Loss: 0.00020717
Iteration 190/1000 | Loss: 0.00020717
Iteration 191/1000 | Loss: 0.00020717
Iteration 192/1000 | Loss: 0.00020717
Iteration 193/1000 | Loss: 0.00020717
Iteration 194/1000 | Loss: 0.00020717
Iteration 195/1000 | Loss: 0.00020717
Iteration 196/1000 | Loss: 0.00020717
Iteration 197/1000 | Loss: 0.00020717
Iteration 198/1000 | Loss: 0.00020717
Iteration 199/1000 | Loss: 0.00020717
Iteration 200/1000 | Loss: 0.00020717
Iteration 201/1000 | Loss: 0.00020717
Iteration 202/1000 | Loss: 0.00020717
Iteration 203/1000 | Loss: 0.00020717
Iteration 204/1000 | Loss: 0.00020717
Iteration 205/1000 | Loss: 0.00020717
Iteration 206/1000 | Loss: 0.00020717
Iteration 207/1000 | Loss: 0.00020717
Iteration 208/1000 | Loss: 0.00020717
Iteration 209/1000 | Loss: 0.00020717
Iteration 210/1000 | Loss: 0.00020717
Iteration 211/1000 | Loss: 0.00020717
Iteration 212/1000 | Loss: 0.00020717
Iteration 213/1000 | Loss: 0.00020717
Iteration 214/1000 | Loss: 0.00020717
Iteration 215/1000 | Loss: 0.00020717
Iteration 216/1000 | Loss: 0.00020717
Iteration 217/1000 | Loss: 0.00020717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [0.0002071741473628208, 0.0002071741473628208, 0.0002071741473628208, 0.0002071741473628208, 0.0002071741473628208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002071741473628208

Optimization complete. Final v2v error: 7.310476303100586 mm

Highest mean error: 14.544557571411133 mm for frame 118

Lowest mean error: 4.573094367980957 mm for frame 151

Saving results

Total time: 171.36284279823303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923004
Iteration 2/25 | Loss: 0.00166795
Iteration 3/25 | Loss: 0.00144542
Iteration 4/25 | Loss: 0.00142867
Iteration 5/25 | Loss: 0.00142468
Iteration 6/25 | Loss: 0.00142393
Iteration 7/25 | Loss: 0.00142393
Iteration 8/25 | Loss: 0.00142393
Iteration 9/25 | Loss: 0.00142393
Iteration 10/25 | Loss: 0.00142393
Iteration 11/25 | Loss: 0.00142393
Iteration 12/25 | Loss: 0.00142393
Iteration 13/25 | Loss: 0.00142393
Iteration 14/25 | Loss: 0.00142393
Iteration 15/25 | Loss: 0.00142393
Iteration 16/25 | Loss: 0.00142393
Iteration 17/25 | Loss: 0.00142393
Iteration 18/25 | Loss: 0.00142393
Iteration 19/25 | Loss: 0.00142393
Iteration 20/25 | Loss: 0.00142393
Iteration 21/25 | Loss: 0.00142393
Iteration 22/25 | Loss: 0.00142393
Iteration 23/25 | Loss: 0.00142393
Iteration 24/25 | Loss: 0.00142393
Iteration 25/25 | Loss: 0.00142393

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44696510
Iteration 2/25 | Loss: 0.00117430
Iteration 3/25 | Loss: 0.00117430
Iteration 4/25 | Loss: 0.00117430
Iteration 5/25 | Loss: 0.00117430
Iteration 6/25 | Loss: 0.00117430
Iteration 7/25 | Loss: 0.00117430
Iteration 8/25 | Loss: 0.00117430
Iteration 9/25 | Loss: 0.00117430
Iteration 10/25 | Loss: 0.00117430
Iteration 11/25 | Loss: 0.00117430
Iteration 12/25 | Loss: 0.00117430
Iteration 13/25 | Loss: 0.00117430
Iteration 14/25 | Loss: 0.00117430
Iteration 15/25 | Loss: 0.00117430
Iteration 16/25 | Loss: 0.00117430
Iteration 17/25 | Loss: 0.00117430
Iteration 18/25 | Loss: 0.00117430
Iteration 19/25 | Loss: 0.00117430
Iteration 20/25 | Loss: 0.00117430
Iteration 21/25 | Loss: 0.00117430
Iteration 22/25 | Loss: 0.00117430
Iteration 23/25 | Loss: 0.00117430
Iteration 24/25 | Loss: 0.00117430
Iteration 25/25 | Loss: 0.00117430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117430
Iteration 2/1000 | Loss: 0.00008071
Iteration 3/1000 | Loss: 0.00006507
Iteration 4/1000 | Loss: 0.00005904
Iteration 5/1000 | Loss: 0.00005682
Iteration 6/1000 | Loss: 0.00005590
Iteration 7/1000 | Loss: 0.00005508
Iteration 8/1000 | Loss: 0.00005441
Iteration 9/1000 | Loss: 0.00005392
Iteration 10/1000 | Loss: 0.00005359
Iteration 11/1000 | Loss: 0.00005338
Iteration 12/1000 | Loss: 0.00005336
Iteration 13/1000 | Loss: 0.00005331
Iteration 14/1000 | Loss: 0.00005326
Iteration 15/1000 | Loss: 0.00005326
Iteration 16/1000 | Loss: 0.00005325
Iteration 17/1000 | Loss: 0.00005322
Iteration 18/1000 | Loss: 0.00005321
Iteration 19/1000 | Loss: 0.00005320
Iteration 20/1000 | Loss: 0.00005320
Iteration 21/1000 | Loss: 0.00005320
Iteration 22/1000 | Loss: 0.00005319
Iteration 23/1000 | Loss: 0.00005319
Iteration 24/1000 | Loss: 0.00005318
Iteration 25/1000 | Loss: 0.00005318
Iteration 26/1000 | Loss: 0.00005318
Iteration 27/1000 | Loss: 0.00005317
Iteration 28/1000 | Loss: 0.00005317
Iteration 29/1000 | Loss: 0.00005317
Iteration 30/1000 | Loss: 0.00005317
Iteration 31/1000 | Loss: 0.00005317
Iteration 32/1000 | Loss: 0.00005317
Iteration 33/1000 | Loss: 0.00005317
Iteration 34/1000 | Loss: 0.00005316
Iteration 35/1000 | Loss: 0.00005316
Iteration 36/1000 | Loss: 0.00005316
Iteration 37/1000 | Loss: 0.00005316
Iteration 38/1000 | Loss: 0.00005316
Iteration 39/1000 | Loss: 0.00005316
Iteration 40/1000 | Loss: 0.00005316
Iteration 41/1000 | Loss: 0.00005315
Iteration 42/1000 | Loss: 0.00005315
Iteration 43/1000 | Loss: 0.00005315
Iteration 44/1000 | Loss: 0.00005315
Iteration 45/1000 | Loss: 0.00005315
Iteration 46/1000 | Loss: 0.00005315
Iteration 47/1000 | Loss: 0.00005314
Iteration 48/1000 | Loss: 0.00005314
Iteration 49/1000 | Loss: 0.00005313
Iteration 50/1000 | Loss: 0.00005313
Iteration 51/1000 | Loss: 0.00005313
Iteration 52/1000 | Loss: 0.00005312
Iteration 53/1000 | Loss: 0.00005312
Iteration 54/1000 | Loss: 0.00005311
Iteration 55/1000 | Loss: 0.00005311
Iteration 56/1000 | Loss: 0.00005311
Iteration 57/1000 | Loss: 0.00005310
Iteration 58/1000 | Loss: 0.00005310
Iteration 59/1000 | Loss: 0.00005310
Iteration 60/1000 | Loss: 0.00005309
Iteration 61/1000 | Loss: 0.00005309
Iteration 62/1000 | Loss: 0.00005309
Iteration 63/1000 | Loss: 0.00005309
Iteration 64/1000 | Loss: 0.00005309
Iteration 65/1000 | Loss: 0.00005309
Iteration 66/1000 | Loss: 0.00005308
Iteration 67/1000 | Loss: 0.00005308
Iteration 68/1000 | Loss: 0.00005307
Iteration 69/1000 | Loss: 0.00005307
Iteration 70/1000 | Loss: 0.00005307
Iteration 71/1000 | Loss: 0.00005307
Iteration 72/1000 | Loss: 0.00005307
Iteration 73/1000 | Loss: 0.00005307
Iteration 74/1000 | Loss: 0.00005307
Iteration 75/1000 | Loss: 0.00005307
Iteration 76/1000 | Loss: 0.00005307
Iteration 77/1000 | Loss: 0.00005306
Iteration 78/1000 | Loss: 0.00005306
Iteration 79/1000 | Loss: 0.00005305
Iteration 80/1000 | Loss: 0.00005305
Iteration 81/1000 | Loss: 0.00005305
Iteration 82/1000 | Loss: 0.00005305
Iteration 83/1000 | Loss: 0.00005305
Iteration 84/1000 | Loss: 0.00005305
Iteration 85/1000 | Loss: 0.00005305
Iteration 86/1000 | Loss: 0.00005305
Iteration 87/1000 | Loss: 0.00005305
Iteration 88/1000 | Loss: 0.00005305
Iteration 89/1000 | Loss: 0.00005304
Iteration 90/1000 | Loss: 0.00005304
Iteration 91/1000 | Loss: 0.00005304
Iteration 92/1000 | Loss: 0.00005304
Iteration 93/1000 | Loss: 0.00005304
Iteration 94/1000 | Loss: 0.00005303
Iteration 95/1000 | Loss: 0.00005303
Iteration 96/1000 | Loss: 0.00005303
Iteration 97/1000 | Loss: 0.00005303
Iteration 98/1000 | Loss: 0.00005303
Iteration 99/1000 | Loss: 0.00005303
Iteration 100/1000 | Loss: 0.00005303
Iteration 101/1000 | Loss: 0.00005303
Iteration 102/1000 | Loss: 0.00005303
Iteration 103/1000 | Loss: 0.00005303
Iteration 104/1000 | Loss: 0.00005303
Iteration 105/1000 | Loss: 0.00005303
Iteration 106/1000 | Loss: 0.00005303
Iteration 107/1000 | Loss: 0.00005303
Iteration 108/1000 | Loss: 0.00005303
Iteration 109/1000 | Loss: 0.00005303
Iteration 110/1000 | Loss: 0.00005303
Iteration 111/1000 | Loss: 0.00005302
Iteration 112/1000 | Loss: 0.00005302
Iteration 113/1000 | Loss: 0.00005302
Iteration 114/1000 | Loss: 0.00005302
Iteration 115/1000 | Loss: 0.00005302
Iteration 116/1000 | Loss: 0.00005302
Iteration 117/1000 | Loss: 0.00005302
Iteration 118/1000 | Loss: 0.00005302
Iteration 119/1000 | Loss: 0.00005302
Iteration 120/1000 | Loss: 0.00005302
Iteration 121/1000 | Loss: 0.00005302
Iteration 122/1000 | Loss: 0.00005302
Iteration 123/1000 | Loss: 0.00005302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [5.3023930377094075e-05, 5.3023930377094075e-05, 5.3023930377094075e-05, 5.3023930377094075e-05, 5.3023930377094075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.3023930377094075e-05

Optimization complete. Final v2v error: 5.872809886932373 mm

Highest mean error: 6.274891376495361 mm for frame 16

Lowest mean error: 5.323017597198486 mm for frame 61

Saving results

Total time: 32.761741161346436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435848
Iteration 2/25 | Loss: 0.00141144
Iteration 3/25 | Loss: 0.00132900
Iteration 4/25 | Loss: 0.00131480
Iteration 5/25 | Loss: 0.00130702
Iteration 6/25 | Loss: 0.00130555
Iteration 7/25 | Loss: 0.00130555
Iteration 8/25 | Loss: 0.00130555
Iteration 9/25 | Loss: 0.00130555
Iteration 10/25 | Loss: 0.00130555
Iteration 11/25 | Loss: 0.00130555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001305552665144205, 0.001305552665144205, 0.001305552665144205, 0.001305552665144205, 0.001305552665144205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001305552665144205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43889809
Iteration 2/25 | Loss: 0.00084444
Iteration 3/25 | Loss: 0.00084444
Iteration 4/25 | Loss: 0.00084444
Iteration 5/25 | Loss: 0.00084444
Iteration 6/25 | Loss: 0.00084444
Iteration 7/25 | Loss: 0.00084444
Iteration 8/25 | Loss: 0.00084444
Iteration 9/25 | Loss: 0.00084444
Iteration 10/25 | Loss: 0.00084444
Iteration 11/25 | Loss: 0.00084444
Iteration 12/25 | Loss: 0.00084444
Iteration 13/25 | Loss: 0.00084444
Iteration 14/25 | Loss: 0.00084444
Iteration 15/25 | Loss: 0.00084444
Iteration 16/25 | Loss: 0.00084444
Iteration 17/25 | Loss: 0.00084444
Iteration 18/25 | Loss: 0.00084444
Iteration 19/25 | Loss: 0.00084444
Iteration 20/25 | Loss: 0.00084444
Iteration 21/25 | Loss: 0.00084444
Iteration 22/25 | Loss: 0.00084444
Iteration 23/25 | Loss: 0.00084444
Iteration 24/25 | Loss: 0.00084444
Iteration 25/25 | Loss: 0.00084444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084444
Iteration 2/1000 | Loss: 0.00005065
Iteration 3/1000 | Loss: 0.00003583
Iteration 4/1000 | Loss: 0.00003265
Iteration 5/1000 | Loss: 0.00003126
Iteration 6/1000 | Loss: 0.00003033
Iteration 7/1000 | Loss: 0.00002976
Iteration 8/1000 | Loss: 0.00002958
Iteration 9/1000 | Loss: 0.00002955
Iteration 10/1000 | Loss: 0.00002955
Iteration 11/1000 | Loss: 0.00002955
Iteration 12/1000 | Loss: 0.00002954
Iteration 13/1000 | Loss: 0.00002939
Iteration 14/1000 | Loss: 0.00002938
Iteration 15/1000 | Loss: 0.00002930
Iteration 16/1000 | Loss: 0.00002930
Iteration 17/1000 | Loss: 0.00002929
Iteration 18/1000 | Loss: 0.00002929
Iteration 19/1000 | Loss: 0.00002920
Iteration 20/1000 | Loss: 0.00002920
Iteration 21/1000 | Loss: 0.00002920
Iteration 22/1000 | Loss: 0.00002919
Iteration 23/1000 | Loss: 0.00002919
Iteration 24/1000 | Loss: 0.00002919
Iteration 25/1000 | Loss: 0.00002919
Iteration 26/1000 | Loss: 0.00002919
Iteration 27/1000 | Loss: 0.00002916
Iteration 28/1000 | Loss: 0.00002916
Iteration 29/1000 | Loss: 0.00002915
Iteration 30/1000 | Loss: 0.00002915
Iteration 31/1000 | Loss: 0.00002915
Iteration 32/1000 | Loss: 0.00002912
Iteration 33/1000 | Loss: 0.00002912
Iteration 34/1000 | Loss: 0.00002911
Iteration 35/1000 | Loss: 0.00002911
Iteration 36/1000 | Loss: 0.00002910
Iteration 37/1000 | Loss: 0.00002910
Iteration 38/1000 | Loss: 0.00002909
Iteration 39/1000 | Loss: 0.00002908
Iteration 40/1000 | Loss: 0.00002908
Iteration 41/1000 | Loss: 0.00002908
Iteration 42/1000 | Loss: 0.00002908
Iteration 43/1000 | Loss: 0.00002908
Iteration 44/1000 | Loss: 0.00002908
Iteration 45/1000 | Loss: 0.00002907
Iteration 46/1000 | Loss: 0.00002907
Iteration 47/1000 | Loss: 0.00002906
Iteration 48/1000 | Loss: 0.00002905
Iteration 49/1000 | Loss: 0.00002905
Iteration 50/1000 | Loss: 0.00002904
Iteration 51/1000 | Loss: 0.00002904
Iteration 52/1000 | Loss: 0.00002904
Iteration 53/1000 | Loss: 0.00002903
Iteration 54/1000 | Loss: 0.00002903
Iteration 55/1000 | Loss: 0.00002902
Iteration 56/1000 | Loss: 0.00002902
Iteration 57/1000 | Loss: 0.00002901
Iteration 58/1000 | Loss: 0.00002901
Iteration 59/1000 | Loss: 0.00002901
Iteration 60/1000 | Loss: 0.00002901
Iteration 61/1000 | Loss: 0.00002901
Iteration 62/1000 | Loss: 0.00002900
Iteration 63/1000 | Loss: 0.00002900
Iteration 64/1000 | Loss: 0.00002900
Iteration 65/1000 | Loss: 0.00002900
Iteration 66/1000 | Loss: 0.00002900
Iteration 67/1000 | Loss: 0.00002900
Iteration 68/1000 | Loss: 0.00002899
Iteration 69/1000 | Loss: 0.00002899
Iteration 70/1000 | Loss: 0.00002899
Iteration 71/1000 | Loss: 0.00002899
Iteration 72/1000 | Loss: 0.00002898
Iteration 73/1000 | Loss: 0.00002898
Iteration 74/1000 | Loss: 0.00002898
Iteration 75/1000 | Loss: 0.00002898
Iteration 76/1000 | Loss: 0.00002898
Iteration 77/1000 | Loss: 0.00002897
Iteration 78/1000 | Loss: 0.00002897
Iteration 79/1000 | Loss: 0.00002897
Iteration 80/1000 | Loss: 0.00002897
Iteration 81/1000 | Loss: 0.00002897
Iteration 82/1000 | Loss: 0.00002897
Iteration 83/1000 | Loss: 0.00002897
Iteration 84/1000 | Loss: 0.00002897
Iteration 85/1000 | Loss: 0.00002897
Iteration 86/1000 | Loss: 0.00002896
Iteration 87/1000 | Loss: 0.00002896
Iteration 88/1000 | Loss: 0.00002896
Iteration 89/1000 | Loss: 0.00002895
Iteration 90/1000 | Loss: 0.00002894
Iteration 91/1000 | Loss: 0.00002894
Iteration 92/1000 | Loss: 0.00002894
Iteration 93/1000 | Loss: 0.00002893
Iteration 94/1000 | Loss: 0.00002893
Iteration 95/1000 | Loss: 0.00002893
Iteration 96/1000 | Loss: 0.00002893
Iteration 97/1000 | Loss: 0.00002892
Iteration 98/1000 | Loss: 0.00002892
Iteration 99/1000 | Loss: 0.00002892
Iteration 100/1000 | Loss: 0.00002892
Iteration 101/1000 | Loss: 0.00002892
Iteration 102/1000 | Loss: 0.00002892
Iteration 103/1000 | Loss: 0.00002891
Iteration 104/1000 | Loss: 0.00002891
Iteration 105/1000 | Loss: 0.00002891
Iteration 106/1000 | Loss: 0.00002891
Iteration 107/1000 | Loss: 0.00002891
Iteration 108/1000 | Loss: 0.00002891
Iteration 109/1000 | Loss: 0.00002891
Iteration 110/1000 | Loss: 0.00002891
Iteration 111/1000 | Loss: 0.00002891
Iteration 112/1000 | Loss: 0.00002891
Iteration 113/1000 | Loss: 0.00002891
Iteration 114/1000 | Loss: 0.00002891
Iteration 115/1000 | Loss: 0.00002891
Iteration 116/1000 | Loss: 0.00002891
Iteration 117/1000 | Loss: 0.00002891
Iteration 118/1000 | Loss: 0.00002891
Iteration 119/1000 | Loss: 0.00002891
Iteration 120/1000 | Loss: 0.00002891
Iteration 121/1000 | Loss: 0.00002891
Iteration 122/1000 | Loss: 0.00002890
Iteration 123/1000 | Loss: 0.00002890
Iteration 124/1000 | Loss: 0.00002890
Iteration 125/1000 | Loss: 0.00002890
Iteration 126/1000 | Loss: 0.00002890
Iteration 127/1000 | Loss: 0.00002890
Iteration 128/1000 | Loss: 0.00002890
Iteration 129/1000 | Loss: 0.00002889
Iteration 130/1000 | Loss: 0.00002889
Iteration 131/1000 | Loss: 0.00002889
Iteration 132/1000 | Loss: 0.00002889
Iteration 133/1000 | Loss: 0.00002889
Iteration 134/1000 | Loss: 0.00002889
Iteration 135/1000 | Loss: 0.00002889
Iteration 136/1000 | Loss: 0.00002889
Iteration 137/1000 | Loss: 0.00002889
Iteration 138/1000 | Loss: 0.00002889
Iteration 139/1000 | Loss: 0.00002889
Iteration 140/1000 | Loss: 0.00002889
Iteration 141/1000 | Loss: 0.00002889
Iteration 142/1000 | Loss: 0.00002889
Iteration 143/1000 | Loss: 0.00002889
Iteration 144/1000 | Loss: 0.00002889
Iteration 145/1000 | Loss: 0.00002889
Iteration 146/1000 | Loss: 0.00002889
Iteration 147/1000 | Loss: 0.00002889
Iteration 148/1000 | Loss: 0.00002889
Iteration 149/1000 | Loss: 0.00002889
Iteration 150/1000 | Loss: 0.00002889
Iteration 151/1000 | Loss: 0.00002889
Iteration 152/1000 | Loss: 0.00002889
Iteration 153/1000 | Loss: 0.00002889
Iteration 154/1000 | Loss: 0.00002889
Iteration 155/1000 | Loss: 0.00002889
Iteration 156/1000 | Loss: 0.00002889
Iteration 157/1000 | Loss: 0.00002889
Iteration 158/1000 | Loss: 0.00002889
Iteration 159/1000 | Loss: 0.00002889
Iteration 160/1000 | Loss: 0.00002889
Iteration 161/1000 | Loss: 0.00002889
Iteration 162/1000 | Loss: 0.00002889
Iteration 163/1000 | Loss: 0.00002889
Iteration 164/1000 | Loss: 0.00002889
Iteration 165/1000 | Loss: 0.00002889
Iteration 166/1000 | Loss: 0.00002889
Iteration 167/1000 | Loss: 0.00002889
Iteration 168/1000 | Loss: 0.00002889
Iteration 169/1000 | Loss: 0.00002889
Iteration 170/1000 | Loss: 0.00002889
Iteration 171/1000 | Loss: 0.00002889
Iteration 172/1000 | Loss: 0.00002889
Iteration 173/1000 | Loss: 0.00002889
Iteration 174/1000 | Loss: 0.00002889
Iteration 175/1000 | Loss: 0.00002889
Iteration 176/1000 | Loss: 0.00002889
Iteration 177/1000 | Loss: 0.00002889
Iteration 178/1000 | Loss: 0.00002889
Iteration 179/1000 | Loss: 0.00002889
Iteration 180/1000 | Loss: 0.00002889
Iteration 181/1000 | Loss: 0.00002889
Iteration 182/1000 | Loss: 0.00002889
Iteration 183/1000 | Loss: 0.00002889
Iteration 184/1000 | Loss: 0.00002889
Iteration 185/1000 | Loss: 0.00002889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.8891085094073787e-05, 2.8891085094073787e-05, 2.8891085094073787e-05, 2.8891085094073787e-05, 2.8891085094073787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8891085094073787e-05

Optimization complete. Final v2v error: 4.688516616821289 mm

Highest mean error: 4.9139580726623535 mm for frame 29

Lowest mean error: 4.5336384773254395 mm for frame 62

Saving results

Total time: 38.477113246917725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118624
Iteration 2/25 | Loss: 0.00198090
Iteration 3/25 | Loss: 0.00156165
Iteration 4/25 | Loss: 0.00152538
Iteration 5/25 | Loss: 0.00151048
Iteration 6/25 | Loss: 0.00150674
Iteration 7/25 | Loss: 0.00150485
Iteration 8/25 | Loss: 0.00150465
Iteration 9/25 | Loss: 0.00150465
Iteration 10/25 | Loss: 0.00150465
Iteration 11/25 | Loss: 0.00150465
Iteration 12/25 | Loss: 0.00150465
Iteration 13/25 | Loss: 0.00150465
Iteration 14/25 | Loss: 0.00150465
Iteration 15/25 | Loss: 0.00150465
Iteration 16/25 | Loss: 0.00150465
Iteration 17/25 | Loss: 0.00150465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015046513872221112, 0.0015046513872221112, 0.0015046513872221112, 0.0015046513872221112, 0.0015046513872221112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015046513872221112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12800467
Iteration 2/25 | Loss: 0.00095318
Iteration 3/25 | Loss: 0.00095316
Iteration 4/25 | Loss: 0.00095316
Iteration 5/25 | Loss: 0.00095316
Iteration 6/25 | Loss: 0.00095316
Iteration 7/25 | Loss: 0.00095316
Iteration 8/25 | Loss: 0.00095316
Iteration 9/25 | Loss: 0.00095316
Iteration 10/25 | Loss: 0.00095316
Iteration 11/25 | Loss: 0.00095316
Iteration 12/25 | Loss: 0.00095316
Iteration 13/25 | Loss: 0.00095316
Iteration 14/25 | Loss: 0.00095316
Iteration 15/25 | Loss: 0.00095316
Iteration 16/25 | Loss: 0.00095316
Iteration 17/25 | Loss: 0.00095316
Iteration 18/25 | Loss: 0.00095316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009531599353067577, 0.0009531599353067577, 0.0009531599353067577, 0.0009531599353067577, 0.0009531599353067577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009531599353067577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095316
Iteration 2/1000 | Loss: 0.00010565
Iteration 3/1000 | Loss: 0.00007863
Iteration 4/1000 | Loss: 0.00006080
Iteration 5/1000 | Loss: 0.00005718
Iteration 6/1000 | Loss: 0.00005430
Iteration 7/1000 | Loss: 0.00005250
Iteration 8/1000 | Loss: 0.00005137
Iteration 9/1000 | Loss: 0.00005063
Iteration 10/1000 | Loss: 0.00005011
Iteration 11/1000 | Loss: 0.00004969
Iteration 12/1000 | Loss: 0.00004928
Iteration 13/1000 | Loss: 0.00004908
Iteration 14/1000 | Loss: 0.00004893
Iteration 15/1000 | Loss: 0.00004883
Iteration 16/1000 | Loss: 0.00004879
Iteration 17/1000 | Loss: 0.00004879
Iteration 18/1000 | Loss: 0.00004879
Iteration 19/1000 | Loss: 0.00004878
Iteration 20/1000 | Loss: 0.00004877
Iteration 21/1000 | Loss: 0.00004876
Iteration 22/1000 | Loss: 0.00004871
Iteration 23/1000 | Loss: 0.00004870
Iteration 24/1000 | Loss: 0.00004869
Iteration 25/1000 | Loss: 0.00004867
Iteration 26/1000 | Loss: 0.00004867
Iteration 27/1000 | Loss: 0.00004867
Iteration 28/1000 | Loss: 0.00004867
Iteration 29/1000 | Loss: 0.00004867
Iteration 30/1000 | Loss: 0.00004867
Iteration 31/1000 | Loss: 0.00004867
Iteration 32/1000 | Loss: 0.00004867
Iteration 33/1000 | Loss: 0.00004866
Iteration 34/1000 | Loss: 0.00004866
Iteration 35/1000 | Loss: 0.00004866
Iteration 36/1000 | Loss: 0.00004866
Iteration 37/1000 | Loss: 0.00004866
Iteration 38/1000 | Loss: 0.00004866
Iteration 39/1000 | Loss: 0.00004866
Iteration 40/1000 | Loss: 0.00004863
Iteration 41/1000 | Loss: 0.00004862
Iteration 42/1000 | Loss: 0.00004862
Iteration 43/1000 | Loss: 0.00004860
Iteration 44/1000 | Loss: 0.00004860
Iteration 45/1000 | Loss: 0.00004859
Iteration 46/1000 | Loss: 0.00004858
Iteration 47/1000 | Loss: 0.00004858
Iteration 48/1000 | Loss: 0.00004858
Iteration 49/1000 | Loss: 0.00004858
Iteration 50/1000 | Loss: 0.00004858
Iteration 51/1000 | Loss: 0.00004858
Iteration 52/1000 | Loss: 0.00004858
Iteration 53/1000 | Loss: 0.00004858
Iteration 54/1000 | Loss: 0.00004858
Iteration 55/1000 | Loss: 0.00004858
Iteration 56/1000 | Loss: 0.00004857
Iteration 57/1000 | Loss: 0.00004857
Iteration 58/1000 | Loss: 0.00004857
Iteration 59/1000 | Loss: 0.00004857
Iteration 60/1000 | Loss: 0.00004857
Iteration 61/1000 | Loss: 0.00004857
Iteration 62/1000 | Loss: 0.00004856
Iteration 63/1000 | Loss: 0.00004854
Iteration 64/1000 | Loss: 0.00004854
Iteration 65/1000 | Loss: 0.00004853
Iteration 66/1000 | Loss: 0.00004853
Iteration 67/1000 | Loss: 0.00004853
Iteration 68/1000 | Loss: 0.00004851
Iteration 69/1000 | Loss: 0.00004851
Iteration 70/1000 | Loss: 0.00004850
Iteration 71/1000 | Loss: 0.00004850
Iteration 72/1000 | Loss: 0.00004850
Iteration 73/1000 | Loss: 0.00004850
Iteration 74/1000 | Loss: 0.00004850
Iteration 75/1000 | Loss: 0.00004849
Iteration 76/1000 | Loss: 0.00004849
Iteration 77/1000 | Loss: 0.00004849
Iteration 78/1000 | Loss: 0.00004849
Iteration 79/1000 | Loss: 0.00004849
Iteration 80/1000 | Loss: 0.00004849
Iteration 81/1000 | Loss: 0.00004849
Iteration 82/1000 | Loss: 0.00004848
Iteration 83/1000 | Loss: 0.00004848
Iteration 84/1000 | Loss: 0.00004847
Iteration 85/1000 | Loss: 0.00004847
Iteration 86/1000 | Loss: 0.00004847
Iteration 87/1000 | Loss: 0.00004847
Iteration 88/1000 | Loss: 0.00004847
Iteration 89/1000 | Loss: 0.00004847
Iteration 90/1000 | Loss: 0.00004846
Iteration 91/1000 | Loss: 0.00004846
Iteration 92/1000 | Loss: 0.00004846
Iteration 93/1000 | Loss: 0.00004846
Iteration 94/1000 | Loss: 0.00004846
Iteration 95/1000 | Loss: 0.00004846
Iteration 96/1000 | Loss: 0.00004846
Iteration 97/1000 | Loss: 0.00004846
Iteration 98/1000 | Loss: 0.00004846
Iteration 99/1000 | Loss: 0.00004846
Iteration 100/1000 | Loss: 0.00004846
Iteration 101/1000 | Loss: 0.00004845
Iteration 102/1000 | Loss: 0.00004845
Iteration 103/1000 | Loss: 0.00004844
Iteration 104/1000 | Loss: 0.00004844
Iteration 105/1000 | Loss: 0.00004843
Iteration 106/1000 | Loss: 0.00004843
Iteration 107/1000 | Loss: 0.00004841
Iteration 108/1000 | Loss: 0.00004841
Iteration 109/1000 | Loss: 0.00004841
Iteration 110/1000 | Loss: 0.00004841
Iteration 111/1000 | Loss: 0.00004840
Iteration 112/1000 | Loss: 0.00004840
Iteration 113/1000 | Loss: 0.00004839
Iteration 114/1000 | Loss: 0.00004839
Iteration 115/1000 | Loss: 0.00004839
Iteration 116/1000 | Loss: 0.00004839
Iteration 117/1000 | Loss: 0.00004839
Iteration 118/1000 | Loss: 0.00004838
Iteration 119/1000 | Loss: 0.00004838
Iteration 120/1000 | Loss: 0.00004838
Iteration 121/1000 | Loss: 0.00004838
Iteration 122/1000 | Loss: 0.00004837
Iteration 123/1000 | Loss: 0.00004836
Iteration 124/1000 | Loss: 0.00004836
Iteration 125/1000 | Loss: 0.00004836
Iteration 126/1000 | Loss: 0.00004836
Iteration 127/1000 | Loss: 0.00004836
Iteration 128/1000 | Loss: 0.00004836
Iteration 129/1000 | Loss: 0.00004835
Iteration 130/1000 | Loss: 0.00004835
Iteration 131/1000 | Loss: 0.00004835
Iteration 132/1000 | Loss: 0.00004835
Iteration 133/1000 | Loss: 0.00004835
Iteration 134/1000 | Loss: 0.00004835
Iteration 135/1000 | Loss: 0.00004835
Iteration 136/1000 | Loss: 0.00004835
Iteration 137/1000 | Loss: 0.00004835
Iteration 138/1000 | Loss: 0.00004834
Iteration 139/1000 | Loss: 0.00004834
Iteration 140/1000 | Loss: 0.00004834
Iteration 141/1000 | Loss: 0.00004834
Iteration 142/1000 | Loss: 0.00004834
Iteration 143/1000 | Loss: 0.00004834
Iteration 144/1000 | Loss: 0.00004834
Iteration 145/1000 | Loss: 0.00004834
Iteration 146/1000 | Loss: 0.00004833
Iteration 147/1000 | Loss: 0.00004833
Iteration 148/1000 | Loss: 0.00004833
Iteration 149/1000 | Loss: 0.00004833
Iteration 150/1000 | Loss: 0.00004833
Iteration 151/1000 | Loss: 0.00004833
Iteration 152/1000 | Loss: 0.00004833
Iteration 153/1000 | Loss: 0.00004833
Iteration 154/1000 | Loss: 0.00004832
Iteration 155/1000 | Loss: 0.00004832
Iteration 156/1000 | Loss: 0.00004832
Iteration 157/1000 | Loss: 0.00004832
Iteration 158/1000 | Loss: 0.00004832
Iteration 159/1000 | Loss: 0.00004831
Iteration 160/1000 | Loss: 0.00004831
Iteration 161/1000 | Loss: 0.00004831
Iteration 162/1000 | Loss: 0.00004831
Iteration 163/1000 | Loss: 0.00004831
Iteration 164/1000 | Loss: 0.00004831
Iteration 165/1000 | Loss: 0.00004831
Iteration 166/1000 | Loss: 0.00004830
Iteration 167/1000 | Loss: 0.00004830
Iteration 168/1000 | Loss: 0.00004830
Iteration 169/1000 | Loss: 0.00004830
Iteration 170/1000 | Loss: 0.00004830
Iteration 171/1000 | Loss: 0.00004829
Iteration 172/1000 | Loss: 0.00004829
Iteration 173/1000 | Loss: 0.00004829
Iteration 174/1000 | Loss: 0.00004829
Iteration 175/1000 | Loss: 0.00004829
Iteration 176/1000 | Loss: 0.00004828
Iteration 177/1000 | Loss: 0.00004828
Iteration 178/1000 | Loss: 0.00004828
Iteration 179/1000 | Loss: 0.00004828
Iteration 180/1000 | Loss: 0.00004828
Iteration 181/1000 | Loss: 0.00004828
Iteration 182/1000 | Loss: 0.00004827
Iteration 183/1000 | Loss: 0.00004827
Iteration 184/1000 | Loss: 0.00004827
Iteration 185/1000 | Loss: 0.00004827
Iteration 186/1000 | Loss: 0.00004827
Iteration 187/1000 | Loss: 0.00004827
Iteration 188/1000 | Loss: 0.00004827
Iteration 189/1000 | Loss: 0.00004827
Iteration 190/1000 | Loss: 0.00004826
Iteration 191/1000 | Loss: 0.00004826
Iteration 192/1000 | Loss: 0.00004826
Iteration 193/1000 | Loss: 0.00004825
Iteration 194/1000 | Loss: 0.00004825
Iteration 195/1000 | Loss: 0.00004825
Iteration 196/1000 | Loss: 0.00004824
Iteration 197/1000 | Loss: 0.00004824
Iteration 198/1000 | Loss: 0.00004824
Iteration 199/1000 | Loss: 0.00004824
Iteration 200/1000 | Loss: 0.00004824
Iteration 201/1000 | Loss: 0.00004824
Iteration 202/1000 | Loss: 0.00004824
Iteration 203/1000 | Loss: 0.00004824
Iteration 204/1000 | Loss: 0.00004824
Iteration 205/1000 | Loss: 0.00004824
Iteration 206/1000 | Loss: 0.00004824
Iteration 207/1000 | Loss: 0.00004824
Iteration 208/1000 | Loss: 0.00004824
Iteration 209/1000 | Loss: 0.00004824
Iteration 210/1000 | Loss: 0.00004824
Iteration 211/1000 | Loss: 0.00004824
Iteration 212/1000 | Loss: 0.00004824
Iteration 213/1000 | Loss: 0.00004824
Iteration 214/1000 | Loss: 0.00004824
Iteration 215/1000 | Loss: 0.00004824
Iteration 216/1000 | Loss: 0.00004824
Iteration 217/1000 | Loss: 0.00004824
Iteration 218/1000 | Loss: 0.00004824
Iteration 219/1000 | Loss: 0.00004824
Iteration 220/1000 | Loss: 0.00004824
Iteration 221/1000 | Loss: 0.00004824
Iteration 222/1000 | Loss: 0.00004824
Iteration 223/1000 | Loss: 0.00004824
Iteration 224/1000 | Loss: 0.00004824
Iteration 225/1000 | Loss: 0.00004824
Iteration 226/1000 | Loss: 0.00004824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [4.8236921429634094e-05, 4.8236921429634094e-05, 4.8236921429634094e-05, 4.8236921429634094e-05, 4.8236921429634094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.8236921429634094e-05

Optimization complete. Final v2v error: 5.692846775054932 mm

Highest mean error: 6.471824645996094 mm for frame 75

Lowest mean error: 5.346188545227051 mm for frame 107

Saving results

Total time: 51.66942572593689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005326
Iteration 2/25 | Loss: 0.00336420
Iteration 3/25 | Loss: 0.00255333
Iteration 4/25 | Loss: 0.00211552
Iteration 5/25 | Loss: 0.00205341
Iteration 6/25 | Loss: 0.00203169
Iteration 7/25 | Loss: 0.00191731
Iteration 8/25 | Loss: 0.00186999
Iteration 9/25 | Loss: 0.00185175
Iteration 10/25 | Loss: 0.00188582
Iteration 11/25 | Loss: 0.00183068
Iteration 12/25 | Loss: 0.00180176
Iteration 13/25 | Loss: 0.00173917
Iteration 14/25 | Loss: 0.00175945
Iteration 15/25 | Loss: 0.00173667
Iteration 16/25 | Loss: 0.00175288
Iteration 17/25 | Loss: 0.00174189
Iteration 18/25 | Loss: 0.00175603
Iteration 19/25 | Loss: 0.00175692
Iteration 20/25 | Loss: 0.00177440
Iteration 21/25 | Loss: 0.00173266
Iteration 22/25 | Loss: 0.00172895
Iteration 23/25 | Loss: 0.00172311
Iteration 24/25 | Loss: 0.00176913
Iteration 25/25 | Loss: 0.00175521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.36254358
Iteration 2/25 | Loss: 0.01381341
Iteration 3/25 | Loss: 0.01197631
Iteration 4/25 | Loss: 0.00826411
Iteration 5/25 | Loss: 0.00741755
Iteration 6/25 | Loss: 0.00717491
Iteration 7/25 | Loss: 0.00693986
Iteration 8/25 | Loss: 0.00693986
Iteration 9/25 | Loss: 0.00693986
Iteration 10/25 | Loss: 0.00693986
Iteration 11/25 | Loss: 0.00693986
Iteration 12/25 | Loss: 0.00693986
Iteration 13/25 | Loss: 0.00693986
Iteration 14/25 | Loss: 0.00693986
Iteration 15/25 | Loss: 0.00693986
Iteration 16/25 | Loss: 0.00693986
Iteration 17/25 | Loss: 0.00693986
Iteration 18/25 | Loss: 0.00693986
Iteration 19/25 | Loss: 0.00693986
Iteration 20/25 | Loss: 0.00693986
Iteration 21/25 | Loss: 0.00693986
Iteration 22/25 | Loss: 0.00693986
Iteration 23/25 | Loss: 0.00693986
Iteration 24/25 | Loss: 0.00693986
Iteration 25/25 | Loss: 0.00693986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00693986
Iteration 2/1000 | Loss: 0.00334871
Iteration 3/1000 | Loss: 0.00651472
Iteration 4/1000 | Loss: 0.00444768
Iteration 5/1000 | Loss: 0.00613397
Iteration 6/1000 | Loss: 0.00424698
Iteration 7/1000 | Loss: 0.00718100
Iteration 8/1000 | Loss: 0.00780204
Iteration 9/1000 | Loss: 0.01009027
Iteration 10/1000 | Loss: 0.00479385
Iteration 11/1000 | Loss: 0.00483591
Iteration 12/1000 | Loss: 0.00429361
Iteration 13/1000 | Loss: 0.01073745
Iteration 14/1000 | Loss: 0.00550833
Iteration 15/1000 | Loss: 0.00737732
Iteration 16/1000 | Loss: 0.00577477
Iteration 17/1000 | Loss: 0.00686891
Iteration 18/1000 | Loss: 0.00639324
Iteration 19/1000 | Loss: 0.00439501
Iteration 20/1000 | Loss: 0.00332280
Iteration 21/1000 | Loss: 0.00567946
Iteration 22/1000 | Loss: 0.00415824
Iteration 23/1000 | Loss: 0.00748324
Iteration 24/1000 | Loss: 0.00479520
Iteration 25/1000 | Loss: 0.00373459
Iteration 26/1000 | Loss: 0.00452191
Iteration 27/1000 | Loss: 0.00636378
Iteration 28/1000 | Loss: 0.00157523
Iteration 29/1000 | Loss: 0.00394996
Iteration 30/1000 | Loss: 0.00300220
Iteration 31/1000 | Loss: 0.00519544
Iteration 32/1000 | Loss: 0.00538544
Iteration 33/1000 | Loss: 0.00424888
Iteration 34/1000 | Loss: 0.00211444
Iteration 35/1000 | Loss: 0.00307435
Iteration 36/1000 | Loss: 0.00215194
Iteration 37/1000 | Loss: 0.00510462
Iteration 38/1000 | Loss: 0.00345756
Iteration 39/1000 | Loss: 0.00354387
Iteration 40/1000 | Loss: 0.00623318
Iteration 41/1000 | Loss: 0.00480149
Iteration 42/1000 | Loss: 0.00331918
Iteration 43/1000 | Loss: 0.00237472
Iteration 44/1000 | Loss: 0.00732918
Iteration 45/1000 | Loss: 0.00298313
Iteration 46/1000 | Loss: 0.00360542
Iteration 47/1000 | Loss: 0.00234937
Iteration 48/1000 | Loss: 0.00608823
Iteration 49/1000 | Loss: 0.00315558
Iteration 50/1000 | Loss: 0.00343253
Iteration 51/1000 | Loss: 0.00247589
Iteration 52/1000 | Loss: 0.00556557
Iteration 53/1000 | Loss: 0.00360433
Iteration 54/1000 | Loss: 0.00446752
Iteration 55/1000 | Loss: 0.00460399
Iteration 56/1000 | Loss: 0.00401474
Iteration 57/1000 | Loss: 0.00375013
Iteration 58/1000 | Loss: 0.00260355
Iteration 59/1000 | Loss: 0.00263478
Iteration 60/1000 | Loss: 0.00073808
Iteration 61/1000 | Loss: 0.00099435
Iteration 62/1000 | Loss: 0.00194669
Iteration 63/1000 | Loss: 0.00092972
Iteration 64/1000 | Loss: 0.00497425
Iteration 65/1000 | Loss: 0.00084770
Iteration 66/1000 | Loss: 0.00355311
Iteration 67/1000 | Loss: 0.00194588
Iteration 68/1000 | Loss: 0.00285163
Iteration 69/1000 | Loss: 0.00192585
Iteration 70/1000 | Loss: 0.00152095
Iteration 71/1000 | Loss: 0.00296965
Iteration 72/1000 | Loss: 0.00240022
Iteration 73/1000 | Loss: 0.00231232
Iteration 74/1000 | Loss: 0.00200719
Iteration 75/1000 | Loss: 0.00347222
Iteration 76/1000 | Loss: 0.00157970
Iteration 77/1000 | Loss: 0.00250673
Iteration 78/1000 | Loss: 0.00339895
Iteration 79/1000 | Loss: 0.00273533
Iteration 80/1000 | Loss: 0.00232516
Iteration 81/1000 | Loss: 0.00112154
Iteration 82/1000 | Loss: 0.00169910
Iteration 83/1000 | Loss: 0.00199815
Iteration 84/1000 | Loss: 0.00263388
Iteration 85/1000 | Loss: 0.00280267
Iteration 86/1000 | Loss: 0.00126012
Iteration 87/1000 | Loss: 0.00163752
Iteration 88/1000 | Loss: 0.00129080
Iteration 89/1000 | Loss: 0.00177327
Iteration 90/1000 | Loss: 0.00412755
Iteration 91/1000 | Loss: 0.00162302
Iteration 92/1000 | Loss: 0.00151819
Iteration 93/1000 | Loss: 0.00156491
Iteration 94/1000 | Loss: 0.00325017
Iteration 95/1000 | Loss: 0.00176989
Iteration 96/1000 | Loss: 0.00324221
Iteration 97/1000 | Loss: 0.00208533
Iteration 98/1000 | Loss: 0.00267865
Iteration 99/1000 | Loss: 0.00201873
Iteration 100/1000 | Loss: 0.00276676
Iteration 101/1000 | Loss: 0.00211403
Iteration 102/1000 | Loss: 0.00242858
Iteration 103/1000 | Loss: 0.00366730
Iteration 104/1000 | Loss: 0.00221116
Iteration 105/1000 | Loss: 0.00245255
Iteration 106/1000 | Loss: 0.00181488
Iteration 107/1000 | Loss: 0.00165923
Iteration 108/1000 | Loss: 0.00120426
Iteration 109/1000 | Loss: 0.00122381
Iteration 110/1000 | Loss: 0.00108576
Iteration 111/1000 | Loss: 0.00187228
Iteration 112/1000 | Loss: 0.00149127
Iteration 113/1000 | Loss: 0.00127364
Iteration 114/1000 | Loss: 0.00148866
Iteration 115/1000 | Loss: 0.00227401
Iteration 116/1000 | Loss: 0.00202771
Iteration 117/1000 | Loss: 0.00278811
Iteration 118/1000 | Loss: 0.00249203
Iteration 119/1000 | Loss: 0.00209055
Iteration 120/1000 | Loss: 0.00161138
Iteration 121/1000 | Loss: 0.00203725
Iteration 122/1000 | Loss: 0.00136683
Iteration 123/1000 | Loss: 0.00140916
Iteration 124/1000 | Loss: 0.00130595
Iteration 125/1000 | Loss: 0.00123113
Iteration 126/1000 | Loss: 0.00119290
Iteration 127/1000 | Loss: 0.00133899
Iteration 128/1000 | Loss: 0.00169653
Iteration 129/1000 | Loss: 0.00136468
Iteration 130/1000 | Loss: 0.00064330
Iteration 131/1000 | Loss: 0.00122929
Iteration 132/1000 | Loss: 0.00057745
Iteration 133/1000 | Loss: 0.00192491
Iteration 134/1000 | Loss: 0.00125634
Iteration 135/1000 | Loss: 0.00153372
Iteration 136/1000 | Loss: 0.00159740
Iteration 137/1000 | Loss: 0.00111291
Iteration 138/1000 | Loss: 0.00149984
Iteration 139/1000 | Loss: 0.00301936
Iteration 140/1000 | Loss: 0.00103194
Iteration 141/1000 | Loss: 0.00257754
Iteration 142/1000 | Loss: 0.00043511
Iteration 143/1000 | Loss: 0.00066556
Iteration 144/1000 | Loss: 0.00130236
Iteration 145/1000 | Loss: 0.00046828
Iteration 146/1000 | Loss: 0.00168060
Iteration 147/1000 | Loss: 0.00038116
Iteration 148/1000 | Loss: 0.00024028
Iteration 149/1000 | Loss: 0.00032433
Iteration 150/1000 | Loss: 0.00063458
Iteration 151/1000 | Loss: 0.00139663
Iteration 152/1000 | Loss: 0.00084110
Iteration 153/1000 | Loss: 0.00159383
Iteration 154/1000 | Loss: 0.00100233
Iteration 155/1000 | Loss: 0.00032777
Iteration 156/1000 | Loss: 0.00045648
Iteration 157/1000 | Loss: 0.00056459
Iteration 158/1000 | Loss: 0.00031324
Iteration 159/1000 | Loss: 0.00095467
Iteration 160/1000 | Loss: 0.00063218
Iteration 161/1000 | Loss: 0.00049982
Iteration 162/1000 | Loss: 0.00041033
Iteration 163/1000 | Loss: 0.00105988
Iteration 164/1000 | Loss: 0.00128060
Iteration 165/1000 | Loss: 0.00174049
Iteration 166/1000 | Loss: 0.00112377
Iteration 167/1000 | Loss: 0.00063520
Iteration 168/1000 | Loss: 0.00120337
Iteration 169/1000 | Loss: 0.00067298
Iteration 170/1000 | Loss: 0.00049512
Iteration 171/1000 | Loss: 0.00051715
Iteration 172/1000 | Loss: 0.00038133
Iteration 173/1000 | Loss: 0.00083634
Iteration 174/1000 | Loss: 0.00054311
Iteration 175/1000 | Loss: 0.00037794
Iteration 176/1000 | Loss: 0.00051969
Iteration 177/1000 | Loss: 0.00011784
Iteration 178/1000 | Loss: 0.00118345
Iteration 179/1000 | Loss: 0.00051022
Iteration 180/1000 | Loss: 0.00072830
Iteration 181/1000 | Loss: 0.00038883
Iteration 182/1000 | Loss: 0.00041701
Iteration 183/1000 | Loss: 0.00040213
Iteration 184/1000 | Loss: 0.00009341
Iteration 185/1000 | Loss: 0.00024822
Iteration 186/1000 | Loss: 0.00305319
Iteration 187/1000 | Loss: 0.00031062
Iteration 188/1000 | Loss: 0.00053221
Iteration 189/1000 | Loss: 0.00046722
Iteration 190/1000 | Loss: 0.00062876
Iteration 191/1000 | Loss: 0.00009809
Iteration 192/1000 | Loss: 0.00007631
Iteration 193/1000 | Loss: 0.00006717
Iteration 194/1000 | Loss: 0.00006241
Iteration 195/1000 | Loss: 0.00005947
Iteration 196/1000 | Loss: 0.00016993
Iteration 197/1000 | Loss: 0.00015176
Iteration 198/1000 | Loss: 0.00010862
Iteration 199/1000 | Loss: 0.00006106
Iteration 200/1000 | Loss: 0.00013022
Iteration 201/1000 | Loss: 0.00010342
Iteration 202/1000 | Loss: 0.00010822
Iteration 203/1000 | Loss: 0.00013430
Iteration 204/1000 | Loss: 0.00007649
Iteration 205/1000 | Loss: 0.00012120
Iteration 206/1000 | Loss: 0.00006813
Iteration 207/1000 | Loss: 0.00005453
Iteration 208/1000 | Loss: 0.00006572
Iteration 209/1000 | Loss: 0.00006156
Iteration 210/1000 | Loss: 0.00005327
Iteration 211/1000 | Loss: 0.00014533
Iteration 212/1000 | Loss: 0.00016039
Iteration 213/1000 | Loss: 0.00007431
Iteration 214/1000 | Loss: 0.00013773
Iteration 215/1000 | Loss: 0.00017313
Iteration 216/1000 | Loss: 0.00007027
Iteration 217/1000 | Loss: 0.00014177
Iteration 218/1000 | Loss: 0.00017481
Iteration 219/1000 | Loss: 0.00031616
Iteration 220/1000 | Loss: 0.00030256
Iteration 221/1000 | Loss: 0.00009500
Iteration 222/1000 | Loss: 0.00014337
Iteration 223/1000 | Loss: 0.00027192
Iteration 224/1000 | Loss: 0.00036122
Iteration 225/1000 | Loss: 0.00008646
Iteration 226/1000 | Loss: 0.00007002
Iteration 227/1000 | Loss: 0.00005600
Iteration 228/1000 | Loss: 0.00062436
Iteration 229/1000 | Loss: 0.00043459
Iteration 230/1000 | Loss: 0.00006804
Iteration 231/1000 | Loss: 0.00007755
Iteration 232/1000 | Loss: 0.00005365
Iteration 233/1000 | Loss: 0.00005071
Iteration 234/1000 | Loss: 0.00006919
Iteration 235/1000 | Loss: 0.00005089
Iteration 236/1000 | Loss: 0.00005214
Iteration 237/1000 | Loss: 0.00004819
Iteration 238/1000 | Loss: 0.00004768
Iteration 239/1000 | Loss: 0.00004857
Iteration 240/1000 | Loss: 0.00004730
Iteration 241/1000 | Loss: 0.00004730
Iteration 242/1000 | Loss: 0.00004730
Iteration 243/1000 | Loss: 0.00004730
Iteration 244/1000 | Loss: 0.00004730
Iteration 245/1000 | Loss: 0.00004730
Iteration 246/1000 | Loss: 0.00004730
Iteration 247/1000 | Loss: 0.00004730
Iteration 248/1000 | Loss: 0.00004730
Iteration 249/1000 | Loss: 0.00004730
Iteration 250/1000 | Loss: 0.00004730
Iteration 251/1000 | Loss: 0.00004729
Iteration 252/1000 | Loss: 0.00004729
Iteration 253/1000 | Loss: 0.00004729
Iteration 254/1000 | Loss: 0.00004729
Iteration 255/1000 | Loss: 0.00004729
Iteration 256/1000 | Loss: 0.00004729
Iteration 257/1000 | Loss: 0.00004729
Iteration 258/1000 | Loss: 0.00004729
Iteration 259/1000 | Loss: 0.00004729
Iteration 260/1000 | Loss: 0.00004728
Iteration 261/1000 | Loss: 0.00004727
Iteration 262/1000 | Loss: 0.00004719
Iteration 263/1000 | Loss: 0.00004716
Iteration 264/1000 | Loss: 0.00004716
Iteration 265/1000 | Loss: 0.00004715
Iteration 266/1000 | Loss: 0.00004715
Iteration 267/1000 | Loss: 0.00004714
Iteration 268/1000 | Loss: 0.00004709
Iteration 269/1000 | Loss: 0.00004707
Iteration 270/1000 | Loss: 0.00004707
Iteration 271/1000 | Loss: 0.00004706
Iteration 272/1000 | Loss: 0.00004706
Iteration 273/1000 | Loss: 0.00004705
Iteration 274/1000 | Loss: 0.00004705
Iteration 275/1000 | Loss: 0.00004704
Iteration 276/1000 | Loss: 0.00004702
Iteration 277/1000 | Loss: 0.00004702
Iteration 278/1000 | Loss: 0.00004701
Iteration 279/1000 | Loss: 0.00004700
Iteration 280/1000 | Loss: 0.00004699
Iteration 281/1000 | Loss: 0.00004697
Iteration 282/1000 | Loss: 0.00004695
Iteration 283/1000 | Loss: 0.00004694
Iteration 284/1000 | Loss: 0.00004693
Iteration 285/1000 | Loss: 0.00009146
Iteration 286/1000 | Loss: 0.00007961
Iteration 287/1000 | Loss: 0.00008882
Iteration 288/1000 | Loss: 0.00005249
Iteration 289/1000 | Loss: 0.00004869
Iteration 290/1000 | Loss: 0.00005407
Iteration 291/1000 | Loss: 0.00006126
Iteration 292/1000 | Loss: 0.00004729
Iteration 293/1000 | Loss: 0.00004688
Iteration 294/1000 | Loss: 0.00004665
Iteration 295/1000 | Loss: 0.00004664
Iteration 296/1000 | Loss: 0.00004660
Iteration 297/1000 | Loss: 0.00004651
Iteration 298/1000 | Loss: 0.00004650
Iteration 299/1000 | Loss: 0.00004650
Iteration 300/1000 | Loss: 0.00004649
Iteration 301/1000 | Loss: 0.00004649
Iteration 302/1000 | Loss: 0.00004648
Iteration 303/1000 | Loss: 0.00004647
Iteration 304/1000 | Loss: 0.00004646
Iteration 305/1000 | Loss: 0.00004646
Iteration 306/1000 | Loss: 0.00004645
Iteration 307/1000 | Loss: 0.00004645
Iteration 308/1000 | Loss: 0.00004644
Iteration 309/1000 | Loss: 0.00004644
Iteration 310/1000 | Loss: 0.00004643
Iteration 311/1000 | Loss: 0.00004642
Iteration 312/1000 | Loss: 0.00004640
Iteration 313/1000 | Loss: 0.00004640
Iteration 314/1000 | Loss: 0.00004639
Iteration 315/1000 | Loss: 0.00004634
Iteration 316/1000 | Loss: 0.00004634
Iteration 317/1000 | Loss: 0.00004633
Iteration 318/1000 | Loss: 0.00004633
Iteration 319/1000 | Loss: 0.00004632
Iteration 320/1000 | Loss: 0.00004632
Iteration 321/1000 | Loss: 0.00004631
Iteration 322/1000 | Loss: 0.00004627
Iteration 323/1000 | Loss: 0.00010168
Iteration 324/1000 | Loss: 0.00008844
Iteration 325/1000 | Loss: 0.00005011
Iteration 326/1000 | Loss: 0.00004854
Iteration 327/1000 | Loss: 0.00119824
Iteration 328/1000 | Loss: 0.00098686
Iteration 329/1000 | Loss: 0.00048906
Iteration 330/1000 | Loss: 0.00015707
Iteration 331/1000 | Loss: 0.00005741
Iteration 332/1000 | Loss: 0.00004970
Iteration 333/1000 | Loss: 0.00004708
Iteration 334/1000 | Loss: 0.00005393
Iteration 335/1000 | Loss: 0.00004602
Iteration 336/1000 | Loss: 0.00004540
Iteration 337/1000 | Loss: 0.00004495
Iteration 338/1000 | Loss: 0.00004458
Iteration 339/1000 | Loss: 0.00007844
Iteration 340/1000 | Loss: 0.00004686
Iteration 341/1000 | Loss: 0.00007981
Iteration 342/1000 | Loss: 0.00004442
Iteration 343/1000 | Loss: 0.00004472
Iteration 344/1000 | Loss: 0.00004432
Iteration 345/1000 | Loss: 0.00004397
Iteration 346/1000 | Loss: 0.00004371
Iteration 347/1000 | Loss: 0.00004356
Iteration 348/1000 | Loss: 0.00004352
Iteration 349/1000 | Loss: 0.00007890
Iteration 350/1000 | Loss: 0.00004477
Iteration 351/1000 | Loss: 0.00004367
Iteration 352/1000 | Loss: 0.00004349
Iteration 353/1000 | Loss: 0.00006110
Iteration 354/1000 | Loss: 0.00004437
Iteration 355/1000 | Loss: 0.00005143
Iteration 356/1000 | Loss: 0.00004426
Iteration 357/1000 | Loss: 0.00005567
Iteration 358/1000 | Loss: 0.00004486
Iteration 359/1000 | Loss: 0.00004342
Iteration 360/1000 | Loss: 0.00004344
Iteration 361/1000 | Loss: 0.00005356
Iteration 362/1000 | Loss: 0.00004464
Iteration 363/1000 | Loss: 0.00004345
Iteration 364/1000 | Loss: 0.00004345
Iteration 365/1000 | Loss: 0.00004345
Iteration 366/1000 | Loss: 0.00004345
Iteration 367/1000 | Loss: 0.00004345
Iteration 368/1000 | Loss: 0.00004345
Iteration 369/1000 | Loss: 0.00004344
Iteration 370/1000 | Loss: 0.00004344
Iteration 371/1000 | Loss: 0.00004344
Iteration 372/1000 | Loss: 0.00004344
Iteration 373/1000 | Loss: 0.00004344
Iteration 374/1000 | Loss: 0.00004651
Iteration 375/1000 | Loss: 0.00004662
Iteration 376/1000 | Loss: 0.00004361
Iteration 377/1000 | Loss: 0.00004337
Iteration 378/1000 | Loss: 0.00004336
Iteration 379/1000 | Loss: 0.00004335
Iteration 380/1000 | Loss: 0.00004334
Iteration 381/1000 | Loss: 0.00004334
Iteration 382/1000 | Loss: 0.00004334
Iteration 383/1000 | Loss: 0.00004334
Iteration 384/1000 | Loss: 0.00004334
Iteration 385/1000 | Loss: 0.00004334
Iteration 386/1000 | Loss: 0.00004334
Iteration 387/1000 | Loss: 0.00004334
Iteration 388/1000 | Loss: 0.00004334
Iteration 389/1000 | Loss: 0.00004334
Iteration 390/1000 | Loss: 0.00004334
Iteration 391/1000 | Loss: 0.00004334
Iteration 392/1000 | Loss: 0.00004333
Iteration 393/1000 | Loss: 0.00004333
Iteration 394/1000 | Loss: 0.00004333
Iteration 395/1000 | Loss: 0.00004333
Iteration 396/1000 | Loss: 0.00004333
Iteration 397/1000 | Loss: 0.00004333
Iteration 398/1000 | Loss: 0.00004333
Iteration 399/1000 | Loss: 0.00004333
Iteration 400/1000 | Loss: 0.00004333
Iteration 401/1000 | Loss: 0.00004333
Iteration 402/1000 | Loss: 0.00004333
Iteration 403/1000 | Loss: 0.00004333
Iteration 404/1000 | Loss: 0.00004333
Iteration 405/1000 | Loss: 0.00004332
Iteration 406/1000 | Loss: 0.00004332
Iteration 407/1000 | Loss: 0.00004332
Iteration 408/1000 | Loss: 0.00004332
Iteration 409/1000 | Loss: 0.00004332
Iteration 410/1000 | Loss: 0.00004332
Iteration 411/1000 | Loss: 0.00004332
Iteration 412/1000 | Loss: 0.00004332
Iteration 413/1000 | Loss: 0.00004331
Iteration 414/1000 | Loss: 0.00004331
Iteration 415/1000 | Loss: 0.00004331
Iteration 416/1000 | Loss: 0.00004331
Iteration 417/1000 | Loss: 0.00004331
Iteration 418/1000 | Loss: 0.00004330
Iteration 419/1000 | Loss: 0.00004330
Iteration 420/1000 | Loss: 0.00004328
Iteration 421/1000 | Loss: 0.00004328
Iteration 422/1000 | Loss: 0.00004327
Iteration 423/1000 | Loss: 0.00004327
Iteration 424/1000 | Loss: 0.00004321
Iteration 425/1000 | Loss: 0.00004321
Iteration 426/1000 | Loss: 0.00004320
Iteration 427/1000 | Loss: 0.00004318
Iteration 428/1000 | Loss: 0.00004315
Iteration 429/1000 | Loss: 0.00004314
Iteration 430/1000 | Loss: 0.00004314
Iteration 431/1000 | Loss: 0.00004313
Iteration 432/1000 | Loss: 0.00004313
Iteration 433/1000 | Loss: 0.00004313
Iteration 434/1000 | Loss: 0.00004312
Iteration 435/1000 | Loss: 0.00004312
Iteration 436/1000 | Loss: 0.00004312
Iteration 437/1000 | Loss: 0.00004312
Iteration 438/1000 | Loss: 0.00004311
Iteration 439/1000 | Loss: 0.00004311
Iteration 440/1000 | Loss: 0.00004311
Iteration 441/1000 | Loss: 0.00004311
Iteration 442/1000 | Loss: 0.00004312
Iteration 443/1000 | Loss: 0.00004312
Iteration 444/1000 | Loss: 0.00004312
Iteration 445/1000 | Loss: 0.00004312
Iteration 446/1000 | Loss: 0.00004312
Iteration 447/1000 | Loss: 0.00004312
Iteration 448/1000 | Loss: 0.00004312
Iteration 449/1000 | Loss: 0.00004312
Iteration 450/1000 | Loss: 0.00004312
Iteration 451/1000 | Loss: 0.00004311
Iteration 452/1000 | Loss: 0.00004311
Iteration 453/1000 | Loss: 0.00004311
Iteration 454/1000 | Loss: 0.00004311
Iteration 455/1000 | Loss: 0.00004311
Iteration 456/1000 | Loss: 0.00004311
Iteration 457/1000 | Loss: 0.00004311
Iteration 458/1000 | Loss: 0.00004310
Iteration 459/1000 | Loss: 0.00004310
Iteration 460/1000 | Loss: 0.00004310
Iteration 461/1000 | Loss: 0.00004310
Iteration 462/1000 | Loss: 0.00004310
Iteration 463/1000 | Loss: 0.00004310
Iteration 464/1000 | Loss: 0.00004310
Iteration 465/1000 | Loss: 0.00004310
Iteration 466/1000 | Loss: 0.00004310
Iteration 467/1000 | Loss: 0.00004310
Iteration 468/1000 | Loss: 0.00004310
Iteration 469/1000 | Loss: 0.00004310
Iteration 470/1000 | Loss: 0.00004310
Iteration 471/1000 | Loss: 0.00004310
Iteration 472/1000 | Loss: 0.00004310
Iteration 473/1000 | Loss: 0.00004310
Iteration 474/1000 | Loss: 0.00004310
Iteration 475/1000 | Loss: 0.00004309
Iteration 476/1000 | Loss: 0.00004309
Iteration 477/1000 | Loss: 0.00004309
Iteration 478/1000 | Loss: 0.00004309
Iteration 479/1000 | Loss: 0.00004309
Iteration 480/1000 | Loss: 0.00004309
Iteration 481/1000 | Loss: 0.00004308
Iteration 482/1000 | Loss: 0.00004308
Iteration 483/1000 | Loss: 0.00004308
Iteration 484/1000 | Loss: 0.00004308
Iteration 485/1000 | Loss: 0.00004308
Iteration 486/1000 | Loss: 0.00004308
Iteration 487/1000 | Loss: 0.00004307
Iteration 488/1000 | Loss: 0.00004307
Iteration 489/1000 | Loss: 0.00004307
Iteration 490/1000 | Loss: 0.00004307
Iteration 491/1000 | Loss: 0.00004306
Iteration 492/1000 | Loss: 0.00004306
Iteration 493/1000 | Loss: 0.00004306
Iteration 494/1000 | Loss: 0.00004306
Iteration 495/1000 | Loss: 0.00004306
Iteration 496/1000 | Loss: 0.00004306
Iteration 497/1000 | Loss: 0.00004305
Iteration 498/1000 | Loss: 0.00004305
Iteration 499/1000 | Loss: 0.00004305
Iteration 500/1000 | Loss: 0.00004305
Iteration 501/1000 | Loss: 0.00004305
Iteration 502/1000 | Loss: 0.00004305
Iteration 503/1000 | Loss: 0.00004305
Iteration 504/1000 | Loss: 0.00004305
Iteration 505/1000 | Loss: 0.00004305
Iteration 506/1000 | Loss: 0.00004305
Iteration 507/1000 | Loss: 0.00004304
Iteration 508/1000 | Loss: 0.00004304
Iteration 509/1000 | Loss: 0.00004304
Iteration 510/1000 | Loss: 0.00004304
Iteration 511/1000 | Loss: 0.00004304
Iteration 512/1000 | Loss: 0.00004300
Iteration 513/1000 | Loss: 0.00004300
Iteration 514/1000 | Loss: 0.00004300
Iteration 515/1000 | Loss: 0.00004299
Iteration 516/1000 | Loss: 0.00004299
Iteration 517/1000 | Loss: 0.00004299
Iteration 518/1000 | Loss: 0.00004299
Iteration 519/1000 | Loss: 0.00004299
Iteration 520/1000 | Loss: 0.00004299
Iteration 521/1000 | Loss: 0.00004298
Iteration 522/1000 | Loss: 0.00004298
Iteration 523/1000 | Loss: 0.00004298
Iteration 524/1000 | Loss: 0.00004298
Iteration 525/1000 | Loss: 0.00004298
Iteration 526/1000 | Loss: 0.00004298
Iteration 527/1000 | Loss: 0.00004298
Iteration 528/1000 | Loss: 0.00004297
Iteration 529/1000 | Loss: 0.00004297
Iteration 530/1000 | Loss: 0.00004297
Iteration 531/1000 | Loss: 0.00004296
Iteration 532/1000 | Loss: 0.00004296
Iteration 533/1000 | Loss: 0.00004296
Iteration 534/1000 | Loss: 0.00004296
Iteration 535/1000 | Loss: 0.00004296
Iteration 536/1000 | Loss: 0.00004296
Iteration 537/1000 | Loss: 0.00004296
Iteration 538/1000 | Loss: 0.00004296
Iteration 539/1000 | Loss: 0.00004295
Iteration 540/1000 | Loss: 0.00004295
Iteration 541/1000 | Loss: 0.00004295
Iteration 542/1000 | Loss: 0.00004295
Iteration 543/1000 | Loss: 0.00004294
Iteration 544/1000 | Loss: 0.00004294
Iteration 545/1000 | Loss: 0.00004293
Iteration 546/1000 | Loss: 0.00004293
Iteration 547/1000 | Loss: 0.00004292
Iteration 548/1000 | Loss: 0.00004292
Iteration 549/1000 | Loss: 0.00004292
Iteration 550/1000 | Loss: 0.00004291
Iteration 551/1000 | Loss: 0.00004291
Iteration 552/1000 | Loss: 0.00004291
Iteration 553/1000 | Loss: 0.00004290
Iteration 554/1000 | Loss: 0.00004290
Iteration 555/1000 | Loss: 0.00004290
Iteration 556/1000 | Loss: 0.00004289
Iteration 557/1000 | Loss: 0.00004289
Iteration 558/1000 | Loss: 0.00004292
Iteration 559/1000 | Loss: 0.00004291
Iteration 560/1000 | Loss: 0.00004291
Iteration 561/1000 | Loss: 0.00004291
Iteration 562/1000 | Loss: 0.00004290
Iteration 563/1000 | Loss: 0.00004290
Iteration 564/1000 | Loss: 0.00004290
Iteration 565/1000 | Loss: 0.00004289
Iteration 566/1000 | Loss: 0.00004289
Iteration 567/1000 | Loss: 0.00004289
Iteration 568/1000 | Loss: 0.00004288
Iteration 569/1000 | Loss: 0.00004288
Iteration 570/1000 | Loss: 0.00004288
Iteration 571/1000 | Loss: 0.00004288
Iteration 572/1000 | Loss: 0.00004288
Iteration 573/1000 | Loss: 0.00004288
Iteration 574/1000 | Loss: 0.00004288
Iteration 575/1000 | Loss: 0.00004288
Iteration 576/1000 | Loss: 0.00004288
Iteration 577/1000 | Loss: 0.00004287
Iteration 578/1000 | Loss: 0.00004287
Iteration 579/1000 | Loss: 0.00004287
Iteration 580/1000 | Loss: 0.00004287
Iteration 581/1000 | Loss: 0.00004287
Iteration 582/1000 | Loss: 0.00004287
Iteration 583/1000 | Loss: 0.00004286
Iteration 584/1000 | Loss: 0.00004286
Iteration 585/1000 | Loss: 0.00004286
Iteration 586/1000 | Loss: 0.00004286
Iteration 587/1000 | Loss: 0.00004286
Iteration 588/1000 | Loss: 0.00004286
Iteration 589/1000 | Loss: 0.00004286
Iteration 590/1000 | Loss: 0.00004286
Iteration 591/1000 | Loss: 0.00004286
Iteration 592/1000 | Loss: 0.00004286
Iteration 593/1000 | Loss: 0.00004286
Iteration 594/1000 | Loss: 0.00004286
Iteration 595/1000 | Loss: 0.00004286
Iteration 596/1000 | Loss: 0.00004286
Iteration 597/1000 | Loss: 0.00004286
Iteration 598/1000 | Loss: 0.00004285
Iteration 599/1000 | Loss: 0.00004285
Iteration 600/1000 | Loss: 0.00004285
Iteration 601/1000 | Loss: 0.00004285
Iteration 602/1000 | Loss: 0.00004285
Iteration 603/1000 | Loss: 0.00004285
Iteration 604/1000 | Loss: 0.00004285
Iteration 605/1000 | Loss: 0.00004285
Iteration 606/1000 | Loss: 0.00004285
Iteration 607/1000 | Loss: 0.00004285
Iteration 608/1000 | Loss: 0.00004285
Iteration 609/1000 | Loss: 0.00004285
Iteration 610/1000 | Loss: 0.00004285
Iteration 611/1000 | Loss: 0.00004284
Iteration 612/1000 | Loss: 0.00004284
Iteration 613/1000 | Loss: 0.00004284
Iteration 614/1000 | Loss: 0.00004284
Iteration 615/1000 | Loss: 0.00004284
Iteration 616/1000 | Loss: 0.00004284
Iteration 617/1000 | Loss: 0.00004284
Iteration 618/1000 | Loss: 0.00004283
Iteration 619/1000 | Loss: 0.00004283
Iteration 620/1000 | Loss: 0.00004283
Iteration 621/1000 | Loss: 0.00004283
Iteration 622/1000 | Loss: 0.00004283
Iteration 623/1000 | Loss: 0.00004283
Iteration 624/1000 | Loss: 0.00004283
Iteration 625/1000 | Loss: 0.00004283
Iteration 626/1000 | Loss: 0.00004283
Iteration 627/1000 | Loss: 0.00004283
Iteration 628/1000 | Loss: 0.00004283
Iteration 629/1000 | Loss: 0.00004283
Iteration 630/1000 | Loss: 0.00004282
Iteration 631/1000 | Loss: 0.00004326
Iteration 632/1000 | Loss: 0.00004326
Iteration 633/1000 | Loss: 0.00004326
Iteration 634/1000 | Loss: 0.00004326
Iteration 635/1000 | Loss: 0.00004325
Iteration 636/1000 | Loss: 0.00004325
Iteration 637/1000 | Loss: 0.00004325
Iteration 638/1000 | Loss: 0.00004325
Iteration 639/1000 | Loss: 0.00004324
Iteration 640/1000 | Loss: 0.00004324
Iteration 641/1000 | Loss: 0.00004323
Iteration 642/1000 | Loss: 0.00004323
Iteration 643/1000 | Loss: 0.00004323
Iteration 644/1000 | Loss: 0.00004322
Iteration 645/1000 | Loss: 0.00004322
Iteration 646/1000 | Loss: 0.00004322
Iteration 647/1000 | Loss: 0.00004322
Iteration 648/1000 | Loss: 0.00004321
Iteration 649/1000 | Loss: 0.00004321
Iteration 650/1000 | Loss: 0.00004318
Iteration 651/1000 | Loss: 0.00004317
Iteration 652/1000 | Loss: 0.00004317
Iteration 653/1000 | Loss: 0.00004316
Iteration 654/1000 | Loss: 0.00004316
Iteration 655/1000 | Loss: 0.00004316
Iteration 656/1000 | Loss: 0.00004315
Iteration 657/1000 | Loss: 0.00004312
Iteration 658/1000 | Loss: 0.00004311
Iteration 659/1000 | Loss: 0.00004310
Iteration 660/1000 | Loss: 0.00004309
Iteration 661/1000 | Loss: 0.00004364
Iteration 662/1000 | Loss: 0.00004363
Iteration 663/1000 | Loss: 0.00004348
Iteration 664/1000 | Loss: 0.00004342
Iteration 665/1000 | Loss: 0.00004341
Iteration 666/1000 | Loss: 0.00004341
Iteration 667/1000 | Loss: 0.00004339
Iteration 668/1000 | Loss: 0.00004339
Iteration 669/1000 | Loss: 0.00004338
Iteration 670/1000 | Loss: 0.00004338
Iteration 671/1000 | Loss: 0.00004338
Iteration 672/1000 | Loss: 0.00004337
Iteration 673/1000 | Loss: 0.00004317
Iteration 674/1000 | Loss: 0.00004317
Iteration 675/1000 | Loss: 0.00004347
Iteration 676/1000 | Loss: 0.00004339
Iteration 677/1000 | Loss: 0.00004338
Iteration 678/1000 | Loss: 0.00004336
Iteration 679/1000 | Loss: 0.00004336
Iteration 680/1000 | Loss: 0.00004336
Iteration 681/1000 | Loss: 0.00004336
Iteration 682/1000 | Loss: 0.00004336
Iteration 683/1000 | Loss: 0.00004336
Iteration 684/1000 | Loss: 0.00004336
Iteration 685/1000 | Loss: 0.00004336
Iteration 686/1000 | Loss: 0.00004336
Iteration 687/1000 | Loss: 0.00004335
Iteration 688/1000 | Loss: 0.00004335
Iteration 689/1000 | Loss: 0.00004335
Iteration 690/1000 | Loss: 0.00004335
Iteration 691/1000 | Loss: 0.00004334
Iteration 692/1000 | Loss: 0.00004334
Iteration 693/1000 | Loss: 0.00004334
Iteration 694/1000 | Loss: 0.00004334
Iteration 695/1000 | Loss: 0.00004334
Iteration 696/1000 | Loss: 0.00004334
Iteration 697/1000 | Loss: 0.00004334
Iteration 698/1000 | Loss: 0.00004334
Iteration 699/1000 | Loss: 0.00004333
Iteration 700/1000 | Loss: 0.00004333
Iteration 701/1000 | Loss: 0.00004333
Iteration 702/1000 | Loss: 0.00004333
Iteration 703/1000 | Loss: 0.00004333
Iteration 704/1000 | Loss: 0.00004333
Iteration 705/1000 | Loss: 0.00004333
Iteration 706/1000 | Loss: 0.00004333
Iteration 707/1000 | Loss: 0.00004333
Iteration 708/1000 | Loss: 0.00004333
Iteration 709/1000 | Loss: 0.00004333
Iteration 710/1000 | Loss: 0.00004333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 710. Stopping optimization.
Last 5 losses: [4.333282049628906e-05, 4.333282049628906e-05, 4.333282049628906e-05, 4.333282049628906e-05, 4.333282049628906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.333282049628906e-05

Optimization complete. Final v2v error: 4.91654634475708 mm

Highest mean error: 20.344722747802734 mm for frame 39

Lowest mean error: 3.7381701469421387 mm for frame 21

Saving results

Total time: 543.4096200466156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762979
Iteration 2/25 | Loss: 0.00175154
Iteration 3/25 | Loss: 0.00146253
Iteration 4/25 | Loss: 0.00139583
Iteration 5/25 | Loss: 0.00138333
Iteration 6/25 | Loss: 0.00138027
Iteration 7/25 | Loss: 0.00138027
Iteration 8/25 | Loss: 0.00138027
Iteration 9/25 | Loss: 0.00138027
Iteration 10/25 | Loss: 0.00138027
Iteration 11/25 | Loss: 0.00138027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013802689500153065, 0.0013802689500153065, 0.0013802689500153065, 0.0013802689500153065, 0.0013802689500153065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013802689500153065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24177861
Iteration 2/25 | Loss: 0.00082273
Iteration 3/25 | Loss: 0.00082271
Iteration 4/25 | Loss: 0.00082271
Iteration 5/25 | Loss: 0.00082271
Iteration 6/25 | Loss: 0.00082271
Iteration 7/25 | Loss: 0.00082271
Iteration 8/25 | Loss: 0.00082271
Iteration 9/25 | Loss: 0.00082271
Iteration 10/25 | Loss: 0.00082271
Iteration 11/25 | Loss: 0.00082271
Iteration 12/25 | Loss: 0.00082271
Iteration 13/25 | Loss: 0.00082271
Iteration 14/25 | Loss: 0.00082271
Iteration 15/25 | Loss: 0.00082271
Iteration 16/25 | Loss: 0.00082271
Iteration 17/25 | Loss: 0.00082271
Iteration 18/25 | Loss: 0.00082271
Iteration 19/25 | Loss: 0.00082271
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008227097569033504, 0.0008227097569033504, 0.0008227097569033504, 0.0008227097569033504, 0.0008227097569033504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008227097569033504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082271
Iteration 2/1000 | Loss: 0.00008218
Iteration 3/1000 | Loss: 0.00005993
Iteration 4/1000 | Loss: 0.00004996
Iteration 5/1000 | Loss: 0.00004704
Iteration 6/1000 | Loss: 0.00004508
Iteration 7/1000 | Loss: 0.00004387
Iteration 8/1000 | Loss: 0.00004293
Iteration 9/1000 | Loss: 0.00004205
Iteration 10/1000 | Loss: 0.00004147
Iteration 11/1000 | Loss: 0.00004092
Iteration 12/1000 | Loss: 0.00004051
Iteration 13/1000 | Loss: 0.00004020
Iteration 14/1000 | Loss: 0.00003993
Iteration 15/1000 | Loss: 0.00003968
Iteration 16/1000 | Loss: 0.00003950
Iteration 17/1000 | Loss: 0.00003938
Iteration 18/1000 | Loss: 0.00003937
Iteration 19/1000 | Loss: 0.00003926
Iteration 20/1000 | Loss: 0.00003923
Iteration 21/1000 | Loss: 0.00003916
Iteration 22/1000 | Loss: 0.00003910
Iteration 23/1000 | Loss: 0.00003910
Iteration 24/1000 | Loss: 0.00003907
Iteration 25/1000 | Loss: 0.00003906
Iteration 26/1000 | Loss: 0.00003906
Iteration 27/1000 | Loss: 0.00003906
Iteration 28/1000 | Loss: 0.00003905
Iteration 29/1000 | Loss: 0.00003905
Iteration 30/1000 | Loss: 0.00003904
Iteration 31/1000 | Loss: 0.00003904
Iteration 32/1000 | Loss: 0.00003903
Iteration 33/1000 | Loss: 0.00003903
Iteration 34/1000 | Loss: 0.00003902
Iteration 35/1000 | Loss: 0.00003902
Iteration 36/1000 | Loss: 0.00003901
Iteration 37/1000 | Loss: 0.00003900
Iteration 38/1000 | Loss: 0.00003899
Iteration 39/1000 | Loss: 0.00003899
Iteration 40/1000 | Loss: 0.00003898
Iteration 41/1000 | Loss: 0.00003898
Iteration 42/1000 | Loss: 0.00003897
Iteration 43/1000 | Loss: 0.00003897
Iteration 44/1000 | Loss: 0.00003896
Iteration 45/1000 | Loss: 0.00003896
Iteration 46/1000 | Loss: 0.00003895
Iteration 47/1000 | Loss: 0.00003894
Iteration 48/1000 | Loss: 0.00003894
Iteration 49/1000 | Loss: 0.00003894
Iteration 50/1000 | Loss: 0.00003894
Iteration 51/1000 | Loss: 0.00003894
Iteration 52/1000 | Loss: 0.00003894
Iteration 53/1000 | Loss: 0.00003894
Iteration 54/1000 | Loss: 0.00003894
Iteration 55/1000 | Loss: 0.00003893
Iteration 56/1000 | Loss: 0.00003893
Iteration 57/1000 | Loss: 0.00003892
Iteration 58/1000 | Loss: 0.00003892
Iteration 59/1000 | Loss: 0.00003891
Iteration 60/1000 | Loss: 0.00003891
Iteration 61/1000 | Loss: 0.00003891
Iteration 62/1000 | Loss: 0.00003891
Iteration 63/1000 | Loss: 0.00003891
Iteration 64/1000 | Loss: 0.00003890
Iteration 65/1000 | Loss: 0.00003890
Iteration 66/1000 | Loss: 0.00003890
Iteration 67/1000 | Loss: 0.00003890
Iteration 68/1000 | Loss: 0.00003890
Iteration 69/1000 | Loss: 0.00003890
Iteration 70/1000 | Loss: 0.00003890
Iteration 71/1000 | Loss: 0.00003890
Iteration 72/1000 | Loss: 0.00003890
Iteration 73/1000 | Loss: 0.00003890
Iteration 74/1000 | Loss: 0.00003889
Iteration 75/1000 | Loss: 0.00003889
Iteration 76/1000 | Loss: 0.00003889
Iteration 77/1000 | Loss: 0.00003888
Iteration 78/1000 | Loss: 0.00003888
Iteration 79/1000 | Loss: 0.00003888
Iteration 80/1000 | Loss: 0.00003887
Iteration 81/1000 | Loss: 0.00003886
Iteration 82/1000 | Loss: 0.00003886
Iteration 83/1000 | Loss: 0.00003886
Iteration 84/1000 | Loss: 0.00003886
Iteration 85/1000 | Loss: 0.00003886
Iteration 86/1000 | Loss: 0.00003886
Iteration 87/1000 | Loss: 0.00003885
Iteration 88/1000 | Loss: 0.00003885
Iteration 89/1000 | Loss: 0.00003885
Iteration 90/1000 | Loss: 0.00003885
Iteration 91/1000 | Loss: 0.00003885
Iteration 92/1000 | Loss: 0.00003884
Iteration 93/1000 | Loss: 0.00003884
Iteration 94/1000 | Loss: 0.00003884
Iteration 95/1000 | Loss: 0.00003884
Iteration 96/1000 | Loss: 0.00003884
Iteration 97/1000 | Loss: 0.00003883
Iteration 98/1000 | Loss: 0.00003883
Iteration 99/1000 | Loss: 0.00003883
Iteration 100/1000 | Loss: 0.00003883
Iteration 101/1000 | Loss: 0.00003883
Iteration 102/1000 | Loss: 0.00003883
Iteration 103/1000 | Loss: 0.00003882
Iteration 104/1000 | Loss: 0.00003882
Iteration 105/1000 | Loss: 0.00003882
Iteration 106/1000 | Loss: 0.00003882
Iteration 107/1000 | Loss: 0.00003882
Iteration 108/1000 | Loss: 0.00003881
Iteration 109/1000 | Loss: 0.00003881
Iteration 110/1000 | Loss: 0.00003881
Iteration 111/1000 | Loss: 0.00003881
Iteration 112/1000 | Loss: 0.00003881
Iteration 113/1000 | Loss: 0.00003881
Iteration 114/1000 | Loss: 0.00003881
Iteration 115/1000 | Loss: 0.00003880
Iteration 116/1000 | Loss: 0.00003880
Iteration 117/1000 | Loss: 0.00003880
Iteration 118/1000 | Loss: 0.00003880
Iteration 119/1000 | Loss: 0.00003879
Iteration 120/1000 | Loss: 0.00003879
Iteration 121/1000 | Loss: 0.00003879
Iteration 122/1000 | Loss: 0.00003879
Iteration 123/1000 | Loss: 0.00003879
Iteration 124/1000 | Loss: 0.00003878
Iteration 125/1000 | Loss: 0.00003878
Iteration 126/1000 | Loss: 0.00003878
Iteration 127/1000 | Loss: 0.00003878
Iteration 128/1000 | Loss: 0.00003878
Iteration 129/1000 | Loss: 0.00003877
Iteration 130/1000 | Loss: 0.00003877
Iteration 131/1000 | Loss: 0.00003877
Iteration 132/1000 | Loss: 0.00003877
Iteration 133/1000 | Loss: 0.00003877
Iteration 134/1000 | Loss: 0.00003876
Iteration 135/1000 | Loss: 0.00003876
Iteration 136/1000 | Loss: 0.00003876
Iteration 137/1000 | Loss: 0.00003876
Iteration 138/1000 | Loss: 0.00003876
Iteration 139/1000 | Loss: 0.00003875
Iteration 140/1000 | Loss: 0.00003875
Iteration 141/1000 | Loss: 0.00003875
Iteration 142/1000 | Loss: 0.00003875
Iteration 143/1000 | Loss: 0.00003875
Iteration 144/1000 | Loss: 0.00003875
Iteration 145/1000 | Loss: 0.00003875
Iteration 146/1000 | Loss: 0.00003875
Iteration 147/1000 | Loss: 0.00003874
Iteration 148/1000 | Loss: 0.00003874
Iteration 149/1000 | Loss: 0.00003874
Iteration 150/1000 | Loss: 0.00003874
Iteration 151/1000 | Loss: 0.00003874
Iteration 152/1000 | Loss: 0.00003874
Iteration 153/1000 | Loss: 0.00003874
Iteration 154/1000 | Loss: 0.00003874
Iteration 155/1000 | Loss: 0.00003874
Iteration 156/1000 | Loss: 0.00003874
Iteration 157/1000 | Loss: 0.00003873
Iteration 158/1000 | Loss: 0.00003873
Iteration 159/1000 | Loss: 0.00003873
Iteration 160/1000 | Loss: 0.00003873
Iteration 161/1000 | Loss: 0.00003873
Iteration 162/1000 | Loss: 0.00003873
Iteration 163/1000 | Loss: 0.00003873
Iteration 164/1000 | Loss: 0.00003873
Iteration 165/1000 | Loss: 0.00003873
Iteration 166/1000 | Loss: 0.00003873
Iteration 167/1000 | Loss: 0.00003872
Iteration 168/1000 | Loss: 0.00003872
Iteration 169/1000 | Loss: 0.00003872
Iteration 170/1000 | Loss: 0.00003872
Iteration 171/1000 | Loss: 0.00003872
Iteration 172/1000 | Loss: 0.00003872
Iteration 173/1000 | Loss: 0.00003872
Iteration 174/1000 | Loss: 0.00003872
Iteration 175/1000 | Loss: 0.00003872
Iteration 176/1000 | Loss: 0.00003872
Iteration 177/1000 | Loss: 0.00003872
Iteration 178/1000 | Loss: 0.00003872
Iteration 179/1000 | Loss: 0.00003872
Iteration 180/1000 | Loss: 0.00003872
Iteration 181/1000 | Loss: 0.00003872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [3.871650915243663e-05, 3.871650915243663e-05, 3.871650915243663e-05, 3.871650915243663e-05, 3.871650915243663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.871650915243663e-05

Optimization complete. Final v2v error: 5.082736492156982 mm

Highest mean error: 6.408999919891357 mm for frame 75

Lowest mean error: 3.865875482559204 mm for frame 214

Saving results

Total time: 56.9119553565979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824072
Iteration 2/25 | Loss: 0.00130453
Iteration 3/25 | Loss: 0.00122535
Iteration 4/25 | Loss: 0.00121638
Iteration 5/25 | Loss: 0.00121363
Iteration 6/25 | Loss: 0.00121363
Iteration 7/25 | Loss: 0.00121363
Iteration 8/25 | Loss: 0.00121363
Iteration 9/25 | Loss: 0.00121363
Iteration 10/25 | Loss: 0.00121363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012136276345700026, 0.0012136276345700026, 0.0012136276345700026, 0.0012136276345700026, 0.0012136276345700026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012136276345700026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53810513
Iteration 2/25 | Loss: 0.00055510
Iteration 3/25 | Loss: 0.00055510
Iteration 4/25 | Loss: 0.00055510
Iteration 5/25 | Loss: 0.00055510
Iteration 6/25 | Loss: 0.00055510
Iteration 7/25 | Loss: 0.00055510
Iteration 8/25 | Loss: 0.00055510
Iteration 9/25 | Loss: 0.00055510
Iteration 10/25 | Loss: 0.00055510
Iteration 11/25 | Loss: 0.00055510
Iteration 12/25 | Loss: 0.00055510
Iteration 13/25 | Loss: 0.00055510
Iteration 14/25 | Loss: 0.00055510
Iteration 15/25 | Loss: 0.00055510
Iteration 16/25 | Loss: 0.00055510
Iteration 17/25 | Loss: 0.00055510
Iteration 18/25 | Loss: 0.00055510
Iteration 19/25 | Loss: 0.00055510
Iteration 20/25 | Loss: 0.00055510
Iteration 21/25 | Loss: 0.00055510
Iteration 22/25 | Loss: 0.00055510
Iteration 23/25 | Loss: 0.00055510
Iteration 24/25 | Loss: 0.00055510
Iteration 25/25 | Loss: 0.00055510

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055510
Iteration 2/1000 | Loss: 0.00003042
Iteration 3/1000 | Loss: 0.00002239
Iteration 4/1000 | Loss: 0.00002042
Iteration 5/1000 | Loss: 0.00001953
Iteration 6/1000 | Loss: 0.00001894
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001807
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00001782
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001774
Iteration 19/1000 | Loss: 0.00001773
Iteration 20/1000 | Loss: 0.00001773
Iteration 21/1000 | Loss: 0.00001772
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001766
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001760
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001759
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001756
Iteration 34/1000 | Loss: 0.00001755
Iteration 35/1000 | Loss: 0.00001755
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001753
Iteration 38/1000 | Loss: 0.00001751
Iteration 39/1000 | Loss: 0.00001751
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001750
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001749
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001742
Iteration 48/1000 | Loss: 0.00001742
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001729
Iteration 58/1000 | Loss: 0.00001729
Iteration 59/1000 | Loss: 0.00001729
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001728
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001725
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001724
Iteration 69/1000 | Loss: 0.00001724
Iteration 70/1000 | Loss: 0.00001724
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001722
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001722
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001721
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001717
Iteration 103/1000 | Loss: 0.00001717
Iteration 104/1000 | Loss: 0.00001717
Iteration 105/1000 | Loss: 0.00001717
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Iteration 109/1000 | Loss: 0.00001716
Iteration 110/1000 | Loss: 0.00001716
Iteration 111/1000 | Loss: 0.00001716
Iteration 112/1000 | Loss: 0.00001715
Iteration 113/1000 | Loss: 0.00001715
Iteration 114/1000 | Loss: 0.00001715
Iteration 115/1000 | Loss: 0.00001715
Iteration 116/1000 | Loss: 0.00001715
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001715
Iteration 120/1000 | Loss: 0.00001715
Iteration 121/1000 | Loss: 0.00001715
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001714
Iteration 126/1000 | Loss: 0.00001714
Iteration 127/1000 | Loss: 0.00001714
Iteration 128/1000 | Loss: 0.00001714
Iteration 129/1000 | Loss: 0.00001714
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Iteration 132/1000 | Loss: 0.00001714
Iteration 133/1000 | Loss: 0.00001713
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001712
Iteration 138/1000 | Loss: 0.00001712
Iteration 139/1000 | Loss: 0.00001712
Iteration 140/1000 | Loss: 0.00001712
Iteration 141/1000 | Loss: 0.00001712
Iteration 142/1000 | Loss: 0.00001712
Iteration 143/1000 | Loss: 0.00001712
Iteration 144/1000 | Loss: 0.00001712
Iteration 145/1000 | Loss: 0.00001712
Iteration 146/1000 | Loss: 0.00001712
Iteration 147/1000 | Loss: 0.00001712
Iteration 148/1000 | Loss: 0.00001712
Iteration 149/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.7123484212788753e-05, 1.7123484212788753e-05, 1.7123484212788753e-05, 1.7123484212788753e-05, 1.7123484212788753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7123484212788753e-05

Optimization complete. Final v2v error: 3.4655046463012695 mm

Highest mean error: 3.865734338760376 mm for frame 176

Lowest mean error: 3.129641532897949 mm for frame 8

Saving results

Total time: 39.029526472091675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049881
Iteration 2/25 | Loss: 0.01049881
Iteration 3/25 | Loss: 0.01049880
Iteration 4/25 | Loss: 0.01049880
Iteration 5/25 | Loss: 0.01049880
Iteration 6/25 | Loss: 0.01049880
Iteration 7/25 | Loss: 0.01049880
Iteration 8/25 | Loss: 0.01049880
Iteration 9/25 | Loss: 0.01049880
Iteration 10/25 | Loss: 0.01049880
Iteration 11/25 | Loss: 0.01049880
Iteration 12/25 | Loss: 0.01049880
Iteration 13/25 | Loss: 0.01049880
Iteration 14/25 | Loss: 0.01049880
Iteration 15/25 | Loss: 0.01049880
Iteration 16/25 | Loss: 0.01049880
Iteration 17/25 | Loss: 0.01049880
Iteration 18/25 | Loss: 0.01049880
Iteration 19/25 | Loss: 0.01049880
Iteration 20/25 | Loss: 0.01049880
Iteration 21/25 | Loss: 0.01049880
Iteration 22/25 | Loss: 0.01049879
Iteration 23/25 | Loss: 0.01049879
Iteration 24/25 | Loss: 0.01049879
Iteration 25/25 | Loss: 0.01049879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55941403
Iteration 2/25 | Loss: 0.12759680
Iteration 3/25 | Loss: 0.12747024
Iteration 4/25 | Loss: 0.12725025
Iteration 5/25 | Loss: 0.12728764
Iteration 6/25 | Loss: 0.12727468
Iteration 7/25 | Loss: 0.12724385
Iteration 8/25 | Loss: 0.12724382
Iteration 9/25 | Loss: 0.12724380
Iteration 10/25 | Loss: 0.12724380
Iteration 11/25 | Loss: 0.12724380
Iteration 12/25 | Loss: 0.12724380
Iteration 13/25 | Loss: 0.12724380
Iteration 14/25 | Loss: 0.12724380
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.1272438019514084, 0.1272438019514084, 0.1272438019514084, 0.1272438019514084, 0.1272438019514084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1272438019514084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12724380
Iteration 2/1000 | Loss: 0.00075741
Iteration 3/1000 | Loss: 0.00023392
Iteration 4/1000 | Loss: 0.00022287
Iteration 5/1000 | Loss: 0.00017123
Iteration 6/1000 | Loss: 0.00055998
Iteration 7/1000 | Loss: 0.00030481
Iteration 8/1000 | Loss: 0.00003065
Iteration 9/1000 | Loss: 0.00002407
Iteration 10/1000 | Loss: 0.00003886
Iteration 11/1000 | Loss: 0.00002634
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00010128
Iteration 14/1000 | Loss: 0.00021729
Iteration 15/1000 | Loss: 0.00002002
Iteration 16/1000 | Loss: 0.00002230
Iteration 17/1000 | Loss: 0.00002942
Iteration 18/1000 | Loss: 0.00018110
Iteration 19/1000 | Loss: 0.00008553
Iteration 20/1000 | Loss: 0.00032244
Iteration 21/1000 | Loss: 0.00002639
Iteration 22/1000 | Loss: 0.00002786
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00005881
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001848
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00004488
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00004286
Iteration 31/1000 | Loss: 0.00005027
Iteration 32/1000 | Loss: 0.00003393
Iteration 33/1000 | Loss: 0.00006015
Iteration 34/1000 | Loss: 0.00021960
Iteration 35/1000 | Loss: 0.00003014
Iteration 36/1000 | Loss: 0.00001373
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001236
Iteration 39/1000 | Loss: 0.00005879
Iteration 40/1000 | Loss: 0.00048147
Iteration 41/1000 | Loss: 0.00055823
Iteration 42/1000 | Loss: 0.00003973
Iteration 43/1000 | Loss: 0.00002879
Iteration 44/1000 | Loss: 0.00009714
Iteration 45/1000 | Loss: 0.00002547
Iteration 46/1000 | Loss: 0.00002505
Iteration 47/1000 | Loss: 0.00004110
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001175
Iteration 52/1000 | Loss: 0.00001175
Iteration 53/1000 | Loss: 0.00001174
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00002387
Iteration 57/1000 | Loss: 0.00009340
Iteration 58/1000 | Loss: 0.00051860
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00018635
Iteration 62/1000 | Loss: 0.00001755
Iteration 63/1000 | Loss: 0.00002867
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001850
Iteration 66/1000 | Loss: 0.00010740
Iteration 67/1000 | Loss: 0.00001430
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00010323
Iteration 70/1000 | Loss: 0.00003732
Iteration 71/1000 | Loss: 0.00009628
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00006779
Iteration 74/1000 | Loss: 0.00002084
Iteration 75/1000 | Loss: 0.00002017
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001130
Iteration 81/1000 | Loss: 0.00001130
Iteration 82/1000 | Loss: 0.00001130
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001130
Iteration 86/1000 | Loss: 0.00001130
Iteration 87/1000 | Loss: 0.00001130
Iteration 88/1000 | Loss: 0.00001130
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.1299266589048784e-05, 1.1299266589048784e-05, 1.1299266589048784e-05, 1.1299266589048784e-05, 1.1299266589048784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1299266589048784e-05

Optimization complete. Final v2v error: 2.8653972148895264 mm

Highest mean error: 3.1097726821899414 mm for frame 200

Lowest mean error: 2.65928053855896 mm for frame 73

Saving results

Total time: 122.13563394546509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791012
Iteration 2/25 | Loss: 0.00147654
Iteration 3/25 | Loss: 0.00123608
Iteration 4/25 | Loss: 0.00119631
Iteration 5/25 | Loss: 0.00118645
Iteration 6/25 | Loss: 0.00118438
Iteration 7/25 | Loss: 0.00118367
Iteration 8/25 | Loss: 0.00118357
Iteration 9/25 | Loss: 0.00118357
Iteration 10/25 | Loss: 0.00118357
Iteration 11/25 | Loss: 0.00118357
Iteration 12/25 | Loss: 0.00118357
Iteration 13/25 | Loss: 0.00118357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011835730401799083, 0.0011835730401799083, 0.0011835730401799083, 0.0011835730401799083, 0.0011835730401799083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011835730401799083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42575967
Iteration 2/25 | Loss: 0.00048630
Iteration 3/25 | Loss: 0.00048630
Iteration 4/25 | Loss: 0.00048630
Iteration 5/25 | Loss: 0.00048630
Iteration 6/25 | Loss: 0.00048630
Iteration 7/25 | Loss: 0.00048630
Iteration 8/25 | Loss: 0.00048629
Iteration 9/25 | Loss: 0.00048629
Iteration 10/25 | Loss: 0.00048629
Iteration 11/25 | Loss: 0.00048629
Iteration 12/25 | Loss: 0.00048629
Iteration 13/25 | Loss: 0.00048629
Iteration 14/25 | Loss: 0.00048629
Iteration 15/25 | Loss: 0.00048629
Iteration 16/25 | Loss: 0.00048629
Iteration 17/25 | Loss: 0.00048629
Iteration 18/25 | Loss: 0.00048629
Iteration 19/25 | Loss: 0.00048629
Iteration 20/25 | Loss: 0.00048629
Iteration 21/25 | Loss: 0.00048629
Iteration 22/25 | Loss: 0.00048629
Iteration 23/25 | Loss: 0.00048629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004862939822487533, 0.0004862939822487533, 0.0004862939822487533, 0.0004862939822487533, 0.0004862939822487533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004862939822487533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048629
Iteration 2/1000 | Loss: 0.00004773
Iteration 3/1000 | Loss: 0.00003566
Iteration 4/1000 | Loss: 0.00002894
Iteration 5/1000 | Loss: 0.00002634
Iteration 6/1000 | Loss: 0.00002529
Iteration 7/1000 | Loss: 0.00002426
Iteration 8/1000 | Loss: 0.00002370
Iteration 9/1000 | Loss: 0.00002322
Iteration 10/1000 | Loss: 0.00002283
Iteration 11/1000 | Loss: 0.00002257
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002217
Iteration 14/1000 | Loss: 0.00002198
Iteration 15/1000 | Loss: 0.00002192
Iteration 16/1000 | Loss: 0.00002185
Iteration 17/1000 | Loss: 0.00002181
Iteration 18/1000 | Loss: 0.00002177
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002175
Iteration 21/1000 | Loss: 0.00002171
Iteration 22/1000 | Loss: 0.00002167
Iteration 23/1000 | Loss: 0.00002167
Iteration 24/1000 | Loss: 0.00002166
Iteration 25/1000 | Loss: 0.00002165
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002164
Iteration 29/1000 | Loss: 0.00002163
Iteration 30/1000 | Loss: 0.00002160
Iteration 31/1000 | Loss: 0.00002155
Iteration 32/1000 | Loss: 0.00002148
Iteration 33/1000 | Loss: 0.00002148
Iteration 34/1000 | Loss: 0.00002147
Iteration 35/1000 | Loss: 0.00002147
Iteration 36/1000 | Loss: 0.00002146
Iteration 37/1000 | Loss: 0.00002146
Iteration 38/1000 | Loss: 0.00002145
Iteration 39/1000 | Loss: 0.00002145
Iteration 40/1000 | Loss: 0.00002144
Iteration 41/1000 | Loss: 0.00002144
Iteration 42/1000 | Loss: 0.00002143
Iteration 43/1000 | Loss: 0.00002143
Iteration 44/1000 | Loss: 0.00002143
Iteration 45/1000 | Loss: 0.00002143
Iteration 46/1000 | Loss: 0.00002143
Iteration 47/1000 | Loss: 0.00002143
Iteration 48/1000 | Loss: 0.00002142
Iteration 49/1000 | Loss: 0.00002141
Iteration 50/1000 | Loss: 0.00002141
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002140
Iteration 53/1000 | Loss: 0.00002140
Iteration 54/1000 | Loss: 0.00002140
Iteration 55/1000 | Loss: 0.00002139
Iteration 56/1000 | Loss: 0.00002139
Iteration 57/1000 | Loss: 0.00002138
Iteration 58/1000 | Loss: 0.00002138
Iteration 59/1000 | Loss: 0.00002138
Iteration 60/1000 | Loss: 0.00002138
Iteration 61/1000 | Loss: 0.00002137
Iteration 62/1000 | Loss: 0.00002137
Iteration 63/1000 | Loss: 0.00002137
Iteration 64/1000 | Loss: 0.00002137
Iteration 65/1000 | Loss: 0.00002136
Iteration 66/1000 | Loss: 0.00002136
Iteration 67/1000 | Loss: 0.00002136
Iteration 68/1000 | Loss: 0.00002136
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002135
Iteration 73/1000 | Loss: 0.00002135
Iteration 74/1000 | Loss: 0.00002135
Iteration 75/1000 | Loss: 0.00002135
Iteration 76/1000 | Loss: 0.00002135
Iteration 77/1000 | Loss: 0.00002134
Iteration 78/1000 | Loss: 0.00002134
Iteration 79/1000 | Loss: 0.00002134
Iteration 80/1000 | Loss: 0.00002134
Iteration 81/1000 | Loss: 0.00002134
Iteration 82/1000 | Loss: 0.00002133
Iteration 83/1000 | Loss: 0.00002133
Iteration 84/1000 | Loss: 0.00002133
Iteration 85/1000 | Loss: 0.00002133
Iteration 86/1000 | Loss: 0.00002133
Iteration 87/1000 | Loss: 0.00002132
Iteration 88/1000 | Loss: 0.00002132
Iteration 89/1000 | Loss: 0.00002132
Iteration 90/1000 | Loss: 0.00002132
Iteration 91/1000 | Loss: 0.00002132
Iteration 92/1000 | Loss: 0.00002132
Iteration 93/1000 | Loss: 0.00002132
Iteration 94/1000 | Loss: 0.00002132
Iteration 95/1000 | Loss: 0.00002132
Iteration 96/1000 | Loss: 0.00002132
Iteration 97/1000 | Loss: 0.00002132
Iteration 98/1000 | Loss: 0.00002131
Iteration 99/1000 | Loss: 0.00002131
Iteration 100/1000 | Loss: 0.00002131
Iteration 101/1000 | Loss: 0.00002131
Iteration 102/1000 | Loss: 0.00002131
Iteration 103/1000 | Loss: 0.00002131
Iteration 104/1000 | Loss: 0.00002131
Iteration 105/1000 | Loss: 0.00002131
Iteration 106/1000 | Loss: 0.00002131
Iteration 107/1000 | Loss: 0.00002131
Iteration 108/1000 | Loss: 0.00002131
Iteration 109/1000 | Loss: 0.00002130
Iteration 110/1000 | Loss: 0.00002130
Iteration 111/1000 | Loss: 0.00002130
Iteration 112/1000 | Loss: 0.00002130
Iteration 113/1000 | Loss: 0.00002130
Iteration 114/1000 | Loss: 0.00002130
Iteration 115/1000 | Loss: 0.00002130
Iteration 116/1000 | Loss: 0.00002130
Iteration 117/1000 | Loss: 0.00002130
Iteration 118/1000 | Loss: 0.00002130
Iteration 119/1000 | Loss: 0.00002130
Iteration 120/1000 | Loss: 0.00002129
Iteration 121/1000 | Loss: 0.00002129
Iteration 122/1000 | Loss: 0.00002129
Iteration 123/1000 | Loss: 0.00002129
Iteration 124/1000 | Loss: 0.00002129
Iteration 125/1000 | Loss: 0.00002129
Iteration 126/1000 | Loss: 0.00002129
Iteration 127/1000 | Loss: 0.00002129
Iteration 128/1000 | Loss: 0.00002129
Iteration 129/1000 | Loss: 0.00002129
Iteration 130/1000 | Loss: 0.00002129
Iteration 131/1000 | Loss: 0.00002129
Iteration 132/1000 | Loss: 0.00002129
Iteration 133/1000 | Loss: 0.00002129
Iteration 134/1000 | Loss: 0.00002129
Iteration 135/1000 | Loss: 0.00002129
Iteration 136/1000 | Loss: 0.00002129
Iteration 137/1000 | Loss: 0.00002129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.129086533386726e-05, 2.129086533386726e-05, 2.129086533386726e-05, 2.129086533386726e-05, 2.129086533386726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.129086533386726e-05

Optimization complete. Final v2v error: 3.8450443744659424 mm

Highest mean error: 4.346654415130615 mm for frame 70

Lowest mean error: 3.197640895843506 mm for frame 133

Saving results

Total time: 43.18395662307739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805104
Iteration 2/25 | Loss: 0.00200400
Iteration 3/25 | Loss: 0.00152290
Iteration 4/25 | Loss: 0.00150054
Iteration 5/25 | Loss: 0.00149154
Iteration 6/25 | Loss: 0.00138708
Iteration 7/25 | Loss: 0.00132263
Iteration 8/25 | Loss: 0.00130680
Iteration 9/25 | Loss: 0.00135041
Iteration 10/25 | Loss: 0.00130404
Iteration 11/25 | Loss: 0.00130243
Iteration 12/25 | Loss: 0.00130197
Iteration 13/25 | Loss: 0.00130181
Iteration 14/25 | Loss: 0.00130153
Iteration 15/25 | Loss: 0.00130814
Iteration 16/25 | Loss: 0.00130039
Iteration 17/25 | Loss: 0.00129933
Iteration 18/25 | Loss: 0.00129916
Iteration 19/25 | Loss: 0.00129916
Iteration 20/25 | Loss: 0.00129916
Iteration 21/25 | Loss: 0.00129916
Iteration 22/25 | Loss: 0.00129916
Iteration 23/25 | Loss: 0.00129916
Iteration 24/25 | Loss: 0.00129916
Iteration 25/25 | Loss: 0.00129916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61973846
Iteration 2/25 | Loss: 0.00084149
Iteration 3/25 | Loss: 0.00084149
Iteration 4/25 | Loss: 0.00084149
Iteration 5/25 | Loss: 0.00084149
Iteration 6/25 | Loss: 0.00084149
Iteration 7/25 | Loss: 0.00084149
Iteration 8/25 | Loss: 0.00084149
Iteration 9/25 | Loss: 0.00084149
Iteration 10/25 | Loss: 0.00084149
Iteration 11/25 | Loss: 0.00084149
Iteration 12/25 | Loss: 0.00084149
Iteration 13/25 | Loss: 0.00084149
Iteration 14/25 | Loss: 0.00084149
Iteration 15/25 | Loss: 0.00084149
Iteration 16/25 | Loss: 0.00084149
Iteration 17/25 | Loss: 0.00084149
Iteration 18/25 | Loss: 0.00084149
Iteration 19/25 | Loss: 0.00084149
Iteration 20/25 | Loss: 0.00084149
Iteration 21/25 | Loss: 0.00084149
Iteration 22/25 | Loss: 0.00084149
Iteration 23/25 | Loss: 0.00084149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008414853946305811, 0.0008414853946305811, 0.0008414853946305811, 0.0008414853946305811, 0.0008414853946305811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008414853946305811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084149
Iteration 2/1000 | Loss: 0.00005278
Iteration 3/1000 | Loss: 0.00003481
Iteration 4/1000 | Loss: 0.00002998
Iteration 5/1000 | Loss: 0.00002767
Iteration 6/1000 | Loss: 0.00002615
Iteration 7/1000 | Loss: 0.00002534
Iteration 8/1000 | Loss: 0.00002474
Iteration 9/1000 | Loss: 0.00002440
Iteration 10/1000 | Loss: 0.00002417
Iteration 11/1000 | Loss: 0.00002404
Iteration 12/1000 | Loss: 0.00002401
Iteration 13/1000 | Loss: 0.00002381
Iteration 14/1000 | Loss: 0.00002365
Iteration 15/1000 | Loss: 0.00002357
Iteration 16/1000 | Loss: 0.00002357
Iteration 17/1000 | Loss: 0.00002350
Iteration 18/1000 | Loss: 0.00002348
Iteration 19/1000 | Loss: 0.00002346
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002345
Iteration 22/1000 | Loss: 0.00002344
Iteration 23/1000 | Loss: 0.00002344
Iteration 24/1000 | Loss: 0.00002343
Iteration 25/1000 | Loss: 0.00002342
Iteration 26/1000 | Loss: 0.00002342
Iteration 27/1000 | Loss: 0.00002342
Iteration 28/1000 | Loss: 0.00002341
Iteration 29/1000 | Loss: 0.00002341
Iteration 30/1000 | Loss: 0.00002340
Iteration 31/1000 | Loss: 0.00002340
Iteration 32/1000 | Loss: 0.00002338
Iteration 33/1000 | Loss: 0.00002337
Iteration 34/1000 | Loss: 0.00002337
Iteration 35/1000 | Loss: 0.00002337
Iteration 36/1000 | Loss: 0.00002336
Iteration 37/1000 | Loss: 0.00002335
Iteration 38/1000 | Loss: 0.00002335
Iteration 39/1000 | Loss: 0.00002335
Iteration 40/1000 | Loss: 0.00002334
Iteration 41/1000 | Loss: 0.00002333
Iteration 42/1000 | Loss: 0.00002333
Iteration 43/1000 | Loss: 0.00002333
Iteration 44/1000 | Loss: 0.00002332
Iteration 45/1000 | Loss: 0.00002332
Iteration 46/1000 | Loss: 0.00002332
Iteration 47/1000 | Loss: 0.00002332
Iteration 48/1000 | Loss: 0.00002331
Iteration 49/1000 | Loss: 0.00002331
Iteration 50/1000 | Loss: 0.00002331
Iteration 51/1000 | Loss: 0.00002330
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00002330
Iteration 54/1000 | Loss: 0.00002330
Iteration 55/1000 | Loss: 0.00002329
Iteration 56/1000 | Loss: 0.00002329
Iteration 57/1000 | Loss: 0.00002329
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002328
Iteration 60/1000 | Loss: 0.00002328
Iteration 61/1000 | Loss: 0.00002328
Iteration 62/1000 | Loss: 0.00002328
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002327
Iteration 65/1000 | Loss: 0.00002327
Iteration 66/1000 | Loss: 0.00002327
Iteration 67/1000 | Loss: 0.00002327
Iteration 68/1000 | Loss: 0.00002327
Iteration 69/1000 | Loss: 0.00002327
Iteration 70/1000 | Loss: 0.00002327
Iteration 71/1000 | Loss: 0.00002326
Iteration 72/1000 | Loss: 0.00002326
Iteration 73/1000 | Loss: 0.00002326
Iteration 74/1000 | Loss: 0.00002326
Iteration 75/1000 | Loss: 0.00002326
Iteration 76/1000 | Loss: 0.00002326
Iteration 77/1000 | Loss: 0.00002326
Iteration 78/1000 | Loss: 0.00002326
Iteration 79/1000 | Loss: 0.00002326
Iteration 80/1000 | Loss: 0.00002326
Iteration 81/1000 | Loss: 0.00002325
Iteration 82/1000 | Loss: 0.00002325
Iteration 83/1000 | Loss: 0.00002325
Iteration 84/1000 | Loss: 0.00002325
Iteration 85/1000 | Loss: 0.00002325
Iteration 86/1000 | Loss: 0.00002324
Iteration 87/1000 | Loss: 0.00002324
Iteration 88/1000 | Loss: 0.00002324
Iteration 89/1000 | Loss: 0.00002324
Iteration 90/1000 | Loss: 0.00002323
Iteration 91/1000 | Loss: 0.00002323
Iteration 92/1000 | Loss: 0.00002323
Iteration 93/1000 | Loss: 0.00002323
Iteration 94/1000 | Loss: 0.00002323
Iteration 95/1000 | Loss: 0.00002323
Iteration 96/1000 | Loss: 0.00002323
Iteration 97/1000 | Loss: 0.00002323
Iteration 98/1000 | Loss: 0.00002323
Iteration 99/1000 | Loss: 0.00002323
Iteration 100/1000 | Loss: 0.00002323
Iteration 101/1000 | Loss: 0.00002322
Iteration 102/1000 | Loss: 0.00002322
Iteration 103/1000 | Loss: 0.00002322
Iteration 104/1000 | Loss: 0.00002322
Iteration 105/1000 | Loss: 0.00002322
Iteration 106/1000 | Loss: 0.00002322
Iteration 107/1000 | Loss: 0.00002322
Iteration 108/1000 | Loss: 0.00002322
Iteration 109/1000 | Loss: 0.00002322
Iteration 110/1000 | Loss: 0.00002322
Iteration 111/1000 | Loss: 0.00002322
Iteration 112/1000 | Loss: 0.00002322
Iteration 113/1000 | Loss: 0.00002322
Iteration 114/1000 | Loss: 0.00002322
Iteration 115/1000 | Loss: 0.00002322
Iteration 116/1000 | Loss: 0.00002322
Iteration 117/1000 | Loss: 0.00002322
Iteration 118/1000 | Loss: 0.00002322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.3223252355819568e-05, 2.3223252355819568e-05, 2.3223252355819568e-05, 2.3223252355819568e-05, 2.3223252355819568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3223252355819568e-05

Optimization complete. Final v2v error: 3.908308267593384 mm

Highest mean error: 4.577220439910889 mm for frame 41

Lowest mean error: 3.018404006958008 mm for frame 2

Saving results

Total time: 58.581071615219116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444140
Iteration 2/25 | Loss: 0.00146121
Iteration 3/25 | Loss: 0.00129260
Iteration 4/25 | Loss: 0.00127225
Iteration 5/25 | Loss: 0.00126675
Iteration 6/25 | Loss: 0.00126546
Iteration 7/25 | Loss: 0.00126525
Iteration 8/25 | Loss: 0.00126525
Iteration 9/25 | Loss: 0.00126525
Iteration 10/25 | Loss: 0.00126525
Iteration 11/25 | Loss: 0.00126525
Iteration 12/25 | Loss: 0.00126525
Iteration 13/25 | Loss: 0.00126525
Iteration 14/25 | Loss: 0.00126525
Iteration 15/25 | Loss: 0.00126525
Iteration 16/25 | Loss: 0.00126525
Iteration 17/25 | Loss: 0.00126525
Iteration 18/25 | Loss: 0.00126525
Iteration 19/25 | Loss: 0.00126525
Iteration 20/25 | Loss: 0.00126525
Iteration 21/25 | Loss: 0.00126525
Iteration 22/25 | Loss: 0.00126525
Iteration 23/25 | Loss: 0.00126525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012652474688366055, 0.0012652474688366055, 0.0012652474688366055, 0.0012652474688366055, 0.0012652474688366055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012652474688366055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.07804012
Iteration 2/25 | Loss: 0.00068206
Iteration 3/25 | Loss: 0.00068202
Iteration 4/25 | Loss: 0.00068202
Iteration 5/25 | Loss: 0.00068202
Iteration 6/25 | Loss: 0.00068202
Iteration 7/25 | Loss: 0.00068202
Iteration 8/25 | Loss: 0.00068202
Iteration 9/25 | Loss: 0.00068202
Iteration 10/25 | Loss: 0.00068202
Iteration 11/25 | Loss: 0.00068202
Iteration 12/25 | Loss: 0.00068202
Iteration 13/25 | Loss: 0.00068202
Iteration 14/25 | Loss: 0.00068202
Iteration 15/25 | Loss: 0.00068202
Iteration 16/25 | Loss: 0.00068202
Iteration 17/25 | Loss: 0.00068202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000682021607644856, 0.000682021607644856, 0.000682021607644856, 0.000682021607644856, 0.000682021607644856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000682021607644856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068202
Iteration 2/1000 | Loss: 0.00004731
Iteration 3/1000 | Loss: 0.00003001
Iteration 4/1000 | Loss: 0.00002785
Iteration 5/1000 | Loss: 0.00002687
Iteration 6/1000 | Loss: 0.00002625
Iteration 7/1000 | Loss: 0.00002554
Iteration 8/1000 | Loss: 0.00002518
Iteration 9/1000 | Loss: 0.00002492
Iteration 10/1000 | Loss: 0.00002471
Iteration 11/1000 | Loss: 0.00002457
Iteration 12/1000 | Loss: 0.00002444
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002432
Iteration 15/1000 | Loss: 0.00002431
Iteration 16/1000 | Loss: 0.00002427
Iteration 17/1000 | Loss: 0.00002425
Iteration 18/1000 | Loss: 0.00002424
Iteration 19/1000 | Loss: 0.00002424
Iteration 20/1000 | Loss: 0.00002423
Iteration 21/1000 | Loss: 0.00002423
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002420
Iteration 24/1000 | Loss: 0.00002419
Iteration 25/1000 | Loss: 0.00002418
Iteration 26/1000 | Loss: 0.00002417
Iteration 27/1000 | Loss: 0.00002415
Iteration 28/1000 | Loss: 0.00002414
Iteration 29/1000 | Loss: 0.00002413
Iteration 30/1000 | Loss: 0.00002412
Iteration 31/1000 | Loss: 0.00002412
Iteration 32/1000 | Loss: 0.00002412
Iteration 33/1000 | Loss: 0.00002411
Iteration 34/1000 | Loss: 0.00002408
Iteration 35/1000 | Loss: 0.00002408
Iteration 36/1000 | Loss: 0.00002407
Iteration 37/1000 | Loss: 0.00002406
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002406
Iteration 40/1000 | Loss: 0.00002405
Iteration 41/1000 | Loss: 0.00002405
Iteration 42/1000 | Loss: 0.00002405
Iteration 43/1000 | Loss: 0.00002405
Iteration 44/1000 | Loss: 0.00002405
Iteration 45/1000 | Loss: 0.00002405
Iteration 46/1000 | Loss: 0.00002405
Iteration 47/1000 | Loss: 0.00002405
Iteration 48/1000 | Loss: 0.00002404
Iteration 49/1000 | Loss: 0.00002403
Iteration 50/1000 | Loss: 0.00002403
Iteration 51/1000 | Loss: 0.00002402
Iteration 52/1000 | Loss: 0.00002402
Iteration 53/1000 | Loss: 0.00002402
Iteration 54/1000 | Loss: 0.00002402
Iteration 55/1000 | Loss: 0.00002401
Iteration 56/1000 | Loss: 0.00002401
Iteration 57/1000 | Loss: 0.00002401
Iteration 58/1000 | Loss: 0.00002401
Iteration 59/1000 | Loss: 0.00002400
Iteration 60/1000 | Loss: 0.00002400
Iteration 61/1000 | Loss: 0.00002400
Iteration 62/1000 | Loss: 0.00002400
Iteration 63/1000 | Loss: 0.00002400
Iteration 64/1000 | Loss: 0.00002400
Iteration 65/1000 | Loss: 0.00002400
Iteration 66/1000 | Loss: 0.00002400
Iteration 67/1000 | Loss: 0.00002400
Iteration 68/1000 | Loss: 0.00002400
Iteration 69/1000 | Loss: 0.00002400
Iteration 70/1000 | Loss: 0.00002400
Iteration 71/1000 | Loss: 0.00002400
Iteration 72/1000 | Loss: 0.00002400
Iteration 73/1000 | Loss: 0.00002400
Iteration 74/1000 | Loss: 0.00002400
Iteration 75/1000 | Loss: 0.00002400
Iteration 76/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [2.3998167307581753e-05, 2.3998167307581753e-05, 2.3998167307581753e-05, 2.3998167307581753e-05, 2.3998167307581753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3998167307581753e-05

Optimization complete. Final v2v error: 4.130670547485352 mm

Highest mean error: 4.651353359222412 mm for frame 59

Lowest mean error: 3.6033644676208496 mm for frame 1

Saving results

Total time: 33.93016815185547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828676
Iteration 2/25 | Loss: 0.00125441
Iteration 3/25 | Loss: 0.00117498
Iteration 4/25 | Loss: 0.00116707
Iteration 5/25 | Loss: 0.00116553
Iteration 6/25 | Loss: 0.00116548
Iteration 7/25 | Loss: 0.00116548
Iteration 8/25 | Loss: 0.00116548
Iteration 9/25 | Loss: 0.00116548
Iteration 10/25 | Loss: 0.00116548
Iteration 11/25 | Loss: 0.00116548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011654786067083478, 0.0011654786067083478, 0.0011654786067083478, 0.0011654786067083478, 0.0011654786067083478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011654786067083478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45506811
Iteration 2/25 | Loss: 0.00059815
Iteration 3/25 | Loss: 0.00059813
Iteration 4/25 | Loss: 0.00059813
Iteration 5/25 | Loss: 0.00059813
Iteration 6/25 | Loss: 0.00059813
Iteration 7/25 | Loss: 0.00059813
Iteration 8/25 | Loss: 0.00059813
Iteration 9/25 | Loss: 0.00059813
Iteration 10/25 | Loss: 0.00059813
Iteration 11/25 | Loss: 0.00059813
Iteration 12/25 | Loss: 0.00059813
Iteration 13/25 | Loss: 0.00059813
Iteration 14/25 | Loss: 0.00059813
Iteration 15/25 | Loss: 0.00059813
Iteration 16/25 | Loss: 0.00059813
Iteration 17/25 | Loss: 0.00059813
Iteration 18/25 | Loss: 0.00059813
Iteration 19/25 | Loss: 0.00059813
Iteration 20/25 | Loss: 0.00059813
Iteration 21/25 | Loss: 0.00059813
Iteration 22/25 | Loss: 0.00059813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005981274880468845, 0.0005981274880468845, 0.0005981274880468845, 0.0005981274880468845, 0.0005981274880468845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005981274880468845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059813
Iteration 2/1000 | Loss: 0.00002509
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001364
Iteration 5/1000 | Loss: 0.00001272
Iteration 6/1000 | Loss: 0.00001215
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001157
Iteration 9/1000 | Loss: 0.00001149
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001141
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001138
Iteration 21/1000 | Loss: 0.00001135
Iteration 22/1000 | Loss: 0.00001134
Iteration 23/1000 | Loss: 0.00001121
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001107
Iteration 29/1000 | Loss: 0.00001107
Iteration 30/1000 | Loss: 0.00001103
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001091
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001088
Iteration 42/1000 | Loss: 0.00001087
Iteration 43/1000 | Loss: 0.00001087
Iteration 44/1000 | Loss: 0.00001087
Iteration 45/1000 | Loss: 0.00001086
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001080
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001078
Iteration 59/1000 | Loss: 0.00001078
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001076
Iteration 65/1000 | Loss: 0.00001076
Iteration 66/1000 | Loss: 0.00001076
Iteration 67/1000 | Loss: 0.00001076
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001076
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001074
Iteration 80/1000 | Loss: 0.00001074
Iteration 81/1000 | Loss: 0.00001074
Iteration 82/1000 | Loss: 0.00001074
Iteration 83/1000 | Loss: 0.00001074
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Iteration 86/1000 | Loss: 0.00001073
Iteration 87/1000 | Loss: 0.00001073
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001070
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001066
Iteration 98/1000 | Loss: 0.00001066
Iteration 99/1000 | Loss: 0.00001066
Iteration 100/1000 | Loss: 0.00001065
Iteration 101/1000 | Loss: 0.00001065
Iteration 102/1000 | Loss: 0.00001065
Iteration 103/1000 | Loss: 0.00001065
Iteration 104/1000 | Loss: 0.00001065
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001062
Iteration 112/1000 | Loss: 0.00001062
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001062
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001061
Iteration 117/1000 | Loss: 0.00001061
Iteration 118/1000 | Loss: 0.00001061
Iteration 119/1000 | Loss: 0.00001060
Iteration 120/1000 | Loss: 0.00001060
Iteration 121/1000 | Loss: 0.00001060
Iteration 122/1000 | Loss: 0.00001060
Iteration 123/1000 | Loss: 0.00001060
Iteration 124/1000 | Loss: 0.00001059
Iteration 125/1000 | Loss: 0.00001059
Iteration 126/1000 | Loss: 0.00001059
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001058
Iteration 129/1000 | Loss: 0.00001058
Iteration 130/1000 | Loss: 0.00001058
Iteration 131/1000 | Loss: 0.00001058
Iteration 132/1000 | Loss: 0.00001058
Iteration 133/1000 | Loss: 0.00001058
Iteration 134/1000 | Loss: 0.00001058
Iteration 135/1000 | Loss: 0.00001058
Iteration 136/1000 | Loss: 0.00001057
Iteration 137/1000 | Loss: 0.00001057
Iteration 138/1000 | Loss: 0.00001057
Iteration 139/1000 | Loss: 0.00001057
Iteration 140/1000 | Loss: 0.00001057
Iteration 141/1000 | Loss: 0.00001057
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001056
Iteration 144/1000 | Loss: 0.00001056
Iteration 145/1000 | Loss: 0.00001056
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001055
Iteration 154/1000 | Loss: 0.00001055
Iteration 155/1000 | Loss: 0.00001055
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Iteration 160/1000 | Loss: 0.00001054
Iteration 161/1000 | Loss: 0.00001054
Iteration 162/1000 | Loss: 0.00001054
Iteration 163/1000 | Loss: 0.00001054
Iteration 164/1000 | Loss: 0.00001054
Iteration 165/1000 | Loss: 0.00001054
Iteration 166/1000 | Loss: 0.00001053
Iteration 167/1000 | Loss: 0.00001053
Iteration 168/1000 | Loss: 0.00001053
Iteration 169/1000 | Loss: 0.00001053
Iteration 170/1000 | Loss: 0.00001053
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001053
Iteration 173/1000 | Loss: 0.00001053
Iteration 174/1000 | Loss: 0.00001053
Iteration 175/1000 | Loss: 0.00001052
Iteration 176/1000 | Loss: 0.00001052
Iteration 177/1000 | Loss: 0.00001052
Iteration 178/1000 | Loss: 0.00001052
Iteration 179/1000 | Loss: 0.00001051
Iteration 180/1000 | Loss: 0.00001051
Iteration 181/1000 | Loss: 0.00001051
Iteration 182/1000 | Loss: 0.00001051
Iteration 183/1000 | Loss: 0.00001051
Iteration 184/1000 | Loss: 0.00001051
Iteration 185/1000 | Loss: 0.00001051
Iteration 186/1000 | Loss: 0.00001051
Iteration 187/1000 | Loss: 0.00001050
Iteration 188/1000 | Loss: 0.00001050
Iteration 189/1000 | Loss: 0.00001050
Iteration 190/1000 | Loss: 0.00001050
Iteration 191/1000 | Loss: 0.00001050
Iteration 192/1000 | Loss: 0.00001049
Iteration 193/1000 | Loss: 0.00001049
Iteration 194/1000 | Loss: 0.00001049
Iteration 195/1000 | Loss: 0.00001049
Iteration 196/1000 | Loss: 0.00001049
Iteration 197/1000 | Loss: 0.00001049
Iteration 198/1000 | Loss: 0.00001049
Iteration 199/1000 | Loss: 0.00001049
Iteration 200/1000 | Loss: 0.00001049
Iteration 201/1000 | Loss: 0.00001049
Iteration 202/1000 | Loss: 0.00001049
Iteration 203/1000 | Loss: 0.00001048
Iteration 204/1000 | Loss: 0.00001048
Iteration 205/1000 | Loss: 0.00001048
Iteration 206/1000 | Loss: 0.00001048
Iteration 207/1000 | Loss: 0.00001048
Iteration 208/1000 | Loss: 0.00001048
Iteration 209/1000 | Loss: 0.00001048
Iteration 210/1000 | Loss: 0.00001048
Iteration 211/1000 | Loss: 0.00001048
Iteration 212/1000 | Loss: 0.00001048
Iteration 213/1000 | Loss: 0.00001048
Iteration 214/1000 | Loss: 0.00001048
Iteration 215/1000 | Loss: 0.00001048
Iteration 216/1000 | Loss: 0.00001048
Iteration 217/1000 | Loss: 0.00001048
Iteration 218/1000 | Loss: 0.00001048
Iteration 219/1000 | Loss: 0.00001048
Iteration 220/1000 | Loss: 0.00001048
Iteration 221/1000 | Loss: 0.00001048
Iteration 222/1000 | Loss: 0.00001048
Iteration 223/1000 | Loss: 0.00001048
Iteration 224/1000 | Loss: 0.00001048
Iteration 225/1000 | Loss: 0.00001048
Iteration 226/1000 | Loss: 0.00001048
Iteration 227/1000 | Loss: 0.00001048
Iteration 228/1000 | Loss: 0.00001048
Iteration 229/1000 | Loss: 0.00001048
Iteration 230/1000 | Loss: 0.00001048
Iteration 231/1000 | Loss: 0.00001048
Iteration 232/1000 | Loss: 0.00001048
Iteration 233/1000 | Loss: 0.00001048
Iteration 234/1000 | Loss: 0.00001048
Iteration 235/1000 | Loss: 0.00001048
Iteration 236/1000 | Loss: 0.00001048
Iteration 237/1000 | Loss: 0.00001048
Iteration 238/1000 | Loss: 0.00001048
Iteration 239/1000 | Loss: 0.00001048
Iteration 240/1000 | Loss: 0.00001048
Iteration 241/1000 | Loss: 0.00001048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.048114791046828e-05, 1.048114791046828e-05, 1.048114791046828e-05, 1.048114791046828e-05, 1.048114791046828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.048114791046828e-05

Optimization complete. Final v2v error: 2.74717378616333 mm

Highest mean error: 2.942570209503174 mm for frame 3

Lowest mean error: 2.63155198097229 mm for frame 143

Saving results

Total time: 41.9422881603241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085973
Iteration 2/25 | Loss: 0.01085973
Iteration 3/25 | Loss: 0.01085972
Iteration 4/25 | Loss: 0.01085972
Iteration 5/25 | Loss: 0.01085972
Iteration 6/25 | Loss: 0.01085972
Iteration 7/25 | Loss: 0.01085971
Iteration 8/25 | Loss: 0.01085971
Iteration 9/25 | Loss: 0.01085971
Iteration 10/25 | Loss: 0.01085971
Iteration 11/25 | Loss: 0.01085971
Iteration 12/25 | Loss: 0.00396378
Iteration 13/25 | Loss: 0.00196748
Iteration 14/25 | Loss: 0.00176448
Iteration 15/25 | Loss: 0.00178573
Iteration 16/25 | Loss: 0.00188851
Iteration 17/25 | Loss: 0.00192508
Iteration 18/25 | Loss: 0.00163024
Iteration 19/25 | Loss: 0.00146550
Iteration 20/25 | Loss: 0.00140461
Iteration 21/25 | Loss: 0.00137109
Iteration 22/25 | Loss: 0.00132619
Iteration 23/25 | Loss: 0.00133086
Iteration 24/25 | Loss: 0.00130318
Iteration 25/25 | Loss: 0.00130710

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56967628
Iteration 2/25 | Loss: 0.00078100
Iteration 3/25 | Loss: 0.00061213
Iteration 4/25 | Loss: 0.00061213
Iteration 5/25 | Loss: 0.00061213
Iteration 6/25 | Loss: 0.00061213
Iteration 7/25 | Loss: 0.00061213
Iteration 8/25 | Loss: 0.00061213
Iteration 9/25 | Loss: 0.00061213
Iteration 10/25 | Loss: 0.00061213
Iteration 11/25 | Loss: 0.00061213
Iteration 12/25 | Loss: 0.00061213
Iteration 13/25 | Loss: 0.00061213
Iteration 14/25 | Loss: 0.00061213
Iteration 15/25 | Loss: 0.00061213
Iteration 16/25 | Loss: 0.00061213
Iteration 17/25 | Loss: 0.00061213
Iteration 18/25 | Loss: 0.00061213
Iteration 19/25 | Loss: 0.00061213
Iteration 20/25 | Loss: 0.00061213
Iteration 21/25 | Loss: 0.00061213
Iteration 22/25 | Loss: 0.00061213
Iteration 23/25 | Loss: 0.00061213
Iteration 24/25 | Loss: 0.00061213
Iteration 25/25 | Loss: 0.00061213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061213
Iteration 2/1000 | Loss: 0.00022859
Iteration 3/1000 | Loss: 0.00010869
Iteration 4/1000 | Loss: 0.00015989
Iteration 5/1000 | Loss: 0.00005200
Iteration 6/1000 | Loss: 0.00003884
Iteration 7/1000 | Loss: 0.00006256
Iteration 8/1000 | Loss: 0.00003743
Iteration 9/1000 | Loss: 0.00007876
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002126
Iteration 12/1000 | Loss: 0.00002028
Iteration 13/1000 | Loss: 0.00002067
Iteration 14/1000 | Loss: 0.00002086
Iteration 15/1000 | Loss: 0.00003558
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002241
Iteration 21/1000 | Loss: 0.00002707
Iteration 22/1000 | Loss: 0.00002032
Iteration 23/1000 | Loss: 0.00002603
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001972
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00002008
Iteration 29/1000 | Loss: 0.00002007
Iteration 30/1000 | Loss: 0.00005249
Iteration 31/1000 | Loss: 0.00003398
Iteration 32/1000 | Loss: 0.00002066
Iteration 33/1000 | Loss: 0.00001995
Iteration 34/1000 | Loss: 0.00001964
Iteration 35/1000 | Loss: 0.00001923
Iteration 36/1000 | Loss: 0.00001923
Iteration 37/1000 | Loss: 0.00001922
Iteration 38/1000 | Loss: 0.00001922
Iteration 39/1000 | Loss: 0.00001922
Iteration 40/1000 | Loss: 0.00001974
Iteration 41/1000 | Loss: 0.00001974
Iteration 42/1000 | Loss: 0.00001974
Iteration 43/1000 | Loss: 0.00002206
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001915
Iteration 47/1000 | Loss: 0.00001915
Iteration 48/1000 | Loss: 0.00001915
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001915
Iteration 53/1000 | Loss: 0.00001915
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001915
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.9145822079735808e-05, 1.9145822079735808e-05, 1.9145822079735808e-05, 1.9145822079735808e-05, 1.9145822079735808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9145822079735808e-05

Optimization complete. Final v2v error: 3.719503879547119 mm

Highest mean error: 3.7905311584472656 mm for frame 21

Lowest mean error: 3.600733518600464 mm for frame 171

Saving results

Total time: 87.92095232009888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994576
Iteration 2/25 | Loss: 0.00994575
Iteration 3/25 | Loss: 0.00994575
Iteration 4/25 | Loss: 0.00472656
Iteration 5/25 | Loss: 0.00231853
Iteration 6/25 | Loss: 0.00209008
Iteration 7/25 | Loss: 0.00187704
Iteration 8/25 | Loss: 0.00197254
Iteration 9/25 | Loss: 0.00198527
Iteration 10/25 | Loss: 0.00180141
Iteration 11/25 | Loss: 0.00168037
Iteration 12/25 | Loss: 0.00166427
Iteration 13/25 | Loss: 0.00163870
Iteration 14/25 | Loss: 0.00160719
Iteration 15/25 | Loss: 0.00158428
Iteration 16/25 | Loss: 0.00157648
Iteration 17/25 | Loss: 0.00155446
Iteration 18/25 | Loss: 0.00154155
Iteration 19/25 | Loss: 0.00153484
Iteration 20/25 | Loss: 0.00153971
Iteration 21/25 | Loss: 0.00153392
Iteration 22/25 | Loss: 0.00152770
Iteration 23/25 | Loss: 0.00152681
Iteration 24/25 | Loss: 0.00152787
Iteration 25/25 | Loss: 0.00153127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46786141
Iteration 2/25 | Loss: 0.00580978
Iteration 3/25 | Loss: 0.00369033
Iteration 4/25 | Loss: 0.00369032
Iteration 5/25 | Loss: 0.00369032
Iteration 6/25 | Loss: 0.00369032
Iteration 7/25 | Loss: 0.00369032
Iteration 8/25 | Loss: 0.00369032
Iteration 9/25 | Loss: 0.00369032
Iteration 10/25 | Loss: 0.00369032
Iteration 11/25 | Loss: 0.00369032
Iteration 12/25 | Loss: 0.00369032
Iteration 13/25 | Loss: 0.00369032
Iteration 14/25 | Loss: 0.00369032
Iteration 15/25 | Loss: 0.00369032
Iteration 16/25 | Loss: 0.00369032
Iteration 17/25 | Loss: 0.00369032
Iteration 18/25 | Loss: 0.00369032
Iteration 19/25 | Loss: 0.00369032
Iteration 20/25 | Loss: 0.00369032
Iteration 21/25 | Loss: 0.00369032
Iteration 22/25 | Loss: 0.00369032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0036903158761560917, 0.0036903158761560917, 0.0036903158761560917, 0.0036903158761560917, 0.0036903158761560917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036903158761560917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00369032
Iteration 2/1000 | Loss: 0.00354442
Iteration 3/1000 | Loss: 0.00067636
Iteration 4/1000 | Loss: 0.00098831
Iteration 5/1000 | Loss: 0.00068635
Iteration 6/1000 | Loss: 0.00104368
Iteration 7/1000 | Loss: 0.00068459
Iteration 8/1000 | Loss: 0.00066148
Iteration 9/1000 | Loss: 0.00040441
Iteration 10/1000 | Loss: 0.00079930
Iteration 11/1000 | Loss: 0.00104622
Iteration 12/1000 | Loss: 0.00030198
Iteration 13/1000 | Loss: 0.00076607
Iteration 14/1000 | Loss: 0.00128231
Iteration 15/1000 | Loss: 0.00058576
Iteration 16/1000 | Loss: 0.00070840
Iteration 17/1000 | Loss: 0.00029033
Iteration 18/1000 | Loss: 0.00038727
Iteration 19/1000 | Loss: 0.00032131
Iteration 20/1000 | Loss: 0.00026977
Iteration 21/1000 | Loss: 0.00062787
Iteration 22/1000 | Loss: 0.00033063
Iteration 23/1000 | Loss: 0.00125496
Iteration 24/1000 | Loss: 0.00069972
Iteration 25/1000 | Loss: 0.00126205
Iteration 26/1000 | Loss: 0.00139273
Iteration 27/1000 | Loss: 0.00141305
Iteration 28/1000 | Loss: 0.00153085
Iteration 29/1000 | Loss: 0.00104744
Iteration 30/1000 | Loss: 0.00047118
Iteration 31/1000 | Loss: 0.00135028
Iteration 32/1000 | Loss: 0.00130344
Iteration 33/1000 | Loss: 0.00104202
Iteration 34/1000 | Loss: 0.00083051
Iteration 35/1000 | Loss: 0.00035741
Iteration 36/1000 | Loss: 0.00036978
Iteration 37/1000 | Loss: 0.00046837
Iteration 38/1000 | Loss: 0.00027162
Iteration 39/1000 | Loss: 0.00031291
Iteration 40/1000 | Loss: 0.00030205
Iteration 41/1000 | Loss: 0.00026404
Iteration 42/1000 | Loss: 0.00032076
Iteration 43/1000 | Loss: 0.00029697
Iteration 44/1000 | Loss: 0.00031039
Iteration 45/1000 | Loss: 0.00045703
Iteration 46/1000 | Loss: 0.00036179
Iteration 47/1000 | Loss: 0.00039621
Iteration 48/1000 | Loss: 0.00023603
Iteration 49/1000 | Loss: 0.00018175
Iteration 50/1000 | Loss: 0.00063081
Iteration 51/1000 | Loss: 0.00030154
Iteration 52/1000 | Loss: 0.00012470
Iteration 53/1000 | Loss: 0.00068966
Iteration 54/1000 | Loss: 0.00038073
Iteration 55/1000 | Loss: 0.00077821
Iteration 56/1000 | Loss: 0.00058630
Iteration 57/1000 | Loss: 0.00057219
Iteration 58/1000 | Loss: 0.00022565
Iteration 59/1000 | Loss: 0.00013693
Iteration 60/1000 | Loss: 0.00012056
Iteration 61/1000 | Loss: 0.00011645
Iteration 62/1000 | Loss: 0.00017812
Iteration 63/1000 | Loss: 0.00011222
Iteration 64/1000 | Loss: 0.00011053
Iteration 65/1000 | Loss: 0.00010889
Iteration 66/1000 | Loss: 0.00083306
Iteration 67/1000 | Loss: 0.00022535
Iteration 68/1000 | Loss: 0.00010992
Iteration 69/1000 | Loss: 0.00011295
Iteration 70/1000 | Loss: 0.00010543
Iteration 71/1000 | Loss: 0.00010490
Iteration 72/1000 | Loss: 0.00010342
Iteration 73/1000 | Loss: 0.00010257
Iteration 74/1000 | Loss: 0.00055952
Iteration 75/1000 | Loss: 0.00017452
Iteration 76/1000 | Loss: 0.00015930
Iteration 77/1000 | Loss: 0.00011063
Iteration 78/1000 | Loss: 0.00020065
Iteration 79/1000 | Loss: 0.00011205
Iteration 80/1000 | Loss: 0.00010332
Iteration 81/1000 | Loss: 0.00010125
Iteration 82/1000 | Loss: 0.00009898
Iteration 83/1000 | Loss: 0.00009724
Iteration 84/1000 | Loss: 0.00009605
Iteration 85/1000 | Loss: 0.00009552
Iteration 86/1000 | Loss: 0.00009490
Iteration 87/1000 | Loss: 0.00035636
Iteration 88/1000 | Loss: 0.00010933
Iteration 89/1000 | Loss: 0.00009703
Iteration 90/1000 | Loss: 0.00009491
Iteration 91/1000 | Loss: 0.00009403
Iteration 92/1000 | Loss: 0.00009309
Iteration 93/1000 | Loss: 0.00020822
Iteration 94/1000 | Loss: 0.00009732
Iteration 95/1000 | Loss: 0.00009409
Iteration 96/1000 | Loss: 0.00009204
Iteration 97/1000 | Loss: 0.00009058
Iteration 98/1000 | Loss: 0.00018785
Iteration 99/1000 | Loss: 0.00029409
Iteration 100/1000 | Loss: 0.00014497
Iteration 101/1000 | Loss: 0.00011296
Iteration 102/1000 | Loss: 0.00009939
Iteration 103/1000 | Loss: 0.00009141
Iteration 104/1000 | Loss: 0.00008913
Iteration 105/1000 | Loss: 0.00008747
Iteration 106/1000 | Loss: 0.00008609
Iteration 107/1000 | Loss: 0.00008550
Iteration 108/1000 | Loss: 0.00008515
Iteration 109/1000 | Loss: 0.00030692
Iteration 110/1000 | Loss: 0.00020701
Iteration 111/1000 | Loss: 0.00020160
Iteration 112/1000 | Loss: 0.00026365
Iteration 113/1000 | Loss: 0.00009828
Iteration 114/1000 | Loss: 0.00009090
Iteration 115/1000 | Loss: 0.00008657
Iteration 116/1000 | Loss: 0.00020332
Iteration 117/1000 | Loss: 0.00060283
Iteration 118/1000 | Loss: 0.00022361
Iteration 119/1000 | Loss: 0.00009195
Iteration 120/1000 | Loss: 0.00008526
Iteration 121/1000 | Loss: 0.00008089
Iteration 122/1000 | Loss: 0.00007701
Iteration 123/1000 | Loss: 0.00007483
Iteration 124/1000 | Loss: 0.00007308
Iteration 125/1000 | Loss: 0.00007237
Iteration 126/1000 | Loss: 0.00007177
Iteration 127/1000 | Loss: 0.00007128
Iteration 128/1000 | Loss: 0.00017375
Iteration 129/1000 | Loss: 0.00007857
Iteration 130/1000 | Loss: 0.00007356
Iteration 131/1000 | Loss: 0.00007187
Iteration 132/1000 | Loss: 0.00007042
Iteration 133/1000 | Loss: 0.00006961
Iteration 134/1000 | Loss: 0.00006916
Iteration 135/1000 | Loss: 0.00006888
Iteration 136/1000 | Loss: 0.00006868
Iteration 137/1000 | Loss: 0.00006857
Iteration 138/1000 | Loss: 0.00006852
Iteration 139/1000 | Loss: 0.00006837
Iteration 140/1000 | Loss: 0.00006837
Iteration 141/1000 | Loss: 0.00006836
Iteration 142/1000 | Loss: 0.00006836
Iteration 143/1000 | Loss: 0.00006835
Iteration 144/1000 | Loss: 0.00006830
Iteration 145/1000 | Loss: 0.00006828
Iteration 146/1000 | Loss: 0.00006828
Iteration 147/1000 | Loss: 0.00006828
Iteration 148/1000 | Loss: 0.00006828
Iteration 149/1000 | Loss: 0.00006828
Iteration 150/1000 | Loss: 0.00006828
Iteration 151/1000 | Loss: 0.00006828
Iteration 152/1000 | Loss: 0.00006828
Iteration 153/1000 | Loss: 0.00006828
Iteration 154/1000 | Loss: 0.00006828
Iteration 155/1000 | Loss: 0.00006828
Iteration 156/1000 | Loss: 0.00006828
Iteration 157/1000 | Loss: 0.00006827
Iteration 158/1000 | Loss: 0.00006827
Iteration 159/1000 | Loss: 0.00006827
Iteration 160/1000 | Loss: 0.00006827
Iteration 161/1000 | Loss: 0.00006827
Iteration 162/1000 | Loss: 0.00006827
Iteration 163/1000 | Loss: 0.00006823
Iteration 164/1000 | Loss: 0.00006822
Iteration 165/1000 | Loss: 0.00006820
Iteration 166/1000 | Loss: 0.00006820
Iteration 167/1000 | Loss: 0.00006817
Iteration 168/1000 | Loss: 0.00006817
Iteration 169/1000 | Loss: 0.00006817
Iteration 170/1000 | Loss: 0.00006817
Iteration 171/1000 | Loss: 0.00006817
Iteration 172/1000 | Loss: 0.00006817
Iteration 173/1000 | Loss: 0.00006817
Iteration 174/1000 | Loss: 0.00006817
Iteration 175/1000 | Loss: 0.00006816
Iteration 176/1000 | Loss: 0.00006816
Iteration 177/1000 | Loss: 0.00006816
Iteration 178/1000 | Loss: 0.00006816
Iteration 179/1000 | Loss: 0.00006815
Iteration 180/1000 | Loss: 0.00006815
Iteration 181/1000 | Loss: 0.00006815
Iteration 182/1000 | Loss: 0.00006815
Iteration 183/1000 | Loss: 0.00006814
Iteration 184/1000 | Loss: 0.00006814
Iteration 185/1000 | Loss: 0.00006813
Iteration 186/1000 | Loss: 0.00006813
Iteration 187/1000 | Loss: 0.00006812
Iteration 188/1000 | Loss: 0.00006812
Iteration 189/1000 | Loss: 0.00006812
Iteration 190/1000 | Loss: 0.00006811
Iteration 191/1000 | Loss: 0.00006811
Iteration 192/1000 | Loss: 0.00006810
Iteration 193/1000 | Loss: 0.00006809
Iteration 194/1000 | Loss: 0.00006809
Iteration 195/1000 | Loss: 0.00006808
Iteration 196/1000 | Loss: 0.00006807
Iteration 197/1000 | Loss: 0.00006807
Iteration 198/1000 | Loss: 0.00006807
Iteration 199/1000 | Loss: 0.00006807
Iteration 200/1000 | Loss: 0.00006806
Iteration 201/1000 | Loss: 0.00006806
Iteration 202/1000 | Loss: 0.00006806
Iteration 203/1000 | Loss: 0.00006806
Iteration 204/1000 | Loss: 0.00006806
Iteration 205/1000 | Loss: 0.00017048
Iteration 206/1000 | Loss: 0.00007547
Iteration 207/1000 | Loss: 0.00007094
Iteration 208/1000 | Loss: 0.00006971
Iteration 209/1000 | Loss: 0.00006839
Iteration 210/1000 | Loss: 0.00006757
Iteration 211/1000 | Loss: 0.00006694
Iteration 212/1000 | Loss: 0.00006675
Iteration 213/1000 | Loss: 0.00006654
Iteration 214/1000 | Loss: 0.00006649
Iteration 215/1000 | Loss: 0.00006636
Iteration 216/1000 | Loss: 0.00006623
Iteration 217/1000 | Loss: 0.00006618
Iteration 218/1000 | Loss: 0.00006618
Iteration 219/1000 | Loss: 0.00006615
Iteration 220/1000 | Loss: 0.00006615
Iteration 221/1000 | Loss: 0.00006615
Iteration 222/1000 | Loss: 0.00006615
Iteration 223/1000 | Loss: 0.00006615
Iteration 224/1000 | Loss: 0.00006614
Iteration 225/1000 | Loss: 0.00006614
Iteration 226/1000 | Loss: 0.00006613
Iteration 227/1000 | Loss: 0.00006612
Iteration 228/1000 | Loss: 0.00006608
Iteration 229/1000 | Loss: 0.00006604
Iteration 230/1000 | Loss: 0.00029731
Iteration 231/1000 | Loss: 0.00007637
Iteration 232/1000 | Loss: 0.00007062
Iteration 233/1000 | Loss: 0.00006824
Iteration 234/1000 | Loss: 0.00029403
Iteration 235/1000 | Loss: 0.00061616
Iteration 236/1000 | Loss: 0.00039796
Iteration 237/1000 | Loss: 0.00022827
Iteration 238/1000 | Loss: 0.00008331
Iteration 239/1000 | Loss: 0.00007324
Iteration 240/1000 | Loss: 0.00050533
Iteration 241/1000 | Loss: 0.00014228
Iteration 242/1000 | Loss: 0.00017373
Iteration 243/1000 | Loss: 0.00009338
Iteration 244/1000 | Loss: 0.00011978
Iteration 245/1000 | Loss: 0.00006820
Iteration 246/1000 | Loss: 0.00006119
Iteration 247/1000 | Loss: 0.00005872
Iteration 248/1000 | Loss: 0.00007056
Iteration 249/1000 | Loss: 0.00005750
Iteration 250/1000 | Loss: 0.00027501
Iteration 251/1000 | Loss: 0.00005907
Iteration 252/1000 | Loss: 0.00005542
Iteration 253/1000 | Loss: 0.00016237
Iteration 254/1000 | Loss: 0.00014982
Iteration 255/1000 | Loss: 0.00005829
Iteration 256/1000 | Loss: 0.00005392
Iteration 257/1000 | Loss: 0.00005110
Iteration 258/1000 | Loss: 0.00035817
Iteration 259/1000 | Loss: 0.00005263
Iteration 260/1000 | Loss: 0.00028959
Iteration 261/1000 | Loss: 0.00023562
Iteration 262/1000 | Loss: 0.00029088
Iteration 263/1000 | Loss: 0.00013985
Iteration 264/1000 | Loss: 0.00014562
Iteration 265/1000 | Loss: 0.00027962
Iteration 266/1000 | Loss: 0.00021553
Iteration 267/1000 | Loss: 0.00017001
Iteration 268/1000 | Loss: 0.00009881
Iteration 269/1000 | Loss: 0.00005847
Iteration 270/1000 | Loss: 0.00030458
Iteration 271/1000 | Loss: 0.00022432
Iteration 272/1000 | Loss: 0.00054744
Iteration 273/1000 | Loss: 0.00022983
Iteration 274/1000 | Loss: 0.00018368
Iteration 275/1000 | Loss: 0.00005771
Iteration 276/1000 | Loss: 0.00006016
Iteration 277/1000 | Loss: 0.00014372
Iteration 278/1000 | Loss: 0.00012423
Iteration 279/1000 | Loss: 0.00005610
Iteration 280/1000 | Loss: 0.00007193
Iteration 281/1000 | Loss: 0.00004986
Iteration 282/1000 | Loss: 0.00062012
Iteration 283/1000 | Loss: 0.00037777
Iteration 284/1000 | Loss: 0.00018017
Iteration 285/1000 | Loss: 0.00009171
Iteration 286/1000 | Loss: 0.00012070
Iteration 287/1000 | Loss: 0.00005086
Iteration 288/1000 | Loss: 0.00020931
Iteration 289/1000 | Loss: 0.00009829
Iteration 290/1000 | Loss: 0.00004700
Iteration 291/1000 | Loss: 0.00004617
Iteration 292/1000 | Loss: 0.00010510
Iteration 293/1000 | Loss: 0.00046738
Iteration 294/1000 | Loss: 0.00006446
Iteration 295/1000 | Loss: 0.00032775
Iteration 296/1000 | Loss: 0.00022963
Iteration 297/1000 | Loss: 0.00015944
Iteration 298/1000 | Loss: 0.00006665
Iteration 299/1000 | Loss: 0.00004453
Iteration 300/1000 | Loss: 0.00008073
Iteration 301/1000 | Loss: 0.00004390
Iteration 302/1000 | Loss: 0.00004339
Iteration 303/1000 | Loss: 0.00004286
Iteration 304/1000 | Loss: 0.00004248
Iteration 305/1000 | Loss: 0.00026895
Iteration 306/1000 | Loss: 0.00019419
Iteration 307/1000 | Loss: 0.00055646
Iteration 308/1000 | Loss: 0.00039857
Iteration 309/1000 | Loss: 0.00055950
Iteration 310/1000 | Loss: 0.00031335
Iteration 311/1000 | Loss: 0.00025958
Iteration 312/1000 | Loss: 0.00040441
Iteration 313/1000 | Loss: 0.00022450
Iteration 314/1000 | Loss: 0.00020571
Iteration 315/1000 | Loss: 0.00005045
Iteration 316/1000 | Loss: 0.00005548
Iteration 317/1000 | Loss: 0.00004735
Iteration 318/1000 | Loss: 0.00004574
Iteration 319/1000 | Loss: 0.00010969
Iteration 320/1000 | Loss: 0.00004394
Iteration 321/1000 | Loss: 0.00008767
Iteration 322/1000 | Loss: 0.00004291
Iteration 323/1000 | Loss: 0.00004245
Iteration 324/1000 | Loss: 0.00004194
Iteration 325/1000 | Loss: 0.00004133
Iteration 326/1000 | Loss: 0.00004083
Iteration 327/1000 | Loss: 0.00025739
Iteration 328/1000 | Loss: 0.00020553
Iteration 329/1000 | Loss: 0.00041858
Iteration 330/1000 | Loss: 0.00021059
Iteration 331/1000 | Loss: 0.00026331
Iteration 332/1000 | Loss: 0.00025337
Iteration 333/1000 | Loss: 0.00004302
Iteration 334/1000 | Loss: 0.00004195
Iteration 335/1000 | Loss: 0.00025933
Iteration 336/1000 | Loss: 0.00004719
Iteration 337/1000 | Loss: 0.00004367
Iteration 338/1000 | Loss: 0.00025316
Iteration 339/1000 | Loss: 0.00024560
Iteration 340/1000 | Loss: 0.00048625
Iteration 341/1000 | Loss: 0.00046312
Iteration 342/1000 | Loss: 0.00008463
Iteration 343/1000 | Loss: 0.00005171
Iteration 344/1000 | Loss: 0.00008678
Iteration 345/1000 | Loss: 0.00004396
Iteration 346/1000 | Loss: 0.00009024
Iteration 347/1000 | Loss: 0.00026091
Iteration 348/1000 | Loss: 0.00024154
Iteration 349/1000 | Loss: 0.00004650
Iteration 350/1000 | Loss: 0.00025982
Iteration 351/1000 | Loss: 0.00026317
Iteration 352/1000 | Loss: 0.00021802
Iteration 353/1000 | Loss: 0.00018763
Iteration 354/1000 | Loss: 0.00005492
Iteration 355/1000 | Loss: 0.00008723
Iteration 356/1000 | Loss: 0.00004545
Iteration 357/1000 | Loss: 0.00005844
Iteration 358/1000 | Loss: 0.00004479
Iteration 359/1000 | Loss: 0.00007424
Iteration 360/1000 | Loss: 0.00004198
Iteration 361/1000 | Loss: 0.00007855
Iteration 362/1000 | Loss: 0.00004101
Iteration 363/1000 | Loss: 0.00027176
Iteration 364/1000 | Loss: 0.00021320
Iteration 365/1000 | Loss: 0.00016531
Iteration 366/1000 | Loss: 0.00005239
Iteration 367/1000 | Loss: 0.00051456
Iteration 368/1000 | Loss: 0.00035782
Iteration 369/1000 | Loss: 0.00009310
Iteration 370/1000 | Loss: 0.00027742
Iteration 371/1000 | Loss: 0.00007101
Iteration 372/1000 | Loss: 0.00027736
Iteration 373/1000 | Loss: 0.00039831
Iteration 374/1000 | Loss: 0.00022001
Iteration 375/1000 | Loss: 0.00027189
Iteration 376/1000 | Loss: 0.00036327
Iteration 377/1000 | Loss: 0.00039120
Iteration 378/1000 | Loss: 0.00031210
Iteration 379/1000 | Loss: 0.00031523
Iteration 380/1000 | Loss: 0.00030282
Iteration 381/1000 | Loss: 0.00007277
Iteration 382/1000 | Loss: 0.00005487
Iteration 383/1000 | Loss: 0.00004601
Iteration 384/1000 | Loss: 0.00005755
Iteration 385/1000 | Loss: 0.00005520
Iteration 386/1000 | Loss: 0.00005117
Iteration 387/1000 | Loss: 0.00004984
Iteration 388/1000 | Loss: 0.00004486
Iteration 389/1000 | Loss: 0.00004540
Iteration 390/1000 | Loss: 0.00024257
Iteration 391/1000 | Loss: 0.00020982
Iteration 392/1000 | Loss: 0.00003753
Iteration 393/1000 | Loss: 0.00024333
Iteration 394/1000 | Loss: 0.00004762
Iteration 395/1000 | Loss: 0.00004109
Iteration 396/1000 | Loss: 0.00003831
Iteration 397/1000 | Loss: 0.00006491
Iteration 398/1000 | Loss: 0.00003666
Iteration 399/1000 | Loss: 0.00003603
Iteration 400/1000 | Loss: 0.00011915
Iteration 401/1000 | Loss: 0.00073489
Iteration 402/1000 | Loss: 0.00044149
Iteration 403/1000 | Loss: 0.00066371
Iteration 404/1000 | Loss: 0.00067915
Iteration 405/1000 | Loss: 0.00005483
Iteration 406/1000 | Loss: 0.00004540
Iteration 407/1000 | Loss: 0.00003794
Iteration 408/1000 | Loss: 0.00035006
Iteration 409/1000 | Loss: 0.00033277
Iteration 410/1000 | Loss: 0.00028446
Iteration 411/1000 | Loss: 0.00005504
Iteration 412/1000 | Loss: 0.00014055
Iteration 413/1000 | Loss: 0.00004127
Iteration 414/1000 | Loss: 0.00003889
Iteration 415/1000 | Loss: 0.00024951
Iteration 416/1000 | Loss: 0.00050053
Iteration 417/1000 | Loss: 0.00023819
Iteration 418/1000 | Loss: 0.00034979
Iteration 419/1000 | Loss: 0.00004713
Iteration 420/1000 | Loss: 0.00010187
Iteration 421/1000 | Loss: 0.00003863
Iteration 422/1000 | Loss: 0.00003669
Iteration 423/1000 | Loss: 0.00007274
Iteration 424/1000 | Loss: 0.00003549
Iteration 425/1000 | Loss: 0.00006835
Iteration 426/1000 | Loss: 0.00003516
Iteration 427/1000 | Loss: 0.00003468
Iteration 428/1000 | Loss: 0.00003436
Iteration 429/1000 | Loss: 0.00003393
Iteration 430/1000 | Loss: 0.00012250
Iteration 431/1000 | Loss: 0.00003363
Iteration 432/1000 | Loss: 0.00026336
Iteration 433/1000 | Loss: 0.00019043
Iteration 434/1000 | Loss: 0.00051309
Iteration 435/1000 | Loss: 0.00037533
Iteration 436/1000 | Loss: 0.00046437
Iteration 437/1000 | Loss: 0.00043721
Iteration 438/1000 | Loss: 0.00019953
Iteration 439/1000 | Loss: 0.00015939
Iteration 440/1000 | Loss: 0.00007467
Iteration 441/1000 | Loss: 0.00041073
Iteration 442/1000 | Loss: 0.00034910
Iteration 443/1000 | Loss: 0.00004818
Iteration 444/1000 | Loss: 0.00004505
Iteration 445/1000 | Loss: 0.00018848
Iteration 446/1000 | Loss: 0.00003791
Iteration 447/1000 | Loss: 0.00003550
Iteration 448/1000 | Loss: 0.00007945
Iteration 449/1000 | Loss: 0.00021042
Iteration 450/1000 | Loss: 0.00004920
Iteration 451/1000 | Loss: 0.00003605
Iteration 452/1000 | Loss: 0.00003330
Iteration 453/1000 | Loss: 0.00003279
Iteration 454/1000 | Loss: 0.00025357
Iteration 455/1000 | Loss: 0.00007312
Iteration 456/1000 | Loss: 0.00005845
Iteration 457/1000 | Loss: 0.00018293
Iteration 458/1000 | Loss: 0.00008109
Iteration 459/1000 | Loss: 0.00003843
Iteration 460/1000 | Loss: 0.00003526
Iteration 461/1000 | Loss: 0.00009496
Iteration 462/1000 | Loss: 0.00031586
Iteration 463/1000 | Loss: 0.00003364
Iteration 464/1000 | Loss: 0.00003267
Iteration 465/1000 | Loss: 0.00003235
Iteration 466/1000 | Loss: 0.00003186
Iteration 467/1000 | Loss: 0.00014101
Iteration 468/1000 | Loss: 0.00004302
Iteration 469/1000 | Loss: 0.00003148
Iteration 470/1000 | Loss: 0.00005506
Iteration 471/1000 | Loss: 0.00006187
Iteration 472/1000 | Loss: 0.00004211
Iteration 473/1000 | Loss: 0.00004843
Iteration 474/1000 | Loss: 0.00003858
Iteration 475/1000 | Loss: 0.00003110
Iteration 476/1000 | Loss: 0.00004199
Iteration 477/1000 | Loss: 0.00003183
Iteration 478/1000 | Loss: 0.00003102
Iteration 479/1000 | Loss: 0.00003059
Iteration 480/1000 | Loss: 0.00003026
Iteration 481/1000 | Loss: 0.00002997
Iteration 482/1000 | Loss: 0.00002968
Iteration 483/1000 | Loss: 0.00025867
Iteration 484/1000 | Loss: 0.00017480
Iteration 485/1000 | Loss: 0.00002989
Iteration 486/1000 | Loss: 0.00026863
Iteration 487/1000 | Loss: 0.00023244
Iteration 488/1000 | Loss: 0.00004029
Iteration 489/1000 | Loss: 0.00025781
Iteration 490/1000 | Loss: 0.00023915
Iteration 491/1000 | Loss: 0.00044992
Iteration 492/1000 | Loss: 0.00005284
Iteration 493/1000 | Loss: 0.00009014
Iteration 494/1000 | Loss: 0.00005873
Iteration 495/1000 | Loss: 0.00007985
Iteration 496/1000 | Loss: 0.00002808
Iteration 497/1000 | Loss: 0.00002714
Iteration 498/1000 | Loss: 0.00007460
Iteration 499/1000 | Loss: 0.00002621
Iteration 500/1000 | Loss: 0.00005630
Iteration 501/1000 | Loss: 0.00002570
Iteration 502/1000 | Loss: 0.00002558
Iteration 503/1000 | Loss: 0.00002557
Iteration 504/1000 | Loss: 0.00002555
Iteration 505/1000 | Loss: 0.00002553
Iteration 506/1000 | Loss: 0.00002552
Iteration 507/1000 | Loss: 0.00002546
Iteration 508/1000 | Loss: 0.00002532
Iteration 509/1000 | Loss: 0.00024214
Iteration 510/1000 | Loss: 0.00025447
Iteration 511/1000 | Loss: 0.00024749
Iteration 512/1000 | Loss: 0.00003245
Iteration 513/1000 | Loss: 0.00002770
Iteration 514/1000 | Loss: 0.00007950
Iteration 515/1000 | Loss: 0.00002693
Iteration 516/1000 | Loss: 0.00008418
Iteration 517/1000 | Loss: 0.00003216
Iteration 518/1000 | Loss: 0.00002641
Iteration 519/1000 | Loss: 0.00002627
Iteration 520/1000 | Loss: 0.00002610
Iteration 521/1000 | Loss: 0.00002608
Iteration 522/1000 | Loss: 0.00002606
Iteration 523/1000 | Loss: 0.00002606
Iteration 524/1000 | Loss: 0.00002605
Iteration 525/1000 | Loss: 0.00002605
Iteration 526/1000 | Loss: 0.00002605
Iteration 527/1000 | Loss: 0.00002604
Iteration 528/1000 | Loss: 0.00002604
Iteration 529/1000 | Loss: 0.00002603
Iteration 530/1000 | Loss: 0.00002603
Iteration 531/1000 | Loss: 0.00002602
Iteration 532/1000 | Loss: 0.00002602
Iteration 533/1000 | Loss: 0.00002602
Iteration 534/1000 | Loss: 0.00002601
Iteration 535/1000 | Loss: 0.00002601
Iteration 536/1000 | Loss: 0.00002601
Iteration 537/1000 | Loss: 0.00002600
Iteration 538/1000 | Loss: 0.00002598
Iteration 539/1000 | Loss: 0.00002595
Iteration 540/1000 | Loss: 0.00002595
Iteration 541/1000 | Loss: 0.00002594
Iteration 542/1000 | Loss: 0.00002594
Iteration 543/1000 | Loss: 0.00002594
Iteration 544/1000 | Loss: 0.00002593
Iteration 545/1000 | Loss: 0.00002593
Iteration 546/1000 | Loss: 0.00002593
Iteration 547/1000 | Loss: 0.00002592
Iteration 548/1000 | Loss: 0.00002589
Iteration 549/1000 | Loss: 0.00002588
Iteration 550/1000 | Loss: 0.00002588
Iteration 551/1000 | Loss: 0.00002587
Iteration 552/1000 | Loss: 0.00002586
Iteration 553/1000 | Loss: 0.00002584
Iteration 554/1000 | Loss: 0.00002584
Iteration 555/1000 | Loss: 0.00002582
Iteration 556/1000 | Loss: 0.00002582
Iteration 557/1000 | Loss: 0.00002581
Iteration 558/1000 | Loss: 0.00002581
Iteration 559/1000 | Loss: 0.00002579
Iteration 560/1000 | Loss: 0.00002579
Iteration 561/1000 | Loss: 0.00002579
Iteration 562/1000 | Loss: 0.00002579
Iteration 563/1000 | Loss: 0.00002578
Iteration 564/1000 | Loss: 0.00002578
Iteration 565/1000 | Loss: 0.00002578
Iteration 566/1000 | Loss: 0.00002578
Iteration 567/1000 | Loss: 0.00002578
Iteration 568/1000 | Loss: 0.00002578
Iteration 569/1000 | Loss: 0.00002578
Iteration 570/1000 | Loss: 0.00002577
Iteration 571/1000 | Loss: 0.00002577
Iteration 572/1000 | Loss: 0.00002577
Iteration 573/1000 | Loss: 0.00002577
Iteration 574/1000 | Loss: 0.00002576
Iteration 575/1000 | Loss: 0.00002576
Iteration 576/1000 | Loss: 0.00002576
Iteration 577/1000 | Loss: 0.00002576
Iteration 578/1000 | Loss: 0.00002575
Iteration 579/1000 | Loss: 0.00002575
Iteration 580/1000 | Loss: 0.00002575
Iteration 581/1000 | Loss: 0.00002575
Iteration 582/1000 | Loss: 0.00002575
Iteration 583/1000 | Loss: 0.00002575
Iteration 584/1000 | Loss: 0.00002575
Iteration 585/1000 | Loss: 0.00002575
Iteration 586/1000 | Loss: 0.00002575
Iteration 587/1000 | Loss: 0.00002575
Iteration 588/1000 | Loss: 0.00002574
Iteration 589/1000 | Loss: 0.00002574
Iteration 590/1000 | Loss: 0.00002574
Iteration 591/1000 | Loss: 0.00002574
Iteration 592/1000 | Loss: 0.00002574
Iteration 593/1000 | Loss: 0.00002574
Iteration 594/1000 | Loss: 0.00002574
Iteration 595/1000 | Loss: 0.00002574
Iteration 596/1000 | Loss: 0.00002574
Iteration 597/1000 | Loss: 0.00002574
Iteration 598/1000 | Loss: 0.00002574
Iteration 599/1000 | Loss: 0.00002574
Iteration 600/1000 | Loss: 0.00002573
Iteration 601/1000 | Loss: 0.00002573
Iteration 602/1000 | Loss: 0.00002573
Iteration 603/1000 | Loss: 0.00002573
Iteration 604/1000 | Loss: 0.00002573
Iteration 605/1000 | Loss: 0.00002573
Iteration 606/1000 | Loss: 0.00002573
Iteration 607/1000 | Loss: 0.00002573
Iteration 608/1000 | Loss: 0.00002573
Iteration 609/1000 | Loss: 0.00002573
Iteration 610/1000 | Loss: 0.00002573
Iteration 611/1000 | Loss: 0.00002573
Iteration 612/1000 | Loss: 0.00002573
Iteration 613/1000 | Loss: 0.00002573
Iteration 614/1000 | Loss: 0.00002573
Iteration 615/1000 | Loss: 0.00002572
Iteration 616/1000 | Loss: 0.00002572
Iteration 617/1000 | Loss: 0.00002572
Iteration 618/1000 | Loss: 0.00002572
Iteration 619/1000 | Loss: 0.00002572
Iteration 620/1000 | Loss: 0.00002572
Iteration 621/1000 | Loss: 0.00002572
Iteration 622/1000 | Loss: 0.00002572
Iteration 623/1000 | Loss: 0.00002572
Iteration 624/1000 | Loss: 0.00002572
Iteration 625/1000 | Loss: 0.00002572
Iteration 626/1000 | Loss: 0.00002572
Iteration 627/1000 | Loss: 0.00002572
Iteration 628/1000 | Loss: 0.00002572
Iteration 629/1000 | Loss: 0.00002572
Iteration 630/1000 | Loss: 0.00002572
Iteration 631/1000 | Loss: 0.00002572
Iteration 632/1000 | Loss: 0.00002572
Iteration 633/1000 | Loss: 0.00002572
Iteration 634/1000 | Loss: 0.00002572
Iteration 635/1000 | Loss: 0.00002572
Iteration 636/1000 | Loss: 0.00002572
Iteration 637/1000 | Loss: 0.00002572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 637. Stopping optimization.
Last 5 losses: [2.5715607989695854e-05, 2.5715607989695854e-05, 2.5715607989695854e-05, 2.5715607989695854e-05, 2.5715607989695854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5715607989695854e-05

Optimization complete. Final v2v error: 3.523421049118042 mm

Highest mean error: 10.801465034484863 mm for frame 18

Lowest mean error: 2.9601686000823975 mm for frame 70

Saving results

Total time: 765.3942549228668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548148
Iteration 2/25 | Loss: 0.00157929
Iteration 3/25 | Loss: 0.00138851
Iteration 4/25 | Loss: 0.00137319
Iteration 5/25 | Loss: 0.00137065
Iteration 6/25 | Loss: 0.00137034
Iteration 7/25 | Loss: 0.00137034
Iteration 8/25 | Loss: 0.00137034
Iteration 9/25 | Loss: 0.00137034
Iteration 10/25 | Loss: 0.00137034
Iteration 11/25 | Loss: 0.00137034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013703443109989166, 0.0013703443109989166, 0.0013703443109989166, 0.0013703443109989166, 0.0013703443109989166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013703443109989166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41664684
Iteration 2/25 | Loss: 0.00097894
Iteration 3/25 | Loss: 0.00097890
Iteration 4/25 | Loss: 0.00097890
Iteration 5/25 | Loss: 0.00097890
Iteration 6/25 | Loss: 0.00097890
Iteration 7/25 | Loss: 0.00097889
Iteration 8/25 | Loss: 0.00097889
Iteration 9/25 | Loss: 0.00097889
Iteration 10/25 | Loss: 0.00097889
Iteration 11/25 | Loss: 0.00097889
Iteration 12/25 | Loss: 0.00097889
Iteration 13/25 | Loss: 0.00097889
Iteration 14/25 | Loss: 0.00097889
Iteration 15/25 | Loss: 0.00097889
Iteration 16/25 | Loss: 0.00097889
Iteration 17/25 | Loss: 0.00097889
Iteration 18/25 | Loss: 0.00097889
Iteration 19/25 | Loss: 0.00097889
Iteration 20/25 | Loss: 0.00097889
Iteration 21/25 | Loss: 0.00097889
Iteration 22/25 | Loss: 0.00097889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009788930183276534, 0.0009788930183276534, 0.0009788930183276534, 0.0009788930183276534, 0.0009788930183276534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009788930183276534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097889
Iteration 2/1000 | Loss: 0.00006756
Iteration 3/1000 | Loss: 0.00003719
Iteration 4/1000 | Loss: 0.00003170
Iteration 5/1000 | Loss: 0.00002957
Iteration 6/1000 | Loss: 0.00002767
Iteration 7/1000 | Loss: 0.00002661
Iteration 8/1000 | Loss: 0.00002569
Iteration 9/1000 | Loss: 0.00002514
Iteration 10/1000 | Loss: 0.00002472
Iteration 11/1000 | Loss: 0.00002454
Iteration 12/1000 | Loss: 0.00002434
Iteration 13/1000 | Loss: 0.00002425
Iteration 14/1000 | Loss: 0.00002419
Iteration 15/1000 | Loss: 0.00002411
Iteration 16/1000 | Loss: 0.00002403
Iteration 17/1000 | Loss: 0.00002400
Iteration 18/1000 | Loss: 0.00002397
Iteration 19/1000 | Loss: 0.00002394
Iteration 20/1000 | Loss: 0.00002393
Iteration 21/1000 | Loss: 0.00002392
Iteration 22/1000 | Loss: 0.00002391
Iteration 23/1000 | Loss: 0.00002391
Iteration 24/1000 | Loss: 0.00002391
Iteration 25/1000 | Loss: 0.00002391
Iteration 26/1000 | Loss: 0.00002391
Iteration 27/1000 | Loss: 0.00002391
Iteration 28/1000 | Loss: 0.00002390
Iteration 29/1000 | Loss: 0.00002390
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002388
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002388
Iteration 34/1000 | Loss: 0.00002388
Iteration 35/1000 | Loss: 0.00002388
Iteration 36/1000 | Loss: 0.00002388
Iteration 37/1000 | Loss: 0.00002388
Iteration 38/1000 | Loss: 0.00002388
Iteration 39/1000 | Loss: 0.00002387
Iteration 40/1000 | Loss: 0.00002387
Iteration 41/1000 | Loss: 0.00002387
Iteration 42/1000 | Loss: 0.00002387
Iteration 43/1000 | Loss: 0.00002386
Iteration 44/1000 | Loss: 0.00002386
Iteration 45/1000 | Loss: 0.00002386
Iteration 46/1000 | Loss: 0.00002385
Iteration 47/1000 | Loss: 0.00002385
Iteration 48/1000 | Loss: 0.00002384
Iteration 49/1000 | Loss: 0.00002384
Iteration 50/1000 | Loss: 0.00002384
Iteration 51/1000 | Loss: 0.00002383
Iteration 52/1000 | Loss: 0.00002383
Iteration 53/1000 | Loss: 0.00002383
Iteration 54/1000 | Loss: 0.00002383
Iteration 55/1000 | Loss: 0.00002383
Iteration 56/1000 | Loss: 0.00002383
Iteration 57/1000 | Loss: 0.00002383
Iteration 58/1000 | Loss: 0.00002383
Iteration 59/1000 | Loss: 0.00002383
Iteration 60/1000 | Loss: 0.00002383
Iteration 61/1000 | Loss: 0.00002383
Iteration 62/1000 | Loss: 0.00002382
Iteration 63/1000 | Loss: 0.00002382
Iteration 64/1000 | Loss: 0.00002381
Iteration 65/1000 | Loss: 0.00002381
Iteration 66/1000 | Loss: 0.00002381
Iteration 67/1000 | Loss: 0.00002381
Iteration 68/1000 | Loss: 0.00002380
Iteration 69/1000 | Loss: 0.00002380
Iteration 70/1000 | Loss: 0.00002380
Iteration 71/1000 | Loss: 0.00002380
Iteration 72/1000 | Loss: 0.00002380
Iteration 73/1000 | Loss: 0.00002380
Iteration 74/1000 | Loss: 0.00002380
Iteration 75/1000 | Loss: 0.00002380
Iteration 76/1000 | Loss: 0.00002379
Iteration 77/1000 | Loss: 0.00002379
Iteration 78/1000 | Loss: 0.00002379
Iteration 79/1000 | Loss: 0.00002379
Iteration 80/1000 | Loss: 0.00002379
Iteration 81/1000 | Loss: 0.00002379
Iteration 82/1000 | Loss: 0.00002378
Iteration 83/1000 | Loss: 0.00002378
Iteration 84/1000 | Loss: 0.00002378
Iteration 85/1000 | Loss: 0.00002377
Iteration 86/1000 | Loss: 0.00002377
Iteration 87/1000 | Loss: 0.00002377
Iteration 88/1000 | Loss: 0.00002377
Iteration 89/1000 | Loss: 0.00002377
Iteration 90/1000 | Loss: 0.00002377
Iteration 91/1000 | Loss: 0.00002377
Iteration 92/1000 | Loss: 0.00002377
Iteration 93/1000 | Loss: 0.00002376
Iteration 94/1000 | Loss: 0.00002376
Iteration 95/1000 | Loss: 0.00002376
Iteration 96/1000 | Loss: 0.00002376
Iteration 97/1000 | Loss: 0.00002376
Iteration 98/1000 | Loss: 0.00002376
Iteration 99/1000 | Loss: 0.00002376
Iteration 100/1000 | Loss: 0.00002376
Iteration 101/1000 | Loss: 0.00002376
Iteration 102/1000 | Loss: 0.00002376
Iteration 103/1000 | Loss: 0.00002376
Iteration 104/1000 | Loss: 0.00002376
Iteration 105/1000 | Loss: 0.00002376
Iteration 106/1000 | Loss: 0.00002376
Iteration 107/1000 | Loss: 0.00002375
Iteration 108/1000 | Loss: 0.00002375
Iteration 109/1000 | Loss: 0.00002375
Iteration 110/1000 | Loss: 0.00002375
Iteration 111/1000 | Loss: 0.00002375
Iteration 112/1000 | Loss: 0.00002375
Iteration 113/1000 | Loss: 0.00002375
Iteration 114/1000 | Loss: 0.00002375
Iteration 115/1000 | Loss: 0.00002375
Iteration 116/1000 | Loss: 0.00002375
Iteration 117/1000 | Loss: 0.00002375
Iteration 118/1000 | Loss: 0.00002375
Iteration 119/1000 | Loss: 0.00002375
Iteration 120/1000 | Loss: 0.00002375
Iteration 121/1000 | Loss: 0.00002375
Iteration 122/1000 | Loss: 0.00002375
Iteration 123/1000 | Loss: 0.00002375
Iteration 124/1000 | Loss: 0.00002375
Iteration 125/1000 | Loss: 0.00002375
Iteration 126/1000 | Loss: 0.00002375
Iteration 127/1000 | Loss: 0.00002375
Iteration 128/1000 | Loss: 0.00002375
Iteration 129/1000 | Loss: 0.00002375
Iteration 130/1000 | Loss: 0.00002375
Iteration 131/1000 | Loss: 0.00002375
Iteration 132/1000 | Loss: 0.00002375
Iteration 133/1000 | Loss: 0.00002375
Iteration 134/1000 | Loss: 0.00002375
Iteration 135/1000 | Loss: 0.00002375
Iteration 136/1000 | Loss: 0.00002375
Iteration 137/1000 | Loss: 0.00002375
Iteration 138/1000 | Loss: 0.00002375
Iteration 139/1000 | Loss: 0.00002375
Iteration 140/1000 | Loss: 0.00002375
Iteration 141/1000 | Loss: 0.00002375
Iteration 142/1000 | Loss: 0.00002375
Iteration 143/1000 | Loss: 0.00002375
Iteration 144/1000 | Loss: 0.00002375
Iteration 145/1000 | Loss: 0.00002375
Iteration 146/1000 | Loss: 0.00002375
Iteration 147/1000 | Loss: 0.00002375
Iteration 148/1000 | Loss: 0.00002375
Iteration 149/1000 | Loss: 0.00002375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.374932955717668e-05, 2.374932955717668e-05, 2.374932955717668e-05, 2.374932955717668e-05, 2.374932955717668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.374932955717668e-05

Optimization complete. Final v2v error: 4.0659661293029785 mm

Highest mean error: 4.726114749908447 mm for frame 141

Lowest mean error: 3.41652250289917 mm for frame 19

Saving results

Total time: 36.807873010635376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488766
Iteration 2/25 | Loss: 0.00154359
Iteration 3/25 | Loss: 0.00139026
Iteration 4/25 | Loss: 0.00136789
Iteration 5/25 | Loss: 0.00136073
Iteration 6/25 | Loss: 0.00135894
Iteration 7/25 | Loss: 0.00135832
Iteration 8/25 | Loss: 0.00135791
Iteration 9/25 | Loss: 0.00135765
Iteration 10/25 | Loss: 0.00135750
Iteration 11/25 | Loss: 0.00135743
Iteration 12/25 | Loss: 0.00135743
Iteration 13/25 | Loss: 0.00135743
Iteration 14/25 | Loss: 0.00135743
Iteration 15/25 | Loss: 0.00135743
Iteration 16/25 | Loss: 0.00135743
Iteration 17/25 | Loss: 0.00135743
Iteration 18/25 | Loss: 0.00135743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013574324548244476, 0.0013574324548244476, 0.0013574324548244476, 0.0013574324548244476, 0.0013574324548244476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013574324548244476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43574965
Iteration 2/25 | Loss: 0.00220399
Iteration 3/25 | Loss: 0.00220399
Iteration 4/25 | Loss: 0.00220399
Iteration 5/25 | Loss: 0.00220399
Iteration 6/25 | Loss: 0.00220399
Iteration 7/25 | Loss: 0.00220399
Iteration 8/25 | Loss: 0.00220399
Iteration 9/25 | Loss: 0.00220399
Iteration 10/25 | Loss: 0.00220399
Iteration 11/25 | Loss: 0.00220399
Iteration 12/25 | Loss: 0.00220399
Iteration 13/25 | Loss: 0.00220399
Iteration 14/25 | Loss: 0.00220399
Iteration 15/25 | Loss: 0.00220399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022039860486984253, 0.0022039860486984253, 0.0022039860486984253, 0.0022039860486984253, 0.0022039860486984253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022039860486984253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220399
Iteration 2/1000 | Loss: 0.00024826
Iteration 3/1000 | Loss: 0.00016448
Iteration 4/1000 | Loss: 0.00013971
Iteration 5/1000 | Loss: 0.00012624
Iteration 6/1000 | Loss: 0.00012020
Iteration 7/1000 | Loss: 0.00011367
Iteration 8/1000 | Loss: 0.00010945
Iteration 9/1000 | Loss: 0.00010426
Iteration 10/1000 | Loss: 0.00010122
Iteration 11/1000 | Loss: 0.00009938
Iteration 12/1000 | Loss: 0.00009804
Iteration 13/1000 | Loss: 0.00009708
Iteration 14/1000 | Loss: 0.00009625
Iteration 15/1000 | Loss: 0.00009523
Iteration 16/1000 | Loss: 0.00009443
Iteration 17/1000 | Loss: 0.00009369
Iteration 18/1000 | Loss: 0.00037843
Iteration 19/1000 | Loss: 0.00009844
Iteration 20/1000 | Loss: 0.00009318
Iteration 21/1000 | Loss: 0.00009147
Iteration 22/1000 | Loss: 0.00009052
Iteration 23/1000 | Loss: 0.00008942
Iteration 24/1000 | Loss: 0.00008849
Iteration 25/1000 | Loss: 0.00084202
Iteration 26/1000 | Loss: 0.00726922
Iteration 27/1000 | Loss: 0.00509676
Iteration 28/1000 | Loss: 0.00013577
Iteration 29/1000 | Loss: 0.00009599
Iteration 30/1000 | Loss: 0.00008855
Iteration 31/1000 | Loss: 0.00008641
Iteration 32/1000 | Loss: 0.00008551
Iteration 33/1000 | Loss: 0.00008448
Iteration 34/1000 | Loss: 0.00103079
Iteration 35/1000 | Loss: 0.00644776
Iteration 36/1000 | Loss: 0.00205777
Iteration 37/1000 | Loss: 0.00010631
Iteration 38/1000 | Loss: 0.00008642
Iteration 39/1000 | Loss: 0.00008199
Iteration 40/1000 | Loss: 0.00042327
Iteration 41/1000 | Loss: 0.00029446
Iteration 42/1000 | Loss: 0.00078566
Iteration 43/1000 | Loss: 0.00055449
Iteration 44/1000 | Loss: 0.00010012
Iteration 45/1000 | Loss: 0.00008785
Iteration 46/1000 | Loss: 0.00008370
Iteration 47/1000 | Loss: 0.00253516
Iteration 48/1000 | Loss: 0.00060677
Iteration 49/1000 | Loss: 0.00008708
Iteration 50/1000 | Loss: 0.00097939
Iteration 51/1000 | Loss: 0.00124857
Iteration 52/1000 | Loss: 0.00008307
Iteration 53/1000 | Loss: 0.00206477
Iteration 54/1000 | Loss: 0.00108388
Iteration 55/1000 | Loss: 0.00449743
Iteration 56/1000 | Loss: 0.00107650
Iteration 57/1000 | Loss: 0.00009579
Iteration 58/1000 | Loss: 0.00090706
Iteration 59/1000 | Loss: 0.00060179
Iteration 60/1000 | Loss: 0.00008145
Iteration 61/1000 | Loss: 0.00089571
Iteration 62/1000 | Loss: 0.00049679
Iteration 63/1000 | Loss: 0.00271480
Iteration 64/1000 | Loss: 0.00067594
Iteration 65/1000 | Loss: 0.00049682
Iteration 66/1000 | Loss: 0.00011273
Iteration 67/1000 | Loss: 0.00035786
Iteration 68/1000 | Loss: 0.00016684
Iteration 69/1000 | Loss: 0.00038553
Iteration 70/1000 | Loss: 0.00035059
Iteration 71/1000 | Loss: 0.00193524
Iteration 72/1000 | Loss: 0.00018331
Iteration 73/1000 | Loss: 0.00008196
Iteration 74/1000 | Loss: 0.00007244
Iteration 75/1000 | Loss: 0.00006818
Iteration 76/1000 | Loss: 0.00006575
Iteration 77/1000 | Loss: 0.00006312
Iteration 78/1000 | Loss: 0.00034085
Iteration 79/1000 | Loss: 0.00006708
Iteration 80/1000 | Loss: 0.00060988
Iteration 81/1000 | Loss: 0.00009139
Iteration 82/1000 | Loss: 0.00006058
Iteration 83/1000 | Loss: 0.00005758
Iteration 84/1000 | Loss: 0.00005618
Iteration 85/1000 | Loss: 0.00049043
Iteration 86/1000 | Loss: 0.00007533
Iteration 87/1000 | Loss: 0.00005546
Iteration 88/1000 | Loss: 0.00005297
Iteration 89/1000 | Loss: 0.00005214
Iteration 90/1000 | Loss: 0.00005133
Iteration 91/1000 | Loss: 0.00005075
Iteration 92/1000 | Loss: 0.00005013
Iteration 93/1000 | Loss: 0.00004969
Iteration 94/1000 | Loss: 0.00126856
Iteration 95/1000 | Loss: 0.00053681
Iteration 96/1000 | Loss: 0.00014458
Iteration 97/1000 | Loss: 0.00005846
Iteration 98/1000 | Loss: 0.00005058
Iteration 99/1000 | Loss: 0.00004957
Iteration 100/1000 | Loss: 0.00004909
Iteration 101/1000 | Loss: 0.00004877
Iteration 102/1000 | Loss: 0.00004846
Iteration 103/1000 | Loss: 0.00004832
Iteration 104/1000 | Loss: 0.00004831
Iteration 105/1000 | Loss: 0.00070579
Iteration 106/1000 | Loss: 0.00021926
Iteration 107/1000 | Loss: 0.00004879
Iteration 108/1000 | Loss: 0.00004806
Iteration 109/1000 | Loss: 0.00118373
Iteration 110/1000 | Loss: 0.00043570
Iteration 111/1000 | Loss: 0.00006300
Iteration 112/1000 | Loss: 0.00005395
Iteration 113/1000 | Loss: 0.00005021
Iteration 114/1000 | Loss: 0.00004875
Iteration 115/1000 | Loss: 0.00004836
Iteration 116/1000 | Loss: 0.00004799
Iteration 117/1000 | Loss: 0.00004780
Iteration 118/1000 | Loss: 0.00004775
Iteration 119/1000 | Loss: 0.00067616
Iteration 120/1000 | Loss: 0.00024616
Iteration 121/1000 | Loss: 0.00004970
Iteration 122/1000 | Loss: 0.00004799
Iteration 123/1000 | Loss: 0.00004778
Iteration 124/1000 | Loss: 0.00004776
Iteration 125/1000 | Loss: 0.00004770
Iteration 126/1000 | Loss: 0.00004770
Iteration 127/1000 | Loss: 0.00004768
Iteration 128/1000 | Loss: 0.00004768
Iteration 129/1000 | Loss: 0.00004767
Iteration 130/1000 | Loss: 0.00004767
Iteration 131/1000 | Loss: 0.00004766
Iteration 132/1000 | Loss: 0.00004766
Iteration 133/1000 | Loss: 0.00004765
Iteration 134/1000 | Loss: 0.00004765
Iteration 135/1000 | Loss: 0.00004764
Iteration 136/1000 | Loss: 0.00004763
Iteration 137/1000 | Loss: 0.00004762
Iteration 138/1000 | Loss: 0.00004761
Iteration 139/1000 | Loss: 0.00004761
Iteration 140/1000 | Loss: 0.00004759
Iteration 141/1000 | Loss: 0.00004758
Iteration 142/1000 | Loss: 0.00004758
Iteration 143/1000 | Loss: 0.00004758
Iteration 144/1000 | Loss: 0.00004758
Iteration 145/1000 | Loss: 0.00004757
Iteration 146/1000 | Loss: 0.00004757
Iteration 147/1000 | Loss: 0.00004756
Iteration 148/1000 | Loss: 0.00004756
Iteration 149/1000 | Loss: 0.00004755
Iteration 150/1000 | Loss: 0.00004753
Iteration 151/1000 | Loss: 0.00004752
Iteration 152/1000 | Loss: 0.00068262
Iteration 153/1000 | Loss: 0.00031609
Iteration 154/1000 | Loss: 0.00004843
Iteration 155/1000 | Loss: 0.00004758
Iteration 156/1000 | Loss: 0.00004757
Iteration 157/1000 | Loss: 0.00004753
Iteration 158/1000 | Loss: 0.00191769
Iteration 159/1000 | Loss: 0.00025179
Iteration 160/1000 | Loss: 0.00071134
Iteration 161/1000 | Loss: 0.00018743
Iteration 162/1000 | Loss: 0.00005173
Iteration 163/1000 | Loss: 0.00004915
Iteration 164/1000 | Loss: 0.00004760
Iteration 165/1000 | Loss: 0.00004682
Iteration 166/1000 | Loss: 0.00004638
Iteration 167/1000 | Loss: 0.00041734
Iteration 168/1000 | Loss: 0.00007323
Iteration 169/1000 | Loss: 0.00004692
Iteration 170/1000 | Loss: 0.00034990
Iteration 171/1000 | Loss: 0.00011640
Iteration 172/1000 | Loss: 0.00005073
Iteration 173/1000 | Loss: 0.00004748
Iteration 174/1000 | Loss: 0.00004626
Iteration 175/1000 | Loss: 0.00060343
Iteration 176/1000 | Loss: 0.00007527
Iteration 177/1000 | Loss: 0.00004612
Iteration 178/1000 | Loss: 0.00048458
Iteration 179/1000 | Loss: 0.00010045
Iteration 180/1000 | Loss: 0.00004647
Iteration 181/1000 | Loss: 0.00035311
Iteration 182/1000 | Loss: 0.00011072
Iteration 183/1000 | Loss: 0.00004620
Iteration 184/1000 | Loss: 0.00004584
Iteration 185/1000 | Loss: 0.00029506
Iteration 186/1000 | Loss: 0.00009528
Iteration 187/1000 | Loss: 0.00004622
Iteration 188/1000 | Loss: 0.00004583
Iteration 189/1000 | Loss: 0.00004581
Iteration 190/1000 | Loss: 0.00004576
Iteration 191/1000 | Loss: 0.00004575
Iteration 192/1000 | Loss: 0.00004574
Iteration 193/1000 | Loss: 0.00004574
Iteration 194/1000 | Loss: 0.00004574
Iteration 195/1000 | Loss: 0.00004573
Iteration 196/1000 | Loss: 0.00004573
Iteration 197/1000 | Loss: 0.00004572
Iteration 198/1000 | Loss: 0.00026240
Iteration 199/1000 | Loss: 0.00010026
Iteration 200/1000 | Loss: 0.00151931
Iteration 201/1000 | Loss: 0.00018354
Iteration 202/1000 | Loss: 0.00049586
Iteration 203/1000 | Loss: 0.00031641
Iteration 204/1000 | Loss: 0.00004678
Iteration 205/1000 | Loss: 0.00004605
Iteration 206/1000 | Loss: 0.00004577
Iteration 207/1000 | Loss: 0.00004576
Iteration 208/1000 | Loss: 0.00004575
Iteration 209/1000 | Loss: 0.00004575
Iteration 210/1000 | Loss: 0.00004573
Iteration 211/1000 | Loss: 0.00029798
Iteration 212/1000 | Loss: 0.00012846
Iteration 213/1000 | Loss: 0.00022056
Iteration 214/1000 | Loss: 0.00015684
Iteration 215/1000 | Loss: 0.00069575
Iteration 216/1000 | Loss: 0.00028018
Iteration 217/1000 | Loss: 0.00006923
Iteration 218/1000 | Loss: 0.00004937
Iteration 219/1000 | Loss: 0.00004681
Iteration 220/1000 | Loss: 0.00004563
Iteration 221/1000 | Loss: 0.00004520
Iteration 222/1000 | Loss: 0.00004481
Iteration 223/1000 | Loss: 0.00054860
Iteration 224/1000 | Loss: 0.00016920
Iteration 225/1000 | Loss: 0.00005352
Iteration 226/1000 | Loss: 0.00004735
Iteration 227/1000 | Loss: 0.00004541
Iteration 228/1000 | Loss: 0.00004453
Iteration 229/1000 | Loss: 0.00004445
Iteration 230/1000 | Loss: 0.00004444
Iteration 231/1000 | Loss: 0.00004434
Iteration 232/1000 | Loss: 0.00004414
Iteration 233/1000 | Loss: 0.00004394
Iteration 234/1000 | Loss: 0.00004393
Iteration 235/1000 | Loss: 0.00004391
Iteration 236/1000 | Loss: 0.00004390
Iteration 237/1000 | Loss: 0.00004388
Iteration 238/1000 | Loss: 0.00004387
Iteration 239/1000 | Loss: 0.00052840
Iteration 240/1000 | Loss: 0.00004712
Iteration 241/1000 | Loss: 0.00004428
Iteration 242/1000 | Loss: 0.00004371
Iteration 243/1000 | Loss: 0.00004328
Iteration 244/1000 | Loss: 0.00004295
Iteration 245/1000 | Loss: 0.00066410
Iteration 246/1000 | Loss: 0.00016020
Iteration 247/1000 | Loss: 0.00004562
Iteration 248/1000 | Loss: 0.00004472
Iteration 249/1000 | Loss: 0.00010094
Iteration 250/1000 | Loss: 0.00009137
Iteration 251/1000 | Loss: 0.00004468
Iteration 252/1000 | Loss: 0.00010330
Iteration 253/1000 | Loss: 0.00010843
Iteration 254/1000 | Loss: 0.00008591
Iteration 255/1000 | Loss: 0.00004384
Iteration 256/1000 | Loss: 0.00004318
Iteration 257/1000 | Loss: 0.00004282
Iteration 258/1000 | Loss: 0.00055171
Iteration 259/1000 | Loss: 0.00013281
Iteration 260/1000 | Loss: 0.00004620
Iteration 261/1000 | Loss: 0.00004318
Iteration 262/1000 | Loss: 0.00046114
Iteration 263/1000 | Loss: 0.00020744
Iteration 264/1000 | Loss: 0.00004745
Iteration 265/1000 | Loss: 0.00004498
Iteration 266/1000 | Loss: 0.00005101
Iteration 267/1000 | Loss: 0.00004339
Iteration 268/1000 | Loss: 0.00004284
Iteration 269/1000 | Loss: 0.00004245
Iteration 270/1000 | Loss: 0.00052872
Iteration 271/1000 | Loss: 0.00026482
Iteration 272/1000 | Loss: 0.00005963
Iteration 273/1000 | Loss: 0.00005063
Iteration 274/1000 | Loss: 0.00004610
Iteration 275/1000 | Loss: 0.00006249
Iteration 276/1000 | Loss: 0.00004374
Iteration 277/1000 | Loss: 0.00004329
Iteration 278/1000 | Loss: 0.00004268
Iteration 279/1000 | Loss: 0.00004235
Iteration 280/1000 | Loss: 0.00004210
Iteration 281/1000 | Loss: 0.00004203
Iteration 282/1000 | Loss: 0.00004202
Iteration 283/1000 | Loss: 0.00004196
Iteration 284/1000 | Loss: 0.00004195
Iteration 285/1000 | Loss: 0.00004193
Iteration 286/1000 | Loss: 0.00004188
Iteration 287/1000 | Loss: 0.00004187
Iteration 288/1000 | Loss: 0.00004185
Iteration 289/1000 | Loss: 0.00067657
Iteration 290/1000 | Loss: 0.00012211
Iteration 291/1000 | Loss: 0.00004601
Iteration 292/1000 | Loss: 0.00004481
Iteration 293/1000 | Loss: 0.00004414
Iteration 294/1000 | Loss: 0.00004298
Iteration 295/1000 | Loss: 0.00055941
Iteration 296/1000 | Loss: 0.00005807
Iteration 297/1000 | Loss: 0.00004362
Iteration 298/1000 | Loss: 0.00004158
Iteration 299/1000 | Loss: 0.00004100
Iteration 300/1000 | Loss: 0.00004068
Iteration 301/1000 | Loss: 0.00004048
Iteration 302/1000 | Loss: 0.00004023
Iteration 303/1000 | Loss: 0.00004004
Iteration 304/1000 | Loss: 0.00004001
Iteration 305/1000 | Loss: 0.00003988
Iteration 306/1000 | Loss: 0.00003983
Iteration 307/1000 | Loss: 0.00003983
Iteration 308/1000 | Loss: 0.00003982
Iteration 309/1000 | Loss: 0.00003982
Iteration 310/1000 | Loss: 0.00003982
Iteration 311/1000 | Loss: 0.00003982
Iteration 312/1000 | Loss: 0.00003981
Iteration 313/1000 | Loss: 0.00003981
Iteration 314/1000 | Loss: 0.00003981
Iteration 315/1000 | Loss: 0.00003981
Iteration 316/1000 | Loss: 0.00003981
Iteration 317/1000 | Loss: 0.00003981
Iteration 318/1000 | Loss: 0.00003981
Iteration 319/1000 | Loss: 0.00003980
Iteration 320/1000 | Loss: 0.00003979
Iteration 321/1000 | Loss: 0.00003978
Iteration 322/1000 | Loss: 0.00003978
Iteration 323/1000 | Loss: 0.00003978
Iteration 324/1000 | Loss: 0.00003978
Iteration 325/1000 | Loss: 0.00003978
Iteration 326/1000 | Loss: 0.00003978
Iteration 327/1000 | Loss: 0.00003978
Iteration 328/1000 | Loss: 0.00003978
Iteration 329/1000 | Loss: 0.00003977
Iteration 330/1000 | Loss: 0.00003977
Iteration 331/1000 | Loss: 0.00003977
Iteration 332/1000 | Loss: 0.00003976
Iteration 333/1000 | Loss: 0.00003975
Iteration 334/1000 | Loss: 0.00003975
Iteration 335/1000 | Loss: 0.00003974
Iteration 336/1000 | Loss: 0.00003974
Iteration 337/1000 | Loss: 0.00003974
Iteration 338/1000 | Loss: 0.00003973
Iteration 339/1000 | Loss: 0.00003972
Iteration 340/1000 | Loss: 0.00003972
Iteration 341/1000 | Loss: 0.00003971
Iteration 342/1000 | Loss: 0.00003971
Iteration 343/1000 | Loss: 0.00003971
Iteration 344/1000 | Loss: 0.00003971
Iteration 345/1000 | Loss: 0.00003971
Iteration 346/1000 | Loss: 0.00003970
Iteration 347/1000 | Loss: 0.00003970
Iteration 348/1000 | Loss: 0.00003970
Iteration 349/1000 | Loss: 0.00003970
Iteration 350/1000 | Loss: 0.00003970
Iteration 351/1000 | Loss: 0.00003969
Iteration 352/1000 | Loss: 0.00003969
Iteration 353/1000 | Loss: 0.00003969
Iteration 354/1000 | Loss: 0.00003969
Iteration 355/1000 | Loss: 0.00003969
Iteration 356/1000 | Loss: 0.00003968
Iteration 357/1000 | Loss: 0.00003968
Iteration 358/1000 | Loss: 0.00003968
Iteration 359/1000 | Loss: 0.00003968
Iteration 360/1000 | Loss: 0.00003968
Iteration 361/1000 | Loss: 0.00003968
Iteration 362/1000 | Loss: 0.00003968
Iteration 363/1000 | Loss: 0.00003968
Iteration 364/1000 | Loss: 0.00003968
Iteration 365/1000 | Loss: 0.00003967
Iteration 366/1000 | Loss: 0.00003967
Iteration 367/1000 | Loss: 0.00003967
Iteration 368/1000 | Loss: 0.00003967
Iteration 369/1000 | Loss: 0.00003967
Iteration 370/1000 | Loss: 0.00003967
Iteration 371/1000 | Loss: 0.00003967
Iteration 372/1000 | Loss: 0.00003967
Iteration 373/1000 | Loss: 0.00003967
Iteration 374/1000 | Loss: 0.00003966
Iteration 375/1000 | Loss: 0.00003966
Iteration 376/1000 | Loss: 0.00003966
Iteration 377/1000 | Loss: 0.00003966
Iteration 378/1000 | Loss: 0.00003966
Iteration 379/1000 | Loss: 0.00003966
Iteration 380/1000 | Loss: 0.00003966
Iteration 381/1000 | Loss: 0.00003966
Iteration 382/1000 | Loss: 0.00003966
Iteration 383/1000 | Loss: 0.00003966
Iteration 384/1000 | Loss: 0.00003966
Iteration 385/1000 | Loss: 0.00003965
Iteration 386/1000 | Loss: 0.00003965
Iteration 387/1000 | Loss: 0.00003965
Iteration 388/1000 | Loss: 0.00003965
Iteration 389/1000 | Loss: 0.00003965
Iteration 390/1000 | Loss: 0.00003965
Iteration 391/1000 | Loss: 0.00003965
Iteration 392/1000 | Loss: 0.00003965
Iteration 393/1000 | Loss: 0.00003965
Iteration 394/1000 | Loss: 0.00003965
Iteration 395/1000 | Loss: 0.00003965
Iteration 396/1000 | Loss: 0.00003964
Iteration 397/1000 | Loss: 0.00003964
Iteration 398/1000 | Loss: 0.00003964
Iteration 399/1000 | Loss: 0.00003964
Iteration 400/1000 | Loss: 0.00003963
Iteration 401/1000 | Loss: 0.00003963
Iteration 402/1000 | Loss: 0.00003963
Iteration 403/1000 | Loss: 0.00003963
Iteration 404/1000 | Loss: 0.00003962
Iteration 405/1000 | Loss: 0.00003962
Iteration 406/1000 | Loss: 0.00003962
Iteration 407/1000 | Loss: 0.00003962
Iteration 408/1000 | Loss: 0.00003961
Iteration 409/1000 | Loss: 0.00003961
Iteration 410/1000 | Loss: 0.00003961
Iteration 411/1000 | Loss: 0.00003960
Iteration 412/1000 | Loss: 0.00003960
Iteration 413/1000 | Loss: 0.00003960
Iteration 414/1000 | Loss: 0.00003960
Iteration 415/1000 | Loss: 0.00003960
Iteration 416/1000 | Loss: 0.00003960
Iteration 417/1000 | Loss: 0.00003960
Iteration 418/1000 | Loss: 0.00003960
Iteration 419/1000 | Loss: 0.00003960
Iteration 420/1000 | Loss: 0.00003960
Iteration 421/1000 | Loss: 0.00003960
Iteration 422/1000 | Loss: 0.00003960
Iteration 423/1000 | Loss: 0.00003960
Iteration 424/1000 | Loss: 0.00003959
Iteration 425/1000 | Loss: 0.00003959
Iteration 426/1000 | Loss: 0.00003959
Iteration 427/1000 | Loss: 0.00003959
Iteration 428/1000 | Loss: 0.00003959
Iteration 429/1000 | Loss: 0.00003959
Iteration 430/1000 | Loss: 0.00003959
Iteration 431/1000 | Loss: 0.00003958
Iteration 432/1000 | Loss: 0.00051637
Iteration 433/1000 | Loss: 0.00014446
Iteration 434/1000 | Loss: 0.00004067
Iteration 435/1000 | Loss: 0.00003985
Iteration 436/1000 | Loss: 0.00003967
Iteration 437/1000 | Loss: 0.00003965
Iteration 438/1000 | Loss: 0.00003965
Iteration 439/1000 | Loss: 0.00003963
Iteration 440/1000 | Loss: 0.00003963
Iteration 441/1000 | Loss: 0.00003962
Iteration 442/1000 | Loss: 0.00003961
Iteration 443/1000 | Loss: 0.00003961
Iteration 444/1000 | Loss: 0.00003961
Iteration 445/1000 | Loss: 0.00003961
Iteration 446/1000 | Loss: 0.00003961
Iteration 447/1000 | Loss: 0.00003961
Iteration 448/1000 | Loss: 0.00003960
Iteration 449/1000 | Loss: 0.00003960
Iteration 450/1000 | Loss: 0.00003960
Iteration 451/1000 | Loss: 0.00003960
Iteration 452/1000 | Loss: 0.00003960
Iteration 453/1000 | Loss: 0.00003960
Iteration 454/1000 | Loss: 0.00003960
Iteration 455/1000 | Loss: 0.00003959
Iteration 456/1000 | Loss: 0.00003959
Iteration 457/1000 | Loss: 0.00003959
Iteration 458/1000 | Loss: 0.00003959
Iteration 459/1000 | Loss: 0.00003959
Iteration 460/1000 | Loss: 0.00003959
Iteration 461/1000 | Loss: 0.00003959
Iteration 462/1000 | Loss: 0.00003959
Iteration 463/1000 | Loss: 0.00003959
Iteration 464/1000 | Loss: 0.00003959
Iteration 465/1000 | Loss: 0.00003959
Iteration 466/1000 | Loss: 0.00003958
Iteration 467/1000 | Loss: 0.00003958
Iteration 468/1000 | Loss: 0.00003958
Iteration 469/1000 | Loss: 0.00003958
Iteration 470/1000 | Loss: 0.00003958
Iteration 471/1000 | Loss: 0.00003958
Iteration 472/1000 | Loss: 0.00003958
Iteration 473/1000 | Loss: 0.00003958
Iteration 474/1000 | Loss: 0.00003958
Iteration 475/1000 | Loss: 0.00003958
Iteration 476/1000 | Loss: 0.00003958
Iteration 477/1000 | Loss: 0.00003958
Iteration 478/1000 | Loss: 0.00003958
Iteration 479/1000 | Loss: 0.00003958
Iteration 480/1000 | Loss: 0.00003958
Iteration 481/1000 | Loss: 0.00003958
Iteration 482/1000 | Loss: 0.00003957
Iteration 483/1000 | Loss: 0.00003957
Iteration 484/1000 | Loss: 0.00003957
Iteration 485/1000 | Loss: 0.00003957
Iteration 486/1000 | Loss: 0.00003957
Iteration 487/1000 | Loss: 0.00003957
Iteration 488/1000 | Loss: 0.00003957
Iteration 489/1000 | Loss: 0.00003957
Iteration 490/1000 | Loss: 0.00003957
Iteration 491/1000 | Loss: 0.00003956
Iteration 492/1000 | Loss: 0.00003956
Iteration 493/1000 | Loss: 0.00003956
Iteration 494/1000 | Loss: 0.00003956
Iteration 495/1000 | Loss: 0.00003956
Iteration 496/1000 | Loss: 0.00003956
Iteration 497/1000 | Loss: 0.00003956
Iteration 498/1000 | Loss: 0.00003956
Iteration 499/1000 | Loss: 0.00003956
Iteration 500/1000 | Loss: 0.00003956
Iteration 501/1000 | Loss: 0.00003956
Iteration 502/1000 | Loss: 0.00003956
Iteration 503/1000 | Loss: 0.00003956
Iteration 504/1000 | Loss: 0.00003956
Iteration 505/1000 | Loss: 0.00003956
Iteration 506/1000 | Loss: 0.00003956
Iteration 507/1000 | Loss: 0.00003956
Iteration 508/1000 | Loss: 0.00003956
Iteration 509/1000 | Loss: 0.00003956
Iteration 510/1000 | Loss: 0.00003955
Iteration 511/1000 | Loss: 0.00003955
Iteration 512/1000 | Loss: 0.00003955
Iteration 513/1000 | Loss: 0.00003955
Iteration 514/1000 | Loss: 0.00003955
Iteration 515/1000 | Loss: 0.00003955
Iteration 516/1000 | Loss: 0.00003955
Iteration 517/1000 | Loss: 0.00003955
Iteration 518/1000 | Loss: 0.00003955
Iteration 519/1000 | Loss: 0.00003955
Iteration 520/1000 | Loss: 0.00003955
Iteration 521/1000 | Loss: 0.00003955
Iteration 522/1000 | Loss: 0.00003955
Iteration 523/1000 | Loss: 0.00003955
Iteration 524/1000 | Loss: 0.00003954
Iteration 525/1000 | Loss: 0.00003954
Iteration 526/1000 | Loss: 0.00003954
Iteration 527/1000 | Loss: 0.00003954
Iteration 528/1000 | Loss: 0.00003954
Iteration 529/1000 | Loss: 0.00003954
Iteration 530/1000 | Loss: 0.00003954
Iteration 531/1000 | Loss: 0.00003954
Iteration 532/1000 | Loss: 0.00003954
Iteration 533/1000 | Loss: 0.00003954
Iteration 534/1000 | Loss: 0.00003954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 534. Stopping optimization.
Last 5 losses: [3.9542108424939215e-05, 3.9542108424939215e-05, 3.9542108424939215e-05, 3.9542108424939215e-05, 3.9542108424939215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9542108424939215e-05

Optimization complete. Final v2v error: 4.059154987335205 mm

Highest mean error: 11.633145332336426 mm for frame 58

Lowest mean error: 3.0602381229400635 mm for frame 26

Saving results

Total time: 409.40845704078674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063577
Iteration 2/25 | Loss: 0.00332870
Iteration 3/25 | Loss: 0.00236791
Iteration 4/25 | Loss: 0.00198156
Iteration 5/25 | Loss: 0.00180243
Iteration 6/25 | Loss: 0.00151907
Iteration 7/25 | Loss: 0.00137864
Iteration 8/25 | Loss: 0.00128304
Iteration 9/25 | Loss: 0.00123369
Iteration 10/25 | Loss: 0.00121460
Iteration 11/25 | Loss: 0.00120677
Iteration 12/25 | Loss: 0.00118645
Iteration 13/25 | Loss: 0.00118537
Iteration 14/25 | Loss: 0.00118518
Iteration 15/25 | Loss: 0.00118505
Iteration 16/25 | Loss: 0.00118505
Iteration 17/25 | Loss: 0.00118504
Iteration 18/25 | Loss: 0.00118504
Iteration 19/25 | Loss: 0.00118504
Iteration 20/25 | Loss: 0.00118504
Iteration 21/25 | Loss: 0.00118504
Iteration 22/25 | Loss: 0.00118504
Iteration 23/25 | Loss: 0.00118503
Iteration 24/25 | Loss: 0.00118503
Iteration 25/25 | Loss: 0.00118503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41157722
Iteration 2/25 | Loss: 0.00060772
Iteration 3/25 | Loss: 0.00060772
Iteration 4/25 | Loss: 0.00060772
Iteration 5/25 | Loss: 0.00060772
Iteration 6/25 | Loss: 0.00060772
Iteration 7/25 | Loss: 0.00060772
Iteration 8/25 | Loss: 0.00060772
Iteration 9/25 | Loss: 0.00060772
Iteration 10/25 | Loss: 0.00060772
Iteration 11/25 | Loss: 0.00060772
Iteration 12/25 | Loss: 0.00060772
Iteration 13/25 | Loss: 0.00060772
Iteration 14/25 | Loss: 0.00060772
Iteration 15/25 | Loss: 0.00060772
Iteration 16/25 | Loss: 0.00060772
Iteration 17/25 | Loss: 0.00060772
Iteration 18/25 | Loss: 0.00060772
Iteration 19/25 | Loss: 0.00060772
Iteration 20/25 | Loss: 0.00060772
Iteration 21/25 | Loss: 0.00060772
Iteration 22/25 | Loss: 0.00060772
Iteration 23/25 | Loss: 0.00060772
Iteration 24/25 | Loss: 0.00060772
Iteration 25/25 | Loss: 0.00060772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060772
Iteration 2/1000 | Loss: 0.00003380
Iteration 3/1000 | Loss: 0.00002407
Iteration 4/1000 | Loss: 0.00002182
Iteration 5/1000 | Loss: 0.00002070
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001951
Iteration 8/1000 | Loss: 0.00001904
Iteration 9/1000 | Loss: 0.00001881
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001823
Iteration 12/1000 | Loss: 0.00001803
Iteration 13/1000 | Loss: 0.00001798
Iteration 14/1000 | Loss: 0.00001780
Iteration 15/1000 | Loss: 0.00001774
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001769
Iteration 19/1000 | Loss: 0.00001768
Iteration 20/1000 | Loss: 0.00001768
Iteration 21/1000 | Loss: 0.00001767
Iteration 22/1000 | Loss: 0.00001767
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001759
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00001757
Iteration 30/1000 | Loss: 0.00001757
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001750
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001747
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001745
Iteration 41/1000 | Loss: 0.00001744
Iteration 42/1000 | Loss: 0.00001744
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001740
Iteration 56/1000 | Loss: 0.00001740
Iteration 57/1000 | Loss: 0.00001739
Iteration 58/1000 | Loss: 0.00001738
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001738
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001728
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001726
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001725
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001725
Iteration 108/1000 | Loss: 0.00001725
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Iteration 120/1000 | Loss: 0.00001725
Iteration 121/1000 | Loss: 0.00001725
Iteration 122/1000 | Loss: 0.00001725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.7246067727683112e-05, 1.7246067727683112e-05, 1.7246067727683112e-05, 1.7246067727683112e-05, 1.7246067727683112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7246067727683112e-05

Optimization complete. Final v2v error: 3.4625322818756104 mm

Highest mean error: 4.033976078033447 mm for frame 76

Lowest mean error: 3.1939125061035156 mm for frame 9

Saving results

Total time: 57.37466597557068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402387
Iteration 2/25 | Loss: 0.00122620
Iteration 3/25 | Loss: 0.00116737
Iteration 4/25 | Loss: 0.00115759
Iteration 5/25 | Loss: 0.00115417
Iteration 6/25 | Loss: 0.00115342
Iteration 7/25 | Loss: 0.00115339
Iteration 8/25 | Loss: 0.00115339
Iteration 9/25 | Loss: 0.00115339
Iteration 10/25 | Loss: 0.00115339
Iteration 11/25 | Loss: 0.00115339
Iteration 12/25 | Loss: 0.00115339
Iteration 13/25 | Loss: 0.00115339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011533871293067932, 0.0011533871293067932, 0.0011533871293067932, 0.0011533871293067932, 0.0011533871293067932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011533871293067932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69441700
Iteration 2/25 | Loss: 0.00063349
Iteration 3/25 | Loss: 0.00063349
Iteration 4/25 | Loss: 0.00063349
Iteration 5/25 | Loss: 0.00063349
Iteration 6/25 | Loss: 0.00063349
Iteration 7/25 | Loss: 0.00063349
Iteration 8/25 | Loss: 0.00063349
Iteration 9/25 | Loss: 0.00063349
Iteration 10/25 | Loss: 0.00063349
Iteration 11/25 | Loss: 0.00063349
Iteration 12/25 | Loss: 0.00063349
Iteration 13/25 | Loss: 0.00063349
Iteration 14/25 | Loss: 0.00063349
Iteration 15/25 | Loss: 0.00063349
Iteration 16/25 | Loss: 0.00063349
Iteration 17/25 | Loss: 0.00063349
Iteration 18/25 | Loss: 0.00063349
Iteration 19/25 | Loss: 0.00063349
Iteration 20/25 | Loss: 0.00063349
Iteration 21/25 | Loss: 0.00063349
Iteration 22/25 | Loss: 0.00063349
Iteration 23/25 | Loss: 0.00063349
Iteration 24/25 | Loss: 0.00063349
Iteration 25/25 | Loss: 0.00063349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063349
Iteration 2/1000 | Loss: 0.00002263
Iteration 3/1000 | Loss: 0.00001667
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001279
Iteration 7/1000 | Loss: 0.00001247
Iteration 8/1000 | Loss: 0.00001225
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001218
Iteration 11/1000 | Loss: 0.00001218
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001184
Iteration 15/1000 | Loss: 0.00001181
Iteration 16/1000 | Loss: 0.00001176
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001172
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001166
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001156
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001153
Iteration 30/1000 | Loss: 0.00001153
Iteration 31/1000 | Loss: 0.00001153
Iteration 32/1000 | Loss: 0.00001153
Iteration 33/1000 | Loss: 0.00001152
Iteration 34/1000 | Loss: 0.00001152
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001151
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001150
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001148
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001147
Iteration 45/1000 | Loss: 0.00001147
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001146
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001144
Iteration 50/1000 | Loss: 0.00001144
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001143
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001136
Iteration 65/1000 | Loss: 0.00001136
Iteration 66/1000 | Loss: 0.00001136
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001135
Iteration 69/1000 | Loss: 0.00001135
Iteration 70/1000 | Loss: 0.00001135
Iteration 71/1000 | Loss: 0.00001135
Iteration 72/1000 | Loss: 0.00001135
Iteration 73/1000 | Loss: 0.00001135
Iteration 74/1000 | Loss: 0.00001135
Iteration 75/1000 | Loss: 0.00001135
Iteration 76/1000 | Loss: 0.00001134
Iteration 77/1000 | Loss: 0.00001133
Iteration 78/1000 | Loss: 0.00001133
Iteration 79/1000 | Loss: 0.00001133
Iteration 80/1000 | Loss: 0.00001133
Iteration 81/1000 | Loss: 0.00001133
Iteration 82/1000 | Loss: 0.00001133
Iteration 83/1000 | Loss: 0.00001133
Iteration 84/1000 | Loss: 0.00001133
Iteration 85/1000 | Loss: 0.00001133
Iteration 86/1000 | Loss: 0.00001132
Iteration 87/1000 | Loss: 0.00001132
Iteration 88/1000 | Loss: 0.00001132
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001131
Iteration 91/1000 | Loss: 0.00001131
Iteration 92/1000 | Loss: 0.00001130
Iteration 93/1000 | Loss: 0.00001130
Iteration 94/1000 | Loss: 0.00001129
Iteration 95/1000 | Loss: 0.00001129
Iteration 96/1000 | Loss: 0.00001129
Iteration 97/1000 | Loss: 0.00001129
Iteration 98/1000 | Loss: 0.00001128
Iteration 99/1000 | Loss: 0.00001128
Iteration 100/1000 | Loss: 0.00001128
Iteration 101/1000 | Loss: 0.00001128
Iteration 102/1000 | Loss: 0.00001128
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001127
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001124
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001123
Iteration 117/1000 | Loss: 0.00001123
Iteration 118/1000 | Loss: 0.00001123
Iteration 119/1000 | Loss: 0.00001123
Iteration 120/1000 | Loss: 0.00001123
Iteration 121/1000 | Loss: 0.00001123
Iteration 122/1000 | Loss: 0.00001123
Iteration 123/1000 | Loss: 0.00001123
Iteration 124/1000 | Loss: 0.00001122
Iteration 125/1000 | Loss: 0.00001122
Iteration 126/1000 | Loss: 0.00001122
Iteration 127/1000 | Loss: 0.00001122
Iteration 128/1000 | Loss: 0.00001122
Iteration 129/1000 | Loss: 0.00001122
Iteration 130/1000 | Loss: 0.00001122
Iteration 131/1000 | Loss: 0.00001122
Iteration 132/1000 | Loss: 0.00001122
Iteration 133/1000 | Loss: 0.00001122
Iteration 134/1000 | Loss: 0.00001122
Iteration 135/1000 | Loss: 0.00001122
Iteration 136/1000 | Loss: 0.00001122
Iteration 137/1000 | Loss: 0.00001122
Iteration 138/1000 | Loss: 0.00001122
Iteration 139/1000 | Loss: 0.00001122
Iteration 140/1000 | Loss: 0.00001121
Iteration 141/1000 | Loss: 0.00001121
Iteration 142/1000 | Loss: 0.00001121
Iteration 143/1000 | Loss: 0.00001121
Iteration 144/1000 | Loss: 0.00001121
Iteration 145/1000 | Loss: 0.00001121
Iteration 146/1000 | Loss: 0.00001120
Iteration 147/1000 | Loss: 0.00001120
Iteration 148/1000 | Loss: 0.00001120
Iteration 149/1000 | Loss: 0.00001120
Iteration 150/1000 | Loss: 0.00001120
Iteration 151/1000 | Loss: 0.00001119
Iteration 152/1000 | Loss: 0.00001119
Iteration 153/1000 | Loss: 0.00001119
Iteration 154/1000 | Loss: 0.00001119
Iteration 155/1000 | Loss: 0.00001119
Iteration 156/1000 | Loss: 0.00001119
Iteration 157/1000 | Loss: 0.00001119
Iteration 158/1000 | Loss: 0.00001118
Iteration 159/1000 | Loss: 0.00001118
Iteration 160/1000 | Loss: 0.00001118
Iteration 161/1000 | Loss: 0.00001118
Iteration 162/1000 | Loss: 0.00001118
Iteration 163/1000 | Loss: 0.00001117
Iteration 164/1000 | Loss: 0.00001117
Iteration 165/1000 | Loss: 0.00001117
Iteration 166/1000 | Loss: 0.00001117
Iteration 167/1000 | Loss: 0.00001117
Iteration 168/1000 | Loss: 0.00001117
Iteration 169/1000 | Loss: 0.00001117
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001116
Iteration 172/1000 | Loss: 0.00001116
Iteration 173/1000 | Loss: 0.00001116
Iteration 174/1000 | Loss: 0.00001116
Iteration 175/1000 | Loss: 0.00001116
Iteration 176/1000 | Loss: 0.00001116
Iteration 177/1000 | Loss: 0.00001116
Iteration 178/1000 | Loss: 0.00001116
Iteration 179/1000 | Loss: 0.00001116
Iteration 180/1000 | Loss: 0.00001116
Iteration 181/1000 | Loss: 0.00001115
Iteration 182/1000 | Loss: 0.00001115
Iteration 183/1000 | Loss: 0.00001115
Iteration 184/1000 | Loss: 0.00001115
Iteration 185/1000 | Loss: 0.00001115
Iteration 186/1000 | Loss: 0.00001115
Iteration 187/1000 | Loss: 0.00001115
Iteration 188/1000 | Loss: 0.00001115
Iteration 189/1000 | Loss: 0.00001115
Iteration 190/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.1154299500049092e-05, 1.1154299500049092e-05, 1.1154299500049092e-05, 1.1154299500049092e-05, 1.1154299500049092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1154299500049092e-05

Optimization complete. Final v2v error: 2.8600611686706543 mm

Highest mean error: 3.18390154838562 mm for frame 98

Lowest mean error: 2.6303184032440186 mm for frame 10

Saving results

Total time: 40.37813711166382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802039
Iteration 2/25 | Loss: 0.00124495
Iteration 3/25 | Loss: 0.00117281
Iteration 4/25 | Loss: 0.00116627
Iteration 5/25 | Loss: 0.00116427
Iteration 6/25 | Loss: 0.00116427
Iteration 7/25 | Loss: 0.00116427
Iteration 8/25 | Loss: 0.00116427
Iteration 9/25 | Loss: 0.00116427
Iteration 10/25 | Loss: 0.00116427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011642660247161984, 0.0011642660247161984, 0.0011642660247161984, 0.0011642660247161984, 0.0011642660247161984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011642660247161984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46336091
Iteration 2/25 | Loss: 0.00065898
Iteration 3/25 | Loss: 0.00065897
Iteration 4/25 | Loss: 0.00065897
Iteration 5/25 | Loss: 0.00065897
Iteration 6/25 | Loss: 0.00065897
Iteration 7/25 | Loss: 0.00065897
Iteration 8/25 | Loss: 0.00065897
Iteration 9/25 | Loss: 0.00065897
Iteration 10/25 | Loss: 0.00065897
Iteration 11/25 | Loss: 0.00065897
Iteration 12/25 | Loss: 0.00065897
Iteration 13/25 | Loss: 0.00065897
Iteration 14/25 | Loss: 0.00065897
Iteration 15/25 | Loss: 0.00065897
Iteration 16/25 | Loss: 0.00065897
Iteration 17/25 | Loss: 0.00065897
Iteration 18/25 | Loss: 0.00065897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000658967939671129, 0.000658967939671129, 0.000658967939671129, 0.000658967939671129, 0.000658967939671129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000658967939671129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065897
Iteration 2/1000 | Loss: 0.00002378
Iteration 3/1000 | Loss: 0.00001556
Iteration 4/1000 | Loss: 0.00001334
Iteration 5/1000 | Loss: 0.00001237
Iteration 6/1000 | Loss: 0.00001183
Iteration 7/1000 | Loss: 0.00001154
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001146
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001132
Iteration 12/1000 | Loss: 0.00001107
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001102
Iteration 15/1000 | Loss: 0.00001101
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001096
Iteration 18/1000 | Loss: 0.00001092
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001087
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001086
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001085
Iteration 26/1000 | Loss: 0.00001085
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001080
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001077
Iteration 44/1000 | Loss: 0.00001077
Iteration 45/1000 | Loss: 0.00001077
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001077
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001077
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001077
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001077
Iteration 63/1000 | Loss: 0.00001077
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.0767392268462572e-05, 1.0767392268462572e-05, 1.0767392268462572e-05, 1.0767392268462572e-05, 1.0767392268462572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0767392268462572e-05

Optimization complete. Final v2v error: 2.8140387535095215 mm

Highest mean error: 2.9145374298095703 mm for frame 108

Lowest mean error: 2.6629748344421387 mm for frame 158

Saving results

Total time: 27.881207942962646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945526
Iteration 2/25 | Loss: 0.00170669
Iteration 3/25 | Loss: 0.00136110
Iteration 4/25 | Loss: 0.00132311
Iteration 5/25 | Loss: 0.00130246
Iteration 6/25 | Loss: 0.00130254
Iteration 7/25 | Loss: 0.00133194
Iteration 8/25 | Loss: 0.00133354
Iteration 9/25 | Loss: 0.00133497
Iteration 10/25 | Loss: 0.00131312
Iteration 11/25 | Loss: 0.00127756
Iteration 12/25 | Loss: 0.00127440
Iteration 13/25 | Loss: 0.00127353
Iteration 14/25 | Loss: 0.00127314
Iteration 15/25 | Loss: 0.00127285
Iteration 16/25 | Loss: 0.00127364
Iteration 17/25 | Loss: 0.00134737
Iteration 18/25 | Loss: 0.00124861
Iteration 19/25 | Loss: 0.00124462
Iteration 20/25 | Loss: 0.00124412
Iteration 21/25 | Loss: 0.00124398
Iteration 22/25 | Loss: 0.00124396
Iteration 23/25 | Loss: 0.00124396
Iteration 24/25 | Loss: 0.00124395
Iteration 25/25 | Loss: 0.00124395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.01570368
Iteration 2/25 | Loss: 0.00065115
Iteration 3/25 | Loss: 0.00065113
Iteration 4/25 | Loss: 0.00065113
Iteration 5/25 | Loss: 0.00065113
Iteration 6/25 | Loss: 0.00065113
Iteration 7/25 | Loss: 0.00065113
Iteration 8/25 | Loss: 0.00065113
Iteration 9/25 | Loss: 0.00065113
Iteration 10/25 | Loss: 0.00065113
Iteration 11/25 | Loss: 0.00065113
Iteration 12/25 | Loss: 0.00065113
Iteration 13/25 | Loss: 0.00065113
Iteration 14/25 | Loss: 0.00065113
Iteration 15/25 | Loss: 0.00065113
Iteration 16/25 | Loss: 0.00065113
Iteration 17/25 | Loss: 0.00065113
Iteration 18/25 | Loss: 0.00065113
Iteration 19/25 | Loss: 0.00065113
Iteration 20/25 | Loss: 0.00065113
Iteration 21/25 | Loss: 0.00065113
Iteration 22/25 | Loss: 0.00065113
Iteration 23/25 | Loss: 0.00065113
Iteration 24/25 | Loss: 0.00065113
Iteration 25/25 | Loss: 0.00065113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065113
Iteration 2/1000 | Loss: 0.00003496
Iteration 3/1000 | Loss: 0.00002492
Iteration 4/1000 | Loss: 0.00002084
Iteration 5/1000 | Loss: 0.00001981
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00087648
Iteration 8/1000 | Loss: 0.00001997
Iteration 9/1000 | Loss: 0.00001886
Iteration 10/1000 | Loss: 0.00001843
Iteration 11/1000 | Loss: 0.00001817
Iteration 12/1000 | Loss: 0.00001816
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001808
Iteration 15/1000 | Loss: 0.00001807
Iteration 16/1000 | Loss: 0.00001788
Iteration 17/1000 | Loss: 0.00001782
Iteration 18/1000 | Loss: 0.00001780
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001761
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001752
Iteration 29/1000 | Loss: 0.00001751
Iteration 30/1000 | Loss: 0.00001750
Iteration 31/1000 | Loss: 0.00001750
Iteration 32/1000 | Loss: 0.00001750
Iteration 33/1000 | Loss: 0.00001749
Iteration 34/1000 | Loss: 0.00001749
Iteration 35/1000 | Loss: 0.00001749
Iteration 36/1000 | Loss: 0.00001748
Iteration 37/1000 | Loss: 0.00001748
Iteration 38/1000 | Loss: 0.00001746
Iteration 39/1000 | Loss: 0.00001746
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001743
Iteration 42/1000 | Loss: 0.00001743
Iteration 43/1000 | Loss: 0.00001743
Iteration 44/1000 | Loss: 0.00001743
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001742
Iteration 48/1000 | Loss: 0.00001742
Iteration 49/1000 | Loss: 0.00001742
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001741
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001736
Iteration 59/1000 | Loss: 0.00001735
Iteration 60/1000 | Loss: 0.00001735
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001730
Iteration 66/1000 | Loss: 0.00001730
Iteration 67/1000 | Loss: 0.00001730
Iteration 68/1000 | Loss: 0.00001730
Iteration 69/1000 | Loss: 0.00001730
Iteration 70/1000 | Loss: 0.00001730
Iteration 71/1000 | Loss: 0.00001729
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001728
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001722
Iteration 118/1000 | Loss: 0.00001722
Iteration 119/1000 | Loss: 0.00001722
Iteration 120/1000 | Loss: 0.00001722
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001722
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001719
Iteration 145/1000 | Loss: 0.00001719
Iteration 146/1000 | Loss: 0.00001719
Iteration 147/1000 | Loss: 0.00001719
Iteration 148/1000 | Loss: 0.00001719
Iteration 149/1000 | Loss: 0.00001719
Iteration 150/1000 | Loss: 0.00001719
Iteration 151/1000 | Loss: 0.00001719
Iteration 152/1000 | Loss: 0.00001718
Iteration 153/1000 | Loss: 0.00001718
Iteration 154/1000 | Loss: 0.00001718
Iteration 155/1000 | Loss: 0.00001718
Iteration 156/1000 | Loss: 0.00001718
Iteration 157/1000 | Loss: 0.00001717
Iteration 158/1000 | Loss: 0.00001717
Iteration 159/1000 | Loss: 0.00001717
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001715
Iteration 166/1000 | Loss: 0.00001715
Iteration 167/1000 | Loss: 0.00001715
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001715
Iteration 174/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.7149146515293978e-05, 1.7149146515293978e-05, 1.7149146515293978e-05, 1.7149146515293978e-05, 1.7149146515293978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7149146515293978e-05

Optimization complete. Final v2v error: 3.434587001800537 mm

Highest mean error: 4.243990898132324 mm for frame 85

Lowest mean error: 2.877403736114502 mm for frame 28

Saving results

Total time: 69.3871922492981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490038
Iteration 2/25 | Loss: 0.00172817
Iteration 3/25 | Loss: 0.00129440
Iteration 4/25 | Loss: 0.00123557
Iteration 5/25 | Loss: 0.00123001
Iteration 6/25 | Loss: 0.00122924
Iteration 7/25 | Loss: 0.00122924
Iteration 8/25 | Loss: 0.00122924
Iteration 9/25 | Loss: 0.00122924
Iteration 10/25 | Loss: 0.00122924
Iteration 11/25 | Loss: 0.00122924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012292360188439488, 0.0012292360188439488, 0.0012292360188439488, 0.0012292360188439488, 0.0012292360188439488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012292360188439488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52996278
Iteration 2/25 | Loss: 0.00053816
Iteration 3/25 | Loss: 0.00053815
Iteration 4/25 | Loss: 0.00053815
Iteration 5/25 | Loss: 0.00053815
Iteration 6/25 | Loss: 0.00053815
Iteration 7/25 | Loss: 0.00053815
Iteration 8/25 | Loss: 0.00053815
Iteration 9/25 | Loss: 0.00053815
Iteration 10/25 | Loss: 0.00053815
Iteration 11/25 | Loss: 0.00053815
Iteration 12/25 | Loss: 0.00053815
Iteration 13/25 | Loss: 0.00053815
Iteration 14/25 | Loss: 0.00053815
Iteration 15/25 | Loss: 0.00053815
Iteration 16/25 | Loss: 0.00053815
Iteration 17/25 | Loss: 0.00053815
Iteration 18/25 | Loss: 0.00053815
Iteration 19/25 | Loss: 0.00053815
Iteration 20/25 | Loss: 0.00053815
Iteration 21/25 | Loss: 0.00053815
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005381504888646305, 0.0005381504888646305, 0.0005381504888646305, 0.0005381504888646305, 0.0005381504888646305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005381504888646305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053815
Iteration 2/1000 | Loss: 0.00003828
Iteration 3/1000 | Loss: 0.00002569
Iteration 4/1000 | Loss: 0.00002312
Iteration 5/1000 | Loss: 0.00002193
Iteration 6/1000 | Loss: 0.00002091
Iteration 7/1000 | Loss: 0.00002035
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001956
Iteration 10/1000 | Loss: 0.00001937
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001892
Iteration 15/1000 | Loss: 0.00001885
Iteration 16/1000 | Loss: 0.00001884
Iteration 17/1000 | Loss: 0.00001880
Iteration 18/1000 | Loss: 0.00001876
Iteration 19/1000 | Loss: 0.00001872
Iteration 20/1000 | Loss: 0.00001872
Iteration 21/1000 | Loss: 0.00001872
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001872
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001871
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001868
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001867
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001861
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001859
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001857
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001854
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001854
Iteration 93/1000 | Loss: 0.00001854
Iteration 94/1000 | Loss: 0.00001854
Iteration 95/1000 | Loss: 0.00001854
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001853
Iteration 102/1000 | Loss: 0.00001853
Iteration 103/1000 | Loss: 0.00001853
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001852
Iteration 106/1000 | Loss: 0.00001852
Iteration 107/1000 | Loss: 0.00001852
Iteration 108/1000 | Loss: 0.00001852
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001851
Iteration 111/1000 | Loss: 0.00001851
Iteration 112/1000 | Loss: 0.00001851
Iteration 113/1000 | Loss: 0.00001851
Iteration 114/1000 | Loss: 0.00001851
Iteration 115/1000 | Loss: 0.00001851
Iteration 116/1000 | Loss: 0.00001851
Iteration 117/1000 | Loss: 0.00001851
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001850
Iteration 125/1000 | Loss: 0.00001850
Iteration 126/1000 | Loss: 0.00001850
Iteration 127/1000 | Loss: 0.00001850
Iteration 128/1000 | Loss: 0.00001850
Iteration 129/1000 | Loss: 0.00001850
Iteration 130/1000 | Loss: 0.00001849
Iteration 131/1000 | Loss: 0.00001849
Iteration 132/1000 | Loss: 0.00001849
Iteration 133/1000 | Loss: 0.00001849
Iteration 134/1000 | Loss: 0.00001849
Iteration 135/1000 | Loss: 0.00001849
Iteration 136/1000 | Loss: 0.00001849
Iteration 137/1000 | Loss: 0.00001849
Iteration 138/1000 | Loss: 0.00001848
Iteration 139/1000 | Loss: 0.00001848
Iteration 140/1000 | Loss: 0.00001848
Iteration 141/1000 | Loss: 0.00001848
Iteration 142/1000 | Loss: 0.00001848
Iteration 143/1000 | Loss: 0.00001848
Iteration 144/1000 | Loss: 0.00001848
Iteration 145/1000 | Loss: 0.00001848
Iteration 146/1000 | Loss: 0.00001848
Iteration 147/1000 | Loss: 0.00001848
Iteration 148/1000 | Loss: 0.00001848
Iteration 149/1000 | Loss: 0.00001848
Iteration 150/1000 | Loss: 0.00001847
Iteration 151/1000 | Loss: 0.00001847
Iteration 152/1000 | Loss: 0.00001847
Iteration 153/1000 | Loss: 0.00001847
Iteration 154/1000 | Loss: 0.00001847
Iteration 155/1000 | Loss: 0.00001846
Iteration 156/1000 | Loss: 0.00001846
Iteration 157/1000 | Loss: 0.00001846
Iteration 158/1000 | Loss: 0.00001846
Iteration 159/1000 | Loss: 0.00001846
Iteration 160/1000 | Loss: 0.00001846
Iteration 161/1000 | Loss: 0.00001846
Iteration 162/1000 | Loss: 0.00001846
Iteration 163/1000 | Loss: 0.00001846
Iteration 164/1000 | Loss: 0.00001845
Iteration 165/1000 | Loss: 0.00001845
Iteration 166/1000 | Loss: 0.00001845
Iteration 167/1000 | Loss: 0.00001845
Iteration 168/1000 | Loss: 0.00001845
Iteration 169/1000 | Loss: 0.00001845
Iteration 170/1000 | Loss: 0.00001845
Iteration 171/1000 | Loss: 0.00001845
Iteration 172/1000 | Loss: 0.00001845
Iteration 173/1000 | Loss: 0.00001845
Iteration 174/1000 | Loss: 0.00001845
Iteration 175/1000 | Loss: 0.00001845
Iteration 176/1000 | Loss: 0.00001844
Iteration 177/1000 | Loss: 0.00001844
Iteration 178/1000 | Loss: 0.00001844
Iteration 179/1000 | Loss: 0.00001844
Iteration 180/1000 | Loss: 0.00001844
Iteration 181/1000 | Loss: 0.00001844
Iteration 182/1000 | Loss: 0.00001844
Iteration 183/1000 | Loss: 0.00001844
Iteration 184/1000 | Loss: 0.00001844
Iteration 185/1000 | Loss: 0.00001844
Iteration 186/1000 | Loss: 0.00001844
Iteration 187/1000 | Loss: 0.00001844
Iteration 188/1000 | Loss: 0.00001844
Iteration 189/1000 | Loss: 0.00001844
Iteration 190/1000 | Loss: 0.00001844
Iteration 191/1000 | Loss: 0.00001844
Iteration 192/1000 | Loss: 0.00001844
Iteration 193/1000 | Loss: 0.00001843
Iteration 194/1000 | Loss: 0.00001843
Iteration 195/1000 | Loss: 0.00001843
Iteration 196/1000 | Loss: 0.00001843
Iteration 197/1000 | Loss: 0.00001843
Iteration 198/1000 | Loss: 0.00001843
Iteration 199/1000 | Loss: 0.00001843
Iteration 200/1000 | Loss: 0.00001843
Iteration 201/1000 | Loss: 0.00001843
Iteration 202/1000 | Loss: 0.00001843
Iteration 203/1000 | Loss: 0.00001843
Iteration 204/1000 | Loss: 0.00001843
Iteration 205/1000 | Loss: 0.00001843
Iteration 206/1000 | Loss: 0.00001843
Iteration 207/1000 | Loss: 0.00001843
Iteration 208/1000 | Loss: 0.00001843
Iteration 209/1000 | Loss: 0.00001843
Iteration 210/1000 | Loss: 0.00001843
Iteration 211/1000 | Loss: 0.00001843
Iteration 212/1000 | Loss: 0.00001843
Iteration 213/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.8431610442348756e-05, 1.8431610442348756e-05, 1.8431610442348756e-05, 1.8431610442348756e-05, 1.8431610442348756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8431610442348756e-05

Optimization complete. Final v2v error: 3.5918145179748535 mm

Highest mean error: 4.092380046844482 mm for frame 133

Lowest mean error: 3.2068586349487305 mm for frame 4

Saving results

Total time: 46.31284284591675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600357
Iteration 2/25 | Loss: 0.00131428
Iteration 3/25 | Loss: 0.00122346
Iteration 4/25 | Loss: 0.00120836
Iteration 5/25 | Loss: 0.00120345
Iteration 6/25 | Loss: 0.00120337
Iteration 7/25 | Loss: 0.00120337
Iteration 8/25 | Loss: 0.00120337
Iteration 9/25 | Loss: 0.00120337
Iteration 10/25 | Loss: 0.00120337
Iteration 11/25 | Loss: 0.00120337
Iteration 12/25 | Loss: 0.00120337
Iteration 13/25 | Loss: 0.00120337
Iteration 14/25 | Loss: 0.00120335
Iteration 15/25 | Loss: 0.00120335
Iteration 16/25 | Loss: 0.00120335
Iteration 17/25 | Loss: 0.00120335
Iteration 18/25 | Loss: 0.00120335
Iteration 19/25 | Loss: 0.00120335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012033467646688223, 0.0012033467646688223, 0.0012033467646688223, 0.0012033467646688223, 0.0012033467646688223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012033467646688223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.25042224
Iteration 2/25 | Loss: 0.00061189
Iteration 3/25 | Loss: 0.00061189
Iteration 4/25 | Loss: 0.00061189
Iteration 5/25 | Loss: 0.00061189
Iteration 6/25 | Loss: 0.00061189
Iteration 7/25 | Loss: 0.00061189
Iteration 8/25 | Loss: 0.00061189
Iteration 9/25 | Loss: 0.00061189
Iteration 10/25 | Loss: 0.00061189
Iteration 11/25 | Loss: 0.00061189
Iteration 12/25 | Loss: 0.00061189
Iteration 13/25 | Loss: 0.00061189
Iteration 14/25 | Loss: 0.00061189
Iteration 15/25 | Loss: 0.00061189
Iteration 16/25 | Loss: 0.00061189
Iteration 17/25 | Loss: 0.00061189
Iteration 18/25 | Loss: 0.00061189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006118876044638455, 0.0006118876044638455, 0.0006118876044638455, 0.0006118876044638455, 0.0006118876044638455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006118876044638455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061189
Iteration 2/1000 | Loss: 0.00003836
Iteration 3/1000 | Loss: 0.00002612
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002172
Iteration 7/1000 | Loss: 0.00002137
Iteration 8/1000 | Loss: 0.00002113
Iteration 9/1000 | Loss: 0.00002071
Iteration 10/1000 | Loss: 0.00002047
Iteration 11/1000 | Loss: 0.00002032
Iteration 12/1000 | Loss: 0.00002018
Iteration 13/1000 | Loss: 0.00002007
Iteration 14/1000 | Loss: 0.00001997
Iteration 15/1000 | Loss: 0.00001996
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001988
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001978
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001976
Iteration 24/1000 | Loss: 0.00001976
Iteration 25/1000 | Loss: 0.00001975
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001974
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001969
Iteration 30/1000 | Loss: 0.00001967
Iteration 31/1000 | Loss: 0.00001966
Iteration 32/1000 | Loss: 0.00001965
Iteration 33/1000 | Loss: 0.00001964
Iteration 34/1000 | Loss: 0.00001964
Iteration 35/1000 | Loss: 0.00001964
Iteration 36/1000 | Loss: 0.00001963
Iteration 37/1000 | Loss: 0.00001963
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001958
Iteration 40/1000 | Loss: 0.00001957
Iteration 41/1000 | Loss: 0.00001957
Iteration 42/1000 | Loss: 0.00001957
Iteration 43/1000 | Loss: 0.00001957
Iteration 44/1000 | Loss: 0.00001956
Iteration 45/1000 | Loss: 0.00001956
Iteration 46/1000 | Loss: 0.00001956
Iteration 47/1000 | Loss: 0.00001956
Iteration 48/1000 | Loss: 0.00001956
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001956
Iteration 51/1000 | Loss: 0.00001955
Iteration 52/1000 | Loss: 0.00001955
Iteration 53/1000 | Loss: 0.00001955
Iteration 54/1000 | Loss: 0.00001955
Iteration 55/1000 | Loss: 0.00001955
Iteration 56/1000 | Loss: 0.00001954
Iteration 57/1000 | Loss: 0.00001954
Iteration 58/1000 | Loss: 0.00001954
Iteration 59/1000 | Loss: 0.00001953
Iteration 60/1000 | Loss: 0.00001953
Iteration 61/1000 | Loss: 0.00001953
Iteration 62/1000 | Loss: 0.00001952
Iteration 63/1000 | Loss: 0.00001952
Iteration 64/1000 | Loss: 0.00001952
Iteration 65/1000 | Loss: 0.00001952
Iteration 66/1000 | Loss: 0.00001952
Iteration 67/1000 | Loss: 0.00001952
Iteration 68/1000 | Loss: 0.00001952
Iteration 69/1000 | Loss: 0.00001951
Iteration 70/1000 | Loss: 0.00001951
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00001951
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001951
Iteration 75/1000 | Loss: 0.00001951
Iteration 76/1000 | Loss: 0.00001950
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001948
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001948
Iteration 92/1000 | Loss: 0.00001948
Iteration 93/1000 | Loss: 0.00001948
Iteration 94/1000 | Loss: 0.00001948
Iteration 95/1000 | Loss: 0.00001948
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001947
Iteration 98/1000 | Loss: 0.00001947
Iteration 99/1000 | Loss: 0.00001947
Iteration 100/1000 | Loss: 0.00001947
Iteration 101/1000 | Loss: 0.00001946
Iteration 102/1000 | Loss: 0.00001946
Iteration 103/1000 | Loss: 0.00001946
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001945
Iteration 107/1000 | Loss: 0.00001945
Iteration 108/1000 | Loss: 0.00001944
Iteration 109/1000 | Loss: 0.00001944
Iteration 110/1000 | Loss: 0.00001944
Iteration 111/1000 | Loss: 0.00001944
Iteration 112/1000 | Loss: 0.00001944
Iteration 113/1000 | Loss: 0.00001944
Iteration 114/1000 | Loss: 0.00001944
Iteration 115/1000 | Loss: 0.00001944
Iteration 116/1000 | Loss: 0.00001944
Iteration 117/1000 | Loss: 0.00001943
Iteration 118/1000 | Loss: 0.00001943
Iteration 119/1000 | Loss: 0.00001943
Iteration 120/1000 | Loss: 0.00001943
Iteration 121/1000 | Loss: 0.00001943
Iteration 122/1000 | Loss: 0.00001943
Iteration 123/1000 | Loss: 0.00001943
Iteration 124/1000 | Loss: 0.00001943
Iteration 125/1000 | Loss: 0.00001943
Iteration 126/1000 | Loss: 0.00001942
Iteration 127/1000 | Loss: 0.00001942
Iteration 128/1000 | Loss: 0.00001942
Iteration 129/1000 | Loss: 0.00001942
Iteration 130/1000 | Loss: 0.00001942
Iteration 131/1000 | Loss: 0.00001942
Iteration 132/1000 | Loss: 0.00001942
Iteration 133/1000 | Loss: 0.00001942
Iteration 134/1000 | Loss: 0.00001942
Iteration 135/1000 | Loss: 0.00001942
Iteration 136/1000 | Loss: 0.00001942
Iteration 137/1000 | Loss: 0.00001942
Iteration 138/1000 | Loss: 0.00001942
Iteration 139/1000 | Loss: 0.00001942
Iteration 140/1000 | Loss: 0.00001941
Iteration 141/1000 | Loss: 0.00001941
Iteration 142/1000 | Loss: 0.00001941
Iteration 143/1000 | Loss: 0.00001941
Iteration 144/1000 | Loss: 0.00001941
Iteration 145/1000 | Loss: 0.00001941
Iteration 146/1000 | Loss: 0.00001941
Iteration 147/1000 | Loss: 0.00001941
Iteration 148/1000 | Loss: 0.00001940
Iteration 149/1000 | Loss: 0.00001940
Iteration 150/1000 | Loss: 0.00001940
Iteration 151/1000 | Loss: 0.00001940
Iteration 152/1000 | Loss: 0.00001940
Iteration 153/1000 | Loss: 0.00001939
Iteration 154/1000 | Loss: 0.00001939
Iteration 155/1000 | Loss: 0.00001939
Iteration 156/1000 | Loss: 0.00001939
Iteration 157/1000 | Loss: 0.00001939
Iteration 158/1000 | Loss: 0.00001939
Iteration 159/1000 | Loss: 0.00001939
Iteration 160/1000 | Loss: 0.00001939
Iteration 161/1000 | Loss: 0.00001938
Iteration 162/1000 | Loss: 0.00001938
Iteration 163/1000 | Loss: 0.00001938
Iteration 164/1000 | Loss: 0.00001938
Iteration 165/1000 | Loss: 0.00001938
Iteration 166/1000 | Loss: 0.00001938
Iteration 167/1000 | Loss: 0.00001938
Iteration 168/1000 | Loss: 0.00001938
Iteration 169/1000 | Loss: 0.00001938
Iteration 170/1000 | Loss: 0.00001938
Iteration 171/1000 | Loss: 0.00001938
Iteration 172/1000 | Loss: 0.00001938
Iteration 173/1000 | Loss: 0.00001938
Iteration 174/1000 | Loss: 0.00001938
Iteration 175/1000 | Loss: 0.00001938
Iteration 176/1000 | Loss: 0.00001937
Iteration 177/1000 | Loss: 0.00001937
Iteration 178/1000 | Loss: 0.00001937
Iteration 179/1000 | Loss: 0.00001937
Iteration 180/1000 | Loss: 0.00001937
Iteration 181/1000 | Loss: 0.00001937
Iteration 182/1000 | Loss: 0.00001937
Iteration 183/1000 | Loss: 0.00001937
Iteration 184/1000 | Loss: 0.00001937
Iteration 185/1000 | Loss: 0.00001937
Iteration 186/1000 | Loss: 0.00001937
Iteration 187/1000 | Loss: 0.00001937
Iteration 188/1000 | Loss: 0.00001937
Iteration 189/1000 | Loss: 0.00001937
Iteration 190/1000 | Loss: 0.00001936
Iteration 191/1000 | Loss: 0.00001936
Iteration 192/1000 | Loss: 0.00001936
Iteration 193/1000 | Loss: 0.00001936
Iteration 194/1000 | Loss: 0.00001936
Iteration 195/1000 | Loss: 0.00001936
Iteration 196/1000 | Loss: 0.00001936
Iteration 197/1000 | Loss: 0.00001936
Iteration 198/1000 | Loss: 0.00001936
Iteration 199/1000 | Loss: 0.00001936
Iteration 200/1000 | Loss: 0.00001936
Iteration 201/1000 | Loss: 0.00001936
Iteration 202/1000 | Loss: 0.00001936
Iteration 203/1000 | Loss: 0.00001936
Iteration 204/1000 | Loss: 0.00001936
Iteration 205/1000 | Loss: 0.00001936
Iteration 206/1000 | Loss: 0.00001935
Iteration 207/1000 | Loss: 0.00001935
Iteration 208/1000 | Loss: 0.00001935
Iteration 209/1000 | Loss: 0.00001935
Iteration 210/1000 | Loss: 0.00001935
Iteration 211/1000 | Loss: 0.00001935
Iteration 212/1000 | Loss: 0.00001935
Iteration 213/1000 | Loss: 0.00001935
Iteration 214/1000 | Loss: 0.00001935
Iteration 215/1000 | Loss: 0.00001935
Iteration 216/1000 | Loss: 0.00001935
Iteration 217/1000 | Loss: 0.00001935
Iteration 218/1000 | Loss: 0.00001935
Iteration 219/1000 | Loss: 0.00001934
Iteration 220/1000 | Loss: 0.00001934
Iteration 221/1000 | Loss: 0.00001934
Iteration 222/1000 | Loss: 0.00001934
Iteration 223/1000 | Loss: 0.00001934
Iteration 224/1000 | Loss: 0.00001934
Iteration 225/1000 | Loss: 0.00001934
Iteration 226/1000 | Loss: 0.00001934
Iteration 227/1000 | Loss: 0.00001934
Iteration 228/1000 | Loss: 0.00001934
Iteration 229/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.9342511222930625e-05, 1.9342511222930625e-05, 1.9342511222930625e-05, 1.9342511222930625e-05, 1.9342511222930625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9342511222930625e-05

Optimization complete. Final v2v error: 3.6009769439697266 mm

Highest mean error: 4.419122219085693 mm for frame 127

Lowest mean error: 3.023925542831421 mm for frame 6

Saving results

Total time: 45.03824472427368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532006
Iteration 2/25 | Loss: 0.00158410
Iteration 3/25 | Loss: 0.00133062
Iteration 4/25 | Loss: 0.00129934
Iteration 5/25 | Loss: 0.00129156
Iteration 6/25 | Loss: 0.00129022
Iteration 7/25 | Loss: 0.00129022
Iteration 8/25 | Loss: 0.00129022
Iteration 9/25 | Loss: 0.00129022
Iteration 10/25 | Loss: 0.00129022
Iteration 11/25 | Loss: 0.00129022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012902248417958617, 0.0012902248417958617, 0.0012902248417958617, 0.0012902248417958617, 0.0012902248417958617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012902248417958617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82540345
Iteration 2/25 | Loss: 0.00062406
Iteration 3/25 | Loss: 0.00062406
Iteration 4/25 | Loss: 0.00062406
Iteration 5/25 | Loss: 0.00062406
Iteration 6/25 | Loss: 0.00062406
Iteration 7/25 | Loss: 0.00062406
Iteration 8/25 | Loss: 0.00062406
Iteration 9/25 | Loss: 0.00062406
Iteration 10/25 | Loss: 0.00062406
Iteration 11/25 | Loss: 0.00062406
Iteration 12/25 | Loss: 0.00062406
Iteration 13/25 | Loss: 0.00062406
Iteration 14/25 | Loss: 0.00062406
Iteration 15/25 | Loss: 0.00062406
Iteration 16/25 | Loss: 0.00062406
Iteration 17/25 | Loss: 0.00062406
Iteration 18/25 | Loss: 0.00062406
Iteration 19/25 | Loss: 0.00062406
Iteration 20/25 | Loss: 0.00062406
Iteration 21/25 | Loss: 0.00062406
Iteration 22/25 | Loss: 0.00062406
Iteration 23/25 | Loss: 0.00062406
Iteration 24/25 | Loss: 0.00062406
Iteration 25/25 | Loss: 0.00062406

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062406
Iteration 2/1000 | Loss: 0.00005354
Iteration 3/1000 | Loss: 0.00003461
Iteration 4/1000 | Loss: 0.00003133
Iteration 5/1000 | Loss: 0.00002960
Iteration 6/1000 | Loss: 0.00002820
Iteration 7/1000 | Loss: 0.00002751
Iteration 8/1000 | Loss: 0.00002708
Iteration 9/1000 | Loss: 0.00002674
Iteration 10/1000 | Loss: 0.00002649
Iteration 11/1000 | Loss: 0.00002639
Iteration 12/1000 | Loss: 0.00002636
Iteration 13/1000 | Loss: 0.00002632
Iteration 14/1000 | Loss: 0.00002627
Iteration 15/1000 | Loss: 0.00002623
Iteration 16/1000 | Loss: 0.00002622
Iteration 17/1000 | Loss: 0.00002620
Iteration 18/1000 | Loss: 0.00002619
Iteration 19/1000 | Loss: 0.00002613
Iteration 20/1000 | Loss: 0.00002612
Iteration 21/1000 | Loss: 0.00002611
Iteration 22/1000 | Loss: 0.00002611
Iteration 23/1000 | Loss: 0.00002611
Iteration 24/1000 | Loss: 0.00002610
Iteration 25/1000 | Loss: 0.00002610
Iteration 26/1000 | Loss: 0.00002610
Iteration 27/1000 | Loss: 0.00002608
Iteration 28/1000 | Loss: 0.00002608
Iteration 29/1000 | Loss: 0.00002608
Iteration 30/1000 | Loss: 0.00002608
Iteration 31/1000 | Loss: 0.00002608
Iteration 32/1000 | Loss: 0.00002607
Iteration 33/1000 | Loss: 0.00002607
Iteration 34/1000 | Loss: 0.00002607
Iteration 35/1000 | Loss: 0.00002607
Iteration 36/1000 | Loss: 0.00002607
Iteration 37/1000 | Loss: 0.00002606
Iteration 38/1000 | Loss: 0.00002606
Iteration 39/1000 | Loss: 0.00002606
Iteration 40/1000 | Loss: 0.00002606
Iteration 41/1000 | Loss: 0.00002606
Iteration 42/1000 | Loss: 0.00002606
Iteration 43/1000 | Loss: 0.00002606
Iteration 44/1000 | Loss: 0.00002605
Iteration 45/1000 | Loss: 0.00002605
Iteration 46/1000 | Loss: 0.00002605
Iteration 47/1000 | Loss: 0.00002605
Iteration 48/1000 | Loss: 0.00002605
Iteration 49/1000 | Loss: 0.00002604
Iteration 50/1000 | Loss: 0.00002604
Iteration 51/1000 | Loss: 0.00002604
Iteration 52/1000 | Loss: 0.00002604
Iteration 53/1000 | Loss: 0.00002604
Iteration 54/1000 | Loss: 0.00002604
Iteration 55/1000 | Loss: 0.00002604
Iteration 56/1000 | Loss: 0.00002604
Iteration 57/1000 | Loss: 0.00002604
Iteration 58/1000 | Loss: 0.00002603
Iteration 59/1000 | Loss: 0.00002603
Iteration 60/1000 | Loss: 0.00002603
Iteration 61/1000 | Loss: 0.00002603
Iteration 62/1000 | Loss: 0.00002603
Iteration 63/1000 | Loss: 0.00002603
Iteration 64/1000 | Loss: 0.00002603
Iteration 65/1000 | Loss: 0.00002603
Iteration 66/1000 | Loss: 0.00002603
Iteration 67/1000 | Loss: 0.00002602
Iteration 68/1000 | Loss: 0.00002602
Iteration 69/1000 | Loss: 0.00002602
Iteration 70/1000 | Loss: 0.00002602
Iteration 71/1000 | Loss: 0.00002602
Iteration 72/1000 | Loss: 0.00002602
Iteration 73/1000 | Loss: 0.00002602
Iteration 74/1000 | Loss: 0.00002602
Iteration 75/1000 | Loss: 0.00002602
Iteration 76/1000 | Loss: 0.00002601
Iteration 77/1000 | Loss: 0.00002601
Iteration 78/1000 | Loss: 0.00002601
Iteration 79/1000 | Loss: 0.00002601
Iteration 80/1000 | Loss: 0.00002601
Iteration 81/1000 | Loss: 0.00002601
Iteration 82/1000 | Loss: 0.00002601
Iteration 83/1000 | Loss: 0.00002601
Iteration 84/1000 | Loss: 0.00002601
Iteration 85/1000 | Loss: 0.00002601
Iteration 86/1000 | Loss: 0.00002600
Iteration 87/1000 | Loss: 0.00002600
Iteration 88/1000 | Loss: 0.00002600
Iteration 89/1000 | Loss: 0.00002600
Iteration 90/1000 | Loss: 0.00002600
Iteration 91/1000 | Loss: 0.00002600
Iteration 92/1000 | Loss: 0.00002600
Iteration 93/1000 | Loss: 0.00002600
Iteration 94/1000 | Loss: 0.00002600
Iteration 95/1000 | Loss: 0.00002600
Iteration 96/1000 | Loss: 0.00002600
Iteration 97/1000 | Loss: 0.00002600
Iteration 98/1000 | Loss: 0.00002600
Iteration 99/1000 | Loss: 0.00002600
Iteration 100/1000 | Loss: 0.00002600
Iteration 101/1000 | Loss: 0.00002600
Iteration 102/1000 | Loss: 0.00002600
Iteration 103/1000 | Loss: 0.00002600
Iteration 104/1000 | Loss: 0.00002600
Iteration 105/1000 | Loss: 0.00002600
Iteration 106/1000 | Loss: 0.00002600
Iteration 107/1000 | Loss: 0.00002600
Iteration 108/1000 | Loss: 0.00002600
Iteration 109/1000 | Loss: 0.00002600
Iteration 110/1000 | Loss: 0.00002600
Iteration 111/1000 | Loss: 0.00002600
Iteration 112/1000 | Loss: 0.00002600
Iteration 113/1000 | Loss: 0.00002600
Iteration 114/1000 | Loss: 0.00002600
Iteration 115/1000 | Loss: 0.00002600
Iteration 116/1000 | Loss: 0.00002600
Iteration 117/1000 | Loss: 0.00002600
Iteration 118/1000 | Loss: 0.00002600
Iteration 119/1000 | Loss: 0.00002600
Iteration 120/1000 | Loss: 0.00002600
Iteration 121/1000 | Loss: 0.00002600
Iteration 122/1000 | Loss: 0.00002600
Iteration 123/1000 | Loss: 0.00002600
Iteration 124/1000 | Loss: 0.00002600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.6001340302173048e-05, 2.6001340302173048e-05, 2.6001340302173048e-05, 2.6001340302173048e-05, 2.6001340302173048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6001340302173048e-05

Optimization complete. Final v2v error: 4.256817817687988 mm

Highest mean error: 4.579576015472412 mm for frame 16

Lowest mean error: 3.9856863021850586 mm for frame 34

Saving results

Total time: 32.50389337539673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856163
Iteration 2/25 | Loss: 0.00130334
Iteration 3/25 | Loss: 0.00120270
Iteration 4/25 | Loss: 0.00118155
Iteration 5/25 | Loss: 0.00117600
Iteration 6/25 | Loss: 0.00117543
Iteration 7/25 | Loss: 0.00117543
Iteration 8/25 | Loss: 0.00117543
Iteration 9/25 | Loss: 0.00117543
Iteration 10/25 | Loss: 0.00117543
Iteration 11/25 | Loss: 0.00117543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001175428507849574, 0.001175428507849574, 0.001175428507849574, 0.001175428507849574, 0.001175428507849574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175428507849574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53317583
Iteration 2/25 | Loss: 0.00057529
Iteration 3/25 | Loss: 0.00057529
Iteration 4/25 | Loss: 0.00057529
Iteration 5/25 | Loss: 0.00057529
Iteration 6/25 | Loss: 0.00057529
Iteration 7/25 | Loss: 0.00057529
Iteration 8/25 | Loss: 0.00057529
Iteration 9/25 | Loss: 0.00057529
Iteration 10/25 | Loss: 0.00057529
Iteration 11/25 | Loss: 0.00057529
Iteration 12/25 | Loss: 0.00057529
Iteration 13/25 | Loss: 0.00057529
Iteration 14/25 | Loss: 0.00057529
Iteration 15/25 | Loss: 0.00057529
Iteration 16/25 | Loss: 0.00057529
Iteration 17/25 | Loss: 0.00057529
Iteration 18/25 | Loss: 0.00057529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005752881406806409, 0.0005752881406806409, 0.0005752881406806409, 0.0005752881406806409, 0.0005752881406806409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005752881406806409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057529
Iteration 2/1000 | Loss: 0.00003099
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001592
Iteration 7/1000 | Loss: 0.00001557
Iteration 8/1000 | Loss: 0.00001524
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00001506
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001494
Iteration 14/1000 | Loss: 0.00001474
Iteration 15/1000 | Loss: 0.00001470
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001464
Iteration 18/1000 | Loss: 0.00001461
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001459
Iteration 21/1000 | Loss: 0.00001458
Iteration 22/1000 | Loss: 0.00001458
Iteration 23/1000 | Loss: 0.00001457
Iteration 24/1000 | Loss: 0.00001454
Iteration 25/1000 | Loss: 0.00001453
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001442
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001435
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001434
Iteration 42/1000 | Loss: 0.00001434
Iteration 43/1000 | Loss: 0.00001434
Iteration 44/1000 | Loss: 0.00001434
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001430
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001429
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001425
Iteration 63/1000 | Loss: 0.00001425
Iteration 64/1000 | Loss: 0.00001425
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001419
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001414
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001409
Iteration 83/1000 | Loss: 0.00001409
Iteration 84/1000 | Loss: 0.00001408
Iteration 85/1000 | Loss: 0.00001408
Iteration 86/1000 | Loss: 0.00001407
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001406
Iteration 94/1000 | Loss: 0.00001405
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001403
Iteration 115/1000 | Loss: 0.00001403
Iteration 116/1000 | Loss: 0.00001403
Iteration 117/1000 | Loss: 0.00001403
Iteration 118/1000 | Loss: 0.00001403
Iteration 119/1000 | Loss: 0.00001403
Iteration 120/1000 | Loss: 0.00001403
Iteration 121/1000 | Loss: 0.00001403
Iteration 122/1000 | Loss: 0.00001403
Iteration 123/1000 | Loss: 0.00001403
Iteration 124/1000 | Loss: 0.00001403
Iteration 125/1000 | Loss: 0.00001403
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001402
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001401
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Iteration 133/1000 | Loss: 0.00001401
Iteration 134/1000 | Loss: 0.00001401
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001400
Iteration 137/1000 | Loss: 0.00001400
Iteration 138/1000 | Loss: 0.00001400
Iteration 139/1000 | Loss: 0.00001400
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001399
Iteration 147/1000 | Loss: 0.00001398
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Iteration 156/1000 | Loss: 0.00001398
Iteration 157/1000 | Loss: 0.00001397
Iteration 158/1000 | Loss: 0.00001397
Iteration 159/1000 | Loss: 0.00001397
Iteration 160/1000 | Loss: 0.00001397
Iteration 161/1000 | Loss: 0.00001397
Iteration 162/1000 | Loss: 0.00001396
Iteration 163/1000 | Loss: 0.00001396
Iteration 164/1000 | Loss: 0.00001396
Iteration 165/1000 | Loss: 0.00001396
Iteration 166/1000 | Loss: 0.00001396
Iteration 167/1000 | Loss: 0.00001396
Iteration 168/1000 | Loss: 0.00001396
Iteration 169/1000 | Loss: 0.00001396
Iteration 170/1000 | Loss: 0.00001396
Iteration 171/1000 | Loss: 0.00001396
Iteration 172/1000 | Loss: 0.00001396
Iteration 173/1000 | Loss: 0.00001396
Iteration 174/1000 | Loss: 0.00001396
Iteration 175/1000 | Loss: 0.00001396
Iteration 176/1000 | Loss: 0.00001396
Iteration 177/1000 | Loss: 0.00001396
Iteration 178/1000 | Loss: 0.00001396
Iteration 179/1000 | Loss: 0.00001396
Iteration 180/1000 | Loss: 0.00001396
Iteration 181/1000 | Loss: 0.00001396
Iteration 182/1000 | Loss: 0.00001396
Iteration 183/1000 | Loss: 0.00001396
Iteration 184/1000 | Loss: 0.00001396
Iteration 185/1000 | Loss: 0.00001396
Iteration 186/1000 | Loss: 0.00001396
Iteration 187/1000 | Loss: 0.00001396
Iteration 188/1000 | Loss: 0.00001396
Iteration 189/1000 | Loss: 0.00001396
Iteration 190/1000 | Loss: 0.00001396
Iteration 191/1000 | Loss: 0.00001396
Iteration 192/1000 | Loss: 0.00001396
Iteration 193/1000 | Loss: 0.00001396
Iteration 194/1000 | Loss: 0.00001396
Iteration 195/1000 | Loss: 0.00001396
Iteration 196/1000 | Loss: 0.00001396
Iteration 197/1000 | Loss: 0.00001396
Iteration 198/1000 | Loss: 0.00001396
Iteration 199/1000 | Loss: 0.00001396
Iteration 200/1000 | Loss: 0.00001396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.3955425856693182e-05, 1.3955425856693182e-05, 1.3955425856693182e-05, 1.3955425856693182e-05, 1.3955425856693182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3955425856693182e-05

Optimization complete. Final v2v error: 3.151747226715088 mm

Highest mean error: 3.5669455528259277 mm for frame 83

Lowest mean error: 3.0384161472320557 mm for frame 35

Saving results

Total time: 40.39705467224121
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00623689
Iteration 2/25 | Loss: 0.00143359
Iteration 3/25 | Loss: 0.00128067
Iteration 4/25 | Loss: 0.00125126
Iteration 5/25 | Loss: 0.00123362
Iteration 6/25 | Loss: 0.00122343
Iteration 7/25 | Loss: 0.00121799
Iteration 8/25 | Loss: 0.00121705
Iteration 9/25 | Loss: 0.00121600
Iteration 10/25 | Loss: 0.00121913
Iteration 11/25 | Loss: 0.00121099
Iteration 12/25 | Loss: 0.00120922
Iteration 13/25 | Loss: 0.00120840
Iteration 14/25 | Loss: 0.00120762
Iteration 15/25 | Loss: 0.00120745
Iteration 16/25 | Loss: 0.00120745
Iteration 17/25 | Loss: 0.00120745
Iteration 18/25 | Loss: 0.00120744
Iteration 19/25 | Loss: 0.00120744
Iteration 20/25 | Loss: 0.00120744
Iteration 21/25 | Loss: 0.00120744
Iteration 22/25 | Loss: 0.00120744
Iteration 23/25 | Loss: 0.00120744
Iteration 24/25 | Loss: 0.00120744
Iteration 25/25 | Loss: 0.00120744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65637517
Iteration 2/25 | Loss: 0.00066755
Iteration 3/25 | Loss: 0.00066754
Iteration 4/25 | Loss: 0.00066754
Iteration 5/25 | Loss: 0.00066754
Iteration 6/25 | Loss: 0.00066754
Iteration 7/25 | Loss: 0.00066754
Iteration 8/25 | Loss: 0.00066754
Iteration 9/25 | Loss: 0.00066754
Iteration 10/25 | Loss: 0.00066754
Iteration 11/25 | Loss: 0.00066754
Iteration 12/25 | Loss: 0.00066754
Iteration 13/25 | Loss: 0.00066754
Iteration 14/25 | Loss: 0.00066754
Iteration 15/25 | Loss: 0.00066754
Iteration 16/25 | Loss: 0.00066754
Iteration 17/25 | Loss: 0.00066754
Iteration 18/25 | Loss: 0.00066754
Iteration 19/25 | Loss: 0.00066754
Iteration 20/25 | Loss: 0.00066754
Iteration 21/25 | Loss: 0.00066754
Iteration 22/25 | Loss: 0.00066754
Iteration 23/25 | Loss: 0.00066754
Iteration 24/25 | Loss: 0.00066754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006675370968878269, 0.0006675370968878269, 0.0006675370968878269, 0.0006675370968878269, 0.0006675370968878269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006675370968878269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066754
Iteration 2/1000 | Loss: 0.00003336
Iteration 3/1000 | Loss: 0.00002513
Iteration 4/1000 | Loss: 0.00002335
Iteration 5/1000 | Loss: 0.00002212
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002035
Iteration 8/1000 | Loss: 0.00001990
Iteration 9/1000 | Loss: 0.00001958
Iteration 10/1000 | Loss: 0.00001927
Iteration 11/1000 | Loss: 0.00001906
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001877
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001871
Iteration 19/1000 | Loss: 0.00001870
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00001861
Iteration 22/1000 | Loss: 0.00001861
Iteration 23/1000 | Loss: 0.00001860
Iteration 24/1000 | Loss: 0.00001860
Iteration 25/1000 | Loss: 0.00001859
Iteration 26/1000 | Loss: 0.00001859
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001858
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001856
Iteration 33/1000 | Loss: 0.00001856
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001855
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001854
Iteration 38/1000 | Loss: 0.00001854
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001853
Iteration 41/1000 | Loss: 0.00001853
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001846
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001845
Iteration 62/1000 | Loss: 0.00001845
Iteration 63/1000 | Loss: 0.00001845
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001843
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001842
Iteration 72/1000 | Loss: 0.00001842
Iteration 73/1000 | Loss: 0.00001842
Iteration 74/1000 | Loss: 0.00001842
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001836
Iteration 102/1000 | Loss: 0.00001836
Iteration 103/1000 | Loss: 0.00001836
Iteration 104/1000 | Loss: 0.00001836
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001835
Iteration 107/1000 | Loss: 0.00001835
Iteration 108/1000 | Loss: 0.00001835
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001834
Iteration 111/1000 | Loss: 0.00001834
Iteration 112/1000 | Loss: 0.00001834
Iteration 113/1000 | Loss: 0.00001834
Iteration 114/1000 | Loss: 0.00001833
Iteration 115/1000 | Loss: 0.00001833
Iteration 116/1000 | Loss: 0.00001833
Iteration 117/1000 | Loss: 0.00001833
Iteration 118/1000 | Loss: 0.00001833
Iteration 119/1000 | Loss: 0.00001833
Iteration 120/1000 | Loss: 0.00001833
Iteration 121/1000 | Loss: 0.00001833
Iteration 122/1000 | Loss: 0.00001833
Iteration 123/1000 | Loss: 0.00001833
Iteration 124/1000 | Loss: 0.00001833
Iteration 125/1000 | Loss: 0.00001833
Iteration 126/1000 | Loss: 0.00001833
Iteration 127/1000 | Loss: 0.00001833
Iteration 128/1000 | Loss: 0.00001833
Iteration 129/1000 | Loss: 0.00001832
Iteration 130/1000 | Loss: 0.00001832
Iteration 131/1000 | Loss: 0.00001832
Iteration 132/1000 | Loss: 0.00001832
Iteration 133/1000 | Loss: 0.00001832
Iteration 134/1000 | Loss: 0.00001832
Iteration 135/1000 | Loss: 0.00001832
Iteration 136/1000 | Loss: 0.00001832
Iteration 137/1000 | Loss: 0.00001831
Iteration 138/1000 | Loss: 0.00001831
Iteration 139/1000 | Loss: 0.00001831
Iteration 140/1000 | Loss: 0.00001831
Iteration 141/1000 | Loss: 0.00001831
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001831
Iteration 144/1000 | Loss: 0.00001831
Iteration 145/1000 | Loss: 0.00001831
Iteration 146/1000 | Loss: 0.00001831
Iteration 147/1000 | Loss: 0.00001831
Iteration 148/1000 | Loss: 0.00001830
Iteration 149/1000 | Loss: 0.00001830
Iteration 150/1000 | Loss: 0.00001830
Iteration 151/1000 | Loss: 0.00001830
Iteration 152/1000 | Loss: 0.00001830
Iteration 153/1000 | Loss: 0.00001830
Iteration 154/1000 | Loss: 0.00001830
Iteration 155/1000 | Loss: 0.00001829
Iteration 156/1000 | Loss: 0.00001829
Iteration 157/1000 | Loss: 0.00001829
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001828
Iteration 164/1000 | Loss: 0.00001828
Iteration 165/1000 | Loss: 0.00001828
Iteration 166/1000 | Loss: 0.00001828
Iteration 167/1000 | Loss: 0.00001828
Iteration 168/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.8284959878656082e-05, 1.8284959878656082e-05, 1.8284959878656082e-05, 1.8284959878656082e-05, 1.8284959878656082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8284959878656082e-05

Optimization complete. Final v2v error: 3.510246992111206 mm

Highest mean error: 4.409399509429932 mm for frame 125

Lowest mean error: 2.982836961746216 mm for frame 69

Saving results

Total time: 64.49086570739746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038518
Iteration 2/25 | Loss: 0.00184004
Iteration 3/25 | Loss: 0.00219607
Iteration 4/25 | Loss: 0.00139559
Iteration 5/25 | Loss: 0.00127929
Iteration 6/25 | Loss: 0.00129534
Iteration 7/25 | Loss: 0.00127797
Iteration 8/25 | Loss: 0.00123505
Iteration 9/25 | Loss: 0.00121581
Iteration 10/25 | Loss: 0.00120408
Iteration 11/25 | Loss: 0.00119169
Iteration 12/25 | Loss: 0.00119170
Iteration 13/25 | Loss: 0.00118600
Iteration 14/25 | Loss: 0.00119053
Iteration 15/25 | Loss: 0.00118719
Iteration 16/25 | Loss: 0.00119348
Iteration 17/25 | Loss: 0.00118294
Iteration 18/25 | Loss: 0.00117726
Iteration 19/25 | Loss: 0.00117483
Iteration 20/25 | Loss: 0.00117410
Iteration 21/25 | Loss: 0.00117395
Iteration 22/25 | Loss: 0.00117395
Iteration 23/25 | Loss: 0.00117395
Iteration 24/25 | Loss: 0.00117395
Iteration 25/25 | Loss: 0.00117395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47964919
Iteration 2/25 | Loss: 0.00128216
Iteration 3/25 | Loss: 0.00128216
Iteration 4/25 | Loss: 0.00072038
Iteration 5/25 | Loss: 0.00072038
Iteration 6/25 | Loss: 0.00072037
Iteration 7/25 | Loss: 0.00072037
Iteration 8/25 | Loss: 0.00072037
Iteration 9/25 | Loss: 0.00072037
Iteration 10/25 | Loss: 0.00072037
Iteration 11/25 | Loss: 0.00072037
Iteration 12/25 | Loss: 0.00072037
Iteration 13/25 | Loss: 0.00072037
Iteration 14/25 | Loss: 0.00072037
Iteration 15/25 | Loss: 0.00072037
Iteration 16/25 | Loss: 0.00072037
Iteration 17/25 | Loss: 0.00072037
Iteration 18/25 | Loss: 0.00072037
Iteration 19/25 | Loss: 0.00072037
Iteration 20/25 | Loss: 0.00072037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000720373704098165, 0.000720373704098165, 0.000720373704098165, 0.000720373704098165, 0.000720373704098165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000720373704098165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072037
Iteration 2/1000 | Loss: 0.00033136
Iteration 3/1000 | Loss: 0.00081742
Iteration 4/1000 | Loss: 0.00154734
Iteration 5/1000 | Loss: 0.00008201
Iteration 6/1000 | Loss: 0.00025409
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001648
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00018840
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00036553
Iteration 13/1000 | Loss: 0.00025211
Iteration 14/1000 | Loss: 0.00006430
Iteration 15/1000 | Loss: 0.00007747
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001372
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001364
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001358
Iteration 28/1000 | Loss: 0.00001355
Iteration 29/1000 | Loss: 0.00001353
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001340
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001329
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00020565
Iteration 61/1000 | Loss: 0.00001369
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001310
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001307
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001307
Iteration 75/1000 | Loss: 0.00001307
Iteration 76/1000 | Loss: 0.00001307
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001306
Iteration 80/1000 | Loss: 0.00001306
Iteration 81/1000 | Loss: 0.00001306
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001306
Iteration 89/1000 | Loss: 0.00001306
Iteration 90/1000 | Loss: 0.00001306
Iteration 91/1000 | Loss: 0.00001306
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001305
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001305
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001303
Iteration 122/1000 | Loss: 0.00001303
Iteration 123/1000 | Loss: 0.00001303
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001302
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001302
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001301
Iteration 138/1000 | Loss: 0.00001301
Iteration 139/1000 | Loss: 0.00001301
Iteration 140/1000 | Loss: 0.00001301
Iteration 141/1000 | Loss: 0.00001301
Iteration 142/1000 | Loss: 0.00001301
Iteration 143/1000 | Loss: 0.00001300
Iteration 144/1000 | Loss: 0.00001300
Iteration 145/1000 | Loss: 0.00001300
Iteration 146/1000 | Loss: 0.00001300
Iteration 147/1000 | Loss: 0.00001300
Iteration 148/1000 | Loss: 0.00001300
Iteration 149/1000 | Loss: 0.00001300
Iteration 150/1000 | Loss: 0.00001300
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001299
Iteration 157/1000 | Loss: 0.00001299
Iteration 158/1000 | Loss: 0.00001299
Iteration 159/1000 | Loss: 0.00001298
Iteration 160/1000 | Loss: 0.00001298
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001298
Iteration 167/1000 | Loss: 0.00001298
Iteration 168/1000 | Loss: 0.00001298
Iteration 169/1000 | Loss: 0.00001298
Iteration 170/1000 | Loss: 0.00001297
Iteration 171/1000 | Loss: 0.00001297
Iteration 172/1000 | Loss: 0.00001297
Iteration 173/1000 | Loss: 0.00001297
Iteration 174/1000 | Loss: 0.00001297
Iteration 175/1000 | Loss: 0.00001297
Iteration 176/1000 | Loss: 0.00001297
Iteration 177/1000 | Loss: 0.00001297
Iteration 178/1000 | Loss: 0.00001297
Iteration 179/1000 | Loss: 0.00001297
Iteration 180/1000 | Loss: 0.00001296
Iteration 181/1000 | Loss: 0.00001296
Iteration 182/1000 | Loss: 0.00001296
Iteration 183/1000 | Loss: 0.00001296
Iteration 184/1000 | Loss: 0.00001296
Iteration 185/1000 | Loss: 0.00001296
Iteration 186/1000 | Loss: 0.00001296
Iteration 187/1000 | Loss: 0.00001296
Iteration 188/1000 | Loss: 0.00001296
Iteration 189/1000 | Loss: 0.00001296
Iteration 190/1000 | Loss: 0.00001296
Iteration 191/1000 | Loss: 0.00001296
Iteration 192/1000 | Loss: 0.00001296
Iteration 193/1000 | Loss: 0.00001296
Iteration 194/1000 | Loss: 0.00001296
Iteration 195/1000 | Loss: 0.00001295
Iteration 196/1000 | Loss: 0.00001295
Iteration 197/1000 | Loss: 0.00001295
Iteration 198/1000 | Loss: 0.00001295
Iteration 199/1000 | Loss: 0.00001295
Iteration 200/1000 | Loss: 0.00001295
Iteration 201/1000 | Loss: 0.00001295
Iteration 202/1000 | Loss: 0.00001295
Iteration 203/1000 | Loss: 0.00001295
Iteration 204/1000 | Loss: 0.00001295
Iteration 205/1000 | Loss: 0.00001295
Iteration 206/1000 | Loss: 0.00001295
Iteration 207/1000 | Loss: 0.00001295
Iteration 208/1000 | Loss: 0.00001295
Iteration 209/1000 | Loss: 0.00001294
Iteration 210/1000 | Loss: 0.00001294
Iteration 211/1000 | Loss: 0.00001294
Iteration 212/1000 | Loss: 0.00001294
Iteration 213/1000 | Loss: 0.00001294
Iteration 214/1000 | Loss: 0.00001294
Iteration 215/1000 | Loss: 0.00001294
Iteration 216/1000 | Loss: 0.00001294
Iteration 217/1000 | Loss: 0.00001294
Iteration 218/1000 | Loss: 0.00001294
Iteration 219/1000 | Loss: 0.00001294
Iteration 220/1000 | Loss: 0.00001293
Iteration 221/1000 | Loss: 0.00001293
Iteration 222/1000 | Loss: 0.00001293
Iteration 223/1000 | Loss: 0.00001293
Iteration 224/1000 | Loss: 0.00001293
Iteration 225/1000 | Loss: 0.00001293
Iteration 226/1000 | Loss: 0.00001293
Iteration 227/1000 | Loss: 0.00001293
Iteration 228/1000 | Loss: 0.00001293
Iteration 229/1000 | Loss: 0.00001293
Iteration 230/1000 | Loss: 0.00001292
Iteration 231/1000 | Loss: 0.00001292
Iteration 232/1000 | Loss: 0.00001292
Iteration 233/1000 | Loss: 0.00001292
Iteration 234/1000 | Loss: 0.00001292
Iteration 235/1000 | Loss: 0.00001292
Iteration 236/1000 | Loss: 0.00001292
Iteration 237/1000 | Loss: 0.00001292
Iteration 238/1000 | Loss: 0.00001292
Iteration 239/1000 | Loss: 0.00001292
Iteration 240/1000 | Loss: 0.00001292
Iteration 241/1000 | Loss: 0.00001292
Iteration 242/1000 | Loss: 0.00001292
Iteration 243/1000 | Loss: 0.00001292
Iteration 244/1000 | Loss: 0.00001291
Iteration 245/1000 | Loss: 0.00001291
Iteration 246/1000 | Loss: 0.00001291
Iteration 247/1000 | Loss: 0.00001291
Iteration 248/1000 | Loss: 0.00001291
Iteration 249/1000 | Loss: 0.00001291
Iteration 250/1000 | Loss: 0.00001291
Iteration 251/1000 | Loss: 0.00001290
Iteration 252/1000 | Loss: 0.00001290
Iteration 253/1000 | Loss: 0.00001290
Iteration 254/1000 | Loss: 0.00001290
Iteration 255/1000 | Loss: 0.00001290
Iteration 256/1000 | Loss: 0.00001290
Iteration 257/1000 | Loss: 0.00001290
Iteration 258/1000 | Loss: 0.00001290
Iteration 259/1000 | Loss: 0.00001290
Iteration 260/1000 | Loss: 0.00001290
Iteration 261/1000 | Loss: 0.00001289
Iteration 262/1000 | Loss: 0.00001289
Iteration 263/1000 | Loss: 0.00001289
Iteration 264/1000 | Loss: 0.00001289
Iteration 265/1000 | Loss: 0.00001289
Iteration 266/1000 | Loss: 0.00001289
Iteration 267/1000 | Loss: 0.00001289
Iteration 268/1000 | Loss: 0.00001289
Iteration 269/1000 | Loss: 0.00001289
Iteration 270/1000 | Loss: 0.00001289
Iteration 271/1000 | Loss: 0.00001289
Iteration 272/1000 | Loss: 0.00001289
Iteration 273/1000 | Loss: 0.00001289
Iteration 274/1000 | Loss: 0.00001289
Iteration 275/1000 | Loss: 0.00001289
Iteration 276/1000 | Loss: 0.00001289
Iteration 277/1000 | Loss: 0.00001289
Iteration 278/1000 | Loss: 0.00001289
Iteration 279/1000 | Loss: 0.00001289
Iteration 280/1000 | Loss: 0.00001289
Iteration 281/1000 | Loss: 0.00001289
Iteration 282/1000 | Loss: 0.00001289
Iteration 283/1000 | Loss: 0.00001289
Iteration 284/1000 | Loss: 0.00001289
Iteration 285/1000 | Loss: 0.00001289
Iteration 286/1000 | Loss: 0.00001289
Iteration 287/1000 | Loss: 0.00001289
Iteration 288/1000 | Loss: 0.00001289
Iteration 289/1000 | Loss: 0.00001289
Iteration 290/1000 | Loss: 0.00001289
Iteration 291/1000 | Loss: 0.00001289
Iteration 292/1000 | Loss: 0.00001289
Iteration 293/1000 | Loss: 0.00001289
Iteration 294/1000 | Loss: 0.00001289
Iteration 295/1000 | Loss: 0.00001289
Iteration 296/1000 | Loss: 0.00001289
Iteration 297/1000 | Loss: 0.00001289
Iteration 298/1000 | Loss: 0.00001289
Iteration 299/1000 | Loss: 0.00001289
Iteration 300/1000 | Loss: 0.00001289
Iteration 301/1000 | Loss: 0.00001289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.289429928874597e-05, 1.289429928874597e-05, 1.289429928874597e-05, 1.289429928874597e-05, 1.289429928874597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.289429928874597e-05

Optimization complete. Final v2v error: 3.0322208404541016 mm

Highest mean error: 3.820413112640381 mm for frame 71

Lowest mean error: 2.7278976440429688 mm for frame 22

Saving results

Total time: 87.99699878692627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795350
Iteration 2/25 | Loss: 0.00139979
Iteration 3/25 | Loss: 0.00120245
Iteration 4/25 | Loss: 0.00118759
Iteration 5/25 | Loss: 0.00118175
Iteration 6/25 | Loss: 0.00118046
Iteration 7/25 | Loss: 0.00118046
Iteration 8/25 | Loss: 0.00118046
Iteration 9/25 | Loss: 0.00118046
Iteration 10/25 | Loss: 0.00118046
Iteration 11/25 | Loss: 0.00118046
Iteration 12/25 | Loss: 0.00118046
Iteration 13/25 | Loss: 0.00118046
Iteration 14/25 | Loss: 0.00118046
Iteration 15/25 | Loss: 0.00118046
Iteration 16/25 | Loss: 0.00118046
Iteration 17/25 | Loss: 0.00118046
Iteration 18/25 | Loss: 0.00118046
Iteration 19/25 | Loss: 0.00118046
Iteration 20/25 | Loss: 0.00118046
Iteration 21/25 | Loss: 0.00118046
Iteration 22/25 | Loss: 0.00118046
Iteration 23/25 | Loss: 0.00118046
Iteration 24/25 | Loss: 0.00118046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001180459395982325, 0.001180459395982325, 0.001180459395982325, 0.001180459395982325, 0.001180459395982325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001180459395982325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27478147
Iteration 2/25 | Loss: 0.00068964
Iteration 3/25 | Loss: 0.00068964
Iteration 4/25 | Loss: 0.00068964
Iteration 5/25 | Loss: 0.00068963
Iteration 6/25 | Loss: 0.00068963
Iteration 7/25 | Loss: 0.00068963
Iteration 8/25 | Loss: 0.00068963
Iteration 9/25 | Loss: 0.00068963
Iteration 10/25 | Loss: 0.00068963
Iteration 11/25 | Loss: 0.00068963
Iteration 12/25 | Loss: 0.00068963
Iteration 13/25 | Loss: 0.00068963
Iteration 14/25 | Loss: 0.00068963
Iteration 15/25 | Loss: 0.00068963
Iteration 16/25 | Loss: 0.00068963
Iteration 17/25 | Loss: 0.00068963
Iteration 18/25 | Loss: 0.00068963
Iteration 19/25 | Loss: 0.00068963
Iteration 20/25 | Loss: 0.00068963
Iteration 21/25 | Loss: 0.00068963
Iteration 22/25 | Loss: 0.00068963
Iteration 23/25 | Loss: 0.00068963
Iteration 24/25 | Loss: 0.00068963
Iteration 25/25 | Loss: 0.00068963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068963
Iteration 2/1000 | Loss: 0.00005242
Iteration 3/1000 | Loss: 0.00002686
Iteration 4/1000 | Loss: 0.00002207
Iteration 5/1000 | Loss: 0.00001983
Iteration 6/1000 | Loss: 0.00001865
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001715
Iteration 9/1000 | Loss: 0.00001666
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001594
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001565
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001554
Iteration 17/1000 | Loss: 0.00001553
Iteration 18/1000 | Loss: 0.00001553
Iteration 19/1000 | Loss: 0.00001552
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001535
Iteration 24/1000 | Loss: 0.00001530
Iteration 25/1000 | Loss: 0.00001525
Iteration 26/1000 | Loss: 0.00001525
Iteration 27/1000 | Loss: 0.00001525
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001522
Iteration 30/1000 | Loss: 0.00001521
Iteration 31/1000 | Loss: 0.00001521
Iteration 32/1000 | Loss: 0.00001520
Iteration 33/1000 | Loss: 0.00001520
Iteration 34/1000 | Loss: 0.00001520
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001519
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001513
Iteration 39/1000 | Loss: 0.00001513
Iteration 40/1000 | Loss: 0.00001511
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001506
Iteration 44/1000 | Loss: 0.00001506
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001505
Iteration 47/1000 | Loss: 0.00001504
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001503
Iteration 50/1000 | Loss: 0.00001502
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001500
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001500
Iteration 62/1000 | Loss: 0.00001500
Iteration 63/1000 | Loss: 0.00001500
Iteration 64/1000 | Loss: 0.00001499
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001497
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001497
Iteration 76/1000 | Loss: 0.00001496
Iteration 77/1000 | Loss: 0.00001496
Iteration 78/1000 | Loss: 0.00001495
Iteration 79/1000 | Loss: 0.00001495
Iteration 80/1000 | Loss: 0.00001494
Iteration 81/1000 | Loss: 0.00001494
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001493
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001492
Iteration 88/1000 | Loss: 0.00001492
Iteration 89/1000 | Loss: 0.00001492
Iteration 90/1000 | Loss: 0.00001492
Iteration 91/1000 | Loss: 0.00001491
Iteration 92/1000 | Loss: 0.00001491
Iteration 93/1000 | Loss: 0.00001491
Iteration 94/1000 | Loss: 0.00001491
Iteration 95/1000 | Loss: 0.00001491
Iteration 96/1000 | Loss: 0.00001490
Iteration 97/1000 | Loss: 0.00001490
Iteration 98/1000 | Loss: 0.00001490
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001486
Iteration 114/1000 | Loss: 0.00001486
Iteration 115/1000 | Loss: 0.00001486
Iteration 116/1000 | Loss: 0.00001486
Iteration 117/1000 | Loss: 0.00001485
Iteration 118/1000 | Loss: 0.00001485
Iteration 119/1000 | Loss: 0.00001485
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001484
Iteration 122/1000 | Loss: 0.00001484
Iteration 123/1000 | Loss: 0.00001484
Iteration 124/1000 | Loss: 0.00001484
Iteration 125/1000 | Loss: 0.00001484
Iteration 126/1000 | Loss: 0.00001484
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001483
Iteration 129/1000 | Loss: 0.00001483
Iteration 130/1000 | Loss: 0.00001483
Iteration 131/1000 | Loss: 0.00001483
Iteration 132/1000 | Loss: 0.00001483
Iteration 133/1000 | Loss: 0.00001483
Iteration 134/1000 | Loss: 0.00001483
Iteration 135/1000 | Loss: 0.00001483
Iteration 136/1000 | Loss: 0.00001483
Iteration 137/1000 | Loss: 0.00001483
Iteration 138/1000 | Loss: 0.00001483
Iteration 139/1000 | Loss: 0.00001483
Iteration 140/1000 | Loss: 0.00001483
Iteration 141/1000 | Loss: 0.00001483
Iteration 142/1000 | Loss: 0.00001483
Iteration 143/1000 | Loss: 0.00001483
Iteration 144/1000 | Loss: 0.00001483
Iteration 145/1000 | Loss: 0.00001483
Iteration 146/1000 | Loss: 0.00001483
Iteration 147/1000 | Loss: 0.00001483
Iteration 148/1000 | Loss: 0.00001483
Iteration 149/1000 | Loss: 0.00001483
Iteration 150/1000 | Loss: 0.00001483
Iteration 151/1000 | Loss: 0.00001483
Iteration 152/1000 | Loss: 0.00001483
Iteration 153/1000 | Loss: 0.00001483
Iteration 154/1000 | Loss: 0.00001483
Iteration 155/1000 | Loss: 0.00001483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.4832806300546508e-05, 1.4832806300546508e-05, 1.4832806300546508e-05, 1.4832806300546508e-05, 1.4832806300546508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4832806300546508e-05

Optimization complete. Final v2v error: 3.164450168609619 mm

Highest mean error: 4.516151428222656 mm for frame 76

Lowest mean error: 2.7010016441345215 mm for frame 31

Saving results

Total time: 44.03610277175903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447337
Iteration 2/25 | Loss: 0.00125975
Iteration 3/25 | Loss: 0.00119677
Iteration 4/25 | Loss: 0.00118213
Iteration 5/25 | Loss: 0.00118043
Iteration 6/25 | Loss: 0.00118043
Iteration 7/25 | Loss: 0.00118043
Iteration 8/25 | Loss: 0.00118043
Iteration 9/25 | Loss: 0.00118043
Iteration 10/25 | Loss: 0.00118043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011804316891357303, 0.0011804316891357303, 0.0011804316891357303, 0.0011804316891357303, 0.0011804316891357303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011804316891357303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61291957
Iteration 2/25 | Loss: 0.00059473
Iteration 3/25 | Loss: 0.00059473
Iteration 4/25 | Loss: 0.00059473
Iteration 5/25 | Loss: 0.00059473
Iteration 6/25 | Loss: 0.00059473
Iteration 7/25 | Loss: 0.00059473
Iteration 8/25 | Loss: 0.00059473
Iteration 9/25 | Loss: 0.00059473
Iteration 10/25 | Loss: 0.00059473
Iteration 11/25 | Loss: 0.00059473
Iteration 12/25 | Loss: 0.00059473
Iteration 13/25 | Loss: 0.00059473
Iteration 14/25 | Loss: 0.00059473
Iteration 15/25 | Loss: 0.00059473
Iteration 16/25 | Loss: 0.00059473
Iteration 17/25 | Loss: 0.00059473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005947278114035726, 0.0005947278114035726, 0.0005947278114035726, 0.0005947278114035726, 0.0005947278114035726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005947278114035726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059473
Iteration 2/1000 | Loss: 0.00002655
Iteration 3/1000 | Loss: 0.00001987
Iteration 4/1000 | Loss: 0.00001826
Iteration 5/1000 | Loss: 0.00001763
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001672
Iteration 8/1000 | Loss: 0.00001643
Iteration 9/1000 | Loss: 0.00001627
Iteration 10/1000 | Loss: 0.00001615
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001591
Iteration 14/1000 | Loss: 0.00001584
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001568
Iteration 17/1000 | Loss: 0.00001568
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001566
Iteration 21/1000 | Loss: 0.00001566
Iteration 22/1000 | Loss: 0.00001565
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001562
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001555
Iteration 27/1000 | Loss: 0.00001553
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001551
Iteration 31/1000 | Loss: 0.00001550
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001547
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001546
Iteration 39/1000 | Loss: 0.00001546
Iteration 40/1000 | Loss: 0.00001545
Iteration 41/1000 | Loss: 0.00001545
Iteration 42/1000 | Loss: 0.00001544
Iteration 43/1000 | Loss: 0.00001544
Iteration 44/1000 | Loss: 0.00001543
Iteration 45/1000 | Loss: 0.00001543
Iteration 46/1000 | Loss: 0.00001542
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001539
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001539
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00001539
Iteration 57/1000 | Loss: 0.00001538
Iteration 58/1000 | Loss: 0.00001537
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001529
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001526
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001525
Iteration 72/1000 | Loss: 0.00001525
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001520
Iteration 100/1000 | Loss: 0.00001520
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001519
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001519
Iteration 116/1000 | Loss: 0.00001519
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001518
Iteration 123/1000 | Loss: 0.00001518
Iteration 124/1000 | Loss: 0.00001518
Iteration 125/1000 | Loss: 0.00001518
Iteration 126/1000 | Loss: 0.00001518
Iteration 127/1000 | Loss: 0.00001518
Iteration 128/1000 | Loss: 0.00001518
Iteration 129/1000 | Loss: 0.00001517
Iteration 130/1000 | Loss: 0.00001517
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001517
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001517
Iteration 137/1000 | Loss: 0.00001517
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001516
Iteration 147/1000 | Loss: 0.00001515
Iteration 148/1000 | Loss: 0.00001515
Iteration 149/1000 | Loss: 0.00001515
Iteration 150/1000 | Loss: 0.00001515
Iteration 151/1000 | Loss: 0.00001515
Iteration 152/1000 | Loss: 0.00001515
Iteration 153/1000 | Loss: 0.00001514
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001514
Iteration 157/1000 | Loss: 0.00001514
Iteration 158/1000 | Loss: 0.00001514
Iteration 159/1000 | Loss: 0.00001514
Iteration 160/1000 | Loss: 0.00001514
Iteration 161/1000 | Loss: 0.00001514
Iteration 162/1000 | Loss: 0.00001514
Iteration 163/1000 | Loss: 0.00001514
Iteration 164/1000 | Loss: 0.00001513
Iteration 165/1000 | Loss: 0.00001513
Iteration 166/1000 | Loss: 0.00001513
Iteration 167/1000 | Loss: 0.00001513
Iteration 168/1000 | Loss: 0.00001513
Iteration 169/1000 | Loss: 0.00001513
Iteration 170/1000 | Loss: 0.00001513
Iteration 171/1000 | Loss: 0.00001513
Iteration 172/1000 | Loss: 0.00001513
Iteration 173/1000 | Loss: 0.00001513
Iteration 174/1000 | Loss: 0.00001513
Iteration 175/1000 | Loss: 0.00001513
Iteration 176/1000 | Loss: 0.00001513
Iteration 177/1000 | Loss: 0.00001513
Iteration 178/1000 | Loss: 0.00001513
Iteration 179/1000 | Loss: 0.00001513
Iteration 180/1000 | Loss: 0.00001513
Iteration 181/1000 | Loss: 0.00001513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.5133662600419484e-05, 1.5133662600419484e-05, 1.5133662600419484e-05, 1.5133662600419484e-05, 1.5133662600419484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5133662600419484e-05

Optimization complete. Final v2v error: 3.287102460861206 mm

Highest mean error: 3.717686414718628 mm for frame 112

Lowest mean error: 3.1690001487731934 mm for frame 51

Saving results

Total time: 39.87244939804077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684845
Iteration 2/25 | Loss: 0.00168172
Iteration 3/25 | Loss: 0.00139072
Iteration 4/25 | Loss: 0.00134664
Iteration 5/25 | Loss: 0.00133983
Iteration 6/25 | Loss: 0.00132856
Iteration 7/25 | Loss: 0.00131012
Iteration 8/25 | Loss: 0.00128958
Iteration 9/25 | Loss: 0.00128378
Iteration 10/25 | Loss: 0.00127144
Iteration 11/25 | Loss: 0.00126649
Iteration 12/25 | Loss: 0.00126113
Iteration 13/25 | Loss: 0.00125628
Iteration 14/25 | Loss: 0.00125485
Iteration 15/25 | Loss: 0.00125351
Iteration 16/25 | Loss: 0.00125368
Iteration 17/25 | Loss: 0.00125372
Iteration 18/25 | Loss: 0.00125320
Iteration 19/25 | Loss: 0.00125291
Iteration 20/25 | Loss: 0.00125309
Iteration 21/25 | Loss: 0.00125309
Iteration 22/25 | Loss: 0.00125240
Iteration 23/25 | Loss: 0.00125249
Iteration 24/25 | Loss: 0.00125207
Iteration 25/25 | Loss: 0.00125262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37199283
Iteration 2/25 | Loss: 0.00070864
Iteration 3/25 | Loss: 0.00070860
Iteration 4/25 | Loss: 0.00070860
Iteration 5/25 | Loss: 0.00070860
Iteration 6/25 | Loss: 0.00070860
Iteration 7/25 | Loss: 0.00070860
Iteration 8/25 | Loss: 0.00070860
Iteration 9/25 | Loss: 0.00070860
Iteration 10/25 | Loss: 0.00070860
Iteration 11/25 | Loss: 0.00070860
Iteration 12/25 | Loss: 0.00070860
Iteration 13/25 | Loss: 0.00070860
Iteration 14/25 | Loss: 0.00070860
Iteration 15/25 | Loss: 0.00070860
Iteration 16/25 | Loss: 0.00070860
Iteration 17/25 | Loss: 0.00070860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007085970137268305, 0.0007085970137268305, 0.0007085970137268305, 0.0007085970137268305, 0.0007085970137268305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007085970137268305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070860
Iteration 2/1000 | Loss: 0.00005155
Iteration 3/1000 | Loss: 0.00003692
Iteration 4/1000 | Loss: 0.00003847
Iteration 5/1000 | Loss: 0.00003077
Iteration 6/1000 | Loss: 0.00003337
Iteration 7/1000 | Loss: 0.00004663
Iteration 8/1000 | Loss: 0.00004291
Iteration 9/1000 | Loss: 0.00003176
Iteration 10/1000 | Loss: 0.00002867
Iteration 11/1000 | Loss: 0.00004859
Iteration 12/1000 | Loss: 0.00004242
Iteration 13/1000 | Loss: 0.00002718
Iteration 14/1000 | Loss: 0.00002646
Iteration 15/1000 | Loss: 0.00002986
Iteration 16/1000 | Loss: 0.00002698
Iteration 17/1000 | Loss: 0.00002537
Iteration 18/1000 | Loss: 0.00002457
Iteration 19/1000 | Loss: 0.00003606
Iteration 20/1000 | Loss: 0.00003700
Iteration 21/1000 | Loss: 0.00003902
Iteration 22/1000 | Loss: 0.00006585
Iteration 23/1000 | Loss: 0.00003883
Iteration 24/1000 | Loss: 0.00005962
Iteration 25/1000 | Loss: 0.00003873
Iteration 26/1000 | Loss: 0.00004380
Iteration 27/1000 | Loss: 0.00004158
Iteration 28/1000 | Loss: 0.00003819
Iteration 29/1000 | Loss: 0.00002619
Iteration 30/1000 | Loss: 0.00002444
Iteration 31/1000 | Loss: 0.00002395
Iteration 32/1000 | Loss: 0.00002351
Iteration 33/1000 | Loss: 0.00002333
Iteration 34/1000 | Loss: 0.00002313
Iteration 35/1000 | Loss: 0.00002310
Iteration 36/1000 | Loss: 0.00002309
Iteration 37/1000 | Loss: 0.00002309
Iteration 38/1000 | Loss: 0.00002308
Iteration 39/1000 | Loss: 0.00002308
Iteration 40/1000 | Loss: 0.00002307
Iteration 41/1000 | Loss: 0.00002304
Iteration 42/1000 | Loss: 0.00002304
Iteration 43/1000 | Loss: 0.00002303
Iteration 44/1000 | Loss: 0.00002302
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002301
Iteration 47/1000 | Loss: 0.00002301
Iteration 48/1000 | Loss: 0.00002300
Iteration 49/1000 | Loss: 0.00002297
Iteration 50/1000 | Loss: 0.00002297
Iteration 51/1000 | Loss: 0.00002296
Iteration 52/1000 | Loss: 0.00002296
Iteration 53/1000 | Loss: 0.00002295
Iteration 54/1000 | Loss: 0.00002295
Iteration 55/1000 | Loss: 0.00002295
Iteration 56/1000 | Loss: 0.00002295
Iteration 57/1000 | Loss: 0.00002294
Iteration 58/1000 | Loss: 0.00002294
Iteration 59/1000 | Loss: 0.00002294
Iteration 60/1000 | Loss: 0.00002294
Iteration 61/1000 | Loss: 0.00002294
Iteration 62/1000 | Loss: 0.00002294
Iteration 63/1000 | Loss: 0.00002294
Iteration 64/1000 | Loss: 0.00002294
Iteration 65/1000 | Loss: 0.00002293
Iteration 66/1000 | Loss: 0.00002293
Iteration 67/1000 | Loss: 0.00002292
Iteration 68/1000 | Loss: 0.00002292
Iteration 69/1000 | Loss: 0.00002292
Iteration 70/1000 | Loss: 0.00002292
Iteration 71/1000 | Loss: 0.00002291
Iteration 72/1000 | Loss: 0.00002291
Iteration 73/1000 | Loss: 0.00002291
Iteration 74/1000 | Loss: 0.00002291
Iteration 75/1000 | Loss: 0.00002291
Iteration 76/1000 | Loss: 0.00002290
Iteration 77/1000 | Loss: 0.00002290
Iteration 78/1000 | Loss: 0.00002290
Iteration 79/1000 | Loss: 0.00002290
Iteration 80/1000 | Loss: 0.00002290
Iteration 81/1000 | Loss: 0.00002290
Iteration 82/1000 | Loss: 0.00002290
Iteration 83/1000 | Loss: 0.00002289
Iteration 84/1000 | Loss: 0.00002289
Iteration 85/1000 | Loss: 0.00002289
Iteration 86/1000 | Loss: 0.00002288
Iteration 87/1000 | Loss: 0.00002288
Iteration 88/1000 | Loss: 0.00002288
Iteration 89/1000 | Loss: 0.00002287
Iteration 90/1000 | Loss: 0.00002287
Iteration 91/1000 | Loss: 0.00002287
Iteration 92/1000 | Loss: 0.00002287
Iteration 93/1000 | Loss: 0.00002286
Iteration 94/1000 | Loss: 0.00002286
Iteration 95/1000 | Loss: 0.00002285
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002284
Iteration 98/1000 | Loss: 0.00002284
Iteration 99/1000 | Loss: 0.00002284
Iteration 100/1000 | Loss: 0.00002284
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002283
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002282
Iteration 106/1000 | Loss: 0.00002282
Iteration 107/1000 | Loss: 0.00002282
Iteration 108/1000 | Loss: 0.00002282
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002280
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002279
Iteration 118/1000 | Loss: 0.00002279
Iteration 119/1000 | Loss: 0.00002279
Iteration 120/1000 | Loss: 0.00002279
Iteration 121/1000 | Loss: 0.00002278
Iteration 122/1000 | Loss: 0.00002278
Iteration 123/1000 | Loss: 0.00002278
Iteration 124/1000 | Loss: 0.00002278
Iteration 125/1000 | Loss: 0.00002277
Iteration 126/1000 | Loss: 0.00002277
Iteration 127/1000 | Loss: 0.00002277
Iteration 128/1000 | Loss: 0.00002277
Iteration 129/1000 | Loss: 0.00002276
Iteration 130/1000 | Loss: 0.00002276
Iteration 131/1000 | Loss: 0.00002275
Iteration 132/1000 | Loss: 0.00002275
Iteration 133/1000 | Loss: 0.00002275
Iteration 134/1000 | Loss: 0.00002274
Iteration 135/1000 | Loss: 0.00002274
Iteration 136/1000 | Loss: 0.00002274
Iteration 137/1000 | Loss: 0.00002273
Iteration 138/1000 | Loss: 0.00002273
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002273
Iteration 141/1000 | Loss: 0.00002273
Iteration 142/1000 | Loss: 0.00002272
Iteration 143/1000 | Loss: 0.00002272
Iteration 144/1000 | Loss: 0.00002271
Iteration 145/1000 | Loss: 0.00002271
Iteration 146/1000 | Loss: 0.00002271
Iteration 147/1000 | Loss: 0.00002270
Iteration 148/1000 | Loss: 0.00002270
Iteration 149/1000 | Loss: 0.00002270
Iteration 150/1000 | Loss: 0.00002269
Iteration 151/1000 | Loss: 0.00002269
Iteration 152/1000 | Loss: 0.00002269
Iteration 153/1000 | Loss: 0.00002269
Iteration 154/1000 | Loss: 0.00002268
Iteration 155/1000 | Loss: 0.00002268
Iteration 156/1000 | Loss: 0.00002268
Iteration 157/1000 | Loss: 0.00002267
Iteration 158/1000 | Loss: 0.00002267
Iteration 159/1000 | Loss: 0.00002266
Iteration 160/1000 | Loss: 0.00002266
Iteration 161/1000 | Loss: 0.00002266
Iteration 162/1000 | Loss: 0.00002266
Iteration 163/1000 | Loss: 0.00002265
Iteration 164/1000 | Loss: 0.00002265
Iteration 165/1000 | Loss: 0.00002265
Iteration 166/1000 | Loss: 0.00002265
Iteration 167/1000 | Loss: 0.00002265
Iteration 168/1000 | Loss: 0.00002265
Iteration 169/1000 | Loss: 0.00002265
Iteration 170/1000 | Loss: 0.00002265
Iteration 171/1000 | Loss: 0.00002265
Iteration 172/1000 | Loss: 0.00002265
Iteration 173/1000 | Loss: 0.00002265
Iteration 174/1000 | Loss: 0.00002265
Iteration 175/1000 | Loss: 0.00002265
Iteration 176/1000 | Loss: 0.00002265
Iteration 177/1000 | Loss: 0.00002265
Iteration 178/1000 | Loss: 0.00002265
Iteration 179/1000 | Loss: 0.00002265
Iteration 180/1000 | Loss: 0.00002265
Iteration 181/1000 | Loss: 0.00002265
Iteration 182/1000 | Loss: 0.00002265
Iteration 183/1000 | Loss: 0.00002265
Iteration 184/1000 | Loss: 0.00002265
Iteration 185/1000 | Loss: 0.00002265
Iteration 186/1000 | Loss: 0.00002265
Iteration 187/1000 | Loss: 0.00002265
Iteration 188/1000 | Loss: 0.00002265
Iteration 189/1000 | Loss: 0.00002265
Iteration 190/1000 | Loss: 0.00002265
Iteration 191/1000 | Loss: 0.00002265
Iteration 192/1000 | Loss: 0.00002265
Iteration 193/1000 | Loss: 0.00002265
Iteration 194/1000 | Loss: 0.00002265
Iteration 195/1000 | Loss: 0.00002265
Iteration 196/1000 | Loss: 0.00002265
Iteration 197/1000 | Loss: 0.00002265
Iteration 198/1000 | Loss: 0.00002265
Iteration 199/1000 | Loss: 0.00002265
Iteration 200/1000 | Loss: 0.00002265
Iteration 201/1000 | Loss: 0.00002265
Iteration 202/1000 | Loss: 0.00002265
Iteration 203/1000 | Loss: 0.00002265
Iteration 204/1000 | Loss: 0.00002265
Iteration 205/1000 | Loss: 0.00002265
Iteration 206/1000 | Loss: 0.00002265
Iteration 207/1000 | Loss: 0.00002265
Iteration 208/1000 | Loss: 0.00002265
Iteration 209/1000 | Loss: 0.00002265
Iteration 210/1000 | Loss: 0.00002265
Iteration 211/1000 | Loss: 0.00002265
Iteration 212/1000 | Loss: 0.00002265
Iteration 213/1000 | Loss: 0.00002265
Iteration 214/1000 | Loss: 0.00002265
Iteration 215/1000 | Loss: 0.00002265
Iteration 216/1000 | Loss: 0.00002265
Iteration 217/1000 | Loss: 0.00002265
Iteration 218/1000 | Loss: 0.00002265
Iteration 219/1000 | Loss: 0.00002265
Iteration 220/1000 | Loss: 0.00002265
Iteration 221/1000 | Loss: 0.00002265
Iteration 222/1000 | Loss: 0.00002265
Iteration 223/1000 | Loss: 0.00002265
Iteration 224/1000 | Loss: 0.00002265
Iteration 225/1000 | Loss: 0.00002265
Iteration 226/1000 | Loss: 0.00002265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.2650461687589996e-05, 2.2650461687589996e-05, 2.2650461687589996e-05, 2.2650461687589996e-05, 2.2650461687589996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2650461687589996e-05

Optimization complete. Final v2v error: 3.8465492725372314 mm

Highest mean error: 11.840583801269531 mm for frame 16

Lowest mean error: 3.079862594604492 mm for frame 191

Saving results

Total time: 123.7709972858429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520869
Iteration 2/25 | Loss: 0.00135910
Iteration 3/25 | Loss: 0.00126116
Iteration 4/25 | Loss: 0.00125430
Iteration 5/25 | Loss: 0.00125371
Iteration 6/25 | Loss: 0.00125371
Iteration 7/25 | Loss: 0.00125371
Iteration 8/25 | Loss: 0.00125371
Iteration 9/25 | Loss: 0.00125371
Iteration 10/25 | Loss: 0.00125371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012537081493064761, 0.0012537081493064761, 0.0012537081493064761, 0.0012537081493064761, 0.0012537081493064761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012537081493064761

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88159537
Iteration 2/25 | Loss: 0.00073575
Iteration 3/25 | Loss: 0.00073574
Iteration 4/25 | Loss: 0.00073574
Iteration 5/25 | Loss: 0.00073574
Iteration 6/25 | Loss: 0.00073574
Iteration 7/25 | Loss: 0.00073574
Iteration 8/25 | Loss: 0.00073574
Iteration 9/25 | Loss: 0.00073574
Iteration 10/25 | Loss: 0.00073574
Iteration 11/25 | Loss: 0.00073574
Iteration 12/25 | Loss: 0.00073574
Iteration 13/25 | Loss: 0.00073574
Iteration 14/25 | Loss: 0.00073574
Iteration 15/25 | Loss: 0.00073574
Iteration 16/25 | Loss: 0.00073574
Iteration 17/25 | Loss: 0.00073574
Iteration 18/25 | Loss: 0.00073574
Iteration 19/25 | Loss: 0.00073574
Iteration 20/25 | Loss: 0.00073574
Iteration 21/25 | Loss: 0.00073574
Iteration 22/25 | Loss: 0.00073574
Iteration 23/25 | Loss: 0.00073574
Iteration 24/25 | Loss: 0.00073574
Iteration 25/25 | Loss: 0.00073574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073574
Iteration 2/1000 | Loss: 0.00002819
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001886
Iteration 5/1000 | Loss: 0.00001818
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001736
Iteration 8/1000 | Loss: 0.00001713
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001670
Iteration 11/1000 | Loss: 0.00001656
Iteration 12/1000 | Loss: 0.00001648
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001640
Iteration 16/1000 | Loss: 0.00001634
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001621
Iteration 19/1000 | Loss: 0.00001620
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001610
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001610
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001609
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001606
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001603
Iteration 33/1000 | Loss: 0.00001603
Iteration 34/1000 | Loss: 0.00001601
Iteration 35/1000 | Loss: 0.00001601
Iteration 36/1000 | Loss: 0.00001600
Iteration 37/1000 | Loss: 0.00001600
Iteration 38/1000 | Loss: 0.00001600
Iteration 39/1000 | Loss: 0.00001599
Iteration 40/1000 | Loss: 0.00001599
Iteration 41/1000 | Loss: 0.00001598
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001591
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001590
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001589
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001589
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001588
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001588
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001587
Iteration 77/1000 | Loss: 0.00001587
Iteration 78/1000 | Loss: 0.00001587
Iteration 79/1000 | Loss: 0.00001586
Iteration 80/1000 | Loss: 0.00001586
Iteration 81/1000 | Loss: 0.00001586
Iteration 82/1000 | Loss: 0.00001586
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001585
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001584
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001584
Iteration 96/1000 | Loss: 0.00001584
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001584
Iteration 99/1000 | Loss: 0.00001584
Iteration 100/1000 | Loss: 0.00001584
Iteration 101/1000 | Loss: 0.00001584
Iteration 102/1000 | Loss: 0.00001584
Iteration 103/1000 | Loss: 0.00001584
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001583
Iteration 108/1000 | Loss: 0.00001583
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001582
Iteration 115/1000 | Loss: 0.00001582
Iteration 116/1000 | Loss: 0.00001582
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001582
Iteration 120/1000 | Loss: 0.00001582
Iteration 121/1000 | Loss: 0.00001582
Iteration 122/1000 | Loss: 0.00001582
Iteration 123/1000 | Loss: 0.00001582
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001581
Iteration 129/1000 | Loss: 0.00001581
Iteration 130/1000 | Loss: 0.00001581
Iteration 131/1000 | Loss: 0.00001581
Iteration 132/1000 | Loss: 0.00001581
Iteration 133/1000 | Loss: 0.00001581
Iteration 134/1000 | Loss: 0.00001581
Iteration 135/1000 | Loss: 0.00001581
Iteration 136/1000 | Loss: 0.00001580
Iteration 137/1000 | Loss: 0.00001580
Iteration 138/1000 | Loss: 0.00001580
Iteration 139/1000 | Loss: 0.00001580
Iteration 140/1000 | Loss: 0.00001580
Iteration 141/1000 | Loss: 0.00001580
Iteration 142/1000 | Loss: 0.00001580
Iteration 143/1000 | Loss: 0.00001580
Iteration 144/1000 | Loss: 0.00001580
Iteration 145/1000 | Loss: 0.00001580
Iteration 146/1000 | Loss: 0.00001580
Iteration 147/1000 | Loss: 0.00001580
Iteration 148/1000 | Loss: 0.00001580
Iteration 149/1000 | Loss: 0.00001579
Iteration 150/1000 | Loss: 0.00001579
Iteration 151/1000 | Loss: 0.00001579
Iteration 152/1000 | Loss: 0.00001579
Iteration 153/1000 | Loss: 0.00001579
Iteration 154/1000 | Loss: 0.00001579
Iteration 155/1000 | Loss: 0.00001579
Iteration 156/1000 | Loss: 0.00001579
Iteration 157/1000 | Loss: 0.00001579
Iteration 158/1000 | Loss: 0.00001579
Iteration 159/1000 | Loss: 0.00001579
Iteration 160/1000 | Loss: 0.00001579
Iteration 161/1000 | Loss: 0.00001579
Iteration 162/1000 | Loss: 0.00001579
Iteration 163/1000 | Loss: 0.00001579
Iteration 164/1000 | Loss: 0.00001579
Iteration 165/1000 | Loss: 0.00001579
Iteration 166/1000 | Loss: 0.00001579
Iteration 167/1000 | Loss: 0.00001579
Iteration 168/1000 | Loss: 0.00001579
Iteration 169/1000 | Loss: 0.00001579
Iteration 170/1000 | Loss: 0.00001579
Iteration 171/1000 | Loss: 0.00001579
Iteration 172/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.5793521015439183e-05, 1.5793521015439183e-05, 1.5793521015439183e-05, 1.5793521015439183e-05, 1.5793521015439183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5793521015439183e-05

Optimization complete. Final v2v error: 3.333871603012085 mm

Highest mean error: 3.379061698913574 mm for frame 0

Lowest mean error: 3.275753974914551 mm for frame 125

Saving results

Total time: 42.27713656425476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517137
Iteration 2/25 | Loss: 0.00138883
Iteration 3/25 | Loss: 0.00128218
Iteration 4/25 | Loss: 0.00126365
Iteration 5/25 | Loss: 0.00125564
Iteration 6/25 | Loss: 0.00125487
Iteration 7/25 | Loss: 0.00125487
Iteration 8/25 | Loss: 0.00125487
Iteration 9/25 | Loss: 0.00125487
Iteration 10/25 | Loss: 0.00125487
Iteration 11/25 | Loss: 0.00125487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001254874630831182, 0.001254874630831182, 0.001254874630831182, 0.001254874630831182, 0.001254874630831182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001254874630831182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79020137
Iteration 2/25 | Loss: 0.00063278
Iteration 3/25 | Loss: 0.00063278
Iteration 4/25 | Loss: 0.00063278
Iteration 5/25 | Loss: 0.00063278
Iteration 6/25 | Loss: 0.00063278
Iteration 7/25 | Loss: 0.00063277
Iteration 8/25 | Loss: 0.00063277
Iteration 9/25 | Loss: 0.00063277
Iteration 10/25 | Loss: 0.00063277
Iteration 11/25 | Loss: 0.00063277
Iteration 12/25 | Loss: 0.00063277
Iteration 13/25 | Loss: 0.00063277
Iteration 14/25 | Loss: 0.00063277
Iteration 15/25 | Loss: 0.00063277
Iteration 16/25 | Loss: 0.00063277
Iteration 17/25 | Loss: 0.00063277
Iteration 18/25 | Loss: 0.00063277
Iteration 19/25 | Loss: 0.00063277
Iteration 20/25 | Loss: 0.00063277
Iteration 21/25 | Loss: 0.00063277
Iteration 22/25 | Loss: 0.00063277
Iteration 23/25 | Loss: 0.00063277
Iteration 24/25 | Loss: 0.00063277
Iteration 25/25 | Loss: 0.00063277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063277
Iteration 2/1000 | Loss: 0.00005293
Iteration 3/1000 | Loss: 0.00003321
Iteration 4/1000 | Loss: 0.00002990
Iteration 5/1000 | Loss: 0.00002894
Iteration 6/1000 | Loss: 0.00002739
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00002577
Iteration 9/1000 | Loss: 0.00002541
Iteration 10/1000 | Loss: 0.00002501
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002433
Iteration 13/1000 | Loss: 0.00002402
Iteration 14/1000 | Loss: 0.00002379
Iteration 15/1000 | Loss: 0.00002375
Iteration 16/1000 | Loss: 0.00002366
Iteration 17/1000 | Loss: 0.00002365
Iteration 18/1000 | Loss: 0.00002352
Iteration 19/1000 | Loss: 0.00002347
Iteration 20/1000 | Loss: 0.00002343
Iteration 21/1000 | Loss: 0.00002343
Iteration 22/1000 | Loss: 0.00002343
Iteration 23/1000 | Loss: 0.00002343
Iteration 24/1000 | Loss: 0.00002343
Iteration 25/1000 | Loss: 0.00002342
Iteration 26/1000 | Loss: 0.00002342
Iteration 27/1000 | Loss: 0.00002332
Iteration 28/1000 | Loss: 0.00002332
Iteration 29/1000 | Loss: 0.00002330
Iteration 30/1000 | Loss: 0.00002330
Iteration 31/1000 | Loss: 0.00002330
Iteration 32/1000 | Loss: 0.00002330
Iteration 33/1000 | Loss: 0.00002330
Iteration 34/1000 | Loss: 0.00002329
Iteration 35/1000 | Loss: 0.00002327
Iteration 36/1000 | Loss: 0.00002326
Iteration 37/1000 | Loss: 0.00002326
Iteration 38/1000 | Loss: 0.00002325
Iteration 39/1000 | Loss: 0.00002325
Iteration 40/1000 | Loss: 0.00002324
Iteration 41/1000 | Loss: 0.00002324
Iteration 42/1000 | Loss: 0.00002322
Iteration 43/1000 | Loss: 0.00002322
Iteration 44/1000 | Loss: 0.00002321
Iteration 45/1000 | Loss: 0.00002320
Iteration 46/1000 | Loss: 0.00002319
Iteration 47/1000 | Loss: 0.00002318
Iteration 48/1000 | Loss: 0.00002318
Iteration 49/1000 | Loss: 0.00002318
Iteration 50/1000 | Loss: 0.00002317
Iteration 51/1000 | Loss: 0.00002317
Iteration 52/1000 | Loss: 0.00002317
Iteration 53/1000 | Loss: 0.00002317
Iteration 54/1000 | Loss: 0.00002317
Iteration 55/1000 | Loss: 0.00002317
Iteration 56/1000 | Loss: 0.00002317
Iteration 57/1000 | Loss: 0.00002316
Iteration 58/1000 | Loss: 0.00002316
Iteration 59/1000 | Loss: 0.00002316
Iteration 60/1000 | Loss: 0.00002316
Iteration 61/1000 | Loss: 0.00002316
Iteration 62/1000 | Loss: 0.00002316
Iteration 63/1000 | Loss: 0.00002316
Iteration 64/1000 | Loss: 0.00002316
Iteration 65/1000 | Loss: 0.00002316
Iteration 66/1000 | Loss: 0.00002315
Iteration 67/1000 | Loss: 0.00002315
Iteration 68/1000 | Loss: 0.00002314
Iteration 69/1000 | Loss: 0.00002314
Iteration 70/1000 | Loss: 0.00002314
Iteration 71/1000 | Loss: 0.00002314
Iteration 72/1000 | Loss: 0.00002313
Iteration 73/1000 | Loss: 0.00002313
Iteration 74/1000 | Loss: 0.00002313
Iteration 75/1000 | Loss: 0.00002313
Iteration 76/1000 | Loss: 0.00002313
Iteration 77/1000 | Loss: 0.00002312
Iteration 78/1000 | Loss: 0.00002312
Iteration 79/1000 | Loss: 0.00002312
Iteration 80/1000 | Loss: 0.00002312
Iteration 81/1000 | Loss: 0.00002311
Iteration 82/1000 | Loss: 0.00002311
Iteration 83/1000 | Loss: 0.00002311
Iteration 84/1000 | Loss: 0.00002311
Iteration 85/1000 | Loss: 0.00002311
Iteration 86/1000 | Loss: 0.00002311
Iteration 87/1000 | Loss: 0.00002310
Iteration 88/1000 | Loss: 0.00002310
Iteration 89/1000 | Loss: 0.00002310
Iteration 90/1000 | Loss: 0.00002310
Iteration 91/1000 | Loss: 0.00002310
Iteration 92/1000 | Loss: 0.00002310
Iteration 93/1000 | Loss: 0.00002310
Iteration 94/1000 | Loss: 0.00002310
Iteration 95/1000 | Loss: 0.00002310
Iteration 96/1000 | Loss: 0.00002309
Iteration 97/1000 | Loss: 0.00002309
Iteration 98/1000 | Loss: 0.00002309
Iteration 99/1000 | Loss: 0.00002309
Iteration 100/1000 | Loss: 0.00002309
Iteration 101/1000 | Loss: 0.00002309
Iteration 102/1000 | Loss: 0.00002308
Iteration 103/1000 | Loss: 0.00002308
Iteration 104/1000 | Loss: 0.00002308
Iteration 105/1000 | Loss: 0.00002308
Iteration 106/1000 | Loss: 0.00002308
Iteration 107/1000 | Loss: 0.00002308
Iteration 108/1000 | Loss: 0.00002308
Iteration 109/1000 | Loss: 0.00002308
Iteration 110/1000 | Loss: 0.00002307
Iteration 111/1000 | Loss: 0.00002306
Iteration 112/1000 | Loss: 0.00002306
Iteration 113/1000 | Loss: 0.00002306
Iteration 114/1000 | Loss: 0.00002306
Iteration 115/1000 | Loss: 0.00002306
Iteration 116/1000 | Loss: 0.00002306
Iteration 117/1000 | Loss: 0.00002306
Iteration 118/1000 | Loss: 0.00002306
Iteration 119/1000 | Loss: 0.00002306
Iteration 120/1000 | Loss: 0.00002306
Iteration 121/1000 | Loss: 0.00002306
Iteration 122/1000 | Loss: 0.00002306
Iteration 123/1000 | Loss: 0.00002306
Iteration 124/1000 | Loss: 0.00002306
Iteration 125/1000 | Loss: 0.00002306
Iteration 126/1000 | Loss: 0.00002306
Iteration 127/1000 | Loss: 0.00002306
Iteration 128/1000 | Loss: 0.00002305
Iteration 129/1000 | Loss: 0.00002305
Iteration 130/1000 | Loss: 0.00002305
Iteration 131/1000 | Loss: 0.00002304
Iteration 132/1000 | Loss: 0.00002304
Iteration 133/1000 | Loss: 0.00002304
Iteration 134/1000 | Loss: 0.00002304
Iteration 135/1000 | Loss: 0.00002304
Iteration 136/1000 | Loss: 0.00002304
Iteration 137/1000 | Loss: 0.00002304
Iteration 138/1000 | Loss: 0.00002304
Iteration 139/1000 | Loss: 0.00002304
Iteration 140/1000 | Loss: 0.00002303
Iteration 141/1000 | Loss: 0.00002303
Iteration 142/1000 | Loss: 0.00002303
Iteration 143/1000 | Loss: 0.00002303
Iteration 144/1000 | Loss: 0.00002303
Iteration 145/1000 | Loss: 0.00002303
Iteration 146/1000 | Loss: 0.00002302
Iteration 147/1000 | Loss: 0.00002302
Iteration 148/1000 | Loss: 0.00002302
Iteration 149/1000 | Loss: 0.00002302
Iteration 150/1000 | Loss: 0.00002302
Iteration 151/1000 | Loss: 0.00002302
Iteration 152/1000 | Loss: 0.00002302
Iteration 153/1000 | Loss: 0.00002302
Iteration 154/1000 | Loss: 0.00002302
Iteration 155/1000 | Loss: 0.00002301
Iteration 156/1000 | Loss: 0.00002301
Iteration 157/1000 | Loss: 0.00002301
Iteration 158/1000 | Loss: 0.00002300
Iteration 159/1000 | Loss: 0.00002300
Iteration 160/1000 | Loss: 0.00002300
Iteration 161/1000 | Loss: 0.00002300
Iteration 162/1000 | Loss: 0.00002299
Iteration 163/1000 | Loss: 0.00002299
Iteration 164/1000 | Loss: 0.00002299
Iteration 165/1000 | Loss: 0.00002299
Iteration 166/1000 | Loss: 0.00002298
Iteration 167/1000 | Loss: 0.00002298
Iteration 168/1000 | Loss: 0.00002298
Iteration 169/1000 | Loss: 0.00002298
Iteration 170/1000 | Loss: 0.00002296
Iteration 171/1000 | Loss: 0.00002295
Iteration 172/1000 | Loss: 0.00002295
Iteration 173/1000 | Loss: 0.00002295
Iteration 174/1000 | Loss: 0.00002295
Iteration 175/1000 | Loss: 0.00002294
Iteration 176/1000 | Loss: 0.00002294
Iteration 177/1000 | Loss: 0.00002294
Iteration 178/1000 | Loss: 0.00002293
Iteration 179/1000 | Loss: 0.00002293
Iteration 180/1000 | Loss: 0.00002293
Iteration 181/1000 | Loss: 0.00002293
Iteration 182/1000 | Loss: 0.00002293
Iteration 183/1000 | Loss: 0.00002293
Iteration 184/1000 | Loss: 0.00002292
Iteration 185/1000 | Loss: 0.00002291
Iteration 186/1000 | Loss: 0.00002291
Iteration 187/1000 | Loss: 0.00002290
Iteration 188/1000 | Loss: 0.00002290
Iteration 189/1000 | Loss: 0.00002290
Iteration 190/1000 | Loss: 0.00002290
Iteration 191/1000 | Loss: 0.00002289
Iteration 192/1000 | Loss: 0.00002289
Iteration 193/1000 | Loss: 0.00002289
Iteration 194/1000 | Loss: 0.00002289
Iteration 195/1000 | Loss: 0.00002289
Iteration 196/1000 | Loss: 0.00002289
Iteration 197/1000 | Loss: 0.00002289
Iteration 198/1000 | Loss: 0.00002289
Iteration 199/1000 | Loss: 0.00002289
Iteration 200/1000 | Loss: 0.00002289
Iteration 201/1000 | Loss: 0.00002289
Iteration 202/1000 | Loss: 0.00002289
Iteration 203/1000 | Loss: 0.00002289
Iteration 204/1000 | Loss: 0.00002289
Iteration 205/1000 | Loss: 0.00002288
Iteration 206/1000 | Loss: 0.00002288
Iteration 207/1000 | Loss: 0.00002288
Iteration 208/1000 | Loss: 0.00002288
Iteration 209/1000 | Loss: 0.00002287
Iteration 210/1000 | Loss: 0.00002287
Iteration 211/1000 | Loss: 0.00002287
Iteration 212/1000 | Loss: 0.00002287
Iteration 213/1000 | Loss: 0.00002287
Iteration 214/1000 | Loss: 0.00002287
Iteration 215/1000 | Loss: 0.00002287
Iteration 216/1000 | Loss: 0.00002287
Iteration 217/1000 | Loss: 0.00002287
Iteration 218/1000 | Loss: 0.00002287
Iteration 219/1000 | Loss: 0.00002287
Iteration 220/1000 | Loss: 0.00002287
Iteration 221/1000 | Loss: 0.00002287
Iteration 222/1000 | Loss: 0.00002287
Iteration 223/1000 | Loss: 0.00002287
Iteration 224/1000 | Loss: 0.00002287
Iteration 225/1000 | Loss: 0.00002287
Iteration 226/1000 | Loss: 0.00002287
Iteration 227/1000 | Loss: 0.00002286
Iteration 228/1000 | Loss: 0.00002286
Iteration 229/1000 | Loss: 0.00002286
Iteration 230/1000 | Loss: 0.00002286
Iteration 231/1000 | Loss: 0.00002286
Iteration 232/1000 | Loss: 0.00002286
Iteration 233/1000 | Loss: 0.00002286
Iteration 234/1000 | Loss: 0.00002286
Iteration 235/1000 | Loss: 0.00002285
Iteration 236/1000 | Loss: 0.00002285
Iteration 237/1000 | Loss: 0.00002285
Iteration 238/1000 | Loss: 0.00002285
Iteration 239/1000 | Loss: 0.00002285
Iteration 240/1000 | Loss: 0.00002285
Iteration 241/1000 | Loss: 0.00002285
Iteration 242/1000 | Loss: 0.00002285
Iteration 243/1000 | Loss: 0.00002285
Iteration 244/1000 | Loss: 0.00002285
Iteration 245/1000 | Loss: 0.00002285
Iteration 246/1000 | Loss: 0.00002284
Iteration 247/1000 | Loss: 0.00002284
Iteration 248/1000 | Loss: 0.00002284
Iteration 249/1000 | Loss: 0.00002284
Iteration 250/1000 | Loss: 0.00002284
Iteration 251/1000 | Loss: 0.00002284
Iteration 252/1000 | Loss: 0.00002284
Iteration 253/1000 | Loss: 0.00002284
Iteration 254/1000 | Loss: 0.00002284
Iteration 255/1000 | Loss: 0.00002284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.2841246391180903e-05, 2.2841246391180903e-05, 2.2841246391180903e-05, 2.2841246391180903e-05, 2.2841246391180903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2841246391180903e-05

Optimization complete. Final v2v error: 3.935769557952881 mm

Highest mean error: 4.43256950378418 mm for frame 197

Lowest mean error: 3.71012806892395 mm for frame 169

Saving results

Total time: 53.93863391876221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744323
Iteration 2/25 | Loss: 0.00178083
Iteration 3/25 | Loss: 0.00137289
Iteration 4/25 | Loss: 0.00132687
Iteration 5/25 | Loss: 0.00132923
Iteration 6/25 | Loss: 0.00129817
Iteration 7/25 | Loss: 0.00129131
Iteration 8/25 | Loss: 0.00129026
Iteration 9/25 | Loss: 0.00128742
Iteration 10/25 | Loss: 0.00128352
Iteration 11/25 | Loss: 0.00128214
Iteration 12/25 | Loss: 0.00128117
Iteration 13/25 | Loss: 0.00128105
Iteration 14/25 | Loss: 0.00128104
Iteration 15/25 | Loss: 0.00128103
Iteration 16/25 | Loss: 0.00128102
Iteration 17/25 | Loss: 0.00128101
Iteration 18/25 | Loss: 0.00128101
Iteration 19/25 | Loss: 0.00128101
Iteration 20/25 | Loss: 0.00128101
Iteration 21/25 | Loss: 0.00128101
Iteration 22/25 | Loss: 0.00128101
Iteration 23/25 | Loss: 0.00128101
Iteration 24/25 | Loss: 0.00128101
Iteration 25/25 | Loss: 0.00128101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81353045
Iteration 2/25 | Loss: 0.00119505
Iteration 3/25 | Loss: 0.00070588
Iteration 4/25 | Loss: 0.00070588
Iteration 5/25 | Loss: 0.00070588
Iteration 6/25 | Loss: 0.00070588
Iteration 7/25 | Loss: 0.00070588
Iteration 8/25 | Loss: 0.00070588
Iteration 9/25 | Loss: 0.00070588
Iteration 10/25 | Loss: 0.00070588
Iteration 11/25 | Loss: 0.00070588
Iteration 12/25 | Loss: 0.00070588
Iteration 13/25 | Loss: 0.00070588
Iteration 14/25 | Loss: 0.00070588
Iteration 15/25 | Loss: 0.00070588
Iteration 16/25 | Loss: 0.00070588
Iteration 17/25 | Loss: 0.00070588
Iteration 18/25 | Loss: 0.00070588
Iteration 19/25 | Loss: 0.00070588
Iteration 20/25 | Loss: 0.00070588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007058817427605391, 0.0007058817427605391, 0.0007058817427605391, 0.0007058817427605391, 0.0007058817427605391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007058817427605391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070588
Iteration 2/1000 | Loss: 0.00006294
Iteration 3/1000 | Loss: 0.00004334
Iteration 4/1000 | Loss: 0.00003872
Iteration 5/1000 | Loss: 0.00003605
Iteration 6/1000 | Loss: 0.00003435
Iteration 7/1000 | Loss: 0.00077569
Iteration 8/1000 | Loss: 0.00007960
Iteration 9/1000 | Loss: 0.00017963
Iteration 10/1000 | Loss: 0.00003620
Iteration 11/1000 | Loss: 0.00003323
Iteration 12/1000 | Loss: 0.00003172
Iteration 13/1000 | Loss: 0.00003078
Iteration 14/1000 | Loss: 0.00003002
Iteration 15/1000 | Loss: 0.00002955
Iteration 16/1000 | Loss: 0.00002917
Iteration 17/1000 | Loss: 0.00002896
Iteration 18/1000 | Loss: 0.00002874
Iteration 19/1000 | Loss: 0.00002873
Iteration 20/1000 | Loss: 0.00002872
Iteration 21/1000 | Loss: 0.00002853
Iteration 22/1000 | Loss: 0.00002839
Iteration 23/1000 | Loss: 0.00002837
Iteration 24/1000 | Loss: 0.00002836
Iteration 25/1000 | Loss: 0.00002836
Iteration 26/1000 | Loss: 0.00002835
Iteration 27/1000 | Loss: 0.00002835
Iteration 28/1000 | Loss: 0.00002834
Iteration 29/1000 | Loss: 0.00002829
Iteration 30/1000 | Loss: 0.00002824
Iteration 31/1000 | Loss: 0.00002824
Iteration 32/1000 | Loss: 0.00002823
Iteration 33/1000 | Loss: 0.00002823
Iteration 34/1000 | Loss: 0.00002823
Iteration 35/1000 | Loss: 0.00002822
Iteration 36/1000 | Loss: 0.00002822
Iteration 37/1000 | Loss: 0.00002821
Iteration 38/1000 | Loss: 0.00002821
Iteration 39/1000 | Loss: 0.00002820
Iteration 40/1000 | Loss: 0.00002819
Iteration 41/1000 | Loss: 0.00002819
Iteration 42/1000 | Loss: 0.00002819
Iteration 43/1000 | Loss: 0.00002818
Iteration 44/1000 | Loss: 0.00002818
Iteration 45/1000 | Loss: 0.00002818
Iteration 46/1000 | Loss: 0.00002818
Iteration 47/1000 | Loss: 0.00002818
Iteration 48/1000 | Loss: 0.00002818
Iteration 49/1000 | Loss: 0.00002818
Iteration 50/1000 | Loss: 0.00002818
Iteration 51/1000 | Loss: 0.00002818
Iteration 52/1000 | Loss: 0.00002818
Iteration 53/1000 | Loss: 0.00002818
Iteration 54/1000 | Loss: 0.00002817
Iteration 55/1000 | Loss: 0.00002817
Iteration 56/1000 | Loss: 0.00002817
Iteration 57/1000 | Loss: 0.00002817
Iteration 58/1000 | Loss: 0.00002817
Iteration 59/1000 | Loss: 0.00002817
Iteration 60/1000 | Loss: 0.00002817
Iteration 61/1000 | Loss: 0.00002816
Iteration 62/1000 | Loss: 0.00002816
Iteration 63/1000 | Loss: 0.00002815
Iteration 64/1000 | Loss: 0.00002815
Iteration 65/1000 | Loss: 0.00002815
Iteration 66/1000 | Loss: 0.00002815
Iteration 67/1000 | Loss: 0.00002815
Iteration 68/1000 | Loss: 0.00002814
Iteration 69/1000 | Loss: 0.00002814
Iteration 70/1000 | Loss: 0.00002814
Iteration 71/1000 | Loss: 0.00002814
Iteration 72/1000 | Loss: 0.00002814
Iteration 73/1000 | Loss: 0.00002814
Iteration 74/1000 | Loss: 0.00002814
Iteration 75/1000 | Loss: 0.00002814
Iteration 76/1000 | Loss: 0.00002814
Iteration 77/1000 | Loss: 0.00002814
Iteration 78/1000 | Loss: 0.00002813
Iteration 79/1000 | Loss: 0.00002813
Iteration 80/1000 | Loss: 0.00002813
Iteration 81/1000 | Loss: 0.00002812
Iteration 82/1000 | Loss: 0.00002812
Iteration 83/1000 | Loss: 0.00002812
Iteration 84/1000 | Loss: 0.00002812
Iteration 85/1000 | Loss: 0.00002812
Iteration 86/1000 | Loss: 0.00002812
Iteration 87/1000 | Loss: 0.00002812
Iteration 88/1000 | Loss: 0.00002812
Iteration 89/1000 | Loss: 0.00002812
Iteration 90/1000 | Loss: 0.00002811
Iteration 91/1000 | Loss: 0.00002811
Iteration 92/1000 | Loss: 0.00002811
Iteration 93/1000 | Loss: 0.00002811
Iteration 94/1000 | Loss: 0.00002811
Iteration 95/1000 | Loss: 0.00002811
Iteration 96/1000 | Loss: 0.00002810
Iteration 97/1000 | Loss: 0.00002810
Iteration 98/1000 | Loss: 0.00002810
Iteration 99/1000 | Loss: 0.00002809
Iteration 100/1000 | Loss: 0.00002809
Iteration 101/1000 | Loss: 0.00002809
Iteration 102/1000 | Loss: 0.00002809
Iteration 103/1000 | Loss: 0.00002808
Iteration 104/1000 | Loss: 0.00002808
Iteration 105/1000 | Loss: 0.00002808
Iteration 106/1000 | Loss: 0.00002808
Iteration 107/1000 | Loss: 0.00002808
Iteration 108/1000 | Loss: 0.00002807
Iteration 109/1000 | Loss: 0.00002807
Iteration 110/1000 | Loss: 0.00002807
Iteration 111/1000 | Loss: 0.00002806
Iteration 112/1000 | Loss: 0.00002806
Iteration 113/1000 | Loss: 0.00002806
Iteration 114/1000 | Loss: 0.00002806
Iteration 115/1000 | Loss: 0.00002805
Iteration 116/1000 | Loss: 0.00002805
Iteration 117/1000 | Loss: 0.00002804
Iteration 118/1000 | Loss: 0.00002804
Iteration 119/1000 | Loss: 0.00002804
Iteration 120/1000 | Loss: 0.00002804
Iteration 121/1000 | Loss: 0.00002804
Iteration 122/1000 | Loss: 0.00002804
Iteration 123/1000 | Loss: 0.00002803
Iteration 124/1000 | Loss: 0.00002803
Iteration 125/1000 | Loss: 0.00002803
Iteration 126/1000 | Loss: 0.00002803
Iteration 127/1000 | Loss: 0.00002803
Iteration 128/1000 | Loss: 0.00002802
Iteration 129/1000 | Loss: 0.00002802
Iteration 130/1000 | Loss: 0.00002801
Iteration 131/1000 | Loss: 0.00002801
Iteration 132/1000 | Loss: 0.00002801
Iteration 133/1000 | Loss: 0.00002801
Iteration 134/1000 | Loss: 0.00002800
Iteration 135/1000 | Loss: 0.00002800
Iteration 136/1000 | Loss: 0.00002799
Iteration 137/1000 | Loss: 0.00002799
Iteration 138/1000 | Loss: 0.00002799
Iteration 139/1000 | Loss: 0.00002799
Iteration 140/1000 | Loss: 0.00002798
Iteration 141/1000 | Loss: 0.00002798
Iteration 142/1000 | Loss: 0.00002798
Iteration 143/1000 | Loss: 0.00002797
Iteration 144/1000 | Loss: 0.00002797
Iteration 145/1000 | Loss: 0.00002797
Iteration 146/1000 | Loss: 0.00002796
Iteration 147/1000 | Loss: 0.00002796
Iteration 148/1000 | Loss: 0.00002796
Iteration 149/1000 | Loss: 0.00002796
Iteration 150/1000 | Loss: 0.00002795
Iteration 151/1000 | Loss: 0.00002795
Iteration 152/1000 | Loss: 0.00002795
Iteration 153/1000 | Loss: 0.00002794
Iteration 154/1000 | Loss: 0.00012400
Iteration 155/1000 | Loss: 0.00002746
Iteration 156/1000 | Loss: 0.00002727
Iteration 157/1000 | Loss: 0.00002697
Iteration 158/1000 | Loss: 0.00002661
Iteration 159/1000 | Loss: 0.00002636
Iteration 160/1000 | Loss: 0.00002627
Iteration 161/1000 | Loss: 0.00002622
Iteration 162/1000 | Loss: 0.00002617
Iteration 163/1000 | Loss: 0.00002617
Iteration 164/1000 | Loss: 0.00002617
Iteration 165/1000 | Loss: 0.00002614
Iteration 166/1000 | Loss: 0.00002613
Iteration 167/1000 | Loss: 0.00002613
Iteration 168/1000 | Loss: 0.00002612
Iteration 169/1000 | Loss: 0.00002612
Iteration 170/1000 | Loss: 0.00002612
Iteration 171/1000 | Loss: 0.00002611
Iteration 172/1000 | Loss: 0.00002611
Iteration 173/1000 | Loss: 0.00002610
Iteration 174/1000 | Loss: 0.00002610
Iteration 175/1000 | Loss: 0.00002610
Iteration 176/1000 | Loss: 0.00002610
Iteration 177/1000 | Loss: 0.00002610
Iteration 178/1000 | Loss: 0.00002609
Iteration 179/1000 | Loss: 0.00002609
Iteration 180/1000 | Loss: 0.00002608
Iteration 181/1000 | Loss: 0.00002608
Iteration 182/1000 | Loss: 0.00002608
Iteration 183/1000 | Loss: 0.00002607
Iteration 184/1000 | Loss: 0.00002607
Iteration 185/1000 | Loss: 0.00002607
Iteration 186/1000 | Loss: 0.00002607
Iteration 187/1000 | Loss: 0.00002606
Iteration 188/1000 | Loss: 0.00002606
Iteration 189/1000 | Loss: 0.00002606
Iteration 190/1000 | Loss: 0.00002606
Iteration 191/1000 | Loss: 0.00002606
Iteration 192/1000 | Loss: 0.00002606
Iteration 193/1000 | Loss: 0.00002606
Iteration 194/1000 | Loss: 0.00002606
Iteration 195/1000 | Loss: 0.00002606
Iteration 196/1000 | Loss: 0.00002606
Iteration 197/1000 | Loss: 0.00002606
Iteration 198/1000 | Loss: 0.00002606
Iteration 199/1000 | Loss: 0.00002606
Iteration 200/1000 | Loss: 0.00002606
Iteration 201/1000 | Loss: 0.00002606
Iteration 202/1000 | Loss: 0.00002606
Iteration 203/1000 | Loss: 0.00002606
Iteration 204/1000 | Loss: 0.00002606
Iteration 205/1000 | Loss: 0.00002606
Iteration 206/1000 | Loss: 0.00002606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.6064235498779453e-05, 2.6064235498779453e-05, 2.6064235498779453e-05, 2.6064235498779453e-05, 2.6064235498779453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6064235498779453e-05

Optimization complete. Final v2v error: 4.098552227020264 mm

Highest mean error: 12.17489242553711 mm for frame 211

Lowest mean error: 3.398484945297241 mm for frame 230

Saving results

Total time: 87.27780222892761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00708010
Iteration 2/25 | Loss: 0.00132063
Iteration 3/25 | Loss: 0.00122898
Iteration 4/25 | Loss: 0.00119670
Iteration 5/25 | Loss: 0.00119030
Iteration 6/25 | Loss: 0.00119011
Iteration 7/25 | Loss: 0.00118738
Iteration 8/25 | Loss: 0.00118665
Iteration 9/25 | Loss: 0.00118619
Iteration 10/25 | Loss: 0.00118595
Iteration 11/25 | Loss: 0.00118710
Iteration 12/25 | Loss: 0.00118549
Iteration 13/25 | Loss: 0.00118832
Iteration 14/25 | Loss: 0.00118531
Iteration 15/25 | Loss: 0.00118531
Iteration 16/25 | Loss: 0.00118531
Iteration 17/25 | Loss: 0.00118531
Iteration 18/25 | Loss: 0.00118530
Iteration 19/25 | Loss: 0.00118530
Iteration 20/25 | Loss: 0.00118530
Iteration 21/25 | Loss: 0.00118530
Iteration 22/25 | Loss: 0.00118530
Iteration 23/25 | Loss: 0.00118529
Iteration 24/25 | Loss: 0.00118529
Iteration 25/25 | Loss: 0.00118529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66635823
Iteration 2/25 | Loss: 0.00061922
Iteration 3/25 | Loss: 0.00061922
Iteration 4/25 | Loss: 0.00061922
Iteration 5/25 | Loss: 0.00061922
Iteration 6/25 | Loss: 0.00061922
Iteration 7/25 | Loss: 0.00061922
Iteration 8/25 | Loss: 0.00061922
Iteration 9/25 | Loss: 0.00061922
Iteration 10/25 | Loss: 0.00061922
Iteration 11/25 | Loss: 0.00061922
Iteration 12/25 | Loss: 0.00061922
Iteration 13/25 | Loss: 0.00061922
Iteration 14/25 | Loss: 0.00061922
Iteration 15/25 | Loss: 0.00061922
Iteration 16/25 | Loss: 0.00061922
Iteration 17/25 | Loss: 0.00061922
Iteration 18/25 | Loss: 0.00061922
Iteration 19/25 | Loss: 0.00061922
Iteration 20/25 | Loss: 0.00061922
Iteration 21/25 | Loss: 0.00061922
Iteration 22/25 | Loss: 0.00061922
Iteration 23/25 | Loss: 0.00061922
Iteration 24/25 | Loss: 0.00061922
Iteration 25/25 | Loss: 0.00061922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061922
Iteration 2/1000 | Loss: 0.00002373
Iteration 3/1000 | Loss: 0.00001878
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001676
Iteration 6/1000 | Loss: 0.00001643
Iteration 7/1000 | Loss: 0.00001638
Iteration 8/1000 | Loss: 0.00032286
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001463
Iteration 13/1000 | Loss: 0.00001450
Iteration 14/1000 | Loss: 0.00001444
Iteration 15/1000 | Loss: 0.00001444
Iteration 16/1000 | Loss: 0.00001442
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001438
Iteration 19/1000 | Loss: 0.00001437
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001432
Iteration 23/1000 | Loss: 0.00001431
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001427
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001426
Iteration 30/1000 | Loss: 0.00001426
Iteration 31/1000 | Loss: 0.00001426
Iteration 32/1000 | Loss: 0.00001425
Iteration 33/1000 | Loss: 0.00001415
Iteration 34/1000 | Loss: 0.00001414
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001409
Iteration 38/1000 | Loss: 0.00001407
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001397
Iteration 45/1000 | Loss: 0.00001397
Iteration 46/1000 | Loss: 0.00001397
Iteration 47/1000 | Loss: 0.00001396
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001395
Iteration 51/1000 | Loss: 0.00001395
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001393
Iteration 55/1000 | Loss: 0.00001393
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001390
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001386
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001384
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001381
Iteration 85/1000 | Loss: 0.00001381
Iteration 86/1000 | Loss: 0.00001381
Iteration 87/1000 | Loss: 0.00001381
Iteration 88/1000 | Loss: 0.00001380
Iteration 89/1000 | Loss: 0.00001380
Iteration 90/1000 | Loss: 0.00001380
Iteration 91/1000 | Loss: 0.00001380
Iteration 92/1000 | Loss: 0.00001380
Iteration 93/1000 | Loss: 0.00001380
Iteration 94/1000 | Loss: 0.00001380
Iteration 95/1000 | Loss: 0.00001380
Iteration 96/1000 | Loss: 0.00001380
Iteration 97/1000 | Loss: 0.00001379
Iteration 98/1000 | Loss: 0.00001379
Iteration 99/1000 | Loss: 0.00001379
Iteration 100/1000 | Loss: 0.00001379
Iteration 101/1000 | Loss: 0.00001379
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001379
Iteration 113/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.378679462504806e-05, 1.378679462504806e-05, 1.378679462504806e-05, 1.378679462504806e-05, 1.378679462504806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.378679462504806e-05

Optimization complete. Final v2v error: 3.1495158672332764 mm

Highest mean error: 4.171707630157471 mm for frame 140

Lowest mean error: 2.904799461364746 mm for frame 247

Saving results

Total time: 61.15642595291138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022272
Iteration 2/25 | Loss: 0.00296257
Iteration 3/25 | Loss: 0.00181221
Iteration 4/25 | Loss: 0.00163756
Iteration 5/25 | Loss: 0.00152074
Iteration 6/25 | Loss: 0.00147602
Iteration 7/25 | Loss: 0.00146554
Iteration 8/25 | Loss: 0.00145946
Iteration 9/25 | Loss: 0.00143666
Iteration 10/25 | Loss: 0.00140241
Iteration 11/25 | Loss: 0.00138845
Iteration 12/25 | Loss: 0.00137630
Iteration 13/25 | Loss: 0.00137739
Iteration 14/25 | Loss: 0.00137466
Iteration 15/25 | Loss: 0.00136581
Iteration 16/25 | Loss: 0.00136807
Iteration 17/25 | Loss: 0.00136294
Iteration 18/25 | Loss: 0.00136112
Iteration 19/25 | Loss: 0.00135630
Iteration 20/25 | Loss: 0.00136008
Iteration 21/25 | Loss: 0.00135740
Iteration 22/25 | Loss: 0.00135263
Iteration 23/25 | Loss: 0.00135398
Iteration 24/25 | Loss: 0.00134433
Iteration 25/25 | Loss: 0.00134431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41925025
Iteration 2/25 | Loss: 0.00220066
Iteration 3/25 | Loss: 0.00134909
Iteration 4/25 | Loss: 0.00134909
Iteration 5/25 | Loss: 0.00134909
Iteration 6/25 | Loss: 0.00134909
Iteration 7/25 | Loss: 0.00134909
Iteration 8/25 | Loss: 0.00134909
Iteration 9/25 | Loss: 0.00134909
Iteration 10/25 | Loss: 0.00134909
Iteration 11/25 | Loss: 0.00134909
Iteration 12/25 | Loss: 0.00134909
Iteration 13/25 | Loss: 0.00134909
Iteration 14/25 | Loss: 0.00134909
Iteration 15/25 | Loss: 0.00134909
Iteration 16/25 | Loss: 0.00134909
Iteration 17/25 | Loss: 0.00134909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013490868732333183, 0.0013490868732333183, 0.0013490868732333183, 0.0013490868732333183, 0.0013490868732333183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013490868732333183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134909
Iteration 2/1000 | Loss: 0.00068022
Iteration 3/1000 | Loss: 0.00048548
Iteration 4/1000 | Loss: 0.00108429
Iteration 5/1000 | Loss: 0.00111523
Iteration 6/1000 | Loss: 0.00123728
Iteration 7/1000 | Loss: 0.00176728
Iteration 8/1000 | Loss: 0.00581806
Iteration 9/1000 | Loss: 0.00218045
Iteration 10/1000 | Loss: 0.00259653
Iteration 11/1000 | Loss: 0.00128148
Iteration 12/1000 | Loss: 0.00021790
Iteration 13/1000 | Loss: 0.00140790
Iteration 14/1000 | Loss: 0.00304274
Iteration 15/1000 | Loss: 0.00108310
Iteration 16/1000 | Loss: 0.00193378
Iteration 17/1000 | Loss: 0.00127796
Iteration 18/1000 | Loss: 0.00122730
Iteration 19/1000 | Loss: 0.00109549
Iteration 20/1000 | Loss: 0.00100879
Iteration 21/1000 | Loss: 0.00122078
Iteration 22/1000 | Loss: 0.00181998
Iteration 23/1000 | Loss: 0.00050607
Iteration 24/1000 | Loss: 0.00008548
Iteration 25/1000 | Loss: 0.00149849
Iteration 26/1000 | Loss: 0.00226549
Iteration 27/1000 | Loss: 0.00155490
Iteration 28/1000 | Loss: 0.00113410
Iteration 29/1000 | Loss: 0.00204286
Iteration 30/1000 | Loss: 0.00134444
Iteration 31/1000 | Loss: 0.00100170
Iteration 32/1000 | Loss: 0.00027142
Iteration 33/1000 | Loss: 0.00025616
Iteration 34/1000 | Loss: 0.00006720
Iteration 35/1000 | Loss: 0.00006044
Iteration 36/1000 | Loss: 0.00005661
Iteration 37/1000 | Loss: 0.00007670
Iteration 38/1000 | Loss: 0.00062282
Iteration 39/1000 | Loss: 0.00006148
Iteration 40/1000 | Loss: 0.00005305
Iteration 41/1000 | Loss: 0.00005057
Iteration 42/1000 | Loss: 0.00062881
Iteration 43/1000 | Loss: 0.00333303
Iteration 44/1000 | Loss: 0.00096806
Iteration 45/1000 | Loss: 0.00006261
Iteration 46/1000 | Loss: 0.00009741
Iteration 47/1000 | Loss: 0.00006061
Iteration 48/1000 | Loss: 0.00012230
Iteration 49/1000 | Loss: 0.00003458
Iteration 50/1000 | Loss: 0.00003131
Iteration 51/1000 | Loss: 0.00026518
Iteration 52/1000 | Loss: 0.00002979
Iteration 53/1000 | Loss: 0.00002736
Iteration 54/1000 | Loss: 0.00021441
Iteration 55/1000 | Loss: 0.00002460
Iteration 56/1000 | Loss: 0.00002317
Iteration 57/1000 | Loss: 0.00004050
Iteration 58/1000 | Loss: 0.00002148
Iteration 59/1000 | Loss: 0.00004496
Iteration 60/1000 | Loss: 0.00002044
Iteration 61/1000 | Loss: 0.00003283
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00003384
Iteration 64/1000 | Loss: 0.00002332
Iteration 65/1000 | Loss: 0.00003441
Iteration 66/1000 | Loss: 0.00001953
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001928
Iteration 70/1000 | Loss: 0.00001928
Iteration 71/1000 | Loss: 0.00001928
Iteration 72/1000 | Loss: 0.00001928
Iteration 73/1000 | Loss: 0.00001928
Iteration 74/1000 | Loss: 0.00001928
Iteration 75/1000 | Loss: 0.00001928
Iteration 76/1000 | Loss: 0.00001928
Iteration 77/1000 | Loss: 0.00001928
Iteration 78/1000 | Loss: 0.00001928
Iteration 79/1000 | Loss: 0.00001928
Iteration 80/1000 | Loss: 0.00001927
Iteration 81/1000 | Loss: 0.00001927
Iteration 82/1000 | Loss: 0.00001927
Iteration 83/1000 | Loss: 0.00001926
Iteration 84/1000 | Loss: 0.00001925
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001923
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001923
Iteration 97/1000 | Loss: 0.00001923
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001920
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00003354
Iteration 109/1000 | Loss: 0.00002050
Iteration 110/1000 | Loss: 0.00002201
Iteration 111/1000 | Loss: 0.00003806
Iteration 112/1000 | Loss: 0.00002162
Iteration 113/1000 | Loss: 0.00004178
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001917
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001916
Iteration 123/1000 | Loss: 0.00001916
Iteration 124/1000 | Loss: 0.00002160
Iteration 125/1000 | Loss: 0.00008853
Iteration 126/1000 | Loss: 0.00002005
Iteration 127/1000 | Loss: 0.00001914
Iteration 128/1000 | Loss: 0.00001914
Iteration 129/1000 | Loss: 0.00001914
Iteration 130/1000 | Loss: 0.00001913
Iteration 131/1000 | Loss: 0.00001913
Iteration 132/1000 | Loss: 0.00001913
Iteration 133/1000 | Loss: 0.00001912
Iteration 134/1000 | Loss: 0.00002038
Iteration 135/1000 | Loss: 0.00002631
Iteration 136/1000 | Loss: 0.00001909
Iteration 137/1000 | Loss: 0.00001909
Iteration 138/1000 | Loss: 0.00001908
Iteration 139/1000 | Loss: 0.00001908
Iteration 140/1000 | Loss: 0.00001908
Iteration 141/1000 | Loss: 0.00001908
Iteration 142/1000 | Loss: 0.00001908
Iteration 143/1000 | Loss: 0.00001908
Iteration 144/1000 | Loss: 0.00001908
Iteration 145/1000 | Loss: 0.00001908
Iteration 146/1000 | Loss: 0.00001908
Iteration 147/1000 | Loss: 0.00001908
Iteration 148/1000 | Loss: 0.00001908
Iteration 149/1000 | Loss: 0.00001908
Iteration 150/1000 | Loss: 0.00001908
Iteration 151/1000 | Loss: 0.00001908
Iteration 152/1000 | Loss: 0.00001908
Iteration 153/1000 | Loss: 0.00001908
Iteration 154/1000 | Loss: 0.00001908
Iteration 155/1000 | Loss: 0.00001908
Iteration 156/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.9082757717114873e-05, 1.9082757717114873e-05, 1.9082757717114873e-05, 1.9082757717114873e-05, 1.9082757717114873e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9082757717114873e-05

Optimization complete. Final v2v error: 3.6249630451202393 mm

Highest mean error: 5.119521141052246 mm for frame 42

Lowest mean error: 2.9936764240264893 mm for frame 25

Saving results

Total time: 183.4524073600769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508759
Iteration 2/25 | Loss: 0.00135022
Iteration 3/25 | Loss: 0.00124743
Iteration 4/25 | Loss: 0.00123999
Iteration 5/25 | Loss: 0.00123820
Iteration 6/25 | Loss: 0.00123820
Iteration 7/25 | Loss: 0.00123820
Iteration 8/25 | Loss: 0.00123820
Iteration 9/25 | Loss: 0.00123820
Iteration 10/25 | Loss: 0.00123820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012382031418383121, 0.0012382031418383121, 0.0012382031418383121, 0.0012382031418383121, 0.0012382031418383121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012382031418383121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89217967
Iteration 2/25 | Loss: 0.00068481
Iteration 3/25 | Loss: 0.00068481
Iteration 4/25 | Loss: 0.00068481
Iteration 5/25 | Loss: 0.00068481
Iteration 6/25 | Loss: 0.00068480
Iteration 7/25 | Loss: 0.00068480
Iteration 8/25 | Loss: 0.00068480
Iteration 9/25 | Loss: 0.00068480
Iteration 10/25 | Loss: 0.00068480
Iteration 11/25 | Loss: 0.00068480
Iteration 12/25 | Loss: 0.00068480
Iteration 13/25 | Loss: 0.00068480
Iteration 14/25 | Loss: 0.00068480
Iteration 15/25 | Loss: 0.00068480
Iteration 16/25 | Loss: 0.00068480
Iteration 17/25 | Loss: 0.00068480
Iteration 18/25 | Loss: 0.00068480
Iteration 19/25 | Loss: 0.00068480
Iteration 20/25 | Loss: 0.00068480
Iteration 21/25 | Loss: 0.00068480
Iteration 22/25 | Loss: 0.00068480
Iteration 23/25 | Loss: 0.00068480
Iteration 24/25 | Loss: 0.00068480
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006848023040220141, 0.0006848023040220141, 0.0006848023040220141, 0.0006848023040220141, 0.0006848023040220141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006848023040220141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068480
Iteration 2/1000 | Loss: 0.00003012
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00002017
Iteration 5/1000 | Loss: 0.00001927
Iteration 6/1000 | Loss: 0.00001873
Iteration 7/1000 | Loss: 0.00001837
Iteration 8/1000 | Loss: 0.00001816
Iteration 9/1000 | Loss: 0.00001784
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001740
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001708
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001695
Iteration 18/1000 | Loss: 0.00001695
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001694
Iteration 24/1000 | Loss: 0.00001685
Iteration 25/1000 | Loss: 0.00001685
Iteration 26/1000 | Loss: 0.00001685
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001685
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001683
Iteration 32/1000 | Loss: 0.00001682
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001679
Iteration 36/1000 | Loss: 0.00001679
Iteration 37/1000 | Loss: 0.00001670
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001668
Iteration 40/1000 | Loss: 0.00001667
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001666
Iteration 43/1000 | Loss: 0.00001665
Iteration 44/1000 | Loss: 0.00001665
Iteration 45/1000 | Loss: 0.00001664
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001663
Iteration 48/1000 | Loss: 0.00001661
Iteration 49/1000 | Loss: 0.00001661
Iteration 50/1000 | Loss: 0.00001661
Iteration 51/1000 | Loss: 0.00001660
Iteration 52/1000 | Loss: 0.00001660
Iteration 53/1000 | Loss: 0.00001660
Iteration 54/1000 | Loss: 0.00001659
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001658
Iteration 59/1000 | Loss: 0.00001658
Iteration 60/1000 | Loss: 0.00001658
Iteration 61/1000 | Loss: 0.00001658
Iteration 62/1000 | Loss: 0.00001657
Iteration 63/1000 | Loss: 0.00001657
Iteration 64/1000 | Loss: 0.00001657
Iteration 65/1000 | Loss: 0.00001657
Iteration 66/1000 | Loss: 0.00001657
Iteration 67/1000 | Loss: 0.00001657
Iteration 68/1000 | Loss: 0.00001657
Iteration 69/1000 | Loss: 0.00001657
Iteration 70/1000 | Loss: 0.00001657
Iteration 71/1000 | Loss: 0.00001657
Iteration 72/1000 | Loss: 0.00001657
Iteration 73/1000 | Loss: 0.00001657
Iteration 74/1000 | Loss: 0.00001657
Iteration 75/1000 | Loss: 0.00001657
Iteration 76/1000 | Loss: 0.00001657
Iteration 77/1000 | Loss: 0.00001656
Iteration 78/1000 | Loss: 0.00001656
Iteration 79/1000 | Loss: 0.00001656
Iteration 80/1000 | Loss: 0.00001656
Iteration 81/1000 | Loss: 0.00001656
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001656
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001656
Iteration 94/1000 | Loss: 0.00001656
Iteration 95/1000 | Loss: 0.00001656
Iteration 96/1000 | Loss: 0.00001656
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.656182939768769e-05, 1.656182939768769e-05, 1.656182939768769e-05, 1.656182939768769e-05, 1.656182939768769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.656182939768769e-05

Optimization complete. Final v2v error: 3.3872523307800293 mm

Highest mean error: 3.4238243103027344 mm for frame 151

Lowest mean error: 3.3277885913848877 mm for frame 86

Saving results

Total time: 34.510738372802734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539696
Iteration 2/25 | Loss: 0.00145395
Iteration 3/25 | Loss: 0.00133802
Iteration 4/25 | Loss: 0.00132336
Iteration 5/25 | Loss: 0.00131810
Iteration 6/25 | Loss: 0.00131807
Iteration 7/25 | Loss: 0.00131807
Iteration 8/25 | Loss: 0.00131807
Iteration 9/25 | Loss: 0.00131807
Iteration 10/25 | Loss: 0.00131807
Iteration 11/25 | Loss: 0.00131807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013180702226236463, 0.0013180702226236463, 0.0013180702226236463, 0.0013180702226236463, 0.0013180702226236463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013180702226236463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80535722
Iteration 2/25 | Loss: 0.00058678
Iteration 3/25 | Loss: 0.00058678
Iteration 4/25 | Loss: 0.00058678
Iteration 5/25 | Loss: 0.00058677
Iteration 6/25 | Loss: 0.00058677
Iteration 7/25 | Loss: 0.00058677
Iteration 8/25 | Loss: 0.00058677
Iteration 9/25 | Loss: 0.00058677
Iteration 10/25 | Loss: 0.00058677
Iteration 11/25 | Loss: 0.00058677
Iteration 12/25 | Loss: 0.00058677
Iteration 13/25 | Loss: 0.00058677
Iteration 14/25 | Loss: 0.00058677
Iteration 15/25 | Loss: 0.00058677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005867730942554772, 0.0005867730942554772, 0.0005867730942554772, 0.0005867730942554772, 0.0005867730942554772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005867730942554772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058677
Iteration 2/1000 | Loss: 0.00005274
Iteration 3/1000 | Loss: 0.00004381
Iteration 4/1000 | Loss: 0.00004150
Iteration 5/1000 | Loss: 0.00004039
Iteration 6/1000 | Loss: 0.00003976
Iteration 7/1000 | Loss: 0.00003920
Iteration 8/1000 | Loss: 0.00003870
Iteration 9/1000 | Loss: 0.00003800
Iteration 10/1000 | Loss: 0.00003753
Iteration 11/1000 | Loss: 0.00003722
Iteration 12/1000 | Loss: 0.00003680
Iteration 13/1000 | Loss: 0.00003644
Iteration 14/1000 | Loss: 0.00003621
Iteration 15/1000 | Loss: 0.00003619
Iteration 16/1000 | Loss: 0.00003607
Iteration 17/1000 | Loss: 0.00003601
Iteration 18/1000 | Loss: 0.00003597
Iteration 19/1000 | Loss: 0.00003586
Iteration 20/1000 | Loss: 0.00003579
Iteration 21/1000 | Loss: 0.00003578
Iteration 22/1000 | Loss: 0.00003577
Iteration 23/1000 | Loss: 0.00003577
Iteration 24/1000 | Loss: 0.00003577
Iteration 25/1000 | Loss: 0.00003577
Iteration 26/1000 | Loss: 0.00003576
Iteration 27/1000 | Loss: 0.00003576
Iteration 28/1000 | Loss: 0.00003576
Iteration 29/1000 | Loss: 0.00003576
Iteration 30/1000 | Loss: 0.00003576
Iteration 31/1000 | Loss: 0.00003575
Iteration 32/1000 | Loss: 0.00003575
Iteration 33/1000 | Loss: 0.00003575
Iteration 34/1000 | Loss: 0.00003573
Iteration 35/1000 | Loss: 0.00003572
Iteration 36/1000 | Loss: 0.00003572
Iteration 37/1000 | Loss: 0.00003572
Iteration 38/1000 | Loss: 0.00003572
Iteration 39/1000 | Loss: 0.00003572
Iteration 40/1000 | Loss: 0.00003572
Iteration 41/1000 | Loss: 0.00003572
Iteration 42/1000 | Loss: 0.00003572
Iteration 43/1000 | Loss: 0.00003572
Iteration 44/1000 | Loss: 0.00003572
Iteration 45/1000 | Loss: 0.00003572
Iteration 46/1000 | Loss: 0.00003571
Iteration 47/1000 | Loss: 0.00003570
Iteration 48/1000 | Loss: 0.00003570
Iteration 49/1000 | Loss: 0.00003569
Iteration 50/1000 | Loss: 0.00003569
Iteration 51/1000 | Loss: 0.00003569
Iteration 52/1000 | Loss: 0.00003569
Iteration 53/1000 | Loss: 0.00003569
Iteration 54/1000 | Loss: 0.00003569
Iteration 55/1000 | Loss: 0.00003569
Iteration 56/1000 | Loss: 0.00003569
Iteration 57/1000 | Loss: 0.00003569
Iteration 58/1000 | Loss: 0.00003569
Iteration 59/1000 | Loss: 0.00003569
Iteration 60/1000 | Loss: 0.00003569
Iteration 61/1000 | Loss: 0.00003568
Iteration 62/1000 | Loss: 0.00003568
Iteration 63/1000 | Loss: 0.00003568
Iteration 64/1000 | Loss: 0.00003568
Iteration 65/1000 | Loss: 0.00003568
Iteration 66/1000 | Loss: 0.00003568
Iteration 67/1000 | Loss: 0.00003568
Iteration 68/1000 | Loss: 0.00003567
Iteration 69/1000 | Loss: 0.00003567
Iteration 70/1000 | Loss: 0.00003567
Iteration 71/1000 | Loss: 0.00003567
Iteration 72/1000 | Loss: 0.00003567
Iteration 73/1000 | Loss: 0.00003567
Iteration 74/1000 | Loss: 0.00003567
Iteration 75/1000 | Loss: 0.00003567
Iteration 76/1000 | Loss: 0.00003566
Iteration 77/1000 | Loss: 0.00003566
Iteration 78/1000 | Loss: 0.00003566
Iteration 79/1000 | Loss: 0.00003566
Iteration 80/1000 | Loss: 0.00003566
Iteration 81/1000 | Loss: 0.00003566
Iteration 82/1000 | Loss: 0.00003566
Iteration 83/1000 | Loss: 0.00003566
Iteration 84/1000 | Loss: 0.00003565
Iteration 85/1000 | Loss: 0.00003565
Iteration 86/1000 | Loss: 0.00003565
Iteration 87/1000 | Loss: 0.00003564
Iteration 88/1000 | Loss: 0.00003564
Iteration 89/1000 | Loss: 0.00003564
Iteration 90/1000 | Loss: 0.00003564
Iteration 91/1000 | Loss: 0.00003563
Iteration 92/1000 | Loss: 0.00003562
Iteration 93/1000 | Loss: 0.00003561
Iteration 94/1000 | Loss: 0.00003561
Iteration 95/1000 | Loss: 0.00003561
Iteration 96/1000 | Loss: 0.00003561
Iteration 97/1000 | Loss: 0.00003561
Iteration 98/1000 | Loss: 0.00003561
Iteration 99/1000 | Loss: 0.00003561
Iteration 100/1000 | Loss: 0.00003561
Iteration 101/1000 | Loss: 0.00003561
Iteration 102/1000 | Loss: 0.00003561
Iteration 103/1000 | Loss: 0.00003560
Iteration 104/1000 | Loss: 0.00003560
Iteration 105/1000 | Loss: 0.00003560
Iteration 106/1000 | Loss: 0.00003560
Iteration 107/1000 | Loss: 0.00003560
Iteration 108/1000 | Loss: 0.00003560
Iteration 109/1000 | Loss: 0.00003560
Iteration 110/1000 | Loss: 0.00003560
Iteration 111/1000 | Loss: 0.00003559
Iteration 112/1000 | Loss: 0.00003559
Iteration 113/1000 | Loss: 0.00003559
Iteration 114/1000 | Loss: 0.00003559
Iteration 115/1000 | Loss: 0.00003559
Iteration 116/1000 | Loss: 0.00003559
Iteration 117/1000 | Loss: 0.00003559
Iteration 118/1000 | Loss: 0.00003559
Iteration 119/1000 | Loss: 0.00003559
Iteration 120/1000 | Loss: 0.00003559
Iteration 121/1000 | Loss: 0.00003559
Iteration 122/1000 | Loss: 0.00003558
Iteration 123/1000 | Loss: 0.00003558
Iteration 124/1000 | Loss: 0.00003558
Iteration 125/1000 | Loss: 0.00003558
Iteration 126/1000 | Loss: 0.00003558
Iteration 127/1000 | Loss: 0.00003558
Iteration 128/1000 | Loss: 0.00003558
Iteration 129/1000 | Loss: 0.00003558
Iteration 130/1000 | Loss: 0.00003558
Iteration 131/1000 | Loss: 0.00003558
Iteration 132/1000 | Loss: 0.00003557
Iteration 133/1000 | Loss: 0.00003557
Iteration 134/1000 | Loss: 0.00003556
Iteration 135/1000 | Loss: 0.00003556
Iteration 136/1000 | Loss: 0.00003555
Iteration 137/1000 | Loss: 0.00003555
Iteration 138/1000 | Loss: 0.00003555
Iteration 139/1000 | Loss: 0.00003555
Iteration 140/1000 | Loss: 0.00003554
Iteration 141/1000 | Loss: 0.00003554
Iteration 142/1000 | Loss: 0.00003554
Iteration 143/1000 | Loss: 0.00003554
Iteration 144/1000 | Loss: 0.00003554
Iteration 145/1000 | Loss: 0.00003554
Iteration 146/1000 | Loss: 0.00003554
Iteration 147/1000 | Loss: 0.00003554
Iteration 148/1000 | Loss: 0.00003554
Iteration 149/1000 | Loss: 0.00003553
Iteration 150/1000 | Loss: 0.00003553
Iteration 151/1000 | Loss: 0.00003553
Iteration 152/1000 | Loss: 0.00003553
Iteration 153/1000 | Loss: 0.00003553
Iteration 154/1000 | Loss: 0.00003553
Iteration 155/1000 | Loss: 0.00003553
Iteration 156/1000 | Loss: 0.00003553
Iteration 157/1000 | Loss: 0.00003552
Iteration 158/1000 | Loss: 0.00003552
Iteration 159/1000 | Loss: 0.00003552
Iteration 160/1000 | Loss: 0.00003552
Iteration 161/1000 | Loss: 0.00003552
Iteration 162/1000 | Loss: 0.00003552
Iteration 163/1000 | Loss: 0.00003552
Iteration 164/1000 | Loss: 0.00003552
Iteration 165/1000 | Loss: 0.00003551
Iteration 166/1000 | Loss: 0.00003551
Iteration 167/1000 | Loss: 0.00003551
Iteration 168/1000 | Loss: 0.00003551
Iteration 169/1000 | Loss: 0.00003550
Iteration 170/1000 | Loss: 0.00003550
Iteration 171/1000 | Loss: 0.00003550
Iteration 172/1000 | Loss: 0.00003550
Iteration 173/1000 | Loss: 0.00003550
Iteration 174/1000 | Loss: 0.00003550
Iteration 175/1000 | Loss: 0.00003549
Iteration 176/1000 | Loss: 0.00003549
Iteration 177/1000 | Loss: 0.00003549
Iteration 178/1000 | Loss: 0.00003549
Iteration 179/1000 | Loss: 0.00003549
Iteration 180/1000 | Loss: 0.00003549
Iteration 181/1000 | Loss: 0.00003549
Iteration 182/1000 | Loss: 0.00003549
Iteration 183/1000 | Loss: 0.00003549
Iteration 184/1000 | Loss: 0.00003549
Iteration 185/1000 | Loss: 0.00003549
Iteration 186/1000 | Loss: 0.00003549
Iteration 187/1000 | Loss: 0.00003549
Iteration 188/1000 | Loss: 0.00003549
Iteration 189/1000 | Loss: 0.00003549
Iteration 190/1000 | Loss: 0.00003549
Iteration 191/1000 | Loss: 0.00003549
Iteration 192/1000 | Loss: 0.00003549
Iteration 193/1000 | Loss: 0.00003549
Iteration 194/1000 | Loss: 0.00003549
Iteration 195/1000 | Loss: 0.00003549
Iteration 196/1000 | Loss: 0.00003549
Iteration 197/1000 | Loss: 0.00003549
Iteration 198/1000 | Loss: 0.00003549
Iteration 199/1000 | Loss: 0.00003549
Iteration 200/1000 | Loss: 0.00003549
Iteration 201/1000 | Loss: 0.00003549
Iteration 202/1000 | Loss: 0.00003549
Iteration 203/1000 | Loss: 0.00003549
Iteration 204/1000 | Loss: 0.00003549
Iteration 205/1000 | Loss: 0.00003549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [3.548560562194325e-05, 3.548560562194325e-05, 3.548560562194325e-05, 3.548560562194325e-05, 3.548560562194325e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.548560562194325e-05

Optimization complete. Final v2v error: 4.716803073883057 mm

Highest mean error: 4.733856201171875 mm for frame 225

Lowest mean error: 4.665092945098877 mm for frame 56

Saving results

Total time: 51.63364887237549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773981
Iteration 2/25 | Loss: 0.00211194
Iteration 3/25 | Loss: 0.00148604
Iteration 4/25 | Loss: 0.00132709
Iteration 5/25 | Loss: 0.00130702
Iteration 6/25 | Loss: 0.00125500
Iteration 7/25 | Loss: 0.00122518
Iteration 8/25 | Loss: 0.00122002
Iteration 9/25 | Loss: 0.00117969
Iteration 10/25 | Loss: 0.00117077
Iteration 11/25 | Loss: 0.00116954
Iteration 12/25 | Loss: 0.00116436
Iteration 13/25 | Loss: 0.00116719
Iteration 14/25 | Loss: 0.00116629
Iteration 15/25 | Loss: 0.00116590
Iteration 16/25 | Loss: 0.00116213
Iteration 17/25 | Loss: 0.00116165
Iteration 18/25 | Loss: 0.00116157
Iteration 19/25 | Loss: 0.00116157
Iteration 20/25 | Loss: 0.00116157
Iteration 21/25 | Loss: 0.00116157
Iteration 22/25 | Loss: 0.00116157
Iteration 23/25 | Loss: 0.00116156
Iteration 24/25 | Loss: 0.00116156
Iteration 25/25 | Loss: 0.00116156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13552666
Iteration 2/25 | Loss: 0.00072664
Iteration 3/25 | Loss: 0.00072664
Iteration 4/25 | Loss: 0.00067538
Iteration 5/25 | Loss: 0.00067538
Iteration 6/25 | Loss: 0.00067538
Iteration 7/25 | Loss: 0.00067538
Iteration 8/25 | Loss: 0.00067538
Iteration 9/25 | Loss: 0.00067538
Iteration 10/25 | Loss: 0.00067538
Iteration 11/25 | Loss: 0.00067538
Iteration 12/25 | Loss: 0.00067538
Iteration 13/25 | Loss: 0.00067538
Iteration 14/25 | Loss: 0.00067538
Iteration 15/25 | Loss: 0.00067538
Iteration 16/25 | Loss: 0.00067538
Iteration 17/25 | Loss: 0.00067538
Iteration 18/25 | Loss: 0.00067538
Iteration 19/25 | Loss: 0.00067538
Iteration 20/25 | Loss: 0.00067538
Iteration 21/25 | Loss: 0.00067538
Iteration 22/25 | Loss: 0.00067538
Iteration 23/25 | Loss: 0.00067538
Iteration 24/25 | Loss: 0.00067538
Iteration 25/25 | Loss: 0.00067538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067538
Iteration 2/1000 | Loss: 0.00002862
Iteration 3/1000 | Loss: 0.00009325
Iteration 4/1000 | Loss: 0.00008303
Iteration 5/1000 | Loss: 0.00002324
Iteration 6/1000 | Loss: 0.00045909
Iteration 7/1000 | Loss: 0.00076303
Iteration 8/1000 | Loss: 0.00003313
Iteration 9/1000 | Loss: 0.00004318
Iteration 10/1000 | Loss: 0.00002740
Iteration 11/1000 | Loss: 0.00001560
Iteration 12/1000 | Loss: 0.00015942
Iteration 13/1000 | Loss: 0.00004656
Iteration 14/1000 | Loss: 0.00005013
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00002956
Iteration 18/1000 | Loss: 0.00011797
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001476
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00011665
Iteration 23/1000 | Loss: 0.00067469
Iteration 24/1000 | Loss: 0.00003088
Iteration 25/1000 | Loss: 0.00005265
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00012720
Iteration 28/1000 | Loss: 0.00026622
Iteration 29/1000 | Loss: 0.00003046
Iteration 30/1000 | Loss: 0.00002325
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00006988
Iteration 33/1000 | Loss: 0.00001622
Iteration 34/1000 | Loss: 0.00002425
Iteration 35/1000 | Loss: 0.00017708
Iteration 36/1000 | Loss: 0.00049800
Iteration 37/1000 | Loss: 0.00017269
Iteration 38/1000 | Loss: 0.00018349
Iteration 39/1000 | Loss: 0.00003862
Iteration 40/1000 | Loss: 0.00005607
Iteration 41/1000 | Loss: 0.00002068
Iteration 42/1000 | Loss: 0.00002704
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00002398
Iteration 45/1000 | Loss: 0.00010937
Iteration 46/1000 | Loss: 0.00002426
Iteration 47/1000 | Loss: 0.00003399
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00003368
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001243
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001229
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00003904
Iteration 71/1000 | Loss: 0.00002881
Iteration 72/1000 | Loss: 0.00001396
Iteration 73/1000 | Loss: 0.00003947
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00014163
Iteration 76/1000 | Loss: 0.00017155
Iteration 77/1000 | Loss: 0.00012469
Iteration 78/1000 | Loss: 0.00004583
Iteration 79/1000 | Loss: 0.00002115
Iteration 80/1000 | Loss: 0.00001495
Iteration 81/1000 | Loss: 0.00001269
Iteration 82/1000 | Loss: 0.00001379
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001198
Iteration 101/1000 | Loss: 0.00001198
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Iteration 131/1000 | Loss: 0.00001191
Iteration 132/1000 | Loss: 0.00001191
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001190
Iteration 138/1000 | Loss: 0.00001190
Iteration 139/1000 | Loss: 0.00001190
Iteration 140/1000 | Loss: 0.00001190
Iteration 141/1000 | Loss: 0.00001190
Iteration 142/1000 | Loss: 0.00001190
Iteration 143/1000 | Loss: 0.00001189
Iteration 144/1000 | Loss: 0.00001189
Iteration 145/1000 | Loss: 0.00001189
Iteration 146/1000 | Loss: 0.00001189
Iteration 147/1000 | Loss: 0.00001189
Iteration 148/1000 | Loss: 0.00001189
Iteration 149/1000 | Loss: 0.00001189
Iteration 150/1000 | Loss: 0.00001189
Iteration 151/1000 | Loss: 0.00001189
Iteration 152/1000 | Loss: 0.00001189
Iteration 153/1000 | Loss: 0.00001189
Iteration 154/1000 | Loss: 0.00001189
Iteration 155/1000 | Loss: 0.00001189
Iteration 156/1000 | Loss: 0.00001189
Iteration 157/1000 | Loss: 0.00001189
Iteration 158/1000 | Loss: 0.00001188
Iteration 159/1000 | Loss: 0.00001188
Iteration 160/1000 | Loss: 0.00001188
Iteration 161/1000 | Loss: 0.00001188
Iteration 162/1000 | Loss: 0.00001188
Iteration 163/1000 | Loss: 0.00001188
Iteration 164/1000 | Loss: 0.00001188
Iteration 165/1000 | Loss: 0.00001188
Iteration 166/1000 | Loss: 0.00001188
Iteration 167/1000 | Loss: 0.00001188
Iteration 168/1000 | Loss: 0.00001188
Iteration 169/1000 | Loss: 0.00001188
Iteration 170/1000 | Loss: 0.00001188
Iteration 171/1000 | Loss: 0.00001188
Iteration 172/1000 | Loss: 0.00001188
Iteration 173/1000 | Loss: 0.00001188
Iteration 174/1000 | Loss: 0.00001188
Iteration 175/1000 | Loss: 0.00001188
Iteration 176/1000 | Loss: 0.00001188
Iteration 177/1000 | Loss: 0.00001188
Iteration 178/1000 | Loss: 0.00001188
Iteration 179/1000 | Loss: 0.00001188
Iteration 180/1000 | Loss: 0.00001188
Iteration 181/1000 | Loss: 0.00001188
Iteration 182/1000 | Loss: 0.00001188
Iteration 183/1000 | Loss: 0.00001188
Iteration 184/1000 | Loss: 0.00001188
Iteration 185/1000 | Loss: 0.00001188
Iteration 186/1000 | Loss: 0.00001188
Iteration 187/1000 | Loss: 0.00001188
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001188
Iteration 193/1000 | Loss: 0.00001188
Iteration 194/1000 | Loss: 0.00001188
Iteration 195/1000 | Loss: 0.00001188
Iteration 196/1000 | Loss: 0.00001188
Iteration 197/1000 | Loss: 0.00001188
Iteration 198/1000 | Loss: 0.00001188
Iteration 199/1000 | Loss: 0.00001188
Iteration 200/1000 | Loss: 0.00001188
Iteration 201/1000 | Loss: 0.00001188
Iteration 202/1000 | Loss: 0.00001188
Iteration 203/1000 | Loss: 0.00001188
Iteration 204/1000 | Loss: 0.00001188
Iteration 205/1000 | Loss: 0.00001188
Iteration 206/1000 | Loss: 0.00001188
Iteration 207/1000 | Loss: 0.00001188
Iteration 208/1000 | Loss: 0.00001188
Iteration 209/1000 | Loss: 0.00001188
Iteration 210/1000 | Loss: 0.00001188
Iteration 211/1000 | Loss: 0.00001188
Iteration 212/1000 | Loss: 0.00001188
Iteration 213/1000 | Loss: 0.00001188
Iteration 214/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.1883155821124092e-05, 1.1883155821124092e-05, 1.1883155821124092e-05, 1.1883155821124092e-05, 1.1883155821124092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1883155821124092e-05

Optimization complete. Final v2v error: 2.9509902000427246 mm

Highest mean error: 3.314302444458008 mm for frame 61

Lowest mean error: 2.8040428161621094 mm for frame 154

Saving results

Total time: 139.16537809371948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397551
Iteration 2/25 | Loss: 0.00129717
Iteration 3/25 | Loss: 0.00122000
Iteration 4/25 | Loss: 0.00121183
Iteration 5/25 | Loss: 0.00120918
Iteration 6/25 | Loss: 0.00120844
Iteration 7/25 | Loss: 0.00120816
Iteration 8/25 | Loss: 0.00120816
Iteration 9/25 | Loss: 0.00120816
Iteration 10/25 | Loss: 0.00120816
Iteration 11/25 | Loss: 0.00120816
Iteration 12/25 | Loss: 0.00120816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001208159257657826, 0.001208159257657826, 0.001208159257657826, 0.001208159257657826, 0.001208159257657826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001208159257657826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56348372
Iteration 2/25 | Loss: 0.00079045
Iteration 3/25 | Loss: 0.00079045
Iteration 4/25 | Loss: 0.00079045
Iteration 5/25 | Loss: 0.00079045
Iteration 6/25 | Loss: 0.00079045
Iteration 7/25 | Loss: 0.00079045
Iteration 8/25 | Loss: 0.00079044
Iteration 9/25 | Loss: 0.00079044
Iteration 10/25 | Loss: 0.00079044
Iteration 11/25 | Loss: 0.00079044
Iteration 12/25 | Loss: 0.00079044
Iteration 13/25 | Loss: 0.00079044
Iteration 14/25 | Loss: 0.00079044
Iteration 15/25 | Loss: 0.00079044
Iteration 16/25 | Loss: 0.00079044
Iteration 17/25 | Loss: 0.00079044
Iteration 18/25 | Loss: 0.00079044
Iteration 19/25 | Loss: 0.00079044
Iteration 20/25 | Loss: 0.00079044
Iteration 21/25 | Loss: 0.00079044
Iteration 22/25 | Loss: 0.00079044
Iteration 23/25 | Loss: 0.00079044
Iteration 24/25 | Loss: 0.00079044
Iteration 25/25 | Loss: 0.00079044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079044
Iteration 2/1000 | Loss: 0.00004014
Iteration 3/1000 | Loss: 0.00002619
Iteration 4/1000 | Loss: 0.00001945
Iteration 5/1000 | Loss: 0.00001781
Iteration 6/1000 | Loss: 0.00001677
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001570
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001517
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001487
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001476
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001471
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00001462
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001462
Iteration 38/1000 | Loss: 0.00001460
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001459
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001456
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001455
Iteration 47/1000 | Loss: 0.00001455
Iteration 48/1000 | Loss: 0.00001454
Iteration 49/1000 | Loss: 0.00001454
Iteration 50/1000 | Loss: 0.00001454
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001453
Iteration 53/1000 | Loss: 0.00001453
Iteration 54/1000 | Loss: 0.00001453
Iteration 55/1000 | Loss: 0.00001453
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001453
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001452
Iteration 61/1000 | Loss: 0.00001452
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001450
Iteration 70/1000 | Loss: 0.00001450
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001449
Iteration 73/1000 | Loss: 0.00001449
Iteration 74/1000 | Loss: 0.00001449
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001447
Iteration 92/1000 | Loss: 0.00001447
Iteration 93/1000 | Loss: 0.00001447
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001446
Iteration 96/1000 | Loss: 0.00001446
Iteration 97/1000 | Loss: 0.00001446
Iteration 98/1000 | Loss: 0.00001446
Iteration 99/1000 | Loss: 0.00001446
Iteration 100/1000 | Loss: 0.00001446
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001446
Iteration 112/1000 | Loss: 0.00001446
Iteration 113/1000 | Loss: 0.00001446
Iteration 114/1000 | Loss: 0.00001446
Iteration 115/1000 | Loss: 0.00001446
Iteration 116/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.4464920241152868e-05, 1.4464920241152868e-05, 1.4464920241152868e-05, 1.4464920241152868e-05, 1.4464920241152868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4464920241152868e-05

Optimization complete. Final v2v error: 3.166590690612793 mm

Highest mean error: 4.507017135620117 mm for frame 55

Lowest mean error: 2.793308973312378 mm for frame 30

Saving results

Total time: 36.15054798126221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973561
Iteration 2/25 | Loss: 0.00176268
Iteration 3/25 | Loss: 0.00149798
Iteration 4/25 | Loss: 0.00143840
Iteration 5/25 | Loss: 0.00141578
Iteration 6/25 | Loss: 0.00141024
Iteration 7/25 | Loss: 0.00140884
Iteration 8/25 | Loss: 0.00140884
Iteration 9/25 | Loss: 0.00140884
Iteration 10/25 | Loss: 0.00140884
Iteration 11/25 | Loss: 0.00140884
Iteration 12/25 | Loss: 0.00140884
Iteration 13/25 | Loss: 0.00140884
Iteration 14/25 | Loss: 0.00140884
Iteration 15/25 | Loss: 0.00140884
Iteration 16/25 | Loss: 0.00140884
Iteration 17/25 | Loss: 0.00140884
Iteration 18/25 | Loss: 0.00140884
Iteration 19/25 | Loss: 0.00140884
Iteration 20/25 | Loss: 0.00140884
Iteration 21/25 | Loss: 0.00140884
Iteration 22/25 | Loss: 0.00140884
Iteration 23/25 | Loss: 0.00140884
Iteration 24/25 | Loss: 0.00140884
Iteration 25/25 | Loss: 0.00140884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35603225
Iteration 2/25 | Loss: 0.00215305
Iteration 3/25 | Loss: 0.00215270
Iteration 4/25 | Loss: 0.00215270
Iteration 5/25 | Loss: 0.00215270
Iteration 6/25 | Loss: 0.00215270
Iteration 7/25 | Loss: 0.00215270
Iteration 8/25 | Loss: 0.00215270
Iteration 9/25 | Loss: 0.00215270
Iteration 10/25 | Loss: 0.00215270
Iteration 11/25 | Loss: 0.00215270
Iteration 12/25 | Loss: 0.00215270
Iteration 13/25 | Loss: 0.00215270
Iteration 14/25 | Loss: 0.00215270
Iteration 15/25 | Loss: 0.00215270
Iteration 16/25 | Loss: 0.00215270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021527011413127184, 0.0021527011413127184, 0.0021527011413127184, 0.0021527011413127184, 0.0021527011413127184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021527011413127184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215270
Iteration 2/1000 | Loss: 0.00022357
Iteration 3/1000 | Loss: 0.00015891
Iteration 4/1000 | Loss: 0.00013519
Iteration 5/1000 | Loss: 0.00012139
Iteration 6/1000 | Loss: 0.00011536
Iteration 7/1000 | Loss: 0.00011073
Iteration 8/1000 | Loss: 0.00010633
Iteration 9/1000 | Loss: 0.00010326
Iteration 10/1000 | Loss: 0.00009998
Iteration 11/1000 | Loss: 0.00069901
Iteration 12/1000 | Loss: 0.00167204
Iteration 13/1000 | Loss: 0.00181768
Iteration 14/1000 | Loss: 0.00211336
Iteration 15/1000 | Loss: 0.00104178
Iteration 16/1000 | Loss: 0.00159903
Iteration 17/1000 | Loss: 0.00131027
Iteration 18/1000 | Loss: 0.00041025
Iteration 19/1000 | Loss: 0.00026926
Iteration 20/1000 | Loss: 0.00011502
Iteration 21/1000 | Loss: 0.00010034
Iteration 22/1000 | Loss: 0.00084030
Iteration 23/1000 | Loss: 0.00056555
Iteration 24/1000 | Loss: 0.00034239
Iteration 25/1000 | Loss: 0.00010079
Iteration 26/1000 | Loss: 0.00079044
Iteration 27/1000 | Loss: 0.00026319
Iteration 28/1000 | Loss: 0.00089370
Iteration 29/1000 | Loss: 0.00047828
Iteration 30/1000 | Loss: 0.00028636
Iteration 31/1000 | Loss: 0.00022609
Iteration 32/1000 | Loss: 0.00075489
Iteration 33/1000 | Loss: 0.00048595
Iteration 34/1000 | Loss: 0.00061420
Iteration 35/1000 | Loss: 0.00009263
Iteration 36/1000 | Loss: 0.00008619
Iteration 37/1000 | Loss: 0.00008342
Iteration 38/1000 | Loss: 0.00008197
Iteration 39/1000 | Loss: 0.00007989
Iteration 40/1000 | Loss: 0.00085722
Iteration 41/1000 | Loss: 0.00150983
Iteration 42/1000 | Loss: 0.00153958
Iteration 43/1000 | Loss: 0.00116794
Iteration 44/1000 | Loss: 0.00126758
Iteration 45/1000 | Loss: 0.00006500
Iteration 46/1000 | Loss: 0.00006136
Iteration 47/1000 | Loss: 0.00005778
Iteration 48/1000 | Loss: 0.00082366
Iteration 49/1000 | Loss: 0.00022127
Iteration 50/1000 | Loss: 0.00005566
Iteration 51/1000 | Loss: 0.00004858
Iteration 52/1000 | Loss: 0.00004619
Iteration 53/1000 | Loss: 0.00004455
Iteration 54/1000 | Loss: 0.00004353
Iteration 55/1000 | Loss: 0.00004241
Iteration 56/1000 | Loss: 0.00004175
Iteration 57/1000 | Loss: 0.00004119
Iteration 58/1000 | Loss: 0.00004084
Iteration 59/1000 | Loss: 0.00004052
Iteration 60/1000 | Loss: 0.00004025
Iteration 61/1000 | Loss: 0.00004004
Iteration 62/1000 | Loss: 0.00003993
Iteration 63/1000 | Loss: 0.00003987
Iteration 64/1000 | Loss: 0.00003987
Iteration 65/1000 | Loss: 0.00003981
Iteration 66/1000 | Loss: 0.00003975
Iteration 67/1000 | Loss: 0.00003967
Iteration 68/1000 | Loss: 0.00003964
Iteration 69/1000 | Loss: 0.00003962
Iteration 70/1000 | Loss: 0.00003958
Iteration 71/1000 | Loss: 0.00003956
Iteration 72/1000 | Loss: 0.00003956
Iteration 73/1000 | Loss: 0.00003955
Iteration 74/1000 | Loss: 0.00003950
Iteration 75/1000 | Loss: 0.00003950
Iteration 76/1000 | Loss: 0.00003946
Iteration 77/1000 | Loss: 0.00003945
Iteration 78/1000 | Loss: 0.00003945
Iteration 79/1000 | Loss: 0.00003945
Iteration 80/1000 | Loss: 0.00003944
Iteration 81/1000 | Loss: 0.00003944
Iteration 82/1000 | Loss: 0.00003943
Iteration 83/1000 | Loss: 0.00003943
Iteration 84/1000 | Loss: 0.00003942
Iteration 85/1000 | Loss: 0.00003939
Iteration 86/1000 | Loss: 0.00003939
Iteration 87/1000 | Loss: 0.00003939
Iteration 88/1000 | Loss: 0.00003938
Iteration 89/1000 | Loss: 0.00003938
Iteration 90/1000 | Loss: 0.00003937
Iteration 91/1000 | Loss: 0.00003937
Iteration 92/1000 | Loss: 0.00003936
Iteration 93/1000 | Loss: 0.00003936
Iteration 94/1000 | Loss: 0.00003935
Iteration 95/1000 | Loss: 0.00003935
Iteration 96/1000 | Loss: 0.00003934
Iteration 97/1000 | Loss: 0.00003934
Iteration 98/1000 | Loss: 0.00003933
Iteration 99/1000 | Loss: 0.00003933
Iteration 100/1000 | Loss: 0.00003933
Iteration 101/1000 | Loss: 0.00003932
Iteration 102/1000 | Loss: 0.00003932
Iteration 103/1000 | Loss: 0.00003932
Iteration 104/1000 | Loss: 0.00003932
Iteration 105/1000 | Loss: 0.00003931
Iteration 106/1000 | Loss: 0.00003931
Iteration 107/1000 | Loss: 0.00003931
Iteration 108/1000 | Loss: 0.00003930
Iteration 109/1000 | Loss: 0.00003930
Iteration 110/1000 | Loss: 0.00003930
Iteration 111/1000 | Loss: 0.00003930
Iteration 112/1000 | Loss: 0.00003929
Iteration 113/1000 | Loss: 0.00003929
Iteration 114/1000 | Loss: 0.00003928
Iteration 115/1000 | Loss: 0.00003928
Iteration 116/1000 | Loss: 0.00003928
Iteration 117/1000 | Loss: 0.00003928
Iteration 118/1000 | Loss: 0.00003927
Iteration 119/1000 | Loss: 0.00003927
Iteration 120/1000 | Loss: 0.00003927
Iteration 121/1000 | Loss: 0.00003927
Iteration 122/1000 | Loss: 0.00003927
Iteration 123/1000 | Loss: 0.00003927
Iteration 124/1000 | Loss: 0.00003927
Iteration 125/1000 | Loss: 0.00003926
Iteration 126/1000 | Loss: 0.00003926
Iteration 127/1000 | Loss: 0.00003926
Iteration 128/1000 | Loss: 0.00003926
Iteration 129/1000 | Loss: 0.00003926
Iteration 130/1000 | Loss: 0.00003925
Iteration 131/1000 | Loss: 0.00003925
Iteration 132/1000 | Loss: 0.00003925
Iteration 133/1000 | Loss: 0.00003925
Iteration 134/1000 | Loss: 0.00003925
Iteration 135/1000 | Loss: 0.00003925
Iteration 136/1000 | Loss: 0.00003925
Iteration 137/1000 | Loss: 0.00003925
Iteration 138/1000 | Loss: 0.00003925
Iteration 139/1000 | Loss: 0.00003925
Iteration 140/1000 | Loss: 0.00003925
Iteration 141/1000 | Loss: 0.00003924
Iteration 142/1000 | Loss: 0.00003923
Iteration 143/1000 | Loss: 0.00003923
Iteration 144/1000 | Loss: 0.00003923
Iteration 145/1000 | Loss: 0.00003923
Iteration 146/1000 | Loss: 0.00003923
Iteration 147/1000 | Loss: 0.00003923
Iteration 148/1000 | Loss: 0.00003923
Iteration 149/1000 | Loss: 0.00003923
Iteration 150/1000 | Loss: 0.00003923
Iteration 151/1000 | Loss: 0.00003923
Iteration 152/1000 | Loss: 0.00003923
Iteration 153/1000 | Loss: 0.00003923
Iteration 154/1000 | Loss: 0.00003923
Iteration 155/1000 | Loss: 0.00003923
Iteration 156/1000 | Loss: 0.00003923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [3.9225717046065256e-05, 3.9225717046065256e-05, 3.9225717046065256e-05, 3.9225717046065256e-05, 3.9225717046065256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9225717046065256e-05

Optimization complete. Final v2v error: 4.966932773590088 mm

Highest mean error: 6.771188259124756 mm for frame 182

Lowest mean error: 3.1360862255096436 mm for frame 38

Saving results

Total time: 125.12622690200806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497618
Iteration 2/25 | Loss: 0.00145595
Iteration 3/25 | Loss: 0.00131482
Iteration 4/25 | Loss: 0.00129769
Iteration 5/25 | Loss: 0.00129357
Iteration 6/25 | Loss: 0.00129270
Iteration 7/25 | Loss: 0.00129270
Iteration 8/25 | Loss: 0.00129270
Iteration 9/25 | Loss: 0.00129270
Iteration 10/25 | Loss: 0.00129270
Iteration 11/25 | Loss: 0.00129270
Iteration 12/25 | Loss: 0.00129270
Iteration 13/25 | Loss: 0.00129270
Iteration 14/25 | Loss: 0.00129270
Iteration 15/25 | Loss: 0.00129270
Iteration 16/25 | Loss: 0.00129270
Iteration 17/25 | Loss: 0.00129270
Iteration 18/25 | Loss: 0.00129270
Iteration 19/25 | Loss: 0.00129270
Iteration 20/25 | Loss: 0.00129270
Iteration 21/25 | Loss: 0.00129270
Iteration 22/25 | Loss: 0.00129270
Iteration 23/25 | Loss: 0.00129270
Iteration 24/25 | Loss: 0.00129270
Iteration 25/25 | Loss: 0.00129270

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46764219
Iteration 2/25 | Loss: 0.00072582
Iteration 3/25 | Loss: 0.00072580
Iteration 4/25 | Loss: 0.00072580
Iteration 5/25 | Loss: 0.00072580
Iteration 6/25 | Loss: 0.00072580
Iteration 7/25 | Loss: 0.00072580
Iteration 8/25 | Loss: 0.00072580
Iteration 9/25 | Loss: 0.00072580
Iteration 10/25 | Loss: 0.00072580
Iteration 11/25 | Loss: 0.00072580
Iteration 12/25 | Loss: 0.00072580
Iteration 13/25 | Loss: 0.00072580
Iteration 14/25 | Loss: 0.00072580
Iteration 15/25 | Loss: 0.00072580
Iteration 16/25 | Loss: 0.00072580
Iteration 17/25 | Loss: 0.00072580
Iteration 18/25 | Loss: 0.00072580
Iteration 19/25 | Loss: 0.00072580
Iteration 20/25 | Loss: 0.00072580
Iteration 21/25 | Loss: 0.00072580
Iteration 22/25 | Loss: 0.00072580
Iteration 23/25 | Loss: 0.00072580
Iteration 24/25 | Loss: 0.00072580
Iteration 25/25 | Loss: 0.00072580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072580
Iteration 2/1000 | Loss: 0.00005271
Iteration 3/1000 | Loss: 0.00003356
Iteration 4/1000 | Loss: 0.00002863
Iteration 5/1000 | Loss: 0.00002703
Iteration 6/1000 | Loss: 0.00002597
Iteration 7/1000 | Loss: 0.00002538
Iteration 8/1000 | Loss: 0.00002481
Iteration 9/1000 | Loss: 0.00002444
Iteration 10/1000 | Loss: 0.00002429
Iteration 11/1000 | Loss: 0.00002419
Iteration 12/1000 | Loss: 0.00002411
Iteration 13/1000 | Loss: 0.00002410
Iteration 14/1000 | Loss: 0.00002407
Iteration 15/1000 | Loss: 0.00002395
Iteration 16/1000 | Loss: 0.00002390
Iteration 17/1000 | Loss: 0.00002387
Iteration 18/1000 | Loss: 0.00002387
Iteration 19/1000 | Loss: 0.00002386
Iteration 20/1000 | Loss: 0.00002385
Iteration 21/1000 | Loss: 0.00002385
Iteration 22/1000 | Loss: 0.00002384
Iteration 23/1000 | Loss: 0.00002384
Iteration 24/1000 | Loss: 0.00002383
Iteration 25/1000 | Loss: 0.00002383
Iteration 26/1000 | Loss: 0.00002383
Iteration 27/1000 | Loss: 0.00002382
Iteration 28/1000 | Loss: 0.00002381
Iteration 29/1000 | Loss: 0.00002381
Iteration 30/1000 | Loss: 0.00002379
Iteration 31/1000 | Loss: 0.00002377
Iteration 32/1000 | Loss: 0.00002377
Iteration 33/1000 | Loss: 0.00002376
Iteration 34/1000 | Loss: 0.00002376
Iteration 35/1000 | Loss: 0.00002375
Iteration 36/1000 | Loss: 0.00002375
Iteration 37/1000 | Loss: 0.00002374
Iteration 38/1000 | Loss: 0.00002374
Iteration 39/1000 | Loss: 0.00002372
Iteration 40/1000 | Loss: 0.00002371
Iteration 41/1000 | Loss: 0.00002371
Iteration 42/1000 | Loss: 0.00002371
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002370
Iteration 45/1000 | Loss: 0.00002370
Iteration 46/1000 | Loss: 0.00002370
Iteration 47/1000 | Loss: 0.00002369
Iteration 48/1000 | Loss: 0.00002369
Iteration 49/1000 | Loss: 0.00002368
Iteration 50/1000 | Loss: 0.00002368
Iteration 51/1000 | Loss: 0.00002368
Iteration 52/1000 | Loss: 0.00002368
Iteration 53/1000 | Loss: 0.00002367
Iteration 54/1000 | Loss: 0.00002366
Iteration 55/1000 | Loss: 0.00002366
Iteration 56/1000 | Loss: 0.00002366
Iteration 57/1000 | Loss: 0.00002366
Iteration 58/1000 | Loss: 0.00002365
Iteration 59/1000 | Loss: 0.00002365
Iteration 60/1000 | Loss: 0.00002365
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002364
Iteration 63/1000 | Loss: 0.00002364
Iteration 64/1000 | Loss: 0.00002363
Iteration 65/1000 | Loss: 0.00002363
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002362
Iteration 68/1000 | Loss: 0.00002362
Iteration 69/1000 | Loss: 0.00002361
Iteration 70/1000 | Loss: 0.00002361
Iteration 71/1000 | Loss: 0.00002361
Iteration 72/1000 | Loss: 0.00002360
Iteration 73/1000 | Loss: 0.00002360
Iteration 74/1000 | Loss: 0.00002359
Iteration 75/1000 | Loss: 0.00002359
Iteration 76/1000 | Loss: 0.00002359
Iteration 77/1000 | Loss: 0.00002359
Iteration 78/1000 | Loss: 0.00002358
Iteration 79/1000 | Loss: 0.00002358
Iteration 80/1000 | Loss: 0.00002358
Iteration 81/1000 | Loss: 0.00002358
Iteration 82/1000 | Loss: 0.00002358
Iteration 83/1000 | Loss: 0.00002358
Iteration 84/1000 | Loss: 0.00002357
Iteration 85/1000 | Loss: 0.00002357
Iteration 86/1000 | Loss: 0.00002357
Iteration 87/1000 | Loss: 0.00002356
Iteration 88/1000 | Loss: 0.00002356
Iteration 89/1000 | Loss: 0.00002356
Iteration 90/1000 | Loss: 0.00002355
Iteration 91/1000 | Loss: 0.00002355
Iteration 92/1000 | Loss: 0.00002354
Iteration 93/1000 | Loss: 0.00002354
Iteration 94/1000 | Loss: 0.00002354
Iteration 95/1000 | Loss: 0.00002353
Iteration 96/1000 | Loss: 0.00002353
Iteration 97/1000 | Loss: 0.00002353
Iteration 98/1000 | Loss: 0.00002353
Iteration 99/1000 | Loss: 0.00002353
Iteration 100/1000 | Loss: 0.00002353
Iteration 101/1000 | Loss: 0.00002353
Iteration 102/1000 | Loss: 0.00002352
Iteration 103/1000 | Loss: 0.00002352
Iteration 104/1000 | Loss: 0.00002352
Iteration 105/1000 | Loss: 0.00002352
Iteration 106/1000 | Loss: 0.00002352
Iteration 107/1000 | Loss: 0.00002351
Iteration 108/1000 | Loss: 0.00002351
Iteration 109/1000 | Loss: 0.00002351
Iteration 110/1000 | Loss: 0.00002351
Iteration 111/1000 | Loss: 0.00002350
Iteration 112/1000 | Loss: 0.00002350
Iteration 113/1000 | Loss: 0.00002350
Iteration 114/1000 | Loss: 0.00002350
Iteration 115/1000 | Loss: 0.00002350
Iteration 116/1000 | Loss: 0.00002350
Iteration 117/1000 | Loss: 0.00002350
Iteration 118/1000 | Loss: 0.00002350
Iteration 119/1000 | Loss: 0.00002350
Iteration 120/1000 | Loss: 0.00002349
Iteration 121/1000 | Loss: 0.00002349
Iteration 122/1000 | Loss: 0.00002349
Iteration 123/1000 | Loss: 0.00002349
Iteration 124/1000 | Loss: 0.00002349
Iteration 125/1000 | Loss: 0.00002349
Iteration 126/1000 | Loss: 0.00002349
Iteration 127/1000 | Loss: 0.00002349
Iteration 128/1000 | Loss: 0.00002348
Iteration 129/1000 | Loss: 0.00002348
Iteration 130/1000 | Loss: 0.00002348
Iteration 131/1000 | Loss: 0.00002348
Iteration 132/1000 | Loss: 0.00002348
Iteration 133/1000 | Loss: 0.00002348
Iteration 134/1000 | Loss: 0.00002348
Iteration 135/1000 | Loss: 0.00002348
Iteration 136/1000 | Loss: 0.00002348
Iteration 137/1000 | Loss: 0.00002348
Iteration 138/1000 | Loss: 0.00002348
Iteration 139/1000 | Loss: 0.00002348
Iteration 140/1000 | Loss: 0.00002347
Iteration 141/1000 | Loss: 0.00002347
Iteration 142/1000 | Loss: 0.00002347
Iteration 143/1000 | Loss: 0.00002347
Iteration 144/1000 | Loss: 0.00002347
Iteration 145/1000 | Loss: 0.00002347
Iteration 146/1000 | Loss: 0.00002346
Iteration 147/1000 | Loss: 0.00002346
Iteration 148/1000 | Loss: 0.00002346
Iteration 149/1000 | Loss: 0.00002346
Iteration 150/1000 | Loss: 0.00002346
Iteration 151/1000 | Loss: 0.00002346
Iteration 152/1000 | Loss: 0.00002346
Iteration 153/1000 | Loss: 0.00002346
Iteration 154/1000 | Loss: 0.00002346
Iteration 155/1000 | Loss: 0.00002346
Iteration 156/1000 | Loss: 0.00002345
Iteration 157/1000 | Loss: 0.00002345
Iteration 158/1000 | Loss: 0.00002345
Iteration 159/1000 | Loss: 0.00002345
Iteration 160/1000 | Loss: 0.00002344
Iteration 161/1000 | Loss: 0.00002344
Iteration 162/1000 | Loss: 0.00002344
Iteration 163/1000 | Loss: 0.00002344
Iteration 164/1000 | Loss: 0.00002344
Iteration 165/1000 | Loss: 0.00002344
Iteration 166/1000 | Loss: 0.00002344
Iteration 167/1000 | Loss: 0.00002344
Iteration 168/1000 | Loss: 0.00002344
Iteration 169/1000 | Loss: 0.00002344
Iteration 170/1000 | Loss: 0.00002344
Iteration 171/1000 | Loss: 0.00002344
Iteration 172/1000 | Loss: 0.00002344
Iteration 173/1000 | Loss: 0.00002343
Iteration 174/1000 | Loss: 0.00002343
Iteration 175/1000 | Loss: 0.00002343
Iteration 176/1000 | Loss: 0.00002343
Iteration 177/1000 | Loss: 0.00002343
Iteration 178/1000 | Loss: 0.00002343
Iteration 179/1000 | Loss: 0.00002342
Iteration 180/1000 | Loss: 0.00002342
Iteration 181/1000 | Loss: 0.00002342
Iteration 182/1000 | Loss: 0.00002342
Iteration 183/1000 | Loss: 0.00002342
Iteration 184/1000 | Loss: 0.00002342
Iteration 185/1000 | Loss: 0.00002341
Iteration 186/1000 | Loss: 0.00002341
Iteration 187/1000 | Loss: 0.00002341
Iteration 188/1000 | Loss: 0.00002341
Iteration 189/1000 | Loss: 0.00002341
Iteration 190/1000 | Loss: 0.00002341
Iteration 191/1000 | Loss: 0.00002341
Iteration 192/1000 | Loss: 0.00002341
Iteration 193/1000 | Loss: 0.00002341
Iteration 194/1000 | Loss: 0.00002341
Iteration 195/1000 | Loss: 0.00002341
Iteration 196/1000 | Loss: 0.00002340
Iteration 197/1000 | Loss: 0.00002340
Iteration 198/1000 | Loss: 0.00002340
Iteration 199/1000 | Loss: 0.00002340
Iteration 200/1000 | Loss: 0.00002340
Iteration 201/1000 | Loss: 0.00002340
Iteration 202/1000 | Loss: 0.00002340
Iteration 203/1000 | Loss: 0.00002340
Iteration 204/1000 | Loss: 0.00002340
Iteration 205/1000 | Loss: 0.00002340
Iteration 206/1000 | Loss: 0.00002340
Iteration 207/1000 | Loss: 0.00002340
Iteration 208/1000 | Loss: 0.00002340
Iteration 209/1000 | Loss: 0.00002340
Iteration 210/1000 | Loss: 0.00002340
Iteration 211/1000 | Loss: 0.00002340
Iteration 212/1000 | Loss: 0.00002339
Iteration 213/1000 | Loss: 0.00002339
Iteration 214/1000 | Loss: 0.00002339
Iteration 215/1000 | Loss: 0.00002339
Iteration 216/1000 | Loss: 0.00002339
Iteration 217/1000 | Loss: 0.00002339
Iteration 218/1000 | Loss: 0.00002339
Iteration 219/1000 | Loss: 0.00002339
Iteration 220/1000 | Loss: 0.00002339
Iteration 221/1000 | Loss: 0.00002339
Iteration 222/1000 | Loss: 0.00002339
Iteration 223/1000 | Loss: 0.00002339
Iteration 224/1000 | Loss: 0.00002339
Iteration 225/1000 | Loss: 0.00002339
Iteration 226/1000 | Loss: 0.00002339
Iteration 227/1000 | Loss: 0.00002339
Iteration 228/1000 | Loss: 0.00002339
Iteration 229/1000 | Loss: 0.00002339
Iteration 230/1000 | Loss: 0.00002339
Iteration 231/1000 | Loss: 0.00002339
Iteration 232/1000 | Loss: 0.00002339
Iteration 233/1000 | Loss: 0.00002339
Iteration 234/1000 | Loss: 0.00002339
Iteration 235/1000 | Loss: 0.00002339
Iteration 236/1000 | Loss: 0.00002339
Iteration 237/1000 | Loss: 0.00002339
Iteration 238/1000 | Loss: 0.00002339
Iteration 239/1000 | Loss: 0.00002339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.3386601242236793e-05, 2.3386601242236793e-05, 2.3386601242236793e-05, 2.3386601242236793e-05, 2.3386601242236793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3386601242236793e-05

Optimization complete. Final v2v error: 3.926297664642334 mm

Highest mean error: 4.528027534484863 mm for frame 132

Lowest mean error: 3.3778891563415527 mm for frame 0

Saving results

Total time: 43.47774291038513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484518
Iteration 2/25 | Loss: 0.00126442
Iteration 3/25 | Loss: 0.00119906
Iteration 4/25 | Loss: 0.00118516
Iteration 5/25 | Loss: 0.00118032
Iteration 6/25 | Loss: 0.00117936
Iteration 7/25 | Loss: 0.00117936
Iteration 8/25 | Loss: 0.00117936
Iteration 9/25 | Loss: 0.00117936
Iteration 10/25 | Loss: 0.00117936
Iteration 11/25 | Loss: 0.00117936
Iteration 12/25 | Loss: 0.00117936
Iteration 13/25 | Loss: 0.00117936
Iteration 14/25 | Loss: 0.00117936
Iteration 15/25 | Loss: 0.00117936
Iteration 16/25 | Loss: 0.00117936
Iteration 17/25 | Loss: 0.00117936
Iteration 18/25 | Loss: 0.00117936
Iteration 19/25 | Loss: 0.00117936
Iteration 20/25 | Loss: 0.00117936
Iteration 21/25 | Loss: 0.00117936
Iteration 22/25 | Loss: 0.00117936
Iteration 23/25 | Loss: 0.00117936
Iteration 24/25 | Loss: 0.00117936
Iteration 25/25 | Loss: 0.00117936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81542206
Iteration 2/25 | Loss: 0.00065357
Iteration 3/25 | Loss: 0.00065356
Iteration 4/25 | Loss: 0.00065356
Iteration 5/25 | Loss: 0.00065356
Iteration 6/25 | Loss: 0.00065356
Iteration 7/25 | Loss: 0.00065356
Iteration 8/25 | Loss: 0.00065356
Iteration 9/25 | Loss: 0.00065356
Iteration 10/25 | Loss: 0.00065356
Iteration 11/25 | Loss: 0.00065356
Iteration 12/25 | Loss: 0.00065356
Iteration 13/25 | Loss: 0.00065356
Iteration 14/25 | Loss: 0.00065356
Iteration 15/25 | Loss: 0.00065356
Iteration 16/25 | Loss: 0.00065356
Iteration 17/25 | Loss: 0.00065356
Iteration 18/25 | Loss: 0.00065356
Iteration 19/25 | Loss: 0.00065356
Iteration 20/25 | Loss: 0.00065356
Iteration 21/25 | Loss: 0.00065356
Iteration 22/25 | Loss: 0.00065356
Iteration 23/25 | Loss: 0.00065356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006535614375025034, 0.0006535614375025034, 0.0006535614375025034, 0.0006535614375025034, 0.0006535614375025034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006535614375025034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065356
Iteration 2/1000 | Loss: 0.00003560
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001556
Iteration 8/1000 | Loss: 0.00001524
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001472
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001468
Iteration 14/1000 | Loss: 0.00001462
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001455
Iteration 19/1000 | Loss: 0.00001451
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001444
Iteration 24/1000 | Loss: 0.00001443
Iteration 25/1000 | Loss: 0.00001438
Iteration 26/1000 | Loss: 0.00001436
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001435
Iteration 29/1000 | Loss: 0.00001435
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001430
Iteration 32/1000 | Loss: 0.00001430
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00001430
Iteration 35/1000 | Loss: 0.00001429
Iteration 36/1000 | Loss: 0.00001429
Iteration 37/1000 | Loss: 0.00001429
Iteration 38/1000 | Loss: 0.00001429
Iteration 39/1000 | Loss: 0.00001429
Iteration 40/1000 | Loss: 0.00001429
Iteration 41/1000 | Loss: 0.00001428
Iteration 42/1000 | Loss: 0.00001425
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001424
Iteration 45/1000 | Loss: 0.00001424
Iteration 46/1000 | Loss: 0.00001423
Iteration 47/1000 | Loss: 0.00001423
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001418
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001417
Iteration 62/1000 | Loss: 0.00001417
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001416
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001414
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001411
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001410
Iteration 73/1000 | Loss: 0.00001410
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001408
Iteration 78/1000 | Loss: 0.00001408
Iteration 79/1000 | Loss: 0.00001408
Iteration 80/1000 | Loss: 0.00001407
Iteration 81/1000 | Loss: 0.00001407
Iteration 82/1000 | Loss: 0.00001407
Iteration 83/1000 | Loss: 0.00001407
Iteration 84/1000 | Loss: 0.00001407
Iteration 85/1000 | Loss: 0.00001406
Iteration 86/1000 | Loss: 0.00001406
Iteration 87/1000 | Loss: 0.00001406
Iteration 88/1000 | Loss: 0.00001406
Iteration 89/1000 | Loss: 0.00001406
Iteration 90/1000 | Loss: 0.00001406
Iteration 91/1000 | Loss: 0.00001406
Iteration 92/1000 | Loss: 0.00001406
Iteration 93/1000 | Loss: 0.00001406
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001405
Iteration 96/1000 | Loss: 0.00001405
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001402
Iteration 111/1000 | Loss: 0.00001402
Iteration 112/1000 | Loss: 0.00001402
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001401
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001399
Iteration 121/1000 | Loss: 0.00001399
Iteration 122/1000 | Loss: 0.00001399
Iteration 123/1000 | Loss: 0.00001399
Iteration 124/1000 | Loss: 0.00001399
Iteration 125/1000 | Loss: 0.00001399
Iteration 126/1000 | Loss: 0.00001399
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001399
Iteration 130/1000 | Loss: 0.00001399
Iteration 131/1000 | Loss: 0.00001399
Iteration 132/1000 | Loss: 0.00001399
Iteration 133/1000 | Loss: 0.00001399
Iteration 134/1000 | Loss: 0.00001399
Iteration 135/1000 | Loss: 0.00001399
Iteration 136/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.398758922732668e-05, 1.398758922732668e-05, 1.398758922732668e-05, 1.398758922732668e-05, 1.398758922732668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.398758922732668e-05

Optimization complete. Final v2v error: 3.2049224376678467 mm

Highest mean error: 3.7051737308502197 mm for frame 126

Lowest mean error: 2.9945616722106934 mm for frame 76

Saving results

Total time: 39.52336812019348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772730
Iteration 2/25 | Loss: 0.00151230
Iteration 3/25 | Loss: 0.00133932
Iteration 4/25 | Loss: 0.00129153
Iteration 5/25 | Loss: 0.00126377
Iteration 6/25 | Loss: 0.00124831
Iteration 7/25 | Loss: 0.00123461
Iteration 8/25 | Loss: 0.00123290
Iteration 9/25 | Loss: 0.00123433
Iteration 10/25 | Loss: 0.00123418
Iteration 11/25 | Loss: 0.00123271
Iteration 12/25 | Loss: 0.00123321
Iteration 13/25 | Loss: 0.00122981
Iteration 14/25 | Loss: 0.00122887
Iteration 15/25 | Loss: 0.00122691
Iteration 16/25 | Loss: 0.00122893
Iteration 17/25 | Loss: 0.00123007
Iteration 18/25 | Loss: 0.00122855
Iteration 19/25 | Loss: 0.00122474
Iteration 20/25 | Loss: 0.00122406
Iteration 21/25 | Loss: 0.00122730
Iteration 22/25 | Loss: 0.00122774
Iteration 23/25 | Loss: 0.00122663
Iteration 24/25 | Loss: 0.00122311
Iteration 25/25 | Loss: 0.00122294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38642693
Iteration 2/25 | Loss: 0.00095037
Iteration 3/25 | Loss: 0.00081624
Iteration 4/25 | Loss: 0.00081624
Iteration 5/25 | Loss: 0.00081624
Iteration 6/25 | Loss: 0.00081624
Iteration 7/25 | Loss: 0.00081624
Iteration 8/25 | Loss: 0.00081624
Iteration 9/25 | Loss: 0.00081624
Iteration 10/25 | Loss: 0.00081624
Iteration 11/25 | Loss: 0.00081624
Iteration 12/25 | Loss: 0.00081624
Iteration 13/25 | Loss: 0.00081624
Iteration 14/25 | Loss: 0.00081624
Iteration 15/25 | Loss: 0.00081624
Iteration 16/25 | Loss: 0.00081624
Iteration 17/25 | Loss: 0.00081624
Iteration 18/25 | Loss: 0.00081624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000816238287370652, 0.000816238287370652, 0.000816238287370652, 0.000816238287370652, 0.000816238287370652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000816238287370652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081624
Iteration 2/1000 | Loss: 0.00058666
Iteration 3/1000 | Loss: 0.00014163
Iteration 4/1000 | Loss: 0.00039941
Iteration 5/1000 | Loss: 0.00038139
Iteration 6/1000 | Loss: 0.00039156
Iteration 7/1000 | Loss: 0.00048816
Iteration 8/1000 | Loss: 0.00017376
Iteration 9/1000 | Loss: 0.00011833
Iteration 10/1000 | Loss: 0.00014842
Iteration 11/1000 | Loss: 0.00020358
Iteration 12/1000 | Loss: 0.00015529
Iteration 13/1000 | Loss: 0.00010819
Iteration 14/1000 | Loss: 0.00007046
Iteration 15/1000 | Loss: 0.00008642
Iteration 16/1000 | Loss: 0.00006053
Iteration 17/1000 | Loss: 0.00017761
Iteration 18/1000 | Loss: 0.00071690
Iteration 19/1000 | Loss: 0.00028564
Iteration 20/1000 | Loss: 0.00030372
Iteration 21/1000 | Loss: 0.00016804
Iteration 22/1000 | Loss: 0.00009222
Iteration 23/1000 | Loss: 0.00071303
Iteration 24/1000 | Loss: 0.00034675
Iteration 25/1000 | Loss: 0.00058187
Iteration 26/1000 | Loss: 0.00043169
Iteration 27/1000 | Loss: 0.00049745
Iteration 28/1000 | Loss: 0.00019748
Iteration 29/1000 | Loss: 0.00007898
Iteration 30/1000 | Loss: 0.00010259
Iteration 31/1000 | Loss: 0.00022009
Iteration 32/1000 | Loss: 0.00075688
Iteration 33/1000 | Loss: 0.00031422
Iteration 34/1000 | Loss: 0.00029985
Iteration 35/1000 | Loss: 0.00077078
Iteration 36/1000 | Loss: 0.00017302
Iteration 37/1000 | Loss: 0.00006550
Iteration 38/1000 | Loss: 0.00005679
Iteration 39/1000 | Loss: 0.00011918
Iteration 40/1000 | Loss: 0.00010772
Iteration 41/1000 | Loss: 0.00013072
Iteration 42/1000 | Loss: 0.00005103
Iteration 43/1000 | Loss: 0.00004549
Iteration 44/1000 | Loss: 0.00004531
Iteration 45/1000 | Loss: 0.00003247
Iteration 46/1000 | Loss: 0.00006880
Iteration 47/1000 | Loss: 0.00007384
Iteration 48/1000 | Loss: 0.00003183
Iteration 49/1000 | Loss: 0.00015896
Iteration 50/1000 | Loss: 0.00004828
Iteration 51/1000 | Loss: 0.00010641
Iteration 52/1000 | Loss: 0.00010178
Iteration 53/1000 | Loss: 0.00009879
Iteration 54/1000 | Loss: 0.00013421
Iteration 55/1000 | Loss: 0.00007291
Iteration 56/1000 | Loss: 0.00010162
Iteration 57/1000 | Loss: 0.00010306
Iteration 58/1000 | Loss: 0.00010313
Iteration 59/1000 | Loss: 0.00012181
Iteration 60/1000 | Loss: 0.00007908
Iteration 61/1000 | Loss: 0.00013240
Iteration 62/1000 | Loss: 0.00012490
Iteration 63/1000 | Loss: 0.00014565
Iteration 64/1000 | Loss: 0.00010978
Iteration 65/1000 | Loss: 0.00025831
Iteration 66/1000 | Loss: 0.00007620
Iteration 67/1000 | Loss: 0.00023875
Iteration 68/1000 | Loss: 0.00015388
Iteration 69/1000 | Loss: 0.00013186
Iteration 70/1000 | Loss: 0.00013708
Iteration 71/1000 | Loss: 0.00013463
Iteration 72/1000 | Loss: 0.00017922
Iteration 73/1000 | Loss: 0.00012779
Iteration 74/1000 | Loss: 0.00010056
Iteration 75/1000 | Loss: 0.00013172
Iteration 76/1000 | Loss: 0.00009489
Iteration 77/1000 | Loss: 0.00011300
Iteration 78/1000 | Loss: 0.00013164
Iteration 79/1000 | Loss: 0.00007706
Iteration 80/1000 | Loss: 0.00004435
Iteration 81/1000 | Loss: 0.00004947
Iteration 82/1000 | Loss: 0.00004998
Iteration 83/1000 | Loss: 0.00002680
Iteration 84/1000 | Loss: 0.00004706
Iteration 85/1000 | Loss: 0.00004791
Iteration 86/1000 | Loss: 0.00004549
Iteration 87/1000 | Loss: 0.00004263
Iteration 88/1000 | Loss: 0.00004605
Iteration 89/1000 | Loss: 0.00014633
Iteration 90/1000 | Loss: 0.00010411
Iteration 91/1000 | Loss: 0.00008740
Iteration 92/1000 | Loss: 0.00011263
Iteration 93/1000 | Loss: 0.00010817
Iteration 94/1000 | Loss: 0.00015015
Iteration 95/1000 | Loss: 0.00012944
Iteration 96/1000 | Loss: 0.00010452
Iteration 97/1000 | Loss: 0.00012200
Iteration 98/1000 | Loss: 0.00010840
Iteration 99/1000 | Loss: 0.00009768
Iteration 100/1000 | Loss: 0.00008930
Iteration 101/1000 | Loss: 0.00009129
Iteration 102/1000 | Loss: 0.00009980
Iteration 103/1000 | Loss: 0.00007552
Iteration 104/1000 | Loss: 0.00008345
Iteration 105/1000 | Loss: 0.00003072
Iteration 106/1000 | Loss: 0.00016127
Iteration 107/1000 | Loss: 0.00006713
Iteration 108/1000 | Loss: 0.00011032
Iteration 109/1000 | Loss: 0.00012310
Iteration 110/1000 | Loss: 0.00011216
Iteration 111/1000 | Loss: 0.00009652
Iteration 112/1000 | Loss: 0.00014736
Iteration 113/1000 | Loss: 0.00002903
Iteration 114/1000 | Loss: 0.00002594
Iteration 115/1000 | Loss: 0.00002531
Iteration 116/1000 | Loss: 0.00002511
Iteration 117/1000 | Loss: 0.00002491
Iteration 118/1000 | Loss: 0.00003538
Iteration 119/1000 | Loss: 0.00002506
Iteration 120/1000 | Loss: 0.00002475
Iteration 121/1000 | Loss: 0.00002456
Iteration 122/1000 | Loss: 0.00002440
Iteration 123/1000 | Loss: 0.00002439
Iteration 124/1000 | Loss: 0.00002438
Iteration 125/1000 | Loss: 0.00002436
Iteration 126/1000 | Loss: 0.00002436
Iteration 127/1000 | Loss: 0.00002435
Iteration 128/1000 | Loss: 0.00002435
Iteration 129/1000 | Loss: 0.00002435
Iteration 130/1000 | Loss: 0.00002434
Iteration 131/1000 | Loss: 0.00002433
Iteration 132/1000 | Loss: 0.00002433
Iteration 133/1000 | Loss: 0.00002433
Iteration 134/1000 | Loss: 0.00002432
Iteration 135/1000 | Loss: 0.00002432
Iteration 136/1000 | Loss: 0.00002432
Iteration 137/1000 | Loss: 0.00002432
Iteration 138/1000 | Loss: 0.00002432
Iteration 139/1000 | Loss: 0.00002432
Iteration 140/1000 | Loss: 0.00002432
Iteration 141/1000 | Loss: 0.00002432
Iteration 142/1000 | Loss: 0.00002432
Iteration 143/1000 | Loss: 0.00002432
Iteration 144/1000 | Loss: 0.00002432
Iteration 145/1000 | Loss: 0.00002432
Iteration 146/1000 | Loss: 0.00002431
Iteration 147/1000 | Loss: 0.00002431
Iteration 148/1000 | Loss: 0.00002431
Iteration 149/1000 | Loss: 0.00002430
Iteration 150/1000 | Loss: 0.00002429
Iteration 151/1000 | Loss: 0.00002429
Iteration 152/1000 | Loss: 0.00002428
Iteration 153/1000 | Loss: 0.00002428
Iteration 154/1000 | Loss: 0.00002427
Iteration 155/1000 | Loss: 0.00002426
Iteration 156/1000 | Loss: 0.00002426
Iteration 157/1000 | Loss: 0.00002426
Iteration 158/1000 | Loss: 0.00002426
Iteration 159/1000 | Loss: 0.00002426
Iteration 160/1000 | Loss: 0.00002425
Iteration 161/1000 | Loss: 0.00002424
Iteration 162/1000 | Loss: 0.00002423
Iteration 163/1000 | Loss: 0.00002423
Iteration 164/1000 | Loss: 0.00002420
Iteration 165/1000 | Loss: 0.00002419
Iteration 166/1000 | Loss: 0.00002419
Iteration 167/1000 | Loss: 0.00002419
Iteration 168/1000 | Loss: 0.00002419
Iteration 169/1000 | Loss: 0.00002419
Iteration 170/1000 | Loss: 0.00002419
Iteration 171/1000 | Loss: 0.00002419
Iteration 172/1000 | Loss: 0.00002418
Iteration 173/1000 | Loss: 0.00002418
Iteration 174/1000 | Loss: 0.00002418
Iteration 175/1000 | Loss: 0.00002418
Iteration 176/1000 | Loss: 0.00002418
Iteration 177/1000 | Loss: 0.00002418
Iteration 178/1000 | Loss: 0.00002418
Iteration 179/1000 | Loss: 0.00002417
Iteration 180/1000 | Loss: 0.00002417
Iteration 181/1000 | Loss: 0.00002414
Iteration 182/1000 | Loss: 0.00002414
Iteration 183/1000 | Loss: 0.00002414
Iteration 184/1000 | Loss: 0.00002414
Iteration 185/1000 | Loss: 0.00002414
Iteration 186/1000 | Loss: 0.00002414
Iteration 187/1000 | Loss: 0.00002414
Iteration 188/1000 | Loss: 0.00002414
Iteration 189/1000 | Loss: 0.00002413
Iteration 190/1000 | Loss: 0.00002413
Iteration 191/1000 | Loss: 0.00002412
Iteration 192/1000 | Loss: 0.00002410
Iteration 193/1000 | Loss: 0.00002409
Iteration 194/1000 | Loss: 0.00002408
Iteration 195/1000 | Loss: 0.00002408
Iteration 196/1000 | Loss: 0.00002408
Iteration 197/1000 | Loss: 0.00002408
Iteration 198/1000 | Loss: 0.00002408
Iteration 199/1000 | Loss: 0.00002407
Iteration 200/1000 | Loss: 0.00002407
Iteration 201/1000 | Loss: 0.00002407
Iteration 202/1000 | Loss: 0.00002406
Iteration 203/1000 | Loss: 0.00002406
Iteration 204/1000 | Loss: 0.00002406
Iteration 205/1000 | Loss: 0.00002406
Iteration 206/1000 | Loss: 0.00002406
Iteration 207/1000 | Loss: 0.00002406
Iteration 208/1000 | Loss: 0.00002406
Iteration 209/1000 | Loss: 0.00002406
Iteration 210/1000 | Loss: 0.00002406
Iteration 211/1000 | Loss: 0.00002405
Iteration 212/1000 | Loss: 0.00002405
Iteration 213/1000 | Loss: 0.00002405
Iteration 214/1000 | Loss: 0.00002404
Iteration 215/1000 | Loss: 0.00002404
Iteration 216/1000 | Loss: 0.00002404
Iteration 217/1000 | Loss: 0.00002404
Iteration 218/1000 | Loss: 0.00002404
Iteration 219/1000 | Loss: 0.00002404
Iteration 220/1000 | Loss: 0.00002403
Iteration 221/1000 | Loss: 0.00002403
Iteration 222/1000 | Loss: 0.00002403
Iteration 223/1000 | Loss: 0.00002403
Iteration 224/1000 | Loss: 0.00002403
Iteration 225/1000 | Loss: 0.00002403
Iteration 226/1000 | Loss: 0.00002403
Iteration 227/1000 | Loss: 0.00002403
Iteration 228/1000 | Loss: 0.00002403
Iteration 229/1000 | Loss: 0.00002403
Iteration 230/1000 | Loss: 0.00002403
Iteration 231/1000 | Loss: 0.00002403
Iteration 232/1000 | Loss: 0.00002403
Iteration 233/1000 | Loss: 0.00002403
Iteration 234/1000 | Loss: 0.00002403
Iteration 235/1000 | Loss: 0.00002403
Iteration 236/1000 | Loss: 0.00002403
Iteration 237/1000 | Loss: 0.00002403
Iteration 238/1000 | Loss: 0.00002403
Iteration 239/1000 | Loss: 0.00002403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.402868449280504e-05, 2.402868449280504e-05, 2.402868449280504e-05, 2.402868449280504e-05, 2.402868449280504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.402868449280504e-05

Optimization complete. Final v2v error: 4.027207851409912 mm

Highest mean error: 8.755518913269043 mm for frame 184

Lowest mean error: 3.6145987510681152 mm for frame 17

Saving results

Total time: 259.9509024620056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ashley_posed_002/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ashley_posed_002/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454898
Iteration 2/25 | Loss: 0.00146258
Iteration 3/25 | Loss: 0.00128248
Iteration 4/25 | Loss: 0.00125386
Iteration 5/25 | Loss: 0.00124629
Iteration 6/25 | Loss: 0.00124474
Iteration 7/25 | Loss: 0.00124409
Iteration 8/25 | Loss: 0.00124409
Iteration 9/25 | Loss: 0.00124409
Iteration 10/25 | Loss: 0.00124409
Iteration 11/25 | Loss: 0.00124409
Iteration 12/25 | Loss: 0.00124409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012440918944776058, 0.0012440918944776058, 0.0012440918944776058, 0.0012440918944776058, 0.0012440918944776058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012440918944776058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24665904
Iteration 2/25 | Loss: 0.00074124
Iteration 3/25 | Loss: 0.00074121
Iteration 4/25 | Loss: 0.00074121
Iteration 5/25 | Loss: 0.00074121
Iteration 6/25 | Loss: 0.00074121
Iteration 7/25 | Loss: 0.00074121
Iteration 8/25 | Loss: 0.00074121
Iteration 9/25 | Loss: 0.00074121
Iteration 10/25 | Loss: 0.00074121
Iteration 11/25 | Loss: 0.00074121
Iteration 12/25 | Loss: 0.00074121
Iteration 13/25 | Loss: 0.00074121
Iteration 14/25 | Loss: 0.00074121
Iteration 15/25 | Loss: 0.00074121
Iteration 16/25 | Loss: 0.00074121
Iteration 17/25 | Loss: 0.00074121
Iteration 18/25 | Loss: 0.00074121
Iteration 19/25 | Loss: 0.00074121
Iteration 20/25 | Loss: 0.00074121
Iteration 21/25 | Loss: 0.00074121
Iteration 22/25 | Loss: 0.00074121
Iteration 23/25 | Loss: 0.00074121
Iteration 24/25 | Loss: 0.00074121
Iteration 25/25 | Loss: 0.00074121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074121
Iteration 2/1000 | Loss: 0.00008084
Iteration 3/1000 | Loss: 0.00004724
Iteration 4/1000 | Loss: 0.00003683
Iteration 5/1000 | Loss: 0.00003326
Iteration 6/1000 | Loss: 0.00003121
Iteration 7/1000 | Loss: 0.00002992
Iteration 8/1000 | Loss: 0.00002924
Iteration 9/1000 | Loss: 0.00002863
Iteration 10/1000 | Loss: 0.00002803
Iteration 11/1000 | Loss: 0.00002772
Iteration 12/1000 | Loss: 0.00002752
Iteration 13/1000 | Loss: 0.00002727
Iteration 14/1000 | Loss: 0.00002716
Iteration 15/1000 | Loss: 0.00002714
Iteration 16/1000 | Loss: 0.00002699
Iteration 17/1000 | Loss: 0.00002695
Iteration 18/1000 | Loss: 0.00002685
Iteration 19/1000 | Loss: 0.00002671
Iteration 20/1000 | Loss: 0.00002660
Iteration 21/1000 | Loss: 0.00002657
Iteration 22/1000 | Loss: 0.00002657
Iteration 23/1000 | Loss: 0.00002656
Iteration 24/1000 | Loss: 0.00002656
Iteration 25/1000 | Loss: 0.00002655
Iteration 26/1000 | Loss: 0.00002654
Iteration 27/1000 | Loss: 0.00002653
Iteration 28/1000 | Loss: 0.00002653
Iteration 29/1000 | Loss: 0.00002653
Iteration 30/1000 | Loss: 0.00002653
Iteration 31/1000 | Loss: 0.00002653
Iteration 32/1000 | Loss: 0.00002652
Iteration 33/1000 | Loss: 0.00002649
Iteration 34/1000 | Loss: 0.00002648
Iteration 35/1000 | Loss: 0.00002648
Iteration 36/1000 | Loss: 0.00002647
Iteration 37/1000 | Loss: 0.00002647
Iteration 38/1000 | Loss: 0.00002646
Iteration 39/1000 | Loss: 0.00002645
Iteration 40/1000 | Loss: 0.00002642
Iteration 41/1000 | Loss: 0.00002641
Iteration 42/1000 | Loss: 0.00002641
Iteration 43/1000 | Loss: 0.00002640
Iteration 44/1000 | Loss: 0.00002639
Iteration 45/1000 | Loss: 0.00002635
Iteration 46/1000 | Loss: 0.00002635
Iteration 47/1000 | Loss: 0.00002635
Iteration 48/1000 | Loss: 0.00002634
Iteration 49/1000 | Loss: 0.00002632
Iteration 50/1000 | Loss: 0.00002631
Iteration 51/1000 | Loss: 0.00002631
Iteration 52/1000 | Loss: 0.00002631
Iteration 53/1000 | Loss: 0.00002631
Iteration 54/1000 | Loss: 0.00002630
Iteration 55/1000 | Loss: 0.00002630
Iteration 56/1000 | Loss: 0.00002630
Iteration 57/1000 | Loss: 0.00002630
Iteration 58/1000 | Loss: 0.00002629
Iteration 59/1000 | Loss: 0.00002629
Iteration 60/1000 | Loss: 0.00002628
Iteration 61/1000 | Loss: 0.00002627
Iteration 62/1000 | Loss: 0.00002626
Iteration 63/1000 | Loss: 0.00002626
Iteration 64/1000 | Loss: 0.00002625
Iteration 65/1000 | Loss: 0.00002625
Iteration 66/1000 | Loss: 0.00002625
Iteration 67/1000 | Loss: 0.00002625
Iteration 68/1000 | Loss: 0.00002625
Iteration 69/1000 | Loss: 0.00002625
Iteration 70/1000 | Loss: 0.00002624
Iteration 71/1000 | Loss: 0.00002624
Iteration 72/1000 | Loss: 0.00002624
Iteration 73/1000 | Loss: 0.00002624
Iteration 74/1000 | Loss: 0.00002623
Iteration 75/1000 | Loss: 0.00002623
Iteration 76/1000 | Loss: 0.00002623
Iteration 77/1000 | Loss: 0.00002621
Iteration 78/1000 | Loss: 0.00002621
Iteration 79/1000 | Loss: 0.00002621
Iteration 80/1000 | Loss: 0.00002621
Iteration 81/1000 | Loss: 0.00002621
Iteration 82/1000 | Loss: 0.00002621
Iteration 83/1000 | Loss: 0.00002621
Iteration 84/1000 | Loss: 0.00002621
Iteration 85/1000 | Loss: 0.00002620
Iteration 86/1000 | Loss: 0.00002619
Iteration 87/1000 | Loss: 0.00002619
Iteration 88/1000 | Loss: 0.00002618
Iteration 89/1000 | Loss: 0.00002618
Iteration 90/1000 | Loss: 0.00002618
Iteration 91/1000 | Loss: 0.00002617
Iteration 92/1000 | Loss: 0.00002617
Iteration 93/1000 | Loss: 0.00002617
Iteration 94/1000 | Loss: 0.00002616
Iteration 95/1000 | Loss: 0.00002616
Iteration 96/1000 | Loss: 0.00002616
Iteration 97/1000 | Loss: 0.00002616
Iteration 98/1000 | Loss: 0.00002616
Iteration 99/1000 | Loss: 0.00002616
Iteration 100/1000 | Loss: 0.00002616
Iteration 101/1000 | Loss: 0.00002616
Iteration 102/1000 | Loss: 0.00002616
Iteration 103/1000 | Loss: 0.00002615
Iteration 104/1000 | Loss: 0.00002615
Iteration 105/1000 | Loss: 0.00002615
Iteration 106/1000 | Loss: 0.00002615
Iteration 107/1000 | Loss: 0.00002615
Iteration 108/1000 | Loss: 0.00002615
Iteration 109/1000 | Loss: 0.00002615
Iteration 110/1000 | Loss: 0.00002615
Iteration 111/1000 | Loss: 0.00002615
Iteration 112/1000 | Loss: 0.00002615
Iteration 113/1000 | Loss: 0.00002615
Iteration 114/1000 | Loss: 0.00002615
Iteration 115/1000 | Loss: 0.00002614
Iteration 116/1000 | Loss: 0.00002614
Iteration 117/1000 | Loss: 0.00002614
Iteration 118/1000 | Loss: 0.00002614
Iteration 119/1000 | Loss: 0.00002614
Iteration 120/1000 | Loss: 0.00002614
Iteration 121/1000 | Loss: 0.00002614
Iteration 122/1000 | Loss: 0.00002614
Iteration 123/1000 | Loss: 0.00002614
Iteration 124/1000 | Loss: 0.00002614
Iteration 125/1000 | Loss: 0.00002614
Iteration 126/1000 | Loss: 0.00002613
Iteration 127/1000 | Loss: 0.00002613
Iteration 128/1000 | Loss: 0.00002613
Iteration 129/1000 | Loss: 0.00002613
Iteration 130/1000 | Loss: 0.00002613
Iteration 131/1000 | Loss: 0.00002613
Iteration 132/1000 | Loss: 0.00002612
Iteration 133/1000 | Loss: 0.00002612
Iteration 134/1000 | Loss: 0.00002612
Iteration 135/1000 | Loss: 0.00002612
Iteration 136/1000 | Loss: 0.00002612
Iteration 137/1000 | Loss: 0.00002612
Iteration 138/1000 | Loss: 0.00002612
Iteration 139/1000 | Loss: 0.00002612
Iteration 140/1000 | Loss: 0.00002612
Iteration 141/1000 | Loss: 0.00002612
Iteration 142/1000 | Loss: 0.00002612
Iteration 143/1000 | Loss: 0.00002612
Iteration 144/1000 | Loss: 0.00002611
Iteration 145/1000 | Loss: 0.00002611
Iteration 146/1000 | Loss: 0.00002611
Iteration 147/1000 | Loss: 0.00002611
Iteration 148/1000 | Loss: 0.00002611
Iteration 149/1000 | Loss: 0.00002611
Iteration 150/1000 | Loss: 0.00002611
Iteration 151/1000 | Loss: 0.00002611
Iteration 152/1000 | Loss: 0.00002611
Iteration 153/1000 | Loss: 0.00002611
Iteration 154/1000 | Loss: 0.00002611
Iteration 155/1000 | Loss: 0.00002611
Iteration 156/1000 | Loss: 0.00002611
Iteration 157/1000 | Loss: 0.00002611
Iteration 158/1000 | Loss: 0.00002611
Iteration 159/1000 | Loss: 0.00002611
Iteration 160/1000 | Loss: 0.00002610
Iteration 161/1000 | Loss: 0.00002610
Iteration 162/1000 | Loss: 0.00002610
Iteration 163/1000 | Loss: 0.00002610
Iteration 164/1000 | Loss: 0.00002610
Iteration 165/1000 | Loss: 0.00002610
Iteration 166/1000 | Loss: 0.00002610
Iteration 167/1000 | Loss: 0.00002610
Iteration 168/1000 | Loss: 0.00002610
Iteration 169/1000 | Loss: 0.00002610
Iteration 170/1000 | Loss: 0.00002610
Iteration 171/1000 | Loss: 0.00002610
Iteration 172/1000 | Loss: 0.00002610
Iteration 173/1000 | Loss: 0.00002610
Iteration 174/1000 | Loss: 0.00002610
Iteration 175/1000 | Loss: 0.00002610
Iteration 176/1000 | Loss: 0.00002610
Iteration 177/1000 | Loss: 0.00002610
Iteration 178/1000 | Loss: 0.00002610
Iteration 179/1000 | Loss: 0.00002610
Iteration 180/1000 | Loss: 0.00002609
Iteration 181/1000 | Loss: 0.00002609
Iteration 182/1000 | Loss: 0.00002609
Iteration 183/1000 | Loss: 0.00002609
Iteration 184/1000 | Loss: 0.00002609
Iteration 185/1000 | Loss: 0.00002609
Iteration 186/1000 | Loss: 0.00002609
Iteration 187/1000 | Loss: 0.00002609
Iteration 188/1000 | Loss: 0.00002609
Iteration 189/1000 | Loss: 0.00002609
Iteration 190/1000 | Loss: 0.00002609
Iteration 191/1000 | Loss: 0.00002609
Iteration 192/1000 | Loss: 0.00002609
Iteration 193/1000 | Loss: 0.00002609
Iteration 194/1000 | Loss: 0.00002609
Iteration 195/1000 | Loss: 0.00002609
Iteration 196/1000 | Loss: 0.00002609
Iteration 197/1000 | Loss: 0.00002609
Iteration 198/1000 | Loss: 0.00002608
Iteration 199/1000 | Loss: 0.00002608
Iteration 200/1000 | Loss: 0.00002608
Iteration 201/1000 | Loss: 0.00002608
Iteration 202/1000 | Loss: 0.00002608
Iteration 203/1000 | Loss: 0.00002608
Iteration 204/1000 | Loss: 0.00002608
Iteration 205/1000 | Loss: 0.00002608
Iteration 206/1000 | Loss: 0.00002608
Iteration 207/1000 | Loss: 0.00002608
Iteration 208/1000 | Loss: 0.00002608
Iteration 209/1000 | Loss: 0.00002608
Iteration 210/1000 | Loss: 0.00002608
Iteration 211/1000 | Loss: 0.00002607
Iteration 212/1000 | Loss: 0.00002607
Iteration 213/1000 | Loss: 0.00002607
Iteration 214/1000 | Loss: 0.00002607
Iteration 215/1000 | Loss: 0.00002607
Iteration 216/1000 | Loss: 0.00002607
Iteration 217/1000 | Loss: 0.00002607
Iteration 218/1000 | Loss: 0.00002607
Iteration 219/1000 | Loss: 0.00002607
Iteration 220/1000 | Loss: 0.00002607
Iteration 221/1000 | Loss: 0.00002607
Iteration 222/1000 | Loss: 0.00002607
Iteration 223/1000 | Loss: 0.00002607
Iteration 224/1000 | Loss: 0.00002607
Iteration 225/1000 | Loss: 0.00002607
Iteration 226/1000 | Loss: 0.00002607
Iteration 227/1000 | Loss: 0.00002607
Iteration 228/1000 | Loss: 0.00002607
Iteration 229/1000 | Loss: 0.00002607
Iteration 230/1000 | Loss: 0.00002607
Iteration 231/1000 | Loss: 0.00002607
Iteration 232/1000 | Loss: 0.00002607
Iteration 233/1000 | Loss: 0.00002607
Iteration 234/1000 | Loss: 0.00002607
Iteration 235/1000 | Loss: 0.00002607
Iteration 236/1000 | Loss: 0.00002607
Iteration 237/1000 | Loss: 0.00002607
Iteration 238/1000 | Loss: 0.00002607
Iteration 239/1000 | Loss: 0.00002607
Iteration 240/1000 | Loss: 0.00002607
Iteration 241/1000 | Loss: 0.00002607
Iteration 242/1000 | Loss: 0.00002607
Iteration 243/1000 | Loss: 0.00002607
Iteration 244/1000 | Loss: 0.00002607
Iteration 245/1000 | Loss: 0.00002607
Iteration 246/1000 | Loss: 0.00002607
Iteration 247/1000 | Loss: 0.00002607
Iteration 248/1000 | Loss: 0.00002607
Iteration 249/1000 | Loss: 0.00002607
Iteration 250/1000 | Loss: 0.00002607
Iteration 251/1000 | Loss: 0.00002607
Iteration 252/1000 | Loss: 0.00002607
Iteration 253/1000 | Loss: 0.00002607
Iteration 254/1000 | Loss: 0.00002607
Iteration 255/1000 | Loss: 0.00002607
Iteration 256/1000 | Loss: 0.00002607
Iteration 257/1000 | Loss: 0.00002607
Iteration 258/1000 | Loss: 0.00002607
Iteration 259/1000 | Loss: 0.00002607
Iteration 260/1000 | Loss: 0.00002607
Iteration 261/1000 | Loss: 0.00002607
Iteration 262/1000 | Loss: 0.00002607
Iteration 263/1000 | Loss: 0.00002607
Iteration 264/1000 | Loss: 0.00002607
Iteration 265/1000 | Loss: 0.00002607
Iteration 266/1000 | Loss: 0.00002607
Iteration 267/1000 | Loss: 0.00002607
Iteration 268/1000 | Loss: 0.00002607
Iteration 269/1000 | Loss: 0.00002607
Iteration 270/1000 | Loss: 0.00002607
Iteration 271/1000 | Loss: 0.00002607
Iteration 272/1000 | Loss: 0.00002607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [2.607224632811267e-05, 2.607224632811267e-05, 2.607224632811267e-05, 2.607224632811267e-05, 2.607224632811267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.607224632811267e-05

Optimization complete. Final v2v error: 4.091665744781494 mm

Highest mean error: 5.961018085479736 mm for frame 80

Lowest mean error: 3.2519025802612305 mm for frame 45

Saving results

Total time: 51.362029790878296
