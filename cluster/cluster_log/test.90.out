Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=90, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5040-5095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067441
Iteration 2/25 | Loss: 0.00185714
Iteration 3/25 | Loss: 0.00153658
Iteration 4/25 | Loss: 0.00152063
Iteration 5/25 | Loss: 0.00149877
Iteration 6/25 | Loss: 0.00139161
Iteration 7/25 | Loss: 0.00131294
Iteration 8/25 | Loss: 0.00127792
Iteration 9/25 | Loss: 0.00127144
Iteration 10/25 | Loss: 0.00126449
Iteration 11/25 | Loss: 0.00126619
Iteration 12/25 | Loss: 0.00126504
Iteration 13/25 | Loss: 0.00126397
Iteration 14/25 | Loss: 0.00126397
Iteration 15/25 | Loss: 0.00126397
Iteration 16/25 | Loss: 0.00126397
Iteration 17/25 | Loss: 0.00126397
Iteration 18/25 | Loss: 0.00126397
Iteration 19/25 | Loss: 0.00126397
Iteration 20/25 | Loss: 0.00126397
Iteration 21/25 | Loss: 0.00126396
Iteration 22/25 | Loss: 0.00126396
Iteration 23/25 | Loss: 0.00126396
Iteration 24/25 | Loss: 0.00126396
Iteration 25/25 | Loss: 0.00126396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61100161
Iteration 2/25 | Loss: 0.00079715
Iteration 3/25 | Loss: 0.00079714
Iteration 4/25 | Loss: 0.00079714
Iteration 5/25 | Loss: 0.00079714
Iteration 6/25 | Loss: 0.00079714
Iteration 7/25 | Loss: 0.00079714
Iteration 8/25 | Loss: 0.00079714
Iteration 9/25 | Loss: 0.00079714
Iteration 10/25 | Loss: 0.00079714
Iteration 11/25 | Loss: 0.00079714
Iteration 12/25 | Loss: 0.00079714
Iteration 13/25 | Loss: 0.00079714
Iteration 14/25 | Loss: 0.00079714
Iteration 15/25 | Loss: 0.00079714
Iteration 16/25 | Loss: 0.00079714
Iteration 17/25 | Loss: 0.00079714
Iteration 18/25 | Loss: 0.00079714
Iteration 19/25 | Loss: 0.00079714
Iteration 20/25 | Loss: 0.00079714
Iteration 21/25 | Loss: 0.00079714
Iteration 22/25 | Loss: 0.00079714
Iteration 23/25 | Loss: 0.00079714
Iteration 24/25 | Loss: 0.00079714
Iteration 25/25 | Loss: 0.00079714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007971369777806103, 0.0007971369777806103, 0.0007971369777806103, 0.0007971369777806103, 0.0007971369777806103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007971369777806103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079714
Iteration 2/1000 | Loss: 0.00003934
Iteration 3/1000 | Loss: 0.00002761
Iteration 4/1000 | Loss: 0.00002460
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00004811
Iteration 8/1000 | Loss: 0.00002154
Iteration 9/1000 | Loss: 0.00003596
Iteration 10/1000 | Loss: 0.00002133
Iteration 11/1000 | Loss: 0.00002115
Iteration 12/1000 | Loss: 0.00002113
Iteration 13/1000 | Loss: 0.00003870
Iteration 14/1000 | Loss: 0.00002084
Iteration 15/1000 | Loss: 0.00002080
Iteration 16/1000 | Loss: 0.00002080
Iteration 17/1000 | Loss: 0.00005661
Iteration 18/1000 | Loss: 0.00002064
Iteration 19/1000 | Loss: 0.00002061
Iteration 20/1000 | Loss: 0.00002059
Iteration 21/1000 | Loss: 0.00002059
Iteration 22/1000 | Loss: 0.00002056
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002052
Iteration 25/1000 | Loss: 0.00002044
Iteration 26/1000 | Loss: 0.00002043
Iteration 27/1000 | Loss: 0.00002043
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002041
Iteration 30/1000 | Loss: 0.00002037
Iteration 31/1000 | Loss: 0.00002033
Iteration 32/1000 | Loss: 0.00002032
Iteration 33/1000 | Loss: 0.00002029
Iteration 34/1000 | Loss: 0.00002028
Iteration 35/1000 | Loss: 0.00002028
Iteration 36/1000 | Loss: 0.00002023
Iteration 37/1000 | Loss: 0.00002017
Iteration 38/1000 | Loss: 0.00002016
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002012
Iteration 41/1000 | Loss: 0.00002011
Iteration 42/1000 | Loss: 0.00002011
Iteration 43/1000 | Loss: 0.00002010
Iteration 44/1000 | Loss: 0.00002010
Iteration 45/1000 | Loss: 0.00002009
Iteration 46/1000 | Loss: 0.00002009
Iteration 47/1000 | Loss: 0.00002009
Iteration 48/1000 | Loss: 0.00002007
Iteration 49/1000 | Loss: 0.00002007
Iteration 50/1000 | Loss: 0.00005473
Iteration 51/1000 | Loss: 0.00002005
Iteration 52/1000 | Loss: 0.00002688
Iteration 53/1000 | Loss: 0.00002004
Iteration 54/1000 | Loss: 0.00002004
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002004
Iteration 57/1000 | Loss: 0.00002003
Iteration 58/1000 | Loss: 0.00002003
Iteration 59/1000 | Loss: 0.00002003
Iteration 60/1000 | Loss: 0.00002002
Iteration 61/1000 | Loss: 0.00002002
Iteration 62/1000 | Loss: 0.00002002
Iteration 63/1000 | Loss: 0.00002002
Iteration 64/1000 | Loss: 0.00002002
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002001
Iteration 69/1000 | Loss: 0.00002001
Iteration 70/1000 | Loss: 0.00002001
Iteration 71/1000 | Loss: 0.00002000
Iteration 72/1000 | Loss: 0.00002000
Iteration 73/1000 | Loss: 0.00002000
Iteration 74/1000 | Loss: 0.00002000
Iteration 75/1000 | Loss: 0.00002000
Iteration 76/1000 | Loss: 0.00002000
Iteration 77/1000 | Loss: 0.00002000
Iteration 78/1000 | Loss: 0.00002000
Iteration 79/1000 | Loss: 0.00002000
Iteration 80/1000 | Loss: 0.00001999
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001999
Iteration 84/1000 | Loss: 0.00001999
Iteration 85/1000 | Loss: 0.00001999
Iteration 86/1000 | Loss: 0.00001999
Iteration 87/1000 | Loss: 0.00001998
Iteration 88/1000 | Loss: 0.00001998
Iteration 89/1000 | Loss: 0.00001998
Iteration 90/1000 | Loss: 0.00001998
Iteration 91/1000 | Loss: 0.00001998
Iteration 92/1000 | Loss: 0.00001998
Iteration 93/1000 | Loss: 0.00001997
Iteration 94/1000 | Loss: 0.00001997
Iteration 95/1000 | Loss: 0.00001997
Iteration 96/1000 | Loss: 0.00001997
Iteration 97/1000 | Loss: 0.00001997
Iteration 98/1000 | Loss: 0.00001997
Iteration 99/1000 | Loss: 0.00001996
Iteration 100/1000 | Loss: 0.00001996
Iteration 101/1000 | Loss: 0.00001996
Iteration 102/1000 | Loss: 0.00001996
Iteration 103/1000 | Loss: 0.00001996
Iteration 104/1000 | Loss: 0.00001996
Iteration 105/1000 | Loss: 0.00001995
Iteration 106/1000 | Loss: 0.00001995
Iteration 107/1000 | Loss: 0.00001995
Iteration 108/1000 | Loss: 0.00001995
Iteration 109/1000 | Loss: 0.00001995
Iteration 110/1000 | Loss: 0.00001994
Iteration 111/1000 | Loss: 0.00001994
Iteration 112/1000 | Loss: 0.00001994
Iteration 113/1000 | Loss: 0.00001994
Iteration 114/1000 | Loss: 0.00004235
Iteration 115/1000 | Loss: 0.00004235
Iteration 116/1000 | Loss: 0.00002029
Iteration 117/1000 | Loss: 0.00001995
Iteration 118/1000 | Loss: 0.00001995
Iteration 119/1000 | Loss: 0.00001994
Iteration 120/1000 | Loss: 0.00001994
Iteration 121/1000 | Loss: 0.00001994
Iteration 122/1000 | Loss: 0.00001993
Iteration 123/1000 | Loss: 0.00001993
Iteration 124/1000 | Loss: 0.00001993
Iteration 125/1000 | Loss: 0.00001993
Iteration 126/1000 | Loss: 0.00001993
Iteration 127/1000 | Loss: 0.00001993
Iteration 128/1000 | Loss: 0.00001993
Iteration 129/1000 | Loss: 0.00001993
Iteration 130/1000 | Loss: 0.00001993
Iteration 131/1000 | Loss: 0.00001993
Iteration 132/1000 | Loss: 0.00001993
Iteration 133/1000 | Loss: 0.00001993
Iteration 134/1000 | Loss: 0.00001993
Iteration 135/1000 | Loss: 0.00001993
Iteration 136/1000 | Loss: 0.00001993
Iteration 137/1000 | Loss: 0.00001993
Iteration 138/1000 | Loss: 0.00001993
Iteration 139/1000 | Loss: 0.00001993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.9928274923586287e-05, 1.9928274923586287e-05, 1.9928274923586287e-05, 1.9928274923586287e-05, 1.9928274923586287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9928274923586287e-05

Optimization complete. Final v2v error: 3.6929526329040527 mm

Highest mean error: 5.066800594329834 mm for frame 80

Lowest mean error: 3.162564516067505 mm for frame 18

Saving results

Total time: 64.65910792350769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755988
Iteration 2/25 | Loss: 0.00149179
Iteration 3/25 | Loss: 0.00133698
Iteration 4/25 | Loss: 0.00130686
Iteration 5/25 | Loss: 0.00130380
Iteration 6/25 | Loss: 0.00131374
Iteration 7/25 | Loss: 0.00129432
Iteration 8/25 | Loss: 0.00128785
Iteration 9/25 | Loss: 0.00128689
Iteration 10/25 | Loss: 0.00128661
Iteration 11/25 | Loss: 0.00128643
Iteration 12/25 | Loss: 0.00128636
Iteration 13/25 | Loss: 0.00128635
Iteration 14/25 | Loss: 0.00128635
Iteration 15/25 | Loss: 0.00128635
Iteration 16/25 | Loss: 0.00128635
Iteration 17/25 | Loss: 0.00128634
Iteration 18/25 | Loss: 0.00128634
Iteration 19/25 | Loss: 0.00128634
Iteration 20/25 | Loss: 0.00128634
Iteration 21/25 | Loss: 0.00128634
Iteration 22/25 | Loss: 0.00128633
Iteration 23/25 | Loss: 0.00128633
Iteration 24/25 | Loss: 0.00128633
Iteration 25/25 | Loss: 0.00128633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.07692909
Iteration 2/25 | Loss: 0.00084473
Iteration 3/25 | Loss: 0.00084466
Iteration 4/25 | Loss: 0.00084466
Iteration 5/25 | Loss: 0.00084466
Iteration 6/25 | Loss: 0.00084466
Iteration 7/25 | Loss: 0.00084465
Iteration 8/25 | Loss: 0.00084465
Iteration 9/25 | Loss: 0.00084465
Iteration 10/25 | Loss: 0.00084465
Iteration 11/25 | Loss: 0.00084465
Iteration 12/25 | Loss: 0.00084465
Iteration 13/25 | Loss: 0.00084465
Iteration 14/25 | Loss: 0.00084465
Iteration 15/25 | Loss: 0.00084465
Iteration 16/25 | Loss: 0.00084465
Iteration 17/25 | Loss: 0.00084465
Iteration 18/25 | Loss: 0.00084465
Iteration 19/25 | Loss: 0.00084465
Iteration 20/25 | Loss: 0.00084465
Iteration 21/25 | Loss: 0.00084465
Iteration 22/25 | Loss: 0.00084465
Iteration 23/25 | Loss: 0.00084465
Iteration 24/25 | Loss: 0.00084465
Iteration 25/25 | Loss: 0.00084465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084465
Iteration 2/1000 | Loss: 0.00003861
Iteration 3/1000 | Loss: 0.00012959
Iteration 4/1000 | Loss: 0.00002539
Iteration 5/1000 | Loss: 0.00005527
Iteration 6/1000 | Loss: 0.00003800
Iteration 7/1000 | Loss: 0.00002345
Iteration 8/1000 | Loss: 0.00002191
Iteration 9/1000 | Loss: 0.00002136
Iteration 10/1000 | Loss: 0.00002089
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002024
Iteration 13/1000 | Loss: 0.00002018
Iteration 14/1000 | Loss: 0.00001997
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001980
Iteration 17/1000 | Loss: 0.00001978
Iteration 18/1000 | Loss: 0.00001974
Iteration 19/1000 | Loss: 0.00001973
Iteration 20/1000 | Loss: 0.00001972
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001970
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001966
Iteration 26/1000 | Loss: 0.00001965
Iteration 27/1000 | Loss: 0.00001963
Iteration 28/1000 | Loss: 0.00001963
Iteration 29/1000 | Loss: 0.00001962
Iteration 30/1000 | Loss: 0.00001962
Iteration 31/1000 | Loss: 0.00001961
Iteration 32/1000 | Loss: 0.00001958
Iteration 33/1000 | Loss: 0.00001957
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001955
Iteration 36/1000 | Loss: 0.00001955
Iteration 37/1000 | Loss: 0.00001954
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001954
Iteration 40/1000 | Loss: 0.00001954
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001954
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001952
Iteration 47/1000 | Loss: 0.00001951
Iteration 48/1000 | Loss: 0.00001951
Iteration 49/1000 | Loss: 0.00001950
Iteration 50/1000 | Loss: 0.00001950
Iteration 51/1000 | Loss: 0.00001950
Iteration 52/1000 | Loss: 0.00001949
Iteration 53/1000 | Loss: 0.00001949
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001947
Iteration 56/1000 | Loss: 0.00001947
Iteration 57/1000 | Loss: 0.00001947
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001944
Iteration 60/1000 | Loss: 0.00001943
Iteration 61/1000 | Loss: 0.00001943
Iteration 62/1000 | Loss: 0.00001942
Iteration 63/1000 | Loss: 0.00001942
Iteration 64/1000 | Loss: 0.00001942
Iteration 65/1000 | Loss: 0.00001942
Iteration 66/1000 | Loss: 0.00001942
Iteration 67/1000 | Loss: 0.00001942
Iteration 68/1000 | Loss: 0.00001942
Iteration 69/1000 | Loss: 0.00001941
Iteration 70/1000 | Loss: 0.00001941
Iteration 71/1000 | Loss: 0.00001941
Iteration 72/1000 | Loss: 0.00001941
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001941
Iteration 76/1000 | Loss: 0.00001941
Iteration 77/1000 | Loss: 0.00001941
Iteration 78/1000 | Loss: 0.00001941
Iteration 79/1000 | Loss: 0.00001941
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001940
Iteration 82/1000 | Loss: 0.00001940
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001940
Iteration 85/1000 | Loss: 0.00001940
Iteration 86/1000 | Loss: 0.00001940
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001939
Iteration 89/1000 | Loss: 0.00001939
Iteration 90/1000 | Loss: 0.00001939
Iteration 91/1000 | Loss: 0.00001939
Iteration 92/1000 | Loss: 0.00001939
Iteration 93/1000 | Loss: 0.00001939
Iteration 94/1000 | Loss: 0.00001939
Iteration 95/1000 | Loss: 0.00001938
Iteration 96/1000 | Loss: 0.00001938
Iteration 97/1000 | Loss: 0.00001938
Iteration 98/1000 | Loss: 0.00001938
Iteration 99/1000 | Loss: 0.00001938
Iteration 100/1000 | Loss: 0.00001938
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001937
Iteration 105/1000 | Loss: 0.00001937
Iteration 106/1000 | Loss: 0.00001937
Iteration 107/1000 | Loss: 0.00001937
Iteration 108/1000 | Loss: 0.00001936
Iteration 109/1000 | Loss: 0.00001936
Iteration 110/1000 | Loss: 0.00001936
Iteration 111/1000 | Loss: 0.00001936
Iteration 112/1000 | Loss: 0.00001936
Iteration 113/1000 | Loss: 0.00001935
Iteration 114/1000 | Loss: 0.00001935
Iteration 115/1000 | Loss: 0.00001935
Iteration 116/1000 | Loss: 0.00001935
Iteration 117/1000 | Loss: 0.00001933
Iteration 118/1000 | Loss: 0.00001933
Iteration 119/1000 | Loss: 0.00001933
Iteration 120/1000 | Loss: 0.00001932
Iteration 121/1000 | Loss: 0.00001932
Iteration 122/1000 | Loss: 0.00001932
Iteration 123/1000 | Loss: 0.00001932
Iteration 124/1000 | Loss: 0.00001932
Iteration 125/1000 | Loss: 0.00001931
Iteration 126/1000 | Loss: 0.00001931
Iteration 127/1000 | Loss: 0.00001931
Iteration 128/1000 | Loss: 0.00001931
Iteration 129/1000 | Loss: 0.00001930
Iteration 130/1000 | Loss: 0.00001930
Iteration 131/1000 | Loss: 0.00001930
Iteration 132/1000 | Loss: 0.00001930
Iteration 133/1000 | Loss: 0.00001930
Iteration 134/1000 | Loss: 0.00001930
Iteration 135/1000 | Loss: 0.00001930
Iteration 136/1000 | Loss: 0.00001930
Iteration 137/1000 | Loss: 0.00001929
Iteration 138/1000 | Loss: 0.00001929
Iteration 139/1000 | Loss: 0.00001929
Iteration 140/1000 | Loss: 0.00001929
Iteration 141/1000 | Loss: 0.00001929
Iteration 142/1000 | Loss: 0.00001929
Iteration 143/1000 | Loss: 0.00001929
Iteration 144/1000 | Loss: 0.00001929
Iteration 145/1000 | Loss: 0.00001929
Iteration 146/1000 | Loss: 0.00001929
Iteration 147/1000 | Loss: 0.00001929
Iteration 148/1000 | Loss: 0.00001928
Iteration 149/1000 | Loss: 0.00001928
Iteration 150/1000 | Loss: 0.00001928
Iteration 151/1000 | Loss: 0.00001928
Iteration 152/1000 | Loss: 0.00001928
Iteration 153/1000 | Loss: 0.00001928
Iteration 154/1000 | Loss: 0.00001928
Iteration 155/1000 | Loss: 0.00001928
Iteration 156/1000 | Loss: 0.00001928
Iteration 157/1000 | Loss: 0.00001928
Iteration 158/1000 | Loss: 0.00001928
Iteration 159/1000 | Loss: 0.00001928
Iteration 160/1000 | Loss: 0.00001927
Iteration 161/1000 | Loss: 0.00001927
Iteration 162/1000 | Loss: 0.00001927
Iteration 163/1000 | Loss: 0.00001927
Iteration 164/1000 | Loss: 0.00001927
Iteration 165/1000 | Loss: 0.00001927
Iteration 166/1000 | Loss: 0.00001927
Iteration 167/1000 | Loss: 0.00001927
Iteration 168/1000 | Loss: 0.00001927
Iteration 169/1000 | Loss: 0.00001927
Iteration 170/1000 | Loss: 0.00001927
Iteration 171/1000 | Loss: 0.00001927
Iteration 172/1000 | Loss: 0.00001926
Iteration 173/1000 | Loss: 0.00001926
Iteration 174/1000 | Loss: 0.00001926
Iteration 175/1000 | Loss: 0.00001926
Iteration 176/1000 | Loss: 0.00001926
Iteration 177/1000 | Loss: 0.00001926
Iteration 178/1000 | Loss: 0.00001926
Iteration 179/1000 | Loss: 0.00001926
Iteration 180/1000 | Loss: 0.00001925
Iteration 181/1000 | Loss: 0.00001925
Iteration 182/1000 | Loss: 0.00001925
Iteration 183/1000 | Loss: 0.00001925
Iteration 184/1000 | Loss: 0.00001925
Iteration 185/1000 | Loss: 0.00001925
Iteration 186/1000 | Loss: 0.00001925
Iteration 187/1000 | Loss: 0.00001925
Iteration 188/1000 | Loss: 0.00001924
Iteration 189/1000 | Loss: 0.00001924
Iteration 190/1000 | Loss: 0.00001924
Iteration 191/1000 | Loss: 0.00001924
Iteration 192/1000 | Loss: 0.00001924
Iteration 193/1000 | Loss: 0.00001924
Iteration 194/1000 | Loss: 0.00001924
Iteration 195/1000 | Loss: 0.00001924
Iteration 196/1000 | Loss: 0.00001924
Iteration 197/1000 | Loss: 0.00001924
Iteration 198/1000 | Loss: 0.00001924
Iteration 199/1000 | Loss: 0.00001924
Iteration 200/1000 | Loss: 0.00001924
Iteration 201/1000 | Loss: 0.00001924
Iteration 202/1000 | Loss: 0.00001924
Iteration 203/1000 | Loss: 0.00001924
Iteration 204/1000 | Loss: 0.00001924
Iteration 205/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.9239760149503127e-05, 1.9239760149503127e-05, 1.9239760149503127e-05, 1.9239760149503127e-05, 1.9239760149503127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9239760149503127e-05

Optimization complete. Final v2v error: 3.6400198936462402 mm

Highest mean error: 4.5690789222717285 mm for frame 138

Lowest mean error: 3.1209378242492676 mm for frame 190

Saving results

Total time: 65.30132794380188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454964
Iteration 2/25 | Loss: 0.00138829
Iteration 3/25 | Loss: 0.00128677
Iteration 4/25 | Loss: 0.00127415
Iteration 5/25 | Loss: 0.00127123
Iteration 6/25 | Loss: 0.00127100
Iteration 7/25 | Loss: 0.00127100
Iteration 8/25 | Loss: 0.00127100
Iteration 9/25 | Loss: 0.00127100
Iteration 10/25 | Loss: 0.00127100
Iteration 11/25 | Loss: 0.00127100
Iteration 12/25 | Loss: 0.00127100
Iteration 13/25 | Loss: 0.00127100
Iteration 14/25 | Loss: 0.00127100
Iteration 15/25 | Loss: 0.00127100
Iteration 16/25 | Loss: 0.00127100
Iteration 17/25 | Loss: 0.00127100
Iteration 18/25 | Loss: 0.00127100
Iteration 19/25 | Loss: 0.00127100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012710044393315911, 0.0012710044393315911, 0.0012710044393315911, 0.0012710044393315911, 0.0012710044393315911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012710044393315911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46236944
Iteration 2/25 | Loss: 0.00079213
Iteration 3/25 | Loss: 0.00079213
Iteration 4/25 | Loss: 0.00079213
Iteration 5/25 | Loss: 0.00079213
Iteration 6/25 | Loss: 0.00079213
Iteration 7/25 | Loss: 0.00079213
Iteration 8/25 | Loss: 0.00079213
Iteration 9/25 | Loss: 0.00079213
Iteration 10/25 | Loss: 0.00079213
Iteration 11/25 | Loss: 0.00079213
Iteration 12/25 | Loss: 0.00079213
Iteration 13/25 | Loss: 0.00079213
Iteration 14/25 | Loss: 0.00079213
Iteration 15/25 | Loss: 0.00079213
Iteration 16/25 | Loss: 0.00079213
Iteration 17/25 | Loss: 0.00079213
Iteration 18/25 | Loss: 0.00079213
Iteration 19/25 | Loss: 0.00079213
Iteration 20/25 | Loss: 0.00079213
Iteration 21/25 | Loss: 0.00079213
Iteration 22/25 | Loss: 0.00079213
Iteration 23/25 | Loss: 0.00079213
Iteration 24/25 | Loss: 0.00079213
Iteration 25/25 | Loss: 0.00079213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079213
Iteration 2/1000 | Loss: 0.00003316
Iteration 3/1000 | Loss: 0.00002653
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002388
Iteration 6/1000 | Loss: 0.00002331
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002242
Iteration 9/1000 | Loss: 0.00002213
Iteration 10/1000 | Loss: 0.00002182
Iteration 11/1000 | Loss: 0.00002161
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002128
Iteration 14/1000 | Loss: 0.00002128
Iteration 15/1000 | Loss: 0.00002121
Iteration 16/1000 | Loss: 0.00002120
Iteration 17/1000 | Loss: 0.00002118
Iteration 18/1000 | Loss: 0.00002117
Iteration 19/1000 | Loss: 0.00002117
Iteration 20/1000 | Loss: 0.00002116
Iteration 21/1000 | Loss: 0.00002116
Iteration 22/1000 | Loss: 0.00002115
Iteration 23/1000 | Loss: 0.00002115
Iteration 24/1000 | Loss: 0.00002114
Iteration 25/1000 | Loss: 0.00002113
Iteration 26/1000 | Loss: 0.00002113
Iteration 27/1000 | Loss: 0.00002111
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00002096
Iteration 30/1000 | Loss: 0.00002095
Iteration 31/1000 | Loss: 0.00002095
Iteration 32/1000 | Loss: 0.00002091
Iteration 33/1000 | Loss: 0.00002091
Iteration 34/1000 | Loss: 0.00002091
Iteration 35/1000 | Loss: 0.00002090
Iteration 36/1000 | Loss: 0.00002090
Iteration 37/1000 | Loss: 0.00002090
Iteration 38/1000 | Loss: 0.00002087
Iteration 39/1000 | Loss: 0.00002087
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002085
Iteration 42/1000 | Loss: 0.00002084
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002082
Iteration 49/1000 | Loss: 0.00002082
Iteration 50/1000 | Loss: 0.00002081
Iteration 51/1000 | Loss: 0.00002080
Iteration 52/1000 | Loss: 0.00002080
Iteration 53/1000 | Loss: 0.00002080
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002079
Iteration 57/1000 | Loss: 0.00002079
Iteration 58/1000 | Loss: 0.00002078
Iteration 59/1000 | Loss: 0.00002078
Iteration 60/1000 | Loss: 0.00002078
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002077
Iteration 63/1000 | Loss: 0.00002077
Iteration 64/1000 | Loss: 0.00002077
Iteration 65/1000 | Loss: 0.00002077
Iteration 66/1000 | Loss: 0.00002077
Iteration 67/1000 | Loss: 0.00002077
Iteration 68/1000 | Loss: 0.00002077
Iteration 69/1000 | Loss: 0.00002077
Iteration 70/1000 | Loss: 0.00002076
Iteration 71/1000 | Loss: 0.00002076
Iteration 72/1000 | Loss: 0.00002076
Iteration 73/1000 | Loss: 0.00002076
Iteration 74/1000 | Loss: 0.00002076
Iteration 75/1000 | Loss: 0.00002076
Iteration 76/1000 | Loss: 0.00002076
Iteration 77/1000 | Loss: 0.00002076
Iteration 78/1000 | Loss: 0.00002076
Iteration 79/1000 | Loss: 0.00002076
Iteration 80/1000 | Loss: 0.00002076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.07622506422922e-05, 2.07622506422922e-05, 2.07622506422922e-05, 2.07622506422922e-05, 2.07622506422922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.07622506422922e-05

Optimization complete. Final v2v error: 3.8387835025787354 mm

Highest mean error: 4.616382598876953 mm for frame 30

Lowest mean error: 3.66479754447937 mm for frame 4

Saving results

Total time: 35.03996753692627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797452
Iteration 2/25 | Loss: 0.00139842
Iteration 3/25 | Loss: 0.00129242
Iteration 4/25 | Loss: 0.00128124
Iteration 5/25 | Loss: 0.00127770
Iteration 6/25 | Loss: 0.00127701
Iteration 7/25 | Loss: 0.00127701
Iteration 8/25 | Loss: 0.00127701
Iteration 9/25 | Loss: 0.00127701
Iteration 10/25 | Loss: 0.00127701
Iteration 11/25 | Loss: 0.00127701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012770063476637006, 0.0012770063476637006, 0.0012770063476637006, 0.0012770063476637006, 0.0012770063476637006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012770063476637006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56151581
Iteration 2/25 | Loss: 0.00073685
Iteration 3/25 | Loss: 0.00073684
Iteration 4/25 | Loss: 0.00073684
Iteration 5/25 | Loss: 0.00073684
Iteration 6/25 | Loss: 0.00073684
Iteration 7/25 | Loss: 0.00073684
Iteration 8/25 | Loss: 0.00073684
Iteration 9/25 | Loss: 0.00073684
Iteration 10/25 | Loss: 0.00073684
Iteration 11/25 | Loss: 0.00073684
Iteration 12/25 | Loss: 0.00073684
Iteration 13/25 | Loss: 0.00073684
Iteration 14/25 | Loss: 0.00073684
Iteration 15/25 | Loss: 0.00073684
Iteration 16/25 | Loss: 0.00073684
Iteration 17/25 | Loss: 0.00073684
Iteration 18/25 | Loss: 0.00073684
Iteration 19/25 | Loss: 0.00073684
Iteration 20/25 | Loss: 0.00073684
Iteration 21/25 | Loss: 0.00073684
Iteration 22/25 | Loss: 0.00073684
Iteration 23/25 | Loss: 0.00073684
Iteration 24/25 | Loss: 0.00073684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007368386723101139, 0.0007368386723101139, 0.0007368386723101139, 0.0007368386723101139, 0.0007368386723101139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007368386723101139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073684
Iteration 2/1000 | Loss: 0.00004609
Iteration 3/1000 | Loss: 0.00002960
Iteration 4/1000 | Loss: 0.00002526
Iteration 5/1000 | Loss: 0.00002378
Iteration 6/1000 | Loss: 0.00002262
Iteration 7/1000 | Loss: 0.00002204
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002116
Iteration 10/1000 | Loss: 0.00002085
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002033
Iteration 13/1000 | Loss: 0.00002022
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00002002
Iteration 16/1000 | Loss: 0.00001985
Iteration 17/1000 | Loss: 0.00001982
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001978
Iteration 21/1000 | Loss: 0.00001978
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001976
Iteration 24/1000 | Loss: 0.00001976
Iteration 25/1000 | Loss: 0.00001975
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001974
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001973
Iteration 34/1000 | Loss: 0.00001973
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001972
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001968
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001966
Iteration 47/1000 | Loss: 0.00001965
Iteration 48/1000 | Loss: 0.00001965
Iteration 49/1000 | Loss: 0.00001965
Iteration 50/1000 | Loss: 0.00001965
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001965
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001964
Iteration 58/1000 | Loss: 0.00001964
Iteration 59/1000 | Loss: 0.00001964
Iteration 60/1000 | Loss: 0.00001964
Iteration 61/1000 | Loss: 0.00001963
Iteration 62/1000 | Loss: 0.00001962
Iteration 63/1000 | Loss: 0.00001962
Iteration 64/1000 | Loss: 0.00001962
Iteration 65/1000 | Loss: 0.00001960
Iteration 66/1000 | Loss: 0.00001960
Iteration 67/1000 | Loss: 0.00001960
Iteration 68/1000 | Loss: 0.00001960
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001960
Iteration 78/1000 | Loss: 0.00001960
Iteration 79/1000 | Loss: 0.00001960
Iteration 80/1000 | Loss: 0.00001960
Iteration 81/1000 | Loss: 0.00001960
Iteration 82/1000 | Loss: 0.00001960
Iteration 83/1000 | Loss: 0.00001960
Iteration 84/1000 | Loss: 0.00001960
Iteration 85/1000 | Loss: 0.00001960
Iteration 86/1000 | Loss: 0.00001960
Iteration 87/1000 | Loss: 0.00001960
Iteration 88/1000 | Loss: 0.00001960
Iteration 89/1000 | Loss: 0.00001960
Iteration 90/1000 | Loss: 0.00001960
Iteration 91/1000 | Loss: 0.00001960
Iteration 92/1000 | Loss: 0.00001960
Iteration 93/1000 | Loss: 0.00001960
Iteration 94/1000 | Loss: 0.00001960
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001960
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.9596727725001983e-05, 1.9596727725001983e-05, 1.9596727725001983e-05, 1.9596727725001983e-05, 1.9596727725001983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9596727725001983e-05

Optimization complete. Final v2v error: 3.68088960647583 mm

Highest mean error: 4.512729167938232 mm for frame 121

Lowest mean error: 2.997286796569824 mm for frame 146

Saving results

Total time: 36.82125806808472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00290509
Iteration 2/25 | Loss: 0.00143842
Iteration 3/25 | Loss: 0.00124600
Iteration 4/25 | Loss: 0.00121585
Iteration 5/25 | Loss: 0.00120536
Iteration 6/25 | Loss: 0.00120181
Iteration 7/25 | Loss: 0.00120152
Iteration 8/25 | Loss: 0.00120152
Iteration 9/25 | Loss: 0.00120152
Iteration 10/25 | Loss: 0.00120152
Iteration 11/25 | Loss: 0.00120152
Iteration 12/25 | Loss: 0.00120152
Iteration 13/25 | Loss: 0.00120152
Iteration 14/25 | Loss: 0.00120152
Iteration 15/25 | Loss: 0.00120152
Iteration 16/25 | Loss: 0.00120152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012015195097774267, 0.0012015195097774267, 0.0012015195097774267, 0.0012015195097774267, 0.0012015195097774267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012015195097774267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44798672
Iteration 2/25 | Loss: 0.00058611
Iteration 3/25 | Loss: 0.00058611
Iteration 4/25 | Loss: 0.00058611
Iteration 5/25 | Loss: 0.00058611
Iteration 6/25 | Loss: 0.00058611
Iteration 7/25 | Loss: 0.00058611
Iteration 8/25 | Loss: 0.00058611
Iteration 9/25 | Loss: 0.00058611
Iteration 10/25 | Loss: 0.00058611
Iteration 11/25 | Loss: 0.00058611
Iteration 12/25 | Loss: 0.00058611
Iteration 13/25 | Loss: 0.00058611
Iteration 14/25 | Loss: 0.00058611
Iteration 15/25 | Loss: 0.00058611
Iteration 16/25 | Loss: 0.00058611
Iteration 17/25 | Loss: 0.00058611
Iteration 18/25 | Loss: 0.00058611
Iteration 19/25 | Loss: 0.00058611
Iteration 20/25 | Loss: 0.00058611
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005861111567355692, 0.0005861111567355692, 0.0005861111567355692, 0.0005861111567355692, 0.0005861111567355692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005861111567355692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058611
Iteration 2/1000 | Loss: 0.00003726
Iteration 3/1000 | Loss: 0.00002443
Iteration 4/1000 | Loss: 0.00002248
Iteration 5/1000 | Loss: 0.00002043
Iteration 6/1000 | Loss: 0.00001912
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001673
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001654
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001641
Iteration 17/1000 | Loss: 0.00001635
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001630
Iteration 22/1000 | Loss: 0.00001629
Iteration 23/1000 | Loss: 0.00001629
Iteration 24/1000 | Loss: 0.00001627
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001626
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001625
Iteration 29/1000 | Loss: 0.00001625
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001623
Iteration 32/1000 | Loss: 0.00001623
Iteration 33/1000 | Loss: 0.00001623
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001621
Iteration 41/1000 | Loss: 0.00001621
Iteration 42/1000 | Loss: 0.00001620
Iteration 43/1000 | Loss: 0.00001620
Iteration 44/1000 | Loss: 0.00001620
Iteration 45/1000 | Loss: 0.00001619
Iteration 46/1000 | Loss: 0.00001619
Iteration 47/1000 | Loss: 0.00001618
Iteration 48/1000 | Loss: 0.00001617
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001615
Iteration 51/1000 | Loss: 0.00001615
Iteration 52/1000 | Loss: 0.00001615
Iteration 53/1000 | Loss: 0.00001615
Iteration 54/1000 | Loss: 0.00001614
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001613
Iteration 59/1000 | Loss: 0.00001613
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001609
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001606
Iteration 71/1000 | Loss: 0.00001606
Iteration 72/1000 | Loss: 0.00001605
Iteration 73/1000 | Loss: 0.00001605
Iteration 74/1000 | Loss: 0.00001604
Iteration 75/1000 | Loss: 0.00001604
Iteration 76/1000 | Loss: 0.00001604
Iteration 77/1000 | Loss: 0.00001604
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001603
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001603
Iteration 82/1000 | Loss: 0.00001602
Iteration 83/1000 | Loss: 0.00001602
Iteration 84/1000 | Loss: 0.00001602
Iteration 85/1000 | Loss: 0.00001602
Iteration 86/1000 | Loss: 0.00001602
Iteration 87/1000 | Loss: 0.00001601
Iteration 88/1000 | Loss: 0.00001601
Iteration 89/1000 | Loss: 0.00001601
Iteration 90/1000 | Loss: 0.00001601
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001600
Iteration 95/1000 | Loss: 0.00001600
Iteration 96/1000 | Loss: 0.00001600
Iteration 97/1000 | Loss: 0.00001600
Iteration 98/1000 | Loss: 0.00001600
Iteration 99/1000 | Loss: 0.00001600
Iteration 100/1000 | Loss: 0.00001599
Iteration 101/1000 | Loss: 0.00001599
Iteration 102/1000 | Loss: 0.00001599
Iteration 103/1000 | Loss: 0.00001599
Iteration 104/1000 | Loss: 0.00001599
Iteration 105/1000 | Loss: 0.00001599
Iteration 106/1000 | Loss: 0.00001599
Iteration 107/1000 | Loss: 0.00001599
Iteration 108/1000 | Loss: 0.00001599
Iteration 109/1000 | Loss: 0.00001599
Iteration 110/1000 | Loss: 0.00001599
Iteration 111/1000 | Loss: 0.00001599
Iteration 112/1000 | Loss: 0.00001598
Iteration 113/1000 | Loss: 0.00001598
Iteration 114/1000 | Loss: 0.00001598
Iteration 115/1000 | Loss: 0.00001598
Iteration 116/1000 | Loss: 0.00001598
Iteration 117/1000 | Loss: 0.00001598
Iteration 118/1000 | Loss: 0.00001598
Iteration 119/1000 | Loss: 0.00001598
Iteration 120/1000 | Loss: 0.00001598
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001597
Iteration 123/1000 | Loss: 0.00001597
Iteration 124/1000 | Loss: 0.00001597
Iteration 125/1000 | Loss: 0.00001597
Iteration 126/1000 | Loss: 0.00001597
Iteration 127/1000 | Loss: 0.00001597
Iteration 128/1000 | Loss: 0.00001597
Iteration 129/1000 | Loss: 0.00001597
Iteration 130/1000 | Loss: 0.00001597
Iteration 131/1000 | Loss: 0.00001597
Iteration 132/1000 | Loss: 0.00001597
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.597177833900787e-05, 1.597177833900787e-05, 1.597177833900787e-05, 1.597177833900787e-05, 1.597177833900787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.597177833900787e-05

Optimization complete. Final v2v error: 3.433476209640503 mm

Highest mean error: 3.6569857597351074 mm for frame 6

Lowest mean error: 3.1074349880218506 mm for frame 0

Saving results

Total time: 43.529627084732056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490321
Iteration 2/25 | Loss: 0.00133224
Iteration 3/25 | Loss: 0.00126015
Iteration 4/25 | Loss: 0.00125198
Iteration 5/25 | Loss: 0.00124956
Iteration 6/25 | Loss: 0.00124956
Iteration 7/25 | Loss: 0.00124956
Iteration 8/25 | Loss: 0.00124956
Iteration 9/25 | Loss: 0.00124956
Iteration 10/25 | Loss: 0.00124956
Iteration 11/25 | Loss: 0.00124956
Iteration 12/25 | Loss: 0.00124956
Iteration 13/25 | Loss: 0.00124956
Iteration 14/25 | Loss: 0.00124956
Iteration 15/25 | Loss: 0.00124956
Iteration 16/25 | Loss: 0.00124956
Iteration 17/25 | Loss: 0.00124956
Iteration 18/25 | Loss: 0.00124956
Iteration 19/25 | Loss: 0.00124956
Iteration 20/25 | Loss: 0.00124956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012495627161115408, 0.0012495627161115408, 0.0012495627161115408, 0.0012495627161115408, 0.0012495627161115408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012495627161115408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.84978867
Iteration 2/25 | Loss: 0.00076095
Iteration 3/25 | Loss: 0.00076094
Iteration 4/25 | Loss: 0.00076094
Iteration 5/25 | Loss: 0.00076094
Iteration 6/25 | Loss: 0.00076094
Iteration 7/25 | Loss: 0.00076094
Iteration 8/25 | Loss: 0.00076094
Iteration 9/25 | Loss: 0.00076094
Iteration 10/25 | Loss: 0.00076094
Iteration 11/25 | Loss: 0.00076094
Iteration 12/25 | Loss: 0.00076094
Iteration 13/25 | Loss: 0.00076094
Iteration 14/25 | Loss: 0.00076094
Iteration 15/25 | Loss: 0.00076094
Iteration 16/25 | Loss: 0.00076094
Iteration 17/25 | Loss: 0.00076094
Iteration 18/25 | Loss: 0.00076094
Iteration 19/25 | Loss: 0.00076094
Iteration 20/25 | Loss: 0.00076094
Iteration 21/25 | Loss: 0.00076094
Iteration 22/25 | Loss: 0.00076094
Iteration 23/25 | Loss: 0.00076094
Iteration 24/25 | Loss: 0.00076094
Iteration 25/25 | Loss: 0.00076094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076094
Iteration 2/1000 | Loss: 0.00002947
Iteration 3/1000 | Loss: 0.00002155
Iteration 4/1000 | Loss: 0.00001952
Iteration 5/1000 | Loss: 0.00001859
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001736
Iteration 8/1000 | Loss: 0.00001695
Iteration 9/1000 | Loss: 0.00001660
Iteration 10/1000 | Loss: 0.00001639
Iteration 11/1000 | Loss: 0.00001629
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001619
Iteration 15/1000 | Loss: 0.00001619
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001605
Iteration 19/1000 | Loss: 0.00001605
Iteration 20/1000 | Loss: 0.00001604
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001602
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001602
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001600
Iteration 30/1000 | Loss: 0.00001599
Iteration 31/1000 | Loss: 0.00001598
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001589
Iteration 44/1000 | Loss: 0.00001589
Iteration 45/1000 | Loss: 0.00001588
Iteration 46/1000 | Loss: 0.00001588
Iteration 47/1000 | Loss: 0.00001587
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001586
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001582
Iteration 54/1000 | Loss: 0.00001582
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001582
Iteration 57/1000 | Loss: 0.00001582
Iteration 58/1000 | Loss: 0.00001581
Iteration 59/1000 | Loss: 0.00001581
Iteration 60/1000 | Loss: 0.00001581
Iteration 61/1000 | Loss: 0.00001580
Iteration 62/1000 | Loss: 0.00001580
Iteration 63/1000 | Loss: 0.00001579
Iteration 64/1000 | Loss: 0.00001579
Iteration 65/1000 | Loss: 0.00001579
Iteration 66/1000 | Loss: 0.00001579
Iteration 67/1000 | Loss: 0.00001579
Iteration 68/1000 | Loss: 0.00001578
Iteration 69/1000 | Loss: 0.00001578
Iteration 70/1000 | Loss: 0.00001578
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00001578
Iteration 73/1000 | Loss: 0.00001578
Iteration 74/1000 | Loss: 0.00001578
Iteration 75/1000 | Loss: 0.00001578
Iteration 76/1000 | Loss: 0.00001578
Iteration 77/1000 | Loss: 0.00001577
Iteration 78/1000 | Loss: 0.00001577
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001577
Iteration 83/1000 | Loss: 0.00001577
Iteration 84/1000 | Loss: 0.00001577
Iteration 85/1000 | Loss: 0.00001577
Iteration 86/1000 | Loss: 0.00001577
Iteration 87/1000 | Loss: 0.00001577
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001576
Iteration 90/1000 | Loss: 0.00001576
Iteration 91/1000 | Loss: 0.00001575
Iteration 92/1000 | Loss: 0.00001574
Iteration 93/1000 | Loss: 0.00001574
Iteration 94/1000 | Loss: 0.00001573
Iteration 95/1000 | Loss: 0.00001573
Iteration 96/1000 | Loss: 0.00001572
Iteration 97/1000 | Loss: 0.00001572
Iteration 98/1000 | Loss: 0.00001572
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00001571
Iteration 102/1000 | Loss: 0.00001570
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001570
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001569
Iteration 112/1000 | Loss: 0.00001569
Iteration 113/1000 | Loss: 0.00001569
Iteration 114/1000 | Loss: 0.00001569
Iteration 115/1000 | Loss: 0.00001569
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001566
Iteration 127/1000 | Loss: 0.00001566
Iteration 128/1000 | Loss: 0.00001566
Iteration 129/1000 | Loss: 0.00001566
Iteration 130/1000 | Loss: 0.00001565
Iteration 131/1000 | Loss: 0.00001565
Iteration 132/1000 | Loss: 0.00001565
Iteration 133/1000 | Loss: 0.00001564
Iteration 134/1000 | Loss: 0.00001564
Iteration 135/1000 | Loss: 0.00001564
Iteration 136/1000 | Loss: 0.00001564
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001563
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001562
Iteration 143/1000 | Loss: 0.00001562
Iteration 144/1000 | Loss: 0.00001562
Iteration 145/1000 | Loss: 0.00001561
Iteration 146/1000 | Loss: 0.00001561
Iteration 147/1000 | Loss: 0.00001561
Iteration 148/1000 | Loss: 0.00001561
Iteration 149/1000 | Loss: 0.00001561
Iteration 150/1000 | Loss: 0.00001561
Iteration 151/1000 | Loss: 0.00001561
Iteration 152/1000 | Loss: 0.00001560
Iteration 153/1000 | Loss: 0.00001560
Iteration 154/1000 | Loss: 0.00001560
Iteration 155/1000 | Loss: 0.00001560
Iteration 156/1000 | Loss: 0.00001560
Iteration 157/1000 | Loss: 0.00001560
Iteration 158/1000 | Loss: 0.00001560
Iteration 159/1000 | Loss: 0.00001560
Iteration 160/1000 | Loss: 0.00001560
Iteration 161/1000 | Loss: 0.00001560
Iteration 162/1000 | Loss: 0.00001560
Iteration 163/1000 | Loss: 0.00001560
Iteration 164/1000 | Loss: 0.00001560
Iteration 165/1000 | Loss: 0.00001560
Iteration 166/1000 | Loss: 0.00001560
Iteration 167/1000 | Loss: 0.00001560
Iteration 168/1000 | Loss: 0.00001560
Iteration 169/1000 | Loss: 0.00001560
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.5598861864418723e-05, 1.5598861864418723e-05, 1.5598861864418723e-05, 1.5598861864418723e-05, 1.5598861864418723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5598861864418723e-05

Optimization complete. Final v2v error: 3.3125507831573486 mm

Highest mean error: 3.7962327003479004 mm for frame 191

Lowest mean error: 3.034388303756714 mm for frame 214

Saving results

Total time: 42.33062720298767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466578
Iteration 2/25 | Loss: 0.00137603
Iteration 3/25 | Loss: 0.00130970
Iteration 4/25 | Loss: 0.00130369
Iteration 5/25 | Loss: 0.00130193
Iteration 6/25 | Loss: 0.00130176
Iteration 7/25 | Loss: 0.00130176
Iteration 8/25 | Loss: 0.00130176
Iteration 9/25 | Loss: 0.00130176
Iteration 10/25 | Loss: 0.00130176
Iteration 11/25 | Loss: 0.00130176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013017583405598998, 0.0013017583405598998, 0.0013017583405598998, 0.0013017583405598998, 0.0013017583405598998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013017583405598998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49541128
Iteration 2/25 | Loss: 0.00090022
Iteration 3/25 | Loss: 0.00090022
Iteration 4/25 | Loss: 0.00090022
Iteration 5/25 | Loss: 0.00090022
Iteration 6/25 | Loss: 0.00090022
Iteration 7/25 | Loss: 0.00090022
Iteration 8/25 | Loss: 0.00090022
Iteration 9/25 | Loss: 0.00090022
Iteration 10/25 | Loss: 0.00090022
Iteration 11/25 | Loss: 0.00090022
Iteration 12/25 | Loss: 0.00090022
Iteration 13/25 | Loss: 0.00090022
Iteration 14/25 | Loss: 0.00090022
Iteration 15/25 | Loss: 0.00090022
Iteration 16/25 | Loss: 0.00090022
Iteration 17/25 | Loss: 0.00090022
Iteration 18/25 | Loss: 0.00090022
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000900216051377356, 0.000900216051377356, 0.000900216051377356, 0.000900216051377356, 0.000900216051377356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000900216051377356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090022
Iteration 2/1000 | Loss: 0.00004468
Iteration 3/1000 | Loss: 0.00002807
Iteration 4/1000 | Loss: 0.00002408
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002141
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00002010
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001977
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001945
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001944
Iteration 20/1000 | Loss: 0.00001943
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001937
Iteration 23/1000 | Loss: 0.00001933
Iteration 24/1000 | Loss: 0.00001932
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001931
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001930
Iteration 32/1000 | Loss: 0.00001930
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001930
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001927
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001925
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001925
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001924
Iteration 56/1000 | Loss: 0.00001924
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001921
Iteration 64/1000 | Loss: 0.00001921
Iteration 65/1000 | Loss: 0.00001921
Iteration 66/1000 | Loss: 0.00001921
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001921
Iteration 69/1000 | Loss: 0.00001921
Iteration 70/1000 | Loss: 0.00001921
Iteration 71/1000 | Loss: 0.00001921
Iteration 72/1000 | Loss: 0.00001921
Iteration 73/1000 | Loss: 0.00001920
Iteration 74/1000 | Loss: 0.00001920
Iteration 75/1000 | Loss: 0.00001920
Iteration 76/1000 | Loss: 0.00001920
Iteration 77/1000 | Loss: 0.00001920
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00001920
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001915
Iteration 87/1000 | Loss: 0.00001915
Iteration 88/1000 | Loss: 0.00001915
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001907
Iteration 104/1000 | Loss: 0.00001907
Iteration 105/1000 | Loss: 0.00001906
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001902
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001901
Iteration 114/1000 | Loss: 0.00001900
Iteration 115/1000 | Loss: 0.00001900
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001899
Iteration 118/1000 | Loss: 0.00001898
Iteration 119/1000 | Loss: 0.00001898
Iteration 120/1000 | Loss: 0.00001898
Iteration 121/1000 | Loss: 0.00001897
Iteration 122/1000 | Loss: 0.00001897
Iteration 123/1000 | Loss: 0.00001895
Iteration 124/1000 | Loss: 0.00001894
Iteration 125/1000 | Loss: 0.00001894
Iteration 126/1000 | Loss: 0.00001893
Iteration 127/1000 | Loss: 0.00001893
Iteration 128/1000 | Loss: 0.00001892
Iteration 129/1000 | Loss: 0.00001892
Iteration 130/1000 | Loss: 0.00001892
Iteration 131/1000 | Loss: 0.00001892
Iteration 132/1000 | Loss: 0.00001891
Iteration 133/1000 | Loss: 0.00001891
Iteration 134/1000 | Loss: 0.00001890
Iteration 135/1000 | Loss: 0.00001890
Iteration 136/1000 | Loss: 0.00001890
Iteration 137/1000 | Loss: 0.00001890
Iteration 138/1000 | Loss: 0.00001890
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Iteration 141/1000 | Loss: 0.00001889
Iteration 142/1000 | Loss: 0.00001889
Iteration 143/1000 | Loss: 0.00001889
Iteration 144/1000 | Loss: 0.00001888
Iteration 145/1000 | Loss: 0.00001888
Iteration 146/1000 | Loss: 0.00001888
Iteration 147/1000 | Loss: 0.00001887
Iteration 148/1000 | Loss: 0.00001887
Iteration 149/1000 | Loss: 0.00001887
Iteration 150/1000 | Loss: 0.00001886
Iteration 151/1000 | Loss: 0.00001886
Iteration 152/1000 | Loss: 0.00001886
Iteration 153/1000 | Loss: 0.00001885
Iteration 154/1000 | Loss: 0.00001885
Iteration 155/1000 | Loss: 0.00001885
Iteration 156/1000 | Loss: 0.00001884
Iteration 157/1000 | Loss: 0.00001884
Iteration 158/1000 | Loss: 0.00001883
Iteration 159/1000 | Loss: 0.00001883
Iteration 160/1000 | Loss: 0.00001883
Iteration 161/1000 | Loss: 0.00001883
Iteration 162/1000 | Loss: 0.00001883
Iteration 163/1000 | Loss: 0.00001883
Iteration 164/1000 | Loss: 0.00001883
Iteration 165/1000 | Loss: 0.00001882
Iteration 166/1000 | Loss: 0.00001881
Iteration 167/1000 | Loss: 0.00001881
Iteration 168/1000 | Loss: 0.00001881
Iteration 169/1000 | Loss: 0.00001881
Iteration 170/1000 | Loss: 0.00001881
Iteration 171/1000 | Loss: 0.00001881
Iteration 172/1000 | Loss: 0.00001880
Iteration 173/1000 | Loss: 0.00001880
Iteration 174/1000 | Loss: 0.00001880
Iteration 175/1000 | Loss: 0.00001880
Iteration 176/1000 | Loss: 0.00001880
Iteration 177/1000 | Loss: 0.00001880
Iteration 178/1000 | Loss: 0.00001880
Iteration 179/1000 | Loss: 0.00001879
Iteration 180/1000 | Loss: 0.00001879
Iteration 181/1000 | Loss: 0.00001879
Iteration 182/1000 | Loss: 0.00001879
Iteration 183/1000 | Loss: 0.00001879
Iteration 184/1000 | Loss: 0.00001879
Iteration 185/1000 | Loss: 0.00001879
Iteration 186/1000 | Loss: 0.00001879
Iteration 187/1000 | Loss: 0.00001878
Iteration 188/1000 | Loss: 0.00001878
Iteration 189/1000 | Loss: 0.00001878
Iteration 190/1000 | Loss: 0.00001878
Iteration 191/1000 | Loss: 0.00001878
Iteration 192/1000 | Loss: 0.00001878
Iteration 193/1000 | Loss: 0.00001878
Iteration 194/1000 | Loss: 0.00001878
Iteration 195/1000 | Loss: 0.00001878
Iteration 196/1000 | Loss: 0.00001878
Iteration 197/1000 | Loss: 0.00001878
Iteration 198/1000 | Loss: 0.00001878
Iteration 199/1000 | Loss: 0.00001878
Iteration 200/1000 | Loss: 0.00001877
Iteration 201/1000 | Loss: 0.00001877
Iteration 202/1000 | Loss: 0.00001877
Iteration 203/1000 | Loss: 0.00001877
Iteration 204/1000 | Loss: 0.00001877
Iteration 205/1000 | Loss: 0.00001877
Iteration 206/1000 | Loss: 0.00001877
Iteration 207/1000 | Loss: 0.00001877
Iteration 208/1000 | Loss: 0.00001876
Iteration 209/1000 | Loss: 0.00001876
Iteration 210/1000 | Loss: 0.00001876
Iteration 211/1000 | Loss: 0.00001876
Iteration 212/1000 | Loss: 0.00001876
Iteration 213/1000 | Loss: 0.00001876
Iteration 214/1000 | Loss: 0.00001876
Iteration 215/1000 | Loss: 0.00001876
Iteration 216/1000 | Loss: 0.00001876
Iteration 217/1000 | Loss: 0.00001876
Iteration 218/1000 | Loss: 0.00001876
Iteration 219/1000 | Loss: 0.00001875
Iteration 220/1000 | Loss: 0.00001875
Iteration 221/1000 | Loss: 0.00001875
Iteration 222/1000 | Loss: 0.00001875
Iteration 223/1000 | Loss: 0.00001875
Iteration 224/1000 | Loss: 0.00001875
Iteration 225/1000 | Loss: 0.00001875
Iteration 226/1000 | Loss: 0.00001875
Iteration 227/1000 | Loss: 0.00001875
Iteration 228/1000 | Loss: 0.00001875
Iteration 229/1000 | Loss: 0.00001874
Iteration 230/1000 | Loss: 0.00001874
Iteration 231/1000 | Loss: 0.00001874
Iteration 232/1000 | Loss: 0.00001874
Iteration 233/1000 | Loss: 0.00001873
Iteration 234/1000 | Loss: 0.00001873
Iteration 235/1000 | Loss: 0.00001873
Iteration 236/1000 | Loss: 0.00001873
Iteration 237/1000 | Loss: 0.00001872
Iteration 238/1000 | Loss: 0.00001872
Iteration 239/1000 | Loss: 0.00001872
Iteration 240/1000 | Loss: 0.00001872
Iteration 241/1000 | Loss: 0.00001872
Iteration 242/1000 | Loss: 0.00001871
Iteration 243/1000 | Loss: 0.00001871
Iteration 244/1000 | Loss: 0.00001871
Iteration 245/1000 | Loss: 0.00001871
Iteration 246/1000 | Loss: 0.00001871
Iteration 247/1000 | Loss: 0.00001871
Iteration 248/1000 | Loss: 0.00001871
Iteration 249/1000 | Loss: 0.00001870
Iteration 250/1000 | Loss: 0.00001870
Iteration 251/1000 | Loss: 0.00001870
Iteration 252/1000 | Loss: 0.00001870
Iteration 253/1000 | Loss: 0.00001870
Iteration 254/1000 | Loss: 0.00001870
Iteration 255/1000 | Loss: 0.00001870
Iteration 256/1000 | Loss: 0.00001870
Iteration 257/1000 | Loss: 0.00001870
Iteration 258/1000 | Loss: 0.00001870
Iteration 259/1000 | Loss: 0.00001870
Iteration 260/1000 | Loss: 0.00001870
Iteration 261/1000 | Loss: 0.00001870
Iteration 262/1000 | Loss: 0.00001870
Iteration 263/1000 | Loss: 0.00001870
Iteration 264/1000 | Loss: 0.00001870
Iteration 265/1000 | Loss: 0.00001869
Iteration 266/1000 | Loss: 0.00001869
Iteration 267/1000 | Loss: 0.00001869
Iteration 268/1000 | Loss: 0.00001869
Iteration 269/1000 | Loss: 0.00001869
Iteration 270/1000 | Loss: 0.00001869
Iteration 271/1000 | Loss: 0.00001869
Iteration 272/1000 | Loss: 0.00001869
Iteration 273/1000 | Loss: 0.00001869
Iteration 274/1000 | Loss: 0.00001869
Iteration 275/1000 | Loss: 0.00001869
Iteration 276/1000 | Loss: 0.00001869
Iteration 277/1000 | Loss: 0.00001869
Iteration 278/1000 | Loss: 0.00001869
Iteration 279/1000 | Loss: 0.00001869
Iteration 280/1000 | Loss: 0.00001869
Iteration 281/1000 | Loss: 0.00001868
Iteration 282/1000 | Loss: 0.00001868
Iteration 283/1000 | Loss: 0.00001868
Iteration 284/1000 | Loss: 0.00001868
Iteration 285/1000 | Loss: 0.00001868
Iteration 286/1000 | Loss: 0.00001868
Iteration 287/1000 | Loss: 0.00001868
Iteration 288/1000 | Loss: 0.00001868
Iteration 289/1000 | Loss: 0.00001868
Iteration 290/1000 | Loss: 0.00001868
Iteration 291/1000 | Loss: 0.00001868
Iteration 292/1000 | Loss: 0.00001868
Iteration 293/1000 | Loss: 0.00001867
Iteration 294/1000 | Loss: 0.00001867
Iteration 295/1000 | Loss: 0.00001867
Iteration 296/1000 | Loss: 0.00001867
Iteration 297/1000 | Loss: 0.00001867
Iteration 298/1000 | Loss: 0.00001867
Iteration 299/1000 | Loss: 0.00001867
Iteration 300/1000 | Loss: 0.00001867
Iteration 301/1000 | Loss: 0.00001867
Iteration 302/1000 | Loss: 0.00001867
Iteration 303/1000 | Loss: 0.00001867
Iteration 304/1000 | Loss: 0.00001867
Iteration 305/1000 | Loss: 0.00001867
Iteration 306/1000 | Loss: 0.00001867
Iteration 307/1000 | Loss: 0.00001867
Iteration 308/1000 | Loss: 0.00001867
Iteration 309/1000 | Loss: 0.00001867
Iteration 310/1000 | Loss: 0.00001867
Iteration 311/1000 | Loss: 0.00001866
Iteration 312/1000 | Loss: 0.00001866
Iteration 313/1000 | Loss: 0.00001866
Iteration 314/1000 | Loss: 0.00001866
Iteration 315/1000 | Loss: 0.00001866
Iteration 316/1000 | Loss: 0.00001866
Iteration 317/1000 | Loss: 0.00001866
Iteration 318/1000 | Loss: 0.00001866
Iteration 319/1000 | Loss: 0.00001866
Iteration 320/1000 | Loss: 0.00001866
Iteration 321/1000 | Loss: 0.00001866
Iteration 322/1000 | Loss: 0.00001866
Iteration 323/1000 | Loss: 0.00001866
Iteration 324/1000 | Loss: 0.00001866
Iteration 325/1000 | Loss: 0.00001866
Iteration 326/1000 | Loss: 0.00001866
Iteration 327/1000 | Loss: 0.00001866
Iteration 328/1000 | Loss: 0.00001866
Iteration 329/1000 | Loss: 0.00001865
Iteration 330/1000 | Loss: 0.00001865
Iteration 331/1000 | Loss: 0.00001865
Iteration 332/1000 | Loss: 0.00001865
Iteration 333/1000 | Loss: 0.00001865
Iteration 334/1000 | Loss: 0.00001865
Iteration 335/1000 | Loss: 0.00001865
Iteration 336/1000 | Loss: 0.00001865
Iteration 337/1000 | Loss: 0.00001865
Iteration 338/1000 | Loss: 0.00001865
Iteration 339/1000 | Loss: 0.00001865
Iteration 340/1000 | Loss: 0.00001865
Iteration 341/1000 | Loss: 0.00001865
Iteration 342/1000 | Loss: 0.00001865
Iteration 343/1000 | Loss: 0.00001865
Iteration 344/1000 | Loss: 0.00001865
Iteration 345/1000 | Loss: 0.00001865
Iteration 346/1000 | Loss: 0.00001865
Iteration 347/1000 | Loss: 0.00001865
Iteration 348/1000 | Loss: 0.00001865
Iteration 349/1000 | Loss: 0.00001865
Iteration 350/1000 | Loss: 0.00001865
Iteration 351/1000 | Loss: 0.00001865
Iteration 352/1000 | Loss: 0.00001865
Iteration 353/1000 | Loss: 0.00001865
Iteration 354/1000 | Loss: 0.00001865
Iteration 355/1000 | Loss: 0.00001865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [1.8654320228961296e-05, 1.8654320228961296e-05, 1.8654320228961296e-05, 1.8654320228961296e-05, 1.8654320228961296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8654320228961296e-05

Optimization complete. Final v2v error: 3.5927555561065674 mm

Highest mean error: 3.9415223598480225 mm for frame 9

Lowest mean error: 3.070115327835083 mm for frame 147

Saving results

Total time: 48.555684328079224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497609
Iteration 2/25 | Loss: 0.00145104
Iteration 3/25 | Loss: 0.00135939
Iteration 4/25 | Loss: 0.00134808
Iteration 5/25 | Loss: 0.00134479
Iteration 6/25 | Loss: 0.00134438
Iteration 7/25 | Loss: 0.00134438
Iteration 8/25 | Loss: 0.00134438
Iteration 9/25 | Loss: 0.00134438
Iteration 10/25 | Loss: 0.00134438
Iteration 11/25 | Loss: 0.00134438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013443783391267061, 0.0013443783391267061, 0.0013443783391267061, 0.0013443783391267061, 0.0013443783391267061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013443783391267061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47606313
Iteration 2/25 | Loss: 0.00085281
Iteration 3/25 | Loss: 0.00085279
Iteration 4/25 | Loss: 0.00085279
Iteration 5/25 | Loss: 0.00085279
Iteration 6/25 | Loss: 0.00085279
Iteration 7/25 | Loss: 0.00085279
Iteration 8/25 | Loss: 0.00085279
Iteration 9/25 | Loss: 0.00085279
Iteration 10/25 | Loss: 0.00085279
Iteration 11/25 | Loss: 0.00085279
Iteration 12/25 | Loss: 0.00085279
Iteration 13/25 | Loss: 0.00085279
Iteration 14/25 | Loss: 0.00085279
Iteration 15/25 | Loss: 0.00085279
Iteration 16/25 | Loss: 0.00085279
Iteration 17/25 | Loss: 0.00085279
Iteration 18/25 | Loss: 0.00085279
Iteration 19/25 | Loss: 0.00085279
Iteration 20/25 | Loss: 0.00085279
Iteration 21/25 | Loss: 0.00085279
Iteration 22/25 | Loss: 0.00085279
Iteration 23/25 | Loss: 0.00085279
Iteration 24/25 | Loss: 0.00085279
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008527851314283907, 0.0008527851314283907, 0.0008527851314283907, 0.0008527851314283907, 0.0008527851314283907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008527851314283907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085279
Iteration 2/1000 | Loss: 0.00004528
Iteration 3/1000 | Loss: 0.00003115
Iteration 4/1000 | Loss: 0.00002831
Iteration 5/1000 | Loss: 0.00002682
Iteration 6/1000 | Loss: 0.00002589
Iteration 7/1000 | Loss: 0.00002524
Iteration 8/1000 | Loss: 0.00002479
Iteration 9/1000 | Loss: 0.00002451
Iteration 10/1000 | Loss: 0.00002444
Iteration 11/1000 | Loss: 0.00002427
Iteration 12/1000 | Loss: 0.00002420
Iteration 13/1000 | Loss: 0.00002415
Iteration 14/1000 | Loss: 0.00002414
Iteration 15/1000 | Loss: 0.00002412
Iteration 16/1000 | Loss: 0.00002409
Iteration 17/1000 | Loss: 0.00002402
Iteration 18/1000 | Loss: 0.00002402
Iteration 19/1000 | Loss: 0.00002400
Iteration 20/1000 | Loss: 0.00002399
Iteration 21/1000 | Loss: 0.00002397
Iteration 22/1000 | Loss: 0.00002396
Iteration 23/1000 | Loss: 0.00002395
Iteration 24/1000 | Loss: 0.00002394
Iteration 25/1000 | Loss: 0.00002394
Iteration 26/1000 | Loss: 0.00002393
Iteration 27/1000 | Loss: 0.00002392
Iteration 28/1000 | Loss: 0.00002390
Iteration 29/1000 | Loss: 0.00002388
Iteration 30/1000 | Loss: 0.00002387
Iteration 31/1000 | Loss: 0.00002386
Iteration 32/1000 | Loss: 0.00002385
Iteration 33/1000 | Loss: 0.00002385
Iteration 34/1000 | Loss: 0.00002384
Iteration 35/1000 | Loss: 0.00002384
Iteration 36/1000 | Loss: 0.00002384
Iteration 37/1000 | Loss: 0.00002383
Iteration 38/1000 | Loss: 0.00002383
Iteration 39/1000 | Loss: 0.00002382
Iteration 40/1000 | Loss: 0.00002381
Iteration 41/1000 | Loss: 0.00002381
Iteration 42/1000 | Loss: 0.00002379
Iteration 43/1000 | Loss: 0.00002379
Iteration 44/1000 | Loss: 0.00002378
Iteration 45/1000 | Loss: 0.00002377
Iteration 46/1000 | Loss: 0.00002376
Iteration 47/1000 | Loss: 0.00002376
Iteration 48/1000 | Loss: 0.00002375
Iteration 49/1000 | Loss: 0.00002375
Iteration 50/1000 | Loss: 0.00002375
Iteration 51/1000 | Loss: 0.00002374
Iteration 52/1000 | Loss: 0.00002374
Iteration 53/1000 | Loss: 0.00002374
Iteration 54/1000 | Loss: 0.00002373
Iteration 55/1000 | Loss: 0.00002373
Iteration 56/1000 | Loss: 0.00002373
Iteration 57/1000 | Loss: 0.00002372
Iteration 58/1000 | Loss: 0.00002372
Iteration 59/1000 | Loss: 0.00002371
Iteration 60/1000 | Loss: 0.00002370
Iteration 61/1000 | Loss: 0.00002370
Iteration 62/1000 | Loss: 0.00002370
Iteration 63/1000 | Loss: 0.00002370
Iteration 64/1000 | Loss: 0.00002370
Iteration 65/1000 | Loss: 0.00002370
Iteration 66/1000 | Loss: 0.00002370
Iteration 67/1000 | Loss: 0.00002369
Iteration 68/1000 | Loss: 0.00002369
Iteration 69/1000 | Loss: 0.00002369
Iteration 70/1000 | Loss: 0.00002369
Iteration 71/1000 | Loss: 0.00002369
Iteration 72/1000 | Loss: 0.00002368
Iteration 73/1000 | Loss: 0.00002368
Iteration 74/1000 | Loss: 0.00002368
Iteration 75/1000 | Loss: 0.00002368
Iteration 76/1000 | Loss: 0.00002367
Iteration 77/1000 | Loss: 0.00002367
Iteration 78/1000 | Loss: 0.00002367
Iteration 79/1000 | Loss: 0.00002367
Iteration 80/1000 | Loss: 0.00002366
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002366
Iteration 83/1000 | Loss: 0.00002366
Iteration 84/1000 | Loss: 0.00002366
Iteration 85/1000 | Loss: 0.00002366
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00002365
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002365
Iteration 90/1000 | Loss: 0.00002365
Iteration 91/1000 | Loss: 0.00002365
Iteration 92/1000 | Loss: 0.00002365
Iteration 93/1000 | Loss: 0.00002365
Iteration 94/1000 | Loss: 0.00002364
Iteration 95/1000 | Loss: 0.00002364
Iteration 96/1000 | Loss: 0.00002364
Iteration 97/1000 | Loss: 0.00002364
Iteration 98/1000 | Loss: 0.00002364
Iteration 99/1000 | Loss: 0.00002363
Iteration 100/1000 | Loss: 0.00002363
Iteration 101/1000 | Loss: 0.00002363
Iteration 102/1000 | Loss: 0.00002363
Iteration 103/1000 | Loss: 0.00002363
Iteration 104/1000 | Loss: 0.00002363
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002362
Iteration 107/1000 | Loss: 0.00002362
Iteration 108/1000 | Loss: 0.00002362
Iteration 109/1000 | Loss: 0.00002362
Iteration 110/1000 | Loss: 0.00002362
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00002361
Iteration 113/1000 | Loss: 0.00002361
Iteration 114/1000 | Loss: 0.00002360
Iteration 115/1000 | Loss: 0.00002360
Iteration 116/1000 | Loss: 0.00002360
Iteration 117/1000 | Loss: 0.00002360
Iteration 118/1000 | Loss: 0.00002360
Iteration 119/1000 | Loss: 0.00002359
Iteration 120/1000 | Loss: 0.00002359
Iteration 121/1000 | Loss: 0.00002359
Iteration 122/1000 | Loss: 0.00002359
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002359
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002358
Iteration 127/1000 | Loss: 0.00002358
Iteration 128/1000 | Loss: 0.00002358
Iteration 129/1000 | Loss: 0.00002358
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002357
Iteration 133/1000 | Loss: 0.00002357
Iteration 134/1000 | Loss: 0.00002357
Iteration 135/1000 | Loss: 0.00002357
Iteration 136/1000 | Loss: 0.00002356
Iteration 137/1000 | Loss: 0.00002356
Iteration 138/1000 | Loss: 0.00002356
Iteration 139/1000 | Loss: 0.00002356
Iteration 140/1000 | Loss: 0.00002356
Iteration 141/1000 | Loss: 0.00002356
Iteration 142/1000 | Loss: 0.00002356
Iteration 143/1000 | Loss: 0.00002356
Iteration 144/1000 | Loss: 0.00002356
Iteration 145/1000 | Loss: 0.00002356
Iteration 146/1000 | Loss: 0.00002356
Iteration 147/1000 | Loss: 0.00002356
Iteration 148/1000 | Loss: 0.00002356
Iteration 149/1000 | Loss: 0.00002356
Iteration 150/1000 | Loss: 0.00002356
Iteration 151/1000 | Loss: 0.00002356
Iteration 152/1000 | Loss: 0.00002356
Iteration 153/1000 | Loss: 0.00002356
Iteration 154/1000 | Loss: 0.00002356
Iteration 155/1000 | Loss: 0.00002356
Iteration 156/1000 | Loss: 0.00002356
Iteration 157/1000 | Loss: 0.00002356
Iteration 158/1000 | Loss: 0.00002356
Iteration 159/1000 | Loss: 0.00002356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.3564367438666523e-05, 2.3564367438666523e-05, 2.3564367438666523e-05, 2.3564367438666523e-05, 2.3564367438666523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3564367438666523e-05

Optimization complete. Final v2v error: 3.9522764682769775 mm

Highest mean error: 4.522673606872559 mm for frame 132

Lowest mean error: 3.4444479942321777 mm for frame 1

Saving results

Total time: 36.805225133895874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080068
Iteration 2/25 | Loss: 0.01080067
Iteration 3/25 | Loss: 0.01080067
Iteration 4/25 | Loss: 0.01080066
Iteration 5/25 | Loss: 0.01080066
Iteration 6/25 | Loss: 0.00373843
Iteration 7/25 | Loss: 0.00301033
Iteration 8/25 | Loss: 0.00277469
Iteration 9/25 | Loss: 0.00250251
Iteration 10/25 | Loss: 0.00239520
Iteration 11/25 | Loss: 0.00216648
Iteration 12/25 | Loss: 0.00196541
Iteration 13/25 | Loss: 0.00192299
Iteration 14/25 | Loss: 0.00191809
Iteration 15/25 | Loss: 0.00182986
Iteration 16/25 | Loss: 0.00174827
Iteration 17/25 | Loss: 0.00171075
Iteration 18/25 | Loss: 0.00165854
Iteration 19/25 | Loss: 0.00163477
Iteration 20/25 | Loss: 0.00161453
Iteration 21/25 | Loss: 0.00160660
Iteration 22/25 | Loss: 0.00160326
Iteration 23/25 | Loss: 0.00160334
Iteration 24/25 | Loss: 0.00159401
Iteration 25/25 | Loss: 0.00159354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25996804
Iteration 2/25 | Loss: 0.00234769
Iteration 3/25 | Loss: 0.00234055
Iteration 4/25 | Loss: 0.00234055
Iteration 5/25 | Loss: 0.00234055
Iteration 6/25 | Loss: 0.00234055
Iteration 7/25 | Loss: 0.00234055
Iteration 8/25 | Loss: 0.00234055
Iteration 9/25 | Loss: 0.00234055
Iteration 10/25 | Loss: 0.00234055
Iteration 11/25 | Loss: 0.00234055
Iteration 12/25 | Loss: 0.00234055
Iteration 13/25 | Loss: 0.00234055
Iteration 14/25 | Loss: 0.00234055
Iteration 15/25 | Loss: 0.00234055
Iteration 16/25 | Loss: 0.00234055
Iteration 17/25 | Loss: 0.00234055
Iteration 18/25 | Loss: 0.00234055
Iteration 19/25 | Loss: 0.00234055
Iteration 20/25 | Loss: 0.00234055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002340545877814293, 0.002340545877814293, 0.002340545877814293, 0.002340545877814293, 0.002340545877814293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002340545877814293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234055
Iteration 2/1000 | Loss: 0.00034664
Iteration 3/1000 | Loss: 0.00035329
Iteration 4/1000 | Loss: 0.00029808
Iteration 5/1000 | Loss: 0.00027367
Iteration 6/1000 | Loss: 0.00032882
Iteration 7/1000 | Loss: 0.00030373
Iteration 8/1000 | Loss: 0.00023416
Iteration 9/1000 | Loss: 0.00063597
Iteration 10/1000 | Loss: 0.00045675
Iteration 11/1000 | Loss: 0.00038004
Iteration 12/1000 | Loss: 0.00031750
Iteration 13/1000 | Loss: 0.00029638
Iteration 14/1000 | Loss: 0.00029175
Iteration 15/1000 | Loss: 0.00055391
Iteration 16/1000 | Loss: 0.00030628
Iteration 17/1000 | Loss: 0.00047885
Iteration 18/1000 | Loss: 0.00032648
Iteration 19/1000 | Loss: 0.00026546
Iteration 20/1000 | Loss: 0.00022140
Iteration 21/1000 | Loss: 0.00061789
Iteration 22/1000 | Loss: 0.00027006
Iteration 23/1000 | Loss: 0.00024621
Iteration 24/1000 | Loss: 0.00014208
Iteration 25/1000 | Loss: 0.00096818
Iteration 26/1000 | Loss: 0.00050756
Iteration 27/1000 | Loss: 0.00030346
Iteration 28/1000 | Loss: 0.00035247
Iteration 29/1000 | Loss: 0.00044616
Iteration 30/1000 | Loss: 0.00018037
Iteration 31/1000 | Loss: 0.00020806
Iteration 32/1000 | Loss: 0.00012273
Iteration 33/1000 | Loss: 0.00041712
Iteration 34/1000 | Loss: 0.00077005
Iteration 35/1000 | Loss: 0.00032309
Iteration 36/1000 | Loss: 0.00033325
Iteration 37/1000 | Loss: 0.00028364
Iteration 38/1000 | Loss: 0.00031730
Iteration 39/1000 | Loss: 0.00016574
Iteration 40/1000 | Loss: 0.00031155
Iteration 41/1000 | Loss: 0.00030087
Iteration 42/1000 | Loss: 0.00029333
Iteration 43/1000 | Loss: 0.00030088
Iteration 44/1000 | Loss: 0.00043477
Iteration 45/1000 | Loss: 0.00036595
Iteration 46/1000 | Loss: 0.00038927
Iteration 47/1000 | Loss: 0.00037672
Iteration 48/1000 | Loss: 0.00019519
Iteration 49/1000 | Loss: 0.00018468
Iteration 50/1000 | Loss: 0.00029435
Iteration 51/1000 | Loss: 0.00031928
Iteration 52/1000 | Loss: 0.00031761
Iteration 53/1000 | Loss: 0.00084244
Iteration 54/1000 | Loss: 0.00039461
Iteration 55/1000 | Loss: 0.00020244
Iteration 56/1000 | Loss: 0.00048738
Iteration 57/1000 | Loss: 0.00013020
Iteration 58/1000 | Loss: 0.00025756
Iteration 59/1000 | Loss: 0.00026291
Iteration 60/1000 | Loss: 0.00014718
Iteration 61/1000 | Loss: 0.00047670
Iteration 62/1000 | Loss: 0.00033282
Iteration 63/1000 | Loss: 0.00062755
Iteration 64/1000 | Loss: 0.00108198
Iteration 65/1000 | Loss: 0.00026107
Iteration 66/1000 | Loss: 0.00013402
Iteration 67/1000 | Loss: 0.00012349
Iteration 68/1000 | Loss: 0.00165023
Iteration 69/1000 | Loss: 0.00073315
Iteration 70/1000 | Loss: 0.00099206
Iteration 71/1000 | Loss: 0.00063347
Iteration 72/1000 | Loss: 0.00042848
Iteration 73/1000 | Loss: 0.00017540
Iteration 74/1000 | Loss: 0.00035304
Iteration 75/1000 | Loss: 0.00048045
Iteration 76/1000 | Loss: 0.00039579
Iteration 77/1000 | Loss: 0.00037411
Iteration 78/1000 | Loss: 0.00018885
Iteration 79/1000 | Loss: 0.00029808
Iteration 80/1000 | Loss: 0.00025088
Iteration 81/1000 | Loss: 0.00014683
Iteration 82/1000 | Loss: 0.00031142
Iteration 83/1000 | Loss: 0.00058507
Iteration 84/1000 | Loss: 0.00044283
Iteration 85/1000 | Loss: 0.00031814
Iteration 86/1000 | Loss: 0.00014390
Iteration 87/1000 | Loss: 0.00098051
Iteration 88/1000 | Loss: 0.00098317
Iteration 89/1000 | Loss: 0.00042742
Iteration 90/1000 | Loss: 0.00045356
Iteration 91/1000 | Loss: 0.00037370
Iteration 92/1000 | Loss: 0.00019150
Iteration 93/1000 | Loss: 0.00043090
Iteration 94/1000 | Loss: 0.00057125
Iteration 95/1000 | Loss: 0.00058085
Iteration 96/1000 | Loss: 0.00080109
Iteration 97/1000 | Loss: 0.00069770
Iteration 98/1000 | Loss: 0.00047899
Iteration 99/1000 | Loss: 0.00046709
Iteration 100/1000 | Loss: 0.00048233
Iteration 101/1000 | Loss: 0.00043344
Iteration 102/1000 | Loss: 0.00019190
Iteration 103/1000 | Loss: 0.00015873
Iteration 104/1000 | Loss: 0.00015665
Iteration 105/1000 | Loss: 0.00007165
Iteration 106/1000 | Loss: 0.00014635
Iteration 107/1000 | Loss: 0.00011563
Iteration 108/1000 | Loss: 0.00014472
Iteration 109/1000 | Loss: 0.00012518
Iteration 110/1000 | Loss: 0.00022306
Iteration 111/1000 | Loss: 0.00032694
Iteration 112/1000 | Loss: 0.00020824
Iteration 113/1000 | Loss: 0.00021204
Iteration 114/1000 | Loss: 0.00031622
Iteration 115/1000 | Loss: 0.00030089
Iteration 116/1000 | Loss: 0.00033351
Iteration 117/1000 | Loss: 0.00009639
Iteration 118/1000 | Loss: 0.00030682
Iteration 119/1000 | Loss: 0.00049381
Iteration 120/1000 | Loss: 0.00050400
Iteration 121/1000 | Loss: 0.00038553
Iteration 122/1000 | Loss: 0.00014803
Iteration 123/1000 | Loss: 0.00029971
Iteration 124/1000 | Loss: 0.00045176
Iteration 125/1000 | Loss: 0.00043450
Iteration 126/1000 | Loss: 0.00019413
Iteration 127/1000 | Loss: 0.00015088
Iteration 128/1000 | Loss: 0.00017195
Iteration 129/1000 | Loss: 0.00015040
Iteration 130/1000 | Loss: 0.00019163
Iteration 131/1000 | Loss: 0.00038670
Iteration 132/1000 | Loss: 0.00085100
Iteration 133/1000 | Loss: 0.00067323
Iteration 134/1000 | Loss: 0.00021877
Iteration 135/1000 | Loss: 0.00009150
Iteration 136/1000 | Loss: 0.00008267
Iteration 137/1000 | Loss: 0.00007901
Iteration 138/1000 | Loss: 0.00008702
Iteration 139/1000 | Loss: 0.00007861
Iteration 140/1000 | Loss: 0.00007934
Iteration 141/1000 | Loss: 0.00033003
Iteration 142/1000 | Loss: 0.00028557
Iteration 143/1000 | Loss: 0.00024108
Iteration 144/1000 | Loss: 0.00013941
Iteration 145/1000 | Loss: 0.00042889
Iteration 146/1000 | Loss: 0.00053469
Iteration 147/1000 | Loss: 0.00037093
Iteration 148/1000 | Loss: 0.00045093
Iteration 149/1000 | Loss: 0.00032489
Iteration 150/1000 | Loss: 0.00027565
Iteration 151/1000 | Loss: 0.00053935
Iteration 152/1000 | Loss: 0.00063463
Iteration 153/1000 | Loss: 0.00021961
Iteration 154/1000 | Loss: 0.00026440
Iteration 155/1000 | Loss: 0.00008412
Iteration 156/1000 | Loss: 0.00008395
Iteration 157/1000 | Loss: 0.00006944
Iteration 158/1000 | Loss: 0.00007089
Iteration 159/1000 | Loss: 0.00007192
Iteration 160/1000 | Loss: 0.00007432
Iteration 161/1000 | Loss: 0.00007506
Iteration 162/1000 | Loss: 0.00007827
Iteration 163/1000 | Loss: 0.00035207
Iteration 164/1000 | Loss: 0.00020367
Iteration 165/1000 | Loss: 0.00048422
Iteration 166/1000 | Loss: 0.00019239
Iteration 167/1000 | Loss: 0.00018065
Iteration 168/1000 | Loss: 0.00015688
Iteration 169/1000 | Loss: 0.00025632
Iteration 170/1000 | Loss: 0.00025933
Iteration 171/1000 | Loss: 0.00017621
Iteration 172/1000 | Loss: 0.00024195
Iteration 173/1000 | Loss: 0.00012346
Iteration 174/1000 | Loss: 0.00015904
Iteration 175/1000 | Loss: 0.00026919
Iteration 176/1000 | Loss: 0.00013845
Iteration 177/1000 | Loss: 0.00007284
Iteration 178/1000 | Loss: 0.00006470
Iteration 179/1000 | Loss: 0.00006909
Iteration 180/1000 | Loss: 0.00006957
Iteration 181/1000 | Loss: 0.00007956
Iteration 182/1000 | Loss: 0.00020943
Iteration 183/1000 | Loss: 0.00016812
Iteration 184/1000 | Loss: 0.00006606
Iteration 185/1000 | Loss: 0.00006987
Iteration 186/1000 | Loss: 0.00019380
Iteration 187/1000 | Loss: 0.00029438
Iteration 188/1000 | Loss: 0.00007795
Iteration 189/1000 | Loss: 0.00006913
Iteration 190/1000 | Loss: 0.00006732
Iteration 191/1000 | Loss: 0.00006870
Iteration 192/1000 | Loss: 0.00021848
Iteration 193/1000 | Loss: 0.00030847
Iteration 194/1000 | Loss: 0.00031073
Iteration 195/1000 | Loss: 0.00040739
Iteration 196/1000 | Loss: 0.00044543
Iteration 197/1000 | Loss: 0.00009909
Iteration 198/1000 | Loss: 0.00026400
Iteration 199/1000 | Loss: 0.00007453
Iteration 200/1000 | Loss: 0.00007119
Iteration 201/1000 | Loss: 0.00007970
Iteration 202/1000 | Loss: 0.00005947
Iteration 203/1000 | Loss: 0.00025909
Iteration 204/1000 | Loss: 0.00025256
Iteration 205/1000 | Loss: 0.00022512
Iteration 206/1000 | Loss: 0.00016886
Iteration 207/1000 | Loss: 0.00022454
Iteration 208/1000 | Loss: 0.00016845
Iteration 209/1000 | Loss: 0.00021482
Iteration 210/1000 | Loss: 0.00008469
Iteration 211/1000 | Loss: 0.00006757
Iteration 212/1000 | Loss: 0.00022152
Iteration 213/1000 | Loss: 0.00020568
Iteration 214/1000 | Loss: 0.00007455
Iteration 215/1000 | Loss: 0.00006699
Iteration 216/1000 | Loss: 0.00021945
Iteration 217/1000 | Loss: 0.00042459
Iteration 218/1000 | Loss: 0.00025517
Iteration 219/1000 | Loss: 0.00034343
Iteration 220/1000 | Loss: 0.00037734
Iteration 221/1000 | Loss: 0.00006626
Iteration 222/1000 | Loss: 0.00006717
Iteration 223/1000 | Loss: 0.00005987
Iteration 224/1000 | Loss: 0.00007509
Iteration 225/1000 | Loss: 0.00038314
Iteration 226/1000 | Loss: 0.00018220
Iteration 227/1000 | Loss: 0.00014931
Iteration 228/1000 | Loss: 0.00011926
Iteration 229/1000 | Loss: 0.00008508
Iteration 230/1000 | Loss: 0.00007064
Iteration 231/1000 | Loss: 0.00006937
Iteration 232/1000 | Loss: 0.00024610
Iteration 233/1000 | Loss: 0.00036293
Iteration 234/1000 | Loss: 0.00037388
Iteration 235/1000 | Loss: 0.00028806
Iteration 236/1000 | Loss: 0.00009466
Iteration 237/1000 | Loss: 0.00006540
Iteration 238/1000 | Loss: 0.00007923
Iteration 239/1000 | Loss: 0.00006116
Iteration 240/1000 | Loss: 0.00005520
Iteration 241/1000 | Loss: 0.00005204
Iteration 242/1000 | Loss: 0.00022686
Iteration 243/1000 | Loss: 0.00005477
Iteration 244/1000 | Loss: 0.00005142
Iteration 245/1000 | Loss: 0.00022895
Iteration 246/1000 | Loss: 0.00020659
Iteration 247/1000 | Loss: 0.00005650
Iteration 248/1000 | Loss: 0.00005215
Iteration 249/1000 | Loss: 0.00004977
Iteration 250/1000 | Loss: 0.00024817
Iteration 251/1000 | Loss: 0.00031903
Iteration 252/1000 | Loss: 0.00014110
Iteration 253/1000 | Loss: 0.00009721
Iteration 254/1000 | Loss: 0.00009787
Iteration 255/1000 | Loss: 0.00005523
Iteration 256/1000 | Loss: 0.00015713
Iteration 257/1000 | Loss: 0.00005155
Iteration 258/1000 | Loss: 0.00016082
Iteration 259/1000 | Loss: 0.00013675
Iteration 260/1000 | Loss: 0.00005246
Iteration 261/1000 | Loss: 0.00015038
Iteration 262/1000 | Loss: 0.00018637
Iteration 263/1000 | Loss: 0.00006698
Iteration 264/1000 | Loss: 0.00021942
Iteration 265/1000 | Loss: 0.00007953
Iteration 266/1000 | Loss: 0.00010571
Iteration 267/1000 | Loss: 0.00005326
Iteration 268/1000 | Loss: 0.00005044
Iteration 269/1000 | Loss: 0.00004925
Iteration 270/1000 | Loss: 0.00020173
Iteration 271/1000 | Loss: 0.00015763
Iteration 272/1000 | Loss: 0.00019397
Iteration 273/1000 | Loss: 0.00045081
Iteration 274/1000 | Loss: 0.00077749
Iteration 275/1000 | Loss: 0.00024523
Iteration 276/1000 | Loss: 0.00022152
Iteration 277/1000 | Loss: 0.00006850
Iteration 278/1000 | Loss: 0.00005857
Iteration 279/1000 | Loss: 0.00005211
Iteration 280/1000 | Loss: 0.00005019
Iteration 281/1000 | Loss: 0.00004838
Iteration 282/1000 | Loss: 0.00022206
Iteration 283/1000 | Loss: 0.00005161
Iteration 284/1000 | Loss: 0.00011097
Iteration 285/1000 | Loss: 0.00004677
Iteration 286/1000 | Loss: 0.00004506
Iteration 287/1000 | Loss: 0.00004408
Iteration 288/1000 | Loss: 0.00004337
Iteration 289/1000 | Loss: 0.00013505
Iteration 290/1000 | Loss: 0.00022816
Iteration 291/1000 | Loss: 0.00005404
Iteration 292/1000 | Loss: 0.00005007
Iteration 293/1000 | Loss: 0.00004817
Iteration 294/1000 | Loss: 0.00027944
Iteration 295/1000 | Loss: 0.00005029
Iteration 296/1000 | Loss: 0.00004545
Iteration 297/1000 | Loss: 0.00004439
Iteration 298/1000 | Loss: 0.00004384
Iteration 299/1000 | Loss: 0.00004356
Iteration 300/1000 | Loss: 0.00004333
Iteration 301/1000 | Loss: 0.00004299
Iteration 302/1000 | Loss: 0.00022013
Iteration 303/1000 | Loss: 0.00021170
Iteration 304/1000 | Loss: 0.00005966
Iteration 305/1000 | Loss: 0.00005069
Iteration 306/1000 | Loss: 0.00004790
Iteration 307/1000 | Loss: 0.00004570
Iteration 308/1000 | Loss: 0.00004492
Iteration 309/1000 | Loss: 0.00004392
Iteration 310/1000 | Loss: 0.00023808
Iteration 311/1000 | Loss: 0.00021113
Iteration 312/1000 | Loss: 0.00020344
Iteration 313/1000 | Loss: 0.00019219
Iteration 314/1000 | Loss: 0.00024846
Iteration 315/1000 | Loss: 0.00019167
Iteration 316/1000 | Loss: 0.00023418
Iteration 317/1000 | Loss: 0.00019891
Iteration 318/1000 | Loss: 0.00005672
Iteration 319/1000 | Loss: 0.00005159
Iteration 320/1000 | Loss: 0.00004775
Iteration 321/1000 | Loss: 0.00004586
Iteration 322/1000 | Loss: 0.00004439
Iteration 323/1000 | Loss: 0.00004370
Iteration 324/1000 | Loss: 0.00004888
Iteration 325/1000 | Loss: 0.00004643
Iteration 326/1000 | Loss: 0.00005407
Iteration 327/1000 | Loss: 0.00004615
Iteration 328/1000 | Loss: 0.00004493
Iteration 329/1000 | Loss: 0.00004779
Iteration 330/1000 | Loss: 0.00004493
Iteration 331/1000 | Loss: 0.00004391
Iteration 332/1000 | Loss: 0.00004286
Iteration 333/1000 | Loss: 0.00004252
Iteration 334/1000 | Loss: 0.00004231
Iteration 335/1000 | Loss: 0.00004216
Iteration 336/1000 | Loss: 0.00004195
Iteration 337/1000 | Loss: 0.00004959
Iteration 338/1000 | Loss: 0.00014550
Iteration 339/1000 | Loss: 0.00010700
Iteration 340/1000 | Loss: 0.00004297
Iteration 341/1000 | Loss: 0.00004876
Iteration 342/1000 | Loss: 0.00013660
Iteration 343/1000 | Loss: 0.00004863
Iteration 344/1000 | Loss: 0.00004352
Iteration 345/1000 | Loss: 0.00004157
Iteration 346/1000 | Loss: 0.00004076
Iteration 347/1000 | Loss: 0.00004035
Iteration 348/1000 | Loss: 0.00004011
Iteration 349/1000 | Loss: 0.00003987
Iteration 350/1000 | Loss: 0.00003969
Iteration 351/1000 | Loss: 0.00003946
Iteration 352/1000 | Loss: 0.00014085
Iteration 353/1000 | Loss: 0.00004398
Iteration 354/1000 | Loss: 0.00011559
Iteration 355/1000 | Loss: 0.00004334
Iteration 356/1000 | Loss: 0.00019280
Iteration 357/1000 | Loss: 0.00019843
Iteration 358/1000 | Loss: 0.00016912
Iteration 359/1000 | Loss: 0.00017452
Iteration 360/1000 | Loss: 0.00004168
Iteration 361/1000 | Loss: 0.00003975
Iteration 362/1000 | Loss: 0.00017699
Iteration 363/1000 | Loss: 0.00004300
Iteration 364/1000 | Loss: 0.00004141
Iteration 365/1000 | Loss: 0.00004036
Iteration 366/1000 | Loss: 0.00003994
Iteration 367/1000 | Loss: 0.00003983
Iteration 368/1000 | Loss: 0.00003978
Iteration 369/1000 | Loss: 0.00017031
Iteration 370/1000 | Loss: 0.00007596
Iteration 371/1000 | Loss: 0.00018752
Iteration 372/1000 | Loss: 0.00010229
Iteration 373/1000 | Loss: 0.00015045
Iteration 374/1000 | Loss: 0.00004906
Iteration 375/1000 | Loss: 0.00014571
Iteration 376/1000 | Loss: 0.00007679
Iteration 377/1000 | Loss: 0.00004698
Iteration 378/1000 | Loss: 0.00004042
Iteration 379/1000 | Loss: 0.00011548
Iteration 380/1000 | Loss: 0.00004165
Iteration 381/1000 | Loss: 0.00004060
Iteration 382/1000 | Loss: 0.00003995
Iteration 383/1000 | Loss: 0.00003911
Iteration 384/1000 | Loss: 0.00003875
Iteration 385/1000 | Loss: 0.00003844
Iteration 386/1000 | Loss: 0.00003841
Iteration 387/1000 | Loss: 0.00003832
Iteration 388/1000 | Loss: 0.00003830
Iteration 389/1000 | Loss: 0.00003828
Iteration 390/1000 | Loss: 0.00003828
Iteration 391/1000 | Loss: 0.00003827
Iteration 392/1000 | Loss: 0.00003826
Iteration 393/1000 | Loss: 0.00003826
Iteration 394/1000 | Loss: 0.00003826
Iteration 395/1000 | Loss: 0.00003825
Iteration 396/1000 | Loss: 0.00003825
Iteration 397/1000 | Loss: 0.00003824
Iteration 398/1000 | Loss: 0.00003824
Iteration 399/1000 | Loss: 0.00003824
Iteration 400/1000 | Loss: 0.00003823
Iteration 401/1000 | Loss: 0.00003821
Iteration 402/1000 | Loss: 0.00003820
Iteration 403/1000 | Loss: 0.00003818
Iteration 404/1000 | Loss: 0.00003818
Iteration 405/1000 | Loss: 0.00003817
Iteration 406/1000 | Loss: 0.00003817
Iteration 407/1000 | Loss: 0.00003817
Iteration 408/1000 | Loss: 0.00003816
Iteration 409/1000 | Loss: 0.00003816
Iteration 410/1000 | Loss: 0.00003816
Iteration 411/1000 | Loss: 0.00003816
Iteration 412/1000 | Loss: 0.00003816
Iteration 413/1000 | Loss: 0.00003816
Iteration 414/1000 | Loss: 0.00003815
Iteration 415/1000 | Loss: 0.00003814
Iteration 416/1000 | Loss: 0.00003814
Iteration 417/1000 | Loss: 0.00003814
Iteration 418/1000 | Loss: 0.00003813
Iteration 419/1000 | Loss: 0.00003813
Iteration 420/1000 | Loss: 0.00003812
Iteration 421/1000 | Loss: 0.00003812
Iteration 422/1000 | Loss: 0.00003812
Iteration 423/1000 | Loss: 0.00003811
Iteration 424/1000 | Loss: 0.00003811
Iteration 425/1000 | Loss: 0.00003810
Iteration 426/1000 | Loss: 0.00003810
Iteration 427/1000 | Loss: 0.00003810
Iteration 428/1000 | Loss: 0.00003809
Iteration 429/1000 | Loss: 0.00003809
Iteration 430/1000 | Loss: 0.00003809
Iteration 431/1000 | Loss: 0.00003809
Iteration 432/1000 | Loss: 0.00003808
Iteration 433/1000 | Loss: 0.00003807
Iteration 434/1000 | Loss: 0.00003807
Iteration 435/1000 | Loss: 0.00003806
Iteration 436/1000 | Loss: 0.00003806
Iteration 437/1000 | Loss: 0.00003806
Iteration 438/1000 | Loss: 0.00003806
Iteration 439/1000 | Loss: 0.00003805
Iteration 440/1000 | Loss: 0.00003805
Iteration 441/1000 | Loss: 0.00003805
Iteration 442/1000 | Loss: 0.00003805
Iteration 443/1000 | Loss: 0.00003805
Iteration 444/1000 | Loss: 0.00003805
Iteration 445/1000 | Loss: 0.00003805
Iteration 446/1000 | Loss: 0.00003804
Iteration 447/1000 | Loss: 0.00003804
Iteration 448/1000 | Loss: 0.00003804
Iteration 449/1000 | Loss: 0.00003804
Iteration 450/1000 | Loss: 0.00003804
Iteration 451/1000 | Loss: 0.00003804
Iteration 452/1000 | Loss: 0.00003804
Iteration 453/1000 | Loss: 0.00003803
Iteration 454/1000 | Loss: 0.00003803
Iteration 455/1000 | Loss: 0.00003803
Iteration 456/1000 | Loss: 0.00003803
Iteration 457/1000 | Loss: 0.00003803
Iteration 458/1000 | Loss: 0.00003803
Iteration 459/1000 | Loss: 0.00003803
Iteration 460/1000 | Loss: 0.00003803
Iteration 461/1000 | Loss: 0.00003803
Iteration 462/1000 | Loss: 0.00003803
Iteration 463/1000 | Loss: 0.00003803
Iteration 464/1000 | Loss: 0.00003803
Iteration 465/1000 | Loss: 0.00003802
Iteration 466/1000 | Loss: 0.00003802
Iteration 467/1000 | Loss: 0.00003802
Iteration 468/1000 | Loss: 0.00003802
Iteration 469/1000 | Loss: 0.00003802
Iteration 470/1000 | Loss: 0.00003801
Iteration 471/1000 | Loss: 0.00003801
Iteration 472/1000 | Loss: 0.00003801
Iteration 473/1000 | Loss: 0.00003800
Iteration 474/1000 | Loss: 0.00003800
Iteration 475/1000 | Loss: 0.00003800
Iteration 476/1000 | Loss: 0.00003800
Iteration 477/1000 | Loss: 0.00003800
Iteration 478/1000 | Loss: 0.00003800
Iteration 479/1000 | Loss: 0.00003800
Iteration 480/1000 | Loss: 0.00003800
Iteration 481/1000 | Loss: 0.00003800
Iteration 482/1000 | Loss: 0.00003800
Iteration 483/1000 | Loss: 0.00003799
Iteration 484/1000 | Loss: 0.00003799
Iteration 485/1000 | Loss: 0.00003799
Iteration 486/1000 | Loss: 0.00003799
Iteration 487/1000 | Loss: 0.00003799
Iteration 488/1000 | Loss: 0.00003799
Iteration 489/1000 | Loss: 0.00003799
Iteration 490/1000 | Loss: 0.00003799
Iteration 491/1000 | Loss: 0.00003799
Iteration 492/1000 | Loss: 0.00003799
Iteration 493/1000 | Loss: 0.00003799
Iteration 494/1000 | Loss: 0.00003799
Iteration 495/1000 | Loss: 0.00003799
Iteration 496/1000 | Loss: 0.00003799
Iteration 497/1000 | Loss: 0.00003799
Iteration 498/1000 | Loss: 0.00003799
Iteration 499/1000 | Loss: 0.00003799
Iteration 500/1000 | Loss: 0.00003799
Iteration 501/1000 | Loss: 0.00003799
Iteration 502/1000 | Loss: 0.00003799
Iteration 503/1000 | Loss: 0.00003799
Iteration 504/1000 | Loss: 0.00003799
Iteration 505/1000 | Loss: 0.00003799
Iteration 506/1000 | Loss: 0.00003799
Iteration 507/1000 | Loss: 0.00003799
Iteration 508/1000 | Loss: 0.00003799
Iteration 509/1000 | Loss: 0.00003799
Iteration 510/1000 | Loss: 0.00003799
Iteration 511/1000 | Loss: 0.00003799
Iteration 512/1000 | Loss: 0.00003799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 512. Stopping optimization.
Last 5 losses: [3.798790567088872e-05, 3.798790567088872e-05, 3.798790567088872e-05, 3.798790567088872e-05, 3.798790567088872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.798790567088872e-05

Optimization complete. Final v2v error: 4.374165058135986 mm

Highest mean error: 10.560712814331055 mm for frame 81

Lowest mean error: 3.714712619781494 mm for frame 18

Saving results

Total time: 673.7178432941437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792596
Iteration 2/25 | Loss: 0.00138581
Iteration 3/25 | Loss: 0.00128020
Iteration 4/25 | Loss: 0.00126395
Iteration 5/25 | Loss: 0.00125982
Iteration 6/25 | Loss: 0.00125958
Iteration 7/25 | Loss: 0.00125958
Iteration 8/25 | Loss: 0.00125958
Iteration 9/25 | Loss: 0.00125958
Iteration 10/25 | Loss: 0.00125958
Iteration 11/25 | Loss: 0.00125958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001259577926248312, 0.001259577926248312, 0.001259577926248312, 0.001259577926248312, 0.001259577926248312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001259577926248312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44009304
Iteration 2/25 | Loss: 0.00068390
Iteration 3/25 | Loss: 0.00068388
Iteration 4/25 | Loss: 0.00068388
Iteration 5/25 | Loss: 0.00068388
Iteration 6/25 | Loss: 0.00068388
Iteration 7/25 | Loss: 0.00068388
Iteration 8/25 | Loss: 0.00068388
Iteration 9/25 | Loss: 0.00068388
Iteration 10/25 | Loss: 0.00068388
Iteration 11/25 | Loss: 0.00068388
Iteration 12/25 | Loss: 0.00068388
Iteration 13/25 | Loss: 0.00068388
Iteration 14/25 | Loss: 0.00068388
Iteration 15/25 | Loss: 0.00068388
Iteration 16/25 | Loss: 0.00068388
Iteration 17/25 | Loss: 0.00068387
Iteration 18/25 | Loss: 0.00068387
Iteration 19/25 | Loss: 0.00068387
Iteration 20/25 | Loss: 0.00068387
Iteration 21/25 | Loss: 0.00068387
Iteration 22/25 | Loss: 0.00068387
Iteration 23/25 | Loss: 0.00068387
Iteration 24/25 | Loss: 0.00068387
Iteration 25/25 | Loss: 0.00068387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068387
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002669
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002246
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00001999
Iteration 9/1000 | Loss: 0.00001969
Iteration 10/1000 | Loss: 0.00001931
Iteration 11/1000 | Loss: 0.00001906
Iteration 12/1000 | Loss: 0.00001903
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001882
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001872
Iteration 18/1000 | Loss: 0.00001871
Iteration 19/1000 | Loss: 0.00001861
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001856
Iteration 24/1000 | Loss: 0.00001855
Iteration 25/1000 | Loss: 0.00001855
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001852
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001852
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001852
Iteration 33/1000 | Loss: 0.00001852
Iteration 34/1000 | Loss: 0.00001852
Iteration 35/1000 | Loss: 0.00001852
Iteration 36/1000 | Loss: 0.00001851
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00001851
Iteration 39/1000 | Loss: 0.00001850
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00001847
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001846
Iteration 44/1000 | Loss: 0.00001843
Iteration 45/1000 | Loss: 0.00001834
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001829
Iteration 48/1000 | Loss: 0.00001828
Iteration 49/1000 | Loss: 0.00001826
Iteration 50/1000 | Loss: 0.00001826
Iteration 51/1000 | Loss: 0.00001825
Iteration 52/1000 | Loss: 0.00001825
Iteration 53/1000 | Loss: 0.00001825
Iteration 54/1000 | Loss: 0.00001825
Iteration 55/1000 | Loss: 0.00001825
Iteration 56/1000 | Loss: 0.00001825
Iteration 57/1000 | Loss: 0.00001825
Iteration 58/1000 | Loss: 0.00001824
Iteration 59/1000 | Loss: 0.00001824
Iteration 60/1000 | Loss: 0.00001824
Iteration 61/1000 | Loss: 0.00001824
Iteration 62/1000 | Loss: 0.00001824
Iteration 63/1000 | Loss: 0.00001824
Iteration 64/1000 | Loss: 0.00001823
Iteration 65/1000 | Loss: 0.00001823
Iteration 66/1000 | Loss: 0.00001822
Iteration 67/1000 | Loss: 0.00001822
Iteration 68/1000 | Loss: 0.00001822
Iteration 69/1000 | Loss: 0.00001822
Iteration 70/1000 | Loss: 0.00001822
Iteration 71/1000 | Loss: 0.00001821
Iteration 72/1000 | Loss: 0.00001821
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001820
Iteration 75/1000 | Loss: 0.00001819
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001815
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001815
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001814
Iteration 92/1000 | Loss: 0.00001814
Iteration 93/1000 | Loss: 0.00001814
Iteration 94/1000 | Loss: 0.00001813
Iteration 95/1000 | Loss: 0.00001813
Iteration 96/1000 | Loss: 0.00001813
Iteration 97/1000 | Loss: 0.00001813
Iteration 98/1000 | Loss: 0.00001813
Iteration 99/1000 | Loss: 0.00001813
Iteration 100/1000 | Loss: 0.00001813
Iteration 101/1000 | Loss: 0.00001813
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001812
Iteration 108/1000 | Loss: 0.00001812
Iteration 109/1000 | Loss: 0.00001812
Iteration 110/1000 | Loss: 0.00001812
Iteration 111/1000 | Loss: 0.00001811
Iteration 112/1000 | Loss: 0.00001811
Iteration 113/1000 | Loss: 0.00001811
Iteration 114/1000 | Loss: 0.00001811
Iteration 115/1000 | Loss: 0.00001811
Iteration 116/1000 | Loss: 0.00001811
Iteration 117/1000 | Loss: 0.00001811
Iteration 118/1000 | Loss: 0.00001811
Iteration 119/1000 | Loss: 0.00001811
Iteration 120/1000 | Loss: 0.00001811
Iteration 121/1000 | Loss: 0.00001811
Iteration 122/1000 | Loss: 0.00001811
Iteration 123/1000 | Loss: 0.00001811
Iteration 124/1000 | Loss: 0.00001811
Iteration 125/1000 | Loss: 0.00001810
Iteration 126/1000 | Loss: 0.00001810
Iteration 127/1000 | Loss: 0.00001810
Iteration 128/1000 | Loss: 0.00001810
Iteration 129/1000 | Loss: 0.00001810
Iteration 130/1000 | Loss: 0.00001810
Iteration 131/1000 | Loss: 0.00001810
Iteration 132/1000 | Loss: 0.00001810
Iteration 133/1000 | Loss: 0.00001810
Iteration 134/1000 | Loss: 0.00001810
Iteration 135/1000 | Loss: 0.00001810
Iteration 136/1000 | Loss: 0.00001810
Iteration 137/1000 | Loss: 0.00001810
Iteration 138/1000 | Loss: 0.00001810
Iteration 139/1000 | Loss: 0.00001810
Iteration 140/1000 | Loss: 0.00001810
Iteration 141/1000 | Loss: 0.00001810
Iteration 142/1000 | Loss: 0.00001810
Iteration 143/1000 | Loss: 0.00001810
Iteration 144/1000 | Loss: 0.00001810
Iteration 145/1000 | Loss: 0.00001810
Iteration 146/1000 | Loss: 0.00001810
Iteration 147/1000 | Loss: 0.00001810
Iteration 148/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.8098104192176834e-05, 1.8098104192176834e-05, 1.8098104192176834e-05, 1.8098104192176834e-05, 1.8098104192176834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8098104192176834e-05

Optimization complete. Final v2v error: 3.560094118118286 mm

Highest mean error: 3.7966883182525635 mm for frame 64

Lowest mean error: 3.3319907188415527 mm for frame 33

Saving results

Total time: 41.51439714431763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786390
Iteration 2/25 | Loss: 0.00153806
Iteration 3/25 | Loss: 0.00132185
Iteration 4/25 | Loss: 0.00129473
Iteration 5/25 | Loss: 0.00129407
Iteration 6/25 | Loss: 0.00128097
Iteration 7/25 | Loss: 0.00127974
Iteration 8/25 | Loss: 0.00127922
Iteration 9/25 | Loss: 0.00127921
Iteration 10/25 | Loss: 0.00127920
Iteration 11/25 | Loss: 0.00127920
Iteration 12/25 | Loss: 0.00127920
Iteration 13/25 | Loss: 0.00127920
Iteration 14/25 | Loss: 0.00127920
Iteration 15/25 | Loss: 0.00127920
Iteration 16/25 | Loss: 0.00127920
Iteration 17/25 | Loss: 0.00127920
Iteration 18/25 | Loss: 0.00127920
Iteration 19/25 | Loss: 0.00127920
Iteration 20/25 | Loss: 0.00127920
Iteration 21/25 | Loss: 0.00127919
Iteration 22/25 | Loss: 0.00127919
Iteration 23/25 | Loss: 0.00127919
Iteration 24/25 | Loss: 0.00127919
Iteration 25/25 | Loss: 0.00127919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26448870
Iteration 2/25 | Loss: 0.00087233
Iteration 3/25 | Loss: 0.00087233
Iteration 4/25 | Loss: 0.00087232
Iteration 5/25 | Loss: 0.00087232
Iteration 6/25 | Loss: 0.00087232
Iteration 7/25 | Loss: 0.00087232
Iteration 8/25 | Loss: 0.00087232
Iteration 9/25 | Loss: 0.00087232
Iteration 10/25 | Loss: 0.00087232
Iteration 11/25 | Loss: 0.00087232
Iteration 12/25 | Loss: 0.00087232
Iteration 13/25 | Loss: 0.00087232
Iteration 14/25 | Loss: 0.00087232
Iteration 15/25 | Loss: 0.00087232
Iteration 16/25 | Loss: 0.00087232
Iteration 17/25 | Loss: 0.00087232
Iteration 18/25 | Loss: 0.00087232
Iteration 19/25 | Loss: 0.00087232
Iteration 20/25 | Loss: 0.00087232
Iteration 21/25 | Loss: 0.00087232
Iteration 22/25 | Loss: 0.00087232
Iteration 23/25 | Loss: 0.00087232
Iteration 24/25 | Loss: 0.00087232
Iteration 25/25 | Loss: 0.00087232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087232
Iteration 2/1000 | Loss: 0.00002905
Iteration 3/1000 | Loss: 0.00002378
Iteration 4/1000 | Loss: 0.00002212
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002056
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00002576
Iteration 10/1000 | Loss: 0.00001915
Iteration 11/1000 | Loss: 0.00001909
Iteration 12/1000 | Loss: 0.00001893
Iteration 13/1000 | Loss: 0.00001884
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00002130
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001870
Iteration 19/1000 | Loss: 0.00001870
Iteration 20/1000 | Loss: 0.00002069
Iteration 21/1000 | Loss: 0.00001863
Iteration 22/1000 | Loss: 0.00001863
Iteration 23/1000 | Loss: 0.00001863
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001862
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001860
Iteration 33/1000 | Loss: 0.00001860
Iteration 34/1000 | Loss: 0.00001858
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001857
Iteration 38/1000 | Loss: 0.00001856
Iteration 39/1000 | Loss: 0.00001856
Iteration 40/1000 | Loss: 0.00001854
Iteration 41/1000 | Loss: 0.00001854
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001853
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001852
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001851
Iteration 49/1000 | Loss: 0.00001851
Iteration 50/1000 | Loss: 0.00001851
Iteration 51/1000 | Loss: 0.00001850
Iteration 52/1000 | Loss: 0.00001850
Iteration 53/1000 | Loss: 0.00001850
Iteration 54/1000 | Loss: 0.00001850
Iteration 55/1000 | Loss: 0.00001850
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001849
Iteration 58/1000 | Loss: 0.00001848
Iteration 59/1000 | Loss: 0.00001848
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001847
Iteration 62/1000 | Loss: 0.00001847
Iteration 63/1000 | Loss: 0.00001847
Iteration 64/1000 | Loss: 0.00001847
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001847
Iteration 67/1000 | Loss: 0.00001847
Iteration 68/1000 | Loss: 0.00001846
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001844
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001842
Iteration 87/1000 | Loss: 0.00001842
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001840
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001839
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001836
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001836
Iteration 121/1000 | Loss: 0.00001836
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001836
Iteration 124/1000 | Loss: 0.00001836
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001835
Iteration 130/1000 | Loss: 0.00001835
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001834
Iteration 134/1000 | Loss: 0.00001834
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001834
Iteration 139/1000 | Loss: 0.00001834
Iteration 140/1000 | Loss: 0.00001834
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001833
Iteration 146/1000 | Loss: 0.00001833
Iteration 147/1000 | Loss: 0.00001833
Iteration 148/1000 | Loss: 0.00001833
Iteration 149/1000 | Loss: 0.00001833
Iteration 150/1000 | Loss: 0.00001832
Iteration 151/1000 | Loss: 0.00001832
Iteration 152/1000 | Loss: 0.00001832
Iteration 153/1000 | Loss: 0.00001832
Iteration 154/1000 | Loss: 0.00001832
Iteration 155/1000 | Loss: 0.00001832
Iteration 156/1000 | Loss: 0.00001832
Iteration 157/1000 | Loss: 0.00001832
Iteration 158/1000 | Loss: 0.00001832
Iteration 159/1000 | Loss: 0.00001832
Iteration 160/1000 | Loss: 0.00001832
Iteration 161/1000 | Loss: 0.00001832
Iteration 162/1000 | Loss: 0.00001832
Iteration 163/1000 | Loss: 0.00001831
Iteration 164/1000 | Loss: 0.00001831
Iteration 165/1000 | Loss: 0.00001831
Iteration 166/1000 | Loss: 0.00001831
Iteration 167/1000 | Loss: 0.00001831
Iteration 168/1000 | Loss: 0.00001831
Iteration 169/1000 | Loss: 0.00001830
Iteration 170/1000 | Loss: 0.00001830
Iteration 171/1000 | Loss: 0.00001830
Iteration 172/1000 | Loss: 0.00001830
Iteration 173/1000 | Loss: 0.00001830
Iteration 174/1000 | Loss: 0.00001830
Iteration 175/1000 | Loss: 0.00001830
Iteration 176/1000 | Loss: 0.00001830
Iteration 177/1000 | Loss: 0.00001830
Iteration 178/1000 | Loss: 0.00001829
Iteration 179/1000 | Loss: 0.00001829
Iteration 180/1000 | Loss: 0.00001829
Iteration 181/1000 | Loss: 0.00001829
Iteration 182/1000 | Loss: 0.00001829
Iteration 183/1000 | Loss: 0.00001829
Iteration 184/1000 | Loss: 0.00001829
Iteration 185/1000 | Loss: 0.00001829
Iteration 186/1000 | Loss: 0.00001829
Iteration 187/1000 | Loss: 0.00001829
Iteration 188/1000 | Loss: 0.00001829
Iteration 189/1000 | Loss: 0.00001829
Iteration 190/1000 | Loss: 0.00001829
Iteration 191/1000 | Loss: 0.00001829
Iteration 192/1000 | Loss: 0.00001829
Iteration 193/1000 | Loss: 0.00001829
Iteration 194/1000 | Loss: 0.00001829
Iteration 195/1000 | Loss: 0.00001829
Iteration 196/1000 | Loss: 0.00001829
Iteration 197/1000 | Loss: 0.00001829
Iteration 198/1000 | Loss: 0.00001829
Iteration 199/1000 | Loss: 0.00001829
Iteration 200/1000 | Loss: 0.00001829
Iteration 201/1000 | Loss: 0.00001829
Iteration 202/1000 | Loss: 0.00001829
Iteration 203/1000 | Loss: 0.00001829
Iteration 204/1000 | Loss: 0.00001829
Iteration 205/1000 | Loss: 0.00001829
Iteration 206/1000 | Loss: 0.00001829
Iteration 207/1000 | Loss: 0.00001829
Iteration 208/1000 | Loss: 0.00001829
Iteration 209/1000 | Loss: 0.00001829
Iteration 210/1000 | Loss: 0.00001829
Iteration 211/1000 | Loss: 0.00001829
Iteration 212/1000 | Loss: 0.00001829
Iteration 213/1000 | Loss: 0.00001829
Iteration 214/1000 | Loss: 0.00001829
Iteration 215/1000 | Loss: 0.00001829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.8294334950041957e-05, 1.8294334950041957e-05, 1.8294334950041957e-05, 1.8294334950041957e-05, 1.8294334950041957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8294334950041957e-05

Optimization complete. Final v2v error: 3.517024278640747 mm

Highest mean error: 5.5934929847717285 mm for frame 48

Lowest mean error: 3.1651031970977783 mm for frame 130

Saving results

Total time: 54.19190287590027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829818
Iteration 2/25 | Loss: 0.00146193
Iteration 3/25 | Loss: 0.00134208
Iteration 4/25 | Loss: 0.00132906
Iteration 5/25 | Loss: 0.00132610
Iteration 6/25 | Loss: 0.00132610
Iteration 7/25 | Loss: 0.00132610
Iteration 8/25 | Loss: 0.00132610
Iteration 9/25 | Loss: 0.00132610
Iteration 10/25 | Loss: 0.00132610
Iteration 11/25 | Loss: 0.00132610
Iteration 12/25 | Loss: 0.00132610
Iteration 13/25 | Loss: 0.00132610
Iteration 14/25 | Loss: 0.00132610
Iteration 15/25 | Loss: 0.00132610
Iteration 16/25 | Loss: 0.00132610
Iteration 17/25 | Loss: 0.00132610
Iteration 18/25 | Loss: 0.00132610
Iteration 19/25 | Loss: 0.00132610
Iteration 20/25 | Loss: 0.00132610
Iteration 21/25 | Loss: 0.00132610
Iteration 22/25 | Loss: 0.00132610
Iteration 23/25 | Loss: 0.00132610
Iteration 24/25 | Loss: 0.00132610
Iteration 25/25 | Loss: 0.00132610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42948306
Iteration 2/25 | Loss: 0.00077542
Iteration 3/25 | Loss: 0.00077536
Iteration 4/25 | Loss: 0.00077536
Iteration 5/25 | Loss: 0.00077536
Iteration 6/25 | Loss: 0.00077536
Iteration 7/25 | Loss: 0.00077536
Iteration 8/25 | Loss: 0.00077536
Iteration 9/25 | Loss: 0.00077536
Iteration 10/25 | Loss: 0.00077536
Iteration 11/25 | Loss: 0.00077536
Iteration 12/25 | Loss: 0.00077536
Iteration 13/25 | Loss: 0.00077536
Iteration 14/25 | Loss: 0.00077536
Iteration 15/25 | Loss: 0.00077536
Iteration 16/25 | Loss: 0.00077536
Iteration 17/25 | Loss: 0.00077536
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007753594545647502, 0.0007753594545647502, 0.0007753594545647502, 0.0007753594545647502, 0.0007753594545647502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007753594545647502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077536
Iteration 2/1000 | Loss: 0.00004705
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002431
Iteration 6/1000 | Loss: 0.00002315
Iteration 7/1000 | Loss: 0.00002217
Iteration 8/1000 | Loss: 0.00002170
Iteration 9/1000 | Loss: 0.00002131
Iteration 10/1000 | Loss: 0.00002095
Iteration 11/1000 | Loss: 0.00002071
Iteration 12/1000 | Loss: 0.00002050
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002026
Iteration 16/1000 | Loss: 0.00002022
Iteration 17/1000 | Loss: 0.00002021
Iteration 18/1000 | Loss: 0.00002020
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002016
Iteration 22/1000 | Loss: 0.00002016
Iteration 23/1000 | Loss: 0.00002010
Iteration 24/1000 | Loss: 0.00002009
Iteration 25/1000 | Loss: 0.00002009
Iteration 26/1000 | Loss: 0.00002008
Iteration 27/1000 | Loss: 0.00002007
Iteration 28/1000 | Loss: 0.00002005
Iteration 29/1000 | Loss: 0.00002005
Iteration 30/1000 | Loss: 0.00002003
Iteration 31/1000 | Loss: 0.00002002
Iteration 32/1000 | Loss: 0.00002001
Iteration 33/1000 | Loss: 0.00002001
Iteration 34/1000 | Loss: 0.00002001
Iteration 35/1000 | Loss: 0.00002001
Iteration 36/1000 | Loss: 0.00002000
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00001999
Iteration 39/1000 | Loss: 0.00001999
Iteration 40/1000 | Loss: 0.00001999
Iteration 41/1000 | Loss: 0.00001999
Iteration 42/1000 | Loss: 0.00001998
Iteration 43/1000 | Loss: 0.00001998
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001996
Iteration 47/1000 | Loss: 0.00001996
Iteration 48/1000 | Loss: 0.00001996
Iteration 49/1000 | Loss: 0.00001995
Iteration 50/1000 | Loss: 0.00001995
Iteration 51/1000 | Loss: 0.00001994
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001992
Iteration 55/1000 | Loss: 0.00001992
Iteration 56/1000 | Loss: 0.00001992
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001991
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001990
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001987
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001983
Iteration 81/1000 | Loss: 0.00001983
Iteration 82/1000 | Loss: 0.00001983
Iteration 83/1000 | Loss: 0.00001983
Iteration 84/1000 | Loss: 0.00001983
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001982
Iteration 90/1000 | Loss: 0.00001981
Iteration 91/1000 | Loss: 0.00001981
Iteration 92/1000 | Loss: 0.00001981
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001981
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00001981
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001980
Iteration 100/1000 | Loss: 0.00001980
Iteration 101/1000 | Loss: 0.00001980
Iteration 102/1000 | Loss: 0.00001980
Iteration 103/1000 | Loss: 0.00001980
Iteration 104/1000 | Loss: 0.00001979
Iteration 105/1000 | Loss: 0.00001979
Iteration 106/1000 | Loss: 0.00001978
Iteration 107/1000 | Loss: 0.00001978
Iteration 108/1000 | Loss: 0.00001978
Iteration 109/1000 | Loss: 0.00001978
Iteration 110/1000 | Loss: 0.00001977
Iteration 111/1000 | Loss: 0.00001977
Iteration 112/1000 | Loss: 0.00001977
Iteration 113/1000 | Loss: 0.00001977
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001976
Iteration 118/1000 | Loss: 0.00001976
Iteration 119/1000 | Loss: 0.00001975
Iteration 120/1000 | Loss: 0.00001975
Iteration 121/1000 | Loss: 0.00001975
Iteration 122/1000 | Loss: 0.00001975
Iteration 123/1000 | Loss: 0.00001975
Iteration 124/1000 | Loss: 0.00001974
Iteration 125/1000 | Loss: 0.00001974
Iteration 126/1000 | Loss: 0.00001974
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001974
Iteration 129/1000 | Loss: 0.00001974
Iteration 130/1000 | Loss: 0.00001974
Iteration 131/1000 | Loss: 0.00001974
Iteration 132/1000 | Loss: 0.00001973
Iteration 133/1000 | Loss: 0.00001973
Iteration 134/1000 | Loss: 0.00001973
Iteration 135/1000 | Loss: 0.00001973
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00001973
Iteration 138/1000 | Loss: 0.00001973
Iteration 139/1000 | Loss: 0.00001973
Iteration 140/1000 | Loss: 0.00001973
Iteration 141/1000 | Loss: 0.00001972
Iteration 142/1000 | Loss: 0.00001972
Iteration 143/1000 | Loss: 0.00001972
Iteration 144/1000 | Loss: 0.00001972
Iteration 145/1000 | Loss: 0.00001971
Iteration 146/1000 | Loss: 0.00001971
Iteration 147/1000 | Loss: 0.00001971
Iteration 148/1000 | Loss: 0.00001971
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001971
Iteration 154/1000 | Loss: 0.00001971
Iteration 155/1000 | Loss: 0.00001971
Iteration 156/1000 | Loss: 0.00001971
Iteration 157/1000 | Loss: 0.00001971
Iteration 158/1000 | Loss: 0.00001971
Iteration 159/1000 | Loss: 0.00001971
Iteration 160/1000 | Loss: 0.00001971
Iteration 161/1000 | Loss: 0.00001971
Iteration 162/1000 | Loss: 0.00001971
Iteration 163/1000 | Loss: 0.00001971
Iteration 164/1000 | Loss: 0.00001971
Iteration 165/1000 | Loss: 0.00001971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.971006531675812e-05, 1.971006531675812e-05, 1.971006531675812e-05, 1.971006531675812e-05, 1.971006531675812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.971006531675812e-05

Optimization complete. Final v2v error: 3.724968671798706 mm

Highest mean error: 4.470610618591309 mm for frame 0

Lowest mean error: 3.3212828636169434 mm for frame 208

Saving results

Total time: 45.76697659492493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791709
Iteration 2/25 | Loss: 0.00129905
Iteration 3/25 | Loss: 0.00122550
Iteration 4/25 | Loss: 0.00121864
Iteration 5/25 | Loss: 0.00121661
Iteration 6/25 | Loss: 0.00121661
Iteration 7/25 | Loss: 0.00121661
Iteration 8/25 | Loss: 0.00121661
Iteration 9/25 | Loss: 0.00121661
Iteration 10/25 | Loss: 0.00121661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012166117085143924, 0.0012166117085143924, 0.0012166117085143924, 0.0012166117085143924, 0.0012166117085143924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012166117085143924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47586834
Iteration 2/25 | Loss: 0.00070890
Iteration 3/25 | Loss: 0.00070890
Iteration 4/25 | Loss: 0.00070890
Iteration 5/25 | Loss: 0.00070890
Iteration 6/25 | Loss: 0.00070890
Iteration 7/25 | Loss: 0.00070890
Iteration 8/25 | Loss: 0.00070890
Iteration 9/25 | Loss: 0.00070890
Iteration 10/25 | Loss: 0.00070890
Iteration 11/25 | Loss: 0.00070890
Iteration 12/25 | Loss: 0.00070890
Iteration 13/25 | Loss: 0.00070890
Iteration 14/25 | Loss: 0.00070890
Iteration 15/25 | Loss: 0.00070890
Iteration 16/25 | Loss: 0.00070890
Iteration 17/25 | Loss: 0.00070890
Iteration 18/25 | Loss: 0.00070890
Iteration 19/25 | Loss: 0.00070890
Iteration 20/25 | Loss: 0.00070890
Iteration 21/25 | Loss: 0.00070890
Iteration 22/25 | Loss: 0.00070890
Iteration 23/25 | Loss: 0.00070890
Iteration 24/25 | Loss: 0.00070890
Iteration 25/25 | Loss: 0.00070890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070890
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001873
Iteration 4/1000 | Loss: 0.00001626
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001353
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001286
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001274
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001269
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001253
Iteration 43/1000 | Loss: 0.00001253
Iteration 44/1000 | Loss: 0.00001252
Iteration 45/1000 | Loss: 0.00001252
Iteration 46/1000 | Loss: 0.00001252
Iteration 47/1000 | Loss: 0.00001252
Iteration 48/1000 | Loss: 0.00001252
Iteration 49/1000 | Loss: 0.00001252
Iteration 50/1000 | Loss: 0.00001251
Iteration 51/1000 | Loss: 0.00001251
Iteration 52/1000 | Loss: 0.00001251
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001251
Iteration 55/1000 | Loss: 0.00001251
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001250
Iteration 59/1000 | Loss: 0.00001249
Iteration 60/1000 | Loss: 0.00001249
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001245
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001237
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001236
Iteration 101/1000 | Loss: 0.00001236
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001233
Iteration 114/1000 | Loss: 0.00001233
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001233
Iteration 117/1000 | Loss: 0.00001233
Iteration 118/1000 | Loss: 0.00001233
Iteration 119/1000 | Loss: 0.00001233
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001231
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001230
Iteration 132/1000 | Loss: 0.00001230
Iteration 133/1000 | Loss: 0.00001230
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001228
Iteration 139/1000 | Loss: 0.00001228
Iteration 140/1000 | Loss: 0.00001227
Iteration 141/1000 | Loss: 0.00001227
Iteration 142/1000 | Loss: 0.00001227
Iteration 143/1000 | Loss: 0.00001226
Iteration 144/1000 | Loss: 0.00001226
Iteration 145/1000 | Loss: 0.00001226
Iteration 146/1000 | Loss: 0.00001226
Iteration 147/1000 | Loss: 0.00001226
Iteration 148/1000 | Loss: 0.00001226
Iteration 149/1000 | Loss: 0.00001226
Iteration 150/1000 | Loss: 0.00001226
Iteration 151/1000 | Loss: 0.00001225
Iteration 152/1000 | Loss: 0.00001225
Iteration 153/1000 | Loss: 0.00001225
Iteration 154/1000 | Loss: 0.00001224
Iteration 155/1000 | Loss: 0.00001224
Iteration 156/1000 | Loss: 0.00001224
Iteration 157/1000 | Loss: 0.00001224
Iteration 158/1000 | Loss: 0.00001224
Iteration 159/1000 | Loss: 0.00001224
Iteration 160/1000 | Loss: 0.00001224
Iteration 161/1000 | Loss: 0.00001223
Iteration 162/1000 | Loss: 0.00001223
Iteration 163/1000 | Loss: 0.00001223
Iteration 164/1000 | Loss: 0.00001223
Iteration 165/1000 | Loss: 0.00001223
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001222
Iteration 169/1000 | Loss: 0.00001222
Iteration 170/1000 | Loss: 0.00001222
Iteration 171/1000 | Loss: 0.00001222
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001221
Iteration 178/1000 | Loss: 0.00001221
Iteration 179/1000 | Loss: 0.00001221
Iteration 180/1000 | Loss: 0.00001220
Iteration 181/1000 | Loss: 0.00001220
Iteration 182/1000 | Loss: 0.00001220
Iteration 183/1000 | Loss: 0.00001220
Iteration 184/1000 | Loss: 0.00001220
Iteration 185/1000 | Loss: 0.00001219
Iteration 186/1000 | Loss: 0.00001219
Iteration 187/1000 | Loss: 0.00001219
Iteration 188/1000 | Loss: 0.00001219
Iteration 189/1000 | Loss: 0.00001219
Iteration 190/1000 | Loss: 0.00001219
Iteration 191/1000 | Loss: 0.00001219
Iteration 192/1000 | Loss: 0.00001219
Iteration 193/1000 | Loss: 0.00001219
Iteration 194/1000 | Loss: 0.00001219
Iteration 195/1000 | Loss: 0.00001219
Iteration 196/1000 | Loss: 0.00001219
Iteration 197/1000 | Loss: 0.00001219
Iteration 198/1000 | Loss: 0.00001219
Iteration 199/1000 | Loss: 0.00001219
Iteration 200/1000 | Loss: 0.00001219
Iteration 201/1000 | Loss: 0.00001218
Iteration 202/1000 | Loss: 0.00001218
Iteration 203/1000 | Loss: 0.00001218
Iteration 204/1000 | Loss: 0.00001218
Iteration 205/1000 | Loss: 0.00001218
Iteration 206/1000 | Loss: 0.00001217
Iteration 207/1000 | Loss: 0.00001217
Iteration 208/1000 | Loss: 0.00001217
Iteration 209/1000 | Loss: 0.00001217
Iteration 210/1000 | Loss: 0.00001217
Iteration 211/1000 | Loss: 0.00001217
Iteration 212/1000 | Loss: 0.00001217
Iteration 213/1000 | Loss: 0.00001217
Iteration 214/1000 | Loss: 0.00001217
Iteration 215/1000 | Loss: 0.00001217
Iteration 216/1000 | Loss: 0.00001216
Iteration 217/1000 | Loss: 0.00001216
Iteration 218/1000 | Loss: 0.00001216
Iteration 219/1000 | Loss: 0.00001216
Iteration 220/1000 | Loss: 0.00001216
Iteration 221/1000 | Loss: 0.00001216
Iteration 222/1000 | Loss: 0.00001216
Iteration 223/1000 | Loss: 0.00001216
Iteration 224/1000 | Loss: 0.00001216
Iteration 225/1000 | Loss: 0.00001216
Iteration 226/1000 | Loss: 0.00001216
Iteration 227/1000 | Loss: 0.00001216
Iteration 228/1000 | Loss: 0.00001215
Iteration 229/1000 | Loss: 0.00001215
Iteration 230/1000 | Loss: 0.00001215
Iteration 231/1000 | Loss: 0.00001215
Iteration 232/1000 | Loss: 0.00001215
Iteration 233/1000 | Loss: 0.00001215
Iteration 234/1000 | Loss: 0.00001215
Iteration 235/1000 | Loss: 0.00001215
Iteration 236/1000 | Loss: 0.00001215
Iteration 237/1000 | Loss: 0.00001215
Iteration 238/1000 | Loss: 0.00001215
Iteration 239/1000 | Loss: 0.00001215
Iteration 240/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.2150093425589148e-05, 1.2150093425589148e-05, 1.2150093425589148e-05, 1.2150093425589148e-05, 1.2150093425589148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2150093425589148e-05

Optimization complete. Final v2v error: 2.9530415534973145 mm

Highest mean error: 3.0944724082946777 mm for frame 59

Lowest mean error: 2.8103747367858887 mm for frame 152

Saving results

Total time: 39.8281147480011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114273
Iteration 2/25 | Loss: 0.01114273
Iteration 3/25 | Loss: 0.01114273
Iteration 4/25 | Loss: 0.01114273
Iteration 5/25 | Loss: 0.01114272
Iteration 6/25 | Loss: 0.01114272
Iteration 7/25 | Loss: 0.01114272
Iteration 8/25 | Loss: 0.01114271
Iteration 9/25 | Loss: 0.01114271
Iteration 10/25 | Loss: 0.01114271
Iteration 11/25 | Loss: 0.01114271
Iteration 12/25 | Loss: 0.01114271
Iteration 13/25 | Loss: 0.01114271
Iteration 14/25 | Loss: 0.01114270
Iteration 15/25 | Loss: 0.01114270
Iteration 16/25 | Loss: 0.01114270
Iteration 17/25 | Loss: 0.01114270
Iteration 18/25 | Loss: 0.01114269
Iteration 19/25 | Loss: 0.01114269
Iteration 20/25 | Loss: 0.01114269
Iteration 21/25 | Loss: 0.01114269
Iteration 22/25 | Loss: 0.01114268
Iteration 23/25 | Loss: 0.01114268
Iteration 24/25 | Loss: 0.01114268
Iteration 25/25 | Loss: 0.01114268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35685015
Iteration 2/25 | Loss: 0.17574991
Iteration 3/25 | Loss: 0.17453334
Iteration 4/25 | Loss: 0.17453332
Iteration 5/25 | Loss: 0.17453329
Iteration 6/25 | Loss: 0.17453329
Iteration 7/25 | Loss: 0.17453329
Iteration 8/25 | Loss: 0.17453329
Iteration 9/25 | Loss: 0.17453325
Iteration 10/25 | Loss: 0.17453325
Iteration 11/25 | Loss: 0.17453325
Iteration 12/25 | Loss: 0.17453325
Iteration 13/25 | Loss: 0.17453325
Iteration 14/25 | Loss: 0.17453325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.17453324794769287, 0.17453324794769287, 0.17453324794769287, 0.17453324794769287, 0.17453324794769287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17453324794769287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17453325
Iteration 2/1000 | Loss: 0.00458523
Iteration 3/1000 | Loss: 0.00235391
Iteration 4/1000 | Loss: 0.00155193
Iteration 5/1000 | Loss: 0.00203043
Iteration 6/1000 | Loss: 0.00215739
Iteration 7/1000 | Loss: 0.00149463
Iteration 8/1000 | Loss: 0.00011412
Iteration 9/1000 | Loss: 0.00007813
Iteration 10/1000 | Loss: 0.00005871
Iteration 11/1000 | Loss: 0.00004673
Iteration 12/1000 | Loss: 0.00003839
Iteration 13/1000 | Loss: 0.00003277
Iteration 14/1000 | Loss: 0.00003061
Iteration 15/1000 | Loss: 0.00002887
Iteration 16/1000 | Loss: 0.00002776
Iteration 17/1000 | Loss: 0.00002666
Iteration 18/1000 | Loss: 0.00002594
Iteration 19/1000 | Loss: 0.00002558
Iteration 20/1000 | Loss: 0.00002536
Iteration 21/1000 | Loss: 0.00002522
Iteration 22/1000 | Loss: 0.00002499
Iteration 23/1000 | Loss: 0.00002482
Iteration 24/1000 | Loss: 0.00002474
Iteration 25/1000 | Loss: 0.00002472
Iteration 26/1000 | Loss: 0.00002471
Iteration 27/1000 | Loss: 0.00002471
Iteration 28/1000 | Loss: 0.00002470
Iteration 29/1000 | Loss: 0.00002470
Iteration 30/1000 | Loss: 0.00002466
Iteration 31/1000 | Loss: 0.00002465
Iteration 32/1000 | Loss: 0.00002465
Iteration 33/1000 | Loss: 0.00002454
Iteration 34/1000 | Loss: 0.00002452
Iteration 35/1000 | Loss: 0.00002450
Iteration 36/1000 | Loss: 0.00002444
Iteration 37/1000 | Loss: 0.00002444
Iteration 38/1000 | Loss: 0.00002443
Iteration 39/1000 | Loss: 0.00002443
Iteration 40/1000 | Loss: 0.00002443
Iteration 41/1000 | Loss: 0.00002442
Iteration 42/1000 | Loss: 0.00002441
Iteration 43/1000 | Loss: 0.00002441
Iteration 44/1000 | Loss: 0.00002440
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002439
Iteration 47/1000 | Loss: 0.00002439
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002437
Iteration 50/1000 | Loss: 0.00002437
Iteration 51/1000 | Loss: 0.00002437
Iteration 52/1000 | Loss: 0.00002436
Iteration 53/1000 | Loss: 0.00002436
Iteration 54/1000 | Loss: 0.00002436
Iteration 55/1000 | Loss: 0.00002436
Iteration 56/1000 | Loss: 0.00002436
Iteration 57/1000 | Loss: 0.00002436
Iteration 58/1000 | Loss: 0.00002435
Iteration 59/1000 | Loss: 0.00002435
Iteration 60/1000 | Loss: 0.00002435
Iteration 61/1000 | Loss: 0.00002435
Iteration 62/1000 | Loss: 0.00002435
Iteration 63/1000 | Loss: 0.00002435
Iteration 64/1000 | Loss: 0.00002435
Iteration 65/1000 | Loss: 0.00002435
Iteration 66/1000 | Loss: 0.00002435
Iteration 67/1000 | Loss: 0.00002435
Iteration 68/1000 | Loss: 0.00002434
Iteration 69/1000 | Loss: 0.00002434
Iteration 70/1000 | Loss: 0.00002434
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00002434
Iteration 73/1000 | Loss: 0.00002434
Iteration 74/1000 | Loss: 0.00002434
Iteration 75/1000 | Loss: 0.00002434
Iteration 76/1000 | Loss: 0.00002433
Iteration 77/1000 | Loss: 0.00002433
Iteration 78/1000 | Loss: 0.00002433
Iteration 79/1000 | Loss: 0.00002433
Iteration 80/1000 | Loss: 0.00002433
Iteration 81/1000 | Loss: 0.00002433
Iteration 82/1000 | Loss: 0.00002433
Iteration 83/1000 | Loss: 0.00002433
Iteration 84/1000 | Loss: 0.00002433
Iteration 85/1000 | Loss: 0.00002433
Iteration 86/1000 | Loss: 0.00002433
Iteration 87/1000 | Loss: 0.00002433
Iteration 88/1000 | Loss: 0.00002433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.4333043256774545e-05, 2.4333043256774545e-05, 2.4333043256774545e-05, 2.4333043256774545e-05, 2.4333043256774545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4333043256774545e-05

Optimization complete. Final v2v error: 3.9721579551696777 mm

Highest mean error: 4.544462203979492 mm for frame 224

Lowest mean error: 3.4658305644989014 mm for frame 19

Saving results

Total time: 54.49458146095276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411754
Iteration 2/25 | Loss: 0.00132843
Iteration 3/25 | Loss: 0.00125100
Iteration 4/25 | Loss: 0.00124667
Iteration 5/25 | Loss: 0.00124559
Iteration 6/25 | Loss: 0.00124559
Iteration 7/25 | Loss: 0.00124559
Iteration 8/25 | Loss: 0.00124559
Iteration 9/25 | Loss: 0.00124559
Iteration 10/25 | Loss: 0.00124559
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00124558643437922, 0.00124558643437922, 0.00124558643437922, 0.00124558643437922, 0.00124558643437922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00124558643437922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77296066
Iteration 2/25 | Loss: 0.00074506
Iteration 3/25 | Loss: 0.00074506
Iteration 4/25 | Loss: 0.00074506
Iteration 5/25 | Loss: 0.00074506
Iteration 6/25 | Loss: 0.00074506
Iteration 7/25 | Loss: 0.00074506
Iteration 8/25 | Loss: 0.00074506
Iteration 9/25 | Loss: 0.00074506
Iteration 10/25 | Loss: 0.00074506
Iteration 11/25 | Loss: 0.00074506
Iteration 12/25 | Loss: 0.00074506
Iteration 13/25 | Loss: 0.00074506
Iteration 14/25 | Loss: 0.00074506
Iteration 15/25 | Loss: 0.00074506
Iteration 16/25 | Loss: 0.00074506
Iteration 17/25 | Loss: 0.00074506
Iteration 18/25 | Loss: 0.00074506
Iteration 19/25 | Loss: 0.00074506
Iteration 20/25 | Loss: 0.00074506
Iteration 21/25 | Loss: 0.00074506
Iteration 22/25 | Loss: 0.00074506
Iteration 23/25 | Loss: 0.00074506
Iteration 24/25 | Loss: 0.00074506
Iteration 25/25 | Loss: 0.00074506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074506
Iteration 2/1000 | Loss: 0.00003142
Iteration 3/1000 | Loss: 0.00001938
Iteration 4/1000 | Loss: 0.00001749
Iteration 5/1000 | Loss: 0.00001621
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001483
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001372
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001327
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001326
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001325
Iteration 23/1000 | Loss: 0.00001325
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001324
Iteration 26/1000 | Loss: 0.00001323
Iteration 27/1000 | Loss: 0.00001323
Iteration 28/1000 | Loss: 0.00001322
Iteration 29/1000 | Loss: 0.00001322
Iteration 30/1000 | Loss: 0.00001320
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001319
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001318
Iteration 36/1000 | Loss: 0.00001318
Iteration 37/1000 | Loss: 0.00001316
Iteration 38/1000 | Loss: 0.00001314
Iteration 39/1000 | Loss: 0.00001314
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001311
Iteration 45/1000 | Loss: 0.00001311
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001309
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001309
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001306
Iteration 63/1000 | Loss: 0.00001306
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001303
Iteration 71/1000 | Loss: 0.00001303
Iteration 72/1000 | Loss: 0.00001303
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001302
Iteration 75/1000 | Loss: 0.00001302
Iteration 76/1000 | Loss: 0.00001302
Iteration 77/1000 | Loss: 0.00001302
Iteration 78/1000 | Loss: 0.00001302
Iteration 79/1000 | Loss: 0.00001301
Iteration 80/1000 | Loss: 0.00001301
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001295
Iteration 106/1000 | Loss: 0.00001295
Iteration 107/1000 | Loss: 0.00001295
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001293
Iteration 116/1000 | Loss: 0.00001293
Iteration 117/1000 | Loss: 0.00001293
Iteration 118/1000 | Loss: 0.00001292
Iteration 119/1000 | Loss: 0.00001292
Iteration 120/1000 | Loss: 0.00001292
Iteration 121/1000 | Loss: 0.00001292
Iteration 122/1000 | Loss: 0.00001292
Iteration 123/1000 | Loss: 0.00001292
Iteration 124/1000 | Loss: 0.00001292
Iteration 125/1000 | Loss: 0.00001292
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001290
Iteration 139/1000 | Loss: 0.00001290
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001289
Iteration 149/1000 | Loss: 0.00001289
Iteration 150/1000 | Loss: 0.00001289
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001288
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001288
Iteration 156/1000 | Loss: 0.00001288
Iteration 157/1000 | Loss: 0.00001288
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001288
Iteration 162/1000 | Loss: 0.00001287
Iteration 163/1000 | Loss: 0.00001287
Iteration 164/1000 | Loss: 0.00001287
Iteration 165/1000 | Loss: 0.00001287
Iteration 166/1000 | Loss: 0.00001287
Iteration 167/1000 | Loss: 0.00001287
Iteration 168/1000 | Loss: 0.00001287
Iteration 169/1000 | Loss: 0.00001286
Iteration 170/1000 | Loss: 0.00001286
Iteration 171/1000 | Loss: 0.00001286
Iteration 172/1000 | Loss: 0.00001286
Iteration 173/1000 | Loss: 0.00001285
Iteration 174/1000 | Loss: 0.00001285
Iteration 175/1000 | Loss: 0.00001285
Iteration 176/1000 | Loss: 0.00001285
Iteration 177/1000 | Loss: 0.00001285
Iteration 178/1000 | Loss: 0.00001285
Iteration 179/1000 | Loss: 0.00001285
Iteration 180/1000 | Loss: 0.00001285
Iteration 181/1000 | Loss: 0.00001285
Iteration 182/1000 | Loss: 0.00001284
Iteration 183/1000 | Loss: 0.00001284
Iteration 184/1000 | Loss: 0.00001284
Iteration 185/1000 | Loss: 0.00001284
Iteration 186/1000 | Loss: 0.00001284
Iteration 187/1000 | Loss: 0.00001284
Iteration 188/1000 | Loss: 0.00001284
Iteration 189/1000 | Loss: 0.00001284
Iteration 190/1000 | Loss: 0.00001283
Iteration 191/1000 | Loss: 0.00001283
Iteration 192/1000 | Loss: 0.00001283
Iteration 193/1000 | Loss: 0.00001283
Iteration 194/1000 | Loss: 0.00001283
Iteration 195/1000 | Loss: 0.00001282
Iteration 196/1000 | Loss: 0.00001282
Iteration 197/1000 | Loss: 0.00001282
Iteration 198/1000 | Loss: 0.00001282
Iteration 199/1000 | Loss: 0.00001281
Iteration 200/1000 | Loss: 0.00001281
Iteration 201/1000 | Loss: 0.00001281
Iteration 202/1000 | Loss: 0.00001281
Iteration 203/1000 | Loss: 0.00001281
Iteration 204/1000 | Loss: 0.00001281
Iteration 205/1000 | Loss: 0.00001281
Iteration 206/1000 | Loss: 0.00001281
Iteration 207/1000 | Loss: 0.00001281
Iteration 208/1000 | Loss: 0.00001280
Iteration 209/1000 | Loss: 0.00001280
Iteration 210/1000 | Loss: 0.00001280
Iteration 211/1000 | Loss: 0.00001280
Iteration 212/1000 | Loss: 0.00001280
Iteration 213/1000 | Loss: 0.00001280
Iteration 214/1000 | Loss: 0.00001279
Iteration 215/1000 | Loss: 0.00001279
Iteration 216/1000 | Loss: 0.00001279
Iteration 217/1000 | Loss: 0.00001279
Iteration 218/1000 | Loss: 0.00001279
Iteration 219/1000 | Loss: 0.00001279
Iteration 220/1000 | Loss: 0.00001279
Iteration 221/1000 | Loss: 0.00001279
Iteration 222/1000 | Loss: 0.00001279
Iteration 223/1000 | Loss: 0.00001279
Iteration 224/1000 | Loss: 0.00001279
Iteration 225/1000 | Loss: 0.00001279
Iteration 226/1000 | Loss: 0.00001279
Iteration 227/1000 | Loss: 0.00001279
Iteration 228/1000 | Loss: 0.00001279
Iteration 229/1000 | Loss: 0.00001279
Iteration 230/1000 | Loss: 0.00001279
Iteration 231/1000 | Loss: 0.00001279
Iteration 232/1000 | Loss: 0.00001279
Iteration 233/1000 | Loss: 0.00001279
Iteration 234/1000 | Loss: 0.00001279
Iteration 235/1000 | Loss: 0.00001279
Iteration 236/1000 | Loss: 0.00001279
Iteration 237/1000 | Loss: 0.00001279
Iteration 238/1000 | Loss: 0.00001279
Iteration 239/1000 | Loss: 0.00001279
Iteration 240/1000 | Loss: 0.00001279
Iteration 241/1000 | Loss: 0.00001279
Iteration 242/1000 | Loss: 0.00001279
Iteration 243/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.2788230378646404e-05, 1.2788230378646404e-05, 1.2788230378646404e-05, 1.2788230378646404e-05, 1.2788230378646404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2788230378646404e-05

Optimization complete. Final v2v error: 3.0594632625579834 mm

Highest mean error: 3.2597577571868896 mm for frame 19

Lowest mean error: 2.9016149044036865 mm for frame 79

Saving results

Total time: 48.60971665382385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795737
Iteration 2/25 | Loss: 0.00132669
Iteration 3/25 | Loss: 0.00123094
Iteration 4/25 | Loss: 0.00122187
Iteration 5/25 | Loss: 0.00121863
Iteration 6/25 | Loss: 0.00121863
Iteration 7/25 | Loss: 0.00121863
Iteration 8/25 | Loss: 0.00121863
Iteration 9/25 | Loss: 0.00121863
Iteration 10/25 | Loss: 0.00121863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012186318635940552, 0.0012186318635940552, 0.0012186318635940552, 0.0012186318635940552, 0.0012186318635940552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012186318635940552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47016549
Iteration 2/25 | Loss: 0.00078475
Iteration 3/25 | Loss: 0.00078475
Iteration 4/25 | Loss: 0.00078475
Iteration 5/25 | Loss: 0.00078475
Iteration 6/25 | Loss: 0.00078475
Iteration 7/25 | Loss: 0.00078475
Iteration 8/25 | Loss: 0.00078475
Iteration 9/25 | Loss: 0.00078475
Iteration 10/25 | Loss: 0.00078475
Iteration 11/25 | Loss: 0.00078475
Iteration 12/25 | Loss: 0.00078475
Iteration 13/25 | Loss: 0.00078475
Iteration 14/25 | Loss: 0.00078475
Iteration 15/25 | Loss: 0.00078475
Iteration 16/25 | Loss: 0.00078475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007847498636692762, 0.0007847498636692762, 0.0007847498636692762, 0.0007847498636692762, 0.0007847498636692762]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007847498636692762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078475
Iteration 2/1000 | Loss: 0.00003370
Iteration 3/1000 | Loss: 0.00002258
Iteration 4/1000 | Loss: 0.00001939
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001666
Iteration 7/1000 | Loss: 0.00001587
Iteration 8/1000 | Loss: 0.00001544
Iteration 9/1000 | Loss: 0.00001518
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001475
Iteration 12/1000 | Loss: 0.00001474
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001462
Iteration 15/1000 | Loss: 0.00001460
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001456
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001452
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001450
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001447
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001445
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001444
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001443
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00001443
Iteration 52/1000 | Loss: 0.00001443
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001442
Iteration 56/1000 | Loss: 0.00001442
Iteration 57/1000 | Loss: 0.00001442
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001441
Iteration 62/1000 | Loss: 0.00001441
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001440
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001432
Iteration 114/1000 | Loss: 0.00001432
Iteration 115/1000 | Loss: 0.00001432
Iteration 116/1000 | Loss: 0.00001432
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001430
Iteration 136/1000 | Loss: 0.00001430
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001429
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00001428
Iteration 144/1000 | Loss: 0.00001428
Iteration 145/1000 | Loss: 0.00001428
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001427
Iteration 149/1000 | Loss: 0.00001427
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001426
Iteration 156/1000 | Loss: 0.00001426
Iteration 157/1000 | Loss: 0.00001426
Iteration 158/1000 | Loss: 0.00001426
Iteration 159/1000 | Loss: 0.00001426
Iteration 160/1000 | Loss: 0.00001426
Iteration 161/1000 | Loss: 0.00001425
Iteration 162/1000 | Loss: 0.00001425
Iteration 163/1000 | Loss: 0.00001425
Iteration 164/1000 | Loss: 0.00001425
Iteration 165/1000 | Loss: 0.00001424
Iteration 166/1000 | Loss: 0.00001424
Iteration 167/1000 | Loss: 0.00001424
Iteration 168/1000 | Loss: 0.00001424
Iteration 169/1000 | Loss: 0.00001424
Iteration 170/1000 | Loss: 0.00001424
Iteration 171/1000 | Loss: 0.00001424
Iteration 172/1000 | Loss: 0.00001424
Iteration 173/1000 | Loss: 0.00001424
Iteration 174/1000 | Loss: 0.00001424
Iteration 175/1000 | Loss: 0.00001424
Iteration 176/1000 | Loss: 0.00001423
Iteration 177/1000 | Loss: 0.00001423
Iteration 178/1000 | Loss: 0.00001423
Iteration 179/1000 | Loss: 0.00001423
Iteration 180/1000 | Loss: 0.00001423
Iteration 181/1000 | Loss: 0.00001423
Iteration 182/1000 | Loss: 0.00001422
Iteration 183/1000 | Loss: 0.00001422
Iteration 184/1000 | Loss: 0.00001422
Iteration 185/1000 | Loss: 0.00001422
Iteration 186/1000 | Loss: 0.00001422
Iteration 187/1000 | Loss: 0.00001422
Iteration 188/1000 | Loss: 0.00001422
Iteration 189/1000 | Loss: 0.00001422
Iteration 190/1000 | Loss: 0.00001421
Iteration 191/1000 | Loss: 0.00001421
Iteration 192/1000 | Loss: 0.00001421
Iteration 193/1000 | Loss: 0.00001421
Iteration 194/1000 | Loss: 0.00001421
Iteration 195/1000 | Loss: 0.00001421
Iteration 196/1000 | Loss: 0.00001421
Iteration 197/1000 | Loss: 0.00001421
Iteration 198/1000 | Loss: 0.00001421
Iteration 199/1000 | Loss: 0.00001421
Iteration 200/1000 | Loss: 0.00001421
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001420
Iteration 206/1000 | Loss: 0.00001420
Iteration 207/1000 | Loss: 0.00001420
Iteration 208/1000 | Loss: 0.00001420
Iteration 209/1000 | Loss: 0.00001420
Iteration 210/1000 | Loss: 0.00001420
Iteration 211/1000 | Loss: 0.00001420
Iteration 212/1000 | Loss: 0.00001420
Iteration 213/1000 | Loss: 0.00001420
Iteration 214/1000 | Loss: 0.00001420
Iteration 215/1000 | Loss: 0.00001420
Iteration 216/1000 | Loss: 0.00001420
Iteration 217/1000 | Loss: 0.00001420
Iteration 218/1000 | Loss: 0.00001419
Iteration 219/1000 | Loss: 0.00001419
Iteration 220/1000 | Loss: 0.00001419
Iteration 221/1000 | Loss: 0.00001419
Iteration 222/1000 | Loss: 0.00001419
Iteration 223/1000 | Loss: 0.00001419
Iteration 224/1000 | Loss: 0.00001419
Iteration 225/1000 | Loss: 0.00001419
Iteration 226/1000 | Loss: 0.00001419
Iteration 227/1000 | Loss: 0.00001419
Iteration 228/1000 | Loss: 0.00001419
Iteration 229/1000 | Loss: 0.00001419
Iteration 230/1000 | Loss: 0.00001419
Iteration 231/1000 | Loss: 0.00001419
Iteration 232/1000 | Loss: 0.00001419
Iteration 233/1000 | Loss: 0.00001419
Iteration 234/1000 | Loss: 0.00001419
Iteration 235/1000 | Loss: 0.00001419
Iteration 236/1000 | Loss: 0.00001419
Iteration 237/1000 | Loss: 0.00001419
Iteration 238/1000 | Loss: 0.00001419
Iteration 239/1000 | Loss: 0.00001419
Iteration 240/1000 | Loss: 0.00001419
Iteration 241/1000 | Loss: 0.00001419
Iteration 242/1000 | Loss: 0.00001419
Iteration 243/1000 | Loss: 0.00001419
Iteration 244/1000 | Loss: 0.00001419
Iteration 245/1000 | Loss: 0.00001419
Iteration 246/1000 | Loss: 0.00001419
Iteration 247/1000 | Loss: 0.00001419
Iteration 248/1000 | Loss: 0.00001419
Iteration 249/1000 | Loss: 0.00001419
Iteration 250/1000 | Loss: 0.00001419
Iteration 251/1000 | Loss: 0.00001419
Iteration 252/1000 | Loss: 0.00001419
Iteration 253/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.4185236068442464e-05, 1.4185236068442464e-05, 1.4185236068442464e-05, 1.4185236068442464e-05, 1.4185236068442464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4185236068442464e-05

Optimization complete. Final v2v error: 3.2192537784576416 mm

Highest mean error: 3.734161138534546 mm for frame 239

Lowest mean error: 2.921693801879883 mm for frame 172

Saving results

Total time: 46.479098081588745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816508
Iteration 2/25 | Loss: 0.00191501
Iteration 3/25 | Loss: 0.00146176
Iteration 4/25 | Loss: 0.00142885
Iteration 5/25 | Loss: 0.00142463
Iteration 6/25 | Loss: 0.00142457
Iteration 7/25 | Loss: 0.00142457
Iteration 8/25 | Loss: 0.00142457
Iteration 9/25 | Loss: 0.00142457
Iteration 10/25 | Loss: 0.00142457
Iteration 11/25 | Loss: 0.00142457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014245703350752592, 0.0014245703350752592, 0.0014245703350752592, 0.0014245703350752592, 0.0014245703350752592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014245703350752592

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86639631
Iteration 2/25 | Loss: 0.00126342
Iteration 3/25 | Loss: 0.00126342
Iteration 4/25 | Loss: 0.00126342
Iteration 5/25 | Loss: 0.00126342
Iteration 6/25 | Loss: 0.00126342
Iteration 7/25 | Loss: 0.00126342
Iteration 8/25 | Loss: 0.00126342
Iteration 9/25 | Loss: 0.00126342
Iteration 10/25 | Loss: 0.00126342
Iteration 11/25 | Loss: 0.00126342
Iteration 12/25 | Loss: 0.00126342
Iteration 13/25 | Loss: 0.00126342
Iteration 14/25 | Loss: 0.00126342
Iteration 15/25 | Loss: 0.00126342
Iteration 16/25 | Loss: 0.00126342
Iteration 17/25 | Loss: 0.00126342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012634170707315207, 0.0012634170707315207, 0.0012634170707315207, 0.0012634170707315207, 0.0012634170707315207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012634170707315207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126342
Iteration 2/1000 | Loss: 0.00004130
Iteration 3/1000 | Loss: 0.00002893
Iteration 4/1000 | Loss: 0.00002633
Iteration 5/1000 | Loss: 0.00002514
Iteration 6/1000 | Loss: 0.00002416
Iteration 7/1000 | Loss: 0.00002376
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00002331
Iteration 10/1000 | Loss: 0.00002322
Iteration 11/1000 | Loss: 0.00002320
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002313
Iteration 14/1000 | Loss: 0.00002311
Iteration 15/1000 | Loss: 0.00002305
Iteration 16/1000 | Loss: 0.00002297
Iteration 17/1000 | Loss: 0.00002297
Iteration 18/1000 | Loss: 0.00002297
Iteration 19/1000 | Loss: 0.00002297
Iteration 20/1000 | Loss: 0.00002297
Iteration 21/1000 | Loss: 0.00002297
Iteration 22/1000 | Loss: 0.00002297
Iteration 23/1000 | Loss: 0.00002297
Iteration 24/1000 | Loss: 0.00002296
Iteration 25/1000 | Loss: 0.00002296
Iteration 26/1000 | Loss: 0.00002295
Iteration 27/1000 | Loss: 0.00002294
Iteration 28/1000 | Loss: 0.00002294
Iteration 29/1000 | Loss: 0.00002294
Iteration 30/1000 | Loss: 0.00002293
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002293
Iteration 33/1000 | Loss: 0.00002293
Iteration 34/1000 | Loss: 0.00002293
Iteration 35/1000 | Loss: 0.00002293
Iteration 36/1000 | Loss: 0.00002293
Iteration 37/1000 | Loss: 0.00002293
Iteration 38/1000 | Loss: 0.00002292
Iteration 39/1000 | Loss: 0.00002292
Iteration 40/1000 | Loss: 0.00002292
Iteration 41/1000 | Loss: 0.00002292
Iteration 42/1000 | Loss: 0.00002292
Iteration 43/1000 | Loss: 0.00002292
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00002290
Iteration 46/1000 | Loss: 0.00002289
Iteration 47/1000 | Loss: 0.00002289
Iteration 48/1000 | Loss: 0.00002289
Iteration 49/1000 | Loss: 0.00002287
Iteration 50/1000 | Loss: 0.00002287
Iteration 51/1000 | Loss: 0.00002287
Iteration 52/1000 | Loss: 0.00002287
Iteration 53/1000 | Loss: 0.00002287
Iteration 54/1000 | Loss: 0.00002286
Iteration 55/1000 | Loss: 0.00002286
Iteration 56/1000 | Loss: 0.00002286
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002286
Iteration 59/1000 | Loss: 0.00002286
Iteration 60/1000 | Loss: 0.00002286
Iteration 61/1000 | Loss: 0.00002286
Iteration 62/1000 | Loss: 0.00002286
Iteration 63/1000 | Loss: 0.00002285
Iteration 64/1000 | Loss: 0.00002285
Iteration 65/1000 | Loss: 0.00002285
Iteration 66/1000 | Loss: 0.00002285
Iteration 67/1000 | Loss: 0.00002284
Iteration 68/1000 | Loss: 0.00002284
Iteration 69/1000 | Loss: 0.00002284
Iteration 70/1000 | Loss: 0.00002284
Iteration 71/1000 | Loss: 0.00002283
Iteration 72/1000 | Loss: 0.00002283
Iteration 73/1000 | Loss: 0.00002283
Iteration 74/1000 | Loss: 0.00002283
Iteration 75/1000 | Loss: 0.00002283
Iteration 76/1000 | Loss: 0.00002282
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002281
Iteration 80/1000 | Loss: 0.00002281
Iteration 81/1000 | Loss: 0.00002281
Iteration 82/1000 | Loss: 0.00002281
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002280
Iteration 91/1000 | Loss: 0.00002280
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002280
Iteration 94/1000 | Loss: 0.00002279
Iteration 95/1000 | Loss: 0.00002279
Iteration 96/1000 | Loss: 0.00002278
Iteration 97/1000 | Loss: 0.00002277
Iteration 98/1000 | Loss: 0.00002277
Iteration 99/1000 | Loss: 0.00002277
Iteration 100/1000 | Loss: 0.00002277
Iteration 101/1000 | Loss: 0.00002277
Iteration 102/1000 | Loss: 0.00002277
Iteration 103/1000 | Loss: 0.00002277
Iteration 104/1000 | Loss: 0.00002276
Iteration 105/1000 | Loss: 0.00002276
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002276
Iteration 108/1000 | Loss: 0.00002276
Iteration 109/1000 | Loss: 0.00002276
Iteration 110/1000 | Loss: 0.00002275
Iteration 111/1000 | Loss: 0.00002275
Iteration 112/1000 | Loss: 0.00002274
Iteration 113/1000 | Loss: 0.00002274
Iteration 114/1000 | Loss: 0.00002274
Iteration 115/1000 | Loss: 0.00002274
Iteration 116/1000 | Loss: 0.00002274
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002274
Iteration 120/1000 | Loss: 0.00002274
Iteration 121/1000 | Loss: 0.00002274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.2744139641872607e-05, 2.2744139641872607e-05, 2.2744139641872607e-05, 2.2744139641872607e-05, 2.2744139641872607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2744139641872607e-05

Optimization complete. Final v2v error: 4.0048627853393555 mm

Highest mean error: 4.257971286773682 mm for frame 169

Lowest mean error: 3.7356553077697754 mm for frame 230

Saving results

Total time: 35.60046410560608
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565427
Iteration 2/25 | Loss: 0.00145009
Iteration 3/25 | Loss: 0.00130331
Iteration 4/25 | Loss: 0.00128462
Iteration 5/25 | Loss: 0.00128127
Iteration 6/25 | Loss: 0.00128127
Iteration 7/25 | Loss: 0.00128127
Iteration 8/25 | Loss: 0.00128127
Iteration 9/25 | Loss: 0.00128127
Iteration 10/25 | Loss: 0.00128127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012812706409022212, 0.0012812706409022212, 0.0012812706409022212, 0.0012812706409022212, 0.0012812706409022212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012812706409022212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39808512
Iteration 2/25 | Loss: 0.00072826
Iteration 3/25 | Loss: 0.00072825
Iteration 4/25 | Loss: 0.00072825
Iteration 5/25 | Loss: 0.00072825
Iteration 6/25 | Loss: 0.00072825
Iteration 7/25 | Loss: 0.00072825
Iteration 8/25 | Loss: 0.00072825
Iteration 9/25 | Loss: 0.00072825
Iteration 10/25 | Loss: 0.00072825
Iteration 11/25 | Loss: 0.00072825
Iteration 12/25 | Loss: 0.00072825
Iteration 13/25 | Loss: 0.00072825
Iteration 14/25 | Loss: 0.00072825
Iteration 15/25 | Loss: 0.00072825
Iteration 16/25 | Loss: 0.00072825
Iteration 17/25 | Loss: 0.00072825
Iteration 18/25 | Loss: 0.00072825
Iteration 19/25 | Loss: 0.00072825
Iteration 20/25 | Loss: 0.00072825
Iteration 21/25 | Loss: 0.00072825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007282493752427399, 0.0007282493752427399, 0.0007282493752427399, 0.0007282493752427399, 0.0007282493752427399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007282493752427399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072825
Iteration 2/1000 | Loss: 0.00003723
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002254
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002086
Iteration 7/1000 | Loss: 0.00002025
Iteration 8/1000 | Loss: 0.00001988
Iteration 9/1000 | Loss: 0.00001953
Iteration 10/1000 | Loss: 0.00001936
Iteration 11/1000 | Loss: 0.00001931
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001917
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00001886
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001883
Iteration 19/1000 | Loss: 0.00001883
Iteration 20/1000 | Loss: 0.00001882
Iteration 21/1000 | Loss: 0.00001882
Iteration 22/1000 | Loss: 0.00001881
Iteration 23/1000 | Loss: 0.00001881
Iteration 24/1000 | Loss: 0.00001880
Iteration 25/1000 | Loss: 0.00001878
Iteration 26/1000 | Loss: 0.00001877
Iteration 27/1000 | Loss: 0.00001877
Iteration 28/1000 | Loss: 0.00001877
Iteration 29/1000 | Loss: 0.00001876
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001875
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001864
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001861
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001859
Iteration 55/1000 | Loss: 0.00001859
Iteration 56/1000 | Loss: 0.00001859
Iteration 57/1000 | Loss: 0.00001859
Iteration 58/1000 | Loss: 0.00001859
Iteration 59/1000 | Loss: 0.00001859
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001857
Iteration 73/1000 | Loss: 0.00001857
Iteration 74/1000 | Loss: 0.00001857
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001857
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001855
Iteration 82/1000 | Loss: 0.00001855
Iteration 83/1000 | Loss: 0.00001855
Iteration 84/1000 | Loss: 0.00001854
Iteration 85/1000 | Loss: 0.00001854
Iteration 86/1000 | Loss: 0.00001854
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001853
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001852
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001851
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001850
Iteration 105/1000 | Loss: 0.00001850
Iteration 106/1000 | Loss: 0.00001850
Iteration 107/1000 | Loss: 0.00001850
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001849
Iteration 110/1000 | Loss: 0.00001849
Iteration 111/1000 | Loss: 0.00001849
Iteration 112/1000 | Loss: 0.00001849
Iteration 113/1000 | Loss: 0.00001849
Iteration 114/1000 | Loss: 0.00001849
Iteration 115/1000 | Loss: 0.00001848
Iteration 116/1000 | Loss: 0.00001848
Iteration 117/1000 | Loss: 0.00001848
Iteration 118/1000 | Loss: 0.00001847
Iteration 119/1000 | Loss: 0.00001847
Iteration 120/1000 | Loss: 0.00001847
Iteration 121/1000 | Loss: 0.00001847
Iteration 122/1000 | Loss: 0.00001847
Iteration 123/1000 | Loss: 0.00001847
Iteration 124/1000 | Loss: 0.00001847
Iteration 125/1000 | Loss: 0.00001847
Iteration 126/1000 | Loss: 0.00001847
Iteration 127/1000 | Loss: 0.00001847
Iteration 128/1000 | Loss: 0.00001847
Iteration 129/1000 | Loss: 0.00001847
Iteration 130/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.8472301235306077e-05, 1.8472301235306077e-05, 1.8472301235306077e-05, 1.8472301235306077e-05, 1.8472301235306077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8472301235306077e-05

Optimization complete. Final v2v error: 3.6056156158447266 mm

Highest mean error: 3.9690444469451904 mm for frame 199

Lowest mean error: 3.242915391921997 mm for frame 109

Saving results

Total time: 39.02061319351196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425467
Iteration 2/25 | Loss: 0.00135054
Iteration 3/25 | Loss: 0.00127438
Iteration 4/25 | Loss: 0.00126795
Iteration 5/25 | Loss: 0.00126707
Iteration 6/25 | Loss: 0.00126707
Iteration 7/25 | Loss: 0.00126707
Iteration 8/25 | Loss: 0.00126707
Iteration 9/25 | Loss: 0.00126707
Iteration 10/25 | Loss: 0.00126707
Iteration 11/25 | Loss: 0.00126707
Iteration 12/25 | Loss: 0.00126707
Iteration 13/25 | Loss: 0.00126707
Iteration 14/25 | Loss: 0.00126707
Iteration 15/25 | Loss: 0.00126707
Iteration 16/25 | Loss: 0.00126707
Iteration 17/25 | Loss: 0.00126704
Iteration 18/25 | Loss: 0.00126704
Iteration 19/25 | Loss: 0.00126704
Iteration 20/25 | Loss: 0.00126704
Iteration 21/25 | Loss: 0.00126704
Iteration 22/25 | Loss: 0.00126704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012670448049902916, 0.0012670448049902916, 0.0012670448049902916, 0.0012670448049902916, 0.0012670448049902916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012670448049902916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47843122
Iteration 2/25 | Loss: 0.00085545
Iteration 3/25 | Loss: 0.00085544
Iteration 4/25 | Loss: 0.00085544
Iteration 5/25 | Loss: 0.00085544
Iteration 6/25 | Loss: 0.00085544
Iteration 7/25 | Loss: 0.00085544
Iteration 8/25 | Loss: 0.00085544
Iteration 9/25 | Loss: 0.00085544
Iteration 10/25 | Loss: 0.00085544
Iteration 11/25 | Loss: 0.00085544
Iteration 12/25 | Loss: 0.00085544
Iteration 13/25 | Loss: 0.00085544
Iteration 14/25 | Loss: 0.00085544
Iteration 15/25 | Loss: 0.00085544
Iteration 16/25 | Loss: 0.00085544
Iteration 17/25 | Loss: 0.00085544
Iteration 18/25 | Loss: 0.00085544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008554404485039413, 0.0008554404485039413, 0.0008554404485039413, 0.0008554404485039413, 0.0008554404485039413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008554404485039413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085544
Iteration 2/1000 | Loss: 0.00003093
Iteration 3/1000 | Loss: 0.00002117
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001819
Iteration 6/1000 | Loss: 0.00001772
Iteration 7/1000 | Loss: 0.00001767
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001746
Iteration 10/1000 | Loss: 0.00001746
Iteration 11/1000 | Loss: 0.00001736
Iteration 12/1000 | Loss: 0.00001736
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001726
Iteration 15/1000 | Loss: 0.00001719
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001707
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001688
Iteration 22/1000 | Loss: 0.00001685
Iteration 23/1000 | Loss: 0.00001682
Iteration 24/1000 | Loss: 0.00001681
Iteration 25/1000 | Loss: 0.00001681
Iteration 26/1000 | Loss: 0.00001681
Iteration 27/1000 | Loss: 0.00001680
Iteration 28/1000 | Loss: 0.00001680
Iteration 29/1000 | Loss: 0.00001680
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00001678
Iteration 34/1000 | Loss: 0.00001677
Iteration 35/1000 | Loss: 0.00001676
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001675
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001674
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001673
Iteration 44/1000 | Loss: 0.00001673
Iteration 45/1000 | Loss: 0.00001673
Iteration 46/1000 | Loss: 0.00001673
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001672
Iteration 50/1000 | Loss: 0.00001672
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001670
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001668
Iteration 57/1000 | Loss: 0.00001668
Iteration 58/1000 | Loss: 0.00001667
Iteration 59/1000 | Loss: 0.00001667
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001666
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001663
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001662
Iteration 68/1000 | Loss: 0.00001660
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001657
Iteration 71/1000 | Loss: 0.00001657
Iteration 72/1000 | Loss: 0.00001656
Iteration 73/1000 | Loss: 0.00001656
Iteration 74/1000 | Loss: 0.00001656
Iteration 75/1000 | Loss: 0.00001656
Iteration 76/1000 | Loss: 0.00001656
Iteration 77/1000 | Loss: 0.00001655
Iteration 78/1000 | Loss: 0.00001655
Iteration 79/1000 | Loss: 0.00001655
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001654
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001653
Iteration 87/1000 | Loss: 0.00001653
Iteration 88/1000 | Loss: 0.00001651
Iteration 89/1000 | Loss: 0.00001651
Iteration 90/1000 | Loss: 0.00001651
Iteration 91/1000 | Loss: 0.00001651
Iteration 92/1000 | Loss: 0.00001651
Iteration 93/1000 | Loss: 0.00001651
Iteration 94/1000 | Loss: 0.00001651
Iteration 95/1000 | Loss: 0.00001651
Iteration 96/1000 | Loss: 0.00001651
Iteration 97/1000 | Loss: 0.00001651
Iteration 98/1000 | Loss: 0.00001650
Iteration 99/1000 | Loss: 0.00001650
Iteration 100/1000 | Loss: 0.00001650
Iteration 101/1000 | Loss: 0.00001650
Iteration 102/1000 | Loss: 0.00001650
Iteration 103/1000 | Loss: 0.00001650
Iteration 104/1000 | Loss: 0.00001649
Iteration 105/1000 | Loss: 0.00001649
Iteration 106/1000 | Loss: 0.00001648
Iteration 107/1000 | Loss: 0.00001648
Iteration 108/1000 | Loss: 0.00001647
Iteration 109/1000 | Loss: 0.00001647
Iteration 110/1000 | Loss: 0.00001646
Iteration 111/1000 | Loss: 0.00001646
Iteration 112/1000 | Loss: 0.00001646
Iteration 113/1000 | Loss: 0.00001646
Iteration 114/1000 | Loss: 0.00001645
Iteration 115/1000 | Loss: 0.00001645
Iteration 116/1000 | Loss: 0.00001645
Iteration 117/1000 | Loss: 0.00001645
Iteration 118/1000 | Loss: 0.00001645
Iteration 119/1000 | Loss: 0.00001645
Iteration 120/1000 | Loss: 0.00001645
Iteration 121/1000 | Loss: 0.00001644
Iteration 122/1000 | Loss: 0.00001644
Iteration 123/1000 | Loss: 0.00001644
Iteration 124/1000 | Loss: 0.00001644
Iteration 125/1000 | Loss: 0.00001643
Iteration 126/1000 | Loss: 0.00001643
Iteration 127/1000 | Loss: 0.00001643
Iteration 128/1000 | Loss: 0.00001643
Iteration 129/1000 | Loss: 0.00001643
Iteration 130/1000 | Loss: 0.00001643
Iteration 131/1000 | Loss: 0.00001643
Iteration 132/1000 | Loss: 0.00001643
Iteration 133/1000 | Loss: 0.00001643
Iteration 134/1000 | Loss: 0.00001643
Iteration 135/1000 | Loss: 0.00001643
Iteration 136/1000 | Loss: 0.00001642
Iteration 137/1000 | Loss: 0.00001642
Iteration 138/1000 | Loss: 0.00001642
Iteration 139/1000 | Loss: 0.00001642
Iteration 140/1000 | Loss: 0.00001642
Iteration 141/1000 | Loss: 0.00001642
Iteration 142/1000 | Loss: 0.00001642
Iteration 143/1000 | Loss: 0.00001642
Iteration 144/1000 | Loss: 0.00001642
Iteration 145/1000 | Loss: 0.00001642
Iteration 146/1000 | Loss: 0.00001642
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001641
Iteration 149/1000 | Loss: 0.00001641
Iteration 150/1000 | Loss: 0.00001641
Iteration 151/1000 | Loss: 0.00001641
Iteration 152/1000 | Loss: 0.00001640
Iteration 153/1000 | Loss: 0.00001640
Iteration 154/1000 | Loss: 0.00001640
Iteration 155/1000 | Loss: 0.00001640
Iteration 156/1000 | Loss: 0.00001639
Iteration 157/1000 | Loss: 0.00001639
Iteration 158/1000 | Loss: 0.00001639
Iteration 159/1000 | Loss: 0.00001639
Iteration 160/1000 | Loss: 0.00001639
Iteration 161/1000 | Loss: 0.00001639
Iteration 162/1000 | Loss: 0.00001639
Iteration 163/1000 | Loss: 0.00001638
Iteration 164/1000 | Loss: 0.00001638
Iteration 165/1000 | Loss: 0.00001638
Iteration 166/1000 | Loss: 0.00001638
Iteration 167/1000 | Loss: 0.00001638
Iteration 168/1000 | Loss: 0.00001637
Iteration 169/1000 | Loss: 0.00001637
Iteration 170/1000 | Loss: 0.00001637
Iteration 171/1000 | Loss: 0.00001637
Iteration 172/1000 | Loss: 0.00001637
Iteration 173/1000 | Loss: 0.00001637
Iteration 174/1000 | Loss: 0.00001637
Iteration 175/1000 | Loss: 0.00001637
Iteration 176/1000 | Loss: 0.00001637
Iteration 177/1000 | Loss: 0.00001637
Iteration 178/1000 | Loss: 0.00001637
Iteration 179/1000 | Loss: 0.00001637
Iteration 180/1000 | Loss: 0.00001637
Iteration 181/1000 | Loss: 0.00001637
Iteration 182/1000 | Loss: 0.00001636
Iteration 183/1000 | Loss: 0.00001636
Iteration 184/1000 | Loss: 0.00001636
Iteration 185/1000 | Loss: 0.00001636
Iteration 186/1000 | Loss: 0.00001636
Iteration 187/1000 | Loss: 0.00001636
Iteration 188/1000 | Loss: 0.00001636
Iteration 189/1000 | Loss: 0.00001636
Iteration 190/1000 | Loss: 0.00001636
Iteration 191/1000 | Loss: 0.00001636
Iteration 192/1000 | Loss: 0.00001636
Iteration 193/1000 | Loss: 0.00001636
Iteration 194/1000 | Loss: 0.00001636
Iteration 195/1000 | Loss: 0.00001635
Iteration 196/1000 | Loss: 0.00001635
Iteration 197/1000 | Loss: 0.00001635
Iteration 198/1000 | Loss: 0.00001635
Iteration 199/1000 | Loss: 0.00001635
Iteration 200/1000 | Loss: 0.00001635
Iteration 201/1000 | Loss: 0.00001635
Iteration 202/1000 | Loss: 0.00001635
Iteration 203/1000 | Loss: 0.00001635
Iteration 204/1000 | Loss: 0.00001635
Iteration 205/1000 | Loss: 0.00001635
Iteration 206/1000 | Loss: 0.00001635
Iteration 207/1000 | Loss: 0.00001635
Iteration 208/1000 | Loss: 0.00001635
Iteration 209/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.6352747479686514e-05, 1.6352747479686514e-05, 1.6352747479686514e-05, 1.6352747479686514e-05, 1.6352747479686514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6352747479686514e-05

Optimization complete. Final v2v error: 3.4104700088500977 mm

Highest mean error: 3.738292932510376 mm for frame 141

Lowest mean error: 3.0975706577301025 mm for frame 202

Saving results

Total time: 38.111555099487305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007051
Iteration 2/25 | Loss: 0.00242390
Iteration 3/25 | Loss: 0.00170662
Iteration 4/25 | Loss: 0.00154137
Iteration 5/25 | Loss: 0.00152472
Iteration 6/25 | Loss: 0.00150873
Iteration 7/25 | Loss: 0.00146411
Iteration 8/25 | Loss: 0.00141042
Iteration 9/25 | Loss: 0.00136549
Iteration 10/25 | Loss: 0.00134111
Iteration 11/25 | Loss: 0.00131666
Iteration 12/25 | Loss: 0.00130948
Iteration 13/25 | Loss: 0.00130266
Iteration 14/25 | Loss: 0.00130426
Iteration 15/25 | Loss: 0.00130291
Iteration 16/25 | Loss: 0.00130261
Iteration 17/25 | Loss: 0.00129778
Iteration 18/25 | Loss: 0.00129661
Iteration 19/25 | Loss: 0.00129727
Iteration 20/25 | Loss: 0.00129202
Iteration 21/25 | Loss: 0.00129060
Iteration 22/25 | Loss: 0.00129396
Iteration 23/25 | Loss: 0.00129257
Iteration 24/25 | Loss: 0.00128866
Iteration 25/25 | Loss: 0.00128744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46310568
Iteration 2/25 | Loss: 0.00109318
Iteration 3/25 | Loss: 0.00094088
Iteration 4/25 | Loss: 0.00094087
Iteration 5/25 | Loss: 0.00094087
Iteration 6/25 | Loss: 0.00094087
Iteration 7/25 | Loss: 0.00094087
Iteration 8/25 | Loss: 0.00094087
Iteration 9/25 | Loss: 0.00094087
Iteration 10/25 | Loss: 0.00094087
Iteration 11/25 | Loss: 0.00094087
Iteration 12/25 | Loss: 0.00094087
Iteration 13/25 | Loss: 0.00094087
Iteration 14/25 | Loss: 0.00094087
Iteration 15/25 | Loss: 0.00094087
Iteration 16/25 | Loss: 0.00094087
Iteration 17/25 | Loss: 0.00094087
Iteration 18/25 | Loss: 0.00094087
Iteration 19/25 | Loss: 0.00094087
Iteration 20/25 | Loss: 0.00094087
Iteration 21/25 | Loss: 0.00094087
Iteration 22/25 | Loss: 0.00094087
Iteration 23/25 | Loss: 0.00094087
Iteration 24/25 | Loss: 0.00094087
Iteration 25/25 | Loss: 0.00094087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094087
Iteration 2/1000 | Loss: 0.00038720
Iteration 3/1000 | Loss: 0.00004532
Iteration 4/1000 | Loss: 0.00003786
Iteration 5/1000 | Loss: 0.00003472
Iteration 6/1000 | Loss: 0.00003240
Iteration 7/1000 | Loss: 0.00003131
Iteration 8/1000 | Loss: 0.00003046
Iteration 9/1000 | Loss: 0.00002995
Iteration 10/1000 | Loss: 0.00023886
Iteration 11/1000 | Loss: 0.00016468
Iteration 12/1000 | Loss: 0.00002949
Iteration 13/1000 | Loss: 0.00024847
Iteration 14/1000 | Loss: 0.00026562
Iteration 15/1000 | Loss: 0.00082964
Iteration 16/1000 | Loss: 0.00055307
Iteration 17/1000 | Loss: 0.00024888
Iteration 18/1000 | Loss: 0.00062633
Iteration 19/1000 | Loss: 0.00053948
Iteration 20/1000 | Loss: 0.00028162
Iteration 21/1000 | Loss: 0.00030981
Iteration 22/1000 | Loss: 0.00010452
Iteration 23/1000 | Loss: 0.00034703
Iteration 24/1000 | Loss: 0.00014363
Iteration 25/1000 | Loss: 0.00035517
Iteration 26/1000 | Loss: 0.00015670
Iteration 27/1000 | Loss: 0.00005686
Iteration 28/1000 | Loss: 0.00033335
Iteration 29/1000 | Loss: 0.00033423
Iteration 30/1000 | Loss: 0.00003554
Iteration 31/1000 | Loss: 0.00003351
Iteration 32/1000 | Loss: 0.00029820
Iteration 33/1000 | Loss: 0.00032083
Iteration 34/1000 | Loss: 0.00019689
Iteration 35/1000 | Loss: 0.00074205
Iteration 36/1000 | Loss: 0.00004534
Iteration 37/1000 | Loss: 0.00003317
Iteration 38/1000 | Loss: 0.00003066
Iteration 39/1000 | Loss: 0.00015830
Iteration 40/1000 | Loss: 0.00008230
Iteration 41/1000 | Loss: 0.00021501
Iteration 42/1000 | Loss: 0.00003559
Iteration 43/1000 | Loss: 0.00005444
Iteration 44/1000 | Loss: 0.00002941
Iteration 45/1000 | Loss: 0.00002734
Iteration 46/1000 | Loss: 0.00008261
Iteration 47/1000 | Loss: 0.00002565
Iteration 48/1000 | Loss: 0.00002485
Iteration 49/1000 | Loss: 0.00002442
Iteration 50/1000 | Loss: 0.00002419
Iteration 51/1000 | Loss: 0.00002399
Iteration 52/1000 | Loss: 0.00002396
Iteration 53/1000 | Loss: 0.00002378
Iteration 54/1000 | Loss: 0.00002373
Iteration 55/1000 | Loss: 0.00002358
Iteration 56/1000 | Loss: 0.00002345
Iteration 57/1000 | Loss: 0.00002342
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002338
Iteration 60/1000 | Loss: 0.00002338
Iteration 61/1000 | Loss: 0.00002338
Iteration 62/1000 | Loss: 0.00002338
Iteration 63/1000 | Loss: 0.00002338
Iteration 64/1000 | Loss: 0.00002338
Iteration 65/1000 | Loss: 0.00002338
Iteration 66/1000 | Loss: 0.00002337
Iteration 67/1000 | Loss: 0.00002337
Iteration 68/1000 | Loss: 0.00002337
Iteration 69/1000 | Loss: 0.00002337
Iteration 70/1000 | Loss: 0.00002337
Iteration 71/1000 | Loss: 0.00002337
Iteration 72/1000 | Loss: 0.00002336
Iteration 73/1000 | Loss: 0.00002336
Iteration 74/1000 | Loss: 0.00002336
Iteration 75/1000 | Loss: 0.00002336
Iteration 76/1000 | Loss: 0.00002336
Iteration 77/1000 | Loss: 0.00002336
Iteration 78/1000 | Loss: 0.00002336
Iteration 79/1000 | Loss: 0.00002336
Iteration 80/1000 | Loss: 0.00002335
Iteration 81/1000 | Loss: 0.00002335
Iteration 82/1000 | Loss: 0.00002335
Iteration 83/1000 | Loss: 0.00002335
Iteration 84/1000 | Loss: 0.00002335
Iteration 85/1000 | Loss: 0.00002335
Iteration 86/1000 | Loss: 0.00002335
Iteration 87/1000 | Loss: 0.00002335
Iteration 88/1000 | Loss: 0.00002335
Iteration 89/1000 | Loss: 0.00002335
Iteration 90/1000 | Loss: 0.00002335
Iteration 91/1000 | Loss: 0.00002335
Iteration 92/1000 | Loss: 0.00002335
Iteration 93/1000 | Loss: 0.00002335
Iteration 94/1000 | Loss: 0.00002335
Iteration 95/1000 | Loss: 0.00002335
Iteration 96/1000 | Loss: 0.00002335
Iteration 97/1000 | Loss: 0.00002335
Iteration 98/1000 | Loss: 0.00002335
Iteration 99/1000 | Loss: 0.00002335
Iteration 100/1000 | Loss: 0.00002335
Iteration 101/1000 | Loss: 0.00002335
Iteration 102/1000 | Loss: 0.00002335
Iteration 103/1000 | Loss: 0.00002335
Iteration 104/1000 | Loss: 0.00002335
Iteration 105/1000 | Loss: 0.00002335
Iteration 106/1000 | Loss: 0.00002335
Iteration 107/1000 | Loss: 0.00002335
Iteration 108/1000 | Loss: 0.00002335
Iteration 109/1000 | Loss: 0.00002335
Iteration 110/1000 | Loss: 0.00002335
Iteration 111/1000 | Loss: 0.00002335
Iteration 112/1000 | Loss: 0.00002335
Iteration 113/1000 | Loss: 0.00002335
Iteration 114/1000 | Loss: 0.00002335
Iteration 115/1000 | Loss: 0.00002335
Iteration 116/1000 | Loss: 0.00002335
Iteration 117/1000 | Loss: 0.00002335
Iteration 118/1000 | Loss: 0.00002335
Iteration 119/1000 | Loss: 0.00002335
Iteration 120/1000 | Loss: 0.00002335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.3346849047811702e-05, 2.3346849047811702e-05, 2.3346849047811702e-05, 2.3346849047811702e-05, 2.3346849047811702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3346849047811702e-05

Optimization complete. Final v2v error: 4.088513374328613 mm

Highest mean error: 4.555875778198242 mm for frame 68

Lowest mean error: 3.4729676246643066 mm for frame 3

Saving results

Total time: 126.3472752571106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772734
Iteration 2/25 | Loss: 0.00167624
Iteration 3/25 | Loss: 0.00138666
Iteration 4/25 | Loss: 0.00134342
Iteration 5/25 | Loss: 0.00135311
Iteration 6/25 | Loss: 0.00134118
Iteration 7/25 | Loss: 0.00133237
Iteration 8/25 | Loss: 0.00133650
Iteration 9/25 | Loss: 0.00132882
Iteration 10/25 | Loss: 0.00132568
Iteration 11/25 | Loss: 0.00132536
Iteration 12/25 | Loss: 0.00132531
Iteration 13/25 | Loss: 0.00132531
Iteration 14/25 | Loss: 0.00132531
Iteration 15/25 | Loss: 0.00132531
Iteration 16/25 | Loss: 0.00132531
Iteration 17/25 | Loss: 0.00132531
Iteration 18/25 | Loss: 0.00132531
Iteration 19/25 | Loss: 0.00132531
Iteration 20/25 | Loss: 0.00132531
Iteration 21/25 | Loss: 0.00132531
Iteration 22/25 | Loss: 0.00132530
Iteration 23/25 | Loss: 0.00132530
Iteration 24/25 | Loss: 0.00132530
Iteration 25/25 | Loss: 0.00132530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61433649
Iteration 2/25 | Loss: 0.00080988
Iteration 3/25 | Loss: 0.00080986
Iteration 4/25 | Loss: 0.00080986
Iteration 5/25 | Loss: 0.00080986
Iteration 6/25 | Loss: 0.00080986
Iteration 7/25 | Loss: 0.00080986
Iteration 8/25 | Loss: 0.00080986
Iteration 9/25 | Loss: 0.00080986
Iteration 10/25 | Loss: 0.00080986
Iteration 11/25 | Loss: 0.00080986
Iteration 12/25 | Loss: 0.00080986
Iteration 13/25 | Loss: 0.00080986
Iteration 14/25 | Loss: 0.00080986
Iteration 15/25 | Loss: 0.00080986
Iteration 16/25 | Loss: 0.00080986
Iteration 17/25 | Loss: 0.00080986
Iteration 18/25 | Loss: 0.00080986
Iteration 19/25 | Loss: 0.00080986
Iteration 20/25 | Loss: 0.00080986
Iteration 21/25 | Loss: 0.00080986
Iteration 22/25 | Loss: 0.00080986
Iteration 23/25 | Loss: 0.00080986
Iteration 24/25 | Loss: 0.00080986
Iteration 25/25 | Loss: 0.00080986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080986
Iteration 2/1000 | Loss: 0.00017128
Iteration 3/1000 | Loss: 0.00015575
Iteration 4/1000 | Loss: 0.00015110
Iteration 5/1000 | Loss: 0.00044686
Iteration 6/1000 | Loss: 0.00002460
Iteration 7/1000 | Loss: 0.00002306
Iteration 8/1000 | Loss: 0.00002226
Iteration 9/1000 | Loss: 0.00002160
Iteration 10/1000 | Loss: 0.00012556
Iteration 11/1000 | Loss: 0.00002120
Iteration 12/1000 | Loss: 0.00002080
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002040
Iteration 15/1000 | Loss: 0.00002033
Iteration 16/1000 | Loss: 0.00002027
Iteration 17/1000 | Loss: 0.00002027
Iteration 18/1000 | Loss: 0.00002027
Iteration 19/1000 | Loss: 0.00002027
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002026
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002018
Iteration 25/1000 | Loss: 0.00002012
Iteration 26/1000 | Loss: 0.00002012
Iteration 27/1000 | Loss: 0.00002012
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002011
Iteration 30/1000 | Loss: 0.00002011
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002009
Iteration 34/1000 | Loss: 0.00002008
Iteration 35/1000 | Loss: 0.00002008
Iteration 36/1000 | Loss: 0.00002008
Iteration 37/1000 | Loss: 0.00002008
Iteration 38/1000 | Loss: 0.00002008
Iteration 39/1000 | Loss: 0.00002007
Iteration 40/1000 | Loss: 0.00002007
Iteration 41/1000 | Loss: 0.00002007
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002006
Iteration 44/1000 | Loss: 0.00002006
Iteration 45/1000 | Loss: 0.00002006
Iteration 46/1000 | Loss: 0.00002006
Iteration 47/1000 | Loss: 0.00002006
Iteration 48/1000 | Loss: 0.00002006
Iteration 49/1000 | Loss: 0.00002006
Iteration 50/1000 | Loss: 0.00002005
Iteration 51/1000 | Loss: 0.00002005
Iteration 52/1000 | Loss: 0.00002005
Iteration 53/1000 | Loss: 0.00002005
Iteration 54/1000 | Loss: 0.00002004
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002004
Iteration 57/1000 | Loss: 0.00002004
Iteration 58/1000 | Loss: 0.00002003
Iteration 59/1000 | Loss: 0.00002003
Iteration 60/1000 | Loss: 0.00002003
Iteration 61/1000 | Loss: 0.00002003
Iteration 62/1000 | Loss: 0.00002002
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00002000
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001999
Iteration 73/1000 | Loss: 0.00001999
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001998
Iteration 84/1000 | Loss: 0.00001998
Iteration 85/1000 | Loss: 0.00001997
Iteration 86/1000 | Loss: 0.00001997
Iteration 87/1000 | Loss: 0.00001997
Iteration 88/1000 | Loss: 0.00001997
Iteration 89/1000 | Loss: 0.00001997
Iteration 90/1000 | Loss: 0.00001997
Iteration 91/1000 | Loss: 0.00001996
Iteration 92/1000 | Loss: 0.00001996
Iteration 93/1000 | Loss: 0.00001996
Iteration 94/1000 | Loss: 0.00001996
Iteration 95/1000 | Loss: 0.00001995
Iteration 96/1000 | Loss: 0.00001995
Iteration 97/1000 | Loss: 0.00001995
Iteration 98/1000 | Loss: 0.00001995
Iteration 99/1000 | Loss: 0.00001995
Iteration 100/1000 | Loss: 0.00001995
Iteration 101/1000 | Loss: 0.00001995
Iteration 102/1000 | Loss: 0.00001994
Iteration 103/1000 | Loss: 0.00001994
Iteration 104/1000 | Loss: 0.00001994
Iteration 105/1000 | Loss: 0.00001994
Iteration 106/1000 | Loss: 0.00001994
Iteration 107/1000 | Loss: 0.00001994
Iteration 108/1000 | Loss: 0.00001994
Iteration 109/1000 | Loss: 0.00001994
Iteration 110/1000 | Loss: 0.00001994
Iteration 111/1000 | Loss: 0.00001994
Iteration 112/1000 | Loss: 0.00001993
Iteration 113/1000 | Loss: 0.00001993
Iteration 114/1000 | Loss: 0.00001993
Iteration 115/1000 | Loss: 0.00001993
Iteration 116/1000 | Loss: 0.00001993
Iteration 117/1000 | Loss: 0.00001992
Iteration 118/1000 | Loss: 0.00001992
Iteration 119/1000 | Loss: 0.00001992
Iteration 120/1000 | Loss: 0.00001992
Iteration 121/1000 | Loss: 0.00001992
Iteration 122/1000 | Loss: 0.00001992
Iteration 123/1000 | Loss: 0.00001992
Iteration 124/1000 | Loss: 0.00001992
Iteration 125/1000 | Loss: 0.00001992
Iteration 126/1000 | Loss: 0.00001992
Iteration 127/1000 | Loss: 0.00001992
Iteration 128/1000 | Loss: 0.00001992
Iteration 129/1000 | Loss: 0.00001992
Iteration 130/1000 | Loss: 0.00001992
Iteration 131/1000 | Loss: 0.00001992
Iteration 132/1000 | Loss: 0.00001992
Iteration 133/1000 | Loss: 0.00001992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.9923929357901216e-05, 1.9923929357901216e-05, 1.9923929357901216e-05, 1.9923929357901216e-05, 1.9923929357901216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9923929357901216e-05

Optimization complete. Final v2v error: 3.7098169326782227 mm

Highest mean error: 3.985245704650879 mm for frame 30

Lowest mean error: 3.434640407562256 mm for frame 174

Saving results

Total time: 57.483598709106445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731701
Iteration 2/25 | Loss: 0.00136445
Iteration 3/25 | Loss: 0.00129653
Iteration 4/25 | Loss: 0.00128450
Iteration 5/25 | Loss: 0.00127932
Iteration 6/25 | Loss: 0.00127763
Iteration 7/25 | Loss: 0.00127755
Iteration 8/25 | Loss: 0.00127755
Iteration 9/25 | Loss: 0.00127755
Iteration 10/25 | Loss: 0.00127755
Iteration 11/25 | Loss: 0.00127755
Iteration 12/25 | Loss: 0.00127755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001277548959478736, 0.001277548959478736, 0.001277548959478736, 0.001277548959478736, 0.001277548959478736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001277548959478736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.58683968
Iteration 2/25 | Loss: 0.00084769
Iteration 3/25 | Loss: 0.00084768
Iteration 4/25 | Loss: 0.00084768
Iteration 5/25 | Loss: 0.00084768
Iteration 6/25 | Loss: 0.00084768
Iteration 7/25 | Loss: 0.00084768
Iteration 8/25 | Loss: 0.00084768
Iteration 9/25 | Loss: 0.00084768
Iteration 10/25 | Loss: 0.00084768
Iteration 11/25 | Loss: 0.00084768
Iteration 12/25 | Loss: 0.00084768
Iteration 13/25 | Loss: 0.00084768
Iteration 14/25 | Loss: 0.00084768
Iteration 15/25 | Loss: 0.00084768
Iteration 16/25 | Loss: 0.00084768
Iteration 17/25 | Loss: 0.00084768
Iteration 18/25 | Loss: 0.00084768
Iteration 19/25 | Loss: 0.00084768
Iteration 20/25 | Loss: 0.00084768
Iteration 21/25 | Loss: 0.00084768
Iteration 22/25 | Loss: 0.00084768
Iteration 23/25 | Loss: 0.00084768
Iteration 24/25 | Loss: 0.00084768
Iteration 25/25 | Loss: 0.00084768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084768
Iteration 2/1000 | Loss: 0.00003848
Iteration 3/1000 | Loss: 0.00002333
Iteration 4/1000 | Loss: 0.00001903
Iteration 5/1000 | Loss: 0.00001760
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001537
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001517
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001480
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001459
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001452
Iteration 22/1000 | Loss: 0.00001443
Iteration 23/1000 | Loss: 0.00001442
Iteration 24/1000 | Loss: 0.00001438
Iteration 25/1000 | Loss: 0.00001437
Iteration 26/1000 | Loss: 0.00001437
Iteration 27/1000 | Loss: 0.00001436
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001434
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001432
Iteration 32/1000 | Loss: 0.00001432
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001431
Iteration 36/1000 | Loss: 0.00001431
Iteration 37/1000 | Loss: 0.00001431
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001431
Iteration 40/1000 | Loss: 0.00001431
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001428
Iteration 44/1000 | Loss: 0.00001428
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001421
Iteration 54/1000 | Loss: 0.00001421
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001420
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001419
Iteration 64/1000 | Loss: 0.00001419
Iteration 65/1000 | Loss: 0.00001419
Iteration 66/1000 | Loss: 0.00001418
Iteration 67/1000 | Loss: 0.00001418
Iteration 68/1000 | Loss: 0.00001418
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001417
Iteration 71/1000 | Loss: 0.00001417
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001416
Iteration 77/1000 | Loss: 0.00001415
Iteration 78/1000 | Loss: 0.00001415
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001414
Iteration 83/1000 | Loss: 0.00001414
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001414
Iteration 88/1000 | Loss: 0.00001414
Iteration 89/1000 | Loss: 0.00001414
Iteration 90/1000 | Loss: 0.00001414
Iteration 91/1000 | Loss: 0.00001414
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001414
Iteration 94/1000 | Loss: 0.00001414
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001414
Iteration 99/1000 | Loss: 0.00001414
Iteration 100/1000 | Loss: 0.00001414
Iteration 101/1000 | Loss: 0.00001414
Iteration 102/1000 | Loss: 0.00001414
Iteration 103/1000 | Loss: 0.00001414
Iteration 104/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4139666745904833e-05, 1.4139666745904833e-05, 1.4139666745904833e-05, 1.4139666745904833e-05, 1.4139666745904833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4139666745904833e-05

Optimization complete. Final v2v error: 3.189648151397705 mm

Highest mean error: 3.4152772426605225 mm for frame 41

Lowest mean error: 2.969533681869507 mm for frame 125

Saving results

Total time: 36.49571490287781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848125
Iteration 2/25 | Loss: 0.00154834
Iteration 3/25 | Loss: 0.00131456
Iteration 4/25 | Loss: 0.00129446
Iteration 5/25 | Loss: 0.00129042
Iteration 6/25 | Loss: 0.00129014
Iteration 7/25 | Loss: 0.00129014
Iteration 8/25 | Loss: 0.00129014
Iteration 9/25 | Loss: 0.00129014
Iteration 10/25 | Loss: 0.00129014
Iteration 11/25 | Loss: 0.00129014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012901414884254336, 0.0012901414884254336, 0.0012901414884254336, 0.0012901414884254336, 0.0012901414884254336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012901414884254336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00364113
Iteration 2/25 | Loss: 0.00058020
Iteration 3/25 | Loss: 0.00058019
Iteration 4/25 | Loss: 0.00058019
Iteration 5/25 | Loss: 0.00058019
Iteration 6/25 | Loss: 0.00058019
Iteration 7/25 | Loss: 0.00058019
Iteration 8/25 | Loss: 0.00058019
Iteration 9/25 | Loss: 0.00058019
Iteration 10/25 | Loss: 0.00058019
Iteration 11/25 | Loss: 0.00058019
Iteration 12/25 | Loss: 0.00058019
Iteration 13/25 | Loss: 0.00058019
Iteration 14/25 | Loss: 0.00058019
Iteration 15/25 | Loss: 0.00058019
Iteration 16/25 | Loss: 0.00058019
Iteration 17/25 | Loss: 0.00058019
Iteration 18/25 | Loss: 0.00058019
Iteration 19/25 | Loss: 0.00058019
Iteration 20/25 | Loss: 0.00058019
Iteration 21/25 | Loss: 0.00058019
Iteration 22/25 | Loss: 0.00058019
Iteration 23/25 | Loss: 0.00058019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005801908555440605, 0.0005801908555440605, 0.0005801908555440605, 0.0005801908555440605, 0.0005801908555440605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005801908555440605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058019
Iteration 2/1000 | Loss: 0.00004146
Iteration 3/1000 | Loss: 0.00003199
Iteration 4/1000 | Loss: 0.00002836
Iteration 5/1000 | Loss: 0.00002683
Iteration 6/1000 | Loss: 0.00002576
Iteration 7/1000 | Loss: 0.00002508
Iteration 8/1000 | Loss: 0.00002465
Iteration 9/1000 | Loss: 0.00002426
Iteration 10/1000 | Loss: 0.00002378
Iteration 11/1000 | Loss: 0.00002351
Iteration 12/1000 | Loss: 0.00002336
Iteration 13/1000 | Loss: 0.00002321
Iteration 14/1000 | Loss: 0.00002305
Iteration 15/1000 | Loss: 0.00002299
Iteration 16/1000 | Loss: 0.00002298
Iteration 17/1000 | Loss: 0.00002296
Iteration 18/1000 | Loss: 0.00002296
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00002294
Iteration 21/1000 | Loss: 0.00002292
Iteration 22/1000 | Loss: 0.00002291
Iteration 23/1000 | Loss: 0.00002291
Iteration 24/1000 | Loss: 0.00002290
Iteration 25/1000 | Loss: 0.00002290
Iteration 26/1000 | Loss: 0.00002290
Iteration 27/1000 | Loss: 0.00002289
Iteration 28/1000 | Loss: 0.00002289
Iteration 29/1000 | Loss: 0.00002289
Iteration 30/1000 | Loss: 0.00002289
Iteration 31/1000 | Loss: 0.00002289
Iteration 32/1000 | Loss: 0.00002288
Iteration 33/1000 | Loss: 0.00002288
Iteration 34/1000 | Loss: 0.00002288
Iteration 35/1000 | Loss: 0.00002288
Iteration 36/1000 | Loss: 0.00002288
Iteration 37/1000 | Loss: 0.00002287
Iteration 38/1000 | Loss: 0.00002287
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002287
Iteration 41/1000 | Loss: 0.00002287
Iteration 42/1000 | Loss: 0.00002287
Iteration 43/1000 | Loss: 0.00002287
Iteration 44/1000 | Loss: 0.00002286
Iteration 45/1000 | Loss: 0.00002286
Iteration 46/1000 | Loss: 0.00002285
Iteration 47/1000 | Loss: 0.00002285
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002284
Iteration 51/1000 | Loss: 0.00002284
Iteration 52/1000 | Loss: 0.00002284
Iteration 53/1000 | Loss: 0.00002284
Iteration 54/1000 | Loss: 0.00002284
Iteration 55/1000 | Loss: 0.00002284
Iteration 56/1000 | Loss: 0.00002284
Iteration 57/1000 | Loss: 0.00002284
Iteration 58/1000 | Loss: 0.00002284
Iteration 59/1000 | Loss: 0.00002283
Iteration 60/1000 | Loss: 0.00002283
Iteration 61/1000 | Loss: 0.00002283
Iteration 62/1000 | Loss: 0.00002283
Iteration 63/1000 | Loss: 0.00002283
Iteration 64/1000 | Loss: 0.00002282
Iteration 65/1000 | Loss: 0.00002282
Iteration 66/1000 | Loss: 0.00002281
Iteration 67/1000 | Loss: 0.00002281
Iteration 68/1000 | Loss: 0.00002281
Iteration 69/1000 | Loss: 0.00002281
Iteration 70/1000 | Loss: 0.00002281
Iteration 71/1000 | Loss: 0.00002281
Iteration 72/1000 | Loss: 0.00002281
Iteration 73/1000 | Loss: 0.00002281
Iteration 74/1000 | Loss: 0.00002281
Iteration 75/1000 | Loss: 0.00002281
Iteration 76/1000 | Loss: 0.00002281
Iteration 77/1000 | Loss: 0.00002281
Iteration 78/1000 | Loss: 0.00002280
Iteration 79/1000 | Loss: 0.00002280
Iteration 80/1000 | Loss: 0.00002280
Iteration 81/1000 | Loss: 0.00002280
Iteration 82/1000 | Loss: 0.00002280
Iteration 83/1000 | Loss: 0.00002280
Iteration 84/1000 | Loss: 0.00002280
Iteration 85/1000 | Loss: 0.00002280
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002280
Iteration 91/1000 | Loss: 0.00002280
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002280
Iteration 94/1000 | Loss: 0.00002280
Iteration 95/1000 | Loss: 0.00002280
Iteration 96/1000 | Loss: 0.00002280
Iteration 97/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.279826730955392e-05, 2.279826730955392e-05, 2.279826730955392e-05, 2.279826730955392e-05, 2.279826730955392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.279826730955392e-05

Optimization complete. Final v2v error: 3.9597465991973877 mm

Highest mean error: 4.074104309082031 mm for frame 43

Lowest mean error: 3.900588274002075 mm for frame 147

Saving results

Total time: 32.35389304161072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536274
Iteration 2/25 | Loss: 0.00138971
Iteration 3/25 | Loss: 0.00129378
Iteration 4/25 | Loss: 0.00128079
Iteration 5/25 | Loss: 0.00127730
Iteration 6/25 | Loss: 0.00127627
Iteration 7/25 | Loss: 0.00127627
Iteration 8/25 | Loss: 0.00127627
Iteration 9/25 | Loss: 0.00127627
Iteration 10/25 | Loss: 0.00127627
Iteration 11/25 | Loss: 0.00127627
Iteration 12/25 | Loss: 0.00127627
Iteration 13/25 | Loss: 0.00127627
Iteration 14/25 | Loss: 0.00127627
Iteration 15/25 | Loss: 0.00127627
Iteration 16/25 | Loss: 0.00127627
Iteration 17/25 | Loss: 0.00127627
Iteration 18/25 | Loss: 0.00127627
Iteration 19/25 | Loss: 0.00127627
Iteration 20/25 | Loss: 0.00127627
Iteration 21/25 | Loss: 0.00127627
Iteration 22/25 | Loss: 0.00127627
Iteration 23/25 | Loss: 0.00127627
Iteration 24/25 | Loss: 0.00127627
Iteration 25/25 | Loss: 0.00127627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.19399023
Iteration 2/25 | Loss: 0.00094883
Iteration 3/25 | Loss: 0.00094883
Iteration 4/25 | Loss: 0.00094883
Iteration 5/25 | Loss: 0.00094883
Iteration 6/25 | Loss: 0.00094883
Iteration 7/25 | Loss: 0.00094882
Iteration 8/25 | Loss: 0.00094882
Iteration 9/25 | Loss: 0.00094882
Iteration 10/25 | Loss: 0.00094882
Iteration 11/25 | Loss: 0.00094882
Iteration 12/25 | Loss: 0.00094882
Iteration 13/25 | Loss: 0.00094882
Iteration 14/25 | Loss: 0.00094882
Iteration 15/25 | Loss: 0.00094882
Iteration 16/25 | Loss: 0.00094882
Iteration 17/25 | Loss: 0.00094882
Iteration 18/25 | Loss: 0.00094882
Iteration 19/25 | Loss: 0.00094882
Iteration 20/25 | Loss: 0.00094882
Iteration 21/25 | Loss: 0.00094882
Iteration 22/25 | Loss: 0.00094882
Iteration 23/25 | Loss: 0.00094882
Iteration 24/25 | Loss: 0.00094882
Iteration 25/25 | Loss: 0.00094882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094882
Iteration 2/1000 | Loss: 0.00002670
Iteration 3/1000 | Loss: 0.00002000
Iteration 4/1000 | Loss: 0.00001817
Iteration 5/1000 | Loss: 0.00001743
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001613
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001581
Iteration 11/1000 | Loss: 0.00001573
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001552
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001543
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001536
Iteration 18/1000 | Loss: 0.00001532
Iteration 19/1000 | Loss: 0.00001531
Iteration 20/1000 | Loss: 0.00001530
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001527
Iteration 24/1000 | Loss: 0.00001527
Iteration 25/1000 | Loss: 0.00001527
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001526
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001525
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001523
Iteration 34/1000 | Loss: 0.00001523
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001523
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001522
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001522
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001521
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001518
Iteration 53/1000 | Loss: 0.00001518
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001517
Iteration 56/1000 | Loss: 0.00001517
Iteration 57/1000 | Loss: 0.00001517
Iteration 58/1000 | Loss: 0.00001517
Iteration 59/1000 | Loss: 0.00001516
Iteration 60/1000 | Loss: 0.00001516
Iteration 61/1000 | Loss: 0.00001516
Iteration 62/1000 | Loss: 0.00001515
Iteration 63/1000 | Loss: 0.00001515
Iteration 64/1000 | Loss: 0.00001515
Iteration 65/1000 | Loss: 0.00001515
Iteration 66/1000 | Loss: 0.00001515
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001514
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001511
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001511
Iteration 88/1000 | Loss: 0.00001511
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001510
Iteration 92/1000 | Loss: 0.00001510
Iteration 93/1000 | Loss: 0.00001510
Iteration 94/1000 | Loss: 0.00001510
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001509
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001509
Iteration 99/1000 | Loss: 0.00001509
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001508
Iteration 104/1000 | Loss: 0.00001508
Iteration 105/1000 | Loss: 0.00001508
Iteration 106/1000 | Loss: 0.00001508
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001507
Iteration 109/1000 | Loss: 0.00001507
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001504
Iteration 117/1000 | Loss: 0.00001504
Iteration 118/1000 | Loss: 0.00001504
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001504
Iteration 121/1000 | Loss: 0.00001504
Iteration 122/1000 | Loss: 0.00001504
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001501
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001501
Iteration 137/1000 | Loss: 0.00001501
Iteration 138/1000 | Loss: 0.00001500
Iteration 139/1000 | Loss: 0.00001500
Iteration 140/1000 | Loss: 0.00001500
Iteration 141/1000 | Loss: 0.00001500
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001499
Iteration 145/1000 | Loss: 0.00001499
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001497
Iteration 157/1000 | Loss: 0.00001497
Iteration 158/1000 | Loss: 0.00001497
Iteration 159/1000 | Loss: 0.00001497
Iteration 160/1000 | Loss: 0.00001497
Iteration 161/1000 | Loss: 0.00001497
Iteration 162/1000 | Loss: 0.00001497
Iteration 163/1000 | Loss: 0.00001497
Iteration 164/1000 | Loss: 0.00001497
Iteration 165/1000 | Loss: 0.00001497
Iteration 166/1000 | Loss: 0.00001497
Iteration 167/1000 | Loss: 0.00001497
Iteration 168/1000 | Loss: 0.00001497
Iteration 169/1000 | Loss: 0.00001496
Iteration 170/1000 | Loss: 0.00001496
Iteration 171/1000 | Loss: 0.00001496
Iteration 172/1000 | Loss: 0.00001496
Iteration 173/1000 | Loss: 0.00001496
Iteration 174/1000 | Loss: 0.00001496
Iteration 175/1000 | Loss: 0.00001496
Iteration 176/1000 | Loss: 0.00001496
Iteration 177/1000 | Loss: 0.00001496
Iteration 178/1000 | Loss: 0.00001496
Iteration 179/1000 | Loss: 0.00001496
Iteration 180/1000 | Loss: 0.00001496
Iteration 181/1000 | Loss: 0.00001496
Iteration 182/1000 | Loss: 0.00001496
Iteration 183/1000 | Loss: 0.00001496
Iteration 184/1000 | Loss: 0.00001496
Iteration 185/1000 | Loss: 0.00001496
Iteration 186/1000 | Loss: 0.00001496
Iteration 187/1000 | Loss: 0.00001496
Iteration 188/1000 | Loss: 0.00001496
Iteration 189/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.495906963100424e-05, 1.495906963100424e-05, 1.495906963100424e-05, 1.495906963100424e-05, 1.495906963100424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.495906963100424e-05

Optimization complete. Final v2v error: 3.2505881786346436 mm

Highest mean error: 3.857428789138794 mm for frame 63

Lowest mean error: 2.972188949584961 mm for frame 0

Saving results

Total time: 40.07003593444824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822055
Iteration 2/25 | Loss: 0.00133207
Iteration 3/25 | Loss: 0.00123673
Iteration 4/25 | Loss: 0.00122691
Iteration 5/25 | Loss: 0.00122499
Iteration 6/25 | Loss: 0.00122466
Iteration 7/25 | Loss: 0.00122466
Iteration 8/25 | Loss: 0.00122466
Iteration 9/25 | Loss: 0.00122466
Iteration 10/25 | Loss: 0.00122466
Iteration 11/25 | Loss: 0.00122466
Iteration 12/25 | Loss: 0.00122466
Iteration 13/25 | Loss: 0.00122466
Iteration 14/25 | Loss: 0.00122466
Iteration 15/25 | Loss: 0.00122466
Iteration 16/25 | Loss: 0.00122466
Iteration 17/25 | Loss: 0.00122466
Iteration 18/25 | Loss: 0.00122466
Iteration 19/25 | Loss: 0.00122466
Iteration 20/25 | Loss: 0.00122466
Iteration 21/25 | Loss: 0.00122466
Iteration 22/25 | Loss: 0.00122466
Iteration 23/25 | Loss: 0.00122466
Iteration 24/25 | Loss: 0.00122466
Iteration 25/25 | Loss: 0.00122466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46129954
Iteration 2/25 | Loss: 0.00074317
Iteration 3/25 | Loss: 0.00074315
Iteration 4/25 | Loss: 0.00074315
Iteration 5/25 | Loss: 0.00074315
Iteration 6/25 | Loss: 0.00074315
Iteration 7/25 | Loss: 0.00074315
Iteration 8/25 | Loss: 0.00074314
Iteration 9/25 | Loss: 0.00074314
Iteration 10/25 | Loss: 0.00074314
Iteration 11/25 | Loss: 0.00074314
Iteration 12/25 | Loss: 0.00074314
Iteration 13/25 | Loss: 0.00074314
Iteration 14/25 | Loss: 0.00074314
Iteration 15/25 | Loss: 0.00074314
Iteration 16/25 | Loss: 0.00074314
Iteration 17/25 | Loss: 0.00074314
Iteration 18/25 | Loss: 0.00074314
Iteration 19/25 | Loss: 0.00074314
Iteration 20/25 | Loss: 0.00074314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007431443664245307, 0.0007431443664245307, 0.0007431443664245307, 0.0007431443664245307, 0.0007431443664245307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007431443664245307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074314
Iteration 2/1000 | Loss: 0.00002239
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001416
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001262
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001206
Iteration 9/1000 | Loss: 0.00001187
Iteration 10/1000 | Loss: 0.00001182
Iteration 11/1000 | Loss: 0.00001182
Iteration 12/1000 | Loss: 0.00001177
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001153
Iteration 17/1000 | Loss: 0.00001152
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001152
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001147
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001147
Iteration 25/1000 | Loss: 0.00001146
Iteration 26/1000 | Loss: 0.00001146
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001137
Iteration 32/1000 | Loss: 0.00001137
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001134
Iteration 37/1000 | Loss: 0.00001130
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001126
Iteration 41/1000 | Loss: 0.00001126
Iteration 42/1000 | Loss: 0.00001123
Iteration 43/1000 | Loss: 0.00001122
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001121
Iteration 48/1000 | Loss: 0.00001121
Iteration 49/1000 | Loss: 0.00001120
Iteration 50/1000 | Loss: 0.00001120
Iteration 51/1000 | Loss: 0.00001119
Iteration 52/1000 | Loss: 0.00001119
Iteration 53/1000 | Loss: 0.00001119
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001118
Iteration 56/1000 | Loss: 0.00001118
Iteration 57/1000 | Loss: 0.00001118
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001116
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001115
Iteration 65/1000 | Loss: 0.00001115
Iteration 66/1000 | Loss: 0.00001115
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001113
Iteration 72/1000 | Loss: 0.00001113
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001113
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001106
Iteration 104/1000 | Loss: 0.00001106
Iteration 105/1000 | Loss: 0.00001105
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001105
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001105
Iteration 110/1000 | Loss: 0.00001105
Iteration 111/1000 | Loss: 0.00001105
Iteration 112/1000 | Loss: 0.00001105
Iteration 113/1000 | Loss: 0.00001105
Iteration 114/1000 | Loss: 0.00001105
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001104
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001103
Iteration 124/1000 | Loss: 0.00001103
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001101
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001101
Iteration 132/1000 | Loss: 0.00001101
Iteration 133/1000 | Loss: 0.00001101
Iteration 134/1000 | Loss: 0.00001101
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001099
Iteration 139/1000 | Loss: 0.00001099
Iteration 140/1000 | Loss: 0.00001099
Iteration 141/1000 | Loss: 0.00001099
Iteration 142/1000 | Loss: 0.00001099
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Iteration 146/1000 | Loss: 0.00001098
Iteration 147/1000 | Loss: 0.00001098
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001098
Iteration 154/1000 | Loss: 0.00001098
Iteration 155/1000 | Loss: 0.00001097
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001097
Iteration 159/1000 | Loss: 0.00001097
Iteration 160/1000 | Loss: 0.00001097
Iteration 161/1000 | Loss: 0.00001096
Iteration 162/1000 | Loss: 0.00001096
Iteration 163/1000 | Loss: 0.00001096
Iteration 164/1000 | Loss: 0.00001096
Iteration 165/1000 | Loss: 0.00001096
Iteration 166/1000 | Loss: 0.00001096
Iteration 167/1000 | Loss: 0.00001096
Iteration 168/1000 | Loss: 0.00001096
Iteration 169/1000 | Loss: 0.00001096
Iteration 170/1000 | Loss: 0.00001096
Iteration 171/1000 | Loss: 0.00001095
Iteration 172/1000 | Loss: 0.00001095
Iteration 173/1000 | Loss: 0.00001095
Iteration 174/1000 | Loss: 0.00001095
Iteration 175/1000 | Loss: 0.00001095
Iteration 176/1000 | Loss: 0.00001095
Iteration 177/1000 | Loss: 0.00001095
Iteration 178/1000 | Loss: 0.00001094
Iteration 179/1000 | Loss: 0.00001094
Iteration 180/1000 | Loss: 0.00001094
Iteration 181/1000 | Loss: 0.00001094
Iteration 182/1000 | Loss: 0.00001094
Iteration 183/1000 | Loss: 0.00001094
Iteration 184/1000 | Loss: 0.00001094
Iteration 185/1000 | Loss: 0.00001094
Iteration 186/1000 | Loss: 0.00001094
Iteration 187/1000 | Loss: 0.00001094
Iteration 188/1000 | Loss: 0.00001094
Iteration 189/1000 | Loss: 0.00001094
Iteration 190/1000 | Loss: 0.00001094
Iteration 191/1000 | Loss: 0.00001094
Iteration 192/1000 | Loss: 0.00001093
Iteration 193/1000 | Loss: 0.00001093
Iteration 194/1000 | Loss: 0.00001093
Iteration 195/1000 | Loss: 0.00001093
Iteration 196/1000 | Loss: 0.00001093
Iteration 197/1000 | Loss: 0.00001093
Iteration 198/1000 | Loss: 0.00001093
Iteration 199/1000 | Loss: 0.00001093
Iteration 200/1000 | Loss: 0.00001093
Iteration 201/1000 | Loss: 0.00001093
Iteration 202/1000 | Loss: 0.00001093
Iteration 203/1000 | Loss: 0.00001093
Iteration 204/1000 | Loss: 0.00001093
Iteration 205/1000 | Loss: 0.00001093
Iteration 206/1000 | Loss: 0.00001093
Iteration 207/1000 | Loss: 0.00001093
Iteration 208/1000 | Loss: 0.00001093
Iteration 209/1000 | Loss: 0.00001093
Iteration 210/1000 | Loss: 0.00001092
Iteration 211/1000 | Loss: 0.00001092
Iteration 212/1000 | Loss: 0.00001092
Iteration 213/1000 | Loss: 0.00001092
Iteration 214/1000 | Loss: 0.00001092
Iteration 215/1000 | Loss: 0.00001092
Iteration 216/1000 | Loss: 0.00001092
Iteration 217/1000 | Loss: 0.00001092
Iteration 218/1000 | Loss: 0.00001092
Iteration 219/1000 | Loss: 0.00001092
Iteration 220/1000 | Loss: 0.00001092
Iteration 221/1000 | Loss: 0.00001092
Iteration 222/1000 | Loss: 0.00001092
Iteration 223/1000 | Loss: 0.00001092
Iteration 224/1000 | Loss: 0.00001092
Iteration 225/1000 | Loss: 0.00001092
Iteration 226/1000 | Loss: 0.00001092
Iteration 227/1000 | Loss: 0.00001092
Iteration 228/1000 | Loss: 0.00001092
Iteration 229/1000 | Loss: 0.00001092
Iteration 230/1000 | Loss: 0.00001092
Iteration 231/1000 | Loss: 0.00001092
Iteration 232/1000 | Loss: 0.00001092
Iteration 233/1000 | Loss: 0.00001092
Iteration 234/1000 | Loss: 0.00001092
Iteration 235/1000 | Loss: 0.00001091
Iteration 236/1000 | Loss: 0.00001091
Iteration 237/1000 | Loss: 0.00001091
Iteration 238/1000 | Loss: 0.00001091
Iteration 239/1000 | Loss: 0.00001091
Iteration 240/1000 | Loss: 0.00001091
Iteration 241/1000 | Loss: 0.00001091
Iteration 242/1000 | Loss: 0.00001091
Iteration 243/1000 | Loss: 0.00001091
Iteration 244/1000 | Loss: 0.00001091
Iteration 245/1000 | Loss: 0.00001091
Iteration 246/1000 | Loss: 0.00001091
Iteration 247/1000 | Loss: 0.00001091
Iteration 248/1000 | Loss: 0.00001091
Iteration 249/1000 | Loss: 0.00001091
Iteration 250/1000 | Loss: 0.00001091
Iteration 251/1000 | Loss: 0.00001091
Iteration 252/1000 | Loss: 0.00001091
Iteration 253/1000 | Loss: 0.00001091
Iteration 254/1000 | Loss: 0.00001091
Iteration 255/1000 | Loss: 0.00001091
Iteration 256/1000 | Loss: 0.00001091
Iteration 257/1000 | Loss: 0.00001091
Iteration 258/1000 | Loss: 0.00001091
Iteration 259/1000 | Loss: 0.00001091
Iteration 260/1000 | Loss: 0.00001091
Iteration 261/1000 | Loss: 0.00001091
Iteration 262/1000 | Loss: 0.00001091
Iteration 263/1000 | Loss: 0.00001091
Iteration 264/1000 | Loss: 0.00001091
Iteration 265/1000 | Loss: 0.00001091
Iteration 266/1000 | Loss: 0.00001091
Iteration 267/1000 | Loss: 0.00001091
Iteration 268/1000 | Loss: 0.00001091
Iteration 269/1000 | Loss: 0.00001091
Iteration 270/1000 | Loss: 0.00001091
Iteration 271/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.0914283848251216e-05, 1.0914283848251216e-05, 1.0914283848251216e-05, 1.0914283848251216e-05, 1.0914283848251216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0914283848251216e-05

Optimization complete. Final v2v error: 2.8107821941375732 mm

Highest mean error: 3.2075979709625244 mm for frame 0

Lowest mean error: 2.6930992603302 mm for frame 96

Saving results

Total time: 40.19664812088013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385651
Iteration 2/25 | Loss: 0.00128992
Iteration 3/25 | Loss: 0.00124717
Iteration 4/25 | Loss: 0.00124219
Iteration 5/25 | Loss: 0.00123889
Iteration 6/25 | Loss: 0.00123868
Iteration 7/25 | Loss: 0.00123868
Iteration 8/25 | Loss: 0.00123868
Iteration 9/25 | Loss: 0.00123868
Iteration 10/25 | Loss: 0.00123868
Iteration 11/25 | Loss: 0.00123868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012386797461658716, 0.0012386797461658716, 0.0012386797461658716, 0.0012386797461658716, 0.0012386797461658716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012386797461658716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61704504
Iteration 2/25 | Loss: 0.00072427
Iteration 3/25 | Loss: 0.00072426
Iteration 4/25 | Loss: 0.00072426
Iteration 5/25 | Loss: 0.00072426
Iteration 6/25 | Loss: 0.00072426
Iteration 7/25 | Loss: 0.00072426
Iteration 8/25 | Loss: 0.00072426
Iteration 9/25 | Loss: 0.00072426
Iteration 10/25 | Loss: 0.00072426
Iteration 11/25 | Loss: 0.00072426
Iteration 12/25 | Loss: 0.00072426
Iteration 13/25 | Loss: 0.00072426
Iteration 14/25 | Loss: 0.00072426
Iteration 15/25 | Loss: 0.00072426
Iteration 16/25 | Loss: 0.00072426
Iteration 17/25 | Loss: 0.00072426
Iteration 18/25 | Loss: 0.00072426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007242570864036679, 0.0007242570864036679, 0.0007242570864036679, 0.0007242570864036679, 0.0007242570864036679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007242570864036679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072426
Iteration 2/1000 | Loss: 0.00002821
Iteration 3/1000 | Loss: 0.00001956
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001612
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001535
Iteration 9/1000 | Loss: 0.00001497
Iteration 10/1000 | Loss: 0.00001472
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001457
Iteration 13/1000 | Loss: 0.00001456
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001439
Iteration 16/1000 | Loss: 0.00001436
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001430
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001429
Iteration 27/1000 | Loss: 0.00001429
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001426
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001425
Iteration 33/1000 | Loss: 0.00001425
Iteration 34/1000 | Loss: 0.00001425
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001423
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001422
Iteration 42/1000 | Loss: 0.00001421
Iteration 43/1000 | Loss: 0.00001421
Iteration 44/1000 | Loss: 0.00001420
Iteration 45/1000 | Loss: 0.00001420
Iteration 46/1000 | Loss: 0.00001419
Iteration 47/1000 | Loss: 0.00001418
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001417
Iteration 50/1000 | Loss: 0.00001417
Iteration 51/1000 | Loss: 0.00001417
Iteration 52/1000 | Loss: 0.00001416
Iteration 53/1000 | Loss: 0.00001416
Iteration 54/1000 | Loss: 0.00001416
Iteration 55/1000 | Loss: 0.00001416
Iteration 56/1000 | Loss: 0.00001416
Iteration 57/1000 | Loss: 0.00001416
Iteration 58/1000 | Loss: 0.00001416
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001415
Iteration 66/1000 | Loss: 0.00001415
Iteration 67/1000 | Loss: 0.00001415
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001414
Iteration 75/1000 | Loss: 0.00001414
Iteration 76/1000 | Loss: 0.00001414
Iteration 77/1000 | Loss: 0.00001414
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001414
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001413
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001413
Iteration 85/1000 | Loss: 0.00001413
Iteration 86/1000 | Loss: 0.00001413
Iteration 87/1000 | Loss: 0.00001413
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001413
Iteration 92/1000 | Loss: 0.00001413
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001412
Iteration 99/1000 | Loss: 0.00001412
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001411
Iteration 110/1000 | Loss: 0.00001411
Iteration 111/1000 | Loss: 0.00001411
Iteration 112/1000 | Loss: 0.00001411
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001410
Iteration 117/1000 | Loss: 0.00001410
Iteration 118/1000 | Loss: 0.00001409
Iteration 119/1000 | Loss: 0.00001409
Iteration 120/1000 | Loss: 0.00001409
Iteration 121/1000 | Loss: 0.00001409
Iteration 122/1000 | Loss: 0.00001409
Iteration 123/1000 | Loss: 0.00001409
Iteration 124/1000 | Loss: 0.00001409
Iteration 125/1000 | Loss: 0.00001409
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001408
Iteration 129/1000 | Loss: 0.00001408
Iteration 130/1000 | Loss: 0.00001408
Iteration 131/1000 | Loss: 0.00001408
Iteration 132/1000 | Loss: 0.00001408
Iteration 133/1000 | Loss: 0.00001407
Iteration 134/1000 | Loss: 0.00001407
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001407
Iteration 141/1000 | Loss: 0.00001407
Iteration 142/1000 | Loss: 0.00001407
Iteration 143/1000 | Loss: 0.00001407
Iteration 144/1000 | Loss: 0.00001407
Iteration 145/1000 | Loss: 0.00001406
Iteration 146/1000 | Loss: 0.00001406
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001406
Iteration 150/1000 | Loss: 0.00001406
Iteration 151/1000 | Loss: 0.00001406
Iteration 152/1000 | Loss: 0.00001406
Iteration 153/1000 | Loss: 0.00001406
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001406
Iteration 158/1000 | Loss: 0.00001406
Iteration 159/1000 | Loss: 0.00001406
Iteration 160/1000 | Loss: 0.00001406
Iteration 161/1000 | Loss: 0.00001406
Iteration 162/1000 | Loss: 0.00001406
Iteration 163/1000 | Loss: 0.00001406
Iteration 164/1000 | Loss: 0.00001406
Iteration 165/1000 | Loss: 0.00001406
Iteration 166/1000 | Loss: 0.00001406
Iteration 167/1000 | Loss: 0.00001406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.4057003681955393e-05, 1.4057003681955393e-05, 1.4057003681955393e-05, 1.4057003681955393e-05, 1.4057003681955393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4057003681955393e-05

Optimization complete. Final v2v error: 3.1806702613830566 mm

Highest mean error: 3.5775630474090576 mm for frame 193

Lowest mean error: 2.8735852241516113 mm for frame 100

Saving results

Total time: 40.75567412376404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812981
Iteration 2/25 | Loss: 0.00131409
Iteration 3/25 | Loss: 0.00123034
Iteration 4/25 | Loss: 0.00122169
Iteration 5/25 | Loss: 0.00121971
Iteration 6/25 | Loss: 0.00121971
Iteration 7/25 | Loss: 0.00121971
Iteration 8/25 | Loss: 0.00121971
Iteration 9/25 | Loss: 0.00121971
Iteration 10/25 | Loss: 0.00121971
Iteration 11/25 | Loss: 0.00121971
Iteration 12/25 | Loss: 0.00121971
Iteration 13/25 | Loss: 0.00121971
Iteration 14/25 | Loss: 0.00121971
Iteration 15/25 | Loss: 0.00121971
Iteration 16/25 | Loss: 0.00121971
Iteration 17/25 | Loss: 0.00121971
Iteration 18/25 | Loss: 0.00121971
Iteration 19/25 | Loss: 0.00121971
Iteration 20/25 | Loss: 0.00121971
Iteration 21/25 | Loss: 0.00121971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012197105679661036, 0.0012197105679661036, 0.0012197105679661036, 0.0012197105679661036, 0.0012197105679661036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012197105679661036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47408044
Iteration 2/25 | Loss: 0.00072610
Iteration 3/25 | Loss: 0.00072610
Iteration 4/25 | Loss: 0.00072610
Iteration 5/25 | Loss: 0.00072610
Iteration 6/25 | Loss: 0.00072610
Iteration 7/25 | Loss: 0.00072610
Iteration 8/25 | Loss: 0.00072610
Iteration 9/25 | Loss: 0.00072610
Iteration 10/25 | Loss: 0.00072610
Iteration 11/25 | Loss: 0.00072610
Iteration 12/25 | Loss: 0.00072610
Iteration 13/25 | Loss: 0.00072610
Iteration 14/25 | Loss: 0.00072610
Iteration 15/25 | Loss: 0.00072610
Iteration 16/25 | Loss: 0.00072610
Iteration 17/25 | Loss: 0.00072610
Iteration 18/25 | Loss: 0.00072610
Iteration 19/25 | Loss: 0.00072610
Iteration 20/25 | Loss: 0.00072610
Iteration 21/25 | Loss: 0.00072610
Iteration 22/25 | Loss: 0.00072610
Iteration 23/25 | Loss: 0.00072610
Iteration 24/25 | Loss: 0.00072610
Iteration 25/25 | Loss: 0.00072610

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072610
Iteration 2/1000 | Loss: 0.00002824
Iteration 3/1000 | Loss: 0.00001937
Iteration 4/1000 | Loss: 0.00001694
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001361
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00001318
Iteration 11/1000 | Loss: 0.00001294
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001289
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001282
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001269
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001264
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001261
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001259
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001259
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001257
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001256
Iteration 54/1000 | Loss: 0.00001256
Iteration 55/1000 | Loss: 0.00001256
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001255
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001253
Iteration 63/1000 | Loss: 0.00001252
Iteration 64/1000 | Loss: 0.00001252
Iteration 65/1000 | Loss: 0.00001251
Iteration 66/1000 | Loss: 0.00001251
Iteration 67/1000 | Loss: 0.00001251
Iteration 68/1000 | Loss: 0.00001250
Iteration 69/1000 | Loss: 0.00001250
Iteration 70/1000 | Loss: 0.00001249
Iteration 71/1000 | Loss: 0.00001249
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001242
Iteration 74/1000 | Loss: 0.00001240
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001238
Iteration 78/1000 | Loss: 0.00001238
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001234
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001231
Iteration 98/1000 | Loss: 0.00001231
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001228
Iteration 103/1000 | Loss: 0.00001228
Iteration 104/1000 | Loss: 0.00001228
Iteration 105/1000 | Loss: 0.00001228
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001225
Iteration 114/1000 | Loss: 0.00001225
Iteration 115/1000 | Loss: 0.00001225
Iteration 116/1000 | Loss: 0.00001225
Iteration 117/1000 | Loss: 0.00001225
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001224
Iteration 125/1000 | Loss: 0.00001224
Iteration 126/1000 | Loss: 0.00001224
Iteration 127/1000 | Loss: 0.00001224
Iteration 128/1000 | Loss: 0.00001224
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001223
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001223
Iteration 135/1000 | Loss: 0.00001222
Iteration 136/1000 | Loss: 0.00001222
Iteration 137/1000 | Loss: 0.00001222
Iteration 138/1000 | Loss: 0.00001222
Iteration 139/1000 | Loss: 0.00001222
Iteration 140/1000 | Loss: 0.00001222
Iteration 141/1000 | Loss: 0.00001222
Iteration 142/1000 | Loss: 0.00001222
Iteration 143/1000 | Loss: 0.00001222
Iteration 144/1000 | Loss: 0.00001221
Iteration 145/1000 | Loss: 0.00001221
Iteration 146/1000 | Loss: 0.00001221
Iteration 147/1000 | Loss: 0.00001221
Iteration 148/1000 | Loss: 0.00001220
Iteration 149/1000 | Loss: 0.00001220
Iteration 150/1000 | Loss: 0.00001220
Iteration 151/1000 | Loss: 0.00001220
Iteration 152/1000 | Loss: 0.00001220
Iteration 153/1000 | Loss: 0.00001219
Iteration 154/1000 | Loss: 0.00001219
Iteration 155/1000 | Loss: 0.00001219
Iteration 156/1000 | Loss: 0.00001219
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001219
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001218
Iteration 161/1000 | Loss: 0.00001218
Iteration 162/1000 | Loss: 0.00001218
Iteration 163/1000 | Loss: 0.00001218
Iteration 164/1000 | Loss: 0.00001217
Iteration 165/1000 | Loss: 0.00001217
Iteration 166/1000 | Loss: 0.00001217
Iteration 167/1000 | Loss: 0.00001217
Iteration 168/1000 | Loss: 0.00001217
Iteration 169/1000 | Loss: 0.00001217
Iteration 170/1000 | Loss: 0.00001217
Iteration 171/1000 | Loss: 0.00001217
Iteration 172/1000 | Loss: 0.00001217
Iteration 173/1000 | Loss: 0.00001217
Iteration 174/1000 | Loss: 0.00001216
Iteration 175/1000 | Loss: 0.00001216
Iteration 176/1000 | Loss: 0.00001216
Iteration 177/1000 | Loss: 0.00001216
Iteration 178/1000 | Loss: 0.00001216
Iteration 179/1000 | Loss: 0.00001215
Iteration 180/1000 | Loss: 0.00001215
Iteration 181/1000 | Loss: 0.00001215
Iteration 182/1000 | Loss: 0.00001215
Iteration 183/1000 | Loss: 0.00001215
Iteration 184/1000 | Loss: 0.00001215
Iteration 185/1000 | Loss: 0.00001215
Iteration 186/1000 | Loss: 0.00001215
Iteration 187/1000 | Loss: 0.00001215
Iteration 188/1000 | Loss: 0.00001215
Iteration 189/1000 | Loss: 0.00001215
Iteration 190/1000 | Loss: 0.00001215
Iteration 191/1000 | Loss: 0.00001215
Iteration 192/1000 | Loss: 0.00001215
Iteration 193/1000 | Loss: 0.00001215
Iteration 194/1000 | Loss: 0.00001215
Iteration 195/1000 | Loss: 0.00001214
Iteration 196/1000 | Loss: 0.00001214
Iteration 197/1000 | Loss: 0.00001214
Iteration 198/1000 | Loss: 0.00001214
Iteration 199/1000 | Loss: 0.00001214
Iteration 200/1000 | Loss: 0.00001214
Iteration 201/1000 | Loss: 0.00001214
Iteration 202/1000 | Loss: 0.00001214
Iteration 203/1000 | Loss: 0.00001214
Iteration 204/1000 | Loss: 0.00001214
Iteration 205/1000 | Loss: 0.00001214
Iteration 206/1000 | Loss: 0.00001214
Iteration 207/1000 | Loss: 0.00001213
Iteration 208/1000 | Loss: 0.00001213
Iteration 209/1000 | Loss: 0.00001213
Iteration 210/1000 | Loss: 0.00001213
Iteration 211/1000 | Loss: 0.00001213
Iteration 212/1000 | Loss: 0.00001213
Iteration 213/1000 | Loss: 0.00001213
Iteration 214/1000 | Loss: 0.00001213
Iteration 215/1000 | Loss: 0.00001213
Iteration 216/1000 | Loss: 0.00001213
Iteration 217/1000 | Loss: 0.00001213
Iteration 218/1000 | Loss: 0.00001213
Iteration 219/1000 | Loss: 0.00001213
Iteration 220/1000 | Loss: 0.00001212
Iteration 221/1000 | Loss: 0.00001212
Iteration 222/1000 | Loss: 0.00001212
Iteration 223/1000 | Loss: 0.00001212
Iteration 224/1000 | Loss: 0.00001212
Iteration 225/1000 | Loss: 0.00001212
Iteration 226/1000 | Loss: 0.00001212
Iteration 227/1000 | Loss: 0.00001212
Iteration 228/1000 | Loss: 0.00001212
Iteration 229/1000 | Loss: 0.00001212
Iteration 230/1000 | Loss: 0.00001212
Iteration 231/1000 | Loss: 0.00001211
Iteration 232/1000 | Loss: 0.00001211
Iteration 233/1000 | Loss: 0.00001211
Iteration 234/1000 | Loss: 0.00001211
Iteration 235/1000 | Loss: 0.00001211
Iteration 236/1000 | Loss: 0.00001211
Iteration 237/1000 | Loss: 0.00001211
Iteration 238/1000 | Loss: 0.00001211
Iteration 239/1000 | Loss: 0.00001211
Iteration 240/1000 | Loss: 0.00001211
Iteration 241/1000 | Loss: 0.00001211
Iteration 242/1000 | Loss: 0.00001211
Iteration 243/1000 | Loss: 0.00001211
Iteration 244/1000 | Loss: 0.00001211
Iteration 245/1000 | Loss: 0.00001211
Iteration 246/1000 | Loss: 0.00001211
Iteration 247/1000 | Loss: 0.00001211
Iteration 248/1000 | Loss: 0.00001211
Iteration 249/1000 | Loss: 0.00001211
Iteration 250/1000 | Loss: 0.00001211
Iteration 251/1000 | Loss: 0.00001211
Iteration 252/1000 | Loss: 0.00001210
Iteration 253/1000 | Loss: 0.00001210
Iteration 254/1000 | Loss: 0.00001210
Iteration 255/1000 | Loss: 0.00001210
Iteration 256/1000 | Loss: 0.00001210
Iteration 257/1000 | Loss: 0.00001210
Iteration 258/1000 | Loss: 0.00001210
Iteration 259/1000 | Loss: 0.00001210
Iteration 260/1000 | Loss: 0.00001210
Iteration 261/1000 | Loss: 0.00001210
Iteration 262/1000 | Loss: 0.00001210
Iteration 263/1000 | Loss: 0.00001210
Iteration 264/1000 | Loss: 0.00001210
Iteration 265/1000 | Loss: 0.00001210
Iteration 266/1000 | Loss: 0.00001210
Iteration 267/1000 | Loss: 0.00001210
Iteration 268/1000 | Loss: 0.00001210
Iteration 269/1000 | Loss: 0.00001210
Iteration 270/1000 | Loss: 0.00001210
Iteration 271/1000 | Loss: 0.00001210
Iteration 272/1000 | Loss: 0.00001209
Iteration 273/1000 | Loss: 0.00001209
Iteration 274/1000 | Loss: 0.00001209
Iteration 275/1000 | Loss: 0.00001209
Iteration 276/1000 | Loss: 0.00001209
Iteration 277/1000 | Loss: 0.00001209
Iteration 278/1000 | Loss: 0.00001209
Iteration 279/1000 | Loss: 0.00001209
Iteration 280/1000 | Loss: 0.00001209
Iteration 281/1000 | Loss: 0.00001209
Iteration 282/1000 | Loss: 0.00001209
Iteration 283/1000 | Loss: 0.00001209
Iteration 284/1000 | Loss: 0.00001209
Iteration 285/1000 | Loss: 0.00001209
Iteration 286/1000 | Loss: 0.00001209
Iteration 287/1000 | Loss: 0.00001209
Iteration 288/1000 | Loss: 0.00001209
Iteration 289/1000 | Loss: 0.00001209
Iteration 290/1000 | Loss: 0.00001209
Iteration 291/1000 | Loss: 0.00001209
Iteration 292/1000 | Loss: 0.00001209
Iteration 293/1000 | Loss: 0.00001209
Iteration 294/1000 | Loss: 0.00001209
Iteration 295/1000 | Loss: 0.00001209
Iteration 296/1000 | Loss: 0.00001209
Iteration 297/1000 | Loss: 0.00001209
Iteration 298/1000 | Loss: 0.00001209
Iteration 299/1000 | Loss: 0.00001209
Iteration 300/1000 | Loss: 0.00001209
Iteration 301/1000 | Loss: 0.00001209
Iteration 302/1000 | Loss: 0.00001209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [1.2085166417818982e-05, 1.2085166417818982e-05, 1.2085166417818982e-05, 1.2085166417818982e-05, 1.2085166417818982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2085166417818982e-05

Optimization complete. Final v2v error: 2.9524643421173096 mm

Highest mean error: 3.176590919494629 mm for frame 68

Lowest mean error: 2.8035831451416016 mm for frame 172

Saving results

Total time: 46.41935133934021
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616537
Iteration 2/25 | Loss: 0.00127891
Iteration 3/25 | Loss: 0.00121707
Iteration 4/25 | Loss: 0.00120694
Iteration 5/25 | Loss: 0.00120373
Iteration 6/25 | Loss: 0.00120373
Iteration 7/25 | Loss: 0.00120373
Iteration 8/25 | Loss: 0.00120373
Iteration 9/25 | Loss: 0.00120373
Iteration 10/25 | Loss: 0.00120373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001203726977109909, 0.001203726977109909, 0.001203726977109909, 0.001203726977109909, 0.001203726977109909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203726977109909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75428128
Iteration 2/25 | Loss: 0.00070876
Iteration 3/25 | Loss: 0.00070876
Iteration 4/25 | Loss: 0.00070876
Iteration 5/25 | Loss: 0.00070876
Iteration 6/25 | Loss: 0.00070876
Iteration 7/25 | Loss: 0.00070876
Iteration 8/25 | Loss: 0.00070876
Iteration 9/25 | Loss: 0.00070876
Iteration 10/25 | Loss: 0.00070876
Iteration 11/25 | Loss: 0.00070876
Iteration 12/25 | Loss: 0.00070876
Iteration 13/25 | Loss: 0.00070876
Iteration 14/25 | Loss: 0.00070876
Iteration 15/25 | Loss: 0.00070876
Iteration 16/25 | Loss: 0.00070876
Iteration 17/25 | Loss: 0.00070876
Iteration 18/25 | Loss: 0.00070876
Iteration 19/25 | Loss: 0.00070876
Iteration 20/25 | Loss: 0.00070876
Iteration 21/25 | Loss: 0.00070876
Iteration 22/25 | Loss: 0.00070876
Iteration 23/25 | Loss: 0.00070876
Iteration 24/25 | Loss: 0.00070876
Iteration 25/25 | Loss: 0.00070876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070876
Iteration 2/1000 | Loss: 0.00002575
Iteration 3/1000 | Loss: 0.00001988
Iteration 4/1000 | Loss: 0.00001788
Iteration 5/1000 | Loss: 0.00001675
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001509
Iteration 9/1000 | Loss: 0.00001484
Iteration 10/1000 | Loss: 0.00001462
Iteration 11/1000 | Loss: 0.00001446
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001431
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001420
Iteration 19/1000 | Loss: 0.00001420
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001414
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001411
Iteration 32/1000 | Loss: 0.00001410
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001408
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001408
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001385
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001382
Iteration 53/1000 | Loss: 0.00001382
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001381
Iteration 56/1000 | Loss: 0.00001381
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001379
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001378
Iteration 67/1000 | Loss: 0.00001378
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001377
Iteration 70/1000 | Loss: 0.00001377
Iteration 71/1000 | Loss: 0.00001377
Iteration 72/1000 | Loss: 0.00001377
Iteration 73/1000 | Loss: 0.00001377
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001376
Iteration 76/1000 | Loss: 0.00001376
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001374
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001372
Iteration 86/1000 | Loss: 0.00001371
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001370
Iteration 89/1000 | Loss: 0.00001370
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001368
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001368
Iteration 102/1000 | Loss: 0.00001367
Iteration 103/1000 | Loss: 0.00001367
Iteration 104/1000 | Loss: 0.00001367
Iteration 105/1000 | Loss: 0.00001367
Iteration 106/1000 | Loss: 0.00001367
Iteration 107/1000 | Loss: 0.00001364
Iteration 108/1000 | Loss: 0.00001364
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001363
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001362
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001359
Iteration 130/1000 | Loss: 0.00001359
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001359
Iteration 133/1000 | Loss: 0.00001359
Iteration 134/1000 | Loss: 0.00001359
Iteration 135/1000 | Loss: 0.00001359
Iteration 136/1000 | Loss: 0.00001359
Iteration 137/1000 | Loss: 0.00001359
Iteration 138/1000 | Loss: 0.00001359
Iteration 139/1000 | Loss: 0.00001358
Iteration 140/1000 | Loss: 0.00001358
Iteration 141/1000 | Loss: 0.00001358
Iteration 142/1000 | Loss: 0.00001358
Iteration 143/1000 | Loss: 0.00001358
Iteration 144/1000 | Loss: 0.00001358
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001357
Iteration 155/1000 | Loss: 0.00001357
Iteration 156/1000 | Loss: 0.00001357
Iteration 157/1000 | Loss: 0.00001356
Iteration 158/1000 | Loss: 0.00001356
Iteration 159/1000 | Loss: 0.00001356
Iteration 160/1000 | Loss: 0.00001356
Iteration 161/1000 | Loss: 0.00001355
Iteration 162/1000 | Loss: 0.00001355
Iteration 163/1000 | Loss: 0.00001355
Iteration 164/1000 | Loss: 0.00001355
Iteration 165/1000 | Loss: 0.00001355
Iteration 166/1000 | Loss: 0.00001355
Iteration 167/1000 | Loss: 0.00001355
Iteration 168/1000 | Loss: 0.00001355
Iteration 169/1000 | Loss: 0.00001355
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001354
Iteration 182/1000 | Loss: 0.00001354
Iteration 183/1000 | Loss: 0.00001354
Iteration 184/1000 | Loss: 0.00001354
Iteration 185/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.3538684470404405e-05, 1.3538684470404405e-05, 1.3538684470404405e-05, 1.3538684470404405e-05, 1.3538684470404405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3538684470404405e-05

Optimization complete. Final v2v error: 3.144604444503784 mm

Highest mean error: 3.447504758834839 mm for frame 54

Lowest mean error: 2.940805435180664 mm for frame 128

Saving results

Total time: 40.63958692550659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398861
Iteration 2/25 | Loss: 0.00127388
Iteration 3/25 | Loss: 0.00121828
Iteration 4/25 | Loss: 0.00121432
Iteration 5/25 | Loss: 0.00121432
Iteration 6/25 | Loss: 0.00121432
Iteration 7/25 | Loss: 0.00121432
Iteration 8/25 | Loss: 0.00121432
Iteration 9/25 | Loss: 0.00121432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.001214318093843758, 0.001214318093843758, 0.001214318093843758, 0.001214318093843758, 0.001214318093843758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001214318093843758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46303415
Iteration 2/25 | Loss: 0.00067120
Iteration 3/25 | Loss: 0.00067119
Iteration 4/25 | Loss: 0.00067119
Iteration 5/25 | Loss: 0.00067119
Iteration 6/25 | Loss: 0.00067119
Iteration 7/25 | Loss: 0.00067119
Iteration 8/25 | Loss: 0.00067119
Iteration 9/25 | Loss: 0.00067119
Iteration 10/25 | Loss: 0.00067119
Iteration 11/25 | Loss: 0.00067119
Iteration 12/25 | Loss: 0.00067119
Iteration 13/25 | Loss: 0.00067119
Iteration 14/25 | Loss: 0.00067119
Iteration 15/25 | Loss: 0.00067119
Iteration 16/25 | Loss: 0.00067119
Iteration 17/25 | Loss: 0.00067119
Iteration 18/25 | Loss: 0.00067119
Iteration 19/25 | Loss: 0.00067119
Iteration 20/25 | Loss: 0.00067119
Iteration 21/25 | Loss: 0.00067119
Iteration 22/25 | Loss: 0.00067119
Iteration 23/25 | Loss: 0.00067119
Iteration 24/25 | Loss: 0.00067119
Iteration 25/25 | Loss: 0.00067119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067119
Iteration 2/1000 | Loss: 0.00002496
Iteration 3/1000 | Loss: 0.00001873
Iteration 4/1000 | Loss: 0.00001680
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001524
Iteration 7/1000 | Loss: 0.00001481
Iteration 8/1000 | Loss: 0.00001443
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001374
Iteration 21/1000 | Loss: 0.00001369
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001362
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001327
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001325
Iteration 34/1000 | Loss: 0.00001325
Iteration 35/1000 | Loss: 0.00001324
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001323
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001313
Iteration 62/1000 | Loss: 0.00001313
Iteration 63/1000 | Loss: 0.00001313
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001312
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001307
Iteration 75/1000 | Loss: 0.00001307
Iteration 76/1000 | Loss: 0.00001307
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001306
Iteration 81/1000 | Loss: 0.00001306
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001304
Iteration 94/1000 | Loss: 0.00001304
Iteration 95/1000 | Loss: 0.00001304
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001304
Iteration 98/1000 | Loss: 0.00001303
Iteration 99/1000 | Loss: 0.00001303
Iteration 100/1000 | Loss: 0.00001303
Iteration 101/1000 | Loss: 0.00001303
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001303
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001302
Iteration 107/1000 | Loss: 0.00001302
Iteration 108/1000 | Loss: 0.00001302
Iteration 109/1000 | Loss: 0.00001302
Iteration 110/1000 | Loss: 0.00001302
Iteration 111/1000 | Loss: 0.00001302
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.3015535841987003e-05, 1.3015535841987003e-05, 1.3015535841987003e-05, 1.3015535841987003e-05, 1.3015535841987003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3015535841987003e-05

Optimization complete. Final v2v error: 3.082955837249756 mm

Highest mean error: 3.144453287124634 mm for frame 138

Lowest mean error: 2.963116407394409 mm for frame 230

Saving results

Total time: 39.48168420791626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042844
Iteration 2/25 | Loss: 0.01042844
Iteration 3/25 | Loss: 0.01042844
Iteration 4/25 | Loss: 0.01042843
Iteration 5/25 | Loss: 0.01042843
Iteration 6/25 | Loss: 0.01042843
Iteration 7/25 | Loss: 0.01042843
Iteration 8/25 | Loss: 0.01042843
Iteration 9/25 | Loss: 0.01042843
Iteration 10/25 | Loss: 0.01042843
Iteration 11/25 | Loss: 0.01042842
Iteration 12/25 | Loss: 0.01042842
Iteration 13/25 | Loss: 0.01042842
Iteration 14/25 | Loss: 0.01042842
Iteration 15/25 | Loss: 0.01042842
Iteration 16/25 | Loss: 0.01042842
Iteration 17/25 | Loss: 0.01042842
Iteration 18/25 | Loss: 0.01042842
Iteration 19/25 | Loss: 0.01042842
Iteration 20/25 | Loss: 0.01042842
Iteration 21/25 | Loss: 0.01042841
Iteration 22/25 | Loss: 0.01042841
Iteration 23/25 | Loss: 0.01042841
Iteration 24/25 | Loss: 0.01042841
Iteration 25/25 | Loss: 0.01042841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58267319
Iteration 2/25 | Loss: 0.12280048
Iteration 3/25 | Loss: 0.12009010
Iteration 4/25 | Loss: 0.12008874
Iteration 5/25 | Loss: 0.12008873
Iteration 6/25 | Loss: 0.12008870
Iteration 7/25 | Loss: 0.12008870
Iteration 8/25 | Loss: 0.12008870
Iteration 9/25 | Loss: 0.12008870
Iteration 10/25 | Loss: 0.12008870
Iteration 11/25 | Loss: 0.12008870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.12008870393037796, 0.12008870393037796, 0.12008870393037796, 0.12008870393037796, 0.12008870393037796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12008870393037796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12008870
Iteration 2/1000 | Loss: 0.00126700
Iteration 3/1000 | Loss: 0.00077150
Iteration 4/1000 | Loss: 0.00014885
Iteration 5/1000 | Loss: 0.00007356
Iteration 6/1000 | Loss: 0.00004538
Iteration 7/1000 | Loss: 0.00003313
Iteration 8/1000 | Loss: 0.00002722
Iteration 9/1000 | Loss: 0.00002354
Iteration 10/1000 | Loss: 0.00002064
Iteration 11/1000 | Loss: 0.00001870
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001500
Iteration 16/1000 | Loss: 0.00001442
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001379
Iteration 19/1000 | Loss: 0.00001347
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001275
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001236
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001226
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001224
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001212
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001210
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001206
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001194
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001185
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001184
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001183
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001177
Iteration 117/1000 | Loss: 0.00001177
Iteration 118/1000 | Loss: 0.00001177
Iteration 119/1000 | Loss: 0.00001177
Iteration 120/1000 | Loss: 0.00001177
Iteration 121/1000 | Loss: 0.00001177
Iteration 122/1000 | Loss: 0.00001176
Iteration 123/1000 | Loss: 0.00001176
Iteration 124/1000 | Loss: 0.00001176
Iteration 125/1000 | Loss: 0.00001175
Iteration 126/1000 | Loss: 0.00001175
Iteration 127/1000 | Loss: 0.00001175
Iteration 128/1000 | Loss: 0.00001175
Iteration 129/1000 | Loss: 0.00001174
Iteration 130/1000 | Loss: 0.00001174
Iteration 131/1000 | Loss: 0.00001174
Iteration 132/1000 | Loss: 0.00001173
Iteration 133/1000 | Loss: 0.00001173
Iteration 134/1000 | Loss: 0.00001173
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001171
Iteration 139/1000 | Loss: 0.00001171
Iteration 140/1000 | Loss: 0.00001171
Iteration 141/1000 | Loss: 0.00001171
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001170
Iteration 150/1000 | Loss: 0.00001170
Iteration 151/1000 | Loss: 0.00001170
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001168
Iteration 162/1000 | Loss: 0.00001168
Iteration 163/1000 | Loss: 0.00001168
Iteration 164/1000 | Loss: 0.00001168
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001168
Iteration 169/1000 | Loss: 0.00001168
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001168
Iteration 172/1000 | Loss: 0.00001167
Iteration 173/1000 | Loss: 0.00001167
Iteration 174/1000 | Loss: 0.00001167
Iteration 175/1000 | Loss: 0.00001167
Iteration 176/1000 | Loss: 0.00001167
Iteration 177/1000 | Loss: 0.00001167
Iteration 178/1000 | Loss: 0.00001167
Iteration 179/1000 | Loss: 0.00001167
Iteration 180/1000 | Loss: 0.00001167
Iteration 181/1000 | Loss: 0.00001167
Iteration 182/1000 | Loss: 0.00001167
Iteration 183/1000 | Loss: 0.00001167
Iteration 184/1000 | Loss: 0.00001167
Iteration 185/1000 | Loss: 0.00001167
Iteration 186/1000 | Loss: 0.00001167
Iteration 187/1000 | Loss: 0.00001167
Iteration 188/1000 | Loss: 0.00001167
Iteration 189/1000 | Loss: 0.00001167
Iteration 190/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.1673145309032407e-05, 1.1673145309032407e-05, 1.1673145309032407e-05, 1.1673145309032407e-05, 1.1673145309032407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1673145309032407e-05

Optimization complete. Final v2v error: 2.9225916862487793 mm

Highest mean error: 3.2440078258514404 mm for frame 79

Lowest mean error: 2.727243423461914 mm for frame 180

Saving results

Total time: 61.718177795410156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029219
Iteration 2/25 | Loss: 0.01029219
Iteration 3/25 | Loss: 0.01029219
Iteration 4/25 | Loss: 0.00259583
Iteration 5/25 | Loss: 0.00194133
Iteration 6/25 | Loss: 0.00161072
Iteration 7/25 | Loss: 0.00154680
Iteration 8/25 | Loss: 0.00150350
Iteration 9/25 | Loss: 0.00148823
Iteration 10/25 | Loss: 0.00144698
Iteration 11/25 | Loss: 0.00142844
Iteration 12/25 | Loss: 0.00140358
Iteration 13/25 | Loss: 0.00141744
Iteration 14/25 | Loss: 0.00142521
Iteration 15/25 | Loss: 0.00141544
Iteration 16/25 | Loss: 0.00142700
Iteration 17/25 | Loss: 0.00139487
Iteration 18/25 | Loss: 0.00140492
Iteration 19/25 | Loss: 0.00138532
Iteration 20/25 | Loss: 0.00137467
Iteration 21/25 | Loss: 0.00137259
Iteration 22/25 | Loss: 0.00136952
Iteration 23/25 | Loss: 0.00136645
Iteration 24/25 | Loss: 0.00136570
Iteration 25/25 | Loss: 0.00136554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41823018
Iteration 2/25 | Loss: 0.00264971
Iteration 3/25 | Loss: 0.00173189
Iteration 4/25 | Loss: 0.00173188
Iteration 5/25 | Loss: 0.00173188
Iteration 6/25 | Loss: 0.00173188
Iteration 7/25 | Loss: 0.00173188
Iteration 8/25 | Loss: 0.00173188
Iteration 9/25 | Loss: 0.00173188
Iteration 10/25 | Loss: 0.00173188
Iteration 11/25 | Loss: 0.00173188
Iteration 12/25 | Loss: 0.00173188
Iteration 13/25 | Loss: 0.00173188
Iteration 14/25 | Loss: 0.00173188
Iteration 15/25 | Loss: 0.00173188
Iteration 16/25 | Loss: 0.00173188
Iteration 17/25 | Loss: 0.00173188
Iteration 18/25 | Loss: 0.00173188
Iteration 19/25 | Loss: 0.00173188
Iteration 20/25 | Loss: 0.00173188
Iteration 21/25 | Loss: 0.00173188
Iteration 22/25 | Loss: 0.00173188
Iteration 23/25 | Loss: 0.00173188
Iteration 24/25 | Loss: 0.00173188
Iteration 25/25 | Loss: 0.00173188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173188
Iteration 2/1000 | Loss: 0.00023085
Iteration 3/1000 | Loss: 0.00146830
Iteration 4/1000 | Loss: 0.00138033
Iteration 5/1000 | Loss: 0.00015129
Iteration 6/1000 | Loss: 0.00012832
Iteration 7/1000 | Loss: 0.00040574
Iteration 8/1000 | Loss: 0.00012120
Iteration 9/1000 | Loss: 0.00013914
Iteration 10/1000 | Loss: 0.00007619
Iteration 11/1000 | Loss: 0.00075255
Iteration 12/1000 | Loss: 0.00007521
Iteration 13/1000 | Loss: 0.00010382
Iteration 14/1000 | Loss: 0.00213688
Iteration 15/1000 | Loss: 0.00298427
Iteration 16/1000 | Loss: 0.00307834
Iteration 17/1000 | Loss: 0.00317865
Iteration 18/1000 | Loss: 0.00104609
Iteration 19/1000 | Loss: 0.00163293
Iteration 20/1000 | Loss: 0.00114543
Iteration 21/1000 | Loss: 0.00092595
Iteration 22/1000 | Loss: 0.00264102
Iteration 23/1000 | Loss: 0.00109206
Iteration 24/1000 | Loss: 0.00045253
Iteration 25/1000 | Loss: 0.00052761
Iteration 26/1000 | Loss: 0.00069634
Iteration 27/1000 | Loss: 0.00389323
Iteration 28/1000 | Loss: 0.00153779
Iteration 29/1000 | Loss: 0.00210341
Iteration 30/1000 | Loss: 0.00188289
Iteration 31/1000 | Loss: 0.00230552
Iteration 32/1000 | Loss: 0.00151415
Iteration 33/1000 | Loss: 0.00064349
Iteration 34/1000 | Loss: 0.00026335
Iteration 35/1000 | Loss: 0.00022071
Iteration 36/1000 | Loss: 0.00065276
Iteration 37/1000 | Loss: 0.00062342
Iteration 38/1000 | Loss: 0.00094049
Iteration 39/1000 | Loss: 0.00097303
Iteration 40/1000 | Loss: 0.00064656
Iteration 41/1000 | Loss: 0.00043117
Iteration 42/1000 | Loss: 0.00046857
Iteration 43/1000 | Loss: 0.00055850
Iteration 44/1000 | Loss: 0.00062419
Iteration 45/1000 | Loss: 0.00091658
Iteration 46/1000 | Loss: 0.00062408
Iteration 47/1000 | Loss: 0.00057542
Iteration 48/1000 | Loss: 0.00009220
Iteration 49/1000 | Loss: 0.00006626
Iteration 50/1000 | Loss: 0.00050073
Iteration 51/1000 | Loss: 0.00089074
Iteration 52/1000 | Loss: 0.00068504
Iteration 53/1000 | Loss: 0.00043144
Iteration 54/1000 | Loss: 0.00029299
Iteration 55/1000 | Loss: 0.00044816
Iteration 56/1000 | Loss: 0.00036474
Iteration 57/1000 | Loss: 0.00038784
Iteration 58/1000 | Loss: 0.00031360
Iteration 59/1000 | Loss: 0.00009128
Iteration 60/1000 | Loss: 0.00006082
Iteration 61/1000 | Loss: 0.00045855
Iteration 62/1000 | Loss: 0.00008865
Iteration 63/1000 | Loss: 0.00034953
Iteration 64/1000 | Loss: 0.00252748
Iteration 65/1000 | Loss: 0.00007394
Iteration 66/1000 | Loss: 0.00006018
Iteration 67/1000 | Loss: 0.00006276
Iteration 68/1000 | Loss: 0.00004972
Iteration 69/1000 | Loss: 0.00005578
Iteration 70/1000 | Loss: 0.00054146
Iteration 71/1000 | Loss: 0.00053224
Iteration 72/1000 | Loss: 0.00005121
Iteration 73/1000 | Loss: 0.00004707
Iteration 74/1000 | Loss: 0.00057015
Iteration 75/1000 | Loss: 0.00035679
Iteration 76/1000 | Loss: 0.00006300
Iteration 77/1000 | Loss: 0.00005778
Iteration 78/1000 | Loss: 0.00005259
Iteration 79/1000 | Loss: 0.00004521
Iteration 80/1000 | Loss: 0.00004437
Iteration 81/1000 | Loss: 0.00280398
Iteration 82/1000 | Loss: 0.00115374
Iteration 83/1000 | Loss: 0.00158986
Iteration 84/1000 | Loss: 0.00017440
Iteration 85/1000 | Loss: 0.00023840
Iteration 86/1000 | Loss: 0.00004330
Iteration 87/1000 | Loss: 0.00003691
Iteration 88/1000 | Loss: 0.00006308
Iteration 89/1000 | Loss: 0.00007116
Iteration 90/1000 | Loss: 0.00002955
Iteration 91/1000 | Loss: 0.00008414
Iteration 92/1000 | Loss: 0.00003401
Iteration 93/1000 | Loss: 0.00005571
Iteration 94/1000 | Loss: 0.00002680
Iteration 95/1000 | Loss: 0.00002385
Iteration 96/1000 | Loss: 0.00002333
Iteration 97/1000 | Loss: 0.00003166
Iteration 98/1000 | Loss: 0.00002555
Iteration 99/1000 | Loss: 0.00005526
Iteration 100/1000 | Loss: 0.00003286
Iteration 101/1000 | Loss: 0.00005710
Iteration 102/1000 | Loss: 0.00002287
Iteration 103/1000 | Loss: 0.00002214
Iteration 104/1000 | Loss: 0.00002195
Iteration 105/1000 | Loss: 0.00002186
Iteration 106/1000 | Loss: 0.00007257
Iteration 107/1000 | Loss: 0.00002641
Iteration 108/1000 | Loss: 0.00002184
Iteration 109/1000 | Loss: 0.00003952
Iteration 110/1000 | Loss: 0.00006150
Iteration 111/1000 | Loss: 0.00002594
Iteration 112/1000 | Loss: 0.00002162
Iteration 113/1000 | Loss: 0.00002162
Iteration 114/1000 | Loss: 0.00002162
Iteration 115/1000 | Loss: 0.00002162
Iteration 116/1000 | Loss: 0.00002161
Iteration 117/1000 | Loss: 0.00002161
Iteration 118/1000 | Loss: 0.00002161
Iteration 119/1000 | Loss: 0.00002161
Iteration 120/1000 | Loss: 0.00002161
Iteration 121/1000 | Loss: 0.00002161
Iteration 122/1000 | Loss: 0.00002161
Iteration 123/1000 | Loss: 0.00002161
Iteration 124/1000 | Loss: 0.00002161
Iteration 125/1000 | Loss: 0.00002161
Iteration 126/1000 | Loss: 0.00002161
Iteration 127/1000 | Loss: 0.00002161
Iteration 128/1000 | Loss: 0.00002161
Iteration 129/1000 | Loss: 0.00002161
Iteration 130/1000 | Loss: 0.00002161
Iteration 131/1000 | Loss: 0.00002161
Iteration 132/1000 | Loss: 0.00002160
Iteration 133/1000 | Loss: 0.00002160
Iteration 134/1000 | Loss: 0.00002160
Iteration 135/1000 | Loss: 0.00002160
Iteration 136/1000 | Loss: 0.00002160
Iteration 137/1000 | Loss: 0.00002160
Iteration 138/1000 | Loss: 0.00002160
Iteration 139/1000 | Loss: 0.00002160
Iteration 140/1000 | Loss: 0.00002160
Iteration 141/1000 | Loss: 0.00002160
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002159
Iteration 146/1000 | Loss: 0.00002159
Iteration 147/1000 | Loss: 0.00002159
Iteration 148/1000 | Loss: 0.00002159
Iteration 149/1000 | Loss: 0.00002158
Iteration 150/1000 | Loss: 0.00002158
Iteration 151/1000 | Loss: 0.00003037
Iteration 152/1000 | Loss: 0.00002198
Iteration 153/1000 | Loss: 0.00002193
Iteration 154/1000 | Loss: 0.00003236
Iteration 155/1000 | Loss: 0.00029233
Iteration 156/1000 | Loss: 0.00002173
Iteration 157/1000 | Loss: 0.00002157
Iteration 158/1000 | Loss: 0.00002157
Iteration 159/1000 | Loss: 0.00002157
Iteration 160/1000 | Loss: 0.00002157
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002157
Iteration 163/1000 | Loss: 0.00002157
Iteration 164/1000 | Loss: 0.00002156
Iteration 165/1000 | Loss: 0.00002156
Iteration 166/1000 | Loss: 0.00002156
Iteration 167/1000 | Loss: 0.00002156
Iteration 168/1000 | Loss: 0.00002156
Iteration 169/1000 | Loss: 0.00002156
Iteration 170/1000 | Loss: 0.00002156
Iteration 171/1000 | Loss: 0.00002155
Iteration 172/1000 | Loss: 0.00002155
Iteration 173/1000 | Loss: 0.00002155
Iteration 174/1000 | Loss: 0.00002155
Iteration 175/1000 | Loss: 0.00002155
Iteration 176/1000 | Loss: 0.00002154
Iteration 177/1000 | Loss: 0.00002154
Iteration 178/1000 | Loss: 0.00002154
Iteration 179/1000 | Loss: 0.00002154
Iteration 180/1000 | Loss: 0.00002154
Iteration 181/1000 | Loss: 0.00002154
Iteration 182/1000 | Loss: 0.00002154
Iteration 183/1000 | Loss: 0.00002154
Iteration 184/1000 | Loss: 0.00002154
Iteration 185/1000 | Loss: 0.00002153
Iteration 186/1000 | Loss: 0.00002153
Iteration 187/1000 | Loss: 0.00002153
Iteration 188/1000 | Loss: 0.00002153
Iteration 189/1000 | Loss: 0.00002153
Iteration 190/1000 | Loss: 0.00002153
Iteration 191/1000 | Loss: 0.00002153
Iteration 192/1000 | Loss: 0.00002153
Iteration 193/1000 | Loss: 0.00002152
Iteration 194/1000 | Loss: 0.00002152
Iteration 195/1000 | Loss: 0.00002152
Iteration 196/1000 | Loss: 0.00002152
Iteration 197/1000 | Loss: 0.00002151
Iteration 198/1000 | Loss: 0.00002151
Iteration 199/1000 | Loss: 0.00002151
Iteration 200/1000 | Loss: 0.00002151
Iteration 201/1000 | Loss: 0.00002151
Iteration 202/1000 | Loss: 0.00002150
Iteration 203/1000 | Loss: 0.00002150
Iteration 204/1000 | Loss: 0.00002150
Iteration 205/1000 | Loss: 0.00002150
Iteration 206/1000 | Loss: 0.00002150
Iteration 207/1000 | Loss: 0.00002149
Iteration 208/1000 | Loss: 0.00002149
Iteration 209/1000 | Loss: 0.00002149
Iteration 210/1000 | Loss: 0.00002149
Iteration 211/1000 | Loss: 0.00002149
Iteration 212/1000 | Loss: 0.00002149
Iteration 213/1000 | Loss: 0.00002149
Iteration 214/1000 | Loss: 0.00002149
Iteration 215/1000 | Loss: 0.00002149
Iteration 216/1000 | Loss: 0.00002149
Iteration 217/1000 | Loss: 0.00002149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.1490968720172532e-05, 2.1490968720172532e-05, 2.1490968720172532e-05, 2.1490968720172532e-05, 2.1490968720172532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1490968720172532e-05

Optimization complete. Final v2v error: 3.8968169689178467 mm

Highest mean error: 5.149857521057129 mm for frame 222

Lowest mean error: 3.4549720287323 mm for frame 203

Saving results

Total time: 236.36344027519226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025165
Iteration 2/25 | Loss: 0.00222719
Iteration 3/25 | Loss: 0.00210221
Iteration 4/25 | Loss: 0.00154481
Iteration 5/25 | Loss: 0.00149889
Iteration 6/25 | Loss: 0.00145727
Iteration 7/25 | Loss: 0.00140960
Iteration 8/25 | Loss: 0.00138013
Iteration 9/25 | Loss: 0.00137581
Iteration 10/25 | Loss: 0.00136337
Iteration 11/25 | Loss: 0.00135034
Iteration 12/25 | Loss: 0.00135037
Iteration 13/25 | Loss: 0.00134548
Iteration 14/25 | Loss: 0.00133976
Iteration 15/25 | Loss: 0.00133406
Iteration 16/25 | Loss: 0.00133626
Iteration 17/25 | Loss: 0.00133433
Iteration 18/25 | Loss: 0.00133263
Iteration 19/25 | Loss: 0.00133234
Iteration 20/25 | Loss: 0.00133233
Iteration 21/25 | Loss: 0.00133233
Iteration 22/25 | Loss: 0.00133233
Iteration 23/25 | Loss: 0.00133233
Iteration 24/25 | Loss: 0.00133233
Iteration 25/25 | Loss: 0.00133233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48646092
Iteration 2/25 | Loss: 0.00096123
Iteration 3/25 | Loss: 0.00096123
Iteration 4/25 | Loss: 0.00089055
Iteration 5/25 | Loss: 0.00089054
Iteration 6/25 | Loss: 0.00089054
Iteration 7/25 | Loss: 0.00089054
Iteration 8/25 | Loss: 0.00089054
Iteration 9/25 | Loss: 0.00089054
Iteration 10/25 | Loss: 0.00089054
Iteration 11/25 | Loss: 0.00089054
Iteration 12/25 | Loss: 0.00089054
Iteration 13/25 | Loss: 0.00089054
Iteration 14/25 | Loss: 0.00089054
Iteration 15/25 | Loss: 0.00089054
Iteration 16/25 | Loss: 0.00089054
Iteration 17/25 | Loss: 0.00089054
Iteration 18/25 | Loss: 0.00089054
Iteration 19/25 | Loss: 0.00089054
Iteration 20/25 | Loss: 0.00089054
Iteration 21/25 | Loss: 0.00089054
Iteration 22/25 | Loss: 0.00089054
Iteration 23/25 | Loss: 0.00089054
Iteration 24/25 | Loss: 0.00089054
Iteration 25/25 | Loss: 0.00089054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089054
Iteration 2/1000 | Loss: 0.00023300
Iteration 3/1000 | Loss: 0.00032189
Iteration 4/1000 | Loss: 0.00052719
Iteration 5/1000 | Loss: 0.00018132
Iteration 6/1000 | Loss: 0.00007603
Iteration 7/1000 | Loss: 0.00035093
Iteration 8/1000 | Loss: 0.00005105
Iteration 9/1000 | Loss: 0.00005773
Iteration 10/1000 | Loss: 0.00002472
Iteration 11/1000 | Loss: 0.00006353
Iteration 12/1000 | Loss: 0.00002449
Iteration 13/1000 | Loss: 0.00008483
Iteration 14/1000 | Loss: 0.00002414
Iteration 15/1000 | Loss: 0.00002376
Iteration 16/1000 | Loss: 0.00002358
Iteration 17/1000 | Loss: 0.00009673
Iteration 18/1000 | Loss: 0.00022769
Iteration 19/1000 | Loss: 0.00024714
Iteration 20/1000 | Loss: 0.00002416
Iteration 21/1000 | Loss: 0.00002326
Iteration 22/1000 | Loss: 0.00004502
Iteration 23/1000 | Loss: 0.00033131
Iteration 24/1000 | Loss: 0.00003845
Iteration 25/1000 | Loss: 0.00002320
Iteration 26/1000 | Loss: 0.00005290
Iteration 27/1000 | Loss: 0.00002313
Iteration 28/1000 | Loss: 0.00002298
Iteration 29/1000 | Loss: 0.00002295
Iteration 30/1000 | Loss: 0.00002295
Iteration 31/1000 | Loss: 0.00002285
Iteration 32/1000 | Loss: 0.00002284
Iteration 33/1000 | Loss: 0.00002280
Iteration 34/1000 | Loss: 0.00002279
Iteration 35/1000 | Loss: 0.00002277
Iteration 36/1000 | Loss: 0.00002276
Iteration 37/1000 | Loss: 0.00002276
Iteration 38/1000 | Loss: 0.00002273
Iteration 39/1000 | Loss: 0.00002272
Iteration 40/1000 | Loss: 0.00002271
Iteration 41/1000 | Loss: 0.00002270
Iteration 42/1000 | Loss: 0.00002268
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002262
Iteration 45/1000 | Loss: 0.00002248
Iteration 46/1000 | Loss: 0.00002246
Iteration 47/1000 | Loss: 0.00002245
Iteration 48/1000 | Loss: 0.00002245
Iteration 49/1000 | Loss: 0.00002244
Iteration 50/1000 | Loss: 0.00002242
Iteration 51/1000 | Loss: 0.00002241
Iteration 52/1000 | Loss: 0.00002240
Iteration 53/1000 | Loss: 0.00002240
Iteration 54/1000 | Loss: 0.00002240
Iteration 55/1000 | Loss: 0.00002238
Iteration 56/1000 | Loss: 0.00002238
Iteration 57/1000 | Loss: 0.00002238
Iteration 58/1000 | Loss: 0.00002237
Iteration 59/1000 | Loss: 0.00002237
Iteration 60/1000 | Loss: 0.00002237
Iteration 61/1000 | Loss: 0.00002237
Iteration 62/1000 | Loss: 0.00002237
Iteration 63/1000 | Loss: 0.00002237
Iteration 64/1000 | Loss: 0.00002237
Iteration 65/1000 | Loss: 0.00002237
Iteration 66/1000 | Loss: 0.00002236
Iteration 67/1000 | Loss: 0.00002236
Iteration 68/1000 | Loss: 0.00002236
Iteration 69/1000 | Loss: 0.00002236
Iteration 70/1000 | Loss: 0.00002235
Iteration 71/1000 | Loss: 0.00002234
Iteration 72/1000 | Loss: 0.00007101
Iteration 73/1000 | Loss: 0.00005403
Iteration 74/1000 | Loss: 0.00002949
Iteration 75/1000 | Loss: 0.00002243
Iteration 76/1000 | Loss: 0.00002230
Iteration 77/1000 | Loss: 0.00002229
Iteration 78/1000 | Loss: 0.00002227
Iteration 79/1000 | Loss: 0.00002227
Iteration 80/1000 | Loss: 0.00002227
Iteration 81/1000 | Loss: 0.00002227
Iteration 82/1000 | Loss: 0.00002227
Iteration 83/1000 | Loss: 0.00002227
Iteration 84/1000 | Loss: 0.00002227
Iteration 85/1000 | Loss: 0.00002227
Iteration 86/1000 | Loss: 0.00002226
Iteration 87/1000 | Loss: 0.00002226
Iteration 88/1000 | Loss: 0.00002226
Iteration 89/1000 | Loss: 0.00002226
Iteration 90/1000 | Loss: 0.00002225
Iteration 91/1000 | Loss: 0.00002225
Iteration 92/1000 | Loss: 0.00002225
Iteration 93/1000 | Loss: 0.00002225
Iteration 94/1000 | Loss: 0.00002225
Iteration 95/1000 | Loss: 0.00002225
Iteration 96/1000 | Loss: 0.00002225
Iteration 97/1000 | Loss: 0.00002225
Iteration 98/1000 | Loss: 0.00002225
Iteration 99/1000 | Loss: 0.00002225
Iteration 100/1000 | Loss: 0.00002225
Iteration 101/1000 | Loss: 0.00002225
Iteration 102/1000 | Loss: 0.00002224
Iteration 103/1000 | Loss: 0.00002224
Iteration 104/1000 | Loss: 0.00002224
Iteration 105/1000 | Loss: 0.00002224
Iteration 106/1000 | Loss: 0.00002224
Iteration 107/1000 | Loss: 0.00002224
Iteration 108/1000 | Loss: 0.00002224
Iteration 109/1000 | Loss: 0.00002224
Iteration 110/1000 | Loss: 0.00002224
Iteration 111/1000 | Loss: 0.00002224
Iteration 112/1000 | Loss: 0.00002224
Iteration 113/1000 | Loss: 0.00002223
Iteration 114/1000 | Loss: 0.00002223
Iteration 115/1000 | Loss: 0.00002223
Iteration 116/1000 | Loss: 0.00002223
Iteration 117/1000 | Loss: 0.00002223
Iteration 118/1000 | Loss: 0.00002223
Iteration 119/1000 | Loss: 0.00002223
Iteration 120/1000 | Loss: 0.00002223
Iteration 121/1000 | Loss: 0.00002223
Iteration 122/1000 | Loss: 0.00002223
Iteration 123/1000 | Loss: 0.00002223
Iteration 124/1000 | Loss: 0.00002223
Iteration 125/1000 | Loss: 0.00002223
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002223
Iteration 128/1000 | Loss: 0.00002223
Iteration 129/1000 | Loss: 0.00002222
Iteration 130/1000 | Loss: 0.00002222
Iteration 131/1000 | Loss: 0.00002222
Iteration 132/1000 | Loss: 0.00002222
Iteration 133/1000 | Loss: 0.00002222
Iteration 134/1000 | Loss: 0.00002222
Iteration 135/1000 | Loss: 0.00002222
Iteration 136/1000 | Loss: 0.00002222
Iteration 137/1000 | Loss: 0.00002222
Iteration 138/1000 | Loss: 0.00002222
Iteration 139/1000 | Loss: 0.00002222
Iteration 140/1000 | Loss: 0.00002222
Iteration 141/1000 | Loss: 0.00002222
Iteration 142/1000 | Loss: 0.00002222
Iteration 143/1000 | Loss: 0.00002222
Iteration 144/1000 | Loss: 0.00002222
Iteration 145/1000 | Loss: 0.00002222
Iteration 146/1000 | Loss: 0.00002222
Iteration 147/1000 | Loss: 0.00002222
Iteration 148/1000 | Loss: 0.00002222
Iteration 149/1000 | Loss: 0.00002222
Iteration 150/1000 | Loss: 0.00002222
Iteration 151/1000 | Loss: 0.00002222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.222166585852392e-05, 2.222166585852392e-05, 2.222166585852392e-05, 2.222166585852392e-05, 2.222166585852392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.222166585852392e-05

Optimization complete. Final v2v error: 3.8228843212127686 mm

Highest mean error: 11.533002853393555 mm for frame 132

Lowest mean error: 3.2999439239501953 mm for frame 82

Saving results

Total time: 101.49947237968445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00654691
Iteration 2/25 | Loss: 0.00135660
Iteration 3/25 | Loss: 0.00128494
Iteration 4/25 | Loss: 0.00127580
Iteration 5/25 | Loss: 0.00127250
Iteration 6/25 | Loss: 0.00127250
Iteration 7/25 | Loss: 0.00127250
Iteration 8/25 | Loss: 0.00127250
Iteration 9/25 | Loss: 0.00127250
Iteration 10/25 | Loss: 0.00127250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012724987464025617, 0.0012724987464025617, 0.0012724987464025617, 0.0012724987464025617, 0.0012724987464025617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012724987464025617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17298651
Iteration 2/25 | Loss: 0.00073587
Iteration 3/25 | Loss: 0.00073587
Iteration 4/25 | Loss: 0.00073587
Iteration 5/25 | Loss: 0.00073587
Iteration 6/25 | Loss: 0.00073587
Iteration 7/25 | Loss: 0.00073587
Iteration 8/25 | Loss: 0.00073587
Iteration 9/25 | Loss: 0.00073587
Iteration 10/25 | Loss: 0.00073587
Iteration 11/25 | Loss: 0.00073587
Iteration 12/25 | Loss: 0.00073587
Iteration 13/25 | Loss: 0.00073587
Iteration 14/25 | Loss: 0.00073587
Iteration 15/25 | Loss: 0.00073587
Iteration 16/25 | Loss: 0.00073587
Iteration 17/25 | Loss: 0.00073587
Iteration 18/25 | Loss: 0.00073587
Iteration 19/25 | Loss: 0.00073587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007358675939030945, 0.0007358675939030945, 0.0007358675939030945, 0.0007358675939030945, 0.0007358675939030945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007358675939030945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073587
Iteration 2/1000 | Loss: 0.00003321
Iteration 3/1000 | Loss: 0.00002558
Iteration 4/1000 | Loss: 0.00002335
Iteration 5/1000 | Loss: 0.00002261
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002119
Iteration 8/1000 | Loss: 0.00002082
Iteration 9/1000 | Loss: 0.00002042
Iteration 10/1000 | Loss: 0.00002001
Iteration 11/1000 | Loss: 0.00001976
Iteration 12/1000 | Loss: 0.00001956
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001936
Iteration 15/1000 | Loss: 0.00001936
Iteration 16/1000 | Loss: 0.00001935
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001934
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001934
Iteration 21/1000 | Loss: 0.00001934
Iteration 22/1000 | Loss: 0.00001933
Iteration 23/1000 | Loss: 0.00001933
Iteration 24/1000 | Loss: 0.00001931
Iteration 25/1000 | Loss: 0.00001929
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001927
Iteration 28/1000 | Loss: 0.00001925
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001920
Iteration 39/1000 | Loss: 0.00001920
Iteration 40/1000 | Loss: 0.00001918
Iteration 41/1000 | Loss: 0.00001918
Iteration 42/1000 | Loss: 0.00001917
Iteration 43/1000 | Loss: 0.00001916
Iteration 44/1000 | Loss: 0.00001916
Iteration 45/1000 | Loss: 0.00001915
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001912
Iteration 48/1000 | Loss: 0.00001912
Iteration 49/1000 | Loss: 0.00001912
Iteration 50/1000 | Loss: 0.00001911
Iteration 51/1000 | Loss: 0.00001911
Iteration 52/1000 | Loss: 0.00001911
Iteration 53/1000 | Loss: 0.00001910
Iteration 54/1000 | Loss: 0.00001910
Iteration 55/1000 | Loss: 0.00001909
Iteration 56/1000 | Loss: 0.00001909
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00001908
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001904
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001904
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001904
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001902
Iteration 79/1000 | Loss: 0.00001902
Iteration 80/1000 | Loss: 0.00001902
Iteration 81/1000 | Loss: 0.00001901
Iteration 82/1000 | Loss: 0.00001901
Iteration 83/1000 | Loss: 0.00001900
Iteration 84/1000 | Loss: 0.00001900
Iteration 85/1000 | Loss: 0.00001899
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001899
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001899
Iteration 90/1000 | Loss: 0.00001899
Iteration 91/1000 | Loss: 0.00001899
Iteration 92/1000 | Loss: 0.00001899
Iteration 93/1000 | Loss: 0.00001899
Iteration 94/1000 | Loss: 0.00001899
Iteration 95/1000 | Loss: 0.00001899
Iteration 96/1000 | Loss: 0.00001898
Iteration 97/1000 | Loss: 0.00001898
Iteration 98/1000 | Loss: 0.00001898
Iteration 99/1000 | Loss: 0.00001898
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001897
Iteration 103/1000 | Loss: 0.00001896
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00001896
Iteration 106/1000 | Loss: 0.00001896
Iteration 107/1000 | Loss: 0.00001896
Iteration 108/1000 | Loss: 0.00001896
Iteration 109/1000 | Loss: 0.00001896
Iteration 110/1000 | Loss: 0.00001896
Iteration 111/1000 | Loss: 0.00001895
Iteration 112/1000 | Loss: 0.00001895
Iteration 113/1000 | Loss: 0.00001895
Iteration 114/1000 | Loss: 0.00001895
Iteration 115/1000 | Loss: 0.00001895
Iteration 116/1000 | Loss: 0.00001895
Iteration 117/1000 | Loss: 0.00001894
Iteration 118/1000 | Loss: 0.00001894
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001894
Iteration 124/1000 | Loss: 0.00001893
Iteration 125/1000 | Loss: 0.00001893
Iteration 126/1000 | Loss: 0.00001893
Iteration 127/1000 | Loss: 0.00001893
Iteration 128/1000 | Loss: 0.00001893
Iteration 129/1000 | Loss: 0.00001893
Iteration 130/1000 | Loss: 0.00001893
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001893
Iteration 136/1000 | Loss: 0.00001893
Iteration 137/1000 | Loss: 0.00001892
Iteration 138/1000 | Loss: 0.00001892
Iteration 139/1000 | Loss: 0.00001892
Iteration 140/1000 | Loss: 0.00001892
Iteration 141/1000 | Loss: 0.00001892
Iteration 142/1000 | Loss: 0.00001892
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001891
Iteration 145/1000 | Loss: 0.00001891
Iteration 146/1000 | Loss: 0.00001891
Iteration 147/1000 | Loss: 0.00001891
Iteration 148/1000 | Loss: 0.00001891
Iteration 149/1000 | Loss: 0.00001891
Iteration 150/1000 | Loss: 0.00001891
Iteration 151/1000 | Loss: 0.00001891
Iteration 152/1000 | Loss: 0.00001891
Iteration 153/1000 | Loss: 0.00001891
Iteration 154/1000 | Loss: 0.00001891
Iteration 155/1000 | Loss: 0.00001891
Iteration 156/1000 | Loss: 0.00001891
Iteration 157/1000 | Loss: 0.00001891
Iteration 158/1000 | Loss: 0.00001891
Iteration 159/1000 | Loss: 0.00001891
Iteration 160/1000 | Loss: 0.00001890
Iteration 161/1000 | Loss: 0.00001890
Iteration 162/1000 | Loss: 0.00001890
Iteration 163/1000 | Loss: 0.00001890
Iteration 164/1000 | Loss: 0.00001889
Iteration 165/1000 | Loss: 0.00001889
Iteration 166/1000 | Loss: 0.00001889
Iteration 167/1000 | Loss: 0.00001889
Iteration 168/1000 | Loss: 0.00001889
Iteration 169/1000 | Loss: 0.00001889
Iteration 170/1000 | Loss: 0.00001889
Iteration 171/1000 | Loss: 0.00001888
Iteration 172/1000 | Loss: 0.00001888
Iteration 173/1000 | Loss: 0.00001888
Iteration 174/1000 | Loss: 0.00001888
Iteration 175/1000 | Loss: 0.00001888
Iteration 176/1000 | Loss: 0.00001888
Iteration 177/1000 | Loss: 0.00001888
Iteration 178/1000 | Loss: 0.00001888
Iteration 179/1000 | Loss: 0.00001888
Iteration 180/1000 | Loss: 0.00001887
Iteration 181/1000 | Loss: 0.00001887
Iteration 182/1000 | Loss: 0.00001887
Iteration 183/1000 | Loss: 0.00001887
Iteration 184/1000 | Loss: 0.00001887
Iteration 185/1000 | Loss: 0.00001887
Iteration 186/1000 | Loss: 0.00001887
Iteration 187/1000 | Loss: 0.00001887
Iteration 188/1000 | Loss: 0.00001887
Iteration 189/1000 | Loss: 0.00001887
Iteration 190/1000 | Loss: 0.00001886
Iteration 191/1000 | Loss: 0.00001886
Iteration 192/1000 | Loss: 0.00001886
Iteration 193/1000 | Loss: 0.00001886
Iteration 194/1000 | Loss: 0.00001886
Iteration 195/1000 | Loss: 0.00001886
Iteration 196/1000 | Loss: 0.00001886
Iteration 197/1000 | Loss: 0.00001886
Iteration 198/1000 | Loss: 0.00001886
Iteration 199/1000 | Loss: 0.00001886
Iteration 200/1000 | Loss: 0.00001886
Iteration 201/1000 | Loss: 0.00001886
Iteration 202/1000 | Loss: 0.00001886
Iteration 203/1000 | Loss: 0.00001886
Iteration 204/1000 | Loss: 0.00001886
Iteration 205/1000 | Loss: 0.00001886
Iteration 206/1000 | Loss: 0.00001886
Iteration 207/1000 | Loss: 0.00001886
Iteration 208/1000 | Loss: 0.00001886
Iteration 209/1000 | Loss: 0.00001886
Iteration 210/1000 | Loss: 0.00001886
Iteration 211/1000 | Loss: 0.00001886
Iteration 212/1000 | Loss: 0.00001886
Iteration 213/1000 | Loss: 0.00001886
Iteration 214/1000 | Loss: 0.00001886
Iteration 215/1000 | Loss: 0.00001886
Iteration 216/1000 | Loss: 0.00001886
Iteration 217/1000 | Loss: 0.00001886
Iteration 218/1000 | Loss: 0.00001886
Iteration 219/1000 | Loss: 0.00001886
Iteration 220/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.886071913759224e-05, 1.886071913759224e-05, 1.886071913759224e-05, 1.886071913759224e-05, 1.886071913759224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.886071913759224e-05

Optimization complete. Final v2v error: 3.6970536708831787 mm

Highest mean error: 4.044609546661377 mm for frame 52

Lowest mean error: 3.447159767150879 mm for frame 155

Saving results

Total time: 39.759366273880005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432242
Iteration 2/25 | Loss: 0.00134111
Iteration 3/25 | Loss: 0.00126251
Iteration 4/25 | Loss: 0.00124447
Iteration 5/25 | Loss: 0.00123933
Iteration 6/25 | Loss: 0.00123824
Iteration 7/25 | Loss: 0.00123764
Iteration 8/25 | Loss: 0.00123764
Iteration 9/25 | Loss: 0.00123764
Iteration 10/25 | Loss: 0.00123764
Iteration 11/25 | Loss: 0.00123764
Iteration 12/25 | Loss: 0.00123764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012376413214951754, 0.0012376413214951754, 0.0012376413214951754, 0.0012376413214951754, 0.0012376413214951754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012376413214951754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91557527
Iteration 2/25 | Loss: 0.00077440
Iteration 3/25 | Loss: 0.00077440
Iteration 4/25 | Loss: 0.00077440
Iteration 5/25 | Loss: 0.00077440
Iteration 6/25 | Loss: 0.00077440
Iteration 7/25 | Loss: 0.00077440
Iteration 8/25 | Loss: 0.00077440
Iteration 9/25 | Loss: 0.00077440
Iteration 10/25 | Loss: 0.00077440
Iteration 11/25 | Loss: 0.00077440
Iteration 12/25 | Loss: 0.00077440
Iteration 13/25 | Loss: 0.00077440
Iteration 14/25 | Loss: 0.00077440
Iteration 15/25 | Loss: 0.00077440
Iteration 16/25 | Loss: 0.00077440
Iteration 17/25 | Loss: 0.00077440
Iteration 18/25 | Loss: 0.00077440
Iteration 19/25 | Loss: 0.00077440
Iteration 20/25 | Loss: 0.00077440
Iteration 21/25 | Loss: 0.00077440
Iteration 22/25 | Loss: 0.00077440
Iteration 23/25 | Loss: 0.00077440
Iteration 24/25 | Loss: 0.00077440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007743994356133044, 0.0007743994356133044, 0.0007743994356133044, 0.0007743994356133044, 0.0007743994356133044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007743994356133044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077440
Iteration 2/1000 | Loss: 0.00003343
Iteration 3/1000 | Loss: 0.00002151
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001845
Iteration 6/1000 | Loss: 0.00001787
Iteration 7/1000 | Loss: 0.00001747
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001702
Iteration 10/1000 | Loss: 0.00001682
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001648
Iteration 14/1000 | Loss: 0.00001642
Iteration 15/1000 | Loss: 0.00001637
Iteration 16/1000 | Loss: 0.00001635
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001622
Iteration 20/1000 | Loss: 0.00001621
Iteration 21/1000 | Loss: 0.00001619
Iteration 22/1000 | Loss: 0.00001618
Iteration 23/1000 | Loss: 0.00001618
Iteration 24/1000 | Loss: 0.00001618
Iteration 25/1000 | Loss: 0.00001618
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001614
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001612
Iteration 32/1000 | Loss: 0.00001612
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001610
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001608
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001607
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001606
Iteration 44/1000 | Loss: 0.00001606
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001600
Iteration 51/1000 | Loss: 0.00001600
Iteration 52/1000 | Loss: 0.00001598
Iteration 53/1000 | Loss: 0.00001597
Iteration 54/1000 | Loss: 0.00001597
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001596
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001596
Iteration 62/1000 | Loss: 0.00001595
Iteration 63/1000 | Loss: 0.00001595
Iteration 64/1000 | Loss: 0.00001594
Iteration 65/1000 | Loss: 0.00001594
Iteration 66/1000 | Loss: 0.00001594
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001593
Iteration 69/1000 | Loss: 0.00001593
Iteration 70/1000 | Loss: 0.00001593
Iteration 71/1000 | Loss: 0.00001593
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001592
Iteration 74/1000 | Loss: 0.00001592
Iteration 75/1000 | Loss: 0.00001592
Iteration 76/1000 | Loss: 0.00001591
Iteration 77/1000 | Loss: 0.00001591
Iteration 78/1000 | Loss: 0.00001591
Iteration 79/1000 | Loss: 0.00001591
Iteration 80/1000 | Loss: 0.00001590
Iteration 81/1000 | Loss: 0.00001590
Iteration 82/1000 | Loss: 0.00001590
Iteration 83/1000 | Loss: 0.00001590
Iteration 84/1000 | Loss: 0.00001590
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001589
Iteration 88/1000 | Loss: 0.00001589
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001588
Iteration 94/1000 | Loss: 0.00001588
Iteration 95/1000 | Loss: 0.00001588
Iteration 96/1000 | Loss: 0.00001588
Iteration 97/1000 | Loss: 0.00001588
Iteration 98/1000 | Loss: 0.00001588
Iteration 99/1000 | Loss: 0.00001588
Iteration 100/1000 | Loss: 0.00001588
Iteration 101/1000 | Loss: 0.00001588
Iteration 102/1000 | Loss: 0.00001587
Iteration 103/1000 | Loss: 0.00001587
Iteration 104/1000 | Loss: 0.00001587
Iteration 105/1000 | Loss: 0.00001587
Iteration 106/1000 | Loss: 0.00001587
Iteration 107/1000 | Loss: 0.00001587
Iteration 108/1000 | Loss: 0.00001587
Iteration 109/1000 | Loss: 0.00001587
Iteration 110/1000 | Loss: 0.00001587
Iteration 111/1000 | Loss: 0.00001587
Iteration 112/1000 | Loss: 0.00001587
Iteration 113/1000 | Loss: 0.00001587
Iteration 114/1000 | Loss: 0.00001587
Iteration 115/1000 | Loss: 0.00001587
Iteration 116/1000 | Loss: 0.00001587
Iteration 117/1000 | Loss: 0.00001587
Iteration 118/1000 | Loss: 0.00001587
Iteration 119/1000 | Loss: 0.00001587
Iteration 120/1000 | Loss: 0.00001587
Iteration 121/1000 | Loss: 0.00001587
Iteration 122/1000 | Loss: 0.00001587
Iteration 123/1000 | Loss: 0.00001587
Iteration 124/1000 | Loss: 0.00001587
Iteration 125/1000 | Loss: 0.00001587
Iteration 126/1000 | Loss: 0.00001587
Iteration 127/1000 | Loss: 0.00001587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5873665688559413e-05, 1.5873665688559413e-05, 1.5873665688559413e-05, 1.5873665688559413e-05, 1.5873665688559413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5873665688559413e-05

Optimization complete. Final v2v error: 3.368999719619751 mm

Highest mean error: 3.975376844406128 mm for frame 107

Lowest mean error: 3.1767866611480713 mm for frame 129

Saving results

Total time: 39.576279640197754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487359
Iteration 2/25 | Loss: 0.00138007
Iteration 3/25 | Loss: 0.00129397
Iteration 4/25 | Loss: 0.00128658
Iteration 5/25 | Loss: 0.00128442
Iteration 6/25 | Loss: 0.00128442
Iteration 7/25 | Loss: 0.00128442
Iteration 8/25 | Loss: 0.00128442
Iteration 9/25 | Loss: 0.00128442
Iteration 10/25 | Loss: 0.00128442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012844243319705129, 0.0012844243319705129, 0.0012844243319705129, 0.0012844243319705129, 0.0012844243319705129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012844243319705129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50181150
Iteration 2/25 | Loss: 0.00084605
Iteration 3/25 | Loss: 0.00084605
Iteration 4/25 | Loss: 0.00084605
Iteration 5/25 | Loss: 0.00084605
Iteration 6/25 | Loss: 0.00084605
Iteration 7/25 | Loss: 0.00084604
Iteration 8/25 | Loss: 0.00084604
Iteration 9/25 | Loss: 0.00084604
Iteration 10/25 | Loss: 0.00084604
Iteration 11/25 | Loss: 0.00084604
Iteration 12/25 | Loss: 0.00084604
Iteration 13/25 | Loss: 0.00084604
Iteration 14/25 | Loss: 0.00084604
Iteration 15/25 | Loss: 0.00084604
Iteration 16/25 | Loss: 0.00084604
Iteration 17/25 | Loss: 0.00084604
Iteration 18/25 | Loss: 0.00084604
Iteration 19/25 | Loss: 0.00084604
Iteration 20/25 | Loss: 0.00084604
Iteration 21/25 | Loss: 0.00084604
Iteration 22/25 | Loss: 0.00084604
Iteration 23/25 | Loss: 0.00084604
Iteration 24/25 | Loss: 0.00084604
Iteration 25/25 | Loss: 0.00084604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084604
Iteration 2/1000 | Loss: 0.00003944
Iteration 3/1000 | Loss: 0.00002436
Iteration 4/1000 | Loss: 0.00002204
Iteration 5/1000 | Loss: 0.00002079
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001923
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00001852
Iteration 10/1000 | Loss: 0.00001829
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001804
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001791
Iteration 21/1000 | Loss: 0.00001790
Iteration 22/1000 | Loss: 0.00001790
Iteration 23/1000 | Loss: 0.00001787
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001784
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001783
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001782
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001777
Iteration 33/1000 | Loss: 0.00001777
Iteration 34/1000 | Loss: 0.00001773
Iteration 35/1000 | Loss: 0.00001772
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001771
Iteration 38/1000 | Loss: 0.00001770
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001768
Iteration 41/1000 | Loss: 0.00001768
Iteration 42/1000 | Loss: 0.00001767
Iteration 43/1000 | Loss: 0.00001766
Iteration 44/1000 | Loss: 0.00001765
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001755
Iteration 57/1000 | Loss: 0.00001755
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001755
Iteration 60/1000 | Loss: 0.00001755
Iteration 61/1000 | Loss: 0.00001755
Iteration 62/1000 | Loss: 0.00001755
Iteration 63/1000 | Loss: 0.00001755
Iteration 64/1000 | Loss: 0.00001755
Iteration 65/1000 | Loss: 0.00001755
Iteration 66/1000 | Loss: 0.00001755
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001755
Iteration 74/1000 | Loss: 0.00001755
Iteration 75/1000 | Loss: 0.00001755
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001750
Iteration 84/1000 | Loss: 0.00001749
Iteration 85/1000 | Loss: 0.00001748
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001747
Iteration 92/1000 | Loss: 0.00001747
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001744
Iteration 100/1000 | Loss: 0.00001744
Iteration 101/1000 | Loss: 0.00001744
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001743
Iteration 108/1000 | Loss: 0.00001743
Iteration 109/1000 | Loss: 0.00001743
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001742
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001741
Iteration 118/1000 | Loss: 0.00001741
Iteration 119/1000 | Loss: 0.00001741
Iteration 120/1000 | Loss: 0.00001741
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001741
Iteration 125/1000 | Loss: 0.00001741
Iteration 126/1000 | Loss: 0.00001741
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001739
Iteration 134/1000 | Loss: 0.00001739
Iteration 135/1000 | Loss: 0.00001739
Iteration 136/1000 | Loss: 0.00001739
Iteration 137/1000 | Loss: 0.00001739
Iteration 138/1000 | Loss: 0.00001739
Iteration 139/1000 | Loss: 0.00001739
Iteration 140/1000 | Loss: 0.00001739
Iteration 141/1000 | Loss: 0.00001739
Iteration 142/1000 | Loss: 0.00001738
Iteration 143/1000 | Loss: 0.00001738
Iteration 144/1000 | Loss: 0.00001738
Iteration 145/1000 | Loss: 0.00001738
Iteration 146/1000 | Loss: 0.00001738
Iteration 147/1000 | Loss: 0.00001738
Iteration 148/1000 | Loss: 0.00001738
Iteration 149/1000 | Loss: 0.00001738
Iteration 150/1000 | Loss: 0.00001738
Iteration 151/1000 | Loss: 0.00001737
Iteration 152/1000 | Loss: 0.00001737
Iteration 153/1000 | Loss: 0.00001737
Iteration 154/1000 | Loss: 0.00001737
Iteration 155/1000 | Loss: 0.00001737
Iteration 156/1000 | Loss: 0.00001737
Iteration 157/1000 | Loss: 0.00001736
Iteration 158/1000 | Loss: 0.00001736
Iteration 159/1000 | Loss: 0.00001736
Iteration 160/1000 | Loss: 0.00001736
Iteration 161/1000 | Loss: 0.00001736
Iteration 162/1000 | Loss: 0.00001736
Iteration 163/1000 | Loss: 0.00001736
Iteration 164/1000 | Loss: 0.00001736
Iteration 165/1000 | Loss: 0.00001736
Iteration 166/1000 | Loss: 0.00001735
Iteration 167/1000 | Loss: 0.00001735
Iteration 168/1000 | Loss: 0.00001735
Iteration 169/1000 | Loss: 0.00001735
Iteration 170/1000 | Loss: 0.00001735
Iteration 171/1000 | Loss: 0.00001735
Iteration 172/1000 | Loss: 0.00001735
Iteration 173/1000 | Loss: 0.00001735
Iteration 174/1000 | Loss: 0.00001735
Iteration 175/1000 | Loss: 0.00001734
Iteration 176/1000 | Loss: 0.00001734
Iteration 177/1000 | Loss: 0.00001734
Iteration 178/1000 | Loss: 0.00001734
Iteration 179/1000 | Loss: 0.00001734
Iteration 180/1000 | Loss: 0.00001734
Iteration 181/1000 | Loss: 0.00001734
Iteration 182/1000 | Loss: 0.00001733
Iteration 183/1000 | Loss: 0.00001733
Iteration 184/1000 | Loss: 0.00001733
Iteration 185/1000 | Loss: 0.00001733
Iteration 186/1000 | Loss: 0.00001733
Iteration 187/1000 | Loss: 0.00001733
Iteration 188/1000 | Loss: 0.00001733
Iteration 189/1000 | Loss: 0.00001733
Iteration 190/1000 | Loss: 0.00001733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.7333948562736623e-05, 1.7333948562736623e-05, 1.7333948562736623e-05, 1.7333948562736623e-05, 1.7333948562736623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7333948562736623e-05

Optimization complete. Final v2v error: 3.4917266368865967 mm

Highest mean error: 3.813782215118408 mm for frame 103

Lowest mean error: 3.2628400325775146 mm for frame 119

Saving results

Total time: 44.934306621551514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537945
Iteration 2/25 | Loss: 0.00150482
Iteration 3/25 | Loss: 0.00134308
Iteration 4/25 | Loss: 0.00132663
Iteration 5/25 | Loss: 0.00132324
Iteration 6/25 | Loss: 0.00132324
Iteration 7/25 | Loss: 0.00132324
Iteration 8/25 | Loss: 0.00132324
Iteration 9/25 | Loss: 0.00132324
Iteration 10/25 | Loss: 0.00132324
Iteration 11/25 | Loss: 0.00132324
Iteration 12/25 | Loss: 0.00132324
Iteration 13/25 | Loss: 0.00132324
Iteration 14/25 | Loss: 0.00132324
Iteration 15/25 | Loss: 0.00132324
Iteration 16/25 | Loss: 0.00132324
Iteration 17/25 | Loss: 0.00132324
Iteration 18/25 | Loss: 0.00132324
Iteration 19/25 | Loss: 0.00132324
Iteration 20/25 | Loss: 0.00132324
Iteration 21/25 | Loss: 0.00132324
Iteration 22/25 | Loss: 0.00132324
Iteration 23/25 | Loss: 0.00132324
Iteration 24/25 | Loss: 0.00132324
Iteration 25/25 | Loss: 0.00132324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84015018
Iteration 2/25 | Loss: 0.00082969
Iteration 3/25 | Loss: 0.00082969
Iteration 4/25 | Loss: 0.00082969
Iteration 5/25 | Loss: 0.00082969
Iteration 6/25 | Loss: 0.00082968
Iteration 7/25 | Loss: 0.00082968
Iteration 8/25 | Loss: 0.00082968
Iteration 9/25 | Loss: 0.00082968
Iteration 10/25 | Loss: 0.00082968
Iteration 11/25 | Loss: 0.00082968
Iteration 12/25 | Loss: 0.00082968
Iteration 13/25 | Loss: 0.00082968
Iteration 14/25 | Loss: 0.00082968
Iteration 15/25 | Loss: 0.00082968
Iteration 16/25 | Loss: 0.00082968
Iteration 17/25 | Loss: 0.00082968
Iteration 18/25 | Loss: 0.00082968
Iteration 19/25 | Loss: 0.00082968
Iteration 20/25 | Loss: 0.00082968
Iteration 21/25 | Loss: 0.00082968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008296833257190883, 0.0008296833257190883, 0.0008296833257190883, 0.0008296833257190883, 0.0008296833257190883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008296833257190883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082968
Iteration 2/1000 | Loss: 0.00003866
Iteration 3/1000 | Loss: 0.00002555
Iteration 4/1000 | Loss: 0.00002294
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001969
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001897
Iteration 10/1000 | Loss: 0.00001859
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001809
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001764
Iteration 15/1000 | Loss: 0.00001757
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001748
Iteration 19/1000 | Loss: 0.00001727
Iteration 20/1000 | Loss: 0.00001711
Iteration 21/1000 | Loss: 0.00001702
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001686
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001685
Iteration 28/1000 | Loss: 0.00001680
Iteration 29/1000 | Loss: 0.00001680
Iteration 30/1000 | Loss: 0.00001678
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001677
Iteration 33/1000 | Loss: 0.00001677
Iteration 34/1000 | Loss: 0.00001677
Iteration 35/1000 | Loss: 0.00001677
Iteration 36/1000 | Loss: 0.00001677
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001677
Iteration 40/1000 | Loss: 0.00001676
Iteration 41/1000 | Loss: 0.00001674
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001673
Iteration 44/1000 | Loss: 0.00001673
Iteration 45/1000 | Loss: 0.00001673
Iteration 46/1000 | Loss: 0.00001672
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001671
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001669
Iteration 51/1000 | Loss: 0.00001669
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00001668
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001667
Iteration 59/1000 | Loss: 0.00001666
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001666
Iteration 62/1000 | Loss: 0.00001666
Iteration 63/1000 | Loss: 0.00001664
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00001664
Iteration 67/1000 | Loss: 0.00001664
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001664
Iteration 72/1000 | Loss: 0.00001664
Iteration 73/1000 | Loss: 0.00001663
Iteration 74/1000 | Loss: 0.00001663
Iteration 75/1000 | Loss: 0.00001662
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001660
Iteration 78/1000 | Loss: 0.00001660
Iteration 79/1000 | Loss: 0.00001660
Iteration 80/1000 | Loss: 0.00001660
Iteration 81/1000 | Loss: 0.00001660
Iteration 82/1000 | Loss: 0.00001660
Iteration 83/1000 | Loss: 0.00001660
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001660
Iteration 88/1000 | Loss: 0.00001659
Iteration 89/1000 | Loss: 0.00001659
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001658
Iteration 93/1000 | Loss: 0.00001658
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001657
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001657
Iteration 98/1000 | Loss: 0.00001657
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001656
Iteration 103/1000 | Loss: 0.00001656
Iteration 104/1000 | Loss: 0.00001656
Iteration 105/1000 | Loss: 0.00001656
Iteration 106/1000 | Loss: 0.00001656
Iteration 107/1000 | Loss: 0.00001656
Iteration 108/1000 | Loss: 0.00001656
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001656
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001656
Iteration 113/1000 | Loss: 0.00001655
Iteration 114/1000 | Loss: 0.00001655
Iteration 115/1000 | Loss: 0.00001654
Iteration 116/1000 | Loss: 0.00001654
Iteration 117/1000 | Loss: 0.00001653
Iteration 118/1000 | Loss: 0.00001653
Iteration 119/1000 | Loss: 0.00001653
Iteration 120/1000 | Loss: 0.00001653
Iteration 121/1000 | Loss: 0.00001653
Iteration 122/1000 | Loss: 0.00001653
Iteration 123/1000 | Loss: 0.00001653
Iteration 124/1000 | Loss: 0.00001653
Iteration 125/1000 | Loss: 0.00001653
Iteration 126/1000 | Loss: 0.00001653
Iteration 127/1000 | Loss: 0.00001653
Iteration 128/1000 | Loss: 0.00001653
Iteration 129/1000 | Loss: 0.00001653
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001652
Iteration 132/1000 | Loss: 0.00001652
Iteration 133/1000 | Loss: 0.00001652
Iteration 134/1000 | Loss: 0.00001652
Iteration 135/1000 | Loss: 0.00001652
Iteration 136/1000 | Loss: 0.00001652
Iteration 137/1000 | Loss: 0.00001652
Iteration 138/1000 | Loss: 0.00001651
Iteration 139/1000 | Loss: 0.00001651
Iteration 140/1000 | Loss: 0.00001651
Iteration 141/1000 | Loss: 0.00001651
Iteration 142/1000 | Loss: 0.00001651
Iteration 143/1000 | Loss: 0.00001651
Iteration 144/1000 | Loss: 0.00001651
Iteration 145/1000 | Loss: 0.00001651
Iteration 146/1000 | Loss: 0.00001651
Iteration 147/1000 | Loss: 0.00001651
Iteration 148/1000 | Loss: 0.00001651
Iteration 149/1000 | Loss: 0.00001650
Iteration 150/1000 | Loss: 0.00001650
Iteration 151/1000 | Loss: 0.00001650
Iteration 152/1000 | Loss: 0.00001650
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001648
Iteration 160/1000 | Loss: 0.00001648
Iteration 161/1000 | Loss: 0.00001647
Iteration 162/1000 | Loss: 0.00001647
Iteration 163/1000 | Loss: 0.00001647
Iteration 164/1000 | Loss: 0.00001646
Iteration 165/1000 | Loss: 0.00001646
Iteration 166/1000 | Loss: 0.00001645
Iteration 167/1000 | Loss: 0.00001645
Iteration 168/1000 | Loss: 0.00001645
Iteration 169/1000 | Loss: 0.00001644
Iteration 170/1000 | Loss: 0.00001644
Iteration 171/1000 | Loss: 0.00001644
Iteration 172/1000 | Loss: 0.00001643
Iteration 173/1000 | Loss: 0.00001643
Iteration 174/1000 | Loss: 0.00001643
Iteration 175/1000 | Loss: 0.00001643
Iteration 176/1000 | Loss: 0.00001643
Iteration 177/1000 | Loss: 0.00001642
Iteration 178/1000 | Loss: 0.00001642
Iteration 179/1000 | Loss: 0.00001641
Iteration 180/1000 | Loss: 0.00001641
Iteration 181/1000 | Loss: 0.00001640
Iteration 182/1000 | Loss: 0.00001640
Iteration 183/1000 | Loss: 0.00001640
Iteration 184/1000 | Loss: 0.00001640
Iteration 185/1000 | Loss: 0.00001640
Iteration 186/1000 | Loss: 0.00001639
Iteration 187/1000 | Loss: 0.00001639
Iteration 188/1000 | Loss: 0.00001639
Iteration 189/1000 | Loss: 0.00001639
Iteration 190/1000 | Loss: 0.00001639
Iteration 191/1000 | Loss: 0.00001639
Iteration 192/1000 | Loss: 0.00001638
Iteration 193/1000 | Loss: 0.00001638
Iteration 194/1000 | Loss: 0.00001638
Iteration 195/1000 | Loss: 0.00001638
Iteration 196/1000 | Loss: 0.00001638
Iteration 197/1000 | Loss: 0.00001638
Iteration 198/1000 | Loss: 0.00001638
Iteration 199/1000 | Loss: 0.00001638
Iteration 200/1000 | Loss: 0.00001638
Iteration 201/1000 | Loss: 0.00001638
Iteration 202/1000 | Loss: 0.00001638
Iteration 203/1000 | Loss: 0.00001638
Iteration 204/1000 | Loss: 0.00001638
Iteration 205/1000 | Loss: 0.00001638
Iteration 206/1000 | Loss: 0.00001637
Iteration 207/1000 | Loss: 0.00001637
Iteration 208/1000 | Loss: 0.00001637
Iteration 209/1000 | Loss: 0.00001637
Iteration 210/1000 | Loss: 0.00001637
Iteration 211/1000 | Loss: 0.00001637
Iteration 212/1000 | Loss: 0.00001637
Iteration 213/1000 | Loss: 0.00001637
Iteration 214/1000 | Loss: 0.00001637
Iteration 215/1000 | Loss: 0.00001637
Iteration 216/1000 | Loss: 0.00001637
Iteration 217/1000 | Loss: 0.00001637
Iteration 218/1000 | Loss: 0.00001637
Iteration 219/1000 | Loss: 0.00001637
Iteration 220/1000 | Loss: 0.00001637
Iteration 221/1000 | Loss: 0.00001637
Iteration 222/1000 | Loss: 0.00001637
Iteration 223/1000 | Loss: 0.00001637
Iteration 224/1000 | Loss: 0.00001637
Iteration 225/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.637328023207374e-05, 1.637328023207374e-05, 1.637328023207374e-05, 1.637328023207374e-05, 1.637328023207374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.637328023207374e-05

Optimization complete. Final v2v error: 3.4170377254486084 mm

Highest mean error: 3.9046475887298584 mm for frame 239

Lowest mean error: 3.3605072498321533 mm for frame 147

Saving results

Total time: 55.615426540374756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366012
Iteration 2/25 | Loss: 0.00138083
Iteration 3/25 | Loss: 0.00123997
Iteration 4/25 | Loss: 0.00121801
Iteration 5/25 | Loss: 0.00121181
Iteration 6/25 | Loss: 0.00120964
Iteration 7/25 | Loss: 0.00120945
Iteration 8/25 | Loss: 0.00120945
Iteration 9/25 | Loss: 0.00120945
Iteration 10/25 | Loss: 0.00120945
Iteration 11/25 | Loss: 0.00120945
Iteration 12/25 | Loss: 0.00120945
Iteration 13/25 | Loss: 0.00120945
Iteration 14/25 | Loss: 0.00120945
Iteration 15/25 | Loss: 0.00120945
Iteration 16/25 | Loss: 0.00120945
Iteration 17/25 | Loss: 0.00120945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001209450070746243, 0.001209450070746243, 0.001209450070746243, 0.001209450070746243, 0.001209450070746243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001209450070746243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43854594
Iteration 2/25 | Loss: 0.00060027
Iteration 3/25 | Loss: 0.00060027
Iteration 4/25 | Loss: 0.00060027
Iteration 5/25 | Loss: 0.00060027
Iteration 6/25 | Loss: 0.00060027
Iteration 7/25 | Loss: 0.00060027
Iteration 8/25 | Loss: 0.00060027
Iteration 9/25 | Loss: 0.00060027
Iteration 10/25 | Loss: 0.00060027
Iteration 11/25 | Loss: 0.00060027
Iteration 12/25 | Loss: 0.00060027
Iteration 13/25 | Loss: 0.00060027
Iteration 14/25 | Loss: 0.00060027
Iteration 15/25 | Loss: 0.00060027
Iteration 16/25 | Loss: 0.00060027
Iteration 17/25 | Loss: 0.00060027
Iteration 18/25 | Loss: 0.00060027
Iteration 19/25 | Loss: 0.00060027
Iteration 20/25 | Loss: 0.00060027
Iteration 21/25 | Loss: 0.00060027
Iteration 22/25 | Loss: 0.00060027
Iteration 23/25 | Loss: 0.00060027
Iteration 24/25 | Loss: 0.00060027
Iteration 25/25 | Loss: 0.00060027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006002659793011844, 0.0006002659793011844, 0.0006002659793011844, 0.0006002659793011844, 0.0006002659793011844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006002659793011844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060027
Iteration 2/1000 | Loss: 0.00003738
Iteration 3/1000 | Loss: 0.00002653
Iteration 4/1000 | Loss: 0.00002197
Iteration 5/1000 | Loss: 0.00002067
Iteration 6/1000 | Loss: 0.00001946
Iteration 7/1000 | Loss: 0.00001871
Iteration 8/1000 | Loss: 0.00001820
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001740
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001698
Iteration 16/1000 | Loss: 0.00001692
Iteration 17/1000 | Loss: 0.00001691
Iteration 18/1000 | Loss: 0.00001689
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001688
Iteration 21/1000 | Loss: 0.00001682
Iteration 22/1000 | Loss: 0.00001679
Iteration 23/1000 | Loss: 0.00001678
Iteration 24/1000 | Loss: 0.00001677
Iteration 25/1000 | Loss: 0.00001675
Iteration 26/1000 | Loss: 0.00001670
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001665
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001664
Iteration 33/1000 | Loss: 0.00001664
Iteration 34/1000 | Loss: 0.00001664
Iteration 35/1000 | Loss: 0.00001663
Iteration 36/1000 | Loss: 0.00001663
Iteration 37/1000 | Loss: 0.00001661
Iteration 38/1000 | Loss: 0.00001661
Iteration 39/1000 | Loss: 0.00001660
Iteration 40/1000 | Loss: 0.00001660
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00001659
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001658
Iteration 45/1000 | Loss: 0.00001658
Iteration 46/1000 | Loss: 0.00001657
Iteration 47/1000 | Loss: 0.00001657
Iteration 48/1000 | Loss: 0.00001656
Iteration 49/1000 | Loss: 0.00001656
Iteration 50/1000 | Loss: 0.00001655
Iteration 51/1000 | Loss: 0.00001655
Iteration 52/1000 | Loss: 0.00001654
Iteration 53/1000 | Loss: 0.00001654
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00001653
Iteration 56/1000 | Loss: 0.00001653
Iteration 57/1000 | Loss: 0.00001653
Iteration 58/1000 | Loss: 0.00001653
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001653
Iteration 61/1000 | Loss: 0.00001652
Iteration 62/1000 | Loss: 0.00001652
Iteration 63/1000 | Loss: 0.00001652
Iteration 64/1000 | Loss: 0.00001652
Iteration 65/1000 | Loss: 0.00001651
Iteration 66/1000 | Loss: 0.00001651
Iteration 67/1000 | Loss: 0.00001651
Iteration 68/1000 | Loss: 0.00001650
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001649
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001649
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001648
Iteration 78/1000 | Loss: 0.00001648
Iteration 79/1000 | Loss: 0.00001648
Iteration 80/1000 | Loss: 0.00001647
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001643
Iteration 101/1000 | Loss: 0.00001643
Iteration 102/1000 | Loss: 0.00001643
Iteration 103/1000 | Loss: 0.00001643
Iteration 104/1000 | Loss: 0.00001643
Iteration 105/1000 | Loss: 0.00001643
Iteration 106/1000 | Loss: 0.00001643
Iteration 107/1000 | Loss: 0.00001643
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001643
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001642
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001641
Iteration 118/1000 | Loss: 0.00001641
Iteration 119/1000 | Loss: 0.00001641
Iteration 120/1000 | Loss: 0.00001641
Iteration 121/1000 | Loss: 0.00001641
Iteration 122/1000 | Loss: 0.00001641
Iteration 123/1000 | Loss: 0.00001641
Iteration 124/1000 | Loss: 0.00001641
Iteration 125/1000 | Loss: 0.00001641
Iteration 126/1000 | Loss: 0.00001641
Iteration 127/1000 | Loss: 0.00001640
Iteration 128/1000 | Loss: 0.00001640
Iteration 129/1000 | Loss: 0.00001640
Iteration 130/1000 | Loss: 0.00001640
Iteration 131/1000 | Loss: 0.00001640
Iteration 132/1000 | Loss: 0.00001640
Iteration 133/1000 | Loss: 0.00001640
Iteration 134/1000 | Loss: 0.00001640
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001639
Iteration 141/1000 | Loss: 0.00001639
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001639
Iteration 150/1000 | Loss: 0.00001639
Iteration 151/1000 | Loss: 0.00001639
Iteration 152/1000 | Loss: 0.00001639
Iteration 153/1000 | Loss: 0.00001639
Iteration 154/1000 | Loss: 0.00001639
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001638
Iteration 159/1000 | Loss: 0.00001638
Iteration 160/1000 | Loss: 0.00001638
Iteration 161/1000 | Loss: 0.00001638
Iteration 162/1000 | Loss: 0.00001638
Iteration 163/1000 | Loss: 0.00001638
Iteration 164/1000 | Loss: 0.00001638
Iteration 165/1000 | Loss: 0.00001638
Iteration 166/1000 | Loss: 0.00001638
Iteration 167/1000 | Loss: 0.00001638
Iteration 168/1000 | Loss: 0.00001638
Iteration 169/1000 | Loss: 0.00001638
Iteration 170/1000 | Loss: 0.00001638
Iteration 171/1000 | Loss: 0.00001638
Iteration 172/1000 | Loss: 0.00001638
Iteration 173/1000 | Loss: 0.00001638
Iteration 174/1000 | Loss: 0.00001638
Iteration 175/1000 | Loss: 0.00001638
Iteration 176/1000 | Loss: 0.00001637
Iteration 177/1000 | Loss: 0.00001637
Iteration 178/1000 | Loss: 0.00001637
Iteration 179/1000 | Loss: 0.00001637
Iteration 180/1000 | Loss: 0.00001637
Iteration 181/1000 | Loss: 0.00001637
Iteration 182/1000 | Loss: 0.00001637
Iteration 183/1000 | Loss: 0.00001637
Iteration 184/1000 | Loss: 0.00001637
Iteration 185/1000 | Loss: 0.00001637
Iteration 186/1000 | Loss: 0.00001637
Iteration 187/1000 | Loss: 0.00001637
Iteration 188/1000 | Loss: 0.00001637
Iteration 189/1000 | Loss: 0.00001637
Iteration 190/1000 | Loss: 0.00001637
Iteration 191/1000 | Loss: 0.00001637
Iteration 192/1000 | Loss: 0.00001637
Iteration 193/1000 | Loss: 0.00001637
Iteration 194/1000 | Loss: 0.00001637
Iteration 195/1000 | Loss: 0.00001637
Iteration 196/1000 | Loss: 0.00001637
Iteration 197/1000 | Loss: 0.00001637
Iteration 198/1000 | Loss: 0.00001637
Iteration 199/1000 | Loss: 0.00001637
Iteration 200/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.6368250726372935e-05, 1.6368250726372935e-05, 1.6368250726372935e-05, 1.6368250726372935e-05, 1.6368250726372935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6368250726372935e-05

Optimization complete. Final v2v error: 3.454728364944458 mm

Highest mean error: 3.8649797439575195 mm for frame 71

Lowest mean error: 3.1542351245880127 mm for frame 158

Saving results

Total time: 42.78000354766846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474182
Iteration 2/25 | Loss: 0.00139245
Iteration 3/25 | Loss: 0.00127712
Iteration 4/25 | Loss: 0.00126530
Iteration 5/25 | Loss: 0.00126238
Iteration 6/25 | Loss: 0.00126236
Iteration 7/25 | Loss: 0.00126236
Iteration 8/25 | Loss: 0.00126236
Iteration 9/25 | Loss: 0.00126236
Iteration 10/25 | Loss: 0.00126236
Iteration 11/25 | Loss: 0.00126236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012623582733795047, 0.0012623582733795047, 0.0012623582733795047, 0.0012623582733795047, 0.0012623582733795047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012623582733795047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.24410725
Iteration 2/25 | Loss: 0.00071452
Iteration 3/25 | Loss: 0.00071451
Iteration 4/25 | Loss: 0.00071451
Iteration 5/25 | Loss: 0.00071451
Iteration 6/25 | Loss: 0.00071451
Iteration 7/25 | Loss: 0.00071450
Iteration 8/25 | Loss: 0.00071450
Iteration 9/25 | Loss: 0.00071450
Iteration 10/25 | Loss: 0.00071450
Iteration 11/25 | Loss: 0.00071450
Iteration 12/25 | Loss: 0.00071450
Iteration 13/25 | Loss: 0.00071450
Iteration 14/25 | Loss: 0.00071450
Iteration 15/25 | Loss: 0.00071450
Iteration 16/25 | Loss: 0.00071450
Iteration 17/25 | Loss: 0.00071450
Iteration 18/25 | Loss: 0.00071450
Iteration 19/25 | Loss: 0.00071450
Iteration 20/25 | Loss: 0.00071450
Iteration 21/25 | Loss: 0.00071450
Iteration 22/25 | Loss: 0.00071450
Iteration 23/25 | Loss: 0.00071450
Iteration 24/25 | Loss: 0.00071450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007145034032873809, 0.0007145034032873809, 0.0007145034032873809, 0.0007145034032873809, 0.0007145034032873809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007145034032873809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071450
Iteration 2/1000 | Loss: 0.00003505
Iteration 3/1000 | Loss: 0.00002268
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001943
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001803
Iteration 8/1000 | Loss: 0.00001767
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001725
Iteration 11/1000 | Loss: 0.00001710
Iteration 12/1000 | Loss: 0.00001707
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001684
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001668
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001652
Iteration 21/1000 | Loss: 0.00001651
Iteration 22/1000 | Loss: 0.00001651
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001637
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001637
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001635
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001632
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001631
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001630
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001629
Iteration 68/1000 | Loss: 0.00001629
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001621
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001619
Iteration 93/1000 | Loss: 0.00001619
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001618
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001617
Iteration 98/1000 | Loss: 0.00001617
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001615
Iteration 102/1000 | Loss: 0.00001615
Iteration 103/1000 | Loss: 0.00001615
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00001614
Iteration 106/1000 | Loss: 0.00001614
Iteration 107/1000 | Loss: 0.00001614
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001612
Iteration 114/1000 | Loss: 0.00001612
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001611
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001610
Iteration 125/1000 | Loss: 0.00001610
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001610
Iteration 128/1000 | Loss: 0.00001610
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001609
Iteration 132/1000 | Loss: 0.00001609
Iteration 133/1000 | Loss: 0.00001609
Iteration 134/1000 | Loss: 0.00001609
Iteration 135/1000 | Loss: 0.00001609
Iteration 136/1000 | Loss: 0.00001609
Iteration 137/1000 | Loss: 0.00001609
Iteration 138/1000 | Loss: 0.00001609
Iteration 139/1000 | Loss: 0.00001609
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001608
Iteration 142/1000 | Loss: 0.00001608
Iteration 143/1000 | Loss: 0.00001608
Iteration 144/1000 | Loss: 0.00001608
Iteration 145/1000 | Loss: 0.00001608
Iteration 146/1000 | Loss: 0.00001608
Iteration 147/1000 | Loss: 0.00001608
Iteration 148/1000 | Loss: 0.00001608
Iteration 149/1000 | Loss: 0.00001608
Iteration 150/1000 | Loss: 0.00001608
Iteration 151/1000 | Loss: 0.00001608
Iteration 152/1000 | Loss: 0.00001608
Iteration 153/1000 | Loss: 0.00001608
Iteration 154/1000 | Loss: 0.00001608
Iteration 155/1000 | Loss: 0.00001608
Iteration 156/1000 | Loss: 0.00001608
Iteration 157/1000 | Loss: 0.00001608
Iteration 158/1000 | Loss: 0.00001608
Iteration 159/1000 | Loss: 0.00001608
Iteration 160/1000 | Loss: 0.00001608
Iteration 161/1000 | Loss: 0.00001608
Iteration 162/1000 | Loss: 0.00001608
Iteration 163/1000 | Loss: 0.00001608
Iteration 164/1000 | Loss: 0.00001608
Iteration 165/1000 | Loss: 0.00001608
Iteration 166/1000 | Loss: 0.00001608
Iteration 167/1000 | Loss: 0.00001608
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.6080381101346575e-05, 1.6080381101346575e-05, 1.6080381101346575e-05, 1.6080381101346575e-05, 1.6080381101346575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6080381101346575e-05

Optimization complete. Final v2v error: 3.375192880630493 mm

Highest mean error: 3.5878076553344727 mm for frame 58

Lowest mean error: 3.104224681854248 mm for frame 136

Saving results

Total time: 38.25933361053467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493015
Iteration 2/25 | Loss: 0.00139964
Iteration 3/25 | Loss: 0.00131023
Iteration 4/25 | Loss: 0.00129534
Iteration 5/25 | Loss: 0.00129061
Iteration 6/25 | Loss: 0.00129011
Iteration 7/25 | Loss: 0.00129011
Iteration 8/25 | Loss: 0.00129011
Iteration 9/25 | Loss: 0.00129011
Iteration 10/25 | Loss: 0.00129011
Iteration 11/25 | Loss: 0.00129011
Iteration 12/25 | Loss: 0.00129011
Iteration 13/25 | Loss: 0.00129011
Iteration 14/25 | Loss: 0.00129011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012901137815788388, 0.0012901137815788388, 0.0012901137815788388, 0.0012901137815788388, 0.0012901137815788388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012901137815788388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48857570
Iteration 2/25 | Loss: 0.00080564
Iteration 3/25 | Loss: 0.00080564
Iteration 4/25 | Loss: 0.00080564
Iteration 5/25 | Loss: 0.00080564
Iteration 6/25 | Loss: 0.00080564
Iteration 7/25 | Loss: 0.00080564
Iteration 8/25 | Loss: 0.00080564
Iteration 9/25 | Loss: 0.00080564
Iteration 10/25 | Loss: 0.00080564
Iteration 11/25 | Loss: 0.00080564
Iteration 12/25 | Loss: 0.00080564
Iteration 13/25 | Loss: 0.00080564
Iteration 14/25 | Loss: 0.00080564
Iteration 15/25 | Loss: 0.00080564
Iteration 16/25 | Loss: 0.00080564
Iteration 17/25 | Loss: 0.00080564
Iteration 18/25 | Loss: 0.00080564
Iteration 19/25 | Loss: 0.00080564
Iteration 20/25 | Loss: 0.00080564
Iteration 21/25 | Loss: 0.00080564
Iteration 22/25 | Loss: 0.00080564
Iteration 23/25 | Loss: 0.00080564
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008056362858042121, 0.0008056362858042121, 0.0008056362858042121, 0.0008056362858042121, 0.0008056362858042121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008056362858042121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080564
Iteration 2/1000 | Loss: 0.00003924
Iteration 3/1000 | Loss: 0.00002952
Iteration 4/1000 | Loss: 0.00002730
Iteration 5/1000 | Loss: 0.00002635
Iteration 6/1000 | Loss: 0.00002570
Iteration 7/1000 | Loss: 0.00002521
Iteration 8/1000 | Loss: 0.00002477
Iteration 9/1000 | Loss: 0.00002448
Iteration 10/1000 | Loss: 0.00002414
Iteration 11/1000 | Loss: 0.00002393
Iteration 12/1000 | Loss: 0.00002371
Iteration 13/1000 | Loss: 0.00002357
Iteration 14/1000 | Loss: 0.00002350
Iteration 15/1000 | Loss: 0.00002349
Iteration 16/1000 | Loss: 0.00002346
Iteration 17/1000 | Loss: 0.00002339
Iteration 18/1000 | Loss: 0.00002331
Iteration 19/1000 | Loss: 0.00002326
Iteration 20/1000 | Loss: 0.00002325
Iteration 21/1000 | Loss: 0.00002325
Iteration 22/1000 | Loss: 0.00002321
Iteration 23/1000 | Loss: 0.00002320
Iteration 24/1000 | Loss: 0.00002316
Iteration 25/1000 | Loss: 0.00002314
Iteration 26/1000 | Loss: 0.00002314
Iteration 27/1000 | Loss: 0.00002313
Iteration 28/1000 | Loss: 0.00002313
Iteration 29/1000 | Loss: 0.00002312
Iteration 30/1000 | Loss: 0.00002312
Iteration 31/1000 | Loss: 0.00002312
Iteration 32/1000 | Loss: 0.00002311
Iteration 33/1000 | Loss: 0.00002311
Iteration 34/1000 | Loss: 0.00002311
Iteration 35/1000 | Loss: 0.00002310
Iteration 36/1000 | Loss: 0.00002310
Iteration 37/1000 | Loss: 0.00002310
Iteration 38/1000 | Loss: 0.00002309
Iteration 39/1000 | Loss: 0.00002309
Iteration 40/1000 | Loss: 0.00002309
Iteration 41/1000 | Loss: 0.00002309
Iteration 42/1000 | Loss: 0.00002309
Iteration 43/1000 | Loss: 0.00002308
Iteration 44/1000 | Loss: 0.00002308
Iteration 45/1000 | Loss: 0.00002307
Iteration 46/1000 | Loss: 0.00002307
Iteration 47/1000 | Loss: 0.00002307
Iteration 48/1000 | Loss: 0.00002307
Iteration 49/1000 | Loss: 0.00002307
Iteration 50/1000 | Loss: 0.00002306
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002306
Iteration 53/1000 | Loss: 0.00002305
Iteration 54/1000 | Loss: 0.00002305
Iteration 55/1000 | Loss: 0.00002305
Iteration 56/1000 | Loss: 0.00002305
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002305
Iteration 59/1000 | Loss: 0.00002304
Iteration 60/1000 | Loss: 0.00002304
Iteration 61/1000 | Loss: 0.00002304
Iteration 62/1000 | Loss: 0.00002304
Iteration 63/1000 | Loss: 0.00002303
Iteration 64/1000 | Loss: 0.00002303
Iteration 65/1000 | Loss: 0.00002303
Iteration 66/1000 | Loss: 0.00002302
Iteration 67/1000 | Loss: 0.00002302
Iteration 68/1000 | Loss: 0.00002302
Iteration 69/1000 | Loss: 0.00002301
Iteration 70/1000 | Loss: 0.00002301
Iteration 71/1000 | Loss: 0.00002301
Iteration 72/1000 | Loss: 0.00002301
Iteration 73/1000 | Loss: 0.00002301
Iteration 74/1000 | Loss: 0.00002301
Iteration 75/1000 | Loss: 0.00002301
Iteration 76/1000 | Loss: 0.00002301
Iteration 77/1000 | Loss: 0.00002300
Iteration 78/1000 | Loss: 0.00002300
Iteration 79/1000 | Loss: 0.00002300
Iteration 80/1000 | Loss: 0.00002300
Iteration 81/1000 | Loss: 0.00002299
Iteration 82/1000 | Loss: 0.00002299
Iteration 83/1000 | Loss: 0.00002299
Iteration 84/1000 | Loss: 0.00002299
Iteration 85/1000 | Loss: 0.00002299
Iteration 86/1000 | Loss: 0.00002299
Iteration 87/1000 | Loss: 0.00002298
Iteration 88/1000 | Loss: 0.00002298
Iteration 89/1000 | Loss: 0.00002298
Iteration 90/1000 | Loss: 0.00002298
Iteration 91/1000 | Loss: 0.00002298
Iteration 92/1000 | Loss: 0.00002298
Iteration 93/1000 | Loss: 0.00002297
Iteration 94/1000 | Loss: 0.00002297
Iteration 95/1000 | Loss: 0.00002297
Iteration 96/1000 | Loss: 0.00002297
Iteration 97/1000 | Loss: 0.00002297
Iteration 98/1000 | Loss: 0.00002297
Iteration 99/1000 | Loss: 0.00002297
Iteration 100/1000 | Loss: 0.00002297
Iteration 101/1000 | Loss: 0.00002297
Iteration 102/1000 | Loss: 0.00002297
Iteration 103/1000 | Loss: 0.00002296
Iteration 104/1000 | Loss: 0.00002296
Iteration 105/1000 | Loss: 0.00002296
Iteration 106/1000 | Loss: 0.00002296
Iteration 107/1000 | Loss: 0.00002296
Iteration 108/1000 | Loss: 0.00002296
Iteration 109/1000 | Loss: 0.00002296
Iteration 110/1000 | Loss: 0.00002296
Iteration 111/1000 | Loss: 0.00002296
Iteration 112/1000 | Loss: 0.00002296
Iteration 113/1000 | Loss: 0.00002296
Iteration 114/1000 | Loss: 0.00002296
Iteration 115/1000 | Loss: 0.00002296
Iteration 116/1000 | Loss: 0.00002296
Iteration 117/1000 | Loss: 0.00002296
Iteration 118/1000 | Loss: 0.00002296
Iteration 119/1000 | Loss: 0.00002296
Iteration 120/1000 | Loss: 0.00002296
Iteration 121/1000 | Loss: 0.00002296
Iteration 122/1000 | Loss: 0.00002296
Iteration 123/1000 | Loss: 0.00002296
Iteration 124/1000 | Loss: 0.00002296
Iteration 125/1000 | Loss: 0.00002296
Iteration 126/1000 | Loss: 0.00002296
Iteration 127/1000 | Loss: 0.00002296
Iteration 128/1000 | Loss: 0.00002296
Iteration 129/1000 | Loss: 0.00002296
Iteration 130/1000 | Loss: 0.00002296
Iteration 131/1000 | Loss: 0.00002296
Iteration 132/1000 | Loss: 0.00002296
Iteration 133/1000 | Loss: 0.00002296
Iteration 134/1000 | Loss: 0.00002296
Iteration 135/1000 | Loss: 0.00002296
Iteration 136/1000 | Loss: 0.00002296
Iteration 137/1000 | Loss: 0.00002296
Iteration 138/1000 | Loss: 0.00002296
Iteration 139/1000 | Loss: 0.00002296
Iteration 140/1000 | Loss: 0.00002296
Iteration 141/1000 | Loss: 0.00002296
Iteration 142/1000 | Loss: 0.00002296
Iteration 143/1000 | Loss: 0.00002296
Iteration 144/1000 | Loss: 0.00002296
Iteration 145/1000 | Loss: 0.00002296
Iteration 146/1000 | Loss: 0.00002296
Iteration 147/1000 | Loss: 0.00002296
Iteration 148/1000 | Loss: 0.00002296
Iteration 149/1000 | Loss: 0.00002296
Iteration 150/1000 | Loss: 0.00002296
Iteration 151/1000 | Loss: 0.00002296
Iteration 152/1000 | Loss: 0.00002296
Iteration 153/1000 | Loss: 0.00002296
Iteration 154/1000 | Loss: 0.00002296
Iteration 155/1000 | Loss: 0.00002296
Iteration 156/1000 | Loss: 0.00002296
Iteration 157/1000 | Loss: 0.00002296
Iteration 158/1000 | Loss: 0.00002296
Iteration 159/1000 | Loss: 0.00002296
Iteration 160/1000 | Loss: 0.00002296
Iteration 161/1000 | Loss: 0.00002296
Iteration 162/1000 | Loss: 0.00002296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.2963915398577228e-05, 2.2963915398577228e-05, 2.2963915398577228e-05, 2.2963915398577228e-05, 2.2963915398577228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2963915398577228e-05

Optimization complete. Final v2v error: 3.967529058456421 mm

Highest mean error: 5.472068786621094 mm for frame 39

Lowest mean error: 3.698885679244995 mm for frame 86

Saving results

Total time: 38.61782789230347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788268
Iteration 2/25 | Loss: 0.00139932
Iteration 3/25 | Loss: 0.00128954
Iteration 4/25 | Loss: 0.00126916
Iteration 5/25 | Loss: 0.00126171
Iteration 6/25 | Loss: 0.00126030
Iteration 7/25 | Loss: 0.00126030
Iteration 8/25 | Loss: 0.00126030
Iteration 9/25 | Loss: 0.00126030
Iteration 10/25 | Loss: 0.00126030
Iteration 11/25 | Loss: 0.00126030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012602992355823517, 0.0012602992355823517, 0.0012602992355823517, 0.0012602992355823517, 0.0012602992355823517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012602992355823517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.27714062
Iteration 2/25 | Loss: 0.00078824
Iteration 3/25 | Loss: 0.00078797
Iteration 4/25 | Loss: 0.00078797
Iteration 5/25 | Loss: 0.00078797
Iteration 6/25 | Loss: 0.00078797
Iteration 7/25 | Loss: 0.00078797
Iteration 8/25 | Loss: 0.00078797
Iteration 9/25 | Loss: 0.00078797
Iteration 10/25 | Loss: 0.00078797
Iteration 11/25 | Loss: 0.00078797
Iteration 12/25 | Loss: 0.00078797
Iteration 13/25 | Loss: 0.00078797
Iteration 14/25 | Loss: 0.00078797
Iteration 15/25 | Loss: 0.00078797
Iteration 16/25 | Loss: 0.00078797
Iteration 17/25 | Loss: 0.00078797
Iteration 18/25 | Loss: 0.00078797
Iteration 19/25 | Loss: 0.00078797
Iteration 20/25 | Loss: 0.00078797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007879712502472103, 0.0007879712502472103, 0.0007879712502472103, 0.0007879712502472103, 0.0007879712502472103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007879712502472103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078797
Iteration 2/1000 | Loss: 0.00006931
Iteration 3/1000 | Loss: 0.00004186
Iteration 4/1000 | Loss: 0.00003297
Iteration 5/1000 | Loss: 0.00002893
Iteration 6/1000 | Loss: 0.00002655
Iteration 7/1000 | Loss: 0.00002494
Iteration 8/1000 | Loss: 0.00002395
Iteration 9/1000 | Loss: 0.00002318
Iteration 10/1000 | Loss: 0.00002259
Iteration 11/1000 | Loss: 0.00002227
Iteration 12/1000 | Loss: 0.00002195
Iteration 13/1000 | Loss: 0.00002170
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002157
Iteration 16/1000 | Loss: 0.00002141
Iteration 17/1000 | Loss: 0.00002140
Iteration 18/1000 | Loss: 0.00002135
Iteration 19/1000 | Loss: 0.00002125
Iteration 20/1000 | Loss: 0.00002118
Iteration 21/1000 | Loss: 0.00002116
Iteration 22/1000 | Loss: 0.00002115
Iteration 23/1000 | Loss: 0.00002115
Iteration 24/1000 | Loss: 0.00002114
Iteration 25/1000 | Loss: 0.00002114
Iteration 26/1000 | Loss: 0.00002113
Iteration 27/1000 | Loss: 0.00002111
Iteration 28/1000 | Loss: 0.00002110
Iteration 29/1000 | Loss: 0.00002108
Iteration 30/1000 | Loss: 0.00002107
Iteration 31/1000 | Loss: 0.00002107
Iteration 32/1000 | Loss: 0.00002107
Iteration 33/1000 | Loss: 0.00002106
Iteration 34/1000 | Loss: 0.00002105
Iteration 35/1000 | Loss: 0.00002105
Iteration 36/1000 | Loss: 0.00002105
Iteration 37/1000 | Loss: 0.00002104
Iteration 38/1000 | Loss: 0.00002104
Iteration 39/1000 | Loss: 0.00002104
Iteration 40/1000 | Loss: 0.00002103
Iteration 41/1000 | Loss: 0.00002103
Iteration 42/1000 | Loss: 0.00002102
Iteration 43/1000 | Loss: 0.00002102
Iteration 44/1000 | Loss: 0.00002102
Iteration 45/1000 | Loss: 0.00002101
Iteration 46/1000 | Loss: 0.00002101
Iteration 47/1000 | Loss: 0.00002101
Iteration 48/1000 | Loss: 0.00002100
Iteration 49/1000 | Loss: 0.00002100
Iteration 50/1000 | Loss: 0.00002100
Iteration 51/1000 | Loss: 0.00002099
Iteration 52/1000 | Loss: 0.00002099
Iteration 53/1000 | Loss: 0.00002099
Iteration 54/1000 | Loss: 0.00002099
Iteration 55/1000 | Loss: 0.00002098
Iteration 56/1000 | Loss: 0.00002098
Iteration 57/1000 | Loss: 0.00002098
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002097
Iteration 61/1000 | Loss: 0.00002097
Iteration 62/1000 | Loss: 0.00002096
Iteration 63/1000 | Loss: 0.00002096
Iteration 64/1000 | Loss: 0.00002095
Iteration 65/1000 | Loss: 0.00002095
Iteration 66/1000 | Loss: 0.00002095
Iteration 67/1000 | Loss: 0.00002095
Iteration 68/1000 | Loss: 0.00002095
Iteration 69/1000 | Loss: 0.00002095
Iteration 70/1000 | Loss: 0.00002094
Iteration 71/1000 | Loss: 0.00002094
Iteration 72/1000 | Loss: 0.00002094
Iteration 73/1000 | Loss: 0.00002094
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002094
Iteration 76/1000 | Loss: 0.00002093
Iteration 77/1000 | Loss: 0.00002093
Iteration 78/1000 | Loss: 0.00002093
Iteration 79/1000 | Loss: 0.00002093
Iteration 80/1000 | Loss: 0.00002093
Iteration 81/1000 | Loss: 0.00002092
Iteration 82/1000 | Loss: 0.00002092
Iteration 83/1000 | Loss: 0.00002092
Iteration 84/1000 | Loss: 0.00002092
Iteration 85/1000 | Loss: 0.00002091
Iteration 86/1000 | Loss: 0.00002091
Iteration 87/1000 | Loss: 0.00002091
Iteration 88/1000 | Loss: 0.00002091
Iteration 89/1000 | Loss: 0.00002091
Iteration 90/1000 | Loss: 0.00002091
Iteration 91/1000 | Loss: 0.00002091
Iteration 92/1000 | Loss: 0.00002090
Iteration 93/1000 | Loss: 0.00002090
Iteration 94/1000 | Loss: 0.00002090
Iteration 95/1000 | Loss: 0.00002090
Iteration 96/1000 | Loss: 0.00002090
Iteration 97/1000 | Loss: 0.00002090
Iteration 98/1000 | Loss: 0.00002090
Iteration 99/1000 | Loss: 0.00002090
Iteration 100/1000 | Loss: 0.00002090
Iteration 101/1000 | Loss: 0.00002090
Iteration 102/1000 | Loss: 0.00002090
Iteration 103/1000 | Loss: 0.00002090
Iteration 104/1000 | Loss: 0.00002090
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002090
Iteration 108/1000 | Loss: 0.00002090
Iteration 109/1000 | Loss: 0.00002090
Iteration 110/1000 | Loss: 0.00002090
Iteration 111/1000 | Loss: 0.00002090
Iteration 112/1000 | Loss: 0.00002090
Iteration 113/1000 | Loss: 0.00002090
Iteration 114/1000 | Loss: 0.00002090
Iteration 115/1000 | Loss: 0.00002090
Iteration 116/1000 | Loss: 0.00002090
Iteration 117/1000 | Loss: 0.00002090
Iteration 118/1000 | Loss: 0.00002090
Iteration 119/1000 | Loss: 0.00002090
Iteration 120/1000 | Loss: 0.00002090
Iteration 121/1000 | Loss: 0.00002090
Iteration 122/1000 | Loss: 0.00002090
Iteration 123/1000 | Loss: 0.00002090
Iteration 124/1000 | Loss: 0.00002090
Iteration 125/1000 | Loss: 0.00002090
Iteration 126/1000 | Loss: 0.00002090
Iteration 127/1000 | Loss: 0.00002090
Iteration 128/1000 | Loss: 0.00002090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.0901607058476657e-05, 2.0901607058476657e-05, 2.0901607058476657e-05, 2.0901607058476657e-05, 2.0901607058476657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0901607058476657e-05

Optimization complete. Final v2v error: 3.7987983226776123 mm

Highest mean error: 5.326289653778076 mm for frame 55

Lowest mean error: 3.074903726577759 mm for frame 39

Saving results

Total time: 39.3181049823761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815073
Iteration 2/25 | Loss: 0.00135896
Iteration 3/25 | Loss: 0.00125227
Iteration 4/25 | Loss: 0.00124034
Iteration 5/25 | Loss: 0.00123717
Iteration 6/25 | Loss: 0.00123684
Iteration 7/25 | Loss: 0.00123684
Iteration 8/25 | Loss: 0.00123684
Iteration 9/25 | Loss: 0.00123684
Iteration 10/25 | Loss: 0.00123684
Iteration 11/25 | Loss: 0.00123684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012368438765406609, 0.0012368438765406609, 0.0012368438765406609, 0.0012368438765406609, 0.0012368438765406609]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012368438765406609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46810174
Iteration 2/25 | Loss: 0.00076504
Iteration 3/25 | Loss: 0.00076504
Iteration 4/25 | Loss: 0.00076504
Iteration 5/25 | Loss: 0.00076504
Iteration 6/25 | Loss: 0.00076504
Iteration 7/25 | Loss: 0.00076504
Iteration 8/25 | Loss: 0.00076503
Iteration 9/25 | Loss: 0.00076503
Iteration 10/25 | Loss: 0.00076503
Iteration 11/25 | Loss: 0.00076503
Iteration 12/25 | Loss: 0.00076503
Iteration 13/25 | Loss: 0.00076503
Iteration 14/25 | Loss: 0.00076503
Iteration 15/25 | Loss: 0.00076503
Iteration 16/25 | Loss: 0.00076503
Iteration 17/25 | Loss: 0.00076503
Iteration 18/25 | Loss: 0.00076503
Iteration 19/25 | Loss: 0.00076503
Iteration 20/25 | Loss: 0.00076503
Iteration 21/25 | Loss: 0.00076503
Iteration 22/25 | Loss: 0.00076503
Iteration 23/25 | Loss: 0.00076503
Iteration 24/25 | Loss: 0.00076503
Iteration 25/25 | Loss: 0.00076503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076503
Iteration 2/1000 | Loss: 0.00002857
Iteration 3/1000 | Loss: 0.00002008
Iteration 4/1000 | Loss: 0.00001746
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001342
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001335
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001325
Iteration 18/1000 | Loss: 0.00001324
Iteration 19/1000 | Loss: 0.00001323
Iteration 20/1000 | Loss: 0.00001323
Iteration 21/1000 | Loss: 0.00001322
Iteration 22/1000 | Loss: 0.00001322
Iteration 23/1000 | Loss: 0.00001322
Iteration 24/1000 | Loss: 0.00001322
Iteration 25/1000 | Loss: 0.00001321
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001321
Iteration 30/1000 | Loss: 0.00001321
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001320
Iteration 34/1000 | Loss: 0.00001320
Iteration 35/1000 | Loss: 0.00001320
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001319
Iteration 39/1000 | Loss: 0.00001319
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001319
Iteration 42/1000 | Loss: 0.00001318
Iteration 43/1000 | Loss: 0.00001318
Iteration 44/1000 | Loss: 0.00001318
Iteration 45/1000 | Loss: 0.00001318
Iteration 46/1000 | Loss: 0.00001318
Iteration 47/1000 | Loss: 0.00001318
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001317
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001316
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001315
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001314
Iteration 65/1000 | Loss: 0.00001314
Iteration 66/1000 | Loss: 0.00001314
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001312
Iteration 76/1000 | Loss: 0.00001312
Iteration 77/1000 | Loss: 0.00001312
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001306
Iteration 97/1000 | Loss: 0.00001306
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001305
Iteration 100/1000 | Loss: 0.00001305
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001303
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001302
Iteration 110/1000 | Loss: 0.00001302
Iteration 111/1000 | Loss: 0.00001302
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001300
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001300
Iteration 127/1000 | Loss: 0.00001300
Iteration 128/1000 | Loss: 0.00001300
Iteration 129/1000 | Loss: 0.00001299
Iteration 130/1000 | Loss: 0.00001299
Iteration 131/1000 | Loss: 0.00001299
Iteration 132/1000 | Loss: 0.00001299
Iteration 133/1000 | Loss: 0.00001298
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001298
Iteration 138/1000 | Loss: 0.00001298
Iteration 139/1000 | Loss: 0.00001297
Iteration 140/1000 | Loss: 0.00001297
Iteration 141/1000 | Loss: 0.00001297
Iteration 142/1000 | Loss: 0.00001297
Iteration 143/1000 | Loss: 0.00001297
Iteration 144/1000 | Loss: 0.00001297
Iteration 145/1000 | Loss: 0.00001297
Iteration 146/1000 | Loss: 0.00001297
Iteration 147/1000 | Loss: 0.00001297
Iteration 148/1000 | Loss: 0.00001297
Iteration 149/1000 | Loss: 0.00001297
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.2968419468961656e-05, 1.2968419468961656e-05, 1.2968419468961656e-05, 1.2968419468961656e-05, 1.2968419468961656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2968419468961656e-05

Optimization complete. Final v2v error: 3.0406453609466553 mm

Highest mean error: 4.005415439605713 mm for frame 66

Lowest mean error: 2.7295961380004883 mm for frame 141

Saving results

Total time: 35.701544523239136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849047
Iteration 2/25 | Loss: 0.00156338
Iteration 3/25 | Loss: 0.00132738
Iteration 4/25 | Loss: 0.00129963
Iteration 5/25 | Loss: 0.00129462
Iteration 6/25 | Loss: 0.00129455
Iteration 7/25 | Loss: 0.00129455
Iteration 8/25 | Loss: 0.00129455
Iteration 9/25 | Loss: 0.00129455
Iteration 10/25 | Loss: 0.00129455
Iteration 11/25 | Loss: 0.00129455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012945471098646522, 0.0012945471098646522, 0.0012945471098646522, 0.0012945471098646522, 0.0012945471098646522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012945471098646522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00276363
Iteration 2/25 | Loss: 0.00059658
Iteration 3/25 | Loss: 0.00059657
Iteration 4/25 | Loss: 0.00059657
Iteration 5/25 | Loss: 0.00059657
Iteration 6/25 | Loss: 0.00059657
Iteration 7/25 | Loss: 0.00059657
Iteration 8/25 | Loss: 0.00059657
Iteration 9/25 | Loss: 0.00059657
Iteration 10/25 | Loss: 0.00059657
Iteration 11/25 | Loss: 0.00059657
Iteration 12/25 | Loss: 0.00059657
Iteration 13/25 | Loss: 0.00059657
Iteration 14/25 | Loss: 0.00059657
Iteration 15/25 | Loss: 0.00059657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005965656600892544, 0.0005965656600892544, 0.0005965656600892544, 0.0005965656600892544, 0.0005965656600892544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005965656600892544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059657
Iteration 2/1000 | Loss: 0.00003937
Iteration 3/1000 | Loss: 0.00003105
Iteration 4/1000 | Loss: 0.00002832
Iteration 5/1000 | Loss: 0.00002662
Iteration 6/1000 | Loss: 0.00002583
Iteration 7/1000 | Loss: 0.00002520
Iteration 8/1000 | Loss: 0.00002467
Iteration 9/1000 | Loss: 0.00002420
Iteration 10/1000 | Loss: 0.00002381
Iteration 11/1000 | Loss: 0.00002342
Iteration 12/1000 | Loss: 0.00002333
Iteration 13/1000 | Loss: 0.00002315
Iteration 14/1000 | Loss: 0.00002308
Iteration 15/1000 | Loss: 0.00002292
Iteration 16/1000 | Loss: 0.00002287
Iteration 17/1000 | Loss: 0.00002285
Iteration 18/1000 | Loss: 0.00002285
Iteration 19/1000 | Loss: 0.00002283
Iteration 20/1000 | Loss: 0.00002281
Iteration 21/1000 | Loss: 0.00002281
Iteration 22/1000 | Loss: 0.00002280
Iteration 23/1000 | Loss: 0.00002280
Iteration 24/1000 | Loss: 0.00002279
Iteration 25/1000 | Loss: 0.00002279
Iteration 26/1000 | Loss: 0.00002279
Iteration 27/1000 | Loss: 0.00002279
Iteration 28/1000 | Loss: 0.00002279
Iteration 29/1000 | Loss: 0.00002279
Iteration 30/1000 | Loss: 0.00002279
Iteration 31/1000 | Loss: 0.00002279
Iteration 32/1000 | Loss: 0.00002279
Iteration 33/1000 | Loss: 0.00002279
Iteration 34/1000 | Loss: 0.00002279
Iteration 35/1000 | Loss: 0.00002279
Iteration 36/1000 | Loss: 0.00002279
Iteration 37/1000 | Loss: 0.00002278
Iteration 38/1000 | Loss: 0.00002278
Iteration 39/1000 | Loss: 0.00002278
Iteration 40/1000 | Loss: 0.00002277
Iteration 41/1000 | Loss: 0.00002277
Iteration 42/1000 | Loss: 0.00002277
Iteration 43/1000 | Loss: 0.00002277
Iteration 44/1000 | Loss: 0.00002277
Iteration 45/1000 | Loss: 0.00002277
Iteration 46/1000 | Loss: 0.00002277
Iteration 47/1000 | Loss: 0.00002277
Iteration 48/1000 | Loss: 0.00002277
Iteration 49/1000 | Loss: 0.00002276
Iteration 50/1000 | Loss: 0.00002276
Iteration 51/1000 | Loss: 0.00002276
Iteration 52/1000 | Loss: 0.00002276
Iteration 53/1000 | Loss: 0.00002276
Iteration 54/1000 | Loss: 0.00002276
Iteration 55/1000 | Loss: 0.00002276
Iteration 56/1000 | Loss: 0.00002276
Iteration 57/1000 | Loss: 0.00002275
Iteration 58/1000 | Loss: 0.00002275
Iteration 59/1000 | Loss: 0.00002275
Iteration 60/1000 | Loss: 0.00002275
Iteration 61/1000 | Loss: 0.00002275
Iteration 62/1000 | Loss: 0.00002275
Iteration 63/1000 | Loss: 0.00002275
Iteration 64/1000 | Loss: 0.00002275
Iteration 65/1000 | Loss: 0.00002275
Iteration 66/1000 | Loss: 0.00002274
Iteration 67/1000 | Loss: 0.00002274
Iteration 68/1000 | Loss: 0.00002274
Iteration 69/1000 | Loss: 0.00002274
Iteration 70/1000 | Loss: 0.00002274
Iteration 71/1000 | Loss: 0.00002274
Iteration 72/1000 | Loss: 0.00002274
Iteration 73/1000 | Loss: 0.00002273
Iteration 74/1000 | Loss: 0.00002273
Iteration 75/1000 | Loss: 0.00002273
Iteration 76/1000 | Loss: 0.00002273
Iteration 77/1000 | Loss: 0.00002273
Iteration 78/1000 | Loss: 0.00002273
Iteration 79/1000 | Loss: 0.00002273
Iteration 80/1000 | Loss: 0.00002272
Iteration 81/1000 | Loss: 0.00002272
Iteration 82/1000 | Loss: 0.00002272
Iteration 83/1000 | Loss: 0.00002272
Iteration 84/1000 | Loss: 0.00002272
Iteration 85/1000 | Loss: 0.00002272
Iteration 86/1000 | Loss: 0.00002271
Iteration 87/1000 | Loss: 0.00002271
Iteration 88/1000 | Loss: 0.00002271
Iteration 89/1000 | Loss: 0.00002271
Iteration 90/1000 | Loss: 0.00002270
Iteration 91/1000 | Loss: 0.00002269
Iteration 92/1000 | Loss: 0.00002269
Iteration 93/1000 | Loss: 0.00002269
Iteration 94/1000 | Loss: 0.00002268
Iteration 95/1000 | Loss: 0.00002268
Iteration 96/1000 | Loss: 0.00002268
Iteration 97/1000 | Loss: 0.00002268
Iteration 98/1000 | Loss: 0.00002268
Iteration 99/1000 | Loss: 0.00002268
Iteration 100/1000 | Loss: 0.00002268
Iteration 101/1000 | Loss: 0.00002268
Iteration 102/1000 | Loss: 0.00002268
Iteration 103/1000 | Loss: 0.00002268
Iteration 104/1000 | Loss: 0.00002268
Iteration 105/1000 | Loss: 0.00002268
Iteration 106/1000 | Loss: 0.00002268
Iteration 107/1000 | Loss: 0.00002268
Iteration 108/1000 | Loss: 0.00002268
Iteration 109/1000 | Loss: 0.00002268
Iteration 110/1000 | Loss: 0.00002267
Iteration 111/1000 | Loss: 0.00002267
Iteration 112/1000 | Loss: 0.00002267
Iteration 113/1000 | Loss: 0.00002267
Iteration 114/1000 | Loss: 0.00002267
Iteration 115/1000 | Loss: 0.00002267
Iteration 116/1000 | Loss: 0.00002267
Iteration 117/1000 | Loss: 0.00002267
Iteration 118/1000 | Loss: 0.00002267
Iteration 119/1000 | Loss: 0.00002267
Iteration 120/1000 | Loss: 0.00002267
Iteration 121/1000 | Loss: 0.00002267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.267051240778528e-05, 2.267051240778528e-05, 2.267051240778528e-05, 2.267051240778528e-05, 2.267051240778528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.267051240778528e-05

Optimization complete. Final v2v error: 3.9380862712860107 mm

Highest mean error: 4.078139781951904 mm for frame 126

Lowest mean error: 3.8525168895721436 mm for frame 12

Saving results

Total time: 39.57385301589966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811716
Iteration 2/25 | Loss: 0.00173101
Iteration 3/25 | Loss: 0.00148696
Iteration 4/25 | Loss: 0.00147176
Iteration 5/25 | Loss: 0.00146955
Iteration 6/25 | Loss: 0.00146953
Iteration 7/25 | Loss: 0.00146953
Iteration 8/25 | Loss: 0.00146953
Iteration 9/25 | Loss: 0.00146953
Iteration 10/25 | Loss: 0.00146953
Iteration 11/25 | Loss: 0.00146953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014695330755785108, 0.0014695330755785108, 0.0014695330755785108, 0.0014695330755785108, 0.0014695330755785108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014695330755785108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.28075904
Iteration 2/25 | Loss: 0.00121780
Iteration 3/25 | Loss: 0.00121780
Iteration 4/25 | Loss: 0.00121780
Iteration 5/25 | Loss: 0.00121779
Iteration 6/25 | Loss: 0.00121779
Iteration 7/25 | Loss: 0.00121779
Iteration 8/25 | Loss: 0.00121779
Iteration 9/25 | Loss: 0.00121779
Iteration 10/25 | Loss: 0.00121779
Iteration 11/25 | Loss: 0.00121779
Iteration 12/25 | Loss: 0.00121779
Iteration 13/25 | Loss: 0.00121779
Iteration 14/25 | Loss: 0.00121779
Iteration 15/25 | Loss: 0.00121779
Iteration 16/25 | Loss: 0.00121779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012177933240309358, 0.0012177933240309358, 0.0012177933240309358, 0.0012177933240309358, 0.0012177933240309358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012177933240309358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121779
Iteration 2/1000 | Loss: 0.00006119
Iteration 3/1000 | Loss: 0.00004371
Iteration 4/1000 | Loss: 0.00003476
Iteration 5/1000 | Loss: 0.00003282
Iteration 6/1000 | Loss: 0.00003074
Iteration 7/1000 | Loss: 0.00002954
Iteration 8/1000 | Loss: 0.00002881
Iteration 9/1000 | Loss: 0.00002818
Iteration 10/1000 | Loss: 0.00002791
Iteration 11/1000 | Loss: 0.00002763
Iteration 12/1000 | Loss: 0.00002744
Iteration 13/1000 | Loss: 0.00002723
Iteration 14/1000 | Loss: 0.00002720
Iteration 15/1000 | Loss: 0.00002712
Iteration 16/1000 | Loss: 0.00002699
Iteration 17/1000 | Loss: 0.00002695
Iteration 18/1000 | Loss: 0.00002694
Iteration 19/1000 | Loss: 0.00002692
Iteration 20/1000 | Loss: 0.00002692
Iteration 21/1000 | Loss: 0.00002691
Iteration 22/1000 | Loss: 0.00002691
Iteration 23/1000 | Loss: 0.00002691
Iteration 24/1000 | Loss: 0.00002691
Iteration 25/1000 | Loss: 0.00002691
Iteration 26/1000 | Loss: 0.00002691
Iteration 27/1000 | Loss: 0.00002691
Iteration 28/1000 | Loss: 0.00002690
Iteration 29/1000 | Loss: 0.00002690
Iteration 30/1000 | Loss: 0.00002690
Iteration 31/1000 | Loss: 0.00002690
Iteration 32/1000 | Loss: 0.00002690
Iteration 33/1000 | Loss: 0.00002687
Iteration 34/1000 | Loss: 0.00002687
Iteration 35/1000 | Loss: 0.00002687
Iteration 36/1000 | Loss: 0.00002687
Iteration 37/1000 | Loss: 0.00002687
Iteration 38/1000 | Loss: 0.00002687
Iteration 39/1000 | Loss: 0.00002687
Iteration 40/1000 | Loss: 0.00002687
Iteration 41/1000 | Loss: 0.00002687
Iteration 42/1000 | Loss: 0.00002686
Iteration 43/1000 | Loss: 0.00002686
Iteration 44/1000 | Loss: 0.00002685
Iteration 45/1000 | Loss: 0.00002685
Iteration 46/1000 | Loss: 0.00002685
Iteration 47/1000 | Loss: 0.00002685
Iteration 48/1000 | Loss: 0.00002684
Iteration 49/1000 | Loss: 0.00002684
Iteration 50/1000 | Loss: 0.00002684
Iteration 51/1000 | Loss: 0.00002684
Iteration 52/1000 | Loss: 0.00002684
Iteration 53/1000 | Loss: 0.00002684
Iteration 54/1000 | Loss: 0.00002684
Iteration 55/1000 | Loss: 0.00002684
Iteration 56/1000 | Loss: 0.00002683
Iteration 57/1000 | Loss: 0.00002683
Iteration 58/1000 | Loss: 0.00002683
Iteration 59/1000 | Loss: 0.00002683
Iteration 60/1000 | Loss: 0.00002683
Iteration 61/1000 | Loss: 0.00002682
Iteration 62/1000 | Loss: 0.00002682
Iteration 63/1000 | Loss: 0.00002682
Iteration 64/1000 | Loss: 0.00002682
Iteration 65/1000 | Loss: 0.00002682
Iteration 66/1000 | Loss: 0.00002682
Iteration 67/1000 | Loss: 0.00002682
Iteration 68/1000 | Loss: 0.00002681
Iteration 69/1000 | Loss: 0.00002681
Iteration 70/1000 | Loss: 0.00002681
Iteration 71/1000 | Loss: 0.00002681
Iteration 72/1000 | Loss: 0.00002681
Iteration 73/1000 | Loss: 0.00002681
Iteration 74/1000 | Loss: 0.00002680
Iteration 75/1000 | Loss: 0.00002680
Iteration 76/1000 | Loss: 0.00002680
Iteration 77/1000 | Loss: 0.00002680
Iteration 78/1000 | Loss: 0.00002680
Iteration 79/1000 | Loss: 0.00002680
Iteration 80/1000 | Loss: 0.00002680
Iteration 81/1000 | Loss: 0.00002680
Iteration 82/1000 | Loss: 0.00002679
Iteration 83/1000 | Loss: 0.00002679
Iteration 84/1000 | Loss: 0.00002679
Iteration 85/1000 | Loss: 0.00002679
Iteration 86/1000 | Loss: 0.00002678
Iteration 87/1000 | Loss: 0.00002678
Iteration 88/1000 | Loss: 0.00002678
Iteration 89/1000 | Loss: 0.00002677
Iteration 90/1000 | Loss: 0.00002677
Iteration 91/1000 | Loss: 0.00002677
Iteration 92/1000 | Loss: 0.00002677
Iteration 93/1000 | Loss: 0.00002677
Iteration 94/1000 | Loss: 0.00002677
Iteration 95/1000 | Loss: 0.00002677
Iteration 96/1000 | Loss: 0.00002677
Iteration 97/1000 | Loss: 0.00002677
Iteration 98/1000 | Loss: 0.00002677
Iteration 99/1000 | Loss: 0.00002676
Iteration 100/1000 | Loss: 0.00002676
Iteration 101/1000 | Loss: 0.00002676
Iteration 102/1000 | Loss: 0.00002676
Iteration 103/1000 | Loss: 0.00002676
Iteration 104/1000 | Loss: 0.00002676
Iteration 105/1000 | Loss: 0.00002676
Iteration 106/1000 | Loss: 0.00002676
Iteration 107/1000 | Loss: 0.00002676
Iteration 108/1000 | Loss: 0.00002676
Iteration 109/1000 | Loss: 0.00002676
Iteration 110/1000 | Loss: 0.00002676
Iteration 111/1000 | Loss: 0.00002676
Iteration 112/1000 | Loss: 0.00002676
Iteration 113/1000 | Loss: 0.00002676
Iteration 114/1000 | Loss: 0.00002676
Iteration 115/1000 | Loss: 0.00002676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.6762138077174313e-05, 2.6762138077174313e-05, 2.6762138077174313e-05, 2.6762138077174313e-05, 2.6762138077174313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6762138077174313e-05

Optimization complete. Final v2v error: 4.194752216339111 mm

Highest mean error: 4.85772180557251 mm for frame 5

Lowest mean error: 3.5405285358428955 mm for frame 142

Saving results

Total time: 33.711397647857666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457645
Iteration 2/25 | Loss: 0.00130204
Iteration 3/25 | Loss: 0.00124698
Iteration 4/25 | Loss: 0.00123858
Iteration 5/25 | Loss: 0.00123579
Iteration 6/25 | Loss: 0.00123554
Iteration 7/25 | Loss: 0.00123554
Iteration 8/25 | Loss: 0.00123554
Iteration 9/25 | Loss: 0.00123554
Iteration 10/25 | Loss: 0.00123554
Iteration 11/25 | Loss: 0.00123554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012355389771983027, 0.0012355389771983027, 0.0012355389771983027, 0.0012355389771983027, 0.0012355389771983027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012355389771983027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.53204632
Iteration 2/25 | Loss: 0.00075139
Iteration 3/25 | Loss: 0.00075138
Iteration 4/25 | Loss: 0.00075138
Iteration 5/25 | Loss: 0.00075138
Iteration 6/25 | Loss: 0.00075138
Iteration 7/25 | Loss: 0.00075138
Iteration 8/25 | Loss: 0.00075138
Iteration 9/25 | Loss: 0.00075138
Iteration 10/25 | Loss: 0.00075138
Iteration 11/25 | Loss: 0.00075138
Iteration 12/25 | Loss: 0.00075138
Iteration 13/25 | Loss: 0.00075138
Iteration 14/25 | Loss: 0.00075138
Iteration 15/25 | Loss: 0.00075138
Iteration 16/25 | Loss: 0.00075138
Iteration 17/25 | Loss: 0.00075138
Iteration 18/25 | Loss: 0.00075138
Iteration 19/25 | Loss: 0.00075138
Iteration 20/25 | Loss: 0.00075138
Iteration 21/25 | Loss: 0.00075138
Iteration 22/25 | Loss: 0.00075138
Iteration 23/25 | Loss: 0.00075138
Iteration 24/25 | Loss: 0.00075138
Iteration 25/25 | Loss: 0.00075138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075138
Iteration 2/1000 | Loss: 0.00003479
Iteration 3/1000 | Loss: 0.00002541
Iteration 4/1000 | Loss: 0.00002271
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00001993
Iteration 7/1000 | Loss: 0.00001927
Iteration 8/1000 | Loss: 0.00001881
Iteration 9/1000 | Loss: 0.00001851
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001784
Iteration 13/1000 | Loss: 0.00001782
Iteration 14/1000 | Loss: 0.00001775
Iteration 15/1000 | Loss: 0.00001771
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001762
Iteration 18/1000 | Loss: 0.00001762
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001756
Iteration 22/1000 | Loss: 0.00001756
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001750
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001714
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001709
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001708
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001704
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001703
Iteration 62/1000 | Loss: 0.00001703
Iteration 63/1000 | Loss: 0.00001703
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001701
Iteration 71/1000 | Loss: 0.00001701
Iteration 72/1000 | Loss: 0.00001701
Iteration 73/1000 | Loss: 0.00001701
Iteration 74/1000 | Loss: 0.00001701
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001700
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001699
Iteration 89/1000 | Loss: 0.00001699
Iteration 90/1000 | Loss: 0.00001699
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001698
Iteration 102/1000 | Loss: 0.00001698
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001696
Iteration 108/1000 | Loss: 0.00001696
Iteration 109/1000 | Loss: 0.00001696
Iteration 110/1000 | Loss: 0.00001696
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001695
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001695
Iteration 117/1000 | Loss: 0.00001695
Iteration 118/1000 | Loss: 0.00001695
Iteration 119/1000 | Loss: 0.00001695
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001694
Iteration 125/1000 | Loss: 0.00001694
Iteration 126/1000 | Loss: 0.00001694
Iteration 127/1000 | Loss: 0.00001694
Iteration 128/1000 | Loss: 0.00001694
Iteration 129/1000 | Loss: 0.00001694
Iteration 130/1000 | Loss: 0.00001693
Iteration 131/1000 | Loss: 0.00001693
Iteration 132/1000 | Loss: 0.00001693
Iteration 133/1000 | Loss: 0.00001693
Iteration 134/1000 | Loss: 0.00001692
Iteration 135/1000 | Loss: 0.00001692
Iteration 136/1000 | Loss: 0.00001692
Iteration 137/1000 | Loss: 0.00001692
Iteration 138/1000 | Loss: 0.00001692
Iteration 139/1000 | Loss: 0.00001692
Iteration 140/1000 | Loss: 0.00001692
Iteration 141/1000 | Loss: 0.00001692
Iteration 142/1000 | Loss: 0.00001692
Iteration 143/1000 | Loss: 0.00001692
Iteration 144/1000 | Loss: 0.00001691
Iteration 145/1000 | Loss: 0.00001691
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001691
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001691
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001691
Iteration 153/1000 | Loss: 0.00001691
Iteration 154/1000 | Loss: 0.00001691
Iteration 155/1000 | Loss: 0.00001691
Iteration 156/1000 | Loss: 0.00001691
Iteration 157/1000 | Loss: 0.00001691
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001690
Iteration 163/1000 | Loss: 0.00001690
Iteration 164/1000 | Loss: 0.00001690
Iteration 165/1000 | Loss: 0.00001690
Iteration 166/1000 | Loss: 0.00001690
Iteration 167/1000 | Loss: 0.00001690
Iteration 168/1000 | Loss: 0.00001690
Iteration 169/1000 | Loss: 0.00001690
Iteration 170/1000 | Loss: 0.00001690
Iteration 171/1000 | Loss: 0.00001690
Iteration 172/1000 | Loss: 0.00001690
Iteration 173/1000 | Loss: 0.00001690
Iteration 174/1000 | Loss: 0.00001690
Iteration 175/1000 | Loss: 0.00001690
Iteration 176/1000 | Loss: 0.00001690
Iteration 177/1000 | Loss: 0.00001690
Iteration 178/1000 | Loss: 0.00001690
Iteration 179/1000 | Loss: 0.00001690
Iteration 180/1000 | Loss: 0.00001690
Iteration 181/1000 | Loss: 0.00001689
Iteration 182/1000 | Loss: 0.00001689
Iteration 183/1000 | Loss: 0.00001689
Iteration 184/1000 | Loss: 0.00001689
Iteration 185/1000 | Loss: 0.00001689
Iteration 186/1000 | Loss: 0.00001689
Iteration 187/1000 | Loss: 0.00001689
Iteration 188/1000 | Loss: 0.00001689
Iteration 189/1000 | Loss: 0.00001689
Iteration 190/1000 | Loss: 0.00001689
Iteration 191/1000 | Loss: 0.00001689
Iteration 192/1000 | Loss: 0.00001689
Iteration 193/1000 | Loss: 0.00001689
Iteration 194/1000 | Loss: 0.00001689
Iteration 195/1000 | Loss: 0.00001689
Iteration 196/1000 | Loss: 0.00001689
Iteration 197/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.6889722246560268e-05, 1.6889722246560268e-05, 1.6889722246560268e-05, 1.6889722246560268e-05, 1.6889722246560268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6889722246560268e-05

Optimization complete. Final v2v error: 3.4482240676879883 mm

Highest mean error: 4.056164741516113 mm for frame 94

Lowest mean error: 3.0774264335632324 mm for frame 31

Saving results

Total time: 42.61250114440918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856773
Iteration 2/25 | Loss: 0.00135551
Iteration 3/25 | Loss: 0.00127364
Iteration 4/25 | Loss: 0.00125891
Iteration 5/25 | Loss: 0.00125498
Iteration 6/25 | Loss: 0.00125498
Iteration 7/25 | Loss: 0.00125498
Iteration 8/25 | Loss: 0.00125498
Iteration 9/25 | Loss: 0.00125498
Iteration 10/25 | Loss: 0.00125498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012549774255603552, 0.0012549774255603552, 0.0012549774255603552, 0.0012549774255603552, 0.0012549774255603552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012549774255603552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.13536930
Iteration 2/25 | Loss: 0.00078141
Iteration 3/25 | Loss: 0.00078141
Iteration 4/25 | Loss: 0.00078141
Iteration 5/25 | Loss: 0.00078141
Iteration 6/25 | Loss: 0.00078141
Iteration 7/25 | Loss: 0.00078140
Iteration 8/25 | Loss: 0.00078140
Iteration 9/25 | Loss: 0.00078140
Iteration 10/25 | Loss: 0.00078140
Iteration 11/25 | Loss: 0.00078140
Iteration 12/25 | Loss: 0.00078140
Iteration 13/25 | Loss: 0.00078140
Iteration 14/25 | Loss: 0.00078140
Iteration 15/25 | Loss: 0.00078140
Iteration 16/25 | Loss: 0.00078140
Iteration 17/25 | Loss: 0.00078140
Iteration 18/25 | Loss: 0.00078140
Iteration 19/25 | Loss: 0.00078140
Iteration 20/25 | Loss: 0.00078140
Iteration 21/25 | Loss: 0.00078140
Iteration 22/25 | Loss: 0.00078140
Iteration 23/25 | Loss: 0.00078140
Iteration 24/25 | Loss: 0.00078140
Iteration 25/25 | Loss: 0.00078140

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078140
Iteration 2/1000 | Loss: 0.00002792
Iteration 3/1000 | Loss: 0.00002227
Iteration 4/1000 | Loss: 0.00002078
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001917
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001852
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001813
Iteration 11/1000 | Loss: 0.00001809
Iteration 12/1000 | Loss: 0.00001796
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001766
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001756
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001752
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001747
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001744
Iteration 25/1000 | Loss: 0.00001743
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001720
Iteration 31/1000 | Loss: 0.00001720
Iteration 32/1000 | Loss: 0.00001720
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001713
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001709
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00001709
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001707
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001705
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001704
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001703
Iteration 69/1000 | Loss: 0.00001703
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001702
Iteration 81/1000 | Loss: 0.00001702
Iteration 82/1000 | Loss: 0.00001702
Iteration 83/1000 | Loss: 0.00001702
Iteration 84/1000 | Loss: 0.00001702
Iteration 85/1000 | Loss: 0.00001702
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001700
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001699
Iteration 101/1000 | Loss: 0.00001699
Iteration 102/1000 | Loss: 0.00001699
Iteration 103/1000 | Loss: 0.00001699
Iteration 104/1000 | Loss: 0.00001699
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001699
Iteration 111/1000 | Loss: 0.00001699
Iteration 112/1000 | Loss: 0.00001699
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001699
Iteration 120/1000 | Loss: 0.00001699
Iteration 121/1000 | Loss: 0.00001699
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001699
Iteration 124/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.6986174159683287e-05, 1.6986174159683287e-05, 1.6986174159683287e-05, 1.6986174159683287e-05, 1.6986174159683287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6986174159683287e-05

Optimization complete. Final v2v error: 3.480510711669922 mm

Highest mean error: 3.818208694458008 mm for frame 199

Lowest mean error: 3.213193893432617 mm for frame 3

Saving results

Total time: 39.067503213882446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812378
Iteration 2/25 | Loss: 0.00144003
Iteration 3/25 | Loss: 0.00131267
Iteration 4/25 | Loss: 0.00128954
Iteration 5/25 | Loss: 0.00128291
Iteration 6/25 | Loss: 0.00128064
Iteration 7/25 | Loss: 0.00128042
Iteration 8/25 | Loss: 0.00128042
Iteration 9/25 | Loss: 0.00128042
Iteration 10/25 | Loss: 0.00128042
Iteration 11/25 | Loss: 0.00128042
Iteration 12/25 | Loss: 0.00128042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012804185971617699, 0.0012804185971617699, 0.0012804185971617699, 0.0012804185971617699, 0.0012804185971617699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012804185971617699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54468250
Iteration 2/25 | Loss: 0.00069903
Iteration 3/25 | Loss: 0.00069903
Iteration 4/25 | Loss: 0.00069903
Iteration 5/25 | Loss: 0.00069902
Iteration 6/25 | Loss: 0.00069902
Iteration 7/25 | Loss: 0.00069902
Iteration 8/25 | Loss: 0.00069902
Iteration 9/25 | Loss: 0.00069902
Iteration 10/25 | Loss: 0.00069902
Iteration 11/25 | Loss: 0.00069902
Iteration 12/25 | Loss: 0.00069902
Iteration 13/25 | Loss: 0.00069902
Iteration 14/25 | Loss: 0.00069902
Iteration 15/25 | Loss: 0.00069902
Iteration 16/25 | Loss: 0.00069902
Iteration 17/25 | Loss: 0.00069902
Iteration 18/25 | Loss: 0.00069902
Iteration 19/25 | Loss: 0.00069902
Iteration 20/25 | Loss: 0.00069902
Iteration 21/25 | Loss: 0.00069902
Iteration 22/25 | Loss: 0.00069902
Iteration 23/25 | Loss: 0.00069902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006990224937908351, 0.0006990224937908351, 0.0006990224937908351, 0.0006990224937908351, 0.0006990224937908351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006990224937908351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069902
Iteration 2/1000 | Loss: 0.00004903
Iteration 3/1000 | Loss: 0.00003766
Iteration 4/1000 | Loss: 0.00003364
Iteration 5/1000 | Loss: 0.00003210
Iteration 6/1000 | Loss: 0.00003110
Iteration 7/1000 | Loss: 0.00003027
Iteration 8/1000 | Loss: 0.00002965
Iteration 9/1000 | Loss: 0.00002912
Iteration 10/1000 | Loss: 0.00002864
Iteration 11/1000 | Loss: 0.00002834
Iteration 12/1000 | Loss: 0.00002805
Iteration 13/1000 | Loss: 0.00002779
Iteration 14/1000 | Loss: 0.00002760
Iteration 15/1000 | Loss: 0.00002755
Iteration 16/1000 | Loss: 0.00002751
Iteration 17/1000 | Loss: 0.00002750
Iteration 18/1000 | Loss: 0.00002750
Iteration 19/1000 | Loss: 0.00002749
Iteration 20/1000 | Loss: 0.00002749
Iteration 21/1000 | Loss: 0.00002743
Iteration 22/1000 | Loss: 0.00002740
Iteration 23/1000 | Loss: 0.00002740
Iteration 24/1000 | Loss: 0.00002739
Iteration 25/1000 | Loss: 0.00002737
Iteration 26/1000 | Loss: 0.00002736
Iteration 27/1000 | Loss: 0.00002736
Iteration 28/1000 | Loss: 0.00002734
Iteration 29/1000 | Loss: 0.00002734
Iteration 30/1000 | Loss: 0.00002734
Iteration 31/1000 | Loss: 0.00002733
Iteration 32/1000 | Loss: 0.00002733
Iteration 33/1000 | Loss: 0.00002733
Iteration 34/1000 | Loss: 0.00002733
Iteration 35/1000 | Loss: 0.00002733
Iteration 36/1000 | Loss: 0.00002733
Iteration 37/1000 | Loss: 0.00002733
Iteration 38/1000 | Loss: 0.00002733
Iteration 39/1000 | Loss: 0.00002733
Iteration 40/1000 | Loss: 0.00002733
Iteration 41/1000 | Loss: 0.00002733
Iteration 42/1000 | Loss: 0.00002733
Iteration 43/1000 | Loss: 0.00002733
Iteration 44/1000 | Loss: 0.00002733
Iteration 45/1000 | Loss: 0.00002733
Iteration 46/1000 | Loss: 0.00002732
Iteration 47/1000 | Loss: 0.00002732
Iteration 48/1000 | Loss: 0.00002731
Iteration 49/1000 | Loss: 0.00002731
Iteration 50/1000 | Loss: 0.00002731
Iteration 51/1000 | Loss: 0.00002730
Iteration 52/1000 | Loss: 0.00002730
Iteration 53/1000 | Loss: 0.00002730
Iteration 54/1000 | Loss: 0.00002730
Iteration 55/1000 | Loss: 0.00002730
Iteration 56/1000 | Loss: 0.00002729
Iteration 57/1000 | Loss: 0.00002729
Iteration 58/1000 | Loss: 0.00002728
Iteration 59/1000 | Loss: 0.00002728
Iteration 60/1000 | Loss: 0.00002728
Iteration 61/1000 | Loss: 0.00002728
Iteration 62/1000 | Loss: 0.00002727
Iteration 63/1000 | Loss: 0.00002727
Iteration 64/1000 | Loss: 0.00002727
Iteration 65/1000 | Loss: 0.00002727
Iteration 66/1000 | Loss: 0.00002726
Iteration 67/1000 | Loss: 0.00002726
Iteration 68/1000 | Loss: 0.00002726
Iteration 69/1000 | Loss: 0.00002726
Iteration 70/1000 | Loss: 0.00002725
Iteration 71/1000 | Loss: 0.00002725
Iteration 72/1000 | Loss: 0.00002725
Iteration 73/1000 | Loss: 0.00002725
Iteration 74/1000 | Loss: 0.00002725
Iteration 75/1000 | Loss: 0.00002725
Iteration 76/1000 | Loss: 0.00002725
Iteration 77/1000 | Loss: 0.00002724
Iteration 78/1000 | Loss: 0.00002724
Iteration 79/1000 | Loss: 0.00002724
Iteration 80/1000 | Loss: 0.00002723
Iteration 81/1000 | Loss: 0.00002723
Iteration 82/1000 | Loss: 0.00002723
Iteration 83/1000 | Loss: 0.00002723
Iteration 84/1000 | Loss: 0.00002723
Iteration 85/1000 | Loss: 0.00002723
Iteration 86/1000 | Loss: 0.00002723
Iteration 87/1000 | Loss: 0.00002723
Iteration 88/1000 | Loss: 0.00002723
Iteration 89/1000 | Loss: 0.00002722
Iteration 90/1000 | Loss: 0.00002722
Iteration 91/1000 | Loss: 0.00002722
Iteration 92/1000 | Loss: 0.00002722
Iteration 93/1000 | Loss: 0.00002721
Iteration 94/1000 | Loss: 0.00002721
Iteration 95/1000 | Loss: 0.00002721
Iteration 96/1000 | Loss: 0.00002721
Iteration 97/1000 | Loss: 0.00002721
Iteration 98/1000 | Loss: 0.00002721
Iteration 99/1000 | Loss: 0.00002720
Iteration 100/1000 | Loss: 0.00002720
Iteration 101/1000 | Loss: 0.00002720
Iteration 102/1000 | Loss: 0.00002720
Iteration 103/1000 | Loss: 0.00002720
Iteration 104/1000 | Loss: 0.00002720
Iteration 105/1000 | Loss: 0.00002720
Iteration 106/1000 | Loss: 0.00002720
Iteration 107/1000 | Loss: 0.00002720
Iteration 108/1000 | Loss: 0.00002720
Iteration 109/1000 | Loss: 0.00002720
Iteration 110/1000 | Loss: 0.00002720
Iteration 111/1000 | Loss: 0.00002720
Iteration 112/1000 | Loss: 0.00002720
Iteration 113/1000 | Loss: 0.00002720
Iteration 114/1000 | Loss: 0.00002720
Iteration 115/1000 | Loss: 0.00002720
Iteration 116/1000 | Loss: 0.00002720
Iteration 117/1000 | Loss: 0.00002720
Iteration 118/1000 | Loss: 0.00002720
Iteration 119/1000 | Loss: 0.00002720
Iteration 120/1000 | Loss: 0.00002720
Iteration 121/1000 | Loss: 0.00002720
Iteration 122/1000 | Loss: 0.00002720
Iteration 123/1000 | Loss: 0.00002720
Iteration 124/1000 | Loss: 0.00002720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.7199206670047715e-05, 2.7199206670047715e-05, 2.7199206670047715e-05, 2.7199206670047715e-05, 2.7199206670047715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7199206670047715e-05

Optimization complete. Final v2v error: 4.329814434051514 mm

Highest mean error: 4.839548587799072 mm for frame 77

Lowest mean error: 3.4343292713165283 mm for frame 0

Saving results

Total time: 37.61630368232727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864936
Iteration 2/25 | Loss: 0.00186771
Iteration 3/25 | Loss: 0.00154201
Iteration 4/25 | Loss: 0.00146666
Iteration 5/25 | Loss: 0.00150891
Iteration 6/25 | Loss: 0.00144026
Iteration 7/25 | Loss: 0.00137415
Iteration 8/25 | Loss: 0.00136584
Iteration 9/25 | Loss: 0.00136963
Iteration 10/25 | Loss: 0.00136522
Iteration 11/25 | Loss: 0.00139484
Iteration 12/25 | Loss: 0.00136121
Iteration 13/25 | Loss: 0.00135448
Iteration 14/25 | Loss: 0.00133068
Iteration 15/25 | Loss: 0.00132885
Iteration 16/25 | Loss: 0.00132837
Iteration 17/25 | Loss: 0.00132174
Iteration 18/25 | Loss: 0.00132033
Iteration 19/25 | Loss: 0.00132317
Iteration 20/25 | Loss: 0.00132214
Iteration 21/25 | Loss: 0.00132148
Iteration 22/25 | Loss: 0.00132001
Iteration 23/25 | Loss: 0.00132001
Iteration 24/25 | Loss: 0.00132001
Iteration 25/25 | Loss: 0.00132001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.63794279
Iteration 2/25 | Loss: 0.00093959
Iteration 3/25 | Loss: 0.00087312
Iteration 4/25 | Loss: 0.00087312
Iteration 5/25 | Loss: 0.00087312
Iteration 6/25 | Loss: 0.00087312
Iteration 7/25 | Loss: 0.00087312
Iteration 8/25 | Loss: 0.00087312
Iteration 9/25 | Loss: 0.00087312
Iteration 10/25 | Loss: 0.00087312
Iteration 11/25 | Loss: 0.00087312
Iteration 12/25 | Loss: 0.00087312
Iteration 13/25 | Loss: 0.00087312
Iteration 14/25 | Loss: 0.00087312
Iteration 15/25 | Loss: 0.00087312
Iteration 16/25 | Loss: 0.00087312
Iteration 17/25 | Loss: 0.00087312
Iteration 18/25 | Loss: 0.00087312
Iteration 19/25 | Loss: 0.00087312
Iteration 20/25 | Loss: 0.00087312
Iteration 21/25 | Loss: 0.00087312
Iteration 22/25 | Loss: 0.00087312
Iteration 23/25 | Loss: 0.00087312
Iteration 24/25 | Loss: 0.00087312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008731153793632984, 0.0008731153793632984, 0.0008731153793632984, 0.0008731153793632984, 0.0008731153793632984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008731153793632984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087312
Iteration 2/1000 | Loss: 0.00004330
Iteration 3/1000 | Loss: 0.00012367
Iteration 4/1000 | Loss: 0.00004447
Iteration 5/1000 | Loss: 0.00002687
Iteration 6/1000 | Loss: 0.00002465
Iteration 7/1000 | Loss: 0.00002379
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00010155
Iteration 10/1000 | Loss: 0.00002191
Iteration 11/1000 | Loss: 0.00002161
Iteration 12/1000 | Loss: 0.00010156
Iteration 13/1000 | Loss: 0.00002253
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002107
Iteration 16/1000 | Loss: 0.00002094
Iteration 17/1000 | Loss: 0.00002076
Iteration 18/1000 | Loss: 0.00002072
Iteration 19/1000 | Loss: 0.00012097
Iteration 20/1000 | Loss: 0.00004312
Iteration 21/1000 | Loss: 0.00002071
Iteration 22/1000 | Loss: 0.00002059
Iteration 23/1000 | Loss: 0.00002059
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002059
Iteration 26/1000 | Loss: 0.00002059
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002059
Iteration 29/1000 | Loss: 0.00002059
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002057
Iteration 32/1000 | Loss: 0.00002057
Iteration 33/1000 | Loss: 0.00002054
Iteration 34/1000 | Loss: 0.00002054
Iteration 35/1000 | Loss: 0.00002054
Iteration 36/1000 | Loss: 0.00002054
Iteration 37/1000 | Loss: 0.00002054
Iteration 38/1000 | Loss: 0.00002054
Iteration 39/1000 | Loss: 0.00002052
Iteration 40/1000 | Loss: 0.00002052
Iteration 41/1000 | Loss: 0.00002050
Iteration 42/1000 | Loss: 0.00002049
Iteration 43/1000 | Loss: 0.00002049
Iteration 44/1000 | Loss: 0.00002049
Iteration 45/1000 | Loss: 0.00002049
Iteration 46/1000 | Loss: 0.00002048
Iteration 47/1000 | Loss: 0.00002048
Iteration 48/1000 | Loss: 0.00002048
Iteration 49/1000 | Loss: 0.00002048
Iteration 50/1000 | Loss: 0.00002048
Iteration 51/1000 | Loss: 0.00002048
Iteration 52/1000 | Loss: 0.00002048
Iteration 53/1000 | Loss: 0.00002048
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002047
Iteration 57/1000 | Loss: 0.00002047
Iteration 58/1000 | Loss: 0.00002047
Iteration 59/1000 | Loss: 0.00002046
Iteration 60/1000 | Loss: 0.00002044
Iteration 61/1000 | Loss: 0.00002044
Iteration 62/1000 | Loss: 0.00002043
Iteration 63/1000 | Loss: 0.00002043
Iteration 64/1000 | Loss: 0.00002043
Iteration 65/1000 | Loss: 0.00002042
Iteration 66/1000 | Loss: 0.00002042
Iteration 67/1000 | Loss: 0.00002042
Iteration 68/1000 | Loss: 0.00002041
Iteration 69/1000 | Loss: 0.00002041
Iteration 70/1000 | Loss: 0.00002041
Iteration 71/1000 | Loss: 0.00002041
Iteration 72/1000 | Loss: 0.00002040
Iteration 73/1000 | Loss: 0.00002040
Iteration 74/1000 | Loss: 0.00002040
Iteration 75/1000 | Loss: 0.00002040
Iteration 76/1000 | Loss: 0.00002040
Iteration 77/1000 | Loss: 0.00002040
Iteration 78/1000 | Loss: 0.00002040
Iteration 79/1000 | Loss: 0.00002040
Iteration 80/1000 | Loss: 0.00002040
Iteration 81/1000 | Loss: 0.00002040
Iteration 82/1000 | Loss: 0.00002040
Iteration 83/1000 | Loss: 0.00002039
Iteration 84/1000 | Loss: 0.00002039
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002038
Iteration 87/1000 | Loss: 0.00002038
Iteration 88/1000 | Loss: 0.00002038
Iteration 89/1000 | Loss: 0.00002038
Iteration 90/1000 | Loss: 0.00002038
Iteration 91/1000 | Loss: 0.00002038
Iteration 92/1000 | Loss: 0.00002037
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00002037
Iteration 95/1000 | Loss: 0.00002037
Iteration 96/1000 | Loss: 0.00002036
Iteration 97/1000 | Loss: 0.00002035
Iteration 98/1000 | Loss: 0.00002035
Iteration 99/1000 | Loss: 0.00002034
Iteration 100/1000 | Loss: 0.00002034
Iteration 101/1000 | Loss: 0.00002034
Iteration 102/1000 | Loss: 0.00002034
Iteration 103/1000 | Loss: 0.00002033
Iteration 104/1000 | Loss: 0.00002033
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002033
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002033
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002033
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002032
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002031
Iteration 119/1000 | Loss: 0.00002031
Iteration 120/1000 | Loss: 0.00002031
Iteration 121/1000 | Loss: 0.00002030
Iteration 122/1000 | Loss: 0.00002030
Iteration 123/1000 | Loss: 0.00002030
Iteration 124/1000 | Loss: 0.00002029
Iteration 125/1000 | Loss: 0.00002029
Iteration 126/1000 | Loss: 0.00002029
Iteration 127/1000 | Loss: 0.00002028
Iteration 128/1000 | Loss: 0.00002028
Iteration 129/1000 | Loss: 0.00002028
Iteration 130/1000 | Loss: 0.00002028
Iteration 131/1000 | Loss: 0.00002026
Iteration 132/1000 | Loss: 0.00002026
Iteration 133/1000 | Loss: 0.00002026
Iteration 134/1000 | Loss: 0.00002026
Iteration 135/1000 | Loss: 0.00002026
Iteration 136/1000 | Loss: 0.00002026
Iteration 137/1000 | Loss: 0.00002026
Iteration 138/1000 | Loss: 0.00002025
Iteration 139/1000 | Loss: 0.00002025
Iteration 140/1000 | Loss: 0.00002025
Iteration 141/1000 | Loss: 0.00002025
Iteration 142/1000 | Loss: 0.00002025
Iteration 143/1000 | Loss: 0.00002024
Iteration 144/1000 | Loss: 0.00002024
Iteration 145/1000 | Loss: 0.00002024
Iteration 146/1000 | Loss: 0.00002024
Iteration 147/1000 | Loss: 0.00002024
Iteration 148/1000 | Loss: 0.00002024
Iteration 149/1000 | Loss: 0.00002024
Iteration 150/1000 | Loss: 0.00002024
Iteration 151/1000 | Loss: 0.00002023
Iteration 152/1000 | Loss: 0.00002023
Iteration 153/1000 | Loss: 0.00002023
Iteration 154/1000 | Loss: 0.00002023
Iteration 155/1000 | Loss: 0.00002023
Iteration 156/1000 | Loss: 0.00002023
Iteration 157/1000 | Loss: 0.00002023
Iteration 158/1000 | Loss: 0.00002023
Iteration 159/1000 | Loss: 0.00002023
Iteration 160/1000 | Loss: 0.00002023
Iteration 161/1000 | Loss: 0.00002023
Iteration 162/1000 | Loss: 0.00002023
Iteration 163/1000 | Loss: 0.00002023
Iteration 164/1000 | Loss: 0.00002022
Iteration 165/1000 | Loss: 0.00002022
Iteration 166/1000 | Loss: 0.00002022
Iteration 167/1000 | Loss: 0.00002022
Iteration 168/1000 | Loss: 0.00002022
Iteration 169/1000 | Loss: 0.00002022
Iteration 170/1000 | Loss: 0.00002022
Iteration 171/1000 | Loss: 0.00002022
Iteration 172/1000 | Loss: 0.00002022
Iteration 173/1000 | Loss: 0.00002022
Iteration 174/1000 | Loss: 0.00002022
Iteration 175/1000 | Loss: 0.00002022
Iteration 176/1000 | Loss: 0.00002022
Iteration 177/1000 | Loss: 0.00002022
Iteration 178/1000 | Loss: 0.00002021
Iteration 179/1000 | Loss: 0.00002021
Iteration 180/1000 | Loss: 0.00002021
Iteration 181/1000 | Loss: 0.00002021
Iteration 182/1000 | Loss: 0.00002021
Iteration 183/1000 | Loss: 0.00002021
Iteration 184/1000 | Loss: 0.00002021
Iteration 185/1000 | Loss: 0.00002021
Iteration 186/1000 | Loss: 0.00002021
Iteration 187/1000 | Loss: 0.00002021
Iteration 188/1000 | Loss: 0.00002021
Iteration 189/1000 | Loss: 0.00002020
Iteration 190/1000 | Loss: 0.00002020
Iteration 191/1000 | Loss: 0.00002020
Iteration 192/1000 | Loss: 0.00002020
Iteration 193/1000 | Loss: 0.00002020
Iteration 194/1000 | Loss: 0.00002020
Iteration 195/1000 | Loss: 0.00002020
Iteration 196/1000 | Loss: 0.00002020
Iteration 197/1000 | Loss: 0.00002020
Iteration 198/1000 | Loss: 0.00002020
Iteration 199/1000 | Loss: 0.00002020
Iteration 200/1000 | Loss: 0.00002020
Iteration 201/1000 | Loss: 0.00002020
Iteration 202/1000 | Loss: 0.00002020
Iteration 203/1000 | Loss: 0.00002020
Iteration 204/1000 | Loss: 0.00002020
Iteration 205/1000 | Loss: 0.00002020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.0202021914883517e-05, 2.0202021914883517e-05, 2.0202021914883517e-05, 2.0202021914883517e-05, 2.0202021914883517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0202021914883517e-05

Optimization complete. Final v2v error: 3.688588857650757 mm

Highest mean error: 5.593433380126953 mm for frame 95

Lowest mean error: 3.2887582778930664 mm for frame 52

Saving results

Total time: 82.43643474578857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698642
Iteration 2/25 | Loss: 0.00157990
Iteration 3/25 | Loss: 0.00129920
Iteration 4/25 | Loss: 0.00126580
Iteration 5/25 | Loss: 0.00125960
Iteration 6/25 | Loss: 0.00125873
Iteration 7/25 | Loss: 0.00125873
Iteration 8/25 | Loss: 0.00125873
Iteration 9/25 | Loss: 0.00125873
Iteration 10/25 | Loss: 0.00125873
Iteration 11/25 | Loss: 0.00125873
Iteration 12/25 | Loss: 0.00125873
Iteration 13/25 | Loss: 0.00125873
Iteration 14/25 | Loss: 0.00125873
Iteration 15/25 | Loss: 0.00125873
Iteration 16/25 | Loss: 0.00125873
Iteration 17/25 | Loss: 0.00125873
Iteration 18/25 | Loss: 0.00125873
Iteration 19/25 | Loss: 0.00125873
Iteration 20/25 | Loss: 0.00125873
Iteration 21/25 | Loss: 0.00125873
Iteration 22/25 | Loss: 0.00125873
Iteration 23/25 | Loss: 0.00125873
Iteration 24/25 | Loss: 0.00125873
Iteration 25/25 | Loss: 0.00125873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49044275
Iteration 2/25 | Loss: 0.00070576
Iteration 3/25 | Loss: 0.00070575
Iteration 4/25 | Loss: 0.00070575
Iteration 5/25 | Loss: 0.00070575
Iteration 6/25 | Loss: 0.00070575
Iteration 7/25 | Loss: 0.00070575
Iteration 8/25 | Loss: 0.00070575
Iteration 9/25 | Loss: 0.00070575
Iteration 10/25 | Loss: 0.00070575
Iteration 11/25 | Loss: 0.00070575
Iteration 12/25 | Loss: 0.00070575
Iteration 13/25 | Loss: 0.00070575
Iteration 14/25 | Loss: 0.00070575
Iteration 15/25 | Loss: 0.00070575
Iteration 16/25 | Loss: 0.00070575
Iteration 17/25 | Loss: 0.00070575
Iteration 18/25 | Loss: 0.00070575
Iteration 19/25 | Loss: 0.00070575
Iteration 20/25 | Loss: 0.00070575
Iteration 21/25 | Loss: 0.00070575
Iteration 22/25 | Loss: 0.00070575
Iteration 23/25 | Loss: 0.00070575
Iteration 24/25 | Loss: 0.00070575
Iteration 25/25 | Loss: 0.00070575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070575
Iteration 2/1000 | Loss: 0.00003684
Iteration 3/1000 | Loss: 0.00002541
Iteration 4/1000 | Loss: 0.00002263
Iteration 5/1000 | Loss: 0.00002112
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001913
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001772
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001753
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001712
Iteration 19/1000 | Loss: 0.00001710
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001683
Iteration 25/1000 | Loss: 0.00001682
Iteration 26/1000 | Loss: 0.00001682
Iteration 27/1000 | Loss: 0.00001681
Iteration 28/1000 | Loss: 0.00001680
Iteration 29/1000 | Loss: 0.00001679
Iteration 30/1000 | Loss: 0.00001679
Iteration 31/1000 | Loss: 0.00001677
Iteration 32/1000 | Loss: 0.00001676
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001674
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00001673
Iteration 39/1000 | Loss: 0.00001673
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00001673
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001673
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001672
Iteration 46/1000 | Loss: 0.00001672
Iteration 47/1000 | Loss: 0.00001672
Iteration 48/1000 | Loss: 0.00001672
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001671
Iteration 51/1000 | Loss: 0.00001671
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001671
Iteration 56/1000 | Loss: 0.00001670
Iteration 57/1000 | Loss: 0.00001670
Iteration 58/1000 | Loss: 0.00001670
Iteration 59/1000 | Loss: 0.00001670
Iteration 60/1000 | Loss: 0.00001669
Iteration 61/1000 | Loss: 0.00001669
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001667
Iteration 66/1000 | Loss: 0.00001666
Iteration 67/1000 | Loss: 0.00001666
Iteration 68/1000 | Loss: 0.00001666
Iteration 69/1000 | Loss: 0.00001665
Iteration 70/1000 | Loss: 0.00001665
Iteration 71/1000 | Loss: 0.00001665
Iteration 72/1000 | Loss: 0.00001665
Iteration 73/1000 | Loss: 0.00001664
Iteration 74/1000 | Loss: 0.00001664
Iteration 75/1000 | Loss: 0.00001664
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001662
Iteration 80/1000 | Loss: 0.00001661
Iteration 81/1000 | Loss: 0.00001661
Iteration 82/1000 | Loss: 0.00001661
Iteration 83/1000 | Loss: 0.00001661
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001660
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001659
Iteration 91/1000 | Loss: 0.00001659
Iteration 92/1000 | Loss: 0.00001659
Iteration 93/1000 | Loss: 0.00001659
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001658
Iteration 103/1000 | Loss: 0.00001658
Iteration 104/1000 | Loss: 0.00001658
Iteration 105/1000 | Loss: 0.00001658
Iteration 106/1000 | Loss: 0.00001658
Iteration 107/1000 | Loss: 0.00001658
Iteration 108/1000 | Loss: 0.00001658
Iteration 109/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.657977009017486e-05, 1.657977009017486e-05, 1.657977009017486e-05, 1.657977009017486e-05, 1.657977009017486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.657977009017486e-05

Optimization complete. Final v2v error: 3.452221155166626 mm

Highest mean error: 3.8874289989471436 mm for frame 234

Lowest mean error: 3.132258415222168 mm for frame 166

Saving results

Total time: 44.940903663635254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809496
Iteration 2/25 | Loss: 0.00145485
Iteration 3/25 | Loss: 0.00133516
Iteration 4/25 | Loss: 0.00132130
Iteration 5/25 | Loss: 0.00131733
Iteration 6/25 | Loss: 0.00131620
Iteration 7/25 | Loss: 0.00131610
Iteration 8/25 | Loss: 0.00131610
Iteration 9/25 | Loss: 0.00131610
Iteration 10/25 | Loss: 0.00131610
Iteration 11/25 | Loss: 0.00131610
Iteration 12/25 | Loss: 0.00131610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001316104200668633, 0.001316104200668633, 0.001316104200668633, 0.001316104200668633, 0.001316104200668633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001316104200668633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47134542
Iteration 2/25 | Loss: 0.00176787
Iteration 3/25 | Loss: 0.00176786
Iteration 4/25 | Loss: 0.00176786
Iteration 5/25 | Loss: 0.00176786
Iteration 6/25 | Loss: 0.00176786
Iteration 7/25 | Loss: 0.00176786
Iteration 8/25 | Loss: 0.00176786
Iteration 9/25 | Loss: 0.00176786
Iteration 10/25 | Loss: 0.00176786
Iteration 11/25 | Loss: 0.00176786
Iteration 12/25 | Loss: 0.00176786
Iteration 13/25 | Loss: 0.00176786
Iteration 14/25 | Loss: 0.00176786
Iteration 15/25 | Loss: 0.00176786
Iteration 16/25 | Loss: 0.00176786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017678552540019155, 0.0017678552540019155, 0.0017678552540019155, 0.0017678552540019155, 0.0017678552540019155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017678552540019155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176786
Iteration 2/1000 | Loss: 0.00013071
Iteration 3/1000 | Loss: 0.00008849
Iteration 4/1000 | Loss: 0.00007649
Iteration 5/1000 | Loss: 0.00006895
Iteration 6/1000 | Loss: 0.00006577
Iteration 7/1000 | Loss: 0.00006347
Iteration 8/1000 | Loss: 0.00006144
Iteration 9/1000 | Loss: 0.00005952
Iteration 10/1000 | Loss: 0.00005824
Iteration 11/1000 | Loss: 0.00005764
Iteration 12/1000 | Loss: 0.00005727
Iteration 13/1000 | Loss: 0.00005690
Iteration 14/1000 | Loss: 0.00005662
Iteration 15/1000 | Loss: 0.00005646
Iteration 16/1000 | Loss: 0.00005626
Iteration 17/1000 | Loss: 0.00005605
Iteration 18/1000 | Loss: 0.00005604
Iteration 19/1000 | Loss: 0.00005590
Iteration 20/1000 | Loss: 0.00005589
Iteration 21/1000 | Loss: 0.00005585
Iteration 22/1000 | Loss: 0.00005562
Iteration 23/1000 | Loss: 0.00005531
Iteration 24/1000 | Loss: 0.00005502
Iteration 25/1000 | Loss: 0.00005460
Iteration 26/1000 | Loss: 0.00005410
Iteration 27/1000 | Loss: 0.00005367
Iteration 28/1000 | Loss: 0.00005332
Iteration 29/1000 | Loss: 0.00040710
Iteration 30/1000 | Loss: 0.00005552
Iteration 31/1000 | Loss: 0.00005299
Iteration 32/1000 | Loss: 0.00005231
Iteration 33/1000 | Loss: 0.00005153
Iteration 34/1000 | Loss: 0.00005097
Iteration 35/1000 | Loss: 0.00059433
Iteration 36/1000 | Loss: 0.00021040
Iteration 37/1000 | Loss: 0.00005895
Iteration 38/1000 | Loss: 0.00005330
Iteration 39/1000 | Loss: 0.00009240
Iteration 40/1000 | Loss: 0.00005191
Iteration 41/1000 | Loss: 0.00004986
Iteration 42/1000 | Loss: 0.00004837
Iteration 43/1000 | Loss: 0.00004759
Iteration 44/1000 | Loss: 0.00004717
Iteration 45/1000 | Loss: 0.00004668
Iteration 46/1000 | Loss: 0.00050517
Iteration 47/1000 | Loss: 0.00004873
Iteration 48/1000 | Loss: 0.00004640
Iteration 49/1000 | Loss: 0.00052278
Iteration 50/1000 | Loss: 0.00043745
Iteration 51/1000 | Loss: 0.00006354
Iteration 52/1000 | Loss: 0.00004613
Iteration 53/1000 | Loss: 0.00004510
Iteration 54/1000 | Loss: 0.00004469
Iteration 55/1000 | Loss: 0.00004433
Iteration 56/1000 | Loss: 0.00059176
Iteration 57/1000 | Loss: 0.00016106
Iteration 58/1000 | Loss: 0.00047713
Iteration 59/1000 | Loss: 0.00014079
Iteration 60/1000 | Loss: 0.00027710
Iteration 61/1000 | Loss: 0.00004768
Iteration 62/1000 | Loss: 0.00004384
Iteration 63/1000 | Loss: 0.00004326
Iteration 64/1000 | Loss: 0.00004278
Iteration 65/1000 | Loss: 0.00004244
Iteration 66/1000 | Loss: 0.00004243
Iteration 67/1000 | Loss: 0.00004215
Iteration 68/1000 | Loss: 0.00004194
Iteration 69/1000 | Loss: 0.00004170
Iteration 70/1000 | Loss: 0.00004157
Iteration 71/1000 | Loss: 0.00004154
Iteration 72/1000 | Loss: 0.00004153
Iteration 73/1000 | Loss: 0.00004153
Iteration 74/1000 | Loss: 0.00004152
Iteration 75/1000 | Loss: 0.00004151
Iteration 76/1000 | Loss: 0.00004151
Iteration 77/1000 | Loss: 0.00004148
Iteration 78/1000 | Loss: 0.00004140
Iteration 79/1000 | Loss: 0.00004138
Iteration 80/1000 | Loss: 0.00004137
Iteration 81/1000 | Loss: 0.00004137
Iteration 82/1000 | Loss: 0.00004131
Iteration 83/1000 | Loss: 0.00004129
Iteration 84/1000 | Loss: 0.00004125
Iteration 85/1000 | Loss: 0.00004121
Iteration 86/1000 | Loss: 0.00004120
Iteration 87/1000 | Loss: 0.00004119
Iteration 88/1000 | Loss: 0.00004118
Iteration 89/1000 | Loss: 0.00004118
Iteration 90/1000 | Loss: 0.00004117
Iteration 91/1000 | Loss: 0.00004116
Iteration 92/1000 | Loss: 0.00004115
Iteration 93/1000 | Loss: 0.00004114
Iteration 94/1000 | Loss: 0.00004113
Iteration 95/1000 | Loss: 0.00004113
Iteration 96/1000 | Loss: 0.00004112
Iteration 97/1000 | Loss: 0.00004112
Iteration 98/1000 | Loss: 0.00004112
Iteration 99/1000 | Loss: 0.00004111
Iteration 100/1000 | Loss: 0.00004111
Iteration 101/1000 | Loss: 0.00004111
Iteration 102/1000 | Loss: 0.00004110
Iteration 103/1000 | Loss: 0.00004110
Iteration 104/1000 | Loss: 0.00004110
Iteration 105/1000 | Loss: 0.00004110
Iteration 106/1000 | Loss: 0.00004109
Iteration 107/1000 | Loss: 0.00004109
Iteration 108/1000 | Loss: 0.00004109
Iteration 109/1000 | Loss: 0.00004109
Iteration 110/1000 | Loss: 0.00004109
Iteration 111/1000 | Loss: 0.00004108
Iteration 112/1000 | Loss: 0.00004108
Iteration 113/1000 | Loss: 0.00004108
Iteration 114/1000 | Loss: 0.00004107
Iteration 115/1000 | Loss: 0.00004107
Iteration 116/1000 | Loss: 0.00004106
Iteration 117/1000 | Loss: 0.00004106
Iteration 118/1000 | Loss: 0.00004106
Iteration 119/1000 | Loss: 0.00004105
Iteration 120/1000 | Loss: 0.00004105
Iteration 121/1000 | Loss: 0.00004105
Iteration 122/1000 | Loss: 0.00004104
Iteration 123/1000 | Loss: 0.00004104
Iteration 124/1000 | Loss: 0.00004103
Iteration 125/1000 | Loss: 0.00004103
Iteration 126/1000 | Loss: 0.00004102
Iteration 127/1000 | Loss: 0.00004102
Iteration 128/1000 | Loss: 0.00004102
Iteration 129/1000 | Loss: 0.00004101
Iteration 130/1000 | Loss: 0.00004101
Iteration 131/1000 | Loss: 0.00004101
Iteration 132/1000 | Loss: 0.00004101
Iteration 133/1000 | Loss: 0.00004101
Iteration 134/1000 | Loss: 0.00004101
Iteration 135/1000 | Loss: 0.00004100
Iteration 136/1000 | Loss: 0.00004100
Iteration 137/1000 | Loss: 0.00004100
Iteration 138/1000 | Loss: 0.00004100
Iteration 139/1000 | Loss: 0.00004100
Iteration 140/1000 | Loss: 0.00004099
Iteration 141/1000 | Loss: 0.00004099
Iteration 142/1000 | Loss: 0.00004099
Iteration 143/1000 | Loss: 0.00004099
Iteration 144/1000 | Loss: 0.00004099
Iteration 145/1000 | Loss: 0.00004099
Iteration 146/1000 | Loss: 0.00004099
Iteration 147/1000 | Loss: 0.00004099
Iteration 148/1000 | Loss: 0.00004098
Iteration 149/1000 | Loss: 0.00004098
Iteration 150/1000 | Loss: 0.00004098
Iteration 151/1000 | Loss: 0.00004098
Iteration 152/1000 | Loss: 0.00004098
Iteration 153/1000 | Loss: 0.00004098
Iteration 154/1000 | Loss: 0.00004098
Iteration 155/1000 | Loss: 0.00004098
Iteration 156/1000 | Loss: 0.00004098
Iteration 157/1000 | Loss: 0.00004098
Iteration 158/1000 | Loss: 0.00004098
Iteration 159/1000 | Loss: 0.00004098
Iteration 160/1000 | Loss: 0.00004098
Iteration 161/1000 | Loss: 0.00004097
Iteration 162/1000 | Loss: 0.00004097
Iteration 163/1000 | Loss: 0.00004097
Iteration 164/1000 | Loss: 0.00004097
Iteration 165/1000 | Loss: 0.00004097
Iteration 166/1000 | Loss: 0.00004097
Iteration 167/1000 | Loss: 0.00004096
Iteration 168/1000 | Loss: 0.00004096
Iteration 169/1000 | Loss: 0.00004096
Iteration 170/1000 | Loss: 0.00004096
Iteration 171/1000 | Loss: 0.00004096
Iteration 172/1000 | Loss: 0.00004096
Iteration 173/1000 | Loss: 0.00004096
Iteration 174/1000 | Loss: 0.00004096
Iteration 175/1000 | Loss: 0.00004096
Iteration 176/1000 | Loss: 0.00004096
Iteration 177/1000 | Loss: 0.00004096
Iteration 178/1000 | Loss: 0.00004096
Iteration 179/1000 | Loss: 0.00004095
Iteration 180/1000 | Loss: 0.00004095
Iteration 181/1000 | Loss: 0.00004095
Iteration 182/1000 | Loss: 0.00004095
Iteration 183/1000 | Loss: 0.00004095
Iteration 184/1000 | Loss: 0.00004095
Iteration 185/1000 | Loss: 0.00004095
Iteration 186/1000 | Loss: 0.00004095
Iteration 187/1000 | Loss: 0.00004095
Iteration 188/1000 | Loss: 0.00004095
Iteration 189/1000 | Loss: 0.00004095
Iteration 190/1000 | Loss: 0.00004095
Iteration 191/1000 | Loss: 0.00004095
Iteration 192/1000 | Loss: 0.00004095
Iteration 193/1000 | Loss: 0.00004095
Iteration 194/1000 | Loss: 0.00004095
Iteration 195/1000 | Loss: 0.00004095
Iteration 196/1000 | Loss: 0.00004095
Iteration 197/1000 | Loss: 0.00004095
Iteration 198/1000 | Loss: 0.00004095
Iteration 199/1000 | Loss: 0.00004095
Iteration 200/1000 | Loss: 0.00004094
Iteration 201/1000 | Loss: 0.00004094
Iteration 202/1000 | Loss: 0.00004094
Iteration 203/1000 | Loss: 0.00004094
Iteration 204/1000 | Loss: 0.00004094
Iteration 205/1000 | Loss: 0.00004094
Iteration 206/1000 | Loss: 0.00004094
Iteration 207/1000 | Loss: 0.00004094
Iteration 208/1000 | Loss: 0.00004094
Iteration 209/1000 | Loss: 0.00004094
Iteration 210/1000 | Loss: 0.00004094
Iteration 211/1000 | Loss: 0.00004094
Iteration 212/1000 | Loss: 0.00004094
Iteration 213/1000 | Loss: 0.00004094
Iteration 214/1000 | Loss: 0.00004094
Iteration 215/1000 | Loss: 0.00004094
Iteration 216/1000 | Loss: 0.00004094
Iteration 217/1000 | Loss: 0.00004094
Iteration 218/1000 | Loss: 0.00004094
Iteration 219/1000 | Loss: 0.00004094
Iteration 220/1000 | Loss: 0.00004093
Iteration 221/1000 | Loss: 0.00004093
Iteration 222/1000 | Loss: 0.00004093
Iteration 223/1000 | Loss: 0.00004093
Iteration 224/1000 | Loss: 0.00004093
Iteration 225/1000 | Loss: 0.00004093
Iteration 226/1000 | Loss: 0.00004093
Iteration 227/1000 | Loss: 0.00004093
Iteration 228/1000 | Loss: 0.00004093
Iteration 229/1000 | Loss: 0.00004093
Iteration 230/1000 | Loss: 0.00004093
Iteration 231/1000 | Loss: 0.00004093
Iteration 232/1000 | Loss: 0.00004093
Iteration 233/1000 | Loss: 0.00004093
Iteration 234/1000 | Loss: 0.00004093
Iteration 235/1000 | Loss: 0.00004092
Iteration 236/1000 | Loss: 0.00004092
Iteration 237/1000 | Loss: 0.00004092
Iteration 238/1000 | Loss: 0.00004092
Iteration 239/1000 | Loss: 0.00004092
Iteration 240/1000 | Loss: 0.00004092
Iteration 241/1000 | Loss: 0.00004092
Iteration 242/1000 | Loss: 0.00004092
Iteration 243/1000 | Loss: 0.00004092
Iteration 244/1000 | Loss: 0.00004092
Iteration 245/1000 | Loss: 0.00004092
Iteration 246/1000 | Loss: 0.00004092
Iteration 247/1000 | Loss: 0.00004092
Iteration 248/1000 | Loss: 0.00004092
Iteration 249/1000 | Loss: 0.00004092
Iteration 250/1000 | Loss: 0.00004092
Iteration 251/1000 | Loss: 0.00004092
Iteration 252/1000 | Loss: 0.00004092
Iteration 253/1000 | Loss: 0.00004092
Iteration 254/1000 | Loss: 0.00004092
Iteration 255/1000 | Loss: 0.00004091
Iteration 256/1000 | Loss: 0.00004091
Iteration 257/1000 | Loss: 0.00004091
Iteration 258/1000 | Loss: 0.00004091
Iteration 259/1000 | Loss: 0.00004091
Iteration 260/1000 | Loss: 0.00004091
Iteration 261/1000 | Loss: 0.00004091
Iteration 262/1000 | Loss: 0.00004091
Iteration 263/1000 | Loss: 0.00004091
Iteration 264/1000 | Loss: 0.00004091
Iteration 265/1000 | Loss: 0.00004091
Iteration 266/1000 | Loss: 0.00004091
Iteration 267/1000 | Loss: 0.00004091
Iteration 268/1000 | Loss: 0.00004091
Iteration 269/1000 | Loss: 0.00004091
Iteration 270/1000 | Loss: 0.00004091
Iteration 271/1000 | Loss: 0.00004091
Iteration 272/1000 | Loss: 0.00004091
Iteration 273/1000 | Loss: 0.00004091
Iteration 274/1000 | Loss: 0.00004091
Iteration 275/1000 | Loss: 0.00004091
Iteration 276/1000 | Loss: 0.00004091
Iteration 277/1000 | Loss: 0.00004091
Iteration 278/1000 | Loss: 0.00004091
Iteration 279/1000 | Loss: 0.00004091
Iteration 280/1000 | Loss: 0.00004091
Iteration 281/1000 | Loss: 0.00004091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [4.090708534931764e-05, 4.090708534931764e-05, 4.090708534931764e-05, 4.090708534931764e-05, 4.090708534931764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.090708534931764e-05

Optimization complete. Final v2v error: 3.6063616275787354 mm

Highest mean error: 11.558937072753906 mm for frame 88

Lowest mean error: 2.6483376026153564 mm for frame 58

Saving results

Total time: 125.38531947135925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831015
Iteration 2/25 | Loss: 0.00155705
Iteration 3/25 | Loss: 0.00139107
Iteration 4/25 | Loss: 0.00136810
Iteration 5/25 | Loss: 0.00136066
Iteration 6/25 | Loss: 0.00135626
Iteration 7/25 | Loss: 0.00135151
Iteration 8/25 | Loss: 0.00135007
Iteration 9/25 | Loss: 0.00134971
Iteration 10/25 | Loss: 0.00134955
Iteration 11/25 | Loss: 0.00134948
Iteration 12/25 | Loss: 0.00134943
Iteration 13/25 | Loss: 0.00134942
Iteration 14/25 | Loss: 0.00134941
Iteration 15/25 | Loss: 0.00134941
Iteration 16/25 | Loss: 0.00134941
Iteration 17/25 | Loss: 0.00134941
Iteration 18/25 | Loss: 0.00134940
Iteration 19/25 | Loss: 0.00134940
Iteration 20/25 | Loss: 0.00134940
Iteration 21/25 | Loss: 0.00134940
Iteration 22/25 | Loss: 0.00134940
Iteration 23/25 | Loss: 0.00134940
Iteration 24/25 | Loss: 0.00134940
Iteration 25/25 | Loss: 0.00134940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31840825
Iteration 2/25 | Loss: 0.00121844
Iteration 3/25 | Loss: 0.00121839
Iteration 4/25 | Loss: 0.00121839
Iteration 5/25 | Loss: 0.00121839
Iteration 6/25 | Loss: 0.00121839
Iteration 7/25 | Loss: 0.00121839
Iteration 8/25 | Loss: 0.00121839
Iteration 9/25 | Loss: 0.00121839
Iteration 10/25 | Loss: 0.00121839
Iteration 11/25 | Loss: 0.00121839
Iteration 12/25 | Loss: 0.00121839
Iteration 13/25 | Loss: 0.00121839
Iteration 14/25 | Loss: 0.00121839
Iteration 15/25 | Loss: 0.00121839
Iteration 16/25 | Loss: 0.00121839
Iteration 17/25 | Loss: 0.00121839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012183889048174024, 0.0012183889048174024, 0.0012183889048174024, 0.0012183889048174024, 0.0012183889048174024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012183889048174024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121839
Iteration 2/1000 | Loss: 0.00013143
Iteration 3/1000 | Loss: 0.00009894
Iteration 4/1000 | Loss: 0.00008169
Iteration 5/1000 | Loss: 0.00007415
Iteration 6/1000 | Loss: 0.00007027
Iteration 7/1000 | Loss: 0.00006773
Iteration 8/1000 | Loss: 0.00006594
Iteration 9/1000 | Loss: 0.00006397
Iteration 10/1000 | Loss: 0.00006292
Iteration 11/1000 | Loss: 0.00006191
Iteration 12/1000 | Loss: 0.00006093
Iteration 13/1000 | Loss: 0.00005966
Iteration 14/1000 | Loss: 0.00005874
Iteration 15/1000 | Loss: 0.00005819
Iteration 16/1000 | Loss: 0.00005746
Iteration 17/1000 | Loss: 0.00005671
Iteration 18/1000 | Loss: 0.00005614
Iteration 19/1000 | Loss: 0.00202098
Iteration 20/1000 | Loss: 0.00109214
Iteration 21/1000 | Loss: 0.00185987
Iteration 22/1000 | Loss: 0.00088193
Iteration 23/1000 | Loss: 0.00087769
Iteration 24/1000 | Loss: 0.00008157
Iteration 25/1000 | Loss: 0.00006250
Iteration 26/1000 | Loss: 0.00108971
Iteration 27/1000 | Loss: 0.00085651
Iteration 28/1000 | Loss: 0.00006705
Iteration 29/1000 | Loss: 0.00005666
Iteration 30/1000 | Loss: 0.00005233
Iteration 31/1000 | Loss: 0.00005016
Iteration 32/1000 | Loss: 0.00032497
Iteration 33/1000 | Loss: 0.00005771
Iteration 34/1000 | Loss: 0.00005218
Iteration 35/1000 | Loss: 0.00004762
Iteration 36/1000 | Loss: 0.00004577
Iteration 37/1000 | Loss: 0.00004471
Iteration 38/1000 | Loss: 0.00004429
Iteration 39/1000 | Loss: 0.00004397
Iteration 40/1000 | Loss: 0.00004354
Iteration 41/1000 | Loss: 0.00004322
Iteration 42/1000 | Loss: 0.00004313
Iteration 43/1000 | Loss: 0.00004309
Iteration 44/1000 | Loss: 0.00004309
Iteration 45/1000 | Loss: 0.00004299
Iteration 46/1000 | Loss: 0.00004277
Iteration 47/1000 | Loss: 0.00004260
Iteration 48/1000 | Loss: 0.00004235
Iteration 49/1000 | Loss: 0.00004221
Iteration 50/1000 | Loss: 0.00004201
Iteration 51/1000 | Loss: 0.00004188
Iteration 52/1000 | Loss: 0.00004188
Iteration 53/1000 | Loss: 0.00004171
Iteration 54/1000 | Loss: 0.00004163
Iteration 55/1000 | Loss: 0.00004160
Iteration 56/1000 | Loss: 0.00004151
Iteration 57/1000 | Loss: 0.00004147
Iteration 58/1000 | Loss: 0.00004147
Iteration 59/1000 | Loss: 0.00004146
Iteration 60/1000 | Loss: 0.00004145
Iteration 61/1000 | Loss: 0.00004137
Iteration 62/1000 | Loss: 0.00004137
Iteration 63/1000 | Loss: 0.00004137
Iteration 64/1000 | Loss: 0.00004137
Iteration 65/1000 | Loss: 0.00004137
Iteration 66/1000 | Loss: 0.00004137
Iteration 67/1000 | Loss: 0.00004137
Iteration 68/1000 | Loss: 0.00004137
Iteration 69/1000 | Loss: 0.00004137
Iteration 70/1000 | Loss: 0.00004137
Iteration 71/1000 | Loss: 0.00004136
Iteration 72/1000 | Loss: 0.00004136
Iteration 73/1000 | Loss: 0.00004136
Iteration 74/1000 | Loss: 0.00004136
Iteration 75/1000 | Loss: 0.00004136
Iteration 76/1000 | Loss: 0.00004135
Iteration 77/1000 | Loss: 0.00004135
Iteration 78/1000 | Loss: 0.00004134
Iteration 79/1000 | Loss: 0.00004134
Iteration 80/1000 | Loss: 0.00004134
Iteration 81/1000 | Loss: 0.00004134
Iteration 82/1000 | Loss: 0.00004134
Iteration 83/1000 | Loss: 0.00004134
Iteration 84/1000 | Loss: 0.00004134
Iteration 85/1000 | Loss: 0.00004134
Iteration 86/1000 | Loss: 0.00004133
Iteration 87/1000 | Loss: 0.00004133
Iteration 88/1000 | Loss: 0.00004133
Iteration 89/1000 | Loss: 0.00004133
Iteration 90/1000 | Loss: 0.00004133
Iteration 91/1000 | Loss: 0.00004133
Iteration 92/1000 | Loss: 0.00004133
Iteration 93/1000 | Loss: 0.00004133
Iteration 94/1000 | Loss: 0.00004133
Iteration 95/1000 | Loss: 0.00004132
Iteration 96/1000 | Loss: 0.00004132
Iteration 97/1000 | Loss: 0.00004132
Iteration 98/1000 | Loss: 0.00004132
Iteration 99/1000 | Loss: 0.00004132
Iteration 100/1000 | Loss: 0.00004132
Iteration 101/1000 | Loss: 0.00004132
Iteration 102/1000 | Loss: 0.00004132
Iteration 103/1000 | Loss: 0.00004131
Iteration 104/1000 | Loss: 0.00004131
Iteration 105/1000 | Loss: 0.00004131
Iteration 106/1000 | Loss: 0.00004131
Iteration 107/1000 | Loss: 0.00004131
Iteration 108/1000 | Loss: 0.00004130
Iteration 109/1000 | Loss: 0.00004130
Iteration 110/1000 | Loss: 0.00004130
Iteration 111/1000 | Loss: 0.00004130
Iteration 112/1000 | Loss: 0.00004130
Iteration 113/1000 | Loss: 0.00004130
Iteration 114/1000 | Loss: 0.00004130
Iteration 115/1000 | Loss: 0.00004130
Iteration 116/1000 | Loss: 0.00004130
Iteration 117/1000 | Loss: 0.00004130
Iteration 118/1000 | Loss: 0.00004129
Iteration 119/1000 | Loss: 0.00004129
Iteration 120/1000 | Loss: 0.00004129
Iteration 121/1000 | Loss: 0.00004128
Iteration 122/1000 | Loss: 0.00004128
Iteration 123/1000 | Loss: 0.00004128
Iteration 124/1000 | Loss: 0.00004128
Iteration 125/1000 | Loss: 0.00004128
Iteration 126/1000 | Loss: 0.00004128
Iteration 127/1000 | Loss: 0.00004128
Iteration 128/1000 | Loss: 0.00004128
Iteration 129/1000 | Loss: 0.00004128
Iteration 130/1000 | Loss: 0.00004128
Iteration 131/1000 | Loss: 0.00004127
Iteration 132/1000 | Loss: 0.00004127
Iteration 133/1000 | Loss: 0.00004127
Iteration 134/1000 | Loss: 0.00004127
Iteration 135/1000 | Loss: 0.00004127
Iteration 136/1000 | Loss: 0.00004127
Iteration 137/1000 | Loss: 0.00004127
Iteration 138/1000 | Loss: 0.00004127
Iteration 139/1000 | Loss: 0.00004127
Iteration 140/1000 | Loss: 0.00004127
Iteration 141/1000 | Loss: 0.00004127
Iteration 142/1000 | Loss: 0.00004127
Iteration 143/1000 | Loss: 0.00004126
Iteration 144/1000 | Loss: 0.00004126
Iteration 145/1000 | Loss: 0.00004126
Iteration 146/1000 | Loss: 0.00004126
Iteration 147/1000 | Loss: 0.00004126
Iteration 148/1000 | Loss: 0.00004126
Iteration 149/1000 | Loss: 0.00004126
Iteration 150/1000 | Loss: 0.00004126
Iteration 151/1000 | Loss: 0.00004126
Iteration 152/1000 | Loss: 0.00004126
Iteration 153/1000 | Loss: 0.00004126
Iteration 154/1000 | Loss: 0.00004126
Iteration 155/1000 | Loss: 0.00004126
Iteration 156/1000 | Loss: 0.00004125
Iteration 157/1000 | Loss: 0.00004125
Iteration 158/1000 | Loss: 0.00004125
Iteration 159/1000 | Loss: 0.00004125
Iteration 160/1000 | Loss: 0.00004125
Iteration 161/1000 | Loss: 0.00004125
Iteration 162/1000 | Loss: 0.00004125
Iteration 163/1000 | Loss: 0.00004125
Iteration 164/1000 | Loss: 0.00004125
Iteration 165/1000 | Loss: 0.00004125
Iteration 166/1000 | Loss: 0.00004125
Iteration 167/1000 | Loss: 0.00004125
Iteration 168/1000 | Loss: 0.00004125
Iteration 169/1000 | Loss: 0.00004125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [4.1249062633141875e-05, 4.1249062633141875e-05, 4.1249062633141875e-05, 4.1249062633141875e-05, 4.1249062633141875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1249062633141875e-05

Optimization complete. Final v2v error: 4.01830530166626 mm

Highest mean error: 12.449604034423828 mm for frame 69

Lowest mean error: 3.3392012119293213 mm for frame 59

Saving results

Total time: 97.21446537971497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000663
Iteration 2/25 | Loss: 0.00240338
Iteration 3/25 | Loss: 0.00182136
Iteration 4/25 | Loss: 0.00171854
Iteration 5/25 | Loss: 0.00164047
Iteration 6/25 | Loss: 0.00162446
Iteration 7/25 | Loss: 0.00158459
Iteration 8/25 | Loss: 0.00151170
Iteration 9/25 | Loss: 0.00148244
Iteration 10/25 | Loss: 0.00146355
Iteration 11/25 | Loss: 0.00142512
Iteration 12/25 | Loss: 0.00140719
Iteration 13/25 | Loss: 0.00139867
Iteration 14/25 | Loss: 0.00139704
Iteration 15/25 | Loss: 0.00137293
Iteration 16/25 | Loss: 0.00135476
Iteration 17/25 | Loss: 0.00134209
Iteration 18/25 | Loss: 0.00133938
Iteration 19/25 | Loss: 0.00133898
Iteration 20/25 | Loss: 0.00134280
Iteration 21/25 | Loss: 0.00134073
Iteration 22/25 | Loss: 0.00133956
Iteration 23/25 | Loss: 0.00133863
Iteration 24/25 | Loss: 0.00133484
Iteration 25/25 | Loss: 0.00132952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.19487882
Iteration 2/25 | Loss: 0.00083771
Iteration 3/25 | Loss: 0.00083771
Iteration 4/25 | Loss: 0.00083771
Iteration 5/25 | Loss: 0.00083771
Iteration 6/25 | Loss: 0.00083771
Iteration 7/25 | Loss: 0.00083771
Iteration 8/25 | Loss: 0.00083771
Iteration 9/25 | Loss: 0.00083771
Iteration 10/25 | Loss: 0.00083771
Iteration 11/25 | Loss: 0.00083771
Iteration 12/25 | Loss: 0.00083771
Iteration 13/25 | Loss: 0.00083771
Iteration 14/25 | Loss: 0.00083771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008377119083888829, 0.0008377119083888829, 0.0008377119083888829, 0.0008377119083888829, 0.0008377119083888829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008377119083888829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083771
Iteration 2/1000 | Loss: 0.00004663
Iteration 3/1000 | Loss: 0.00011695
Iteration 4/1000 | Loss: 0.00002766
Iteration 5/1000 | Loss: 0.00002578
Iteration 6/1000 | Loss: 0.00011124
Iteration 7/1000 | Loss: 0.00002347
Iteration 8/1000 | Loss: 0.00042048
Iteration 9/1000 | Loss: 0.00222533
Iteration 10/1000 | Loss: 0.00230830
Iteration 11/1000 | Loss: 0.00140635
Iteration 12/1000 | Loss: 0.00006096
Iteration 13/1000 | Loss: 0.00004362
Iteration 14/1000 | Loss: 0.00002929
Iteration 15/1000 | Loss: 0.00002797
Iteration 16/1000 | Loss: 0.00002336
Iteration 17/1000 | Loss: 0.00002371
Iteration 18/1000 | Loss: 0.00002195
Iteration 19/1000 | Loss: 0.00002154
Iteration 20/1000 | Loss: 0.00002129
Iteration 21/1000 | Loss: 0.00002120
Iteration 22/1000 | Loss: 0.00002119
Iteration 23/1000 | Loss: 0.00002115
Iteration 24/1000 | Loss: 0.00002113
Iteration 25/1000 | Loss: 0.00002113
Iteration 26/1000 | Loss: 0.00002112
Iteration 27/1000 | Loss: 0.00002108
Iteration 28/1000 | Loss: 0.00002103
Iteration 29/1000 | Loss: 0.00010068
Iteration 30/1000 | Loss: 0.00002096
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002087
Iteration 33/1000 | Loss: 0.00002087
Iteration 34/1000 | Loss: 0.00002087
Iteration 35/1000 | Loss: 0.00002086
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002082
Iteration 38/1000 | Loss: 0.00002082
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002081
Iteration 42/1000 | Loss: 0.00002080
Iteration 43/1000 | Loss: 0.00002080
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002079
Iteration 46/1000 | Loss: 0.00002079
Iteration 47/1000 | Loss: 0.00002078
Iteration 48/1000 | Loss: 0.00002078
Iteration 49/1000 | Loss: 0.00002078
Iteration 50/1000 | Loss: 0.00002078
Iteration 51/1000 | Loss: 0.00002077
Iteration 52/1000 | Loss: 0.00002077
Iteration 53/1000 | Loss: 0.00002077
Iteration 54/1000 | Loss: 0.00002076
Iteration 55/1000 | Loss: 0.00002076
Iteration 56/1000 | Loss: 0.00002076
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002075
Iteration 62/1000 | Loss: 0.00002075
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002074
Iteration 65/1000 | Loss: 0.00011377
Iteration 66/1000 | Loss: 0.00002961
Iteration 67/1000 | Loss: 0.00005610
Iteration 68/1000 | Loss: 0.00002077
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002071
Iteration 84/1000 | Loss: 0.00002071
Iteration 85/1000 | Loss: 0.00002071
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002070
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00008004
Iteration 99/1000 | Loss: 0.00002137
Iteration 100/1000 | Loss: 0.00002075
Iteration 101/1000 | Loss: 0.00002072
Iteration 102/1000 | Loss: 0.00002069
Iteration 103/1000 | Loss: 0.00002069
Iteration 104/1000 | Loss: 0.00002068
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002067
Iteration 114/1000 | Loss: 0.00002067
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002065
Iteration 118/1000 | Loss: 0.00002065
Iteration 119/1000 | Loss: 0.00002065
Iteration 120/1000 | Loss: 0.00002065
Iteration 121/1000 | Loss: 0.00002065
Iteration 122/1000 | Loss: 0.00002065
Iteration 123/1000 | Loss: 0.00002065
Iteration 124/1000 | Loss: 0.00002065
Iteration 125/1000 | Loss: 0.00002065
Iteration 126/1000 | Loss: 0.00002065
Iteration 127/1000 | Loss: 0.00002065
Iteration 128/1000 | Loss: 0.00002065
Iteration 129/1000 | Loss: 0.00002065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.0650353690143675e-05, 2.0650353690143675e-05, 2.0650353690143675e-05, 2.0650353690143675e-05, 2.0650353690143675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0650353690143675e-05

Optimization complete. Final v2v error: 3.7383711338043213 mm

Highest mean error: 7.0050272941589355 mm for frame 79

Lowest mean error: 3.3749518394470215 mm for frame 16

Saving results

Total time: 91.61364531517029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422923
Iteration 2/25 | Loss: 0.00137214
Iteration 3/25 | Loss: 0.00128555
Iteration 4/25 | Loss: 0.00127289
Iteration 5/25 | Loss: 0.00126891
Iteration 6/25 | Loss: 0.00126785
Iteration 7/25 | Loss: 0.00126785
Iteration 8/25 | Loss: 0.00126785
Iteration 9/25 | Loss: 0.00126785
Iteration 10/25 | Loss: 0.00126785
Iteration 11/25 | Loss: 0.00126785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012678502826020122, 0.0012678502826020122, 0.0012678502826020122, 0.0012678502826020122, 0.0012678502826020122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012678502826020122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46744394
Iteration 2/25 | Loss: 0.00088963
Iteration 3/25 | Loss: 0.00088963
Iteration 4/25 | Loss: 0.00088963
Iteration 5/25 | Loss: 0.00088963
Iteration 6/25 | Loss: 0.00088963
Iteration 7/25 | Loss: 0.00088963
Iteration 8/25 | Loss: 0.00088963
Iteration 9/25 | Loss: 0.00088963
Iteration 10/25 | Loss: 0.00088963
Iteration 11/25 | Loss: 0.00088963
Iteration 12/25 | Loss: 0.00088963
Iteration 13/25 | Loss: 0.00088963
Iteration 14/25 | Loss: 0.00088963
Iteration 15/25 | Loss: 0.00088963
Iteration 16/25 | Loss: 0.00088963
Iteration 17/25 | Loss: 0.00088963
Iteration 18/25 | Loss: 0.00088963
Iteration 19/25 | Loss: 0.00088962
Iteration 20/25 | Loss: 0.00088962
Iteration 21/25 | Loss: 0.00088962
Iteration 22/25 | Loss: 0.00088962
Iteration 23/25 | Loss: 0.00088962
Iteration 24/25 | Loss: 0.00088962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000889624934643507, 0.000889624934643507, 0.000889624934643507, 0.000889624934643507, 0.000889624934643507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000889624934643507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088962
Iteration 2/1000 | Loss: 0.00002749
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00001611
Iteration 7/1000 | Loss: 0.00001581
Iteration 8/1000 | Loss: 0.00001565
Iteration 9/1000 | Loss: 0.00001565
Iteration 10/1000 | Loss: 0.00001543
Iteration 11/1000 | Loss: 0.00001537
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001513
Iteration 17/1000 | Loss: 0.00001512
Iteration 18/1000 | Loss: 0.00001512
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001510
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001506
Iteration 28/1000 | Loss: 0.00001505
Iteration 29/1000 | Loss: 0.00001505
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001503
Iteration 32/1000 | Loss: 0.00001503
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001502
Iteration 35/1000 | Loss: 0.00001502
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001500
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001500
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001498
Iteration 49/1000 | Loss: 0.00001498
Iteration 50/1000 | Loss: 0.00001498
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001496
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001494
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001490
Iteration 75/1000 | Loss: 0.00001490
Iteration 76/1000 | Loss: 0.00001490
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001489
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001489
Iteration 86/1000 | Loss: 0.00001489
Iteration 87/1000 | Loss: 0.00001488
Iteration 88/1000 | Loss: 0.00001488
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001488
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001486
Iteration 97/1000 | Loss: 0.00001486
Iteration 98/1000 | Loss: 0.00001486
Iteration 99/1000 | Loss: 0.00001485
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001484
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001484
Iteration 104/1000 | Loss: 0.00001483
Iteration 105/1000 | Loss: 0.00001483
Iteration 106/1000 | Loss: 0.00001483
Iteration 107/1000 | Loss: 0.00001483
Iteration 108/1000 | Loss: 0.00001482
Iteration 109/1000 | Loss: 0.00001482
Iteration 110/1000 | Loss: 0.00001482
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001481
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001481
Iteration 119/1000 | Loss: 0.00001480
Iteration 120/1000 | Loss: 0.00001480
Iteration 121/1000 | Loss: 0.00001480
Iteration 122/1000 | Loss: 0.00001480
Iteration 123/1000 | Loss: 0.00001480
Iteration 124/1000 | Loss: 0.00001480
Iteration 125/1000 | Loss: 0.00001480
Iteration 126/1000 | Loss: 0.00001480
Iteration 127/1000 | Loss: 0.00001480
Iteration 128/1000 | Loss: 0.00001480
Iteration 129/1000 | Loss: 0.00001480
Iteration 130/1000 | Loss: 0.00001480
Iteration 131/1000 | Loss: 0.00001480
Iteration 132/1000 | Loss: 0.00001479
Iteration 133/1000 | Loss: 0.00001479
Iteration 134/1000 | Loss: 0.00001479
Iteration 135/1000 | Loss: 0.00001479
Iteration 136/1000 | Loss: 0.00001479
Iteration 137/1000 | Loss: 0.00001479
Iteration 138/1000 | Loss: 0.00001479
Iteration 139/1000 | Loss: 0.00001478
Iteration 140/1000 | Loss: 0.00001478
Iteration 141/1000 | Loss: 0.00001478
Iteration 142/1000 | Loss: 0.00001478
Iteration 143/1000 | Loss: 0.00001478
Iteration 144/1000 | Loss: 0.00001478
Iteration 145/1000 | Loss: 0.00001478
Iteration 146/1000 | Loss: 0.00001478
Iteration 147/1000 | Loss: 0.00001478
Iteration 148/1000 | Loss: 0.00001477
Iteration 149/1000 | Loss: 0.00001477
Iteration 150/1000 | Loss: 0.00001477
Iteration 151/1000 | Loss: 0.00001477
Iteration 152/1000 | Loss: 0.00001477
Iteration 153/1000 | Loss: 0.00001477
Iteration 154/1000 | Loss: 0.00001477
Iteration 155/1000 | Loss: 0.00001477
Iteration 156/1000 | Loss: 0.00001477
Iteration 157/1000 | Loss: 0.00001477
Iteration 158/1000 | Loss: 0.00001476
Iteration 159/1000 | Loss: 0.00001476
Iteration 160/1000 | Loss: 0.00001476
Iteration 161/1000 | Loss: 0.00001476
Iteration 162/1000 | Loss: 0.00001476
Iteration 163/1000 | Loss: 0.00001476
Iteration 164/1000 | Loss: 0.00001476
Iteration 165/1000 | Loss: 0.00001476
Iteration 166/1000 | Loss: 0.00001476
Iteration 167/1000 | Loss: 0.00001475
Iteration 168/1000 | Loss: 0.00001475
Iteration 169/1000 | Loss: 0.00001475
Iteration 170/1000 | Loss: 0.00001475
Iteration 171/1000 | Loss: 0.00001475
Iteration 172/1000 | Loss: 0.00001475
Iteration 173/1000 | Loss: 0.00001475
Iteration 174/1000 | Loss: 0.00001475
Iteration 175/1000 | Loss: 0.00001475
Iteration 176/1000 | Loss: 0.00001475
Iteration 177/1000 | Loss: 0.00001475
Iteration 178/1000 | Loss: 0.00001475
Iteration 179/1000 | Loss: 0.00001475
Iteration 180/1000 | Loss: 0.00001475
Iteration 181/1000 | Loss: 0.00001475
Iteration 182/1000 | Loss: 0.00001475
Iteration 183/1000 | Loss: 0.00001475
Iteration 184/1000 | Loss: 0.00001475
Iteration 185/1000 | Loss: 0.00001474
Iteration 186/1000 | Loss: 0.00001474
Iteration 187/1000 | Loss: 0.00001474
Iteration 188/1000 | Loss: 0.00001474
Iteration 189/1000 | Loss: 0.00001474
Iteration 190/1000 | Loss: 0.00001474
Iteration 191/1000 | Loss: 0.00001474
Iteration 192/1000 | Loss: 0.00001474
Iteration 193/1000 | Loss: 0.00001474
Iteration 194/1000 | Loss: 0.00001474
Iteration 195/1000 | Loss: 0.00001474
Iteration 196/1000 | Loss: 0.00001474
Iteration 197/1000 | Loss: 0.00001474
Iteration 198/1000 | Loss: 0.00001474
Iteration 199/1000 | Loss: 0.00001474
Iteration 200/1000 | Loss: 0.00001474
Iteration 201/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.474191049055662e-05, 1.474191049055662e-05, 1.474191049055662e-05, 1.474191049055662e-05, 1.474191049055662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.474191049055662e-05

Optimization complete. Final v2v error: 3.2478561401367188 mm

Highest mean error: 3.6072583198547363 mm for frame 6

Lowest mean error: 3.088275909423828 mm for frame 114

Saving results

Total time: 37.15388059616089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406326
Iteration 2/25 | Loss: 0.00133942
Iteration 3/25 | Loss: 0.00123040
Iteration 4/25 | Loss: 0.00121598
Iteration 5/25 | Loss: 0.00121340
Iteration 6/25 | Loss: 0.00121317
Iteration 7/25 | Loss: 0.00121317
Iteration 8/25 | Loss: 0.00121317
Iteration 9/25 | Loss: 0.00121317
Iteration 10/25 | Loss: 0.00121317
Iteration 11/25 | Loss: 0.00121317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012131716357544065, 0.0012131716357544065, 0.0012131716357544065, 0.0012131716357544065, 0.0012131716357544065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012131716357544065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46323681
Iteration 2/25 | Loss: 0.00065979
Iteration 3/25 | Loss: 0.00065978
Iteration 4/25 | Loss: 0.00065978
Iteration 5/25 | Loss: 0.00065978
Iteration 6/25 | Loss: 0.00065978
Iteration 7/25 | Loss: 0.00065978
Iteration 8/25 | Loss: 0.00065978
Iteration 9/25 | Loss: 0.00065978
Iteration 10/25 | Loss: 0.00065978
Iteration 11/25 | Loss: 0.00065978
Iteration 12/25 | Loss: 0.00065978
Iteration 13/25 | Loss: 0.00065978
Iteration 14/25 | Loss: 0.00065978
Iteration 15/25 | Loss: 0.00065978
Iteration 16/25 | Loss: 0.00065978
Iteration 17/25 | Loss: 0.00065978
Iteration 18/25 | Loss: 0.00065978
Iteration 19/25 | Loss: 0.00065978
Iteration 20/25 | Loss: 0.00065978
Iteration 21/25 | Loss: 0.00065978
Iteration 22/25 | Loss: 0.00065978
Iteration 23/25 | Loss: 0.00065978
Iteration 24/25 | Loss: 0.00065978
Iteration 25/25 | Loss: 0.00065978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065978
Iteration 2/1000 | Loss: 0.00002661
Iteration 3/1000 | Loss: 0.00002047
Iteration 4/1000 | Loss: 0.00001882
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001641
Iteration 7/1000 | Loss: 0.00001581
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001530
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001461
Iteration 14/1000 | Loss: 0.00001457
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001447
Iteration 17/1000 | Loss: 0.00001446
Iteration 18/1000 | Loss: 0.00001443
Iteration 19/1000 | Loss: 0.00001440
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001439
Iteration 22/1000 | Loss: 0.00001439
Iteration 23/1000 | Loss: 0.00001433
Iteration 24/1000 | Loss: 0.00001431
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001425
Iteration 27/1000 | Loss: 0.00001424
Iteration 28/1000 | Loss: 0.00001420
Iteration 29/1000 | Loss: 0.00001420
Iteration 30/1000 | Loss: 0.00001419
Iteration 31/1000 | Loss: 0.00001419
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001405
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001404
Iteration 42/1000 | Loss: 0.00001404
Iteration 43/1000 | Loss: 0.00001404
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001399
Iteration 54/1000 | Loss: 0.00001399
Iteration 55/1000 | Loss: 0.00001398
Iteration 56/1000 | Loss: 0.00001398
Iteration 57/1000 | Loss: 0.00001398
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001397
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001393
Iteration 76/1000 | Loss: 0.00001393
Iteration 77/1000 | Loss: 0.00001393
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001392
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001390
Iteration 84/1000 | Loss: 0.00001390
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.3880804544896819e-05, 1.3880804544896819e-05, 1.3880804544896819e-05, 1.3880804544896819e-05, 1.3880804544896819e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3880804544896819e-05

Optimization complete. Final v2v error: 3.1896986961364746 mm

Highest mean error: 3.483567714691162 mm for frame 74

Lowest mean error: 2.987354040145874 mm for frame 18

Saving results

Total time: 37.266382455825806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039057
Iteration 2/25 | Loss: 0.00179455
Iteration 3/25 | Loss: 0.00178309
Iteration 4/25 | Loss: 0.00147930
Iteration 5/25 | Loss: 0.00142040
Iteration 6/25 | Loss: 0.00143918
Iteration 7/25 | Loss: 0.00142809
Iteration 8/25 | Loss: 0.00135408
Iteration 9/25 | Loss: 0.00131286
Iteration 10/25 | Loss: 0.00133254
Iteration 11/25 | Loss: 0.00128275
Iteration 12/25 | Loss: 0.00127530
Iteration 13/25 | Loss: 0.00126993
Iteration 14/25 | Loss: 0.00127412
Iteration 15/25 | Loss: 0.00128285
Iteration 16/25 | Loss: 0.00127799
Iteration 17/25 | Loss: 0.00129040
Iteration 18/25 | Loss: 0.00128346
Iteration 19/25 | Loss: 0.00127615
Iteration 20/25 | Loss: 0.00127882
Iteration 21/25 | Loss: 0.00127799
Iteration 22/25 | Loss: 0.00127093
Iteration 23/25 | Loss: 0.00127032
Iteration 24/25 | Loss: 0.00127343
Iteration 25/25 | Loss: 0.00127831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48821831
Iteration 2/25 | Loss: 0.00100310
Iteration 3/25 | Loss: 0.00100310
Iteration 4/25 | Loss: 0.00100310
Iteration 5/25 | Loss: 0.00100310
Iteration 6/25 | Loss: 0.00098520
Iteration 7/25 | Loss: 0.00098520
Iteration 8/25 | Loss: 0.00098520
Iteration 9/25 | Loss: 0.00098520
Iteration 10/25 | Loss: 0.00098520
Iteration 11/25 | Loss: 0.00098520
Iteration 12/25 | Loss: 0.00098520
Iteration 13/25 | Loss: 0.00098520
Iteration 14/25 | Loss: 0.00098520
Iteration 15/25 | Loss: 0.00098520
Iteration 16/25 | Loss: 0.00098520
Iteration 17/25 | Loss: 0.00098520
Iteration 18/25 | Loss: 0.00098520
Iteration 19/25 | Loss: 0.00098520
Iteration 20/25 | Loss: 0.00098520
Iteration 21/25 | Loss: 0.00098520
Iteration 22/25 | Loss: 0.00098520
Iteration 23/25 | Loss: 0.00098520
Iteration 24/25 | Loss: 0.00098520
Iteration 25/25 | Loss: 0.00098520

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098520
Iteration 2/1000 | Loss: 0.00028190
Iteration 3/1000 | Loss: 0.00169085
Iteration 4/1000 | Loss: 0.00004369
Iteration 5/1000 | Loss: 0.00003895
Iteration 6/1000 | Loss: 0.00003259
Iteration 7/1000 | Loss: 0.00240423
Iteration 8/1000 | Loss: 0.00054036
Iteration 9/1000 | Loss: 0.00056881
Iteration 10/1000 | Loss: 0.00039684
Iteration 11/1000 | Loss: 0.00042005
Iteration 12/1000 | Loss: 0.00021591
Iteration 13/1000 | Loss: 0.00005506
Iteration 14/1000 | Loss: 0.00003746
Iteration 15/1000 | Loss: 0.00003337
Iteration 16/1000 | Loss: 0.00013873
Iteration 17/1000 | Loss: 0.00003128
Iteration 18/1000 | Loss: 0.00003733
Iteration 19/1000 | Loss: 0.00002510
Iteration 20/1000 | Loss: 0.00002157
Iteration 21/1000 | Loss: 0.00002029
Iteration 22/1000 | Loss: 0.00001951
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001816
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00001694
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001589
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001563
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001545
Iteration 45/1000 | Loss: 0.00001544
Iteration 46/1000 | Loss: 0.00001544
Iteration 47/1000 | Loss: 0.00001543
Iteration 48/1000 | Loss: 0.00001543
Iteration 49/1000 | Loss: 0.00001542
Iteration 50/1000 | Loss: 0.00001542
Iteration 51/1000 | Loss: 0.00001542
Iteration 52/1000 | Loss: 0.00001542
Iteration 53/1000 | Loss: 0.00001542
Iteration 54/1000 | Loss: 0.00001542
Iteration 55/1000 | Loss: 0.00001541
Iteration 56/1000 | Loss: 0.00001541
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001540
Iteration 61/1000 | Loss: 0.00001540
Iteration 62/1000 | Loss: 0.00001540
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001539
Iteration 66/1000 | Loss: 0.00001539
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001539
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001538
Iteration 73/1000 | Loss: 0.00001538
Iteration 74/1000 | Loss: 0.00001538
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001537
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001536
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001536
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00003212
Iteration 87/1000 | Loss: 0.00003212
Iteration 88/1000 | Loss: 0.00003212
Iteration 89/1000 | Loss: 0.00003212
Iteration 90/1000 | Loss: 0.00003212
Iteration 91/1000 | Loss: 0.00003212
Iteration 92/1000 | Loss: 0.00003212
Iteration 93/1000 | Loss: 0.00003212
Iteration 94/1000 | Loss: 0.00003212
Iteration 95/1000 | Loss: 0.00003212
Iteration 96/1000 | Loss: 0.00003212
Iteration 97/1000 | Loss: 0.00003212
Iteration 98/1000 | Loss: 0.00003212
Iteration 99/1000 | Loss: 0.00003211
Iteration 100/1000 | Loss: 0.00003211
Iteration 101/1000 | Loss: 0.00003211
Iteration 102/1000 | Loss: 0.00003211
Iteration 103/1000 | Loss: 0.00003211
Iteration 104/1000 | Loss: 0.00003211
Iteration 105/1000 | Loss: 0.00003211
Iteration 106/1000 | Loss: 0.00003211
Iteration 107/1000 | Loss: 0.00003211
Iteration 108/1000 | Loss: 0.00003211
Iteration 109/1000 | Loss: 0.00003211
Iteration 110/1000 | Loss: 0.00003211
Iteration 111/1000 | Loss: 0.00003211
Iteration 112/1000 | Loss: 0.00003211
Iteration 113/1000 | Loss: 0.00003211
Iteration 114/1000 | Loss: 0.00003211
Iteration 115/1000 | Loss: 0.00003211
Iteration 116/1000 | Loss: 0.00003211
Iteration 117/1000 | Loss: 0.00003211
Iteration 118/1000 | Loss: 0.00003211
Iteration 119/1000 | Loss: 0.00003211
Iteration 120/1000 | Loss: 0.00003211
Iteration 121/1000 | Loss: 0.00003211
Iteration 122/1000 | Loss: 0.00003211
Iteration 123/1000 | Loss: 0.00003211
Iteration 124/1000 | Loss: 0.00003211
Iteration 125/1000 | Loss: 0.00003211
Iteration 126/1000 | Loss: 0.00003211
Iteration 127/1000 | Loss: 0.00003211
Iteration 128/1000 | Loss: 0.00003211
Iteration 129/1000 | Loss: 0.00003211
Iteration 130/1000 | Loss: 0.00003211
Iteration 131/1000 | Loss: 0.00003211
Iteration 132/1000 | Loss: 0.00003211
Iteration 133/1000 | Loss: 0.00003211
Iteration 134/1000 | Loss: 0.00003211
Iteration 135/1000 | Loss: 0.00003211
Iteration 136/1000 | Loss: 0.00003211
Iteration 137/1000 | Loss: 0.00003211
Iteration 138/1000 | Loss: 0.00003211
Iteration 139/1000 | Loss: 0.00003211
Iteration 140/1000 | Loss: 0.00003211
Iteration 141/1000 | Loss: 0.00003211
Iteration 142/1000 | Loss: 0.00003211
Iteration 143/1000 | Loss: 0.00003211
Iteration 144/1000 | Loss: 0.00003211
Iteration 145/1000 | Loss: 0.00003211
Iteration 146/1000 | Loss: 0.00003211
Iteration 147/1000 | Loss: 0.00003211
Iteration 148/1000 | Loss: 0.00003211
Iteration 149/1000 | Loss: 0.00003211
Iteration 150/1000 | Loss: 0.00003211
Iteration 151/1000 | Loss: 0.00003211
Iteration 152/1000 | Loss: 0.00003211
Iteration 153/1000 | Loss: 0.00003211
Iteration 154/1000 | Loss: 0.00003211
Iteration 155/1000 | Loss: 0.00003211
Iteration 156/1000 | Loss: 0.00003211
Iteration 157/1000 | Loss: 0.00003211
Iteration 158/1000 | Loss: 0.00003211
Iteration 159/1000 | Loss: 0.00003211
Iteration 160/1000 | Loss: 0.00003211
Iteration 161/1000 | Loss: 0.00003211
Iteration 162/1000 | Loss: 0.00003211
Iteration 163/1000 | Loss: 0.00003211
Iteration 164/1000 | Loss: 0.00003211
Iteration 165/1000 | Loss: 0.00003211
Iteration 166/1000 | Loss: 0.00003211
Iteration 167/1000 | Loss: 0.00003211
Iteration 168/1000 | Loss: 0.00003211
Iteration 169/1000 | Loss: 0.00003211
Iteration 170/1000 | Loss: 0.00003211
Iteration 171/1000 | Loss: 0.00003211
Iteration 172/1000 | Loss: 0.00003211
Iteration 173/1000 | Loss: 0.00003211
Iteration 174/1000 | Loss: 0.00003211
Iteration 175/1000 | Loss: 0.00003211
Iteration 176/1000 | Loss: 0.00003211
Iteration 177/1000 | Loss: 0.00003211
Iteration 178/1000 | Loss: 0.00003211
Iteration 179/1000 | Loss: 0.00003211
Iteration 180/1000 | Loss: 0.00003211
Iteration 181/1000 | Loss: 0.00003211
Iteration 182/1000 | Loss: 0.00003211
Iteration 183/1000 | Loss: 0.00003211
Iteration 184/1000 | Loss: 0.00003211
Iteration 185/1000 | Loss: 0.00003211
Iteration 186/1000 | Loss: 0.00003211
Iteration 187/1000 | Loss: 0.00003211
Iteration 188/1000 | Loss: 0.00003211
Iteration 189/1000 | Loss: 0.00003211
Iteration 190/1000 | Loss: 0.00003211
Iteration 191/1000 | Loss: 0.00003211
Iteration 192/1000 | Loss: 0.00003211
Iteration 193/1000 | Loss: 0.00003211
Iteration 194/1000 | Loss: 0.00003211
Iteration 195/1000 | Loss: 0.00003211
Iteration 196/1000 | Loss: 0.00003211
Iteration 197/1000 | Loss: 0.00003211
Iteration 198/1000 | Loss: 0.00003211
Iteration 199/1000 | Loss: 0.00003211
Iteration 200/1000 | Loss: 0.00003211
Iteration 201/1000 | Loss: 0.00003211
Iteration 202/1000 | Loss: 0.00003211
Iteration 203/1000 | Loss: 0.00003211
Iteration 204/1000 | Loss: 0.00005023
Iteration 205/1000 | Loss: 0.00002196
Iteration 206/1000 | Loss: 0.00001542
Iteration 207/1000 | Loss: 0.00001534
Iteration 208/1000 | Loss: 0.00001531
Iteration 209/1000 | Loss: 0.00001531
Iteration 210/1000 | Loss: 0.00001531
Iteration 211/1000 | Loss: 0.00001531
Iteration 212/1000 | Loss: 0.00001531
Iteration 213/1000 | Loss: 0.00001531
Iteration 214/1000 | Loss: 0.00001531
Iteration 215/1000 | Loss: 0.00001530
Iteration 216/1000 | Loss: 0.00001530
Iteration 217/1000 | Loss: 0.00001530
Iteration 218/1000 | Loss: 0.00001530
Iteration 219/1000 | Loss: 0.00001530
Iteration 220/1000 | Loss: 0.00001530
Iteration 221/1000 | Loss: 0.00001530
Iteration 222/1000 | Loss: 0.00001530
Iteration 223/1000 | Loss: 0.00001530
Iteration 224/1000 | Loss: 0.00001530
Iteration 225/1000 | Loss: 0.00001530
Iteration 226/1000 | Loss: 0.00001530
Iteration 227/1000 | Loss: 0.00001529
Iteration 228/1000 | Loss: 0.00001529
Iteration 229/1000 | Loss: 0.00001529
Iteration 230/1000 | Loss: 0.00001529
Iteration 231/1000 | Loss: 0.00001529
Iteration 232/1000 | Loss: 0.00001529
Iteration 233/1000 | Loss: 0.00001529
Iteration 234/1000 | Loss: 0.00001529
Iteration 235/1000 | Loss: 0.00001529
Iteration 236/1000 | Loss: 0.00001529
Iteration 237/1000 | Loss: 0.00001529
Iteration 238/1000 | Loss: 0.00001529
Iteration 239/1000 | Loss: 0.00001529
Iteration 240/1000 | Loss: 0.00001529
Iteration 241/1000 | Loss: 0.00001529
Iteration 242/1000 | Loss: 0.00001529
Iteration 243/1000 | Loss: 0.00001529
Iteration 244/1000 | Loss: 0.00001529
Iteration 245/1000 | Loss: 0.00001529
Iteration 246/1000 | Loss: 0.00001529
Iteration 247/1000 | Loss: 0.00001529
Iteration 248/1000 | Loss: 0.00001529
Iteration 249/1000 | Loss: 0.00001529
Iteration 250/1000 | Loss: 0.00001529
Iteration 251/1000 | Loss: 0.00001529
Iteration 252/1000 | Loss: 0.00001529
Iteration 253/1000 | Loss: 0.00001529
Iteration 254/1000 | Loss: 0.00001529
Iteration 255/1000 | Loss: 0.00001529
Iteration 256/1000 | Loss: 0.00001529
Iteration 257/1000 | Loss: 0.00001529
Iteration 258/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.5287512724171393e-05, 1.5287512724171393e-05, 1.5287512724171393e-05, 1.5287512724171393e-05, 1.5287512724171393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5287512724171393e-05

Optimization complete. Final v2v error: 3.3206300735473633 mm

Highest mean error: 4.199567794799805 mm for frame 82

Lowest mean error: 3.1089541912078857 mm for frame 134

Saving results

Total time: 112.93679690361023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023711
Iteration 2/25 | Loss: 0.00263403
Iteration 3/25 | Loss: 0.00214461
Iteration 4/25 | Loss: 0.00212889
Iteration 5/25 | Loss: 0.00183220
Iteration 6/25 | Loss: 0.00168311
Iteration 7/25 | Loss: 0.00160505
Iteration 8/25 | Loss: 0.00158125
Iteration 9/25 | Loss: 0.00157472
Iteration 10/25 | Loss: 0.00153186
Iteration 11/25 | Loss: 0.00152755
Iteration 12/25 | Loss: 0.00151930
Iteration 13/25 | Loss: 0.00151047
Iteration 14/25 | Loss: 0.00150664
Iteration 15/25 | Loss: 0.00150602
Iteration 16/25 | Loss: 0.00150479
Iteration 17/25 | Loss: 0.00151010
Iteration 18/25 | Loss: 0.00150132
Iteration 19/25 | Loss: 0.00150039
Iteration 20/25 | Loss: 0.00150025
Iteration 21/25 | Loss: 0.00150025
Iteration 22/25 | Loss: 0.00150025
Iteration 23/25 | Loss: 0.00150024
Iteration 24/25 | Loss: 0.00150024
Iteration 25/25 | Loss: 0.00150024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44853842
Iteration 2/25 | Loss: 0.00315030
Iteration 3/25 | Loss: 0.00284981
Iteration 4/25 | Loss: 0.00284981
Iteration 5/25 | Loss: 0.00284981
Iteration 6/25 | Loss: 0.00284981
Iteration 7/25 | Loss: 0.00284980
Iteration 8/25 | Loss: 0.00284980
Iteration 9/25 | Loss: 0.00284980
Iteration 10/25 | Loss: 0.00284980
Iteration 11/25 | Loss: 0.00284980
Iteration 12/25 | Loss: 0.00284980
Iteration 13/25 | Loss: 0.00284980
Iteration 14/25 | Loss: 0.00284980
Iteration 15/25 | Loss: 0.00284980
Iteration 16/25 | Loss: 0.00284980
Iteration 17/25 | Loss: 0.00284980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0028498037718236446, 0.0028498037718236446, 0.0028498037718236446, 0.0028498037718236446, 0.0028498037718236446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028498037718236446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284980
Iteration 2/1000 | Loss: 0.00041086
Iteration 3/1000 | Loss: 0.00051686
Iteration 4/1000 | Loss: 0.00024728
Iteration 5/1000 | Loss: 0.00253546
Iteration 6/1000 | Loss: 0.00031227
Iteration 7/1000 | Loss: 0.00066930
Iteration 8/1000 | Loss: 0.00019495
Iteration 9/1000 | Loss: 0.00073562
Iteration 10/1000 | Loss: 0.00072102
Iteration 11/1000 | Loss: 0.00023348
Iteration 12/1000 | Loss: 0.00014661
Iteration 13/1000 | Loss: 0.00053626
Iteration 14/1000 | Loss: 0.00077492
Iteration 15/1000 | Loss: 0.00058435
Iteration 16/1000 | Loss: 0.00047655
Iteration 17/1000 | Loss: 0.00024889
Iteration 18/1000 | Loss: 0.00035029
Iteration 19/1000 | Loss: 0.00028228
Iteration 20/1000 | Loss: 0.00014335
Iteration 21/1000 | Loss: 0.00011266
Iteration 22/1000 | Loss: 0.00033114
Iteration 23/1000 | Loss: 0.00010521
Iteration 24/1000 | Loss: 0.00014001
Iteration 25/1000 | Loss: 0.00075802
Iteration 26/1000 | Loss: 0.00084633
Iteration 27/1000 | Loss: 0.00074019
Iteration 28/1000 | Loss: 0.00009559
Iteration 29/1000 | Loss: 0.00019104
Iteration 30/1000 | Loss: 0.00011500
Iteration 31/1000 | Loss: 0.00009343
Iteration 32/1000 | Loss: 0.00021711
Iteration 33/1000 | Loss: 0.00008561
Iteration 34/1000 | Loss: 0.00008177
Iteration 35/1000 | Loss: 0.00008019
Iteration 36/1000 | Loss: 0.00014505
Iteration 37/1000 | Loss: 0.00087673
Iteration 38/1000 | Loss: 0.00008889
Iteration 39/1000 | Loss: 0.00014855
Iteration 40/1000 | Loss: 0.00007722
Iteration 41/1000 | Loss: 0.00010077
Iteration 42/1000 | Loss: 0.00014759
Iteration 43/1000 | Loss: 0.00007937
Iteration 44/1000 | Loss: 0.00007501
Iteration 45/1000 | Loss: 0.00008923
Iteration 46/1000 | Loss: 0.00007304
Iteration 47/1000 | Loss: 0.00007128
Iteration 48/1000 | Loss: 0.00006996
Iteration 49/1000 | Loss: 0.00015296
Iteration 50/1000 | Loss: 0.00006997
Iteration 51/1000 | Loss: 0.00006773
Iteration 52/1000 | Loss: 0.00006644
Iteration 53/1000 | Loss: 0.00014101
Iteration 54/1000 | Loss: 0.00006529
Iteration 55/1000 | Loss: 0.00006449
Iteration 56/1000 | Loss: 0.00006375
Iteration 57/1000 | Loss: 0.00006323
Iteration 58/1000 | Loss: 0.00015526
Iteration 59/1000 | Loss: 0.00006892
Iteration 60/1000 | Loss: 0.00006276
Iteration 61/1000 | Loss: 0.00008545
Iteration 62/1000 | Loss: 0.00006251
Iteration 63/1000 | Loss: 0.00006219
Iteration 64/1000 | Loss: 0.00006191
Iteration 65/1000 | Loss: 0.00006177
Iteration 66/1000 | Loss: 0.00006177
Iteration 67/1000 | Loss: 0.00006162
Iteration 68/1000 | Loss: 0.00006162
Iteration 69/1000 | Loss: 0.00006160
Iteration 70/1000 | Loss: 0.00006154
Iteration 71/1000 | Loss: 0.00006149
Iteration 72/1000 | Loss: 0.00006146
Iteration 73/1000 | Loss: 0.00013608
Iteration 74/1000 | Loss: 0.00027892
Iteration 75/1000 | Loss: 0.00009007
Iteration 76/1000 | Loss: 0.00007008
Iteration 77/1000 | Loss: 0.00006158
Iteration 78/1000 | Loss: 0.00006136
Iteration 79/1000 | Loss: 0.00006132
Iteration 80/1000 | Loss: 0.00006132
Iteration 81/1000 | Loss: 0.00006132
Iteration 82/1000 | Loss: 0.00006132
Iteration 83/1000 | Loss: 0.00006132
Iteration 84/1000 | Loss: 0.00006131
Iteration 85/1000 | Loss: 0.00006131
Iteration 86/1000 | Loss: 0.00006131
Iteration 87/1000 | Loss: 0.00006131
Iteration 88/1000 | Loss: 0.00006131
Iteration 89/1000 | Loss: 0.00006131
Iteration 90/1000 | Loss: 0.00006131
Iteration 91/1000 | Loss: 0.00006130
Iteration 92/1000 | Loss: 0.00006130
Iteration 93/1000 | Loss: 0.00006130
Iteration 94/1000 | Loss: 0.00006130
Iteration 95/1000 | Loss: 0.00006130
Iteration 96/1000 | Loss: 0.00006129
Iteration 97/1000 | Loss: 0.00006129
Iteration 98/1000 | Loss: 0.00006129
Iteration 99/1000 | Loss: 0.00006129
Iteration 100/1000 | Loss: 0.00006129
Iteration 101/1000 | Loss: 0.00006129
Iteration 102/1000 | Loss: 0.00006128
Iteration 103/1000 | Loss: 0.00006128
Iteration 104/1000 | Loss: 0.00006127
Iteration 105/1000 | Loss: 0.00006127
Iteration 106/1000 | Loss: 0.00006127
Iteration 107/1000 | Loss: 0.00006127
Iteration 108/1000 | Loss: 0.00006126
Iteration 109/1000 | Loss: 0.00006126
Iteration 110/1000 | Loss: 0.00006125
Iteration 111/1000 | Loss: 0.00006125
Iteration 112/1000 | Loss: 0.00006125
Iteration 113/1000 | Loss: 0.00006125
Iteration 114/1000 | Loss: 0.00006124
Iteration 115/1000 | Loss: 0.00006124
Iteration 116/1000 | Loss: 0.00006124
Iteration 117/1000 | Loss: 0.00006124
Iteration 118/1000 | Loss: 0.00006124
Iteration 119/1000 | Loss: 0.00006124
Iteration 120/1000 | Loss: 0.00006124
Iteration 121/1000 | Loss: 0.00006124
Iteration 122/1000 | Loss: 0.00006124
Iteration 123/1000 | Loss: 0.00006123
Iteration 124/1000 | Loss: 0.00006123
Iteration 125/1000 | Loss: 0.00006123
Iteration 126/1000 | Loss: 0.00006123
Iteration 127/1000 | Loss: 0.00006123
Iteration 128/1000 | Loss: 0.00006122
Iteration 129/1000 | Loss: 0.00006122
Iteration 130/1000 | Loss: 0.00006122
Iteration 131/1000 | Loss: 0.00006122
Iteration 132/1000 | Loss: 0.00006122
Iteration 133/1000 | Loss: 0.00006122
Iteration 134/1000 | Loss: 0.00006122
Iteration 135/1000 | Loss: 0.00006121
Iteration 136/1000 | Loss: 0.00006121
Iteration 137/1000 | Loss: 0.00006121
Iteration 138/1000 | Loss: 0.00006121
Iteration 139/1000 | Loss: 0.00006121
Iteration 140/1000 | Loss: 0.00006121
Iteration 141/1000 | Loss: 0.00006121
Iteration 142/1000 | Loss: 0.00006121
Iteration 143/1000 | Loss: 0.00006121
Iteration 144/1000 | Loss: 0.00006121
Iteration 145/1000 | Loss: 0.00006121
Iteration 146/1000 | Loss: 0.00006121
Iteration 147/1000 | Loss: 0.00006121
Iteration 148/1000 | Loss: 0.00006121
Iteration 149/1000 | Loss: 0.00006121
Iteration 150/1000 | Loss: 0.00006121
Iteration 151/1000 | Loss: 0.00006121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [6.120698526501656e-05, 6.120698526501656e-05, 6.120698526501656e-05, 6.120698526501656e-05, 6.120698526501656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.120698526501656e-05

Optimization complete. Final v2v error: 4.470325946807861 mm

Highest mean error: 10.88524341583252 mm for frame 37

Lowest mean error: 3.3198816776275635 mm for frame 89

Saving results

Total time: 137.56980919837952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alison_posed_001/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alison_posed_001/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596798
Iteration 2/25 | Loss: 0.00130931
Iteration 3/25 | Loss: 0.00124189
Iteration 4/25 | Loss: 0.00123511
Iteration 5/25 | Loss: 0.00123303
Iteration 6/25 | Loss: 0.00123303
Iteration 7/25 | Loss: 0.00123303
Iteration 8/25 | Loss: 0.00123303
Iteration 9/25 | Loss: 0.00123303
Iteration 10/25 | Loss: 0.00123303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012330296449363232, 0.0012330296449363232, 0.0012330296449363232, 0.0012330296449363232, 0.0012330296449363232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012330296449363232

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22408652
Iteration 2/25 | Loss: 0.00076360
Iteration 3/25 | Loss: 0.00076359
Iteration 4/25 | Loss: 0.00076359
Iteration 5/25 | Loss: 0.00076359
Iteration 6/25 | Loss: 0.00076359
Iteration 7/25 | Loss: 0.00076359
Iteration 8/25 | Loss: 0.00076359
Iteration 9/25 | Loss: 0.00076359
Iteration 10/25 | Loss: 0.00076359
Iteration 11/25 | Loss: 0.00076359
Iteration 12/25 | Loss: 0.00076359
Iteration 13/25 | Loss: 0.00076359
Iteration 14/25 | Loss: 0.00076359
Iteration 15/25 | Loss: 0.00076359
Iteration 16/25 | Loss: 0.00076359
Iteration 17/25 | Loss: 0.00076359
Iteration 18/25 | Loss: 0.00076359
Iteration 19/25 | Loss: 0.00076359
Iteration 20/25 | Loss: 0.00076359
Iteration 21/25 | Loss: 0.00076359
Iteration 22/25 | Loss: 0.00076359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007635916699655354, 0.0007635916699655354, 0.0007635916699655354, 0.0007635916699655354, 0.0007635916699655354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007635916699655354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076359
Iteration 2/1000 | Loss: 0.00002921
Iteration 3/1000 | Loss: 0.00002214
Iteration 4/1000 | Loss: 0.00001984
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001721
Iteration 8/1000 | Loss: 0.00001692
Iteration 9/1000 | Loss: 0.00001654
Iteration 10/1000 | Loss: 0.00001631
Iteration 11/1000 | Loss: 0.00001606
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001564
Iteration 17/1000 | Loss: 0.00001559
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001550
Iteration 20/1000 | Loss: 0.00001546
Iteration 21/1000 | Loss: 0.00001546
Iteration 22/1000 | Loss: 0.00001544
Iteration 23/1000 | Loss: 0.00001543
Iteration 24/1000 | Loss: 0.00001543
Iteration 25/1000 | Loss: 0.00001542
Iteration 26/1000 | Loss: 0.00001542
Iteration 27/1000 | Loss: 0.00001541
Iteration 28/1000 | Loss: 0.00001541
Iteration 29/1000 | Loss: 0.00001540
Iteration 30/1000 | Loss: 0.00001540
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001539
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001539
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001539
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001539
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001538
Iteration 43/1000 | Loss: 0.00001538
Iteration 44/1000 | Loss: 0.00001537
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001536
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001532
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001531
Iteration 59/1000 | Loss: 0.00001531
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001529
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001526
Iteration 68/1000 | Loss: 0.00001526
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001525
Iteration 71/1000 | Loss: 0.00001525
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001520
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001519
Iteration 105/1000 | Loss: 0.00001519
Iteration 106/1000 | Loss: 0.00001519
Iteration 107/1000 | Loss: 0.00001519
Iteration 108/1000 | Loss: 0.00001519
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Iteration 127/1000 | Loss: 0.00001517
Iteration 128/1000 | Loss: 0.00001517
Iteration 129/1000 | Loss: 0.00001517
Iteration 130/1000 | Loss: 0.00001517
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001517
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001515
Iteration 146/1000 | Loss: 0.00001515
Iteration 147/1000 | Loss: 0.00001515
Iteration 148/1000 | Loss: 0.00001515
Iteration 149/1000 | Loss: 0.00001515
Iteration 150/1000 | Loss: 0.00001515
Iteration 151/1000 | Loss: 0.00001515
Iteration 152/1000 | Loss: 0.00001515
Iteration 153/1000 | Loss: 0.00001515
Iteration 154/1000 | Loss: 0.00001515
Iteration 155/1000 | Loss: 0.00001515
Iteration 156/1000 | Loss: 0.00001515
Iteration 157/1000 | Loss: 0.00001515
Iteration 158/1000 | Loss: 0.00001515
Iteration 159/1000 | Loss: 0.00001515
Iteration 160/1000 | Loss: 0.00001515
Iteration 161/1000 | Loss: 0.00001515
Iteration 162/1000 | Loss: 0.00001515
Iteration 163/1000 | Loss: 0.00001515
Iteration 164/1000 | Loss: 0.00001515
Iteration 165/1000 | Loss: 0.00001515
Iteration 166/1000 | Loss: 0.00001515
Iteration 167/1000 | Loss: 0.00001515
Iteration 168/1000 | Loss: 0.00001515
Iteration 169/1000 | Loss: 0.00001515
Iteration 170/1000 | Loss: 0.00001515
Iteration 171/1000 | Loss: 0.00001515
Iteration 172/1000 | Loss: 0.00001515
Iteration 173/1000 | Loss: 0.00001515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.5154359061853029e-05, 1.5154359061853029e-05, 1.5154359061853029e-05, 1.5154359061853029e-05, 1.5154359061853029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5154359061853029e-05

Optimization complete. Final v2v error: 3.3119096755981445 mm

Highest mean error: 3.5876922607421875 mm for frame 141

Lowest mean error: 3.1262645721435547 mm for frame 110

Saving results

Total time: 40.53202676773071
