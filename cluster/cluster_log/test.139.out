Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=139, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7784-7839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120353
Iteration 2/25 | Loss: 0.00124153
Iteration 3/25 | Loss: 0.00100264
Iteration 4/25 | Loss: 0.00097312
Iteration 5/25 | Loss: 0.00096566
Iteration 6/25 | Loss: 0.00096381
Iteration 7/25 | Loss: 0.00096364
Iteration 8/25 | Loss: 0.00096364
Iteration 9/25 | Loss: 0.00096364
Iteration 10/25 | Loss: 0.00096364
Iteration 11/25 | Loss: 0.00096364
Iteration 12/25 | Loss: 0.00096364
Iteration 13/25 | Loss: 0.00096364
Iteration 14/25 | Loss: 0.00096364
Iteration 15/25 | Loss: 0.00096364
Iteration 16/25 | Loss: 0.00096364
Iteration 17/25 | Loss: 0.00096364
Iteration 18/25 | Loss: 0.00096364
Iteration 19/25 | Loss: 0.00096364
Iteration 20/25 | Loss: 0.00096364
Iteration 21/25 | Loss: 0.00096364
Iteration 22/25 | Loss: 0.00096364
Iteration 23/25 | Loss: 0.00096364
Iteration 24/25 | Loss: 0.00096364
Iteration 25/25 | Loss: 0.00096364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20835519
Iteration 2/25 | Loss: 0.00067783
Iteration 3/25 | Loss: 0.00067783
Iteration 4/25 | Loss: 0.00067783
Iteration 5/25 | Loss: 0.00067782
Iteration 6/25 | Loss: 0.00067782
Iteration 7/25 | Loss: 0.00067782
Iteration 8/25 | Loss: 0.00067782
Iteration 9/25 | Loss: 0.00067782
Iteration 10/25 | Loss: 0.00067782
Iteration 11/25 | Loss: 0.00067782
Iteration 12/25 | Loss: 0.00067782
Iteration 13/25 | Loss: 0.00067782
Iteration 14/25 | Loss: 0.00067782
Iteration 15/25 | Loss: 0.00067782
Iteration 16/25 | Loss: 0.00067782
Iteration 17/25 | Loss: 0.00067782
Iteration 18/25 | Loss: 0.00067782
Iteration 19/25 | Loss: 0.00067782
Iteration 20/25 | Loss: 0.00067782
Iteration 21/25 | Loss: 0.00067782
Iteration 22/25 | Loss: 0.00067782
Iteration 23/25 | Loss: 0.00067782
Iteration 24/25 | Loss: 0.00067782
Iteration 25/25 | Loss: 0.00067782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067782
Iteration 2/1000 | Loss: 0.00002460
Iteration 3/1000 | Loss: 0.00001629
Iteration 4/1000 | Loss: 0.00001439
Iteration 5/1000 | Loss: 0.00001341
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001205
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001193
Iteration 14/1000 | Loss: 0.00001191
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001185
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001172
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001171
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001168
Iteration 56/1000 | Loss: 0.00001168
Iteration 57/1000 | Loss: 0.00001167
Iteration 58/1000 | Loss: 0.00001167
Iteration 59/1000 | Loss: 0.00001167
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001166
Iteration 62/1000 | Loss: 0.00001166
Iteration 63/1000 | Loss: 0.00001166
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001165
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001164
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001164
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001163
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001163
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001162
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001162
Iteration 104/1000 | Loss: 0.00001162
Iteration 105/1000 | Loss: 0.00001162
Iteration 106/1000 | Loss: 0.00001162
Iteration 107/1000 | Loss: 0.00001162
Iteration 108/1000 | Loss: 0.00001162
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001162
Iteration 111/1000 | Loss: 0.00001162
Iteration 112/1000 | Loss: 0.00001162
Iteration 113/1000 | Loss: 0.00001162
Iteration 114/1000 | Loss: 0.00001162
Iteration 115/1000 | Loss: 0.00001162
Iteration 116/1000 | Loss: 0.00001162
Iteration 117/1000 | Loss: 0.00001162
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001162
Iteration 120/1000 | Loss: 0.00001162
Iteration 121/1000 | Loss: 0.00001162
Iteration 122/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.1621961675700732e-05, 1.1621961675700732e-05, 1.1621961675700732e-05, 1.1621961675700732e-05, 1.1621961675700732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1621961675700732e-05

Optimization complete. Final v2v error: 2.8539769649505615 mm

Highest mean error: 3.1159260272979736 mm for frame 263

Lowest mean error: 2.614708185195923 mm for frame 183

Saving results

Total time: 38.295801877975464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799116
Iteration 2/25 | Loss: 0.00157989
Iteration 3/25 | Loss: 0.00118684
Iteration 4/25 | Loss: 0.00113878
Iteration 5/25 | Loss: 0.00112192
Iteration 6/25 | Loss: 0.00111747
Iteration 7/25 | Loss: 0.00111646
Iteration 8/25 | Loss: 0.00111646
Iteration 9/25 | Loss: 0.00111646
Iteration 10/25 | Loss: 0.00111646
Iteration 11/25 | Loss: 0.00111646
Iteration 12/25 | Loss: 0.00111646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011164615862071514, 0.0011164615862071514, 0.0011164615862071514, 0.0011164615862071514, 0.0011164615862071514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011164615862071514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.84849358
Iteration 2/25 | Loss: 0.00072791
Iteration 3/25 | Loss: 0.00072767
Iteration 4/25 | Loss: 0.00072767
Iteration 5/25 | Loss: 0.00072767
Iteration 6/25 | Loss: 0.00072767
Iteration 7/25 | Loss: 0.00072767
Iteration 8/25 | Loss: 0.00072767
Iteration 9/25 | Loss: 0.00072767
Iteration 10/25 | Loss: 0.00072767
Iteration 11/25 | Loss: 0.00072767
Iteration 12/25 | Loss: 0.00072767
Iteration 13/25 | Loss: 0.00072767
Iteration 14/25 | Loss: 0.00072767
Iteration 15/25 | Loss: 0.00072767
Iteration 16/25 | Loss: 0.00072767
Iteration 17/25 | Loss: 0.00072767
Iteration 18/25 | Loss: 0.00072767
Iteration 19/25 | Loss: 0.00072767
Iteration 20/25 | Loss: 0.00072767
Iteration 21/25 | Loss: 0.00072767
Iteration 22/25 | Loss: 0.00072767
Iteration 23/25 | Loss: 0.00072767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007276684045791626, 0.0007276684045791626, 0.0007276684045791626, 0.0007276684045791626, 0.0007276684045791626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007276684045791626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072767
Iteration 2/1000 | Loss: 0.00008917
Iteration 3/1000 | Loss: 0.00030634
Iteration 4/1000 | Loss: 0.00016548
Iteration 5/1000 | Loss: 0.00006647
Iteration 6/1000 | Loss: 0.00005731
Iteration 7/1000 | Loss: 0.00004900
Iteration 8/1000 | Loss: 0.00004670
Iteration 9/1000 | Loss: 0.00004549
Iteration 10/1000 | Loss: 0.00004458
Iteration 11/1000 | Loss: 0.00004384
Iteration 12/1000 | Loss: 0.00004312
Iteration 13/1000 | Loss: 0.00004259
Iteration 14/1000 | Loss: 0.00004220
Iteration 15/1000 | Loss: 0.00004184
Iteration 16/1000 | Loss: 0.00004153
Iteration 17/1000 | Loss: 0.00004126
Iteration 18/1000 | Loss: 0.00004100
Iteration 19/1000 | Loss: 0.00004081
Iteration 20/1000 | Loss: 0.00004064
Iteration 21/1000 | Loss: 0.00004055
Iteration 22/1000 | Loss: 0.00004036
Iteration 23/1000 | Loss: 0.00004026
Iteration 24/1000 | Loss: 0.00004017
Iteration 25/1000 | Loss: 0.00004007
Iteration 26/1000 | Loss: 0.00004006
Iteration 27/1000 | Loss: 0.00004006
Iteration 28/1000 | Loss: 0.00003999
Iteration 29/1000 | Loss: 0.00003995
Iteration 30/1000 | Loss: 0.00003995
Iteration 31/1000 | Loss: 0.00003995
Iteration 32/1000 | Loss: 0.00003995
Iteration 33/1000 | Loss: 0.00003995
Iteration 34/1000 | Loss: 0.00003995
Iteration 35/1000 | Loss: 0.00003995
Iteration 36/1000 | Loss: 0.00003995
Iteration 37/1000 | Loss: 0.00003995
Iteration 38/1000 | Loss: 0.00003995
Iteration 39/1000 | Loss: 0.00003995
Iteration 40/1000 | Loss: 0.00003995
Iteration 41/1000 | Loss: 0.00003993
Iteration 42/1000 | Loss: 0.00003993
Iteration 43/1000 | Loss: 0.00003993
Iteration 44/1000 | Loss: 0.00003992
Iteration 45/1000 | Loss: 0.00003992
Iteration 46/1000 | Loss: 0.00003990
Iteration 47/1000 | Loss: 0.00003989
Iteration 48/1000 | Loss: 0.00003989
Iteration 49/1000 | Loss: 0.00003989
Iteration 50/1000 | Loss: 0.00003989
Iteration 51/1000 | Loss: 0.00003989
Iteration 52/1000 | Loss: 0.00003986
Iteration 53/1000 | Loss: 0.00003986
Iteration 54/1000 | Loss: 0.00003985
Iteration 55/1000 | Loss: 0.00003983
Iteration 56/1000 | Loss: 0.00003983
Iteration 57/1000 | Loss: 0.00003983
Iteration 58/1000 | Loss: 0.00003982
Iteration 59/1000 | Loss: 0.00003982
Iteration 60/1000 | Loss: 0.00003982
Iteration 61/1000 | Loss: 0.00003982
Iteration 62/1000 | Loss: 0.00003982
Iteration 63/1000 | Loss: 0.00003982
Iteration 64/1000 | Loss: 0.00003981
Iteration 65/1000 | Loss: 0.00003981
Iteration 66/1000 | Loss: 0.00003981
Iteration 67/1000 | Loss: 0.00003981
Iteration 68/1000 | Loss: 0.00003981
Iteration 69/1000 | Loss: 0.00003980
Iteration 70/1000 | Loss: 0.00003980
Iteration 71/1000 | Loss: 0.00003980
Iteration 72/1000 | Loss: 0.00003980
Iteration 73/1000 | Loss: 0.00003980
Iteration 74/1000 | Loss: 0.00003979
Iteration 75/1000 | Loss: 0.00003979
Iteration 76/1000 | Loss: 0.00003979
Iteration 77/1000 | Loss: 0.00003979
Iteration 78/1000 | Loss: 0.00003979
Iteration 79/1000 | Loss: 0.00003979
Iteration 80/1000 | Loss: 0.00003979
Iteration 81/1000 | Loss: 0.00003978
Iteration 82/1000 | Loss: 0.00003978
Iteration 83/1000 | Loss: 0.00003977
Iteration 84/1000 | Loss: 0.00003977
Iteration 85/1000 | Loss: 0.00003977
Iteration 86/1000 | Loss: 0.00003977
Iteration 87/1000 | Loss: 0.00003977
Iteration 88/1000 | Loss: 0.00003977
Iteration 89/1000 | Loss: 0.00003977
Iteration 90/1000 | Loss: 0.00003976
Iteration 91/1000 | Loss: 0.00003976
Iteration 92/1000 | Loss: 0.00003976
Iteration 93/1000 | Loss: 0.00003976
Iteration 94/1000 | Loss: 0.00003976
Iteration 95/1000 | Loss: 0.00003976
Iteration 96/1000 | Loss: 0.00003975
Iteration 97/1000 | Loss: 0.00003975
Iteration 98/1000 | Loss: 0.00003975
Iteration 99/1000 | Loss: 0.00003975
Iteration 100/1000 | Loss: 0.00003975
Iteration 101/1000 | Loss: 0.00003975
Iteration 102/1000 | Loss: 0.00003975
Iteration 103/1000 | Loss: 0.00003975
Iteration 104/1000 | Loss: 0.00003975
Iteration 105/1000 | Loss: 0.00003975
Iteration 106/1000 | Loss: 0.00003974
Iteration 107/1000 | Loss: 0.00003974
Iteration 108/1000 | Loss: 0.00003974
Iteration 109/1000 | Loss: 0.00003974
Iteration 110/1000 | Loss: 0.00003973
Iteration 111/1000 | Loss: 0.00003973
Iteration 112/1000 | Loss: 0.00003973
Iteration 113/1000 | Loss: 0.00003973
Iteration 114/1000 | Loss: 0.00003973
Iteration 115/1000 | Loss: 0.00003973
Iteration 116/1000 | Loss: 0.00003973
Iteration 117/1000 | Loss: 0.00003973
Iteration 118/1000 | Loss: 0.00003973
Iteration 119/1000 | Loss: 0.00003973
Iteration 120/1000 | Loss: 0.00003973
Iteration 121/1000 | Loss: 0.00003973
Iteration 122/1000 | Loss: 0.00003973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [3.973319326178171e-05, 3.973319326178171e-05, 3.973319326178171e-05, 3.973319326178171e-05, 3.973319326178171e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.973319326178171e-05

Optimization complete. Final v2v error: 5.0282697677612305 mm

Highest mean error: 7.1883063316345215 mm for frame 145

Lowest mean error: 3.667896032333374 mm for frame 17

Saving results

Total time: 61.731805086135864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836168
Iteration 2/25 | Loss: 0.00124262
Iteration 3/25 | Loss: 0.00102800
Iteration 4/25 | Loss: 0.00100186
Iteration 5/25 | Loss: 0.00099303
Iteration 6/25 | Loss: 0.00099009
Iteration 7/25 | Loss: 0.00099009
Iteration 8/25 | Loss: 0.00099009
Iteration 9/25 | Loss: 0.00099009
Iteration 10/25 | Loss: 0.00099009
Iteration 11/25 | Loss: 0.00099009
Iteration 12/25 | Loss: 0.00099009
Iteration 13/25 | Loss: 0.00099009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009900919394567609, 0.0009900919394567609, 0.0009900919394567609, 0.0009900919394567609, 0.0009900919394567609]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009900919394567609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69230103
Iteration 2/25 | Loss: 0.00057303
Iteration 3/25 | Loss: 0.00057303
Iteration 4/25 | Loss: 0.00057303
Iteration 5/25 | Loss: 0.00057303
Iteration 6/25 | Loss: 0.00057303
Iteration 7/25 | Loss: 0.00057303
Iteration 8/25 | Loss: 0.00057303
Iteration 9/25 | Loss: 0.00057303
Iteration 10/25 | Loss: 0.00057303
Iteration 11/25 | Loss: 0.00057303
Iteration 12/25 | Loss: 0.00057303
Iteration 13/25 | Loss: 0.00057303
Iteration 14/25 | Loss: 0.00057303
Iteration 15/25 | Loss: 0.00057303
Iteration 16/25 | Loss: 0.00057303
Iteration 17/25 | Loss: 0.00057303
Iteration 18/25 | Loss: 0.00057303
Iteration 19/25 | Loss: 0.00057303
Iteration 20/25 | Loss: 0.00057303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005730285192839801, 0.0005730285192839801, 0.0005730285192839801, 0.0005730285192839801, 0.0005730285192839801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005730285192839801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057303
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001729
Iteration 4/1000 | Loss: 0.00001429
Iteration 5/1000 | Loss: 0.00001328
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001222
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001215
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001202
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001201
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001193
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001177
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001173
Iteration 99/1000 | Loss: 0.00001173
Iteration 100/1000 | Loss: 0.00001173
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001170
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001169
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Iteration 141/1000 | Loss: 0.00001169
Iteration 142/1000 | Loss: 0.00001169
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1692806765495334e-05, 1.1692806765495334e-05, 1.1692806765495334e-05, 1.1692806765495334e-05, 1.1692806765495334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1692806765495334e-05

Optimization complete. Final v2v error: 2.9234726428985596 mm

Highest mean error: 3.5747478008270264 mm for frame 200

Lowest mean error: 2.5344443321228027 mm for frame 75

Saving results

Total time: 37.58868670463562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808979
Iteration 2/25 | Loss: 0.00151724
Iteration 3/25 | Loss: 0.00112128
Iteration 4/25 | Loss: 0.00107890
Iteration 5/25 | Loss: 0.00106717
Iteration 6/25 | Loss: 0.00107176
Iteration 7/25 | Loss: 0.00106266
Iteration 8/25 | Loss: 0.00105761
Iteration 9/25 | Loss: 0.00105554
Iteration 10/25 | Loss: 0.00105403
Iteration 11/25 | Loss: 0.00105370
Iteration 12/25 | Loss: 0.00105370
Iteration 13/25 | Loss: 0.00105370
Iteration 14/25 | Loss: 0.00105370
Iteration 15/25 | Loss: 0.00105369
Iteration 16/25 | Loss: 0.00105369
Iteration 17/25 | Loss: 0.00105369
Iteration 18/25 | Loss: 0.00105369
Iteration 19/25 | Loss: 0.00105369
Iteration 20/25 | Loss: 0.00105369
Iteration 21/25 | Loss: 0.00105369
Iteration 22/25 | Loss: 0.00105369
Iteration 23/25 | Loss: 0.00105369
Iteration 24/25 | Loss: 0.00105369
Iteration 25/25 | Loss: 0.00105369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33680212
Iteration 2/25 | Loss: 0.00035717
Iteration 3/25 | Loss: 0.00035715
Iteration 4/25 | Loss: 0.00035715
Iteration 5/25 | Loss: 0.00035715
Iteration 6/25 | Loss: 0.00035715
Iteration 7/25 | Loss: 0.00035715
Iteration 8/25 | Loss: 0.00035715
Iteration 9/25 | Loss: 0.00035715
Iteration 10/25 | Loss: 0.00035715
Iteration 11/25 | Loss: 0.00035715
Iteration 12/25 | Loss: 0.00035715
Iteration 13/25 | Loss: 0.00035715
Iteration 14/25 | Loss: 0.00035715
Iteration 15/25 | Loss: 0.00035715
Iteration 16/25 | Loss: 0.00035715
Iteration 17/25 | Loss: 0.00035715
Iteration 18/25 | Loss: 0.00035715
Iteration 19/25 | Loss: 0.00035715
Iteration 20/25 | Loss: 0.00035715
Iteration 21/25 | Loss: 0.00035715
Iteration 22/25 | Loss: 0.00035715
Iteration 23/25 | Loss: 0.00035715
Iteration 24/25 | Loss: 0.00035715
Iteration 25/25 | Loss: 0.00035715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035715
Iteration 2/1000 | Loss: 0.00003657
Iteration 3/1000 | Loss: 0.00002642
Iteration 4/1000 | Loss: 0.00002414
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002176
Iteration 7/1000 | Loss: 0.00002107
Iteration 8/1000 | Loss: 0.00002065
Iteration 9/1000 | Loss: 0.00002036
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00002009
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001988
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001984
Iteration 16/1000 | Loss: 0.00001982
Iteration 17/1000 | Loss: 0.00001981
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001977
Iteration 21/1000 | Loss: 0.00001977
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001973
Iteration 24/1000 | Loss: 0.00001972
Iteration 25/1000 | Loss: 0.00001969
Iteration 26/1000 | Loss: 0.00001969
Iteration 27/1000 | Loss: 0.00001969
Iteration 28/1000 | Loss: 0.00001969
Iteration 29/1000 | Loss: 0.00001969
Iteration 30/1000 | Loss: 0.00001969
Iteration 31/1000 | Loss: 0.00001969
Iteration 32/1000 | Loss: 0.00001969
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001965
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001964
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001962
Iteration 50/1000 | Loss: 0.00001962
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001961
Iteration 53/1000 | Loss: 0.00001961
Iteration 54/1000 | Loss: 0.00001961
Iteration 55/1000 | Loss: 0.00001960
Iteration 56/1000 | Loss: 0.00001960
Iteration 57/1000 | Loss: 0.00001960
Iteration 58/1000 | Loss: 0.00001960
Iteration 59/1000 | Loss: 0.00001960
Iteration 60/1000 | Loss: 0.00001960
Iteration 61/1000 | Loss: 0.00001960
Iteration 62/1000 | Loss: 0.00001960
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001959
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001959
Iteration 68/1000 | Loss: 0.00001959
Iteration 69/1000 | Loss: 0.00001958
Iteration 70/1000 | Loss: 0.00001958
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001957
Iteration 75/1000 | Loss: 0.00001956
Iteration 76/1000 | Loss: 0.00001956
Iteration 77/1000 | Loss: 0.00001956
Iteration 78/1000 | Loss: 0.00001955
Iteration 79/1000 | Loss: 0.00001955
Iteration 80/1000 | Loss: 0.00001954
Iteration 81/1000 | Loss: 0.00001953
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001953
Iteration 84/1000 | Loss: 0.00001953
Iteration 85/1000 | Loss: 0.00001953
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001952
Iteration 91/1000 | Loss: 0.00001952
Iteration 92/1000 | Loss: 0.00001952
Iteration 93/1000 | Loss: 0.00001952
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001950
Iteration 96/1000 | Loss: 0.00001950
Iteration 97/1000 | Loss: 0.00001950
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001949
Iteration 101/1000 | Loss: 0.00001949
Iteration 102/1000 | Loss: 0.00001949
Iteration 103/1000 | Loss: 0.00001948
Iteration 104/1000 | Loss: 0.00001948
Iteration 105/1000 | Loss: 0.00001948
Iteration 106/1000 | Loss: 0.00001948
Iteration 107/1000 | Loss: 0.00001948
Iteration 108/1000 | Loss: 0.00001948
Iteration 109/1000 | Loss: 0.00001948
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001947
Iteration 113/1000 | Loss: 0.00001947
Iteration 114/1000 | Loss: 0.00001947
Iteration 115/1000 | Loss: 0.00001947
Iteration 116/1000 | Loss: 0.00001947
Iteration 117/1000 | Loss: 0.00001946
Iteration 118/1000 | Loss: 0.00001946
Iteration 119/1000 | Loss: 0.00001945
Iteration 120/1000 | Loss: 0.00001945
Iteration 121/1000 | Loss: 0.00001944
Iteration 122/1000 | Loss: 0.00001944
Iteration 123/1000 | Loss: 0.00001944
Iteration 124/1000 | Loss: 0.00001943
Iteration 125/1000 | Loss: 0.00001943
Iteration 126/1000 | Loss: 0.00001943
Iteration 127/1000 | Loss: 0.00001943
Iteration 128/1000 | Loss: 0.00001943
Iteration 129/1000 | Loss: 0.00001943
Iteration 130/1000 | Loss: 0.00001943
Iteration 131/1000 | Loss: 0.00001943
Iteration 132/1000 | Loss: 0.00001943
Iteration 133/1000 | Loss: 0.00001943
Iteration 134/1000 | Loss: 0.00001942
Iteration 135/1000 | Loss: 0.00001941
Iteration 136/1000 | Loss: 0.00001941
Iteration 137/1000 | Loss: 0.00001941
Iteration 138/1000 | Loss: 0.00001941
Iteration 139/1000 | Loss: 0.00001941
Iteration 140/1000 | Loss: 0.00001941
Iteration 141/1000 | Loss: 0.00001940
Iteration 142/1000 | Loss: 0.00001940
Iteration 143/1000 | Loss: 0.00001940
Iteration 144/1000 | Loss: 0.00001940
Iteration 145/1000 | Loss: 0.00001939
Iteration 146/1000 | Loss: 0.00001939
Iteration 147/1000 | Loss: 0.00001939
Iteration 148/1000 | Loss: 0.00001939
Iteration 149/1000 | Loss: 0.00001939
Iteration 150/1000 | Loss: 0.00001938
Iteration 151/1000 | Loss: 0.00001938
Iteration 152/1000 | Loss: 0.00001938
Iteration 153/1000 | Loss: 0.00001938
Iteration 154/1000 | Loss: 0.00001937
Iteration 155/1000 | Loss: 0.00001937
Iteration 156/1000 | Loss: 0.00001937
Iteration 157/1000 | Loss: 0.00001937
Iteration 158/1000 | Loss: 0.00001937
Iteration 159/1000 | Loss: 0.00001937
Iteration 160/1000 | Loss: 0.00001937
Iteration 161/1000 | Loss: 0.00001937
Iteration 162/1000 | Loss: 0.00001936
Iteration 163/1000 | Loss: 0.00001936
Iteration 164/1000 | Loss: 0.00001936
Iteration 165/1000 | Loss: 0.00001936
Iteration 166/1000 | Loss: 0.00001936
Iteration 167/1000 | Loss: 0.00001936
Iteration 168/1000 | Loss: 0.00001936
Iteration 169/1000 | Loss: 0.00001936
Iteration 170/1000 | Loss: 0.00001936
Iteration 171/1000 | Loss: 0.00001935
Iteration 172/1000 | Loss: 0.00001935
Iteration 173/1000 | Loss: 0.00001935
Iteration 174/1000 | Loss: 0.00001935
Iteration 175/1000 | Loss: 0.00001935
Iteration 176/1000 | Loss: 0.00001935
Iteration 177/1000 | Loss: 0.00001934
Iteration 178/1000 | Loss: 0.00001934
Iteration 179/1000 | Loss: 0.00001934
Iteration 180/1000 | Loss: 0.00001934
Iteration 181/1000 | Loss: 0.00001934
Iteration 182/1000 | Loss: 0.00001934
Iteration 183/1000 | Loss: 0.00001934
Iteration 184/1000 | Loss: 0.00001934
Iteration 185/1000 | Loss: 0.00001934
Iteration 186/1000 | Loss: 0.00001934
Iteration 187/1000 | Loss: 0.00001934
Iteration 188/1000 | Loss: 0.00001934
Iteration 189/1000 | Loss: 0.00001934
Iteration 190/1000 | Loss: 0.00001934
Iteration 191/1000 | Loss: 0.00001934
Iteration 192/1000 | Loss: 0.00001933
Iteration 193/1000 | Loss: 0.00001933
Iteration 194/1000 | Loss: 0.00001933
Iteration 195/1000 | Loss: 0.00001933
Iteration 196/1000 | Loss: 0.00001933
Iteration 197/1000 | Loss: 0.00001933
Iteration 198/1000 | Loss: 0.00001933
Iteration 199/1000 | Loss: 0.00001933
Iteration 200/1000 | Loss: 0.00001933
Iteration 201/1000 | Loss: 0.00001933
Iteration 202/1000 | Loss: 0.00001933
Iteration 203/1000 | Loss: 0.00001933
Iteration 204/1000 | Loss: 0.00001933
Iteration 205/1000 | Loss: 0.00001933
Iteration 206/1000 | Loss: 0.00001933
Iteration 207/1000 | Loss: 0.00001933
Iteration 208/1000 | Loss: 0.00001933
Iteration 209/1000 | Loss: 0.00001933
Iteration 210/1000 | Loss: 0.00001933
Iteration 211/1000 | Loss: 0.00001933
Iteration 212/1000 | Loss: 0.00001933
Iteration 213/1000 | Loss: 0.00001933
Iteration 214/1000 | Loss: 0.00001933
Iteration 215/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.9332137526362203e-05, 1.9332137526362203e-05, 1.9332137526362203e-05, 1.9332137526362203e-05, 1.9332137526362203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9332137526362203e-05

Optimization complete. Final v2v error: 3.45630145072937 mm

Highest mean error: 5.449172019958496 mm for frame 64

Lowest mean error: 2.9283504486083984 mm for frame 81

Saving results

Total time: 57.73213267326355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423621
Iteration 2/25 | Loss: 0.00105567
Iteration 3/25 | Loss: 0.00097207
Iteration 4/25 | Loss: 0.00095300
Iteration 5/25 | Loss: 0.00094676
Iteration 6/25 | Loss: 0.00094562
Iteration 7/25 | Loss: 0.00094562
Iteration 8/25 | Loss: 0.00094562
Iteration 9/25 | Loss: 0.00094562
Iteration 10/25 | Loss: 0.00094562
Iteration 11/25 | Loss: 0.00094562
Iteration 12/25 | Loss: 0.00094562
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009456228581257164, 0.0009456228581257164, 0.0009456228581257164, 0.0009456228581257164, 0.0009456228581257164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009456228581257164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33988321
Iteration 2/25 | Loss: 0.00066726
Iteration 3/25 | Loss: 0.00066726
Iteration 4/25 | Loss: 0.00066726
Iteration 5/25 | Loss: 0.00066726
Iteration 6/25 | Loss: 0.00066726
Iteration 7/25 | Loss: 0.00066726
Iteration 8/25 | Loss: 0.00066726
Iteration 9/25 | Loss: 0.00066726
Iteration 10/25 | Loss: 0.00066726
Iteration 11/25 | Loss: 0.00066726
Iteration 12/25 | Loss: 0.00066726
Iteration 13/25 | Loss: 0.00066726
Iteration 14/25 | Loss: 0.00066726
Iteration 15/25 | Loss: 0.00066726
Iteration 16/25 | Loss: 0.00066726
Iteration 17/25 | Loss: 0.00066726
Iteration 18/25 | Loss: 0.00066726
Iteration 19/25 | Loss: 0.00066726
Iteration 20/25 | Loss: 0.00066726
Iteration 21/25 | Loss: 0.00066726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006672601448372006, 0.0006672601448372006, 0.0006672601448372006, 0.0006672601448372006, 0.0006672601448372006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006672601448372006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066726
Iteration 2/1000 | Loss: 0.00001726
Iteration 3/1000 | Loss: 0.00001383
Iteration 4/1000 | Loss: 0.00001243
Iteration 5/1000 | Loss: 0.00001184
Iteration 6/1000 | Loss: 0.00001152
Iteration 7/1000 | Loss: 0.00001123
Iteration 8/1000 | Loss: 0.00001120
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001119
Iteration 12/1000 | Loss: 0.00001119
Iteration 13/1000 | Loss: 0.00001118
Iteration 14/1000 | Loss: 0.00001118
Iteration 15/1000 | Loss: 0.00001110
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001101
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001099
Iteration 27/1000 | Loss: 0.00001099
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001098
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001098
Iteration 40/1000 | Loss: 0.00001098
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001097
Iteration 44/1000 | Loss: 0.00001097
Iteration 45/1000 | Loss: 0.00001097
Iteration 46/1000 | Loss: 0.00001096
Iteration 47/1000 | Loss: 0.00001095
Iteration 48/1000 | Loss: 0.00001095
Iteration 49/1000 | Loss: 0.00001095
Iteration 50/1000 | Loss: 0.00001095
Iteration 51/1000 | Loss: 0.00001095
Iteration 52/1000 | Loss: 0.00001095
Iteration 53/1000 | Loss: 0.00001095
Iteration 54/1000 | Loss: 0.00001095
Iteration 55/1000 | Loss: 0.00001095
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001095
Iteration 59/1000 | Loss: 0.00001095
Iteration 60/1000 | Loss: 0.00001095
Iteration 61/1000 | Loss: 0.00001095
Iteration 62/1000 | Loss: 0.00001094
Iteration 63/1000 | Loss: 0.00001094
Iteration 64/1000 | Loss: 0.00001094
Iteration 65/1000 | Loss: 0.00001094
Iteration 66/1000 | Loss: 0.00001094
Iteration 67/1000 | Loss: 0.00001094
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001093
Iteration 70/1000 | Loss: 0.00001093
Iteration 71/1000 | Loss: 0.00001093
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001093
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001092
Iteration 90/1000 | Loss: 0.00001092
Iteration 91/1000 | Loss: 0.00001092
Iteration 92/1000 | Loss: 0.00001092
Iteration 93/1000 | Loss: 0.00001092
Iteration 94/1000 | Loss: 0.00001092
Iteration 95/1000 | Loss: 0.00001092
Iteration 96/1000 | Loss: 0.00001092
Iteration 97/1000 | Loss: 0.00001092
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001091
Iteration 100/1000 | Loss: 0.00001091
Iteration 101/1000 | Loss: 0.00001091
Iteration 102/1000 | Loss: 0.00001091
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001090
Iteration 106/1000 | Loss: 0.00001090
Iteration 107/1000 | Loss: 0.00001090
Iteration 108/1000 | Loss: 0.00001090
Iteration 109/1000 | Loss: 0.00001089
Iteration 110/1000 | Loss: 0.00001089
Iteration 111/1000 | Loss: 0.00001089
Iteration 112/1000 | Loss: 0.00001089
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001089
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001088
Iteration 120/1000 | Loss: 0.00001088
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001087
Iteration 128/1000 | Loss: 0.00001087
Iteration 129/1000 | Loss: 0.00001087
Iteration 130/1000 | Loss: 0.00001087
Iteration 131/1000 | Loss: 0.00001087
Iteration 132/1000 | Loss: 0.00001087
Iteration 133/1000 | Loss: 0.00001087
Iteration 134/1000 | Loss: 0.00001087
Iteration 135/1000 | Loss: 0.00001087
Iteration 136/1000 | Loss: 0.00001087
Iteration 137/1000 | Loss: 0.00001087
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001087
Iteration 140/1000 | Loss: 0.00001087
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001087
Iteration 148/1000 | Loss: 0.00001087
Iteration 149/1000 | Loss: 0.00001087
Iteration 150/1000 | Loss: 0.00001087
Iteration 151/1000 | Loss: 0.00001087
Iteration 152/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.086706015485106e-05, 1.086706015485106e-05, 1.086706015485106e-05, 1.086706015485106e-05, 1.086706015485106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.086706015485106e-05

Optimization complete. Final v2v error: 2.7913568019866943 mm

Highest mean error: 2.9261159896850586 mm for frame 156

Lowest mean error: 2.6521432399749756 mm for frame 47

Saving results

Total time: 31.463559865951538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773188
Iteration 2/25 | Loss: 0.00117409
Iteration 3/25 | Loss: 0.00098906
Iteration 4/25 | Loss: 0.00097179
Iteration 5/25 | Loss: 0.00096893
Iteration 6/25 | Loss: 0.00096893
Iteration 7/25 | Loss: 0.00096893
Iteration 8/25 | Loss: 0.00096893
Iteration 9/25 | Loss: 0.00096893
Iteration 10/25 | Loss: 0.00096893
Iteration 11/25 | Loss: 0.00096893
Iteration 12/25 | Loss: 0.00096893
Iteration 13/25 | Loss: 0.00096893
Iteration 14/25 | Loss: 0.00096893
Iteration 15/25 | Loss: 0.00096893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009689301368780434, 0.0009689301368780434, 0.0009689301368780434, 0.0009689301368780434, 0.0009689301368780434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009689301368780434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32280791
Iteration 2/25 | Loss: 0.00049699
Iteration 3/25 | Loss: 0.00049697
Iteration 4/25 | Loss: 0.00049697
Iteration 5/25 | Loss: 0.00049696
Iteration 6/25 | Loss: 0.00049696
Iteration 7/25 | Loss: 0.00049696
Iteration 8/25 | Loss: 0.00049696
Iteration 9/25 | Loss: 0.00049696
Iteration 10/25 | Loss: 0.00049696
Iteration 11/25 | Loss: 0.00049696
Iteration 12/25 | Loss: 0.00049696
Iteration 13/25 | Loss: 0.00049696
Iteration 14/25 | Loss: 0.00049696
Iteration 15/25 | Loss: 0.00049696
Iteration 16/25 | Loss: 0.00049696
Iteration 17/25 | Loss: 0.00049696
Iteration 18/25 | Loss: 0.00049696
Iteration 19/25 | Loss: 0.00049696
Iteration 20/25 | Loss: 0.00049696
Iteration 21/25 | Loss: 0.00049696
Iteration 22/25 | Loss: 0.00049696
Iteration 23/25 | Loss: 0.00049696
Iteration 24/25 | Loss: 0.00049696
Iteration 25/25 | Loss: 0.00049696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049696
Iteration 2/1000 | Loss: 0.00002136
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001676
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001539
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001446
Iteration 12/1000 | Loss: 0.00001445
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001440
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001438
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001437
Iteration 20/1000 | Loss: 0.00001434
Iteration 21/1000 | Loss: 0.00001434
Iteration 22/1000 | Loss: 0.00001434
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001434
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001434
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001433
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001426
Iteration 33/1000 | Loss: 0.00001425
Iteration 34/1000 | Loss: 0.00001422
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001421
Iteration 37/1000 | Loss: 0.00001421
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001420
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001417
Iteration 42/1000 | Loss: 0.00001417
Iteration 43/1000 | Loss: 0.00001417
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001416
Iteration 48/1000 | Loss: 0.00001416
Iteration 49/1000 | Loss: 0.00001415
Iteration 50/1000 | Loss: 0.00001415
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001413
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001413
Iteration 58/1000 | Loss: 0.00001413
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001413
Iteration 61/1000 | Loss: 0.00001413
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001411
Iteration 67/1000 | Loss: 0.00001411
Iteration 68/1000 | Loss: 0.00001411
Iteration 69/1000 | Loss: 0.00001411
Iteration 70/1000 | Loss: 0.00001411
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001410
Iteration 73/1000 | Loss: 0.00001410
Iteration 74/1000 | Loss: 0.00001410
Iteration 75/1000 | Loss: 0.00001410
Iteration 76/1000 | Loss: 0.00001410
Iteration 77/1000 | Loss: 0.00001410
Iteration 78/1000 | Loss: 0.00001410
Iteration 79/1000 | Loss: 0.00001410
Iteration 80/1000 | Loss: 0.00001410
Iteration 81/1000 | Loss: 0.00001410
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001410
Iteration 87/1000 | Loss: 0.00001410
Iteration 88/1000 | Loss: 0.00001410
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001410
Iteration 91/1000 | Loss: 0.00001410
Iteration 92/1000 | Loss: 0.00001410
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.4100669432082213e-05, 1.4100669432082213e-05, 1.4100669432082213e-05, 1.4100669432082213e-05, 1.4100669432082213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4100669432082213e-05

Optimization complete. Final v2v error: 3.1312057971954346 mm

Highest mean error: 3.8510453701019287 mm for frame 236

Lowest mean error: 2.690765619277954 mm for frame 45

Saving results

Total time: 32.425171852111816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389434
Iteration 2/25 | Loss: 0.00111501
Iteration 3/25 | Loss: 0.00095518
Iteration 4/25 | Loss: 0.00094103
Iteration 5/25 | Loss: 0.00093756
Iteration 6/25 | Loss: 0.00093628
Iteration 7/25 | Loss: 0.00093590
Iteration 8/25 | Loss: 0.00093590
Iteration 9/25 | Loss: 0.00093590
Iteration 10/25 | Loss: 0.00093590
Iteration 11/25 | Loss: 0.00093590
Iteration 12/25 | Loss: 0.00093590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009359018295072019, 0.0009359018295072019, 0.0009359018295072019, 0.0009359018295072019, 0.0009359018295072019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009359018295072019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43460989
Iteration 2/25 | Loss: 0.00065492
Iteration 3/25 | Loss: 0.00065491
Iteration 4/25 | Loss: 0.00065491
Iteration 5/25 | Loss: 0.00065491
Iteration 6/25 | Loss: 0.00065491
Iteration 7/25 | Loss: 0.00065491
Iteration 8/25 | Loss: 0.00065491
Iteration 9/25 | Loss: 0.00065491
Iteration 10/25 | Loss: 0.00065491
Iteration 11/25 | Loss: 0.00065491
Iteration 12/25 | Loss: 0.00065491
Iteration 13/25 | Loss: 0.00065491
Iteration 14/25 | Loss: 0.00065491
Iteration 15/25 | Loss: 0.00065491
Iteration 16/25 | Loss: 0.00065491
Iteration 17/25 | Loss: 0.00065491
Iteration 18/25 | Loss: 0.00065491
Iteration 19/25 | Loss: 0.00065491
Iteration 20/25 | Loss: 0.00065491
Iteration 21/25 | Loss: 0.00065491
Iteration 22/25 | Loss: 0.00065491
Iteration 23/25 | Loss: 0.00065491
Iteration 24/25 | Loss: 0.00065491
Iteration 25/25 | Loss: 0.00065491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065491
Iteration 2/1000 | Loss: 0.00003505
Iteration 3/1000 | Loss: 0.00001946
Iteration 4/1000 | Loss: 0.00001294
Iteration 5/1000 | Loss: 0.00001166
Iteration 6/1000 | Loss: 0.00001090
Iteration 7/1000 | Loss: 0.00001037
Iteration 8/1000 | Loss: 0.00001005
Iteration 9/1000 | Loss: 0.00000984
Iteration 10/1000 | Loss: 0.00000967
Iteration 11/1000 | Loss: 0.00000966
Iteration 12/1000 | Loss: 0.00000965
Iteration 13/1000 | Loss: 0.00000964
Iteration 14/1000 | Loss: 0.00000960
Iteration 15/1000 | Loss: 0.00000957
Iteration 16/1000 | Loss: 0.00000956
Iteration 17/1000 | Loss: 0.00000954
Iteration 18/1000 | Loss: 0.00000953
Iteration 19/1000 | Loss: 0.00000951
Iteration 20/1000 | Loss: 0.00000949
Iteration 21/1000 | Loss: 0.00000949
Iteration 22/1000 | Loss: 0.00000948
Iteration 23/1000 | Loss: 0.00000947
Iteration 24/1000 | Loss: 0.00000943
Iteration 25/1000 | Loss: 0.00000943
Iteration 26/1000 | Loss: 0.00000940
Iteration 27/1000 | Loss: 0.00000936
Iteration 28/1000 | Loss: 0.00000936
Iteration 29/1000 | Loss: 0.00000933
Iteration 30/1000 | Loss: 0.00000933
Iteration 31/1000 | Loss: 0.00000932
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000932
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000931
Iteration 36/1000 | Loss: 0.00000930
Iteration 37/1000 | Loss: 0.00000929
Iteration 38/1000 | Loss: 0.00000928
Iteration 39/1000 | Loss: 0.00000928
Iteration 40/1000 | Loss: 0.00000927
Iteration 41/1000 | Loss: 0.00000927
Iteration 42/1000 | Loss: 0.00000926
Iteration 43/1000 | Loss: 0.00000926
Iteration 44/1000 | Loss: 0.00000926
Iteration 45/1000 | Loss: 0.00000926
Iteration 46/1000 | Loss: 0.00000926
Iteration 47/1000 | Loss: 0.00000926
Iteration 48/1000 | Loss: 0.00000926
Iteration 49/1000 | Loss: 0.00000925
Iteration 50/1000 | Loss: 0.00000925
Iteration 51/1000 | Loss: 0.00000925
Iteration 52/1000 | Loss: 0.00000924
Iteration 53/1000 | Loss: 0.00000924
Iteration 54/1000 | Loss: 0.00000924
Iteration 55/1000 | Loss: 0.00000924
Iteration 56/1000 | Loss: 0.00000924
Iteration 57/1000 | Loss: 0.00000924
Iteration 58/1000 | Loss: 0.00000924
Iteration 59/1000 | Loss: 0.00000924
Iteration 60/1000 | Loss: 0.00000924
Iteration 61/1000 | Loss: 0.00000924
Iteration 62/1000 | Loss: 0.00000924
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000924
Iteration 65/1000 | Loss: 0.00000923
Iteration 66/1000 | Loss: 0.00000923
Iteration 67/1000 | Loss: 0.00000923
Iteration 68/1000 | Loss: 0.00000923
Iteration 69/1000 | Loss: 0.00000923
Iteration 70/1000 | Loss: 0.00000922
Iteration 71/1000 | Loss: 0.00000922
Iteration 72/1000 | Loss: 0.00000922
Iteration 73/1000 | Loss: 0.00000921
Iteration 74/1000 | Loss: 0.00000921
Iteration 75/1000 | Loss: 0.00000921
Iteration 76/1000 | Loss: 0.00000920
Iteration 77/1000 | Loss: 0.00000920
Iteration 78/1000 | Loss: 0.00000919
Iteration 79/1000 | Loss: 0.00000919
Iteration 80/1000 | Loss: 0.00000919
Iteration 81/1000 | Loss: 0.00000919
Iteration 82/1000 | Loss: 0.00000919
Iteration 83/1000 | Loss: 0.00000919
Iteration 84/1000 | Loss: 0.00000918
Iteration 85/1000 | Loss: 0.00000918
Iteration 86/1000 | Loss: 0.00000918
Iteration 87/1000 | Loss: 0.00000918
Iteration 88/1000 | Loss: 0.00000917
Iteration 89/1000 | Loss: 0.00000917
Iteration 90/1000 | Loss: 0.00000917
Iteration 91/1000 | Loss: 0.00000916
Iteration 92/1000 | Loss: 0.00000916
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000916
Iteration 95/1000 | Loss: 0.00000916
Iteration 96/1000 | Loss: 0.00000916
Iteration 97/1000 | Loss: 0.00000916
Iteration 98/1000 | Loss: 0.00000916
Iteration 99/1000 | Loss: 0.00000916
Iteration 100/1000 | Loss: 0.00000916
Iteration 101/1000 | Loss: 0.00000916
Iteration 102/1000 | Loss: 0.00000915
Iteration 103/1000 | Loss: 0.00000915
Iteration 104/1000 | Loss: 0.00000915
Iteration 105/1000 | Loss: 0.00000915
Iteration 106/1000 | Loss: 0.00000915
Iteration 107/1000 | Loss: 0.00000915
Iteration 108/1000 | Loss: 0.00000915
Iteration 109/1000 | Loss: 0.00000915
Iteration 110/1000 | Loss: 0.00000915
Iteration 111/1000 | Loss: 0.00000915
Iteration 112/1000 | Loss: 0.00000915
Iteration 113/1000 | Loss: 0.00000914
Iteration 114/1000 | Loss: 0.00000914
Iteration 115/1000 | Loss: 0.00000914
Iteration 116/1000 | Loss: 0.00000914
Iteration 117/1000 | Loss: 0.00000914
Iteration 118/1000 | Loss: 0.00000914
Iteration 119/1000 | Loss: 0.00000914
Iteration 120/1000 | Loss: 0.00000914
Iteration 121/1000 | Loss: 0.00000914
Iteration 122/1000 | Loss: 0.00000914
Iteration 123/1000 | Loss: 0.00000914
Iteration 124/1000 | Loss: 0.00000914
Iteration 125/1000 | Loss: 0.00000914
Iteration 126/1000 | Loss: 0.00000914
Iteration 127/1000 | Loss: 0.00000914
Iteration 128/1000 | Loss: 0.00000914
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000913
Iteration 132/1000 | Loss: 0.00000913
Iteration 133/1000 | Loss: 0.00000913
Iteration 134/1000 | Loss: 0.00000913
Iteration 135/1000 | Loss: 0.00000913
Iteration 136/1000 | Loss: 0.00000913
Iteration 137/1000 | Loss: 0.00000913
Iteration 138/1000 | Loss: 0.00000913
Iteration 139/1000 | Loss: 0.00000913
Iteration 140/1000 | Loss: 0.00000913
Iteration 141/1000 | Loss: 0.00000913
Iteration 142/1000 | Loss: 0.00000913
Iteration 143/1000 | Loss: 0.00000912
Iteration 144/1000 | Loss: 0.00000912
Iteration 145/1000 | Loss: 0.00000912
Iteration 146/1000 | Loss: 0.00000912
Iteration 147/1000 | Loss: 0.00000912
Iteration 148/1000 | Loss: 0.00000912
Iteration 149/1000 | Loss: 0.00000912
Iteration 150/1000 | Loss: 0.00000912
Iteration 151/1000 | Loss: 0.00000912
Iteration 152/1000 | Loss: 0.00000912
Iteration 153/1000 | Loss: 0.00000912
Iteration 154/1000 | Loss: 0.00000912
Iteration 155/1000 | Loss: 0.00000912
Iteration 156/1000 | Loss: 0.00000912
Iteration 157/1000 | Loss: 0.00000912
Iteration 158/1000 | Loss: 0.00000912
Iteration 159/1000 | Loss: 0.00000912
Iteration 160/1000 | Loss: 0.00000912
Iteration 161/1000 | Loss: 0.00000912
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Iteration 164/1000 | Loss: 0.00000911
Iteration 165/1000 | Loss: 0.00000911
Iteration 166/1000 | Loss: 0.00000911
Iteration 167/1000 | Loss: 0.00000911
Iteration 168/1000 | Loss: 0.00000911
Iteration 169/1000 | Loss: 0.00000911
Iteration 170/1000 | Loss: 0.00000911
Iteration 171/1000 | Loss: 0.00000911
Iteration 172/1000 | Loss: 0.00000911
Iteration 173/1000 | Loss: 0.00000911
Iteration 174/1000 | Loss: 0.00000911
Iteration 175/1000 | Loss: 0.00000911
Iteration 176/1000 | Loss: 0.00000911
Iteration 177/1000 | Loss: 0.00000911
Iteration 178/1000 | Loss: 0.00000911
Iteration 179/1000 | Loss: 0.00000911
Iteration 180/1000 | Loss: 0.00000911
Iteration 181/1000 | Loss: 0.00000911
Iteration 182/1000 | Loss: 0.00000911
Iteration 183/1000 | Loss: 0.00000911
Iteration 184/1000 | Loss: 0.00000911
Iteration 185/1000 | Loss: 0.00000911
Iteration 186/1000 | Loss: 0.00000911
Iteration 187/1000 | Loss: 0.00000911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [9.108332960749976e-06, 9.108332960749976e-06, 9.108332960749976e-06, 9.108332960749976e-06, 9.108332960749976e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.108332960749976e-06

Optimization complete. Final v2v error: 2.496829032897949 mm

Highest mean error: 3.604598045349121 mm for frame 47

Lowest mean error: 2.1849071979522705 mm for frame 84

Saving results

Total time: 36.872480392456055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826958
Iteration 2/25 | Loss: 0.00123967
Iteration 3/25 | Loss: 0.00098204
Iteration 4/25 | Loss: 0.00095968
Iteration 5/25 | Loss: 0.00095698
Iteration 6/25 | Loss: 0.00095642
Iteration 7/25 | Loss: 0.00095620
Iteration 8/25 | Loss: 0.00095618
Iteration 9/25 | Loss: 0.00095618
Iteration 10/25 | Loss: 0.00095618
Iteration 11/25 | Loss: 0.00095618
Iteration 12/25 | Loss: 0.00095618
Iteration 13/25 | Loss: 0.00095618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009561770129948854, 0.0009561770129948854, 0.0009561770129948854, 0.0009561770129948854, 0.0009561770129948854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009561770129948854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31170535
Iteration 2/25 | Loss: 0.00049952
Iteration 3/25 | Loss: 0.00049951
Iteration 4/25 | Loss: 0.00049951
Iteration 5/25 | Loss: 0.00049951
Iteration 6/25 | Loss: 0.00049951
Iteration 7/25 | Loss: 0.00049951
Iteration 8/25 | Loss: 0.00049951
Iteration 9/25 | Loss: 0.00049951
Iteration 10/25 | Loss: 0.00049951
Iteration 11/25 | Loss: 0.00049951
Iteration 12/25 | Loss: 0.00049951
Iteration 13/25 | Loss: 0.00049951
Iteration 14/25 | Loss: 0.00049951
Iteration 15/25 | Loss: 0.00049951
Iteration 16/25 | Loss: 0.00049951
Iteration 17/25 | Loss: 0.00049951
Iteration 18/25 | Loss: 0.00049951
Iteration 19/25 | Loss: 0.00049951
Iteration 20/25 | Loss: 0.00049951
Iteration 21/25 | Loss: 0.00049951
Iteration 22/25 | Loss: 0.00049951
Iteration 23/25 | Loss: 0.00049951
Iteration 24/25 | Loss: 0.00049951
Iteration 25/25 | Loss: 0.00049951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004995081690140069, 0.0004995081690140069, 0.0004995081690140069, 0.0004995081690140069, 0.0004995081690140069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004995081690140069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049951
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00002000
Iteration 4/1000 | Loss: 0.00001584
Iteration 5/1000 | Loss: 0.00001432
Iteration 6/1000 | Loss: 0.00001333
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001248
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001193
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001184
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001182
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001172
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001166
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001166
Iteration 88/1000 | Loss: 0.00001166
Iteration 89/1000 | Loss: 0.00001166
Iteration 90/1000 | Loss: 0.00001166
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.1658413313853089e-05, 1.1658413313853089e-05, 1.1658413313853089e-05, 1.1658413313853089e-05, 1.1658413313853089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1658413313853089e-05

Optimization complete. Final v2v error: 2.863229274749756 mm

Highest mean error: 3.245037794113159 mm for frame 80

Lowest mean error: 2.608970880508423 mm for frame 19

Saving results

Total time: 30.943625926971436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386093
Iteration 2/25 | Loss: 0.00102686
Iteration 3/25 | Loss: 0.00090835
Iteration 4/25 | Loss: 0.00090168
Iteration 5/25 | Loss: 0.00089997
Iteration 6/25 | Loss: 0.00089997
Iteration 7/25 | Loss: 0.00089997
Iteration 8/25 | Loss: 0.00089997
Iteration 9/25 | Loss: 0.00089997
Iteration 10/25 | Loss: 0.00089997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0008999717538245022, 0.0008999717538245022, 0.0008999717538245022, 0.0008999717538245022, 0.0008999717538245022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008999717538245022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31978643
Iteration 2/25 | Loss: 0.00052791
Iteration 3/25 | Loss: 0.00052791
Iteration 4/25 | Loss: 0.00052791
Iteration 5/25 | Loss: 0.00052790
Iteration 6/25 | Loss: 0.00052790
Iteration 7/25 | Loss: 0.00052790
Iteration 8/25 | Loss: 0.00052790
Iteration 9/25 | Loss: 0.00052790
Iteration 10/25 | Loss: 0.00052790
Iteration 11/25 | Loss: 0.00052790
Iteration 12/25 | Loss: 0.00052790
Iteration 13/25 | Loss: 0.00052790
Iteration 14/25 | Loss: 0.00052790
Iteration 15/25 | Loss: 0.00052790
Iteration 16/25 | Loss: 0.00052790
Iteration 17/25 | Loss: 0.00052790
Iteration 18/25 | Loss: 0.00052790
Iteration 19/25 | Loss: 0.00052790
Iteration 20/25 | Loss: 0.00052790
Iteration 21/25 | Loss: 0.00052790
Iteration 22/25 | Loss: 0.00052790
Iteration 23/25 | Loss: 0.00052790
Iteration 24/25 | Loss: 0.00052790
Iteration 25/25 | Loss: 0.00052790

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052790
Iteration 2/1000 | Loss: 0.00001412
Iteration 3/1000 | Loss: 0.00001066
Iteration 4/1000 | Loss: 0.00000974
Iteration 5/1000 | Loss: 0.00000923
Iteration 6/1000 | Loss: 0.00000880
Iteration 7/1000 | Loss: 0.00000880
Iteration 8/1000 | Loss: 0.00000866
Iteration 9/1000 | Loss: 0.00000854
Iteration 10/1000 | Loss: 0.00000849
Iteration 11/1000 | Loss: 0.00000849
Iteration 12/1000 | Loss: 0.00000849
Iteration 13/1000 | Loss: 0.00000848
Iteration 14/1000 | Loss: 0.00000847
Iteration 15/1000 | Loss: 0.00000846
Iteration 16/1000 | Loss: 0.00000846
Iteration 17/1000 | Loss: 0.00000845
Iteration 18/1000 | Loss: 0.00000844
Iteration 19/1000 | Loss: 0.00000843
Iteration 20/1000 | Loss: 0.00000841
Iteration 21/1000 | Loss: 0.00000840
Iteration 22/1000 | Loss: 0.00000839
Iteration 23/1000 | Loss: 0.00000838
Iteration 24/1000 | Loss: 0.00000837
Iteration 25/1000 | Loss: 0.00000837
Iteration 26/1000 | Loss: 0.00000836
Iteration 27/1000 | Loss: 0.00000836
Iteration 28/1000 | Loss: 0.00000836
Iteration 29/1000 | Loss: 0.00000835
Iteration 30/1000 | Loss: 0.00000835
Iteration 31/1000 | Loss: 0.00000835
Iteration 32/1000 | Loss: 0.00000834
Iteration 33/1000 | Loss: 0.00000834
Iteration 34/1000 | Loss: 0.00000834
Iteration 35/1000 | Loss: 0.00000834
Iteration 36/1000 | Loss: 0.00000834
Iteration 37/1000 | Loss: 0.00000834
Iteration 38/1000 | Loss: 0.00000833
Iteration 39/1000 | Loss: 0.00000833
Iteration 40/1000 | Loss: 0.00000833
Iteration 41/1000 | Loss: 0.00000832
Iteration 42/1000 | Loss: 0.00000832
Iteration 43/1000 | Loss: 0.00000832
Iteration 44/1000 | Loss: 0.00000832
Iteration 45/1000 | Loss: 0.00000832
Iteration 46/1000 | Loss: 0.00000832
Iteration 47/1000 | Loss: 0.00000832
Iteration 48/1000 | Loss: 0.00000832
Iteration 49/1000 | Loss: 0.00000832
Iteration 50/1000 | Loss: 0.00000832
Iteration 51/1000 | Loss: 0.00000831
Iteration 52/1000 | Loss: 0.00000831
Iteration 53/1000 | Loss: 0.00000831
Iteration 54/1000 | Loss: 0.00000831
Iteration 55/1000 | Loss: 0.00000830
Iteration 56/1000 | Loss: 0.00000830
Iteration 57/1000 | Loss: 0.00000830
Iteration 58/1000 | Loss: 0.00000830
Iteration 59/1000 | Loss: 0.00000830
Iteration 60/1000 | Loss: 0.00000830
Iteration 61/1000 | Loss: 0.00000829
Iteration 62/1000 | Loss: 0.00000829
Iteration 63/1000 | Loss: 0.00000829
Iteration 64/1000 | Loss: 0.00000829
Iteration 65/1000 | Loss: 0.00000829
Iteration 66/1000 | Loss: 0.00000829
Iteration 67/1000 | Loss: 0.00000828
Iteration 68/1000 | Loss: 0.00000828
Iteration 69/1000 | Loss: 0.00000828
Iteration 70/1000 | Loss: 0.00000827
Iteration 71/1000 | Loss: 0.00000827
Iteration 72/1000 | Loss: 0.00000827
Iteration 73/1000 | Loss: 0.00000827
Iteration 74/1000 | Loss: 0.00000827
Iteration 75/1000 | Loss: 0.00000826
Iteration 76/1000 | Loss: 0.00000826
Iteration 77/1000 | Loss: 0.00000826
Iteration 78/1000 | Loss: 0.00000826
Iteration 79/1000 | Loss: 0.00000826
Iteration 80/1000 | Loss: 0.00000826
Iteration 81/1000 | Loss: 0.00000826
Iteration 82/1000 | Loss: 0.00000826
Iteration 83/1000 | Loss: 0.00000825
Iteration 84/1000 | Loss: 0.00000825
Iteration 85/1000 | Loss: 0.00000825
Iteration 86/1000 | Loss: 0.00000825
Iteration 87/1000 | Loss: 0.00000825
Iteration 88/1000 | Loss: 0.00000824
Iteration 89/1000 | Loss: 0.00000824
Iteration 90/1000 | Loss: 0.00000824
Iteration 91/1000 | Loss: 0.00000824
Iteration 92/1000 | Loss: 0.00000824
Iteration 93/1000 | Loss: 0.00000824
Iteration 94/1000 | Loss: 0.00000823
Iteration 95/1000 | Loss: 0.00000823
Iteration 96/1000 | Loss: 0.00000823
Iteration 97/1000 | Loss: 0.00000823
Iteration 98/1000 | Loss: 0.00000823
Iteration 99/1000 | Loss: 0.00000823
Iteration 100/1000 | Loss: 0.00000822
Iteration 101/1000 | Loss: 0.00000822
Iteration 102/1000 | Loss: 0.00000822
Iteration 103/1000 | Loss: 0.00000822
Iteration 104/1000 | Loss: 0.00000822
Iteration 105/1000 | Loss: 0.00000822
Iteration 106/1000 | Loss: 0.00000822
Iteration 107/1000 | Loss: 0.00000822
Iteration 108/1000 | Loss: 0.00000822
Iteration 109/1000 | Loss: 0.00000822
Iteration 110/1000 | Loss: 0.00000822
Iteration 111/1000 | Loss: 0.00000822
Iteration 112/1000 | Loss: 0.00000822
Iteration 113/1000 | Loss: 0.00000822
Iteration 114/1000 | Loss: 0.00000822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [8.218832590500824e-06, 8.218832590500824e-06, 8.218832590500824e-06, 8.218832590500824e-06, 8.218832590500824e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.218832590500824e-06

Optimization complete. Final v2v error: 2.4521291255950928 mm

Highest mean error: 2.5487060546875 mm for frame 157

Lowest mean error: 2.3520545959472656 mm for frame 253

Saving results

Total time: 29.59776735305786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499804
Iteration 2/25 | Loss: 0.00102685
Iteration 3/25 | Loss: 0.00093832
Iteration 4/25 | Loss: 0.00092111
Iteration 5/25 | Loss: 0.00091537
Iteration 6/25 | Loss: 0.00091421
Iteration 7/25 | Loss: 0.00091421
Iteration 8/25 | Loss: 0.00091421
Iteration 9/25 | Loss: 0.00091421
Iteration 10/25 | Loss: 0.00091421
Iteration 11/25 | Loss: 0.00091421
Iteration 12/25 | Loss: 0.00091421
Iteration 13/25 | Loss: 0.00091421
Iteration 14/25 | Loss: 0.00091421
Iteration 15/25 | Loss: 0.00091421
Iteration 16/25 | Loss: 0.00091421
Iteration 17/25 | Loss: 0.00091421
Iteration 18/25 | Loss: 0.00091421
Iteration 19/25 | Loss: 0.00091421
Iteration 20/25 | Loss: 0.00091421
Iteration 21/25 | Loss: 0.00091421
Iteration 22/25 | Loss: 0.00091421
Iteration 23/25 | Loss: 0.00091421
Iteration 24/25 | Loss: 0.00091421
Iteration 25/25 | Loss: 0.00091421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45354486
Iteration 2/25 | Loss: 0.00060258
Iteration 3/25 | Loss: 0.00060258
Iteration 4/25 | Loss: 0.00060258
Iteration 5/25 | Loss: 0.00060258
Iteration 6/25 | Loss: 0.00060258
Iteration 7/25 | Loss: 0.00060258
Iteration 8/25 | Loss: 0.00060258
Iteration 9/25 | Loss: 0.00060258
Iteration 10/25 | Loss: 0.00060258
Iteration 11/25 | Loss: 0.00060258
Iteration 12/25 | Loss: 0.00060258
Iteration 13/25 | Loss: 0.00060258
Iteration 14/25 | Loss: 0.00060258
Iteration 15/25 | Loss: 0.00060258
Iteration 16/25 | Loss: 0.00060258
Iteration 17/25 | Loss: 0.00060258
Iteration 18/25 | Loss: 0.00060258
Iteration 19/25 | Loss: 0.00060258
Iteration 20/25 | Loss: 0.00060258
Iteration 21/25 | Loss: 0.00060258
Iteration 22/25 | Loss: 0.00060258
Iteration 23/25 | Loss: 0.00060258
Iteration 24/25 | Loss: 0.00060258
Iteration 25/25 | Loss: 0.00060258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060258
Iteration 2/1000 | Loss: 0.00001968
Iteration 3/1000 | Loss: 0.00001322
Iteration 4/1000 | Loss: 0.00001173
Iteration 5/1000 | Loss: 0.00001117
Iteration 6/1000 | Loss: 0.00001067
Iteration 7/1000 | Loss: 0.00001057
Iteration 8/1000 | Loss: 0.00001025
Iteration 9/1000 | Loss: 0.00001002
Iteration 10/1000 | Loss: 0.00001000
Iteration 11/1000 | Loss: 0.00000987
Iteration 12/1000 | Loss: 0.00000982
Iteration 13/1000 | Loss: 0.00000981
Iteration 14/1000 | Loss: 0.00000979
Iteration 15/1000 | Loss: 0.00000973
Iteration 16/1000 | Loss: 0.00000971
Iteration 17/1000 | Loss: 0.00000969
Iteration 18/1000 | Loss: 0.00000969
Iteration 19/1000 | Loss: 0.00000968
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000968
Iteration 22/1000 | Loss: 0.00000968
Iteration 23/1000 | Loss: 0.00000967
Iteration 24/1000 | Loss: 0.00000967
Iteration 25/1000 | Loss: 0.00000966
Iteration 26/1000 | Loss: 0.00000966
Iteration 27/1000 | Loss: 0.00000966
Iteration 28/1000 | Loss: 0.00000965
Iteration 29/1000 | Loss: 0.00000965
Iteration 30/1000 | Loss: 0.00000965
Iteration 31/1000 | Loss: 0.00000964
Iteration 32/1000 | Loss: 0.00000964
Iteration 33/1000 | Loss: 0.00000964
Iteration 34/1000 | Loss: 0.00000964
Iteration 35/1000 | Loss: 0.00000964
Iteration 36/1000 | Loss: 0.00000964
Iteration 37/1000 | Loss: 0.00000964
Iteration 38/1000 | Loss: 0.00000963
Iteration 39/1000 | Loss: 0.00000962
Iteration 40/1000 | Loss: 0.00000961
Iteration 41/1000 | Loss: 0.00000961
Iteration 42/1000 | Loss: 0.00000961
Iteration 43/1000 | Loss: 0.00000961
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000960
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000958
Iteration 48/1000 | Loss: 0.00000957
Iteration 49/1000 | Loss: 0.00000956
Iteration 50/1000 | Loss: 0.00000956
Iteration 51/1000 | Loss: 0.00000956
Iteration 52/1000 | Loss: 0.00000956
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000956
Iteration 57/1000 | Loss: 0.00000956
Iteration 58/1000 | Loss: 0.00000956
Iteration 59/1000 | Loss: 0.00000956
Iteration 60/1000 | Loss: 0.00000956
Iteration 61/1000 | Loss: 0.00000956
Iteration 62/1000 | Loss: 0.00000956
Iteration 63/1000 | Loss: 0.00000956
Iteration 64/1000 | Loss: 0.00000956
Iteration 65/1000 | Loss: 0.00000956
Iteration 66/1000 | Loss: 0.00000956
Iteration 67/1000 | Loss: 0.00000956
Iteration 68/1000 | Loss: 0.00000956
Iteration 69/1000 | Loss: 0.00000956
Iteration 70/1000 | Loss: 0.00000956
Iteration 71/1000 | Loss: 0.00000956
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000956
Iteration 74/1000 | Loss: 0.00000956
Iteration 75/1000 | Loss: 0.00000956
Iteration 76/1000 | Loss: 0.00000956
Iteration 77/1000 | Loss: 0.00000956
Iteration 78/1000 | Loss: 0.00000956
Iteration 79/1000 | Loss: 0.00000956
Iteration 80/1000 | Loss: 0.00000956
Iteration 81/1000 | Loss: 0.00000956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [9.558466445014346e-06, 9.558466445014346e-06, 9.558466445014346e-06, 9.558466445014346e-06, 9.558466445014346e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.558466445014346e-06

Optimization complete. Final v2v error: 2.673637628555298 mm

Highest mean error: 2.889418840408325 mm for frame 114

Lowest mean error: 2.468843698501587 mm for frame 81

Saving results

Total time: 28.04495859146118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889958
Iteration 2/25 | Loss: 0.00112335
Iteration 3/25 | Loss: 0.00099635
Iteration 4/25 | Loss: 0.00097268
Iteration 5/25 | Loss: 0.00096630
Iteration 6/25 | Loss: 0.00096535
Iteration 7/25 | Loss: 0.00096535
Iteration 8/25 | Loss: 0.00096535
Iteration 9/25 | Loss: 0.00096535
Iteration 10/25 | Loss: 0.00096535
Iteration 11/25 | Loss: 0.00096535
Iteration 12/25 | Loss: 0.00096535
Iteration 13/25 | Loss: 0.00096535
Iteration 14/25 | Loss: 0.00096535
Iteration 15/25 | Loss: 0.00096535
Iteration 16/25 | Loss: 0.00096535
Iteration 17/25 | Loss: 0.00096535
Iteration 18/25 | Loss: 0.00096535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009653523447923362, 0.0009653523447923362, 0.0009653523447923362, 0.0009653523447923362, 0.0009653523447923362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009653523447923362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28552282
Iteration 2/25 | Loss: 0.00082849
Iteration 3/25 | Loss: 0.00082849
Iteration 4/25 | Loss: 0.00082849
Iteration 5/25 | Loss: 0.00082849
Iteration 6/25 | Loss: 0.00082849
Iteration 7/25 | Loss: 0.00082849
Iteration 8/25 | Loss: 0.00082849
Iteration 9/25 | Loss: 0.00082849
Iteration 10/25 | Loss: 0.00082849
Iteration 11/25 | Loss: 0.00082849
Iteration 12/25 | Loss: 0.00082849
Iteration 13/25 | Loss: 0.00082849
Iteration 14/25 | Loss: 0.00082849
Iteration 15/25 | Loss: 0.00082849
Iteration 16/25 | Loss: 0.00082849
Iteration 17/25 | Loss: 0.00082849
Iteration 18/25 | Loss: 0.00082849
Iteration 19/25 | Loss: 0.00082849
Iteration 20/25 | Loss: 0.00082849
Iteration 21/25 | Loss: 0.00082849
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008284872747026384, 0.0008284872747026384, 0.0008284872747026384, 0.0008284872747026384, 0.0008284872747026384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008284872747026384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082849
Iteration 2/1000 | Loss: 0.00003945
Iteration 3/1000 | Loss: 0.00002242
Iteration 4/1000 | Loss: 0.00001884
Iteration 5/1000 | Loss: 0.00001778
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001624
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001528
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001482
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001468
Iteration 14/1000 | Loss: 0.00001464
Iteration 15/1000 | Loss: 0.00001464
Iteration 16/1000 | Loss: 0.00001462
Iteration 17/1000 | Loss: 0.00001462
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001459
Iteration 20/1000 | Loss: 0.00001459
Iteration 21/1000 | Loss: 0.00001459
Iteration 22/1000 | Loss: 0.00001458
Iteration 23/1000 | Loss: 0.00001457
Iteration 24/1000 | Loss: 0.00001457
Iteration 25/1000 | Loss: 0.00001456
Iteration 26/1000 | Loss: 0.00001455
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001453
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001450
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001448
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001447
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001445
Iteration 68/1000 | Loss: 0.00001445
Iteration 69/1000 | Loss: 0.00001445
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001444
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001443
Iteration 75/1000 | Loss: 0.00001443
Iteration 76/1000 | Loss: 0.00001443
Iteration 77/1000 | Loss: 0.00001443
Iteration 78/1000 | Loss: 0.00001442
Iteration 79/1000 | Loss: 0.00001442
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001441
Iteration 88/1000 | Loss: 0.00001441
Iteration 89/1000 | Loss: 0.00001441
Iteration 90/1000 | Loss: 0.00001441
Iteration 91/1000 | Loss: 0.00001441
Iteration 92/1000 | Loss: 0.00001441
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.440543474018341e-05, 1.440543474018341e-05, 1.440543474018341e-05, 1.440543474018341e-05, 1.440543474018341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.440543474018341e-05

Optimization complete. Final v2v error: 3.177248001098633 mm

Highest mean error: 3.675238609313965 mm for frame 238

Lowest mean error: 2.7879934310913086 mm for frame 153

Saving results

Total time: 36.13437223434448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023882
Iteration 2/25 | Loss: 0.00429722
Iteration 3/25 | Loss: 0.00243516
Iteration 4/25 | Loss: 0.00206887
Iteration 5/25 | Loss: 0.00189364
Iteration 6/25 | Loss: 0.00181867
Iteration 7/25 | Loss: 0.00177156
Iteration 8/25 | Loss: 0.00174877
Iteration 9/25 | Loss: 0.00174978
Iteration 10/25 | Loss: 0.00171611
Iteration 11/25 | Loss: 0.00167754
Iteration 12/25 | Loss: 0.00166989
Iteration 13/25 | Loss: 0.00166634
Iteration 14/25 | Loss: 0.00165659
Iteration 15/25 | Loss: 0.00164694
Iteration 16/25 | Loss: 0.00163984
Iteration 17/25 | Loss: 0.00163712
Iteration 18/25 | Loss: 0.00163631
Iteration 19/25 | Loss: 0.00163616
Iteration 20/25 | Loss: 0.00163610
Iteration 21/25 | Loss: 0.00163610
Iteration 22/25 | Loss: 0.00163609
Iteration 23/25 | Loss: 0.00163609
Iteration 24/25 | Loss: 0.00163609
Iteration 25/25 | Loss: 0.00163609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26654243
Iteration 2/25 | Loss: 0.00597574
Iteration 3/25 | Loss: 0.00436185
Iteration 4/25 | Loss: 0.00436185
Iteration 5/25 | Loss: 0.00436185
Iteration 6/25 | Loss: 0.00436185
Iteration 7/25 | Loss: 0.00436185
Iteration 8/25 | Loss: 0.00436184
Iteration 9/25 | Loss: 0.00436184
Iteration 10/25 | Loss: 0.00436184
Iteration 11/25 | Loss: 0.00436184
Iteration 12/25 | Loss: 0.00436184
Iteration 13/25 | Loss: 0.00436184
Iteration 14/25 | Loss: 0.00436184
Iteration 15/25 | Loss: 0.00436184
Iteration 16/25 | Loss: 0.00436184
Iteration 17/25 | Loss: 0.00436184
Iteration 18/25 | Loss: 0.00436184
Iteration 19/25 | Loss: 0.00436184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004361843224614859, 0.004361843224614859, 0.004361843224614859, 0.004361843224614859, 0.004361843224614859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004361843224614859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00436184
Iteration 2/1000 | Loss: 0.00490190
Iteration 3/1000 | Loss: 0.00218332
Iteration 4/1000 | Loss: 0.00057237
Iteration 5/1000 | Loss: 0.00098466
Iteration 6/1000 | Loss: 0.00050900
Iteration 7/1000 | Loss: 0.00036261
Iteration 8/1000 | Loss: 0.00033425
Iteration 9/1000 | Loss: 0.00068579
Iteration 10/1000 | Loss: 0.00065183
Iteration 11/1000 | Loss: 0.01908137
Iteration 12/1000 | Loss: 0.00564494
Iteration 13/1000 | Loss: 0.00052881
Iteration 14/1000 | Loss: 0.00162972
Iteration 15/1000 | Loss: 0.00557122
Iteration 16/1000 | Loss: 0.00027083
Iteration 17/1000 | Loss: 0.00121399
Iteration 18/1000 | Loss: 0.00295495
Iteration 19/1000 | Loss: 0.00243426
Iteration 20/1000 | Loss: 0.00211016
Iteration 21/1000 | Loss: 0.00062481
Iteration 22/1000 | Loss: 0.00055107
Iteration 23/1000 | Loss: 0.00020639
Iteration 24/1000 | Loss: 0.00119547
Iteration 25/1000 | Loss: 0.00487304
Iteration 26/1000 | Loss: 0.00443749
Iteration 27/1000 | Loss: 0.00592571
Iteration 28/1000 | Loss: 0.00065500
Iteration 29/1000 | Loss: 0.00049684
Iteration 30/1000 | Loss: 0.00006871
Iteration 31/1000 | Loss: 0.00023412
Iteration 32/1000 | Loss: 0.00035748
Iteration 33/1000 | Loss: 0.00048263
Iteration 34/1000 | Loss: 0.00002772
Iteration 35/1000 | Loss: 0.00035765
Iteration 36/1000 | Loss: 0.00025723
Iteration 37/1000 | Loss: 0.00024574
Iteration 38/1000 | Loss: 0.00130107
Iteration 39/1000 | Loss: 0.00036452
Iteration 40/1000 | Loss: 0.00022527
Iteration 41/1000 | Loss: 0.00003006
Iteration 42/1000 | Loss: 0.00015000
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00013318
Iteration 45/1000 | Loss: 0.00006268
Iteration 46/1000 | Loss: 0.00004820
Iteration 47/1000 | Loss: 0.00001164
Iteration 48/1000 | Loss: 0.00003674
Iteration 49/1000 | Loss: 0.00004568
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00004367
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00011344
Iteration 54/1000 | Loss: 0.00000974
Iteration 55/1000 | Loss: 0.00000946
Iteration 56/1000 | Loss: 0.00003112
Iteration 57/1000 | Loss: 0.00000940
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000924
Iteration 60/1000 | Loss: 0.00000924
Iteration 61/1000 | Loss: 0.00000922
Iteration 62/1000 | Loss: 0.00000922
Iteration 63/1000 | Loss: 0.00000922
Iteration 64/1000 | Loss: 0.00000921
Iteration 65/1000 | Loss: 0.00003030
Iteration 66/1000 | Loss: 0.00000928
Iteration 67/1000 | Loss: 0.00000909
Iteration 68/1000 | Loss: 0.00000907
Iteration 69/1000 | Loss: 0.00000907
Iteration 70/1000 | Loss: 0.00000907
Iteration 71/1000 | Loss: 0.00000907
Iteration 72/1000 | Loss: 0.00000906
Iteration 73/1000 | Loss: 0.00000906
Iteration 74/1000 | Loss: 0.00000906
Iteration 75/1000 | Loss: 0.00000906
Iteration 76/1000 | Loss: 0.00000905
Iteration 77/1000 | Loss: 0.00000905
Iteration 78/1000 | Loss: 0.00000905
Iteration 79/1000 | Loss: 0.00000905
Iteration 80/1000 | Loss: 0.00000905
Iteration 81/1000 | Loss: 0.00000905
Iteration 82/1000 | Loss: 0.00000905
Iteration 83/1000 | Loss: 0.00000904
Iteration 84/1000 | Loss: 0.00000904
Iteration 85/1000 | Loss: 0.00000904
Iteration 86/1000 | Loss: 0.00000904
Iteration 87/1000 | Loss: 0.00007716
Iteration 88/1000 | Loss: 0.00025934
Iteration 89/1000 | Loss: 0.00028699
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00000934
Iteration 92/1000 | Loss: 0.00000914
Iteration 93/1000 | Loss: 0.00000904
Iteration 94/1000 | Loss: 0.00000902
Iteration 95/1000 | Loss: 0.00000902
Iteration 96/1000 | Loss: 0.00000902
Iteration 97/1000 | Loss: 0.00000902
Iteration 98/1000 | Loss: 0.00000902
Iteration 99/1000 | Loss: 0.00000902
Iteration 100/1000 | Loss: 0.00000901
Iteration 101/1000 | Loss: 0.00000901
Iteration 102/1000 | Loss: 0.00006600
Iteration 103/1000 | Loss: 0.00000914
Iteration 104/1000 | Loss: 0.00000899
Iteration 105/1000 | Loss: 0.00000896
Iteration 106/1000 | Loss: 0.00000896
Iteration 107/1000 | Loss: 0.00000896
Iteration 108/1000 | Loss: 0.00000895
Iteration 109/1000 | Loss: 0.00000895
Iteration 110/1000 | Loss: 0.00000895
Iteration 111/1000 | Loss: 0.00000895
Iteration 112/1000 | Loss: 0.00000895
Iteration 113/1000 | Loss: 0.00000895
Iteration 114/1000 | Loss: 0.00000895
Iteration 115/1000 | Loss: 0.00000895
Iteration 116/1000 | Loss: 0.00000895
Iteration 117/1000 | Loss: 0.00000894
Iteration 118/1000 | Loss: 0.00000894
Iteration 119/1000 | Loss: 0.00000894
Iteration 120/1000 | Loss: 0.00000894
Iteration 121/1000 | Loss: 0.00000894
Iteration 122/1000 | Loss: 0.00000894
Iteration 123/1000 | Loss: 0.00000894
Iteration 124/1000 | Loss: 0.00000894
Iteration 125/1000 | Loss: 0.00000894
Iteration 126/1000 | Loss: 0.00000894
Iteration 127/1000 | Loss: 0.00000894
Iteration 128/1000 | Loss: 0.00000894
Iteration 129/1000 | Loss: 0.00000894
Iteration 130/1000 | Loss: 0.00000894
Iteration 131/1000 | Loss: 0.00000894
Iteration 132/1000 | Loss: 0.00000894
Iteration 133/1000 | Loss: 0.00000894
Iteration 134/1000 | Loss: 0.00000894
Iteration 135/1000 | Loss: 0.00000894
Iteration 136/1000 | Loss: 0.00000894
Iteration 137/1000 | Loss: 0.00000894
Iteration 138/1000 | Loss: 0.00000894
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [8.943230568547733e-06, 8.943230568547733e-06, 8.943230568547733e-06, 8.943230568547733e-06, 8.943230568547733e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.943230568547733e-06

Optimization complete. Final v2v error: 2.571441650390625 mm

Highest mean error: 2.868443489074707 mm for frame 23

Lowest mean error: 2.420850992202759 mm for frame 30

Saving results

Total time: 128.97578048706055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871721
Iteration 2/25 | Loss: 0.00157981
Iteration 3/25 | Loss: 0.00117379
Iteration 4/25 | Loss: 0.00111210
Iteration 5/25 | Loss: 0.00109803
Iteration 6/25 | Loss: 0.00112115
Iteration 7/25 | Loss: 0.00109947
Iteration 8/25 | Loss: 0.00107546
Iteration 9/25 | Loss: 0.00106550
Iteration 10/25 | Loss: 0.00106792
Iteration 11/25 | Loss: 0.00106655
Iteration 12/25 | Loss: 0.00106338
Iteration 13/25 | Loss: 0.00106241
Iteration 14/25 | Loss: 0.00106200
Iteration 15/25 | Loss: 0.00106188
Iteration 16/25 | Loss: 0.00106185
Iteration 17/25 | Loss: 0.00106185
Iteration 18/25 | Loss: 0.00106184
Iteration 19/25 | Loss: 0.00106184
Iteration 20/25 | Loss: 0.00106184
Iteration 21/25 | Loss: 0.00106184
Iteration 22/25 | Loss: 0.00106184
Iteration 23/25 | Loss: 0.00106184
Iteration 24/25 | Loss: 0.00106184
Iteration 25/25 | Loss: 0.00106184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24668181
Iteration 2/25 | Loss: 0.00051636
Iteration 3/25 | Loss: 0.00051636
Iteration 4/25 | Loss: 0.00051636
Iteration 5/25 | Loss: 0.00051636
Iteration 6/25 | Loss: 0.00051636
Iteration 7/25 | Loss: 0.00051635
Iteration 8/25 | Loss: 0.00051635
Iteration 9/25 | Loss: 0.00051635
Iteration 10/25 | Loss: 0.00051635
Iteration 11/25 | Loss: 0.00051635
Iteration 12/25 | Loss: 0.00051635
Iteration 13/25 | Loss: 0.00051635
Iteration 14/25 | Loss: 0.00051635
Iteration 15/25 | Loss: 0.00051635
Iteration 16/25 | Loss: 0.00051635
Iteration 17/25 | Loss: 0.00051635
Iteration 18/25 | Loss: 0.00051635
Iteration 19/25 | Loss: 0.00051635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005163543974049389, 0.0005163543974049389, 0.0005163543974049389, 0.0005163543974049389, 0.0005163543974049389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005163543974049389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051635
Iteration 2/1000 | Loss: 0.00005280
Iteration 3/1000 | Loss: 0.00002909
Iteration 4/1000 | Loss: 0.00002265
Iteration 5/1000 | Loss: 0.00002081
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00001909
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001793
Iteration 12/1000 | Loss: 0.00001786
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001766
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001760
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001757
Iteration 19/1000 | Loss: 0.00001757
Iteration 20/1000 | Loss: 0.00001756
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001744
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001728
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001727
Iteration 45/1000 | Loss: 0.00001727
Iteration 46/1000 | Loss: 0.00001727
Iteration 47/1000 | Loss: 0.00001726
Iteration 48/1000 | Loss: 0.00001726
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001725
Iteration 60/1000 | Loss: 0.00001725
Iteration 61/1000 | Loss: 0.00001725
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001724
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001723
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001723
Iteration 75/1000 | Loss: 0.00001723
Iteration 76/1000 | Loss: 0.00001723
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001721
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001721
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001718
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001717
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001717
Iteration 113/1000 | Loss: 0.00001717
Iteration 114/1000 | Loss: 0.00001717
Iteration 115/1000 | Loss: 0.00001717
Iteration 116/1000 | Loss: 0.00001717
Iteration 117/1000 | Loss: 0.00001717
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001716
Iteration 120/1000 | Loss: 0.00001716
Iteration 121/1000 | Loss: 0.00001716
Iteration 122/1000 | Loss: 0.00001716
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001715
Iteration 129/1000 | Loss: 0.00001715
Iteration 130/1000 | Loss: 0.00001715
Iteration 131/1000 | Loss: 0.00001715
Iteration 132/1000 | Loss: 0.00001714
Iteration 133/1000 | Loss: 0.00001714
Iteration 134/1000 | Loss: 0.00001714
Iteration 135/1000 | Loss: 0.00001714
Iteration 136/1000 | Loss: 0.00001714
Iteration 137/1000 | Loss: 0.00001714
Iteration 138/1000 | Loss: 0.00001714
Iteration 139/1000 | Loss: 0.00001714
Iteration 140/1000 | Loss: 0.00001714
Iteration 141/1000 | Loss: 0.00001714
Iteration 142/1000 | Loss: 0.00001714
Iteration 143/1000 | Loss: 0.00001714
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001713
Iteration 146/1000 | Loss: 0.00001713
Iteration 147/1000 | Loss: 0.00001713
Iteration 148/1000 | Loss: 0.00001713
Iteration 149/1000 | Loss: 0.00001713
Iteration 150/1000 | Loss: 0.00001713
Iteration 151/1000 | Loss: 0.00001713
Iteration 152/1000 | Loss: 0.00001713
Iteration 153/1000 | Loss: 0.00001713
Iteration 154/1000 | Loss: 0.00001713
Iteration 155/1000 | Loss: 0.00001713
Iteration 156/1000 | Loss: 0.00001712
Iteration 157/1000 | Loss: 0.00001712
Iteration 158/1000 | Loss: 0.00001712
Iteration 159/1000 | Loss: 0.00001712
Iteration 160/1000 | Loss: 0.00001712
Iteration 161/1000 | Loss: 0.00001712
Iteration 162/1000 | Loss: 0.00001712
Iteration 163/1000 | Loss: 0.00001712
Iteration 164/1000 | Loss: 0.00001712
Iteration 165/1000 | Loss: 0.00001712
Iteration 166/1000 | Loss: 0.00001712
Iteration 167/1000 | Loss: 0.00001712
Iteration 168/1000 | Loss: 0.00001712
Iteration 169/1000 | Loss: 0.00001712
Iteration 170/1000 | Loss: 0.00001712
Iteration 171/1000 | Loss: 0.00001712
Iteration 172/1000 | Loss: 0.00001712
Iteration 173/1000 | Loss: 0.00001712
Iteration 174/1000 | Loss: 0.00001712
Iteration 175/1000 | Loss: 0.00001712
Iteration 176/1000 | Loss: 0.00001712
Iteration 177/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.7119489712058567e-05, 1.7119489712058567e-05, 1.7119489712058567e-05, 1.7119489712058567e-05, 1.7119489712058567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7119489712058567e-05

Optimization complete. Final v2v error: 3.490192413330078 mm

Highest mean error: 3.8116254806518555 mm for frame 146

Lowest mean error: 3.078995704650879 mm for frame 37

Saving results

Total time: 64.27587509155273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889905
Iteration 2/25 | Loss: 0.00150110
Iteration 3/25 | Loss: 0.00109506
Iteration 4/25 | Loss: 0.00105819
Iteration 5/25 | Loss: 0.00105164
Iteration 6/25 | Loss: 0.00104953
Iteration 7/25 | Loss: 0.00104932
Iteration 8/25 | Loss: 0.00104932
Iteration 9/25 | Loss: 0.00104932
Iteration 10/25 | Loss: 0.00104932
Iteration 11/25 | Loss: 0.00104932
Iteration 12/25 | Loss: 0.00104932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010493150912225246, 0.0010493150912225246, 0.0010493150912225246, 0.0010493150912225246, 0.0010493150912225246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010493150912225246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34251118
Iteration 2/25 | Loss: 0.00062982
Iteration 3/25 | Loss: 0.00062981
Iteration 4/25 | Loss: 0.00062981
Iteration 5/25 | Loss: 0.00062981
Iteration 6/25 | Loss: 0.00062981
Iteration 7/25 | Loss: 0.00062981
Iteration 8/25 | Loss: 0.00062981
Iteration 9/25 | Loss: 0.00062981
Iteration 10/25 | Loss: 0.00062981
Iteration 11/25 | Loss: 0.00062981
Iteration 12/25 | Loss: 0.00062981
Iteration 13/25 | Loss: 0.00062981
Iteration 14/25 | Loss: 0.00062981
Iteration 15/25 | Loss: 0.00062981
Iteration 16/25 | Loss: 0.00062981
Iteration 17/25 | Loss: 0.00062981
Iteration 18/25 | Loss: 0.00062981
Iteration 19/25 | Loss: 0.00062981
Iteration 20/25 | Loss: 0.00062981
Iteration 21/25 | Loss: 0.00062981
Iteration 22/25 | Loss: 0.00062981
Iteration 23/25 | Loss: 0.00062981
Iteration 24/25 | Loss: 0.00062981
Iteration 25/25 | Loss: 0.00062981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062981
Iteration 2/1000 | Loss: 0.00004230
Iteration 3/1000 | Loss: 0.00002933
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002593
Iteration 6/1000 | Loss: 0.00002529
Iteration 7/1000 | Loss: 0.00002476
Iteration 8/1000 | Loss: 0.00002432
Iteration 9/1000 | Loss: 0.00002405
Iteration 10/1000 | Loss: 0.00002384
Iteration 11/1000 | Loss: 0.00002362
Iteration 12/1000 | Loss: 0.00002348
Iteration 13/1000 | Loss: 0.00002338
Iteration 14/1000 | Loss: 0.00002329
Iteration 15/1000 | Loss: 0.00002322
Iteration 16/1000 | Loss: 0.00002313
Iteration 17/1000 | Loss: 0.00002313
Iteration 18/1000 | Loss: 0.00002312
Iteration 19/1000 | Loss: 0.00002309
Iteration 20/1000 | Loss: 0.00002309
Iteration 21/1000 | Loss: 0.00002309
Iteration 22/1000 | Loss: 0.00002308
Iteration 23/1000 | Loss: 0.00002308
Iteration 24/1000 | Loss: 0.00002305
Iteration 25/1000 | Loss: 0.00002305
Iteration 26/1000 | Loss: 0.00002300
Iteration 27/1000 | Loss: 0.00002300
Iteration 28/1000 | Loss: 0.00002300
Iteration 29/1000 | Loss: 0.00002300
Iteration 30/1000 | Loss: 0.00002300
Iteration 31/1000 | Loss: 0.00002300
Iteration 32/1000 | Loss: 0.00002300
Iteration 33/1000 | Loss: 0.00002300
Iteration 34/1000 | Loss: 0.00002300
Iteration 35/1000 | Loss: 0.00002299
Iteration 36/1000 | Loss: 0.00002299
Iteration 37/1000 | Loss: 0.00002299
Iteration 38/1000 | Loss: 0.00002299
Iteration 39/1000 | Loss: 0.00002299
Iteration 40/1000 | Loss: 0.00002297
Iteration 41/1000 | Loss: 0.00002297
Iteration 42/1000 | Loss: 0.00002297
Iteration 43/1000 | Loss: 0.00002297
Iteration 44/1000 | Loss: 0.00002297
Iteration 45/1000 | Loss: 0.00002297
Iteration 46/1000 | Loss: 0.00002297
Iteration 47/1000 | Loss: 0.00002297
Iteration 48/1000 | Loss: 0.00002297
Iteration 49/1000 | Loss: 0.00002297
Iteration 50/1000 | Loss: 0.00002296
Iteration 51/1000 | Loss: 0.00002296
Iteration 52/1000 | Loss: 0.00002295
Iteration 53/1000 | Loss: 0.00002295
Iteration 54/1000 | Loss: 0.00002294
Iteration 55/1000 | Loss: 0.00002293
Iteration 56/1000 | Loss: 0.00002293
Iteration 57/1000 | Loss: 0.00002293
Iteration 58/1000 | Loss: 0.00002292
Iteration 59/1000 | Loss: 0.00002290
Iteration 60/1000 | Loss: 0.00002289
Iteration 61/1000 | Loss: 0.00002289
Iteration 62/1000 | Loss: 0.00002289
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002287
Iteration 65/1000 | Loss: 0.00002287
Iteration 66/1000 | Loss: 0.00002287
Iteration 67/1000 | Loss: 0.00002287
Iteration 68/1000 | Loss: 0.00002287
Iteration 69/1000 | Loss: 0.00002286
Iteration 70/1000 | Loss: 0.00002285
Iteration 71/1000 | Loss: 0.00002285
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002283
Iteration 75/1000 | Loss: 0.00002283
Iteration 76/1000 | Loss: 0.00002283
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002282
Iteration 83/1000 | Loss: 0.00002282
Iteration 84/1000 | Loss: 0.00002282
Iteration 85/1000 | Loss: 0.00002282
Iteration 86/1000 | Loss: 0.00002282
Iteration 87/1000 | Loss: 0.00002282
Iteration 88/1000 | Loss: 0.00002282
Iteration 89/1000 | Loss: 0.00002281
Iteration 90/1000 | Loss: 0.00002281
Iteration 91/1000 | Loss: 0.00002281
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002280
Iteration 94/1000 | Loss: 0.00002280
Iteration 95/1000 | Loss: 0.00002279
Iteration 96/1000 | Loss: 0.00002279
Iteration 97/1000 | Loss: 0.00002279
Iteration 98/1000 | Loss: 0.00002279
Iteration 99/1000 | Loss: 0.00002278
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002278
Iteration 102/1000 | Loss: 0.00002278
Iteration 103/1000 | Loss: 0.00002278
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00002277
Iteration 107/1000 | Loss: 0.00002277
Iteration 108/1000 | Loss: 0.00002277
Iteration 109/1000 | Loss: 0.00002276
Iteration 110/1000 | Loss: 0.00002276
Iteration 111/1000 | Loss: 0.00002276
Iteration 112/1000 | Loss: 0.00002276
Iteration 113/1000 | Loss: 0.00002276
Iteration 114/1000 | Loss: 0.00002276
Iteration 115/1000 | Loss: 0.00002276
Iteration 116/1000 | Loss: 0.00002276
Iteration 117/1000 | Loss: 0.00002275
Iteration 118/1000 | Loss: 0.00002275
Iteration 119/1000 | Loss: 0.00002275
Iteration 120/1000 | Loss: 0.00002275
Iteration 121/1000 | Loss: 0.00002275
Iteration 122/1000 | Loss: 0.00002275
Iteration 123/1000 | Loss: 0.00002275
Iteration 124/1000 | Loss: 0.00002275
Iteration 125/1000 | Loss: 0.00002275
Iteration 126/1000 | Loss: 0.00002275
Iteration 127/1000 | Loss: 0.00002275
Iteration 128/1000 | Loss: 0.00002275
Iteration 129/1000 | Loss: 0.00002275
Iteration 130/1000 | Loss: 0.00002275
Iteration 131/1000 | Loss: 0.00002275
Iteration 132/1000 | Loss: 0.00002275
Iteration 133/1000 | Loss: 0.00002275
Iteration 134/1000 | Loss: 0.00002275
Iteration 135/1000 | Loss: 0.00002275
Iteration 136/1000 | Loss: 0.00002275
Iteration 137/1000 | Loss: 0.00002275
Iteration 138/1000 | Loss: 0.00002275
Iteration 139/1000 | Loss: 0.00002275
Iteration 140/1000 | Loss: 0.00002275
Iteration 141/1000 | Loss: 0.00002275
Iteration 142/1000 | Loss: 0.00002275
Iteration 143/1000 | Loss: 0.00002275
Iteration 144/1000 | Loss: 0.00002275
Iteration 145/1000 | Loss: 0.00002275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.2754622477805242e-05, 2.2754622477805242e-05, 2.2754622477805242e-05, 2.2754622477805242e-05, 2.2754622477805242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2754622477805242e-05

Optimization complete. Final v2v error: 3.6761178970336914 mm

Highest mean error: 5.564332008361816 mm for frame 149

Lowest mean error: 2.406618595123291 mm for frame 8

Saving results

Total time: 40.72551870346069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433584
Iteration 2/25 | Loss: 0.00105333
Iteration 3/25 | Loss: 0.00094984
Iteration 4/25 | Loss: 0.00092780
Iteration 5/25 | Loss: 0.00092291
Iteration 6/25 | Loss: 0.00092215
Iteration 7/25 | Loss: 0.00092214
Iteration 8/25 | Loss: 0.00092214
Iteration 9/25 | Loss: 0.00092214
Iteration 10/25 | Loss: 0.00092214
Iteration 11/25 | Loss: 0.00092214
Iteration 12/25 | Loss: 0.00092214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009221375803463161, 0.0009221375803463161, 0.0009221375803463161, 0.0009221375803463161, 0.0009221375803463161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009221375803463161

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35170102
Iteration 2/25 | Loss: 0.00063298
Iteration 3/25 | Loss: 0.00063298
Iteration 4/25 | Loss: 0.00063298
Iteration 5/25 | Loss: 0.00063298
Iteration 6/25 | Loss: 0.00063298
Iteration 7/25 | Loss: 0.00063298
Iteration 8/25 | Loss: 0.00063298
Iteration 9/25 | Loss: 0.00063298
Iteration 10/25 | Loss: 0.00063298
Iteration 11/25 | Loss: 0.00063298
Iteration 12/25 | Loss: 0.00063298
Iteration 13/25 | Loss: 0.00063298
Iteration 14/25 | Loss: 0.00063298
Iteration 15/25 | Loss: 0.00063298
Iteration 16/25 | Loss: 0.00063298
Iteration 17/25 | Loss: 0.00063298
Iteration 18/25 | Loss: 0.00063298
Iteration 19/25 | Loss: 0.00063298
Iteration 20/25 | Loss: 0.00063298
Iteration 21/25 | Loss: 0.00063298
Iteration 22/25 | Loss: 0.00063298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006329770549200475, 0.0006329770549200475, 0.0006329770549200475, 0.0006329770549200475, 0.0006329770549200475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006329770549200475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063298
Iteration 2/1000 | Loss: 0.00002351
Iteration 3/1000 | Loss: 0.00001407
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001212
Iteration 6/1000 | Loss: 0.00001164
Iteration 7/1000 | Loss: 0.00001118
Iteration 8/1000 | Loss: 0.00001106
Iteration 9/1000 | Loss: 0.00001106
Iteration 10/1000 | Loss: 0.00001106
Iteration 11/1000 | Loss: 0.00001095
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001089
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001086
Iteration 16/1000 | Loss: 0.00001080
Iteration 17/1000 | Loss: 0.00001079
Iteration 18/1000 | Loss: 0.00001078
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001076
Iteration 22/1000 | Loss: 0.00001075
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001071
Iteration 25/1000 | Loss: 0.00001071
Iteration 26/1000 | Loss: 0.00001070
Iteration 27/1000 | Loss: 0.00001070
Iteration 28/1000 | Loss: 0.00001069
Iteration 29/1000 | Loss: 0.00001069
Iteration 30/1000 | Loss: 0.00001069
Iteration 31/1000 | Loss: 0.00001069
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001068
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001068
Iteration 37/1000 | Loss: 0.00001068
Iteration 38/1000 | Loss: 0.00001068
Iteration 39/1000 | Loss: 0.00001068
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001068
Iteration 42/1000 | Loss: 0.00001068
Iteration 43/1000 | Loss: 0.00001068
Iteration 44/1000 | Loss: 0.00001068
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001068
Iteration 47/1000 | Loss: 0.00001068
Iteration 48/1000 | Loss: 0.00001068
Iteration 49/1000 | Loss: 0.00001068
Iteration 50/1000 | Loss: 0.00001068
Iteration 51/1000 | Loss: 0.00001068
Iteration 52/1000 | Loss: 0.00001068
Iteration 53/1000 | Loss: 0.00001068
Iteration 54/1000 | Loss: 0.00001068
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001068
Iteration 60/1000 | Loss: 0.00001068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.0681088497221936e-05, 1.0681088497221936e-05, 1.0681088497221936e-05, 1.0681088497221936e-05, 1.0681088497221936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0681088497221936e-05

Optimization complete. Final v2v error: 2.7982141971588135 mm

Highest mean error: 2.9447402954101562 mm for frame 101

Lowest mean error: 2.5398337841033936 mm for frame 131

Saving results

Total time: 24.42760419845581
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00347503
Iteration 2/25 | Loss: 0.00096583
Iteration 3/25 | Loss: 0.00088960
Iteration 4/25 | Loss: 0.00088260
Iteration 5/25 | Loss: 0.00088005
Iteration 6/25 | Loss: 0.00087961
Iteration 7/25 | Loss: 0.00087961
Iteration 8/25 | Loss: 0.00087961
Iteration 9/25 | Loss: 0.00087961
Iteration 10/25 | Loss: 0.00087961
Iteration 11/25 | Loss: 0.00087961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008796119946055114, 0.0008796119946055114, 0.0008796119946055114, 0.0008796119946055114, 0.0008796119946055114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008796119946055114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34758520
Iteration 2/25 | Loss: 0.00063257
Iteration 3/25 | Loss: 0.00063257
Iteration 4/25 | Loss: 0.00063257
Iteration 5/25 | Loss: 0.00063257
Iteration 6/25 | Loss: 0.00063257
Iteration 7/25 | Loss: 0.00063257
Iteration 8/25 | Loss: 0.00063257
Iteration 9/25 | Loss: 0.00063257
Iteration 10/25 | Loss: 0.00063257
Iteration 11/25 | Loss: 0.00063257
Iteration 12/25 | Loss: 0.00063257
Iteration 13/25 | Loss: 0.00063257
Iteration 14/25 | Loss: 0.00063257
Iteration 15/25 | Loss: 0.00063257
Iteration 16/25 | Loss: 0.00063257
Iteration 17/25 | Loss: 0.00063257
Iteration 18/25 | Loss: 0.00063257
Iteration 19/25 | Loss: 0.00063257
Iteration 20/25 | Loss: 0.00063257
Iteration 21/25 | Loss: 0.00063257
Iteration 22/25 | Loss: 0.00063257
Iteration 23/25 | Loss: 0.00063257
Iteration 24/25 | Loss: 0.00063257
Iteration 25/25 | Loss: 0.00063257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063257
Iteration 2/1000 | Loss: 0.00001532
Iteration 3/1000 | Loss: 0.00000854
Iteration 4/1000 | Loss: 0.00000763
Iteration 5/1000 | Loss: 0.00000707
Iteration 6/1000 | Loss: 0.00000700
Iteration 7/1000 | Loss: 0.00000680
Iteration 8/1000 | Loss: 0.00000663
Iteration 9/1000 | Loss: 0.00000663
Iteration 10/1000 | Loss: 0.00000662
Iteration 11/1000 | Loss: 0.00000661
Iteration 12/1000 | Loss: 0.00000661
Iteration 13/1000 | Loss: 0.00000660
Iteration 14/1000 | Loss: 0.00000660
Iteration 15/1000 | Loss: 0.00000660
Iteration 16/1000 | Loss: 0.00000660
Iteration 17/1000 | Loss: 0.00000659
Iteration 18/1000 | Loss: 0.00000659
Iteration 19/1000 | Loss: 0.00000659
Iteration 20/1000 | Loss: 0.00000658
Iteration 21/1000 | Loss: 0.00000658
Iteration 22/1000 | Loss: 0.00000658
Iteration 23/1000 | Loss: 0.00000657
Iteration 24/1000 | Loss: 0.00000655
Iteration 25/1000 | Loss: 0.00000655
Iteration 26/1000 | Loss: 0.00000655
Iteration 27/1000 | Loss: 0.00000655
Iteration 28/1000 | Loss: 0.00000655
Iteration 29/1000 | Loss: 0.00000654
Iteration 30/1000 | Loss: 0.00000654
Iteration 31/1000 | Loss: 0.00000654
Iteration 32/1000 | Loss: 0.00000654
Iteration 33/1000 | Loss: 0.00000654
Iteration 34/1000 | Loss: 0.00000654
Iteration 35/1000 | Loss: 0.00000653
Iteration 36/1000 | Loss: 0.00000653
Iteration 37/1000 | Loss: 0.00000653
Iteration 38/1000 | Loss: 0.00000652
Iteration 39/1000 | Loss: 0.00000652
Iteration 40/1000 | Loss: 0.00000652
Iteration 41/1000 | Loss: 0.00000651
Iteration 42/1000 | Loss: 0.00000651
Iteration 43/1000 | Loss: 0.00000649
Iteration 44/1000 | Loss: 0.00000649
Iteration 45/1000 | Loss: 0.00000648
Iteration 46/1000 | Loss: 0.00000648
Iteration 47/1000 | Loss: 0.00000648
Iteration 48/1000 | Loss: 0.00000648
Iteration 49/1000 | Loss: 0.00000648
Iteration 50/1000 | Loss: 0.00000648
Iteration 51/1000 | Loss: 0.00000647
Iteration 52/1000 | Loss: 0.00000647
Iteration 53/1000 | Loss: 0.00000647
Iteration 54/1000 | Loss: 0.00000647
Iteration 55/1000 | Loss: 0.00000646
Iteration 56/1000 | Loss: 0.00000645
Iteration 57/1000 | Loss: 0.00000645
Iteration 58/1000 | Loss: 0.00000644
Iteration 59/1000 | Loss: 0.00000644
Iteration 60/1000 | Loss: 0.00000644
Iteration 61/1000 | Loss: 0.00000644
Iteration 62/1000 | Loss: 0.00000644
Iteration 63/1000 | Loss: 0.00000644
Iteration 64/1000 | Loss: 0.00000644
Iteration 65/1000 | Loss: 0.00000644
Iteration 66/1000 | Loss: 0.00000644
Iteration 67/1000 | Loss: 0.00000643
Iteration 68/1000 | Loss: 0.00000643
Iteration 69/1000 | Loss: 0.00000643
Iteration 70/1000 | Loss: 0.00000643
Iteration 71/1000 | Loss: 0.00000643
Iteration 72/1000 | Loss: 0.00000643
Iteration 73/1000 | Loss: 0.00000643
Iteration 74/1000 | Loss: 0.00000642
Iteration 75/1000 | Loss: 0.00000642
Iteration 76/1000 | Loss: 0.00000642
Iteration 77/1000 | Loss: 0.00000642
Iteration 78/1000 | Loss: 0.00000642
Iteration 79/1000 | Loss: 0.00000641
Iteration 80/1000 | Loss: 0.00000641
Iteration 81/1000 | Loss: 0.00000641
Iteration 82/1000 | Loss: 0.00000641
Iteration 83/1000 | Loss: 0.00000641
Iteration 84/1000 | Loss: 0.00000641
Iteration 85/1000 | Loss: 0.00000641
Iteration 86/1000 | Loss: 0.00000641
Iteration 87/1000 | Loss: 0.00000640
Iteration 88/1000 | Loss: 0.00000640
Iteration 89/1000 | Loss: 0.00000640
Iteration 90/1000 | Loss: 0.00000640
Iteration 91/1000 | Loss: 0.00000640
Iteration 92/1000 | Loss: 0.00000640
Iteration 93/1000 | Loss: 0.00000640
Iteration 94/1000 | Loss: 0.00000640
Iteration 95/1000 | Loss: 0.00000640
Iteration 96/1000 | Loss: 0.00000640
Iteration 97/1000 | Loss: 0.00000640
Iteration 98/1000 | Loss: 0.00000640
Iteration 99/1000 | Loss: 0.00000639
Iteration 100/1000 | Loss: 0.00000639
Iteration 101/1000 | Loss: 0.00000639
Iteration 102/1000 | Loss: 0.00000637
Iteration 103/1000 | Loss: 0.00000637
Iteration 104/1000 | Loss: 0.00000637
Iteration 105/1000 | Loss: 0.00000637
Iteration 106/1000 | Loss: 0.00000637
Iteration 107/1000 | Loss: 0.00000637
Iteration 108/1000 | Loss: 0.00000637
Iteration 109/1000 | Loss: 0.00000637
Iteration 110/1000 | Loss: 0.00000637
Iteration 111/1000 | Loss: 0.00000637
Iteration 112/1000 | Loss: 0.00000636
Iteration 113/1000 | Loss: 0.00000636
Iteration 114/1000 | Loss: 0.00000636
Iteration 115/1000 | Loss: 0.00000636
Iteration 116/1000 | Loss: 0.00000636
Iteration 117/1000 | Loss: 0.00000635
Iteration 118/1000 | Loss: 0.00000635
Iteration 119/1000 | Loss: 0.00000634
Iteration 120/1000 | Loss: 0.00000634
Iteration 121/1000 | Loss: 0.00000634
Iteration 122/1000 | Loss: 0.00000634
Iteration 123/1000 | Loss: 0.00000634
Iteration 124/1000 | Loss: 0.00000634
Iteration 125/1000 | Loss: 0.00000634
Iteration 126/1000 | Loss: 0.00000633
Iteration 127/1000 | Loss: 0.00000633
Iteration 128/1000 | Loss: 0.00000633
Iteration 129/1000 | Loss: 0.00000633
Iteration 130/1000 | Loss: 0.00000632
Iteration 131/1000 | Loss: 0.00000632
Iteration 132/1000 | Loss: 0.00000632
Iteration 133/1000 | Loss: 0.00000632
Iteration 134/1000 | Loss: 0.00000632
Iteration 135/1000 | Loss: 0.00000632
Iteration 136/1000 | Loss: 0.00000631
Iteration 137/1000 | Loss: 0.00000631
Iteration 138/1000 | Loss: 0.00000631
Iteration 139/1000 | Loss: 0.00000631
Iteration 140/1000 | Loss: 0.00000631
Iteration 141/1000 | Loss: 0.00000631
Iteration 142/1000 | Loss: 0.00000631
Iteration 143/1000 | Loss: 0.00000631
Iteration 144/1000 | Loss: 0.00000631
Iteration 145/1000 | Loss: 0.00000631
Iteration 146/1000 | Loss: 0.00000631
Iteration 147/1000 | Loss: 0.00000631
Iteration 148/1000 | Loss: 0.00000631
Iteration 149/1000 | Loss: 0.00000631
Iteration 150/1000 | Loss: 0.00000631
Iteration 151/1000 | Loss: 0.00000631
Iteration 152/1000 | Loss: 0.00000630
Iteration 153/1000 | Loss: 0.00000630
Iteration 154/1000 | Loss: 0.00000630
Iteration 155/1000 | Loss: 0.00000630
Iteration 156/1000 | Loss: 0.00000630
Iteration 157/1000 | Loss: 0.00000630
Iteration 158/1000 | Loss: 0.00000630
Iteration 159/1000 | Loss: 0.00000630
Iteration 160/1000 | Loss: 0.00000630
Iteration 161/1000 | Loss: 0.00000630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [6.304062026174506e-06, 6.304062026174506e-06, 6.304062026174506e-06, 6.304062026174506e-06, 6.304062026174506e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.304062026174506e-06

Optimization complete. Final v2v error: 2.166841506958008 mm

Highest mean error: 2.247976541519165 mm for frame 130

Lowest mean error: 2.132748603820801 mm for frame 32

Saving results

Total time: 29.020007610321045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075399
Iteration 2/25 | Loss: 0.00195107
Iteration 3/25 | Loss: 0.00141807
Iteration 4/25 | Loss: 0.00106684
Iteration 5/25 | Loss: 0.00102745
Iteration 6/25 | Loss: 0.00102439
Iteration 7/25 | Loss: 0.00103032
Iteration 8/25 | Loss: 0.00103702
Iteration 9/25 | Loss: 0.00102327
Iteration 10/25 | Loss: 0.00101634
Iteration 11/25 | Loss: 0.00100945
Iteration 12/25 | Loss: 0.00100041
Iteration 13/25 | Loss: 0.00099688
Iteration 14/25 | Loss: 0.00099527
Iteration 15/25 | Loss: 0.00099480
Iteration 16/25 | Loss: 0.00099463
Iteration 17/25 | Loss: 0.00099460
Iteration 18/25 | Loss: 0.00099460
Iteration 19/25 | Loss: 0.00099460
Iteration 20/25 | Loss: 0.00099460
Iteration 21/25 | Loss: 0.00099460
Iteration 22/25 | Loss: 0.00099460
Iteration 23/25 | Loss: 0.00099460
Iteration 24/25 | Loss: 0.00099460
Iteration 25/25 | Loss: 0.00099459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45367098
Iteration 2/25 | Loss: 0.00069591
Iteration 3/25 | Loss: 0.00069591
Iteration 4/25 | Loss: 0.00069591
Iteration 5/25 | Loss: 0.00069591
Iteration 6/25 | Loss: 0.00069590
Iteration 7/25 | Loss: 0.00069590
Iteration 8/25 | Loss: 0.00069590
Iteration 9/25 | Loss: 0.00069590
Iteration 10/25 | Loss: 0.00069590
Iteration 11/25 | Loss: 0.00069590
Iteration 12/25 | Loss: 0.00069590
Iteration 13/25 | Loss: 0.00069590
Iteration 14/25 | Loss: 0.00069590
Iteration 15/25 | Loss: 0.00069590
Iteration 16/25 | Loss: 0.00069590
Iteration 17/25 | Loss: 0.00069590
Iteration 18/25 | Loss: 0.00069590
Iteration 19/25 | Loss: 0.00069590
Iteration 20/25 | Loss: 0.00069590
Iteration 21/25 | Loss: 0.00069590
Iteration 22/25 | Loss: 0.00069590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006959032034501433, 0.0006959032034501433, 0.0006959032034501433, 0.0006959032034501433, 0.0006959032034501433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006959032034501433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069590
Iteration 2/1000 | Loss: 0.00002614
Iteration 3/1000 | Loss: 0.00001850
Iteration 4/1000 | Loss: 0.00001655
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001390
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001378
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001372
Iteration 27/1000 | Loss: 0.00001372
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001368
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00001365
Iteration 43/1000 | Loss: 0.00001365
Iteration 44/1000 | Loss: 0.00001365
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00001363
Iteration 51/1000 | Loss: 0.00001363
Iteration 52/1000 | Loss: 0.00001363
Iteration 53/1000 | Loss: 0.00001363
Iteration 54/1000 | Loss: 0.00001363
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001362
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001362
Iteration 62/1000 | Loss: 0.00001362
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.3623974155052565e-05, 1.3623974155052565e-05, 1.3623974155052565e-05, 1.3623974155052565e-05, 1.3623974155052565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3623974155052565e-05

Optimization complete. Final v2v error: 3.087327241897583 mm

Highest mean error: 3.527686834335327 mm for frame 1

Lowest mean error: 2.6200478076934814 mm for frame 55

Saving results

Total time: 49.73862051963806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049269
Iteration 2/25 | Loss: 0.00420926
Iteration 3/25 | Loss: 0.00264292
Iteration 4/25 | Loss: 0.00229475
Iteration 5/25 | Loss: 0.00212181
Iteration 6/25 | Loss: 0.00184023
Iteration 7/25 | Loss: 0.00181180
Iteration 8/25 | Loss: 0.00190857
Iteration 9/25 | Loss: 0.00174705
Iteration 10/25 | Loss: 0.00181796
Iteration 11/25 | Loss: 0.00170329
Iteration 12/25 | Loss: 0.00171056
Iteration 13/25 | Loss: 0.00171190
Iteration 14/25 | Loss: 0.00163914
Iteration 15/25 | Loss: 0.00159748
Iteration 16/25 | Loss: 0.00156924
Iteration 17/25 | Loss: 0.00155402
Iteration 18/25 | Loss: 0.00154839
Iteration 19/25 | Loss: 0.00154790
Iteration 20/25 | Loss: 0.00154622
Iteration 21/25 | Loss: 0.00154497
Iteration 22/25 | Loss: 0.00154202
Iteration 23/25 | Loss: 0.00153821
Iteration 24/25 | Loss: 0.00153619
Iteration 25/25 | Loss: 0.00153360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30855691
Iteration 2/25 | Loss: 0.00756014
Iteration 3/25 | Loss: 0.00486158
Iteration 4/25 | Loss: 0.00486158
Iteration 5/25 | Loss: 0.00486158
Iteration 6/25 | Loss: 0.00486158
Iteration 7/25 | Loss: 0.00486158
Iteration 8/25 | Loss: 0.00486158
Iteration 9/25 | Loss: 0.00486158
Iteration 10/25 | Loss: 0.00486158
Iteration 11/25 | Loss: 0.00486158
Iteration 12/25 | Loss: 0.00486158
Iteration 13/25 | Loss: 0.00486158
Iteration 14/25 | Loss: 0.00486158
Iteration 15/25 | Loss: 0.00486158
Iteration 16/25 | Loss: 0.00486158
Iteration 17/25 | Loss: 0.00486158
Iteration 18/25 | Loss: 0.00486158
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004861578345298767, 0.004861578345298767, 0.004861578345298767, 0.004861578345298767, 0.004861578345298767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004861578345298767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00486158
Iteration 2/1000 | Loss: 0.00394259
Iteration 3/1000 | Loss: 0.00750246
Iteration 4/1000 | Loss: 0.00291507
Iteration 5/1000 | Loss: 0.00109484
Iteration 6/1000 | Loss: 0.00070334
Iteration 7/1000 | Loss: 0.00170627
Iteration 8/1000 | Loss: 0.00227746
Iteration 9/1000 | Loss: 0.00210942
Iteration 10/1000 | Loss: 0.00099546
Iteration 11/1000 | Loss: 0.00130708
Iteration 12/1000 | Loss: 0.00167639
Iteration 13/1000 | Loss: 0.00091718
Iteration 14/1000 | Loss: 0.00274963
Iteration 15/1000 | Loss: 0.00048491
Iteration 16/1000 | Loss: 0.00028142
Iteration 17/1000 | Loss: 0.00055205
Iteration 18/1000 | Loss: 0.00037314
Iteration 19/1000 | Loss: 0.00071875
Iteration 20/1000 | Loss: 0.00029924
Iteration 21/1000 | Loss: 0.00072091
Iteration 22/1000 | Loss: 0.00059290
Iteration 23/1000 | Loss: 0.00128423
Iteration 24/1000 | Loss: 0.00054099
Iteration 25/1000 | Loss: 0.00059664
Iteration 26/1000 | Loss: 0.00038689
Iteration 27/1000 | Loss: 0.00061702
Iteration 28/1000 | Loss: 0.00159312
Iteration 29/1000 | Loss: 0.00086143
Iteration 30/1000 | Loss: 0.00083081
Iteration 31/1000 | Loss: 0.00064654
Iteration 32/1000 | Loss: 0.00040511
Iteration 33/1000 | Loss: 0.00050907
Iteration 34/1000 | Loss: 0.00022293
Iteration 35/1000 | Loss: 0.00037725
Iteration 36/1000 | Loss: 0.00031758
Iteration 37/1000 | Loss: 0.00037341
Iteration 38/1000 | Loss: 0.00048378
Iteration 39/1000 | Loss: 0.00143370
Iteration 40/1000 | Loss: 0.00080027
Iteration 41/1000 | Loss: 0.00025989
Iteration 42/1000 | Loss: 0.00025213
Iteration 43/1000 | Loss: 0.00041733
Iteration 44/1000 | Loss: 0.00024952
Iteration 45/1000 | Loss: 0.00028155
Iteration 46/1000 | Loss: 0.00029384
Iteration 47/1000 | Loss: 0.00015629
Iteration 48/1000 | Loss: 0.00013815
Iteration 49/1000 | Loss: 0.00062577
Iteration 50/1000 | Loss: 0.00165500
Iteration 51/1000 | Loss: 0.00176851
Iteration 52/1000 | Loss: 0.00126423
Iteration 53/1000 | Loss: 0.00254969
Iteration 54/1000 | Loss: 0.00325534
Iteration 55/1000 | Loss: 0.00047954
Iteration 56/1000 | Loss: 0.00214168
Iteration 57/1000 | Loss: 0.00024176
Iteration 58/1000 | Loss: 0.00015946
Iteration 59/1000 | Loss: 0.00018433
Iteration 60/1000 | Loss: 0.00013099
Iteration 61/1000 | Loss: 0.00016585
Iteration 62/1000 | Loss: 0.00012385
Iteration 63/1000 | Loss: 0.00022824
Iteration 64/1000 | Loss: 0.00101076
Iteration 65/1000 | Loss: 0.00046860
Iteration 66/1000 | Loss: 0.00070371
Iteration 67/1000 | Loss: 0.00089496
Iteration 68/1000 | Loss: 0.00046530
Iteration 69/1000 | Loss: 0.00019210
Iteration 70/1000 | Loss: 0.00013460
Iteration 71/1000 | Loss: 0.00014642
Iteration 72/1000 | Loss: 0.00011427
Iteration 73/1000 | Loss: 0.00011117
Iteration 74/1000 | Loss: 0.00019308
Iteration 75/1000 | Loss: 0.00041327
Iteration 76/1000 | Loss: 0.00247056
Iteration 77/1000 | Loss: 0.00189292
Iteration 78/1000 | Loss: 0.00312602
Iteration 79/1000 | Loss: 0.00282778
Iteration 80/1000 | Loss: 0.00506275
Iteration 81/1000 | Loss: 0.00449571
Iteration 82/1000 | Loss: 0.00036052
Iteration 83/1000 | Loss: 0.00205907
Iteration 84/1000 | Loss: 0.00101999
Iteration 85/1000 | Loss: 0.00033058
Iteration 86/1000 | Loss: 0.00028596
Iteration 87/1000 | Loss: 0.00199691
Iteration 88/1000 | Loss: 0.00261617
Iteration 89/1000 | Loss: 0.00324898
Iteration 90/1000 | Loss: 0.00096429
Iteration 91/1000 | Loss: 0.00224732
Iteration 92/1000 | Loss: 0.00243607
Iteration 93/1000 | Loss: 0.00263071
Iteration 94/1000 | Loss: 0.00212918
Iteration 95/1000 | Loss: 0.00232541
Iteration 96/1000 | Loss: 0.00113074
Iteration 97/1000 | Loss: 0.00176204
Iteration 98/1000 | Loss: 0.00161093
Iteration 99/1000 | Loss: 0.00074978
Iteration 100/1000 | Loss: 0.00048733
Iteration 101/1000 | Loss: 0.00075856
Iteration 102/1000 | Loss: 0.00010967
Iteration 103/1000 | Loss: 0.00042390
Iteration 104/1000 | Loss: 0.00018958
Iteration 105/1000 | Loss: 0.00008495
Iteration 106/1000 | Loss: 0.00007423
Iteration 107/1000 | Loss: 0.00120123
Iteration 108/1000 | Loss: 0.00043704
Iteration 109/1000 | Loss: 0.00129315
Iteration 110/1000 | Loss: 0.00038407
Iteration 111/1000 | Loss: 0.00008506
Iteration 112/1000 | Loss: 0.00006071
Iteration 113/1000 | Loss: 0.00004729
Iteration 114/1000 | Loss: 0.00003848
Iteration 115/1000 | Loss: 0.00003542
Iteration 116/1000 | Loss: 0.00003245
Iteration 117/1000 | Loss: 0.00003092
Iteration 118/1000 | Loss: 0.00002956
Iteration 119/1000 | Loss: 0.00002868
Iteration 120/1000 | Loss: 0.00016823
Iteration 121/1000 | Loss: 0.00002779
Iteration 122/1000 | Loss: 0.00002728
Iteration 123/1000 | Loss: 0.00002697
Iteration 124/1000 | Loss: 0.00002665
Iteration 125/1000 | Loss: 0.00002638
Iteration 126/1000 | Loss: 0.00002618
Iteration 127/1000 | Loss: 0.00002614
Iteration 128/1000 | Loss: 0.00011811
Iteration 129/1000 | Loss: 0.00002634
Iteration 130/1000 | Loss: 0.00002591
Iteration 131/1000 | Loss: 0.00002577
Iteration 132/1000 | Loss: 0.00002574
Iteration 133/1000 | Loss: 0.00002574
Iteration 134/1000 | Loss: 0.00002574
Iteration 135/1000 | Loss: 0.00002573
Iteration 136/1000 | Loss: 0.00002573
Iteration 137/1000 | Loss: 0.00002572
Iteration 138/1000 | Loss: 0.00002563
Iteration 139/1000 | Loss: 0.00002562
Iteration 140/1000 | Loss: 0.00002562
Iteration 141/1000 | Loss: 0.00002561
Iteration 142/1000 | Loss: 0.00002554
Iteration 143/1000 | Loss: 0.00002554
Iteration 144/1000 | Loss: 0.00002553
Iteration 145/1000 | Loss: 0.00002546
Iteration 146/1000 | Loss: 0.00002543
Iteration 147/1000 | Loss: 0.00002542
Iteration 148/1000 | Loss: 0.00002542
Iteration 149/1000 | Loss: 0.00002541
Iteration 150/1000 | Loss: 0.00002541
Iteration 151/1000 | Loss: 0.00002541
Iteration 152/1000 | Loss: 0.00002541
Iteration 153/1000 | Loss: 0.00002540
Iteration 154/1000 | Loss: 0.00002540
Iteration 155/1000 | Loss: 0.00002540
Iteration 156/1000 | Loss: 0.00002540
Iteration 157/1000 | Loss: 0.00002540
Iteration 158/1000 | Loss: 0.00002540
Iteration 159/1000 | Loss: 0.00002540
Iteration 160/1000 | Loss: 0.00002539
Iteration 161/1000 | Loss: 0.00002539
Iteration 162/1000 | Loss: 0.00002539
Iteration 163/1000 | Loss: 0.00002539
Iteration 164/1000 | Loss: 0.00002539
Iteration 165/1000 | Loss: 0.00002539
Iteration 166/1000 | Loss: 0.00002538
Iteration 167/1000 | Loss: 0.00002538
Iteration 168/1000 | Loss: 0.00002538
Iteration 169/1000 | Loss: 0.00002538
Iteration 170/1000 | Loss: 0.00002538
Iteration 171/1000 | Loss: 0.00002538
Iteration 172/1000 | Loss: 0.00002537
Iteration 173/1000 | Loss: 0.00002537
Iteration 174/1000 | Loss: 0.00002537
Iteration 175/1000 | Loss: 0.00002536
Iteration 176/1000 | Loss: 0.00002536
Iteration 177/1000 | Loss: 0.00002535
Iteration 178/1000 | Loss: 0.00002533
Iteration 179/1000 | Loss: 0.00002533
Iteration 180/1000 | Loss: 0.00021219
Iteration 181/1000 | Loss: 0.00021219
Iteration 182/1000 | Loss: 0.00089541
Iteration 183/1000 | Loss: 0.00126871
Iteration 184/1000 | Loss: 0.00018868
Iteration 185/1000 | Loss: 0.00006605
Iteration 186/1000 | Loss: 0.00005352
Iteration 187/1000 | Loss: 0.00028185
Iteration 188/1000 | Loss: 0.00004443
Iteration 189/1000 | Loss: 0.00003579
Iteration 190/1000 | Loss: 0.00003265
Iteration 191/1000 | Loss: 0.00005354
Iteration 192/1000 | Loss: 0.00003121
Iteration 193/1000 | Loss: 0.00003044
Iteration 194/1000 | Loss: 0.00070399
Iteration 195/1000 | Loss: 0.00037513
Iteration 196/1000 | Loss: 0.00035244
Iteration 197/1000 | Loss: 0.00003188
Iteration 198/1000 | Loss: 0.00023285
Iteration 199/1000 | Loss: 0.00050216
Iteration 200/1000 | Loss: 0.00071053
Iteration 201/1000 | Loss: 0.00003012
Iteration 202/1000 | Loss: 0.00006244
Iteration 203/1000 | Loss: 0.00002550
Iteration 204/1000 | Loss: 0.00002452
Iteration 205/1000 | Loss: 0.00002356
Iteration 206/1000 | Loss: 0.00002315
Iteration 207/1000 | Loss: 0.00002281
Iteration 208/1000 | Loss: 0.00002277
Iteration 209/1000 | Loss: 0.00002257
Iteration 210/1000 | Loss: 0.00002242
Iteration 211/1000 | Loss: 0.00002240
Iteration 212/1000 | Loss: 0.00002238
Iteration 213/1000 | Loss: 0.00002237
Iteration 214/1000 | Loss: 0.00002235
Iteration 215/1000 | Loss: 0.00002232
Iteration 216/1000 | Loss: 0.00002229
Iteration 217/1000 | Loss: 0.00002229
Iteration 218/1000 | Loss: 0.00002227
Iteration 219/1000 | Loss: 0.00002225
Iteration 220/1000 | Loss: 0.00002211
Iteration 221/1000 | Loss: 0.00002205
Iteration 222/1000 | Loss: 0.00002193
Iteration 223/1000 | Loss: 0.00002193
Iteration 224/1000 | Loss: 0.00003577
Iteration 225/1000 | Loss: 0.00017317
Iteration 226/1000 | Loss: 0.00002671
Iteration 227/1000 | Loss: 0.00002278
Iteration 228/1000 | Loss: 0.00002221
Iteration 229/1000 | Loss: 0.00002176
Iteration 230/1000 | Loss: 0.00002154
Iteration 231/1000 | Loss: 0.00002138
Iteration 232/1000 | Loss: 0.00002133
Iteration 233/1000 | Loss: 0.00002132
Iteration 234/1000 | Loss: 0.00002131
Iteration 235/1000 | Loss: 0.00002130
Iteration 236/1000 | Loss: 0.00002130
Iteration 237/1000 | Loss: 0.00002130
Iteration 238/1000 | Loss: 0.00002128
Iteration 239/1000 | Loss: 0.00002126
Iteration 240/1000 | Loss: 0.00002126
Iteration 241/1000 | Loss: 0.00002126
Iteration 242/1000 | Loss: 0.00002125
Iteration 243/1000 | Loss: 0.00002125
Iteration 244/1000 | Loss: 0.00002124
Iteration 245/1000 | Loss: 0.00002124
Iteration 246/1000 | Loss: 0.00002123
Iteration 247/1000 | Loss: 0.00002122
Iteration 248/1000 | Loss: 0.00002122
Iteration 249/1000 | Loss: 0.00002121
Iteration 250/1000 | Loss: 0.00002121
Iteration 251/1000 | Loss: 0.00002121
Iteration 252/1000 | Loss: 0.00002120
Iteration 253/1000 | Loss: 0.00002120
Iteration 254/1000 | Loss: 0.00002120
Iteration 255/1000 | Loss: 0.00002119
Iteration 256/1000 | Loss: 0.00002119
Iteration 257/1000 | Loss: 0.00002119
Iteration 258/1000 | Loss: 0.00002119
Iteration 259/1000 | Loss: 0.00002119
Iteration 260/1000 | Loss: 0.00002119
Iteration 261/1000 | Loss: 0.00002118
Iteration 262/1000 | Loss: 0.00002118
Iteration 263/1000 | Loss: 0.00002118
Iteration 264/1000 | Loss: 0.00002118
Iteration 265/1000 | Loss: 0.00002118
Iteration 266/1000 | Loss: 0.00002118
Iteration 267/1000 | Loss: 0.00002118
Iteration 268/1000 | Loss: 0.00002117
Iteration 269/1000 | Loss: 0.00002117
Iteration 270/1000 | Loss: 0.00002117
Iteration 271/1000 | Loss: 0.00002115
Iteration 272/1000 | Loss: 0.00002115
Iteration 273/1000 | Loss: 0.00002115
Iteration 274/1000 | Loss: 0.00002114
Iteration 275/1000 | Loss: 0.00002114
Iteration 276/1000 | Loss: 0.00002114
Iteration 277/1000 | Loss: 0.00002114
Iteration 278/1000 | Loss: 0.00002113
Iteration 279/1000 | Loss: 0.00002113
Iteration 280/1000 | Loss: 0.00002113
Iteration 281/1000 | Loss: 0.00002113
Iteration 282/1000 | Loss: 0.00002112
Iteration 283/1000 | Loss: 0.00002112
Iteration 284/1000 | Loss: 0.00002112
Iteration 285/1000 | Loss: 0.00002112
Iteration 286/1000 | Loss: 0.00002112
Iteration 287/1000 | Loss: 0.00002112
Iteration 288/1000 | Loss: 0.00002111
Iteration 289/1000 | Loss: 0.00002111
Iteration 290/1000 | Loss: 0.00002111
Iteration 291/1000 | Loss: 0.00002111
Iteration 292/1000 | Loss: 0.00002111
Iteration 293/1000 | Loss: 0.00002111
Iteration 294/1000 | Loss: 0.00002111
Iteration 295/1000 | Loss: 0.00002111
Iteration 296/1000 | Loss: 0.00002111
Iteration 297/1000 | Loss: 0.00002111
Iteration 298/1000 | Loss: 0.00002111
Iteration 299/1000 | Loss: 0.00002111
Iteration 300/1000 | Loss: 0.00002110
Iteration 301/1000 | Loss: 0.00002110
Iteration 302/1000 | Loss: 0.00002110
Iteration 303/1000 | Loss: 0.00002110
Iteration 304/1000 | Loss: 0.00002110
Iteration 305/1000 | Loss: 0.00002110
Iteration 306/1000 | Loss: 0.00002110
Iteration 307/1000 | Loss: 0.00002110
Iteration 308/1000 | Loss: 0.00002110
Iteration 309/1000 | Loss: 0.00002110
Iteration 310/1000 | Loss: 0.00002110
Iteration 311/1000 | Loss: 0.00002110
Iteration 312/1000 | Loss: 0.00002110
Iteration 313/1000 | Loss: 0.00002110
Iteration 314/1000 | Loss: 0.00002110
Iteration 315/1000 | Loss: 0.00002110
Iteration 316/1000 | Loss: 0.00002109
Iteration 317/1000 | Loss: 0.00002109
Iteration 318/1000 | Loss: 0.00002109
Iteration 319/1000 | Loss: 0.00002109
Iteration 320/1000 | Loss: 0.00002109
Iteration 321/1000 | Loss: 0.00002109
Iteration 322/1000 | Loss: 0.00002109
Iteration 323/1000 | Loss: 0.00002109
Iteration 324/1000 | Loss: 0.00002109
Iteration 325/1000 | Loss: 0.00002109
Iteration 326/1000 | Loss: 0.00002109
Iteration 327/1000 | Loss: 0.00002109
Iteration 328/1000 | Loss: 0.00002109
Iteration 329/1000 | Loss: 0.00002109
Iteration 330/1000 | Loss: 0.00002109
Iteration 331/1000 | Loss: 0.00002109
Iteration 332/1000 | Loss: 0.00002109
Iteration 333/1000 | Loss: 0.00002109
Iteration 334/1000 | Loss: 0.00002109
Iteration 335/1000 | Loss: 0.00002109
Iteration 336/1000 | Loss: 0.00002109
Iteration 337/1000 | Loss: 0.00002109
Iteration 338/1000 | Loss: 0.00002109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 338. Stopping optimization.
Last 5 losses: [2.108993794536218e-05, 2.108993794536218e-05, 2.108993794536218e-05, 2.108993794536218e-05, 2.108993794536218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.108993794536218e-05

Optimization complete. Final v2v error: 3.20096755027771 mm

Highest mean error: 12.23747444152832 mm for frame 30

Lowest mean error: 2.6977381706237793 mm for frame 102

Saving results

Total time: 291.1363775730133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00614921
Iteration 2/25 | Loss: 0.00103638
Iteration 3/25 | Loss: 0.00093791
Iteration 4/25 | Loss: 0.00092669
Iteration 5/25 | Loss: 0.00092346
Iteration 6/25 | Loss: 0.00092288
Iteration 7/25 | Loss: 0.00092288
Iteration 8/25 | Loss: 0.00092288
Iteration 9/25 | Loss: 0.00092288
Iteration 10/25 | Loss: 0.00092288
Iteration 11/25 | Loss: 0.00092288
Iteration 12/25 | Loss: 0.00092288
Iteration 13/25 | Loss: 0.00092288
Iteration 14/25 | Loss: 0.00092288
Iteration 15/25 | Loss: 0.00092288
Iteration 16/25 | Loss: 0.00092288
Iteration 17/25 | Loss: 0.00092288
Iteration 18/25 | Loss: 0.00092288
Iteration 19/25 | Loss: 0.00092288
Iteration 20/25 | Loss: 0.00092288
Iteration 21/25 | Loss: 0.00092288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009228778071701527, 0.0009228778071701527, 0.0009228778071701527, 0.0009228778071701527, 0.0009228778071701527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009228778071701527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.10665822
Iteration 2/25 | Loss: 0.00057269
Iteration 3/25 | Loss: 0.00057269
Iteration 4/25 | Loss: 0.00057269
Iteration 5/25 | Loss: 0.00057269
Iteration 6/25 | Loss: 0.00057269
Iteration 7/25 | Loss: 0.00057269
Iteration 8/25 | Loss: 0.00057269
Iteration 9/25 | Loss: 0.00057269
Iteration 10/25 | Loss: 0.00057269
Iteration 11/25 | Loss: 0.00057269
Iteration 12/25 | Loss: 0.00057269
Iteration 13/25 | Loss: 0.00057269
Iteration 14/25 | Loss: 0.00057269
Iteration 15/25 | Loss: 0.00057269
Iteration 16/25 | Loss: 0.00057269
Iteration 17/25 | Loss: 0.00057269
Iteration 18/25 | Loss: 0.00057269
Iteration 19/25 | Loss: 0.00057269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005726874223910272, 0.0005726874223910272, 0.0005726874223910272, 0.0005726874223910272, 0.0005726874223910272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005726874223910272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057269
Iteration 2/1000 | Loss: 0.00002244
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001390
Iteration 5/1000 | Loss: 0.00001278
Iteration 6/1000 | Loss: 0.00001226
Iteration 7/1000 | Loss: 0.00001183
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001149
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001144
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001139
Iteration 17/1000 | Loss: 0.00001138
Iteration 18/1000 | Loss: 0.00001137
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001136
Iteration 28/1000 | Loss: 0.00001136
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001134
Iteration 36/1000 | Loss: 0.00001134
Iteration 37/1000 | Loss: 0.00001134
Iteration 38/1000 | Loss: 0.00001133
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001132
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001131
Iteration 47/1000 | Loss: 0.00001130
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001130
Iteration 50/1000 | Loss: 0.00001130
Iteration 51/1000 | Loss: 0.00001130
Iteration 52/1000 | Loss: 0.00001129
Iteration 53/1000 | Loss: 0.00001129
Iteration 54/1000 | Loss: 0.00001129
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001129
Iteration 57/1000 | Loss: 0.00001129
Iteration 58/1000 | Loss: 0.00001128
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001128
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001127
Iteration 65/1000 | Loss: 0.00001127
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001126
Iteration 68/1000 | Loss: 0.00001126
Iteration 69/1000 | Loss: 0.00001126
Iteration 70/1000 | Loss: 0.00001125
Iteration 71/1000 | Loss: 0.00001125
Iteration 72/1000 | Loss: 0.00001125
Iteration 73/1000 | Loss: 0.00001124
Iteration 74/1000 | Loss: 0.00001124
Iteration 75/1000 | Loss: 0.00001124
Iteration 76/1000 | Loss: 0.00001124
Iteration 77/1000 | Loss: 0.00001124
Iteration 78/1000 | Loss: 0.00001124
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001123
Iteration 82/1000 | Loss: 0.00001122
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001121
Iteration 85/1000 | Loss: 0.00001121
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001121
Iteration 88/1000 | Loss: 0.00001120
Iteration 89/1000 | Loss: 0.00001120
Iteration 90/1000 | Loss: 0.00001120
Iteration 91/1000 | Loss: 0.00001120
Iteration 92/1000 | Loss: 0.00001120
Iteration 93/1000 | Loss: 0.00001120
Iteration 94/1000 | Loss: 0.00001120
Iteration 95/1000 | Loss: 0.00001120
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001120
Iteration 98/1000 | Loss: 0.00001120
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001118
Iteration 108/1000 | Loss: 0.00001118
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001117
Iteration 116/1000 | Loss: 0.00001117
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001117
Iteration 123/1000 | Loss: 0.00001117
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001116
Iteration 126/1000 | Loss: 0.00001116
Iteration 127/1000 | Loss: 0.00001116
Iteration 128/1000 | Loss: 0.00001116
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001116
Iteration 136/1000 | Loss: 0.00001116
Iteration 137/1000 | Loss: 0.00001116
Iteration 138/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.1163769158883952e-05, 1.1163769158883952e-05, 1.1163769158883952e-05, 1.1163769158883952e-05, 1.1163769158883952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1163769158883952e-05

Optimization complete. Final v2v error: 2.836179256439209 mm

Highest mean error: 3.101064920425415 mm for frame 133

Lowest mean error: 2.619683265686035 mm for frame 103

Saving results

Total time: 31.47140622138977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027352
Iteration 2/25 | Loss: 0.01027352
Iteration 3/25 | Loss: 0.00304642
Iteration 4/25 | Loss: 0.00213327
Iteration 5/25 | Loss: 0.00206112
Iteration 6/25 | Loss: 0.00205570
Iteration 7/25 | Loss: 0.00208263
Iteration 8/25 | Loss: 0.00196299
Iteration 9/25 | Loss: 0.00187579
Iteration 10/25 | Loss: 0.00186749
Iteration 11/25 | Loss: 0.00182573
Iteration 12/25 | Loss: 0.00182549
Iteration 13/25 | Loss: 0.00182062
Iteration 14/25 | Loss: 0.00180768
Iteration 15/25 | Loss: 0.00180140
Iteration 16/25 | Loss: 0.00179953
Iteration 17/25 | Loss: 0.00179875
Iteration 18/25 | Loss: 0.00179837
Iteration 19/25 | Loss: 0.00179828
Iteration 20/25 | Loss: 0.00179828
Iteration 21/25 | Loss: 0.00179827
Iteration 22/25 | Loss: 0.00179827
Iteration 23/25 | Loss: 0.00179827
Iteration 24/25 | Loss: 0.00179827
Iteration 25/25 | Loss: 0.00179827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28656530
Iteration 2/25 | Loss: 0.00575507
Iteration 3/25 | Loss: 0.00575506
Iteration 4/25 | Loss: 0.00575506
Iteration 5/25 | Loss: 0.00575506
Iteration 6/25 | Loss: 0.00575506
Iteration 7/25 | Loss: 0.00575506
Iteration 8/25 | Loss: 0.00575506
Iteration 9/25 | Loss: 0.00575506
Iteration 10/25 | Loss: 0.00575506
Iteration 11/25 | Loss: 0.00575506
Iteration 12/25 | Loss: 0.00575506
Iteration 13/25 | Loss: 0.00575506
Iteration 14/25 | Loss: 0.00575506
Iteration 15/25 | Loss: 0.00575506
Iteration 16/25 | Loss: 0.00575506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005755060818046331, 0.005755060818046331, 0.005755060818046331, 0.005755060818046331, 0.005755060818046331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005755060818046331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00575506
Iteration 2/1000 | Loss: 0.00085510
Iteration 3/1000 | Loss: 0.00066216
Iteration 4/1000 | Loss: 0.00057071
Iteration 5/1000 | Loss: 0.00050707
Iteration 6/1000 | Loss: 0.00045022
Iteration 7/1000 | Loss: 0.00041165
Iteration 8/1000 | Loss: 0.00038913
Iteration 9/1000 | Loss: 0.00037134
Iteration 10/1000 | Loss: 0.01326206
Iteration 11/1000 | Loss: 0.01295881
Iteration 12/1000 | Loss: 0.00063366
Iteration 13/1000 | Loss: 0.00042301
Iteration 14/1000 | Loss: 0.00025619
Iteration 15/1000 | Loss: 0.00014944
Iteration 16/1000 | Loss: 0.00010623
Iteration 17/1000 | Loss: 0.00007942
Iteration 18/1000 | Loss: 0.00005864
Iteration 19/1000 | Loss: 0.00004051
Iteration 20/1000 | Loss: 0.00003041
Iteration 21/1000 | Loss: 0.00002618
Iteration 22/1000 | Loss: 0.00002223
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001520
Iteration 25/1000 | Loss: 0.00001372
Iteration 26/1000 | Loss: 0.00001263
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001114
Iteration 30/1000 | Loss: 0.00001092
Iteration 31/1000 | Loss: 0.00001083
Iteration 32/1000 | Loss: 0.00001076
Iteration 33/1000 | Loss: 0.00001063
Iteration 34/1000 | Loss: 0.00001060
Iteration 35/1000 | Loss: 0.00001057
Iteration 36/1000 | Loss: 0.00001054
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001054
Iteration 39/1000 | Loss: 0.00001054
Iteration 40/1000 | Loss: 0.00001054
Iteration 41/1000 | Loss: 0.00001054
Iteration 42/1000 | Loss: 0.00001054
Iteration 43/1000 | Loss: 0.00001053
Iteration 44/1000 | Loss: 0.00001053
Iteration 45/1000 | Loss: 0.00001053
Iteration 46/1000 | Loss: 0.00001053
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001052
Iteration 49/1000 | Loss: 0.00001052
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001052
Iteration 52/1000 | Loss: 0.00001051
Iteration 53/1000 | Loss: 0.00001051
Iteration 54/1000 | Loss: 0.00001051
Iteration 55/1000 | Loss: 0.00001051
Iteration 56/1000 | Loss: 0.00001051
Iteration 57/1000 | Loss: 0.00001050
Iteration 58/1000 | Loss: 0.00001050
Iteration 59/1000 | Loss: 0.00001050
Iteration 60/1000 | Loss: 0.00001050
Iteration 61/1000 | Loss: 0.00001049
Iteration 62/1000 | Loss: 0.00001048
Iteration 63/1000 | Loss: 0.00001048
Iteration 64/1000 | Loss: 0.00001048
Iteration 65/1000 | Loss: 0.00001048
Iteration 66/1000 | Loss: 0.00001048
Iteration 67/1000 | Loss: 0.00001048
Iteration 68/1000 | Loss: 0.00001048
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001048
Iteration 74/1000 | Loss: 0.00001048
Iteration 75/1000 | Loss: 0.00001048
Iteration 76/1000 | Loss: 0.00001048
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001047
Iteration 82/1000 | Loss: 0.00001047
Iteration 83/1000 | Loss: 0.00001046
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001046
Iteration 86/1000 | Loss: 0.00001046
Iteration 87/1000 | Loss: 0.00001046
Iteration 88/1000 | Loss: 0.00001046
Iteration 89/1000 | Loss: 0.00001046
Iteration 90/1000 | Loss: 0.00001046
Iteration 91/1000 | Loss: 0.00001046
Iteration 92/1000 | Loss: 0.00001046
Iteration 93/1000 | Loss: 0.00001046
Iteration 94/1000 | Loss: 0.00001045
Iteration 95/1000 | Loss: 0.00001045
Iteration 96/1000 | Loss: 0.00001045
Iteration 97/1000 | Loss: 0.00001045
Iteration 98/1000 | Loss: 0.00001045
Iteration 99/1000 | Loss: 0.00001045
Iteration 100/1000 | Loss: 0.00001044
Iteration 101/1000 | Loss: 0.00001044
Iteration 102/1000 | Loss: 0.00001044
Iteration 103/1000 | Loss: 0.00001044
Iteration 104/1000 | Loss: 0.00001044
Iteration 105/1000 | Loss: 0.00001044
Iteration 106/1000 | Loss: 0.00001044
Iteration 107/1000 | Loss: 0.00001044
Iteration 108/1000 | Loss: 0.00001044
Iteration 109/1000 | Loss: 0.00001044
Iteration 110/1000 | Loss: 0.00001044
Iteration 111/1000 | Loss: 0.00001044
Iteration 112/1000 | Loss: 0.00001044
Iteration 113/1000 | Loss: 0.00001044
Iteration 114/1000 | Loss: 0.00001044
Iteration 115/1000 | Loss: 0.00001044
Iteration 116/1000 | Loss: 0.00001044
Iteration 117/1000 | Loss: 0.00001044
Iteration 118/1000 | Loss: 0.00001044
Iteration 119/1000 | Loss: 0.00001044
Iteration 120/1000 | Loss: 0.00001044
Iteration 121/1000 | Loss: 0.00001044
Iteration 122/1000 | Loss: 0.00001044
Iteration 123/1000 | Loss: 0.00001044
Iteration 124/1000 | Loss: 0.00001044
Iteration 125/1000 | Loss: 0.00001044
Iteration 126/1000 | Loss: 0.00001044
Iteration 127/1000 | Loss: 0.00001044
Iteration 128/1000 | Loss: 0.00001044
Iteration 129/1000 | Loss: 0.00001044
Iteration 130/1000 | Loss: 0.00001044
Iteration 131/1000 | Loss: 0.00001044
Iteration 132/1000 | Loss: 0.00001044
Iteration 133/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.0438140634505544e-05, 1.0438140634505544e-05, 1.0438140634505544e-05, 1.0438140634505544e-05, 1.0438140634505544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0438140634505544e-05

Optimization complete. Final v2v error: 2.7430179119110107 mm

Highest mean error: 2.933006525039673 mm for frame 83

Lowest mean error: 2.6600875854492188 mm for frame 151

Saving results

Total time: 92.24915266036987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093722
Iteration 2/25 | Loss: 0.00112656
Iteration 3/25 | Loss: 0.00096368
Iteration 4/25 | Loss: 0.00095560
Iteration 5/25 | Loss: 0.00095319
Iteration 6/25 | Loss: 0.00095268
Iteration 7/25 | Loss: 0.00095268
Iteration 8/25 | Loss: 0.00095268
Iteration 9/25 | Loss: 0.00095268
Iteration 10/25 | Loss: 0.00095268
Iteration 11/25 | Loss: 0.00095268
Iteration 12/25 | Loss: 0.00095268
Iteration 13/25 | Loss: 0.00095268
Iteration 14/25 | Loss: 0.00095268
Iteration 15/25 | Loss: 0.00095268
Iteration 16/25 | Loss: 0.00095268
Iteration 17/25 | Loss: 0.00095268
Iteration 18/25 | Loss: 0.00095268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009526842623017728, 0.0009526842623017728, 0.0009526842623017728, 0.0009526842623017728, 0.0009526842623017728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009526842623017728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.14534426
Iteration 2/25 | Loss: 0.00059803
Iteration 3/25 | Loss: 0.00059803
Iteration 4/25 | Loss: 0.00059803
Iteration 5/25 | Loss: 0.00059803
Iteration 6/25 | Loss: 0.00059803
Iteration 7/25 | Loss: 0.00059803
Iteration 8/25 | Loss: 0.00059803
Iteration 9/25 | Loss: 0.00059803
Iteration 10/25 | Loss: 0.00059803
Iteration 11/25 | Loss: 0.00059803
Iteration 12/25 | Loss: 0.00059803
Iteration 13/25 | Loss: 0.00059803
Iteration 14/25 | Loss: 0.00059803
Iteration 15/25 | Loss: 0.00059803
Iteration 16/25 | Loss: 0.00059803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005980309215374291, 0.0005980309215374291, 0.0005980309215374291, 0.0005980309215374291, 0.0005980309215374291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005980309215374291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059803
Iteration 2/1000 | Loss: 0.00002304
Iteration 3/1000 | Loss: 0.00001413
Iteration 4/1000 | Loss: 0.00001226
Iteration 5/1000 | Loss: 0.00001151
Iteration 6/1000 | Loss: 0.00001123
Iteration 7/1000 | Loss: 0.00001103
Iteration 8/1000 | Loss: 0.00001089
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001084
Iteration 12/1000 | Loss: 0.00001083
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001083
Iteration 15/1000 | Loss: 0.00001083
Iteration 16/1000 | Loss: 0.00001083
Iteration 17/1000 | Loss: 0.00001083
Iteration 18/1000 | Loss: 0.00001083
Iteration 19/1000 | Loss: 0.00001081
Iteration 20/1000 | Loss: 0.00001081
Iteration 21/1000 | Loss: 0.00001080
Iteration 22/1000 | Loss: 0.00001080
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001080
Iteration 25/1000 | Loss: 0.00001080
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001079
Iteration 28/1000 | Loss: 0.00001079
Iteration 29/1000 | Loss: 0.00001076
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001075
Iteration 32/1000 | Loss: 0.00001075
Iteration 33/1000 | Loss: 0.00001074
Iteration 34/1000 | Loss: 0.00001073
Iteration 35/1000 | Loss: 0.00001072
Iteration 36/1000 | Loss: 0.00001072
Iteration 37/1000 | Loss: 0.00001071
Iteration 38/1000 | Loss: 0.00001070
Iteration 39/1000 | Loss: 0.00001070
Iteration 40/1000 | Loss: 0.00001069
Iteration 41/1000 | Loss: 0.00001069
Iteration 42/1000 | Loss: 0.00001069
Iteration 43/1000 | Loss: 0.00001069
Iteration 44/1000 | Loss: 0.00001069
Iteration 45/1000 | Loss: 0.00001068
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001067
Iteration 51/1000 | Loss: 0.00001067
Iteration 52/1000 | Loss: 0.00001067
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001066
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001065
Iteration 59/1000 | Loss: 0.00001065
Iteration 60/1000 | Loss: 0.00001065
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001065
Iteration 64/1000 | Loss: 0.00001065
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001064
Iteration 68/1000 | Loss: 0.00001064
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001064
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001063
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001063
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001063
Iteration 80/1000 | Loss: 0.00001063
Iteration 81/1000 | Loss: 0.00001063
Iteration 82/1000 | Loss: 0.00001062
Iteration 83/1000 | Loss: 0.00001062
Iteration 84/1000 | Loss: 0.00001062
Iteration 85/1000 | Loss: 0.00001062
Iteration 86/1000 | Loss: 0.00001062
Iteration 87/1000 | Loss: 0.00001061
Iteration 88/1000 | Loss: 0.00001061
Iteration 89/1000 | Loss: 0.00001061
Iteration 90/1000 | Loss: 0.00001061
Iteration 91/1000 | Loss: 0.00001061
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001061
Iteration 96/1000 | Loss: 0.00001060
Iteration 97/1000 | Loss: 0.00001060
Iteration 98/1000 | Loss: 0.00001060
Iteration 99/1000 | Loss: 0.00001060
Iteration 100/1000 | Loss: 0.00001060
Iteration 101/1000 | Loss: 0.00001060
Iteration 102/1000 | Loss: 0.00001060
Iteration 103/1000 | Loss: 0.00001060
Iteration 104/1000 | Loss: 0.00001060
Iteration 105/1000 | Loss: 0.00001060
Iteration 106/1000 | Loss: 0.00001059
Iteration 107/1000 | Loss: 0.00001059
Iteration 108/1000 | Loss: 0.00001059
Iteration 109/1000 | Loss: 0.00001059
Iteration 110/1000 | Loss: 0.00001059
Iteration 111/1000 | Loss: 0.00001059
Iteration 112/1000 | Loss: 0.00001059
Iteration 113/1000 | Loss: 0.00001059
Iteration 114/1000 | Loss: 0.00001059
Iteration 115/1000 | Loss: 0.00001059
Iteration 116/1000 | Loss: 0.00001059
Iteration 117/1000 | Loss: 0.00001059
Iteration 118/1000 | Loss: 0.00001059
Iteration 119/1000 | Loss: 0.00001059
Iteration 120/1000 | Loss: 0.00001058
Iteration 121/1000 | Loss: 0.00001058
Iteration 122/1000 | Loss: 0.00001058
Iteration 123/1000 | Loss: 0.00001058
Iteration 124/1000 | Loss: 0.00001057
Iteration 125/1000 | Loss: 0.00001057
Iteration 126/1000 | Loss: 0.00001057
Iteration 127/1000 | Loss: 0.00001057
Iteration 128/1000 | Loss: 0.00001057
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001057
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001056
Iteration 141/1000 | Loss: 0.00001056
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001056
Iteration 144/1000 | Loss: 0.00001056
Iteration 145/1000 | Loss: 0.00001056
Iteration 146/1000 | Loss: 0.00001056
Iteration 147/1000 | Loss: 0.00001056
Iteration 148/1000 | Loss: 0.00001056
Iteration 149/1000 | Loss: 0.00001056
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001055
Iteration 154/1000 | Loss: 0.00001055
Iteration 155/1000 | Loss: 0.00001055
Iteration 156/1000 | Loss: 0.00001055
Iteration 157/1000 | Loss: 0.00001055
Iteration 158/1000 | Loss: 0.00001055
Iteration 159/1000 | Loss: 0.00001055
Iteration 160/1000 | Loss: 0.00001055
Iteration 161/1000 | Loss: 0.00001055
Iteration 162/1000 | Loss: 0.00001055
Iteration 163/1000 | Loss: 0.00001055
Iteration 164/1000 | Loss: 0.00001055
Iteration 165/1000 | Loss: 0.00001055
Iteration 166/1000 | Loss: 0.00001055
Iteration 167/1000 | Loss: 0.00001055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.0549656508374028e-05, 1.0549656508374028e-05, 1.0549656508374028e-05, 1.0549656508374028e-05, 1.0549656508374028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0549656508374028e-05

Optimization complete. Final v2v error: 2.7391302585601807 mm

Highest mean error: 3.0980300903320312 mm for frame 100

Lowest mean error: 2.520372152328491 mm for frame 113

Saving results

Total time: 29.735057830810547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749330
Iteration 2/25 | Loss: 0.00114414
Iteration 3/25 | Loss: 0.00095538
Iteration 4/25 | Loss: 0.00093566
Iteration 5/25 | Loss: 0.00091895
Iteration 6/25 | Loss: 0.00091687
Iteration 7/25 | Loss: 0.00091561
Iteration 8/25 | Loss: 0.00091447
Iteration 9/25 | Loss: 0.00091386
Iteration 10/25 | Loss: 0.00091326
Iteration 11/25 | Loss: 0.00091289
Iteration 12/25 | Loss: 0.00091273
Iteration 13/25 | Loss: 0.00091264
Iteration 14/25 | Loss: 0.00091264
Iteration 15/25 | Loss: 0.00091264
Iteration 16/25 | Loss: 0.00091264
Iteration 17/25 | Loss: 0.00091264
Iteration 18/25 | Loss: 0.00091264
Iteration 19/25 | Loss: 0.00091264
Iteration 20/25 | Loss: 0.00091263
Iteration 21/25 | Loss: 0.00091263
Iteration 22/25 | Loss: 0.00091263
Iteration 23/25 | Loss: 0.00091263
Iteration 24/25 | Loss: 0.00091263
Iteration 25/25 | Loss: 0.00091263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80480313
Iteration 2/25 | Loss: 0.00065848
Iteration 3/25 | Loss: 0.00065848
Iteration 4/25 | Loss: 0.00065848
Iteration 5/25 | Loss: 0.00065847
Iteration 6/25 | Loss: 0.00065847
Iteration 7/25 | Loss: 0.00065847
Iteration 8/25 | Loss: 0.00065847
Iteration 9/25 | Loss: 0.00065847
Iteration 10/25 | Loss: 0.00065847
Iteration 11/25 | Loss: 0.00065847
Iteration 12/25 | Loss: 0.00065847
Iteration 13/25 | Loss: 0.00065847
Iteration 14/25 | Loss: 0.00065847
Iteration 15/25 | Loss: 0.00065847
Iteration 16/25 | Loss: 0.00065847
Iteration 17/25 | Loss: 0.00065847
Iteration 18/25 | Loss: 0.00065847
Iteration 19/25 | Loss: 0.00065847
Iteration 20/25 | Loss: 0.00065847
Iteration 21/25 | Loss: 0.00065847
Iteration 22/25 | Loss: 0.00065847
Iteration 23/25 | Loss: 0.00065847
Iteration 24/25 | Loss: 0.00065847
Iteration 25/25 | Loss: 0.00065847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065847
Iteration 2/1000 | Loss: 0.00001574
Iteration 3/1000 | Loss: 0.00001153
Iteration 4/1000 | Loss: 0.00001055
Iteration 5/1000 | Loss: 0.00005831
Iteration 6/1000 | Loss: 0.00001007
Iteration 7/1000 | Loss: 0.00000974
Iteration 8/1000 | Loss: 0.00000973
Iteration 9/1000 | Loss: 0.00000970
Iteration 10/1000 | Loss: 0.00000957
Iteration 11/1000 | Loss: 0.00000950
Iteration 12/1000 | Loss: 0.00000949
Iteration 13/1000 | Loss: 0.00000948
Iteration 14/1000 | Loss: 0.00000940
Iteration 15/1000 | Loss: 0.00000939
Iteration 16/1000 | Loss: 0.00000939
Iteration 17/1000 | Loss: 0.00000933
Iteration 18/1000 | Loss: 0.00000932
Iteration 19/1000 | Loss: 0.00000931
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000930
Iteration 25/1000 | Loss: 0.00000929
Iteration 26/1000 | Loss: 0.00000929
Iteration 27/1000 | Loss: 0.00000928
Iteration 28/1000 | Loss: 0.00000928
Iteration 29/1000 | Loss: 0.00000927
Iteration 30/1000 | Loss: 0.00000927
Iteration 31/1000 | Loss: 0.00000927
Iteration 32/1000 | Loss: 0.00000926
Iteration 33/1000 | Loss: 0.00000925
Iteration 34/1000 | Loss: 0.00000925
Iteration 35/1000 | Loss: 0.00000924
Iteration 36/1000 | Loss: 0.00000924
Iteration 37/1000 | Loss: 0.00000924
Iteration 38/1000 | Loss: 0.00000924
Iteration 39/1000 | Loss: 0.00000924
Iteration 40/1000 | Loss: 0.00000924
Iteration 41/1000 | Loss: 0.00000924
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000924
Iteration 44/1000 | Loss: 0.00000924
Iteration 45/1000 | Loss: 0.00000924
Iteration 46/1000 | Loss: 0.00000924
Iteration 47/1000 | Loss: 0.00000924
Iteration 48/1000 | Loss: 0.00000924
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000924
Iteration 51/1000 | Loss: 0.00000924
Iteration 52/1000 | Loss: 0.00000924
Iteration 53/1000 | Loss: 0.00000924
Iteration 54/1000 | Loss: 0.00000924
Iteration 55/1000 | Loss: 0.00000924
Iteration 56/1000 | Loss: 0.00000924
Iteration 57/1000 | Loss: 0.00000924
Iteration 58/1000 | Loss: 0.00000924
Iteration 59/1000 | Loss: 0.00000924
Iteration 60/1000 | Loss: 0.00000924
Iteration 61/1000 | Loss: 0.00000924
Iteration 62/1000 | Loss: 0.00000924
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000924
Iteration 65/1000 | Loss: 0.00000924
Iteration 66/1000 | Loss: 0.00000924
Iteration 67/1000 | Loss: 0.00000924
Iteration 68/1000 | Loss: 0.00000924
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000924
Iteration 71/1000 | Loss: 0.00000924
Iteration 72/1000 | Loss: 0.00000924
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000924
Iteration 75/1000 | Loss: 0.00000924
Iteration 76/1000 | Loss: 0.00000924
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000924
Iteration 81/1000 | Loss: 0.00000924
Iteration 82/1000 | Loss: 0.00000924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [9.237426638719626e-06, 9.237426638719626e-06, 9.237426638719626e-06, 9.237426638719626e-06, 9.237426638719626e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.237426638719626e-06

Optimization complete. Final v2v error: 2.572143077850342 mm

Highest mean error: 2.8906736373901367 mm for frame 149

Lowest mean error: 2.3780038356781006 mm for frame 32

Saving results

Total time: 44.11349582672119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998951
Iteration 2/25 | Loss: 0.00163987
Iteration 3/25 | Loss: 0.00117713
Iteration 4/25 | Loss: 0.00114785
Iteration 5/25 | Loss: 0.00113961
Iteration 6/25 | Loss: 0.00113756
Iteration 7/25 | Loss: 0.00113756
Iteration 8/25 | Loss: 0.00113756
Iteration 9/25 | Loss: 0.00113756
Iteration 10/25 | Loss: 0.00113756
Iteration 11/25 | Loss: 0.00113756
Iteration 12/25 | Loss: 0.00113756
Iteration 13/25 | Loss: 0.00113756
Iteration 14/25 | Loss: 0.00113756
Iteration 15/25 | Loss: 0.00113756
Iteration 16/25 | Loss: 0.00113756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011375604663044214, 0.0011375604663044214, 0.0011375604663044214, 0.0011375604663044214, 0.0011375604663044214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011375604663044214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81808293
Iteration 2/25 | Loss: 0.00069411
Iteration 3/25 | Loss: 0.00069411
Iteration 4/25 | Loss: 0.00069411
Iteration 5/25 | Loss: 0.00069411
Iteration 6/25 | Loss: 0.00069411
Iteration 7/25 | Loss: 0.00069411
Iteration 8/25 | Loss: 0.00069411
Iteration 9/25 | Loss: 0.00069411
Iteration 10/25 | Loss: 0.00069411
Iteration 11/25 | Loss: 0.00069411
Iteration 12/25 | Loss: 0.00069411
Iteration 13/25 | Loss: 0.00069411
Iteration 14/25 | Loss: 0.00069411
Iteration 15/25 | Loss: 0.00069411
Iteration 16/25 | Loss: 0.00069411
Iteration 17/25 | Loss: 0.00069411
Iteration 18/25 | Loss: 0.00069411
Iteration 19/25 | Loss: 0.00069411
Iteration 20/25 | Loss: 0.00069411
Iteration 21/25 | Loss: 0.00069411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000694108719471842, 0.000694108719471842, 0.000694108719471842, 0.000694108719471842, 0.000694108719471842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000694108719471842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069411
Iteration 2/1000 | Loss: 0.00005772
Iteration 3/1000 | Loss: 0.00004361
Iteration 4/1000 | Loss: 0.00004141
Iteration 5/1000 | Loss: 0.00003941
Iteration 6/1000 | Loss: 0.00003845
Iteration 7/1000 | Loss: 0.00003762
Iteration 8/1000 | Loss: 0.00003702
Iteration 9/1000 | Loss: 0.00003662
Iteration 10/1000 | Loss: 0.00003624
Iteration 11/1000 | Loss: 0.00003596
Iteration 12/1000 | Loss: 0.00003573
Iteration 13/1000 | Loss: 0.00003551
Iteration 14/1000 | Loss: 0.00003526
Iteration 15/1000 | Loss: 0.00003502
Iteration 16/1000 | Loss: 0.00003483
Iteration 17/1000 | Loss: 0.00003466
Iteration 18/1000 | Loss: 0.00003451
Iteration 19/1000 | Loss: 0.00003432
Iteration 20/1000 | Loss: 0.00003428
Iteration 21/1000 | Loss: 0.00003424
Iteration 22/1000 | Loss: 0.00003423
Iteration 23/1000 | Loss: 0.00003422
Iteration 24/1000 | Loss: 0.00003421
Iteration 25/1000 | Loss: 0.00003421
Iteration 26/1000 | Loss: 0.00003416
Iteration 27/1000 | Loss: 0.00003416
Iteration 28/1000 | Loss: 0.00003414
Iteration 29/1000 | Loss: 0.00003413
Iteration 30/1000 | Loss: 0.00003412
Iteration 31/1000 | Loss: 0.00003411
Iteration 32/1000 | Loss: 0.00003411
Iteration 33/1000 | Loss: 0.00003411
Iteration 34/1000 | Loss: 0.00003411
Iteration 35/1000 | Loss: 0.00003410
Iteration 36/1000 | Loss: 0.00003410
Iteration 37/1000 | Loss: 0.00003409
Iteration 38/1000 | Loss: 0.00003409
Iteration 39/1000 | Loss: 0.00003409
Iteration 40/1000 | Loss: 0.00003408
Iteration 41/1000 | Loss: 0.00003408
Iteration 42/1000 | Loss: 0.00003407
Iteration 43/1000 | Loss: 0.00003406
Iteration 44/1000 | Loss: 0.00003406
Iteration 45/1000 | Loss: 0.00003405
Iteration 46/1000 | Loss: 0.00003405
Iteration 47/1000 | Loss: 0.00003405
Iteration 48/1000 | Loss: 0.00003405
Iteration 49/1000 | Loss: 0.00003405
Iteration 50/1000 | Loss: 0.00003405
Iteration 51/1000 | Loss: 0.00003405
Iteration 52/1000 | Loss: 0.00003405
Iteration 53/1000 | Loss: 0.00003405
Iteration 54/1000 | Loss: 0.00003404
Iteration 55/1000 | Loss: 0.00003404
Iteration 56/1000 | Loss: 0.00003404
Iteration 57/1000 | Loss: 0.00003404
Iteration 58/1000 | Loss: 0.00003404
Iteration 59/1000 | Loss: 0.00003404
Iteration 60/1000 | Loss: 0.00003404
Iteration 61/1000 | Loss: 0.00003403
Iteration 62/1000 | Loss: 0.00003403
Iteration 63/1000 | Loss: 0.00003403
Iteration 64/1000 | Loss: 0.00003402
Iteration 65/1000 | Loss: 0.00003402
Iteration 66/1000 | Loss: 0.00003402
Iteration 67/1000 | Loss: 0.00003402
Iteration 68/1000 | Loss: 0.00003402
Iteration 69/1000 | Loss: 0.00003402
Iteration 70/1000 | Loss: 0.00003402
Iteration 71/1000 | Loss: 0.00003402
Iteration 72/1000 | Loss: 0.00003402
Iteration 73/1000 | Loss: 0.00003402
Iteration 74/1000 | Loss: 0.00003402
Iteration 75/1000 | Loss: 0.00003402
Iteration 76/1000 | Loss: 0.00003401
Iteration 77/1000 | Loss: 0.00003401
Iteration 78/1000 | Loss: 0.00003401
Iteration 79/1000 | Loss: 0.00003401
Iteration 80/1000 | Loss: 0.00003400
Iteration 81/1000 | Loss: 0.00003400
Iteration 82/1000 | Loss: 0.00003400
Iteration 83/1000 | Loss: 0.00003400
Iteration 84/1000 | Loss: 0.00003400
Iteration 85/1000 | Loss: 0.00003400
Iteration 86/1000 | Loss: 0.00003400
Iteration 87/1000 | Loss: 0.00003400
Iteration 88/1000 | Loss: 0.00003400
Iteration 89/1000 | Loss: 0.00003400
Iteration 90/1000 | Loss: 0.00003400
Iteration 91/1000 | Loss: 0.00003400
Iteration 92/1000 | Loss: 0.00003399
Iteration 93/1000 | Loss: 0.00003399
Iteration 94/1000 | Loss: 0.00003399
Iteration 95/1000 | Loss: 0.00003399
Iteration 96/1000 | Loss: 0.00003399
Iteration 97/1000 | Loss: 0.00003399
Iteration 98/1000 | Loss: 0.00003399
Iteration 99/1000 | Loss: 0.00003399
Iteration 100/1000 | Loss: 0.00003399
Iteration 101/1000 | Loss: 0.00003398
Iteration 102/1000 | Loss: 0.00003397
Iteration 103/1000 | Loss: 0.00003397
Iteration 104/1000 | Loss: 0.00003397
Iteration 105/1000 | Loss: 0.00003396
Iteration 106/1000 | Loss: 0.00003396
Iteration 107/1000 | Loss: 0.00003396
Iteration 108/1000 | Loss: 0.00003396
Iteration 109/1000 | Loss: 0.00003396
Iteration 110/1000 | Loss: 0.00003396
Iteration 111/1000 | Loss: 0.00003395
Iteration 112/1000 | Loss: 0.00003395
Iteration 113/1000 | Loss: 0.00003395
Iteration 114/1000 | Loss: 0.00003395
Iteration 115/1000 | Loss: 0.00003395
Iteration 116/1000 | Loss: 0.00003395
Iteration 117/1000 | Loss: 0.00003395
Iteration 118/1000 | Loss: 0.00003394
Iteration 119/1000 | Loss: 0.00003394
Iteration 120/1000 | Loss: 0.00003394
Iteration 121/1000 | Loss: 0.00003394
Iteration 122/1000 | Loss: 0.00003394
Iteration 123/1000 | Loss: 0.00003394
Iteration 124/1000 | Loss: 0.00003394
Iteration 125/1000 | Loss: 0.00003394
Iteration 126/1000 | Loss: 0.00003394
Iteration 127/1000 | Loss: 0.00003393
Iteration 128/1000 | Loss: 0.00003393
Iteration 129/1000 | Loss: 0.00003393
Iteration 130/1000 | Loss: 0.00003393
Iteration 131/1000 | Loss: 0.00003393
Iteration 132/1000 | Loss: 0.00003393
Iteration 133/1000 | Loss: 0.00003393
Iteration 134/1000 | Loss: 0.00003393
Iteration 135/1000 | Loss: 0.00003393
Iteration 136/1000 | Loss: 0.00003393
Iteration 137/1000 | Loss: 0.00003393
Iteration 138/1000 | Loss: 0.00003393
Iteration 139/1000 | Loss: 0.00003393
Iteration 140/1000 | Loss: 0.00003393
Iteration 141/1000 | Loss: 0.00003393
Iteration 142/1000 | Loss: 0.00003393
Iteration 143/1000 | Loss: 0.00003393
Iteration 144/1000 | Loss: 0.00003393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.393331644474529e-05, 3.393331644474529e-05, 3.393331644474529e-05, 3.393331644474529e-05, 3.393331644474529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.393331644474529e-05

Optimization complete. Final v2v error: 4.661677360534668 mm

Highest mean error: 5.969383716583252 mm for frame 93

Lowest mean error: 3.363753318786621 mm for frame 0

Saving results

Total time: 52.92329216003418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851441
Iteration 2/25 | Loss: 0.00122875
Iteration 3/25 | Loss: 0.00104401
Iteration 4/25 | Loss: 0.00101166
Iteration 5/25 | Loss: 0.00099828
Iteration 6/25 | Loss: 0.00099488
Iteration 7/25 | Loss: 0.00099378
Iteration 8/25 | Loss: 0.00099333
Iteration 9/25 | Loss: 0.00099333
Iteration 10/25 | Loss: 0.00099333
Iteration 11/25 | Loss: 0.00099333
Iteration 12/25 | Loss: 0.00099333
Iteration 13/25 | Loss: 0.00099333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009933332912623882, 0.0009933332912623882, 0.0009933332912623882, 0.0009933332912623882, 0.0009933332912623882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009933332912623882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31762969
Iteration 2/25 | Loss: 0.00096239
Iteration 3/25 | Loss: 0.00096239
Iteration 4/25 | Loss: 0.00096239
Iteration 5/25 | Loss: 0.00096239
Iteration 6/25 | Loss: 0.00096239
Iteration 7/25 | Loss: 0.00096239
Iteration 8/25 | Loss: 0.00096239
Iteration 9/25 | Loss: 0.00096239
Iteration 10/25 | Loss: 0.00096239
Iteration 11/25 | Loss: 0.00096239
Iteration 12/25 | Loss: 0.00096239
Iteration 13/25 | Loss: 0.00096239
Iteration 14/25 | Loss: 0.00096239
Iteration 15/25 | Loss: 0.00096239
Iteration 16/25 | Loss: 0.00096239
Iteration 17/25 | Loss: 0.00096239
Iteration 18/25 | Loss: 0.00096239
Iteration 19/25 | Loss: 0.00096239
Iteration 20/25 | Loss: 0.00096239
Iteration 21/25 | Loss: 0.00096239
Iteration 22/25 | Loss: 0.00096239
Iteration 23/25 | Loss: 0.00096239
Iteration 24/25 | Loss: 0.00096239
Iteration 25/25 | Loss: 0.00096239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096239
Iteration 2/1000 | Loss: 0.00004741
Iteration 3/1000 | Loss: 0.00002672
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001899
Iteration 6/1000 | Loss: 0.00001812
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001699
Iteration 10/1000 | Loss: 0.00001676
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001644
Iteration 15/1000 | Loss: 0.00001644
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001643
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001638
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001636
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001636
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001635
Iteration 39/1000 | Loss: 0.00001634
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001632
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001628
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001624
Iteration 77/1000 | Loss: 0.00001624
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001621
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001621
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001619
Iteration 95/1000 | Loss: 0.00001619
Iteration 96/1000 | Loss: 0.00001618
Iteration 97/1000 | Loss: 0.00001618
Iteration 98/1000 | Loss: 0.00001618
Iteration 99/1000 | Loss: 0.00001618
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001617
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001616
Iteration 107/1000 | Loss: 0.00001616
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001616
Iteration 111/1000 | Loss: 0.00001616
Iteration 112/1000 | Loss: 0.00001616
Iteration 113/1000 | Loss: 0.00001616
Iteration 114/1000 | Loss: 0.00001615
Iteration 115/1000 | Loss: 0.00001615
Iteration 116/1000 | Loss: 0.00001615
Iteration 117/1000 | Loss: 0.00001615
Iteration 118/1000 | Loss: 0.00001615
Iteration 119/1000 | Loss: 0.00001615
Iteration 120/1000 | Loss: 0.00001615
Iteration 121/1000 | Loss: 0.00001615
Iteration 122/1000 | Loss: 0.00001615
Iteration 123/1000 | Loss: 0.00001615
Iteration 124/1000 | Loss: 0.00001614
Iteration 125/1000 | Loss: 0.00001614
Iteration 126/1000 | Loss: 0.00001614
Iteration 127/1000 | Loss: 0.00001614
Iteration 128/1000 | Loss: 0.00001614
Iteration 129/1000 | Loss: 0.00001614
Iteration 130/1000 | Loss: 0.00001614
Iteration 131/1000 | Loss: 0.00001614
Iteration 132/1000 | Loss: 0.00001613
Iteration 133/1000 | Loss: 0.00001613
Iteration 134/1000 | Loss: 0.00001613
Iteration 135/1000 | Loss: 0.00001613
Iteration 136/1000 | Loss: 0.00001613
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001613
Iteration 139/1000 | Loss: 0.00001613
Iteration 140/1000 | Loss: 0.00001613
Iteration 141/1000 | Loss: 0.00001613
Iteration 142/1000 | Loss: 0.00001613
Iteration 143/1000 | Loss: 0.00001613
Iteration 144/1000 | Loss: 0.00001612
Iteration 145/1000 | Loss: 0.00001612
Iteration 146/1000 | Loss: 0.00001612
Iteration 147/1000 | Loss: 0.00001612
Iteration 148/1000 | Loss: 0.00001612
Iteration 149/1000 | Loss: 0.00001612
Iteration 150/1000 | Loss: 0.00001612
Iteration 151/1000 | Loss: 0.00001612
Iteration 152/1000 | Loss: 0.00001611
Iteration 153/1000 | Loss: 0.00001611
Iteration 154/1000 | Loss: 0.00001611
Iteration 155/1000 | Loss: 0.00001611
Iteration 156/1000 | Loss: 0.00001611
Iteration 157/1000 | Loss: 0.00001610
Iteration 158/1000 | Loss: 0.00001610
Iteration 159/1000 | Loss: 0.00001610
Iteration 160/1000 | Loss: 0.00001610
Iteration 161/1000 | Loss: 0.00001610
Iteration 162/1000 | Loss: 0.00001610
Iteration 163/1000 | Loss: 0.00001610
Iteration 164/1000 | Loss: 0.00001610
Iteration 165/1000 | Loss: 0.00001609
Iteration 166/1000 | Loss: 0.00001609
Iteration 167/1000 | Loss: 0.00001609
Iteration 168/1000 | Loss: 0.00001609
Iteration 169/1000 | Loss: 0.00001609
Iteration 170/1000 | Loss: 0.00001609
Iteration 171/1000 | Loss: 0.00001609
Iteration 172/1000 | Loss: 0.00001609
Iteration 173/1000 | Loss: 0.00001609
Iteration 174/1000 | Loss: 0.00001609
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001608
Iteration 178/1000 | Loss: 0.00001608
Iteration 179/1000 | Loss: 0.00001608
Iteration 180/1000 | Loss: 0.00001607
Iteration 181/1000 | Loss: 0.00001607
Iteration 182/1000 | Loss: 0.00001607
Iteration 183/1000 | Loss: 0.00001607
Iteration 184/1000 | Loss: 0.00001607
Iteration 185/1000 | Loss: 0.00001607
Iteration 186/1000 | Loss: 0.00001607
Iteration 187/1000 | Loss: 0.00001607
Iteration 188/1000 | Loss: 0.00001607
Iteration 189/1000 | Loss: 0.00001607
Iteration 190/1000 | Loss: 0.00001607
Iteration 191/1000 | Loss: 0.00001607
Iteration 192/1000 | Loss: 0.00001607
Iteration 193/1000 | Loss: 0.00001606
Iteration 194/1000 | Loss: 0.00001606
Iteration 195/1000 | Loss: 0.00001606
Iteration 196/1000 | Loss: 0.00001606
Iteration 197/1000 | Loss: 0.00001606
Iteration 198/1000 | Loss: 0.00001606
Iteration 199/1000 | Loss: 0.00001606
Iteration 200/1000 | Loss: 0.00001606
Iteration 201/1000 | Loss: 0.00001606
Iteration 202/1000 | Loss: 0.00001606
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001606
Iteration 205/1000 | Loss: 0.00001606
Iteration 206/1000 | Loss: 0.00001605
Iteration 207/1000 | Loss: 0.00001605
Iteration 208/1000 | Loss: 0.00001605
Iteration 209/1000 | Loss: 0.00001605
Iteration 210/1000 | Loss: 0.00001605
Iteration 211/1000 | Loss: 0.00001605
Iteration 212/1000 | Loss: 0.00001605
Iteration 213/1000 | Loss: 0.00001605
Iteration 214/1000 | Loss: 0.00001605
Iteration 215/1000 | Loss: 0.00001605
Iteration 216/1000 | Loss: 0.00001605
Iteration 217/1000 | Loss: 0.00001605
Iteration 218/1000 | Loss: 0.00001605
Iteration 219/1000 | Loss: 0.00001605
Iteration 220/1000 | Loss: 0.00001605
Iteration 221/1000 | Loss: 0.00001605
Iteration 222/1000 | Loss: 0.00001605
Iteration 223/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.604978388058953e-05, 1.604978388058953e-05, 1.604978388058953e-05, 1.604978388058953e-05, 1.604978388058953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.604978388058953e-05

Optimization complete. Final v2v error: 3.369823694229126 mm

Highest mean error: 4.489513397216797 mm for frame 58

Lowest mean error: 3.1095879077911377 mm for frame 97

Saving results

Total time: 42.16868710517883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00658456
Iteration 2/25 | Loss: 0.00138332
Iteration 3/25 | Loss: 0.00115271
Iteration 4/25 | Loss: 0.00113063
Iteration 5/25 | Loss: 0.00112750
Iteration 6/25 | Loss: 0.00112720
Iteration 7/25 | Loss: 0.00112720
Iteration 8/25 | Loss: 0.00112720
Iteration 9/25 | Loss: 0.00112720
Iteration 10/25 | Loss: 0.00112720
Iteration 11/25 | Loss: 0.00112720
Iteration 12/25 | Loss: 0.00112720
Iteration 13/25 | Loss: 0.00112720
Iteration 14/25 | Loss: 0.00112720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001127202296629548, 0.001127202296629548, 0.001127202296629548, 0.001127202296629548, 0.001127202296629548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001127202296629548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62291509
Iteration 2/25 | Loss: 0.00053221
Iteration 3/25 | Loss: 0.00053221
Iteration 4/25 | Loss: 0.00053221
Iteration 5/25 | Loss: 0.00053221
Iteration 6/25 | Loss: 0.00053221
Iteration 7/25 | Loss: 0.00053221
Iteration 8/25 | Loss: 0.00053221
Iteration 9/25 | Loss: 0.00053221
Iteration 10/25 | Loss: 0.00053221
Iteration 11/25 | Loss: 0.00053221
Iteration 12/25 | Loss: 0.00053221
Iteration 13/25 | Loss: 0.00053221
Iteration 14/25 | Loss: 0.00053221
Iteration 15/25 | Loss: 0.00053221
Iteration 16/25 | Loss: 0.00053221
Iteration 17/25 | Loss: 0.00053221
Iteration 18/25 | Loss: 0.00053221
Iteration 19/25 | Loss: 0.00053221
Iteration 20/25 | Loss: 0.00053221
Iteration 21/25 | Loss: 0.00053221
Iteration 22/25 | Loss: 0.00053221
Iteration 23/25 | Loss: 0.00053221
Iteration 24/25 | Loss: 0.00053221
Iteration 25/25 | Loss: 0.00053221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053221
Iteration 2/1000 | Loss: 0.00004675
Iteration 3/1000 | Loss: 0.00003528
Iteration 4/1000 | Loss: 0.00003236
Iteration 5/1000 | Loss: 0.00003147
Iteration 6/1000 | Loss: 0.00003092
Iteration 7/1000 | Loss: 0.00003062
Iteration 8/1000 | Loss: 0.00003040
Iteration 9/1000 | Loss: 0.00003016
Iteration 10/1000 | Loss: 0.00003006
Iteration 11/1000 | Loss: 0.00003005
Iteration 12/1000 | Loss: 0.00003000
Iteration 13/1000 | Loss: 0.00002991
Iteration 14/1000 | Loss: 0.00002975
Iteration 15/1000 | Loss: 0.00002960
Iteration 16/1000 | Loss: 0.00002945
Iteration 17/1000 | Loss: 0.00002940
Iteration 18/1000 | Loss: 0.00002923
Iteration 19/1000 | Loss: 0.00002923
Iteration 20/1000 | Loss: 0.00002921
Iteration 21/1000 | Loss: 0.00002905
Iteration 22/1000 | Loss: 0.00002901
Iteration 23/1000 | Loss: 0.00002900
Iteration 24/1000 | Loss: 0.00002896
Iteration 25/1000 | Loss: 0.00002895
Iteration 26/1000 | Loss: 0.00002895
Iteration 27/1000 | Loss: 0.00002894
Iteration 28/1000 | Loss: 0.00002894
Iteration 29/1000 | Loss: 0.00002892
Iteration 30/1000 | Loss: 0.00002892
Iteration 31/1000 | Loss: 0.00002891
Iteration 32/1000 | Loss: 0.00002890
Iteration 33/1000 | Loss: 0.00002890
Iteration 34/1000 | Loss: 0.00002882
Iteration 35/1000 | Loss: 0.00002881
Iteration 36/1000 | Loss: 0.00002877
Iteration 37/1000 | Loss: 0.00002877
Iteration 38/1000 | Loss: 0.00002875
Iteration 39/1000 | Loss: 0.00002875
Iteration 40/1000 | Loss: 0.00002872
Iteration 41/1000 | Loss: 0.00002871
Iteration 42/1000 | Loss: 0.00002870
Iteration 43/1000 | Loss: 0.00002870
Iteration 44/1000 | Loss: 0.00002869
Iteration 45/1000 | Loss: 0.00002867
Iteration 46/1000 | Loss: 0.00002867
Iteration 47/1000 | Loss: 0.00002866
Iteration 48/1000 | Loss: 0.00002866
Iteration 49/1000 | Loss: 0.00002866
Iteration 50/1000 | Loss: 0.00002866
Iteration 51/1000 | Loss: 0.00002865
Iteration 52/1000 | Loss: 0.00002865
Iteration 53/1000 | Loss: 0.00002862
Iteration 54/1000 | Loss: 0.00002861
Iteration 55/1000 | Loss: 0.00002860
Iteration 56/1000 | Loss: 0.00002857
Iteration 57/1000 | Loss: 0.00002857
Iteration 58/1000 | Loss: 0.00002855
Iteration 59/1000 | Loss: 0.00002855
Iteration 60/1000 | Loss: 0.00002855
Iteration 61/1000 | Loss: 0.00002855
Iteration 62/1000 | Loss: 0.00002855
Iteration 63/1000 | Loss: 0.00002855
Iteration 64/1000 | Loss: 0.00002854
Iteration 65/1000 | Loss: 0.00002854
Iteration 66/1000 | Loss: 0.00002854
Iteration 67/1000 | Loss: 0.00002853
Iteration 68/1000 | Loss: 0.00002852
Iteration 69/1000 | Loss: 0.00002852
Iteration 70/1000 | Loss: 0.00002852
Iteration 71/1000 | Loss: 0.00002852
Iteration 72/1000 | Loss: 0.00002852
Iteration 73/1000 | Loss: 0.00002852
Iteration 74/1000 | Loss: 0.00002852
Iteration 75/1000 | Loss: 0.00002852
Iteration 76/1000 | Loss: 0.00002851
Iteration 77/1000 | Loss: 0.00002851
Iteration 78/1000 | Loss: 0.00002851
Iteration 79/1000 | Loss: 0.00002851
Iteration 80/1000 | Loss: 0.00002851
Iteration 81/1000 | Loss: 0.00002851
Iteration 82/1000 | Loss: 0.00002851
Iteration 83/1000 | Loss: 0.00002851
Iteration 84/1000 | Loss: 0.00002851
Iteration 85/1000 | Loss: 0.00002850
Iteration 86/1000 | Loss: 0.00002850
Iteration 87/1000 | Loss: 0.00002850
Iteration 88/1000 | Loss: 0.00002850
Iteration 89/1000 | Loss: 0.00002850
Iteration 90/1000 | Loss: 0.00002850
Iteration 91/1000 | Loss: 0.00002850
Iteration 92/1000 | Loss: 0.00002849
Iteration 93/1000 | Loss: 0.00002849
Iteration 94/1000 | Loss: 0.00002849
Iteration 95/1000 | Loss: 0.00002849
Iteration 96/1000 | Loss: 0.00002849
Iteration 97/1000 | Loss: 0.00002849
Iteration 98/1000 | Loss: 0.00002848
Iteration 99/1000 | Loss: 0.00002848
Iteration 100/1000 | Loss: 0.00002847
Iteration 101/1000 | Loss: 0.00002847
Iteration 102/1000 | Loss: 0.00002847
Iteration 103/1000 | Loss: 0.00002847
Iteration 104/1000 | Loss: 0.00002847
Iteration 105/1000 | Loss: 0.00002847
Iteration 106/1000 | Loss: 0.00002847
Iteration 107/1000 | Loss: 0.00002846
Iteration 108/1000 | Loss: 0.00002846
Iteration 109/1000 | Loss: 0.00002846
Iteration 110/1000 | Loss: 0.00002846
Iteration 111/1000 | Loss: 0.00002845
Iteration 112/1000 | Loss: 0.00002845
Iteration 113/1000 | Loss: 0.00002845
Iteration 114/1000 | Loss: 0.00002845
Iteration 115/1000 | Loss: 0.00002845
Iteration 116/1000 | Loss: 0.00002845
Iteration 117/1000 | Loss: 0.00002845
Iteration 118/1000 | Loss: 0.00002845
Iteration 119/1000 | Loss: 0.00002845
Iteration 120/1000 | Loss: 0.00002845
Iteration 121/1000 | Loss: 0.00002845
Iteration 122/1000 | Loss: 0.00002845
Iteration 123/1000 | Loss: 0.00002844
Iteration 124/1000 | Loss: 0.00002844
Iteration 125/1000 | Loss: 0.00002844
Iteration 126/1000 | Loss: 0.00002844
Iteration 127/1000 | Loss: 0.00002844
Iteration 128/1000 | Loss: 0.00002844
Iteration 129/1000 | Loss: 0.00002844
Iteration 130/1000 | Loss: 0.00002844
Iteration 131/1000 | Loss: 0.00002844
Iteration 132/1000 | Loss: 0.00002843
Iteration 133/1000 | Loss: 0.00002843
Iteration 134/1000 | Loss: 0.00002843
Iteration 135/1000 | Loss: 0.00002843
Iteration 136/1000 | Loss: 0.00002843
Iteration 137/1000 | Loss: 0.00002843
Iteration 138/1000 | Loss: 0.00002843
Iteration 139/1000 | Loss: 0.00002843
Iteration 140/1000 | Loss: 0.00002843
Iteration 141/1000 | Loss: 0.00002843
Iteration 142/1000 | Loss: 0.00002843
Iteration 143/1000 | Loss: 0.00002843
Iteration 144/1000 | Loss: 0.00002843
Iteration 145/1000 | Loss: 0.00002843
Iteration 146/1000 | Loss: 0.00002843
Iteration 147/1000 | Loss: 0.00002843
Iteration 148/1000 | Loss: 0.00002843
Iteration 149/1000 | Loss: 0.00002843
Iteration 150/1000 | Loss: 0.00002843
Iteration 151/1000 | Loss: 0.00002843
Iteration 152/1000 | Loss: 0.00002843
Iteration 153/1000 | Loss: 0.00002843
Iteration 154/1000 | Loss: 0.00002843
Iteration 155/1000 | Loss: 0.00002843
Iteration 156/1000 | Loss: 0.00002843
Iteration 157/1000 | Loss: 0.00002843
Iteration 158/1000 | Loss: 0.00002843
Iteration 159/1000 | Loss: 0.00002843
Iteration 160/1000 | Loss: 0.00002843
Iteration 161/1000 | Loss: 0.00002843
Iteration 162/1000 | Loss: 0.00002843
Iteration 163/1000 | Loss: 0.00002843
Iteration 164/1000 | Loss: 0.00002843
Iteration 165/1000 | Loss: 0.00002843
Iteration 166/1000 | Loss: 0.00002843
Iteration 167/1000 | Loss: 0.00002843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.8426298740669154e-05, 2.8426298740669154e-05, 2.8426298740669154e-05, 2.8426298740669154e-05, 2.8426298740669154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8426298740669154e-05

Optimization complete. Final v2v error: 4.246452808380127 mm

Highest mean error: 4.992237091064453 mm for frame 172

Lowest mean error: 4.042569160461426 mm for frame 134

Saving results

Total time: 50.470075845718384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492120
Iteration 2/25 | Loss: 0.00113366
Iteration 3/25 | Loss: 0.00101786
Iteration 4/25 | Loss: 0.00099806
Iteration 5/25 | Loss: 0.00099117
Iteration 6/25 | Loss: 0.00098960
Iteration 7/25 | Loss: 0.00098891
Iteration 8/25 | Loss: 0.00098891
Iteration 9/25 | Loss: 0.00098891
Iteration 10/25 | Loss: 0.00098891
Iteration 11/25 | Loss: 0.00098891
Iteration 12/25 | Loss: 0.00098891
Iteration 13/25 | Loss: 0.00098891
Iteration 14/25 | Loss: 0.00098891
Iteration 15/25 | Loss: 0.00098891
Iteration 16/25 | Loss: 0.00098891
Iteration 17/25 | Loss: 0.00098891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00098890601657331, 0.00098890601657331, 0.00098890601657331, 0.00098890601657331, 0.00098890601657331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00098890601657331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.24373960
Iteration 2/25 | Loss: 0.00078805
Iteration 3/25 | Loss: 0.00078804
Iteration 4/25 | Loss: 0.00078803
Iteration 5/25 | Loss: 0.00078803
Iteration 6/25 | Loss: 0.00078803
Iteration 7/25 | Loss: 0.00078803
Iteration 8/25 | Loss: 0.00078803
Iteration 9/25 | Loss: 0.00078803
Iteration 10/25 | Loss: 0.00078803
Iteration 11/25 | Loss: 0.00078803
Iteration 12/25 | Loss: 0.00078803
Iteration 13/25 | Loss: 0.00078803
Iteration 14/25 | Loss: 0.00078803
Iteration 15/25 | Loss: 0.00078803
Iteration 16/25 | Loss: 0.00078803
Iteration 17/25 | Loss: 0.00078803
Iteration 18/25 | Loss: 0.00078803
Iteration 19/25 | Loss: 0.00078803
Iteration 20/25 | Loss: 0.00078803
Iteration 21/25 | Loss: 0.00078803
Iteration 22/25 | Loss: 0.00078803
Iteration 23/25 | Loss: 0.00078803
Iteration 24/25 | Loss: 0.00078803
Iteration 25/25 | Loss: 0.00078803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078803
Iteration 2/1000 | Loss: 0.00003645
Iteration 3/1000 | Loss: 0.00002195
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001799
Iteration 6/1000 | Loss: 0.00001724
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001634
Iteration 11/1000 | Loss: 0.00001621
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001598
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001596
Iteration 26/1000 | Loss: 0.00001595
Iteration 27/1000 | Loss: 0.00001595
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001593
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001591
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001590
Iteration 37/1000 | Loss: 0.00001590
Iteration 38/1000 | Loss: 0.00001590
Iteration 39/1000 | Loss: 0.00001589
Iteration 40/1000 | Loss: 0.00001589
Iteration 41/1000 | Loss: 0.00001589
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001588
Iteration 44/1000 | Loss: 0.00001588
Iteration 45/1000 | Loss: 0.00001587
Iteration 46/1000 | Loss: 0.00001587
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001586
Iteration 49/1000 | Loss: 0.00001586
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001584
Iteration 54/1000 | Loss: 0.00001584
Iteration 55/1000 | Loss: 0.00001584
Iteration 56/1000 | Loss: 0.00001583
Iteration 57/1000 | Loss: 0.00001583
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001583
Iteration 61/1000 | Loss: 0.00001583
Iteration 62/1000 | Loss: 0.00001583
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001582
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001582
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001581
Iteration 77/1000 | Loss: 0.00001581
Iteration 78/1000 | Loss: 0.00001581
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001581
Iteration 83/1000 | Loss: 0.00001581
Iteration 84/1000 | Loss: 0.00001580
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001580
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001579
Iteration 110/1000 | Loss: 0.00001579
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001579
Iteration 115/1000 | Loss: 0.00001579
Iteration 116/1000 | Loss: 0.00001579
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001579
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001578
Iteration 131/1000 | Loss: 0.00001578
Iteration 132/1000 | Loss: 0.00001578
Iteration 133/1000 | Loss: 0.00001578
Iteration 134/1000 | Loss: 0.00001578
Iteration 135/1000 | Loss: 0.00001578
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.578092633280903e-05, 1.578092633280903e-05, 1.578092633280903e-05, 1.578092633280903e-05, 1.578092633280903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.578092633280903e-05

Optimization complete. Final v2v error: 3.3834710121154785 mm

Highest mean error: 4.231743335723877 mm for frame 17

Lowest mean error: 2.6260931491851807 mm for frame 0

Saving results

Total time: 34.606618881225586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066229
Iteration 2/25 | Loss: 0.00313092
Iteration 3/25 | Loss: 0.00205707
Iteration 4/25 | Loss: 0.00177933
Iteration 5/25 | Loss: 0.00168917
Iteration 6/25 | Loss: 0.00149768
Iteration 7/25 | Loss: 0.00134955
Iteration 8/25 | Loss: 0.00128133
Iteration 9/25 | Loss: 0.00120205
Iteration 10/25 | Loss: 0.00117026
Iteration 11/25 | Loss: 0.00115709
Iteration 12/25 | Loss: 0.00113721
Iteration 13/25 | Loss: 0.00112754
Iteration 14/25 | Loss: 0.00112125
Iteration 15/25 | Loss: 0.00112111
Iteration 16/25 | Loss: 0.00111583
Iteration 17/25 | Loss: 0.00111310
Iteration 18/25 | Loss: 0.00111246
Iteration 19/25 | Loss: 0.00111234
Iteration 20/25 | Loss: 0.00111277
Iteration 21/25 | Loss: 0.00111316
Iteration 22/25 | Loss: 0.00111275
Iteration 23/25 | Loss: 0.00111369
Iteration 24/25 | Loss: 0.00110887
Iteration 25/25 | Loss: 0.00110780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32955980
Iteration 2/25 | Loss: 0.00236354
Iteration 3/25 | Loss: 0.00191190
Iteration 4/25 | Loss: 0.00191190
Iteration 5/25 | Loss: 0.00191190
Iteration 6/25 | Loss: 0.00191190
Iteration 7/25 | Loss: 0.00191190
Iteration 8/25 | Loss: 0.00191190
Iteration 9/25 | Loss: 0.00191190
Iteration 10/25 | Loss: 0.00191190
Iteration 11/25 | Loss: 0.00191190
Iteration 12/25 | Loss: 0.00191190
Iteration 13/25 | Loss: 0.00191190
Iteration 14/25 | Loss: 0.00191190
Iteration 15/25 | Loss: 0.00191190
Iteration 16/25 | Loss: 0.00191190
Iteration 17/25 | Loss: 0.00191190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019118950003758073, 0.0019118950003758073, 0.0019118950003758073, 0.0019118950003758073, 0.0019118950003758073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019118950003758073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191190
Iteration 2/1000 | Loss: 0.00062684
Iteration 3/1000 | Loss: 0.00024576
Iteration 4/1000 | Loss: 0.00042786
Iteration 5/1000 | Loss: 0.00039821
Iteration 6/1000 | Loss: 0.00022280
Iteration 7/1000 | Loss: 0.00022973
Iteration 8/1000 | Loss: 0.00013019
Iteration 9/1000 | Loss: 0.00012159
Iteration 10/1000 | Loss: 0.00288328
Iteration 11/1000 | Loss: 0.00091127
Iteration 12/1000 | Loss: 0.00237708
Iteration 13/1000 | Loss: 0.00047838
Iteration 14/1000 | Loss: 0.00026108
Iteration 15/1000 | Loss: 0.00031051
Iteration 16/1000 | Loss: 0.00013562
Iteration 17/1000 | Loss: 0.00007427
Iteration 18/1000 | Loss: 0.00011165
Iteration 19/1000 | Loss: 0.00004306
Iteration 20/1000 | Loss: 0.00008362
Iteration 21/1000 | Loss: 0.00059003
Iteration 22/1000 | Loss: 0.00024339
Iteration 23/1000 | Loss: 0.00022578
Iteration 24/1000 | Loss: 0.00003525
Iteration 25/1000 | Loss: 0.00006256
Iteration 26/1000 | Loss: 0.00003369
Iteration 27/1000 | Loss: 0.00003754
Iteration 28/1000 | Loss: 0.00003088
Iteration 29/1000 | Loss: 0.00003007
Iteration 30/1000 | Loss: 0.00002954
Iteration 31/1000 | Loss: 0.00003994
Iteration 32/1000 | Loss: 0.00007655
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002731
Iteration 35/1000 | Loss: 0.00002449
Iteration 36/1000 | Loss: 0.00003613
Iteration 37/1000 | Loss: 0.00002403
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002438
Iteration 40/1000 | Loss: 0.00002438
Iteration 41/1000 | Loss: 0.00002295
Iteration 42/1000 | Loss: 0.00002164
Iteration 43/1000 | Loss: 0.00002164
Iteration 44/1000 | Loss: 0.00002224
Iteration 45/1000 | Loss: 0.00002155
Iteration 46/1000 | Loss: 0.00002155
Iteration 47/1000 | Loss: 0.00002155
Iteration 48/1000 | Loss: 0.00002155
Iteration 49/1000 | Loss: 0.00002155
Iteration 50/1000 | Loss: 0.00002155
Iteration 51/1000 | Loss: 0.00002155
Iteration 52/1000 | Loss: 0.00002155
Iteration 53/1000 | Loss: 0.00002155
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002153
Iteration 56/1000 | Loss: 0.00002152
Iteration 57/1000 | Loss: 0.00002151
Iteration 58/1000 | Loss: 0.00002151
Iteration 59/1000 | Loss: 0.00002150
Iteration 60/1000 | Loss: 0.00002150
Iteration 61/1000 | Loss: 0.00002150
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002149
Iteration 65/1000 | Loss: 0.00002149
Iteration 66/1000 | Loss: 0.00002149
Iteration 67/1000 | Loss: 0.00002148
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002148
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00002148
Iteration 72/1000 | Loss: 0.00002148
Iteration 73/1000 | Loss: 0.00002148
Iteration 74/1000 | Loss: 0.00002147
Iteration 75/1000 | Loss: 0.00002146
Iteration 76/1000 | Loss: 0.00002146
Iteration 77/1000 | Loss: 0.00002146
Iteration 78/1000 | Loss: 0.00002146
Iteration 79/1000 | Loss: 0.00002146
Iteration 80/1000 | Loss: 0.00002145
Iteration 81/1000 | Loss: 0.00002145
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002145
Iteration 87/1000 | Loss: 0.00002145
Iteration 88/1000 | Loss: 0.00002145
Iteration 89/1000 | Loss: 0.00002145
Iteration 90/1000 | Loss: 0.00002144
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002143
Iteration 95/1000 | Loss: 0.00002143
Iteration 96/1000 | Loss: 0.00002143
Iteration 97/1000 | Loss: 0.00002143
Iteration 98/1000 | Loss: 0.00002143
Iteration 99/1000 | Loss: 0.00002143
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002143
Iteration 104/1000 | Loss: 0.00002142
Iteration 105/1000 | Loss: 0.00002142
Iteration 106/1000 | Loss: 0.00002142
Iteration 107/1000 | Loss: 0.00002142
Iteration 108/1000 | Loss: 0.00002142
Iteration 109/1000 | Loss: 0.00002142
Iteration 110/1000 | Loss: 0.00002142
Iteration 111/1000 | Loss: 0.00002142
Iteration 112/1000 | Loss: 0.00002141
Iteration 113/1000 | Loss: 0.00002141
Iteration 114/1000 | Loss: 0.00002141
Iteration 115/1000 | Loss: 0.00002141
Iteration 116/1000 | Loss: 0.00002141
Iteration 117/1000 | Loss: 0.00002141
Iteration 118/1000 | Loss: 0.00002141
Iteration 119/1000 | Loss: 0.00002141
Iteration 120/1000 | Loss: 0.00002141
Iteration 121/1000 | Loss: 0.00002140
Iteration 122/1000 | Loss: 0.00002140
Iteration 123/1000 | Loss: 0.00002140
Iteration 124/1000 | Loss: 0.00002140
Iteration 125/1000 | Loss: 0.00002140
Iteration 126/1000 | Loss: 0.00002140
Iteration 127/1000 | Loss: 0.00002140
Iteration 128/1000 | Loss: 0.00002140
Iteration 129/1000 | Loss: 0.00002140
Iteration 130/1000 | Loss: 0.00002140
Iteration 131/1000 | Loss: 0.00002140
Iteration 132/1000 | Loss: 0.00002140
Iteration 133/1000 | Loss: 0.00002140
Iteration 134/1000 | Loss: 0.00002140
Iteration 135/1000 | Loss: 0.00002140
Iteration 136/1000 | Loss: 0.00002140
Iteration 137/1000 | Loss: 0.00002140
Iteration 138/1000 | Loss: 0.00002139
Iteration 139/1000 | Loss: 0.00002139
Iteration 140/1000 | Loss: 0.00002139
Iteration 141/1000 | Loss: 0.00002139
Iteration 142/1000 | Loss: 0.00002139
Iteration 143/1000 | Loss: 0.00002139
Iteration 144/1000 | Loss: 0.00002139
Iteration 145/1000 | Loss: 0.00002139
Iteration 146/1000 | Loss: 0.00002139
Iteration 147/1000 | Loss: 0.00002139
Iteration 148/1000 | Loss: 0.00002139
Iteration 149/1000 | Loss: 0.00002139
Iteration 150/1000 | Loss: 0.00002139
Iteration 151/1000 | Loss: 0.00002138
Iteration 152/1000 | Loss: 0.00002138
Iteration 153/1000 | Loss: 0.00002138
Iteration 154/1000 | Loss: 0.00002138
Iteration 155/1000 | Loss: 0.00002138
Iteration 156/1000 | Loss: 0.00002138
Iteration 157/1000 | Loss: 0.00002138
Iteration 158/1000 | Loss: 0.00002138
Iteration 159/1000 | Loss: 0.00002138
Iteration 160/1000 | Loss: 0.00002138
Iteration 161/1000 | Loss: 0.00002138
Iteration 162/1000 | Loss: 0.00002138
Iteration 163/1000 | Loss: 0.00002138
Iteration 164/1000 | Loss: 0.00002138
Iteration 165/1000 | Loss: 0.00002138
Iteration 166/1000 | Loss: 0.00002138
Iteration 167/1000 | Loss: 0.00002138
Iteration 168/1000 | Loss: 0.00002138
Iteration 169/1000 | Loss: 0.00002138
Iteration 170/1000 | Loss: 0.00002138
Iteration 171/1000 | Loss: 0.00002137
Iteration 172/1000 | Loss: 0.00002137
Iteration 173/1000 | Loss: 0.00002137
Iteration 174/1000 | Loss: 0.00002137
Iteration 175/1000 | Loss: 0.00002137
Iteration 176/1000 | Loss: 0.00002137
Iteration 177/1000 | Loss: 0.00002137
Iteration 178/1000 | Loss: 0.00002137
Iteration 179/1000 | Loss: 0.00002137
Iteration 180/1000 | Loss: 0.00002137
Iteration 181/1000 | Loss: 0.00002137
Iteration 182/1000 | Loss: 0.00002137
Iteration 183/1000 | Loss: 0.00002137
Iteration 184/1000 | Loss: 0.00002137
Iteration 185/1000 | Loss: 0.00002137
Iteration 186/1000 | Loss: 0.00002137
Iteration 187/1000 | Loss: 0.00002137
Iteration 188/1000 | Loss: 0.00002137
Iteration 189/1000 | Loss: 0.00002137
Iteration 190/1000 | Loss: 0.00002137
Iteration 191/1000 | Loss: 0.00002137
Iteration 192/1000 | Loss: 0.00002137
Iteration 193/1000 | Loss: 0.00002137
Iteration 194/1000 | Loss: 0.00002137
Iteration 195/1000 | Loss: 0.00002137
Iteration 196/1000 | Loss: 0.00002137
Iteration 197/1000 | Loss: 0.00002137
Iteration 198/1000 | Loss: 0.00002137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.1368996385717764e-05, 2.1368996385717764e-05, 2.1368996385717764e-05, 2.1368996385717764e-05, 2.1368996385717764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1368996385717764e-05

Optimization complete. Final v2v error: 2.9207215309143066 mm

Highest mean error: 21.39533233642578 mm for frame 84

Lowest mean error: 2.4981768131256104 mm for frame 56

Saving results

Total time: 126.88126301765442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797537
Iteration 2/25 | Loss: 0.00116097
Iteration 3/25 | Loss: 0.00100677
Iteration 4/25 | Loss: 0.00098469
Iteration 5/25 | Loss: 0.00097387
Iteration 6/25 | Loss: 0.00097134
Iteration 7/25 | Loss: 0.00097134
Iteration 8/25 | Loss: 0.00097134
Iteration 9/25 | Loss: 0.00097134
Iteration 10/25 | Loss: 0.00097134
Iteration 11/25 | Loss: 0.00097134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009713382460176945, 0.0009713382460176945, 0.0009713382460176945, 0.0009713382460176945, 0.0009713382460176945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009713382460176945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44580817
Iteration 2/25 | Loss: 0.00070146
Iteration 3/25 | Loss: 0.00070145
Iteration 4/25 | Loss: 0.00070145
Iteration 5/25 | Loss: 0.00070145
Iteration 6/25 | Loss: 0.00070145
Iteration 7/25 | Loss: 0.00070145
Iteration 8/25 | Loss: 0.00070145
Iteration 9/25 | Loss: 0.00070145
Iteration 10/25 | Loss: 0.00070145
Iteration 11/25 | Loss: 0.00070145
Iteration 12/25 | Loss: 0.00070145
Iteration 13/25 | Loss: 0.00070145
Iteration 14/25 | Loss: 0.00070145
Iteration 15/25 | Loss: 0.00070145
Iteration 16/25 | Loss: 0.00070145
Iteration 17/25 | Loss: 0.00070145
Iteration 18/25 | Loss: 0.00070145
Iteration 19/25 | Loss: 0.00070145
Iteration 20/25 | Loss: 0.00070145
Iteration 21/25 | Loss: 0.00070145
Iteration 22/25 | Loss: 0.00070145
Iteration 23/25 | Loss: 0.00070145
Iteration 24/25 | Loss: 0.00070145
Iteration 25/25 | Loss: 0.00070145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070145
Iteration 2/1000 | Loss: 0.00002354
Iteration 3/1000 | Loss: 0.00001701
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001498
Iteration 6/1000 | Loss: 0.00001463
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001413
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001372
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001367
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001349
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001349
Iteration 32/1000 | Loss: 0.00001349
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001348
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001343
Iteration 71/1000 | Loss: 0.00001343
Iteration 72/1000 | Loss: 0.00001343
Iteration 73/1000 | Loss: 0.00001343
Iteration 74/1000 | Loss: 0.00001343
Iteration 75/1000 | Loss: 0.00001343
Iteration 76/1000 | Loss: 0.00001343
Iteration 77/1000 | Loss: 0.00001343
Iteration 78/1000 | Loss: 0.00001343
Iteration 79/1000 | Loss: 0.00001343
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00001343
Iteration 84/1000 | Loss: 0.00001343
Iteration 85/1000 | Loss: 0.00001343
Iteration 86/1000 | Loss: 0.00001343
Iteration 87/1000 | Loss: 0.00001343
Iteration 88/1000 | Loss: 0.00001343
Iteration 89/1000 | Loss: 0.00001343
Iteration 90/1000 | Loss: 0.00001343
Iteration 91/1000 | Loss: 0.00001343
Iteration 92/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.3428731108433567e-05, 1.3428731108433567e-05, 1.3428731108433567e-05, 1.3428731108433567e-05, 1.3428731108433567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3428731108433567e-05

Optimization complete. Final v2v error: 3.122015953063965 mm

Highest mean error: 3.8691964149475098 mm for frame 204

Lowest mean error: 2.666208028793335 mm for frame 10

Saving results

Total time: 33.411030530929565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083200
Iteration 2/25 | Loss: 0.00154681
Iteration 3/25 | Loss: 0.00107545
Iteration 4/25 | Loss: 0.00105928
Iteration 5/25 | Loss: 0.00105572
Iteration 6/25 | Loss: 0.00105523
Iteration 7/25 | Loss: 0.00105523
Iteration 8/25 | Loss: 0.00105523
Iteration 9/25 | Loss: 0.00105523
Iteration 10/25 | Loss: 0.00105523
Iteration 11/25 | Loss: 0.00105523
Iteration 12/25 | Loss: 0.00105523
Iteration 13/25 | Loss: 0.00105523
Iteration 14/25 | Loss: 0.00105523
Iteration 15/25 | Loss: 0.00105523
Iteration 16/25 | Loss: 0.00105523
Iteration 17/25 | Loss: 0.00105523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010552324820309877, 0.0010552324820309877, 0.0010552324820309877, 0.0010552324820309877, 0.0010552324820309877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010552324820309877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43541905
Iteration 2/25 | Loss: 0.00052398
Iteration 3/25 | Loss: 0.00052397
Iteration 4/25 | Loss: 0.00052397
Iteration 5/25 | Loss: 0.00052397
Iteration 6/25 | Loss: 0.00052397
Iteration 7/25 | Loss: 0.00052397
Iteration 8/25 | Loss: 0.00052397
Iteration 9/25 | Loss: 0.00052397
Iteration 10/25 | Loss: 0.00052397
Iteration 11/25 | Loss: 0.00052397
Iteration 12/25 | Loss: 0.00052397
Iteration 13/25 | Loss: 0.00052397
Iteration 14/25 | Loss: 0.00052397
Iteration 15/25 | Loss: 0.00052397
Iteration 16/25 | Loss: 0.00052397
Iteration 17/25 | Loss: 0.00052397
Iteration 18/25 | Loss: 0.00052397
Iteration 19/25 | Loss: 0.00052397
Iteration 20/25 | Loss: 0.00052397
Iteration 21/25 | Loss: 0.00052397
Iteration 22/25 | Loss: 0.00052397
Iteration 23/25 | Loss: 0.00052397
Iteration 24/25 | Loss: 0.00052397
Iteration 25/25 | Loss: 0.00052397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052397
Iteration 2/1000 | Loss: 0.00004786
Iteration 3/1000 | Loss: 0.00003331
Iteration 4/1000 | Loss: 0.00003066
Iteration 5/1000 | Loss: 0.00002897
Iteration 6/1000 | Loss: 0.00002812
Iteration 7/1000 | Loss: 0.00002762
Iteration 8/1000 | Loss: 0.00002712
Iteration 9/1000 | Loss: 0.00002690
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002643
Iteration 12/1000 | Loss: 0.00002630
Iteration 13/1000 | Loss: 0.00002614
Iteration 14/1000 | Loss: 0.00002607
Iteration 15/1000 | Loss: 0.00002600
Iteration 16/1000 | Loss: 0.00002586
Iteration 17/1000 | Loss: 0.00002586
Iteration 18/1000 | Loss: 0.00002584
Iteration 19/1000 | Loss: 0.00002572
Iteration 20/1000 | Loss: 0.00002569
Iteration 21/1000 | Loss: 0.00002561
Iteration 22/1000 | Loss: 0.00002555
Iteration 23/1000 | Loss: 0.00002548
Iteration 24/1000 | Loss: 0.00002546
Iteration 25/1000 | Loss: 0.00002542
Iteration 26/1000 | Loss: 0.00002536
Iteration 27/1000 | Loss: 0.00002536
Iteration 28/1000 | Loss: 0.00002534
Iteration 29/1000 | Loss: 0.00002532
Iteration 30/1000 | Loss: 0.00002532
Iteration 31/1000 | Loss: 0.00002532
Iteration 32/1000 | Loss: 0.00002532
Iteration 33/1000 | Loss: 0.00002531
Iteration 34/1000 | Loss: 0.00002531
Iteration 35/1000 | Loss: 0.00002531
Iteration 36/1000 | Loss: 0.00002531
Iteration 37/1000 | Loss: 0.00002530
Iteration 38/1000 | Loss: 0.00002530
Iteration 39/1000 | Loss: 0.00002530
Iteration 40/1000 | Loss: 0.00002530
Iteration 41/1000 | Loss: 0.00002529
Iteration 42/1000 | Loss: 0.00002528
Iteration 43/1000 | Loss: 0.00002528
Iteration 44/1000 | Loss: 0.00002528
Iteration 45/1000 | Loss: 0.00002527
Iteration 46/1000 | Loss: 0.00002527
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002526
Iteration 49/1000 | Loss: 0.00002526
Iteration 50/1000 | Loss: 0.00002524
Iteration 51/1000 | Loss: 0.00002524
Iteration 52/1000 | Loss: 0.00002524
Iteration 53/1000 | Loss: 0.00002523
Iteration 54/1000 | Loss: 0.00002523
Iteration 55/1000 | Loss: 0.00002522
Iteration 56/1000 | Loss: 0.00002522
Iteration 57/1000 | Loss: 0.00002522
Iteration 58/1000 | Loss: 0.00002522
Iteration 59/1000 | Loss: 0.00002521
Iteration 60/1000 | Loss: 0.00002521
Iteration 61/1000 | Loss: 0.00002521
Iteration 62/1000 | Loss: 0.00002521
Iteration 63/1000 | Loss: 0.00002520
Iteration 64/1000 | Loss: 0.00002520
Iteration 65/1000 | Loss: 0.00002520
Iteration 66/1000 | Loss: 0.00002520
Iteration 67/1000 | Loss: 0.00002519
Iteration 68/1000 | Loss: 0.00002519
Iteration 69/1000 | Loss: 0.00002518
Iteration 70/1000 | Loss: 0.00002518
Iteration 71/1000 | Loss: 0.00002518
Iteration 72/1000 | Loss: 0.00002517
Iteration 73/1000 | Loss: 0.00002517
Iteration 74/1000 | Loss: 0.00002517
Iteration 75/1000 | Loss: 0.00002516
Iteration 76/1000 | Loss: 0.00002516
Iteration 77/1000 | Loss: 0.00002516
Iteration 78/1000 | Loss: 0.00002516
Iteration 79/1000 | Loss: 0.00002516
Iteration 80/1000 | Loss: 0.00002516
Iteration 81/1000 | Loss: 0.00002516
Iteration 82/1000 | Loss: 0.00002516
Iteration 83/1000 | Loss: 0.00002515
Iteration 84/1000 | Loss: 0.00002515
Iteration 85/1000 | Loss: 0.00002515
Iteration 86/1000 | Loss: 0.00002515
Iteration 87/1000 | Loss: 0.00002515
Iteration 88/1000 | Loss: 0.00002515
Iteration 89/1000 | Loss: 0.00002515
Iteration 90/1000 | Loss: 0.00002515
Iteration 91/1000 | Loss: 0.00002515
Iteration 92/1000 | Loss: 0.00002515
Iteration 93/1000 | Loss: 0.00002515
Iteration 94/1000 | Loss: 0.00002515
Iteration 95/1000 | Loss: 0.00002515
Iteration 96/1000 | Loss: 0.00002515
Iteration 97/1000 | Loss: 0.00002515
Iteration 98/1000 | Loss: 0.00002515
Iteration 99/1000 | Loss: 0.00002515
Iteration 100/1000 | Loss: 0.00002515
Iteration 101/1000 | Loss: 0.00002515
Iteration 102/1000 | Loss: 0.00002515
Iteration 103/1000 | Loss: 0.00002515
Iteration 104/1000 | Loss: 0.00002515
Iteration 105/1000 | Loss: 0.00002515
Iteration 106/1000 | Loss: 0.00002515
Iteration 107/1000 | Loss: 0.00002515
Iteration 108/1000 | Loss: 0.00002515
Iteration 109/1000 | Loss: 0.00002515
Iteration 110/1000 | Loss: 0.00002515
Iteration 111/1000 | Loss: 0.00002515
Iteration 112/1000 | Loss: 0.00002515
Iteration 113/1000 | Loss: 0.00002515
Iteration 114/1000 | Loss: 0.00002515
Iteration 115/1000 | Loss: 0.00002515
Iteration 116/1000 | Loss: 0.00002515
Iteration 117/1000 | Loss: 0.00002515
Iteration 118/1000 | Loss: 0.00002515
Iteration 119/1000 | Loss: 0.00002515
Iteration 120/1000 | Loss: 0.00002515
Iteration 121/1000 | Loss: 0.00002515
Iteration 122/1000 | Loss: 0.00002515
Iteration 123/1000 | Loss: 0.00002515
Iteration 124/1000 | Loss: 0.00002515
Iteration 125/1000 | Loss: 0.00002515
Iteration 126/1000 | Loss: 0.00002515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.5149414796032943e-05, 2.5149414796032943e-05, 2.5149414796032943e-05, 2.5149414796032943e-05, 2.5149414796032943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5149414796032943e-05

Optimization complete. Final v2v error: 3.9257378578186035 mm

Highest mean error: 5.0934882164001465 mm for frame 27

Lowest mean error: 2.9640159606933594 mm for frame 0

Saving results

Total time: 44.80353307723999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034279
Iteration 2/25 | Loss: 0.00301539
Iteration 3/25 | Loss: 0.00177884
Iteration 4/25 | Loss: 0.00159117
Iteration 5/25 | Loss: 0.00153928
Iteration 6/25 | Loss: 0.00160284
Iteration 7/25 | Loss: 0.00149063
Iteration 8/25 | Loss: 0.00138950
Iteration 9/25 | Loss: 0.00130786
Iteration 10/25 | Loss: 0.00126671
Iteration 11/25 | Loss: 0.00123540
Iteration 12/25 | Loss: 0.00121010
Iteration 13/25 | Loss: 0.00120620
Iteration 14/25 | Loss: 0.00120154
Iteration 15/25 | Loss: 0.00118971
Iteration 16/25 | Loss: 0.00117092
Iteration 17/25 | Loss: 0.00115465
Iteration 18/25 | Loss: 0.00115917
Iteration 19/25 | Loss: 0.00114954
Iteration 20/25 | Loss: 0.00114319
Iteration 21/25 | Loss: 0.00114287
Iteration 22/25 | Loss: 0.00114013
Iteration 23/25 | Loss: 0.00113864
Iteration 24/25 | Loss: 0.00114206
Iteration 25/25 | Loss: 0.00113787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34202123
Iteration 2/25 | Loss: 0.00294915
Iteration 3/25 | Loss: 0.00257222
Iteration 4/25 | Loss: 0.00257222
Iteration 5/25 | Loss: 0.00257222
Iteration 6/25 | Loss: 0.00257222
Iteration 7/25 | Loss: 0.00257222
Iteration 8/25 | Loss: 0.00257222
Iteration 9/25 | Loss: 0.00257222
Iteration 10/25 | Loss: 0.00257222
Iteration 11/25 | Loss: 0.00257222
Iteration 12/25 | Loss: 0.00257222
Iteration 13/25 | Loss: 0.00257222
Iteration 14/25 | Loss: 0.00257222
Iteration 15/25 | Loss: 0.00257222
Iteration 16/25 | Loss: 0.00257222
Iteration 17/25 | Loss: 0.00257222
Iteration 18/25 | Loss: 0.00257222
Iteration 19/25 | Loss: 0.00257222
Iteration 20/25 | Loss: 0.00257222
Iteration 21/25 | Loss: 0.00257222
Iteration 22/25 | Loss: 0.00257222
Iteration 23/25 | Loss: 0.00257222
Iteration 24/25 | Loss: 0.00257222
Iteration 25/25 | Loss: 0.00257222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257222
Iteration 2/1000 | Loss: 0.00086492
Iteration 3/1000 | Loss: 0.00066965
Iteration 4/1000 | Loss: 0.00078227
Iteration 5/1000 | Loss: 0.00081604
Iteration 6/1000 | Loss: 0.00040832
Iteration 7/1000 | Loss: 0.00052725
Iteration 8/1000 | Loss: 0.00128203
Iteration 9/1000 | Loss: 0.00071672
Iteration 10/1000 | Loss: 0.00126938
Iteration 11/1000 | Loss: 0.00270630
Iteration 12/1000 | Loss: 0.00088132
Iteration 13/1000 | Loss: 0.00113423
Iteration 14/1000 | Loss: 0.00024547
Iteration 15/1000 | Loss: 0.00126810
Iteration 16/1000 | Loss: 0.00200941
Iteration 17/1000 | Loss: 0.00024685
Iteration 18/1000 | Loss: 0.00062942
Iteration 19/1000 | Loss: 0.00018828
Iteration 20/1000 | Loss: 0.00045405
Iteration 21/1000 | Loss: 0.00025026
Iteration 22/1000 | Loss: 0.00071650
Iteration 23/1000 | Loss: 0.00075952
Iteration 24/1000 | Loss: 0.00041712
Iteration 25/1000 | Loss: 0.00099943
Iteration 26/1000 | Loss: 0.00036841
Iteration 27/1000 | Loss: 0.00168455
Iteration 28/1000 | Loss: 0.00102759
Iteration 29/1000 | Loss: 0.00083902
Iteration 30/1000 | Loss: 0.00077700
Iteration 31/1000 | Loss: 0.00074027
Iteration 32/1000 | Loss: 0.00045349
Iteration 33/1000 | Loss: 0.00035119
Iteration 34/1000 | Loss: 0.00062828
Iteration 35/1000 | Loss: 0.00033271
Iteration 36/1000 | Loss: 0.00107437
Iteration 37/1000 | Loss: 0.00026763
Iteration 38/1000 | Loss: 0.00041487
Iteration 39/1000 | Loss: 0.00036603
Iteration 40/1000 | Loss: 0.00032079
Iteration 41/1000 | Loss: 0.00030080
Iteration 42/1000 | Loss: 0.00021141
Iteration 43/1000 | Loss: 0.00015216
Iteration 44/1000 | Loss: 0.00042010
Iteration 45/1000 | Loss: 0.00017582
Iteration 46/1000 | Loss: 0.00021970
Iteration 47/1000 | Loss: 0.00035729
Iteration 48/1000 | Loss: 0.00023144
Iteration 49/1000 | Loss: 0.00035692
Iteration 50/1000 | Loss: 0.00032706
Iteration 51/1000 | Loss: 0.00040688
Iteration 52/1000 | Loss: 0.00089592
Iteration 53/1000 | Loss: 0.00058215
Iteration 54/1000 | Loss: 0.00073721
Iteration 55/1000 | Loss: 0.00084421
Iteration 56/1000 | Loss: 0.00023982
Iteration 57/1000 | Loss: 0.00033879
Iteration 58/1000 | Loss: 0.00032675
Iteration 59/1000 | Loss: 0.00039153
Iteration 60/1000 | Loss: 0.00015198
Iteration 61/1000 | Loss: 0.00029311
Iteration 62/1000 | Loss: 0.00034752
Iteration 63/1000 | Loss: 0.00047793
Iteration 64/1000 | Loss: 0.00053917
Iteration 65/1000 | Loss: 0.00030953
Iteration 66/1000 | Loss: 0.00033924
Iteration 67/1000 | Loss: 0.00024430
Iteration 68/1000 | Loss: 0.00045292
Iteration 69/1000 | Loss: 0.00041697
Iteration 70/1000 | Loss: 0.00042992
Iteration 71/1000 | Loss: 0.00056098
Iteration 72/1000 | Loss: 0.00015821
Iteration 73/1000 | Loss: 0.00028963
Iteration 74/1000 | Loss: 0.00063577
Iteration 75/1000 | Loss: 0.00057814
Iteration 76/1000 | Loss: 0.00051439
Iteration 77/1000 | Loss: 0.00052583
Iteration 78/1000 | Loss: 0.00023120
Iteration 79/1000 | Loss: 0.00037888
Iteration 80/1000 | Loss: 0.00015791
Iteration 81/1000 | Loss: 0.00022425
Iteration 82/1000 | Loss: 0.00048871
Iteration 83/1000 | Loss: 0.00033111
Iteration 84/1000 | Loss: 0.00025120
Iteration 85/1000 | Loss: 0.00016675
Iteration 86/1000 | Loss: 0.00013740
Iteration 87/1000 | Loss: 0.00012716
Iteration 88/1000 | Loss: 0.00011918
Iteration 89/1000 | Loss: 0.00025634
Iteration 90/1000 | Loss: 0.00073347
Iteration 91/1000 | Loss: 0.00047991
Iteration 92/1000 | Loss: 0.00049216
Iteration 93/1000 | Loss: 0.00030065
Iteration 94/1000 | Loss: 0.00065902
Iteration 95/1000 | Loss: 0.00114451
Iteration 96/1000 | Loss: 0.00076795
Iteration 97/1000 | Loss: 0.00120711
Iteration 98/1000 | Loss: 0.00037877
Iteration 99/1000 | Loss: 0.00017933
Iteration 100/1000 | Loss: 0.00037667
Iteration 101/1000 | Loss: 0.00014887
Iteration 102/1000 | Loss: 0.00009298
Iteration 103/1000 | Loss: 0.00067014
Iteration 104/1000 | Loss: 0.00089150
Iteration 105/1000 | Loss: 0.00062544
Iteration 106/1000 | Loss: 0.00059664
Iteration 107/1000 | Loss: 0.00041141
Iteration 108/1000 | Loss: 0.00046709
Iteration 109/1000 | Loss: 0.00084936
Iteration 110/1000 | Loss: 0.00032463
Iteration 111/1000 | Loss: 0.00010504
Iteration 112/1000 | Loss: 0.00032545
Iteration 113/1000 | Loss: 0.00070860
Iteration 114/1000 | Loss: 0.00016318
Iteration 115/1000 | Loss: 0.00021717
Iteration 116/1000 | Loss: 0.00036491
Iteration 117/1000 | Loss: 0.00021339
Iteration 118/1000 | Loss: 0.00006341
Iteration 119/1000 | Loss: 0.00018978
Iteration 120/1000 | Loss: 0.00006250
Iteration 121/1000 | Loss: 0.00035352
Iteration 122/1000 | Loss: 0.00035308
Iteration 123/1000 | Loss: 0.00014400
Iteration 124/1000 | Loss: 0.00025157
Iteration 125/1000 | Loss: 0.00025816
Iteration 126/1000 | Loss: 0.00022759
Iteration 127/1000 | Loss: 0.00034820
Iteration 128/1000 | Loss: 0.00125721
Iteration 129/1000 | Loss: 0.00041057
Iteration 130/1000 | Loss: 0.00061163
Iteration 131/1000 | Loss: 0.00047621
Iteration 132/1000 | Loss: 0.00041610
Iteration 133/1000 | Loss: 0.00041847
Iteration 134/1000 | Loss: 0.00010039
Iteration 135/1000 | Loss: 0.00022148
Iteration 136/1000 | Loss: 0.00007026
Iteration 137/1000 | Loss: 0.00006345
Iteration 138/1000 | Loss: 0.00050433
Iteration 139/1000 | Loss: 0.00008960
Iteration 140/1000 | Loss: 0.00006910
Iteration 141/1000 | Loss: 0.00005602
Iteration 142/1000 | Loss: 0.00005146
Iteration 143/1000 | Loss: 0.00005965
Iteration 144/1000 | Loss: 0.00006463
Iteration 145/1000 | Loss: 0.00022143
Iteration 146/1000 | Loss: 0.00030676
Iteration 147/1000 | Loss: 0.00017776
Iteration 148/1000 | Loss: 0.00008446
Iteration 149/1000 | Loss: 0.00007855
Iteration 150/1000 | Loss: 0.00055444
Iteration 151/1000 | Loss: 0.00005780
Iteration 152/1000 | Loss: 0.00005677
Iteration 153/1000 | Loss: 0.00004692
Iteration 154/1000 | Loss: 0.00004819
Iteration 155/1000 | Loss: 0.00006432
Iteration 156/1000 | Loss: 0.00036709
Iteration 157/1000 | Loss: 0.00006916
Iteration 158/1000 | Loss: 0.00006340
Iteration 159/1000 | Loss: 0.00008121
Iteration 160/1000 | Loss: 0.00004853
Iteration 161/1000 | Loss: 0.00003942
Iteration 162/1000 | Loss: 0.00004855
Iteration 163/1000 | Loss: 0.00006511
Iteration 164/1000 | Loss: 0.00005049
Iteration 165/1000 | Loss: 0.00005228
Iteration 166/1000 | Loss: 0.00004951
Iteration 167/1000 | Loss: 0.00005128
Iteration 168/1000 | Loss: 0.00004798
Iteration 169/1000 | Loss: 0.00037362
Iteration 170/1000 | Loss: 0.00192468
Iteration 171/1000 | Loss: 0.00041208
Iteration 172/1000 | Loss: 0.00009309
Iteration 173/1000 | Loss: 0.00005090
Iteration 174/1000 | Loss: 0.00004097
Iteration 175/1000 | Loss: 0.00005145
Iteration 176/1000 | Loss: 0.00003242
Iteration 177/1000 | Loss: 0.00003019
Iteration 178/1000 | Loss: 0.00002840
Iteration 179/1000 | Loss: 0.00003931
Iteration 180/1000 | Loss: 0.00002844
Iteration 181/1000 | Loss: 0.00002671
Iteration 182/1000 | Loss: 0.00002696
Iteration 183/1000 | Loss: 0.00002632
Iteration 184/1000 | Loss: 0.00002600
Iteration 185/1000 | Loss: 0.00002586
Iteration 186/1000 | Loss: 0.00002573
Iteration 187/1000 | Loss: 0.00002571
Iteration 188/1000 | Loss: 0.00002578
Iteration 189/1000 | Loss: 0.00002578
Iteration 190/1000 | Loss: 0.00002552
Iteration 191/1000 | Loss: 0.00002550
Iteration 192/1000 | Loss: 0.00002546
Iteration 193/1000 | Loss: 0.00002539
Iteration 194/1000 | Loss: 0.00002537
Iteration 195/1000 | Loss: 0.00002536
Iteration 196/1000 | Loss: 0.00002535
Iteration 197/1000 | Loss: 0.00002533
Iteration 198/1000 | Loss: 0.00002527
Iteration 199/1000 | Loss: 0.00002526
Iteration 200/1000 | Loss: 0.00002526
Iteration 201/1000 | Loss: 0.00002526
Iteration 202/1000 | Loss: 0.00002526
Iteration 203/1000 | Loss: 0.00002526
Iteration 204/1000 | Loss: 0.00002526
Iteration 205/1000 | Loss: 0.00002526
Iteration 206/1000 | Loss: 0.00002526
Iteration 207/1000 | Loss: 0.00002526
Iteration 208/1000 | Loss: 0.00002526
Iteration 209/1000 | Loss: 0.00002525
Iteration 210/1000 | Loss: 0.00002525
Iteration 211/1000 | Loss: 0.00002524
Iteration 212/1000 | Loss: 0.00002524
Iteration 213/1000 | Loss: 0.00002524
Iteration 214/1000 | Loss: 0.00002522
Iteration 215/1000 | Loss: 0.00002519
Iteration 216/1000 | Loss: 0.00002556
Iteration 217/1000 | Loss: 0.00002525
Iteration 218/1000 | Loss: 0.00002524
Iteration 219/1000 | Loss: 0.00002517
Iteration 220/1000 | Loss: 0.00002513
Iteration 221/1000 | Loss: 0.00002513
Iteration 222/1000 | Loss: 0.00002511
Iteration 223/1000 | Loss: 0.00002510
Iteration 224/1000 | Loss: 0.00002510
Iteration 225/1000 | Loss: 0.00002510
Iteration 226/1000 | Loss: 0.00002510
Iteration 227/1000 | Loss: 0.00002509
Iteration 228/1000 | Loss: 0.00002561
Iteration 229/1000 | Loss: 0.00002517
Iteration 230/1000 | Loss: 0.00002506
Iteration 231/1000 | Loss: 0.00002506
Iteration 232/1000 | Loss: 0.00002506
Iteration 233/1000 | Loss: 0.00002506
Iteration 234/1000 | Loss: 0.00002506
Iteration 235/1000 | Loss: 0.00002506
Iteration 236/1000 | Loss: 0.00002506
Iteration 237/1000 | Loss: 0.00002506
Iteration 238/1000 | Loss: 0.00002505
Iteration 239/1000 | Loss: 0.00002554
Iteration 240/1000 | Loss: 0.00020742
Iteration 241/1000 | Loss: 0.00019845
Iteration 242/1000 | Loss: 0.00003732
Iteration 243/1000 | Loss: 0.00002999
Iteration 244/1000 | Loss: 0.00005600
Iteration 245/1000 | Loss: 0.00004713
Iteration 246/1000 | Loss: 0.00002767
Iteration 247/1000 | Loss: 0.00002364
Iteration 248/1000 | Loss: 0.00002275
Iteration 249/1000 | Loss: 0.00007881
Iteration 250/1000 | Loss: 0.00002276
Iteration 251/1000 | Loss: 0.00002210
Iteration 252/1000 | Loss: 0.00002187
Iteration 253/1000 | Loss: 0.00027935
Iteration 254/1000 | Loss: 0.00003898
Iteration 255/1000 | Loss: 0.00005636
Iteration 256/1000 | Loss: 0.00002505
Iteration 257/1000 | Loss: 0.00002292
Iteration 258/1000 | Loss: 0.00002145
Iteration 259/1000 | Loss: 0.00002131
Iteration 260/1000 | Loss: 0.00002061
Iteration 261/1000 | Loss: 0.00002026
Iteration 262/1000 | Loss: 0.00002012
Iteration 263/1000 | Loss: 0.00002005
Iteration 264/1000 | Loss: 0.00001994
Iteration 265/1000 | Loss: 0.00001992
Iteration 266/1000 | Loss: 0.00001992
Iteration 267/1000 | Loss: 0.00001991
Iteration 268/1000 | Loss: 0.00001991
Iteration 269/1000 | Loss: 0.00001990
Iteration 270/1000 | Loss: 0.00002035
Iteration 271/1000 | Loss: 0.00002035
Iteration 272/1000 | Loss: 0.00001997
Iteration 273/1000 | Loss: 0.00001985
Iteration 274/1000 | Loss: 0.00001985
Iteration 275/1000 | Loss: 0.00001984
Iteration 276/1000 | Loss: 0.00001984
Iteration 277/1000 | Loss: 0.00001984
Iteration 278/1000 | Loss: 0.00001984
Iteration 279/1000 | Loss: 0.00001984
Iteration 280/1000 | Loss: 0.00001984
Iteration 281/1000 | Loss: 0.00001984
Iteration 282/1000 | Loss: 0.00002026
Iteration 283/1000 | Loss: 0.00001992
Iteration 284/1000 | Loss: 0.00004680
Iteration 285/1000 | Loss: 0.00004678
Iteration 286/1000 | Loss: 0.00004678
Iteration 287/1000 | Loss: 0.00018814
Iteration 288/1000 | Loss: 0.00002006
Iteration 289/1000 | Loss: 0.00001981
Iteration 290/1000 | Loss: 0.00001980
Iteration 291/1000 | Loss: 0.00001980
Iteration 292/1000 | Loss: 0.00001979
Iteration 293/1000 | Loss: 0.00001975
Iteration 294/1000 | Loss: 0.00001975
Iteration 295/1000 | Loss: 0.00001975
Iteration 296/1000 | Loss: 0.00001975
Iteration 297/1000 | Loss: 0.00001975
Iteration 298/1000 | Loss: 0.00001975
Iteration 299/1000 | Loss: 0.00001975
Iteration 300/1000 | Loss: 0.00001975
Iteration 301/1000 | Loss: 0.00001975
Iteration 302/1000 | Loss: 0.00001975
Iteration 303/1000 | Loss: 0.00001974
Iteration 304/1000 | Loss: 0.00001974
Iteration 305/1000 | Loss: 0.00001974
Iteration 306/1000 | Loss: 0.00001974
Iteration 307/1000 | Loss: 0.00001974
Iteration 308/1000 | Loss: 0.00001974
Iteration 309/1000 | Loss: 0.00001974
Iteration 310/1000 | Loss: 0.00001973
Iteration 311/1000 | Loss: 0.00001973
Iteration 312/1000 | Loss: 0.00001973
Iteration 313/1000 | Loss: 0.00001973
Iteration 314/1000 | Loss: 0.00002012
Iteration 315/1000 | Loss: 0.00001984
Iteration 316/1000 | Loss: 0.00004459
Iteration 317/1000 | Loss: 0.00001984
Iteration 318/1000 | Loss: 0.00001978
Iteration 319/1000 | Loss: 0.00001977
Iteration 320/1000 | Loss: 0.00001977
Iteration 321/1000 | Loss: 0.00001976
Iteration 322/1000 | Loss: 0.00001976
Iteration 323/1000 | Loss: 0.00001976
Iteration 324/1000 | Loss: 0.00001976
Iteration 325/1000 | Loss: 0.00001975
Iteration 326/1000 | Loss: 0.00001975
Iteration 327/1000 | Loss: 0.00001974
Iteration 328/1000 | Loss: 0.00001973
Iteration 329/1000 | Loss: 0.00001973
Iteration 330/1000 | Loss: 0.00001972
Iteration 331/1000 | Loss: 0.00001972
Iteration 332/1000 | Loss: 0.00001972
Iteration 333/1000 | Loss: 0.00001972
Iteration 334/1000 | Loss: 0.00001972
Iteration 335/1000 | Loss: 0.00001972
Iteration 336/1000 | Loss: 0.00001972
Iteration 337/1000 | Loss: 0.00001972
Iteration 338/1000 | Loss: 0.00001971
Iteration 339/1000 | Loss: 0.00001971
Iteration 340/1000 | Loss: 0.00001971
Iteration 341/1000 | Loss: 0.00001971
Iteration 342/1000 | Loss: 0.00001971
Iteration 343/1000 | Loss: 0.00001970
Iteration 344/1000 | Loss: 0.00001970
Iteration 345/1000 | Loss: 0.00001970
Iteration 346/1000 | Loss: 0.00001970
Iteration 347/1000 | Loss: 0.00001970
Iteration 348/1000 | Loss: 0.00001969
Iteration 349/1000 | Loss: 0.00001969
Iteration 350/1000 | Loss: 0.00001969
Iteration 351/1000 | Loss: 0.00001969
Iteration 352/1000 | Loss: 0.00001969
Iteration 353/1000 | Loss: 0.00001968
Iteration 354/1000 | Loss: 0.00001968
Iteration 355/1000 | Loss: 0.00001968
Iteration 356/1000 | Loss: 0.00001968
Iteration 357/1000 | Loss: 0.00001968
Iteration 358/1000 | Loss: 0.00001968
Iteration 359/1000 | Loss: 0.00001968
Iteration 360/1000 | Loss: 0.00001968
Iteration 361/1000 | Loss: 0.00001968
Iteration 362/1000 | Loss: 0.00001968
Iteration 363/1000 | Loss: 0.00001968
Iteration 364/1000 | Loss: 0.00001967
Iteration 365/1000 | Loss: 0.00001967
Iteration 366/1000 | Loss: 0.00001967
Iteration 367/1000 | Loss: 0.00001967
Iteration 368/1000 | Loss: 0.00001967
Iteration 369/1000 | Loss: 0.00001967
Iteration 370/1000 | Loss: 0.00001967
Iteration 371/1000 | Loss: 0.00001967
Iteration 372/1000 | Loss: 0.00001967
Iteration 373/1000 | Loss: 0.00001967
Iteration 374/1000 | Loss: 0.00001967
Iteration 375/1000 | Loss: 0.00002006
Iteration 376/1000 | Loss: 0.00001978
Iteration 377/1000 | Loss: 0.00001977
Iteration 378/1000 | Loss: 0.00001971
Iteration 379/1000 | Loss: 0.00001970
Iteration 380/1000 | Loss: 0.00001969
Iteration 381/1000 | Loss: 0.00001968
Iteration 382/1000 | Loss: 0.00001967
Iteration 383/1000 | Loss: 0.00001967
Iteration 384/1000 | Loss: 0.00001966
Iteration 385/1000 | Loss: 0.00001966
Iteration 386/1000 | Loss: 0.00001966
Iteration 387/1000 | Loss: 0.00001966
Iteration 388/1000 | Loss: 0.00001966
Iteration 389/1000 | Loss: 0.00001966
Iteration 390/1000 | Loss: 0.00001966
Iteration 391/1000 | Loss: 0.00001966
Iteration 392/1000 | Loss: 0.00001966
Iteration 393/1000 | Loss: 0.00001966
Iteration 394/1000 | Loss: 0.00001966
Iteration 395/1000 | Loss: 0.00001966
Iteration 396/1000 | Loss: 0.00001966
Iteration 397/1000 | Loss: 0.00001965
Iteration 398/1000 | Loss: 0.00001965
Iteration 399/1000 | Loss: 0.00001965
Iteration 400/1000 | Loss: 0.00001965
Iteration 401/1000 | Loss: 0.00001964
Iteration 402/1000 | Loss: 0.00002012
Iteration 403/1000 | Loss: 0.00002012
Iteration 404/1000 | Loss: 0.00001977
Iteration 405/1000 | Loss: 0.00001965
Iteration 406/1000 | Loss: 0.00001965
Iteration 407/1000 | Loss: 0.00001965
Iteration 408/1000 | Loss: 0.00001965
Iteration 409/1000 | Loss: 0.00001964
Iteration 410/1000 | Loss: 0.00001964
Iteration 411/1000 | Loss: 0.00001964
Iteration 412/1000 | Loss: 0.00001964
Iteration 413/1000 | Loss: 0.00001964
Iteration 414/1000 | Loss: 0.00001964
Iteration 415/1000 | Loss: 0.00001964
Iteration 416/1000 | Loss: 0.00001964
Iteration 417/1000 | Loss: 0.00001964
Iteration 418/1000 | Loss: 0.00001964
Iteration 419/1000 | Loss: 0.00001964
Iteration 420/1000 | Loss: 0.00001964
Iteration 421/1000 | Loss: 0.00001964
Iteration 422/1000 | Loss: 0.00001964
Iteration 423/1000 | Loss: 0.00001964
Iteration 424/1000 | Loss: 0.00001964
Iteration 425/1000 | Loss: 0.00001964
Iteration 426/1000 | Loss: 0.00001964
Iteration 427/1000 | Loss: 0.00001964
Iteration 428/1000 | Loss: 0.00001964
Iteration 429/1000 | Loss: 0.00001964
Iteration 430/1000 | Loss: 0.00001964
Iteration 431/1000 | Loss: 0.00001964
Iteration 432/1000 | Loss: 0.00001964
Iteration 433/1000 | Loss: 0.00001964
Iteration 434/1000 | Loss: 0.00001964
Iteration 435/1000 | Loss: 0.00001964
Iteration 436/1000 | Loss: 0.00001964
Iteration 437/1000 | Loss: 0.00001964
Iteration 438/1000 | Loss: 0.00001964
Iteration 439/1000 | Loss: 0.00001964
Iteration 440/1000 | Loss: 0.00001964
Iteration 441/1000 | Loss: 0.00001964
Iteration 442/1000 | Loss: 0.00001964
Iteration 443/1000 | Loss: 0.00001964
Iteration 444/1000 | Loss: 0.00001964
Iteration 445/1000 | Loss: 0.00001964
Iteration 446/1000 | Loss: 0.00001964
Iteration 447/1000 | Loss: 0.00001964
Iteration 448/1000 | Loss: 0.00001964
Iteration 449/1000 | Loss: 0.00001964
Iteration 450/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 450. Stopping optimization.
Last 5 losses: [1.96353648789227e-05, 1.96353648789227e-05, 1.96353648789227e-05, 1.96353648789227e-05, 1.96353648789227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.96353648789227e-05

Optimization complete. Final v2v error: 2.6290059089660645 mm

Highest mean error: 22.486543655395508 mm for frame 117

Lowest mean error: 1.8244215250015259 mm for frame 171

Saving results

Total time: 425.07110023498535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904296
Iteration 2/25 | Loss: 0.00106022
Iteration 3/25 | Loss: 0.00093791
Iteration 4/25 | Loss: 0.00092561
Iteration 5/25 | Loss: 0.00092214
Iteration 6/25 | Loss: 0.00092107
Iteration 7/25 | Loss: 0.00092107
Iteration 8/25 | Loss: 0.00092107
Iteration 9/25 | Loss: 0.00092107
Iteration 10/25 | Loss: 0.00092107
Iteration 11/25 | Loss: 0.00092107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009210724965669215, 0.0009210724965669215, 0.0009210724965669215, 0.0009210724965669215, 0.0009210724965669215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009210724965669215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97681534
Iteration 2/25 | Loss: 0.00063263
Iteration 3/25 | Loss: 0.00063263
Iteration 4/25 | Loss: 0.00063263
Iteration 5/25 | Loss: 0.00063263
Iteration 6/25 | Loss: 0.00063263
Iteration 7/25 | Loss: 0.00063263
Iteration 8/25 | Loss: 0.00063263
Iteration 9/25 | Loss: 0.00063263
Iteration 10/25 | Loss: 0.00063263
Iteration 11/25 | Loss: 0.00063263
Iteration 12/25 | Loss: 0.00063262
Iteration 13/25 | Loss: 0.00063262
Iteration 14/25 | Loss: 0.00063262
Iteration 15/25 | Loss: 0.00063262
Iteration 16/25 | Loss: 0.00063262
Iteration 17/25 | Loss: 0.00063262
Iteration 18/25 | Loss: 0.00063262
Iteration 19/25 | Loss: 0.00063262
Iteration 20/25 | Loss: 0.00063262
Iteration 21/25 | Loss: 0.00063262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006326247821561992, 0.0006326247821561992, 0.0006326247821561992, 0.0006326247821561992, 0.0006326247821561992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006326247821561992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063262
Iteration 2/1000 | Loss: 0.00002086
Iteration 3/1000 | Loss: 0.00001394
Iteration 4/1000 | Loss: 0.00001289
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001200
Iteration 7/1000 | Loss: 0.00001200
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001153
Iteration 12/1000 | Loss: 0.00001153
Iteration 13/1000 | Loss: 0.00001151
Iteration 14/1000 | Loss: 0.00001150
Iteration 15/1000 | Loss: 0.00001150
Iteration 16/1000 | Loss: 0.00001149
Iteration 17/1000 | Loss: 0.00001149
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001149
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001147
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001147
Iteration 27/1000 | Loss: 0.00001147
Iteration 28/1000 | Loss: 0.00001146
Iteration 29/1000 | Loss: 0.00001146
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001144
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001141
Iteration 34/1000 | Loss: 0.00001139
Iteration 35/1000 | Loss: 0.00001139
Iteration 36/1000 | Loss: 0.00001139
Iteration 37/1000 | Loss: 0.00001139
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001139
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001138
Iteration 42/1000 | Loss: 0.00001138
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001131
Iteration 64/1000 | Loss: 0.00001131
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001127
Iteration 80/1000 | Loss: 0.00001127
Iteration 81/1000 | Loss: 0.00001127
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001126
Iteration 84/1000 | Loss: 0.00001126
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001125
Iteration 87/1000 | Loss: 0.00001125
Iteration 88/1000 | Loss: 0.00001125
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001123
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001122
Iteration 99/1000 | Loss: 0.00001122
Iteration 100/1000 | Loss: 0.00001122
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001121
Iteration 103/1000 | Loss: 0.00001121
Iteration 104/1000 | Loss: 0.00001121
Iteration 105/1000 | Loss: 0.00001121
Iteration 106/1000 | Loss: 0.00001121
Iteration 107/1000 | Loss: 0.00001120
Iteration 108/1000 | Loss: 0.00001120
Iteration 109/1000 | Loss: 0.00001120
Iteration 110/1000 | Loss: 0.00001120
Iteration 111/1000 | Loss: 0.00001120
Iteration 112/1000 | Loss: 0.00001120
Iteration 113/1000 | Loss: 0.00001120
Iteration 114/1000 | Loss: 0.00001119
Iteration 115/1000 | Loss: 0.00001119
Iteration 116/1000 | Loss: 0.00001119
Iteration 117/1000 | Loss: 0.00001119
Iteration 118/1000 | Loss: 0.00001119
Iteration 119/1000 | Loss: 0.00001119
Iteration 120/1000 | Loss: 0.00001119
Iteration 121/1000 | Loss: 0.00001118
Iteration 122/1000 | Loss: 0.00001118
Iteration 123/1000 | Loss: 0.00001118
Iteration 124/1000 | Loss: 0.00001118
Iteration 125/1000 | Loss: 0.00001118
Iteration 126/1000 | Loss: 0.00001118
Iteration 127/1000 | Loss: 0.00001118
Iteration 128/1000 | Loss: 0.00001118
Iteration 129/1000 | Loss: 0.00001118
Iteration 130/1000 | Loss: 0.00001118
Iteration 131/1000 | Loss: 0.00001118
Iteration 132/1000 | Loss: 0.00001118
Iteration 133/1000 | Loss: 0.00001118
Iteration 134/1000 | Loss: 0.00001118
Iteration 135/1000 | Loss: 0.00001118
Iteration 136/1000 | Loss: 0.00001117
Iteration 137/1000 | Loss: 0.00001117
Iteration 138/1000 | Loss: 0.00001117
Iteration 139/1000 | Loss: 0.00001117
Iteration 140/1000 | Loss: 0.00001117
Iteration 141/1000 | Loss: 0.00001117
Iteration 142/1000 | Loss: 0.00001117
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001117
Iteration 146/1000 | Loss: 0.00001117
Iteration 147/1000 | Loss: 0.00001117
Iteration 148/1000 | Loss: 0.00001117
Iteration 149/1000 | Loss: 0.00001117
Iteration 150/1000 | Loss: 0.00001117
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001117
Iteration 157/1000 | Loss: 0.00001117
Iteration 158/1000 | Loss: 0.00001117
Iteration 159/1000 | Loss: 0.00001117
Iteration 160/1000 | Loss: 0.00001117
Iteration 161/1000 | Loss: 0.00001117
Iteration 162/1000 | Loss: 0.00001117
Iteration 163/1000 | Loss: 0.00001117
Iteration 164/1000 | Loss: 0.00001117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1166269359819125e-05, 1.1166269359819125e-05, 1.1166269359819125e-05, 1.1166269359819125e-05, 1.1166269359819125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1166269359819125e-05

Optimization complete. Final v2v error: 2.7171735763549805 mm

Highest mean error: 3.7486205101013184 mm for frame 53

Lowest mean error: 2.4130632877349854 mm for frame 79

Saving results

Total time: 29.80831003189087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782809
Iteration 2/25 | Loss: 0.00132594
Iteration 3/25 | Loss: 0.00101291
Iteration 4/25 | Loss: 0.00097354
Iteration 5/25 | Loss: 0.00096683
Iteration 6/25 | Loss: 0.00096571
Iteration 7/25 | Loss: 0.00096571
Iteration 8/25 | Loss: 0.00096571
Iteration 9/25 | Loss: 0.00096571
Iteration 10/25 | Loss: 0.00096571
Iteration 11/25 | Loss: 0.00096571
Iteration 12/25 | Loss: 0.00096571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009657084010541439, 0.0009657084010541439, 0.0009657084010541439, 0.0009657084010541439, 0.0009657084010541439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009657084010541439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30393326
Iteration 2/25 | Loss: 0.00042836
Iteration 3/25 | Loss: 0.00042836
Iteration 4/25 | Loss: 0.00042836
Iteration 5/25 | Loss: 0.00042836
Iteration 6/25 | Loss: 0.00042836
Iteration 7/25 | Loss: 0.00042836
Iteration 8/25 | Loss: 0.00042836
Iteration 9/25 | Loss: 0.00042836
Iteration 10/25 | Loss: 0.00042836
Iteration 11/25 | Loss: 0.00042836
Iteration 12/25 | Loss: 0.00042836
Iteration 13/25 | Loss: 0.00042836
Iteration 14/25 | Loss: 0.00042836
Iteration 15/25 | Loss: 0.00042836
Iteration 16/25 | Loss: 0.00042836
Iteration 17/25 | Loss: 0.00042836
Iteration 18/25 | Loss: 0.00042836
Iteration 19/25 | Loss: 0.00042836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000428359373472631, 0.000428359373472631, 0.000428359373472631, 0.000428359373472631, 0.000428359373472631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000428359373472631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042836
Iteration 2/1000 | Loss: 0.00002404
Iteration 3/1000 | Loss: 0.00001703
Iteration 4/1000 | Loss: 0.00001523
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001374
Iteration 7/1000 | Loss: 0.00001301
Iteration 8/1000 | Loss: 0.00001258
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001220
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001211
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001209
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001208
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001204
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001202
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001197
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001194
Iteration 45/1000 | Loss: 0.00001194
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001194
Iteration 49/1000 | Loss: 0.00001194
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001193
Iteration 72/1000 | Loss: 0.00001193
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.1933844689338002e-05, 1.1933844689338002e-05, 1.1933844689338002e-05, 1.1933844689338002e-05, 1.1933844689338002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1933844689338002e-05

Optimization complete. Final v2v error: 2.9177448749542236 mm

Highest mean error: 3.1281960010528564 mm for frame 77

Lowest mean error: 2.685607671737671 mm for frame 34

Saving results

Total time: 32.08772945404053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913144
Iteration 2/25 | Loss: 0.00283164
Iteration 3/25 | Loss: 0.00178054
Iteration 4/25 | Loss: 0.00132081
Iteration 5/25 | Loss: 0.00127390
Iteration 6/25 | Loss: 0.00120610
Iteration 7/25 | Loss: 0.00119151
Iteration 8/25 | Loss: 0.00114711
Iteration 9/25 | Loss: 0.00113368
Iteration 10/25 | Loss: 0.00113339
Iteration 11/25 | Loss: 0.00112731
Iteration 12/25 | Loss: 0.00112540
Iteration 13/25 | Loss: 0.00112251
Iteration 14/25 | Loss: 0.00112189
Iteration 15/25 | Loss: 0.00112016
Iteration 16/25 | Loss: 0.00111958
Iteration 17/25 | Loss: 0.00111993
Iteration 18/25 | Loss: 0.00111921
Iteration 19/25 | Loss: 0.00111930
Iteration 20/25 | Loss: 0.00111876
Iteration 21/25 | Loss: 0.00111924
Iteration 22/25 | Loss: 0.00111875
Iteration 23/25 | Loss: 0.00111870
Iteration 24/25 | Loss: 0.00111826
Iteration 25/25 | Loss: 0.00111844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43288851
Iteration 2/25 | Loss: 0.00364284
Iteration 3/25 | Loss: 0.00214867
Iteration 4/25 | Loss: 0.00214863
Iteration 5/25 | Loss: 0.00214862
Iteration 6/25 | Loss: 0.00214862
Iteration 7/25 | Loss: 0.00214862
Iteration 8/25 | Loss: 0.00214862
Iteration 9/25 | Loss: 0.00214862
Iteration 10/25 | Loss: 0.00214862
Iteration 11/25 | Loss: 0.00214862
Iteration 12/25 | Loss: 0.00214862
Iteration 13/25 | Loss: 0.00214862
Iteration 14/25 | Loss: 0.00214862
Iteration 15/25 | Loss: 0.00214862
Iteration 16/25 | Loss: 0.00214862
Iteration 17/25 | Loss: 0.00214862
Iteration 18/25 | Loss: 0.00214862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002148621017113328, 0.002148621017113328, 0.002148621017113328, 0.002148621017113328, 0.002148621017113328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002148621017113328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214862
Iteration 2/1000 | Loss: 0.00395380
Iteration 3/1000 | Loss: 0.00223780
Iteration 4/1000 | Loss: 0.00430229
Iteration 5/1000 | Loss: 0.00254358
Iteration 6/1000 | Loss: 0.00133139
Iteration 7/1000 | Loss: 0.00086200
Iteration 8/1000 | Loss: 0.00062283
Iteration 9/1000 | Loss: 0.00035221
Iteration 10/1000 | Loss: 0.00091129
Iteration 11/1000 | Loss: 0.00215465
Iteration 12/1000 | Loss: 0.00075309
Iteration 13/1000 | Loss: 0.00041091
Iteration 14/1000 | Loss: 0.00051252
Iteration 15/1000 | Loss: 0.00068055
Iteration 16/1000 | Loss: 0.00014049
Iteration 17/1000 | Loss: 0.00092626
Iteration 18/1000 | Loss: 0.00230226
Iteration 19/1000 | Loss: 0.00234741
Iteration 20/1000 | Loss: 0.00075693
Iteration 21/1000 | Loss: 0.00063538
Iteration 22/1000 | Loss: 0.00058335
Iteration 23/1000 | Loss: 0.00011860
Iteration 24/1000 | Loss: 0.00086893
Iteration 25/1000 | Loss: 0.00035110
Iteration 26/1000 | Loss: 0.00009190
Iteration 27/1000 | Loss: 0.00399369
Iteration 28/1000 | Loss: 0.00192298
Iteration 29/1000 | Loss: 0.00129111
Iteration 30/1000 | Loss: 0.00333253
Iteration 31/1000 | Loss: 0.00151127
Iteration 32/1000 | Loss: 0.00150900
Iteration 33/1000 | Loss: 0.00164628
Iteration 34/1000 | Loss: 0.00189305
Iteration 35/1000 | Loss: 0.00312810
Iteration 36/1000 | Loss: 0.00187424
Iteration 37/1000 | Loss: 0.00117978
Iteration 38/1000 | Loss: 0.00015445
Iteration 39/1000 | Loss: 0.00010289
Iteration 40/1000 | Loss: 0.00008253
Iteration 41/1000 | Loss: 0.00092766
Iteration 42/1000 | Loss: 0.00105120
Iteration 43/1000 | Loss: 0.00099720
Iteration 44/1000 | Loss: 0.00097828
Iteration 45/1000 | Loss: 0.00250409
Iteration 46/1000 | Loss: 0.00147867
Iteration 47/1000 | Loss: 0.00205824
Iteration 48/1000 | Loss: 0.00109330
Iteration 49/1000 | Loss: 0.00036070
Iteration 50/1000 | Loss: 0.00085437
Iteration 51/1000 | Loss: 0.00012018
Iteration 52/1000 | Loss: 0.00099320
Iteration 53/1000 | Loss: 0.00060948
Iteration 54/1000 | Loss: 0.00077651
Iteration 55/1000 | Loss: 0.00044151
Iteration 56/1000 | Loss: 0.00051427
Iteration 57/1000 | Loss: 0.00031815
Iteration 58/1000 | Loss: 0.00037827
Iteration 59/1000 | Loss: 0.00084598
Iteration 60/1000 | Loss: 0.00038272
Iteration 61/1000 | Loss: 0.00027583
Iteration 62/1000 | Loss: 0.00009803
Iteration 63/1000 | Loss: 0.00024625
Iteration 64/1000 | Loss: 0.00034681
Iteration 65/1000 | Loss: 0.00066591
Iteration 66/1000 | Loss: 0.00065205
Iteration 67/1000 | Loss: 0.00131415
Iteration 68/1000 | Loss: 0.00071485
Iteration 69/1000 | Loss: 0.00105182
Iteration 70/1000 | Loss: 0.00098861
Iteration 71/1000 | Loss: 0.00092876
Iteration 72/1000 | Loss: 0.00126808
Iteration 73/1000 | Loss: 0.00061748
Iteration 74/1000 | Loss: 0.00020437
Iteration 75/1000 | Loss: 0.00087370
Iteration 76/1000 | Loss: 0.00068566
Iteration 77/1000 | Loss: 0.00087883
Iteration 78/1000 | Loss: 0.00024192
Iteration 79/1000 | Loss: 0.00010317
Iteration 80/1000 | Loss: 0.00048503
Iteration 81/1000 | Loss: 0.00063682
Iteration 82/1000 | Loss: 0.00107546
Iteration 83/1000 | Loss: 0.00028468
Iteration 84/1000 | Loss: 0.00008281
Iteration 85/1000 | Loss: 0.00062140
Iteration 86/1000 | Loss: 0.00006963
Iteration 87/1000 | Loss: 0.00072428
Iteration 88/1000 | Loss: 0.00160701
Iteration 89/1000 | Loss: 0.00109002
Iteration 90/1000 | Loss: 0.00060575
Iteration 91/1000 | Loss: 0.00128412
Iteration 92/1000 | Loss: 0.00050325
Iteration 93/1000 | Loss: 0.00106834
Iteration 94/1000 | Loss: 0.00075182
Iteration 95/1000 | Loss: 0.00048787
Iteration 96/1000 | Loss: 0.00009412
Iteration 97/1000 | Loss: 0.00066036
Iteration 98/1000 | Loss: 0.00076389
Iteration 99/1000 | Loss: 0.00080840
Iteration 100/1000 | Loss: 0.00046268
Iteration 101/1000 | Loss: 0.00022303
Iteration 102/1000 | Loss: 0.00035770
Iteration 103/1000 | Loss: 0.00008714
Iteration 104/1000 | Loss: 0.00023634
Iteration 105/1000 | Loss: 0.00032888
Iteration 106/1000 | Loss: 0.00061327
Iteration 107/1000 | Loss: 0.00031297
Iteration 108/1000 | Loss: 0.00012477
Iteration 109/1000 | Loss: 0.00063177
Iteration 110/1000 | Loss: 0.00006826
Iteration 111/1000 | Loss: 0.00038242
Iteration 112/1000 | Loss: 0.00027954
Iteration 113/1000 | Loss: 0.00026952
Iteration 114/1000 | Loss: 0.00055400
Iteration 115/1000 | Loss: 0.00056024
Iteration 116/1000 | Loss: 0.00026753
Iteration 117/1000 | Loss: 0.00038598
Iteration 118/1000 | Loss: 0.00020478
Iteration 119/1000 | Loss: 0.00069806
Iteration 120/1000 | Loss: 0.00114927
Iteration 121/1000 | Loss: 0.00201817
Iteration 122/1000 | Loss: 0.00092555
Iteration 123/1000 | Loss: 0.00007227
Iteration 124/1000 | Loss: 0.00006014
Iteration 125/1000 | Loss: 0.00071676
Iteration 126/1000 | Loss: 0.00052960
Iteration 127/1000 | Loss: 0.00085956
Iteration 128/1000 | Loss: 0.00005572
Iteration 129/1000 | Loss: 0.00005168
Iteration 130/1000 | Loss: 0.00005114
Iteration 131/1000 | Loss: 0.00004673
Iteration 132/1000 | Loss: 0.00045316
Iteration 133/1000 | Loss: 0.00013961
Iteration 134/1000 | Loss: 0.00004423
Iteration 135/1000 | Loss: 0.00004338
Iteration 136/1000 | Loss: 0.00004260
Iteration 137/1000 | Loss: 0.00042673
Iteration 138/1000 | Loss: 0.00159334
Iteration 139/1000 | Loss: 0.00090590
Iteration 140/1000 | Loss: 0.00007621
Iteration 141/1000 | Loss: 0.00005255
Iteration 142/1000 | Loss: 0.00004437
Iteration 143/1000 | Loss: 0.00059590
Iteration 144/1000 | Loss: 0.00041359
Iteration 145/1000 | Loss: 0.00041449
Iteration 146/1000 | Loss: 0.00031355
Iteration 147/1000 | Loss: 0.00004479
Iteration 148/1000 | Loss: 0.00004193
Iteration 149/1000 | Loss: 0.00004148
Iteration 150/1000 | Loss: 0.00071844
Iteration 151/1000 | Loss: 0.00292741
Iteration 152/1000 | Loss: 0.00165078
Iteration 153/1000 | Loss: 0.00066518
Iteration 154/1000 | Loss: 0.00043346
Iteration 155/1000 | Loss: 0.00027037
Iteration 156/1000 | Loss: 0.00132558
Iteration 157/1000 | Loss: 0.00046347
Iteration 158/1000 | Loss: 0.00083771
Iteration 159/1000 | Loss: 0.00072605
Iteration 160/1000 | Loss: 0.00036009
Iteration 161/1000 | Loss: 0.00004180
Iteration 162/1000 | Loss: 0.00061954
Iteration 163/1000 | Loss: 0.00044100
Iteration 164/1000 | Loss: 0.00051554
Iteration 165/1000 | Loss: 0.00061325
Iteration 166/1000 | Loss: 0.00046663
Iteration 167/1000 | Loss: 0.00042434
Iteration 168/1000 | Loss: 0.00004939
Iteration 169/1000 | Loss: 0.00060072
Iteration 170/1000 | Loss: 0.00027095
Iteration 171/1000 | Loss: 0.00005249
Iteration 172/1000 | Loss: 0.00004218
Iteration 173/1000 | Loss: 0.00035904
Iteration 174/1000 | Loss: 0.00010326
Iteration 175/1000 | Loss: 0.00003721
Iteration 176/1000 | Loss: 0.00003152
Iteration 177/1000 | Loss: 0.00003127
Iteration 178/1000 | Loss: 0.00002784
Iteration 179/1000 | Loss: 0.00068742
Iteration 180/1000 | Loss: 0.00011829
Iteration 181/1000 | Loss: 0.00016982
Iteration 182/1000 | Loss: 0.00021490
Iteration 183/1000 | Loss: 0.00003777
Iteration 184/1000 | Loss: 0.00003324
Iteration 185/1000 | Loss: 0.00002852
Iteration 186/1000 | Loss: 0.00004616
Iteration 187/1000 | Loss: 0.00002660
Iteration 188/1000 | Loss: 0.00002721
Iteration 189/1000 | Loss: 0.00002564
Iteration 190/1000 | Loss: 0.00002458
Iteration 191/1000 | Loss: 0.00078266
Iteration 192/1000 | Loss: 0.00031064
Iteration 193/1000 | Loss: 0.00004784
Iteration 194/1000 | Loss: 0.00068666
Iteration 195/1000 | Loss: 0.00006849
Iteration 196/1000 | Loss: 0.00006260
Iteration 197/1000 | Loss: 0.00002550
Iteration 198/1000 | Loss: 0.00002461
Iteration 199/1000 | Loss: 0.00002250
Iteration 200/1000 | Loss: 0.00002183
Iteration 201/1000 | Loss: 0.00002155
Iteration 202/1000 | Loss: 0.00002126
Iteration 203/1000 | Loss: 0.00002314
Iteration 204/1000 | Loss: 0.00002151
Iteration 205/1000 | Loss: 0.00002101
Iteration 206/1000 | Loss: 0.00002075
Iteration 207/1000 | Loss: 0.00002287
Iteration 208/1000 | Loss: 0.00002094
Iteration 209/1000 | Loss: 0.00002050
Iteration 210/1000 | Loss: 0.00002050
Iteration 211/1000 | Loss: 0.00002050
Iteration 212/1000 | Loss: 0.00002050
Iteration 213/1000 | Loss: 0.00002050
Iteration 214/1000 | Loss: 0.00002050
Iteration 215/1000 | Loss: 0.00002050
Iteration 216/1000 | Loss: 0.00002050
Iteration 217/1000 | Loss: 0.00002050
Iteration 218/1000 | Loss: 0.00002050
Iteration 219/1000 | Loss: 0.00002049
Iteration 220/1000 | Loss: 0.00002049
Iteration 221/1000 | Loss: 0.00002049
Iteration 222/1000 | Loss: 0.00002049
Iteration 223/1000 | Loss: 0.00002049
Iteration 224/1000 | Loss: 0.00002048
Iteration 225/1000 | Loss: 0.00002048
Iteration 226/1000 | Loss: 0.00002048
Iteration 227/1000 | Loss: 0.00002048
Iteration 228/1000 | Loss: 0.00002048
Iteration 229/1000 | Loss: 0.00002048
Iteration 230/1000 | Loss: 0.00002047
Iteration 231/1000 | Loss: 0.00002047
Iteration 232/1000 | Loss: 0.00002047
Iteration 233/1000 | Loss: 0.00002047
Iteration 234/1000 | Loss: 0.00002046
Iteration 235/1000 | Loss: 0.00002044
Iteration 236/1000 | Loss: 0.00002043
Iteration 237/1000 | Loss: 0.00002043
Iteration 238/1000 | Loss: 0.00002043
Iteration 239/1000 | Loss: 0.00002043
Iteration 240/1000 | Loss: 0.00002043
Iteration 241/1000 | Loss: 0.00002043
Iteration 242/1000 | Loss: 0.00002043
Iteration 243/1000 | Loss: 0.00002043
Iteration 244/1000 | Loss: 0.00002043
Iteration 245/1000 | Loss: 0.00002043
Iteration 246/1000 | Loss: 0.00002043
Iteration 247/1000 | Loss: 0.00002042
Iteration 248/1000 | Loss: 0.00002042
Iteration 249/1000 | Loss: 0.00002239
Iteration 250/1000 | Loss: 0.00002238
Iteration 251/1000 | Loss: 0.00002088
Iteration 252/1000 | Loss: 0.00002037
Iteration 253/1000 | Loss: 0.00002036
Iteration 254/1000 | Loss: 0.00002036
Iteration 255/1000 | Loss: 0.00002036
Iteration 256/1000 | Loss: 0.00002036
Iteration 257/1000 | Loss: 0.00002036
Iteration 258/1000 | Loss: 0.00002036
Iteration 259/1000 | Loss: 0.00002195
Iteration 260/1000 | Loss: 0.00002114
Iteration 261/1000 | Loss: 0.00002208
Iteration 262/1000 | Loss: 0.00002137
Iteration 263/1000 | Loss: 0.00002209
Iteration 264/1000 | Loss: 0.00002205
Iteration 265/1000 | Loss: 0.00002225
Iteration 266/1000 | Loss: 0.00002213
Iteration 267/1000 | Loss: 0.00002208
Iteration 268/1000 | Loss: 0.00002130
Iteration 269/1000 | Loss: 0.00002209
Iteration 270/1000 | Loss: 0.00002215
Iteration 271/1000 | Loss: 0.00002215
Iteration 272/1000 | Loss: 0.00002200
Iteration 273/1000 | Loss: 0.00002216
Iteration 274/1000 | Loss: 0.00002210
Iteration 275/1000 | Loss: 0.00002214
Iteration 276/1000 | Loss: 0.00002212
Iteration 277/1000 | Loss: 0.00002211
Iteration 278/1000 | Loss: 0.00002164
Iteration 279/1000 | Loss: 0.00002197
Iteration 280/1000 | Loss: 0.00002186
Iteration 281/1000 | Loss: 0.00002194
Iteration 282/1000 | Loss: 0.00002220
Iteration 283/1000 | Loss: 0.00002213
Iteration 284/1000 | Loss: 0.00002231
Iteration 285/1000 | Loss: 0.00002231
Iteration 286/1000 | Loss: 0.00002236
Iteration 287/1000 | Loss: 0.00002210
Iteration 288/1000 | Loss: 0.00002197
Iteration 289/1000 | Loss: 0.00002209
Iteration 290/1000 | Loss: 0.00002208
Iteration 291/1000 | Loss: 0.00002209
Iteration 292/1000 | Loss: 0.00002224
Iteration 293/1000 | Loss: 0.00002223
Iteration 294/1000 | Loss: 0.00002223
Iteration 295/1000 | Loss: 0.00002137
Iteration 296/1000 | Loss: 0.00002180
Iteration 297/1000 | Loss: 0.00002151
Iteration 298/1000 | Loss: 0.00002221
Iteration 299/1000 | Loss: 0.00002234
Iteration 300/1000 | Loss: 0.00002225
Iteration 301/1000 | Loss: 0.00002239
Iteration 302/1000 | Loss: 0.00002234
Iteration 303/1000 | Loss: 0.00002233
Iteration 304/1000 | Loss: 0.00002145
Iteration 305/1000 | Loss: 0.00002200
Iteration 306/1000 | Loss: 0.00002243
Iteration 307/1000 | Loss: 0.00002082
Iteration 308/1000 | Loss: 0.00002205
Iteration 309/1000 | Loss: 0.00002205
Iteration 310/1000 | Loss: 0.00002179
Iteration 311/1000 | Loss: 0.00002210
Iteration 312/1000 | Loss: 0.00002242
Iteration 313/1000 | Loss: 0.00002232
Iteration 314/1000 | Loss: 0.00002217
Iteration 315/1000 | Loss: 0.00002290
Iteration 316/1000 | Loss: 0.00002243
Iteration 317/1000 | Loss: 0.00002246
Iteration 318/1000 | Loss: 0.00002222
Iteration 319/1000 | Loss: 0.00002270
Iteration 320/1000 | Loss: 0.00002243
Iteration 321/1000 | Loss: 0.00002249
Iteration 322/1000 | Loss: 0.00002215
Iteration 323/1000 | Loss: 0.00002282
Iteration 324/1000 | Loss: 0.00002210
Iteration 325/1000 | Loss: 0.00002251
Iteration 326/1000 | Loss: 0.00002255
Iteration 327/1000 | Loss: 0.00002261
Iteration 328/1000 | Loss: 0.00002235
Iteration 329/1000 | Loss: 0.00002234
Iteration 330/1000 | Loss: 0.00002089
Iteration 331/1000 | Loss: 0.00002330
Iteration 332/1000 | Loss: 0.00002215
Iteration 333/1000 | Loss: 0.00002307
Iteration 334/1000 | Loss: 0.00002208
Iteration 335/1000 | Loss: 0.00002272
Iteration 336/1000 | Loss: 0.00002219
Iteration 337/1000 | Loss: 0.00002257
Iteration 338/1000 | Loss: 0.00002246
Iteration 339/1000 | Loss: 0.00002284
Iteration 340/1000 | Loss: 0.00002245
Iteration 341/1000 | Loss: 0.00002292
Iteration 342/1000 | Loss: 0.00002033
Iteration 343/1000 | Loss: 0.00002229
Iteration 344/1000 | Loss: 0.00002265
Iteration 345/1000 | Loss: 0.00002035
Iteration 346/1000 | Loss: 0.00002257
Iteration 347/1000 | Loss: 0.00002247
Iteration 348/1000 | Loss: 0.00002230
Iteration 349/1000 | Loss: 0.00002276
Iteration 350/1000 | Loss: 0.00002246
Iteration 351/1000 | Loss: 0.00002284
Iteration 352/1000 | Loss: 0.00002245
Iteration 353/1000 | Loss: 0.00002250
Iteration 354/1000 | Loss: 0.00002228
Iteration 355/1000 | Loss: 0.00002287
Iteration 356/1000 | Loss: 0.00002202
Iteration 357/1000 | Loss: 0.00002278
Iteration 358/1000 | Loss: 0.00002258
Iteration 359/1000 | Loss: 0.00002256
Iteration 360/1000 | Loss: 0.00002222
Iteration 361/1000 | Loss: 0.00002278
Iteration 362/1000 | Loss: 0.00002277
Iteration 363/1000 | Loss: 0.00002120
Iteration 364/1000 | Loss: 0.00002259
Iteration 365/1000 | Loss: 0.00002153
Iteration 366/1000 | Loss: 0.00002239
Iteration 367/1000 | Loss: 0.00002043
Iteration 368/1000 | Loss: 0.00002152
Iteration 369/1000 | Loss: 0.00002149
Iteration 370/1000 | Loss: 0.00002233
Iteration 371/1000 | Loss: 0.00002215
Iteration 372/1000 | Loss: 0.00002195
Iteration 373/1000 | Loss: 0.00002220
Iteration 374/1000 | Loss: 0.00002222
Iteration 375/1000 | Loss: 0.00002189
Iteration 376/1000 | Loss: 0.00002232
Iteration 377/1000 | Loss: 0.00002214
Iteration 378/1000 | Loss: 0.00002235
Iteration 379/1000 | Loss: 0.00002231
Iteration 380/1000 | Loss: 0.00002231
Iteration 381/1000 | Loss: 0.00002299
Iteration 382/1000 | Loss: 0.00002190
Iteration 383/1000 | Loss: 0.00002244
Iteration 384/1000 | Loss: 0.00002211
Iteration 385/1000 | Loss: 0.00002222
Iteration 386/1000 | Loss: 0.00002247
Iteration 387/1000 | Loss: 0.00002246
Iteration 388/1000 | Loss: 0.00002208
Iteration 389/1000 | Loss: 0.00002207
Iteration 390/1000 | Loss: 0.00002206
Iteration 391/1000 | Loss: 0.00002207
Iteration 392/1000 | Loss: 0.00002210
Iteration 393/1000 | Loss: 0.00002234
Iteration 394/1000 | Loss: 0.00002221
Iteration 395/1000 | Loss: 0.00002236
Iteration 396/1000 | Loss: 0.00002242
Iteration 397/1000 | Loss: 0.00002225
Iteration 398/1000 | Loss: 0.00002218
Iteration 399/1000 | Loss: 0.00002267
Iteration 400/1000 | Loss: 0.00002208
Iteration 401/1000 | Loss: 0.00002245
Iteration 402/1000 | Loss: 0.00002248
Iteration 403/1000 | Loss: 0.00002237
Iteration 404/1000 | Loss: 0.00002222
Iteration 405/1000 | Loss: 0.00002262
Iteration 406/1000 | Loss: 0.00002238
Iteration 407/1000 | Loss: 0.00002238
Iteration 408/1000 | Loss: 0.00002203
Iteration 409/1000 | Loss: 0.00002206
Iteration 410/1000 | Loss: 0.00002251
Iteration 411/1000 | Loss: 0.00002213
Iteration 412/1000 | Loss: 0.00002225
Iteration 413/1000 | Loss: 0.00002225
Iteration 414/1000 | Loss: 0.00002204
Iteration 415/1000 | Loss: 0.00002230
Iteration 416/1000 | Loss: 0.00002222
Iteration 417/1000 | Loss: 0.00002221
Iteration 418/1000 | Loss: 0.00002235
Iteration 419/1000 | Loss: 0.00002138
Iteration 420/1000 | Loss: 0.00002130
Iteration 421/1000 | Loss: 0.00002214
Iteration 422/1000 | Loss: 0.00002199
Iteration 423/1000 | Loss: 0.00002237
Iteration 424/1000 | Loss: 0.00002210
Iteration 425/1000 | Loss: 0.00002211
Iteration 426/1000 | Loss: 0.00002215
Iteration 427/1000 | Loss: 0.00002225
Iteration 428/1000 | Loss: 0.00002241
Iteration 429/1000 | Loss: 0.00002261
Iteration 430/1000 | Loss: 0.00002231
Iteration 431/1000 | Loss: 0.00002234
Iteration 432/1000 | Loss: 0.00002274
Iteration 433/1000 | Loss: 0.00002228
Iteration 434/1000 | Loss: 0.00002248
Iteration 435/1000 | Loss: 0.00002344
Iteration 436/1000 | Loss: 0.00002230
Iteration 437/1000 | Loss: 0.00002291
Iteration 438/1000 | Loss: 0.00002228
Iteration 439/1000 | Loss: 0.00002258
Iteration 440/1000 | Loss: 0.00002227
Iteration 441/1000 | Loss: 0.00002270
Iteration 442/1000 | Loss: 0.00002250
Iteration 443/1000 | Loss: 0.00002227
Iteration 444/1000 | Loss: 0.00002162
Iteration 445/1000 | Loss: 0.00002251
Iteration 446/1000 | Loss: 0.00002239
Iteration 447/1000 | Loss: 0.00002289
Iteration 448/1000 | Loss: 0.00002228
Iteration 449/1000 | Loss: 0.00002267
Iteration 450/1000 | Loss: 0.00002237
Iteration 451/1000 | Loss: 0.00002230
Iteration 452/1000 | Loss: 0.00002237
Iteration 453/1000 | Loss: 0.00002260
Iteration 454/1000 | Loss: 0.00002225
Iteration 455/1000 | Loss: 0.00002250
Iteration 456/1000 | Loss: 0.00002249
Iteration 457/1000 | Loss: 0.00002169
Iteration 458/1000 | Loss: 0.00002250
Iteration 459/1000 | Loss: 0.00002353
Iteration 460/1000 | Loss: 0.00002163
Iteration 461/1000 | Loss: 0.00002165
Iteration 462/1000 | Loss: 0.00002090
Iteration 463/1000 | Loss: 0.00002199
Iteration 464/1000 | Loss: 0.00002223
Iteration 465/1000 | Loss: 0.00002210
Iteration 466/1000 | Loss: 0.00002156
Iteration 467/1000 | Loss: 0.00002169
Iteration 468/1000 | Loss: 0.00002166
Iteration 469/1000 | Loss: 0.00002235
Iteration 470/1000 | Loss: 0.00002141
Iteration 471/1000 | Loss: 0.00002160
Iteration 472/1000 | Loss: 0.00002139
Iteration 473/1000 | Loss: 0.00002200
Iteration 474/1000 | Loss: 0.00002156
Iteration 475/1000 | Loss: 0.00002180
Iteration 476/1000 | Loss: 0.00002081
Iteration 477/1000 | Loss: 0.00002140
Iteration 478/1000 | Loss: 0.00002148
Iteration 479/1000 | Loss: 0.00002170
Iteration 480/1000 | Loss: 0.00002136
Iteration 481/1000 | Loss: 0.00002187
Iteration 482/1000 | Loss: 0.00002151
Iteration 483/1000 | Loss: 0.00002197
Iteration 484/1000 | Loss: 0.00002197
Iteration 485/1000 | Loss: 0.00002142
Iteration 486/1000 | Loss: 0.00002173
Iteration 487/1000 | Loss: 0.00002178
Iteration 488/1000 | Loss: 0.00002182
Iteration 489/1000 | Loss: 0.00002139
Iteration 490/1000 | Loss: 0.00002157
Iteration 491/1000 | Loss: 0.00002178
Iteration 492/1000 | Loss: 0.00002165
Iteration 493/1000 | Loss: 0.00002165
Iteration 494/1000 | Loss: 0.00002155
Iteration 495/1000 | Loss: 0.00002140
Iteration 496/1000 | Loss: 0.00002173
Iteration 497/1000 | Loss: 0.00002148
Iteration 498/1000 | Loss: 0.00002137
Iteration 499/1000 | Loss: 0.00002136
Iteration 500/1000 | Loss: 0.00002165
Iteration 501/1000 | Loss: 0.00002193
Iteration 502/1000 | Loss: 0.00002198
Iteration 503/1000 | Loss: 0.00002198
Iteration 504/1000 | Loss: 0.00002216
Iteration 505/1000 | Loss: 0.00002185
Iteration 506/1000 | Loss: 0.00002251
Iteration 507/1000 | Loss: 0.00002217
Iteration 508/1000 | Loss: 0.00002265
Iteration 509/1000 | Loss: 0.00002021
Iteration 510/1000 | Loss: 0.00002021
Iteration 511/1000 | Loss: 0.00002021
Iteration 512/1000 | Loss: 0.00002021
Iteration 513/1000 | Loss: 0.00002021
Iteration 514/1000 | Loss: 0.00002021
Iteration 515/1000 | Loss: 0.00002021
Iteration 516/1000 | Loss: 0.00002021
Iteration 517/1000 | Loss: 0.00002021
Iteration 518/1000 | Loss: 0.00002021
Iteration 519/1000 | Loss: 0.00002021
Iteration 520/1000 | Loss: 0.00002021
Iteration 521/1000 | Loss: 0.00002021
Iteration 522/1000 | Loss: 0.00002021
Iteration 523/1000 | Loss: 0.00002021
Iteration 524/1000 | Loss: 0.00002021
Iteration 525/1000 | Loss: 0.00002021
Iteration 526/1000 | Loss: 0.00002021
Iteration 527/1000 | Loss: 0.00002021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 527. Stopping optimization.
Last 5 losses: [2.0205117834848352e-05, 2.0205117834848352e-05, 2.0205117834848352e-05, 2.0205117834848352e-05, 2.0205117834848352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0205117834848352e-05

Optimization complete. Final v2v error: 3.0807886123657227 mm

Highest mean error: 12.72249984741211 mm for frame 56

Lowest mean error: 2.1150476932525635 mm for frame 155

Saving results

Total time: 621.9560151100159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064097
Iteration 2/25 | Loss: 0.00146213
Iteration 3/25 | Loss: 0.00116240
Iteration 4/25 | Loss: 0.00112989
Iteration 5/25 | Loss: 0.00111952
Iteration 6/25 | Loss: 0.00111699
Iteration 7/25 | Loss: 0.00111616
Iteration 8/25 | Loss: 0.00111616
Iteration 9/25 | Loss: 0.00111616
Iteration 10/25 | Loss: 0.00111616
Iteration 11/25 | Loss: 0.00111616
Iteration 12/25 | Loss: 0.00111616
Iteration 13/25 | Loss: 0.00111616
Iteration 14/25 | Loss: 0.00111616
Iteration 15/25 | Loss: 0.00111616
Iteration 16/25 | Loss: 0.00111616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011161562288179994, 0.0011161562288179994, 0.0011161562288179994, 0.0011161562288179994, 0.0011161562288179994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011161562288179994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72073811
Iteration 2/25 | Loss: 0.00086649
Iteration 3/25 | Loss: 0.00086645
Iteration 4/25 | Loss: 0.00086645
Iteration 5/25 | Loss: 0.00086645
Iteration 6/25 | Loss: 0.00086645
Iteration 7/25 | Loss: 0.00086645
Iteration 8/25 | Loss: 0.00086645
Iteration 9/25 | Loss: 0.00086645
Iteration 10/25 | Loss: 0.00086645
Iteration 11/25 | Loss: 0.00086645
Iteration 12/25 | Loss: 0.00086645
Iteration 13/25 | Loss: 0.00086645
Iteration 14/25 | Loss: 0.00086645
Iteration 15/25 | Loss: 0.00086645
Iteration 16/25 | Loss: 0.00086645
Iteration 17/25 | Loss: 0.00086645
Iteration 18/25 | Loss: 0.00086645
Iteration 19/25 | Loss: 0.00086645
Iteration 20/25 | Loss: 0.00086645
Iteration 21/25 | Loss: 0.00086645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008664510096423328, 0.0008664510096423328, 0.0008664510096423328, 0.0008664510096423328, 0.0008664510096423328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008664510096423328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086645
Iteration 2/1000 | Loss: 0.00006398
Iteration 3/1000 | Loss: 0.00004562
Iteration 4/1000 | Loss: 0.00003958
Iteration 5/1000 | Loss: 0.00003786
Iteration 6/1000 | Loss: 0.00003633
Iteration 7/1000 | Loss: 0.00003571
Iteration 8/1000 | Loss: 0.00003518
Iteration 9/1000 | Loss: 0.00003472
Iteration 10/1000 | Loss: 0.00003428
Iteration 11/1000 | Loss: 0.00003397
Iteration 12/1000 | Loss: 0.00003374
Iteration 13/1000 | Loss: 0.00003353
Iteration 14/1000 | Loss: 0.00003334
Iteration 15/1000 | Loss: 0.00003320
Iteration 16/1000 | Loss: 0.00003310
Iteration 17/1000 | Loss: 0.00003310
Iteration 18/1000 | Loss: 0.00003309
Iteration 19/1000 | Loss: 0.00003309
Iteration 20/1000 | Loss: 0.00003307
Iteration 21/1000 | Loss: 0.00003307
Iteration 22/1000 | Loss: 0.00003307
Iteration 23/1000 | Loss: 0.00003307
Iteration 24/1000 | Loss: 0.00003307
Iteration 25/1000 | Loss: 0.00003306
Iteration 26/1000 | Loss: 0.00003306
Iteration 27/1000 | Loss: 0.00003306
Iteration 28/1000 | Loss: 0.00003306
Iteration 29/1000 | Loss: 0.00003306
Iteration 30/1000 | Loss: 0.00003303
Iteration 31/1000 | Loss: 0.00003301
Iteration 32/1000 | Loss: 0.00003292
Iteration 33/1000 | Loss: 0.00003289
Iteration 34/1000 | Loss: 0.00003289
Iteration 35/1000 | Loss: 0.00003289
Iteration 36/1000 | Loss: 0.00003288
Iteration 37/1000 | Loss: 0.00003288
Iteration 38/1000 | Loss: 0.00003288
Iteration 39/1000 | Loss: 0.00003288
Iteration 40/1000 | Loss: 0.00003288
Iteration 41/1000 | Loss: 0.00003288
Iteration 42/1000 | Loss: 0.00003288
Iteration 43/1000 | Loss: 0.00003287
Iteration 44/1000 | Loss: 0.00003287
Iteration 45/1000 | Loss: 0.00003286
Iteration 46/1000 | Loss: 0.00003285
Iteration 47/1000 | Loss: 0.00003284
Iteration 48/1000 | Loss: 0.00003281
Iteration 49/1000 | Loss: 0.00003278
Iteration 50/1000 | Loss: 0.00003278
Iteration 51/1000 | Loss: 0.00003277
Iteration 52/1000 | Loss: 0.00003277
Iteration 53/1000 | Loss: 0.00003277
Iteration 54/1000 | Loss: 0.00003276
Iteration 55/1000 | Loss: 0.00003276
Iteration 56/1000 | Loss: 0.00003276
Iteration 57/1000 | Loss: 0.00003276
Iteration 58/1000 | Loss: 0.00003276
Iteration 59/1000 | Loss: 0.00003275
Iteration 60/1000 | Loss: 0.00003273
Iteration 61/1000 | Loss: 0.00003273
Iteration 62/1000 | Loss: 0.00003273
Iteration 63/1000 | Loss: 0.00003272
Iteration 64/1000 | Loss: 0.00003272
Iteration 65/1000 | Loss: 0.00003272
Iteration 66/1000 | Loss: 0.00003272
Iteration 67/1000 | Loss: 0.00003272
Iteration 68/1000 | Loss: 0.00003272
Iteration 69/1000 | Loss: 0.00003272
Iteration 70/1000 | Loss: 0.00003271
Iteration 71/1000 | Loss: 0.00003270
Iteration 72/1000 | Loss: 0.00003269
Iteration 73/1000 | Loss: 0.00003269
Iteration 74/1000 | Loss: 0.00003269
Iteration 75/1000 | Loss: 0.00003269
Iteration 76/1000 | Loss: 0.00003269
Iteration 77/1000 | Loss: 0.00003269
Iteration 78/1000 | Loss: 0.00003268
Iteration 79/1000 | Loss: 0.00003268
Iteration 80/1000 | Loss: 0.00003268
Iteration 81/1000 | Loss: 0.00003268
Iteration 82/1000 | Loss: 0.00003268
Iteration 83/1000 | Loss: 0.00003268
Iteration 84/1000 | Loss: 0.00003268
Iteration 85/1000 | Loss: 0.00003268
Iteration 86/1000 | Loss: 0.00003268
Iteration 87/1000 | Loss: 0.00003267
Iteration 88/1000 | Loss: 0.00003267
Iteration 89/1000 | Loss: 0.00003267
Iteration 90/1000 | Loss: 0.00003267
Iteration 91/1000 | Loss: 0.00003267
Iteration 92/1000 | Loss: 0.00003267
Iteration 93/1000 | Loss: 0.00003267
Iteration 94/1000 | Loss: 0.00003267
Iteration 95/1000 | Loss: 0.00003266
Iteration 96/1000 | Loss: 0.00003266
Iteration 97/1000 | Loss: 0.00003265
Iteration 98/1000 | Loss: 0.00003265
Iteration 99/1000 | Loss: 0.00003265
Iteration 100/1000 | Loss: 0.00003263
Iteration 101/1000 | Loss: 0.00003263
Iteration 102/1000 | Loss: 0.00003263
Iteration 103/1000 | Loss: 0.00003263
Iteration 104/1000 | Loss: 0.00003263
Iteration 105/1000 | Loss: 0.00003263
Iteration 106/1000 | Loss: 0.00003263
Iteration 107/1000 | Loss: 0.00003263
Iteration 108/1000 | Loss: 0.00003263
Iteration 109/1000 | Loss: 0.00003262
Iteration 110/1000 | Loss: 0.00003262
Iteration 111/1000 | Loss: 0.00003262
Iteration 112/1000 | Loss: 0.00003262
Iteration 113/1000 | Loss: 0.00003262
Iteration 114/1000 | Loss: 0.00003262
Iteration 115/1000 | Loss: 0.00003262
Iteration 116/1000 | Loss: 0.00003262
Iteration 117/1000 | Loss: 0.00003262
Iteration 118/1000 | Loss: 0.00003261
Iteration 119/1000 | Loss: 0.00003260
Iteration 120/1000 | Loss: 0.00003260
Iteration 121/1000 | Loss: 0.00003260
Iteration 122/1000 | Loss: 0.00003260
Iteration 123/1000 | Loss: 0.00003260
Iteration 124/1000 | Loss: 0.00003260
Iteration 125/1000 | Loss: 0.00003259
Iteration 126/1000 | Loss: 0.00003259
Iteration 127/1000 | Loss: 0.00003259
Iteration 128/1000 | Loss: 0.00003259
Iteration 129/1000 | Loss: 0.00003259
Iteration 130/1000 | Loss: 0.00003259
Iteration 131/1000 | Loss: 0.00003259
Iteration 132/1000 | Loss: 0.00003259
Iteration 133/1000 | Loss: 0.00003259
Iteration 134/1000 | Loss: 0.00003258
Iteration 135/1000 | Loss: 0.00003258
Iteration 136/1000 | Loss: 0.00003258
Iteration 137/1000 | Loss: 0.00003258
Iteration 138/1000 | Loss: 0.00003258
Iteration 139/1000 | Loss: 0.00003258
Iteration 140/1000 | Loss: 0.00003258
Iteration 141/1000 | Loss: 0.00003258
Iteration 142/1000 | Loss: 0.00003257
Iteration 143/1000 | Loss: 0.00003257
Iteration 144/1000 | Loss: 0.00003257
Iteration 145/1000 | Loss: 0.00003257
Iteration 146/1000 | Loss: 0.00003257
Iteration 147/1000 | Loss: 0.00003256
Iteration 148/1000 | Loss: 0.00003256
Iteration 149/1000 | Loss: 0.00003256
Iteration 150/1000 | Loss: 0.00003256
Iteration 151/1000 | Loss: 0.00003256
Iteration 152/1000 | Loss: 0.00003256
Iteration 153/1000 | Loss: 0.00003256
Iteration 154/1000 | Loss: 0.00003256
Iteration 155/1000 | Loss: 0.00003256
Iteration 156/1000 | Loss: 0.00003256
Iteration 157/1000 | Loss: 0.00003256
Iteration 158/1000 | Loss: 0.00003256
Iteration 159/1000 | Loss: 0.00003256
Iteration 160/1000 | Loss: 0.00003255
Iteration 161/1000 | Loss: 0.00003255
Iteration 162/1000 | Loss: 0.00003255
Iteration 163/1000 | Loss: 0.00003254
Iteration 164/1000 | Loss: 0.00003254
Iteration 165/1000 | Loss: 0.00003253
Iteration 166/1000 | Loss: 0.00003253
Iteration 167/1000 | Loss: 0.00003253
Iteration 168/1000 | Loss: 0.00003253
Iteration 169/1000 | Loss: 0.00003253
Iteration 170/1000 | Loss: 0.00003253
Iteration 171/1000 | Loss: 0.00003253
Iteration 172/1000 | Loss: 0.00003253
Iteration 173/1000 | Loss: 0.00003252
Iteration 174/1000 | Loss: 0.00003252
Iteration 175/1000 | Loss: 0.00003252
Iteration 176/1000 | Loss: 0.00003252
Iteration 177/1000 | Loss: 0.00003251
Iteration 178/1000 | Loss: 0.00003251
Iteration 179/1000 | Loss: 0.00003251
Iteration 180/1000 | Loss: 0.00003251
Iteration 181/1000 | Loss: 0.00003251
Iteration 182/1000 | Loss: 0.00003251
Iteration 183/1000 | Loss: 0.00003251
Iteration 184/1000 | Loss: 0.00003251
Iteration 185/1000 | Loss: 0.00003251
Iteration 186/1000 | Loss: 0.00003250
Iteration 187/1000 | Loss: 0.00003250
Iteration 188/1000 | Loss: 0.00003250
Iteration 189/1000 | Loss: 0.00003250
Iteration 190/1000 | Loss: 0.00003250
Iteration 191/1000 | Loss: 0.00003250
Iteration 192/1000 | Loss: 0.00003250
Iteration 193/1000 | Loss: 0.00003250
Iteration 194/1000 | Loss: 0.00003250
Iteration 195/1000 | Loss: 0.00003250
Iteration 196/1000 | Loss: 0.00003250
Iteration 197/1000 | Loss: 0.00003250
Iteration 198/1000 | Loss: 0.00003249
Iteration 199/1000 | Loss: 0.00003249
Iteration 200/1000 | Loss: 0.00003249
Iteration 201/1000 | Loss: 0.00003249
Iteration 202/1000 | Loss: 0.00003249
Iteration 203/1000 | Loss: 0.00003249
Iteration 204/1000 | Loss: 0.00003249
Iteration 205/1000 | Loss: 0.00003249
Iteration 206/1000 | Loss: 0.00003249
Iteration 207/1000 | Loss: 0.00003249
Iteration 208/1000 | Loss: 0.00003249
Iteration 209/1000 | Loss: 0.00003249
Iteration 210/1000 | Loss: 0.00003248
Iteration 211/1000 | Loss: 0.00003248
Iteration 212/1000 | Loss: 0.00003248
Iteration 213/1000 | Loss: 0.00003248
Iteration 214/1000 | Loss: 0.00003248
Iteration 215/1000 | Loss: 0.00003248
Iteration 216/1000 | Loss: 0.00003248
Iteration 217/1000 | Loss: 0.00003248
Iteration 218/1000 | Loss: 0.00003248
Iteration 219/1000 | Loss: 0.00003248
Iteration 220/1000 | Loss: 0.00003248
Iteration 221/1000 | Loss: 0.00003248
Iteration 222/1000 | Loss: 0.00003248
Iteration 223/1000 | Loss: 0.00003248
Iteration 224/1000 | Loss: 0.00003247
Iteration 225/1000 | Loss: 0.00003247
Iteration 226/1000 | Loss: 0.00003247
Iteration 227/1000 | Loss: 0.00003247
Iteration 228/1000 | Loss: 0.00003247
Iteration 229/1000 | Loss: 0.00003246
Iteration 230/1000 | Loss: 0.00003246
Iteration 231/1000 | Loss: 0.00003246
Iteration 232/1000 | Loss: 0.00003246
Iteration 233/1000 | Loss: 0.00003246
Iteration 234/1000 | Loss: 0.00003245
Iteration 235/1000 | Loss: 0.00003245
Iteration 236/1000 | Loss: 0.00003245
Iteration 237/1000 | Loss: 0.00003245
Iteration 238/1000 | Loss: 0.00003245
Iteration 239/1000 | Loss: 0.00003245
Iteration 240/1000 | Loss: 0.00003245
Iteration 241/1000 | Loss: 0.00003245
Iteration 242/1000 | Loss: 0.00003245
Iteration 243/1000 | Loss: 0.00003245
Iteration 244/1000 | Loss: 0.00003245
Iteration 245/1000 | Loss: 0.00003245
Iteration 246/1000 | Loss: 0.00003244
Iteration 247/1000 | Loss: 0.00003244
Iteration 248/1000 | Loss: 0.00003244
Iteration 249/1000 | Loss: 0.00003244
Iteration 250/1000 | Loss: 0.00003244
Iteration 251/1000 | Loss: 0.00003244
Iteration 252/1000 | Loss: 0.00003244
Iteration 253/1000 | Loss: 0.00003244
Iteration 254/1000 | Loss: 0.00003244
Iteration 255/1000 | Loss: 0.00003244
Iteration 256/1000 | Loss: 0.00003244
Iteration 257/1000 | Loss: 0.00003244
Iteration 258/1000 | Loss: 0.00003244
Iteration 259/1000 | Loss: 0.00003244
Iteration 260/1000 | Loss: 0.00003244
Iteration 261/1000 | Loss: 0.00003244
Iteration 262/1000 | Loss: 0.00003243
Iteration 263/1000 | Loss: 0.00003243
Iteration 264/1000 | Loss: 0.00003243
Iteration 265/1000 | Loss: 0.00003243
Iteration 266/1000 | Loss: 0.00003243
Iteration 267/1000 | Loss: 0.00003243
Iteration 268/1000 | Loss: 0.00003243
Iteration 269/1000 | Loss: 0.00003243
Iteration 270/1000 | Loss: 0.00003243
Iteration 271/1000 | Loss: 0.00003243
Iteration 272/1000 | Loss: 0.00003243
Iteration 273/1000 | Loss: 0.00003243
Iteration 274/1000 | Loss: 0.00003243
Iteration 275/1000 | Loss: 0.00003243
Iteration 276/1000 | Loss: 0.00003243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [3.2431176805403084e-05, 3.2431176805403084e-05, 3.2431176805403084e-05, 3.2431176805403084e-05, 3.2431176805403084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2431176805403084e-05

Optimization complete. Final v2v error: 4.567298412322998 mm

Highest mean error: 5.2810211181640625 mm for frame 0

Lowest mean error: 4.1613545417785645 mm for frame 57

Saving results

Total time: 54.088897943496704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519419
Iteration 2/25 | Loss: 0.00132660
Iteration 3/25 | Loss: 0.00105489
Iteration 4/25 | Loss: 0.00102133
Iteration 5/25 | Loss: 0.00101102
Iteration 6/25 | Loss: 0.00100963
Iteration 7/25 | Loss: 0.00100963
Iteration 8/25 | Loss: 0.00100963
Iteration 9/25 | Loss: 0.00100963
Iteration 10/25 | Loss: 0.00100963
Iteration 11/25 | Loss: 0.00100963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010096317855641246, 0.0010096317855641246, 0.0010096317855641246, 0.0010096317855641246, 0.0010096317855641246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010096317855641246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74687451
Iteration 2/25 | Loss: 0.00061777
Iteration 3/25 | Loss: 0.00061777
Iteration 4/25 | Loss: 0.00061776
Iteration 5/25 | Loss: 0.00061776
Iteration 6/25 | Loss: 0.00061776
Iteration 7/25 | Loss: 0.00061776
Iteration 8/25 | Loss: 0.00061776
Iteration 9/25 | Loss: 0.00061776
Iteration 10/25 | Loss: 0.00061776
Iteration 11/25 | Loss: 0.00061776
Iteration 12/25 | Loss: 0.00061776
Iteration 13/25 | Loss: 0.00061776
Iteration 14/25 | Loss: 0.00061776
Iteration 15/25 | Loss: 0.00061776
Iteration 16/25 | Loss: 0.00061776
Iteration 17/25 | Loss: 0.00061776
Iteration 18/25 | Loss: 0.00061776
Iteration 19/25 | Loss: 0.00061776
Iteration 20/25 | Loss: 0.00061776
Iteration 21/25 | Loss: 0.00061776
Iteration 22/25 | Loss: 0.00061776
Iteration 23/25 | Loss: 0.00061776
Iteration 24/25 | Loss: 0.00061776
Iteration 25/25 | Loss: 0.00061776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061776
Iteration 2/1000 | Loss: 0.00004628
Iteration 3/1000 | Loss: 0.00003322
Iteration 4/1000 | Loss: 0.00003069
Iteration 5/1000 | Loss: 0.00002949
Iteration 6/1000 | Loss: 0.00002845
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002737
Iteration 9/1000 | Loss: 0.00002696
Iteration 10/1000 | Loss: 0.00002676
Iteration 11/1000 | Loss: 0.00002651
Iteration 12/1000 | Loss: 0.00002632
Iteration 13/1000 | Loss: 0.00002612
Iteration 14/1000 | Loss: 0.00002587
Iteration 15/1000 | Loss: 0.00002572
Iteration 16/1000 | Loss: 0.00002556
Iteration 17/1000 | Loss: 0.00002539
Iteration 18/1000 | Loss: 0.00002521
Iteration 19/1000 | Loss: 0.00002497
Iteration 20/1000 | Loss: 0.00002491
Iteration 21/1000 | Loss: 0.00002478
Iteration 22/1000 | Loss: 0.00002462
Iteration 23/1000 | Loss: 0.00002459
Iteration 24/1000 | Loss: 0.00002457
Iteration 25/1000 | Loss: 0.00002455
Iteration 26/1000 | Loss: 0.00002455
Iteration 27/1000 | Loss: 0.00002455
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002455
Iteration 30/1000 | Loss: 0.00002455
Iteration 31/1000 | Loss: 0.00002455
Iteration 32/1000 | Loss: 0.00002455
Iteration 33/1000 | Loss: 0.00002455
Iteration 34/1000 | Loss: 0.00002454
Iteration 35/1000 | Loss: 0.00002454
Iteration 36/1000 | Loss: 0.00002454
Iteration 37/1000 | Loss: 0.00002454
Iteration 38/1000 | Loss: 0.00002454
Iteration 39/1000 | Loss: 0.00002453
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002453
Iteration 42/1000 | Loss: 0.00002453
Iteration 43/1000 | Loss: 0.00002453
Iteration 44/1000 | Loss: 0.00002452
Iteration 45/1000 | Loss: 0.00002451
Iteration 46/1000 | Loss: 0.00002451
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002451
Iteration 49/1000 | Loss: 0.00002451
Iteration 50/1000 | Loss: 0.00002450
Iteration 51/1000 | Loss: 0.00002450
Iteration 52/1000 | Loss: 0.00002449
Iteration 53/1000 | Loss: 0.00002449
Iteration 54/1000 | Loss: 0.00002449
Iteration 55/1000 | Loss: 0.00002449
Iteration 56/1000 | Loss: 0.00002448
Iteration 57/1000 | Loss: 0.00002446
Iteration 58/1000 | Loss: 0.00002445
Iteration 59/1000 | Loss: 0.00002444
Iteration 60/1000 | Loss: 0.00002444
Iteration 61/1000 | Loss: 0.00002444
Iteration 62/1000 | Loss: 0.00002443
Iteration 63/1000 | Loss: 0.00002443
Iteration 64/1000 | Loss: 0.00002443
Iteration 65/1000 | Loss: 0.00002442
Iteration 66/1000 | Loss: 0.00002442
Iteration 67/1000 | Loss: 0.00002442
Iteration 68/1000 | Loss: 0.00002441
Iteration 69/1000 | Loss: 0.00002441
Iteration 70/1000 | Loss: 0.00002441
Iteration 71/1000 | Loss: 0.00002440
Iteration 72/1000 | Loss: 0.00002440
Iteration 73/1000 | Loss: 0.00002440
Iteration 74/1000 | Loss: 0.00002439
Iteration 75/1000 | Loss: 0.00002439
Iteration 76/1000 | Loss: 0.00002439
Iteration 77/1000 | Loss: 0.00002439
Iteration 78/1000 | Loss: 0.00002439
Iteration 79/1000 | Loss: 0.00002439
Iteration 80/1000 | Loss: 0.00002439
Iteration 81/1000 | Loss: 0.00002439
Iteration 82/1000 | Loss: 0.00002439
Iteration 83/1000 | Loss: 0.00002438
Iteration 84/1000 | Loss: 0.00002437
Iteration 85/1000 | Loss: 0.00002437
Iteration 86/1000 | Loss: 0.00002437
Iteration 87/1000 | Loss: 0.00002436
Iteration 88/1000 | Loss: 0.00002436
Iteration 89/1000 | Loss: 0.00002435
Iteration 90/1000 | Loss: 0.00002435
Iteration 91/1000 | Loss: 0.00002434
Iteration 92/1000 | Loss: 0.00002434
Iteration 93/1000 | Loss: 0.00002434
Iteration 94/1000 | Loss: 0.00002434
Iteration 95/1000 | Loss: 0.00002433
Iteration 96/1000 | Loss: 0.00002433
Iteration 97/1000 | Loss: 0.00002433
Iteration 98/1000 | Loss: 0.00002432
Iteration 99/1000 | Loss: 0.00002432
Iteration 100/1000 | Loss: 0.00002432
Iteration 101/1000 | Loss: 0.00002432
Iteration 102/1000 | Loss: 0.00002432
Iteration 103/1000 | Loss: 0.00002431
Iteration 104/1000 | Loss: 0.00002431
Iteration 105/1000 | Loss: 0.00002431
Iteration 106/1000 | Loss: 0.00002430
Iteration 107/1000 | Loss: 0.00002430
Iteration 108/1000 | Loss: 0.00002430
Iteration 109/1000 | Loss: 0.00002430
Iteration 110/1000 | Loss: 0.00002430
Iteration 111/1000 | Loss: 0.00002430
Iteration 112/1000 | Loss: 0.00002430
Iteration 113/1000 | Loss: 0.00002430
Iteration 114/1000 | Loss: 0.00002430
Iteration 115/1000 | Loss: 0.00002429
Iteration 116/1000 | Loss: 0.00002429
Iteration 117/1000 | Loss: 0.00002429
Iteration 118/1000 | Loss: 0.00002429
Iteration 119/1000 | Loss: 0.00002429
Iteration 120/1000 | Loss: 0.00002429
Iteration 121/1000 | Loss: 0.00002429
Iteration 122/1000 | Loss: 0.00002429
Iteration 123/1000 | Loss: 0.00002429
Iteration 124/1000 | Loss: 0.00002429
Iteration 125/1000 | Loss: 0.00002429
Iteration 126/1000 | Loss: 0.00002428
Iteration 127/1000 | Loss: 0.00002428
Iteration 128/1000 | Loss: 0.00002428
Iteration 129/1000 | Loss: 0.00002428
Iteration 130/1000 | Loss: 0.00002428
Iteration 131/1000 | Loss: 0.00002428
Iteration 132/1000 | Loss: 0.00002428
Iteration 133/1000 | Loss: 0.00002428
Iteration 134/1000 | Loss: 0.00002427
Iteration 135/1000 | Loss: 0.00002427
Iteration 136/1000 | Loss: 0.00002427
Iteration 137/1000 | Loss: 0.00002427
Iteration 138/1000 | Loss: 0.00002427
Iteration 139/1000 | Loss: 0.00002427
Iteration 140/1000 | Loss: 0.00002426
Iteration 141/1000 | Loss: 0.00002426
Iteration 142/1000 | Loss: 0.00002426
Iteration 143/1000 | Loss: 0.00002426
Iteration 144/1000 | Loss: 0.00002426
Iteration 145/1000 | Loss: 0.00002426
Iteration 146/1000 | Loss: 0.00002426
Iteration 147/1000 | Loss: 0.00002425
Iteration 148/1000 | Loss: 0.00002425
Iteration 149/1000 | Loss: 0.00002424
Iteration 150/1000 | Loss: 0.00002424
Iteration 151/1000 | Loss: 0.00002424
Iteration 152/1000 | Loss: 0.00002424
Iteration 153/1000 | Loss: 0.00002424
Iteration 154/1000 | Loss: 0.00002424
Iteration 155/1000 | Loss: 0.00002424
Iteration 156/1000 | Loss: 0.00002424
Iteration 157/1000 | Loss: 0.00002423
Iteration 158/1000 | Loss: 0.00002423
Iteration 159/1000 | Loss: 0.00002423
Iteration 160/1000 | Loss: 0.00002423
Iteration 161/1000 | Loss: 0.00002423
Iteration 162/1000 | Loss: 0.00002423
Iteration 163/1000 | Loss: 0.00002423
Iteration 164/1000 | Loss: 0.00002423
Iteration 165/1000 | Loss: 0.00002423
Iteration 166/1000 | Loss: 0.00002423
Iteration 167/1000 | Loss: 0.00002423
Iteration 168/1000 | Loss: 0.00002423
Iteration 169/1000 | Loss: 0.00002423
Iteration 170/1000 | Loss: 0.00002423
Iteration 171/1000 | Loss: 0.00002423
Iteration 172/1000 | Loss: 0.00002423
Iteration 173/1000 | Loss: 0.00002423
Iteration 174/1000 | Loss: 0.00002423
Iteration 175/1000 | Loss: 0.00002423
Iteration 176/1000 | Loss: 0.00002423
Iteration 177/1000 | Loss: 0.00002423
Iteration 178/1000 | Loss: 0.00002423
Iteration 179/1000 | Loss: 0.00002423
Iteration 180/1000 | Loss: 0.00002423
Iteration 181/1000 | Loss: 0.00002423
Iteration 182/1000 | Loss: 0.00002423
Iteration 183/1000 | Loss: 0.00002423
Iteration 184/1000 | Loss: 0.00002423
Iteration 185/1000 | Loss: 0.00002423
Iteration 186/1000 | Loss: 0.00002423
Iteration 187/1000 | Loss: 0.00002423
Iteration 188/1000 | Loss: 0.00002423
Iteration 189/1000 | Loss: 0.00002423
Iteration 190/1000 | Loss: 0.00002423
Iteration 191/1000 | Loss: 0.00002423
Iteration 192/1000 | Loss: 0.00002423
Iteration 193/1000 | Loss: 0.00002423
Iteration 194/1000 | Loss: 0.00002423
Iteration 195/1000 | Loss: 0.00002423
Iteration 196/1000 | Loss: 0.00002423
Iteration 197/1000 | Loss: 0.00002423
Iteration 198/1000 | Loss: 0.00002423
Iteration 199/1000 | Loss: 0.00002423
Iteration 200/1000 | Loss: 0.00002423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.422907346044667e-05, 2.422907346044667e-05, 2.422907346044667e-05, 2.422907346044667e-05, 2.422907346044667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.422907346044667e-05

Optimization complete. Final v2v error: 4.054964542388916 mm

Highest mean error: 4.1822896003723145 mm for frame 16

Lowest mean error: 3.913417100906372 mm for frame 49

Saving results

Total time: 59.87006378173828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955222
Iteration 2/25 | Loss: 0.00178025
Iteration 3/25 | Loss: 0.00133419
Iteration 4/25 | Loss: 0.00125252
Iteration 5/25 | Loss: 0.00122995
Iteration 6/25 | Loss: 0.00118848
Iteration 7/25 | Loss: 0.00112851
Iteration 8/25 | Loss: 0.00111330
Iteration 9/25 | Loss: 0.00110165
Iteration 10/25 | Loss: 0.00109278
Iteration 11/25 | Loss: 0.00109004
Iteration 12/25 | Loss: 0.00109318
Iteration 13/25 | Loss: 0.00108414
Iteration 14/25 | Loss: 0.00108256
Iteration 15/25 | Loss: 0.00108123
Iteration 16/25 | Loss: 0.00108063
Iteration 17/25 | Loss: 0.00108337
Iteration 18/25 | Loss: 0.00108530
Iteration 19/25 | Loss: 0.00108084
Iteration 20/25 | Loss: 0.00108256
Iteration 21/25 | Loss: 0.00107739
Iteration 22/25 | Loss: 0.00107453
Iteration 23/25 | Loss: 0.00107317
Iteration 24/25 | Loss: 0.00107107
Iteration 25/25 | Loss: 0.00107056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33014941
Iteration 2/25 | Loss: 0.00072870
Iteration 3/25 | Loss: 0.00072870
Iteration 4/25 | Loss: 0.00072870
Iteration 5/25 | Loss: 0.00072870
Iteration 6/25 | Loss: 0.00072870
Iteration 7/25 | Loss: 0.00072870
Iteration 8/25 | Loss: 0.00072870
Iteration 9/25 | Loss: 0.00072870
Iteration 10/25 | Loss: 0.00072870
Iteration 11/25 | Loss: 0.00072870
Iteration 12/25 | Loss: 0.00072869
Iteration 13/25 | Loss: 0.00072869
Iteration 14/25 | Loss: 0.00072870
Iteration 15/25 | Loss: 0.00072869
Iteration 16/25 | Loss: 0.00072869
Iteration 17/25 | Loss: 0.00072869
Iteration 18/25 | Loss: 0.00072869
Iteration 19/25 | Loss: 0.00072869
Iteration 20/25 | Loss: 0.00072869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007286949548870325, 0.0007286949548870325, 0.0007286949548870325, 0.0007286949548870325, 0.0007286949548870325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007286949548870325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072869
Iteration 2/1000 | Loss: 0.00034931
Iteration 3/1000 | Loss: 0.00028428
Iteration 4/1000 | Loss: 0.00020697
Iteration 5/1000 | Loss: 0.00017625
Iteration 6/1000 | Loss: 0.00012496
Iteration 7/1000 | Loss: 0.00009557
Iteration 8/1000 | Loss: 0.00008306
Iteration 9/1000 | Loss: 0.00011974
Iteration 10/1000 | Loss: 0.00010745
Iteration 11/1000 | Loss: 0.00017998
Iteration 12/1000 | Loss: 0.00014130
Iteration 13/1000 | Loss: 0.00012887
Iteration 14/1000 | Loss: 0.00011664
Iteration 15/1000 | Loss: 0.00008260
Iteration 16/1000 | Loss: 0.00006808
Iteration 17/1000 | Loss: 0.00009947
Iteration 18/1000 | Loss: 0.00011400
Iteration 19/1000 | Loss: 0.00006408
Iteration 20/1000 | Loss: 0.00005675
Iteration 21/1000 | Loss: 0.00004924
Iteration 22/1000 | Loss: 0.00004504
Iteration 23/1000 | Loss: 0.00004832
Iteration 24/1000 | Loss: 0.00004073
Iteration 25/1000 | Loss: 0.00003818
Iteration 26/1000 | Loss: 0.00015542
Iteration 27/1000 | Loss: 0.00005184
Iteration 28/1000 | Loss: 0.00004537
Iteration 29/1000 | Loss: 0.00003918
Iteration 30/1000 | Loss: 0.00003473
Iteration 31/1000 | Loss: 0.00003346
Iteration 32/1000 | Loss: 0.00003246
Iteration 33/1000 | Loss: 0.00003095
Iteration 34/1000 | Loss: 0.00002999
Iteration 35/1000 | Loss: 0.00002881
Iteration 36/1000 | Loss: 0.00002805
Iteration 37/1000 | Loss: 0.00002748
Iteration 38/1000 | Loss: 0.00002704
Iteration 39/1000 | Loss: 0.00003669
Iteration 40/1000 | Loss: 0.00004119
Iteration 41/1000 | Loss: 0.00003388
Iteration 42/1000 | Loss: 0.00003728
Iteration 43/1000 | Loss: 0.00003194
Iteration 44/1000 | Loss: 0.00003634
Iteration 45/1000 | Loss: 0.00003268
Iteration 46/1000 | Loss: 0.00002737
Iteration 47/1000 | Loss: 0.00002681
Iteration 48/1000 | Loss: 0.00002643
Iteration 49/1000 | Loss: 0.00003076
Iteration 50/1000 | Loss: 0.00002849
Iteration 51/1000 | Loss: 0.00003090
Iteration 52/1000 | Loss: 0.00002909
Iteration 53/1000 | Loss: 0.00003270
Iteration 54/1000 | Loss: 0.00003709
Iteration 55/1000 | Loss: 0.00003778
Iteration 56/1000 | Loss: 0.00003416
Iteration 57/1000 | Loss: 0.00003287
Iteration 58/1000 | Loss: 0.00003381
Iteration 59/1000 | Loss: 0.00005201
Iteration 60/1000 | Loss: 0.00003964
Iteration 61/1000 | Loss: 0.00004749
Iteration 62/1000 | Loss: 0.00003226
Iteration 63/1000 | Loss: 0.00003528
Iteration 64/1000 | Loss: 0.00003127
Iteration 65/1000 | Loss: 0.00003380
Iteration 66/1000 | Loss: 0.00003097
Iteration 67/1000 | Loss: 0.00002782
Iteration 68/1000 | Loss: 0.00002635
Iteration 69/1000 | Loss: 0.00002590
Iteration 70/1000 | Loss: 0.00002578
Iteration 71/1000 | Loss: 0.00002577
Iteration 72/1000 | Loss: 0.00002573
Iteration 73/1000 | Loss: 0.00002543
Iteration 74/1000 | Loss: 0.00002525
Iteration 75/1000 | Loss: 0.00005215
Iteration 76/1000 | Loss: 0.00005219
Iteration 77/1000 | Loss: 0.00003125
Iteration 78/1000 | Loss: 0.00002556
Iteration 79/1000 | Loss: 0.00005198
Iteration 80/1000 | Loss: 0.00004110
Iteration 81/1000 | Loss: 0.00002505
Iteration 82/1000 | Loss: 0.00005012
Iteration 83/1000 | Loss: 0.00004019
Iteration 84/1000 | Loss: 0.00005066
Iteration 85/1000 | Loss: 0.00003788
Iteration 86/1000 | Loss: 0.00005083
Iteration 87/1000 | Loss: 0.00004777
Iteration 88/1000 | Loss: 0.00005643
Iteration 89/1000 | Loss: 0.00005139
Iteration 90/1000 | Loss: 0.00004546
Iteration 91/1000 | Loss: 0.00006266
Iteration 92/1000 | Loss: 0.00005465
Iteration 93/1000 | Loss: 0.00005946
Iteration 94/1000 | Loss: 0.00004268
Iteration 95/1000 | Loss: 0.00006050
Iteration 96/1000 | Loss: 0.00004179
Iteration 97/1000 | Loss: 0.00005813
Iteration 98/1000 | Loss: 0.00005884
Iteration 99/1000 | Loss: 0.00006148
Iteration 100/1000 | Loss: 0.00006253
Iteration 101/1000 | Loss: 0.00005237
Iteration 102/1000 | Loss: 0.00005609
Iteration 103/1000 | Loss: 0.00004734
Iteration 104/1000 | Loss: 0.00004275
Iteration 105/1000 | Loss: 0.00003864
Iteration 106/1000 | Loss: 0.00002623
Iteration 107/1000 | Loss: 0.00002560
Iteration 108/1000 | Loss: 0.00002513
Iteration 109/1000 | Loss: 0.00002477
Iteration 110/1000 | Loss: 0.00002452
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002432
Iteration 113/1000 | Loss: 0.00002428
Iteration 114/1000 | Loss: 0.00002427
Iteration 115/1000 | Loss: 0.00002427
Iteration 116/1000 | Loss: 0.00002426
Iteration 117/1000 | Loss: 0.00002425
Iteration 118/1000 | Loss: 0.00002425
Iteration 119/1000 | Loss: 0.00002423
Iteration 120/1000 | Loss: 0.00002423
Iteration 121/1000 | Loss: 0.00002423
Iteration 122/1000 | Loss: 0.00002423
Iteration 123/1000 | Loss: 0.00002423
Iteration 124/1000 | Loss: 0.00002422
Iteration 125/1000 | Loss: 0.00002422
Iteration 126/1000 | Loss: 0.00002422
Iteration 127/1000 | Loss: 0.00002422
Iteration 128/1000 | Loss: 0.00002422
Iteration 129/1000 | Loss: 0.00002422
Iteration 130/1000 | Loss: 0.00002421
Iteration 131/1000 | Loss: 0.00002421
Iteration 132/1000 | Loss: 0.00002421
Iteration 133/1000 | Loss: 0.00002421
Iteration 134/1000 | Loss: 0.00002421
Iteration 135/1000 | Loss: 0.00002420
Iteration 136/1000 | Loss: 0.00002420
Iteration 137/1000 | Loss: 0.00002420
Iteration 138/1000 | Loss: 0.00002419
Iteration 139/1000 | Loss: 0.00002418
Iteration 140/1000 | Loss: 0.00002418
Iteration 141/1000 | Loss: 0.00002418
Iteration 142/1000 | Loss: 0.00002417
Iteration 143/1000 | Loss: 0.00002417
Iteration 144/1000 | Loss: 0.00002417
Iteration 145/1000 | Loss: 0.00002417
Iteration 146/1000 | Loss: 0.00002417
Iteration 147/1000 | Loss: 0.00002416
Iteration 148/1000 | Loss: 0.00002416
Iteration 149/1000 | Loss: 0.00002416
Iteration 150/1000 | Loss: 0.00002416
Iteration 151/1000 | Loss: 0.00002416
Iteration 152/1000 | Loss: 0.00002416
Iteration 153/1000 | Loss: 0.00002415
Iteration 154/1000 | Loss: 0.00002415
Iteration 155/1000 | Loss: 0.00002415
Iteration 156/1000 | Loss: 0.00002415
Iteration 157/1000 | Loss: 0.00002415
Iteration 158/1000 | Loss: 0.00002415
Iteration 159/1000 | Loss: 0.00002415
Iteration 160/1000 | Loss: 0.00002415
Iteration 161/1000 | Loss: 0.00002415
Iteration 162/1000 | Loss: 0.00002414
Iteration 163/1000 | Loss: 0.00002414
Iteration 164/1000 | Loss: 0.00002414
Iteration 165/1000 | Loss: 0.00002414
Iteration 166/1000 | Loss: 0.00002414
Iteration 167/1000 | Loss: 0.00002414
Iteration 168/1000 | Loss: 0.00002413
Iteration 169/1000 | Loss: 0.00002413
Iteration 170/1000 | Loss: 0.00002413
Iteration 171/1000 | Loss: 0.00002413
Iteration 172/1000 | Loss: 0.00002412
Iteration 173/1000 | Loss: 0.00002410
Iteration 174/1000 | Loss: 0.00002410
Iteration 175/1000 | Loss: 0.00002410
Iteration 176/1000 | Loss: 0.00002410
Iteration 177/1000 | Loss: 0.00002410
Iteration 178/1000 | Loss: 0.00002409
Iteration 179/1000 | Loss: 0.00002409
Iteration 180/1000 | Loss: 0.00002408
Iteration 181/1000 | Loss: 0.00002408
Iteration 182/1000 | Loss: 0.00002408
Iteration 183/1000 | Loss: 0.00002407
Iteration 184/1000 | Loss: 0.00002407
Iteration 185/1000 | Loss: 0.00002407
Iteration 186/1000 | Loss: 0.00002407
Iteration 187/1000 | Loss: 0.00002407
Iteration 188/1000 | Loss: 0.00002407
Iteration 189/1000 | Loss: 0.00002407
Iteration 190/1000 | Loss: 0.00002407
Iteration 191/1000 | Loss: 0.00002407
Iteration 192/1000 | Loss: 0.00002406
Iteration 193/1000 | Loss: 0.00002406
Iteration 194/1000 | Loss: 0.00002406
Iteration 195/1000 | Loss: 0.00002406
Iteration 196/1000 | Loss: 0.00002406
Iteration 197/1000 | Loss: 0.00002405
Iteration 198/1000 | Loss: 0.00002405
Iteration 199/1000 | Loss: 0.00002404
Iteration 200/1000 | Loss: 0.00002403
Iteration 201/1000 | Loss: 0.00002403
Iteration 202/1000 | Loss: 0.00002403
Iteration 203/1000 | Loss: 0.00002403
Iteration 204/1000 | Loss: 0.00002402
Iteration 205/1000 | Loss: 0.00002402
Iteration 206/1000 | Loss: 0.00002402
Iteration 207/1000 | Loss: 0.00002402
Iteration 208/1000 | Loss: 0.00002402
Iteration 209/1000 | Loss: 0.00002402
Iteration 210/1000 | Loss: 0.00002402
Iteration 211/1000 | Loss: 0.00002402
Iteration 212/1000 | Loss: 0.00002401
Iteration 213/1000 | Loss: 0.00002401
Iteration 214/1000 | Loss: 0.00002401
Iteration 215/1000 | Loss: 0.00002401
Iteration 216/1000 | Loss: 0.00002401
Iteration 217/1000 | Loss: 0.00002401
Iteration 218/1000 | Loss: 0.00002401
Iteration 219/1000 | Loss: 0.00002401
Iteration 220/1000 | Loss: 0.00002401
Iteration 221/1000 | Loss: 0.00002401
Iteration 222/1000 | Loss: 0.00002401
Iteration 223/1000 | Loss: 0.00002401
Iteration 224/1000 | Loss: 0.00002401
Iteration 225/1000 | Loss: 0.00002400
Iteration 226/1000 | Loss: 0.00002400
Iteration 227/1000 | Loss: 0.00002400
Iteration 228/1000 | Loss: 0.00002400
Iteration 229/1000 | Loss: 0.00002400
Iteration 230/1000 | Loss: 0.00002400
Iteration 231/1000 | Loss: 0.00002400
Iteration 232/1000 | Loss: 0.00002400
Iteration 233/1000 | Loss: 0.00002400
Iteration 234/1000 | Loss: 0.00002400
Iteration 235/1000 | Loss: 0.00002400
Iteration 236/1000 | Loss: 0.00002400
Iteration 237/1000 | Loss: 0.00002400
Iteration 238/1000 | Loss: 0.00002400
Iteration 239/1000 | Loss: 0.00002400
Iteration 240/1000 | Loss: 0.00002400
Iteration 241/1000 | Loss: 0.00002400
Iteration 242/1000 | Loss: 0.00002400
Iteration 243/1000 | Loss: 0.00002400
Iteration 244/1000 | Loss: 0.00002400
Iteration 245/1000 | Loss: 0.00002400
Iteration 246/1000 | Loss: 0.00002400
Iteration 247/1000 | Loss: 0.00002400
Iteration 248/1000 | Loss: 0.00002400
Iteration 249/1000 | Loss: 0.00002400
Iteration 250/1000 | Loss: 0.00002400
Iteration 251/1000 | Loss: 0.00002400
Iteration 252/1000 | Loss: 0.00002400
Iteration 253/1000 | Loss: 0.00002400
Iteration 254/1000 | Loss: 0.00002400
Iteration 255/1000 | Loss: 0.00002400
Iteration 256/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [2.3999702534638345e-05, 2.3999702534638345e-05, 2.3999702534638345e-05, 2.3999702534638345e-05, 2.3999702534638345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3999702534638345e-05

Optimization complete. Final v2v error: 3.740488052368164 mm

Highest mean error: 7.633676052093506 mm for frame 104

Lowest mean error: 2.6453351974487305 mm for frame 74

Saving results

Total time: 213.3909239768982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480290
Iteration 2/25 | Loss: 0.00103413
Iteration 3/25 | Loss: 0.00095547
Iteration 4/25 | Loss: 0.00094788
Iteration 5/25 | Loss: 0.00094554
Iteration 6/25 | Loss: 0.00094503
Iteration 7/25 | Loss: 0.00094503
Iteration 8/25 | Loss: 0.00094503
Iteration 9/25 | Loss: 0.00094503
Iteration 10/25 | Loss: 0.00094503
Iteration 11/25 | Loss: 0.00094503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009450254146941006, 0.0009450254146941006, 0.0009450254146941006, 0.0009450254146941006, 0.0009450254146941006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009450254146941006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35862756
Iteration 2/25 | Loss: 0.00063474
Iteration 3/25 | Loss: 0.00063474
Iteration 4/25 | Loss: 0.00063473
Iteration 5/25 | Loss: 0.00063473
Iteration 6/25 | Loss: 0.00063473
Iteration 7/25 | Loss: 0.00063473
Iteration 8/25 | Loss: 0.00063473
Iteration 9/25 | Loss: 0.00063473
Iteration 10/25 | Loss: 0.00063473
Iteration 11/25 | Loss: 0.00063473
Iteration 12/25 | Loss: 0.00063473
Iteration 13/25 | Loss: 0.00063473
Iteration 14/25 | Loss: 0.00063473
Iteration 15/25 | Loss: 0.00063473
Iteration 16/25 | Loss: 0.00063473
Iteration 17/25 | Loss: 0.00063473
Iteration 18/25 | Loss: 0.00063473
Iteration 19/25 | Loss: 0.00063473
Iteration 20/25 | Loss: 0.00063473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006347313756123185, 0.0006347313756123185, 0.0006347313756123185, 0.0006347313756123185, 0.0006347313756123185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006347313756123185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063473
Iteration 2/1000 | Loss: 0.00002019
Iteration 3/1000 | Loss: 0.00001532
Iteration 4/1000 | Loss: 0.00001445
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001346
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001344
Iteration 13/1000 | Loss: 0.00001343
Iteration 14/1000 | Loss: 0.00001342
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001340
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001334
Iteration 23/1000 | Loss: 0.00001334
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001329
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001328
Iteration 35/1000 | Loss: 0.00001328
Iteration 36/1000 | Loss: 0.00001327
Iteration 37/1000 | Loss: 0.00001327
Iteration 38/1000 | Loss: 0.00001327
Iteration 39/1000 | Loss: 0.00001326
Iteration 40/1000 | Loss: 0.00001326
Iteration 41/1000 | Loss: 0.00001326
Iteration 42/1000 | Loss: 0.00001326
Iteration 43/1000 | Loss: 0.00001326
Iteration 44/1000 | Loss: 0.00001325
Iteration 45/1000 | Loss: 0.00001325
Iteration 46/1000 | Loss: 0.00001325
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001325
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001325
Iteration 52/1000 | Loss: 0.00001325
Iteration 53/1000 | Loss: 0.00001325
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001322
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001321
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001320
Iteration 64/1000 | Loss: 0.00001319
Iteration 65/1000 | Loss: 0.00001319
Iteration 66/1000 | Loss: 0.00001319
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001318
Iteration 69/1000 | Loss: 0.00001318
Iteration 70/1000 | Loss: 0.00001318
Iteration 71/1000 | Loss: 0.00001318
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001318
Iteration 77/1000 | Loss: 0.00001318
Iteration 78/1000 | Loss: 0.00001318
Iteration 79/1000 | Loss: 0.00001317
Iteration 80/1000 | Loss: 0.00001317
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001315
Iteration 84/1000 | Loss: 0.00001315
Iteration 85/1000 | Loss: 0.00001315
Iteration 86/1000 | Loss: 0.00001315
Iteration 87/1000 | Loss: 0.00001315
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001314
Iteration 92/1000 | Loss: 0.00001314
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001312
Iteration 95/1000 | Loss: 0.00001312
Iteration 96/1000 | Loss: 0.00001312
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001311
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001309
Iteration 105/1000 | Loss: 0.00001309
Iteration 106/1000 | Loss: 0.00001309
Iteration 107/1000 | Loss: 0.00001309
Iteration 108/1000 | Loss: 0.00001309
Iteration 109/1000 | Loss: 0.00001309
Iteration 110/1000 | Loss: 0.00001309
Iteration 111/1000 | Loss: 0.00001309
Iteration 112/1000 | Loss: 0.00001309
Iteration 113/1000 | Loss: 0.00001309
Iteration 114/1000 | Loss: 0.00001309
Iteration 115/1000 | Loss: 0.00001309
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001309
Iteration 118/1000 | Loss: 0.00001309
Iteration 119/1000 | Loss: 0.00001309
Iteration 120/1000 | Loss: 0.00001309
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.3088796549709514e-05, 1.3088796549709514e-05, 1.3088796549709514e-05, 1.3088796549709514e-05, 1.3088796549709514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3088796549709514e-05

Optimization complete. Final v2v error: 3.000131607055664 mm

Highest mean error: 3.3111751079559326 mm for frame 28

Lowest mean error: 2.713574171066284 mm for frame 50

Saving results

Total time: 31.14332151412964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805106
Iteration 2/25 | Loss: 0.00137854
Iteration 3/25 | Loss: 0.00115331
Iteration 4/25 | Loss: 0.00108266
Iteration 5/25 | Loss: 0.00101972
Iteration 6/25 | Loss: 0.00101379
Iteration 7/25 | Loss: 0.00101223
Iteration 8/25 | Loss: 0.00101155
Iteration 9/25 | Loss: 0.00101138
Iteration 10/25 | Loss: 0.00101136
Iteration 11/25 | Loss: 0.00101136
Iteration 12/25 | Loss: 0.00101136
Iteration 13/25 | Loss: 0.00101136
Iteration 14/25 | Loss: 0.00101136
Iteration 15/25 | Loss: 0.00101136
Iteration 16/25 | Loss: 0.00101135
Iteration 17/25 | Loss: 0.00101135
Iteration 18/25 | Loss: 0.00101135
Iteration 19/25 | Loss: 0.00101135
Iteration 20/25 | Loss: 0.00101135
Iteration 21/25 | Loss: 0.00101135
Iteration 22/25 | Loss: 0.00101135
Iteration 23/25 | Loss: 0.00101135
Iteration 24/25 | Loss: 0.00101135
Iteration 25/25 | Loss: 0.00101134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77109182
Iteration 2/25 | Loss: 0.00073891
Iteration 3/25 | Loss: 0.00073891
Iteration 4/25 | Loss: 0.00073891
Iteration 5/25 | Loss: 0.00073891
Iteration 6/25 | Loss: 0.00073891
Iteration 7/25 | Loss: 0.00073891
Iteration 8/25 | Loss: 0.00073891
Iteration 9/25 | Loss: 0.00073891
Iteration 10/25 | Loss: 0.00073891
Iteration 11/25 | Loss: 0.00073891
Iteration 12/25 | Loss: 0.00073891
Iteration 13/25 | Loss: 0.00073891
Iteration 14/25 | Loss: 0.00073891
Iteration 15/25 | Loss: 0.00073891
Iteration 16/25 | Loss: 0.00073891
Iteration 17/25 | Loss: 0.00073891
Iteration 18/25 | Loss: 0.00073891
Iteration 19/25 | Loss: 0.00073891
Iteration 20/25 | Loss: 0.00073891
Iteration 21/25 | Loss: 0.00073891
Iteration 22/25 | Loss: 0.00073891
Iteration 23/25 | Loss: 0.00073891
Iteration 24/25 | Loss: 0.00073891
Iteration 25/25 | Loss: 0.00073891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073891
Iteration 2/1000 | Loss: 0.00004696
Iteration 3/1000 | Loss: 0.00002858
Iteration 4/1000 | Loss: 0.00002501
Iteration 5/1000 | Loss: 0.00002406
Iteration 6/1000 | Loss: 0.00002328
Iteration 7/1000 | Loss: 0.00002276
Iteration 8/1000 | Loss: 0.00002227
Iteration 9/1000 | Loss: 0.00002181
Iteration 10/1000 | Loss: 0.00002154
Iteration 11/1000 | Loss: 0.00002131
Iteration 12/1000 | Loss: 0.00002116
Iteration 13/1000 | Loss: 0.00002112
Iteration 14/1000 | Loss: 0.00002112
Iteration 15/1000 | Loss: 0.00002109
Iteration 16/1000 | Loss: 0.00002109
Iteration 17/1000 | Loss: 0.00002109
Iteration 18/1000 | Loss: 0.00002109
Iteration 19/1000 | Loss: 0.00002108
Iteration 20/1000 | Loss: 0.00002108
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002104
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002103
Iteration 27/1000 | Loss: 0.00002103
Iteration 28/1000 | Loss: 0.00002103
Iteration 29/1000 | Loss: 0.00002103
Iteration 30/1000 | Loss: 0.00002103
Iteration 31/1000 | Loss: 0.00002103
Iteration 32/1000 | Loss: 0.00002103
Iteration 33/1000 | Loss: 0.00002102
Iteration 34/1000 | Loss: 0.00002102
Iteration 35/1000 | Loss: 0.00002101
Iteration 36/1000 | Loss: 0.00002101
Iteration 37/1000 | Loss: 0.00002100
Iteration 38/1000 | Loss: 0.00002100
Iteration 39/1000 | Loss: 0.00002099
Iteration 40/1000 | Loss: 0.00002099
Iteration 41/1000 | Loss: 0.00002098
Iteration 42/1000 | Loss: 0.00002098
Iteration 43/1000 | Loss: 0.00002098
Iteration 44/1000 | Loss: 0.00002098
Iteration 45/1000 | Loss: 0.00002097
Iteration 46/1000 | Loss: 0.00002097
Iteration 47/1000 | Loss: 0.00002097
Iteration 48/1000 | Loss: 0.00002096
Iteration 49/1000 | Loss: 0.00002096
Iteration 50/1000 | Loss: 0.00002095
Iteration 51/1000 | Loss: 0.00002095
Iteration 52/1000 | Loss: 0.00002095
Iteration 53/1000 | Loss: 0.00002095
Iteration 54/1000 | Loss: 0.00002094
Iteration 55/1000 | Loss: 0.00002094
Iteration 56/1000 | Loss: 0.00002094
Iteration 57/1000 | Loss: 0.00002094
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002093
Iteration 60/1000 | Loss: 0.00002093
Iteration 61/1000 | Loss: 0.00002093
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002092
Iteration 66/1000 | Loss: 0.00002092
Iteration 67/1000 | Loss: 0.00002092
Iteration 68/1000 | Loss: 0.00002092
Iteration 69/1000 | Loss: 0.00002091
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002090
Iteration 72/1000 | Loss: 0.00002090
Iteration 73/1000 | Loss: 0.00002090
Iteration 74/1000 | Loss: 0.00002089
Iteration 75/1000 | Loss: 0.00002089
Iteration 76/1000 | Loss: 0.00002089
Iteration 77/1000 | Loss: 0.00002089
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002089
Iteration 80/1000 | Loss: 0.00002089
Iteration 81/1000 | Loss: 0.00002089
Iteration 82/1000 | Loss: 0.00002089
Iteration 83/1000 | Loss: 0.00002088
Iteration 84/1000 | Loss: 0.00002088
Iteration 85/1000 | Loss: 0.00002088
Iteration 86/1000 | Loss: 0.00002087
Iteration 87/1000 | Loss: 0.00002087
Iteration 88/1000 | Loss: 0.00002087
Iteration 89/1000 | Loss: 0.00002087
Iteration 90/1000 | Loss: 0.00002086
Iteration 91/1000 | Loss: 0.00002086
Iteration 92/1000 | Loss: 0.00002086
Iteration 93/1000 | Loss: 0.00002085
Iteration 94/1000 | Loss: 0.00002085
Iteration 95/1000 | Loss: 0.00002085
Iteration 96/1000 | Loss: 0.00002085
Iteration 97/1000 | Loss: 0.00002085
Iteration 98/1000 | Loss: 0.00002085
Iteration 99/1000 | Loss: 0.00002085
Iteration 100/1000 | Loss: 0.00002085
Iteration 101/1000 | Loss: 0.00002085
Iteration 102/1000 | Loss: 0.00002085
Iteration 103/1000 | Loss: 0.00002085
Iteration 104/1000 | Loss: 0.00002085
Iteration 105/1000 | Loss: 0.00002085
Iteration 106/1000 | Loss: 0.00002085
Iteration 107/1000 | Loss: 0.00002084
Iteration 108/1000 | Loss: 0.00002084
Iteration 109/1000 | Loss: 0.00002084
Iteration 110/1000 | Loss: 0.00002084
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002084
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002084
Iteration 117/1000 | Loss: 0.00002084
Iteration 118/1000 | Loss: 0.00002084
Iteration 119/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.0844552636845037e-05, 2.0844552636845037e-05, 2.0844552636845037e-05, 2.0844552636845037e-05, 2.0844552636845037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0844552636845037e-05

Optimization complete. Final v2v error: 3.8222787380218506 mm

Highest mean error: 4.523857116699219 mm for frame 10

Lowest mean error: 2.8120973110198975 mm for frame 138

Saving results

Total time: 40.63409161567688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109410
Iteration 2/25 | Loss: 0.01109410
Iteration 3/25 | Loss: 0.01109409
Iteration 4/25 | Loss: 0.01109409
Iteration 5/25 | Loss: 0.00393186
Iteration 6/25 | Loss: 0.00241712
Iteration 7/25 | Loss: 0.00210100
Iteration 8/25 | Loss: 0.00188242
Iteration 9/25 | Loss: 0.00164918
Iteration 10/25 | Loss: 0.00159816
Iteration 11/25 | Loss: 0.00149405
Iteration 12/25 | Loss: 0.00144800
Iteration 13/25 | Loss: 0.00141309
Iteration 14/25 | Loss: 0.00140763
Iteration 15/25 | Loss: 0.00140335
Iteration 16/25 | Loss: 0.00140095
Iteration 17/25 | Loss: 0.00139605
Iteration 18/25 | Loss: 0.00139153
Iteration 19/25 | Loss: 0.00139230
Iteration 20/25 | Loss: 0.00138713
Iteration 21/25 | Loss: 0.00138628
Iteration 22/25 | Loss: 0.00138528
Iteration 23/25 | Loss: 0.00138450
Iteration 24/25 | Loss: 0.00138635
Iteration 25/25 | Loss: 0.00138476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22272062
Iteration 2/25 | Loss: 0.00343050
Iteration 3/25 | Loss: 0.00329884
Iteration 4/25 | Loss: 0.00329883
Iteration 5/25 | Loss: 0.00329883
Iteration 6/25 | Loss: 0.00329883
Iteration 7/25 | Loss: 0.00329883
Iteration 8/25 | Loss: 0.00329883
Iteration 9/25 | Loss: 0.00329883
Iteration 10/25 | Loss: 0.00329883
Iteration 11/25 | Loss: 0.00329883
Iteration 12/25 | Loss: 0.00329883
Iteration 13/25 | Loss: 0.00329883
Iteration 14/25 | Loss: 0.00329883
Iteration 15/25 | Loss: 0.00329883
Iteration 16/25 | Loss: 0.00329883
Iteration 17/25 | Loss: 0.00329883
Iteration 18/25 | Loss: 0.00329883
Iteration 19/25 | Loss: 0.00329883
Iteration 20/25 | Loss: 0.00329883
Iteration 21/25 | Loss: 0.00329883
Iteration 22/25 | Loss: 0.00329883
Iteration 23/25 | Loss: 0.00329883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0032988309394568205, 0.0032988309394568205, 0.0032988309394568205, 0.0032988309394568205, 0.0032988309394568205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032988309394568205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00329883
Iteration 2/1000 | Loss: 0.00105550
Iteration 3/1000 | Loss: 0.00078425
Iteration 4/1000 | Loss: 0.00079237
Iteration 5/1000 | Loss: 0.00063522
Iteration 6/1000 | Loss: 0.00268024
Iteration 7/1000 | Loss: 0.00038164
Iteration 8/1000 | Loss: 0.00085051
Iteration 9/1000 | Loss: 0.00151828
Iteration 10/1000 | Loss: 0.00033442
Iteration 11/1000 | Loss: 0.00042720
Iteration 12/1000 | Loss: 0.00033550
Iteration 13/1000 | Loss: 0.00079845
Iteration 14/1000 | Loss: 0.00385802
Iteration 15/1000 | Loss: 0.00522642
Iteration 16/1000 | Loss: 0.01608623
Iteration 17/1000 | Loss: 0.00427157
Iteration 18/1000 | Loss: 0.00296999
Iteration 19/1000 | Loss: 0.00455711
Iteration 20/1000 | Loss: 0.00246564
Iteration 21/1000 | Loss: 0.00101732
Iteration 22/1000 | Loss: 0.00302312
Iteration 23/1000 | Loss: 0.00260327
Iteration 24/1000 | Loss: 0.00251369
Iteration 25/1000 | Loss: 0.00312009
Iteration 26/1000 | Loss: 0.00222505
Iteration 27/1000 | Loss: 0.00215405
Iteration 28/1000 | Loss: 0.00046955
Iteration 29/1000 | Loss: 0.00094336
Iteration 30/1000 | Loss: 0.00110520
Iteration 31/1000 | Loss: 0.00433109
Iteration 32/1000 | Loss: 0.00406572
Iteration 33/1000 | Loss: 0.00209240
Iteration 34/1000 | Loss: 0.00507706
Iteration 35/1000 | Loss: 0.00315250
Iteration 36/1000 | Loss: 0.00291885
Iteration 37/1000 | Loss: 0.00013460
Iteration 38/1000 | Loss: 0.00124167
Iteration 39/1000 | Loss: 0.00010136
Iteration 40/1000 | Loss: 0.00008401
Iteration 41/1000 | Loss: 0.00264442
Iteration 42/1000 | Loss: 0.00064807
Iteration 43/1000 | Loss: 0.00006495
Iteration 44/1000 | Loss: 0.00012458
Iteration 45/1000 | Loss: 0.00004178
Iteration 46/1000 | Loss: 0.00004340
Iteration 47/1000 | Loss: 0.00006471
Iteration 48/1000 | Loss: 0.00006450
Iteration 49/1000 | Loss: 0.00003788
Iteration 50/1000 | Loss: 0.00003119
Iteration 51/1000 | Loss: 0.00004528
Iteration 52/1000 | Loss: 0.00002527
Iteration 53/1000 | Loss: 0.00008541
Iteration 54/1000 | Loss: 0.00002401
Iteration 55/1000 | Loss: 0.00002304
Iteration 56/1000 | Loss: 0.00006057
Iteration 57/1000 | Loss: 0.00002876
Iteration 58/1000 | Loss: 0.00026184
Iteration 59/1000 | Loss: 0.00006331
Iteration 60/1000 | Loss: 0.00002390
Iteration 61/1000 | Loss: 0.00003443
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00003826
Iteration 64/1000 | Loss: 0.00006958
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00002329
Iteration 69/1000 | Loss: 0.00002491
Iteration 70/1000 | Loss: 0.00005358
Iteration 71/1000 | Loss: 0.00023487
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001642
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001642
Iteration 78/1000 | Loss: 0.00001642
Iteration 79/1000 | Loss: 0.00001642
Iteration 80/1000 | Loss: 0.00001641
Iteration 81/1000 | Loss: 0.00001641
Iteration 82/1000 | Loss: 0.00001641
Iteration 83/1000 | Loss: 0.00001641
Iteration 84/1000 | Loss: 0.00001640
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001638
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001687
Iteration 91/1000 | Loss: 0.00001634
Iteration 92/1000 | Loss: 0.00001634
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001634
Iteration 95/1000 | Loss: 0.00001634
Iteration 96/1000 | Loss: 0.00001634
Iteration 97/1000 | Loss: 0.00003005
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00001627
Iteration 100/1000 | Loss: 0.00001626
Iteration 101/1000 | Loss: 0.00001626
Iteration 102/1000 | Loss: 0.00001626
Iteration 103/1000 | Loss: 0.00001626
Iteration 104/1000 | Loss: 0.00001626
Iteration 105/1000 | Loss: 0.00001625
Iteration 106/1000 | Loss: 0.00001625
Iteration 107/1000 | Loss: 0.00001625
Iteration 108/1000 | Loss: 0.00001624
Iteration 109/1000 | Loss: 0.00001624
Iteration 110/1000 | Loss: 0.00001624
Iteration 111/1000 | Loss: 0.00001623
Iteration 112/1000 | Loss: 0.00001623
Iteration 113/1000 | Loss: 0.00001623
Iteration 114/1000 | Loss: 0.00001623
Iteration 115/1000 | Loss: 0.00001623
Iteration 116/1000 | Loss: 0.00001623
Iteration 117/1000 | Loss: 0.00001622
Iteration 118/1000 | Loss: 0.00001622
Iteration 119/1000 | Loss: 0.00001622
Iteration 120/1000 | Loss: 0.00001622
Iteration 121/1000 | Loss: 0.00001622
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001621
Iteration 124/1000 | Loss: 0.00001621
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00002864
Iteration 127/1000 | Loss: 0.00001634
Iteration 128/1000 | Loss: 0.00002332
Iteration 129/1000 | Loss: 0.00002332
Iteration 130/1000 | Loss: 0.00001630
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001608
Iteration 133/1000 | Loss: 0.00001608
Iteration 134/1000 | Loss: 0.00001608
Iteration 135/1000 | Loss: 0.00001608
Iteration 136/1000 | Loss: 0.00001608
Iteration 137/1000 | Loss: 0.00001608
Iteration 138/1000 | Loss: 0.00001608
Iteration 139/1000 | Loss: 0.00001608
Iteration 140/1000 | Loss: 0.00001608
Iteration 141/1000 | Loss: 0.00001608
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001607
Iteration 144/1000 | Loss: 0.00001607
Iteration 145/1000 | Loss: 0.00001607
Iteration 146/1000 | Loss: 0.00001607
Iteration 147/1000 | Loss: 0.00001607
Iteration 148/1000 | Loss: 0.00001607
Iteration 149/1000 | Loss: 0.00001607
Iteration 150/1000 | Loss: 0.00001606
Iteration 151/1000 | Loss: 0.00001606
Iteration 152/1000 | Loss: 0.00001606
Iteration 153/1000 | Loss: 0.00001606
Iteration 154/1000 | Loss: 0.00001606
Iteration 155/1000 | Loss: 0.00001606
Iteration 156/1000 | Loss: 0.00001606
Iteration 157/1000 | Loss: 0.00001761
Iteration 158/1000 | Loss: 0.00001608
Iteration 159/1000 | Loss: 0.00001608
Iteration 160/1000 | Loss: 0.00001608
Iteration 161/1000 | Loss: 0.00001608
Iteration 162/1000 | Loss: 0.00001608
Iteration 163/1000 | Loss: 0.00001608
Iteration 164/1000 | Loss: 0.00001608
Iteration 165/1000 | Loss: 0.00001608
Iteration 166/1000 | Loss: 0.00001607
Iteration 167/1000 | Loss: 0.00001607
Iteration 168/1000 | Loss: 0.00001969
Iteration 169/1000 | Loss: 0.00001611
Iteration 170/1000 | Loss: 0.00001912
Iteration 171/1000 | Loss: 0.00001607
Iteration 172/1000 | Loss: 0.00001710
Iteration 173/1000 | Loss: 0.00001605
Iteration 174/1000 | Loss: 0.00001605
Iteration 175/1000 | Loss: 0.00001605
Iteration 176/1000 | Loss: 0.00001605
Iteration 177/1000 | Loss: 0.00001605
Iteration 178/1000 | Loss: 0.00001605
Iteration 179/1000 | Loss: 0.00001605
Iteration 180/1000 | Loss: 0.00001611
Iteration 181/1000 | Loss: 0.00001608
Iteration 182/1000 | Loss: 0.00001605
Iteration 183/1000 | Loss: 0.00001605
Iteration 184/1000 | Loss: 0.00001605
Iteration 185/1000 | Loss: 0.00001605
Iteration 186/1000 | Loss: 0.00001605
Iteration 187/1000 | Loss: 0.00001605
Iteration 188/1000 | Loss: 0.00001605
Iteration 189/1000 | Loss: 0.00001605
Iteration 190/1000 | Loss: 0.00001605
Iteration 191/1000 | Loss: 0.00001605
Iteration 192/1000 | Loss: 0.00001605
Iteration 193/1000 | Loss: 0.00001605
Iteration 194/1000 | Loss: 0.00001605
Iteration 195/1000 | Loss: 0.00001605
Iteration 196/1000 | Loss: 0.00001605
Iteration 197/1000 | Loss: 0.00001605
Iteration 198/1000 | Loss: 0.00001605
Iteration 199/1000 | Loss: 0.00001605
Iteration 200/1000 | Loss: 0.00001605
Iteration 201/1000 | Loss: 0.00001605
Iteration 202/1000 | Loss: 0.00001605
Iteration 203/1000 | Loss: 0.00001605
Iteration 204/1000 | Loss: 0.00001605
Iteration 205/1000 | Loss: 0.00001605
Iteration 206/1000 | Loss: 0.00001605
Iteration 207/1000 | Loss: 0.00001605
Iteration 208/1000 | Loss: 0.00001605
Iteration 209/1000 | Loss: 0.00001605
Iteration 210/1000 | Loss: 0.00001605
Iteration 211/1000 | Loss: 0.00001605
Iteration 212/1000 | Loss: 0.00001605
Iteration 213/1000 | Loss: 0.00001605
Iteration 214/1000 | Loss: 0.00001605
Iteration 215/1000 | Loss: 0.00001605
Iteration 216/1000 | Loss: 0.00001605
Iteration 217/1000 | Loss: 0.00001605
Iteration 218/1000 | Loss: 0.00001605
Iteration 219/1000 | Loss: 0.00001605
Iteration 220/1000 | Loss: 0.00001605
Iteration 221/1000 | Loss: 0.00001605
Iteration 222/1000 | Loss: 0.00001605
Iteration 223/1000 | Loss: 0.00001605
Iteration 224/1000 | Loss: 0.00001605
Iteration 225/1000 | Loss: 0.00001605
Iteration 226/1000 | Loss: 0.00001605
Iteration 227/1000 | Loss: 0.00001605
Iteration 228/1000 | Loss: 0.00001605
Iteration 229/1000 | Loss: 0.00001605
Iteration 230/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.604564386070706e-05, 1.604564386070706e-05, 1.604564386070706e-05, 1.604564386070706e-05, 1.604564386070706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.604564386070706e-05

Optimization complete. Final v2v error: 3.103252649307251 mm

Highest mean error: 5.507041931152344 mm for frame 62

Lowest mean error: 2.4211723804473877 mm for frame 104

Saving results

Total time: 171.95450353622437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423818
Iteration 2/25 | Loss: 0.00117570
Iteration 3/25 | Loss: 0.00096786
Iteration 4/25 | Loss: 0.00094295
Iteration 5/25 | Loss: 0.00093954
Iteration 6/25 | Loss: 0.00093850
Iteration 7/25 | Loss: 0.00093850
Iteration 8/25 | Loss: 0.00093850
Iteration 9/25 | Loss: 0.00093850
Iteration 10/25 | Loss: 0.00093850
Iteration 11/25 | Loss: 0.00093850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009384950390085578, 0.0009384950390085578, 0.0009384950390085578, 0.0009384950390085578, 0.0009384950390085578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009384950390085578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.03076553
Iteration 2/25 | Loss: 0.00043826
Iteration 3/25 | Loss: 0.00043824
Iteration 4/25 | Loss: 0.00043824
Iteration 5/25 | Loss: 0.00043823
Iteration 6/25 | Loss: 0.00043823
Iteration 7/25 | Loss: 0.00043823
Iteration 8/25 | Loss: 0.00043823
Iteration 9/25 | Loss: 0.00043823
Iteration 10/25 | Loss: 0.00043823
Iteration 11/25 | Loss: 0.00043823
Iteration 12/25 | Loss: 0.00043823
Iteration 13/25 | Loss: 0.00043823
Iteration 14/25 | Loss: 0.00043823
Iteration 15/25 | Loss: 0.00043823
Iteration 16/25 | Loss: 0.00043823
Iteration 17/25 | Loss: 0.00043823
Iteration 18/25 | Loss: 0.00043823
Iteration 19/25 | Loss: 0.00043823
Iteration 20/25 | Loss: 0.00043823
Iteration 21/25 | Loss: 0.00043823
Iteration 22/25 | Loss: 0.00043823
Iteration 23/25 | Loss: 0.00043823
Iteration 24/25 | Loss: 0.00043823
Iteration 25/25 | Loss: 0.00043823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004382327606435865, 0.0004382327606435865, 0.0004382327606435865, 0.0004382327606435865, 0.0004382327606435865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004382327606435865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043823
Iteration 2/1000 | Loss: 0.00002113
Iteration 3/1000 | Loss: 0.00001564
Iteration 4/1000 | Loss: 0.00001381
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001255
Iteration 7/1000 | Loss: 0.00001217
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001179
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001165
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001163
Iteration 26/1000 | Loss: 0.00001163
Iteration 27/1000 | Loss: 0.00001163
Iteration 28/1000 | Loss: 0.00001163
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001162
Iteration 31/1000 | Loss: 0.00001162
Iteration 32/1000 | Loss: 0.00001162
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001162
Iteration 50/1000 | Loss: 0.00001162
Iteration 51/1000 | Loss: 0.00001162
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001162
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001162
Iteration 61/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.1618650205491576e-05, 1.1618650205491576e-05, 1.1618650205491576e-05, 1.1618650205491576e-05, 1.1618650205491576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1618650205491576e-05

Optimization complete. Final v2v error: 2.8780457973480225 mm

Highest mean error: 3.2709007263183594 mm for frame 104

Lowest mean error: 2.6217098236083984 mm for frame 55

Saving results

Total time: 24.52441954612732
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00744881
Iteration 2/25 | Loss: 0.00154808
Iteration 3/25 | Loss: 0.00115213
Iteration 4/25 | Loss: 0.00110698
Iteration 5/25 | Loss: 0.00109967
Iteration 6/25 | Loss: 0.00109862
Iteration 7/25 | Loss: 0.00109862
Iteration 8/25 | Loss: 0.00109862
Iteration 9/25 | Loss: 0.00109862
Iteration 10/25 | Loss: 0.00109862
Iteration 11/25 | Loss: 0.00109862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010986229171976447, 0.0010986229171976447, 0.0010986229171976447, 0.0010986229171976447, 0.0010986229171976447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010986229171976447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42199492
Iteration 2/25 | Loss: 0.00068249
Iteration 3/25 | Loss: 0.00068249
Iteration 4/25 | Loss: 0.00068249
Iteration 5/25 | Loss: 0.00068249
Iteration 6/25 | Loss: 0.00068249
Iteration 7/25 | Loss: 0.00068249
Iteration 8/25 | Loss: 0.00068249
Iteration 9/25 | Loss: 0.00068249
Iteration 10/25 | Loss: 0.00068249
Iteration 11/25 | Loss: 0.00068249
Iteration 12/25 | Loss: 0.00068249
Iteration 13/25 | Loss: 0.00068249
Iteration 14/25 | Loss: 0.00068249
Iteration 15/25 | Loss: 0.00068249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006824859883636236, 0.0006824859883636236, 0.0006824859883636236, 0.0006824859883636236, 0.0006824859883636236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006824859883636236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068249
Iteration 2/1000 | Loss: 0.00006062
Iteration 3/1000 | Loss: 0.00003399
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002522
Iteration 6/1000 | Loss: 0.00002391
Iteration 7/1000 | Loss: 0.00002320
Iteration 8/1000 | Loss: 0.00002284
Iteration 9/1000 | Loss: 0.00002251
Iteration 10/1000 | Loss: 0.00002229
Iteration 11/1000 | Loss: 0.00002221
Iteration 12/1000 | Loss: 0.00002214
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002199
Iteration 15/1000 | Loss: 0.00002197
Iteration 16/1000 | Loss: 0.00002196
Iteration 17/1000 | Loss: 0.00002196
Iteration 18/1000 | Loss: 0.00002195
Iteration 19/1000 | Loss: 0.00002194
Iteration 20/1000 | Loss: 0.00002193
Iteration 21/1000 | Loss: 0.00002190
Iteration 22/1000 | Loss: 0.00002190
Iteration 23/1000 | Loss: 0.00002186
Iteration 24/1000 | Loss: 0.00002186
Iteration 25/1000 | Loss: 0.00002184
Iteration 26/1000 | Loss: 0.00002181
Iteration 27/1000 | Loss: 0.00002181
Iteration 28/1000 | Loss: 0.00002181
Iteration 29/1000 | Loss: 0.00002181
Iteration 30/1000 | Loss: 0.00002180
Iteration 31/1000 | Loss: 0.00002178
Iteration 32/1000 | Loss: 0.00002177
Iteration 33/1000 | Loss: 0.00002175
Iteration 34/1000 | Loss: 0.00002175
Iteration 35/1000 | Loss: 0.00002174
Iteration 36/1000 | Loss: 0.00002174
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002171
Iteration 40/1000 | Loss: 0.00002171
Iteration 41/1000 | Loss: 0.00002171
Iteration 42/1000 | Loss: 0.00002171
Iteration 43/1000 | Loss: 0.00002170
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002169
Iteration 46/1000 | Loss: 0.00002167
Iteration 47/1000 | Loss: 0.00002167
Iteration 48/1000 | Loss: 0.00002167
Iteration 49/1000 | Loss: 0.00002167
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002165
Iteration 52/1000 | Loss: 0.00002164
Iteration 53/1000 | Loss: 0.00002164
Iteration 54/1000 | Loss: 0.00002163
Iteration 55/1000 | Loss: 0.00002163
Iteration 56/1000 | Loss: 0.00002163
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002162
Iteration 59/1000 | Loss: 0.00002162
Iteration 60/1000 | Loss: 0.00002162
Iteration 61/1000 | Loss: 0.00002161
Iteration 62/1000 | Loss: 0.00002161
Iteration 63/1000 | Loss: 0.00002161
Iteration 64/1000 | Loss: 0.00002161
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002161
Iteration 67/1000 | Loss: 0.00002161
Iteration 68/1000 | Loss: 0.00002160
Iteration 69/1000 | Loss: 0.00002160
Iteration 70/1000 | Loss: 0.00002160
Iteration 71/1000 | Loss: 0.00002160
Iteration 72/1000 | Loss: 0.00002159
Iteration 73/1000 | Loss: 0.00002159
Iteration 74/1000 | Loss: 0.00002159
Iteration 75/1000 | Loss: 0.00002159
Iteration 76/1000 | Loss: 0.00002159
Iteration 77/1000 | Loss: 0.00002159
Iteration 78/1000 | Loss: 0.00002158
Iteration 79/1000 | Loss: 0.00002158
Iteration 80/1000 | Loss: 0.00002158
Iteration 81/1000 | Loss: 0.00002157
Iteration 82/1000 | Loss: 0.00002157
Iteration 83/1000 | Loss: 0.00002156
Iteration 84/1000 | Loss: 0.00002156
Iteration 85/1000 | Loss: 0.00002155
Iteration 86/1000 | Loss: 0.00002155
Iteration 87/1000 | Loss: 0.00002155
Iteration 88/1000 | Loss: 0.00002155
Iteration 89/1000 | Loss: 0.00002154
Iteration 90/1000 | Loss: 0.00002154
Iteration 91/1000 | Loss: 0.00002154
Iteration 92/1000 | Loss: 0.00002154
Iteration 93/1000 | Loss: 0.00002154
Iteration 94/1000 | Loss: 0.00002153
Iteration 95/1000 | Loss: 0.00002153
Iteration 96/1000 | Loss: 0.00002153
Iteration 97/1000 | Loss: 0.00002152
Iteration 98/1000 | Loss: 0.00002152
Iteration 99/1000 | Loss: 0.00002152
Iteration 100/1000 | Loss: 0.00002152
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002151
Iteration 103/1000 | Loss: 0.00002151
Iteration 104/1000 | Loss: 0.00002151
Iteration 105/1000 | Loss: 0.00002151
Iteration 106/1000 | Loss: 0.00002151
Iteration 107/1000 | Loss: 0.00002151
Iteration 108/1000 | Loss: 0.00002151
Iteration 109/1000 | Loss: 0.00002151
Iteration 110/1000 | Loss: 0.00002150
Iteration 111/1000 | Loss: 0.00002150
Iteration 112/1000 | Loss: 0.00002150
Iteration 113/1000 | Loss: 0.00002150
Iteration 114/1000 | Loss: 0.00002150
Iteration 115/1000 | Loss: 0.00002150
Iteration 116/1000 | Loss: 0.00002149
Iteration 117/1000 | Loss: 0.00002149
Iteration 118/1000 | Loss: 0.00002149
Iteration 119/1000 | Loss: 0.00002149
Iteration 120/1000 | Loss: 0.00002149
Iteration 121/1000 | Loss: 0.00002149
Iteration 122/1000 | Loss: 0.00002148
Iteration 123/1000 | Loss: 0.00002148
Iteration 124/1000 | Loss: 0.00002148
Iteration 125/1000 | Loss: 0.00002147
Iteration 126/1000 | Loss: 0.00002147
Iteration 127/1000 | Loss: 0.00002147
Iteration 128/1000 | Loss: 0.00002147
Iteration 129/1000 | Loss: 0.00002147
Iteration 130/1000 | Loss: 0.00002147
Iteration 131/1000 | Loss: 0.00002147
Iteration 132/1000 | Loss: 0.00002147
Iteration 133/1000 | Loss: 0.00002147
Iteration 134/1000 | Loss: 0.00002147
Iteration 135/1000 | Loss: 0.00002147
Iteration 136/1000 | Loss: 0.00002147
Iteration 137/1000 | Loss: 0.00002147
Iteration 138/1000 | Loss: 0.00002147
Iteration 139/1000 | Loss: 0.00002147
Iteration 140/1000 | Loss: 0.00002146
Iteration 141/1000 | Loss: 0.00002146
Iteration 142/1000 | Loss: 0.00002146
Iteration 143/1000 | Loss: 0.00002145
Iteration 144/1000 | Loss: 0.00002145
Iteration 145/1000 | Loss: 0.00002145
Iteration 146/1000 | Loss: 0.00002145
Iteration 147/1000 | Loss: 0.00002145
Iteration 148/1000 | Loss: 0.00002144
Iteration 149/1000 | Loss: 0.00002144
Iteration 150/1000 | Loss: 0.00002144
Iteration 151/1000 | Loss: 0.00002144
Iteration 152/1000 | Loss: 0.00002144
Iteration 153/1000 | Loss: 0.00002144
Iteration 154/1000 | Loss: 0.00002144
Iteration 155/1000 | Loss: 0.00002144
Iteration 156/1000 | Loss: 0.00002144
Iteration 157/1000 | Loss: 0.00002143
Iteration 158/1000 | Loss: 0.00002143
Iteration 159/1000 | Loss: 0.00002143
Iteration 160/1000 | Loss: 0.00002143
Iteration 161/1000 | Loss: 0.00002143
Iteration 162/1000 | Loss: 0.00002143
Iteration 163/1000 | Loss: 0.00002143
Iteration 164/1000 | Loss: 0.00002143
Iteration 165/1000 | Loss: 0.00002143
Iteration 166/1000 | Loss: 0.00002143
Iteration 167/1000 | Loss: 0.00002143
Iteration 168/1000 | Loss: 0.00002143
Iteration 169/1000 | Loss: 0.00002143
Iteration 170/1000 | Loss: 0.00002143
Iteration 171/1000 | Loss: 0.00002143
Iteration 172/1000 | Loss: 0.00002143
Iteration 173/1000 | Loss: 0.00002143
Iteration 174/1000 | Loss: 0.00002143
Iteration 175/1000 | Loss: 0.00002143
Iteration 176/1000 | Loss: 0.00002143
Iteration 177/1000 | Loss: 0.00002143
Iteration 178/1000 | Loss: 0.00002143
Iteration 179/1000 | Loss: 0.00002143
Iteration 180/1000 | Loss: 0.00002143
Iteration 181/1000 | Loss: 0.00002143
Iteration 182/1000 | Loss: 0.00002143
Iteration 183/1000 | Loss: 0.00002143
Iteration 184/1000 | Loss: 0.00002143
Iteration 185/1000 | Loss: 0.00002143
Iteration 186/1000 | Loss: 0.00002143
Iteration 187/1000 | Loss: 0.00002143
Iteration 188/1000 | Loss: 0.00002143
Iteration 189/1000 | Loss: 0.00002143
Iteration 190/1000 | Loss: 0.00002143
Iteration 191/1000 | Loss: 0.00002143
Iteration 192/1000 | Loss: 0.00002143
Iteration 193/1000 | Loss: 0.00002143
Iteration 194/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.143346137017943e-05, 2.143346137017943e-05, 2.143346137017943e-05, 2.143346137017943e-05, 2.143346137017943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.143346137017943e-05

Optimization complete. Final v2v error: 3.804809808731079 mm

Highest mean error: 4.452696800231934 mm for frame 22

Lowest mean error: 3.2726783752441406 mm for frame 210

Saving results

Total time: 45.04840970039368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081595
Iteration 2/25 | Loss: 0.00227389
Iteration 3/25 | Loss: 0.00151353
Iteration 4/25 | Loss: 0.00143030
Iteration 5/25 | Loss: 0.00137992
Iteration 6/25 | Loss: 0.00133955
Iteration 7/25 | Loss: 0.00132122
Iteration 8/25 | Loss: 0.00131401
Iteration 9/25 | Loss: 0.00130425
Iteration 10/25 | Loss: 0.00129058
Iteration 11/25 | Loss: 0.00126679
Iteration 12/25 | Loss: 0.00124667
Iteration 13/25 | Loss: 0.00123768
Iteration 14/25 | Loss: 0.00121448
Iteration 15/25 | Loss: 0.00118979
Iteration 16/25 | Loss: 0.00117863
Iteration 17/25 | Loss: 0.00117542
Iteration 18/25 | Loss: 0.00116803
Iteration 19/25 | Loss: 0.00116094
Iteration 20/25 | Loss: 0.00115596
Iteration 21/25 | Loss: 0.00115750
Iteration 22/25 | Loss: 0.00114715
Iteration 23/25 | Loss: 0.00114269
Iteration 24/25 | Loss: 0.00113959
Iteration 25/25 | Loss: 0.00113858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36271274
Iteration 2/25 | Loss: 0.00193654
Iteration 3/25 | Loss: 0.00193654
Iteration 4/25 | Loss: 0.00193654
Iteration 5/25 | Loss: 0.00193654
Iteration 6/25 | Loss: 0.00193653
Iteration 7/25 | Loss: 0.00193653
Iteration 8/25 | Loss: 0.00193653
Iteration 9/25 | Loss: 0.00193653
Iteration 10/25 | Loss: 0.00193653
Iteration 11/25 | Loss: 0.00193653
Iteration 12/25 | Loss: 0.00193653
Iteration 13/25 | Loss: 0.00193653
Iteration 14/25 | Loss: 0.00193653
Iteration 15/25 | Loss: 0.00193653
Iteration 16/25 | Loss: 0.00193653
Iteration 17/25 | Loss: 0.00193653
Iteration 18/25 | Loss: 0.00193653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001936533604748547, 0.001936533604748547, 0.001936533604748547, 0.001936533604748547, 0.001936533604748547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001936533604748547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193653
Iteration 2/1000 | Loss: 0.00026990
Iteration 3/1000 | Loss: 0.00021557
Iteration 4/1000 | Loss: 0.00108275
Iteration 5/1000 | Loss: 0.00120678
Iteration 6/1000 | Loss: 0.00064202
Iteration 7/1000 | Loss: 0.00136700
Iteration 8/1000 | Loss: 0.00027796
Iteration 9/1000 | Loss: 0.00018198
Iteration 10/1000 | Loss: 0.00015970
Iteration 11/1000 | Loss: 0.00013780
Iteration 12/1000 | Loss: 0.00012032
Iteration 13/1000 | Loss: 0.00010701
Iteration 14/1000 | Loss: 0.00009767
Iteration 15/1000 | Loss: 0.00009090
Iteration 16/1000 | Loss: 0.00070332
Iteration 17/1000 | Loss: 0.00030480
Iteration 18/1000 | Loss: 0.00072006
Iteration 19/1000 | Loss: 0.00010050
Iteration 20/1000 | Loss: 0.00009042
Iteration 21/1000 | Loss: 0.00008514
Iteration 22/1000 | Loss: 0.00106563
Iteration 23/1000 | Loss: 0.00242082
Iteration 24/1000 | Loss: 0.00555930
Iteration 25/1000 | Loss: 0.00088573
Iteration 26/1000 | Loss: 0.00016885
Iteration 27/1000 | Loss: 0.00011322
Iteration 28/1000 | Loss: 0.00007194
Iteration 29/1000 | Loss: 0.00004480
Iteration 30/1000 | Loss: 0.00003345
Iteration 31/1000 | Loss: 0.00002836
Iteration 32/1000 | Loss: 0.00002444
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00001988
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00065672
Iteration 39/1000 | Loss: 0.00066316
Iteration 40/1000 | Loss: 0.00041846
Iteration 41/1000 | Loss: 0.00002959
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001677
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001362
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001206
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001177
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001164
Iteration 53/1000 | Loss: 0.00001160
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001149
Iteration 56/1000 | Loss: 0.00001149
Iteration 57/1000 | Loss: 0.00001148
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001143
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001141
Iteration 66/1000 | Loss: 0.00001141
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001140
Iteration 75/1000 | Loss: 0.00001140
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001138
Iteration 78/1000 | Loss: 0.00001138
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001137
Iteration 81/1000 | Loss: 0.00063417
Iteration 82/1000 | Loss: 0.00023296
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001168
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00062852
Iteration 87/1000 | Loss: 0.00028575
Iteration 88/1000 | Loss: 0.00001270
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001142
Iteration 92/1000 | Loss: 0.00001138
Iteration 93/1000 | Loss: 0.00061999
Iteration 94/1000 | Loss: 0.00001938
Iteration 95/1000 | Loss: 0.00001397
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001091
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00000969
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000932
Iteration 102/1000 | Loss: 0.00000931
Iteration 103/1000 | Loss: 0.00000923
Iteration 104/1000 | Loss: 0.00000923
Iteration 105/1000 | Loss: 0.00000922
Iteration 106/1000 | Loss: 0.00000921
Iteration 107/1000 | Loss: 0.00000921
Iteration 108/1000 | Loss: 0.00000921
Iteration 109/1000 | Loss: 0.00000921
Iteration 110/1000 | Loss: 0.00000920
Iteration 111/1000 | Loss: 0.00000920
Iteration 112/1000 | Loss: 0.00000920
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000918
Iteration 117/1000 | Loss: 0.00000918
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000915
Iteration 122/1000 | Loss: 0.00000915
Iteration 123/1000 | Loss: 0.00000915
Iteration 124/1000 | Loss: 0.00000915
Iteration 125/1000 | Loss: 0.00000915
Iteration 126/1000 | Loss: 0.00000915
Iteration 127/1000 | Loss: 0.00000915
Iteration 128/1000 | Loss: 0.00000915
Iteration 129/1000 | Loss: 0.00000914
Iteration 130/1000 | Loss: 0.00000914
Iteration 131/1000 | Loss: 0.00000913
Iteration 132/1000 | Loss: 0.00000913
Iteration 133/1000 | Loss: 0.00000913
Iteration 134/1000 | Loss: 0.00000913
Iteration 135/1000 | Loss: 0.00000913
Iteration 136/1000 | Loss: 0.00000912
Iteration 137/1000 | Loss: 0.00000912
Iteration 138/1000 | Loss: 0.00000912
Iteration 139/1000 | Loss: 0.00000912
Iteration 140/1000 | Loss: 0.00000912
Iteration 141/1000 | Loss: 0.00000911
Iteration 142/1000 | Loss: 0.00000911
Iteration 143/1000 | Loss: 0.00000911
Iteration 144/1000 | Loss: 0.00000910
Iteration 145/1000 | Loss: 0.00000910
Iteration 146/1000 | Loss: 0.00000910
Iteration 147/1000 | Loss: 0.00000910
Iteration 148/1000 | Loss: 0.00000910
Iteration 149/1000 | Loss: 0.00000910
Iteration 150/1000 | Loss: 0.00000910
Iteration 151/1000 | Loss: 0.00000910
Iteration 152/1000 | Loss: 0.00000910
Iteration 153/1000 | Loss: 0.00000910
Iteration 154/1000 | Loss: 0.00000910
Iteration 155/1000 | Loss: 0.00000909
Iteration 156/1000 | Loss: 0.00000909
Iteration 157/1000 | Loss: 0.00000908
Iteration 158/1000 | Loss: 0.00000908
Iteration 159/1000 | Loss: 0.00000908
Iteration 160/1000 | Loss: 0.00000908
Iteration 161/1000 | Loss: 0.00000908
Iteration 162/1000 | Loss: 0.00000908
Iteration 163/1000 | Loss: 0.00000907
Iteration 164/1000 | Loss: 0.00000907
Iteration 165/1000 | Loss: 0.00000907
Iteration 166/1000 | Loss: 0.00000907
Iteration 167/1000 | Loss: 0.00000907
Iteration 168/1000 | Loss: 0.00000907
Iteration 169/1000 | Loss: 0.00000907
Iteration 170/1000 | Loss: 0.00000907
Iteration 171/1000 | Loss: 0.00000907
Iteration 172/1000 | Loss: 0.00000907
Iteration 173/1000 | Loss: 0.00000907
Iteration 174/1000 | Loss: 0.00000907
Iteration 175/1000 | Loss: 0.00000907
Iteration 176/1000 | Loss: 0.00000907
Iteration 177/1000 | Loss: 0.00000907
Iteration 178/1000 | Loss: 0.00000906
Iteration 179/1000 | Loss: 0.00000906
Iteration 180/1000 | Loss: 0.00000906
Iteration 181/1000 | Loss: 0.00000906
Iteration 182/1000 | Loss: 0.00000906
Iteration 183/1000 | Loss: 0.00000906
Iteration 184/1000 | Loss: 0.00000906
Iteration 185/1000 | Loss: 0.00000906
Iteration 186/1000 | Loss: 0.00000906
Iteration 187/1000 | Loss: 0.00000906
Iteration 188/1000 | Loss: 0.00000906
Iteration 189/1000 | Loss: 0.00000906
Iteration 190/1000 | Loss: 0.00000906
Iteration 191/1000 | Loss: 0.00000906
Iteration 192/1000 | Loss: 0.00000906
Iteration 193/1000 | Loss: 0.00000906
Iteration 194/1000 | Loss: 0.00000906
Iteration 195/1000 | Loss: 0.00000906
Iteration 196/1000 | Loss: 0.00000906
Iteration 197/1000 | Loss: 0.00000906
Iteration 198/1000 | Loss: 0.00000906
Iteration 199/1000 | Loss: 0.00000906
Iteration 200/1000 | Loss: 0.00000906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [9.060730008059181e-06, 9.060730008059181e-06, 9.060730008059181e-06, 9.060730008059181e-06, 9.060730008059181e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.060730008059181e-06

Optimization complete. Final v2v error: 2.4792845249176025 mm

Highest mean error: 8.254798889160156 mm for frame 28

Lowest mean error: 2.0602242946624756 mm for frame 18

Saving results

Total time: 158.53320574760437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513777
Iteration 2/25 | Loss: 0.00116348
Iteration 3/25 | Loss: 0.00099125
Iteration 4/25 | Loss: 0.00097801
Iteration 5/25 | Loss: 0.00097801
Iteration 6/25 | Loss: 0.00097801
Iteration 7/25 | Loss: 0.00097801
Iteration 8/25 | Loss: 0.00097801
Iteration 9/25 | Loss: 0.00097801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0009780062828212976, 0.0009780062828212976, 0.0009780062828212976, 0.0009780062828212976, 0.0009780062828212976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009780062828212976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80165565
Iteration 2/25 | Loss: 0.00031364
Iteration 3/25 | Loss: 0.00031364
Iteration 4/25 | Loss: 0.00031364
Iteration 5/25 | Loss: 0.00031364
Iteration 6/25 | Loss: 0.00031364
Iteration 7/25 | Loss: 0.00031364
Iteration 8/25 | Loss: 0.00031364
Iteration 9/25 | Loss: 0.00031364
Iteration 10/25 | Loss: 0.00031364
Iteration 11/25 | Loss: 0.00031364
Iteration 12/25 | Loss: 0.00031364
Iteration 13/25 | Loss: 0.00031364
Iteration 14/25 | Loss: 0.00031364
Iteration 15/25 | Loss: 0.00031364
Iteration 16/25 | Loss: 0.00031364
Iteration 17/25 | Loss: 0.00031364
Iteration 18/25 | Loss: 0.00031364
Iteration 19/25 | Loss: 0.00031364
Iteration 20/25 | Loss: 0.00031364
Iteration 21/25 | Loss: 0.00031364
Iteration 22/25 | Loss: 0.00031364
Iteration 23/25 | Loss: 0.00031364
Iteration 24/25 | Loss: 0.00031364
Iteration 25/25 | Loss: 0.00031364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031364
Iteration 2/1000 | Loss: 0.00002292
Iteration 3/1000 | Loss: 0.00001629
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001400
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001399
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001375
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001374
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001368
Iteration 32/1000 | Loss: 0.00001363
Iteration 33/1000 | Loss: 0.00001357
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001339
Iteration 63/1000 | Loss: 0.00001339
Iteration 64/1000 | Loss: 0.00001339
Iteration 65/1000 | Loss: 0.00001339
Iteration 66/1000 | Loss: 0.00001338
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001336
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001330
Iteration 84/1000 | Loss: 0.00001330
Iteration 85/1000 | Loss: 0.00001330
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001330
Iteration 93/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.3302796105563175e-05, 1.3302796105563175e-05, 1.3302796105563175e-05, 1.3302796105563175e-05, 1.3302796105563175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3302796105563175e-05

Optimization complete. Final v2v error: 3.0413546562194824 mm

Highest mean error: 3.0738704204559326 mm for frame 247

Lowest mean error: 2.987250804901123 mm for frame 173

Saving results

Total time: 29.771225452423096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877860
Iteration 2/25 | Loss: 0.00126599
Iteration 3/25 | Loss: 0.00108980
Iteration 4/25 | Loss: 0.00106509
Iteration 5/25 | Loss: 0.00105803
Iteration 6/25 | Loss: 0.00105628
Iteration 7/25 | Loss: 0.00105628
Iteration 8/25 | Loss: 0.00105628
Iteration 9/25 | Loss: 0.00105628
Iteration 10/25 | Loss: 0.00105628
Iteration 11/25 | Loss: 0.00105628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010562792886048555, 0.0010562792886048555, 0.0010562792886048555, 0.0010562792886048555, 0.0010562792886048555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010562792886048555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15621722
Iteration 2/25 | Loss: 0.00049262
Iteration 3/25 | Loss: 0.00049262
Iteration 4/25 | Loss: 0.00049262
Iteration 5/25 | Loss: 0.00049262
Iteration 6/25 | Loss: 0.00049262
Iteration 7/25 | Loss: 0.00049262
Iteration 8/25 | Loss: 0.00049262
Iteration 9/25 | Loss: 0.00049262
Iteration 10/25 | Loss: 0.00049262
Iteration 11/25 | Loss: 0.00049262
Iteration 12/25 | Loss: 0.00049262
Iteration 13/25 | Loss: 0.00049262
Iteration 14/25 | Loss: 0.00049262
Iteration 15/25 | Loss: 0.00049262
Iteration 16/25 | Loss: 0.00049262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004926174296997488, 0.0004926174296997488, 0.0004926174296997488, 0.0004926174296997488, 0.0004926174296997488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004926174296997488

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049262
Iteration 2/1000 | Loss: 0.00003831
Iteration 3/1000 | Loss: 0.00002649
Iteration 4/1000 | Loss: 0.00002411
Iteration 5/1000 | Loss: 0.00002284
Iteration 6/1000 | Loss: 0.00002233
Iteration 7/1000 | Loss: 0.00002175
Iteration 8/1000 | Loss: 0.00002143
Iteration 9/1000 | Loss: 0.00002112
Iteration 10/1000 | Loss: 0.00002103
Iteration 11/1000 | Loss: 0.00002091
Iteration 12/1000 | Loss: 0.00002087
Iteration 13/1000 | Loss: 0.00002075
Iteration 14/1000 | Loss: 0.00002067
Iteration 15/1000 | Loss: 0.00002067
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002063
Iteration 19/1000 | Loss: 0.00002062
Iteration 20/1000 | Loss: 0.00002061
Iteration 21/1000 | Loss: 0.00002061
Iteration 22/1000 | Loss: 0.00002060
Iteration 23/1000 | Loss: 0.00002060
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002059
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002054
Iteration 29/1000 | Loss: 0.00002054
Iteration 30/1000 | Loss: 0.00002050
Iteration 31/1000 | Loss: 0.00002046
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002044
Iteration 34/1000 | Loss: 0.00002043
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002042
Iteration 38/1000 | Loss: 0.00002041
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002041
Iteration 42/1000 | Loss: 0.00002040
Iteration 43/1000 | Loss: 0.00002040
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002040
Iteration 46/1000 | Loss: 0.00002040
Iteration 47/1000 | Loss: 0.00002040
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002039
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002039
Iteration 52/1000 | Loss: 0.00002039
Iteration 53/1000 | Loss: 0.00002037
Iteration 54/1000 | Loss: 0.00002037
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002036
Iteration 60/1000 | Loss: 0.00002036
Iteration 61/1000 | Loss: 0.00002036
Iteration 62/1000 | Loss: 0.00002036
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002036
Iteration 66/1000 | Loss: 0.00002036
Iteration 67/1000 | Loss: 0.00002036
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00002035
Iteration 70/1000 | Loss: 0.00002034
Iteration 71/1000 | Loss: 0.00002034
Iteration 72/1000 | Loss: 0.00002034
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002033
Iteration 75/1000 | Loss: 0.00002033
Iteration 76/1000 | Loss: 0.00002033
Iteration 77/1000 | Loss: 0.00002033
Iteration 78/1000 | Loss: 0.00002032
Iteration 79/1000 | Loss: 0.00002032
Iteration 80/1000 | Loss: 0.00002032
Iteration 81/1000 | Loss: 0.00002032
Iteration 82/1000 | Loss: 0.00002031
Iteration 83/1000 | Loss: 0.00002031
Iteration 84/1000 | Loss: 0.00002031
Iteration 85/1000 | Loss: 0.00002031
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00002031
Iteration 88/1000 | Loss: 0.00002031
Iteration 89/1000 | Loss: 0.00002031
Iteration 90/1000 | Loss: 0.00002031
Iteration 91/1000 | Loss: 0.00002031
Iteration 92/1000 | Loss: 0.00002031
Iteration 93/1000 | Loss: 0.00002031
Iteration 94/1000 | Loss: 0.00002031
Iteration 95/1000 | Loss: 0.00002031
Iteration 96/1000 | Loss: 0.00002031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.0307596059865318e-05, 2.0307596059865318e-05, 2.0307596059865318e-05, 2.0307596059865318e-05, 2.0307596059865318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0307596059865318e-05

Optimization complete. Final v2v error: 3.7142655849456787 mm

Highest mean error: 4.938846111297607 mm for frame 126

Lowest mean error: 2.777970314025879 mm for frame 51

Saving results

Total time: 37.77456879615784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491529
Iteration 2/25 | Loss: 0.00108451
Iteration 3/25 | Loss: 0.00096488
Iteration 4/25 | Loss: 0.00094840
Iteration 5/25 | Loss: 0.00094385
Iteration 6/25 | Loss: 0.00094248
Iteration 7/25 | Loss: 0.00094238
Iteration 8/25 | Loss: 0.00094238
Iteration 9/25 | Loss: 0.00094238
Iteration 10/25 | Loss: 0.00094238
Iteration 11/25 | Loss: 0.00094238
Iteration 12/25 | Loss: 0.00094238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009423812152817845, 0.0009423812152817845, 0.0009423812152817845, 0.0009423812152817845, 0.0009423812152817845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009423812152817845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35689628
Iteration 2/25 | Loss: 0.00056652
Iteration 3/25 | Loss: 0.00056650
Iteration 4/25 | Loss: 0.00056650
Iteration 5/25 | Loss: 0.00056650
Iteration 6/25 | Loss: 0.00056649
Iteration 7/25 | Loss: 0.00056649
Iteration 8/25 | Loss: 0.00056649
Iteration 9/25 | Loss: 0.00056649
Iteration 10/25 | Loss: 0.00056649
Iteration 11/25 | Loss: 0.00056649
Iteration 12/25 | Loss: 0.00056649
Iteration 13/25 | Loss: 0.00056649
Iteration 14/25 | Loss: 0.00056649
Iteration 15/25 | Loss: 0.00056649
Iteration 16/25 | Loss: 0.00056649
Iteration 17/25 | Loss: 0.00056649
Iteration 18/25 | Loss: 0.00056649
Iteration 19/25 | Loss: 0.00056649
Iteration 20/25 | Loss: 0.00056649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005664931377395988, 0.0005664931377395988, 0.0005664931377395988, 0.0005664931377395988, 0.0005664931377395988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005664931377395988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056649
Iteration 2/1000 | Loss: 0.00002171
Iteration 3/1000 | Loss: 0.00001403
Iteration 4/1000 | Loss: 0.00001250
Iteration 5/1000 | Loss: 0.00001188
Iteration 6/1000 | Loss: 0.00001157
Iteration 7/1000 | Loss: 0.00001130
Iteration 8/1000 | Loss: 0.00001109
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001077
Iteration 11/1000 | Loss: 0.00001075
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001072
Iteration 14/1000 | Loss: 0.00001071
Iteration 15/1000 | Loss: 0.00001063
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001059
Iteration 19/1000 | Loss: 0.00001059
Iteration 20/1000 | Loss: 0.00001058
Iteration 21/1000 | Loss: 0.00001058
Iteration 22/1000 | Loss: 0.00001057
Iteration 23/1000 | Loss: 0.00001057
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001051
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001048
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001048
Iteration 36/1000 | Loss: 0.00001048
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001047
Iteration 39/1000 | Loss: 0.00001047
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001047
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001046
Iteration 44/1000 | Loss: 0.00001045
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001045
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001044
Iteration 49/1000 | Loss: 0.00001044
Iteration 50/1000 | Loss: 0.00001043
Iteration 51/1000 | Loss: 0.00001043
Iteration 52/1000 | Loss: 0.00001042
Iteration 53/1000 | Loss: 0.00001042
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001040
Iteration 60/1000 | Loss: 0.00001040
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001039
Iteration 67/1000 | Loss: 0.00001039
Iteration 68/1000 | Loss: 0.00001039
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001037
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001037
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001036
Iteration 83/1000 | Loss: 0.00001036
Iteration 84/1000 | Loss: 0.00001036
Iteration 85/1000 | Loss: 0.00001036
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001035
Iteration 90/1000 | Loss: 0.00001035
Iteration 91/1000 | Loss: 0.00001035
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001035
Iteration 94/1000 | Loss: 0.00001035
Iteration 95/1000 | Loss: 0.00001035
Iteration 96/1000 | Loss: 0.00001035
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001035
Iteration 100/1000 | Loss: 0.00001035
Iteration 101/1000 | Loss: 0.00001035
Iteration 102/1000 | Loss: 0.00001035
Iteration 103/1000 | Loss: 0.00001035
Iteration 104/1000 | Loss: 0.00001035
Iteration 105/1000 | Loss: 0.00001034
Iteration 106/1000 | Loss: 0.00001034
Iteration 107/1000 | Loss: 0.00001034
Iteration 108/1000 | Loss: 0.00001034
Iteration 109/1000 | Loss: 0.00001034
Iteration 110/1000 | Loss: 0.00001034
Iteration 111/1000 | Loss: 0.00001033
Iteration 112/1000 | Loss: 0.00001033
Iteration 113/1000 | Loss: 0.00001033
Iteration 114/1000 | Loss: 0.00001033
Iteration 115/1000 | Loss: 0.00001033
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001033
Iteration 118/1000 | Loss: 0.00001033
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001033
Iteration 121/1000 | Loss: 0.00001033
Iteration 122/1000 | Loss: 0.00001033
Iteration 123/1000 | Loss: 0.00001033
Iteration 124/1000 | Loss: 0.00001033
Iteration 125/1000 | Loss: 0.00001033
Iteration 126/1000 | Loss: 0.00001033
Iteration 127/1000 | Loss: 0.00001033
Iteration 128/1000 | Loss: 0.00001033
Iteration 129/1000 | Loss: 0.00001033
Iteration 130/1000 | Loss: 0.00001033
Iteration 131/1000 | Loss: 0.00001033
Iteration 132/1000 | Loss: 0.00001033
Iteration 133/1000 | Loss: 0.00001033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.0328164535167161e-05, 1.0328164535167161e-05, 1.0328164535167161e-05, 1.0328164535167161e-05, 1.0328164535167161e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0328164535167161e-05

Optimization complete. Final v2v error: 2.7484848499298096 mm

Highest mean error: 3.4581830501556396 mm for frame 49

Lowest mean error: 2.4229249954223633 mm for frame 0

Saving results

Total time: 32.75833559036255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646638
Iteration 2/25 | Loss: 0.00107280
Iteration 3/25 | Loss: 0.00095577
Iteration 4/25 | Loss: 0.00094327
Iteration 5/25 | Loss: 0.00093892
Iteration 6/25 | Loss: 0.00093795
Iteration 7/25 | Loss: 0.00093795
Iteration 8/25 | Loss: 0.00093795
Iteration 9/25 | Loss: 0.00093795
Iteration 10/25 | Loss: 0.00093795
Iteration 11/25 | Loss: 0.00093795
Iteration 12/25 | Loss: 0.00093795
Iteration 13/25 | Loss: 0.00093795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009379507973790169, 0.0009379507973790169, 0.0009379507973790169, 0.0009379507973790169, 0.0009379507973790169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009379507973790169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38267910
Iteration 2/25 | Loss: 0.00061946
Iteration 3/25 | Loss: 0.00061946
Iteration 4/25 | Loss: 0.00061946
Iteration 5/25 | Loss: 0.00061946
Iteration 6/25 | Loss: 0.00061946
Iteration 7/25 | Loss: 0.00061946
Iteration 8/25 | Loss: 0.00061946
Iteration 9/25 | Loss: 0.00061946
Iteration 10/25 | Loss: 0.00061946
Iteration 11/25 | Loss: 0.00061946
Iteration 12/25 | Loss: 0.00061946
Iteration 13/25 | Loss: 0.00061946
Iteration 14/25 | Loss: 0.00061946
Iteration 15/25 | Loss: 0.00061946
Iteration 16/25 | Loss: 0.00061946
Iteration 17/25 | Loss: 0.00061946
Iteration 18/25 | Loss: 0.00061946
Iteration 19/25 | Loss: 0.00061946
Iteration 20/25 | Loss: 0.00061946
Iteration 21/25 | Loss: 0.00061946
Iteration 22/25 | Loss: 0.00061946
Iteration 23/25 | Loss: 0.00061946
Iteration 24/25 | Loss: 0.00061946
Iteration 25/25 | Loss: 0.00061946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006194610032252967, 0.0006194610032252967, 0.0006194610032252967, 0.0006194610032252967, 0.0006194610032252967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006194610032252967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061946
Iteration 2/1000 | Loss: 0.00001815
Iteration 3/1000 | Loss: 0.00001270
Iteration 4/1000 | Loss: 0.00001143
Iteration 5/1000 | Loss: 0.00001096
Iteration 6/1000 | Loss: 0.00001049
Iteration 7/1000 | Loss: 0.00001017
Iteration 8/1000 | Loss: 0.00001003
Iteration 9/1000 | Loss: 0.00000989
Iteration 10/1000 | Loss: 0.00000984
Iteration 11/1000 | Loss: 0.00000983
Iteration 12/1000 | Loss: 0.00000982
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000973
Iteration 15/1000 | Loss: 0.00000972
Iteration 16/1000 | Loss: 0.00000966
Iteration 17/1000 | Loss: 0.00000966
Iteration 18/1000 | Loss: 0.00000966
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000965
Iteration 21/1000 | Loss: 0.00000965
Iteration 22/1000 | Loss: 0.00000964
Iteration 23/1000 | Loss: 0.00000964
Iteration 24/1000 | Loss: 0.00000962
Iteration 25/1000 | Loss: 0.00000961
Iteration 26/1000 | Loss: 0.00000961
Iteration 27/1000 | Loss: 0.00000961
Iteration 28/1000 | Loss: 0.00000961
Iteration 29/1000 | Loss: 0.00000960
Iteration 30/1000 | Loss: 0.00000960
Iteration 31/1000 | Loss: 0.00000959
Iteration 32/1000 | Loss: 0.00000959
Iteration 33/1000 | Loss: 0.00000958
Iteration 34/1000 | Loss: 0.00000957
Iteration 35/1000 | Loss: 0.00000956
Iteration 36/1000 | Loss: 0.00000956
Iteration 37/1000 | Loss: 0.00000956
Iteration 38/1000 | Loss: 0.00000956
Iteration 39/1000 | Loss: 0.00000956
Iteration 40/1000 | Loss: 0.00000956
Iteration 41/1000 | Loss: 0.00000956
Iteration 42/1000 | Loss: 0.00000955
Iteration 43/1000 | Loss: 0.00000954
Iteration 44/1000 | Loss: 0.00000954
Iteration 45/1000 | Loss: 0.00000953
Iteration 46/1000 | Loss: 0.00000953
Iteration 47/1000 | Loss: 0.00000952
Iteration 48/1000 | Loss: 0.00000952
Iteration 49/1000 | Loss: 0.00000952
Iteration 50/1000 | Loss: 0.00000952
Iteration 51/1000 | Loss: 0.00000952
Iteration 52/1000 | Loss: 0.00000952
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000951
Iteration 55/1000 | Loss: 0.00000951
Iteration 56/1000 | Loss: 0.00000951
Iteration 57/1000 | Loss: 0.00000951
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000951
Iteration 60/1000 | Loss: 0.00000950
Iteration 61/1000 | Loss: 0.00000950
Iteration 62/1000 | Loss: 0.00000950
Iteration 63/1000 | Loss: 0.00000949
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000949
Iteration 70/1000 | Loss: 0.00000948
Iteration 71/1000 | Loss: 0.00000948
Iteration 72/1000 | Loss: 0.00000948
Iteration 73/1000 | Loss: 0.00000948
Iteration 74/1000 | Loss: 0.00000948
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000948
Iteration 77/1000 | Loss: 0.00000948
Iteration 78/1000 | Loss: 0.00000948
Iteration 79/1000 | Loss: 0.00000948
Iteration 80/1000 | Loss: 0.00000948
Iteration 81/1000 | Loss: 0.00000948
Iteration 82/1000 | Loss: 0.00000948
Iteration 83/1000 | Loss: 0.00000948
Iteration 84/1000 | Loss: 0.00000948
Iteration 85/1000 | Loss: 0.00000947
Iteration 86/1000 | Loss: 0.00000947
Iteration 87/1000 | Loss: 0.00000947
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000946
Iteration 94/1000 | Loss: 0.00000946
Iteration 95/1000 | Loss: 0.00000946
Iteration 96/1000 | Loss: 0.00000946
Iteration 97/1000 | Loss: 0.00000946
Iteration 98/1000 | Loss: 0.00000946
Iteration 99/1000 | Loss: 0.00000946
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000946
Iteration 102/1000 | Loss: 0.00000946
Iteration 103/1000 | Loss: 0.00000945
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000945
Iteration 108/1000 | Loss: 0.00000944
Iteration 109/1000 | Loss: 0.00000944
Iteration 110/1000 | Loss: 0.00000944
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000944
Iteration 114/1000 | Loss: 0.00000944
Iteration 115/1000 | Loss: 0.00000944
Iteration 116/1000 | Loss: 0.00000944
Iteration 117/1000 | Loss: 0.00000944
Iteration 118/1000 | Loss: 0.00000944
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000944
Iteration 121/1000 | Loss: 0.00000943
Iteration 122/1000 | Loss: 0.00000943
Iteration 123/1000 | Loss: 0.00000943
Iteration 124/1000 | Loss: 0.00000943
Iteration 125/1000 | Loss: 0.00000943
Iteration 126/1000 | Loss: 0.00000943
Iteration 127/1000 | Loss: 0.00000943
Iteration 128/1000 | Loss: 0.00000942
Iteration 129/1000 | Loss: 0.00000942
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000942
Iteration 136/1000 | Loss: 0.00000942
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Iteration 143/1000 | Loss: 0.00000941
Iteration 144/1000 | Loss: 0.00000941
Iteration 145/1000 | Loss: 0.00000941
Iteration 146/1000 | Loss: 0.00000941
Iteration 147/1000 | Loss: 0.00000941
Iteration 148/1000 | Loss: 0.00000941
Iteration 149/1000 | Loss: 0.00000941
Iteration 150/1000 | Loss: 0.00000941
Iteration 151/1000 | Loss: 0.00000941
Iteration 152/1000 | Loss: 0.00000941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [9.409101949131582e-06, 9.409101949131582e-06, 9.409101949131582e-06, 9.409101949131582e-06, 9.409101949131582e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.409101949131582e-06

Optimization complete. Final v2v error: 2.5837032794952393 mm

Highest mean error: 3.0995755195617676 mm for frame 10

Lowest mean error: 2.2016706466674805 mm for frame 67

Saving results

Total time: 35.91233229637146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417270
Iteration 2/25 | Loss: 0.00107398
Iteration 3/25 | Loss: 0.00094461
Iteration 4/25 | Loss: 0.00093731
Iteration 5/25 | Loss: 0.00093608
Iteration 6/25 | Loss: 0.00093608
Iteration 7/25 | Loss: 0.00093608
Iteration 8/25 | Loss: 0.00093608
Iteration 9/25 | Loss: 0.00093608
Iteration 10/25 | Loss: 0.00093608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009360757539980114, 0.0009360757539980114, 0.0009360757539980114, 0.0009360757539980114, 0.0009360757539980114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009360757539980114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34994805
Iteration 2/25 | Loss: 0.00055864
Iteration 3/25 | Loss: 0.00055864
Iteration 4/25 | Loss: 0.00055864
Iteration 5/25 | Loss: 0.00055864
Iteration 6/25 | Loss: 0.00055864
Iteration 7/25 | Loss: 0.00055864
Iteration 8/25 | Loss: 0.00055864
Iteration 9/25 | Loss: 0.00055864
Iteration 10/25 | Loss: 0.00055864
Iteration 11/25 | Loss: 0.00055864
Iteration 12/25 | Loss: 0.00055864
Iteration 13/25 | Loss: 0.00055864
Iteration 14/25 | Loss: 0.00055864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005586412153206766, 0.0005586412153206766, 0.0005586412153206766, 0.0005586412153206766, 0.0005586412153206766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005586412153206766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055864
Iteration 2/1000 | Loss: 0.00001557
Iteration 3/1000 | Loss: 0.00001061
Iteration 4/1000 | Loss: 0.00000958
Iteration 5/1000 | Loss: 0.00000921
Iteration 6/1000 | Loss: 0.00000890
Iteration 7/1000 | Loss: 0.00000872
Iteration 8/1000 | Loss: 0.00000864
Iteration 9/1000 | Loss: 0.00000864
Iteration 10/1000 | Loss: 0.00000863
Iteration 11/1000 | Loss: 0.00000852
Iteration 12/1000 | Loss: 0.00000849
Iteration 13/1000 | Loss: 0.00000848
Iteration 14/1000 | Loss: 0.00000847
Iteration 15/1000 | Loss: 0.00000847
Iteration 16/1000 | Loss: 0.00000846
Iteration 17/1000 | Loss: 0.00000839
Iteration 18/1000 | Loss: 0.00000834
Iteration 19/1000 | Loss: 0.00000834
Iteration 20/1000 | Loss: 0.00000834
Iteration 21/1000 | Loss: 0.00000834
Iteration 22/1000 | Loss: 0.00000834
Iteration 23/1000 | Loss: 0.00000834
Iteration 24/1000 | Loss: 0.00000833
Iteration 25/1000 | Loss: 0.00000833
Iteration 26/1000 | Loss: 0.00000833
Iteration 27/1000 | Loss: 0.00000833
Iteration 28/1000 | Loss: 0.00000831
Iteration 29/1000 | Loss: 0.00000830
Iteration 30/1000 | Loss: 0.00000830
Iteration 31/1000 | Loss: 0.00000830
Iteration 32/1000 | Loss: 0.00000829
Iteration 33/1000 | Loss: 0.00000829
Iteration 34/1000 | Loss: 0.00000829
Iteration 35/1000 | Loss: 0.00000829
Iteration 36/1000 | Loss: 0.00000829
Iteration 37/1000 | Loss: 0.00000828
Iteration 38/1000 | Loss: 0.00000828
Iteration 39/1000 | Loss: 0.00000828
Iteration 40/1000 | Loss: 0.00000828
Iteration 41/1000 | Loss: 0.00000828
Iteration 42/1000 | Loss: 0.00000827
Iteration 43/1000 | Loss: 0.00000827
Iteration 44/1000 | Loss: 0.00000826
Iteration 45/1000 | Loss: 0.00000826
Iteration 46/1000 | Loss: 0.00000825
Iteration 47/1000 | Loss: 0.00000825
Iteration 48/1000 | Loss: 0.00000825
Iteration 49/1000 | Loss: 0.00000825
Iteration 50/1000 | Loss: 0.00000825
Iteration 51/1000 | Loss: 0.00000825
Iteration 52/1000 | Loss: 0.00000825
Iteration 53/1000 | Loss: 0.00000825
Iteration 54/1000 | Loss: 0.00000825
Iteration 55/1000 | Loss: 0.00000825
Iteration 56/1000 | Loss: 0.00000824
Iteration 57/1000 | Loss: 0.00000824
Iteration 58/1000 | Loss: 0.00000824
Iteration 59/1000 | Loss: 0.00000824
Iteration 60/1000 | Loss: 0.00000823
Iteration 61/1000 | Loss: 0.00000823
Iteration 62/1000 | Loss: 0.00000823
Iteration 63/1000 | Loss: 0.00000822
Iteration 64/1000 | Loss: 0.00000822
Iteration 65/1000 | Loss: 0.00000822
Iteration 66/1000 | Loss: 0.00000822
Iteration 67/1000 | Loss: 0.00000822
Iteration 68/1000 | Loss: 0.00000822
Iteration 69/1000 | Loss: 0.00000822
Iteration 70/1000 | Loss: 0.00000821
Iteration 71/1000 | Loss: 0.00000821
Iteration 72/1000 | Loss: 0.00000821
Iteration 73/1000 | Loss: 0.00000821
Iteration 74/1000 | Loss: 0.00000821
Iteration 75/1000 | Loss: 0.00000821
Iteration 76/1000 | Loss: 0.00000821
Iteration 77/1000 | Loss: 0.00000821
Iteration 78/1000 | Loss: 0.00000821
Iteration 79/1000 | Loss: 0.00000821
Iteration 80/1000 | Loss: 0.00000821
Iteration 81/1000 | Loss: 0.00000821
Iteration 82/1000 | Loss: 0.00000821
Iteration 83/1000 | Loss: 0.00000821
Iteration 84/1000 | Loss: 0.00000820
Iteration 85/1000 | Loss: 0.00000820
Iteration 86/1000 | Loss: 0.00000820
Iteration 87/1000 | Loss: 0.00000820
Iteration 88/1000 | Loss: 0.00000820
Iteration 89/1000 | Loss: 0.00000820
Iteration 90/1000 | Loss: 0.00000820
Iteration 91/1000 | Loss: 0.00000820
Iteration 92/1000 | Loss: 0.00000820
Iteration 93/1000 | Loss: 0.00000820
Iteration 94/1000 | Loss: 0.00000820
Iteration 95/1000 | Loss: 0.00000820
Iteration 96/1000 | Loss: 0.00000820
Iteration 97/1000 | Loss: 0.00000820
Iteration 98/1000 | Loss: 0.00000820
Iteration 99/1000 | Loss: 0.00000820
Iteration 100/1000 | Loss: 0.00000820
Iteration 101/1000 | Loss: 0.00000820
Iteration 102/1000 | Loss: 0.00000820
Iteration 103/1000 | Loss: 0.00000820
Iteration 104/1000 | Loss: 0.00000820
Iteration 105/1000 | Loss: 0.00000820
Iteration 106/1000 | Loss: 0.00000820
Iteration 107/1000 | Loss: 0.00000820
Iteration 108/1000 | Loss: 0.00000820
Iteration 109/1000 | Loss: 0.00000820
Iteration 110/1000 | Loss: 0.00000820
Iteration 111/1000 | Loss: 0.00000820
Iteration 112/1000 | Loss: 0.00000820
Iteration 113/1000 | Loss: 0.00000820
Iteration 114/1000 | Loss: 0.00000820
Iteration 115/1000 | Loss: 0.00000820
Iteration 116/1000 | Loss: 0.00000820
Iteration 117/1000 | Loss: 0.00000820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [8.196543603844475e-06, 8.196543603844475e-06, 8.196543603844475e-06, 8.196543603844475e-06, 8.196543603844475e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.196543603844475e-06

Optimization complete. Final v2v error: 2.37202787399292 mm

Highest mean error: 2.6926374435424805 mm for frame 74

Lowest mean error: 2.1050705909729004 mm for frame 94

Saving results

Total time: 27.589723825454712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00344381
Iteration 2/25 | Loss: 0.00135381
Iteration 3/25 | Loss: 0.00106603
Iteration 4/25 | Loss: 0.00098056
Iteration 5/25 | Loss: 0.00096589
Iteration 6/25 | Loss: 0.00096252
Iteration 7/25 | Loss: 0.00096156
Iteration 8/25 | Loss: 0.00096128
Iteration 9/25 | Loss: 0.00096127
Iteration 10/25 | Loss: 0.00096127
Iteration 11/25 | Loss: 0.00096127
Iteration 12/25 | Loss: 0.00096127
Iteration 13/25 | Loss: 0.00096127
Iteration 14/25 | Loss: 0.00096127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009612698922865093, 0.0009612698922865093, 0.0009612698922865093, 0.0009612698922865093, 0.0009612698922865093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009612698922865093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32793856
Iteration 2/25 | Loss: 0.00078096
Iteration 3/25 | Loss: 0.00078095
Iteration 4/25 | Loss: 0.00078095
Iteration 5/25 | Loss: 0.00078095
Iteration 6/25 | Loss: 0.00078095
Iteration 7/25 | Loss: 0.00078095
Iteration 8/25 | Loss: 0.00078095
Iteration 9/25 | Loss: 0.00078095
Iteration 10/25 | Loss: 0.00078095
Iteration 11/25 | Loss: 0.00078095
Iteration 12/25 | Loss: 0.00078095
Iteration 13/25 | Loss: 0.00078095
Iteration 14/25 | Loss: 0.00078095
Iteration 15/25 | Loss: 0.00078095
Iteration 16/25 | Loss: 0.00078095
Iteration 17/25 | Loss: 0.00078095
Iteration 18/25 | Loss: 0.00078095
Iteration 19/25 | Loss: 0.00078095
Iteration 20/25 | Loss: 0.00078095
Iteration 21/25 | Loss: 0.00078095
Iteration 22/25 | Loss: 0.00078095
Iteration 23/25 | Loss: 0.00078095
Iteration 24/25 | Loss: 0.00078095
Iteration 25/25 | Loss: 0.00078095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078095
Iteration 2/1000 | Loss: 0.00004022
Iteration 3/1000 | Loss: 0.00002316
Iteration 4/1000 | Loss: 0.00001498
Iteration 5/1000 | Loss: 0.00001375
Iteration 6/1000 | Loss: 0.00001306
Iteration 7/1000 | Loss: 0.00001260
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001216
Iteration 10/1000 | Loss: 0.00001216
Iteration 11/1000 | Loss: 0.00001214
Iteration 12/1000 | Loss: 0.00001211
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001189
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001180
Iteration 38/1000 | Loss: 0.00001179
Iteration 39/1000 | Loss: 0.00001179
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001178
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001177
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001176
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001167
Iteration 75/1000 | Loss: 0.00001167
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001166
Iteration 81/1000 | Loss: 0.00001166
Iteration 82/1000 | Loss: 0.00001166
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001165
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001164
Iteration 95/1000 | Loss: 0.00001164
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001162
Iteration 104/1000 | Loss: 0.00001162
Iteration 105/1000 | Loss: 0.00001162
Iteration 106/1000 | Loss: 0.00001161
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001161
Iteration 110/1000 | Loss: 0.00001161
Iteration 111/1000 | Loss: 0.00001161
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001160
Iteration 128/1000 | Loss: 0.00001160
Iteration 129/1000 | Loss: 0.00001160
Iteration 130/1000 | Loss: 0.00001160
Iteration 131/1000 | Loss: 0.00001160
Iteration 132/1000 | Loss: 0.00001160
Iteration 133/1000 | Loss: 0.00001160
Iteration 134/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.1599960998864844e-05, 1.1599960998864844e-05, 1.1599960998864844e-05, 1.1599960998864844e-05, 1.1599960998864844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1599960998864844e-05

Optimization complete. Final v2v error: 2.8438007831573486 mm

Highest mean error: 3.604109048843384 mm for frame 69

Lowest mean error: 2.356132745742798 mm for frame 151

Saving results

Total time: 34.96814179420471
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486225
Iteration 2/25 | Loss: 0.00108622
Iteration 3/25 | Loss: 0.00098328
Iteration 4/25 | Loss: 0.00096743
Iteration 5/25 | Loss: 0.00096283
Iteration 6/25 | Loss: 0.00096166
Iteration 7/25 | Loss: 0.00096166
Iteration 8/25 | Loss: 0.00096166
Iteration 9/25 | Loss: 0.00096166
Iteration 10/25 | Loss: 0.00096166
Iteration 11/25 | Loss: 0.00096166
Iteration 12/25 | Loss: 0.00096166
Iteration 13/25 | Loss: 0.00096166
Iteration 14/25 | Loss: 0.00096166
Iteration 15/25 | Loss: 0.00096166
Iteration 16/25 | Loss: 0.00096166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000961662211921066, 0.000961662211921066, 0.000961662211921066, 0.000961662211921066, 0.000961662211921066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000961662211921066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37140238
Iteration 2/25 | Loss: 0.00064730
Iteration 3/25 | Loss: 0.00064729
Iteration 4/25 | Loss: 0.00064729
Iteration 5/25 | Loss: 0.00064729
Iteration 6/25 | Loss: 0.00064729
Iteration 7/25 | Loss: 0.00064729
Iteration 8/25 | Loss: 0.00064729
Iteration 9/25 | Loss: 0.00064729
Iteration 10/25 | Loss: 0.00064729
Iteration 11/25 | Loss: 0.00064729
Iteration 12/25 | Loss: 0.00064729
Iteration 13/25 | Loss: 0.00064729
Iteration 14/25 | Loss: 0.00064729
Iteration 15/25 | Loss: 0.00064729
Iteration 16/25 | Loss: 0.00064729
Iteration 17/25 | Loss: 0.00064729
Iteration 18/25 | Loss: 0.00064729
Iteration 19/25 | Loss: 0.00064729
Iteration 20/25 | Loss: 0.00064729
Iteration 21/25 | Loss: 0.00064729
Iteration 22/25 | Loss: 0.00064729
Iteration 23/25 | Loss: 0.00064729
Iteration 24/25 | Loss: 0.00064729
Iteration 25/25 | Loss: 0.00064729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064729
Iteration 2/1000 | Loss: 0.00002264
Iteration 3/1000 | Loss: 0.00001470
Iteration 4/1000 | Loss: 0.00001299
Iteration 5/1000 | Loss: 0.00001243
Iteration 6/1000 | Loss: 0.00001209
Iteration 7/1000 | Loss: 0.00001180
Iteration 8/1000 | Loss: 0.00001170
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001155
Iteration 12/1000 | Loss: 0.00001153
Iteration 13/1000 | Loss: 0.00001153
Iteration 14/1000 | Loss: 0.00001152
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001132
Iteration 22/1000 | Loss: 0.00001129
Iteration 23/1000 | Loss: 0.00001129
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001127
Iteration 29/1000 | Loss: 0.00001127
Iteration 30/1000 | Loss: 0.00001127
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001126
Iteration 34/1000 | Loss: 0.00001126
Iteration 35/1000 | Loss: 0.00001126
Iteration 36/1000 | Loss: 0.00001126
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001125
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001123
Iteration 46/1000 | Loss: 0.00001123
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001123
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [1.1231955795665272e-05, 1.1231955795665272e-05, 1.1231955795665272e-05, 1.1231955795665272e-05, 1.1231955795665272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1231955795665272e-05

Optimization complete. Final v2v error: 2.83504056930542 mm

Highest mean error: 3.428605556488037 mm for frame 60

Lowest mean error: 2.3174827098846436 mm for frame 233

Saving results

Total time: 29.01453709602356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430752
Iteration 2/25 | Loss: 0.00102963
Iteration 3/25 | Loss: 0.00094359
Iteration 4/25 | Loss: 0.00092866
Iteration 5/25 | Loss: 0.00092300
Iteration 6/25 | Loss: 0.00092157
Iteration 7/25 | Loss: 0.00092149
Iteration 8/25 | Loss: 0.00092149
Iteration 9/25 | Loss: 0.00092149
Iteration 10/25 | Loss: 0.00092149
Iteration 11/25 | Loss: 0.00092149
Iteration 12/25 | Loss: 0.00092149
Iteration 13/25 | Loss: 0.00092149
Iteration 14/25 | Loss: 0.00092149
Iteration 15/25 | Loss: 0.00092149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009214921155944467, 0.0009214921155944467, 0.0009214921155944467, 0.0009214921155944467, 0.0009214921155944467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009214921155944467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54276991
Iteration 2/25 | Loss: 0.00063623
Iteration 3/25 | Loss: 0.00063623
Iteration 4/25 | Loss: 0.00063623
Iteration 5/25 | Loss: 0.00063622
Iteration 6/25 | Loss: 0.00063622
Iteration 7/25 | Loss: 0.00063622
Iteration 8/25 | Loss: 0.00063622
Iteration 9/25 | Loss: 0.00063622
Iteration 10/25 | Loss: 0.00063622
Iteration 11/25 | Loss: 0.00063622
Iteration 12/25 | Loss: 0.00063622
Iteration 13/25 | Loss: 0.00063622
Iteration 14/25 | Loss: 0.00063622
Iteration 15/25 | Loss: 0.00063622
Iteration 16/25 | Loss: 0.00063622
Iteration 17/25 | Loss: 0.00063622
Iteration 18/25 | Loss: 0.00063622
Iteration 19/25 | Loss: 0.00063622
Iteration 20/25 | Loss: 0.00063622
Iteration 21/25 | Loss: 0.00063622
Iteration 22/25 | Loss: 0.00063622
Iteration 23/25 | Loss: 0.00063622
Iteration 24/25 | Loss: 0.00063622
Iteration 25/25 | Loss: 0.00063622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063622
Iteration 2/1000 | Loss: 0.00002933
Iteration 3/1000 | Loss: 0.00001613
Iteration 4/1000 | Loss: 0.00001308
Iteration 5/1000 | Loss: 0.00001223
Iteration 6/1000 | Loss: 0.00001162
Iteration 7/1000 | Loss: 0.00001122
Iteration 8/1000 | Loss: 0.00001095
Iteration 9/1000 | Loss: 0.00001075
Iteration 10/1000 | Loss: 0.00001072
Iteration 11/1000 | Loss: 0.00001070
Iteration 12/1000 | Loss: 0.00001068
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001065
Iteration 16/1000 | Loss: 0.00001064
Iteration 17/1000 | Loss: 0.00001064
Iteration 18/1000 | Loss: 0.00001064
Iteration 19/1000 | Loss: 0.00001061
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001058
Iteration 22/1000 | Loss: 0.00001057
Iteration 23/1000 | Loss: 0.00001056
Iteration 24/1000 | Loss: 0.00001056
Iteration 25/1000 | Loss: 0.00001055
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001051
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001050
Iteration 30/1000 | Loss: 0.00001049
Iteration 31/1000 | Loss: 0.00001048
Iteration 32/1000 | Loss: 0.00001047
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001045
Iteration 38/1000 | Loss: 0.00001045
Iteration 39/1000 | Loss: 0.00001045
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001044
Iteration 42/1000 | Loss: 0.00001044
Iteration 43/1000 | Loss: 0.00001043
Iteration 44/1000 | Loss: 0.00001043
Iteration 45/1000 | Loss: 0.00001043
Iteration 46/1000 | Loss: 0.00001042
Iteration 47/1000 | Loss: 0.00001041
Iteration 48/1000 | Loss: 0.00001041
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001040
Iteration 53/1000 | Loss: 0.00001040
Iteration 54/1000 | Loss: 0.00001040
Iteration 55/1000 | Loss: 0.00001040
Iteration 56/1000 | Loss: 0.00001040
Iteration 57/1000 | Loss: 0.00001039
Iteration 58/1000 | Loss: 0.00001039
Iteration 59/1000 | Loss: 0.00001039
Iteration 60/1000 | Loss: 0.00001039
Iteration 61/1000 | Loss: 0.00001039
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001038
Iteration 66/1000 | Loss: 0.00001038
Iteration 67/1000 | Loss: 0.00001038
Iteration 68/1000 | Loss: 0.00001038
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001037
Iteration 73/1000 | Loss: 0.00001037
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001036
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001034
Iteration 90/1000 | Loss: 0.00001034
Iteration 91/1000 | Loss: 0.00001034
Iteration 92/1000 | Loss: 0.00001034
Iteration 93/1000 | Loss: 0.00001034
Iteration 94/1000 | Loss: 0.00001034
Iteration 95/1000 | Loss: 0.00001034
Iteration 96/1000 | Loss: 0.00001034
Iteration 97/1000 | Loss: 0.00001033
Iteration 98/1000 | Loss: 0.00001033
Iteration 99/1000 | Loss: 0.00001033
Iteration 100/1000 | Loss: 0.00001033
Iteration 101/1000 | Loss: 0.00001033
Iteration 102/1000 | Loss: 0.00001033
Iteration 103/1000 | Loss: 0.00001033
Iteration 104/1000 | Loss: 0.00001033
Iteration 105/1000 | Loss: 0.00001033
Iteration 106/1000 | Loss: 0.00001032
Iteration 107/1000 | Loss: 0.00001032
Iteration 108/1000 | Loss: 0.00001032
Iteration 109/1000 | Loss: 0.00001032
Iteration 110/1000 | Loss: 0.00001031
Iteration 111/1000 | Loss: 0.00001031
Iteration 112/1000 | Loss: 0.00001031
Iteration 113/1000 | Loss: 0.00001031
Iteration 114/1000 | Loss: 0.00001031
Iteration 115/1000 | Loss: 0.00001031
Iteration 116/1000 | Loss: 0.00001031
Iteration 117/1000 | Loss: 0.00001030
Iteration 118/1000 | Loss: 0.00001030
Iteration 119/1000 | Loss: 0.00001030
Iteration 120/1000 | Loss: 0.00001030
Iteration 121/1000 | Loss: 0.00001030
Iteration 122/1000 | Loss: 0.00001030
Iteration 123/1000 | Loss: 0.00001030
Iteration 124/1000 | Loss: 0.00001030
Iteration 125/1000 | Loss: 0.00001030
Iteration 126/1000 | Loss: 0.00001030
Iteration 127/1000 | Loss: 0.00001030
Iteration 128/1000 | Loss: 0.00001030
Iteration 129/1000 | Loss: 0.00001030
Iteration 130/1000 | Loss: 0.00001030
Iteration 131/1000 | Loss: 0.00001030
Iteration 132/1000 | Loss: 0.00001030
Iteration 133/1000 | Loss: 0.00001030
Iteration 134/1000 | Loss: 0.00001030
Iteration 135/1000 | Loss: 0.00001030
Iteration 136/1000 | Loss: 0.00001030
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001030
Iteration 140/1000 | Loss: 0.00001030
Iteration 141/1000 | Loss: 0.00001030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.0297096196154598e-05, 1.0297096196154598e-05, 1.0297096196154598e-05, 1.0297096196154598e-05, 1.0297096196154598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0297096196154598e-05

Optimization complete. Final v2v error: 2.726759195327759 mm

Highest mean error: 3.174912214279175 mm for frame 47

Lowest mean error: 2.2948086261749268 mm for frame 103

Saving results

Total time: 32.442238569259644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521903
Iteration 2/25 | Loss: 0.00108977
Iteration 3/25 | Loss: 0.00098799
Iteration 4/25 | Loss: 0.00097116
Iteration 5/25 | Loss: 0.00096760
Iteration 6/25 | Loss: 0.00096733
Iteration 7/25 | Loss: 0.00096733
Iteration 8/25 | Loss: 0.00096733
Iteration 9/25 | Loss: 0.00096733
Iteration 10/25 | Loss: 0.00096733
Iteration 11/25 | Loss: 0.00096733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009673288441263139, 0.0009673288441263139, 0.0009673288441263139, 0.0009673288441263139, 0.0009673288441263139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009673288441263139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75503159
Iteration 2/25 | Loss: 0.00057110
Iteration 3/25 | Loss: 0.00057109
Iteration 4/25 | Loss: 0.00057109
Iteration 5/25 | Loss: 0.00057109
Iteration 6/25 | Loss: 0.00057109
Iteration 7/25 | Loss: 0.00057109
Iteration 8/25 | Loss: 0.00057109
Iteration 9/25 | Loss: 0.00057109
Iteration 10/25 | Loss: 0.00057109
Iteration 11/25 | Loss: 0.00057109
Iteration 12/25 | Loss: 0.00057109
Iteration 13/25 | Loss: 0.00057109
Iteration 14/25 | Loss: 0.00057109
Iteration 15/25 | Loss: 0.00057109
Iteration 16/25 | Loss: 0.00057109
Iteration 17/25 | Loss: 0.00057109
Iteration 18/25 | Loss: 0.00057109
Iteration 19/25 | Loss: 0.00057109
Iteration 20/25 | Loss: 0.00057109
Iteration 21/25 | Loss: 0.00057109
Iteration 22/25 | Loss: 0.00057109
Iteration 23/25 | Loss: 0.00057109
Iteration 24/25 | Loss: 0.00057109
Iteration 25/25 | Loss: 0.00057109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057109
Iteration 2/1000 | Loss: 0.00002424
Iteration 3/1000 | Loss: 0.00001855
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001503
Iteration 8/1000 | Loss: 0.00001485
Iteration 9/1000 | Loss: 0.00001472
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001444
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001438
Iteration 21/1000 | Loss: 0.00001437
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001436
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001434
Iteration 30/1000 | Loss: 0.00001434
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001433
Iteration 34/1000 | Loss: 0.00001432
Iteration 35/1000 | Loss: 0.00001432
Iteration 36/1000 | Loss: 0.00001432
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001430
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001430
Iteration 42/1000 | Loss: 0.00001429
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001429
Iteration 45/1000 | Loss: 0.00001428
Iteration 46/1000 | Loss: 0.00001428
Iteration 47/1000 | Loss: 0.00001428
Iteration 48/1000 | Loss: 0.00001428
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001427
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001427
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001425
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001424
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001423
Iteration 79/1000 | Loss: 0.00001423
Iteration 80/1000 | Loss: 0.00001423
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001423
Iteration 86/1000 | Loss: 0.00001423
Iteration 87/1000 | Loss: 0.00001423
Iteration 88/1000 | Loss: 0.00001423
Iteration 89/1000 | Loss: 0.00001423
Iteration 90/1000 | Loss: 0.00001423
Iteration 91/1000 | Loss: 0.00001423
Iteration 92/1000 | Loss: 0.00001423
Iteration 93/1000 | Loss: 0.00001423
Iteration 94/1000 | Loss: 0.00001423
Iteration 95/1000 | Loss: 0.00001423
Iteration 96/1000 | Loss: 0.00001423
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001423
Iteration 99/1000 | Loss: 0.00001423
Iteration 100/1000 | Loss: 0.00001423
Iteration 101/1000 | Loss: 0.00001423
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001423
Iteration 104/1000 | Loss: 0.00001423
Iteration 105/1000 | Loss: 0.00001423
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Iteration 116/1000 | Loss: 0.00001423
Iteration 117/1000 | Loss: 0.00001423
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00001423
Iteration 124/1000 | Loss: 0.00001423
Iteration 125/1000 | Loss: 0.00001423
Iteration 126/1000 | Loss: 0.00001423
Iteration 127/1000 | Loss: 0.00001423
Iteration 128/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.4230443412088789e-05, 1.4230443412088789e-05, 1.4230443412088789e-05, 1.4230443412088789e-05, 1.4230443412088789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4230443412088789e-05

Optimization complete. Final v2v error: 3.153477430343628 mm

Highest mean error: 3.644685745239258 mm for frame 208

Lowest mean error: 2.7613611221313477 mm for frame 140

Saving results

Total time: 32.33096671104431
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833030
Iteration 2/25 | Loss: 0.00173924
Iteration 3/25 | Loss: 0.00112985
Iteration 4/25 | Loss: 0.00104533
Iteration 5/25 | Loss: 0.00104180
Iteration 6/25 | Loss: 0.00104135
Iteration 7/25 | Loss: 0.00104135
Iteration 8/25 | Loss: 0.00104135
Iteration 9/25 | Loss: 0.00104135
Iteration 10/25 | Loss: 0.00104135
Iteration 11/25 | Loss: 0.00104135
Iteration 12/25 | Loss: 0.00104135
Iteration 13/25 | Loss: 0.00104135
Iteration 14/25 | Loss: 0.00104135
Iteration 15/25 | Loss: 0.00104135
Iteration 16/25 | Loss: 0.00104135
Iteration 17/25 | Loss: 0.00104135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001041349838487804, 0.001041349838487804, 0.001041349838487804, 0.001041349838487804, 0.001041349838487804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001041349838487804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34034240
Iteration 2/25 | Loss: 0.00047498
Iteration 3/25 | Loss: 0.00047497
Iteration 4/25 | Loss: 0.00047497
Iteration 5/25 | Loss: 0.00047497
Iteration 6/25 | Loss: 0.00047497
Iteration 7/25 | Loss: 0.00047497
Iteration 8/25 | Loss: 0.00047497
Iteration 9/25 | Loss: 0.00047497
Iteration 10/25 | Loss: 0.00047497
Iteration 11/25 | Loss: 0.00047497
Iteration 12/25 | Loss: 0.00047497
Iteration 13/25 | Loss: 0.00047497
Iteration 14/25 | Loss: 0.00047497
Iteration 15/25 | Loss: 0.00047497
Iteration 16/25 | Loss: 0.00047497
Iteration 17/25 | Loss: 0.00047497
Iteration 18/25 | Loss: 0.00047497
Iteration 19/25 | Loss: 0.00047497
Iteration 20/25 | Loss: 0.00047497
Iteration 21/25 | Loss: 0.00047497
Iteration 22/25 | Loss: 0.00047497
Iteration 23/25 | Loss: 0.00047497
Iteration 24/25 | Loss: 0.00047497
Iteration 25/25 | Loss: 0.00047497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047497
Iteration 2/1000 | Loss: 0.00002761
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001613
Iteration 5/1000 | Loss: 0.00001534
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001452
Iteration 9/1000 | Loss: 0.00001430
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001396
Iteration 16/1000 | Loss: 0.00001396
Iteration 17/1000 | Loss: 0.00001396
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001392
Iteration 30/1000 | Loss: 0.00001392
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001387
Iteration 36/1000 | Loss: 0.00001387
Iteration 37/1000 | Loss: 0.00001387
Iteration 38/1000 | Loss: 0.00001387
Iteration 39/1000 | Loss: 0.00001386
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001385
Iteration 43/1000 | Loss: 0.00001385
Iteration 44/1000 | Loss: 0.00001384
Iteration 45/1000 | Loss: 0.00001384
Iteration 46/1000 | Loss: 0.00001382
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001382
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001381
Iteration 54/1000 | Loss: 0.00001380
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001378
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001377
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001375
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001374
Iteration 86/1000 | Loss: 0.00001374
Iteration 87/1000 | Loss: 0.00001374
Iteration 88/1000 | Loss: 0.00001374
Iteration 89/1000 | Loss: 0.00001374
Iteration 90/1000 | Loss: 0.00001374
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001374
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001373
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001372
Iteration 104/1000 | Loss: 0.00001372
Iteration 105/1000 | Loss: 0.00001372
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001371
Iteration 110/1000 | Loss: 0.00001371
Iteration 111/1000 | Loss: 0.00001371
Iteration 112/1000 | Loss: 0.00001371
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001370
Iteration 115/1000 | Loss: 0.00001370
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001369
Iteration 120/1000 | Loss: 0.00001369
Iteration 121/1000 | Loss: 0.00001369
Iteration 122/1000 | Loss: 0.00001369
Iteration 123/1000 | Loss: 0.00001369
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001369
Iteration 134/1000 | Loss: 0.00001369
Iteration 135/1000 | Loss: 0.00001369
Iteration 136/1000 | Loss: 0.00001369
Iteration 137/1000 | Loss: 0.00001369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3686565580428578e-05, 1.3686565580428578e-05, 1.3686565580428578e-05, 1.3686565580428578e-05, 1.3686565580428578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3686565580428578e-05

Optimization complete. Final v2v error: 3.0504345893859863 mm

Highest mean error: 3.4977777004241943 mm for frame 136

Lowest mean error: 2.711782693862915 mm for frame 107

Saving results

Total time: 32.04779505729675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064399
Iteration 2/25 | Loss: 0.00176323
Iteration 3/25 | Loss: 0.00129522
Iteration 4/25 | Loss: 0.00118852
Iteration 5/25 | Loss: 0.00111197
Iteration 6/25 | Loss: 0.00108711
Iteration 7/25 | Loss: 0.00109998
Iteration 8/25 | Loss: 0.00106244
Iteration 9/25 | Loss: 0.00105017
Iteration 10/25 | Loss: 0.00104273
Iteration 11/25 | Loss: 0.00107828
Iteration 12/25 | Loss: 0.00108104
Iteration 13/25 | Loss: 0.00103487
Iteration 14/25 | Loss: 0.00101852
Iteration 15/25 | Loss: 0.00101293
Iteration 16/25 | Loss: 0.00100623
Iteration 17/25 | Loss: 0.00100504
Iteration 18/25 | Loss: 0.00100454
Iteration 19/25 | Loss: 0.00103198
Iteration 20/25 | Loss: 0.00099797
Iteration 21/25 | Loss: 0.00099415
Iteration 22/25 | Loss: 0.00099384
Iteration 23/25 | Loss: 0.00099357
Iteration 24/25 | Loss: 0.00099310
Iteration 25/25 | Loss: 0.00099256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47971165
Iteration 2/25 | Loss: 0.00192398
Iteration 3/25 | Loss: 0.00079154
Iteration 4/25 | Loss: 0.00079154
Iteration 5/25 | Loss: 0.00079154
Iteration 6/25 | Loss: 0.00079154
Iteration 7/25 | Loss: 0.00079154
Iteration 8/25 | Loss: 0.00079154
Iteration 9/25 | Loss: 0.00079154
Iteration 10/25 | Loss: 0.00079154
Iteration 11/25 | Loss: 0.00079154
Iteration 12/25 | Loss: 0.00079154
Iteration 13/25 | Loss: 0.00079154
Iteration 14/25 | Loss: 0.00079154
Iteration 15/25 | Loss: 0.00079154
Iteration 16/25 | Loss: 0.00079154
Iteration 17/25 | Loss: 0.00079154
Iteration 18/25 | Loss: 0.00079154
Iteration 19/25 | Loss: 0.00079154
Iteration 20/25 | Loss: 0.00079154
Iteration 21/25 | Loss: 0.00079154
Iteration 22/25 | Loss: 0.00079154
Iteration 23/25 | Loss: 0.00079154
Iteration 24/25 | Loss: 0.00079154
Iteration 25/25 | Loss: 0.00079154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079154
Iteration 2/1000 | Loss: 0.00115349
Iteration 3/1000 | Loss: 0.00012655
Iteration 4/1000 | Loss: 0.00007945
Iteration 5/1000 | Loss: 0.00093293
Iteration 6/1000 | Loss: 0.00004853
Iteration 7/1000 | Loss: 0.00004350
Iteration 8/1000 | Loss: 0.00058004
Iteration 9/1000 | Loss: 0.00050190
Iteration 10/1000 | Loss: 0.00080635
Iteration 11/1000 | Loss: 0.00042991
Iteration 12/1000 | Loss: 0.00031679
Iteration 13/1000 | Loss: 0.00027320
Iteration 14/1000 | Loss: 0.00053421
Iteration 15/1000 | Loss: 0.00114742
Iteration 16/1000 | Loss: 0.00081408
Iteration 17/1000 | Loss: 0.00278619
Iteration 18/1000 | Loss: 0.00004760
Iteration 19/1000 | Loss: 0.00084161
Iteration 20/1000 | Loss: 0.00005477
Iteration 21/1000 | Loss: 0.00003825
Iteration 22/1000 | Loss: 0.00003430
Iteration 23/1000 | Loss: 0.00003288
Iteration 24/1000 | Loss: 0.00041432
Iteration 25/1000 | Loss: 0.00025412
Iteration 26/1000 | Loss: 0.00038980
Iteration 27/1000 | Loss: 0.00003926
Iteration 28/1000 | Loss: 0.00002777
Iteration 29/1000 | Loss: 0.00002620
Iteration 30/1000 | Loss: 0.00002517
Iteration 31/1000 | Loss: 0.00002445
Iteration 32/1000 | Loss: 0.00002414
Iteration 33/1000 | Loss: 0.00002386
Iteration 34/1000 | Loss: 0.00002357
Iteration 35/1000 | Loss: 0.00002348
Iteration 36/1000 | Loss: 0.00002347
Iteration 37/1000 | Loss: 0.00002347
Iteration 38/1000 | Loss: 0.00002346
Iteration 39/1000 | Loss: 0.00002345
Iteration 40/1000 | Loss: 0.00002340
Iteration 41/1000 | Loss: 0.00002339
Iteration 42/1000 | Loss: 0.00002337
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002326
Iteration 45/1000 | Loss: 0.00002321
Iteration 46/1000 | Loss: 0.00002321
Iteration 47/1000 | Loss: 0.00002320
Iteration 48/1000 | Loss: 0.00002314
Iteration 49/1000 | Loss: 0.00002309
Iteration 50/1000 | Loss: 0.00002306
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002306
Iteration 53/1000 | Loss: 0.00002306
Iteration 54/1000 | Loss: 0.00002306
Iteration 55/1000 | Loss: 0.00002306
Iteration 56/1000 | Loss: 0.00002306
Iteration 57/1000 | Loss: 0.00002306
Iteration 58/1000 | Loss: 0.00002305
Iteration 59/1000 | Loss: 0.00002305
Iteration 60/1000 | Loss: 0.00002305
Iteration 61/1000 | Loss: 0.00002305
Iteration 62/1000 | Loss: 0.00002305
Iteration 63/1000 | Loss: 0.00002305
Iteration 64/1000 | Loss: 0.00002305
Iteration 65/1000 | Loss: 0.00002305
Iteration 66/1000 | Loss: 0.00002304
Iteration 67/1000 | Loss: 0.00002303
Iteration 68/1000 | Loss: 0.00002303
Iteration 69/1000 | Loss: 0.00002303
Iteration 70/1000 | Loss: 0.00002302
Iteration 71/1000 | Loss: 0.00002302
Iteration 72/1000 | Loss: 0.00002302
Iteration 73/1000 | Loss: 0.00002302
Iteration 74/1000 | Loss: 0.00002302
Iteration 75/1000 | Loss: 0.00002302
Iteration 76/1000 | Loss: 0.00002301
Iteration 77/1000 | Loss: 0.00002301
Iteration 78/1000 | Loss: 0.00002301
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002301
Iteration 81/1000 | Loss: 0.00002301
Iteration 82/1000 | Loss: 0.00002301
Iteration 83/1000 | Loss: 0.00002300
Iteration 84/1000 | Loss: 0.00002300
Iteration 85/1000 | Loss: 0.00002300
Iteration 86/1000 | Loss: 0.00002300
Iteration 87/1000 | Loss: 0.00002300
Iteration 88/1000 | Loss: 0.00002300
Iteration 89/1000 | Loss: 0.00002300
Iteration 90/1000 | Loss: 0.00002300
Iteration 91/1000 | Loss: 0.00002300
Iteration 92/1000 | Loss: 0.00002300
Iteration 93/1000 | Loss: 0.00002300
Iteration 94/1000 | Loss: 0.00002300
Iteration 95/1000 | Loss: 0.00002300
Iteration 96/1000 | Loss: 0.00002300
Iteration 97/1000 | Loss: 0.00002300
Iteration 98/1000 | Loss: 0.00002300
Iteration 99/1000 | Loss: 0.00002300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.2999667635303922e-05, 2.2999667635303922e-05, 2.2999667635303922e-05, 2.2999667635303922e-05, 2.2999667635303922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2999667635303922e-05

Optimization complete. Final v2v error: 3.0192720890045166 mm

Highest mean error: 21.62650489807129 mm for frame 85

Lowest mean error: 2.1734039783477783 mm for frame 98

Saving results

Total time: 98.97753119468689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387335
Iteration 2/25 | Loss: 0.00109619
Iteration 3/25 | Loss: 0.00094271
Iteration 4/25 | Loss: 0.00093118
Iteration 5/25 | Loss: 0.00092897
Iteration 6/25 | Loss: 0.00092839
Iteration 7/25 | Loss: 0.00092839
Iteration 8/25 | Loss: 0.00092839
Iteration 9/25 | Loss: 0.00092839
Iteration 10/25 | Loss: 0.00092839
Iteration 11/25 | Loss: 0.00092839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009283885010518134, 0.0009283885010518134, 0.0009283885010518134, 0.0009283885010518134, 0.0009283885010518134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009283885010518134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32028449
Iteration 2/25 | Loss: 0.00062499
Iteration 3/25 | Loss: 0.00062498
Iteration 4/25 | Loss: 0.00062498
Iteration 5/25 | Loss: 0.00062498
Iteration 6/25 | Loss: 0.00062498
Iteration 7/25 | Loss: 0.00062498
Iteration 8/25 | Loss: 0.00062498
Iteration 9/25 | Loss: 0.00062498
Iteration 10/25 | Loss: 0.00062498
Iteration 11/25 | Loss: 0.00062498
Iteration 12/25 | Loss: 0.00062498
Iteration 13/25 | Loss: 0.00062498
Iteration 14/25 | Loss: 0.00062498
Iteration 15/25 | Loss: 0.00062498
Iteration 16/25 | Loss: 0.00062498
Iteration 17/25 | Loss: 0.00062498
Iteration 18/25 | Loss: 0.00062498
Iteration 19/25 | Loss: 0.00062498
Iteration 20/25 | Loss: 0.00062498
Iteration 21/25 | Loss: 0.00062498
Iteration 22/25 | Loss: 0.00062498
Iteration 23/25 | Loss: 0.00062498
Iteration 24/25 | Loss: 0.00062498
Iteration 25/25 | Loss: 0.00062498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062498
Iteration 2/1000 | Loss: 0.00002675
Iteration 3/1000 | Loss: 0.00001274
Iteration 4/1000 | Loss: 0.00001078
Iteration 5/1000 | Loss: 0.00000992
Iteration 6/1000 | Loss: 0.00000930
Iteration 7/1000 | Loss: 0.00000901
Iteration 8/1000 | Loss: 0.00000890
Iteration 9/1000 | Loss: 0.00000878
Iteration 10/1000 | Loss: 0.00000868
Iteration 11/1000 | Loss: 0.00000857
Iteration 12/1000 | Loss: 0.00000856
Iteration 13/1000 | Loss: 0.00000855
Iteration 14/1000 | Loss: 0.00000854
Iteration 15/1000 | Loss: 0.00000854
Iteration 16/1000 | Loss: 0.00000854
Iteration 17/1000 | Loss: 0.00000852
Iteration 18/1000 | Loss: 0.00000852
Iteration 19/1000 | Loss: 0.00000852
Iteration 20/1000 | Loss: 0.00000852
Iteration 21/1000 | Loss: 0.00000847
Iteration 22/1000 | Loss: 0.00000847
Iteration 23/1000 | Loss: 0.00000846
Iteration 24/1000 | Loss: 0.00000846
Iteration 25/1000 | Loss: 0.00000845
Iteration 26/1000 | Loss: 0.00000844
Iteration 27/1000 | Loss: 0.00000844
Iteration 28/1000 | Loss: 0.00000844
Iteration 29/1000 | Loss: 0.00000843
Iteration 30/1000 | Loss: 0.00000843
Iteration 31/1000 | Loss: 0.00000843
Iteration 32/1000 | Loss: 0.00000843
Iteration 33/1000 | Loss: 0.00000842
Iteration 34/1000 | Loss: 0.00000841
Iteration 35/1000 | Loss: 0.00000841
Iteration 36/1000 | Loss: 0.00000841
Iteration 37/1000 | Loss: 0.00000841
Iteration 38/1000 | Loss: 0.00000840
Iteration 39/1000 | Loss: 0.00000839
Iteration 40/1000 | Loss: 0.00000839
Iteration 41/1000 | Loss: 0.00000839
Iteration 42/1000 | Loss: 0.00000839
Iteration 43/1000 | Loss: 0.00000838
Iteration 44/1000 | Loss: 0.00000838
Iteration 45/1000 | Loss: 0.00000838
Iteration 46/1000 | Loss: 0.00000837
Iteration 47/1000 | Loss: 0.00000836
Iteration 48/1000 | Loss: 0.00000835
Iteration 49/1000 | Loss: 0.00000835
Iteration 50/1000 | Loss: 0.00000835
Iteration 51/1000 | Loss: 0.00000835
Iteration 52/1000 | Loss: 0.00000834
Iteration 53/1000 | Loss: 0.00000834
Iteration 54/1000 | Loss: 0.00000833
Iteration 55/1000 | Loss: 0.00000833
Iteration 56/1000 | Loss: 0.00000832
Iteration 57/1000 | Loss: 0.00000831
Iteration 58/1000 | Loss: 0.00000831
Iteration 59/1000 | Loss: 0.00000831
Iteration 60/1000 | Loss: 0.00000831
Iteration 61/1000 | Loss: 0.00000831
Iteration 62/1000 | Loss: 0.00000831
Iteration 63/1000 | Loss: 0.00000830
Iteration 64/1000 | Loss: 0.00000830
Iteration 65/1000 | Loss: 0.00000830
Iteration 66/1000 | Loss: 0.00000829
Iteration 67/1000 | Loss: 0.00000829
Iteration 68/1000 | Loss: 0.00000829
Iteration 69/1000 | Loss: 0.00000828
Iteration 70/1000 | Loss: 0.00000828
Iteration 71/1000 | Loss: 0.00000828
Iteration 72/1000 | Loss: 0.00000828
Iteration 73/1000 | Loss: 0.00000828
Iteration 74/1000 | Loss: 0.00000828
Iteration 75/1000 | Loss: 0.00000828
Iteration 76/1000 | Loss: 0.00000828
Iteration 77/1000 | Loss: 0.00000828
Iteration 78/1000 | Loss: 0.00000827
Iteration 79/1000 | Loss: 0.00000827
Iteration 80/1000 | Loss: 0.00000827
Iteration 81/1000 | Loss: 0.00000826
Iteration 82/1000 | Loss: 0.00000826
Iteration 83/1000 | Loss: 0.00000826
Iteration 84/1000 | Loss: 0.00000825
Iteration 85/1000 | Loss: 0.00000825
Iteration 86/1000 | Loss: 0.00000825
Iteration 87/1000 | Loss: 0.00000825
Iteration 88/1000 | Loss: 0.00000825
Iteration 89/1000 | Loss: 0.00000825
Iteration 90/1000 | Loss: 0.00000824
Iteration 91/1000 | Loss: 0.00000824
Iteration 92/1000 | Loss: 0.00000824
Iteration 93/1000 | Loss: 0.00000824
Iteration 94/1000 | Loss: 0.00000824
Iteration 95/1000 | Loss: 0.00000824
Iteration 96/1000 | Loss: 0.00000824
Iteration 97/1000 | Loss: 0.00000824
Iteration 98/1000 | Loss: 0.00000824
Iteration 99/1000 | Loss: 0.00000824
Iteration 100/1000 | Loss: 0.00000824
Iteration 101/1000 | Loss: 0.00000824
Iteration 102/1000 | Loss: 0.00000824
Iteration 103/1000 | Loss: 0.00000824
Iteration 104/1000 | Loss: 0.00000824
Iteration 105/1000 | Loss: 0.00000824
Iteration 106/1000 | Loss: 0.00000824
Iteration 107/1000 | Loss: 0.00000824
Iteration 108/1000 | Loss: 0.00000824
Iteration 109/1000 | Loss: 0.00000824
Iteration 110/1000 | Loss: 0.00000824
Iteration 111/1000 | Loss: 0.00000823
Iteration 112/1000 | Loss: 0.00000823
Iteration 113/1000 | Loss: 0.00000823
Iteration 114/1000 | Loss: 0.00000823
Iteration 115/1000 | Loss: 0.00000823
Iteration 116/1000 | Loss: 0.00000823
Iteration 117/1000 | Loss: 0.00000823
Iteration 118/1000 | Loss: 0.00000823
Iteration 119/1000 | Loss: 0.00000823
Iteration 120/1000 | Loss: 0.00000823
Iteration 121/1000 | Loss: 0.00000822
Iteration 122/1000 | Loss: 0.00000822
Iteration 123/1000 | Loss: 0.00000822
Iteration 124/1000 | Loss: 0.00000822
Iteration 125/1000 | Loss: 0.00000822
Iteration 126/1000 | Loss: 0.00000822
Iteration 127/1000 | Loss: 0.00000822
Iteration 128/1000 | Loss: 0.00000822
Iteration 129/1000 | Loss: 0.00000822
Iteration 130/1000 | Loss: 0.00000822
Iteration 131/1000 | Loss: 0.00000821
Iteration 132/1000 | Loss: 0.00000821
Iteration 133/1000 | Loss: 0.00000821
Iteration 134/1000 | Loss: 0.00000821
Iteration 135/1000 | Loss: 0.00000821
Iteration 136/1000 | Loss: 0.00000821
Iteration 137/1000 | Loss: 0.00000821
Iteration 138/1000 | Loss: 0.00000821
Iteration 139/1000 | Loss: 0.00000821
Iteration 140/1000 | Loss: 0.00000821
Iteration 141/1000 | Loss: 0.00000820
Iteration 142/1000 | Loss: 0.00000820
Iteration 143/1000 | Loss: 0.00000820
Iteration 144/1000 | Loss: 0.00000820
Iteration 145/1000 | Loss: 0.00000820
Iteration 146/1000 | Loss: 0.00000820
Iteration 147/1000 | Loss: 0.00000820
Iteration 148/1000 | Loss: 0.00000819
Iteration 149/1000 | Loss: 0.00000819
Iteration 150/1000 | Loss: 0.00000819
Iteration 151/1000 | Loss: 0.00000819
Iteration 152/1000 | Loss: 0.00000819
Iteration 153/1000 | Loss: 0.00000819
Iteration 154/1000 | Loss: 0.00000819
Iteration 155/1000 | Loss: 0.00000819
Iteration 156/1000 | Loss: 0.00000819
Iteration 157/1000 | Loss: 0.00000819
Iteration 158/1000 | Loss: 0.00000819
Iteration 159/1000 | Loss: 0.00000819
Iteration 160/1000 | Loss: 0.00000818
Iteration 161/1000 | Loss: 0.00000818
Iteration 162/1000 | Loss: 0.00000818
Iteration 163/1000 | Loss: 0.00000818
Iteration 164/1000 | Loss: 0.00000818
Iteration 165/1000 | Loss: 0.00000818
Iteration 166/1000 | Loss: 0.00000818
Iteration 167/1000 | Loss: 0.00000818
Iteration 168/1000 | Loss: 0.00000818
Iteration 169/1000 | Loss: 0.00000818
Iteration 170/1000 | Loss: 0.00000818
Iteration 171/1000 | Loss: 0.00000818
Iteration 172/1000 | Loss: 0.00000818
Iteration 173/1000 | Loss: 0.00000818
Iteration 174/1000 | Loss: 0.00000818
Iteration 175/1000 | Loss: 0.00000818
Iteration 176/1000 | Loss: 0.00000818
Iteration 177/1000 | Loss: 0.00000818
Iteration 178/1000 | Loss: 0.00000817
Iteration 179/1000 | Loss: 0.00000817
Iteration 180/1000 | Loss: 0.00000817
Iteration 181/1000 | Loss: 0.00000817
Iteration 182/1000 | Loss: 0.00000817
Iteration 183/1000 | Loss: 0.00000817
Iteration 184/1000 | Loss: 0.00000817
Iteration 185/1000 | Loss: 0.00000817
Iteration 186/1000 | Loss: 0.00000817
Iteration 187/1000 | Loss: 0.00000817
Iteration 188/1000 | Loss: 0.00000817
Iteration 189/1000 | Loss: 0.00000817
Iteration 190/1000 | Loss: 0.00000817
Iteration 191/1000 | Loss: 0.00000817
Iteration 192/1000 | Loss: 0.00000817
Iteration 193/1000 | Loss: 0.00000817
Iteration 194/1000 | Loss: 0.00000817
Iteration 195/1000 | Loss: 0.00000817
Iteration 196/1000 | Loss: 0.00000817
Iteration 197/1000 | Loss: 0.00000816
Iteration 198/1000 | Loss: 0.00000816
Iteration 199/1000 | Loss: 0.00000816
Iteration 200/1000 | Loss: 0.00000816
Iteration 201/1000 | Loss: 0.00000816
Iteration 202/1000 | Loss: 0.00000816
Iteration 203/1000 | Loss: 0.00000816
Iteration 204/1000 | Loss: 0.00000816
Iteration 205/1000 | Loss: 0.00000816
Iteration 206/1000 | Loss: 0.00000816
Iteration 207/1000 | Loss: 0.00000816
Iteration 208/1000 | Loss: 0.00000816
Iteration 209/1000 | Loss: 0.00000816
Iteration 210/1000 | Loss: 0.00000815
Iteration 211/1000 | Loss: 0.00000815
Iteration 212/1000 | Loss: 0.00000815
Iteration 213/1000 | Loss: 0.00000815
Iteration 214/1000 | Loss: 0.00000815
Iteration 215/1000 | Loss: 0.00000815
Iteration 216/1000 | Loss: 0.00000815
Iteration 217/1000 | Loss: 0.00000815
Iteration 218/1000 | Loss: 0.00000815
Iteration 219/1000 | Loss: 0.00000815
Iteration 220/1000 | Loss: 0.00000815
Iteration 221/1000 | Loss: 0.00000815
Iteration 222/1000 | Loss: 0.00000815
Iteration 223/1000 | Loss: 0.00000815
Iteration 224/1000 | Loss: 0.00000814
Iteration 225/1000 | Loss: 0.00000814
Iteration 226/1000 | Loss: 0.00000814
Iteration 227/1000 | Loss: 0.00000814
Iteration 228/1000 | Loss: 0.00000814
Iteration 229/1000 | Loss: 0.00000814
Iteration 230/1000 | Loss: 0.00000814
Iteration 231/1000 | Loss: 0.00000814
Iteration 232/1000 | Loss: 0.00000814
Iteration 233/1000 | Loss: 0.00000814
Iteration 234/1000 | Loss: 0.00000814
Iteration 235/1000 | Loss: 0.00000814
Iteration 236/1000 | Loss: 0.00000814
Iteration 237/1000 | Loss: 0.00000814
Iteration 238/1000 | Loss: 0.00000814
Iteration 239/1000 | Loss: 0.00000814
Iteration 240/1000 | Loss: 0.00000814
Iteration 241/1000 | Loss: 0.00000814
Iteration 242/1000 | Loss: 0.00000813
Iteration 243/1000 | Loss: 0.00000813
Iteration 244/1000 | Loss: 0.00000813
Iteration 245/1000 | Loss: 0.00000813
Iteration 246/1000 | Loss: 0.00000813
Iteration 247/1000 | Loss: 0.00000813
Iteration 248/1000 | Loss: 0.00000813
Iteration 249/1000 | Loss: 0.00000813
Iteration 250/1000 | Loss: 0.00000813
Iteration 251/1000 | Loss: 0.00000813
Iteration 252/1000 | Loss: 0.00000813
Iteration 253/1000 | Loss: 0.00000813
Iteration 254/1000 | Loss: 0.00000812
Iteration 255/1000 | Loss: 0.00000812
Iteration 256/1000 | Loss: 0.00000812
Iteration 257/1000 | Loss: 0.00000812
Iteration 258/1000 | Loss: 0.00000812
Iteration 259/1000 | Loss: 0.00000812
Iteration 260/1000 | Loss: 0.00000812
Iteration 261/1000 | Loss: 0.00000812
Iteration 262/1000 | Loss: 0.00000812
Iteration 263/1000 | Loss: 0.00000812
Iteration 264/1000 | Loss: 0.00000811
Iteration 265/1000 | Loss: 0.00000811
Iteration 266/1000 | Loss: 0.00000811
Iteration 267/1000 | Loss: 0.00000811
Iteration 268/1000 | Loss: 0.00000811
Iteration 269/1000 | Loss: 0.00000811
Iteration 270/1000 | Loss: 0.00000811
Iteration 271/1000 | Loss: 0.00000811
Iteration 272/1000 | Loss: 0.00000811
Iteration 273/1000 | Loss: 0.00000811
Iteration 274/1000 | Loss: 0.00000811
Iteration 275/1000 | Loss: 0.00000811
Iteration 276/1000 | Loss: 0.00000811
Iteration 277/1000 | Loss: 0.00000811
Iteration 278/1000 | Loss: 0.00000811
Iteration 279/1000 | Loss: 0.00000810
Iteration 280/1000 | Loss: 0.00000810
Iteration 281/1000 | Loss: 0.00000810
Iteration 282/1000 | Loss: 0.00000810
Iteration 283/1000 | Loss: 0.00000810
Iteration 284/1000 | Loss: 0.00000810
Iteration 285/1000 | Loss: 0.00000810
Iteration 286/1000 | Loss: 0.00000810
Iteration 287/1000 | Loss: 0.00000810
Iteration 288/1000 | Loss: 0.00000810
Iteration 289/1000 | Loss: 0.00000810
Iteration 290/1000 | Loss: 0.00000810
Iteration 291/1000 | Loss: 0.00000810
Iteration 292/1000 | Loss: 0.00000810
Iteration 293/1000 | Loss: 0.00000810
Iteration 294/1000 | Loss: 0.00000810
Iteration 295/1000 | Loss: 0.00000810
Iteration 296/1000 | Loss: 0.00000810
Iteration 297/1000 | Loss: 0.00000810
Iteration 298/1000 | Loss: 0.00000810
Iteration 299/1000 | Loss: 0.00000810
Iteration 300/1000 | Loss: 0.00000810
Iteration 301/1000 | Loss: 0.00000810
Iteration 302/1000 | Loss: 0.00000810
Iteration 303/1000 | Loss: 0.00000810
Iteration 304/1000 | Loss: 0.00000810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 304. Stopping optimization.
Last 5 losses: [8.095621524262242e-06, 8.095621524262242e-06, 8.095621524262242e-06, 8.095621524262242e-06, 8.095621524262242e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.095621524262242e-06

Optimization complete. Final v2v error: 2.42364764213562 mm

Highest mean error: 2.7625138759613037 mm for frame 41

Lowest mean error: 2.2549490928649902 mm for frame 3

Saving results

Total time: 39.49989867210388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484133
Iteration 2/25 | Loss: 0.00113960
Iteration 3/25 | Loss: 0.00101347
Iteration 4/25 | Loss: 0.00100676
Iteration 5/25 | Loss: 0.00100479
Iteration 6/25 | Loss: 0.00100479
Iteration 7/25 | Loss: 0.00100479
Iteration 8/25 | Loss: 0.00100479
Iteration 9/25 | Loss: 0.00100479
Iteration 10/25 | Loss: 0.00100479
Iteration 11/25 | Loss: 0.00100479
Iteration 12/25 | Loss: 0.00100479
Iteration 13/25 | Loss: 0.00100479
Iteration 14/25 | Loss: 0.00100479
Iteration 15/25 | Loss: 0.00100479
Iteration 16/25 | Loss: 0.00100479
Iteration 17/25 | Loss: 0.00100479
Iteration 18/25 | Loss: 0.00100479
Iteration 19/25 | Loss: 0.00100479
Iteration 20/25 | Loss: 0.00100479
Iteration 21/25 | Loss: 0.00100479
Iteration 22/25 | Loss: 0.00100479
Iteration 23/25 | Loss: 0.00100479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010047921678051353, 0.0010047921678051353, 0.0010047921678051353, 0.0010047921678051353, 0.0010047921678051353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010047921678051353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32338846
Iteration 2/25 | Loss: 0.00064141
Iteration 3/25 | Loss: 0.00064139
Iteration 4/25 | Loss: 0.00064139
Iteration 5/25 | Loss: 0.00064139
Iteration 6/25 | Loss: 0.00064139
Iteration 7/25 | Loss: 0.00064139
Iteration 8/25 | Loss: 0.00064139
Iteration 9/25 | Loss: 0.00064139
Iteration 10/25 | Loss: 0.00064139
Iteration 11/25 | Loss: 0.00064139
Iteration 12/25 | Loss: 0.00064139
Iteration 13/25 | Loss: 0.00064139
Iteration 14/25 | Loss: 0.00064139
Iteration 15/25 | Loss: 0.00064139
Iteration 16/25 | Loss: 0.00064139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006413906230591238, 0.0006413906230591238, 0.0006413906230591238, 0.0006413906230591238, 0.0006413906230591238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006413906230591238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064139
Iteration 2/1000 | Loss: 0.00003253
Iteration 3/1000 | Loss: 0.00001870
Iteration 4/1000 | Loss: 0.00001586
Iteration 5/1000 | Loss: 0.00001474
Iteration 6/1000 | Loss: 0.00001427
Iteration 7/1000 | Loss: 0.00001386
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001324
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001312
Iteration 15/1000 | Loss: 0.00001308
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001300
Iteration 18/1000 | Loss: 0.00001299
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001298
Iteration 21/1000 | Loss: 0.00001296
Iteration 22/1000 | Loss: 0.00001293
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001292
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001287
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001286
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001283
Iteration 36/1000 | Loss: 0.00001283
Iteration 37/1000 | Loss: 0.00001283
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001282
Iteration 40/1000 | Loss: 0.00001282
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00001281
Iteration 45/1000 | Loss: 0.00001281
Iteration 46/1000 | Loss: 0.00001280
Iteration 47/1000 | Loss: 0.00001280
Iteration 48/1000 | Loss: 0.00001279
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001278
Iteration 52/1000 | Loss: 0.00001278
Iteration 53/1000 | Loss: 0.00001278
Iteration 54/1000 | Loss: 0.00001278
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001276
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00001276
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001274
Iteration 71/1000 | Loss: 0.00001273
Iteration 72/1000 | Loss: 0.00001273
Iteration 73/1000 | Loss: 0.00001273
Iteration 74/1000 | Loss: 0.00001273
Iteration 75/1000 | Loss: 0.00001273
Iteration 76/1000 | Loss: 0.00001273
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001272
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001271
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001269
Iteration 85/1000 | Loss: 0.00001269
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001268
Iteration 88/1000 | Loss: 0.00001268
Iteration 89/1000 | Loss: 0.00001268
Iteration 90/1000 | Loss: 0.00001268
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001268
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001266
Iteration 102/1000 | Loss: 0.00001266
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001265
Iteration 105/1000 | Loss: 0.00001265
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001261
Iteration 116/1000 | Loss: 0.00001261
Iteration 117/1000 | Loss: 0.00001261
Iteration 118/1000 | Loss: 0.00001260
Iteration 119/1000 | Loss: 0.00001260
Iteration 120/1000 | Loss: 0.00001260
Iteration 121/1000 | Loss: 0.00001260
Iteration 122/1000 | Loss: 0.00001260
Iteration 123/1000 | Loss: 0.00001260
Iteration 124/1000 | Loss: 0.00001260
Iteration 125/1000 | Loss: 0.00001260
Iteration 126/1000 | Loss: 0.00001260
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001259
Iteration 129/1000 | Loss: 0.00001259
Iteration 130/1000 | Loss: 0.00001259
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001259
Iteration 133/1000 | Loss: 0.00001259
Iteration 134/1000 | Loss: 0.00001258
Iteration 135/1000 | Loss: 0.00001258
Iteration 136/1000 | Loss: 0.00001258
Iteration 137/1000 | Loss: 0.00001258
Iteration 138/1000 | Loss: 0.00001258
Iteration 139/1000 | Loss: 0.00001258
Iteration 140/1000 | Loss: 0.00001257
Iteration 141/1000 | Loss: 0.00001257
Iteration 142/1000 | Loss: 0.00001257
Iteration 143/1000 | Loss: 0.00001257
Iteration 144/1000 | Loss: 0.00001256
Iteration 145/1000 | Loss: 0.00001256
Iteration 146/1000 | Loss: 0.00001256
Iteration 147/1000 | Loss: 0.00001256
Iteration 148/1000 | Loss: 0.00001255
Iteration 149/1000 | Loss: 0.00001255
Iteration 150/1000 | Loss: 0.00001255
Iteration 151/1000 | Loss: 0.00001255
Iteration 152/1000 | Loss: 0.00001255
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001255
Iteration 155/1000 | Loss: 0.00001255
Iteration 156/1000 | Loss: 0.00001255
Iteration 157/1000 | Loss: 0.00001255
Iteration 158/1000 | Loss: 0.00001255
Iteration 159/1000 | Loss: 0.00001255
Iteration 160/1000 | Loss: 0.00001255
Iteration 161/1000 | Loss: 0.00001255
Iteration 162/1000 | Loss: 0.00001254
Iteration 163/1000 | Loss: 0.00001254
Iteration 164/1000 | Loss: 0.00001254
Iteration 165/1000 | Loss: 0.00001254
Iteration 166/1000 | Loss: 0.00001254
Iteration 167/1000 | Loss: 0.00001254
Iteration 168/1000 | Loss: 0.00001254
Iteration 169/1000 | Loss: 0.00001254
Iteration 170/1000 | Loss: 0.00001254
Iteration 171/1000 | Loss: 0.00001254
Iteration 172/1000 | Loss: 0.00001254
Iteration 173/1000 | Loss: 0.00001254
Iteration 174/1000 | Loss: 0.00001254
Iteration 175/1000 | Loss: 0.00001254
Iteration 176/1000 | Loss: 0.00001254
Iteration 177/1000 | Loss: 0.00001254
Iteration 178/1000 | Loss: 0.00001254
Iteration 179/1000 | Loss: 0.00001253
Iteration 180/1000 | Loss: 0.00001253
Iteration 181/1000 | Loss: 0.00001253
Iteration 182/1000 | Loss: 0.00001253
Iteration 183/1000 | Loss: 0.00001253
Iteration 184/1000 | Loss: 0.00001253
Iteration 185/1000 | Loss: 0.00001253
Iteration 186/1000 | Loss: 0.00001253
Iteration 187/1000 | Loss: 0.00001253
Iteration 188/1000 | Loss: 0.00001253
Iteration 189/1000 | Loss: 0.00001253
Iteration 190/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.2532212167570833e-05, 1.2532212167570833e-05, 1.2532212167570833e-05, 1.2532212167570833e-05, 1.2532212167570833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2532212167570833e-05

Optimization complete. Final v2v error: 2.9813671112060547 mm

Highest mean error: 3.3932507038116455 mm for frame 72

Lowest mean error: 2.601471424102783 mm for frame 210

Saving results

Total time: 41.99484634399414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_009/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_009/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107178
Iteration 2/25 | Loss: 0.01107178
Iteration 3/25 | Loss: 0.01107178
Iteration 4/25 | Loss: 0.00285734
Iteration 5/25 | Loss: 0.00204046
Iteration 6/25 | Loss: 0.00180068
Iteration 7/25 | Loss: 0.00172346
Iteration 8/25 | Loss: 0.00149727
Iteration 9/25 | Loss: 0.00137973
Iteration 10/25 | Loss: 0.00129999
Iteration 11/25 | Loss: 0.00127154
Iteration 12/25 | Loss: 0.00126742
Iteration 13/25 | Loss: 0.00125465
Iteration 14/25 | Loss: 0.00124821
Iteration 15/25 | Loss: 0.00123542
Iteration 16/25 | Loss: 0.00122477
Iteration 17/25 | Loss: 0.00121858
Iteration 18/25 | Loss: 0.00121981
Iteration 19/25 | Loss: 0.00121897
Iteration 20/25 | Loss: 0.00121699
Iteration 21/25 | Loss: 0.00121684
Iteration 22/25 | Loss: 0.00121684
Iteration 23/25 | Loss: 0.00121684
Iteration 24/25 | Loss: 0.00121684
Iteration 25/25 | Loss: 0.00121684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.43394661
Iteration 2/25 | Loss: 0.00124528
Iteration 3/25 | Loss: 0.00124527
Iteration 4/25 | Loss: 0.00115057
Iteration 5/25 | Loss: 0.00115055
Iteration 6/25 | Loss: 0.00115055
Iteration 7/25 | Loss: 0.00115055
Iteration 8/25 | Loss: 0.00115054
Iteration 9/25 | Loss: 0.00115054
Iteration 10/25 | Loss: 0.00115054
Iteration 11/25 | Loss: 0.00115054
Iteration 12/25 | Loss: 0.00115054
Iteration 13/25 | Loss: 0.00115054
Iteration 14/25 | Loss: 0.00115054
Iteration 15/25 | Loss: 0.00115054
Iteration 16/25 | Loss: 0.00115054
Iteration 17/25 | Loss: 0.00115054
Iteration 18/25 | Loss: 0.00115054
Iteration 19/25 | Loss: 0.00115054
Iteration 20/25 | Loss: 0.00115054
Iteration 21/25 | Loss: 0.00115054
Iteration 22/25 | Loss: 0.00115054
Iteration 23/25 | Loss: 0.00115054
Iteration 24/25 | Loss: 0.00115054
Iteration 25/25 | Loss: 0.00115054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115054
Iteration 2/1000 | Loss: 0.00053407
Iteration 3/1000 | Loss: 0.00018272
Iteration 4/1000 | Loss: 0.00018908
Iteration 5/1000 | Loss: 0.00085343
Iteration 6/1000 | Loss: 0.00019170
Iteration 7/1000 | Loss: 0.00012312
Iteration 8/1000 | Loss: 0.00011771
Iteration 9/1000 | Loss: 0.00018712
Iteration 10/1000 | Loss: 0.00182142
Iteration 11/1000 | Loss: 0.00369902
Iteration 12/1000 | Loss: 0.00051527
Iteration 13/1000 | Loss: 0.00082020
Iteration 14/1000 | Loss: 0.00041029
Iteration 15/1000 | Loss: 0.00010263
Iteration 16/1000 | Loss: 0.00016656
Iteration 17/1000 | Loss: 0.00139019
Iteration 18/1000 | Loss: 0.00075892
Iteration 19/1000 | Loss: 0.00059375
Iteration 20/1000 | Loss: 0.00028934
Iteration 21/1000 | Loss: 0.00010250
Iteration 22/1000 | Loss: 0.00010025
Iteration 23/1000 | Loss: 0.00062638
Iteration 24/1000 | Loss: 0.00110595
Iteration 25/1000 | Loss: 0.00369306
Iteration 26/1000 | Loss: 0.00260023
Iteration 27/1000 | Loss: 0.00207035
Iteration 28/1000 | Loss: 0.00124670
Iteration 29/1000 | Loss: 0.00017059
Iteration 30/1000 | Loss: 0.00124567
Iteration 31/1000 | Loss: 0.00297007
Iteration 32/1000 | Loss: 0.00109124
Iteration 33/1000 | Loss: 0.00058085
Iteration 34/1000 | Loss: 0.00014636
Iteration 35/1000 | Loss: 0.00010471
Iteration 36/1000 | Loss: 0.00034619
Iteration 37/1000 | Loss: 0.00006057
Iteration 38/1000 | Loss: 0.00005300
Iteration 39/1000 | Loss: 0.00005402
Iteration 40/1000 | Loss: 0.00004698
Iteration 41/1000 | Loss: 0.00004406
Iteration 42/1000 | Loss: 0.00004296
Iteration 43/1000 | Loss: 0.00004158
Iteration 44/1000 | Loss: 0.00008032
Iteration 45/1000 | Loss: 0.00014861
Iteration 46/1000 | Loss: 0.00008424
Iteration 47/1000 | Loss: 0.00003957
Iteration 48/1000 | Loss: 0.00003925
Iteration 49/1000 | Loss: 0.00017414
Iteration 50/1000 | Loss: 0.00004249
Iteration 51/1000 | Loss: 0.00005807
Iteration 52/1000 | Loss: 0.00003880
Iteration 53/1000 | Loss: 0.00004841
Iteration 54/1000 | Loss: 0.00003840
Iteration 55/1000 | Loss: 0.00013448
Iteration 56/1000 | Loss: 0.00004720
Iteration 57/1000 | Loss: 0.00003832
Iteration 58/1000 | Loss: 0.00006935
Iteration 59/1000 | Loss: 0.00004087
Iteration 60/1000 | Loss: 0.00003928
Iteration 61/1000 | Loss: 0.00003815
Iteration 62/1000 | Loss: 0.00003807
Iteration 63/1000 | Loss: 0.00010073
Iteration 64/1000 | Loss: 0.00006761
Iteration 65/1000 | Loss: 0.00006208
Iteration 66/1000 | Loss: 0.00003952
Iteration 67/1000 | Loss: 0.00005963
Iteration 68/1000 | Loss: 0.00003797
Iteration 69/1000 | Loss: 0.00003795
Iteration 70/1000 | Loss: 0.00003795
Iteration 71/1000 | Loss: 0.00004841
Iteration 72/1000 | Loss: 0.00005191
Iteration 73/1000 | Loss: 0.00004902
Iteration 74/1000 | Loss: 0.00003889
Iteration 75/1000 | Loss: 0.00003791
Iteration 76/1000 | Loss: 0.00003791
Iteration 77/1000 | Loss: 0.00003790
Iteration 78/1000 | Loss: 0.00003790
Iteration 79/1000 | Loss: 0.00003790
Iteration 80/1000 | Loss: 0.00003790
Iteration 81/1000 | Loss: 0.00003790
Iteration 82/1000 | Loss: 0.00003790
Iteration 83/1000 | Loss: 0.00003790
Iteration 84/1000 | Loss: 0.00003790
Iteration 85/1000 | Loss: 0.00003790
Iteration 86/1000 | Loss: 0.00004421
Iteration 87/1000 | Loss: 0.00003786
Iteration 88/1000 | Loss: 0.00003785
Iteration 89/1000 | Loss: 0.00003785
Iteration 90/1000 | Loss: 0.00003785
Iteration 91/1000 | Loss: 0.00003785
Iteration 92/1000 | Loss: 0.00003785
Iteration 93/1000 | Loss: 0.00003785
Iteration 94/1000 | Loss: 0.00003785
Iteration 95/1000 | Loss: 0.00003785
Iteration 96/1000 | Loss: 0.00003785
Iteration 97/1000 | Loss: 0.00003784
Iteration 98/1000 | Loss: 0.00003784
Iteration 99/1000 | Loss: 0.00003784
Iteration 100/1000 | Loss: 0.00003784
Iteration 101/1000 | Loss: 0.00003784
Iteration 102/1000 | Loss: 0.00003784
Iteration 103/1000 | Loss: 0.00003784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [3.784296495723538e-05, 3.784296495723538e-05, 3.784296495723538e-05, 3.784296495723538e-05, 3.784296495723538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.784296495723538e-05

Optimization complete. Final v2v error: 5.015427112579346 mm

Highest mean error: 9.392935752868652 mm for frame 78

Lowest mean error: 4.533853530883789 mm for frame 120

Saving results

Total time: 142.40290522575378
