Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=271, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15176-15231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089593
Iteration 2/25 | Loss: 0.00248678
Iteration 3/25 | Loss: 0.00131254
Iteration 4/25 | Loss: 0.00115149
Iteration 5/25 | Loss: 0.00105300
Iteration 6/25 | Loss: 0.00104881
Iteration 7/25 | Loss: 0.00099954
Iteration 8/25 | Loss: 0.00095696
Iteration 9/25 | Loss: 0.00094320
Iteration 10/25 | Loss: 0.00093865
Iteration 11/25 | Loss: 0.00093274
Iteration 12/25 | Loss: 0.00091386
Iteration 13/25 | Loss: 0.00093140
Iteration 14/25 | Loss: 0.00090360
Iteration 15/25 | Loss: 0.00088721
Iteration 16/25 | Loss: 0.00088555
Iteration 17/25 | Loss: 0.00088583
Iteration 18/25 | Loss: 0.00087927
Iteration 19/25 | Loss: 0.00087048
Iteration 20/25 | Loss: 0.00086417
Iteration 21/25 | Loss: 0.00086217
Iteration 22/25 | Loss: 0.00086066
Iteration 23/25 | Loss: 0.00086030
Iteration 24/25 | Loss: 0.00085989
Iteration 25/25 | Loss: 0.00085967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49633241
Iteration 2/25 | Loss: 0.00073547
Iteration 3/25 | Loss: 0.00071775
Iteration 4/25 | Loss: 0.00071775
Iteration 5/25 | Loss: 0.00071775
Iteration 6/25 | Loss: 0.00071775
Iteration 7/25 | Loss: 0.00071775
Iteration 8/25 | Loss: 0.00071775
Iteration 9/25 | Loss: 0.00071775
Iteration 10/25 | Loss: 0.00071775
Iteration 11/25 | Loss: 0.00071775
Iteration 12/25 | Loss: 0.00071775
Iteration 13/25 | Loss: 0.00071775
Iteration 14/25 | Loss: 0.00071775
Iteration 15/25 | Loss: 0.00071775
Iteration 16/25 | Loss: 0.00071775
Iteration 17/25 | Loss: 0.00071775
Iteration 18/25 | Loss: 0.00071775
Iteration 19/25 | Loss: 0.00071775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007177460938692093, 0.0007177460938692093, 0.0007177460938692093, 0.0007177460938692093, 0.0007177460938692093]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007177460938692093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071775
Iteration 2/1000 | Loss: 0.00006873
Iteration 3/1000 | Loss: 0.00012269
Iteration 4/1000 | Loss: 0.00007786
Iteration 5/1000 | Loss: 0.00018599
Iteration 6/1000 | Loss: 0.00006526
Iteration 7/1000 | Loss: 0.00002491
Iteration 8/1000 | Loss: 0.00002455
Iteration 9/1000 | Loss: 0.00009315
Iteration 10/1000 | Loss: 0.00007765
Iteration 11/1000 | Loss: 0.00002376
Iteration 12/1000 | Loss: 0.00006073
Iteration 13/1000 | Loss: 0.00004649
Iteration 14/1000 | Loss: 0.00002351
Iteration 15/1000 | Loss: 0.00006359
Iteration 16/1000 | Loss: 0.00002318
Iteration 17/1000 | Loss: 0.00002300
Iteration 18/1000 | Loss: 0.00002288
Iteration 19/1000 | Loss: 0.00007682
Iteration 20/1000 | Loss: 0.00002379
Iteration 21/1000 | Loss: 0.00002284
Iteration 22/1000 | Loss: 0.00002272
Iteration 23/1000 | Loss: 0.00002272
Iteration 24/1000 | Loss: 0.00003303
Iteration 25/1000 | Loss: 0.00003302
Iteration 26/1000 | Loss: 0.00002513
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002265
Iteration 29/1000 | Loss: 0.00002265
Iteration 30/1000 | Loss: 0.00002265
Iteration 31/1000 | Loss: 0.00002265
Iteration 32/1000 | Loss: 0.00002264
Iteration 33/1000 | Loss: 0.00002264
Iteration 34/1000 | Loss: 0.00002264
Iteration 35/1000 | Loss: 0.00002264
Iteration 36/1000 | Loss: 0.00002264
Iteration 37/1000 | Loss: 0.00002264
Iteration 38/1000 | Loss: 0.00002264
Iteration 39/1000 | Loss: 0.00002264
Iteration 40/1000 | Loss: 0.00002264
Iteration 41/1000 | Loss: 0.00002264
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002264
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002264
Iteration 47/1000 | Loss: 0.00002264
Iteration 48/1000 | Loss: 0.00002264
Iteration 49/1000 | Loss: 0.00002264
Iteration 50/1000 | Loss: 0.00002264
Iteration 51/1000 | Loss: 0.00002264
Iteration 52/1000 | Loss: 0.00002264
Iteration 53/1000 | Loss: 0.00002264
Iteration 54/1000 | Loss: 0.00002264
Iteration 55/1000 | Loss: 0.00002264
Iteration 56/1000 | Loss: 0.00002264
Iteration 57/1000 | Loss: 0.00002264
Iteration 58/1000 | Loss: 0.00002264
Iteration 59/1000 | Loss: 0.00002264
Iteration 60/1000 | Loss: 0.00002264
Iteration 61/1000 | Loss: 0.00002264
Iteration 62/1000 | Loss: 0.00002264
Iteration 63/1000 | Loss: 0.00002264
Iteration 64/1000 | Loss: 0.00002264
Iteration 65/1000 | Loss: 0.00002264
Iteration 66/1000 | Loss: 0.00002264
Iteration 67/1000 | Loss: 0.00002264
Iteration 68/1000 | Loss: 0.00002264
Iteration 69/1000 | Loss: 0.00002264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.263689020765014e-05, 2.263689020765014e-05, 2.263689020765014e-05, 2.263689020765014e-05, 2.263689020765014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.263689020765014e-05

Optimization complete. Final v2v error: 4.004336833953857 mm

Highest mean error: 4.487329483032227 mm for frame 43

Lowest mean error: 3.519379138946533 mm for frame 142

Saving results

Total time: 92.59403562545776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852005
Iteration 2/25 | Loss: 0.00132670
Iteration 3/25 | Loss: 0.00097513
Iteration 4/25 | Loss: 0.00090157
Iteration 5/25 | Loss: 0.00088817
Iteration 6/25 | Loss: 0.00088537
Iteration 7/25 | Loss: 0.00088445
Iteration 8/25 | Loss: 0.00088441
Iteration 9/25 | Loss: 0.00088441
Iteration 10/25 | Loss: 0.00088441
Iteration 11/25 | Loss: 0.00088441
Iteration 12/25 | Loss: 0.00088441
Iteration 13/25 | Loss: 0.00088441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008844091789796948, 0.0008844091789796948, 0.0008844091789796948, 0.0008844091789796948, 0.0008844091789796948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008844091789796948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70417881
Iteration 2/25 | Loss: 0.00087426
Iteration 3/25 | Loss: 0.00087422
Iteration 4/25 | Loss: 0.00087422
Iteration 5/25 | Loss: 0.00087422
Iteration 6/25 | Loss: 0.00087422
Iteration 7/25 | Loss: 0.00087421
Iteration 8/25 | Loss: 0.00087421
Iteration 9/25 | Loss: 0.00087421
Iteration 10/25 | Loss: 0.00087421
Iteration 11/25 | Loss: 0.00087421
Iteration 12/25 | Loss: 0.00087421
Iteration 13/25 | Loss: 0.00087421
Iteration 14/25 | Loss: 0.00087421
Iteration 15/25 | Loss: 0.00087421
Iteration 16/25 | Loss: 0.00087421
Iteration 17/25 | Loss: 0.00087421
Iteration 18/25 | Loss: 0.00087421
Iteration 19/25 | Loss: 0.00087421
Iteration 20/25 | Loss: 0.00087421
Iteration 21/25 | Loss: 0.00087421
Iteration 22/25 | Loss: 0.00087421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008742140489630401, 0.0008742140489630401, 0.0008742140489630401, 0.0008742140489630401, 0.0008742140489630401]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008742140489630401

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087421
Iteration 2/1000 | Loss: 0.00007555
Iteration 3/1000 | Loss: 0.00005222
Iteration 4/1000 | Loss: 0.00004106
Iteration 5/1000 | Loss: 0.00003707
Iteration 6/1000 | Loss: 0.00003495
Iteration 7/1000 | Loss: 0.00003371
Iteration 8/1000 | Loss: 0.00003263
Iteration 9/1000 | Loss: 0.00003195
Iteration 10/1000 | Loss: 0.00003129
Iteration 11/1000 | Loss: 0.00003079
Iteration 12/1000 | Loss: 0.00003033
Iteration 13/1000 | Loss: 0.00002987
Iteration 14/1000 | Loss: 0.00002954
Iteration 15/1000 | Loss: 0.00002930
Iteration 16/1000 | Loss: 0.00002905
Iteration 17/1000 | Loss: 0.00002896
Iteration 18/1000 | Loss: 0.00002880
Iteration 19/1000 | Loss: 0.00002864
Iteration 20/1000 | Loss: 0.00002849
Iteration 21/1000 | Loss: 0.00002848
Iteration 22/1000 | Loss: 0.00002843
Iteration 23/1000 | Loss: 0.00002843
Iteration 24/1000 | Loss: 0.00002842
Iteration 25/1000 | Loss: 0.00002841
Iteration 26/1000 | Loss: 0.00002840
Iteration 27/1000 | Loss: 0.00002839
Iteration 28/1000 | Loss: 0.00002839
Iteration 29/1000 | Loss: 0.00002839
Iteration 30/1000 | Loss: 0.00002838
Iteration 31/1000 | Loss: 0.00002838
Iteration 32/1000 | Loss: 0.00002838
Iteration 33/1000 | Loss: 0.00002837
Iteration 34/1000 | Loss: 0.00002837
Iteration 35/1000 | Loss: 0.00002836
Iteration 36/1000 | Loss: 0.00002836
Iteration 37/1000 | Loss: 0.00002836
Iteration 38/1000 | Loss: 0.00002835
Iteration 39/1000 | Loss: 0.00002835
Iteration 40/1000 | Loss: 0.00002834
Iteration 41/1000 | Loss: 0.00002834
Iteration 42/1000 | Loss: 0.00002834
Iteration 43/1000 | Loss: 0.00002833
Iteration 44/1000 | Loss: 0.00002833
Iteration 45/1000 | Loss: 0.00002833
Iteration 46/1000 | Loss: 0.00002833
Iteration 47/1000 | Loss: 0.00002832
Iteration 48/1000 | Loss: 0.00002831
Iteration 49/1000 | Loss: 0.00002831
Iteration 50/1000 | Loss: 0.00002831
Iteration 51/1000 | Loss: 0.00002830
Iteration 52/1000 | Loss: 0.00002830
Iteration 53/1000 | Loss: 0.00002830
Iteration 54/1000 | Loss: 0.00002830
Iteration 55/1000 | Loss: 0.00002830
Iteration 56/1000 | Loss: 0.00002830
Iteration 57/1000 | Loss: 0.00002830
Iteration 58/1000 | Loss: 0.00002830
Iteration 59/1000 | Loss: 0.00002830
Iteration 60/1000 | Loss: 0.00002829
Iteration 61/1000 | Loss: 0.00002829
Iteration 62/1000 | Loss: 0.00002829
Iteration 63/1000 | Loss: 0.00002829
Iteration 64/1000 | Loss: 0.00002829
Iteration 65/1000 | Loss: 0.00002829
Iteration 66/1000 | Loss: 0.00002829
Iteration 67/1000 | Loss: 0.00002828
Iteration 68/1000 | Loss: 0.00002828
Iteration 69/1000 | Loss: 0.00002828
Iteration 70/1000 | Loss: 0.00002828
Iteration 71/1000 | Loss: 0.00002828
Iteration 72/1000 | Loss: 0.00002828
Iteration 73/1000 | Loss: 0.00002828
Iteration 74/1000 | Loss: 0.00002828
Iteration 75/1000 | Loss: 0.00002828
Iteration 76/1000 | Loss: 0.00002828
Iteration 77/1000 | Loss: 0.00002827
Iteration 78/1000 | Loss: 0.00002827
Iteration 79/1000 | Loss: 0.00002827
Iteration 80/1000 | Loss: 0.00002827
Iteration 81/1000 | Loss: 0.00002827
Iteration 82/1000 | Loss: 0.00002827
Iteration 83/1000 | Loss: 0.00002827
Iteration 84/1000 | Loss: 0.00002827
Iteration 85/1000 | Loss: 0.00002826
Iteration 86/1000 | Loss: 0.00002826
Iteration 87/1000 | Loss: 0.00002826
Iteration 88/1000 | Loss: 0.00002826
Iteration 89/1000 | Loss: 0.00002826
Iteration 90/1000 | Loss: 0.00002826
Iteration 91/1000 | Loss: 0.00002826
Iteration 92/1000 | Loss: 0.00002826
Iteration 93/1000 | Loss: 0.00002826
Iteration 94/1000 | Loss: 0.00002825
Iteration 95/1000 | Loss: 0.00002825
Iteration 96/1000 | Loss: 0.00002825
Iteration 97/1000 | Loss: 0.00002825
Iteration 98/1000 | Loss: 0.00002825
Iteration 99/1000 | Loss: 0.00002825
Iteration 100/1000 | Loss: 0.00002825
Iteration 101/1000 | Loss: 0.00002824
Iteration 102/1000 | Loss: 0.00002824
Iteration 103/1000 | Loss: 0.00002824
Iteration 104/1000 | Loss: 0.00002824
Iteration 105/1000 | Loss: 0.00002824
Iteration 106/1000 | Loss: 0.00002824
Iteration 107/1000 | Loss: 0.00002823
Iteration 108/1000 | Loss: 0.00002823
Iteration 109/1000 | Loss: 0.00002823
Iteration 110/1000 | Loss: 0.00002823
Iteration 111/1000 | Loss: 0.00002823
Iteration 112/1000 | Loss: 0.00002823
Iteration 113/1000 | Loss: 0.00002823
Iteration 114/1000 | Loss: 0.00002823
Iteration 115/1000 | Loss: 0.00002823
Iteration 116/1000 | Loss: 0.00002823
Iteration 117/1000 | Loss: 0.00002823
Iteration 118/1000 | Loss: 0.00002823
Iteration 119/1000 | Loss: 0.00002823
Iteration 120/1000 | Loss: 0.00002822
Iteration 121/1000 | Loss: 0.00002822
Iteration 122/1000 | Loss: 0.00002822
Iteration 123/1000 | Loss: 0.00002822
Iteration 124/1000 | Loss: 0.00002822
Iteration 125/1000 | Loss: 0.00002822
Iteration 126/1000 | Loss: 0.00002822
Iteration 127/1000 | Loss: 0.00002822
Iteration 128/1000 | Loss: 0.00002822
Iteration 129/1000 | Loss: 0.00002822
Iteration 130/1000 | Loss: 0.00002822
Iteration 131/1000 | Loss: 0.00002822
Iteration 132/1000 | Loss: 0.00002822
Iteration 133/1000 | Loss: 0.00002822
Iteration 134/1000 | Loss: 0.00002822
Iteration 135/1000 | Loss: 0.00002822
Iteration 136/1000 | Loss: 0.00002822
Iteration 137/1000 | Loss: 0.00002822
Iteration 138/1000 | Loss: 0.00002822
Iteration 139/1000 | Loss: 0.00002822
Iteration 140/1000 | Loss: 0.00002822
Iteration 141/1000 | Loss: 0.00002822
Iteration 142/1000 | Loss: 0.00002822
Iteration 143/1000 | Loss: 0.00002822
Iteration 144/1000 | Loss: 0.00002822
Iteration 145/1000 | Loss: 0.00002822
Iteration 146/1000 | Loss: 0.00002822
Iteration 147/1000 | Loss: 0.00002822
Iteration 148/1000 | Loss: 0.00002822
Iteration 149/1000 | Loss: 0.00002822
Iteration 150/1000 | Loss: 0.00002822
Iteration 151/1000 | Loss: 0.00002822
Iteration 152/1000 | Loss: 0.00002822
Iteration 153/1000 | Loss: 0.00002822
Iteration 154/1000 | Loss: 0.00002822
Iteration 155/1000 | Loss: 0.00002822
Iteration 156/1000 | Loss: 0.00002822
Iteration 157/1000 | Loss: 0.00002822
Iteration 158/1000 | Loss: 0.00002822
Iteration 159/1000 | Loss: 0.00002822
Iteration 160/1000 | Loss: 0.00002822
Iteration 161/1000 | Loss: 0.00002822
Iteration 162/1000 | Loss: 0.00002822
Iteration 163/1000 | Loss: 0.00002822
Iteration 164/1000 | Loss: 0.00002822
Iteration 165/1000 | Loss: 0.00002822
Iteration 166/1000 | Loss: 0.00002822
Iteration 167/1000 | Loss: 0.00002822
Iteration 168/1000 | Loss: 0.00002822
Iteration 169/1000 | Loss: 0.00002822
Iteration 170/1000 | Loss: 0.00002822
Iteration 171/1000 | Loss: 0.00002822
Iteration 172/1000 | Loss: 0.00002822
Iteration 173/1000 | Loss: 0.00002822
Iteration 174/1000 | Loss: 0.00002822
Iteration 175/1000 | Loss: 0.00002822
Iteration 176/1000 | Loss: 0.00002822
Iteration 177/1000 | Loss: 0.00002822
Iteration 178/1000 | Loss: 0.00002822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.8223790650372393e-05, 2.8223790650372393e-05, 2.8223790650372393e-05, 2.8223790650372393e-05, 2.8223790650372393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8223790650372393e-05

Optimization complete. Final v2v error: 4.202324867248535 mm

Highest mean error: 6.05074405670166 mm for frame 163

Lowest mean error: 2.907412052154541 mm for frame 56

Saving results

Total time: 50.00606155395508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048465
Iteration 2/25 | Loss: 0.00210309
Iteration 3/25 | Loss: 0.00175034
Iteration 4/25 | Loss: 0.00166112
Iteration 5/25 | Loss: 0.00123124
Iteration 6/25 | Loss: 0.00147379
Iteration 7/25 | Loss: 0.00118495
Iteration 8/25 | Loss: 0.00095367
Iteration 9/25 | Loss: 0.00088415
Iteration 10/25 | Loss: 0.00086887
Iteration 11/25 | Loss: 0.00086626
Iteration 12/25 | Loss: 0.00086479
Iteration 13/25 | Loss: 0.00086424
Iteration 14/25 | Loss: 0.00086357
Iteration 15/25 | Loss: 0.00086314
Iteration 16/25 | Loss: 0.00086289
Iteration 17/25 | Loss: 0.00086285
Iteration 18/25 | Loss: 0.00086285
Iteration 19/25 | Loss: 0.00086285
Iteration 20/25 | Loss: 0.00086285
Iteration 21/25 | Loss: 0.00086285
Iteration 22/25 | Loss: 0.00086285
Iteration 23/25 | Loss: 0.00086284
Iteration 24/25 | Loss: 0.00086284
Iteration 25/25 | Loss: 0.00086284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55615401
Iteration 2/25 | Loss: 0.00085419
Iteration 3/25 | Loss: 0.00085418
Iteration 4/25 | Loss: 0.00085418
Iteration 5/25 | Loss: 0.00085418
Iteration 6/25 | Loss: 0.00085418
Iteration 7/25 | Loss: 0.00085418
Iteration 8/25 | Loss: 0.00085418
Iteration 9/25 | Loss: 0.00085418
Iteration 10/25 | Loss: 0.00085418
Iteration 11/25 | Loss: 0.00085418
Iteration 12/25 | Loss: 0.00085418
Iteration 13/25 | Loss: 0.00085418
Iteration 14/25 | Loss: 0.00085418
Iteration 15/25 | Loss: 0.00085418
Iteration 16/25 | Loss: 0.00085418
Iteration 17/25 | Loss: 0.00085418
Iteration 18/25 | Loss: 0.00085418
Iteration 19/25 | Loss: 0.00085418
Iteration 20/25 | Loss: 0.00085418
Iteration 21/25 | Loss: 0.00085418
Iteration 22/25 | Loss: 0.00085418
Iteration 23/25 | Loss: 0.00085418
Iteration 24/25 | Loss: 0.00085418
Iteration 25/25 | Loss: 0.00085418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008541776915080845, 0.0008541776915080845, 0.0008541776915080845, 0.0008541776915080845, 0.0008541776915080845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008541776915080845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085418
Iteration 2/1000 | Loss: 0.00003076
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00002077
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001971
Iteration 9/1000 | Loss: 0.00001946
Iteration 10/1000 | Loss: 0.00001944
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001934
Iteration 13/1000 | Loss: 0.00001932
Iteration 14/1000 | Loss: 0.00001931
Iteration 15/1000 | Loss: 0.00001931
Iteration 16/1000 | Loss: 0.00001928
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001921
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00001920
Iteration 23/1000 | Loss: 0.00001918
Iteration 24/1000 | Loss: 0.00001918
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001917
Iteration 27/1000 | Loss: 0.00001917
Iteration 28/1000 | Loss: 0.00001914
Iteration 29/1000 | Loss: 0.00001914
Iteration 30/1000 | Loss: 0.00001913
Iteration 31/1000 | Loss: 0.00001913
Iteration 32/1000 | Loss: 0.00001912
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001911
Iteration 36/1000 | Loss: 0.00001911
Iteration 37/1000 | Loss: 0.00001910
Iteration 38/1000 | Loss: 0.00001910
Iteration 39/1000 | Loss: 0.00001910
Iteration 40/1000 | Loss: 0.00001910
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001909
Iteration 44/1000 | Loss: 0.00001909
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001907
Iteration 65/1000 | Loss: 0.00001906
Iteration 66/1000 | Loss: 0.00001906
Iteration 67/1000 | Loss: 0.00001906
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001905
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001905
Iteration 75/1000 | Loss: 0.00001905
Iteration 76/1000 | Loss: 0.00001905
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001905
Iteration 80/1000 | Loss: 0.00001905
Iteration 81/1000 | Loss: 0.00001904
Iteration 82/1000 | Loss: 0.00001904
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001904
Iteration 89/1000 | Loss: 0.00001904
Iteration 90/1000 | Loss: 0.00001904
Iteration 91/1000 | Loss: 0.00001904
Iteration 92/1000 | Loss: 0.00001904
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001903
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001903
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001902
Iteration 101/1000 | Loss: 0.00001902
Iteration 102/1000 | Loss: 0.00001902
Iteration 103/1000 | Loss: 0.00001902
Iteration 104/1000 | Loss: 0.00001902
Iteration 105/1000 | Loss: 0.00001902
Iteration 106/1000 | Loss: 0.00001902
Iteration 107/1000 | Loss: 0.00001902
Iteration 108/1000 | Loss: 0.00001902
Iteration 109/1000 | Loss: 0.00001902
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001901
Iteration 112/1000 | Loss: 0.00001901
Iteration 113/1000 | Loss: 0.00001901
Iteration 114/1000 | Loss: 0.00001901
Iteration 115/1000 | Loss: 0.00001901
Iteration 116/1000 | Loss: 0.00001901
Iteration 117/1000 | Loss: 0.00001901
Iteration 118/1000 | Loss: 0.00001901
Iteration 119/1000 | Loss: 0.00001901
Iteration 120/1000 | Loss: 0.00001901
Iteration 121/1000 | Loss: 0.00001901
Iteration 122/1000 | Loss: 0.00001901
Iteration 123/1000 | Loss: 0.00001901
Iteration 124/1000 | Loss: 0.00001901
Iteration 125/1000 | Loss: 0.00001901
Iteration 126/1000 | Loss: 0.00001901
Iteration 127/1000 | Loss: 0.00001901
Iteration 128/1000 | Loss: 0.00001901
Iteration 129/1000 | Loss: 0.00001900
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001900
Iteration 132/1000 | Loss: 0.00001900
Iteration 133/1000 | Loss: 0.00001900
Iteration 134/1000 | Loss: 0.00001900
Iteration 135/1000 | Loss: 0.00001900
Iteration 136/1000 | Loss: 0.00001900
Iteration 137/1000 | Loss: 0.00001900
Iteration 138/1000 | Loss: 0.00001900
Iteration 139/1000 | Loss: 0.00001900
Iteration 140/1000 | Loss: 0.00001900
Iteration 141/1000 | Loss: 0.00001899
Iteration 142/1000 | Loss: 0.00001899
Iteration 143/1000 | Loss: 0.00001899
Iteration 144/1000 | Loss: 0.00001899
Iteration 145/1000 | Loss: 0.00001899
Iteration 146/1000 | Loss: 0.00001899
Iteration 147/1000 | Loss: 0.00001899
Iteration 148/1000 | Loss: 0.00001899
Iteration 149/1000 | Loss: 0.00001899
Iteration 150/1000 | Loss: 0.00001899
Iteration 151/1000 | Loss: 0.00001899
Iteration 152/1000 | Loss: 0.00001899
Iteration 153/1000 | Loss: 0.00001899
Iteration 154/1000 | Loss: 0.00001898
Iteration 155/1000 | Loss: 0.00001898
Iteration 156/1000 | Loss: 0.00001898
Iteration 157/1000 | Loss: 0.00001898
Iteration 158/1000 | Loss: 0.00001898
Iteration 159/1000 | Loss: 0.00001898
Iteration 160/1000 | Loss: 0.00001898
Iteration 161/1000 | Loss: 0.00001898
Iteration 162/1000 | Loss: 0.00001897
Iteration 163/1000 | Loss: 0.00001897
Iteration 164/1000 | Loss: 0.00001897
Iteration 165/1000 | Loss: 0.00001897
Iteration 166/1000 | Loss: 0.00001897
Iteration 167/1000 | Loss: 0.00001897
Iteration 168/1000 | Loss: 0.00001897
Iteration 169/1000 | Loss: 0.00001897
Iteration 170/1000 | Loss: 0.00001897
Iteration 171/1000 | Loss: 0.00001897
Iteration 172/1000 | Loss: 0.00001897
Iteration 173/1000 | Loss: 0.00001897
Iteration 174/1000 | Loss: 0.00001897
Iteration 175/1000 | Loss: 0.00001897
Iteration 176/1000 | Loss: 0.00001897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.896782123367302e-05, 1.896782123367302e-05, 1.896782123367302e-05, 1.896782123367302e-05, 1.896782123367302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.896782123367302e-05

Optimization complete. Final v2v error: 3.6331803798675537 mm

Highest mean error: 3.7360477447509766 mm for frame 195

Lowest mean error: 3.543142080307007 mm for frame 183

Saving results

Total time: 59.65248227119446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393689
Iteration 2/25 | Loss: 0.00099358
Iteration 3/25 | Loss: 0.00078761
Iteration 4/25 | Loss: 0.00077470
Iteration 5/25 | Loss: 0.00076639
Iteration 6/25 | Loss: 0.00076428
Iteration 7/25 | Loss: 0.00076425
Iteration 8/25 | Loss: 0.00076425
Iteration 9/25 | Loss: 0.00076425
Iteration 10/25 | Loss: 0.00076425
Iteration 11/25 | Loss: 0.00076425
Iteration 12/25 | Loss: 0.00076425
Iteration 13/25 | Loss: 0.00076425
Iteration 14/25 | Loss: 0.00076425
Iteration 15/25 | Loss: 0.00076425
Iteration 16/25 | Loss: 0.00076425
Iteration 17/25 | Loss: 0.00076425
Iteration 18/25 | Loss: 0.00076425
Iteration 19/25 | Loss: 0.00076425
Iteration 20/25 | Loss: 0.00076425
Iteration 21/25 | Loss: 0.00076425
Iteration 22/25 | Loss: 0.00076425
Iteration 23/25 | Loss: 0.00076425
Iteration 24/25 | Loss: 0.00076425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007642467389814556, 0.0007642467389814556, 0.0007642467389814556, 0.0007642467389814556, 0.0007642467389814556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007642467389814556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68697929
Iteration 2/25 | Loss: 0.00104516
Iteration 3/25 | Loss: 0.00104516
Iteration 4/25 | Loss: 0.00104516
Iteration 5/25 | Loss: 0.00104516
Iteration 6/25 | Loss: 0.00104516
Iteration 7/25 | Loss: 0.00104516
Iteration 8/25 | Loss: 0.00104516
Iteration 9/25 | Loss: 0.00104516
Iteration 10/25 | Loss: 0.00104516
Iteration 11/25 | Loss: 0.00104516
Iteration 12/25 | Loss: 0.00104516
Iteration 13/25 | Loss: 0.00104516
Iteration 14/25 | Loss: 0.00104516
Iteration 15/25 | Loss: 0.00104516
Iteration 16/25 | Loss: 0.00104516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010451576672494411, 0.0010451576672494411, 0.0010451576672494411, 0.0010451576672494411, 0.0010451576672494411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010451576672494411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104516
Iteration 2/1000 | Loss: 0.00002490
Iteration 3/1000 | Loss: 0.00001672
Iteration 4/1000 | Loss: 0.00001500
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001344
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001299
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001290
Iteration 15/1000 | Loss: 0.00001282
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001273
Iteration 19/1000 | Loss: 0.00001271
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001268
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001267
Iteration 30/1000 | Loss: 0.00001267
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001267
Iteration 33/1000 | Loss: 0.00001267
Iteration 34/1000 | Loss: 0.00001266
Iteration 35/1000 | Loss: 0.00001266
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001265
Iteration 38/1000 | Loss: 0.00001265
Iteration 39/1000 | Loss: 0.00001265
Iteration 40/1000 | Loss: 0.00001265
Iteration 41/1000 | Loss: 0.00001265
Iteration 42/1000 | Loss: 0.00001265
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001262
Iteration 56/1000 | Loss: 0.00001262
Iteration 57/1000 | Loss: 0.00001261
Iteration 58/1000 | Loss: 0.00001261
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001260
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001260
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001260
Iteration 71/1000 | Loss: 0.00001260
Iteration 72/1000 | Loss: 0.00001260
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001259
Iteration 83/1000 | Loss: 0.00001259
Iteration 84/1000 | Loss: 0.00001259
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.2593274732353166e-05, 1.2593274732353166e-05, 1.2593274732353166e-05, 1.2593274732353166e-05, 1.2593274732353166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2593274732353166e-05

Optimization complete. Final v2v error: 3.0152652263641357 mm

Highest mean error: 3.2393815517425537 mm for frame 10

Lowest mean error: 2.8470213413238525 mm for frame 80

Saving results

Total time: 34.84610152244568
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938717
Iteration 2/25 | Loss: 0.00114021
Iteration 3/25 | Loss: 0.00085089
Iteration 4/25 | Loss: 0.00079243
Iteration 5/25 | Loss: 0.00077425
Iteration 6/25 | Loss: 0.00076976
Iteration 7/25 | Loss: 0.00076802
Iteration 8/25 | Loss: 0.00076233
Iteration 9/25 | Loss: 0.00075697
Iteration 10/25 | Loss: 0.00075300
Iteration 11/25 | Loss: 0.00075187
Iteration 12/25 | Loss: 0.00075146
Iteration 13/25 | Loss: 0.00075135
Iteration 14/25 | Loss: 0.00075135
Iteration 15/25 | Loss: 0.00075135
Iteration 16/25 | Loss: 0.00075135
Iteration 17/25 | Loss: 0.00075135
Iteration 18/25 | Loss: 0.00075135
Iteration 19/25 | Loss: 0.00075135
Iteration 20/25 | Loss: 0.00075135
Iteration 21/25 | Loss: 0.00075134
Iteration 22/25 | Loss: 0.00075134
Iteration 23/25 | Loss: 0.00075134
Iteration 24/25 | Loss: 0.00075134
Iteration 25/25 | Loss: 0.00075134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54704154
Iteration 2/25 | Loss: 0.00066705
Iteration 3/25 | Loss: 0.00066702
Iteration 4/25 | Loss: 0.00066702
Iteration 5/25 | Loss: 0.00066702
Iteration 6/25 | Loss: 0.00066702
Iteration 7/25 | Loss: 0.00066702
Iteration 8/25 | Loss: 0.00066701
Iteration 9/25 | Loss: 0.00066701
Iteration 10/25 | Loss: 0.00066701
Iteration 11/25 | Loss: 0.00066701
Iteration 12/25 | Loss: 0.00066701
Iteration 13/25 | Loss: 0.00066701
Iteration 14/25 | Loss: 0.00066701
Iteration 15/25 | Loss: 0.00066701
Iteration 16/25 | Loss: 0.00066701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006670146249234676, 0.0006670146249234676, 0.0006670146249234676, 0.0006670146249234676, 0.0006670146249234676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006670146249234676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066701
Iteration 2/1000 | Loss: 0.00002752
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001546
Iteration 6/1000 | Loss: 0.00001443
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001328
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001296
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001296
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001296
Iteration 29/1000 | Loss: 0.00001296
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001296
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001296
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001296
Iteration 49/1000 | Loss: 0.00001296
Iteration 50/1000 | Loss: 0.00001296
Iteration 51/1000 | Loss: 0.00001296
Iteration 52/1000 | Loss: 0.00001296
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001296
Iteration 56/1000 | Loss: 0.00001296
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001296
Iteration 60/1000 | Loss: 0.00001296
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001296
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001296
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001296
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001296
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001296
Iteration 91/1000 | Loss: 0.00001296
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.2962768778379541e-05, 1.2962768778379541e-05, 1.2962768778379541e-05, 1.2962768778379541e-05, 1.2962768778379541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2962768778379541e-05

Optimization complete. Final v2v error: 3.0883567333221436 mm

Highest mean error: 3.2910923957824707 mm for frame 95

Lowest mean error: 2.8247666358947754 mm for frame 165

Saving results

Total time: 48.34938192367554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806571
Iteration 2/25 | Loss: 0.00130860
Iteration 3/25 | Loss: 0.00104685
Iteration 4/25 | Loss: 0.00096998
Iteration 5/25 | Loss: 0.00093604
Iteration 6/25 | Loss: 0.00090366
Iteration 7/25 | Loss: 0.00089506
Iteration 8/25 | Loss: 0.00088991
Iteration 9/25 | Loss: 0.00089017
Iteration 10/25 | Loss: 0.00088868
Iteration 11/25 | Loss: 0.00088829
Iteration 12/25 | Loss: 0.00088746
Iteration 13/25 | Loss: 0.00088684
Iteration 14/25 | Loss: 0.00088644
Iteration 15/25 | Loss: 0.00088609
Iteration 16/25 | Loss: 0.00088492
Iteration 17/25 | Loss: 0.00088799
Iteration 18/25 | Loss: 0.00088862
Iteration 19/25 | Loss: 0.00088818
Iteration 20/25 | Loss: 0.00088647
Iteration 21/25 | Loss: 0.00088414
Iteration 22/25 | Loss: 0.00088681
Iteration 23/25 | Loss: 0.00088768
Iteration 24/25 | Loss: 0.00089145
Iteration 25/25 | Loss: 0.00088766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49135447
Iteration 2/25 | Loss: 0.00158322
Iteration 3/25 | Loss: 0.00158316
Iteration 4/25 | Loss: 0.00158316
Iteration 5/25 | Loss: 0.00158316
Iteration 6/25 | Loss: 0.00158316
Iteration 7/25 | Loss: 0.00158316
Iteration 8/25 | Loss: 0.00158316
Iteration 9/25 | Loss: 0.00158316
Iteration 10/25 | Loss: 0.00158316
Iteration 11/25 | Loss: 0.00158316
Iteration 12/25 | Loss: 0.00158316
Iteration 13/25 | Loss: 0.00158316
Iteration 14/25 | Loss: 0.00158316
Iteration 15/25 | Loss: 0.00158316
Iteration 16/25 | Loss: 0.00158316
Iteration 17/25 | Loss: 0.00158316
Iteration 18/25 | Loss: 0.00158316
Iteration 19/25 | Loss: 0.00158316
Iteration 20/25 | Loss: 0.00158316
Iteration 21/25 | Loss: 0.00158316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015831615310162306, 0.0015831615310162306, 0.0015831615310162306, 0.0015831615310162306, 0.0015831615310162306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015831615310162306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158316
Iteration 2/1000 | Loss: 0.00072415
Iteration 3/1000 | Loss: 0.00030821
Iteration 4/1000 | Loss: 0.00086568
Iteration 5/1000 | Loss: 0.00116659
Iteration 6/1000 | Loss: 0.00025079
Iteration 7/1000 | Loss: 0.00071834
Iteration 8/1000 | Loss: 0.00040468
Iteration 9/1000 | Loss: 0.00007551
Iteration 10/1000 | Loss: 0.00074134
Iteration 11/1000 | Loss: 0.00055987
Iteration 12/1000 | Loss: 0.00054605
Iteration 13/1000 | Loss: 0.00038422
Iteration 14/1000 | Loss: 0.00008761
Iteration 15/1000 | Loss: 0.00005252
Iteration 16/1000 | Loss: 0.00008167
Iteration 17/1000 | Loss: 0.00004319
Iteration 18/1000 | Loss: 0.00010755
Iteration 19/1000 | Loss: 0.00072208
Iteration 20/1000 | Loss: 0.00044223
Iteration 21/1000 | Loss: 0.00055682
Iteration 22/1000 | Loss: 0.00024787
Iteration 23/1000 | Loss: 0.00004787
Iteration 24/1000 | Loss: 0.00008110
Iteration 25/1000 | Loss: 0.00009999
Iteration 26/1000 | Loss: 0.00003583
Iteration 27/1000 | Loss: 0.00004830
Iteration 28/1000 | Loss: 0.00022863
Iteration 29/1000 | Loss: 0.00022788
Iteration 30/1000 | Loss: 0.00006628
Iteration 31/1000 | Loss: 0.00004030
Iteration 32/1000 | Loss: 0.00003446
Iteration 33/1000 | Loss: 0.00005085
Iteration 34/1000 | Loss: 0.00003303
Iteration 35/1000 | Loss: 0.00003158
Iteration 36/1000 | Loss: 0.00002852
Iteration 37/1000 | Loss: 0.00002795
Iteration 38/1000 | Loss: 0.00002743
Iteration 39/1000 | Loss: 0.00002795
Iteration 40/1000 | Loss: 0.00002714
Iteration 41/1000 | Loss: 0.00002653
Iteration 42/1000 | Loss: 0.00002627
Iteration 43/1000 | Loss: 0.00002611
Iteration 44/1000 | Loss: 0.00004722
Iteration 45/1000 | Loss: 0.00002587
Iteration 46/1000 | Loss: 0.00002576
Iteration 47/1000 | Loss: 0.00002572
Iteration 48/1000 | Loss: 0.00002572
Iteration 49/1000 | Loss: 0.00002572
Iteration 50/1000 | Loss: 0.00002572
Iteration 51/1000 | Loss: 0.00002572
Iteration 52/1000 | Loss: 0.00002572
Iteration 53/1000 | Loss: 0.00002572
Iteration 54/1000 | Loss: 0.00002572
Iteration 55/1000 | Loss: 0.00002572
Iteration 56/1000 | Loss: 0.00002571
Iteration 57/1000 | Loss: 0.00002571
Iteration 58/1000 | Loss: 0.00002570
Iteration 59/1000 | Loss: 0.00002570
Iteration 60/1000 | Loss: 0.00002570
Iteration 61/1000 | Loss: 0.00002570
Iteration 62/1000 | Loss: 0.00002570
Iteration 63/1000 | Loss: 0.00002569
Iteration 64/1000 | Loss: 0.00002569
Iteration 65/1000 | Loss: 0.00002569
Iteration 66/1000 | Loss: 0.00002569
Iteration 67/1000 | Loss: 0.00002569
Iteration 68/1000 | Loss: 0.00002569
Iteration 69/1000 | Loss: 0.00002569
Iteration 70/1000 | Loss: 0.00002569
Iteration 71/1000 | Loss: 0.00002568
Iteration 72/1000 | Loss: 0.00002568
Iteration 73/1000 | Loss: 0.00002568
Iteration 74/1000 | Loss: 0.00002568
Iteration 75/1000 | Loss: 0.00002568
Iteration 76/1000 | Loss: 0.00002568
Iteration 77/1000 | Loss: 0.00002568
Iteration 78/1000 | Loss: 0.00002568
Iteration 79/1000 | Loss: 0.00002567
Iteration 80/1000 | Loss: 0.00002567
Iteration 81/1000 | Loss: 0.00002567
Iteration 82/1000 | Loss: 0.00002567
Iteration 83/1000 | Loss: 0.00002566
Iteration 84/1000 | Loss: 0.00002566
Iteration 85/1000 | Loss: 0.00002566
Iteration 86/1000 | Loss: 0.00002566
Iteration 87/1000 | Loss: 0.00002566
Iteration 88/1000 | Loss: 0.00002566
Iteration 89/1000 | Loss: 0.00002566
Iteration 90/1000 | Loss: 0.00002565
Iteration 91/1000 | Loss: 0.00002565
Iteration 92/1000 | Loss: 0.00002565
Iteration 93/1000 | Loss: 0.00002564
Iteration 94/1000 | Loss: 0.00002564
Iteration 95/1000 | Loss: 0.00002564
Iteration 96/1000 | Loss: 0.00002564
Iteration 97/1000 | Loss: 0.00002564
Iteration 98/1000 | Loss: 0.00002564
Iteration 99/1000 | Loss: 0.00002563
Iteration 100/1000 | Loss: 0.00002563
Iteration 101/1000 | Loss: 0.00002563
Iteration 102/1000 | Loss: 0.00002563
Iteration 103/1000 | Loss: 0.00002563
Iteration 104/1000 | Loss: 0.00002562
Iteration 105/1000 | Loss: 0.00002562
Iteration 106/1000 | Loss: 0.00002562
Iteration 107/1000 | Loss: 0.00002562
Iteration 108/1000 | Loss: 0.00002562
Iteration 109/1000 | Loss: 0.00002561
Iteration 110/1000 | Loss: 0.00002561
Iteration 111/1000 | Loss: 0.00002561
Iteration 112/1000 | Loss: 0.00002561
Iteration 113/1000 | Loss: 0.00002561
Iteration 114/1000 | Loss: 0.00002561
Iteration 115/1000 | Loss: 0.00002560
Iteration 116/1000 | Loss: 0.00002560
Iteration 117/1000 | Loss: 0.00004014
Iteration 118/1000 | Loss: 0.00002560
Iteration 119/1000 | Loss: 0.00002558
Iteration 120/1000 | Loss: 0.00002558
Iteration 121/1000 | Loss: 0.00002557
Iteration 122/1000 | Loss: 0.00002557
Iteration 123/1000 | Loss: 0.00002557
Iteration 124/1000 | Loss: 0.00002557
Iteration 125/1000 | Loss: 0.00002557
Iteration 126/1000 | Loss: 0.00002557
Iteration 127/1000 | Loss: 0.00002557
Iteration 128/1000 | Loss: 0.00002557
Iteration 129/1000 | Loss: 0.00002557
Iteration 130/1000 | Loss: 0.00002557
Iteration 131/1000 | Loss: 0.00002557
Iteration 132/1000 | Loss: 0.00002557
Iteration 133/1000 | Loss: 0.00002557
Iteration 134/1000 | Loss: 0.00002557
Iteration 135/1000 | Loss: 0.00002557
Iteration 136/1000 | Loss: 0.00002557
Iteration 137/1000 | Loss: 0.00002557
Iteration 138/1000 | Loss: 0.00002557
Iteration 139/1000 | Loss: 0.00002556
Iteration 140/1000 | Loss: 0.00002556
Iteration 141/1000 | Loss: 0.00002556
Iteration 142/1000 | Loss: 0.00002556
Iteration 143/1000 | Loss: 0.00002556
Iteration 144/1000 | Loss: 0.00002556
Iteration 145/1000 | Loss: 0.00002556
Iteration 146/1000 | Loss: 0.00002555
Iteration 147/1000 | Loss: 0.00002555
Iteration 148/1000 | Loss: 0.00002555
Iteration 149/1000 | Loss: 0.00002555
Iteration 150/1000 | Loss: 0.00002555
Iteration 151/1000 | Loss: 0.00002554
Iteration 152/1000 | Loss: 0.00002554
Iteration 153/1000 | Loss: 0.00002554
Iteration 154/1000 | Loss: 0.00002554
Iteration 155/1000 | Loss: 0.00002554
Iteration 156/1000 | Loss: 0.00002554
Iteration 157/1000 | Loss: 0.00002554
Iteration 158/1000 | Loss: 0.00002554
Iteration 159/1000 | Loss: 0.00002554
Iteration 160/1000 | Loss: 0.00002553
Iteration 161/1000 | Loss: 0.00002553
Iteration 162/1000 | Loss: 0.00002553
Iteration 163/1000 | Loss: 0.00002553
Iteration 164/1000 | Loss: 0.00002553
Iteration 165/1000 | Loss: 0.00002553
Iteration 166/1000 | Loss: 0.00002553
Iteration 167/1000 | Loss: 0.00002553
Iteration 168/1000 | Loss: 0.00002553
Iteration 169/1000 | Loss: 0.00002552
Iteration 170/1000 | Loss: 0.00002552
Iteration 171/1000 | Loss: 0.00002552
Iteration 172/1000 | Loss: 0.00002552
Iteration 173/1000 | Loss: 0.00002552
Iteration 174/1000 | Loss: 0.00002552
Iteration 175/1000 | Loss: 0.00002552
Iteration 176/1000 | Loss: 0.00002552
Iteration 177/1000 | Loss: 0.00002551
Iteration 178/1000 | Loss: 0.00002551
Iteration 179/1000 | Loss: 0.00002551
Iteration 180/1000 | Loss: 0.00002551
Iteration 181/1000 | Loss: 0.00002551
Iteration 182/1000 | Loss: 0.00002551
Iteration 183/1000 | Loss: 0.00002551
Iteration 184/1000 | Loss: 0.00002551
Iteration 185/1000 | Loss: 0.00002550
Iteration 186/1000 | Loss: 0.00002550
Iteration 187/1000 | Loss: 0.00002550
Iteration 188/1000 | Loss: 0.00002550
Iteration 189/1000 | Loss: 0.00002550
Iteration 190/1000 | Loss: 0.00002550
Iteration 191/1000 | Loss: 0.00002550
Iteration 192/1000 | Loss: 0.00002549
Iteration 193/1000 | Loss: 0.00002549
Iteration 194/1000 | Loss: 0.00002549
Iteration 195/1000 | Loss: 0.00002549
Iteration 196/1000 | Loss: 0.00002549
Iteration 197/1000 | Loss: 0.00002549
Iteration 198/1000 | Loss: 0.00002549
Iteration 199/1000 | Loss: 0.00002548
Iteration 200/1000 | Loss: 0.00002548
Iteration 201/1000 | Loss: 0.00002548
Iteration 202/1000 | Loss: 0.00002548
Iteration 203/1000 | Loss: 0.00002548
Iteration 204/1000 | Loss: 0.00002548
Iteration 205/1000 | Loss: 0.00002548
Iteration 206/1000 | Loss: 0.00002548
Iteration 207/1000 | Loss: 0.00002548
Iteration 208/1000 | Loss: 0.00002548
Iteration 209/1000 | Loss: 0.00002548
Iteration 210/1000 | Loss: 0.00002548
Iteration 211/1000 | Loss: 0.00002548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.5477720555500127e-05, 2.5477720555500127e-05, 2.5477720555500127e-05, 2.5477720555500127e-05, 2.5477720555500127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5477720555500127e-05

Optimization complete. Final v2v error: 4.184056282043457 mm

Highest mean error: 4.8356475830078125 mm for frame 181

Lowest mean error: 3.890524387359619 mm for frame 162

Saving results

Total time: 139.50406765937805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890707
Iteration 2/25 | Loss: 0.00108936
Iteration 3/25 | Loss: 0.00081607
Iteration 4/25 | Loss: 0.00077500
Iteration 5/25 | Loss: 0.00075945
Iteration 6/25 | Loss: 0.00075195
Iteration 7/25 | Loss: 0.00075375
Iteration 8/25 | Loss: 0.00074740
Iteration 9/25 | Loss: 0.00075928
Iteration 10/25 | Loss: 0.00073982
Iteration 11/25 | Loss: 0.00073664
Iteration 12/25 | Loss: 0.00073496
Iteration 13/25 | Loss: 0.00073476
Iteration 14/25 | Loss: 0.00073459
Iteration 15/25 | Loss: 0.00073434
Iteration 16/25 | Loss: 0.00073408
Iteration 17/25 | Loss: 0.00073388
Iteration 18/25 | Loss: 0.00073374
Iteration 19/25 | Loss: 0.00073372
Iteration 20/25 | Loss: 0.00073372
Iteration 21/25 | Loss: 0.00073372
Iteration 22/25 | Loss: 0.00073372
Iteration 23/25 | Loss: 0.00073371
Iteration 24/25 | Loss: 0.00073371
Iteration 25/25 | Loss: 0.00073371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68614078
Iteration 2/25 | Loss: 0.00083138
Iteration 3/25 | Loss: 0.00083137
Iteration 4/25 | Loss: 0.00083137
Iteration 5/25 | Loss: 0.00083137
Iteration 6/25 | Loss: 0.00083137
Iteration 7/25 | Loss: 0.00083137
Iteration 8/25 | Loss: 0.00083137
Iteration 9/25 | Loss: 0.00083137
Iteration 10/25 | Loss: 0.00083137
Iteration 11/25 | Loss: 0.00083137
Iteration 12/25 | Loss: 0.00083137
Iteration 13/25 | Loss: 0.00083137
Iteration 14/25 | Loss: 0.00083137
Iteration 15/25 | Loss: 0.00083137
Iteration 16/25 | Loss: 0.00083137
Iteration 17/25 | Loss: 0.00083137
Iteration 18/25 | Loss: 0.00083137
Iteration 19/25 | Loss: 0.00083137
Iteration 20/25 | Loss: 0.00083137
Iteration 21/25 | Loss: 0.00083137
Iteration 22/25 | Loss: 0.00083137
Iteration 23/25 | Loss: 0.00083137
Iteration 24/25 | Loss: 0.00083137
Iteration 25/25 | Loss: 0.00083137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008313709404319525, 0.0008313709404319525, 0.0008313709404319525, 0.0008313709404319525, 0.0008313709404319525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008313709404319525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083137
Iteration 2/1000 | Loss: 0.00003078
Iteration 3/1000 | Loss: 0.00002135
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001833
Iteration 6/1000 | Loss: 0.00001745
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001677
Iteration 9/1000 | Loss: 0.00001649
Iteration 10/1000 | Loss: 0.00001629
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001612
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00034529
Iteration 16/1000 | Loss: 0.00002233
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001687
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001468
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001396
Iteration 27/1000 | Loss: 0.00001380
Iteration 28/1000 | Loss: 0.00001378
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001376
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001374
Iteration 34/1000 | Loss: 0.00001373
Iteration 35/1000 | Loss: 0.00001368
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001354
Iteration 49/1000 | Loss: 0.00001354
Iteration 50/1000 | Loss: 0.00001354
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001352
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001352
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001345
Iteration 97/1000 | Loss: 0.00001345
Iteration 98/1000 | Loss: 0.00001345
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001345
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001345
Iteration 105/1000 | Loss: 0.00001345
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001345
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001345
Iteration 113/1000 | Loss: 0.00001344
Iteration 114/1000 | Loss: 0.00001344
Iteration 115/1000 | Loss: 0.00001344
Iteration 116/1000 | Loss: 0.00001344
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001344
Iteration 125/1000 | Loss: 0.00001344
Iteration 126/1000 | Loss: 0.00001344
Iteration 127/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.344386600976577e-05, 1.344386600976577e-05, 1.344386600976577e-05, 1.344386600976577e-05, 1.344386600976577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.344386600976577e-05

Optimization complete. Final v2v error: 3.0278847217559814 mm

Highest mean error: 4.590795516967773 mm for frame 85

Lowest mean error: 2.559715986251831 mm for frame 128

Saving results

Total time: 69.05729675292969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818455
Iteration 2/25 | Loss: 0.00118161
Iteration 3/25 | Loss: 0.00092491
Iteration 4/25 | Loss: 0.00085701
Iteration 5/25 | Loss: 0.00083579
Iteration 6/25 | Loss: 0.00081758
Iteration 7/25 | Loss: 0.00080566
Iteration 8/25 | Loss: 0.00079814
Iteration 9/25 | Loss: 0.00079478
Iteration 10/25 | Loss: 0.00080347
Iteration 11/25 | Loss: 0.00081803
Iteration 12/25 | Loss: 0.00082617
Iteration 13/25 | Loss: 0.00081394
Iteration 14/25 | Loss: 0.00080465
Iteration 15/25 | Loss: 0.00078998
Iteration 16/25 | Loss: 0.00078443
Iteration 17/25 | Loss: 0.00078061
Iteration 18/25 | Loss: 0.00077514
Iteration 19/25 | Loss: 0.00077339
Iteration 20/25 | Loss: 0.00077274
Iteration 21/25 | Loss: 0.00077240
Iteration 22/25 | Loss: 0.00077219
Iteration 23/25 | Loss: 0.00077210
Iteration 24/25 | Loss: 0.00077209
Iteration 25/25 | Loss: 0.00077209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60363209
Iteration 2/25 | Loss: 0.00092397
Iteration 3/25 | Loss: 0.00092395
Iteration 4/25 | Loss: 0.00092395
Iteration 5/25 | Loss: 0.00092395
Iteration 6/25 | Loss: 0.00092395
Iteration 7/25 | Loss: 0.00092395
Iteration 8/25 | Loss: 0.00092395
Iteration 9/25 | Loss: 0.00092395
Iteration 10/25 | Loss: 0.00092395
Iteration 11/25 | Loss: 0.00092395
Iteration 12/25 | Loss: 0.00092395
Iteration 13/25 | Loss: 0.00092395
Iteration 14/25 | Loss: 0.00092395
Iteration 15/25 | Loss: 0.00092395
Iteration 16/25 | Loss: 0.00092395
Iteration 17/25 | Loss: 0.00092395
Iteration 18/25 | Loss: 0.00092395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009239476639777422, 0.0009239476639777422, 0.0009239476639777422, 0.0009239476639777422, 0.0009239476639777422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009239476639777422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092395
Iteration 2/1000 | Loss: 0.00030503
Iteration 3/1000 | Loss: 0.00003552
Iteration 4/1000 | Loss: 0.00003084
Iteration 5/1000 | Loss: 0.00002711
Iteration 6/1000 | Loss: 0.00020723
Iteration 7/1000 | Loss: 0.00018486
Iteration 8/1000 | Loss: 0.00033164
Iteration 9/1000 | Loss: 0.00033871
Iteration 10/1000 | Loss: 0.00007627
Iteration 11/1000 | Loss: 0.00015791
Iteration 12/1000 | Loss: 0.00004169
Iteration 13/1000 | Loss: 0.00003213
Iteration 14/1000 | Loss: 0.00002774
Iteration 15/1000 | Loss: 0.00002584
Iteration 16/1000 | Loss: 0.00002515
Iteration 17/1000 | Loss: 0.00002441
Iteration 18/1000 | Loss: 0.00002368
Iteration 19/1000 | Loss: 0.00002318
Iteration 20/1000 | Loss: 0.00002285
Iteration 21/1000 | Loss: 0.00025588
Iteration 22/1000 | Loss: 0.00044854
Iteration 23/1000 | Loss: 0.00016886
Iteration 24/1000 | Loss: 0.00012450
Iteration 25/1000 | Loss: 0.00007446
Iteration 26/1000 | Loss: 0.00030250
Iteration 27/1000 | Loss: 0.00029640
Iteration 28/1000 | Loss: 0.00007140
Iteration 29/1000 | Loss: 0.00021453
Iteration 30/1000 | Loss: 0.00041222
Iteration 31/1000 | Loss: 0.00026781
Iteration 32/1000 | Loss: 0.00019304
Iteration 33/1000 | Loss: 0.00020627
Iteration 34/1000 | Loss: 0.00062987
Iteration 35/1000 | Loss: 0.00028475
Iteration 36/1000 | Loss: 0.00019781
Iteration 37/1000 | Loss: 0.00014542
Iteration 38/1000 | Loss: 0.00017159
Iteration 39/1000 | Loss: 0.00009675
Iteration 40/1000 | Loss: 0.00014510
Iteration 41/1000 | Loss: 0.00019864
Iteration 42/1000 | Loss: 0.00016306
Iteration 43/1000 | Loss: 0.00019579
Iteration 44/1000 | Loss: 0.00015408
Iteration 45/1000 | Loss: 0.00008876
Iteration 46/1000 | Loss: 0.00002470
Iteration 47/1000 | Loss: 0.00002318
Iteration 48/1000 | Loss: 0.00017556
Iteration 49/1000 | Loss: 0.00021242
Iteration 50/1000 | Loss: 0.00031255
Iteration 51/1000 | Loss: 0.00038738
Iteration 52/1000 | Loss: 0.00023457
Iteration 53/1000 | Loss: 0.00011149
Iteration 54/1000 | Loss: 0.00012642
Iteration 55/1000 | Loss: 0.00013672
Iteration 56/1000 | Loss: 0.00007326
Iteration 57/1000 | Loss: 0.00014513
Iteration 58/1000 | Loss: 0.00010520
Iteration 59/1000 | Loss: 0.00017640
Iteration 60/1000 | Loss: 0.00010663
Iteration 61/1000 | Loss: 0.00026319
Iteration 62/1000 | Loss: 0.00042802
Iteration 63/1000 | Loss: 0.00011140
Iteration 64/1000 | Loss: 0.00030164
Iteration 65/1000 | Loss: 0.00028165
Iteration 66/1000 | Loss: 0.00026118
Iteration 67/1000 | Loss: 0.00033340
Iteration 68/1000 | Loss: 0.00046521
Iteration 69/1000 | Loss: 0.00009800
Iteration 70/1000 | Loss: 0.00030722
Iteration 71/1000 | Loss: 0.00039840
Iteration 72/1000 | Loss: 0.00011856
Iteration 73/1000 | Loss: 0.00012404
Iteration 74/1000 | Loss: 0.00023605
Iteration 75/1000 | Loss: 0.00031804
Iteration 76/1000 | Loss: 0.00009832
Iteration 77/1000 | Loss: 0.00010276
Iteration 78/1000 | Loss: 0.00004582
Iteration 79/1000 | Loss: 0.00009608
Iteration 80/1000 | Loss: 0.00011489
Iteration 81/1000 | Loss: 0.00007515
Iteration 82/1000 | Loss: 0.00003620
Iteration 83/1000 | Loss: 0.00015650
Iteration 84/1000 | Loss: 0.00004016
Iteration 85/1000 | Loss: 0.00002978
Iteration 86/1000 | Loss: 0.00002594
Iteration 87/1000 | Loss: 0.00002397
Iteration 88/1000 | Loss: 0.00002278
Iteration 89/1000 | Loss: 0.00002177
Iteration 90/1000 | Loss: 0.00002102
Iteration 91/1000 | Loss: 0.00002052
Iteration 92/1000 | Loss: 0.00002002
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001946
Iteration 95/1000 | Loss: 0.00001926
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001921
Iteration 101/1000 | Loss: 0.00001921
Iteration 102/1000 | Loss: 0.00001920
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00001920
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001917
Iteration 108/1000 | Loss: 0.00001916
Iteration 109/1000 | Loss: 0.00001915
Iteration 110/1000 | Loss: 0.00001911
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001908
Iteration 113/1000 | Loss: 0.00001908
Iteration 114/1000 | Loss: 0.00001907
Iteration 115/1000 | Loss: 0.00001907
Iteration 116/1000 | Loss: 0.00001906
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001905
Iteration 119/1000 | Loss: 0.00001905
Iteration 120/1000 | Loss: 0.00001904
Iteration 121/1000 | Loss: 0.00001904
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001902
Iteration 126/1000 | Loss: 0.00001902
Iteration 127/1000 | Loss: 0.00001901
Iteration 128/1000 | Loss: 0.00001901
Iteration 129/1000 | Loss: 0.00001901
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001900
Iteration 132/1000 | Loss: 0.00001900
Iteration 133/1000 | Loss: 0.00001899
Iteration 134/1000 | Loss: 0.00001899
Iteration 135/1000 | Loss: 0.00001899
Iteration 136/1000 | Loss: 0.00001899
Iteration 137/1000 | Loss: 0.00001899
Iteration 138/1000 | Loss: 0.00001899
Iteration 139/1000 | Loss: 0.00001899
Iteration 140/1000 | Loss: 0.00001898
Iteration 141/1000 | Loss: 0.00001898
Iteration 142/1000 | Loss: 0.00001898
Iteration 143/1000 | Loss: 0.00001898
Iteration 144/1000 | Loss: 0.00001897
Iteration 145/1000 | Loss: 0.00001897
Iteration 146/1000 | Loss: 0.00001896
Iteration 147/1000 | Loss: 0.00001896
Iteration 148/1000 | Loss: 0.00001896
Iteration 149/1000 | Loss: 0.00001895
Iteration 150/1000 | Loss: 0.00001895
Iteration 151/1000 | Loss: 0.00001895
Iteration 152/1000 | Loss: 0.00001894
Iteration 153/1000 | Loss: 0.00001894
Iteration 154/1000 | Loss: 0.00001894
Iteration 155/1000 | Loss: 0.00001894
Iteration 156/1000 | Loss: 0.00001894
Iteration 157/1000 | Loss: 0.00001894
Iteration 158/1000 | Loss: 0.00001894
Iteration 159/1000 | Loss: 0.00001893
Iteration 160/1000 | Loss: 0.00001893
Iteration 161/1000 | Loss: 0.00001893
Iteration 162/1000 | Loss: 0.00001893
Iteration 163/1000 | Loss: 0.00001893
Iteration 164/1000 | Loss: 0.00001893
Iteration 165/1000 | Loss: 0.00001893
Iteration 166/1000 | Loss: 0.00001893
Iteration 167/1000 | Loss: 0.00001893
Iteration 168/1000 | Loss: 0.00001892
Iteration 169/1000 | Loss: 0.00001892
Iteration 170/1000 | Loss: 0.00001892
Iteration 171/1000 | Loss: 0.00001892
Iteration 172/1000 | Loss: 0.00001892
Iteration 173/1000 | Loss: 0.00001892
Iteration 174/1000 | Loss: 0.00001891
Iteration 175/1000 | Loss: 0.00001891
Iteration 176/1000 | Loss: 0.00001891
Iteration 177/1000 | Loss: 0.00001891
Iteration 178/1000 | Loss: 0.00001891
Iteration 179/1000 | Loss: 0.00001891
Iteration 180/1000 | Loss: 0.00001891
Iteration 181/1000 | Loss: 0.00001891
Iteration 182/1000 | Loss: 0.00001891
Iteration 183/1000 | Loss: 0.00001891
Iteration 184/1000 | Loss: 0.00001891
Iteration 185/1000 | Loss: 0.00001891
Iteration 186/1000 | Loss: 0.00001891
Iteration 187/1000 | Loss: 0.00001891
Iteration 188/1000 | Loss: 0.00001891
Iteration 189/1000 | Loss: 0.00001891
Iteration 190/1000 | Loss: 0.00001891
Iteration 191/1000 | Loss: 0.00001891
Iteration 192/1000 | Loss: 0.00001891
Iteration 193/1000 | Loss: 0.00001891
Iteration 194/1000 | Loss: 0.00001891
Iteration 195/1000 | Loss: 0.00001891
Iteration 196/1000 | Loss: 0.00001891
Iteration 197/1000 | Loss: 0.00001891
Iteration 198/1000 | Loss: 0.00001891
Iteration 199/1000 | Loss: 0.00001891
Iteration 200/1000 | Loss: 0.00001891
Iteration 201/1000 | Loss: 0.00001891
Iteration 202/1000 | Loss: 0.00001891
Iteration 203/1000 | Loss: 0.00001891
Iteration 204/1000 | Loss: 0.00001891
Iteration 205/1000 | Loss: 0.00001891
Iteration 206/1000 | Loss: 0.00001891
Iteration 207/1000 | Loss: 0.00001891
Iteration 208/1000 | Loss: 0.00001891
Iteration 209/1000 | Loss: 0.00001891
Iteration 210/1000 | Loss: 0.00001891
Iteration 211/1000 | Loss: 0.00001891
Iteration 212/1000 | Loss: 0.00001891
Iteration 213/1000 | Loss: 0.00001891
Iteration 214/1000 | Loss: 0.00001891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.8911388906417415e-05, 1.8911388906417415e-05, 1.8911388906417415e-05, 1.8911388906417415e-05, 1.8911388906417415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8911388906417415e-05

Optimization complete. Final v2v error: 3.521056652069092 mm

Highest mean error: 5.2899885177612305 mm for frame 219

Lowest mean error: 2.9256372451782227 mm for frame 54

Saving results

Total time: 208.74531602859497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114337
Iteration 2/25 | Loss: 0.00276575
Iteration 3/25 | Loss: 0.00128611
Iteration 4/25 | Loss: 0.00107353
Iteration 5/25 | Loss: 0.00099803
Iteration 6/25 | Loss: 0.00097290
Iteration 7/25 | Loss: 0.00091439
Iteration 8/25 | Loss: 0.00087406
Iteration 9/25 | Loss: 0.00084383
Iteration 10/25 | Loss: 0.00083642
Iteration 11/25 | Loss: 0.00083577
Iteration 12/25 | Loss: 0.00083391
Iteration 13/25 | Loss: 0.00082781
Iteration 14/25 | Loss: 0.00082743
Iteration 15/25 | Loss: 0.00083841
Iteration 16/25 | Loss: 0.00082077
Iteration 17/25 | Loss: 0.00081990
Iteration 18/25 | Loss: 0.00082923
Iteration 19/25 | Loss: 0.00082531
Iteration 20/25 | Loss: 0.00081566
Iteration 21/25 | Loss: 0.00081724
Iteration 22/25 | Loss: 0.00081483
Iteration 23/25 | Loss: 0.00081074
Iteration 24/25 | Loss: 0.00081292
Iteration 25/25 | Loss: 0.00080869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52106333
Iteration 2/25 | Loss: 0.00124533
Iteration 3/25 | Loss: 0.00124532
Iteration 4/25 | Loss: 0.00124532
Iteration 5/25 | Loss: 0.00124532
Iteration 6/25 | Loss: 0.00124532
Iteration 7/25 | Loss: 0.00124532
Iteration 8/25 | Loss: 0.00124532
Iteration 9/25 | Loss: 0.00124532
Iteration 10/25 | Loss: 0.00124532
Iteration 11/25 | Loss: 0.00124532
Iteration 12/25 | Loss: 0.00124532
Iteration 13/25 | Loss: 0.00124532
Iteration 14/25 | Loss: 0.00124532
Iteration 15/25 | Loss: 0.00124532
Iteration 16/25 | Loss: 0.00124532
Iteration 17/25 | Loss: 0.00124532
Iteration 18/25 | Loss: 0.00124532
Iteration 19/25 | Loss: 0.00124532
Iteration 20/25 | Loss: 0.00124532
Iteration 21/25 | Loss: 0.00124532
Iteration 22/25 | Loss: 0.00124532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012453210074454546, 0.0012453210074454546, 0.0012453210074454546, 0.0012453210074454546, 0.0012453210074454546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012453210074454546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124532
Iteration 2/1000 | Loss: 0.00011069
Iteration 3/1000 | Loss: 0.00014109
Iteration 4/1000 | Loss: 0.00005660
Iteration 5/1000 | Loss: 0.00019453
Iteration 6/1000 | Loss: 0.00058095
Iteration 7/1000 | Loss: 0.00085493
Iteration 8/1000 | Loss: 0.00109693
Iteration 9/1000 | Loss: 0.00016776
Iteration 10/1000 | Loss: 0.00027034
Iteration 11/1000 | Loss: 0.00092326
Iteration 12/1000 | Loss: 0.00029423
Iteration 13/1000 | Loss: 0.00123387
Iteration 14/1000 | Loss: 0.00059918
Iteration 15/1000 | Loss: 0.00062995
Iteration 16/1000 | Loss: 0.00027095
Iteration 17/1000 | Loss: 0.00032450
Iteration 18/1000 | Loss: 0.00007829
Iteration 19/1000 | Loss: 0.00033433
Iteration 20/1000 | Loss: 0.00032076
Iteration 21/1000 | Loss: 0.00026767
Iteration 22/1000 | Loss: 0.00035938
Iteration 23/1000 | Loss: 0.00030626
Iteration 24/1000 | Loss: 0.00024473
Iteration 25/1000 | Loss: 0.00024430
Iteration 26/1000 | Loss: 0.00022854
Iteration 27/1000 | Loss: 0.00006235
Iteration 28/1000 | Loss: 0.00065231
Iteration 29/1000 | Loss: 0.00020241
Iteration 30/1000 | Loss: 0.00005281
Iteration 31/1000 | Loss: 0.00004573
Iteration 32/1000 | Loss: 0.00006552
Iteration 33/1000 | Loss: 0.00003962
Iteration 34/1000 | Loss: 0.00003629
Iteration 35/1000 | Loss: 0.00015519
Iteration 36/1000 | Loss: 0.00003847
Iteration 37/1000 | Loss: 0.00003260
Iteration 38/1000 | Loss: 0.00003088
Iteration 39/1000 | Loss: 0.00002993
Iteration 40/1000 | Loss: 0.00002893
Iteration 41/1000 | Loss: 0.00002816
Iteration 42/1000 | Loss: 0.00002773
Iteration 43/1000 | Loss: 0.00002735
Iteration 44/1000 | Loss: 0.00002824
Iteration 45/1000 | Loss: 0.00002673
Iteration 46/1000 | Loss: 0.00002634
Iteration 47/1000 | Loss: 0.00002605
Iteration 48/1000 | Loss: 0.00002573
Iteration 49/1000 | Loss: 0.00002670
Iteration 50/1000 | Loss: 0.00024132
Iteration 51/1000 | Loss: 0.00018642
Iteration 52/1000 | Loss: 0.00022161
Iteration 53/1000 | Loss: 0.00026453
Iteration 54/1000 | Loss: 0.00019608
Iteration 55/1000 | Loss: 0.00003538
Iteration 56/1000 | Loss: 0.00002963
Iteration 57/1000 | Loss: 0.00033709
Iteration 58/1000 | Loss: 0.00004258
Iteration 59/1000 | Loss: 0.00027245
Iteration 60/1000 | Loss: 0.00003984
Iteration 61/1000 | Loss: 0.00007574
Iteration 62/1000 | Loss: 0.00013398
Iteration 63/1000 | Loss: 0.00031533
Iteration 64/1000 | Loss: 0.00172252
Iteration 65/1000 | Loss: 0.00102517
Iteration 66/1000 | Loss: 0.00025535
Iteration 67/1000 | Loss: 0.00026466
Iteration 68/1000 | Loss: 0.00008331
Iteration 69/1000 | Loss: 0.00026966
Iteration 70/1000 | Loss: 0.00025602
Iteration 71/1000 | Loss: 0.00009755
Iteration 72/1000 | Loss: 0.00007769
Iteration 73/1000 | Loss: 0.00020657
Iteration 74/1000 | Loss: 0.00013410
Iteration 75/1000 | Loss: 0.00003187
Iteration 76/1000 | Loss: 0.00003010
Iteration 77/1000 | Loss: 0.00002891
Iteration 78/1000 | Loss: 0.00002752
Iteration 79/1000 | Loss: 0.00002598
Iteration 80/1000 | Loss: 0.00027233
Iteration 81/1000 | Loss: 0.00015250
Iteration 82/1000 | Loss: 0.00028772
Iteration 83/1000 | Loss: 0.00016202
Iteration 84/1000 | Loss: 0.00032488
Iteration 85/1000 | Loss: 0.00003127
Iteration 86/1000 | Loss: 0.00022916
Iteration 87/1000 | Loss: 0.00014731
Iteration 88/1000 | Loss: 0.00007715
Iteration 89/1000 | Loss: 0.00013153
Iteration 90/1000 | Loss: 0.00006598
Iteration 91/1000 | Loss: 0.00012309
Iteration 92/1000 | Loss: 0.00006871
Iteration 93/1000 | Loss: 0.00024749
Iteration 94/1000 | Loss: 0.00020001
Iteration 95/1000 | Loss: 0.00020896
Iteration 96/1000 | Loss: 0.00022199
Iteration 97/1000 | Loss: 0.00029270
Iteration 98/1000 | Loss: 0.00033695
Iteration 99/1000 | Loss: 0.00003147
Iteration 100/1000 | Loss: 0.00002940
Iteration 101/1000 | Loss: 0.00002797
Iteration 102/1000 | Loss: 0.00002754
Iteration 103/1000 | Loss: 0.00002584
Iteration 104/1000 | Loss: 0.00002469
Iteration 105/1000 | Loss: 0.00002389
Iteration 106/1000 | Loss: 0.00022714
Iteration 107/1000 | Loss: 0.00002431
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002159
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002008
Iteration 112/1000 | Loss: 0.00001986
Iteration 113/1000 | Loss: 0.00001982
Iteration 114/1000 | Loss: 0.00001962
Iteration 115/1000 | Loss: 0.00001958
Iteration 116/1000 | Loss: 0.00001956
Iteration 117/1000 | Loss: 0.00001955
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001940
Iteration 121/1000 | Loss: 0.00001939
Iteration 122/1000 | Loss: 0.00001936
Iteration 123/1000 | Loss: 0.00001936
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001935
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001935
Iteration 133/1000 | Loss: 0.00001934
Iteration 134/1000 | Loss: 0.00001933
Iteration 135/1000 | Loss: 0.00001931
Iteration 136/1000 | Loss: 0.00001931
Iteration 137/1000 | Loss: 0.00001931
Iteration 138/1000 | Loss: 0.00001930
Iteration 139/1000 | Loss: 0.00001930
Iteration 140/1000 | Loss: 0.00001930
Iteration 141/1000 | Loss: 0.00001929
Iteration 142/1000 | Loss: 0.00001929
Iteration 143/1000 | Loss: 0.00001929
Iteration 144/1000 | Loss: 0.00001929
Iteration 145/1000 | Loss: 0.00001928
Iteration 146/1000 | Loss: 0.00001928
Iteration 147/1000 | Loss: 0.00001927
Iteration 148/1000 | Loss: 0.00001927
Iteration 149/1000 | Loss: 0.00001926
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001925
Iteration 152/1000 | Loss: 0.00001924
Iteration 153/1000 | Loss: 0.00001924
Iteration 154/1000 | Loss: 0.00001924
Iteration 155/1000 | Loss: 0.00001924
Iteration 156/1000 | Loss: 0.00001923
Iteration 157/1000 | Loss: 0.00001923
Iteration 158/1000 | Loss: 0.00001923
Iteration 159/1000 | Loss: 0.00001923
Iteration 160/1000 | Loss: 0.00001923
Iteration 161/1000 | Loss: 0.00001923
Iteration 162/1000 | Loss: 0.00001923
Iteration 163/1000 | Loss: 0.00001922
Iteration 164/1000 | Loss: 0.00001922
Iteration 165/1000 | Loss: 0.00001922
Iteration 166/1000 | Loss: 0.00001922
Iteration 167/1000 | Loss: 0.00001922
Iteration 168/1000 | Loss: 0.00001922
Iteration 169/1000 | Loss: 0.00001922
Iteration 170/1000 | Loss: 0.00001922
Iteration 171/1000 | Loss: 0.00001922
Iteration 172/1000 | Loss: 0.00001922
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001922
Iteration 175/1000 | Loss: 0.00001922
Iteration 176/1000 | Loss: 0.00001922
Iteration 177/1000 | Loss: 0.00001922
Iteration 178/1000 | Loss: 0.00001922
Iteration 179/1000 | Loss: 0.00001921
Iteration 180/1000 | Loss: 0.00001921
Iteration 181/1000 | Loss: 0.00001921
Iteration 182/1000 | Loss: 0.00001921
Iteration 183/1000 | Loss: 0.00001921
Iteration 184/1000 | Loss: 0.00001921
Iteration 185/1000 | Loss: 0.00001921
Iteration 186/1000 | Loss: 0.00001921
Iteration 187/1000 | Loss: 0.00001921
Iteration 188/1000 | Loss: 0.00001921
Iteration 189/1000 | Loss: 0.00001921
Iteration 190/1000 | Loss: 0.00001921
Iteration 191/1000 | Loss: 0.00001921
Iteration 192/1000 | Loss: 0.00001921
Iteration 193/1000 | Loss: 0.00001921
Iteration 194/1000 | Loss: 0.00001921
Iteration 195/1000 | Loss: 0.00001921
Iteration 196/1000 | Loss: 0.00001921
Iteration 197/1000 | Loss: 0.00001921
Iteration 198/1000 | Loss: 0.00001921
Iteration 199/1000 | Loss: 0.00001921
Iteration 200/1000 | Loss: 0.00001921
Iteration 201/1000 | Loss: 0.00001921
Iteration 202/1000 | Loss: 0.00001921
Iteration 203/1000 | Loss: 0.00001921
Iteration 204/1000 | Loss: 0.00001921
Iteration 205/1000 | Loss: 0.00001921
Iteration 206/1000 | Loss: 0.00001921
Iteration 207/1000 | Loss: 0.00001921
Iteration 208/1000 | Loss: 0.00001921
Iteration 209/1000 | Loss: 0.00001921
Iteration 210/1000 | Loss: 0.00001921
Iteration 211/1000 | Loss: 0.00001921
Iteration 212/1000 | Loss: 0.00001921
Iteration 213/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.9209612219128758e-05, 1.9209612219128758e-05, 1.9209612219128758e-05, 1.9209612219128758e-05, 1.9209612219128758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9209612219128758e-05

Optimization complete. Final v2v error: 3.664820909500122 mm

Highest mean error: 5.202415943145752 mm for frame 71

Lowest mean error: 3.4272537231445312 mm for frame 22

Saving results

Total time: 222.66837453842163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532822
Iteration 2/25 | Loss: 0.00094264
Iteration 3/25 | Loss: 0.00079954
Iteration 4/25 | Loss: 0.00077306
Iteration 5/25 | Loss: 0.00076535
Iteration 6/25 | Loss: 0.00076375
Iteration 7/25 | Loss: 0.00076326
Iteration 8/25 | Loss: 0.00076326
Iteration 9/25 | Loss: 0.00076326
Iteration 10/25 | Loss: 0.00076326
Iteration 11/25 | Loss: 0.00076326
Iteration 12/25 | Loss: 0.00076326
Iteration 13/25 | Loss: 0.00076326
Iteration 14/25 | Loss: 0.00076326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000763259653467685, 0.000763259653467685, 0.000763259653467685, 0.000763259653467685, 0.000763259653467685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000763259653467685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30951226
Iteration 2/25 | Loss: 0.00084203
Iteration 3/25 | Loss: 0.00084203
Iteration 4/25 | Loss: 0.00084203
Iteration 5/25 | Loss: 0.00084202
Iteration 6/25 | Loss: 0.00084202
Iteration 7/25 | Loss: 0.00084202
Iteration 8/25 | Loss: 0.00084202
Iteration 9/25 | Loss: 0.00084202
Iteration 10/25 | Loss: 0.00084202
Iteration 11/25 | Loss: 0.00084202
Iteration 12/25 | Loss: 0.00084202
Iteration 13/25 | Loss: 0.00084202
Iteration 14/25 | Loss: 0.00084202
Iteration 15/25 | Loss: 0.00084202
Iteration 16/25 | Loss: 0.00084202
Iteration 17/25 | Loss: 0.00084202
Iteration 18/25 | Loss: 0.00084202
Iteration 19/25 | Loss: 0.00084202
Iteration 20/25 | Loss: 0.00084202
Iteration 21/25 | Loss: 0.00084202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000842022942379117, 0.000842022942379117, 0.000842022942379117, 0.000842022942379117, 0.000842022942379117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000842022942379117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084202
Iteration 2/1000 | Loss: 0.00002411
Iteration 3/1000 | Loss: 0.00001476
Iteration 4/1000 | Loss: 0.00001298
Iteration 5/1000 | Loss: 0.00001232
Iteration 6/1000 | Loss: 0.00001198
Iteration 7/1000 | Loss: 0.00001186
Iteration 8/1000 | Loss: 0.00001170
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001149
Iteration 14/1000 | Loss: 0.00001149
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001147
Iteration 17/1000 | Loss: 0.00001143
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001142
Iteration 20/1000 | Loss: 0.00001141
Iteration 21/1000 | Loss: 0.00001141
Iteration 22/1000 | Loss: 0.00001132
Iteration 23/1000 | Loss: 0.00001124
Iteration 24/1000 | Loss: 0.00001121
Iteration 25/1000 | Loss: 0.00001120
Iteration 26/1000 | Loss: 0.00001120
Iteration 27/1000 | Loss: 0.00001119
Iteration 28/1000 | Loss: 0.00001119
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001117
Iteration 32/1000 | Loss: 0.00001117
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001117
Iteration 36/1000 | Loss: 0.00001117
Iteration 37/1000 | Loss: 0.00001117
Iteration 38/1000 | Loss: 0.00001116
Iteration 39/1000 | Loss: 0.00001116
Iteration 40/1000 | Loss: 0.00001116
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001116
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001115
Iteration 49/1000 | Loss: 0.00001115
Iteration 50/1000 | Loss: 0.00001114
Iteration 51/1000 | Loss: 0.00001114
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001112
Iteration 56/1000 | Loss: 0.00001112
Iteration 57/1000 | Loss: 0.00001111
Iteration 58/1000 | Loss: 0.00001111
Iteration 59/1000 | Loss: 0.00001111
Iteration 60/1000 | Loss: 0.00001111
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001111
Iteration 63/1000 | Loss: 0.00001111
Iteration 64/1000 | Loss: 0.00001111
Iteration 65/1000 | Loss: 0.00001111
Iteration 66/1000 | Loss: 0.00001111
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001109
Iteration 85/1000 | Loss: 0.00001109
Iteration 86/1000 | Loss: 0.00001109
Iteration 87/1000 | Loss: 0.00001109
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001108
Iteration 94/1000 | Loss: 0.00001108
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001107
Iteration 98/1000 | Loss: 0.00001107
Iteration 99/1000 | Loss: 0.00001107
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001106
Iteration 104/1000 | Loss: 0.00001106
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001106
Iteration 107/1000 | Loss: 0.00001106
Iteration 108/1000 | Loss: 0.00001106
Iteration 109/1000 | Loss: 0.00001105
Iteration 110/1000 | Loss: 0.00001105
Iteration 111/1000 | Loss: 0.00001105
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001104
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001103
Iteration 119/1000 | Loss: 0.00001103
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001102
Iteration 122/1000 | Loss: 0.00001102
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001101
Iteration 129/1000 | Loss: 0.00001101
Iteration 130/1000 | Loss: 0.00001101
Iteration 131/1000 | Loss: 0.00001101
Iteration 132/1000 | Loss: 0.00001101
Iteration 133/1000 | Loss: 0.00001101
Iteration 134/1000 | Loss: 0.00001101
Iteration 135/1000 | Loss: 0.00001101
Iteration 136/1000 | Loss: 0.00001101
Iteration 137/1000 | Loss: 0.00001101
Iteration 138/1000 | Loss: 0.00001101
Iteration 139/1000 | Loss: 0.00001101
Iteration 140/1000 | Loss: 0.00001101
Iteration 141/1000 | Loss: 0.00001101
Iteration 142/1000 | Loss: 0.00001101
Iteration 143/1000 | Loss: 0.00001101
Iteration 144/1000 | Loss: 0.00001101
Iteration 145/1000 | Loss: 0.00001101
Iteration 146/1000 | Loss: 0.00001101
Iteration 147/1000 | Loss: 0.00001101
Iteration 148/1000 | Loss: 0.00001101
Iteration 149/1000 | Loss: 0.00001101
Iteration 150/1000 | Loss: 0.00001101
Iteration 151/1000 | Loss: 0.00001101
Iteration 152/1000 | Loss: 0.00001101
Iteration 153/1000 | Loss: 0.00001101
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Iteration 185/1000 | Loss: 0.00001101
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1011873539246153e-05, 1.1011873539246153e-05, 1.1011873539246153e-05, 1.1011873539246153e-05, 1.1011873539246153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1011873539246153e-05

Optimization complete. Final v2v error: 2.8008065223693848 mm

Highest mean error: 3.188572883605957 mm for frame 141

Lowest mean error: 2.7223262786865234 mm for frame 107

Saving results

Total time: 34.6947820186615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00721303
Iteration 2/25 | Loss: 0.00138860
Iteration 3/25 | Loss: 0.00088625
Iteration 4/25 | Loss: 0.00081821
Iteration 5/25 | Loss: 0.00079110
Iteration 6/25 | Loss: 0.00077923
Iteration 7/25 | Loss: 0.00079166
Iteration 8/25 | Loss: 0.00079213
Iteration 9/25 | Loss: 0.00078347
Iteration 10/25 | Loss: 0.00076765
Iteration 11/25 | Loss: 0.00075763
Iteration 12/25 | Loss: 0.00075164
Iteration 13/25 | Loss: 0.00074762
Iteration 14/25 | Loss: 0.00074747
Iteration 15/25 | Loss: 0.00074499
Iteration 16/25 | Loss: 0.00074589
Iteration 17/25 | Loss: 0.00074383
Iteration 18/25 | Loss: 0.00074374
Iteration 19/25 | Loss: 0.00074364
Iteration 20/25 | Loss: 0.00074460
Iteration 21/25 | Loss: 0.00074330
Iteration 22/25 | Loss: 0.00074391
Iteration 23/25 | Loss: 0.00074390
Iteration 24/25 | Loss: 0.00074377
Iteration 25/25 | Loss: 0.00074376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.20719099
Iteration 2/25 | Loss: 0.00091146
Iteration 3/25 | Loss: 0.00091143
Iteration 4/25 | Loss: 0.00091143
Iteration 5/25 | Loss: 0.00091143
Iteration 6/25 | Loss: 0.00091143
Iteration 7/25 | Loss: 0.00091143
Iteration 8/25 | Loss: 0.00091143
Iteration 9/25 | Loss: 0.00091143
Iteration 10/25 | Loss: 0.00091143
Iteration 11/25 | Loss: 0.00091143
Iteration 12/25 | Loss: 0.00091143
Iteration 13/25 | Loss: 0.00091143
Iteration 14/25 | Loss: 0.00091143
Iteration 15/25 | Loss: 0.00091143
Iteration 16/25 | Loss: 0.00091143
Iteration 17/25 | Loss: 0.00091143
Iteration 18/25 | Loss: 0.00091143
Iteration 19/25 | Loss: 0.00091143
Iteration 20/25 | Loss: 0.00091143
Iteration 21/25 | Loss: 0.00091143
Iteration 22/25 | Loss: 0.00091143
Iteration 23/25 | Loss: 0.00091143
Iteration 24/25 | Loss: 0.00091143
Iteration 25/25 | Loss: 0.00091143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091143
Iteration 2/1000 | Loss: 0.00003585
Iteration 3/1000 | Loss: 0.00002146
Iteration 4/1000 | Loss: 0.00002501
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00002985
Iteration 7/1000 | Loss: 0.00002867
Iteration 8/1000 | Loss: 0.00002083
Iteration 9/1000 | Loss: 0.00002692
Iteration 10/1000 | Loss: 0.00002845
Iteration 11/1000 | Loss: 0.00003006
Iteration 12/1000 | Loss: 0.00002807
Iteration 13/1000 | Loss: 0.00003027
Iteration 14/1000 | Loss: 0.00002776
Iteration 15/1000 | Loss: 0.00002926
Iteration 16/1000 | Loss: 0.00002735
Iteration 17/1000 | Loss: 0.00002781
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00003573
Iteration 20/1000 | Loss: 0.00004220
Iteration 21/1000 | Loss: 0.00016480
Iteration 22/1000 | Loss: 0.00003647
Iteration 23/1000 | Loss: 0.00002175
Iteration 24/1000 | Loss: 0.00016832
Iteration 25/1000 | Loss: 0.00002249
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001488
Iteration 30/1000 | Loss: 0.00001466
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001442
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001416
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001415
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001409
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001404
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001402
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001400
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001400
Iteration 89/1000 | Loss: 0.00001400
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001399
Iteration 95/1000 | Loss: 0.00001399
Iteration 96/1000 | Loss: 0.00001399
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001398
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001397
Iteration 117/1000 | Loss: 0.00001397
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001397
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001397
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.3969600331620313e-05, 1.3969600331620313e-05, 1.3969600331620313e-05, 1.3969600331620313e-05, 1.3969600331620313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3969600331620313e-05

Optimization complete. Final v2v error: 3.1548914909362793 mm

Highest mean error: 4.310734272003174 mm for frame 158

Lowest mean error: 2.6363658905029297 mm for frame 222

Saving results

Total time: 112.57928442955017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011028
Iteration 2/25 | Loss: 0.00171693
Iteration 3/25 | Loss: 0.00106339
Iteration 4/25 | Loss: 0.00101661
Iteration 5/25 | Loss: 0.00100486
Iteration 6/25 | Loss: 0.00100189
Iteration 7/25 | Loss: 0.00100122
Iteration 8/25 | Loss: 0.00100121
Iteration 9/25 | Loss: 0.00100121
Iteration 10/25 | Loss: 0.00100121
Iteration 11/25 | Loss: 0.00100121
Iteration 12/25 | Loss: 0.00100120
Iteration 13/25 | Loss: 0.00100120
Iteration 14/25 | Loss: 0.00100120
Iteration 15/25 | Loss: 0.00100120
Iteration 16/25 | Loss: 0.00100120
Iteration 17/25 | Loss: 0.00100120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010012005222961307, 0.0010012005222961307, 0.0010012005222961307, 0.0010012005222961307, 0.0010012005222961307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010012005222961307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28170204
Iteration 2/25 | Loss: 0.00053731
Iteration 3/25 | Loss: 0.00053731
Iteration 4/25 | Loss: 0.00053731
Iteration 5/25 | Loss: 0.00053731
Iteration 6/25 | Loss: 0.00053731
Iteration 7/25 | Loss: 0.00053731
Iteration 8/25 | Loss: 0.00053731
Iteration 9/25 | Loss: 0.00053731
Iteration 10/25 | Loss: 0.00053731
Iteration 11/25 | Loss: 0.00053731
Iteration 12/25 | Loss: 0.00053731
Iteration 13/25 | Loss: 0.00053731
Iteration 14/25 | Loss: 0.00053731
Iteration 15/25 | Loss: 0.00053731
Iteration 16/25 | Loss: 0.00053731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005373116582632065, 0.0005373116582632065, 0.0005373116582632065, 0.0005373116582632065, 0.0005373116582632065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005373116582632065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053731
Iteration 2/1000 | Loss: 0.00007229
Iteration 3/1000 | Loss: 0.00004922
Iteration 4/1000 | Loss: 0.00004227
Iteration 5/1000 | Loss: 0.00004003
Iteration 6/1000 | Loss: 0.00003901
Iteration 7/1000 | Loss: 0.00003827
Iteration 8/1000 | Loss: 0.00003760
Iteration 9/1000 | Loss: 0.00003692
Iteration 10/1000 | Loss: 0.00003630
Iteration 11/1000 | Loss: 0.00003596
Iteration 12/1000 | Loss: 0.00003565
Iteration 13/1000 | Loss: 0.00003526
Iteration 14/1000 | Loss: 0.00003502
Iteration 15/1000 | Loss: 0.00003489
Iteration 16/1000 | Loss: 0.00003488
Iteration 17/1000 | Loss: 0.00003473
Iteration 18/1000 | Loss: 0.00003471
Iteration 19/1000 | Loss: 0.00003459
Iteration 20/1000 | Loss: 0.00003456
Iteration 21/1000 | Loss: 0.00003455
Iteration 22/1000 | Loss: 0.00003455
Iteration 23/1000 | Loss: 0.00003454
Iteration 24/1000 | Loss: 0.00003454
Iteration 25/1000 | Loss: 0.00003441
Iteration 26/1000 | Loss: 0.00003439
Iteration 27/1000 | Loss: 0.00003422
Iteration 28/1000 | Loss: 0.00003414
Iteration 29/1000 | Loss: 0.00003407
Iteration 30/1000 | Loss: 0.00003406
Iteration 31/1000 | Loss: 0.00003406
Iteration 32/1000 | Loss: 0.00003406
Iteration 33/1000 | Loss: 0.00003406
Iteration 34/1000 | Loss: 0.00003406
Iteration 35/1000 | Loss: 0.00003406
Iteration 36/1000 | Loss: 0.00003405
Iteration 37/1000 | Loss: 0.00003405
Iteration 38/1000 | Loss: 0.00003405
Iteration 39/1000 | Loss: 0.00003405
Iteration 40/1000 | Loss: 0.00003404
Iteration 41/1000 | Loss: 0.00003402
Iteration 42/1000 | Loss: 0.00003400
Iteration 43/1000 | Loss: 0.00003396
Iteration 44/1000 | Loss: 0.00003396
Iteration 45/1000 | Loss: 0.00003393
Iteration 46/1000 | Loss: 0.00003392
Iteration 47/1000 | Loss: 0.00003392
Iteration 48/1000 | Loss: 0.00003392
Iteration 49/1000 | Loss: 0.00003391
Iteration 50/1000 | Loss: 0.00003391
Iteration 51/1000 | Loss: 0.00003391
Iteration 52/1000 | Loss: 0.00003391
Iteration 53/1000 | Loss: 0.00003390
Iteration 54/1000 | Loss: 0.00003390
Iteration 55/1000 | Loss: 0.00003390
Iteration 56/1000 | Loss: 0.00003390
Iteration 57/1000 | Loss: 0.00003390
Iteration 58/1000 | Loss: 0.00003390
Iteration 59/1000 | Loss: 0.00003388
Iteration 60/1000 | Loss: 0.00003387
Iteration 61/1000 | Loss: 0.00003387
Iteration 62/1000 | Loss: 0.00003386
Iteration 63/1000 | Loss: 0.00003386
Iteration 64/1000 | Loss: 0.00003383
Iteration 65/1000 | Loss: 0.00003380
Iteration 66/1000 | Loss: 0.00003380
Iteration 67/1000 | Loss: 0.00003378
Iteration 68/1000 | Loss: 0.00003377
Iteration 69/1000 | Loss: 0.00003377
Iteration 70/1000 | Loss: 0.00003376
Iteration 71/1000 | Loss: 0.00003376
Iteration 72/1000 | Loss: 0.00003375
Iteration 73/1000 | Loss: 0.00003374
Iteration 74/1000 | Loss: 0.00003374
Iteration 75/1000 | Loss: 0.00003373
Iteration 76/1000 | Loss: 0.00003373
Iteration 77/1000 | Loss: 0.00003373
Iteration 78/1000 | Loss: 0.00003373
Iteration 79/1000 | Loss: 0.00003371
Iteration 80/1000 | Loss: 0.00003371
Iteration 81/1000 | Loss: 0.00003371
Iteration 82/1000 | Loss: 0.00003371
Iteration 83/1000 | Loss: 0.00003370
Iteration 84/1000 | Loss: 0.00003370
Iteration 85/1000 | Loss: 0.00003370
Iteration 86/1000 | Loss: 0.00003370
Iteration 87/1000 | Loss: 0.00003370
Iteration 88/1000 | Loss: 0.00003370
Iteration 89/1000 | Loss: 0.00003370
Iteration 90/1000 | Loss: 0.00003370
Iteration 91/1000 | Loss: 0.00003369
Iteration 92/1000 | Loss: 0.00003369
Iteration 93/1000 | Loss: 0.00003369
Iteration 94/1000 | Loss: 0.00003369
Iteration 95/1000 | Loss: 0.00003368
Iteration 96/1000 | Loss: 0.00003368
Iteration 97/1000 | Loss: 0.00003368
Iteration 98/1000 | Loss: 0.00003368
Iteration 99/1000 | Loss: 0.00003368
Iteration 100/1000 | Loss: 0.00003367
Iteration 101/1000 | Loss: 0.00003367
Iteration 102/1000 | Loss: 0.00003367
Iteration 103/1000 | Loss: 0.00003367
Iteration 104/1000 | Loss: 0.00003366
Iteration 105/1000 | Loss: 0.00003366
Iteration 106/1000 | Loss: 0.00003366
Iteration 107/1000 | Loss: 0.00003366
Iteration 108/1000 | Loss: 0.00003366
Iteration 109/1000 | Loss: 0.00003366
Iteration 110/1000 | Loss: 0.00003366
Iteration 111/1000 | Loss: 0.00003366
Iteration 112/1000 | Loss: 0.00003366
Iteration 113/1000 | Loss: 0.00003366
Iteration 114/1000 | Loss: 0.00003366
Iteration 115/1000 | Loss: 0.00003366
Iteration 116/1000 | Loss: 0.00003365
Iteration 117/1000 | Loss: 0.00003365
Iteration 118/1000 | Loss: 0.00003365
Iteration 119/1000 | Loss: 0.00003365
Iteration 120/1000 | Loss: 0.00003365
Iteration 121/1000 | Loss: 0.00003364
Iteration 122/1000 | Loss: 0.00003364
Iteration 123/1000 | Loss: 0.00003364
Iteration 124/1000 | Loss: 0.00003364
Iteration 125/1000 | Loss: 0.00003364
Iteration 126/1000 | Loss: 0.00003364
Iteration 127/1000 | Loss: 0.00003363
Iteration 128/1000 | Loss: 0.00003363
Iteration 129/1000 | Loss: 0.00003363
Iteration 130/1000 | Loss: 0.00003363
Iteration 131/1000 | Loss: 0.00003363
Iteration 132/1000 | Loss: 0.00003363
Iteration 133/1000 | Loss: 0.00003363
Iteration 134/1000 | Loss: 0.00003363
Iteration 135/1000 | Loss: 0.00003362
Iteration 136/1000 | Loss: 0.00003362
Iteration 137/1000 | Loss: 0.00003362
Iteration 138/1000 | Loss: 0.00003362
Iteration 139/1000 | Loss: 0.00003362
Iteration 140/1000 | Loss: 0.00003362
Iteration 141/1000 | Loss: 0.00003362
Iteration 142/1000 | Loss: 0.00003362
Iteration 143/1000 | Loss: 0.00003362
Iteration 144/1000 | Loss: 0.00003362
Iteration 145/1000 | Loss: 0.00003362
Iteration 146/1000 | Loss: 0.00003362
Iteration 147/1000 | Loss: 0.00003362
Iteration 148/1000 | Loss: 0.00003362
Iteration 149/1000 | Loss: 0.00003362
Iteration 150/1000 | Loss: 0.00003362
Iteration 151/1000 | Loss: 0.00003362
Iteration 152/1000 | Loss: 0.00003361
Iteration 153/1000 | Loss: 0.00003361
Iteration 154/1000 | Loss: 0.00003361
Iteration 155/1000 | Loss: 0.00003361
Iteration 156/1000 | Loss: 0.00003361
Iteration 157/1000 | Loss: 0.00003361
Iteration 158/1000 | Loss: 0.00003361
Iteration 159/1000 | Loss: 0.00003361
Iteration 160/1000 | Loss: 0.00003361
Iteration 161/1000 | Loss: 0.00003361
Iteration 162/1000 | Loss: 0.00003360
Iteration 163/1000 | Loss: 0.00003360
Iteration 164/1000 | Loss: 0.00003360
Iteration 165/1000 | Loss: 0.00003360
Iteration 166/1000 | Loss: 0.00003360
Iteration 167/1000 | Loss: 0.00003359
Iteration 168/1000 | Loss: 0.00003358
Iteration 169/1000 | Loss: 0.00003358
Iteration 170/1000 | Loss: 0.00003358
Iteration 171/1000 | Loss: 0.00003358
Iteration 172/1000 | Loss: 0.00003358
Iteration 173/1000 | Loss: 0.00003358
Iteration 174/1000 | Loss: 0.00003357
Iteration 175/1000 | Loss: 0.00003357
Iteration 176/1000 | Loss: 0.00003357
Iteration 177/1000 | Loss: 0.00003357
Iteration 178/1000 | Loss: 0.00003357
Iteration 179/1000 | Loss: 0.00003357
Iteration 180/1000 | Loss: 0.00003357
Iteration 181/1000 | Loss: 0.00003357
Iteration 182/1000 | Loss: 0.00003357
Iteration 183/1000 | Loss: 0.00003357
Iteration 184/1000 | Loss: 0.00003357
Iteration 185/1000 | Loss: 0.00003357
Iteration 186/1000 | Loss: 0.00003357
Iteration 187/1000 | Loss: 0.00003357
Iteration 188/1000 | Loss: 0.00003357
Iteration 189/1000 | Loss: 0.00003357
Iteration 190/1000 | Loss: 0.00003357
Iteration 191/1000 | Loss: 0.00003356
Iteration 192/1000 | Loss: 0.00003356
Iteration 193/1000 | Loss: 0.00003356
Iteration 194/1000 | Loss: 0.00003356
Iteration 195/1000 | Loss: 0.00003356
Iteration 196/1000 | Loss: 0.00003356
Iteration 197/1000 | Loss: 0.00003356
Iteration 198/1000 | Loss: 0.00003356
Iteration 199/1000 | Loss: 0.00003356
Iteration 200/1000 | Loss: 0.00003356
Iteration 201/1000 | Loss: 0.00003356
Iteration 202/1000 | Loss: 0.00003356
Iteration 203/1000 | Loss: 0.00003356
Iteration 204/1000 | Loss: 0.00003356
Iteration 205/1000 | Loss: 0.00003356
Iteration 206/1000 | Loss: 0.00003356
Iteration 207/1000 | Loss: 0.00003356
Iteration 208/1000 | Loss: 0.00003356
Iteration 209/1000 | Loss: 0.00003356
Iteration 210/1000 | Loss: 0.00003356
Iteration 211/1000 | Loss: 0.00003356
Iteration 212/1000 | Loss: 0.00003355
Iteration 213/1000 | Loss: 0.00003355
Iteration 214/1000 | Loss: 0.00003355
Iteration 215/1000 | Loss: 0.00003355
Iteration 216/1000 | Loss: 0.00003355
Iteration 217/1000 | Loss: 0.00003355
Iteration 218/1000 | Loss: 0.00003355
Iteration 219/1000 | Loss: 0.00003355
Iteration 220/1000 | Loss: 0.00003355
Iteration 221/1000 | Loss: 0.00003355
Iteration 222/1000 | Loss: 0.00003355
Iteration 223/1000 | Loss: 0.00003355
Iteration 224/1000 | Loss: 0.00003355
Iteration 225/1000 | Loss: 0.00003355
Iteration 226/1000 | Loss: 0.00003355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [3.355166336405091e-05, 3.355166336405091e-05, 3.355166336405091e-05, 3.355166336405091e-05, 3.355166336405091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.355166336405091e-05

Optimization complete. Final v2v error: 4.526519775390625 mm

Highest mean error: 5.2947187423706055 mm for frame 74

Lowest mean error: 3.8103742599487305 mm for frame 37

Saving results

Total time: 57.27102184295654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677197
Iteration 2/25 | Loss: 0.00089567
Iteration 3/25 | Loss: 0.00078356
Iteration 4/25 | Loss: 0.00075392
Iteration 5/25 | Loss: 0.00074519
Iteration 6/25 | Loss: 0.00074356
Iteration 7/25 | Loss: 0.00074316
Iteration 8/25 | Loss: 0.00074316
Iteration 9/25 | Loss: 0.00074316
Iteration 10/25 | Loss: 0.00074316
Iteration 11/25 | Loss: 0.00074316
Iteration 12/25 | Loss: 0.00074316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000743160373531282, 0.000743160373531282, 0.000743160373531282, 0.000743160373531282, 0.000743160373531282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000743160373531282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.47915506
Iteration 2/25 | Loss: 0.00094476
Iteration 3/25 | Loss: 0.00094476
Iteration 4/25 | Loss: 0.00094476
Iteration 5/25 | Loss: 0.00094476
Iteration 6/25 | Loss: 0.00094476
Iteration 7/25 | Loss: 0.00094476
Iteration 8/25 | Loss: 0.00094476
Iteration 9/25 | Loss: 0.00094475
Iteration 10/25 | Loss: 0.00094475
Iteration 11/25 | Loss: 0.00094475
Iteration 12/25 | Loss: 0.00094475
Iteration 13/25 | Loss: 0.00094475
Iteration 14/25 | Loss: 0.00094475
Iteration 15/25 | Loss: 0.00094475
Iteration 16/25 | Loss: 0.00094475
Iteration 17/25 | Loss: 0.00094475
Iteration 18/25 | Loss: 0.00094475
Iteration 19/25 | Loss: 0.00094475
Iteration 20/25 | Loss: 0.00094475
Iteration 21/25 | Loss: 0.00094475
Iteration 22/25 | Loss: 0.00094475
Iteration 23/25 | Loss: 0.00094475
Iteration 24/25 | Loss: 0.00094475
Iteration 25/25 | Loss: 0.00094475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094475
Iteration 2/1000 | Loss: 0.00002930
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001622
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001524
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001433
Iteration 13/1000 | Loss: 0.00001427
Iteration 14/1000 | Loss: 0.00001424
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00001415
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001414
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00001411
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001410
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001410
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001408
Iteration 30/1000 | Loss: 0.00001407
Iteration 31/1000 | Loss: 0.00001407
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00001407
Iteration 34/1000 | Loss: 0.00001407
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001406
Iteration 38/1000 | Loss: 0.00001406
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001406
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001406
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001405
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001404
Iteration 48/1000 | Loss: 0.00001404
Iteration 49/1000 | Loss: 0.00001404
Iteration 50/1000 | Loss: 0.00001404
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001401
Iteration 68/1000 | Loss: 0.00001401
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001400
Iteration 76/1000 | Loss: 0.00001400
Iteration 77/1000 | Loss: 0.00001400
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001398
Iteration 87/1000 | Loss: 0.00001398
Iteration 88/1000 | Loss: 0.00001398
Iteration 89/1000 | Loss: 0.00001398
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001397
Iteration 92/1000 | Loss: 0.00001397
Iteration 93/1000 | Loss: 0.00001396
Iteration 94/1000 | Loss: 0.00001396
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001395
Iteration 98/1000 | Loss: 0.00001395
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001393
Iteration 102/1000 | Loss: 0.00001393
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001390
Iteration 122/1000 | Loss: 0.00001390
Iteration 123/1000 | Loss: 0.00001390
Iteration 124/1000 | Loss: 0.00001390
Iteration 125/1000 | Loss: 0.00001389
Iteration 126/1000 | Loss: 0.00001389
Iteration 127/1000 | Loss: 0.00001389
Iteration 128/1000 | Loss: 0.00001389
Iteration 129/1000 | Loss: 0.00001389
Iteration 130/1000 | Loss: 0.00001389
Iteration 131/1000 | Loss: 0.00001389
Iteration 132/1000 | Loss: 0.00001389
Iteration 133/1000 | Loss: 0.00001389
Iteration 134/1000 | Loss: 0.00001389
Iteration 135/1000 | Loss: 0.00001389
Iteration 136/1000 | Loss: 0.00001389
Iteration 137/1000 | Loss: 0.00001389
Iteration 138/1000 | Loss: 0.00001389
Iteration 139/1000 | Loss: 0.00001389
Iteration 140/1000 | Loss: 0.00001389
Iteration 141/1000 | Loss: 0.00001389
Iteration 142/1000 | Loss: 0.00001389
Iteration 143/1000 | Loss: 0.00001389
Iteration 144/1000 | Loss: 0.00001389
Iteration 145/1000 | Loss: 0.00001389
Iteration 146/1000 | Loss: 0.00001389
Iteration 147/1000 | Loss: 0.00001389
Iteration 148/1000 | Loss: 0.00001389
Iteration 149/1000 | Loss: 0.00001389
Iteration 150/1000 | Loss: 0.00001389
Iteration 151/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.389003409713041e-05, 1.389003409713041e-05, 1.389003409713041e-05, 1.389003409713041e-05, 1.389003409713041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.389003409713041e-05

Optimization complete. Final v2v error: 3.1423938274383545 mm

Highest mean error: 3.373689889907837 mm for frame 20

Lowest mean error: 2.978433132171631 mm for frame 143

Saving results

Total time: 34.87868356704712
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963271
Iteration 2/25 | Loss: 0.00103639
Iteration 3/25 | Loss: 0.00080819
Iteration 4/25 | Loss: 0.00078038
Iteration 5/25 | Loss: 0.00077018
Iteration 6/25 | Loss: 0.00075921
Iteration 7/25 | Loss: 0.00076310
Iteration 8/25 | Loss: 0.00075875
Iteration 9/25 | Loss: 0.00075838
Iteration 10/25 | Loss: 0.00075837
Iteration 11/25 | Loss: 0.00075837
Iteration 12/25 | Loss: 0.00075837
Iteration 13/25 | Loss: 0.00075836
Iteration 14/25 | Loss: 0.00075836
Iteration 15/25 | Loss: 0.00075836
Iteration 16/25 | Loss: 0.00075836
Iteration 17/25 | Loss: 0.00075836
Iteration 18/25 | Loss: 0.00075836
Iteration 19/25 | Loss: 0.00075836
Iteration 20/25 | Loss: 0.00075836
Iteration 21/25 | Loss: 0.00075836
Iteration 22/25 | Loss: 0.00075836
Iteration 23/25 | Loss: 0.00075836
Iteration 24/25 | Loss: 0.00075836
Iteration 25/25 | Loss: 0.00075836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63565540
Iteration 2/25 | Loss: 0.00075001
Iteration 3/25 | Loss: 0.00074998
Iteration 4/25 | Loss: 0.00074998
Iteration 5/25 | Loss: 0.00074998
Iteration 6/25 | Loss: 0.00074998
Iteration 7/25 | Loss: 0.00074998
Iteration 8/25 | Loss: 0.00074998
Iteration 9/25 | Loss: 0.00074998
Iteration 10/25 | Loss: 0.00074998
Iteration 11/25 | Loss: 0.00074998
Iteration 12/25 | Loss: 0.00074998
Iteration 13/25 | Loss: 0.00074998
Iteration 14/25 | Loss: 0.00074998
Iteration 15/25 | Loss: 0.00074998
Iteration 16/25 | Loss: 0.00074998
Iteration 17/25 | Loss: 0.00074998
Iteration 18/25 | Loss: 0.00074998
Iteration 19/25 | Loss: 0.00074998
Iteration 20/25 | Loss: 0.00074998
Iteration 21/25 | Loss: 0.00074998
Iteration 22/25 | Loss: 0.00074998
Iteration 23/25 | Loss: 0.00074998
Iteration 24/25 | Loss: 0.00074998
Iteration 25/25 | Loss: 0.00074998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074998
Iteration 2/1000 | Loss: 0.00004249
Iteration 3/1000 | Loss: 0.00002720
Iteration 4/1000 | Loss: 0.00002429
Iteration 5/1000 | Loss: 0.00002296
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00002117
Iteration 8/1000 | Loss: 0.00002070
Iteration 9/1000 | Loss: 0.00002032
Iteration 10/1000 | Loss: 0.00002008
Iteration 11/1000 | Loss: 0.00001986
Iteration 12/1000 | Loss: 0.00001972
Iteration 13/1000 | Loss: 0.00001964
Iteration 14/1000 | Loss: 0.00001954
Iteration 15/1000 | Loss: 0.00001953
Iteration 16/1000 | Loss: 0.00001952
Iteration 17/1000 | Loss: 0.00001951
Iteration 18/1000 | Loss: 0.00001951
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001949
Iteration 21/1000 | Loss: 0.00001949
Iteration 22/1000 | Loss: 0.00001948
Iteration 23/1000 | Loss: 0.00001948
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001944
Iteration 26/1000 | Loss: 0.00001944
Iteration 27/1000 | Loss: 0.00001942
Iteration 28/1000 | Loss: 0.00001942
Iteration 29/1000 | Loss: 0.00001942
Iteration 30/1000 | Loss: 0.00001938
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001938
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001937
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001933
Iteration 39/1000 | Loss: 0.00001933
Iteration 40/1000 | Loss: 0.00001932
Iteration 41/1000 | Loss: 0.00001932
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001925
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001922
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001922
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001916
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001915
Iteration 87/1000 | Loss: 0.00001915
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001914
Iteration 92/1000 | Loss: 0.00001914
Iteration 93/1000 | Loss: 0.00001914
Iteration 94/1000 | Loss: 0.00001914
Iteration 95/1000 | Loss: 0.00001914
Iteration 96/1000 | Loss: 0.00001914
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001914
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001913
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001913
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001912
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001912
Iteration 115/1000 | Loss: 0.00001912
Iteration 116/1000 | Loss: 0.00001912
Iteration 117/1000 | Loss: 0.00001912
Iteration 118/1000 | Loss: 0.00001912
Iteration 119/1000 | Loss: 0.00001912
Iteration 120/1000 | Loss: 0.00001912
Iteration 121/1000 | Loss: 0.00001912
Iteration 122/1000 | Loss: 0.00001912
Iteration 123/1000 | Loss: 0.00001912
Iteration 124/1000 | Loss: 0.00001912
Iteration 125/1000 | Loss: 0.00001912
Iteration 126/1000 | Loss: 0.00001912
Iteration 127/1000 | Loss: 0.00001911
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001911
Iteration 130/1000 | Loss: 0.00001911
Iteration 131/1000 | Loss: 0.00001911
Iteration 132/1000 | Loss: 0.00001911
Iteration 133/1000 | Loss: 0.00001911
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001911
Iteration 136/1000 | Loss: 0.00001911
Iteration 137/1000 | Loss: 0.00001911
Iteration 138/1000 | Loss: 0.00001911
Iteration 139/1000 | Loss: 0.00001911
Iteration 140/1000 | Loss: 0.00001911
Iteration 141/1000 | Loss: 0.00001911
Iteration 142/1000 | Loss: 0.00001911
Iteration 143/1000 | Loss: 0.00001911
Iteration 144/1000 | Loss: 0.00001911
Iteration 145/1000 | Loss: 0.00001911
Iteration 146/1000 | Loss: 0.00001911
Iteration 147/1000 | Loss: 0.00001911
Iteration 148/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.9112809241050854e-05, 1.9112809241050854e-05, 1.9112809241050854e-05, 1.9112809241050854e-05, 1.9112809241050854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9112809241050854e-05

Optimization complete. Final v2v error: 3.4143483638763428 mm

Highest mean error: 5.790424346923828 mm for frame 179

Lowest mean error: 2.808993339538574 mm for frame 80

Saving results

Total time: 51.68554162979126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432430
Iteration 2/25 | Loss: 0.00090316
Iteration 3/25 | Loss: 0.00073124
Iteration 4/25 | Loss: 0.00071508
Iteration 5/25 | Loss: 0.00070844
Iteration 6/25 | Loss: 0.00070620
Iteration 7/25 | Loss: 0.00070595
Iteration 8/25 | Loss: 0.00070595
Iteration 9/25 | Loss: 0.00070595
Iteration 10/25 | Loss: 0.00070595
Iteration 11/25 | Loss: 0.00070595
Iteration 12/25 | Loss: 0.00070595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007059496711008251, 0.0007059496711008251, 0.0007059496711008251, 0.0007059496711008251, 0.0007059496711008251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007059496711008251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58436430
Iteration 2/25 | Loss: 0.00082804
Iteration 3/25 | Loss: 0.00082804
Iteration 4/25 | Loss: 0.00082804
Iteration 5/25 | Loss: 0.00082804
Iteration 6/25 | Loss: 0.00082804
Iteration 7/25 | Loss: 0.00082804
Iteration 8/25 | Loss: 0.00082804
Iteration 9/25 | Loss: 0.00082804
Iteration 10/25 | Loss: 0.00082804
Iteration 11/25 | Loss: 0.00082804
Iteration 12/25 | Loss: 0.00082804
Iteration 13/25 | Loss: 0.00082804
Iteration 14/25 | Loss: 0.00082804
Iteration 15/25 | Loss: 0.00082804
Iteration 16/25 | Loss: 0.00082804
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008280356414616108, 0.0008280356414616108, 0.0008280356414616108, 0.0008280356414616108, 0.0008280356414616108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008280356414616108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082804
Iteration 2/1000 | Loss: 0.00002012
Iteration 3/1000 | Loss: 0.00001413
Iteration 4/1000 | Loss: 0.00001294
Iteration 5/1000 | Loss: 0.00001227
Iteration 6/1000 | Loss: 0.00001193
Iteration 7/1000 | Loss: 0.00001180
Iteration 8/1000 | Loss: 0.00001175
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001151
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001130
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001127
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001125
Iteration 23/1000 | Loss: 0.00001125
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001124
Iteration 28/1000 | Loss: 0.00001124
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001121
Iteration 35/1000 | Loss: 0.00001120
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001117
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001115
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001114
Iteration 51/1000 | Loss: 0.00001114
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001113
Iteration 56/1000 | Loss: 0.00001113
Iteration 57/1000 | Loss: 0.00001112
Iteration 58/1000 | Loss: 0.00001112
Iteration 59/1000 | Loss: 0.00001112
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001112
Iteration 62/1000 | Loss: 0.00001112
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001111
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001109
Iteration 81/1000 | Loss: 0.00001109
Iteration 82/1000 | Loss: 0.00001109
Iteration 83/1000 | Loss: 0.00001109
Iteration 84/1000 | Loss: 0.00001109
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001108
Iteration 87/1000 | Loss: 0.00001108
Iteration 88/1000 | Loss: 0.00001107
Iteration 89/1000 | Loss: 0.00001107
Iteration 90/1000 | Loss: 0.00001107
Iteration 91/1000 | Loss: 0.00001107
Iteration 92/1000 | Loss: 0.00001107
Iteration 93/1000 | Loss: 0.00001107
Iteration 94/1000 | Loss: 0.00001107
Iteration 95/1000 | Loss: 0.00001106
Iteration 96/1000 | Loss: 0.00001106
Iteration 97/1000 | Loss: 0.00001106
Iteration 98/1000 | Loss: 0.00001106
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001103
Iteration 104/1000 | Loss: 0.00001103
Iteration 105/1000 | Loss: 0.00001103
Iteration 106/1000 | Loss: 0.00001103
Iteration 107/1000 | Loss: 0.00001103
Iteration 108/1000 | Loss: 0.00001103
Iteration 109/1000 | Loss: 0.00001103
Iteration 110/1000 | Loss: 0.00001102
Iteration 111/1000 | Loss: 0.00001102
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001102
Iteration 114/1000 | Loss: 0.00001102
Iteration 115/1000 | Loss: 0.00001102
Iteration 116/1000 | Loss: 0.00001102
Iteration 117/1000 | Loss: 0.00001102
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001102
Iteration 122/1000 | Loss: 0.00001102
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001102
Iteration 130/1000 | Loss: 0.00001102
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001102
Iteration 135/1000 | Loss: 0.00001102
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001102
Iteration 139/1000 | Loss: 0.00001102
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1019468729500659e-05, 1.1019468729500659e-05, 1.1019468729500659e-05, 1.1019468729500659e-05, 1.1019468729500659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1019468729500659e-05

Optimization complete. Final v2v error: 2.7703592777252197 mm

Highest mean error: 2.915337562561035 mm for frame 195

Lowest mean error: 2.661116123199463 mm for frame 229

Saving results

Total time: 36.425372838974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460043
Iteration 2/25 | Loss: 0.00114204
Iteration 3/25 | Loss: 0.00080752
Iteration 4/25 | Loss: 0.00076394
Iteration 5/25 | Loss: 0.00075320
Iteration 6/25 | Loss: 0.00075012
Iteration 7/25 | Loss: 0.00074927
Iteration 8/25 | Loss: 0.00074915
Iteration 9/25 | Loss: 0.00074915
Iteration 10/25 | Loss: 0.00074915
Iteration 11/25 | Loss: 0.00074915
Iteration 12/25 | Loss: 0.00074915
Iteration 13/25 | Loss: 0.00074915
Iteration 14/25 | Loss: 0.00074915
Iteration 15/25 | Loss: 0.00074915
Iteration 16/25 | Loss: 0.00074915
Iteration 17/25 | Loss: 0.00074915
Iteration 18/25 | Loss: 0.00074915
Iteration 19/25 | Loss: 0.00074915
Iteration 20/25 | Loss: 0.00074915
Iteration 21/25 | Loss: 0.00074915
Iteration 22/25 | Loss: 0.00074915
Iteration 23/25 | Loss: 0.00074915
Iteration 24/25 | Loss: 0.00074915
Iteration 25/25 | Loss: 0.00074915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57955670
Iteration 2/25 | Loss: 0.00079007
Iteration 3/25 | Loss: 0.00079006
Iteration 4/25 | Loss: 0.00079006
Iteration 5/25 | Loss: 0.00079006
Iteration 6/25 | Loss: 0.00079006
Iteration 7/25 | Loss: 0.00079006
Iteration 8/25 | Loss: 0.00079006
Iteration 9/25 | Loss: 0.00079006
Iteration 10/25 | Loss: 0.00079006
Iteration 11/25 | Loss: 0.00079006
Iteration 12/25 | Loss: 0.00079006
Iteration 13/25 | Loss: 0.00079006
Iteration 14/25 | Loss: 0.00079006
Iteration 15/25 | Loss: 0.00079006
Iteration 16/25 | Loss: 0.00079006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007900564814917743, 0.0007900564814917743, 0.0007900564814917743, 0.0007900564814917743, 0.0007900564814917743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007900564814917743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079006
Iteration 2/1000 | Loss: 0.00002914
Iteration 3/1000 | Loss: 0.00002058
Iteration 4/1000 | Loss: 0.00001773
Iteration 5/1000 | Loss: 0.00001684
Iteration 6/1000 | Loss: 0.00001621
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001530
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001511
Iteration 12/1000 | Loss: 0.00001508
Iteration 13/1000 | Loss: 0.00001498
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001483
Iteration 16/1000 | Loss: 0.00001479
Iteration 17/1000 | Loss: 0.00001479
Iteration 18/1000 | Loss: 0.00001478
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001475
Iteration 25/1000 | Loss: 0.00001475
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001473
Iteration 38/1000 | Loss: 0.00001473
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001472
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001471
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001470
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001468
Iteration 52/1000 | Loss: 0.00001467
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001464
Iteration 60/1000 | Loss: 0.00001464
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001464
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001463
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001462
Iteration 74/1000 | Loss: 0.00001462
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001460
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001455
Iteration 97/1000 | Loss: 0.00001455
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001454
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001451
Iteration 111/1000 | Loss: 0.00001451
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001450
Iteration 116/1000 | Loss: 0.00001450
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001449
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001448
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001447
Iteration 131/1000 | Loss: 0.00001447
Iteration 132/1000 | Loss: 0.00001447
Iteration 133/1000 | Loss: 0.00001447
Iteration 134/1000 | Loss: 0.00001447
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00001446
Iteration 138/1000 | Loss: 0.00001446
Iteration 139/1000 | Loss: 0.00001446
Iteration 140/1000 | Loss: 0.00001446
Iteration 141/1000 | Loss: 0.00001446
Iteration 142/1000 | Loss: 0.00001446
Iteration 143/1000 | Loss: 0.00001446
Iteration 144/1000 | Loss: 0.00001446
Iteration 145/1000 | Loss: 0.00001446
Iteration 146/1000 | Loss: 0.00001446
Iteration 147/1000 | Loss: 0.00001446
Iteration 148/1000 | Loss: 0.00001446
Iteration 149/1000 | Loss: 0.00001446
Iteration 150/1000 | Loss: 0.00001446
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001445
Iteration 153/1000 | Loss: 0.00001445
Iteration 154/1000 | Loss: 0.00001445
Iteration 155/1000 | Loss: 0.00001445
Iteration 156/1000 | Loss: 0.00001445
Iteration 157/1000 | Loss: 0.00001445
Iteration 158/1000 | Loss: 0.00001445
Iteration 159/1000 | Loss: 0.00001445
Iteration 160/1000 | Loss: 0.00001445
Iteration 161/1000 | Loss: 0.00001445
Iteration 162/1000 | Loss: 0.00001445
Iteration 163/1000 | Loss: 0.00001445
Iteration 164/1000 | Loss: 0.00001445
Iteration 165/1000 | Loss: 0.00001445
Iteration 166/1000 | Loss: 0.00001445
Iteration 167/1000 | Loss: 0.00001445
Iteration 168/1000 | Loss: 0.00001445
Iteration 169/1000 | Loss: 0.00001445
Iteration 170/1000 | Loss: 0.00001445
Iteration 171/1000 | Loss: 0.00001445
Iteration 172/1000 | Loss: 0.00001445
Iteration 173/1000 | Loss: 0.00001445
Iteration 174/1000 | Loss: 0.00001445
Iteration 175/1000 | Loss: 0.00001445
Iteration 176/1000 | Loss: 0.00001445
Iteration 177/1000 | Loss: 0.00001445
Iteration 178/1000 | Loss: 0.00001445
Iteration 179/1000 | Loss: 0.00001445
Iteration 180/1000 | Loss: 0.00001445
Iteration 181/1000 | Loss: 0.00001445
Iteration 182/1000 | Loss: 0.00001445
Iteration 183/1000 | Loss: 0.00001445
Iteration 184/1000 | Loss: 0.00001445
Iteration 185/1000 | Loss: 0.00001445
Iteration 186/1000 | Loss: 0.00001445
Iteration 187/1000 | Loss: 0.00001445
Iteration 188/1000 | Loss: 0.00001445
Iteration 189/1000 | Loss: 0.00001445
Iteration 190/1000 | Loss: 0.00001445
Iteration 191/1000 | Loss: 0.00001445
Iteration 192/1000 | Loss: 0.00001445
Iteration 193/1000 | Loss: 0.00001445
Iteration 194/1000 | Loss: 0.00001445
Iteration 195/1000 | Loss: 0.00001445
Iteration 196/1000 | Loss: 0.00001445
Iteration 197/1000 | Loss: 0.00001445
Iteration 198/1000 | Loss: 0.00001445
Iteration 199/1000 | Loss: 0.00001445
Iteration 200/1000 | Loss: 0.00001445
Iteration 201/1000 | Loss: 0.00001445
Iteration 202/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.4445331544266082e-05, 1.4445331544266082e-05, 1.4445331544266082e-05, 1.4445331544266082e-05, 1.4445331544266082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4445331544266082e-05

Optimization complete. Final v2v error: 3.1990914344787598 mm

Highest mean error: 4.514643669128418 mm for frame 57

Lowest mean error: 2.8110663890838623 mm for frame 8

Saving results

Total time: 41.832634925842285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920609
Iteration 2/25 | Loss: 0.00156423
Iteration 3/25 | Loss: 0.00109644
Iteration 4/25 | Loss: 0.00104234
Iteration 5/25 | Loss: 0.00096567
Iteration 6/25 | Loss: 0.00095281
Iteration 7/25 | Loss: 0.00094403
Iteration 8/25 | Loss: 0.00095289
Iteration 9/25 | Loss: 0.00093644
Iteration 10/25 | Loss: 0.00091681
Iteration 11/25 | Loss: 0.00090936
Iteration 12/25 | Loss: 0.00090390
Iteration 13/25 | Loss: 0.00089606
Iteration 14/25 | Loss: 0.00089197
Iteration 15/25 | Loss: 0.00089237
Iteration 16/25 | Loss: 0.00088908
Iteration 17/25 | Loss: 0.00088935
Iteration 18/25 | Loss: 0.00089188
Iteration 19/25 | Loss: 0.00089158
Iteration 20/25 | Loss: 0.00089147
Iteration 21/25 | Loss: 0.00088907
Iteration 22/25 | Loss: 0.00088928
Iteration 23/25 | Loss: 0.00089068
Iteration 24/25 | Loss: 0.00089179
Iteration 25/25 | Loss: 0.00089163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.92526817
Iteration 2/25 | Loss: 0.00157898
Iteration 3/25 | Loss: 0.00157888
Iteration 4/25 | Loss: 0.00157888
Iteration 5/25 | Loss: 0.00157888
Iteration 6/25 | Loss: 0.00157888
Iteration 7/25 | Loss: 0.00157888
Iteration 8/25 | Loss: 0.00157888
Iteration 9/25 | Loss: 0.00157888
Iteration 10/25 | Loss: 0.00157888
Iteration 11/25 | Loss: 0.00157888
Iteration 12/25 | Loss: 0.00157888
Iteration 13/25 | Loss: 0.00157888
Iteration 14/25 | Loss: 0.00157888
Iteration 15/25 | Loss: 0.00157888
Iteration 16/25 | Loss: 0.00157888
Iteration 17/25 | Loss: 0.00157888
Iteration 18/25 | Loss: 0.00157888
Iteration 19/25 | Loss: 0.00157888
Iteration 20/25 | Loss: 0.00157888
Iteration 21/25 | Loss: 0.00157888
Iteration 22/25 | Loss: 0.00157888
Iteration 23/25 | Loss: 0.00157888
Iteration 24/25 | Loss: 0.00157888
Iteration 25/25 | Loss: 0.00157888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157888
Iteration 2/1000 | Loss: 0.00030035
Iteration 3/1000 | Loss: 0.00035904
Iteration 4/1000 | Loss: 0.00026346
Iteration 5/1000 | Loss: 0.00015209
Iteration 6/1000 | Loss: 0.00007204
Iteration 7/1000 | Loss: 0.00055693
Iteration 8/1000 | Loss: 0.00037258
Iteration 9/1000 | Loss: 0.00480023
Iteration 10/1000 | Loss: 0.00065732
Iteration 11/1000 | Loss: 0.00005791
Iteration 12/1000 | Loss: 0.00006984
Iteration 13/1000 | Loss: 0.00004442
Iteration 14/1000 | Loss: 0.00007278
Iteration 15/1000 | Loss: 0.00099052
Iteration 16/1000 | Loss: 0.00006449
Iteration 17/1000 | Loss: 0.00006878
Iteration 18/1000 | Loss: 0.00006233
Iteration 19/1000 | Loss: 0.00006078
Iteration 20/1000 | Loss: 0.00005474
Iteration 21/1000 | Loss: 0.00005501
Iteration 22/1000 | Loss: 0.00005420
Iteration 23/1000 | Loss: 0.00003832
Iteration 24/1000 | Loss: 0.00005444
Iteration 25/1000 | Loss: 0.00004564
Iteration 26/1000 | Loss: 0.00004212
Iteration 27/1000 | Loss: 0.00004124
Iteration 28/1000 | Loss: 0.00005812
Iteration 29/1000 | Loss: 0.00005403
Iteration 30/1000 | Loss: 0.00004741
Iteration 31/1000 | Loss: 0.00004939
Iteration 32/1000 | Loss: 0.00006687
Iteration 33/1000 | Loss: 0.00004450
Iteration 34/1000 | Loss: 0.00003930
Iteration 35/1000 | Loss: 0.00003739
Iteration 36/1000 | Loss: 0.00003604
Iteration 37/1000 | Loss: 0.00003495
Iteration 38/1000 | Loss: 0.00003447
Iteration 39/1000 | Loss: 0.00003447
Iteration 40/1000 | Loss: 0.00003426
Iteration 41/1000 | Loss: 0.00003409
Iteration 42/1000 | Loss: 0.00003392
Iteration 43/1000 | Loss: 0.00003373
Iteration 44/1000 | Loss: 0.00003350
Iteration 45/1000 | Loss: 0.00003323
Iteration 46/1000 | Loss: 0.00003314
Iteration 47/1000 | Loss: 0.00003312
Iteration 48/1000 | Loss: 0.00003311
Iteration 49/1000 | Loss: 0.00003311
Iteration 50/1000 | Loss: 0.00003309
Iteration 51/1000 | Loss: 0.00003309
Iteration 52/1000 | Loss: 0.00003307
Iteration 53/1000 | Loss: 0.00003307
Iteration 54/1000 | Loss: 0.00003306
Iteration 55/1000 | Loss: 0.00003306
Iteration 56/1000 | Loss: 0.00003306
Iteration 57/1000 | Loss: 0.00003306
Iteration 58/1000 | Loss: 0.00003305
Iteration 59/1000 | Loss: 0.00003305
Iteration 60/1000 | Loss: 0.00003305
Iteration 61/1000 | Loss: 0.00003304
Iteration 62/1000 | Loss: 0.00003304
Iteration 63/1000 | Loss: 0.00003304
Iteration 64/1000 | Loss: 0.00003303
Iteration 65/1000 | Loss: 0.00003303
Iteration 66/1000 | Loss: 0.00003303
Iteration 67/1000 | Loss: 0.00003303
Iteration 68/1000 | Loss: 0.00003303
Iteration 69/1000 | Loss: 0.00003302
Iteration 70/1000 | Loss: 0.00003302
Iteration 71/1000 | Loss: 0.00003302
Iteration 72/1000 | Loss: 0.00003302
Iteration 73/1000 | Loss: 0.00003301
Iteration 74/1000 | Loss: 0.00003301
Iteration 75/1000 | Loss: 0.00003301
Iteration 76/1000 | Loss: 0.00003300
Iteration 77/1000 | Loss: 0.00003300
Iteration 78/1000 | Loss: 0.00003300
Iteration 79/1000 | Loss: 0.00003299
Iteration 80/1000 | Loss: 0.00003299
Iteration 81/1000 | Loss: 0.00003299
Iteration 82/1000 | Loss: 0.00003298
Iteration 83/1000 | Loss: 0.00003298
Iteration 84/1000 | Loss: 0.00003298
Iteration 85/1000 | Loss: 0.00003297
Iteration 86/1000 | Loss: 0.00003297
Iteration 87/1000 | Loss: 0.00003297
Iteration 88/1000 | Loss: 0.00003296
Iteration 89/1000 | Loss: 0.00003296
Iteration 90/1000 | Loss: 0.00003296
Iteration 91/1000 | Loss: 0.00003295
Iteration 92/1000 | Loss: 0.00003295
Iteration 93/1000 | Loss: 0.00003295
Iteration 94/1000 | Loss: 0.00003294
Iteration 95/1000 | Loss: 0.00003294
Iteration 96/1000 | Loss: 0.00003294
Iteration 97/1000 | Loss: 0.00003294
Iteration 98/1000 | Loss: 0.00003293
Iteration 99/1000 | Loss: 0.00003293
Iteration 100/1000 | Loss: 0.00003293
Iteration 101/1000 | Loss: 0.00003293
Iteration 102/1000 | Loss: 0.00003293
Iteration 103/1000 | Loss: 0.00003293
Iteration 104/1000 | Loss: 0.00003293
Iteration 105/1000 | Loss: 0.00003292
Iteration 106/1000 | Loss: 0.00003292
Iteration 107/1000 | Loss: 0.00003292
Iteration 108/1000 | Loss: 0.00003292
Iteration 109/1000 | Loss: 0.00003292
Iteration 110/1000 | Loss: 0.00003292
Iteration 111/1000 | Loss: 0.00003291
Iteration 112/1000 | Loss: 0.00003291
Iteration 113/1000 | Loss: 0.00003291
Iteration 114/1000 | Loss: 0.00003291
Iteration 115/1000 | Loss: 0.00003291
Iteration 116/1000 | Loss: 0.00003291
Iteration 117/1000 | Loss: 0.00003291
Iteration 118/1000 | Loss: 0.00003291
Iteration 119/1000 | Loss: 0.00003291
Iteration 120/1000 | Loss: 0.00003291
Iteration 121/1000 | Loss: 0.00003291
Iteration 122/1000 | Loss: 0.00003291
Iteration 123/1000 | Loss: 0.00003290
Iteration 124/1000 | Loss: 0.00003290
Iteration 125/1000 | Loss: 0.00003290
Iteration 126/1000 | Loss: 0.00003290
Iteration 127/1000 | Loss: 0.00003290
Iteration 128/1000 | Loss: 0.00003290
Iteration 129/1000 | Loss: 0.00003290
Iteration 130/1000 | Loss: 0.00003290
Iteration 131/1000 | Loss: 0.00003290
Iteration 132/1000 | Loss: 0.00003289
Iteration 133/1000 | Loss: 0.00003289
Iteration 134/1000 | Loss: 0.00003289
Iteration 135/1000 | Loss: 0.00003289
Iteration 136/1000 | Loss: 0.00003289
Iteration 137/1000 | Loss: 0.00003289
Iteration 138/1000 | Loss: 0.00003289
Iteration 139/1000 | Loss: 0.00003289
Iteration 140/1000 | Loss: 0.00003289
Iteration 141/1000 | Loss: 0.00003288
Iteration 142/1000 | Loss: 0.00003288
Iteration 143/1000 | Loss: 0.00003288
Iteration 144/1000 | Loss: 0.00003288
Iteration 145/1000 | Loss: 0.00003288
Iteration 146/1000 | Loss: 0.00003288
Iteration 147/1000 | Loss: 0.00003287
Iteration 148/1000 | Loss: 0.00003287
Iteration 149/1000 | Loss: 0.00003287
Iteration 150/1000 | Loss: 0.00003287
Iteration 151/1000 | Loss: 0.00003287
Iteration 152/1000 | Loss: 0.00003286
Iteration 153/1000 | Loss: 0.00003286
Iteration 154/1000 | Loss: 0.00003286
Iteration 155/1000 | Loss: 0.00003286
Iteration 156/1000 | Loss: 0.00003285
Iteration 157/1000 | Loss: 0.00003285
Iteration 158/1000 | Loss: 0.00003285
Iteration 159/1000 | Loss: 0.00003284
Iteration 160/1000 | Loss: 0.00003284
Iteration 161/1000 | Loss: 0.00003284
Iteration 162/1000 | Loss: 0.00003284
Iteration 163/1000 | Loss: 0.00003283
Iteration 164/1000 | Loss: 0.00003283
Iteration 165/1000 | Loss: 0.00003283
Iteration 166/1000 | Loss: 0.00003282
Iteration 167/1000 | Loss: 0.00003282
Iteration 168/1000 | Loss: 0.00003282
Iteration 169/1000 | Loss: 0.00003282
Iteration 170/1000 | Loss: 0.00003282
Iteration 171/1000 | Loss: 0.00003282
Iteration 172/1000 | Loss: 0.00003282
Iteration 173/1000 | Loss: 0.00003281
Iteration 174/1000 | Loss: 0.00003281
Iteration 175/1000 | Loss: 0.00003281
Iteration 176/1000 | Loss: 0.00003281
Iteration 177/1000 | Loss: 0.00003281
Iteration 178/1000 | Loss: 0.00003281
Iteration 179/1000 | Loss: 0.00003280
Iteration 180/1000 | Loss: 0.00003280
Iteration 181/1000 | Loss: 0.00003280
Iteration 182/1000 | Loss: 0.00003280
Iteration 183/1000 | Loss: 0.00003280
Iteration 184/1000 | Loss: 0.00003280
Iteration 185/1000 | Loss: 0.00003280
Iteration 186/1000 | Loss: 0.00003280
Iteration 187/1000 | Loss: 0.00003280
Iteration 188/1000 | Loss: 0.00003279
Iteration 189/1000 | Loss: 0.00003279
Iteration 190/1000 | Loss: 0.00003279
Iteration 191/1000 | Loss: 0.00003279
Iteration 192/1000 | Loss: 0.00003279
Iteration 193/1000 | Loss: 0.00003279
Iteration 194/1000 | Loss: 0.00003279
Iteration 195/1000 | Loss: 0.00003279
Iteration 196/1000 | Loss: 0.00003279
Iteration 197/1000 | Loss: 0.00003278
Iteration 198/1000 | Loss: 0.00003278
Iteration 199/1000 | Loss: 0.00003278
Iteration 200/1000 | Loss: 0.00003278
Iteration 201/1000 | Loss: 0.00003277
Iteration 202/1000 | Loss: 0.00003277
Iteration 203/1000 | Loss: 0.00003277
Iteration 204/1000 | Loss: 0.00003277
Iteration 205/1000 | Loss: 0.00003277
Iteration 206/1000 | Loss: 0.00003277
Iteration 207/1000 | Loss: 0.00003277
Iteration 208/1000 | Loss: 0.00003276
Iteration 209/1000 | Loss: 0.00003276
Iteration 210/1000 | Loss: 0.00003276
Iteration 211/1000 | Loss: 0.00003276
Iteration 212/1000 | Loss: 0.00003276
Iteration 213/1000 | Loss: 0.00003276
Iteration 214/1000 | Loss: 0.00003276
Iteration 215/1000 | Loss: 0.00003275
Iteration 216/1000 | Loss: 0.00003275
Iteration 217/1000 | Loss: 0.00003275
Iteration 218/1000 | Loss: 0.00003275
Iteration 219/1000 | Loss: 0.00003275
Iteration 220/1000 | Loss: 0.00003274
Iteration 221/1000 | Loss: 0.00003274
Iteration 222/1000 | Loss: 0.00003274
Iteration 223/1000 | Loss: 0.00003274
Iteration 224/1000 | Loss: 0.00003274
Iteration 225/1000 | Loss: 0.00003273
Iteration 226/1000 | Loss: 0.00003273
Iteration 227/1000 | Loss: 0.00003273
Iteration 228/1000 | Loss: 0.00003273
Iteration 229/1000 | Loss: 0.00003273
Iteration 230/1000 | Loss: 0.00003273
Iteration 231/1000 | Loss: 0.00003273
Iteration 232/1000 | Loss: 0.00003273
Iteration 233/1000 | Loss: 0.00003273
Iteration 234/1000 | Loss: 0.00003273
Iteration 235/1000 | Loss: 0.00003273
Iteration 236/1000 | Loss: 0.00003273
Iteration 237/1000 | Loss: 0.00003273
Iteration 238/1000 | Loss: 0.00003272
Iteration 239/1000 | Loss: 0.00003272
Iteration 240/1000 | Loss: 0.00003272
Iteration 241/1000 | Loss: 0.00003272
Iteration 242/1000 | Loss: 0.00003272
Iteration 243/1000 | Loss: 0.00003272
Iteration 244/1000 | Loss: 0.00003272
Iteration 245/1000 | Loss: 0.00003272
Iteration 246/1000 | Loss: 0.00003272
Iteration 247/1000 | Loss: 0.00003272
Iteration 248/1000 | Loss: 0.00003272
Iteration 249/1000 | Loss: 0.00003272
Iteration 250/1000 | Loss: 0.00003272
Iteration 251/1000 | Loss: 0.00003272
Iteration 252/1000 | Loss: 0.00003272
Iteration 253/1000 | Loss: 0.00003272
Iteration 254/1000 | Loss: 0.00003272
Iteration 255/1000 | Loss: 0.00003272
Iteration 256/1000 | Loss: 0.00003272
Iteration 257/1000 | Loss: 0.00003272
Iteration 258/1000 | Loss: 0.00003272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [3.272333924542181e-05, 3.272333924542181e-05, 3.272333924542181e-05, 3.272333924542181e-05, 3.272333924542181e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.272333924542181e-05

Optimization complete. Final v2v error: 4.588310241699219 mm

Highest mean error: 11.694727897644043 mm for frame 66

Lowest mean error: 3.3745357990264893 mm for frame 133

Saving results

Total time: 141.47790813446045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808054
Iteration 2/25 | Loss: 0.00095685
Iteration 3/25 | Loss: 0.00074831
Iteration 4/25 | Loss: 0.00072749
Iteration 5/25 | Loss: 0.00072194
Iteration 6/25 | Loss: 0.00072023
Iteration 7/25 | Loss: 0.00072023
Iteration 8/25 | Loss: 0.00072023
Iteration 9/25 | Loss: 0.00072023
Iteration 10/25 | Loss: 0.00072023
Iteration 11/25 | Loss: 0.00072023
Iteration 12/25 | Loss: 0.00072023
Iteration 13/25 | Loss: 0.00072023
Iteration 14/25 | Loss: 0.00072023
Iteration 15/25 | Loss: 0.00072023
Iteration 16/25 | Loss: 0.00072022
Iteration 17/25 | Loss: 0.00072022
Iteration 18/25 | Loss: 0.00072022
Iteration 19/25 | Loss: 0.00072022
Iteration 20/25 | Loss: 0.00072022
Iteration 21/25 | Loss: 0.00072022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007202163687907159, 0.0007202163687907159, 0.0007202163687907159, 0.0007202163687907159, 0.0007202163687907159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007202163687907159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56563532
Iteration 2/25 | Loss: 0.00089038
Iteration 3/25 | Loss: 0.00089038
Iteration 4/25 | Loss: 0.00089038
Iteration 5/25 | Loss: 0.00089037
Iteration 6/25 | Loss: 0.00089037
Iteration 7/25 | Loss: 0.00089037
Iteration 8/25 | Loss: 0.00089037
Iteration 9/25 | Loss: 0.00089037
Iteration 10/25 | Loss: 0.00089037
Iteration 11/25 | Loss: 0.00089037
Iteration 12/25 | Loss: 0.00089037
Iteration 13/25 | Loss: 0.00089037
Iteration 14/25 | Loss: 0.00089037
Iteration 15/25 | Loss: 0.00089037
Iteration 16/25 | Loss: 0.00089037
Iteration 17/25 | Loss: 0.00089037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008903731941245496, 0.0008903731941245496, 0.0008903731941245496, 0.0008903731941245496, 0.0008903731941245496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008903731941245496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089037
Iteration 2/1000 | Loss: 0.00002246
Iteration 3/1000 | Loss: 0.00001559
Iteration 4/1000 | Loss: 0.00001334
Iteration 5/1000 | Loss: 0.00001263
Iteration 6/1000 | Loss: 0.00001193
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001144
Iteration 9/1000 | Loss: 0.00001135
Iteration 10/1000 | Loss: 0.00001134
Iteration 11/1000 | Loss: 0.00001132
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001115
Iteration 19/1000 | Loss: 0.00001106
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001091
Iteration 23/1000 | Loss: 0.00001088
Iteration 24/1000 | Loss: 0.00001087
Iteration 25/1000 | Loss: 0.00001081
Iteration 26/1000 | Loss: 0.00001080
Iteration 27/1000 | Loss: 0.00001079
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001078
Iteration 30/1000 | Loss: 0.00001078
Iteration 31/1000 | Loss: 0.00001078
Iteration 32/1000 | Loss: 0.00001078
Iteration 33/1000 | Loss: 0.00001078
Iteration 34/1000 | Loss: 0.00001078
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001078
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001078
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001077
Iteration 44/1000 | Loss: 0.00001077
Iteration 45/1000 | Loss: 0.00001077
Iteration 46/1000 | Loss: 0.00001077
Iteration 47/1000 | Loss: 0.00001076
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001073
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001070
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001069
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001068
Iteration 60/1000 | Loss: 0.00001068
Iteration 61/1000 | Loss: 0.00001068
Iteration 62/1000 | Loss: 0.00001068
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001068
Iteration 65/1000 | Loss: 0.00001068
Iteration 66/1000 | Loss: 0.00001068
Iteration 67/1000 | Loss: 0.00001068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.0678360013116617e-05, 1.0678360013116617e-05, 1.0678360013116617e-05, 1.0678360013116617e-05, 1.0678360013116617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0678360013116617e-05

Optimization complete. Final v2v error: 2.7643325328826904 mm

Highest mean error: 3.0586841106414795 mm for frame 123

Lowest mean error: 2.5835719108581543 mm for frame 224

Saving results

Total time: 33.045400619506836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997202
Iteration 2/25 | Loss: 0.00200921
Iteration 3/25 | Loss: 0.00149204
Iteration 4/25 | Loss: 0.00128964
Iteration 5/25 | Loss: 0.00131323
Iteration 6/25 | Loss: 0.00135035
Iteration 7/25 | Loss: 0.00121115
Iteration 8/25 | Loss: 0.00101736
Iteration 9/25 | Loss: 0.00095921
Iteration 10/25 | Loss: 0.00094997
Iteration 11/25 | Loss: 0.00096470
Iteration 12/25 | Loss: 0.00094601
Iteration 13/25 | Loss: 0.00091387
Iteration 14/25 | Loss: 0.00090165
Iteration 15/25 | Loss: 0.00088880
Iteration 16/25 | Loss: 0.00088020
Iteration 17/25 | Loss: 0.00089065
Iteration 18/25 | Loss: 0.00088589
Iteration 19/25 | Loss: 0.00088177
Iteration 20/25 | Loss: 0.00087329
Iteration 21/25 | Loss: 0.00087279
Iteration 22/25 | Loss: 0.00085899
Iteration 23/25 | Loss: 0.00085837
Iteration 24/25 | Loss: 0.00085212
Iteration 25/25 | Loss: 0.00084653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57338917
Iteration 2/25 | Loss: 0.00191647
Iteration 3/25 | Loss: 0.00191647
Iteration 4/25 | Loss: 0.00191647
Iteration 5/25 | Loss: 0.00191647
Iteration 6/25 | Loss: 0.00191647
Iteration 7/25 | Loss: 0.00191647
Iteration 8/25 | Loss: 0.00191647
Iteration 9/25 | Loss: 0.00191647
Iteration 10/25 | Loss: 0.00191647
Iteration 11/25 | Loss: 0.00191647
Iteration 12/25 | Loss: 0.00191647
Iteration 13/25 | Loss: 0.00191647
Iteration 14/25 | Loss: 0.00191647
Iteration 15/25 | Loss: 0.00191647
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019164658151566982, 0.0019164658151566982, 0.0019164658151566982, 0.0019164658151566982, 0.0019164658151566982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019164658151566982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191647
Iteration 2/1000 | Loss: 0.00213265
Iteration 3/1000 | Loss: 0.00103181
Iteration 4/1000 | Loss: 0.00014131
Iteration 5/1000 | Loss: 0.00178537
Iteration 6/1000 | Loss: 0.00011824
Iteration 7/1000 | Loss: 0.00007377
Iteration 8/1000 | Loss: 0.00006286
Iteration 9/1000 | Loss: 0.00006053
Iteration 10/1000 | Loss: 0.00004648
Iteration 11/1000 | Loss: 0.00006014
Iteration 12/1000 | Loss: 0.00005223
Iteration 13/1000 | Loss: 0.00004837
Iteration 14/1000 | Loss: 0.00005522
Iteration 15/1000 | Loss: 0.00038985
Iteration 16/1000 | Loss: 0.00008479
Iteration 17/1000 | Loss: 0.00005950
Iteration 18/1000 | Loss: 0.00005708
Iteration 19/1000 | Loss: 0.00005752
Iteration 20/1000 | Loss: 0.00005021
Iteration 21/1000 | Loss: 0.00007338
Iteration 22/1000 | Loss: 0.00004934
Iteration 23/1000 | Loss: 0.00004963
Iteration 24/1000 | Loss: 0.00004915
Iteration 25/1000 | Loss: 0.00004831
Iteration 26/1000 | Loss: 0.00035886
Iteration 27/1000 | Loss: 0.00004659
Iteration 28/1000 | Loss: 0.00006123
Iteration 29/1000 | Loss: 0.00005261
Iteration 30/1000 | Loss: 0.00003323
Iteration 31/1000 | Loss: 0.00005318
Iteration 32/1000 | Loss: 0.00003868
Iteration 33/1000 | Loss: 0.00006301
Iteration 34/1000 | Loss: 0.00006376
Iteration 35/1000 | Loss: 0.00004829
Iteration 36/1000 | Loss: 0.00003201
Iteration 37/1000 | Loss: 0.00005775
Iteration 38/1000 | Loss: 0.00004156
Iteration 39/1000 | Loss: 0.00004850
Iteration 40/1000 | Loss: 0.00004664
Iteration 41/1000 | Loss: 0.00004661
Iteration 42/1000 | Loss: 0.00005112
Iteration 43/1000 | Loss: 0.00013663
Iteration 44/1000 | Loss: 0.00013883
Iteration 45/1000 | Loss: 0.00005637
Iteration 46/1000 | Loss: 0.00004469
Iteration 47/1000 | Loss: 0.00004873
Iteration 48/1000 | Loss: 0.00004638
Iteration 49/1000 | Loss: 0.00004784
Iteration 50/1000 | Loss: 0.00004380
Iteration 51/1000 | Loss: 0.00004437
Iteration 52/1000 | Loss: 0.00002835
Iteration 53/1000 | Loss: 0.00004163
Iteration 54/1000 | Loss: 0.00002566
Iteration 55/1000 | Loss: 0.00002066
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001815
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001749
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00033425
Iteration 64/1000 | Loss: 0.00002477
Iteration 65/1000 | Loss: 0.00002194
Iteration 66/1000 | Loss: 0.00001942
Iteration 67/1000 | Loss: 0.00001881
Iteration 68/1000 | Loss: 0.00001838
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001759
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001707
Iteration 76/1000 | Loss: 0.00001706
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001704
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001694
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001692
Iteration 88/1000 | Loss: 0.00001691
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001690
Iteration 91/1000 | Loss: 0.00001687
Iteration 92/1000 | Loss: 0.00001686
Iteration 93/1000 | Loss: 0.00001686
Iteration 94/1000 | Loss: 0.00001686
Iteration 95/1000 | Loss: 0.00001685
Iteration 96/1000 | Loss: 0.00001685
Iteration 97/1000 | Loss: 0.00001685
Iteration 98/1000 | Loss: 0.00001685
Iteration 99/1000 | Loss: 0.00001685
Iteration 100/1000 | Loss: 0.00001683
Iteration 101/1000 | Loss: 0.00001683
Iteration 102/1000 | Loss: 0.00001682
Iteration 103/1000 | Loss: 0.00001682
Iteration 104/1000 | Loss: 0.00001682
Iteration 105/1000 | Loss: 0.00001680
Iteration 106/1000 | Loss: 0.00001679
Iteration 107/1000 | Loss: 0.00001679
Iteration 108/1000 | Loss: 0.00001678
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00001677
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001677
Iteration 113/1000 | Loss: 0.00001677
Iteration 114/1000 | Loss: 0.00001677
Iteration 115/1000 | Loss: 0.00001677
Iteration 116/1000 | Loss: 0.00001677
Iteration 117/1000 | Loss: 0.00001677
Iteration 118/1000 | Loss: 0.00001677
Iteration 119/1000 | Loss: 0.00001677
Iteration 120/1000 | Loss: 0.00001676
Iteration 121/1000 | Loss: 0.00001676
Iteration 122/1000 | Loss: 0.00001676
Iteration 123/1000 | Loss: 0.00001675
Iteration 124/1000 | Loss: 0.00001675
Iteration 125/1000 | Loss: 0.00001675
Iteration 126/1000 | Loss: 0.00001674
Iteration 127/1000 | Loss: 0.00001674
Iteration 128/1000 | Loss: 0.00001674
Iteration 129/1000 | Loss: 0.00001673
Iteration 130/1000 | Loss: 0.00001673
Iteration 131/1000 | Loss: 0.00001673
Iteration 132/1000 | Loss: 0.00001673
Iteration 133/1000 | Loss: 0.00001673
Iteration 134/1000 | Loss: 0.00001673
Iteration 135/1000 | Loss: 0.00001673
Iteration 136/1000 | Loss: 0.00001673
Iteration 137/1000 | Loss: 0.00001672
Iteration 138/1000 | Loss: 0.00001672
Iteration 139/1000 | Loss: 0.00001672
Iteration 140/1000 | Loss: 0.00001672
Iteration 141/1000 | Loss: 0.00001672
Iteration 142/1000 | Loss: 0.00001671
Iteration 143/1000 | Loss: 0.00001671
Iteration 144/1000 | Loss: 0.00001671
Iteration 145/1000 | Loss: 0.00001671
Iteration 146/1000 | Loss: 0.00001670
Iteration 147/1000 | Loss: 0.00001670
Iteration 148/1000 | Loss: 0.00001670
Iteration 149/1000 | Loss: 0.00001670
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001669
Iteration 152/1000 | Loss: 0.00034399
Iteration 153/1000 | Loss: 0.00027863
Iteration 154/1000 | Loss: 0.00004026
Iteration 155/1000 | Loss: 0.00002999
Iteration 156/1000 | Loss: 0.00002431
Iteration 157/1000 | Loss: 0.00035202
Iteration 158/1000 | Loss: 0.00004416
Iteration 159/1000 | Loss: 0.00002887
Iteration 160/1000 | Loss: 0.00002486
Iteration 161/1000 | Loss: 0.00002285
Iteration 162/1000 | Loss: 0.00002145
Iteration 163/1000 | Loss: 0.00001995
Iteration 164/1000 | Loss: 0.00001857
Iteration 165/1000 | Loss: 0.00003619
Iteration 166/1000 | Loss: 0.00002507
Iteration 167/1000 | Loss: 0.00002131
Iteration 168/1000 | Loss: 0.00001731
Iteration 169/1000 | Loss: 0.00003093
Iteration 170/1000 | Loss: 0.00002614
Iteration 171/1000 | Loss: 0.00002899
Iteration 172/1000 | Loss: 0.00003141
Iteration 173/1000 | Loss: 0.00003378
Iteration 174/1000 | Loss: 0.00003117
Iteration 175/1000 | Loss: 0.00003102
Iteration 176/1000 | Loss: 0.00001704
Iteration 177/1000 | Loss: 0.00001612
Iteration 178/1000 | Loss: 0.00003168
Iteration 179/1000 | Loss: 0.00003427
Iteration 180/1000 | Loss: 0.00003107
Iteration 181/1000 | Loss: 0.00002870
Iteration 182/1000 | Loss: 0.00003089
Iteration 183/1000 | Loss: 0.00003242
Iteration 184/1000 | Loss: 0.00003023
Iteration 185/1000 | Loss: 0.00002735
Iteration 186/1000 | Loss: 0.00003004
Iteration 187/1000 | Loss: 0.00001807
Iteration 188/1000 | Loss: 0.00001745
Iteration 189/1000 | Loss: 0.00002443
Iteration 190/1000 | Loss: 0.00001627
Iteration 191/1000 | Loss: 0.00001587
Iteration 192/1000 | Loss: 0.00001582
Iteration 193/1000 | Loss: 0.00001582
Iteration 194/1000 | Loss: 0.00001582
Iteration 195/1000 | Loss: 0.00001581
Iteration 196/1000 | Loss: 0.00001581
Iteration 197/1000 | Loss: 0.00001581
Iteration 198/1000 | Loss: 0.00001581
Iteration 199/1000 | Loss: 0.00001581
Iteration 200/1000 | Loss: 0.00001581
Iteration 201/1000 | Loss: 0.00001580
Iteration 202/1000 | Loss: 0.00001580
Iteration 203/1000 | Loss: 0.00001580
Iteration 204/1000 | Loss: 0.00001579
Iteration 205/1000 | Loss: 0.00001579
Iteration 206/1000 | Loss: 0.00001579
Iteration 207/1000 | Loss: 0.00001579
Iteration 208/1000 | Loss: 0.00001578
Iteration 209/1000 | Loss: 0.00001578
Iteration 210/1000 | Loss: 0.00001578
Iteration 211/1000 | Loss: 0.00001578
Iteration 212/1000 | Loss: 0.00001577
Iteration 213/1000 | Loss: 0.00001576
Iteration 214/1000 | Loss: 0.00001576
Iteration 215/1000 | Loss: 0.00001575
Iteration 216/1000 | Loss: 0.00001575
Iteration 217/1000 | Loss: 0.00001575
Iteration 218/1000 | Loss: 0.00001575
Iteration 219/1000 | Loss: 0.00001575
Iteration 220/1000 | Loss: 0.00001575
Iteration 221/1000 | Loss: 0.00001574
Iteration 222/1000 | Loss: 0.00001574
Iteration 223/1000 | Loss: 0.00001574
Iteration 224/1000 | Loss: 0.00001574
Iteration 225/1000 | Loss: 0.00001573
Iteration 226/1000 | Loss: 0.00001573
Iteration 227/1000 | Loss: 0.00001573
Iteration 228/1000 | Loss: 0.00001573
Iteration 229/1000 | Loss: 0.00001573
Iteration 230/1000 | Loss: 0.00001572
Iteration 231/1000 | Loss: 0.00001572
Iteration 232/1000 | Loss: 0.00001572
Iteration 233/1000 | Loss: 0.00001572
Iteration 234/1000 | Loss: 0.00001572
Iteration 235/1000 | Loss: 0.00001572
Iteration 236/1000 | Loss: 0.00001572
Iteration 237/1000 | Loss: 0.00001571
Iteration 238/1000 | Loss: 0.00001571
Iteration 239/1000 | Loss: 0.00001571
Iteration 240/1000 | Loss: 0.00001571
Iteration 241/1000 | Loss: 0.00001571
Iteration 242/1000 | Loss: 0.00001571
Iteration 243/1000 | Loss: 0.00001571
Iteration 244/1000 | Loss: 0.00001571
Iteration 245/1000 | Loss: 0.00001570
Iteration 246/1000 | Loss: 0.00001570
Iteration 247/1000 | Loss: 0.00001569
Iteration 248/1000 | Loss: 0.00001569
Iteration 249/1000 | Loss: 0.00001569
Iteration 250/1000 | Loss: 0.00001569
Iteration 251/1000 | Loss: 0.00001569
Iteration 252/1000 | Loss: 0.00001568
Iteration 253/1000 | Loss: 0.00001568
Iteration 254/1000 | Loss: 0.00001568
Iteration 255/1000 | Loss: 0.00001568
Iteration 256/1000 | Loss: 0.00001568
Iteration 257/1000 | Loss: 0.00001568
Iteration 258/1000 | Loss: 0.00001568
Iteration 259/1000 | Loss: 0.00001568
Iteration 260/1000 | Loss: 0.00001568
Iteration 261/1000 | Loss: 0.00001568
Iteration 262/1000 | Loss: 0.00001568
Iteration 263/1000 | Loss: 0.00001567
Iteration 264/1000 | Loss: 0.00001567
Iteration 265/1000 | Loss: 0.00001567
Iteration 266/1000 | Loss: 0.00001566
Iteration 267/1000 | Loss: 0.00001566
Iteration 268/1000 | Loss: 0.00001566
Iteration 269/1000 | Loss: 0.00001566
Iteration 270/1000 | Loss: 0.00001566
Iteration 271/1000 | Loss: 0.00001565
Iteration 272/1000 | Loss: 0.00001565
Iteration 273/1000 | Loss: 0.00001565
Iteration 274/1000 | Loss: 0.00001565
Iteration 275/1000 | Loss: 0.00001565
Iteration 276/1000 | Loss: 0.00001565
Iteration 277/1000 | Loss: 0.00001565
Iteration 278/1000 | Loss: 0.00001564
Iteration 279/1000 | Loss: 0.00001564
Iteration 280/1000 | Loss: 0.00001564
Iteration 281/1000 | Loss: 0.00001563
Iteration 282/1000 | Loss: 0.00001560
Iteration 283/1000 | Loss: 0.00001560
Iteration 284/1000 | Loss: 0.00001560
Iteration 285/1000 | Loss: 0.00001560
Iteration 286/1000 | Loss: 0.00001560
Iteration 287/1000 | Loss: 0.00001559
Iteration 288/1000 | Loss: 0.00001559
Iteration 289/1000 | Loss: 0.00001559
Iteration 290/1000 | Loss: 0.00001559
Iteration 291/1000 | Loss: 0.00001559
Iteration 292/1000 | Loss: 0.00001559
Iteration 293/1000 | Loss: 0.00001559
Iteration 294/1000 | Loss: 0.00001559
Iteration 295/1000 | Loss: 0.00001559
Iteration 296/1000 | Loss: 0.00001559
Iteration 297/1000 | Loss: 0.00001558
Iteration 298/1000 | Loss: 0.00001558
Iteration 299/1000 | Loss: 0.00001558
Iteration 300/1000 | Loss: 0.00001558
Iteration 301/1000 | Loss: 0.00001558
Iteration 302/1000 | Loss: 0.00001558
Iteration 303/1000 | Loss: 0.00001558
Iteration 304/1000 | Loss: 0.00001557
Iteration 305/1000 | Loss: 0.00001557
Iteration 306/1000 | Loss: 0.00001557
Iteration 307/1000 | Loss: 0.00003612
Iteration 308/1000 | Loss: 0.00003072
Iteration 309/1000 | Loss: 0.00002002
Iteration 310/1000 | Loss: 0.00001769
Iteration 311/1000 | Loss: 0.00001623
Iteration 312/1000 | Loss: 0.00001557
Iteration 313/1000 | Loss: 0.00001513
Iteration 314/1000 | Loss: 0.00001494
Iteration 315/1000 | Loss: 0.00001482
Iteration 316/1000 | Loss: 0.00001479
Iteration 317/1000 | Loss: 0.00001477
Iteration 318/1000 | Loss: 0.00001476
Iteration 319/1000 | Loss: 0.00001476
Iteration 320/1000 | Loss: 0.00001475
Iteration 321/1000 | Loss: 0.00001473
Iteration 322/1000 | Loss: 0.00001473
Iteration 323/1000 | Loss: 0.00001473
Iteration 324/1000 | Loss: 0.00001473
Iteration 325/1000 | Loss: 0.00001473
Iteration 326/1000 | Loss: 0.00001473
Iteration 327/1000 | Loss: 0.00001473
Iteration 328/1000 | Loss: 0.00001473
Iteration 329/1000 | Loss: 0.00001472
Iteration 330/1000 | Loss: 0.00001471
Iteration 331/1000 | Loss: 0.00001471
Iteration 332/1000 | Loss: 0.00001470
Iteration 333/1000 | Loss: 0.00001470
Iteration 334/1000 | Loss: 0.00001470
Iteration 335/1000 | Loss: 0.00001469
Iteration 336/1000 | Loss: 0.00001469
Iteration 337/1000 | Loss: 0.00001468
Iteration 338/1000 | Loss: 0.00001468
Iteration 339/1000 | Loss: 0.00001468
Iteration 340/1000 | Loss: 0.00001468
Iteration 341/1000 | Loss: 0.00001468
Iteration 342/1000 | Loss: 0.00001468
Iteration 343/1000 | Loss: 0.00001467
Iteration 344/1000 | Loss: 0.00001467
Iteration 345/1000 | Loss: 0.00001467
Iteration 346/1000 | Loss: 0.00001467
Iteration 347/1000 | Loss: 0.00001466
Iteration 348/1000 | Loss: 0.00001466
Iteration 349/1000 | Loss: 0.00001466
Iteration 350/1000 | Loss: 0.00001466
Iteration 351/1000 | Loss: 0.00001466
Iteration 352/1000 | Loss: 0.00001465
Iteration 353/1000 | Loss: 0.00001465
Iteration 354/1000 | Loss: 0.00001465
Iteration 355/1000 | Loss: 0.00001465
Iteration 356/1000 | Loss: 0.00001465
Iteration 357/1000 | Loss: 0.00001465
Iteration 358/1000 | Loss: 0.00001465
Iteration 359/1000 | Loss: 0.00001465
Iteration 360/1000 | Loss: 0.00001465
Iteration 361/1000 | Loss: 0.00001465
Iteration 362/1000 | Loss: 0.00001465
Iteration 363/1000 | Loss: 0.00001464
Iteration 364/1000 | Loss: 0.00001464
Iteration 365/1000 | Loss: 0.00001464
Iteration 366/1000 | Loss: 0.00001464
Iteration 367/1000 | Loss: 0.00001464
Iteration 368/1000 | Loss: 0.00001464
Iteration 369/1000 | Loss: 0.00001464
Iteration 370/1000 | Loss: 0.00001464
Iteration 371/1000 | Loss: 0.00001464
Iteration 372/1000 | Loss: 0.00001464
Iteration 373/1000 | Loss: 0.00001464
Iteration 374/1000 | Loss: 0.00001464
Iteration 375/1000 | Loss: 0.00001464
Iteration 376/1000 | Loss: 0.00001463
Iteration 377/1000 | Loss: 0.00001463
Iteration 378/1000 | Loss: 0.00001463
Iteration 379/1000 | Loss: 0.00001463
Iteration 380/1000 | Loss: 0.00001463
Iteration 381/1000 | Loss: 0.00001463
Iteration 382/1000 | Loss: 0.00001463
Iteration 383/1000 | Loss: 0.00001463
Iteration 384/1000 | Loss: 0.00001463
Iteration 385/1000 | Loss: 0.00001463
Iteration 386/1000 | Loss: 0.00001463
Iteration 387/1000 | Loss: 0.00001462
Iteration 388/1000 | Loss: 0.00001462
Iteration 389/1000 | Loss: 0.00001462
Iteration 390/1000 | Loss: 0.00001462
Iteration 391/1000 | Loss: 0.00001462
Iteration 392/1000 | Loss: 0.00001462
Iteration 393/1000 | Loss: 0.00001462
Iteration 394/1000 | Loss: 0.00001462
Iteration 395/1000 | Loss: 0.00001462
Iteration 396/1000 | Loss: 0.00001462
Iteration 397/1000 | Loss: 0.00001461
Iteration 398/1000 | Loss: 0.00001461
Iteration 399/1000 | Loss: 0.00001461
Iteration 400/1000 | Loss: 0.00001461
Iteration 401/1000 | Loss: 0.00001461
Iteration 402/1000 | Loss: 0.00001461
Iteration 403/1000 | Loss: 0.00001461
Iteration 404/1000 | Loss: 0.00001461
Iteration 405/1000 | Loss: 0.00001461
Iteration 406/1000 | Loss: 0.00001461
Iteration 407/1000 | Loss: 0.00001461
Iteration 408/1000 | Loss: 0.00001461
Iteration 409/1000 | Loss: 0.00001461
Iteration 410/1000 | Loss: 0.00001461
Iteration 411/1000 | Loss: 0.00001460
Iteration 412/1000 | Loss: 0.00001460
Iteration 413/1000 | Loss: 0.00001460
Iteration 414/1000 | Loss: 0.00001460
Iteration 415/1000 | Loss: 0.00001460
Iteration 416/1000 | Loss: 0.00001460
Iteration 417/1000 | Loss: 0.00001460
Iteration 418/1000 | Loss: 0.00001460
Iteration 419/1000 | Loss: 0.00001460
Iteration 420/1000 | Loss: 0.00001460
Iteration 421/1000 | Loss: 0.00001460
Iteration 422/1000 | Loss: 0.00001460
Iteration 423/1000 | Loss: 0.00001460
Iteration 424/1000 | Loss: 0.00001460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 424. Stopping optimization.
Last 5 losses: [1.4603806448576506e-05, 1.4603806448576506e-05, 1.4603806448576506e-05, 1.4603806448576506e-05, 1.4603806448576506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4603806448576506e-05

Optimization complete. Final v2v error: 3.1485326290130615 mm

Highest mean error: 4.829689025878906 mm for frame 49

Lowest mean error: 2.730461835861206 mm for frame 31

Saving results

Total time: 234.5888533592224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545008
Iteration 2/25 | Loss: 0.00112988
Iteration 3/25 | Loss: 0.00087229
Iteration 4/25 | Loss: 0.00083402
Iteration 5/25 | Loss: 0.00082403
Iteration 6/25 | Loss: 0.00082201
Iteration 7/25 | Loss: 0.00082174
Iteration 8/25 | Loss: 0.00082174
Iteration 9/25 | Loss: 0.00082174
Iteration 10/25 | Loss: 0.00082174
Iteration 11/25 | Loss: 0.00082174
Iteration 12/25 | Loss: 0.00082174
Iteration 13/25 | Loss: 0.00082174
Iteration 14/25 | Loss: 0.00082174
Iteration 15/25 | Loss: 0.00082174
Iteration 16/25 | Loss: 0.00082174
Iteration 17/25 | Loss: 0.00082174
Iteration 18/25 | Loss: 0.00082174
Iteration 19/25 | Loss: 0.00082174
Iteration 20/25 | Loss: 0.00082174
Iteration 21/25 | Loss: 0.00082174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008217424619942904, 0.0008217424619942904, 0.0008217424619942904, 0.0008217424619942904, 0.0008217424619942904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008217424619942904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54135406
Iteration 2/25 | Loss: 0.00084835
Iteration 3/25 | Loss: 0.00084833
Iteration 4/25 | Loss: 0.00084833
Iteration 5/25 | Loss: 0.00084833
Iteration 6/25 | Loss: 0.00084833
Iteration 7/25 | Loss: 0.00084833
Iteration 8/25 | Loss: 0.00084833
Iteration 9/25 | Loss: 0.00084833
Iteration 10/25 | Loss: 0.00084833
Iteration 11/25 | Loss: 0.00084833
Iteration 12/25 | Loss: 0.00084833
Iteration 13/25 | Loss: 0.00084833
Iteration 14/25 | Loss: 0.00084833
Iteration 15/25 | Loss: 0.00084833
Iteration 16/25 | Loss: 0.00084833
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008483259007334709, 0.0008483259007334709, 0.0008483259007334709, 0.0008483259007334709, 0.0008483259007334709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008483259007334709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084833
Iteration 2/1000 | Loss: 0.00004403
Iteration 3/1000 | Loss: 0.00003330
Iteration 4/1000 | Loss: 0.00003020
Iteration 5/1000 | Loss: 0.00002829
Iteration 6/1000 | Loss: 0.00002711
Iteration 7/1000 | Loss: 0.00002617
Iteration 8/1000 | Loss: 0.00002535
Iteration 9/1000 | Loss: 0.00002504
Iteration 10/1000 | Loss: 0.00002477
Iteration 11/1000 | Loss: 0.00002449
Iteration 12/1000 | Loss: 0.00002443
Iteration 13/1000 | Loss: 0.00002440
Iteration 14/1000 | Loss: 0.00002419
Iteration 15/1000 | Loss: 0.00002419
Iteration 16/1000 | Loss: 0.00002419
Iteration 17/1000 | Loss: 0.00002419
Iteration 18/1000 | Loss: 0.00002417
Iteration 19/1000 | Loss: 0.00002416
Iteration 20/1000 | Loss: 0.00002415
Iteration 21/1000 | Loss: 0.00002412
Iteration 22/1000 | Loss: 0.00002412
Iteration 23/1000 | Loss: 0.00002409
Iteration 24/1000 | Loss: 0.00002409
Iteration 25/1000 | Loss: 0.00002408
Iteration 26/1000 | Loss: 0.00002408
Iteration 27/1000 | Loss: 0.00002407
Iteration 28/1000 | Loss: 0.00002407
Iteration 29/1000 | Loss: 0.00002407
Iteration 30/1000 | Loss: 0.00002406
Iteration 31/1000 | Loss: 0.00002406
Iteration 32/1000 | Loss: 0.00002406
Iteration 33/1000 | Loss: 0.00002406
Iteration 34/1000 | Loss: 0.00002406
Iteration 35/1000 | Loss: 0.00002406
Iteration 36/1000 | Loss: 0.00002406
Iteration 37/1000 | Loss: 0.00002406
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002406
Iteration 40/1000 | Loss: 0.00002406
Iteration 41/1000 | Loss: 0.00002406
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002406
Iteration 44/1000 | Loss: 0.00002406
Iteration 45/1000 | Loss: 0.00002406
Iteration 46/1000 | Loss: 0.00002406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 46. Stopping optimization.
Last 5 losses: [2.406331623205915e-05, 2.406331623205915e-05, 2.406331623205915e-05, 2.406331623205915e-05, 2.406331623205915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.406331623205915e-05

Optimization complete. Final v2v error: 4.10532808303833 mm

Highest mean error: 4.370873928070068 mm for frame 49

Lowest mean error: 3.8292694091796875 mm for frame 130

Saving results

Total time: 30.27628207206726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059645
Iteration 2/25 | Loss: 0.00181770
Iteration 3/25 | Loss: 0.00113789
Iteration 4/25 | Loss: 0.00097562
Iteration 5/25 | Loss: 0.00092463
Iteration 6/25 | Loss: 0.00089270
Iteration 7/25 | Loss: 0.00087600
Iteration 8/25 | Loss: 0.00087206
Iteration 9/25 | Loss: 0.00087312
Iteration 10/25 | Loss: 0.00088082
Iteration 11/25 | Loss: 0.00087486
Iteration 12/25 | Loss: 0.00086636
Iteration 13/25 | Loss: 0.00085707
Iteration 14/25 | Loss: 0.00084894
Iteration 15/25 | Loss: 0.00084474
Iteration 16/25 | Loss: 0.00084295
Iteration 17/25 | Loss: 0.00083964
Iteration 18/25 | Loss: 0.00083775
Iteration 19/25 | Loss: 0.00083777
Iteration 20/25 | Loss: 0.00083846
Iteration 21/25 | Loss: 0.00083597
Iteration 22/25 | Loss: 0.00083551
Iteration 23/25 | Loss: 0.00083529
Iteration 24/25 | Loss: 0.00083506
Iteration 25/25 | Loss: 0.00083474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53643823
Iteration 2/25 | Loss: 0.00107775
Iteration 3/25 | Loss: 0.00107775
Iteration 4/25 | Loss: 0.00107775
Iteration 5/25 | Loss: 0.00107775
Iteration 6/25 | Loss: 0.00107775
Iteration 7/25 | Loss: 0.00107775
Iteration 8/25 | Loss: 0.00107775
Iteration 9/25 | Loss: 0.00107775
Iteration 10/25 | Loss: 0.00107775
Iteration 11/25 | Loss: 0.00107775
Iteration 12/25 | Loss: 0.00107775
Iteration 13/25 | Loss: 0.00107775
Iteration 14/25 | Loss: 0.00107775
Iteration 15/25 | Loss: 0.00107775
Iteration 16/25 | Loss: 0.00107775
Iteration 17/25 | Loss: 0.00107775
Iteration 18/25 | Loss: 0.00107775
Iteration 19/25 | Loss: 0.00107775
Iteration 20/25 | Loss: 0.00107775
Iteration 21/25 | Loss: 0.00107775
Iteration 22/25 | Loss: 0.00107775
Iteration 23/25 | Loss: 0.00107775
Iteration 24/25 | Loss: 0.00107775
Iteration 25/25 | Loss: 0.00107775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107775
Iteration 2/1000 | Loss: 0.00004669
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00003499
Iteration 5/1000 | Loss: 0.00002353
Iteration 6/1000 | Loss: 0.00003639
Iteration 7/1000 | Loss: 0.00017095
Iteration 8/1000 | Loss: 0.00004146
Iteration 9/1000 | Loss: 0.00003397
Iteration 10/1000 | Loss: 0.00003816
Iteration 11/1000 | Loss: 0.00003752
Iteration 12/1000 | Loss: 0.00003777
Iteration 13/1000 | Loss: 0.00002660
Iteration 14/1000 | Loss: 0.00003324
Iteration 15/1000 | Loss: 0.00003446
Iteration 16/1000 | Loss: 0.00003899
Iteration 17/1000 | Loss: 0.00003150
Iteration 18/1000 | Loss: 0.00003724
Iteration 19/1000 | Loss: 0.00002950
Iteration 20/1000 | Loss: 0.00003559
Iteration 21/1000 | Loss: 0.00002959
Iteration 22/1000 | Loss: 0.00003517
Iteration 23/1000 | Loss: 0.00003541
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002569
Iteration 26/1000 | Loss: 0.00002285
Iteration 27/1000 | Loss: 0.00003076
Iteration 28/1000 | Loss: 0.00003376
Iteration 29/1000 | Loss: 0.00003318
Iteration 30/1000 | Loss: 0.00003749
Iteration 31/1000 | Loss: 0.00003033
Iteration 32/1000 | Loss: 0.00003516
Iteration 33/1000 | Loss: 0.00003430
Iteration 34/1000 | Loss: 0.00003879
Iteration 35/1000 | Loss: 0.00003396
Iteration 36/1000 | Loss: 0.00004920
Iteration 37/1000 | Loss: 0.00003707
Iteration 38/1000 | Loss: 0.00002563
Iteration 39/1000 | Loss: 0.00003861
Iteration 40/1000 | Loss: 0.00004489
Iteration 41/1000 | Loss: 0.00004912
Iteration 42/1000 | Loss: 0.00002295
Iteration 43/1000 | Loss: 0.00001961
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001751
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001725
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001723
Iteration 54/1000 | Loss: 0.00001720
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001704
Iteration 74/1000 | Loss: 0.00001704
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001700
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001700
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001696
Iteration 94/1000 | Loss: 0.00001696
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001693
Iteration 97/1000 | Loss: 0.00001693
Iteration 98/1000 | Loss: 0.00001692
Iteration 99/1000 | Loss: 0.00001692
Iteration 100/1000 | Loss: 0.00001692
Iteration 101/1000 | Loss: 0.00001692
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001691
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001688
Iteration 123/1000 | Loss: 0.00001688
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001684
Iteration 141/1000 | Loss: 0.00001684
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001683
Iteration 147/1000 | Loss: 0.00001683
Iteration 148/1000 | Loss: 0.00001683
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001683
Iteration 151/1000 | Loss: 0.00001683
Iteration 152/1000 | Loss: 0.00001683
Iteration 153/1000 | Loss: 0.00001683
Iteration 154/1000 | Loss: 0.00001683
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001682
Iteration 161/1000 | Loss: 0.00001682
Iteration 162/1000 | Loss: 0.00001682
Iteration 163/1000 | Loss: 0.00001682
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001681
Iteration 171/1000 | Loss: 0.00001681
Iteration 172/1000 | Loss: 0.00001681
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001681
Iteration 177/1000 | Loss: 0.00001681
Iteration 178/1000 | Loss: 0.00001681
Iteration 179/1000 | Loss: 0.00001681
Iteration 180/1000 | Loss: 0.00001681
Iteration 181/1000 | Loss: 0.00001681
Iteration 182/1000 | Loss: 0.00001681
Iteration 183/1000 | Loss: 0.00001681
Iteration 184/1000 | Loss: 0.00001680
Iteration 185/1000 | Loss: 0.00001680
Iteration 186/1000 | Loss: 0.00001680
Iteration 187/1000 | Loss: 0.00001680
Iteration 188/1000 | Loss: 0.00001680
Iteration 189/1000 | Loss: 0.00001679
Iteration 190/1000 | Loss: 0.00001679
Iteration 191/1000 | Loss: 0.00001679
Iteration 192/1000 | Loss: 0.00001679
Iteration 193/1000 | Loss: 0.00001679
Iteration 194/1000 | Loss: 0.00001679
Iteration 195/1000 | Loss: 0.00001679
Iteration 196/1000 | Loss: 0.00001679
Iteration 197/1000 | Loss: 0.00001679
Iteration 198/1000 | Loss: 0.00001679
Iteration 199/1000 | Loss: 0.00001679
Iteration 200/1000 | Loss: 0.00001679
Iteration 201/1000 | Loss: 0.00001679
Iteration 202/1000 | Loss: 0.00001679
Iteration 203/1000 | Loss: 0.00001679
Iteration 204/1000 | Loss: 0.00001679
Iteration 205/1000 | Loss: 0.00001679
Iteration 206/1000 | Loss: 0.00001679
Iteration 207/1000 | Loss: 0.00001679
Iteration 208/1000 | Loss: 0.00001679
Iteration 209/1000 | Loss: 0.00001679
Iteration 210/1000 | Loss: 0.00001679
Iteration 211/1000 | Loss: 0.00001679
Iteration 212/1000 | Loss: 0.00001679
Iteration 213/1000 | Loss: 0.00001679
Iteration 214/1000 | Loss: 0.00001679
Iteration 215/1000 | Loss: 0.00001679
Iteration 216/1000 | Loss: 0.00001679
Iteration 217/1000 | Loss: 0.00001679
Iteration 218/1000 | Loss: 0.00001679
Iteration 219/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.6789537767181173e-05, 1.6789537767181173e-05, 1.6789537767181173e-05, 1.6789537767181173e-05, 1.6789537767181173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6789537767181173e-05

Optimization complete. Final v2v error: 3.4896275997161865 mm

Highest mean error: 4.422341346740723 mm for frame 1

Lowest mean error: 3.319317579269409 mm for frame 13

Saving results

Total time: 144.34196162223816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055645
Iteration 2/25 | Loss: 0.00275536
Iteration 3/25 | Loss: 0.00194046
Iteration 4/25 | Loss: 0.00165257
Iteration 5/25 | Loss: 0.00155488
Iteration 6/25 | Loss: 0.00155302
Iteration 7/25 | Loss: 0.00145726
Iteration 8/25 | Loss: 0.00134371
Iteration 9/25 | Loss: 0.00128362
Iteration 10/25 | Loss: 0.00123577
Iteration 11/25 | Loss: 0.00116755
Iteration 12/25 | Loss: 0.00109935
Iteration 13/25 | Loss: 0.00106703
Iteration 14/25 | Loss: 0.00105248
Iteration 15/25 | Loss: 0.00104386
Iteration 16/25 | Loss: 0.00103175
Iteration 17/25 | Loss: 0.00101602
Iteration 18/25 | Loss: 0.00101087
Iteration 19/25 | Loss: 0.00101276
Iteration 20/25 | Loss: 0.00102163
Iteration 21/25 | Loss: 0.00101674
Iteration 22/25 | Loss: 0.00101502
Iteration 23/25 | Loss: 0.00100267
Iteration 24/25 | Loss: 0.00100745
Iteration 25/25 | Loss: 0.00099788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56154060
Iteration 2/25 | Loss: 0.00424476
Iteration 3/25 | Loss: 0.00310137
Iteration 4/25 | Loss: 0.00310137
Iteration 5/25 | Loss: 0.00310137
Iteration 6/25 | Loss: 0.00310137
Iteration 7/25 | Loss: 0.00310137
Iteration 8/25 | Loss: 0.00310137
Iteration 9/25 | Loss: 0.00310137
Iteration 10/25 | Loss: 0.00310137
Iteration 11/25 | Loss: 0.00310137
Iteration 12/25 | Loss: 0.00310137
Iteration 13/25 | Loss: 0.00310137
Iteration 14/25 | Loss: 0.00310137
Iteration 15/25 | Loss: 0.00310137
Iteration 16/25 | Loss: 0.00310137
Iteration 17/25 | Loss: 0.00310137
Iteration 18/25 | Loss: 0.00310137
Iteration 19/25 | Loss: 0.00310137
Iteration 20/25 | Loss: 0.00310137
Iteration 21/25 | Loss: 0.00310137
Iteration 22/25 | Loss: 0.00310137
Iteration 23/25 | Loss: 0.00310137
Iteration 24/25 | Loss: 0.00310137
Iteration 25/25 | Loss: 0.00310137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00310137
Iteration 2/1000 | Loss: 0.00279839
Iteration 3/1000 | Loss: 0.00084587
Iteration 4/1000 | Loss: 0.00098170
Iteration 5/1000 | Loss: 0.00095001
Iteration 6/1000 | Loss: 0.00025446
Iteration 7/1000 | Loss: 0.00060011
Iteration 8/1000 | Loss: 0.00026100
Iteration 9/1000 | Loss: 0.00034196
Iteration 10/1000 | Loss: 0.00026319
Iteration 11/1000 | Loss: 0.00022815
Iteration 12/1000 | Loss: 0.00019938
Iteration 13/1000 | Loss: 0.00014047
Iteration 14/1000 | Loss: 0.00040185
Iteration 15/1000 | Loss: 0.00446950
Iteration 16/1000 | Loss: 0.00183995
Iteration 17/1000 | Loss: 0.00097227
Iteration 18/1000 | Loss: 0.00034141
Iteration 19/1000 | Loss: 0.00213685
Iteration 20/1000 | Loss: 0.00145415
Iteration 21/1000 | Loss: 0.00319086
Iteration 22/1000 | Loss: 0.00352880
Iteration 23/1000 | Loss: 0.00207206
Iteration 24/1000 | Loss: 0.00294403
Iteration 25/1000 | Loss: 0.00078788
Iteration 26/1000 | Loss: 0.00180563
Iteration 27/1000 | Loss: 0.00094513
Iteration 28/1000 | Loss: 0.00244487
Iteration 29/1000 | Loss: 0.00081251
Iteration 30/1000 | Loss: 0.00066363
Iteration 31/1000 | Loss: 0.00116021
Iteration 32/1000 | Loss: 0.00039302
Iteration 33/1000 | Loss: 0.00086143
Iteration 34/1000 | Loss: 0.00014949
Iteration 35/1000 | Loss: 0.00079936
Iteration 36/1000 | Loss: 0.00008682
Iteration 37/1000 | Loss: 0.00031113
Iteration 38/1000 | Loss: 0.00165649
Iteration 39/1000 | Loss: 0.00056850
Iteration 40/1000 | Loss: 0.00078367
Iteration 41/1000 | Loss: 0.00027203
Iteration 42/1000 | Loss: 0.00008314
Iteration 43/1000 | Loss: 0.00010180
Iteration 44/1000 | Loss: 0.00088838
Iteration 45/1000 | Loss: 0.00007639
Iteration 46/1000 | Loss: 0.00015188
Iteration 47/1000 | Loss: 0.00034618
Iteration 48/1000 | Loss: 0.00023547
Iteration 49/1000 | Loss: 0.00016697
Iteration 50/1000 | Loss: 0.00009244
Iteration 51/1000 | Loss: 0.00012724
Iteration 52/1000 | Loss: 0.00014159
Iteration 53/1000 | Loss: 0.00019706
Iteration 54/1000 | Loss: 0.00019895
Iteration 55/1000 | Loss: 0.00034221
Iteration 56/1000 | Loss: 0.00025190
Iteration 57/1000 | Loss: 0.00054090
Iteration 58/1000 | Loss: 0.00014264
Iteration 59/1000 | Loss: 0.00006410
Iteration 60/1000 | Loss: 0.00007766
Iteration 61/1000 | Loss: 0.00008373
Iteration 62/1000 | Loss: 0.00008353
Iteration 63/1000 | Loss: 0.00039287
Iteration 64/1000 | Loss: 0.00202290
Iteration 65/1000 | Loss: 0.00078351
Iteration 66/1000 | Loss: 0.00064266
Iteration 67/1000 | Loss: 0.00038297
Iteration 68/1000 | Loss: 0.00058786
Iteration 69/1000 | Loss: 0.00039332
Iteration 70/1000 | Loss: 0.00019252
Iteration 71/1000 | Loss: 0.00017517
Iteration 72/1000 | Loss: 0.00008372
Iteration 73/1000 | Loss: 0.00007373
Iteration 74/1000 | Loss: 0.00005565
Iteration 75/1000 | Loss: 0.00036796
Iteration 76/1000 | Loss: 0.00023083
Iteration 77/1000 | Loss: 0.00036478
Iteration 78/1000 | Loss: 0.00023987
Iteration 79/1000 | Loss: 0.00021957
Iteration 80/1000 | Loss: 0.00006240
Iteration 81/1000 | Loss: 0.00022433
Iteration 82/1000 | Loss: 0.00021498
Iteration 83/1000 | Loss: 0.00005809
Iteration 84/1000 | Loss: 0.00008229
Iteration 85/1000 | Loss: 0.00005670
Iteration 86/1000 | Loss: 0.00007257
Iteration 87/1000 | Loss: 0.00005032
Iteration 88/1000 | Loss: 0.00005762
Iteration 89/1000 | Loss: 0.00006837
Iteration 90/1000 | Loss: 0.00007009
Iteration 91/1000 | Loss: 0.00007000
Iteration 92/1000 | Loss: 0.00007829
Iteration 93/1000 | Loss: 0.00008171
Iteration 94/1000 | Loss: 0.00028882
Iteration 95/1000 | Loss: 0.00018071
Iteration 96/1000 | Loss: 0.00033303
Iteration 97/1000 | Loss: 0.00026855
Iteration 98/1000 | Loss: 0.00022576
Iteration 99/1000 | Loss: 0.00028823
Iteration 100/1000 | Loss: 0.00019916
Iteration 101/1000 | Loss: 0.00007158
Iteration 102/1000 | Loss: 0.00015290
Iteration 103/1000 | Loss: 0.00006549
Iteration 104/1000 | Loss: 0.00014674
Iteration 105/1000 | Loss: 0.00018498
Iteration 106/1000 | Loss: 0.00025123
Iteration 107/1000 | Loss: 0.00013794
Iteration 108/1000 | Loss: 0.00014596
Iteration 109/1000 | Loss: 0.00008191
Iteration 110/1000 | Loss: 0.00052726
Iteration 111/1000 | Loss: 0.00024218
Iteration 112/1000 | Loss: 0.00047510
Iteration 113/1000 | Loss: 0.00024315
Iteration 114/1000 | Loss: 0.00048069
Iteration 115/1000 | Loss: 0.00031780
Iteration 116/1000 | Loss: 0.00040977
Iteration 117/1000 | Loss: 0.00031816
Iteration 118/1000 | Loss: 0.00006181
Iteration 119/1000 | Loss: 0.00006872
Iteration 120/1000 | Loss: 0.00030084
Iteration 121/1000 | Loss: 0.00026921
Iteration 122/1000 | Loss: 0.00024444
Iteration 123/1000 | Loss: 0.00005628
Iteration 124/1000 | Loss: 0.00011530
Iteration 125/1000 | Loss: 0.00009740
Iteration 126/1000 | Loss: 0.00008347
Iteration 127/1000 | Loss: 0.00048304
Iteration 128/1000 | Loss: 0.00008814
Iteration 129/1000 | Loss: 0.00016509
Iteration 130/1000 | Loss: 0.00008454
Iteration 131/1000 | Loss: 0.00041343
Iteration 132/1000 | Loss: 0.00022977
Iteration 133/1000 | Loss: 0.00014323
Iteration 134/1000 | Loss: 0.00007213
Iteration 135/1000 | Loss: 0.00006098
Iteration 136/1000 | Loss: 0.00009118
Iteration 137/1000 | Loss: 0.00012850
Iteration 138/1000 | Loss: 0.00005310
Iteration 139/1000 | Loss: 0.00008311
Iteration 140/1000 | Loss: 0.00005291
Iteration 141/1000 | Loss: 0.00007990
Iteration 142/1000 | Loss: 0.00006133
Iteration 143/1000 | Loss: 0.00005466
Iteration 144/1000 | Loss: 0.00006562
Iteration 145/1000 | Loss: 0.00043833
Iteration 146/1000 | Loss: 0.00027447
Iteration 147/1000 | Loss: 0.00052151
Iteration 148/1000 | Loss: 0.00053600
Iteration 149/1000 | Loss: 0.00018511
Iteration 150/1000 | Loss: 0.00046797
Iteration 151/1000 | Loss: 0.00007088
Iteration 152/1000 | Loss: 0.00007452
Iteration 153/1000 | Loss: 0.00005403
Iteration 154/1000 | Loss: 0.00023225
Iteration 155/1000 | Loss: 0.00009347
Iteration 156/1000 | Loss: 0.00019617
Iteration 157/1000 | Loss: 0.00014269
Iteration 158/1000 | Loss: 0.00020970
Iteration 159/1000 | Loss: 0.00022300
Iteration 160/1000 | Loss: 0.00012162
Iteration 161/1000 | Loss: 0.00004561
Iteration 162/1000 | Loss: 0.00026391
Iteration 163/1000 | Loss: 0.00023547
Iteration 164/1000 | Loss: 0.00023039
Iteration 165/1000 | Loss: 0.00015615
Iteration 166/1000 | Loss: 0.00023605
Iteration 167/1000 | Loss: 0.00025875
Iteration 168/1000 | Loss: 0.00022862
Iteration 169/1000 | Loss: 0.00020145
Iteration 170/1000 | Loss: 0.00021423
Iteration 171/1000 | Loss: 0.00016916
Iteration 172/1000 | Loss: 0.00017576
Iteration 173/1000 | Loss: 0.00017482
Iteration 174/1000 | Loss: 0.00015106
Iteration 175/1000 | Loss: 0.00018390
Iteration 176/1000 | Loss: 0.00008997
Iteration 177/1000 | Loss: 0.00003906
Iteration 178/1000 | Loss: 0.00015102
Iteration 179/1000 | Loss: 0.00019562
Iteration 180/1000 | Loss: 0.00007485
Iteration 181/1000 | Loss: 0.00033298
Iteration 182/1000 | Loss: 0.00007658
Iteration 183/1000 | Loss: 0.00005514
Iteration 184/1000 | Loss: 0.00006999
Iteration 185/1000 | Loss: 0.00006926
Iteration 186/1000 | Loss: 0.00024353
Iteration 187/1000 | Loss: 0.00017248
Iteration 188/1000 | Loss: 0.00015806
Iteration 189/1000 | Loss: 0.00009464
Iteration 190/1000 | Loss: 0.00006559
Iteration 191/1000 | Loss: 0.00006813
Iteration 192/1000 | Loss: 0.00006398
Iteration 193/1000 | Loss: 0.00004505
Iteration 194/1000 | Loss: 0.00010396
Iteration 195/1000 | Loss: 0.00005181
Iteration 196/1000 | Loss: 0.00021367
Iteration 197/1000 | Loss: 0.00049696
Iteration 198/1000 | Loss: 0.00012177
Iteration 199/1000 | Loss: 0.00036575
Iteration 200/1000 | Loss: 0.00008329
Iteration 201/1000 | Loss: 0.00007070
Iteration 202/1000 | Loss: 0.00006892
Iteration 203/1000 | Loss: 0.00011809
Iteration 204/1000 | Loss: 0.00005513
Iteration 205/1000 | Loss: 0.00004476
Iteration 206/1000 | Loss: 0.00005582
Iteration 207/1000 | Loss: 0.00003920
Iteration 208/1000 | Loss: 0.00004259
Iteration 209/1000 | Loss: 0.00006321
Iteration 210/1000 | Loss: 0.00006002
Iteration 211/1000 | Loss: 0.00008447
Iteration 212/1000 | Loss: 0.00006252
Iteration 213/1000 | Loss: 0.00005642
Iteration 214/1000 | Loss: 0.00005353
Iteration 215/1000 | Loss: 0.00007686
Iteration 216/1000 | Loss: 0.00009080
Iteration 217/1000 | Loss: 0.00007683
Iteration 218/1000 | Loss: 0.00006340
Iteration 219/1000 | Loss: 0.00006624
Iteration 220/1000 | Loss: 0.00010087
Iteration 221/1000 | Loss: 0.00008741
Iteration 222/1000 | Loss: 0.00014804
Iteration 223/1000 | Loss: 0.00012167
Iteration 224/1000 | Loss: 0.00009257
Iteration 225/1000 | Loss: 0.00017089
Iteration 226/1000 | Loss: 0.00012455
Iteration 227/1000 | Loss: 0.00007782
Iteration 228/1000 | Loss: 0.00022598
Iteration 229/1000 | Loss: 0.00018667
Iteration 230/1000 | Loss: 0.00022525
Iteration 231/1000 | Loss: 0.00019808
Iteration 232/1000 | Loss: 0.00018308
Iteration 233/1000 | Loss: 0.00003966
Iteration 234/1000 | Loss: 0.00006820
Iteration 235/1000 | Loss: 0.00021567
Iteration 236/1000 | Loss: 0.00017300
Iteration 237/1000 | Loss: 0.00020864
Iteration 238/1000 | Loss: 0.00007080
Iteration 239/1000 | Loss: 0.00011311
Iteration 240/1000 | Loss: 0.00008494
Iteration 241/1000 | Loss: 0.00006509
Iteration 242/1000 | Loss: 0.00009213
Iteration 243/1000 | Loss: 0.00006306
Iteration 244/1000 | Loss: 0.00004447
Iteration 245/1000 | Loss: 0.00007737
Iteration 246/1000 | Loss: 0.00014730
Iteration 247/1000 | Loss: 0.00004982
Iteration 248/1000 | Loss: 0.00008323
Iteration 249/1000 | Loss: 0.00007620
Iteration 250/1000 | Loss: 0.00004787
Iteration 251/1000 | Loss: 0.00005471
Iteration 252/1000 | Loss: 0.00005827
Iteration 253/1000 | Loss: 0.00003356
Iteration 254/1000 | Loss: 0.00003867
Iteration 255/1000 | Loss: 0.00005684
Iteration 256/1000 | Loss: 0.00004662
Iteration 257/1000 | Loss: 0.00004835
Iteration 258/1000 | Loss: 0.00005310
Iteration 259/1000 | Loss: 0.00004724
Iteration 260/1000 | Loss: 0.00070742
Iteration 261/1000 | Loss: 0.00006733
Iteration 262/1000 | Loss: 0.00009197
Iteration 263/1000 | Loss: 0.00006061
Iteration 264/1000 | Loss: 0.00007804
Iteration 265/1000 | Loss: 0.00009581
Iteration 266/1000 | Loss: 0.00007748
Iteration 267/1000 | Loss: 0.00006104
Iteration 268/1000 | Loss: 0.00005273
Iteration 269/1000 | Loss: 0.00004456
Iteration 270/1000 | Loss: 0.00005802
Iteration 271/1000 | Loss: 0.00013263
Iteration 272/1000 | Loss: 0.00011480
Iteration 273/1000 | Loss: 0.00019741
Iteration 274/1000 | Loss: 0.00006580
Iteration 275/1000 | Loss: 0.00007702
Iteration 276/1000 | Loss: 0.00007023
Iteration 277/1000 | Loss: 0.00007546
Iteration 278/1000 | Loss: 0.00007041
Iteration 279/1000 | Loss: 0.00003601
Iteration 280/1000 | Loss: 0.00005057
Iteration 281/1000 | Loss: 0.00006425
Iteration 282/1000 | Loss: 0.00006000
Iteration 283/1000 | Loss: 0.00005716
Iteration 284/1000 | Loss: 0.00006543
Iteration 285/1000 | Loss: 0.00005816
Iteration 286/1000 | Loss: 0.00007227
Iteration 287/1000 | Loss: 0.00007874
Iteration 288/1000 | Loss: 0.00007380
Iteration 289/1000 | Loss: 0.00006197
Iteration 290/1000 | Loss: 0.00052135
Iteration 291/1000 | Loss: 0.00015759
Iteration 292/1000 | Loss: 0.00033069
Iteration 293/1000 | Loss: 0.00030032
Iteration 294/1000 | Loss: 0.00007369
Iteration 295/1000 | Loss: 0.00004737
Iteration 296/1000 | Loss: 0.00004991
Iteration 297/1000 | Loss: 0.00003730
Iteration 298/1000 | Loss: 0.00005278
Iteration 299/1000 | Loss: 0.00006226
Iteration 300/1000 | Loss: 0.00005492
Iteration 301/1000 | Loss: 0.00005577
Iteration 302/1000 | Loss: 0.00007094
Iteration 303/1000 | Loss: 0.00005636
Iteration 304/1000 | Loss: 0.00003365
Iteration 305/1000 | Loss: 0.00002853
Iteration 306/1000 | Loss: 0.00003524
Iteration 307/1000 | Loss: 0.00007507
Iteration 308/1000 | Loss: 0.00004257
Iteration 309/1000 | Loss: 0.00005033
Iteration 310/1000 | Loss: 0.00003219
Iteration 311/1000 | Loss: 0.00004318
Iteration 312/1000 | Loss: 0.00003566
Iteration 313/1000 | Loss: 0.00003410
Iteration 314/1000 | Loss: 0.00002831
Iteration 315/1000 | Loss: 0.00002678
Iteration 316/1000 | Loss: 0.00004791
Iteration 317/1000 | Loss: 0.00004391
Iteration 318/1000 | Loss: 0.00004613
Iteration 319/1000 | Loss: 0.00005090
Iteration 320/1000 | Loss: 0.00005065
Iteration 321/1000 | Loss: 0.00004834
Iteration 322/1000 | Loss: 0.00014268
Iteration 323/1000 | Loss: 0.00004095
Iteration 324/1000 | Loss: 0.00003710
Iteration 325/1000 | Loss: 0.00004888
Iteration 326/1000 | Loss: 0.00004115
Iteration 327/1000 | Loss: 0.00003678
Iteration 328/1000 | Loss: 0.00004220
Iteration 329/1000 | Loss: 0.00004381
Iteration 330/1000 | Loss: 0.00004161
Iteration 331/1000 | Loss: 0.00004791
Iteration 332/1000 | Loss: 0.00003886
Iteration 333/1000 | Loss: 0.00005513
Iteration 334/1000 | Loss: 0.00011508
Iteration 335/1000 | Loss: 0.00005006
Iteration 336/1000 | Loss: 0.00004374
Iteration 337/1000 | Loss: 0.00004543
Iteration 338/1000 | Loss: 0.00004627
Iteration 339/1000 | Loss: 0.00005062
Iteration 340/1000 | Loss: 0.00003267
Iteration 341/1000 | Loss: 0.00005562
Iteration 342/1000 | Loss: 0.00004070
Iteration 343/1000 | Loss: 0.00003316
Iteration 344/1000 | Loss: 0.00005738
Iteration 345/1000 | Loss: 0.00003035
Iteration 346/1000 | Loss: 0.00004396
Iteration 347/1000 | Loss: 0.00004429
Iteration 348/1000 | Loss: 0.00003747
Iteration 349/1000 | Loss: 0.00003428
Iteration 350/1000 | Loss: 0.00005990
Iteration 351/1000 | Loss: 0.00004311
Iteration 352/1000 | Loss: 0.00004632
Iteration 353/1000 | Loss: 0.00004313
Iteration 354/1000 | Loss: 0.00004573
Iteration 355/1000 | Loss: 0.00004323
Iteration 356/1000 | Loss: 0.00007385
Iteration 357/1000 | Loss: 0.00004242
Iteration 358/1000 | Loss: 0.00004579
Iteration 359/1000 | Loss: 0.00004843
Iteration 360/1000 | Loss: 0.00004435
Iteration 361/1000 | Loss: 0.00005961
Iteration 362/1000 | Loss: 0.00004373
Iteration 363/1000 | Loss: 0.00004154
Iteration 364/1000 | Loss: 0.00004765
Iteration 365/1000 | Loss: 0.00004348
Iteration 366/1000 | Loss: 0.00007498
Iteration 367/1000 | Loss: 0.00004093
Iteration 368/1000 | Loss: 0.00004253
Iteration 369/1000 | Loss: 0.00005748
Iteration 370/1000 | Loss: 0.00004331
Iteration 371/1000 | Loss: 0.00006045
Iteration 372/1000 | Loss: 0.00004113
Iteration 373/1000 | Loss: 0.00004477
Iteration 374/1000 | Loss: 0.00005269
Iteration 375/1000 | Loss: 0.00004672
Iteration 376/1000 | Loss: 0.00004264
Iteration 377/1000 | Loss: 0.00006744
Iteration 378/1000 | Loss: 0.00004246
Iteration 379/1000 | Loss: 0.00005161
Iteration 380/1000 | Loss: 0.00004370
Iteration 381/1000 | Loss: 0.00004764
Iteration 382/1000 | Loss: 0.00004564
Iteration 383/1000 | Loss: 0.00005317
Iteration 384/1000 | Loss: 0.00004560
Iteration 385/1000 | Loss: 0.00005411
Iteration 386/1000 | Loss: 0.00004580
Iteration 387/1000 | Loss: 0.00005456
Iteration 388/1000 | Loss: 0.00005645
Iteration 389/1000 | Loss: 0.00005593
Iteration 390/1000 | Loss: 0.00004712
Iteration 391/1000 | Loss: 0.00003769
Iteration 392/1000 | Loss: 0.00006451
Iteration 393/1000 | Loss: 0.00005498
Iteration 394/1000 | Loss: 0.00003744
Iteration 395/1000 | Loss: 0.00004973
Iteration 396/1000 | Loss: 0.00003007
Iteration 397/1000 | Loss: 0.00003631
Iteration 398/1000 | Loss: 0.00002731
Iteration 399/1000 | Loss: 0.00003000
Iteration 400/1000 | Loss: 0.00006788
Iteration 401/1000 | Loss: 0.00005294
Iteration 402/1000 | Loss: 0.00002910
Iteration 403/1000 | Loss: 0.00002529
Iteration 404/1000 | Loss: 0.00003286
Iteration 405/1000 | Loss: 0.00003540
Iteration 406/1000 | Loss: 0.00003196
Iteration 407/1000 | Loss: 0.00003174
Iteration 408/1000 | Loss: 0.00003127
Iteration 409/1000 | Loss: 0.00005000
Iteration 410/1000 | Loss: 0.00004144
Iteration 411/1000 | Loss: 0.00005083
Iteration 412/1000 | Loss: 0.00006073
Iteration 413/1000 | Loss: 0.00005704
Iteration 414/1000 | Loss: 0.00005160
Iteration 415/1000 | Loss: 0.00006080
Iteration 416/1000 | Loss: 0.00004755
Iteration 417/1000 | Loss: 0.00002563
Iteration 418/1000 | Loss: 0.00002471
Iteration 419/1000 | Loss: 0.00002042
Iteration 420/1000 | Loss: 0.00003690
Iteration 421/1000 | Loss: 0.00008027
Iteration 422/1000 | Loss: 0.00008443
Iteration 423/1000 | Loss: 0.00005706
Iteration 424/1000 | Loss: 0.00004461
Iteration 425/1000 | Loss: 0.00003595
Iteration 426/1000 | Loss: 0.00002770
Iteration 427/1000 | Loss: 0.00002603
Iteration 428/1000 | Loss: 0.00007541
Iteration 429/1000 | Loss: 0.00005335
Iteration 430/1000 | Loss: 0.00007028
Iteration 431/1000 | Loss: 0.00006069
Iteration 432/1000 | Loss: 0.00002332
Iteration 433/1000 | Loss: 0.00007519
Iteration 434/1000 | Loss: 0.00003588
Iteration 435/1000 | Loss: 0.00003631
Iteration 436/1000 | Loss: 0.00005322
Iteration 437/1000 | Loss: 0.00004848
Iteration 438/1000 | Loss: 0.00004689
Iteration 439/1000 | Loss: 0.00004482
Iteration 440/1000 | Loss: 0.00005614
Iteration 441/1000 | Loss: 0.00004600
Iteration 442/1000 | Loss: 0.00005153
Iteration 443/1000 | Loss: 0.00004026
Iteration 444/1000 | Loss: 0.00004501
Iteration 445/1000 | Loss: 0.00003765
Iteration 446/1000 | Loss: 0.00004320
Iteration 447/1000 | Loss: 0.00004417
Iteration 448/1000 | Loss: 0.00004399
Iteration 449/1000 | Loss: 0.00004990
Iteration 450/1000 | Loss: 0.00004773
Iteration 451/1000 | Loss: 0.00003873
Iteration 452/1000 | Loss: 0.00004369
Iteration 453/1000 | Loss: 0.00004432
Iteration 454/1000 | Loss: 0.00004705
Iteration 455/1000 | Loss: 0.00004008
Iteration 456/1000 | Loss: 0.00004350
Iteration 457/1000 | Loss: 0.00003962
Iteration 458/1000 | Loss: 0.00004315
Iteration 459/1000 | Loss: 0.00003863
Iteration 460/1000 | Loss: 0.00004371
Iteration 461/1000 | Loss: 0.00004523
Iteration 462/1000 | Loss: 0.00004531
Iteration 463/1000 | Loss: 0.00003905
Iteration 464/1000 | Loss: 0.00004282
Iteration 465/1000 | Loss: 0.00002900
Iteration 466/1000 | Loss: 0.00004089
Iteration 467/1000 | Loss: 0.00006551
Iteration 468/1000 | Loss: 0.00003988
Iteration 469/1000 | Loss: 0.00004146
Iteration 470/1000 | Loss: 0.00004176
Iteration 471/1000 | Loss: 0.00003804
Iteration 472/1000 | Loss: 0.00005472
Iteration 473/1000 | Loss: 0.00004131
Iteration 474/1000 | Loss: 0.00002726
Iteration 475/1000 | Loss: 0.00002336
Iteration 476/1000 | Loss: 0.00002053
Iteration 477/1000 | Loss: 0.00001932
Iteration 478/1000 | Loss: 0.00001798
Iteration 479/1000 | Loss: 0.00001688
Iteration 480/1000 | Loss: 0.00001646
Iteration 481/1000 | Loss: 0.00001620
Iteration 482/1000 | Loss: 0.00001617
Iteration 483/1000 | Loss: 0.00001596
Iteration 484/1000 | Loss: 0.00001579
Iteration 485/1000 | Loss: 0.00001566
Iteration 486/1000 | Loss: 0.00001565
Iteration 487/1000 | Loss: 0.00001564
Iteration 488/1000 | Loss: 0.00001559
Iteration 489/1000 | Loss: 0.00016040
Iteration 490/1000 | Loss: 0.00002090
Iteration 491/1000 | Loss: 0.00001910
Iteration 492/1000 | Loss: 0.00001775
Iteration 493/1000 | Loss: 0.00001697
Iteration 494/1000 | Loss: 0.00001670
Iteration 495/1000 | Loss: 0.00001662
Iteration 496/1000 | Loss: 0.00001658
Iteration 497/1000 | Loss: 0.00001657
Iteration 498/1000 | Loss: 0.00001656
Iteration 499/1000 | Loss: 0.00001655
Iteration 500/1000 | Loss: 0.00001636
Iteration 501/1000 | Loss: 0.00001634
Iteration 502/1000 | Loss: 0.00001612
Iteration 503/1000 | Loss: 0.00001592
Iteration 504/1000 | Loss: 0.00001590
Iteration 505/1000 | Loss: 0.00001590
Iteration 506/1000 | Loss: 0.00001589
Iteration 507/1000 | Loss: 0.00001585
Iteration 508/1000 | Loss: 0.00001581
Iteration 509/1000 | Loss: 0.00001579
Iteration 510/1000 | Loss: 0.00001579
Iteration 511/1000 | Loss: 0.00001578
Iteration 512/1000 | Loss: 0.00001578
Iteration 513/1000 | Loss: 0.00001578
Iteration 514/1000 | Loss: 0.00001577
Iteration 515/1000 | Loss: 0.00001576
Iteration 516/1000 | Loss: 0.00001575
Iteration 517/1000 | Loss: 0.00001575
Iteration 518/1000 | Loss: 0.00001575
Iteration 519/1000 | Loss: 0.00001574
Iteration 520/1000 | Loss: 0.00001574
Iteration 521/1000 | Loss: 0.00001574
Iteration 522/1000 | Loss: 0.00001574
Iteration 523/1000 | Loss: 0.00001573
Iteration 524/1000 | Loss: 0.00001571
Iteration 525/1000 | Loss: 0.00001571
Iteration 526/1000 | Loss: 0.00001570
Iteration 527/1000 | Loss: 0.00001569
Iteration 528/1000 | Loss: 0.00001569
Iteration 529/1000 | Loss: 0.00001569
Iteration 530/1000 | Loss: 0.00001569
Iteration 531/1000 | Loss: 0.00001569
Iteration 532/1000 | Loss: 0.00001569
Iteration 533/1000 | Loss: 0.00001568
Iteration 534/1000 | Loss: 0.00001568
Iteration 535/1000 | Loss: 0.00001568
Iteration 536/1000 | Loss: 0.00001568
Iteration 537/1000 | Loss: 0.00001568
Iteration 538/1000 | Loss: 0.00001568
Iteration 539/1000 | Loss: 0.00001568
Iteration 540/1000 | Loss: 0.00001568
Iteration 541/1000 | Loss: 0.00001567
Iteration 542/1000 | Loss: 0.00001567
Iteration 543/1000 | Loss: 0.00001567
Iteration 544/1000 | Loss: 0.00001567
Iteration 545/1000 | Loss: 0.00001567
Iteration 546/1000 | Loss: 0.00001567
Iteration 547/1000 | Loss: 0.00001566
Iteration 548/1000 | Loss: 0.00001566
Iteration 549/1000 | Loss: 0.00001566
Iteration 550/1000 | Loss: 0.00001566
Iteration 551/1000 | Loss: 0.00001565
Iteration 552/1000 | Loss: 0.00001565
Iteration 553/1000 | Loss: 0.00001565
Iteration 554/1000 | Loss: 0.00001565
Iteration 555/1000 | Loss: 0.00001565
Iteration 556/1000 | Loss: 0.00001564
Iteration 557/1000 | Loss: 0.00001564
Iteration 558/1000 | Loss: 0.00001564
Iteration 559/1000 | Loss: 0.00001563
Iteration 560/1000 | Loss: 0.00001563
Iteration 561/1000 | Loss: 0.00001563
Iteration 562/1000 | Loss: 0.00001563
Iteration 563/1000 | Loss: 0.00001563
Iteration 564/1000 | Loss: 0.00001563
Iteration 565/1000 | Loss: 0.00001563
Iteration 566/1000 | Loss: 0.00001563
Iteration 567/1000 | Loss: 0.00001563
Iteration 568/1000 | Loss: 0.00001563
Iteration 569/1000 | Loss: 0.00001563
Iteration 570/1000 | Loss: 0.00001562
Iteration 571/1000 | Loss: 0.00001562
Iteration 572/1000 | Loss: 0.00001562
Iteration 573/1000 | Loss: 0.00001562
Iteration 574/1000 | Loss: 0.00001562
Iteration 575/1000 | Loss: 0.00001562
Iteration 576/1000 | Loss: 0.00001562
Iteration 577/1000 | Loss: 0.00001562
Iteration 578/1000 | Loss: 0.00001562
Iteration 579/1000 | Loss: 0.00001561
Iteration 580/1000 | Loss: 0.00001561
Iteration 581/1000 | Loss: 0.00001561
Iteration 582/1000 | Loss: 0.00001561
Iteration 583/1000 | Loss: 0.00001561
Iteration 584/1000 | Loss: 0.00001561
Iteration 585/1000 | Loss: 0.00001560
Iteration 586/1000 | Loss: 0.00001560
Iteration 587/1000 | Loss: 0.00001560
Iteration 588/1000 | Loss: 0.00001560
Iteration 589/1000 | Loss: 0.00001559
Iteration 590/1000 | Loss: 0.00001559
Iteration 591/1000 | Loss: 0.00001559
Iteration 592/1000 | Loss: 0.00001559
Iteration 593/1000 | Loss: 0.00001559
Iteration 594/1000 | Loss: 0.00001559
Iteration 595/1000 | Loss: 0.00001558
Iteration 596/1000 | Loss: 0.00001558
Iteration 597/1000 | Loss: 0.00001558
Iteration 598/1000 | Loss: 0.00001558
Iteration 599/1000 | Loss: 0.00001558
Iteration 600/1000 | Loss: 0.00001558
Iteration 601/1000 | Loss: 0.00001558
Iteration 602/1000 | Loss: 0.00001558
Iteration 603/1000 | Loss: 0.00001557
Iteration 604/1000 | Loss: 0.00001557
Iteration 605/1000 | Loss: 0.00001557
Iteration 606/1000 | Loss: 0.00001557
Iteration 607/1000 | Loss: 0.00001557
Iteration 608/1000 | Loss: 0.00001556
Iteration 609/1000 | Loss: 0.00001556
Iteration 610/1000 | Loss: 0.00001556
Iteration 611/1000 | Loss: 0.00001556
Iteration 612/1000 | Loss: 0.00001556
Iteration 613/1000 | Loss: 0.00001556
Iteration 614/1000 | Loss: 0.00001556
Iteration 615/1000 | Loss: 0.00001556
Iteration 616/1000 | Loss: 0.00001556
Iteration 617/1000 | Loss: 0.00001556
Iteration 618/1000 | Loss: 0.00001555
Iteration 619/1000 | Loss: 0.00001555
Iteration 620/1000 | Loss: 0.00001555
Iteration 621/1000 | Loss: 0.00001555
Iteration 622/1000 | Loss: 0.00001555
Iteration 623/1000 | Loss: 0.00001555
Iteration 624/1000 | Loss: 0.00001555
Iteration 625/1000 | Loss: 0.00001555
Iteration 626/1000 | Loss: 0.00001555
Iteration 627/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 627. Stopping optimization.
Last 5 losses: [1.5552970580756664e-05, 1.5552970580756664e-05, 1.5552970580756664e-05, 1.5552970580756664e-05, 1.5552970580756664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5552970580756664e-05

Optimization complete. Final v2v error: 3.303840398788452 mm

Highest mean error: 6.904773235321045 mm for frame 12

Lowest mean error: 2.959338665008545 mm for frame 221

Saving results

Total time: 862.4122059345245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357746
Iteration 2/25 | Loss: 0.00076635
Iteration 3/25 | Loss: 0.00068796
Iteration 4/25 | Loss: 0.00067134
Iteration 5/25 | Loss: 0.00066599
Iteration 6/25 | Loss: 0.00066439
Iteration 7/25 | Loss: 0.00066386
Iteration 8/25 | Loss: 0.00066386
Iteration 9/25 | Loss: 0.00066386
Iteration 10/25 | Loss: 0.00066386
Iteration 11/25 | Loss: 0.00066386
Iteration 12/25 | Loss: 0.00066386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000663855520542711, 0.000663855520542711, 0.000663855520542711, 0.000663855520542711, 0.000663855520542711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000663855520542711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62972045
Iteration 2/25 | Loss: 0.00083118
Iteration 3/25 | Loss: 0.00083118
Iteration 4/25 | Loss: 0.00083118
Iteration 5/25 | Loss: 0.00083118
Iteration 6/25 | Loss: 0.00083117
Iteration 7/25 | Loss: 0.00083117
Iteration 8/25 | Loss: 0.00083117
Iteration 9/25 | Loss: 0.00083117
Iteration 10/25 | Loss: 0.00083117
Iteration 11/25 | Loss: 0.00083117
Iteration 12/25 | Loss: 0.00083117
Iteration 13/25 | Loss: 0.00083117
Iteration 14/25 | Loss: 0.00083117
Iteration 15/25 | Loss: 0.00083117
Iteration 16/25 | Loss: 0.00083117
Iteration 17/25 | Loss: 0.00083117
Iteration 18/25 | Loss: 0.00083117
Iteration 19/25 | Loss: 0.00083117
Iteration 20/25 | Loss: 0.00083117
Iteration 21/25 | Loss: 0.00083117
Iteration 22/25 | Loss: 0.00083117
Iteration 23/25 | Loss: 0.00083117
Iteration 24/25 | Loss: 0.00083117
Iteration 25/25 | Loss: 0.00083117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083117
Iteration 2/1000 | Loss: 0.00002088
Iteration 3/1000 | Loss: 0.00001217
Iteration 4/1000 | Loss: 0.00001131
Iteration 5/1000 | Loss: 0.00001058
Iteration 6/1000 | Loss: 0.00001023
Iteration 7/1000 | Loss: 0.00001004
Iteration 8/1000 | Loss: 0.00000997
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000992
Iteration 11/1000 | Loss: 0.00000986
Iteration 12/1000 | Loss: 0.00000981
Iteration 13/1000 | Loss: 0.00000981
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000978
Iteration 16/1000 | Loss: 0.00000977
Iteration 17/1000 | Loss: 0.00000972
Iteration 18/1000 | Loss: 0.00000969
Iteration 19/1000 | Loss: 0.00000968
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000968
Iteration 22/1000 | Loss: 0.00000968
Iteration 23/1000 | Loss: 0.00000967
Iteration 24/1000 | Loss: 0.00000966
Iteration 25/1000 | Loss: 0.00000965
Iteration 26/1000 | Loss: 0.00000965
Iteration 27/1000 | Loss: 0.00000963
Iteration 28/1000 | Loss: 0.00000963
Iteration 29/1000 | Loss: 0.00000963
Iteration 30/1000 | Loss: 0.00000963
Iteration 31/1000 | Loss: 0.00000963
Iteration 32/1000 | Loss: 0.00000963
Iteration 33/1000 | Loss: 0.00000963
Iteration 34/1000 | Loss: 0.00000963
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000963
Iteration 37/1000 | Loss: 0.00000962
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000961
Iteration 40/1000 | Loss: 0.00000960
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000959
Iteration 44/1000 | Loss: 0.00000959
Iteration 45/1000 | Loss: 0.00000958
Iteration 46/1000 | Loss: 0.00000958
Iteration 47/1000 | Loss: 0.00000958
Iteration 48/1000 | Loss: 0.00000957
Iteration 49/1000 | Loss: 0.00000955
Iteration 50/1000 | Loss: 0.00000954
Iteration 51/1000 | Loss: 0.00000954
Iteration 52/1000 | Loss: 0.00000954
Iteration 53/1000 | Loss: 0.00000954
Iteration 54/1000 | Loss: 0.00000954
Iteration 55/1000 | Loss: 0.00000953
Iteration 56/1000 | Loss: 0.00000953
Iteration 57/1000 | Loss: 0.00000953
Iteration 58/1000 | Loss: 0.00000952
Iteration 59/1000 | Loss: 0.00000952
Iteration 60/1000 | Loss: 0.00000952
Iteration 61/1000 | Loss: 0.00000952
Iteration 62/1000 | Loss: 0.00000952
Iteration 63/1000 | Loss: 0.00000952
Iteration 64/1000 | Loss: 0.00000951
Iteration 65/1000 | Loss: 0.00000951
Iteration 66/1000 | Loss: 0.00000951
Iteration 67/1000 | Loss: 0.00000951
Iteration 68/1000 | Loss: 0.00000951
Iteration 69/1000 | Loss: 0.00000951
Iteration 70/1000 | Loss: 0.00000951
Iteration 71/1000 | Loss: 0.00000951
Iteration 72/1000 | Loss: 0.00000951
Iteration 73/1000 | Loss: 0.00000951
Iteration 74/1000 | Loss: 0.00000951
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000950
Iteration 77/1000 | Loss: 0.00000950
Iteration 78/1000 | Loss: 0.00000950
Iteration 79/1000 | Loss: 0.00000950
Iteration 80/1000 | Loss: 0.00000949
Iteration 81/1000 | Loss: 0.00000949
Iteration 82/1000 | Loss: 0.00000949
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000949
Iteration 86/1000 | Loss: 0.00000949
Iteration 87/1000 | Loss: 0.00000949
Iteration 88/1000 | Loss: 0.00000949
Iteration 89/1000 | Loss: 0.00000949
Iteration 90/1000 | Loss: 0.00000949
Iteration 91/1000 | Loss: 0.00000949
Iteration 92/1000 | Loss: 0.00000949
Iteration 93/1000 | Loss: 0.00000949
Iteration 94/1000 | Loss: 0.00000949
Iteration 95/1000 | Loss: 0.00000949
Iteration 96/1000 | Loss: 0.00000949
Iteration 97/1000 | Loss: 0.00000949
Iteration 98/1000 | Loss: 0.00000949
Iteration 99/1000 | Loss: 0.00000949
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000949
Iteration 102/1000 | Loss: 0.00000949
Iteration 103/1000 | Loss: 0.00000949
Iteration 104/1000 | Loss: 0.00000949
Iteration 105/1000 | Loss: 0.00000949
Iteration 106/1000 | Loss: 0.00000949
Iteration 107/1000 | Loss: 0.00000949
Iteration 108/1000 | Loss: 0.00000949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [9.490519005339593e-06, 9.490519005339593e-06, 9.490519005339593e-06, 9.490519005339593e-06, 9.490519005339593e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.490519005339593e-06

Optimization complete. Final v2v error: 2.635115146636963 mm

Highest mean error: 2.8406012058258057 mm for frame 116

Lowest mean error: 2.585395336151123 mm for frame 30

Saving results

Total time: 30.203630447387695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082289
Iteration 2/25 | Loss: 0.00242859
Iteration 3/25 | Loss: 0.00149985
Iteration 4/25 | Loss: 0.00108275
Iteration 5/25 | Loss: 0.00094112
Iteration 6/25 | Loss: 0.00092073
Iteration 7/25 | Loss: 0.00089140
Iteration 8/25 | Loss: 0.00085808
Iteration 9/25 | Loss: 0.00081135
Iteration 10/25 | Loss: 0.00079330
Iteration 11/25 | Loss: 0.00077556
Iteration 12/25 | Loss: 0.00074814
Iteration 13/25 | Loss: 0.00074335
Iteration 14/25 | Loss: 0.00073495
Iteration 15/25 | Loss: 0.00073670
Iteration 16/25 | Loss: 0.00073188
Iteration 17/25 | Loss: 0.00072479
Iteration 18/25 | Loss: 0.00073135
Iteration 19/25 | Loss: 0.00072589
Iteration 20/25 | Loss: 0.00072046
Iteration 21/25 | Loss: 0.00072215
Iteration 22/25 | Loss: 0.00071855
Iteration 23/25 | Loss: 0.00071854
Iteration 24/25 | Loss: 0.00071853
Iteration 25/25 | Loss: 0.00071852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06255770
Iteration 2/25 | Loss: 0.00089364
Iteration 3/25 | Loss: 0.00089363
Iteration 4/25 | Loss: 0.00089363
Iteration 5/25 | Loss: 0.00082354
Iteration 6/25 | Loss: 0.00082354
Iteration 7/25 | Loss: 0.00082354
Iteration 8/25 | Loss: 0.00082354
Iteration 9/25 | Loss: 0.00082354
Iteration 10/25 | Loss: 0.00082354
Iteration 11/25 | Loss: 0.00082354
Iteration 12/25 | Loss: 0.00082354
Iteration 13/25 | Loss: 0.00082354
Iteration 14/25 | Loss: 0.00082354
Iteration 15/25 | Loss: 0.00082354
Iteration 16/25 | Loss: 0.00082354
Iteration 17/25 | Loss: 0.00082354
Iteration 18/25 | Loss: 0.00082354
Iteration 19/25 | Loss: 0.00082354
Iteration 20/25 | Loss: 0.00082354
Iteration 21/25 | Loss: 0.00082354
Iteration 22/25 | Loss: 0.00082354
Iteration 23/25 | Loss: 0.00082354
Iteration 24/25 | Loss: 0.00082354
Iteration 25/25 | Loss: 0.00082354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082354
Iteration 2/1000 | Loss: 0.00011100
Iteration 3/1000 | Loss: 0.00005959
Iteration 4/1000 | Loss: 0.00001728
Iteration 5/1000 | Loss: 0.00001640
Iteration 6/1000 | Loss: 0.00010275
Iteration 7/1000 | Loss: 0.00005616
Iteration 8/1000 | Loss: 0.00001542
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00009697
Iteration 11/1000 | Loss: 0.00009985
Iteration 12/1000 | Loss: 0.00004940
Iteration 13/1000 | Loss: 0.00004582
Iteration 14/1000 | Loss: 0.00001482
Iteration 15/1000 | Loss: 0.00007464
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001463
Iteration 18/1000 | Loss: 0.00007272
Iteration 19/1000 | Loss: 0.00002368
Iteration 20/1000 | Loss: 0.00007153
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001444
Iteration 25/1000 | Loss: 0.00001442
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001440
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001435
Iteration 32/1000 | Loss: 0.00001435
Iteration 33/1000 | Loss: 0.00001434
Iteration 34/1000 | Loss: 0.00001424
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001423
Iteration 39/1000 | Loss: 0.00001423
Iteration 40/1000 | Loss: 0.00001422
Iteration 41/1000 | Loss: 0.00001422
Iteration 42/1000 | Loss: 0.00001422
Iteration 43/1000 | Loss: 0.00001422
Iteration 44/1000 | Loss: 0.00001422
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001419
Iteration 57/1000 | Loss: 0.00005782
Iteration 58/1000 | Loss: 0.00013793
Iteration 59/1000 | Loss: 0.00004295
Iteration 60/1000 | Loss: 0.00001772
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00003707
Iteration 63/1000 | Loss: 0.00001414
Iteration 64/1000 | Loss: 0.00005069
Iteration 65/1000 | Loss: 0.00001795
Iteration 66/1000 | Loss: 0.00001501
Iteration 67/1000 | Loss: 0.00001407
Iteration 68/1000 | Loss: 0.00001407
Iteration 69/1000 | Loss: 0.00001406
Iteration 70/1000 | Loss: 0.00001406
Iteration 71/1000 | Loss: 0.00001406
Iteration 72/1000 | Loss: 0.00001406
Iteration 73/1000 | Loss: 0.00001406
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001406
Iteration 78/1000 | Loss: 0.00001406
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001406
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001405
Iteration 89/1000 | Loss: 0.00001405
Iteration 90/1000 | Loss: 0.00001405
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001403
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001782
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001402
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001401
Iteration 125/1000 | Loss: 0.00001401
Iteration 126/1000 | Loss: 0.00001401
Iteration 127/1000 | Loss: 0.00001401
Iteration 128/1000 | Loss: 0.00001401
Iteration 129/1000 | Loss: 0.00001401
Iteration 130/1000 | Loss: 0.00001401
Iteration 131/1000 | Loss: 0.00001401
Iteration 132/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.401403915224364e-05, 1.401403915224364e-05, 1.401403915224364e-05, 1.401403915224364e-05, 1.401403915224364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.401403915224364e-05

Optimization complete. Final v2v error: 3.184490442276001 mm

Highest mean error: 3.6630518436431885 mm for frame 118

Lowest mean error: 3.005462169647217 mm for frame 10

Saving results

Total time: 93.86155080795288
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959491
Iteration 2/25 | Loss: 0.00130493
Iteration 3/25 | Loss: 0.00096586
Iteration 4/25 | Loss: 0.00084459
Iteration 5/25 | Loss: 0.00081148
Iteration 6/25 | Loss: 0.00080548
Iteration 7/25 | Loss: 0.00080306
Iteration 8/25 | Loss: 0.00079993
Iteration 9/25 | Loss: 0.00079893
Iteration 10/25 | Loss: 0.00080665
Iteration 11/25 | Loss: 0.00080560
Iteration 12/25 | Loss: 0.00082281
Iteration 13/25 | Loss: 0.00080613
Iteration 14/25 | Loss: 0.00080222
Iteration 15/25 | Loss: 0.00079642
Iteration 16/25 | Loss: 0.00079565
Iteration 17/25 | Loss: 0.00079801
Iteration 18/25 | Loss: 0.00079709
Iteration 19/25 | Loss: 0.00079612
Iteration 20/25 | Loss: 0.00079625
Iteration 21/25 | Loss: 0.00079588
Iteration 22/25 | Loss: 0.00079665
Iteration 23/25 | Loss: 0.00079645
Iteration 24/25 | Loss: 0.00079534
Iteration 25/25 | Loss: 0.00079534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65660596
Iteration 2/25 | Loss: 0.00091137
Iteration 3/25 | Loss: 0.00090510
Iteration 4/25 | Loss: 0.00090510
Iteration 5/25 | Loss: 0.00090510
Iteration 6/25 | Loss: 0.00090510
Iteration 7/25 | Loss: 0.00090510
Iteration 8/25 | Loss: 0.00090510
Iteration 9/25 | Loss: 0.00090510
Iteration 10/25 | Loss: 0.00090510
Iteration 11/25 | Loss: 0.00090510
Iteration 12/25 | Loss: 0.00090510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009050970547832549, 0.0009050970547832549, 0.0009050970547832549, 0.0009050970547832549, 0.0009050970547832549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009050970547832549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090510
Iteration 2/1000 | Loss: 0.00004856
Iteration 3/1000 | Loss: 0.00002700
Iteration 4/1000 | Loss: 0.00002442
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002209
Iteration 7/1000 | Loss: 0.00002149
Iteration 8/1000 | Loss: 0.00002091
Iteration 9/1000 | Loss: 0.00002050
Iteration 10/1000 | Loss: 0.00002023
Iteration 11/1000 | Loss: 0.00002002
Iteration 12/1000 | Loss: 0.00001993
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001972
Iteration 16/1000 | Loss: 0.00001970
Iteration 17/1000 | Loss: 0.00001970
Iteration 18/1000 | Loss: 0.00001969
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001968
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001967
Iteration 23/1000 | Loss: 0.00001966
Iteration 24/1000 | Loss: 0.00001966
Iteration 25/1000 | Loss: 0.00001965
Iteration 26/1000 | Loss: 0.00001965
Iteration 27/1000 | Loss: 0.00001964
Iteration 28/1000 | Loss: 0.00001964
Iteration 29/1000 | Loss: 0.00001964
Iteration 30/1000 | Loss: 0.00001964
Iteration 31/1000 | Loss: 0.00001964
Iteration 32/1000 | Loss: 0.00001964
Iteration 33/1000 | Loss: 0.00001964
Iteration 34/1000 | Loss: 0.00001964
Iteration 35/1000 | Loss: 0.00001964
Iteration 36/1000 | Loss: 0.00001963
Iteration 37/1000 | Loss: 0.00001963
Iteration 38/1000 | Loss: 0.00001962
Iteration 39/1000 | Loss: 0.00001962
Iteration 40/1000 | Loss: 0.00001961
Iteration 41/1000 | Loss: 0.00001961
Iteration 42/1000 | Loss: 0.00001961
Iteration 43/1000 | Loss: 0.00001961
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001960
Iteration 46/1000 | Loss: 0.00001960
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001959
Iteration 50/1000 | Loss: 0.00001959
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001958
Iteration 53/1000 | Loss: 0.00001958
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001958
Iteration 58/1000 | Loss: 0.00001958
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001958
Iteration 61/1000 | Loss: 0.00001957
Iteration 62/1000 | Loss: 0.00001957
Iteration 63/1000 | Loss: 0.00001957
Iteration 64/1000 | Loss: 0.00001957
Iteration 65/1000 | Loss: 0.00001957
Iteration 66/1000 | Loss: 0.00001957
Iteration 67/1000 | Loss: 0.00001957
Iteration 68/1000 | Loss: 0.00001957
Iteration 69/1000 | Loss: 0.00001957
Iteration 70/1000 | Loss: 0.00001957
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001956
Iteration 75/1000 | Loss: 0.00001956
Iteration 76/1000 | Loss: 0.00001956
Iteration 77/1000 | Loss: 0.00001956
Iteration 78/1000 | Loss: 0.00001956
Iteration 79/1000 | Loss: 0.00001956
Iteration 80/1000 | Loss: 0.00001956
Iteration 81/1000 | Loss: 0.00001956
Iteration 82/1000 | Loss: 0.00001956
Iteration 83/1000 | Loss: 0.00001956
Iteration 84/1000 | Loss: 0.00001956
Iteration 85/1000 | Loss: 0.00001956
Iteration 86/1000 | Loss: 0.00001956
Iteration 87/1000 | Loss: 0.00001956
Iteration 88/1000 | Loss: 0.00001956
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00001956
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001955
Iteration 93/1000 | Loss: 0.00001955
Iteration 94/1000 | Loss: 0.00001955
Iteration 95/1000 | Loss: 0.00001955
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001955
Iteration 100/1000 | Loss: 0.00001955
Iteration 101/1000 | Loss: 0.00001955
Iteration 102/1000 | Loss: 0.00001955
Iteration 103/1000 | Loss: 0.00001955
Iteration 104/1000 | Loss: 0.00001955
Iteration 105/1000 | Loss: 0.00001955
Iteration 106/1000 | Loss: 0.00001955
Iteration 107/1000 | Loss: 0.00001955
Iteration 108/1000 | Loss: 0.00001955
Iteration 109/1000 | Loss: 0.00001955
Iteration 110/1000 | Loss: 0.00001955
Iteration 111/1000 | Loss: 0.00001955
Iteration 112/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.9553559468477033e-05, 1.9553559468477033e-05, 1.9553559468477033e-05, 1.9553559468477033e-05, 1.9553559468477033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9553559468477033e-05

Optimization complete. Final v2v error: 3.729076623916626 mm

Highest mean error: 4.803224086761475 mm for frame 39

Lowest mean error: 3.2328147888183594 mm for frame 121

Saving results

Total time: 65.54400992393494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025043
Iteration 2/25 | Loss: 0.00405954
Iteration 3/25 | Loss: 0.00306536
Iteration 4/25 | Loss: 0.00241469
Iteration 5/25 | Loss: 0.00238823
Iteration 6/25 | Loss: 0.00259844
Iteration 7/25 | Loss: 0.00228606
Iteration 8/25 | Loss: 0.00210278
Iteration 9/25 | Loss: 0.00201927
Iteration 10/25 | Loss: 0.00192571
Iteration 11/25 | Loss: 0.00170298
Iteration 12/25 | Loss: 0.00171466
Iteration 13/25 | Loss: 0.00165922
Iteration 14/25 | Loss: 0.00162927
Iteration 15/25 | Loss: 0.00160919
Iteration 16/25 | Loss: 0.00165113
Iteration 17/25 | Loss: 0.00160251
Iteration 18/25 | Loss: 0.00165485
Iteration 19/25 | Loss: 0.00156626
Iteration 20/25 | Loss: 0.00155970
Iteration 21/25 | Loss: 0.00157486
Iteration 22/25 | Loss: 0.00157485
Iteration 23/25 | Loss: 0.00163097
Iteration 24/25 | Loss: 0.00156890
Iteration 25/25 | Loss: 0.00159140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.31776285
Iteration 2/25 | Loss: 0.04645531
Iteration 3/25 | Loss: 0.02494046
Iteration 4/25 | Loss: 0.01899331
Iteration 5/25 | Loss: 0.01321791
Iteration 6/25 | Loss: 0.01185337
Iteration 7/25 | Loss: 0.01185337
Iteration 8/25 | Loss: 0.01185337
Iteration 9/25 | Loss: 0.01185337
Iteration 10/25 | Loss: 0.01185337
Iteration 11/25 | Loss: 0.01185337
Iteration 12/25 | Loss: 0.01185336
Iteration 13/25 | Loss: 0.01185336
Iteration 14/25 | Loss: 0.01185336
Iteration 15/25 | Loss: 0.01185336
Iteration 16/25 | Loss: 0.01185336
Iteration 17/25 | Loss: 0.01185336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.011853364296257496, 0.011853364296257496, 0.011853364296257496, 0.011853364296257496, 0.011853364296257496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.011853364296257496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01185336
Iteration 2/1000 | Loss: 0.01515201
Iteration 3/1000 | Loss: 0.01297858
Iteration 4/1000 | Loss: 0.01477791
Iteration 5/1000 | Loss: 0.01509989
Iteration 6/1000 | Loss: 0.02672551
Iteration 7/1000 | Loss: 0.01001570
Iteration 8/1000 | Loss: 0.01057664
Iteration 9/1000 | Loss: 0.01279965
Iteration 10/1000 | Loss: 0.01004267
Iteration 11/1000 | Loss: 0.01563028
Iteration 12/1000 | Loss: 0.01410051
Iteration 13/1000 | Loss: 0.01029426
Iteration 14/1000 | Loss: 0.00888416
Iteration 15/1000 | Loss: 0.01807150
Iteration 16/1000 | Loss: 0.00972525
Iteration 17/1000 | Loss: 0.02139014
Iteration 18/1000 | Loss: 0.01244181
Iteration 19/1000 | Loss: 0.01581055
Iteration 20/1000 | Loss: 0.00993368
Iteration 21/1000 | Loss: 0.01251822
Iteration 22/1000 | Loss: 0.00991245
Iteration 23/1000 | Loss: 0.02342973
Iteration 24/1000 | Loss: 0.01047825
Iteration 25/1000 | Loss: 0.00967371
Iteration 26/1000 | Loss: 0.01364061
Iteration 27/1000 | Loss: 0.00923053
Iteration 28/1000 | Loss: 0.01133281
Iteration 29/1000 | Loss: 0.00964479
Iteration 30/1000 | Loss: 0.01234025
Iteration 31/1000 | Loss: 0.00962927
Iteration 32/1000 | Loss: 0.01367771
Iteration 33/1000 | Loss: 0.00876470
Iteration 34/1000 | Loss: 0.01202451
Iteration 35/1000 | Loss: 0.01350692
Iteration 36/1000 | Loss: 0.01156858
Iteration 37/1000 | Loss: 0.00857609
Iteration 38/1000 | Loss: 0.00966030
Iteration 39/1000 | Loss: 0.00884155
Iteration 40/1000 | Loss: 0.00819426
Iteration 41/1000 | Loss: 0.00895328
Iteration 42/1000 | Loss: 0.00875918
Iteration 43/1000 | Loss: 0.01892216
Iteration 44/1000 | Loss: 0.00918470
Iteration 45/1000 | Loss: 0.01167432
Iteration 46/1000 | Loss: 0.01287575
Iteration 47/1000 | Loss: 0.01384416
Iteration 48/1000 | Loss: 0.01073696
Iteration 49/1000 | Loss: 0.01347819
Iteration 50/1000 | Loss: 0.01226088
Iteration 51/1000 | Loss: 0.00956833
Iteration 52/1000 | Loss: 0.00922897
Iteration 53/1000 | Loss: 0.01073677
Iteration 54/1000 | Loss: 0.00840027
Iteration 55/1000 | Loss: 0.00886660
Iteration 56/1000 | Loss: 0.01102284
Iteration 57/1000 | Loss: 0.01356065
Iteration 58/1000 | Loss: 0.00974391
Iteration 59/1000 | Loss: 0.01057623
Iteration 60/1000 | Loss: 0.01221266
Iteration 61/1000 | Loss: 0.01303777
Iteration 62/1000 | Loss: 0.01041877
Iteration 63/1000 | Loss: 0.00922233
Iteration 64/1000 | Loss: 0.00908452
Iteration 65/1000 | Loss: 0.01110360
Iteration 66/1000 | Loss: 0.01100450
Iteration 67/1000 | Loss: 0.01003479
Iteration 68/1000 | Loss: 0.01006698
Iteration 69/1000 | Loss: 0.01203316
Iteration 70/1000 | Loss: 0.01092325
Iteration 71/1000 | Loss: 0.01030727
Iteration 72/1000 | Loss: 0.01583887
Iteration 73/1000 | Loss: 0.00875077
Iteration 74/1000 | Loss: 0.01705560
Iteration 75/1000 | Loss: 0.00803131
Iteration 76/1000 | Loss: 0.00843032
Iteration 77/1000 | Loss: 0.00816951
Iteration 78/1000 | Loss: 0.01203327
Iteration 79/1000 | Loss: 0.00860639
Iteration 80/1000 | Loss: 0.00907453
Iteration 81/1000 | Loss: 0.00904391
Iteration 82/1000 | Loss: 0.00954628
Iteration 83/1000 | Loss: 0.01011923
Iteration 84/1000 | Loss: 0.00998566
Iteration 85/1000 | Loss: 0.00924907
Iteration 86/1000 | Loss: 0.01028706
Iteration 87/1000 | Loss: 0.00731515
Iteration 88/1000 | Loss: 0.00890267
Iteration 89/1000 | Loss: 0.01320197
Iteration 90/1000 | Loss: 0.00846952
Iteration 91/1000 | Loss: 0.01021798
Iteration 92/1000 | Loss: 0.00797054
Iteration 93/1000 | Loss: 0.00643636
Iteration 94/1000 | Loss: 0.00887539
Iteration 95/1000 | Loss: 0.00766196
Iteration 96/1000 | Loss: 0.01416462
Iteration 97/1000 | Loss: 0.00872896
Iteration 98/1000 | Loss: 0.01526894
Iteration 99/1000 | Loss: 0.00887933
Iteration 100/1000 | Loss: 0.00954916
Iteration 101/1000 | Loss: 0.00870597
Iteration 102/1000 | Loss: 0.01699663
Iteration 103/1000 | Loss: 0.01086563
Iteration 104/1000 | Loss: 0.01171806
Iteration 105/1000 | Loss: 0.01066423
Iteration 106/1000 | Loss: 0.00859204
Iteration 107/1000 | Loss: 0.01021512
Iteration 108/1000 | Loss: 0.01044184
Iteration 109/1000 | Loss: 0.01038478
Iteration 110/1000 | Loss: 0.00862635
Iteration 111/1000 | Loss: 0.01603940
Iteration 112/1000 | Loss: 0.01208629
Iteration 113/1000 | Loss: 0.01117571
Iteration 114/1000 | Loss: 0.00859624
Iteration 115/1000 | Loss: 0.00814357
Iteration 116/1000 | Loss: 0.02036415
Iteration 117/1000 | Loss: 0.01863345
Iteration 118/1000 | Loss: 0.01432235
Iteration 119/1000 | Loss: 0.01449313
Iteration 120/1000 | Loss: 0.01519105
Iteration 121/1000 | Loss: 0.01242417
Iteration 122/1000 | Loss: 0.01007107
Iteration 123/1000 | Loss: 0.01019888
Iteration 124/1000 | Loss: 0.00930566
Iteration 125/1000 | Loss: 0.00826747
Iteration 126/1000 | Loss: 0.01868690
Iteration 127/1000 | Loss: 0.01250898
Iteration 128/1000 | Loss: 0.00767779
Iteration 129/1000 | Loss: 0.00724500
Iteration 130/1000 | Loss: 0.01013503
Iteration 131/1000 | Loss: 0.00942310
Iteration 132/1000 | Loss: 0.00771061
Iteration 133/1000 | Loss: 0.00909459
Iteration 134/1000 | Loss: 0.00763055
Iteration 135/1000 | Loss: 0.01821531
Iteration 136/1000 | Loss: 0.01321168
Iteration 137/1000 | Loss: 0.00903618
Iteration 138/1000 | Loss: 0.01026066
Iteration 139/1000 | Loss: 0.00998608
Iteration 140/1000 | Loss: 0.01374282
Iteration 141/1000 | Loss: 0.02158214
Iteration 142/1000 | Loss: 0.01034990
Iteration 143/1000 | Loss: 0.00725186
Iteration 144/1000 | Loss: 0.01185294
Iteration 145/1000 | Loss: 0.00909899
Iteration 146/1000 | Loss: 0.01201332
Iteration 147/1000 | Loss: 0.00856681
Iteration 148/1000 | Loss: 0.01080294
Iteration 149/1000 | Loss: 0.00950473
Iteration 150/1000 | Loss: 0.00715248
Iteration 151/1000 | Loss: 0.00658681
Iteration 152/1000 | Loss: 0.01625112
Iteration 153/1000 | Loss: 0.00675658
Iteration 154/1000 | Loss: 0.01136398
Iteration 155/1000 | Loss: 0.00818814
Iteration 156/1000 | Loss: 0.01513469
Iteration 157/1000 | Loss: 0.01274252
Iteration 158/1000 | Loss: 0.00863980
Iteration 159/1000 | Loss: 0.00608925
Iteration 160/1000 | Loss: 0.00650328
Iteration 161/1000 | Loss: 0.01315067
Iteration 162/1000 | Loss: 0.01092284
Iteration 163/1000 | Loss: 0.01154226
Iteration 164/1000 | Loss: 0.00956040
Iteration 165/1000 | Loss: 0.00696464
Iteration 166/1000 | Loss: 0.02047283
Iteration 167/1000 | Loss: 0.01516884
Iteration 168/1000 | Loss: 0.00805533
Iteration 169/1000 | Loss: 0.00783240
Iteration 170/1000 | Loss: 0.01647341
Iteration 171/1000 | Loss: 0.00906491
Iteration 172/1000 | Loss: 0.00830359
Iteration 173/1000 | Loss: 0.00744401
Iteration 174/1000 | Loss: 0.00757150
Iteration 175/1000 | Loss: 0.00741249
Iteration 176/1000 | Loss: 0.00687528
Iteration 177/1000 | Loss: 0.01468186
Iteration 178/1000 | Loss: 0.00802893
Iteration 179/1000 | Loss: 0.01051130
Iteration 180/1000 | Loss: 0.01153844
Iteration 181/1000 | Loss: 0.00967695
Iteration 182/1000 | Loss: 0.00627045
Iteration 183/1000 | Loss: 0.01758587
Iteration 184/1000 | Loss: 0.01446363
Iteration 185/1000 | Loss: 0.01007477
Iteration 186/1000 | Loss: 0.01015489
Iteration 187/1000 | Loss: 0.01013271
Iteration 188/1000 | Loss: 0.01237016
Iteration 189/1000 | Loss: 0.01817689
Iteration 190/1000 | Loss: 0.01131201
Iteration 191/1000 | Loss: 0.01748129
Iteration 192/1000 | Loss: 0.00804289
Iteration 193/1000 | Loss: 0.01223514
Iteration 194/1000 | Loss: 0.00785214
Iteration 195/1000 | Loss: 0.01104730
Iteration 196/1000 | Loss: 0.00935418
Iteration 197/1000 | Loss: 0.00856547
Iteration 198/1000 | Loss: 0.01088753
Iteration 199/1000 | Loss: 0.00612062
Iteration 200/1000 | Loss: 0.00710693
Iteration 201/1000 | Loss: 0.01109921
Iteration 202/1000 | Loss: 0.01451854
Iteration 203/1000 | Loss: 0.01012708
Iteration 204/1000 | Loss: 0.01000322
Iteration 205/1000 | Loss: 0.01106662
Iteration 206/1000 | Loss: 0.01125590
Iteration 207/1000 | Loss: 0.00811641
Iteration 208/1000 | Loss: 0.00721657
Iteration 209/1000 | Loss: 0.00649031
Iteration 210/1000 | Loss: 0.01190903
Iteration 211/1000 | Loss: 0.01237691
Iteration 212/1000 | Loss: 0.00684364
Iteration 213/1000 | Loss: 0.00948974
Iteration 214/1000 | Loss: 0.00750636
Iteration 215/1000 | Loss: 0.00695894
Iteration 216/1000 | Loss: 0.01171261
Iteration 217/1000 | Loss: 0.00997502
Iteration 218/1000 | Loss: 0.00493005
Iteration 219/1000 | Loss: 0.00648863
Iteration 220/1000 | Loss: 0.00506816
Iteration 221/1000 | Loss: 0.00642189
Iteration 222/1000 | Loss: 0.00856479
Iteration 223/1000 | Loss: 0.00556947
Iteration 224/1000 | Loss: 0.01346570
Iteration 225/1000 | Loss: 0.01051353
Iteration 226/1000 | Loss: 0.00676939
Iteration 227/1000 | Loss: 0.00872707
Iteration 228/1000 | Loss: 0.00759313
Iteration 229/1000 | Loss: 0.01361398
Iteration 230/1000 | Loss: 0.01128849
Iteration 231/1000 | Loss: 0.00537617
Iteration 232/1000 | Loss: 0.00861385
Iteration 233/1000 | Loss: 0.00906537
Iteration 234/1000 | Loss: 0.00980776
Iteration 235/1000 | Loss: 0.01255770
Iteration 236/1000 | Loss: 0.00664151
Iteration 237/1000 | Loss: 0.00871544
Iteration 238/1000 | Loss: 0.00510794
Iteration 239/1000 | Loss: 0.00642904
Iteration 240/1000 | Loss: 0.00883125
Iteration 241/1000 | Loss: 0.01043719
Iteration 242/1000 | Loss: 0.00544816
Iteration 243/1000 | Loss: 0.00633041
Iteration 244/1000 | Loss: 0.00617797
Iteration 245/1000 | Loss: 0.00696941
Iteration 246/1000 | Loss: 0.00820846
Iteration 247/1000 | Loss: 0.00613485
Iteration 248/1000 | Loss: 0.00644027
Iteration 249/1000 | Loss: 0.00516079
Iteration 250/1000 | Loss: 0.01312423
Iteration 251/1000 | Loss: 0.00568258
Iteration 252/1000 | Loss: 0.00599916
Iteration 253/1000 | Loss: 0.00460924
Iteration 254/1000 | Loss: 0.00772038
Iteration 255/1000 | Loss: 0.00491173
Iteration 256/1000 | Loss: 0.00430746
Iteration 257/1000 | Loss: 0.01197292
Iteration 258/1000 | Loss: 0.00480351
Iteration 259/1000 | Loss: 0.00894676
Iteration 260/1000 | Loss: 0.00491926
Iteration 261/1000 | Loss: 0.01407883
Iteration 262/1000 | Loss: 0.01094506
Iteration 263/1000 | Loss: 0.00425755
Iteration 264/1000 | Loss: 0.00470166
Iteration 265/1000 | Loss: 0.00416705
Iteration 266/1000 | Loss: 0.00519729
Iteration 267/1000 | Loss: 0.00568904
Iteration 268/1000 | Loss: 0.00561370
Iteration 269/1000 | Loss: 0.00535452
Iteration 270/1000 | Loss: 0.00400791
Iteration 271/1000 | Loss: 0.00688348
Iteration 272/1000 | Loss: 0.00721932
Iteration 273/1000 | Loss: 0.00508659
Iteration 274/1000 | Loss: 0.00995444
Iteration 275/1000 | Loss: 0.00407555
Iteration 276/1000 | Loss: 0.00705635
Iteration 277/1000 | Loss: 0.00540716
Iteration 278/1000 | Loss: 0.00470769
Iteration 279/1000 | Loss: 0.00371559
Iteration 280/1000 | Loss: 0.00315744
Iteration 281/1000 | Loss: 0.00314438
Iteration 282/1000 | Loss: 0.00342314
Iteration 283/1000 | Loss: 0.00298503
Iteration 284/1000 | Loss: 0.00749948
Iteration 285/1000 | Loss: 0.00394237
Iteration 286/1000 | Loss: 0.00480520
Iteration 287/1000 | Loss: 0.00402091
Iteration 288/1000 | Loss: 0.00253430
Iteration 289/1000 | Loss: 0.00463309
Iteration 290/1000 | Loss: 0.00310653
Iteration 291/1000 | Loss: 0.00382027
Iteration 292/1000 | Loss: 0.00657697
Iteration 293/1000 | Loss: 0.00892114
Iteration 294/1000 | Loss: 0.00388928
Iteration 295/1000 | Loss: 0.00294573
Iteration 296/1000 | Loss: 0.00085747
Iteration 297/1000 | Loss: 0.00081486
Iteration 298/1000 | Loss: 0.00116455
Iteration 299/1000 | Loss: 0.00181547
Iteration 300/1000 | Loss: 0.00255528
Iteration 301/1000 | Loss: 0.00111873
Iteration 302/1000 | Loss: 0.00085540
Iteration 303/1000 | Loss: 0.00250928
Iteration 304/1000 | Loss: 0.00068209
Iteration 305/1000 | Loss: 0.00089093
Iteration 306/1000 | Loss: 0.00250954
Iteration 307/1000 | Loss: 0.00130233
Iteration 308/1000 | Loss: 0.00719389
Iteration 309/1000 | Loss: 0.00394852
Iteration 310/1000 | Loss: 0.00073097
Iteration 311/1000 | Loss: 0.00567890
Iteration 312/1000 | Loss: 0.00549419
Iteration 313/1000 | Loss: 0.00522275
Iteration 314/1000 | Loss: 0.00204784
Iteration 315/1000 | Loss: 0.00142253
Iteration 316/1000 | Loss: 0.00230144
Iteration 317/1000 | Loss: 0.00085008
Iteration 318/1000 | Loss: 0.00242907
Iteration 319/1000 | Loss: 0.00228860
Iteration 320/1000 | Loss: 0.00234369
Iteration 321/1000 | Loss: 0.00354659
Iteration 322/1000 | Loss: 0.00133732
Iteration 323/1000 | Loss: 0.00038581
Iteration 324/1000 | Loss: 0.00323464
Iteration 325/1000 | Loss: 0.00079584
Iteration 326/1000 | Loss: 0.00093209
Iteration 327/1000 | Loss: 0.00081865
Iteration 328/1000 | Loss: 0.00177561
Iteration 329/1000 | Loss: 0.00065612
Iteration 330/1000 | Loss: 0.00095791
Iteration 331/1000 | Loss: 0.00030044
Iteration 332/1000 | Loss: 0.00022805
Iteration 333/1000 | Loss: 0.00079514
Iteration 334/1000 | Loss: 0.00128427
Iteration 335/1000 | Loss: 0.00042617
Iteration 336/1000 | Loss: 0.00125377
Iteration 337/1000 | Loss: 0.00176954
Iteration 338/1000 | Loss: 0.00196340
Iteration 339/1000 | Loss: 0.00084362
Iteration 340/1000 | Loss: 0.00111046
Iteration 341/1000 | Loss: 0.00118044
Iteration 342/1000 | Loss: 0.00091642
Iteration 343/1000 | Loss: 0.00224235
Iteration 344/1000 | Loss: 0.00743672
Iteration 345/1000 | Loss: 0.00312999
Iteration 346/1000 | Loss: 0.00052156
Iteration 347/1000 | Loss: 0.00229930
Iteration 348/1000 | Loss: 0.00092013
Iteration 349/1000 | Loss: 0.00065238
Iteration 350/1000 | Loss: 0.00115128
Iteration 351/1000 | Loss: 0.00143789
Iteration 352/1000 | Loss: 0.00085471
Iteration 353/1000 | Loss: 0.00063417
Iteration 354/1000 | Loss: 0.00238121
Iteration 355/1000 | Loss: 0.00132918
Iteration 356/1000 | Loss: 0.00119172
Iteration 357/1000 | Loss: 0.00433059
Iteration 358/1000 | Loss: 0.00072186
Iteration 359/1000 | Loss: 0.00058970
Iteration 360/1000 | Loss: 0.00231994
Iteration 361/1000 | Loss: 0.00120317
Iteration 362/1000 | Loss: 0.00164327
Iteration 363/1000 | Loss: 0.00051878
Iteration 364/1000 | Loss: 0.00150551
Iteration 365/1000 | Loss: 0.00184168
Iteration 366/1000 | Loss: 0.00311614
Iteration 367/1000 | Loss: 0.00031241
Iteration 368/1000 | Loss: 0.00052946
Iteration 369/1000 | Loss: 0.00334738
Iteration 370/1000 | Loss: 0.00062558
Iteration 371/1000 | Loss: 0.00039080
Iteration 372/1000 | Loss: 0.00050820
Iteration 373/1000 | Loss: 0.00138014
Iteration 374/1000 | Loss: 0.00060417
Iteration 375/1000 | Loss: 0.00031755
Iteration 376/1000 | Loss: 0.00034908
Iteration 377/1000 | Loss: 0.00403484
Iteration 378/1000 | Loss: 0.00104423
Iteration 379/1000 | Loss: 0.00055526
Iteration 380/1000 | Loss: 0.00342099
Iteration 381/1000 | Loss: 0.00084670
Iteration 382/1000 | Loss: 0.00093605
Iteration 383/1000 | Loss: 0.00031582
Iteration 384/1000 | Loss: 0.00037924
Iteration 385/1000 | Loss: 0.00014474
Iteration 386/1000 | Loss: 0.00014412
Iteration 387/1000 | Loss: 0.00041538
Iteration 388/1000 | Loss: 0.00398735
Iteration 389/1000 | Loss: 0.00047707
Iteration 390/1000 | Loss: 0.00026370
Iteration 391/1000 | Loss: 0.00015040
Iteration 392/1000 | Loss: 0.00016670
Iteration 393/1000 | Loss: 0.00043424
Iteration 394/1000 | Loss: 0.00103247
Iteration 395/1000 | Loss: 0.00036531
Iteration 396/1000 | Loss: 0.00055912
Iteration 397/1000 | Loss: 0.00128840
Iteration 398/1000 | Loss: 0.00022395
Iteration 399/1000 | Loss: 0.00019782
Iteration 400/1000 | Loss: 0.00062161
Iteration 401/1000 | Loss: 0.00065399
Iteration 402/1000 | Loss: 0.00061863
Iteration 403/1000 | Loss: 0.00053792
Iteration 404/1000 | Loss: 0.00086202
Iteration 405/1000 | Loss: 0.00123325
Iteration 406/1000 | Loss: 0.00301082
Iteration 407/1000 | Loss: 0.00122510
Iteration 408/1000 | Loss: 0.00082545
Iteration 409/1000 | Loss: 0.00039104
Iteration 410/1000 | Loss: 0.00048006
Iteration 411/1000 | Loss: 0.00404059
Iteration 412/1000 | Loss: 0.00111767
Iteration 413/1000 | Loss: 0.00051164
Iteration 414/1000 | Loss: 0.00376095
Iteration 415/1000 | Loss: 0.00031158
Iteration 416/1000 | Loss: 0.00097651
Iteration 417/1000 | Loss: 0.00016937
Iteration 418/1000 | Loss: 0.00158717
Iteration 419/1000 | Loss: 0.00022789
Iteration 420/1000 | Loss: 0.00018711
Iteration 421/1000 | Loss: 0.00188650
Iteration 422/1000 | Loss: 0.00024889
Iteration 423/1000 | Loss: 0.00027726
Iteration 424/1000 | Loss: 0.00323633
Iteration 425/1000 | Loss: 0.00082346
Iteration 426/1000 | Loss: 0.00056008
Iteration 427/1000 | Loss: 0.00018373
Iteration 428/1000 | Loss: 0.00025878
Iteration 429/1000 | Loss: 0.00032147
Iteration 430/1000 | Loss: 0.00013751
Iteration 431/1000 | Loss: 0.00008842
Iteration 432/1000 | Loss: 0.00005938
Iteration 433/1000 | Loss: 0.00007570
Iteration 434/1000 | Loss: 0.00005033
Iteration 435/1000 | Loss: 0.00004784
Iteration 436/1000 | Loss: 0.00008081
Iteration 437/1000 | Loss: 0.00154585
Iteration 438/1000 | Loss: 0.00020228
Iteration 439/1000 | Loss: 0.00195479
Iteration 440/1000 | Loss: 0.00236071
Iteration 441/1000 | Loss: 0.00250232
Iteration 442/1000 | Loss: 0.00059518
Iteration 443/1000 | Loss: 0.00109931
Iteration 444/1000 | Loss: 0.00034676
Iteration 445/1000 | Loss: 0.00028200
Iteration 446/1000 | Loss: 0.00027274
Iteration 447/1000 | Loss: 0.00015780
Iteration 448/1000 | Loss: 0.00016131
Iteration 449/1000 | Loss: 0.00013929
Iteration 450/1000 | Loss: 0.00011212
Iteration 451/1000 | Loss: 0.00021854
Iteration 452/1000 | Loss: 0.00018033
Iteration 453/1000 | Loss: 0.00318418
Iteration 454/1000 | Loss: 0.00455969
Iteration 455/1000 | Loss: 0.00070692
Iteration 456/1000 | Loss: 0.00206712
Iteration 457/1000 | Loss: 0.00076856
Iteration 458/1000 | Loss: 0.00059214
Iteration 459/1000 | Loss: 0.00169715
Iteration 460/1000 | Loss: 0.00021874
Iteration 461/1000 | Loss: 0.00021578
Iteration 462/1000 | Loss: 0.00021768
Iteration 463/1000 | Loss: 0.00031180
Iteration 464/1000 | Loss: 0.00024989
Iteration 465/1000 | Loss: 0.00014335
Iteration 466/1000 | Loss: 0.00005970
Iteration 467/1000 | Loss: 0.00004192
Iteration 468/1000 | Loss: 0.00138830
Iteration 469/1000 | Loss: 0.00007042
Iteration 470/1000 | Loss: 0.00003832
Iteration 471/1000 | Loss: 0.00003098
Iteration 472/1000 | Loss: 0.00002863
Iteration 473/1000 | Loss: 0.00002707
Iteration 474/1000 | Loss: 0.00155336
Iteration 475/1000 | Loss: 0.00010324
Iteration 476/1000 | Loss: 0.00002708
Iteration 477/1000 | Loss: 0.00138378
Iteration 478/1000 | Loss: 0.00005433
Iteration 479/1000 | Loss: 0.00003333
Iteration 480/1000 | Loss: 0.00002545
Iteration 481/1000 | Loss: 0.00002405
Iteration 482/1000 | Loss: 0.00002338
Iteration 483/1000 | Loss: 0.00002292
Iteration 484/1000 | Loss: 0.00002250
Iteration 485/1000 | Loss: 0.00002209
Iteration 486/1000 | Loss: 0.00004658
Iteration 487/1000 | Loss: 0.00005320
Iteration 488/1000 | Loss: 0.00005627
Iteration 489/1000 | Loss: 0.00004615
Iteration 490/1000 | Loss: 0.00004620
Iteration 491/1000 | Loss: 0.00004255
Iteration 492/1000 | Loss: 0.00003913
Iteration 493/1000 | Loss: 0.00004352
Iteration 494/1000 | Loss: 0.00004040
Iteration 495/1000 | Loss: 0.00007331
Iteration 496/1000 | Loss: 0.00003985
Iteration 497/1000 | Loss: 0.00007277
Iteration 498/1000 | Loss: 0.00003554
Iteration 499/1000 | Loss: 0.00002668
Iteration 500/1000 | Loss: 0.00002455
Iteration 501/1000 | Loss: 0.00002314
Iteration 502/1000 | Loss: 0.00002260
Iteration 503/1000 | Loss: 0.00002223
Iteration 504/1000 | Loss: 0.00002181
Iteration 505/1000 | Loss: 0.00005947
Iteration 506/1000 | Loss: 0.00005556
Iteration 507/1000 | Loss: 0.00004119
Iteration 508/1000 | Loss: 0.00005567
Iteration 509/1000 | Loss: 0.00004413
Iteration 510/1000 | Loss: 0.00002578
Iteration 511/1000 | Loss: 0.00004697
Iteration 512/1000 | Loss: 0.00004308
Iteration 513/1000 | Loss: 0.00003274
Iteration 514/1000 | Loss: 0.00002897
Iteration 515/1000 | Loss: 0.00016427
Iteration 516/1000 | Loss: 0.00009827
Iteration 517/1000 | Loss: 0.00004774
Iteration 518/1000 | Loss: 0.00004160
Iteration 519/1000 | Loss: 0.00004460
Iteration 520/1000 | Loss: 0.00004199
Iteration 521/1000 | Loss: 0.00004316
Iteration 522/1000 | Loss: 0.00004896
Iteration 523/1000 | Loss: 0.00003642
Iteration 524/1000 | Loss: 0.00003642
Iteration 525/1000 | Loss: 0.00004239
Iteration 526/1000 | Loss: 0.00004566
Iteration 527/1000 | Loss: 0.00004333
Iteration 528/1000 | Loss: 0.00005403
Iteration 529/1000 | Loss: 0.00004403
Iteration 530/1000 | Loss: 0.00004148
Iteration 531/1000 | Loss: 0.00004462
Iteration 532/1000 | Loss: 0.00006109
Iteration 533/1000 | Loss: 0.00004209
Iteration 534/1000 | Loss: 0.00005777
Iteration 535/1000 | Loss: 0.00003213
Iteration 536/1000 | Loss: 0.00004309
Iteration 537/1000 | Loss: 0.00015492
Iteration 538/1000 | Loss: 0.00013273
Iteration 539/1000 | Loss: 0.00172343
Iteration 540/1000 | Loss: 0.00012870
Iteration 541/1000 | Loss: 0.00076722
Iteration 542/1000 | Loss: 0.00005913
Iteration 543/1000 | Loss: 0.00003960
Iteration 544/1000 | Loss: 0.00004545
Iteration 545/1000 | Loss: 0.00004312
Iteration 546/1000 | Loss: 0.00004181
Iteration 547/1000 | Loss: 0.00003525
Iteration 548/1000 | Loss: 0.00003525
Iteration 549/1000 | Loss: 0.00004095
Iteration 550/1000 | Loss: 0.00003381
Iteration 551/1000 | Loss: 0.00003500
Iteration 552/1000 | Loss: 0.00003661
Iteration 553/1000 | Loss: 0.00003506
Iteration 554/1000 | Loss: 0.00004331
Iteration 555/1000 | Loss: 0.00004502
Iteration 556/1000 | Loss: 0.00003857
Iteration 557/1000 | Loss: 0.00003176
Iteration 558/1000 | Loss: 0.00004323
Iteration 559/1000 | Loss: 0.00003715
Iteration 560/1000 | Loss: 0.00003696
Iteration 561/1000 | Loss: 0.00005425
Iteration 562/1000 | Loss: 0.00004038
Iteration 563/1000 | Loss: 0.00004449
Iteration 564/1000 | Loss: 0.00004237
Iteration 565/1000 | Loss: 0.00003867
Iteration 566/1000 | Loss: 0.00003888
Iteration 567/1000 | Loss: 0.00004864
Iteration 568/1000 | Loss: 0.00003605
Iteration 569/1000 | Loss: 0.00003792
Iteration 570/1000 | Loss: 0.00003649
Iteration 571/1000 | Loss: 0.00003779
Iteration 572/1000 | Loss: 0.00003663
Iteration 573/1000 | Loss: 0.00003704
Iteration 574/1000 | Loss: 0.00003720
Iteration 575/1000 | Loss: 0.00003445
Iteration 576/1000 | Loss: 0.00003882
Iteration 577/1000 | Loss: 0.00003781
Iteration 578/1000 | Loss: 0.00006195
Iteration 579/1000 | Loss: 0.00003949
Iteration 580/1000 | Loss: 0.00005506
Iteration 581/1000 | Loss: 0.00004010
Iteration 582/1000 | Loss: 0.00003646
Iteration 583/1000 | Loss: 0.00003326
Iteration 584/1000 | Loss: 0.00004233
Iteration 585/1000 | Loss: 0.00004087
Iteration 586/1000 | Loss: 0.00003623
Iteration 587/1000 | Loss: 0.00003289
Iteration 588/1000 | Loss: 0.00004247
Iteration 589/1000 | Loss: 0.00004967
Iteration 590/1000 | Loss: 0.00165007
Iteration 591/1000 | Loss: 0.00006958
Iteration 592/1000 | Loss: 0.00004680
Iteration 593/1000 | Loss: 0.00004316
Iteration 594/1000 | Loss: 0.00005310
Iteration 595/1000 | Loss: 0.00002971
Iteration 596/1000 | Loss: 0.00002322
Iteration 597/1000 | Loss: 0.00002186
Iteration 598/1000 | Loss: 0.00002117
Iteration 599/1000 | Loss: 0.00002080
Iteration 600/1000 | Loss: 0.00180236
Iteration 601/1000 | Loss: 0.00013761
Iteration 602/1000 | Loss: 0.00113768
Iteration 603/1000 | Loss: 0.00002180
Iteration 604/1000 | Loss: 0.00002074
Iteration 605/1000 | Loss: 0.00002035
Iteration 606/1000 | Loss: 0.00002019
Iteration 607/1000 | Loss: 0.00002018
Iteration 608/1000 | Loss: 0.00002017
Iteration 609/1000 | Loss: 0.00013114
Iteration 610/1000 | Loss: 0.00005180
Iteration 611/1000 | Loss: 0.00012026
Iteration 612/1000 | Loss: 0.00003090
Iteration 613/1000 | Loss: 0.00002429
Iteration 614/1000 | Loss: 0.00002263
Iteration 615/1000 | Loss: 0.00002162
Iteration 616/1000 | Loss: 0.00002124
Iteration 617/1000 | Loss: 0.00002087
Iteration 618/1000 | Loss: 0.00002049
Iteration 619/1000 | Loss: 0.00002024
Iteration 620/1000 | Loss: 0.00002014
Iteration 621/1000 | Loss: 0.00002011
Iteration 622/1000 | Loss: 0.00002004
Iteration 623/1000 | Loss: 0.00001983
Iteration 624/1000 | Loss: 0.00001970
Iteration 625/1000 | Loss: 0.00001969
Iteration 626/1000 | Loss: 0.00001960
Iteration 627/1000 | Loss: 0.00001959
Iteration 628/1000 | Loss: 0.00001958
Iteration 629/1000 | Loss: 0.00001956
Iteration 630/1000 | Loss: 0.00001955
Iteration 631/1000 | Loss: 0.00001943
Iteration 632/1000 | Loss: 0.00001934
Iteration 633/1000 | Loss: 0.00001931
Iteration 634/1000 | Loss: 0.00001919
Iteration 635/1000 | Loss: 0.00001918
Iteration 636/1000 | Loss: 0.00001912
Iteration 637/1000 | Loss: 0.00001911
Iteration 638/1000 | Loss: 0.00001911
Iteration 639/1000 | Loss: 0.00001908
Iteration 640/1000 | Loss: 0.00001903
Iteration 641/1000 | Loss: 0.00001902
Iteration 642/1000 | Loss: 0.00001891
Iteration 643/1000 | Loss: 0.00001888
Iteration 644/1000 | Loss: 0.00001887
Iteration 645/1000 | Loss: 0.00001884
Iteration 646/1000 | Loss: 0.00001884
Iteration 647/1000 | Loss: 0.00001884
Iteration 648/1000 | Loss: 0.00001884
Iteration 649/1000 | Loss: 0.00001883
Iteration 650/1000 | Loss: 0.00001883
Iteration 651/1000 | Loss: 0.00001882
Iteration 652/1000 | Loss: 0.00001882
Iteration 653/1000 | Loss: 0.00001882
Iteration 654/1000 | Loss: 0.00001881
Iteration 655/1000 | Loss: 0.00001881
Iteration 656/1000 | Loss: 0.00001880
Iteration 657/1000 | Loss: 0.00001880
Iteration 658/1000 | Loss: 0.00001880
Iteration 659/1000 | Loss: 0.00001879
Iteration 660/1000 | Loss: 0.00001879
Iteration 661/1000 | Loss: 0.00001879
Iteration 662/1000 | Loss: 0.00001879
Iteration 663/1000 | Loss: 0.00001878
Iteration 664/1000 | Loss: 0.00001878
Iteration 665/1000 | Loss: 0.00001878
Iteration 666/1000 | Loss: 0.00001878
Iteration 667/1000 | Loss: 0.00001878
Iteration 668/1000 | Loss: 0.00001878
Iteration 669/1000 | Loss: 0.00001878
Iteration 670/1000 | Loss: 0.00001878
Iteration 671/1000 | Loss: 0.00001877
Iteration 672/1000 | Loss: 0.00001877
Iteration 673/1000 | Loss: 0.00001877
Iteration 674/1000 | Loss: 0.00001877
Iteration 675/1000 | Loss: 0.00001877
Iteration 676/1000 | Loss: 0.00001877
Iteration 677/1000 | Loss: 0.00001877
Iteration 678/1000 | Loss: 0.00001877
Iteration 679/1000 | Loss: 0.00001876
Iteration 680/1000 | Loss: 0.00001876
Iteration 681/1000 | Loss: 0.00001876
Iteration 682/1000 | Loss: 0.00001876
Iteration 683/1000 | Loss: 0.00001875
Iteration 684/1000 | Loss: 0.00001875
Iteration 685/1000 | Loss: 0.00001875
Iteration 686/1000 | Loss: 0.00001875
Iteration 687/1000 | Loss: 0.00001875
Iteration 688/1000 | Loss: 0.00001875
Iteration 689/1000 | Loss: 0.00001875
Iteration 690/1000 | Loss: 0.00001874
Iteration 691/1000 | Loss: 0.00001874
Iteration 692/1000 | Loss: 0.00001874
Iteration 693/1000 | Loss: 0.00001874
Iteration 694/1000 | Loss: 0.00001873
Iteration 695/1000 | Loss: 0.00001873
Iteration 696/1000 | Loss: 0.00001873
Iteration 697/1000 | Loss: 0.00001873
Iteration 698/1000 | Loss: 0.00001873
Iteration 699/1000 | Loss: 0.00001872
Iteration 700/1000 | Loss: 0.00001872
Iteration 701/1000 | Loss: 0.00001872
Iteration 702/1000 | Loss: 0.00001872
Iteration 703/1000 | Loss: 0.00001872
Iteration 704/1000 | Loss: 0.00001872
Iteration 705/1000 | Loss: 0.00001872
Iteration 706/1000 | Loss: 0.00001872
Iteration 707/1000 | Loss: 0.00001872
Iteration 708/1000 | Loss: 0.00001872
Iteration 709/1000 | Loss: 0.00001872
Iteration 710/1000 | Loss: 0.00001872
Iteration 711/1000 | Loss: 0.00001871
Iteration 712/1000 | Loss: 0.00001871
Iteration 713/1000 | Loss: 0.00001871
Iteration 714/1000 | Loss: 0.00001871
Iteration 715/1000 | Loss: 0.00001871
Iteration 716/1000 | Loss: 0.00001871
Iteration 717/1000 | Loss: 0.00001871
Iteration 718/1000 | Loss: 0.00001871
Iteration 719/1000 | Loss: 0.00001871
Iteration 720/1000 | Loss: 0.00001871
Iteration 721/1000 | Loss: 0.00001871
Iteration 722/1000 | Loss: 0.00001871
Iteration 723/1000 | Loss: 0.00001871
Iteration 724/1000 | Loss: 0.00001871
Iteration 725/1000 | Loss: 0.00001871
Iteration 726/1000 | Loss: 0.00001871
Iteration 727/1000 | Loss: 0.00001871
Iteration 728/1000 | Loss: 0.00001871
Iteration 729/1000 | Loss: 0.00001871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 729. Stopping optimization.
Last 5 losses: [1.871037602541037e-05, 1.871037602541037e-05, 1.871037602541037e-05, 1.871037602541037e-05, 1.871037602541037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.871037602541037e-05

Optimization complete. Final v2v error: 3.556626081466675 mm

Highest mean error: 5.552128314971924 mm for frame 45

Lowest mean error: 2.716928005218506 mm for frame 99

Saving results

Total time: 936.9401209354401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502214
Iteration 2/25 | Loss: 0.00123430
Iteration 3/25 | Loss: 0.00086472
Iteration 4/25 | Loss: 0.00077222
Iteration 5/25 | Loss: 0.00075092
Iteration 6/25 | Loss: 0.00074508
Iteration 7/25 | Loss: 0.00074396
Iteration 8/25 | Loss: 0.00074396
Iteration 9/25 | Loss: 0.00074396
Iteration 10/25 | Loss: 0.00074396
Iteration 11/25 | Loss: 0.00074396
Iteration 12/25 | Loss: 0.00074396
Iteration 13/25 | Loss: 0.00074396
Iteration 14/25 | Loss: 0.00074396
Iteration 15/25 | Loss: 0.00074396
Iteration 16/25 | Loss: 0.00074396
Iteration 17/25 | Loss: 0.00074396
Iteration 18/25 | Loss: 0.00074396
Iteration 19/25 | Loss: 0.00074396
Iteration 20/25 | Loss: 0.00074396
Iteration 21/25 | Loss: 0.00074396
Iteration 22/25 | Loss: 0.00074396
Iteration 23/25 | Loss: 0.00074396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007439571199938655, 0.0007439571199938655, 0.0007439571199938655, 0.0007439571199938655, 0.0007439571199938655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007439571199938655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56770551
Iteration 2/25 | Loss: 0.00083341
Iteration 3/25 | Loss: 0.00083341
Iteration 4/25 | Loss: 0.00083341
Iteration 5/25 | Loss: 0.00083341
Iteration 6/25 | Loss: 0.00083341
Iteration 7/25 | Loss: 0.00083341
Iteration 8/25 | Loss: 0.00083341
Iteration 9/25 | Loss: 0.00083341
Iteration 10/25 | Loss: 0.00083341
Iteration 11/25 | Loss: 0.00083341
Iteration 12/25 | Loss: 0.00083341
Iteration 13/25 | Loss: 0.00083341
Iteration 14/25 | Loss: 0.00083341
Iteration 15/25 | Loss: 0.00083341
Iteration 16/25 | Loss: 0.00083341
Iteration 17/25 | Loss: 0.00083341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008334111771546304, 0.0008334111771546304, 0.0008334111771546304, 0.0008334111771546304, 0.0008334111771546304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008334111771546304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083341
Iteration 2/1000 | Loss: 0.00002491
Iteration 3/1000 | Loss: 0.00001864
Iteration 4/1000 | Loss: 0.00001698
Iteration 5/1000 | Loss: 0.00001603
Iteration 6/1000 | Loss: 0.00001544
Iteration 7/1000 | Loss: 0.00001504
Iteration 8/1000 | Loss: 0.00001471
Iteration 9/1000 | Loss: 0.00001457
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001419
Iteration 14/1000 | Loss: 0.00001414
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001405
Iteration 18/1000 | Loss: 0.00001405
Iteration 19/1000 | Loss: 0.00001404
Iteration 20/1000 | Loss: 0.00001404
Iteration 21/1000 | Loss: 0.00001402
Iteration 22/1000 | Loss: 0.00001402
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001402
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001402
Iteration 27/1000 | Loss: 0.00001402
Iteration 28/1000 | Loss: 0.00001401
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001399
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001399
Iteration 40/1000 | Loss: 0.00001399
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001397
Iteration 45/1000 | Loss: 0.00001397
Iteration 46/1000 | Loss: 0.00001397
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001396
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001395
Iteration 62/1000 | Loss: 0.00001395
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001394
Iteration 69/1000 | Loss: 0.00001394
Iteration 70/1000 | Loss: 0.00001394
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.394300034007756e-05, 1.394300034007756e-05, 1.394300034007756e-05, 1.394300034007756e-05, 1.394300034007756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.394300034007756e-05

Optimization complete. Final v2v error: 3.1037814617156982 mm

Highest mean error: 4.33067512512207 mm for frame 90

Lowest mean error: 2.8329646587371826 mm for frame 28

Saving results

Total time: 34.58147144317627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856315
Iteration 2/25 | Loss: 0.00111341
Iteration 3/25 | Loss: 0.00088872
Iteration 4/25 | Loss: 0.00083554
Iteration 5/25 | Loss: 0.00080813
Iteration 6/25 | Loss: 0.00080217
Iteration 7/25 | Loss: 0.00079995
Iteration 8/25 | Loss: 0.00079916
Iteration 9/25 | Loss: 0.00079916
Iteration 10/25 | Loss: 0.00079916
Iteration 11/25 | Loss: 0.00079916
Iteration 12/25 | Loss: 0.00079916
Iteration 13/25 | Loss: 0.00079916
Iteration 14/25 | Loss: 0.00079916
Iteration 15/25 | Loss: 0.00079916
Iteration 16/25 | Loss: 0.00079916
Iteration 17/25 | Loss: 0.00079916
Iteration 18/25 | Loss: 0.00079916
Iteration 19/25 | Loss: 0.00079916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007991584716364741, 0.0007991584716364741, 0.0007991584716364741, 0.0007991584716364741, 0.0007991584716364741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007991584716364741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71564531
Iteration 2/25 | Loss: 0.00131580
Iteration 3/25 | Loss: 0.00131580
Iteration 4/25 | Loss: 0.00131580
Iteration 5/25 | Loss: 0.00131580
Iteration 6/25 | Loss: 0.00131580
Iteration 7/25 | Loss: 0.00131580
Iteration 8/25 | Loss: 0.00131580
Iteration 9/25 | Loss: 0.00131580
Iteration 10/25 | Loss: 0.00131580
Iteration 11/25 | Loss: 0.00131580
Iteration 12/25 | Loss: 0.00131580
Iteration 13/25 | Loss: 0.00131580
Iteration 14/25 | Loss: 0.00131580
Iteration 15/25 | Loss: 0.00131580
Iteration 16/25 | Loss: 0.00131580
Iteration 17/25 | Loss: 0.00131580
Iteration 18/25 | Loss: 0.00131580
Iteration 19/25 | Loss: 0.00131580
Iteration 20/25 | Loss: 0.00131580
Iteration 21/25 | Loss: 0.00131580
Iteration 22/25 | Loss: 0.00131580
Iteration 23/25 | Loss: 0.00131580
Iteration 24/25 | Loss: 0.00131580
Iteration 25/25 | Loss: 0.00131580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131580
Iteration 2/1000 | Loss: 0.00005127
Iteration 3/1000 | Loss: 0.00004087
Iteration 4/1000 | Loss: 0.00003333
Iteration 5/1000 | Loss: 0.00003037
Iteration 6/1000 | Loss: 0.00002853
Iteration 7/1000 | Loss: 0.00002738
Iteration 8/1000 | Loss: 0.00002661
Iteration 9/1000 | Loss: 0.00002571
Iteration 10/1000 | Loss: 0.00002499
Iteration 11/1000 | Loss: 0.00002464
Iteration 12/1000 | Loss: 0.00002433
Iteration 13/1000 | Loss: 0.00002404
Iteration 14/1000 | Loss: 0.00002385
Iteration 15/1000 | Loss: 0.00002369
Iteration 16/1000 | Loss: 0.00002366
Iteration 17/1000 | Loss: 0.00002360
Iteration 18/1000 | Loss: 0.00002360
Iteration 19/1000 | Loss: 0.00002359
Iteration 20/1000 | Loss: 0.00002358
Iteration 21/1000 | Loss: 0.00002357
Iteration 22/1000 | Loss: 0.00002357
Iteration 23/1000 | Loss: 0.00002356
Iteration 24/1000 | Loss: 0.00002356
Iteration 25/1000 | Loss: 0.00002355
Iteration 26/1000 | Loss: 0.00002354
Iteration 27/1000 | Loss: 0.00002354
Iteration 28/1000 | Loss: 0.00002353
Iteration 29/1000 | Loss: 0.00002352
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002352
Iteration 32/1000 | Loss: 0.00002351
Iteration 33/1000 | Loss: 0.00002351
Iteration 34/1000 | Loss: 0.00002351
Iteration 35/1000 | Loss: 0.00002351
Iteration 36/1000 | Loss: 0.00002351
Iteration 37/1000 | Loss: 0.00002351
Iteration 38/1000 | Loss: 0.00002351
Iteration 39/1000 | Loss: 0.00002350
Iteration 40/1000 | Loss: 0.00002350
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002348
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002347
Iteration 45/1000 | Loss: 0.00002347
Iteration 46/1000 | Loss: 0.00002347
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002346
Iteration 50/1000 | Loss: 0.00002346
Iteration 51/1000 | Loss: 0.00002346
Iteration 52/1000 | Loss: 0.00002345
Iteration 53/1000 | Loss: 0.00002345
Iteration 54/1000 | Loss: 0.00002344
Iteration 55/1000 | Loss: 0.00002344
Iteration 56/1000 | Loss: 0.00002344
Iteration 57/1000 | Loss: 0.00002344
Iteration 58/1000 | Loss: 0.00002343
Iteration 59/1000 | Loss: 0.00002343
Iteration 60/1000 | Loss: 0.00002343
Iteration 61/1000 | Loss: 0.00002342
Iteration 62/1000 | Loss: 0.00002342
Iteration 63/1000 | Loss: 0.00002341
Iteration 64/1000 | Loss: 0.00002341
Iteration 65/1000 | Loss: 0.00002341
Iteration 66/1000 | Loss: 0.00002341
Iteration 67/1000 | Loss: 0.00002341
Iteration 68/1000 | Loss: 0.00002341
Iteration 69/1000 | Loss: 0.00002341
Iteration 70/1000 | Loss: 0.00002341
Iteration 71/1000 | Loss: 0.00002340
Iteration 72/1000 | Loss: 0.00002340
Iteration 73/1000 | Loss: 0.00002340
Iteration 74/1000 | Loss: 0.00002340
Iteration 75/1000 | Loss: 0.00002340
Iteration 76/1000 | Loss: 0.00002340
Iteration 77/1000 | Loss: 0.00002340
Iteration 78/1000 | Loss: 0.00002340
Iteration 79/1000 | Loss: 0.00002339
Iteration 80/1000 | Loss: 0.00002339
Iteration 81/1000 | Loss: 0.00002339
Iteration 82/1000 | Loss: 0.00002338
Iteration 83/1000 | Loss: 0.00002338
Iteration 84/1000 | Loss: 0.00002338
Iteration 85/1000 | Loss: 0.00002338
Iteration 86/1000 | Loss: 0.00002337
Iteration 87/1000 | Loss: 0.00002337
Iteration 88/1000 | Loss: 0.00002337
Iteration 89/1000 | Loss: 0.00002336
Iteration 90/1000 | Loss: 0.00002336
Iteration 91/1000 | Loss: 0.00002336
Iteration 92/1000 | Loss: 0.00002336
Iteration 93/1000 | Loss: 0.00002336
Iteration 94/1000 | Loss: 0.00002336
Iteration 95/1000 | Loss: 0.00002336
Iteration 96/1000 | Loss: 0.00002336
Iteration 97/1000 | Loss: 0.00002335
Iteration 98/1000 | Loss: 0.00002335
Iteration 99/1000 | Loss: 0.00002335
Iteration 100/1000 | Loss: 0.00002334
Iteration 101/1000 | Loss: 0.00002334
Iteration 102/1000 | Loss: 0.00002334
Iteration 103/1000 | Loss: 0.00002333
Iteration 104/1000 | Loss: 0.00002333
Iteration 105/1000 | Loss: 0.00002333
Iteration 106/1000 | Loss: 0.00002333
Iteration 107/1000 | Loss: 0.00002332
Iteration 108/1000 | Loss: 0.00002332
Iteration 109/1000 | Loss: 0.00002332
Iteration 110/1000 | Loss: 0.00002332
Iteration 111/1000 | Loss: 0.00002332
Iteration 112/1000 | Loss: 0.00002332
Iteration 113/1000 | Loss: 0.00002331
Iteration 114/1000 | Loss: 0.00002331
Iteration 115/1000 | Loss: 0.00002331
Iteration 116/1000 | Loss: 0.00002331
Iteration 117/1000 | Loss: 0.00002331
Iteration 118/1000 | Loss: 0.00002331
Iteration 119/1000 | Loss: 0.00002331
Iteration 120/1000 | Loss: 0.00002331
Iteration 121/1000 | Loss: 0.00002330
Iteration 122/1000 | Loss: 0.00002330
Iteration 123/1000 | Loss: 0.00002330
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002330
Iteration 126/1000 | Loss: 0.00002329
Iteration 127/1000 | Loss: 0.00002329
Iteration 128/1000 | Loss: 0.00002329
Iteration 129/1000 | Loss: 0.00002329
Iteration 130/1000 | Loss: 0.00002329
Iteration 131/1000 | Loss: 0.00002328
Iteration 132/1000 | Loss: 0.00002328
Iteration 133/1000 | Loss: 0.00002328
Iteration 134/1000 | Loss: 0.00002328
Iteration 135/1000 | Loss: 0.00002328
Iteration 136/1000 | Loss: 0.00002328
Iteration 137/1000 | Loss: 0.00002328
Iteration 138/1000 | Loss: 0.00002328
Iteration 139/1000 | Loss: 0.00002328
Iteration 140/1000 | Loss: 0.00002328
Iteration 141/1000 | Loss: 0.00002328
Iteration 142/1000 | Loss: 0.00002328
Iteration 143/1000 | Loss: 0.00002328
Iteration 144/1000 | Loss: 0.00002328
Iteration 145/1000 | Loss: 0.00002328
Iteration 146/1000 | Loss: 0.00002328
Iteration 147/1000 | Loss: 0.00002328
Iteration 148/1000 | Loss: 0.00002328
Iteration 149/1000 | Loss: 0.00002328
Iteration 150/1000 | Loss: 0.00002328
Iteration 151/1000 | Loss: 0.00002328
Iteration 152/1000 | Loss: 0.00002328
Iteration 153/1000 | Loss: 0.00002328
Iteration 154/1000 | Loss: 0.00002328
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.3275058993021958e-05, 2.3275058993021958e-05, 2.3275058993021958e-05, 2.3275058993021958e-05, 2.3275058993021958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3275058993021958e-05

Optimization complete. Final v2v error: 3.9345273971557617 mm

Highest mean error: 6.119812488555908 mm for frame 123

Lowest mean error: 2.7665855884552 mm for frame 1

Saving results

Total time: 44.376911878585815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980121
Iteration 2/25 | Loss: 0.00531213
Iteration 3/25 | Loss: 0.00378470
Iteration 4/25 | Loss: 0.00282886
Iteration 5/25 | Loss: 0.00277008
Iteration 6/25 | Loss: 0.00222736
Iteration 7/25 | Loss: 0.00186316
Iteration 8/25 | Loss: 0.00172698
Iteration 9/25 | Loss: 0.00164195
Iteration 10/25 | Loss: 0.00158151
Iteration 11/25 | Loss: 0.00154793
Iteration 12/25 | Loss: 0.00157998
Iteration 13/25 | Loss: 0.00160146
Iteration 14/25 | Loss: 0.00146226
Iteration 15/25 | Loss: 0.00139348
Iteration 16/25 | Loss: 0.00135018
Iteration 17/25 | Loss: 0.00135002
Iteration 18/25 | Loss: 0.00133015
Iteration 19/25 | Loss: 0.00132118
Iteration 20/25 | Loss: 0.00130056
Iteration 21/25 | Loss: 0.00129142
Iteration 22/25 | Loss: 0.00128230
Iteration 23/25 | Loss: 0.00127817
Iteration 24/25 | Loss: 0.00127837
Iteration 25/25 | Loss: 0.00127174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52846062
Iteration 2/25 | Loss: 0.00293115
Iteration 3/25 | Loss: 0.00281077
Iteration 4/25 | Loss: 0.00281076
Iteration 5/25 | Loss: 0.00281076
Iteration 6/25 | Loss: 0.00281076
Iteration 7/25 | Loss: 0.00281076
Iteration 8/25 | Loss: 0.00281076
Iteration 9/25 | Loss: 0.00281076
Iteration 10/25 | Loss: 0.00281076
Iteration 11/25 | Loss: 0.00281076
Iteration 12/25 | Loss: 0.00281076
Iteration 13/25 | Loss: 0.00281076
Iteration 14/25 | Loss: 0.00281076
Iteration 15/25 | Loss: 0.00281076
Iteration 16/25 | Loss: 0.00281076
Iteration 17/25 | Loss: 0.00281076
Iteration 18/25 | Loss: 0.00281076
Iteration 19/25 | Loss: 0.00281076
Iteration 20/25 | Loss: 0.00281076
Iteration 21/25 | Loss: 0.00281076
Iteration 22/25 | Loss: 0.00281076
Iteration 23/25 | Loss: 0.00281076
Iteration 24/25 | Loss: 0.00281076
Iteration 25/25 | Loss: 0.00281076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00281076
Iteration 2/1000 | Loss: 0.00060176
Iteration 3/1000 | Loss: 0.00069895
Iteration 4/1000 | Loss: 0.00053680
Iteration 5/1000 | Loss: 0.01251881
Iteration 6/1000 | Loss: 0.01253699
Iteration 7/1000 | Loss: 0.00597768
Iteration 8/1000 | Loss: 0.00338973
Iteration 9/1000 | Loss: 0.00301065
Iteration 10/1000 | Loss: 0.00327243
Iteration 11/1000 | Loss: 0.00240110
Iteration 12/1000 | Loss: 0.00154271
Iteration 13/1000 | Loss: 0.00019719
Iteration 14/1000 | Loss: 0.00142956
Iteration 15/1000 | Loss: 0.00099753
Iteration 16/1000 | Loss: 0.00065172
Iteration 17/1000 | Loss: 0.00039861
Iteration 18/1000 | Loss: 0.00013857
Iteration 19/1000 | Loss: 0.00011288
Iteration 20/1000 | Loss: 0.00010668
Iteration 21/1000 | Loss: 0.00019828
Iteration 22/1000 | Loss: 0.00008884
Iteration 23/1000 | Loss: 0.00018096
Iteration 24/1000 | Loss: 0.00026065
Iteration 25/1000 | Loss: 0.00014359
Iteration 26/1000 | Loss: 0.00054331
Iteration 27/1000 | Loss: 0.00048766
Iteration 28/1000 | Loss: 0.00032983
Iteration 29/1000 | Loss: 0.00018864
Iteration 30/1000 | Loss: 0.00031657
Iteration 31/1000 | Loss: 0.00011062
Iteration 32/1000 | Loss: 0.00030412
Iteration 33/1000 | Loss: 0.00039816
Iteration 34/1000 | Loss: 0.00063002
Iteration 35/1000 | Loss: 0.00048432
Iteration 36/1000 | Loss: 0.00073305
Iteration 37/1000 | Loss: 0.00017788
Iteration 38/1000 | Loss: 0.00007772
Iteration 39/1000 | Loss: 0.00007725
Iteration 40/1000 | Loss: 0.00007337
Iteration 41/1000 | Loss: 0.00006769
Iteration 42/1000 | Loss: 0.00019333
Iteration 43/1000 | Loss: 0.00038482
Iteration 44/1000 | Loss: 0.00011728
Iteration 45/1000 | Loss: 0.00009923
Iteration 46/1000 | Loss: 0.00006632
Iteration 47/1000 | Loss: 0.00006563
Iteration 48/1000 | Loss: 0.00021861
Iteration 49/1000 | Loss: 0.00007925
Iteration 50/1000 | Loss: 0.00007237
Iteration 51/1000 | Loss: 0.00005725
Iteration 52/1000 | Loss: 0.00005344
Iteration 53/1000 | Loss: 0.00059928
Iteration 54/1000 | Loss: 0.00020908
Iteration 55/1000 | Loss: 0.00005341
Iteration 56/1000 | Loss: 0.00028119
Iteration 57/1000 | Loss: 0.00015915
Iteration 58/1000 | Loss: 0.00028956
Iteration 59/1000 | Loss: 0.00014968
Iteration 60/1000 | Loss: 0.00035532
Iteration 61/1000 | Loss: 0.00014354
Iteration 62/1000 | Loss: 0.00028874
Iteration 63/1000 | Loss: 0.00011550
Iteration 64/1000 | Loss: 0.00005471
Iteration 65/1000 | Loss: 0.00005157
Iteration 66/1000 | Loss: 0.00005916
Iteration 67/1000 | Loss: 0.00004733
Iteration 68/1000 | Loss: 0.00005378
Iteration 69/1000 | Loss: 0.00005467
Iteration 70/1000 | Loss: 0.00023186
Iteration 71/1000 | Loss: 0.00058660
Iteration 72/1000 | Loss: 0.00029312
Iteration 73/1000 | Loss: 0.00031877
Iteration 74/1000 | Loss: 0.00022225
Iteration 75/1000 | Loss: 0.00046178
Iteration 76/1000 | Loss: 0.00039070
Iteration 77/1000 | Loss: 0.00042621
Iteration 78/1000 | Loss: 0.00023938
Iteration 79/1000 | Loss: 0.00016749
Iteration 80/1000 | Loss: 0.00006013
Iteration 81/1000 | Loss: 0.00005893
Iteration 82/1000 | Loss: 0.00005516
Iteration 83/1000 | Loss: 0.00016469
Iteration 84/1000 | Loss: 0.00005182
Iteration 85/1000 | Loss: 0.00031750
Iteration 86/1000 | Loss: 0.00014129
Iteration 87/1000 | Loss: 0.00005105
Iteration 88/1000 | Loss: 0.00037546
Iteration 89/1000 | Loss: 0.00008187
Iteration 90/1000 | Loss: 0.00005788
Iteration 91/1000 | Loss: 0.00005275
Iteration 92/1000 | Loss: 0.00004851
Iteration 93/1000 | Loss: 0.00005117
Iteration 94/1000 | Loss: 0.00005012
Iteration 95/1000 | Loss: 0.00053253
Iteration 96/1000 | Loss: 0.00017365
Iteration 97/1000 | Loss: 0.00005727
Iteration 98/1000 | Loss: 0.00005286
Iteration 99/1000 | Loss: 0.00053140
Iteration 100/1000 | Loss: 0.00017574
Iteration 101/1000 | Loss: 0.00005728
Iteration 102/1000 | Loss: 0.00011276
Iteration 103/1000 | Loss: 0.00005537
Iteration 104/1000 | Loss: 0.00042755
Iteration 105/1000 | Loss: 0.00006075
Iteration 106/1000 | Loss: 0.00012769
Iteration 107/1000 | Loss: 0.00011265
Iteration 108/1000 | Loss: 0.00009845
Iteration 109/1000 | Loss: 0.00005033
Iteration 110/1000 | Loss: 0.00005296
Iteration 111/1000 | Loss: 0.00004837
Iteration 112/1000 | Loss: 0.00005270
Iteration 113/1000 | Loss: 0.00015825
Iteration 114/1000 | Loss: 0.00007683
Iteration 115/1000 | Loss: 0.00004848
Iteration 116/1000 | Loss: 0.00005162
Iteration 117/1000 | Loss: 0.00030122
Iteration 118/1000 | Loss: 0.00019124
Iteration 119/1000 | Loss: 0.00028128
Iteration 120/1000 | Loss: 0.00143842
Iteration 121/1000 | Loss: 0.00012858
Iteration 122/1000 | Loss: 0.00131781
Iteration 123/1000 | Loss: 0.00017184
Iteration 124/1000 | Loss: 0.00006306
Iteration 125/1000 | Loss: 0.00004969
Iteration 126/1000 | Loss: 0.00005008
Iteration 127/1000 | Loss: 0.00079720
Iteration 128/1000 | Loss: 0.00019446
Iteration 129/1000 | Loss: 0.00051957
Iteration 130/1000 | Loss: 0.00004681
Iteration 131/1000 | Loss: 0.00003222
Iteration 132/1000 | Loss: 0.00002836
Iteration 133/1000 | Loss: 0.00032294
Iteration 134/1000 | Loss: 0.00021543
Iteration 135/1000 | Loss: 0.00031970
Iteration 136/1000 | Loss: 0.00020974
Iteration 137/1000 | Loss: 0.00002749
Iteration 138/1000 | Loss: 0.00002607
Iteration 139/1000 | Loss: 0.00028960
Iteration 140/1000 | Loss: 0.00019838
Iteration 141/1000 | Loss: 0.00030804
Iteration 142/1000 | Loss: 0.00042653
Iteration 143/1000 | Loss: 0.00003098
Iteration 144/1000 | Loss: 0.00016765
Iteration 145/1000 | Loss: 0.00002595
Iteration 146/1000 | Loss: 0.00002470
Iteration 147/1000 | Loss: 0.00002423
Iteration 148/1000 | Loss: 0.00002395
Iteration 149/1000 | Loss: 0.00002373
Iteration 150/1000 | Loss: 0.00002346
Iteration 151/1000 | Loss: 0.00002320
Iteration 152/1000 | Loss: 0.00002290
Iteration 153/1000 | Loss: 0.00006237
Iteration 154/1000 | Loss: 0.00020449
Iteration 155/1000 | Loss: 0.00023606
Iteration 156/1000 | Loss: 0.00004742
Iteration 157/1000 | Loss: 0.00003410
Iteration 158/1000 | Loss: 0.00054543
Iteration 159/1000 | Loss: 0.00005373
Iteration 160/1000 | Loss: 0.00002535
Iteration 161/1000 | Loss: 0.00009713
Iteration 162/1000 | Loss: 0.00007159
Iteration 163/1000 | Loss: 0.00004233
Iteration 164/1000 | Loss: 0.00005513
Iteration 165/1000 | Loss: 0.00006062
Iteration 166/1000 | Loss: 0.00005543
Iteration 167/1000 | Loss: 0.00020889
Iteration 168/1000 | Loss: 0.00006658
Iteration 169/1000 | Loss: 0.00005469
Iteration 170/1000 | Loss: 0.00005535
Iteration 171/1000 | Loss: 0.00009529
Iteration 172/1000 | Loss: 0.00002432
Iteration 173/1000 | Loss: 0.00006246
Iteration 174/1000 | Loss: 0.00002284
Iteration 175/1000 | Loss: 0.00002233
Iteration 176/1000 | Loss: 0.00002198
Iteration 177/1000 | Loss: 0.00002187
Iteration 178/1000 | Loss: 0.00002179
Iteration 179/1000 | Loss: 0.00002179
Iteration 180/1000 | Loss: 0.00002178
Iteration 181/1000 | Loss: 0.00002178
Iteration 182/1000 | Loss: 0.00002178
Iteration 183/1000 | Loss: 0.00002177
Iteration 184/1000 | Loss: 0.00002177
Iteration 185/1000 | Loss: 0.00002170
Iteration 186/1000 | Loss: 0.00002163
Iteration 187/1000 | Loss: 0.00002159
Iteration 188/1000 | Loss: 0.00002158
Iteration 189/1000 | Loss: 0.00002154
Iteration 190/1000 | Loss: 0.00002147
Iteration 191/1000 | Loss: 0.00002147
Iteration 192/1000 | Loss: 0.00002146
Iteration 193/1000 | Loss: 0.00002143
Iteration 194/1000 | Loss: 0.00002143
Iteration 195/1000 | Loss: 0.00002142
Iteration 196/1000 | Loss: 0.00002142
Iteration 197/1000 | Loss: 0.00002142
Iteration 198/1000 | Loss: 0.00002142
Iteration 199/1000 | Loss: 0.00002141
Iteration 200/1000 | Loss: 0.00002141
Iteration 201/1000 | Loss: 0.00002141
Iteration 202/1000 | Loss: 0.00002141
Iteration 203/1000 | Loss: 0.00002140
Iteration 204/1000 | Loss: 0.00002140
Iteration 205/1000 | Loss: 0.00002140
Iteration 206/1000 | Loss: 0.00002140
Iteration 207/1000 | Loss: 0.00002140
Iteration 208/1000 | Loss: 0.00002140
Iteration 209/1000 | Loss: 0.00002140
Iteration 210/1000 | Loss: 0.00002140
Iteration 211/1000 | Loss: 0.00002140
Iteration 212/1000 | Loss: 0.00002139
Iteration 213/1000 | Loss: 0.00002139
Iteration 214/1000 | Loss: 0.00002139
Iteration 215/1000 | Loss: 0.00002139
Iteration 216/1000 | Loss: 0.00002139
Iteration 217/1000 | Loss: 0.00002138
Iteration 218/1000 | Loss: 0.00002138
Iteration 219/1000 | Loss: 0.00002138
Iteration 220/1000 | Loss: 0.00002138
Iteration 221/1000 | Loss: 0.00002138
Iteration 222/1000 | Loss: 0.00002137
Iteration 223/1000 | Loss: 0.00002137
Iteration 224/1000 | Loss: 0.00002137
Iteration 225/1000 | Loss: 0.00002137
Iteration 226/1000 | Loss: 0.00002137
Iteration 227/1000 | Loss: 0.00002137
Iteration 228/1000 | Loss: 0.00002137
Iteration 229/1000 | Loss: 0.00002137
Iteration 230/1000 | Loss: 0.00002137
Iteration 231/1000 | Loss: 0.00002136
Iteration 232/1000 | Loss: 0.00002136
Iteration 233/1000 | Loss: 0.00002136
Iteration 234/1000 | Loss: 0.00002136
Iteration 235/1000 | Loss: 0.00002136
Iteration 236/1000 | Loss: 0.00002136
Iteration 237/1000 | Loss: 0.00002136
Iteration 238/1000 | Loss: 0.00002136
Iteration 239/1000 | Loss: 0.00002135
Iteration 240/1000 | Loss: 0.00002135
Iteration 241/1000 | Loss: 0.00002135
Iteration 242/1000 | Loss: 0.00002135
Iteration 243/1000 | Loss: 0.00002135
Iteration 244/1000 | Loss: 0.00002135
Iteration 245/1000 | Loss: 0.00002135
Iteration 246/1000 | Loss: 0.00002135
Iteration 247/1000 | Loss: 0.00002135
Iteration 248/1000 | Loss: 0.00002135
Iteration 249/1000 | Loss: 0.00002135
Iteration 250/1000 | Loss: 0.00002135
Iteration 251/1000 | Loss: 0.00002135
Iteration 252/1000 | Loss: 0.00002135
Iteration 253/1000 | Loss: 0.00002135
Iteration 254/1000 | Loss: 0.00002135
Iteration 255/1000 | Loss: 0.00002135
Iteration 256/1000 | Loss: 0.00002135
Iteration 257/1000 | Loss: 0.00002135
Iteration 258/1000 | Loss: 0.00002134
Iteration 259/1000 | Loss: 0.00002134
Iteration 260/1000 | Loss: 0.00002134
Iteration 261/1000 | Loss: 0.00002134
Iteration 262/1000 | Loss: 0.00002134
Iteration 263/1000 | Loss: 0.00002134
Iteration 264/1000 | Loss: 0.00002134
Iteration 265/1000 | Loss: 0.00002134
Iteration 266/1000 | Loss: 0.00002133
Iteration 267/1000 | Loss: 0.00002133
Iteration 268/1000 | Loss: 0.00002133
Iteration 269/1000 | Loss: 0.00002133
Iteration 270/1000 | Loss: 0.00002133
Iteration 271/1000 | Loss: 0.00002133
Iteration 272/1000 | Loss: 0.00002133
Iteration 273/1000 | Loss: 0.00002133
Iteration 274/1000 | Loss: 0.00002133
Iteration 275/1000 | Loss: 0.00002133
Iteration 276/1000 | Loss: 0.00002132
Iteration 277/1000 | Loss: 0.00002132
Iteration 278/1000 | Loss: 0.00002132
Iteration 279/1000 | Loss: 0.00002132
Iteration 280/1000 | Loss: 0.00002132
Iteration 281/1000 | Loss: 0.00002132
Iteration 282/1000 | Loss: 0.00002132
Iteration 283/1000 | Loss: 0.00002132
Iteration 284/1000 | Loss: 0.00002132
Iteration 285/1000 | Loss: 0.00002132
Iteration 286/1000 | Loss: 0.00002132
Iteration 287/1000 | Loss: 0.00002132
Iteration 288/1000 | Loss: 0.00002132
Iteration 289/1000 | Loss: 0.00002132
Iteration 290/1000 | Loss: 0.00002132
Iteration 291/1000 | Loss: 0.00002132
Iteration 292/1000 | Loss: 0.00002132
Iteration 293/1000 | Loss: 0.00002132
Iteration 294/1000 | Loss: 0.00002132
Iteration 295/1000 | Loss: 0.00002132
Iteration 296/1000 | Loss: 0.00002132
Iteration 297/1000 | Loss: 0.00002132
Iteration 298/1000 | Loss: 0.00002132
Iteration 299/1000 | Loss: 0.00002132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [2.1324613044271246e-05, 2.1324613044271246e-05, 2.1324613044271246e-05, 2.1324613044271246e-05, 2.1324613044271246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1324613044271246e-05

Optimization complete. Final v2v error: 3.882375717163086 mm

Highest mean error: 4.513616561889648 mm for frame 169

Lowest mean error: 3.5008349418640137 mm for frame 21

Saving results

Total time: 328.77731251716614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810545
Iteration 2/25 | Loss: 0.00110463
Iteration 3/25 | Loss: 0.00090120
Iteration 4/25 | Loss: 0.00085231
Iteration 5/25 | Loss: 0.00083484
Iteration 6/25 | Loss: 0.00083011
Iteration 7/25 | Loss: 0.00082870
Iteration 8/25 | Loss: 0.00082849
Iteration 9/25 | Loss: 0.00082849
Iteration 10/25 | Loss: 0.00082849
Iteration 11/25 | Loss: 0.00082849
Iteration 12/25 | Loss: 0.00082849
Iteration 13/25 | Loss: 0.00082849
Iteration 14/25 | Loss: 0.00082849
Iteration 15/25 | Loss: 0.00082849
Iteration 16/25 | Loss: 0.00082849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008284944342449307, 0.0008284944342449307, 0.0008284944342449307, 0.0008284944342449307, 0.0008284944342449307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008284944342449307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57667983
Iteration 2/25 | Loss: 0.00107372
Iteration 3/25 | Loss: 0.00107371
Iteration 4/25 | Loss: 0.00107370
Iteration 5/25 | Loss: 0.00107370
Iteration 6/25 | Loss: 0.00107370
Iteration 7/25 | Loss: 0.00107370
Iteration 8/25 | Loss: 0.00107370
Iteration 9/25 | Loss: 0.00107370
Iteration 10/25 | Loss: 0.00107370
Iteration 11/25 | Loss: 0.00107370
Iteration 12/25 | Loss: 0.00107370
Iteration 13/25 | Loss: 0.00107370
Iteration 14/25 | Loss: 0.00107370
Iteration 15/25 | Loss: 0.00107370
Iteration 16/25 | Loss: 0.00107370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001073703053407371, 0.001073703053407371, 0.001073703053407371, 0.001073703053407371, 0.001073703053407371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001073703053407371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107370
Iteration 2/1000 | Loss: 0.00004721
Iteration 3/1000 | Loss: 0.00003453
Iteration 4/1000 | Loss: 0.00003002
Iteration 5/1000 | Loss: 0.00002832
Iteration 6/1000 | Loss: 0.00002690
Iteration 7/1000 | Loss: 0.00002625
Iteration 8/1000 | Loss: 0.00002559
Iteration 9/1000 | Loss: 0.00002517
Iteration 10/1000 | Loss: 0.00002516
Iteration 11/1000 | Loss: 0.00002486
Iteration 12/1000 | Loss: 0.00002476
Iteration 13/1000 | Loss: 0.00002468
Iteration 14/1000 | Loss: 0.00002446
Iteration 15/1000 | Loss: 0.00002438
Iteration 16/1000 | Loss: 0.00002436
Iteration 17/1000 | Loss: 0.00002432
Iteration 18/1000 | Loss: 0.00002430
Iteration 19/1000 | Loss: 0.00002429
Iteration 20/1000 | Loss: 0.00002429
Iteration 21/1000 | Loss: 0.00002425
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002421
Iteration 24/1000 | Loss: 0.00002420
Iteration 25/1000 | Loss: 0.00002420
Iteration 26/1000 | Loss: 0.00002420
Iteration 27/1000 | Loss: 0.00002419
Iteration 28/1000 | Loss: 0.00002418
Iteration 29/1000 | Loss: 0.00002418
Iteration 30/1000 | Loss: 0.00002418
Iteration 31/1000 | Loss: 0.00002418
Iteration 32/1000 | Loss: 0.00002417
Iteration 33/1000 | Loss: 0.00002417
Iteration 34/1000 | Loss: 0.00002417
Iteration 35/1000 | Loss: 0.00002416
Iteration 36/1000 | Loss: 0.00002416
Iteration 37/1000 | Loss: 0.00002416
Iteration 38/1000 | Loss: 0.00002416
Iteration 39/1000 | Loss: 0.00002416
Iteration 40/1000 | Loss: 0.00002415
Iteration 41/1000 | Loss: 0.00002415
Iteration 42/1000 | Loss: 0.00002415
Iteration 43/1000 | Loss: 0.00002415
Iteration 44/1000 | Loss: 0.00002414
Iteration 45/1000 | Loss: 0.00002414
Iteration 46/1000 | Loss: 0.00002414
Iteration 47/1000 | Loss: 0.00002414
Iteration 48/1000 | Loss: 0.00002414
Iteration 49/1000 | Loss: 0.00002414
Iteration 50/1000 | Loss: 0.00002414
Iteration 51/1000 | Loss: 0.00002414
Iteration 52/1000 | Loss: 0.00002414
Iteration 53/1000 | Loss: 0.00002414
Iteration 54/1000 | Loss: 0.00002413
Iteration 55/1000 | Loss: 0.00002413
Iteration 56/1000 | Loss: 0.00002413
Iteration 57/1000 | Loss: 0.00002413
Iteration 58/1000 | Loss: 0.00002413
Iteration 59/1000 | Loss: 0.00002412
Iteration 60/1000 | Loss: 0.00002412
Iteration 61/1000 | Loss: 0.00002412
Iteration 62/1000 | Loss: 0.00002411
Iteration 63/1000 | Loss: 0.00002411
Iteration 64/1000 | Loss: 0.00002411
Iteration 65/1000 | Loss: 0.00002411
Iteration 66/1000 | Loss: 0.00002410
Iteration 67/1000 | Loss: 0.00002410
Iteration 68/1000 | Loss: 0.00002410
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002410
Iteration 71/1000 | Loss: 0.00002409
Iteration 72/1000 | Loss: 0.00002409
Iteration 73/1000 | Loss: 0.00002409
Iteration 74/1000 | Loss: 0.00002409
Iteration 75/1000 | Loss: 0.00002408
Iteration 76/1000 | Loss: 0.00002408
Iteration 77/1000 | Loss: 0.00002408
Iteration 78/1000 | Loss: 0.00002408
Iteration 79/1000 | Loss: 0.00002408
Iteration 80/1000 | Loss: 0.00002407
Iteration 81/1000 | Loss: 0.00002407
Iteration 82/1000 | Loss: 0.00002407
Iteration 83/1000 | Loss: 0.00002407
Iteration 84/1000 | Loss: 0.00002407
Iteration 85/1000 | Loss: 0.00002406
Iteration 86/1000 | Loss: 0.00002406
Iteration 87/1000 | Loss: 0.00002406
Iteration 88/1000 | Loss: 0.00002406
Iteration 89/1000 | Loss: 0.00002406
Iteration 90/1000 | Loss: 0.00002406
Iteration 91/1000 | Loss: 0.00002406
Iteration 92/1000 | Loss: 0.00002406
Iteration 93/1000 | Loss: 0.00002406
Iteration 94/1000 | Loss: 0.00002406
Iteration 95/1000 | Loss: 0.00002406
Iteration 96/1000 | Loss: 0.00002406
Iteration 97/1000 | Loss: 0.00002406
Iteration 98/1000 | Loss: 0.00002406
Iteration 99/1000 | Loss: 0.00002405
Iteration 100/1000 | Loss: 0.00002405
Iteration 101/1000 | Loss: 0.00002405
Iteration 102/1000 | Loss: 0.00002405
Iteration 103/1000 | Loss: 0.00002405
Iteration 104/1000 | Loss: 0.00002405
Iteration 105/1000 | Loss: 0.00002405
Iteration 106/1000 | Loss: 0.00002405
Iteration 107/1000 | Loss: 0.00002405
Iteration 108/1000 | Loss: 0.00002405
Iteration 109/1000 | Loss: 0.00002405
Iteration 110/1000 | Loss: 0.00002405
Iteration 111/1000 | Loss: 0.00002405
Iteration 112/1000 | Loss: 0.00002405
Iteration 113/1000 | Loss: 0.00002404
Iteration 114/1000 | Loss: 0.00002404
Iteration 115/1000 | Loss: 0.00002404
Iteration 116/1000 | Loss: 0.00002404
Iteration 117/1000 | Loss: 0.00002404
Iteration 118/1000 | Loss: 0.00002403
Iteration 119/1000 | Loss: 0.00002403
Iteration 120/1000 | Loss: 0.00002403
Iteration 121/1000 | Loss: 0.00002403
Iteration 122/1000 | Loss: 0.00002403
Iteration 123/1000 | Loss: 0.00002403
Iteration 124/1000 | Loss: 0.00002403
Iteration 125/1000 | Loss: 0.00002403
Iteration 126/1000 | Loss: 0.00002403
Iteration 127/1000 | Loss: 0.00002403
Iteration 128/1000 | Loss: 0.00002403
Iteration 129/1000 | Loss: 0.00002402
Iteration 130/1000 | Loss: 0.00002402
Iteration 131/1000 | Loss: 0.00002402
Iteration 132/1000 | Loss: 0.00002402
Iteration 133/1000 | Loss: 0.00002402
Iteration 134/1000 | Loss: 0.00002402
Iteration 135/1000 | Loss: 0.00002402
Iteration 136/1000 | Loss: 0.00002402
Iteration 137/1000 | Loss: 0.00002402
Iteration 138/1000 | Loss: 0.00002402
Iteration 139/1000 | Loss: 0.00002402
Iteration 140/1000 | Loss: 0.00002402
Iteration 141/1000 | Loss: 0.00002402
Iteration 142/1000 | Loss: 0.00002402
Iteration 143/1000 | Loss: 0.00002402
Iteration 144/1000 | Loss: 0.00002402
Iteration 145/1000 | Loss: 0.00002402
Iteration 146/1000 | Loss: 0.00002402
Iteration 147/1000 | Loss: 0.00002402
Iteration 148/1000 | Loss: 0.00002402
Iteration 149/1000 | Loss: 0.00002402
Iteration 150/1000 | Loss: 0.00002402
Iteration 151/1000 | Loss: 0.00002402
Iteration 152/1000 | Loss: 0.00002402
Iteration 153/1000 | Loss: 0.00002402
Iteration 154/1000 | Loss: 0.00002402
Iteration 155/1000 | Loss: 0.00002402
Iteration 156/1000 | Loss: 0.00002402
Iteration 157/1000 | Loss: 0.00002402
Iteration 158/1000 | Loss: 0.00002402
Iteration 159/1000 | Loss: 0.00002402
Iteration 160/1000 | Loss: 0.00002402
Iteration 161/1000 | Loss: 0.00002402
Iteration 162/1000 | Loss: 0.00002402
Iteration 163/1000 | Loss: 0.00002402
Iteration 164/1000 | Loss: 0.00002402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.4018032490857877e-05, 2.4018032490857877e-05, 2.4018032490857877e-05, 2.4018032490857877e-05, 2.4018032490857877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4018032490857877e-05

Optimization complete. Final v2v error: 4.015583038330078 mm

Highest mean error: 4.895249843597412 mm for frame 153

Lowest mean error: 3.5981404781341553 mm for frame 168

Saving results

Total time: 41.14190626144409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830370
Iteration 2/25 | Loss: 0.00099888
Iteration 3/25 | Loss: 0.00080070
Iteration 4/25 | Loss: 0.00075888
Iteration 5/25 | Loss: 0.00077682
Iteration 6/25 | Loss: 0.00075430
Iteration 7/25 | Loss: 0.00073714
Iteration 8/25 | Loss: 0.00072668
Iteration 9/25 | Loss: 0.00072501
Iteration 10/25 | Loss: 0.00072435
Iteration 11/25 | Loss: 0.00072401
Iteration 12/25 | Loss: 0.00072384
Iteration 13/25 | Loss: 0.00072371
Iteration 14/25 | Loss: 0.00072369
Iteration 15/25 | Loss: 0.00072369
Iteration 16/25 | Loss: 0.00072369
Iteration 17/25 | Loss: 0.00072368
Iteration 18/25 | Loss: 0.00072368
Iteration 19/25 | Loss: 0.00072368
Iteration 20/25 | Loss: 0.00072368
Iteration 21/25 | Loss: 0.00072368
Iteration 22/25 | Loss: 0.00072368
Iteration 23/25 | Loss: 0.00072368
Iteration 24/25 | Loss: 0.00072368
Iteration 25/25 | Loss: 0.00072368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53298950
Iteration 2/25 | Loss: 0.00079464
Iteration 3/25 | Loss: 0.00079463
Iteration 4/25 | Loss: 0.00079463
Iteration 5/25 | Loss: 0.00079463
Iteration 6/25 | Loss: 0.00079463
Iteration 7/25 | Loss: 0.00079463
Iteration 8/25 | Loss: 0.00079463
Iteration 9/25 | Loss: 0.00079463
Iteration 10/25 | Loss: 0.00079463
Iteration 11/25 | Loss: 0.00079463
Iteration 12/25 | Loss: 0.00079463
Iteration 13/25 | Loss: 0.00079463
Iteration 14/25 | Loss: 0.00079463
Iteration 15/25 | Loss: 0.00079463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007946331170387566, 0.0007946331170387566, 0.0007946331170387566, 0.0007946331170387566, 0.0007946331170387566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007946331170387566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079463
Iteration 2/1000 | Loss: 0.00003198
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001622
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001504
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001439
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001412
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001371
Iteration 23/1000 | Loss: 0.00001371
Iteration 24/1000 | Loss: 0.00001369
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001362
Iteration 33/1000 | Loss: 0.00001362
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001361
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001360
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001359
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001356
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001355
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001352
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001351
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00001348
Iteration 100/1000 | Loss: 0.00001348
Iteration 101/1000 | Loss: 0.00001348
Iteration 102/1000 | Loss: 0.00001348
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001347
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001347
Iteration 111/1000 | Loss: 0.00001347
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001346
Iteration 117/1000 | Loss: 0.00001346
Iteration 118/1000 | Loss: 0.00001346
Iteration 119/1000 | Loss: 0.00001346
Iteration 120/1000 | Loss: 0.00001346
Iteration 121/1000 | Loss: 0.00001346
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001345
Iteration 126/1000 | Loss: 0.00001345
Iteration 127/1000 | Loss: 0.00001345
Iteration 128/1000 | Loss: 0.00001345
Iteration 129/1000 | Loss: 0.00001345
Iteration 130/1000 | Loss: 0.00001345
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001344
Iteration 136/1000 | Loss: 0.00001344
Iteration 137/1000 | Loss: 0.00001344
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Iteration 141/1000 | Loss: 0.00001343
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001342
Iteration 150/1000 | Loss: 0.00001342
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001342
Iteration 158/1000 | Loss: 0.00001342
Iteration 159/1000 | Loss: 0.00001341
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001341
Iteration 166/1000 | Loss: 0.00001341
Iteration 167/1000 | Loss: 0.00001341
Iteration 168/1000 | Loss: 0.00001341
Iteration 169/1000 | Loss: 0.00001341
Iteration 170/1000 | Loss: 0.00001341
Iteration 171/1000 | Loss: 0.00001341
Iteration 172/1000 | Loss: 0.00001341
Iteration 173/1000 | Loss: 0.00001341
Iteration 174/1000 | Loss: 0.00001341
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001341
Iteration 177/1000 | Loss: 0.00001341
Iteration 178/1000 | Loss: 0.00001341
Iteration 179/1000 | Loss: 0.00001341
Iteration 180/1000 | Loss: 0.00001341
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001340
Iteration 188/1000 | Loss: 0.00001340
Iteration 189/1000 | Loss: 0.00001340
Iteration 190/1000 | Loss: 0.00001340
Iteration 191/1000 | Loss: 0.00001340
Iteration 192/1000 | Loss: 0.00001340
Iteration 193/1000 | Loss: 0.00001340
Iteration 194/1000 | Loss: 0.00001340
Iteration 195/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.340464223176241e-05, 1.340464223176241e-05, 1.340464223176241e-05, 1.340464223176241e-05, 1.340464223176241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.340464223176241e-05

Optimization complete. Final v2v error: 3.009122133255005 mm

Highest mean error: 4.423871994018555 mm for frame 224

Lowest mean error: 2.589327573776245 mm for frame 35

Saving results

Total time: 62.81637167930603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061671
Iteration 2/25 | Loss: 0.00397732
Iteration 3/25 | Loss: 0.00290106
Iteration 4/25 | Loss: 0.00223953
Iteration 5/25 | Loss: 0.00158245
Iteration 6/25 | Loss: 0.00125238
Iteration 7/25 | Loss: 0.00101491
Iteration 8/25 | Loss: 0.00094389
Iteration 9/25 | Loss: 0.00091466
Iteration 10/25 | Loss: 0.00088862
Iteration 11/25 | Loss: 0.00087190
Iteration 12/25 | Loss: 0.00086292
Iteration 13/25 | Loss: 0.00086336
Iteration 14/25 | Loss: 0.00085979
Iteration 15/25 | Loss: 0.00085678
Iteration 16/25 | Loss: 0.00086245
Iteration 17/25 | Loss: 0.00086133
Iteration 18/25 | Loss: 0.00085831
Iteration 19/25 | Loss: 0.00085733
Iteration 20/25 | Loss: 0.00085702
Iteration 21/25 | Loss: 0.00086100
Iteration 22/25 | Loss: 0.00086046
Iteration 23/25 | Loss: 0.00085934
Iteration 24/25 | Loss: 0.00085608
Iteration 25/25 | Loss: 0.00085376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54211032
Iteration 2/25 | Loss: 0.00128067
Iteration 3/25 | Loss: 0.00121992
Iteration 4/25 | Loss: 0.00121992
Iteration 5/25 | Loss: 0.00121992
Iteration 6/25 | Loss: 0.00121992
Iteration 7/25 | Loss: 0.00121992
Iteration 8/25 | Loss: 0.00121992
Iteration 9/25 | Loss: 0.00121992
Iteration 10/25 | Loss: 0.00121992
Iteration 11/25 | Loss: 0.00121992
Iteration 12/25 | Loss: 0.00121992
Iteration 13/25 | Loss: 0.00121992
Iteration 14/25 | Loss: 0.00121992
Iteration 15/25 | Loss: 0.00121992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001219920814037323, 0.001219920814037323, 0.001219920814037323, 0.001219920814037323, 0.001219920814037323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001219920814037323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121992
Iteration 2/1000 | Loss: 0.00015978
Iteration 3/1000 | Loss: 0.00006735
Iteration 4/1000 | Loss: 0.00005625
Iteration 5/1000 | Loss: 0.00004945
Iteration 6/1000 | Loss: 0.00004493
Iteration 7/1000 | Loss: 0.00004187
Iteration 8/1000 | Loss: 0.00003983
Iteration 9/1000 | Loss: 0.00003849
Iteration 10/1000 | Loss: 0.00249354
Iteration 11/1000 | Loss: 0.00004989
Iteration 12/1000 | Loss: 0.00003713
Iteration 13/1000 | Loss: 0.00002904
Iteration 14/1000 | Loss: 0.00002418
Iteration 15/1000 | Loss: 0.00002050
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001806
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001715
Iteration 20/1000 | Loss: 0.00001684
Iteration 21/1000 | Loss: 0.00001662
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001647
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001645
Iteration 29/1000 | Loss: 0.00001641
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001632
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001623
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001619
Iteration 53/1000 | Loss: 0.00001619
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001618
Iteration 62/1000 | Loss: 0.00001618
Iteration 63/1000 | Loss: 0.00001618
Iteration 64/1000 | Loss: 0.00001617
Iteration 65/1000 | Loss: 0.00001617
Iteration 66/1000 | Loss: 0.00001617
Iteration 67/1000 | Loss: 0.00001617
Iteration 68/1000 | Loss: 0.00001617
Iteration 69/1000 | Loss: 0.00001617
Iteration 70/1000 | Loss: 0.00001616
Iteration 71/1000 | Loss: 0.00001616
Iteration 72/1000 | Loss: 0.00001616
Iteration 73/1000 | Loss: 0.00001616
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001615
Iteration 81/1000 | Loss: 0.00001615
Iteration 82/1000 | Loss: 0.00001615
Iteration 83/1000 | Loss: 0.00001615
Iteration 84/1000 | Loss: 0.00001615
Iteration 85/1000 | Loss: 0.00001615
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001615
Iteration 95/1000 | Loss: 0.00001615
Iteration 96/1000 | Loss: 0.00001615
Iteration 97/1000 | Loss: 0.00001615
Iteration 98/1000 | Loss: 0.00001615
Iteration 99/1000 | Loss: 0.00001615
Iteration 100/1000 | Loss: 0.00001615
Iteration 101/1000 | Loss: 0.00001615
Iteration 102/1000 | Loss: 0.00001615
Iteration 103/1000 | Loss: 0.00001615
Iteration 104/1000 | Loss: 0.00001615
Iteration 105/1000 | Loss: 0.00001615
Iteration 106/1000 | Loss: 0.00001615
Iteration 107/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.614819120732136e-05, 1.614819120732136e-05, 1.614819120732136e-05, 1.614819120732136e-05, 1.614819120732136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.614819120732136e-05

Optimization complete. Final v2v error: 3.379366397857666 mm

Highest mean error: 3.5340769290924072 mm for frame 7

Lowest mean error: 3.185509443283081 mm for frame 0

Saving results

Total time: 83.84659504890442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845616
Iteration 2/25 | Loss: 0.00097428
Iteration 3/25 | Loss: 0.00075808
Iteration 4/25 | Loss: 0.00072219
Iteration 5/25 | Loss: 0.00071192
Iteration 6/25 | Loss: 0.00070957
Iteration 7/25 | Loss: 0.00070892
Iteration 8/25 | Loss: 0.00070892
Iteration 9/25 | Loss: 0.00070892
Iteration 10/25 | Loss: 0.00070892
Iteration 11/25 | Loss: 0.00070892
Iteration 12/25 | Loss: 0.00070892
Iteration 13/25 | Loss: 0.00070892
Iteration 14/25 | Loss: 0.00070892
Iteration 15/25 | Loss: 0.00070892
Iteration 16/25 | Loss: 0.00070892
Iteration 17/25 | Loss: 0.00070892
Iteration 18/25 | Loss: 0.00070892
Iteration 19/25 | Loss: 0.00070892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007089204154908657, 0.0007089204154908657, 0.0007089204154908657, 0.0007089204154908657, 0.0007089204154908657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007089204154908657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56719422
Iteration 2/25 | Loss: 0.00079414
Iteration 3/25 | Loss: 0.00079413
Iteration 4/25 | Loss: 0.00079413
Iteration 5/25 | Loss: 0.00079413
Iteration 6/25 | Loss: 0.00079413
Iteration 7/25 | Loss: 0.00079413
Iteration 8/25 | Loss: 0.00079413
Iteration 9/25 | Loss: 0.00079413
Iteration 10/25 | Loss: 0.00079413
Iteration 11/25 | Loss: 0.00079413
Iteration 12/25 | Loss: 0.00079413
Iteration 13/25 | Loss: 0.00079413
Iteration 14/25 | Loss: 0.00079413
Iteration 15/25 | Loss: 0.00079413
Iteration 16/25 | Loss: 0.00079413
Iteration 17/25 | Loss: 0.00079413
Iteration 18/25 | Loss: 0.00079413
Iteration 19/25 | Loss: 0.00079413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007941302610561252, 0.0007941302610561252, 0.0007941302610561252, 0.0007941302610561252, 0.0007941302610561252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007941302610561252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079413
Iteration 2/1000 | Loss: 0.00002521
Iteration 3/1000 | Loss: 0.00001633
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001317
Iteration 7/1000 | Loss: 0.00001280
Iteration 8/1000 | Loss: 0.00001251
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001206
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001202
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001201
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001193
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001187
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001178
Iteration 48/1000 | Loss: 0.00001178
Iteration 49/1000 | Loss: 0.00001178
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001175
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001170
Iteration 87/1000 | Loss: 0.00001170
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001168
Iteration 92/1000 | Loss: 0.00001168
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001166
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001165
Iteration 112/1000 | Loss: 0.00001164
Iteration 113/1000 | Loss: 0.00001164
Iteration 114/1000 | Loss: 0.00001164
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001164
Iteration 120/1000 | Loss: 0.00001164
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001163
Iteration 132/1000 | Loss: 0.00001163
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001163
Iteration 135/1000 | Loss: 0.00001162
Iteration 136/1000 | Loss: 0.00001162
Iteration 137/1000 | Loss: 0.00001162
Iteration 138/1000 | Loss: 0.00001162
Iteration 139/1000 | Loss: 0.00001162
Iteration 140/1000 | Loss: 0.00001162
Iteration 141/1000 | Loss: 0.00001162
Iteration 142/1000 | Loss: 0.00001162
Iteration 143/1000 | Loss: 0.00001162
Iteration 144/1000 | Loss: 0.00001162
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001161
Iteration 147/1000 | Loss: 0.00001161
Iteration 148/1000 | Loss: 0.00001161
Iteration 149/1000 | Loss: 0.00001161
Iteration 150/1000 | Loss: 0.00001161
Iteration 151/1000 | Loss: 0.00001161
Iteration 152/1000 | Loss: 0.00001161
Iteration 153/1000 | Loss: 0.00001161
Iteration 154/1000 | Loss: 0.00001161
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001160
Iteration 165/1000 | Loss: 0.00001160
Iteration 166/1000 | Loss: 0.00001160
Iteration 167/1000 | Loss: 0.00001160
Iteration 168/1000 | Loss: 0.00001160
Iteration 169/1000 | Loss: 0.00001160
Iteration 170/1000 | Loss: 0.00001160
Iteration 171/1000 | Loss: 0.00001160
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001159
Iteration 175/1000 | Loss: 0.00001159
Iteration 176/1000 | Loss: 0.00001159
Iteration 177/1000 | Loss: 0.00001159
Iteration 178/1000 | Loss: 0.00001159
Iteration 179/1000 | Loss: 0.00001159
Iteration 180/1000 | Loss: 0.00001159
Iteration 181/1000 | Loss: 0.00001159
Iteration 182/1000 | Loss: 0.00001159
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001159
Iteration 190/1000 | Loss: 0.00001159
Iteration 191/1000 | Loss: 0.00001159
Iteration 192/1000 | Loss: 0.00001159
Iteration 193/1000 | Loss: 0.00001159
Iteration 194/1000 | Loss: 0.00001159
Iteration 195/1000 | Loss: 0.00001159
Iteration 196/1000 | Loss: 0.00001159
Iteration 197/1000 | Loss: 0.00001159
Iteration 198/1000 | Loss: 0.00001159
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001158
Iteration 204/1000 | Loss: 0.00001158
Iteration 205/1000 | Loss: 0.00001158
Iteration 206/1000 | Loss: 0.00001158
Iteration 207/1000 | Loss: 0.00001158
Iteration 208/1000 | Loss: 0.00001158
Iteration 209/1000 | Loss: 0.00001158
Iteration 210/1000 | Loss: 0.00001158
Iteration 211/1000 | Loss: 0.00001158
Iteration 212/1000 | Loss: 0.00001158
Iteration 213/1000 | Loss: 0.00001158
Iteration 214/1000 | Loss: 0.00001158
Iteration 215/1000 | Loss: 0.00001158
Iteration 216/1000 | Loss: 0.00001158
Iteration 217/1000 | Loss: 0.00001158
Iteration 218/1000 | Loss: 0.00001158
Iteration 219/1000 | Loss: 0.00001158
Iteration 220/1000 | Loss: 0.00001157
Iteration 221/1000 | Loss: 0.00001157
Iteration 222/1000 | Loss: 0.00001157
Iteration 223/1000 | Loss: 0.00001157
Iteration 224/1000 | Loss: 0.00001157
Iteration 225/1000 | Loss: 0.00001157
Iteration 226/1000 | Loss: 0.00001157
Iteration 227/1000 | Loss: 0.00001157
Iteration 228/1000 | Loss: 0.00001157
Iteration 229/1000 | Loss: 0.00001157
Iteration 230/1000 | Loss: 0.00001157
Iteration 231/1000 | Loss: 0.00001157
Iteration 232/1000 | Loss: 0.00001157
Iteration 233/1000 | Loss: 0.00001157
Iteration 234/1000 | Loss: 0.00001157
Iteration 235/1000 | Loss: 0.00001157
Iteration 236/1000 | Loss: 0.00001157
Iteration 237/1000 | Loss: 0.00001157
Iteration 238/1000 | Loss: 0.00001156
Iteration 239/1000 | Loss: 0.00001156
Iteration 240/1000 | Loss: 0.00001156
Iteration 241/1000 | Loss: 0.00001156
Iteration 242/1000 | Loss: 0.00001156
Iteration 243/1000 | Loss: 0.00001156
Iteration 244/1000 | Loss: 0.00001156
Iteration 245/1000 | Loss: 0.00001156
Iteration 246/1000 | Loss: 0.00001156
Iteration 247/1000 | Loss: 0.00001156
Iteration 248/1000 | Loss: 0.00001156
Iteration 249/1000 | Loss: 0.00001156
Iteration 250/1000 | Loss: 0.00001156
Iteration 251/1000 | Loss: 0.00001156
Iteration 252/1000 | Loss: 0.00001156
Iteration 253/1000 | Loss: 0.00001156
Iteration 254/1000 | Loss: 0.00001156
Iteration 255/1000 | Loss: 0.00001156
Iteration 256/1000 | Loss: 0.00001156
Iteration 257/1000 | Loss: 0.00001156
Iteration 258/1000 | Loss: 0.00001156
Iteration 259/1000 | Loss: 0.00001156
Iteration 260/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.1560690836631693e-05, 1.1560690836631693e-05, 1.1560690836631693e-05, 1.1560690836631693e-05, 1.1560690836631693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1560690836631693e-05

Optimization complete. Final v2v error: 2.862060785293579 mm

Highest mean error: 3.8766324520111084 mm for frame 86

Lowest mean error: 2.4973862171173096 mm for frame 115

Saving results

Total time: 42.785255670547485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090565
Iteration 2/25 | Loss: 0.00143785
Iteration 3/25 | Loss: 0.00101239
Iteration 4/25 | Loss: 0.00092256
Iteration 5/25 | Loss: 0.00089972
Iteration 6/25 | Loss: 0.00089240
Iteration 7/25 | Loss: 0.00089049
Iteration 8/25 | Loss: 0.00089047
Iteration 9/25 | Loss: 0.00089047
Iteration 10/25 | Loss: 0.00089047
Iteration 11/25 | Loss: 0.00089047
Iteration 12/25 | Loss: 0.00089047
Iteration 13/25 | Loss: 0.00089047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008904710994102061, 0.0008904710994102061, 0.0008904710994102061, 0.0008904710994102061, 0.0008904710994102061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008904710994102061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07411039
Iteration 2/25 | Loss: 0.00122215
Iteration 3/25 | Loss: 0.00122211
Iteration 4/25 | Loss: 0.00122211
Iteration 5/25 | Loss: 0.00122211
Iteration 6/25 | Loss: 0.00122211
Iteration 7/25 | Loss: 0.00122211
Iteration 8/25 | Loss: 0.00122211
Iteration 9/25 | Loss: 0.00122211
Iteration 10/25 | Loss: 0.00122211
Iteration 11/25 | Loss: 0.00122211
Iteration 12/25 | Loss: 0.00122211
Iteration 13/25 | Loss: 0.00122211
Iteration 14/25 | Loss: 0.00122211
Iteration 15/25 | Loss: 0.00122211
Iteration 16/25 | Loss: 0.00122211
Iteration 17/25 | Loss: 0.00122211
Iteration 18/25 | Loss: 0.00122211
Iteration 19/25 | Loss: 0.00122211
Iteration 20/25 | Loss: 0.00122211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012221060460433364, 0.0012221060460433364, 0.0012221060460433364, 0.0012221060460433364, 0.0012221060460433364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012221060460433364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122211
Iteration 2/1000 | Loss: 0.00004353
Iteration 3/1000 | Loss: 0.00003249
Iteration 4/1000 | Loss: 0.00002898
Iteration 5/1000 | Loss: 0.00002770
Iteration 6/1000 | Loss: 0.00002693
Iteration 7/1000 | Loss: 0.00002623
Iteration 8/1000 | Loss: 0.00002582
Iteration 9/1000 | Loss: 0.00002545
Iteration 10/1000 | Loss: 0.00002517
Iteration 11/1000 | Loss: 0.00002499
Iteration 12/1000 | Loss: 0.00002495
Iteration 13/1000 | Loss: 0.00002480
Iteration 14/1000 | Loss: 0.00002468
Iteration 15/1000 | Loss: 0.00002453
Iteration 16/1000 | Loss: 0.00002443
Iteration 17/1000 | Loss: 0.00002436
Iteration 18/1000 | Loss: 0.00002436
Iteration 19/1000 | Loss: 0.00002432
Iteration 20/1000 | Loss: 0.00002432
Iteration 21/1000 | Loss: 0.00002431
Iteration 22/1000 | Loss: 0.00002431
Iteration 23/1000 | Loss: 0.00002431
Iteration 24/1000 | Loss: 0.00002430
Iteration 25/1000 | Loss: 0.00002428
Iteration 26/1000 | Loss: 0.00002427
Iteration 27/1000 | Loss: 0.00002427
Iteration 28/1000 | Loss: 0.00002427
Iteration 29/1000 | Loss: 0.00002427
Iteration 30/1000 | Loss: 0.00002427
Iteration 31/1000 | Loss: 0.00002427
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002426
Iteration 34/1000 | Loss: 0.00002425
Iteration 35/1000 | Loss: 0.00002425
Iteration 36/1000 | Loss: 0.00002424
Iteration 37/1000 | Loss: 0.00002424
Iteration 38/1000 | Loss: 0.00002424
Iteration 39/1000 | Loss: 0.00002424
Iteration 40/1000 | Loss: 0.00002423
Iteration 41/1000 | Loss: 0.00002423
Iteration 42/1000 | Loss: 0.00002423
Iteration 43/1000 | Loss: 0.00002423
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00002423
Iteration 46/1000 | Loss: 0.00002423
Iteration 47/1000 | Loss: 0.00002423
Iteration 48/1000 | Loss: 0.00002423
Iteration 49/1000 | Loss: 0.00002423
Iteration 50/1000 | Loss: 0.00002422
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002421
Iteration 55/1000 | Loss: 0.00002421
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002421
Iteration 58/1000 | Loss: 0.00002421
Iteration 59/1000 | Loss: 0.00002421
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002421
Iteration 63/1000 | Loss: 0.00002421
Iteration 64/1000 | Loss: 0.00002421
Iteration 65/1000 | Loss: 0.00002421
Iteration 66/1000 | Loss: 0.00002420
Iteration 67/1000 | Loss: 0.00002420
Iteration 68/1000 | Loss: 0.00002420
Iteration 69/1000 | Loss: 0.00002419
Iteration 70/1000 | Loss: 0.00002419
Iteration 71/1000 | Loss: 0.00002418
Iteration 72/1000 | Loss: 0.00002418
Iteration 73/1000 | Loss: 0.00002418
Iteration 74/1000 | Loss: 0.00002418
Iteration 75/1000 | Loss: 0.00002418
Iteration 76/1000 | Loss: 0.00002418
Iteration 77/1000 | Loss: 0.00002418
Iteration 78/1000 | Loss: 0.00002418
Iteration 79/1000 | Loss: 0.00002418
Iteration 80/1000 | Loss: 0.00002417
Iteration 81/1000 | Loss: 0.00002417
Iteration 82/1000 | Loss: 0.00002417
Iteration 83/1000 | Loss: 0.00002416
Iteration 84/1000 | Loss: 0.00002416
Iteration 85/1000 | Loss: 0.00002416
Iteration 86/1000 | Loss: 0.00002416
Iteration 87/1000 | Loss: 0.00002416
Iteration 88/1000 | Loss: 0.00002416
Iteration 89/1000 | Loss: 0.00002416
Iteration 90/1000 | Loss: 0.00002416
Iteration 91/1000 | Loss: 0.00002416
Iteration 92/1000 | Loss: 0.00002415
Iteration 93/1000 | Loss: 0.00002415
Iteration 94/1000 | Loss: 0.00002415
Iteration 95/1000 | Loss: 0.00002415
Iteration 96/1000 | Loss: 0.00002415
Iteration 97/1000 | Loss: 0.00002415
Iteration 98/1000 | Loss: 0.00002415
Iteration 99/1000 | Loss: 0.00002415
Iteration 100/1000 | Loss: 0.00002415
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Iteration 103/1000 | Loss: 0.00002415
Iteration 104/1000 | Loss: 0.00002415
Iteration 105/1000 | Loss: 0.00002415
Iteration 106/1000 | Loss: 0.00002415
Iteration 107/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.41512680076994e-05, 2.41512680076994e-05, 2.41512680076994e-05, 2.41512680076994e-05, 2.41512680076994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.41512680076994e-05

Optimization complete. Final v2v error: 4.061001300811768 mm

Highest mean error: 5.0723490715026855 mm for frame 158

Lowest mean error: 3.3100836277008057 mm for frame 233

Saving results

Total time: 44.58901381492615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862611
Iteration 2/25 | Loss: 0.00120717
Iteration 3/25 | Loss: 0.00087020
Iteration 4/25 | Loss: 0.00082936
Iteration 5/25 | Loss: 0.00082263
Iteration 6/25 | Loss: 0.00082071
Iteration 7/25 | Loss: 0.00082067
Iteration 8/25 | Loss: 0.00082067
Iteration 9/25 | Loss: 0.00082067
Iteration 10/25 | Loss: 0.00082067
Iteration 11/25 | Loss: 0.00082067
Iteration 12/25 | Loss: 0.00082067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000820665096398443, 0.000820665096398443, 0.000820665096398443, 0.000820665096398443, 0.000820665096398443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000820665096398443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10548103
Iteration 2/25 | Loss: 0.00071045
Iteration 3/25 | Loss: 0.00071044
Iteration 4/25 | Loss: 0.00071044
Iteration 5/25 | Loss: 0.00071044
Iteration 6/25 | Loss: 0.00071044
Iteration 7/25 | Loss: 0.00071044
Iteration 8/25 | Loss: 0.00071044
Iteration 9/25 | Loss: 0.00071044
Iteration 10/25 | Loss: 0.00071044
Iteration 11/25 | Loss: 0.00071044
Iteration 12/25 | Loss: 0.00071044
Iteration 13/25 | Loss: 0.00071044
Iteration 14/25 | Loss: 0.00071044
Iteration 15/25 | Loss: 0.00071044
Iteration 16/25 | Loss: 0.00071044
Iteration 17/25 | Loss: 0.00071044
Iteration 18/25 | Loss: 0.00071044
Iteration 19/25 | Loss: 0.00071044
Iteration 20/25 | Loss: 0.00071044
Iteration 21/25 | Loss: 0.00071044
Iteration 22/25 | Loss: 0.00071044
Iteration 23/25 | Loss: 0.00071044
Iteration 24/25 | Loss: 0.00071044
Iteration 25/25 | Loss: 0.00071044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071044
Iteration 2/1000 | Loss: 0.00002782
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00002201
Iteration 5/1000 | Loss: 0.00002126
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002024
Iteration 8/1000 | Loss: 0.00002001
Iteration 9/1000 | Loss: 0.00001986
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001975
Iteration 12/1000 | Loss: 0.00001973
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001968
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001958
Iteration 17/1000 | Loss: 0.00001957
Iteration 18/1000 | Loss: 0.00001957
Iteration 19/1000 | Loss: 0.00001957
Iteration 20/1000 | Loss: 0.00001956
Iteration 21/1000 | Loss: 0.00001956
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001955
Iteration 25/1000 | Loss: 0.00001955
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001954
Iteration 28/1000 | Loss: 0.00001954
Iteration 29/1000 | Loss: 0.00001953
Iteration 30/1000 | Loss: 0.00001952
Iteration 31/1000 | Loss: 0.00001950
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001950
Iteration 35/1000 | Loss: 0.00001950
Iteration 36/1000 | Loss: 0.00001950
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001949
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001947
Iteration 43/1000 | Loss: 0.00001947
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001947
Iteration 46/1000 | Loss: 0.00001947
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001947
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00001946
Iteration 62/1000 | Loss: 0.00001946
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001946
Iteration 75/1000 | Loss: 0.00001946
Iteration 76/1000 | Loss: 0.00001946
Iteration 77/1000 | Loss: 0.00001946
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.9462244381429628e-05, 1.9462244381429628e-05, 1.9462244381429628e-05, 1.9462244381429628e-05, 1.9462244381429628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9462244381429628e-05

Optimization complete. Final v2v error: 3.7656149864196777 mm

Highest mean error: 4.138398170471191 mm for frame 34

Lowest mean error: 3.3870389461517334 mm for frame 149

Saving results

Total time: 28.151934385299683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010398
Iteration 2/25 | Loss: 0.01010398
Iteration 3/25 | Loss: 0.00443271
Iteration 4/25 | Loss: 0.00311273
Iteration 5/25 | Loss: 0.00285163
Iteration 6/25 | Loss: 0.00277996
Iteration 7/25 | Loss: 0.00206659
Iteration 8/25 | Loss: 0.00204728
Iteration 9/25 | Loss: 0.00182261
Iteration 10/25 | Loss: 0.00179162
Iteration 11/25 | Loss: 0.00169921
Iteration 12/25 | Loss: 0.00168492
Iteration 13/25 | Loss: 0.00162218
Iteration 14/25 | Loss: 0.00159697
Iteration 15/25 | Loss: 0.00159047
Iteration 16/25 | Loss: 0.00156541
Iteration 17/25 | Loss: 0.00154472
Iteration 18/25 | Loss: 0.00154321
Iteration 19/25 | Loss: 0.00153478
Iteration 20/25 | Loss: 0.00153640
Iteration 21/25 | Loss: 0.00152889
Iteration 22/25 | Loss: 0.00152804
Iteration 23/25 | Loss: 0.00152805
Iteration 24/25 | Loss: 0.00152609
Iteration 25/25 | Loss: 0.00152590

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48646569
Iteration 2/25 | Loss: 0.02098795
Iteration 3/25 | Loss: 0.00487343
Iteration 4/25 | Loss: 0.00461582
Iteration 5/25 | Loss: 0.00461582
Iteration 6/25 | Loss: 0.00461582
Iteration 7/25 | Loss: 0.00461582
Iteration 8/25 | Loss: 0.00461582
Iteration 9/25 | Loss: 0.00461582
Iteration 10/25 | Loss: 0.00461582
Iteration 11/25 | Loss: 0.00461582
Iteration 12/25 | Loss: 0.00461582
Iteration 13/25 | Loss: 0.00461582
Iteration 14/25 | Loss: 0.00461582
Iteration 15/25 | Loss: 0.00461582
Iteration 16/25 | Loss: 0.00461582
Iteration 17/25 | Loss: 0.00461582
Iteration 18/25 | Loss: 0.00461582
Iteration 19/25 | Loss: 0.00461582
Iteration 20/25 | Loss: 0.00461582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004615817219018936, 0.004615817219018936, 0.004615817219018936, 0.004615817219018936, 0.004615817219018936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004615817219018936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00461582
Iteration 2/1000 | Loss: 0.01156195
Iteration 3/1000 | Loss: 0.00086761
Iteration 4/1000 | Loss: 0.00221275
Iteration 5/1000 | Loss: 0.00528997
Iteration 6/1000 | Loss: 0.00466090
Iteration 7/1000 | Loss: 0.00633402
Iteration 8/1000 | Loss: 0.00190649
Iteration 9/1000 | Loss: 0.00383876
Iteration 10/1000 | Loss: 0.00150905
Iteration 11/1000 | Loss: 0.00150370
Iteration 12/1000 | Loss: 0.00105539
Iteration 13/1000 | Loss: 0.00049046
Iteration 14/1000 | Loss: 0.00085536
Iteration 15/1000 | Loss: 0.00261573
Iteration 16/1000 | Loss: 0.02021843
Iteration 17/1000 | Loss: 0.02839953
Iteration 18/1000 | Loss: 0.01306266
Iteration 19/1000 | Loss: 0.01231554
Iteration 20/1000 | Loss: 0.00892626
Iteration 21/1000 | Loss: 0.00326219
Iteration 22/1000 | Loss: 0.00454862
Iteration 23/1000 | Loss: 0.00176551
Iteration 24/1000 | Loss: 0.00326506
Iteration 25/1000 | Loss: 0.00257088
Iteration 26/1000 | Loss: 0.00136715
Iteration 27/1000 | Loss: 0.00263101
Iteration 28/1000 | Loss: 0.00092691
Iteration 29/1000 | Loss: 0.00485423
Iteration 30/1000 | Loss: 0.00218442
Iteration 31/1000 | Loss: 0.00262315
Iteration 32/1000 | Loss: 0.00018889
Iteration 33/1000 | Loss: 0.00055376
Iteration 34/1000 | Loss: 0.00037815
Iteration 35/1000 | Loss: 0.00089520
Iteration 36/1000 | Loss: 0.00081684
Iteration 37/1000 | Loss: 0.00008036
Iteration 38/1000 | Loss: 0.00061099
Iteration 39/1000 | Loss: 0.00103943
Iteration 40/1000 | Loss: 0.00082187
Iteration 41/1000 | Loss: 0.00007761
Iteration 42/1000 | Loss: 0.00004838
Iteration 43/1000 | Loss: 0.00018485
Iteration 44/1000 | Loss: 0.00176425
Iteration 45/1000 | Loss: 0.00033229
Iteration 46/1000 | Loss: 0.00049064
Iteration 47/1000 | Loss: 0.00010925
Iteration 48/1000 | Loss: 0.00005980
Iteration 49/1000 | Loss: 0.00032789
Iteration 50/1000 | Loss: 0.00077741
Iteration 51/1000 | Loss: 0.00015800
Iteration 52/1000 | Loss: 0.00015303
Iteration 53/1000 | Loss: 0.00011505
Iteration 54/1000 | Loss: 0.00002751
Iteration 55/1000 | Loss: 0.00014692
Iteration 56/1000 | Loss: 0.00037059
Iteration 57/1000 | Loss: 0.00064855
Iteration 58/1000 | Loss: 0.00025427
Iteration 59/1000 | Loss: 0.00151974
Iteration 60/1000 | Loss: 0.00020887
Iteration 61/1000 | Loss: 0.00006937
Iteration 62/1000 | Loss: 0.00002380
Iteration 63/1000 | Loss: 0.00045899
Iteration 64/1000 | Loss: 0.00007840
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00001950
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00037176
Iteration 69/1000 | Loss: 0.00001860
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001753
Iteration 72/1000 | Loss: 0.00001728
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00025895
Iteration 78/1000 | Loss: 0.00042339
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00001811
Iteration 81/1000 | Loss: 0.00001727
Iteration 82/1000 | Loss: 0.00030508
Iteration 83/1000 | Loss: 0.00003392
Iteration 84/1000 | Loss: 0.00004283
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001696
Iteration 87/1000 | Loss: 0.00001678
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001675
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001670
Iteration 95/1000 | Loss: 0.00001670
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001667
Iteration 99/1000 | Loss: 0.00001667
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001666
Iteration 102/1000 | Loss: 0.00001666
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001663
Iteration 111/1000 | Loss: 0.00001662
Iteration 112/1000 | Loss: 0.00001662
Iteration 113/1000 | Loss: 0.00001661
Iteration 114/1000 | Loss: 0.00001661
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001659
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001655
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001651
Iteration 123/1000 | Loss: 0.00001651
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001648
Iteration 133/1000 | Loss: 0.00001648
Iteration 134/1000 | Loss: 0.00001648
Iteration 135/1000 | Loss: 0.00001648
Iteration 136/1000 | Loss: 0.00001648
Iteration 137/1000 | Loss: 0.00001648
Iteration 138/1000 | Loss: 0.00001647
Iteration 139/1000 | Loss: 0.00001647
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001646
Iteration 142/1000 | Loss: 0.00001646
Iteration 143/1000 | Loss: 0.00001645
Iteration 144/1000 | Loss: 0.00001645
Iteration 145/1000 | Loss: 0.00001645
Iteration 146/1000 | Loss: 0.00001645
Iteration 147/1000 | Loss: 0.00001645
Iteration 148/1000 | Loss: 0.00001645
Iteration 149/1000 | Loss: 0.00001645
Iteration 150/1000 | Loss: 0.00001645
Iteration 151/1000 | Loss: 0.00001644
Iteration 152/1000 | Loss: 0.00001644
Iteration 153/1000 | Loss: 0.00001644
Iteration 154/1000 | Loss: 0.00001644
Iteration 155/1000 | Loss: 0.00001644
Iteration 156/1000 | Loss: 0.00001644
Iteration 157/1000 | Loss: 0.00001644
Iteration 158/1000 | Loss: 0.00001644
Iteration 159/1000 | Loss: 0.00001644
Iteration 160/1000 | Loss: 0.00001644
Iteration 161/1000 | Loss: 0.00001644
Iteration 162/1000 | Loss: 0.00001644
Iteration 163/1000 | Loss: 0.00001644
Iteration 164/1000 | Loss: 0.00001644
Iteration 165/1000 | Loss: 0.00001644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.6439647879451513e-05, 1.6439647879451513e-05, 1.6439647879451513e-05, 1.6439647879451513e-05, 1.6439647879451513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6439647879451513e-05

Optimization complete. Final v2v error: 3.5010154247283936 mm

Highest mean error: 4.880321979522705 mm for frame 17

Lowest mean error: 3.285487174987793 mm for frame 92

Saving results

Total time: 194.44600343704224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621754
Iteration 2/25 | Loss: 0.00082863
Iteration 3/25 | Loss: 0.00073977
Iteration 4/25 | Loss: 0.00072890
Iteration 5/25 | Loss: 0.00072499
Iteration 6/25 | Loss: 0.00072387
Iteration 7/25 | Loss: 0.00072387
Iteration 8/25 | Loss: 0.00072387
Iteration 9/25 | Loss: 0.00072387
Iteration 10/25 | Loss: 0.00072387
Iteration 11/25 | Loss: 0.00072387
Iteration 12/25 | Loss: 0.00072387
Iteration 13/25 | Loss: 0.00072387
Iteration 14/25 | Loss: 0.00072387
Iteration 15/25 | Loss: 0.00072387
Iteration 16/25 | Loss: 0.00072387
Iteration 17/25 | Loss: 0.00072387
Iteration 18/25 | Loss: 0.00072387
Iteration 19/25 | Loss: 0.00072387
Iteration 20/25 | Loss: 0.00072387
Iteration 21/25 | Loss: 0.00072387
Iteration 22/25 | Loss: 0.00072387
Iteration 23/25 | Loss: 0.00072387
Iteration 24/25 | Loss: 0.00072387
Iteration 25/25 | Loss: 0.00072387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.90609264
Iteration 2/25 | Loss: 0.00093399
Iteration 3/25 | Loss: 0.00093396
Iteration 4/25 | Loss: 0.00093396
Iteration 5/25 | Loss: 0.00093396
Iteration 6/25 | Loss: 0.00093395
Iteration 7/25 | Loss: 0.00093395
Iteration 8/25 | Loss: 0.00093395
Iteration 9/25 | Loss: 0.00093395
Iteration 10/25 | Loss: 0.00093395
Iteration 11/25 | Loss: 0.00093395
Iteration 12/25 | Loss: 0.00093395
Iteration 13/25 | Loss: 0.00093395
Iteration 14/25 | Loss: 0.00093395
Iteration 15/25 | Loss: 0.00093395
Iteration 16/25 | Loss: 0.00093395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009339535608887672, 0.0009339535608887672, 0.0009339535608887672, 0.0009339535608887672, 0.0009339535608887672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009339535608887672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093395
Iteration 2/1000 | Loss: 0.00002137
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001353
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001287
Iteration 8/1000 | Loss: 0.00001274
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001245
Iteration 12/1000 | Loss: 0.00001236
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001221
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001214
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00001212
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001211
Iteration 34/1000 | Loss: 0.00001211
Iteration 35/1000 | Loss: 0.00001210
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001210
Iteration 45/1000 | Loss: 0.00001210
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001210
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001208
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001207
Iteration 66/1000 | Loss: 0.00001207
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001197
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001197
Iteration 117/1000 | Loss: 0.00001197
Iteration 118/1000 | Loss: 0.00001197
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001197
Iteration 124/1000 | Loss: 0.00001197
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001197
Iteration 135/1000 | Loss: 0.00001197
Iteration 136/1000 | Loss: 0.00001197
Iteration 137/1000 | Loss: 0.00001197
Iteration 138/1000 | Loss: 0.00001197
Iteration 139/1000 | Loss: 0.00001197
Iteration 140/1000 | Loss: 0.00001197
Iteration 141/1000 | Loss: 0.00001197
Iteration 142/1000 | Loss: 0.00001197
Iteration 143/1000 | Loss: 0.00001197
Iteration 144/1000 | Loss: 0.00001197
Iteration 145/1000 | Loss: 0.00001197
Iteration 146/1000 | Loss: 0.00001197
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001197
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001197
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001197
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001197
Iteration 173/1000 | Loss: 0.00001197
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001197
Iteration 177/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.1965719750151038e-05, 1.1965719750151038e-05, 1.1965719750151038e-05, 1.1965719750151038e-05, 1.1965719750151038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1965719750151038e-05

Optimization complete. Final v2v error: 2.945833921432495 mm

Highest mean error: 3.251565933227539 mm for frame 117

Lowest mean error: 2.7355620861053467 mm for frame 21

Saving results

Total time: 38.303733348846436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017994
Iteration 2/25 | Loss: 0.00186971
Iteration 3/25 | Loss: 0.00124655
Iteration 4/25 | Loss: 0.00124213
Iteration 5/25 | Loss: 0.00103510
Iteration 6/25 | Loss: 0.00087137
Iteration 7/25 | Loss: 0.00083032
Iteration 8/25 | Loss: 0.00082958
Iteration 9/25 | Loss: 0.00082107
Iteration 10/25 | Loss: 0.00081432
Iteration 11/25 | Loss: 0.00080643
Iteration 12/25 | Loss: 0.00080468
Iteration 13/25 | Loss: 0.00079922
Iteration 14/25 | Loss: 0.00079028
Iteration 15/25 | Loss: 0.00078384
Iteration 16/25 | Loss: 0.00078461
Iteration 17/25 | Loss: 0.00078393
Iteration 18/25 | Loss: 0.00078382
Iteration 19/25 | Loss: 0.00078401
Iteration 20/25 | Loss: 0.00078438
Iteration 21/25 | Loss: 0.00078274
Iteration 22/25 | Loss: 0.00078310
Iteration 23/25 | Loss: 0.00078278
Iteration 24/25 | Loss: 0.00078335
Iteration 25/25 | Loss: 0.00078304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54307163
Iteration 2/25 | Loss: 0.00100159
Iteration 3/25 | Loss: 0.00100159
Iteration 4/25 | Loss: 0.00100159
Iteration 5/25 | Loss: 0.00100159
Iteration 6/25 | Loss: 0.00100159
Iteration 7/25 | Loss: 0.00100159
Iteration 8/25 | Loss: 0.00100159
Iteration 9/25 | Loss: 0.00100159
Iteration 10/25 | Loss: 0.00100159
Iteration 11/25 | Loss: 0.00100159
Iteration 12/25 | Loss: 0.00100159
Iteration 13/25 | Loss: 0.00100159
Iteration 14/25 | Loss: 0.00100159
Iteration 15/25 | Loss: 0.00100159
Iteration 16/25 | Loss: 0.00100159
Iteration 17/25 | Loss: 0.00100159
Iteration 18/25 | Loss: 0.00100159
Iteration 19/25 | Loss: 0.00100159
Iteration 20/25 | Loss: 0.00100159
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010015902807936072, 0.0010015902807936072, 0.0010015902807936072, 0.0010015902807936072, 0.0010015902807936072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010015902807936072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100159
Iteration 2/1000 | Loss: 0.00003781
Iteration 3/1000 | Loss: 0.00003214
Iteration 4/1000 | Loss: 0.00004913
Iteration 5/1000 | Loss: 0.00004617
Iteration 6/1000 | Loss: 0.00006384
Iteration 7/1000 | Loss: 0.00006556
Iteration 8/1000 | Loss: 0.00005706
Iteration 9/1000 | Loss: 0.00006491
Iteration 10/1000 | Loss: 0.00005775
Iteration 11/1000 | Loss: 0.00007359
Iteration 12/1000 | Loss: 0.00019687
Iteration 13/1000 | Loss: 0.00031534
Iteration 14/1000 | Loss: 0.00005004
Iteration 15/1000 | Loss: 0.00005275
Iteration 16/1000 | Loss: 0.00003485
Iteration 17/1000 | Loss: 0.00004077
Iteration 18/1000 | Loss: 0.00004713
Iteration 19/1000 | Loss: 0.00004340
Iteration 20/1000 | Loss: 0.00003717
Iteration 21/1000 | Loss: 0.00004033
Iteration 22/1000 | Loss: 0.00004412
Iteration 23/1000 | Loss: 0.00005311
Iteration 24/1000 | Loss: 0.00004776
Iteration 25/1000 | Loss: 0.00003938
Iteration 26/1000 | Loss: 0.00005104
Iteration 27/1000 | Loss: 0.00005240
Iteration 28/1000 | Loss: 0.00004625
Iteration 29/1000 | Loss: 0.00004355
Iteration 30/1000 | Loss: 0.00005061
Iteration 31/1000 | Loss: 0.00007577
Iteration 32/1000 | Loss: 0.00006681
Iteration 33/1000 | Loss: 0.00015927
Iteration 34/1000 | Loss: 0.00006006
Iteration 35/1000 | Loss: 0.00008459
Iteration 36/1000 | Loss: 0.00005564
Iteration 37/1000 | Loss: 0.00006078
Iteration 38/1000 | Loss: 0.00007858
Iteration 39/1000 | Loss: 0.00008816
Iteration 40/1000 | Loss: 0.00003788
Iteration 41/1000 | Loss: 0.00002715
Iteration 42/1000 | Loss: 0.00002402
Iteration 43/1000 | Loss: 0.00002994
Iteration 44/1000 | Loss: 0.00003333
Iteration 45/1000 | Loss: 0.00002581
Iteration 46/1000 | Loss: 0.00002844
Iteration 47/1000 | Loss: 0.00003487
Iteration 48/1000 | Loss: 0.00002957
Iteration 49/1000 | Loss: 0.00002564
Iteration 50/1000 | Loss: 0.00003287
Iteration 51/1000 | Loss: 0.00002547
Iteration 52/1000 | Loss: 0.00002409
Iteration 53/1000 | Loss: 0.00002821
Iteration 54/1000 | Loss: 0.00003068
Iteration 55/1000 | Loss: 0.00002821
Iteration 56/1000 | Loss: 0.00003027
Iteration 57/1000 | Loss: 0.00002864
Iteration 58/1000 | Loss: 0.00002855
Iteration 59/1000 | Loss: 0.00003304
Iteration 60/1000 | Loss: 0.00002787
Iteration 61/1000 | Loss: 0.00003274
Iteration 62/1000 | Loss: 0.00002600
Iteration 63/1000 | Loss: 0.00003343
Iteration 64/1000 | Loss: 0.00002831
Iteration 65/1000 | Loss: 0.00002908
Iteration 66/1000 | Loss: 0.00003173
Iteration 67/1000 | Loss: 0.00002888
Iteration 68/1000 | Loss: 0.00002255
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001500
Iteration 76/1000 | Loss: 0.00001500
Iteration 77/1000 | Loss: 0.00001500
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001499
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001497
Iteration 96/1000 | Loss: 0.00001497
Iteration 97/1000 | Loss: 0.00001497
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001496
Iteration 110/1000 | Loss: 0.00001496
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001495
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001495
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001494
Iteration 121/1000 | Loss: 0.00001494
Iteration 122/1000 | Loss: 0.00001494
Iteration 123/1000 | Loss: 0.00001494
Iteration 124/1000 | Loss: 0.00001494
Iteration 125/1000 | Loss: 0.00001493
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001492
Iteration 128/1000 | Loss: 0.00001492
Iteration 129/1000 | Loss: 0.00001492
Iteration 130/1000 | Loss: 0.00001492
Iteration 131/1000 | Loss: 0.00001492
Iteration 132/1000 | Loss: 0.00001492
Iteration 133/1000 | Loss: 0.00001492
Iteration 134/1000 | Loss: 0.00001492
Iteration 135/1000 | Loss: 0.00001492
Iteration 136/1000 | Loss: 0.00001492
Iteration 137/1000 | Loss: 0.00001492
Iteration 138/1000 | Loss: 0.00001492
Iteration 139/1000 | Loss: 0.00001492
Iteration 140/1000 | Loss: 0.00001492
Iteration 141/1000 | Loss: 0.00001492
Iteration 142/1000 | Loss: 0.00001492
Iteration 143/1000 | Loss: 0.00001492
Iteration 144/1000 | Loss: 0.00001492
Iteration 145/1000 | Loss: 0.00001492
Iteration 146/1000 | Loss: 0.00001492
Iteration 147/1000 | Loss: 0.00001492
Iteration 148/1000 | Loss: 0.00001492
Iteration 149/1000 | Loss: 0.00001492
Iteration 150/1000 | Loss: 0.00001492
Iteration 151/1000 | Loss: 0.00001492
Iteration 152/1000 | Loss: 0.00001492
Iteration 153/1000 | Loss: 0.00001492
Iteration 154/1000 | Loss: 0.00001492
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001492
Iteration 158/1000 | Loss: 0.00001492
Iteration 159/1000 | Loss: 0.00001492
Iteration 160/1000 | Loss: 0.00001492
Iteration 161/1000 | Loss: 0.00001492
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001492
Iteration 164/1000 | Loss: 0.00001492
Iteration 165/1000 | Loss: 0.00001492
Iteration 166/1000 | Loss: 0.00001492
Iteration 167/1000 | Loss: 0.00001492
Iteration 168/1000 | Loss: 0.00001492
Iteration 169/1000 | Loss: 0.00001492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4919339264451992e-05, 1.4919339264451992e-05, 1.4919339264451992e-05, 1.4919339264451992e-05, 1.4919339264451992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4919339264451992e-05

Optimization complete. Final v2v error: 3.176892042160034 mm

Highest mean error: 4.582627296447754 mm for frame 210

Lowest mean error: 2.994680166244507 mm for frame 232

Saving results

Total time: 177.84193897247314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829197
Iteration 2/25 | Loss: 0.00117108
Iteration 3/25 | Loss: 0.00081220
Iteration 4/25 | Loss: 0.00076669
Iteration 5/25 | Loss: 0.00075376
Iteration 6/25 | Loss: 0.00075074
Iteration 7/25 | Loss: 0.00075027
Iteration 8/25 | Loss: 0.00075027
Iteration 9/25 | Loss: 0.00075027
Iteration 10/25 | Loss: 0.00075027
Iteration 11/25 | Loss: 0.00075027
Iteration 12/25 | Loss: 0.00075027
Iteration 13/25 | Loss: 0.00075027
Iteration 14/25 | Loss: 0.00075027
Iteration 15/25 | Loss: 0.00075027
Iteration 16/25 | Loss: 0.00075027
Iteration 17/25 | Loss: 0.00075027
Iteration 18/25 | Loss: 0.00075027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007502686348743737, 0.0007502686348743737, 0.0007502686348743737, 0.0007502686348743737, 0.0007502686348743737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007502686348743737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28191662
Iteration 2/25 | Loss: 0.00079290
Iteration 3/25 | Loss: 0.00079289
Iteration 4/25 | Loss: 0.00079289
Iteration 5/25 | Loss: 0.00079289
Iteration 6/25 | Loss: 0.00079289
Iteration 7/25 | Loss: 0.00079289
Iteration 8/25 | Loss: 0.00079289
Iteration 9/25 | Loss: 0.00079289
Iteration 10/25 | Loss: 0.00079289
Iteration 11/25 | Loss: 0.00079289
Iteration 12/25 | Loss: 0.00079289
Iteration 13/25 | Loss: 0.00079289
Iteration 14/25 | Loss: 0.00079289
Iteration 15/25 | Loss: 0.00079289
Iteration 16/25 | Loss: 0.00079289
Iteration 17/25 | Loss: 0.00079289
Iteration 18/25 | Loss: 0.00079289
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007928903214633465, 0.0007928903214633465, 0.0007928903214633465, 0.0007928903214633465, 0.0007928903214633465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007928903214633465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079289
Iteration 2/1000 | Loss: 0.00004137
Iteration 3/1000 | Loss: 0.00002969
Iteration 4/1000 | Loss: 0.00002447
Iteration 5/1000 | Loss: 0.00002283
Iteration 6/1000 | Loss: 0.00002139
Iteration 7/1000 | Loss: 0.00002084
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00001987
Iteration 10/1000 | Loss: 0.00001961
Iteration 11/1000 | Loss: 0.00001936
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001911
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001900
Iteration 16/1000 | Loss: 0.00001884
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001873
Iteration 19/1000 | Loss: 0.00001872
Iteration 20/1000 | Loss: 0.00001871
Iteration 21/1000 | Loss: 0.00001871
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001871
Iteration 26/1000 | Loss: 0.00001870
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001870
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001870
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001869
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001862
Iteration 44/1000 | Loss: 0.00001861
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001861
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001857
Iteration 50/1000 | Loss: 0.00001857
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001855
Iteration 56/1000 | Loss: 0.00001855
Iteration 57/1000 | Loss: 0.00001855
Iteration 58/1000 | Loss: 0.00001854
Iteration 59/1000 | Loss: 0.00001854
Iteration 60/1000 | Loss: 0.00001853
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001852
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001850
Iteration 66/1000 | Loss: 0.00001850
Iteration 67/1000 | Loss: 0.00001849
Iteration 68/1000 | Loss: 0.00001849
Iteration 69/1000 | Loss: 0.00001849
Iteration 70/1000 | Loss: 0.00001847
Iteration 71/1000 | Loss: 0.00001847
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001846
Iteration 75/1000 | Loss: 0.00001846
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001845
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001844
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001836
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001836
Iteration 113/1000 | Loss: 0.00001836
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001835
Iteration 119/1000 | Loss: 0.00001835
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001834
Iteration 124/1000 | Loss: 0.00001834
Iteration 125/1000 | Loss: 0.00001834
Iteration 126/1000 | Loss: 0.00001834
Iteration 127/1000 | Loss: 0.00001834
Iteration 128/1000 | Loss: 0.00001834
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001833
Iteration 135/1000 | Loss: 0.00001833
Iteration 136/1000 | Loss: 0.00001833
Iteration 137/1000 | Loss: 0.00001833
Iteration 138/1000 | Loss: 0.00001832
Iteration 139/1000 | Loss: 0.00001832
Iteration 140/1000 | Loss: 0.00001832
Iteration 141/1000 | Loss: 0.00001832
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001831
Iteration 144/1000 | Loss: 0.00001831
Iteration 145/1000 | Loss: 0.00001831
Iteration 146/1000 | Loss: 0.00001831
Iteration 147/1000 | Loss: 0.00001831
Iteration 148/1000 | Loss: 0.00001831
Iteration 149/1000 | Loss: 0.00001831
Iteration 150/1000 | Loss: 0.00001831
Iteration 151/1000 | Loss: 0.00001831
Iteration 152/1000 | Loss: 0.00001831
Iteration 153/1000 | Loss: 0.00001830
Iteration 154/1000 | Loss: 0.00001830
Iteration 155/1000 | Loss: 0.00001830
Iteration 156/1000 | Loss: 0.00001830
Iteration 157/1000 | Loss: 0.00001830
Iteration 158/1000 | Loss: 0.00001829
Iteration 159/1000 | Loss: 0.00001829
Iteration 160/1000 | Loss: 0.00001829
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001828
Iteration 165/1000 | Loss: 0.00001828
Iteration 166/1000 | Loss: 0.00001828
Iteration 167/1000 | Loss: 0.00001828
Iteration 168/1000 | Loss: 0.00001828
Iteration 169/1000 | Loss: 0.00001828
Iteration 170/1000 | Loss: 0.00001828
Iteration 171/1000 | Loss: 0.00001828
Iteration 172/1000 | Loss: 0.00001828
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001828
Iteration 176/1000 | Loss: 0.00001828
Iteration 177/1000 | Loss: 0.00001827
Iteration 178/1000 | Loss: 0.00001827
Iteration 179/1000 | Loss: 0.00001827
Iteration 180/1000 | Loss: 0.00001827
Iteration 181/1000 | Loss: 0.00001827
Iteration 182/1000 | Loss: 0.00001827
Iteration 183/1000 | Loss: 0.00001827
Iteration 184/1000 | Loss: 0.00001827
Iteration 185/1000 | Loss: 0.00001827
Iteration 186/1000 | Loss: 0.00001826
Iteration 187/1000 | Loss: 0.00001826
Iteration 188/1000 | Loss: 0.00001826
Iteration 189/1000 | Loss: 0.00001826
Iteration 190/1000 | Loss: 0.00001826
Iteration 191/1000 | Loss: 0.00001826
Iteration 192/1000 | Loss: 0.00001825
Iteration 193/1000 | Loss: 0.00001825
Iteration 194/1000 | Loss: 0.00001825
Iteration 195/1000 | Loss: 0.00001825
Iteration 196/1000 | Loss: 0.00001825
Iteration 197/1000 | Loss: 0.00001825
Iteration 198/1000 | Loss: 0.00001825
Iteration 199/1000 | Loss: 0.00001825
Iteration 200/1000 | Loss: 0.00001824
Iteration 201/1000 | Loss: 0.00001824
Iteration 202/1000 | Loss: 0.00001824
Iteration 203/1000 | Loss: 0.00001824
Iteration 204/1000 | Loss: 0.00001824
Iteration 205/1000 | Loss: 0.00001824
Iteration 206/1000 | Loss: 0.00001824
Iteration 207/1000 | Loss: 0.00001824
Iteration 208/1000 | Loss: 0.00001824
Iteration 209/1000 | Loss: 0.00001824
Iteration 210/1000 | Loss: 0.00001823
Iteration 211/1000 | Loss: 0.00001823
Iteration 212/1000 | Loss: 0.00001823
Iteration 213/1000 | Loss: 0.00001823
Iteration 214/1000 | Loss: 0.00001823
Iteration 215/1000 | Loss: 0.00001823
Iteration 216/1000 | Loss: 0.00001823
Iteration 217/1000 | Loss: 0.00001823
Iteration 218/1000 | Loss: 0.00001823
Iteration 219/1000 | Loss: 0.00001823
Iteration 220/1000 | Loss: 0.00001823
Iteration 221/1000 | Loss: 0.00001823
Iteration 222/1000 | Loss: 0.00001823
Iteration 223/1000 | Loss: 0.00001823
Iteration 224/1000 | Loss: 0.00001823
Iteration 225/1000 | Loss: 0.00001823
Iteration 226/1000 | Loss: 0.00001823
Iteration 227/1000 | Loss: 0.00001823
Iteration 228/1000 | Loss: 0.00001823
Iteration 229/1000 | Loss: 0.00001823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.8228893168270588e-05, 1.8228893168270588e-05, 1.8228893168270588e-05, 1.8228893168270588e-05, 1.8228893168270588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8228893168270588e-05

Optimization complete. Final v2v error: 3.553008556365967 mm

Highest mean error: 4.910492420196533 mm for frame 82

Lowest mean error: 2.7505085468292236 mm for frame 197

Saving results

Total time: 54.04062747955322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847959
Iteration 2/25 | Loss: 0.00137483
Iteration 3/25 | Loss: 0.00094915
Iteration 4/25 | Loss: 0.00084061
Iteration 5/25 | Loss: 0.00082712
Iteration 6/25 | Loss: 0.00081035
Iteration 7/25 | Loss: 0.00079471
Iteration 8/25 | Loss: 0.00078471
Iteration 9/25 | Loss: 0.00078219
Iteration 10/25 | Loss: 0.00078150
Iteration 11/25 | Loss: 0.00078106
Iteration 12/25 | Loss: 0.00078655
Iteration 13/25 | Loss: 0.00078644
Iteration 14/25 | Loss: 0.00078264
Iteration 15/25 | Loss: 0.00078499
Iteration 16/25 | Loss: 0.00078478
Iteration 17/25 | Loss: 0.00078444
Iteration 18/25 | Loss: 0.00078339
Iteration 19/25 | Loss: 0.00078340
Iteration 20/25 | Loss: 0.00078444
Iteration 21/25 | Loss: 0.00078454
Iteration 22/25 | Loss: 0.00078417
Iteration 23/25 | Loss: 0.00078449
Iteration 24/25 | Loss: 0.00078200
Iteration 25/25 | Loss: 0.00078236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61561716
Iteration 2/25 | Loss: 0.00095240
Iteration 3/25 | Loss: 0.00095239
Iteration 4/25 | Loss: 0.00095239
Iteration 5/25 | Loss: 0.00095239
Iteration 6/25 | Loss: 0.00095239
Iteration 7/25 | Loss: 0.00095239
Iteration 8/25 | Loss: 0.00095239
Iteration 9/25 | Loss: 0.00095239
Iteration 10/25 | Loss: 0.00095239
Iteration 11/25 | Loss: 0.00095239
Iteration 12/25 | Loss: 0.00095239
Iteration 13/25 | Loss: 0.00095239
Iteration 14/25 | Loss: 0.00095239
Iteration 15/25 | Loss: 0.00095239
Iteration 16/25 | Loss: 0.00095239
Iteration 17/25 | Loss: 0.00095239
Iteration 18/25 | Loss: 0.00095239
Iteration 19/25 | Loss: 0.00095239
Iteration 20/25 | Loss: 0.00095239
Iteration 21/25 | Loss: 0.00095239
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009523859480395913, 0.0009523859480395913, 0.0009523859480395913, 0.0009523859480395913, 0.0009523859480395913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009523859480395913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095239
Iteration 2/1000 | Loss: 0.00005455
Iteration 3/1000 | Loss: 0.00004437
Iteration 4/1000 | Loss: 0.00006456
Iteration 5/1000 | Loss: 0.00005057
Iteration 6/1000 | Loss: 0.00005480
Iteration 7/1000 | Loss: 0.00006634
Iteration 8/1000 | Loss: 0.00006209
Iteration 9/1000 | Loss: 0.00004865
Iteration 10/1000 | Loss: 0.00003947
Iteration 11/1000 | Loss: 0.00003713
Iteration 12/1000 | Loss: 0.00008376
Iteration 13/1000 | Loss: 0.00006095
Iteration 14/1000 | Loss: 0.00004730
Iteration 15/1000 | Loss: 0.00006599
Iteration 16/1000 | Loss: 0.00005669
Iteration 17/1000 | Loss: 0.00008707
Iteration 18/1000 | Loss: 0.00007319
Iteration 19/1000 | Loss: 0.00007318
Iteration 20/1000 | Loss: 0.00006374
Iteration 21/1000 | Loss: 0.00006979
Iteration 22/1000 | Loss: 0.00006403
Iteration 23/1000 | Loss: 0.00006124
Iteration 24/1000 | Loss: 0.00006385
Iteration 25/1000 | Loss: 0.00006225
Iteration 26/1000 | Loss: 0.00006600
Iteration 27/1000 | Loss: 0.00006120
Iteration 28/1000 | Loss: 0.00006128
Iteration 29/1000 | Loss: 0.00006518
Iteration 30/1000 | Loss: 0.00006475
Iteration 31/1000 | Loss: 0.00006576
Iteration 32/1000 | Loss: 0.00006349
Iteration 33/1000 | Loss: 0.00005851
Iteration 34/1000 | Loss: 0.00005041
Iteration 35/1000 | Loss: 0.00005925
Iteration 36/1000 | Loss: 0.00006566
Iteration 37/1000 | Loss: 0.00006187
Iteration 38/1000 | Loss: 0.00006146
Iteration 39/1000 | Loss: 0.00006060
Iteration 40/1000 | Loss: 0.00006534
Iteration 41/1000 | Loss: 0.00006856
Iteration 42/1000 | Loss: 0.00006399
Iteration 43/1000 | Loss: 0.00005733
Iteration 44/1000 | Loss: 0.00006225
Iteration 45/1000 | Loss: 0.00005698
Iteration 46/1000 | Loss: 0.00006407
Iteration 47/1000 | Loss: 0.00006562
Iteration 48/1000 | Loss: 0.00006154
Iteration 49/1000 | Loss: 0.00006135
Iteration 50/1000 | Loss: 0.00006705
Iteration 51/1000 | Loss: 0.00006433
Iteration 52/1000 | Loss: 0.00006154
Iteration 53/1000 | Loss: 0.00006098
Iteration 54/1000 | Loss: 0.00004951
Iteration 55/1000 | Loss: 0.00004006
Iteration 56/1000 | Loss: 0.00005660
Iteration 57/1000 | Loss: 0.00005908
Iteration 58/1000 | Loss: 0.00006689
Iteration 59/1000 | Loss: 0.00005824
Iteration 60/1000 | Loss: 0.00006102
Iteration 61/1000 | Loss: 0.00006269
Iteration 62/1000 | Loss: 0.00006060
Iteration 63/1000 | Loss: 0.00006255
Iteration 64/1000 | Loss: 0.00006015
Iteration 65/1000 | Loss: 0.00006193
Iteration 66/1000 | Loss: 0.00005968
Iteration 67/1000 | Loss: 0.00003225
Iteration 68/1000 | Loss: 0.00006044
Iteration 69/1000 | Loss: 0.00005903
Iteration 70/1000 | Loss: 0.00006610
Iteration 71/1000 | Loss: 0.00006743
Iteration 72/1000 | Loss: 0.00006007
Iteration 73/1000 | Loss: 0.00006247
Iteration 74/1000 | Loss: 0.00005929
Iteration 75/1000 | Loss: 0.00006395
Iteration 76/1000 | Loss: 0.00006422
Iteration 77/1000 | Loss: 0.00006223
Iteration 78/1000 | Loss: 0.00006030
Iteration 79/1000 | Loss: 0.00005827
Iteration 80/1000 | Loss: 0.00005885
Iteration 81/1000 | Loss: 0.00006004
Iteration 82/1000 | Loss: 0.00005933
Iteration 83/1000 | Loss: 0.00006183
Iteration 84/1000 | Loss: 0.00005841
Iteration 85/1000 | Loss: 0.00006245
Iteration 86/1000 | Loss: 0.00005793
Iteration 87/1000 | Loss: 0.00003900
Iteration 88/1000 | Loss: 0.00005213
Iteration 89/1000 | Loss: 0.00006514
Iteration 90/1000 | Loss: 0.00007945
Iteration 91/1000 | Loss: 0.00005850
Iteration 92/1000 | Loss: 0.00006766
Iteration 93/1000 | Loss: 0.00007186
Iteration 94/1000 | Loss: 0.00006323
Iteration 95/1000 | Loss: 0.00007261
Iteration 96/1000 | Loss: 0.00006080
Iteration 97/1000 | Loss: 0.00007495
Iteration 98/1000 | Loss: 0.00006075
Iteration 99/1000 | Loss: 0.00007627
Iteration 100/1000 | Loss: 0.00006930
Iteration 101/1000 | Loss: 0.00007252
Iteration 102/1000 | Loss: 0.00006341
Iteration 103/1000 | Loss: 0.00007129
Iteration 104/1000 | Loss: 0.00005893
Iteration 105/1000 | Loss: 0.00006783
Iteration 106/1000 | Loss: 0.00003795
Iteration 107/1000 | Loss: 0.00003724
Iteration 108/1000 | Loss: 0.00002969
Iteration 109/1000 | Loss: 0.00004243
Iteration 110/1000 | Loss: 0.00002732
Iteration 111/1000 | Loss: 0.00003208
Iteration 112/1000 | Loss: 0.00002713
Iteration 113/1000 | Loss: 0.00002367
Iteration 114/1000 | Loss: 0.00002254
Iteration 115/1000 | Loss: 0.00004801
Iteration 116/1000 | Loss: 0.00003499
Iteration 117/1000 | Loss: 0.00003451
Iteration 118/1000 | Loss: 0.00003277
Iteration 119/1000 | Loss: 0.00003742
Iteration 120/1000 | Loss: 0.00003125
Iteration 121/1000 | Loss: 0.00005456
Iteration 122/1000 | Loss: 0.00002525
Iteration 123/1000 | Loss: 0.00002237
Iteration 124/1000 | Loss: 0.00002071
Iteration 125/1000 | Loss: 0.00001953
Iteration 126/1000 | Loss: 0.00001891
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001818
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001797
Iteration 135/1000 | Loss: 0.00001797
Iteration 136/1000 | Loss: 0.00001796
Iteration 137/1000 | Loss: 0.00001795
Iteration 138/1000 | Loss: 0.00001794
Iteration 139/1000 | Loss: 0.00001794
Iteration 140/1000 | Loss: 0.00001794
Iteration 141/1000 | Loss: 0.00001793
Iteration 142/1000 | Loss: 0.00001793
Iteration 143/1000 | Loss: 0.00001792
Iteration 144/1000 | Loss: 0.00001792
Iteration 145/1000 | Loss: 0.00001791
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001790
Iteration 149/1000 | Loss: 0.00001789
Iteration 150/1000 | Loss: 0.00001786
Iteration 151/1000 | Loss: 0.00001786
Iteration 152/1000 | Loss: 0.00001785
Iteration 153/1000 | Loss: 0.00001784
Iteration 154/1000 | Loss: 0.00001784
Iteration 155/1000 | Loss: 0.00001784
Iteration 156/1000 | Loss: 0.00001783
Iteration 157/1000 | Loss: 0.00001783
Iteration 158/1000 | Loss: 0.00001779
Iteration 159/1000 | Loss: 0.00001779
Iteration 160/1000 | Loss: 0.00001778
Iteration 161/1000 | Loss: 0.00001770
Iteration 162/1000 | Loss: 0.00001770
Iteration 163/1000 | Loss: 0.00001770
Iteration 164/1000 | Loss: 0.00001769
Iteration 165/1000 | Loss: 0.00001768
Iteration 166/1000 | Loss: 0.00001767
Iteration 167/1000 | Loss: 0.00001767
Iteration 168/1000 | Loss: 0.00001766
Iteration 169/1000 | Loss: 0.00001766
Iteration 170/1000 | Loss: 0.00001765
Iteration 171/1000 | Loss: 0.00001765
Iteration 172/1000 | Loss: 0.00001764
Iteration 173/1000 | Loss: 0.00001764
Iteration 174/1000 | Loss: 0.00001762
Iteration 175/1000 | Loss: 0.00001762
Iteration 176/1000 | Loss: 0.00001761
Iteration 177/1000 | Loss: 0.00001761
Iteration 178/1000 | Loss: 0.00001760
Iteration 179/1000 | Loss: 0.00001760
Iteration 180/1000 | Loss: 0.00001759
Iteration 181/1000 | Loss: 0.00001759
Iteration 182/1000 | Loss: 0.00001759
Iteration 183/1000 | Loss: 0.00001758
Iteration 184/1000 | Loss: 0.00001758
Iteration 185/1000 | Loss: 0.00001758
Iteration 186/1000 | Loss: 0.00001757
Iteration 187/1000 | Loss: 0.00001757
Iteration 188/1000 | Loss: 0.00001757
Iteration 189/1000 | Loss: 0.00001756
Iteration 190/1000 | Loss: 0.00001756
Iteration 191/1000 | Loss: 0.00001756
Iteration 192/1000 | Loss: 0.00001755
Iteration 193/1000 | Loss: 0.00001755
Iteration 194/1000 | Loss: 0.00001755
Iteration 195/1000 | Loss: 0.00001754
Iteration 196/1000 | Loss: 0.00001754
Iteration 197/1000 | Loss: 0.00001754
Iteration 198/1000 | Loss: 0.00001754
Iteration 199/1000 | Loss: 0.00001753
Iteration 200/1000 | Loss: 0.00001753
Iteration 201/1000 | Loss: 0.00001753
Iteration 202/1000 | Loss: 0.00001753
Iteration 203/1000 | Loss: 0.00001753
Iteration 204/1000 | Loss: 0.00001752
Iteration 205/1000 | Loss: 0.00001752
Iteration 206/1000 | Loss: 0.00001752
Iteration 207/1000 | Loss: 0.00001751
Iteration 208/1000 | Loss: 0.00001751
Iteration 209/1000 | Loss: 0.00001751
Iteration 210/1000 | Loss: 0.00001751
Iteration 211/1000 | Loss: 0.00001750
Iteration 212/1000 | Loss: 0.00001750
Iteration 213/1000 | Loss: 0.00001750
Iteration 214/1000 | Loss: 0.00001750
Iteration 215/1000 | Loss: 0.00001750
Iteration 216/1000 | Loss: 0.00001750
Iteration 217/1000 | Loss: 0.00001750
Iteration 218/1000 | Loss: 0.00001750
Iteration 219/1000 | Loss: 0.00001750
Iteration 220/1000 | Loss: 0.00001750
Iteration 221/1000 | Loss: 0.00001750
Iteration 222/1000 | Loss: 0.00001750
Iteration 223/1000 | Loss: 0.00001750
Iteration 224/1000 | Loss: 0.00001749
Iteration 225/1000 | Loss: 0.00001749
Iteration 226/1000 | Loss: 0.00001749
Iteration 227/1000 | Loss: 0.00001749
Iteration 228/1000 | Loss: 0.00001749
Iteration 229/1000 | Loss: 0.00001749
Iteration 230/1000 | Loss: 0.00001749
Iteration 231/1000 | Loss: 0.00001749
Iteration 232/1000 | Loss: 0.00001749
Iteration 233/1000 | Loss: 0.00001748
Iteration 234/1000 | Loss: 0.00001748
Iteration 235/1000 | Loss: 0.00001748
Iteration 236/1000 | Loss: 0.00001748
Iteration 237/1000 | Loss: 0.00001748
Iteration 238/1000 | Loss: 0.00001748
Iteration 239/1000 | Loss: 0.00001748
Iteration 240/1000 | Loss: 0.00001748
Iteration 241/1000 | Loss: 0.00001748
Iteration 242/1000 | Loss: 0.00001748
Iteration 243/1000 | Loss: 0.00001748
Iteration 244/1000 | Loss: 0.00001748
Iteration 245/1000 | Loss: 0.00001748
Iteration 246/1000 | Loss: 0.00001748
Iteration 247/1000 | Loss: 0.00001748
Iteration 248/1000 | Loss: 0.00001748
Iteration 249/1000 | Loss: 0.00001748
Iteration 250/1000 | Loss: 0.00001748
Iteration 251/1000 | Loss: 0.00001748
Iteration 252/1000 | Loss: 0.00001748
Iteration 253/1000 | Loss: 0.00001748
Iteration 254/1000 | Loss: 0.00001748
Iteration 255/1000 | Loss: 0.00001748
Iteration 256/1000 | Loss: 0.00001748
Iteration 257/1000 | Loss: 0.00001748
Iteration 258/1000 | Loss: 0.00001748
Iteration 259/1000 | Loss: 0.00001748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.7476382708991878e-05, 1.7476382708991878e-05, 1.7476382708991878e-05, 1.7476382708991878e-05, 1.7476382708991878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7476382708991878e-05

Optimization complete. Final v2v error: 3.4659523963928223 mm

Highest mean error: 4.59902286529541 mm for frame 13

Lowest mean error: 2.791590452194214 mm for frame 82

Saving results

Total time: 273.78339076042175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465930
Iteration 2/25 | Loss: 0.00115382
Iteration 3/25 | Loss: 0.00074915
Iteration 4/25 | Loss: 0.00072332
Iteration 5/25 | Loss: 0.00071737
Iteration 6/25 | Loss: 0.00071554
Iteration 7/25 | Loss: 0.00071510
Iteration 8/25 | Loss: 0.00071510
Iteration 9/25 | Loss: 0.00071510
Iteration 10/25 | Loss: 0.00071510
Iteration 11/25 | Loss: 0.00071510
Iteration 12/25 | Loss: 0.00071510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000715104048140347, 0.000715104048140347, 0.000715104048140347, 0.000715104048140347, 0.000715104048140347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000715104048140347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64211917
Iteration 2/25 | Loss: 0.00070195
Iteration 3/25 | Loss: 0.00070195
Iteration 4/25 | Loss: 0.00070195
Iteration 5/25 | Loss: 0.00070195
Iteration 6/25 | Loss: 0.00070195
Iteration 7/25 | Loss: 0.00070195
Iteration 8/25 | Loss: 0.00070194
Iteration 9/25 | Loss: 0.00070194
Iteration 10/25 | Loss: 0.00070194
Iteration 11/25 | Loss: 0.00070194
Iteration 12/25 | Loss: 0.00070194
Iteration 13/25 | Loss: 0.00070194
Iteration 14/25 | Loss: 0.00070194
Iteration 15/25 | Loss: 0.00070194
Iteration 16/25 | Loss: 0.00070194
Iteration 17/25 | Loss: 0.00070194
Iteration 18/25 | Loss: 0.00070194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007019439362920821, 0.0007019439362920821, 0.0007019439362920821, 0.0007019439362920821, 0.0007019439362920821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007019439362920821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070194
Iteration 2/1000 | Loss: 0.00002601
Iteration 3/1000 | Loss: 0.00001763
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001521
Iteration 7/1000 | Loss: 0.00001476
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001391
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001387
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001386
Iteration 26/1000 | Loss: 0.00001386
Iteration 27/1000 | Loss: 0.00001386
Iteration 28/1000 | Loss: 0.00001386
Iteration 29/1000 | Loss: 0.00001386
Iteration 30/1000 | Loss: 0.00001386
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001382
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001379
Iteration 35/1000 | Loss: 0.00001379
Iteration 36/1000 | Loss: 0.00001378
Iteration 37/1000 | Loss: 0.00001378
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001378
Iteration 40/1000 | Loss: 0.00001377
Iteration 41/1000 | Loss: 0.00001376
Iteration 42/1000 | Loss: 0.00001376
Iteration 43/1000 | Loss: 0.00001376
Iteration 44/1000 | Loss: 0.00001376
Iteration 45/1000 | Loss: 0.00001376
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001375
Iteration 48/1000 | Loss: 0.00001375
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00001375
Iteration 51/1000 | Loss: 0.00001375
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001373
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001372
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001370
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001364
Iteration 83/1000 | Loss: 0.00001364
Iteration 84/1000 | Loss: 0.00001364
Iteration 85/1000 | Loss: 0.00001363
Iteration 86/1000 | Loss: 0.00001363
Iteration 87/1000 | Loss: 0.00001363
Iteration 88/1000 | Loss: 0.00001363
Iteration 89/1000 | Loss: 0.00001363
Iteration 90/1000 | Loss: 0.00001363
Iteration 91/1000 | Loss: 0.00001363
Iteration 92/1000 | Loss: 0.00001362
Iteration 93/1000 | Loss: 0.00001362
Iteration 94/1000 | Loss: 0.00001362
Iteration 95/1000 | Loss: 0.00001362
Iteration 96/1000 | Loss: 0.00001362
Iteration 97/1000 | Loss: 0.00001362
Iteration 98/1000 | Loss: 0.00001362
Iteration 99/1000 | Loss: 0.00001361
Iteration 100/1000 | Loss: 0.00001361
Iteration 101/1000 | Loss: 0.00001361
Iteration 102/1000 | Loss: 0.00001361
Iteration 103/1000 | Loss: 0.00001361
Iteration 104/1000 | Loss: 0.00001360
Iteration 105/1000 | Loss: 0.00001360
Iteration 106/1000 | Loss: 0.00001360
Iteration 107/1000 | Loss: 0.00001360
Iteration 108/1000 | Loss: 0.00001360
Iteration 109/1000 | Loss: 0.00001360
Iteration 110/1000 | Loss: 0.00001360
Iteration 111/1000 | Loss: 0.00001360
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001358
Iteration 120/1000 | Loss: 0.00001358
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001358
Iteration 126/1000 | Loss: 0.00001358
Iteration 127/1000 | Loss: 0.00001357
Iteration 128/1000 | Loss: 0.00001357
Iteration 129/1000 | Loss: 0.00001357
Iteration 130/1000 | Loss: 0.00001357
Iteration 131/1000 | Loss: 0.00001357
Iteration 132/1000 | Loss: 0.00001357
Iteration 133/1000 | Loss: 0.00001357
Iteration 134/1000 | Loss: 0.00001357
Iteration 135/1000 | Loss: 0.00001357
Iteration 136/1000 | Loss: 0.00001357
Iteration 137/1000 | Loss: 0.00001357
Iteration 138/1000 | Loss: 0.00001357
Iteration 139/1000 | Loss: 0.00001357
Iteration 140/1000 | Loss: 0.00001357
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001357
Iteration 143/1000 | Loss: 0.00001357
Iteration 144/1000 | Loss: 0.00001357
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.3570950613939203e-05, 1.3570950613939203e-05, 1.3570950613939203e-05, 1.3570950613939203e-05, 1.3570950613939203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3570950613939203e-05

Optimization complete. Final v2v error: 3.0512278079986572 mm

Highest mean error: 3.6984915733337402 mm for frame 22

Lowest mean error: 2.491128921508789 mm for frame 107

Saving results

Total time: 37.27946877479553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073578
Iteration 2/25 | Loss: 0.00093008
Iteration 3/25 | Loss: 0.00076849
Iteration 4/25 | Loss: 0.00074813
Iteration 5/25 | Loss: 0.00074115
Iteration 6/25 | Loss: 0.00073962
Iteration 7/25 | Loss: 0.00073924
Iteration 8/25 | Loss: 0.00073924
Iteration 9/25 | Loss: 0.00073924
Iteration 10/25 | Loss: 0.00073924
Iteration 11/25 | Loss: 0.00073924
Iteration 12/25 | Loss: 0.00073924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007392377010546625, 0.0007392377010546625, 0.0007392377010546625, 0.0007392377010546625, 0.0007392377010546625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007392377010546625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.34429312
Iteration 2/25 | Loss: 0.00084974
Iteration 3/25 | Loss: 0.00084974
Iteration 4/25 | Loss: 0.00084974
Iteration 5/25 | Loss: 0.00084974
Iteration 6/25 | Loss: 0.00084974
Iteration 7/25 | Loss: 0.00084974
Iteration 8/25 | Loss: 0.00084974
Iteration 9/25 | Loss: 0.00084974
Iteration 10/25 | Loss: 0.00084974
Iteration 11/25 | Loss: 0.00084974
Iteration 12/25 | Loss: 0.00084974
Iteration 13/25 | Loss: 0.00084974
Iteration 14/25 | Loss: 0.00084974
Iteration 15/25 | Loss: 0.00084974
Iteration 16/25 | Loss: 0.00084974
Iteration 17/25 | Loss: 0.00084974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008497399976477027, 0.0008497399976477027, 0.0008497399976477027, 0.0008497399976477027, 0.0008497399976477027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008497399976477027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084974
Iteration 2/1000 | Loss: 0.00002662
Iteration 3/1000 | Loss: 0.00001611
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001413
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001324
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001295
Iteration 18/1000 | Loss: 0.00001291
Iteration 19/1000 | Loss: 0.00001290
Iteration 20/1000 | Loss: 0.00001284
Iteration 21/1000 | Loss: 0.00001284
Iteration 22/1000 | Loss: 0.00001283
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001282
Iteration 26/1000 | Loss: 0.00001281
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001280
Iteration 29/1000 | Loss: 0.00001280
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001276
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001275
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001274
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001273
Iteration 47/1000 | Loss: 0.00001273
Iteration 48/1000 | Loss: 0.00001273
Iteration 49/1000 | Loss: 0.00001273
Iteration 50/1000 | Loss: 0.00001273
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001273
Iteration 54/1000 | Loss: 0.00001273
Iteration 55/1000 | Loss: 0.00001273
Iteration 56/1000 | Loss: 0.00001272
Iteration 57/1000 | Loss: 0.00001272
Iteration 58/1000 | Loss: 0.00001272
Iteration 59/1000 | Loss: 0.00001271
Iteration 60/1000 | Loss: 0.00001271
Iteration 61/1000 | Loss: 0.00001271
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001271
Iteration 64/1000 | Loss: 0.00001271
Iteration 65/1000 | Loss: 0.00001271
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001269
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001269
Iteration 77/1000 | Loss: 0.00001269
Iteration 78/1000 | Loss: 0.00001269
Iteration 79/1000 | Loss: 0.00001269
Iteration 80/1000 | Loss: 0.00001269
Iteration 81/1000 | Loss: 0.00001269
Iteration 82/1000 | Loss: 0.00001269
Iteration 83/1000 | Loss: 0.00001268
Iteration 84/1000 | Loss: 0.00001268
Iteration 85/1000 | Loss: 0.00001268
Iteration 86/1000 | Loss: 0.00001268
Iteration 87/1000 | Loss: 0.00001268
Iteration 88/1000 | Loss: 0.00001268
Iteration 89/1000 | Loss: 0.00001267
Iteration 90/1000 | Loss: 0.00001267
Iteration 91/1000 | Loss: 0.00001267
Iteration 92/1000 | Loss: 0.00001267
Iteration 93/1000 | Loss: 0.00001267
Iteration 94/1000 | Loss: 0.00001267
Iteration 95/1000 | Loss: 0.00001267
Iteration 96/1000 | Loss: 0.00001266
Iteration 97/1000 | Loss: 0.00001266
Iteration 98/1000 | Loss: 0.00001266
Iteration 99/1000 | Loss: 0.00001266
Iteration 100/1000 | Loss: 0.00001265
Iteration 101/1000 | Loss: 0.00001265
Iteration 102/1000 | Loss: 0.00001265
Iteration 103/1000 | Loss: 0.00001265
Iteration 104/1000 | Loss: 0.00001264
Iteration 105/1000 | Loss: 0.00001264
Iteration 106/1000 | Loss: 0.00001264
Iteration 107/1000 | Loss: 0.00001264
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001263
Iteration 112/1000 | Loss: 0.00001263
Iteration 113/1000 | Loss: 0.00001263
Iteration 114/1000 | Loss: 0.00001263
Iteration 115/1000 | Loss: 0.00001263
Iteration 116/1000 | Loss: 0.00001263
Iteration 117/1000 | Loss: 0.00001263
Iteration 118/1000 | Loss: 0.00001263
Iteration 119/1000 | Loss: 0.00001263
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001263
Iteration 127/1000 | Loss: 0.00001263
Iteration 128/1000 | Loss: 0.00001263
Iteration 129/1000 | Loss: 0.00001263
Iteration 130/1000 | Loss: 0.00001263
Iteration 131/1000 | Loss: 0.00001263
Iteration 132/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2625740964722354e-05, 1.2625740964722354e-05, 1.2625740964722354e-05, 1.2625740964722354e-05, 1.2625740964722354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2625740964722354e-05

Optimization complete. Final v2v error: 3.023331880569458 mm

Highest mean error: 3.2208898067474365 mm for frame 100

Lowest mean error: 2.824375867843628 mm for frame 81

Saving results

Total time: 33.08560848236084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910109
Iteration 2/25 | Loss: 0.00143563
Iteration 3/25 | Loss: 0.00104285
Iteration 4/25 | Loss: 0.00098370
Iteration 5/25 | Loss: 0.00097110
Iteration 6/25 | Loss: 0.00096802
Iteration 7/25 | Loss: 0.00097658
Iteration 8/25 | Loss: 0.00096010
Iteration 9/25 | Loss: 0.00097127
Iteration 10/25 | Loss: 0.00095130
Iteration 11/25 | Loss: 0.00093214
Iteration 12/25 | Loss: 0.00092402
Iteration 13/25 | Loss: 0.00091660
Iteration 14/25 | Loss: 0.00091427
Iteration 15/25 | Loss: 0.00091729
Iteration 16/25 | Loss: 0.00091757
Iteration 17/25 | Loss: 0.00091109
Iteration 18/25 | Loss: 0.00090856
Iteration 19/25 | Loss: 0.00090760
Iteration 20/25 | Loss: 0.00090729
Iteration 21/25 | Loss: 0.00090724
Iteration 22/25 | Loss: 0.00090724
Iteration 23/25 | Loss: 0.00090724
Iteration 24/25 | Loss: 0.00090724
Iteration 25/25 | Loss: 0.00090724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06056130
Iteration 2/25 | Loss: 0.00077056
Iteration 3/25 | Loss: 0.00077055
Iteration 4/25 | Loss: 0.00077055
Iteration 5/25 | Loss: 0.00077055
Iteration 6/25 | Loss: 0.00077055
Iteration 7/25 | Loss: 0.00077055
Iteration 8/25 | Loss: 0.00077055
Iteration 9/25 | Loss: 0.00077055
Iteration 10/25 | Loss: 0.00077055
Iteration 11/25 | Loss: 0.00077055
Iteration 12/25 | Loss: 0.00077055
Iteration 13/25 | Loss: 0.00077055
Iteration 14/25 | Loss: 0.00077055
Iteration 15/25 | Loss: 0.00077055
Iteration 16/25 | Loss: 0.00077055
Iteration 17/25 | Loss: 0.00077055
Iteration 18/25 | Loss: 0.00077055
Iteration 19/25 | Loss: 0.00077055
Iteration 20/25 | Loss: 0.00077055
Iteration 21/25 | Loss: 0.00077055
Iteration 22/25 | Loss: 0.00077055
Iteration 23/25 | Loss: 0.00077055
Iteration 24/25 | Loss: 0.00077055
Iteration 25/25 | Loss: 0.00077055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077055
Iteration 2/1000 | Loss: 0.00005023
Iteration 3/1000 | Loss: 0.00003968
Iteration 4/1000 | Loss: 0.00003718
Iteration 5/1000 | Loss: 0.00003501
Iteration 6/1000 | Loss: 0.00003390
Iteration 7/1000 | Loss: 0.00003276
Iteration 8/1000 | Loss: 0.00003221
Iteration 9/1000 | Loss: 0.00003190
Iteration 10/1000 | Loss: 0.00003166
Iteration 11/1000 | Loss: 0.00003155
Iteration 12/1000 | Loss: 0.00003155
Iteration 13/1000 | Loss: 0.00003154
Iteration 14/1000 | Loss: 0.00003154
Iteration 15/1000 | Loss: 0.00003154
Iteration 16/1000 | Loss: 0.00003154
Iteration 17/1000 | Loss: 0.00003154
Iteration 18/1000 | Loss: 0.00003154
Iteration 19/1000 | Loss: 0.00003143
Iteration 20/1000 | Loss: 0.00003142
Iteration 21/1000 | Loss: 0.00003141
Iteration 22/1000 | Loss: 0.00003141
Iteration 23/1000 | Loss: 0.00003139
Iteration 24/1000 | Loss: 0.00003139
Iteration 25/1000 | Loss: 0.00003139
Iteration 26/1000 | Loss: 0.00003139
Iteration 27/1000 | Loss: 0.00003131
Iteration 28/1000 | Loss: 0.00003128
Iteration 29/1000 | Loss: 0.00003128
Iteration 30/1000 | Loss: 0.00003128
Iteration 31/1000 | Loss: 0.00003128
Iteration 32/1000 | Loss: 0.00003128
Iteration 33/1000 | Loss: 0.00003128
Iteration 34/1000 | Loss: 0.00003128
Iteration 35/1000 | Loss: 0.00003128
Iteration 36/1000 | Loss: 0.00003128
Iteration 37/1000 | Loss: 0.00003128
Iteration 38/1000 | Loss: 0.00003128
Iteration 39/1000 | Loss: 0.00003128
Iteration 40/1000 | Loss: 0.00003128
Iteration 41/1000 | Loss: 0.00003128
Iteration 42/1000 | Loss: 0.00003128
Iteration 43/1000 | Loss: 0.00003128
Iteration 44/1000 | Loss: 0.00003128
Iteration 45/1000 | Loss: 0.00003128
Iteration 46/1000 | Loss: 0.00003128
Iteration 47/1000 | Loss: 0.00003128
Iteration 48/1000 | Loss: 0.00003128
Iteration 49/1000 | Loss: 0.00003128
Iteration 50/1000 | Loss: 0.00003128
Iteration 51/1000 | Loss: 0.00003128
Iteration 52/1000 | Loss: 0.00003128
Iteration 53/1000 | Loss: 0.00003128
Iteration 54/1000 | Loss: 0.00003128
Iteration 55/1000 | Loss: 0.00003128
Iteration 56/1000 | Loss: 0.00003128
Iteration 57/1000 | Loss: 0.00003128
Iteration 58/1000 | Loss: 0.00003128
Iteration 59/1000 | Loss: 0.00003128
Iteration 60/1000 | Loss: 0.00003128
Iteration 61/1000 | Loss: 0.00003128
Iteration 62/1000 | Loss: 0.00003128
Iteration 63/1000 | Loss: 0.00003128
Iteration 64/1000 | Loss: 0.00003128
Iteration 65/1000 | Loss: 0.00003128
Iteration 66/1000 | Loss: 0.00003128
Iteration 67/1000 | Loss: 0.00003128
Iteration 68/1000 | Loss: 0.00003128
Iteration 69/1000 | Loss: 0.00003128
Iteration 70/1000 | Loss: 0.00003128
Iteration 71/1000 | Loss: 0.00003128
Iteration 72/1000 | Loss: 0.00003128
Iteration 73/1000 | Loss: 0.00003128
Iteration 74/1000 | Loss: 0.00003128
Iteration 75/1000 | Loss: 0.00003128
Iteration 76/1000 | Loss: 0.00003128
Iteration 77/1000 | Loss: 0.00003128
Iteration 78/1000 | Loss: 0.00003128
Iteration 79/1000 | Loss: 0.00003128
Iteration 80/1000 | Loss: 0.00003128
Iteration 81/1000 | Loss: 0.00003128
Iteration 82/1000 | Loss: 0.00003128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [3.127648597001098e-05, 3.127648597001098e-05, 3.127648597001098e-05, 3.127648597001098e-05, 3.127648597001098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.127648597001098e-05

Optimization complete. Final v2v error: 4.6769304275512695 mm

Highest mean error: 4.918229103088379 mm for frame 122

Lowest mean error: 4.539340972900391 mm for frame 39

Saving results

Total time: 56.87241721153259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069324
Iteration 2/25 | Loss: 0.00264580
Iteration 3/25 | Loss: 0.00158566
Iteration 4/25 | Loss: 0.00101002
Iteration 5/25 | Loss: 0.00088336
Iteration 6/25 | Loss: 0.00089179
Iteration 7/25 | Loss: 0.00080465
Iteration 8/25 | Loss: 0.00079263
Iteration 9/25 | Loss: 0.00077834
Iteration 10/25 | Loss: 0.00076906
Iteration 11/25 | Loss: 0.00076895
Iteration 12/25 | Loss: 0.00077519
Iteration 13/25 | Loss: 0.00077329
Iteration 14/25 | Loss: 0.00076929
Iteration 15/25 | Loss: 0.00076777
Iteration 16/25 | Loss: 0.00076720
Iteration 17/25 | Loss: 0.00076697
Iteration 18/25 | Loss: 0.00076693
Iteration 19/25 | Loss: 0.00076693
Iteration 20/25 | Loss: 0.00076693
Iteration 21/25 | Loss: 0.00076692
Iteration 22/25 | Loss: 0.00076692
Iteration 23/25 | Loss: 0.00076692
Iteration 24/25 | Loss: 0.00076692
Iteration 25/25 | Loss: 0.00076692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56629288
Iteration 2/25 | Loss: 0.00090798
Iteration 3/25 | Loss: 0.00090798
Iteration 4/25 | Loss: 0.00090798
Iteration 5/25 | Loss: 0.00090798
Iteration 6/25 | Loss: 0.00090798
Iteration 7/25 | Loss: 0.00090798
Iteration 8/25 | Loss: 0.00090798
Iteration 9/25 | Loss: 0.00090798
Iteration 10/25 | Loss: 0.00090798
Iteration 11/25 | Loss: 0.00090798
Iteration 12/25 | Loss: 0.00090798
Iteration 13/25 | Loss: 0.00090798
Iteration 14/25 | Loss: 0.00090798
Iteration 15/25 | Loss: 0.00090798
Iteration 16/25 | Loss: 0.00090798
Iteration 17/25 | Loss: 0.00090798
Iteration 18/25 | Loss: 0.00090798
Iteration 19/25 | Loss: 0.00090798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009079761221073568, 0.0009079761221073568, 0.0009079761221073568, 0.0009079761221073568, 0.0009079761221073568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009079761221073568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090798
Iteration 2/1000 | Loss: 0.00002475
Iteration 3/1000 | Loss: 0.00001872
Iteration 4/1000 | Loss: 0.00001718
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001568
Iteration 7/1000 | Loss: 0.00001537
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001386
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001382
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001380
Iteration 22/1000 | Loss: 0.00001379
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001378
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001377
Iteration 30/1000 | Loss: 0.00001377
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001377
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001377
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001377
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001375
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001375
Iteration 43/1000 | Loss: 0.00001375
Iteration 44/1000 | Loss: 0.00001375
Iteration 45/1000 | Loss: 0.00001375
Iteration 46/1000 | Loss: 0.00001375
Iteration 47/1000 | Loss: 0.00001374
Iteration 48/1000 | Loss: 0.00001374
Iteration 49/1000 | Loss: 0.00001374
Iteration 50/1000 | Loss: 0.00001374
Iteration 51/1000 | Loss: 0.00001374
Iteration 52/1000 | Loss: 0.00001374
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001373
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001369
Iteration 69/1000 | Loss: 0.00001369
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001368
Iteration 73/1000 | Loss: 0.00001368
Iteration 74/1000 | Loss: 0.00001368
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001368
Iteration 77/1000 | Loss: 0.00001367
Iteration 78/1000 | Loss: 0.00001367
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001367
Iteration 82/1000 | Loss: 0.00001367
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001367
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001367
Iteration 101/1000 | Loss: 0.00001367
Iteration 102/1000 | Loss: 0.00001367
Iteration 103/1000 | Loss: 0.00001367
Iteration 104/1000 | Loss: 0.00001367
Iteration 105/1000 | Loss: 0.00001367
Iteration 106/1000 | Loss: 0.00001367
Iteration 107/1000 | Loss: 0.00001367
Iteration 108/1000 | Loss: 0.00001367
Iteration 109/1000 | Loss: 0.00001367
Iteration 110/1000 | Loss: 0.00001367
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001367
Iteration 113/1000 | Loss: 0.00001367
Iteration 114/1000 | Loss: 0.00001367
Iteration 115/1000 | Loss: 0.00001367
Iteration 116/1000 | Loss: 0.00001367
Iteration 117/1000 | Loss: 0.00001367
Iteration 118/1000 | Loss: 0.00001367
Iteration 119/1000 | Loss: 0.00001367
Iteration 120/1000 | Loss: 0.00001367
Iteration 121/1000 | Loss: 0.00001367
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.3666363884112798e-05, 1.3666363884112798e-05, 1.3666363884112798e-05, 1.3666363884112798e-05, 1.3666363884112798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3666363884112798e-05

Optimization complete. Final v2v error: 3.1476566791534424 mm

Highest mean error: 3.4181923866271973 mm for frame 102

Lowest mean error: 2.755241870880127 mm for frame 147

Saving results

Total time: 53.23623466491699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471138
Iteration 2/25 | Loss: 0.00110353
Iteration 3/25 | Loss: 0.00085002
Iteration 4/25 | Loss: 0.00078942
Iteration 5/25 | Loss: 0.00077020
Iteration 6/25 | Loss: 0.00076611
Iteration 7/25 | Loss: 0.00076479
Iteration 8/25 | Loss: 0.00076469
Iteration 9/25 | Loss: 0.00076469
Iteration 10/25 | Loss: 0.00076469
Iteration 11/25 | Loss: 0.00076469
Iteration 12/25 | Loss: 0.00076469
Iteration 13/25 | Loss: 0.00076469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007646893500350416, 0.0007646893500350416, 0.0007646893500350416, 0.0007646893500350416, 0.0007646893500350416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007646893500350416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.50101376
Iteration 2/25 | Loss: 0.00089795
Iteration 3/25 | Loss: 0.00089795
Iteration 4/25 | Loss: 0.00089794
Iteration 5/25 | Loss: 0.00089794
Iteration 6/25 | Loss: 0.00089794
Iteration 7/25 | Loss: 0.00089794
Iteration 8/25 | Loss: 0.00089794
Iteration 9/25 | Loss: 0.00089794
Iteration 10/25 | Loss: 0.00089794
Iteration 11/25 | Loss: 0.00089794
Iteration 12/25 | Loss: 0.00089794
Iteration 13/25 | Loss: 0.00089794
Iteration 14/25 | Loss: 0.00089794
Iteration 15/25 | Loss: 0.00089794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008979433332569897, 0.0008979433332569897, 0.0008979433332569897, 0.0008979433332569897, 0.0008979433332569897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008979433332569897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089794
Iteration 2/1000 | Loss: 0.00003649
Iteration 3/1000 | Loss: 0.00002489
Iteration 4/1000 | Loss: 0.00002162
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001954
Iteration 7/1000 | Loss: 0.00001888
Iteration 8/1000 | Loss: 0.00001855
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001749
Iteration 13/1000 | Loss: 0.00001740
Iteration 14/1000 | Loss: 0.00001732
Iteration 15/1000 | Loss: 0.00001731
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001726
Iteration 18/1000 | Loss: 0.00001724
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001718
Iteration 22/1000 | Loss: 0.00001717
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001714
Iteration 27/1000 | Loss: 0.00001713
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001712
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001704
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00001703
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001701
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001698
Iteration 59/1000 | Loss: 0.00001698
Iteration 60/1000 | Loss: 0.00001698
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00001698
Iteration 64/1000 | Loss: 0.00001698
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001696
Iteration 73/1000 | Loss: 0.00001696
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001695
Iteration 78/1000 | Loss: 0.00001695
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001695
Iteration 82/1000 | Loss: 0.00001695
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001694
Iteration 88/1000 | Loss: 0.00001694
Iteration 89/1000 | Loss: 0.00001694
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001693
Iteration 95/1000 | Loss: 0.00001693
Iteration 96/1000 | Loss: 0.00001692
Iteration 97/1000 | Loss: 0.00001692
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001691
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001691
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001689
Iteration 106/1000 | Loss: 0.00001689
Iteration 107/1000 | Loss: 0.00001689
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001688
Iteration 111/1000 | Loss: 0.00001688
Iteration 112/1000 | Loss: 0.00001688
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001686
Iteration 118/1000 | Loss: 0.00001686
Iteration 119/1000 | Loss: 0.00001686
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001685
Iteration 124/1000 | Loss: 0.00001685
Iteration 125/1000 | Loss: 0.00001685
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001684
Iteration 128/1000 | Loss: 0.00001684
Iteration 129/1000 | Loss: 0.00001684
Iteration 130/1000 | Loss: 0.00001684
Iteration 131/1000 | Loss: 0.00001684
Iteration 132/1000 | Loss: 0.00001684
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001684
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001683
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001683
Iteration 140/1000 | Loss: 0.00001683
Iteration 141/1000 | Loss: 0.00001683
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001683
Iteration 146/1000 | Loss: 0.00001683
Iteration 147/1000 | Loss: 0.00001683
Iteration 148/1000 | Loss: 0.00001683
Iteration 149/1000 | Loss: 0.00001683
Iteration 150/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.6830217646202073e-05, 1.6830217646202073e-05, 1.6830217646202073e-05, 1.6830217646202073e-05, 1.6830217646202073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6830217646202073e-05

Optimization complete. Final v2v error: 3.4294543266296387 mm

Highest mean error: 4.438462257385254 mm for frame 56

Lowest mean error: 2.8780300617218018 mm for frame 102

Saving results

Total time: 46.91041445732117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393756
Iteration 2/25 | Loss: 0.00087134
Iteration 3/25 | Loss: 0.00072111
Iteration 4/25 | Loss: 0.00069750
Iteration 5/25 | Loss: 0.00069189
Iteration 6/25 | Loss: 0.00069000
Iteration 7/25 | Loss: 0.00068947
Iteration 8/25 | Loss: 0.00068947
Iteration 9/25 | Loss: 0.00068947
Iteration 10/25 | Loss: 0.00068947
Iteration 11/25 | Loss: 0.00068947
Iteration 12/25 | Loss: 0.00068947
Iteration 13/25 | Loss: 0.00068947
Iteration 14/25 | Loss: 0.00068947
Iteration 15/25 | Loss: 0.00068947
Iteration 16/25 | Loss: 0.00068947
Iteration 17/25 | Loss: 0.00068947
Iteration 18/25 | Loss: 0.00068947
Iteration 19/25 | Loss: 0.00068947
Iteration 20/25 | Loss: 0.00068947
Iteration 21/25 | Loss: 0.00068947
Iteration 22/25 | Loss: 0.00068947
Iteration 23/25 | Loss: 0.00068947
Iteration 24/25 | Loss: 0.00068947
Iteration 25/25 | Loss: 0.00068947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55692220
Iteration 2/25 | Loss: 0.00074168
Iteration 3/25 | Loss: 0.00074167
Iteration 4/25 | Loss: 0.00074167
Iteration 5/25 | Loss: 0.00074167
Iteration 6/25 | Loss: 0.00074167
Iteration 7/25 | Loss: 0.00074167
Iteration 8/25 | Loss: 0.00074167
Iteration 9/25 | Loss: 0.00074167
Iteration 10/25 | Loss: 0.00074167
Iteration 11/25 | Loss: 0.00074167
Iteration 12/25 | Loss: 0.00074167
Iteration 13/25 | Loss: 0.00074167
Iteration 14/25 | Loss: 0.00074167
Iteration 15/25 | Loss: 0.00074167
Iteration 16/25 | Loss: 0.00074167
Iteration 17/25 | Loss: 0.00074167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007416664739139378, 0.0007416664739139378, 0.0007416664739139378, 0.0007416664739139378, 0.0007416664739139378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007416664739139378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074167
Iteration 2/1000 | Loss: 0.00002166
Iteration 3/1000 | Loss: 0.00001282
Iteration 4/1000 | Loss: 0.00001149
Iteration 5/1000 | Loss: 0.00001080
Iteration 6/1000 | Loss: 0.00001037
Iteration 7/1000 | Loss: 0.00001010
Iteration 8/1000 | Loss: 0.00001001
Iteration 9/1000 | Loss: 0.00001001
Iteration 10/1000 | Loss: 0.00000999
Iteration 11/1000 | Loss: 0.00000998
Iteration 12/1000 | Loss: 0.00000998
Iteration 13/1000 | Loss: 0.00000998
Iteration 14/1000 | Loss: 0.00000998
Iteration 15/1000 | Loss: 0.00000997
Iteration 16/1000 | Loss: 0.00000995
Iteration 17/1000 | Loss: 0.00000995
Iteration 18/1000 | Loss: 0.00000994
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000992
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000990
Iteration 26/1000 | Loss: 0.00000987
Iteration 27/1000 | Loss: 0.00000985
Iteration 28/1000 | Loss: 0.00000983
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000981
Iteration 31/1000 | Loss: 0.00000980
Iteration 32/1000 | Loss: 0.00000980
Iteration 33/1000 | Loss: 0.00000979
Iteration 34/1000 | Loss: 0.00000979
Iteration 35/1000 | Loss: 0.00000978
Iteration 36/1000 | Loss: 0.00000978
Iteration 37/1000 | Loss: 0.00000977
Iteration 38/1000 | Loss: 0.00000977
Iteration 39/1000 | Loss: 0.00000977
Iteration 40/1000 | Loss: 0.00000976
Iteration 41/1000 | Loss: 0.00000976
Iteration 42/1000 | Loss: 0.00000976
Iteration 43/1000 | Loss: 0.00000975
Iteration 44/1000 | Loss: 0.00000975
Iteration 45/1000 | Loss: 0.00000974
Iteration 46/1000 | Loss: 0.00000974
Iteration 47/1000 | Loss: 0.00000974
Iteration 48/1000 | Loss: 0.00000973
Iteration 49/1000 | Loss: 0.00000973
Iteration 50/1000 | Loss: 0.00000973
Iteration 51/1000 | Loss: 0.00000973
Iteration 52/1000 | Loss: 0.00000973
Iteration 53/1000 | Loss: 0.00000972
Iteration 54/1000 | Loss: 0.00000972
Iteration 55/1000 | Loss: 0.00000972
Iteration 56/1000 | Loss: 0.00000972
Iteration 57/1000 | Loss: 0.00000971
Iteration 58/1000 | Loss: 0.00000971
Iteration 59/1000 | Loss: 0.00000971
Iteration 60/1000 | Loss: 0.00000970
Iteration 61/1000 | Loss: 0.00000970
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000969
Iteration 64/1000 | Loss: 0.00000969
Iteration 65/1000 | Loss: 0.00000968
Iteration 66/1000 | Loss: 0.00000968
Iteration 67/1000 | Loss: 0.00000967
Iteration 68/1000 | Loss: 0.00000967
Iteration 69/1000 | Loss: 0.00000966
Iteration 70/1000 | Loss: 0.00000966
Iteration 71/1000 | Loss: 0.00000966
Iteration 72/1000 | Loss: 0.00000966
Iteration 73/1000 | Loss: 0.00000966
Iteration 74/1000 | Loss: 0.00000966
Iteration 75/1000 | Loss: 0.00000965
Iteration 76/1000 | Loss: 0.00000965
Iteration 77/1000 | Loss: 0.00000965
Iteration 78/1000 | Loss: 0.00000964
Iteration 79/1000 | Loss: 0.00000964
Iteration 80/1000 | Loss: 0.00000964
Iteration 81/1000 | Loss: 0.00000963
Iteration 82/1000 | Loss: 0.00000963
Iteration 83/1000 | Loss: 0.00000963
Iteration 84/1000 | Loss: 0.00000963
Iteration 85/1000 | Loss: 0.00000962
Iteration 86/1000 | Loss: 0.00000962
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000961
Iteration 90/1000 | Loss: 0.00000961
Iteration 91/1000 | Loss: 0.00000961
Iteration 92/1000 | Loss: 0.00000961
Iteration 93/1000 | Loss: 0.00000960
Iteration 94/1000 | Loss: 0.00000960
Iteration 95/1000 | Loss: 0.00000960
Iteration 96/1000 | Loss: 0.00000960
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000960
Iteration 99/1000 | Loss: 0.00000960
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000959
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000958
Iteration 116/1000 | Loss: 0.00000958
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000958
Iteration 119/1000 | Loss: 0.00000957
Iteration 120/1000 | Loss: 0.00000957
Iteration 121/1000 | Loss: 0.00000957
Iteration 122/1000 | Loss: 0.00000957
Iteration 123/1000 | Loss: 0.00000957
Iteration 124/1000 | Loss: 0.00000957
Iteration 125/1000 | Loss: 0.00000957
Iteration 126/1000 | Loss: 0.00000957
Iteration 127/1000 | Loss: 0.00000956
Iteration 128/1000 | Loss: 0.00000956
Iteration 129/1000 | Loss: 0.00000956
Iteration 130/1000 | Loss: 0.00000956
Iteration 131/1000 | Loss: 0.00000956
Iteration 132/1000 | Loss: 0.00000956
Iteration 133/1000 | Loss: 0.00000955
Iteration 134/1000 | Loss: 0.00000955
Iteration 135/1000 | Loss: 0.00000955
Iteration 136/1000 | Loss: 0.00000955
Iteration 137/1000 | Loss: 0.00000954
Iteration 138/1000 | Loss: 0.00000954
Iteration 139/1000 | Loss: 0.00000954
Iteration 140/1000 | Loss: 0.00000954
Iteration 141/1000 | Loss: 0.00000953
Iteration 142/1000 | Loss: 0.00000953
Iteration 143/1000 | Loss: 0.00000953
Iteration 144/1000 | Loss: 0.00000953
Iteration 145/1000 | Loss: 0.00000953
Iteration 146/1000 | Loss: 0.00000952
Iteration 147/1000 | Loss: 0.00000952
Iteration 148/1000 | Loss: 0.00000952
Iteration 149/1000 | Loss: 0.00000952
Iteration 150/1000 | Loss: 0.00000952
Iteration 151/1000 | Loss: 0.00000952
Iteration 152/1000 | Loss: 0.00000951
Iteration 153/1000 | Loss: 0.00000951
Iteration 154/1000 | Loss: 0.00000951
Iteration 155/1000 | Loss: 0.00000951
Iteration 156/1000 | Loss: 0.00000951
Iteration 157/1000 | Loss: 0.00000951
Iteration 158/1000 | Loss: 0.00000951
Iteration 159/1000 | Loss: 0.00000951
Iteration 160/1000 | Loss: 0.00000950
Iteration 161/1000 | Loss: 0.00000950
Iteration 162/1000 | Loss: 0.00000950
Iteration 163/1000 | Loss: 0.00000950
Iteration 164/1000 | Loss: 0.00000950
Iteration 165/1000 | Loss: 0.00000950
Iteration 166/1000 | Loss: 0.00000950
Iteration 167/1000 | Loss: 0.00000950
Iteration 168/1000 | Loss: 0.00000950
Iteration 169/1000 | Loss: 0.00000950
Iteration 170/1000 | Loss: 0.00000950
Iteration 171/1000 | Loss: 0.00000950
Iteration 172/1000 | Loss: 0.00000949
Iteration 173/1000 | Loss: 0.00000949
Iteration 174/1000 | Loss: 0.00000949
Iteration 175/1000 | Loss: 0.00000949
Iteration 176/1000 | Loss: 0.00000949
Iteration 177/1000 | Loss: 0.00000949
Iteration 178/1000 | Loss: 0.00000949
Iteration 179/1000 | Loss: 0.00000949
Iteration 180/1000 | Loss: 0.00000949
Iteration 181/1000 | Loss: 0.00000949
Iteration 182/1000 | Loss: 0.00000949
Iteration 183/1000 | Loss: 0.00000949
Iteration 184/1000 | Loss: 0.00000949
Iteration 185/1000 | Loss: 0.00000948
Iteration 186/1000 | Loss: 0.00000948
Iteration 187/1000 | Loss: 0.00000948
Iteration 188/1000 | Loss: 0.00000948
Iteration 189/1000 | Loss: 0.00000948
Iteration 190/1000 | Loss: 0.00000948
Iteration 191/1000 | Loss: 0.00000948
Iteration 192/1000 | Loss: 0.00000948
Iteration 193/1000 | Loss: 0.00000948
Iteration 194/1000 | Loss: 0.00000948
Iteration 195/1000 | Loss: 0.00000948
Iteration 196/1000 | Loss: 0.00000948
Iteration 197/1000 | Loss: 0.00000948
Iteration 198/1000 | Loss: 0.00000948
Iteration 199/1000 | Loss: 0.00000948
Iteration 200/1000 | Loss: 0.00000948
Iteration 201/1000 | Loss: 0.00000948
Iteration 202/1000 | Loss: 0.00000948
Iteration 203/1000 | Loss: 0.00000948
Iteration 204/1000 | Loss: 0.00000948
Iteration 205/1000 | Loss: 0.00000948
Iteration 206/1000 | Loss: 0.00000948
Iteration 207/1000 | Loss: 0.00000948
Iteration 208/1000 | Loss: 0.00000948
Iteration 209/1000 | Loss: 0.00000948
Iteration 210/1000 | Loss: 0.00000948
Iteration 211/1000 | Loss: 0.00000948
Iteration 212/1000 | Loss: 0.00000948
Iteration 213/1000 | Loss: 0.00000948
Iteration 214/1000 | Loss: 0.00000948
Iteration 215/1000 | Loss: 0.00000948
Iteration 216/1000 | Loss: 0.00000948
Iteration 217/1000 | Loss: 0.00000948
Iteration 218/1000 | Loss: 0.00000948
Iteration 219/1000 | Loss: 0.00000948
Iteration 220/1000 | Loss: 0.00000948
Iteration 221/1000 | Loss: 0.00000948
Iteration 222/1000 | Loss: 0.00000948
Iteration 223/1000 | Loss: 0.00000948
Iteration 224/1000 | Loss: 0.00000948
Iteration 225/1000 | Loss: 0.00000948
Iteration 226/1000 | Loss: 0.00000948
Iteration 227/1000 | Loss: 0.00000948
Iteration 228/1000 | Loss: 0.00000948
Iteration 229/1000 | Loss: 0.00000948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [9.475211300014053e-06, 9.475211300014053e-06, 9.475211300014053e-06, 9.475211300014053e-06, 9.475211300014053e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.475211300014053e-06

Optimization complete. Final v2v error: 2.6148030757904053 mm

Highest mean error: 3.442354440689087 mm for frame 72

Lowest mean error: 2.474158525466919 mm for frame 113

Saving results

Total time: 38.71119952201843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398585
Iteration 2/25 | Loss: 0.00085081
Iteration 3/25 | Loss: 0.00074250
Iteration 4/25 | Loss: 0.00071502
Iteration 5/25 | Loss: 0.00070999
Iteration 6/25 | Loss: 0.00070873
Iteration 7/25 | Loss: 0.00070845
Iteration 8/25 | Loss: 0.00070845
Iteration 9/25 | Loss: 0.00070845
Iteration 10/25 | Loss: 0.00070845
Iteration 11/25 | Loss: 0.00070845
Iteration 12/25 | Loss: 0.00070845
Iteration 13/25 | Loss: 0.00070845
Iteration 14/25 | Loss: 0.00070845
Iteration 15/25 | Loss: 0.00070845
Iteration 16/25 | Loss: 0.00070845
Iteration 17/25 | Loss: 0.00070845
Iteration 18/25 | Loss: 0.00070845
Iteration 19/25 | Loss: 0.00070843
Iteration 20/25 | Loss: 0.00070843
Iteration 21/25 | Loss: 0.00070843
Iteration 22/25 | Loss: 0.00070843
Iteration 23/25 | Loss: 0.00070843
Iteration 24/25 | Loss: 0.00070843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007084321114234626, 0.0007084321114234626, 0.0007084321114234626, 0.0007084321114234626, 0.0007084321114234626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007084321114234626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94269037
Iteration 2/25 | Loss: 0.00081364
Iteration 3/25 | Loss: 0.00081364
Iteration 4/25 | Loss: 0.00081364
Iteration 5/25 | Loss: 0.00081364
Iteration 6/25 | Loss: 0.00081364
Iteration 7/25 | Loss: 0.00081364
Iteration 8/25 | Loss: 0.00081364
Iteration 9/25 | Loss: 0.00081364
Iteration 10/25 | Loss: 0.00081364
Iteration 11/25 | Loss: 0.00081363
Iteration 12/25 | Loss: 0.00081363
Iteration 13/25 | Loss: 0.00081363
Iteration 14/25 | Loss: 0.00081363
Iteration 15/25 | Loss: 0.00081363
Iteration 16/25 | Loss: 0.00081363
Iteration 17/25 | Loss: 0.00081363
Iteration 18/25 | Loss: 0.00081363
Iteration 19/25 | Loss: 0.00081363
Iteration 20/25 | Loss: 0.00081363
Iteration 21/25 | Loss: 0.00081363
Iteration 22/25 | Loss: 0.00081363
Iteration 23/25 | Loss: 0.00081363
Iteration 24/25 | Loss: 0.00081363
Iteration 25/25 | Loss: 0.00081363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008136347751133144, 0.0008136347751133144, 0.0008136347751133144, 0.0008136347751133144, 0.0008136347751133144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008136347751133144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081363
Iteration 2/1000 | Loss: 0.00002415
Iteration 3/1000 | Loss: 0.00001844
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001580
Iteration 8/1000 | Loss: 0.00001558
Iteration 9/1000 | Loss: 0.00001536
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001520
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001520
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001515
Iteration 17/1000 | Loss: 0.00001515
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001509
Iteration 22/1000 | Loss: 0.00001509
Iteration 23/1000 | Loss: 0.00001509
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001504
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001503
Iteration 31/1000 | Loss: 0.00001503
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001501
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001499
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001496
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001496
Iteration 52/1000 | Loss: 0.00001496
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001495
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001494
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001494
Iteration 65/1000 | Loss: 0.00001494
Iteration 66/1000 | Loss: 0.00001494
Iteration 67/1000 | Loss: 0.00001494
Iteration 68/1000 | Loss: 0.00001494
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001493
Iteration 71/1000 | Loss: 0.00001493
Iteration 72/1000 | Loss: 0.00001493
Iteration 73/1000 | Loss: 0.00001493
Iteration 74/1000 | Loss: 0.00001493
Iteration 75/1000 | Loss: 0.00001493
Iteration 76/1000 | Loss: 0.00001493
Iteration 77/1000 | Loss: 0.00001493
Iteration 78/1000 | Loss: 0.00001493
Iteration 79/1000 | Loss: 0.00001493
Iteration 80/1000 | Loss: 0.00001493
Iteration 81/1000 | Loss: 0.00001492
Iteration 82/1000 | Loss: 0.00001492
Iteration 83/1000 | Loss: 0.00001492
Iteration 84/1000 | Loss: 0.00001492
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001491
Iteration 87/1000 | Loss: 0.00001490
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001489
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001486
Iteration 99/1000 | Loss: 0.00001485
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001484
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001483
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001482
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001480
Iteration 113/1000 | Loss: 0.00001480
Iteration 114/1000 | Loss: 0.00001480
Iteration 115/1000 | Loss: 0.00001480
Iteration 116/1000 | Loss: 0.00001479
Iteration 117/1000 | Loss: 0.00001479
Iteration 118/1000 | Loss: 0.00001479
Iteration 119/1000 | Loss: 0.00001479
Iteration 120/1000 | Loss: 0.00001479
Iteration 121/1000 | Loss: 0.00001479
Iteration 122/1000 | Loss: 0.00001479
Iteration 123/1000 | Loss: 0.00001479
Iteration 124/1000 | Loss: 0.00001479
Iteration 125/1000 | Loss: 0.00001479
Iteration 126/1000 | Loss: 0.00001479
Iteration 127/1000 | Loss: 0.00001478
Iteration 128/1000 | Loss: 0.00001478
Iteration 129/1000 | Loss: 0.00001478
Iteration 130/1000 | Loss: 0.00001478
Iteration 131/1000 | Loss: 0.00001478
Iteration 132/1000 | Loss: 0.00001478
Iteration 133/1000 | Loss: 0.00001478
Iteration 134/1000 | Loss: 0.00001478
Iteration 135/1000 | Loss: 0.00001478
Iteration 136/1000 | Loss: 0.00001478
Iteration 137/1000 | Loss: 0.00001478
Iteration 138/1000 | Loss: 0.00001477
Iteration 139/1000 | Loss: 0.00001477
Iteration 140/1000 | Loss: 0.00001477
Iteration 141/1000 | Loss: 0.00001477
Iteration 142/1000 | Loss: 0.00001477
Iteration 143/1000 | Loss: 0.00001477
Iteration 144/1000 | Loss: 0.00001477
Iteration 145/1000 | Loss: 0.00001477
Iteration 146/1000 | Loss: 0.00001477
Iteration 147/1000 | Loss: 0.00001477
Iteration 148/1000 | Loss: 0.00001477
Iteration 149/1000 | Loss: 0.00001477
Iteration 150/1000 | Loss: 0.00001477
Iteration 151/1000 | Loss: 0.00001477
Iteration 152/1000 | Loss: 0.00001477
Iteration 153/1000 | Loss: 0.00001477
Iteration 154/1000 | Loss: 0.00001477
Iteration 155/1000 | Loss: 0.00001477
Iteration 156/1000 | Loss: 0.00001477
Iteration 157/1000 | Loss: 0.00001477
Iteration 158/1000 | Loss: 0.00001477
Iteration 159/1000 | Loss: 0.00001477
Iteration 160/1000 | Loss: 0.00001477
Iteration 161/1000 | Loss: 0.00001477
Iteration 162/1000 | Loss: 0.00001477
Iteration 163/1000 | Loss: 0.00001477
Iteration 164/1000 | Loss: 0.00001477
Iteration 165/1000 | Loss: 0.00001477
Iteration 166/1000 | Loss: 0.00001477
Iteration 167/1000 | Loss: 0.00001477
Iteration 168/1000 | Loss: 0.00001477
Iteration 169/1000 | Loss: 0.00001477
Iteration 170/1000 | Loss: 0.00001477
Iteration 171/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.4773588191019371e-05, 1.4773588191019371e-05, 1.4773588191019371e-05, 1.4773588191019371e-05, 1.4773588191019371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4773588191019371e-05

Optimization complete. Final v2v error: 3.2643089294433594 mm

Highest mean error: 3.4593610763549805 mm for frame 148

Lowest mean error: 3.053912878036499 mm for frame 115

Saving results

Total time: 37.64713191986084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440131
Iteration 2/25 | Loss: 0.00096789
Iteration 3/25 | Loss: 0.00082537
Iteration 4/25 | Loss: 0.00079210
Iteration 5/25 | Loss: 0.00078146
Iteration 6/25 | Loss: 0.00077957
Iteration 7/25 | Loss: 0.00077905
Iteration 8/25 | Loss: 0.00077905
Iteration 9/25 | Loss: 0.00077905
Iteration 10/25 | Loss: 0.00077905
Iteration 11/25 | Loss: 0.00077905
Iteration 12/25 | Loss: 0.00077905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007790462695993483, 0.0007790462695993483, 0.0007790462695993483, 0.0007790462695993483, 0.0007790462695993483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007790462695993483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50044918
Iteration 2/25 | Loss: 0.00086982
Iteration 3/25 | Loss: 0.00086981
Iteration 4/25 | Loss: 0.00086981
Iteration 5/25 | Loss: 0.00086981
Iteration 6/25 | Loss: 0.00086981
Iteration 7/25 | Loss: 0.00086981
Iteration 8/25 | Loss: 0.00086981
Iteration 9/25 | Loss: 0.00086981
Iteration 10/25 | Loss: 0.00086981
Iteration 11/25 | Loss: 0.00086981
Iteration 12/25 | Loss: 0.00086981
Iteration 13/25 | Loss: 0.00086981
Iteration 14/25 | Loss: 0.00086981
Iteration 15/25 | Loss: 0.00086981
Iteration 16/25 | Loss: 0.00086981
Iteration 17/25 | Loss: 0.00086981
Iteration 18/25 | Loss: 0.00086981
Iteration 19/25 | Loss: 0.00086981
Iteration 20/25 | Loss: 0.00086981
Iteration 21/25 | Loss: 0.00086981
Iteration 22/25 | Loss: 0.00086981
Iteration 23/25 | Loss: 0.00086981
Iteration 24/25 | Loss: 0.00086981
Iteration 25/25 | Loss: 0.00086981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086981
Iteration 2/1000 | Loss: 0.00004761
Iteration 3/1000 | Loss: 0.00003204
Iteration 4/1000 | Loss: 0.00002616
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002325
Iteration 8/1000 | Loss: 0.00002271
Iteration 9/1000 | Loss: 0.00002227
Iteration 10/1000 | Loss: 0.00002192
Iteration 11/1000 | Loss: 0.00002167
Iteration 12/1000 | Loss: 0.00002155
Iteration 13/1000 | Loss: 0.00002149
Iteration 14/1000 | Loss: 0.00002149
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002139
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002128
Iteration 19/1000 | Loss: 0.00002128
Iteration 20/1000 | Loss: 0.00002125
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002121
Iteration 23/1000 | Loss: 0.00002120
Iteration 24/1000 | Loss: 0.00002119
Iteration 25/1000 | Loss: 0.00002117
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002116
Iteration 28/1000 | Loss: 0.00002116
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002115
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002113
Iteration 35/1000 | Loss: 0.00002113
Iteration 36/1000 | Loss: 0.00002112
Iteration 37/1000 | Loss: 0.00002112
Iteration 38/1000 | Loss: 0.00002112
Iteration 39/1000 | Loss: 0.00002111
Iteration 40/1000 | Loss: 0.00002111
Iteration 41/1000 | Loss: 0.00002111
Iteration 42/1000 | Loss: 0.00002110
Iteration 43/1000 | Loss: 0.00002110
Iteration 44/1000 | Loss: 0.00002110
Iteration 45/1000 | Loss: 0.00002110
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002109
Iteration 54/1000 | Loss: 0.00002109
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002108
Iteration 59/1000 | Loss: 0.00002107
Iteration 60/1000 | Loss: 0.00002107
Iteration 61/1000 | Loss: 0.00002107
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002106
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002105
Iteration 66/1000 | Loss: 0.00002105
Iteration 67/1000 | Loss: 0.00002104
Iteration 68/1000 | Loss: 0.00002104
Iteration 69/1000 | Loss: 0.00002103
Iteration 70/1000 | Loss: 0.00002103
Iteration 71/1000 | Loss: 0.00002102
Iteration 72/1000 | Loss: 0.00002102
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00002101
Iteration 75/1000 | Loss: 0.00002100
Iteration 76/1000 | Loss: 0.00002100
Iteration 77/1000 | Loss: 0.00002100
Iteration 78/1000 | Loss: 0.00002099
Iteration 79/1000 | Loss: 0.00002099
Iteration 80/1000 | Loss: 0.00002099
Iteration 81/1000 | Loss: 0.00002099
Iteration 82/1000 | Loss: 0.00002099
Iteration 83/1000 | Loss: 0.00002098
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002098
Iteration 86/1000 | Loss: 0.00002097
Iteration 87/1000 | Loss: 0.00002097
Iteration 88/1000 | Loss: 0.00002097
Iteration 89/1000 | Loss: 0.00002097
Iteration 90/1000 | Loss: 0.00002096
Iteration 91/1000 | Loss: 0.00002096
Iteration 92/1000 | Loss: 0.00002096
Iteration 93/1000 | Loss: 0.00002096
Iteration 94/1000 | Loss: 0.00002095
Iteration 95/1000 | Loss: 0.00002095
Iteration 96/1000 | Loss: 0.00002095
Iteration 97/1000 | Loss: 0.00002095
Iteration 98/1000 | Loss: 0.00002094
Iteration 99/1000 | Loss: 0.00002094
Iteration 100/1000 | Loss: 0.00002094
Iteration 101/1000 | Loss: 0.00002094
Iteration 102/1000 | Loss: 0.00002094
Iteration 103/1000 | Loss: 0.00002094
Iteration 104/1000 | Loss: 0.00002094
Iteration 105/1000 | Loss: 0.00002094
Iteration 106/1000 | Loss: 0.00002094
Iteration 107/1000 | Loss: 0.00002093
Iteration 108/1000 | Loss: 0.00002093
Iteration 109/1000 | Loss: 0.00002093
Iteration 110/1000 | Loss: 0.00002093
Iteration 111/1000 | Loss: 0.00002093
Iteration 112/1000 | Loss: 0.00002093
Iteration 113/1000 | Loss: 0.00002093
Iteration 114/1000 | Loss: 0.00002093
Iteration 115/1000 | Loss: 0.00002093
Iteration 116/1000 | Loss: 0.00002093
Iteration 117/1000 | Loss: 0.00002093
Iteration 118/1000 | Loss: 0.00002093
Iteration 119/1000 | Loss: 0.00002093
Iteration 120/1000 | Loss: 0.00002092
Iteration 121/1000 | Loss: 0.00002092
Iteration 122/1000 | Loss: 0.00002092
Iteration 123/1000 | Loss: 0.00002092
Iteration 124/1000 | Loss: 0.00002092
Iteration 125/1000 | Loss: 0.00002092
Iteration 126/1000 | Loss: 0.00002092
Iteration 127/1000 | Loss: 0.00002092
Iteration 128/1000 | Loss: 0.00002091
Iteration 129/1000 | Loss: 0.00002091
Iteration 130/1000 | Loss: 0.00002091
Iteration 131/1000 | Loss: 0.00002091
Iteration 132/1000 | Loss: 0.00002091
Iteration 133/1000 | Loss: 0.00002091
Iteration 134/1000 | Loss: 0.00002091
Iteration 135/1000 | Loss: 0.00002091
Iteration 136/1000 | Loss: 0.00002091
Iteration 137/1000 | Loss: 0.00002091
Iteration 138/1000 | Loss: 0.00002091
Iteration 139/1000 | Loss: 0.00002091
Iteration 140/1000 | Loss: 0.00002091
Iteration 141/1000 | Loss: 0.00002091
Iteration 142/1000 | Loss: 0.00002091
Iteration 143/1000 | Loss: 0.00002091
Iteration 144/1000 | Loss: 0.00002091
Iteration 145/1000 | Loss: 0.00002091
Iteration 146/1000 | Loss: 0.00002090
Iteration 147/1000 | Loss: 0.00002090
Iteration 148/1000 | Loss: 0.00002090
Iteration 149/1000 | Loss: 0.00002090
Iteration 150/1000 | Loss: 0.00002090
Iteration 151/1000 | Loss: 0.00002090
Iteration 152/1000 | Loss: 0.00002090
Iteration 153/1000 | Loss: 0.00002090
Iteration 154/1000 | Loss: 0.00002090
Iteration 155/1000 | Loss: 0.00002090
Iteration 156/1000 | Loss: 0.00002090
Iteration 157/1000 | Loss: 0.00002090
Iteration 158/1000 | Loss: 0.00002090
Iteration 159/1000 | Loss: 0.00002090
Iteration 160/1000 | Loss: 0.00002090
Iteration 161/1000 | Loss: 0.00002090
Iteration 162/1000 | Loss: 0.00002090
Iteration 163/1000 | Loss: 0.00002090
Iteration 164/1000 | Loss: 0.00002090
Iteration 165/1000 | Loss: 0.00002090
Iteration 166/1000 | Loss: 0.00002090
Iteration 167/1000 | Loss: 0.00002089
Iteration 168/1000 | Loss: 0.00002089
Iteration 169/1000 | Loss: 0.00002089
Iteration 170/1000 | Loss: 0.00002089
Iteration 171/1000 | Loss: 0.00002089
Iteration 172/1000 | Loss: 0.00002089
Iteration 173/1000 | Loss: 0.00002089
Iteration 174/1000 | Loss: 0.00002089
Iteration 175/1000 | Loss: 0.00002089
Iteration 176/1000 | Loss: 0.00002089
Iteration 177/1000 | Loss: 0.00002089
Iteration 178/1000 | Loss: 0.00002089
Iteration 179/1000 | Loss: 0.00002089
Iteration 180/1000 | Loss: 0.00002089
Iteration 181/1000 | Loss: 0.00002089
Iteration 182/1000 | Loss: 0.00002089
Iteration 183/1000 | Loss: 0.00002089
Iteration 184/1000 | Loss: 0.00002089
Iteration 185/1000 | Loss: 0.00002089
Iteration 186/1000 | Loss: 0.00002089
Iteration 187/1000 | Loss: 0.00002088
Iteration 188/1000 | Loss: 0.00002088
Iteration 189/1000 | Loss: 0.00002088
Iteration 190/1000 | Loss: 0.00002088
Iteration 191/1000 | Loss: 0.00002088
Iteration 192/1000 | Loss: 0.00002088
Iteration 193/1000 | Loss: 0.00002088
Iteration 194/1000 | Loss: 0.00002088
Iteration 195/1000 | Loss: 0.00002088
Iteration 196/1000 | Loss: 0.00002088
Iteration 197/1000 | Loss: 0.00002088
Iteration 198/1000 | Loss: 0.00002088
Iteration 199/1000 | Loss: 0.00002088
Iteration 200/1000 | Loss: 0.00002088
Iteration 201/1000 | Loss: 0.00002088
Iteration 202/1000 | Loss: 0.00002088
Iteration 203/1000 | Loss: 0.00002088
Iteration 204/1000 | Loss: 0.00002088
Iteration 205/1000 | Loss: 0.00002088
Iteration 206/1000 | Loss: 0.00002088
Iteration 207/1000 | Loss: 0.00002088
Iteration 208/1000 | Loss: 0.00002088
Iteration 209/1000 | Loss: 0.00002088
Iteration 210/1000 | Loss: 0.00002088
Iteration 211/1000 | Loss: 0.00002088
Iteration 212/1000 | Loss: 0.00002088
Iteration 213/1000 | Loss: 0.00002087
Iteration 214/1000 | Loss: 0.00002087
Iteration 215/1000 | Loss: 0.00002087
Iteration 216/1000 | Loss: 0.00002087
Iteration 217/1000 | Loss: 0.00002087
Iteration 218/1000 | Loss: 0.00002087
Iteration 219/1000 | Loss: 0.00002087
Iteration 220/1000 | Loss: 0.00002087
Iteration 221/1000 | Loss: 0.00002087
Iteration 222/1000 | Loss: 0.00002087
Iteration 223/1000 | Loss: 0.00002087
Iteration 224/1000 | Loss: 0.00002087
Iteration 225/1000 | Loss: 0.00002087
Iteration 226/1000 | Loss: 0.00002087
Iteration 227/1000 | Loss: 0.00002087
Iteration 228/1000 | Loss: 0.00002087
Iteration 229/1000 | Loss: 0.00002087
Iteration 230/1000 | Loss: 0.00002087
Iteration 231/1000 | Loss: 0.00002087
Iteration 232/1000 | Loss: 0.00002087
Iteration 233/1000 | Loss: 0.00002087
Iteration 234/1000 | Loss: 0.00002087
Iteration 235/1000 | Loss: 0.00002087
Iteration 236/1000 | Loss: 0.00002087
Iteration 237/1000 | Loss: 0.00002087
Iteration 238/1000 | Loss: 0.00002087
Iteration 239/1000 | Loss: 0.00002087
Iteration 240/1000 | Loss: 0.00002087
Iteration 241/1000 | Loss: 0.00002087
Iteration 242/1000 | Loss: 0.00002087
Iteration 243/1000 | Loss: 0.00002087
Iteration 244/1000 | Loss: 0.00002087
Iteration 245/1000 | Loss: 0.00002087
Iteration 246/1000 | Loss: 0.00002087
Iteration 247/1000 | Loss: 0.00002087
Iteration 248/1000 | Loss: 0.00002087
Iteration 249/1000 | Loss: 0.00002087
Iteration 250/1000 | Loss: 0.00002087
Iteration 251/1000 | Loss: 0.00002087
Iteration 252/1000 | Loss: 0.00002087
Iteration 253/1000 | Loss: 0.00002087
Iteration 254/1000 | Loss: 0.00002087
Iteration 255/1000 | Loss: 0.00002087
Iteration 256/1000 | Loss: 0.00002087
Iteration 257/1000 | Loss: 0.00002087
Iteration 258/1000 | Loss: 0.00002087
Iteration 259/1000 | Loss: 0.00002087
Iteration 260/1000 | Loss: 0.00002087
Iteration 261/1000 | Loss: 0.00002087
Iteration 262/1000 | Loss: 0.00002087
Iteration 263/1000 | Loss: 0.00002087
Iteration 264/1000 | Loss: 0.00002087
Iteration 265/1000 | Loss: 0.00002087
Iteration 266/1000 | Loss: 0.00002087
Iteration 267/1000 | Loss: 0.00002087
Iteration 268/1000 | Loss: 0.00002087
Iteration 269/1000 | Loss: 0.00002087
Iteration 270/1000 | Loss: 0.00002087
Iteration 271/1000 | Loss: 0.00002087
Iteration 272/1000 | Loss: 0.00002087
Iteration 273/1000 | Loss: 0.00002087
Iteration 274/1000 | Loss: 0.00002087
Iteration 275/1000 | Loss: 0.00002087
Iteration 276/1000 | Loss: 0.00002087
Iteration 277/1000 | Loss: 0.00002087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [2.087061329802964e-05, 2.087061329802964e-05, 2.087061329802964e-05, 2.087061329802964e-05, 2.087061329802964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.087061329802964e-05

Optimization complete. Final v2v error: 3.781050682067871 mm

Highest mean error: 4.0240278244018555 mm for frame 101

Lowest mean error: 3.413078546524048 mm for frame 25

Saving results

Total time: 44.39924597740173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479624
Iteration 2/25 | Loss: 0.00106078
Iteration 3/25 | Loss: 0.00087325
Iteration 4/25 | Loss: 0.00082242
Iteration 5/25 | Loss: 0.00080445
Iteration 6/25 | Loss: 0.00080111
Iteration 7/25 | Loss: 0.00079962
Iteration 8/25 | Loss: 0.00079944
Iteration 9/25 | Loss: 0.00079944
Iteration 10/25 | Loss: 0.00079944
Iteration 11/25 | Loss: 0.00079944
Iteration 12/25 | Loss: 0.00079944
Iteration 13/25 | Loss: 0.00079944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007994425250217319, 0.0007994425250217319, 0.0007994425250217319, 0.0007994425250217319, 0.0007994425250217319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007994425250217319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59233475
Iteration 2/25 | Loss: 0.00154312
Iteration 3/25 | Loss: 0.00154311
Iteration 4/25 | Loss: 0.00154311
Iteration 5/25 | Loss: 0.00154311
Iteration 6/25 | Loss: 0.00154311
Iteration 7/25 | Loss: 0.00154311
Iteration 8/25 | Loss: 0.00154311
Iteration 9/25 | Loss: 0.00154311
Iteration 10/25 | Loss: 0.00154311
Iteration 11/25 | Loss: 0.00154311
Iteration 12/25 | Loss: 0.00154311
Iteration 13/25 | Loss: 0.00154311
Iteration 14/25 | Loss: 0.00154311
Iteration 15/25 | Loss: 0.00154311
Iteration 16/25 | Loss: 0.00154311
Iteration 17/25 | Loss: 0.00154311
Iteration 18/25 | Loss: 0.00154311
Iteration 19/25 | Loss: 0.00154311
Iteration 20/25 | Loss: 0.00154311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001543111982755363, 0.001543111982755363, 0.001543111982755363, 0.001543111982755363, 0.001543111982755363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001543111982755363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154311
Iteration 2/1000 | Loss: 0.00004726
Iteration 3/1000 | Loss: 0.00003782
Iteration 4/1000 | Loss: 0.00003191
Iteration 5/1000 | Loss: 0.00002776
Iteration 6/1000 | Loss: 0.00002623
Iteration 7/1000 | Loss: 0.00002466
Iteration 8/1000 | Loss: 0.00002378
Iteration 9/1000 | Loss: 0.00002332
Iteration 10/1000 | Loss: 0.00002294
Iteration 11/1000 | Loss: 0.00002272
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002229
Iteration 15/1000 | Loss: 0.00002227
Iteration 16/1000 | Loss: 0.00002215
Iteration 17/1000 | Loss: 0.00002210
Iteration 18/1000 | Loss: 0.00002207
Iteration 19/1000 | Loss: 0.00002207
Iteration 20/1000 | Loss: 0.00002205
Iteration 21/1000 | Loss: 0.00002205
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002201
Iteration 25/1000 | Loss: 0.00002198
Iteration 26/1000 | Loss: 0.00002197
Iteration 27/1000 | Loss: 0.00002197
Iteration 28/1000 | Loss: 0.00002196
Iteration 29/1000 | Loss: 0.00002196
Iteration 30/1000 | Loss: 0.00002195
Iteration 31/1000 | Loss: 0.00002194
Iteration 32/1000 | Loss: 0.00002193
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00002193
Iteration 35/1000 | Loss: 0.00002192
Iteration 36/1000 | Loss: 0.00002191
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002189
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002188
Iteration 41/1000 | Loss: 0.00002188
Iteration 42/1000 | Loss: 0.00002188
Iteration 43/1000 | Loss: 0.00002187
Iteration 44/1000 | Loss: 0.00002187
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002186
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002185
Iteration 50/1000 | Loss: 0.00002185
Iteration 51/1000 | Loss: 0.00002185
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00002184
Iteration 54/1000 | Loss: 0.00002184
Iteration 55/1000 | Loss: 0.00002184
Iteration 56/1000 | Loss: 0.00002183
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002183
Iteration 59/1000 | Loss: 0.00002182
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002180
Iteration 71/1000 | Loss: 0.00002180
Iteration 72/1000 | Loss: 0.00002180
Iteration 73/1000 | Loss: 0.00002180
Iteration 74/1000 | Loss: 0.00002179
Iteration 75/1000 | Loss: 0.00002179
Iteration 76/1000 | Loss: 0.00002179
Iteration 77/1000 | Loss: 0.00002179
Iteration 78/1000 | Loss: 0.00002179
Iteration 79/1000 | Loss: 0.00002179
Iteration 80/1000 | Loss: 0.00002178
Iteration 81/1000 | Loss: 0.00002178
Iteration 82/1000 | Loss: 0.00002178
Iteration 83/1000 | Loss: 0.00002178
Iteration 84/1000 | Loss: 0.00002178
Iteration 85/1000 | Loss: 0.00002178
Iteration 86/1000 | Loss: 0.00002178
Iteration 87/1000 | Loss: 0.00002178
Iteration 88/1000 | Loss: 0.00002178
Iteration 89/1000 | Loss: 0.00002178
Iteration 90/1000 | Loss: 0.00002177
Iteration 91/1000 | Loss: 0.00002177
Iteration 92/1000 | Loss: 0.00002177
Iteration 93/1000 | Loss: 0.00002177
Iteration 94/1000 | Loss: 0.00002176
Iteration 95/1000 | Loss: 0.00002176
Iteration 96/1000 | Loss: 0.00002176
Iteration 97/1000 | Loss: 0.00002176
Iteration 98/1000 | Loss: 0.00002176
Iteration 99/1000 | Loss: 0.00002176
Iteration 100/1000 | Loss: 0.00002176
Iteration 101/1000 | Loss: 0.00002175
Iteration 102/1000 | Loss: 0.00002175
Iteration 103/1000 | Loss: 0.00002175
Iteration 104/1000 | Loss: 0.00002175
Iteration 105/1000 | Loss: 0.00002175
Iteration 106/1000 | Loss: 0.00002174
Iteration 107/1000 | Loss: 0.00002174
Iteration 108/1000 | Loss: 0.00002174
Iteration 109/1000 | Loss: 0.00002174
Iteration 110/1000 | Loss: 0.00002174
Iteration 111/1000 | Loss: 0.00002173
Iteration 112/1000 | Loss: 0.00002173
Iteration 113/1000 | Loss: 0.00002173
Iteration 114/1000 | Loss: 0.00002173
Iteration 115/1000 | Loss: 0.00002173
Iteration 116/1000 | Loss: 0.00002173
Iteration 117/1000 | Loss: 0.00002173
Iteration 118/1000 | Loss: 0.00002173
Iteration 119/1000 | Loss: 0.00002173
Iteration 120/1000 | Loss: 0.00002172
Iteration 121/1000 | Loss: 0.00002172
Iteration 122/1000 | Loss: 0.00002172
Iteration 123/1000 | Loss: 0.00002172
Iteration 124/1000 | Loss: 0.00002172
Iteration 125/1000 | Loss: 0.00002172
Iteration 126/1000 | Loss: 0.00002172
Iteration 127/1000 | Loss: 0.00002172
Iteration 128/1000 | Loss: 0.00002172
Iteration 129/1000 | Loss: 0.00002171
Iteration 130/1000 | Loss: 0.00002171
Iteration 131/1000 | Loss: 0.00002171
Iteration 132/1000 | Loss: 0.00002171
Iteration 133/1000 | Loss: 0.00002171
Iteration 134/1000 | Loss: 0.00002171
Iteration 135/1000 | Loss: 0.00002171
Iteration 136/1000 | Loss: 0.00002171
Iteration 137/1000 | Loss: 0.00002171
Iteration 138/1000 | Loss: 0.00002171
Iteration 139/1000 | Loss: 0.00002171
Iteration 140/1000 | Loss: 0.00002171
Iteration 141/1000 | Loss: 0.00002171
Iteration 142/1000 | Loss: 0.00002171
Iteration 143/1000 | Loss: 0.00002171
Iteration 144/1000 | Loss: 0.00002171
Iteration 145/1000 | Loss: 0.00002171
Iteration 146/1000 | Loss: 0.00002170
Iteration 147/1000 | Loss: 0.00002170
Iteration 148/1000 | Loss: 0.00002170
Iteration 149/1000 | Loss: 0.00002170
Iteration 150/1000 | Loss: 0.00002170
Iteration 151/1000 | Loss: 0.00002170
Iteration 152/1000 | Loss: 0.00002170
Iteration 153/1000 | Loss: 0.00002170
Iteration 154/1000 | Loss: 0.00002170
Iteration 155/1000 | Loss: 0.00002170
Iteration 156/1000 | Loss: 0.00002170
Iteration 157/1000 | Loss: 0.00002170
Iteration 158/1000 | Loss: 0.00002170
Iteration 159/1000 | Loss: 0.00002170
Iteration 160/1000 | Loss: 0.00002170
Iteration 161/1000 | Loss: 0.00002170
Iteration 162/1000 | Loss: 0.00002170
Iteration 163/1000 | Loss: 0.00002170
Iteration 164/1000 | Loss: 0.00002170
Iteration 165/1000 | Loss: 0.00002170
Iteration 166/1000 | Loss: 0.00002170
Iteration 167/1000 | Loss: 0.00002170
Iteration 168/1000 | Loss: 0.00002170
Iteration 169/1000 | Loss: 0.00002170
Iteration 170/1000 | Loss: 0.00002170
Iteration 171/1000 | Loss: 0.00002170
Iteration 172/1000 | Loss: 0.00002170
Iteration 173/1000 | Loss: 0.00002170
Iteration 174/1000 | Loss: 0.00002170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.1696880139643326e-05, 2.1696880139643326e-05, 2.1696880139643326e-05, 2.1696880139643326e-05, 2.1696880139643326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1696880139643326e-05

Optimization complete. Final v2v error: 3.794947385787964 mm

Highest mean error: 4.546664714813232 mm for frame 175

Lowest mean error: 3.455739974975586 mm for frame 163

Saving results

Total time: 45.33876633644104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831203
Iteration 2/25 | Loss: 0.00166177
Iteration 3/25 | Loss: 0.00095989
Iteration 4/25 | Loss: 0.00081582
Iteration 5/25 | Loss: 0.00079084
Iteration 6/25 | Loss: 0.00078824
Iteration 7/25 | Loss: 0.00078789
Iteration 8/25 | Loss: 0.00078789
Iteration 9/25 | Loss: 0.00078789
Iteration 10/25 | Loss: 0.00078789
Iteration 11/25 | Loss: 0.00078789
Iteration 12/25 | Loss: 0.00078789
Iteration 13/25 | Loss: 0.00078789
Iteration 14/25 | Loss: 0.00078789
Iteration 15/25 | Loss: 0.00078789
Iteration 16/25 | Loss: 0.00078789
Iteration 17/25 | Loss: 0.00078789
Iteration 18/25 | Loss: 0.00078789
Iteration 19/25 | Loss: 0.00078789
Iteration 20/25 | Loss: 0.00078789
Iteration 21/25 | Loss: 0.00078789
Iteration 22/25 | Loss: 0.00078789
Iteration 23/25 | Loss: 0.00078789
Iteration 24/25 | Loss: 0.00078789
Iteration 25/25 | Loss: 0.00078789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45577204
Iteration 2/25 | Loss: 0.00084829
Iteration 3/25 | Loss: 0.00084829
Iteration 4/25 | Loss: 0.00084829
Iteration 5/25 | Loss: 0.00084829
Iteration 6/25 | Loss: 0.00084829
Iteration 7/25 | Loss: 0.00084829
Iteration 8/25 | Loss: 0.00084829
Iteration 9/25 | Loss: 0.00084829
Iteration 10/25 | Loss: 0.00084829
Iteration 11/25 | Loss: 0.00084829
Iteration 12/25 | Loss: 0.00084829
Iteration 13/25 | Loss: 0.00084829
Iteration 14/25 | Loss: 0.00084829
Iteration 15/25 | Loss: 0.00084829
Iteration 16/25 | Loss: 0.00084829
Iteration 17/25 | Loss: 0.00084829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008482885314151645, 0.0008482885314151645, 0.0008482885314151645, 0.0008482885314151645, 0.0008482885314151645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008482885314151645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084829
Iteration 2/1000 | Loss: 0.00002868
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001972
Iteration 5/1000 | Loss: 0.00001895
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001763
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001732
Iteration 11/1000 | Loss: 0.00001726
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001706
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001698
Iteration 21/1000 | Loss: 0.00001698
Iteration 22/1000 | Loss: 0.00001698
Iteration 23/1000 | Loss: 0.00001698
Iteration 24/1000 | Loss: 0.00001698
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001696
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001694
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00001693
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001693
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001693
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001691
Iteration 67/1000 | Loss: 0.00001691
Iteration 68/1000 | Loss: 0.00001691
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001691
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.690715907898266e-05, 1.690715907898266e-05, 1.690715907898266e-05, 1.690715907898266e-05, 1.690715907898266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.690715907898266e-05

Optimization complete. Final v2v error: 3.41924786567688 mm

Highest mean error: 3.834245204925537 mm for frame 201

Lowest mean error: 3.1757309436798096 mm for frame 230

Saving results

Total time: 33.04618573188782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122388
Iteration 2/25 | Loss: 0.00495708
Iteration 3/25 | Loss: 0.00298748
Iteration 4/25 | Loss: 0.00269235
Iteration 5/25 | Loss: 0.00237095
Iteration 6/25 | Loss: 0.00215448
Iteration 7/25 | Loss: 0.00202749
Iteration 8/25 | Loss: 0.00195686
Iteration 9/25 | Loss: 0.00191132
Iteration 10/25 | Loss: 0.00188559
Iteration 11/25 | Loss: 0.00187663
Iteration 12/25 | Loss: 0.00187025
Iteration 13/25 | Loss: 0.00185758
Iteration 14/25 | Loss: 0.00184912
Iteration 15/25 | Loss: 0.00183068
Iteration 16/25 | Loss: 0.00179861
Iteration 17/25 | Loss: 0.00169875
Iteration 18/25 | Loss: 0.00164177
Iteration 19/25 | Loss: 0.00159273
Iteration 20/25 | Loss: 0.00157730
Iteration 21/25 | Loss: 0.00157036
Iteration 22/25 | Loss: 0.00156659
Iteration 23/25 | Loss: 0.00155215
Iteration 24/25 | Loss: 0.00154786
Iteration 25/25 | Loss: 0.00154054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.09820890
Iteration 2/25 | Loss: 0.00783031
Iteration 3/25 | Loss: 0.00660076
Iteration 4/25 | Loss: 0.00660076
Iteration 5/25 | Loss: 0.00660076
Iteration 6/25 | Loss: 0.00660076
Iteration 7/25 | Loss: 0.00660076
Iteration 8/25 | Loss: 0.00660076
Iteration 9/25 | Loss: 0.00660075
Iteration 10/25 | Loss: 0.00660075
Iteration 11/25 | Loss: 0.00660076
Iteration 12/25 | Loss: 0.00660075
Iteration 13/25 | Loss: 0.00660075
Iteration 14/25 | Loss: 0.00660075
Iteration 15/25 | Loss: 0.00660075
Iteration 16/25 | Loss: 0.00660075
Iteration 17/25 | Loss: 0.00660075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.006600754801183939, 0.006600754801183939, 0.006600754801183939, 0.006600754801183939, 0.006600754801183939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006600754801183939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00660075
Iteration 2/1000 | Loss: 0.00454221
Iteration 3/1000 | Loss: 0.00334033
Iteration 4/1000 | Loss: 0.00072172
Iteration 5/1000 | Loss: 0.00094728
Iteration 6/1000 | Loss: 0.00060017
Iteration 7/1000 | Loss: 0.00103928
Iteration 8/1000 | Loss: 0.00099261
Iteration 9/1000 | Loss: 0.00079185
Iteration 10/1000 | Loss: 0.00139375
Iteration 11/1000 | Loss: 0.00144111
Iteration 12/1000 | Loss: 0.00232896
Iteration 13/1000 | Loss: 0.00146370
Iteration 14/1000 | Loss: 0.00164454
Iteration 15/1000 | Loss: 0.00170636
Iteration 16/1000 | Loss: 0.00128733
Iteration 17/1000 | Loss: 0.00121833
Iteration 18/1000 | Loss: 0.00126881
Iteration 19/1000 | Loss: 0.00086081
Iteration 20/1000 | Loss: 0.00086800
Iteration 21/1000 | Loss: 0.00060024
Iteration 22/1000 | Loss: 0.00073129
Iteration 23/1000 | Loss: 0.00066034
Iteration 24/1000 | Loss: 0.00122758
Iteration 25/1000 | Loss: 0.00052309
Iteration 26/1000 | Loss: 0.00118179
Iteration 27/1000 | Loss: 0.00464775
Iteration 28/1000 | Loss: 0.00421048
Iteration 29/1000 | Loss: 0.00856372
Iteration 30/1000 | Loss: 0.00575647
Iteration 31/1000 | Loss: 0.00746611
Iteration 32/1000 | Loss: 0.00412689
Iteration 33/1000 | Loss: 0.00071200
Iteration 34/1000 | Loss: 0.00059432
Iteration 35/1000 | Loss: 0.00097166
Iteration 36/1000 | Loss: 0.00067097
Iteration 37/1000 | Loss: 0.00031360
Iteration 38/1000 | Loss: 0.00105266
Iteration 39/1000 | Loss: 0.00058550
Iteration 40/1000 | Loss: 0.00050249
Iteration 41/1000 | Loss: 0.00029943
Iteration 42/1000 | Loss: 0.00028619
Iteration 43/1000 | Loss: 0.00045438
Iteration 44/1000 | Loss: 0.00033820
Iteration 45/1000 | Loss: 0.00127799
Iteration 46/1000 | Loss: 0.00141057
Iteration 47/1000 | Loss: 0.00131230
Iteration 48/1000 | Loss: 0.00141162
Iteration 49/1000 | Loss: 0.00167172
Iteration 50/1000 | Loss: 0.00107262
Iteration 51/1000 | Loss: 0.00280419
Iteration 52/1000 | Loss: 0.00168262
Iteration 53/1000 | Loss: 0.00052501
Iteration 54/1000 | Loss: 0.00141183
Iteration 55/1000 | Loss: 0.00118309
Iteration 56/1000 | Loss: 0.00048733
Iteration 57/1000 | Loss: 0.00105841
Iteration 58/1000 | Loss: 0.00109359
Iteration 59/1000 | Loss: 0.00034543
Iteration 60/1000 | Loss: 0.00072777
Iteration 61/1000 | Loss: 0.00445645
Iteration 62/1000 | Loss: 0.00104700
Iteration 63/1000 | Loss: 0.00186816
Iteration 64/1000 | Loss: 0.00086544
Iteration 65/1000 | Loss: 0.00129325
Iteration 66/1000 | Loss: 0.00195330
Iteration 67/1000 | Loss: 0.00369909
Iteration 68/1000 | Loss: 0.00584590
Iteration 69/1000 | Loss: 0.00114820
Iteration 70/1000 | Loss: 0.00100382
Iteration 71/1000 | Loss: 0.00043836
Iteration 72/1000 | Loss: 0.00057672
Iteration 73/1000 | Loss: 0.00299873
Iteration 74/1000 | Loss: 0.00142201
Iteration 75/1000 | Loss: 0.00049920
Iteration 76/1000 | Loss: 0.00055356
Iteration 77/1000 | Loss: 0.00107905
Iteration 78/1000 | Loss: 0.00257987
Iteration 79/1000 | Loss: 0.00347562
Iteration 80/1000 | Loss: 0.00182896
Iteration 81/1000 | Loss: 0.00095690
Iteration 82/1000 | Loss: 0.00101267
Iteration 83/1000 | Loss: 0.00048902
Iteration 84/1000 | Loss: 0.00096031
Iteration 85/1000 | Loss: 0.00158344
Iteration 86/1000 | Loss: 0.00131312
Iteration 87/1000 | Loss: 0.00232941
Iteration 88/1000 | Loss: 0.00081658
Iteration 89/1000 | Loss: 0.00248440
Iteration 90/1000 | Loss: 0.00312068
Iteration 91/1000 | Loss: 0.00255769
Iteration 92/1000 | Loss: 0.00057121
Iteration 93/1000 | Loss: 0.00092143
Iteration 94/1000 | Loss: 0.00073719
Iteration 95/1000 | Loss: 0.00037461
Iteration 96/1000 | Loss: 0.00036409
Iteration 97/1000 | Loss: 0.00206477
Iteration 98/1000 | Loss: 0.00484949
Iteration 99/1000 | Loss: 0.00073217
Iteration 100/1000 | Loss: 0.00026444
Iteration 101/1000 | Loss: 0.00036918
Iteration 102/1000 | Loss: 0.00031486
Iteration 103/1000 | Loss: 0.00094359
Iteration 104/1000 | Loss: 0.00093709
Iteration 105/1000 | Loss: 0.00117055
Iteration 106/1000 | Loss: 0.00016805
Iteration 107/1000 | Loss: 0.00031766
Iteration 108/1000 | Loss: 0.00027448
Iteration 109/1000 | Loss: 0.00137035
Iteration 110/1000 | Loss: 0.00224585
Iteration 111/1000 | Loss: 0.00049419
Iteration 112/1000 | Loss: 0.00085480
Iteration 113/1000 | Loss: 0.00033441
Iteration 114/1000 | Loss: 0.00068954
Iteration 115/1000 | Loss: 0.00051519
Iteration 116/1000 | Loss: 0.00128422
Iteration 117/1000 | Loss: 0.00082807
Iteration 118/1000 | Loss: 0.00029080
Iteration 119/1000 | Loss: 0.00066174
Iteration 120/1000 | Loss: 0.00039434
Iteration 121/1000 | Loss: 0.00033677
Iteration 122/1000 | Loss: 0.00032996
Iteration 123/1000 | Loss: 0.00154600
Iteration 124/1000 | Loss: 0.00076903
Iteration 125/1000 | Loss: 0.00017628
Iteration 126/1000 | Loss: 0.00021646
Iteration 127/1000 | Loss: 0.00138566
Iteration 128/1000 | Loss: 0.00062198
Iteration 129/1000 | Loss: 0.00081550
Iteration 130/1000 | Loss: 0.00082164
Iteration 131/1000 | Loss: 0.00140021
Iteration 132/1000 | Loss: 0.00049798
Iteration 133/1000 | Loss: 0.00021747
Iteration 134/1000 | Loss: 0.00025958
Iteration 135/1000 | Loss: 0.00013318
Iteration 136/1000 | Loss: 0.00015572
Iteration 137/1000 | Loss: 0.00010371
Iteration 138/1000 | Loss: 0.00009728
Iteration 139/1000 | Loss: 0.00041087
Iteration 140/1000 | Loss: 0.00088167
Iteration 141/1000 | Loss: 0.00082875
Iteration 142/1000 | Loss: 0.00345428
Iteration 143/1000 | Loss: 0.00269335
Iteration 144/1000 | Loss: 0.00155098
Iteration 145/1000 | Loss: 0.00126336
Iteration 146/1000 | Loss: 0.00076589
Iteration 147/1000 | Loss: 0.00071877
Iteration 148/1000 | Loss: 0.00049563
Iteration 149/1000 | Loss: 0.00020977
Iteration 150/1000 | Loss: 0.00017054
Iteration 151/1000 | Loss: 0.00010743
Iteration 152/1000 | Loss: 0.00010568
Iteration 153/1000 | Loss: 0.00009323
Iteration 154/1000 | Loss: 0.00052121
Iteration 155/1000 | Loss: 0.00183629
Iteration 156/1000 | Loss: 0.00053883
Iteration 157/1000 | Loss: 0.00039965
Iteration 158/1000 | Loss: 0.00094445
Iteration 159/1000 | Loss: 0.00219106
Iteration 160/1000 | Loss: 0.00076743
Iteration 161/1000 | Loss: 0.00063730
Iteration 162/1000 | Loss: 0.00053396
Iteration 163/1000 | Loss: 0.00065522
Iteration 164/1000 | Loss: 0.00012008
Iteration 165/1000 | Loss: 0.00144062
Iteration 166/1000 | Loss: 0.00040911
Iteration 167/1000 | Loss: 0.00024316
Iteration 168/1000 | Loss: 0.00020462
Iteration 169/1000 | Loss: 0.00012020
Iteration 170/1000 | Loss: 0.00013053
Iteration 171/1000 | Loss: 0.00100438
Iteration 172/1000 | Loss: 0.00023807
Iteration 173/1000 | Loss: 0.00123929
Iteration 174/1000 | Loss: 0.00014745
Iteration 175/1000 | Loss: 0.00010519
Iteration 176/1000 | Loss: 0.00010557
Iteration 177/1000 | Loss: 0.00009513
Iteration 178/1000 | Loss: 0.00044509
Iteration 179/1000 | Loss: 0.00036019
Iteration 180/1000 | Loss: 0.00029207
Iteration 181/1000 | Loss: 0.00031217
Iteration 182/1000 | Loss: 0.00034992
Iteration 183/1000 | Loss: 0.00052280
Iteration 184/1000 | Loss: 0.00253238
Iteration 185/1000 | Loss: 0.00058611
Iteration 186/1000 | Loss: 0.00089005
Iteration 187/1000 | Loss: 0.00050388
Iteration 188/1000 | Loss: 0.00073789
Iteration 189/1000 | Loss: 0.00052713
Iteration 190/1000 | Loss: 0.00057450
Iteration 191/1000 | Loss: 0.00056834
Iteration 192/1000 | Loss: 0.00061369
Iteration 193/1000 | Loss: 0.00035239
Iteration 194/1000 | Loss: 0.00031713
Iteration 195/1000 | Loss: 0.00028027
Iteration 196/1000 | Loss: 0.00032210
Iteration 197/1000 | Loss: 0.00042738
Iteration 198/1000 | Loss: 0.00053674
Iteration 199/1000 | Loss: 0.00009219
Iteration 200/1000 | Loss: 0.00010258
Iteration 201/1000 | Loss: 0.00044539
Iteration 202/1000 | Loss: 0.00009860
Iteration 203/1000 | Loss: 0.00124265
Iteration 204/1000 | Loss: 0.00056591
Iteration 205/1000 | Loss: 0.00043897
Iteration 206/1000 | Loss: 0.00025982
Iteration 207/1000 | Loss: 0.00009148
Iteration 208/1000 | Loss: 0.00093758
Iteration 209/1000 | Loss: 0.00067456
Iteration 210/1000 | Loss: 0.00038803
Iteration 211/1000 | Loss: 0.00045133
Iteration 212/1000 | Loss: 0.00084878
Iteration 213/1000 | Loss: 0.00014674
Iteration 214/1000 | Loss: 0.00037416
Iteration 215/1000 | Loss: 0.00102697
Iteration 216/1000 | Loss: 0.00032118
Iteration 217/1000 | Loss: 0.00028683
Iteration 218/1000 | Loss: 0.00044433
Iteration 219/1000 | Loss: 0.00020698
Iteration 220/1000 | Loss: 0.00024091
Iteration 221/1000 | Loss: 0.00027930
Iteration 222/1000 | Loss: 0.00015494
Iteration 223/1000 | Loss: 0.00008601
Iteration 224/1000 | Loss: 0.00008422
Iteration 225/1000 | Loss: 0.00007739
Iteration 226/1000 | Loss: 0.00008206
Iteration 227/1000 | Loss: 0.00007161
Iteration 228/1000 | Loss: 0.00027559
Iteration 229/1000 | Loss: 0.00009876
Iteration 230/1000 | Loss: 0.00083730
Iteration 231/1000 | Loss: 0.00409256
Iteration 232/1000 | Loss: 0.00361552
Iteration 233/1000 | Loss: 0.00107489
Iteration 234/1000 | Loss: 0.00037650
Iteration 235/1000 | Loss: 0.00061275
Iteration 236/1000 | Loss: 0.00013457
Iteration 237/1000 | Loss: 0.00010717
Iteration 238/1000 | Loss: 0.00009840
Iteration 239/1000 | Loss: 0.00009498
Iteration 240/1000 | Loss: 0.00007551
Iteration 241/1000 | Loss: 0.00228867
Iteration 242/1000 | Loss: 0.00221546
Iteration 243/1000 | Loss: 0.00087095
Iteration 244/1000 | Loss: 0.00031570
Iteration 245/1000 | Loss: 0.00063373
Iteration 246/1000 | Loss: 0.00053609
Iteration 247/1000 | Loss: 0.00009724
Iteration 248/1000 | Loss: 0.00007727
Iteration 249/1000 | Loss: 0.00051457
Iteration 250/1000 | Loss: 0.00041037
Iteration 251/1000 | Loss: 0.00008573
Iteration 252/1000 | Loss: 0.00038861
Iteration 253/1000 | Loss: 0.00059358
Iteration 254/1000 | Loss: 0.00026875
Iteration 255/1000 | Loss: 0.00043320
Iteration 256/1000 | Loss: 0.00015903
Iteration 257/1000 | Loss: 0.00006836
Iteration 258/1000 | Loss: 0.00019669
Iteration 259/1000 | Loss: 0.00011076
Iteration 260/1000 | Loss: 0.00027682
Iteration 261/1000 | Loss: 0.00029187
Iteration 262/1000 | Loss: 0.00027082
Iteration 263/1000 | Loss: 0.00349740
Iteration 264/1000 | Loss: 0.00023783
Iteration 265/1000 | Loss: 0.00100710
Iteration 266/1000 | Loss: 0.00029973
Iteration 267/1000 | Loss: 0.00023988
Iteration 268/1000 | Loss: 0.00041540
Iteration 269/1000 | Loss: 0.00025615
Iteration 270/1000 | Loss: 0.00022061
Iteration 271/1000 | Loss: 0.00035030
Iteration 272/1000 | Loss: 0.00025157
Iteration 273/1000 | Loss: 0.00048360
Iteration 274/1000 | Loss: 0.00028610
Iteration 275/1000 | Loss: 0.00019814
Iteration 276/1000 | Loss: 0.00032693
Iteration 277/1000 | Loss: 0.00019406
Iteration 278/1000 | Loss: 0.00045424
Iteration 279/1000 | Loss: 0.00029890
Iteration 280/1000 | Loss: 0.00039941
Iteration 281/1000 | Loss: 0.00024228
Iteration 282/1000 | Loss: 0.00008399
Iteration 283/1000 | Loss: 0.00026101
Iteration 284/1000 | Loss: 0.00009622
Iteration 285/1000 | Loss: 0.00065202
Iteration 286/1000 | Loss: 0.00008718
Iteration 287/1000 | Loss: 0.00015285
Iteration 288/1000 | Loss: 0.00015996
Iteration 289/1000 | Loss: 0.00006881
Iteration 290/1000 | Loss: 0.00006506
Iteration 291/1000 | Loss: 0.00008897
Iteration 292/1000 | Loss: 0.00123181
Iteration 293/1000 | Loss: 0.00318611
Iteration 294/1000 | Loss: 0.00066672
Iteration 295/1000 | Loss: 0.00019770
Iteration 296/1000 | Loss: 0.00147302
Iteration 297/1000 | Loss: 0.00151825
Iteration 298/1000 | Loss: 0.00038367
Iteration 299/1000 | Loss: 0.00009458
Iteration 300/1000 | Loss: 0.00233669
Iteration 301/1000 | Loss: 0.00017090
Iteration 302/1000 | Loss: 0.00024825
Iteration 303/1000 | Loss: 0.00007939
Iteration 304/1000 | Loss: 0.00006530
Iteration 305/1000 | Loss: 0.00245710
Iteration 306/1000 | Loss: 0.00050511
Iteration 307/1000 | Loss: 0.00106727
Iteration 308/1000 | Loss: 0.00050727
Iteration 309/1000 | Loss: 0.00063071
Iteration 310/1000 | Loss: 0.00026635
Iteration 311/1000 | Loss: 0.00015769
Iteration 312/1000 | Loss: 0.00133617
Iteration 313/1000 | Loss: 0.00028715
Iteration 314/1000 | Loss: 0.00037170
Iteration 315/1000 | Loss: 0.00013303
Iteration 316/1000 | Loss: 0.00019842
Iteration 317/1000 | Loss: 0.00031916
Iteration 318/1000 | Loss: 0.00032580
Iteration 319/1000 | Loss: 0.00037759
Iteration 320/1000 | Loss: 0.00025877
Iteration 321/1000 | Loss: 0.00025768
Iteration 322/1000 | Loss: 0.00023651
Iteration 323/1000 | Loss: 0.00018607
Iteration 324/1000 | Loss: 0.00034597
Iteration 325/1000 | Loss: 0.00030406
Iteration 326/1000 | Loss: 0.00007449
Iteration 327/1000 | Loss: 0.00028661
Iteration 328/1000 | Loss: 0.00026582
Iteration 329/1000 | Loss: 0.00006107
Iteration 330/1000 | Loss: 0.00005786
Iteration 331/1000 | Loss: 0.00005558
Iteration 332/1000 | Loss: 0.00008731
Iteration 333/1000 | Loss: 0.00022892
Iteration 334/1000 | Loss: 0.00009222
Iteration 335/1000 | Loss: 0.00006087
Iteration 336/1000 | Loss: 0.00005730
Iteration 337/1000 | Loss: 0.00005509
Iteration 338/1000 | Loss: 0.00005359
Iteration 339/1000 | Loss: 0.00005252
Iteration 340/1000 | Loss: 0.00005185
Iteration 341/1000 | Loss: 0.00005150
Iteration 342/1000 | Loss: 0.00005128
Iteration 343/1000 | Loss: 0.00005113
Iteration 344/1000 | Loss: 0.00155704
Iteration 345/1000 | Loss: 0.00039757
Iteration 346/1000 | Loss: 0.00688383
Iteration 347/1000 | Loss: 0.00140559
Iteration 348/1000 | Loss: 0.00207642
Iteration 349/1000 | Loss: 0.00104218
Iteration 350/1000 | Loss: 0.00254553
Iteration 351/1000 | Loss: 0.00121553
Iteration 352/1000 | Loss: 0.00045613
Iteration 353/1000 | Loss: 0.00010526
Iteration 354/1000 | Loss: 0.00007603
Iteration 355/1000 | Loss: 0.00006614
Iteration 356/1000 | Loss: 0.00044109
Iteration 357/1000 | Loss: 0.00006067
Iteration 358/1000 | Loss: 0.00005818
Iteration 359/1000 | Loss: 0.00007507
Iteration 360/1000 | Loss: 0.00005965
Iteration 361/1000 | Loss: 0.00005719
Iteration 362/1000 | Loss: 0.00005553
Iteration 363/1000 | Loss: 0.00005450
Iteration 364/1000 | Loss: 0.00005371
Iteration 365/1000 | Loss: 0.00018592
Iteration 366/1000 | Loss: 0.00007013
Iteration 367/1000 | Loss: 0.00005407
Iteration 368/1000 | Loss: 0.00005154
Iteration 369/1000 | Loss: 0.00005037
Iteration 370/1000 | Loss: 0.00004965
Iteration 371/1000 | Loss: 0.00004904
Iteration 372/1000 | Loss: 0.00018263
Iteration 373/1000 | Loss: 0.00005063
Iteration 374/1000 | Loss: 0.00004824
Iteration 375/1000 | Loss: 0.00004731
Iteration 376/1000 | Loss: 0.00029400
Iteration 377/1000 | Loss: 0.00027320
Iteration 378/1000 | Loss: 0.00004976
Iteration 379/1000 | Loss: 0.00028847
Iteration 380/1000 | Loss: 0.00318269
Iteration 381/1000 | Loss: 0.00035934
Iteration 382/1000 | Loss: 0.00023767
Iteration 383/1000 | Loss: 0.00025960
Iteration 384/1000 | Loss: 0.00006900
Iteration 385/1000 | Loss: 0.00005648
Iteration 386/1000 | Loss: 0.00005140
Iteration 387/1000 | Loss: 0.00004926
Iteration 388/1000 | Loss: 0.00004805
Iteration 389/1000 | Loss: 0.00004758
Iteration 390/1000 | Loss: 0.00004724
Iteration 391/1000 | Loss: 0.00004694
Iteration 392/1000 | Loss: 0.00033573
Iteration 393/1000 | Loss: 0.00015296
Iteration 394/1000 | Loss: 0.00032125
Iteration 395/1000 | Loss: 0.00019470
Iteration 396/1000 | Loss: 0.00030064
Iteration 397/1000 | Loss: 0.00017574
Iteration 398/1000 | Loss: 0.00025338
Iteration 399/1000 | Loss: 0.00017827
Iteration 400/1000 | Loss: 0.00004943
Iteration 401/1000 | Loss: 0.00032554
Iteration 402/1000 | Loss: 0.00017828
Iteration 403/1000 | Loss: 0.00005110
Iteration 404/1000 | Loss: 0.00004964
Iteration 405/1000 | Loss: 0.00004860
Iteration 406/1000 | Loss: 0.00004783
Iteration 407/1000 | Loss: 0.00004722
Iteration 408/1000 | Loss: 0.00004675
Iteration 409/1000 | Loss: 0.00004643
Iteration 410/1000 | Loss: 0.00004624
Iteration 411/1000 | Loss: 0.00033142
Iteration 412/1000 | Loss: 0.00005207
Iteration 413/1000 | Loss: 0.00004885
Iteration 414/1000 | Loss: 0.00004700
Iteration 415/1000 | Loss: 0.00004629
Iteration 416/1000 | Loss: 0.00004595
Iteration 417/1000 | Loss: 0.00004577
Iteration 418/1000 | Loss: 0.00004567
Iteration 419/1000 | Loss: 0.00004547
Iteration 420/1000 | Loss: 0.00004528
Iteration 421/1000 | Loss: 0.00004521
Iteration 422/1000 | Loss: 0.00004519
Iteration 423/1000 | Loss: 0.00004519
Iteration 424/1000 | Loss: 0.00004518
Iteration 425/1000 | Loss: 0.00004518
Iteration 426/1000 | Loss: 0.00004517
Iteration 427/1000 | Loss: 0.00004517
Iteration 428/1000 | Loss: 0.00004516
Iteration 429/1000 | Loss: 0.00004514
Iteration 430/1000 | Loss: 0.00004513
Iteration 431/1000 | Loss: 0.00004513
Iteration 432/1000 | Loss: 0.00004512
Iteration 433/1000 | Loss: 0.00004512
Iteration 434/1000 | Loss: 0.00004512
Iteration 435/1000 | Loss: 0.00004511
Iteration 436/1000 | Loss: 0.00004511
Iteration 437/1000 | Loss: 0.00004511
Iteration 438/1000 | Loss: 0.00004511
Iteration 439/1000 | Loss: 0.00004511
Iteration 440/1000 | Loss: 0.00004510
Iteration 441/1000 | Loss: 0.00004510
Iteration 442/1000 | Loss: 0.00004510
Iteration 443/1000 | Loss: 0.00004510
Iteration 444/1000 | Loss: 0.00004510
Iteration 445/1000 | Loss: 0.00004510
Iteration 446/1000 | Loss: 0.00004509
Iteration 447/1000 | Loss: 0.00004509
Iteration 448/1000 | Loss: 0.00004509
Iteration 449/1000 | Loss: 0.00004509
Iteration 450/1000 | Loss: 0.00004509
Iteration 451/1000 | Loss: 0.00004509
Iteration 452/1000 | Loss: 0.00004509
Iteration 453/1000 | Loss: 0.00004509
Iteration 454/1000 | Loss: 0.00004509
Iteration 455/1000 | Loss: 0.00004508
Iteration 456/1000 | Loss: 0.00004508
Iteration 457/1000 | Loss: 0.00004508
Iteration 458/1000 | Loss: 0.00004508
Iteration 459/1000 | Loss: 0.00004508
Iteration 460/1000 | Loss: 0.00004508
Iteration 461/1000 | Loss: 0.00004508
Iteration 462/1000 | Loss: 0.00004508
Iteration 463/1000 | Loss: 0.00004508
Iteration 464/1000 | Loss: 0.00004508
Iteration 465/1000 | Loss: 0.00004507
Iteration 466/1000 | Loss: 0.00004507
Iteration 467/1000 | Loss: 0.00004507
Iteration 468/1000 | Loss: 0.00004507
Iteration 469/1000 | Loss: 0.00004506
Iteration 470/1000 | Loss: 0.00004506
Iteration 471/1000 | Loss: 0.00004506
Iteration 472/1000 | Loss: 0.00004506
Iteration 473/1000 | Loss: 0.00004506
Iteration 474/1000 | Loss: 0.00004506
Iteration 475/1000 | Loss: 0.00004506
Iteration 476/1000 | Loss: 0.00004506
Iteration 477/1000 | Loss: 0.00004505
Iteration 478/1000 | Loss: 0.00004505
Iteration 479/1000 | Loss: 0.00004505
Iteration 480/1000 | Loss: 0.00004505
Iteration 481/1000 | Loss: 0.00004504
Iteration 482/1000 | Loss: 0.00004503
Iteration 483/1000 | Loss: 0.00004502
Iteration 484/1000 | Loss: 0.00004501
Iteration 485/1000 | Loss: 0.00004501
Iteration 486/1000 | Loss: 0.00004501
Iteration 487/1000 | Loss: 0.00004500
Iteration 488/1000 | Loss: 0.00004500
Iteration 489/1000 | Loss: 0.00004500
Iteration 490/1000 | Loss: 0.00004499
Iteration 491/1000 | Loss: 0.00004499
Iteration 492/1000 | Loss: 0.00004499
Iteration 493/1000 | Loss: 0.00004499
Iteration 494/1000 | Loss: 0.00004499
Iteration 495/1000 | Loss: 0.00004499
Iteration 496/1000 | Loss: 0.00004499
Iteration 497/1000 | Loss: 0.00004498
Iteration 498/1000 | Loss: 0.00004498
Iteration 499/1000 | Loss: 0.00004498
Iteration 500/1000 | Loss: 0.00004498
Iteration 501/1000 | Loss: 0.00004497
Iteration 502/1000 | Loss: 0.00004497
Iteration 503/1000 | Loss: 0.00004497
Iteration 504/1000 | Loss: 0.00004497
Iteration 505/1000 | Loss: 0.00004497
Iteration 506/1000 | Loss: 0.00004497
Iteration 507/1000 | Loss: 0.00004497
Iteration 508/1000 | Loss: 0.00004497
Iteration 509/1000 | Loss: 0.00004497
Iteration 510/1000 | Loss: 0.00004497
Iteration 511/1000 | Loss: 0.00004496
Iteration 512/1000 | Loss: 0.00004496
Iteration 513/1000 | Loss: 0.00004496
Iteration 514/1000 | Loss: 0.00004496
Iteration 515/1000 | Loss: 0.00004496
Iteration 516/1000 | Loss: 0.00004496
Iteration 517/1000 | Loss: 0.00004496
Iteration 518/1000 | Loss: 0.00004496
Iteration 519/1000 | Loss: 0.00004496
Iteration 520/1000 | Loss: 0.00004496
Iteration 521/1000 | Loss: 0.00004496
Iteration 522/1000 | Loss: 0.00004496
Iteration 523/1000 | Loss: 0.00004496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 523. Stopping optimization.
Last 5 losses: [4.496289329836145e-05, 4.496289329836145e-05, 4.496289329836145e-05, 4.496289329836145e-05, 4.496289329836145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.496289329836145e-05

Optimization complete. Final v2v error: 4.644077777862549 mm

Highest mean error: 14.278687477111816 mm for frame 222

Lowest mean error: 3.361985206604004 mm for frame 187

Saving results

Total time: 734.2909638881683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978289
Iteration 2/25 | Loss: 0.00262523
Iteration 3/25 | Loss: 0.00201057
Iteration 4/25 | Loss: 0.00182066
Iteration 5/25 | Loss: 0.00154763
Iteration 6/25 | Loss: 0.00164329
Iteration 7/25 | Loss: 0.00176725
Iteration 8/25 | Loss: 0.00152638
Iteration 9/25 | Loss: 0.00124105
Iteration 10/25 | Loss: 0.00105942
Iteration 11/25 | Loss: 0.00094662
Iteration 12/25 | Loss: 0.00093402
Iteration 13/25 | Loss: 0.00092002
Iteration 14/25 | Loss: 0.00090962
Iteration 15/25 | Loss: 0.00090433
Iteration 16/25 | Loss: 0.00089462
Iteration 17/25 | Loss: 0.00088490
Iteration 18/25 | Loss: 0.00087598
Iteration 19/25 | Loss: 0.00087613
Iteration 20/25 | Loss: 0.00087218
Iteration 21/25 | Loss: 0.00087065
Iteration 22/25 | Loss: 0.00087196
Iteration 23/25 | Loss: 0.00087133
Iteration 24/25 | Loss: 0.00086970
Iteration 25/25 | Loss: 0.00086735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55696571
Iteration 2/25 | Loss: 0.00154778
Iteration 3/25 | Loss: 0.00154778
Iteration 4/25 | Loss: 0.00154778
Iteration 5/25 | Loss: 0.00154778
Iteration 6/25 | Loss: 0.00154778
Iteration 7/25 | Loss: 0.00154778
Iteration 8/25 | Loss: 0.00154778
Iteration 9/25 | Loss: 0.00154778
Iteration 10/25 | Loss: 0.00154778
Iteration 11/25 | Loss: 0.00154778
Iteration 12/25 | Loss: 0.00154778
Iteration 13/25 | Loss: 0.00154778
Iteration 14/25 | Loss: 0.00154778
Iteration 15/25 | Loss: 0.00154778
Iteration 16/25 | Loss: 0.00154778
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015477774431928992, 0.0015477774431928992, 0.0015477774431928992, 0.0015477774431928992, 0.0015477774431928992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015477774431928992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154778
Iteration 2/1000 | Loss: 0.00016072
Iteration 3/1000 | Loss: 0.00012088
Iteration 4/1000 | Loss: 0.00009959
Iteration 5/1000 | Loss: 0.00008433
Iteration 6/1000 | Loss: 0.00007473
Iteration 7/1000 | Loss: 0.00006956
Iteration 8/1000 | Loss: 0.00006761
Iteration 9/1000 | Loss: 0.00720710
Iteration 10/1000 | Loss: 0.00666767
Iteration 11/1000 | Loss: 0.00658307
Iteration 12/1000 | Loss: 0.00091188
Iteration 13/1000 | Loss: 0.00316550
Iteration 14/1000 | Loss: 0.00365893
Iteration 15/1000 | Loss: 0.00066418
Iteration 16/1000 | Loss: 0.00369410
Iteration 17/1000 | Loss: 0.00365683
Iteration 18/1000 | Loss: 0.00225787
Iteration 19/1000 | Loss: 0.00207792
Iteration 20/1000 | Loss: 0.00279234
Iteration 21/1000 | Loss: 0.00021983
Iteration 22/1000 | Loss: 0.00013163
Iteration 23/1000 | Loss: 0.00104381
Iteration 24/1000 | Loss: 0.00279121
Iteration 25/1000 | Loss: 0.00271243
Iteration 26/1000 | Loss: 0.00228610
Iteration 27/1000 | Loss: 0.00110888
Iteration 28/1000 | Loss: 0.00011500
Iteration 29/1000 | Loss: 0.00007697
Iteration 30/1000 | Loss: 0.00005784
Iteration 31/1000 | Loss: 0.00103653
Iteration 32/1000 | Loss: 0.00018838
Iteration 33/1000 | Loss: 0.00068842
Iteration 34/1000 | Loss: 0.00017967
Iteration 35/1000 | Loss: 0.00060398
Iteration 36/1000 | Loss: 0.00006202
Iteration 37/1000 | Loss: 0.00058538
Iteration 38/1000 | Loss: 0.00006683
Iteration 39/1000 | Loss: 0.00003886
Iteration 40/1000 | Loss: 0.00002892
Iteration 41/1000 | Loss: 0.00002424
Iteration 42/1000 | Loss: 0.00002241
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00001894
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001579
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001561
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001555
Iteration 69/1000 | Loss: 0.00001555
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001554
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001553
Iteration 82/1000 | Loss: 0.00001552
Iteration 83/1000 | Loss: 0.00001552
Iteration 84/1000 | Loss: 0.00001552
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001549
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00001549
Iteration 102/1000 | Loss: 0.00001549
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001548
Iteration 107/1000 | Loss: 0.00001548
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001548
Iteration 113/1000 | Loss: 0.00001548
Iteration 114/1000 | Loss: 0.00001548
Iteration 115/1000 | Loss: 0.00001548
Iteration 116/1000 | Loss: 0.00001547
Iteration 117/1000 | Loss: 0.00001547
Iteration 118/1000 | Loss: 0.00001547
Iteration 119/1000 | Loss: 0.00001547
Iteration 120/1000 | Loss: 0.00001547
Iteration 121/1000 | Loss: 0.00001547
Iteration 122/1000 | Loss: 0.00001547
Iteration 123/1000 | Loss: 0.00001547
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001546
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001545
Iteration 143/1000 | Loss: 0.00001545
Iteration 144/1000 | Loss: 0.00001545
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001545
Iteration 150/1000 | Loss: 0.00001545
Iteration 151/1000 | Loss: 0.00001545
Iteration 152/1000 | Loss: 0.00001545
Iteration 153/1000 | Loss: 0.00001545
Iteration 154/1000 | Loss: 0.00001544
Iteration 155/1000 | Loss: 0.00001544
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001543
Iteration 164/1000 | Loss: 0.00001543
Iteration 165/1000 | Loss: 0.00001543
Iteration 166/1000 | Loss: 0.00001543
Iteration 167/1000 | Loss: 0.00001543
Iteration 168/1000 | Loss: 0.00001543
Iteration 169/1000 | Loss: 0.00001543
Iteration 170/1000 | Loss: 0.00001543
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001543
Iteration 174/1000 | Loss: 0.00001543
Iteration 175/1000 | Loss: 0.00001543
Iteration 176/1000 | Loss: 0.00001543
Iteration 177/1000 | Loss: 0.00001543
Iteration 178/1000 | Loss: 0.00001543
Iteration 179/1000 | Loss: 0.00001543
Iteration 180/1000 | Loss: 0.00001543
Iteration 181/1000 | Loss: 0.00001543
Iteration 182/1000 | Loss: 0.00001543
Iteration 183/1000 | Loss: 0.00001543
Iteration 184/1000 | Loss: 0.00001543
Iteration 185/1000 | Loss: 0.00001543
Iteration 186/1000 | Loss: 0.00001543
Iteration 187/1000 | Loss: 0.00001543
Iteration 188/1000 | Loss: 0.00001543
Iteration 189/1000 | Loss: 0.00001543
Iteration 190/1000 | Loss: 0.00001543
Iteration 191/1000 | Loss: 0.00001543
Iteration 192/1000 | Loss: 0.00001543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.5427776816068217e-05, 1.5427776816068217e-05, 1.5427776816068217e-05, 1.5427776816068217e-05, 1.5427776816068217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5427776816068217e-05

Optimization complete. Final v2v error: 3.372051239013672 mm

Highest mean error: 3.579880714416504 mm for frame 3

Lowest mean error: 3.1907291412353516 mm for frame 16

Saving results

Total time: 125.32263827323914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865953
Iteration 2/25 | Loss: 0.00086260
Iteration 3/25 | Loss: 0.00073664
Iteration 4/25 | Loss: 0.00072000
Iteration 5/25 | Loss: 0.00071531
Iteration 6/25 | Loss: 0.00071425
Iteration 7/25 | Loss: 0.00071405
Iteration 8/25 | Loss: 0.00071405
Iteration 9/25 | Loss: 0.00071405
Iteration 10/25 | Loss: 0.00071405
Iteration 11/25 | Loss: 0.00071405
Iteration 12/25 | Loss: 0.00071405
Iteration 13/25 | Loss: 0.00071405
Iteration 14/25 | Loss: 0.00071405
Iteration 15/25 | Loss: 0.00071405
Iteration 16/25 | Loss: 0.00071405
Iteration 17/25 | Loss: 0.00071405
Iteration 18/25 | Loss: 0.00071405
Iteration 19/25 | Loss: 0.00071405
Iteration 20/25 | Loss: 0.00071405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007140464731492102, 0.0007140464731492102, 0.0007140464731492102, 0.0007140464731492102, 0.0007140464731492102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007140464731492102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75342143
Iteration 2/25 | Loss: 0.00082046
Iteration 3/25 | Loss: 0.00082045
Iteration 4/25 | Loss: 0.00082045
Iteration 5/25 | Loss: 0.00082045
Iteration 6/25 | Loss: 0.00082045
Iteration 7/25 | Loss: 0.00082045
Iteration 8/25 | Loss: 0.00082045
Iteration 9/25 | Loss: 0.00082045
Iteration 10/25 | Loss: 0.00082045
Iteration 11/25 | Loss: 0.00082045
Iteration 12/25 | Loss: 0.00082045
Iteration 13/25 | Loss: 0.00082045
Iteration 14/25 | Loss: 0.00082045
Iteration 15/25 | Loss: 0.00082045
Iteration 16/25 | Loss: 0.00082045
Iteration 17/25 | Loss: 0.00082045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000820451823528856, 0.000820451823528856, 0.000820451823528856, 0.000820451823528856, 0.000820451823528856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000820451823528856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082045
Iteration 2/1000 | Loss: 0.00002326
Iteration 3/1000 | Loss: 0.00001455
Iteration 4/1000 | Loss: 0.00001353
Iteration 5/1000 | Loss: 0.00001301
Iteration 6/1000 | Loss: 0.00001262
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001219
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001212
Iteration 11/1000 | Loss: 0.00001206
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001204
Iteration 14/1000 | Loss: 0.00001203
Iteration 15/1000 | Loss: 0.00001197
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001193
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001189
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001188
Iteration 28/1000 | Loss: 0.00001188
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001188
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001185
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001184
Iteration 40/1000 | Loss: 0.00001184
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001182
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001180
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001176
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001165
Iteration 112/1000 | Loss: 0.00001164
Iteration 113/1000 | Loss: 0.00001164
Iteration 114/1000 | Loss: 0.00001164
Iteration 115/1000 | Loss: 0.00001163
Iteration 116/1000 | Loss: 0.00001163
Iteration 117/1000 | Loss: 0.00001163
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001162
Iteration 120/1000 | Loss: 0.00001162
Iteration 121/1000 | Loss: 0.00001162
Iteration 122/1000 | Loss: 0.00001162
Iteration 123/1000 | Loss: 0.00001162
Iteration 124/1000 | Loss: 0.00001162
Iteration 125/1000 | Loss: 0.00001162
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001161
Iteration 128/1000 | Loss: 0.00001161
Iteration 129/1000 | Loss: 0.00001161
Iteration 130/1000 | Loss: 0.00001161
Iteration 131/1000 | Loss: 0.00001161
Iteration 132/1000 | Loss: 0.00001161
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001161
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001161
Iteration 137/1000 | Loss: 0.00001161
Iteration 138/1000 | Loss: 0.00001161
Iteration 139/1000 | Loss: 0.00001161
Iteration 140/1000 | Loss: 0.00001161
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Iteration 143/1000 | Loss: 0.00001161
Iteration 144/1000 | Loss: 0.00001161
Iteration 145/1000 | Loss: 0.00001161
Iteration 146/1000 | Loss: 0.00001161
Iteration 147/1000 | Loss: 0.00001161
Iteration 148/1000 | Loss: 0.00001161
Iteration 149/1000 | Loss: 0.00001160
Iteration 150/1000 | Loss: 0.00001160
Iteration 151/1000 | Loss: 0.00001160
Iteration 152/1000 | Loss: 0.00001160
Iteration 153/1000 | Loss: 0.00001160
Iteration 154/1000 | Loss: 0.00001160
Iteration 155/1000 | Loss: 0.00001160
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1603979146457277e-05, 1.1603979146457277e-05, 1.1603979146457277e-05, 1.1603979146457277e-05, 1.1603979146457277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1603979146457277e-05

Optimization complete. Final v2v error: 2.899149179458618 mm

Highest mean error: 3.288266658782959 mm for frame 67

Lowest mean error: 2.7756235599517822 mm for frame 115

Saving results

Total time: 34.31477189064026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862323
Iteration 2/25 | Loss: 0.00167995
Iteration 3/25 | Loss: 0.00114557
Iteration 4/25 | Loss: 0.00112652
Iteration 5/25 | Loss: 0.00112106
Iteration 6/25 | Loss: 0.00112019
Iteration 7/25 | Loss: 0.00112019
Iteration 8/25 | Loss: 0.00112019
Iteration 9/25 | Loss: 0.00112019
Iteration 10/25 | Loss: 0.00112019
Iteration 11/25 | Loss: 0.00112019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001120191067457199, 0.001120191067457199, 0.001120191067457199, 0.001120191067457199, 0.001120191067457199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001120191067457199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39862794
Iteration 2/25 | Loss: 0.00064985
Iteration 3/25 | Loss: 0.00064985
Iteration 4/25 | Loss: 0.00064985
Iteration 5/25 | Loss: 0.00064985
Iteration 6/25 | Loss: 0.00064985
Iteration 7/25 | Loss: 0.00064985
Iteration 8/25 | Loss: 0.00064984
Iteration 9/25 | Loss: 0.00064984
Iteration 10/25 | Loss: 0.00064984
Iteration 11/25 | Loss: 0.00064984
Iteration 12/25 | Loss: 0.00064984
Iteration 13/25 | Loss: 0.00064984
Iteration 14/25 | Loss: 0.00064984
Iteration 15/25 | Loss: 0.00064984
Iteration 16/25 | Loss: 0.00064984
Iteration 17/25 | Loss: 0.00064984
Iteration 18/25 | Loss: 0.00064984
Iteration 19/25 | Loss: 0.00064984
Iteration 20/25 | Loss: 0.00064984
Iteration 21/25 | Loss: 0.00064984
Iteration 22/25 | Loss: 0.00064984
Iteration 23/25 | Loss: 0.00064984
Iteration 24/25 | Loss: 0.00064984
Iteration 25/25 | Loss: 0.00064984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064984
Iteration 2/1000 | Loss: 0.00007496
Iteration 3/1000 | Loss: 0.00005691
Iteration 4/1000 | Loss: 0.00004879
Iteration 5/1000 | Loss: 0.00004635
Iteration 6/1000 | Loss: 0.00004496
Iteration 7/1000 | Loss: 0.00004407
Iteration 8/1000 | Loss: 0.00004337
Iteration 9/1000 | Loss: 0.00004290
Iteration 10/1000 | Loss: 0.00004257
Iteration 11/1000 | Loss: 0.00004201
Iteration 12/1000 | Loss: 0.00004146
Iteration 13/1000 | Loss: 0.00004106
Iteration 14/1000 | Loss: 0.00004084
Iteration 15/1000 | Loss: 0.00004060
Iteration 16/1000 | Loss: 0.00004044
Iteration 17/1000 | Loss: 0.00004034
Iteration 18/1000 | Loss: 0.00004031
Iteration 19/1000 | Loss: 0.00004019
Iteration 20/1000 | Loss: 0.00004013
Iteration 21/1000 | Loss: 0.00004013
Iteration 22/1000 | Loss: 0.00004013
Iteration 23/1000 | Loss: 0.00004011
Iteration 24/1000 | Loss: 0.00004010
Iteration 25/1000 | Loss: 0.00004007
Iteration 26/1000 | Loss: 0.00004006
Iteration 27/1000 | Loss: 0.00004006
Iteration 28/1000 | Loss: 0.00004006
Iteration 29/1000 | Loss: 0.00004005
Iteration 30/1000 | Loss: 0.00004005
Iteration 31/1000 | Loss: 0.00004005
Iteration 32/1000 | Loss: 0.00004005
Iteration 33/1000 | Loss: 0.00004004
Iteration 34/1000 | Loss: 0.00004004
Iteration 35/1000 | Loss: 0.00004004
Iteration 36/1000 | Loss: 0.00004004
Iteration 37/1000 | Loss: 0.00004004
Iteration 38/1000 | Loss: 0.00004004
Iteration 39/1000 | Loss: 0.00004003
Iteration 40/1000 | Loss: 0.00004003
Iteration 41/1000 | Loss: 0.00004002
Iteration 42/1000 | Loss: 0.00004002
Iteration 43/1000 | Loss: 0.00004002
Iteration 44/1000 | Loss: 0.00004002
Iteration 45/1000 | Loss: 0.00004001
Iteration 46/1000 | Loss: 0.00004001
Iteration 47/1000 | Loss: 0.00004001
Iteration 48/1000 | Loss: 0.00004001
Iteration 49/1000 | Loss: 0.00004001
Iteration 50/1000 | Loss: 0.00004000
Iteration 51/1000 | Loss: 0.00004000
Iteration 52/1000 | Loss: 0.00004000
Iteration 53/1000 | Loss: 0.00004000
Iteration 54/1000 | Loss: 0.00004000
Iteration 55/1000 | Loss: 0.00004000
Iteration 56/1000 | Loss: 0.00004000
Iteration 57/1000 | Loss: 0.00004000
Iteration 58/1000 | Loss: 0.00004000
Iteration 59/1000 | Loss: 0.00003999
Iteration 60/1000 | Loss: 0.00003999
Iteration 61/1000 | Loss: 0.00003998
Iteration 62/1000 | Loss: 0.00003998
Iteration 63/1000 | Loss: 0.00003998
Iteration 64/1000 | Loss: 0.00003998
Iteration 65/1000 | Loss: 0.00003998
Iteration 66/1000 | Loss: 0.00003998
Iteration 67/1000 | Loss: 0.00003998
Iteration 68/1000 | Loss: 0.00003998
Iteration 69/1000 | Loss: 0.00003998
Iteration 70/1000 | Loss: 0.00003997
Iteration 71/1000 | Loss: 0.00003997
Iteration 72/1000 | Loss: 0.00003997
Iteration 73/1000 | Loss: 0.00003996
Iteration 74/1000 | Loss: 0.00003996
Iteration 75/1000 | Loss: 0.00003996
Iteration 76/1000 | Loss: 0.00003996
Iteration 77/1000 | Loss: 0.00003996
Iteration 78/1000 | Loss: 0.00003996
Iteration 79/1000 | Loss: 0.00003996
Iteration 80/1000 | Loss: 0.00003996
Iteration 81/1000 | Loss: 0.00003996
Iteration 82/1000 | Loss: 0.00003996
Iteration 83/1000 | Loss: 0.00003996
Iteration 84/1000 | Loss: 0.00003995
Iteration 85/1000 | Loss: 0.00003995
Iteration 86/1000 | Loss: 0.00003995
Iteration 87/1000 | Loss: 0.00003995
Iteration 88/1000 | Loss: 0.00003995
Iteration 89/1000 | Loss: 0.00003995
Iteration 90/1000 | Loss: 0.00003995
Iteration 91/1000 | Loss: 0.00003995
Iteration 92/1000 | Loss: 0.00003995
Iteration 93/1000 | Loss: 0.00003995
Iteration 94/1000 | Loss: 0.00003995
Iteration 95/1000 | Loss: 0.00003995
Iteration 96/1000 | Loss: 0.00003995
Iteration 97/1000 | Loss: 0.00003995
Iteration 98/1000 | Loss: 0.00003995
Iteration 99/1000 | Loss: 0.00003995
Iteration 100/1000 | Loss: 0.00003995
Iteration 101/1000 | Loss: 0.00003995
Iteration 102/1000 | Loss: 0.00003995
Iteration 103/1000 | Loss: 0.00003995
Iteration 104/1000 | Loss: 0.00003995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [3.995075530838221e-05, 3.995075530838221e-05, 3.995075530838221e-05, 3.995075530838221e-05, 3.995075530838221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.995075530838221e-05

Optimization complete. Final v2v error: 4.879762649536133 mm

Highest mean error: 5.0832839012146 mm for frame 114

Lowest mean error: 4.6805291175842285 mm for frame 131

Saving results

Total time: 41.79183650016785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111775
Iteration 2/25 | Loss: 0.00201694
Iteration 3/25 | Loss: 0.00115844
Iteration 4/25 | Loss: 0.00097902
Iteration 5/25 | Loss: 0.00090837
Iteration 6/25 | Loss: 0.00089104
Iteration 7/25 | Loss: 0.00087368
Iteration 8/25 | Loss: 0.00085205
Iteration 9/25 | Loss: 0.00083110
Iteration 10/25 | Loss: 0.00082894
Iteration 11/25 | Loss: 0.00080762
Iteration 12/25 | Loss: 0.00080044
Iteration 13/25 | Loss: 0.00079933
Iteration 14/25 | Loss: 0.00079378
Iteration 15/25 | Loss: 0.00079300
Iteration 16/25 | Loss: 0.00079439
Iteration 17/25 | Loss: 0.00079133
Iteration 18/25 | Loss: 0.00078979
Iteration 19/25 | Loss: 0.00078556
Iteration 20/25 | Loss: 0.00078038
Iteration 21/25 | Loss: 0.00077782
Iteration 22/25 | Loss: 0.00078055
Iteration 23/25 | Loss: 0.00077742
Iteration 24/25 | Loss: 0.00077871
Iteration 25/25 | Loss: 0.00078773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.73086882
Iteration 2/25 | Loss: 0.00121036
Iteration 3/25 | Loss: 0.00111547
Iteration 4/25 | Loss: 0.00111547
Iteration 5/25 | Loss: 0.00111547
Iteration 6/25 | Loss: 0.00111547
Iteration 7/25 | Loss: 0.00111547
Iteration 8/25 | Loss: 0.00111547
Iteration 9/25 | Loss: 0.00111547
Iteration 10/25 | Loss: 0.00111547
Iteration 11/25 | Loss: 0.00111547
Iteration 12/25 | Loss: 0.00111547
Iteration 13/25 | Loss: 0.00111547
Iteration 14/25 | Loss: 0.00111547
Iteration 15/25 | Loss: 0.00111547
Iteration 16/25 | Loss: 0.00111547
Iteration 17/25 | Loss: 0.00111547
Iteration 18/25 | Loss: 0.00111547
Iteration 19/25 | Loss: 0.00111547
Iteration 20/25 | Loss: 0.00111547
Iteration 21/25 | Loss: 0.00111547
Iteration 22/25 | Loss: 0.00111547
Iteration 23/25 | Loss: 0.00111547
Iteration 24/25 | Loss: 0.00111547
Iteration 25/25 | Loss: 0.00111547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111547
Iteration 2/1000 | Loss: 0.00067246
Iteration 3/1000 | Loss: 0.00016014
Iteration 4/1000 | Loss: 0.00039024
Iteration 5/1000 | Loss: 0.00033695
Iteration 6/1000 | Loss: 0.00024426
Iteration 7/1000 | Loss: 0.00005753
Iteration 8/1000 | Loss: 0.00033275
Iteration 9/1000 | Loss: 0.00065602
Iteration 10/1000 | Loss: 0.00032058
Iteration 11/1000 | Loss: 0.00034950
Iteration 12/1000 | Loss: 0.00032182
Iteration 13/1000 | Loss: 0.00074915
Iteration 14/1000 | Loss: 0.00073981
Iteration 15/1000 | Loss: 0.00045583
Iteration 16/1000 | Loss: 0.00022353
Iteration 17/1000 | Loss: 0.00019552
Iteration 18/1000 | Loss: 0.00017716
Iteration 19/1000 | Loss: 0.00009902
Iteration 20/1000 | Loss: 0.00013011
Iteration 21/1000 | Loss: 0.00023063
Iteration 22/1000 | Loss: 0.00009509
Iteration 23/1000 | Loss: 0.00022161
Iteration 24/1000 | Loss: 0.00021516
Iteration 25/1000 | Loss: 0.00023974
Iteration 26/1000 | Loss: 0.00024368
Iteration 27/1000 | Loss: 0.00017421
Iteration 28/1000 | Loss: 0.00014604
Iteration 29/1000 | Loss: 0.00003909
Iteration 30/1000 | Loss: 0.00017956
Iteration 31/1000 | Loss: 0.00018765
Iteration 32/1000 | Loss: 0.00076309
Iteration 33/1000 | Loss: 0.00081948
Iteration 34/1000 | Loss: 0.00052357
Iteration 35/1000 | Loss: 0.00023216
Iteration 36/1000 | Loss: 0.00063593
Iteration 37/1000 | Loss: 0.00020377
Iteration 38/1000 | Loss: 0.00007087
Iteration 39/1000 | Loss: 0.00006612
Iteration 40/1000 | Loss: 0.00004184
Iteration 41/1000 | Loss: 0.00028526
Iteration 42/1000 | Loss: 0.00021540
Iteration 43/1000 | Loss: 0.00046874
Iteration 44/1000 | Loss: 0.00043872
Iteration 45/1000 | Loss: 0.00058504
Iteration 46/1000 | Loss: 0.00015244
Iteration 47/1000 | Loss: 0.00034228
Iteration 48/1000 | Loss: 0.00011653
Iteration 49/1000 | Loss: 0.00006291
Iteration 50/1000 | Loss: 0.00004466
Iteration 51/1000 | Loss: 0.00004178
Iteration 52/1000 | Loss: 0.00017300
Iteration 53/1000 | Loss: 0.00025922
Iteration 54/1000 | Loss: 0.00038874
Iteration 55/1000 | Loss: 0.00029011
Iteration 56/1000 | Loss: 0.00016334
Iteration 57/1000 | Loss: 0.00010103
Iteration 58/1000 | Loss: 0.00003653
Iteration 59/1000 | Loss: 0.00003707
Iteration 60/1000 | Loss: 0.00002988
Iteration 61/1000 | Loss: 0.00020516
Iteration 62/1000 | Loss: 0.00012446
Iteration 63/1000 | Loss: 0.00009872
Iteration 64/1000 | Loss: 0.00009616
Iteration 65/1000 | Loss: 0.00025517
Iteration 66/1000 | Loss: 0.00058481
Iteration 67/1000 | Loss: 0.00042272
Iteration 68/1000 | Loss: 0.00016880
Iteration 69/1000 | Loss: 0.00011672
Iteration 70/1000 | Loss: 0.00003321
Iteration 71/1000 | Loss: 0.00022002
Iteration 72/1000 | Loss: 0.00033462
Iteration 73/1000 | Loss: 0.00012888
Iteration 74/1000 | Loss: 0.00011508
Iteration 75/1000 | Loss: 0.00003820
Iteration 76/1000 | Loss: 0.00004052
Iteration 77/1000 | Loss: 0.00003830
Iteration 78/1000 | Loss: 0.00046656
Iteration 79/1000 | Loss: 0.00003983
Iteration 80/1000 | Loss: 0.00004326
Iteration 81/1000 | Loss: 0.00002883
Iteration 82/1000 | Loss: 0.00018064
Iteration 83/1000 | Loss: 0.00024105
Iteration 84/1000 | Loss: 0.00017592
Iteration 85/1000 | Loss: 0.00023848
Iteration 86/1000 | Loss: 0.00003626
Iteration 87/1000 | Loss: 0.00002927
Iteration 88/1000 | Loss: 0.00003450
Iteration 89/1000 | Loss: 0.00002713
Iteration 90/1000 | Loss: 0.00002856
Iteration 91/1000 | Loss: 0.00002541
Iteration 92/1000 | Loss: 0.00003215
Iteration 93/1000 | Loss: 0.00003166
Iteration 94/1000 | Loss: 0.00003203
Iteration 95/1000 | Loss: 0.00003149
Iteration 96/1000 | Loss: 0.00003040
Iteration 97/1000 | Loss: 0.00003091
Iteration 98/1000 | Loss: 0.00002971
Iteration 99/1000 | Loss: 0.00003142
Iteration 100/1000 | Loss: 0.00003108
Iteration 101/1000 | Loss: 0.00003147
Iteration 102/1000 | Loss: 0.00003161
Iteration 103/1000 | Loss: 0.00003144
Iteration 104/1000 | Loss: 0.00003287
Iteration 105/1000 | Loss: 0.00002941
Iteration 106/1000 | Loss: 0.00002705
Iteration 107/1000 | Loss: 0.00003186
Iteration 108/1000 | Loss: 0.00003234
Iteration 109/1000 | Loss: 0.00003127
Iteration 110/1000 | Loss: 0.00003467
Iteration 111/1000 | Loss: 0.00003167
Iteration 112/1000 | Loss: 0.00003224
Iteration 113/1000 | Loss: 0.00003561
Iteration 114/1000 | Loss: 0.00003085
Iteration 115/1000 | Loss: 0.00003111
Iteration 116/1000 | Loss: 0.00003100
Iteration 117/1000 | Loss: 0.00003205
Iteration 118/1000 | Loss: 0.00003339
Iteration 119/1000 | Loss: 0.00002978
Iteration 120/1000 | Loss: 0.00003093
Iteration 121/1000 | Loss: 0.00003071
Iteration 122/1000 | Loss: 0.00003034
Iteration 123/1000 | Loss: 0.00003091
Iteration 124/1000 | Loss: 0.00003020
Iteration 125/1000 | Loss: 0.00003016
Iteration 126/1000 | Loss: 0.00003047
Iteration 127/1000 | Loss: 0.00002930
Iteration 128/1000 | Loss: 0.00002746
Iteration 129/1000 | Loss: 0.00002970
Iteration 130/1000 | Loss: 0.00003010
Iteration 131/1000 | Loss: 0.00002676
Iteration 132/1000 | Loss: 0.00003147
Iteration 133/1000 | Loss: 0.00003424
Iteration 134/1000 | Loss: 0.00002573
Iteration 135/1000 | Loss: 0.00003013
Iteration 136/1000 | Loss: 0.00003030
Iteration 137/1000 | Loss: 0.00003149
Iteration 138/1000 | Loss: 0.00003050
Iteration 139/1000 | Loss: 0.00003094
Iteration 140/1000 | Loss: 0.00003093
Iteration 141/1000 | Loss: 0.00003309
Iteration 142/1000 | Loss: 0.00003048
Iteration 143/1000 | Loss: 0.00003276
Iteration 144/1000 | Loss: 0.00003083
Iteration 145/1000 | Loss: 0.00003056
Iteration 146/1000 | Loss: 0.00003289
Iteration 147/1000 | Loss: 0.00003122
Iteration 148/1000 | Loss: 0.00003055
Iteration 149/1000 | Loss: 0.00003376
Iteration 150/1000 | Loss: 0.00003376
Iteration 151/1000 | Loss: 0.00003276
Iteration 152/1000 | Loss: 0.00003211
Iteration 153/1000 | Loss: 0.00003132
Iteration 154/1000 | Loss: 0.00003304
Iteration 155/1000 | Loss: 0.00003694
Iteration 156/1000 | Loss: 0.00003515
Iteration 157/1000 | Loss: 0.00003450
Iteration 158/1000 | Loss: 0.00003563
Iteration 159/1000 | Loss: 0.00002996
Iteration 160/1000 | Loss: 0.00003145
Iteration 161/1000 | Loss: 0.00002670
Iteration 162/1000 | Loss: 0.00003087
Iteration 163/1000 | Loss: 0.00003522
Iteration 164/1000 | Loss: 0.00004259
Iteration 165/1000 | Loss: 0.00004007
Iteration 166/1000 | Loss: 0.00003977
Iteration 167/1000 | Loss: 0.00003126
Iteration 168/1000 | Loss: 0.00003098
Iteration 169/1000 | Loss: 0.00003126
Iteration 170/1000 | Loss: 0.00002882
Iteration 171/1000 | Loss: 0.00003417
Iteration 172/1000 | Loss: 0.00003222
Iteration 173/1000 | Loss: 0.00002452
Iteration 174/1000 | Loss: 0.00002766
Iteration 175/1000 | Loss: 0.00003042
Iteration 176/1000 | Loss: 0.00003074
Iteration 177/1000 | Loss: 0.00003047
Iteration 178/1000 | Loss: 0.00002865
Iteration 179/1000 | Loss: 0.00003662
Iteration 180/1000 | Loss: 0.00002983
Iteration 181/1000 | Loss: 0.00003260
Iteration 182/1000 | Loss: 0.00003102
Iteration 183/1000 | Loss: 0.00003027
Iteration 184/1000 | Loss: 0.00002900
Iteration 185/1000 | Loss: 0.00003065
Iteration 186/1000 | Loss: 0.00002297
Iteration 187/1000 | Loss: 0.00002918
Iteration 188/1000 | Loss: 0.00002977
Iteration 189/1000 | Loss: 0.00003142
Iteration 190/1000 | Loss: 0.00003068
Iteration 191/1000 | Loss: 0.00003512
Iteration 192/1000 | Loss: 0.00003065
Iteration 193/1000 | Loss: 0.00003543
Iteration 194/1000 | Loss: 0.00003133
Iteration 195/1000 | Loss: 0.00003077
Iteration 196/1000 | Loss: 0.00003667
Iteration 197/1000 | Loss: 0.00003041
Iteration 198/1000 | Loss: 0.00003191
Iteration 199/1000 | Loss: 0.00003035
Iteration 200/1000 | Loss: 0.00003610
Iteration 201/1000 | Loss: 0.00002999
Iteration 202/1000 | Loss: 0.00003139
Iteration 203/1000 | Loss: 0.00002988
Iteration 204/1000 | Loss: 0.00003477
Iteration 205/1000 | Loss: 0.00002973
Iteration 206/1000 | Loss: 0.00002689
Iteration 207/1000 | Loss: 0.00003148
Iteration 208/1000 | Loss: 0.00002712
Iteration 209/1000 | Loss: 0.00003278
Iteration 210/1000 | Loss: 0.00003045
Iteration 211/1000 | Loss: 0.00003059
Iteration 212/1000 | Loss: 0.00003173
Iteration 213/1000 | Loss: 0.00003127
Iteration 214/1000 | Loss: 0.00002198
Iteration 215/1000 | Loss: 0.00001960
Iteration 216/1000 | Loss: 0.00001851
Iteration 217/1000 | Loss: 0.00001809
Iteration 218/1000 | Loss: 0.00001781
Iteration 219/1000 | Loss: 0.00001775
Iteration 220/1000 | Loss: 0.00001774
Iteration 221/1000 | Loss: 0.00001774
Iteration 222/1000 | Loss: 0.00001762
Iteration 223/1000 | Loss: 0.00001760
Iteration 224/1000 | Loss: 0.00001760
Iteration 225/1000 | Loss: 0.00001759
Iteration 226/1000 | Loss: 0.00001757
Iteration 227/1000 | Loss: 0.00001757
Iteration 228/1000 | Loss: 0.00001757
Iteration 229/1000 | Loss: 0.00001756
Iteration 230/1000 | Loss: 0.00001755
Iteration 231/1000 | Loss: 0.00001755
Iteration 232/1000 | Loss: 0.00001754
Iteration 233/1000 | Loss: 0.00001754
Iteration 234/1000 | Loss: 0.00001754
Iteration 235/1000 | Loss: 0.00001754
Iteration 236/1000 | Loss: 0.00001754
Iteration 237/1000 | Loss: 0.00001754
Iteration 238/1000 | Loss: 0.00001754
Iteration 239/1000 | Loss: 0.00001754
Iteration 240/1000 | Loss: 0.00001753
Iteration 241/1000 | Loss: 0.00001753
Iteration 242/1000 | Loss: 0.00001753
Iteration 243/1000 | Loss: 0.00001753
Iteration 244/1000 | Loss: 0.00001753
Iteration 245/1000 | Loss: 0.00001752
Iteration 246/1000 | Loss: 0.00001752
Iteration 247/1000 | Loss: 0.00001749
Iteration 248/1000 | Loss: 0.00001749
Iteration 249/1000 | Loss: 0.00001749
Iteration 250/1000 | Loss: 0.00001749
Iteration 251/1000 | Loss: 0.00001749
Iteration 252/1000 | Loss: 0.00001748
Iteration 253/1000 | Loss: 0.00001748
Iteration 254/1000 | Loss: 0.00001748
Iteration 255/1000 | Loss: 0.00001748
Iteration 256/1000 | Loss: 0.00001747
Iteration 257/1000 | Loss: 0.00001747
Iteration 258/1000 | Loss: 0.00001747
Iteration 259/1000 | Loss: 0.00001747
Iteration 260/1000 | Loss: 0.00001747
Iteration 261/1000 | Loss: 0.00001747
Iteration 262/1000 | Loss: 0.00001747
Iteration 263/1000 | Loss: 0.00001747
Iteration 264/1000 | Loss: 0.00001747
Iteration 265/1000 | Loss: 0.00001747
Iteration 266/1000 | Loss: 0.00001747
Iteration 267/1000 | Loss: 0.00001747
Iteration 268/1000 | Loss: 0.00001747
Iteration 269/1000 | Loss: 0.00001747
Iteration 270/1000 | Loss: 0.00001747
Iteration 271/1000 | Loss: 0.00001747
Iteration 272/1000 | Loss: 0.00001747
Iteration 273/1000 | Loss: 0.00001747
Iteration 274/1000 | Loss: 0.00001747
Iteration 275/1000 | Loss: 0.00001747
Iteration 276/1000 | Loss: 0.00001747
Iteration 277/1000 | Loss: 0.00001747
Iteration 278/1000 | Loss: 0.00001747
Iteration 279/1000 | Loss: 0.00001747
Iteration 280/1000 | Loss: 0.00001747
Iteration 281/1000 | Loss: 0.00001747
Iteration 282/1000 | Loss: 0.00001747
Iteration 283/1000 | Loss: 0.00001747
Iteration 284/1000 | Loss: 0.00001747
Iteration 285/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [1.7465177734266035e-05, 1.7465177734266035e-05, 1.7465177734266035e-05, 1.7465177734266035e-05, 1.7465177734266035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7465177734266035e-05

Optimization complete. Final v2v error: 3.50413179397583 mm

Highest mean error: 8.785507202148438 mm for frame 129

Lowest mean error: 3.0628371238708496 mm for frame 231

Saving results

Total time: 417.73960041999817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494597
Iteration 2/25 | Loss: 0.00088156
Iteration 3/25 | Loss: 0.00078087
Iteration 4/25 | Loss: 0.00075906
Iteration 5/25 | Loss: 0.00075257
Iteration 6/25 | Loss: 0.00075129
Iteration 7/25 | Loss: 0.00075106
Iteration 8/25 | Loss: 0.00075106
Iteration 9/25 | Loss: 0.00075106
Iteration 10/25 | Loss: 0.00075106
Iteration 11/25 | Loss: 0.00075106
Iteration 12/25 | Loss: 0.00075106
Iteration 13/25 | Loss: 0.00075106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007510604918934405, 0.0007510604918934405, 0.0007510604918934405, 0.0007510604918934405, 0.0007510604918934405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007510604918934405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54605591
Iteration 2/25 | Loss: 0.00083571
Iteration 3/25 | Loss: 0.00083567
Iteration 4/25 | Loss: 0.00083567
Iteration 5/25 | Loss: 0.00083567
Iteration 6/25 | Loss: 0.00083567
Iteration 7/25 | Loss: 0.00083567
Iteration 8/25 | Loss: 0.00083567
Iteration 9/25 | Loss: 0.00083567
Iteration 10/25 | Loss: 0.00083567
Iteration 11/25 | Loss: 0.00083567
Iteration 12/25 | Loss: 0.00083567
Iteration 13/25 | Loss: 0.00083567
Iteration 14/25 | Loss: 0.00083567
Iteration 15/25 | Loss: 0.00083567
Iteration 16/25 | Loss: 0.00083567
Iteration 17/25 | Loss: 0.00083567
Iteration 18/25 | Loss: 0.00083567
Iteration 19/25 | Loss: 0.00083567
Iteration 20/25 | Loss: 0.00083567
Iteration 21/25 | Loss: 0.00083567
Iteration 22/25 | Loss: 0.00083567
Iteration 23/25 | Loss: 0.00083567
Iteration 24/25 | Loss: 0.00083567
Iteration 25/25 | Loss: 0.00083567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083567
Iteration 2/1000 | Loss: 0.00002529
Iteration 3/1000 | Loss: 0.00001953
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001670
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001532
Iteration 9/1000 | Loss: 0.00001519
Iteration 10/1000 | Loss: 0.00001516
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00001482
Iteration 16/1000 | Loss: 0.00001481
Iteration 17/1000 | Loss: 0.00001480
Iteration 18/1000 | Loss: 0.00001480
Iteration 19/1000 | Loss: 0.00001478
Iteration 20/1000 | Loss: 0.00001477
Iteration 21/1000 | Loss: 0.00001477
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001473
Iteration 32/1000 | Loss: 0.00001473
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001470
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001466
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001465
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001464
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001463
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001461
Iteration 79/1000 | Loss: 0.00001461
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001461
Iteration 84/1000 | Loss: 0.00001461
Iteration 85/1000 | Loss: 0.00001461
Iteration 86/1000 | Loss: 0.00001461
Iteration 87/1000 | Loss: 0.00001461
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001461
Iteration 94/1000 | Loss: 0.00001461
Iteration 95/1000 | Loss: 0.00001461
Iteration 96/1000 | Loss: 0.00001461
Iteration 97/1000 | Loss: 0.00001461
Iteration 98/1000 | Loss: 0.00001461
Iteration 99/1000 | Loss: 0.00001461
Iteration 100/1000 | Loss: 0.00001461
Iteration 101/1000 | Loss: 0.00001461
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001461
Iteration 109/1000 | Loss: 0.00001461
Iteration 110/1000 | Loss: 0.00001461
Iteration 111/1000 | Loss: 0.00001461
Iteration 112/1000 | Loss: 0.00001461
Iteration 113/1000 | Loss: 0.00001461
Iteration 114/1000 | Loss: 0.00001461
Iteration 115/1000 | Loss: 0.00001461
Iteration 116/1000 | Loss: 0.00001461
Iteration 117/1000 | Loss: 0.00001461
Iteration 118/1000 | Loss: 0.00001461
Iteration 119/1000 | Loss: 0.00001461
Iteration 120/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.4606016520701814e-05, 1.4606016520701814e-05, 1.4606016520701814e-05, 1.4606016520701814e-05, 1.4606016520701814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4606016520701814e-05

Optimization complete. Final v2v error: 3.2546985149383545 mm

Highest mean error: 3.9683115482330322 mm for frame 47

Lowest mean error: 2.864183187484741 mm for frame 23

Saving results

Total time: 32.87860894203186
