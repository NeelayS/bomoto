Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=124, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6944-6999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643256
Iteration 2/25 | Loss: 0.00160201
Iteration 3/25 | Loss: 0.00125044
Iteration 4/25 | Loss: 0.00116485
Iteration 5/25 | Loss: 0.00115994
Iteration 6/25 | Loss: 0.00115176
Iteration 7/25 | Loss: 0.00114989
Iteration 8/25 | Loss: 0.00114920
Iteration 9/25 | Loss: 0.00114857
Iteration 10/25 | Loss: 0.00115143
Iteration 11/25 | Loss: 0.00114773
Iteration 12/25 | Loss: 0.00115117
Iteration 13/25 | Loss: 0.00114755
Iteration 14/25 | Loss: 0.00114743
Iteration 15/25 | Loss: 0.00114743
Iteration 16/25 | Loss: 0.00114743
Iteration 17/25 | Loss: 0.00114743
Iteration 18/25 | Loss: 0.00114743
Iteration 19/25 | Loss: 0.00114743
Iteration 20/25 | Loss: 0.00114743
Iteration 21/25 | Loss: 0.00114743
Iteration 22/25 | Loss: 0.00114743
Iteration 23/25 | Loss: 0.00114743
Iteration 24/25 | Loss: 0.00114743
Iteration 25/25 | Loss: 0.00114743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.93828392
Iteration 2/25 | Loss: 0.00067873
Iteration 3/25 | Loss: 0.00067868
Iteration 4/25 | Loss: 0.00067868
Iteration 5/25 | Loss: 0.00067868
Iteration 6/25 | Loss: 0.00067868
Iteration 7/25 | Loss: 0.00067868
Iteration 8/25 | Loss: 0.00067868
Iteration 9/25 | Loss: 0.00067868
Iteration 10/25 | Loss: 0.00067868
Iteration 11/25 | Loss: 0.00067868
Iteration 12/25 | Loss: 0.00067868
Iteration 13/25 | Loss: 0.00067868
Iteration 14/25 | Loss: 0.00067868
Iteration 15/25 | Loss: 0.00067868
Iteration 16/25 | Loss: 0.00067868
Iteration 17/25 | Loss: 0.00067868
Iteration 18/25 | Loss: 0.00067868
Iteration 19/25 | Loss: 0.00067868
Iteration 20/25 | Loss: 0.00067868
Iteration 21/25 | Loss: 0.00067868
Iteration 22/25 | Loss: 0.00067868
Iteration 23/25 | Loss: 0.00067868
Iteration 24/25 | Loss: 0.00067868
Iteration 25/25 | Loss: 0.00067868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067868
Iteration 2/1000 | Loss: 0.00002915
Iteration 3/1000 | Loss: 0.00001913
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00007936
Iteration 6/1000 | Loss: 0.00001719
Iteration 7/1000 | Loss: 0.00001544
Iteration 8/1000 | Loss: 0.00001528
Iteration 9/1000 | Loss: 0.00001502
Iteration 10/1000 | Loss: 0.00001487
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001452
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001444
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001441
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001435
Iteration 24/1000 | Loss: 0.00001434
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001433
Iteration 28/1000 | Loss: 0.00001433
Iteration 29/1000 | Loss: 0.00001430
Iteration 30/1000 | Loss: 0.00001427
Iteration 31/1000 | Loss: 0.00001427
Iteration 32/1000 | Loss: 0.00001426
Iteration 33/1000 | Loss: 0.00001426
Iteration 34/1000 | Loss: 0.00001425
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001421
Iteration 37/1000 | Loss: 0.00001420
Iteration 38/1000 | Loss: 0.00001419
Iteration 39/1000 | Loss: 0.00001419
Iteration 40/1000 | Loss: 0.00001418
Iteration 41/1000 | Loss: 0.00001416
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001415
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001411
Iteration 46/1000 | Loss: 0.00001411
Iteration 47/1000 | Loss: 0.00001411
Iteration 48/1000 | Loss: 0.00001409
Iteration 49/1000 | Loss: 0.00001409
Iteration 50/1000 | Loss: 0.00001409
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001405
Iteration 57/1000 | Loss: 0.00001405
Iteration 58/1000 | Loss: 0.00001404
Iteration 59/1000 | Loss: 0.00001404
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001400
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001393
Iteration 84/1000 | Loss: 0.00001393
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001391
Iteration 90/1000 | Loss: 0.00001391
Iteration 91/1000 | Loss: 0.00001391
Iteration 92/1000 | Loss: 0.00001391
Iteration 93/1000 | Loss: 0.00001390
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001389
Iteration 103/1000 | Loss: 0.00001389
Iteration 104/1000 | Loss: 0.00001389
Iteration 105/1000 | Loss: 0.00001389
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001388
Iteration 108/1000 | Loss: 0.00001388
Iteration 109/1000 | Loss: 0.00001387
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001387
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001387
Iteration 114/1000 | Loss: 0.00001387
Iteration 115/1000 | Loss: 0.00001387
Iteration 116/1000 | Loss: 0.00001387
Iteration 117/1000 | Loss: 0.00001387
Iteration 118/1000 | Loss: 0.00001387
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001387
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001385
Iteration 129/1000 | Loss: 0.00001385
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001384
Iteration 140/1000 | Loss: 0.00001384
Iteration 141/1000 | Loss: 0.00001384
Iteration 142/1000 | Loss: 0.00001384
Iteration 143/1000 | Loss: 0.00001384
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001384
Iteration 152/1000 | Loss: 0.00001384
Iteration 153/1000 | Loss: 0.00001384
Iteration 154/1000 | Loss: 0.00001384
Iteration 155/1000 | Loss: 0.00001383
Iteration 156/1000 | Loss: 0.00001383
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001382
Iteration 160/1000 | Loss: 0.00001382
Iteration 161/1000 | Loss: 0.00001382
Iteration 162/1000 | Loss: 0.00001382
Iteration 163/1000 | Loss: 0.00001382
Iteration 164/1000 | Loss: 0.00001382
Iteration 165/1000 | Loss: 0.00001382
Iteration 166/1000 | Loss: 0.00001381
Iteration 167/1000 | Loss: 0.00001381
Iteration 168/1000 | Loss: 0.00001381
Iteration 169/1000 | Loss: 0.00001381
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001381
Iteration 172/1000 | Loss: 0.00001381
Iteration 173/1000 | Loss: 0.00001380
Iteration 174/1000 | Loss: 0.00001380
Iteration 175/1000 | Loss: 0.00001380
Iteration 176/1000 | Loss: 0.00001380
Iteration 177/1000 | Loss: 0.00001379
Iteration 178/1000 | Loss: 0.00001379
Iteration 179/1000 | Loss: 0.00001379
Iteration 180/1000 | Loss: 0.00001379
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001377
Iteration 186/1000 | Loss: 0.00001377
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001376
Iteration 199/1000 | Loss: 0.00001376
Iteration 200/1000 | Loss: 0.00001376
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.3763326023763511e-05, 1.3763326023763511e-05, 1.3763326023763511e-05, 1.3763326023763511e-05, 1.3763326023763511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3763326023763511e-05

Optimization complete. Final v2v error: 3.0558407306671143 mm

Highest mean error: 3.9535224437713623 mm for frame 92

Lowest mean error: 2.584810495376587 mm for frame 44

Saving results

Total time: 70.99226021766663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389881
Iteration 2/25 | Loss: 0.00112611
Iteration 3/25 | Loss: 0.00108166
Iteration 4/25 | Loss: 0.00107450
Iteration 5/25 | Loss: 0.00107193
Iteration 6/25 | Loss: 0.00107165
Iteration 7/25 | Loss: 0.00107165
Iteration 8/25 | Loss: 0.00107165
Iteration 9/25 | Loss: 0.00107165
Iteration 10/25 | Loss: 0.00107165
Iteration 11/25 | Loss: 0.00107165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010716452961787581, 0.0010716452961787581, 0.0010716452961787581, 0.0010716452961787581, 0.0010716452961787581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010716452961787581

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42684364
Iteration 2/25 | Loss: 0.00065636
Iteration 3/25 | Loss: 0.00065636
Iteration 4/25 | Loss: 0.00065636
Iteration 5/25 | Loss: 0.00065636
Iteration 6/25 | Loss: 0.00065636
Iteration 7/25 | Loss: 0.00065636
Iteration 8/25 | Loss: 0.00065636
Iteration 9/25 | Loss: 0.00065636
Iteration 10/25 | Loss: 0.00065636
Iteration 11/25 | Loss: 0.00065636
Iteration 12/25 | Loss: 0.00065636
Iteration 13/25 | Loss: 0.00065636
Iteration 14/25 | Loss: 0.00065636
Iteration 15/25 | Loss: 0.00065636
Iteration 16/25 | Loss: 0.00065636
Iteration 17/25 | Loss: 0.00065636
Iteration 18/25 | Loss: 0.00065636
Iteration 19/25 | Loss: 0.00065636
Iteration 20/25 | Loss: 0.00065636
Iteration 21/25 | Loss: 0.00065636
Iteration 22/25 | Loss: 0.00065636
Iteration 23/25 | Loss: 0.00065636
Iteration 24/25 | Loss: 0.00065636
Iteration 25/25 | Loss: 0.00065636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065636
Iteration 2/1000 | Loss: 0.00001934
Iteration 3/1000 | Loss: 0.00001292
Iteration 4/1000 | Loss: 0.00001176
Iteration 5/1000 | Loss: 0.00001118
Iteration 6/1000 | Loss: 0.00001105
Iteration 7/1000 | Loss: 0.00001087
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001076
Iteration 12/1000 | Loss: 0.00001067
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001058
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001047
Iteration 17/1000 | Loss: 0.00001046
Iteration 18/1000 | Loss: 0.00001046
Iteration 19/1000 | Loss: 0.00001042
Iteration 20/1000 | Loss: 0.00001039
Iteration 21/1000 | Loss: 0.00001039
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001036
Iteration 24/1000 | Loss: 0.00001034
Iteration 25/1000 | Loss: 0.00001034
Iteration 26/1000 | Loss: 0.00001033
Iteration 27/1000 | Loss: 0.00001033
Iteration 28/1000 | Loss: 0.00001033
Iteration 29/1000 | Loss: 0.00001032
Iteration 30/1000 | Loss: 0.00001032
Iteration 31/1000 | Loss: 0.00001031
Iteration 32/1000 | Loss: 0.00001031
Iteration 33/1000 | Loss: 0.00001031
Iteration 34/1000 | Loss: 0.00001031
Iteration 35/1000 | Loss: 0.00001031
Iteration 36/1000 | Loss: 0.00001030
Iteration 37/1000 | Loss: 0.00001030
Iteration 38/1000 | Loss: 0.00001030
Iteration 39/1000 | Loss: 0.00001030
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001030
Iteration 42/1000 | Loss: 0.00001030
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001029
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001029
Iteration 48/1000 | Loss: 0.00001029
Iteration 49/1000 | Loss: 0.00001028
Iteration 50/1000 | Loss: 0.00001028
Iteration 51/1000 | Loss: 0.00001028
Iteration 52/1000 | Loss: 0.00001027
Iteration 53/1000 | Loss: 0.00001027
Iteration 54/1000 | Loss: 0.00001027
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001025
Iteration 58/1000 | Loss: 0.00001025
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001024
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001023
Iteration 64/1000 | Loss: 0.00001023
Iteration 65/1000 | Loss: 0.00001023
Iteration 66/1000 | Loss: 0.00001022
Iteration 67/1000 | Loss: 0.00001022
Iteration 68/1000 | Loss: 0.00001022
Iteration 69/1000 | Loss: 0.00001022
Iteration 70/1000 | Loss: 0.00001022
Iteration 71/1000 | Loss: 0.00001022
Iteration 72/1000 | Loss: 0.00001022
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001021
Iteration 75/1000 | Loss: 0.00001021
Iteration 76/1000 | Loss: 0.00001021
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001020
Iteration 79/1000 | Loss: 0.00001020
Iteration 80/1000 | Loss: 0.00001020
Iteration 81/1000 | Loss: 0.00001020
Iteration 82/1000 | Loss: 0.00001020
Iteration 83/1000 | Loss: 0.00001020
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001020
Iteration 86/1000 | Loss: 0.00001019
Iteration 87/1000 | Loss: 0.00001019
Iteration 88/1000 | Loss: 0.00001019
Iteration 89/1000 | Loss: 0.00001018
Iteration 90/1000 | Loss: 0.00001018
Iteration 91/1000 | Loss: 0.00001018
Iteration 92/1000 | Loss: 0.00001018
Iteration 93/1000 | Loss: 0.00001018
Iteration 94/1000 | Loss: 0.00001018
Iteration 95/1000 | Loss: 0.00001017
Iteration 96/1000 | Loss: 0.00001017
Iteration 97/1000 | Loss: 0.00001017
Iteration 98/1000 | Loss: 0.00001017
Iteration 99/1000 | Loss: 0.00001017
Iteration 100/1000 | Loss: 0.00001017
Iteration 101/1000 | Loss: 0.00001017
Iteration 102/1000 | Loss: 0.00001017
Iteration 103/1000 | Loss: 0.00001017
Iteration 104/1000 | Loss: 0.00001017
Iteration 105/1000 | Loss: 0.00001016
Iteration 106/1000 | Loss: 0.00001016
Iteration 107/1000 | Loss: 0.00001016
Iteration 108/1000 | Loss: 0.00001016
Iteration 109/1000 | Loss: 0.00001016
Iteration 110/1000 | Loss: 0.00001016
Iteration 111/1000 | Loss: 0.00001016
Iteration 112/1000 | Loss: 0.00001016
Iteration 113/1000 | Loss: 0.00001015
Iteration 114/1000 | Loss: 0.00001015
Iteration 115/1000 | Loss: 0.00001015
Iteration 116/1000 | Loss: 0.00001015
Iteration 117/1000 | Loss: 0.00001015
Iteration 118/1000 | Loss: 0.00001015
Iteration 119/1000 | Loss: 0.00001015
Iteration 120/1000 | Loss: 0.00001014
Iteration 121/1000 | Loss: 0.00001014
Iteration 122/1000 | Loss: 0.00001014
Iteration 123/1000 | Loss: 0.00001014
Iteration 124/1000 | Loss: 0.00001014
Iteration 125/1000 | Loss: 0.00001014
Iteration 126/1000 | Loss: 0.00001014
Iteration 127/1000 | Loss: 0.00001014
Iteration 128/1000 | Loss: 0.00001014
Iteration 129/1000 | Loss: 0.00001014
Iteration 130/1000 | Loss: 0.00001014
Iteration 131/1000 | Loss: 0.00001014
Iteration 132/1000 | Loss: 0.00001014
Iteration 133/1000 | Loss: 0.00001014
Iteration 134/1000 | Loss: 0.00001014
Iteration 135/1000 | Loss: 0.00001014
Iteration 136/1000 | Loss: 0.00001014
Iteration 137/1000 | Loss: 0.00001014
Iteration 138/1000 | Loss: 0.00001014
Iteration 139/1000 | Loss: 0.00001013
Iteration 140/1000 | Loss: 0.00001013
Iteration 141/1000 | Loss: 0.00001013
Iteration 142/1000 | Loss: 0.00001012
Iteration 143/1000 | Loss: 0.00001012
Iteration 144/1000 | Loss: 0.00001012
Iteration 145/1000 | Loss: 0.00001012
Iteration 146/1000 | Loss: 0.00001012
Iteration 147/1000 | Loss: 0.00001011
Iteration 148/1000 | Loss: 0.00001011
Iteration 149/1000 | Loss: 0.00001011
Iteration 150/1000 | Loss: 0.00001011
Iteration 151/1000 | Loss: 0.00001011
Iteration 152/1000 | Loss: 0.00001011
Iteration 153/1000 | Loss: 0.00001011
Iteration 154/1000 | Loss: 0.00001011
Iteration 155/1000 | Loss: 0.00001011
Iteration 156/1000 | Loss: 0.00001011
Iteration 157/1000 | Loss: 0.00001011
Iteration 158/1000 | Loss: 0.00001011
Iteration 159/1000 | Loss: 0.00001011
Iteration 160/1000 | Loss: 0.00001011
Iteration 161/1000 | Loss: 0.00001011
Iteration 162/1000 | Loss: 0.00001011
Iteration 163/1000 | Loss: 0.00001011
Iteration 164/1000 | Loss: 0.00001011
Iteration 165/1000 | Loss: 0.00001011
Iteration 166/1000 | Loss: 0.00001011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.0107208254339639e-05, 1.0107208254339639e-05, 1.0107208254339639e-05, 1.0107208254339639e-05, 1.0107208254339639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0107208254339639e-05

Optimization complete. Final v2v error: 2.7223031520843506 mm

Highest mean error: 2.9504690170288086 mm for frame 107

Lowest mean error: 2.636699914932251 mm for frame 46

Saving results

Total time: 31.19822669029236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026568
Iteration 2/25 | Loss: 0.00159345
Iteration 3/25 | Loss: 0.00129368
Iteration 4/25 | Loss: 0.00126562
Iteration 5/25 | Loss: 0.00125644
Iteration 6/25 | Loss: 0.00125412
Iteration 7/25 | Loss: 0.00125412
Iteration 8/25 | Loss: 0.00125412
Iteration 9/25 | Loss: 0.00125412
Iteration 10/25 | Loss: 0.00125412
Iteration 11/25 | Loss: 0.00125412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012541242176666856, 0.0012541242176666856, 0.0012541242176666856, 0.0012541242176666856, 0.0012541242176666856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012541242176666856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93496323
Iteration 2/25 | Loss: 0.00090439
Iteration 3/25 | Loss: 0.00090437
Iteration 4/25 | Loss: 0.00090437
Iteration 5/25 | Loss: 0.00090437
Iteration 6/25 | Loss: 0.00090437
Iteration 7/25 | Loss: 0.00090437
Iteration 8/25 | Loss: 0.00090437
Iteration 9/25 | Loss: 0.00090437
Iteration 10/25 | Loss: 0.00090436
Iteration 11/25 | Loss: 0.00090436
Iteration 12/25 | Loss: 0.00090436
Iteration 13/25 | Loss: 0.00090436
Iteration 14/25 | Loss: 0.00090436
Iteration 15/25 | Loss: 0.00090436
Iteration 16/25 | Loss: 0.00090436
Iteration 17/25 | Loss: 0.00090436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009043647442013025, 0.0009043647442013025, 0.0009043647442013025, 0.0009043647442013025, 0.0009043647442013025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009043647442013025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090436
Iteration 2/1000 | Loss: 0.00007669
Iteration 3/1000 | Loss: 0.00004290
Iteration 4/1000 | Loss: 0.00003200
Iteration 5/1000 | Loss: 0.00002968
Iteration 6/1000 | Loss: 0.00002855
Iteration 7/1000 | Loss: 0.00002793
Iteration 8/1000 | Loss: 0.00002716
Iteration 9/1000 | Loss: 0.00002670
Iteration 10/1000 | Loss: 0.00002631
Iteration 11/1000 | Loss: 0.00002600
Iteration 12/1000 | Loss: 0.00002579
Iteration 13/1000 | Loss: 0.00002561
Iteration 14/1000 | Loss: 0.00002544
Iteration 15/1000 | Loss: 0.00002534
Iteration 16/1000 | Loss: 0.00002532
Iteration 17/1000 | Loss: 0.00002531
Iteration 18/1000 | Loss: 0.00002531
Iteration 19/1000 | Loss: 0.00002529
Iteration 20/1000 | Loss: 0.00002529
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002526
Iteration 23/1000 | Loss: 0.00002525
Iteration 24/1000 | Loss: 0.00002519
Iteration 25/1000 | Loss: 0.00002514
Iteration 26/1000 | Loss: 0.00002508
Iteration 27/1000 | Loss: 0.00002506
Iteration 28/1000 | Loss: 0.00002505
Iteration 29/1000 | Loss: 0.00002505
Iteration 30/1000 | Loss: 0.00002501
Iteration 31/1000 | Loss: 0.00002498
Iteration 32/1000 | Loss: 0.00002498
Iteration 33/1000 | Loss: 0.00002495
Iteration 34/1000 | Loss: 0.00002492
Iteration 35/1000 | Loss: 0.00002489
Iteration 36/1000 | Loss: 0.00002488
Iteration 37/1000 | Loss: 0.00002486
Iteration 38/1000 | Loss: 0.00002485
Iteration 39/1000 | Loss: 0.00002484
Iteration 40/1000 | Loss: 0.00002484
Iteration 41/1000 | Loss: 0.00002484
Iteration 42/1000 | Loss: 0.00002484
Iteration 43/1000 | Loss: 0.00002484
Iteration 44/1000 | Loss: 0.00002483
Iteration 45/1000 | Loss: 0.00002483
Iteration 46/1000 | Loss: 0.00002483
Iteration 47/1000 | Loss: 0.00002483
Iteration 48/1000 | Loss: 0.00002482
Iteration 49/1000 | Loss: 0.00002482
Iteration 50/1000 | Loss: 0.00002482
Iteration 51/1000 | Loss: 0.00002482
Iteration 52/1000 | Loss: 0.00002482
Iteration 53/1000 | Loss: 0.00002482
Iteration 54/1000 | Loss: 0.00002482
Iteration 55/1000 | Loss: 0.00002481
Iteration 56/1000 | Loss: 0.00002481
Iteration 57/1000 | Loss: 0.00002481
Iteration 58/1000 | Loss: 0.00002481
Iteration 59/1000 | Loss: 0.00002481
Iteration 60/1000 | Loss: 0.00002481
Iteration 61/1000 | Loss: 0.00002480
Iteration 62/1000 | Loss: 0.00002480
Iteration 63/1000 | Loss: 0.00002478
Iteration 64/1000 | Loss: 0.00002478
Iteration 65/1000 | Loss: 0.00002478
Iteration 66/1000 | Loss: 0.00002478
Iteration 67/1000 | Loss: 0.00002478
Iteration 68/1000 | Loss: 0.00002478
Iteration 69/1000 | Loss: 0.00002477
Iteration 70/1000 | Loss: 0.00002477
Iteration 71/1000 | Loss: 0.00002477
Iteration 72/1000 | Loss: 0.00002477
Iteration 73/1000 | Loss: 0.00002477
Iteration 74/1000 | Loss: 0.00002477
Iteration 75/1000 | Loss: 0.00002477
Iteration 76/1000 | Loss: 0.00002477
Iteration 77/1000 | Loss: 0.00002477
Iteration 78/1000 | Loss: 0.00002477
Iteration 79/1000 | Loss: 0.00002476
Iteration 80/1000 | Loss: 0.00002476
Iteration 81/1000 | Loss: 0.00002475
Iteration 82/1000 | Loss: 0.00002475
Iteration 83/1000 | Loss: 0.00002475
Iteration 84/1000 | Loss: 0.00002475
Iteration 85/1000 | Loss: 0.00002475
Iteration 86/1000 | Loss: 0.00002475
Iteration 87/1000 | Loss: 0.00002475
Iteration 88/1000 | Loss: 0.00002475
Iteration 89/1000 | Loss: 0.00002474
Iteration 90/1000 | Loss: 0.00002474
Iteration 91/1000 | Loss: 0.00002474
Iteration 92/1000 | Loss: 0.00002474
Iteration 93/1000 | Loss: 0.00002474
Iteration 94/1000 | Loss: 0.00002474
Iteration 95/1000 | Loss: 0.00002474
Iteration 96/1000 | Loss: 0.00002474
Iteration 97/1000 | Loss: 0.00002474
Iteration 98/1000 | Loss: 0.00002473
Iteration 99/1000 | Loss: 0.00002473
Iteration 100/1000 | Loss: 0.00002473
Iteration 101/1000 | Loss: 0.00002473
Iteration 102/1000 | Loss: 0.00002473
Iteration 103/1000 | Loss: 0.00002472
Iteration 104/1000 | Loss: 0.00002472
Iteration 105/1000 | Loss: 0.00002472
Iteration 106/1000 | Loss: 0.00002472
Iteration 107/1000 | Loss: 0.00002472
Iteration 108/1000 | Loss: 0.00002472
Iteration 109/1000 | Loss: 0.00002472
Iteration 110/1000 | Loss: 0.00002472
Iteration 111/1000 | Loss: 0.00002471
Iteration 112/1000 | Loss: 0.00002471
Iteration 113/1000 | Loss: 0.00002471
Iteration 114/1000 | Loss: 0.00002471
Iteration 115/1000 | Loss: 0.00002471
Iteration 116/1000 | Loss: 0.00002471
Iteration 117/1000 | Loss: 0.00002471
Iteration 118/1000 | Loss: 0.00002471
Iteration 119/1000 | Loss: 0.00002470
Iteration 120/1000 | Loss: 0.00002470
Iteration 121/1000 | Loss: 0.00002470
Iteration 122/1000 | Loss: 0.00002470
Iteration 123/1000 | Loss: 0.00002470
Iteration 124/1000 | Loss: 0.00002470
Iteration 125/1000 | Loss: 0.00002470
Iteration 126/1000 | Loss: 0.00002470
Iteration 127/1000 | Loss: 0.00002470
Iteration 128/1000 | Loss: 0.00002470
Iteration 129/1000 | Loss: 0.00002470
Iteration 130/1000 | Loss: 0.00002470
Iteration 131/1000 | Loss: 0.00002470
Iteration 132/1000 | Loss: 0.00002469
Iteration 133/1000 | Loss: 0.00002469
Iteration 134/1000 | Loss: 0.00002469
Iteration 135/1000 | Loss: 0.00002469
Iteration 136/1000 | Loss: 0.00002469
Iteration 137/1000 | Loss: 0.00002469
Iteration 138/1000 | Loss: 0.00002469
Iteration 139/1000 | Loss: 0.00002469
Iteration 140/1000 | Loss: 0.00002469
Iteration 141/1000 | Loss: 0.00002469
Iteration 142/1000 | Loss: 0.00002469
Iteration 143/1000 | Loss: 0.00002469
Iteration 144/1000 | Loss: 0.00002468
Iteration 145/1000 | Loss: 0.00002468
Iteration 146/1000 | Loss: 0.00002468
Iteration 147/1000 | Loss: 0.00002468
Iteration 148/1000 | Loss: 0.00002468
Iteration 149/1000 | Loss: 0.00002468
Iteration 150/1000 | Loss: 0.00002468
Iteration 151/1000 | Loss: 0.00002468
Iteration 152/1000 | Loss: 0.00002468
Iteration 153/1000 | Loss: 0.00002468
Iteration 154/1000 | Loss: 0.00002468
Iteration 155/1000 | Loss: 0.00002468
Iteration 156/1000 | Loss: 0.00002468
Iteration 157/1000 | Loss: 0.00002468
Iteration 158/1000 | Loss: 0.00002468
Iteration 159/1000 | Loss: 0.00002468
Iteration 160/1000 | Loss: 0.00002468
Iteration 161/1000 | Loss: 0.00002468
Iteration 162/1000 | Loss: 0.00002468
Iteration 163/1000 | Loss: 0.00002468
Iteration 164/1000 | Loss: 0.00002468
Iteration 165/1000 | Loss: 0.00002468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.468461934768129e-05, 2.468461934768129e-05, 2.468461934768129e-05, 2.468461934768129e-05, 2.468461934768129e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.468461934768129e-05

Optimization complete. Final v2v error: 4.100442409515381 mm

Highest mean error: 4.964670658111572 mm for frame 66

Lowest mean error: 3.434323310852051 mm for frame 28

Saving results

Total time: 48.4892520904541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004586
Iteration 2/25 | Loss: 0.00225135
Iteration 3/25 | Loss: 0.00157440
Iteration 4/25 | Loss: 0.00144046
Iteration 5/25 | Loss: 0.00142708
Iteration 6/25 | Loss: 0.00125263
Iteration 7/25 | Loss: 0.00121961
Iteration 8/25 | Loss: 0.00118551
Iteration 9/25 | Loss: 0.00118068
Iteration 10/25 | Loss: 0.00116759
Iteration 11/25 | Loss: 0.00117463
Iteration 12/25 | Loss: 0.00117106
Iteration 13/25 | Loss: 0.00116517
Iteration 14/25 | Loss: 0.00116186
Iteration 15/25 | Loss: 0.00115978
Iteration 16/25 | Loss: 0.00116211
Iteration 17/25 | Loss: 0.00116597
Iteration 18/25 | Loss: 0.00116400
Iteration 19/25 | Loss: 0.00116154
Iteration 20/25 | Loss: 0.00116095
Iteration 21/25 | Loss: 0.00115954
Iteration 22/25 | Loss: 0.00115656
Iteration 23/25 | Loss: 0.00115619
Iteration 24/25 | Loss: 0.00115625
Iteration 25/25 | Loss: 0.00115834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53347504
Iteration 2/25 | Loss: 0.00096516
Iteration 3/25 | Loss: 0.00081922
Iteration 4/25 | Loss: 0.00081922
Iteration 5/25 | Loss: 0.00081922
Iteration 6/25 | Loss: 0.00081922
Iteration 7/25 | Loss: 0.00081922
Iteration 8/25 | Loss: 0.00081921
Iteration 9/25 | Loss: 0.00081921
Iteration 10/25 | Loss: 0.00081921
Iteration 11/25 | Loss: 0.00081921
Iteration 12/25 | Loss: 0.00081921
Iteration 13/25 | Loss: 0.00081921
Iteration 14/25 | Loss: 0.00081921
Iteration 15/25 | Loss: 0.00081921
Iteration 16/25 | Loss: 0.00081921
Iteration 17/25 | Loss: 0.00081921
Iteration 18/25 | Loss: 0.00081921
Iteration 19/25 | Loss: 0.00081921
Iteration 20/25 | Loss: 0.00081921
Iteration 21/25 | Loss: 0.00081921
Iteration 22/25 | Loss: 0.00081921
Iteration 23/25 | Loss: 0.00081921
Iteration 24/25 | Loss: 0.00081921
Iteration 25/25 | Loss: 0.00081921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081921
Iteration 2/1000 | Loss: 0.00017573
Iteration 3/1000 | Loss: 0.00021607
Iteration 4/1000 | Loss: 0.00020110
Iteration 5/1000 | Loss: 0.00009086
Iteration 6/1000 | Loss: 0.00004388
Iteration 7/1000 | Loss: 0.00005291
Iteration 8/1000 | Loss: 0.00004018
Iteration 9/1000 | Loss: 0.00002714
Iteration 10/1000 | Loss: 0.00008984
Iteration 11/1000 | Loss: 0.00003147
Iteration 12/1000 | Loss: 0.00004534
Iteration 13/1000 | Loss: 0.00002890
Iteration 14/1000 | Loss: 0.00002301
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001724
Iteration 18/1000 | Loss: 0.00002976
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001736
Iteration 21/1000 | Loss: 0.00001814
Iteration 22/1000 | Loss: 0.00001646
Iteration 23/1000 | Loss: 0.00001646
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001646
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001646
Iteration 36/1000 | Loss: 0.00001646
Iteration 37/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [1.6455853256047703e-05, 1.6455853256047703e-05, 1.6455853256047703e-05, 1.6455853256047703e-05, 1.6455853256047703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6455853256047703e-05

Optimization complete. Final v2v error: 3.238908290863037 mm

Highest mean error: 12.23211669921875 mm for frame 182

Lowest mean error: 2.807159185409546 mm for frame 6

Saving results

Total time: 86.91760301589966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858553
Iteration 2/25 | Loss: 0.00122558
Iteration 3/25 | Loss: 0.00113474
Iteration 4/25 | Loss: 0.00111879
Iteration 5/25 | Loss: 0.00111334
Iteration 6/25 | Loss: 0.00111326
Iteration 7/25 | Loss: 0.00111326
Iteration 8/25 | Loss: 0.00111326
Iteration 9/25 | Loss: 0.00111326
Iteration 10/25 | Loss: 0.00111326
Iteration 11/25 | Loss: 0.00111326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011132576037198305, 0.0011132576037198305, 0.0011132576037198305, 0.0011132576037198305, 0.0011132576037198305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011132576037198305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60528839
Iteration 2/25 | Loss: 0.00052988
Iteration 3/25 | Loss: 0.00052988
Iteration 4/25 | Loss: 0.00052988
Iteration 5/25 | Loss: 0.00052988
Iteration 6/25 | Loss: 0.00052988
Iteration 7/25 | Loss: 0.00052988
Iteration 8/25 | Loss: 0.00052988
Iteration 9/25 | Loss: 0.00052988
Iteration 10/25 | Loss: 0.00052988
Iteration 11/25 | Loss: 0.00052988
Iteration 12/25 | Loss: 0.00052988
Iteration 13/25 | Loss: 0.00052988
Iteration 14/25 | Loss: 0.00052988
Iteration 15/25 | Loss: 0.00052988
Iteration 16/25 | Loss: 0.00052988
Iteration 17/25 | Loss: 0.00052988
Iteration 18/25 | Loss: 0.00052988
Iteration 19/25 | Loss: 0.00052988
Iteration 20/25 | Loss: 0.00052988
Iteration 21/25 | Loss: 0.00052988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005298754549585283, 0.0005298754549585283, 0.0005298754549585283, 0.0005298754549585283, 0.0005298754549585283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005298754549585283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052988
Iteration 2/1000 | Loss: 0.00002217
Iteration 3/1000 | Loss: 0.00001674
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001477
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001469
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001440
Iteration 13/1000 | Loss: 0.00001431
Iteration 14/1000 | Loss: 0.00001428
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00001408
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001404
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001402
Iteration 36/1000 | Loss: 0.00001402
Iteration 37/1000 | Loss: 0.00001401
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001400
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001396
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001392
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001387
Iteration 51/1000 | Loss: 0.00001387
Iteration 52/1000 | Loss: 0.00001386
Iteration 53/1000 | Loss: 0.00001386
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001385
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001379
Iteration 70/1000 | Loss: 0.00001379
Iteration 71/1000 | Loss: 0.00001379
Iteration 72/1000 | Loss: 0.00001379
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001374
Iteration 88/1000 | Loss: 0.00001374
Iteration 89/1000 | Loss: 0.00001374
Iteration 90/1000 | Loss: 0.00001373
Iteration 91/1000 | Loss: 0.00001373
Iteration 92/1000 | Loss: 0.00001373
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001371
Iteration 95/1000 | Loss: 0.00001371
Iteration 96/1000 | Loss: 0.00001371
Iteration 97/1000 | Loss: 0.00001371
Iteration 98/1000 | Loss: 0.00001371
Iteration 99/1000 | Loss: 0.00001370
Iteration 100/1000 | Loss: 0.00001370
Iteration 101/1000 | Loss: 0.00001370
Iteration 102/1000 | Loss: 0.00001370
Iteration 103/1000 | Loss: 0.00001370
Iteration 104/1000 | Loss: 0.00001370
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001369
Iteration 119/1000 | Loss: 0.00001369
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001367
Iteration 123/1000 | Loss: 0.00001367
Iteration 124/1000 | Loss: 0.00001367
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001367
Iteration 127/1000 | Loss: 0.00001367
Iteration 128/1000 | Loss: 0.00001367
Iteration 129/1000 | Loss: 0.00001367
Iteration 130/1000 | Loss: 0.00001366
Iteration 131/1000 | Loss: 0.00001366
Iteration 132/1000 | Loss: 0.00001366
Iteration 133/1000 | Loss: 0.00001366
Iteration 134/1000 | Loss: 0.00001366
Iteration 135/1000 | Loss: 0.00001366
Iteration 136/1000 | Loss: 0.00001366
Iteration 137/1000 | Loss: 0.00001366
Iteration 138/1000 | Loss: 0.00001366
Iteration 139/1000 | Loss: 0.00001365
Iteration 140/1000 | Loss: 0.00001365
Iteration 141/1000 | Loss: 0.00001365
Iteration 142/1000 | Loss: 0.00001364
Iteration 143/1000 | Loss: 0.00001364
Iteration 144/1000 | Loss: 0.00001364
Iteration 145/1000 | Loss: 0.00001364
Iteration 146/1000 | Loss: 0.00001364
Iteration 147/1000 | Loss: 0.00001363
Iteration 148/1000 | Loss: 0.00001363
Iteration 149/1000 | Loss: 0.00001363
Iteration 150/1000 | Loss: 0.00001363
Iteration 151/1000 | Loss: 0.00001362
Iteration 152/1000 | Loss: 0.00001362
Iteration 153/1000 | Loss: 0.00001361
Iteration 154/1000 | Loss: 0.00001361
Iteration 155/1000 | Loss: 0.00001361
Iteration 156/1000 | Loss: 0.00001361
Iteration 157/1000 | Loss: 0.00001361
Iteration 158/1000 | Loss: 0.00001361
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001361
Iteration 161/1000 | Loss: 0.00001361
Iteration 162/1000 | Loss: 0.00001361
Iteration 163/1000 | Loss: 0.00001361
Iteration 164/1000 | Loss: 0.00001361
Iteration 165/1000 | Loss: 0.00001361
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001360
Iteration 169/1000 | Loss: 0.00001360
Iteration 170/1000 | Loss: 0.00001360
Iteration 171/1000 | Loss: 0.00001360
Iteration 172/1000 | Loss: 0.00001360
Iteration 173/1000 | Loss: 0.00001360
Iteration 174/1000 | Loss: 0.00001360
Iteration 175/1000 | Loss: 0.00001360
Iteration 176/1000 | Loss: 0.00001360
Iteration 177/1000 | Loss: 0.00001360
Iteration 178/1000 | Loss: 0.00001360
Iteration 179/1000 | Loss: 0.00001360
Iteration 180/1000 | Loss: 0.00001360
Iteration 181/1000 | Loss: 0.00001360
Iteration 182/1000 | Loss: 0.00001360
Iteration 183/1000 | Loss: 0.00001360
Iteration 184/1000 | Loss: 0.00001360
Iteration 185/1000 | Loss: 0.00001360
Iteration 186/1000 | Loss: 0.00001360
Iteration 187/1000 | Loss: 0.00001360
Iteration 188/1000 | Loss: 0.00001360
Iteration 189/1000 | Loss: 0.00001360
Iteration 190/1000 | Loss: 0.00001360
Iteration 191/1000 | Loss: 0.00001360
Iteration 192/1000 | Loss: 0.00001360
Iteration 193/1000 | Loss: 0.00001360
Iteration 194/1000 | Loss: 0.00001360
Iteration 195/1000 | Loss: 0.00001360
Iteration 196/1000 | Loss: 0.00001360
Iteration 197/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.359530051558977e-05, 1.359530051558977e-05, 1.359530051558977e-05, 1.359530051558977e-05, 1.359530051558977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.359530051558977e-05

Optimization complete. Final v2v error: 3.1282455921173096 mm

Highest mean error: 3.539459705352783 mm for frame 232

Lowest mean error: 2.754114866256714 mm for frame 61

Saving results

Total time: 40.926922082901
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419172
Iteration 2/25 | Loss: 0.00121172
Iteration 3/25 | Loss: 0.00111985
Iteration 4/25 | Loss: 0.00110763
Iteration 5/25 | Loss: 0.00110308
Iteration 6/25 | Loss: 0.00110275
Iteration 7/25 | Loss: 0.00110275
Iteration 8/25 | Loss: 0.00110275
Iteration 9/25 | Loss: 0.00110275
Iteration 10/25 | Loss: 0.00110275
Iteration 11/25 | Loss: 0.00110275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001102749607525766, 0.001102749607525766, 0.001102749607525766, 0.001102749607525766, 0.001102749607525766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001102749607525766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44600415
Iteration 2/25 | Loss: 0.00075866
Iteration 3/25 | Loss: 0.00075866
Iteration 4/25 | Loss: 0.00075865
Iteration 5/25 | Loss: 0.00075865
Iteration 6/25 | Loss: 0.00075865
Iteration 7/25 | Loss: 0.00075865
Iteration 8/25 | Loss: 0.00075865
Iteration 9/25 | Loss: 0.00075865
Iteration 10/25 | Loss: 0.00075865
Iteration 11/25 | Loss: 0.00075865
Iteration 12/25 | Loss: 0.00075865
Iteration 13/25 | Loss: 0.00075865
Iteration 14/25 | Loss: 0.00075865
Iteration 15/25 | Loss: 0.00075865
Iteration 16/25 | Loss: 0.00075865
Iteration 17/25 | Loss: 0.00075865
Iteration 18/25 | Loss: 0.00075865
Iteration 19/25 | Loss: 0.00075865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007586524006910622, 0.0007586524006910622, 0.0007586524006910622, 0.0007586524006910622, 0.0007586524006910622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007586524006910622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075865
Iteration 2/1000 | Loss: 0.00002131
Iteration 3/1000 | Loss: 0.00001542
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001346
Iteration 6/1000 | Loss: 0.00001313
Iteration 7/1000 | Loss: 0.00001296
Iteration 8/1000 | Loss: 0.00001281
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001242
Iteration 11/1000 | Loss: 0.00001234
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001231
Iteration 14/1000 | Loss: 0.00001231
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00001223
Iteration 17/1000 | Loss: 0.00001221
Iteration 18/1000 | Loss: 0.00001221
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001220
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001219
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001214
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001212
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001210
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001210
Iteration 60/1000 | Loss: 0.00001210
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001207
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001205
Iteration 87/1000 | Loss: 0.00001205
Iteration 88/1000 | Loss: 0.00001205
Iteration 89/1000 | Loss: 0.00001205
Iteration 90/1000 | Loss: 0.00001205
Iteration 91/1000 | Loss: 0.00001205
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001201
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001201
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001195
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001190
Iteration 159/1000 | Loss: 0.00001190
Iteration 160/1000 | Loss: 0.00001190
Iteration 161/1000 | Loss: 0.00001190
Iteration 162/1000 | Loss: 0.00001190
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001190
Iteration 166/1000 | Loss: 0.00001190
Iteration 167/1000 | Loss: 0.00001190
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001190
Iteration 170/1000 | Loss: 0.00001190
Iteration 171/1000 | Loss: 0.00001190
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001190
Iteration 176/1000 | Loss: 0.00001190
Iteration 177/1000 | Loss: 0.00001190
Iteration 178/1000 | Loss: 0.00001190
Iteration 179/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1896853720827494e-05, 1.1896853720827494e-05, 1.1896853720827494e-05, 1.1896853720827494e-05, 1.1896853720827494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1896853720827494e-05

Optimization complete. Final v2v error: 2.882378101348877 mm

Highest mean error: 3.0448155403137207 mm for frame 168

Lowest mean error: 2.7178170680999756 mm for frame 48

Saving results

Total time: 36.03098654747009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378011
Iteration 2/25 | Loss: 0.00098714
Iteration 3/25 | Loss: 0.00072514
Iteration 4/25 | Loss: 0.00068095
Iteration 5/25 | Loss: 0.00067192
Iteration 6/25 | Loss: 0.00066914
Iteration 7/25 | Loss: 0.00066840
Iteration 8/25 | Loss: 0.00066835
Iteration 9/25 | Loss: 0.00066835
Iteration 10/25 | Loss: 0.00066834
Iteration 11/25 | Loss: 0.00066834
Iteration 12/25 | Loss: 0.00066834
Iteration 13/25 | Loss: 0.00066834
Iteration 14/25 | Loss: 0.00066834
Iteration 15/25 | Loss: 0.00066834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006683447281830013, 0.0006683447281830013, 0.0006683447281830013, 0.0006683447281830013, 0.0006683447281830013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006683447281830013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57084417
Iteration 2/25 | Loss: 0.00028732
Iteration 3/25 | Loss: 0.00028731
Iteration 4/25 | Loss: 0.00028731
Iteration 5/25 | Loss: 0.00028731
Iteration 6/25 | Loss: 0.00028731
Iteration 7/25 | Loss: 0.00028731
Iteration 8/25 | Loss: 0.00028731
Iteration 9/25 | Loss: 0.00028731
Iteration 10/25 | Loss: 0.00028731
Iteration 11/25 | Loss: 0.00028731
Iteration 12/25 | Loss: 0.00028731
Iteration 13/25 | Loss: 0.00028731
Iteration 14/25 | Loss: 0.00028731
Iteration 15/25 | Loss: 0.00028731
Iteration 16/25 | Loss: 0.00028731
Iteration 17/25 | Loss: 0.00028731
Iteration 18/25 | Loss: 0.00028731
Iteration 19/25 | Loss: 0.00028731
Iteration 20/25 | Loss: 0.00028731
Iteration 21/25 | Loss: 0.00028731
Iteration 22/25 | Loss: 0.00028731
Iteration 23/25 | Loss: 0.00028731
Iteration 24/25 | Loss: 0.00028731
Iteration 25/25 | Loss: 0.00028731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028731
Iteration 2/1000 | Loss: 0.00003245
Iteration 3/1000 | Loss: 0.00002144
Iteration 4/1000 | Loss: 0.00001959
Iteration 5/1000 | Loss: 0.00001885
Iteration 6/1000 | Loss: 0.00001813
Iteration 7/1000 | Loss: 0.00001775
Iteration 8/1000 | Loss: 0.00001728
Iteration 9/1000 | Loss: 0.00001699
Iteration 10/1000 | Loss: 0.00001681
Iteration 11/1000 | Loss: 0.00001666
Iteration 12/1000 | Loss: 0.00001664
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001646
Iteration 18/1000 | Loss: 0.00001641
Iteration 19/1000 | Loss: 0.00001641
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001641
Iteration 23/1000 | Loss: 0.00001640
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001637
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001636
Iteration 35/1000 | Loss: 0.00001635
Iteration 36/1000 | Loss: 0.00001634
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001632
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001630
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001619
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001619
Iteration 52/1000 | Loss: 0.00001618
Iteration 53/1000 | Loss: 0.00001618
Iteration 54/1000 | Loss: 0.00001618
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001610
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001610
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001609
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001608
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001607
Iteration 125/1000 | Loss: 0.00001607
Iteration 126/1000 | Loss: 0.00001607
Iteration 127/1000 | Loss: 0.00001607
Iteration 128/1000 | Loss: 0.00001607
Iteration 129/1000 | Loss: 0.00001606
Iteration 130/1000 | Loss: 0.00001606
Iteration 131/1000 | Loss: 0.00001606
Iteration 132/1000 | Loss: 0.00001606
Iteration 133/1000 | Loss: 0.00001606
Iteration 134/1000 | Loss: 0.00001606
Iteration 135/1000 | Loss: 0.00001606
Iteration 136/1000 | Loss: 0.00001606
Iteration 137/1000 | Loss: 0.00001606
Iteration 138/1000 | Loss: 0.00001606
Iteration 139/1000 | Loss: 0.00001606
Iteration 140/1000 | Loss: 0.00001606
Iteration 141/1000 | Loss: 0.00001606
Iteration 142/1000 | Loss: 0.00001606
Iteration 143/1000 | Loss: 0.00001606
Iteration 144/1000 | Loss: 0.00001606
Iteration 145/1000 | Loss: 0.00001605
Iteration 146/1000 | Loss: 0.00001605
Iteration 147/1000 | Loss: 0.00001605
Iteration 148/1000 | Loss: 0.00001605
Iteration 149/1000 | Loss: 0.00001605
Iteration 150/1000 | Loss: 0.00001605
Iteration 151/1000 | Loss: 0.00001605
Iteration 152/1000 | Loss: 0.00001605
Iteration 153/1000 | Loss: 0.00001605
Iteration 154/1000 | Loss: 0.00001605
Iteration 155/1000 | Loss: 0.00001605
Iteration 156/1000 | Loss: 0.00001605
Iteration 157/1000 | Loss: 0.00001604
Iteration 158/1000 | Loss: 0.00001604
Iteration 159/1000 | Loss: 0.00001604
Iteration 160/1000 | Loss: 0.00001604
Iteration 161/1000 | Loss: 0.00001604
Iteration 162/1000 | Loss: 0.00001604
Iteration 163/1000 | Loss: 0.00001604
Iteration 164/1000 | Loss: 0.00001604
Iteration 165/1000 | Loss: 0.00001604
Iteration 166/1000 | Loss: 0.00001604
Iteration 167/1000 | Loss: 0.00001604
Iteration 168/1000 | Loss: 0.00001604
Iteration 169/1000 | Loss: 0.00001604
Iteration 170/1000 | Loss: 0.00001604
Iteration 171/1000 | Loss: 0.00001604
Iteration 172/1000 | Loss: 0.00001604
Iteration 173/1000 | Loss: 0.00001604
Iteration 174/1000 | Loss: 0.00001604
Iteration 175/1000 | Loss: 0.00001604
Iteration 176/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.6042475181166083e-05, 1.6042475181166083e-05, 1.6042475181166083e-05, 1.6042475181166083e-05, 1.6042475181166083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6042475181166083e-05

Optimization complete. Final v2v error: 3.3612027168273926 mm

Highest mean error: 3.6467835903167725 mm for frame 110

Lowest mean error: 3.086501359939575 mm for frame 14

Saving results

Total time: 41.815603256225586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832183
Iteration 2/25 | Loss: 0.00103520
Iteration 3/25 | Loss: 0.00076075
Iteration 4/25 | Loss: 0.00069203
Iteration 5/25 | Loss: 0.00067348
Iteration 6/25 | Loss: 0.00066769
Iteration 7/25 | Loss: 0.00066617
Iteration 8/25 | Loss: 0.00066580
Iteration 9/25 | Loss: 0.00066580
Iteration 10/25 | Loss: 0.00066580
Iteration 11/25 | Loss: 0.00066580
Iteration 12/25 | Loss: 0.00066580
Iteration 13/25 | Loss: 0.00066580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006657952326349914, 0.0006657952326349914, 0.0006657952326349914, 0.0006657952326349914, 0.0006657952326349914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006657952326349914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53967392
Iteration 2/25 | Loss: 0.00039791
Iteration 3/25 | Loss: 0.00039791
Iteration 4/25 | Loss: 0.00039791
Iteration 5/25 | Loss: 0.00039791
Iteration 6/25 | Loss: 0.00039791
Iteration 7/25 | Loss: 0.00039791
Iteration 8/25 | Loss: 0.00039791
Iteration 9/25 | Loss: 0.00039791
Iteration 10/25 | Loss: 0.00039791
Iteration 11/25 | Loss: 0.00039791
Iteration 12/25 | Loss: 0.00039791
Iteration 13/25 | Loss: 0.00039791
Iteration 14/25 | Loss: 0.00039791
Iteration 15/25 | Loss: 0.00039791
Iteration 16/25 | Loss: 0.00039791
Iteration 17/25 | Loss: 0.00039791
Iteration 18/25 | Loss: 0.00039791
Iteration 19/25 | Loss: 0.00039791
Iteration 20/25 | Loss: 0.00039791
Iteration 21/25 | Loss: 0.00039791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00039790928713046014, 0.00039790928713046014, 0.00039790928713046014, 0.00039790928713046014, 0.00039790928713046014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039790928713046014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039791
Iteration 2/1000 | Loss: 0.00002833
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001476
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001211
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001154
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001125
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001122
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001115
Iteration 24/1000 | Loss: 0.00001114
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001112
Iteration 29/1000 | Loss: 0.00001111
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001109
Iteration 33/1000 | Loss: 0.00001109
Iteration 34/1000 | Loss: 0.00001108
Iteration 35/1000 | Loss: 0.00001108
Iteration 36/1000 | Loss: 0.00001107
Iteration 37/1000 | Loss: 0.00001107
Iteration 38/1000 | Loss: 0.00001107
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001106
Iteration 42/1000 | Loss: 0.00001106
Iteration 43/1000 | Loss: 0.00001106
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001105
Iteration 49/1000 | Loss: 0.00001105
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001102
Iteration 61/1000 | Loss: 0.00001101
Iteration 62/1000 | Loss: 0.00001101
Iteration 63/1000 | Loss: 0.00001101
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001101
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001100
Iteration 69/1000 | Loss: 0.00001100
Iteration 70/1000 | Loss: 0.00001100
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001099
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001098
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001096
Iteration 91/1000 | Loss: 0.00001096
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001096
Iteration 100/1000 | Loss: 0.00001096
Iteration 101/1000 | Loss: 0.00001095
Iteration 102/1000 | Loss: 0.00001095
Iteration 103/1000 | Loss: 0.00001095
Iteration 104/1000 | Loss: 0.00001095
Iteration 105/1000 | Loss: 0.00001095
Iteration 106/1000 | Loss: 0.00001095
Iteration 107/1000 | Loss: 0.00001095
Iteration 108/1000 | Loss: 0.00001095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.0952595403068699e-05, 1.0952595403068699e-05, 1.0952595403068699e-05, 1.0952595403068699e-05, 1.0952595403068699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0952595403068699e-05

Optimization complete. Final v2v error: 2.8162338733673096 mm

Highest mean error: 3.4033401012420654 mm for frame 46

Lowest mean error: 2.5316162109375 mm for frame 175

Saving results

Total time: 38.72272753715515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045794
Iteration 2/25 | Loss: 0.00221605
Iteration 3/25 | Loss: 0.00128872
Iteration 4/25 | Loss: 0.00125212
Iteration 5/25 | Loss: 0.00115869
Iteration 6/25 | Loss: 0.00100421
Iteration 7/25 | Loss: 0.00095862
Iteration 8/25 | Loss: 0.00093951
Iteration 9/25 | Loss: 0.00093324
Iteration 10/25 | Loss: 0.00093588
Iteration 11/25 | Loss: 0.00091667
Iteration 12/25 | Loss: 0.00089745
Iteration 13/25 | Loss: 0.00089163
Iteration 14/25 | Loss: 0.00089518
Iteration 15/25 | Loss: 0.00088609
Iteration 16/25 | Loss: 0.00088431
Iteration 17/25 | Loss: 0.00087438
Iteration 18/25 | Loss: 0.00087563
Iteration 19/25 | Loss: 0.00086643
Iteration 20/25 | Loss: 0.00085733
Iteration 21/25 | Loss: 0.00085661
Iteration 22/25 | Loss: 0.00086736
Iteration 23/25 | Loss: 0.00086596
Iteration 24/25 | Loss: 0.00085493
Iteration 25/25 | Loss: 0.00084866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47637939
Iteration 2/25 | Loss: 0.00199890
Iteration 3/25 | Loss: 0.00133414
Iteration 4/25 | Loss: 0.00133414
Iteration 5/25 | Loss: 0.00133414
Iteration 6/25 | Loss: 0.00133414
Iteration 7/25 | Loss: 0.00133414
Iteration 8/25 | Loss: 0.00133413
Iteration 9/25 | Loss: 0.00133413
Iteration 10/25 | Loss: 0.00133413
Iteration 11/25 | Loss: 0.00133413
Iteration 12/25 | Loss: 0.00133413
Iteration 13/25 | Loss: 0.00133413
Iteration 14/25 | Loss: 0.00133413
Iteration 15/25 | Loss: 0.00133413
Iteration 16/25 | Loss: 0.00133413
Iteration 17/25 | Loss: 0.00133413
Iteration 18/25 | Loss: 0.00133413
Iteration 19/25 | Loss: 0.00133413
Iteration 20/25 | Loss: 0.00133413
Iteration 21/25 | Loss: 0.00133413
Iteration 22/25 | Loss: 0.00133413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013341340236365795, 0.0013341340236365795, 0.0013341340236365795, 0.0013341340236365795, 0.0013341340236365795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013341340236365795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133413
Iteration 2/1000 | Loss: 0.00023616
Iteration 3/1000 | Loss: 0.00177444
Iteration 4/1000 | Loss: 0.00046596
Iteration 5/1000 | Loss: 0.00086635
Iteration 6/1000 | Loss: 0.00012647
Iteration 7/1000 | Loss: 0.00012044
Iteration 8/1000 | Loss: 0.00071464
Iteration 9/1000 | Loss: 0.00038303
Iteration 10/1000 | Loss: 0.00029209
Iteration 11/1000 | Loss: 0.00012833
Iteration 12/1000 | Loss: 0.00010726
Iteration 13/1000 | Loss: 0.00012377
Iteration 14/1000 | Loss: 0.00012027
Iteration 15/1000 | Loss: 0.00029010
Iteration 16/1000 | Loss: 0.00021037
Iteration 17/1000 | Loss: 0.00145615
Iteration 18/1000 | Loss: 0.00122182
Iteration 19/1000 | Loss: 0.00225399
Iteration 20/1000 | Loss: 0.00509963
Iteration 21/1000 | Loss: 0.00135670
Iteration 22/1000 | Loss: 0.00097991
Iteration 23/1000 | Loss: 0.00025037
Iteration 24/1000 | Loss: 0.00114306
Iteration 25/1000 | Loss: 0.00007719
Iteration 26/1000 | Loss: 0.00029960
Iteration 27/1000 | Loss: 0.00009774
Iteration 28/1000 | Loss: 0.00047050
Iteration 29/1000 | Loss: 0.00041956
Iteration 30/1000 | Loss: 0.00041723
Iteration 31/1000 | Loss: 0.00010089
Iteration 32/1000 | Loss: 0.00005292
Iteration 33/1000 | Loss: 0.00004464
Iteration 34/1000 | Loss: 0.00058470
Iteration 35/1000 | Loss: 0.00022875
Iteration 36/1000 | Loss: 0.00017518
Iteration 37/1000 | Loss: 0.00025799
Iteration 38/1000 | Loss: 0.00012098
Iteration 39/1000 | Loss: 0.00003745
Iteration 40/1000 | Loss: 0.00003687
Iteration 41/1000 | Loss: 0.00010420
Iteration 42/1000 | Loss: 0.00002809
Iteration 43/1000 | Loss: 0.00002788
Iteration 44/1000 | Loss: 0.00002274
Iteration 45/1000 | Loss: 0.00002031
Iteration 46/1000 | Loss: 0.00032156
Iteration 47/1000 | Loss: 0.00160281
Iteration 48/1000 | Loss: 0.00163645
Iteration 49/1000 | Loss: 0.00083365
Iteration 50/1000 | Loss: 0.00034472
Iteration 51/1000 | Loss: 0.00044643
Iteration 52/1000 | Loss: 0.00009712
Iteration 53/1000 | Loss: 0.00002838
Iteration 54/1000 | Loss: 0.00023767
Iteration 55/1000 | Loss: 0.00015393
Iteration 56/1000 | Loss: 0.00043048
Iteration 57/1000 | Loss: 0.00005680
Iteration 58/1000 | Loss: 0.00019417
Iteration 59/1000 | Loss: 0.00003763
Iteration 60/1000 | Loss: 0.00003844
Iteration 61/1000 | Loss: 0.00003075
Iteration 62/1000 | Loss: 0.00003297
Iteration 63/1000 | Loss: 0.00002891
Iteration 64/1000 | Loss: 0.00002827
Iteration 65/1000 | Loss: 0.00002504
Iteration 66/1000 | Loss: 0.00002768
Iteration 67/1000 | Loss: 0.00002835
Iteration 68/1000 | Loss: 0.00002658
Iteration 69/1000 | Loss: 0.00002682
Iteration 70/1000 | Loss: 0.00002888
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00001793
Iteration 73/1000 | Loss: 0.00001669
Iteration 74/1000 | Loss: 0.00001620
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001610
Iteration 77/1000 | Loss: 0.00001610
Iteration 78/1000 | Loss: 0.00001601
Iteration 79/1000 | Loss: 0.00001596
Iteration 80/1000 | Loss: 0.00001594
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001580
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001569
Iteration 89/1000 | Loss: 0.00001568
Iteration 90/1000 | Loss: 0.00001568
Iteration 91/1000 | Loss: 0.00001567
Iteration 92/1000 | Loss: 0.00001567
Iteration 93/1000 | Loss: 0.00001566
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001565
Iteration 96/1000 | Loss: 0.00001565
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001565
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001562
Iteration 106/1000 | Loss: 0.00001562
Iteration 107/1000 | Loss: 0.00001562
Iteration 108/1000 | Loss: 0.00001561
Iteration 109/1000 | Loss: 0.00001561
Iteration 110/1000 | Loss: 0.00001561
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001556
Iteration 119/1000 | Loss: 0.00001556
Iteration 120/1000 | Loss: 0.00001556
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001555
Iteration 123/1000 | Loss: 0.00001555
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001552
Iteration 130/1000 | Loss: 0.00001552
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001551
Iteration 134/1000 | Loss: 0.00001551
Iteration 135/1000 | Loss: 0.00001551
Iteration 136/1000 | Loss: 0.00001551
Iteration 137/1000 | Loss: 0.00001551
Iteration 138/1000 | Loss: 0.00001550
Iteration 139/1000 | Loss: 0.00001550
Iteration 140/1000 | Loss: 0.00001550
Iteration 141/1000 | Loss: 0.00001550
Iteration 142/1000 | Loss: 0.00001550
Iteration 143/1000 | Loss: 0.00001550
Iteration 144/1000 | Loss: 0.00001550
Iteration 145/1000 | Loss: 0.00001549
Iteration 146/1000 | Loss: 0.00001549
Iteration 147/1000 | Loss: 0.00001549
Iteration 148/1000 | Loss: 0.00001549
Iteration 149/1000 | Loss: 0.00001549
Iteration 150/1000 | Loss: 0.00001549
Iteration 151/1000 | Loss: 0.00001549
Iteration 152/1000 | Loss: 0.00001548
Iteration 153/1000 | Loss: 0.00001548
Iteration 154/1000 | Loss: 0.00001548
Iteration 155/1000 | Loss: 0.00001548
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001547
Iteration 159/1000 | Loss: 0.00001546
Iteration 160/1000 | Loss: 0.00001546
Iteration 161/1000 | Loss: 0.00001546
Iteration 162/1000 | Loss: 0.00001546
Iteration 163/1000 | Loss: 0.00001546
Iteration 164/1000 | Loss: 0.00001546
Iteration 165/1000 | Loss: 0.00001546
Iteration 166/1000 | Loss: 0.00001546
Iteration 167/1000 | Loss: 0.00001546
Iteration 168/1000 | Loss: 0.00001546
Iteration 169/1000 | Loss: 0.00001546
Iteration 170/1000 | Loss: 0.00001546
Iteration 171/1000 | Loss: 0.00001546
Iteration 172/1000 | Loss: 0.00001546
Iteration 173/1000 | Loss: 0.00001546
Iteration 174/1000 | Loss: 0.00001546
Iteration 175/1000 | Loss: 0.00001546
Iteration 176/1000 | Loss: 0.00001546
Iteration 177/1000 | Loss: 0.00001546
Iteration 178/1000 | Loss: 0.00001546
Iteration 179/1000 | Loss: 0.00001546
Iteration 180/1000 | Loss: 0.00001546
Iteration 181/1000 | Loss: 0.00001546
Iteration 182/1000 | Loss: 0.00001546
Iteration 183/1000 | Loss: 0.00001546
Iteration 184/1000 | Loss: 0.00001546
Iteration 185/1000 | Loss: 0.00001546
Iteration 186/1000 | Loss: 0.00001546
Iteration 187/1000 | Loss: 0.00001546
Iteration 188/1000 | Loss: 0.00001546
Iteration 189/1000 | Loss: 0.00001546
Iteration 190/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.5457253539352678e-05, 1.5457253539352678e-05, 1.5457253539352678e-05, 1.5457253539352678e-05, 1.5457253539352678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5457253539352678e-05

Optimization complete. Final v2v error: 3.035022020339966 mm

Highest mean error: 7.368861675262451 mm for frame 77

Lowest mean error: 2.608466148376465 mm for frame 111

Saving results

Total time: 164.28660774230957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021835
Iteration 2/25 | Loss: 0.01021835
Iteration 3/25 | Loss: 0.01021835
Iteration 4/25 | Loss: 0.00400174
Iteration 5/25 | Loss: 0.00240472
Iteration 6/25 | Loss: 0.00179726
Iteration 7/25 | Loss: 0.00164955
Iteration 8/25 | Loss: 0.00160032
Iteration 9/25 | Loss: 0.00158254
Iteration 10/25 | Loss: 0.00150164
Iteration 11/25 | Loss: 0.00142364
Iteration 12/25 | Loss: 0.00140185
Iteration 13/25 | Loss: 0.00136638
Iteration 14/25 | Loss: 0.00130389
Iteration 15/25 | Loss: 0.00126961
Iteration 16/25 | Loss: 0.00125077
Iteration 17/25 | Loss: 0.00123756
Iteration 18/25 | Loss: 0.00123136
Iteration 19/25 | Loss: 0.00122343
Iteration 20/25 | Loss: 0.00121665
Iteration 21/25 | Loss: 0.00121671
Iteration 22/25 | Loss: 0.00121626
Iteration 23/25 | Loss: 0.00121634
Iteration 24/25 | Loss: 0.00121486
Iteration 25/25 | Loss: 0.00121217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43381703
Iteration 2/25 | Loss: 0.00451933
Iteration 3/25 | Loss: 0.00451933
Iteration 4/25 | Loss: 0.00451933
Iteration 5/25 | Loss: 0.00451933
Iteration 6/25 | Loss: 0.00451933
Iteration 7/25 | Loss: 0.00451933
Iteration 8/25 | Loss: 0.00451933
Iteration 9/25 | Loss: 0.00451933
Iteration 10/25 | Loss: 0.00451933
Iteration 11/25 | Loss: 0.00451933
Iteration 12/25 | Loss: 0.00451933
Iteration 13/25 | Loss: 0.00451933
Iteration 14/25 | Loss: 0.00451933
Iteration 15/25 | Loss: 0.00451933
Iteration 16/25 | Loss: 0.00451933
Iteration 17/25 | Loss: 0.00451933
Iteration 18/25 | Loss: 0.00451933
Iteration 19/25 | Loss: 0.00451933
Iteration 20/25 | Loss: 0.00451933
Iteration 21/25 | Loss: 0.00451933
Iteration 22/25 | Loss: 0.00451933
Iteration 23/25 | Loss: 0.00451933
Iteration 24/25 | Loss: 0.00451933
Iteration 25/25 | Loss: 0.00451933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00451933
Iteration 2/1000 | Loss: 0.00092004
Iteration 3/1000 | Loss: 0.00085836
Iteration 4/1000 | Loss: 0.00102684
Iteration 5/1000 | Loss: 0.00109409
Iteration 6/1000 | Loss: 0.00164415
Iteration 7/1000 | Loss: 0.01291895
Iteration 8/1000 | Loss: 0.00363488
Iteration 9/1000 | Loss: 0.00887194
Iteration 10/1000 | Loss: 0.00212214
Iteration 11/1000 | Loss: 0.00356993
Iteration 12/1000 | Loss: 0.00084770
Iteration 13/1000 | Loss: 0.00058168
Iteration 14/1000 | Loss: 0.00131730
Iteration 15/1000 | Loss: 0.00074540
Iteration 16/1000 | Loss: 0.00062955
Iteration 17/1000 | Loss: 0.00121710
Iteration 18/1000 | Loss: 0.00105012
Iteration 19/1000 | Loss: 0.00095203
Iteration 20/1000 | Loss: 0.00026883
Iteration 21/1000 | Loss: 0.00128037
Iteration 22/1000 | Loss: 0.00102121
Iteration 23/1000 | Loss: 0.00050598
Iteration 24/1000 | Loss: 0.00060075
Iteration 25/1000 | Loss: 0.00020553
Iteration 26/1000 | Loss: 0.00044960
Iteration 27/1000 | Loss: 0.00045272
Iteration 28/1000 | Loss: 0.00035039
Iteration 29/1000 | Loss: 0.00011672
Iteration 30/1000 | Loss: 0.00036258
Iteration 31/1000 | Loss: 0.00056979
Iteration 32/1000 | Loss: 0.00029019
Iteration 33/1000 | Loss: 0.00066507
Iteration 34/1000 | Loss: 0.00199045
Iteration 35/1000 | Loss: 0.00037763
Iteration 36/1000 | Loss: 0.00037477
Iteration 37/1000 | Loss: 0.00039786
Iteration 38/1000 | Loss: 0.00059208
Iteration 39/1000 | Loss: 0.00022397
Iteration 40/1000 | Loss: 0.00037060
Iteration 41/1000 | Loss: 0.00017325
Iteration 42/1000 | Loss: 0.00012036
Iteration 43/1000 | Loss: 0.00029865
Iteration 44/1000 | Loss: 0.00036567
Iteration 45/1000 | Loss: 0.00040972
Iteration 46/1000 | Loss: 0.00032661
Iteration 47/1000 | Loss: 0.00011693
Iteration 48/1000 | Loss: 0.00018797
Iteration 49/1000 | Loss: 0.00016254
Iteration 50/1000 | Loss: 0.00034964
Iteration 51/1000 | Loss: 0.00057514
Iteration 52/1000 | Loss: 0.00033965
Iteration 53/1000 | Loss: 0.00005966
Iteration 54/1000 | Loss: 0.00018647
Iteration 55/1000 | Loss: 0.00085447
Iteration 56/1000 | Loss: 0.00011218
Iteration 57/1000 | Loss: 0.00004175
Iteration 58/1000 | Loss: 0.00008253
Iteration 59/1000 | Loss: 0.00019323
Iteration 60/1000 | Loss: 0.00004527
Iteration 61/1000 | Loss: 0.00047095
Iteration 62/1000 | Loss: 0.00022828
Iteration 63/1000 | Loss: 0.00036305
Iteration 64/1000 | Loss: 0.00009040
Iteration 65/1000 | Loss: 0.00003928
Iteration 66/1000 | Loss: 0.00002846
Iteration 67/1000 | Loss: 0.00002649
Iteration 68/1000 | Loss: 0.00018990
Iteration 69/1000 | Loss: 0.00015305
Iteration 70/1000 | Loss: 0.00002817
Iteration 71/1000 | Loss: 0.00002575
Iteration 72/1000 | Loss: 0.00002482
Iteration 73/1000 | Loss: 0.00002351
Iteration 74/1000 | Loss: 0.00002281
Iteration 75/1000 | Loss: 0.00022563
Iteration 76/1000 | Loss: 0.00010110
Iteration 77/1000 | Loss: 0.00003862
Iteration 78/1000 | Loss: 0.00003199
Iteration 79/1000 | Loss: 0.00002419
Iteration 80/1000 | Loss: 0.00002299
Iteration 81/1000 | Loss: 0.00002229
Iteration 82/1000 | Loss: 0.00002172
Iteration 83/1000 | Loss: 0.00002120
Iteration 84/1000 | Loss: 0.00002086
Iteration 85/1000 | Loss: 0.00002075
Iteration 86/1000 | Loss: 0.00002071
Iteration 87/1000 | Loss: 0.00002069
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002068
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00002067
Iteration 93/1000 | Loss: 0.00002067
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002061
Iteration 97/1000 | Loss: 0.00002061
Iteration 98/1000 | Loss: 0.00002058
Iteration 99/1000 | Loss: 0.00002055
Iteration 100/1000 | Loss: 0.00002054
Iteration 101/1000 | Loss: 0.00002054
Iteration 102/1000 | Loss: 0.00002053
Iteration 103/1000 | Loss: 0.00002052
Iteration 104/1000 | Loss: 0.00002052
Iteration 105/1000 | Loss: 0.00002052
Iteration 106/1000 | Loss: 0.00002051
Iteration 107/1000 | Loss: 0.00002051
Iteration 108/1000 | Loss: 0.00002051
Iteration 109/1000 | Loss: 0.00002051
Iteration 110/1000 | Loss: 0.00002051
Iteration 111/1000 | Loss: 0.00002051
Iteration 112/1000 | Loss: 0.00002051
Iteration 113/1000 | Loss: 0.00002050
Iteration 114/1000 | Loss: 0.00002050
Iteration 115/1000 | Loss: 0.00002050
Iteration 116/1000 | Loss: 0.00002050
Iteration 117/1000 | Loss: 0.00002049
Iteration 118/1000 | Loss: 0.00002049
Iteration 119/1000 | Loss: 0.00002049
Iteration 120/1000 | Loss: 0.00002049
Iteration 121/1000 | Loss: 0.00002048
Iteration 122/1000 | Loss: 0.00002048
Iteration 123/1000 | Loss: 0.00002048
Iteration 124/1000 | Loss: 0.00002048
Iteration 125/1000 | Loss: 0.00002048
Iteration 126/1000 | Loss: 0.00002047
Iteration 127/1000 | Loss: 0.00002047
Iteration 128/1000 | Loss: 0.00002047
Iteration 129/1000 | Loss: 0.00002046
Iteration 130/1000 | Loss: 0.00002046
Iteration 131/1000 | Loss: 0.00002046
Iteration 132/1000 | Loss: 0.00002046
Iteration 133/1000 | Loss: 0.00002045
Iteration 134/1000 | Loss: 0.00002045
Iteration 135/1000 | Loss: 0.00002045
Iteration 136/1000 | Loss: 0.00002045
Iteration 137/1000 | Loss: 0.00002045
Iteration 138/1000 | Loss: 0.00002044
Iteration 139/1000 | Loss: 0.00002044
Iteration 140/1000 | Loss: 0.00002044
Iteration 141/1000 | Loss: 0.00002044
Iteration 142/1000 | Loss: 0.00002044
Iteration 143/1000 | Loss: 0.00002044
Iteration 144/1000 | Loss: 0.00002044
Iteration 145/1000 | Loss: 0.00002043
Iteration 146/1000 | Loss: 0.00002043
Iteration 147/1000 | Loss: 0.00002043
Iteration 148/1000 | Loss: 0.00002043
Iteration 149/1000 | Loss: 0.00002043
Iteration 150/1000 | Loss: 0.00002043
Iteration 151/1000 | Loss: 0.00002043
Iteration 152/1000 | Loss: 0.00002042
Iteration 153/1000 | Loss: 0.00002042
Iteration 154/1000 | Loss: 0.00002042
Iteration 155/1000 | Loss: 0.00002042
Iteration 156/1000 | Loss: 0.00002042
Iteration 157/1000 | Loss: 0.00002042
Iteration 158/1000 | Loss: 0.00002042
Iteration 159/1000 | Loss: 0.00002042
Iteration 160/1000 | Loss: 0.00002042
Iteration 161/1000 | Loss: 0.00002042
Iteration 162/1000 | Loss: 0.00002042
Iteration 163/1000 | Loss: 0.00002041
Iteration 164/1000 | Loss: 0.00002041
Iteration 165/1000 | Loss: 0.00002041
Iteration 166/1000 | Loss: 0.00002041
Iteration 167/1000 | Loss: 0.00002041
Iteration 168/1000 | Loss: 0.00002041
Iteration 169/1000 | Loss: 0.00002041
Iteration 170/1000 | Loss: 0.00002040
Iteration 171/1000 | Loss: 0.00002040
Iteration 172/1000 | Loss: 0.00002040
Iteration 173/1000 | Loss: 0.00002040
Iteration 174/1000 | Loss: 0.00002040
Iteration 175/1000 | Loss: 0.00002040
Iteration 176/1000 | Loss: 0.00002040
Iteration 177/1000 | Loss: 0.00002040
Iteration 178/1000 | Loss: 0.00002040
Iteration 179/1000 | Loss: 0.00002040
Iteration 180/1000 | Loss: 0.00002040
Iteration 181/1000 | Loss: 0.00002040
Iteration 182/1000 | Loss: 0.00002040
Iteration 183/1000 | Loss: 0.00002040
Iteration 184/1000 | Loss: 0.00002040
Iteration 185/1000 | Loss: 0.00002040
Iteration 186/1000 | Loss: 0.00002040
Iteration 187/1000 | Loss: 0.00002040
Iteration 188/1000 | Loss: 0.00002040
Iteration 189/1000 | Loss: 0.00002040
Iteration 190/1000 | Loss: 0.00002040
Iteration 191/1000 | Loss: 0.00002040
Iteration 192/1000 | Loss: 0.00002040
Iteration 193/1000 | Loss: 0.00002040
Iteration 194/1000 | Loss: 0.00002040
Iteration 195/1000 | Loss: 0.00002040
Iteration 196/1000 | Loss: 0.00002040
Iteration 197/1000 | Loss: 0.00002040
Iteration 198/1000 | Loss: 0.00002040
Iteration 199/1000 | Loss: 0.00002040
Iteration 200/1000 | Loss: 0.00002040
Iteration 201/1000 | Loss: 0.00002040
Iteration 202/1000 | Loss: 0.00002040
Iteration 203/1000 | Loss: 0.00002040
Iteration 204/1000 | Loss: 0.00002040
Iteration 205/1000 | Loss: 0.00002040
Iteration 206/1000 | Loss: 0.00002040
Iteration 207/1000 | Loss: 0.00002040
Iteration 208/1000 | Loss: 0.00002040
Iteration 209/1000 | Loss: 0.00002040
Iteration 210/1000 | Loss: 0.00002040
Iteration 211/1000 | Loss: 0.00002040
Iteration 212/1000 | Loss: 0.00002040
Iteration 213/1000 | Loss: 0.00002040
Iteration 214/1000 | Loss: 0.00002040
Iteration 215/1000 | Loss: 0.00002040
Iteration 216/1000 | Loss: 0.00002040
Iteration 217/1000 | Loss: 0.00002040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.0398778360686265e-05, 2.0398778360686265e-05, 2.0398778360686265e-05, 2.0398778360686265e-05, 2.0398778360686265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0398778360686265e-05

Optimization complete. Final v2v error: 3.8670332431793213 mm

Highest mean error: 4.957026958465576 mm for frame 0

Lowest mean error: 3.4589791297912598 mm for frame 6

Saving results

Total time: 192.59413146972656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408891
Iteration 2/25 | Loss: 0.00084444
Iteration 3/25 | Loss: 0.00070548
Iteration 4/25 | Loss: 0.00068367
Iteration 5/25 | Loss: 0.00067763
Iteration 6/25 | Loss: 0.00067627
Iteration 7/25 | Loss: 0.00067594
Iteration 8/25 | Loss: 0.00067594
Iteration 9/25 | Loss: 0.00067594
Iteration 10/25 | Loss: 0.00067594
Iteration 11/25 | Loss: 0.00067594
Iteration 12/25 | Loss: 0.00067594
Iteration 13/25 | Loss: 0.00067594
Iteration 14/25 | Loss: 0.00067594
Iteration 15/25 | Loss: 0.00067594
Iteration 16/25 | Loss: 0.00067594
Iteration 17/25 | Loss: 0.00067594
Iteration 18/25 | Loss: 0.00067594
Iteration 19/25 | Loss: 0.00067594
Iteration 20/25 | Loss: 0.00067594
Iteration 21/25 | Loss: 0.00067594
Iteration 22/25 | Loss: 0.00067594
Iteration 23/25 | Loss: 0.00067594
Iteration 24/25 | Loss: 0.00067594
Iteration 25/25 | Loss: 0.00067594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.84771061
Iteration 2/25 | Loss: 0.00033338
Iteration 3/25 | Loss: 0.00033337
Iteration 4/25 | Loss: 0.00033336
Iteration 5/25 | Loss: 0.00033336
Iteration 6/25 | Loss: 0.00033336
Iteration 7/25 | Loss: 0.00033336
Iteration 8/25 | Loss: 0.00033336
Iteration 9/25 | Loss: 0.00033336
Iteration 10/25 | Loss: 0.00033336
Iteration 11/25 | Loss: 0.00033336
Iteration 12/25 | Loss: 0.00033336
Iteration 13/25 | Loss: 0.00033336
Iteration 14/25 | Loss: 0.00033336
Iteration 15/25 | Loss: 0.00033336
Iteration 16/25 | Loss: 0.00033336
Iteration 17/25 | Loss: 0.00033336
Iteration 18/25 | Loss: 0.00033336
Iteration 19/25 | Loss: 0.00033336
Iteration 20/25 | Loss: 0.00033336
Iteration 21/25 | Loss: 0.00033336
Iteration 22/25 | Loss: 0.00033336
Iteration 23/25 | Loss: 0.00033336
Iteration 24/25 | Loss: 0.00033336
Iteration 25/25 | Loss: 0.00033336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033336
Iteration 2/1000 | Loss: 0.00002519
Iteration 3/1000 | Loss: 0.00001745
Iteration 4/1000 | Loss: 0.00001627
Iteration 5/1000 | Loss: 0.00001539
Iteration 6/1000 | Loss: 0.00001482
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001404
Iteration 10/1000 | Loss: 0.00001396
Iteration 11/1000 | Loss: 0.00001396
Iteration 12/1000 | Loss: 0.00001392
Iteration 13/1000 | Loss: 0.00001388
Iteration 14/1000 | Loss: 0.00001386
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001362
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001361
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001359
Iteration 35/1000 | Loss: 0.00001358
Iteration 36/1000 | Loss: 0.00001358
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001358
Iteration 40/1000 | Loss: 0.00001358
Iteration 41/1000 | Loss: 0.00001358
Iteration 42/1000 | Loss: 0.00001358
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001358
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001357
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001357
Iteration 50/1000 | Loss: 0.00001356
Iteration 51/1000 | Loss: 0.00001356
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001356
Iteration 54/1000 | Loss: 0.00001356
Iteration 55/1000 | Loss: 0.00001355
Iteration 56/1000 | Loss: 0.00001355
Iteration 57/1000 | Loss: 0.00001355
Iteration 58/1000 | Loss: 0.00001354
Iteration 59/1000 | Loss: 0.00001354
Iteration 60/1000 | Loss: 0.00001354
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001348
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001346
Iteration 87/1000 | Loss: 0.00001346
Iteration 88/1000 | Loss: 0.00001346
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001345
Iteration 92/1000 | Loss: 0.00001344
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001343
Iteration 95/1000 | Loss: 0.00001343
Iteration 96/1000 | Loss: 0.00001343
Iteration 97/1000 | Loss: 0.00001343
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001342
Iteration 101/1000 | Loss: 0.00001342
Iteration 102/1000 | Loss: 0.00001342
Iteration 103/1000 | Loss: 0.00001342
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001341
Iteration 107/1000 | Loss: 0.00001341
Iteration 108/1000 | Loss: 0.00001341
Iteration 109/1000 | Loss: 0.00001341
Iteration 110/1000 | Loss: 0.00001341
Iteration 111/1000 | Loss: 0.00001341
Iteration 112/1000 | Loss: 0.00001340
Iteration 113/1000 | Loss: 0.00001340
Iteration 114/1000 | Loss: 0.00001340
Iteration 115/1000 | Loss: 0.00001340
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001339
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001339
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001339
Iteration 127/1000 | Loss: 0.00001339
Iteration 128/1000 | Loss: 0.00001339
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001338
Iteration 133/1000 | Loss: 0.00001338
Iteration 134/1000 | Loss: 0.00001338
Iteration 135/1000 | Loss: 0.00001338
Iteration 136/1000 | Loss: 0.00001338
Iteration 137/1000 | Loss: 0.00001338
Iteration 138/1000 | Loss: 0.00001338
Iteration 139/1000 | Loss: 0.00001338
Iteration 140/1000 | Loss: 0.00001338
Iteration 141/1000 | Loss: 0.00001338
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001338
Iteration 144/1000 | Loss: 0.00001338
Iteration 145/1000 | Loss: 0.00001338
Iteration 146/1000 | Loss: 0.00001338
Iteration 147/1000 | Loss: 0.00001338
Iteration 148/1000 | Loss: 0.00001338
Iteration 149/1000 | Loss: 0.00001338
Iteration 150/1000 | Loss: 0.00001338
Iteration 151/1000 | Loss: 0.00001338
Iteration 152/1000 | Loss: 0.00001338
Iteration 153/1000 | Loss: 0.00001338
Iteration 154/1000 | Loss: 0.00001338
Iteration 155/1000 | Loss: 0.00001338
Iteration 156/1000 | Loss: 0.00001338
Iteration 157/1000 | Loss: 0.00001338
Iteration 158/1000 | Loss: 0.00001338
Iteration 159/1000 | Loss: 0.00001338
Iteration 160/1000 | Loss: 0.00001338
Iteration 161/1000 | Loss: 0.00001338
Iteration 162/1000 | Loss: 0.00001338
Iteration 163/1000 | Loss: 0.00001338
Iteration 164/1000 | Loss: 0.00001338
Iteration 165/1000 | Loss: 0.00001338
Iteration 166/1000 | Loss: 0.00001338
Iteration 167/1000 | Loss: 0.00001338
Iteration 168/1000 | Loss: 0.00001338
Iteration 169/1000 | Loss: 0.00001338
Iteration 170/1000 | Loss: 0.00001338
Iteration 171/1000 | Loss: 0.00001338
Iteration 172/1000 | Loss: 0.00001338
Iteration 173/1000 | Loss: 0.00001338
Iteration 174/1000 | Loss: 0.00001338
Iteration 175/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.3376168681134004e-05, 1.3376168681134004e-05, 1.3376168681134004e-05, 1.3376168681134004e-05, 1.3376168681134004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3376168681134004e-05

Optimization complete. Final v2v error: 3.1396052837371826 mm

Highest mean error: 3.5790154933929443 mm for frame 89

Lowest mean error: 2.8630166053771973 mm for frame 42

Saving results

Total time: 37.703898906707764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816360
Iteration 2/25 | Loss: 0.00084536
Iteration 3/25 | Loss: 0.00067405
Iteration 4/25 | Loss: 0.00065350
Iteration 5/25 | Loss: 0.00064738
Iteration 6/25 | Loss: 0.00064596
Iteration 7/25 | Loss: 0.00064566
Iteration 8/25 | Loss: 0.00064566
Iteration 9/25 | Loss: 0.00064566
Iteration 10/25 | Loss: 0.00064566
Iteration 11/25 | Loss: 0.00064566
Iteration 12/25 | Loss: 0.00064566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006456638220697641, 0.0006456638220697641, 0.0006456638220697641, 0.0006456638220697641, 0.0006456638220697641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006456638220697641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48983598
Iteration 2/25 | Loss: 0.00031446
Iteration 3/25 | Loss: 0.00031446
Iteration 4/25 | Loss: 0.00031446
Iteration 5/25 | Loss: 0.00031446
Iteration 6/25 | Loss: 0.00031446
Iteration 7/25 | Loss: 0.00031446
Iteration 8/25 | Loss: 0.00031446
Iteration 9/25 | Loss: 0.00031446
Iteration 10/25 | Loss: 0.00031446
Iteration 11/25 | Loss: 0.00031446
Iteration 12/25 | Loss: 0.00031446
Iteration 13/25 | Loss: 0.00031446
Iteration 14/25 | Loss: 0.00031446
Iteration 15/25 | Loss: 0.00031446
Iteration 16/25 | Loss: 0.00031446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00031445903005078435, 0.00031445903005078435, 0.00031445903005078435, 0.00031445903005078435, 0.00031445903005078435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031445903005078435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031446
Iteration 2/1000 | Loss: 0.00001939
Iteration 3/1000 | Loss: 0.00001280
Iteration 4/1000 | Loss: 0.00001110
Iteration 5/1000 | Loss: 0.00001071
Iteration 6/1000 | Loss: 0.00001018
Iteration 7/1000 | Loss: 0.00001014
Iteration 8/1000 | Loss: 0.00000991
Iteration 9/1000 | Loss: 0.00000986
Iteration 10/1000 | Loss: 0.00000984
Iteration 11/1000 | Loss: 0.00000980
Iteration 12/1000 | Loss: 0.00000980
Iteration 13/1000 | Loss: 0.00000980
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000980
Iteration 16/1000 | Loss: 0.00000980
Iteration 17/1000 | Loss: 0.00000980
Iteration 18/1000 | Loss: 0.00000980
Iteration 19/1000 | Loss: 0.00000979
Iteration 20/1000 | Loss: 0.00000975
Iteration 21/1000 | Loss: 0.00000971
Iteration 22/1000 | Loss: 0.00000969
Iteration 23/1000 | Loss: 0.00000969
Iteration 24/1000 | Loss: 0.00000969
Iteration 25/1000 | Loss: 0.00000968
Iteration 26/1000 | Loss: 0.00000968
Iteration 27/1000 | Loss: 0.00000967
Iteration 28/1000 | Loss: 0.00000967
Iteration 29/1000 | Loss: 0.00000966
Iteration 30/1000 | Loss: 0.00000965
Iteration 31/1000 | Loss: 0.00000964
Iteration 32/1000 | Loss: 0.00000964
Iteration 33/1000 | Loss: 0.00000964
Iteration 34/1000 | Loss: 0.00000963
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000962
Iteration 37/1000 | Loss: 0.00000961
Iteration 38/1000 | Loss: 0.00000961
Iteration 39/1000 | Loss: 0.00000960
Iteration 40/1000 | Loss: 0.00000960
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000960
Iteration 46/1000 | Loss: 0.00000960
Iteration 47/1000 | Loss: 0.00000959
Iteration 48/1000 | Loss: 0.00000959
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000959
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000958
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000955
Iteration 57/1000 | Loss: 0.00000955
Iteration 58/1000 | Loss: 0.00000954
Iteration 59/1000 | Loss: 0.00000954
Iteration 60/1000 | Loss: 0.00000954
Iteration 61/1000 | Loss: 0.00000953
Iteration 62/1000 | Loss: 0.00000953
Iteration 63/1000 | Loss: 0.00000952
Iteration 64/1000 | Loss: 0.00000952
Iteration 65/1000 | Loss: 0.00000952
Iteration 66/1000 | Loss: 0.00000951
Iteration 67/1000 | Loss: 0.00000951
Iteration 68/1000 | Loss: 0.00000951
Iteration 69/1000 | Loss: 0.00000950
Iteration 70/1000 | Loss: 0.00000950
Iteration 71/1000 | Loss: 0.00000950
Iteration 72/1000 | Loss: 0.00000950
Iteration 73/1000 | Loss: 0.00000949
Iteration 74/1000 | Loss: 0.00000948
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000948
Iteration 77/1000 | Loss: 0.00000948
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000947
Iteration 80/1000 | Loss: 0.00000947
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000946
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000945
Iteration 85/1000 | Loss: 0.00000945
Iteration 86/1000 | Loss: 0.00000945
Iteration 87/1000 | Loss: 0.00000945
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000943
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000943
Iteration 96/1000 | Loss: 0.00000943
Iteration 97/1000 | Loss: 0.00000943
Iteration 98/1000 | Loss: 0.00000943
Iteration 99/1000 | Loss: 0.00000942
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000942
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000942
Iteration 107/1000 | Loss: 0.00000942
Iteration 108/1000 | Loss: 0.00000942
Iteration 109/1000 | Loss: 0.00000942
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000941
Iteration 114/1000 | Loss: 0.00000941
Iteration 115/1000 | Loss: 0.00000941
Iteration 116/1000 | Loss: 0.00000941
Iteration 117/1000 | Loss: 0.00000941
Iteration 118/1000 | Loss: 0.00000941
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000940
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000940
Iteration 125/1000 | Loss: 0.00000940
Iteration 126/1000 | Loss: 0.00000940
Iteration 127/1000 | Loss: 0.00000940
Iteration 128/1000 | Loss: 0.00000940
Iteration 129/1000 | Loss: 0.00000940
Iteration 130/1000 | Loss: 0.00000940
Iteration 131/1000 | Loss: 0.00000940
Iteration 132/1000 | Loss: 0.00000939
Iteration 133/1000 | Loss: 0.00000939
Iteration 134/1000 | Loss: 0.00000939
Iteration 135/1000 | Loss: 0.00000939
Iteration 136/1000 | Loss: 0.00000939
Iteration 137/1000 | Loss: 0.00000939
Iteration 138/1000 | Loss: 0.00000939
Iteration 139/1000 | Loss: 0.00000939
Iteration 140/1000 | Loss: 0.00000939
Iteration 141/1000 | Loss: 0.00000939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [9.39286474022083e-06, 9.39286474022083e-06, 9.39286474022083e-06, 9.39286474022083e-06, 9.39286474022083e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.39286474022083e-06

Optimization complete. Final v2v error: 2.5966503620147705 mm

Highest mean error: 2.776495933532715 mm for frame 45

Lowest mean error: 2.4738123416900635 mm for frame 139

Saving results

Total time: 31.588727474212646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480534
Iteration 2/25 | Loss: 0.00080311
Iteration 3/25 | Loss: 0.00070589
Iteration 4/25 | Loss: 0.00068095
Iteration 5/25 | Loss: 0.00067105
Iteration 6/25 | Loss: 0.00066926
Iteration 7/25 | Loss: 0.00066889
Iteration 8/25 | Loss: 0.00066889
Iteration 9/25 | Loss: 0.00066889
Iteration 10/25 | Loss: 0.00066889
Iteration 11/25 | Loss: 0.00066889
Iteration 12/25 | Loss: 0.00066889
Iteration 13/25 | Loss: 0.00066889
Iteration 14/25 | Loss: 0.00066889
Iteration 15/25 | Loss: 0.00066889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006688925204798579, 0.0006688925204798579, 0.0006688925204798579, 0.0006688925204798579, 0.0006688925204798579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006688925204798579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40183544
Iteration 2/25 | Loss: 0.00033854
Iteration 3/25 | Loss: 0.00033854
Iteration 4/25 | Loss: 0.00033854
Iteration 5/25 | Loss: 0.00033854
Iteration 6/25 | Loss: 0.00033854
Iteration 7/25 | Loss: 0.00033854
Iteration 8/25 | Loss: 0.00033854
Iteration 9/25 | Loss: 0.00033854
Iteration 10/25 | Loss: 0.00033854
Iteration 11/25 | Loss: 0.00033854
Iteration 12/25 | Loss: 0.00033854
Iteration 13/25 | Loss: 0.00033854
Iteration 14/25 | Loss: 0.00033854
Iteration 15/25 | Loss: 0.00033854
Iteration 16/25 | Loss: 0.00033854
Iteration 17/25 | Loss: 0.00033854
Iteration 18/25 | Loss: 0.00033854
Iteration 19/25 | Loss: 0.00033854
Iteration 20/25 | Loss: 0.00033854
Iteration 21/25 | Loss: 0.00033854
Iteration 22/25 | Loss: 0.00033854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003385382005944848, 0.0003385382005944848, 0.0003385382005944848, 0.0003385382005944848, 0.0003385382005944848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003385382005944848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033854
Iteration 2/1000 | Loss: 0.00002613
Iteration 3/1000 | Loss: 0.00001782
Iteration 4/1000 | Loss: 0.00001647
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001484
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001422
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001412
Iteration 11/1000 | Loss: 0.00001411
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001388
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001387
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001387
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001387
Iteration 28/1000 | Loss: 0.00001387
Iteration 29/1000 | Loss: 0.00001387
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001384
Iteration 33/1000 | Loss: 0.00001384
Iteration 34/1000 | Loss: 0.00001384
Iteration 35/1000 | Loss: 0.00001383
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001379
Iteration 46/1000 | Loss: 0.00001379
Iteration 47/1000 | Loss: 0.00001379
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001378
Iteration 51/1000 | Loss: 0.00001378
Iteration 52/1000 | Loss: 0.00001378
Iteration 53/1000 | Loss: 0.00001378
Iteration 54/1000 | Loss: 0.00001378
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001376
Iteration 59/1000 | Loss: 0.00001376
Iteration 60/1000 | Loss: 0.00001376
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001376
Iteration 65/1000 | Loss: 0.00001375
Iteration 66/1000 | Loss: 0.00001375
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001374
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001373
Iteration 72/1000 | Loss: 0.00001373
Iteration 73/1000 | Loss: 0.00001373
Iteration 74/1000 | Loss: 0.00001373
Iteration 75/1000 | Loss: 0.00001373
Iteration 76/1000 | Loss: 0.00001373
Iteration 77/1000 | Loss: 0.00001373
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001373
Iteration 80/1000 | Loss: 0.00001373
Iteration 81/1000 | Loss: 0.00001373
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001373
Iteration 84/1000 | Loss: 0.00001373
Iteration 85/1000 | Loss: 0.00001373
Iteration 86/1000 | Loss: 0.00001373
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001373
Iteration 91/1000 | Loss: 0.00001373
Iteration 92/1000 | Loss: 0.00001373
Iteration 93/1000 | Loss: 0.00001373
Iteration 94/1000 | Loss: 0.00001373
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001373
Iteration 98/1000 | Loss: 0.00001373
Iteration 99/1000 | Loss: 0.00001373
Iteration 100/1000 | Loss: 0.00001373
Iteration 101/1000 | Loss: 0.00001373
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00001373
Iteration 105/1000 | Loss: 0.00001373
Iteration 106/1000 | Loss: 0.00001373
Iteration 107/1000 | Loss: 0.00001373
Iteration 108/1000 | Loss: 0.00001373
Iteration 109/1000 | Loss: 0.00001373
Iteration 110/1000 | Loss: 0.00001373
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001373
Iteration 113/1000 | Loss: 0.00001373
Iteration 114/1000 | Loss: 0.00001373
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001373
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001373
Iteration 151/1000 | Loss: 0.00001373
Iteration 152/1000 | Loss: 0.00001373
Iteration 153/1000 | Loss: 0.00001373
Iteration 154/1000 | Loss: 0.00001373
Iteration 155/1000 | Loss: 0.00001373
Iteration 156/1000 | Loss: 0.00001373
Iteration 157/1000 | Loss: 0.00001373
Iteration 158/1000 | Loss: 0.00001373
Iteration 159/1000 | Loss: 0.00001373
Iteration 160/1000 | Loss: 0.00001373
Iteration 161/1000 | Loss: 0.00001373
Iteration 162/1000 | Loss: 0.00001373
Iteration 163/1000 | Loss: 0.00001373
Iteration 164/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3726950783166103e-05, 1.3726950783166103e-05, 1.3726950783166103e-05, 1.3726950783166103e-05, 1.3726950783166103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3726950783166103e-05

Optimization complete. Final v2v error: 3.142268180847168 mm

Highest mean error: 3.4550511837005615 mm for frame 109

Lowest mean error: 2.974672794342041 mm for frame 46

Saving results

Total time: 31.9025137424469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997848
Iteration 2/25 | Loss: 0.00287300
Iteration 3/25 | Loss: 0.00199051
Iteration 4/25 | Loss: 0.00164481
Iteration 5/25 | Loss: 0.00145741
Iteration 6/25 | Loss: 0.00136827
Iteration 7/25 | Loss: 0.00121356
Iteration 8/25 | Loss: 0.00104901
Iteration 9/25 | Loss: 0.00098800
Iteration 10/25 | Loss: 0.00096327
Iteration 11/25 | Loss: 0.00095181
Iteration 12/25 | Loss: 0.00094754
Iteration 13/25 | Loss: 0.00094886
Iteration 14/25 | Loss: 0.00094605
Iteration 15/25 | Loss: 0.00094215
Iteration 16/25 | Loss: 0.00094402
Iteration 17/25 | Loss: 0.00094393
Iteration 18/25 | Loss: 0.00094135
Iteration 19/25 | Loss: 0.00094308
Iteration 20/25 | Loss: 0.00094279
Iteration 21/25 | Loss: 0.00094100
Iteration 22/25 | Loss: 0.00094082
Iteration 23/25 | Loss: 0.00094376
Iteration 24/25 | Loss: 0.00094063
Iteration 25/25 | Loss: 0.00094059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48826122
Iteration 2/25 | Loss: 0.00080578
Iteration 3/25 | Loss: 0.00070508
Iteration 4/25 | Loss: 0.00070508
Iteration 5/25 | Loss: 0.00070508
Iteration 6/25 | Loss: 0.00070508
Iteration 7/25 | Loss: 0.00070508
Iteration 8/25 | Loss: 0.00070508
Iteration 9/25 | Loss: 0.00070508
Iteration 10/25 | Loss: 0.00070508
Iteration 11/25 | Loss: 0.00070508
Iteration 12/25 | Loss: 0.00070508
Iteration 13/25 | Loss: 0.00070508
Iteration 14/25 | Loss: 0.00070508
Iteration 15/25 | Loss: 0.00070508
Iteration 16/25 | Loss: 0.00070508
Iteration 17/25 | Loss: 0.00070508
Iteration 18/25 | Loss: 0.00070508
Iteration 19/25 | Loss: 0.00070508
Iteration 20/25 | Loss: 0.00070508
Iteration 21/25 | Loss: 0.00070508
Iteration 22/25 | Loss: 0.00070508
Iteration 23/25 | Loss: 0.00070508
Iteration 24/25 | Loss: 0.00070508
Iteration 25/25 | Loss: 0.00070508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007050816784612834, 0.0007050816784612834, 0.0007050816784612834, 0.0007050816784612834, 0.0007050816784612834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007050816784612834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070508
Iteration 2/1000 | Loss: 0.00008266
Iteration 3/1000 | Loss: 0.00032358
Iteration 4/1000 | Loss: 0.00008330
Iteration 5/1000 | Loss: 0.00006959
Iteration 6/1000 | Loss: 0.00004494
Iteration 7/1000 | Loss: 0.00055412
Iteration 8/1000 | Loss: 0.00005490
Iteration 9/1000 | Loss: 0.00004427
Iteration 10/1000 | Loss: 0.00004062
Iteration 11/1000 | Loss: 0.00003827
Iteration 12/1000 | Loss: 0.00005843
Iteration 13/1000 | Loss: 0.00057845
Iteration 14/1000 | Loss: 0.00004328
Iteration 15/1000 | Loss: 0.00003572
Iteration 16/1000 | Loss: 0.00005166
Iteration 17/1000 | Loss: 0.00003322
Iteration 18/1000 | Loss: 0.00003266
Iteration 19/1000 | Loss: 0.00003232
Iteration 20/1000 | Loss: 0.00003192
Iteration 21/1000 | Loss: 0.00003169
Iteration 22/1000 | Loss: 0.00003146
Iteration 23/1000 | Loss: 0.00003145
Iteration 24/1000 | Loss: 0.00003142
Iteration 25/1000 | Loss: 0.00003142
Iteration 26/1000 | Loss: 0.00003134
Iteration 27/1000 | Loss: 0.00003132
Iteration 28/1000 | Loss: 0.00003132
Iteration 29/1000 | Loss: 0.00003131
Iteration 30/1000 | Loss: 0.00003131
Iteration 31/1000 | Loss: 0.00003131
Iteration 32/1000 | Loss: 0.00003131
Iteration 33/1000 | Loss: 0.00003131
Iteration 34/1000 | Loss: 0.00003130
Iteration 35/1000 | Loss: 0.00003129
Iteration 36/1000 | Loss: 0.00003129
Iteration 37/1000 | Loss: 0.00003128
Iteration 38/1000 | Loss: 0.00003128
Iteration 39/1000 | Loss: 0.00003128
Iteration 40/1000 | Loss: 0.00003128
Iteration 41/1000 | Loss: 0.00003127
Iteration 42/1000 | Loss: 0.00003127
Iteration 43/1000 | Loss: 0.00003127
Iteration 44/1000 | Loss: 0.00003127
Iteration 45/1000 | Loss: 0.00003127
Iteration 46/1000 | Loss: 0.00003127
Iteration 47/1000 | Loss: 0.00003127
Iteration 48/1000 | Loss: 0.00003127
Iteration 49/1000 | Loss: 0.00003124
Iteration 50/1000 | Loss: 0.00003124
Iteration 51/1000 | Loss: 0.00003124
Iteration 52/1000 | Loss: 0.00003124
Iteration 53/1000 | Loss: 0.00003124
Iteration 54/1000 | Loss: 0.00003124
Iteration 55/1000 | Loss: 0.00003124
Iteration 56/1000 | Loss: 0.00003124
Iteration 57/1000 | Loss: 0.00003124
Iteration 58/1000 | Loss: 0.00003124
Iteration 59/1000 | Loss: 0.00003124
Iteration 60/1000 | Loss: 0.00003123
Iteration 61/1000 | Loss: 0.00003123
Iteration 62/1000 | Loss: 0.00003123
Iteration 63/1000 | Loss: 0.00003123
Iteration 64/1000 | Loss: 0.00003123
Iteration 65/1000 | Loss: 0.00003122
Iteration 66/1000 | Loss: 0.00003122
Iteration 67/1000 | Loss: 0.00003122
Iteration 68/1000 | Loss: 0.00003122
Iteration 69/1000 | Loss: 0.00003121
Iteration 70/1000 | Loss: 0.00003120
Iteration 71/1000 | Loss: 0.00003120
Iteration 72/1000 | Loss: 0.00003120
Iteration 73/1000 | Loss: 0.00003120
Iteration 74/1000 | Loss: 0.00003117
Iteration 75/1000 | Loss: 0.00003117
Iteration 76/1000 | Loss: 0.00003116
Iteration 77/1000 | Loss: 0.00003116
Iteration 78/1000 | Loss: 0.00003116
Iteration 79/1000 | Loss: 0.00003116
Iteration 80/1000 | Loss: 0.00003116
Iteration 81/1000 | Loss: 0.00003115
Iteration 82/1000 | Loss: 0.00003115
Iteration 83/1000 | Loss: 0.00003114
Iteration 84/1000 | Loss: 0.00003114
Iteration 85/1000 | Loss: 0.00003114
Iteration 86/1000 | Loss: 0.00003114
Iteration 87/1000 | Loss: 0.00003114
Iteration 88/1000 | Loss: 0.00003113
Iteration 89/1000 | Loss: 0.00003113
Iteration 90/1000 | Loss: 0.00003113
Iteration 91/1000 | Loss: 0.00003113
Iteration 92/1000 | Loss: 0.00003113
Iteration 93/1000 | Loss: 0.00003113
Iteration 94/1000 | Loss: 0.00003113
Iteration 95/1000 | Loss: 0.00003113
Iteration 96/1000 | Loss: 0.00003113
Iteration 97/1000 | Loss: 0.00003113
Iteration 98/1000 | Loss: 0.00003113
Iteration 99/1000 | Loss: 0.00003113
Iteration 100/1000 | Loss: 0.00003113
Iteration 101/1000 | Loss: 0.00003113
Iteration 102/1000 | Loss: 0.00003113
Iteration 103/1000 | Loss: 0.00003113
Iteration 104/1000 | Loss: 0.00003113
Iteration 105/1000 | Loss: 0.00003113
Iteration 106/1000 | Loss: 0.00003113
Iteration 107/1000 | Loss: 0.00003113
Iteration 108/1000 | Loss: 0.00003113
Iteration 109/1000 | Loss: 0.00003113
Iteration 110/1000 | Loss: 0.00003113
Iteration 111/1000 | Loss: 0.00003113
Iteration 112/1000 | Loss: 0.00003113
Iteration 113/1000 | Loss: 0.00003113
Iteration 114/1000 | Loss: 0.00003113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.113053026027046e-05, 3.113053026027046e-05, 3.113053026027046e-05, 3.113053026027046e-05, 3.113053026027046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.113053026027046e-05

Optimization complete. Final v2v error: 4.531257629394531 mm

Highest mean error: 12.455656051635742 mm for frame 232

Lowest mean error: 3.5241992473602295 mm for frame 155

Saving results

Total time: 94.39915823936462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795420
Iteration 2/25 | Loss: 0.00127953
Iteration 3/25 | Loss: 0.00083915
Iteration 4/25 | Loss: 0.00077649
Iteration 5/25 | Loss: 0.00076564
Iteration 6/25 | Loss: 0.00076305
Iteration 7/25 | Loss: 0.00076286
Iteration 8/25 | Loss: 0.00076286
Iteration 9/25 | Loss: 0.00076286
Iteration 10/25 | Loss: 0.00076286
Iteration 11/25 | Loss: 0.00076286
Iteration 12/25 | Loss: 0.00076286
Iteration 13/25 | Loss: 0.00076286
Iteration 14/25 | Loss: 0.00076286
Iteration 15/25 | Loss: 0.00076286
Iteration 16/25 | Loss: 0.00076286
Iteration 17/25 | Loss: 0.00076286
Iteration 18/25 | Loss: 0.00076286
Iteration 19/25 | Loss: 0.00076286
Iteration 20/25 | Loss: 0.00076286
Iteration 21/25 | Loss: 0.00076286
Iteration 22/25 | Loss: 0.00076286
Iteration 23/25 | Loss: 0.00076286
Iteration 24/25 | Loss: 0.00076286
Iteration 25/25 | Loss: 0.00076286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007628624443896115, 0.0007628624443896115, 0.0007628624443896115, 0.0007628624443896115, 0.0007628624443896115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007628624443896115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60474634
Iteration 2/25 | Loss: 0.00034978
Iteration 3/25 | Loss: 0.00034976
Iteration 4/25 | Loss: 0.00034976
Iteration 5/25 | Loss: 0.00034976
Iteration 6/25 | Loss: 0.00034976
Iteration 7/25 | Loss: 0.00034976
Iteration 8/25 | Loss: 0.00034976
Iteration 9/25 | Loss: 0.00034976
Iteration 10/25 | Loss: 0.00034976
Iteration 11/25 | Loss: 0.00034976
Iteration 12/25 | Loss: 0.00034976
Iteration 13/25 | Loss: 0.00034976
Iteration 14/25 | Loss: 0.00034976
Iteration 15/25 | Loss: 0.00034976
Iteration 16/25 | Loss: 0.00034976
Iteration 17/25 | Loss: 0.00034976
Iteration 18/25 | Loss: 0.00034976
Iteration 19/25 | Loss: 0.00034976
Iteration 20/25 | Loss: 0.00034976
Iteration 21/25 | Loss: 0.00034976
Iteration 22/25 | Loss: 0.00034976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003497604629956186, 0.0003497604629956186, 0.0003497604629956186, 0.0003497604629956186, 0.0003497604629956186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003497604629956186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034976
Iteration 2/1000 | Loss: 0.00002794
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00002011
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001933
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001890
Iteration 10/1000 | Loss: 0.00001870
Iteration 11/1000 | Loss: 0.00001854
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001840
Iteration 14/1000 | Loss: 0.00001840
Iteration 15/1000 | Loss: 0.00001840
Iteration 16/1000 | Loss: 0.00001840
Iteration 17/1000 | Loss: 0.00001840
Iteration 18/1000 | Loss: 0.00001840
Iteration 19/1000 | Loss: 0.00001840
Iteration 20/1000 | Loss: 0.00001840
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001839
Iteration 23/1000 | Loss: 0.00001839
Iteration 24/1000 | Loss: 0.00001839
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001839
Iteration 27/1000 | Loss: 0.00001839
Iteration 28/1000 | Loss: 0.00001839
Iteration 29/1000 | Loss: 0.00001839
Iteration 30/1000 | Loss: 0.00001839
Iteration 31/1000 | Loss: 0.00001838
Iteration 32/1000 | Loss: 0.00001838
Iteration 33/1000 | Loss: 0.00001838
Iteration 34/1000 | Loss: 0.00001838
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001837
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001836
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001835
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001832
Iteration 64/1000 | Loss: 0.00001832
Iteration 65/1000 | Loss: 0.00001832
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001831
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001830
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001829
Iteration 104/1000 | Loss: 0.00001829
Iteration 105/1000 | Loss: 0.00001829
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001829
Iteration 108/1000 | Loss: 0.00001829
Iteration 109/1000 | Loss: 0.00001829
Iteration 110/1000 | Loss: 0.00001829
Iteration 111/1000 | Loss: 0.00001829
Iteration 112/1000 | Loss: 0.00001829
Iteration 113/1000 | Loss: 0.00001829
Iteration 114/1000 | Loss: 0.00001829
Iteration 115/1000 | Loss: 0.00001829
Iteration 116/1000 | Loss: 0.00001829
Iteration 117/1000 | Loss: 0.00001829
Iteration 118/1000 | Loss: 0.00001829
Iteration 119/1000 | Loss: 0.00001829
Iteration 120/1000 | Loss: 0.00001829
Iteration 121/1000 | Loss: 0.00001829
Iteration 122/1000 | Loss: 0.00001829
Iteration 123/1000 | Loss: 0.00001829
Iteration 124/1000 | Loss: 0.00001829
Iteration 125/1000 | Loss: 0.00001829
Iteration 126/1000 | Loss: 0.00001829
Iteration 127/1000 | Loss: 0.00001829
Iteration 128/1000 | Loss: 0.00001829
Iteration 129/1000 | Loss: 0.00001829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8290533262188546e-05, 1.8290533262188546e-05, 1.8290533262188546e-05, 1.8290533262188546e-05, 1.8290533262188546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8290533262188546e-05

Optimization complete. Final v2v error: 3.604597806930542 mm

Highest mean error: 3.8444550037384033 mm for frame 222

Lowest mean error: 3.388538122177124 mm for frame 181

Saving results

Total time: 37.54344081878662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835025
Iteration 2/25 | Loss: 0.00077521
Iteration 3/25 | Loss: 0.00069024
Iteration 4/25 | Loss: 0.00067813
Iteration 5/25 | Loss: 0.00067396
Iteration 6/25 | Loss: 0.00067301
Iteration 7/25 | Loss: 0.00067287
Iteration 8/25 | Loss: 0.00067287
Iteration 9/25 | Loss: 0.00067287
Iteration 10/25 | Loss: 0.00067287
Iteration 11/25 | Loss: 0.00067287
Iteration 12/25 | Loss: 0.00067287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006728671141900122, 0.0006728671141900122, 0.0006728671141900122, 0.0006728671141900122, 0.0006728671141900122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006728671141900122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.21672821
Iteration 2/25 | Loss: 0.00037075
Iteration 3/25 | Loss: 0.00037073
Iteration 4/25 | Loss: 0.00037073
Iteration 5/25 | Loss: 0.00037073
Iteration 6/25 | Loss: 0.00037073
Iteration 7/25 | Loss: 0.00037073
Iteration 8/25 | Loss: 0.00037073
Iteration 9/25 | Loss: 0.00037073
Iteration 10/25 | Loss: 0.00037073
Iteration 11/25 | Loss: 0.00037073
Iteration 12/25 | Loss: 0.00037073
Iteration 13/25 | Loss: 0.00037073
Iteration 14/25 | Loss: 0.00037073
Iteration 15/25 | Loss: 0.00037073
Iteration 16/25 | Loss: 0.00037073
Iteration 17/25 | Loss: 0.00037073
Iteration 18/25 | Loss: 0.00037073
Iteration 19/25 | Loss: 0.00037073
Iteration 20/25 | Loss: 0.00037073
Iteration 21/25 | Loss: 0.00037073
Iteration 22/25 | Loss: 0.00037073
Iteration 23/25 | Loss: 0.00037073
Iteration 24/25 | Loss: 0.00037073
Iteration 25/25 | Loss: 0.00037073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037073
Iteration 2/1000 | Loss: 0.00002260
Iteration 3/1000 | Loss: 0.00001687
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001493
Iteration 6/1000 | Loss: 0.00001432
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001380
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001371
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001361
Iteration 21/1000 | Loss: 0.00001360
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001358
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001351
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001345
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00001345
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001339
Iteration 63/1000 | Loss: 0.00001339
Iteration 64/1000 | Loss: 0.00001339
Iteration 65/1000 | Loss: 0.00001339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.338512356596766e-05, 1.338512356596766e-05, 1.338512356596766e-05, 1.338512356596766e-05, 1.338512356596766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.338512356596766e-05

Optimization complete. Final v2v error: 3.1160175800323486 mm

Highest mean error: 3.284975290298462 mm for frame 123

Lowest mean error: 2.8535525798797607 mm for frame 1

Saving results

Total time: 27.412064790725708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911931
Iteration 2/25 | Loss: 0.00085596
Iteration 3/25 | Loss: 0.00067825
Iteration 4/25 | Loss: 0.00065297
Iteration 5/25 | Loss: 0.00064701
Iteration 6/25 | Loss: 0.00064567
Iteration 7/25 | Loss: 0.00064546
Iteration 8/25 | Loss: 0.00064546
Iteration 9/25 | Loss: 0.00064546
Iteration 10/25 | Loss: 0.00064546
Iteration 11/25 | Loss: 0.00064546
Iteration 12/25 | Loss: 0.00064546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006454637041315436, 0.0006454637041315436, 0.0006454637041315436, 0.0006454637041315436, 0.0006454637041315436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006454637041315436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88232875
Iteration 2/25 | Loss: 0.00025622
Iteration 3/25 | Loss: 0.00025622
Iteration 4/25 | Loss: 0.00025622
Iteration 5/25 | Loss: 0.00025622
Iteration 6/25 | Loss: 0.00025622
Iteration 7/25 | Loss: 0.00025622
Iteration 8/25 | Loss: 0.00025622
Iteration 9/25 | Loss: 0.00025622
Iteration 10/25 | Loss: 0.00025622
Iteration 11/25 | Loss: 0.00025622
Iteration 12/25 | Loss: 0.00025622
Iteration 13/25 | Loss: 0.00025622
Iteration 14/25 | Loss: 0.00025622
Iteration 15/25 | Loss: 0.00025622
Iteration 16/25 | Loss: 0.00025622
Iteration 17/25 | Loss: 0.00025622
Iteration 18/25 | Loss: 0.00025622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002562182489782572, 0.0002562182489782572, 0.0002562182489782572, 0.0002562182489782572, 0.0002562182489782572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002562182489782572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025622
Iteration 2/1000 | Loss: 0.00002024
Iteration 3/1000 | Loss: 0.00001536
Iteration 4/1000 | Loss: 0.00001421
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001275
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001224
Iteration 9/1000 | Loss: 0.00001216
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001199
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001188
Iteration 17/1000 | Loss: 0.00001187
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001186
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001184
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001172
Iteration 63/1000 | Loss: 0.00001172
Iteration 64/1000 | Loss: 0.00001171
Iteration 65/1000 | Loss: 0.00001171
Iteration 66/1000 | Loss: 0.00001171
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001170
Iteration 69/1000 | Loss: 0.00001170
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001169
Iteration 72/1000 | Loss: 0.00001169
Iteration 73/1000 | Loss: 0.00001169
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001168
Iteration 76/1000 | Loss: 0.00001168
Iteration 77/1000 | Loss: 0.00001168
Iteration 78/1000 | Loss: 0.00001168
Iteration 79/1000 | Loss: 0.00001168
Iteration 80/1000 | Loss: 0.00001168
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001166
Iteration 84/1000 | Loss: 0.00001166
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001162
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001161
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001158
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001157
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001157
Iteration 143/1000 | Loss: 0.00001157
Iteration 144/1000 | Loss: 0.00001157
Iteration 145/1000 | Loss: 0.00001157
Iteration 146/1000 | Loss: 0.00001157
Iteration 147/1000 | Loss: 0.00001157
Iteration 148/1000 | Loss: 0.00001157
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001157
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.156609596364433e-05, 1.156609596364433e-05, 1.156609596364433e-05, 1.156609596364433e-05, 1.156609596364433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.156609596364433e-05

Optimization complete. Final v2v error: 2.8749897480010986 mm

Highest mean error: 3.0143749713897705 mm for frame 110

Lowest mean error: 2.723454236984253 mm for frame 7

Saving results

Total time: 40.7722647190094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830580
Iteration 2/25 | Loss: 0.00107072
Iteration 3/25 | Loss: 0.00072340
Iteration 4/25 | Loss: 0.00068889
Iteration 5/25 | Loss: 0.00068076
Iteration 6/25 | Loss: 0.00067936
Iteration 7/25 | Loss: 0.00067905
Iteration 8/25 | Loss: 0.00067903
Iteration 9/25 | Loss: 0.00067903
Iteration 10/25 | Loss: 0.00067903
Iteration 11/25 | Loss: 0.00067903
Iteration 12/25 | Loss: 0.00067903
Iteration 13/25 | Loss: 0.00067903
Iteration 14/25 | Loss: 0.00067903
Iteration 15/25 | Loss: 0.00067903
Iteration 16/25 | Loss: 0.00067903
Iteration 17/25 | Loss: 0.00067903
Iteration 18/25 | Loss: 0.00067903
Iteration 19/25 | Loss: 0.00067903
Iteration 20/25 | Loss: 0.00067903
Iteration 21/25 | Loss: 0.00067903
Iteration 22/25 | Loss: 0.00067903
Iteration 23/25 | Loss: 0.00067903
Iteration 24/25 | Loss: 0.00067903
Iteration 25/25 | Loss: 0.00067903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27442837
Iteration 2/25 | Loss: 0.00028838
Iteration 3/25 | Loss: 0.00028838
Iteration 4/25 | Loss: 0.00028838
Iteration 5/25 | Loss: 0.00028838
Iteration 6/25 | Loss: 0.00028838
Iteration 7/25 | Loss: 0.00028838
Iteration 8/25 | Loss: 0.00028838
Iteration 9/25 | Loss: 0.00028838
Iteration 10/25 | Loss: 0.00028838
Iteration 11/25 | Loss: 0.00028838
Iteration 12/25 | Loss: 0.00028838
Iteration 13/25 | Loss: 0.00028838
Iteration 14/25 | Loss: 0.00028838
Iteration 15/25 | Loss: 0.00028838
Iteration 16/25 | Loss: 0.00028838
Iteration 17/25 | Loss: 0.00028838
Iteration 18/25 | Loss: 0.00028838
Iteration 19/25 | Loss: 0.00028838
Iteration 20/25 | Loss: 0.00028838
Iteration 21/25 | Loss: 0.00028838
Iteration 22/25 | Loss: 0.00028838
Iteration 23/25 | Loss: 0.00028838
Iteration 24/25 | Loss: 0.00028838
Iteration 25/25 | Loss: 0.00028838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028838
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00002409
Iteration 4/1000 | Loss: 0.00002108
Iteration 5/1000 | Loss: 0.00001981
Iteration 6/1000 | Loss: 0.00001883
Iteration 7/1000 | Loss: 0.00001816
Iteration 8/1000 | Loss: 0.00001769
Iteration 9/1000 | Loss: 0.00001735
Iteration 10/1000 | Loss: 0.00001719
Iteration 11/1000 | Loss: 0.00001706
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001700
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001682
Iteration 16/1000 | Loss: 0.00001681
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001666
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001662
Iteration 21/1000 | Loss: 0.00001659
Iteration 22/1000 | Loss: 0.00001659
Iteration 23/1000 | Loss: 0.00001659
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001656
Iteration 28/1000 | Loss: 0.00001655
Iteration 29/1000 | Loss: 0.00001654
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001652
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001651
Iteration 36/1000 | Loss: 0.00001651
Iteration 37/1000 | Loss: 0.00001650
Iteration 38/1000 | Loss: 0.00001649
Iteration 39/1000 | Loss: 0.00001649
Iteration 40/1000 | Loss: 0.00001649
Iteration 41/1000 | Loss: 0.00001648
Iteration 42/1000 | Loss: 0.00001648
Iteration 43/1000 | Loss: 0.00001648
Iteration 44/1000 | Loss: 0.00001647
Iteration 45/1000 | Loss: 0.00001647
Iteration 46/1000 | Loss: 0.00001647
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001647
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001646
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001646
Iteration 59/1000 | Loss: 0.00001646
Iteration 60/1000 | Loss: 0.00001646
Iteration 61/1000 | Loss: 0.00001646
Iteration 62/1000 | Loss: 0.00001646
Iteration 63/1000 | Loss: 0.00001646
Iteration 64/1000 | Loss: 0.00001646
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001646
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001646
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001646
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001646
Iteration 81/1000 | Loss: 0.00001646
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.6459238395327702e-05, 1.6459238395327702e-05, 1.6459238395327702e-05, 1.6459238395327702e-05, 1.6459238395327702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6459238395327702e-05

Optimization complete. Final v2v error: 3.353358745574951 mm

Highest mean error: 4.946552753448486 mm for frame 62

Lowest mean error: 2.75132417678833 mm for frame 90

Saving results

Total time: 36.957847595214844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618649
Iteration 2/25 | Loss: 0.00142471
Iteration 3/25 | Loss: 0.00090522
Iteration 4/25 | Loss: 0.00086936
Iteration 5/25 | Loss: 0.00085693
Iteration 6/25 | Loss: 0.00085370
Iteration 7/25 | Loss: 0.00085304
Iteration 8/25 | Loss: 0.00085298
Iteration 9/25 | Loss: 0.00085298
Iteration 10/25 | Loss: 0.00085298
Iteration 11/25 | Loss: 0.00085298
Iteration 12/25 | Loss: 0.00085298
Iteration 13/25 | Loss: 0.00085298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008529844344593585, 0.0008529844344593585, 0.0008529844344593585, 0.0008529844344593585, 0.0008529844344593585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008529844344593585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95067996
Iteration 2/25 | Loss: 0.00037148
Iteration 3/25 | Loss: 0.00037147
Iteration 4/25 | Loss: 0.00037147
Iteration 5/25 | Loss: 0.00037147
Iteration 6/25 | Loss: 0.00037147
Iteration 7/25 | Loss: 0.00037147
Iteration 8/25 | Loss: 0.00037147
Iteration 9/25 | Loss: 0.00037147
Iteration 10/25 | Loss: 0.00037147
Iteration 11/25 | Loss: 0.00037147
Iteration 12/25 | Loss: 0.00037147
Iteration 13/25 | Loss: 0.00037147
Iteration 14/25 | Loss: 0.00037147
Iteration 15/25 | Loss: 0.00037147
Iteration 16/25 | Loss: 0.00037147
Iteration 17/25 | Loss: 0.00037147
Iteration 18/25 | Loss: 0.00037147
Iteration 19/25 | Loss: 0.00037147
Iteration 20/25 | Loss: 0.00037147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0003714666818268597, 0.0003714666818268597, 0.0003714666818268597, 0.0003714666818268597, 0.0003714666818268597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003714666818268597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037147
Iteration 2/1000 | Loss: 0.00005158
Iteration 3/1000 | Loss: 0.00003695
Iteration 4/1000 | Loss: 0.00003457
Iteration 5/1000 | Loss: 0.00003306
Iteration 6/1000 | Loss: 0.00003232
Iteration 7/1000 | Loss: 0.00003158
Iteration 8/1000 | Loss: 0.00003113
Iteration 9/1000 | Loss: 0.00003074
Iteration 10/1000 | Loss: 0.00003050
Iteration 11/1000 | Loss: 0.00003023
Iteration 12/1000 | Loss: 0.00003001
Iteration 13/1000 | Loss: 0.00002983
Iteration 14/1000 | Loss: 0.00002967
Iteration 15/1000 | Loss: 0.00002951
Iteration 16/1000 | Loss: 0.00002935
Iteration 17/1000 | Loss: 0.00002927
Iteration 18/1000 | Loss: 0.00002927
Iteration 19/1000 | Loss: 0.00002916
Iteration 20/1000 | Loss: 0.00002912
Iteration 21/1000 | Loss: 0.00002907
Iteration 22/1000 | Loss: 0.00002901
Iteration 23/1000 | Loss: 0.00002901
Iteration 24/1000 | Loss: 0.00002901
Iteration 25/1000 | Loss: 0.00002901
Iteration 26/1000 | Loss: 0.00002900
Iteration 27/1000 | Loss: 0.00002897
Iteration 28/1000 | Loss: 0.00002897
Iteration 29/1000 | Loss: 0.00002897
Iteration 30/1000 | Loss: 0.00002897
Iteration 31/1000 | Loss: 0.00002897
Iteration 32/1000 | Loss: 0.00002897
Iteration 33/1000 | Loss: 0.00002897
Iteration 34/1000 | Loss: 0.00002897
Iteration 35/1000 | Loss: 0.00002897
Iteration 36/1000 | Loss: 0.00002896
Iteration 37/1000 | Loss: 0.00002896
Iteration 38/1000 | Loss: 0.00002896
Iteration 39/1000 | Loss: 0.00002896
Iteration 40/1000 | Loss: 0.00002896
Iteration 41/1000 | Loss: 0.00002896
Iteration 42/1000 | Loss: 0.00002896
Iteration 43/1000 | Loss: 0.00002896
Iteration 44/1000 | Loss: 0.00002895
Iteration 45/1000 | Loss: 0.00002894
Iteration 46/1000 | Loss: 0.00002894
Iteration 47/1000 | Loss: 0.00002893
Iteration 48/1000 | Loss: 0.00002893
Iteration 49/1000 | Loss: 0.00002893
Iteration 50/1000 | Loss: 0.00002893
Iteration 51/1000 | Loss: 0.00002893
Iteration 52/1000 | Loss: 0.00002892
Iteration 53/1000 | Loss: 0.00002892
Iteration 54/1000 | Loss: 0.00002892
Iteration 55/1000 | Loss: 0.00002892
Iteration 56/1000 | Loss: 0.00002892
Iteration 57/1000 | Loss: 0.00002892
Iteration 58/1000 | Loss: 0.00002891
Iteration 59/1000 | Loss: 0.00002891
Iteration 60/1000 | Loss: 0.00002891
Iteration 61/1000 | Loss: 0.00002891
Iteration 62/1000 | Loss: 0.00002891
Iteration 63/1000 | Loss: 0.00002890
Iteration 64/1000 | Loss: 0.00002890
Iteration 65/1000 | Loss: 0.00002890
Iteration 66/1000 | Loss: 0.00002890
Iteration 67/1000 | Loss: 0.00002890
Iteration 68/1000 | Loss: 0.00002890
Iteration 69/1000 | Loss: 0.00002889
Iteration 70/1000 | Loss: 0.00002889
Iteration 71/1000 | Loss: 0.00002889
Iteration 72/1000 | Loss: 0.00002889
Iteration 73/1000 | Loss: 0.00002889
Iteration 74/1000 | Loss: 0.00002889
Iteration 75/1000 | Loss: 0.00002889
Iteration 76/1000 | Loss: 0.00002889
Iteration 77/1000 | Loss: 0.00002889
Iteration 78/1000 | Loss: 0.00002888
Iteration 79/1000 | Loss: 0.00002888
Iteration 80/1000 | Loss: 0.00002888
Iteration 81/1000 | Loss: 0.00002888
Iteration 82/1000 | Loss: 0.00002888
Iteration 83/1000 | Loss: 0.00002888
Iteration 84/1000 | Loss: 0.00002887
Iteration 85/1000 | Loss: 0.00002887
Iteration 86/1000 | Loss: 0.00002887
Iteration 87/1000 | Loss: 0.00002887
Iteration 88/1000 | Loss: 0.00002886
Iteration 89/1000 | Loss: 0.00002886
Iteration 90/1000 | Loss: 0.00002886
Iteration 91/1000 | Loss: 0.00002886
Iteration 92/1000 | Loss: 0.00002886
Iteration 93/1000 | Loss: 0.00002886
Iteration 94/1000 | Loss: 0.00002886
Iteration 95/1000 | Loss: 0.00002886
Iteration 96/1000 | Loss: 0.00002886
Iteration 97/1000 | Loss: 0.00002885
Iteration 98/1000 | Loss: 0.00002885
Iteration 99/1000 | Loss: 0.00002885
Iteration 100/1000 | Loss: 0.00002885
Iteration 101/1000 | Loss: 0.00002885
Iteration 102/1000 | Loss: 0.00002885
Iteration 103/1000 | Loss: 0.00002885
Iteration 104/1000 | Loss: 0.00002885
Iteration 105/1000 | Loss: 0.00002885
Iteration 106/1000 | Loss: 0.00002885
Iteration 107/1000 | Loss: 0.00002885
Iteration 108/1000 | Loss: 0.00002885
Iteration 109/1000 | Loss: 0.00002885
Iteration 110/1000 | Loss: 0.00002884
Iteration 111/1000 | Loss: 0.00002884
Iteration 112/1000 | Loss: 0.00002884
Iteration 113/1000 | Loss: 0.00002884
Iteration 114/1000 | Loss: 0.00002884
Iteration 115/1000 | Loss: 0.00002884
Iteration 116/1000 | Loss: 0.00002884
Iteration 117/1000 | Loss: 0.00002884
Iteration 118/1000 | Loss: 0.00002883
Iteration 119/1000 | Loss: 0.00002883
Iteration 120/1000 | Loss: 0.00002883
Iteration 121/1000 | Loss: 0.00002883
Iteration 122/1000 | Loss: 0.00002883
Iteration 123/1000 | Loss: 0.00002883
Iteration 124/1000 | Loss: 0.00002883
Iteration 125/1000 | Loss: 0.00002883
Iteration 126/1000 | Loss: 0.00002883
Iteration 127/1000 | Loss: 0.00002883
Iteration 128/1000 | Loss: 0.00002883
Iteration 129/1000 | Loss: 0.00002883
Iteration 130/1000 | Loss: 0.00002883
Iteration 131/1000 | Loss: 0.00002883
Iteration 132/1000 | Loss: 0.00002882
Iteration 133/1000 | Loss: 0.00002882
Iteration 134/1000 | Loss: 0.00002882
Iteration 135/1000 | Loss: 0.00002882
Iteration 136/1000 | Loss: 0.00002882
Iteration 137/1000 | Loss: 0.00002882
Iteration 138/1000 | Loss: 0.00002882
Iteration 139/1000 | Loss: 0.00002882
Iteration 140/1000 | Loss: 0.00002882
Iteration 141/1000 | Loss: 0.00002882
Iteration 142/1000 | Loss: 0.00002882
Iteration 143/1000 | Loss: 0.00002881
Iteration 144/1000 | Loss: 0.00002881
Iteration 145/1000 | Loss: 0.00002881
Iteration 146/1000 | Loss: 0.00002881
Iteration 147/1000 | Loss: 0.00002881
Iteration 148/1000 | Loss: 0.00002881
Iteration 149/1000 | Loss: 0.00002881
Iteration 150/1000 | Loss: 0.00002881
Iteration 151/1000 | Loss: 0.00002881
Iteration 152/1000 | Loss: 0.00002881
Iteration 153/1000 | Loss: 0.00002881
Iteration 154/1000 | Loss: 0.00002880
Iteration 155/1000 | Loss: 0.00002880
Iteration 156/1000 | Loss: 0.00002880
Iteration 157/1000 | Loss: 0.00002880
Iteration 158/1000 | Loss: 0.00002880
Iteration 159/1000 | Loss: 0.00002880
Iteration 160/1000 | Loss: 0.00002880
Iteration 161/1000 | Loss: 0.00002880
Iteration 162/1000 | Loss: 0.00002880
Iteration 163/1000 | Loss: 0.00002880
Iteration 164/1000 | Loss: 0.00002880
Iteration 165/1000 | Loss: 0.00002880
Iteration 166/1000 | Loss: 0.00002880
Iteration 167/1000 | Loss: 0.00002880
Iteration 168/1000 | Loss: 0.00002880
Iteration 169/1000 | Loss: 0.00002880
Iteration 170/1000 | Loss: 0.00002880
Iteration 171/1000 | Loss: 0.00002880
Iteration 172/1000 | Loss: 0.00002879
Iteration 173/1000 | Loss: 0.00002879
Iteration 174/1000 | Loss: 0.00002879
Iteration 175/1000 | Loss: 0.00002879
Iteration 176/1000 | Loss: 0.00002879
Iteration 177/1000 | Loss: 0.00002879
Iteration 178/1000 | Loss: 0.00002879
Iteration 179/1000 | Loss: 0.00002879
Iteration 180/1000 | Loss: 0.00002879
Iteration 181/1000 | Loss: 0.00002879
Iteration 182/1000 | Loss: 0.00002879
Iteration 183/1000 | Loss: 0.00002879
Iteration 184/1000 | Loss: 0.00002879
Iteration 185/1000 | Loss: 0.00002879
Iteration 186/1000 | Loss: 0.00002879
Iteration 187/1000 | Loss: 0.00002879
Iteration 188/1000 | Loss: 0.00002879
Iteration 189/1000 | Loss: 0.00002879
Iteration 190/1000 | Loss: 0.00002879
Iteration 191/1000 | Loss: 0.00002879
Iteration 192/1000 | Loss: 0.00002879
Iteration 193/1000 | Loss: 0.00002879
Iteration 194/1000 | Loss: 0.00002879
Iteration 195/1000 | Loss: 0.00002879
Iteration 196/1000 | Loss: 0.00002879
Iteration 197/1000 | Loss: 0.00002879
Iteration 198/1000 | Loss: 0.00002879
Iteration 199/1000 | Loss: 0.00002879
Iteration 200/1000 | Loss: 0.00002879
Iteration 201/1000 | Loss: 0.00002879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.8787269911845215e-05, 2.8787269911845215e-05, 2.8787269911845215e-05, 2.8787269911845215e-05, 2.8787269911845215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8787269911845215e-05

Optimization complete. Final v2v error: 4.149674415588379 mm

Highest mean error: 4.931674957275391 mm for frame 98

Lowest mean error: 3.2209229469299316 mm for frame 49

Saving results

Total time: 49.169228076934814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988318
Iteration 2/25 | Loss: 0.00126059
Iteration 3/25 | Loss: 0.00073272
Iteration 4/25 | Loss: 0.00069087
Iteration 5/25 | Loss: 0.00067986
Iteration 6/25 | Loss: 0.00067731
Iteration 7/25 | Loss: 0.00067644
Iteration 8/25 | Loss: 0.00067641
Iteration 9/25 | Loss: 0.00067641
Iteration 10/25 | Loss: 0.00067641
Iteration 11/25 | Loss: 0.00067641
Iteration 12/25 | Loss: 0.00067641
Iteration 13/25 | Loss: 0.00067641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006764099816791713, 0.0006764099816791713, 0.0006764099816791713, 0.0006764099816791713, 0.0006764099816791713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006764099816791713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76459122
Iteration 2/25 | Loss: 0.00028073
Iteration 3/25 | Loss: 0.00028073
Iteration 4/25 | Loss: 0.00028072
Iteration 5/25 | Loss: 0.00028072
Iteration 6/25 | Loss: 0.00028072
Iteration 7/25 | Loss: 0.00028072
Iteration 8/25 | Loss: 0.00028072
Iteration 9/25 | Loss: 0.00028072
Iteration 10/25 | Loss: 0.00028072
Iteration 11/25 | Loss: 0.00028072
Iteration 12/25 | Loss: 0.00028072
Iteration 13/25 | Loss: 0.00028072
Iteration 14/25 | Loss: 0.00028072
Iteration 15/25 | Loss: 0.00028072
Iteration 16/25 | Loss: 0.00028072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00028072315035387874, 0.00028072315035387874, 0.00028072315035387874, 0.00028072315035387874, 0.00028072315035387874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028072315035387874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028072
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001895
Iteration 4/1000 | Loss: 0.00001779
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001609
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001576
Iteration 10/1000 | Loss: 0.00001575
Iteration 11/1000 | Loss: 0.00001575
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001573
Iteration 14/1000 | Loss: 0.00001572
Iteration 15/1000 | Loss: 0.00001566
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001561
Iteration 20/1000 | Loss: 0.00001559
Iteration 21/1000 | Loss: 0.00001559
Iteration 22/1000 | Loss: 0.00001558
Iteration 23/1000 | Loss: 0.00001558
Iteration 24/1000 | Loss: 0.00001558
Iteration 25/1000 | Loss: 0.00001557
Iteration 26/1000 | Loss: 0.00001557
Iteration 27/1000 | Loss: 0.00001556
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00001554
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001553
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001549
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001547
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001546
Iteration 41/1000 | Loss: 0.00001546
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00001545
Iteration 44/1000 | Loss: 0.00001544
Iteration 45/1000 | Loss: 0.00001543
Iteration 46/1000 | Loss: 0.00001542
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001540
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001538
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001538
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001538
Iteration 58/1000 | Loss: 0.00001538
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001538
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001537
Iteration 65/1000 | Loss: 0.00001537
Iteration 66/1000 | Loss: 0.00001536
Iteration 67/1000 | Loss: 0.00001535
Iteration 68/1000 | Loss: 0.00001535
Iteration 69/1000 | Loss: 0.00001535
Iteration 70/1000 | Loss: 0.00001535
Iteration 71/1000 | Loss: 0.00001534
Iteration 72/1000 | Loss: 0.00001534
Iteration 73/1000 | Loss: 0.00001534
Iteration 74/1000 | Loss: 0.00001534
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001534
Iteration 78/1000 | Loss: 0.00001534
Iteration 79/1000 | Loss: 0.00001534
Iteration 80/1000 | Loss: 0.00001534
Iteration 81/1000 | Loss: 0.00001534
Iteration 82/1000 | Loss: 0.00001533
Iteration 83/1000 | Loss: 0.00001533
Iteration 84/1000 | Loss: 0.00001533
Iteration 85/1000 | Loss: 0.00001532
Iteration 86/1000 | Loss: 0.00001532
Iteration 87/1000 | Loss: 0.00001532
Iteration 88/1000 | Loss: 0.00001532
Iteration 89/1000 | Loss: 0.00001532
Iteration 90/1000 | Loss: 0.00001531
Iteration 91/1000 | Loss: 0.00001531
Iteration 92/1000 | Loss: 0.00001531
Iteration 93/1000 | Loss: 0.00001531
Iteration 94/1000 | Loss: 0.00001531
Iteration 95/1000 | Loss: 0.00001530
Iteration 96/1000 | Loss: 0.00001530
Iteration 97/1000 | Loss: 0.00001530
Iteration 98/1000 | Loss: 0.00001530
Iteration 99/1000 | Loss: 0.00001529
Iteration 100/1000 | Loss: 0.00001529
Iteration 101/1000 | Loss: 0.00001529
Iteration 102/1000 | Loss: 0.00001528
Iteration 103/1000 | Loss: 0.00001528
Iteration 104/1000 | Loss: 0.00001528
Iteration 105/1000 | Loss: 0.00001528
Iteration 106/1000 | Loss: 0.00001528
Iteration 107/1000 | Loss: 0.00001528
Iteration 108/1000 | Loss: 0.00001528
Iteration 109/1000 | Loss: 0.00001528
Iteration 110/1000 | Loss: 0.00001528
Iteration 111/1000 | Loss: 0.00001527
Iteration 112/1000 | Loss: 0.00001527
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001525
Iteration 116/1000 | Loss: 0.00001525
Iteration 117/1000 | Loss: 0.00001525
Iteration 118/1000 | Loss: 0.00001524
Iteration 119/1000 | Loss: 0.00001524
Iteration 120/1000 | Loss: 0.00001524
Iteration 121/1000 | Loss: 0.00001524
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001523
Iteration 125/1000 | Loss: 0.00001523
Iteration 126/1000 | Loss: 0.00001523
Iteration 127/1000 | Loss: 0.00001523
Iteration 128/1000 | Loss: 0.00001523
Iteration 129/1000 | Loss: 0.00001523
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00001522
Iteration 132/1000 | Loss: 0.00001522
Iteration 133/1000 | Loss: 0.00001522
Iteration 134/1000 | Loss: 0.00001522
Iteration 135/1000 | Loss: 0.00001522
Iteration 136/1000 | Loss: 0.00001522
Iteration 137/1000 | Loss: 0.00001522
Iteration 138/1000 | Loss: 0.00001522
Iteration 139/1000 | Loss: 0.00001522
Iteration 140/1000 | Loss: 0.00001522
Iteration 141/1000 | Loss: 0.00001522
Iteration 142/1000 | Loss: 0.00001522
Iteration 143/1000 | Loss: 0.00001522
Iteration 144/1000 | Loss: 0.00001522
Iteration 145/1000 | Loss: 0.00001522
Iteration 146/1000 | Loss: 0.00001522
Iteration 147/1000 | Loss: 0.00001522
Iteration 148/1000 | Loss: 0.00001522
Iteration 149/1000 | Loss: 0.00001521
Iteration 150/1000 | Loss: 0.00001521
Iteration 151/1000 | Loss: 0.00001521
Iteration 152/1000 | Loss: 0.00001521
Iteration 153/1000 | Loss: 0.00001521
Iteration 154/1000 | Loss: 0.00001521
Iteration 155/1000 | Loss: 0.00001521
Iteration 156/1000 | Loss: 0.00001521
Iteration 157/1000 | Loss: 0.00001521
Iteration 158/1000 | Loss: 0.00001521
Iteration 159/1000 | Loss: 0.00001520
Iteration 160/1000 | Loss: 0.00001520
Iteration 161/1000 | Loss: 0.00001520
Iteration 162/1000 | Loss: 0.00001520
Iteration 163/1000 | Loss: 0.00001520
Iteration 164/1000 | Loss: 0.00001520
Iteration 165/1000 | Loss: 0.00001520
Iteration 166/1000 | Loss: 0.00001520
Iteration 167/1000 | Loss: 0.00001520
Iteration 168/1000 | Loss: 0.00001520
Iteration 169/1000 | Loss: 0.00001520
Iteration 170/1000 | Loss: 0.00001520
Iteration 171/1000 | Loss: 0.00001520
Iteration 172/1000 | Loss: 0.00001520
Iteration 173/1000 | Loss: 0.00001520
Iteration 174/1000 | Loss: 0.00001520
Iteration 175/1000 | Loss: 0.00001520
Iteration 176/1000 | Loss: 0.00001520
Iteration 177/1000 | Loss: 0.00001520
Iteration 178/1000 | Loss: 0.00001520
Iteration 179/1000 | Loss: 0.00001520
Iteration 180/1000 | Loss: 0.00001520
Iteration 181/1000 | Loss: 0.00001520
Iteration 182/1000 | Loss: 0.00001520
Iteration 183/1000 | Loss: 0.00001520
Iteration 184/1000 | Loss: 0.00001520
Iteration 185/1000 | Loss: 0.00001520
Iteration 186/1000 | Loss: 0.00001520
Iteration 187/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.5195158994174562e-05, 1.5195158994174562e-05, 1.5195158994174562e-05, 1.5195158994174562e-05, 1.5195158994174562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5195158994174562e-05

Optimization complete. Final v2v error: 3.3027586936950684 mm

Highest mean error: 3.5542173385620117 mm for frame 77

Lowest mean error: 3.1346285343170166 mm for frame 8

Saving results

Total time: 36.80578136444092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095337
Iteration 2/25 | Loss: 0.00171540
Iteration 3/25 | Loss: 0.00116638
Iteration 4/25 | Loss: 0.00107433
Iteration 5/25 | Loss: 0.00101334
Iteration 6/25 | Loss: 0.00097263
Iteration 7/25 | Loss: 0.00096617
Iteration 8/25 | Loss: 0.00094588
Iteration 9/25 | Loss: 0.00096031
Iteration 10/25 | Loss: 0.00089404
Iteration 11/25 | Loss: 0.00087365
Iteration 12/25 | Loss: 0.00086836
Iteration 13/25 | Loss: 0.00087925
Iteration 14/25 | Loss: 0.00087401
Iteration 15/25 | Loss: 0.00088250
Iteration 16/25 | Loss: 0.00088612
Iteration 17/25 | Loss: 0.00088052
Iteration 18/25 | Loss: 0.00087164
Iteration 19/25 | Loss: 0.00086129
Iteration 20/25 | Loss: 0.00085655
Iteration 21/25 | Loss: 0.00085241
Iteration 22/25 | Loss: 0.00084937
Iteration 23/25 | Loss: 0.00084811
Iteration 24/25 | Loss: 0.00084722
Iteration 25/25 | Loss: 0.00084827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32487130
Iteration 2/25 | Loss: 0.00030538
Iteration 3/25 | Loss: 0.00030534
Iteration 4/25 | Loss: 0.00030534
Iteration 5/25 | Loss: 0.00030534
Iteration 6/25 | Loss: 0.00030534
Iteration 7/25 | Loss: 0.00030534
Iteration 8/25 | Loss: 0.00030534
Iteration 9/25 | Loss: 0.00030534
Iteration 10/25 | Loss: 0.00030534
Iteration 11/25 | Loss: 0.00030534
Iteration 12/25 | Loss: 0.00030534
Iteration 13/25 | Loss: 0.00030534
Iteration 14/25 | Loss: 0.00030534
Iteration 15/25 | Loss: 0.00030534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0003053373657166958, 0.0003053373657166958, 0.0003053373657166958, 0.0003053373657166958, 0.0003053373657166958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003053373657166958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030534
Iteration 2/1000 | Loss: 0.00004982
Iteration 3/1000 | Loss: 0.00003575
Iteration 4/1000 | Loss: 0.00003299
Iteration 5/1000 | Loss: 0.00004735
Iteration 6/1000 | Loss: 0.00003387
Iteration 7/1000 | Loss: 0.00003101
Iteration 8/1000 | Loss: 0.00002934
Iteration 9/1000 | Loss: 0.00002845
Iteration 10/1000 | Loss: 0.00002815
Iteration 11/1000 | Loss: 0.00002782
Iteration 12/1000 | Loss: 0.00002756
Iteration 13/1000 | Loss: 0.00002738
Iteration 14/1000 | Loss: 0.00002734
Iteration 15/1000 | Loss: 0.00002714
Iteration 16/1000 | Loss: 0.00002706
Iteration 17/1000 | Loss: 0.00002701
Iteration 18/1000 | Loss: 0.00002699
Iteration 19/1000 | Loss: 0.00002698
Iteration 20/1000 | Loss: 0.00002698
Iteration 21/1000 | Loss: 0.00002697
Iteration 22/1000 | Loss: 0.00002697
Iteration 23/1000 | Loss: 0.00002697
Iteration 24/1000 | Loss: 0.00002697
Iteration 25/1000 | Loss: 0.00002696
Iteration 26/1000 | Loss: 0.00002696
Iteration 27/1000 | Loss: 0.00002696
Iteration 28/1000 | Loss: 0.00002696
Iteration 29/1000 | Loss: 0.00002696
Iteration 30/1000 | Loss: 0.00002696
Iteration 31/1000 | Loss: 0.00002695
Iteration 32/1000 | Loss: 0.00002684
Iteration 33/1000 | Loss: 0.00002681
Iteration 34/1000 | Loss: 0.00002675
Iteration 35/1000 | Loss: 0.00002674
Iteration 36/1000 | Loss: 0.00002673
Iteration 37/1000 | Loss: 0.00002670
Iteration 38/1000 | Loss: 0.00002670
Iteration 39/1000 | Loss: 0.00002669
Iteration 40/1000 | Loss: 0.00002668
Iteration 41/1000 | Loss: 0.00002668
Iteration 42/1000 | Loss: 0.00002666
Iteration 43/1000 | Loss: 0.00002665
Iteration 44/1000 | Loss: 0.00002665
Iteration 45/1000 | Loss: 0.00002665
Iteration 46/1000 | Loss: 0.00002665
Iteration 47/1000 | Loss: 0.00002664
Iteration 48/1000 | Loss: 0.00002664
Iteration 49/1000 | Loss: 0.00002664
Iteration 50/1000 | Loss: 0.00002663
Iteration 51/1000 | Loss: 0.00002663
Iteration 52/1000 | Loss: 0.00002663
Iteration 53/1000 | Loss: 0.00002663
Iteration 54/1000 | Loss: 0.00002663
Iteration 55/1000 | Loss: 0.00002663
Iteration 56/1000 | Loss: 0.00002663
Iteration 57/1000 | Loss: 0.00002663
Iteration 58/1000 | Loss: 0.00002663
Iteration 59/1000 | Loss: 0.00002663
Iteration 60/1000 | Loss: 0.00002663
Iteration 61/1000 | Loss: 0.00002663
Iteration 62/1000 | Loss: 0.00002662
Iteration 63/1000 | Loss: 0.00002662
Iteration 64/1000 | Loss: 0.00002662
Iteration 65/1000 | Loss: 0.00002662
Iteration 66/1000 | Loss: 0.00002661
Iteration 67/1000 | Loss: 0.00002661
Iteration 68/1000 | Loss: 0.00002661
Iteration 69/1000 | Loss: 0.00002661
Iteration 70/1000 | Loss: 0.00002661
Iteration 71/1000 | Loss: 0.00002661
Iteration 72/1000 | Loss: 0.00002661
Iteration 73/1000 | Loss: 0.00002661
Iteration 74/1000 | Loss: 0.00002661
Iteration 75/1000 | Loss: 0.00002661
Iteration 76/1000 | Loss: 0.00002661
Iteration 77/1000 | Loss: 0.00002661
Iteration 78/1000 | Loss: 0.00002661
Iteration 79/1000 | Loss: 0.00002661
Iteration 80/1000 | Loss: 0.00002661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.6607465770212002e-05, 2.6607465770212002e-05, 2.6607465770212002e-05, 2.6607465770212002e-05, 2.6607465770212002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6607465770212002e-05

Optimization complete. Final v2v error: 4.170908451080322 mm

Highest mean error: 6.309512138366699 mm for frame 65

Lowest mean error: 3.614394426345825 mm for frame 3

Saving results

Total time: 86.73470520973206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794970
Iteration 2/25 | Loss: 0.00094183
Iteration 3/25 | Loss: 0.00071012
Iteration 4/25 | Loss: 0.00068118
Iteration 5/25 | Loss: 0.00067359
Iteration 6/25 | Loss: 0.00067376
Iteration 7/25 | Loss: 0.00067342
Iteration 8/25 | Loss: 0.00067019
Iteration 9/25 | Loss: 0.00066876
Iteration 10/25 | Loss: 0.00066807
Iteration 11/25 | Loss: 0.00066745
Iteration 12/25 | Loss: 0.00066705
Iteration 13/25 | Loss: 0.00066691
Iteration 14/25 | Loss: 0.00066679
Iteration 15/25 | Loss: 0.00066677
Iteration 16/25 | Loss: 0.00066676
Iteration 17/25 | Loss: 0.00066676
Iteration 18/25 | Loss: 0.00066676
Iteration 19/25 | Loss: 0.00066676
Iteration 20/25 | Loss: 0.00066676
Iteration 21/25 | Loss: 0.00066676
Iteration 22/25 | Loss: 0.00066676
Iteration 23/25 | Loss: 0.00066676
Iteration 24/25 | Loss: 0.00066676
Iteration 25/25 | Loss: 0.00066675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.54403949
Iteration 2/25 | Loss: 0.00033877
Iteration 3/25 | Loss: 0.00033458
Iteration 4/25 | Loss: 0.00033458
Iteration 5/25 | Loss: 0.00033458
Iteration 6/25 | Loss: 0.00033458
Iteration 7/25 | Loss: 0.00033458
Iteration 8/25 | Loss: 0.00033458
Iteration 9/25 | Loss: 0.00033458
Iteration 10/25 | Loss: 0.00033458
Iteration 11/25 | Loss: 0.00033458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0003345767327118665, 0.0003345767327118665, 0.0003345767327118665, 0.0003345767327118665, 0.0003345767327118665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003345767327118665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033458
Iteration 2/1000 | Loss: 0.00003132
Iteration 3/1000 | Loss: 0.00002881
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00005078
Iteration 6/1000 | Loss: 0.00018063
Iteration 7/1000 | Loss: 0.00001457
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001373
Iteration 11/1000 | Loss: 0.00001352
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001346
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001333
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001317
Iteration 18/1000 | Loss: 0.00001315
Iteration 19/1000 | Loss: 0.00001315
Iteration 20/1000 | Loss: 0.00001309
Iteration 21/1000 | Loss: 0.00001308
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001305
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001300
Iteration 35/1000 | Loss: 0.00001300
Iteration 36/1000 | Loss: 0.00001299
Iteration 37/1000 | Loss: 0.00001299
Iteration 38/1000 | Loss: 0.00001299
Iteration 39/1000 | Loss: 0.00001298
Iteration 40/1000 | Loss: 0.00001298
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001297
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001282
Iteration 62/1000 | Loss: 0.00001282
Iteration 63/1000 | Loss: 0.00001282
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001280
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001279
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001278
Iteration 81/1000 | Loss: 0.00001278
Iteration 82/1000 | Loss: 0.00001278
Iteration 83/1000 | Loss: 0.00001278
Iteration 84/1000 | Loss: 0.00001278
Iteration 85/1000 | Loss: 0.00001278
Iteration 86/1000 | Loss: 0.00001278
Iteration 87/1000 | Loss: 0.00001278
Iteration 88/1000 | Loss: 0.00001277
Iteration 89/1000 | Loss: 0.00001277
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001276
Iteration 94/1000 | Loss: 0.00001276
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001275
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001274
Iteration 106/1000 | Loss: 0.00001274
Iteration 107/1000 | Loss: 0.00001274
Iteration 108/1000 | Loss: 0.00001274
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001273
Iteration 113/1000 | Loss: 0.00001273
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001273
Iteration 117/1000 | Loss: 0.00001273
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001273
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001273
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001272
Iteration 131/1000 | Loss: 0.00001272
Iteration 132/1000 | Loss: 0.00001272
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001271
Iteration 135/1000 | Loss: 0.00001271
Iteration 136/1000 | Loss: 0.00001271
Iteration 137/1000 | Loss: 0.00001271
Iteration 138/1000 | Loss: 0.00001271
Iteration 139/1000 | Loss: 0.00001271
Iteration 140/1000 | Loss: 0.00001271
Iteration 141/1000 | Loss: 0.00001271
Iteration 142/1000 | Loss: 0.00001271
Iteration 143/1000 | Loss: 0.00001271
Iteration 144/1000 | Loss: 0.00001271
Iteration 145/1000 | Loss: 0.00001271
Iteration 146/1000 | Loss: 0.00001271
Iteration 147/1000 | Loss: 0.00001271
Iteration 148/1000 | Loss: 0.00001271
Iteration 149/1000 | Loss: 0.00001271
Iteration 150/1000 | Loss: 0.00001271
Iteration 151/1000 | Loss: 0.00001271
Iteration 152/1000 | Loss: 0.00001271
Iteration 153/1000 | Loss: 0.00001271
Iteration 154/1000 | Loss: 0.00001270
Iteration 155/1000 | Loss: 0.00001270
Iteration 156/1000 | Loss: 0.00001270
Iteration 157/1000 | Loss: 0.00001270
Iteration 158/1000 | Loss: 0.00001270
Iteration 159/1000 | Loss: 0.00001270
Iteration 160/1000 | Loss: 0.00001270
Iteration 161/1000 | Loss: 0.00001270
Iteration 162/1000 | Loss: 0.00001270
Iteration 163/1000 | Loss: 0.00001270
Iteration 164/1000 | Loss: 0.00001270
Iteration 165/1000 | Loss: 0.00001270
Iteration 166/1000 | Loss: 0.00001270
Iteration 167/1000 | Loss: 0.00001270
Iteration 168/1000 | Loss: 0.00001270
Iteration 169/1000 | Loss: 0.00001269
Iteration 170/1000 | Loss: 0.00001269
Iteration 171/1000 | Loss: 0.00001269
Iteration 172/1000 | Loss: 0.00001269
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00001269
Iteration 175/1000 | Loss: 0.00001269
Iteration 176/1000 | Loss: 0.00001269
Iteration 177/1000 | Loss: 0.00001269
Iteration 178/1000 | Loss: 0.00001269
Iteration 179/1000 | Loss: 0.00001269
Iteration 180/1000 | Loss: 0.00001269
Iteration 181/1000 | Loss: 0.00001269
Iteration 182/1000 | Loss: 0.00001269
Iteration 183/1000 | Loss: 0.00001269
Iteration 184/1000 | Loss: 0.00001269
Iteration 185/1000 | Loss: 0.00001268
Iteration 186/1000 | Loss: 0.00001268
Iteration 187/1000 | Loss: 0.00001268
Iteration 188/1000 | Loss: 0.00001268
Iteration 189/1000 | Loss: 0.00001268
Iteration 190/1000 | Loss: 0.00001268
Iteration 191/1000 | Loss: 0.00001268
Iteration 192/1000 | Loss: 0.00001268
Iteration 193/1000 | Loss: 0.00001268
Iteration 194/1000 | Loss: 0.00001268
Iteration 195/1000 | Loss: 0.00001268
Iteration 196/1000 | Loss: 0.00001268
Iteration 197/1000 | Loss: 0.00001268
Iteration 198/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.2682858141488396e-05, 1.2682858141488396e-05, 1.2682858141488396e-05, 1.2682858141488396e-05, 1.2682858141488396e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2682858141488396e-05

Optimization complete. Final v2v error: 3.036464214324951 mm

Highest mean error: 3.3072354793548584 mm for frame 43

Lowest mean error: 2.7169301509857178 mm for frame 22

Saving results

Total time: 60.40207266807556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043527
Iteration 2/25 | Loss: 0.00198330
Iteration 3/25 | Loss: 0.00142703
Iteration 4/25 | Loss: 0.00114352
Iteration 5/25 | Loss: 0.00107708
Iteration 6/25 | Loss: 0.00105600
Iteration 7/25 | Loss: 0.00097942
Iteration 8/25 | Loss: 0.00095540
Iteration 9/25 | Loss: 0.00089571
Iteration 10/25 | Loss: 0.00088537
Iteration 11/25 | Loss: 0.00086862
Iteration 12/25 | Loss: 0.00084101
Iteration 13/25 | Loss: 0.00083298
Iteration 14/25 | Loss: 0.00083588
Iteration 15/25 | Loss: 0.00082123
Iteration 16/25 | Loss: 0.00081918
Iteration 17/25 | Loss: 0.00081125
Iteration 18/25 | Loss: 0.00080654
Iteration 19/25 | Loss: 0.00079399
Iteration 20/25 | Loss: 0.00077736
Iteration 21/25 | Loss: 0.00077724
Iteration 22/25 | Loss: 0.00076275
Iteration 23/25 | Loss: 0.00076303
Iteration 24/25 | Loss: 0.00075770
Iteration 25/25 | Loss: 0.00075343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.20116854
Iteration 2/25 | Loss: 0.00037925
Iteration 3/25 | Loss: 0.00037925
Iteration 4/25 | Loss: 0.00037925
Iteration 5/25 | Loss: 0.00037925
Iteration 6/25 | Loss: 0.00037925
Iteration 7/25 | Loss: 0.00037925
Iteration 8/25 | Loss: 0.00037925
Iteration 9/25 | Loss: 0.00037925
Iteration 10/25 | Loss: 0.00037925
Iteration 11/25 | Loss: 0.00037925
Iteration 12/25 | Loss: 0.00037925
Iteration 13/25 | Loss: 0.00037925
Iteration 14/25 | Loss: 0.00037925
Iteration 15/25 | Loss: 0.00037925
Iteration 16/25 | Loss: 0.00037925
Iteration 17/25 | Loss: 0.00037925
Iteration 18/25 | Loss: 0.00037925
Iteration 19/25 | Loss: 0.00037925
Iteration 20/25 | Loss: 0.00037925
Iteration 21/25 | Loss: 0.00037925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00037924875505268574, 0.00037924875505268574, 0.00037924875505268574, 0.00037924875505268574, 0.00037924875505268574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037924875505268574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037925
Iteration 2/1000 | Loss: 0.00031301
Iteration 3/1000 | Loss: 0.00014643
Iteration 4/1000 | Loss: 0.00034342
Iteration 5/1000 | Loss: 0.00026604
Iteration 6/1000 | Loss: 0.00022374
Iteration 7/1000 | Loss: 0.00004771
Iteration 8/1000 | Loss: 0.00021937
Iteration 9/1000 | Loss: 0.00023332
Iteration 10/1000 | Loss: 0.00002979
Iteration 11/1000 | Loss: 0.00002438
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00035706
Iteration 14/1000 | Loss: 0.00003532
Iteration 15/1000 | Loss: 0.00002137
Iteration 16/1000 | Loss: 0.00001957
Iteration 17/1000 | Loss: 0.00001847
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001678
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001648
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001643
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001633
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001629
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001625
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001622
Iteration 40/1000 | Loss: 0.00001622
Iteration 41/1000 | Loss: 0.00001622
Iteration 42/1000 | Loss: 0.00001621
Iteration 43/1000 | Loss: 0.00001621
Iteration 44/1000 | Loss: 0.00001621
Iteration 45/1000 | Loss: 0.00001621
Iteration 46/1000 | Loss: 0.00001620
Iteration 47/1000 | Loss: 0.00001620
Iteration 48/1000 | Loss: 0.00001620
Iteration 49/1000 | Loss: 0.00001619
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001619
Iteration 52/1000 | Loss: 0.00001619
Iteration 53/1000 | Loss: 0.00001619
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001616
Iteration 67/1000 | Loss: 0.00001616
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001613
Iteration 82/1000 | Loss: 0.00001613
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001612
Iteration 94/1000 | Loss: 0.00001611
Iteration 95/1000 | Loss: 0.00001611
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001608
Iteration 105/1000 | Loss: 0.00001607
Iteration 106/1000 | Loss: 0.00001607
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001606
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001604
Iteration 111/1000 | Loss: 0.00001604
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001604
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001604
Iteration 123/1000 | Loss: 0.00001604
Iteration 124/1000 | Loss: 0.00001604
Iteration 125/1000 | Loss: 0.00001604
Iteration 126/1000 | Loss: 0.00001604
Iteration 127/1000 | Loss: 0.00001604
Iteration 128/1000 | Loss: 0.00001604
Iteration 129/1000 | Loss: 0.00001604
Iteration 130/1000 | Loss: 0.00001604
Iteration 131/1000 | Loss: 0.00001604
Iteration 132/1000 | Loss: 0.00001604
Iteration 133/1000 | Loss: 0.00001604
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001604
Iteration 137/1000 | Loss: 0.00001604
Iteration 138/1000 | Loss: 0.00001604
Iteration 139/1000 | Loss: 0.00001604
Iteration 140/1000 | Loss: 0.00001604
Iteration 141/1000 | Loss: 0.00001604
Iteration 142/1000 | Loss: 0.00001604
Iteration 143/1000 | Loss: 0.00001604
Iteration 144/1000 | Loss: 0.00001604
Iteration 145/1000 | Loss: 0.00001604
Iteration 146/1000 | Loss: 0.00001604
Iteration 147/1000 | Loss: 0.00001604
Iteration 148/1000 | Loss: 0.00001604
Iteration 149/1000 | Loss: 0.00001604
Iteration 150/1000 | Loss: 0.00001604
Iteration 151/1000 | Loss: 0.00001604
Iteration 152/1000 | Loss: 0.00001604
Iteration 153/1000 | Loss: 0.00001604
Iteration 154/1000 | Loss: 0.00001604
Iteration 155/1000 | Loss: 0.00001604
Iteration 156/1000 | Loss: 0.00001604
Iteration 157/1000 | Loss: 0.00001604
Iteration 158/1000 | Loss: 0.00001604
Iteration 159/1000 | Loss: 0.00001604
Iteration 160/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.603541568329092e-05, 1.603541568329092e-05, 1.603541568329092e-05, 1.603541568329092e-05, 1.603541568329092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.603541568329092e-05

Optimization complete. Final v2v error: 3.427499532699585 mm

Highest mean error: 3.9577200412750244 mm for frame 54

Lowest mean error: 3.109482526779175 mm for frame 16

Saving results

Total time: 89.08198571205139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848714
Iteration 2/25 | Loss: 0.00095973
Iteration 3/25 | Loss: 0.00075864
Iteration 4/25 | Loss: 0.00072321
Iteration 5/25 | Loss: 0.00071255
Iteration 6/25 | Loss: 0.00071014
Iteration 7/25 | Loss: 0.00070952
Iteration 8/25 | Loss: 0.00070951
Iteration 9/25 | Loss: 0.00070951
Iteration 10/25 | Loss: 0.00070951
Iteration 11/25 | Loss: 0.00070951
Iteration 12/25 | Loss: 0.00070951
Iteration 13/25 | Loss: 0.00070951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007095143664628267, 0.0007095143664628267, 0.0007095143664628267, 0.0007095143664628267, 0.0007095143664628267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007095143664628267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60805261
Iteration 2/25 | Loss: 0.00037739
Iteration 3/25 | Loss: 0.00037738
Iteration 4/25 | Loss: 0.00037738
Iteration 5/25 | Loss: 0.00037738
Iteration 6/25 | Loss: 0.00037738
Iteration 7/25 | Loss: 0.00037738
Iteration 8/25 | Loss: 0.00037738
Iteration 9/25 | Loss: 0.00037738
Iteration 10/25 | Loss: 0.00037738
Iteration 11/25 | Loss: 0.00037738
Iteration 12/25 | Loss: 0.00037738
Iteration 13/25 | Loss: 0.00037738
Iteration 14/25 | Loss: 0.00037738
Iteration 15/25 | Loss: 0.00037738
Iteration 16/25 | Loss: 0.00037738
Iteration 17/25 | Loss: 0.00037738
Iteration 18/25 | Loss: 0.00037738
Iteration 19/25 | Loss: 0.00037738
Iteration 20/25 | Loss: 0.00037738
Iteration 21/25 | Loss: 0.00037738
Iteration 22/25 | Loss: 0.00037738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00037738162791356444, 0.00037738162791356444, 0.00037738162791356444, 0.00037738162791356444, 0.00037738162791356444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037738162791356444

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037738
Iteration 2/1000 | Loss: 0.00002975
Iteration 3/1000 | Loss: 0.00002096
Iteration 4/1000 | Loss: 0.00001900
Iteration 5/1000 | Loss: 0.00001812
Iteration 6/1000 | Loss: 0.00001746
Iteration 7/1000 | Loss: 0.00001704
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001640
Iteration 10/1000 | Loss: 0.00001634
Iteration 11/1000 | Loss: 0.00001616
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001606
Iteration 14/1000 | Loss: 0.00001601
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001588
Iteration 18/1000 | Loss: 0.00001587
Iteration 19/1000 | Loss: 0.00001587
Iteration 20/1000 | Loss: 0.00001587
Iteration 21/1000 | Loss: 0.00001586
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001581
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001579
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001576
Iteration 53/1000 | Loss: 0.00001576
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001575
Iteration 60/1000 | Loss: 0.00001574
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001573
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001572
Iteration 90/1000 | Loss: 0.00001572
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001571
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00001571
Iteration 102/1000 | Loss: 0.00001571
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001570
Iteration 106/1000 | Loss: 0.00001570
Iteration 107/1000 | Loss: 0.00001570
Iteration 108/1000 | Loss: 0.00001570
Iteration 109/1000 | Loss: 0.00001570
Iteration 110/1000 | Loss: 0.00001570
Iteration 111/1000 | Loss: 0.00001570
Iteration 112/1000 | Loss: 0.00001570
Iteration 113/1000 | Loss: 0.00001569
Iteration 114/1000 | Loss: 0.00001569
Iteration 115/1000 | Loss: 0.00001569
Iteration 116/1000 | Loss: 0.00001569
Iteration 117/1000 | Loss: 0.00001569
Iteration 118/1000 | Loss: 0.00001569
Iteration 119/1000 | Loss: 0.00001569
Iteration 120/1000 | Loss: 0.00001569
Iteration 121/1000 | Loss: 0.00001569
Iteration 122/1000 | Loss: 0.00001569
Iteration 123/1000 | Loss: 0.00001569
Iteration 124/1000 | Loss: 0.00001568
Iteration 125/1000 | Loss: 0.00001568
Iteration 126/1000 | Loss: 0.00001568
Iteration 127/1000 | Loss: 0.00001568
Iteration 128/1000 | Loss: 0.00001568
Iteration 129/1000 | Loss: 0.00001568
Iteration 130/1000 | Loss: 0.00001568
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001567
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001566
Iteration 141/1000 | Loss: 0.00001566
Iteration 142/1000 | Loss: 0.00001566
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001565
Iteration 149/1000 | Loss: 0.00001565
Iteration 150/1000 | Loss: 0.00001565
Iteration 151/1000 | Loss: 0.00001565
Iteration 152/1000 | Loss: 0.00001564
Iteration 153/1000 | Loss: 0.00001564
Iteration 154/1000 | Loss: 0.00001564
Iteration 155/1000 | Loss: 0.00001564
Iteration 156/1000 | Loss: 0.00001564
Iteration 157/1000 | Loss: 0.00001564
Iteration 158/1000 | Loss: 0.00001564
Iteration 159/1000 | Loss: 0.00001564
Iteration 160/1000 | Loss: 0.00001563
Iteration 161/1000 | Loss: 0.00001563
Iteration 162/1000 | Loss: 0.00001563
Iteration 163/1000 | Loss: 0.00001563
Iteration 164/1000 | Loss: 0.00001563
Iteration 165/1000 | Loss: 0.00001563
Iteration 166/1000 | Loss: 0.00001563
Iteration 167/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.563325167808216e-05, 1.563325167808216e-05, 1.563325167808216e-05, 1.563325167808216e-05, 1.563325167808216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.563325167808216e-05

Optimization complete. Final v2v error: 3.309558153152466 mm

Highest mean error: 4.028022766113281 mm for frame 86

Lowest mean error: 2.7416796684265137 mm for frame 146

Saving results

Total time: 39.09976243972778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410001
Iteration 2/25 | Loss: 0.00087442
Iteration 3/25 | Loss: 0.00071124
Iteration 4/25 | Loss: 0.00069000
Iteration 5/25 | Loss: 0.00068182
Iteration 6/25 | Loss: 0.00068054
Iteration 7/25 | Loss: 0.00068051
Iteration 8/25 | Loss: 0.00068023
Iteration 9/25 | Loss: 0.00068023
Iteration 10/25 | Loss: 0.00068023
Iteration 11/25 | Loss: 0.00068023
Iteration 12/25 | Loss: 0.00068023
Iteration 13/25 | Loss: 0.00068023
Iteration 14/25 | Loss: 0.00068023
Iteration 15/25 | Loss: 0.00068023
Iteration 16/25 | Loss: 0.00068023
Iteration 17/25 | Loss: 0.00068023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000680226949043572, 0.000680226949043572, 0.000680226949043572, 0.000680226949043572, 0.000680226949043572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000680226949043572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52648222
Iteration 2/25 | Loss: 0.00034131
Iteration 3/25 | Loss: 0.00034131
Iteration 4/25 | Loss: 0.00034131
Iteration 5/25 | Loss: 0.00034131
Iteration 6/25 | Loss: 0.00034131
Iteration 7/25 | Loss: 0.00034131
Iteration 8/25 | Loss: 0.00034131
Iteration 9/25 | Loss: 0.00034131
Iteration 10/25 | Loss: 0.00034131
Iteration 11/25 | Loss: 0.00034131
Iteration 12/25 | Loss: 0.00034131
Iteration 13/25 | Loss: 0.00034131
Iteration 14/25 | Loss: 0.00034131
Iteration 15/25 | Loss: 0.00034131
Iteration 16/25 | Loss: 0.00034131
Iteration 17/25 | Loss: 0.00034131
Iteration 18/25 | Loss: 0.00034131
Iteration 19/25 | Loss: 0.00034131
Iteration 20/25 | Loss: 0.00034131
Iteration 21/25 | Loss: 0.00034131
Iteration 22/25 | Loss: 0.00034131
Iteration 23/25 | Loss: 0.00034131
Iteration 24/25 | Loss: 0.00034131
Iteration 25/25 | Loss: 0.00034131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034131
Iteration 2/1000 | Loss: 0.00002589
Iteration 3/1000 | Loss: 0.00001591
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001417
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001347
Iteration 11/1000 | Loss: 0.00001337
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001329
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001328
Iteration 19/1000 | Loss: 0.00001327
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001327
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001324
Iteration 24/1000 | Loss: 0.00001323
Iteration 25/1000 | Loss: 0.00001323
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001322
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001310
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001310
Iteration 46/1000 | Loss: 0.00001310
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001309
Iteration 55/1000 | Loss: 0.00001309
Iteration 56/1000 | Loss: 0.00001309
Iteration 57/1000 | Loss: 0.00001309
Iteration 58/1000 | Loss: 0.00001308
Iteration 59/1000 | Loss: 0.00001308
Iteration 60/1000 | Loss: 0.00001308
Iteration 61/1000 | Loss: 0.00001307
Iteration 62/1000 | Loss: 0.00001307
Iteration 63/1000 | Loss: 0.00001307
Iteration 64/1000 | Loss: 0.00001307
Iteration 65/1000 | Loss: 0.00001307
Iteration 66/1000 | Loss: 0.00001307
Iteration 67/1000 | Loss: 0.00001306
Iteration 68/1000 | Loss: 0.00001306
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001306
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001305
Iteration 73/1000 | Loss: 0.00001305
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001304
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001302
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001301
Iteration 92/1000 | Loss: 0.00001300
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001299
Iteration 96/1000 | Loss: 0.00001299
Iteration 97/1000 | Loss: 0.00001299
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001297
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001297
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001296
Iteration 117/1000 | Loss: 0.00001296
Iteration 118/1000 | Loss: 0.00001296
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001295
Iteration 121/1000 | Loss: 0.00001295
Iteration 122/1000 | Loss: 0.00001295
Iteration 123/1000 | Loss: 0.00001295
Iteration 124/1000 | Loss: 0.00001295
Iteration 125/1000 | Loss: 0.00001295
Iteration 126/1000 | Loss: 0.00001295
Iteration 127/1000 | Loss: 0.00001295
Iteration 128/1000 | Loss: 0.00001295
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001294
Iteration 137/1000 | Loss: 0.00001294
Iteration 138/1000 | Loss: 0.00001293
Iteration 139/1000 | Loss: 0.00001293
Iteration 140/1000 | Loss: 0.00001293
Iteration 141/1000 | Loss: 0.00001293
Iteration 142/1000 | Loss: 0.00001292
Iteration 143/1000 | Loss: 0.00001292
Iteration 144/1000 | Loss: 0.00001292
Iteration 145/1000 | Loss: 0.00001292
Iteration 146/1000 | Loss: 0.00001292
Iteration 147/1000 | Loss: 0.00001292
Iteration 148/1000 | Loss: 0.00001292
Iteration 149/1000 | Loss: 0.00001292
Iteration 150/1000 | Loss: 0.00001292
Iteration 151/1000 | Loss: 0.00001292
Iteration 152/1000 | Loss: 0.00001292
Iteration 153/1000 | Loss: 0.00001292
Iteration 154/1000 | Loss: 0.00001292
Iteration 155/1000 | Loss: 0.00001292
Iteration 156/1000 | Loss: 0.00001292
Iteration 157/1000 | Loss: 0.00001292
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001292
Iteration 161/1000 | Loss: 0.00001292
Iteration 162/1000 | Loss: 0.00001292
Iteration 163/1000 | Loss: 0.00001292
Iteration 164/1000 | Loss: 0.00001292
Iteration 165/1000 | Loss: 0.00001292
Iteration 166/1000 | Loss: 0.00001292
Iteration 167/1000 | Loss: 0.00001292
Iteration 168/1000 | Loss: 0.00001292
Iteration 169/1000 | Loss: 0.00001292
Iteration 170/1000 | Loss: 0.00001292
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.2920678273076192e-05, 1.2920678273076192e-05, 1.2920678273076192e-05, 1.2920678273076192e-05, 1.2920678273076192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2920678273076192e-05

Optimization complete. Final v2v error: 3.046844005584717 mm

Highest mean error: 3.182985544204712 mm for frame 141

Lowest mean error: 2.9103190898895264 mm for frame 187

Saving results

Total time: 36.228172063827515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539132
Iteration 2/25 | Loss: 0.00088542
Iteration 3/25 | Loss: 0.00071207
Iteration 4/25 | Loss: 0.00069153
Iteration 5/25 | Loss: 0.00068736
Iteration 6/25 | Loss: 0.00068592
Iteration 7/25 | Loss: 0.00068570
Iteration 8/25 | Loss: 0.00068570
Iteration 9/25 | Loss: 0.00068570
Iteration 10/25 | Loss: 0.00068570
Iteration 11/25 | Loss: 0.00068570
Iteration 12/25 | Loss: 0.00068570
Iteration 13/25 | Loss: 0.00068570
Iteration 14/25 | Loss: 0.00068570
Iteration 15/25 | Loss: 0.00068570
Iteration 16/25 | Loss: 0.00068570
Iteration 17/25 | Loss: 0.00068570
Iteration 18/25 | Loss: 0.00068570
Iteration 19/25 | Loss: 0.00068570
Iteration 20/25 | Loss: 0.00068570
Iteration 21/25 | Loss: 0.00068570
Iteration 22/25 | Loss: 0.00068570
Iteration 23/25 | Loss: 0.00068570
Iteration 24/25 | Loss: 0.00068570
Iteration 25/25 | Loss: 0.00068570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006857023690827191, 0.0006857023690827191, 0.0006857023690827191, 0.0006857023690827191, 0.0006857023690827191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006857023690827191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18543363
Iteration 2/25 | Loss: 0.00028155
Iteration 3/25 | Loss: 0.00028154
Iteration 4/25 | Loss: 0.00028154
Iteration 5/25 | Loss: 0.00028154
Iteration 6/25 | Loss: 0.00028154
Iteration 7/25 | Loss: 0.00028154
Iteration 8/25 | Loss: 0.00028154
Iteration 9/25 | Loss: 0.00028154
Iteration 10/25 | Loss: 0.00028154
Iteration 11/25 | Loss: 0.00028154
Iteration 12/25 | Loss: 0.00028154
Iteration 13/25 | Loss: 0.00028154
Iteration 14/25 | Loss: 0.00028154
Iteration 15/25 | Loss: 0.00028154
Iteration 16/25 | Loss: 0.00028154
Iteration 17/25 | Loss: 0.00028154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00028154163737781346, 0.00028154163737781346, 0.00028154163737781346, 0.00028154163737781346, 0.00028154163737781346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028154163737781346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028154
Iteration 2/1000 | Loss: 0.00002140
Iteration 3/1000 | Loss: 0.00001505
Iteration 4/1000 | Loss: 0.00001389
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001246
Iteration 12/1000 | Loss: 0.00001241
Iteration 13/1000 | Loss: 0.00001234
Iteration 14/1000 | Loss: 0.00001234
Iteration 15/1000 | Loss: 0.00001227
Iteration 16/1000 | Loss: 0.00001222
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001210
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001201
Iteration 25/1000 | Loss: 0.00001200
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001198
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001186
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001185
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001177
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001176
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001171
Iteration 139/1000 | Loss: 0.00001171
Iteration 140/1000 | Loss: 0.00001171
Iteration 141/1000 | Loss: 0.00001171
Iteration 142/1000 | Loss: 0.00001171
Iteration 143/1000 | Loss: 0.00001171
Iteration 144/1000 | Loss: 0.00001171
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.1708030797308311e-05, 1.1708030797308311e-05, 1.1708030797308311e-05, 1.1708030797308311e-05, 1.1708030797308311e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1708030797308311e-05

Optimization complete. Final v2v error: 2.8921315670013428 mm

Highest mean error: 3.387331962585449 mm for frame 63

Lowest mean error: 2.640482187271118 mm for frame 14

Saving results

Total time: 36.2179958820343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558328
Iteration 2/25 | Loss: 0.00116374
Iteration 3/25 | Loss: 0.00081567
Iteration 4/25 | Loss: 0.00074450
Iteration 5/25 | Loss: 0.00070597
Iteration 6/25 | Loss: 0.00069971
Iteration 7/25 | Loss: 0.00070020
Iteration 8/25 | Loss: 0.00069828
Iteration 9/25 | Loss: 0.00069508
Iteration 10/25 | Loss: 0.00069301
Iteration 11/25 | Loss: 0.00069161
Iteration 12/25 | Loss: 0.00069043
Iteration 13/25 | Loss: 0.00068988
Iteration 14/25 | Loss: 0.00068959
Iteration 15/25 | Loss: 0.00068946
Iteration 16/25 | Loss: 0.00068942
Iteration 17/25 | Loss: 0.00068942
Iteration 18/25 | Loss: 0.00068942
Iteration 19/25 | Loss: 0.00068941
Iteration 20/25 | Loss: 0.00068941
Iteration 21/25 | Loss: 0.00068941
Iteration 22/25 | Loss: 0.00068941
Iteration 23/25 | Loss: 0.00068941
Iteration 24/25 | Loss: 0.00068941
Iteration 25/25 | Loss: 0.00068941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74951613
Iteration 2/25 | Loss: 0.00031998
Iteration 3/25 | Loss: 0.00031998
Iteration 4/25 | Loss: 0.00031998
Iteration 5/25 | Loss: 0.00031998
Iteration 6/25 | Loss: 0.00031998
Iteration 7/25 | Loss: 0.00031998
Iteration 8/25 | Loss: 0.00031998
Iteration 9/25 | Loss: 0.00031998
Iteration 10/25 | Loss: 0.00031998
Iteration 11/25 | Loss: 0.00031998
Iteration 12/25 | Loss: 0.00031998
Iteration 13/25 | Loss: 0.00031998
Iteration 14/25 | Loss: 0.00031998
Iteration 15/25 | Loss: 0.00031998
Iteration 16/25 | Loss: 0.00031998
Iteration 17/25 | Loss: 0.00031998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031998116173781455, 0.00031998116173781455, 0.00031998116173781455, 0.00031998116173781455, 0.00031998116173781455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031998116173781455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031998
Iteration 2/1000 | Loss: 0.00003223
Iteration 3/1000 | Loss: 0.00002305
Iteration 4/1000 | Loss: 0.00002146
Iteration 5/1000 | Loss: 0.00002028
Iteration 6/1000 | Loss: 0.00001963
Iteration 7/1000 | Loss: 0.00001895
Iteration 8/1000 | Loss: 0.00001853
Iteration 9/1000 | Loss: 0.00001825
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001803
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001796
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001795
Iteration 16/1000 | Loss: 0.00001791
Iteration 17/1000 | Loss: 0.00001789
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00001782
Iteration 21/1000 | Loss: 0.00001781
Iteration 22/1000 | Loss: 0.00001780
Iteration 23/1000 | Loss: 0.00001777
Iteration 24/1000 | Loss: 0.00001772
Iteration 25/1000 | Loss: 0.00001768
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001766
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001763
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001762
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001762
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001762
Iteration 43/1000 | Loss: 0.00001762
Iteration 44/1000 | Loss: 0.00001762
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001762
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.7618294805288315e-05, 1.7618294805288315e-05, 1.7618294805288315e-05, 1.7618294805288315e-05, 1.7618294805288315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7618294805288315e-05

Optimization complete. Final v2v error: 3.5461621284484863 mm

Highest mean error: 4.272328853607178 mm for frame 186

Lowest mean error: 3.2697341442108154 mm for frame 214

Saving results

Total time: 53.59159302711487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515935
Iteration 2/25 | Loss: 0.00102088
Iteration 3/25 | Loss: 0.00077143
Iteration 4/25 | Loss: 0.00072030
Iteration 5/25 | Loss: 0.00070848
Iteration 6/25 | Loss: 0.00070792
Iteration 7/25 | Loss: 0.00070792
Iteration 8/25 | Loss: 0.00070792
Iteration 9/25 | Loss: 0.00070792
Iteration 10/25 | Loss: 0.00070792
Iteration 11/25 | Loss: 0.00070792
Iteration 12/25 | Loss: 0.00070792
Iteration 13/25 | Loss: 0.00070792
Iteration 14/25 | Loss: 0.00070792
Iteration 15/25 | Loss: 0.00070792
Iteration 16/25 | Loss: 0.00070792
Iteration 17/25 | Loss: 0.00070792
Iteration 18/25 | Loss: 0.00070792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007079213974066079, 0.0007079213974066079, 0.0007079213974066079, 0.0007079213974066079, 0.0007079213974066079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007079213974066079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82057440
Iteration 2/25 | Loss: 0.00031182
Iteration 3/25 | Loss: 0.00031182
Iteration 4/25 | Loss: 0.00031182
Iteration 5/25 | Loss: 0.00031182
Iteration 6/25 | Loss: 0.00031182
Iteration 7/25 | Loss: 0.00031182
Iteration 8/25 | Loss: 0.00031182
Iteration 9/25 | Loss: 0.00031182
Iteration 10/25 | Loss: 0.00031182
Iteration 11/25 | Loss: 0.00031182
Iteration 12/25 | Loss: 0.00031182
Iteration 13/25 | Loss: 0.00031182
Iteration 14/25 | Loss: 0.00031182
Iteration 15/25 | Loss: 0.00031182
Iteration 16/25 | Loss: 0.00031182
Iteration 17/25 | Loss: 0.00031182
Iteration 18/25 | Loss: 0.00031182
Iteration 19/25 | Loss: 0.00031182
Iteration 20/25 | Loss: 0.00031182
Iteration 21/25 | Loss: 0.00031182
Iteration 22/25 | Loss: 0.00031182
Iteration 23/25 | Loss: 0.00031182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003118157619610429, 0.0003118157619610429, 0.0003118157619610429, 0.0003118157619610429, 0.0003118157619610429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003118157619610429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031182
Iteration 2/1000 | Loss: 0.00004045
Iteration 3/1000 | Loss: 0.00002586
Iteration 4/1000 | Loss: 0.00002420
Iteration 5/1000 | Loss: 0.00002314
Iteration 6/1000 | Loss: 0.00002220
Iteration 7/1000 | Loss: 0.00002144
Iteration 8/1000 | Loss: 0.00002103
Iteration 9/1000 | Loss: 0.00002078
Iteration 10/1000 | Loss: 0.00002049
Iteration 11/1000 | Loss: 0.00002024
Iteration 12/1000 | Loss: 0.00002007
Iteration 13/1000 | Loss: 0.00001987
Iteration 14/1000 | Loss: 0.00001983
Iteration 15/1000 | Loss: 0.00001979
Iteration 16/1000 | Loss: 0.00001978
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00001977
Iteration 19/1000 | Loss: 0.00001967
Iteration 20/1000 | Loss: 0.00001966
Iteration 21/1000 | Loss: 0.00001966
Iteration 22/1000 | Loss: 0.00001965
Iteration 23/1000 | Loss: 0.00001965
Iteration 24/1000 | Loss: 0.00001964
Iteration 25/1000 | Loss: 0.00001964
Iteration 26/1000 | Loss: 0.00001963
Iteration 27/1000 | Loss: 0.00001963
Iteration 28/1000 | Loss: 0.00001962
Iteration 29/1000 | Loss: 0.00001961
Iteration 30/1000 | Loss: 0.00001958
Iteration 31/1000 | Loss: 0.00001958
Iteration 32/1000 | Loss: 0.00001957
Iteration 33/1000 | Loss: 0.00001957
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001953
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001952
Iteration 40/1000 | Loss: 0.00001952
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001951
Iteration 45/1000 | Loss: 0.00001951
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00001949
Iteration 48/1000 | Loss: 0.00001949
Iteration 49/1000 | Loss: 0.00001949
Iteration 50/1000 | Loss: 0.00001949
Iteration 51/1000 | Loss: 0.00001949
Iteration 52/1000 | Loss: 0.00001949
Iteration 53/1000 | Loss: 0.00001949
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001949
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00001946
Iteration 62/1000 | Loss: 0.00001946
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001945
Iteration 65/1000 | Loss: 0.00001945
Iteration 66/1000 | Loss: 0.00001945
Iteration 67/1000 | Loss: 0.00001943
Iteration 68/1000 | Loss: 0.00001943
Iteration 69/1000 | Loss: 0.00001943
Iteration 70/1000 | Loss: 0.00001943
Iteration 71/1000 | Loss: 0.00001943
Iteration 72/1000 | Loss: 0.00001943
Iteration 73/1000 | Loss: 0.00001943
Iteration 74/1000 | Loss: 0.00001943
Iteration 75/1000 | Loss: 0.00001942
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001942
Iteration 78/1000 | Loss: 0.00001942
Iteration 79/1000 | Loss: 0.00001942
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001940
Iteration 82/1000 | Loss: 0.00001939
Iteration 83/1000 | Loss: 0.00001939
Iteration 84/1000 | Loss: 0.00001939
Iteration 85/1000 | Loss: 0.00001939
Iteration 86/1000 | Loss: 0.00001939
Iteration 87/1000 | Loss: 0.00001938
Iteration 88/1000 | Loss: 0.00001938
Iteration 89/1000 | Loss: 0.00001938
Iteration 90/1000 | Loss: 0.00001938
Iteration 91/1000 | Loss: 0.00001938
Iteration 92/1000 | Loss: 0.00001938
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001937
Iteration 96/1000 | Loss: 0.00001937
Iteration 97/1000 | Loss: 0.00001937
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001936
Iteration 105/1000 | Loss: 0.00001936
Iteration 106/1000 | Loss: 0.00001936
Iteration 107/1000 | Loss: 0.00001936
Iteration 108/1000 | Loss: 0.00001936
Iteration 109/1000 | Loss: 0.00001936
Iteration 110/1000 | Loss: 0.00001936
Iteration 111/1000 | Loss: 0.00001936
Iteration 112/1000 | Loss: 0.00001936
Iteration 113/1000 | Loss: 0.00001936
Iteration 114/1000 | Loss: 0.00001936
Iteration 115/1000 | Loss: 0.00001936
Iteration 116/1000 | Loss: 0.00001936
Iteration 117/1000 | Loss: 0.00001935
Iteration 118/1000 | Loss: 0.00001935
Iteration 119/1000 | Loss: 0.00001935
Iteration 120/1000 | Loss: 0.00001934
Iteration 121/1000 | Loss: 0.00001934
Iteration 122/1000 | Loss: 0.00001934
Iteration 123/1000 | Loss: 0.00001934
Iteration 124/1000 | Loss: 0.00001934
Iteration 125/1000 | Loss: 0.00001934
Iteration 126/1000 | Loss: 0.00001933
Iteration 127/1000 | Loss: 0.00001933
Iteration 128/1000 | Loss: 0.00001933
Iteration 129/1000 | Loss: 0.00001933
Iteration 130/1000 | Loss: 0.00001933
Iteration 131/1000 | Loss: 0.00001933
Iteration 132/1000 | Loss: 0.00001933
Iteration 133/1000 | Loss: 0.00001933
Iteration 134/1000 | Loss: 0.00001933
Iteration 135/1000 | Loss: 0.00001933
Iteration 136/1000 | Loss: 0.00001933
Iteration 137/1000 | Loss: 0.00001933
Iteration 138/1000 | Loss: 0.00001933
Iteration 139/1000 | Loss: 0.00001932
Iteration 140/1000 | Loss: 0.00001932
Iteration 141/1000 | Loss: 0.00001932
Iteration 142/1000 | Loss: 0.00001932
Iteration 143/1000 | Loss: 0.00001932
Iteration 144/1000 | Loss: 0.00001932
Iteration 145/1000 | Loss: 0.00001932
Iteration 146/1000 | Loss: 0.00001932
Iteration 147/1000 | Loss: 0.00001932
Iteration 148/1000 | Loss: 0.00001932
Iteration 149/1000 | Loss: 0.00001932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.9324908862472512e-05, 1.9324908862472512e-05, 1.9324908862472512e-05, 1.9324908862472512e-05, 1.9324908862472512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9324908862472512e-05

Optimization complete. Final v2v error: 3.707211971282959 mm

Highest mean error: 4.6334357261657715 mm for frame 265

Lowest mean error: 3.604625701904297 mm for frame 24

Saving results

Total time: 47.079864501953125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502490
Iteration 2/25 | Loss: 0.00125309
Iteration 3/25 | Loss: 0.00081609
Iteration 4/25 | Loss: 0.00071930
Iteration 5/25 | Loss: 0.00069837
Iteration 6/25 | Loss: 0.00069342
Iteration 7/25 | Loss: 0.00069217
Iteration 8/25 | Loss: 0.00069188
Iteration 9/25 | Loss: 0.00069188
Iteration 10/25 | Loss: 0.00069188
Iteration 11/25 | Loss: 0.00069188
Iteration 12/25 | Loss: 0.00069188
Iteration 13/25 | Loss: 0.00069188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000691882916726172, 0.000691882916726172, 0.000691882916726172, 0.000691882916726172, 0.000691882916726172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000691882916726172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54810119
Iteration 2/25 | Loss: 0.00035005
Iteration 3/25 | Loss: 0.00035005
Iteration 4/25 | Loss: 0.00035005
Iteration 5/25 | Loss: 0.00035005
Iteration 6/25 | Loss: 0.00035005
Iteration 7/25 | Loss: 0.00035005
Iteration 8/25 | Loss: 0.00035005
Iteration 9/25 | Loss: 0.00035005
Iteration 10/25 | Loss: 0.00035005
Iteration 11/25 | Loss: 0.00035005
Iteration 12/25 | Loss: 0.00035005
Iteration 13/25 | Loss: 0.00035005
Iteration 14/25 | Loss: 0.00035005
Iteration 15/25 | Loss: 0.00035005
Iteration 16/25 | Loss: 0.00035005
Iteration 17/25 | Loss: 0.00035005
Iteration 18/25 | Loss: 0.00035005
Iteration 19/25 | Loss: 0.00035005
Iteration 20/25 | Loss: 0.00035005
Iteration 21/25 | Loss: 0.00035005
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00035004792152903974, 0.00035004792152903974, 0.00035004792152903974, 0.00035004792152903974, 0.00035004792152903974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035004792152903974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035005
Iteration 2/1000 | Loss: 0.00002568
Iteration 3/1000 | Loss: 0.00001878
Iteration 4/1000 | Loss: 0.00001650
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001486
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001382
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001381
Iteration 13/1000 | Loss: 0.00001381
Iteration 14/1000 | Loss: 0.00001378
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001365
Iteration 17/1000 | Loss: 0.00001361
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001359
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001357
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001353
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001349
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001347
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001345
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001333
Iteration 84/1000 | Loss: 0.00001333
Iteration 85/1000 | Loss: 0.00001333
Iteration 86/1000 | Loss: 0.00001333
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001333
Iteration 90/1000 | Loss: 0.00001333
Iteration 91/1000 | Loss: 0.00001333
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001332
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001332
Iteration 99/1000 | Loss: 0.00001332
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001332
Iteration 108/1000 | Loss: 0.00001332
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001331
Iteration 111/1000 | Loss: 0.00001331
Iteration 112/1000 | Loss: 0.00001331
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001331
Iteration 116/1000 | Loss: 0.00001331
Iteration 117/1000 | Loss: 0.00001331
Iteration 118/1000 | Loss: 0.00001331
Iteration 119/1000 | Loss: 0.00001331
Iteration 120/1000 | Loss: 0.00001331
Iteration 121/1000 | Loss: 0.00001331
Iteration 122/1000 | Loss: 0.00001331
Iteration 123/1000 | Loss: 0.00001330
Iteration 124/1000 | Loss: 0.00001330
Iteration 125/1000 | Loss: 0.00001330
Iteration 126/1000 | Loss: 0.00001330
Iteration 127/1000 | Loss: 0.00001330
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001328
Iteration 135/1000 | Loss: 0.00001328
Iteration 136/1000 | Loss: 0.00001328
Iteration 137/1000 | Loss: 0.00001328
Iteration 138/1000 | Loss: 0.00001328
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001326
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001324
Iteration 164/1000 | Loss: 0.00001324
Iteration 165/1000 | Loss: 0.00001324
Iteration 166/1000 | Loss: 0.00001324
Iteration 167/1000 | Loss: 0.00001324
Iteration 168/1000 | Loss: 0.00001324
Iteration 169/1000 | Loss: 0.00001324
Iteration 170/1000 | Loss: 0.00001324
Iteration 171/1000 | Loss: 0.00001324
Iteration 172/1000 | Loss: 0.00001324
Iteration 173/1000 | Loss: 0.00001324
Iteration 174/1000 | Loss: 0.00001324
Iteration 175/1000 | Loss: 0.00001323
Iteration 176/1000 | Loss: 0.00001323
Iteration 177/1000 | Loss: 0.00001323
Iteration 178/1000 | Loss: 0.00001323
Iteration 179/1000 | Loss: 0.00001323
Iteration 180/1000 | Loss: 0.00001323
Iteration 181/1000 | Loss: 0.00001323
Iteration 182/1000 | Loss: 0.00001323
Iteration 183/1000 | Loss: 0.00001323
Iteration 184/1000 | Loss: 0.00001323
Iteration 185/1000 | Loss: 0.00001323
Iteration 186/1000 | Loss: 0.00001323
Iteration 187/1000 | Loss: 0.00001323
Iteration 188/1000 | Loss: 0.00001322
Iteration 189/1000 | Loss: 0.00001322
Iteration 190/1000 | Loss: 0.00001322
Iteration 191/1000 | Loss: 0.00001322
Iteration 192/1000 | Loss: 0.00001322
Iteration 193/1000 | Loss: 0.00001322
Iteration 194/1000 | Loss: 0.00001322
Iteration 195/1000 | Loss: 0.00001322
Iteration 196/1000 | Loss: 0.00001322
Iteration 197/1000 | Loss: 0.00001322
Iteration 198/1000 | Loss: 0.00001322
Iteration 199/1000 | Loss: 0.00001322
Iteration 200/1000 | Loss: 0.00001322
Iteration 201/1000 | Loss: 0.00001322
Iteration 202/1000 | Loss: 0.00001321
Iteration 203/1000 | Loss: 0.00001321
Iteration 204/1000 | Loss: 0.00001321
Iteration 205/1000 | Loss: 0.00001321
Iteration 206/1000 | Loss: 0.00001321
Iteration 207/1000 | Loss: 0.00001321
Iteration 208/1000 | Loss: 0.00001321
Iteration 209/1000 | Loss: 0.00001321
Iteration 210/1000 | Loss: 0.00001321
Iteration 211/1000 | Loss: 0.00001321
Iteration 212/1000 | Loss: 0.00001321
Iteration 213/1000 | Loss: 0.00001321
Iteration 214/1000 | Loss: 0.00001321
Iteration 215/1000 | Loss: 0.00001321
Iteration 216/1000 | Loss: 0.00001321
Iteration 217/1000 | Loss: 0.00001321
Iteration 218/1000 | Loss: 0.00001320
Iteration 219/1000 | Loss: 0.00001320
Iteration 220/1000 | Loss: 0.00001320
Iteration 221/1000 | Loss: 0.00001320
Iteration 222/1000 | Loss: 0.00001320
Iteration 223/1000 | Loss: 0.00001320
Iteration 224/1000 | Loss: 0.00001320
Iteration 225/1000 | Loss: 0.00001320
Iteration 226/1000 | Loss: 0.00001320
Iteration 227/1000 | Loss: 0.00001320
Iteration 228/1000 | Loss: 0.00001320
Iteration 229/1000 | Loss: 0.00001320
Iteration 230/1000 | Loss: 0.00001320
Iteration 231/1000 | Loss: 0.00001320
Iteration 232/1000 | Loss: 0.00001320
Iteration 233/1000 | Loss: 0.00001320
Iteration 234/1000 | Loss: 0.00001320
Iteration 235/1000 | Loss: 0.00001320
Iteration 236/1000 | Loss: 0.00001320
Iteration 237/1000 | Loss: 0.00001320
Iteration 238/1000 | Loss: 0.00001320
Iteration 239/1000 | Loss: 0.00001320
Iteration 240/1000 | Loss: 0.00001320
Iteration 241/1000 | Loss: 0.00001319
Iteration 242/1000 | Loss: 0.00001319
Iteration 243/1000 | Loss: 0.00001319
Iteration 244/1000 | Loss: 0.00001319
Iteration 245/1000 | Loss: 0.00001319
Iteration 246/1000 | Loss: 0.00001319
Iteration 247/1000 | Loss: 0.00001319
Iteration 248/1000 | Loss: 0.00001319
Iteration 249/1000 | Loss: 0.00001319
Iteration 250/1000 | Loss: 0.00001319
Iteration 251/1000 | Loss: 0.00001319
Iteration 252/1000 | Loss: 0.00001319
Iteration 253/1000 | Loss: 0.00001319
Iteration 254/1000 | Loss: 0.00001319
Iteration 255/1000 | Loss: 0.00001319
Iteration 256/1000 | Loss: 0.00001319
Iteration 257/1000 | Loss: 0.00001319
Iteration 258/1000 | Loss: 0.00001319
Iteration 259/1000 | Loss: 0.00001319
Iteration 260/1000 | Loss: 0.00001319
Iteration 261/1000 | Loss: 0.00001319
Iteration 262/1000 | Loss: 0.00001319
Iteration 263/1000 | Loss: 0.00001319
Iteration 264/1000 | Loss: 0.00001319
Iteration 265/1000 | Loss: 0.00001319
Iteration 266/1000 | Loss: 0.00001319
Iteration 267/1000 | Loss: 0.00001319
Iteration 268/1000 | Loss: 0.00001319
Iteration 269/1000 | Loss: 0.00001319
Iteration 270/1000 | Loss: 0.00001319
Iteration 271/1000 | Loss: 0.00001319
Iteration 272/1000 | Loss: 0.00001319
Iteration 273/1000 | Loss: 0.00001319
Iteration 274/1000 | Loss: 0.00001319
Iteration 275/1000 | Loss: 0.00001319
Iteration 276/1000 | Loss: 0.00001319
Iteration 277/1000 | Loss: 0.00001319
Iteration 278/1000 | Loss: 0.00001319
Iteration 279/1000 | Loss: 0.00001319
Iteration 280/1000 | Loss: 0.00001319
Iteration 281/1000 | Loss: 0.00001319
Iteration 282/1000 | Loss: 0.00001319
Iteration 283/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [1.3191796824685298e-05, 1.3191796824685298e-05, 1.3191796824685298e-05, 1.3191796824685298e-05, 1.3191796824685298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3191796824685298e-05

Optimization complete. Final v2v error: 3.033416748046875 mm

Highest mean error: 4.0055718421936035 mm for frame 75

Lowest mean error: 2.6581075191497803 mm for frame 120

Saving results

Total time: 45.1125693321228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048406
Iteration 2/25 | Loss: 0.01048406
Iteration 3/25 | Loss: 0.01048406
Iteration 4/25 | Loss: 0.01048406
Iteration 5/25 | Loss: 0.01048406
Iteration 6/25 | Loss: 0.01048406
Iteration 7/25 | Loss: 0.01048406
Iteration 8/25 | Loss: 0.01048406
Iteration 9/25 | Loss: 0.01048405
Iteration 10/25 | Loss: 0.01048405
Iteration 11/25 | Loss: 0.01048405
Iteration 12/25 | Loss: 0.01048405
Iteration 13/25 | Loss: 0.01048405
Iteration 14/25 | Loss: 0.01048405
Iteration 15/25 | Loss: 0.01048405
Iteration 16/25 | Loss: 0.01048405
Iteration 17/25 | Loss: 0.01048405
Iteration 18/25 | Loss: 0.01048405
Iteration 19/25 | Loss: 0.01048405
Iteration 20/25 | Loss: 0.01048404
Iteration 21/25 | Loss: 0.01048404
Iteration 22/25 | Loss: 0.01048404
Iteration 23/25 | Loss: 0.01048404
Iteration 24/25 | Loss: 0.01048404
Iteration 25/25 | Loss: 0.01048404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79139614
Iteration 2/25 | Loss: 0.12500320
Iteration 3/25 | Loss: 0.11214477
Iteration 4/25 | Loss: 0.10672823
Iteration 5/25 | Loss: 0.10672822
Iteration 6/25 | Loss: 0.10672822
Iteration 7/25 | Loss: 0.10672820
Iteration 8/25 | Loss: 0.10672820
Iteration 9/25 | Loss: 0.10672820
Iteration 10/25 | Loss: 0.10672820
Iteration 11/25 | Loss: 0.10672820
Iteration 12/25 | Loss: 0.10672820
Iteration 13/25 | Loss: 0.10672820
Iteration 14/25 | Loss: 0.10672820
Iteration 15/25 | Loss: 0.10672820
Iteration 16/25 | Loss: 0.10672820
Iteration 17/25 | Loss: 0.10672820
Iteration 18/25 | Loss: 0.10672820
Iteration 19/25 | Loss: 0.10672820
Iteration 20/25 | Loss: 0.10672820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.106728196144104, 0.106728196144104, 0.106728196144104, 0.106728196144104, 0.106728196144104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.106728196144104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10672820
Iteration 2/1000 | Loss: 0.00537749
Iteration 3/1000 | Loss: 0.00616945
Iteration 4/1000 | Loss: 0.00146719
Iteration 5/1000 | Loss: 0.00084166
Iteration 6/1000 | Loss: 0.00083415
Iteration 7/1000 | Loss: 0.00028052
Iteration 8/1000 | Loss: 0.00497735
Iteration 9/1000 | Loss: 0.00046817
Iteration 10/1000 | Loss: 0.00012265
Iteration 11/1000 | Loss: 0.00008549
Iteration 12/1000 | Loss: 0.00068569
Iteration 13/1000 | Loss: 0.00027546
Iteration 14/1000 | Loss: 0.00106116
Iteration 15/1000 | Loss: 0.00024757
Iteration 16/1000 | Loss: 0.00020079
Iteration 17/1000 | Loss: 0.00008227
Iteration 18/1000 | Loss: 0.00005708
Iteration 19/1000 | Loss: 0.00006754
Iteration 20/1000 | Loss: 0.00016359
Iteration 21/1000 | Loss: 0.00012856
Iteration 22/1000 | Loss: 0.00003137
Iteration 23/1000 | Loss: 0.00012181
Iteration 24/1000 | Loss: 0.00005595
Iteration 25/1000 | Loss: 0.00038987
Iteration 26/1000 | Loss: 0.00002874
Iteration 27/1000 | Loss: 0.00004809
Iteration 28/1000 | Loss: 0.00003549
Iteration 29/1000 | Loss: 0.00002418
Iteration 30/1000 | Loss: 0.00003773
Iteration 31/1000 | Loss: 0.00003755
Iteration 32/1000 | Loss: 0.00006556
Iteration 33/1000 | Loss: 0.00004775
Iteration 34/1000 | Loss: 0.00002732
Iteration 35/1000 | Loss: 0.00002845
Iteration 36/1000 | Loss: 0.00003041
Iteration 37/1000 | Loss: 0.00001928
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00007057
Iteration 40/1000 | Loss: 0.00002827
Iteration 41/1000 | Loss: 0.00002292
Iteration 42/1000 | Loss: 0.00006188
Iteration 43/1000 | Loss: 0.00002307
Iteration 44/1000 | Loss: 0.00003685
Iteration 45/1000 | Loss: 0.00001844
Iteration 46/1000 | Loss: 0.00002765
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00012047
Iteration 49/1000 | Loss: 0.00008657
Iteration 50/1000 | Loss: 0.00003984
Iteration 51/1000 | Loss: 0.00002287
Iteration 52/1000 | Loss: 0.00002215
Iteration 53/1000 | Loss: 0.00003082
Iteration 54/1000 | Loss: 0.00002746
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00002244
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00003234
Iteration 60/1000 | Loss: 0.00002808
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00002002
Iteration 63/1000 | Loss: 0.00003345
Iteration 64/1000 | Loss: 0.00008216
Iteration 65/1000 | Loss: 0.00024227
Iteration 66/1000 | Loss: 0.00032879
Iteration 67/1000 | Loss: 0.00001953
Iteration 68/1000 | Loss: 0.00001703
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00004836
Iteration 72/1000 | Loss: 0.00016351
Iteration 73/1000 | Loss: 0.00001806
Iteration 74/1000 | Loss: 0.00005243
Iteration 75/1000 | Loss: 0.00002188
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001704
Iteration 80/1000 | Loss: 0.00001704
Iteration 81/1000 | Loss: 0.00002211
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001691
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001690
Iteration 93/1000 | Loss: 0.00001690
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001691
Iteration 102/1000 | Loss: 0.00001691
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001766
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.689372948021628e-05, 1.689372948021628e-05, 1.689372948021628e-05, 1.689372948021628e-05, 1.689372948021628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.689372948021628e-05

Optimization complete. Final v2v error: 3.4626193046569824 mm

Highest mean error: 3.6523234844207764 mm for frame 136

Lowest mean error: 3.360008478164673 mm for frame 235

Saving results

Total time: 131.7113914489746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677735
Iteration 2/25 | Loss: 0.00089982
Iteration 3/25 | Loss: 0.00070621
Iteration 4/25 | Loss: 0.00067758
Iteration 5/25 | Loss: 0.00066693
Iteration 6/25 | Loss: 0.00066483
Iteration 7/25 | Loss: 0.00066418
Iteration 8/25 | Loss: 0.00066410
Iteration 9/25 | Loss: 0.00066410
Iteration 10/25 | Loss: 0.00066410
Iteration 11/25 | Loss: 0.00066410
Iteration 12/25 | Loss: 0.00066410
Iteration 13/25 | Loss: 0.00066410
Iteration 14/25 | Loss: 0.00066410
Iteration 15/25 | Loss: 0.00066410
Iteration 16/25 | Loss: 0.00066410
Iteration 17/25 | Loss: 0.00066410
Iteration 18/25 | Loss: 0.00066410
Iteration 19/25 | Loss: 0.00066410
Iteration 20/25 | Loss: 0.00066410
Iteration 21/25 | Loss: 0.00066410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006641004583798349, 0.0006641004583798349, 0.0006641004583798349, 0.0006641004583798349, 0.0006641004583798349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006641004583798349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58363712
Iteration 2/25 | Loss: 0.00031855
Iteration 3/25 | Loss: 0.00031855
Iteration 4/25 | Loss: 0.00031855
Iteration 5/25 | Loss: 0.00031855
Iteration 6/25 | Loss: 0.00031855
Iteration 7/25 | Loss: 0.00031855
Iteration 8/25 | Loss: 0.00031855
Iteration 9/25 | Loss: 0.00031855
Iteration 10/25 | Loss: 0.00031854
Iteration 11/25 | Loss: 0.00031854
Iteration 12/25 | Loss: 0.00031854
Iteration 13/25 | Loss: 0.00031854
Iteration 14/25 | Loss: 0.00031854
Iteration 15/25 | Loss: 0.00031854
Iteration 16/25 | Loss: 0.00031854
Iteration 17/25 | Loss: 0.00031854
Iteration 18/25 | Loss: 0.00031854
Iteration 19/25 | Loss: 0.00031854
Iteration 20/25 | Loss: 0.00031854
Iteration 21/25 | Loss: 0.00031854
Iteration 22/25 | Loss: 0.00031854
Iteration 23/25 | Loss: 0.00031854
Iteration 24/25 | Loss: 0.00031854
Iteration 25/25 | Loss: 0.00031854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031854
Iteration 2/1000 | Loss: 0.00002442
Iteration 3/1000 | Loss: 0.00001768
Iteration 4/1000 | Loss: 0.00001584
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001405
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001385
Iteration 11/1000 | Loss: 0.00001382
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001376
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001368
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001355
Iteration 24/1000 | Loss: 0.00001354
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001345
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001342
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001342
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001337
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001335
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001334
Iteration 53/1000 | Loss: 0.00001334
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001332
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001332
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001331
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001327
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001323
Iteration 90/1000 | Loss: 0.00001323
Iteration 91/1000 | Loss: 0.00001323
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001322
Iteration 96/1000 | Loss: 0.00001322
Iteration 97/1000 | Loss: 0.00001322
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001322
Iteration 108/1000 | Loss: 0.00001322
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001321
Iteration 115/1000 | Loss: 0.00001321
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001320
Iteration 127/1000 | Loss: 0.00001320
Iteration 128/1000 | Loss: 0.00001320
Iteration 129/1000 | Loss: 0.00001320
Iteration 130/1000 | Loss: 0.00001320
Iteration 131/1000 | Loss: 0.00001320
Iteration 132/1000 | Loss: 0.00001320
Iteration 133/1000 | Loss: 0.00001320
Iteration 134/1000 | Loss: 0.00001320
Iteration 135/1000 | Loss: 0.00001320
Iteration 136/1000 | Loss: 0.00001320
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001320
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001320
Iteration 142/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.3196959116612561e-05, 1.3196959116612561e-05, 1.3196959116612561e-05, 1.3196959116612561e-05, 1.3196959116612561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3196959116612561e-05

Optimization complete. Final v2v error: 3.076892375946045 mm

Highest mean error: 3.9780447483062744 mm for frame 73

Lowest mean error: 2.6267075538635254 mm for frame 117

Saving results

Total time: 38.21162009239197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107980
Iteration 2/25 | Loss: 0.00204717
Iteration 3/25 | Loss: 0.00115123
Iteration 4/25 | Loss: 0.00096800
Iteration 5/25 | Loss: 0.00091336
Iteration 6/25 | Loss: 0.00085446
Iteration 7/25 | Loss: 0.00081631
Iteration 8/25 | Loss: 0.00079860
Iteration 9/25 | Loss: 0.00078324
Iteration 10/25 | Loss: 0.00077998
Iteration 11/25 | Loss: 0.00078563
Iteration 12/25 | Loss: 0.00077603
Iteration 13/25 | Loss: 0.00076835
Iteration 14/25 | Loss: 0.00077511
Iteration 15/25 | Loss: 0.00077272
Iteration 16/25 | Loss: 0.00076252
Iteration 17/25 | Loss: 0.00075754
Iteration 18/25 | Loss: 0.00074643
Iteration 19/25 | Loss: 0.00074342
Iteration 20/25 | Loss: 0.00073906
Iteration 21/25 | Loss: 0.00073972
Iteration 22/25 | Loss: 0.00073841
Iteration 23/25 | Loss: 0.00073058
Iteration 24/25 | Loss: 0.00072670
Iteration 25/25 | Loss: 0.00072615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45760405
Iteration 2/25 | Loss: 0.00036062
Iteration 3/25 | Loss: 0.00036061
Iteration 4/25 | Loss: 0.00036061
Iteration 5/25 | Loss: 0.00036061
Iteration 6/25 | Loss: 0.00036061
Iteration 7/25 | Loss: 0.00036061
Iteration 8/25 | Loss: 0.00036061
Iteration 9/25 | Loss: 0.00036061
Iteration 10/25 | Loss: 0.00036061
Iteration 11/25 | Loss: 0.00036061
Iteration 12/25 | Loss: 0.00036061
Iteration 13/25 | Loss: 0.00036061
Iteration 14/25 | Loss: 0.00036061
Iteration 15/25 | Loss: 0.00036061
Iteration 16/25 | Loss: 0.00036061
Iteration 17/25 | Loss: 0.00036061
Iteration 18/25 | Loss: 0.00036061
Iteration 19/25 | Loss: 0.00036061
Iteration 20/25 | Loss: 0.00036061
Iteration 21/25 | Loss: 0.00036061
Iteration 22/25 | Loss: 0.00036061
Iteration 23/25 | Loss: 0.00036061
Iteration 24/25 | Loss: 0.00036061
Iteration 25/25 | Loss: 0.00036061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036061
Iteration 2/1000 | Loss: 0.00025758
Iteration 3/1000 | Loss: 0.00020867
Iteration 4/1000 | Loss: 0.00020341
Iteration 5/1000 | Loss: 0.00003643
Iteration 6/1000 | Loss: 0.00005339
Iteration 7/1000 | Loss: 0.00002966
Iteration 8/1000 | Loss: 0.00003115
Iteration 9/1000 | Loss: 0.00021339
Iteration 10/1000 | Loss: 0.00026129
Iteration 11/1000 | Loss: 0.00004164
Iteration 12/1000 | Loss: 0.00005156
Iteration 13/1000 | Loss: 0.00004291
Iteration 14/1000 | Loss: 0.00004395
Iteration 15/1000 | Loss: 0.00002885
Iteration 16/1000 | Loss: 0.00023099
Iteration 17/1000 | Loss: 0.00022108
Iteration 18/1000 | Loss: 0.00018132
Iteration 19/1000 | Loss: 0.00018819
Iteration 20/1000 | Loss: 0.00014128
Iteration 21/1000 | Loss: 0.00025058
Iteration 22/1000 | Loss: 0.00014323
Iteration 23/1000 | Loss: 0.00020540
Iteration 24/1000 | Loss: 0.00004435
Iteration 25/1000 | Loss: 0.00004069
Iteration 26/1000 | Loss: 0.00004838
Iteration 27/1000 | Loss: 0.00003369
Iteration 28/1000 | Loss: 0.00028663
Iteration 29/1000 | Loss: 0.00015839
Iteration 30/1000 | Loss: 0.00018389
Iteration 31/1000 | Loss: 0.00018639
Iteration 32/1000 | Loss: 0.00023244
Iteration 33/1000 | Loss: 0.00018810
Iteration 34/1000 | Loss: 0.00003546
Iteration 35/1000 | Loss: 0.00002924
Iteration 36/1000 | Loss: 0.00002554
Iteration 37/1000 | Loss: 0.00002390
Iteration 38/1000 | Loss: 0.00002250
Iteration 39/1000 | Loss: 0.00002163
Iteration 40/1000 | Loss: 0.00002365
Iteration 41/1000 | Loss: 0.00002094
Iteration 42/1000 | Loss: 0.00002047
Iteration 43/1000 | Loss: 0.00002010
Iteration 44/1000 | Loss: 0.00002005
Iteration 45/1000 | Loss: 0.00002003
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001972
Iteration 48/1000 | Loss: 0.00001967
Iteration 49/1000 | Loss: 0.00001962
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001953
Iteration 53/1000 | Loss: 0.00001952
Iteration 54/1000 | Loss: 0.00001952
Iteration 55/1000 | Loss: 0.00001952
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001951
Iteration 58/1000 | Loss: 0.00001950
Iteration 59/1000 | Loss: 0.00001950
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001949
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001945
Iteration 65/1000 | Loss: 0.00001944
Iteration 66/1000 | Loss: 0.00001944
Iteration 67/1000 | Loss: 0.00001944
Iteration 68/1000 | Loss: 0.00001943
Iteration 69/1000 | Loss: 0.00001943
Iteration 70/1000 | Loss: 0.00001942
Iteration 71/1000 | Loss: 0.00001942
Iteration 72/1000 | Loss: 0.00001941
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001940
Iteration 75/1000 | Loss: 0.00001940
Iteration 76/1000 | Loss: 0.00001940
Iteration 77/1000 | Loss: 0.00001940
Iteration 78/1000 | Loss: 0.00001939
Iteration 79/1000 | Loss: 0.00001939
Iteration 80/1000 | Loss: 0.00001939
Iteration 81/1000 | Loss: 0.00001939
Iteration 82/1000 | Loss: 0.00001938
Iteration 83/1000 | Loss: 0.00001938
Iteration 84/1000 | Loss: 0.00001938
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001935
Iteration 96/1000 | Loss: 0.00001935
Iteration 97/1000 | Loss: 0.00001935
Iteration 98/1000 | Loss: 0.00001935
Iteration 99/1000 | Loss: 0.00001934
Iteration 100/1000 | Loss: 0.00001934
Iteration 101/1000 | Loss: 0.00001934
Iteration 102/1000 | Loss: 0.00001934
Iteration 103/1000 | Loss: 0.00001934
Iteration 104/1000 | Loss: 0.00001933
Iteration 105/1000 | Loss: 0.00001933
Iteration 106/1000 | Loss: 0.00001933
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001931
Iteration 114/1000 | Loss: 0.00001931
Iteration 115/1000 | Loss: 0.00001931
Iteration 116/1000 | Loss: 0.00001930
Iteration 117/1000 | Loss: 0.00001930
Iteration 118/1000 | Loss: 0.00001930
Iteration 119/1000 | Loss: 0.00001929
Iteration 120/1000 | Loss: 0.00001929
Iteration 121/1000 | Loss: 0.00001929
Iteration 122/1000 | Loss: 0.00001929
Iteration 123/1000 | Loss: 0.00001929
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001928
Iteration 126/1000 | Loss: 0.00001928
Iteration 127/1000 | Loss: 0.00001928
Iteration 128/1000 | Loss: 0.00001928
Iteration 129/1000 | Loss: 0.00001927
Iteration 130/1000 | Loss: 0.00001927
Iteration 131/1000 | Loss: 0.00001927
Iteration 132/1000 | Loss: 0.00001927
Iteration 133/1000 | Loss: 0.00001927
Iteration 134/1000 | Loss: 0.00001927
Iteration 135/1000 | Loss: 0.00001927
Iteration 136/1000 | Loss: 0.00001927
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001927
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Iteration 143/1000 | Loss: 0.00001927
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001927
Iteration 146/1000 | Loss: 0.00001927
Iteration 147/1000 | Loss: 0.00001926
Iteration 148/1000 | Loss: 0.00001926
Iteration 149/1000 | Loss: 0.00001926
Iteration 150/1000 | Loss: 0.00001926
Iteration 151/1000 | Loss: 0.00001926
Iteration 152/1000 | Loss: 0.00001926
Iteration 153/1000 | Loss: 0.00001926
Iteration 154/1000 | Loss: 0.00001926
Iteration 155/1000 | Loss: 0.00001926
Iteration 156/1000 | Loss: 0.00001926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.9264849470346235e-05, 1.9264849470346235e-05, 1.9264849470346235e-05, 1.9264849470346235e-05, 1.9264849470346235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9264849470346235e-05

Optimization complete. Final v2v error: 3.6640236377716064 mm

Highest mean error: 4.923306941986084 mm for frame 79

Lowest mean error: 3.3788275718688965 mm for frame 10

Saving results

Total time: 126.55918312072754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979630
Iteration 2/25 | Loss: 0.00295568
Iteration 3/25 | Loss: 0.00195201
Iteration 4/25 | Loss: 0.00165613
Iteration 5/25 | Loss: 0.00141833
Iteration 6/25 | Loss: 0.00122451
Iteration 7/25 | Loss: 0.00108936
Iteration 8/25 | Loss: 0.00106933
Iteration 9/25 | Loss: 0.00100697
Iteration 10/25 | Loss: 0.00097993
Iteration 11/25 | Loss: 0.00097300
Iteration 12/25 | Loss: 0.00093722
Iteration 13/25 | Loss: 0.00090925
Iteration 14/25 | Loss: 0.00090019
Iteration 15/25 | Loss: 0.00088442
Iteration 16/25 | Loss: 0.00088106
Iteration 17/25 | Loss: 0.00086865
Iteration 18/25 | Loss: 0.00086481
Iteration 19/25 | Loss: 0.00086306
Iteration 20/25 | Loss: 0.00085962
Iteration 21/25 | Loss: 0.00085832
Iteration 22/25 | Loss: 0.00086099
Iteration 23/25 | Loss: 0.00085621
Iteration 24/25 | Loss: 0.00085495
Iteration 25/25 | Loss: 0.00085440

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52138579
Iteration 2/25 | Loss: 0.00398380
Iteration 3/25 | Loss: 0.00161687
Iteration 4/25 | Loss: 0.00161685
Iteration 5/25 | Loss: 0.00161685
Iteration 6/25 | Loss: 0.00161685
Iteration 7/25 | Loss: 0.00161685
Iteration 8/25 | Loss: 0.00161685
Iteration 9/25 | Loss: 0.00161685
Iteration 10/25 | Loss: 0.00161685
Iteration 11/25 | Loss: 0.00161685
Iteration 12/25 | Loss: 0.00161685
Iteration 13/25 | Loss: 0.00161685
Iteration 14/25 | Loss: 0.00161685
Iteration 15/25 | Loss: 0.00161685
Iteration 16/25 | Loss: 0.00161685
Iteration 17/25 | Loss: 0.00161685
Iteration 18/25 | Loss: 0.00161685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001616848399862647, 0.001616848399862647, 0.001616848399862647, 0.001616848399862647, 0.001616848399862647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001616848399862647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161685
Iteration 2/1000 | Loss: 0.00170648
Iteration 3/1000 | Loss: 0.00237491
Iteration 4/1000 | Loss: 0.00059738
Iteration 5/1000 | Loss: 0.00027492
Iteration 6/1000 | Loss: 0.00018343
Iteration 7/1000 | Loss: 0.00012402
Iteration 8/1000 | Loss: 0.00027961
Iteration 9/1000 | Loss: 0.00061166
Iteration 10/1000 | Loss: 0.00023091
Iteration 11/1000 | Loss: 0.00021888
Iteration 12/1000 | Loss: 0.00075115
Iteration 13/1000 | Loss: 0.00041717
Iteration 14/1000 | Loss: 0.00048349
Iteration 15/1000 | Loss: 0.00052390
Iteration 16/1000 | Loss: 0.00018623
Iteration 17/1000 | Loss: 0.00045556
Iteration 18/1000 | Loss: 0.00069083
Iteration 19/1000 | Loss: 0.00010585
Iteration 20/1000 | Loss: 0.00019079
Iteration 21/1000 | Loss: 0.00022999
Iteration 22/1000 | Loss: 0.00006172
Iteration 23/1000 | Loss: 0.00009035
Iteration 24/1000 | Loss: 0.00007184
Iteration 25/1000 | Loss: 0.00034034
Iteration 26/1000 | Loss: 0.00094721
Iteration 27/1000 | Loss: 0.00067910
Iteration 28/1000 | Loss: 0.00080928
Iteration 29/1000 | Loss: 0.00048307
Iteration 30/1000 | Loss: 0.00039743
Iteration 31/1000 | Loss: 0.00040865
Iteration 32/1000 | Loss: 0.00012660
Iteration 33/1000 | Loss: 0.00029082
Iteration 34/1000 | Loss: 0.00018750
Iteration 35/1000 | Loss: 0.00027092
Iteration 36/1000 | Loss: 0.00019538
Iteration 37/1000 | Loss: 0.00035694
Iteration 38/1000 | Loss: 0.00008453
Iteration 39/1000 | Loss: 0.00013458
Iteration 40/1000 | Loss: 0.00006385
Iteration 41/1000 | Loss: 0.00006676
Iteration 42/1000 | Loss: 0.00011020
Iteration 43/1000 | Loss: 0.00046049
Iteration 44/1000 | Loss: 0.00077598
Iteration 45/1000 | Loss: 0.00360663
Iteration 46/1000 | Loss: 0.00160893
Iteration 47/1000 | Loss: 0.00054589
Iteration 48/1000 | Loss: 0.00098606
Iteration 49/1000 | Loss: 0.00035743
Iteration 50/1000 | Loss: 0.00043340
Iteration 51/1000 | Loss: 0.00042201
Iteration 52/1000 | Loss: 0.00016021
Iteration 53/1000 | Loss: 0.00023519
Iteration 54/1000 | Loss: 0.00018376
Iteration 55/1000 | Loss: 0.00053351
Iteration 56/1000 | Loss: 0.00005474
Iteration 57/1000 | Loss: 0.00016423
Iteration 58/1000 | Loss: 0.00094702
Iteration 59/1000 | Loss: 0.00024568
Iteration 60/1000 | Loss: 0.00005585
Iteration 61/1000 | Loss: 0.00004860
Iteration 62/1000 | Loss: 0.00096367
Iteration 63/1000 | Loss: 0.00164249
Iteration 64/1000 | Loss: 0.00004610
Iteration 65/1000 | Loss: 0.00004102
Iteration 66/1000 | Loss: 0.00136666
Iteration 67/1000 | Loss: 0.00006784
Iteration 68/1000 | Loss: 0.00010025
Iteration 69/1000 | Loss: 0.00003565
Iteration 70/1000 | Loss: 0.00033627
Iteration 71/1000 | Loss: 0.00003363
Iteration 72/1000 | Loss: 0.00023131
Iteration 73/1000 | Loss: 0.00006503
Iteration 74/1000 | Loss: 0.00069277
Iteration 75/1000 | Loss: 0.00024053
Iteration 76/1000 | Loss: 0.00049893
Iteration 77/1000 | Loss: 0.00019760
Iteration 78/1000 | Loss: 0.00016435
Iteration 79/1000 | Loss: 0.00026406
Iteration 80/1000 | Loss: 0.00025169
Iteration 81/1000 | Loss: 0.00027382
Iteration 82/1000 | Loss: 0.00090180
Iteration 83/1000 | Loss: 0.00084726
Iteration 84/1000 | Loss: 0.00063470
Iteration 85/1000 | Loss: 0.00013739
Iteration 86/1000 | Loss: 0.00004258
Iteration 87/1000 | Loss: 0.00004289
Iteration 88/1000 | Loss: 0.00002901
Iteration 89/1000 | Loss: 0.00003658
Iteration 90/1000 | Loss: 0.00002720
Iteration 91/1000 | Loss: 0.00017511
Iteration 92/1000 | Loss: 0.00002562
Iteration 93/1000 | Loss: 0.00015155
Iteration 94/1000 | Loss: 0.00002469
Iteration 95/1000 | Loss: 0.00025241
Iteration 96/1000 | Loss: 0.00002471
Iteration 97/1000 | Loss: 0.00002405
Iteration 98/1000 | Loss: 0.00002373
Iteration 99/1000 | Loss: 0.00002349
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002326
Iteration 102/1000 | Loss: 0.00002316
Iteration 103/1000 | Loss: 0.00002314
Iteration 104/1000 | Loss: 0.00002305
Iteration 105/1000 | Loss: 0.00002304
Iteration 106/1000 | Loss: 0.00002298
Iteration 107/1000 | Loss: 0.00002298
Iteration 108/1000 | Loss: 0.00002296
Iteration 109/1000 | Loss: 0.00002295
Iteration 110/1000 | Loss: 0.00002295
Iteration 111/1000 | Loss: 0.00002294
Iteration 112/1000 | Loss: 0.00002294
Iteration 113/1000 | Loss: 0.00002294
Iteration 114/1000 | Loss: 0.00002294
Iteration 115/1000 | Loss: 0.00002293
Iteration 116/1000 | Loss: 0.00002293
Iteration 117/1000 | Loss: 0.00002293
Iteration 118/1000 | Loss: 0.00002292
Iteration 119/1000 | Loss: 0.00002292
Iteration 120/1000 | Loss: 0.00002292
Iteration 121/1000 | Loss: 0.00002292
Iteration 122/1000 | Loss: 0.00002291
Iteration 123/1000 | Loss: 0.00002291
Iteration 124/1000 | Loss: 0.00002291
Iteration 125/1000 | Loss: 0.00002288
Iteration 126/1000 | Loss: 0.00002288
Iteration 127/1000 | Loss: 0.00002286
Iteration 128/1000 | Loss: 0.00002286
Iteration 129/1000 | Loss: 0.00030373
Iteration 130/1000 | Loss: 0.00027485
Iteration 131/1000 | Loss: 0.00002339
Iteration 132/1000 | Loss: 0.00010430
Iteration 133/1000 | Loss: 0.00071370
Iteration 134/1000 | Loss: 0.00005391
Iteration 135/1000 | Loss: 0.00004694
Iteration 136/1000 | Loss: 0.00002181
Iteration 137/1000 | Loss: 0.00008317
Iteration 138/1000 | Loss: 0.00002032
Iteration 139/1000 | Loss: 0.00001953
Iteration 140/1000 | Loss: 0.00001917
Iteration 141/1000 | Loss: 0.00001891
Iteration 142/1000 | Loss: 0.00001880
Iteration 143/1000 | Loss: 0.00001873
Iteration 144/1000 | Loss: 0.00001871
Iteration 145/1000 | Loss: 0.00001869
Iteration 146/1000 | Loss: 0.00001868
Iteration 147/1000 | Loss: 0.00001862
Iteration 148/1000 | Loss: 0.00001862
Iteration 149/1000 | Loss: 0.00001861
Iteration 150/1000 | Loss: 0.00001861
Iteration 151/1000 | Loss: 0.00001858
Iteration 152/1000 | Loss: 0.00001858
Iteration 153/1000 | Loss: 0.00001854
Iteration 154/1000 | Loss: 0.00001854
Iteration 155/1000 | Loss: 0.00001852
Iteration 156/1000 | Loss: 0.00001852
Iteration 157/1000 | Loss: 0.00001852
Iteration 158/1000 | Loss: 0.00001852
Iteration 159/1000 | Loss: 0.00001852
Iteration 160/1000 | Loss: 0.00001852
Iteration 161/1000 | Loss: 0.00001852
Iteration 162/1000 | Loss: 0.00001852
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001850
Iteration 170/1000 | Loss: 0.00001850
Iteration 171/1000 | Loss: 0.00001849
Iteration 172/1000 | Loss: 0.00001849
Iteration 173/1000 | Loss: 0.00001849
Iteration 174/1000 | Loss: 0.00001848
Iteration 175/1000 | Loss: 0.00001848
Iteration 176/1000 | Loss: 0.00001848
Iteration 177/1000 | Loss: 0.00001848
Iteration 178/1000 | Loss: 0.00001848
Iteration 179/1000 | Loss: 0.00001848
Iteration 180/1000 | Loss: 0.00001847
Iteration 181/1000 | Loss: 0.00001847
Iteration 182/1000 | Loss: 0.00001847
Iteration 183/1000 | Loss: 0.00001847
Iteration 184/1000 | Loss: 0.00001846
Iteration 185/1000 | Loss: 0.00001846
Iteration 186/1000 | Loss: 0.00001846
Iteration 187/1000 | Loss: 0.00001846
Iteration 188/1000 | Loss: 0.00001846
Iteration 189/1000 | Loss: 0.00001846
Iteration 190/1000 | Loss: 0.00001846
Iteration 191/1000 | Loss: 0.00001846
Iteration 192/1000 | Loss: 0.00001846
Iteration 193/1000 | Loss: 0.00001846
Iteration 194/1000 | Loss: 0.00001846
Iteration 195/1000 | Loss: 0.00001846
Iteration 196/1000 | Loss: 0.00001845
Iteration 197/1000 | Loss: 0.00001845
Iteration 198/1000 | Loss: 0.00001845
Iteration 199/1000 | Loss: 0.00001845
Iteration 200/1000 | Loss: 0.00001845
Iteration 201/1000 | Loss: 0.00001845
Iteration 202/1000 | Loss: 0.00001845
Iteration 203/1000 | Loss: 0.00001845
Iteration 204/1000 | Loss: 0.00001845
Iteration 205/1000 | Loss: 0.00001845
Iteration 206/1000 | Loss: 0.00001845
Iteration 207/1000 | Loss: 0.00001845
Iteration 208/1000 | Loss: 0.00001845
Iteration 209/1000 | Loss: 0.00001845
Iteration 210/1000 | Loss: 0.00001845
Iteration 211/1000 | Loss: 0.00001845
Iteration 212/1000 | Loss: 0.00001845
Iteration 213/1000 | Loss: 0.00001845
Iteration 214/1000 | Loss: 0.00001845
Iteration 215/1000 | Loss: 0.00001845
Iteration 216/1000 | Loss: 0.00001845
Iteration 217/1000 | Loss: 0.00001845
Iteration 218/1000 | Loss: 0.00001845
Iteration 219/1000 | Loss: 0.00001845
Iteration 220/1000 | Loss: 0.00001845
Iteration 221/1000 | Loss: 0.00001845
Iteration 222/1000 | Loss: 0.00001845
Iteration 223/1000 | Loss: 0.00001845
Iteration 224/1000 | Loss: 0.00001845
Iteration 225/1000 | Loss: 0.00001845
Iteration 226/1000 | Loss: 0.00001845
Iteration 227/1000 | Loss: 0.00001845
Iteration 228/1000 | Loss: 0.00001845
Iteration 229/1000 | Loss: 0.00001845
Iteration 230/1000 | Loss: 0.00001845
Iteration 231/1000 | Loss: 0.00001845
Iteration 232/1000 | Loss: 0.00001845
Iteration 233/1000 | Loss: 0.00001845
Iteration 234/1000 | Loss: 0.00001845
Iteration 235/1000 | Loss: 0.00001845
Iteration 236/1000 | Loss: 0.00001845
Iteration 237/1000 | Loss: 0.00001845
Iteration 238/1000 | Loss: 0.00001845
Iteration 239/1000 | Loss: 0.00001845
Iteration 240/1000 | Loss: 0.00001845
Iteration 241/1000 | Loss: 0.00001845
Iteration 242/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.844679718487896e-05, 1.844679718487896e-05, 1.844679718487896e-05, 1.844679718487896e-05, 1.844679718487896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.844679718487896e-05

Optimization complete. Final v2v error: 3.456972122192383 mm

Highest mean error: 12.365361213684082 mm for frame 115

Lowest mean error: 3.1120986938476562 mm for frame 201

Saving results

Total time: 251.18517541885376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038295
Iteration 2/25 | Loss: 0.00174160
Iteration 3/25 | Loss: 0.00118699
Iteration 4/25 | Loss: 0.00126659
Iteration 5/25 | Loss: 0.00106501
Iteration 6/25 | Loss: 0.00101625
Iteration 7/25 | Loss: 0.00094553
Iteration 8/25 | Loss: 0.00092333
Iteration 9/25 | Loss: 0.00097358
Iteration 10/25 | Loss: 0.00090262
Iteration 11/25 | Loss: 0.00089819
Iteration 12/25 | Loss: 0.00091185
Iteration 13/25 | Loss: 0.00087158
Iteration 14/25 | Loss: 0.00086373
Iteration 15/25 | Loss: 0.00086179
Iteration 16/25 | Loss: 0.00086093
Iteration 17/25 | Loss: 0.00086500
Iteration 18/25 | Loss: 0.00086744
Iteration 19/25 | Loss: 0.00086712
Iteration 20/25 | Loss: 0.00085042
Iteration 21/25 | Loss: 0.00084268
Iteration 22/25 | Loss: 0.00083980
Iteration 23/25 | Loss: 0.00083928
Iteration 24/25 | Loss: 0.00083912
Iteration 25/25 | Loss: 0.00083910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95377767
Iteration 2/25 | Loss: 0.00120926
Iteration 3/25 | Loss: 0.00120923
Iteration 4/25 | Loss: 0.00120923
Iteration 5/25 | Loss: 0.00120923
Iteration 6/25 | Loss: 0.00120923
Iteration 7/25 | Loss: 0.00120923
Iteration 8/25 | Loss: 0.00120923
Iteration 9/25 | Loss: 0.00120923
Iteration 10/25 | Loss: 0.00120923
Iteration 11/25 | Loss: 0.00120923
Iteration 12/25 | Loss: 0.00120923
Iteration 13/25 | Loss: 0.00120923
Iteration 14/25 | Loss: 0.00120923
Iteration 15/25 | Loss: 0.00120923
Iteration 16/25 | Loss: 0.00120923
Iteration 17/25 | Loss: 0.00120923
Iteration 18/25 | Loss: 0.00120923
Iteration 19/25 | Loss: 0.00120923
Iteration 20/25 | Loss: 0.00120923
Iteration 21/25 | Loss: 0.00120923
Iteration 22/25 | Loss: 0.00120923
Iteration 23/25 | Loss: 0.00120923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012092251563444734, 0.0012092251563444734, 0.0012092251563444734, 0.0012092251563444734, 0.0012092251563444734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012092251563444734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120923
Iteration 2/1000 | Loss: 0.00563702
Iteration 3/1000 | Loss: 0.00047045
Iteration 4/1000 | Loss: 0.00063739
Iteration 5/1000 | Loss: 0.00080312
Iteration 6/1000 | Loss: 0.00030478
Iteration 7/1000 | Loss: 0.00092820
Iteration 8/1000 | Loss: 0.00126297
Iteration 9/1000 | Loss: 0.00061665
Iteration 10/1000 | Loss: 0.00026741
Iteration 11/1000 | Loss: 0.00007971
Iteration 12/1000 | Loss: 0.00048727
Iteration 13/1000 | Loss: 0.00007070
Iteration 14/1000 | Loss: 0.00004678
Iteration 15/1000 | Loss: 0.00003945
Iteration 16/1000 | Loss: 0.00039967
Iteration 17/1000 | Loss: 0.00031830
Iteration 18/1000 | Loss: 0.00036908
Iteration 19/1000 | Loss: 0.00003742
Iteration 20/1000 | Loss: 0.00003090
Iteration 21/1000 | Loss: 0.00002803
Iteration 22/1000 | Loss: 0.00002516
Iteration 23/1000 | Loss: 0.00002403
Iteration 24/1000 | Loss: 0.00002320
Iteration 25/1000 | Loss: 0.00002250
Iteration 26/1000 | Loss: 0.00002184
Iteration 27/1000 | Loss: 0.00002132
Iteration 28/1000 | Loss: 0.00002092
Iteration 29/1000 | Loss: 0.00002072
Iteration 30/1000 | Loss: 0.00002044
Iteration 31/1000 | Loss: 0.00002021
Iteration 32/1000 | Loss: 0.00002014
Iteration 33/1000 | Loss: 0.00002013
Iteration 34/1000 | Loss: 0.00002008
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002004
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00001992
Iteration 40/1000 | Loss: 0.00001989
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001986
Iteration 44/1000 | Loss: 0.00001986
Iteration 45/1000 | Loss: 0.00001985
Iteration 46/1000 | Loss: 0.00001985
Iteration 47/1000 | Loss: 0.00001985
Iteration 48/1000 | Loss: 0.00001984
Iteration 49/1000 | Loss: 0.00001984
Iteration 50/1000 | Loss: 0.00001984
Iteration 51/1000 | Loss: 0.00001983
Iteration 52/1000 | Loss: 0.00001983
Iteration 53/1000 | Loss: 0.00001983
Iteration 54/1000 | Loss: 0.00001982
Iteration 55/1000 | Loss: 0.00001982
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001981
Iteration 58/1000 | Loss: 0.00001981
Iteration 59/1000 | Loss: 0.00001981
Iteration 60/1000 | Loss: 0.00001981
Iteration 61/1000 | Loss: 0.00001981
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001980
Iteration 65/1000 | Loss: 0.00001980
Iteration 66/1000 | Loss: 0.00001980
Iteration 67/1000 | Loss: 0.00001980
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001979
Iteration 70/1000 | Loss: 0.00001979
Iteration 71/1000 | Loss: 0.00001979
Iteration 72/1000 | Loss: 0.00001979
Iteration 73/1000 | Loss: 0.00001979
Iteration 74/1000 | Loss: 0.00001978
Iteration 75/1000 | Loss: 0.00001978
Iteration 76/1000 | Loss: 0.00001978
Iteration 77/1000 | Loss: 0.00001978
Iteration 78/1000 | Loss: 0.00001978
Iteration 79/1000 | Loss: 0.00001978
Iteration 80/1000 | Loss: 0.00001978
Iteration 81/1000 | Loss: 0.00001978
Iteration 82/1000 | Loss: 0.00001978
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.978026921278797e-05, 1.978026921278797e-05, 1.978026921278797e-05, 1.978026921278797e-05, 1.978026921278797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.978026921278797e-05

Optimization complete. Final v2v error: 3.320051670074463 mm

Highest mean error: 12.499306678771973 mm for frame 36

Lowest mean error: 2.5723817348480225 mm for frame 31

Saving results

Total time: 93.29896950721741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092014
Iteration 2/25 | Loss: 0.01092014
Iteration 3/25 | Loss: 0.01092013
Iteration 4/25 | Loss: 0.01092013
Iteration 5/25 | Loss: 0.00247942
Iteration 6/25 | Loss: 0.00143705
Iteration 7/25 | Loss: 0.00128269
Iteration 8/25 | Loss: 0.00125255
Iteration 9/25 | Loss: 0.00119862
Iteration 10/25 | Loss: 0.00116784
Iteration 11/25 | Loss: 0.00116542
Iteration 12/25 | Loss: 0.00113743
Iteration 13/25 | Loss: 0.00111013
Iteration 14/25 | Loss: 0.00108099
Iteration 15/25 | Loss: 0.00106278
Iteration 16/25 | Loss: 0.00104437
Iteration 17/25 | Loss: 0.00102969
Iteration 18/25 | Loss: 0.00102766
Iteration 19/25 | Loss: 0.00100959
Iteration 20/25 | Loss: 0.00100468
Iteration 21/25 | Loss: 0.00100479
Iteration 22/25 | Loss: 0.00100748
Iteration 23/25 | Loss: 0.00100337
Iteration 24/25 | Loss: 0.00100267
Iteration 25/25 | Loss: 0.00099865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40739691
Iteration 2/25 | Loss: 0.00329167
Iteration 3/25 | Loss: 0.00224271
Iteration 4/25 | Loss: 0.00224271
Iteration 5/25 | Loss: 0.00224271
Iteration 6/25 | Loss: 0.00224270
Iteration 7/25 | Loss: 0.00224270
Iteration 8/25 | Loss: 0.00224270
Iteration 9/25 | Loss: 0.00224270
Iteration 10/25 | Loss: 0.00224270
Iteration 11/25 | Loss: 0.00224270
Iteration 12/25 | Loss: 0.00224270
Iteration 13/25 | Loss: 0.00224270
Iteration 14/25 | Loss: 0.00224270
Iteration 15/25 | Loss: 0.00224270
Iteration 16/25 | Loss: 0.00224270
Iteration 17/25 | Loss: 0.00224270
Iteration 18/25 | Loss: 0.00224270
Iteration 19/25 | Loss: 0.00224270
Iteration 20/25 | Loss: 0.00224270
Iteration 21/25 | Loss: 0.00224270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022427027579396963, 0.0022427027579396963, 0.0022427027579396963, 0.0022427027579396963, 0.0022427027579396963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022427027579396963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224270
Iteration 2/1000 | Loss: 0.00229298
Iteration 3/1000 | Loss: 0.00518987
Iteration 4/1000 | Loss: 0.00285727
Iteration 5/1000 | Loss: 0.00182366
Iteration 6/1000 | Loss: 0.00072658
Iteration 7/1000 | Loss: 0.00100114
Iteration 8/1000 | Loss: 0.00131827
Iteration 9/1000 | Loss: 0.00292089
Iteration 10/1000 | Loss: 0.00036697
Iteration 11/1000 | Loss: 0.00028222
Iteration 12/1000 | Loss: 0.00115928
Iteration 13/1000 | Loss: 0.00076334
Iteration 14/1000 | Loss: 0.00084375
Iteration 15/1000 | Loss: 0.00115137
Iteration 16/1000 | Loss: 0.00075838
Iteration 17/1000 | Loss: 0.00059219
Iteration 18/1000 | Loss: 0.00079665
Iteration 19/1000 | Loss: 0.00047280
Iteration 20/1000 | Loss: 0.00148161
Iteration 21/1000 | Loss: 0.00044406
Iteration 22/1000 | Loss: 0.00039033
Iteration 23/1000 | Loss: 0.00026555
Iteration 24/1000 | Loss: 0.00021610
Iteration 25/1000 | Loss: 0.00020793
Iteration 26/1000 | Loss: 0.00013044
Iteration 27/1000 | Loss: 0.00145884
Iteration 28/1000 | Loss: 0.00437681
Iteration 29/1000 | Loss: 0.00112429
Iteration 30/1000 | Loss: 0.00145715
Iteration 31/1000 | Loss: 0.00077220
Iteration 32/1000 | Loss: 0.00160329
Iteration 33/1000 | Loss: 0.00070626
Iteration 34/1000 | Loss: 0.00205801
Iteration 35/1000 | Loss: 0.00076195
Iteration 36/1000 | Loss: 0.00095285
Iteration 37/1000 | Loss: 0.00153587
Iteration 38/1000 | Loss: 0.00079733
Iteration 39/1000 | Loss: 0.00072277
Iteration 40/1000 | Loss: 0.00112290
Iteration 41/1000 | Loss: 0.00052945
Iteration 42/1000 | Loss: 0.00087702
Iteration 43/1000 | Loss: 0.00232345
Iteration 44/1000 | Loss: 0.00069793
Iteration 45/1000 | Loss: 0.00059253
Iteration 46/1000 | Loss: 0.00091589
Iteration 47/1000 | Loss: 0.00246442
Iteration 48/1000 | Loss: 0.00173446
Iteration 49/1000 | Loss: 0.00481515
Iteration 50/1000 | Loss: 0.00215525
Iteration 51/1000 | Loss: 0.00320517
Iteration 52/1000 | Loss: 0.00111076
Iteration 53/1000 | Loss: 0.00119875
Iteration 54/1000 | Loss: 0.00184979
Iteration 55/1000 | Loss: 0.00070872
Iteration 56/1000 | Loss: 0.00030555
Iteration 57/1000 | Loss: 0.00075047
Iteration 58/1000 | Loss: 0.00194191
Iteration 59/1000 | Loss: 0.00154221
Iteration 60/1000 | Loss: 0.00040709
Iteration 61/1000 | Loss: 0.00175730
Iteration 62/1000 | Loss: 0.00079739
Iteration 63/1000 | Loss: 0.00093221
Iteration 64/1000 | Loss: 0.00024073
Iteration 65/1000 | Loss: 0.00103281
Iteration 66/1000 | Loss: 0.00101681
Iteration 67/1000 | Loss: 0.00104763
Iteration 68/1000 | Loss: 0.00226736
Iteration 69/1000 | Loss: 0.00181587
Iteration 70/1000 | Loss: 0.00249358
Iteration 71/1000 | Loss: 0.00143720
Iteration 72/1000 | Loss: 0.00252433
Iteration 73/1000 | Loss: 0.00468836
Iteration 74/1000 | Loss: 0.00462290
Iteration 75/1000 | Loss: 0.00044755
Iteration 76/1000 | Loss: 0.00126636
Iteration 77/1000 | Loss: 0.00197843
Iteration 78/1000 | Loss: 0.00209645
Iteration 79/1000 | Loss: 0.00204269
Iteration 80/1000 | Loss: 0.00137690
Iteration 81/1000 | Loss: 0.00066635
Iteration 82/1000 | Loss: 0.00056979
Iteration 83/1000 | Loss: 0.00284882
Iteration 84/1000 | Loss: 0.00168354
Iteration 85/1000 | Loss: 0.00065248
Iteration 86/1000 | Loss: 0.00044664
Iteration 87/1000 | Loss: 0.00028915
Iteration 88/1000 | Loss: 0.00153156
Iteration 89/1000 | Loss: 0.00044354
Iteration 90/1000 | Loss: 0.00037016
Iteration 91/1000 | Loss: 0.00041515
Iteration 92/1000 | Loss: 0.00062634
Iteration 93/1000 | Loss: 0.00101224
Iteration 94/1000 | Loss: 0.00068801
Iteration 95/1000 | Loss: 0.00223757
Iteration 96/1000 | Loss: 0.00175470
Iteration 97/1000 | Loss: 0.00192924
Iteration 98/1000 | Loss: 0.00018869
Iteration 99/1000 | Loss: 0.00055559
Iteration 100/1000 | Loss: 0.00064745
Iteration 101/1000 | Loss: 0.00017242
Iteration 102/1000 | Loss: 0.00006182
Iteration 103/1000 | Loss: 0.00038278
Iteration 104/1000 | Loss: 0.00049286
Iteration 105/1000 | Loss: 0.00087836
Iteration 106/1000 | Loss: 0.00065004
Iteration 107/1000 | Loss: 0.00051454
Iteration 108/1000 | Loss: 0.00045468
Iteration 109/1000 | Loss: 0.00049783
Iteration 110/1000 | Loss: 0.00014570
Iteration 111/1000 | Loss: 0.00025224
Iteration 112/1000 | Loss: 0.00012152
Iteration 113/1000 | Loss: 0.00026383
Iteration 114/1000 | Loss: 0.00015593
Iteration 115/1000 | Loss: 0.00028609
Iteration 116/1000 | Loss: 0.00022206
Iteration 117/1000 | Loss: 0.00037973
Iteration 118/1000 | Loss: 0.00025801
Iteration 119/1000 | Loss: 0.00052432
Iteration 120/1000 | Loss: 0.00019947
Iteration 121/1000 | Loss: 0.00031604
Iteration 122/1000 | Loss: 0.00169868
Iteration 123/1000 | Loss: 0.00057539
Iteration 124/1000 | Loss: 0.00044418
Iteration 125/1000 | Loss: 0.00061577
Iteration 126/1000 | Loss: 0.00028956
Iteration 127/1000 | Loss: 0.00011881
Iteration 128/1000 | Loss: 0.00091442
Iteration 129/1000 | Loss: 0.00063788
Iteration 130/1000 | Loss: 0.00071768
Iteration 131/1000 | Loss: 0.00043042
Iteration 132/1000 | Loss: 0.00170555
Iteration 133/1000 | Loss: 0.00053585
Iteration 134/1000 | Loss: 0.00078619
Iteration 135/1000 | Loss: 0.00012635
Iteration 136/1000 | Loss: 0.00046844
Iteration 137/1000 | Loss: 0.00028498
Iteration 138/1000 | Loss: 0.00039908
Iteration 139/1000 | Loss: 0.00011743
Iteration 140/1000 | Loss: 0.00047785
Iteration 141/1000 | Loss: 0.00037217
Iteration 142/1000 | Loss: 0.00046749
Iteration 143/1000 | Loss: 0.00031228
Iteration 144/1000 | Loss: 0.00043576
Iteration 145/1000 | Loss: 0.00037240
Iteration 146/1000 | Loss: 0.00036127
Iteration 147/1000 | Loss: 0.00042522
Iteration 148/1000 | Loss: 0.00040744
Iteration 149/1000 | Loss: 0.00057542
Iteration 150/1000 | Loss: 0.00034701
Iteration 151/1000 | Loss: 0.00049089
Iteration 152/1000 | Loss: 0.00042132
Iteration 153/1000 | Loss: 0.00038623
Iteration 154/1000 | Loss: 0.00034656
Iteration 155/1000 | Loss: 0.00022189
Iteration 156/1000 | Loss: 0.00063131
Iteration 157/1000 | Loss: 0.00020691
Iteration 158/1000 | Loss: 0.00031368
Iteration 159/1000 | Loss: 0.00047301
Iteration 160/1000 | Loss: 0.00020584
Iteration 161/1000 | Loss: 0.00026906
Iteration 162/1000 | Loss: 0.00035843
Iteration 163/1000 | Loss: 0.00047155
Iteration 164/1000 | Loss: 0.00154143
Iteration 165/1000 | Loss: 0.00053001
Iteration 166/1000 | Loss: 0.00005538
Iteration 167/1000 | Loss: 0.00042953
Iteration 168/1000 | Loss: 0.00105695
Iteration 169/1000 | Loss: 0.00056311
Iteration 170/1000 | Loss: 0.00074560
Iteration 171/1000 | Loss: 0.00069487
Iteration 172/1000 | Loss: 0.00004869
Iteration 173/1000 | Loss: 0.00079812
Iteration 174/1000 | Loss: 0.00188639
Iteration 175/1000 | Loss: 0.00023545
Iteration 176/1000 | Loss: 0.00048582
Iteration 177/1000 | Loss: 0.00005900
Iteration 178/1000 | Loss: 0.00015503
Iteration 179/1000 | Loss: 0.00009023
Iteration 180/1000 | Loss: 0.00008532
Iteration 181/1000 | Loss: 0.00028861
Iteration 182/1000 | Loss: 0.00021109
Iteration 183/1000 | Loss: 0.00021996
Iteration 184/1000 | Loss: 0.00018742
Iteration 185/1000 | Loss: 0.00010804
Iteration 186/1000 | Loss: 0.00086273
Iteration 187/1000 | Loss: 0.00039626
Iteration 188/1000 | Loss: 0.00010096
Iteration 189/1000 | Loss: 0.00045705
Iteration 190/1000 | Loss: 0.00031686
Iteration 191/1000 | Loss: 0.00080600
Iteration 192/1000 | Loss: 0.00044379
Iteration 193/1000 | Loss: 0.00077951
Iteration 194/1000 | Loss: 0.00009872
Iteration 195/1000 | Loss: 0.00011760
Iteration 196/1000 | Loss: 0.00006728
Iteration 197/1000 | Loss: 0.00061773
Iteration 198/1000 | Loss: 0.00075551
Iteration 199/1000 | Loss: 0.00079527
Iteration 200/1000 | Loss: 0.00089980
Iteration 201/1000 | Loss: 0.00147891
Iteration 202/1000 | Loss: 0.00029958
Iteration 203/1000 | Loss: 0.00079893
Iteration 204/1000 | Loss: 0.00035045
Iteration 205/1000 | Loss: 0.00014095
Iteration 206/1000 | Loss: 0.00009995
Iteration 207/1000 | Loss: 0.00069025
Iteration 208/1000 | Loss: 0.00018152
Iteration 209/1000 | Loss: 0.00025782
Iteration 210/1000 | Loss: 0.00004952
Iteration 211/1000 | Loss: 0.00012339
Iteration 212/1000 | Loss: 0.00066227
Iteration 213/1000 | Loss: 0.00039067
Iteration 214/1000 | Loss: 0.00006999
Iteration 215/1000 | Loss: 0.00067168
Iteration 216/1000 | Loss: 0.00049689
Iteration 217/1000 | Loss: 0.00065381
Iteration 218/1000 | Loss: 0.00017044
Iteration 219/1000 | Loss: 0.00010187
Iteration 220/1000 | Loss: 0.00037382
Iteration 221/1000 | Loss: 0.00086020
Iteration 222/1000 | Loss: 0.00014370
Iteration 223/1000 | Loss: 0.00040745
Iteration 224/1000 | Loss: 0.00009296
Iteration 225/1000 | Loss: 0.00038064
Iteration 226/1000 | Loss: 0.00015638
Iteration 227/1000 | Loss: 0.00023517
Iteration 228/1000 | Loss: 0.00034438
Iteration 229/1000 | Loss: 0.00023928
Iteration 230/1000 | Loss: 0.00013924
Iteration 231/1000 | Loss: 0.00137217
Iteration 232/1000 | Loss: 0.00021952
Iteration 233/1000 | Loss: 0.00047028
Iteration 234/1000 | Loss: 0.00137770
Iteration 235/1000 | Loss: 0.00026416
Iteration 236/1000 | Loss: 0.00015528
Iteration 237/1000 | Loss: 0.00096133
Iteration 238/1000 | Loss: 0.00049270
Iteration 239/1000 | Loss: 0.00004169
Iteration 240/1000 | Loss: 0.00003485
Iteration 241/1000 | Loss: 0.00003046
Iteration 242/1000 | Loss: 0.00002879
Iteration 243/1000 | Loss: 0.00043389
Iteration 244/1000 | Loss: 0.00186439
Iteration 245/1000 | Loss: 0.00020206
Iteration 246/1000 | Loss: 0.00023093
Iteration 247/1000 | Loss: 0.00066504
Iteration 248/1000 | Loss: 0.00030228
Iteration 249/1000 | Loss: 0.00053056
Iteration 250/1000 | Loss: 0.00047444
Iteration 251/1000 | Loss: 0.00062111
Iteration 252/1000 | Loss: 0.00016102
Iteration 253/1000 | Loss: 0.00041400
Iteration 254/1000 | Loss: 0.00003002
Iteration 255/1000 | Loss: 0.00034612
Iteration 256/1000 | Loss: 0.00056026
Iteration 257/1000 | Loss: 0.00082258
Iteration 258/1000 | Loss: 0.00025800
Iteration 259/1000 | Loss: 0.00028698
Iteration 260/1000 | Loss: 0.00004355
Iteration 261/1000 | Loss: 0.00057121
Iteration 262/1000 | Loss: 0.00003478
Iteration 263/1000 | Loss: 0.00002783
Iteration 264/1000 | Loss: 0.00002562
Iteration 265/1000 | Loss: 0.00015940
Iteration 266/1000 | Loss: 0.00098783
Iteration 267/1000 | Loss: 0.00056228
Iteration 268/1000 | Loss: 0.00059407
Iteration 269/1000 | Loss: 0.00040362
Iteration 270/1000 | Loss: 0.00055893
Iteration 271/1000 | Loss: 0.00002729
Iteration 272/1000 | Loss: 0.00005602
Iteration 273/1000 | Loss: 0.00002410
Iteration 274/1000 | Loss: 0.00018596
Iteration 275/1000 | Loss: 0.00020677
Iteration 276/1000 | Loss: 0.00018529
Iteration 277/1000 | Loss: 0.00020837
Iteration 278/1000 | Loss: 0.00003258
Iteration 279/1000 | Loss: 0.00002402
Iteration 280/1000 | Loss: 0.00026544
Iteration 281/1000 | Loss: 0.00003443
Iteration 282/1000 | Loss: 0.00002726
Iteration 283/1000 | Loss: 0.00002272
Iteration 284/1000 | Loss: 0.00017260
Iteration 285/1000 | Loss: 0.00012611
Iteration 286/1000 | Loss: 0.00015977
Iteration 287/1000 | Loss: 0.00054478
Iteration 288/1000 | Loss: 0.00010546
Iteration 289/1000 | Loss: 0.00016813
Iteration 290/1000 | Loss: 0.00009610
Iteration 291/1000 | Loss: 0.00022517
Iteration 292/1000 | Loss: 0.00006161
Iteration 293/1000 | Loss: 0.00004668
Iteration 294/1000 | Loss: 0.00002239
Iteration 295/1000 | Loss: 0.00002160
Iteration 296/1000 | Loss: 0.00002107
Iteration 297/1000 | Loss: 0.00002071
Iteration 298/1000 | Loss: 0.00002037
Iteration 299/1000 | Loss: 0.00002001
Iteration 300/1000 | Loss: 0.00001971
Iteration 301/1000 | Loss: 0.00001952
Iteration 302/1000 | Loss: 0.00001936
Iteration 303/1000 | Loss: 0.00001931
Iteration 304/1000 | Loss: 0.00001922
Iteration 305/1000 | Loss: 0.00001921
Iteration 306/1000 | Loss: 0.00001920
Iteration 307/1000 | Loss: 0.00001920
Iteration 308/1000 | Loss: 0.00001919
Iteration 309/1000 | Loss: 0.00001918
Iteration 310/1000 | Loss: 0.00001918
Iteration 311/1000 | Loss: 0.00001917
Iteration 312/1000 | Loss: 0.00001917
Iteration 313/1000 | Loss: 0.00001916
Iteration 314/1000 | Loss: 0.00001916
Iteration 315/1000 | Loss: 0.00001915
Iteration 316/1000 | Loss: 0.00001915
Iteration 317/1000 | Loss: 0.00001914
Iteration 318/1000 | Loss: 0.00001913
Iteration 319/1000 | Loss: 0.00001913
Iteration 320/1000 | Loss: 0.00001912
Iteration 321/1000 | Loss: 0.00001911
Iteration 322/1000 | Loss: 0.00001910
Iteration 323/1000 | Loss: 0.00001910
Iteration 324/1000 | Loss: 0.00001909
Iteration 325/1000 | Loss: 0.00001908
Iteration 326/1000 | Loss: 0.00001908
Iteration 327/1000 | Loss: 0.00001907
Iteration 328/1000 | Loss: 0.00001906
Iteration 329/1000 | Loss: 0.00001906
Iteration 330/1000 | Loss: 0.00001905
Iteration 331/1000 | Loss: 0.00001905
Iteration 332/1000 | Loss: 0.00001904
Iteration 333/1000 | Loss: 0.00001904
Iteration 334/1000 | Loss: 0.00001904
Iteration 335/1000 | Loss: 0.00001903
Iteration 336/1000 | Loss: 0.00001903
Iteration 337/1000 | Loss: 0.00001903
Iteration 338/1000 | Loss: 0.00001902
Iteration 339/1000 | Loss: 0.00001902
Iteration 340/1000 | Loss: 0.00001902
Iteration 341/1000 | Loss: 0.00001902
Iteration 342/1000 | Loss: 0.00001902
Iteration 343/1000 | Loss: 0.00001901
Iteration 344/1000 | Loss: 0.00001901
Iteration 345/1000 | Loss: 0.00001901
Iteration 346/1000 | Loss: 0.00001900
Iteration 347/1000 | Loss: 0.00001900
Iteration 348/1000 | Loss: 0.00001900
Iteration 349/1000 | Loss: 0.00001900
Iteration 350/1000 | Loss: 0.00001899
Iteration 351/1000 | Loss: 0.00001899
Iteration 352/1000 | Loss: 0.00001899
Iteration 353/1000 | Loss: 0.00001898
Iteration 354/1000 | Loss: 0.00001898
Iteration 355/1000 | Loss: 0.00001898
Iteration 356/1000 | Loss: 0.00001898
Iteration 357/1000 | Loss: 0.00001898
Iteration 358/1000 | Loss: 0.00001898
Iteration 359/1000 | Loss: 0.00001897
Iteration 360/1000 | Loss: 0.00001897
Iteration 361/1000 | Loss: 0.00001897
Iteration 362/1000 | Loss: 0.00001897
Iteration 363/1000 | Loss: 0.00001897
Iteration 364/1000 | Loss: 0.00001897
Iteration 365/1000 | Loss: 0.00001897
Iteration 366/1000 | Loss: 0.00001897
Iteration 367/1000 | Loss: 0.00001897
Iteration 368/1000 | Loss: 0.00001897
Iteration 369/1000 | Loss: 0.00001897
Iteration 370/1000 | Loss: 0.00001897
Iteration 371/1000 | Loss: 0.00001897
Iteration 372/1000 | Loss: 0.00001897
Iteration 373/1000 | Loss: 0.00001897
Iteration 374/1000 | Loss: 0.00001897
Iteration 375/1000 | Loss: 0.00001897
Iteration 376/1000 | Loss: 0.00001897
Iteration 377/1000 | Loss: 0.00001897
Iteration 378/1000 | Loss: 0.00001897
Iteration 379/1000 | Loss: 0.00001897
Iteration 380/1000 | Loss: 0.00001897
Iteration 381/1000 | Loss: 0.00001897
Iteration 382/1000 | Loss: 0.00001897
Iteration 383/1000 | Loss: 0.00001897
Iteration 384/1000 | Loss: 0.00001897
Iteration 385/1000 | Loss: 0.00001897
Iteration 386/1000 | Loss: 0.00001897
Iteration 387/1000 | Loss: 0.00001897
Iteration 388/1000 | Loss: 0.00001897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 388. Stopping optimization.
Last 5 losses: [1.8967775758937933e-05, 1.8967775758937933e-05, 1.8967775758937933e-05, 1.8967775758937933e-05, 1.8967775758937933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8967775758937933e-05

Optimization complete. Final v2v error: 3.466932535171509 mm

Highest mean error: 13.402826309204102 mm for frame 207

Lowest mean error: 2.833665609359741 mm for frame 90

Saving results

Total time: 539.6860485076904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855604
Iteration 2/25 | Loss: 0.00085038
Iteration 3/25 | Loss: 0.00069210
Iteration 4/25 | Loss: 0.00066918
Iteration 5/25 | Loss: 0.00066294
Iteration 6/25 | Loss: 0.00066147
Iteration 7/25 | Loss: 0.00066130
Iteration 8/25 | Loss: 0.00066130
Iteration 9/25 | Loss: 0.00066130
Iteration 10/25 | Loss: 0.00066130
Iteration 11/25 | Loss: 0.00066130
Iteration 12/25 | Loss: 0.00066130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006613011937588453, 0.0006613011937588453, 0.0006613011937588453, 0.0006613011937588453, 0.0006613011937588453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006613011937588453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47673023
Iteration 2/25 | Loss: 0.00027151
Iteration 3/25 | Loss: 0.00027150
Iteration 4/25 | Loss: 0.00027150
Iteration 5/25 | Loss: 0.00027150
Iteration 6/25 | Loss: 0.00027150
Iteration 7/25 | Loss: 0.00027150
Iteration 8/25 | Loss: 0.00027150
Iteration 9/25 | Loss: 0.00027150
Iteration 10/25 | Loss: 0.00027150
Iteration 11/25 | Loss: 0.00027150
Iteration 12/25 | Loss: 0.00027149
Iteration 13/25 | Loss: 0.00027149
Iteration 14/25 | Loss: 0.00027149
Iteration 15/25 | Loss: 0.00027149
Iteration 16/25 | Loss: 0.00027149
Iteration 17/25 | Loss: 0.00027149
Iteration 18/25 | Loss: 0.00027149
Iteration 19/25 | Loss: 0.00027149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002714949077926576, 0.0002714949077926576, 0.0002714949077926576, 0.0002714949077926576, 0.0002714949077926576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002714949077926576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027149
Iteration 2/1000 | Loss: 0.00002487
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001320
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001299
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001288
Iteration 17/1000 | Loss: 0.00001285
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001280
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001278
Iteration 24/1000 | Loss: 0.00001278
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001277
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001276
Iteration 31/1000 | Loss: 0.00001276
Iteration 32/1000 | Loss: 0.00001276
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001275
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001271
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001270
Iteration 42/1000 | Loss: 0.00001270
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001269
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001262
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001261
Iteration 72/1000 | Loss: 0.00001261
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001261
Iteration 75/1000 | Loss: 0.00001260
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001258
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001257
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001255
Iteration 113/1000 | Loss: 0.00001255
Iteration 114/1000 | Loss: 0.00001255
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001255
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001255
Iteration 122/1000 | Loss: 0.00001255
Iteration 123/1000 | Loss: 0.00001255
Iteration 124/1000 | Loss: 0.00001255
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001255
Iteration 128/1000 | Loss: 0.00001255
Iteration 129/1000 | Loss: 0.00001255
Iteration 130/1000 | Loss: 0.00001255
Iteration 131/1000 | Loss: 0.00001255
Iteration 132/1000 | Loss: 0.00001255
Iteration 133/1000 | Loss: 0.00001255
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001255
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Iteration 138/1000 | Loss: 0.00001255
Iteration 139/1000 | Loss: 0.00001255
Iteration 140/1000 | Loss: 0.00001255
Iteration 141/1000 | Loss: 0.00001255
Iteration 142/1000 | Loss: 0.00001255
Iteration 143/1000 | Loss: 0.00001255
Iteration 144/1000 | Loss: 0.00001255
Iteration 145/1000 | Loss: 0.00001255
Iteration 146/1000 | Loss: 0.00001255
Iteration 147/1000 | Loss: 0.00001255
Iteration 148/1000 | Loss: 0.00001255
Iteration 149/1000 | Loss: 0.00001255
Iteration 150/1000 | Loss: 0.00001255
Iteration 151/1000 | Loss: 0.00001255
Iteration 152/1000 | Loss: 0.00001255
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001255
Iteration 155/1000 | Loss: 0.00001255
Iteration 156/1000 | Loss: 0.00001255
Iteration 157/1000 | Loss: 0.00001255
Iteration 158/1000 | Loss: 0.00001255
Iteration 159/1000 | Loss: 0.00001255
Iteration 160/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.2553318811114877e-05, 1.2553318811114877e-05, 1.2553318811114877e-05, 1.2553318811114877e-05, 1.2553318811114877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2553318811114877e-05

Optimization complete. Final v2v error: 2.9706218242645264 mm

Highest mean error: 3.1723835468292236 mm for frame 30

Lowest mean error: 2.8346328735351562 mm for frame 77

Saving results

Total time: 36.4428927898407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389102
Iteration 2/25 | Loss: 0.00076519
Iteration 3/25 | Loss: 0.00064735
Iteration 4/25 | Loss: 0.00063088
Iteration 5/25 | Loss: 0.00062585
Iteration 6/25 | Loss: 0.00062435
Iteration 7/25 | Loss: 0.00062395
Iteration 8/25 | Loss: 0.00062395
Iteration 9/25 | Loss: 0.00062395
Iteration 10/25 | Loss: 0.00062395
Iteration 11/25 | Loss: 0.00062395
Iteration 12/25 | Loss: 0.00062395
Iteration 13/25 | Loss: 0.00062395
Iteration 14/25 | Loss: 0.00062395
Iteration 15/25 | Loss: 0.00062395
Iteration 16/25 | Loss: 0.00062395
Iteration 17/25 | Loss: 0.00062395
Iteration 18/25 | Loss: 0.00062395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006239507347345352, 0.0006239507347345352, 0.0006239507347345352, 0.0006239507347345352, 0.0006239507347345352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006239507347345352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48007035
Iteration 2/25 | Loss: 0.00027772
Iteration 3/25 | Loss: 0.00027772
Iteration 4/25 | Loss: 0.00027772
Iteration 5/25 | Loss: 0.00027771
Iteration 6/25 | Loss: 0.00027771
Iteration 7/25 | Loss: 0.00027771
Iteration 8/25 | Loss: 0.00027771
Iteration 9/25 | Loss: 0.00027771
Iteration 10/25 | Loss: 0.00027771
Iteration 11/25 | Loss: 0.00027771
Iteration 12/25 | Loss: 0.00027771
Iteration 13/25 | Loss: 0.00027771
Iteration 14/25 | Loss: 0.00027771
Iteration 15/25 | Loss: 0.00027771
Iteration 16/25 | Loss: 0.00027771
Iteration 17/25 | Loss: 0.00027771
Iteration 18/25 | Loss: 0.00027771
Iteration 19/25 | Loss: 0.00027771
Iteration 20/25 | Loss: 0.00027771
Iteration 21/25 | Loss: 0.00027771
Iteration 22/25 | Loss: 0.00027771
Iteration 23/25 | Loss: 0.00027771
Iteration 24/25 | Loss: 0.00027771
Iteration 25/25 | Loss: 0.00027771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027771
Iteration 2/1000 | Loss: 0.00001490
Iteration 3/1000 | Loss: 0.00001052
Iteration 4/1000 | Loss: 0.00000957
Iteration 5/1000 | Loss: 0.00000902
Iteration 6/1000 | Loss: 0.00000872
Iteration 7/1000 | Loss: 0.00000860
Iteration 8/1000 | Loss: 0.00000858
Iteration 9/1000 | Loss: 0.00000852
Iteration 10/1000 | Loss: 0.00000852
Iteration 11/1000 | Loss: 0.00000852
Iteration 12/1000 | Loss: 0.00000852
Iteration 13/1000 | Loss: 0.00000851
Iteration 14/1000 | Loss: 0.00000851
Iteration 15/1000 | Loss: 0.00000851
Iteration 16/1000 | Loss: 0.00000851
Iteration 17/1000 | Loss: 0.00000851
Iteration 18/1000 | Loss: 0.00000848
Iteration 19/1000 | Loss: 0.00000847
Iteration 20/1000 | Loss: 0.00000847
Iteration 21/1000 | Loss: 0.00000847
Iteration 22/1000 | Loss: 0.00000847
Iteration 23/1000 | Loss: 0.00000847
Iteration 24/1000 | Loss: 0.00000846
Iteration 25/1000 | Loss: 0.00000846
Iteration 26/1000 | Loss: 0.00000846
Iteration 27/1000 | Loss: 0.00000845
Iteration 28/1000 | Loss: 0.00000845
Iteration 29/1000 | Loss: 0.00000845
Iteration 30/1000 | Loss: 0.00000845
Iteration 31/1000 | Loss: 0.00000844
Iteration 32/1000 | Loss: 0.00000843
Iteration 33/1000 | Loss: 0.00000843
Iteration 34/1000 | Loss: 0.00000842
Iteration 35/1000 | Loss: 0.00000842
Iteration 36/1000 | Loss: 0.00000841
Iteration 37/1000 | Loss: 0.00000841
Iteration 38/1000 | Loss: 0.00000841
Iteration 39/1000 | Loss: 0.00000841
Iteration 40/1000 | Loss: 0.00000841
Iteration 41/1000 | Loss: 0.00000840
Iteration 42/1000 | Loss: 0.00000840
Iteration 43/1000 | Loss: 0.00000840
Iteration 44/1000 | Loss: 0.00000840
Iteration 45/1000 | Loss: 0.00000839
Iteration 46/1000 | Loss: 0.00000839
Iteration 47/1000 | Loss: 0.00000839
Iteration 48/1000 | Loss: 0.00000838
Iteration 49/1000 | Loss: 0.00000838
Iteration 50/1000 | Loss: 0.00000838
Iteration 51/1000 | Loss: 0.00000837
Iteration 52/1000 | Loss: 0.00000837
Iteration 53/1000 | Loss: 0.00000836
Iteration 54/1000 | Loss: 0.00000836
Iteration 55/1000 | Loss: 0.00000835
Iteration 56/1000 | Loss: 0.00000835
Iteration 57/1000 | Loss: 0.00000834
Iteration 58/1000 | Loss: 0.00000834
Iteration 59/1000 | Loss: 0.00000834
Iteration 60/1000 | Loss: 0.00000834
Iteration 61/1000 | Loss: 0.00000833
Iteration 62/1000 | Loss: 0.00000832
Iteration 63/1000 | Loss: 0.00000830
Iteration 64/1000 | Loss: 0.00000830
Iteration 65/1000 | Loss: 0.00000830
Iteration 66/1000 | Loss: 0.00000829
Iteration 67/1000 | Loss: 0.00000829
Iteration 68/1000 | Loss: 0.00000828
Iteration 69/1000 | Loss: 0.00000828
Iteration 70/1000 | Loss: 0.00000828
Iteration 71/1000 | Loss: 0.00000828
Iteration 72/1000 | Loss: 0.00000828
Iteration 73/1000 | Loss: 0.00000828
Iteration 74/1000 | Loss: 0.00000826
Iteration 75/1000 | Loss: 0.00000825
Iteration 76/1000 | Loss: 0.00000825
Iteration 77/1000 | Loss: 0.00000825
Iteration 78/1000 | Loss: 0.00000825
Iteration 79/1000 | Loss: 0.00000825
Iteration 80/1000 | Loss: 0.00000825
Iteration 81/1000 | Loss: 0.00000825
Iteration 82/1000 | Loss: 0.00000824
Iteration 83/1000 | Loss: 0.00000824
Iteration 84/1000 | Loss: 0.00000824
Iteration 85/1000 | Loss: 0.00000823
Iteration 86/1000 | Loss: 0.00000822
Iteration 87/1000 | Loss: 0.00000822
Iteration 88/1000 | Loss: 0.00000822
Iteration 89/1000 | Loss: 0.00000822
Iteration 90/1000 | Loss: 0.00000821
Iteration 91/1000 | Loss: 0.00000821
Iteration 92/1000 | Loss: 0.00000821
Iteration 93/1000 | Loss: 0.00000821
Iteration 94/1000 | Loss: 0.00000820
Iteration 95/1000 | Loss: 0.00000820
Iteration 96/1000 | Loss: 0.00000820
Iteration 97/1000 | Loss: 0.00000819
Iteration 98/1000 | Loss: 0.00000819
Iteration 99/1000 | Loss: 0.00000818
Iteration 100/1000 | Loss: 0.00000818
Iteration 101/1000 | Loss: 0.00000818
Iteration 102/1000 | Loss: 0.00000818
Iteration 103/1000 | Loss: 0.00000818
Iteration 104/1000 | Loss: 0.00000817
Iteration 105/1000 | Loss: 0.00000817
Iteration 106/1000 | Loss: 0.00000817
Iteration 107/1000 | Loss: 0.00000817
Iteration 108/1000 | Loss: 0.00000816
Iteration 109/1000 | Loss: 0.00000816
Iteration 110/1000 | Loss: 0.00000816
Iteration 111/1000 | Loss: 0.00000816
Iteration 112/1000 | Loss: 0.00000815
Iteration 113/1000 | Loss: 0.00000815
Iteration 114/1000 | Loss: 0.00000815
Iteration 115/1000 | Loss: 0.00000815
Iteration 116/1000 | Loss: 0.00000815
Iteration 117/1000 | Loss: 0.00000814
Iteration 118/1000 | Loss: 0.00000814
Iteration 119/1000 | Loss: 0.00000814
Iteration 120/1000 | Loss: 0.00000814
Iteration 121/1000 | Loss: 0.00000814
Iteration 122/1000 | Loss: 0.00000814
Iteration 123/1000 | Loss: 0.00000813
Iteration 124/1000 | Loss: 0.00000813
Iteration 125/1000 | Loss: 0.00000813
Iteration 126/1000 | Loss: 0.00000813
Iteration 127/1000 | Loss: 0.00000812
Iteration 128/1000 | Loss: 0.00000812
Iteration 129/1000 | Loss: 0.00000812
Iteration 130/1000 | Loss: 0.00000812
Iteration 131/1000 | Loss: 0.00000812
Iteration 132/1000 | Loss: 0.00000812
Iteration 133/1000 | Loss: 0.00000811
Iteration 134/1000 | Loss: 0.00000811
Iteration 135/1000 | Loss: 0.00000811
Iteration 136/1000 | Loss: 0.00000811
Iteration 137/1000 | Loss: 0.00000811
Iteration 138/1000 | Loss: 0.00000811
Iteration 139/1000 | Loss: 0.00000811
Iteration 140/1000 | Loss: 0.00000811
Iteration 141/1000 | Loss: 0.00000811
Iteration 142/1000 | Loss: 0.00000811
Iteration 143/1000 | Loss: 0.00000811
Iteration 144/1000 | Loss: 0.00000811
Iteration 145/1000 | Loss: 0.00000811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [8.110120688797906e-06, 8.110120688797906e-06, 8.110120688797906e-06, 8.110120688797906e-06, 8.110120688797906e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.110120688797906e-06

Optimization complete. Final v2v error: 2.4521727561950684 mm

Highest mean error: 2.5453200340270996 mm for frame 74

Lowest mean error: 2.412039041519165 mm for frame 109

Saving results

Total time: 32.497501850128174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786229
Iteration 2/25 | Loss: 0.00176167
Iteration 3/25 | Loss: 0.00119079
Iteration 4/25 | Loss: 0.00109109
Iteration 5/25 | Loss: 0.00097267
Iteration 6/25 | Loss: 0.00091922
Iteration 7/25 | Loss: 0.00087238
Iteration 8/25 | Loss: 0.00083763
Iteration 9/25 | Loss: 0.00083170
Iteration 10/25 | Loss: 0.00081758
Iteration 11/25 | Loss: 0.00081173
Iteration 12/25 | Loss: 0.00081262
Iteration 13/25 | Loss: 0.00080811
Iteration 14/25 | Loss: 0.00080531
Iteration 15/25 | Loss: 0.00080432
Iteration 16/25 | Loss: 0.00080293
Iteration 17/25 | Loss: 0.00080135
Iteration 18/25 | Loss: 0.00080251
Iteration 19/25 | Loss: 0.00080503
Iteration 20/25 | Loss: 0.00079584
Iteration 21/25 | Loss: 0.00078982
Iteration 22/25 | Loss: 0.00078744
Iteration 23/25 | Loss: 0.00078704
Iteration 24/25 | Loss: 0.00079054
Iteration 25/25 | Loss: 0.00078559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37297440
Iteration 2/25 | Loss: 0.00045914
Iteration 3/25 | Loss: 0.00045914
Iteration 4/25 | Loss: 0.00045914
Iteration 5/25 | Loss: 0.00045914
Iteration 6/25 | Loss: 0.00045914
Iteration 7/25 | Loss: 0.00045914
Iteration 8/25 | Loss: 0.00045914
Iteration 9/25 | Loss: 0.00045914
Iteration 10/25 | Loss: 0.00045914
Iteration 11/25 | Loss: 0.00045914
Iteration 12/25 | Loss: 0.00045914
Iteration 13/25 | Loss: 0.00045914
Iteration 14/25 | Loss: 0.00045914
Iteration 15/25 | Loss: 0.00045914
Iteration 16/25 | Loss: 0.00045914
Iteration 17/25 | Loss: 0.00045914
Iteration 18/25 | Loss: 0.00045914
Iteration 19/25 | Loss: 0.00045914
Iteration 20/25 | Loss: 0.00045914
Iteration 21/25 | Loss: 0.00045914
Iteration 22/25 | Loss: 0.00045914
Iteration 23/25 | Loss: 0.00045914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004591394681483507, 0.0004591394681483507, 0.0004591394681483507, 0.0004591394681483507, 0.0004591394681483507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004591394681483507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045914
Iteration 2/1000 | Loss: 0.00100935
Iteration 3/1000 | Loss: 0.00009045
Iteration 4/1000 | Loss: 0.00004567
Iteration 5/1000 | Loss: 0.00003109
Iteration 6/1000 | Loss: 0.00002566
Iteration 7/1000 | Loss: 0.00002264
Iteration 8/1000 | Loss: 0.00002147
Iteration 9/1000 | Loss: 0.00002079
Iteration 10/1000 | Loss: 0.00002014
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001941
Iteration 13/1000 | Loss: 0.00001919
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001889
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001883
Iteration 20/1000 | Loss: 0.00001879
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001876
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001862
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001860
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001854
Iteration 40/1000 | Loss: 0.00001854
Iteration 41/1000 | Loss: 0.00001854
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001853
Iteration 44/1000 | Loss: 0.00001852
Iteration 45/1000 | Loss: 0.00001850
Iteration 46/1000 | Loss: 0.00001850
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001847
Iteration 51/1000 | Loss: 0.00001847
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001846
Iteration 57/1000 | Loss: 0.00001846
Iteration 58/1000 | Loss: 0.00001846
Iteration 59/1000 | Loss: 0.00001846
Iteration 60/1000 | Loss: 0.00001846
Iteration 61/1000 | Loss: 0.00001846
Iteration 62/1000 | Loss: 0.00001846
Iteration 63/1000 | Loss: 0.00001846
Iteration 64/1000 | Loss: 0.00001845
Iteration 65/1000 | Loss: 0.00001845
Iteration 66/1000 | Loss: 0.00001845
Iteration 67/1000 | Loss: 0.00001845
Iteration 68/1000 | Loss: 0.00001844
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001840
Iteration 89/1000 | Loss: 0.00001840
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001838
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001838
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001837
Iteration 126/1000 | Loss: 0.00001836
Iteration 127/1000 | Loss: 0.00001836
Iteration 128/1000 | Loss: 0.00001836
Iteration 129/1000 | Loss: 0.00001836
Iteration 130/1000 | Loss: 0.00001836
Iteration 131/1000 | Loss: 0.00001836
Iteration 132/1000 | Loss: 0.00001836
Iteration 133/1000 | Loss: 0.00001836
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001835
Iteration 136/1000 | Loss: 0.00001835
Iteration 137/1000 | Loss: 0.00001835
Iteration 138/1000 | Loss: 0.00001835
Iteration 139/1000 | Loss: 0.00001835
Iteration 140/1000 | Loss: 0.00001835
Iteration 141/1000 | Loss: 0.00001835
Iteration 142/1000 | Loss: 0.00001834
Iteration 143/1000 | Loss: 0.00001834
Iteration 144/1000 | Loss: 0.00001834
Iteration 145/1000 | Loss: 0.00001834
Iteration 146/1000 | Loss: 0.00001834
Iteration 147/1000 | Loss: 0.00001834
Iteration 148/1000 | Loss: 0.00001834
Iteration 149/1000 | Loss: 0.00001834
Iteration 150/1000 | Loss: 0.00001834
Iteration 151/1000 | Loss: 0.00001834
Iteration 152/1000 | Loss: 0.00001834
Iteration 153/1000 | Loss: 0.00001834
Iteration 154/1000 | Loss: 0.00001834
Iteration 155/1000 | Loss: 0.00001834
Iteration 156/1000 | Loss: 0.00001834
Iteration 157/1000 | Loss: 0.00001834
Iteration 158/1000 | Loss: 0.00001834
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001833
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001833
Iteration 165/1000 | Loss: 0.00001833
Iteration 166/1000 | Loss: 0.00001833
Iteration 167/1000 | Loss: 0.00001833
Iteration 168/1000 | Loss: 0.00001833
Iteration 169/1000 | Loss: 0.00001833
Iteration 170/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.8331935280002654e-05, 1.8331935280002654e-05, 1.8331935280002654e-05, 1.8331935280002654e-05, 1.8331935280002654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8331935280002654e-05

Optimization complete. Final v2v error: 3.672870635986328 mm

Highest mean error: 3.893892765045166 mm for frame 0

Lowest mean error: 3.494513988494873 mm for frame 67

Saving results

Total time: 90.53070592880249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801042
Iteration 2/25 | Loss: 0.00107108
Iteration 3/25 | Loss: 0.00076850
Iteration 4/25 | Loss: 0.00071948
Iteration 5/25 | Loss: 0.00071034
Iteration 6/25 | Loss: 0.00070765
Iteration 7/25 | Loss: 0.00070752
Iteration 8/25 | Loss: 0.00070752
Iteration 9/25 | Loss: 0.00070752
Iteration 10/25 | Loss: 0.00070752
Iteration 11/25 | Loss: 0.00070752
Iteration 12/25 | Loss: 0.00070752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000707524421159178, 0.000707524421159178, 0.000707524421159178, 0.000707524421159178, 0.000707524421159178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000707524421159178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46881032
Iteration 2/25 | Loss: 0.00035076
Iteration 3/25 | Loss: 0.00035075
Iteration 4/25 | Loss: 0.00035075
Iteration 5/25 | Loss: 0.00035075
Iteration 6/25 | Loss: 0.00035075
Iteration 7/25 | Loss: 0.00035074
Iteration 8/25 | Loss: 0.00035074
Iteration 9/25 | Loss: 0.00035074
Iteration 10/25 | Loss: 0.00035074
Iteration 11/25 | Loss: 0.00035074
Iteration 12/25 | Loss: 0.00035074
Iteration 13/25 | Loss: 0.00035074
Iteration 14/25 | Loss: 0.00035074
Iteration 15/25 | Loss: 0.00035074
Iteration 16/25 | Loss: 0.00035074
Iteration 17/25 | Loss: 0.00035074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003507436776999384, 0.0003507436776999384, 0.0003507436776999384, 0.0003507436776999384, 0.0003507436776999384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003507436776999384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035074
Iteration 2/1000 | Loss: 0.00002395
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001720
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001574
Iteration 7/1000 | Loss: 0.00001541
Iteration 8/1000 | Loss: 0.00001512
Iteration 9/1000 | Loss: 0.00001491
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001457
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001456
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001456
Iteration 26/1000 | Loss: 0.00001455
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001447
Iteration 39/1000 | Loss: 0.00001447
Iteration 40/1000 | Loss: 0.00001447
Iteration 41/1000 | Loss: 0.00001447
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001447
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001447
Iteration 49/1000 | Loss: 0.00001447
Iteration 50/1000 | Loss: 0.00001447
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001442
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001441
Iteration 76/1000 | Loss: 0.00001441
Iteration 77/1000 | Loss: 0.00001441
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001439
Iteration 88/1000 | Loss: 0.00001439
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001438
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001437
Iteration 98/1000 | Loss: 0.00001437
Iteration 99/1000 | Loss: 0.00001437
Iteration 100/1000 | Loss: 0.00001437
Iteration 101/1000 | Loss: 0.00001437
Iteration 102/1000 | Loss: 0.00001437
Iteration 103/1000 | Loss: 0.00001437
Iteration 104/1000 | Loss: 0.00001436
Iteration 105/1000 | Loss: 0.00001436
Iteration 106/1000 | Loss: 0.00001436
Iteration 107/1000 | Loss: 0.00001436
Iteration 108/1000 | Loss: 0.00001436
Iteration 109/1000 | Loss: 0.00001436
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001435
Iteration 112/1000 | Loss: 0.00001435
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001433
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001430
Iteration 130/1000 | Loss: 0.00001430
Iteration 131/1000 | Loss: 0.00001430
Iteration 132/1000 | Loss: 0.00001430
Iteration 133/1000 | Loss: 0.00001430
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001430
Iteration 136/1000 | Loss: 0.00001430
Iteration 137/1000 | Loss: 0.00001430
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.4295997971203178e-05, 1.4295997971203178e-05, 1.4295997971203178e-05, 1.4295997971203178e-05, 1.4295997971203178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4295997971203178e-05

Optimization complete. Final v2v error: 3.1356959342956543 mm

Highest mean error: 3.627995729446411 mm for frame 159

Lowest mean error: 2.764739751815796 mm for frame 6

Saving results

Total time: 38.94237279891968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809547
Iteration 2/25 | Loss: 0.00116375
Iteration 3/25 | Loss: 0.00091317
Iteration 4/25 | Loss: 0.00083726
Iteration 5/25 | Loss: 0.00081089
Iteration 6/25 | Loss: 0.00080094
Iteration 7/25 | Loss: 0.00079796
Iteration 8/25 | Loss: 0.00078978
Iteration 9/25 | Loss: 0.00078776
Iteration 10/25 | Loss: 0.00078720
Iteration 11/25 | Loss: 0.00078705
Iteration 12/25 | Loss: 0.00078695
Iteration 13/25 | Loss: 0.00078640
Iteration 14/25 | Loss: 0.00078939
Iteration 15/25 | Loss: 0.00078605
Iteration 16/25 | Loss: 0.00078317
Iteration 17/25 | Loss: 0.00078167
Iteration 18/25 | Loss: 0.00078134
Iteration 19/25 | Loss: 0.00077961
Iteration 20/25 | Loss: 0.00077913
Iteration 21/25 | Loss: 0.00077901
Iteration 22/25 | Loss: 0.00077900
Iteration 23/25 | Loss: 0.00077900
Iteration 24/25 | Loss: 0.00077900
Iteration 25/25 | Loss: 0.00077899

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46586561
Iteration 2/25 | Loss: 0.00082622
Iteration 3/25 | Loss: 0.00082622
Iteration 4/25 | Loss: 0.00082622
Iteration 5/25 | Loss: 0.00082622
Iteration 6/25 | Loss: 0.00082622
Iteration 7/25 | Loss: 0.00082622
Iteration 8/25 | Loss: 0.00082622
Iteration 9/25 | Loss: 0.00082621
Iteration 10/25 | Loss: 0.00082621
Iteration 11/25 | Loss: 0.00082621
Iteration 12/25 | Loss: 0.00082621
Iteration 13/25 | Loss: 0.00082621
Iteration 14/25 | Loss: 0.00082621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008262144983746111, 0.0008262144983746111, 0.0008262144983746111, 0.0008262144983746111, 0.0008262144983746111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008262144983746111

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082621
Iteration 2/1000 | Loss: 0.00008605
Iteration 3/1000 | Loss: 0.00018452
Iteration 4/1000 | Loss: 0.00029223
Iteration 5/1000 | Loss: 0.00021686
Iteration 6/1000 | Loss: 0.00007532
Iteration 7/1000 | Loss: 0.00007403
Iteration 8/1000 | Loss: 0.00005116
Iteration 9/1000 | Loss: 0.00042028
Iteration 10/1000 | Loss: 0.00004889
Iteration 11/1000 | Loss: 0.00004117
Iteration 12/1000 | Loss: 0.00048351
Iteration 13/1000 | Loss: 0.00013616
Iteration 14/1000 | Loss: 0.00003582
Iteration 15/1000 | Loss: 0.00003400
Iteration 16/1000 | Loss: 0.00003281
Iteration 17/1000 | Loss: 0.00003202
Iteration 18/1000 | Loss: 0.00003151
Iteration 19/1000 | Loss: 0.00003106
Iteration 20/1000 | Loss: 0.00003086
Iteration 21/1000 | Loss: 0.00003058
Iteration 22/1000 | Loss: 0.00003034
Iteration 23/1000 | Loss: 0.00003010
Iteration 24/1000 | Loss: 0.00003010
Iteration 25/1000 | Loss: 0.00003007
Iteration 26/1000 | Loss: 0.00002991
Iteration 27/1000 | Loss: 0.00002973
Iteration 28/1000 | Loss: 0.00002966
Iteration 29/1000 | Loss: 0.00002965
Iteration 30/1000 | Loss: 0.00002965
Iteration 31/1000 | Loss: 0.00002964
Iteration 32/1000 | Loss: 0.00002963
Iteration 33/1000 | Loss: 0.00002960
Iteration 34/1000 | Loss: 0.00002957
Iteration 35/1000 | Loss: 0.00002957
Iteration 36/1000 | Loss: 0.00002953
Iteration 37/1000 | Loss: 0.00002951
Iteration 38/1000 | Loss: 0.00002951
Iteration 39/1000 | Loss: 0.00002951
Iteration 40/1000 | Loss: 0.00002950
Iteration 41/1000 | Loss: 0.00002950
Iteration 42/1000 | Loss: 0.00002946
Iteration 43/1000 | Loss: 0.00002946
Iteration 44/1000 | Loss: 0.00002945
Iteration 45/1000 | Loss: 0.00002945
Iteration 46/1000 | Loss: 0.00002945
Iteration 47/1000 | Loss: 0.00002944
Iteration 48/1000 | Loss: 0.00002944
Iteration 49/1000 | Loss: 0.00002943
Iteration 50/1000 | Loss: 0.00002943
Iteration 51/1000 | Loss: 0.00002943
Iteration 52/1000 | Loss: 0.00002943
Iteration 53/1000 | Loss: 0.00002942
Iteration 54/1000 | Loss: 0.00002942
Iteration 55/1000 | Loss: 0.00002942
Iteration 56/1000 | Loss: 0.00002942
Iteration 57/1000 | Loss: 0.00002942
Iteration 58/1000 | Loss: 0.00002942
Iteration 59/1000 | Loss: 0.00002942
Iteration 60/1000 | Loss: 0.00002942
Iteration 61/1000 | Loss: 0.00002942
Iteration 62/1000 | Loss: 0.00002941
Iteration 63/1000 | Loss: 0.00002941
Iteration 64/1000 | Loss: 0.00002941
Iteration 65/1000 | Loss: 0.00002941
Iteration 66/1000 | Loss: 0.00002941
Iteration 67/1000 | Loss: 0.00002940
Iteration 68/1000 | Loss: 0.00002940
Iteration 69/1000 | Loss: 0.00002940
Iteration 70/1000 | Loss: 0.00002940
Iteration 71/1000 | Loss: 0.00002940
Iteration 72/1000 | Loss: 0.00002940
Iteration 73/1000 | Loss: 0.00002939
Iteration 74/1000 | Loss: 0.00002939
Iteration 75/1000 | Loss: 0.00002938
Iteration 76/1000 | Loss: 0.00002938
Iteration 77/1000 | Loss: 0.00002938
Iteration 78/1000 | Loss: 0.00002938
Iteration 79/1000 | Loss: 0.00002937
Iteration 80/1000 | Loss: 0.00002937
Iteration 81/1000 | Loss: 0.00002937
Iteration 82/1000 | Loss: 0.00002937
Iteration 83/1000 | Loss: 0.00002936
Iteration 84/1000 | Loss: 0.00002936
Iteration 85/1000 | Loss: 0.00002936
Iteration 86/1000 | Loss: 0.00002936
Iteration 87/1000 | Loss: 0.00002935
Iteration 88/1000 | Loss: 0.00002935
Iteration 89/1000 | Loss: 0.00002935
Iteration 90/1000 | Loss: 0.00002934
Iteration 91/1000 | Loss: 0.00002934
Iteration 92/1000 | Loss: 0.00002934
Iteration 93/1000 | Loss: 0.00002934
Iteration 94/1000 | Loss: 0.00002934
Iteration 95/1000 | Loss: 0.00002934
Iteration 96/1000 | Loss: 0.00002934
Iteration 97/1000 | Loss: 0.00002934
Iteration 98/1000 | Loss: 0.00002933
Iteration 99/1000 | Loss: 0.00002933
Iteration 100/1000 | Loss: 0.00002933
Iteration 101/1000 | Loss: 0.00002933
Iteration 102/1000 | Loss: 0.00002933
Iteration 103/1000 | Loss: 0.00002933
Iteration 104/1000 | Loss: 0.00002932
Iteration 105/1000 | Loss: 0.00002932
Iteration 106/1000 | Loss: 0.00002932
Iteration 107/1000 | Loss: 0.00002932
Iteration 108/1000 | Loss: 0.00002932
Iteration 109/1000 | Loss: 0.00002932
Iteration 110/1000 | Loss: 0.00002932
Iteration 111/1000 | Loss: 0.00002932
Iteration 112/1000 | Loss: 0.00002932
Iteration 113/1000 | Loss: 0.00002932
Iteration 114/1000 | Loss: 0.00002932
Iteration 115/1000 | Loss: 0.00002932
Iteration 116/1000 | Loss: 0.00002932
Iteration 117/1000 | Loss: 0.00002932
Iteration 118/1000 | Loss: 0.00002932
Iteration 119/1000 | Loss: 0.00002932
Iteration 120/1000 | Loss: 0.00002931
Iteration 121/1000 | Loss: 0.00002931
Iteration 122/1000 | Loss: 0.00002931
Iteration 123/1000 | Loss: 0.00002931
Iteration 124/1000 | Loss: 0.00002931
Iteration 125/1000 | Loss: 0.00002931
Iteration 126/1000 | Loss: 0.00002931
Iteration 127/1000 | Loss: 0.00002931
Iteration 128/1000 | Loss: 0.00002931
Iteration 129/1000 | Loss: 0.00002931
Iteration 130/1000 | Loss: 0.00002931
Iteration 131/1000 | Loss: 0.00002931
Iteration 132/1000 | Loss: 0.00002930
Iteration 133/1000 | Loss: 0.00002930
Iteration 134/1000 | Loss: 0.00002930
Iteration 135/1000 | Loss: 0.00002930
Iteration 136/1000 | Loss: 0.00002930
Iteration 137/1000 | Loss: 0.00002930
Iteration 138/1000 | Loss: 0.00002930
Iteration 139/1000 | Loss: 0.00002930
Iteration 140/1000 | Loss: 0.00002930
Iteration 141/1000 | Loss: 0.00002930
Iteration 142/1000 | Loss: 0.00002929
Iteration 143/1000 | Loss: 0.00002929
Iteration 144/1000 | Loss: 0.00002929
Iteration 145/1000 | Loss: 0.00002929
Iteration 146/1000 | Loss: 0.00002929
Iteration 147/1000 | Loss: 0.00002929
Iteration 148/1000 | Loss: 0.00002929
Iteration 149/1000 | Loss: 0.00002929
Iteration 150/1000 | Loss: 0.00002929
Iteration 151/1000 | Loss: 0.00002929
Iteration 152/1000 | Loss: 0.00002929
Iteration 153/1000 | Loss: 0.00002929
Iteration 154/1000 | Loss: 0.00002929
Iteration 155/1000 | Loss: 0.00002929
Iteration 156/1000 | Loss: 0.00002929
Iteration 157/1000 | Loss: 0.00002929
Iteration 158/1000 | Loss: 0.00002929
Iteration 159/1000 | Loss: 0.00002929
Iteration 160/1000 | Loss: 0.00002929
Iteration 161/1000 | Loss: 0.00002929
Iteration 162/1000 | Loss: 0.00002929
Iteration 163/1000 | Loss: 0.00002929
Iteration 164/1000 | Loss: 0.00002929
Iteration 165/1000 | Loss: 0.00002929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.9287748475326225e-05, 2.9287748475326225e-05, 2.9287748475326225e-05, 2.9287748475326225e-05, 2.9287748475326225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9287748475326225e-05

Optimization complete. Final v2v error: 3.840163230895996 mm

Highest mean error: 12.25924301147461 mm for frame 21

Lowest mean error: 3.1617062091827393 mm for frame 152

Saving results

Total time: 94.32605814933777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00634211
Iteration 2/25 | Loss: 0.00089106
Iteration 3/25 | Loss: 0.00074867
Iteration 4/25 | Loss: 0.00071717
Iteration 5/25 | Loss: 0.00070779
Iteration 6/25 | Loss: 0.00070637
Iteration 7/25 | Loss: 0.00070588
Iteration 8/25 | Loss: 0.00070588
Iteration 9/25 | Loss: 0.00070588
Iteration 10/25 | Loss: 0.00070588
Iteration 11/25 | Loss: 0.00070588
Iteration 12/25 | Loss: 0.00070588
Iteration 13/25 | Loss: 0.00070588
Iteration 14/25 | Loss: 0.00070588
Iteration 15/25 | Loss: 0.00070588
Iteration 16/25 | Loss: 0.00070588
Iteration 17/25 | Loss: 0.00070588
Iteration 18/25 | Loss: 0.00070588
Iteration 19/25 | Loss: 0.00070588
Iteration 20/25 | Loss: 0.00070588
Iteration 21/25 | Loss: 0.00070588
Iteration 22/25 | Loss: 0.00070588
Iteration 23/25 | Loss: 0.00070588
Iteration 24/25 | Loss: 0.00070588
Iteration 25/25 | Loss: 0.00070588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66251254
Iteration 2/25 | Loss: 0.00029395
Iteration 3/25 | Loss: 0.00029395
Iteration 4/25 | Loss: 0.00029395
Iteration 5/25 | Loss: 0.00029395
Iteration 6/25 | Loss: 0.00029395
Iteration 7/25 | Loss: 0.00029395
Iteration 8/25 | Loss: 0.00029395
Iteration 9/25 | Loss: 0.00029395
Iteration 10/25 | Loss: 0.00029395
Iteration 11/25 | Loss: 0.00029395
Iteration 12/25 | Loss: 0.00029395
Iteration 13/25 | Loss: 0.00029395
Iteration 14/25 | Loss: 0.00029395
Iteration 15/25 | Loss: 0.00029395
Iteration 16/25 | Loss: 0.00029395
Iteration 17/25 | Loss: 0.00029395
Iteration 18/25 | Loss: 0.00029395
Iteration 19/25 | Loss: 0.00029395
Iteration 20/25 | Loss: 0.00029395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002939450496342033, 0.0002939450496342033, 0.0002939450496342033, 0.0002939450496342033, 0.0002939450496342033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002939450496342033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029395
Iteration 2/1000 | Loss: 0.00003153
Iteration 3/1000 | Loss: 0.00002226
Iteration 4/1000 | Loss: 0.00002062
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001928
Iteration 7/1000 | Loss: 0.00001886
Iteration 8/1000 | Loss: 0.00001865
Iteration 9/1000 | Loss: 0.00001855
Iteration 10/1000 | Loss: 0.00001845
Iteration 11/1000 | Loss: 0.00001845
Iteration 12/1000 | Loss: 0.00001844
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001837
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001830
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001825
Iteration 22/1000 | Loss: 0.00001823
Iteration 23/1000 | Loss: 0.00001821
Iteration 24/1000 | Loss: 0.00001814
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001809
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001807
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001805
Iteration 36/1000 | Loss: 0.00001804
Iteration 37/1000 | Loss: 0.00001804
Iteration 38/1000 | Loss: 0.00001803
Iteration 39/1000 | Loss: 0.00001803
Iteration 40/1000 | Loss: 0.00001803
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001800
Iteration 44/1000 | Loss: 0.00001799
Iteration 45/1000 | Loss: 0.00001799
Iteration 46/1000 | Loss: 0.00001798
Iteration 47/1000 | Loss: 0.00001797
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001796
Iteration 50/1000 | Loss: 0.00001795
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001793
Iteration 53/1000 | Loss: 0.00001793
Iteration 54/1000 | Loss: 0.00001793
Iteration 55/1000 | Loss: 0.00001792
Iteration 56/1000 | Loss: 0.00001791
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001790
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001789
Iteration 61/1000 | Loss: 0.00001788
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001786
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001785
Iteration 74/1000 | Loss: 0.00001785
Iteration 75/1000 | Loss: 0.00001785
Iteration 76/1000 | Loss: 0.00001785
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001784
Iteration 80/1000 | Loss: 0.00001784
Iteration 81/1000 | Loss: 0.00001784
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001782
Iteration 91/1000 | Loss: 0.00001782
Iteration 92/1000 | Loss: 0.00001781
Iteration 93/1000 | Loss: 0.00001781
Iteration 94/1000 | Loss: 0.00001781
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001781
Iteration 104/1000 | Loss: 0.00001781
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001780
Iteration 108/1000 | Loss: 0.00001780
Iteration 109/1000 | Loss: 0.00001780
Iteration 110/1000 | Loss: 0.00001780
Iteration 111/1000 | Loss: 0.00001780
Iteration 112/1000 | Loss: 0.00001780
Iteration 113/1000 | Loss: 0.00001780
Iteration 114/1000 | Loss: 0.00001780
Iteration 115/1000 | Loss: 0.00001779
Iteration 116/1000 | Loss: 0.00001779
Iteration 117/1000 | Loss: 0.00001779
Iteration 118/1000 | Loss: 0.00001779
Iteration 119/1000 | Loss: 0.00001779
Iteration 120/1000 | Loss: 0.00001779
Iteration 121/1000 | Loss: 0.00001779
Iteration 122/1000 | Loss: 0.00001779
Iteration 123/1000 | Loss: 0.00001779
Iteration 124/1000 | Loss: 0.00001779
Iteration 125/1000 | Loss: 0.00001779
Iteration 126/1000 | Loss: 0.00001779
Iteration 127/1000 | Loss: 0.00001779
Iteration 128/1000 | Loss: 0.00001779
Iteration 129/1000 | Loss: 0.00001779
Iteration 130/1000 | Loss: 0.00001779
Iteration 131/1000 | Loss: 0.00001778
Iteration 132/1000 | Loss: 0.00001778
Iteration 133/1000 | Loss: 0.00001778
Iteration 134/1000 | Loss: 0.00001778
Iteration 135/1000 | Loss: 0.00001778
Iteration 136/1000 | Loss: 0.00001778
Iteration 137/1000 | Loss: 0.00001778
Iteration 138/1000 | Loss: 0.00001778
Iteration 139/1000 | Loss: 0.00001778
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001778
Iteration 142/1000 | Loss: 0.00001778
Iteration 143/1000 | Loss: 0.00001778
Iteration 144/1000 | Loss: 0.00001778
Iteration 145/1000 | Loss: 0.00001778
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001777
Iteration 148/1000 | Loss: 0.00001777
Iteration 149/1000 | Loss: 0.00001777
Iteration 150/1000 | Loss: 0.00001777
Iteration 151/1000 | Loss: 0.00001777
Iteration 152/1000 | Loss: 0.00001777
Iteration 153/1000 | Loss: 0.00001777
Iteration 154/1000 | Loss: 0.00001777
Iteration 155/1000 | Loss: 0.00001777
Iteration 156/1000 | Loss: 0.00001777
Iteration 157/1000 | Loss: 0.00001777
Iteration 158/1000 | Loss: 0.00001777
Iteration 159/1000 | Loss: 0.00001777
Iteration 160/1000 | Loss: 0.00001777
Iteration 161/1000 | Loss: 0.00001777
Iteration 162/1000 | Loss: 0.00001777
Iteration 163/1000 | Loss: 0.00001777
Iteration 164/1000 | Loss: 0.00001777
Iteration 165/1000 | Loss: 0.00001776
Iteration 166/1000 | Loss: 0.00001776
Iteration 167/1000 | Loss: 0.00001776
Iteration 168/1000 | Loss: 0.00001776
Iteration 169/1000 | Loss: 0.00001776
Iteration 170/1000 | Loss: 0.00001776
Iteration 171/1000 | Loss: 0.00001776
Iteration 172/1000 | Loss: 0.00001776
Iteration 173/1000 | Loss: 0.00001776
Iteration 174/1000 | Loss: 0.00001776
Iteration 175/1000 | Loss: 0.00001776
Iteration 176/1000 | Loss: 0.00001776
Iteration 177/1000 | Loss: 0.00001776
Iteration 178/1000 | Loss: 0.00001776
Iteration 179/1000 | Loss: 0.00001776
Iteration 180/1000 | Loss: 0.00001775
Iteration 181/1000 | Loss: 0.00001775
Iteration 182/1000 | Loss: 0.00001775
Iteration 183/1000 | Loss: 0.00001775
Iteration 184/1000 | Loss: 0.00001775
Iteration 185/1000 | Loss: 0.00001775
Iteration 186/1000 | Loss: 0.00001775
Iteration 187/1000 | Loss: 0.00001775
Iteration 188/1000 | Loss: 0.00001775
Iteration 189/1000 | Loss: 0.00001775
Iteration 190/1000 | Loss: 0.00001775
Iteration 191/1000 | Loss: 0.00001775
Iteration 192/1000 | Loss: 0.00001775
Iteration 193/1000 | Loss: 0.00001775
Iteration 194/1000 | Loss: 0.00001775
Iteration 195/1000 | Loss: 0.00001775
Iteration 196/1000 | Loss: 0.00001775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.7753580323187634e-05, 1.7753580323187634e-05, 1.7753580323187634e-05, 1.7753580323187634e-05, 1.7753580323187634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7753580323187634e-05

Optimization complete. Final v2v error: 3.507664918899536 mm

Highest mean error: 3.923311710357666 mm for frame 102

Lowest mean error: 3.144928216934204 mm for frame 115

Saving results

Total time: 38.78270506858826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113406
Iteration 2/25 | Loss: 0.00540628
Iteration 3/25 | Loss: 0.00288810
Iteration 4/25 | Loss: 0.00205388
Iteration 5/25 | Loss: 0.00189844
Iteration 6/25 | Loss: 0.00176615
Iteration 7/25 | Loss: 0.00161112
Iteration 8/25 | Loss: 0.00154647
Iteration 9/25 | Loss: 0.00141524
Iteration 10/25 | Loss: 0.00136094
Iteration 11/25 | Loss: 0.00134286
Iteration 12/25 | Loss: 0.00131878
Iteration 13/25 | Loss: 0.00132213
Iteration 14/25 | Loss: 0.00129140
Iteration 15/25 | Loss: 0.00130180
Iteration 16/25 | Loss: 0.00129071
Iteration 17/25 | Loss: 0.00128231
Iteration 18/25 | Loss: 0.00127010
Iteration 19/25 | Loss: 0.00126805
Iteration 20/25 | Loss: 0.00127385
Iteration 21/25 | Loss: 0.00126606
Iteration 22/25 | Loss: 0.00126926
Iteration 23/25 | Loss: 0.00126331
Iteration 24/25 | Loss: 0.00127057
Iteration 25/25 | Loss: 0.00126755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56871289
Iteration 2/25 | Loss: 0.00269268
Iteration 3/25 | Loss: 0.00230049
Iteration 4/25 | Loss: 0.00230048
Iteration 5/25 | Loss: 0.00230048
Iteration 6/25 | Loss: 0.00230048
Iteration 7/25 | Loss: 0.00230048
Iteration 8/25 | Loss: 0.00230048
Iteration 9/25 | Loss: 0.00230048
Iteration 10/25 | Loss: 0.00230048
Iteration 11/25 | Loss: 0.00230048
Iteration 12/25 | Loss: 0.00230048
Iteration 13/25 | Loss: 0.00230048
Iteration 14/25 | Loss: 0.00230048
Iteration 15/25 | Loss: 0.00230048
Iteration 16/25 | Loss: 0.00230048
Iteration 17/25 | Loss: 0.00230048
Iteration 18/25 | Loss: 0.00230048
Iteration 19/25 | Loss: 0.00230048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0023004794493317604, 0.0023004794493317604, 0.0023004794493317604, 0.0023004794493317604, 0.0023004794493317604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023004794493317604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230048
Iteration 2/1000 | Loss: 0.00062949
Iteration 3/1000 | Loss: 0.00042099
Iteration 4/1000 | Loss: 0.00073323
Iteration 5/1000 | Loss: 0.00034957
Iteration 6/1000 | Loss: 0.00105738
Iteration 7/1000 | Loss: 0.00032002
Iteration 8/1000 | Loss: 0.00023388
Iteration 9/1000 | Loss: 0.00028653
Iteration 10/1000 | Loss: 0.00029656
Iteration 11/1000 | Loss: 0.00564905
Iteration 12/1000 | Loss: 0.00556298
Iteration 13/1000 | Loss: 0.00044712
Iteration 14/1000 | Loss: 0.00054188
Iteration 15/1000 | Loss: 0.00031476
Iteration 16/1000 | Loss: 0.00074039
Iteration 17/1000 | Loss: 0.00082512
Iteration 18/1000 | Loss: 0.00039990
Iteration 19/1000 | Loss: 0.00014476
Iteration 20/1000 | Loss: 0.00020812
Iteration 21/1000 | Loss: 0.00005056
Iteration 22/1000 | Loss: 0.00031450
Iteration 23/1000 | Loss: 0.00019922
Iteration 24/1000 | Loss: 0.00009340
Iteration 25/1000 | Loss: 0.00005604
Iteration 26/1000 | Loss: 0.00024085
Iteration 27/1000 | Loss: 0.00044303
Iteration 28/1000 | Loss: 0.00008496
Iteration 29/1000 | Loss: 0.00012843
Iteration 30/1000 | Loss: 0.00008822
Iteration 31/1000 | Loss: 0.00011725
Iteration 32/1000 | Loss: 0.00002839
Iteration 33/1000 | Loss: 0.00008462
Iteration 34/1000 | Loss: 0.00002891
Iteration 35/1000 | Loss: 0.00008293
Iteration 36/1000 | Loss: 0.00002439
Iteration 37/1000 | Loss: 0.00003790
Iteration 38/1000 | Loss: 0.00003701
Iteration 39/1000 | Loss: 0.00010789
Iteration 40/1000 | Loss: 0.00003247
Iteration 41/1000 | Loss: 0.00008045
Iteration 42/1000 | Loss: 0.00002312
Iteration 43/1000 | Loss: 0.00002442
Iteration 44/1000 | Loss: 0.00006476
Iteration 45/1000 | Loss: 0.00002273
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00002594
Iteration 48/1000 | Loss: 0.00007340
Iteration 49/1000 | Loss: 0.00074325
Iteration 50/1000 | Loss: 0.00099378
Iteration 51/1000 | Loss: 0.00014187
Iteration 52/1000 | Loss: 0.00010804
Iteration 53/1000 | Loss: 0.00008928
Iteration 54/1000 | Loss: 0.00019381
Iteration 55/1000 | Loss: 0.00245265
Iteration 56/1000 | Loss: 0.00010187
Iteration 57/1000 | Loss: 0.00002498
Iteration 58/1000 | Loss: 0.00003177
Iteration 59/1000 | Loss: 0.00020220
Iteration 60/1000 | Loss: 0.00003770
Iteration 61/1000 | Loss: 0.00005401
Iteration 62/1000 | Loss: 0.00003819
Iteration 63/1000 | Loss: 0.00005884
Iteration 64/1000 | Loss: 0.00002378
Iteration 65/1000 | Loss: 0.00003356
Iteration 66/1000 | Loss: 0.00012115
Iteration 67/1000 | Loss: 0.00005058
Iteration 68/1000 | Loss: 0.00003962
Iteration 69/1000 | Loss: 0.00002231
Iteration 70/1000 | Loss: 0.00002536
Iteration 71/1000 | Loss: 0.00003726
Iteration 72/1000 | Loss: 0.00002540
Iteration 73/1000 | Loss: 0.00002342
Iteration 74/1000 | Loss: 0.00002227
Iteration 75/1000 | Loss: 0.00002227
Iteration 76/1000 | Loss: 0.00002227
Iteration 77/1000 | Loss: 0.00002227
Iteration 78/1000 | Loss: 0.00002227
Iteration 79/1000 | Loss: 0.00002227
Iteration 80/1000 | Loss: 0.00002227
Iteration 81/1000 | Loss: 0.00002227
Iteration 82/1000 | Loss: 0.00002227
Iteration 83/1000 | Loss: 0.00002227
Iteration 84/1000 | Loss: 0.00002597
Iteration 85/1000 | Loss: 0.00002229
Iteration 86/1000 | Loss: 0.00002227
Iteration 87/1000 | Loss: 0.00002227
Iteration 88/1000 | Loss: 0.00002227
Iteration 89/1000 | Loss: 0.00002227
Iteration 90/1000 | Loss: 0.00002227
Iteration 91/1000 | Loss: 0.00002227
Iteration 92/1000 | Loss: 0.00002227
Iteration 93/1000 | Loss: 0.00002227
Iteration 94/1000 | Loss: 0.00002226
Iteration 95/1000 | Loss: 0.00002226
Iteration 96/1000 | Loss: 0.00002227
Iteration 97/1000 | Loss: 0.00002227
Iteration 98/1000 | Loss: 0.00002224
Iteration 99/1000 | Loss: 0.00002224
Iteration 100/1000 | Loss: 0.00002224
Iteration 101/1000 | Loss: 0.00002224
Iteration 102/1000 | Loss: 0.00002223
Iteration 103/1000 | Loss: 0.00002223
Iteration 104/1000 | Loss: 0.00002223
Iteration 105/1000 | Loss: 0.00002223
Iteration 106/1000 | Loss: 0.00002223
Iteration 107/1000 | Loss: 0.00002223
Iteration 108/1000 | Loss: 0.00002223
Iteration 109/1000 | Loss: 0.00002223
Iteration 110/1000 | Loss: 0.00002223
Iteration 111/1000 | Loss: 0.00002222
Iteration 112/1000 | Loss: 0.00002222
Iteration 113/1000 | Loss: 0.00002222
Iteration 114/1000 | Loss: 0.00002954
Iteration 115/1000 | Loss: 0.00002243
Iteration 116/1000 | Loss: 0.00002220
Iteration 117/1000 | Loss: 0.00002220
Iteration 118/1000 | Loss: 0.00002220
Iteration 119/1000 | Loss: 0.00002220
Iteration 120/1000 | Loss: 0.00002220
Iteration 121/1000 | Loss: 0.00002220
Iteration 122/1000 | Loss: 0.00002220
Iteration 123/1000 | Loss: 0.00002220
Iteration 124/1000 | Loss: 0.00002220
Iteration 125/1000 | Loss: 0.00002220
Iteration 126/1000 | Loss: 0.00002220
Iteration 127/1000 | Loss: 0.00002220
Iteration 128/1000 | Loss: 0.00002220
Iteration 129/1000 | Loss: 0.00002220
Iteration 130/1000 | Loss: 0.00002220
Iteration 131/1000 | Loss: 0.00002220
Iteration 132/1000 | Loss: 0.00002220
Iteration 133/1000 | Loss: 0.00002220
Iteration 134/1000 | Loss: 0.00002220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.2196207282831892e-05, 2.2196207282831892e-05, 2.2196207282831892e-05, 2.2196207282831892e-05, 2.2196207282831892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2196207282831892e-05

Optimization complete. Final v2v error: 3.9912877082824707 mm

Highest mean error: 4.343501567840576 mm for frame 107

Lowest mean error: 3.821171522140503 mm for frame 43

Saving results

Total time: 153.16329622268677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050659
Iteration 2/25 | Loss: 0.00282401
Iteration 3/25 | Loss: 0.00225910
Iteration 4/25 | Loss: 0.00191660
Iteration 5/25 | Loss: 0.00177683
Iteration 6/25 | Loss: 0.00159768
Iteration 7/25 | Loss: 0.00157953
Iteration 8/25 | Loss: 0.00145940
Iteration 9/25 | Loss: 0.00128661
Iteration 10/25 | Loss: 0.00113833
Iteration 11/25 | Loss: 0.00109936
Iteration 12/25 | Loss: 0.00103989
Iteration 13/25 | Loss: 0.00101648
Iteration 14/25 | Loss: 0.00099143
Iteration 15/25 | Loss: 0.00095873
Iteration 16/25 | Loss: 0.00094789
Iteration 17/25 | Loss: 0.00095377
Iteration 18/25 | Loss: 0.00094210
Iteration 19/25 | Loss: 0.00093796
Iteration 20/25 | Loss: 0.00093239
Iteration 21/25 | Loss: 0.00092930
Iteration 22/25 | Loss: 0.00092830
Iteration 23/25 | Loss: 0.00092799
Iteration 24/25 | Loss: 0.00092777
Iteration 25/25 | Loss: 0.00093123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49915636
Iteration 2/25 | Loss: 0.00127573
Iteration 3/25 | Loss: 0.00102040
Iteration 4/25 | Loss: 0.00102040
Iteration 5/25 | Loss: 0.00102040
Iteration 6/25 | Loss: 0.00102040
Iteration 7/25 | Loss: 0.00102040
Iteration 8/25 | Loss: 0.00102040
Iteration 9/25 | Loss: 0.00102040
Iteration 10/25 | Loss: 0.00102040
Iteration 11/25 | Loss: 0.00102040
Iteration 12/25 | Loss: 0.00102040
Iteration 13/25 | Loss: 0.00102040
Iteration 14/25 | Loss: 0.00102040
Iteration 15/25 | Loss: 0.00102040
Iteration 16/25 | Loss: 0.00102040
Iteration 17/25 | Loss: 0.00102040
Iteration 18/25 | Loss: 0.00102040
Iteration 19/25 | Loss: 0.00102040
Iteration 20/25 | Loss: 0.00102040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010203965939581394, 0.0010203965939581394, 0.0010203965939581394, 0.0010203965939581394, 0.0010203965939581394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010203965939581394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102040
Iteration 2/1000 | Loss: 0.00197044
Iteration 3/1000 | Loss: 0.00126433
Iteration 4/1000 | Loss: 0.00082456
Iteration 5/1000 | Loss: 0.00022491
Iteration 6/1000 | Loss: 0.00016542
Iteration 7/1000 | Loss: 0.00066189
Iteration 8/1000 | Loss: 0.00010573
Iteration 9/1000 | Loss: 0.00122188
Iteration 10/1000 | Loss: 0.00047832
Iteration 11/1000 | Loss: 0.00027917
Iteration 12/1000 | Loss: 0.00005442
Iteration 13/1000 | Loss: 0.00004533
Iteration 14/1000 | Loss: 0.00064353
Iteration 15/1000 | Loss: 0.00057722
Iteration 16/1000 | Loss: 0.00066643
Iteration 17/1000 | Loss: 0.00041495
Iteration 18/1000 | Loss: 0.00019559
Iteration 19/1000 | Loss: 0.00082408
Iteration 20/1000 | Loss: 0.00017005
Iteration 21/1000 | Loss: 0.00056531
Iteration 22/1000 | Loss: 0.00042570
Iteration 23/1000 | Loss: 0.00007919
Iteration 24/1000 | Loss: 0.00076378
Iteration 25/1000 | Loss: 0.00049017
Iteration 26/1000 | Loss: 0.00021842
Iteration 27/1000 | Loss: 0.00004466
Iteration 28/1000 | Loss: 0.00003536
Iteration 29/1000 | Loss: 0.00009284
Iteration 30/1000 | Loss: 0.00003136
Iteration 31/1000 | Loss: 0.00002974
Iteration 32/1000 | Loss: 0.00005700
Iteration 33/1000 | Loss: 0.00002847
Iteration 34/1000 | Loss: 0.00002753
Iteration 35/1000 | Loss: 0.00038621
Iteration 36/1000 | Loss: 0.00042517
Iteration 37/1000 | Loss: 0.00002986
Iteration 38/1000 | Loss: 0.00002724
Iteration 39/1000 | Loss: 0.00002653
Iteration 40/1000 | Loss: 0.00034920
Iteration 41/1000 | Loss: 0.00004660
Iteration 42/1000 | Loss: 0.00004973
Iteration 43/1000 | Loss: 0.00005469
Iteration 44/1000 | Loss: 0.00002642
Iteration 45/1000 | Loss: 0.00002608
Iteration 46/1000 | Loss: 0.00002582
Iteration 47/1000 | Loss: 0.00002564
Iteration 48/1000 | Loss: 0.00002562
Iteration 49/1000 | Loss: 0.00002547
Iteration 50/1000 | Loss: 0.00002542
Iteration 51/1000 | Loss: 0.00002535
Iteration 52/1000 | Loss: 0.00077011
Iteration 53/1000 | Loss: 0.00003529
Iteration 54/1000 | Loss: 0.00002596
Iteration 55/1000 | Loss: 0.00002380
Iteration 56/1000 | Loss: 0.00002289
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002169
Iteration 60/1000 | Loss: 0.00002167
Iteration 61/1000 | Loss: 0.00002167
Iteration 62/1000 | Loss: 0.00002167
Iteration 63/1000 | Loss: 0.00002167
Iteration 64/1000 | Loss: 0.00002167
Iteration 65/1000 | Loss: 0.00002167
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002166
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002137
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002115
Iteration 74/1000 | Loss: 0.00002096
Iteration 75/1000 | Loss: 0.00002085
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002081
Iteration 78/1000 | Loss: 0.00002080
Iteration 79/1000 | Loss: 0.00002079
Iteration 80/1000 | Loss: 0.00002079
Iteration 81/1000 | Loss: 0.00002079
Iteration 82/1000 | Loss: 0.00002078
Iteration 83/1000 | Loss: 0.00002078
Iteration 84/1000 | Loss: 0.00002077
Iteration 85/1000 | Loss: 0.00002077
Iteration 86/1000 | Loss: 0.00002077
Iteration 87/1000 | Loss: 0.00002076
Iteration 88/1000 | Loss: 0.00002076
Iteration 89/1000 | Loss: 0.00002074
Iteration 90/1000 | Loss: 0.00002074
Iteration 91/1000 | Loss: 0.00002074
Iteration 92/1000 | Loss: 0.00002074
Iteration 93/1000 | Loss: 0.00002074
Iteration 94/1000 | Loss: 0.00002073
Iteration 95/1000 | Loss: 0.00002073
Iteration 96/1000 | Loss: 0.00015171
Iteration 97/1000 | Loss: 0.00003733
Iteration 98/1000 | Loss: 0.00002363
Iteration 99/1000 | Loss: 0.00002145
Iteration 100/1000 | Loss: 0.00002079
Iteration 101/1000 | Loss: 0.00001996
Iteration 102/1000 | Loss: 0.00035830
Iteration 103/1000 | Loss: 0.00002921
Iteration 104/1000 | Loss: 0.00002263
Iteration 105/1000 | Loss: 0.00002118
Iteration 106/1000 | Loss: 0.00001986
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001927
Iteration 109/1000 | Loss: 0.00001922
Iteration 110/1000 | Loss: 0.00001920
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00001918
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001917
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001912
Iteration 119/1000 | Loss: 0.00001912
Iteration 120/1000 | Loss: 0.00001911
Iteration 121/1000 | Loss: 0.00001911
Iteration 122/1000 | Loss: 0.00001911
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001909
Iteration 126/1000 | Loss: 0.00001909
Iteration 127/1000 | Loss: 0.00001908
Iteration 128/1000 | Loss: 0.00001908
Iteration 129/1000 | Loss: 0.00001908
Iteration 130/1000 | Loss: 0.00001908
Iteration 131/1000 | Loss: 0.00001908
Iteration 132/1000 | Loss: 0.00001908
Iteration 133/1000 | Loss: 0.00001908
Iteration 134/1000 | Loss: 0.00001908
Iteration 135/1000 | Loss: 0.00001908
Iteration 136/1000 | Loss: 0.00001908
Iteration 137/1000 | Loss: 0.00001907
Iteration 138/1000 | Loss: 0.00001907
Iteration 139/1000 | Loss: 0.00001907
Iteration 140/1000 | Loss: 0.00001907
Iteration 141/1000 | Loss: 0.00001907
Iteration 142/1000 | Loss: 0.00001907
Iteration 143/1000 | Loss: 0.00001907
Iteration 144/1000 | Loss: 0.00001906
Iteration 145/1000 | Loss: 0.00001906
Iteration 146/1000 | Loss: 0.00001906
Iteration 147/1000 | Loss: 0.00001906
Iteration 148/1000 | Loss: 0.00001906
Iteration 149/1000 | Loss: 0.00001906
Iteration 150/1000 | Loss: 0.00001906
Iteration 151/1000 | Loss: 0.00001906
Iteration 152/1000 | Loss: 0.00001906
Iteration 153/1000 | Loss: 0.00001906
Iteration 154/1000 | Loss: 0.00001906
Iteration 155/1000 | Loss: 0.00001906
Iteration 156/1000 | Loss: 0.00001906
Iteration 157/1000 | Loss: 0.00001906
Iteration 158/1000 | Loss: 0.00001906
Iteration 159/1000 | Loss: 0.00001906
Iteration 160/1000 | Loss: 0.00001906
Iteration 161/1000 | Loss: 0.00001906
Iteration 162/1000 | Loss: 0.00001906
Iteration 163/1000 | Loss: 0.00001906
Iteration 164/1000 | Loss: 0.00001906
Iteration 165/1000 | Loss: 0.00001906
Iteration 166/1000 | Loss: 0.00001906
Iteration 167/1000 | Loss: 0.00001906
Iteration 168/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.9062290448346175e-05, 1.9062290448346175e-05, 1.9062290448346175e-05, 1.9062290448346175e-05, 1.9062290448346175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9062290448346175e-05

Optimization complete. Final v2v error: 3.6601381301879883 mm

Highest mean error: 4.551651477813721 mm for frame 87

Lowest mean error: 3.284666061401367 mm for frame 101

Saving results

Total time: 169.77441430091858
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472375
Iteration 2/25 | Loss: 0.00101274
Iteration 3/25 | Loss: 0.00076348
Iteration 4/25 | Loss: 0.00074015
Iteration 5/25 | Loss: 0.00073266
Iteration 6/25 | Loss: 0.00073008
Iteration 7/25 | Loss: 0.00072950
Iteration 8/25 | Loss: 0.00072950
Iteration 9/25 | Loss: 0.00072950
Iteration 10/25 | Loss: 0.00072950
Iteration 11/25 | Loss: 0.00072950
Iteration 12/25 | Loss: 0.00072950
Iteration 13/25 | Loss: 0.00072950
Iteration 14/25 | Loss: 0.00072950
Iteration 15/25 | Loss: 0.00072950
Iteration 16/25 | Loss: 0.00072950
Iteration 17/25 | Loss: 0.00072950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007294967072084546, 0.0007294967072084546, 0.0007294967072084546, 0.0007294967072084546, 0.0007294967072084546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007294967072084546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46190143
Iteration 2/25 | Loss: 0.00036293
Iteration 3/25 | Loss: 0.00036291
Iteration 4/25 | Loss: 0.00036291
Iteration 5/25 | Loss: 0.00036291
Iteration 6/25 | Loss: 0.00036291
Iteration 7/25 | Loss: 0.00036291
Iteration 8/25 | Loss: 0.00036291
Iteration 9/25 | Loss: 0.00036291
Iteration 10/25 | Loss: 0.00036291
Iteration 11/25 | Loss: 0.00036291
Iteration 12/25 | Loss: 0.00036291
Iteration 13/25 | Loss: 0.00036291
Iteration 14/25 | Loss: 0.00036291
Iteration 15/25 | Loss: 0.00036291
Iteration 16/25 | Loss: 0.00036291
Iteration 17/25 | Loss: 0.00036291
Iteration 18/25 | Loss: 0.00036291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00036291085416451097, 0.00036291085416451097, 0.00036291085416451097, 0.00036291085416451097, 0.00036291085416451097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036291085416451097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036291
Iteration 2/1000 | Loss: 0.00002360
Iteration 3/1000 | Loss: 0.00001773
Iteration 4/1000 | Loss: 0.00001636
Iteration 5/1000 | Loss: 0.00001534
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001414
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001388
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001376
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001370
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001366
Iteration 26/1000 | Loss: 0.00001365
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001363
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001360
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001358
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001357
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001356
Iteration 75/1000 | Loss: 0.00001356
Iteration 76/1000 | Loss: 0.00001356
Iteration 77/1000 | Loss: 0.00001356
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001355
Iteration 80/1000 | Loss: 0.00001355
Iteration 81/1000 | Loss: 0.00001355
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001355
Iteration 84/1000 | Loss: 0.00001355
Iteration 85/1000 | Loss: 0.00001355
Iteration 86/1000 | Loss: 0.00001355
Iteration 87/1000 | Loss: 0.00001355
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.3551417396229226e-05, 1.3551417396229226e-05, 1.3551417396229226e-05, 1.3551417396229226e-05, 1.3551417396229226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3551417396229226e-05

Optimization complete. Final v2v error: 3.1266682147979736 mm

Highest mean error: 3.5572173595428467 mm for frame 73

Lowest mean error: 2.76507306098938 mm for frame 191

Saving results

Total time: 35.46278524398804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405414
Iteration 2/25 | Loss: 0.00082122
Iteration 3/25 | Loss: 0.00068396
Iteration 4/25 | Loss: 0.00066044
Iteration 5/25 | Loss: 0.00065596
Iteration 6/25 | Loss: 0.00065476
Iteration 7/25 | Loss: 0.00065452
Iteration 8/25 | Loss: 0.00065452
Iteration 9/25 | Loss: 0.00065452
Iteration 10/25 | Loss: 0.00065452
Iteration 11/25 | Loss: 0.00065452
Iteration 12/25 | Loss: 0.00065452
Iteration 13/25 | Loss: 0.00065452
Iteration 14/25 | Loss: 0.00065452
Iteration 15/25 | Loss: 0.00065452
Iteration 16/25 | Loss: 0.00065452
Iteration 17/25 | Loss: 0.00065452
Iteration 18/25 | Loss: 0.00065452
Iteration 19/25 | Loss: 0.00065452
Iteration 20/25 | Loss: 0.00065452
Iteration 21/25 | Loss: 0.00065452
Iteration 22/25 | Loss: 0.00065452
Iteration 23/25 | Loss: 0.00065452
Iteration 24/25 | Loss: 0.00065452
Iteration 25/25 | Loss: 0.00065452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006545228534378111, 0.0006545228534378111, 0.0006545228534378111, 0.0006545228534378111, 0.0006545228534378111]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006545228534378111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.48829460
Iteration 2/25 | Loss: 0.00030173
Iteration 3/25 | Loss: 0.00030172
Iteration 4/25 | Loss: 0.00030172
Iteration 5/25 | Loss: 0.00030172
Iteration 6/25 | Loss: 0.00030172
Iteration 7/25 | Loss: 0.00030172
Iteration 8/25 | Loss: 0.00030172
Iteration 9/25 | Loss: 0.00030172
Iteration 10/25 | Loss: 0.00030172
Iteration 11/25 | Loss: 0.00030172
Iteration 12/25 | Loss: 0.00030172
Iteration 13/25 | Loss: 0.00030172
Iteration 14/25 | Loss: 0.00030172
Iteration 15/25 | Loss: 0.00030172
Iteration 16/25 | Loss: 0.00030172
Iteration 17/25 | Loss: 0.00030172
Iteration 18/25 | Loss: 0.00030172
Iteration 19/25 | Loss: 0.00030172
Iteration 20/25 | Loss: 0.00030172
Iteration 21/25 | Loss: 0.00030172
Iteration 22/25 | Loss: 0.00030172
Iteration 23/25 | Loss: 0.00030172
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003017210110556334, 0.0003017210110556334, 0.0003017210110556334, 0.0003017210110556334, 0.0003017210110556334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003017210110556334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030172
Iteration 2/1000 | Loss: 0.00002744
Iteration 3/1000 | Loss: 0.00001745
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001463
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001402
Iteration 11/1000 | Loss: 0.00001385
Iteration 12/1000 | Loss: 0.00001374
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001358
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001353
Iteration 19/1000 | Loss: 0.00001352
Iteration 20/1000 | Loss: 0.00001351
Iteration 21/1000 | Loss: 0.00001350
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001349
Iteration 24/1000 | Loss: 0.00001349
Iteration 25/1000 | Loss: 0.00001349
Iteration 26/1000 | Loss: 0.00001349
Iteration 27/1000 | Loss: 0.00001349
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001342
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001341
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001322
Iteration 106/1000 | Loss: 0.00001322
Iteration 107/1000 | Loss: 0.00001322
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001320
Iteration 114/1000 | Loss: 0.00001320
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001320
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001318
Iteration 130/1000 | Loss: 0.00001318
Iteration 131/1000 | Loss: 0.00001318
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001318
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001318
Iteration 141/1000 | Loss: 0.00001317
Iteration 142/1000 | Loss: 0.00001317
Iteration 143/1000 | Loss: 0.00001317
Iteration 144/1000 | Loss: 0.00001317
Iteration 145/1000 | Loss: 0.00001317
Iteration 146/1000 | Loss: 0.00001317
Iteration 147/1000 | Loss: 0.00001317
Iteration 148/1000 | Loss: 0.00001317
Iteration 149/1000 | Loss: 0.00001317
Iteration 150/1000 | Loss: 0.00001317
Iteration 151/1000 | Loss: 0.00001316
Iteration 152/1000 | Loss: 0.00001316
Iteration 153/1000 | Loss: 0.00001316
Iteration 154/1000 | Loss: 0.00001316
Iteration 155/1000 | Loss: 0.00001316
Iteration 156/1000 | Loss: 0.00001316
Iteration 157/1000 | Loss: 0.00001316
Iteration 158/1000 | Loss: 0.00001316
Iteration 159/1000 | Loss: 0.00001316
Iteration 160/1000 | Loss: 0.00001316
Iteration 161/1000 | Loss: 0.00001316
Iteration 162/1000 | Loss: 0.00001316
Iteration 163/1000 | Loss: 0.00001315
Iteration 164/1000 | Loss: 0.00001315
Iteration 165/1000 | Loss: 0.00001315
Iteration 166/1000 | Loss: 0.00001315
Iteration 167/1000 | Loss: 0.00001315
Iteration 168/1000 | Loss: 0.00001315
Iteration 169/1000 | Loss: 0.00001315
Iteration 170/1000 | Loss: 0.00001315
Iteration 171/1000 | Loss: 0.00001315
Iteration 172/1000 | Loss: 0.00001315
Iteration 173/1000 | Loss: 0.00001315
Iteration 174/1000 | Loss: 0.00001315
Iteration 175/1000 | Loss: 0.00001315
Iteration 176/1000 | Loss: 0.00001315
Iteration 177/1000 | Loss: 0.00001315
Iteration 178/1000 | Loss: 0.00001315
Iteration 179/1000 | Loss: 0.00001315
Iteration 180/1000 | Loss: 0.00001315
Iteration 181/1000 | Loss: 0.00001315
Iteration 182/1000 | Loss: 0.00001315
Iteration 183/1000 | Loss: 0.00001315
Iteration 184/1000 | Loss: 0.00001315
Iteration 185/1000 | Loss: 0.00001315
Iteration 186/1000 | Loss: 0.00001315
Iteration 187/1000 | Loss: 0.00001315
Iteration 188/1000 | Loss: 0.00001315
Iteration 189/1000 | Loss: 0.00001315
Iteration 190/1000 | Loss: 0.00001315
Iteration 191/1000 | Loss: 0.00001315
Iteration 192/1000 | Loss: 0.00001315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.3150735867384356e-05, 1.3150735867384356e-05, 1.3150735867384356e-05, 1.3150735867384356e-05, 1.3150735867384356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3150735867384356e-05

Optimization complete. Final v2v error: 3.091585636138916 mm

Highest mean error: 3.6086440086364746 mm for frame 52

Lowest mean error: 2.8921899795532227 mm for frame 123

Saving results

Total time: 40.27948975563049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463638
Iteration 2/25 | Loss: 0.00090759
Iteration 3/25 | Loss: 0.00073688
Iteration 4/25 | Loss: 0.00071027
Iteration 5/25 | Loss: 0.00070395
Iteration 6/25 | Loss: 0.00070288
Iteration 7/25 | Loss: 0.00070265
Iteration 8/25 | Loss: 0.00070265
Iteration 9/25 | Loss: 0.00070265
Iteration 10/25 | Loss: 0.00070265
Iteration 11/25 | Loss: 0.00070265
Iteration 12/25 | Loss: 0.00070265
Iteration 13/25 | Loss: 0.00070265
Iteration 14/25 | Loss: 0.00070265
Iteration 15/25 | Loss: 0.00070265
Iteration 16/25 | Loss: 0.00070265
Iteration 17/25 | Loss: 0.00070265
Iteration 18/25 | Loss: 0.00070265
Iteration 19/25 | Loss: 0.00070265
Iteration 20/25 | Loss: 0.00070265
Iteration 21/25 | Loss: 0.00070265
Iteration 22/25 | Loss: 0.00070265
Iteration 23/25 | Loss: 0.00070265
Iteration 24/25 | Loss: 0.00070265
Iteration 25/25 | Loss: 0.00070265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.16624689
Iteration 2/25 | Loss: 0.00031098
Iteration 3/25 | Loss: 0.00031098
Iteration 4/25 | Loss: 0.00031098
Iteration 5/25 | Loss: 0.00031098
Iteration 6/25 | Loss: 0.00031098
Iteration 7/25 | Loss: 0.00031098
Iteration 8/25 | Loss: 0.00031098
Iteration 9/25 | Loss: 0.00031098
Iteration 10/25 | Loss: 0.00031098
Iteration 11/25 | Loss: 0.00031098
Iteration 12/25 | Loss: 0.00031098
Iteration 13/25 | Loss: 0.00031098
Iteration 14/25 | Loss: 0.00031098
Iteration 15/25 | Loss: 0.00031098
Iteration 16/25 | Loss: 0.00031098
Iteration 17/25 | Loss: 0.00031098
Iteration 18/25 | Loss: 0.00031098
Iteration 19/25 | Loss: 0.00031098
Iteration 20/25 | Loss: 0.00031098
Iteration 21/25 | Loss: 0.00031098
Iteration 22/25 | Loss: 0.00031098
Iteration 23/25 | Loss: 0.00031098
Iteration 24/25 | Loss: 0.00031098
Iteration 25/25 | Loss: 0.00031098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031098
Iteration 2/1000 | Loss: 0.00002330
Iteration 3/1000 | Loss: 0.00001885
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001687
Iteration 6/1000 | Loss: 0.00001633
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001566
Iteration 9/1000 | Loss: 0.00001554
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001535
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001524
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001515
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001514
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001513
Iteration 27/1000 | Loss: 0.00001512
Iteration 28/1000 | Loss: 0.00001511
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00001507
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001499
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001495
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001492
Iteration 64/1000 | Loss: 0.00001492
Iteration 65/1000 | Loss: 0.00001491
Iteration 66/1000 | Loss: 0.00001491
Iteration 67/1000 | Loss: 0.00001490
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001486
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001485
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001484
Iteration 85/1000 | Loss: 0.00001484
Iteration 86/1000 | Loss: 0.00001484
Iteration 87/1000 | Loss: 0.00001484
Iteration 88/1000 | Loss: 0.00001484
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001484
Iteration 93/1000 | Loss: 0.00001484
Iteration 94/1000 | Loss: 0.00001484
Iteration 95/1000 | Loss: 0.00001484
Iteration 96/1000 | Loss: 0.00001484
Iteration 97/1000 | Loss: 0.00001484
Iteration 98/1000 | Loss: 0.00001484
Iteration 99/1000 | Loss: 0.00001484
Iteration 100/1000 | Loss: 0.00001484
Iteration 101/1000 | Loss: 0.00001484
Iteration 102/1000 | Loss: 0.00001484
Iteration 103/1000 | Loss: 0.00001484
Iteration 104/1000 | Loss: 0.00001484
Iteration 105/1000 | Loss: 0.00001484
Iteration 106/1000 | Loss: 0.00001484
Iteration 107/1000 | Loss: 0.00001484
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001484
Iteration 111/1000 | Loss: 0.00001484
Iteration 112/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.4838386960036587e-05, 1.4838386960036587e-05, 1.4838386960036587e-05, 1.4838386960036587e-05, 1.4838386960036587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4838386960036587e-05

Optimization complete. Final v2v error: 3.2443861961364746 mm

Highest mean error: 3.5867857933044434 mm for frame 88

Lowest mean error: 2.953087568283081 mm for frame 15

Saving results

Total time: 34.0579628944397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846328
Iteration 2/25 | Loss: 0.00106175
Iteration 3/25 | Loss: 0.00077890
Iteration 4/25 | Loss: 0.00071705
Iteration 5/25 | Loss: 0.00070963
Iteration 6/25 | Loss: 0.00070907
Iteration 7/25 | Loss: 0.00070907
Iteration 8/25 | Loss: 0.00070907
Iteration 9/25 | Loss: 0.00070907
Iteration 10/25 | Loss: 0.00070907
Iteration 11/25 | Loss: 0.00070907
Iteration 12/25 | Loss: 0.00070907
Iteration 13/25 | Loss: 0.00070907
Iteration 14/25 | Loss: 0.00070907
Iteration 15/25 | Loss: 0.00070907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007090712897479534, 0.0007090712897479534, 0.0007090712897479534, 0.0007090712897479534, 0.0007090712897479534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007090712897479534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43672991
Iteration 2/25 | Loss: 0.00031796
Iteration 3/25 | Loss: 0.00031794
Iteration 4/25 | Loss: 0.00031794
Iteration 5/25 | Loss: 0.00031794
Iteration 6/25 | Loss: 0.00031794
Iteration 7/25 | Loss: 0.00031794
Iteration 8/25 | Loss: 0.00031794
Iteration 9/25 | Loss: 0.00031794
Iteration 10/25 | Loss: 0.00031794
Iteration 11/25 | Loss: 0.00031794
Iteration 12/25 | Loss: 0.00031793
Iteration 13/25 | Loss: 0.00031793
Iteration 14/25 | Loss: 0.00031793
Iteration 15/25 | Loss: 0.00031793
Iteration 16/25 | Loss: 0.00031793
Iteration 17/25 | Loss: 0.00031793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003179349296260625, 0.0003179349296260625, 0.0003179349296260625, 0.0003179349296260625, 0.0003179349296260625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003179349296260625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031793
Iteration 2/1000 | Loss: 0.00002799
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001835
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001617
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001558
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001533
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001507
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001503
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001496
Iteration 24/1000 | Loss: 0.00001493
Iteration 25/1000 | Loss: 0.00001492
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001488
Iteration 28/1000 | Loss: 0.00001488
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001487
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001482
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001480
Iteration 40/1000 | Loss: 0.00001480
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001480
Iteration 43/1000 | Loss: 0.00001480
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001479
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001478
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001477
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001474
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001474
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001471
Iteration 82/1000 | Loss: 0.00001471
Iteration 83/1000 | Loss: 0.00001471
Iteration 84/1000 | Loss: 0.00001470
Iteration 85/1000 | Loss: 0.00001470
Iteration 86/1000 | Loss: 0.00001470
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001469
Iteration 91/1000 | Loss: 0.00001468
Iteration 92/1000 | Loss: 0.00001468
Iteration 93/1000 | Loss: 0.00001468
Iteration 94/1000 | Loss: 0.00001468
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001468
Iteration 97/1000 | Loss: 0.00001468
Iteration 98/1000 | Loss: 0.00001468
Iteration 99/1000 | Loss: 0.00001468
Iteration 100/1000 | Loss: 0.00001467
Iteration 101/1000 | Loss: 0.00001467
Iteration 102/1000 | Loss: 0.00001467
Iteration 103/1000 | Loss: 0.00001467
Iteration 104/1000 | Loss: 0.00001467
Iteration 105/1000 | Loss: 0.00001467
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001467
Iteration 108/1000 | Loss: 0.00001467
Iteration 109/1000 | Loss: 0.00001467
Iteration 110/1000 | Loss: 0.00001466
Iteration 111/1000 | Loss: 0.00001466
Iteration 112/1000 | Loss: 0.00001466
Iteration 113/1000 | Loss: 0.00001466
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001466
Iteration 116/1000 | Loss: 0.00001466
Iteration 117/1000 | Loss: 0.00001466
Iteration 118/1000 | Loss: 0.00001466
Iteration 119/1000 | Loss: 0.00001466
Iteration 120/1000 | Loss: 0.00001466
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001466
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001465
Iteration 126/1000 | Loss: 0.00001465
Iteration 127/1000 | Loss: 0.00001465
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001465
Iteration 130/1000 | Loss: 0.00001465
Iteration 131/1000 | Loss: 0.00001465
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Iteration 138/1000 | Loss: 0.00001465
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001465
Iteration 141/1000 | Loss: 0.00001465
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001465
Iteration 144/1000 | Loss: 0.00001464
Iteration 145/1000 | Loss: 0.00001464
Iteration 146/1000 | Loss: 0.00001464
Iteration 147/1000 | Loss: 0.00001464
Iteration 148/1000 | Loss: 0.00001464
Iteration 149/1000 | Loss: 0.00001464
Iteration 150/1000 | Loss: 0.00001464
Iteration 151/1000 | Loss: 0.00001464
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001464
Iteration 154/1000 | Loss: 0.00001464
Iteration 155/1000 | Loss: 0.00001464
Iteration 156/1000 | Loss: 0.00001464
Iteration 157/1000 | Loss: 0.00001464
Iteration 158/1000 | Loss: 0.00001464
Iteration 159/1000 | Loss: 0.00001464
Iteration 160/1000 | Loss: 0.00001464
Iteration 161/1000 | Loss: 0.00001464
Iteration 162/1000 | Loss: 0.00001464
Iteration 163/1000 | Loss: 0.00001464
Iteration 164/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.4640433619206306e-05, 1.4640433619206306e-05, 1.4640433619206306e-05, 1.4640433619206306e-05, 1.4640433619206306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4640433619206306e-05

Optimization complete. Final v2v error: 3.2424585819244385 mm

Highest mean error: 4.160731315612793 mm for frame 2

Lowest mean error: 2.9696671962738037 mm for frame 168

Saving results

Total time: 37.699026346206665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855680
Iteration 2/25 | Loss: 0.00144091
Iteration 3/25 | Loss: 0.00085252
Iteration 4/25 | Loss: 0.00078246
Iteration 5/25 | Loss: 0.00074471
Iteration 6/25 | Loss: 0.00072068
Iteration 7/25 | Loss: 0.00071545
Iteration 8/25 | Loss: 0.00070567
Iteration 9/25 | Loss: 0.00070303
Iteration 10/25 | Loss: 0.00070036
Iteration 11/25 | Loss: 0.00069972
Iteration 12/25 | Loss: 0.00069548
Iteration 13/25 | Loss: 0.00069367
Iteration 14/25 | Loss: 0.00069281
Iteration 15/25 | Loss: 0.00069245
Iteration 16/25 | Loss: 0.00069236
Iteration 17/25 | Loss: 0.00069234
Iteration 18/25 | Loss: 0.00069233
Iteration 19/25 | Loss: 0.00069233
Iteration 20/25 | Loss: 0.00069233
Iteration 21/25 | Loss: 0.00069233
Iteration 22/25 | Loss: 0.00069233
Iteration 23/25 | Loss: 0.00069233
Iteration 24/25 | Loss: 0.00069233
Iteration 25/25 | Loss: 0.00069233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00534916
Iteration 2/25 | Loss: 0.00036115
Iteration 3/25 | Loss: 0.00036113
Iteration 4/25 | Loss: 0.00036113
Iteration 5/25 | Loss: 0.00036113
Iteration 6/25 | Loss: 0.00036113
Iteration 7/25 | Loss: 0.00036113
Iteration 8/25 | Loss: 0.00036113
Iteration 9/25 | Loss: 0.00036113
Iteration 10/25 | Loss: 0.00036113
Iteration 11/25 | Loss: 0.00036113
Iteration 12/25 | Loss: 0.00036113
Iteration 13/25 | Loss: 0.00036113
Iteration 14/25 | Loss: 0.00036113
Iteration 15/25 | Loss: 0.00036113
Iteration 16/25 | Loss: 0.00036113
Iteration 17/25 | Loss: 0.00036113
Iteration 18/25 | Loss: 0.00036113
Iteration 19/25 | Loss: 0.00036113
Iteration 20/25 | Loss: 0.00036113
Iteration 21/25 | Loss: 0.00036113
Iteration 22/25 | Loss: 0.00036113
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00036112850648351014, 0.00036112850648351014, 0.00036112850648351014, 0.00036112850648351014, 0.00036112850648351014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036112850648351014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036113
Iteration 2/1000 | Loss: 0.00002726
Iteration 3/1000 | Loss: 0.00001785
Iteration 4/1000 | Loss: 0.00001654
Iteration 5/1000 | Loss: 0.00001571
Iteration 6/1000 | Loss: 0.00001533
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001490
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001484
Iteration 11/1000 | Loss: 0.00001483
Iteration 12/1000 | Loss: 0.00001474
Iteration 13/1000 | Loss: 0.00001472
Iteration 14/1000 | Loss: 0.00001460
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001447
Iteration 19/1000 | Loss: 0.00001443
Iteration 20/1000 | Loss: 0.00001443
Iteration 21/1000 | Loss: 0.00001442
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001439
Iteration 26/1000 | Loss: 0.00001436
Iteration 27/1000 | Loss: 0.00001430
Iteration 28/1000 | Loss: 0.00001430
Iteration 29/1000 | Loss: 0.00001429
Iteration 30/1000 | Loss: 0.00001428
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001424
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001423
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001422
Iteration 37/1000 | Loss: 0.00001422
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001419
Iteration 40/1000 | Loss: 0.00001419
Iteration 41/1000 | Loss: 0.00001419
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001416
Iteration 48/1000 | Loss: 0.00001416
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001415
Iteration 52/1000 | Loss: 0.00001415
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001415
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001414
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001413
Iteration 61/1000 | Loss: 0.00001413
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001412
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.4115597878117114e-05, 1.4115597878117114e-05, 1.4115597878117114e-05, 1.4115597878117114e-05, 1.4115597878117114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4115597878117114e-05

Optimization complete. Final v2v error: 3.174851417541504 mm

Highest mean error: 3.6957783699035645 mm for frame 222

Lowest mean error: 2.7965550422668457 mm for frame 37

Saving results

Total time: 57.2298150062561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00342964
Iteration 2/25 | Loss: 0.00090055
Iteration 3/25 | Loss: 0.00076456
Iteration 4/25 | Loss: 0.00072719
Iteration 5/25 | Loss: 0.00071815
Iteration 6/25 | Loss: 0.00071628
Iteration 7/25 | Loss: 0.00071564
Iteration 8/25 | Loss: 0.00071564
Iteration 9/25 | Loss: 0.00071564
Iteration 10/25 | Loss: 0.00071564
Iteration 11/25 | Loss: 0.00071564
Iteration 12/25 | Loss: 0.00071564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007156424107961357, 0.0007156424107961357, 0.0007156424107961357, 0.0007156424107961357, 0.0007156424107961357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007156424107961357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46917343
Iteration 2/25 | Loss: 0.00039832
Iteration 3/25 | Loss: 0.00039832
Iteration 4/25 | Loss: 0.00039832
Iteration 5/25 | Loss: 0.00039832
Iteration 6/25 | Loss: 0.00039832
Iteration 7/25 | Loss: 0.00039832
Iteration 8/25 | Loss: 0.00039832
Iteration 9/25 | Loss: 0.00039832
Iteration 10/25 | Loss: 0.00039832
Iteration 11/25 | Loss: 0.00039832
Iteration 12/25 | Loss: 0.00039832
Iteration 13/25 | Loss: 0.00039832
Iteration 14/25 | Loss: 0.00039832
Iteration 15/25 | Loss: 0.00039832
Iteration 16/25 | Loss: 0.00039832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00039831639151088893, 0.00039831639151088893, 0.00039831639151088893, 0.00039831639151088893, 0.00039831639151088893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00039831639151088893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039832
Iteration 2/1000 | Loss: 0.00005125
Iteration 3/1000 | Loss: 0.00003534
Iteration 4/1000 | Loss: 0.00002857
Iteration 5/1000 | Loss: 0.00002611
Iteration 6/1000 | Loss: 0.00002447
Iteration 7/1000 | Loss: 0.00002349
Iteration 8/1000 | Loss: 0.00002296
Iteration 9/1000 | Loss: 0.00002239
Iteration 10/1000 | Loss: 0.00002200
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002150
Iteration 13/1000 | Loss: 0.00002134
Iteration 14/1000 | Loss: 0.00002123
Iteration 15/1000 | Loss: 0.00002117
Iteration 16/1000 | Loss: 0.00002109
Iteration 17/1000 | Loss: 0.00002109
Iteration 18/1000 | Loss: 0.00002108
Iteration 19/1000 | Loss: 0.00002108
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002104
Iteration 23/1000 | Loss: 0.00002103
Iteration 24/1000 | Loss: 0.00002103
Iteration 25/1000 | Loss: 0.00002102
Iteration 26/1000 | Loss: 0.00002099
Iteration 27/1000 | Loss: 0.00002093
Iteration 28/1000 | Loss: 0.00002092
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002089
Iteration 31/1000 | Loss: 0.00002089
Iteration 32/1000 | Loss: 0.00002087
Iteration 33/1000 | Loss: 0.00002087
Iteration 34/1000 | Loss: 0.00002087
Iteration 35/1000 | Loss: 0.00002086
Iteration 36/1000 | Loss: 0.00002086
Iteration 37/1000 | Loss: 0.00002085
Iteration 38/1000 | Loss: 0.00002085
Iteration 39/1000 | Loss: 0.00002085
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002084
Iteration 43/1000 | Loss: 0.00002083
Iteration 44/1000 | Loss: 0.00002083
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002081
Iteration 47/1000 | Loss: 0.00002081
Iteration 48/1000 | Loss: 0.00002081
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002078
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002078
Iteration 55/1000 | Loss: 0.00002077
Iteration 56/1000 | Loss: 0.00002076
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002074
Iteration 63/1000 | Loss: 0.00002073
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002072
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002070
Iteration 71/1000 | Loss: 0.00002070
Iteration 72/1000 | Loss: 0.00002070
Iteration 73/1000 | Loss: 0.00002069
Iteration 74/1000 | Loss: 0.00002069
Iteration 75/1000 | Loss: 0.00002069
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002068
Iteration 79/1000 | Loss: 0.00002067
Iteration 80/1000 | Loss: 0.00002067
Iteration 81/1000 | Loss: 0.00002067
Iteration 82/1000 | Loss: 0.00002066
Iteration 83/1000 | Loss: 0.00002066
Iteration 84/1000 | Loss: 0.00002066
Iteration 85/1000 | Loss: 0.00002066
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002065
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002064
Iteration 91/1000 | Loss: 0.00002064
Iteration 92/1000 | Loss: 0.00002064
Iteration 93/1000 | Loss: 0.00002064
Iteration 94/1000 | Loss: 0.00002063
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002063
Iteration 98/1000 | Loss: 0.00002063
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002063
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002062
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002062
Iteration 110/1000 | Loss: 0.00002062
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002061
Iteration 114/1000 | Loss: 0.00002061
Iteration 115/1000 | Loss: 0.00002061
Iteration 116/1000 | Loss: 0.00002061
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002060
Iteration 120/1000 | Loss: 0.00002060
Iteration 121/1000 | Loss: 0.00002060
Iteration 122/1000 | Loss: 0.00002060
Iteration 123/1000 | Loss: 0.00002060
Iteration 124/1000 | Loss: 0.00002060
Iteration 125/1000 | Loss: 0.00002060
Iteration 126/1000 | Loss: 0.00002060
Iteration 127/1000 | Loss: 0.00002060
Iteration 128/1000 | Loss: 0.00002060
Iteration 129/1000 | Loss: 0.00002060
Iteration 130/1000 | Loss: 0.00002060
Iteration 131/1000 | Loss: 0.00002060
Iteration 132/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.0596085960278288e-05, 2.0596085960278288e-05, 2.0596085960278288e-05, 2.0596085960278288e-05, 2.0596085960278288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0596085960278288e-05

Optimization complete. Final v2v error: 3.7515220642089844 mm

Highest mean error: 4.347979545593262 mm for frame 189

Lowest mean error: 3.3507261276245117 mm for frame 74

Saving results

Total time: 48.96925926208496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886994
Iteration 2/25 | Loss: 0.00128315
Iteration 3/25 | Loss: 0.00085917
Iteration 4/25 | Loss: 0.00079291
Iteration 5/25 | Loss: 0.00076875
Iteration 6/25 | Loss: 0.00076166
Iteration 7/25 | Loss: 0.00075927
Iteration 8/25 | Loss: 0.00075802
Iteration 9/25 | Loss: 0.00075764
Iteration 10/25 | Loss: 0.00075763
Iteration 11/25 | Loss: 0.00075763
Iteration 12/25 | Loss: 0.00075763
Iteration 13/25 | Loss: 0.00075763
Iteration 14/25 | Loss: 0.00075763
Iteration 15/25 | Loss: 0.00075763
Iteration 16/25 | Loss: 0.00075763
Iteration 17/25 | Loss: 0.00075763
Iteration 18/25 | Loss: 0.00075763
Iteration 19/25 | Loss: 0.00075763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007576288771815598, 0.0007576288771815598, 0.0007576288771815598, 0.0007576288771815598, 0.0007576288771815598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007576288771815598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29508138
Iteration 2/25 | Loss: 0.00033420
Iteration 3/25 | Loss: 0.00033420
Iteration 4/25 | Loss: 0.00033420
Iteration 5/25 | Loss: 0.00033420
Iteration 6/25 | Loss: 0.00033420
Iteration 7/25 | Loss: 0.00033420
Iteration 8/25 | Loss: 0.00033420
Iteration 9/25 | Loss: 0.00033420
Iteration 10/25 | Loss: 0.00033420
Iteration 11/25 | Loss: 0.00033420
Iteration 12/25 | Loss: 0.00033420
Iteration 13/25 | Loss: 0.00033420
Iteration 14/25 | Loss: 0.00033420
Iteration 15/25 | Loss: 0.00033420
Iteration 16/25 | Loss: 0.00033420
Iteration 17/25 | Loss: 0.00033420
Iteration 18/25 | Loss: 0.00033420
Iteration 19/25 | Loss: 0.00033420
Iteration 20/25 | Loss: 0.00033420
Iteration 21/25 | Loss: 0.00033420
Iteration 22/25 | Loss: 0.00033420
Iteration 23/25 | Loss: 0.00033420
Iteration 24/25 | Loss: 0.00033420
Iteration 25/25 | Loss: 0.00033420

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033420
Iteration 2/1000 | Loss: 0.00004794
Iteration 3/1000 | Loss: 0.00003275
Iteration 4/1000 | Loss: 0.00002683
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002425
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002311
Iteration 9/1000 | Loss: 0.00002267
Iteration 10/1000 | Loss: 0.00002231
Iteration 11/1000 | Loss: 0.00002205
Iteration 12/1000 | Loss: 0.00002185
Iteration 13/1000 | Loss: 0.00002174
Iteration 14/1000 | Loss: 0.00002167
Iteration 15/1000 | Loss: 0.00002151
Iteration 16/1000 | Loss: 0.00002148
Iteration 17/1000 | Loss: 0.00002136
Iteration 18/1000 | Loss: 0.00002135
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002131
Iteration 22/1000 | Loss: 0.00002130
Iteration 23/1000 | Loss: 0.00002130
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002129
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002128
Iteration 28/1000 | Loss: 0.00002128
Iteration 29/1000 | Loss: 0.00002127
Iteration 30/1000 | Loss: 0.00002126
Iteration 31/1000 | Loss: 0.00002126
Iteration 32/1000 | Loss: 0.00002124
Iteration 33/1000 | Loss: 0.00002124
Iteration 34/1000 | Loss: 0.00002124
Iteration 35/1000 | Loss: 0.00002124
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002123
Iteration 39/1000 | Loss: 0.00002123
Iteration 40/1000 | Loss: 0.00002123
Iteration 41/1000 | Loss: 0.00002123
Iteration 42/1000 | Loss: 0.00002123
Iteration 43/1000 | Loss: 0.00002123
Iteration 44/1000 | Loss: 0.00002121
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002120
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002119
Iteration 55/1000 | Loss: 0.00002119
Iteration 56/1000 | Loss: 0.00002119
Iteration 57/1000 | Loss: 0.00002119
Iteration 58/1000 | Loss: 0.00002118
Iteration 59/1000 | Loss: 0.00002118
Iteration 60/1000 | Loss: 0.00002118
Iteration 61/1000 | Loss: 0.00002118
Iteration 62/1000 | Loss: 0.00002118
Iteration 63/1000 | Loss: 0.00002117
Iteration 64/1000 | Loss: 0.00002117
Iteration 65/1000 | Loss: 0.00002117
Iteration 66/1000 | Loss: 0.00002117
Iteration 67/1000 | Loss: 0.00002117
Iteration 68/1000 | Loss: 0.00002117
Iteration 69/1000 | Loss: 0.00002117
Iteration 70/1000 | Loss: 0.00002117
Iteration 71/1000 | Loss: 0.00002117
Iteration 72/1000 | Loss: 0.00002117
Iteration 73/1000 | Loss: 0.00002117
Iteration 74/1000 | Loss: 0.00002117
Iteration 75/1000 | Loss: 0.00002117
Iteration 76/1000 | Loss: 0.00002116
Iteration 77/1000 | Loss: 0.00002116
Iteration 78/1000 | Loss: 0.00002116
Iteration 79/1000 | Loss: 0.00002116
Iteration 80/1000 | Loss: 0.00002116
Iteration 81/1000 | Loss: 0.00002115
Iteration 82/1000 | Loss: 0.00002115
Iteration 83/1000 | Loss: 0.00002115
Iteration 84/1000 | Loss: 0.00002114
Iteration 85/1000 | Loss: 0.00002114
Iteration 86/1000 | Loss: 0.00002114
Iteration 87/1000 | Loss: 0.00002114
Iteration 88/1000 | Loss: 0.00002114
Iteration 89/1000 | Loss: 0.00002113
Iteration 90/1000 | Loss: 0.00002113
Iteration 91/1000 | Loss: 0.00002113
Iteration 92/1000 | Loss: 0.00002113
Iteration 93/1000 | Loss: 0.00002112
Iteration 94/1000 | Loss: 0.00002112
Iteration 95/1000 | Loss: 0.00002112
Iteration 96/1000 | Loss: 0.00002111
Iteration 97/1000 | Loss: 0.00002111
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002110
Iteration 101/1000 | Loss: 0.00002110
Iteration 102/1000 | Loss: 0.00002110
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002110
Iteration 106/1000 | Loss: 0.00002109
Iteration 107/1000 | Loss: 0.00002109
Iteration 108/1000 | Loss: 0.00002109
Iteration 109/1000 | Loss: 0.00002109
Iteration 110/1000 | Loss: 0.00002108
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002108
Iteration 113/1000 | Loss: 0.00002108
Iteration 114/1000 | Loss: 0.00002107
Iteration 115/1000 | Loss: 0.00002107
Iteration 116/1000 | Loss: 0.00002107
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002106
Iteration 119/1000 | Loss: 0.00002106
Iteration 120/1000 | Loss: 0.00002106
Iteration 121/1000 | Loss: 0.00002105
Iteration 122/1000 | Loss: 0.00002105
Iteration 123/1000 | Loss: 0.00002105
Iteration 124/1000 | Loss: 0.00002105
Iteration 125/1000 | Loss: 0.00002105
Iteration 126/1000 | Loss: 0.00002104
Iteration 127/1000 | Loss: 0.00002104
Iteration 128/1000 | Loss: 0.00002104
Iteration 129/1000 | Loss: 0.00002104
Iteration 130/1000 | Loss: 0.00002103
Iteration 131/1000 | Loss: 0.00002103
Iteration 132/1000 | Loss: 0.00002103
Iteration 133/1000 | Loss: 0.00002103
Iteration 134/1000 | Loss: 0.00002103
Iteration 135/1000 | Loss: 0.00002102
Iteration 136/1000 | Loss: 0.00002102
Iteration 137/1000 | Loss: 0.00002102
Iteration 138/1000 | Loss: 0.00002102
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002102
Iteration 143/1000 | Loss: 0.00002101
Iteration 144/1000 | Loss: 0.00002101
Iteration 145/1000 | Loss: 0.00002101
Iteration 146/1000 | Loss: 0.00002101
Iteration 147/1000 | Loss: 0.00002101
Iteration 148/1000 | Loss: 0.00002100
Iteration 149/1000 | Loss: 0.00002100
Iteration 150/1000 | Loss: 0.00002100
Iteration 151/1000 | Loss: 0.00002100
Iteration 152/1000 | Loss: 0.00002100
Iteration 153/1000 | Loss: 0.00002100
Iteration 154/1000 | Loss: 0.00002099
Iteration 155/1000 | Loss: 0.00002099
Iteration 156/1000 | Loss: 0.00002099
Iteration 157/1000 | Loss: 0.00002099
Iteration 158/1000 | Loss: 0.00002099
Iteration 159/1000 | Loss: 0.00002099
Iteration 160/1000 | Loss: 0.00002099
Iteration 161/1000 | Loss: 0.00002099
Iteration 162/1000 | Loss: 0.00002098
Iteration 163/1000 | Loss: 0.00002098
Iteration 164/1000 | Loss: 0.00002098
Iteration 165/1000 | Loss: 0.00002097
Iteration 166/1000 | Loss: 0.00002097
Iteration 167/1000 | Loss: 0.00002097
Iteration 168/1000 | Loss: 0.00002097
Iteration 169/1000 | Loss: 0.00002097
Iteration 170/1000 | Loss: 0.00002097
Iteration 171/1000 | Loss: 0.00002097
Iteration 172/1000 | Loss: 0.00002097
Iteration 173/1000 | Loss: 0.00002097
Iteration 174/1000 | Loss: 0.00002096
Iteration 175/1000 | Loss: 0.00002096
Iteration 176/1000 | Loss: 0.00002096
Iteration 177/1000 | Loss: 0.00002096
Iteration 178/1000 | Loss: 0.00002096
Iteration 179/1000 | Loss: 0.00002096
Iteration 180/1000 | Loss: 0.00002096
Iteration 181/1000 | Loss: 0.00002096
Iteration 182/1000 | Loss: 0.00002095
Iteration 183/1000 | Loss: 0.00002095
Iteration 184/1000 | Loss: 0.00002095
Iteration 185/1000 | Loss: 0.00002095
Iteration 186/1000 | Loss: 0.00002095
Iteration 187/1000 | Loss: 0.00002095
Iteration 188/1000 | Loss: 0.00002095
Iteration 189/1000 | Loss: 0.00002095
Iteration 190/1000 | Loss: 0.00002095
Iteration 191/1000 | Loss: 0.00002095
Iteration 192/1000 | Loss: 0.00002095
Iteration 193/1000 | Loss: 0.00002094
Iteration 194/1000 | Loss: 0.00002094
Iteration 195/1000 | Loss: 0.00002094
Iteration 196/1000 | Loss: 0.00002094
Iteration 197/1000 | Loss: 0.00002094
Iteration 198/1000 | Loss: 0.00002094
Iteration 199/1000 | Loss: 0.00002094
Iteration 200/1000 | Loss: 0.00002094
Iteration 201/1000 | Loss: 0.00002093
Iteration 202/1000 | Loss: 0.00002093
Iteration 203/1000 | Loss: 0.00002093
Iteration 204/1000 | Loss: 0.00002093
Iteration 205/1000 | Loss: 0.00002093
Iteration 206/1000 | Loss: 0.00002093
Iteration 207/1000 | Loss: 0.00002093
Iteration 208/1000 | Loss: 0.00002093
Iteration 209/1000 | Loss: 0.00002093
Iteration 210/1000 | Loss: 0.00002093
Iteration 211/1000 | Loss: 0.00002093
Iteration 212/1000 | Loss: 0.00002092
Iteration 213/1000 | Loss: 0.00002092
Iteration 214/1000 | Loss: 0.00002092
Iteration 215/1000 | Loss: 0.00002092
Iteration 216/1000 | Loss: 0.00002092
Iteration 217/1000 | Loss: 0.00002092
Iteration 218/1000 | Loss: 0.00002092
Iteration 219/1000 | Loss: 0.00002092
Iteration 220/1000 | Loss: 0.00002092
Iteration 221/1000 | Loss: 0.00002092
Iteration 222/1000 | Loss: 0.00002092
Iteration 223/1000 | Loss: 0.00002092
Iteration 224/1000 | Loss: 0.00002092
Iteration 225/1000 | Loss: 0.00002091
Iteration 226/1000 | Loss: 0.00002091
Iteration 227/1000 | Loss: 0.00002091
Iteration 228/1000 | Loss: 0.00002091
Iteration 229/1000 | Loss: 0.00002091
Iteration 230/1000 | Loss: 0.00002091
Iteration 231/1000 | Loss: 0.00002091
Iteration 232/1000 | Loss: 0.00002091
Iteration 233/1000 | Loss: 0.00002091
Iteration 234/1000 | Loss: 0.00002091
Iteration 235/1000 | Loss: 0.00002091
Iteration 236/1000 | Loss: 0.00002091
Iteration 237/1000 | Loss: 0.00002090
Iteration 238/1000 | Loss: 0.00002090
Iteration 239/1000 | Loss: 0.00002090
Iteration 240/1000 | Loss: 0.00002090
Iteration 241/1000 | Loss: 0.00002090
Iteration 242/1000 | Loss: 0.00002090
Iteration 243/1000 | Loss: 0.00002090
Iteration 244/1000 | Loss: 0.00002090
Iteration 245/1000 | Loss: 0.00002089
Iteration 246/1000 | Loss: 0.00002089
Iteration 247/1000 | Loss: 0.00002089
Iteration 248/1000 | Loss: 0.00002089
Iteration 249/1000 | Loss: 0.00002089
Iteration 250/1000 | Loss: 0.00002089
Iteration 251/1000 | Loss: 0.00002089
Iteration 252/1000 | Loss: 0.00002089
Iteration 253/1000 | Loss: 0.00002089
Iteration 254/1000 | Loss: 0.00002089
Iteration 255/1000 | Loss: 0.00002089
Iteration 256/1000 | Loss: 0.00002089
Iteration 257/1000 | Loss: 0.00002089
Iteration 258/1000 | Loss: 0.00002089
Iteration 259/1000 | Loss: 0.00002089
Iteration 260/1000 | Loss: 0.00002089
Iteration 261/1000 | Loss: 0.00002089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.089267763949465e-05, 2.089267763949465e-05, 2.089267763949465e-05, 2.089267763949465e-05, 2.089267763949465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.089267763949465e-05

Optimization complete. Final v2v error: 3.734797716140747 mm

Highest mean error: 6.160186767578125 mm for frame 90

Lowest mean error: 2.708178758621216 mm for frame 14

Saving results

Total time: 53.022066593170166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825259
Iteration 2/25 | Loss: 0.00102397
Iteration 3/25 | Loss: 0.00075446
Iteration 4/25 | Loss: 0.00070488
Iteration 5/25 | Loss: 0.00069108
Iteration 6/25 | Loss: 0.00068776
Iteration 7/25 | Loss: 0.00068740
Iteration 8/25 | Loss: 0.00068740
Iteration 9/25 | Loss: 0.00068740
Iteration 10/25 | Loss: 0.00068740
Iteration 11/25 | Loss: 0.00068740
Iteration 12/25 | Loss: 0.00068740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006873951060697436, 0.0006873951060697436, 0.0006873951060697436, 0.0006873951060697436, 0.0006873951060697436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006873951060697436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44265079
Iteration 2/25 | Loss: 0.00039963
Iteration 3/25 | Loss: 0.00039958
Iteration 4/25 | Loss: 0.00039958
Iteration 5/25 | Loss: 0.00039958
Iteration 6/25 | Loss: 0.00039958
Iteration 7/25 | Loss: 0.00039958
Iteration 8/25 | Loss: 0.00039958
Iteration 9/25 | Loss: 0.00039958
Iteration 10/25 | Loss: 0.00039958
Iteration 11/25 | Loss: 0.00039958
Iteration 12/25 | Loss: 0.00039958
Iteration 13/25 | Loss: 0.00039958
Iteration 14/25 | Loss: 0.00039958
Iteration 15/25 | Loss: 0.00039958
Iteration 16/25 | Loss: 0.00039958
Iteration 17/25 | Loss: 0.00039958
Iteration 18/25 | Loss: 0.00039958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003995760635007173, 0.0003995760635007173, 0.0003995760635007173, 0.0003995760635007173, 0.0003995760635007173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003995760635007173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039958
Iteration 2/1000 | Loss: 0.00003766
Iteration 3/1000 | Loss: 0.00002551
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001889
Iteration 8/1000 | Loss: 0.00001849
Iteration 9/1000 | Loss: 0.00001812
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001765
Iteration 14/1000 | Loss: 0.00001764
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001752
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001721
Iteration 21/1000 | Loss: 0.00001720
Iteration 22/1000 | Loss: 0.00001720
Iteration 23/1000 | Loss: 0.00001719
Iteration 24/1000 | Loss: 0.00001716
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001715
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001714
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001707
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00001703
Iteration 49/1000 | Loss: 0.00001701
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001701
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001698
Iteration 55/1000 | Loss: 0.00001698
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001698
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001693
Iteration 74/1000 | Loss: 0.00001693
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001692
Iteration 79/1000 | Loss: 0.00001692
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001690
Iteration 87/1000 | Loss: 0.00001690
Iteration 88/1000 | Loss: 0.00001690
Iteration 89/1000 | Loss: 0.00001689
Iteration 90/1000 | Loss: 0.00001689
Iteration 91/1000 | Loss: 0.00001689
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001689
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001688
Iteration 100/1000 | Loss: 0.00001688
Iteration 101/1000 | Loss: 0.00001688
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001687
Iteration 106/1000 | Loss: 0.00001687
Iteration 107/1000 | Loss: 0.00001687
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001686
Iteration 122/1000 | Loss: 0.00001686
Iteration 123/1000 | Loss: 0.00001686
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.6856125512276776e-05, 1.6856125512276776e-05, 1.6856125512276776e-05, 1.6856125512276776e-05, 1.6856125512276776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6856125512276776e-05

Optimization complete. Final v2v error: 3.440688133239746 mm

Highest mean error: 4.730546474456787 mm for frame 159

Lowest mean error: 2.8661739826202393 mm for frame 181

Saving results

Total time: 45.78622031211853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881185
Iteration 2/25 | Loss: 0.00098659
Iteration 3/25 | Loss: 0.00074415
Iteration 4/25 | Loss: 0.00071721
Iteration 5/25 | Loss: 0.00071104
Iteration 6/25 | Loss: 0.00071011
Iteration 7/25 | Loss: 0.00071011
Iteration 8/25 | Loss: 0.00071011
Iteration 9/25 | Loss: 0.00071011
Iteration 10/25 | Loss: 0.00071011
Iteration 11/25 | Loss: 0.00071011
Iteration 12/25 | Loss: 0.00071011
Iteration 13/25 | Loss: 0.00071011
Iteration 14/25 | Loss: 0.00071011
Iteration 15/25 | Loss: 0.00071011
Iteration 16/25 | Loss: 0.00071011
Iteration 17/25 | Loss: 0.00071011
Iteration 18/25 | Loss: 0.00071011
Iteration 19/25 | Loss: 0.00071011
Iteration 20/25 | Loss: 0.00071011
Iteration 21/25 | Loss: 0.00071011
Iteration 22/25 | Loss: 0.00071011
Iteration 23/25 | Loss: 0.00071011
Iteration 24/25 | Loss: 0.00071011
Iteration 25/25 | Loss: 0.00071011
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007101111114025116, 0.0007101111114025116, 0.0007101111114025116, 0.0007101111114025116, 0.0007101111114025116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007101111114025116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51014662
Iteration 2/25 | Loss: 0.00031928
Iteration 3/25 | Loss: 0.00031928
Iteration 4/25 | Loss: 0.00031928
Iteration 5/25 | Loss: 0.00031928
Iteration 6/25 | Loss: 0.00031928
Iteration 7/25 | Loss: 0.00031928
Iteration 8/25 | Loss: 0.00031928
Iteration 9/25 | Loss: 0.00031927
Iteration 10/25 | Loss: 0.00031927
Iteration 11/25 | Loss: 0.00031927
Iteration 12/25 | Loss: 0.00031927
Iteration 13/25 | Loss: 0.00031927
Iteration 14/25 | Loss: 0.00031927
Iteration 15/25 | Loss: 0.00031927
Iteration 16/25 | Loss: 0.00031927
Iteration 17/25 | Loss: 0.00031927
Iteration 18/25 | Loss: 0.00031927
Iteration 19/25 | Loss: 0.00031927
Iteration 20/25 | Loss: 0.00031927
Iteration 21/25 | Loss: 0.00031927
Iteration 22/25 | Loss: 0.00031927
Iteration 23/25 | Loss: 0.00031927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00031927457894198596, 0.00031927457894198596, 0.00031927457894198596, 0.00031927457894198596, 0.00031927457894198596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031927457894198596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031927
Iteration 2/1000 | Loss: 0.00002893
Iteration 3/1000 | Loss: 0.00002286
Iteration 4/1000 | Loss: 0.00002116
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001907
Iteration 8/1000 | Loss: 0.00001871
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00001805
Iteration 13/1000 | Loss: 0.00001802
Iteration 14/1000 | Loss: 0.00001802
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001794
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001793
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001790
Iteration 26/1000 | Loss: 0.00001788
Iteration 27/1000 | Loss: 0.00001788
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001788
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001787
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001786
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001784
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001778
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001777
Iteration 64/1000 | Loss: 0.00001777
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001776
Iteration 67/1000 | Loss: 0.00001776
Iteration 68/1000 | Loss: 0.00001776
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001775
Iteration 71/1000 | Loss: 0.00001775
Iteration 72/1000 | Loss: 0.00001775
Iteration 73/1000 | Loss: 0.00001774
Iteration 74/1000 | Loss: 0.00001774
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00001774
Iteration 77/1000 | Loss: 0.00001774
Iteration 78/1000 | Loss: 0.00001773
Iteration 79/1000 | Loss: 0.00001773
Iteration 80/1000 | Loss: 0.00001773
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001771
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001770
Iteration 92/1000 | Loss: 0.00001770
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001769
Iteration 99/1000 | Loss: 0.00001769
Iteration 100/1000 | Loss: 0.00001769
Iteration 101/1000 | Loss: 0.00001769
Iteration 102/1000 | Loss: 0.00001769
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.7691148968879133e-05, 1.7691148968879133e-05, 1.7691148968879133e-05, 1.7691148968879133e-05, 1.7691148968879133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7691148968879133e-05

Optimization complete. Final v2v error: 3.5008275508880615 mm

Highest mean error: 4.433697700500488 mm for frame 165

Lowest mean error: 3.016232967376709 mm for frame 199

Saving results

Total time: 38.46512579917908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985622
Iteration 2/25 | Loss: 0.00217134
Iteration 3/25 | Loss: 0.00150449
Iteration 4/25 | Loss: 0.00130697
Iteration 5/25 | Loss: 0.00116093
Iteration 6/25 | Loss: 0.00107348
Iteration 7/25 | Loss: 0.00104938
Iteration 8/25 | Loss: 0.00103826
Iteration 9/25 | Loss: 0.00106933
Iteration 10/25 | Loss: 0.00104698
Iteration 11/25 | Loss: 0.00103785
Iteration 12/25 | Loss: 0.00098482
Iteration 13/25 | Loss: 0.00093409
Iteration 14/25 | Loss: 0.00088712
Iteration 15/25 | Loss: 0.00086621
Iteration 16/25 | Loss: 0.00084852
Iteration 17/25 | Loss: 0.00084233
Iteration 18/25 | Loss: 0.00084081
Iteration 19/25 | Loss: 0.00084025
Iteration 20/25 | Loss: 0.00083998
Iteration 21/25 | Loss: 0.00083981
Iteration 22/25 | Loss: 0.00083979
Iteration 23/25 | Loss: 0.00083979
Iteration 24/25 | Loss: 0.00083979
Iteration 25/25 | Loss: 0.00083979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53088045
Iteration 2/25 | Loss: 0.00088750
Iteration 3/25 | Loss: 0.00088383
Iteration 4/25 | Loss: 0.00088383
Iteration 5/25 | Loss: 0.00088383
Iteration 6/25 | Loss: 0.00088382
Iteration 7/25 | Loss: 0.00088382
Iteration 8/25 | Loss: 0.00088382
Iteration 9/25 | Loss: 0.00088382
Iteration 10/25 | Loss: 0.00088382
Iteration 11/25 | Loss: 0.00088382
Iteration 12/25 | Loss: 0.00088382
Iteration 13/25 | Loss: 0.00088382
Iteration 14/25 | Loss: 0.00088382
Iteration 15/25 | Loss: 0.00088382
Iteration 16/25 | Loss: 0.00088382
Iteration 17/25 | Loss: 0.00088382
Iteration 18/25 | Loss: 0.00088382
Iteration 19/25 | Loss: 0.00088382
Iteration 20/25 | Loss: 0.00088382
Iteration 21/25 | Loss: 0.00088382
Iteration 22/25 | Loss: 0.00088382
Iteration 23/25 | Loss: 0.00088382
Iteration 24/25 | Loss: 0.00088382
Iteration 25/25 | Loss: 0.00088382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088382
Iteration 2/1000 | Loss: 0.00039298
Iteration 3/1000 | Loss: 0.00043919
Iteration 4/1000 | Loss: 0.00030424
Iteration 5/1000 | Loss: 0.00011965
Iteration 6/1000 | Loss: 0.00020735
Iteration 7/1000 | Loss: 0.00007169
Iteration 8/1000 | Loss: 0.00142874
Iteration 9/1000 | Loss: 0.00008481
Iteration 10/1000 | Loss: 0.00021643
Iteration 11/1000 | Loss: 0.00006569
Iteration 12/1000 | Loss: 0.00005040
Iteration 13/1000 | Loss: 0.00004385
Iteration 14/1000 | Loss: 0.00004010
Iteration 15/1000 | Loss: 0.00003691
Iteration 16/1000 | Loss: 0.00003543
Iteration 17/1000 | Loss: 0.00003470
Iteration 18/1000 | Loss: 0.00003382
Iteration 19/1000 | Loss: 0.00004134
Iteration 20/1000 | Loss: 0.00003304
Iteration 21/1000 | Loss: 0.00003716
Iteration 22/1000 | Loss: 0.00003229
Iteration 23/1000 | Loss: 0.00003188
Iteration 24/1000 | Loss: 0.00003146
Iteration 25/1000 | Loss: 0.00003116
Iteration 26/1000 | Loss: 0.00003088
Iteration 27/1000 | Loss: 0.00003058
Iteration 28/1000 | Loss: 0.00082384
Iteration 29/1000 | Loss: 0.00004835
Iteration 30/1000 | Loss: 0.00003446
Iteration 31/1000 | Loss: 0.00003110
Iteration 32/1000 | Loss: 0.00004372
Iteration 33/1000 | Loss: 0.00002945
Iteration 34/1000 | Loss: 0.00002889
Iteration 35/1000 | Loss: 0.00002819
Iteration 36/1000 | Loss: 0.00007396
Iteration 37/1000 | Loss: 0.00003324
Iteration 38/1000 | Loss: 0.00003759
Iteration 39/1000 | Loss: 0.00003588
Iteration 40/1000 | Loss: 0.00002750
Iteration 41/1000 | Loss: 0.00002740
Iteration 42/1000 | Loss: 0.00002738
Iteration 43/1000 | Loss: 0.00002738
Iteration 44/1000 | Loss: 0.00002737
Iteration 45/1000 | Loss: 0.00002737
Iteration 46/1000 | Loss: 0.00002728
Iteration 47/1000 | Loss: 0.00002728
Iteration 48/1000 | Loss: 0.00002726
Iteration 49/1000 | Loss: 0.00002723
Iteration 50/1000 | Loss: 0.00002723
Iteration 51/1000 | Loss: 0.00002722
Iteration 52/1000 | Loss: 0.00002720
Iteration 53/1000 | Loss: 0.00002716
Iteration 54/1000 | Loss: 0.00002709
Iteration 55/1000 | Loss: 0.00002707
Iteration 56/1000 | Loss: 0.00002707
Iteration 57/1000 | Loss: 0.00002706
Iteration 58/1000 | Loss: 0.00002706
Iteration 59/1000 | Loss: 0.00002706
Iteration 60/1000 | Loss: 0.00002706
Iteration 61/1000 | Loss: 0.00002706
Iteration 62/1000 | Loss: 0.00002705
Iteration 63/1000 | Loss: 0.00002705
Iteration 64/1000 | Loss: 0.00002705
Iteration 65/1000 | Loss: 0.00002705
Iteration 66/1000 | Loss: 0.00002704
Iteration 67/1000 | Loss: 0.00002704
Iteration 68/1000 | Loss: 0.00002704
Iteration 69/1000 | Loss: 0.00002704
Iteration 70/1000 | Loss: 0.00002704
Iteration 71/1000 | Loss: 0.00002704
Iteration 72/1000 | Loss: 0.00002704
Iteration 73/1000 | Loss: 0.00002703
Iteration 74/1000 | Loss: 0.00002703
Iteration 75/1000 | Loss: 0.00002703
Iteration 76/1000 | Loss: 0.00002703
Iteration 77/1000 | Loss: 0.00002703
Iteration 78/1000 | Loss: 0.00002703
Iteration 79/1000 | Loss: 0.00002703
Iteration 80/1000 | Loss: 0.00002703
Iteration 81/1000 | Loss: 0.00002703
Iteration 82/1000 | Loss: 0.00002702
Iteration 83/1000 | Loss: 0.00002702
Iteration 84/1000 | Loss: 0.00002702
Iteration 85/1000 | Loss: 0.00002702
Iteration 86/1000 | Loss: 0.00002702
Iteration 87/1000 | Loss: 0.00002701
Iteration 88/1000 | Loss: 0.00002701
Iteration 89/1000 | Loss: 0.00002701
Iteration 90/1000 | Loss: 0.00002701
Iteration 91/1000 | Loss: 0.00002701
Iteration 92/1000 | Loss: 0.00002701
Iteration 93/1000 | Loss: 0.00002701
Iteration 94/1000 | Loss: 0.00002701
Iteration 95/1000 | Loss: 0.00002700
Iteration 96/1000 | Loss: 0.00002699
Iteration 97/1000 | Loss: 0.00002699
Iteration 98/1000 | Loss: 0.00002699
Iteration 99/1000 | Loss: 0.00002699
Iteration 100/1000 | Loss: 0.00002699
Iteration 101/1000 | Loss: 0.00002699
Iteration 102/1000 | Loss: 0.00002699
Iteration 103/1000 | Loss: 0.00002699
Iteration 104/1000 | Loss: 0.00002699
Iteration 105/1000 | Loss: 0.00002699
Iteration 106/1000 | Loss: 0.00002699
Iteration 107/1000 | Loss: 0.00002699
Iteration 108/1000 | Loss: 0.00002698
Iteration 109/1000 | Loss: 0.00002698
Iteration 110/1000 | Loss: 0.00002698
Iteration 111/1000 | Loss: 0.00002698
Iteration 112/1000 | Loss: 0.00002698
Iteration 113/1000 | Loss: 0.00002698
Iteration 114/1000 | Loss: 0.00002698
Iteration 115/1000 | Loss: 0.00002698
Iteration 116/1000 | Loss: 0.00002698
Iteration 117/1000 | Loss: 0.00002698
Iteration 118/1000 | Loss: 0.00002698
Iteration 119/1000 | Loss: 0.00002697
Iteration 120/1000 | Loss: 0.00002697
Iteration 121/1000 | Loss: 0.00002697
Iteration 122/1000 | Loss: 0.00002697
Iteration 123/1000 | Loss: 0.00002697
Iteration 124/1000 | Loss: 0.00002697
Iteration 125/1000 | Loss: 0.00002696
Iteration 126/1000 | Loss: 0.00002696
Iteration 127/1000 | Loss: 0.00002696
Iteration 128/1000 | Loss: 0.00002696
Iteration 129/1000 | Loss: 0.00002696
Iteration 130/1000 | Loss: 0.00002696
Iteration 131/1000 | Loss: 0.00002695
Iteration 132/1000 | Loss: 0.00002695
Iteration 133/1000 | Loss: 0.00002695
Iteration 134/1000 | Loss: 0.00002695
Iteration 135/1000 | Loss: 0.00002694
Iteration 136/1000 | Loss: 0.00002694
Iteration 137/1000 | Loss: 0.00002694
Iteration 138/1000 | Loss: 0.00002694
Iteration 139/1000 | Loss: 0.00002694
Iteration 140/1000 | Loss: 0.00002694
Iteration 141/1000 | Loss: 0.00002694
Iteration 142/1000 | Loss: 0.00002694
Iteration 143/1000 | Loss: 0.00002693
Iteration 144/1000 | Loss: 0.00002693
Iteration 145/1000 | Loss: 0.00002693
Iteration 146/1000 | Loss: 0.00002693
Iteration 147/1000 | Loss: 0.00002693
Iteration 148/1000 | Loss: 0.00002693
Iteration 149/1000 | Loss: 0.00002693
Iteration 150/1000 | Loss: 0.00002693
Iteration 151/1000 | Loss: 0.00002693
Iteration 152/1000 | Loss: 0.00002693
Iteration 153/1000 | Loss: 0.00002692
Iteration 154/1000 | Loss: 0.00002692
Iteration 155/1000 | Loss: 0.00002692
Iteration 156/1000 | Loss: 0.00002692
Iteration 157/1000 | Loss: 0.00002692
Iteration 158/1000 | Loss: 0.00002692
Iteration 159/1000 | Loss: 0.00002692
Iteration 160/1000 | Loss: 0.00002691
Iteration 161/1000 | Loss: 0.00002691
Iteration 162/1000 | Loss: 0.00002691
Iteration 163/1000 | Loss: 0.00002691
Iteration 164/1000 | Loss: 0.00002691
Iteration 165/1000 | Loss: 0.00002691
Iteration 166/1000 | Loss: 0.00002691
Iteration 167/1000 | Loss: 0.00002691
Iteration 168/1000 | Loss: 0.00002691
Iteration 169/1000 | Loss: 0.00002691
Iteration 170/1000 | Loss: 0.00002691
Iteration 171/1000 | Loss: 0.00002691
Iteration 172/1000 | Loss: 0.00002691
Iteration 173/1000 | Loss: 0.00002691
Iteration 174/1000 | Loss: 0.00002690
Iteration 175/1000 | Loss: 0.00002690
Iteration 176/1000 | Loss: 0.00002690
Iteration 177/1000 | Loss: 0.00002690
Iteration 178/1000 | Loss: 0.00002690
Iteration 179/1000 | Loss: 0.00002690
Iteration 180/1000 | Loss: 0.00002690
Iteration 181/1000 | Loss: 0.00002690
Iteration 182/1000 | Loss: 0.00002690
Iteration 183/1000 | Loss: 0.00002690
Iteration 184/1000 | Loss: 0.00002690
Iteration 185/1000 | Loss: 0.00002690
Iteration 186/1000 | Loss: 0.00002690
Iteration 187/1000 | Loss: 0.00002690
Iteration 188/1000 | Loss: 0.00002690
Iteration 189/1000 | Loss: 0.00002690
Iteration 190/1000 | Loss: 0.00002690
Iteration 191/1000 | Loss: 0.00002690
Iteration 192/1000 | Loss: 0.00002690
Iteration 193/1000 | Loss: 0.00002690
Iteration 194/1000 | Loss: 0.00002690
Iteration 195/1000 | Loss: 0.00002690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.6903491743723862e-05, 2.6903491743723862e-05, 2.6903491743723862e-05, 2.6903491743723862e-05, 2.6903491743723862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6903491743723862e-05

Optimization complete. Final v2v error: 4.202359676361084 mm

Highest mean error: 5.387219429016113 mm for frame 46

Lowest mean error: 3.0815112590789795 mm for frame 12

Saving results

Total time: 105.41846704483032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426062
Iteration 2/25 | Loss: 0.00087750
Iteration 3/25 | Loss: 0.00073262
Iteration 4/25 | Loss: 0.00070700
Iteration 5/25 | Loss: 0.00069945
Iteration 6/25 | Loss: 0.00069811
Iteration 7/25 | Loss: 0.00069787
Iteration 8/25 | Loss: 0.00069787
Iteration 9/25 | Loss: 0.00069787
Iteration 10/25 | Loss: 0.00069787
Iteration 11/25 | Loss: 0.00069787
Iteration 12/25 | Loss: 0.00069787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006978718447498977, 0.0006978718447498977, 0.0006978718447498977, 0.0006978718447498977, 0.0006978718447498977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006978718447498977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.65033722
Iteration 2/25 | Loss: 0.00032485
Iteration 3/25 | Loss: 0.00032485
Iteration 4/25 | Loss: 0.00032485
Iteration 5/25 | Loss: 0.00032485
Iteration 6/25 | Loss: 0.00032485
Iteration 7/25 | Loss: 0.00032484
Iteration 8/25 | Loss: 0.00032484
Iteration 9/25 | Loss: 0.00032484
Iteration 10/25 | Loss: 0.00032484
Iteration 11/25 | Loss: 0.00032484
Iteration 12/25 | Loss: 0.00032484
Iteration 13/25 | Loss: 0.00032484
Iteration 14/25 | Loss: 0.00032484
Iteration 15/25 | Loss: 0.00032484
Iteration 16/25 | Loss: 0.00032484
Iteration 17/25 | Loss: 0.00032484
Iteration 18/25 | Loss: 0.00032484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003248435095883906, 0.0003248435095883906, 0.0003248435095883906, 0.0003248435095883906, 0.0003248435095883906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003248435095883906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032484
Iteration 2/1000 | Loss: 0.00002109
Iteration 3/1000 | Loss: 0.00001698
Iteration 4/1000 | Loss: 0.00001618
Iteration 5/1000 | Loss: 0.00001517
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001435
Iteration 9/1000 | Loss: 0.00001434
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001407
Iteration 14/1000 | Loss: 0.00001406
Iteration 15/1000 | Loss: 0.00001405
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001405
Iteration 18/1000 | Loss: 0.00001404
Iteration 19/1000 | Loss: 0.00001403
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001398
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001394
Iteration 24/1000 | Loss: 0.00001393
Iteration 25/1000 | Loss: 0.00001390
Iteration 26/1000 | Loss: 0.00001390
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001389
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001387
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001384
Iteration 34/1000 | Loss: 0.00001384
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001384
Iteration 41/1000 | Loss: 0.00001384
Iteration 42/1000 | Loss: 0.00001384
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001381
Iteration 50/1000 | Loss: 0.00001381
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001381
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001381
Iteration 56/1000 | Loss: 0.00001381
Iteration 57/1000 | Loss: 0.00001381
Iteration 58/1000 | Loss: 0.00001381
Iteration 59/1000 | Loss: 0.00001381
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001380
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001378
Iteration 67/1000 | Loss: 0.00001378
Iteration 68/1000 | Loss: 0.00001378
Iteration 69/1000 | Loss: 0.00001377
Iteration 70/1000 | Loss: 0.00001377
Iteration 71/1000 | Loss: 0.00001377
Iteration 72/1000 | Loss: 0.00001377
Iteration 73/1000 | Loss: 0.00001377
Iteration 74/1000 | Loss: 0.00001377
Iteration 75/1000 | Loss: 0.00001377
Iteration 76/1000 | Loss: 0.00001377
Iteration 77/1000 | Loss: 0.00001376
Iteration 78/1000 | Loss: 0.00001374
Iteration 79/1000 | Loss: 0.00001373
Iteration 80/1000 | Loss: 0.00001373
Iteration 81/1000 | Loss: 0.00001373
Iteration 82/1000 | Loss: 0.00001373
Iteration 83/1000 | Loss: 0.00001372
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001365
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001364
Iteration 106/1000 | Loss: 0.00001364
Iteration 107/1000 | Loss: 0.00001364
Iteration 108/1000 | Loss: 0.00001364
Iteration 109/1000 | Loss: 0.00001364
Iteration 110/1000 | Loss: 0.00001363
Iteration 111/1000 | Loss: 0.00001363
Iteration 112/1000 | Loss: 0.00001363
Iteration 113/1000 | Loss: 0.00001363
Iteration 114/1000 | Loss: 0.00001363
Iteration 115/1000 | Loss: 0.00001363
Iteration 116/1000 | Loss: 0.00001363
Iteration 117/1000 | Loss: 0.00001363
Iteration 118/1000 | Loss: 0.00001363
Iteration 119/1000 | Loss: 0.00001363
Iteration 120/1000 | Loss: 0.00001363
Iteration 121/1000 | Loss: 0.00001363
Iteration 122/1000 | Loss: 0.00001363
Iteration 123/1000 | Loss: 0.00001363
Iteration 124/1000 | Loss: 0.00001363
Iteration 125/1000 | Loss: 0.00001363
Iteration 126/1000 | Loss: 0.00001363
Iteration 127/1000 | Loss: 0.00001363
Iteration 128/1000 | Loss: 0.00001363
Iteration 129/1000 | Loss: 0.00001363
Iteration 130/1000 | Loss: 0.00001362
Iteration 131/1000 | Loss: 0.00001362
Iteration 132/1000 | Loss: 0.00001362
Iteration 133/1000 | Loss: 0.00001362
Iteration 134/1000 | Loss: 0.00001361
Iteration 135/1000 | Loss: 0.00001361
Iteration 136/1000 | Loss: 0.00001361
Iteration 137/1000 | Loss: 0.00001361
Iteration 138/1000 | Loss: 0.00001361
Iteration 139/1000 | Loss: 0.00001361
Iteration 140/1000 | Loss: 0.00001361
Iteration 141/1000 | Loss: 0.00001361
Iteration 142/1000 | Loss: 0.00001361
Iteration 143/1000 | Loss: 0.00001361
Iteration 144/1000 | Loss: 0.00001360
Iteration 145/1000 | Loss: 0.00001360
Iteration 146/1000 | Loss: 0.00001359
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001359
Iteration 156/1000 | Loss: 0.00001359
Iteration 157/1000 | Loss: 0.00001359
Iteration 158/1000 | Loss: 0.00001359
Iteration 159/1000 | Loss: 0.00001359
Iteration 160/1000 | Loss: 0.00001358
Iteration 161/1000 | Loss: 0.00001358
Iteration 162/1000 | Loss: 0.00001358
Iteration 163/1000 | Loss: 0.00001358
Iteration 164/1000 | Loss: 0.00001358
Iteration 165/1000 | Loss: 0.00001357
Iteration 166/1000 | Loss: 0.00001357
Iteration 167/1000 | Loss: 0.00001357
Iteration 168/1000 | Loss: 0.00001357
Iteration 169/1000 | Loss: 0.00001357
Iteration 170/1000 | Loss: 0.00001357
Iteration 171/1000 | Loss: 0.00001357
Iteration 172/1000 | Loss: 0.00001357
Iteration 173/1000 | Loss: 0.00001357
Iteration 174/1000 | Loss: 0.00001357
Iteration 175/1000 | Loss: 0.00001357
Iteration 176/1000 | Loss: 0.00001357
Iteration 177/1000 | Loss: 0.00001357
Iteration 178/1000 | Loss: 0.00001356
Iteration 179/1000 | Loss: 0.00001356
Iteration 180/1000 | Loss: 0.00001356
Iteration 181/1000 | Loss: 0.00001356
Iteration 182/1000 | Loss: 0.00001356
Iteration 183/1000 | Loss: 0.00001356
Iteration 184/1000 | Loss: 0.00001356
Iteration 185/1000 | Loss: 0.00001356
Iteration 186/1000 | Loss: 0.00001356
Iteration 187/1000 | Loss: 0.00001356
Iteration 188/1000 | Loss: 0.00001355
Iteration 189/1000 | Loss: 0.00001355
Iteration 190/1000 | Loss: 0.00001355
Iteration 191/1000 | Loss: 0.00001355
Iteration 192/1000 | Loss: 0.00001355
Iteration 193/1000 | Loss: 0.00001355
Iteration 194/1000 | Loss: 0.00001355
Iteration 195/1000 | Loss: 0.00001355
Iteration 196/1000 | Loss: 0.00001355
Iteration 197/1000 | Loss: 0.00001355
Iteration 198/1000 | Loss: 0.00001355
Iteration 199/1000 | Loss: 0.00001355
Iteration 200/1000 | Loss: 0.00001355
Iteration 201/1000 | Loss: 0.00001355
Iteration 202/1000 | Loss: 0.00001355
Iteration 203/1000 | Loss: 0.00001355
Iteration 204/1000 | Loss: 0.00001355
Iteration 205/1000 | Loss: 0.00001355
Iteration 206/1000 | Loss: 0.00001354
Iteration 207/1000 | Loss: 0.00001354
Iteration 208/1000 | Loss: 0.00001354
Iteration 209/1000 | Loss: 0.00001354
Iteration 210/1000 | Loss: 0.00001354
Iteration 211/1000 | Loss: 0.00001354
Iteration 212/1000 | Loss: 0.00001354
Iteration 213/1000 | Loss: 0.00001354
Iteration 214/1000 | Loss: 0.00001354
Iteration 215/1000 | Loss: 0.00001354
Iteration 216/1000 | Loss: 0.00001354
Iteration 217/1000 | Loss: 0.00001354
Iteration 218/1000 | Loss: 0.00001354
Iteration 219/1000 | Loss: 0.00001354
Iteration 220/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.3544723515224177e-05, 1.3544723515224177e-05, 1.3544723515224177e-05, 1.3544723515224177e-05, 1.3544723515224177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3544723515224177e-05

Optimization complete. Final v2v error: 3.113685131072998 mm

Highest mean error: 3.4243316650390625 mm for frame 82

Lowest mean error: 2.943026065826416 mm for frame 27

Saving results

Total time: 39.3942391872406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592322
Iteration 2/25 | Loss: 0.00124181
Iteration 3/25 | Loss: 0.00092776
Iteration 4/25 | Loss: 0.00088942
Iteration 5/25 | Loss: 0.00086954
Iteration 6/25 | Loss: 0.00086291
Iteration 7/25 | Loss: 0.00086002
Iteration 8/25 | Loss: 0.00085898
Iteration 9/25 | Loss: 0.00085869
Iteration 10/25 | Loss: 0.00085848
Iteration 11/25 | Loss: 0.00085839
Iteration 12/25 | Loss: 0.00085839
Iteration 13/25 | Loss: 0.00085838
Iteration 14/25 | Loss: 0.00085838
Iteration 15/25 | Loss: 0.00085838
Iteration 16/25 | Loss: 0.00085835
Iteration 17/25 | Loss: 0.00085835
Iteration 18/25 | Loss: 0.00085835
Iteration 19/25 | Loss: 0.00085835
Iteration 20/25 | Loss: 0.00085835
Iteration 21/25 | Loss: 0.00085835
Iteration 22/25 | Loss: 0.00085834
Iteration 23/25 | Loss: 0.00085834
Iteration 24/25 | Loss: 0.00085834
Iteration 25/25 | Loss: 0.00085834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42063522
Iteration 2/25 | Loss: 0.00109072
Iteration 3/25 | Loss: 0.00109072
Iteration 4/25 | Loss: 0.00109072
Iteration 5/25 | Loss: 0.00109072
Iteration 6/25 | Loss: 0.00109072
Iteration 7/25 | Loss: 0.00109072
Iteration 8/25 | Loss: 0.00109072
Iteration 9/25 | Loss: 0.00109072
Iteration 10/25 | Loss: 0.00109072
Iteration 11/25 | Loss: 0.00109072
Iteration 12/25 | Loss: 0.00109072
Iteration 13/25 | Loss: 0.00109072
Iteration 14/25 | Loss: 0.00109072
Iteration 15/25 | Loss: 0.00109072
Iteration 16/25 | Loss: 0.00109072
Iteration 17/25 | Loss: 0.00109072
Iteration 18/25 | Loss: 0.00109072
Iteration 19/25 | Loss: 0.00109072
Iteration 20/25 | Loss: 0.00109072
Iteration 21/25 | Loss: 0.00109072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010907200630754232, 0.0010907200630754232, 0.0010907200630754232, 0.0010907200630754232, 0.0010907200630754232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010907200630754232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109072
Iteration 2/1000 | Loss: 0.00014631
Iteration 3/1000 | Loss: 0.00011367
Iteration 4/1000 | Loss: 0.00009590
Iteration 5/1000 | Loss: 0.00123441
Iteration 6/1000 | Loss: 0.00009749
Iteration 7/1000 | Loss: 0.00044732
Iteration 8/1000 | Loss: 0.00008219
Iteration 9/1000 | Loss: 0.00017495
Iteration 10/1000 | Loss: 0.00006609
Iteration 11/1000 | Loss: 0.00038810
Iteration 12/1000 | Loss: 0.00005933
Iteration 13/1000 | Loss: 0.00005288
Iteration 14/1000 | Loss: 0.00035317
Iteration 15/1000 | Loss: 0.00005609
Iteration 16/1000 | Loss: 0.00004779
Iteration 17/1000 | Loss: 0.00051298
Iteration 18/1000 | Loss: 0.00005457
Iteration 19/1000 | Loss: 0.00004733
Iteration 20/1000 | Loss: 0.00004356
Iteration 21/1000 | Loss: 0.00004080
Iteration 22/1000 | Loss: 0.00003895
Iteration 23/1000 | Loss: 0.00027310
Iteration 24/1000 | Loss: 0.00004246
Iteration 25/1000 | Loss: 0.00003658
Iteration 26/1000 | Loss: 0.00003522
Iteration 27/1000 | Loss: 0.00003444
Iteration 28/1000 | Loss: 0.00003385
Iteration 29/1000 | Loss: 0.00003342
Iteration 30/1000 | Loss: 0.00003306
Iteration 31/1000 | Loss: 0.00003257
Iteration 32/1000 | Loss: 0.00003226
Iteration 33/1000 | Loss: 0.00003206
Iteration 34/1000 | Loss: 0.00003201
Iteration 35/1000 | Loss: 0.00003196
Iteration 36/1000 | Loss: 0.00003194
Iteration 37/1000 | Loss: 0.00003193
Iteration 38/1000 | Loss: 0.00003189
Iteration 39/1000 | Loss: 0.00057582
Iteration 40/1000 | Loss: 0.00318224
Iteration 41/1000 | Loss: 0.00014923
Iteration 42/1000 | Loss: 0.00004360
Iteration 43/1000 | Loss: 0.00005442
Iteration 44/1000 | Loss: 0.00003004
Iteration 45/1000 | Loss: 0.00002709
Iteration 46/1000 | Loss: 0.00002512
Iteration 47/1000 | Loss: 0.00002354
Iteration 48/1000 | Loss: 0.00002278
Iteration 49/1000 | Loss: 0.00002220
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002139
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002090
Iteration 54/1000 | Loss: 0.00002078
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002072
Iteration 58/1000 | Loss: 0.00002072
Iteration 59/1000 | Loss: 0.00002072
Iteration 60/1000 | Loss: 0.00002072
Iteration 61/1000 | Loss: 0.00002072
Iteration 62/1000 | Loss: 0.00002071
Iteration 63/1000 | Loss: 0.00002071
Iteration 64/1000 | Loss: 0.00002070
Iteration 65/1000 | Loss: 0.00002070
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002066
Iteration 73/1000 | Loss: 0.00002066
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00002065
Iteration 76/1000 | Loss: 0.00002065
Iteration 77/1000 | Loss: 0.00002064
Iteration 78/1000 | Loss: 0.00002064
Iteration 79/1000 | Loss: 0.00002064
Iteration 80/1000 | Loss: 0.00002063
Iteration 81/1000 | Loss: 0.00002063
Iteration 82/1000 | Loss: 0.00002063
Iteration 83/1000 | Loss: 0.00002063
Iteration 84/1000 | Loss: 0.00002062
Iteration 85/1000 | Loss: 0.00002062
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002061
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002060
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002059
Iteration 101/1000 | Loss: 0.00002059
Iteration 102/1000 | Loss: 0.00002058
Iteration 103/1000 | Loss: 0.00002058
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002058
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002057
Iteration 112/1000 | Loss: 0.00002057
Iteration 113/1000 | Loss: 0.00002057
Iteration 114/1000 | Loss: 0.00002057
Iteration 115/1000 | Loss: 0.00002057
Iteration 116/1000 | Loss: 0.00002057
Iteration 117/1000 | Loss: 0.00002057
Iteration 118/1000 | Loss: 0.00002057
Iteration 119/1000 | Loss: 0.00002057
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002057
Iteration 122/1000 | Loss: 0.00002057
Iteration 123/1000 | Loss: 0.00002057
Iteration 124/1000 | Loss: 0.00002057
Iteration 125/1000 | Loss: 0.00002056
Iteration 126/1000 | Loss: 0.00002056
Iteration 127/1000 | Loss: 0.00002056
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002056
Iteration 130/1000 | Loss: 0.00002056
Iteration 131/1000 | Loss: 0.00002056
Iteration 132/1000 | Loss: 0.00002056
Iteration 133/1000 | Loss: 0.00002056
Iteration 134/1000 | Loss: 0.00002056
Iteration 135/1000 | Loss: 0.00002056
Iteration 136/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.056089579127729e-05, 2.056089579127729e-05, 2.056089579127729e-05, 2.056089579127729e-05, 2.056089579127729e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.056089579127729e-05

Optimization complete. Final v2v error: 3.7381317615509033 mm

Highest mean error: 4.847214698791504 mm for frame 84

Lowest mean error: 3.024839162826538 mm for frame 154

Saving results

Total time: 109.15923690795898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ethan_posed_015/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ethan_posed_015/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053607
Iteration 2/25 | Loss: 0.00311501
Iteration 3/25 | Loss: 0.00171993
Iteration 4/25 | Loss: 0.00123321
Iteration 5/25 | Loss: 0.00109054
Iteration 6/25 | Loss: 0.00101187
Iteration 7/25 | Loss: 0.00094653
Iteration 8/25 | Loss: 0.00090631
Iteration 9/25 | Loss: 0.00082831
Iteration 10/25 | Loss: 0.00078261
Iteration 11/25 | Loss: 0.00075734
Iteration 12/25 | Loss: 0.00073784
Iteration 13/25 | Loss: 0.00072742
Iteration 14/25 | Loss: 0.00072218
Iteration 15/25 | Loss: 0.00071933
Iteration 16/25 | Loss: 0.00071819
Iteration 17/25 | Loss: 0.00071779
Iteration 18/25 | Loss: 0.00071762
Iteration 19/25 | Loss: 0.00071752
Iteration 20/25 | Loss: 0.00071752
Iteration 21/25 | Loss: 0.00071751
Iteration 22/25 | Loss: 0.00071751
Iteration 23/25 | Loss: 0.00071751
Iteration 24/25 | Loss: 0.00071751
Iteration 25/25 | Loss: 0.00071751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47329700
Iteration 2/25 | Loss: 0.00032941
Iteration 3/25 | Loss: 0.00032941
Iteration 4/25 | Loss: 0.00032941
Iteration 5/25 | Loss: 0.00032941
Iteration 6/25 | Loss: 0.00032941
Iteration 7/25 | Loss: 0.00032941
Iteration 8/25 | Loss: 0.00032941
Iteration 9/25 | Loss: 0.00032941
Iteration 10/25 | Loss: 0.00032941
Iteration 11/25 | Loss: 0.00032941
Iteration 12/25 | Loss: 0.00032941
Iteration 13/25 | Loss: 0.00032941
Iteration 14/25 | Loss: 0.00032941
Iteration 15/25 | Loss: 0.00032941
Iteration 16/25 | Loss: 0.00032941
Iteration 17/25 | Loss: 0.00032941
Iteration 18/25 | Loss: 0.00032941
Iteration 19/25 | Loss: 0.00032941
Iteration 20/25 | Loss: 0.00032941
Iteration 21/25 | Loss: 0.00032941
Iteration 22/25 | Loss: 0.00032941
Iteration 23/25 | Loss: 0.00032941
Iteration 24/25 | Loss: 0.00032941
Iteration 25/25 | Loss: 0.00032941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032941
Iteration 2/1000 | Loss: 0.00002859
Iteration 3/1000 | Loss: 0.00002025
Iteration 4/1000 | Loss: 0.00001878
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001608
Iteration 9/1000 | Loss: 0.00001606
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001572
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001562
Iteration 14/1000 | Loss: 0.00001560
Iteration 15/1000 | Loss: 0.00001559
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001552
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00001552
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00001552
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001552
Iteration 31/1000 | Loss: 0.00001552
Iteration 32/1000 | Loss: 0.00001552
Iteration 33/1000 | Loss: 0.00001552
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001552
Iteration 40/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [1.5518491636612453e-05, 1.5518491636612453e-05, 1.5518491636612453e-05, 1.5518491636612453e-05, 1.5518491636612453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5518491636612453e-05

Optimization complete. Final v2v error: 3.2803304195404053 mm

Highest mean error: 3.528564453125 mm for frame 122

Lowest mean error: 3.0402793884277344 mm for frame 189

Saving results

Total time: 56.99800181388855
