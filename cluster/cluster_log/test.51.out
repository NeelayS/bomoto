Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=51, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2856-2911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503840
Iteration 2/25 | Loss: 0.00147131
Iteration 3/25 | Loss: 0.00119067
Iteration 4/25 | Loss: 0.00116883
Iteration 5/25 | Loss: 0.00116256
Iteration 6/25 | Loss: 0.00116112
Iteration 7/25 | Loss: 0.00116112
Iteration 8/25 | Loss: 0.00116112
Iteration 9/25 | Loss: 0.00116112
Iteration 10/25 | Loss: 0.00116112
Iteration 11/25 | Loss: 0.00116112
Iteration 12/25 | Loss: 0.00116112
Iteration 13/25 | Loss: 0.00116112
Iteration 14/25 | Loss: 0.00116112
Iteration 15/25 | Loss: 0.00116112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011611166410148144, 0.0011611166410148144, 0.0011611166410148144, 0.0011611166410148144, 0.0011611166410148144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011611166410148144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07318997
Iteration 2/25 | Loss: 0.00089322
Iteration 3/25 | Loss: 0.00089322
Iteration 4/25 | Loss: 0.00089322
Iteration 5/25 | Loss: 0.00089322
Iteration 6/25 | Loss: 0.00089322
Iteration 7/25 | Loss: 0.00089322
Iteration 8/25 | Loss: 0.00089322
Iteration 9/25 | Loss: 0.00089322
Iteration 10/25 | Loss: 0.00089322
Iteration 11/25 | Loss: 0.00089322
Iteration 12/25 | Loss: 0.00089322
Iteration 13/25 | Loss: 0.00089322
Iteration 14/25 | Loss: 0.00089322
Iteration 15/25 | Loss: 0.00089322
Iteration 16/25 | Loss: 0.00089322
Iteration 17/25 | Loss: 0.00089322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008932207711040974, 0.0008932207711040974, 0.0008932207711040974, 0.0008932207711040974, 0.0008932207711040974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008932207711040974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089322
Iteration 2/1000 | Loss: 0.00006824
Iteration 3/1000 | Loss: 0.00004208
Iteration 4/1000 | Loss: 0.00003048
Iteration 5/1000 | Loss: 0.00002738
Iteration 6/1000 | Loss: 0.00002587
Iteration 7/1000 | Loss: 0.00002468
Iteration 8/1000 | Loss: 0.00002404
Iteration 9/1000 | Loss: 0.00002332
Iteration 10/1000 | Loss: 0.00002288
Iteration 11/1000 | Loss: 0.00002245
Iteration 12/1000 | Loss: 0.00002213
Iteration 13/1000 | Loss: 0.00002189
Iteration 14/1000 | Loss: 0.00002166
Iteration 15/1000 | Loss: 0.00002145
Iteration 16/1000 | Loss: 0.00002128
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00002117
Iteration 19/1000 | Loss: 0.00002111
Iteration 20/1000 | Loss: 0.00002110
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002104
Iteration 23/1000 | Loss: 0.00002099
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002095
Iteration 26/1000 | Loss: 0.00002095
Iteration 27/1000 | Loss: 0.00002095
Iteration 28/1000 | Loss: 0.00002095
Iteration 29/1000 | Loss: 0.00002093
Iteration 30/1000 | Loss: 0.00002092
Iteration 31/1000 | Loss: 0.00002092
Iteration 32/1000 | Loss: 0.00002092
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002091
Iteration 35/1000 | Loss: 0.00002091
Iteration 36/1000 | Loss: 0.00002091
Iteration 37/1000 | Loss: 0.00002091
Iteration 38/1000 | Loss: 0.00002091
Iteration 39/1000 | Loss: 0.00002090
Iteration 40/1000 | Loss: 0.00002090
Iteration 41/1000 | Loss: 0.00002090
Iteration 42/1000 | Loss: 0.00002089
Iteration 43/1000 | Loss: 0.00002089
Iteration 44/1000 | Loss: 0.00002089
Iteration 45/1000 | Loss: 0.00002089
Iteration 46/1000 | Loss: 0.00002089
Iteration 47/1000 | Loss: 0.00002089
Iteration 48/1000 | Loss: 0.00002089
Iteration 49/1000 | Loss: 0.00002088
Iteration 50/1000 | Loss: 0.00002088
Iteration 51/1000 | Loss: 0.00002088
Iteration 52/1000 | Loss: 0.00002087
Iteration 53/1000 | Loss: 0.00002087
Iteration 54/1000 | Loss: 0.00002087
Iteration 55/1000 | Loss: 0.00002087
Iteration 56/1000 | Loss: 0.00002087
Iteration 57/1000 | Loss: 0.00002086
Iteration 58/1000 | Loss: 0.00002086
Iteration 59/1000 | Loss: 0.00002085
Iteration 60/1000 | Loss: 0.00002085
Iteration 61/1000 | Loss: 0.00002085
Iteration 62/1000 | Loss: 0.00002085
Iteration 63/1000 | Loss: 0.00002085
Iteration 64/1000 | Loss: 0.00002084
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00002084
Iteration 67/1000 | Loss: 0.00002083
Iteration 68/1000 | Loss: 0.00002082
Iteration 69/1000 | Loss: 0.00002082
Iteration 70/1000 | Loss: 0.00002082
Iteration 71/1000 | Loss: 0.00002081
Iteration 72/1000 | Loss: 0.00002081
Iteration 73/1000 | Loss: 0.00002081
Iteration 74/1000 | Loss: 0.00002080
Iteration 75/1000 | Loss: 0.00002080
Iteration 76/1000 | Loss: 0.00002080
Iteration 77/1000 | Loss: 0.00002080
Iteration 78/1000 | Loss: 0.00002080
Iteration 79/1000 | Loss: 0.00002080
Iteration 80/1000 | Loss: 0.00002080
Iteration 81/1000 | Loss: 0.00002079
Iteration 82/1000 | Loss: 0.00002079
Iteration 83/1000 | Loss: 0.00002079
Iteration 84/1000 | Loss: 0.00002079
Iteration 85/1000 | Loss: 0.00002079
Iteration 86/1000 | Loss: 0.00002079
Iteration 87/1000 | Loss: 0.00002079
Iteration 88/1000 | Loss: 0.00002079
Iteration 89/1000 | Loss: 0.00002078
Iteration 90/1000 | Loss: 0.00002078
Iteration 91/1000 | Loss: 0.00002078
Iteration 92/1000 | Loss: 0.00002078
Iteration 93/1000 | Loss: 0.00002078
Iteration 94/1000 | Loss: 0.00002077
Iteration 95/1000 | Loss: 0.00002077
Iteration 96/1000 | Loss: 0.00002077
Iteration 97/1000 | Loss: 0.00002077
Iteration 98/1000 | Loss: 0.00002077
Iteration 99/1000 | Loss: 0.00002077
Iteration 100/1000 | Loss: 0.00002077
Iteration 101/1000 | Loss: 0.00002076
Iteration 102/1000 | Loss: 0.00002076
Iteration 103/1000 | Loss: 0.00002076
Iteration 104/1000 | Loss: 0.00002076
Iteration 105/1000 | Loss: 0.00002076
Iteration 106/1000 | Loss: 0.00002075
Iteration 107/1000 | Loss: 0.00002075
Iteration 108/1000 | Loss: 0.00002075
Iteration 109/1000 | Loss: 0.00002074
Iteration 110/1000 | Loss: 0.00002074
Iteration 111/1000 | Loss: 0.00002074
Iteration 112/1000 | Loss: 0.00002074
Iteration 113/1000 | Loss: 0.00002073
Iteration 114/1000 | Loss: 0.00002073
Iteration 115/1000 | Loss: 0.00002073
Iteration 116/1000 | Loss: 0.00002072
Iteration 117/1000 | Loss: 0.00002072
Iteration 118/1000 | Loss: 0.00002072
Iteration 119/1000 | Loss: 0.00002072
Iteration 120/1000 | Loss: 0.00002071
Iteration 121/1000 | Loss: 0.00002071
Iteration 122/1000 | Loss: 0.00002071
Iteration 123/1000 | Loss: 0.00002070
Iteration 124/1000 | Loss: 0.00002070
Iteration 125/1000 | Loss: 0.00002070
Iteration 126/1000 | Loss: 0.00002070
Iteration 127/1000 | Loss: 0.00002069
Iteration 128/1000 | Loss: 0.00002069
Iteration 129/1000 | Loss: 0.00002069
Iteration 130/1000 | Loss: 0.00002069
Iteration 131/1000 | Loss: 0.00002069
Iteration 132/1000 | Loss: 0.00002069
Iteration 133/1000 | Loss: 0.00002069
Iteration 134/1000 | Loss: 0.00002068
Iteration 135/1000 | Loss: 0.00002068
Iteration 136/1000 | Loss: 0.00002068
Iteration 137/1000 | Loss: 0.00002068
Iteration 138/1000 | Loss: 0.00002068
Iteration 139/1000 | Loss: 0.00002068
Iteration 140/1000 | Loss: 0.00002068
Iteration 141/1000 | Loss: 0.00002068
Iteration 142/1000 | Loss: 0.00002068
Iteration 143/1000 | Loss: 0.00002068
Iteration 144/1000 | Loss: 0.00002068
Iteration 145/1000 | Loss: 0.00002068
Iteration 146/1000 | Loss: 0.00002068
Iteration 147/1000 | Loss: 0.00002067
Iteration 148/1000 | Loss: 0.00002067
Iteration 149/1000 | Loss: 0.00002067
Iteration 150/1000 | Loss: 0.00002067
Iteration 151/1000 | Loss: 0.00002067
Iteration 152/1000 | Loss: 0.00002067
Iteration 153/1000 | Loss: 0.00002067
Iteration 154/1000 | Loss: 0.00002067
Iteration 155/1000 | Loss: 0.00002067
Iteration 156/1000 | Loss: 0.00002067
Iteration 157/1000 | Loss: 0.00002067
Iteration 158/1000 | Loss: 0.00002067
Iteration 159/1000 | Loss: 0.00002067
Iteration 160/1000 | Loss: 0.00002066
Iteration 161/1000 | Loss: 0.00002066
Iteration 162/1000 | Loss: 0.00002066
Iteration 163/1000 | Loss: 0.00002066
Iteration 164/1000 | Loss: 0.00002066
Iteration 165/1000 | Loss: 0.00002066
Iteration 166/1000 | Loss: 0.00002066
Iteration 167/1000 | Loss: 0.00002066
Iteration 168/1000 | Loss: 0.00002066
Iteration 169/1000 | Loss: 0.00002066
Iteration 170/1000 | Loss: 0.00002066
Iteration 171/1000 | Loss: 0.00002066
Iteration 172/1000 | Loss: 0.00002066
Iteration 173/1000 | Loss: 0.00002066
Iteration 174/1000 | Loss: 0.00002066
Iteration 175/1000 | Loss: 0.00002066
Iteration 176/1000 | Loss: 0.00002066
Iteration 177/1000 | Loss: 0.00002066
Iteration 178/1000 | Loss: 0.00002066
Iteration 179/1000 | Loss: 0.00002066
Iteration 180/1000 | Loss: 0.00002066
Iteration 181/1000 | Loss: 0.00002066
Iteration 182/1000 | Loss: 0.00002066
Iteration 183/1000 | Loss: 0.00002066
Iteration 184/1000 | Loss: 0.00002066
Iteration 185/1000 | Loss: 0.00002066
Iteration 186/1000 | Loss: 0.00002066
Iteration 187/1000 | Loss: 0.00002066
Iteration 188/1000 | Loss: 0.00002066
Iteration 189/1000 | Loss: 0.00002066
Iteration 190/1000 | Loss: 0.00002066
Iteration 191/1000 | Loss: 0.00002066
Iteration 192/1000 | Loss: 0.00002066
Iteration 193/1000 | Loss: 0.00002066
Iteration 194/1000 | Loss: 0.00002066
Iteration 195/1000 | Loss: 0.00002066
Iteration 196/1000 | Loss: 0.00002066
Iteration 197/1000 | Loss: 0.00002066
Iteration 198/1000 | Loss: 0.00002066
Iteration 199/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.066187335003633e-05, 2.066187335003633e-05, 2.066187335003633e-05, 2.066187335003633e-05, 2.066187335003633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.066187335003633e-05

Optimization complete. Final v2v error: 3.710691213607788 mm

Highest mean error: 4.943098545074463 mm for frame 161

Lowest mean error: 2.4861485958099365 mm for frame 198

Saving results

Total time: 50.71653723716736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527994
Iteration 2/25 | Loss: 0.00133562
Iteration 3/25 | Loss: 0.00124671
Iteration 4/25 | Loss: 0.00123624
Iteration 5/25 | Loss: 0.00123167
Iteration 6/25 | Loss: 0.00123167
Iteration 7/25 | Loss: 0.00123167
Iteration 8/25 | Loss: 0.00123167
Iteration 9/25 | Loss: 0.00123167
Iteration 10/25 | Loss: 0.00123167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012316732900217175, 0.0012316732900217175, 0.0012316732900217175, 0.0012316732900217175, 0.0012316732900217175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012316732900217175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75272763
Iteration 2/25 | Loss: 0.00070836
Iteration 3/25 | Loss: 0.00070836
Iteration 4/25 | Loss: 0.00070836
Iteration 5/25 | Loss: 0.00070836
Iteration 6/25 | Loss: 0.00070836
Iteration 7/25 | Loss: 0.00070836
Iteration 8/25 | Loss: 0.00070835
Iteration 9/25 | Loss: 0.00070835
Iteration 10/25 | Loss: 0.00070835
Iteration 11/25 | Loss: 0.00070835
Iteration 12/25 | Loss: 0.00070835
Iteration 13/25 | Loss: 0.00070835
Iteration 14/25 | Loss: 0.00070835
Iteration 15/25 | Loss: 0.00070835
Iteration 16/25 | Loss: 0.00070835
Iteration 17/25 | Loss: 0.00070835
Iteration 18/25 | Loss: 0.00070835
Iteration 19/25 | Loss: 0.00070835
Iteration 20/25 | Loss: 0.00070835
Iteration 21/25 | Loss: 0.00070835
Iteration 22/25 | Loss: 0.00070835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007083543459884822, 0.0007083543459884822, 0.0007083543459884822, 0.0007083543459884822, 0.0007083543459884822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007083543459884822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070835
Iteration 2/1000 | Loss: 0.00004071
Iteration 3/1000 | Loss: 0.00003711
Iteration 4/1000 | Loss: 0.00003553
Iteration 5/1000 | Loss: 0.00003454
Iteration 6/1000 | Loss: 0.00003422
Iteration 7/1000 | Loss: 0.00003372
Iteration 8/1000 | Loss: 0.00003332
Iteration 9/1000 | Loss: 0.00003290
Iteration 10/1000 | Loss: 0.00003259
Iteration 11/1000 | Loss: 0.00003253
Iteration 12/1000 | Loss: 0.00003234
Iteration 13/1000 | Loss: 0.00003203
Iteration 14/1000 | Loss: 0.00003189
Iteration 15/1000 | Loss: 0.00003176
Iteration 16/1000 | Loss: 0.00003161
Iteration 17/1000 | Loss: 0.00003160
Iteration 18/1000 | Loss: 0.00003151
Iteration 19/1000 | Loss: 0.00003150
Iteration 20/1000 | Loss: 0.00003146
Iteration 21/1000 | Loss: 0.00003146
Iteration 22/1000 | Loss: 0.00003143
Iteration 23/1000 | Loss: 0.00003143
Iteration 24/1000 | Loss: 0.00003141
Iteration 25/1000 | Loss: 0.00003141
Iteration 26/1000 | Loss: 0.00003140
Iteration 27/1000 | Loss: 0.00003140
Iteration 28/1000 | Loss: 0.00003140
Iteration 29/1000 | Loss: 0.00003139
Iteration 30/1000 | Loss: 0.00003138
Iteration 31/1000 | Loss: 0.00003138
Iteration 32/1000 | Loss: 0.00003137
Iteration 33/1000 | Loss: 0.00003137
Iteration 34/1000 | Loss: 0.00003136
Iteration 35/1000 | Loss: 0.00003136
Iteration 36/1000 | Loss: 0.00003136
Iteration 37/1000 | Loss: 0.00003135
Iteration 38/1000 | Loss: 0.00003135
Iteration 39/1000 | Loss: 0.00003134
Iteration 40/1000 | Loss: 0.00003134
Iteration 41/1000 | Loss: 0.00003134
Iteration 42/1000 | Loss: 0.00003134
Iteration 43/1000 | Loss: 0.00003134
Iteration 44/1000 | Loss: 0.00003134
Iteration 45/1000 | Loss: 0.00003134
Iteration 46/1000 | Loss: 0.00003133
Iteration 47/1000 | Loss: 0.00003133
Iteration 48/1000 | Loss: 0.00003133
Iteration 49/1000 | Loss: 0.00003133
Iteration 50/1000 | Loss: 0.00003132
Iteration 51/1000 | Loss: 0.00003132
Iteration 52/1000 | Loss: 0.00003132
Iteration 53/1000 | Loss: 0.00003130
Iteration 54/1000 | Loss: 0.00003130
Iteration 55/1000 | Loss: 0.00003130
Iteration 56/1000 | Loss: 0.00003130
Iteration 57/1000 | Loss: 0.00003130
Iteration 58/1000 | Loss: 0.00003130
Iteration 59/1000 | Loss: 0.00003130
Iteration 60/1000 | Loss: 0.00003129
Iteration 61/1000 | Loss: 0.00003129
Iteration 62/1000 | Loss: 0.00003129
Iteration 63/1000 | Loss: 0.00003129
Iteration 64/1000 | Loss: 0.00003129
Iteration 65/1000 | Loss: 0.00003129
Iteration 66/1000 | Loss: 0.00003129
Iteration 67/1000 | Loss: 0.00003129
Iteration 68/1000 | Loss: 0.00003129
Iteration 69/1000 | Loss: 0.00003129
Iteration 70/1000 | Loss: 0.00003129
Iteration 71/1000 | Loss: 0.00003129
Iteration 72/1000 | Loss: 0.00003129
Iteration 73/1000 | Loss: 0.00003128
Iteration 74/1000 | Loss: 0.00003128
Iteration 75/1000 | Loss: 0.00003128
Iteration 76/1000 | Loss: 0.00003128
Iteration 77/1000 | Loss: 0.00003128
Iteration 78/1000 | Loss: 0.00003128
Iteration 79/1000 | Loss: 0.00003128
Iteration 80/1000 | Loss: 0.00003127
Iteration 81/1000 | Loss: 0.00003127
Iteration 82/1000 | Loss: 0.00003127
Iteration 83/1000 | Loss: 0.00003127
Iteration 84/1000 | Loss: 0.00003127
Iteration 85/1000 | Loss: 0.00003126
Iteration 86/1000 | Loss: 0.00003126
Iteration 87/1000 | Loss: 0.00003126
Iteration 88/1000 | Loss: 0.00003126
Iteration 89/1000 | Loss: 0.00003126
Iteration 90/1000 | Loss: 0.00003126
Iteration 91/1000 | Loss: 0.00003125
Iteration 92/1000 | Loss: 0.00003124
Iteration 93/1000 | Loss: 0.00003124
Iteration 94/1000 | Loss: 0.00003124
Iteration 95/1000 | Loss: 0.00003124
Iteration 96/1000 | Loss: 0.00003124
Iteration 97/1000 | Loss: 0.00003124
Iteration 98/1000 | Loss: 0.00003124
Iteration 99/1000 | Loss: 0.00003124
Iteration 100/1000 | Loss: 0.00003124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [3.1236089853337035e-05, 3.1236089853337035e-05, 3.1236089853337035e-05, 3.1236089853337035e-05, 3.1236089853337035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1236089853337035e-05

Optimization complete. Final v2v error: 4.398998260498047 mm

Highest mean error: 4.419295787811279 mm for frame 225

Lowest mean error: 4.366126537322998 mm for frame 116

Saving results

Total time: 42.9844491481781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435084
Iteration 2/25 | Loss: 0.00117898
Iteration 3/25 | Loss: 0.00111239
Iteration 4/25 | Loss: 0.00109587
Iteration 5/25 | Loss: 0.00109042
Iteration 6/25 | Loss: 0.00108936
Iteration 7/25 | Loss: 0.00108936
Iteration 8/25 | Loss: 0.00108936
Iteration 9/25 | Loss: 0.00108936
Iteration 10/25 | Loss: 0.00108936
Iteration 11/25 | Loss: 0.00108936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010893635917454958, 0.0010893635917454958, 0.0010893635917454958, 0.0010893635917454958, 0.0010893635917454958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010893635917454958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43177032
Iteration 2/25 | Loss: 0.00084638
Iteration 3/25 | Loss: 0.00084638
Iteration 4/25 | Loss: 0.00084638
Iteration 5/25 | Loss: 0.00084638
Iteration 6/25 | Loss: 0.00084638
Iteration 7/25 | Loss: 0.00084638
Iteration 8/25 | Loss: 0.00084638
Iteration 9/25 | Loss: 0.00084638
Iteration 10/25 | Loss: 0.00084638
Iteration 11/25 | Loss: 0.00084638
Iteration 12/25 | Loss: 0.00084638
Iteration 13/25 | Loss: 0.00084638
Iteration 14/25 | Loss: 0.00084638
Iteration 15/25 | Loss: 0.00084638
Iteration 16/25 | Loss: 0.00084638
Iteration 17/25 | Loss: 0.00084638
Iteration 18/25 | Loss: 0.00084638
Iteration 19/25 | Loss: 0.00084638
Iteration 20/25 | Loss: 0.00084638
Iteration 21/25 | Loss: 0.00084638
Iteration 22/25 | Loss: 0.00084638
Iteration 23/25 | Loss: 0.00084638
Iteration 24/25 | Loss: 0.00084638
Iteration 25/25 | Loss: 0.00084638

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084638
Iteration 2/1000 | Loss: 0.00002486
Iteration 3/1000 | Loss: 0.00001623
Iteration 4/1000 | Loss: 0.00001479
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001383
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001325
Iteration 9/1000 | Loss: 0.00001319
Iteration 10/1000 | Loss: 0.00001295
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001262
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001238
Iteration 35/1000 | Loss: 0.00001238
Iteration 36/1000 | Loss: 0.00001238
Iteration 37/1000 | Loss: 0.00001238
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001237
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001237
Iteration 45/1000 | Loss: 0.00001233
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00001232
Iteration 49/1000 | Loss: 0.00001232
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001231
Iteration 54/1000 | Loss: 0.00001231
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001230
Iteration 58/1000 | Loss: 0.00001229
Iteration 59/1000 | Loss: 0.00001229
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001226
Iteration 67/1000 | Loss: 0.00001226
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001226
Iteration 70/1000 | Loss: 0.00001225
Iteration 71/1000 | Loss: 0.00001225
Iteration 72/1000 | Loss: 0.00001225
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001221
Iteration 82/1000 | Loss: 0.00001221
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001220
Iteration 85/1000 | Loss: 0.00001220
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001219
Iteration 88/1000 | Loss: 0.00001219
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001213
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001213
Iteration 105/1000 | Loss: 0.00001212
Iteration 106/1000 | Loss: 0.00001212
Iteration 107/1000 | Loss: 0.00001212
Iteration 108/1000 | Loss: 0.00001211
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001210
Iteration 120/1000 | Loss: 0.00001210
Iteration 121/1000 | Loss: 0.00001210
Iteration 122/1000 | Loss: 0.00001210
Iteration 123/1000 | Loss: 0.00001210
Iteration 124/1000 | Loss: 0.00001209
Iteration 125/1000 | Loss: 0.00001209
Iteration 126/1000 | Loss: 0.00001209
Iteration 127/1000 | Loss: 0.00001209
Iteration 128/1000 | Loss: 0.00001209
Iteration 129/1000 | Loss: 0.00001209
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001208
Iteration 133/1000 | Loss: 0.00001208
Iteration 134/1000 | Loss: 0.00001208
Iteration 135/1000 | Loss: 0.00001208
Iteration 136/1000 | Loss: 0.00001208
Iteration 137/1000 | Loss: 0.00001208
Iteration 138/1000 | Loss: 0.00001208
Iteration 139/1000 | Loss: 0.00001208
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001207
Iteration 144/1000 | Loss: 0.00001207
Iteration 145/1000 | Loss: 0.00001207
Iteration 146/1000 | Loss: 0.00001207
Iteration 147/1000 | Loss: 0.00001207
Iteration 148/1000 | Loss: 0.00001207
Iteration 149/1000 | Loss: 0.00001207
Iteration 150/1000 | Loss: 0.00001207
Iteration 151/1000 | Loss: 0.00001207
Iteration 152/1000 | Loss: 0.00001207
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001206
Iteration 157/1000 | Loss: 0.00001206
Iteration 158/1000 | Loss: 0.00001206
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001206
Iteration 161/1000 | Loss: 0.00001206
Iteration 162/1000 | Loss: 0.00001206
Iteration 163/1000 | Loss: 0.00001206
Iteration 164/1000 | Loss: 0.00001206
Iteration 165/1000 | Loss: 0.00001206
Iteration 166/1000 | Loss: 0.00001206
Iteration 167/1000 | Loss: 0.00001206
Iteration 168/1000 | Loss: 0.00001205
Iteration 169/1000 | Loss: 0.00001205
Iteration 170/1000 | Loss: 0.00001205
Iteration 171/1000 | Loss: 0.00001205
Iteration 172/1000 | Loss: 0.00001205
Iteration 173/1000 | Loss: 0.00001205
Iteration 174/1000 | Loss: 0.00001205
Iteration 175/1000 | Loss: 0.00001205
Iteration 176/1000 | Loss: 0.00001205
Iteration 177/1000 | Loss: 0.00001205
Iteration 178/1000 | Loss: 0.00001205
Iteration 179/1000 | Loss: 0.00001205
Iteration 180/1000 | Loss: 0.00001205
Iteration 181/1000 | Loss: 0.00001205
Iteration 182/1000 | Loss: 0.00001205
Iteration 183/1000 | Loss: 0.00001205
Iteration 184/1000 | Loss: 0.00001205
Iteration 185/1000 | Loss: 0.00001205
Iteration 186/1000 | Loss: 0.00001205
Iteration 187/1000 | Loss: 0.00001205
Iteration 188/1000 | Loss: 0.00001205
Iteration 189/1000 | Loss: 0.00001205
Iteration 190/1000 | Loss: 0.00001205
Iteration 191/1000 | Loss: 0.00001205
Iteration 192/1000 | Loss: 0.00001205
Iteration 193/1000 | Loss: 0.00001205
Iteration 194/1000 | Loss: 0.00001205
Iteration 195/1000 | Loss: 0.00001205
Iteration 196/1000 | Loss: 0.00001205
Iteration 197/1000 | Loss: 0.00001205
Iteration 198/1000 | Loss: 0.00001205
Iteration 199/1000 | Loss: 0.00001205
Iteration 200/1000 | Loss: 0.00001205
Iteration 201/1000 | Loss: 0.00001205
Iteration 202/1000 | Loss: 0.00001205
Iteration 203/1000 | Loss: 0.00001205
Iteration 204/1000 | Loss: 0.00001205
Iteration 205/1000 | Loss: 0.00001205
Iteration 206/1000 | Loss: 0.00001205
Iteration 207/1000 | Loss: 0.00001205
Iteration 208/1000 | Loss: 0.00001205
Iteration 209/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.2049763427057769e-05, 1.2049763427057769e-05, 1.2049763427057769e-05, 1.2049763427057769e-05, 1.2049763427057769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2049763427057769e-05

Optimization complete. Final v2v error: 2.972816228866577 mm

Highest mean error: 3.4213550090789795 mm for frame 78

Lowest mean error: 2.83844256401062 mm for frame 123

Saving results

Total time: 38.746270179748535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954386
Iteration 2/25 | Loss: 0.00142298
Iteration 3/25 | Loss: 0.00120919
Iteration 4/25 | Loss: 0.00119300
Iteration 5/25 | Loss: 0.00118845
Iteration 6/25 | Loss: 0.00118673
Iteration 7/25 | Loss: 0.00118668
Iteration 8/25 | Loss: 0.00118668
Iteration 9/25 | Loss: 0.00118668
Iteration 10/25 | Loss: 0.00118668
Iteration 11/25 | Loss: 0.00118668
Iteration 12/25 | Loss: 0.00118668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011866842396557331, 0.0011866842396557331, 0.0011866842396557331, 0.0011866842396557331, 0.0011866842396557331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011866842396557331

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33205044
Iteration 2/25 | Loss: 0.00107560
Iteration 3/25 | Loss: 0.00107560
Iteration 4/25 | Loss: 0.00107560
Iteration 5/25 | Loss: 0.00107560
Iteration 6/25 | Loss: 0.00107560
Iteration 7/25 | Loss: 0.00107560
Iteration 8/25 | Loss: 0.00107560
Iteration 9/25 | Loss: 0.00107560
Iteration 10/25 | Loss: 0.00107560
Iteration 11/25 | Loss: 0.00107560
Iteration 12/25 | Loss: 0.00107560
Iteration 13/25 | Loss: 0.00107560
Iteration 14/25 | Loss: 0.00107560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010756014380604029, 0.0010756014380604029, 0.0010756014380604029, 0.0010756014380604029, 0.0010756014380604029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010756014380604029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107560
Iteration 2/1000 | Loss: 0.00003748
Iteration 3/1000 | Loss: 0.00002801
Iteration 4/1000 | Loss: 0.00002524
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002310
Iteration 7/1000 | Loss: 0.00002229
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002122
Iteration 10/1000 | Loss: 0.00002092
Iteration 11/1000 | Loss: 0.00002069
Iteration 12/1000 | Loss: 0.00002046
Iteration 13/1000 | Loss: 0.00002032
Iteration 14/1000 | Loss: 0.00002031
Iteration 15/1000 | Loss: 0.00002027
Iteration 16/1000 | Loss: 0.00002027
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002025
Iteration 22/1000 | Loss: 0.00002025
Iteration 23/1000 | Loss: 0.00002025
Iteration 24/1000 | Loss: 0.00002024
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00002024
Iteration 27/1000 | Loss: 0.00002024
Iteration 28/1000 | Loss: 0.00002024
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002023
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002022
Iteration 36/1000 | Loss: 0.00002022
Iteration 37/1000 | Loss: 0.00002022
Iteration 38/1000 | Loss: 0.00002021
Iteration 39/1000 | Loss: 0.00002021
Iteration 40/1000 | Loss: 0.00002020
Iteration 41/1000 | Loss: 0.00002020
Iteration 42/1000 | Loss: 0.00002019
Iteration 43/1000 | Loss: 0.00002019
Iteration 44/1000 | Loss: 0.00002018
Iteration 45/1000 | Loss: 0.00002018
Iteration 46/1000 | Loss: 0.00002017
Iteration 47/1000 | Loss: 0.00002017
Iteration 48/1000 | Loss: 0.00002017
Iteration 49/1000 | Loss: 0.00002017
Iteration 50/1000 | Loss: 0.00002016
Iteration 51/1000 | Loss: 0.00002016
Iteration 52/1000 | Loss: 0.00002016
Iteration 53/1000 | Loss: 0.00002016
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002015
Iteration 56/1000 | Loss: 0.00002015
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002014
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00002013
Iteration 62/1000 | Loss: 0.00002013
Iteration 63/1000 | Loss: 0.00002012
Iteration 64/1000 | Loss: 0.00002012
Iteration 65/1000 | Loss: 0.00002011
Iteration 66/1000 | Loss: 0.00002011
Iteration 67/1000 | Loss: 0.00002011
Iteration 68/1000 | Loss: 0.00002010
Iteration 69/1000 | Loss: 0.00002010
Iteration 70/1000 | Loss: 0.00002010
Iteration 71/1000 | Loss: 0.00002009
Iteration 72/1000 | Loss: 0.00002009
Iteration 73/1000 | Loss: 0.00002009
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002009
Iteration 79/1000 | Loss: 0.00002009
Iteration 80/1000 | Loss: 0.00002009
Iteration 81/1000 | Loss: 0.00002009
Iteration 82/1000 | Loss: 0.00002009
Iteration 83/1000 | Loss: 0.00002008
Iteration 84/1000 | Loss: 0.00002008
Iteration 85/1000 | Loss: 0.00002007
Iteration 86/1000 | Loss: 0.00002007
Iteration 87/1000 | Loss: 0.00002007
Iteration 88/1000 | Loss: 0.00002007
Iteration 89/1000 | Loss: 0.00002007
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00002007
Iteration 93/1000 | Loss: 0.00002007
Iteration 94/1000 | Loss: 0.00002006
Iteration 95/1000 | Loss: 0.00002006
Iteration 96/1000 | Loss: 0.00002005
Iteration 97/1000 | Loss: 0.00002005
Iteration 98/1000 | Loss: 0.00002005
Iteration 99/1000 | Loss: 0.00002005
Iteration 100/1000 | Loss: 0.00002005
Iteration 101/1000 | Loss: 0.00002005
Iteration 102/1000 | Loss: 0.00002004
Iteration 103/1000 | Loss: 0.00002004
Iteration 104/1000 | Loss: 0.00002004
Iteration 105/1000 | Loss: 0.00002004
Iteration 106/1000 | Loss: 0.00002004
Iteration 107/1000 | Loss: 0.00002004
Iteration 108/1000 | Loss: 0.00002004
Iteration 109/1000 | Loss: 0.00002004
Iteration 110/1000 | Loss: 0.00002004
Iteration 111/1000 | Loss: 0.00002004
Iteration 112/1000 | Loss: 0.00002004
Iteration 113/1000 | Loss: 0.00002004
Iteration 114/1000 | Loss: 0.00002004
Iteration 115/1000 | Loss: 0.00002004
Iteration 116/1000 | Loss: 0.00002004
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002003
Iteration 119/1000 | Loss: 0.00002003
Iteration 120/1000 | Loss: 0.00002003
Iteration 121/1000 | Loss: 0.00002003
Iteration 122/1000 | Loss: 0.00002003
Iteration 123/1000 | Loss: 0.00002003
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002003
Iteration 126/1000 | Loss: 0.00002003
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002002
Iteration 129/1000 | Loss: 0.00002002
Iteration 130/1000 | Loss: 0.00002002
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002002
Iteration 133/1000 | Loss: 0.00002002
Iteration 134/1000 | Loss: 0.00002002
Iteration 135/1000 | Loss: 0.00002001
Iteration 136/1000 | Loss: 0.00002001
Iteration 137/1000 | Loss: 0.00002001
Iteration 138/1000 | Loss: 0.00002001
Iteration 139/1000 | Loss: 0.00002001
Iteration 140/1000 | Loss: 0.00002000
Iteration 141/1000 | Loss: 0.00002000
Iteration 142/1000 | Loss: 0.00002000
Iteration 143/1000 | Loss: 0.00002000
Iteration 144/1000 | Loss: 0.00002000
Iteration 145/1000 | Loss: 0.00001999
Iteration 146/1000 | Loss: 0.00001999
Iteration 147/1000 | Loss: 0.00001999
Iteration 148/1000 | Loss: 0.00001999
Iteration 149/1000 | Loss: 0.00001999
Iteration 150/1000 | Loss: 0.00001999
Iteration 151/1000 | Loss: 0.00001999
Iteration 152/1000 | Loss: 0.00001999
Iteration 153/1000 | Loss: 0.00001999
Iteration 154/1000 | Loss: 0.00001998
Iteration 155/1000 | Loss: 0.00001998
Iteration 156/1000 | Loss: 0.00001998
Iteration 157/1000 | Loss: 0.00001998
Iteration 158/1000 | Loss: 0.00001998
Iteration 159/1000 | Loss: 0.00001998
Iteration 160/1000 | Loss: 0.00001998
Iteration 161/1000 | Loss: 0.00001998
Iteration 162/1000 | Loss: 0.00001998
Iteration 163/1000 | Loss: 0.00001998
Iteration 164/1000 | Loss: 0.00001998
Iteration 165/1000 | Loss: 0.00001998
Iteration 166/1000 | Loss: 0.00001998
Iteration 167/1000 | Loss: 0.00001998
Iteration 168/1000 | Loss: 0.00001998
Iteration 169/1000 | Loss: 0.00001998
Iteration 170/1000 | Loss: 0.00001998
Iteration 171/1000 | Loss: 0.00001998
Iteration 172/1000 | Loss: 0.00001998
Iteration 173/1000 | Loss: 0.00001997
Iteration 174/1000 | Loss: 0.00001997
Iteration 175/1000 | Loss: 0.00001997
Iteration 176/1000 | Loss: 0.00001997
Iteration 177/1000 | Loss: 0.00001997
Iteration 178/1000 | Loss: 0.00001997
Iteration 179/1000 | Loss: 0.00001997
Iteration 180/1000 | Loss: 0.00001997
Iteration 181/1000 | Loss: 0.00001996
Iteration 182/1000 | Loss: 0.00001996
Iteration 183/1000 | Loss: 0.00001996
Iteration 184/1000 | Loss: 0.00001996
Iteration 185/1000 | Loss: 0.00001996
Iteration 186/1000 | Loss: 0.00001996
Iteration 187/1000 | Loss: 0.00001996
Iteration 188/1000 | Loss: 0.00001996
Iteration 189/1000 | Loss: 0.00001996
Iteration 190/1000 | Loss: 0.00001996
Iteration 191/1000 | Loss: 0.00001996
Iteration 192/1000 | Loss: 0.00001996
Iteration 193/1000 | Loss: 0.00001996
Iteration 194/1000 | Loss: 0.00001996
Iteration 195/1000 | Loss: 0.00001996
Iteration 196/1000 | Loss: 0.00001996
Iteration 197/1000 | Loss: 0.00001996
Iteration 198/1000 | Loss: 0.00001995
Iteration 199/1000 | Loss: 0.00001995
Iteration 200/1000 | Loss: 0.00001995
Iteration 201/1000 | Loss: 0.00001995
Iteration 202/1000 | Loss: 0.00001995
Iteration 203/1000 | Loss: 0.00001995
Iteration 204/1000 | Loss: 0.00001995
Iteration 205/1000 | Loss: 0.00001995
Iteration 206/1000 | Loss: 0.00001995
Iteration 207/1000 | Loss: 0.00001995
Iteration 208/1000 | Loss: 0.00001995
Iteration 209/1000 | Loss: 0.00001995
Iteration 210/1000 | Loss: 0.00001995
Iteration 211/1000 | Loss: 0.00001995
Iteration 212/1000 | Loss: 0.00001995
Iteration 213/1000 | Loss: 0.00001995
Iteration 214/1000 | Loss: 0.00001995
Iteration 215/1000 | Loss: 0.00001994
Iteration 216/1000 | Loss: 0.00001994
Iteration 217/1000 | Loss: 0.00001994
Iteration 218/1000 | Loss: 0.00001994
Iteration 219/1000 | Loss: 0.00001994
Iteration 220/1000 | Loss: 0.00001994
Iteration 221/1000 | Loss: 0.00001994
Iteration 222/1000 | Loss: 0.00001994
Iteration 223/1000 | Loss: 0.00001994
Iteration 224/1000 | Loss: 0.00001994
Iteration 225/1000 | Loss: 0.00001994
Iteration 226/1000 | Loss: 0.00001994
Iteration 227/1000 | Loss: 0.00001994
Iteration 228/1000 | Loss: 0.00001994
Iteration 229/1000 | Loss: 0.00001994
Iteration 230/1000 | Loss: 0.00001994
Iteration 231/1000 | Loss: 0.00001994
Iteration 232/1000 | Loss: 0.00001994
Iteration 233/1000 | Loss: 0.00001994
Iteration 234/1000 | Loss: 0.00001994
Iteration 235/1000 | Loss: 0.00001994
Iteration 236/1000 | Loss: 0.00001994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.9937642719014548e-05, 1.9937642719014548e-05, 1.9937642719014548e-05, 1.9937642719014548e-05, 1.9937642719014548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9937642719014548e-05

Optimization complete. Final v2v error: 3.787205696105957 mm

Highest mean error: 3.8673794269561768 mm for frame 21

Lowest mean error: 3.6083216667175293 mm for frame 151

Saving results

Total time: 41.1593177318573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826603
Iteration 2/25 | Loss: 0.00120100
Iteration 3/25 | Loss: 0.00108963
Iteration 4/25 | Loss: 0.00107579
Iteration 5/25 | Loss: 0.00107207
Iteration 6/25 | Loss: 0.00107092
Iteration 7/25 | Loss: 0.00107066
Iteration 8/25 | Loss: 0.00107066
Iteration 9/25 | Loss: 0.00107066
Iteration 10/25 | Loss: 0.00107066
Iteration 11/25 | Loss: 0.00107066
Iteration 12/25 | Loss: 0.00107066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010706620523706079, 0.0010706620523706079, 0.0010706620523706079, 0.0010706620523706079, 0.0010706620523706079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010706620523706079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38936436
Iteration 2/25 | Loss: 0.00088341
Iteration 3/25 | Loss: 0.00088341
Iteration 4/25 | Loss: 0.00088340
Iteration 5/25 | Loss: 0.00088340
Iteration 6/25 | Loss: 0.00088340
Iteration 7/25 | Loss: 0.00088340
Iteration 8/25 | Loss: 0.00088340
Iteration 9/25 | Loss: 0.00088340
Iteration 10/25 | Loss: 0.00088340
Iteration 11/25 | Loss: 0.00088340
Iteration 12/25 | Loss: 0.00088340
Iteration 13/25 | Loss: 0.00088340
Iteration 14/25 | Loss: 0.00088340
Iteration 15/25 | Loss: 0.00088340
Iteration 16/25 | Loss: 0.00088340
Iteration 17/25 | Loss: 0.00088340
Iteration 18/25 | Loss: 0.00088340
Iteration 19/25 | Loss: 0.00088340
Iteration 20/25 | Loss: 0.00088340
Iteration 21/25 | Loss: 0.00088340
Iteration 22/25 | Loss: 0.00088340
Iteration 23/25 | Loss: 0.00088340
Iteration 24/25 | Loss: 0.00088340
Iteration 25/25 | Loss: 0.00088340

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088340
Iteration 2/1000 | Loss: 0.00002734
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00001306
Iteration 5/1000 | Loss: 0.00001163
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001043
Iteration 8/1000 | Loss: 0.00001000
Iteration 9/1000 | Loss: 0.00000986
Iteration 10/1000 | Loss: 0.00000983
Iteration 11/1000 | Loss: 0.00000956
Iteration 12/1000 | Loss: 0.00000938
Iteration 13/1000 | Loss: 0.00000937
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000924
Iteration 16/1000 | Loss: 0.00000921
Iteration 17/1000 | Loss: 0.00000920
Iteration 18/1000 | Loss: 0.00000919
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000918
Iteration 21/1000 | Loss: 0.00000917
Iteration 22/1000 | Loss: 0.00000917
Iteration 23/1000 | Loss: 0.00000917
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000916
Iteration 26/1000 | Loss: 0.00000916
Iteration 27/1000 | Loss: 0.00000916
Iteration 28/1000 | Loss: 0.00000916
Iteration 29/1000 | Loss: 0.00000916
Iteration 30/1000 | Loss: 0.00000914
Iteration 31/1000 | Loss: 0.00000913
Iteration 32/1000 | Loss: 0.00000912
Iteration 33/1000 | Loss: 0.00000912
Iteration 34/1000 | Loss: 0.00000912
Iteration 35/1000 | Loss: 0.00000912
Iteration 36/1000 | Loss: 0.00000912
Iteration 37/1000 | Loss: 0.00000911
Iteration 38/1000 | Loss: 0.00000911
Iteration 39/1000 | Loss: 0.00000911
Iteration 40/1000 | Loss: 0.00000910
Iteration 41/1000 | Loss: 0.00000910
Iteration 42/1000 | Loss: 0.00000910
Iteration 43/1000 | Loss: 0.00000909
Iteration 44/1000 | Loss: 0.00000909
Iteration 45/1000 | Loss: 0.00000909
Iteration 46/1000 | Loss: 0.00000909
Iteration 47/1000 | Loss: 0.00000909
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000908
Iteration 50/1000 | Loss: 0.00000908
Iteration 51/1000 | Loss: 0.00000908
Iteration 52/1000 | Loss: 0.00000907
Iteration 53/1000 | Loss: 0.00000907
Iteration 54/1000 | Loss: 0.00000907
Iteration 55/1000 | Loss: 0.00000905
Iteration 56/1000 | Loss: 0.00000905
Iteration 57/1000 | Loss: 0.00000905
Iteration 58/1000 | Loss: 0.00000905
Iteration 59/1000 | Loss: 0.00000904
Iteration 60/1000 | Loss: 0.00000904
Iteration 61/1000 | Loss: 0.00000904
Iteration 62/1000 | Loss: 0.00000903
Iteration 63/1000 | Loss: 0.00000903
Iteration 64/1000 | Loss: 0.00000902
Iteration 65/1000 | Loss: 0.00000902
Iteration 66/1000 | Loss: 0.00000902
Iteration 67/1000 | Loss: 0.00000902
Iteration 68/1000 | Loss: 0.00000902
Iteration 69/1000 | Loss: 0.00000901
Iteration 70/1000 | Loss: 0.00000901
Iteration 71/1000 | Loss: 0.00000901
Iteration 72/1000 | Loss: 0.00000901
Iteration 73/1000 | Loss: 0.00000901
Iteration 74/1000 | Loss: 0.00000901
Iteration 75/1000 | Loss: 0.00000900
Iteration 76/1000 | Loss: 0.00000900
Iteration 77/1000 | Loss: 0.00000900
Iteration 78/1000 | Loss: 0.00000899
Iteration 79/1000 | Loss: 0.00000899
Iteration 80/1000 | Loss: 0.00000899
Iteration 81/1000 | Loss: 0.00000899
Iteration 82/1000 | Loss: 0.00000898
Iteration 83/1000 | Loss: 0.00000898
Iteration 84/1000 | Loss: 0.00000897
Iteration 85/1000 | Loss: 0.00000897
Iteration 86/1000 | Loss: 0.00000897
Iteration 87/1000 | Loss: 0.00000897
Iteration 88/1000 | Loss: 0.00000896
Iteration 89/1000 | Loss: 0.00000896
Iteration 90/1000 | Loss: 0.00000896
Iteration 91/1000 | Loss: 0.00000895
Iteration 92/1000 | Loss: 0.00000895
Iteration 93/1000 | Loss: 0.00000895
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000894
Iteration 96/1000 | Loss: 0.00000894
Iteration 97/1000 | Loss: 0.00000894
Iteration 98/1000 | Loss: 0.00000893
Iteration 99/1000 | Loss: 0.00000893
Iteration 100/1000 | Loss: 0.00000893
Iteration 101/1000 | Loss: 0.00000892
Iteration 102/1000 | Loss: 0.00000892
Iteration 103/1000 | Loss: 0.00000891
Iteration 104/1000 | Loss: 0.00000891
Iteration 105/1000 | Loss: 0.00000891
Iteration 106/1000 | Loss: 0.00000890
Iteration 107/1000 | Loss: 0.00000890
Iteration 108/1000 | Loss: 0.00000890
Iteration 109/1000 | Loss: 0.00000890
Iteration 110/1000 | Loss: 0.00000890
Iteration 111/1000 | Loss: 0.00000890
Iteration 112/1000 | Loss: 0.00000889
Iteration 113/1000 | Loss: 0.00000889
Iteration 114/1000 | Loss: 0.00000889
Iteration 115/1000 | Loss: 0.00000889
Iteration 116/1000 | Loss: 0.00000889
Iteration 117/1000 | Loss: 0.00000888
Iteration 118/1000 | Loss: 0.00000888
Iteration 119/1000 | Loss: 0.00000888
Iteration 120/1000 | Loss: 0.00000888
Iteration 121/1000 | Loss: 0.00000888
Iteration 122/1000 | Loss: 0.00000888
Iteration 123/1000 | Loss: 0.00000888
Iteration 124/1000 | Loss: 0.00000888
Iteration 125/1000 | Loss: 0.00000888
Iteration 126/1000 | Loss: 0.00000888
Iteration 127/1000 | Loss: 0.00000888
Iteration 128/1000 | Loss: 0.00000888
Iteration 129/1000 | Loss: 0.00000888
Iteration 130/1000 | Loss: 0.00000888
Iteration 131/1000 | Loss: 0.00000888
Iteration 132/1000 | Loss: 0.00000888
Iteration 133/1000 | Loss: 0.00000887
Iteration 134/1000 | Loss: 0.00000887
Iteration 135/1000 | Loss: 0.00000887
Iteration 136/1000 | Loss: 0.00000887
Iteration 137/1000 | Loss: 0.00000887
Iteration 138/1000 | Loss: 0.00000887
Iteration 139/1000 | Loss: 0.00000887
Iteration 140/1000 | Loss: 0.00000887
Iteration 141/1000 | Loss: 0.00000887
Iteration 142/1000 | Loss: 0.00000887
Iteration 143/1000 | Loss: 0.00000886
Iteration 144/1000 | Loss: 0.00000886
Iteration 145/1000 | Loss: 0.00000886
Iteration 146/1000 | Loss: 0.00000886
Iteration 147/1000 | Loss: 0.00000886
Iteration 148/1000 | Loss: 0.00000886
Iteration 149/1000 | Loss: 0.00000886
Iteration 150/1000 | Loss: 0.00000886
Iteration 151/1000 | Loss: 0.00000886
Iteration 152/1000 | Loss: 0.00000886
Iteration 153/1000 | Loss: 0.00000886
Iteration 154/1000 | Loss: 0.00000885
Iteration 155/1000 | Loss: 0.00000885
Iteration 156/1000 | Loss: 0.00000885
Iteration 157/1000 | Loss: 0.00000885
Iteration 158/1000 | Loss: 0.00000885
Iteration 159/1000 | Loss: 0.00000885
Iteration 160/1000 | Loss: 0.00000885
Iteration 161/1000 | Loss: 0.00000885
Iteration 162/1000 | Loss: 0.00000885
Iteration 163/1000 | Loss: 0.00000884
Iteration 164/1000 | Loss: 0.00000884
Iteration 165/1000 | Loss: 0.00000884
Iteration 166/1000 | Loss: 0.00000884
Iteration 167/1000 | Loss: 0.00000884
Iteration 168/1000 | Loss: 0.00000884
Iteration 169/1000 | Loss: 0.00000884
Iteration 170/1000 | Loss: 0.00000884
Iteration 171/1000 | Loss: 0.00000884
Iteration 172/1000 | Loss: 0.00000883
Iteration 173/1000 | Loss: 0.00000883
Iteration 174/1000 | Loss: 0.00000883
Iteration 175/1000 | Loss: 0.00000883
Iteration 176/1000 | Loss: 0.00000883
Iteration 177/1000 | Loss: 0.00000883
Iteration 178/1000 | Loss: 0.00000883
Iteration 179/1000 | Loss: 0.00000883
Iteration 180/1000 | Loss: 0.00000883
Iteration 181/1000 | Loss: 0.00000883
Iteration 182/1000 | Loss: 0.00000883
Iteration 183/1000 | Loss: 0.00000883
Iteration 184/1000 | Loss: 0.00000883
Iteration 185/1000 | Loss: 0.00000883
Iteration 186/1000 | Loss: 0.00000883
Iteration 187/1000 | Loss: 0.00000883
Iteration 188/1000 | Loss: 0.00000883
Iteration 189/1000 | Loss: 0.00000883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [8.825983059068676e-06, 8.825983059068676e-06, 8.825983059068676e-06, 8.825983059068676e-06, 8.825983059068676e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.825983059068676e-06

Optimization complete. Final v2v error: 2.5152957439422607 mm

Highest mean error: 3.6773524284362793 mm for frame 89

Lowest mean error: 2.2416062355041504 mm for frame 41

Saving results

Total time: 46.216524600982666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416604
Iteration 2/25 | Loss: 0.00121231
Iteration 3/25 | Loss: 0.00112166
Iteration 4/25 | Loss: 0.00110957
Iteration 5/25 | Loss: 0.00110578
Iteration 6/25 | Loss: 0.00110533
Iteration 7/25 | Loss: 0.00110533
Iteration 8/25 | Loss: 0.00110533
Iteration 9/25 | Loss: 0.00110533
Iteration 10/25 | Loss: 0.00110533
Iteration 11/25 | Loss: 0.00110533
Iteration 12/25 | Loss: 0.00110533
Iteration 13/25 | Loss: 0.00110533
Iteration 14/25 | Loss: 0.00110533
Iteration 15/25 | Loss: 0.00110533
Iteration 16/25 | Loss: 0.00110533
Iteration 17/25 | Loss: 0.00110533
Iteration 18/25 | Loss: 0.00110533
Iteration 19/25 | Loss: 0.00110533
Iteration 20/25 | Loss: 0.00110533
Iteration 21/25 | Loss: 0.00110533
Iteration 22/25 | Loss: 0.00110533
Iteration 23/25 | Loss: 0.00110533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011053313501179218, 0.0011053313501179218, 0.0011053313501179218, 0.0011053313501179218, 0.0011053313501179218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011053313501179218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39510441
Iteration 2/25 | Loss: 0.00099528
Iteration 3/25 | Loss: 0.00099528
Iteration 4/25 | Loss: 0.00099528
Iteration 5/25 | Loss: 0.00099528
Iteration 6/25 | Loss: 0.00099528
Iteration 7/25 | Loss: 0.00099528
Iteration 8/25 | Loss: 0.00099528
Iteration 9/25 | Loss: 0.00099528
Iteration 10/25 | Loss: 0.00099527
Iteration 11/25 | Loss: 0.00099527
Iteration 12/25 | Loss: 0.00099527
Iteration 13/25 | Loss: 0.00099527
Iteration 14/25 | Loss: 0.00099527
Iteration 15/25 | Loss: 0.00099527
Iteration 16/25 | Loss: 0.00099527
Iteration 17/25 | Loss: 0.00099527
Iteration 18/25 | Loss: 0.00099527
Iteration 19/25 | Loss: 0.00099527
Iteration 20/25 | Loss: 0.00099527
Iteration 21/25 | Loss: 0.00099527
Iteration 22/25 | Loss: 0.00099527
Iteration 23/25 | Loss: 0.00099527
Iteration 24/25 | Loss: 0.00099527
Iteration 25/25 | Loss: 0.00099527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099527
Iteration 2/1000 | Loss: 0.00001966
Iteration 3/1000 | Loss: 0.00001492
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001278
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001207
Iteration 10/1000 | Loss: 0.00001188
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001168
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001168
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001168
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001167
Iteration 25/1000 | Loss: 0.00001167
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001162
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001161
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001158
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001155
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001154
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001151
Iteration 60/1000 | Loss: 0.00001151
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001149
Iteration 64/1000 | Loss: 0.00001149
Iteration 65/1000 | Loss: 0.00001149
Iteration 66/1000 | Loss: 0.00001149
Iteration 67/1000 | Loss: 0.00001149
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001147
Iteration 76/1000 | Loss: 0.00001147
Iteration 77/1000 | Loss: 0.00001147
Iteration 78/1000 | Loss: 0.00001146
Iteration 79/1000 | Loss: 0.00001146
Iteration 80/1000 | Loss: 0.00001146
Iteration 81/1000 | Loss: 0.00001145
Iteration 82/1000 | Loss: 0.00001145
Iteration 83/1000 | Loss: 0.00001145
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001144
Iteration 87/1000 | Loss: 0.00001143
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001142
Iteration 92/1000 | Loss: 0.00001142
Iteration 93/1000 | Loss: 0.00001141
Iteration 94/1000 | Loss: 0.00001141
Iteration 95/1000 | Loss: 0.00001141
Iteration 96/1000 | Loss: 0.00001140
Iteration 97/1000 | Loss: 0.00001140
Iteration 98/1000 | Loss: 0.00001140
Iteration 99/1000 | Loss: 0.00001140
Iteration 100/1000 | Loss: 0.00001139
Iteration 101/1000 | Loss: 0.00001139
Iteration 102/1000 | Loss: 0.00001139
Iteration 103/1000 | Loss: 0.00001139
Iteration 104/1000 | Loss: 0.00001138
Iteration 105/1000 | Loss: 0.00001138
Iteration 106/1000 | Loss: 0.00001138
Iteration 107/1000 | Loss: 0.00001138
Iteration 108/1000 | Loss: 0.00001138
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001137
Iteration 113/1000 | Loss: 0.00001137
Iteration 114/1000 | Loss: 0.00001136
Iteration 115/1000 | Loss: 0.00001136
Iteration 116/1000 | Loss: 0.00001136
Iteration 117/1000 | Loss: 0.00001136
Iteration 118/1000 | Loss: 0.00001136
Iteration 119/1000 | Loss: 0.00001136
Iteration 120/1000 | Loss: 0.00001136
Iteration 121/1000 | Loss: 0.00001135
Iteration 122/1000 | Loss: 0.00001135
Iteration 123/1000 | Loss: 0.00001135
Iteration 124/1000 | Loss: 0.00001135
Iteration 125/1000 | Loss: 0.00001135
Iteration 126/1000 | Loss: 0.00001135
Iteration 127/1000 | Loss: 0.00001135
Iteration 128/1000 | Loss: 0.00001135
Iteration 129/1000 | Loss: 0.00001135
Iteration 130/1000 | Loss: 0.00001135
Iteration 131/1000 | Loss: 0.00001135
Iteration 132/1000 | Loss: 0.00001135
Iteration 133/1000 | Loss: 0.00001134
Iteration 134/1000 | Loss: 0.00001134
Iteration 135/1000 | Loss: 0.00001134
Iteration 136/1000 | Loss: 0.00001134
Iteration 137/1000 | Loss: 0.00001134
Iteration 138/1000 | Loss: 0.00001134
Iteration 139/1000 | Loss: 0.00001134
Iteration 140/1000 | Loss: 0.00001134
Iteration 141/1000 | Loss: 0.00001134
Iteration 142/1000 | Loss: 0.00001134
Iteration 143/1000 | Loss: 0.00001134
Iteration 144/1000 | Loss: 0.00001134
Iteration 145/1000 | Loss: 0.00001134
Iteration 146/1000 | Loss: 0.00001134
Iteration 147/1000 | Loss: 0.00001134
Iteration 148/1000 | Loss: 0.00001134
Iteration 149/1000 | Loss: 0.00001134
Iteration 150/1000 | Loss: 0.00001134
Iteration 151/1000 | Loss: 0.00001134
Iteration 152/1000 | Loss: 0.00001133
Iteration 153/1000 | Loss: 0.00001133
Iteration 154/1000 | Loss: 0.00001133
Iteration 155/1000 | Loss: 0.00001133
Iteration 156/1000 | Loss: 0.00001133
Iteration 157/1000 | Loss: 0.00001133
Iteration 158/1000 | Loss: 0.00001133
Iteration 159/1000 | Loss: 0.00001133
Iteration 160/1000 | Loss: 0.00001133
Iteration 161/1000 | Loss: 0.00001133
Iteration 162/1000 | Loss: 0.00001133
Iteration 163/1000 | Loss: 0.00001133
Iteration 164/1000 | Loss: 0.00001133
Iteration 165/1000 | Loss: 0.00001133
Iteration 166/1000 | Loss: 0.00001133
Iteration 167/1000 | Loss: 0.00001133
Iteration 168/1000 | Loss: 0.00001133
Iteration 169/1000 | Loss: 0.00001133
Iteration 170/1000 | Loss: 0.00001133
Iteration 171/1000 | Loss: 0.00001133
Iteration 172/1000 | Loss: 0.00001133
Iteration 173/1000 | Loss: 0.00001133
Iteration 174/1000 | Loss: 0.00001133
Iteration 175/1000 | Loss: 0.00001133
Iteration 176/1000 | Loss: 0.00001133
Iteration 177/1000 | Loss: 0.00001133
Iteration 178/1000 | Loss: 0.00001133
Iteration 179/1000 | Loss: 0.00001133
Iteration 180/1000 | Loss: 0.00001133
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001133
Iteration 183/1000 | Loss: 0.00001133
Iteration 184/1000 | Loss: 0.00001133
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001133
Iteration 187/1000 | Loss: 0.00001133
Iteration 188/1000 | Loss: 0.00001133
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001133
Iteration 194/1000 | Loss: 0.00001133
Iteration 195/1000 | Loss: 0.00001133
Iteration 196/1000 | Loss: 0.00001133
Iteration 197/1000 | Loss: 0.00001133
Iteration 198/1000 | Loss: 0.00001133
Iteration 199/1000 | Loss: 0.00001133
Iteration 200/1000 | Loss: 0.00001133
Iteration 201/1000 | Loss: 0.00001133
Iteration 202/1000 | Loss: 0.00001133
Iteration 203/1000 | Loss: 0.00001133
Iteration 204/1000 | Loss: 0.00001133
Iteration 205/1000 | Loss: 0.00001133
Iteration 206/1000 | Loss: 0.00001133
Iteration 207/1000 | Loss: 0.00001133
Iteration 208/1000 | Loss: 0.00001133
Iteration 209/1000 | Loss: 0.00001133
Iteration 210/1000 | Loss: 0.00001133
Iteration 211/1000 | Loss: 0.00001133
Iteration 212/1000 | Loss: 0.00001133
Iteration 213/1000 | Loss: 0.00001133
Iteration 214/1000 | Loss: 0.00001133
Iteration 215/1000 | Loss: 0.00001133
Iteration 216/1000 | Loss: 0.00001133
Iteration 217/1000 | Loss: 0.00001133
Iteration 218/1000 | Loss: 0.00001133
Iteration 219/1000 | Loss: 0.00001133
Iteration 220/1000 | Loss: 0.00001133
Iteration 221/1000 | Loss: 0.00001133
Iteration 222/1000 | Loss: 0.00001133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.1327276297379285e-05, 1.1327276297379285e-05, 1.1327276297379285e-05, 1.1327276297379285e-05, 1.1327276297379285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1327276297379285e-05

Optimization complete. Final v2v error: 2.817899227142334 mm

Highest mean error: 2.9627206325531006 mm for frame 172

Lowest mean error: 2.6567890644073486 mm for frame 50

Saving results

Total time: 38.17093801498413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398672
Iteration 2/25 | Loss: 0.00115693
Iteration 3/25 | Loss: 0.00106465
Iteration 4/25 | Loss: 0.00105865
Iteration 5/25 | Loss: 0.00105721
Iteration 6/25 | Loss: 0.00105677
Iteration 7/25 | Loss: 0.00105677
Iteration 8/25 | Loss: 0.00105677
Iteration 9/25 | Loss: 0.00105677
Iteration 10/25 | Loss: 0.00105677
Iteration 11/25 | Loss: 0.00105677
Iteration 12/25 | Loss: 0.00105677
Iteration 13/25 | Loss: 0.00105677
Iteration 14/25 | Loss: 0.00105677
Iteration 15/25 | Loss: 0.00105677
Iteration 16/25 | Loss: 0.00105677
Iteration 17/25 | Loss: 0.00105677
Iteration 18/25 | Loss: 0.00105677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001056767301633954, 0.001056767301633954, 0.001056767301633954, 0.001056767301633954, 0.001056767301633954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001056767301633954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35982370
Iteration 2/25 | Loss: 0.00091917
Iteration 3/25 | Loss: 0.00091916
Iteration 4/25 | Loss: 0.00091916
Iteration 5/25 | Loss: 0.00091916
Iteration 6/25 | Loss: 0.00091916
Iteration 7/25 | Loss: 0.00091916
Iteration 8/25 | Loss: 0.00091916
Iteration 9/25 | Loss: 0.00091916
Iteration 10/25 | Loss: 0.00091916
Iteration 11/25 | Loss: 0.00091916
Iteration 12/25 | Loss: 0.00091916
Iteration 13/25 | Loss: 0.00091916
Iteration 14/25 | Loss: 0.00091916
Iteration 15/25 | Loss: 0.00091916
Iteration 16/25 | Loss: 0.00091916
Iteration 17/25 | Loss: 0.00091916
Iteration 18/25 | Loss: 0.00091916
Iteration 19/25 | Loss: 0.00091916
Iteration 20/25 | Loss: 0.00091916
Iteration 21/25 | Loss: 0.00091916
Iteration 22/25 | Loss: 0.00091916
Iteration 23/25 | Loss: 0.00091916
Iteration 24/25 | Loss: 0.00091916
Iteration 25/25 | Loss: 0.00091916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091916
Iteration 2/1000 | Loss: 0.00002459
Iteration 3/1000 | Loss: 0.00001492
Iteration 4/1000 | Loss: 0.00001136
Iteration 5/1000 | Loss: 0.00001032
Iteration 6/1000 | Loss: 0.00000972
Iteration 7/1000 | Loss: 0.00000937
Iteration 8/1000 | Loss: 0.00000904
Iteration 9/1000 | Loss: 0.00000883
Iteration 10/1000 | Loss: 0.00000882
Iteration 11/1000 | Loss: 0.00000881
Iteration 12/1000 | Loss: 0.00000880
Iteration 13/1000 | Loss: 0.00000879
Iteration 14/1000 | Loss: 0.00000878
Iteration 15/1000 | Loss: 0.00000873
Iteration 16/1000 | Loss: 0.00000859
Iteration 17/1000 | Loss: 0.00000856
Iteration 18/1000 | Loss: 0.00000852
Iteration 19/1000 | Loss: 0.00000851
Iteration 20/1000 | Loss: 0.00000850
Iteration 21/1000 | Loss: 0.00000845
Iteration 22/1000 | Loss: 0.00000843
Iteration 23/1000 | Loss: 0.00000842
Iteration 24/1000 | Loss: 0.00000834
Iteration 25/1000 | Loss: 0.00000834
Iteration 26/1000 | Loss: 0.00000833
Iteration 27/1000 | Loss: 0.00000832
Iteration 28/1000 | Loss: 0.00000832
Iteration 29/1000 | Loss: 0.00000831
Iteration 30/1000 | Loss: 0.00000831
Iteration 31/1000 | Loss: 0.00000831
Iteration 32/1000 | Loss: 0.00000830
Iteration 33/1000 | Loss: 0.00000830
Iteration 34/1000 | Loss: 0.00000830
Iteration 35/1000 | Loss: 0.00000829
Iteration 36/1000 | Loss: 0.00000828
Iteration 37/1000 | Loss: 0.00000828
Iteration 38/1000 | Loss: 0.00000827
Iteration 39/1000 | Loss: 0.00000827
Iteration 40/1000 | Loss: 0.00000827
Iteration 41/1000 | Loss: 0.00000827
Iteration 42/1000 | Loss: 0.00000827
Iteration 43/1000 | Loss: 0.00000827
Iteration 44/1000 | Loss: 0.00000827
Iteration 45/1000 | Loss: 0.00000827
Iteration 46/1000 | Loss: 0.00000827
Iteration 47/1000 | Loss: 0.00000827
Iteration 48/1000 | Loss: 0.00000827
Iteration 49/1000 | Loss: 0.00000827
Iteration 50/1000 | Loss: 0.00000827
Iteration 51/1000 | Loss: 0.00000827
Iteration 52/1000 | Loss: 0.00000827
Iteration 53/1000 | Loss: 0.00000827
Iteration 54/1000 | Loss: 0.00000827
Iteration 55/1000 | Loss: 0.00000827
Iteration 56/1000 | Loss: 0.00000827
Iteration 57/1000 | Loss: 0.00000827
Iteration 58/1000 | Loss: 0.00000827
Iteration 59/1000 | Loss: 0.00000827
Iteration 60/1000 | Loss: 0.00000827
Iteration 61/1000 | Loss: 0.00000827
Iteration 62/1000 | Loss: 0.00000827
Iteration 63/1000 | Loss: 0.00000827
Iteration 64/1000 | Loss: 0.00000827
Iteration 65/1000 | Loss: 0.00000827
Iteration 66/1000 | Loss: 0.00000827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [8.266421900771093e-06, 8.266421900771093e-06, 8.266421900771093e-06, 8.266421900771093e-06, 8.266421900771093e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.266421900771093e-06

Optimization complete. Final v2v error: 2.461627244949341 mm

Highest mean error: 3.4150052070617676 mm for frame 61

Lowest mean error: 2.278707504272461 mm for frame 88

Saving results

Total time: 30.085704565048218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436997
Iteration 2/25 | Loss: 0.00124435
Iteration 3/25 | Loss: 0.00112053
Iteration 4/25 | Loss: 0.00110895
Iteration 5/25 | Loss: 0.00110530
Iteration 6/25 | Loss: 0.00110445
Iteration 7/25 | Loss: 0.00110445
Iteration 8/25 | Loss: 0.00110445
Iteration 9/25 | Loss: 0.00110445
Iteration 10/25 | Loss: 0.00110445
Iteration 11/25 | Loss: 0.00110445
Iteration 12/25 | Loss: 0.00110445
Iteration 13/25 | Loss: 0.00110445
Iteration 14/25 | Loss: 0.00110445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011044531129300594, 0.0011044531129300594, 0.0011044531129300594, 0.0011044531129300594, 0.0011044531129300594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011044531129300594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38257575
Iteration 2/25 | Loss: 0.00087924
Iteration 3/25 | Loss: 0.00087923
Iteration 4/25 | Loss: 0.00087923
Iteration 5/25 | Loss: 0.00087923
Iteration 6/25 | Loss: 0.00087923
Iteration 7/25 | Loss: 0.00087923
Iteration 8/25 | Loss: 0.00087923
Iteration 9/25 | Loss: 0.00087923
Iteration 10/25 | Loss: 0.00087923
Iteration 11/25 | Loss: 0.00087923
Iteration 12/25 | Loss: 0.00087923
Iteration 13/25 | Loss: 0.00087923
Iteration 14/25 | Loss: 0.00087923
Iteration 15/25 | Loss: 0.00087923
Iteration 16/25 | Loss: 0.00087923
Iteration 17/25 | Loss: 0.00087923
Iteration 18/25 | Loss: 0.00087923
Iteration 19/25 | Loss: 0.00087923
Iteration 20/25 | Loss: 0.00087923
Iteration 21/25 | Loss: 0.00087923
Iteration 22/25 | Loss: 0.00087923
Iteration 23/25 | Loss: 0.00087923
Iteration 24/25 | Loss: 0.00087923
Iteration 25/25 | Loss: 0.00087923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087923
Iteration 2/1000 | Loss: 0.00001817
Iteration 3/1000 | Loss: 0.00001440
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001298
Iteration 6/1000 | Loss: 0.00001268
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001259
Iteration 9/1000 | Loss: 0.00001257
Iteration 10/1000 | Loss: 0.00001240
Iteration 11/1000 | Loss: 0.00001215
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001187
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001184
Iteration 18/1000 | Loss: 0.00001183
Iteration 19/1000 | Loss: 0.00001183
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001177
Iteration 34/1000 | Loss: 0.00001177
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001176
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001171
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001160
Iteration 73/1000 | Loss: 0.00001160
Iteration 74/1000 | Loss: 0.00001160
Iteration 75/1000 | Loss: 0.00001160
Iteration 76/1000 | Loss: 0.00001160
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001159
Iteration 79/1000 | Loss: 0.00001159
Iteration 80/1000 | Loss: 0.00001159
Iteration 81/1000 | Loss: 0.00001159
Iteration 82/1000 | Loss: 0.00001159
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001156
Iteration 88/1000 | Loss: 0.00001156
Iteration 89/1000 | Loss: 0.00001156
Iteration 90/1000 | Loss: 0.00001155
Iteration 91/1000 | Loss: 0.00001155
Iteration 92/1000 | Loss: 0.00001150
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001149
Iteration 97/1000 | Loss: 0.00001149
Iteration 98/1000 | Loss: 0.00001149
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001143
Iteration 116/1000 | Loss: 0.00001143
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001143
Iteration 119/1000 | Loss: 0.00001143
Iteration 120/1000 | Loss: 0.00001143
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001142
Iteration 125/1000 | Loss: 0.00001142
Iteration 126/1000 | Loss: 0.00001142
Iteration 127/1000 | Loss: 0.00001142
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001141
Iteration 134/1000 | Loss: 0.00001141
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Iteration 143/1000 | Loss: 0.00001140
Iteration 144/1000 | Loss: 0.00001140
Iteration 145/1000 | Loss: 0.00001140
Iteration 146/1000 | Loss: 0.00001140
Iteration 147/1000 | Loss: 0.00001140
Iteration 148/1000 | Loss: 0.00001140
Iteration 149/1000 | Loss: 0.00001140
Iteration 150/1000 | Loss: 0.00001140
Iteration 151/1000 | Loss: 0.00001140
Iteration 152/1000 | Loss: 0.00001140
Iteration 153/1000 | Loss: 0.00001140
Iteration 154/1000 | Loss: 0.00001140
Iteration 155/1000 | Loss: 0.00001140
Iteration 156/1000 | Loss: 0.00001140
Iteration 157/1000 | Loss: 0.00001140
Iteration 158/1000 | Loss: 0.00001140
Iteration 159/1000 | Loss: 0.00001140
Iteration 160/1000 | Loss: 0.00001140
Iteration 161/1000 | Loss: 0.00001140
Iteration 162/1000 | Loss: 0.00001140
Iteration 163/1000 | Loss: 0.00001140
Iteration 164/1000 | Loss: 0.00001140
Iteration 165/1000 | Loss: 0.00001140
Iteration 166/1000 | Loss: 0.00001140
Iteration 167/1000 | Loss: 0.00001140
Iteration 168/1000 | Loss: 0.00001140
Iteration 169/1000 | Loss: 0.00001140
Iteration 170/1000 | Loss: 0.00001140
Iteration 171/1000 | Loss: 0.00001140
Iteration 172/1000 | Loss: 0.00001140
Iteration 173/1000 | Loss: 0.00001140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.13951527964673e-05, 1.13951527964673e-05, 1.13951527964673e-05, 1.13951527964673e-05, 1.13951527964673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.13951527964673e-05

Optimization complete. Final v2v error: 2.8045308589935303 mm

Highest mean error: 2.978774070739746 mm for frame 134

Lowest mean error: 2.571335792541504 mm for frame 232

Saving results

Total time: 39.09865164756775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787090
Iteration 2/25 | Loss: 0.00135836
Iteration 3/25 | Loss: 0.00118793
Iteration 4/25 | Loss: 0.00117784
Iteration 5/25 | Loss: 0.00117717
Iteration 6/25 | Loss: 0.00117717
Iteration 7/25 | Loss: 0.00117717
Iteration 8/25 | Loss: 0.00117717
Iteration 9/25 | Loss: 0.00117717
Iteration 10/25 | Loss: 0.00117717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011771711288020015, 0.0011771711288020015, 0.0011771711288020015, 0.0011771711288020015, 0.0011771711288020015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011771711288020015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32576156
Iteration 2/25 | Loss: 0.00056570
Iteration 3/25 | Loss: 0.00056570
Iteration 4/25 | Loss: 0.00056570
Iteration 5/25 | Loss: 0.00056570
Iteration 6/25 | Loss: 0.00056570
Iteration 7/25 | Loss: 0.00056570
Iteration 8/25 | Loss: 0.00056569
Iteration 9/25 | Loss: 0.00056569
Iteration 10/25 | Loss: 0.00056569
Iteration 11/25 | Loss: 0.00056569
Iteration 12/25 | Loss: 0.00056569
Iteration 13/25 | Loss: 0.00056569
Iteration 14/25 | Loss: 0.00056569
Iteration 15/25 | Loss: 0.00056569
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000565694528631866, 0.000565694528631866, 0.000565694528631866, 0.000565694528631866, 0.000565694528631866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000565694528631866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056569
Iteration 2/1000 | Loss: 0.00002914
Iteration 3/1000 | Loss: 0.00002248
Iteration 4/1000 | Loss: 0.00002078
Iteration 5/1000 | Loss: 0.00001976
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001807
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00001779
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001774
Iteration 17/1000 | Loss: 0.00001773
Iteration 18/1000 | Loss: 0.00001772
Iteration 19/1000 | Loss: 0.00001771
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001769
Iteration 22/1000 | Loss: 0.00001768
Iteration 23/1000 | Loss: 0.00001768
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001768
Iteration 26/1000 | Loss: 0.00001766
Iteration 27/1000 | Loss: 0.00001766
Iteration 28/1000 | Loss: 0.00001766
Iteration 29/1000 | Loss: 0.00001766
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001765
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001764
Iteration 35/1000 | Loss: 0.00001763
Iteration 36/1000 | Loss: 0.00001763
Iteration 37/1000 | Loss: 0.00001763
Iteration 38/1000 | Loss: 0.00001763
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001763
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001762
Iteration 48/1000 | Loss: 0.00001762
Iteration 49/1000 | Loss: 0.00001762
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001762
Iteration 52/1000 | Loss: 0.00001762
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001761
Iteration 55/1000 | Loss: 0.00001761
Iteration 56/1000 | Loss: 0.00001760
Iteration 57/1000 | Loss: 0.00001760
Iteration 58/1000 | Loss: 0.00001759
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001757
Iteration 64/1000 | Loss: 0.00001757
Iteration 65/1000 | Loss: 0.00001757
Iteration 66/1000 | Loss: 0.00001757
Iteration 67/1000 | Loss: 0.00001757
Iteration 68/1000 | Loss: 0.00001757
Iteration 69/1000 | Loss: 0.00001757
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001755
Iteration 79/1000 | Loss: 0.00001755
Iteration 80/1000 | Loss: 0.00001755
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001752
Iteration 92/1000 | Loss: 0.00001752
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001752
Iteration 104/1000 | Loss: 0.00001752
Iteration 105/1000 | Loss: 0.00001752
Iteration 106/1000 | Loss: 0.00001752
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001751
Iteration 110/1000 | Loss: 0.00001751
Iteration 111/1000 | Loss: 0.00001751
Iteration 112/1000 | Loss: 0.00001751
Iteration 113/1000 | Loss: 0.00001751
Iteration 114/1000 | Loss: 0.00001751
Iteration 115/1000 | Loss: 0.00001751
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001751
Iteration 118/1000 | Loss: 0.00001751
Iteration 119/1000 | Loss: 0.00001750
Iteration 120/1000 | Loss: 0.00001750
Iteration 121/1000 | Loss: 0.00001750
Iteration 122/1000 | Loss: 0.00001750
Iteration 123/1000 | Loss: 0.00001750
Iteration 124/1000 | Loss: 0.00001750
Iteration 125/1000 | Loss: 0.00001750
Iteration 126/1000 | Loss: 0.00001750
Iteration 127/1000 | Loss: 0.00001750
Iteration 128/1000 | Loss: 0.00001750
Iteration 129/1000 | Loss: 0.00001750
Iteration 130/1000 | Loss: 0.00001750
Iteration 131/1000 | Loss: 0.00001750
Iteration 132/1000 | Loss: 0.00001750
Iteration 133/1000 | Loss: 0.00001750
Iteration 134/1000 | Loss: 0.00001750
Iteration 135/1000 | Loss: 0.00001750
Iteration 136/1000 | Loss: 0.00001750
Iteration 137/1000 | Loss: 0.00001750
Iteration 138/1000 | Loss: 0.00001750
Iteration 139/1000 | Loss: 0.00001750
Iteration 140/1000 | Loss: 0.00001750
Iteration 141/1000 | Loss: 0.00001750
Iteration 142/1000 | Loss: 0.00001750
Iteration 143/1000 | Loss: 0.00001750
Iteration 144/1000 | Loss: 0.00001750
Iteration 145/1000 | Loss: 0.00001750
Iteration 146/1000 | Loss: 0.00001750
Iteration 147/1000 | Loss: 0.00001750
Iteration 148/1000 | Loss: 0.00001750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.750234696373809e-05, 1.750234696373809e-05, 1.750234696373809e-05, 1.750234696373809e-05, 1.750234696373809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.750234696373809e-05

Optimization complete. Final v2v error: 3.484144687652588 mm

Highest mean error: 3.7388124465942383 mm for frame 55

Lowest mean error: 3.2657811641693115 mm for frame 119

Saving results

Total time: 34.41058564186096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052225
Iteration 2/25 | Loss: 0.01052225
Iteration 3/25 | Loss: 0.00234825
Iteration 4/25 | Loss: 0.00148665
Iteration 5/25 | Loss: 0.00139972
Iteration 6/25 | Loss: 0.00135161
Iteration 7/25 | Loss: 0.00131373
Iteration 8/25 | Loss: 0.00128608
Iteration 9/25 | Loss: 0.00120580
Iteration 10/25 | Loss: 0.00118763
Iteration 11/25 | Loss: 0.00117156
Iteration 12/25 | Loss: 0.00115720
Iteration 13/25 | Loss: 0.00114208
Iteration 14/25 | Loss: 0.00114094
Iteration 15/25 | Loss: 0.00113869
Iteration 16/25 | Loss: 0.00112732
Iteration 17/25 | Loss: 0.00112840
Iteration 18/25 | Loss: 0.00112979
Iteration 19/25 | Loss: 0.00112244
Iteration 20/25 | Loss: 0.00113314
Iteration 21/25 | Loss: 0.00112184
Iteration 22/25 | Loss: 0.00112160
Iteration 23/25 | Loss: 0.00111882
Iteration 24/25 | Loss: 0.00111875
Iteration 25/25 | Loss: 0.00111875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30182791
Iteration 2/25 | Loss: 0.00111899
Iteration 3/25 | Loss: 0.00101671
Iteration 4/25 | Loss: 0.00101671
Iteration 5/25 | Loss: 0.00101671
Iteration 6/25 | Loss: 0.00101671
Iteration 7/25 | Loss: 0.00101671
Iteration 8/25 | Loss: 0.00101671
Iteration 9/25 | Loss: 0.00101671
Iteration 10/25 | Loss: 0.00101671
Iteration 11/25 | Loss: 0.00101671
Iteration 12/25 | Loss: 0.00101671
Iteration 13/25 | Loss: 0.00101671
Iteration 14/25 | Loss: 0.00101671
Iteration 15/25 | Loss: 0.00101671
Iteration 16/25 | Loss: 0.00101671
Iteration 17/25 | Loss: 0.00101671
Iteration 18/25 | Loss: 0.00101671
Iteration 19/25 | Loss: 0.00101671
Iteration 20/25 | Loss: 0.00101671
Iteration 21/25 | Loss: 0.00101671
Iteration 22/25 | Loss: 0.00101671
Iteration 23/25 | Loss: 0.00101671
Iteration 24/25 | Loss: 0.00101671
Iteration 25/25 | Loss: 0.00101671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101671
Iteration 2/1000 | Loss: 0.00007959
Iteration 3/1000 | Loss: 0.00006156
Iteration 4/1000 | Loss: 0.00005284
Iteration 5/1000 | Loss: 0.00004817
Iteration 6/1000 | Loss: 0.00004649
Iteration 7/1000 | Loss: 0.00004460
Iteration 8/1000 | Loss: 0.00004326
Iteration 9/1000 | Loss: 0.00004183
Iteration 10/1000 | Loss: 0.00004080
Iteration 11/1000 | Loss: 0.00003957
Iteration 12/1000 | Loss: 0.00061267
Iteration 13/1000 | Loss: 0.00385261
Iteration 14/1000 | Loss: 0.00221076
Iteration 15/1000 | Loss: 0.00016899
Iteration 16/1000 | Loss: 0.00013942
Iteration 17/1000 | Loss: 0.00009415
Iteration 18/1000 | Loss: 0.00004700
Iteration 19/1000 | Loss: 0.00007267
Iteration 20/1000 | Loss: 0.00054431
Iteration 21/1000 | Loss: 0.00005853
Iteration 22/1000 | Loss: 0.00005754
Iteration 23/1000 | Loss: 0.00002036
Iteration 24/1000 | Loss: 0.00002274
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00006036
Iteration 27/1000 | Loss: 0.00007004
Iteration 28/1000 | Loss: 0.00007235
Iteration 29/1000 | Loss: 0.00008515
Iteration 30/1000 | Loss: 0.00001286
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00000957
Iteration 34/1000 | Loss: 0.00000915
Iteration 35/1000 | Loss: 0.00000896
Iteration 36/1000 | Loss: 0.00000876
Iteration 37/1000 | Loss: 0.00000868
Iteration 38/1000 | Loss: 0.00000868
Iteration 39/1000 | Loss: 0.00000867
Iteration 40/1000 | Loss: 0.00000866
Iteration 41/1000 | Loss: 0.00000857
Iteration 42/1000 | Loss: 0.00000857
Iteration 43/1000 | Loss: 0.00000851
Iteration 44/1000 | Loss: 0.00000838
Iteration 45/1000 | Loss: 0.00000834
Iteration 46/1000 | Loss: 0.00000832
Iteration 47/1000 | Loss: 0.00000832
Iteration 48/1000 | Loss: 0.00000831
Iteration 49/1000 | Loss: 0.00000831
Iteration 50/1000 | Loss: 0.00000830
Iteration 51/1000 | Loss: 0.00000829
Iteration 52/1000 | Loss: 0.00000829
Iteration 53/1000 | Loss: 0.00000828
Iteration 54/1000 | Loss: 0.00000827
Iteration 55/1000 | Loss: 0.00000827
Iteration 56/1000 | Loss: 0.00000826
Iteration 57/1000 | Loss: 0.00000826
Iteration 58/1000 | Loss: 0.00000826
Iteration 59/1000 | Loss: 0.00000825
Iteration 60/1000 | Loss: 0.00000825
Iteration 61/1000 | Loss: 0.00000825
Iteration 62/1000 | Loss: 0.00000824
Iteration 63/1000 | Loss: 0.00000824
Iteration 64/1000 | Loss: 0.00000824
Iteration 65/1000 | Loss: 0.00000824
Iteration 66/1000 | Loss: 0.00000824
Iteration 67/1000 | Loss: 0.00000824
Iteration 68/1000 | Loss: 0.00000824
Iteration 69/1000 | Loss: 0.00000823
Iteration 70/1000 | Loss: 0.00000823
Iteration 71/1000 | Loss: 0.00000823
Iteration 72/1000 | Loss: 0.00000823
Iteration 73/1000 | Loss: 0.00000823
Iteration 74/1000 | Loss: 0.00000823
Iteration 75/1000 | Loss: 0.00000823
Iteration 76/1000 | Loss: 0.00000823
Iteration 77/1000 | Loss: 0.00000822
Iteration 78/1000 | Loss: 0.00000822
Iteration 79/1000 | Loss: 0.00000822
Iteration 80/1000 | Loss: 0.00000821
Iteration 81/1000 | Loss: 0.00000821
Iteration 82/1000 | Loss: 0.00000821
Iteration 83/1000 | Loss: 0.00000821
Iteration 84/1000 | Loss: 0.00000821
Iteration 85/1000 | Loss: 0.00000821
Iteration 86/1000 | Loss: 0.00000820
Iteration 87/1000 | Loss: 0.00000820
Iteration 88/1000 | Loss: 0.00000820
Iteration 89/1000 | Loss: 0.00000820
Iteration 90/1000 | Loss: 0.00000820
Iteration 91/1000 | Loss: 0.00000820
Iteration 92/1000 | Loss: 0.00000820
Iteration 93/1000 | Loss: 0.00000820
Iteration 94/1000 | Loss: 0.00000820
Iteration 95/1000 | Loss: 0.00000820
Iteration 96/1000 | Loss: 0.00000820
Iteration 97/1000 | Loss: 0.00000820
Iteration 98/1000 | Loss: 0.00000820
Iteration 99/1000 | Loss: 0.00000820
Iteration 100/1000 | Loss: 0.00000819
Iteration 101/1000 | Loss: 0.00000819
Iteration 102/1000 | Loss: 0.00000819
Iteration 103/1000 | Loss: 0.00000819
Iteration 104/1000 | Loss: 0.00000819
Iteration 105/1000 | Loss: 0.00000818
Iteration 106/1000 | Loss: 0.00000818
Iteration 107/1000 | Loss: 0.00000818
Iteration 108/1000 | Loss: 0.00000818
Iteration 109/1000 | Loss: 0.00000818
Iteration 110/1000 | Loss: 0.00000818
Iteration 111/1000 | Loss: 0.00000818
Iteration 112/1000 | Loss: 0.00000818
Iteration 113/1000 | Loss: 0.00000818
Iteration 114/1000 | Loss: 0.00000817
Iteration 115/1000 | Loss: 0.00000817
Iteration 116/1000 | Loss: 0.00000817
Iteration 117/1000 | Loss: 0.00000817
Iteration 118/1000 | Loss: 0.00000817
Iteration 119/1000 | Loss: 0.00000817
Iteration 120/1000 | Loss: 0.00000817
Iteration 121/1000 | Loss: 0.00000817
Iteration 122/1000 | Loss: 0.00000816
Iteration 123/1000 | Loss: 0.00000816
Iteration 124/1000 | Loss: 0.00000816
Iteration 125/1000 | Loss: 0.00000816
Iteration 126/1000 | Loss: 0.00000816
Iteration 127/1000 | Loss: 0.00000816
Iteration 128/1000 | Loss: 0.00000816
Iteration 129/1000 | Loss: 0.00000816
Iteration 130/1000 | Loss: 0.00000816
Iteration 131/1000 | Loss: 0.00000815
Iteration 132/1000 | Loss: 0.00000815
Iteration 133/1000 | Loss: 0.00000815
Iteration 134/1000 | Loss: 0.00000815
Iteration 135/1000 | Loss: 0.00000815
Iteration 136/1000 | Loss: 0.00000815
Iteration 137/1000 | Loss: 0.00000815
Iteration 138/1000 | Loss: 0.00000815
Iteration 139/1000 | Loss: 0.00000815
Iteration 140/1000 | Loss: 0.00000815
Iteration 141/1000 | Loss: 0.00000815
Iteration 142/1000 | Loss: 0.00000815
Iteration 143/1000 | Loss: 0.00000815
Iteration 144/1000 | Loss: 0.00000815
Iteration 145/1000 | Loss: 0.00000815
Iteration 146/1000 | Loss: 0.00000815
Iteration 147/1000 | Loss: 0.00000815
Iteration 148/1000 | Loss: 0.00000815
Iteration 149/1000 | Loss: 0.00000815
Iteration 150/1000 | Loss: 0.00000815
Iteration 151/1000 | Loss: 0.00000815
Iteration 152/1000 | Loss: 0.00000815
Iteration 153/1000 | Loss: 0.00000815
Iteration 154/1000 | Loss: 0.00000815
Iteration 155/1000 | Loss: 0.00000815
Iteration 156/1000 | Loss: 0.00000815
Iteration 157/1000 | Loss: 0.00000815
Iteration 158/1000 | Loss: 0.00000815
Iteration 159/1000 | Loss: 0.00000815
Iteration 160/1000 | Loss: 0.00000815
Iteration 161/1000 | Loss: 0.00000815
Iteration 162/1000 | Loss: 0.00000815
Iteration 163/1000 | Loss: 0.00000815
Iteration 164/1000 | Loss: 0.00000815
Iteration 165/1000 | Loss: 0.00000815
Iteration 166/1000 | Loss: 0.00000815
Iteration 167/1000 | Loss: 0.00000815
Iteration 168/1000 | Loss: 0.00000815
Iteration 169/1000 | Loss: 0.00000815
Iteration 170/1000 | Loss: 0.00000815
Iteration 171/1000 | Loss: 0.00000815
Iteration 172/1000 | Loss: 0.00000815
Iteration 173/1000 | Loss: 0.00000815
Iteration 174/1000 | Loss: 0.00000815
Iteration 175/1000 | Loss: 0.00000815
Iteration 176/1000 | Loss: 0.00000815
Iteration 177/1000 | Loss: 0.00000815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [8.149097993737087e-06, 8.149097993737087e-06, 8.149097993737087e-06, 8.149097993737087e-06, 8.149097993737087e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.149097993737087e-06

Optimization complete. Final v2v error: 2.453383445739746 mm

Highest mean error: 2.5975685119628906 mm for frame 82

Lowest mean error: 2.358867883682251 mm for frame 17

Saving results

Total time: 100.8263750076294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396080
Iteration 2/25 | Loss: 0.00113138
Iteration 3/25 | Loss: 0.00105636
Iteration 4/25 | Loss: 0.00104552
Iteration 5/25 | Loss: 0.00104214
Iteration 6/25 | Loss: 0.00104145
Iteration 7/25 | Loss: 0.00104145
Iteration 8/25 | Loss: 0.00104145
Iteration 9/25 | Loss: 0.00104145
Iteration 10/25 | Loss: 0.00104145
Iteration 11/25 | Loss: 0.00104145
Iteration 12/25 | Loss: 0.00104145
Iteration 13/25 | Loss: 0.00104145
Iteration 14/25 | Loss: 0.00104145
Iteration 15/25 | Loss: 0.00104145
Iteration 16/25 | Loss: 0.00104145
Iteration 17/25 | Loss: 0.00104145
Iteration 18/25 | Loss: 0.00104145
Iteration 19/25 | Loss: 0.00104145
Iteration 20/25 | Loss: 0.00104145
Iteration 21/25 | Loss: 0.00104145
Iteration 22/25 | Loss: 0.00104145
Iteration 23/25 | Loss: 0.00104145
Iteration 24/25 | Loss: 0.00104145
Iteration 25/25 | Loss: 0.00104145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93412125
Iteration 2/25 | Loss: 0.00075370
Iteration 3/25 | Loss: 0.00075370
Iteration 4/25 | Loss: 0.00075370
Iteration 5/25 | Loss: 0.00075370
Iteration 6/25 | Loss: 0.00075370
Iteration 7/25 | Loss: 0.00075370
Iteration 8/25 | Loss: 0.00075370
Iteration 9/25 | Loss: 0.00075370
Iteration 10/25 | Loss: 0.00075370
Iteration 11/25 | Loss: 0.00075370
Iteration 12/25 | Loss: 0.00075370
Iteration 13/25 | Loss: 0.00075370
Iteration 14/25 | Loss: 0.00075370
Iteration 15/25 | Loss: 0.00075370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007536984048783779, 0.0007536984048783779, 0.0007536984048783779, 0.0007536984048783779, 0.0007536984048783779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007536984048783779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075370
Iteration 2/1000 | Loss: 0.00002010
Iteration 3/1000 | Loss: 0.00001377
Iteration 4/1000 | Loss: 0.00001203
Iteration 5/1000 | Loss: 0.00001136
Iteration 6/1000 | Loss: 0.00001089
Iteration 7/1000 | Loss: 0.00001050
Iteration 8/1000 | Loss: 0.00001019
Iteration 9/1000 | Loss: 0.00001000
Iteration 10/1000 | Loss: 0.00000979
Iteration 11/1000 | Loss: 0.00000978
Iteration 12/1000 | Loss: 0.00000969
Iteration 13/1000 | Loss: 0.00000956
Iteration 14/1000 | Loss: 0.00000950
Iteration 15/1000 | Loss: 0.00000950
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000938
Iteration 18/1000 | Loss: 0.00000937
Iteration 19/1000 | Loss: 0.00000937
Iteration 20/1000 | Loss: 0.00000936
Iteration 21/1000 | Loss: 0.00000934
Iteration 22/1000 | Loss: 0.00000934
Iteration 23/1000 | Loss: 0.00000933
Iteration 24/1000 | Loss: 0.00000933
Iteration 25/1000 | Loss: 0.00000932
Iteration 26/1000 | Loss: 0.00000931
Iteration 27/1000 | Loss: 0.00000931
Iteration 28/1000 | Loss: 0.00000929
Iteration 29/1000 | Loss: 0.00000929
Iteration 30/1000 | Loss: 0.00000929
Iteration 31/1000 | Loss: 0.00000929
Iteration 32/1000 | Loss: 0.00000929
Iteration 33/1000 | Loss: 0.00000929
Iteration 34/1000 | Loss: 0.00000929
Iteration 35/1000 | Loss: 0.00000929
Iteration 36/1000 | Loss: 0.00000929
Iteration 37/1000 | Loss: 0.00000929
Iteration 38/1000 | Loss: 0.00000928
Iteration 39/1000 | Loss: 0.00000928
Iteration 40/1000 | Loss: 0.00000928
Iteration 41/1000 | Loss: 0.00000928
Iteration 42/1000 | Loss: 0.00000928
Iteration 43/1000 | Loss: 0.00000928
Iteration 44/1000 | Loss: 0.00000925
Iteration 45/1000 | Loss: 0.00000925
Iteration 46/1000 | Loss: 0.00000924
Iteration 47/1000 | Loss: 0.00000924
Iteration 48/1000 | Loss: 0.00000923
Iteration 49/1000 | Loss: 0.00000923
Iteration 50/1000 | Loss: 0.00000919
Iteration 51/1000 | Loss: 0.00000918
Iteration 52/1000 | Loss: 0.00000916
Iteration 53/1000 | Loss: 0.00000916
Iteration 54/1000 | Loss: 0.00000916
Iteration 55/1000 | Loss: 0.00000916
Iteration 56/1000 | Loss: 0.00000916
Iteration 57/1000 | Loss: 0.00000916
Iteration 58/1000 | Loss: 0.00000916
Iteration 59/1000 | Loss: 0.00000916
Iteration 60/1000 | Loss: 0.00000916
Iteration 61/1000 | Loss: 0.00000915
Iteration 62/1000 | Loss: 0.00000915
Iteration 63/1000 | Loss: 0.00000914
Iteration 64/1000 | Loss: 0.00000914
Iteration 65/1000 | Loss: 0.00000914
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000913
Iteration 70/1000 | Loss: 0.00000913
Iteration 71/1000 | Loss: 0.00000912
Iteration 72/1000 | Loss: 0.00000912
Iteration 73/1000 | Loss: 0.00000912
Iteration 74/1000 | Loss: 0.00000912
Iteration 75/1000 | Loss: 0.00000911
Iteration 76/1000 | Loss: 0.00000911
Iteration 77/1000 | Loss: 0.00000911
Iteration 78/1000 | Loss: 0.00000911
Iteration 79/1000 | Loss: 0.00000911
Iteration 80/1000 | Loss: 0.00000910
Iteration 81/1000 | Loss: 0.00000910
Iteration 82/1000 | Loss: 0.00000910
Iteration 83/1000 | Loss: 0.00000910
Iteration 84/1000 | Loss: 0.00000910
Iteration 85/1000 | Loss: 0.00000909
Iteration 86/1000 | Loss: 0.00000909
Iteration 87/1000 | Loss: 0.00000909
Iteration 88/1000 | Loss: 0.00000909
Iteration 89/1000 | Loss: 0.00000909
Iteration 90/1000 | Loss: 0.00000909
Iteration 91/1000 | Loss: 0.00000909
Iteration 92/1000 | Loss: 0.00000909
Iteration 93/1000 | Loss: 0.00000909
Iteration 94/1000 | Loss: 0.00000909
Iteration 95/1000 | Loss: 0.00000908
Iteration 96/1000 | Loss: 0.00000908
Iteration 97/1000 | Loss: 0.00000908
Iteration 98/1000 | Loss: 0.00000908
Iteration 99/1000 | Loss: 0.00000908
Iteration 100/1000 | Loss: 0.00000908
Iteration 101/1000 | Loss: 0.00000908
Iteration 102/1000 | Loss: 0.00000908
Iteration 103/1000 | Loss: 0.00000908
Iteration 104/1000 | Loss: 0.00000907
Iteration 105/1000 | Loss: 0.00000907
Iteration 106/1000 | Loss: 0.00000907
Iteration 107/1000 | Loss: 0.00000907
Iteration 108/1000 | Loss: 0.00000906
Iteration 109/1000 | Loss: 0.00000906
Iteration 110/1000 | Loss: 0.00000906
Iteration 111/1000 | Loss: 0.00000906
Iteration 112/1000 | Loss: 0.00000906
Iteration 113/1000 | Loss: 0.00000906
Iteration 114/1000 | Loss: 0.00000906
Iteration 115/1000 | Loss: 0.00000905
Iteration 116/1000 | Loss: 0.00000905
Iteration 117/1000 | Loss: 0.00000905
Iteration 118/1000 | Loss: 0.00000905
Iteration 119/1000 | Loss: 0.00000905
Iteration 120/1000 | Loss: 0.00000905
Iteration 121/1000 | Loss: 0.00000904
Iteration 122/1000 | Loss: 0.00000904
Iteration 123/1000 | Loss: 0.00000904
Iteration 124/1000 | Loss: 0.00000903
Iteration 125/1000 | Loss: 0.00000903
Iteration 126/1000 | Loss: 0.00000903
Iteration 127/1000 | Loss: 0.00000903
Iteration 128/1000 | Loss: 0.00000903
Iteration 129/1000 | Loss: 0.00000903
Iteration 130/1000 | Loss: 0.00000903
Iteration 131/1000 | Loss: 0.00000902
Iteration 132/1000 | Loss: 0.00000902
Iteration 133/1000 | Loss: 0.00000902
Iteration 134/1000 | Loss: 0.00000902
Iteration 135/1000 | Loss: 0.00000902
Iteration 136/1000 | Loss: 0.00000902
Iteration 137/1000 | Loss: 0.00000902
Iteration 138/1000 | Loss: 0.00000902
Iteration 139/1000 | Loss: 0.00000902
Iteration 140/1000 | Loss: 0.00000902
Iteration 141/1000 | Loss: 0.00000902
Iteration 142/1000 | Loss: 0.00000901
Iteration 143/1000 | Loss: 0.00000901
Iteration 144/1000 | Loss: 0.00000901
Iteration 145/1000 | Loss: 0.00000901
Iteration 146/1000 | Loss: 0.00000901
Iteration 147/1000 | Loss: 0.00000901
Iteration 148/1000 | Loss: 0.00000901
Iteration 149/1000 | Loss: 0.00000901
Iteration 150/1000 | Loss: 0.00000901
Iteration 151/1000 | Loss: 0.00000901
Iteration 152/1000 | Loss: 0.00000901
Iteration 153/1000 | Loss: 0.00000901
Iteration 154/1000 | Loss: 0.00000901
Iteration 155/1000 | Loss: 0.00000901
Iteration 156/1000 | Loss: 0.00000900
Iteration 157/1000 | Loss: 0.00000900
Iteration 158/1000 | Loss: 0.00000900
Iteration 159/1000 | Loss: 0.00000900
Iteration 160/1000 | Loss: 0.00000900
Iteration 161/1000 | Loss: 0.00000900
Iteration 162/1000 | Loss: 0.00000900
Iteration 163/1000 | Loss: 0.00000900
Iteration 164/1000 | Loss: 0.00000900
Iteration 165/1000 | Loss: 0.00000900
Iteration 166/1000 | Loss: 0.00000899
Iteration 167/1000 | Loss: 0.00000899
Iteration 168/1000 | Loss: 0.00000899
Iteration 169/1000 | Loss: 0.00000899
Iteration 170/1000 | Loss: 0.00000899
Iteration 171/1000 | Loss: 0.00000899
Iteration 172/1000 | Loss: 0.00000899
Iteration 173/1000 | Loss: 0.00000899
Iteration 174/1000 | Loss: 0.00000899
Iteration 175/1000 | Loss: 0.00000899
Iteration 176/1000 | Loss: 0.00000899
Iteration 177/1000 | Loss: 0.00000899
Iteration 178/1000 | Loss: 0.00000899
Iteration 179/1000 | Loss: 0.00000899
Iteration 180/1000 | Loss: 0.00000899
Iteration 181/1000 | Loss: 0.00000899
Iteration 182/1000 | Loss: 0.00000899
Iteration 183/1000 | Loss: 0.00000899
Iteration 184/1000 | Loss: 0.00000899
Iteration 185/1000 | Loss: 0.00000899
Iteration 186/1000 | Loss: 0.00000899
Iteration 187/1000 | Loss: 0.00000899
Iteration 188/1000 | Loss: 0.00000899
Iteration 189/1000 | Loss: 0.00000899
Iteration 190/1000 | Loss: 0.00000899
Iteration 191/1000 | Loss: 0.00000899
Iteration 192/1000 | Loss: 0.00000899
Iteration 193/1000 | Loss: 0.00000899
Iteration 194/1000 | Loss: 0.00000899
Iteration 195/1000 | Loss: 0.00000899
Iteration 196/1000 | Loss: 0.00000899
Iteration 197/1000 | Loss: 0.00000899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [8.991447430162225e-06, 8.991447430162225e-06, 8.991447430162225e-06, 8.991447430162225e-06, 8.991447430162225e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.991447430162225e-06

Optimization complete. Final v2v error: 2.5964784622192383 mm

Highest mean error: 2.9969682693481445 mm for frame 89

Lowest mean error: 2.5018362998962402 mm for frame 136

Saving results

Total time: 39.84840774536133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993608
Iteration 2/25 | Loss: 0.00244366
Iteration 3/25 | Loss: 0.00179786
Iteration 4/25 | Loss: 0.00166060
Iteration 5/25 | Loss: 0.00168928
Iteration 6/25 | Loss: 0.00174635
Iteration 7/25 | Loss: 0.00165975
Iteration 8/25 | Loss: 0.00155434
Iteration 9/25 | Loss: 0.00152098
Iteration 10/25 | Loss: 0.00143978
Iteration 11/25 | Loss: 0.00141579
Iteration 12/25 | Loss: 0.00139684
Iteration 13/25 | Loss: 0.00138853
Iteration 14/25 | Loss: 0.00138880
Iteration 15/25 | Loss: 0.00137008
Iteration 16/25 | Loss: 0.00137363
Iteration 17/25 | Loss: 0.00136950
Iteration 18/25 | Loss: 0.00136665
Iteration 19/25 | Loss: 0.00136074
Iteration 20/25 | Loss: 0.00135577
Iteration 21/25 | Loss: 0.00135047
Iteration 22/25 | Loss: 0.00134644
Iteration 23/25 | Loss: 0.00135007
Iteration 24/25 | Loss: 0.00134958
Iteration 25/25 | Loss: 0.00135155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36387098
Iteration 2/25 | Loss: 0.00279476
Iteration 3/25 | Loss: 0.00261345
Iteration 4/25 | Loss: 0.00261345
Iteration 5/25 | Loss: 0.00261345
Iteration 6/25 | Loss: 0.00261345
Iteration 7/25 | Loss: 0.00261345
Iteration 8/25 | Loss: 0.00261345
Iteration 9/25 | Loss: 0.00261345
Iteration 10/25 | Loss: 0.00261345
Iteration 11/25 | Loss: 0.00261345
Iteration 12/25 | Loss: 0.00261345
Iteration 13/25 | Loss: 0.00261345
Iteration 14/25 | Loss: 0.00261345
Iteration 15/25 | Loss: 0.00261345
Iteration 16/25 | Loss: 0.00261345
Iteration 17/25 | Loss: 0.00261345
Iteration 18/25 | Loss: 0.00261345
Iteration 19/25 | Loss: 0.00261345
Iteration 20/25 | Loss: 0.00261345
Iteration 21/25 | Loss: 0.00261345
Iteration 22/25 | Loss: 0.00261345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0026134455110877752, 0.0026134455110877752, 0.0026134455110877752, 0.0026134455110877752, 0.0026134455110877752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026134455110877752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261345
Iteration 2/1000 | Loss: 0.00141367
Iteration 3/1000 | Loss: 0.00165824
Iteration 4/1000 | Loss: 0.00116598
Iteration 5/1000 | Loss: 0.00049914
Iteration 6/1000 | Loss: 0.00056707
Iteration 7/1000 | Loss: 0.00099553
Iteration 8/1000 | Loss: 0.00029077
Iteration 9/1000 | Loss: 0.00116328
Iteration 10/1000 | Loss: 0.00040857
Iteration 11/1000 | Loss: 0.00117849
Iteration 12/1000 | Loss: 0.00024535
Iteration 13/1000 | Loss: 0.00170104
Iteration 14/1000 | Loss: 0.00036922
Iteration 15/1000 | Loss: 0.00128234
Iteration 16/1000 | Loss: 0.00022451
Iteration 17/1000 | Loss: 0.00020501
Iteration 18/1000 | Loss: 0.00029683
Iteration 19/1000 | Loss: 0.00081790
Iteration 20/1000 | Loss: 0.00022718
Iteration 21/1000 | Loss: 0.00034658
Iteration 22/1000 | Loss: 0.00030749
Iteration 23/1000 | Loss: 0.00035248
Iteration 24/1000 | Loss: 0.00025213
Iteration 25/1000 | Loss: 0.00034957
Iteration 26/1000 | Loss: 0.00021336
Iteration 27/1000 | Loss: 0.00098848
Iteration 28/1000 | Loss: 0.00101386
Iteration 29/1000 | Loss: 0.00084980
Iteration 30/1000 | Loss: 0.00061629
Iteration 31/1000 | Loss: 0.00094974
Iteration 32/1000 | Loss: 0.00043671
Iteration 33/1000 | Loss: 0.00049838
Iteration 34/1000 | Loss: 0.00019314
Iteration 35/1000 | Loss: 0.00037024
Iteration 36/1000 | Loss: 0.00017852
Iteration 37/1000 | Loss: 0.00016188
Iteration 38/1000 | Loss: 0.00013064
Iteration 39/1000 | Loss: 0.00024503
Iteration 40/1000 | Loss: 0.00016744
Iteration 41/1000 | Loss: 0.00029449
Iteration 42/1000 | Loss: 0.00015389
Iteration 43/1000 | Loss: 0.00015452
Iteration 44/1000 | Loss: 0.00012995
Iteration 45/1000 | Loss: 0.00046684
Iteration 46/1000 | Loss: 0.00142539
Iteration 47/1000 | Loss: 0.00067506
Iteration 48/1000 | Loss: 0.00148873
Iteration 49/1000 | Loss: 0.00066294
Iteration 50/1000 | Loss: 0.00114513
Iteration 51/1000 | Loss: 0.00146352
Iteration 52/1000 | Loss: 0.00018218
Iteration 53/1000 | Loss: 0.00015161
Iteration 54/1000 | Loss: 0.00017051
Iteration 55/1000 | Loss: 0.00015949
Iteration 56/1000 | Loss: 0.00012939
Iteration 57/1000 | Loss: 0.00057810
Iteration 58/1000 | Loss: 0.00016891
Iteration 59/1000 | Loss: 0.00064617
Iteration 60/1000 | Loss: 0.00066159
Iteration 61/1000 | Loss: 0.00043648
Iteration 62/1000 | Loss: 0.00046261
Iteration 63/1000 | Loss: 0.00047298
Iteration 64/1000 | Loss: 0.00021509
Iteration 65/1000 | Loss: 0.00054687
Iteration 66/1000 | Loss: 0.00046864
Iteration 67/1000 | Loss: 0.00020021
Iteration 68/1000 | Loss: 0.00011978
Iteration 69/1000 | Loss: 0.00030655
Iteration 70/1000 | Loss: 0.00031185
Iteration 71/1000 | Loss: 0.00017719
Iteration 72/1000 | Loss: 0.00047893
Iteration 73/1000 | Loss: 0.00011000
Iteration 74/1000 | Loss: 0.00013142
Iteration 75/1000 | Loss: 0.00010443
Iteration 76/1000 | Loss: 0.00022466
Iteration 77/1000 | Loss: 0.00014501
Iteration 78/1000 | Loss: 0.00012932
Iteration 79/1000 | Loss: 0.00010679
Iteration 80/1000 | Loss: 0.00010586
Iteration 81/1000 | Loss: 0.00010740
Iteration 82/1000 | Loss: 0.00010230
Iteration 83/1000 | Loss: 0.00011361
Iteration 84/1000 | Loss: 0.00010886
Iteration 85/1000 | Loss: 0.00010479
Iteration 86/1000 | Loss: 0.00010938
Iteration 87/1000 | Loss: 0.00011037
Iteration 88/1000 | Loss: 0.00009402
Iteration 89/1000 | Loss: 0.00009990
Iteration 90/1000 | Loss: 0.00060142
Iteration 91/1000 | Loss: 0.00015818
Iteration 92/1000 | Loss: 0.00018789
Iteration 93/1000 | Loss: 0.00008228
Iteration 94/1000 | Loss: 0.00009172
Iteration 95/1000 | Loss: 0.00054500
Iteration 96/1000 | Loss: 0.00026349
Iteration 97/1000 | Loss: 0.00007770
Iteration 98/1000 | Loss: 0.00012170
Iteration 99/1000 | Loss: 0.00062932
Iteration 100/1000 | Loss: 0.00009623
Iteration 101/1000 | Loss: 0.00047784
Iteration 102/1000 | Loss: 0.00107561
Iteration 103/1000 | Loss: 0.00063101
Iteration 104/1000 | Loss: 0.00029244
Iteration 105/1000 | Loss: 0.00025913
Iteration 106/1000 | Loss: 0.00014948
Iteration 107/1000 | Loss: 0.00047861
Iteration 108/1000 | Loss: 0.00065858
Iteration 109/1000 | Loss: 0.00021034
Iteration 110/1000 | Loss: 0.00081477
Iteration 111/1000 | Loss: 0.00069027
Iteration 112/1000 | Loss: 0.00009409
Iteration 113/1000 | Loss: 0.00009371
Iteration 114/1000 | Loss: 0.00011104
Iteration 115/1000 | Loss: 0.00011093
Iteration 116/1000 | Loss: 0.00008927
Iteration 117/1000 | Loss: 0.00007907
Iteration 118/1000 | Loss: 0.00008433
Iteration 119/1000 | Loss: 0.00009888
Iteration 120/1000 | Loss: 0.00010574
Iteration 121/1000 | Loss: 0.00008765
Iteration 122/1000 | Loss: 0.00008031
Iteration 123/1000 | Loss: 0.00008681
Iteration 124/1000 | Loss: 0.00008259
Iteration 125/1000 | Loss: 0.00008646
Iteration 126/1000 | Loss: 0.00011251
Iteration 127/1000 | Loss: 0.00008885
Iteration 128/1000 | Loss: 0.00009497
Iteration 129/1000 | Loss: 0.00009226
Iteration 130/1000 | Loss: 0.00008642
Iteration 131/1000 | Loss: 0.00008978
Iteration 132/1000 | Loss: 0.00013276
Iteration 133/1000 | Loss: 0.00009799
Iteration 134/1000 | Loss: 0.00008949
Iteration 135/1000 | Loss: 0.00010417
Iteration 136/1000 | Loss: 0.00007945
Iteration 137/1000 | Loss: 0.00008607
Iteration 138/1000 | Loss: 0.00008169
Iteration 139/1000 | Loss: 0.00009230
Iteration 140/1000 | Loss: 0.00008070
Iteration 141/1000 | Loss: 0.00008339
Iteration 142/1000 | Loss: 0.00008908
Iteration 143/1000 | Loss: 0.00008559
Iteration 144/1000 | Loss: 0.00009077
Iteration 145/1000 | Loss: 0.00007727
Iteration 146/1000 | Loss: 0.00007319
Iteration 147/1000 | Loss: 0.00008819
Iteration 148/1000 | Loss: 0.00009279
Iteration 149/1000 | Loss: 0.00008526
Iteration 150/1000 | Loss: 0.00008715
Iteration 151/1000 | Loss: 0.00007896
Iteration 152/1000 | Loss: 0.00009244
Iteration 153/1000 | Loss: 0.00008902
Iteration 154/1000 | Loss: 0.00008991
Iteration 155/1000 | Loss: 0.00008884
Iteration 156/1000 | Loss: 0.00009220
Iteration 157/1000 | Loss: 0.00010268
Iteration 158/1000 | Loss: 0.00010503
Iteration 159/1000 | Loss: 0.00009032
Iteration 160/1000 | Loss: 0.00009339
Iteration 161/1000 | Loss: 0.00008981
Iteration 162/1000 | Loss: 0.00009232
Iteration 163/1000 | Loss: 0.00009007
Iteration 164/1000 | Loss: 0.00009305
Iteration 165/1000 | Loss: 0.00040950
Iteration 166/1000 | Loss: 0.00010484
Iteration 167/1000 | Loss: 0.00009652
Iteration 168/1000 | Loss: 0.00008678
Iteration 169/1000 | Loss: 0.00007280
Iteration 170/1000 | Loss: 0.00005708
Iteration 171/1000 | Loss: 0.00006774
Iteration 172/1000 | Loss: 0.00009138
Iteration 173/1000 | Loss: 0.00009371
Iteration 174/1000 | Loss: 0.00007333
Iteration 175/1000 | Loss: 0.00008018
Iteration 176/1000 | Loss: 0.00053122
Iteration 177/1000 | Loss: 0.00010531
Iteration 178/1000 | Loss: 0.00053939
Iteration 179/1000 | Loss: 0.00009251
Iteration 180/1000 | Loss: 0.00006508
Iteration 181/1000 | Loss: 0.00007245
Iteration 182/1000 | Loss: 0.00006903
Iteration 183/1000 | Loss: 0.00006332
Iteration 184/1000 | Loss: 0.00006461
Iteration 185/1000 | Loss: 0.00006655
Iteration 186/1000 | Loss: 0.00006523
Iteration 187/1000 | Loss: 0.00007561
Iteration 188/1000 | Loss: 0.00006015
Iteration 189/1000 | Loss: 0.00007884
Iteration 190/1000 | Loss: 0.00007313
Iteration 191/1000 | Loss: 0.00006518
Iteration 192/1000 | Loss: 0.00006653
Iteration 193/1000 | Loss: 0.00006654
Iteration 194/1000 | Loss: 0.00006656
Iteration 195/1000 | Loss: 0.00006705
Iteration 196/1000 | Loss: 0.00006797
Iteration 197/1000 | Loss: 0.00007425
Iteration 198/1000 | Loss: 0.00006915
Iteration 199/1000 | Loss: 0.00007319
Iteration 200/1000 | Loss: 0.00006864
Iteration 201/1000 | Loss: 0.00007723
Iteration 202/1000 | Loss: 0.00008133
Iteration 203/1000 | Loss: 0.00007357
Iteration 204/1000 | Loss: 0.00007683
Iteration 205/1000 | Loss: 0.00007717
Iteration 206/1000 | Loss: 0.00007724
Iteration 207/1000 | Loss: 0.00007463
Iteration 208/1000 | Loss: 0.00023756
Iteration 209/1000 | Loss: 0.00017733
Iteration 210/1000 | Loss: 0.00009790
Iteration 211/1000 | Loss: 0.00011214
Iteration 212/1000 | Loss: 0.00005325
Iteration 213/1000 | Loss: 0.00034902
Iteration 214/1000 | Loss: 0.00068171
Iteration 215/1000 | Loss: 0.00012137
Iteration 216/1000 | Loss: 0.00014053
Iteration 217/1000 | Loss: 0.00029826
Iteration 218/1000 | Loss: 0.00008324
Iteration 219/1000 | Loss: 0.00010668
Iteration 220/1000 | Loss: 0.00005873
Iteration 221/1000 | Loss: 0.00004536
Iteration 222/1000 | Loss: 0.00004075
Iteration 223/1000 | Loss: 0.00005392
Iteration 224/1000 | Loss: 0.00004407
Iteration 225/1000 | Loss: 0.00005205
Iteration 226/1000 | Loss: 0.00004643
Iteration 227/1000 | Loss: 0.00003132
Iteration 228/1000 | Loss: 0.00005393
Iteration 229/1000 | Loss: 0.00005726
Iteration 230/1000 | Loss: 0.00005157
Iteration 231/1000 | Loss: 0.00004688
Iteration 232/1000 | Loss: 0.00044194
Iteration 233/1000 | Loss: 0.00030855
Iteration 234/1000 | Loss: 0.00038428
Iteration 235/1000 | Loss: 0.00048678
Iteration 236/1000 | Loss: 0.00015944
Iteration 237/1000 | Loss: 0.00019915
Iteration 238/1000 | Loss: 0.00007506
Iteration 239/1000 | Loss: 0.00032271
Iteration 240/1000 | Loss: 0.00063818
Iteration 241/1000 | Loss: 0.00020828
Iteration 242/1000 | Loss: 0.00011370
Iteration 243/1000 | Loss: 0.00014508
Iteration 244/1000 | Loss: 0.00018504
Iteration 245/1000 | Loss: 0.00004948
Iteration 246/1000 | Loss: 0.00015799
Iteration 247/1000 | Loss: 0.00003674
Iteration 248/1000 | Loss: 0.00015852
Iteration 249/1000 | Loss: 0.00017723
Iteration 250/1000 | Loss: 0.00015840
Iteration 251/1000 | Loss: 0.00012678
Iteration 252/1000 | Loss: 0.00018278
Iteration 253/1000 | Loss: 0.00014434
Iteration 254/1000 | Loss: 0.00005222
Iteration 255/1000 | Loss: 0.00019455
Iteration 256/1000 | Loss: 0.00014595
Iteration 257/1000 | Loss: 0.00005267
Iteration 258/1000 | Loss: 0.00004209
Iteration 259/1000 | Loss: 0.00004431
Iteration 260/1000 | Loss: 0.00004220
Iteration 261/1000 | Loss: 0.00003426
Iteration 262/1000 | Loss: 0.00019960
Iteration 263/1000 | Loss: 0.00090219
Iteration 264/1000 | Loss: 0.00030113
Iteration 265/1000 | Loss: 0.00043513
Iteration 266/1000 | Loss: 0.00034861
Iteration 267/1000 | Loss: 0.00019842
Iteration 268/1000 | Loss: 0.00015163
Iteration 269/1000 | Loss: 0.00019771
Iteration 270/1000 | Loss: 0.00012616
Iteration 271/1000 | Loss: 0.00003915
Iteration 272/1000 | Loss: 0.00011149
Iteration 273/1000 | Loss: 0.00022269
Iteration 274/1000 | Loss: 0.00006443
Iteration 275/1000 | Loss: 0.00002825
Iteration 276/1000 | Loss: 0.00002431
Iteration 277/1000 | Loss: 0.00002133
Iteration 278/1000 | Loss: 0.00001990
Iteration 279/1000 | Loss: 0.00019004
Iteration 280/1000 | Loss: 0.00002829
Iteration 281/1000 | Loss: 0.00002205
Iteration 282/1000 | Loss: 0.00001940
Iteration 283/1000 | Loss: 0.00001738
Iteration 284/1000 | Loss: 0.00001636
Iteration 285/1000 | Loss: 0.00001560
Iteration 286/1000 | Loss: 0.00001527
Iteration 287/1000 | Loss: 0.00001523
Iteration 288/1000 | Loss: 0.00001500
Iteration 289/1000 | Loss: 0.00001475
Iteration 290/1000 | Loss: 0.00001467
Iteration 291/1000 | Loss: 0.00001464
Iteration 292/1000 | Loss: 0.00001456
Iteration 293/1000 | Loss: 0.00001453
Iteration 294/1000 | Loss: 0.00001449
Iteration 295/1000 | Loss: 0.00001446
Iteration 296/1000 | Loss: 0.00001445
Iteration 297/1000 | Loss: 0.00001445
Iteration 298/1000 | Loss: 0.00001443
Iteration 299/1000 | Loss: 0.00001442
Iteration 300/1000 | Loss: 0.00001442
Iteration 301/1000 | Loss: 0.00001440
Iteration 302/1000 | Loss: 0.00001440
Iteration 303/1000 | Loss: 0.00001439
Iteration 304/1000 | Loss: 0.00001439
Iteration 305/1000 | Loss: 0.00001438
Iteration 306/1000 | Loss: 0.00001437
Iteration 307/1000 | Loss: 0.00001436
Iteration 308/1000 | Loss: 0.00001435
Iteration 309/1000 | Loss: 0.00001434
Iteration 310/1000 | Loss: 0.00001434
Iteration 311/1000 | Loss: 0.00001432
Iteration 312/1000 | Loss: 0.00001432
Iteration 313/1000 | Loss: 0.00001431
Iteration 314/1000 | Loss: 0.00001431
Iteration 315/1000 | Loss: 0.00001431
Iteration 316/1000 | Loss: 0.00001430
Iteration 317/1000 | Loss: 0.00001430
Iteration 318/1000 | Loss: 0.00001429
Iteration 319/1000 | Loss: 0.00001429
Iteration 320/1000 | Loss: 0.00001429
Iteration 321/1000 | Loss: 0.00001428
Iteration 322/1000 | Loss: 0.00001427
Iteration 323/1000 | Loss: 0.00001425
Iteration 324/1000 | Loss: 0.00001424
Iteration 325/1000 | Loss: 0.00001423
Iteration 326/1000 | Loss: 0.00001422
Iteration 327/1000 | Loss: 0.00001422
Iteration 328/1000 | Loss: 0.00001422
Iteration 329/1000 | Loss: 0.00001421
Iteration 330/1000 | Loss: 0.00001421
Iteration 331/1000 | Loss: 0.00001421
Iteration 332/1000 | Loss: 0.00001420
Iteration 333/1000 | Loss: 0.00001420
Iteration 334/1000 | Loss: 0.00001420
Iteration 335/1000 | Loss: 0.00001420
Iteration 336/1000 | Loss: 0.00001420
Iteration 337/1000 | Loss: 0.00001420
Iteration 338/1000 | Loss: 0.00001420
Iteration 339/1000 | Loss: 0.00001420
Iteration 340/1000 | Loss: 0.00001420
Iteration 341/1000 | Loss: 0.00001420
Iteration 342/1000 | Loss: 0.00001419
Iteration 343/1000 | Loss: 0.00001419
Iteration 344/1000 | Loss: 0.00001419
Iteration 345/1000 | Loss: 0.00001419
Iteration 346/1000 | Loss: 0.00001419
Iteration 347/1000 | Loss: 0.00001419
Iteration 348/1000 | Loss: 0.00001419
Iteration 349/1000 | Loss: 0.00001419
Iteration 350/1000 | Loss: 0.00001419
Iteration 351/1000 | Loss: 0.00001419
Iteration 352/1000 | Loss: 0.00001419
Iteration 353/1000 | Loss: 0.00001418
Iteration 354/1000 | Loss: 0.00001418
Iteration 355/1000 | Loss: 0.00001418
Iteration 356/1000 | Loss: 0.00001418
Iteration 357/1000 | Loss: 0.00001418
Iteration 358/1000 | Loss: 0.00001418
Iteration 359/1000 | Loss: 0.00001418
Iteration 360/1000 | Loss: 0.00001418
Iteration 361/1000 | Loss: 0.00001418
Iteration 362/1000 | Loss: 0.00001418
Iteration 363/1000 | Loss: 0.00001418
Iteration 364/1000 | Loss: 0.00001418
Iteration 365/1000 | Loss: 0.00001417
Iteration 366/1000 | Loss: 0.00001417
Iteration 367/1000 | Loss: 0.00001417
Iteration 368/1000 | Loss: 0.00001417
Iteration 369/1000 | Loss: 0.00001417
Iteration 370/1000 | Loss: 0.00001417
Iteration 371/1000 | Loss: 0.00001417
Iteration 372/1000 | Loss: 0.00001417
Iteration 373/1000 | Loss: 0.00001417
Iteration 374/1000 | Loss: 0.00001417
Iteration 375/1000 | Loss: 0.00001417
Iteration 376/1000 | Loss: 0.00001417
Iteration 377/1000 | Loss: 0.00001417
Iteration 378/1000 | Loss: 0.00001416
Iteration 379/1000 | Loss: 0.00001416
Iteration 380/1000 | Loss: 0.00001416
Iteration 381/1000 | Loss: 0.00001416
Iteration 382/1000 | Loss: 0.00001416
Iteration 383/1000 | Loss: 0.00001416
Iteration 384/1000 | Loss: 0.00001416
Iteration 385/1000 | Loss: 0.00001415
Iteration 386/1000 | Loss: 0.00001415
Iteration 387/1000 | Loss: 0.00001415
Iteration 388/1000 | Loss: 0.00001415
Iteration 389/1000 | Loss: 0.00001415
Iteration 390/1000 | Loss: 0.00001415
Iteration 391/1000 | Loss: 0.00001415
Iteration 392/1000 | Loss: 0.00001415
Iteration 393/1000 | Loss: 0.00001415
Iteration 394/1000 | Loss: 0.00001415
Iteration 395/1000 | Loss: 0.00001415
Iteration 396/1000 | Loss: 0.00001415
Iteration 397/1000 | Loss: 0.00001415
Iteration 398/1000 | Loss: 0.00001414
Iteration 399/1000 | Loss: 0.00001414
Iteration 400/1000 | Loss: 0.00001414
Iteration 401/1000 | Loss: 0.00001414
Iteration 402/1000 | Loss: 0.00001413
Iteration 403/1000 | Loss: 0.00001413
Iteration 404/1000 | Loss: 0.00001413
Iteration 405/1000 | Loss: 0.00001413
Iteration 406/1000 | Loss: 0.00001412
Iteration 407/1000 | Loss: 0.00001412
Iteration 408/1000 | Loss: 0.00001412
Iteration 409/1000 | Loss: 0.00001412
Iteration 410/1000 | Loss: 0.00001412
Iteration 411/1000 | Loss: 0.00001412
Iteration 412/1000 | Loss: 0.00001412
Iteration 413/1000 | Loss: 0.00001412
Iteration 414/1000 | Loss: 0.00001412
Iteration 415/1000 | Loss: 0.00001412
Iteration 416/1000 | Loss: 0.00001412
Iteration 417/1000 | Loss: 0.00001412
Iteration 418/1000 | Loss: 0.00001412
Iteration 419/1000 | Loss: 0.00001412
Iteration 420/1000 | Loss: 0.00001412
Iteration 421/1000 | Loss: 0.00001412
Iteration 422/1000 | Loss: 0.00001412
Iteration 423/1000 | Loss: 0.00001411
Iteration 424/1000 | Loss: 0.00001411
Iteration 425/1000 | Loss: 0.00001411
Iteration 426/1000 | Loss: 0.00001411
Iteration 427/1000 | Loss: 0.00001411
Iteration 428/1000 | Loss: 0.00001411
Iteration 429/1000 | Loss: 0.00001411
Iteration 430/1000 | Loss: 0.00001411
Iteration 431/1000 | Loss: 0.00001411
Iteration 432/1000 | Loss: 0.00001411
Iteration 433/1000 | Loss: 0.00001411
Iteration 434/1000 | Loss: 0.00001411
Iteration 435/1000 | Loss: 0.00001411
Iteration 436/1000 | Loss: 0.00001411
Iteration 437/1000 | Loss: 0.00001410
Iteration 438/1000 | Loss: 0.00001410
Iteration 439/1000 | Loss: 0.00001410
Iteration 440/1000 | Loss: 0.00001410
Iteration 441/1000 | Loss: 0.00001410
Iteration 442/1000 | Loss: 0.00001410
Iteration 443/1000 | Loss: 0.00001410
Iteration 444/1000 | Loss: 0.00001410
Iteration 445/1000 | Loss: 0.00001410
Iteration 446/1000 | Loss: 0.00001410
Iteration 447/1000 | Loss: 0.00001410
Iteration 448/1000 | Loss: 0.00001410
Iteration 449/1000 | Loss: 0.00001410
Iteration 450/1000 | Loss: 0.00001410
Iteration 451/1000 | Loss: 0.00001410
Iteration 452/1000 | Loss: 0.00001410
Iteration 453/1000 | Loss: 0.00001410
Iteration 454/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 454. Stopping optimization.
Last 5 losses: [1.4099537111178506e-05, 1.4099537111178506e-05, 1.4099537111178506e-05, 1.4099537111178506e-05, 1.4099537111178506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4099537111178506e-05

Optimization complete. Final v2v error: 2.8502957820892334 mm

Highest mean error: 10.865134239196777 mm for frame 17

Lowest mean error: 2.3202168941497803 mm for frame 191

Saving results

Total time: 529.3399569988251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452699
Iteration 2/25 | Loss: 0.00126115
Iteration 3/25 | Loss: 0.00112838
Iteration 4/25 | Loss: 0.00110677
Iteration 5/25 | Loss: 0.00110037
Iteration 6/25 | Loss: 0.00109857
Iteration 7/25 | Loss: 0.00109845
Iteration 8/25 | Loss: 0.00109845
Iteration 9/25 | Loss: 0.00109845
Iteration 10/25 | Loss: 0.00109845
Iteration 11/25 | Loss: 0.00109845
Iteration 12/25 | Loss: 0.00109845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010984528344124556, 0.0010984528344124556, 0.0010984528344124556, 0.0010984528344124556, 0.0010984528344124556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010984528344124556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30912018
Iteration 2/25 | Loss: 0.00077322
Iteration 3/25 | Loss: 0.00077322
Iteration 4/25 | Loss: 0.00077321
Iteration 5/25 | Loss: 0.00077321
Iteration 6/25 | Loss: 0.00077321
Iteration 7/25 | Loss: 0.00077321
Iteration 8/25 | Loss: 0.00077321
Iteration 9/25 | Loss: 0.00077321
Iteration 10/25 | Loss: 0.00077321
Iteration 11/25 | Loss: 0.00077321
Iteration 12/25 | Loss: 0.00077321
Iteration 13/25 | Loss: 0.00077321
Iteration 14/25 | Loss: 0.00077321
Iteration 15/25 | Loss: 0.00077321
Iteration 16/25 | Loss: 0.00077321
Iteration 17/25 | Loss: 0.00077321
Iteration 18/25 | Loss: 0.00077321
Iteration 19/25 | Loss: 0.00077321
Iteration 20/25 | Loss: 0.00077321
Iteration 21/25 | Loss: 0.00077321
Iteration 22/25 | Loss: 0.00077321
Iteration 23/25 | Loss: 0.00077321
Iteration 24/25 | Loss: 0.00077321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007732114754617214, 0.0007732114754617214, 0.0007732114754617214, 0.0007732114754617214, 0.0007732114754617214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007732114754617214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077321
Iteration 2/1000 | Loss: 0.00002972
Iteration 3/1000 | Loss: 0.00001991
Iteration 4/1000 | Loss: 0.00001587
Iteration 5/1000 | Loss: 0.00001492
Iteration 6/1000 | Loss: 0.00001438
Iteration 7/1000 | Loss: 0.00001404
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001342
Iteration 10/1000 | Loss: 0.00001322
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001279
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001278
Iteration 23/1000 | Loss: 0.00001277
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001272
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001269
Iteration 29/1000 | Loss: 0.00001267
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001259
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001257
Iteration 58/1000 | Loss: 0.00001256
Iteration 59/1000 | Loss: 0.00001256
Iteration 60/1000 | Loss: 0.00001256
Iteration 61/1000 | Loss: 0.00001256
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001255
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001254
Iteration 75/1000 | Loss: 0.00001254
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001254
Iteration 78/1000 | Loss: 0.00001254
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001253
Iteration 85/1000 | Loss: 0.00001253
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001251
Iteration 91/1000 | Loss: 0.00001251
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001250
Iteration 94/1000 | Loss: 0.00001250
Iteration 95/1000 | Loss: 0.00001250
Iteration 96/1000 | Loss: 0.00001249
Iteration 97/1000 | Loss: 0.00001249
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001248
Iteration 103/1000 | Loss: 0.00001248
Iteration 104/1000 | Loss: 0.00001248
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001247
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001246
Iteration 115/1000 | Loss: 0.00001246
Iteration 116/1000 | Loss: 0.00001245
Iteration 117/1000 | Loss: 0.00001245
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001243
Iteration 124/1000 | Loss: 0.00001243
Iteration 125/1000 | Loss: 0.00001243
Iteration 126/1000 | Loss: 0.00001242
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001241
Iteration 129/1000 | Loss: 0.00001241
Iteration 130/1000 | Loss: 0.00001241
Iteration 131/1000 | Loss: 0.00001241
Iteration 132/1000 | Loss: 0.00001241
Iteration 133/1000 | Loss: 0.00001241
Iteration 134/1000 | Loss: 0.00001240
Iteration 135/1000 | Loss: 0.00001240
Iteration 136/1000 | Loss: 0.00001240
Iteration 137/1000 | Loss: 0.00001240
Iteration 138/1000 | Loss: 0.00001240
Iteration 139/1000 | Loss: 0.00001240
Iteration 140/1000 | Loss: 0.00001240
Iteration 141/1000 | Loss: 0.00001240
Iteration 142/1000 | Loss: 0.00001240
Iteration 143/1000 | Loss: 0.00001240
Iteration 144/1000 | Loss: 0.00001240
Iteration 145/1000 | Loss: 0.00001240
Iteration 146/1000 | Loss: 0.00001239
Iteration 147/1000 | Loss: 0.00001239
Iteration 148/1000 | Loss: 0.00001239
Iteration 149/1000 | Loss: 0.00001238
Iteration 150/1000 | Loss: 0.00001238
Iteration 151/1000 | Loss: 0.00001238
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001237
Iteration 155/1000 | Loss: 0.00001237
Iteration 156/1000 | Loss: 0.00001237
Iteration 157/1000 | Loss: 0.00001237
Iteration 158/1000 | Loss: 0.00001236
Iteration 159/1000 | Loss: 0.00001236
Iteration 160/1000 | Loss: 0.00001236
Iteration 161/1000 | Loss: 0.00001236
Iteration 162/1000 | Loss: 0.00001235
Iteration 163/1000 | Loss: 0.00001235
Iteration 164/1000 | Loss: 0.00001235
Iteration 165/1000 | Loss: 0.00001235
Iteration 166/1000 | Loss: 0.00001235
Iteration 167/1000 | Loss: 0.00001235
Iteration 168/1000 | Loss: 0.00001235
Iteration 169/1000 | Loss: 0.00001234
Iteration 170/1000 | Loss: 0.00001234
Iteration 171/1000 | Loss: 0.00001234
Iteration 172/1000 | Loss: 0.00001234
Iteration 173/1000 | Loss: 0.00001234
Iteration 174/1000 | Loss: 0.00001234
Iteration 175/1000 | Loss: 0.00001234
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Iteration 180/1000 | Loss: 0.00001234
Iteration 181/1000 | Loss: 0.00001234
Iteration 182/1000 | Loss: 0.00001234
Iteration 183/1000 | Loss: 0.00001234
Iteration 184/1000 | Loss: 0.00001234
Iteration 185/1000 | Loss: 0.00001234
Iteration 186/1000 | Loss: 0.00001234
Iteration 187/1000 | Loss: 0.00001234
Iteration 188/1000 | Loss: 0.00001234
Iteration 189/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.2338027772784699e-05, 1.2338027772784699e-05, 1.2338027772784699e-05, 1.2338027772784699e-05, 1.2338027772784699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2338027772784699e-05

Optimization complete. Final v2v error: 2.993417501449585 mm

Highest mean error: 3.456721782684326 mm for frame 41

Lowest mean error: 2.6104929447174072 mm for frame 83

Saving results

Total time: 40.56289458274841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975857
Iteration 2/25 | Loss: 0.00975857
Iteration 3/25 | Loss: 0.00975857
Iteration 4/25 | Loss: 0.00975857
Iteration 5/25 | Loss: 0.00975857
Iteration 6/25 | Loss: 0.00975857
Iteration 7/25 | Loss: 0.00975856
Iteration 8/25 | Loss: 0.00975856
Iteration 9/25 | Loss: 0.00975856
Iteration 10/25 | Loss: 0.00975856
Iteration 11/25 | Loss: 0.00975856
Iteration 12/25 | Loss: 0.00975856
Iteration 13/25 | Loss: 0.00975856
Iteration 14/25 | Loss: 0.00975856
Iteration 15/25 | Loss: 0.00975856
Iteration 16/25 | Loss: 0.00975856
Iteration 17/25 | Loss: 0.00975856
Iteration 18/25 | Loss: 0.00975855
Iteration 19/25 | Loss: 0.00975855
Iteration 20/25 | Loss: 0.00975855
Iteration 21/25 | Loss: 0.00975855
Iteration 22/25 | Loss: 0.00975855
Iteration 23/25 | Loss: 0.00975855
Iteration 24/25 | Loss: 0.00975855
Iteration 25/25 | Loss: 0.00975855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51576030
Iteration 2/25 | Loss: 0.18389736
Iteration 3/25 | Loss: 0.18389529
Iteration 4/25 | Loss: 0.18389523
Iteration 5/25 | Loss: 0.18389523
Iteration 6/25 | Loss: 0.18389522
Iteration 7/25 | Loss: 0.18389522
Iteration 8/25 | Loss: 0.18389522
Iteration 9/25 | Loss: 0.18389519
Iteration 10/25 | Loss: 0.18389519
Iteration 11/25 | Loss: 0.18389519
Iteration 12/25 | Loss: 0.18389517
Iteration 13/25 | Loss: 0.18389517
Iteration 14/25 | Loss: 0.18389517
Iteration 15/25 | Loss: 0.18389517
Iteration 16/25 | Loss: 0.18389517
Iteration 17/25 | Loss: 0.18389517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.18389517068862915, 0.18389517068862915, 0.18389517068862915, 0.18389517068862915, 0.18389517068862915]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18389517068862915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18389517
Iteration 2/1000 | Loss: 0.01651839
Iteration 3/1000 | Loss: 0.00766163
Iteration 4/1000 | Loss: 0.00241536
Iteration 5/1000 | Loss: 0.00101989
Iteration 6/1000 | Loss: 0.00081010
Iteration 7/1000 | Loss: 0.00283874
Iteration 8/1000 | Loss: 0.00097599
Iteration 9/1000 | Loss: 0.00091378
Iteration 10/1000 | Loss: 0.00049851
Iteration 11/1000 | Loss: 0.00039690
Iteration 12/1000 | Loss: 0.00245207
Iteration 13/1000 | Loss: 0.00019584
Iteration 14/1000 | Loss: 0.00172429
Iteration 15/1000 | Loss: 0.00096597
Iteration 16/1000 | Loss: 0.00057439
Iteration 17/1000 | Loss: 0.00011006
Iteration 18/1000 | Loss: 0.00032447
Iteration 19/1000 | Loss: 0.00061384
Iteration 20/1000 | Loss: 0.00173624
Iteration 21/1000 | Loss: 0.00027525
Iteration 22/1000 | Loss: 0.00027162
Iteration 23/1000 | Loss: 0.00025926
Iteration 24/1000 | Loss: 0.00096416
Iteration 25/1000 | Loss: 0.00005746
Iteration 26/1000 | Loss: 0.00093668
Iteration 27/1000 | Loss: 0.00094621
Iteration 28/1000 | Loss: 0.00032012
Iteration 29/1000 | Loss: 0.00032496
Iteration 30/1000 | Loss: 0.00095461
Iteration 31/1000 | Loss: 0.00014317
Iteration 32/1000 | Loss: 0.00011379
Iteration 33/1000 | Loss: 0.00004838
Iteration 34/1000 | Loss: 0.00003877
Iteration 35/1000 | Loss: 0.00050862
Iteration 36/1000 | Loss: 0.00044164
Iteration 37/1000 | Loss: 0.00006421
Iteration 38/1000 | Loss: 0.00003897
Iteration 39/1000 | Loss: 0.00007757
Iteration 40/1000 | Loss: 0.00008098
Iteration 41/1000 | Loss: 0.00027467
Iteration 42/1000 | Loss: 0.00002686
Iteration 43/1000 | Loss: 0.00002993
Iteration 44/1000 | Loss: 0.00010474
Iteration 45/1000 | Loss: 0.00002376
Iteration 46/1000 | Loss: 0.00036151
Iteration 47/1000 | Loss: 0.00008825
Iteration 48/1000 | Loss: 0.00012911
Iteration 49/1000 | Loss: 0.00008536
Iteration 50/1000 | Loss: 0.00002724
Iteration 51/1000 | Loss: 0.00020920
Iteration 52/1000 | Loss: 0.00018559
Iteration 53/1000 | Loss: 0.00014827
Iteration 54/1000 | Loss: 0.00153351
Iteration 55/1000 | Loss: 0.00005780
Iteration 56/1000 | Loss: 0.00014887
Iteration 57/1000 | Loss: 0.00108007
Iteration 58/1000 | Loss: 0.00068832
Iteration 59/1000 | Loss: 0.00050739
Iteration 60/1000 | Loss: 0.00005389
Iteration 61/1000 | Loss: 0.00010785
Iteration 62/1000 | Loss: 0.00007483
Iteration 63/1000 | Loss: 0.00016566
Iteration 64/1000 | Loss: 0.00047506
Iteration 65/1000 | Loss: 0.00031472
Iteration 66/1000 | Loss: 0.00018952
Iteration 67/1000 | Loss: 0.00002310
Iteration 68/1000 | Loss: 0.00003716
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002097
Iteration 71/1000 | Loss: 0.00002066
Iteration 72/1000 | Loss: 0.00002052
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00007843
Iteration 75/1000 | Loss: 0.00010881
Iteration 76/1000 | Loss: 0.00002023
Iteration 77/1000 | Loss: 0.00008395
Iteration 78/1000 | Loss: 0.00013078
Iteration 79/1000 | Loss: 0.00002301
Iteration 80/1000 | Loss: 0.00002001
Iteration 81/1000 | Loss: 0.00001988
Iteration 82/1000 | Loss: 0.00001984
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001983
Iteration 94/1000 | Loss: 0.00001983
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001982
Iteration 98/1000 | Loss: 0.00001982
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001982
Iteration 101/1000 | Loss: 0.00001982
Iteration 102/1000 | Loss: 0.00001982
Iteration 103/1000 | Loss: 0.00001982
Iteration 104/1000 | Loss: 0.00001982
Iteration 105/1000 | Loss: 0.00001982
Iteration 106/1000 | Loss: 0.00001982
Iteration 107/1000 | Loss: 0.00001982
Iteration 108/1000 | Loss: 0.00001981
Iteration 109/1000 | Loss: 0.00001981
Iteration 110/1000 | Loss: 0.00001980
Iteration 111/1000 | Loss: 0.00001980
Iteration 112/1000 | Loss: 0.00001980
Iteration 113/1000 | Loss: 0.00001979
Iteration 114/1000 | Loss: 0.00001979
Iteration 115/1000 | Loss: 0.00001978
Iteration 116/1000 | Loss: 0.00001978
Iteration 117/1000 | Loss: 0.00001977
Iteration 118/1000 | Loss: 0.00001977
Iteration 119/1000 | Loss: 0.00001977
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001977
Iteration 122/1000 | Loss: 0.00001977
Iteration 123/1000 | Loss: 0.00001977
Iteration 124/1000 | Loss: 0.00001976
Iteration 125/1000 | Loss: 0.00001976
Iteration 126/1000 | Loss: 0.00001976
Iteration 127/1000 | Loss: 0.00001976
Iteration 128/1000 | Loss: 0.00001976
Iteration 129/1000 | Loss: 0.00001976
Iteration 130/1000 | Loss: 0.00001976
Iteration 131/1000 | Loss: 0.00001976
Iteration 132/1000 | Loss: 0.00001976
Iteration 133/1000 | Loss: 0.00001975
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001975
Iteration 136/1000 | Loss: 0.00001975
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001974
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Iteration 141/1000 | Loss: 0.00001974
Iteration 142/1000 | Loss: 0.00001973
Iteration 143/1000 | Loss: 0.00001973
Iteration 144/1000 | Loss: 0.00001973
Iteration 145/1000 | Loss: 0.00001972
Iteration 146/1000 | Loss: 0.00001972
Iteration 147/1000 | Loss: 0.00001972
Iteration 148/1000 | Loss: 0.00001972
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001971
Iteration 154/1000 | Loss: 0.00001971
Iteration 155/1000 | Loss: 0.00001970
Iteration 156/1000 | Loss: 0.00001970
Iteration 157/1000 | Loss: 0.00001970
Iteration 158/1000 | Loss: 0.00001969
Iteration 159/1000 | Loss: 0.00001969
Iteration 160/1000 | Loss: 0.00001969
Iteration 161/1000 | Loss: 0.00001969
Iteration 162/1000 | Loss: 0.00001968
Iteration 163/1000 | Loss: 0.00001968
Iteration 164/1000 | Loss: 0.00001968
Iteration 165/1000 | Loss: 0.00001968
Iteration 166/1000 | Loss: 0.00001968
Iteration 167/1000 | Loss: 0.00001968
Iteration 168/1000 | Loss: 0.00001968
Iteration 169/1000 | Loss: 0.00001968
Iteration 170/1000 | Loss: 0.00001968
Iteration 171/1000 | Loss: 0.00001968
Iteration 172/1000 | Loss: 0.00001968
Iteration 173/1000 | Loss: 0.00001968
Iteration 174/1000 | Loss: 0.00001968
Iteration 175/1000 | Loss: 0.00001968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.9681470803334378e-05, 1.9681470803334378e-05, 1.9681470803334378e-05, 1.9681470803334378e-05, 1.9681470803334378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9681470803334378e-05

Optimization complete. Final v2v error: 3.843975782394409 mm

Highest mean error: 4.256199836730957 mm for frame 196

Lowest mean error: 3.691420793533325 mm for frame 151

Saving results

Total time: 140.85100769996643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788366
Iteration 2/25 | Loss: 0.00127140
Iteration 3/25 | Loss: 0.00114500
Iteration 4/25 | Loss: 0.00112505
Iteration 5/25 | Loss: 0.00111737
Iteration 6/25 | Loss: 0.00111552
Iteration 7/25 | Loss: 0.00111552
Iteration 8/25 | Loss: 0.00111552
Iteration 9/25 | Loss: 0.00111552
Iteration 10/25 | Loss: 0.00111552
Iteration 11/25 | Loss: 0.00111552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011155219981446862, 0.0011155219981446862, 0.0011155219981446862, 0.0011155219981446862, 0.0011155219981446862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011155219981446862

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.15870190
Iteration 2/25 | Loss: 0.00110295
Iteration 3/25 | Loss: 0.00110282
Iteration 4/25 | Loss: 0.00110282
Iteration 5/25 | Loss: 0.00110282
Iteration 6/25 | Loss: 0.00110282
Iteration 7/25 | Loss: 0.00110282
Iteration 8/25 | Loss: 0.00110282
Iteration 9/25 | Loss: 0.00110282
Iteration 10/25 | Loss: 0.00110282
Iteration 11/25 | Loss: 0.00110282
Iteration 12/25 | Loss: 0.00110282
Iteration 13/25 | Loss: 0.00110282
Iteration 14/25 | Loss: 0.00110282
Iteration 15/25 | Loss: 0.00110282
Iteration 16/25 | Loss: 0.00110282
Iteration 17/25 | Loss: 0.00110282
Iteration 18/25 | Loss: 0.00110282
Iteration 19/25 | Loss: 0.00110282
Iteration 20/25 | Loss: 0.00110282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001102816197089851, 0.001102816197089851, 0.001102816197089851, 0.001102816197089851, 0.001102816197089851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001102816197089851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110282
Iteration 2/1000 | Loss: 0.00006114
Iteration 3/1000 | Loss: 0.00003845
Iteration 4/1000 | Loss: 0.00002750
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002108
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001756
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001732
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001688
Iteration 16/1000 | Loss: 0.00001671
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001670
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001664
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001655
Iteration 31/1000 | Loss: 0.00001654
Iteration 32/1000 | Loss: 0.00001654
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001648
Iteration 35/1000 | Loss: 0.00001648
Iteration 36/1000 | Loss: 0.00001648
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001647
Iteration 39/1000 | Loss: 0.00001646
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001641
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001638
Iteration 56/1000 | Loss: 0.00001638
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001635
Iteration 66/1000 | Loss: 0.00001634
Iteration 67/1000 | Loss: 0.00001634
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00001632
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001631
Iteration 88/1000 | Loss: 0.00001631
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001629
Iteration 101/1000 | Loss: 0.00001628
Iteration 102/1000 | Loss: 0.00001628
Iteration 103/1000 | Loss: 0.00001628
Iteration 104/1000 | Loss: 0.00001628
Iteration 105/1000 | Loss: 0.00001628
Iteration 106/1000 | Loss: 0.00001628
Iteration 107/1000 | Loss: 0.00001628
Iteration 108/1000 | Loss: 0.00001628
Iteration 109/1000 | Loss: 0.00001628
Iteration 110/1000 | Loss: 0.00001628
Iteration 111/1000 | Loss: 0.00001627
Iteration 112/1000 | Loss: 0.00001627
Iteration 113/1000 | Loss: 0.00001627
Iteration 114/1000 | Loss: 0.00001627
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001627
Iteration 117/1000 | Loss: 0.00001627
Iteration 118/1000 | Loss: 0.00001626
Iteration 119/1000 | Loss: 0.00001626
Iteration 120/1000 | Loss: 0.00001626
Iteration 121/1000 | Loss: 0.00001625
Iteration 122/1000 | Loss: 0.00001625
Iteration 123/1000 | Loss: 0.00001625
Iteration 124/1000 | Loss: 0.00001625
Iteration 125/1000 | Loss: 0.00001625
Iteration 126/1000 | Loss: 0.00001625
Iteration 127/1000 | Loss: 0.00001625
Iteration 128/1000 | Loss: 0.00001625
Iteration 129/1000 | Loss: 0.00001625
Iteration 130/1000 | Loss: 0.00001625
Iteration 131/1000 | Loss: 0.00001625
Iteration 132/1000 | Loss: 0.00001625
Iteration 133/1000 | Loss: 0.00001624
Iteration 134/1000 | Loss: 0.00001624
Iteration 135/1000 | Loss: 0.00001624
Iteration 136/1000 | Loss: 0.00001624
Iteration 137/1000 | Loss: 0.00001624
Iteration 138/1000 | Loss: 0.00001624
Iteration 139/1000 | Loss: 0.00001624
Iteration 140/1000 | Loss: 0.00001624
Iteration 141/1000 | Loss: 0.00001624
Iteration 142/1000 | Loss: 0.00001624
Iteration 143/1000 | Loss: 0.00001624
Iteration 144/1000 | Loss: 0.00001624
Iteration 145/1000 | Loss: 0.00001623
Iteration 146/1000 | Loss: 0.00001623
Iteration 147/1000 | Loss: 0.00001623
Iteration 148/1000 | Loss: 0.00001623
Iteration 149/1000 | Loss: 0.00001623
Iteration 150/1000 | Loss: 0.00001623
Iteration 151/1000 | Loss: 0.00001623
Iteration 152/1000 | Loss: 0.00001623
Iteration 153/1000 | Loss: 0.00001623
Iteration 154/1000 | Loss: 0.00001623
Iteration 155/1000 | Loss: 0.00001623
Iteration 156/1000 | Loss: 0.00001623
Iteration 157/1000 | Loss: 0.00001622
Iteration 158/1000 | Loss: 0.00001622
Iteration 159/1000 | Loss: 0.00001622
Iteration 160/1000 | Loss: 0.00001622
Iteration 161/1000 | Loss: 0.00001622
Iteration 162/1000 | Loss: 0.00001621
Iteration 163/1000 | Loss: 0.00001621
Iteration 164/1000 | Loss: 0.00001621
Iteration 165/1000 | Loss: 0.00001621
Iteration 166/1000 | Loss: 0.00001621
Iteration 167/1000 | Loss: 0.00001621
Iteration 168/1000 | Loss: 0.00001621
Iteration 169/1000 | Loss: 0.00001621
Iteration 170/1000 | Loss: 0.00001621
Iteration 171/1000 | Loss: 0.00001621
Iteration 172/1000 | Loss: 0.00001621
Iteration 173/1000 | Loss: 0.00001620
Iteration 174/1000 | Loss: 0.00001620
Iteration 175/1000 | Loss: 0.00001620
Iteration 176/1000 | Loss: 0.00001620
Iteration 177/1000 | Loss: 0.00001620
Iteration 178/1000 | Loss: 0.00001620
Iteration 179/1000 | Loss: 0.00001620
Iteration 180/1000 | Loss: 0.00001620
Iteration 181/1000 | Loss: 0.00001620
Iteration 182/1000 | Loss: 0.00001620
Iteration 183/1000 | Loss: 0.00001620
Iteration 184/1000 | Loss: 0.00001620
Iteration 185/1000 | Loss: 0.00001619
Iteration 186/1000 | Loss: 0.00001619
Iteration 187/1000 | Loss: 0.00001619
Iteration 188/1000 | Loss: 0.00001619
Iteration 189/1000 | Loss: 0.00001619
Iteration 190/1000 | Loss: 0.00001619
Iteration 191/1000 | Loss: 0.00001619
Iteration 192/1000 | Loss: 0.00001618
Iteration 193/1000 | Loss: 0.00001618
Iteration 194/1000 | Loss: 0.00001618
Iteration 195/1000 | Loss: 0.00001618
Iteration 196/1000 | Loss: 0.00001618
Iteration 197/1000 | Loss: 0.00001618
Iteration 198/1000 | Loss: 0.00001618
Iteration 199/1000 | Loss: 0.00001618
Iteration 200/1000 | Loss: 0.00001618
Iteration 201/1000 | Loss: 0.00001618
Iteration 202/1000 | Loss: 0.00001618
Iteration 203/1000 | Loss: 0.00001618
Iteration 204/1000 | Loss: 0.00001618
Iteration 205/1000 | Loss: 0.00001618
Iteration 206/1000 | Loss: 0.00001618
Iteration 207/1000 | Loss: 0.00001617
Iteration 208/1000 | Loss: 0.00001617
Iteration 209/1000 | Loss: 0.00001617
Iteration 210/1000 | Loss: 0.00001617
Iteration 211/1000 | Loss: 0.00001617
Iteration 212/1000 | Loss: 0.00001617
Iteration 213/1000 | Loss: 0.00001617
Iteration 214/1000 | Loss: 0.00001617
Iteration 215/1000 | Loss: 0.00001617
Iteration 216/1000 | Loss: 0.00001617
Iteration 217/1000 | Loss: 0.00001616
Iteration 218/1000 | Loss: 0.00001616
Iteration 219/1000 | Loss: 0.00001616
Iteration 220/1000 | Loss: 0.00001616
Iteration 221/1000 | Loss: 0.00001616
Iteration 222/1000 | Loss: 0.00001616
Iteration 223/1000 | Loss: 0.00001616
Iteration 224/1000 | Loss: 0.00001616
Iteration 225/1000 | Loss: 0.00001616
Iteration 226/1000 | Loss: 0.00001616
Iteration 227/1000 | Loss: 0.00001616
Iteration 228/1000 | Loss: 0.00001616
Iteration 229/1000 | Loss: 0.00001615
Iteration 230/1000 | Loss: 0.00001615
Iteration 231/1000 | Loss: 0.00001615
Iteration 232/1000 | Loss: 0.00001615
Iteration 233/1000 | Loss: 0.00001615
Iteration 234/1000 | Loss: 0.00001615
Iteration 235/1000 | Loss: 0.00001615
Iteration 236/1000 | Loss: 0.00001615
Iteration 237/1000 | Loss: 0.00001615
Iteration 238/1000 | Loss: 0.00001615
Iteration 239/1000 | Loss: 0.00001615
Iteration 240/1000 | Loss: 0.00001615
Iteration 241/1000 | Loss: 0.00001615
Iteration 242/1000 | Loss: 0.00001615
Iteration 243/1000 | Loss: 0.00001615
Iteration 244/1000 | Loss: 0.00001615
Iteration 245/1000 | Loss: 0.00001615
Iteration 246/1000 | Loss: 0.00001615
Iteration 247/1000 | Loss: 0.00001615
Iteration 248/1000 | Loss: 0.00001615
Iteration 249/1000 | Loss: 0.00001615
Iteration 250/1000 | Loss: 0.00001615
Iteration 251/1000 | Loss: 0.00001615
Iteration 252/1000 | Loss: 0.00001615
Iteration 253/1000 | Loss: 0.00001615
Iteration 254/1000 | Loss: 0.00001615
Iteration 255/1000 | Loss: 0.00001615
Iteration 256/1000 | Loss: 0.00001615
Iteration 257/1000 | Loss: 0.00001615
Iteration 258/1000 | Loss: 0.00001615
Iteration 259/1000 | Loss: 0.00001615
Iteration 260/1000 | Loss: 0.00001615
Iteration 261/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.6146856069099158e-05, 1.6146856069099158e-05, 1.6146856069099158e-05, 1.6146856069099158e-05, 1.6146856069099158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6146856069099158e-05

Optimization complete. Final v2v error: 3.3597214221954346 mm

Highest mean error: 4.939907550811768 mm for frame 55

Lowest mean error: 2.700709581375122 mm for frame 40

Saving results

Total time: 45.8232524394989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919274
Iteration 2/25 | Loss: 0.00136948
Iteration 3/25 | Loss: 0.00117996
Iteration 4/25 | Loss: 0.00114991
Iteration 5/25 | Loss: 0.00114115
Iteration 6/25 | Loss: 0.00113919
Iteration 7/25 | Loss: 0.00113917
Iteration 8/25 | Loss: 0.00113917
Iteration 9/25 | Loss: 0.00113917
Iteration 10/25 | Loss: 0.00113917
Iteration 11/25 | Loss: 0.00113917
Iteration 12/25 | Loss: 0.00113917
Iteration 13/25 | Loss: 0.00113917
Iteration 14/25 | Loss: 0.00113917
Iteration 15/25 | Loss: 0.00113917
Iteration 16/25 | Loss: 0.00113917
Iteration 17/25 | Loss: 0.00113917
Iteration 18/25 | Loss: 0.00113917
Iteration 19/25 | Loss: 0.00113917
Iteration 20/25 | Loss: 0.00113917
Iteration 21/25 | Loss: 0.00113917
Iteration 22/25 | Loss: 0.00113917
Iteration 23/25 | Loss: 0.00113917
Iteration 24/25 | Loss: 0.00113917
Iteration 25/25 | Loss: 0.00113917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001139173866249621, 0.001139173866249621, 0.001139173866249621, 0.001139173866249621, 0.001139173866249621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001139173866249621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46854866
Iteration 2/25 | Loss: 0.00102542
Iteration 3/25 | Loss: 0.00102542
Iteration 4/25 | Loss: 0.00102542
Iteration 5/25 | Loss: 0.00102542
Iteration 6/25 | Loss: 0.00102542
Iteration 7/25 | Loss: 0.00102542
Iteration 8/25 | Loss: 0.00102542
Iteration 9/25 | Loss: 0.00102542
Iteration 10/25 | Loss: 0.00102542
Iteration 11/25 | Loss: 0.00102542
Iteration 12/25 | Loss: 0.00102542
Iteration 13/25 | Loss: 0.00102542
Iteration 14/25 | Loss: 0.00102542
Iteration 15/25 | Loss: 0.00102542
Iteration 16/25 | Loss: 0.00102542
Iteration 17/25 | Loss: 0.00102542
Iteration 18/25 | Loss: 0.00102542
Iteration 19/25 | Loss: 0.00102542
Iteration 20/25 | Loss: 0.00102542
Iteration 21/25 | Loss: 0.00102542
Iteration 22/25 | Loss: 0.00102542
Iteration 23/25 | Loss: 0.00102542
Iteration 24/25 | Loss: 0.00102542
Iteration 25/25 | Loss: 0.00102542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102542
Iteration 2/1000 | Loss: 0.00004672
Iteration 3/1000 | Loss: 0.00002745
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001851
Iteration 8/1000 | Loss: 0.00001822
Iteration 9/1000 | Loss: 0.00001794
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001771
Iteration 12/1000 | Loss: 0.00001756
Iteration 13/1000 | Loss: 0.00001755
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001736
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001733
Iteration 20/1000 | Loss: 0.00001732
Iteration 21/1000 | Loss: 0.00001730
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001721
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001720
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001711
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001698
Iteration 45/1000 | Loss: 0.00001697
Iteration 46/1000 | Loss: 0.00001696
Iteration 47/1000 | Loss: 0.00001693
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001689
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001686
Iteration 57/1000 | Loss: 0.00001686
Iteration 58/1000 | Loss: 0.00001685
Iteration 59/1000 | Loss: 0.00001685
Iteration 60/1000 | Loss: 0.00001685
Iteration 61/1000 | Loss: 0.00001683
Iteration 62/1000 | Loss: 0.00001682
Iteration 63/1000 | Loss: 0.00001682
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001682
Iteration 66/1000 | Loss: 0.00001682
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001680
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001678
Iteration 79/1000 | Loss: 0.00001678
Iteration 80/1000 | Loss: 0.00001678
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001676
Iteration 84/1000 | Loss: 0.00001676
Iteration 85/1000 | Loss: 0.00001676
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001675
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001675
Iteration 92/1000 | Loss: 0.00001675
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001672
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001671
Iteration 110/1000 | Loss: 0.00001671
Iteration 111/1000 | Loss: 0.00001671
Iteration 112/1000 | Loss: 0.00001670
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001669
Iteration 119/1000 | Loss: 0.00001669
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001667
Iteration 125/1000 | Loss: 0.00001667
Iteration 126/1000 | Loss: 0.00001667
Iteration 127/1000 | Loss: 0.00001667
Iteration 128/1000 | Loss: 0.00001667
Iteration 129/1000 | Loss: 0.00001666
Iteration 130/1000 | Loss: 0.00001666
Iteration 131/1000 | Loss: 0.00001666
Iteration 132/1000 | Loss: 0.00001666
Iteration 133/1000 | Loss: 0.00001666
Iteration 134/1000 | Loss: 0.00001666
Iteration 135/1000 | Loss: 0.00001665
Iteration 136/1000 | Loss: 0.00001665
Iteration 137/1000 | Loss: 0.00001665
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001665
Iteration 140/1000 | Loss: 0.00001664
Iteration 141/1000 | Loss: 0.00001664
Iteration 142/1000 | Loss: 0.00001664
Iteration 143/1000 | Loss: 0.00001663
Iteration 144/1000 | Loss: 0.00001663
Iteration 145/1000 | Loss: 0.00001663
Iteration 146/1000 | Loss: 0.00001663
Iteration 147/1000 | Loss: 0.00001663
Iteration 148/1000 | Loss: 0.00001662
Iteration 149/1000 | Loss: 0.00001662
Iteration 150/1000 | Loss: 0.00001662
Iteration 151/1000 | Loss: 0.00001662
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001661
Iteration 154/1000 | Loss: 0.00001661
Iteration 155/1000 | Loss: 0.00001661
Iteration 156/1000 | Loss: 0.00001661
Iteration 157/1000 | Loss: 0.00001660
Iteration 158/1000 | Loss: 0.00001660
Iteration 159/1000 | Loss: 0.00001660
Iteration 160/1000 | Loss: 0.00001660
Iteration 161/1000 | Loss: 0.00001660
Iteration 162/1000 | Loss: 0.00001660
Iteration 163/1000 | Loss: 0.00001660
Iteration 164/1000 | Loss: 0.00001660
Iteration 165/1000 | Loss: 0.00001660
Iteration 166/1000 | Loss: 0.00001660
Iteration 167/1000 | Loss: 0.00001660
Iteration 168/1000 | Loss: 0.00001659
Iteration 169/1000 | Loss: 0.00001659
Iteration 170/1000 | Loss: 0.00001659
Iteration 171/1000 | Loss: 0.00001659
Iteration 172/1000 | Loss: 0.00001658
Iteration 173/1000 | Loss: 0.00001658
Iteration 174/1000 | Loss: 0.00001658
Iteration 175/1000 | Loss: 0.00001658
Iteration 176/1000 | Loss: 0.00001658
Iteration 177/1000 | Loss: 0.00001657
Iteration 178/1000 | Loss: 0.00001657
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001656
Iteration 182/1000 | Loss: 0.00001656
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Iteration 185/1000 | Loss: 0.00001654
Iteration 186/1000 | Loss: 0.00001654
Iteration 187/1000 | Loss: 0.00001654
Iteration 188/1000 | Loss: 0.00001654
Iteration 189/1000 | Loss: 0.00001654
Iteration 190/1000 | Loss: 0.00001654
Iteration 191/1000 | Loss: 0.00001654
Iteration 192/1000 | Loss: 0.00001653
Iteration 193/1000 | Loss: 0.00001653
Iteration 194/1000 | Loss: 0.00001653
Iteration 195/1000 | Loss: 0.00001653
Iteration 196/1000 | Loss: 0.00001653
Iteration 197/1000 | Loss: 0.00001653
Iteration 198/1000 | Loss: 0.00001653
Iteration 199/1000 | Loss: 0.00001653
Iteration 200/1000 | Loss: 0.00001653
Iteration 201/1000 | Loss: 0.00001653
Iteration 202/1000 | Loss: 0.00001652
Iteration 203/1000 | Loss: 0.00001652
Iteration 204/1000 | Loss: 0.00001652
Iteration 205/1000 | Loss: 0.00001652
Iteration 206/1000 | Loss: 0.00001652
Iteration 207/1000 | Loss: 0.00001652
Iteration 208/1000 | Loss: 0.00001651
Iteration 209/1000 | Loss: 0.00001651
Iteration 210/1000 | Loss: 0.00001651
Iteration 211/1000 | Loss: 0.00001650
Iteration 212/1000 | Loss: 0.00001650
Iteration 213/1000 | Loss: 0.00001650
Iteration 214/1000 | Loss: 0.00001650
Iteration 215/1000 | Loss: 0.00001650
Iteration 216/1000 | Loss: 0.00001650
Iteration 217/1000 | Loss: 0.00001650
Iteration 218/1000 | Loss: 0.00001650
Iteration 219/1000 | Loss: 0.00001650
Iteration 220/1000 | Loss: 0.00001650
Iteration 221/1000 | Loss: 0.00001650
Iteration 222/1000 | Loss: 0.00001650
Iteration 223/1000 | Loss: 0.00001650
Iteration 224/1000 | Loss: 0.00001650
Iteration 225/1000 | Loss: 0.00001650
Iteration 226/1000 | Loss: 0.00001650
Iteration 227/1000 | Loss: 0.00001650
Iteration 228/1000 | Loss: 0.00001650
Iteration 229/1000 | Loss: 0.00001650
Iteration 230/1000 | Loss: 0.00001650
Iteration 231/1000 | Loss: 0.00001649
Iteration 232/1000 | Loss: 0.00001649
Iteration 233/1000 | Loss: 0.00001649
Iteration 234/1000 | Loss: 0.00001649
Iteration 235/1000 | Loss: 0.00001649
Iteration 236/1000 | Loss: 0.00001649
Iteration 237/1000 | Loss: 0.00001649
Iteration 238/1000 | Loss: 0.00001649
Iteration 239/1000 | Loss: 0.00001649
Iteration 240/1000 | Loss: 0.00001649
Iteration 241/1000 | Loss: 0.00001648
Iteration 242/1000 | Loss: 0.00001648
Iteration 243/1000 | Loss: 0.00001648
Iteration 244/1000 | Loss: 0.00001648
Iteration 245/1000 | Loss: 0.00001648
Iteration 246/1000 | Loss: 0.00001648
Iteration 247/1000 | Loss: 0.00001648
Iteration 248/1000 | Loss: 0.00001648
Iteration 249/1000 | Loss: 0.00001648
Iteration 250/1000 | Loss: 0.00001648
Iteration 251/1000 | Loss: 0.00001648
Iteration 252/1000 | Loss: 0.00001648
Iteration 253/1000 | Loss: 0.00001648
Iteration 254/1000 | Loss: 0.00001647
Iteration 255/1000 | Loss: 0.00001647
Iteration 256/1000 | Loss: 0.00001647
Iteration 257/1000 | Loss: 0.00001647
Iteration 258/1000 | Loss: 0.00001647
Iteration 259/1000 | Loss: 0.00001647
Iteration 260/1000 | Loss: 0.00001647
Iteration 261/1000 | Loss: 0.00001647
Iteration 262/1000 | Loss: 0.00001647
Iteration 263/1000 | Loss: 0.00001647
Iteration 264/1000 | Loss: 0.00001647
Iteration 265/1000 | Loss: 0.00001647
Iteration 266/1000 | Loss: 0.00001647
Iteration 267/1000 | Loss: 0.00001647
Iteration 268/1000 | Loss: 0.00001647
Iteration 269/1000 | Loss: 0.00001647
Iteration 270/1000 | Loss: 0.00001647
Iteration 271/1000 | Loss: 0.00001647
Iteration 272/1000 | Loss: 0.00001647
Iteration 273/1000 | Loss: 0.00001647
Iteration 274/1000 | Loss: 0.00001647
Iteration 275/1000 | Loss: 0.00001647
Iteration 276/1000 | Loss: 0.00001647
Iteration 277/1000 | Loss: 0.00001647
Iteration 278/1000 | Loss: 0.00001647
Iteration 279/1000 | Loss: 0.00001647
Iteration 280/1000 | Loss: 0.00001647
Iteration 281/1000 | Loss: 0.00001647
Iteration 282/1000 | Loss: 0.00001647
Iteration 283/1000 | Loss: 0.00001647
Iteration 284/1000 | Loss: 0.00001647
Iteration 285/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [1.6467975001432933e-05, 1.6467975001432933e-05, 1.6467975001432933e-05, 1.6467975001432933e-05, 1.6467975001432933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6467975001432933e-05

Optimization complete. Final v2v error: 3.410195827484131 mm

Highest mean error: 3.6814115047454834 mm for frame 168

Lowest mean error: 3.059924364089966 mm for frame 0

Saving results

Total time: 50.2940092086792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376088
Iteration 2/25 | Loss: 0.00122842
Iteration 3/25 | Loss: 0.00110331
Iteration 4/25 | Loss: 0.00109065
Iteration 5/25 | Loss: 0.00108719
Iteration 6/25 | Loss: 0.00108694
Iteration 7/25 | Loss: 0.00108694
Iteration 8/25 | Loss: 0.00108694
Iteration 9/25 | Loss: 0.00108694
Iteration 10/25 | Loss: 0.00108694
Iteration 11/25 | Loss: 0.00108694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010869442485272884, 0.0010869442485272884, 0.0010869442485272884, 0.0010869442485272884, 0.0010869442485272884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010869442485272884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32530928
Iteration 2/25 | Loss: 0.00089251
Iteration 3/25 | Loss: 0.00089251
Iteration 4/25 | Loss: 0.00089251
Iteration 5/25 | Loss: 0.00089250
Iteration 6/25 | Loss: 0.00089250
Iteration 7/25 | Loss: 0.00089250
Iteration 8/25 | Loss: 0.00089250
Iteration 9/25 | Loss: 0.00089250
Iteration 10/25 | Loss: 0.00089250
Iteration 11/25 | Loss: 0.00089250
Iteration 12/25 | Loss: 0.00089250
Iteration 13/25 | Loss: 0.00089250
Iteration 14/25 | Loss: 0.00089250
Iteration 15/25 | Loss: 0.00089250
Iteration 16/25 | Loss: 0.00089250
Iteration 17/25 | Loss: 0.00089250
Iteration 18/25 | Loss: 0.00089250
Iteration 19/25 | Loss: 0.00089250
Iteration 20/25 | Loss: 0.00089250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008925035945139825, 0.0008925035945139825, 0.0008925035945139825, 0.0008925035945139825, 0.0008925035945139825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008925035945139825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089250
Iteration 2/1000 | Loss: 0.00003469
Iteration 3/1000 | Loss: 0.00002046
Iteration 4/1000 | Loss: 0.00001822
Iteration 5/1000 | Loss: 0.00001714
Iteration 6/1000 | Loss: 0.00001616
Iteration 7/1000 | Loss: 0.00001551
Iteration 8/1000 | Loss: 0.00001508
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001397
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001362
Iteration 22/1000 | Loss: 0.00001361
Iteration 23/1000 | Loss: 0.00001361
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001349
Iteration 26/1000 | Loss: 0.00001347
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001339
Iteration 37/1000 | Loss: 0.00001338
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001336
Iteration 40/1000 | Loss: 0.00001336
Iteration 41/1000 | Loss: 0.00001336
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00001335
Iteration 46/1000 | Loss: 0.00001335
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001334
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001333
Iteration 54/1000 | Loss: 0.00001333
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001332
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001332
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001329
Iteration 74/1000 | Loss: 0.00001329
Iteration 75/1000 | Loss: 0.00001329
Iteration 76/1000 | Loss: 0.00001329
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001329
Iteration 80/1000 | Loss: 0.00001329
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Iteration 98/1000 | Loss: 0.00001329
Iteration 99/1000 | Loss: 0.00001329
Iteration 100/1000 | Loss: 0.00001329
Iteration 101/1000 | Loss: 0.00001329
Iteration 102/1000 | Loss: 0.00001329
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001329
Iteration 114/1000 | Loss: 0.00001329
Iteration 115/1000 | Loss: 0.00001329
Iteration 116/1000 | Loss: 0.00001329
Iteration 117/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.3287054571264889e-05, 1.3287054571264889e-05, 1.3287054571264889e-05, 1.3287054571264889e-05, 1.3287054571264889e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3287054571264889e-05

Optimization complete. Final v2v error: 3.0734329223632812 mm

Highest mean error: 3.582900285720825 mm for frame 196

Lowest mean error: 2.6505794525146484 mm for frame 132

Saving results

Total time: 37.809489727020264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772011
Iteration 2/25 | Loss: 0.00170734
Iteration 3/25 | Loss: 0.00130082
Iteration 4/25 | Loss: 0.00120772
Iteration 5/25 | Loss: 0.00118248
Iteration 6/25 | Loss: 0.00118878
Iteration 7/25 | Loss: 0.00117128
Iteration 8/25 | Loss: 0.00116747
Iteration 9/25 | Loss: 0.00116707
Iteration 10/25 | Loss: 0.00116696
Iteration 11/25 | Loss: 0.00117357
Iteration 12/25 | Loss: 0.00116920
Iteration 13/25 | Loss: 0.00116726
Iteration 14/25 | Loss: 0.00116369
Iteration 15/25 | Loss: 0.00116320
Iteration 16/25 | Loss: 0.00116307
Iteration 17/25 | Loss: 0.00116307
Iteration 18/25 | Loss: 0.00116307
Iteration 19/25 | Loss: 0.00116307
Iteration 20/25 | Loss: 0.00116306
Iteration 21/25 | Loss: 0.00116306
Iteration 22/25 | Loss: 0.00116306
Iteration 23/25 | Loss: 0.00116306
Iteration 24/25 | Loss: 0.00116306
Iteration 25/25 | Loss: 0.00116306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07245708
Iteration 2/25 | Loss: 0.00075187
Iteration 3/25 | Loss: 0.00075186
Iteration 4/25 | Loss: 0.00075186
Iteration 5/25 | Loss: 0.00075186
Iteration 6/25 | Loss: 0.00075186
Iteration 7/25 | Loss: 0.00075185
Iteration 8/25 | Loss: 0.00075185
Iteration 9/25 | Loss: 0.00075185
Iteration 10/25 | Loss: 0.00075185
Iteration 11/25 | Loss: 0.00075185
Iteration 12/25 | Loss: 0.00075185
Iteration 13/25 | Loss: 0.00075185
Iteration 14/25 | Loss: 0.00075185
Iteration 15/25 | Loss: 0.00075185
Iteration 16/25 | Loss: 0.00075185
Iteration 17/25 | Loss: 0.00075185
Iteration 18/25 | Loss: 0.00075185
Iteration 19/25 | Loss: 0.00075185
Iteration 20/25 | Loss: 0.00075185
Iteration 21/25 | Loss: 0.00075185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007518537458963692, 0.0007518537458963692, 0.0007518537458963692, 0.0007518537458963692, 0.0007518537458963692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007518537458963692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075185
Iteration 2/1000 | Loss: 0.00003239
Iteration 3/1000 | Loss: 0.00001826
Iteration 4/1000 | Loss: 0.00001650
Iteration 5/1000 | Loss: 0.00001545
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001363
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001318
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001314
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001314
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001304
Iteration 43/1000 | Loss: 0.00001304
Iteration 44/1000 | Loss: 0.00001303
Iteration 45/1000 | Loss: 0.00001303
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001301
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001296
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001295
Iteration 66/1000 | Loss: 0.00001295
Iteration 67/1000 | Loss: 0.00001295
Iteration 68/1000 | Loss: 0.00001295
Iteration 69/1000 | Loss: 0.00001295
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001294
Iteration 72/1000 | Loss: 0.00001294
Iteration 73/1000 | Loss: 0.00001294
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001293
Iteration 78/1000 | Loss: 0.00001293
Iteration 79/1000 | Loss: 0.00001293
Iteration 80/1000 | Loss: 0.00001293
Iteration 81/1000 | Loss: 0.00001293
Iteration 82/1000 | Loss: 0.00001293
Iteration 83/1000 | Loss: 0.00001293
Iteration 84/1000 | Loss: 0.00001292
Iteration 85/1000 | Loss: 0.00001292
Iteration 86/1000 | Loss: 0.00001292
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001292
Iteration 95/1000 | Loss: 0.00001292
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001291
Iteration 98/1000 | Loss: 0.00001291
Iteration 99/1000 | Loss: 0.00001291
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001291
Iteration 103/1000 | Loss: 0.00001291
Iteration 104/1000 | Loss: 0.00001291
Iteration 105/1000 | Loss: 0.00001291
Iteration 106/1000 | Loss: 0.00001291
Iteration 107/1000 | Loss: 0.00001291
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001291
Iteration 112/1000 | Loss: 0.00001291
Iteration 113/1000 | Loss: 0.00001291
Iteration 114/1000 | Loss: 0.00001291
Iteration 115/1000 | Loss: 0.00001291
Iteration 116/1000 | Loss: 0.00001291
Iteration 117/1000 | Loss: 0.00001291
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001291
Iteration 121/1000 | Loss: 0.00001291
Iteration 122/1000 | Loss: 0.00001291
Iteration 123/1000 | Loss: 0.00001291
Iteration 124/1000 | Loss: 0.00001291
Iteration 125/1000 | Loss: 0.00001291
Iteration 126/1000 | Loss: 0.00001291
Iteration 127/1000 | Loss: 0.00001291
Iteration 128/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.290787804464344e-05, 1.290787804464344e-05, 1.290787804464344e-05, 1.290787804464344e-05, 1.290787804464344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.290787804464344e-05

Optimization complete. Final v2v error: 3.0462284088134766 mm

Highest mean error: 3.2867844104766846 mm for frame 235

Lowest mean error: 2.7896807193756104 mm for frame 17

Saving results

Total time: 61.53976321220398
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017132
Iteration 2/25 | Loss: 0.01017132
Iteration 3/25 | Loss: 0.01017132
Iteration 4/25 | Loss: 0.01017131
Iteration 5/25 | Loss: 0.01017131
Iteration 6/25 | Loss: 0.01017131
Iteration 7/25 | Loss: 0.01017131
Iteration 8/25 | Loss: 0.01017131
Iteration 9/25 | Loss: 0.01017131
Iteration 10/25 | Loss: 0.01017131
Iteration 11/25 | Loss: 0.01017131
Iteration 12/25 | Loss: 0.01017131
Iteration 13/25 | Loss: 0.01017131
Iteration 14/25 | Loss: 0.01017131
Iteration 15/25 | Loss: 0.01017130
Iteration 16/25 | Loss: 0.01017130
Iteration 17/25 | Loss: 0.01017130
Iteration 18/25 | Loss: 0.01017130
Iteration 19/25 | Loss: 0.01017130
Iteration 20/25 | Loss: 0.01017130
Iteration 21/25 | Loss: 0.01017130
Iteration 22/25 | Loss: 0.01017130
Iteration 23/25 | Loss: 0.01017130
Iteration 24/25 | Loss: 0.01017129
Iteration 25/25 | Loss: 0.01017129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31504643
Iteration 2/25 | Loss: 0.18166840
Iteration 3/25 | Loss: 0.18070847
Iteration 4/25 | Loss: 0.17721827
Iteration 5/25 | Loss: 0.17721823
Iteration 6/25 | Loss: 0.17721818
Iteration 7/25 | Loss: 0.17721818
Iteration 8/25 | Loss: 0.17721818
Iteration 9/25 | Loss: 0.17721818
Iteration 10/25 | Loss: 0.17721817
Iteration 11/25 | Loss: 0.17721817
Iteration 12/25 | Loss: 0.17721818
Iteration 13/25 | Loss: 0.17721817
Iteration 14/25 | Loss: 0.17728430
Iteration 15/25 | Loss: 0.17727943
Iteration 16/25 | Loss: 0.17721830
Iteration 17/25 | Loss: 0.17721823
Iteration 18/25 | Loss: 0.17721821
Iteration 19/25 | Loss: 0.17721821
Iteration 20/25 | Loss: 0.17721818
Iteration 21/25 | Loss: 0.17721818
Iteration 22/25 | Loss: 0.17721818
Iteration 23/25 | Loss: 0.17721818
Iteration 24/25 | Loss: 0.17721818
Iteration 25/25 | Loss: 0.17721817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17721817
Iteration 2/1000 | Loss: 0.01006170
Iteration 3/1000 | Loss: 0.00025142
Iteration 4/1000 | Loss: 0.00295896
Iteration 5/1000 | Loss: 0.00070291
Iteration 6/1000 | Loss: 0.00078091
Iteration 7/1000 | Loss: 0.00085445
Iteration 8/1000 | Loss: 0.00757328
Iteration 9/1000 | Loss: 0.00021405
Iteration 10/1000 | Loss: 0.00003782
Iteration 11/1000 | Loss: 0.00088649
Iteration 12/1000 | Loss: 0.00007166
Iteration 13/1000 | Loss: 0.00023640
Iteration 14/1000 | Loss: 0.00021963
Iteration 15/1000 | Loss: 0.00002511
Iteration 16/1000 | Loss: 0.00002188
Iteration 17/1000 | Loss: 0.00005717
Iteration 18/1000 | Loss: 0.00002197
Iteration 19/1000 | Loss: 0.00001784
Iteration 20/1000 | Loss: 0.00033004
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001614
Iteration 23/1000 | Loss: 0.00002051
Iteration 24/1000 | Loss: 0.00001655
Iteration 25/1000 | Loss: 0.00001534
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00014577
Iteration 32/1000 | Loss: 0.00037295
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00014419
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001321
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001275
Iteration 46/1000 | Loss: 0.00001275
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001274
Iteration 50/1000 | Loss: 0.00001274
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001272
Iteration 53/1000 | Loss: 0.00001272
Iteration 54/1000 | Loss: 0.00001272
Iteration 55/1000 | Loss: 0.00001272
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001271
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001269
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001268
Iteration 63/1000 | Loss: 0.00001268
Iteration 64/1000 | Loss: 0.00001268
Iteration 65/1000 | Loss: 0.00001268
Iteration 66/1000 | Loss: 0.00001268
Iteration 67/1000 | Loss: 0.00001268
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001267
Iteration 71/1000 | Loss: 0.00001267
Iteration 72/1000 | Loss: 0.00001267
Iteration 73/1000 | Loss: 0.00001267
Iteration 74/1000 | Loss: 0.00001267
Iteration 75/1000 | Loss: 0.00001267
Iteration 76/1000 | Loss: 0.00001266
Iteration 77/1000 | Loss: 0.00001266
Iteration 78/1000 | Loss: 0.00001266
Iteration 79/1000 | Loss: 0.00001266
Iteration 80/1000 | Loss: 0.00001266
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001265
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001264
Iteration 90/1000 | Loss: 0.00001264
Iteration 91/1000 | Loss: 0.00001264
Iteration 92/1000 | Loss: 0.00001264
Iteration 93/1000 | Loss: 0.00001264
Iteration 94/1000 | Loss: 0.00001264
Iteration 95/1000 | Loss: 0.00001264
Iteration 96/1000 | Loss: 0.00001263
Iteration 97/1000 | Loss: 0.00001263
Iteration 98/1000 | Loss: 0.00001263
Iteration 99/1000 | Loss: 0.00001262
Iteration 100/1000 | Loss: 0.00001262
Iteration 101/1000 | Loss: 0.00001262
Iteration 102/1000 | Loss: 0.00001261
Iteration 103/1000 | Loss: 0.00001261
Iteration 104/1000 | Loss: 0.00001261
Iteration 105/1000 | Loss: 0.00001261
Iteration 106/1000 | Loss: 0.00001260
Iteration 107/1000 | Loss: 0.00001260
Iteration 108/1000 | Loss: 0.00001260
Iteration 109/1000 | Loss: 0.00001260
Iteration 110/1000 | Loss: 0.00001260
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Iteration 113/1000 | Loss: 0.00001259
Iteration 114/1000 | Loss: 0.00001259
Iteration 115/1000 | Loss: 0.00001259
Iteration 116/1000 | Loss: 0.00001259
Iteration 117/1000 | Loss: 0.00001259
Iteration 118/1000 | Loss: 0.00001259
Iteration 119/1000 | Loss: 0.00001259
Iteration 120/1000 | Loss: 0.00001259
Iteration 121/1000 | Loss: 0.00001259
Iteration 122/1000 | Loss: 0.00001259
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001258
Iteration 125/1000 | Loss: 0.00001258
Iteration 126/1000 | Loss: 0.00001258
Iteration 127/1000 | Loss: 0.00001258
Iteration 128/1000 | Loss: 0.00001258
Iteration 129/1000 | Loss: 0.00001258
Iteration 130/1000 | Loss: 0.00001258
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001279
Iteration 133/1000 | Loss: 0.00001279
Iteration 134/1000 | Loss: 0.00001278
Iteration 135/1000 | Loss: 0.00001278
Iteration 136/1000 | Loss: 0.00001278
Iteration 137/1000 | Loss: 0.00001260
Iteration 138/1000 | Loss: 0.00001260
Iteration 139/1000 | Loss: 0.00001260
Iteration 140/1000 | Loss: 0.00001259
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001257
Iteration 143/1000 | Loss: 0.00001257
Iteration 144/1000 | Loss: 0.00001257
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001257
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001257
Iteration 152/1000 | Loss: 0.00001256
Iteration 153/1000 | Loss: 0.00001256
Iteration 154/1000 | Loss: 0.00001256
Iteration 155/1000 | Loss: 0.00001256
Iteration 156/1000 | Loss: 0.00001256
Iteration 157/1000 | Loss: 0.00001256
Iteration 158/1000 | Loss: 0.00001256
Iteration 159/1000 | Loss: 0.00001256
Iteration 160/1000 | Loss: 0.00001256
Iteration 161/1000 | Loss: 0.00001256
Iteration 162/1000 | Loss: 0.00001256
Iteration 163/1000 | Loss: 0.00001256
Iteration 164/1000 | Loss: 0.00001256
Iteration 165/1000 | Loss: 0.00001256
Iteration 166/1000 | Loss: 0.00001256
Iteration 167/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.2563488780870102e-05, 1.2563488780870102e-05, 1.2563488780870102e-05, 1.2563488780870102e-05, 1.2563488780870102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2563488780870102e-05

Optimization complete. Final v2v error: 2.8732755184173584 mm

Highest mean error: 8.504631042480469 mm for frame 180

Lowest mean error: 2.6547651290893555 mm for frame 163

Saving results

Total time: 83.70541834831238
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987079
Iteration 2/25 | Loss: 0.00217630
Iteration 3/25 | Loss: 0.00161189
Iteration 4/25 | Loss: 0.00146345
Iteration 5/25 | Loss: 0.00160378
Iteration 6/25 | Loss: 0.00146961
Iteration 7/25 | Loss: 0.00133306
Iteration 8/25 | Loss: 0.00128404
Iteration 9/25 | Loss: 0.00122721
Iteration 10/25 | Loss: 0.00118941
Iteration 11/25 | Loss: 0.00117664
Iteration 12/25 | Loss: 0.00115679
Iteration 13/25 | Loss: 0.00114353
Iteration 14/25 | Loss: 0.00113764
Iteration 15/25 | Loss: 0.00114367
Iteration 16/25 | Loss: 0.00113667
Iteration 17/25 | Loss: 0.00113411
Iteration 18/25 | Loss: 0.00113081
Iteration 19/25 | Loss: 0.00112589
Iteration 20/25 | Loss: 0.00112164
Iteration 21/25 | Loss: 0.00111993
Iteration 22/25 | Loss: 0.00111717
Iteration 23/25 | Loss: 0.00111787
Iteration 24/25 | Loss: 0.00111459
Iteration 25/25 | Loss: 0.00111396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45825326
Iteration 2/25 | Loss: 0.00079613
Iteration 3/25 | Loss: 0.00079613
Iteration 4/25 | Loss: 0.00079613
Iteration 5/25 | Loss: 0.00079613
Iteration 6/25 | Loss: 0.00079612
Iteration 7/25 | Loss: 0.00079612
Iteration 8/25 | Loss: 0.00079612
Iteration 9/25 | Loss: 0.00079612
Iteration 10/25 | Loss: 0.00079612
Iteration 11/25 | Loss: 0.00079612
Iteration 12/25 | Loss: 0.00079612
Iteration 13/25 | Loss: 0.00079612
Iteration 14/25 | Loss: 0.00079612
Iteration 15/25 | Loss: 0.00079612
Iteration 16/25 | Loss: 0.00079612
Iteration 17/25 | Loss: 0.00079612
Iteration 18/25 | Loss: 0.00079612
Iteration 19/25 | Loss: 0.00079612
Iteration 20/25 | Loss: 0.00079612
Iteration 21/25 | Loss: 0.00079612
Iteration 22/25 | Loss: 0.00079612
Iteration 23/25 | Loss: 0.00079612
Iteration 24/25 | Loss: 0.00079612
Iteration 25/25 | Loss: 0.00079612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079612
Iteration 2/1000 | Loss: 0.00060457
Iteration 3/1000 | Loss: 0.00003408
Iteration 4/1000 | Loss: 0.00125658
Iteration 5/1000 | Loss: 0.00142035
Iteration 6/1000 | Loss: 0.00126007
Iteration 7/1000 | Loss: 0.00123956
Iteration 8/1000 | Loss: 0.00087393
Iteration 9/1000 | Loss: 0.00155836
Iteration 10/1000 | Loss: 0.00091670
Iteration 11/1000 | Loss: 0.00075093
Iteration 12/1000 | Loss: 0.00113755
Iteration 13/1000 | Loss: 0.00101235
Iteration 14/1000 | Loss: 0.00166467
Iteration 15/1000 | Loss: 0.00086049
Iteration 16/1000 | Loss: 0.00153523
Iteration 17/1000 | Loss: 0.00007372
Iteration 18/1000 | Loss: 0.00004572
Iteration 19/1000 | Loss: 0.00172129
Iteration 20/1000 | Loss: 0.00160238
Iteration 21/1000 | Loss: 0.00251814
Iteration 22/1000 | Loss: 0.00157593
Iteration 23/1000 | Loss: 0.00068490
Iteration 24/1000 | Loss: 0.00114999
Iteration 25/1000 | Loss: 0.00010035
Iteration 26/1000 | Loss: 0.00016286
Iteration 27/1000 | Loss: 0.00003172
Iteration 28/1000 | Loss: 0.00208857
Iteration 29/1000 | Loss: 0.00131110
Iteration 30/1000 | Loss: 0.00137725
Iteration 31/1000 | Loss: 0.00062592
Iteration 32/1000 | Loss: 0.00008027
Iteration 33/1000 | Loss: 0.00003840
Iteration 34/1000 | Loss: 0.00002954
Iteration 35/1000 | Loss: 0.00051534
Iteration 36/1000 | Loss: 0.00002953
Iteration 37/1000 | Loss: 0.00034683
Iteration 38/1000 | Loss: 0.00004398
Iteration 39/1000 | Loss: 0.00002616
Iteration 40/1000 | Loss: 0.00002352
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00003265
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001729
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00002711
Iteration 49/1000 | Loss: 0.00001841
Iteration 50/1000 | Loss: 0.00001641
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001614
Iteration 53/1000 | Loss: 0.00001614
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001661
Iteration 56/1000 | Loss: 0.00001660
Iteration 57/1000 | Loss: 0.00002521
Iteration 58/1000 | Loss: 0.00001635
Iteration 59/1000 | Loss: 0.00001634
Iteration 60/1000 | Loss: 0.00001877
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001576
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001576
Iteration 70/1000 | Loss: 0.00001576
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001580
Iteration 78/1000 | Loss: 0.00001571
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001567
Iteration 95/1000 | Loss: 0.00001567
Iteration 96/1000 | Loss: 0.00001567
Iteration 97/1000 | Loss: 0.00001566
Iteration 98/1000 | Loss: 0.00001566
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001565
Iteration 101/1000 | Loss: 0.00001565
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001556
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001555
Iteration 111/1000 | Loss: 0.00001555
Iteration 112/1000 | Loss: 0.00001555
Iteration 113/1000 | Loss: 0.00001555
Iteration 114/1000 | Loss: 0.00001555
Iteration 115/1000 | Loss: 0.00001555
Iteration 116/1000 | Loss: 0.00001555
Iteration 117/1000 | Loss: 0.00001555
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001554
Iteration 120/1000 | Loss: 0.00001662
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001545
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00027892
Iteration 129/1000 | Loss: 0.00002466
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001537
Iteration 133/1000 | Loss: 0.00001535
Iteration 134/1000 | Loss: 0.00001534
Iteration 135/1000 | Loss: 0.00001534
Iteration 136/1000 | Loss: 0.00001529
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001527
Iteration 140/1000 | Loss: 0.00001527
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001526
Iteration 143/1000 | Loss: 0.00001526
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001524
Iteration 146/1000 | Loss: 0.00001524
Iteration 147/1000 | Loss: 0.00001897
Iteration 148/1000 | Loss: 0.00001524
Iteration 149/1000 | Loss: 0.00001524
Iteration 150/1000 | Loss: 0.00001523
Iteration 151/1000 | Loss: 0.00001523
Iteration 152/1000 | Loss: 0.00001523
Iteration 153/1000 | Loss: 0.00001523
Iteration 154/1000 | Loss: 0.00001523
Iteration 155/1000 | Loss: 0.00001523
Iteration 156/1000 | Loss: 0.00001523
Iteration 157/1000 | Loss: 0.00001522
Iteration 158/1000 | Loss: 0.00001522
Iteration 159/1000 | Loss: 0.00001521
Iteration 160/1000 | Loss: 0.00001521
Iteration 161/1000 | Loss: 0.00001521
Iteration 162/1000 | Loss: 0.00001521
Iteration 163/1000 | Loss: 0.00001521
Iteration 164/1000 | Loss: 0.00001521
Iteration 165/1000 | Loss: 0.00001520
Iteration 166/1000 | Loss: 0.00001520
Iteration 167/1000 | Loss: 0.00001828
Iteration 168/1000 | Loss: 0.00001521
Iteration 169/1000 | Loss: 0.00001519
Iteration 170/1000 | Loss: 0.00001519
Iteration 171/1000 | Loss: 0.00001519
Iteration 172/1000 | Loss: 0.00001517
Iteration 173/1000 | Loss: 0.00001516
Iteration 174/1000 | Loss: 0.00001516
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00001515
Iteration 177/1000 | Loss: 0.00001514
Iteration 178/1000 | Loss: 0.00001512
Iteration 179/1000 | Loss: 0.00001497
Iteration 180/1000 | Loss: 0.00003469
Iteration 181/1000 | Loss: 0.00001479
Iteration 182/1000 | Loss: 0.00001470
Iteration 183/1000 | Loss: 0.00001467
Iteration 184/1000 | Loss: 0.00001466
Iteration 185/1000 | Loss: 0.00001455
Iteration 186/1000 | Loss: 0.00001454
Iteration 187/1000 | Loss: 0.00034476
Iteration 188/1000 | Loss: 0.00006896
Iteration 189/1000 | Loss: 0.00004016
Iteration 190/1000 | Loss: 0.00001594
Iteration 191/1000 | Loss: 0.00002442
Iteration 192/1000 | Loss: 0.00035187
Iteration 193/1000 | Loss: 0.00038655
Iteration 194/1000 | Loss: 0.00013624
Iteration 195/1000 | Loss: 0.00003052
Iteration 196/1000 | Loss: 0.00001921
Iteration 197/1000 | Loss: 0.00001455
Iteration 198/1000 | Loss: 0.00001533
Iteration 199/1000 | Loss: 0.00001442
Iteration 200/1000 | Loss: 0.00001441
Iteration 201/1000 | Loss: 0.00001441
Iteration 202/1000 | Loss: 0.00001441
Iteration 203/1000 | Loss: 0.00001441
Iteration 204/1000 | Loss: 0.00001441
Iteration 205/1000 | Loss: 0.00001441
Iteration 206/1000 | Loss: 0.00001441
Iteration 207/1000 | Loss: 0.00001441
Iteration 208/1000 | Loss: 0.00001441
Iteration 209/1000 | Loss: 0.00001441
Iteration 210/1000 | Loss: 0.00001441
Iteration 211/1000 | Loss: 0.00001440
Iteration 212/1000 | Loss: 0.00001440
Iteration 213/1000 | Loss: 0.00001438
Iteration 214/1000 | Loss: 0.00001437
Iteration 215/1000 | Loss: 0.00001437
Iteration 216/1000 | Loss: 0.00001437
Iteration 217/1000 | Loss: 0.00001436
Iteration 218/1000 | Loss: 0.00001436
Iteration 219/1000 | Loss: 0.00001436
Iteration 220/1000 | Loss: 0.00001436
Iteration 221/1000 | Loss: 0.00001436
Iteration 222/1000 | Loss: 0.00001436
Iteration 223/1000 | Loss: 0.00001436
Iteration 224/1000 | Loss: 0.00001436
Iteration 225/1000 | Loss: 0.00001436
Iteration 226/1000 | Loss: 0.00001436
Iteration 227/1000 | Loss: 0.00001436
Iteration 228/1000 | Loss: 0.00001435
Iteration 229/1000 | Loss: 0.00001435
Iteration 230/1000 | Loss: 0.00001435
Iteration 231/1000 | Loss: 0.00001435
Iteration 232/1000 | Loss: 0.00001435
Iteration 233/1000 | Loss: 0.00001435
Iteration 234/1000 | Loss: 0.00001434
Iteration 235/1000 | Loss: 0.00001434
Iteration 236/1000 | Loss: 0.00001434
Iteration 237/1000 | Loss: 0.00001434
Iteration 238/1000 | Loss: 0.00001434
Iteration 239/1000 | Loss: 0.00001434
Iteration 240/1000 | Loss: 0.00001434
Iteration 241/1000 | Loss: 0.00001434
Iteration 242/1000 | Loss: 0.00001434
Iteration 243/1000 | Loss: 0.00001434
Iteration 244/1000 | Loss: 0.00001433
Iteration 245/1000 | Loss: 0.00001433
Iteration 246/1000 | Loss: 0.00001433
Iteration 247/1000 | Loss: 0.00001433
Iteration 248/1000 | Loss: 0.00001433
Iteration 249/1000 | Loss: 0.00001433
Iteration 250/1000 | Loss: 0.00001433
Iteration 251/1000 | Loss: 0.00001433
Iteration 252/1000 | Loss: 0.00001433
Iteration 253/1000 | Loss: 0.00001433
Iteration 254/1000 | Loss: 0.00001433
Iteration 255/1000 | Loss: 0.00001433
Iteration 256/1000 | Loss: 0.00001433
Iteration 257/1000 | Loss: 0.00001433
Iteration 258/1000 | Loss: 0.00001433
Iteration 259/1000 | Loss: 0.00001433
Iteration 260/1000 | Loss: 0.00001433
Iteration 261/1000 | Loss: 0.00001433
Iteration 262/1000 | Loss: 0.00001433
Iteration 263/1000 | Loss: 0.00001433
Iteration 264/1000 | Loss: 0.00001433
Iteration 265/1000 | Loss: 0.00001433
Iteration 266/1000 | Loss: 0.00001433
Iteration 267/1000 | Loss: 0.00001433
Iteration 268/1000 | Loss: 0.00001433
Iteration 269/1000 | Loss: 0.00001433
Iteration 270/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.4325446500151884e-05, 1.4325446500151884e-05, 1.4325446500151884e-05, 1.4325446500151884e-05, 1.4325446500151884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4325446500151884e-05

Optimization complete. Final v2v error: 2.9244046211242676 mm

Highest mean error: 10.87670612335205 mm for frame 97

Lowest mean error: 2.4636571407318115 mm for frame 125

Saving results

Total time: 169.93618273735046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941050
Iteration 2/25 | Loss: 0.00484542
Iteration 3/25 | Loss: 0.00364426
Iteration 4/25 | Loss: 0.00313091
Iteration 5/25 | Loss: 0.00263054
Iteration 6/25 | Loss: 0.00217596
Iteration 7/25 | Loss: 0.00190775
Iteration 8/25 | Loss: 0.00182882
Iteration 9/25 | Loss: 0.00177350
Iteration 10/25 | Loss: 0.00182191
Iteration 11/25 | Loss: 0.00174960
Iteration 12/25 | Loss: 0.00159485
Iteration 13/25 | Loss: 0.00156636
Iteration 14/25 | Loss: 0.00153213
Iteration 15/25 | Loss: 0.00152638
Iteration 16/25 | Loss: 0.00151042
Iteration 17/25 | Loss: 0.00149509
Iteration 18/25 | Loss: 0.00146176
Iteration 19/25 | Loss: 0.00147939
Iteration 20/25 | Loss: 0.00142745
Iteration 21/25 | Loss: 0.00144884
Iteration 22/25 | Loss: 0.00141376
Iteration 23/25 | Loss: 0.00142003
Iteration 24/25 | Loss: 0.00140923
Iteration 25/25 | Loss: 0.00142080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33283579
Iteration 2/25 | Loss: 0.00147000
Iteration 3/25 | Loss: 0.00147000
Iteration 4/25 | Loss: 0.00147000
Iteration 5/25 | Loss: 0.00147000
Iteration 6/25 | Loss: 0.00147000
Iteration 7/25 | Loss: 0.00147000
Iteration 8/25 | Loss: 0.00147000
Iteration 9/25 | Loss: 0.00147000
Iteration 10/25 | Loss: 0.00147000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001469996408559382, 0.001469996408559382, 0.001469996408559382, 0.001469996408559382, 0.001469996408559382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001469996408559382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147000
Iteration 2/1000 | Loss: 0.00054258
Iteration 3/1000 | Loss: 0.00044518
Iteration 4/1000 | Loss: 0.00035435
Iteration 5/1000 | Loss: 0.00042166
Iteration 6/1000 | Loss: 0.00038087
Iteration 7/1000 | Loss: 0.00093731
Iteration 8/1000 | Loss: 0.00064316
Iteration 9/1000 | Loss: 0.00068618
Iteration 10/1000 | Loss: 0.00181800
Iteration 11/1000 | Loss: 0.00257299
Iteration 12/1000 | Loss: 0.00157298
Iteration 13/1000 | Loss: 0.00657195
Iteration 14/1000 | Loss: 0.00509811
Iteration 15/1000 | Loss: 0.00064685
Iteration 16/1000 | Loss: 0.00061909
Iteration 17/1000 | Loss: 0.00037928
Iteration 18/1000 | Loss: 0.00043903
Iteration 19/1000 | Loss: 0.00035620
Iteration 20/1000 | Loss: 0.00038235
Iteration 21/1000 | Loss: 0.00098546
Iteration 22/1000 | Loss: 0.00208504
Iteration 23/1000 | Loss: 0.00058389
Iteration 24/1000 | Loss: 0.00039105
Iteration 25/1000 | Loss: 0.00114863
Iteration 26/1000 | Loss: 0.00057254
Iteration 27/1000 | Loss: 0.00033760
Iteration 28/1000 | Loss: 0.00086512
Iteration 29/1000 | Loss: 0.00044338
Iteration 30/1000 | Loss: 0.00040865
Iteration 31/1000 | Loss: 0.00054902
Iteration 32/1000 | Loss: 0.00039130
Iteration 33/1000 | Loss: 0.00073485
Iteration 34/1000 | Loss: 0.00034404
Iteration 35/1000 | Loss: 0.00076308
Iteration 36/1000 | Loss: 0.00034603
Iteration 37/1000 | Loss: 0.00043540
Iteration 38/1000 | Loss: 0.00146348
Iteration 39/1000 | Loss: 0.00034262
Iteration 40/1000 | Loss: 0.00038748
Iteration 41/1000 | Loss: 0.00108236
Iteration 42/1000 | Loss: 0.00012065
Iteration 43/1000 | Loss: 0.00015386
Iteration 44/1000 | Loss: 0.00019542
Iteration 45/1000 | Loss: 0.00015256
Iteration 46/1000 | Loss: 0.00016410
Iteration 47/1000 | Loss: 0.00030262
Iteration 48/1000 | Loss: 0.00019903
Iteration 49/1000 | Loss: 0.00020833
Iteration 50/1000 | Loss: 0.00014600
Iteration 51/1000 | Loss: 0.00029131
Iteration 52/1000 | Loss: 0.00014652
Iteration 53/1000 | Loss: 0.00011341
Iteration 54/1000 | Loss: 0.00016199
Iteration 55/1000 | Loss: 0.00015095
Iteration 56/1000 | Loss: 0.00023887
Iteration 57/1000 | Loss: 0.00012533
Iteration 58/1000 | Loss: 0.00014571
Iteration 59/1000 | Loss: 0.00023245
Iteration 60/1000 | Loss: 0.00019348
Iteration 61/1000 | Loss: 0.00014862
Iteration 62/1000 | Loss: 0.00011432
Iteration 63/1000 | Loss: 0.00018664
Iteration 64/1000 | Loss: 0.00010583
Iteration 65/1000 | Loss: 0.00012126
Iteration 66/1000 | Loss: 0.00008354
Iteration 67/1000 | Loss: 0.00007914
Iteration 68/1000 | Loss: 0.00009959
Iteration 69/1000 | Loss: 0.00025404
Iteration 70/1000 | Loss: 0.00011157
Iteration 71/1000 | Loss: 0.00017903
Iteration 72/1000 | Loss: 0.00010367
Iteration 73/1000 | Loss: 0.00009692
Iteration 74/1000 | Loss: 0.00007882
Iteration 75/1000 | Loss: 0.00019837
Iteration 76/1000 | Loss: 0.00010510
Iteration 77/1000 | Loss: 0.00007092
Iteration 78/1000 | Loss: 0.00006979
Iteration 79/1000 | Loss: 0.00007819
Iteration 80/1000 | Loss: 0.00011116
Iteration 81/1000 | Loss: 0.00006642
Iteration 82/1000 | Loss: 0.00006817
Iteration 83/1000 | Loss: 0.00006536
Iteration 84/1000 | Loss: 0.00012634
Iteration 85/1000 | Loss: 0.00007183
Iteration 86/1000 | Loss: 0.00007739
Iteration 87/1000 | Loss: 0.00005966
Iteration 88/1000 | Loss: 0.00012950
Iteration 89/1000 | Loss: 0.00005878
Iteration 90/1000 | Loss: 0.00006645
Iteration 91/1000 | Loss: 0.00005557
Iteration 92/1000 | Loss: 0.00012411
Iteration 93/1000 | Loss: 0.00005926
Iteration 94/1000 | Loss: 0.00025050
Iteration 95/1000 | Loss: 0.00008074
Iteration 96/1000 | Loss: 0.00011298
Iteration 97/1000 | Loss: 0.00012604
Iteration 98/1000 | Loss: 0.00018079
Iteration 99/1000 | Loss: 0.00009010
Iteration 100/1000 | Loss: 0.00014608
Iteration 101/1000 | Loss: 0.00006514
Iteration 102/1000 | Loss: 0.00009092
Iteration 103/1000 | Loss: 0.00005881
Iteration 104/1000 | Loss: 0.00009809
Iteration 105/1000 | Loss: 0.00005714
Iteration 106/1000 | Loss: 0.00031624
Iteration 107/1000 | Loss: 0.00035061
Iteration 108/1000 | Loss: 0.00018227
Iteration 109/1000 | Loss: 0.00002706
Iteration 110/1000 | Loss: 0.00004703
Iteration 111/1000 | Loss: 0.00004488
Iteration 112/1000 | Loss: 0.00005817
Iteration 113/1000 | Loss: 0.00004777
Iteration 114/1000 | Loss: 0.00007015
Iteration 115/1000 | Loss: 0.00006052
Iteration 116/1000 | Loss: 0.00006837
Iteration 117/1000 | Loss: 0.00005823
Iteration 118/1000 | Loss: 0.00006425
Iteration 119/1000 | Loss: 0.00013567
Iteration 120/1000 | Loss: 0.00034914
Iteration 121/1000 | Loss: 0.00006142
Iteration 122/1000 | Loss: 0.00014722
Iteration 123/1000 | Loss: 0.00004490
Iteration 124/1000 | Loss: 0.00003908
Iteration 125/1000 | Loss: 0.00009830
Iteration 126/1000 | Loss: 0.00004586
Iteration 127/1000 | Loss: 0.00012566
Iteration 128/1000 | Loss: 0.00006302
Iteration 129/1000 | Loss: 0.00008508
Iteration 130/1000 | Loss: 0.00004100
Iteration 131/1000 | Loss: 0.00004420
Iteration 132/1000 | Loss: 0.00004016
Iteration 133/1000 | Loss: 0.00004397
Iteration 134/1000 | Loss: 0.00003970
Iteration 135/1000 | Loss: 0.00004396
Iteration 136/1000 | Loss: 0.00006361
Iteration 137/1000 | Loss: 0.00004550
Iteration 138/1000 | Loss: 0.00066873
Iteration 139/1000 | Loss: 0.00006414
Iteration 140/1000 | Loss: 0.00006906
Iteration 141/1000 | Loss: 0.00006320
Iteration 142/1000 | Loss: 0.00004259
Iteration 143/1000 | Loss: 0.00005498
Iteration 144/1000 | Loss: 0.00004419
Iteration 145/1000 | Loss: 0.00011503
Iteration 146/1000 | Loss: 0.00004699
Iteration 147/1000 | Loss: 0.00004850
Iteration 148/1000 | Loss: 0.00006021
Iteration 149/1000 | Loss: 0.00006416
Iteration 150/1000 | Loss: 0.00006504
Iteration 151/1000 | Loss: 0.00006348
Iteration 152/1000 | Loss: 0.00004653
Iteration 153/1000 | Loss: 0.00006318
Iteration 154/1000 | Loss: 0.00004779
Iteration 155/1000 | Loss: 0.00006223
Iteration 156/1000 | Loss: 0.00005081
Iteration 157/1000 | Loss: 0.00006025
Iteration 158/1000 | Loss: 0.00021232
Iteration 159/1000 | Loss: 0.00011351
Iteration 160/1000 | Loss: 0.00020786
Iteration 161/1000 | Loss: 0.00016412
Iteration 162/1000 | Loss: 0.00006679
Iteration 163/1000 | Loss: 0.00009966
Iteration 164/1000 | Loss: 0.00007664
Iteration 165/1000 | Loss: 0.00004084
Iteration 166/1000 | Loss: 0.00003240
Iteration 167/1000 | Loss: 0.00004848
Iteration 168/1000 | Loss: 0.00007950
Iteration 169/1000 | Loss: 0.00003407
Iteration 170/1000 | Loss: 0.00004591
Iteration 171/1000 | Loss: 0.00006317
Iteration 172/1000 | Loss: 0.00004657
Iteration 173/1000 | Loss: 0.00003874
Iteration 174/1000 | Loss: 0.00006221
Iteration 175/1000 | Loss: 0.00004408
Iteration 176/1000 | Loss: 0.00003989
Iteration 177/1000 | Loss: 0.00006320
Iteration 178/1000 | Loss: 0.00004112
Iteration 179/1000 | Loss: 0.00003822
Iteration 180/1000 | Loss: 0.00006286
Iteration 181/1000 | Loss: 0.00003953
Iteration 182/1000 | Loss: 0.00004307
Iteration 183/1000 | Loss: 0.00009192
Iteration 184/1000 | Loss: 0.00006526
Iteration 185/1000 | Loss: 0.00006283
Iteration 186/1000 | Loss: 0.00006242
Iteration 187/1000 | Loss: 0.00006767
Iteration 188/1000 | Loss: 0.00006287
Iteration 189/1000 | Loss: 0.00006499
Iteration 190/1000 | Loss: 0.00005199
Iteration 191/1000 | Loss: 0.00002195
Iteration 192/1000 | Loss: 0.00003927
Iteration 193/1000 | Loss: 0.00007030
Iteration 194/1000 | Loss: 0.00006057
Iteration 195/1000 | Loss: 0.00005001
Iteration 196/1000 | Loss: 0.00006317
Iteration 197/1000 | Loss: 0.00009170
Iteration 198/1000 | Loss: 0.00006918
Iteration 199/1000 | Loss: 0.00004937
Iteration 200/1000 | Loss: 0.00004680
Iteration 201/1000 | Loss: 0.00004160
Iteration 202/1000 | Loss: 0.00037428
Iteration 203/1000 | Loss: 0.00004784
Iteration 204/1000 | Loss: 0.00020663
Iteration 205/1000 | Loss: 0.00005875
Iteration 206/1000 | Loss: 0.00005713
Iteration 207/1000 | Loss: 0.00002351
Iteration 208/1000 | Loss: 0.00002144
Iteration 209/1000 | Loss: 0.00002060
Iteration 210/1000 | Loss: 0.00002031
Iteration 211/1000 | Loss: 0.00002020
Iteration 212/1000 | Loss: 0.00002007
Iteration 213/1000 | Loss: 0.00002006
Iteration 214/1000 | Loss: 0.00001996
Iteration 215/1000 | Loss: 0.00001994
Iteration 216/1000 | Loss: 0.00001990
Iteration 217/1000 | Loss: 0.00001986
Iteration 218/1000 | Loss: 0.00001985
Iteration 219/1000 | Loss: 0.00001985
Iteration 220/1000 | Loss: 0.00001985
Iteration 221/1000 | Loss: 0.00001985
Iteration 222/1000 | Loss: 0.00001985
Iteration 223/1000 | Loss: 0.00001985
Iteration 224/1000 | Loss: 0.00001985
Iteration 225/1000 | Loss: 0.00001985
Iteration 226/1000 | Loss: 0.00001984
Iteration 227/1000 | Loss: 0.00001984
Iteration 228/1000 | Loss: 0.00001984
Iteration 229/1000 | Loss: 0.00001984
Iteration 230/1000 | Loss: 0.00001984
Iteration 231/1000 | Loss: 0.00001984
Iteration 232/1000 | Loss: 0.00001984
Iteration 233/1000 | Loss: 0.00001984
Iteration 234/1000 | Loss: 0.00001984
Iteration 235/1000 | Loss: 0.00001983
Iteration 236/1000 | Loss: 0.00001983
Iteration 237/1000 | Loss: 0.00001983
Iteration 238/1000 | Loss: 0.00001983
Iteration 239/1000 | Loss: 0.00001983
Iteration 240/1000 | Loss: 0.00001983
Iteration 241/1000 | Loss: 0.00001981
Iteration 242/1000 | Loss: 0.00001980
Iteration 243/1000 | Loss: 0.00001980
Iteration 244/1000 | Loss: 0.00001980
Iteration 245/1000 | Loss: 0.00001980
Iteration 246/1000 | Loss: 0.00001980
Iteration 247/1000 | Loss: 0.00001980
Iteration 248/1000 | Loss: 0.00001980
Iteration 249/1000 | Loss: 0.00001980
Iteration 250/1000 | Loss: 0.00001980
Iteration 251/1000 | Loss: 0.00001979
Iteration 252/1000 | Loss: 0.00001979
Iteration 253/1000 | Loss: 0.00001978
Iteration 254/1000 | Loss: 0.00001977
Iteration 255/1000 | Loss: 0.00001977
Iteration 256/1000 | Loss: 0.00001977
Iteration 257/1000 | Loss: 0.00001977
Iteration 258/1000 | Loss: 0.00001977
Iteration 259/1000 | Loss: 0.00001977
Iteration 260/1000 | Loss: 0.00001976
Iteration 261/1000 | Loss: 0.00001976
Iteration 262/1000 | Loss: 0.00001976
Iteration 263/1000 | Loss: 0.00001976
Iteration 264/1000 | Loss: 0.00001976
Iteration 265/1000 | Loss: 0.00001976
Iteration 266/1000 | Loss: 0.00001976
Iteration 267/1000 | Loss: 0.00001975
Iteration 268/1000 | Loss: 0.00001974
Iteration 269/1000 | Loss: 0.00001974
Iteration 270/1000 | Loss: 0.00001974
Iteration 271/1000 | Loss: 0.00001974
Iteration 272/1000 | Loss: 0.00001973
Iteration 273/1000 | Loss: 0.00001973
Iteration 274/1000 | Loss: 0.00001972
Iteration 275/1000 | Loss: 0.00001972
Iteration 276/1000 | Loss: 0.00001972
Iteration 277/1000 | Loss: 0.00001972
Iteration 278/1000 | Loss: 0.00001971
Iteration 279/1000 | Loss: 0.00001970
Iteration 280/1000 | Loss: 0.00001970
Iteration 281/1000 | Loss: 0.00001970
Iteration 282/1000 | Loss: 0.00001970
Iteration 283/1000 | Loss: 0.00001969
Iteration 284/1000 | Loss: 0.00001969
Iteration 285/1000 | Loss: 0.00001969
Iteration 286/1000 | Loss: 0.00001969
Iteration 287/1000 | Loss: 0.00001969
Iteration 288/1000 | Loss: 0.00001969
Iteration 289/1000 | Loss: 0.00001969
Iteration 290/1000 | Loss: 0.00001969
Iteration 291/1000 | Loss: 0.00001969
Iteration 292/1000 | Loss: 0.00001969
Iteration 293/1000 | Loss: 0.00001969
Iteration 294/1000 | Loss: 0.00001968
Iteration 295/1000 | Loss: 0.00001968
Iteration 296/1000 | Loss: 0.00001967
Iteration 297/1000 | Loss: 0.00001964
Iteration 298/1000 | Loss: 0.00001963
Iteration 299/1000 | Loss: 0.00001963
Iteration 300/1000 | Loss: 0.00001958
Iteration 301/1000 | Loss: 0.00001957
Iteration 302/1000 | Loss: 0.00001957
Iteration 303/1000 | Loss: 0.00001957
Iteration 304/1000 | Loss: 0.00001956
Iteration 305/1000 | Loss: 0.00001955
Iteration 306/1000 | Loss: 0.00001955
Iteration 307/1000 | Loss: 0.00001955
Iteration 308/1000 | Loss: 0.00001955
Iteration 309/1000 | Loss: 0.00001955
Iteration 310/1000 | Loss: 0.00001955
Iteration 311/1000 | Loss: 0.00001955
Iteration 312/1000 | Loss: 0.00001955
Iteration 313/1000 | Loss: 0.00001955
Iteration 314/1000 | Loss: 0.00001955
Iteration 315/1000 | Loss: 0.00001955
Iteration 316/1000 | Loss: 0.00001955
Iteration 317/1000 | Loss: 0.00001955
Iteration 318/1000 | Loss: 0.00001955
Iteration 319/1000 | Loss: 0.00001955
Iteration 320/1000 | Loss: 0.00001955
Iteration 321/1000 | Loss: 0.00001955
Iteration 322/1000 | Loss: 0.00001955
Iteration 323/1000 | Loss: 0.00001955
Iteration 324/1000 | Loss: 0.00001955
Iteration 325/1000 | Loss: 0.00001955
Iteration 326/1000 | Loss: 0.00001955
Iteration 327/1000 | Loss: 0.00001955
Iteration 328/1000 | Loss: 0.00001955
Iteration 329/1000 | Loss: 0.00001955
Iteration 330/1000 | Loss: 0.00001955
Iteration 331/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.9549052012735046e-05, 1.9549052012735046e-05, 1.9549052012735046e-05, 1.9549052012735046e-05, 1.9549052012735046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9549052012735046e-05

Optimization complete. Final v2v error: 3.7605905532836914 mm

Highest mean error: 5.387357234954834 mm for frame 169

Lowest mean error: 3.1929988861083984 mm for frame 24

Saving results

Total time: 373.5869641304016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375650
Iteration 2/25 | Loss: 0.00126257
Iteration 3/25 | Loss: 0.00112095
Iteration 4/25 | Loss: 0.00109993
Iteration 5/25 | Loss: 0.00109316
Iteration 6/25 | Loss: 0.00109089
Iteration 7/25 | Loss: 0.00108998
Iteration 8/25 | Loss: 0.00108998
Iteration 9/25 | Loss: 0.00108998
Iteration 10/25 | Loss: 0.00108998
Iteration 11/25 | Loss: 0.00108998
Iteration 12/25 | Loss: 0.00108998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010899794287979603, 0.0010899794287979603, 0.0010899794287979603, 0.0010899794287979603, 0.0010899794287979603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010899794287979603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43047547
Iteration 2/25 | Loss: 0.00111488
Iteration 3/25 | Loss: 0.00111488
Iteration 4/25 | Loss: 0.00111488
Iteration 5/25 | Loss: 0.00111488
Iteration 6/25 | Loss: 0.00111488
Iteration 7/25 | Loss: 0.00111487
Iteration 8/25 | Loss: 0.00111487
Iteration 9/25 | Loss: 0.00111487
Iteration 10/25 | Loss: 0.00111487
Iteration 11/25 | Loss: 0.00111487
Iteration 12/25 | Loss: 0.00111487
Iteration 13/25 | Loss: 0.00111487
Iteration 14/25 | Loss: 0.00111487
Iteration 15/25 | Loss: 0.00111487
Iteration 16/25 | Loss: 0.00111487
Iteration 17/25 | Loss: 0.00111487
Iteration 18/25 | Loss: 0.00111487
Iteration 19/25 | Loss: 0.00111487
Iteration 20/25 | Loss: 0.00111487
Iteration 21/25 | Loss: 0.00111487
Iteration 22/25 | Loss: 0.00111487
Iteration 23/25 | Loss: 0.00111487
Iteration 24/25 | Loss: 0.00111487
Iteration 25/25 | Loss: 0.00111487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111487
Iteration 2/1000 | Loss: 0.00002627
Iteration 3/1000 | Loss: 0.00001711
Iteration 4/1000 | Loss: 0.00001577
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001451
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001392
Iteration 9/1000 | Loss: 0.00001372
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001338
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001328
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001322
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001316
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001312
Iteration 25/1000 | Loss: 0.00001312
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001310
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001306
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001304
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001303
Iteration 72/1000 | Loss: 0.00001303
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001303
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001302
Iteration 88/1000 | Loss: 0.00001302
Iteration 89/1000 | Loss: 0.00001302
Iteration 90/1000 | Loss: 0.00001302
Iteration 91/1000 | Loss: 0.00001302
Iteration 92/1000 | Loss: 0.00001302
Iteration 93/1000 | Loss: 0.00001301
Iteration 94/1000 | Loss: 0.00001301
Iteration 95/1000 | Loss: 0.00001301
Iteration 96/1000 | Loss: 0.00001301
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001299
Iteration 101/1000 | Loss: 0.00001299
Iteration 102/1000 | Loss: 0.00001299
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001298
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001298
Iteration 109/1000 | Loss: 0.00001297
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001293
Iteration 128/1000 | Loss: 0.00001293
Iteration 129/1000 | Loss: 0.00001293
Iteration 130/1000 | Loss: 0.00001293
Iteration 131/1000 | Loss: 0.00001293
Iteration 132/1000 | Loss: 0.00001292
Iteration 133/1000 | Loss: 0.00001292
Iteration 134/1000 | Loss: 0.00001292
Iteration 135/1000 | Loss: 0.00001292
Iteration 136/1000 | Loss: 0.00001292
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001291
Iteration 141/1000 | Loss: 0.00001291
Iteration 142/1000 | Loss: 0.00001291
Iteration 143/1000 | Loss: 0.00001291
Iteration 144/1000 | Loss: 0.00001290
Iteration 145/1000 | Loss: 0.00001290
Iteration 146/1000 | Loss: 0.00001290
Iteration 147/1000 | Loss: 0.00001290
Iteration 148/1000 | Loss: 0.00001290
Iteration 149/1000 | Loss: 0.00001290
Iteration 150/1000 | Loss: 0.00001290
Iteration 151/1000 | Loss: 0.00001290
Iteration 152/1000 | Loss: 0.00001290
Iteration 153/1000 | Loss: 0.00001290
Iteration 154/1000 | Loss: 0.00001290
Iteration 155/1000 | Loss: 0.00001290
Iteration 156/1000 | Loss: 0.00001290
Iteration 157/1000 | Loss: 0.00001289
Iteration 158/1000 | Loss: 0.00001289
Iteration 159/1000 | Loss: 0.00001289
Iteration 160/1000 | Loss: 0.00001289
Iteration 161/1000 | Loss: 0.00001289
Iteration 162/1000 | Loss: 0.00001289
Iteration 163/1000 | Loss: 0.00001288
Iteration 164/1000 | Loss: 0.00001288
Iteration 165/1000 | Loss: 0.00001288
Iteration 166/1000 | Loss: 0.00001288
Iteration 167/1000 | Loss: 0.00001288
Iteration 168/1000 | Loss: 0.00001288
Iteration 169/1000 | Loss: 0.00001288
Iteration 170/1000 | Loss: 0.00001288
Iteration 171/1000 | Loss: 0.00001288
Iteration 172/1000 | Loss: 0.00001288
Iteration 173/1000 | Loss: 0.00001287
Iteration 174/1000 | Loss: 0.00001287
Iteration 175/1000 | Loss: 0.00001287
Iteration 176/1000 | Loss: 0.00001287
Iteration 177/1000 | Loss: 0.00001287
Iteration 178/1000 | Loss: 0.00001287
Iteration 179/1000 | Loss: 0.00001287
Iteration 180/1000 | Loss: 0.00001287
Iteration 181/1000 | Loss: 0.00001287
Iteration 182/1000 | Loss: 0.00001287
Iteration 183/1000 | Loss: 0.00001287
Iteration 184/1000 | Loss: 0.00001287
Iteration 185/1000 | Loss: 0.00001287
Iteration 186/1000 | Loss: 0.00001287
Iteration 187/1000 | Loss: 0.00001287
Iteration 188/1000 | Loss: 0.00001286
Iteration 189/1000 | Loss: 0.00001286
Iteration 190/1000 | Loss: 0.00001286
Iteration 191/1000 | Loss: 0.00001286
Iteration 192/1000 | Loss: 0.00001286
Iteration 193/1000 | Loss: 0.00001286
Iteration 194/1000 | Loss: 0.00001286
Iteration 195/1000 | Loss: 0.00001286
Iteration 196/1000 | Loss: 0.00001286
Iteration 197/1000 | Loss: 0.00001286
Iteration 198/1000 | Loss: 0.00001286
Iteration 199/1000 | Loss: 0.00001286
Iteration 200/1000 | Loss: 0.00001286
Iteration 201/1000 | Loss: 0.00001286
Iteration 202/1000 | Loss: 0.00001286
Iteration 203/1000 | Loss: 0.00001286
Iteration 204/1000 | Loss: 0.00001286
Iteration 205/1000 | Loss: 0.00001286
Iteration 206/1000 | Loss: 0.00001286
Iteration 207/1000 | Loss: 0.00001286
Iteration 208/1000 | Loss: 0.00001286
Iteration 209/1000 | Loss: 0.00001286
Iteration 210/1000 | Loss: 0.00001286
Iteration 211/1000 | Loss: 0.00001286
Iteration 212/1000 | Loss: 0.00001286
Iteration 213/1000 | Loss: 0.00001286
Iteration 214/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.2857580259151291e-05, 1.2857580259151291e-05, 1.2857580259151291e-05, 1.2857580259151291e-05, 1.2857580259151291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2857580259151291e-05

Optimization complete. Final v2v error: 2.977773904800415 mm

Highest mean error: 3.676227569580078 mm for frame 143

Lowest mean error: 2.3148555755615234 mm for frame 6

Saving results

Total time: 41.40729594230652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065921
Iteration 2/25 | Loss: 0.01065921
Iteration 3/25 | Loss: 0.01065921
Iteration 4/25 | Loss: 0.01065921
Iteration 5/25 | Loss: 0.01065920
Iteration 6/25 | Loss: 0.01065920
Iteration 7/25 | Loss: 0.01065920
Iteration 8/25 | Loss: 0.01065920
Iteration 9/25 | Loss: 0.01065920
Iteration 10/25 | Loss: 0.01065920
Iteration 11/25 | Loss: 0.01065920
Iteration 12/25 | Loss: 0.01065920
Iteration 13/25 | Loss: 0.01065920
Iteration 14/25 | Loss: 0.01065920
Iteration 15/25 | Loss: 0.01065920
Iteration 16/25 | Loss: 0.01065920
Iteration 17/25 | Loss: 0.01065920
Iteration 18/25 | Loss: 0.01065920
Iteration 19/25 | Loss: 0.01065919
Iteration 20/25 | Loss: 0.01065919
Iteration 21/25 | Loss: 0.01065919
Iteration 22/25 | Loss: 0.01065919
Iteration 23/25 | Loss: 0.01065919
Iteration 24/25 | Loss: 0.01065919
Iteration 25/25 | Loss: 0.01065919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56606209
Iteration 2/25 | Loss: 0.06505383
Iteration 3/25 | Loss: 0.06504032
Iteration 4/25 | Loss: 0.06504032
Iteration 5/25 | Loss: 0.06504031
Iteration 6/25 | Loss: 0.06504031
Iteration 7/25 | Loss: 0.06504031
Iteration 8/25 | Loss: 0.06504031
Iteration 9/25 | Loss: 0.06504031
Iteration 10/25 | Loss: 0.06504031
Iteration 11/25 | Loss: 0.06504031
Iteration 12/25 | Loss: 0.06504031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.06504030525684357, 0.06504030525684357, 0.06504030525684357, 0.06504030525684357, 0.06504030525684357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06504030525684357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06504031
Iteration 2/1000 | Loss: 0.00347834
Iteration 3/1000 | Loss: 0.00162430
Iteration 4/1000 | Loss: 0.00161655
Iteration 5/1000 | Loss: 0.00101666
Iteration 6/1000 | Loss: 0.00022191
Iteration 7/1000 | Loss: 0.00089192
Iteration 8/1000 | Loss: 0.00364536
Iteration 9/1000 | Loss: 0.00237568
Iteration 10/1000 | Loss: 0.00177604
Iteration 11/1000 | Loss: 0.00094882
Iteration 12/1000 | Loss: 0.00088847
Iteration 13/1000 | Loss: 0.00008368
Iteration 14/1000 | Loss: 0.00055753
Iteration 15/1000 | Loss: 0.00007181
Iteration 16/1000 | Loss: 0.00092661
Iteration 17/1000 | Loss: 0.00076335
Iteration 18/1000 | Loss: 0.00030738
Iteration 19/1000 | Loss: 0.00028654
Iteration 20/1000 | Loss: 0.00060499
Iteration 21/1000 | Loss: 0.00004272
Iteration 22/1000 | Loss: 0.00006288
Iteration 23/1000 | Loss: 0.00003852
Iteration 24/1000 | Loss: 0.00003619
Iteration 25/1000 | Loss: 0.00072472
Iteration 26/1000 | Loss: 0.00191602
Iteration 27/1000 | Loss: 0.00012272
Iteration 28/1000 | Loss: 0.00028459
Iteration 29/1000 | Loss: 0.00003281
Iteration 30/1000 | Loss: 0.00014395
Iteration 31/1000 | Loss: 0.00042277
Iteration 32/1000 | Loss: 0.00003408
Iteration 33/1000 | Loss: 0.00003056
Iteration 34/1000 | Loss: 0.00020270
Iteration 35/1000 | Loss: 0.00002843
Iteration 36/1000 | Loss: 0.00050073
Iteration 37/1000 | Loss: 0.00003177
Iteration 38/1000 | Loss: 0.00002792
Iteration 39/1000 | Loss: 0.00002649
Iteration 40/1000 | Loss: 0.00002582
Iteration 41/1000 | Loss: 0.00002591
Iteration 42/1000 | Loss: 0.00037717
Iteration 43/1000 | Loss: 0.00002529
Iteration 44/1000 | Loss: 0.00002889
Iteration 45/1000 | Loss: 0.00002434
Iteration 46/1000 | Loss: 0.00047319
Iteration 47/1000 | Loss: 0.00002428
Iteration 48/1000 | Loss: 0.00002319
Iteration 49/1000 | Loss: 0.00002260
Iteration 50/1000 | Loss: 0.00002224
Iteration 51/1000 | Loss: 0.00002190
Iteration 52/1000 | Loss: 0.00002150
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002073
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002069
Iteration 59/1000 | Loss: 0.00002061
Iteration 60/1000 | Loss: 0.00002058
Iteration 61/1000 | Loss: 0.00002053
Iteration 62/1000 | Loss: 0.00002039
Iteration 63/1000 | Loss: 0.00002037
Iteration 64/1000 | Loss: 0.00002037
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002031
Iteration 68/1000 | Loss: 0.00002026
Iteration 69/1000 | Loss: 0.00002023
Iteration 70/1000 | Loss: 0.00002023
Iteration 71/1000 | Loss: 0.00002022
Iteration 72/1000 | Loss: 0.00002020
Iteration 73/1000 | Loss: 0.00002018
Iteration 74/1000 | Loss: 0.00002017
Iteration 75/1000 | Loss: 0.00002017
Iteration 76/1000 | Loss: 0.00002017
Iteration 77/1000 | Loss: 0.00002015
Iteration 78/1000 | Loss: 0.00002015
Iteration 79/1000 | Loss: 0.00002015
Iteration 80/1000 | Loss: 0.00002014
Iteration 81/1000 | Loss: 0.00002014
Iteration 82/1000 | Loss: 0.00002014
Iteration 83/1000 | Loss: 0.00002014
Iteration 84/1000 | Loss: 0.00002014
Iteration 85/1000 | Loss: 0.00002014
Iteration 86/1000 | Loss: 0.00002014
Iteration 87/1000 | Loss: 0.00002013
Iteration 88/1000 | Loss: 0.00002013
Iteration 89/1000 | Loss: 0.00002013
Iteration 90/1000 | Loss: 0.00002013
Iteration 91/1000 | Loss: 0.00002012
Iteration 92/1000 | Loss: 0.00002012
Iteration 93/1000 | Loss: 0.00002012
Iteration 94/1000 | Loss: 0.00002012
Iteration 95/1000 | Loss: 0.00002012
Iteration 96/1000 | Loss: 0.00002011
Iteration 97/1000 | Loss: 0.00002011
Iteration 98/1000 | Loss: 0.00002011
Iteration 99/1000 | Loss: 0.00002011
Iteration 100/1000 | Loss: 0.00002011
Iteration 101/1000 | Loss: 0.00002011
Iteration 102/1000 | Loss: 0.00002011
Iteration 103/1000 | Loss: 0.00002010
Iteration 104/1000 | Loss: 0.00002010
Iteration 105/1000 | Loss: 0.00002010
Iteration 106/1000 | Loss: 0.00002010
Iteration 107/1000 | Loss: 0.00002009
Iteration 108/1000 | Loss: 0.00002009
Iteration 109/1000 | Loss: 0.00002009
Iteration 110/1000 | Loss: 0.00002009
Iteration 111/1000 | Loss: 0.00002009
Iteration 112/1000 | Loss: 0.00002009
Iteration 113/1000 | Loss: 0.00002009
Iteration 114/1000 | Loss: 0.00002009
Iteration 115/1000 | Loss: 0.00002009
Iteration 116/1000 | Loss: 0.00002009
Iteration 117/1000 | Loss: 0.00002009
Iteration 118/1000 | Loss: 0.00002009
Iteration 119/1000 | Loss: 0.00002009
Iteration 120/1000 | Loss: 0.00002009
Iteration 121/1000 | Loss: 0.00002009
Iteration 122/1000 | Loss: 0.00002008
Iteration 123/1000 | Loss: 0.00002008
Iteration 124/1000 | Loss: 0.00002008
Iteration 125/1000 | Loss: 0.00002008
Iteration 126/1000 | Loss: 0.00002007
Iteration 127/1000 | Loss: 0.00002007
Iteration 128/1000 | Loss: 0.00002007
Iteration 129/1000 | Loss: 0.00002007
Iteration 130/1000 | Loss: 0.00002007
Iteration 131/1000 | Loss: 0.00002007
Iteration 132/1000 | Loss: 0.00002007
Iteration 133/1000 | Loss: 0.00002006
Iteration 134/1000 | Loss: 0.00002006
Iteration 135/1000 | Loss: 0.00002006
Iteration 136/1000 | Loss: 0.00002006
Iteration 137/1000 | Loss: 0.00002005
Iteration 138/1000 | Loss: 0.00002005
Iteration 139/1000 | Loss: 0.00002005
Iteration 140/1000 | Loss: 0.00002005
Iteration 141/1000 | Loss: 0.00002004
Iteration 142/1000 | Loss: 0.00002004
Iteration 143/1000 | Loss: 0.00002004
Iteration 144/1000 | Loss: 0.00002004
Iteration 145/1000 | Loss: 0.00002004
Iteration 146/1000 | Loss: 0.00002004
Iteration 147/1000 | Loss: 0.00002003
Iteration 148/1000 | Loss: 0.00002003
Iteration 149/1000 | Loss: 0.00002003
Iteration 150/1000 | Loss: 0.00002003
Iteration 151/1000 | Loss: 0.00002003
Iteration 152/1000 | Loss: 0.00002003
Iteration 153/1000 | Loss: 0.00002003
Iteration 154/1000 | Loss: 0.00002003
Iteration 155/1000 | Loss: 0.00002003
Iteration 156/1000 | Loss: 0.00002003
Iteration 157/1000 | Loss: 0.00002002
Iteration 158/1000 | Loss: 0.00002002
Iteration 159/1000 | Loss: 0.00002002
Iteration 160/1000 | Loss: 0.00002002
Iteration 161/1000 | Loss: 0.00002002
Iteration 162/1000 | Loss: 0.00002002
Iteration 163/1000 | Loss: 0.00002002
Iteration 164/1000 | Loss: 0.00002002
Iteration 165/1000 | Loss: 0.00002002
Iteration 166/1000 | Loss: 0.00002002
Iteration 167/1000 | Loss: 0.00002002
Iteration 168/1000 | Loss: 0.00002001
Iteration 169/1000 | Loss: 0.00002001
Iteration 170/1000 | Loss: 0.00002001
Iteration 171/1000 | Loss: 0.00002001
Iteration 172/1000 | Loss: 0.00002001
Iteration 173/1000 | Loss: 0.00002001
Iteration 174/1000 | Loss: 0.00002001
Iteration 175/1000 | Loss: 0.00002000
Iteration 176/1000 | Loss: 0.00002000
Iteration 177/1000 | Loss: 0.00002000
Iteration 178/1000 | Loss: 0.00002000
Iteration 179/1000 | Loss: 0.00002000
Iteration 180/1000 | Loss: 0.00002000
Iteration 181/1000 | Loss: 0.00001999
Iteration 182/1000 | Loss: 0.00001999
Iteration 183/1000 | Loss: 0.00001999
Iteration 184/1000 | Loss: 0.00001999
Iteration 185/1000 | Loss: 0.00001999
Iteration 186/1000 | Loss: 0.00001999
Iteration 187/1000 | Loss: 0.00001999
Iteration 188/1000 | Loss: 0.00001999
Iteration 189/1000 | Loss: 0.00001999
Iteration 190/1000 | Loss: 0.00001999
Iteration 191/1000 | Loss: 0.00001999
Iteration 192/1000 | Loss: 0.00001999
Iteration 193/1000 | Loss: 0.00001999
Iteration 194/1000 | Loss: 0.00001999
Iteration 195/1000 | Loss: 0.00001998
Iteration 196/1000 | Loss: 0.00001998
Iteration 197/1000 | Loss: 0.00001998
Iteration 198/1000 | Loss: 0.00001997
Iteration 199/1000 | Loss: 0.00001997
Iteration 200/1000 | Loss: 0.00001997
Iteration 201/1000 | Loss: 0.00001997
Iteration 202/1000 | Loss: 0.00001997
Iteration 203/1000 | Loss: 0.00001997
Iteration 204/1000 | Loss: 0.00001997
Iteration 205/1000 | Loss: 0.00001997
Iteration 206/1000 | Loss: 0.00001997
Iteration 207/1000 | Loss: 0.00001996
Iteration 208/1000 | Loss: 0.00001996
Iteration 209/1000 | Loss: 0.00001996
Iteration 210/1000 | Loss: 0.00001996
Iteration 211/1000 | Loss: 0.00001996
Iteration 212/1000 | Loss: 0.00001996
Iteration 213/1000 | Loss: 0.00001996
Iteration 214/1000 | Loss: 0.00001996
Iteration 215/1000 | Loss: 0.00001996
Iteration 216/1000 | Loss: 0.00001996
Iteration 217/1000 | Loss: 0.00001996
Iteration 218/1000 | Loss: 0.00001996
Iteration 219/1000 | Loss: 0.00001996
Iteration 220/1000 | Loss: 0.00001995
Iteration 221/1000 | Loss: 0.00001995
Iteration 222/1000 | Loss: 0.00001995
Iteration 223/1000 | Loss: 0.00001995
Iteration 224/1000 | Loss: 0.00001995
Iteration 225/1000 | Loss: 0.00001995
Iteration 226/1000 | Loss: 0.00001995
Iteration 227/1000 | Loss: 0.00001995
Iteration 228/1000 | Loss: 0.00001995
Iteration 229/1000 | Loss: 0.00001994
Iteration 230/1000 | Loss: 0.00001994
Iteration 231/1000 | Loss: 0.00001994
Iteration 232/1000 | Loss: 0.00001994
Iteration 233/1000 | Loss: 0.00001994
Iteration 234/1000 | Loss: 0.00001994
Iteration 235/1000 | Loss: 0.00001994
Iteration 236/1000 | Loss: 0.00001994
Iteration 237/1000 | Loss: 0.00001994
Iteration 238/1000 | Loss: 0.00001994
Iteration 239/1000 | Loss: 0.00003737
Iteration 240/1000 | Loss: 0.00003737
Iteration 241/1000 | Loss: 0.00040530
Iteration 242/1000 | Loss: 0.00002627
Iteration 243/1000 | Loss: 0.00002149
Iteration 244/1000 | Loss: 0.00002030
Iteration 245/1000 | Loss: 0.00030889
Iteration 246/1000 | Loss: 0.00003009
Iteration 247/1000 | Loss: 0.00002296
Iteration 248/1000 | Loss: 0.00002069
Iteration 249/1000 | Loss: 0.00002012
Iteration 250/1000 | Loss: 0.00002004
Iteration 251/1000 | Loss: 0.00002003
Iteration 252/1000 | Loss: 0.00002001
Iteration 253/1000 | Loss: 0.00002000
Iteration 254/1000 | Loss: 0.00001998
Iteration 255/1000 | Loss: 0.00001997
Iteration 256/1000 | Loss: 0.00001997
Iteration 257/1000 | Loss: 0.00001997
Iteration 258/1000 | Loss: 0.00001997
Iteration 259/1000 | Loss: 0.00001997
Iteration 260/1000 | Loss: 0.00001997
Iteration 261/1000 | Loss: 0.00001997
Iteration 262/1000 | Loss: 0.00001997
Iteration 263/1000 | Loss: 0.00001997
Iteration 264/1000 | Loss: 0.00001996
Iteration 265/1000 | Loss: 0.00001996
Iteration 266/1000 | Loss: 0.00001996
Iteration 267/1000 | Loss: 0.00001996
Iteration 268/1000 | Loss: 0.00003703
Iteration 269/1000 | Loss: 0.00003703
Iteration 270/1000 | Loss: 0.00002332
Iteration 271/1000 | Loss: 0.00001997
Iteration 272/1000 | Loss: 0.00001997
Iteration 273/1000 | Loss: 0.00001996
Iteration 274/1000 | Loss: 0.00001995
Iteration 275/1000 | Loss: 0.00001995
Iteration 276/1000 | Loss: 0.00001995
Iteration 277/1000 | Loss: 0.00001995
Iteration 278/1000 | Loss: 0.00003693
Iteration 279/1000 | Loss: 0.00002241
Iteration 280/1000 | Loss: 0.00002030
Iteration 281/1000 | Loss: 0.00001999
Iteration 282/1000 | Loss: 0.00001995
Iteration 283/1000 | Loss: 0.00001994
Iteration 284/1000 | Loss: 0.00001994
Iteration 285/1000 | Loss: 0.00001994
Iteration 286/1000 | Loss: 0.00001994
Iteration 287/1000 | Loss: 0.00001993
Iteration 288/1000 | Loss: 0.00003689
Iteration 289/1000 | Loss: 0.00002331
Iteration 290/1000 | Loss: 0.00002053
Iteration 291/1000 | Loss: 0.00002004
Iteration 292/1000 | Loss: 0.00001997
Iteration 293/1000 | Loss: 0.00001997
Iteration 294/1000 | Loss: 0.00001996
Iteration 295/1000 | Loss: 0.00001996
Iteration 296/1000 | Loss: 0.00001996
Iteration 297/1000 | Loss: 0.00001996
Iteration 298/1000 | Loss: 0.00001996
Iteration 299/1000 | Loss: 0.00001996
Iteration 300/1000 | Loss: 0.00001996
Iteration 301/1000 | Loss: 0.00001996
Iteration 302/1000 | Loss: 0.00001996
Iteration 303/1000 | Loss: 0.00001996
Iteration 304/1000 | Loss: 0.00001995
Iteration 305/1000 | Loss: 0.00001995
Iteration 306/1000 | Loss: 0.00001995
Iteration 307/1000 | Loss: 0.00001995
Iteration 308/1000 | Loss: 0.00001994
Iteration 309/1000 | Loss: 0.00001994
Iteration 310/1000 | Loss: 0.00001993
Iteration 311/1000 | Loss: 0.00001993
Iteration 312/1000 | Loss: 0.00001993
Iteration 313/1000 | Loss: 0.00001993
Iteration 314/1000 | Loss: 0.00003695
Iteration 315/1000 | Loss: 0.00002261
Iteration 316/1000 | Loss: 0.00002024
Iteration 317/1000 | Loss: 0.00001995
Iteration 318/1000 | Loss: 0.00001994
Iteration 319/1000 | Loss: 0.00001994
Iteration 320/1000 | Loss: 0.00001994
Iteration 321/1000 | Loss: 0.00001994
Iteration 322/1000 | Loss: 0.00001994
Iteration 323/1000 | Loss: 0.00001994
Iteration 324/1000 | Loss: 0.00001994
Iteration 325/1000 | Loss: 0.00001993
Iteration 326/1000 | Loss: 0.00001993
Iteration 327/1000 | Loss: 0.00001993
Iteration 328/1000 | Loss: 0.00001993
Iteration 329/1000 | Loss: 0.00001993
Iteration 330/1000 | Loss: 0.00001993
Iteration 331/1000 | Loss: 0.00001993
Iteration 332/1000 | Loss: 0.00001993
Iteration 333/1000 | Loss: 0.00001993
Iteration 334/1000 | Loss: 0.00001993
Iteration 335/1000 | Loss: 0.00001993
Iteration 336/1000 | Loss: 0.00001992
Iteration 337/1000 | Loss: 0.00001992
Iteration 338/1000 | Loss: 0.00003681
Iteration 339/1000 | Loss: 0.00002282
Iteration 340/1000 | Loss: 0.00002069
Iteration 341/1000 | Loss: 0.00002014
Iteration 342/1000 | Loss: 0.00001998
Iteration 343/1000 | Loss: 0.00001996
Iteration 344/1000 | Loss: 0.00001995
Iteration 345/1000 | Loss: 0.00001995
Iteration 346/1000 | Loss: 0.00001995
Iteration 347/1000 | Loss: 0.00001995
Iteration 348/1000 | Loss: 0.00001995
Iteration 349/1000 | Loss: 0.00001995
Iteration 350/1000 | Loss: 0.00001994
Iteration 351/1000 | Loss: 0.00001994
Iteration 352/1000 | Loss: 0.00001994
Iteration 353/1000 | Loss: 0.00001994
Iteration 354/1000 | Loss: 0.00001994
Iteration 355/1000 | Loss: 0.00001994
Iteration 356/1000 | Loss: 0.00001994
Iteration 357/1000 | Loss: 0.00001994
Iteration 358/1000 | Loss: 0.00001994
Iteration 359/1000 | Loss: 0.00001993
Iteration 360/1000 | Loss: 0.00001993
Iteration 361/1000 | Loss: 0.00001993
Iteration 362/1000 | Loss: 0.00001993
Iteration 363/1000 | Loss: 0.00001993
Iteration 364/1000 | Loss: 0.00001993
Iteration 365/1000 | Loss: 0.00001993
Iteration 366/1000 | Loss: 0.00001993
Iteration 367/1000 | Loss: 0.00001993
Iteration 368/1000 | Loss: 0.00001993
Iteration 369/1000 | Loss: 0.00001993
Iteration 370/1000 | Loss: 0.00001993
Iteration 371/1000 | Loss: 0.00003644
Iteration 372/1000 | Loss: 0.00002275
Iteration 373/1000 | Loss: 0.00001993
Iteration 374/1000 | Loss: 0.00001993
Iteration 375/1000 | Loss: 0.00001993
Iteration 376/1000 | Loss: 0.00001992
Iteration 377/1000 | Loss: 0.00001992
Iteration 378/1000 | Loss: 0.00001992
Iteration 379/1000 | Loss: 0.00001992
Iteration 380/1000 | Loss: 0.00001992
Iteration 381/1000 | Loss: 0.00001992
Iteration 382/1000 | Loss: 0.00001992
Iteration 383/1000 | Loss: 0.00001992
Iteration 384/1000 | Loss: 0.00001991
Iteration 385/1000 | Loss: 0.00001991
Iteration 386/1000 | Loss: 0.00001991
Iteration 387/1000 | Loss: 0.00001991
Iteration 388/1000 | Loss: 0.00001991
Iteration 389/1000 | Loss: 0.00001991
Iteration 390/1000 | Loss: 0.00003643
Iteration 391/1000 | Loss: 0.00002182
Iteration 392/1000 | Loss: 0.00002072
Iteration 393/1000 | Loss: 0.00002006
Iteration 394/1000 | Loss: 0.00002003
Iteration 395/1000 | Loss: 0.00002002
Iteration 396/1000 | Loss: 0.00002002
Iteration 397/1000 | Loss: 0.00001998
Iteration 398/1000 | Loss: 0.00001997
Iteration 399/1000 | Loss: 0.00001997
Iteration 400/1000 | Loss: 0.00001996
Iteration 401/1000 | Loss: 0.00001995
Iteration 402/1000 | Loss: 0.00001995
Iteration 403/1000 | Loss: 0.00001994
Iteration 404/1000 | Loss: 0.00001994
Iteration 405/1000 | Loss: 0.00003598
Iteration 406/1000 | Loss: 0.00048478
Iteration 407/1000 | Loss: 0.00005345
Iteration 408/1000 | Loss: 0.00002018
Iteration 409/1000 | Loss: 0.00003681
Iteration 410/1000 | Loss: 0.00002279
Iteration 411/1000 | Loss: 0.00002002
Iteration 412/1000 | Loss: 0.00001996
Iteration 413/1000 | Loss: 0.00001996
Iteration 414/1000 | Loss: 0.00001995
Iteration 415/1000 | Loss: 0.00001995
Iteration 416/1000 | Loss: 0.00001995
Iteration 417/1000 | Loss: 0.00001995
Iteration 418/1000 | Loss: 0.00001995
Iteration 419/1000 | Loss: 0.00001995
Iteration 420/1000 | Loss: 0.00001995
Iteration 421/1000 | Loss: 0.00001995
Iteration 422/1000 | Loss: 0.00001995
Iteration 423/1000 | Loss: 0.00001995
Iteration 424/1000 | Loss: 0.00001995
Iteration 425/1000 | Loss: 0.00001995
Iteration 426/1000 | Loss: 0.00001994
Iteration 427/1000 | Loss: 0.00001994
Iteration 428/1000 | Loss: 0.00001994
Iteration 429/1000 | Loss: 0.00001994
Iteration 430/1000 | Loss: 0.00001994
Iteration 431/1000 | Loss: 0.00001994
Iteration 432/1000 | Loss: 0.00001993
Iteration 433/1000 | Loss: 0.00001993
Iteration 434/1000 | Loss: 0.00001993
Iteration 435/1000 | Loss: 0.00001993
Iteration 436/1000 | Loss: 0.00001993
Iteration 437/1000 | Loss: 0.00001993
Iteration 438/1000 | Loss: 0.00001993
Iteration 439/1000 | Loss: 0.00001993
Iteration 440/1000 | Loss: 0.00001993
Iteration 441/1000 | Loss: 0.00001993
Iteration 442/1000 | Loss: 0.00001993
Iteration 443/1000 | Loss: 0.00001993
Iteration 444/1000 | Loss: 0.00001993
Iteration 445/1000 | Loss: 0.00001993
Iteration 446/1000 | Loss: 0.00003659
Iteration 447/1000 | Loss: 0.00002357
Iteration 448/1000 | Loss: 0.00001996
Iteration 449/1000 | Loss: 0.00003674
Iteration 450/1000 | Loss: 0.00002392
Iteration 451/1000 | Loss: 0.00001995
Iteration 452/1000 | Loss: 0.00003658
Iteration 453/1000 | Loss: 0.00002319
Iteration 454/1000 | Loss: 0.00002002
Iteration 455/1000 | Loss: 0.00001993
Iteration 456/1000 | Loss: 0.00003684
Iteration 457/1000 | Loss: 0.00002257
Iteration 458/1000 | Loss: 0.00001991
Iteration 459/1000 | Loss: 0.00001991
Iteration 460/1000 | Loss: 0.00001991
Iteration 461/1000 | Loss: 0.00001991
Iteration 462/1000 | Loss: 0.00001991
Iteration 463/1000 | Loss: 0.00001991
Iteration 464/1000 | Loss: 0.00001991
Iteration 465/1000 | Loss: 0.00001991
Iteration 466/1000 | Loss: 0.00001990
Iteration 467/1000 | Loss: 0.00001990
Iteration 468/1000 | Loss: 0.00001990
Iteration 469/1000 | Loss: 0.00001990
Iteration 470/1000 | Loss: 0.00001990
Iteration 471/1000 | Loss: 0.00001990
Iteration 472/1000 | Loss: 0.00001990
Iteration 473/1000 | Loss: 0.00001990
Iteration 474/1000 | Loss: 0.00001989
Iteration 475/1000 | Loss: 0.00001989
Iteration 476/1000 | Loss: 0.00001989
Iteration 477/1000 | Loss: 0.00001989
Iteration 478/1000 | Loss: 0.00003690
Iteration 479/1000 | Loss: 0.00002230
Iteration 480/1000 | Loss: 0.00003693
Iteration 481/1000 | Loss: 0.00002853
Iteration 482/1000 | Loss: 0.00002108
Iteration 483/1000 | Loss: 0.00002037
Iteration 484/1000 | Loss: 0.00001991
Iteration 485/1000 | Loss: 0.00001990
Iteration 486/1000 | Loss: 0.00001990
Iteration 487/1000 | Loss: 0.00001990
Iteration 488/1000 | Loss: 0.00001989
Iteration 489/1000 | Loss: 0.00001989
Iteration 490/1000 | Loss: 0.00001989
Iteration 491/1000 | Loss: 0.00001989
Iteration 492/1000 | Loss: 0.00001989
Iteration 493/1000 | Loss: 0.00001989
Iteration 494/1000 | Loss: 0.00003682
Iteration 495/1000 | Loss: 0.00002301
Iteration 496/1000 | Loss: 0.00001990
Iteration 497/1000 | Loss: 0.00001989
Iteration 498/1000 | Loss: 0.00001989
Iteration 499/1000 | Loss: 0.00001989
Iteration 500/1000 | Loss: 0.00001989
Iteration 501/1000 | Loss: 0.00001989
Iteration 502/1000 | Loss: 0.00001989
Iteration 503/1000 | Loss: 0.00001989
Iteration 504/1000 | Loss: 0.00001989
Iteration 505/1000 | Loss: 0.00001989
Iteration 506/1000 | Loss: 0.00001989
Iteration 507/1000 | Loss: 0.00001989
Iteration 508/1000 | Loss: 0.00001989
Iteration 509/1000 | Loss: 0.00001988
Iteration 510/1000 | Loss: 0.00001988
Iteration 511/1000 | Loss: 0.00001988
Iteration 512/1000 | Loss: 0.00001988
Iteration 513/1000 | Loss: 0.00001988
Iteration 514/1000 | Loss: 0.00001988
Iteration 515/1000 | Loss: 0.00001988
Iteration 516/1000 | Loss: 0.00001988
Iteration 517/1000 | Loss: 0.00001988
Iteration 518/1000 | Loss: 0.00001988
Iteration 519/1000 | Loss: 0.00001988
Iteration 520/1000 | Loss: 0.00001988
Iteration 521/1000 | Loss: 0.00001988
Iteration 522/1000 | Loss: 0.00001988
Iteration 523/1000 | Loss: 0.00001988
Iteration 524/1000 | Loss: 0.00001988
Iteration 525/1000 | Loss: 0.00001988
Iteration 526/1000 | Loss: 0.00001988
Iteration 527/1000 | Loss: 0.00003696
Iteration 528/1000 | Loss: 0.00002208
Iteration 529/1000 | Loss: 0.00001992
Iteration 530/1000 | Loss: 0.00001989
Iteration 531/1000 | Loss: 0.00001989
Iteration 532/1000 | Loss: 0.00001989
Iteration 533/1000 | Loss: 0.00001989
Iteration 534/1000 | Loss: 0.00001988
Iteration 535/1000 | Loss: 0.00001988
Iteration 536/1000 | Loss: 0.00001988
Iteration 537/1000 | Loss: 0.00001988
Iteration 538/1000 | Loss: 0.00001988
Iteration 539/1000 | Loss: 0.00003716
Iteration 540/1000 | Loss: 0.00002331
Iteration 541/1000 | Loss: 0.00002013
Iteration 542/1000 | Loss: 0.00001991
Iteration 543/1000 | Loss: 0.00001991
Iteration 544/1000 | Loss: 0.00001991
Iteration 545/1000 | Loss: 0.00001991
Iteration 546/1000 | Loss: 0.00001990
Iteration 547/1000 | Loss: 0.00001990
Iteration 548/1000 | Loss: 0.00001990
Iteration 549/1000 | Loss: 0.00001990
Iteration 550/1000 | Loss: 0.00001990
Iteration 551/1000 | Loss: 0.00001990
Iteration 552/1000 | Loss: 0.00001990
Iteration 553/1000 | Loss: 0.00001990
Iteration 554/1000 | Loss: 0.00001990
Iteration 555/1000 | Loss: 0.00001990
Iteration 556/1000 | Loss: 0.00001989
Iteration 557/1000 | Loss: 0.00001989
Iteration 558/1000 | Loss: 0.00003737
Iteration 559/1000 | Loss: 0.00002389
Iteration 560/1000 | Loss: 0.00002009
Iteration 561/1000 | Loss: 0.00001993
Iteration 562/1000 | Loss: 0.00001991
Iteration 563/1000 | Loss: 0.00001990
Iteration 564/1000 | Loss: 0.00001990
Iteration 565/1000 | Loss: 0.00001990
Iteration 566/1000 | Loss: 0.00001990
Iteration 567/1000 | Loss: 0.00001989
Iteration 568/1000 | Loss: 0.00001989
Iteration 569/1000 | Loss: 0.00001989
Iteration 570/1000 | Loss: 0.00001989
Iteration 571/1000 | Loss: 0.00001989
Iteration 572/1000 | Loss: 0.00001989
Iteration 573/1000 | Loss: 0.00001989
Iteration 574/1000 | Loss: 0.00001989
Iteration 575/1000 | Loss: 0.00001989
Iteration 576/1000 | Loss: 0.00001989
Iteration 577/1000 | Loss: 0.00001989
Iteration 578/1000 | Loss: 0.00001989
Iteration 579/1000 | Loss: 0.00001989
Iteration 580/1000 | Loss: 0.00001989
Iteration 581/1000 | Loss: 0.00001989
Iteration 582/1000 | Loss: 0.00001989
Iteration 583/1000 | Loss: 0.00001989
Iteration 584/1000 | Loss: 0.00001988
Iteration 585/1000 | Loss: 0.00001988
Iteration 586/1000 | Loss: 0.00001988
Iteration 587/1000 | Loss: 0.00001988
Iteration 588/1000 | Loss: 0.00001988
Iteration 589/1000 | Loss: 0.00001988
Iteration 590/1000 | Loss: 0.00001988
Iteration 591/1000 | Loss: 0.00001988
Iteration 592/1000 | Loss: 0.00001988
Iteration 593/1000 | Loss: 0.00001988
Iteration 594/1000 | Loss: 0.00001988
Iteration 595/1000 | Loss: 0.00001988
Iteration 596/1000 | Loss: 0.00001988
Iteration 597/1000 | Loss: 0.00001988
Iteration 598/1000 | Loss: 0.00001988
Iteration 599/1000 | Loss: 0.00001988
Iteration 600/1000 | Loss: 0.00001988
Iteration 601/1000 | Loss: 0.00001988
Iteration 602/1000 | Loss: 0.00001988
Iteration 603/1000 | Loss: 0.00001987
Iteration 604/1000 | Loss: 0.00001987
Iteration 605/1000 | Loss: 0.00001987
Iteration 606/1000 | Loss: 0.00001987
Iteration 607/1000 | Loss: 0.00001987
Iteration 608/1000 | Loss: 0.00001987
Iteration 609/1000 | Loss: 0.00003759
Iteration 610/1000 | Loss: 0.00002247
Iteration 611/1000 | Loss: 0.00001989
Iteration 612/1000 | Loss: 0.00001989
Iteration 613/1000 | Loss: 0.00001989
Iteration 614/1000 | Loss: 0.00001989
Iteration 615/1000 | Loss: 0.00001989
Iteration 616/1000 | Loss: 0.00001988
Iteration 617/1000 | Loss: 0.00001988
Iteration 618/1000 | Loss: 0.00001988
Iteration 619/1000 | Loss: 0.00001988
Iteration 620/1000 | Loss: 0.00001988
Iteration 621/1000 | Loss: 0.00001988
Iteration 622/1000 | Loss: 0.00001988
Iteration 623/1000 | Loss: 0.00001988
Iteration 624/1000 | Loss: 0.00001988
Iteration 625/1000 | Loss: 0.00001988
Iteration 626/1000 | Loss: 0.00001988
Iteration 627/1000 | Loss: 0.00001988
Iteration 628/1000 | Loss: 0.00001987
Iteration 629/1000 | Loss: 0.00001987
Iteration 630/1000 | Loss: 0.00001987
Iteration 631/1000 | Loss: 0.00001987
Iteration 632/1000 | Loss: 0.00001987
Iteration 633/1000 | Loss: 0.00001987
Iteration 634/1000 | Loss: 0.00001987
Iteration 635/1000 | Loss: 0.00001987
Iteration 636/1000 | Loss: 0.00001987
Iteration 637/1000 | Loss: 0.00001987
Iteration 638/1000 | Loss: 0.00001987
Iteration 639/1000 | Loss: 0.00001987
Iteration 640/1000 | Loss: 0.00001987
Iteration 641/1000 | Loss: 0.00001987
Iteration 642/1000 | Loss: 0.00001987
Iteration 643/1000 | Loss: 0.00001987
Iteration 644/1000 | Loss: 0.00001987
Iteration 645/1000 | Loss: 0.00001987
Iteration 646/1000 | Loss: 0.00001987
Iteration 647/1000 | Loss: 0.00001987
Iteration 648/1000 | Loss: 0.00001987
Iteration 649/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 649. Stopping optimization.
Last 5 losses: [1.986912320717238e-05, 1.986912320717238e-05, 1.986912320717238e-05, 1.986912320717238e-05, 1.986912320717238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.986912320717238e-05

Optimization complete. Final v2v error: 3.3886890411376953 mm

Highest mean error: 20.655672073364258 mm for frame 131

Lowest mean error: 2.758297920227051 mm for frame 69

Saving results

Total time: 239.83775925636292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070658
Iteration 2/25 | Loss: 0.00204255
Iteration 3/25 | Loss: 0.00142191
Iteration 4/25 | Loss: 0.00131018
Iteration 5/25 | Loss: 0.00134410
Iteration 6/25 | Loss: 0.00126564
Iteration 7/25 | Loss: 0.00118664
Iteration 8/25 | Loss: 0.00113284
Iteration 9/25 | Loss: 0.00112533
Iteration 10/25 | Loss: 0.00111623
Iteration 11/25 | Loss: 0.00111263
Iteration 12/25 | Loss: 0.00111302
Iteration 13/25 | Loss: 0.00111194
Iteration 14/25 | Loss: 0.00111190
Iteration 15/25 | Loss: 0.00111190
Iteration 16/25 | Loss: 0.00111189
Iteration 17/25 | Loss: 0.00111189
Iteration 18/25 | Loss: 0.00111189
Iteration 19/25 | Loss: 0.00111189
Iteration 20/25 | Loss: 0.00111189
Iteration 21/25 | Loss: 0.00111189
Iteration 22/25 | Loss: 0.00111189
Iteration 23/25 | Loss: 0.00111189
Iteration 24/25 | Loss: 0.00111188
Iteration 25/25 | Loss: 0.00111188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51220238
Iteration 2/25 | Loss: 0.00091306
Iteration 3/25 | Loss: 0.00091306
Iteration 4/25 | Loss: 0.00091306
Iteration 5/25 | Loss: 0.00091306
Iteration 6/25 | Loss: 0.00091306
Iteration 7/25 | Loss: 0.00091306
Iteration 8/25 | Loss: 0.00091306
Iteration 9/25 | Loss: 0.00091306
Iteration 10/25 | Loss: 0.00091306
Iteration 11/25 | Loss: 0.00091306
Iteration 12/25 | Loss: 0.00091306
Iteration 13/25 | Loss: 0.00091306
Iteration 14/25 | Loss: 0.00091306
Iteration 15/25 | Loss: 0.00091306
Iteration 16/25 | Loss: 0.00091306
Iteration 17/25 | Loss: 0.00091306
Iteration 18/25 | Loss: 0.00091306
Iteration 19/25 | Loss: 0.00091306
Iteration 20/25 | Loss: 0.00091306
Iteration 21/25 | Loss: 0.00091306
Iteration 22/25 | Loss: 0.00091306
Iteration 23/25 | Loss: 0.00091306
Iteration 24/25 | Loss: 0.00091306
Iteration 25/25 | Loss: 0.00091306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091306
Iteration 2/1000 | Loss: 0.00003415
Iteration 3/1000 | Loss: 0.00002317
Iteration 4/1000 | Loss: 0.00001989
Iteration 5/1000 | Loss: 0.00001863
Iteration 6/1000 | Loss: 0.00001778
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001694
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001647
Iteration 11/1000 | Loss: 0.00001627
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001597
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001581
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001576
Iteration 19/1000 | Loss: 0.00001567
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001565
Iteration 22/1000 | Loss: 0.00001565
Iteration 23/1000 | Loss: 0.00001565
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001563
Iteration 26/1000 | Loss: 0.00001563
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001559
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001559
Iteration 35/1000 | Loss: 0.00001558
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001555
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001554
Iteration 40/1000 | Loss: 0.00001554
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001551
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001550
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001550
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001549
Iteration 65/1000 | Loss: 0.00001549
Iteration 66/1000 | Loss: 0.00001548
Iteration 67/1000 | Loss: 0.00001548
Iteration 68/1000 | Loss: 0.00001548
Iteration 69/1000 | Loss: 0.00001548
Iteration 70/1000 | Loss: 0.00001548
Iteration 71/1000 | Loss: 0.00001548
Iteration 72/1000 | Loss: 0.00001548
Iteration 73/1000 | Loss: 0.00001548
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001547
Iteration 77/1000 | Loss: 0.00001547
Iteration 78/1000 | Loss: 0.00001547
Iteration 79/1000 | Loss: 0.00001547
Iteration 80/1000 | Loss: 0.00001547
Iteration 81/1000 | Loss: 0.00001547
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001545
Iteration 88/1000 | Loss: 0.00001545
Iteration 89/1000 | Loss: 0.00001545
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001544
Iteration 96/1000 | Loss: 0.00001544
Iteration 97/1000 | Loss: 0.00001544
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001543
Iteration 100/1000 | Loss: 0.00001543
Iteration 101/1000 | Loss: 0.00001543
Iteration 102/1000 | Loss: 0.00001543
Iteration 103/1000 | Loss: 0.00001543
Iteration 104/1000 | Loss: 0.00001543
Iteration 105/1000 | Loss: 0.00001542
Iteration 106/1000 | Loss: 0.00001542
Iteration 107/1000 | Loss: 0.00001542
Iteration 108/1000 | Loss: 0.00001542
Iteration 109/1000 | Loss: 0.00001542
Iteration 110/1000 | Loss: 0.00001542
Iteration 111/1000 | Loss: 0.00001542
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001541
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001540
Iteration 126/1000 | Loss: 0.00001540
Iteration 127/1000 | Loss: 0.00001540
Iteration 128/1000 | Loss: 0.00001540
Iteration 129/1000 | Loss: 0.00001540
Iteration 130/1000 | Loss: 0.00001540
Iteration 131/1000 | Loss: 0.00001540
Iteration 132/1000 | Loss: 0.00001540
Iteration 133/1000 | Loss: 0.00001540
Iteration 134/1000 | Loss: 0.00001540
Iteration 135/1000 | Loss: 0.00001540
Iteration 136/1000 | Loss: 0.00001540
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001540
Iteration 140/1000 | Loss: 0.00001540
Iteration 141/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.5398485629702918e-05, 1.5398485629702918e-05, 1.5398485629702918e-05, 1.5398485629702918e-05, 1.5398485629702918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5398485629702918e-05

Optimization complete. Final v2v error: 3.2698256969451904 mm

Highest mean error: 4.663737773895264 mm for frame 80

Lowest mean error: 2.7321524620056152 mm for frame 35

Saving results

Total time: 55.84310793876648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503771
Iteration 2/25 | Loss: 0.00121661
Iteration 3/25 | Loss: 0.00112830
Iteration 4/25 | Loss: 0.00111048
Iteration 5/25 | Loss: 0.00110588
Iteration 6/25 | Loss: 0.00110572
Iteration 7/25 | Loss: 0.00110572
Iteration 8/25 | Loss: 0.00110572
Iteration 9/25 | Loss: 0.00110572
Iteration 10/25 | Loss: 0.00110572
Iteration 11/25 | Loss: 0.00110572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011057187803089619, 0.0011057187803089619, 0.0011057187803089619, 0.0011057187803089619, 0.0011057187803089619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011057187803089619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77372360
Iteration 2/25 | Loss: 0.00075237
Iteration 3/25 | Loss: 0.00075236
Iteration 4/25 | Loss: 0.00075236
Iteration 5/25 | Loss: 0.00075236
Iteration 6/25 | Loss: 0.00075236
Iteration 7/25 | Loss: 0.00075236
Iteration 8/25 | Loss: 0.00075235
Iteration 9/25 | Loss: 0.00075235
Iteration 10/25 | Loss: 0.00075235
Iteration 11/25 | Loss: 0.00075235
Iteration 12/25 | Loss: 0.00075235
Iteration 13/25 | Loss: 0.00075235
Iteration 14/25 | Loss: 0.00075235
Iteration 15/25 | Loss: 0.00075235
Iteration 16/25 | Loss: 0.00075235
Iteration 17/25 | Loss: 0.00075235
Iteration 18/25 | Loss: 0.00075235
Iteration 19/25 | Loss: 0.00075235
Iteration 20/25 | Loss: 0.00075235
Iteration 21/25 | Loss: 0.00075235
Iteration 22/25 | Loss: 0.00075235
Iteration 23/25 | Loss: 0.00075235
Iteration 24/25 | Loss: 0.00075235
Iteration 25/25 | Loss: 0.00075235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075235
Iteration 2/1000 | Loss: 0.00002553
Iteration 3/1000 | Loss: 0.00001965
Iteration 4/1000 | Loss: 0.00001834
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001681
Iteration 7/1000 | Loss: 0.00001646
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001557
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001539
Iteration 14/1000 | Loss: 0.00001522
Iteration 15/1000 | Loss: 0.00001522
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001517
Iteration 18/1000 | Loss: 0.00001516
Iteration 19/1000 | Loss: 0.00001516
Iteration 20/1000 | Loss: 0.00001514
Iteration 21/1000 | Loss: 0.00001514
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001506
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001505
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001503
Iteration 32/1000 | Loss: 0.00001503
Iteration 33/1000 | Loss: 0.00001502
Iteration 34/1000 | Loss: 0.00001502
Iteration 35/1000 | Loss: 0.00001501
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001492
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001492
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001489
Iteration 62/1000 | Loss: 0.00001489
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001484
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001483
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001479
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001478
Iteration 97/1000 | Loss: 0.00001478
Iteration 98/1000 | Loss: 0.00001478
Iteration 99/1000 | Loss: 0.00001478
Iteration 100/1000 | Loss: 0.00001478
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001477
Iteration 105/1000 | Loss: 0.00001477
Iteration 106/1000 | Loss: 0.00001477
Iteration 107/1000 | Loss: 0.00001477
Iteration 108/1000 | Loss: 0.00001477
Iteration 109/1000 | Loss: 0.00001477
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001477
Iteration 112/1000 | Loss: 0.00001477
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001475
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001475
Iteration 125/1000 | Loss: 0.00001475
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001474
Iteration 128/1000 | Loss: 0.00001474
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001474
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001473
Iteration 134/1000 | Loss: 0.00001473
Iteration 135/1000 | Loss: 0.00001473
Iteration 136/1000 | Loss: 0.00001473
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00001473
Iteration 153/1000 | Loss: 0.00001473
Iteration 154/1000 | Loss: 0.00001473
Iteration 155/1000 | Loss: 0.00001473
Iteration 156/1000 | Loss: 0.00001473
Iteration 157/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.4726232620887458e-05, 1.4726232620887458e-05, 1.4726232620887458e-05, 1.4726232620887458e-05, 1.4726232620887458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4726232620887458e-05

Optimization complete. Final v2v error: 3.2561700344085693 mm

Highest mean error: 3.6078221797943115 mm for frame 95

Lowest mean error: 2.9245758056640625 mm for frame 216

Saving results

Total time: 42.05913829803467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479528
Iteration 2/25 | Loss: 0.00120683
Iteration 3/25 | Loss: 0.00111529
Iteration 4/25 | Loss: 0.00110488
Iteration 5/25 | Loss: 0.00110227
Iteration 6/25 | Loss: 0.00110227
Iteration 7/25 | Loss: 0.00110227
Iteration 8/25 | Loss: 0.00110227
Iteration 9/25 | Loss: 0.00110227
Iteration 10/25 | Loss: 0.00110227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011022725375369191, 0.0011022725375369191, 0.0011022725375369191, 0.0011022725375369191, 0.0011022725375369191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011022725375369191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.40882301
Iteration 2/25 | Loss: 0.00075077
Iteration 3/25 | Loss: 0.00075077
Iteration 4/25 | Loss: 0.00075077
Iteration 5/25 | Loss: 0.00075077
Iteration 6/25 | Loss: 0.00075077
Iteration 7/25 | Loss: 0.00075076
Iteration 8/25 | Loss: 0.00075076
Iteration 9/25 | Loss: 0.00075076
Iteration 10/25 | Loss: 0.00075076
Iteration 11/25 | Loss: 0.00075076
Iteration 12/25 | Loss: 0.00075076
Iteration 13/25 | Loss: 0.00075076
Iteration 14/25 | Loss: 0.00075076
Iteration 15/25 | Loss: 0.00075076
Iteration 16/25 | Loss: 0.00075076
Iteration 17/25 | Loss: 0.00075076
Iteration 18/25 | Loss: 0.00075076
Iteration 19/25 | Loss: 0.00075076
Iteration 20/25 | Loss: 0.00075076
Iteration 21/25 | Loss: 0.00075076
Iteration 22/25 | Loss: 0.00075076
Iteration 23/25 | Loss: 0.00075076
Iteration 24/25 | Loss: 0.00075076
Iteration 25/25 | Loss: 0.00075076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075076
Iteration 2/1000 | Loss: 0.00001969
Iteration 3/1000 | Loss: 0.00001627
Iteration 4/1000 | Loss: 0.00001479
Iteration 5/1000 | Loss: 0.00001408
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001323
Iteration 8/1000 | Loss: 0.00001294
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001247
Iteration 12/1000 | Loss: 0.00001240
Iteration 13/1000 | Loss: 0.00001234
Iteration 14/1000 | Loss: 0.00001234
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001229
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001210
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001206
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001197
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001192
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001191
Iteration 61/1000 | Loss: 0.00001191
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001190
Iteration 67/1000 | Loss: 0.00001190
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001183
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001180
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001177
Iteration 90/1000 | Loss: 0.00001177
Iteration 91/1000 | Loss: 0.00001177
Iteration 92/1000 | Loss: 0.00001177
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001172
Iteration 117/1000 | Loss: 0.00001172
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001171
Iteration 126/1000 | Loss: 0.00001171
Iteration 127/1000 | Loss: 0.00001171
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00001170
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001169
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001168
Iteration 143/1000 | Loss: 0.00001168
Iteration 144/1000 | Loss: 0.00001168
Iteration 145/1000 | Loss: 0.00001168
Iteration 146/1000 | Loss: 0.00001168
Iteration 147/1000 | Loss: 0.00001167
Iteration 148/1000 | Loss: 0.00001167
Iteration 149/1000 | Loss: 0.00001167
Iteration 150/1000 | Loss: 0.00001167
Iteration 151/1000 | Loss: 0.00001167
Iteration 152/1000 | Loss: 0.00001167
Iteration 153/1000 | Loss: 0.00001167
Iteration 154/1000 | Loss: 0.00001167
Iteration 155/1000 | Loss: 0.00001167
Iteration 156/1000 | Loss: 0.00001167
Iteration 157/1000 | Loss: 0.00001167
Iteration 158/1000 | Loss: 0.00001167
Iteration 159/1000 | Loss: 0.00001167
Iteration 160/1000 | Loss: 0.00001167
Iteration 161/1000 | Loss: 0.00001167
Iteration 162/1000 | Loss: 0.00001167
Iteration 163/1000 | Loss: 0.00001167
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001166
Iteration 166/1000 | Loss: 0.00001166
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001166
Iteration 170/1000 | Loss: 0.00001166
Iteration 171/1000 | Loss: 0.00001166
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001166
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1656488823064137e-05, 1.1656488823064137e-05, 1.1656488823064137e-05, 1.1656488823064137e-05, 1.1656488823064137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1656488823064137e-05

Optimization complete. Final v2v error: 2.922471761703491 mm

Highest mean error: 3.1785120964050293 mm for frame 220

Lowest mean error: 2.7407751083374023 mm for frame 116

Saving results

Total time: 43.746479749679565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821623
Iteration 2/25 | Loss: 0.00147683
Iteration 3/25 | Loss: 0.00126193
Iteration 4/25 | Loss: 0.00124303
Iteration 5/25 | Loss: 0.00123820
Iteration 6/25 | Loss: 0.00123734
Iteration 7/25 | Loss: 0.00123734
Iteration 8/25 | Loss: 0.00123734
Iteration 9/25 | Loss: 0.00123734
Iteration 10/25 | Loss: 0.00123734
Iteration 11/25 | Loss: 0.00123734
Iteration 12/25 | Loss: 0.00123734
Iteration 13/25 | Loss: 0.00123734
Iteration 14/25 | Loss: 0.00123734
Iteration 15/25 | Loss: 0.00123734
Iteration 16/25 | Loss: 0.00123734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012373391073197126, 0.0012373391073197126, 0.0012373391073197126, 0.0012373391073197126, 0.0012373391073197126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012373391073197126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12522304
Iteration 2/25 | Loss: 0.00092402
Iteration 3/25 | Loss: 0.00092399
Iteration 4/25 | Loss: 0.00092399
Iteration 5/25 | Loss: 0.00092399
Iteration 6/25 | Loss: 0.00092399
Iteration 7/25 | Loss: 0.00092399
Iteration 8/25 | Loss: 0.00092399
Iteration 9/25 | Loss: 0.00092399
Iteration 10/25 | Loss: 0.00092399
Iteration 11/25 | Loss: 0.00092399
Iteration 12/25 | Loss: 0.00092399
Iteration 13/25 | Loss: 0.00092399
Iteration 14/25 | Loss: 0.00092399
Iteration 15/25 | Loss: 0.00092399
Iteration 16/25 | Loss: 0.00092399
Iteration 17/25 | Loss: 0.00092399
Iteration 18/25 | Loss: 0.00092399
Iteration 19/25 | Loss: 0.00092399
Iteration 20/25 | Loss: 0.00092399
Iteration 21/25 | Loss: 0.00092399
Iteration 22/25 | Loss: 0.00092399
Iteration 23/25 | Loss: 0.00092399
Iteration 24/25 | Loss: 0.00092399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009239879436790943, 0.0009239879436790943, 0.0009239879436790943, 0.0009239879436790943, 0.0009239879436790943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009239879436790943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092399
Iteration 2/1000 | Loss: 0.00004211
Iteration 3/1000 | Loss: 0.00002918
Iteration 4/1000 | Loss: 0.00002552
Iteration 5/1000 | Loss: 0.00002441
Iteration 6/1000 | Loss: 0.00002336
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002232
Iteration 9/1000 | Loss: 0.00002203
Iteration 10/1000 | Loss: 0.00002184
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002156
Iteration 13/1000 | Loss: 0.00002144
Iteration 14/1000 | Loss: 0.00002136
Iteration 15/1000 | Loss: 0.00002130
Iteration 16/1000 | Loss: 0.00002130
Iteration 17/1000 | Loss: 0.00002123
Iteration 18/1000 | Loss: 0.00002121
Iteration 19/1000 | Loss: 0.00002121
Iteration 20/1000 | Loss: 0.00002120
Iteration 21/1000 | Loss: 0.00002119
Iteration 22/1000 | Loss: 0.00002119
Iteration 23/1000 | Loss: 0.00002117
Iteration 24/1000 | Loss: 0.00002117
Iteration 25/1000 | Loss: 0.00002116
Iteration 26/1000 | Loss: 0.00002116
Iteration 27/1000 | Loss: 0.00002116
Iteration 28/1000 | Loss: 0.00002115
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002114
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002113
Iteration 35/1000 | Loss: 0.00002113
Iteration 36/1000 | Loss: 0.00002113
Iteration 37/1000 | Loss: 0.00002113
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002112
Iteration 40/1000 | Loss: 0.00002112
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002112
Iteration 43/1000 | Loss: 0.00002112
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00002111
Iteration 46/1000 | Loss: 0.00002111
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002109
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002109
Iteration 54/1000 | Loss: 0.00002109
Iteration 55/1000 | Loss: 0.00002109
Iteration 56/1000 | Loss: 0.00002109
Iteration 57/1000 | Loss: 0.00002109
Iteration 58/1000 | Loss: 0.00002109
Iteration 59/1000 | Loss: 0.00002109
Iteration 60/1000 | Loss: 0.00002109
Iteration 61/1000 | Loss: 0.00002108
Iteration 62/1000 | Loss: 0.00002108
Iteration 63/1000 | Loss: 0.00002108
Iteration 64/1000 | Loss: 0.00002108
Iteration 65/1000 | Loss: 0.00002108
Iteration 66/1000 | Loss: 0.00002107
Iteration 67/1000 | Loss: 0.00002107
Iteration 68/1000 | Loss: 0.00002107
Iteration 69/1000 | Loss: 0.00002106
Iteration 70/1000 | Loss: 0.00002106
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002105
Iteration 73/1000 | Loss: 0.00002105
Iteration 74/1000 | Loss: 0.00002105
Iteration 75/1000 | Loss: 0.00002104
Iteration 76/1000 | Loss: 0.00002104
Iteration 77/1000 | Loss: 0.00002104
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002103
Iteration 80/1000 | Loss: 0.00002103
Iteration 81/1000 | Loss: 0.00002103
Iteration 82/1000 | Loss: 0.00002102
Iteration 83/1000 | Loss: 0.00002102
Iteration 84/1000 | Loss: 0.00002102
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002101
Iteration 88/1000 | Loss: 0.00002101
Iteration 89/1000 | Loss: 0.00002101
Iteration 90/1000 | Loss: 0.00002101
Iteration 91/1000 | Loss: 0.00002101
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002101
Iteration 96/1000 | Loss: 0.00002100
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002100
Iteration 101/1000 | Loss: 0.00002100
Iteration 102/1000 | Loss: 0.00002100
Iteration 103/1000 | Loss: 0.00002100
Iteration 104/1000 | Loss: 0.00002099
Iteration 105/1000 | Loss: 0.00002099
Iteration 106/1000 | Loss: 0.00002099
Iteration 107/1000 | Loss: 0.00002099
Iteration 108/1000 | Loss: 0.00002098
Iteration 109/1000 | Loss: 0.00002098
Iteration 110/1000 | Loss: 0.00002098
Iteration 111/1000 | Loss: 0.00002098
Iteration 112/1000 | Loss: 0.00002097
Iteration 113/1000 | Loss: 0.00002097
Iteration 114/1000 | Loss: 0.00002097
Iteration 115/1000 | Loss: 0.00002097
Iteration 116/1000 | Loss: 0.00002097
Iteration 117/1000 | Loss: 0.00002096
Iteration 118/1000 | Loss: 0.00002096
Iteration 119/1000 | Loss: 0.00002095
Iteration 120/1000 | Loss: 0.00002095
Iteration 121/1000 | Loss: 0.00002095
Iteration 122/1000 | Loss: 0.00002095
Iteration 123/1000 | Loss: 0.00002095
Iteration 124/1000 | Loss: 0.00002094
Iteration 125/1000 | Loss: 0.00002094
Iteration 126/1000 | Loss: 0.00002094
Iteration 127/1000 | Loss: 0.00002094
Iteration 128/1000 | Loss: 0.00002094
Iteration 129/1000 | Loss: 0.00002094
Iteration 130/1000 | Loss: 0.00002094
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002093
Iteration 133/1000 | Loss: 0.00002093
Iteration 134/1000 | Loss: 0.00002093
Iteration 135/1000 | Loss: 0.00002093
Iteration 136/1000 | Loss: 0.00002093
Iteration 137/1000 | Loss: 0.00002093
Iteration 138/1000 | Loss: 0.00002093
Iteration 139/1000 | Loss: 0.00002093
Iteration 140/1000 | Loss: 0.00002092
Iteration 141/1000 | Loss: 0.00002092
Iteration 142/1000 | Loss: 0.00002092
Iteration 143/1000 | Loss: 0.00002092
Iteration 144/1000 | Loss: 0.00002092
Iteration 145/1000 | Loss: 0.00002091
Iteration 146/1000 | Loss: 0.00002091
Iteration 147/1000 | Loss: 0.00002091
Iteration 148/1000 | Loss: 0.00002091
Iteration 149/1000 | Loss: 0.00002091
Iteration 150/1000 | Loss: 0.00002091
Iteration 151/1000 | Loss: 0.00002091
Iteration 152/1000 | Loss: 0.00002091
Iteration 153/1000 | Loss: 0.00002091
Iteration 154/1000 | Loss: 0.00002090
Iteration 155/1000 | Loss: 0.00002090
Iteration 156/1000 | Loss: 0.00002090
Iteration 157/1000 | Loss: 0.00002090
Iteration 158/1000 | Loss: 0.00002090
Iteration 159/1000 | Loss: 0.00002090
Iteration 160/1000 | Loss: 0.00002090
Iteration 161/1000 | Loss: 0.00002090
Iteration 162/1000 | Loss: 0.00002089
Iteration 163/1000 | Loss: 0.00002089
Iteration 164/1000 | Loss: 0.00002089
Iteration 165/1000 | Loss: 0.00002088
Iteration 166/1000 | Loss: 0.00002088
Iteration 167/1000 | Loss: 0.00002088
Iteration 168/1000 | Loss: 0.00002088
Iteration 169/1000 | Loss: 0.00002087
Iteration 170/1000 | Loss: 0.00002087
Iteration 171/1000 | Loss: 0.00002087
Iteration 172/1000 | Loss: 0.00002087
Iteration 173/1000 | Loss: 0.00002087
Iteration 174/1000 | Loss: 0.00002087
Iteration 175/1000 | Loss: 0.00002087
Iteration 176/1000 | Loss: 0.00002087
Iteration 177/1000 | Loss: 0.00002087
Iteration 178/1000 | Loss: 0.00002087
Iteration 179/1000 | Loss: 0.00002087
Iteration 180/1000 | Loss: 0.00002087
Iteration 181/1000 | Loss: 0.00002087
Iteration 182/1000 | Loss: 0.00002087
Iteration 183/1000 | Loss: 0.00002087
Iteration 184/1000 | Loss: 0.00002086
Iteration 185/1000 | Loss: 0.00002086
Iteration 186/1000 | Loss: 0.00002086
Iteration 187/1000 | Loss: 0.00002086
Iteration 188/1000 | Loss: 0.00002086
Iteration 189/1000 | Loss: 0.00002085
Iteration 190/1000 | Loss: 0.00002085
Iteration 191/1000 | Loss: 0.00002085
Iteration 192/1000 | Loss: 0.00002085
Iteration 193/1000 | Loss: 0.00002085
Iteration 194/1000 | Loss: 0.00002085
Iteration 195/1000 | Loss: 0.00002085
Iteration 196/1000 | Loss: 0.00002085
Iteration 197/1000 | Loss: 0.00002085
Iteration 198/1000 | Loss: 0.00002085
Iteration 199/1000 | Loss: 0.00002084
Iteration 200/1000 | Loss: 0.00002084
Iteration 201/1000 | Loss: 0.00002084
Iteration 202/1000 | Loss: 0.00002084
Iteration 203/1000 | Loss: 0.00002084
Iteration 204/1000 | Loss: 0.00002084
Iteration 205/1000 | Loss: 0.00002084
Iteration 206/1000 | Loss: 0.00002084
Iteration 207/1000 | Loss: 0.00002084
Iteration 208/1000 | Loss: 0.00002083
Iteration 209/1000 | Loss: 0.00002083
Iteration 210/1000 | Loss: 0.00002083
Iteration 211/1000 | Loss: 0.00002083
Iteration 212/1000 | Loss: 0.00002083
Iteration 213/1000 | Loss: 0.00002082
Iteration 214/1000 | Loss: 0.00002082
Iteration 215/1000 | Loss: 0.00002082
Iteration 216/1000 | Loss: 0.00002082
Iteration 217/1000 | Loss: 0.00002082
Iteration 218/1000 | Loss: 0.00002082
Iteration 219/1000 | Loss: 0.00002082
Iteration 220/1000 | Loss: 0.00002082
Iteration 221/1000 | Loss: 0.00002081
Iteration 222/1000 | Loss: 0.00002081
Iteration 223/1000 | Loss: 0.00002081
Iteration 224/1000 | Loss: 0.00002081
Iteration 225/1000 | Loss: 0.00002081
Iteration 226/1000 | Loss: 0.00002081
Iteration 227/1000 | Loss: 0.00002081
Iteration 228/1000 | Loss: 0.00002081
Iteration 229/1000 | Loss: 0.00002081
Iteration 230/1000 | Loss: 0.00002081
Iteration 231/1000 | Loss: 0.00002081
Iteration 232/1000 | Loss: 0.00002081
Iteration 233/1000 | Loss: 0.00002080
Iteration 234/1000 | Loss: 0.00002080
Iteration 235/1000 | Loss: 0.00002080
Iteration 236/1000 | Loss: 0.00002080
Iteration 237/1000 | Loss: 0.00002079
Iteration 238/1000 | Loss: 0.00002079
Iteration 239/1000 | Loss: 0.00002079
Iteration 240/1000 | Loss: 0.00002079
Iteration 241/1000 | Loss: 0.00002079
Iteration 242/1000 | Loss: 0.00002079
Iteration 243/1000 | Loss: 0.00002079
Iteration 244/1000 | Loss: 0.00002079
Iteration 245/1000 | Loss: 0.00002079
Iteration 246/1000 | Loss: 0.00002079
Iteration 247/1000 | Loss: 0.00002079
Iteration 248/1000 | Loss: 0.00002078
Iteration 249/1000 | Loss: 0.00002078
Iteration 250/1000 | Loss: 0.00002078
Iteration 251/1000 | Loss: 0.00002078
Iteration 252/1000 | Loss: 0.00002078
Iteration 253/1000 | Loss: 0.00002078
Iteration 254/1000 | Loss: 0.00002078
Iteration 255/1000 | Loss: 0.00002077
Iteration 256/1000 | Loss: 0.00002077
Iteration 257/1000 | Loss: 0.00002077
Iteration 258/1000 | Loss: 0.00002077
Iteration 259/1000 | Loss: 0.00002077
Iteration 260/1000 | Loss: 0.00002076
Iteration 261/1000 | Loss: 0.00002076
Iteration 262/1000 | Loss: 0.00002076
Iteration 263/1000 | Loss: 0.00002076
Iteration 264/1000 | Loss: 0.00002076
Iteration 265/1000 | Loss: 0.00002076
Iteration 266/1000 | Loss: 0.00002076
Iteration 267/1000 | Loss: 0.00002076
Iteration 268/1000 | Loss: 0.00002076
Iteration 269/1000 | Loss: 0.00002076
Iteration 270/1000 | Loss: 0.00002076
Iteration 271/1000 | Loss: 0.00002076
Iteration 272/1000 | Loss: 0.00002076
Iteration 273/1000 | Loss: 0.00002076
Iteration 274/1000 | Loss: 0.00002076
Iteration 275/1000 | Loss: 0.00002076
Iteration 276/1000 | Loss: 0.00002076
Iteration 277/1000 | Loss: 0.00002076
Iteration 278/1000 | Loss: 0.00002076
Iteration 279/1000 | Loss: 0.00002076
Iteration 280/1000 | Loss: 0.00002076
Iteration 281/1000 | Loss: 0.00002076
Iteration 282/1000 | Loss: 0.00002076
Iteration 283/1000 | Loss: 0.00002076
Iteration 284/1000 | Loss: 0.00002076
Iteration 285/1000 | Loss: 0.00002076
Iteration 286/1000 | Loss: 0.00002076
Iteration 287/1000 | Loss: 0.00002076
Iteration 288/1000 | Loss: 0.00002076
Iteration 289/1000 | Loss: 0.00002076
Iteration 290/1000 | Loss: 0.00002076
Iteration 291/1000 | Loss: 0.00002076
Iteration 292/1000 | Loss: 0.00002076
Iteration 293/1000 | Loss: 0.00002076
Iteration 294/1000 | Loss: 0.00002076
Iteration 295/1000 | Loss: 0.00002076
Iteration 296/1000 | Loss: 0.00002076
Iteration 297/1000 | Loss: 0.00002076
Iteration 298/1000 | Loss: 0.00002076
Iteration 299/1000 | Loss: 0.00002076
Iteration 300/1000 | Loss: 0.00002076
Iteration 301/1000 | Loss: 0.00002076
Iteration 302/1000 | Loss: 0.00002076
Iteration 303/1000 | Loss: 0.00002076
Iteration 304/1000 | Loss: 0.00002076
Iteration 305/1000 | Loss: 0.00002076
Iteration 306/1000 | Loss: 0.00002076
Iteration 307/1000 | Loss: 0.00002076
Iteration 308/1000 | Loss: 0.00002076
Iteration 309/1000 | Loss: 0.00002076
Iteration 310/1000 | Loss: 0.00002076
Iteration 311/1000 | Loss: 0.00002076
Iteration 312/1000 | Loss: 0.00002076
Iteration 313/1000 | Loss: 0.00002076
Iteration 314/1000 | Loss: 0.00002076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [2.0757246602443047e-05, 2.0757246602443047e-05, 2.0757246602443047e-05, 2.0757246602443047e-05, 2.0757246602443047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0757246602443047e-05

Optimization complete. Final v2v error: 3.637444257736206 mm

Highest mean error: 4.449822425842285 mm for frame 60

Lowest mean error: 3.0952093601226807 mm for frame 25

Saving results

Total time: 46.65118741989136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418766
Iteration 2/25 | Loss: 0.00135922
Iteration 3/25 | Loss: 0.00118402
Iteration 4/25 | Loss: 0.00116353
Iteration 5/25 | Loss: 0.00115846
Iteration 6/25 | Loss: 0.00115700
Iteration 7/25 | Loss: 0.00115659
Iteration 8/25 | Loss: 0.00115659
Iteration 9/25 | Loss: 0.00115659
Iteration 10/25 | Loss: 0.00115659
Iteration 11/25 | Loss: 0.00115659
Iteration 12/25 | Loss: 0.00115659
Iteration 13/25 | Loss: 0.00115659
Iteration 14/25 | Loss: 0.00115659
Iteration 15/25 | Loss: 0.00115659
Iteration 16/25 | Loss: 0.00115659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011565895983949304, 0.0011565895983949304, 0.0011565895983949304, 0.0011565895983949304, 0.0011565895983949304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011565895983949304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.87851286
Iteration 2/25 | Loss: 0.00078405
Iteration 3/25 | Loss: 0.00078403
Iteration 4/25 | Loss: 0.00078403
Iteration 5/25 | Loss: 0.00078403
Iteration 6/25 | Loss: 0.00078403
Iteration 7/25 | Loss: 0.00078403
Iteration 8/25 | Loss: 0.00078403
Iteration 9/25 | Loss: 0.00078403
Iteration 10/25 | Loss: 0.00078403
Iteration 11/25 | Loss: 0.00078403
Iteration 12/25 | Loss: 0.00078403
Iteration 13/25 | Loss: 0.00078403
Iteration 14/25 | Loss: 0.00078403
Iteration 15/25 | Loss: 0.00078403
Iteration 16/25 | Loss: 0.00078403
Iteration 17/25 | Loss: 0.00078403
Iteration 18/25 | Loss: 0.00078403
Iteration 19/25 | Loss: 0.00078403
Iteration 20/25 | Loss: 0.00078403
Iteration 21/25 | Loss: 0.00078403
Iteration 22/25 | Loss: 0.00078403
Iteration 23/25 | Loss: 0.00078403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007840269827283919, 0.0007840269827283919, 0.0007840269827283919, 0.0007840269827283919, 0.0007840269827283919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007840269827283919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078403
Iteration 2/1000 | Loss: 0.00003451
Iteration 3/1000 | Loss: 0.00002368
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002095
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00001985
Iteration 8/1000 | Loss: 0.00001951
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00001895
Iteration 11/1000 | Loss: 0.00001882
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001854
Iteration 14/1000 | Loss: 0.00001850
Iteration 15/1000 | Loss: 0.00001847
Iteration 16/1000 | Loss: 0.00001846
Iteration 17/1000 | Loss: 0.00001845
Iteration 18/1000 | Loss: 0.00001845
Iteration 19/1000 | Loss: 0.00001845
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001842
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001841
Iteration 24/1000 | Loss: 0.00001837
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001837
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00001834
Iteration 31/1000 | Loss: 0.00001833
Iteration 32/1000 | Loss: 0.00001833
Iteration 33/1000 | Loss: 0.00001833
Iteration 34/1000 | Loss: 0.00001833
Iteration 35/1000 | Loss: 0.00001832
Iteration 36/1000 | Loss: 0.00001832
Iteration 37/1000 | Loss: 0.00001832
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001832
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001829
Iteration 44/1000 | Loss: 0.00001829
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001828
Iteration 47/1000 | Loss: 0.00001828
Iteration 48/1000 | Loss: 0.00001826
Iteration 49/1000 | Loss: 0.00001826
Iteration 50/1000 | Loss: 0.00001826
Iteration 51/1000 | Loss: 0.00001826
Iteration 52/1000 | Loss: 0.00001826
Iteration 53/1000 | Loss: 0.00001826
Iteration 54/1000 | Loss: 0.00001826
Iteration 55/1000 | Loss: 0.00001826
Iteration 56/1000 | Loss: 0.00001826
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001825
Iteration 59/1000 | Loss: 0.00001825
Iteration 60/1000 | Loss: 0.00001825
Iteration 61/1000 | Loss: 0.00001825
Iteration 62/1000 | Loss: 0.00001824
Iteration 63/1000 | Loss: 0.00001824
Iteration 64/1000 | Loss: 0.00001824
Iteration 65/1000 | Loss: 0.00001824
Iteration 66/1000 | Loss: 0.00001824
Iteration 67/1000 | Loss: 0.00001824
Iteration 68/1000 | Loss: 0.00001823
Iteration 69/1000 | Loss: 0.00001823
Iteration 70/1000 | Loss: 0.00001823
Iteration 71/1000 | Loss: 0.00001823
Iteration 72/1000 | Loss: 0.00001823
Iteration 73/1000 | Loss: 0.00001823
Iteration 74/1000 | Loss: 0.00001823
Iteration 75/1000 | Loss: 0.00001822
Iteration 76/1000 | Loss: 0.00001822
Iteration 77/1000 | Loss: 0.00001822
Iteration 78/1000 | Loss: 0.00001822
Iteration 79/1000 | Loss: 0.00001822
Iteration 80/1000 | Loss: 0.00001821
Iteration 81/1000 | Loss: 0.00001821
Iteration 82/1000 | Loss: 0.00001821
Iteration 83/1000 | Loss: 0.00001821
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001820
Iteration 87/1000 | Loss: 0.00001820
Iteration 88/1000 | Loss: 0.00001820
Iteration 89/1000 | Loss: 0.00001820
Iteration 90/1000 | Loss: 0.00001820
Iteration 91/1000 | Loss: 0.00001820
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001819
Iteration 96/1000 | Loss: 0.00001819
Iteration 97/1000 | Loss: 0.00001819
Iteration 98/1000 | Loss: 0.00001819
Iteration 99/1000 | Loss: 0.00001818
Iteration 100/1000 | Loss: 0.00001818
Iteration 101/1000 | Loss: 0.00001818
Iteration 102/1000 | Loss: 0.00001818
Iteration 103/1000 | Loss: 0.00001818
Iteration 104/1000 | Loss: 0.00001818
Iteration 105/1000 | Loss: 0.00001818
Iteration 106/1000 | Loss: 0.00001818
Iteration 107/1000 | Loss: 0.00001817
Iteration 108/1000 | Loss: 0.00001817
Iteration 109/1000 | Loss: 0.00001817
Iteration 110/1000 | Loss: 0.00001817
Iteration 111/1000 | Loss: 0.00001817
Iteration 112/1000 | Loss: 0.00001817
Iteration 113/1000 | Loss: 0.00001817
Iteration 114/1000 | Loss: 0.00001817
Iteration 115/1000 | Loss: 0.00001817
Iteration 116/1000 | Loss: 0.00001817
Iteration 117/1000 | Loss: 0.00001817
Iteration 118/1000 | Loss: 0.00001817
Iteration 119/1000 | Loss: 0.00001816
Iteration 120/1000 | Loss: 0.00001816
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001816
Iteration 123/1000 | Loss: 0.00001816
Iteration 124/1000 | Loss: 0.00001816
Iteration 125/1000 | Loss: 0.00001816
Iteration 126/1000 | Loss: 0.00001816
Iteration 127/1000 | Loss: 0.00001816
Iteration 128/1000 | Loss: 0.00001816
Iteration 129/1000 | Loss: 0.00001816
Iteration 130/1000 | Loss: 0.00001816
Iteration 131/1000 | Loss: 0.00001816
Iteration 132/1000 | Loss: 0.00001816
Iteration 133/1000 | Loss: 0.00001816
Iteration 134/1000 | Loss: 0.00001816
Iteration 135/1000 | Loss: 0.00001816
Iteration 136/1000 | Loss: 0.00001816
Iteration 137/1000 | Loss: 0.00001816
Iteration 138/1000 | Loss: 0.00001816
Iteration 139/1000 | Loss: 0.00001816
Iteration 140/1000 | Loss: 0.00001816
Iteration 141/1000 | Loss: 0.00001816
Iteration 142/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.8156093574361876e-05, 1.8156093574361876e-05, 1.8156093574361876e-05, 1.8156093574361876e-05, 1.8156093574361876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8156093574361876e-05

Optimization complete. Final v2v error: 3.605642080307007 mm

Highest mean error: 4.2735490798950195 mm for frame 49

Lowest mean error: 3.219564199447632 mm for frame 1

Saving results

Total time: 38.55457782745361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443921
Iteration 2/25 | Loss: 0.00119163
Iteration 3/25 | Loss: 0.00111640
Iteration 4/25 | Loss: 0.00110261
Iteration 5/25 | Loss: 0.00109842
Iteration 6/25 | Loss: 0.00109842
Iteration 7/25 | Loss: 0.00109842
Iteration 8/25 | Loss: 0.00109842
Iteration 9/25 | Loss: 0.00109842
Iteration 10/25 | Loss: 0.00109842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001098422333598137, 0.001098422333598137, 0.001098422333598137, 0.001098422333598137, 0.001098422333598137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001098422333598137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36533535
Iteration 2/25 | Loss: 0.00085994
Iteration 3/25 | Loss: 0.00085994
Iteration 4/25 | Loss: 0.00085994
Iteration 5/25 | Loss: 0.00085994
Iteration 6/25 | Loss: 0.00085994
Iteration 7/25 | Loss: 0.00085994
Iteration 8/25 | Loss: 0.00085994
Iteration 9/25 | Loss: 0.00085994
Iteration 10/25 | Loss: 0.00085994
Iteration 11/25 | Loss: 0.00085994
Iteration 12/25 | Loss: 0.00085994
Iteration 13/25 | Loss: 0.00085994
Iteration 14/25 | Loss: 0.00085994
Iteration 15/25 | Loss: 0.00085994
Iteration 16/25 | Loss: 0.00085994
Iteration 17/25 | Loss: 0.00085994
Iteration 18/25 | Loss: 0.00085994
Iteration 19/25 | Loss: 0.00085994
Iteration 20/25 | Loss: 0.00085994
Iteration 21/25 | Loss: 0.00085994
Iteration 22/25 | Loss: 0.00085994
Iteration 23/25 | Loss: 0.00085994
Iteration 24/25 | Loss: 0.00085994
Iteration 25/25 | Loss: 0.00085994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085994
Iteration 2/1000 | Loss: 0.00001931
Iteration 3/1000 | Loss: 0.00001421
Iteration 4/1000 | Loss: 0.00001346
Iteration 5/1000 | Loss: 0.00001305
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001266
Iteration 9/1000 | Loss: 0.00001266
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001227
Iteration 12/1000 | Loss: 0.00001223
Iteration 13/1000 | Loss: 0.00001219
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001211
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001211
Iteration 19/1000 | Loss: 0.00001211
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001205
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001198
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001186
Iteration 37/1000 | Loss: 0.00001186
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001185
Iteration 44/1000 | Loss: 0.00001185
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001181
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001176
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001170
Iteration 87/1000 | Loss: 0.00001170
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001168
Iteration 92/1000 | Loss: 0.00001168
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001166
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001166
Iteration 108/1000 | Loss: 0.00001166
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001165
Iteration 112/1000 | Loss: 0.00001165
Iteration 113/1000 | Loss: 0.00001165
Iteration 114/1000 | Loss: 0.00001165
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001163
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001162
Iteration 123/1000 | Loss: 0.00001162
Iteration 124/1000 | Loss: 0.00001162
Iteration 125/1000 | Loss: 0.00001162
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001162
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001161
Iteration 131/1000 | Loss: 0.00001161
Iteration 132/1000 | Loss: 0.00001161
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001161
Iteration 135/1000 | Loss: 0.00001160
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001160
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001160
Iteration 140/1000 | Loss: 0.00001160
Iteration 141/1000 | Loss: 0.00001160
Iteration 142/1000 | Loss: 0.00001160
Iteration 143/1000 | Loss: 0.00001160
Iteration 144/1000 | Loss: 0.00001160
Iteration 145/1000 | Loss: 0.00001159
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001159
Iteration 148/1000 | Loss: 0.00001159
Iteration 149/1000 | Loss: 0.00001159
Iteration 150/1000 | Loss: 0.00001158
Iteration 151/1000 | Loss: 0.00001158
Iteration 152/1000 | Loss: 0.00001158
Iteration 153/1000 | Loss: 0.00001158
Iteration 154/1000 | Loss: 0.00001158
Iteration 155/1000 | Loss: 0.00001158
Iteration 156/1000 | Loss: 0.00001158
Iteration 157/1000 | Loss: 0.00001158
Iteration 158/1000 | Loss: 0.00001158
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001157
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Iteration 163/1000 | Loss: 0.00001157
Iteration 164/1000 | Loss: 0.00001157
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001155
Iteration 172/1000 | Loss: 0.00001155
Iteration 173/1000 | Loss: 0.00001155
Iteration 174/1000 | Loss: 0.00001154
Iteration 175/1000 | Loss: 0.00001154
Iteration 176/1000 | Loss: 0.00001154
Iteration 177/1000 | Loss: 0.00001154
Iteration 178/1000 | Loss: 0.00001154
Iteration 179/1000 | Loss: 0.00001154
Iteration 180/1000 | Loss: 0.00001154
Iteration 181/1000 | Loss: 0.00001153
Iteration 182/1000 | Loss: 0.00001153
Iteration 183/1000 | Loss: 0.00001153
Iteration 184/1000 | Loss: 0.00001153
Iteration 185/1000 | Loss: 0.00001153
Iteration 186/1000 | Loss: 0.00001153
Iteration 187/1000 | Loss: 0.00001153
Iteration 188/1000 | Loss: 0.00001152
Iteration 189/1000 | Loss: 0.00001152
Iteration 190/1000 | Loss: 0.00001152
Iteration 191/1000 | Loss: 0.00001151
Iteration 192/1000 | Loss: 0.00001151
Iteration 193/1000 | Loss: 0.00001151
Iteration 194/1000 | Loss: 0.00001151
Iteration 195/1000 | Loss: 0.00001150
Iteration 196/1000 | Loss: 0.00001150
Iteration 197/1000 | Loss: 0.00001150
Iteration 198/1000 | Loss: 0.00001150
Iteration 199/1000 | Loss: 0.00001150
Iteration 200/1000 | Loss: 0.00001150
Iteration 201/1000 | Loss: 0.00001150
Iteration 202/1000 | Loss: 0.00001149
Iteration 203/1000 | Loss: 0.00001149
Iteration 204/1000 | Loss: 0.00001149
Iteration 205/1000 | Loss: 0.00001149
Iteration 206/1000 | Loss: 0.00001149
Iteration 207/1000 | Loss: 0.00001149
Iteration 208/1000 | Loss: 0.00001149
Iteration 209/1000 | Loss: 0.00001149
Iteration 210/1000 | Loss: 0.00001149
Iteration 211/1000 | Loss: 0.00001149
Iteration 212/1000 | Loss: 0.00001149
Iteration 213/1000 | Loss: 0.00001149
Iteration 214/1000 | Loss: 0.00001149
Iteration 215/1000 | Loss: 0.00001149
Iteration 216/1000 | Loss: 0.00001149
Iteration 217/1000 | Loss: 0.00001149
Iteration 218/1000 | Loss: 0.00001149
Iteration 219/1000 | Loss: 0.00001149
Iteration 220/1000 | Loss: 0.00001149
Iteration 221/1000 | Loss: 0.00001149
Iteration 222/1000 | Loss: 0.00001149
Iteration 223/1000 | Loss: 0.00001149
Iteration 224/1000 | Loss: 0.00001149
Iteration 225/1000 | Loss: 0.00001149
Iteration 226/1000 | Loss: 0.00001149
Iteration 227/1000 | Loss: 0.00001149
Iteration 228/1000 | Loss: 0.00001149
Iteration 229/1000 | Loss: 0.00001149
Iteration 230/1000 | Loss: 0.00001149
Iteration 231/1000 | Loss: 0.00001149
Iteration 232/1000 | Loss: 0.00001149
Iteration 233/1000 | Loss: 0.00001149
Iteration 234/1000 | Loss: 0.00001149
Iteration 235/1000 | Loss: 0.00001149
Iteration 236/1000 | Loss: 0.00001149
Iteration 237/1000 | Loss: 0.00001149
Iteration 238/1000 | Loss: 0.00001149
Iteration 239/1000 | Loss: 0.00001149
Iteration 240/1000 | Loss: 0.00001149
Iteration 241/1000 | Loss: 0.00001149
Iteration 242/1000 | Loss: 0.00001149
Iteration 243/1000 | Loss: 0.00001149
Iteration 244/1000 | Loss: 0.00001149
Iteration 245/1000 | Loss: 0.00001149
Iteration 246/1000 | Loss: 0.00001149
Iteration 247/1000 | Loss: 0.00001149
Iteration 248/1000 | Loss: 0.00001149
Iteration 249/1000 | Loss: 0.00001149
Iteration 250/1000 | Loss: 0.00001149
Iteration 251/1000 | Loss: 0.00001149
Iteration 252/1000 | Loss: 0.00001149
Iteration 253/1000 | Loss: 0.00001149
Iteration 254/1000 | Loss: 0.00001149
Iteration 255/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.14949289127253e-05, 1.14949289127253e-05, 1.14949289127253e-05, 1.14949289127253e-05, 1.14949289127253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.14949289127253e-05

Optimization complete. Final v2v error: 2.8488376140594482 mm

Highest mean error: 3.1976187229156494 mm for frame 97

Lowest mean error: 2.579207181930542 mm for frame 1

Saving results

Total time: 44.96760630607605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788015
Iteration 2/25 | Loss: 0.00137731
Iteration 3/25 | Loss: 0.00113705
Iteration 4/25 | Loss: 0.00109643
Iteration 5/25 | Loss: 0.00108594
Iteration 6/25 | Loss: 0.00108335
Iteration 7/25 | Loss: 0.00108281
Iteration 8/25 | Loss: 0.00108281
Iteration 9/25 | Loss: 0.00108281
Iteration 10/25 | Loss: 0.00108281
Iteration 11/25 | Loss: 0.00108281
Iteration 12/25 | Loss: 0.00108281
Iteration 13/25 | Loss: 0.00108281
Iteration 14/25 | Loss: 0.00108281
Iteration 15/25 | Loss: 0.00108281
Iteration 16/25 | Loss: 0.00108281
Iteration 17/25 | Loss: 0.00108281
Iteration 18/25 | Loss: 0.00108281
Iteration 19/25 | Loss: 0.00108281
Iteration 20/25 | Loss: 0.00108281
Iteration 21/25 | Loss: 0.00108281
Iteration 22/25 | Loss: 0.00108281
Iteration 23/25 | Loss: 0.00108281
Iteration 24/25 | Loss: 0.00108281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010828126687556505, 0.0010828126687556505, 0.0010828126687556505, 0.0010828126687556505, 0.0010828126687556505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010828126687556505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29261172
Iteration 2/25 | Loss: 0.00083599
Iteration 3/25 | Loss: 0.00083599
Iteration 4/25 | Loss: 0.00083599
Iteration 5/25 | Loss: 0.00083599
Iteration 6/25 | Loss: 0.00083599
Iteration 7/25 | Loss: 0.00083599
Iteration 8/25 | Loss: 0.00083599
Iteration 9/25 | Loss: 0.00083599
Iteration 10/25 | Loss: 0.00083599
Iteration 11/25 | Loss: 0.00083599
Iteration 12/25 | Loss: 0.00083599
Iteration 13/25 | Loss: 0.00083599
Iteration 14/25 | Loss: 0.00083599
Iteration 15/25 | Loss: 0.00083599
Iteration 16/25 | Loss: 0.00083599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008359880885109305, 0.0008359880885109305, 0.0008359880885109305, 0.0008359880885109305, 0.0008359880885109305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008359880885109305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083599
Iteration 2/1000 | Loss: 0.00004602
Iteration 3/1000 | Loss: 0.00002996
Iteration 4/1000 | Loss: 0.00002305
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00001960
Iteration 7/1000 | Loss: 0.00001880
Iteration 8/1000 | Loss: 0.00001826
Iteration 9/1000 | Loss: 0.00001781
Iteration 10/1000 | Loss: 0.00001750
Iteration 11/1000 | Loss: 0.00001731
Iteration 12/1000 | Loss: 0.00001726
Iteration 13/1000 | Loss: 0.00001709
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001694
Iteration 16/1000 | Loss: 0.00001681
Iteration 17/1000 | Loss: 0.00001673
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001668
Iteration 23/1000 | Loss: 0.00001664
Iteration 24/1000 | Loss: 0.00001662
Iteration 25/1000 | Loss: 0.00001661
Iteration 26/1000 | Loss: 0.00001661
Iteration 27/1000 | Loss: 0.00001661
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001657
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001654
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001652
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001647
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001645
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001644
Iteration 59/1000 | Loss: 0.00001644
Iteration 60/1000 | Loss: 0.00001644
Iteration 61/1000 | Loss: 0.00001644
Iteration 62/1000 | Loss: 0.00001644
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001643
Iteration 65/1000 | Loss: 0.00001643
Iteration 66/1000 | Loss: 0.00001643
Iteration 67/1000 | Loss: 0.00001642
Iteration 68/1000 | Loss: 0.00001642
Iteration 69/1000 | Loss: 0.00001642
Iteration 70/1000 | Loss: 0.00001642
Iteration 71/1000 | Loss: 0.00001642
Iteration 72/1000 | Loss: 0.00001642
Iteration 73/1000 | Loss: 0.00001642
Iteration 74/1000 | Loss: 0.00001642
Iteration 75/1000 | Loss: 0.00001642
Iteration 76/1000 | Loss: 0.00001642
Iteration 77/1000 | Loss: 0.00001641
Iteration 78/1000 | Loss: 0.00001641
Iteration 79/1000 | Loss: 0.00001640
Iteration 80/1000 | Loss: 0.00001640
Iteration 81/1000 | Loss: 0.00001640
Iteration 82/1000 | Loss: 0.00001639
Iteration 83/1000 | Loss: 0.00001639
Iteration 84/1000 | Loss: 0.00001639
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001638
Iteration 87/1000 | Loss: 0.00001638
Iteration 88/1000 | Loss: 0.00001638
Iteration 89/1000 | Loss: 0.00001638
Iteration 90/1000 | Loss: 0.00001638
Iteration 91/1000 | Loss: 0.00001638
Iteration 92/1000 | Loss: 0.00001638
Iteration 93/1000 | Loss: 0.00001637
Iteration 94/1000 | Loss: 0.00001637
Iteration 95/1000 | Loss: 0.00001637
Iteration 96/1000 | Loss: 0.00001637
Iteration 97/1000 | Loss: 0.00001637
Iteration 98/1000 | Loss: 0.00001637
Iteration 99/1000 | Loss: 0.00001637
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001635
Iteration 109/1000 | Loss: 0.00001635
Iteration 110/1000 | Loss: 0.00001635
Iteration 111/1000 | Loss: 0.00001635
Iteration 112/1000 | Loss: 0.00001635
Iteration 113/1000 | Loss: 0.00001635
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001635
Iteration 116/1000 | Loss: 0.00001635
Iteration 117/1000 | Loss: 0.00001634
Iteration 118/1000 | Loss: 0.00001634
Iteration 119/1000 | Loss: 0.00001634
Iteration 120/1000 | Loss: 0.00001634
Iteration 121/1000 | Loss: 0.00001634
Iteration 122/1000 | Loss: 0.00001634
Iteration 123/1000 | Loss: 0.00001634
Iteration 124/1000 | Loss: 0.00001634
Iteration 125/1000 | Loss: 0.00001634
Iteration 126/1000 | Loss: 0.00001634
Iteration 127/1000 | Loss: 0.00001634
Iteration 128/1000 | Loss: 0.00001634
Iteration 129/1000 | Loss: 0.00001634
Iteration 130/1000 | Loss: 0.00001633
Iteration 131/1000 | Loss: 0.00001633
Iteration 132/1000 | Loss: 0.00001633
Iteration 133/1000 | Loss: 0.00001633
Iteration 134/1000 | Loss: 0.00001633
Iteration 135/1000 | Loss: 0.00001633
Iteration 136/1000 | Loss: 0.00001633
Iteration 137/1000 | Loss: 0.00001633
Iteration 138/1000 | Loss: 0.00001633
Iteration 139/1000 | Loss: 0.00001633
Iteration 140/1000 | Loss: 0.00001633
Iteration 141/1000 | Loss: 0.00001633
Iteration 142/1000 | Loss: 0.00001633
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001632
Iteration 145/1000 | Loss: 0.00001632
Iteration 146/1000 | Loss: 0.00001632
Iteration 147/1000 | Loss: 0.00001632
Iteration 148/1000 | Loss: 0.00001632
Iteration 149/1000 | Loss: 0.00001632
Iteration 150/1000 | Loss: 0.00001632
Iteration 151/1000 | Loss: 0.00001632
Iteration 152/1000 | Loss: 0.00001631
Iteration 153/1000 | Loss: 0.00001631
Iteration 154/1000 | Loss: 0.00001631
Iteration 155/1000 | Loss: 0.00001631
Iteration 156/1000 | Loss: 0.00001631
Iteration 157/1000 | Loss: 0.00001631
Iteration 158/1000 | Loss: 0.00001631
Iteration 159/1000 | Loss: 0.00001631
Iteration 160/1000 | Loss: 0.00001630
Iteration 161/1000 | Loss: 0.00001630
Iteration 162/1000 | Loss: 0.00001630
Iteration 163/1000 | Loss: 0.00001630
Iteration 164/1000 | Loss: 0.00001630
Iteration 165/1000 | Loss: 0.00001630
Iteration 166/1000 | Loss: 0.00001630
Iteration 167/1000 | Loss: 0.00001630
Iteration 168/1000 | Loss: 0.00001630
Iteration 169/1000 | Loss: 0.00001630
Iteration 170/1000 | Loss: 0.00001630
Iteration 171/1000 | Loss: 0.00001630
Iteration 172/1000 | Loss: 0.00001630
Iteration 173/1000 | Loss: 0.00001630
Iteration 174/1000 | Loss: 0.00001630
Iteration 175/1000 | Loss: 0.00001630
Iteration 176/1000 | Loss: 0.00001630
Iteration 177/1000 | Loss: 0.00001629
Iteration 178/1000 | Loss: 0.00001629
Iteration 179/1000 | Loss: 0.00001629
Iteration 180/1000 | Loss: 0.00001629
Iteration 181/1000 | Loss: 0.00001629
Iteration 182/1000 | Loss: 0.00001629
Iteration 183/1000 | Loss: 0.00001629
Iteration 184/1000 | Loss: 0.00001629
Iteration 185/1000 | Loss: 0.00001629
Iteration 186/1000 | Loss: 0.00001629
Iteration 187/1000 | Loss: 0.00001629
Iteration 188/1000 | Loss: 0.00001629
Iteration 189/1000 | Loss: 0.00001629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.6294576198561117e-05, 1.6294576198561117e-05, 1.6294576198561117e-05, 1.6294576198561117e-05, 1.6294576198561117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6294576198561117e-05

Optimization complete. Final v2v error: 3.4089250564575195 mm

Highest mean error: 3.9315311908721924 mm for frame 71

Lowest mean error: 2.677367925643921 mm for frame 131

Saving results

Total time: 43.777244329452515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439350
Iteration 2/25 | Loss: 0.00118253
Iteration 3/25 | Loss: 0.00110668
Iteration 4/25 | Loss: 0.00109898
Iteration 5/25 | Loss: 0.00109639
Iteration 6/25 | Loss: 0.00109577
Iteration 7/25 | Loss: 0.00109577
Iteration 8/25 | Loss: 0.00109577
Iteration 9/25 | Loss: 0.00109577
Iteration 10/25 | Loss: 0.00109577
Iteration 11/25 | Loss: 0.00109577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001095765852369368, 0.001095765852369368, 0.001095765852369368, 0.001095765852369368, 0.001095765852369368]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095765852369368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36335170
Iteration 2/25 | Loss: 0.00092976
Iteration 3/25 | Loss: 0.00092975
Iteration 4/25 | Loss: 0.00092975
Iteration 5/25 | Loss: 0.00092975
Iteration 6/25 | Loss: 0.00092975
Iteration 7/25 | Loss: 0.00092975
Iteration 8/25 | Loss: 0.00092975
Iteration 9/25 | Loss: 0.00092975
Iteration 10/25 | Loss: 0.00092975
Iteration 11/25 | Loss: 0.00092975
Iteration 12/25 | Loss: 0.00092975
Iteration 13/25 | Loss: 0.00092975
Iteration 14/25 | Loss: 0.00092975
Iteration 15/25 | Loss: 0.00092975
Iteration 16/25 | Loss: 0.00092975
Iteration 17/25 | Loss: 0.00092975
Iteration 18/25 | Loss: 0.00092975
Iteration 19/25 | Loss: 0.00092975
Iteration 20/25 | Loss: 0.00092975
Iteration 21/25 | Loss: 0.00092975
Iteration 22/25 | Loss: 0.00092975
Iteration 23/25 | Loss: 0.00092975
Iteration 24/25 | Loss: 0.00092975
Iteration 25/25 | Loss: 0.00092975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092975
Iteration 2/1000 | Loss: 0.00002918
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001313
Iteration 5/1000 | Loss: 0.00001206
Iteration 6/1000 | Loss: 0.00001146
Iteration 7/1000 | Loss: 0.00001099
Iteration 8/1000 | Loss: 0.00001067
Iteration 9/1000 | Loss: 0.00001043
Iteration 10/1000 | Loss: 0.00001023
Iteration 11/1000 | Loss: 0.00001013
Iteration 12/1000 | Loss: 0.00001004
Iteration 13/1000 | Loss: 0.00001004
Iteration 14/1000 | Loss: 0.00001004
Iteration 15/1000 | Loss: 0.00001004
Iteration 16/1000 | Loss: 0.00001004
Iteration 17/1000 | Loss: 0.00001004
Iteration 18/1000 | Loss: 0.00001004
Iteration 19/1000 | Loss: 0.00001004
Iteration 20/1000 | Loss: 0.00001003
Iteration 21/1000 | Loss: 0.00001003
Iteration 22/1000 | Loss: 0.00001003
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001001
Iteration 26/1000 | Loss: 0.00000999
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000999
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000998
Iteration 35/1000 | Loss: 0.00000998
Iteration 36/1000 | Loss: 0.00000998
Iteration 37/1000 | Loss: 0.00000997
Iteration 38/1000 | Loss: 0.00000997
Iteration 39/1000 | Loss: 0.00000997
Iteration 40/1000 | Loss: 0.00000997
Iteration 41/1000 | Loss: 0.00000996
Iteration 42/1000 | Loss: 0.00000996
Iteration 43/1000 | Loss: 0.00000995
Iteration 44/1000 | Loss: 0.00000995
Iteration 45/1000 | Loss: 0.00000994
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000993
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000992
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000991
Iteration 54/1000 | Loss: 0.00000991
Iteration 55/1000 | Loss: 0.00000991
Iteration 56/1000 | Loss: 0.00000991
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000990
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000989
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000988
Iteration 64/1000 | Loss: 0.00000988
Iteration 65/1000 | Loss: 0.00000987
Iteration 66/1000 | Loss: 0.00000987
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000984
Iteration 72/1000 | Loss: 0.00000984
Iteration 73/1000 | Loss: 0.00000984
Iteration 74/1000 | Loss: 0.00000984
Iteration 75/1000 | Loss: 0.00000984
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000983
Iteration 78/1000 | Loss: 0.00000983
Iteration 79/1000 | Loss: 0.00000983
Iteration 80/1000 | Loss: 0.00000983
Iteration 81/1000 | Loss: 0.00000983
Iteration 82/1000 | Loss: 0.00000982
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000980
Iteration 93/1000 | Loss: 0.00000980
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000978
Iteration 111/1000 | Loss: 0.00000978
Iteration 112/1000 | Loss: 0.00000978
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000975
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000975
Iteration 134/1000 | Loss: 0.00000975
Iteration 135/1000 | Loss: 0.00000975
Iteration 136/1000 | Loss: 0.00000975
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Iteration 140/1000 | Loss: 0.00000974
Iteration 141/1000 | Loss: 0.00000974
Iteration 142/1000 | Loss: 0.00000974
Iteration 143/1000 | Loss: 0.00000973
Iteration 144/1000 | Loss: 0.00000973
Iteration 145/1000 | Loss: 0.00000973
Iteration 146/1000 | Loss: 0.00000973
Iteration 147/1000 | Loss: 0.00000972
Iteration 148/1000 | Loss: 0.00000972
Iteration 149/1000 | Loss: 0.00000972
Iteration 150/1000 | Loss: 0.00000972
Iteration 151/1000 | Loss: 0.00000971
Iteration 152/1000 | Loss: 0.00000971
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000969
Iteration 156/1000 | Loss: 0.00000969
Iteration 157/1000 | Loss: 0.00000969
Iteration 158/1000 | Loss: 0.00000969
Iteration 159/1000 | Loss: 0.00000969
Iteration 160/1000 | Loss: 0.00000969
Iteration 161/1000 | Loss: 0.00000969
Iteration 162/1000 | Loss: 0.00000969
Iteration 163/1000 | Loss: 0.00000969
Iteration 164/1000 | Loss: 0.00000969
Iteration 165/1000 | Loss: 0.00000969
Iteration 166/1000 | Loss: 0.00000969
Iteration 167/1000 | Loss: 0.00000969
Iteration 168/1000 | Loss: 0.00000968
Iteration 169/1000 | Loss: 0.00000968
Iteration 170/1000 | Loss: 0.00000968
Iteration 171/1000 | Loss: 0.00000968
Iteration 172/1000 | Loss: 0.00000968
Iteration 173/1000 | Loss: 0.00000968
Iteration 174/1000 | Loss: 0.00000968
Iteration 175/1000 | Loss: 0.00000968
Iteration 176/1000 | Loss: 0.00000968
Iteration 177/1000 | Loss: 0.00000967
Iteration 178/1000 | Loss: 0.00000967
Iteration 179/1000 | Loss: 0.00000967
Iteration 180/1000 | Loss: 0.00000967
Iteration 181/1000 | Loss: 0.00000967
Iteration 182/1000 | Loss: 0.00000967
Iteration 183/1000 | Loss: 0.00000967
Iteration 184/1000 | Loss: 0.00000966
Iteration 185/1000 | Loss: 0.00000965
Iteration 186/1000 | Loss: 0.00000965
Iteration 187/1000 | Loss: 0.00000965
Iteration 188/1000 | Loss: 0.00000965
Iteration 189/1000 | Loss: 0.00000964
Iteration 190/1000 | Loss: 0.00000964
Iteration 191/1000 | Loss: 0.00000964
Iteration 192/1000 | Loss: 0.00000964
Iteration 193/1000 | Loss: 0.00000964
Iteration 194/1000 | Loss: 0.00000964
Iteration 195/1000 | Loss: 0.00000964
Iteration 196/1000 | Loss: 0.00000964
Iteration 197/1000 | Loss: 0.00000964
Iteration 198/1000 | Loss: 0.00000964
Iteration 199/1000 | Loss: 0.00000963
Iteration 200/1000 | Loss: 0.00000963
Iteration 201/1000 | Loss: 0.00000963
Iteration 202/1000 | Loss: 0.00000963
Iteration 203/1000 | Loss: 0.00000963
Iteration 204/1000 | Loss: 0.00000963
Iteration 205/1000 | Loss: 0.00000963
Iteration 206/1000 | Loss: 0.00000963
Iteration 207/1000 | Loss: 0.00000963
Iteration 208/1000 | Loss: 0.00000963
Iteration 209/1000 | Loss: 0.00000963
Iteration 210/1000 | Loss: 0.00000962
Iteration 211/1000 | Loss: 0.00000962
Iteration 212/1000 | Loss: 0.00000962
Iteration 213/1000 | Loss: 0.00000962
Iteration 214/1000 | Loss: 0.00000962
Iteration 215/1000 | Loss: 0.00000962
Iteration 216/1000 | Loss: 0.00000962
Iteration 217/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.624498488847166e-06, 9.624498488847166e-06, 9.624498488847166e-06, 9.624498488847166e-06, 9.624498488847166e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.624498488847166e-06

Optimization complete. Final v2v error: 2.6211626529693604 mm

Highest mean error: 2.9597885608673096 mm for frame 111

Lowest mean error: 2.4144973754882812 mm for frame 18

Saving results

Total time: 40.77515125274658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117047
Iteration 2/25 | Loss: 0.00218033
Iteration 3/25 | Loss: 0.00164715
Iteration 4/25 | Loss: 0.00152170
Iteration 5/25 | Loss: 0.00155272
Iteration 6/25 | Loss: 0.00153341
Iteration 7/25 | Loss: 0.00148443
Iteration 8/25 | Loss: 0.00143176
Iteration 9/25 | Loss: 0.00140528
Iteration 10/25 | Loss: 0.00138379
Iteration 11/25 | Loss: 0.00137935
Iteration 12/25 | Loss: 0.00137934
Iteration 13/25 | Loss: 0.00137726
Iteration 14/25 | Loss: 0.00137396
Iteration 15/25 | Loss: 0.00137549
Iteration 16/25 | Loss: 0.00137536
Iteration 17/25 | Loss: 0.00137568
Iteration 18/25 | Loss: 0.00137309
Iteration 19/25 | Loss: 0.00137237
Iteration 20/25 | Loss: 0.00137236
Iteration 21/25 | Loss: 0.00137086
Iteration 22/25 | Loss: 0.00137050
Iteration 23/25 | Loss: 0.00137118
Iteration 24/25 | Loss: 0.00137066
Iteration 25/25 | Loss: 0.00137024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28481770
Iteration 2/25 | Loss: 0.00176664
Iteration 3/25 | Loss: 0.00176664
Iteration 4/25 | Loss: 0.00176664
Iteration 5/25 | Loss: 0.00176664
Iteration 6/25 | Loss: 0.00176664
Iteration 7/25 | Loss: 0.00176664
Iteration 8/25 | Loss: 0.00176664
Iteration 9/25 | Loss: 0.00176664
Iteration 10/25 | Loss: 0.00176664
Iteration 11/25 | Loss: 0.00176664
Iteration 12/25 | Loss: 0.00176664
Iteration 13/25 | Loss: 0.00176664
Iteration 14/25 | Loss: 0.00176664
Iteration 15/25 | Loss: 0.00176664
Iteration 16/25 | Loss: 0.00176664
Iteration 17/25 | Loss: 0.00176664
Iteration 18/25 | Loss: 0.00176664
Iteration 19/25 | Loss: 0.00176664
Iteration 20/25 | Loss: 0.00176664
Iteration 21/25 | Loss: 0.00176664
Iteration 22/25 | Loss: 0.00176664
Iteration 23/25 | Loss: 0.00176664
Iteration 24/25 | Loss: 0.00176664
Iteration 25/25 | Loss: 0.00176664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176664
Iteration 2/1000 | Loss: 0.00026415
Iteration 3/1000 | Loss: 0.00288137
Iteration 4/1000 | Loss: 0.00153284
Iteration 5/1000 | Loss: 0.00095211
Iteration 6/1000 | Loss: 0.00019047
Iteration 7/1000 | Loss: 0.00131853
Iteration 8/1000 | Loss: 0.00134963
Iteration 9/1000 | Loss: 0.00088809
Iteration 10/1000 | Loss: 0.00037375
Iteration 11/1000 | Loss: 0.00047657
Iteration 12/1000 | Loss: 0.00029909
Iteration 13/1000 | Loss: 0.00013149
Iteration 14/1000 | Loss: 0.00022219
Iteration 15/1000 | Loss: 0.00016068
Iteration 16/1000 | Loss: 0.00026188
Iteration 17/1000 | Loss: 0.00047127
Iteration 18/1000 | Loss: 0.00049916
Iteration 19/1000 | Loss: 0.00021552
Iteration 20/1000 | Loss: 0.00023683
Iteration 21/1000 | Loss: 0.00015762
Iteration 22/1000 | Loss: 0.00007120
Iteration 23/1000 | Loss: 0.00038360
Iteration 24/1000 | Loss: 0.00057546
Iteration 25/1000 | Loss: 0.00031061
Iteration 26/1000 | Loss: 0.00031639
Iteration 27/1000 | Loss: 0.00053636
Iteration 28/1000 | Loss: 0.00018268
Iteration 29/1000 | Loss: 0.00049911
Iteration 30/1000 | Loss: 0.00029086
Iteration 31/1000 | Loss: 0.00007788
Iteration 32/1000 | Loss: 0.00010844
Iteration 33/1000 | Loss: 0.00023620
Iteration 34/1000 | Loss: 0.00022220
Iteration 35/1000 | Loss: 0.00063253
Iteration 36/1000 | Loss: 0.00025659
Iteration 37/1000 | Loss: 0.00031048
Iteration 38/1000 | Loss: 0.00022978
Iteration 39/1000 | Loss: 0.00036238
Iteration 40/1000 | Loss: 0.00010888
Iteration 41/1000 | Loss: 0.00006877
Iteration 42/1000 | Loss: 0.00006039
Iteration 43/1000 | Loss: 0.00041116
Iteration 44/1000 | Loss: 0.00030654
Iteration 45/1000 | Loss: 0.00024795
Iteration 46/1000 | Loss: 0.00026465
Iteration 47/1000 | Loss: 0.00022961
Iteration 48/1000 | Loss: 0.00011255
Iteration 49/1000 | Loss: 0.00018836
Iteration 50/1000 | Loss: 0.00009853
Iteration 51/1000 | Loss: 0.00011619
Iteration 52/1000 | Loss: 0.00011065
Iteration 53/1000 | Loss: 0.00008565
Iteration 54/1000 | Loss: 0.00013615
Iteration 55/1000 | Loss: 0.00022541
Iteration 56/1000 | Loss: 0.00013839
Iteration 57/1000 | Loss: 0.00015529
Iteration 58/1000 | Loss: 0.00020032
Iteration 59/1000 | Loss: 0.00010603
Iteration 60/1000 | Loss: 0.00012838
Iteration 61/1000 | Loss: 0.00010367
Iteration 62/1000 | Loss: 0.00015174
Iteration 63/1000 | Loss: 0.00015259
Iteration 64/1000 | Loss: 0.00012578
Iteration 65/1000 | Loss: 0.00012825
Iteration 66/1000 | Loss: 0.00011908
Iteration 67/1000 | Loss: 0.00012605
Iteration 68/1000 | Loss: 0.00011513
Iteration 69/1000 | Loss: 0.00014463
Iteration 70/1000 | Loss: 0.00011858
Iteration 71/1000 | Loss: 0.00013761
Iteration 72/1000 | Loss: 0.00011944
Iteration 73/1000 | Loss: 0.00015578
Iteration 74/1000 | Loss: 0.00012087
Iteration 75/1000 | Loss: 0.00009911
Iteration 76/1000 | Loss: 0.00009597
Iteration 77/1000 | Loss: 0.00011228
Iteration 78/1000 | Loss: 0.00010395
Iteration 79/1000 | Loss: 0.00007959
Iteration 80/1000 | Loss: 0.00007133
Iteration 81/1000 | Loss: 0.00007298
Iteration 82/1000 | Loss: 0.00010133
Iteration 83/1000 | Loss: 0.00013368
Iteration 84/1000 | Loss: 0.00009806
Iteration 85/1000 | Loss: 0.00018249
Iteration 86/1000 | Loss: 0.00015296
Iteration 87/1000 | Loss: 0.00015034
Iteration 88/1000 | Loss: 0.00014081
Iteration 89/1000 | Loss: 0.00015045
Iteration 90/1000 | Loss: 0.00013751
Iteration 91/1000 | Loss: 0.00015218
Iteration 92/1000 | Loss: 0.00013579
Iteration 93/1000 | Loss: 0.00013549
Iteration 94/1000 | Loss: 0.00008046
Iteration 95/1000 | Loss: 0.00013476
Iteration 96/1000 | Loss: 0.00013639
Iteration 97/1000 | Loss: 0.00014661
Iteration 98/1000 | Loss: 0.00006774
Iteration 99/1000 | Loss: 0.00011514
Iteration 100/1000 | Loss: 0.00008185
Iteration 101/1000 | Loss: 0.00010041
Iteration 102/1000 | Loss: 0.00014963
Iteration 103/1000 | Loss: 0.00012691
Iteration 104/1000 | Loss: 0.00021650
Iteration 105/1000 | Loss: 0.00015119
Iteration 106/1000 | Loss: 0.00012581
Iteration 107/1000 | Loss: 0.00006845
Iteration 108/1000 | Loss: 0.00007811
Iteration 109/1000 | Loss: 0.00011839
Iteration 110/1000 | Loss: 0.00015522
Iteration 111/1000 | Loss: 0.00010250
Iteration 112/1000 | Loss: 0.00010178
Iteration 113/1000 | Loss: 0.00009708
Iteration 114/1000 | Loss: 0.00011141
Iteration 115/1000 | Loss: 0.00009743
Iteration 116/1000 | Loss: 0.00006860
Iteration 117/1000 | Loss: 0.00009419
Iteration 118/1000 | Loss: 0.00014379
Iteration 119/1000 | Loss: 0.00012325
Iteration 120/1000 | Loss: 0.00014559
Iteration 121/1000 | Loss: 0.00010346
Iteration 122/1000 | Loss: 0.00005416
Iteration 123/1000 | Loss: 0.00008675
Iteration 124/1000 | Loss: 0.00006048
Iteration 125/1000 | Loss: 0.00056515
Iteration 126/1000 | Loss: 0.00261412
Iteration 127/1000 | Loss: 0.00149257
Iteration 128/1000 | Loss: 0.00015514
Iteration 129/1000 | Loss: 0.00023553
Iteration 130/1000 | Loss: 0.00021505
Iteration 131/1000 | Loss: 0.00021097
Iteration 132/1000 | Loss: 0.00054208
Iteration 133/1000 | Loss: 0.00006247
Iteration 134/1000 | Loss: 0.00058036
Iteration 135/1000 | Loss: 0.00065670
Iteration 136/1000 | Loss: 0.00030009
Iteration 137/1000 | Loss: 0.00099681
Iteration 138/1000 | Loss: 0.00074562
Iteration 139/1000 | Loss: 0.00007653
Iteration 140/1000 | Loss: 0.00005293
Iteration 141/1000 | Loss: 0.00034498
Iteration 142/1000 | Loss: 0.00023745
Iteration 143/1000 | Loss: 0.00048563
Iteration 144/1000 | Loss: 0.00007603
Iteration 145/1000 | Loss: 0.00046579
Iteration 146/1000 | Loss: 0.00005634
Iteration 147/1000 | Loss: 0.00004841
Iteration 148/1000 | Loss: 0.00004521
Iteration 149/1000 | Loss: 0.00004318
Iteration 150/1000 | Loss: 0.00089659
Iteration 151/1000 | Loss: 0.00040696
Iteration 152/1000 | Loss: 0.00133141
Iteration 153/1000 | Loss: 0.00063426
Iteration 154/1000 | Loss: 0.00167961
Iteration 155/1000 | Loss: 0.00130116
Iteration 156/1000 | Loss: 0.00023104
Iteration 157/1000 | Loss: 0.00008628
Iteration 158/1000 | Loss: 0.00006641
Iteration 159/1000 | Loss: 0.00040185
Iteration 160/1000 | Loss: 0.00004924
Iteration 161/1000 | Loss: 0.00004508
Iteration 162/1000 | Loss: 0.00004102
Iteration 163/1000 | Loss: 0.00003741
Iteration 164/1000 | Loss: 0.00003543
Iteration 165/1000 | Loss: 0.00025273
Iteration 166/1000 | Loss: 0.00020693
Iteration 167/1000 | Loss: 0.00003577
Iteration 168/1000 | Loss: 0.00026945
Iteration 169/1000 | Loss: 0.00034856
Iteration 170/1000 | Loss: 0.00026815
Iteration 171/1000 | Loss: 0.00069064
Iteration 172/1000 | Loss: 0.00061748
Iteration 173/1000 | Loss: 0.00032823
Iteration 174/1000 | Loss: 0.00071474
Iteration 175/1000 | Loss: 0.00034178
Iteration 176/1000 | Loss: 0.00025786
Iteration 177/1000 | Loss: 0.00037820
Iteration 178/1000 | Loss: 0.00022570
Iteration 179/1000 | Loss: 0.00045378
Iteration 180/1000 | Loss: 0.00036124
Iteration 181/1000 | Loss: 0.00033300
Iteration 182/1000 | Loss: 0.00031556
Iteration 183/1000 | Loss: 0.00027917
Iteration 184/1000 | Loss: 0.00031538
Iteration 185/1000 | Loss: 0.00023064
Iteration 186/1000 | Loss: 0.00019915
Iteration 187/1000 | Loss: 0.00003977
Iteration 188/1000 | Loss: 0.00003612
Iteration 189/1000 | Loss: 0.00026700
Iteration 190/1000 | Loss: 0.00028609
Iteration 191/1000 | Loss: 0.00025304
Iteration 192/1000 | Loss: 0.00025073
Iteration 193/1000 | Loss: 0.00025896
Iteration 194/1000 | Loss: 0.00010748
Iteration 195/1000 | Loss: 0.00016678
Iteration 196/1000 | Loss: 0.00017336
Iteration 197/1000 | Loss: 0.00003339
Iteration 198/1000 | Loss: 0.00003120
Iteration 199/1000 | Loss: 0.00024729
Iteration 200/1000 | Loss: 0.00014546
Iteration 201/1000 | Loss: 0.00003135
Iteration 202/1000 | Loss: 0.00024894
Iteration 203/1000 | Loss: 0.00021041
Iteration 204/1000 | Loss: 0.00003830
Iteration 205/1000 | Loss: 0.00025959
Iteration 206/1000 | Loss: 0.00015273
Iteration 207/1000 | Loss: 0.00032840
Iteration 208/1000 | Loss: 0.00013198
Iteration 209/1000 | Loss: 0.00062204
Iteration 210/1000 | Loss: 0.00046353
Iteration 211/1000 | Loss: 0.00005179
Iteration 212/1000 | Loss: 0.00003866
Iteration 213/1000 | Loss: 0.00003510
Iteration 214/1000 | Loss: 0.00026156
Iteration 215/1000 | Loss: 0.00029789
Iteration 216/1000 | Loss: 0.00003546
Iteration 217/1000 | Loss: 0.00003360
Iteration 218/1000 | Loss: 0.00003235
Iteration 219/1000 | Loss: 0.00003166
Iteration 220/1000 | Loss: 0.00003120
Iteration 221/1000 | Loss: 0.00024959
Iteration 222/1000 | Loss: 0.00018270
Iteration 223/1000 | Loss: 0.00039372
Iteration 224/1000 | Loss: 0.00018380
Iteration 225/1000 | Loss: 0.00003102
Iteration 226/1000 | Loss: 0.00025713
Iteration 227/1000 | Loss: 0.00046261
Iteration 228/1000 | Loss: 0.00005588
Iteration 229/1000 | Loss: 0.00010463
Iteration 230/1000 | Loss: 0.00003870
Iteration 231/1000 | Loss: 0.00003471
Iteration 232/1000 | Loss: 0.00003322
Iteration 233/1000 | Loss: 0.00003162
Iteration 234/1000 | Loss: 0.00003081
Iteration 235/1000 | Loss: 0.00003042
Iteration 236/1000 | Loss: 0.00003004
Iteration 237/1000 | Loss: 0.00018187
Iteration 238/1000 | Loss: 0.00013167
Iteration 239/1000 | Loss: 0.00003716
Iteration 240/1000 | Loss: 0.00003227
Iteration 241/1000 | Loss: 0.00002994
Iteration 242/1000 | Loss: 0.00002954
Iteration 243/1000 | Loss: 0.00002952
Iteration 244/1000 | Loss: 0.00019611
Iteration 245/1000 | Loss: 0.00010581
Iteration 246/1000 | Loss: 0.00002965
Iteration 247/1000 | Loss: 0.00017509
Iteration 248/1000 | Loss: 0.00017408
Iteration 249/1000 | Loss: 0.00002958
Iteration 250/1000 | Loss: 0.00016269
Iteration 251/1000 | Loss: 0.00017402
Iteration 252/1000 | Loss: 0.00002941
Iteration 253/1000 | Loss: 0.00014852
Iteration 254/1000 | Loss: 0.00017583
Iteration 255/1000 | Loss: 0.00002967
Iteration 256/1000 | Loss: 0.00002936
Iteration 257/1000 | Loss: 0.00012352
Iteration 258/1000 | Loss: 0.00061839
Iteration 259/1000 | Loss: 0.00039342
Iteration 260/1000 | Loss: 0.00010312
Iteration 261/1000 | Loss: 0.00042575
Iteration 262/1000 | Loss: 0.00036110
Iteration 263/1000 | Loss: 0.00019457
Iteration 264/1000 | Loss: 0.00021149
Iteration 265/1000 | Loss: 0.00009338
Iteration 266/1000 | Loss: 0.00003726
Iteration 267/1000 | Loss: 0.00010404
Iteration 268/1000 | Loss: 0.00015589
Iteration 269/1000 | Loss: 0.00003724
Iteration 270/1000 | Loss: 0.00003072
Iteration 271/1000 | Loss: 0.00002889
Iteration 272/1000 | Loss: 0.00002828
Iteration 273/1000 | Loss: 0.00002788
Iteration 274/1000 | Loss: 0.00002764
Iteration 275/1000 | Loss: 0.00022594
Iteration 276/1000 | Loss: 0.00003636
Iteration 277/1000 | Loss: 0.00003015
Iteration 278/1000 | Loss: 0.00002884
Iteration 279/1000 | Loss: 0.00002703
Iteration 280/1000 | Loss: 0.00002633
Iteration 281/1000 | Loss: 0.00002603
Iteration 282/1000 | Loss: 0.00002592
Iteration 283/1000 | Loss: 0.00002577
Iteration 284/1000 | Loss: 0.00002571
Iteration 285/1000 | Loss: 0.00002570
Iteration 286/1000 | Loss: 0.00002570
Iteration 287/1000 | Loss: 0.00002570
Iteration 288/1000 | Loss: 0.00002570
Iteration 289/1000 | Loss: 0.00002568
Iteration 290/1000 | Loss: 0.00002567
Iteration 291/1000 | Loss: 0.00002567
Iteration 292/1000 | Loss: 0.00002567
Iteration 293/1000 | Loss: 0.00002567
Iteration 294/1000 | Loss: 0.00002567
Iteration 295/1000 | Loss: 0.00002567
Iteration 296/1000 | Loss: 0.00002567
Iteration 297/1000 | Loss: 0.00002567
Iteration 298/1000 | Loss: 0.00002567
Iteration 299/1000 | Loss: 0.00002567
Iteration 300/1000 | Loss: 0.00002567
Iteration 301/1000 | Loss: 0.00002567
Iteration 302/1000 | Loss: 0.00002566
Iteration 303/1000 | Loss: 0.00002566
Iteration 304/1000 | Loss: 0.00002566
Iteration 305/1000 | Loss: 0.00002566
Iteration 306/1000 | Loss: 0.00002566
Iteration 307/1000 | Loss: 0.00002565
Iteration 308/1000 | Loss: 0.00002565
Iteration 309/1000 | Loss: 0.00002565
Iteration 310/1000 | Loss: 0.00002565
Iteration 311/1000 | Loss: 0.00002565
Iteration 312/1000 | Loss: 0.00002564
Iteration 313/1000 | Loss: 0.00002564
Iteration 314/1000 | Loss: 0.00002564
Iteration 315/1000 | Loss: 0.00002564
Iteration 316/1000 | Loss: 0.00002564
Iteration 317/1000 | Loss: 0.00002564
Iteration 318/1000 | Loss: 0.00002563
Iteration 319/1000 | Loss: 0.00002563
Iteration 320/1000 | Loss: 0.00002563
Iteration 321/1000 | Loss: 0.00002562
Iteration 322/1000 | Loss: 0.00002562
Iteration 323/1000 | Loss: 0.00002562
Iteration 324/1000 | Loss: 0.00002562
Iteration 325/1000 | Loss: 0.00002562
Iteration 326/1000 | Loss: 0.00002562
Iteration 327/1000 | Loss: 0.00002561
Iteration 328/1000 | Loss: 0.00002561
Iteration 329/1000 | Loss: 0.00002561
Iteration 330/1000 | Loss: 0.00002561
Iteration 331/1000 | Loss: 0.00002561
Iteration 332/1000 | Loss: 0.00002560
Iteration 333/1000 | Loss: 0.00002560
Iteration 334/1000 | Loss: 0.00002560
Iteration 335/1000 | Loss: 0.00002560
Iteration 336/1000 | Loss: 0.00002560
Iteration 337/1000 | Loss: 0.00002560
Iteration 338/1000 | Loss: 0.00002560
Iteration 339/1000 | Loss: 0.00002560
Iteration 340/1000 | Loss: 0.00002560
Iteration 341/1000 | Loss: 0.00002560
Iteration 342/1000 | Loss: 0.00002559
Iteration 343/1000 | Loss: 0.00002559
Iteration 344/1000 | Loss: 0.00002559
Iteration 345/1000 | Loss: 0.00002559
Iteration 346/1000 | Loss: 0.00002559
Iteration 347/1000 | Loss: 0.00002559
Iteration 348/1000 | Loss: 0.00002559
Iteration 349/1000 | Loss: 0.00002559
Iteration 350/1000 | Loss: 0.00002559
Iteration 351/1000 | Loss: 0.00002559
Iteration 352/1000 | Loss: 0.00002559
Iteration 353/1000 | Loss: 0.00002559
Iteration 354/1000 | Loss: 0.00002559
Iteration 355/1000 | Loss: 0.00002559
Iteration 356/1000 | Loss: 0.00002559
Iteration 357/1000 | Loss: 0.00002559
Iteration 358/1000 | Loss: 0.00002559
Iteration 359/1000 | Loss: 0.00002559
Iteration 360/1000 | Loss: 0.00002559
Iteration 361/1000 | Loss: 0.00002559
Iteration 362/1000 | Loss: 0.00002559
Iteration 363/1000 | Loss: 0.00002558
Iteration 364/1000 | Loss: 0.00002558
Iteration 365/1000 | Loss: 0.00002558
Iteration 366/1000 | Loss: 0.00002558
Iteration 367/1000 | Loss: 0.00002558
Iteration 368/1000 | Loss: 0.00002558
Iteration 369/1000 | Loss: 0.00002558
Iteration 370/1000 | Loss: 0.00002558
Iteration 371/1000 | Loss: 0.00002558
Iteration 372/1000 | Loss: 0.00002558
Iteration 373/1000 | Loss: 0.00002558
Iteration 374/1000 | Loss: 0.00002558
Iteration 375/1000 | Loss: 0.00002558
Iteration 376/1000 | Loss: 0.00002558
Iteration 377/1000 | Loss: 0.00002558
Iteration 378/1000 | Loss: 0.00002557
Iteration 379/1000 | Loss: 0.00002557
Iteration 380/1000 | Loss: 0.00002557
Iteration 381/1000 | Loss: 0.00002557
Iteration 382/1000 | Loss: 0.00002557
Iteration 383/1000 | Loss: 0.00002557
Iteration 384/1000 | Loss: 0.00002557
Iteration 385/1000 | Loss: 0.00002557
Iteration 386/1000 | Loss: 0.00002557
Iteration 387/1000 | Loss: 0.00002557
Iteration 388/1000 | Loss: 0.00002557
Iteration 389/1000 | Loss: 0.00002557
Iteration 390/1000 | Loss: 0.00002557
Iteration 391/1000 | Loss: 0.00002557
Iteration 392/1000 | Loss: 0.00002557
Iteration 393/1000 | Loss: 0.00002557
Iteration 394/1000 | Loss: 0.00002557
Iteration 395/1000 | Loss: 0.00002556
Iteration 396/1000 | Loss: 0.00002556
Iteration 397/1000 | Loss: 0.00002556
Iteration 398/1000 | Loss: 0.00002556
Iteration 399/1000 | Loss: 0.00002556
Iteration 400/1000 | Loss: 0.00002556
Iteration 401/1000 | Loss: 0.00002556
Iteration 402/1000 | Loss: 0.00002556
Iteration 403/1000 | Loss: 0.00002556
Iteration 404/1000 | Loss: 0.00002556
Iteration 405/1000 | Loss: 0.00002556
Iteration 406/1000 | Loss: 0.00002556
Iteration 407/1000 | Loss: 0.00002556
Iteration 408/1000 | Loss: 0.00002556
Iteration 409/1000 | Loss: 0.00002556
Iteration 410/1000 | Loss: 0.00002556
Iteration 411/1000 | Loss: 0.00002556
Iteration 412/1000 | Loss: 0.00002556
Iteration 413/1000 | Loss: 0.00002556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 413. Stopping optimization.
Last 5 losses: [2.5563656890881248e-05, 2.5563656890881248e-05, 2.5563656890881248e-05, 2.5563656890881248e-05, 2.5563656890881248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5563656890881248e-05

Optimization complete. Final v2v error: 4.110937595367432 mm

Highest mean error: 5.564829349517822 mm for frame 197

Lowest mean error: 3.248652696609497 mm for frame 34

Saving results

Total time: 517.9611999988556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718511
Iteration 2/25 | Loss: 0.00121209
Iteration 3/25 | Loss: 0.00111877
Iteration 4/25 | Loss: 0.00110836
Iteration 5/25 | Loss: 0.00110607
Iteration 6/25 | Loss: 0.00110607
Iteration 7/25 | Loss: 0.00110607
Iteration 8/25 | Loss: 0.00110607
Iteration 9/25 | Loss: 0.00110607
Iteration 10/25 | Loss: 0.00110607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011060689575970173, 0.0011060689575970173, 0.0011060689575970173, 0.0011060689575970173, 0.0011060689575970173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011060689575970173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48091495
Iteration 2/25 | Loss: 0.00079889
Iteration 3/25 | Loss: 0.00079888
Iteration 4/25 | Loss: 0.00079888
Iteration 5/25 | Loss: 0.00079888
Iteration 6/25 | Loss: 0.00079888
Iteration 7/25 | Loss: 0.00079888
Iteration 8/25 | Loss: 0.00079888
Iteration 9/25 | Loss: 0.00079888
Iteration 10/25 | Loss: 0.00079888
Iteration 11/25 | Loss: 0.00079888
Iteration 12/25 | Loss: 0.00079888
Iteration 13/25 | Loss: 0.00079888
Iteration 14/25 | Loss: 0.00079888
Iteration 15/25 | Loss: 0.00079888
Iteration 16/25 | Loss: 0.00079888
Iteration 17/25 | Loss: 0.00079888
Iteration 18/25 | Loss: 0.00079888
Iteration 19/25 | Loss: 0.00079888
Iteration 20/25 | Loss: 0.00079888
Iteration 21/25 | Loss: 0.00079888
Iteration 22/25 | Loss: 0.00079888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000798876746557653, 0.000798876746557653, 0.000798876746557653, 0.000798876746557653, 0.000798876746557653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000798876746557653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079888
Iteration 2/1000 | Loss: 0.00002285
Iteration 3/1000 | Loss: 0.00001398
Iteration 4/1000 | Loss: 0.00001256
Iteration 5/1000 | Loss: 0.00001191
Iteration 6/1000 | Loss: 0.00001150
Iteration 7/1000 | Loss: 0.00001121
Iteration 8/1000 | Loss: 0.00001103
Iteration 9/1000 | Loss: 0.00001079
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001057
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001027
Iteration 15/1000 | Loss: 0.00001023
Iteration 16/1000 | Loss: 0.00001020
Iteration 17/1000 | Loss: 0.00001016
Iteration 18/1000 | Loss: 0.00001016
Iteration 19/1000 | Loss: 0.00001014
Iteration 20/1000 | Loss: 0.00001014
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001013
Iteration 23/1000 | Loss: 0.00001012
Iteration 24/1000 | Loss: 0.00001012
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001008
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001006
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001005
Iteration 37/1000 | Loss: 0.00001003
Iteration 38/1000 | Loss: 0.00001003
Iteration 39/1000 | Loss: 0.00001002
Iteration 40/1000 | Loss: 0.00001002
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00001000
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000998
Iteration 45/1000 | Loss: 0.00000998
Iteration 46/1000 | Loss: 0.00000998
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000997
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00000997
Iteration 53/1000 | Loss: 0.00000997
Iteration 54/1000 | Loss: 0.00000996
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000993
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000993
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000992
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000990
Iteration 69/1000 | Loss: 0.00000990
Iteration 70/1000 | Loss: 0.00000989
Iteration 71/1000 | Loss: 0.00000989
Iteration 72/1000 | Loss: 0.00000989
Iteration 73/1000 | Loss: 0.00000989
Iteration 74/1000 | Loss: 0.00000989
Iteration 75/1000 | Loss: 0.00000988
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000985
Iteration 81/1000 | Loss: 0.00000985
Iteration 82/1000 | Loss: 0.00000984
Iteration 83/1000 | Loss: 0.00000984
Iteration 84/1000 | Loss: 0.00000983
Iteration 85/1000 | Loss: 0.00000983
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000982
Iteration 90/1000 | Loss: 0.00000982
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000979
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000975
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000975
Iteration 128/1000 | Loss: 0.00000975
Iteration 129/1000 | Loss: 0.00000975
Iteration 130/1000 | Loss: 0.00000975
Iteration 131/1000 | Loss: 0.00000975
Iteration 132/1000 | Loss: 0.00000975
Iteration 133/1000 | Loss: 0.00000975
Iteration 134/1000 | Loss: 0.00000975
Iteration 135/1000 | Loss: 0.00000975
Iteration 136/1000 | Loss: 0.00000975
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Iteration 140/1000 | Loss: 0.00000974
Iteration 141/1000 | Loss: 0.00000974
Iteration 142/1000 | Loss: 0.00000974
Iteration 143/1000 | Loss: 0.00000974
Iteration 144/1000 | Loss: 0.00000974
Iteration 145/1000 | Loss: 0.00000974
Iteration 146/1000 | Loss: 0.00000974
Iteration 147/1000 | Loss: 0.00000974
Iteration 148/1000 | Loss: 0.00000974
Iteration 149/1000 | Loss: 0.00000974
Iteration 150/1000 | Loss: 0.00000974
Iteration 151/1000 | Loss: 0.00000974
Iteration 152/1000 | Loss: 0.00000973
Iteration 153/1000 | Loss: 0.00000973
Iteration 154/1000 | Loss: 0.00000973
Iteration 155/1000 | Loss: 0.00000973
Iteration 156/1000 | Loss: 0.00000973
Iteration 157/1000 | Loss: 0.00000973
Iteration 158/1000 | Loss: 0.00000973
Iteration 159/1000 | Loss: 0.00000973
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000973
Iteration 165/1000 | Loss: 0.00000973
Iteration 166/1000 | Loss: 0.00000973
Iteration 167/1000 | Loss: 0.00000973
Iteration 168/1000 | Loss: 0.00000973
Iteration 169/1000 | Loss: 0.00000973
Iteration 170/1000 | Loss: 0.00000972
Iteration 171/1000 | Loss: 0.00000972
Iteration 172/1000 | Loss: 0.00000972
Iteration 173/1000 | Loss: 0.00000972
Iteration 174/1000 | Loss: 0.00000972
Iteration 175/1000 | Loss: 0.00000972
Iteration 176/1000 | Loss: 0.00000972
Iteration 177/1000 | Loss: 0.00000972
Iteration 178/1000 | Loss: 0.00000972
Iteration 179/1000 | Loss: 0.00000972
Iteration 180/1000 | Loss: 0.00000972
Iteration 181/1000 | Loss: 0.00000972
Iteration 182/1000 | Loss: 0.00000972
Iteration 183/1000 | Loss: 0.00000972
Iteration 184/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [9.721675269247498e-06, 9.721675269247498e-06, 9.721675269247498e-06, 9.721675269247498e-06, 9.721675269247498e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.721675269247498e-06

Optimization complete. Final v2v error: 2.674776554107666 mm

Highest mean error: 2.987819194793701 mm for frame 20

Lowest mean error: 2.422963857650757 mm for frame 47

Saving results

Total time: 41.378326416015625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801496
Iteration 2/25 | Loss: 0.00132819
Iteration 3/25 | Loss: 0.00110068
Iteration 4/25 | Loss: 0.00108527
Iteration 5/25 | Loss: 0.00108002
Iteration 6/25 | Loss: 0.00107825
Iteration 7/25 | Loss: 0.00107809
Iteration 8/25 | Loss: 0.00107809
Iteration 9/25 | Loss: 0.00107809
Iteration 10/25 | Loss: 0.00107809
Iteration 11/25 | Loss: 0.00107809
Iteration 12/25 | Loss: 0.00107809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010780941229313612, 0.0010780941229313612, 0.0010780941229313612, 0.0010780941229313612, 0.0010780941229313612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010780941229313612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17357147
Iteration 2/25 | Loss: 0.00092175
Iteration 3/25 | Loss: 0.00092174
Iteration 4/25 | Loss: 0.00092174
Iteration 5/25 | Loss: 0.00092174
Iteration 6/25 | Loss: 0.00092174
Iteration 7/25 | Loss: 0.00092174
Iteration 8/25 | Loss: 0.00092174
Iteration 9/25 | Loss: 0.00092174
Iteration 10/25 | Loss: 0.00092174
Iteration 11/25 | Loss: 0.00092174
Iteration 12/25 | Loss: 0.00092174
Iteration 13/25 | Loss: 0.00092174
Iteration 14/25 | Loss: 0.00092174
Iteration 15/25 | Loss: 0.00092174
Iteration 16/25 | Loss: 0.00092174
Iteration 17/25 | Loss: 0.00092174
Iteration 18/25 | Loss: 0.00092174
Iteration 19/25 | Loss: 0.00092174
Iteration 20/25 | Loss: 0.00092174
Iteration 21/25 | Loss: 0.00092174
Iteration 22/25 | Loss: 0.00092174
Iteration 23/25 | Loss: 0.00092174
Iteration 24/25 | Loss: 0.00092174
Iteration 25/25 | Loss: 0.00092174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092174
Iteration 2/1000 | Loss: 0.00004299
Iteration 3/1000 | Loss: 0.00002624
Iteration 4/1000 | Loss: 0.00001923
Iteration 5/1000 | Loss: 0.00001700
Iteration 6/1000 | Loss: 0.00001566
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001229
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001220
Iteration 37/1000 | Loss: 0.00001219
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001212
Iteration 54/1000 | Loss: 0.00001212
Iteration 55/1000 | Loss: 0.00001212
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001206
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001205
Iteration 65/1000 | Loss: 0.00001203
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001202
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001199
Iteration 91/1000 | Loss: 0.00001199
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001198
Iteration 99/1000 | Loss: 0.00001198
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001193
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001191
Iteration 145/1000 | Loss: 0.00001191
Iteration 146/1000 | Loss: 0.00001191
Iteration 147/1000 | Loss: 0.00001191
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Iteration 167/1000 | Loss: 0.00001191
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Iteration 170/1000 | Loss: 0.00001191
Iteration 171/1000 | Loss: 0.00001191
Iteration 172/1000 | Loss: 0.00001191
Iteration 173/1000 | Loss: 0.00001191
Iteration 174/1000 | Loss: 0.00001191
Iteration 175/1000 | Loss: 0.00001191
Iteration 176/1000 | Loss: 0.00001191
Iteration 177/1000 | Loss: 0.00001191
Iteration 178/1000 | Loss: 0.00001191
Iteration 179/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1909603927051648e-05, 1.1909603927051648e-05, 1.1909603927051648e-05, 1.1909603927051648e-05, 1.1909603927051648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1909603927051648e-05

Optimization complete. Final v2v error: 2.8191511631011963 mm

Highest mean error: 4.146120548248291 mm for frame 62

Lowest mean error: 2.3081870079040527 mm for frame 91

Saving results

Total time: 43.45127248764038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425054
Iteration 2/25 | Loss: 0.00118430
Iteration 3/25 | Loss: 0.00111192
Iteration 4/25 | Loss: 0.00109326
Iteration 5/25 | Loss: 0.00108707
Iteration 6/25 | Loss: 0.00108592
Iteration 7/25 | Loss: 0.00108583
Iteration 8/25 | Loss: 0.00108583
Iteration 9/25 | Loss: 0.00108583
Iteration 10/25 | Loss: 0.00108583
Iteration 11/25 | Loss: 0.00108583
Iteration 12/25 | Loss: 0.00108583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010858310852199793, 0.0010858310852199793, 0.0010858310852199793, 0.0010858310852199793, 0.0010858310852199793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010858310852199793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38671553
Iteration 2/25 | Loss: 0.00080057
Iteration 3/25 | Loss: 0.00080057
Iteration 4/25 | Loss: 0.00080057
Iteration 5/25 | Loss: 0.00080056
Iteration 6/25 | Loss: 0.00080056
Iteration 7/25 | Loss: 0.00080056
Iteration 8/25 | Loss: 0.00080056
Iteration 9/25 | Loss: 0.00080056
Iteration 10/25 | Loss: 0.00080056
Iteration 11/25 | Loss: 0.00080056
Iteration 12/25 | Loss: 0.00080056
Iteration 13/25 | Loss: 0.00080056
Iteration 14/25 | Loss: 0.00080056
Iteration 15/25 | Loss: 0.00080056
Iteration 16/25 | Loss: 0.00080056
Iteration 17/25 | Loss: 0.00080056
Iteration 18/25 | Loss: 0.00080056
Iteration 19/25 | Loss: 0.00080056
Iteration 20/25 | Loss: 0.00080056
Iteration 21/25 | Loss: 0.00080056
Iteration 22/25 | Loss: 0.00080056
Iteration 23/25 | Loss: 0.00080056
Iteration 24/25 | Loss: 0.00080056
Iteration 25/25 | Loss: 0.00080056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080056
Iteration 2/1000 | Loss: 0.00002539
Iteration 3/1000 | Loss: 0.00001568
Iteration 4/1000 | Loss: 0.00001454
Iteration 5/1000 | Loss: 0.00001396
Iteration 6/1000 | Loss: 0.00001357
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00001308
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001237
Iteration 15/1000 | Loss: 0.00001229
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001201
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001199
Iteration 36/1000 | Loss: 0.00001199
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001197
Iteration 43/1000 | Loss: 0.00001197
Iteration 44/1000 | Loss: 0.00001197
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001196
Iteration 47/1000 | Loss: 0.00001196
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001191
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001190
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001189
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001186
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001185
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001183
Iteration 86/1000 | Loss: 0.00001183
Iteration 87/1000 | Loss: 0.00001183
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001176
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001176
Iteration 138/1000 | Loss: 0.00001176
Iteration 139/1000 | Loss: 0.00001176
Iteration 140/1000 | Loss: 0.00001176
Iteration 141/1000 | Loss: 0.00001176
Iteration 142/1000 | Loss: 0.00001176
Iteration 143/1000 | Loss: 0.00001176
Iteration 144/1000 | Loss: 0.00001176
Iteration 145/1000 | Loss: 0.00001176
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001176
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001176
Iteration 168/1000 | Loss: 0.00001176
Iteration 169/1000 | Loss: 0.00001176
Iteration 170/1000 | Loss: 0.00001176
Iteration 171/1000 | Loss: 0.00001176
Iteration 172/1000 | Loss: 0.00001176
Iteration 173/1000 | Loss: 0.00001176
Iteration 174/1000 | Loss: 0.00001176
Iteration 175/1000 | Loss: 0.00001176
Iteration 176/1000 | Loss: 0.00001176
Iteration 177/1000 | Loss: 0.00001176
Iteration 178/1000 | Loss: 0.00001176
Iteration 179/1000 | Loss: 0.00001176
Iteration 180/1000 | Loss: 0.00001176
Iteration 181/1000 | Loss: 0.00001176
Iteration 182/1000 | Loss: 0.00001176
Iteration 183/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1764526789193042e-05, 1.1764526789193042e-05, 1.1764526789193042e-05, 1.1764526789193042e-05, 1.1764526789193042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1764526789193042e-05

Optimization complete. Final v2v error: 2.964573860168457 mm

Highest mean error: 3.1445975303649902 mm for frame 91

Lowest mean error: 2.665506601333618 mm for frame 128

Saving results

Total time: 39.03552031517029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822274
Iteration 2/25 | Loss: 0.00140944
Iteration 3/25 | Loss: 0.00115744
Iteration 4/25 | Loss: 0.00113320
Iteration 5/25 | Loss: 0.00113079
Iteration 6/25 | Loss: 0.00113067
Iteration 7/25 | Loss: 0.00113067
Iteration 8/25 | Loss: 0.00113067
Iteration 9/25 | Loss: 0.00113067
Iteration 10/25 | Loss: 0.00113067
Iteration 11/25 | Loss: 0.00113067
Iteration 12/25 | Loss: 0.00113067
Iteration 13/25 | Loss: 0.00113067
Iteration 14/25 | Loss: 0.00113067
Iteration 15/25 | Loss: 0.00113067
Iteration 16/25 | Loss: 0.00113067
Iteration 17/25 | Loss: 0.00113067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001130668562836945, 0.001130668562836945, 0.001130668562836945, 0.001130668562836945, 0.001130668562836945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001130668562836945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96626157
Iteration 2/25 | Loss: 0.00042566
Iteration 3/25 | Loss: 0.00042565
Iteration 4/25 | Loss: 0.00042564
Iteration 5/25 | Loss: 0.00042564
Iteration 6/25 | Loss: 0.00042564
Iteration 7/25 | Loss: 0.00042564
Iteration 8/25 | Loss: 0.00042564
Iteration 9/25 | Loss: 0.00042564
Iteration 10/25 | Loss: 0.00042564
Iteration 11/25 | Loss: 0.00042564
Iteration 12/25 | Loss: 0.00042564
Iteration 13/25 | Loss: 0.00042564
Iteration 14/25 | Loss: 0.00042564
Iteration 15/25 | Loss: 0.00042564
Iteration 16/25 | Loss: 0.00042564
Iteration 17/25 | Loss: 0.00042564
Iteration 18/25 | Loss: 0.00042564
Iteration 19/25 | Loss: 0.00042564
Iteration 20/25 | Loss: 0.00042564
Iteration 21/25 | Loss: 0.00042564
Iteration 22/25 | Loss: 0.00042564
Iteration 23/25 | Loss: 0.00042564
Iteration 24/25 | Loss: 0.00042564
Iteration 25/25 | Loss: 0.00042564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042564
Iteration 2/1000 | Loss: 0.00002991
Iteration 3/1000 | Loss: 0.00002126
Iteration 4/1000 | Loss: 0.00001881
Iteration 5/1000 | Loss: 0.00001798
Iteration 6/1000 | Loss: 0.00001757
Iteration 7/1000 | Loss: 0.00001705
Iteration 8/1000 | Loss: 0.00001672
Iteration 9/1000 | Loss: 0.00001652
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001623
Iteration 12/1000 | Loss: 0.00001622
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001621
Iteration 15/1000 | Loss: 0.00001620
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001613
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001605
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001594
Iteration 27/1000 | Loss: 0.00001594
Iteration 28/1000 | Loss: 0.00001593
Iteration 29/1000 | Loss: 0.00001593
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001585
Iteration 32/1000 | Loss: 0.00001583
Iteration 33/1000 | Loss: 0.00001582
Iteration 34/1000 | Loss: 0.00001582
Iteration 35/1000 | Loss: 0.00001582
Iteration 36/1000 | Loss: 0.00001582
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001582
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001582
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001582
Iteration 43/1000 | Loss: 0.00001582
Iteration 44/1000 | Loss: 0.00001582
Iteration 45/1000 | Loss: 0.00001582
Iteration 46/1000 | Loss: 0.00001582
Iteration 47/1000 | Loss: 0.00001582
Iteration 48/1000 | Loss: 0.00001582
Iteration 49/1000 | Loss: 0.00001582
Iteration 50/1000 | Loss: 0.00001582
Iteration 51/1000 | Loss: 0.00001582
Iteration 52/1000 | Loss: 0.00001582
Iteration 53/1000 | Loss: 0.00001582
Iteration 54/1000 | Loss: 0.00001582
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001582
Iteration 57/1000 | Loss: 0.00001582
Iteration 58/1000 | Loss: 0.00001582
Iteration 59/1000 | Loss: 0.00001582
Iteration 60/1000 | Loss: 0.00001582
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001582
Iteration 63/1000 | Loss: 0.00001582
Iteration 64/1000 | Loss: 0.00001581
Iteration 65/1000 | Loss: 0.00001581
Iteration 66/1000 | Loss: 0.00001581
Iteration 67/1000 | Loss: 0.00001581
Iteration 68/1000 | Loss: 0.00001581
Iteration 69/1000 | Loss: 0.00001581
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001581
Iteration 72/1000 | Loss: 0.00001581
Iteration 73/1000 | Loss: 0.00001581
Iteration 74/1000 | Loss: 0.00001581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.581468495714944e-05, 1.581468495714944e-05, 1.581468495714944e-05, 1.581468495714944e-05, 1.581468495714944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.581468495714944e-05

Optimization complete. Final v2v error: 3.3451361656188965 mm

Highest mean error: 3.685171365737915 mm for frame 23

Lowest mean error: 3.161989212036133 mm for frame 142

Saving results

Total time: 30.75085735321045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810977
Iteration 2/25 | Loss: 0.00113889
Iteration 3/25 | Loss: 0.00107450
Iteration 4/25 | Loss: 0.00105304
Iteration 5/25 | Loss: 0.00104592
Iteration 6/25 | Loss: 0.00104512
Iteration 7/25 | Loss: 0.00104512
Iteration 8/25 | Loss: 0.00104512
Iteration 9/25 | Loss: 0.00104512
Iteration 10/25 | Loss: 0.00104512
Iteration 11/25 | Loss: 0.00104512
Iteration 12/25 | Loss: 0.00104512
Iteration 13/25 | Loss: 0.00104512
Iteration 14/25 | Loss: 0.00104512
Iteration 15/25 | Loss: 0.00104512
Iteration 16/25 | Loss: 0.00104512
Iteration 17/25 | Loss: 0.00104512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010451199486851692, 0.0010451199486851692, 0.0010451199486851692, 0.0010451199486851692, 0.0010451199486851692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010451199486851692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.36626101
Iteration 2/25 | Loss: 0.00118327
Iteration 3/25 | Loss: 0.00118322
Iteration 4/25 | Loss: 0.00118322
Iteration 5/25 | Loss: 0.00118322
Iteration 6/25 | Loss: 0.00118322
Iteration 7/25 | Loss: 0.00118322
Iteration 8/25 | Loss: 0.00118321
Iteration 9/25 | Loss: 0.00118321
Iteration 10/25 | Loss: 0.00118321
Iteration 11/25 | Loss: 0.00118321
Iteration 12/25 | Loss: 0.00118321
Iteration 13/25 | Loss: 0.00118321
Iteration 14/25 | Loss: 0.00118321
Iteration 15/25 | Loss: 0.00118321
Iteration 16/25 | Loss: 0.00118321
Iteration 17/25 | Loss: 0.00118321
Iteration 18/25 | Loss: 0.00118321
Iteration 19/25 | Loss: 0.00118321
Iteration 20/25 | Loss: 0.00118321
Iteration 21/25 | Loss: 0.00118321
Iteration 22/25 | Loss: 0.00118321
Iteration 23/25 | Loss: 0.00118321
Iteration 24/25 | Loss: 0.00118321
Iteration 25/25 | Loss: 0.00118321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118321
Iteration 2/1000 | Loss: 0.00003158
Iteration 3/1000 | Loss: 0.00001912
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001476
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001369
Iteration 8/1000 | Loss: 0.00001320
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001271
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001217
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001197
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001188
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001174
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001158
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001158
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001158
Iteration 104/1000 | Loss: 0.00001158
Iteration 105/1000 | Loss: 0.00001158
Iteration 106/1000 | Loss: 0.00001158
Iteration 107/1000 | Loss: 0.00001158
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.157756196334958e-05, 1.157756196334958e-05, 1.157756196334958e-05, 1.157756196334958e-05, 1.157756196334958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.157756196334958e-05

Optimization complete. Final v2v error: 2.910167932510376 mm

Highest mean error: 3.4044628143310547 mm for frame 56

Lowest mean error: 2.536334753036499 mm for frame 225

Saving results

Total time: 39.89681029319763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967554
Iteration 2/25 | Loss: 0.00967554
Iteration 3/25 | Loss: 0.00967554
Iteration 4/25 | Loss: 0.00967554
Iteration 5/25 | Loss: 0.00967554
Iteration 6/25 | Loss: 0.00967554
Iteration 7/25 | Loss: 0.00967554
Iteration 8/25 | Loss: 0.00967554
Iteration 9/25 | Loss: 0.00967554
Iteration 10/25 | Loss: 0.00967554
Iteration 11/25 | Loss: 0.00967554
Iteration 12/25 | Loss: 0.00967553
Iteration 13/25 | Loss: 0.00967553
Iteration 14/25 | Loss: 0.00967553
Iteration 15/25 | Loss: 0.00967553
Iteration 16/25 | Loss: 0.00967553
Iteration 17/25 | Loss: 0.00967553
Iteration 18/25 | Loss: 0.00967553
Iteration 19/25 | Loss: 0.00967553
Iteration 20/25 | Loss: 0.00967553
Iteration 21/25 | Loss: 0.00967553
Iteration 22/25 | Loss: 0.00967552
Iteration 23/25 | Loss: 0.00967552
Iteration 24/25 | Loss: 0.00967552
Iteration 25/25 | Loss: 0.00967552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66188204
Iteration 2/25 | Loss: 0.15560600
Iteration 3/25 | Loss: 0.14739791
Iteration 4/25 | Loss: 0.14583166
Iteration 5/25 | Loss: 0.14566040
Iteration 6/25 | Loss: 0.14654513
Iteration 7/25 | Loss: 0.14549054
Iteration 8/25 | Loss: 0.14555439
Iteration 9/25 | Loss: 0.14544111
Iteration 10/25 | Loss: 0.14544106
Iteration 11/25 | Loss: 0.14544106
Iteration 12/25 | Loss: 0.14544106
Iteration 13/25 | Loss: 0.14544104
Iteration 14/25 | Loss: 0.14544103
Iteration 15/25 | Loss: 0.14544103
Iteration 16/25 | Loss: 0.14544104
Iteration 17/25 | Loss: 0.14544104
Iteration 18/25 | Loss: 0.14544101
Iteration 19/25 | Loss: 0.14544101
Iteration 20/25 | Loss: 0.14544101
Iteration 21/25 | Loss: 0.14544101
Iteration 22/25 | Loss: 0.14544101
Iteration 23/25 | Loss: 0.14544101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.14544101059436798, 0.14544101059436798, 0.14544101059436798, 0.14544101059436798, 0.14544101059436798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14544101059436798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14544101
Iteration 2/1000 | Loss: 0.00955175
Iteration 3/1000 | Loss: 0.00283845
Iteration 4/1000 | Loss: 0.00195883
Iteration 5/1000 | Loss: 0.00229667
Iteration 6/1000 | Loss: 0.00038400
Iteration 7/1000 | Loss: 0.00048371
Iteration 8/1000 | Loss: 0.00146627
Iteration 9/1000 | Loss: 0.00060265
Iteration 10/1000 | Loss: 0.00090637
Iteration 11/1000 | Loss: 0.00046364
Iteration 12/1000 | Loss: 0.00007340
Iteration 13/1000 | Loss: 0.00009292
Iteration 14/1000 | Loss: 0.00029859
Iteration 15/1000 | Loss: 0.00093411
Iteration 16/1000 | Loss: 0.00312241
Iteration 17/1000 | Loss: 0.00007569
Iteration 18/1000 | Loss: 0.00010898
Iteration 19/1000 | Loss: 0.00062421
Iteration 20/1000 | Loss: 0.00005209
Iteration 21/1000 | Loss: 0.00004169
Iteration 22/1000 | Loss: 0.00044115
Iteration 23/1000 | Loss: 0.00139466
Iteration 24/1000 | Loss: 0.00015880
Iteration 25/1000 | Loss: 0.00029502
Iteration 26/1000 | Loss: 0.00076193
Iteration 27/1000 | Loss: 0.00004684
Iteration 28/1000 | Loss: 0.00005706
Iteration 29/1000 | Loss: 0.00002455
Iteration 30/1000 | Loss: 0.00010827
Iteration 31/1000 | Loss: 0.00103881
Iteration 32/1000 | Loss: 0.00284429
Iteration 33/1000 | Loss: 0.00034042
Iteration 34/1000 | Loss: 0.00043195
Iteration 35/1000 | Loss: 0.00022521
Iteration 36/1000 | Loss: 0.00013250
Iteration 37/1000 | Loss: 0.00009582
Iteration 38/1000 | Loss: 0.00002137
Iteration 39/1000 | Loss: 0.00002054
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00029521
Iteration 42/1000 | Loss: 0.00002479
Iteration 43/1000 | Loss: 0.00027781
Iteration 44/1000 | Loss: 0.00001936
Iteration 45/1000 | Loss: 0.00001899
Iteration 46/1000 | Loss: 0.00021416
Iteration 47/1000 | Loss: 0.00208910
Iteration 48/1000 | Loss: 0.00119118
Iteration 49/1000 | Loss: 0.00089070
Iteration 50/1000 | Loss: 0.00044521
Iteration 51/1000 | Loss: 0.00057270
Iteration 52/1000 | Loss: 0.00010280
Iteration 53/1000 | Loss: 0.00098553
Iteration 54/1000 | Loss: 0.00161443
Iteration 55/1000 | Loss: 0.00045711
Iteration 56/1000 | Loss: 0.00010299
Iteration 57/1000 | Loss: 0.00060919
Iteration 58/1000 | Loss: 0.00002289
Iteration 59/1000 | Loss: 0.00002027
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001789
Iteration 64/1000 | Loss: 0.00001788
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001785
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001784
Iteration 70/1000 | Loss: 0.00001783
Iteration 71/1000 | Loss: 0.00001783
Iteration 72/1000 | Loss: 0.00001783
Iteration 73/1000 | Loss: 0.00001782
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001781
Iteration 76/1000 | Loss: 0.00001780
Iteration 77/1000 | Loss: 0.00001780
Iteration 78/1000 | Loss: 0.00001779
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001766
Iteration 84/1000 | Loss: 0.00001766
Iteration 85/1000 | Loss: 0.00001765
Iteration 86/1000 | Loss: 0.00001765
Iteration 87/1000 | Loss: 0.00001764
Iteration 88/1000 | Loss: 0.00001764
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001762
Iteration 92/1000 | Loss: 0.00001762
Iteration 93/1000 | Loss: 0.00001761
Iteration 94/1000 | Loss: 0.00001760
Iteration 95/1000 | Loss: 0.00001760
Iteration 96/1000 | Loss: 0.00001759
Iteration 97/1000 | Loss: 0.00001759
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001755
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001753
Iteration 111/1000 | Loss: 0.00001753
Iteration 112/1000 | Loss: 0.00001753
Iteration 113/1000 | Loss: 0.00001752
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001751
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00005925
Iteration 120/1000 | Loss: 0.00006829
Iteration 121/1000 | Loss: 0.00008088
Iteration 122/1000 | Loss: 0.00008819
Iteration 123/1000 | Loss: 0.00006260
Iteration 124/1000 | Loss: 0.00003891
Iteration 125/1000 | Loss: 0.00001765
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001748
Iteration 128/1000 | Loss: 0.00001898
Iteration 129/1000 | Loss: 0.00001751
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001748
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001747
Iteration 138/1000 | Loss: 0.00001747
Iteration 139/1000 | Loss: 0.00001746
Iteration 140/1000 | Loss: 0.00001746
Iteration 141/1000 | Loss: 0.00001745
Iteration 142/1000 | Loss: 0.00001745
Iteration 143/1000 | Loss: 0.00001745
Iteration 144/1000 | Loss: 0.00001745
Iteration 145/1000 | Loss: 0.00001745
Iteration 146/1000 | Loss: 0.00001745
Iteration 147/1000 | Loss: 0.00001745
Iteration 148/1000 | Loss: 0.00001744
Iteration 149/1000 | Loss: 0.00001744
Iteration 150/1000 | Loss: 0.00001744
Iteration 151/1000 | Loss: 0.00001744
Iteration 152/1000 | Loss: 0.00001759
Iteration 153/1000 | Loss: 0.00001759
Iteration 154/1000 | Loss: 0.00001752
Iteration 155/1000 | Loss: 0.00001743
Iteration 156/1000 | Loss: 0.00001742
Iteration 157/1000 | Loss: 0.00001742
Iteration 158/1000 | Loss: 0.00001742
Iteration 159/1000 | Loss: 0.00001742
Iteration 160/1000 | Loss: 0.00001757
Iteration 161/1000 | Loss: 0.00001742
Iteration 162/1000 | Loss: 0.00001742
Iteration 163/1000 | Loss: 0.00001741
Iteration 164/1000 | Loss: 0.00001741
Iteration 165/1000 | Loss: 0.00001741
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001741
Iteration 168/1000 | Loss: 0.00001741
Iteration 169/1000 | Loss: 0.00001741
Iteration 170/1000 | Loss: 0.00001741
Iteration 171/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.7413529349141754e-05, 1.7413529349141754e-05, 1.7413529349141754e-05, 1.7413529349141754e-05, 1.7413529349141754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7413529349141754e-05

Optimization complete. Final v2v error: 3.5683186054229736 mm

Highest mean error: 4.960256576538086 mm for frame 175

Lowest mean error: 3.218214273452759 mm for frame 85

Saving results

Total time: 142.40985941886902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040444
Iteration 2/25 | Loss: 0.00162333
Iteration 3/25 | Loss: 0.00149308
Iteration 4/25 | Loss: 0.00127469
Iteration 5/25 | Loss: 0.00124477
Iteration 6/25 | Loss: 0.00122171
Iteration 7/25 | Loss: 0.00119008
Iteration 8/25 | Loss: 0.00117830
Iteration 9/25 | Loss: 0.00117373
Iteration 10/25 | Loss: 0.00116794
Iteration 11/25 | Loss: 0.00116647
Iteration 12/25 | Loss: 0.00116616
Iteration 13/25 | Loss: 0.00116595
Iteration 14/25 | Loss: 0.00116583
Iteration 15/25 | Loss: 0.00116790
Iteration 16/25 | Loss: 0.00118032
Iteration 17/25 | Loss: 0.00116848
Iteration 18/25 | Loss: 0.00116159
Iteration 19/25 | Loss: 0.00115887
Iteration 20/25 | Loss: 0.00115841
Iteration 21/25 | Loss: 0.00115783
Iteration 22/25 | Loss: 0.00115749
Iteration 23/25 | Loss: 0.00115737
Iteration 24/25 | Loss: 0.00115737
Iteration 25/25 | Loss: 0.00115736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61117077
Iteration 2/25 | Loss: 0.00108188
Iteration 3/25 | Loss: 0.00101386
Iteration 4/25 | Loss: 0.00101386
Iteration 5/25 | Loss: 0.00101385
Iteration 6/25 | Loss: 0.00101385
Iteration 7/25 | Loss: 0.00101385
Iteration 8/25 | Loss: 0.00101385
Iteration 9/25 | Loss: 0.00101385
Iteration 10/25 | Loss: 0.00101385
Iteration 11/25 | Loss: 0.00101385
Iteration 12/25 | Loss: 0.00101385
Iteration 13/25 | Loss: 0.00101385
Iteration 14/25 | Loss: 0.00101385
Iteration 15/25 | Loss: 0.00101385
Iteration 16/25 | Loss: 0.00101385
Iteration 17/25 | Loss: 0.00101385
Iteration 18/25 | Loss: 0.00101385
Iteration 19/25 | Loss: 0.00101385
Iteration 20/25 | Loss: 0.00101385
Iteration 21/25 | Loss: 0.00101385
Iteration 22/25 | Loss: 0.00101385
Iteration 23/25 | Loss: 0.00101385
Iteration 24/25 | Loss: 0.00101385
Iteration 25/25 | Loss: 0.00101385
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010138507932424545, 0.0010138507932424545, 0.0010138507932424545, 0.0010138507932424545, 0.0010138507932424545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010138507932424545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101385
Iteration 2/1000 | Loss: 0.00008841
Iteration 3/1000 | Loss: 0.00002492
Iteration 4/1000 | Loss: 0.00005809
Iteration 5/1000 | Loss: 0.00001930
Iteration 6/1000 | Loss: 0.00001749
Iteration 7/1000 | Loss: 0.00017173
Iteration 8/1000 | Loss: 0.00007815
Iteration 9/1000 | Loss: 0.00009730
Iteration 10/1000 | Loss: 0.00014386
Iteration 11/1000 | Loss: 0.00004821
Iteration 12/1000 | Loss: 0.00001684
Iteration 13/1000 | Loss: 0.00001661
Iteration 14/1000 | Loss: 0.00001648
Iteration 15/1000 | Loss: 0.00006791
Iteration 16/1000 | Loss: 0.00005420
Iteration 17/1000 | Loss: 0.00001625
Iteration 18/1000 | Loss: 0.00001605
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001578
Iteration 21/1000 | Loss: 0.00007673
Iteration 22/1000 | Loss: 0.00008835
Iteration 23/1000 | Loss: 0.00001708
Iteration 24/1000 | Loss: 0.00002318
Iteration 25/1000 | Loss: 0.00002698
Iteration 26/1000 | Loss: 0.00001556
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00001540
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001532
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00007955
Iteration 39/1000 | Loss: 0.00001744
Iteration 40/1000 | Loss: 0.00005950
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001814
Iteration 43/1000 | Loss: 0.00001523
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00003642
Iteration 54/1000 | Loss: 0.00001931
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001519
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001517
Iteration 68/1000 | Loss: 0.00001517
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001517
Iteration 71/1000 | Loss: 0.00001517
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001517
Iteration 81/1000 | Loss: 0.00001517
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00003328
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001511
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001510
Iteration 92/1000 | Loss: 0.00001510
Iteration 93/1000 | Loss: 0.00001509
Iteration 94/1000 | Loss: 0.00001509
Iteration 95/1000 | Loss: 0.00001509
Iteration 96/1000 | Loss: 0.00001509
Iteration 97/1000 | Loss: 0.00001509
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001507
Iteration 108/1000 | Loss: 0.00001507
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001506
Iteration 113/1000 | Loss: 0.00001506
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001505
Iteration 117/1000 | Loss: 0.00001505
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001504
Iteration 120/1000 | Loss: 0.00001504
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001502
Iteration 128/1000 | Loss: 0.00001502
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001501
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001501
Iteration 137/1000 | Loss: 0.00001501
Iteration 138/1000 | Loss: 0.00001501
Iteration 139/1000 | Loss: 0.00001501
Iteration 140/1000 | Loss: 0.00001501
Iteration 141/1000 | Loss: 0.00001501
Iteration 142/1000 | Loss: 0.00001501
Iteration 143/1000 | Loss: 0.00001501
Iteration 144/1000 | Loss: 0.00001501
Iteration 145/1000 | Loss: 0.00001500
Iteration 146/1000 | Loss: 0.00001500
Iteration 147/1000 | Loss: 0.00001500
Iteration 148/1000 | Loss: 0.00001500
Iteration 149/1000 | Loss: 0.00001500
Iteration 150/1000 | Loss: 0.00001500
Iteration 151/1000 | Loss: 0.00001500
Iteration 152/1000 | Loss: 0.00001500
Iteration 153/1000 | Loss: 0.00001500
Iteration 154/1000 | Loss: 0.00001500
Iteration 155/1000 | Loss: 0.00001500
Iteration 156/1000 | Loss: 0.00001500
Iteration 157/1000 | Loss: 0.00001500
Iteration 158/1000 | Loss: 0.00001500
Iteration 159/1000 | Loss: 0.00001500
Iteration 160/1000 | Loss: 0.00001500
Iteration 161/1000 | Loss: 0.00001499
Iteration 162/1000 | Loss: 0.00001499
Iteration 163/1000 | Loss: 0.00001499
Iteration 164/1000 | Loss: 0.00001499
Iteration 165/1000 | Loss: 0.00001499
Iteration 166/1000 | Loss: 0.00001499
Iteration 167/1000 | Loss: 0.00001499
Iteration 168/1000 | Loss: 0.00001499
Iteration 169/1000 | Loss: 0.00001499
Iteration 170/1000 | Loss: 0.00001499
Iteration 171/1000 | Loss: 0.00001499
Iteration 172/1000 | Loss: 0.00001498
Iteration 173/1000 | Loss: 0.00001498
Iteration 174/1000 | Loss: 0.00001498
Iteration 175/1000 | Loss: 0.00001498
Iteration 176/1000 | Loss: 0.00001498
Iteration 177/1000 | Loss: 0.00001498
Iteration 178/1000 | Loss: 0.00001498
Iteration 179/1000 | Loss: 0.00001498
Iteration 180/1000 | Loss: 0.00001498
Iteration 181/1000 | Loss: 0.00001498
Iteration 182/1000 | Loss: 0.00001498
Iteration 183/1000 | Loss: 0.00001498
Iteration 184/1000 | Loss: 0.00001498
Iteration 185/1000 | Loss: 0.00001498
Iteration 186/1000 | Loss: 0.00001498
Iteration 187/1000 | Loss: 0.00001498
Iteration 188/1000 | Loss: 0.00001498
Iteration 189/1000 | Loss: 0.00001498
Iteration 190/1000 | Loss: 0.00001498
Iteration 191/1000 | Loss: 0.00001498
Iteration 192/1000 | Loss: 0.00001498
Iteration 193/1000 | Loss: 0.00001498
Iteration 194/1000 | Loss: 0.00001498
Iteration 195/1000 | Loss: 0.00001498
Iteration 196/1000 | Loss: 0.00001498
Iteration 197/1000 | Loss: 0.00001498
Iteration 198/1000 | Loss: 0.00001498
Iteration 199/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.4975824342400301e-05, 1.4975824342400301e-05, 1.4975824342400301e-05, 1.4975824342400301e-05, 1.4975824342400301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4975824342400301e-05

Optimization complete. Final v2v error: 3.2459752559661865 mm

Highest mean error: 4.627892971038818 mm for frame 73

Lowest mean error: 2.7895214557647705 mm for frame 200

Saving results

Total time: 111.25367760658264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909042
Iteration 2/25 | Loss: 0.00253469
Iteration 3/25 | Loss: 0.00174550
Iteration 4/25 | Loss: 0.00163011
Iteration 5/25 | Loss: 0.00160173
Iteration 6/25 | Loss: 0.00155625
Iteration 7/25 | Loss: 0.00154442
Iteration 8/25 | Loss: 0.00154780
Iteration 9/25 | Loss: 0.00152589
Iteration 10/25 | Loss: 0.00152286
Iteration 11/25 | Loss: 0.00152188
Iteration 12/25 | Loss: 0.00152109
Iteration 13/25 | Loss: 0.00151977
Iteration 14/25 | Loss: 0.00151802
Iteration 15/25 | Loss: 0.00151665
Iteration 16/25 | Loss: 0.00151600
Iteration 17/25 | Loss: 0.00151567
Iteration 18/25 | Loss: 0.00151564
Iteration 19/25 | Loss: 0.00151564
Iteration 20/25 | Loss: 0.00151564
Iteration 21/25 | Loss: 0.00151564
Iteration 22/25 | Loss: 0.00151564
Iteration 23/25 | Loss: 0.00151564
Iteration 24/25 | Loss: 0.00151564
Iteration 25/25 | Loss: 0.00151564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38029146
Iteration 2/25 | Loss: 0.00479143
Iteration 3/25 | Loss: 0.00440639
Iteration 4/25 | Loss: 0.00440639
Iteration 5/25 | Loss: 0.00440639
Iteration 6/25 | Loss: 0.00440639
Iteration 7/25 | Loss: 0.00440639
Iteration 8/25 | Loss: 0.00440639
Iteration 9/25 | Loss: 0.00440639
Iteration 10/25 | Loss: 0.00440639
Iteration 11/25 | Loss: 0.00440639
Iteration 12/25 | Loss: 0.00440639
Iteration 13/25 | Loss: 0.00440639
Iteration 14/25 | Loss: 0.00440639
Iteration 15/25 | Loss: 0.00440639
Iteration 16/25 | Loss: 0.00440639
Iteration 17/25 | Loss: 0.00440639
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004406385589390993, 0.004406385589390993, 0.004406385589390993, 0.004406385589390993, 0.004406385589390993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004406385589390993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00440639
Iteration 2/1000 | Loss: 0.00583390
Iteration 3/1000 | Loss: 0.00380829
Iteration 4/1000 | Loss: 0.00361303
Iteration 5/1000 | Loss: 0.00124551
Iteration 6/1000 | Loss: 0.00062535
Iteration 7/1000 | Loss: 0.00089388
Iteration 8/1000 | Loss: 0.00112147
Iteration 9/1000 | Loss: 0.00233495
Iteration 10/1000 | Loss: 0.00120274
Iteration 11/1000 | Loss: 0.00040506
Iteration 12/1000 | Loss: 0.00029703
Iteration 13/1000 | Loss: 0.00046130
Iteration 14/1000 | Loss: 0.00027580
Iteration 15/1000 | Loss: 0.00024112
Iteration 16/1000 | Loss: 0.00019173
Iteration 17/1000 | Loss: 0.00076148
Iteration 18/1000 | Loss: 0.00075480
Iteration 19/1000 | Loss: 0.00020313
Iteration 20/1000 | Loss: 0.00078747
Iteration 21/1000 | Loss: 0.00046381
Iteration 22/1000 | Loss: 0.00016269
Iteration 23/1000 | Loss: 0.00015685
Iteration 24/1000 | Loss: 0.00081094
Iteration 25/1000 | Loss: 0.00034790
Iteration 26/1000 | Loss: 0.00076531
Iteration 27/1000 | Loss: 0.00043709
Iteration 28/1000 | Loss: 0.00050334
Iteration 29/1000 | Loss: 0.00073179
Iteration 30/1000 | Loss: 0.00038150
Iteration 31/1000 | Loss: 0.00047204
Iteration 32/1000 | Loss: 0.00034652
Iteration 33/1000 | Loss: 0.00041910
Iteration 34/1000 | Loss: 0.00013908
Iteration 35/1000 | Loss: 0.00018208
Iteration 36/1000 | Loss: 0.00032517
Iteration 37/1000 | Loss: 0.00013281
Iteration 38/1000 | Loss: 0.00018317
Iteration 39/1000 | Loss: 0.00012115
Iteration 40/1000 | Loss: 0.00064564
Iteration 41/1000 | Loss: 0.00079986
Iteration 42/1000 | Loss: 0.00066888
Iteration 43/1000 | Loss: 0.00132262
Iteration 44/1000 | Loss: 0.00103329
Iteration 45/1000 | Loss: 0.00125266
Iteration 46/1000 | Loss: 0.00075014
Iteration 47/1000 | Loss: 0.00135068
Iteration 48/1000 | Loss: 0.00110662
Iteration 49/1000 | Loss: 0.00069213
Iteration 50/1000 | Loss: 0.00113337
Iteration 51/1000 | Loss: 0.00097486
Iteration 52/1000 | Loss: 0.00081259
Iteration 53/1000 | Loss: 0.00076949
Iteration 54/1000 | Loss: 0.00042709
Iteration 55/1000 | Loss: 0.00051445
Iteration 56/1000 | Loss: 0.00064014
Iteration 57/1000 | Loss: 0.00044327
Iteration 58/1000 | Loss: 0.00053395
Iteration 59/1000 | Loss: 0.00014750
Iteration 60/1000 | Loss: 0.00010546
Iteration 61/1000 | Loss: 0.00037376
Iteration 62/1000 | Loss: 0.00164810
Iteration 63/1000 | Loss: 0.00278999
Iteration 64/1000 | Loss: 0.00044957
Iteration 65/1000 | Loss: 0.00009852
Iteration 66/1000 | Loss: 0.00061469
Iteration 67/1000 | Loss: 0.00266837
Iteration 68/1000 | Loss: 0.00155345
Iteration 69/1000 | Loss: 0.00186638
Iteration 70/1000 | Loss: 0.00302525
Iteration 71/1000 | Loss: 0.00083024
Iteration 72/1000 | Loss: 0.00048746
Iteration 73/1000 | Loss: 0.00043732
Iteration 74/1000 | Loss: 0.00041104
Iteration 75/1000 | Loss: 0.00009733
Iteration 76/1000 | Loss: 0.00122740
Iteration 77/1000 | Loss: 0.00097506
Iteration 78/1000 | Loss: 0.00012533
Iteration 79/1000 | Loss: 0.00022491
Iteration 80/1000 | Loss: 0.00039531
Iteration 81/1000 | Loss: 0.00051953
Iteration 82/1000 | Loss: 0.00038727
Iteration 83/1000 | Loss: 0.00104474
Iteration 84/1000 | Loss: 0.00267365
Iteration 85/1000 | Loss: 0.00125354
Iteration 86/1000 | Loss: 0.00191424
Iteration 87/1000 | Loss: 0.00193703
Iteration 88/1000 | Loss: 0.00143579
Iteration 89/1000 | Loss: 0.00218960
Iteration 90/1000 | Loss: 0.00107902
Iteration 91/1000 | Loss: 0.00207309
Iteration 92/1000 | Loss: 0.00129970
Iteration 93/1000 | Loss: 0.00180607
Iteration 94/1000 | Loss: 0.00107904
Iteration 95/1000 | Loss: 0.00083344
Iteration 96/1000 | Loss: 0.00237964
Iteration 97/1000 | Loss: 0.00140240
Iteration 98/1000 | Loss: 0.00166516
Iteration 99/1000 | Loss: 0.00230458
Iteration 100/1000 | Loss: 0.00172411
Iteration 101/1000 | Loss: 0.00151212
Iteration 102/1000 | Loss: 0.00140108
Iteration 103/1000 | Loss: 0.00116252
Iteration 104/1000 | Loss: 0.00122199
Iteration 105/1000 | Loss: 0.00090330
Iteration 106/1000 | Loss: 0.00108874
Iteration 107/1000 | Loss: 0.00069888
Iteration 108/1000 | Loss: 0.00057004
Iteration 109/1000 | Loss: 0.00070469
Iteration 110/1000 | Loss: 0.00081889
Iteration 111/1000 | Loss: 0.00028799
Iteration 112/1000 | Loss: 0.00037609
Iteration 113/1000 | Loss: 0.00034763
Iteration 114/1000 | Loss: 0.00029832
Iteration 115/1000 | Loss: 0.00006884
Iteration 116/1000 | Loss: 0.00118729
Iteration 117/1000 | Loss: 0.00118703
Iteration 118/1000 | Loss: 0.00142288
Iteration 119/1000 | Loss: 0.00069040
Iteration 120/1000 | Loss: 0.00090539
Iteration 121/1000 | Loss: 0.00092846
Iteration 122/1000 | Loss: 0.00098720
Iteration 123/1000 | Loss: 0.00064592
Iteration 124/1000 | Loss: 0.00062805
Iteration 125/1000 | Loss: 0.00058353
Iteration 126/1000 | Loss: 0.00006316
Iteration 127/1000 | Loss: 0.00004998
Iteration 128/1000 | Loss: 0.00004387
Iteration 129/1000 | Loss: 0.00003811
Iteration 130/1000 | Loss: 0.00003442
Iteration 131/1000 | Loss: 0.00003219
Iteration 132/1000 | Loss: 0.00003060
Iteration 133/1000 | Loss: 0.00002936
Iteration 134/1000 | Loss: 0.00035025
Iteration 135/1000 | Loss: 0.00018412
Iteration 136/1000 | Loss: 0.00035172
Iteration 137/1000 | Loss: 0.00020998
Iteration 138/1000 | Loss: 0.00018799
Iteration 139/1000 | Loss: 0.00009426
Iteration 140/1000 | Loss: 0.00018201
Iteration 141/1000 | Loss: 0.00029408
Iteration 142/1000 | Loss: 0.00016901
Iteration 143/1000 | Loss: 0.00002982
Iteration 144/1000 | Loss: 0.00002747
Iteration 145/1000 | Loss: 0.00002632
Iteration 146/1000 | Loss: 0.00002568
Iteration 147/1000 | Loss: 0.00002522
Iteration 148/1000 | Loss: 0.00002500
Iteration 149/1000 | Loss: 0.00002472
Iteration 150/1000 | Loss: 0.00002469
Iteration 151/1000 | Loss: 0.00002448
Iteration 152/1000 | Loss: 0.00032221
Iteration 153/1000 | Loss: 0.00013105
Iteration 154/1000 | Loss: 0.00004473
Iteration 155/1000 | Loss: 0.00003073
Iteration 156/1000 | Loss: 0.00059693
Iteration 157/1000 | Loss: 0.00029611
Iteration 158/1000 | Loss: 0.00022719
Iteration 159/1000 | Loss: 0.00013686
Iteration 160/1000 | Loss: 0.00013021
Iteration 161/1000 | Loss: 0.00008365
Iteration 162/1000 | Loss: 0.00003825
Iteration 163/1000 | Loss: 0.00003149
Iteration 164/1000 | Loss: 0.00002881
Iteration 165/1000 | Loss: 0.00002733
Iteration 166/1000 | Loss: 0.00002649
Iteration 167/1000 | Loss: 0.00002590
Iteration 168/1000 | Loss: 0.00003021
Iteration 169/1000 | Loss: 0.00002943
Iteration 170/1000 | Loss: 0.00002843
Iteration 171/1000 | Loss: 0.00002648
Iteration 172/1000 | Loss: 0.00002570
Iteration 173/1000 | Loss: 0.00002505
Iteration 174/1000 | Loss: 0.00002466
Iteration 175/1000 | Loss: 0.00002453
Iteration 176/1000 | Loss: 0.00002435
Iteration 177/1000 | Loss: 0.00002431
Iteration 178/1000 | Loss: 0.00002430
Iteration 179/1000 | Loss: 0.00002426
Iteration 180/1000 | Loss: 0.00002426
Iteration 181/1000 | Loss: 0.00002425
Iteration 182/1000 | Loss: 0.00002422
Iteration 183/1000 | Loss: 0.00002422
Iteration 184/1000 | Loss: 0.00002422
Iteration 185/1000 | Loss: 0.00002422
Iteration 186/1000 | Loss: 0.00002422
Iteration 187/1000 | Loss: 0.00002422
Iteration 188/1000 | Loss: 0.00002422
Iteration 189/1000 | Loss: 0.00002422
Iteration 190/1000 | Loss: 0.00002422
Iteration 191/1000 | Loss: 0.00002422
Iteration 192/1000 | Loss: 0.00002421
Iteration 193/1000 | Loss: 0.00002421
Iteration 194/1000 | Loss: 0.00002421
Iteration 195/1000 | Loss: 0.00002420
Iteration 196/1000 | Loss: 0.00002420
Iteration 197/1000 | Loss: 0.00002419
Iteration 198/1000 | Loss: 0.00002419
Iteration 199/1000 | Loss: 0.00002418
Iteration 200/1000 | Loss: 0.00002418
Iteration 201/1000 | Loss: 0.00002418
Iteration 202/1000 | Loss: 0.00002418
Iteration 203/1000 | Loss: 0.00002418
Iteration 204/1000 | Loss: 0.00002418
Iteration 205/1000 | Loss: 0.00002417
Iteration 206/1000 | Loss: 0.00002417
Iteration 207/1000 | Loss: 0.00002417
Iteration 208/1000 | Loss: 0.00002417
Iteration 209/1000 | Loss: 0.00002417
Iteration 210/1000 | Loss: 0.00002417
Iteration 211/1000 | Loss: 0.00002417
Iteration 212/1000 | Loss: 0.00002417
Iteration 213/1000 | Loss: 0.00002417
Iteration 214/1000 | Loss: 0.00002417
Iteration 215/1000 | Loss: 0.00002417
Iteration 216/1000 | Loss: 0.00002417
Iteration 217/1000 | Loss: 0.00002417
Iteration 218/1000 | Loss: 0.00002417
Iteration 219/1000 | Loss: 0.00002417
Iteration 220/1000 | Loss: 0.00002416
Iteration 221/1000 | Loss: 0.00002416
Iteration 222/1000 | Loss: 0.00002416
Iteration 223/1000 | Loss: 0.00002416
Iteration 224/1000 | Loss: 0.00002416
Iteration 225/1000 | Loss: 0.00002416
Iteration 226/1000 | Loss: 0.00002416
Iteration 227/1000 | Loss: 0.00002416
Iteration 228/1000 | Loss: 0.00002416
Iteration 229/1000 | Loss: 0.00002416
Iteration 230/1000 | Loss: 0.00002416
Iteration 231/1000 | Loss: 0.00002416
Iteration 232/1000 | Loss: 0.00002416
Iteration 233/1000 | Loss: 0.00002416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.4160648536053486e-05, 2.4160648536053486e-05, 2.4160648536053486e-05, 2.4160648536053486e-05, 2.4160648536053486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4160648536053486e-05

Optimization complete. Final v2v error: 3.3441944122314453 mm

Highest mean error: 12.332060813903809 mm for frame 85

Lowest mean error: 2.7951159477233887 mm for frame 1

Saving results

Total time: 317.50188422203064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833538
Iteration 2/25 | Loss: 0.00129450
Iteration 3/25 | Loss: 0.00116477
Iteration 4/25 | Loss: 0.00114717
Iteration 5/25 | Loss: 0.00114239
Iteration 6/25 | Loss: 0.00114100
Iteration 7/25 | Loss: 0.00114030
Iteration 8/25 | Loss: 0.00113961
Iteration 9/25 | Loss: 0.00114127
Iteration 10/25 | Loss: 0.00114165
Iteration 11/25 | Loss: 0.00114236
Iteration 12/25 | Loss: 0.00113900
Iteration 13/25 | Loss: 0.00114055
Iteration 14/25 | Loss: 0.00114189
Iteration 15/25 | Loss: 0.00114083
Iteration 16/25 | Loss: 0.00114034
Iteration 17/25 | Loss: 0.00114027
Iteration 18/25 | Loss: 0.00114212
Iteration 19/25 | Loss: 0.00114062
Iteration 20/25 | Loss: 0.00114002
Iteration 21/25 | Loss: 0.00114125
Iteration 22/25 | Loss: 0.00114145
Iteration 23/25 | Loss: 0.00114140
Iteration 24/25 | Loss: 0.00114074
Iteration 25/25 | Loss: 0.00113866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35494721
Iteration 2/25 | Loss: 0.00166401
Iteration 3/25 | Loss: 0.00166400
Iteration 4/25 | Loss: 0.00166400
Iteration 5/25 | Loss: 0.00166400
Iteration 6/25 | Loss: 0.00166400
Iteration 7/25 | Loss: 0.00166400
Iteration 8/25 | Loss: 0.00166400
Iteration 9/25 | Loss: 0.00166400
Iteration 10/25 | Loss: 0.00166400
Iteration 11/25 | Loss: 0.00166400
Iteration 12/25 | Loss: 0.00166400
Iteration 13/25 | Loss: 0.00166400
Iteration 14/25 | Loss: 0.00166400
Iteration 15/25 | Loss: 0.00166400
Iteration 16/25 | Loss: 0.00166400
Iteration 17/25 | Loss: 0.00166400
Iteration 18/25 | Loss: 0.00166400
Iteration 19/25 | Loss: 0.00166400
Iteration 20/25 | Loss: 0.00166400
Iteration 21/25 | Loss: 0.00166400
Iteration 22/25 | Loss: 0.00166400
Iteration 23/25 | Loss: 0.00166400
Iteration 24/25 | Loss: 0.00166400
Iteration 25/25 | Loss: 0.00166400

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166400
Iteration 2/1000 | Loss: 0.00009920
Iteration 3/1000 | Loss: 0.00006751
Iteration 4/1000 | Loss: 0.00005817
Iteration 5/1000 | Loss: 0.00005257
Iteration 6/1000 | Loss: 0.00005014
Iteration 7/1000 | Loss: 0.00004854
Iteration 8/1000 | Loss: 0.00004723
Iteration 9/1000 | Loss: 0.00013275
Iteration 10/1000 | Loss: 0.00005048
Iteration 11/1000 | Loss: 0.00004735
Iteration 12/1000 | Loss: 0.00004620
Iteration 13/1000 | Loss: 0.00005094
Iteration 14/1000 | Loss: 0.00012645
Iteration 15/1000 | Loss: 0.00005166
Iteration 16/1000 | Loss: 0.00009787
Iteration 17/1000 | Loss: 0.00006453
Iteration 18/1000 | Loss: 0.00004751
Iteration 19/1000 | Loss: 0.00008906
Iteration 20/1000 | Loss: 0.00011051
Iteration 21/1000 | Loss: 0.00022751
Iteration 22/1000 | Loss: 0.00015740
Iteration 23/1000 | Loss: 0.00007550
Iteration 24/1000 | Loss: 0.00008362
Iteration 25/1000 | Loss: 0.00007131
Iteration 26/1000 | Loss: 0.00007285
Iteration 27/1000 | Loss: 0.00006308
Iteration 28/1000 | Loss: 0.00006857
Iteration 29/1000 | Loss: 0.00004620
Iteration 30/1000 | Loss: 0.00005344
Iteration 31/1000 | Loss: 0.00010559
Iteration 32/1000 | Loss: 0.00014511
Iteration 33/1000 | Loss: 0.00011985
Iteration 34/1000 | Loss: 0.00005074
Iteration 35/1000 | Loss: 0.00009208
Iteration 36/1000 | Loss: 0.00016390
Iteration 37/1000 | Loss: 0.00008356
Iteration 38/1000 | Loss: 0.00016691
Iteration 39/1000 | Loss: 0.00013948
Iteration 40/1000 | Loss: 0.00004514
Iteration 41/1000 | Loss: 0.00004384
Iteration 42/1000 | Loss: 0.00004330
Iteration 43/1000 | Loss: 0.00004296
Iteration 44/1000 | Loss: 0.00014105
Iteration 45/1000 | Loss: 0.00005124
Iteration 46/1000 | Loss: 0.00004543
Iteration 47/1000 | Loss: 0.00004400
Iteration 48/1000 | Loss: 0.00005699
Iteration 49/1000 | Loss: 0.00004371
Iteration 50/1000 | Loss: 0.00012370
Iteration 51/1000 | Loss: 0.00009505
Iteration 52/1000 | Loss: 0.00043185
Iteration 53/1000 | Loss: 0.00018123
Iteration 54/1000 | Loss: 0.00004775
Iteration 55/1000 | Loss: 0.00010812
Iteration 56/1000 | Loss: 0.00005244
Iteration 57/1000 | Loss: 0.00010145
Iteration 58/1000 | Loss: 0.00008141
Iteration 59/1000 | Loss: 0.00004408
Iteration 60/1000 | Loss: 0.00006618
Iteration 61/1000 | Loss: 0.00005827
Iteration 62/1000 | Loss: 0.00033754
Iteration 63/1000 | Loss: 0.00006841
Iteration 64/1000 | Loss: 0.00007090
Iteration 65/1000 | Loss: 0.00005436
Iteration 66/1000 | Loss: 0.00004334
Iteration 67/1000 | Loss: 0.00005683
Iteration 68/1000 | Loss: 0.00018800
Iteration 69/1000 | Loss: 0.00005970
Iteration 70/1000 | Loss: 0.00004396
Iteration 71/1000 | Loss: 0.00021901
Iteration 72/1000 | Loss: 0.00005344
Iteration 73/1000 | Loss: 0.00006252
Iteration 74/1000 | Loss: 0.00015966
Iteration 75/1000 | Loss: 0.00004609
Iteration 76/1000 | Loss: 0.00005241
Iteration 77/1000 | Loss: 0.00004438
Iteration 78/1000 | Loss: 0.00006081
Iteration 79/1000 | Loss: 0.00006194
Iteration 80/1000 | Loss: 0.00006725
Iteration 81/1000 | Loss: 0.00006763
Iteration 82/1000 | Loss: 0.00004302
Iteration 83/1000 | Loss: 0.00004197
Iteration 84/1000 | Loss: 0.00004146
Iteration 85/1000 | Loss: 0.00008242
Iteration 86/1000 | Loss: 0.00005388
Iteration 87/1000 | Loss: 0.00005867
Iteration 88/1000 | Loss: 0.00005851
Iteration 89/1000 | Loss: 0.00006385
Iteration 90/1000 | Loss: 0.00006922
Iteration 91/1000 | Loss: 0.00004138
Iteration 92/1000 | Loss: 0.00007330
Iteration 93/1000 | Loss: 0.00011976
Iteration 94/1000 | Loss: 0.00006924
Iteration 95/1000 | Loss: 0.00010966
Iteration 96/1000 | Loss: 0.00006780
Iteration 97/1000 | Loss: 0.00011943
Iteration 98/1000 | Loss: 0.00006444
Iteration 99/1000 | Loss: 0.00006243
Iteration 100/1000 | Loss: 0.00004008
Iteration 101/1000 | Loss: 0.00003980
Iteration 102/1000 | Loss: 0.00010401
Iteration 103/1000 | Loss: 0.00004742
Iteration 104/1000 | Loss: 0.00003961
Iteration 105/1000 | Loss: 0.00011823
Iteration 106/1000 | Loss: 0.00004647
Iteration 107/1000 | Loss: 0.00011744
Iteration 108/1000 | Loss: 0.00004702
Iteration 109/1000 | Loss: 0.00014890
Iteration 110/1000 | Loss: 0.00060480
Iteration 111/1000 | Loss: 0.00044640
Iteration 112/1000 | Loss: 0.00011460
Iteration 113/1000 | Loss: 0.00004501
Iteration 114/1000 | Loss: 0.00004229
Iteration 115/1000 | Loss: 0.00047562
Iteration 116/1000 | Loss: 0.00050835
Iteration 117/1000 | Loss: 0.00015233
Iteration 118/1000 | Loss: 0.00037021
Iteration 119/1000 | Loss: 0.00011808
Iteration 120/1000 | Loss: 0.00003928
Iteration 121/1000 | Loss: 0.00003805
Iteration 122/1000 | Loss: 0.00027982
Iteration 123/1000 | Loss: 0.00014007
Iteration 124/1000 | Loss: 0.00004190
Iteration 125/1000 | Loss: 0.00003816
Iteration 126/1000 | Loss: 0.00011112
Iteration 127/1000 | Loss: 0.00003787
Iteration 128/1000 | Loss: 0.00003567
Iteration 129/1000 | Loss: 0.00003511
Iteration 130/1000 | Loss: 0.00003465
Iteration 131/1000 | Loss: 0.00003459
Iteration 132/1000 | Loss: 0.00003456
Iteration 133/1000 | Loss: 0.00003455
Iteration 134/1000 | Loss: 0.00003451
Iteration 135/1000 | Loss: 0.00003449
Iteration 136/1000 | Loss: 0.00003448
Iteration 137/1000 | Loss: 0.00003446
Iteration 138/1000 | Loss: 0.00003435
Iteration 139/1000 | Loss: 0.00003433
Iteration 140/1000 | Loss: 0.00003432
Iteration 141/1000 | Loss: 0.00003432
Iteration 142/1000 | Loss: 0.00003428
Iteration 143/1000 | Loss: 0.00003425
Iteration 144/1000 | Loss: 0.00003414
Iteration 145/1000 | Loss: 0.00003413
Iteration 146/1000 | Loss: 0.00003412
Iteration 147/1000 | Loss: 0.00003412
Iteration 148/1000 | Loss: 0.00003410
Iteration 149/1000 | Loss: 0.00003409
Iteration 150/1000 | Loss: 0.00003406
Iteration 151/1000 | Loss: 0.00003404
Iteration 152/1000 | Loss: 0.00003404
Iteration 153/1000 | Loss: 0.00003404
Iteration 154/1000 | Loss: 0.00003403
Iteration 155/1000 | Loss: 0.00003402
Iteration 156/1000 | Loss: 0.00003402
Iteration 157/1000 | Loss: 0.00003401
Iteration 158/1000 | Loss: 0.00003401
Iteration 159/1000 | Loss: 0.00003401
Iteration 160/1000 | Loss: 0.00003400
Iteration 161/1000 | Loss: 0.00003399
Iteration 162/1000 | Loss: 0.00003399
Iteration 163/1000 | Loss: 0.00003399
Iteration 164/1000 | Loss: 0.00003398
Iteration 165/1000 | Loss: 0.00003398
Iteration 166/1000 | Loss: 0.00003397
Iteration 167/1000 | Loss: 0.00003396
Iteration 168/1000 | Loss: 0.00003395
Iteration 169/1000 | Loss: 0.00003395
Iteration 170/1000 | Loss: 0.00003395
Iteration 171/1000 | Loss: 0.00003394
Iteration 172/1000 | Loss: 0.00003394
Iteration 173/1000 | Loss: 0.00003394
Iteration 174/1000 | Loss: 0.00003393
Iteration 175/1000 | Loss: 0.00003392
Iteration 176/1000 | Loss: 0.00003392
Iteration 177/1000 | Loss: 0.00003391
Iteration 178/1000 | Loss: 0.00003391
Iteration 179/1000 | Loss: 0.00003391
Iteration 180/1000 | Loss: 0.00003390
Iteration 181/1000 | Loss: 0.00003390
Iteration 182/1000 | Loss: 0.00003390
Iteration 183/1000 | Loss: 0.00003389
Iteration 184/1000 | Loss: 0.00003389
Iteration 185/1000 | Loss: 0.00003389
Iteration 186/1000 | Loss: 0.00003388
Iteration 187/1000 | Loss: 0.00003388
Iteration 188/1000 | Loss: 0.00003388
Iteration 189/1000 | Loss: 0.00003387
Iteration 190/1000 | Loss: 0.00003386
Iteration 191/1000 | Loss: 0.00003386
Iteration 192/1000 | Loss: 0.00003385
Iteration 193/1000 | Loss: 0.00003385
Iteration 194/1000 | Loss: 0.00003384
Iteration 195/1000 | Loss: 0.00003384
Iteration 196/1000 | Loss: 0.00003384
Iteration 197/1000 | Loss: 0.00003384
Iteration 198/1000 | Loss: 0.00003384
Iteration 199/1000 | Loss: 0.00003383
Iteration 200/1000 | Loss: 0.00003383
Iteration 201/1000 | Loss: 0.00003383
Iteration 202/1000 | Loss: 0.00003383
Iteration 203/1000 | Loss: 0.00003382
Iteration 204/1000 | Loss: 0.00003382
Iteration 205/1000 | Loss: 0.00003381
Iteration 206/1000 | Loss: 0.00003381
Iteration 207/1000 | Loss: 0.00003381
Iteration 208/1000 | Loss: 0.00003381
Iteration 209/1000 | Loss: 0.00003380
Iteration 210/1000 | Loss: 0.00003380
Iteration 211/1000 | Loss: 0.00003376
Iteration 212/1000 | Loss: 0.00003376
Iteration 213/1000 | Loss: 0.00003376
Iteration 214/1000 | Loss: 0.00003375
Iteration 215/1000 | Loss: 0.00003375
Iteration 216/1000 | Loss: 0.00003375
Iteration 217/1000 | Loss: 0.00003375
Iteration 218/1000 | Loss: 0.00003374
Iteration 219/1000 | Loss: 0.00003374
Iteration 220/1000 | Loss: 0.00003374
Iteration 221/1000 | Loss: 0.00003373
Iteration 222/1000 | Loss: 0.00003373
Iteration 223/1000 | Loss: 0.00003373
Iteration 224/1000 | Loss: 0.00003373
Iteration 225/1000 | Loss: 0.00003372
Iteration 226/1000 | Loss: 0.00003372
Iteration 227/1000 | Loss: 0.00003372
Iteration 228/1000 | Loss: 0.00003372
Iteration 229/1000 | Loss: 0.00003372
Iteration 230/1000 | Loss: 0.00003372
Iteration 231/1000 | Loss: 0.00003372
Iteration 232/1000 | Loss: 0.00003371
Iteration 233/1000 | Loss: 0.00003371
Iteration 234/1000 | Loss: 0.00003371
Iteration 235/1000 | Loss: 0.00003371
Iteration 236/1000 | Loss: 0.00003371
Iteration 237/1000 | Loss: 0.00003371
Iteration 238/1000 | Loss: 0.00003371
Iteration 239/1000 | Loss: 0.00003371
Iteration 240/1000 | Loss: 0.00003370
Iteration 241/1000 | Loss: 0.00003370
Iteration 242/1000 | Loss: 0.00003370
Iteration 243/1000 | Loss: 0.00003370
Iteration 244/1000 | Loss: 0.00003370
Iteration 245/1000 | Loss: 0.00003370
Iteration 246/1000 | Loss: 0.00003370
Iteration 247/1000 | Loss: 0.00003370
Iteration 248/1000 | Loss: 0.00003369
Iteration 249/1000 | Loss: 0.00003369
Iteration 250/1000 | Loss: 0.00003369
Iteration 251/1000 | Loss: 0.00003369
Iteration 252/1000 | Loss: 0.00003369
Iteration 253/1000 | Loss: 0.00003369
Iteration 254/1000 | Loss: 0.00003369
Iteration 255/1000 | Loss: 0.00003369
Iteration 256/1000 | Loss: 0.00003369
Iteration 257/1000 | Loss: 0.00003369
Iteration 258/1000 | Loss: 0.00003369
Iteration 259/1000 | Loss: 0.00003369
Iteration 260/1000 | Loss: 0.00003368
Iteration 261/1000 | Loss: 0.00003368
Iteration 262/1000 | Loss: 0.00003368
Iteration 263/1000 | Loss: 0.00003368
Iteration 264/1000 | Loss: 0.00003368
Iteration 265/1000 | Loss: 0.00003368
Iteration 266/1000 | Loss: 0.00003368
Iteration 267/1000 | Loss: 0.00003367
Iteration 268/1000 | Loss: 0.00003367
Iteration 269/1000 | Loss: 0.00003367
Iteration 270/1000 | Loss: 0.00003367
Iteration 271/1000 | Loss: 0.00003367
Iteration 272/1000 | Loss: 0.00003367
Iteration 273/1000 | Loss: 0.00003367
Iteration 274/1000 | Loss: 0.00003367
Iteration 275/1000 | Loss: 0.00003367
Iteration 276/1000 | Loss: 0.00003367
Iteration 277/1000 | Loss: 0.00003366
Iteration 278/1000 | Loss: 0.00003366
Iteration 279/1000 | Loss: 0.00003366
Iteration 280/1000 | Loss: 0.00003366
Iteration 281/1000 | Loss: 0.00003366
Iteration 282/1000 | Loss: 0.00003366
Iteration 283/1000 | Loss: 0.00003366
Iteration 284/1000 | Loss: 0.00003365
Iteration 285/1000 | Loss: 0.00003365
Iteration 286/1000 | Loss: 0.00003365
Iteration 287/1000 | Loss: 0.00003365
Iteration 288/1000 | Loss: 0.00003365
Iteration 289/1000 | Loss: 0.00003364
Iteration 290/1000 | Loss: 0.00003364
Iteration 291/1000 | Loss: 0.00003364
Iteration 292/1000 | Loss: 0.00003364
Iteration 293/1000 | Loss: 0.00003364
Iteration 294/1000 | Loss: 0.00003364
Iteration 295/1000 | Loss: 0.00003364
Iteration 296/1000 | Loss: 0.00003364
Iteration 297/1000 | Loss: 0.00003364
Iteration 298/1000 | Loss: 0.00003364
Iteration 299/1000 | Loss: 0.00003363
Iteration 300/1000 | Loss: 0.00003363
Iteration 301/1000 | Loss: 0.00003363
Iteration 302/1000 | Loss: 0.00003363
Iteration 303/1000 | Loss: 0.00003363
Iteration 304/1000 | Loss: 0.00003363
Iteration 305/1000 | Loss: 0.00003363
Iteration 306/1000 | Loss: 0.00003363
Iteration 307/1000 | Loss: 0.00003363
Iteration 308/1000 | Loss: 0.00003363
Iteration 309/1000 | Loss: 0.00003363
Iteration 310/1000 | Loss: 0.00003363
Iteration 311/1000 | Loss: 0.00003363
Iteration 312/1000 | Loss: 0.00003363
Iteration 313/1000 | Loss: 0.00003363
Iteration 314/1000 | Loss: 0.00003362
Iteration 315/1000 | Loss: 0.00003362
Iteration 316/1000 | Loss: 0.00003362
Iteration 317/1000 | Loss: 0.00003362
Iteration 318/1000 | Loss: 0.00003362
Iteration 319/1000 | Loss: 0.00003361
Iteration 320/1000 | Loss: 0.00003361
Iteration 321/1000 | Loss: 0.00003361
Iteration 322/1000 | Loss: 0.00003361
Iteration 323/1000 | Loss: 0.00003361
Iteration 324/1000 | Loss: 0.00003361
Iteration 325/1000 | Loss: 0.00003361
Iteration 326/1000 | Loss: 0.00003361
Iteration 327/1000 | Loss: 0.00003360
Iteration 328/1000 | Loss: 0.00003360
Iteration 329/1000 | Loss: 0.00003360
Iteration 330/1000 | Loss: 0.00003360
Iteration 331/1000 | Loss: 0.00003360
Iteration 332/1000 | Loss: 0.00003360
Iteration 333/1000 | Loss: 0.00003360
Iteration 334/1000 | Loss: 0.00003360
Iteration 335/1000 | Loss: 0.00003360
Iteration 336/1000 | Loss: 0.00003359
Iteration 337/1000 | Loss: 0.00003359
Iteration 338/1000 | Loss: 0.00003359
Iteration 339/1000 | Loss: 0.00003359
Iteration 340/1000 | Loss: 0.00003359
Iteration 341/1000 | Loss: 0.00003359
Iteration 342/1000 | Loss: 0.00003359
Iteration 343/1000 | Loss: 0.00003359
Iteration 344/1000 | Loss: 0.00003359
Iteration 345/1000 | Loss: 0.00003359
Iteration 346/1000 | Loss: 0.00003359
Iteration 347/1000 | Loss: 0.00003359
Iteration 348/1000 | Loss: 0.00003359
Iteration 349/1000 | Loss: 0.00003359
Iteration 350/1000 | Loss: 0.00003359
Iteration 351/1000 | Loss: 0.00003359
Iteration 352/1000 | Loss: 0.00003359
Iteration 353/1000 | Loss: 0.00003359
Iteration 354/1000 | Loss: 0.00003358
Iteration 355/1000 | Loss: 0.00003358
Iteration 356/1000 | Loss: 0.00003358
Iteration 357/1000 | Loss: 0.00003358
Iteration 358/1000 | Loss: 0.00003358
Iteration 359/1000 | Loss: 0.00003358
Iteration 360/1000 | Loss: 0.00003358
Iteration 361/1000 | Loss: 0.00003358
Iteration 362/1000 | Loss: 0.00003358
Iteration 363/1000 | Loss: 0.00003358
Iteration 364/1000 | Loss: 0.00003358
Iteration 365/1000 | Loss: 0.00003358
Iteration 366/1000 | Loss: 0.00003358
Iteration 367/1000 | Loss: 0.00003358
Iteration 368/1000 | Loss: 0.00003358
Iteration 369/1000 | Loss: 0.00003358
Iteration 370/1000 | Loss: 0.00003358
Iteration 371/1000 | Loss: 0.00003358
Iteration 372/1000 | Loss: 0.00003358
Iteration 373/1000 | Loss: 0.00003358
Iteration 374/1000 | Loss: 0.00003358
Iteration 375/1000 | Loss: 0.00003358
Iteration 376/1000 | Loss: 0.00003358
Iteration 377/1000 | Loss: 0.00003357
Iteration 378/1000 | Loss: 0.00003357
Iteration 379/1000 | Loss: 0.00003357
Iteration 380/1000 | Loss: 0.00003357
Iteration 381/1000 | Loss: 0.00003357
Iteration 382/1000 | Loss: 0.00003357
Iteration 383/1000 | Loss: 0.00003357
Iteration 384/1000 | Loss: 0.00003357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 384. Stopping optimization.
Last 5 losses: [3.3574011467862874e-05, 3.3574011467862874e-05, 3.3574011467862874e-05, 3.3574011467862874e-05, 3.3574011467862874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3574011467862874e-05

Optimization complete. Final v2v error: 2.99212384223938 mm

Highest mean error: 12.070194244384766 mm for frame 114

Lowest mean error: 2.139185905456543 mm for frame 77

Saving results

Total time: 292.082373380661
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412571
Iteration 2/25 | Loss: 0.00121962
Iteration 3/25 | Loss: 0.00110187
Iteration 4/25 | Loss: 0.00108695
Iteration 5/25 | Loss: 0.00108342
Iteration 6/25 | Loss: 0.00108265
Iteration 7/25 | Loss: 0.00108265
Iteration 8/25 | Loss: 0.00108265
Iteration 9/25 | Loss: 0.00108265
Iteration 10/25 | Loss: 0.00108265
Iteration 11/25 | Loss: 0.00108265
Iteration 12/25 | Loss: 0.00108265
Iteration 13/25 | Loss: 0.00108265
Iteration 14/25 | Loss: 0.00108265
Iteration 15/25 | Loss: 0.00108265
Iteration 16/25 | Loss: 0.00108265
Iteration 17/25 | Loss: 0.00108265
Iteration 18/25 | Loss: 0.00108265
Iteration 19/25 | Loss: 0.00108265
Iteration 20/25 | Loss: 0.00108265
Iteration 21/25 | Loss: 0.00108265
Iteration 22/25 | Loss: 0.00108265
Iteration 23/25 | Loss: 0.00108265
Iteration 24/25 | Loss: 0.00108265
Iteration 25/25 | Loss: 0.00108265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.74072695
Iteration 2/25 | Loss: 0.00077661
Iteration 3/25 | Loss: 0.00077660
Iteration 4/25 | Loss: 0.00077659
Iteration 5/25 | Loss: 0.00077659
Iteration 6/25 | Loss: 0.00077659
Iteration 7/25 | Loss: 0.00077659
Iteration 8/25 | Loss: 0.00077659
Iteration 9/25 | Loss: 0.00077659
Iteration 10/25 | Loss: 0.00077659
Iteration 11/25 | Loss: 0.00077659
Iteration 12/25 | Loss: 0.00077659
Iteration 13/25 | Loss: 0.00077659
Iteration 14/25 | Loss: 0.00077659
Iteration 15/25 | Loss: 0.00077659
Iteration 16/25 | Loss: 0.00077659
Iteration 17/25 | Loss: 0.00077659
Iteration 18/25 | Loss: 0.00077659
Iteration 19/25 | Loss: 0.00077659
Iteration 20/25 | Loss: 0.00077659
Iteration 21/25 | Loss: 0.00077659
Iteration 22/25 | Loss: 0.00077659
Iteration 23/25 | Loss: 0.00077659
Iteration 24/25 | Loss: 0.00077659
Iteration 25/25 | Loss: 0.00077659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077659
Iteration 2/1000 | Loss: 0.00002354
Iteration 3/1000 | Loss: 0.00001669
Iteration 4/1000 | Loss: 0.00001402
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001172
Iteration 9/1000 | Loss: 0.00001159
Iteration 10/1000 | Loss: 0.00001134
Iteration 11/1000 | Loss: 0.00001115
Iteration 12/1000 | Loss: 0.00001109
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001103
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001090
Iteration 22/1000 | Loss: 0.00001088
Iteration 23/1000 | Loss: 0.00001088
Iteration 24/1000 | Loss: 0.00001088
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001084
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001084
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001080
Iteration 31/1000 | Loss: 0.00001080
Iteration 32/1000 | Loss: 0.00001080
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001071
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001070
Iteration 56/1000 | Loss: 0.00001070
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001070
Iteration 59/1000 | Loss: 0.00001070
Iteration 60/1000 | Loss: 0.00001070
Iteration 61/1000 | Loss: 0.00001068
Iteration 62/1000 | Loss: 0.00001068
Iteration 63/1000 | Loss: 0.00001067
Iteration 64/1000 | Loss: 0.00001067
Iteration 65/1000 | Loss: 0.00001067
Iteration 66/1000 | Loss: 0.00001067
Iteration 67/1000 | Loss: 0.00001067
Iteration 68/1000 | Loss: 0.00001067
Iteration 69/1000 | Loss: 0.00001067
Iteration 70/1000 | Loss: 0.00001067
Iteration 71/1000 | Loss: 0.00001067
Iteration 72/1000 | Loss: 0.00001067
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001067
Iteration 77/1000 | Loss: 0.00001067
Iteration 78/1000 | Loss: 0.00001066
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001064
Iteration 85/1000 | Loss: 0.00001064
Iteration 86/1000 | Loss: 0.00001064
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001063
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001060
Iteration 96/1000 | Loss: 0.00001060
Iteration 97/1000 | Loss: 0.00001060
Iteration 98/1000 | Loss: 0.00001060
Iteration 99/1000 | Loss: 0.00001060
Iteration 100/1000 | Loss: 0.00001060
Iteration 101/1000 | Loss: 0.00001060
Iteration 102/1000 | Loss: 0.00001060
Iteration 103/1000 | Loss: 0.00001059
Iteration 104/1000 | Loss: 0.00001059
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001057
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001057
Iteration 114/1000 | Loss: 0.00001057
Iteration 115/1000 | Loss: 0.00001057
Iteration 116/1000 | Loss: 0.00001056
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001055
Iteration 119/1000 | Loss: 0.00001055
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.054293716151733e-05, 1.054293716151733e-05, 1.054293716151733e-05, 1.054293716151733e-05, 1.054293716151733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.054293716151733e-05

Optimization complete. Final v2v error: 2.775205373764038 mm

Highest mean error: 3.421271324157715 mm for frame 85

Lowest mean error: 2.4561588764190674 mm for frame 41

Saving results

Total time: 35.88822364807129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113358
Iteration 2/25 | Loss: 0.01113358
Iteration 3/25 | Loss: 0.01113358
Iteration 4/25 | Loss: 0.01113358
Iteration 5/25 | Loss: 0.01113357
Iteration 6/25 | Loss: 0.01113357
Iteration 7/25 | Loss: 0.01113357
Iteration 8/25 | Loss: 0.01113356
Iteration 9/25 | Loss: 0.01113356
Iteration 10/25 | Loss: 0.01113356
Iteration 11/25 | Loss: 0.01113356
Iteration 12/25 | Loss: 0.01113356
Iteration 13/25 | Loss: 0.01113356
Iteration 14/25 | Loss: 0.01113355
Iteration 15/25 | Loss: 0.01113355
Iteration 16/25 | Loss: 0.01113355
Iteration 17/25 | Loss: 0.01113355
Iteration 18/25 | Loss: 0.01113355
Iteration 19/25 | Loss: 0.01113354
Iteration 20/25 | Loss: 0.01113354
Iteration 21/25 | Loss: 0.01113354
Iteration 22/25 | Loss: 0.01113353
Iteration 23/25 | Loss: 0.01113353
Iteration 24/25 | Loss: 0.01113353
Iteration 25/25 | Loss: 0.01113353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26218390
Iteration 2/25 | Loss: 0.17503820
Iteration 3/25 | Loss: 0.17498083
Iteration 4/25 | Loss: 0.17498083
Iteration 5/25 | Loss: 0.17498079
Iteration 6/25 | Loss: 0.17498077
Iteration 7/25 | Loss: 0.17498076
Iteration 8/25 | Loss: 0.17498076
Iteration 9/25 | Loss: 0.17498076
Iteration 10/25 | Loss: 0.17498076
Iteration 11/25 | Loss: 0.17498076
Iteration 12/25 | Loss: 0.17498076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.1749807596206665, 0.1749807596206665, 0.1749807596206665, 0.1749807596206665, 0.1749807596206665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1749807596206665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17498076
Iteration 2/1000 | Loss: 0.00469327
Iteration 3/1000 | Loss: 0.00197974
Iteration 4/1000 | Loss: 0.00050869
Iteration 5/1000 | Loss: 0.00024128
Iteration 6/1000 | Loss: 0.00013936
Iteration 7/1000 | Loss: 0.00008440
Iteration 8/1000 | Loss: 0.00006057
Iteration 9/1000 | Loss: 0.00004519
Iteration 10/1000 | Loss: 0.00003655
Iteration 11/1000 | Loss: 0.00003080
Iteration 12/1000 | Loss: 0.00002639
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002301
Iteration 15/1000 | Loss: 0.00002202
Iteration 16/1000 | Loss: 0.00002118
Iteration 17/1000 | Loss: 0.00002080
Iteration 18/1000 | Loss: 0.00002057
Iteration 19/1000 | Loss: 0.00002030
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00001998
Iteration 22/1000 | Loss: 0.00001994
Iteration 23/1000 | Loss: 0.00001993
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001991
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001990
Iteration 28/1000 | Loss: 0.00001985
Iteration 29/1000 | Loss: 0.00001978
Iteration 30/1000 | Loss: 0.00001975
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001970
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00001964
Iteration 36/1000 | Loss: 0.00001964
Iteration 37/1000 | Loss: 0.00001963
Iteration 38/1000 | Loss: 0.00001962
Iteration 39/1000 | Loss: 0.00001962
Iteration 40/1000 | Loss: 0.00001961
Iteration 41/1000 | Loss: 0.00001961
Iteration 42/1000 | Loss: 0.00001961
Iteration 43/1000 | Loss: 0.00001961
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001960
Iteration 46/1000 | Loss: 0.00001960
Iteration 47/1000 | Loss: 0.00001960
Iteration 48/1000 | Loss: 0.00001960
Iteration 49/1000 | Loss: 0.00001960
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001959
Iteration 55/1000 | Loss: 0.00001959
Iteration 56/1000 | Loss: 0.00001959
Iteration 57/1000 | Loss: 0.00001959
Iteration 58/1000 | Loss: 0.00001959
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001958
Iteration 61/1000 | Loss: 0.00001958
Iteration 62/1000 | Loss: 0.00001958
Iteration 63/1000 | Loss: 0.00001958
Iteration 64/1000 | Loss: 0.00001958
Iteration 65/1000 | Loss: 0.00001957
Iteration 66/1000 | Loss: 0.00001957
Iteration 67/1000 | Loss: 0.00001957
Iteration 68/1000 | Loss: 0.00001957
Iteration 69/1000 | Loss: 0.00001957
Iteration 70/1000 | Loss: 0.00001957
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001957
Iteration 75/1000 | Loss: 0.00001957
Iteration 76/1000 | Loss: 0.00001957
Iteration 77/1000 | Loss: 0.00001957
Iteration 78/1000 | Loss: 0.00001957
Iteration 79/1000 | Loss: 0.00001957
Iteration 80/1000 | Loss: 0.00001957
Iteration 81/1000 | Loss: 0.00001957
Iteration 82/1000 | Loss: 0.00001957
Iteration 83/1000 | Loss: 0.00001957
Iteration 84/1000 | Loss: 0.00001957
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.9574945326894522e-05, 1.9574945326894522e-05, 1.9574945326894522e-05, 1.9574945326894522e-05, 1.9574945326894522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9574945326894522e-05

Optimization complete. Final v2v error: 3.561941623687744 mm

Highest mean error: 4.22623872756958 mm for frame 225

Lowest mean error: 3.105966806411743 mm for frame 21

Saving results

Total time: 50.54583120346069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853544
Iteration 2/25 | Loss: 0.00158584
Iteration 3/25 | Loss: 0.00126317
Iteration 4/25 | Loss: 0.00121636
Iteration 5/25 | Loss: 0.00120885
Iteration 6/25 | Loss: 0.00120712
Iteration 7/25 | Loss: 0.00120683
Iteration 8/25 | Loss: 0.00120683
Iteration 9/25 | Loss: 0.00120683
Iteration 10/25 | Loss: 0.00120683
Iteration 11/25 | Loss: 0.00120683
Iteration 12/25 | Loss: 0.00120683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012068260693922639, 0.0012068260693922639, 0.0012068260693922639, 0.0012068260693922639, 0.0012068260693922639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012068260693922639

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24980283
Iteration 2/25 | Loss: 0.00087946
Iteration 3/25 | Loss: 0.00087945
Iteration 4/25 | Loss: 0.00087945
Iteration 5/25 | Loss: 0.00087945
Iteration 6/25 | Loss: 0.00087945
Iteration 7/25 | Loss: 0.00087945
Iteration 8/25 | Loss: 0.00087945
Iteration 9/25 | Loss: 0.00087945
Iteration 10/25 | Loss: 0.00087945
Iteration 11/25 | Loss: 0.00087945
Iteration 12/25 | Loss: 0.00087945
Iteration 13/25 | Loss: 0.00087945
Iteration 14/25 | Loss: 0.00087945
Iteration 15/25 | Loss: 0.00087945
Iteration 16/25 | Loss: 0.00087945
Iteration 17/25 | Loss: 0.00087945
Iteration 18/25 | Loss: 0.00087945
Iteration 19/25 | Loss: 0.00087945
Iteration 20/25 | Loss: 0.00087945
Iteration 21/25 | Loss: 0.00087945
Iteration 22/25 | Loss: 0.00087945
Iteration 23/25 | Loss: 0.00087945
Iteration 24/25 | Loss: 0.00087945
Iteration 25/25 | Loss: 0.00087945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087945
Iteration 2/1000 | Loss: 0.00004968
Iteration 3/1000 | Loss: 0.00002993
Iteration 4/1000 | Loss: 0.00002462
Iteration 5/1000 | Loss: 0.00002262
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00001999
Iteration 9/1000 | Loss: 0.00001973
Iteration 10/1000 | Loss: 0.00001956
Iteration 11/1000 | Loss: 0.00001943
Iteration 12/1000 | Loss: 0.00001935
Iteration 13/1000 | Loss: 0.00001928
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001907
Iteration 16/1000 | Loss: 0.00001906
Iteration 17/1000 | Loss: 0.00001906
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001903
Iteration 20/1000 | Loss: 0.00001901
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001891
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001873
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001865
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001863
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001863
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001862
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001861
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00001861
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001859
Iteration 71/1000 | Loss: 0.00001859
Iteration 72/1000 | Loss: 0.00001859
Iteration 73/1000 | Loss: 0.00001859
Iteration 74/1000 | Loss: 0.00001859
Iteration 75/1000 | Loss: 0.00001859
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Iteration 81/1000 | Loss: 0.00001858
Iteration 82/1000 | Loss: 0.00001858
Iteration 83/1000 | Loss: 0.00001858
Iteration 84/1000 | Loss: 0.00001858
Iteration 85/1000 | Loss: 0.00001858
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001857
Iteration 90/1000 | Loss: 0.00001857
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001857
Iteration 93/1000 | Loss: 0.00001857
Iteration 94/1000 | Loss: 0.00001857
Iteration 95/1000 | Loss: 0.00001857
Iteration 96/1000 | Loss: 0.00001857
Iteration 97/1000 | Loss: 0.00001857
Iteration 98/1000 | Loss: 0.00001857
Iteration 99/1000 | Loss: 0.00001857
Iteration 100/1000 | Loss: 0.00001856
Iteration 101/1000 | Loss: 0.00001856
Iteration 102/1000 | Loss: 0.00001856
Iteration 103/1000 | Loss: 0.00001856
Iteration 104/1000 | Loss: 0.00001856
Iteration 105/1000 | Loss: 0.00001856
Iteration 106/1000 | Loss: 0.00001856
Iteration 107/1000 | Loss: 0.00001856
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001856
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001856
Iteration 116/1000 | Loss: 0.00001856
Iteration 117/1000 | Loss: 0.00001856
Iteration 118/1000 | Loss: 0.00001856
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Iteration 124/1000 | Loss: 0.00001855
Iteration 125/1000 | Loss: 0.00001855
Iteration 126/1000 | Loss: 0.00001855
Iteration 127/1000 | Loss: 0.00001855
Iteration 128/1000 | Loss: 0.00001855
Iteration 129/1000 | Loss: 0.00001855
Iteration 130/1000 | Loss: 0.00001855
Iteration 131/1000 | Loss: 0.00001855
Iteration 132/1000 | Loss: 0.00001854
Iteration 133/1000 | Loss: 0.00001854
Iteration 134/1000 | Loss: 0.00001854
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001853
Iteration 139/1000 | Loss: 0.00001853
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001851
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001850
Iteration 160/1000 | Loss: 0.00001850
Iteration 161/1000 | Loss: 0.00001850
Iteration 162/1000 | Loss: 0.00001850
Iteration 163/1000 | Loss: 0.00001850
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001849
Iteration 170/1000 | Loss: 0.00001849
Iteration 171/1000 | Loss: 0.00001849
Iteration 172/1000 | Loss: 0.00001849
Iteration 173/1000 | Loss: 0.00001849
Iteration 174/1000 | Loss: 0.00001849
Iteration 175/1000 | Loss: 0.00001849
Iteration 176/1000 | Loss: 0.00001848
Iteration 177/1000 | Loss: 0.00001848
Iteration 178/1000 | Loss: 0.00001848
Iteration 179/1000 | Loss: 0.00001848
Iteration 180/1000 | Loss: 0.00001848
Iteration 181/1000 | Loss: 0.00001848
Iteration 182/1000 | Loss: 0.00001847
Iteration 183/1000 | Loss: 0.00001847
Iteration 184/1000 | Loss: 0.00001847
Iteration 185/1000 | Loss: 0.00001847
Iteration 186/1000 | Loss: 0.00001847
Iteration 187/1000 | Loss: 0.00001846
Iteration 188/1000 | Loss: 0.00001846
Iteration 189/1000 | Loss: 0.00001846
Iteration 190/1000 | Loss: 0.00001846
Iteration 191/1000 | Loss: 0.00001846
Iteration 192/1000 | Loss: 0.00001846
Iteration 193/1000 | Loss: 0.00001845
Iteration 194/1000 | Loss: 0.00001845
Iteration 195/1000 | Loss: 0.00001845
Iteration 196/1000 | Loss: 0.00001845
Iteration 197/1000 | Loss: 0.00001845
Iteration 198/1000 | Loss: 0.00001845
Iteration 199/1000 | Loss: 0.00001845
Iteration 200/1000 | Loss: 0.00001844
Iteration 201/1000 | Loss: 0.00001844
Iteration 202/1000 | Loss: 0.00001844
Iteration 203/1000 | Loss: 0.00001844
Iteration 204/1000 | Loss: 0.00001844
Iteration 205/1000 | Loss: 0.00001844
Iteration 206/1000 | Loss: 0.00001844
Iteration 207/1000 | Loss: 0.00001844
Iteration 208/1000 | Loss: 0.00001844
Iteration 209/1000 | Loss: 0.00001843
Iteration 210/1000 | Loss: 0.00001843
Iteration 211/1000 | Loss: 0.00001843
Iteration 212/1000 | Loss: 0.00001843
Iteration 213/1000 | Loss: 0.00001843
Iteration 214/1000 | Loss: 0.00001843
Iteration 215/1000 | Loss: 0.00001843
Iteration 216/1000 | Loss: 0.00001843
Iteration 217/1000 | Loss: 0.00001843
Iteration 218/1000 | Loss: 0.00001843
Iteration 219/1000 | Loss: 0.00001843
Iteration 220/1000 | Loss: 0.00001843
Iteration 221/1000 | Loss: 0.00001843
Iteration 222/1000 | Loss: 0.00001843
Iteration 223/1000 | Loss: 0.00001843
Iteration 224/1000 | Loss: 0.00001842
Iteration 225/1000 | Loss: 0.00001842
Iteration 226/1000 | Loss: 0.00001842
Iteration 227/1000 | Loss: 0.00001842
Iteration 228/1000 | Loss: 0.00001842
Iteration 229/1000 | Loss: 0.00001842
Iteration 230/1000 | Loss: 0.00001842
Iteration 231/1000 | Loss: 0.00001842
Iteration 232/1000 | Loss: 0.00001842
Iteration 233/1000 | Loss: 0.00001842
Iteration 234/1000 | Loss: 0.00001842
Iteration 235/1000 | Loss: 0.00001842
Iteration 236/1000 | Loss: 0.00001842
Iteration 237/1000 | Loss: 0.00001841
Iteration 238/1000 | Loss: 0.00001841
Iteration 239/1000 | Loss: 0.00001841
Iteration 240/1000 | Loss: 0.00001841
Iteration 241/1000 | Loss: 0.00001841
Iteration 242/1000 | Loss: 0.00001841
Iteration 243/1000 | Loss: 0.00001841
Iteration 244/1000 | Loss: 0.00001841
Iteration 245/1000 | Loss: 0.00001841
Iteration 246/1000 | Loss: 0.00001841
Iteration 247/1000 | Loss: 0.00001841
Iteration 248/1000 | Loss: 0.00001841
Iteration 249/1000 | Loss: 0.00001841
Iteration 250/1000 | Loss: 0.00001841
Iteration 251/1000 | Loss: 0.00001841
Iteration 252/1000 | Loss: 0.00001841
Iteration 253/1000 | Loss: 0.00001841
Iteration 254/1000 | Loss: 0.00001840
Iteration 255/1000 | Loss: 0.00001840
Iteration 256/1000 | Loss: 0.00001840
Iteration 257/1000 | Loss: 0.00001840
Iteration 258/1000 | Loss: 0.00001840
Iteration 259/1000 | Loss: 0.00001840
Iteration 260/1000 | Loss: 0.00001840
Iteration 261/1000 | Loss: 0.00001840
Iteration 262/1000 | Loss: 0.00001840
Iteration 263/1000 | Loss: 0.00001840
Iteration 264/1000 | Loss: 0.00001840
Iteration 265/1000 | Loss: 0.00001840
Iteration 266/1000 | Loss: 0.00001840
Iteration 267/1000 | Loss: 0.00001840
Iteration 268/1000 | Loss: 0.00001840
Iteration 269/1000 | Loss: 0.00001840
Iteration 270/1000 | Loss: 0.00001840
Iteration 271/1000 | Loss: 0.00001839
Iteration 272/1000 | Loss: 0.00001839
Iteration 273/1000 | Loss: 0.00001839
Iteration 274/1000 | Loss: 0.00001839
Iteration 275/1000 | Loss: 0.00001839
Iteration 276/1000 | Loss: 0.00001839
Iteration 277/1000 | Loss: 0.00001839
Iteration 278/1000 | Loss: 0.00001839
Iteration 279/1000 | Loss: 0.00001839
Iteration 280/1000 | Loss: 0.00001839
Iteration 281/1000 | Loss: 0.00001839
Iteration 282/1000 | Loss: 0.00001839
Iteration 283/1000 | Loss: 0.00001838
Iteration 284/1000 | Loss: 0.00001838
Iteration 285/1000 | Loss: 0.00001838
Iteration 286/1000 | Loss: 0.00001838
Iteration 287/1000 | Loss: 0.00001838
Iteration 288/1000 | Loss: 0.00001838
Iteration 289/1000 | Loss: 0.00001838
Iteration 290/1000 | Loss: 0.00001838
Iteration 291/1000 | Loss: 0.00001838
Iteration 292/1000 | Loss: 0.00001838
Iteration 293/1000 | Loss: 0.00001838
Iteration 294/1000 | Loss: 0.00001838
Iteration 295/1000 | Loss: 0.00001838
Iteration 296/1000 | Loss: 0.00001838
Iteration 297/1000 | Loss: 0.00001838
Iteration 298/1000 | Loss: 0.00001838
Iteration 299/1000 | Loss: 0.00001838
Iteration 300/1000 | Loss: 0.00001838
Iteration 301/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 301. Stopping optimization.
Last 5 losses: [1.838001116993837e-05, 1.838001116993837e-05, 1.838001116993837e-05, 1.838001116993837e-05, 1.838001116993837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.838001116993837e-05

Optimization complete. Final v2v error: 3.620062828063965 mm

Highest mean error: 3.8567135334014893 mm for frame 115

Lowest mean error: 3.1780619621276855 mm for frame 36

Saving results

Total time: 47.88379120826721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553846
Iteration 2/25 | Loss: 0.00135312
Iteration 3/25 | Loss: 0.00115029
Iteration 4/25 | Loss: 0.00112435
Iteration 5/25 | Loss: 0.00111691
Iteration 6/25 | Loss: 0.00111657
Iteration 7/25 | Loss: 0.00111418
Iteration 8/25 | Loss: 0.00111384
Iteration 9/25 | Loss: 0.00111381
Iteration 10/25 | Loss: 0.00111381
Iteration 11/25 | Loss: 0.00111380
Iteration 12/25 | Loss: 0.00111380
Iteration 13/25 | Loss: 0.00111380
Iteration 14/25 | Loss: 0.00111380
Iteration 15/25 | Loss: 0.00111380
Iteration 16/25 | Loss: 0.00111380
Iteration 17/25 | Loss: 0.00111380
Iteration 18/25 | Loss: 0.00111380
Iteration 19/25 | Loss: 0.00111380
Iteration 20/25 | Loss: 0.00111380
Iteration 21/25 | Loss: 0.00111380
Iteration 22/25 | Loss: 0.00111380
Iteration 23/25 | Loss: 0.00111380
Iteration 24/25 | Loss: 0.00111379
Iteration 25/25 | Loss: 0.00111379

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22044563
Iteration 2/25 | Loss: 0.00082163
Iteration 3/25 | Loss: 0.00077398
Iteration 4/25 | Loss: 0.00077397
Iteration 5/25 | Loss: 0.00077397
Iteration 6/25 | Loss: 0.00077397
Iteration 7/25 | Loss: 0.00077397
Iteration 8/25 | Loss: 0.00077397
Iteration 9/25 | Loss: 0.00077397
Iteration 10/25 | Loss: 0.00077397
Iteration 11/25 | Loss: 0.00077397
Iteration 12/25 | Loss: 0.00077397
Iteration 13/25 | Loss: 0.00077397
Iteration 14/25 | Loss: 0.00077397
Iteration 15/25 | Loss: 0.00077397
Iteration 16/25 | Loss: 0.00077397
Iteration 17/25 | Loss: 0.00077397
Iteration 18/25 | Loss: 0.00077397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007739721331745386, 0.0007739721331745386, 0.0007739721331745386, 0.0007739721331745386, 0.0007739721331745386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007739721331745386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077397
Iteration 2/1000 | Loss: 0.00009569
Iteration 3/1000 | Loss: 0.00005113
Iteration 4/1000 | Loss: 0.00010861
Iteration 5/1000 | Loss: 0.00001949
Iteration 6/1000 | Loss: 0.00002773
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00002801
Iteration 9/1000 | Loss: 0.00004323
Iteration 10/1000 | Loss: 0.00003372
Iteration 11/1000 | Loss: 0.00001268
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001233
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00009188
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001196
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001193
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00003623
Iteration 42/1000 | Loss: 0.00003638
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00003912
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001180
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001179
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001177
Iteration 88/1000 | Loss: 0.00001177
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001176
Iteration 92/1000 | Loss: 0.00001176
Iteration 93/1000 | Loss: 0.00001176
Iteration 94/1000 | Loss: 0.00001176
Iteration 95/1000 | Loss: 0.00001176
Iteration 96/1000 | Loss: 0.00001176
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001171
Iteration 127/1000 | Loss: 0.00005384
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001166
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00002545
Iteration 133/1000 | Loss: 0.00001295
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001165
Iteration 136/1000 | Loss: 0.00001165
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001164
Iteration 139/1000 | Loss: 0.00001164
Iteration 140/1000 | Loss: 0.00001164
Iteration 141/1000 | Loss: 0.00001164
Iteration 142/1000 | Loss: 0.00001164
Iteration 143/1000 | Loss: 0.00001164
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001163
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001163
Iteration 149/1000 | Loss: 0.00001163
Iteration 150/1000 | Loss: 0.00001163
Iteration 151/1000 | Loss: 0.00001163
Iteration 152/1000 | Loss: 0.00001163
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001163
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001163
Iteration 171/1000 | Loss: 0.00001163
Iteration 172/1000 | Loss: 0.00001163
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001163
Iteration 180/1000 | Loss: 0.00001163
Iteration 181/1000 | Loss: 0.00001162
Iteration 182/1000 | Loss: 0.00001162
Iteration 183/1000 | Loss: 0.00001162
Iteration 184/1000 | Loss: 0.00001162
Iteration 185/1000 | Loss: 0.00001162
Iteration 186/1000 | Loss: 0.00001162
Iteration 187/1000 | Loss: 0.00001162
Iteration 188/1000 | Loss: 0.00001162
Iteration 189/1000 | Loss: 0.00001162
Iteration 190/1000 | Loss: 0.00001162
Iteration 191/1000 | Loss: 0.00001162
Iteration 192/1000 | Loss: 0.00001162
Iteration 193/1000 | Loss: 0.00001162
Iteration 194/1000 | Loss: 0.00001162
Iteration 195/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.1624429134826642e-05, 1.1624429134826642e-05, 1.1624429134826642e-05, 1.1624429134826642e-05, 1.1624429134826642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1624429134826642e-05

Optimization complete. Final v2v error: 2.8907463550567627 mm

Highest mean error: 3.334118604660034 mm for frame 213

Lowest mean error: 2.6382839679718018 mm for frame 226

Saving results

Total time: 68.77205657958984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446838
Iteration 2/25 | Loss: 0.00135044
Iteration 3/25 | Loss: 0.00119615
Iteration 4/25 | Loss: 0.00118240
Iteration 5/25 | Loss: 0.00117987
Iteration 6/25 | Loss: 0.00117987
Iteration 7/25 | Loss: 0.00117987
Iteration 8/25 | Loss: 0.00117987
Iteration 9/25 | Loss: 0.00117987
Iteration 10/25 | Loss: 0.00117987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011798694031313062, 0.0011798694031313062, 0.0011798694031313062, 0.0011798694031313062, 0.0011798694031313062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011798694031313062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32152247
Iteration 2/25 | Loss: 0.00077811
Iteration 3/25 | Loss: 0.00077810
Iteration 4/25 | Loss: 0.00077810
Iteration 5/25 | Loss: 0.00077810
Iteration 6/25 | Loss: 0.00077810
Iteration 7/25 | Loss: 0.00077810
Iteration 8/25 | Loss: 0.00077810
Iteration 9/25 | Loss: 0.00077810
Iteration 10/25 | Loss: 0.00077810
Iteration 11/25 | Loss: 0.00077810
Iteration 12/25 | Loss: 0.00077810
Iteration 13/25 | Loss: 0.00077810
Iteration 14/25 | Loss: 0.00077810
Iteration 15/25 | Loss: 0.00077810
Iteration 16/25 | Loss: 0.00077810
Iteration 17/25 | Loss: 0.00077810
Iteration 18/25 | Loss: 0.00077810
Iteration 19/25 | Loss: 0.00077810
Iteration 20/25 | Loss: 0.00077810
Iteration 21/25 | Loss: 0.00077810
Iteration 22/25 | Loss: 0.00077810
Iteration 23/25 | Loss: 0.00077810
Iteration 24/25 | Loss: 0.00077810
Iteration 25/25 | Loss: 0.00077810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077810
Iteration 2/1000 | Loss: 0.00002826
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001280
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001177
Iteration 9/1000 | Loss: 0.00001177
Iteration 10/1000 | Loss: 0.00001177
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001158
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001156
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001143
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001140
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001139
Iteration 24/1000 | Loss: 0.00001139
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001135
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001132
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001124
Iteration 40/1000 | Loss: 0.00001123
Iteration 41/1000 | Loss: 0.00001121
Iteration 42/1000 | Loss: 0.00001119
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001118
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001117
Iteration 47/1000 | Loss: 0.00001116
Iteration 48/1000 | Loss: 0.00001113
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001111
Iteration 53/1000 | Loss: 0.00001111
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001110
Iteration 56/1000 | Loss: 0.00001110
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001106
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001103
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001103
Iteration 83/1000 | Loss: 0.00001103
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001103
Iteration 95/1000 | Loss: 0.00001103
Iteration 96/1000 | Loss: 0.00001103
Iteration 97/1000 | Loss: 0.00001103
Iteration 98/1000 | Loss: 0.00001103
Iteration 99/1000 | Loss: 0.00001103
Iteration 100/1000 | Loss: 0.00001103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.102568148780847e-05, 1.102568148780847e-05, 1.102568148780847e-05, 1.102568148780847e-05, 1.102568148780847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.102568148780847e-05

Optimization complete. Final v2v error: 2.847062349319458 mm

Highest mean error: 3.1004412174224854 mm for frame 95

Lowest mean error: 2.700924873352051 mm for frame 23

Saving results

Total time: 29.280228853225708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837205
Iteration 2/25 | Loss: 0.00144644
Iteration 3/25 | Loss: 0.00118368
Iteration 4/25 | Loss: 0.00112428
Iteration 5/25 | Loss: 0.00112607
Iteration 6/25 | Loss: 0.00112581
Iteration 7/25 | Loss: 0.00111059
Iteration 8/25 | Loss: 0.00110469
Iteration 9/25 | Loss: 0.00109488
Iteration 10/25 | Loss: 0.00108774
Iteration 11/25 | Loss: 0.00108229
Iteration 12/25 | Loss: 0.00108062
Iteration 13/25 | Loss: 0.00108013
Iteration 14/25 | Loss: 0.00107993
Iteration 15/25 | Loss: 0.00107987
Iteration 16/25 | Loss: 0.00107986
Iteration 17/25 | Loss: 0.00107986
Iteration 18/25 | Loss: 0.00107986
Iteration 19/25 | Loss: 0.00107986
Iteration 20/25 | Loss: 0.00107986
Iteration 21/25 | Loss: 0.00107986
Iteration 22/25 | Loss: 0.00107986
Iteration 23/25 | Loss: 0.00107986
Iteration 24/25 | Loss: 0.00107986
Iteration 25/25 | Loss: 0.00107986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010798571165651083, 0.0010798571165651083, 0.0010798571165651083, 0.0010798571165651083, 0.0010798571165651083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010798571165651083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.89664698
Iteration 2/25 | Loss: 0.00090786
Iteration 3/25 | Loss: 0.00084093
Iteration 4/25 | Loss: 0.00084093
Iteration 5/25 | Loss: 0.00084093
Iteration 6/25 | Loss: 0.00084093
Iteration 7/25 | Loss: 0.00084093
Iteration 8/25 | Loss: 0.00084093
Iteration 9/25 | Loss: 0.00084093
Iteration 10/25 | Loss: 0.00084093
Iteration 11/25 | Loss: 0.00084093
Iteration 12/25 | Loss: 0.00084093
Iteration 13/25 | Loss: 0.00084093
Iteration 14/25 | Loss: 0.00084093
Iteration 15/25 | Loss: 0.00084093
Iteration 16/25 | Loss: 0.00084093
Iteration 17/25 | Loss: 0.00084093
Iteration 18/25 | Loss: 0.00084093
Iteration 19/25 | Loss: 0.00084093
Iteration 20/25 | Loss: 0.00084093
Iteration 21/25 | Loss: 0.00084093
Iteration 22/25 | Loss: 0.00084093
Iteration 23/25 | Loss: 0.00084093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008409260772168636, 0.0008409260772168636, 0.0008409260772168636, 0.0008409260772168636, 0.0008409260772168636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008409260772168636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084093
Iteration 2/1000 | Loss: 0.00009588
Iteration 3/1000 | Loss: 0.00001483
Iteration 4/1000 | Loss: 0.00001361
Iteration 5/1000 | Loss: 0.00008687
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001194
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001158
Iteration 14/1000 | Loss: 0.00001158
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001141
Iteration 17/1000 | Loss: 0.00001139
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001126
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001125
Iteration 23/1000 | Loss: 0.00008506
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001117
Iteration 26/1000 | Loss: 0.00001117
Iteration 27/1000 | Loss: 0.00001116
Iteration 28/1000 | Loss: 0.00001114
Iteration 29/1000 | Loss: 0.00001113
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001111
Iteration 33/1000 | Loss: 0.00001110
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001110
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001109
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001107
Iteration 40/1000 | Loss: 0.00001107
Iteration 41/1000 | Loss: 0.00001106
Iteration 42/1000 | Loss: 0.00010004
Iteration 43/1000 | Loss: 0.00001153
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001093
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001090
Iteration 54/1000 | Loss: 0.00001090
Iteration 55/1000 | Loss: 0.00001089
Iteration 56/1000 | Loss: 0.00001089
Iteration 57/1000 | Loss: 0.00001089
Iteration 58/1000 | Loss: 0.00001089
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001088
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001087
Iteration 70/1000 | Loss: 0.00001087
Iteration 71/1000 | Loss: 0.00001087
Iteration 72/1000 | Loss: 0.00001087
Iteration 73/1000 | Loss: 0.00001086
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001085
Iteration 77/1000 | Loss: 0.00001085
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001084
Iteration 80/1000 | Loss: 0.00001083
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001080
Iteration 86/1000 | Loss: 0.00001080
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001079
Iteration 89/1000 | Loss: 0.00001079
Iteration 90/1000 | Loss: 0.00001078
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001075
Iteration 102/1000 | Loss: 0.00001075
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00006932
Iteration 107/1000 | Loss: 0.00001096
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00003348
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001080
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00002077
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001074
Iteration 131/1000 | Loss: 0.00001074
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001894
Iteration 138/1000 | Loss: 0.00001075
Iteration 139/1000 | Loss: 0.00001075
Iteration 140/1000 | Loss: 0.00001075
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001074
Iteration 145/1000 | Loss: 0.00001074
Iteration 146/1000 | Loss: 0.00001074
Iteration 147/1000 | Loss: 0.00001074
Iteration 148/1000 | Loss: 0.00001074
Iteration 149/1000 | Loss: 0.00001074
Iteration 150/1000 | Loss: 0.00001074
Iteration 151/1000 | Loss: 0.00001074
Iteration 152/1000 | Loss: 0.00001074
Iteration 153/1000 | Loss: 0.00001074
Iteration 154/1000 | Loss: 0.00001074
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001071
Iteration 163/1000 | Loss: 0.00001071
Iteration 164/1000 | Loss: 0.00001071
Iteration 165/1000 | Loss: 0.00001071
Iteration 166/1000 | Loss: 0.00001071
Iteration 167/1000 | Loss: 0.00001070
Iteration 168/1000 | Loss: 0.00001070
Iteration 169/1000 | Loss: 0.00001070
Iteration 170/1000 | Loss: 0.00001070
Iteration 171/1000 | Loss: 0.00001069
Iteration 172/1000 | Loss: 0.00001069
Iteration 173/1000 | Loss: 0.00001069
Iteration 174/1000 | Loss: 0.00001069
Iteration 175/1000 | Loss: 0.00001069
Iteration 176/1000 | Loss: 0.00001069
Iteration 177/1000 | Loss: 0.00001069
Iteration 178/1000 | Loss: 0.00001069
Iteration 179/1000 | Loss: 0.00001069
Iteration 180/1000 | Loss: 0.00001069
Iteration 181/1000 | Loss: 0.00001069
Iteration 182/1000 | Loss: 0.00001068
Iteration 183/1000 | Loss: 0.00001068
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001067
Iteration 187/1000 | Loss: 0.00001067
Iteration 188/1000 | Loss: 0.00001067
Iteration 189/1000 | Loss: 0.00001066
Iteration 190/1000 | Loss: 0.00001066
Iteration 191/1000 | Loss: 0.00001066
Iteration 192/1000 | Loss: 0.00001066
Iteration 193/1000 | Loss: 0.00001066
Iteration 194/1000 | Loss: 0.00001066
Iteration 195/1000 | Loss: 0.00001066
Iteration 196/1000 | Loss: 0.00001066
Iteration 197/1000 | Loss: 0.00001066
Iteration 198/1000 | Loss: 0.00001066
Iteration 199/1000 | Loss: 0.00001066
Iteration 200/1000 | Loss: 0.00001066
Iteration 201/1000 | Loss: 0.00001066
Iteration 202/1000 | Loss: 0.00001066
Iteration 203/1000 | Loss: 0.00001065
Iteration 204/1000 | Loss: 0.00001065
Iteration 205/1000 | Loss: 0.00001065
Iteration 206/1000 | Loss: 0.00001065
Iteration 207/1000 | Loss: 0.00001065
Iteration 208/1000 | Loss: 0.00001065
Iteration 209/1000 | Loss: 0.00001065
Iteration 210/1000 | Loss: 0.00001065
Iteration 211/1000 | Loss: 0.00001065
Iteration 212/1000 | Loss: 0.00001065
Iteration 213/1000 | Loss: 0.00001065
Iteration 214/1000 | Loss: 0.00001065
Iteration 215/1000 | Loss: 0.00001065
Iteration 216/1000 | Loss: 0.00001065
Iteration 217/1000 | Loss: 0.00001065
Iteration 218/1000 | Loss: 0.00001065
Iteration 219/1000 | Loss: 0.00001065
Iteration 220/1000 | Loss: 0.00001065
Iteration 221/1000 | Loss: 0.00001065
Iteration 222/1000 | Loss: 0.00001065
Iteration 223/1000 | Loss: 0.00001065
Iteration 224/1000 | Loss: 0.00001065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.065081414708402e-05, 1.065081414708402e-05, 1.065081414708402e-05, 1.065081414708402e-05, 1.065081414708402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.065081414708402e-05

Optimization complete. Final v2v error: 2.791713237762451 mm

Highest mean error: 3.3363683223724365 mm for frame 7

Lowest mean error: 2.462189197540283 mm for frame 99

Saving results

Total time: 83.11546277999878
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054064
Iteration 2/25 | Loss: 0.01054064
Iteration 3/25 | Loss: 0.01054064
Iteration 4/25 | Loss: 0.01054064
Iteration 5/25 | Loss: 0.01054064
Iteration 6/25 | Loss: 0.01054064
Iteration 7/25 | Loss: 0.01054064
Iteration 8/25 | Loss: 0.01054063
Iteration 9/25 | Loss: 0.01054063
Iteration 10/25 | Loss: 0.01054063
Iteration 11/25 | Loss: 0.01054063
Iteration 12/25 | Loss: 0.01054063
Iteration 13/25 | Loss: 0.01054063
Iteration 14/25 | Loss: 0.01054063
Iteration 15/25 | Loss: 0.01054063
Iteration 16/25 | Loss: 0.01054063
Iteration 17/25 | Loss: 0.01054063
Iteration 18/25 | Loss: 0.01054062
Iteration 19/25 | Loss: 0.01054062
Iteration 20/25 | Loss: 0.01054062
Iteration 21/25 | Loss: 0.01054062
Iteration 22/25 | Loss: 0.01054062
Iteration 23/25 | Loss: 0.01054062
Iteration 24/25 | Loss: 0.01054062
Iteration 25/25 | Loss: 0.01054062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71347666
Iteration 2/25 | Loss: 0.05952790
Iteration 3/25 | Loss: 0.05952666
Iteration 4/25 | Loss: 0.05952664
Iteration 5/25 | Loss: 0.05952664
Iteration 6/25 | Loss: 0.05952663
Iteration 7/25 | Loss: 0.05952663
Iteration 8/25 | Loss: 0.05952663
Iteration 9/25 | Loss: 0.05952663
Iteration 10/25 | Loss: 0.05952663
Iteration 11/25 | Loss: 0.05952663
Iteration 12/25 | Loss: 0.05952663
Iteration 13/25 | Loss: 0.05952663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.059526629745960236, 0.059526629745960236, 0.059526629745960236, 0.059526629745960236, 0.059526629745960236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.059526629745960236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05952663
Iteration 2/1000 | Loss: 0.00604894
Iteration 3/1000 | Loss: 0.00025759
Iteration 4/1000 | Loss: 0.00141926
Iteration 5/1000 | Loss: 0.00252292
Iteration 6/1000 | Loss: 0.00164141
Iteration 7/1000 | Loss: 0.00013487
Iteration 8/1000 | Loss: 0.00024911
Iteration 9/1000 | Loss: 0.00139677
Iteration 10/1000 | Loss: 0.00100228
Iteration 11/1000 | Loss: 0.00153391
Iteration 12/1000 | Loss: 0.00046623
Iteration 13/1000 | Loss: 0.00049394
Iteration 14/1000 | Loss: 0.00021066
Iteration 15/1000 | Loss: 0.00003794
Iteration 16/1000 | Loss: 0.00093874
Iteration 17/1000 | Loss: 0.00011269
Iteration 18/1000 | Loss: 0.00029624
Iteration 19/1000 | Loss: 0.00024466
Iteration 20/1000 | Loss: 0.00040415
Iteration 21/1000 | Loss: 0.00026571
Iteration 22/1000 | Loss: 0.00034396
Iteration 23/1000 | Loss: 0.00074672
Iteration 24/1000 | Loss: 0.00043868
Iteration 25/1000 | Loss: 0.00020417
Iteration 26/1000 | Loss: 0.00003494
Iteration 27/1000 | Loss: 0.00027323
Iteration 28/1000 | Loss: 0.00042470
Iteration 29/1000 | Loss: 0.00027556
Iteration 30/1000 | Loss: 0.00012384
Iteration 31/1000 | Loss: 0.00003286
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00019089
Iteration 34/1000 | Loss: 0.00047595
Iteration 35/1000 | Loss: 0.00002758
Iteration 36/1000 | Loss: 0.00010207
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00018231
Iteration 39/1000 | Loss: 0.00002976
Iteration 40/1000 | Loss: 0.00003229
Iteration 41/1000 | Loss: 0.00057741
Iteration 42/1000 | Loss: 0.00158195
Iteration 43/1000 | Loss: 0.00185874
Iteration 44/1000 | Loss: 0.00136031
Iteration 45/1000 | Loss: 0.00159809
Iteration 46/1000 | Loss: 0.00129530
Iteration 47/1000 | Loss: 0.00002976
Iteration 48/1000 | Loss: 0.00005837
Iteration 49/1000 | Loss: 0.00002131
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00017684
Iteration 52/1000 | Loss: 0.00005904
Iteration 53/1000 | Loss: 0.00013847
Iteration 54/1000 | Loss: 0.00004364
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00007380
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001460
Iteration 60/1000 | Loss: 0.00025250
Iteration 61/1000 | Loss: 0.00009593
Iteration 62/1000 | Loss: 0.00001535
Iteration 63/1000 | Loss: 0.00003129
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00010734
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00009384
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001133
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00005287
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00018981
Iteration 78/1000 | Loss: 0.00048407
Iteration 79/1000 | Loss: 0.00047249
Iteration 80/1000 | Loss: 0.00032623
Iteration 81/1000 | Loss: 0.00002010
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001117
Iteration 86/1000 | Loss: 0.00001107
Iteration 87/1000 | Loss: 0.00001101
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001098
Iteration 90/1000 | Loss: 0.00001098
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001093
Iteration 93/1000 | Loss: 0.00001093
Iteration 94/1000 | Loss: 0.00001092
Iteration 95/1000 | Loss: 0.00006281
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00002170
Iteration 98/1000 | Loss: 0.00019589
Iteration 99/1000 | Loss: 0.00001758
Iteration 100/1000 | Loss: 0.00002578
Iteration 101/1000 | Loss: 0.00012453
Iteration 102/1000 | Loss: 0.00001998
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00021349
Iteration 105/1000 | Loss: 0.00004863
Iteration 106/1000 | Loss: 0.00006480
Iteration 107/1000 | Loss: 0.00006468
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001095
Iteration 111/1000 | Loss: 0.00001086
Iteration 112/1000 | Loss: 0.00001080
Iteration 113/1000 | Loss: 0.00001077
Iteration 114/1000 | Loss: 0.00001077
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00004340
Iteration 118/1000 | Loss: 0.00004340
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001218
Iteration 121/1000 | Loss: 0.00001079
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001074
Iteration 131/1000 | Loss: 0.00001074
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001073
Iteration 138/1000 | Loss: 0.00001073
Iteration 139/1000 | Loss: 0.00001073
Iteration 140/1000 | Loss: 0.00001073
Iteration 141/1000 | Loss: 0.00001073
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001073
Iteration 145/1000 | Loss: 0.00001073
Iteration 146/1000 | Loss: 0.00001073
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001072
Iteration 149/1000 | Loss: 0.00001072
Iteration 150/1000 | Loss: 0.00001072
Iteration 151/1000 | Loss: 0.00001072
Iteration 152/1000 | Loss: 0.00001072
Iteration 153/1000 | Loss: 0.00001072
Iteration 154/1000 | Loss: 0.00001072
Iteration 155/1000 | Loss: 0.00001072
Iteration 156/1000 | Loss: 0.00001072
Iteration 157/1000 | Loss: 0.00001072
Iteration 158/1000 | Loss: 0.00001072
Iteration 159/1000 | Loss: 0.00001072
Iteration 160/1000 | Loss: 0.00001072
Iteration 161/1000 | Loss: 0.00001072
Iteration 162/1000 | Loss: 0.00001072
Iteration 163/1000 | Loss: 0.00001072
Iteration 164/1000 | Loss: 0.00001072
Iteration 165/1000 | Loss: 0.00001072
Iteration 166/1000 | Loss: 0.00001072
Iteration 167/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.0719525562308263e-05, 1.0719525562308263e-05, 1.0719525562308263e-05, 1.0719525562308263e-05, 1.0719525562308263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0719525562308263e-05

Optimization complete. Final v2v error: 2.8063604831695557 mm

Highest mean error: 3.4645445346832275 mm for frame 129

Lowest mean error: 2.494009256362915 mm for frame 249

Saving results

Total time: 180.5105721950531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976598
Iteration 2/25 | Loss: 0.00976598
Iteration 3/25 | Loss: 0.00976597
Iteration 4/25 | Loss: 0.00408214
Iteration 5/25 | Loss: 0.00225323
Iteration 6/25 | Loss: 0.00196738
Iteration 7/25 | Loss: 0.00181472
Iteration 8/25 | Loss: 0.00175307
Iteration 9/25 | Loss: 0.00172312
Iteration 10/25 | Loss: 0.00169283
Iteration 11/25 | Loss: 0.00168479
Iteration 12/25 | Loss: 0.00167732
Iteration 13/25 | Loss: 0.00167819
Iteration 14/25 | Loss: 0.00166723
Iteration 15/25 | Loss: 0.00166162
Iteration 16/25 | Loss: 0.00165459
Iteration 17/25 | Loss: 0.00165610
Iteration 18/25 | Loss: 0.00164668
Iteration 19/25 | Loss: 0.00164552
Iteration 20/25 | Loss: 0.00164796
Iteration 21/25 | Loss: 0.00164626
Iteration 22/25 | Loss: 0.00164187
Iteration 23/25 | Loss: 0.00164484
Iteration 24/25 | Loss: 0.00164108
Iteration 25/25 | Loss: 0.00167006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35323048
Iteration 2/25 | Loss: 0.00796965
Iteration 3/25 | Loss: 0.00770311
Iteration 4/25 | Loss: 0.00770311
Iteration 5/25 | Loss: 0.00770311
Iteration 6/25 | Loss: 0.00770311
Iteration 7/25 | Loss: 0.00770311
Iteration 8/25 | Loss: 0.00770311
Iteration 9/25 | Loss: 0.00770311
Iteration 10/25 | Loss: 0.00770311
Iteration 11/25 | Loss: 0.00770311
Iteration 12/25 | Loss: 0.00770311
Iteration 13/25 | Loss: 0.00770311
Iteration 14/25 | Loss: 0.00770311
Iteration 15/25 | Loss: 0.00770311
Iteration 16/25 | Loss: 0.00770311
Iteration 17/25 | Loss: 0.00770311
Iteration 18/25 | Loss: 0.00770311
Iteration 19/25 | Loss: 0.00770311
Iteration 20/25 | Loss: 0.00770311
Iteration 21/25 | Loss: 0.00770311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.007703107316046953, 0.007703107316046953, 0.007703107316046953, 0.007703107316046953, 0.007703107316046953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007703107316046953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00770311
Iteration 2/1000 | Loss: 0.00150305
Iteration 3/1000 | Loss: 0.00133537
Iteration 4/1000 | Loss: 0.00085985
Iteration 5/1000 | Loss: 0.00091013
Iteration 6/1000 | Loss: 0.00165894
Iteration 7/1000 | Loss: 0.00093179
Iteration 8/1000 | Loss: 0.00115113
Iteration 9/1000 | Loss: 0.00052900
Iteration 10/1000 | Loss: 0.00082168
Iteration 11/1000 | Loss: 0.00107688
Iteration 12/1000 | Loss: 0.00100004
Iteration 13/1000 | Loss: 0.00040571
Iteration 14/1000 | Loss: 0.00054325
Iteration 15/1000 | Loss: 0.00056088
Iteration 16/1000 | Loss: 0.00060219
Iteration 17/1000 | Loss: 0.00041320
Iteration 18/1000 | Loss: 0.00051787
Iteration 19/1000 | Loss: 0.00068430
Iteration 20/1000 | Loss: 0.00047992
Iteration 21/1000 | Loss: 0.00085528
Iteration 22/1000 | Loss: 0.00041035
Iteration 23/1000 | Loss: 0.00081898
Iteration 24/1000 | Loss: 0.00032051
Iteration 25/1000 | Loss: 0.00040953
Iteration 26/1000 | Loss: 0.00072886
Iteration 27/1000 | Loss: 0.00117957
Iteration 28/1000 | Loss: 0.00053079
Iteration 29/1000 | Loss: 0.00133532
Iteration 30/1000 | Loss: 0.00061203
Iteration 31/1000 | Loss: 0.00075702
Iteration 32/1000 | Loss: 0.00028327
Iteration 33/1000 | Loss: 0.00081101
Iteration 34/1000 | Loss: 0.00021721
Iteration 35/1000 | Loss: 0.00077014
Iteration 36/1000 | Loss: 0.00359604
Iteration 37/1000 | Loss: 0.00035301
Iteration 38/1000 | Loss: 0.00359264
Iteration 39/1000 | Loss: 0.00054240
Iteration 40/1000 | Loss: 0.00019842
Iteration 41/1000 | Loss: 0.00033850
Iteration 42/1000 | Loss: 0.00102490
Iteration 43/1000 | Loss: 0.00052715
Iteration 44/1000 | Loss: 0.00017395
Iteration 45/1000 | Loss: 0.00068524
Iteration 46/1000 | Loss: 0.00032309
Iteration 47/1000 | Loss: 0.00016522
Iteration 48/1000 | Loss: 0.00015689
Iteration 49/1000 | Loss: 0.00050203
Iteration 50/1000 | Loss: 0.00135785
Iteration 51/1000 | Loss: 0.00016175
Iteration 52/1000 | Loss: 0.00014928
Iteration 53/1000 | Loss: 0.00014860
Iteration 54/1000 | Loss: 0.00029629
Iteration 55/1000 | Loss: 0.00062975
Iteration 56/1000 | Loss: 0.00014655
Iteration 57/1000 | Loss: 0.00014037
Iteration 58/1000 | Loss: 0.00014632
Iteration 59/1000 | Loss: 0.00026812
Iteration 60/1000 | Loss: 0.00116366
Iteration 61/1000 | Loss: 0.00019796
Iteration 62/1000 | Loss: 0.00064409
Iteration 63/1000 | Loss: 0.00014403
Iteration 64/1000 | Loss: 0.00013150
Iteration 65/1000 | Loss: 0.00013596
Iteration 66/1000 | Loss: 0.00040424
Iteration 67/1000 | Loss: 0.00019685
Iteration 68/1000 | Loss: 0.00013957
Iteration 69/1000 | Loss: 0.00013104
Iteration 70/1000 | Loss: 0.00012441
Iteration 71/1000 | Loss: 0.00012132
Iteration 72/1000 | Loss: 0.00044070
Iteration 73/1000 | Loss: 0.00016485
Iteration 74/1000 | Loss: 0.00015235
Iteration 75/1000 | Loss: 0.00011700
Iteration 76/1000 | Loss: 0.00043437
Iteration 77/1000 | Loss: 0.00012607
Iteration 78/1000 | Loss: 0.00011756
Iteration 79/1000 | Loss: 0.00013657
Iteration 80/1000 | Loss: 0.00029838
Iteration 81/1000 | Loss: 0.00047191
Iteration 82/1000 | Loss: 0.00011180
Iteration 83/1000 | Loss: 0.00031133
Iteration 84/1000 | Loss: 0.00030572
Iteration 85/1000 | Loss: 0.00050055
Iteration 86/1000 | Loss: 0.00028242
Iteration 87/1000 | Loss: 0.00011325
Iteration 88/1000 | Loss: 0.00012307
Iteration 89/1000 | Loss: 0.00010815
Iteration 90/1000 | Loss: 0.00010782
Iteration 91/1000 | Loss: 0.00036829
Iteration 92/1000 | Loss: 0.00013279
Iteration 93/1000 | Loss: 0.00027273
Iteration 94/1000 | Loss: 0.00047761
Iteration 95/1000 | Loss: 0.00014128
Iteration 96/1000 | Loss: 0.00011272
Iteration 97/1000 | Loss: 0.00015769
Iteration 98/1000 | Loss: 0.00011364
Iteration 99/1000 | Loss: 0.00010559
Iteration 100/1000 | Loss: 0.00012374
Iteration 101/1000 | Loss: 0.00010376
Iteration 102/1000 | Loss: 0.00010823
Iteration 103/1000 | Loss: 0.00010718
Iteration 104/1000 | Loss: 0.00010525
Iteration 105/1000 | Loss: 0.00010127
Iteration 106/1000 | Loss: 0.00010085
Iteration 107/1000 | Loss: 0.00010976
Iteration 108/1000 | Loss: 0.00010156
Iteration 109/1000 | Loss: 0.00010154
Iteration 110/1000 | Loss: 0.00030339
Iteration 111/1000 | Loss: 0.00011943
Iteration 112/1000 | Loss: 0.00010324
Iteration 113/1000 | Loss: 0.00010122
Iteration 114/1000 | Loss: 0.00010041
Iteration 115/1000 | Loss: 0.00032893
Iteration 116/1000 | Loss: 0.00021890
Iteration 117/1000 | Loss: 0.00017761
Iteration 118/1000 | Loss: 0.00015910
Iteration 119/1000 | Loss: 0.00009919
Iteration 120/1000 | Loss: 0.00010773
Iteration 121/1000 | Loss: 0.00009736
Iteration 122/1000 | Loss: 0.00010798
Iteration 123/1000 | Loss: 0.00009709
Iteration 124/1000 | Loss: 0.00009733
Iteration 125/1000 | Loss: 0.00009861
Iteration 126/1000 | Loss: 0.00009981
Iteration 127/1000 | Loss: 0.00009603
Iteration 128/1000 | Loss: 0.00059471
Iteration 129/1000 | Loss: 0.00059471
Iteration 130/1000 | Loss: 0.00019064
Iteration 131/1000 | Loss: 0.00016178
Iteration 132/1000 | Loss: 0.00010001
Iteration 133/1000 | Loss: 0.00009665
Iteration 134/1000 | Loss: 0.00010806
Iteration 135/1000 | Loss: 0.00009728
Iteration 136/1000 | Loss: 0.00009865
Iteration 137/1000 | Loss: 0.00015636
Iteration 138/1000 | Loss: 0.00009741
Iteration 139/1000 | Loss: 0.00010036
Iteration 140/1000 | Loss: 0.00009270
Iteration 141/1000 | Loss: 0.00009789
Iteration 142/1000 | Loss: 0.00012204
Iteration 143/1000 | Loss: 0.00009403
Iteration 144/1000 | Loss: 0.00009228
Iteration 145/1000 | Loss: 0.00009466
Iteration 146/1000 | Loss: 0.00009194
Iteration 147/1000 | Loss: 0.00033076
Iteration 148/1000 | Loss: 0.00010049
Iteration 149/1000 | Loss: 0.00080983
Iteration 150/1000 | Loss: 0.00108039
Iteration 151/1000 | Loss: 0.00113035
Iteration 152/1000 | Loss: 0.00117358
Iteration 153/1000 | Loss: 0.00052997
Iteration 154/1000 | Loss: 0.00072454
Iteration 155/1000 | Loss: 0.00031323
Iteration 156/1000 | Loss: 0.00027380
Iteration 157/1000 | Loss: 0.00022492
Iteration 158/1000 | Loss: 0.00012600
Iteration 159/1000 | Loss: 0.00010034
Iteration 160/1000 | Loss: 0.00008920
Iteration 161/1000 | Loss: 0.00008187
Iteration 162/1000 | Loss: 0.00017881
Iteration 163/1000 | Loss: 0.00008045
Iteration 164/1000 | Loss: 0.00010434
Iteration 165/1000 | Loss: 0.00007748
Iteration 166/1000 | Loss: 0.00035480
Iteration 167/1000 | Loss: 0.00013147
Iteration 168/1000 | Loss: 0.00032355
Iteration 169/1000 | Loss: 0.00015149
Iteration 170/1000 | Loss: 0.00026011
Iteration 171/1000 | Loss: 0.00029635
Iteration 172/1000 | Loss: 0.00013911
Iteration 173/1000 | Loss: 0.00021343
Iteration 174/1000 | Loss: 0.00008063
Iteration 175/1000 | Loss: 0.00007593
Iteration 176/1000 | Loss: 0.00018909
Iteration 177/1000 | Loss: 0.00012947
Iteration 178/1000 | Loss: 0.00015321
Iteration 179/1000 | Loss: 0.00011759
Iteration 180/1000 | Loss: 0.00027109
Iteration 181/1000 | Loss: 0.00020579
Iteration 182/1000 | Loss: 0.00013513
Iteration 183/1000 | Loss: 0.00007648
Iteration 184/1000 | Loss: 0.00007464
Iteration 185/1000 | Loss: 0.00007452
Iteration 186/1000 | Loss: 0.00007366
Iteration 187/1000 | Loss: 0.00007354
Iteration 188/1000 | Loss: 0.00008101
Iteration 189/1000 | Loss: 0.00007518
Iteration 190/1000 | Loss: 0.00011826
Iteration 191/1000 | Loss: 0.00008584
Iteration 192/1000 | Loss: 0.00007224
Iteration 193/1000 | Loss: 0.00007201
Iteration 194/1000 | Loss: 0.00007200
Iteration 195/1000 | Loss: 0.00007200
Iteration 196/1000 | Loss: 0.00007200
Iteration 197/1000 | Loss: 0.00007200
Iteration 198/1000 | Loss: 0.00007200
Iteration 199/1000 | Loss: 0.00007200
Iteration 200/1000 | Loss: 0.00007200
Iteration 201/1000 | Loss: 0.00007200
Iteration 202/1000 | Loss: 0.00007200
Iteration 203/1000 | Loss: 0.00007200
Iteration 204/1000 | Loss: 0.00007200
Iteration 205/1000 | Loss: 0.00007200
Iteration 206/1000 | Loss: 0.00007199
Iteration 207/1000 | Loss: 0.00007199
Iteration 208/1000 | Loss: 0.00007198
Iteration 209/1000 | Loss: 0.00007610
Iteration 210/1000 | Loss: 0.00007625
Iteration 211/1000 | Loss: 0.00007195
Iteration 212/1000 | Loss: 0.00007246
Iteration 213/1000 | Loss: 0.00007226
Iteration 214/1000 | Loss: 0.00007364
Iteration 215/1000 | Loss: 0.00008891
Iteration 216/1000 | Loss: 0.00007202
Iteration 217/1000 | Loss: 0.00007182
Iteration 218/1000 | Loss: 0.00007182
Iteration 219/1000 | Loss: 0.00007181
Iteration 220/1000 | Loss: 0.00018918
Iteration 221/1000 | Loss: 0.00008093
Iteration 222/1000 | Loss: 0.00008435
Iteration 223/1000 | Loss: 0.00035853
Iteration 224/1000 | Loss: 0.00031331
Iteration 225/1000 | Loss: 0.00031896
Iteration 226/1000 | Loss: 0.00012091
Iteration 227/1000 | Loss: 0.00008805
Iteration 228/1000 | Loss: 0.00007634
Iteration 229/1000 | Loss: 0.00007786
Iteration 230/1000 | Loss: 0.00007700
Iteration 231/1000 | Loss: 0.00008556
Iteration 232/1000 | Loss: 0.00007650
Iteration 233/1000 | Loss: 0.00006896
Iteration 234/1000 | Loss: 0.00008194
Iteration 235/1000 | Loss: 0.00007037
Iteration 236/1000 | Loss: 0.00006918
Iteration 237/1000 | Loss: 0.00007466
Iteration 238/1000 | Loss: 0.00006810
Iteration 239/1000 | Loss: 0.00008434
Iteration 240/1000 | Loss: 0.00006792
Iteration 241/1000 | Loss: 0.00007055
Iteration 242/1000 | Loss: 0.00006779
Iteration 243/1000 | Loss: 0.00006778
Iteration 244/1000 | Loss: 0.00006778
Iteration 245/1000 | Loss: 0.00008747
Iteration 246/1000 | Loss: 0.00008797
Iteration 247/1000 | Loss: 0.00007067
Iteration 248/1000 | Loss: 0.00006773
Iteration 249/1000 | Loss: 0.00007751
Iteration 250/1000 | Loss: 0.00006971
Iteration 251/1000 | Loss: 0.00007359
Iteration 252/1000 | Loss: 0.00006834
Iteration 253/1000 | Loss: 0.00006834
Iteration 254/1000 | Loss: 0.00037103
Iteration 255/1000 | Loss: 0.00062508
Iteration 256/1000 | Loss: 0.00006768
Iteration 257/1000 | Loss: 0.00034152
Iteration 258/1000 | Loss: 0.00046422
Iteration 259/1000 | Loss: 0.00033224
Iteration 260/1000 | Loss: 0.00013266
Iteration 261/1000 | Loss: 0.00007351
Iteration 262/1000 | Loss: 0.00007183
Iteration 263/1000 | Loss: 0.00006843
Iteration 264/1000 | Loss: 0.00007593
Iteration 265/1000 | Loss: 0.00009472
Iteration 266/1000 | Loss: 0.00010534
Iteration 267/1000 | Loss: 0.00012681
Iteration 268/1000 | Loss: 0.00007998
Iteration 269/1000 | Loss: 0.00006655
Iteration 270/1000 | Loss: 0.00006633
Iteration 271/1000 | Loss: 0.00008736
Iteration 272/1000 | Loss: 0.00007311
Iteration 273/1000 | Loss: 0.00006685
Iteration 274/1000 | Loss: 0.00006714
Iteration 275/1000 | Loss: 0.00006845
Iteration 276/1000 | Loss: 0.00006720
Iteration 277/1000 | Loss: 0.00006617
Iteration 278/1000 | Loss: 0.00006606
Iteration 279/1000 | Loss: 0.00006606
Iteration 280/1000 | Loss: 0.00006606
Iteration 281/1000 | Loss: 0.00006606
Iteration 282/1000 | Loss: 0.00006606
Iteration 283/1000 | Loss: 0.00006606
Iteration 284/1000 | Loss: 0.00006606
Iteration 285/1000 | Loss: 0.00006606
Iteration 286/1000 | Loss: 0.00006606
Iteration 287/1000 | Loss: 0.00006606
Iteration 288/1000 | Loss: 0.00006606
Iteration 289/1000 | Loss: 0.00006606
Iteration 290/1000 | Loss: 0.00006606
Iteration 291/1000 | Loss: 0.00006606
Iteration 292/1000 | Loss: 0.00006606
Iteration 293/1000 | Loss: 0.00006606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [6.605893577216193e-05, 6.605893577216193e-05, 6.605893577216193e-05, 6.605893577216193e-05, 6.605893577216193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.605893577216193e-05

Optimization complete. Final v2v error: 4.203488349914551 mm

Highest mean error: 11.754676818847656 mm for frame 145

Lowest mean error: 2.7838470935821533 mm for frame 100

Saving results

Total time: 438.12701439857483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019361
Iteration 2/25 | Loss: 0.00180925
Iteration 3/25 | Loss: 0.00142922
Iteration 4/25 | Loss: 0.00133274
Iteration 5/25 | Loss: 0.00138656
Iteration 6/25 | Loss: 0.00132194
Iteration 7/25 | Loss: 0.00132880
Iteration 8/25 | Loss: 0.00125686
Iteration 9/25 | Loss: 0.00125432
Iteration 10/25 | Loss: 0.00121940
Iteration 11/25 | Loss: 0.00120254
Iteration 12/25 | Loss: 0.00116897
Iteration 13/25 | Loss: 0.00115346
Iteration 14/25 | Loss: 0.00115536
Iteration 15/25 | Loss: 0.00114442
Iteration 16/25 | Loss: 0.00114920
Iteration 17/25 | Loss: 0.00114033
Iteration 18/25 | Loss: 0.00114501
Iteration 19/25 | Loss: 0.00114544
Iteration 20/25 | Loss: 0.00114443
Iteration 21/25 | Loss: 0.00113918
Iteration 22/25 | Loss: 0.00114426
Iteration 23/25 | Loss: 0.00115114
Iteration 24/25 | Loss: 0.00114580
Iteration 25/25 | Loss: 0.00114065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41135716
Iteration 2/25 | Loss: 0.00118905
Iteration 3/25 | Loss: 0.00118905
Iteration 4/25 | Loss: 0.00118905
Iteration 5/25 | Loss: 0.00118905
Iteration 6/25 | Loss: 0.00118905
Iteration 7/25 | Loss: 0.00118905
Iteration 8/25 | Loss: 0.00118905
Iteration 9/25 | Loss: 0.00118905
Iteration 10/25 | Loss: 0.00118905
Iteration 11/25 | Loss: 0.00118905
Iteration 12/25 | Loss: 0.00118905
Iteration 13/25 | Loss: 0.00118905
Iteration 14/25 | Loss: 0.00118905
Iteration 15/25 | Loss: 0.00118905
Iteration 16/25 | Loss: 0.00118905
Iteration 17/25 | Loss: 0.00118905
Iteration 18/25 | Loss: 0.00118905
Iteration 19/25 | Loss: 0.00118905
Iteration 20/25 | Loss: 0.00118905
Iteration 21/25 | Loss: 0.00118905
Iteration 22/25 | Loss: 0.00118905
Iteration 23/25 | Loss: 0.00118905
Iteration 24/25 | Loss: 0.00118905
Iteration 25/25 | Loss: 0.00118905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118905
Iteration 2/1000 | Loss: 0.00070894
Iteration 3/1000 | Loss: 0.00022039
Iteration 4/1000 | Loss: 0.00019199
Iteration 5/1000 | Loss: 0.00013451
Iteration 6/1000 | Loss: 0.00004707
Iteration 7/1000 | Loss: 0.00075882
Iteration 8/1000 | Loss: 0.00066592
Iteration 9/1000 | Loss: 0.00112078
Iteration 10/1000 | Loss: 0.00067870
Iteration 11/1000 | Loss: 0.00039782
Iteration 12/1000 | Loss: 0.00021721
Iteration 13/1000 | Loss: 0.00017379
Iteration 14/1000 | Loss: 0.00038861
Iteration 15/1000 | Loss: 0.00048726
Iteration 16/1000 | Loss: 0.00045265
Iteration 17/1000 | Loss: 0.00017781
Iteration 18/1000 | Loss: 0.00018492
Iteration 19/1000 | Loss: 0.00023154
Iteration 20/1000 | Loss: 0.00008761
Iteration 21/1000 | Loss: 0.00025610
Iteration 22/1000 | Loss: 0.00021554
Iteration 23/1000 | Loss: 0.00035420
Iteration 24/1000 | Loss: 0.00033293
Iteration 25/1000 | Loss: 0.00013494
Iteration 26/1000 | Loss: 0.00011100
Iteration 27/1000 | Loss: 0.00055847
Iteration 28/1000 | Loss: 0.00044886
Iteration 29/1000 | Loss: 0.00026405
Iteration 30/1000 | Loss: 0.00014699
Iteration 31/1000 | Loss: 0.00080842
Iteration 32/1000 | Loss: 0.00078808
Iteration 33/1000 | Loss: 0.00070629
Iteration 34/1000 | Loss: 0.00058268
Iteration 35/1000 | Loss: 0.00042278
Iteration 36/1000 | Loss: 0.00047225
Iteration 37/1000 | Loss: 0.00066823
Iteration 38/1000 | Loss: 0.00059008
Iteration 39/1000 | Loss: 0.00044008
Iteration 40/1000 | Loss: 0.00023767
Iteration 41/1000 | Loss: 0.00020545
Iteration 42/1000 | Loss: 0.00007976
Iteration 43/1000 | Loss: 0.00006875
Iteration 44/1000 | Loss: 0.00019794
Iteration 45/1000 | Loss: 0.00019766
Iteration 46/1000 | Loss: 0.00004099
Iteration 47/1000 | Loss: 0.00050348
Iteration 48/1000 | Loss: 0.00071803
Iteration 49/1000 | Loss: 0.00081409
Iteration 50/1000 | Loss: 0.00020887
Iteration 51/1000 | Loss: 0.00008061
Iteration 52/1000 | Loss: 0.00016819
Iteration 53/1000 | Loss: 0.00004038
Iteration 54/1000 | Loss: 0.00003458
Iteration 55/1000 | Loss: 0.00003111
Iteration 56/1000 | Loss: 0.00045860
Iteration 57/1000 | Loss: 0.00055483
Iteration 58/1000 | Loss: 0.00027884
Iteration 59/1000 | Loss: 0.00022103
Iteration 60/1000 | Loss: 0.00021937
Iteration 61/1000 | Loss: 0.00019332
Iteration 62/1000 | Loss: 0.00018296
Iteration 63/1000 | Loss: 0.00034641
Iteration 64/1000 | Loss: 0.00023110
Iteration 65/1000 | Loss: 0.00033264
Iteration 66/1000 | Loss: 0.00050989
Iteration 67/1000 | Loss: 0.00058437
Iteration 68/1000 | Loss: 0.00030102
Iteration 69/1000 | Loss: 0.00019408
Iteration 70/1000 | Loss: 0.00030717
Iteration 71/1000 | Loss: 0.00008599
Iteration 72/1000 | Loss: 0.00011558
Iteration 73/1000 | Loss: 0.00002991
Iteration 74/1000 | Loss: 0.00039929
Iteration 75/1000 | Loss: 0.00004211
Iteration 76/1000 | Loss: 0.00043112
Iteration 77/1000 | Loss: 0.00020684
Iteration 78/1000 | Loss: 0.00032602
Iteration 79/1000 | Loss: 0.00025957
Iteration 80/1000 | Loss: 0.00031677
Iteration 81/1000 | Loss: 0.00028394
Iteration 82/1000 | Loss: 0.00019804
Iteration 83/1000 | Loss: 0.00025408
Iteration 84/1000 | Loss: 0.00035419
Iteration 85/1000 | Loss: 0.00019115
Iteration 86/1000 | Loss: 0.00022636
Iteration 87/1000 | Loss: 0.00003252
Iteration 88/1000 | Loss: 0.00002939
Iteration 89/1000 | Loss: 0.00013535
Iteration 90/1000 | Loss: 0.00002615
Iteration 91/1000 | Loss: 0.00002352
Iteration 92/1000 | Loss: 0.00002145
Iteration 93/1000 | Loss: 0.00001953
Iteration 94/1000 | Loss: 0.00030379
Iteration 95/1000 | Loss: 0.00002247
Iteration 96/1000 | Loss: 0.00001753
Iteration 97/1000 | Loss: 0.00001588
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001262
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001199
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001193
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001191
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00023691
Iteration 117/1000 | Loss: 0.00018882
Iteration 118/1000 | Loss: 0.00022027
Iteration 119/1000 | Loss: 0.00002748
Iteration 120/1000 | Loss: 0.00002009
Iteration 121/1000 | Loss: 0.00001843
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001656
Iteration 124/1000 | Loss: 0.00001590
Iteration 125/1000 | Loss: 0.00001541
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001491
Iteration 128/1000 | Loss: 0.00001465
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001393
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00028869
Iteration 136/1000 | Loss: 0.00001824
Iteration 137/1000 | Loss: 0.00001509
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001116
Iteration 145/1000 | Loss: 0.00001116
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001110
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001105
Iteration 151/1000 | Loss: 0.00001104
Iteration 152/1000 | Loss: 0.00001104
Iteration 153/1000 | Loss: 0.00001104
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001103
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001100
Iteration 164/1000 | Loss: 0.00001100
Iteration 165/1000 | Loss: 0.00001099
Iteration 166/1000 | Loss: 0.00001099
Iteration 167/1000 | Loss: 0.00001098
Iteration 168/1000 | Loss: 0.00001098
Iteration 169/1000 | Loss: 0.00001098
Iteration 170/1000 | Loss: 0.00001098
Iteration 171/1000 | Loss: 0.00001098
Iteration 172/1000 | Loss: 0.00001098
Iteration 173/1000 | Loss: 0.00001097
Iteration 174/1000 | Loss: 0.00001097
Iteration 175/1000 | Loss: 0.00001097
Iteration 176/1000 | Loss: 0.00001097
Iteration 177/1000 | Loss: 0.00001097
Iteration 178/1000 | Loss: 0.00001096
Iteration 179/1000 | Loss: 0.00001096
Iteration 180/1000 | Loss: 0.00001096
Iteration 181/1000 | Loss: 0.00001096
Iteration 182/1000 | Loss: 0.00001095
Iteration 183/1000 | Loss: 0.00001095
Iteration 184/1000 | Loss: 0.00001095
Iteration 185/1000 | Loss: 0.00001095
Iteration 186/1000 | Loss: 0.00001095
Iteration 187/1000 | Loss: 0.00001094
Iteration 188/1000 | Loss: 0.00001094
Iteration 189/1000 | Loss: 0.00001094
Iteration 190/1000 | Loss: 0.00001094
Iteration 191/1000 | Loss: 0.00001094
Iteration 192/1000 | Loss: 0.00001094
Iteration 193/1000 | Loss: 0.00001094
Iteration 194/1000 | Loss: 0.00001094
Iteration 195/1000 | Loss: 0.00001094
Iteration 196/1000 | Loss: 0.00001094
Iteration 197/1000 | Loss: 0.00001094
Iteration 198/1000 | Loss: 0.00001094
Iteration 199/1000 | Loss: 0.00001094
Iteration 200/1000 | Loss: 0.00001093
Iteration 201/1000 | Loss: 0.00001093
Iteration 202/1000 | Loss: 0.00001093
Iteration 203/1000 | Loss: 0.00001093
Iteration 204/1000 | Loss: 0.00001093
Iteration 205/1000 | Loss: 0.00001093
Iteration 206/1000 | Loss: 0.00001093
Iteration 207/1000 | Loss: 0.00001093
Iteration 208/1000 | Loss: 0.00001093
Iteration 209/1000 | Loss: 0.00001093
Iteration 210/1000 | Loss: 0.00001093
Iteration 211/1000 | Loss: 0.00001093
Iteration 212/1000 | Loss: 0.00001093
Iteration 213/1000 | Loss: 0.00001093
Iteration 214/1000 | Loss: 0.00001093
Iteration 215/1000 | Loss: 0.00001093
Iteration 216/1000 | Loss: 0.00001093
Iteration 217/1000 | Loss: 0.00001093
Iteration 218/1000 | Loss: 0.00001093
Iteration 219/1000 | Loss: 0.00001093
Iteration 220/1000 | Loss: 0.00001093
Iteration 221/1000 | Loss: 0.00001093
Iteration 222/1000 | Loss: 0.00001093
Iteration 223/1000 | Loss: 0.00001093
Iteration 224/1000 | Loss: 0.00001093
Iteration 225/1000 | Loss: 0.00001093
Iteration 226/1000 | Loss: 0.00001093
Iteration 227/1000 | Loss: 0.00001093
Iteration 228/1000 | Loss: 0.00001093
Iteration 229/1000 | Loss: 0.00001093
Iteration 230/1000 | Loss: 0.00001093
Iteration 231/1000 | Loss: 0.00001093
Iteration 232/1000 | Loss: 0.00001093
Iteration 233/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.092614093067823e-05, 1.092614093067823e-05, 1.092614093067823e-05, 1.092614093067823e-05, 1.092614093067823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.092614093067823e-05

Optimization complete. Final v2v error: 2.757667303085327 mm

Highest mean error: 4.274517059326172 mm for frame 83

Lowest mean error: 2.3128135204315186 mm for frame 133

Saving results

Total time: 241.03223848342896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030982
Iteration 2/25 | Loss: 0.00256925
Iteration 3/25 | Loss: 0.00176893
Iteration 4/25 | Loss: 0.00146301
Iteration 5/25 | Loss: 0.00154169
Iteration 6/25 | Loss: 0.00134963
Iteration 7/25 | Loss: 0.00128011
Iteration 8/25 | Loss: 0.00121184
Iteration 9/25 | Loss: 0.00121504
Iteration 10/25 | Loss: 0.00116508
Iteration 11/25 | Loss: 0.00114964
Iteration 12/25 | Loss: 0.00113962
Iteration 13/25 | Loss: 0.00112589
Iteration 14/25 | Loss: 0.00112978
Iteration 15/25 | Loss: 0.00112927
Iteration 16/25 | Loss: 0.00112043
Iteration 17/25 | Loss: 0.00112353
Iteration 18/25 | Loss: 0.00113430
Iteration 19/25 | Loss: 0.00112557
Iteration 20/25 | Loss: 0.00113098
Iteration 21/25 | Loss: 0.00112438
Iteration 22/25 | Loss: 0.00112152
Iteration 23/25 | Loss: 0.00112584
Iteration 24/25 | Loss: 0.00113049
Iteration 25/25 | Loss: 0.00112807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46177721
Iteration 2/25 | Loss: 0.00160090
Iteration 3/25 | Loss: 0.00145389
Iteration 4/25 | Loss: 0.00145389
Iteration 5/25 | Loss: 0.00145389
Iteration 6/25 | Loss: 0.00145389
Iteration 7/25 | Loss: 0.00145389
Iteration 8/25 | Loss: 0.00145389
Iteration 9/25 | Loss: 0.00145388
Iteration 10/25 | Loss: 0.00145388
Iteration 11/25 | Loss: 0.00145388
Iteration 12/25 | Loss: 0.00145388
Iteration 13/25 | Loss: 0.00145388
Iteration 14/25 | Loss: 0.00145388
Iteration 15/25 | Loss: 0.00145388
Iteration 16/25 | Loss: 0.00145388
Iteration 17/25 | Loss: 0.00145388
Iteration 18/25 | Loss: 0.00145388
Iteration 19/25 | Loss: 0.00145388
Iteration 20/25 | Loss: 0.00145388
Iteration 21/25 | Loss: 0.00145388
Iteration 22/25 | Loss: 0.00145388
Iteration 23/25 | Loss: 0.00145388
Iteration 24/25 | Loss: 0.00145388
Iteration 25/25 | Loss: 0.00145388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145388
Iteration 2/1000 | Loss: 0.00033053
Iteration 3/1000 | Loss: 0.00065939
Iteration 4/1000 | Loss: 0.00023702
Iteration 5/1000 | Loss: 0.00041284
Iteration 6/1000 | Loss: 0.00034394
Iteration 7/1000 | Loss: 0.00072957
Iteration 8/1000 | Loss: 0.00056666
Iteration 9/1000 | Loss: 0.00089514
Iteration 10/1000 | Loss: 0.00040410
Iteration 11/1000 | Loss: 0.00076572
Iteration 12/1000 | Loss: 0.00019645
Iteration 13/1000 | Loss: 0.00008678
Iteration 14/1000 | Loss: 0.00040362
Iteration 15/1000 | Loss: 0.00030083
Iteration 16/1000 | Loss: 0.00037953
Iteration 17/1000 | Loss: 0.00031529
Iteration 18/1000 | Loss: 0.00059068
Iteration 19/1000 | Loss: 0.00008913
Iteration 20/1000 | Loss: 0.00003915
Iteration 21/1000 | Loss: 0.00015993
Iteration 22/1000 | Loss: 0.00012351
Iteration 23/1000 | Loss: 0.00014604
Iteration 24/1000 | Loss: 0.00054689
Iteration 25/1000 | Loss: 0.00025637
Iteration 26/1000 | Loss: 0.00049073
Iteration 27/1000 | Loss: 0.00043842
Iteration 28/1000 | Loss: 0.00017931
Iteration 29/1000 | Loss: 0.00058681
Iteration 30/1000 | Loss: 0.00026084
Iteration 31/1000 | Loss: 0.00095333
Iteration 32/1000 | Loss: 0.00035718
Iteration 33/1000 | Loss: 0.00017395
Iteration 34/1000 | Loss: 0.00019256
Iteration 35/1000 | Loss: 0.00008546
Iteration 36/1000 | Loss: 0.00013487
Iteration 37/1000 | Loss: 0.00018274
Iteration 38/1000 | Loss: 0.00022321
Iteration 39/1000 | Loss: 0.00025161
Iteration 40/1000 | Loss: 0.00026945
Iteration 41/1000 | Loss: 0.00013403
Iteration 42/1000 | Loss: 0.00031478
Iteration 43/1000 | Loss: 0.00016838
Iteration 44/1000 | Loss: 0.00013397
Iteration 45/1000 | Loss: 0.00006351
Iteration 46/1000 | Loss: 0.00003068
Iteration 47/1000 | Loss: 0.00020216
Iteration 48/1000 | Loss: 0.00015885
Iteration 49/1000 | Loss: 0.00015675
Iteration 50/1000 | Loss: 0.00003239
Iteration 51/1000 | Loss: 0.00002679
Iteration 52/1000 | Loss: 0.00003096
Iteration 53/1000 | Loss: 0.00002433
Iteration 54/1000 | Loss: 0.00022265
Iteration 55/1000 | Loss: 0.00062611
Iteration 56/1000 | Loss: 0.00062808
Iteration 57/1000 | Loss: 0.00036011
Iteration 58/1000 | Loss: 0.00041014
Iteration 59/1000 | Loss: 0.00035988
Iteration 60/1000 | Loss: 0.00031780
Iteration 61/1000 | Loss: 0.00015508
Iteration 62/1000 | Loss: 0.00002991
Iteration 63/1000 | Loss: 0.00005305
Iteration 64/1000 | Loss: 0.00002409
Iteration 65/1000 | Loss: 0.00013600
Iteration 66/1000 | Loss: 0.00014185
Iteration 67/1000 | Loss: 0.00010617
Iteration 68/1000 | Loss: 0.00003625
Iteration 69/1000 | Loss: 0.00002280
Iteration 70/1000 | Loss: 0.00005042
Iteration 71/1000 | Loss: 0.00044886
Iteration 72/1000 | Loss: 0.00036477
Iteration 73/1000 | Loss: 0.00041107
Iteration 74/1000 | Loss: 0.00069350
Iteration 75/1000 | Loss: 0.00011349
Iteration 76/1000 | Loss: 0.00002892
Iteration 77/1000 | Loss: 0.00005916
Iteration 78/1000 | Loss: 0.00002010
Iteration 79/1000 | Loss: 0.00003184
Iteration 80/1000 | Loss: 0.00024751
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00003751
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00002579
Iteration 87/1000 | Loss: 0.00001104
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00002373
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001068
Iteration 93/1000 | Loss: 0.00001253
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001048
Iteration 99/1000 | Loss: 0.00001048
Iteration 100/1000 | Loss: 0.00001048
Iteration 101/1000 | Loss: 0.00001048
Iteration 102/1000 | Loss: 0.00001048
Iteration 103/1000 | Loss: 0.00001048
Iteration 104/1000 | Loss: 0.00001048
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001047
Iteration 107/1000 | Loss: 0.00001047
Iteration 108/1000 | Loss: 0.00001047
Iteration 109/1000 | Loss: 0.00001047
Iteration 110/1000 | Loss: 0.00001047
Iteration 111/1000 | Loss: 0.00001047
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001047
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001046
Iteration 116/1000 | Loss: 0.00001046
Iteration 117/1000 | Loss: 0.00001046
Iteration 118/1000 | Loss: 0.00001046
Iteration 119/1000 | Loss: 0.00001046
Iteration 120/1000 | Loss: 0.00001046
Iteration 121/1000 | Loss: 0.00001046
Iteration 122/1000 | Loss: 0.00001046
Iteration 123/1000 | Loss: 0.00001045
Iteration 124/1000 | Loss: 0.00001045
Iteration 125/1000 | Loss: 0.00001045
Iteration 126/1000 | Loss: 0.00001045
Iteration 127/1000 | Loss: 0.00001045
Iteration 128/1000 | Loss: 0.00001045
Iteration 129/1000 | Loss: 0.00001045
Iteration 130/1000 | Loss: 0.00001044
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001044
Iteration 133/1000 | Loss: 0.00001044
Iteration 134/1000 | Loss: 0.00001043
Iteration 135/1000 | Loss: 0.00001043
Iteration 136/1000 | Loss: 0.00001043
Iteration 137/1000 | Loss: 0.00001043
Iteration 138/1000 | Loss: 0.00001043
Iteration 139/1000 | Loss: 0.00001043
Iteration 140/1000 | Loss: 0.00001043
Iteration 141/1000 | Loss: 0.00001043
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001042
Iteration 146/1000 | Loss: 0.00001057
Iteration 147/1000 | Loss: 0.00001041
Iteration 148/1000 | Loss: 0.00001040
Iteration 149/1000 | Loss: 0.00001040
Iteration 150/1000 | Loss: 0.00001040
Iteration 151/1000 | Loss: 0.00001040
Iteration 152/1000 | Loss: 0.00001039
Iteration 153/1000 | Loss: 0.00001039
Iteration 154/1000 | Loss: 0.00001039
Iteration 155/1000 | Loss: 0.00001039
Iteration 156/1000 | Loss: 0.00001039
Iteration 157/1000 | Loss: 0.00001039
Iteration 158/1000 | Loss: 0.00001039
Iteration 159/1000 | Loss: 0.00001039
Iteration 160/1000 | Loss: 0.00001039
Iteration 161/1000 | Loss: 0.00001039
Iteration 162/1000 | Loss: 0.00001039
Iteration 163/1000 | Loss: 0.00001039
Iteration 164/1000 | Loss: 0.00001039
Iteration 165/1000 | Loss: 0.00001039
Iteration 166/1000 | Loss: 0.00001039
Iteration 167/1000 | Loss: 0.00001039
Iteration 168/1000 | Loss: 0.00001039
Iteration 169/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.0391501746198628e-05, 1.0391501746198628e-05, 1.0391501746198628e-05, 1.0391501746198628e-05, 1.0391501746198628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0391501746198628e-05

Optimization complete. Final v2v error: 2.6458613872528076 mm

Highest mean error: 5.733851909637451 mm for frame 89

Lowest mean error: 2.382827043533325 mm for frame 97

Saving results

Total time: 187.43722224235535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621846
Iteration 2/25 | Loss: 0.00114856
Iteration 3/25 | Loss: 0.00108079
Iteration 4/25 | Loss: 0.00106916
Iteration 5/25 | Loss: 0.00106537
Iteration 6/25 | Loss: 0.00106477
Iteration 7/25 | Loss: 0.00106477
Iteration 8/25 | Loss: 0.00106477
Iteration 9/25 | Loss: 0.00106477
Iteration 10/25 | Loss: 0.00106477
Iteration 11/25 | Loss: 0.00106477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010647729504853487, 0.0010647729504853487, 0.0010647729504853487, 0.0010647729504853487, 0.0010647729504853487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010647729504853487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93475533
Iteration 2/25 | Loss: 0.00080059
Iteration 3/25 | Loss: 0.00080058
Iteration 4/25 | Loss: 0.00080058
Iteration 5/25 | Loss: 0.00080058
Iteration 6/25 | Loss: 0.00080058
Iteration 7/25 | Loss: 0.00080058
Iteration 8/25 | Loss: 0.00080058
Iteration 9/25 | Loss: 0.00080058
Iteration 10/25 | Loss: 0.00080058
Iteration 11/25 | Loss: 0.00080058
Iteration 12/25 | Loss: 0.00080058
Iteration 13/25 | Loss: 0.00080058
Iteration 14/25 | Loss: 0.00080058
Iteration 15/25 | Loss: 0.00080058
Iteration 16/25 | Loss: 0.00080058
Iteration 17/25 | Loss: 0.00080058
Iteration 18/25 | Loss: 0.00080058
Iteration 19/25 | Loss: 0.00080058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008005777490325272, 0.0008005777490325272, 0.0008005777490325272, 0.0008005777490325272, 0.0008005777490325272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008005777490325272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080058
Iteration 2/1000 | Loss: 0.00002247
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001380
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001229
Iteration 7/1000 | Loss: 0.00001175
Iteration 8/1000 | Loss: 0.00001146
Iteration 9/1000 | Loss: 0.00001113
Iteration 10/1000 | Loss: 0.00001089
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001065
Iteration 15/1000 | Loss: 0.00001064
Iteration 16/1000 | Loss: 0.00001064
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001051
Iteration 21/1000 | Loss: 0.00001050
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001045
Iteration 26/1000 | Loss: 0.00001044
Iteration 27/1000 | Loss: 0.00001043
Iteration 28/1000 | Loss: 0.00001042
Iteration 29/1000 | Loss: 0.00001042
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001041
Iteration 32/1000 | Loss: 0.00001041
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001040
Iteration 35/1000 | Loss: 0.00001040
Iteration 36/1000 | Loss: 0.00001040
Iteration 37/1000 | Loss: 0.00001039
Iteration 38/1000 | Loss: 0.00001038
Iteration 39/1000 | Loss: 0.00001038
Iteration 40/1000 | Loss: 0.00001036
Iteration 41/1000 | Loss: 0.00001035
Iteration 42/1000 | Loss: 0.00001030
Iteration 43/1000 | Loss: 0.00001025
Iteration 44/1000 | Loss: 0.00001024
Iteration 45/1000 | Loss: 0.00001024
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001021
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001021
Iteration 51/1000 | Loss: 0.00001020
Iteration 52/1000 | Loss: 0.00001020
Iteration 53/1000 | Loss: 0.00001020
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001019
Iteration 59/1000 | Loss: 0.00001019
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001016
Iteration 65/1000 | Loss: 0.00001016
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001015
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001014
Iteration 70/1000 | Loss: 0.00001014
Iteration 71/1000 | Loss: 0.00001014
Iteration 72/1000 | Loss: 0.00001013
Iteration 73/1000 | Loss: 0.00001013
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001012
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001010
Iteration 82/1000 | Loss: 0.00001010
Iteration 83/1000 | Loss: 0.00001010
Iteration 84/1000 | Loss: 0.00001009
Iteration 85/1000 | Loss: 0.00001009
Iteration 86/1000 | Loss: 0.00001009
Iteration 87/1000 | Loss: 0.00001009
Iteration 88/1000 | Loss: 0.00001009
Iteration 89/1000 | Loss: 0.00001009
Iteration 90/1000 | Loss: 0.00001009
Iteration 91/1000 | Loss: 0.00001008
Iteration 92/1000 | Loss: 0.00001008
Iteration 93/1000 | Loss: 0.00001008
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001007
Iteration 96/1000 | Loss: 0.00001007
Iteration 97/1000 | Loss: 0.00001006
Iteration 98/1000 | Loss: 0.00001006
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001006
Iteration 101/1000 | Loss: 0.00001006
Iteration 102/1000 | Loss: 0.00001006
Iteration 103/1000 | Loss: 0.00001006
Iteration 104/1000 | Loss: 0.00001005
Iteration 105/1000 | Loss: 0.00001005
Iteration 106/1000 | Loss: 0.00001005
Iteration 107/1000 | Loss: 0.00001005
Iteration 108/1000 | Loss: 0.00001004
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001004
Iteration 111/1000 | Loss: 0.00001004
Iteration 112/1000 | Loss: 0.00001004
Iteration 113/1000 | Loss: 0.00001004
Iteration 114/1000 | Loss: 0.00001004
Iteration 115/1000 | Loss: 0.00001004
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001003
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001002
Iteration 123/1000 | Loss: 0.00001002
Iteration 124/1000 | Loss: 0.00001002
Iteration 125/1000 | Loss: 0.00001002
Iteration 126/1000 | Loss: 0.00001002
Iteration 127/1000 | Loss: 0.00001002
Iteration 128/1000 | Loss: 0.00001002
Iteration 129/1000 | Loss: 0.00001002
Iteration 130/1000 | Loss: 0.00001002
Iteration 131/1000 | Loss: 0.00001002
Iteration 132/1000 | Loss: 0.00001002
Iteration 133/1000 | Loss: 0.00001001
Iteration 134/1000 | Loss: 0.00001001
Iteration 135/1000 | Loss: 0.00001001
Iteration 136/1000 | Loss: 0.00001001
Iteration 137/1000 | Loss: 0.00001001
Iteration 138/1000 | Loss: 0.00001001
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00001000
Iteration 143/1000 | Loss: 0.00001000
Iteration 144/1000 | Loss: 0.00001000
Iteration 145/1000 | Loss: 0.00001000
Iteration 146/1000 | Loss: 0.00001000
Iteration 147/1000 | Loss: 0.00001000
Iteration 148/1000 | Loss: 0.00000999
Iteration 149/1000 | Loss: 0.00000999
Iteration 150/1000 | Loss: 0.00000999
Iteration 151/1000 | Loss: 0.00000999
Iteration 152/1000 | Loss: 0.00000999
Iteration 153/1000 | Loss: 0.00000999
Iteration 154/1000 | Loss: 0.00000998
Iteration 155/1000 | Loss: 0.00000998
Iteration 156/1000 | Loss: 0.00000998
Iteration 157/1000 | Loss: 0.00000998
Iteration 158/1000 | Loss: 0.00000998
Iteration 159/1000 | Loss: 0.00000998
Iteration 160/1000 | Loss: 0.00000998
Iteration 161/1000 | Loss: 0.00000998
Iteration 162/1000 | Loss: 0.00000998
Iteration 163/1000 | Loss: 0.00000998
Iteration 164/1000 | Loss: 0.00000997
Iteration 165/1000 | Loss: 0.00000997
Iteration 166/1000 | Loss: 0.00000997
Iteration 167/1000 | Loss: 0.00000997
Iteration 168/1000 | Loss: 0.00000997
Iteration 169/1000 | Loss: 0.00000997
Iteration 170/1000 | Loss: 0.00000997
Iteration 171/1000 | Loss: 0.00000997
Iteration 172/1000 | Loss: 0.00000997
Iteration 173/1000 | Loss: 0.00000997
Iteration 174/1000 | Loss: 0.00000997
Iteration 175/1000 | Loss: 0.00000997
Iteration 176/1000 | Loss: 0.00000997
Iteration 177/1000 | Loss: 0.00000997
Iteration 178/1000 | Loss: 0.00000997
Iteration 179/1000 | Loss: 0.00000997
Iteration 180/1000 | Loss: 0.00000997
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [9.967951882572379e-06, 9.967951882572379e-06, 9.967951882572379e-06, 9.967951882572379e-06, 9.967951882572379e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.967951882572379e-06

Optimization complete. Final v2v error: 2.737577438354492 mm

Highest mean error: 3.177716016769409 mm for frame 106

Lowest mean error: 2.509300470352173 mm for frame 145

Saving results

Total time: 41.38052177429199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713013
Iteration 2/25 | Loss: 0.00155169
Iteration 3/25 | Loss: 0.00127460
Iteration 4/25 | Loss: 0.00123719
Iteration 5/25 | Loss: 0.00122645
Iteration 6/25 | Loss: 0.00122410
Iteration 7/25 | Loss: 0.00122270
Iteration 8/25 | Loss: 0.00122172
Iteration 9/25 | Loss: 0.00122088
Iteration 10/25 | Loss: 0.00122402
Iteration 11/25 | Loss: 0.00121906
Iteration 12/25 | Loss: 0.00121811
Iteration 13/25 | Loss: 0.00121782
Iteration 14/25 | Loss: 0.00121771
Iteration 15/25 | Loss: 0.00121769
Iteration 16/25 | Loss: 0.00121768
Iteration 17/25 | Loss: 0.00121768
Iteration 18/25 | Loss: 0.00121768
Iteration 19/25 | Loss: 0.00121768
Iteration 20/25 | Loss: 0.00121768
Iteration 21/25 | Loss: 0.00121768
Iteration 22/25 | Loss: 0.00121768
Iteration 23/25 | Loss: 0.00121767
Iteration 24/25 | Loss: 0.00121767
Iteration 25/25 | Loss: 0.00121767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25830817
Iteration 2/25 | Loss: 0.00108382
Iteration 3/25 | Loss: 0.00108378
Iteration 4/25 | Loss: 0.00108378
Iteration 5/25 | Loss: 0.00108378
Iteration 6/25 | Loss: 0.00108378
Iteration 7/25 | Loss: 0.00108378
Iteration 8/25 | Loss: 0.00108378
Iteration 9/25 | Loss: 0.00108378
Iteration 10/25 | Loss: 0.00108378
Iteration 11/25 | Loss: 0.00108378
Iteration 12/25 | Loss: 0.00108378
Iteration 13/25 | Loss: 0.00108378
Iteration 14/25 | Loss: 0.00108378
Iteration 15/25 | Loss: 0.00108378
Iteration 16/25 | Loss: 0.00108378
Iteration 17/25 | Loss: 0.00108378
Iteration 18/25 | Loss: 0.00108378
Iteration 19/25 | Loss: 0.00108378
Iteration 20/25 | Loss: 0.00108378
Iteration 21/25 | Loss: 0.00108378
Iteration 22/25 | Loss: 0.00108378
Iteration 23/25 | Loss: 0.00108378
Iteration 24/25 | Loss: 0.00108378
Iteration 25/25 | Loss: 0.00108378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108378
Iteration 2/1000 | Loss: 0.00008390
Iteration 3/1000 | Loss: 0.00005421
Iteration 4/1000 | Loss: 0.00004788
Iteration 5/1000 | Loss: 0.00004584
Iteration 6/1000 | Loss: 0.00004432
Iteration 7/1000 | Loss: 0.00004352
Iteration 8/1000 | Loss: 0.00004302
Iteration 9/1000 | Loss: 0.00004256
Iteration 10/1000 | Loss: 0.00004212
Iteration 11/1000 | Loss: 0.00004181
Iteration 12/1000 | Loss: 0.00004152
Iteration 13/1000 | Loss: 0.00004120
Iteration 14/1000 | Loss: 0.00004101
Iteration 15/1000 | Loss: 0.00004086
Iteration 16/1000 | Loss: 0.00004078
Iteration 17/1000 | Loss: 0.00004076
Iteration 18/1000 | Loss: 0.00004072
Iteration 19/1000 | Loss: 0.00004066
Iteration 20/1000 | Loss: 0.00004058
Iteration 21/1000 | Loss: 0.00004053
Iteration 22/1000 | Loss: 0.00004052
Iteration 23/1000 | Loss: 0.00004052
Iteration 24/1000 | Loss: 0.00004051
Iteration 25/1000 | Loss: 0.00004046
Iteration 26/1000 | Loss: 0.00004045
Iteration 27/1000 | Loss: 0.00004044
Iteration 28/1000 | Loss: 0.00004043
Iteration 29/1000 | Loss: 0.00004042
Iteration 30/1000 | Loss: 0.00004042
Iteration 31/1000 | Loss: 0.00004042
Iteration 32/1000 | Loss: 0.00004041
Iteration 33/1000 | Loss: 0.00004040
Iteration 34/1000 | Loss: 0.00004035
Iteration 35/1000 | Loss: 0.00004033
Iteration 36/1000 | Loss: 0.00004033
Iteration 37/1000 | Loss: 0.00004032
Iteration 38/1000 | Loss: 0.00004030
Iteration 39/1000 | Loss: 0.00004028
Iteration 40/1000 | Loss: 0.00004025
Iteration 41/1000 | Loss: 0.00004025
Iteration 42/1000 | Loss: 0.00004024
Iteration 43/1000 | Loss: 0.00004019
Iteration 44/1000 | Loss: 0.00004018
Iteration 45/1000 | Loss: 0.00004017
Iteration 46/1000 | Loss: 0.00004017
Iteration 47/1000 | Loss: 0.00004016
Iteration 48/1000 | Loss: 0.00004015
Iteration 49/1000 | Loss: 0.00004014
Iteration 50/1000 | Loss: 0.00004011
Iteration 51/1000 | Loss: 0.00004011
Iteration 52/1000 | Loss: 0.00004006
Iteration 53/1000 | Loss: 0.00004006
Iteration 54/1000 | Loss: 0.00004005
Iteration 55/1000 | Loss: 0.00004002
Iteration 56/1000 | Loss: 0.00004002
Iteration 57/1000 | Loss: 0.00004002
Iteration 58/1000 | Loss: 0.00004002
Iteration 59/1000 | Loss: 0.00004002
Iteration 60/1000 | Loss: 0.00004002
Iteration 61/1000 | Loss: 0.00004002
Iteration 62/1000 | Loss: 0.00004001
Iteration 63/1000 | Loss: 0.00004001
Iteration 64/1000 | Loss: 0.00004001
Iteration 65/1000 | Loss: 0.00004001
Iteration 66/1000 | Loss: 0.00004001
Iteration 67/1000 | Loss: 0.00004000
Iteration 68/1000 | Loss: 0.00004000
Iteration 69/1000 | Loss: 0.00004000
Iteration 70/1000 | Loss: 0.00004000
Iteration 71/1000 | Loss: 0.00004000
Iteration 72/1000 | Loss: 0.00004000
Iteration 73/1000 | Loss: 0.00004000
Iteration 74/1000 | Loss: 0.00004000
Iteration 75/1000 | Loss: 0.00004000
Iteration 76/1000 | Loss: 0.00003999
Iteration 77/1000 | Loss: 0.00003999
Iteration 78/1000 | Loss: 0.00003999
Iteration 79/1000 | Loss: 0.00003999
Iteration 80/1000 | Loss: 0.00003999
Iteration 81/1000 | Loss: 0.00003999
Iteration 82/1000 | Loss: 0.00003999
Iteration 83/1000 | Loss: 0.00003998
Iteration 84/1000 | Loss: 0.00003998
Iteration 85/1000 | Loss: 0.00003997
Iteration 86/1000 | Loss: 0.00003997
Iteration 87/1000 | Loss: 0.00003997
Iteration 88/1000 | Loss: 0.00003996
Iteration 89/1000 | Loss: 0.00003996
Iteration 90/1000 | Loss: 0.00003996
Iteration 91/1000 | Loss: 0.00003996
Iteration 92/1000 | Loss: 0.00003996
Iteration 93/1000 | Loss: 0.00003996
Iteration 94/1000 | Loss: 0.00003996
Iteration 95/1000 | Loss: 0.00003996
Iteration 96/1000 | Loss: 0.00003996
Iteration 97/1000 | Loss: 0.00003995
Iteration 98/1000 | Loss: 0.00003995
Iteration 99/1000 | Loss: 0.00003995
Iteration 100/1000 | Loss: 0.00003995
Iteration 101/1000 | Loss: 0.00003995
Iteration 102/1000 | Loss: 0.00003995
Iteration 103/1000 | Loss: 0.00003995
Iteration 104/1000 | Loss: 0.00003994
Iteration 105/1000 | Loss: 0.00003994
Iteration 106/1000 | Loss: 0.00003994
Iteration 107/1000 | Loss: 0.00003994
Iteration 108/1000 | Loss: 0.00003994
Iteration 109/1000 | Loss: 0.00003993
Iteration 110/1000 | Loss: 0.00003993
Iteration 111/1000 | Loss: 0.00003993
Iteration 112/1000 | Loss: 0.00003993
Iteration 113/1000 | Loss: 0.00003993
Iteration 114/1000 | Loss: 0.00003993
Iteration 115/1000 | Loss: 0.00003992
Iteration 116/1000 | Loss: 0.00003992
Iteration 117/1000 | Loss: 0.00003991
Iteration 118/1000 | Loss: 0.00003991
Iteration 119/1000 | Loss: 0.00003991
Iteration 120/1000 | Loss: 0.00003991
Iteration 121/1000 | Loss: 0.00003991
Iteration 122/1000 | Loss: 0.00003991
Iteration 123/1000 | Loss: 0.00003991
Iteration 124/1000 | Loss: 0.00003990
Iteration 125/1000 | Loss: 0.00003990
Iteration 126/1000 | Loss: 0.00003990
Iteration 127/1000 | Loss: 0.00003990
Iteration 128/1000 | Loss: 0.00003989
Iteration 129/1000 | Loss: 0.00003989
Iteration 130/1000 | Loss: 0.00003989
Iteration 131/1000 | Loss: 0.00003988
Iteration 132/1000 | Loss: 0.00003988
Iteration 133/1000 | Loss: 0.00003988
Iteration 134/1000 | Loss: 0.00003987
Iteration 135/1000 | Loss: 0.00003987
Iteration 136/1000 | Loss: 0.00003987
Iteration 137/1000 | Loss: 0.00003986
Iteration 138/1000 | Loss: 0.00003986
Iteration 139/1000 | Loss: 0.00003986
Iteration 140/1000 | Loss: 0.00003986
Iteration 141/1000 | Loss: 0.00003985
Iteration 142/1000 | Loss: 0.00003985
Iteration 143/1000 | Loss: 0.00003985
Iteration 144/1000 | Loss: 0.00003984
Iteration 145/1000 | Loss: 0.00003984
Iteration 146/1000 | Loss: 0.00003984
Iteration 147/1000 | Loss: 0.00003983
Iteration 148/1000 | Loss: 0.00003982
Iteration 149/1000 | Loss: 0.00003982
Iteration 150/1000 | Loss: 0.00003982
Iteration 151/1000 | Loss: 0.00003982
Iteration 152/1000 | Loss: 0.00003982
Iteration 153/1000 | Loss: 0.00003981
Iteration 154/1000 | Loss: 0.00003981
Iteration 155/1000 | Loss: 0.00003981
Iteration 156/1000 | Loss: 0.00003981
Iteration 157/1000 | Loss: 0.00003981
Iteration 158/1000 | Loss: 0.00003981
Iteration 159/1000 | Loss: 0.00003981
Iteration 160/1000 | Loss: 0.00003980
Iteration 161/1000 | Loss: 0.00003980
Iteration 162/1000 | Loss: 0.00003979
Iteration 163/1000 | Loss: 0.00003979
Iteration 164/1000 | Loss: 0.00003979
Iteration 165/1000 | Loss: 0.00003978
Iteration 166/1000 | Loss: 0.00003978
Iteration 167/1000 | Loss: 0.00003977
Iteration 168/1000 | Loss: 0.00003977
Iteration 169/1000 | Loss: 0.00003977
Iteration 170/1000 | Loss: 0.00003977
Iteration 171/1000 | Loss: 0.00003977
Iteration 172/1000 | Loss: 0.00003977
Iteration 173/1000 | Loss: 0.00003976
Iteration 174/1000 | Loss: 0.00003976
Iteration 175/1000 | Loss: 0.00003976
Iteration 176/1000 | Loss: 0.00003976
Iteration 177/1000 | Loss: 0.00003976
Iteration 178/1000 | Loss: 0.00003976
Iteration 179/1000 | Loss: 0.00003976
Iteration 180/1000 | Loss: 0.00003976
Iteration 181/1000 | Loss: 0.00003976
Iteration 182/1000 | Loss: 0.00003976
Iteration 183/1000 | Loss: 0.00003975
Iteration 184/1000 | Loss: 0.00003975
Iteration 185/1000 | Loss: 0.00003975
Iteration 186/1000 | Loss: 0.00003975
Iteration 187/1000 | Loss: 0.00003974
Iteration 188/1000 | Loss: 0.00003974
Iteration 189/1000 | Loss: 0.00003974
Iteration 190/1000 | Loss: 0.00003974
Iteration 191/1000 | Loss: 0.00003974
Iteration 192/1000 | Loss: 0.00003973
Iteration 193/1000 | Loss: 0.00003973
Iteration 194/1000 | Loss: 0.00003973
Iteration 195/1000 | Loss: 0.00003973
Iteration 196/1000 | Loss: 0.00003973
Iteration 197/1000 | Loss: 0.00003972
Iteration 198/1000 | Loss: 0.00003972
Iteration 199/1000 | Loss: 0.00003972
Iteration 200/1000 | Loss: 0.00003971
Iteration 201/1000 | Loss: 0.00003971
Iteration 202/1000 | Loss: 0.00003971
Iteration 203/1000 | Loss: 0.00003971
Iteration 204/1000 | Loss: 0.00003971
Iteration 205/1000 | Loss: 0.00003971
Iteration 206/1000 | Loss: 0.00003971
Iteration 207/1000 | Loss: 0.00003971
Iteration 208/1000 | Loss: 0.00003970
Iteration 209/1000 | Loss: 0.00003970
Iteration 210/1000 | Loss: 0.00003970
Iteration 211/1000 | Loss: 0.00003969
Iteration 212/1000 | Loss: 0.00003969
Iteration 213/1000 | Loss: 0.00003969
Iteration 214/1000 | Loss: 0.00003969
Iteration 215/1000 | Loss: 0.00003968
Iteration 216/1000 | Loss: 0.00003968
Iteration 217/1000 | Loss: 0.00003967
Iteration 218/1000 | Loss: 0.00003967
Iteration 219/1000 | Loss: 0.00003967
Iteration 220/1000 | Loss: 0.00003966
Iteration 221/1000 | Loss: 0.00003966
Iteration 222/1000 | Loss: 0.00003966
Iteration 223/1000 | Loss: 0.00003965
Iteration 224/1000 | Loss: 0.00003965
Iteration 225/1000 | Loss: 0.00003965
Iteration 226/1000 | Loss: 0.00003965
Iteration 227/1000 | Loss: 0.00003964
Iteration 228/1000 | Loss: 0.00003964
Iteration 229/1000 | Loss: 0.00003964
Iteration 230/1000 | Loss: 0.00003964
Iteration 231/1000 | Loss: 0.00003964
Iteration 232/1000 | Loss: 0.00003963
Iteration 233/1000 | Loss: 0.00003963
Iteration 234/1000 | Loss: 0.00003963
Iteration 235/1000 | Loss: 0.00003963
Iteration 236/1000 | Loss: 0.00003963
Iteration 237/1000 | Loss: 0.00003963
Iteration 238/1000 | Loss: 0.00003963
Iteration 239/1000 | Loss: 0.00003962
Iteration 240/1000 | Loss: 0.00003962
Iteration 241/1000 | Loss: 0.00003962
Iteration 242/1000 | Loss: 0.00003962
Iteration 243/1000 | Loss: 0.00003961
Iteration 244/1000 | Loss: 0.00003961
Iteration 245/1000 | Loss: 0.00003961
Iteration 246/1000 | Loss: 0.00003961
Iteration 247/1000 | Loss: 0.00003961
Iteration 248/1000 | Loss: 0.00003960
Iteration 249/1000 | Loss: 0.00003960
Iteration 250/1000 | Loss: 0.00003960
Iteration 251/1000 | Loss: 0.00003960
Iteration 252/1000 | Loss: 0.00003960
Iteration 253/1000 | Loss: 0.00003959
Iteration 254/1000 | Loss: 0.00003959
Iteration 255/1000 | Loss: 0.00003959
Iteration 256/1000 | Loss: 0.00003958
Iteration 257/1000 | Loss: 0.00003958
Iteration 258/1000 | Loss: 0.00003958
Iteration 259/1000 | Loss: 0.00003957
Iteration 260/1000 | Loss: 0.00003957
Iteration 261/1000 | Loss: 0.00003957
Iteration 262/1000 | Loss: 0.00003957
Iteration 263/1000 | Loss: 0.00003956
Iteration 264/1000 | Loss: 0.00003956
Iteration 265/1000 | Loss: 0.00003956
Iteration 266/1000 | Loss: 0.00003955
Iteration 267/1000 | Loss: 0.00003955
Iteration 268/1000 | Loss: 0.00003955
Iteration 269/1000 | Loss: 0.00003954
Iteration 270/1000 | Loss: 0.00003954
Iteration 271/1000 | Loss: 0.00003954
Iteration 272/1000 | Loss: 0.00003954
Iteration 273/1000 | Loss: 0.00003953
Iteration 274/1000 | Loss: 0.00003953
Iteration 275/1000 | Loss: 0.00003953
Iteration 276/1000 | Loss: 0.00003953
Iteration 277/1000 | Loss: 0.00003952
Iteration 278/1000 | Loss: 0.00003952
Iteration 279/1000 | Loss: 0.00003952
Iteration 280/1000 | Loss: 0.00003952
Iteration 281/1000 | Loss: 0.00003952
Iteration 282/1000 | Loss: 0.00003952
Iteration 283/1000 | Loss: 0.00003952
Iteration 284/1000 | Loss: 0.00003952
Iteration 285/1000 | Loss: 0.00003952
Iteration 286/1000 | Loss: 0.00003951
Iteration 287/1000 | Loss: 0.00003951
Iteration 288/1000 | Loss: 0.00003951
Iteration 289/1000 | Loss: 0.00003951
Iteration 290/1000 | Loss: 0.00003951
Iteration 291/1000 | Loss: 0.00003951
Iteration 292/1000 | Loss: 0.00003951
Iteration 293/1000 | Loss: 0.00003950
Iteration 294/1000 | Loss: 0.00003950
Iteration 295/1000 | Loss: 0.00003950
Iteration 296/1000 | Loss: 0.00003950
Iteration 297/1000 | Loss: 0.00003950
Iteration 298/1000 | Loss: 0.00003949
Iteration 299/1000 | Loss: 0.00003949
Iteration 300/1000 | Loss: 0.00003949
Iteration 301/1000 | Loss: 0.00003949
Iteration 302/1000 | Loss: 0.00003948
Iteration 303/1000 | Loss: 0.00003948
Iteration 304/1000 | Loss: 0.00003948
Iteration 305/1000 | Loss: 0.00003948
Iteration 306/1000 | Loss: 0.00003948
Iteration 307/1000 | Loss: 0.00003947
Iteration 308/1000 | Loss: 0.00003947
Iteration 309/1000 | Loss: 0.00003947
Iteration 310/1000 | Loss: 0.00003947
Iteration 311/1000 | Loss: 0.00003947
Iteration 312/1000 | Loss: 0.00003947
Iteration 313/1000 | Loss: 0.00003947
Iteration 314/1000 | Loss: 0.00003947
Iteration 315/1000 | Loss: 0.00003947
Iteration 316/1000 | Loss: 0.00003947
Iteration 317/1000 | Loss: 0.00003947
Iteration 318/1000 | Loss: 0.00003947
Iteration 319/1000 | Loss: 0.00003947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 319. Stopping optimization.
Last 5 losses: [3.946637298213318e-05, 3.946637298213318e-05, 3.946637298213318e-05, 3.946637298213318e-05, 3.946637298213318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.946637298213318e-05

Optimization complete. Final v2v error: 3.881769895553589 mm

Highest mean error: 11.498014450073242 mm for frame 70

Lowest mean error: 2.8337297439575195 mm for frame 36

Saving results

Total time: 87.7676169872284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487282
Iteration 2/25 | Loss: 0.00119841
Iteration 3/25 | Loss: 0.00111839
Iteration 4/25 | Loss: 0.00110784
Iteration 5/25 | Loss: 0.00110420
Iteration 6/25 | Loss: 0.00110357
Iteration 7/25 | Loss: 0.00110357
Iteration 8/25 | Loss: 0.00110357
Iteration 9/25 | Loss: 0.00110357
Iteration 10/25 | Loss: 0.00110357
Iteration 11/25 | Loss: 0.00110357
Iteration 12/25 | Loss: 0.00110357
Iteration 13/25 | Loss: 0.00110357
Iteration 14/25 | Loss: 0.00110357
Iteration 15/25 | Loss: 0.00110357
Iteration 16/25 | Loss: 0.00110357
Iteration 17/25 | Loss: 0.00110357
Iteration 18/25 | Loss: 0.00110357
Iteration 19/25 | Loss: 0.00110357
Iteration 20/25 | Loss: 0.00110357
Iteration 21/25 | Loss: 0.00110357
Iteration 22/25 | Loss: 0.00110357
Iteration 23/25 | Loss: 0.00110357
Iteration 24/25 | Loss: 0.00110357
Iteration 25/25 | Loss: 0.00110357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33143342
Iteration 2/25 | Loss: 0.00076322
Iteration 3/25 | Loss: 0.00076320
Iteration 4/25 | Loss: 0.00076319
Iteration 5/25 | Loss: 0.00076319
Iteration 6/25 | Loss: 0.00076319
Iteration 7/25 | Loss: 0.00076319
Iteration 8/25 | Loss: 0.00076319
Iteration 9/25 | Loss: 0.00076319
Iteration 10/25 | Loss: 0.00076319
Iteration 11/25 | Loss: 0.00076319
Iteration 12/25 | Loss: 0.00076319
Iteration 13/25 | Loss: 0.00076319
Iteration 14/25 | Loss: 0.00076319
Iteration 15/25 | Loss: 0.00076319
Iteration 16/25 | Loss: 0.00076319
Iteration 17/25 | Loss: 0.00076319
Iteration 18/25 | Loss: 0.00076319
Iteration 19/25 | Loss: 0.00076319
Iteration 20/25 | Loss: 0.00076319
Iteration 21/25 | Loss: 0.00076319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007631923072040081, 0.0007631923072040081, 0.0007631923072040081, 0.0007631923072040081, 0.0007631923072040081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007631923072040081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076319
Iteration 2/1000 | Loss: 0.00002697
Iteration 3/1000 | Loss: 0.00002077
Iteration 4/1000 | Loss: 0.00001822
Iteration 5/1000 | Loss: 0.00001725
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00001603
Iteration 8/1000 | Loss: 0.00001555
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001514
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001484
Iteration 13/1000 | Loss: 0.00001475
Iteration 14/1000 | Loss: 0.00001468
Iteration 15/1000 | Loss: 0.00001463
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001459
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001449
Iteration 21/1000 | Loss: 0.00001448
Iteration 22/1000 | Loss: 0.00001444
Iteration 23/1000 | Loss: 0.00001443
Iteration 24/1000 | Loss: 0.00001442
Iteration 25/1000 | Loss: 0.00001442
Iteration 26/1000 | Loss: 0.00001442
Iteration 27/1000 | Loss: 0.00001442
Iteration 28/1000 | Loss: 0.00001442
Iteration 29/1000 | Loss: 0.00001441
Iteration 30/1000 | Loss: 0.00001441
Iteration 31/1000 | Loss: 0.00001441
Iteration 32/1000 | Loss: 0.00001441
Iteration 33/1000 | Loss: 0.00001440
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001440
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001439
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001438
Iteration 42/1000 | Loss: 0.00001437
Iteration 43/1000 | Loss: 0.00001435
Iteration 44/1000 | Loss: 0.00001435
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001426
Iteration 51/1000 | Loss: 0.00001425
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001421
Iteration 64/1000 | Loss: 0.00001421
Iteration 65/1000 | Loss: 0.00001421
Iteration 66/1000 | Loss: 0.00001421
Iteration 67/1000 | Loss: 0.00001420
Iteration 68/1000 | Loss: 0.00001420
Iteration 69/1000 | Loss: 0.00001420
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001419
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001416
Iteration 79/1000 | Loss: 0.00001416
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001414
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001413
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001411
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001410
Iteration 100/1000 | Loss: 0.00001410
Iteration 101/1000 | Loss: 0.00001410
Iteration 102/1000 | Loss: 0.00001410
Iteration 103/1000 | Loss: 0.00001410
Iteration 104/1000 | Loss: 0.00001410
Iteration 105/1000 | Loss: 0.00001410
Iteration 106/1000 | Loss: 0.00001410
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001410
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001409
Iteration 112/1000 | Loss: 0.00001409
Iteration 113/1000 | Loss: 0.00001408
Iteration 114/1000 | Loss: 0.00001408
Iteration 115/1000 | Loss: 0.00001407
Iteration 116/1000 | Loss: 0.00001407
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001406
Iteration 119/1000 | Loss: 0.00001406
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001406
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001404
Iteration 129/1000 | Loss: 0.00001404
Iteration 130/1000 | Loss: 0.00001403
Iteration 131/1000 | Loss: 0.00001403
Iteration 132/1000 | Loss: 0.00001403
Iteration 133/1000 | Loss: 0.00001403
Iteration 134/1000 | Loss: 0.00001403
Iteration 135/1000 | Loss: 0.00001403
Iteration 136/1000 | Loss: 0.00001403
Iteration 137/1000 | Loss: 0.00001403
Iteration 138/1000 | Loss: 0.00001403
Iteration 139/1000 | Loss: 0.00001403
Iteration 140/1000 | Loss: 0.00001403
Iteration 141/1000 | Loss: 0.00001403
Iteration 142/1000 | Loss: 0.00001403
Iteration 143/1000 | Loss: 0.00001402
Iteration 144/1000 | Loss: 0.00001402
Iteration 145/1000 | Loss: 0.00001402
Iteration 146/1000 | Loss: 0.00001402
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001402
Iteration 149/1000 | Loss: 0.00001402
Iteration 150/1000 | Loss: 0.00001402
Iteration 151/1000 | Loss: 0.00001402
Iteration 152/1000 | Loss: 0.00001402
Iteration 153/1000 | Loss: 0.00001402
Iteration 154/1000 | Loss: 0.00001401
Iteration 155/1000 | Loss: 0.00001401
Iteration 156/1000 | Loss: 0.00001401
Iteration 157/1000 | Loss: 0.00001401
Iteration 158/1000 | Loss: 0.00001401
Iteration 159/1000 | Loss: 0.00001401
Iteration 160/1000 | Loss: 0.00001401
Iteration 161/1000 | Loss: 0.00001401
Iteration 162/1000 | Loss: 0.00001401
Iteration 163/1000 | Loss: 0.00001400
Iteration 164/1000 | Loss: 0.00001400
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001400
Iteration 168/1000 | Loss: 0.00001400
Iteration 169/1000 | Loss: 0.00001400
Iteration 170/1000 | Loss: 0.00001400
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Iteration 173/1000 | Loss: 0.00001400
Iteration 174/1000 | Loss: 0.00001399
Iteration 175/1000 | Loss: 0.00001399
Iteration 176/1000 | Loss: 0.00001399
Iteration 177/1000 | Loss: 0.00001399
Iteration 178/1000 | Loss: 0.00001399
Iteration 179/1000 | Loss: 0.00001399
Iteration 180/1000 | Loss: 0.00001399
Iteration 181/1000 | Loss: 0.00001399
Iteration 182/1000 | Loss: 0.00001399
Iteration 183/1000 | Loss: 0.00001398
Iteration 184/1000 | Loss: 0.00001398
Iteration 185/1000 | Loss: 0.00001398
Iteration 186/1000 | Loss: 0.00001398
Iteration 187/1000 | Loss: 0.00001398
Iteration 188/1000 | Loss: 0.00001398
Iteration 189/1000 | Loss: 0.00001398
Iteration 190/1000 | Loss: 0.00001398
Iteration 191/1000 | Loss: 0.00001398
Iteration 192/1000 | Loss: 0.00001398
Iteration 193/1000 | Loss: 0.00001398
Iteration 194/1000 | Loss: 0.00001398
Iteration 195/1000 | Loss: 0.00001398
Iteration 196/1000 | Loss: 0.00001398
Iteration 197/1000 | Loss: 0.00001398
Iteration 198/1000 | Loss: 0.00001397
Iteration 199/1000 | Loss: 0.00001397
Iteration 200/1000 | Loss: 0.00001397
Iteration 201/1000 | Loss: 0.00001397
Iteration 202/1000 | Loss: 0.00001397
Iteration 203/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.3974903595226351e-05, 1.3974903595226351e-05, 1.3974903595226351e-05, 1.3974903595226351e-05, 1.3974903595226351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3974903595226351e-05

Optimization complete. Final v2v error: 3.15848445892334 mm

Highest mean error: 3.410877227783203 mm for frame 102

Lowest mean error: 2.8401477336883545 mm for frame 17

Saving results

Total time: 42.521172761917114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797535
Iteration 2/25 | Loss: 0.00174901
Iteration 3/25 | Loss: 0.00127975
Iteration 4/25 | Loss: 0.00123054
Iteration 5/25 | Loss: 0.00122547
Iteration 6/25 | Loss: 0.00122520
Iteration 7/25 | Loss: 0.00122520
Iteration 8/25 | Loss: 0.00122520
Iteration 9/25 | Loss: 0.00122520
Iteration 10/25 | Loss: 0.00122520
Iteration 11/25 | Loss: 0.00122520
Iteration 12/25 | Loss: 0.00122520
Iteration 13/25 | Loss: 0.00122520
Iteration 14/25 | Loss: 0.00122520
Iteration 15/25 | Loss: 0.00122520
Iteration 16/25 | Loss: 0.00122520
Iteration 17/25 | Loss: 0.00122520
Iteration 18/25 | Loss: 0.00122520
Iteration 19/25 | Loss: 0.00122520
Iteration 20/25 | Loss: 0.00122520
Iteration 21/25 | Loss: 0.00122520
Iteration 22/25 | Loss: 0.00122520
Iteration 23/25 | Loss: 0.00122520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001225202577188611, 0.001225202577188611, 0.001225202577188611, 0.001225202577188611, 0.001225202577188611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225202577188611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22596061
Iteration 2/25 | Loss: 0.00059706
Iteration 3/25 | Loss: 0.00059703
Iteration 4/25 | Loss: 0.00059703
Iteration 5/25 | Loss: 0.00059703
Iteration 6/25 | Loss: 0.00059703
Iteration 7/25 | Loss: 0.00059703
Iteration 8/25 | Loss: 0.00059703
Iteration 9/25 | Loss: 0.00059703
Iteration 10/25 | Loss: 0.00059703
Iteration 11/25 | Loss: 0.00059703
Iteration 12/25 | Loss: 0.00059703
Iteration 13/25 | Loss: 0.00059703
Iteration 14/25 | Loss: 0.00059703
Iteration 15/25 | Loss: 0.00059703
Iteration 16/25 | Loss: 0.00059703
Iteration 17/25 | Loss: 0.00059703
Iteration 18/25 | Loss: 0.00059703
Iteration 19/25 | Loss: 0.00059703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005970303900539875, 0.0005970303900539875, 0.0005970303900539875, 0.0005970303900539875, 0.0005970303900539875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005970303900539875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059703
Iteration 2/1000 | Loss: 0.00003588
Iteration 3/1000 | Loss: 0.00002453
Iteration 4/1000 | Loss: 0.00002257
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002086
Iteration 7/1000 | Loss: 0.00002054
Iteration 8/1000 | Loss: 0.00002018
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001977
Iteration 11/1000 | Loss: 0.00001956
Iteration 12/1000 | Loss: 0.00001946
Iteration 13/1000 | Loss: 0.00001929
Iteration 14/1000 | Loss: 0.00001917
Iteration 15/1000 | Loss: 0.00001905
Iteration 16/1000 | Loss: 0.00001903
Iteration 17/1000 | Loss: 0.00001903
Iteration 18/1000 | Loss: 0.00001903
Iteration 19/1000 | Loss: 0.00001903
Iteration 20/1000 | Loss: 0.00001901
Iteration 21/1000 | Loss: 0.00001901
Iteration 22/1000 | Loss: 0.00001900
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001898
Iteration 28/1000 | Loss: 0.00001897
Iteration 29/1000 | Loss: 0.00001897
Iteration 30/1000 | Loss: 0.00001896
Iteration 31/1000 | Loss: 0.00001896
Iteration 32/1000 | Loss: 0.00001896
Iteration 33/1000 | Loss: 0.00001896
Iteration 34/1000 | Loss: 0.00001895
Iteration 35/1000 | Loss: 0.00001895
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001890
Iteration 48/1000 | Loss: 0.00001890
Iteration 49/1000 | Loss: 0.00001890
Iteration 50/1000 | Loss: 0.00001890
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001889
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001888
Iteration 57/1000 | Loss: 0.00001888
Iteration 58/1000 | Loss: 0.00001888
Iteration 59/1000 | Loss: 0.00001888
Iteration 60/1000 | Loss: 0.00001887
Iteration 61/1000 | Loss: 0.00001887
Iteration 62/1000 | Loss: 0.00001887
Iteration 63/1000 | Loss: 0.00001887
Iteration 64/1000 | Loss: 0.00001887
Iteration 65/1000 | Loss: 0.00001887
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001886
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001886
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001886
Iteration 82/1000 | Loss: 0.00001886
Iteration 83/1000 | Loss: 0.00001886
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.8856959286495112e-05, 1.8856959286495112e-05, 1.8856959286495112e-05, 1.8856959286495112e-05, 1.8856959286495112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8856959286495112e-05

Optimization complete. Final v2v error: 3.6321446895599365 mm

Highest mean error: 4.002692222595215 mm for frame 126

Lowest mean error: 3.395843267440796 mm for frame 214

Saving results

Total time: 37.57897090911865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_034/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_034/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00674999
Iteration 2/25 | Loss: 0.00166708
Iteration 3/25 | Loss: 0.00150750
Iteration 4/25 | Loss: 0.00121201
Iteration 5/25 | Loss: 0.00119393
Iteration 6/25 | Loss: 0.00118275
Iteration 7/25 | Loss: 0.00119412
Iteration 8/25 | Loss: 0.00117425
Iteration 9/25 | Loss: 0.00117305
Iteration 10/25 | Loss: 0.00117286
Iteration 11/25 | Loss: 0.00117262
Iteration 12/25 | Loss: 0.00117233
Iteration 13/25 | Loss: 0.00117213
Iteration 14/25 | Loss: 0.00117200
Iteration 15/25 | Loss: 0.00117194
Iteration 16/25 | Loss: 0.00117194
Iteration 17/25 | Loss: 0.00117194
Iteration 18/25 | Loss: 0.00117194
Iteration 19/25 | Loss: 0.00117194
Iteration 20/25 | Loss: 0.00117194
Iteration 21/25 | Loss: 0.00117192
Iteration 22/25 | Loss: 0.00117192
Iteration 23/25 | Loss: 0.00117192
Iteration 24/25 | Loss: 0.00117191
Iteration 25/25 | Loss: 0.00117191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11964655
Iteration 2/25 | Loss: 0.00120536
Iteration 3/25 | Loss: 0.00120536
Iteration 4/25 | Loss: 0.00120536
Iteration 5/25 | Loss: 0.00120536
Iteration 6/25 | Loss: 0.00120536
Iteration 7/25 | Loss: 0.00120536
Iteration 8/25 | Loss: 0.00120536
Iteration 9/25 | Loss: 0.00120536
Iteration 10/25 | Loss: 0.00120536
Iteration 11/25 | Loss: 0.00120536
Iteration 12/25 | Loss: 0.00120536
Iteration 13/25 | Loss: 0.00120536
Iteration 14/25 | Loss: 0.00120536
Iteration 15/25 | Loss: 0.00120536
Iteration 16/25 | Loss: 0.00120536
Iteration 17/25 | Loss: 0.00120536
Iteration 18/25 | Loss: 0.00120536
Iteration 19/25 | Loss: 0.00120536
Iteration 20/25 | Loss: 0.00120536
Iteration 21/25 | Loss: 0.00120536
Iteration 22/25 | Loss: 0.00120536
Iteration 23/25 | Loss: 0.00120536
Iteration 24/25 | Loss: 0.00120536
Iteration 25/25 | Loss: 0.00120536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120536
Iteration 2/1000 | Loss: 0.00016145
Iteration 3/1000 | Loss: 0.00004421
Iteration 4/1000 | Loss: 0.00003609
Iteration 5/1000 | Loss: 0.00002996
Iteration 6/1000 | Loss: 0.00002545
Iteration 7/1000 | Loss: 0.00002593
Iteration 8/1000 | Loss: 0.00002320
Iteration 9/1000 | Loss: 0.00002225
Iteration 10/1000 | Loss: 0.00002169
Iteration 11/1000 | Loss: 0.00002124
Iteration 12/1000 | Loss: 0.00002082
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00039259
Iteration 15/1000 | Loss: 0.00003360
Iteration 16/1000 | Loss: 0.00002859
Iteration 17/1000 | Loss: 0.00002033
Iteration 18/1000 | Loss: 0.00001972
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00001885
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001865
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001844
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001844
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001843
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001841
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001835
Iteration 43/1000 | Loss: 0.00001834
Iteration 44/1000 | Loss: 0.00001834
Iteration 45/1000 | Loss: 0.00001834
Iteration 46/1000 | Loss: 0.00001833
Iteration 47/1000 | Loss: 0.00001833
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001829
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001824
Iteration 54/1000 | Loss: 0.00001820
Iteration 55/1000 | Loss: 0.00001820
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001818
Iteration 61/1000 | Loss: 0.00001818
Iteration 62/1000 | Loss: 0.00001817
Iteration 63/1000 | Loss: 0.00001817
Iteration 64/1000 | Loss: 0.00001816
Iteration 65/1000 | Loss: 0.00001816
Iteration 66/1000 | Loss: 0.00001816
Iteration 67/1000 | Loss: 0.00001815
Iteration 68/1000 | Loss: 0.00001815
Iteration 69/1000 | Loss: 0.00001815
Iteration 70/1000 | Loss: 0.00001814
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00001814
Iteration 73/1000 | Loss: 0.00001813
Iteration 74/1000 | Loss: 0.00001813
Iteration 75/1000 | Loss: 0.00001813
Iteration 76/1000 | Loss: 0.00001812
Iteration 77/1000 | Loss: 0.00001810
Iteration 78/1000 | Loss: 0.00001810
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001807
Iteration 83/1000 | Loss: 0.00001807
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001805
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001804
Iteration 92/1000 | Loss: 0.00001804
Iteration 93/1000 | Loss: 0.00001804
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001803
Iteration 97/1000 | Loss: 0.00001803
Iteration 98/1000 | Loss: 0.00001803
Iteration 99/1000 | Loss: 0.00001803
Iteration 100/1000 | Loss: 0.00001803
Iteration 101/1000 | Loss: 0.00001803
Iteration 102/1000 | Loss: 0.00001803
Iteration 103/1000 | Loss: 0.00001803
Iteration 104/1000 | Loss: 0.00001803
Iteration 105/1000 | Loss: 0.00001803
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001802
Iteration 109/1000 | Loss: 0.00001802
Iteration 110/1000 | Loss: 0.00001802
Iteration 111/1000 | Loss: 0.00001802
Iteration 112/1000 | Loss: 0.00001802
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001800
Iteration 120/1000 | Loss: 0.00001800
Iteration 121/1000 | Loss: 0.00001800
Iteration 122/1000 | Loss: 0.00001800
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001799
Iteration 125/1000 | Loss: 0.00001799
Iteration 126/1000 | Loss: 0.00001799
Iteration 127/1000 | Loss: 0.00001799
Iteration 128/1000 | Loss: 0.00001799
Iteration 129/1000 | Loss: 0.00001798
Iteration 130/1000 | Loss: 0.00001798
Iteration 131/1000 | Loss: 0.00001798
Iteration 132/1000 | Loss: 0.00001798
Iteration 133/1000 | Loss: 0.00001798
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001798
Iteration 145/1000 | Loss: 0.00001798
Iteration 146/1000 | Loss: 0.00001798
Iteration 147/1000 | Loss: 0.00001798
Iteration 148/1000 | Loss: 0.00001798
Iteration 149/1000 | Loss: 0.00001798
Iteration 150/1000 | Loss: 0.00001798
Iteration 151/1000 | Loss: 0.00001798
Iteration 152/1000 | Loss: 0.00001798
Iteration 153/1000 | Loss: 0.00001798
Iteration 154/1000 | Loss: 0.00001798
Iteration 155/1000 | Loss: 0.00001798
Iteration 156/1000 | Loss: 0.00001798
Iteration 157/1000 | Loss: 0.00001798
Iteration 158/1000 | Loss: 0.00001798
Iteration 159/1000 | Loss: 0.00001798
Iteration 160/1000 | Loss: 0.00001798
Iteration 161/1000 | Loss: 0.00001798
Iteration 162/1000 | Loss: 0.00001798
Iteration 163/1000 | Loss: 0.00001798
Iteration 164/1000 | Loss: 0.00001798
Iteration 165/1000 | Loss: 0.00001798
Iteration 166/1000 | Loss: 0.00001798
Iteration 167/1000 | Loss: 0.00001798
Iteration 168/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.7976168237510137e-05, 1.7976168237510137e-05, 1.7976168237510137e-05, 1.7976168237510137e-05, 1.7976168237510137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7976168237510137e-05

Optimization complete. Final v2v error: 3.4451143741607666 mm

Highest mean error: 5.381404399871826 mm for frame 12

Lowest mean error: 2.4447317123413086 mm for frame 113

Saving results

Total time: 79.4059841632843
