Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=240, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13440-13495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061431
Iteration 2/25 | Loss: 0.00498204
Iteration 3/25 | Loss: 0.00316159
Iteration 4/25 | Loss: 0.00242539
Iteration 5/25 | Loss: 0.00144588
Iteration 6/25 | Loss: 0.00143967
Iteration 7/25 | Loss: 0.00121421
Iteration 8/25 | Loss: 0.00109614
Iteration 9/25 | Loss: 0.00094986
Iteration 10/25 | Loss: 0.00088853
Iteration 11/25 | Loss: 0.00085785
Iteration 12/25 | Loss: 0.00084104
Iteration 13/25 | Loss: 0.00082804
Iteration 14/25 | Loss: 0.00081920
Iteration 15/25 | Loss: 0.00081462
Iteration 16/25 | Loss: 0.00080946
Iteration 17/25 | Loss: 0.00081201
Iteration 18/25 | Loss: 0.00082062
Iteration 19/25 | Loss: 0.00080498
Iteration 20/25 | Loss: 0.00080851
Iteration 21/25 | Loss: 0.00080551
Iteration 22/25 | Loss: 0.00080486
Iteration 23/25 | Loss: 0.00080394
Iteration 24/25 | Loss: 0.00080389
Iteration 25/25 | Loss: 0.00080389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45262635
Iteration 2/25 | Loss: 0.00025101
Iteration 3/25 | Loss: 0.00025101
Iteration 4/25 | Loss: 0.00025101
Iteration 5/25 | Loss: 0.00025101
Iteration 6/25 | Loss: 0.00025101
Iteration 7/25 | Loss: 0.00025101
Iteration 8/25 | Loss: 0.00025101
Iteration 9/25 | Loss: 0.00025101
Iteration 10/25 | Loss: 0.00025101
Iteration 11/25 | Loss: 0.00025101
Iteration 12/25 | Loss: 0.00025101
Iteration 13/25 | Loss: 0.00025101
Iteration 14/25 | Loss: 0.00025101
Iteration 15/25 | Loss: 0.00025101
Iteration 16/25 | Loss: 0.00025101
Iteration 17/25 | Loss: 0.00025101
Iteration 18/25 | Loss: 0.00025101
Iteration 19/25 | Loss: 0.00025101
Iteration 20/25 | Loss: 0.00025101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00025100517086684704, 0.00025100517086684704, 0.00025100517086684704, 0.00025100517086684704, 0.00025100517086684704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025100517086684704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025101
Iteration 2/1000 | Loss: 0.00016954
Iteration 3/1000 | Loss: 0.00002562
Iteration 4/1000 | Loss: 0.00002242
Iteration 5/1000 | Loss: 0.00002137
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00024934
Iteration 10/1000 | Loss: 0.00005832
Iteration 11/1000 | Loss: 0.00005712
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001900
Iteration 15/1000 | Loss: 0.00001895
Iteration 16/1000 | Loss: 0.00001892
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001886
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001882
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001881
Iteration 34/1000 | Loss: 0.00001881
Iteration 35/1000 | Loss: 0.00001880
Iteration 36/1000 | Loss: 0.00010325
Iteration 37/1000 | Loss: 0.00005290
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001874
Iteration 42/1000 | Loss: 0.00001874
Iteration 43/1000 | Loss: 0.00001874
Iteration 44/1000 | Loss: 0.00001874
Iteration 45/1000 | Loss: 0.00001874
Iteration 46/1000 | Loss: 0.00001874
Iteration 47/1000 | Loss: 0.00001874
Iteration 48/1000 | Loss: 0.00001874
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001873
Iteration 51/1000 | Loss: 0.00001873
Iteration 52/1000 | Loss: 0.00001873
Iteration 53/1000 | Loss: 0.00001873
Iteration 54/1000 | Loss: 0.00008446
Iteration 55/1000 | Loss: 0.00001920
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001870
Iteration 59/1000 | Loss: 0.00001870
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001869
Iteration 63/1000 | Loss: 0.00001869
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001867
Iteration 68/1000 | Loss: 0.00001867
Iteration 69/1000 | Loss: 0.00001867
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001866
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001866
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001866
Iteration 81/1000 | Loss: 0.00001866
Iteration 82/1000 | Loss: 0.00001866
Iteration 83/1000 | Loss: 0.00001866
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001864
Iteration 89/1000 | Loss: 0.00001864
Iteration 90/1000 | Loss: 0.00001864
Iteration 91/1000 | Loss: 0.00001864
Iteration 92/1000 | Loss: 0.00001864
Iteration 93/1000 | Loss: 0.00001864
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001863
Iteration 96/1000 | Loss: 0.00001863
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001862
Iteration 99/1000 | Loss: 0.00001862
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001860
Iteration 104/1000 | Loss: 0.00001860
Iteration 105/1000 | Loss: 0.00001860
Iteration 106/1000 | Loss: 0.00001860
Iteration 107/1000 | Loss: 0.00001859
Iteration 108/1000 | Loss: 0.00001859
Iteration 109/1000 | Loss: 0.00001858
Iteration 110/1000 | Loss: 0.00001858
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001857
Iteration 113/1000 | Loss: 0.00001857
Iteration 114/1000 | Loss: 0.00001857
Iteration 115/1000 | Loss: 0.00001857
Iteration 116/1000 | Loss: 0.00001857
Iteration 117/1000 | Loss: 0.00001857
Iteration 118/1000 | Loss: 0.00001857
Iteration 119/1000 | Loss: 0.00001857
Iteration 120/1000 | Loss: 0.00001857
Iteration 121/1000 | Loss: 0.00001857
Iteration 122/1000 | Loss: 0.00001857
Iteration 123/1000 | Loss: 0.00001856
Iteration 124/1000 | Loss: 0.00001856
Iteration 125/1000 | Loss: 0.00001856
Iteration 126/1000 | Loss: 0.00001856
Iteration 127/1000 | Loss: 0.00001856
Iteration 128/1000 | Loss: 0.00001856
Iteration 129/1000 | Loss: 0.00001856
Iteration 130/1000 | Loss: 0.00001856
Iteration 131/1000 | Loss: 0.00001856
Iteration 132/1000 | Loss: 0.00001856
Iteration 133/1000 | Loss: 0.00001856
Iteration 134/1000 | Loss: 0.00001856
Iteration 135/1000 | Loss: 0.00001855
Iteration 136/1000 | Loss: 0.00001855
Iteration 137/1000 | Loss: 0.00001855
Iteration 138/1000 | Loss: 0.00001855
Iteration 139/1000 | Loss: 0.00001855
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001855
Iteration 142/1000 | Loss: 0.00001855
Iteration 143/1000 | Loss: 0.00001855
Iteration 144/1000 | Loss: 0.00001855
Iteration 145/1000 | Loss: 0.00001855
Iteration 146/1000 | Loss: 0.00001855
Iteration 147/1000 | Loss: 0.00001855
Iteration 148/1000 | Loss: 0.00001854
Iteration 149/1000 | Loss: 0.00001854
Iteration 150/1000 | Loss: 0.00001854
Iteration 151/1000 | Loss: 0.00001854
Iteration 152/1000 | Loss: 0.00001854
Iteration 153/1000 | Loss: 0.00001854
Iteration 154/1000 | Loss: 0.00001854
Iteration 155/1000 | Loss: 0.00001854
Iteration 156/1000 | Loss: 0.00001854
Iteration 157/1000 | Loss: 0.00001854
Iteration 158/1000 | Loss: 0.00001854
Iteration 159/1000 | Loss: 0.00001854
Iteration 160/1000 | Loss: 0.00001854
Iteration 161/1000 | Loss: 0.00001854
Iteration 162/1000 | Loss: 0.00001854
Iteration 163/1000 | Loss: 0.00001854
Iteration 164/1000 | Loss: 0.00001854
Iteration 165/1000 | Loss: 0.00001854
Iteration 166/1000 | Loss: 0.00001854
Iteration 167/1000 | Loss: 0.00001854
Iteration 168/1000 | Loss: 0.00001854
Iteration 169/1000 | Loss: 0.00001854
Iteration 170/1000 | Loss: 0.00001854
Iteration 171/1000 | Loss: 0.00001854
Iteration 172/1000 | Loss: 0.00001854
Iteration 173/1000 | Loss: 0.00001854
Iteration 174/1000 | Loss: 0.00001854
Iteration 175/1000 | Loss: 0.00001854
Iteration 176/1000 | Loss: 0.00001854
Iteration 177/1000 | Loss: 0.00001854
Iteration 178/1000 | Loss: 0.00001854
Iteration 179/1000 | Loss: 0.00001854
Iteration 180/1000 | Loss: 0.00001854
Iteration 181/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.8537086361902766e-05, 1.8537086361902766e-05, 1.8537086361902766e-05, 1.8537086361902766e-05, 1.8537086361902766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8537086361902766e-05

Optimization complete. Final v2v error: 3.6564741134643555 mm

Highest mean error: 4.078836917877197 mm for frame 20

Lowest mean error: 3.419456958770752 mm for frame 103

Saving results

Total time: 91.99550676345825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00686304
Iteration 2/25 | Loss: 0.00088633
Iteration 3/25 | Loss: 0.00076867
Iteration 4/25 | Loss: 0.00074031
Iteration 5/25 | Loss: 0.00072864
Iteration 6/25 | Loss: 0.00072680
Iteration 7/25 | Loss: 0.00072641
Iteration 8/25 | Loss: 0.00072641
Iteration 9/25 | Loss: 0.00072641
Iteration 10/25 | Loss: 0.00072641
Iteration 11/25 | Loss: 0.00072641
Iteration 12/25 | Loss: 0.00072641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007264104788191617, 0.0007264104788191617, 0.0007264104788191617, 0.0007264104788191617, 0.0007264104788191617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007264104788191617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.02747965
Iteration 2/25 | Loss: 0.00024278
Iteration 3/25 | Loss: 0.00024278
Iteration 4/25 | Loss: 0.00024278
Iteration 5/25 | Loss: 0.00024277
Iteration 6/25 | Loss: 0.00024277
Iteration 7/25 | Loss: 0.00024277
Iteration 8/25 | Loss: 0.00024277
Iteration 9/25 | Loss: 0.00024277
Iteration 10/25 | Loss: 0.00024277
Iteration 11/25 | Loss: 0.00024277
Iteration 12/25 | Loss: 0.00024277
Iteration 13/25 | Loss: 0.00024277
Iteration 14/25 | Loss: 0.00024277
Iteration 15/25 | Loss: 0.00024277
Iteration 16/25 | Loss: 0.00024277
Iteration 17/25 | Loss: 0.00024277
Iteration 18/25 | Loss: 0.00024277
Iteration 19/25 | Loss: 0.00024277
Iteration 20/25 | Loss: 0.00024277
Iteration 21/25 | Loss: 0.00024277
Iteration 22/25 | Loss: 0.00024277
Iteration 23/25 | Loss: 0.00024277
Iteration 24/25 | Loss: 0.00024277
Iteration 25/25 | Loss: 0.00024277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024277
Iteration 2/1000 | Loss: 0.00003269
Iteration 3/1000 | Loss: 0.00001877
Iteration 4/1000 | Loss: 0.00001716
Iteration 5/1000 | Loss: 0.00001599
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001516
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001474
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001457
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001456
Iteration 20/1000 | Loss: 0.00001456
Iteration 21/1000 | Loss: 0.00001454
Iteration 22/1000 | Loss: 0.00001453
Iteration 23/1000 | Loss: 0.00001453
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001450
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001450
Iteration 29/1000 | Loss: 0.00001450
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001446
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001445
Iteration 37/1000 | Loss: 0.00001444
Iteration 38/1000 | Loss: 0.00001444
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001443
Iteration 42/1000 | Loss: 0.00001442
Iteration 43/1000 | Loss: 0.00001442
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001442
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001442
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001441
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001441
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001441
Iteration 56/1000 | Loss: 0.00001441
Iteration 57/1000 | Loss: 0.00001440
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001439
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001437
Iteration 66/1000 | Loss: 0.00001437
Iteration 67/1000 | Loss: 0.00001437
Iteration 68/1000 | Loss: 0.00001437
Iteration 69/1000 | Loss: 0.00001437
Iteration 70/1000 | Loss: 0.00001436
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001433
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001429
Iteration 99/1000 | Loss: 0.00001427
Iteration 100/1000 | Loss: 0.00001427
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001425
Iteration 105/1000 | Loss: 0.00001425
Iteration 106/1000 | Loss: 0.00001425
Iteration 107/1000 | Loss: 0.00001425
Iteration 108/1000 | Loss: 0.00001425
Iteration 109/1000 | Loss: 0.00001425
Iteration 110/1000 | Loss: 0.00001425
Iteration 111/1000 | Loss: 0.00001425
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001424
Iteration 114/1000 | Loss: 0.00001423
Iteration 115/1000 | Loss: 0.00001423
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001421
Iteration 121/1000 | Loss: 0.00001421
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001421
Iteration 136/1000 | Loss: 0.00001421
Iteration 137/1000 | Loss: 0.00001421
Iteration 138/1000 | Loss: 0.00001421
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001420
Iteration 146/1000 | Loss: 0.00001420
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001420
Iteration 149/1000 | Loss: 0.00001420
Iteration 150/1000 | Loss: 0.00001420
Iteration 151/1000 | Loss: 0.00001420
Iteration 152/1000 | Loss: 0.00001420
Iteration 153/1000 | Loss: 0.00001420
Iteration 154/1000 | Loss: 0.00001420
Iteration 155/1000 | Loss: 0.00001420
Iteration 156/1000 | Loss: 0.00001420
Iteration 157/1000 | Loss: 0.00001420
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001420
Iteration 161/1000 | Loss: 0.00001420
Iteration 162/1000 | Loss: 0.00001420
Iteration 163/1000 | Loss: 0.00001420
Iteration 164/1000 | Loss: 0.00001420
Iteration 165/1000 | Loss: 0.00001420
Iteration 166/1000 | Loss: 0.00001420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.4204492799763102e-05, 1.4204492799763102e-05, 1.4204492799763102e-05, 1.4204492799763102e-05, 1.4204492799763102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4204492799763102e-05

Optimization complete. Final v2v error: 3.1992855072021484 mm

Highest mean error: 3.4125568866729736 mm for frame 45

Lowest mean error: 2.968639373779297 mm for frame 2

Saving results

Total time: 35.84710645675659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491798
Iteration 2/25 | Loss: 0.00087936
Iteration 3/25 | Loss: 0.00077321
Iteration 4/25 | Loss: 0.00074908
Iteration 5/25 | Loss: 0.00073908
Iteration 6/25 | Loss: 0.00073680
Iteration 7/25 | Loss: 0.00073667
Iteration 8/25 | Loss: 0.00073667
Iteration 9/25 | Loss: 0.00073667
Iteration 10/25 | Loss: 0.00073667
Iteration 11/25 | Loss: 0.00073667
Iteration 12/25 | Loss: 0.00073667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007366732461377978, 0.0007366732461377978, 0.0007366732461377978, 0.0007366732461377978, 0.0007366732461377978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007366732461377978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63342571
Iteration 2/25 | Loss: 0.00024767
Iteration 3/25 | Loss: 0.00024766
Iteration 4/25 | Loss: 0.00024766
Iteration 5/25 | Loss: 0.00024766
Iteration 6/25 | Loss: 0.00024766
Iteration 7/25 | Loss: 0.00024766
Iteration 8/25 | Loss: 0.00024766
Iteration 9/25 | Loss: 0.00024765
Iteration 10/25 | Loss: 0.00024765
Iteration 11/25 | Loss: 0.00024765
Iteration 12/25 | Loss: 0.00024765
Iteration 13/25 | Loss: 0.00024765
Iteration 14/25 | Loss: 0.00024765
Iteration 15/25 | Loss: 0.00024765
Iteration 16/25 | Loss: 0.00024765
Iteration 17/25 | Loss: 0.00024765
Iteration 18/25 | Loss: 0.00024765
Iteration 19/25 | Loss: 0.00024765
Iteration 20/25 | Loss: 0.00024765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00024765412672422826, 0.00024765412672422826, 0.00024765412672422826, 0.00024765412672422826, 0.00024765412672422826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024765412672422826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024765
Iteration 2/1000 | Loss: 0.00003206
Iteration 3/1000 | Loss: 0.00002238
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001904
Iteration 6/1000 | Loss: 0.00001839
Iteration 7/1000 | Loss: 0.00001784
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001700
Iteration 10/1000 | Loss: 0.00001681
Iteration 11/1000 | Loss: 0.00001663
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001661
Iteration 14/1000 | Loss: 0.00001654
Iteration 15/1000 | Loss: 0.00001654
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001647
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001645
Iteration 21/1000 | Loss: 0.00001645
Iteration 22/1000 | Loss: 0.00001644
Iteration 23/1000 | Loss: 0.00001641
Iteration 24/1000 | Loss: 0.00001640
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001640
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001638
Iteration 34/1000 | Loss: 0.00001638
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001637
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001637
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001636
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001635
Iteration 55/1000 | Loss: 0.00001635
Iteration 56/1000 | Loss: 0.00001635
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001634
Iteration 59/1000 | Loss: 0.00001634
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001633
Iteration 63/1000 | Loss: 0.00001633
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001630
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001629
Iteration 79/1000 | Loss: 0.00001629
Iteration 80/1000 | Loss: 0.00001629
Iteration 81/1000 | Loss: 0.00001628
Iteration 82/1000 | Loss: 0.00001628
Iteration 83/1000 | Loss: 0.00001628
Iteration 84/1000 | Loss: 0.00001627
Iteration 85/1000 | Loss: 0.00001627
Iteration 86/1000 | Loss: 0.00001627
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001626
Iteration 89/1000 | Loss: 0.00001626
Iteration 90/1000 | Loss: 0.00001626
Iteration 91/1000 | Loss: 0.00001626
Iteration 92/1000 | Loss: 0.00001625
Iteration 93/1000 | Loss: 0.00001625
Iteration 94/1000 | Loss: 0.00001625
Iteration 95/1000 | Loss: 0.00001625
Iteration 96/1000 | Loss: 0.00001624
Iteration 97/1000 | Loss: 0.00001624
Iteration 98/1000 | Loss: 0.00001624
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001623
Iteration 101/1000 | Loss: 0.00001623
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001622
Iteration 114/1000 | Loss: 0.00001622
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001620
Iteration 124/1000 | Loss: 0.00001620
Iteration 125/1000 | Loss: 0.00001620
Iteration 126/1000 | Loss: 0.00001620
Iteration 127/1000 | Loss: 0.00001620
Iteration 128/1000 | Loss: 0.00001620
Iteration 129/1000 | Loss: 0.00001620
Iteration 130/1000 | Loss: 0.00001620
Iteration 131/1000 | Loss: 0.00001620
Iteration 132/1000 | Loss: 0.00001620
Iteration 133/1000 | Loss: 0.00001620
Iteration 134/1000 | Loss: 0.00001620
Iteration 135/1000 | Loss: 0.00001620
Iteration 136/1000 | Loss: 0.00001620
Iteration 137/1000 | Loss: 0.00001620
Iteration 138/1000 | Loss: 0.00001619
Iteration 139/1000 | Loss: 0.00001619
Iteration 140/1000 | Loss: 0.00001619
Iteration 141/1000 | Loss: 0.00001619
Iteration 142/1000 | Loss: 0.00001619
Iteration 143/1000 | Loss: 0.00001619
Iteration 144/1000 | Loss: 0.00001619
Iteration 145/1000 | Loss: 0.00001619
Iteration 146/1000 | Loss: 0.00001619
Iteration 147/1000 | Loss: 0.00001619
Iteration 148/1000 | Loss: 0.00001619
Iteration 149/1000 | Loss: 0.00001619
Iteration 150/1000 | Loss: 0.00001618
Iteration 151/1000 | Loss: 0.00001618
Iteration 152/1000 | Loss: 0.00001618
Iteration 153/1000 | Loss: 0.00001618
Iteration 154/1000 | Loss: 0.00001618
Iteration 155/1000 | Loss: 0.00001618
Iteration 156/1000 | Loss: 0.00001618
Iteration 157/1000 | Loss: 0.00001618
Iteration 158/1000 | Loss: 0.00001618
Iteration 159/1000 | Loss: 0.00001618
Iteration 160/1000 | Loss: 0.00001618
Iteration 161/1000 | Loss: 0.00001618
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001618
Iteration 169/1000 | Loss: 0.00001618
Iteration 170/1000 | Loss: 0.00001618
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001618
Iteration 174/1000 | Loss: 0.00001618
Iteration 175/1000 | Loss: 0.00001618
Iteration 176/1000 | Loss: 0.00001618
Iteration 177/1000 | Loss: 0.00001618
Iteration 178/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.618010901438538e-05, 1.618010901438538e-05, 1.618010901438538e-05, 1.618010901438538e-05, 1.618010901438538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.618010901438538e-05

Optimization complete. Final v2v error: 3.4448325634002686 mm

Highest mean error: 4.480953693389893 mm for frame 227

Lowest mean error: 2.981788158416748 mm for frame 109

Saving results

Total time: 42.61745619773865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802190
Iteration 2/25 | Loss: 0.00136181
Iteration 3/25 | Loss: 0.00092905
Iteration 4/25 | Loss: 0.00083884
Iteration 5/25 | Loss: 0.00081752
Iteration 6/25 | Loss: 0.00080998
Iteration 7/25 | Loss: 0.00080776
Iteration 8/25 | Loss: 0.00080730
Iteration 9/25 | Loss: 0.00080719
Iteration 10/25 | Loss: 0.00080719
Iteration 11/25 | Loss: 0.00080719
Iteration 12/25 | Loss: 0.00080719
Iteration 13/25 | Loss: 0.00080719
Iteration 14/25 | Loss: 0.00080719
Iteration 15/25 | Loss: 0.00080719
Iteration 16/25 | Loss: 0.00080719
Iteration 17/25 | Loss: 0.00080719
Iteration 18/25 | Loss: 0.00080718
Iteration 19/25 | Loss: 0.00080718
Iteration 20/25 | Loss: 0.00080718
Iteration 21/25 | Loss: 0.00080718
Iteration 22/25 | Loss: 0.00080718
Iteration 23/25 | Loss: 0.00080718
Iteration 24/25 | Loss: 0.00080718
Iteration 25/25 | Loss: 0.00080718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38297999
Iteration 2/25 | Loss: 0.00031531
Iteration 3/25 | Loss: 0.00031531
Iteration 4/25 | Loss: 0.00031531
Iteration 5/25 | Loss: 0.00031531
Iteration 6/25 | Loss: 0.00031531
Iteration 7/25 | Loss: 0.00031531
Iteration 8/25 | Loss: 0.00031531
Iteration 9/25 | Loss: 0.00031531
Iteration 10/25 | Loss: 0.00031531
Iteration 11/25 | Loss: 0.00031531
Iteration 12/25 | Loss: 0.00031531
Iteration 13/25 | Loss: 0.00031531
Iteration 14/25 | Loss: 0.00031531
Iteration 15/25 | Loss: 0.00031531
Iteration 16/25 | Loss: 0.00031531
Iteration 17/25 | Loss: 0.00031531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00031530746491625905, 0.00031530746491625905, 0.00031530746491625905, 0.00031530746491625905, 0.00031530746491625905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031530746491625905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031531
Iteration 2/1000 | Loss: 0.00004628
Iteration 3/1000 | Loss: 0.00003341
Iteration 4/1000 | Loss: 0.00002908
Iteration 5/1000 | Loss: 0.00002751
Iteration 6/1000 | Loss: 0.00002646
Iteration 7/1000 | Loss: 0.00002562
Iteration 8/1000 | Loss: 0.00002487
Iteration 9/1000 | Loss: 0.00002447
Iteration 10/1000 | Loss: 0.00002414
Iteration 11/1000 | Loss: 0.00002390
Iteration 12/1000 | Loss: 0.00002368
Iteration 13/1000 | Loss: 0.00002366
Iteration 14/1000 | Loss: 0.00002358
Iteration 15/1000 | Loss: 0.00002358
Iteration 16/1000 | Loss: 0.00002350
Iteration 17/1000 | Loss: 0.00002349
Iteration 18/1000 | Loss: 0.00002348
Iteration 19/1000 | Loss: 0.00002348
Iteration 20/1000 | Loss: 0.00002343
Iteration 21/1000 | Loss: 0.00002341
Iteration 22/1000 | Loss: 0.00002340
Iteration 23/1000 | Loss: 0.00002339
Iteration 24/1000 | Loss: 0.00002338
Iteration 25/1000 | Loss: 0.00002338
Iteration 26/1000 | Loss: 0.00002337
Iteration 27/1000 | Loss: 0.00002337
Iteration 28/1000 | Loss: 0.00002336
Iteration 29/1000 | Loss: 0.00002336
Iteration 30/1000 | Loss: 0.00002335
Iteration 31/1000 | Loss: 0.00002334
Iteration 32/1000 | Loss: 0.00002334
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002333
Iteration 35/1000 | Loss: 0.00002333
Iteration 36/1000 | Loss: 0.00002333
Iteration 37/1000 | Loss: 0.00002332
Iteration 38/1000 | Loss: 0.00002332
Iteration 39/1000 | Loss: 0.00002332
Iteration 40/1000 | Loss: 0.00002331
Iteration 41/1000 | Loss: 0.00002331
Iteration 42/1000 | Loss: 0.00002331
Iteration 43/1000 | Loss: 0.00002330
Iteration 44/1000 | Loss: 0.00002330
Iteration 45/1000 | Loss: 0.00002329
Iteration 46/1000 | Loss: 0.00002329
Iteration 47/1000 | Loss: 0.00002329
Iteration 48/1000 | Loss: 0.00002329
Iteration 49/1000 | Loss: 0.00002329
Iteration 50/1000 | Loss: 0.00002328
Iteration 51/1000 | Loss: 0.00002328
Iteration 52/1000 | Loss: 0.00002328
Iteration 53/1000 | Loss: 0.00002328
Iteration 54/1000 | Loss: 0.00002328
Iteration 55/1000 | Loss: 0.00002328
Iteration 56/1000 | Loss: 0.00002328
Iteration 57/1000 | Loss: 0.00002328
Iteration 58/1000 | Loss: 0.00002328
Iteration 59/1000 | Loss: 0.00002327
Iteration 60/1000 | Loss: 0.00002327
Iteration 61/1000 | Loss: 0.00002327
Iteration 62/1000 | Loss: 0.00002327
Iteration 63/1000 | Loss: 0.00002327
Iteration 64/1000 | Loss: 0.00002327
Iteration 65/1000 | Loss: 0.00002326
Iteration 66/1000 | Loss: 0.00002326
Iteration 67/1000 | Loss: 0.00002326
Iteration 68/1000 | Loss: 0.00002326
Iteration 69/1000 | Loss: 0.00002326
Iteration 70/1000 | Loss: 0.00002325
Iteration 71/1000 | Loss: 0.00002325
Iteration 72/1000 | Loss: 0.00002325
Iteration 73/1000 | Loss: 0.00002325
Iteration 74/1000 | Loss: 0.00002324
Iteration 75/1000 | Loss: 0.00002324
Iteration 76/1000 | Loss: 0.00002324
Iteration 77/1000 | Loss: 0.00002324
Iteration 78/1000 | Loss: 0.00002323
Iteration 79/1000 | Loss: 0.00002323
Iteration 80/1000 | Loss: 0.00002323
Iteration 81/1000 | Loss: 0.00002323
Iteration 82/1000 | Loss: 0.00002323
Iteration 83/1000 | Loss: 0.00002322
Iteration 84/1000 | Loss: 0.00002322
Iteration 85/1000 | Loss: 0.00002322
Iteration 86/1000 | Loss: 0.00002322
Iteration 87/1000 | Loss: 0.00002322
Iteration 88/1000 | Loss: 0.00002322
Iteration 89/1000 | Loss: 0.00002322
Iteration 90/1000 | Loss: 0.00002322
Iteration 91/1000 | Loss: 0.00002322
Iteration 92/1000 | Loss: 0.00002322
Iteration 93/1000 | Loss: 0.00002322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.3217480702442117e-05, 2.3217480702442117e-05, 2.3217480702442117e-05, 2.3217480702442117e-05, 2.3217480702442117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3217480702442117e-05

Optimization complete. Final v2v error: 3.9884605407714844 mm

Highest mean error: 8.087992668151855 mm for frame 65

Lowest mean error: 3.2216718196868896 mm for frame 189

Saving results

Total time: 46.480149269104004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387738
Iteration 2/25 | Loss: 0.00079593
Iteration 3/25 | Loss: 0.00071637
Iteration 4/25 | Loss: 0.00070451
Iteration 5/25 | Loss: 0.00070061
Iteration 6/25 | Loss: 0.00070026
Iteration 7/25 | Loss: 0.00070026
Iteration 8/25 | Loss: 0.00070026
Iteration 9/25 | Loss: 0.00070026
Iteration 10/25 | Loss: 0.00070026
Iteration 11/25 | Loss: 0.00070026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007002599304541945, 0.0007002599304541945, 0.0007002599304541945, 0.0007002599304541945, 0.0007002599304541945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007002599304541945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45868754
Iteration 2/25 | Loss: 0.00022576
Iteration 3/25 | Loss: 0.00022576
Iteration 4/25 | Loss: 0.00022576
Iteration 5/25 | Loss: 0.00022576
Iteration 6/25 | Loss: 0.00022576
Iteration 7/25 | Loss: 0.00022576
Iteration 8/25 | Loss: 0.00022576
Iteration 9/25 | Loss: 0.00022576
Iteration 10/25 | Loss: 0.00022576
Iteration 11/25 | Loss: 0.00022576
Iteration 12/25 | Loss: 0.00022576
Iteration 13/25 | Loss: 0.00022576
Iteration 14/25 | Loss: 0.00022576
Iteration 15/25 | Loss: 0.00022576
Iteration 16/25 | Loss: 0.00022576
Iteration 17/25 | Loss: 0.00022576
Iteration 18/25 | Loss: 0.00022576
Iteration 19/25 | Loss: 0.00022576
Iteration 20/25 | Loss: 0.00022576
Iteration 21/25 | Loss: 0.00022576
Iteration 22/25 | Loss: 0.00022576
Iteration 23/25 | Loss: 0.00022576
Iteration 24/25 | Loss: 0.00022576
Iteration 25/25 | Loss: 0.00022576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022576
Iteration 2/1000 | Loss: 0.00001916
Iteration 3/1000 | Loss: 0.00001478
Iteration 4/1000 | Loss: 0.00001358
Iteration 5/1000 | Loss: 0.00001301
Iteration 6/1000 | Loss: 0.00001265
Iteration 7/1000 | Loss: 0.00001245
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001237
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001217
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001204
Iteration 16/1000 | Loss: 0.00001204
Iteration 17/1000 | Loss: 0.00001201
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001187
Iteration 22/1000 | Loss: 0.00001187
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001182
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001182
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001175
Iteration 35/1000 | Loss: 0.00001175
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001171
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001166
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001163
Iteration 75/1000 | Loss: 0.00001163
Iteration 76/1000 | Loss: 0.00001163
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001163
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.1632701898633968e-05, 1.1632701898633968e-05, 1.1632701898633968e-05, 1.1632701898633968e-05, 1.1632701898633968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1632701898633968e-05

Optimization complete. Final v2v error: 2.9019126892089844 mm

Highest mean error: 3.0308594703674316 mm for frame 42

Lowest mean error: 2.776409149169922 mm for frame 224

Saving results

Total time: 33.6471107006073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462891
Iteration 2/25 | Loss: 0.00094576
Iteration 3/25 | Loss: 0.00077221
Iteration 4/25 | Loss: 0.00074679
Iteration 5/25 | Loss: 0.00073887
Iteration 6/25 | Loss: 0.00073752
Iteration 7/25 | Loss: 0.00073718
Iteration 8/25 | Loss: 0.00073718
Iteration 9/25 | Loss: 0.00073718
Iteration 10/25 | Loss: 0.00073718
Iteration 11/25 | Loss: 0.00073718
Iteration 12/25 | Loss: 0.00073718
Iteration 13/25 | Loss: 0.00073718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007371774408966303, 0.0007371774408966303, 0.0007371774408966303, 0.0007371774408966303, 0.0007371774408966303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007371774408966303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14106894
Iteration 2/25 | Loss: 0.00024869
Iteration 3/25 | Loss: 0.00024869
Iteration 4/25 | Loss: 0.00024869
Iteration 5/25 | Loss: 0.00024869
Iteration 6/25 | Loss: 0.00024869
Iteration 7/25 | Loss: 0.00024869
Iteration 8/25 | Loss: 0.00024869
Iteration 9/25 | Loss: 0.00024869
Iteration 10/25 | Loss: 0.00024869
Iteration 11/25 | Loss: 0.00024869
Iteration 12/25 | Loss: 0.00024869
Iteration 13/25 | Loss: 0.00024869
Iteration 14/25 | Loss: 0.00024869
Iteration 15/25 | Loss: 0.00024869
Iteration 16/25 | Loss: 0.00024869
Iteration 17/25 | Loss: 0.00024869
Iteration 18/25 | Loss: 0.00024869
Iteration 19/25 | Loss: 0.00024869
Iteration 20/25 | Loss: 0.00024869
Iteration 21/25 | Loss: 0.00024869
Iteration 22/25 | Loss: 0.00024869
Iteration 23/25 | Loss: 0.00024869
Iteration 24/25 | Loss: 0.00024869
Iteration 25/25 | Loss: 0.00024869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024869
Iteration 2/1000 | Loss: 0.00002741
Iteration 3/1000 | Loss: 0.00001940
Iteration 4/1000 | Loss: 0.00001777
Iteration 5/1000 | Loss: 0.00001704
Iteration 6/1000 | Loss: 0.00001647
Iteration 7/1000 | Loss: 0.00001602
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001564
Iteration 10/1000 | Loss: 0.00001547
Iteration 11/1000 | Loss: 0.00001544
Iteration 12/1000 | Loss: 0.00001541
Iteration 13/1000 | Loss: 0.00001536
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001519
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001509
Iteration 18/1000 | Loss: 0.00001509
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001507
Iteration 21/1000 | Loss: 0.00001506
Iteration 22/1000 | Loss: 0.00001506
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001506
Iteration 25/1000 | Loss: 0.00001505
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001505
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001503
Iteration 31/1000 | Loss: 0.00001503
Iteration 32/1000 | Loss: 0.00001503
Iteration 33/1000 | Loss: 0.00001502
Iteration 34/1000 | Loss: 0.00001502
Iteration 35/1000 | Loss: 0.00001501
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001500
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001499
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001498
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001496
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001485
Iteration 58/1000 | Loss: 0.00001485
Iteration 59/1000 | Loss: 0.00001485
Iteration 60/1000 | Loss: 0.00001485
Iteration 61/1000 | Loss: 0.00001484
Iteration 62/1000 | Loss: 0.00001483
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00001482
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001475
Iteration 98/1000 | Loss: 0.00001475
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001473
Iteration 109/1000 | Loss: 0.00001473
Iteration 110/1000 | Loss: 0.00001473
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001473
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001473
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001472
Iteration 119/1000 | Loss: 0.00001472
Iteration 120/1000 | Loss: 0.00001472
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001471
Iteration 128/1000 | Loss: 0.00001471
Iteration 129/1000 | Loss: 0.00001471
Iteration 130/1000 | Loss: 0.00001471
Iteration 131/1000 | Loss: 0.00001471
Iteration 132/1000 | Loss: 0.00001471
Iteration 133/1000 | Loss: 0.00001471
Iteration 134/1000 | Loss: 0.00001471
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001471
Iteration 138/1000 | Loss: 0.00001471
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001471
Iteration 141/1000 | Loss: 0.00001471
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001471
Iteration 149/1000 | Loss: 0.00001471
Iteration 150/1000 | Loss: 0.00001471
Iteration 151/1000 | Loss: 0.00001471
Iteration 152/1000 | Loss: 0.00001471
Iteration 153/1000 | Loss: 0.00001471
Iteration 154/1000 | Loss: 0.00001471
Iteration 155/1000 | Loss: 0.00001471
Iteration 156/1000 | Loss: 0.00001471
Iteration 157/1000 | Loss: 0.00001471
Iteration 158/1000 | Loss: 0.00001471
Iteration 159/1000 | Loss: 0.00001471
Iteration 160/1000 | Loss: 0.00001471
Iteration 161/1000 | Loss: 0.00001471
Iteration 162/1000 | Loss: 0.00001471
Iteration 163/1000 | Loss: 0.00001471
Iteration 164/1000 | Loss: 0.00001471
Iteration 165/1000 | Loss: 0.00001471
Iteration 166/1000 | Loss: 0.00001471
Iteration 167/1000 | Loss: 0.00001471
Iteration 168/1000 | Loss: 0.00001471
Iteration 169/1000 | Loss: 0.00001471
Iteration 170/1000 | Loss: 0.00001471
Iteration 171/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.4706577530887444e-05, 1.4706577530887444e-05, 1.4706577530887444e-05, 1.4706577530887444e-05, 1.4706577530887444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4706577530887444e-05

Optimization complete. Final v2v error: 3.231288194656372 mm

Highest mean error: 3.5488812923431396 mm for frame 85

Lowest mean error: 2.9438133239746094 mm for frame 15

Saving results

Total time: 38.796616077423096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387578
Iteration 2/25 | Loss: 0.00089608
Iteration 3/25 | Loss: 0.00073249
Iteration 4/25 | Loss: 0.00071751
Iteration 5/25 | Loss: 0.00071015
Iteration 6/25 | Loss: 0.00070802
Iteration 7/25 | Loss: 0.00070800
Iteration 8/25 | Loss: 0.00070800
Iteration 9/25 | Loss: 0.00070800
Iteration 10/25 | Loss: 0.00070800
Iteration 11/25 | Loss: 0.00070800
Iteration 12/25 | Loss: 0.00070800
Iteration 13/25 | Loss: 0.00070800
Iteration 14/25 | Loss: 0.00070800
Iteration 15/25 | Loss: 0.00070800
Iteration 16/25 | Loss: 0.00070800
Iteration 17/25 | Loss: 0.00070800
Iteration 18/25 | Loss: 0.00070800
Iteration 19/25 | Loss: 0.00070800
Iteration 20/25 | Loss: 0.00070800
Iteration 21/25 | Loss: 0.00070800
Iteration 22/25 | Loss: 0.00070800
Iteration 23/25 | Loss: 0.00070800
Iteration 24/25 | Loss: 0.00070800
Iteration 25/25 | Loss: 0.00070800

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73589611
Iteration 2/25 | Loss: 0.00025441
Iteration 3/25 | Loss: 0.00025441
Iteration 4/25 | Loss: 0.00025441
Iteration 5/25 | Loss: 0.00025441
Iteration 6/25 | Loss: 0.00025441
Iteration 7/25 | Loss: 0.00025441
Iteration 8/25 | Loss: 0.00025441
Iteration 9/25 | Loss: 0.00025441
Iteration 10/25 | Loss: 0.00025441
Iteration 11/25 | Loss: 0.00025441
Iteration 12/25 | Loss: 0.00025441
Iteration 13/25 | Loss: 0.00025441
Iteration 14/25 | Loss: 0.00025441
Iteration 15/25 | Loss: 0.00025441
Iteration 16/25 | Loss: 0.00025441
Iteration 17/25 | Loss: 0.00025441
Iteration 18/25 | Loss: 0.00025441
Iteration 19/25 | Loss: 0.00025441
Iteration 20/25 | Loss: 0.00025441
Iteration 21/25 | Loss: 0.00025441
Iteration 22/25 | Loss: 0.00025441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002544058079365641, 0.0002544058079365641, 0.0002544058079365641, 0.0002544058079365641, 0.0002544058079365641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002544058079365641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025441
Iteration 2/1000 | Loss: 0.00002133
Iteration 3/1000 | Loss: 0.00001459
Iteration 4/1000 | Loss: 0.00001336
Iteration 5/1000 | Loss: 0.00001249
Iteration 6/1000 | Loss: 0.00001202
Iteration 7/1000 | Loss: 0.00001179
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001146
Iteration 10/1000 | Loss: 0.00001136
Iteration 11/1000 | Loss: 0.00001127
Iteration 12/1000 | Loss: 0.00001122
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001096
Iteration 17/1000 | Loss: 0.00001092
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001088
Iteration 21/1000 | Loss: 0.00001088
Iteration 22/1000 | Loss: 0.00001087
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001084
Iteration 27/1000 | Loss: 0.00001083
Iteration 28/1000 | Loss: 0.00001083
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001082
Iteration 32/1000 | Loss: 0.00001082
Iteration 33/1000 | Loss: 0.00001082
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001077
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001075
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001074
Iteration 47/1000 | Loss: 0.00001074
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001074
Iteration 50/1000 | Loss: 0.00001074
Iteration 51/1000 | Loss: 0.00001074
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001073
Iteration 54/1000 | Loss: 0.00001073
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001072
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001072
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001072
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001072
Iteration 70/1000 | Loss: 0.00001071
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001071
Iteration 78/1000 | Loss: 0.00001071
Iteration 79/1000 | Loss: 0.00001070
Iteration 80/1000 | Loss: 0.00001070
Iteration 81/1000 | Loss: 0.00001070
Iteration 82/1000 | Loss: 0.00001070
Iteration 83/1000 | Loss: 0.00001070
Iteration 84/1000 | Loss: 0.00001070
Iteration 85/1000 | Loss: 0.00001070
Iteration 86/1000 | Loss: 0.00001070
Iteration 87/1000 | Loss: 0.00001069
Iteration 88/1000 | Loss: 0.00001069
Iteration 89/1000 | Loss: 0.00001069
Iteration 90/1000 | Loss: 0.00001069
Iteration 91/1000 | Loss: 0.00001069
Iteration 92/1000 | Loss: 0.00001069
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001069
Iteration 95/1000 | Loss: 0.00001069
Iteration 96/1000 | Loss: 0.00001069
Iteration 97/1000 | Loss: 0.00001069
Iteration 98/1000 | Loss: 0.00001069
Iteration 99/1000 | Loss: 0.00001069
Iteration 100/1000 | Loss: 0.00001069
Iteration 101/1000 | Loss: 0.00001069
Iteration 102/1000 | Loss: 0.00001069
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001069
Iteration 106/1000 | Loss: 0.00001069
Iteration 107/1000 | Loss: 0.00001069
Iteration 108/1000 | Loss: 0.00001069
Iteration 109/1000 | Loss: 0.00001069
Iteration 110/1000 | Loss: 0.00001069
Iteration 111/1000 | Loss: 0.00001069
Iteration 112/1000 | Loss: 0.00001069
Iteration 113/1000 | Loss: 0.00001069
Iteration 114/1000 | Loss: 0.00001069
Iteration 115/1000 | Loss: 0.00001069
Iteration 116/1000 | Loss: 0.00001069
Iteration 117/1000 | Loss: 0.00001069
Iteration 118/1000 | Loss: 0.00001069
Iteration 119/1000 | Loss: 0.00001069
Iteration 120/1000 | Loss: 0.00001069
Iteration 121/1000 | Loss: 0.00001069
Iteration 122/1000 | Loss: 0.00001069
Iteration 123/1000 | Loss: 0.00001069
Iteration 124/1000 | Loss: 0.00001069
Iteration 125/1000 | Loss: 0.00001069
Iteration 126/1000 | Loss: 0.00001069
Iteration 127/1000 | Loss: 0.00001069
Iteration 128/1000 | Loss: 0.00001069
Iteration 129/1000 | Loss: 0.00001069
Iteration 130/1000 | Loss: 0.00001069
Iteration 131/1000 | Loss: 0.00001069
Iteration 132/1000 | Loss: 0.00001069
Iteration 133/1000 | Loss: 0.00001069
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.0693221156543586e-05, 1.0693221156543586e-05, 1.0693221156543586e-05, 1.0693221156543586e-05, 1.0693221156543586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0693221156543586e-05

Optimization complete. Final v2v error: 2.7800710201263428 mm

Highest mean error: 2.999321937561035 mm for frame 53

Lowest mean error: 2.6543941497802734 mm for frame 195

Saving results

Total time: 38.855809688568115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480629
Iteration 2/25 | Loss: 0.00111918
Iteration 3/25 | Loss: 0.00078825
Iteration 4/25 | Loss: 0.00072730
Iteration 5/25 | Loss: 0.00072043
Iteration 6/25 | Loss: 0.00071775
Iteration 7/25 | Loss: 0.00071686
Iteration 8/25 | Loss: 0.00071686
Iteration 9/25 | Loss: 0.00071686
Iteration 10/25 | Loss: 0.00071686
Iteration 11/25 | Loss: 0.00071686
Iteration 12/25 | Loss: 0.00071686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007168602896854281, 0.0007168602896854281, 0.0007168602896854281, 0.0007168602896854281, 0.0007168602896854281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007168602896854281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47351396
Iteration 2/25 | Loss: 0.00024367
Iteration 3/25 | Loss: 0.00024367
Iteration 4/25 | Loss: 0.00024367
Iteration 5/25 | Loss: 0.00024367
Iteration 6/25 | Loss: 0.00024367
Iteration 7/25 | Loss: 0.00024367
Iteration 8/25 | Loss: 0.00024367
Iteration 9/25 | Loss: 0.00024367
Iteration 10/25 | Loss: 0.00024367
Iteration 11/25 | Loss: 0.00024367
Iteration 12/25 | Loss: 0.00024367
Iteration 13/25 | Loss: 0.00024367
Iteration 14/25 | Loss: 0.00024367
Iteration 15/25 | Loss: 0.00024367
Iteration 16/25 | Loss: 0.00024367
Iteration 17/25 | Loss: 0.00024367
Iteration 18/25 | Loss: 0.00024367
Iteration 19/25 | Loss: 0.00024367
Iteration 20/25 | Loss: 0.00024367
Iteration 21/25 | Loss: 0.00024367
Iteration 22/25 | Loss: 0.00024367
Iteration 23/25 | Loss: 0.00024367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00024366710567846894, 0.00024366710567846894, 0.00024366710567846894, 0.00024366710567846894, 0.00024366710567846894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024366710567846894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024367
Iteration 2/1000 | Loss: 0.00002580
Iteration 3/1000 | Loss: 0.00001704
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001506
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001424
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001360
Iteration 11/1000 | Loss: 0.00001357
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001334
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001330
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001328
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001326
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001326
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001325
Iteration 35/1000 | Loss: 0.00001325
Iteration 36/1000 | Loss: 0.00001325
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001325
Iteration 40/1000 | Loss: 0.00001325
Iteration 41/1000 | Loss: 0.00001325
Iteration 42/1000 | Loss: 0.00001325
Iteration 43/1000 | Loss: 0.00001324
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001324
Iteration 46/1000 | Loss: 0.00001323
Iteration 47/1000 | Loss: 0.00001323
Iteration 48/1000 | Loss: 0.00001323
Iteration 49/1000 | Loss: 0.00001323
Iteration 50/1000 | Loss: 0.00001323
Iteration 51/1000 | Loss: 0.00001323
Iteration 52/1000 | Loss: 0.00001323
Iteration 53/1000 | Loss: 0.00001323
Iteration 54/1000 | Loss: 0.00001323
Iteration 55/1000 | Loss: 0.00001322
Iteration 56/1000 | Loss: 0.00001322
Iteration 57/1000 | Loss: 0.00001322
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001322
Iteration 60/1000 | Loss: 0.00001322
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001320
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00001320
Iteration 69/1000 | Loss: 0.00001320
Iteration 70/1000 | Loss: 0.00001319
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001319
Iteration 73/1000 | Loss: 0.00001319
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001318
Iteration 77/1000 | Loss: 0.00001318
Iteration 78/1000 | Loss: 0.00001318
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001317
Iteration 83/1000 | Loss: 0.00001317
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001317
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001315
Iteration 95/1000 | Loss: 0.00001315
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001315
Iteration 103/1000 | Loss: 0.00001315
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001314
Iteration 106/1000 | Loss: 0.00001314
Iteration 107/1000 | Loss: 0.00001314
Iteration 108/1000 | Loss: 0.00001314
Iteration 109/1000 | Loss: 0.00001313
Iteration 110/1000 | Loss: 0.00001313
Iteration 111/1000 | Loss: 0.00001313
Iteration 112/1000 | Loss: 0.00001313
Iteration 113/1000 | Loss: 0.00001313
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001311
Iteration 119/1000 | Loss: 0.00001311
Iteration 120/1000 | Loss: 0.00001311
Iteration 121/1000 | Loss: 0.00001311
Iteration 122/1000 | Loss: 0.00001311
Iteration 123/1000 | Loss: 0.00001311
Iteration 124/1000 | Loss: 0.00001311
Iteration 125/1000 | Loss: 0.00001311
Iteration 126/1000 | Loss: 0.00001310
Iteration 127/1000 | Loss: 0.00001310
Iteration 128/1000 | Loss: 0.00001310
Iteration 129/1000 | Loss: 0.00001310
Iteration 130/1000 | Loss: 0.00001310
Iteration 131/1000 | Loss: 0.00001310
Iteration 132/1000 | Loss: 0.00001310
Iteration 133/1000 | Loss: 0.00001310
Iteration 134/1000 | Loss: 0.00001310
Iteration 135/1000 | Loss: 0.00001310
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001308
Iteration 141/1000 | Loss: 0.00001308
Iteration 142/1000 | Loss: 0.00001308
Iteration 143/1000 | Loss: 0.00001308
Iteration 144/1000 | Loss: 0.00001308
Iteration 145/1000 | Loss: 0.00001308
Iteration 146/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.3082619261695072e-05, 1.3082619261695072e-05, 1.3082619261695072e-05, 1.3082619261695072e-05, 1.3082619261695072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3082619261695072e-05

Optimization complete. Final v2v error: 2.8715295791625977 mm

Highest mean error: 4.4435553550720215 mm for frame 82

Lowest mean error: 2.3805670738220215 mm for frame 154

Saving results

Total time: 38.79927110671997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851037
Iteration 2/25 | Loss: 0.00144305
Iteration 3/25 | Loss: 0.00088505
Iteration 4/25 | Loss: 0.00081319
Iteration 5/25 | Loss: 0.00078195
Iteration 6/25 | Loss: 0.00076473
Iteration 7/25 | Loss: 0.00075821
Iteration 8/25 | Loss: 0.00075666
Iteration 9/25 | Loss: 0.00074391
Iteration 10/25 | Loss: 0.00073991
Iteration 11/25 | Loss: 0.00073194
Iteration 12/25 | Loss: 0.00072916
Iteration 13/25 | Loss: 0.00072780
Iteration 14/25 | Loss: 0.00072729
Iteration 15/25 | Loss: 0.00072713
Iteration 16/25 | Loss: 0.00072710
Iteration 17/25 | Loss: 0.00072710
Iteration 18/25 | Loss: 0.00072710
Iteration 19/25 | Loss: 0.00072710
Iteration 20/25 | Loss: 0.00072710
Iteration 21/25 | Loss: 0.00072709
Iteration 22/25 | Loss: 0.00072709
Iteration 23/25 | Loss: 0.00072709
Iteration 24/25 | Loss: 0.00072709
Iteration 25/25 | Loss: 0.00072709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98299122
Iteration 2/25 | Loss: 0.00027154
Iteration 3/25 | Loss: 0.00027153
Iteration 4/25 | Loss: 0.00027153
Iteration 5/25 | Loss: 0.00027153
Iteration 6/25 | Loss: 0.00027153
Iteration 7/25 | Loss: 0.00027153
Iteration 8/25 | Loss: 0.00027153
Iteration 9/25 | Loss: 0.00027153
Iteration 10/25 | Loss: 0.00027153
Iteration 11/25 | Loss: 0.00027153
Iteration 12/25 | Loss: 0.00027153
Iteration 13/25 | Loss: 0.00027153
Iteration 14/25 | Loss: 0.00027153
Iteration 15/25 | Loss: 0.00027153
Iteration 16/25 | Loss: 0.00027153
Iteration 17/25 | Loss: 0.00027153
Iteration 18/25 | Loss: 0.00027153
Iteration 19/25 | Loss: 0.00027153
Iteration 20/25 | Loss: 0.00027153
Iteration 21/25 | Loss: 0.00027153
Iteration 22/25 | Loss: 0.00027153
Iteration 23/25 | Loss: 0.00027153
Iteration 24/25 | Loss: 0.00027153
Iteration 25/25 | Loss: 0.00027153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027153
Iteration 2/1000 | Loss: 0.00002611
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001553
Iteration 6/1000 | Loss: 0.00001518
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001472
Iteration 9/1000 | Loss: 0.00001470
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001457
Iteration 13/1000 | Loss: 0.00001453
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001451
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001449
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001443
Iteration 26/1000 | Loss: 0.00001439
Iteration 27/1000 | Loss: 0.00001433
Iteration 28/1000 | Loss: 0.00001427
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001425
Iteration 31/1000 | Loss: 0.00001424
Iteration 32/1000 | Loss: 0.00001423
Iteration 33/1000 | Loss: 0.00001423
Iteration 34/1000 | Loss: 0.00001422
Iteration 35/1000 | Loss: 0.00001421
Iteration 36/1000 | Loss: 0.00001420
Iteration 37/1000 | Loss: 0.00001419
Iteration 38/1000 | Loss: 0.00001419
Iteration 39/1000 | Loss: 0.00001419
Iteration 40/1000 | Loss: 0.00001418
Iteration 41/1000 | Loss: 0.00001418
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001418
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001415
Iteration 52/1000 | Loss: 0.00001415
Iteration 53/1000 | Loss: 0.00001415
Iteration 54/1000 | Loss: 0.00001414
Iteration 55/1000 | Loss: 0.00001414
Iteration 56/1000 | Loss: 0.00001414
Iteration 57/1000 | Loss: 0.00001413
Iteration 58/1000 | Loss: 0.00001413
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001412
Iteration 61/1000 | Loss: 0.00001411
Iteration 62/1000 | Loss: 0.00001411
Iteration 63/1000 | Loss: 0.00001411
Iteration 64/1000 | Loss: 0.00001411
Iteration 65/1000 | Loss: 0.00001411
Iteration 66/1000 | Loss: 0.00001411
Iteration 67/1000 | Loss: 0.00001410
Iteration 68/1000 | Loss: 0.00001410
Iteration 69/1000 | Loss: 0.00001410
Iteration 70/1000 | Loss: 0.00001410
Iteration 71/1000 | Loss: 0.00001410
Iteration 72/1000 | Loss: 0.00001410
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001408
Iteration 76/1000 | Loss: 0.00001408
Iteration 77/1000 | Loss: 0.00001408
Iteration 78/1000 | Loss: 0.00001407
Iteration 79/1000 | Loss: 0.00001407
Iteration 80/1000 | Loss: 0.00001407
Iteration 81/1000 | Loss: 0.00001406
Iteration 82/1000 | Loss: 0.00001406
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001403
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001402
Iteration 92/1000 | Loss: 0.00001401
Iteration 93/1000 | Loss: 0.00001400
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001400
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001397
Iteration 109/1000 | Loss: 0.00001397
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001397
Iteration 117/1000 | Loss: 0.00001397
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001397
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001397
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001397
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001397
Iteration 130/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.3966970982437488e-05, 1.3966970982437488e-05, 1.3966970982437488e-05, 1.3966970982437488e-05, 1.3966970982437488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3966970982437488e-05

Optimization complete. Final v2v error: 3.17004132270813 mm

Highest mean error: 3.6806297302246094 mm for frame 222

Lowest mean error: 2.6674909591674805 mm for frame 38

Saving results

Total time: 59.766979455947876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848250
Iteration 2/25 | Loss: 0.00087362
Iteration 3/25 | Loss: 0.00077698
Iteration 4/25 | Loss: 0.00075032
Iteration 5/25 | Loss: 0.00074501
Iteration 6/25 | Loss: 0.00074397
Iteration 7/25 | Loss: 0.00074397
Iteration 8/25 | Loss: 0.00074397
Iteration 9/25 | Loss: 0.00074397
Iteration 10/25 | Loss: 0.00074397
Iteration 11/25 | Loss: 0.00074397
Iteration 12/25 | Loss: 0.00074397
Iteration 13/25 | Loss: 0.00074397
Iteration 14/25 | Loss: 0.00074397
Iteration 15/25 | Loss: 0.00074397
Iteration 16/25 | Loss: 0.00074397
Iteration 17/25 | Loss: 0.00074397
Iteration 18/25 | Loss: 0.00074397
Iteration 19/25 | Loss: 0.00074397
Iteration 20/25 | Loss: 0.00074397
Iteration 21/25 | Loss: 0.00074397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007439711480401456, 0.0007439711480401456, 0.0007439711480401456, 0.0007439711480401456, 0.0007439711480401456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007439711480401456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11171150
Iteration 2/25 | Loss: 0.00023830
Iteration 3/25 | Loss: 0.00023828
Iteration 4/25 | Loss: 0.00023828
Iteration 5/25 | Loss: 0.00023828
Iteration 6/25 | Loss: 0.00023827
Iteration 7/25 | Loss: 0.00023827
Iteration 8/25 | Loss: 0.00023827
Iteration 9/25 | Loss: 0.00023827
Iteration 10/25 | Loss: 0.00023827
Iteration 11/25 | Loss: 0.00023827
Iteration 12/25 | Loss: 0.00023827
Iteration 13/25 | Loss: 0.00023827
Iteration 14/25 | Loss: 0.00023827
Iteration 15/25 | Loss: 0.00023827
Iteration 16/25 | Loss: 0.00023827
Iteration 17/25 | Loss: 0.00023827
Iteration 18/25 | Loss: 0.00023827
Iteration 19/25 | Loss: 0.00023827
Iteration 20/25 | Loss: 0.00023827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00023827370023354888, 0.00023827370023354888, 0.00023827370023354888, 0.00023827370023354888, 0.00023827370023354888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023827370023354888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023827
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002359
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00002069
Iteration 6/1000 | Loss: 0.00002008
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001901
Iteration 10/1000 | Loss: 0.00001889
Iteration 11/1000 | Loss: 0.00001883
Iteration 12/1000 | Loss: 0.00001879
Iteration 13/1000 | Loss: 0.00001876
Iteration 14/1000 | Loss: 0.00001873
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001871
Iteration 19/1000 | Loss: 0.00001871
Iteration 20/1000 | Loss: 0.00001871
Iteration 21/1000 | Loss: 0.00001871
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001869
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001865
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001865
Iteration 29/1000 | Loss: 0.00001865
Iteration 30/1000 | Loss: 0.00001864
Iteration 31/1000 | Loss: 0.00001864
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001862
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001860
Iteration 41/1000 | Loss: 0.00001860
Iteration 42/1000 | Loss: 0.00001860
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001856
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001850
Iteration 69/1000 | Loss: 0.00001850
Iteration 70/1000 | Loss: 0.00001845
Iteration 71/1000 | Loss: 0.00001842
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001837
Iteration 76/1000 | Loss: 0.00001837
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001833
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001830
Iteration 84/1000 | Loss: 0.00001829
Iteration 85/1000 | Loss: 0.00001829
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001827
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001827
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001825
Iteration 105/1000 | Loss: 0.00001825
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001824
Iteration 109/1000 | Loss: 0.00001823
Iteration 110/1000 | Loss: 0.00001823
Iteration 111/1000 | Loss: 0.00001823
Iteration 112/1000 | Loss: 0.00001823
Iteration 113/1000 | Loss: 0.00001823
Iteration 114/1000 | Loss: 0.00001823
Iteration 115/1000 | Loss: 0.00001822
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001821
Iteration 119/1000 | Loss: 0.00001821
Iteration 120/1000 | Loss: 0.00001820
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.8202765204478055e-05, 1.8202765204478055e-05, 1.8202765204478055e-05, 1.8202765204478055e-05, 1.8202765204478055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8202765204478055e-05

Optimization complete. Final v2v error: 3.640287399291992 mm

Highest mean error: 4.31807279586792 mm for frame 140

Lowest mean error: 3.124340057373047 mm for frame 22

Saving results

Total time: 40.77135872840881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439104
Iteration 2/25 | Loss: 0.00084035
Iteration 3/25 | Loss: 0.00071969
Iteration 4/25 | Loss: 0.00070368
Iteration 5/25 | Loss: 0.00069718
Iteration 6/25 | Loss: 0.00069553
Iteration 7/25 | Loss: 0.00069499
Iteration 8/25 | Loss: 0.00069499
Iteration 9/25 | Loss: 0.00069499
Iteration 10/25 | Loss: 0.00069499
Iteration 11/25 | Loss: 0.00069499
Iteration 12/25 | Loss: 0.00069499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000694994640070945, 0.000694994640070945, 0.000694994640070945, 0.000694994640070945, 0.000694994640070945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000694994640070945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46655977
Iteration 2/25 | Loss: 0.00023923
Iteration 3/25 | Loss: 0.00023921
Iteration 4/25 | Loss: 0.00023921
Iteration 5/25 | Loss: 0.00023921
Iteration 6/25 | Loss: 0.00023921
Iteration 7/25 | Loss: 0.00023921
Iteration 8/25 | Loss: 0.00023921
Iteration 9/25 | Loss: 0.00023921
Iteration 10/25 | Loss: 0.00023921
Iteration 11/25 | Loss: 0.00023921
Iteration 12/25 | Loss: 0.00023921
Iteration 13/25 | Loss: 0.00023921
Iteration 14/25 | Loss: 0.00023921
Iteration 15/25 | Loss: 0.00023921
Iteration 16/25 | Loss: 0.00023921
Iteration 17/25 | Loss: 0.00023921
Iteration 18/25 | Loss: 0.00023921
Iteration 19/25 | Loss: 0.00023921
Iteration 20/25 | Loss: 0.00023921
Iteration 21/25 | Loss: 0.00023921
Iteration 22/25 | Loss: 0.00023921
Iteration 23/25 | Loss: 0.00023921
Iteration 24/25 | Loss: 0.00023921
Iteration 25/25 | Loss: 0.00023921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023921
Iteration 2/1000 | Loss: 0.00001907
Iteration 3/1000 | Loss: 0.00001400
Iteration 4/1000 | Loss: 0.00001282
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001163
Iteration 7/1000 | Loss: 0.00001136
Iteration 8/1000 | Loss: 0.00001133
Iteration 9/1000 | Loss: 0.00001115
Iteration 10/1000 | Loss: 0.00001105
Iteration 11/1000 | Loss: 0.00001105
Iteration 12/1000 | Loss: 0.00001102
Iteration 13/1000 | Loss: 0.00001101
Iteration 14/1000 | Loss: 0.00001099
Iteration 15/1000 | Loss: 0.00001099
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001096
Iteration 20/1000 | Loss: 0.00001096
Iteration 21/1000 | Loss: 0.00001096
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001094
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001092
Iteration 26/1000 | Loss: 0.00001092
Iteration 27/1000 | Loss: 0.00001091
Iteration 28/1000 | Loss: 0.00001091
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001090
Iteration 32/1000 | Loss: 0.00001089
Iteration 33/1000 | Loss: 0.00001089
Iteration 34/1000 | Loss: 0.00001088
Iteration 35/1000 | Loss: 0.00001088
Iteration 36/1000 | Loss: 0.00001087
Iteration 37/1000 | Loss: 0.00001086
Iteration 38/1000 | Loss: 0.00001086
Iteration 39/1000 | Loss: 0.00001086
Iteration 40/1000 | Loss: 0.00001086
Iteration 41/1000 | Loss: 0.00001086
Iteration 42/1000 | Loss: 0.00001086
Iteration 43/1000 | Loss: 0.00001085
Iteration 44/1000 | Loss: 0.00001085
Iteration 45/1000 | Loss: 0.00001084
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001083
Iteration 53/1000 | Loss: 0.00001083
Iteration 54/1000 | Loss: 0.00001083
Iteration 55/1000 | Loss: 0.00001083
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00001081
Iteration 61/1000 | Loss: 0.00001081
Iteration 62/1000 | Loss: 0.00001081
Iteration 63/1000 | Loss: 0.00001080
Iteration 64/1000 | Loss: 0.00001080
Iteration 65/1000 | Loss: 0.00001080
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001080
Iteration 69/1000 | Loss: 0.00001079
Iteration 70/1000 | Loss: 0.00001079
Iteration 71/1000 | Loss: 0.00001079
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001078
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001077
Iteration 77/1000 | Loss: 0.00001077
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001076
Iteration 88/1000 | Loss: 0.00001076
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001075
Iteration 94/1000 | Loss: 0.00001075
Iteration 95/1000 | Loss: 0.00001075
Iteration 96/1000 | Loss: 0.00001075
Iteration 97/1000 | Loss: 0.00001075
Iteration 98/1000 | Loss: 0.00001075
Iteration 99/1000 | Loss: 0.00001075
Iteration 100/1000 | Loss: 0.00001075
Iteration 101/1000 | Loss: 0.00001075
Iteration 102/1000 | Loss: 0.00001075
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001074
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001074
Iteration 116/1000 | Loss: 0.00001074
Iteration 117/1000 | Loss: 0.00001074
Iteration 118/1000 | Loss: 0.00001074
Iteration 119/1000 | Loss: 0.00001074
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.0738563105405774e-05, 1.0738563105405774e-05, 1.0738563105405774e-05, 1.0738563105405774e-05, 1.0738563105405774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0738563105405774e-05

Optimization complete. Final v2v error: 2.8063488006591797 mm

Highest mean error: 3.071821928024292 mm for frame 109

Lowest mean error: 2.5703651905059814 mm for frame 19

Saving results

Total time: 31.44744849205017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466972
Iteration 2/25 | Loss: 0.00096280
Iteration 3/25 | Loss: 0.00076747
Iteration 4/25 | Loss: 0.00074712
Iteration 5/25 | Loss: 0.00074063
Iteration 6/25 | Loss: 0.00073899
Iteration 7/25 | Loss: 0.00073869
Iteration 8/25 | Loss: 0.00073869
Iteration 9/25 | Loss: 0.00073869
Iteration 10/25 | Loss: 0.00073869
Iteration 11/25 | Loss: 0.00073869
Iteration 12/25 | Loss: 0.00073869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007386893266811967, 0.0007386893266811967, 0.0007386893266811967, 0.0007386893266811967, 0.0007386893266811967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007386893266811967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46545625
Iteration 2/25 | Loss: 0.00025784
Iteration 3/25 | Loss: 0.00025783
Iteration 4/25 | Loss: 0.00025783
Iteration 5/25 | Loss: 0.00025783
Iteration 6/25 | Loss: 0.00025783
Iteration 7/25 | Loss: 0.00025783
Iteration 8/25 | Loss: 0.00025783
Iteration 9/25 | Loss: 0.00025783
Iteration 10/25 | Loss: 0.00025783
Iteration 11/25 | Loss: 0.00025783
Iteration 12/25 | Loss: 0.00025783
Iteration 13/25 | Loss: 0.00025783
Iteration 14/25 | Loss: 0.00025783
Iteration 15/25 | Loss: 0.00025783
Iteration 16/25 | Loss: 0.00025783
Iteration 17/25 | Loss: 0.00025783
Iteration 18/25 | Loss: 0.00025783
Iteration 19/25 | Loss: 0.00025783
Iteration 20/25 | Loss: 0.00025783
Iteration 21/25 | Loss: 0.00025783
Iteration 22/25 | Loss: 0.00025783
Iteration 23/25 | Loss: 0.00025783
Iteration 24/25 | Loss: 0.00025783
Iteration 25/25 | Loss: 0.00025783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025783
Iteration 2/1000 | Loss: 0.00002195
Iteration 3/1000 | Loss: 0.00001757
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001596
Iteration 6/1000 | Loss: 0.00001555
Iteration 7/1000 | Loss: 0.00001531
Iteration 8/1000 | Loss: 0.00001516
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001494
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001488
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001468
Iteration 23/1000 | Loss: 0.00001467
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001467
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001462
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001459
Iteration 33/1000 | Loss: 0.00001456
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001454
Iteration 37/1000 | Loss: 0.00001454
Iteration 38/1000 | Loss: 0.00001453
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001448
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001447
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001447
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001447
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001446
Iteration 71/1000 | Loss: 0.00001446
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001445
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001445
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001443
Iteration 110/1000 | Loss: 0.00001443
Iteration 111/1000 | Loss: 0.00001443
Iteration 112/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.4430109331442509e-05, 1.4430109331442509e-05, 1.4430109331442509e-05, 1.4430109331442509e-05, 1.4430109331442509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4430109331442509e-05

Optimization complete. Final v2v error: 3.180818796157837 mm

Highest mean error: 3.481390953063965 mm for frame 126

Lowest mean error: 2.850008964538574 mm for frame 185

Saving results

Total time: 39.090073347091675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849968
Iteration 2/25 | Loss: 0.00156458
Iteration 3/25 | Loss: 0.00109806
Iteration 4/25 | Loss: 0.00107440
Iteration 5/25 | Loss: 0.00106715
Iteration 6/25 | Loss: 0.00106599
Iteration 7/25 | Loss: 0.00106589
Iteration 8/25 | Loss: 0.00106589
Iteration 9/25 | Loss: 0.00106589
Iteration 10/25 | Loss: 0.00106590
Iteration 11/25 | Loss: 0.00106589
Iteration 12/25 | Loss: 0.00106589
Iteration 13/25 | Loss: 0.00106589
Iteration 14/25 | Loss: 0.00106589
Iteration 15/25 | Loss: 0.00106589
Iteration 16/25 | Loss: 0.00106589
Iteration 17/25 | Loss: 0.00106589
Iteration 18/25 | Loss: 0.00106589
Iteration 19/25 | Loss: 0.00106589
Iteration 20/25 | Loss: 0.00106590
Iteration 21/25 | Loss: 0.00106590
Iteration 22/25 | Loss: 0.00106589
Iteration 23/25 | Loss: 0.00106589
Iteration 24/25 | Loss: 0.00106590
Iteration 25/25 | Loss: 0.00106590

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60892534
Iteration 2/25 | Loss: 0.00047348
Iteration 3/25 | Loss: 0.00047347
Iteration 4/25 | Loss: 0.00047347
Iteration 5/25 | Loss: 0.00047347
Iteration 6/25 | Loss: 0.00047347
Iteration 7/25 | Loss: 0.00047347
Iteration 8/25 | Loss: 0.00047347
Iteration 9/25 | Loss: 0.00047347
Iteration 10/25 | Loss: 0.00047347
Iteration 11/25 | Loss: 0.00047347
Iteration 12/25 | Loss: 0.00047347
Iteration 13/25 | Loss: 0.00047347
Iteration 14/25 | Loss: 0.00047347
Iteration 15/25 | Loss: 0.00047347
Iteration 16/25 | Loss: 0.00047347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00047347272629849613, 0.00047347272629849613, 0.00047347272629849613, 0.00047347272629849613, 0.00047347272629849613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00047347272629849613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047347
Iteration 2/1000 | Loss: 0.00006262
Iteration 3/1000 | Loss: 0.00004780
Iteration 4/1000 | Loss: 0.00004214
Iteration 5/1000 | Loss: 0.00004040
Iteration 6/1000 | Loss: 0.00003953
Iteration 7/1000 | Loss: 0.00003891
Iteration 8/1000 | Loss: 0.00003826
Iteration 9/1000 | Loss: 0.00003777
Iteration 10/1000 | Loss: 0.00003739
Iteration 11/1000 | Loss: 0.00003711
Iteration 12/1000 | Loss: 0.00003680
Iteration 13/1000 | Loss: 0.00003654
Iteration 14/1000 | Loss: 0.00003622
Iteration 15/1000 | Loss: 0.00003595
Iteration 16/1000 | Loss: 0.00003574
Iteration 17/1000 | Loss: 0.00003556
Iteration 18/1000 | Loss: 0.00003539
Iteration 19/1000 | Loss: 0.00003524
Iteration 20/1000 | Loss: 0.00003522
Iteration 21/1000 | Loss: 0.00003520
Iteration 22/1000 | Loss: 0.00003518
Iteration 23/1000 | Loss: 0.00003512
Iteration 24/1000 | Loss: 0.00003508
Iteration 25/1000 | Loss: 0.00003508
Iteration 26/1000 | Loss: 0.00003507
Iteration 27/1000 | Loss: 0.00003507
Iteration 28/1000 | Loss: 0.00003504
Iteration 29/1000 | Loss: 0.00003502
Iteration 30/1000 | Loss: 0.00003501
Iteration 31/1000 | Loss: 0.00003501
Iteration 32/1000 | Loss: 0.00003499
Iteration 33/1000 | Loss: 0.00003499
Iteration 34/1000 | Loss: 0.00003499
Iteration 35/1000 | Loss: 0.00003498
Iteration 36/1000 | Loss: 0.00003498
Iteration 37/1000 | Loss: 0.00003498
Iteration 38/1000 | Loss: 0.00003496
Iteration 39/1000 | Loss: 0.00003496
Iteration 40/1000 | Loss: 0.00003496
Iteration 41/1000 | Loss: 0.00003496
Iteration 42/1000 | Loss: 0.00003496
Iteration 43/1000 | Loss: 0.00003496
Iteration 44/1000 | Loss: 0.00003496
Iteration 45/1000 | Loss: 0.00003496
Iteration 46/1000 | Loss: 0.00003496
Iteration 47/1000 | Loss: 0.00003496
Iteration 48/1000 | Loss: 0.00003496
Iteration 49/1000 | Loss: 0.00003495
Iteration 50/1000 | Loss: 0.00003494
Iteration 51/1000 | Loss: 0.00003494
Iteration 52/1000 | Loss: 0.00003494
Iteration 53/1000 | Loss: 0.00003494
Iteration 54/1000 | Loss: 0.00003494
Iteration 55/1000 | Loss: 0.00003494
Iteration 56/1000 | Loss: 0.00003493
Iteration 57/1000 | Loss: 0.00003493
Iteration 58/1000 | Loss: 0.00003492
Iteration 59/1000 | Loss: 0.00003492
Iteration 60/1000 | Loss: 0.00003492
Iteration 61/1000 | Loss: 0.00003492
Iteration 62/1000 | Loss: 0.00003492
Iteration 63/1000 | Loss: 0.00003492
Iteration 64/1000 | Loss: 0.00003492
Iteration 65/1000 | Loss: 0.00003492
Iteration 66/1000 | Loss: 0.00003491
Iteration 67/1000 | Loss: 0.00003491
Iteration 68/1000 | Loss: 0.00003491
Iteration 69/1000 | Loss: 0.00003491
Iteration 70/1000 | Loss: 0.00003491
Iteration 71/1000 | Loss: 0.00003491
Iteration 72/1000 | Loss: 0.00003490
Iteration 73/1000 | Loss: 0.00003490
Iteration 74/1000 | Loss: 0.00003490
Iteration 75/1000 | Loss: 0.00003490
Iteration 76/1000 | Loss: 0.00003490
Iteration 77/1000 | Loss: 0.00003490
Iteration 78/1000 | Loss: 0.00003490
Iteration 79/1000 | Loss: 0.00003490
Iteration 80/1000 | Loss: 0.00003490
Iteration 81/1000 | Loss: 0.00003490
Iteration 82/1000 | Loss: 0.00003490
Iteration 83/1000 | Loss: 0.00003490
Iteration 84/1000 | Loss: 0.00003490
Iteration 85/1000 | Loss: 0.00003489
Iteration 86/1000 | Loss: 0.00003489
Iteration 87/1000 | Loss: 0.00003488
Iteration 88/1000 | Loss: 0.00003488
Iteration 89/1000 | Loss: 0.00003487
Iteration 90/1000 | Loss: 0.00003487
Iteration 91/1000 | Loss: 0.00003486
Iteration 92/1000 | Loss: 0.00003485
Iteration 93/1000 | Loss: 0.00003485
Iteration 94/1000 | Loss: 0.00003485
Iteration 95/1000 | Loss: 0.00003485
Iteration 96/1000 | Loss: 0.00003485
Iteration 97/1000 | Loss: 0.00003485
Iteration 98/1000 | Loss: 0.00003485
Iteration 99/1000 | Loss: 0.00003485
Iteration 100/1000 | Loss: 0.00003484
Iteration 101/1000 | Loss: 0.00003484
Iteration 102/1000 | Loss: 0.00003484
Iteration 103/1000 | Loss: 0.00003483
Iteration 104/1000 | Loss: 0.00003483
Iteration 105/1000 | Loss: 0.00003482
Iteration 106/1000 | Loss: 0.00003482
Iteration 107/1000 | Loss: 0.00003482
Iteration 108/1000 | Loss: 0.00003481
Iteration 109/1000 | Loss: 0.00003481
Iteration 110/1000 | Loss: 0.00003481
Iteration 111/1000 | Loss: 0.00003481
Iteration 112/1000 | Loss: 0.00003481
Iteration 113/1000 | Loss: 0.00003481
Iteration 114/1000 | Loss: 0.00003481
Iteration 115/1000 | Loss: 0.00003481
Iteration 116/1000 | Loss: 0.00003481
Iteration 117/1000 | Loss: 0.00003481
Iteration 118/1000 | Loss: 0.00003481
Iteration 119/1000 | Loss: 0.00003480
Iteration 120/1000 | Loss: 0.00003480
Iteration 121/1000 | Loss: 0.00003480
Iteration 122/1000 | Loss: 0.00003480
Iteration 123/1000 | Loss: 0.00003480
Iteration 124/1000 | Loss: 0.00003479
Iteration 125/1000 | Loss: 0.00003479
Iteration 126/1000 | Loss: 0.00003479
Iteration 127/1000 | Loss: 0.00003479
Iteration 128/1000 | Loss: 0.00003479
Iteration 129/1000 | Loss: 0.00003479
Iteration 130/1000 | Loss: 0.00003479
Iteration 131/1000 | Loss: 0.00003479
Iteration 132/1000 | Loss: 0.00003478
Iteration 133/1000 | Loss: 0.00003478
Iteration 134/1000 | Loss: 0.00003478
Iteration 135/1000 | Loss: 0.00003478
Iteration 136/1000 | Loss: 0.00003478
Iteration 137/1000 | Loss: 0.00003478
Iteration 138/1000 | Loss: 0.00003478
Iteration 139/1000 | Loss: 0.00003478
Iteration 140/1000 | Loss: 0.00003478
Iteration 141/1000 | Loss: 0.00003477
Iteration 142/1000 | Loss: 0.00003477
Iteration 143/1000 | Loss: 0.00003477
Iteration 144/1000 | Loss: 0.00003477
Iteration 145/1000 | Loss: 0.00003477
Iteration 146/1000 | Loss: 0.00003477
Iteration 147/1000 | Loss: 0.00003477
Iteration 148/1000 | Loss: 0.00003477
Iteration 149/1000 | Loss: 0.00003477
Iteration 150/1000 | Loss: 0.00003477
Iteration 151/1000 | Loss: 0.00003477
Iteration 152/1000 | Loss: 0.00003477
Iteration 153/1000 | Loss: 0.00003477
Iteration 154/1000 | Loss: 0.00003477
Iteration 155/1000 | Loss: 0.00003477
Iteration 156/1000 | Loss: 0.00003477
Iteration 157/1000 | Loss: 0.00003477
Iteration 158/1000 | Loss: 0.00003477
Iteration 159/1000 | Loss: 0.00003477
Iteration 160/1000 | Loss: 0.00003477
Iteration 161/1000 | Loss: 0.00003477
Iteration 162/1000 | Loss: 0.00003477
Iteration 163/1000 | Loss: 0.00003477
Iteration 164/1000 | Loss: 0.00003477
Iteration 165/1000 | Loss: 0.00003477
Iteration 166/1000 | Loss: 0.00003477
Iteration 167/1000 | Loss: 0.00003477
Iteration 168/1000 | Loss: 0.00003477
Iteration 169/1000 | Loss: 0.00003477
Iteration 170/1000 | Loss: 0.00003477
Iteration 171/1000 | Loss: 0.00003477
Iteration 172/1000 | Loss: 0.00003477
Iteration 173/1000 | Loss: 0.00003477
Iteration 174/1000 | Loss: 0.00003477
Iteration 175/1000 | Loss: 0.00003477
Iteration 176/1000 | Loss: 0.00003477
Iteration 177/1000 | Loss: 0.00003477
Iteration 178/1000 | Loss: 0.00003477
Iteration 179/1000 | Loss: 0.00003477
Iteration 180/1000 | Loss: 0.00003477
Iteration 181/1000 | Loss: 0.00003477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [3.477235077298246e-05, 3.477235077298246e-05, 3.477235077298246e-05, 3.477235077298246e-05, 3.477235077298246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.477235077298246e-05

Optimization complete. Final v2v error: 4.7052130699157715 mm

Highest mean error: 5.815122127532959 mm for frame 164

Lowest mean error: 3.66520094871521 mm for frame 10

Saving results

Total time: 49.80809688568115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796979
Iteration 2/25 | Loss: 0.00107618
Iteration 3/25 | Loss: 0.00076649
Iteration 4/25 | Loss: 0.00072051
Iteration 5/25 | Loss: 0.00071325
Iteration 6/25 | Loss: 0.00071048
Iteration 7/25 | Loss: 0.00070993
Iteration 8/25 | Loss: 0.00070993
Iteration 9/25 | Loss: 0.00070993
Iteration 10/25 | Loss: 0.00070993
Iteration 11/25 | Loss: 0.00070993
Iteration 12/25 | Loss: 0.00070993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007099341601133347, 0.0007099341601133347, 0.0007099341601133347, 0.0007099341601133347, 0.0007099341601133347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007099341601133347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46781766
Iteration 2/25 | Loss: 0.00023896
Iteration 3/25 | Loss: 0.00023896
Iteration 4/25 | Loss: 0.00023896
Iteration 5/25 | Loss: 0.00023895
Iteration 6/25 | Loss: 0.00023895
Iteration 7/25 | Loss: 0.00023895
Iteration 8/25 | Loss: 0.00023895
Iteration 9/25 | Loss: 0.00023895
Iteration 10/25 | Loss: 0.00023895
Iteration 11/25 | Loss: 0.00023895
Iteration 12/25 | Loss: 0.00023895
Iteration 13/25 | Loss: 0.00023895
Iteration 14/25 | Loss: 0.00023895
Iteration 15/25 | Loss: 0.00023895
Iteration 16/25 | Loss: 0.00023895
Iteration 17/25 | Loss: 0.00023895
Iteration 18/25 | Loss: 0.00023895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00023895243066363037, 0.00023895243066363037, 0.00023895243066363037, 0.00023895243066363037, 0.00023895243066363037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023895243066363037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023895
Iteration 2/1000 | Loss: 0.00002292
Iteration 3/1000 | Loss: 0.00001519
Iteration 4/1000 | Loss: 0.00001360
Iteration 5/1000 | Loss: 0.00001285
Iteration 6/1000 | Loss: 0.00001222
Iteration 7/1000 | Loss: 0.00001196
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001163
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001137
Iteration 16/1000 | Loss: 0.00001136
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001130
Iteration 21/1000 | Loss: 0.00001130
Iteration 22/1000 | Loss: 0.00001130
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001126
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001113
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001112
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001111
Iteration 33/1000 | Loss: 0.00001110
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001108
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001100
Iteration 39/1000 | Loss: 0.00001100
Iteration 40/1000 | Loss: 0.00001100
Iteration 41/1000 | Loss: 0.00001099
Iteration 42/1000 | Loss: 0.00001099
Iteration 43/1000 | Loss: 0.00001098
Iteration 44/1000 | Loss: 0.00001098
Iteration 45/1000 | Loss: 0.00001098
Iteration 46/1000 | Loss: 0.00001098
Iteration 47/1000 | Loss: 0.00001098
Iteration 48/1000 | Loss: 0.00001098
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001098
Iteration 52/1000 | Loss: 0.00001097
Iteration 53/1000 | Loss: 0.00001097
Iteration 54/1000 | Loss: 0.00001097
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001096
Iteration 57/1000 | Loss: 0.00001096
Iteration 58/1000 | Loss: 0.00001096
Iteration 59/1000 | Loss: 0.00001096
Iteration 60/1000 | Loss: 0.00001096
Iteration 61/1000 | Loss: 0.00001096
Iteration 62/1000 | Loss: 0.00001096
Iteration 63/1000 | Loss: 0.00001095
Iteration 64/1000 | Loss: 0.00001095
Iteration 65/1000 | Loss: 0.00001095
Iteration 66/1000 | Loss: 0.00001095
Iteration 67/1000 | Loss: 0.00001095
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001095
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001092
Iteration 82/1000 | Loss: 0.00001092
Iteration 83/1000 | Loss: 0.00001092
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001092
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001087
Iteration 111/1000 | Loss: 0.00001087
Iteration 112/1000 | Loss: 0.00001087
Iteration 113/1000 | Loss: 0.00001087
Iteration 114/1000 | Loss: 0.00001087
Iteration 115/1000 | Loss: 0.00001087
Iteration 116/1000 | Loss: 0.00001087
Iteration 117/1000 | Loss: 0.00001087
Iteration 118/1000 | Loss: 0.00001086
Iteration 119/1000 | Loss: 0.00001086
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001086
Iteration 123/1000 | Loss: 0.00001086
Iteration 124/1000 | Loss: 0.00001086
Iteration 125/1000 | Loss: 0.00001086
Iteration 126/1000 | Loss: 0.00001086
Iteration 127/1000 | Loss: 0.00001085
Iteration 128/1000 | Loss: 0.00001085
Iteration 129/1000 | Loss: 0.00001085
Iteration 130/1000 | Loss: 0.00001085
Iteration 131/1000 | Loss: 0.00001085
Iteration 132/1000 | Loss: 0.00001085
Iteration 133/1000 | Loss: 0.00001085
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001085
Iteration 136/1000 | Loss: 0.00001085
Iteration 137/1000 | Loss: 0.00001085
Iteration 138/1000 | Loss: 0.00001085
Iteration 139/1000 | Loss: 0.00001085
Iteration 140/1000 | Loss: 0.00001085
Iteration 141/1000 | Loss: 0.00001085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.0850525541172829e-05, 1.0850525541172829e-05, 1.0850525541172829e-05, 1.0850525541172829e-05, 1.0850525541172829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0850525541172829e-05

Optimization complete. Final v2v error: 2.776221752166748 mm

Highest mean error: 2.9969048500061035 mm for frame 163

Lowest mean error: 2.630436897277832 mm for frame 41

Saving results

Total time: 41.12022852897644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826661
Iteration 2/25 | Loss: 0.00132313
Iteration 3/25 | Loss: 0.00102146
Iteration 4/25 | Loss: 0.00096259
Iteration 5/25 | Loss: 0.00096204
Iteration 6/25 | Loss: 0.00098841
Iteration 7/25 | Loss: 0.00095818
Iteration 8/25 | Loss: 0.00097064
Iteration 9/25 | Loss: 0.00088341
Iteration 10/25 | Loss: 0.00083148
Iteration 11/25 | Loss: 0.00080402
Iteration 12/25 | Loss: 0.00083670
Iteration 13/25 | Loss: 0.00079050
Iteration 14/25 | Loss: 0.00078551
Iteration 15/25 | Loss: 0.00078526
Iteration 16/25 | Loss: 0.00078508
Iteration 17/25 | Loss: 0.00078483
Iteration 18/25 | Loss: 0.00078457
Iteration 19/25 | Loss: 0.00078434
Iteration 20/25 | Loss: 0.00079404
Iteration 21/25 | Loss: 0.00078263
Iteration 22/25 | Loss: 0.00077995
Iteration 23/25 | Loss: 0.00077895
Iteration 24/25 | Loss: 0.00078549
Iteration 25/25 | Loss: 0.00077641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39927399
Iteration 2/25 | Loss: 0.00024906
Iteration 3/25 | Loss: 0.00024904
Iteration 4/25 | Loss: 0.00024904
Iteration 5/25 | Loss: 0.00024904
Iteration 6/25 | Loss: 0.00024904
Iteration 7/25 | Loss: 0.00024904
Iteration 8/25 | Loss: 0.00024904
Iteration 9/25 | Loss: 0.00024904
Iteration 10/25 | Loss: 0.00024904
Iteration 11/25 | Loss: 0.00024904
Iteration 12/25 | Loss: 0.00024904
Iteration 13/25 | Loss: 0.00024904
Iteration 14/25 | Loss: 0.00024904
Iteration 15/25 | Loss: 0.00024904
Iteration 16/25 | Loss: 0.00024904
Iteration 17/25 | Loss: 0.00024904
Iteration 18/25 | Loss: 0.00024904
Iteration 19/25 | Loss: 0.00024904
Iteration 20/25 | Loss: 0.00024904
Iteration 21/25 | Loss: 0.00024904
Iteration 22/25 | Loss: 0.00024904
Iteration 23/25 | Loss: 0.00024904
Iteration 24/25 | Loss: 0.00024904
Iteration 25/25 | Loss: 0.00024904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024904
Iteration 2/1000 | Loss: 0.00004099
Iteration 3/1000 | Loss: 0.00096976
Iteration 4/1000 | Loss: 0.00063297
Iteration 5/1000 | Loss: 0.00075692
Iteration 6/1000 | Loss: 0.00038872
Iteration 7/1000 | Loss: 0.00086607
Iteration 8/1000 | Loss: 0.00120425
Iteration 9/1000 | Loss: 0.00111848
Iteration 10/1000 | Loss: 0.00089404
Iteration 11/1000 | Loss: 0.00059327
Iteration 12/1000 | Loss: 0.00069186
Iteration 13/1000 | Loss: 0.00085176
Iteration 14/1000 | Loss: 0.00072311
Iteration 15/1000 | Loss: 0.00077731
Iteration 16/1000 | Loss: 0.00003891
Iteration 17/1000 | Loss: 0.00095352
Iteration 18/1000 | Loss: 0.00077875
Iteration 19/1000 | Loss: 0.00085164
Iteration 20/1000 | Loss: 0.00091155
Iteration 21/1000 | Loss: 0.00087513
Iteration 22/1000 | Loss: 0.00003452
Iteration 23/1000 | Loss: 0.00002519
Iteration 24/1000 | Loss: 0.00002380
Iteration 25/1000 | Loss: 0.00002303
Iteration 26/1000 | Loss: 0.00002230
Iteration 27/1000 | Loss: 0.00002193
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002137
Iteration 30/1000 | Loss: 0.00002116
Iteration 31/1000 | Loss: 0.00002113
Iteration 32/1000 | Loss: 0.00002105
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002092
Iteration 35/1000 | Loss: 0.00002087
Iteration 36/1000 | Loss: 0.00002086
Iteration 37/1000 | Loss: 0.00002085
Iteration 38/1000 | Loss: 0.00002081
Iteration 39/1000 | Loss: 0.00002078
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002076
Iteration 42/1000 | Loss: 0.00002076
Iteration 43/1000 | Loss: 0.00002076
Iteration 44/1000 | Loss: 0.00002075
Iteration 45/1000 | Loss: 0.00002075
Iteration 46/1000 | Loss: 0.00002075
Iteration 47/1000 | Loss: 0.00002073
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002073
Iteration 50/1000 | Loss: 0.00002072
Iteration 51/1000 | Loss: 0.00002072
Iteration 52/1000 | Loss: 0.00002072
Iteration 53/1000 | Loss: 0.00002071
Iteration 54/1000 | Loss: 0.00002071
Iteration 55/1000 | Loss: 0.00002071
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002070
Iteration 59/1000 | Loss: 0.00002069
Iteration 60/1000 | Loss: 0.00002069
Iteration 61/1000 | Loss: 0.00002069
Iteration 62/1000 | Loss: 0.00002068
Iteration 63/1000 | Loss: 0.00002068
Iteration 64/1000 | Loss: 0.00002067
Iteration 65/1000 | Loss: 0.00002067
Iteration 66/1000 | Loss: 0.00002067
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002066
Iteration 69/1000 | Loss: 0.00002066
Iteration 70/1000 | Loss: 0.00002066
Iteration 71/1000 | Loss: 0.00002065
Iteration 72/1000 | Loss: 0.00002065
Iteration 73/1000 | Loss: 0.00002065
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00002064
Iteration 76/1000 | Loss: 0.00002064
Iteration 77/1000 | Loss: 0.00002064
Iteration 78/1000 | Loss: 0.00002064
Iteration 79/1000 | Loss: 0.00002064
Iteration 80/1000 | Loss: 0.00002063
Iteration 81/1000 | Loss: 0.00002063
Iteration 82/1000 | Loss: 0.00002063
Iteration 83/1000 | Loss: 0.00002063
Iteration 84/1000 | Loss: 0.00002063
Iteration 85/1000 | Loss: 0.00002062
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002062
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002060
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002059
Iteration 99/1000 | Loss: 0.00002059
Iteration 100/1000 | Loss: 0.00002059
Iteration 101/1000 | Loss: 0.00002059
Iteration 102/1000 | Loss: 0.00002059
Iteration 103/1000 | Loss: 0.00002059
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002058
Iteration 110/1000 | Loss: 0.00002058
Iteration 111/1000 | Loss: 0.00002058
Iteration 112/1000 | Loss: 0.00002058
Iteration 113/1000 | Loss: 0.00002058
Iteration 114/1000 | Loss: 0.00002058
Iteration 115/1000 | Loss: 0.00002058
Iteration 116/1000 | Loss: 0.00002058
Iteration 117/1000 | Loss: 0.00002058
Iteration 118/1000 | Loss: 0.00002058
Iteration 119/1000 | Loss: 0.00002058
Iteration 120/1000 | Loss: 0.00002058
Iteration 121/1000 | Loss: 0.00002058
Iteration 122/1000 | Loss: 0.00002058
Iteration 123/1000 | Loss: 0.00002057
Iteration 124/1000 | Loss: 0.00002057
Iteration 125/1000 | Loss: 0.00002057
Iteration 126/1000 | Loss: 0.00002057
Iteration 127/1000 | Loss: 0.00002057
Iteration 128/1000 | Loss: 0.00002057
Iteration 129/1000 | Loss: 0.00002057
Iteration 130/1000 | Loss: 0.00002057
Iteration 131/1000 | Loss: 0.00002057
Iteration 132/1000 | Loss: 0.00002057
Iteration 133/1000 | Loss: 0.00002057
Iteration 134/1000 | Loss: 0.00002057
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002057
Iteration 137/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.0569546904880553e-05, 2.0569546904880553e-05, 2.0569546904880553e-05, 2.0569546904880553e-05, 2.0569546904880553e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0569546904880553e-05

Optimization complete. Final v2v error: 3.8169195652008057 mm

Highest mean error: 4.820460319519043 mm for frame 35

Lowest mean error: 3.3830978870391846 mm for frame 131

Saving results

Total time: 98.75598406791687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838669
Iteration 2/25 | Loss: 0.00146014
Iteration 3/25 | Loss: 0.00089088
Iteration 4/25 | Loss: 0.00080787
Iteration 5/25 | Loss: 0.00080180
Iteration 6/25 | Loss: 0.00080173
Iteration 7/25 | Loss: 0.00080173
Iteration 8/25 | Loss: 0.00080173
Iteration 9/25 | Loss: 0.00080173
Iteration 10/25 | Loss: 0.00080173
Iteration 11/25 | Loss: 0.00080173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008017266518436372, 0.0008017266518436372, 0.0008017266518436372, 0.0008017266518436372, 0.0008017266518436372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008017266518436372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44965494
Iteration 2/25 | Loss: 0.00021636
Iteration 3/25 | Loss: 0.00021636
Iteration 4/25 | Loss: 0.00021636
Iteration 5/25 | Loss: 0.00021636
Iteration 6/25 | Loss: 0.00021636
Iteration 7/25 | Loss: 0.00021636
Iteration 8/25 | Loss: 0.00021636
Iteration 9/25 | Loss: 0.00021636
Iteration 10/25 | Loss: 0.00021636
Iteration 11/25 | Loss: 0.00021636
Iteration 12/25 | Loss: 0.00021636
Iteration 13/25 | Loss: 0.00021636
Iteration 14/25 | Loss: 0.00021636
Iteration 15/25 | Loss: 0.00021636
Iteration 16/25 | Loss: 0.00021636
Iteration 17/25 | Loss: 0.00021636
Iteration 18/25 | Loss: 0.00021636
Iteration 19/25 | Loss: 0.00021636
Iteration 20/25 | Loss: 0.00021636
Iteration 21/25 | Loss: 0.00021636
Iteration 22/25 | Loss: 0.00021636
Iteration 23/25 | Loss: 0.00021636
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00021635602752212435, 0.00021635602752212435, 0.00021635602752212435, 0.00021635602752212435, 0.00021635602752212435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021635602752212435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021636
Iteration 2/1000 | Loss: 0.00003159
Iteration 3/1000 | Loss: 0.00002530
Iteration 4/1000 | Loss: 0.00002329
Iteration 5/1000 | Loss: 0.00002195
Iteration 6/1000 | Loss: 0.00002127
Iteration 7/1000 | Loss: 0.00002051
Iteration 8/1000 | Loss: 0.00002017
Iteration 9/1000 | Loss: 0.00001989
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001949
Iteration 12/1000 | Loss: 0.00001945
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001942
Iteration 15/1000 | Loss: 0.00001935
Iteration 16/1000 | Loss: 0.00001934
Iteration 17/1000 | Loss: 0.00001933
Iteration 18/1000 | Loss: 0.00001933
Iteration 19/1000 | Loss: 0.00001933
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001933
Iteration 22/1000 | Loss: 0.00001933
Iteration 23/1000 | Loss: 0.00001933
Iteration 24/1000 | Loss: 0.00001932
Iteration 25/1000 | Loss: 0.00001932
Iteration 26/1000 | Loss: 0.00001932
Iteration 27/1000 | Loss: 0.00001931
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001922
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001921
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001920
Iteration 38/1000 | Loss: 0.00001920
Iteration 39/1000 | Loss: 0.00001920
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001919
Iteration 43/1000 | Loss: 0.00001919
Iteration 44/1000 | Loss: 0.00001918
Iteration 45/1000 | Loss: 0.00001918
Iteration 46/1000 | Loss: 0.00001918
Iteration 47/1000 | Loss: 0.00001918
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001917
Iteration 51/1000 | Loss: 0.00001917
Iteration 52/1000 | Loss: 0.00001917
Iteration 53/1000 | Loss: 0.00001917
Iteration 54/1000 | Loss: 0.00001917
Iteration 55/1000 | Loss: 0.00001917
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001917
Iteration 58/1000 | Loss: 0.00001917
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001916
Iteration 61/1000 | Loss: 0.00001916
Iteration 62/1000 | Loss: 0.00001916
Iteration 63/1000 | Loss: 0.00001916
Iteration 64/1000 | Loss: 0.00001916
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001913
Iteration 75/1000 | Loss: 0.00001913
Iteration 76/1000 | Loss: 0.00001913
Iteration 77/1000 | Loss: 0.00001913
Iteration 78/1000 | Loss: 0.00001913
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001913
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001912
Iteration 94/1000 | Loss: 0.00001912
Iteration 95/1000 | Loss: 0.00001912
Iteration 96/1000 | Loss: 0.00001912
Iteration 97/1000 | Loss: 0.00001912
Iteration 98/1000 | Loss: 0.00001912
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Iteration 106/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.912261541292537e-05, 1.912261541292537e-05, 1.912261541292537e-05, 1.912261541292537e-05, 1.912261541292537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.912261541292537e-05

Optimization complete. Final v2v error: 3.600597858428955 mm

Highest mean error: 3.665382146835327 mm for frame 151

Lowest mean error: 3.3219988346099854 mm for frame 2

Saving results

Total time: 36.35539531707764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453990
Iteration 2/25 | Loss: 0.00106839
Iteration 3/25 | Loss: 0.00078065
Iteration 4/25 | Loss: 0.00073829
Iteration 5/25 | Loss: 0.00073083
Iteration 6/25 | Loss: 0.00072879
Iteration 7/25 | Loss: 0.00072827
Iteration 8/25 | Loss: 0.00072827
Iteration 9/25 | Loss: 0.00072827
Iteration 10/25 | Loss: 0.00072827
Iteration 11/25 | Loss: 0.00072826
Iteration 12/25 | Loss: 0.00072826
Iteration 13/25 | Loss: 0.00072826
Iteration 14/25 | Loss: 0.00072826
Iteration 15/25 | Loss: 0.00072826
Iteration 16/25 | Loss: 0.00072826
Iteration 17/25 | Loss: 0.00072826
Iteration 18/25 | Loss: 0.00072826
Iteration 19/25 | Loss: 0.00072826
Iteration 20/25 | Loss: 0.00072826
Iteration 21/25 | Loss: 0.00072826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000728260725736618, 0.000728260725736618, 0.000728260725736618, 0.000728260725736618, 0.000728260725736618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000728260725736618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48062336
Iteration 2/25 | Loss: 0.00022957
Iteration 3/25 | Loss: 0.00022957
Iteration 4/25 | Loss: 0.00022957
Iteration 5/25 | Loss: 0.00022957
Iteration 6/25 | Loss: 0.00022957
Iteration 7/25 | Loss: 0.00022957
Iteration 8/25 | Loss: 0.00022957
Iteration 9/25 | Loss: 0.00022957
Iteration 10/25 | Loss: 0.00022957
Iteration 11/25 | Loss: 0.00022957
Iteration 12/25 | Loss: 0.00022957
Iteration 13/25 | Loss: 0.00022957
Iteration 14/25 | Loss: 0.00022957
Iteration 15/25 | Loss: 0.00022957
Iteration 16/25 | Loss: 0.00022957
Iteration 17/25 | Loss: 0.00022957
Iteration 18/25 | Loss: 0.00022957
Iteration 19/25 | Loss: 0.00022957
Iteration 20/25 | Loss: 0.00022957
Iteration 21/25 | Loss: 0.00022957
Iteration 22/25 | Loss: 0.00022957
Iteration 23/25 | Loss: 0.00022957
Iteration 24/25 | Loss: 0.00022957
Iteration 25/25 | Loss: 0.00022957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022957
Iteration 2/1000 | Loss: 0.00003080
Iteration 3/1000 | Loss: 0.00001992
Iteration 4/1000 | Loss: 0.00001757
Iteration 5/1000 | Loss: 0.00001666
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001556
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001502
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001489
Iteration 16/1000 | Loss: 0.00001488
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001476
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001469
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001452
Iteration 31/1000 | Loss: 0.00001450
Iteration 32/1000 | Loss: 0.00001450
Iteration 33/1000 | Loss: 0.00001449
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001447
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001446
Iteration 41/1000 | Loss: 0.00001446
Iteration 42/1000 | Loss: 0.00001446
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001446
Iteration 46/1000 | Loss: 0.00001446
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001442
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001441
Iteration 54/1000 | Loss: 0.00001441
Iteration 55/1000 | Loss: 0.00001441
Iteration 56/1000 | Loss: 0.00001441
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001439
Iteration 60/1000 | Loss: 0.00001439
Iteration 61/1000 | Loss: 0.00001439
Iteration 62/1000 | Loss: 0.00001439
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001438
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001436
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001435
Iteration 81/1000 | Loss: 0.00001435
Iteration 82/1000 | Loss: 0.00001435
Iteration 83/1000 | Loss: 0.00001435
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001434
Iteration 87/1000 | Loss: 0.00001434
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001429
Iteration 108/1000 | Loss: 0.00001429
Iteration 109/1000 | Loss: 0.00001429
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001426
Iteration 117/1000 | Loss: 0.00001426
Iteration 118/1000 | Loss: 0.00001426
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001425
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001423
Iteration 130/1000 | Loss: 0.00001423
Iteration 131/1000 | Loss: 0.00001423
Iteration 132/1000 | Loss: 0.00001423
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001422
Iteration 135/1000 | Loss: 0.00001422
Iteration 136/1000 | Loss: 0.00001422
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001422
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001420
Iteration 146/1000 | Loss: 0.00001420
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001420
Iteration 149/1000 | Loss: 0.00001420
Iteration 150/1000 | Loss: 0.00001420
Iteration 151/1000 | Loss: 0.00001420
Iteration 152/1000 | Loss: 0.00001420
Iteration 153/1000 | Loss: 0.00001419
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001419
Iteration 158/1000 | Loss: 0.00001419
Iteration 159/1000 | Loss: 0.00001419
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001419
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001418
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001417
Iteration 171/1000 | Loss: 0.00001417
Iteration 172/1000 | Loss: 0.00001417
Iteration 173/1000 | Loss: 0.00001417
Iteration 174/1000 | Loss: 0.00001417
Iteration 175/1000 | Loss: 0.00001417
Iteration 176/1000 | Loss: 0.00001417
Iteration 177/1000 | Loss: 0.00001417
Iteration 178/1000 | Loss: 0.00001417
Iteration 179/1000 | Loss: 0.00001417
Iteration 180/1000 | Loss: 0.00001417
Iteration 181/1000 | Loss: 0.00001417
Iteration 182/1000 | Loss: 0.00001416
Iteration 183/1000 | Loss: 0.00001416
Iteration 184/1000 | Loss: 0.00001416
Iteration 185/1000 | Loss: 0.00001416
Iteration 186/1000 | Loss: 0.00001416
Iteration 187/1000 | Loss: 0.00001416
Iteration 188/1000 | Loss: 0.00001416
Iteration 189/1000 | Loss: 0.00001416
Iteration 190/1000 | Loss: 0.00001416
Iteration 191/1000 | Loss: 0.00001416
Iteration 192/1000 | Loss: 0.00001416
Iteration 193/1000 | Loss: 0.00001416
Iteration 194/1000 | Loss: 0.00001416
Iteration 195/1000 | Loss: 0.00001416
Iteration 196/1000 | Loss: 0.00001416
Iteration 197/1000 | Loss: 0.00001416
Iteration 198/1000 | Loss: 0.00001416
Iteration 199/1000 | Loss: 0.00001416
Iteration 200/1000 | Loss: 0.00001415
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001415
Iteration 203/1000 | Loss: 0.00001415
Iteration 204/1000 | Loss: 0.00001415
Iteration 205/1000 | Loss: 0.00001415
Iteration 206/1000 | Loss: 0.00001415
Iteration 207/1000 | Loss: 0.00001415
Iteration 208/1000 | Loss: 0.00001415
Iteration 209/1000 | Loss: 0.00001415
Iteration 210/1000 | Loss: 0.00001415
Iteration 211/1000 | Loss: 0.00001415
Iteration 212/1000 | Loss: 0.00001414
Iteration 213/1000 | Loss: 0.00001414
Iteration 214/1000 | Loss: 0.00001414
Iteration 215/1000 | Loss: 0.00001414
Iteration 216/1000 | Loss: 0.00001414
Iteration 217/1000 | Loss: 0.00001414
Iteration 218/1000 | Loss: 0.00001414
Iteration 219/1000 | Loss: 0.00001414
Iteration 220/1000 | Loss: 0.00001414
Iteration 221/1000 | Loss: 0.00001414
Iteration 222/1000 | Loss: 0.00001414
Iteration 223/1000 | Loss: 0.00001414
Iteration 224/1000 | Loss: 0.00001414
Iteration 225/1000 | Loss: 0.00001414
Iteration 226/1000 | Loss: 0.00001414
Iteration 227/1000 | Loss: 0.00001414
Iteration 228/1000 | Loss: 0.00001414
Iteration 229/1000 | Loss: 0.00001414
Iteration 230/1000 | Loss: 0.00001414
Iteration 231/1000 | Loss: 0.00001414
Iteration 232/1000 | Loss: 0.00001414
Iteration 233/1000 | Loss: 0.00001414
Iteration 234/1000 | Loss: 0.00001414
Iteration 235/1000 | Loss: 0.00001414
Iteration 236/1000 | Loss: 0.00001414
Iteration 237/1000 | Loss: 0.00001414
Iteration 238/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.4142651707516052e-05, 1.4142651707516052e-05, 1.4142651707516052e-05, 1.4142651707516052e-05, 1.4142651707516052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4142651707516052e-05

Optimization complete. Final v2v error: 3.1618404388427734 mm

Highest mean error: 4.519590377807617 mm for frame 57

Lowest mean error: 2.747673988342285 mm for frame 5

Saving results

Total time: 42.9908492565155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494161
Iteration 2/25 | Loss: 0.00097769
Iteration 3/25 | Loss: 0.00079852
Iteration 4/25 | Loss: 0.00077111
Iteration 5/25 | Loss: 0.00076046
Iteration 6/25 | Loss: 0.00075838
Iteration 7/25 | Loss: 0.00075822
Iteration 8/25 | Loss: 0.00075822
Iteration 9/25 | Loss: 0.00075822
Iteration 10/25 | Loss: 0.00075822
Iteration 11/25 | Loss: 0.00075822
Iteration 12/25 | Loss: 0.00075822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000758222711738199, 0.000758222711738199, 0.000758222711738199, 0.000758222711738199, 0.000758222711738199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000758222711738199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47521079
Iteration 2/25 | Loss: 0.00027372
Iteration 3/25 | Loss: 0.00027369
Iteration 4/25 | Loss: 0.00027369
Iteration 5/25 | Loss: 0.00027369
Iteration 6/25 | Loss: 0.00027369
Iteration 7/25 | Loss: 0.00027369
Iteration 8/25 | Loss: 0.00027369
Iteration 9/25 | Loss: 0.00027369
Iteration 10/25 | Loss: 0.00027369
Iteration 11/25 | Loss: 0.00027369
Iteration 12/25 | Loss: 0.00027369
Iteration 13/25 | Loss: 0.00027368
Iteration 14/25 | Loss: 0.00027368
Iteration 15/25 | Loss: 0.00027368
Iteration 16/25 | Loss: 0.00027368
Iteration 17/25 | Loss: 0.00027368
Iteration 18/25 | Loss: 0.00027368
Iteration 19/25 | Loss: 0.00027368
Iteration 20/25 | Loss: 0.00027368
Iteration 21/25 | Loss: 0.00027368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00027368494193069637, 0.00027368494193069637, 0.00027368494193069637, 0.00027368494193069637, 0.00027368494193069637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027368494193069637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027368
Iteration 2/1000 | Loss: 0.00003035
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00001951
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001715
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001690
Iteration 11/1000 | Loss: 0.00001673
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001670
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001663
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001653
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001650
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001645
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001637
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001636
Iteration 42/1000 | Loss: 0.00001636
Iteration 43/1000 | Loss: 0.00001635
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00001634
Iteration 47/1000 | Loss: 0.00001634
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001633
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001632
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001630
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001629
Iteration 64/1000 | Loss: 0.00001629
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001628
Iteration 67/1000 | Loss: 0.00001628
Iteration 68/1000 | Loss: 0.00001628
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001628
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001627
Iteration 73/1000 | Loss: 0.00001627
Iteration 74/1000 | Loss: 0.00001627
Iteration 75/1000 | Loss: 0.00001627
Iteration 76/1000 | Loss: 0.00001627
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001626
Iteration 81/1000 | Loss: 0.00001626
Iteration 82/1000 | Loss: 0.00001626
Iteration 83/1000 | Loss: 0.00001626
Iteration 84/1000 | Loss: 0.00001626
Iteration 85/1000 | Loss: 0.00001625
Iteration 86/1000 | Loss: 0.00001625
Iteration 87/1000 | Loss: 0.00001625
Iteration 88/1000 | Loss: 0.00001625
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001624
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001621
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001618
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001617
Iteration 132/1000 | Loss: 0.00001617
Iteration 133/1000 | Loss: 0.00001617
Iteration 134/1000 | Loss: 0.00001617
Iteration 135/1000 | Loss: 0.00001617
Iteration 136/1000 | Loss: 0.00001617
Iteration 137/1000 | Loss: 0.00001617
Iteration 138/1000 | Loss: 0.00001617
Iteration 139/1000 | Loss: 0.00001617
Iteration 140/1000 | Loss: 0.00001616
Iteration 141/1000 | Loss: 0.00001616
Iteration 142/1000 | Loss: 0.00001616
Iteration 143/1000 | Loss: 0.00001616
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Iteration 147/1000 | Loss: 0.00001615
Iteration 148/1000 | Loss: 0.00001615
Iteration 149/1000 | Loss: 0.00001615
Iteration 150/1000 | Loss: 0.00001615
Iteration 151/1000 | Loss: 0.00001615
Iteration 152/1000 | Loss: 0.00001615
Iteration 153/1000 | Loss: 0.00001615
Iteration 154/1000 | Loss: 0.00001615
Iteration 155/1000 | Loss: 0.00001615
Iteration 156/1000 | Loss: 0.00001614
Iteration 157/1000 | Loss: 0.00001614
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Iteration 162/1000 | Loss: 0.00001614
Iteration 163/1000 | Loss: 0.00001614
Iteration 164/1000 | Loss: 0.00001614
Iteration 165/1000 | Loss: 0.00001614
Iteration 166/1000 | Loss: 0.00001614
Iteration 167/1000 | Loss: 0.00001614
Iteration 168/1000 | Loss: 0.00001614
Iteration 169/1000 | Loss: 0.00001614
Iteration 170/1000 | Loss: 0.00001614
Iteration 171/1000 | Loss: 0.00001614
Iteration 172/1000 | Loss: 0.00001614
Iteration 173/1000 | Loss: 0.00001614
Iteration 174/1000 | Loss: 0.00001614
Iteration 175/1000 | Loss: 0.00001614
Iteration 176/1000 | Loss: 0.00001614
Iteration 177/1000 | Loss: 0.00001614
Iteration 178/1000 | Loss: 0.00001614
Iteration 179/1000 | Loss: 0.00001614
Iteration 180/1000 | Loss: 0.00001614
Iteration 181/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.6136886188178323e-05, 1.6136886188178323e-05, 1.6136886188178323e-05, 1.6136886188178323e-05, 1.6136886188178323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6136886188178323e-05

Optimization complete. Final v2v error: 3.361199378967285 mm

Highest mean error: 4.099637031555176 mm for frame 30

Lowest mean error: 2.867338180541992 mm for frame 146

Saving results

Total time: 43.5733597278595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00197383
Iteration 2/25 | Loss: 0.00086178
Iteration 3/25 | Loss: 0.00076623
Iteration 4/25 | Loss: 0.00072471
Iteration 5/25 | Loss: 0.00070874
Iteration 6/25 | Loss: 0.00070579
Iteration 7/25 | Loss: 0.00070413
Iteration 8/25 | Loss: 0.00070392
Iteration 9/25 | Loss: 0.00070392
Iteration 10/25 | Loss: 0.00070392
Iteration 11/25 | Loss: 0.00070392
Iteration 12/25 | Loss: 0.00070392
Iteration 13/25 | Loss: 0.00070392
Iteration 14/25 | Loss: 0.00070392
Iteration 15/25 | Loss: 0.00070392
Iteration 16/25 | Loss: 0.00070392
Iteration 17/25 | Loss: 0.00070392
Iteration 18/25 | Loss: 0.00070392
Iteration 19/25 | Loss: 0.00070392
Iteration 20/25 | Loss: 0.00070392
Iteration 21/25 | Loss: 0.00070392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007039214251562953, 0.0007039214251562953, 0.0007039214251562953, 0.0007039214251562953, 0.0007039214251562953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007039214251562953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47058809
Iteration 2/25 | Loss: 0.00024242
Iteration 3/25 | Loss: 0.00024242
Iteration 4/25 | Loss: 0.00024242
Iteration 5/25 | Loss: 0.00024242
Iteration 6/25 | Loss: 0.00024242
Iteration 7/25 | Loss: 0.00024242
Iteration 8/25 | Loss: 0.00024242
Iteration 9/25 | Loss: 0.00024242
Iteration 10/25 | Loss: 0.00024242
Iteration 11/25 | Loss: 0.00024242
Iteration 12/25 | Loss: 0.00024242
Iteration 13/25 | Loss: 0.00024242
Iteration 14/25 | Loss: 0.00024242
Iteration 15/25 | Loss: 0.00024242
Iteration 16/25 | Loss: 0.00024242
Iteration 17/25 | Loss: 0.00024242
Iteration 18/25 | Loss: 0.00024242
Iteration 19/25 | Loss: 0.00024242
Iteration 20/25 | Loss: 0.00024242
Iteration 21/25 | Loss: 0.00024242
Iteration 22/25 | Loss: 0.00024242
Iteration 23/25 | Loss: 0.00024242
Iteration 24/25 | Loss: 0.00024242
Iteration 25/25 | Loss: 0.00024242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024242
Iteration 2/1000 | Loss: 0.00003774
Iteration 3/1000 | Loss: 0.00002363
Iteration 4/1000 | Loss: 0.00001945
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00001637
Iteration 8/1000 | Loss: 0.00001595
Iteration 9/1000 | Loss: 0.00001586
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001548
Iteration 13/1000 | Loss: 0.00001546
Iteration 14/1000 | Loss: 0.00001544
Iteration 15/1000 | Loss: 0.00001537
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001534
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001534
Iteration 20/1000 | Loss: 0.00001534
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001525
Iteration 25/1000 | Loss: 0.00001525
Iteration 26/1000 | Loss: 0.00001524
Iteration 27/1000 | Loss: 0.00001514
Iteration 28/1000 | Loss: 0.00001514
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001512
Iteration 31/1000 | Loss: 0.00001511
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001510
Iteration 35/1000 | Loss: 0.00001510
Iteration 36/1000 | Loss: 0.00001509
Iteration 37/1000 | Loss: 0.00001509
Iteration 38/1000 | Loss: 0.00001509
Iteration 39/1000 | Loss: 0.00001509
Iteration 40/1000 | Loss: 0.00001509
Iteration 41/1000 | Loss: 0.00001509
Iteration 42/1000 | Loss: 0.00001508
Iteration 43/1000 | Loss: 0.00001508
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001507
Iteration 46/1000 | Loss: 0.00001507
Iteration 47/1000 | Loss: 0.00001507
Iteration 48/1000 | Loss: 0.00001507
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00001506
Iteration 51/1000 | Loss: 0.00001506
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001506
Iteration 58/1000 | Loss: 0.00001506
Iteration 59/1000 | Loss: 0.00001505
Iteration 60/1000 | Loss: 0.00001505
Iteration 61/1000 | Loss: 0.00001505
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001504
Iteration 64/1000 | Loss: 0.00001503
Iteration 65/1000 | Loss: 0.00001503
Iteration 66/1000 | Loss: 0.00001503
Iteration 67/1000 | Loss: 0.00001503
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001502
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001502
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001501
Iteration 82/1000 | Loss: 0.00001501
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001500
Iteration 89/1000 | Loss: 0.00001500
Iteration 90/1000 | Loss: 0.00001500
Iteration 91/1000 | Loss: 0.00001500
Iteration 92/1000 | Loss: 0.00001500
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001500
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001500
Iteration 97/1000 | Loss: 0.00001500
Iteration 98/1000 | Loss: 0.00001500
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001499
Iteration 101/1000 | Loss: 0.00001499
Iteration 102/1000 | Loss: 0.00001499
Iteration 103/1000 | Loss: 0.00001499
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001498
Iteration 108/1000 | Loss: 0.00001498
Iteration 109/1000 | Loss: 0.00001498
Iteration 110/1000 | Loss: 0.00001498
Iteration 111/1000 | Loss: 0.00001498
Iteration 112/1000 | Loss: 0.00001498
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001498
Iteration 121/1000 | Loss: 0.00001498
Iteration 122/1000 | Loss: 0.00001498
Iteration 123/1000 | Loss: 0.00001498
Iteration 124/1000 | Loss: 0.00001498
Iteration 125/1000 | Loss: 0.00001498
Iteration 126/1000 | Loss: 0.00001498
Iteration 127/1000 | Loss: 0.00001498
Iteration 128/1000 | Loss: 0.00001498
Iteration 129/1000 | Loss: 0.00001498
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001498
Iteration 136/1000 | Loss: 0.00001498
Iteration 137/1000 | Loss: 0.00001498
Iteration 138/1000 | Loss: 0.00001498
Iteration 139/1000 | Loss: 0.00001498
Iteration 140/1000 | Loss: 0.00001498
Iteration 141/1000 | Loss: 0.00001498
Iteration 142/1000 | Loss: 0.00001498
Iteration 143/1000 | Loss: 0.00001498
Iteration 144/1000 | Loss: 0.00001498
Iteration 145/1000 | Loss: 0.00001498
Iteration 146/1000 | Loss: 0.00001498
Iteration 147/1000 | Loss: 0.00001498
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001498
Iteration 157/1000 | Loss: 0.00001498
Iteration 158/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4977623322920408e-05, 1.4977623322920408e-05, 1.4977623322920408e-05, 1.4977623322920408e-05, 1.4977623322920408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4977623322920408e-05

Optimization complete. Final v2v error: 3.287982940673828 mm

Highest mean error: 3.534125566482544 mm for frame 22

Lowest mean error: 3.0813324451446533 mm for frame 85

Saving results

Total time: 39.57330846786499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806664
Iteration 2/25 | Loss: 0.00091211
Iteration 3/25 | Loss: 0.00073920
Iteration 4/25 | Loss: 0.00070569
Iteration 5/25 | Loss: 0.00069605
Iteration 6/25 | Loss: 0.00069380
Iteration 7/25 | Loss: 0.00069359
Iteration 8/25 | Loss: 0.00069359
Iteration 9/25 | Loss: 0.00069359
Iteration 10/25 | Loss: 0.00069359
Iteration 11/25 | Loss: 0.00069359
Iteration 12/25 | Loss: 0.00069359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006935906712897122, 0.0006935906712897122, 0.0006935906712897122, 0.0006935906712897122, 0.0006935906712897122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006935906712897122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45620477
Iteration 2/25 | Loss: 0.00021798
Iteration 3/25 | Loss: 0.00021798
Iteration 4/25 | Loss: 0.00021798
Iteration 5/25 | Loss: 0.00021798
Iteration 6/25 | Loss: 0.00021798
Iteration 7/25 | Loss: 0.00021798
Iteration 8/25 | Loss: 0.00021798
Iteration 9/25 | Loss: 0.00021798
Iteration 10/25 | Loss: 0.00021797
Iteration 11/25 | Loss: 0.00021797
Iteration 12/25 | Loss: 0.00021797
Iteration 13/25 | Loss: 0.00021797
Iteration 14/25 | Loss: 0.00021797
Iteration 15/25 | Loss: 0.00021797
Iteration 16/25 | Loss: 0.00021797
Iteration 17/25 | Loss: 0.00021797
Iteration 18/25 | Loss: 0.00021797
Iteration 19/25 | Loss: 0.00021797
Iteration 20/25 | Loss: 0.00021797
Iteration 21/25 | Loss: 0.00021797
Iteration 22/25 | Loss: 0.00021797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00021797466615680605, 0.00021797466615680605, 0.00021797466615680605, 0.00021797466615680605, 0.00021797466615680605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021797466615680605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021797
Iteration 2/1000 | Loss: 0.00001749
Iteration 3/1000 | Loss: 0.00001339
Iteration 4/1000 | Loss: 0.00001252
Iteration 5/1000 | Loss: 0.00001195
Iteration 6/1000 | Loss: 0.00001143
Iteration 7/1000 | Loss: 0.00001120
Iteration 8/1000 | Loss: 0.00001106
Iteration 9/1000 | Loss: 0.00001092
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001076
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001065
Iteration 15/1000 | Loss: 0.00001065
Iteration 16/1000 | Loss: 0.00001063
Iteration 17/1000 | Loss: 0.00001059
Iteration 18/1000 | Loss: 0.00001056
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001053
Iteration 23/1000 | Loss: 0.00001052
Iteration 24/1000 | Loss: 0.00001052
Iteration 25/1000 | Loss: 0.00001052
Iteration 26/1000 | Loss: 0.00001051
Iteration 27/1000 | Loss: 0.00001051
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001050
Iteration 30/1000 | Loss: 0.00001050
Iteration 31/1000 | Loss: 0.00001049
Iteration 32/1000 | Loss: 0.00001048
Iteration 33/1000 | Loss: 0.00001047
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001041
Iteration 45/1000 | Loss: 0.00001041
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001040
Iteration 49/1000 | Loss: 0.00001040
Iteration 50/1000 | Loss: 0.00001039
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001039
Iteration 56/1000 | Loss: 0.00001039
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001038
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001037
Iteration 65/1000 | Loss: 0.00001037
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001037
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001036
Iteration 74/1000 | Loss: 0.00001036
Iteration 75/1000 | Loss: 0.00001036
Iteration 76/1000 | Loss: 0.00001036
Iteration 77/1000 | Loss: 0.00001036
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001035
Iteration 81/1000 | Loss: 0.00001035
Iteration 82/1000 | Loss: 0.00001035
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001034
Iteration 89/1000 | Loss: 0.00001034
Iteration 90/1000 | Loss: 0.00001034
Iteration 91/1000 | Loss: 0.00001034
Iteration 92/1000 | Loss: 0.00001034
Iteration 93/1000 | Loss: 0.00001034
Iteration 94/1000 | Loss: 0.00001033
Iteration 95/1000 | Loss: 0.00001033
Iteration 96/1000 | Loss: 0.00001033
Iteration 97/1000 | Loss: 0.00001033
Iteration 98/1000 | Loss: 0.00001033
Iteration 99/1000 | Loss: 0.00001033
Iteration 100/1000 | Loss: 0.00001033
Iteration 101/1000 | Loss: 0.00001033
Iteration 102/1000 | Loss: 0.00001032
Iteration 103/1000 | Loss: 0.00001032
Iteration 104/1000 | Loss: 0.00001032
Iteration 105/1000 | Loss: 0.00001032
Iteration 106/1000 | Loss: 0.00001031
Iteration 107/1000 | Loss: 0.00001031
Iteration 108/1000 | Loss: 0.00001031
Iteration 109/1000 | Loss: 0.00001031
Iteration 110/1000 | Loss: 0.00001031
Iteration 111/1000 | Loss: 0.00001031
Iteration 112/1000 | Loss: 0.00001030
Iteration 113/1000 | Loss: 0.00001030
Iteration 114/1000 | Loss: 0.00001030
Iteration 115/1000 | Loss: 0.00001029
Iteration 116/1000 | Loss: 0.00001029
Iteration 117/1000 | Loss: 0.00001028
Iteration 118/1000 | Loss: 0.00001028
Iteration 119/1000 | Loss: 0.00001028
Iteration 120/1000 | Loss: 0.00001027
Iteration 121/1000 | Loss: 0.00001027
Iteration 122/1000 | Loss: 0.00001027
Iteration 123/1000 | Loss: 0.00001027
Iteration 124/1000 | Loss: 0.00001026
Iteration 125/1000 | Loss: 0.00001026
Iteration 126/1000 | Loss: 0.00001026
Iteration 127/1000 | Loss: 0.00001026
Iteration 128/1000 | Loss: 0.00001025
Iteration 129/1000 | Loss: 0.00001025
Iteration 130/1000 | Loss: 0.00001025
Iteration 131/1000 | Loss: 0.00001025
Iteration 132/1000 | Loss: 0.00001025
Iteration 133/1000 | Loss: 0.00001025
Iteration 134/1000 | Loss: 0.00001025
Iteration 135/1000 | Loss: 0.00001025
Iteration 136/1000 | Loss: 0.00001025
Iteration 137/1000 | Loss: 0.00001024
Iteration 138/1000 | Loss: 0.00001024
Iteration 139/1000 | Loss: 0.00001024
Iteration 140/1000 | Loss: 0.00001024
Iteration 141/1000 | Loss: 0.00001024
Iteration 142/1000 | Loss: 0.00001024
Iteration 143/1000 | Loss: 0.00001024
Iteration 144/1000 | Loss: 0.00001023
Iteration 145/1000 | Loss: 0.00001023
Iteration 146/1000 | Loss: 0.00001023
Iteration 147/1000 | Loss: 0.00001023
Iteration 148/1000 | Loss: 0.00001023
Iteration 149/1000 | Loss: 0.00001023
Iteration 150/1000 | Loss: 0.00001023
Iteration 151/1000 | Loss: 0.00001023
Iteration 152/1000 | Loss: 0.00001023
Iteration 153/1000 | Loss: 0.00001023
Iteration 154/1000 | Loss: 0.00001023
Iteration 155/1000 | Loss: 0.00001023
Iteration 156/1000 | Loss: 0.00001023
Iteration 157/1000 | Loss: 0.00001023
Iteration 158/1000 | Loss: 0.00001023
Iteration 159/1000 | Loss: 0.00001023
Iteration 160/1000 | Loss: 0.00001023
Iteration 161/1000 | Loss: 0.00001023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.0227518941974267e-05, 1.0227518941974267e-05, 1.0227518941974267e-05, 1.0227518941974267e-05, 1.0227518941974267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0227518941974267e-05

Optimization complete. Final v2v error: 2.684300422668457 mm

Highest mean error: 2.9077987670898438 mm for frame 115

Lowest mean error: 2.534423351287842 mm for frame 191

Saving results

Total time: 38.38606095314026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047979
Iteration 2/25 | Loss: 0.00181172
Iteration 3/25 | Loss: 0.00115765
Iteration 4/25 | Loss: 0.00097056
Iteration 5/25 | Loss: 0.00089400
Iteration 6/25 | Loss: 0.00084752
Iteration 7/25 | Loss: 0.00082320
Iteration 8/25 | Loss: 0.00081528
Iteration 9/25 | Loss: 0.00081307
Iteration 10/25 | Loss: 0.00081268
Iteration 11/25 | Loss: 0.00081265
Iteration 12/25 | Loss: 0.00081265
Iteration 13/25 | Loss: 0.00081265
Iteration 14/25 | Loss: 0.00081265
Iteration 15/25 | Loss: 0.00081265
Iteration 16/25 | Loss: 0.00081265
Iteration 17/25 | Loss: 0.00081265
Iteration 18/25 | Loss: 0.00081265
Iteration 19/25 | Loss: 0.00081265
Iteration 20/25 | Loss: 0.00081265
Iteration 21/25 | Loss: 0.00081265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008126526372507215, 0.0008126526372507215, 0.0008126526372507215, 0.0008126526372507215, 0.0008126526372507215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008126526372507215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42756283
Iteration 2/25 | Loss: 0.00030409
Iteration 3/25 | Loss: 0.00030408
Iteration 4/25 | Loss: 0.00030408
Iteration 5/25 | Loss: 0.00030408
Iteration 6/25 | Loss: 0.00030408
Iteration 7/25 | Loss: 0.00030408
Iteration 8/25 | Loss: 0.00030408
Iteration 9/25 | Loss: 0.00030408
Iteration 10/25 | Loss: 0.00030408
Iteration 11/25 | Loss: 0.00030408
Iteration 12/25 | Loss: 0.00030408
Iteration 13/25 | Loss: 0.00030408
Iteration 14/25 | Loss: 0.00030408
Iteration 15/25 | Loss: 0.00030408
Iteration 16/25 | Loss: 0.00030408
Iteration 17/25 | Loss: 0.00030408
Iteration 18/25 | Loss: 0.00030408
Iteration 19/25 | Loss: 0.00030408
Iteration 20/25 | Loss: 0.00030408
Iteration 21/25 | Loss: 0.00030408
Iteration 22/25 | Loss: 0.00030408
Iteration 23/25 | Loss: 0.00030408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00030408278689719737, 0.00030408278689719737, 0.00030408278689719737, 0.00030408278689719737, 0.00030408278689719737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030408278689719737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030408
Iteration 2/1000 | Loss: 0.00003133
Iteration 3/1000 | Loss: 0.00002302
Iteration 4/1000 | Loss: 0.00002140
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001927
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001834
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001811
Iteration 20/1000 | Loss: 0.00001811
Iteration 21/1000 | Loss: 0.00001811
Iteration 22/1000 | Loss: 0.00001811
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001811
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001811
Iteration 29/1000 | Loss: 0.00001811
Iteration 30/1000 | Loss: 0.00001810
Iteration 31/1000 | Loss: 0.00001810
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001808
Iteration 41/1000 | Loss: 0.00001808
Iteration 42/1000 | Loss: 0.00001808
Iteration 43/1000 | Loss: 0.00001808
Iteration 44/1000 | Loss: 0.00001807
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001807
Iteration 48/1000 | Loss: 0.00001806
Iteration 49/1000 | Loss: 0.00001806
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001805
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001805
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001805
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001803
Iteration 70/1000 | Loss: 0.00001803
Iteration 71/1000 | Loss: 0.00001803
Iteration 72/1000 | Loss: 0.00001803
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001802
Iteration 75/1000 | Loss: 0.00001802
Iteration 76/1000 | Loss: 0.00001802
Iteration 77/1000 | Loss: 0.00001802
Iteration 78/1000 | Loss: 0.00001802
Iteration 79/1000 | Loss: 0.00001801
Iteration 80/1000 | Loss: 0.00001801
Iteration 81/1000 | Loss: 0.00001801
Iteration 82/1000 | Loss: 0.00001801
Iteration 83/1000 | Loss: 0.00001801
Iteration 84/1000 | Loss: 0.00001800
Iteration 85/1000 | Loss: 0.00001800
Iteration 86/1000 | Loss: 0.00001800
Iteration 87/1000 | Loss: 0.00001800
Iteration 88/1000 | Loss: 0.00001800
Iteration 89/1000 | Loss: 0.00001800
Iteration 90/1000 | Loss: 0.00001800
Iteration 91/1000 | Loss: 0.00001800
Iteration 92/1000 | Loss: 0.00001800
Iteration 93/1000 | Loss: 0.00001800
Iteration 94/1000 | Loss: 0.00001800
Iteration 95/1000 | Loss: 0.00001800
Iteration 96/1000 | Loss: 0.00001800
Iteration 97/1000 | Loss: 0.00001800
Iteration 98/1000 | Loss: 0.00001799
Iteration 99/1000 | Loss: 0.00001799
Iteration 100/1000 | Loss: 0.00001799
Iteration 101/1000 | Loss: 0.00001799
Iteration 102/1000 | Loss: 0.00001799
Iteration 103/1000 | Loss: 0.00001799
Iteration 104/1000 | Loss: 0.00001799
Iteration 105/1000 | Loss: 0.00001799
Iteration 106/1000 | Loss: 0.00001799
Iteration 107/1000 | Loss: 0.00001798
Iteration 108/1000 | Loss: 0.00001798
Iteration 109/1000 | Loss: 0.00001798
Iteration 110/1000 | Loss: 0.00001798
Iteration 111/1000 | Loss: 0.00001798
Iteration 112/1000 | Loss: 0.00001798
Iteration 113/1000 | Loss: 0.00001798
Iteration 114/1000 | Loss: 0.00001798
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001798
Iteration 117/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.7976648450712673e-05, 1.7976648450712673e-05, 1.7976648450712673e-05, 1.7976648450712673e-05, 1.7976648450712673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7976648450712673e-05

Optimization complete. Final v2v error: 3.487787961959839 mm

Highest mean error: 3.72015118598938 mm for frame 1

Lowest mean error: 3.3522040843963623 mm for frame 131

Saving results

Total time: 40.414057970047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802583
Iteration 2/25 | Loss: 0.00108407
Iteration 3/25 | Loss: 0.00083400
Iteration 4/25 | Loss: 0.00078175
Iteration 5/25 | Loss: 0.00076415
Iteration 6/25 | Loss: 0.00075800
Iteration 7/25 | Loss: 0.00076644
Iteration 8/25 | Loss: 0.00076436
Iteration 9/25 | Loss: 0.00075096
Iteration 10/25 | Loss: 0.00074294
Iteration 11/25 | Loss: 0.00073787
Iteration 12/25 | Loss: 0.00073508
Iteration 13/25 | Loss: 0.00073405
Iteration 14/25 | Loss: 0.00073349
Iteration 15/25 | Loss: 0.00073315
Iteration 16/25 | Loss: 0.00073306
Iteration 17/25 | Loss: 0.00073306
Iteration 18/25 | Loss: 0.00073306
Iteration 19/25 | Loss: 0.00073306
Iteration 20/25 | Loss: 0.00073306
Iteration 21/25 | Loss: 0.00073306
Iteration 22/25 | Loss: 0.00073306
Iteration 23/25 | Loss: 0.00073305
Iteration 24/25 | Loss: 0.00073305
Iteration 25/25 | Loss: 0.00073305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72911799
Iteration 2/25 | Loss: 0.00024009
Iteration 3/25 | Loss: 0.00024009
Iteration 4/25 | Loss: 0.00024009
Iteration 5/25 | Loss: 0.00024008
Iteration 6/25 | Loss: 0.00024008
Iteration 7/25 | Loss: 0.00024008
Iteration 8/25 | Loss: 0.00024008
Iteration 9/25 | Loss: 0.00024008
Iteration 10/25 | Loss: 0.00024008
Iteration 11/25 | Loss: 0.00024008
Iteration 12/25 | Loss: 0.00024008
Iteration 13/25 | Loss: 0.00024008
Iteration 14/25 | Loss: 0.00024008
Iteration 15/25 | Loss: 0.00024008
Iteration 16/25 | Loss: 0.00024008
Iteration 17/25 | Loss: 0.00024008
Iteration 18/25 | Loss: 0.00024008
Iteration 19/25 | Loss: 0.00024008
Iteration 20/25 | Loss: 0.00024008
Iteration 21/25 | Loss: 0.00024008
Iteration 22/25 | Loss: 0.00024008
Iteration 23/25 | Loss: 0.00024008
Iteration 24/25 | Loss: 0.00024008
Iteration 25/25 | Loss: 0.00024008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024008
Iteration 2/1000 | Loss: 0.00002946
Iteration 3/1000 | Loss: 0.00004506
Iteration 4/1000 | Loss: 0.00002001
Iteration 5/1000 | Loss: 0.00001854
Iteration 6/1000 | Loss: 0.00001795
Iteration 7/1000 | Loss: 0.00001767
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001707
Iteration 12/1000 | Loss: 0.00001693
Iteration 13/1000 | Loss: 0.00001691
Iteration 14/1000 | Loss: 0.00001691
Iteration 15/1000 | Loss: 0.00001689
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00004275
Iteration 19/1000 | Loss: 0.00010645
Iteration 20/1000 | Loss: 0.00006803
Iteration 21/1000 | Loss: 0.00006132
Iteration 22/1000 | Loss: 0.00001964
Iteration 23/1000 | Loss: 0.00006555
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001663
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001662
Iteration 35/1000 | Loss: 0.00001662
Iteration 36/1000 | Loss: 0.00001662
Iteration 37/1000 | Loss: 0.00001662
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001660
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001660
Iteration 47/1000 | Loss: 0.00001660
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00002091
Iteration 53/1000 | Loss: 0.00001657
Iteration 54/1000 | Loss: 0.00001652
Iteration 55/1000 | Loss: 0.00001651
Iteration 56/1000 | Loss: 0.00001651
Iteration 57/1000 | Loss: 0.00001650
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001647
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001644
Iteration 72/1000 | Loss: 0.00001643
Iteration 73/1000 | Loss: 0.00001642
Iteration 74/1000 | Loss: 0.00001640
Iteration 75/1000 | Loss: 0.00001640
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001640
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00005337
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001982
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00002529
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001641
Iteration 90/1000 | Loss: 0.00001641
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001640
Iteration 93/1000 | Loss: 0.00001821
Iteration 94/1000 | Loss: 0.00001821
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00006055
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001652
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001635
Iteration 105/1000 | Loss: 0.00001635
Iteration 106/1000 | Loss: 0.00001635
Iteration 107/1000 | Loss: 0.00001635
Iteration 108/1000 | Loss: 0.00001635
Iteration 109/1000 | Loss: 0.00001635
Iteration 110/1000 | Loss: 0.00001635
Iteration 111/1000 | Loss: 0.00001635
Iteration 112/1000 | Loss: 0.00001635
Iteration 113/1000 | Loss: 0.00001635
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001635
Iteration 116/1000 | Loss: 0.00001635
Iteration 117/1000 | Loss: 0.00001635
Iteration 118/1000 | Loss: 0.00001635
Iteration 119/1000 | Loss: 0.00001635
Iteration 120/1000 | Loss: 0.00001635
Iteration 121/1000 | Loss: 0.00001635
Iteration 122/1000 | Loss: 0.00001635
Iteration 123/1000 | Loss: 0.00001635
Iteration 124/1000 | Loss: 0.00001635
Iteration 125/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.6346833945135586e-05, 1.6346833945135586e-05, 1.6346833945135586e-05, 1.6346833945135586e-05, 1.6346833945135586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6346833945135586e-05

Optimization complete. Final v2v error: 3.3721234798431396 mm

Highest mean error: 4.182486057281494 mm for frame 45

Lowest mean error: 2.9039905071258545 mm for frame 77

Saving results

Total time: 74.090749502182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048515
Iteration 2/25 | Loss: 0.00224373
Iteration 3/25 | Loss: 0.00158766
Iteration 4/25 | Loss: 0.00131275
Iteration 5/25 | Loss: 0.00120924
Iteration 6/25 | Loss: 0.00103388
Iteration 7/25 | Loss: 0.00102488
Iteration 8/25 | Loss: 0.00100273
Iteration 9/25 | Loss: 0.00101269
Iteration 10/25 | Loss: 0.00102904
Iteration 11/25 | Loss: 0.00095525
Iteration 12/25 | Loss: 0.00095995
Iteration 13/25 | Loss: 0.00094212
Iteration 14/25 | Loss: 0.00095699
Iteration 15/25 | Loss: 0.00094331
Iteration 16/25 | Loss: 0.00095016
Iteration 17/25 | Loss: 0.00094781
Iteration 18/25 | Loss: 0.00091418
Iteration 19/25 | Loss: 0.00093407
Iteration 20/25 | Loss: 0.00092285
Iteration 21/25 | Loss: 0.00088797
Iteration 22/25 | Loss: 0.00088776
Iteration 23/25 | Loss: 0.00088438
Iteration 24/25 | Loss: 0.00087298
Iteration 25/25 | Loss: 0.00086573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51413131
Iteration 2/25 | Loss: 0.00073475
Iteration 3/25 | Loss: 0.00073474
Iteration 4/25 | Loss: 0.00073474
Iteration 5/25 | Loss: 0.00073474
Iteration 6/25 | Loss: 0.00073474
Iteration 7/25 | Loss: 0.00073474
Iteration 8/25 | Loss: 0.00073474
Iteration 9/25 | Loss: 0.00073474
Iteration 10/25 | Loss: 0.00073474
Iteration 11/25 | Loss: 0.00073474
Iteration 12/25 | Loss: 0.00073474
Iteration 13/25 | Loss: 0.00073474
Iteration 14/25 | Loss: 0.00073474
Iteration 15/25 | Loss: 0.00073474
Iteration 16/25 | Loss: 0.00073474
Iteration 17/25 | Loss: 0.00073474
Iteration 18/25 | Loss: 0.00073474
Iteration 19/25 | Loss: 0.00073474
Iteration 20/25 | Loss: 0.00073474
Iteration 21/25 | Loss: 0.00073474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007347392966039479, 0.0007347392966039479, 0.0007347392966039479, 0.0007347392966039479, 0.0007347392966039479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007347392966039479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073474
Iteration 2/1000 | Loss: 0.00018574
Iteration 3/1000 | Loss: 0.00040437
Iteration 4/1000 | Loss: 0.00011293
Iteration 5/1000 | Loss: 0.00008025
Iteration 6/1000 | Loss: 0.00006629
Iteration 7/1000 | Loss: 0.00017787
Iteration 8/1000 | Loss: 0.00006143
Iteration 9/1000 | Loss: 0.00011964
Iteration 10/1000 | Loss: 0.00041353
Iteration 11/1000 | Loss: 0.00007039
Iteration 12/1000 | Loss: 0.00008055
Iteration 13/1000 | Loss: 0.00006955
Iteration 14/1000 | Loss: 0.00006247
Iteration 15/1000 | Loss: 0.00006146
Iteration 16/1000 | Loss: 0.00005742
Iteration 17/1000 | Loss: 0.00007339
Iteration 18/1000 | Loss: 0.00207632
Iteration 19/1000 | Loss: 0.00059534
Iteration 20/1000 | Loss: 0.00012488
Iteration 21/1000 | Loss: 0.00017733
Iteration 22/1000 | Loss: 0.00007496
Iteration 23/1000 | Loss: 0.00010392
Iteration 24/1000 | Loss: 0.00004986
Iteration 25/1000 | Loss: 0.00009478
Iteration 26/1000 | Loss: 0.00022426
Iteration 27/1000 | Loss: 0.00051282
Iteration 28/1000 | Loss: 0.00049697
Iteration 29/1000 | Loss: 0.00008394
Iteration 30/1000 | Loss: 0.00004741
Iteration 31/1000 | Loss: 0.00004876
Iteration 32/1000 | Loss: 0.00004218
Iteration 33/1000 | Loss: 0.00006549
Iteration 34/1000 | Loss: 0.00022168
Iteration 35/1000 | Loss: 0.00002943
Iteration 36/1000 | Loss: 0.00007372
Iteration 37/1000 | Loss: 0.00007998
Iteration 38/1000 | Loss: 0.00006908
Iteration 39/1000 | Loss: 0.00002279
Iteration 40/1000 | Loss: 0.00002182
Iteration 41/1000 | Loss: 0.00020015
Iteration 42/1000 | Loss: 0.00004781
Iteration 43/1000 | Loss: 0.00002125
Iteration 44/1000 | Loss: 0.00002085
Iteration 45/1000 | Loss: 0.00018438
Iteration 46/1000 | Loss: 0.00002407
Iteration 47/1000 | Loss: 0.00002035
Iteration 48/1000 | Loss: 0.00002017
Iteration 49/1000 | Loss: 0.00010860
Iteration 50/1000 | Loss: 0.00011617
Iteration 51/1000 | Loss: 0.00008562
Iteration 52/1000 | Loss: 0.00002316
Iteration 53/1000 | Loss: 0.00001977
Iteration 54/1000 | Loss: 0.00001970
Iteration 55/1000 | Loss: 0.00001970
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001969
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001963
Iteration 71/1000 | Loss: 0.00001962
Iteration 72/1000 | Loss: 0.00001962
Iteration 73/1000 | Loss: 0.00001961
Iteration 74/1000 | Loss: 0.00001961
Iteration 75/1000 | Loss: 0.00001961
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001960
Iteration 78/1000 | Loss: 0.00001960
Iteration 79/1000 | Loss: 0.00001960
Iteration 80/1000 | Loss: 0.00005387
Iteration 81/1000 | Loss: 0.00001977
Iteration 82/1000 | Loss: 0.00001955
Iteration 83/1000 | Loss: 0.00001955
Iteration 84/1000 | Loss: 0.00001954
Iteration 85/1000 | Loss: 0.00001954
Iteration 86/1000 | Loss: 0.00001954
Iteration 87/1000 | Loss: 0.00001954
Iteration 88/1000 | Loss: 0.00001954
Iteration 89/1000 | Loss: 0.00001954
Iteration 90/1000 | Loss: 0.00001954
Iteration 91/1000 | Loss: 0.00001954
Iteration 92/1000 | Loss: 0.00001954
Iteration 93/1000 | Loss: 0.00001953
Iteration 94/1000 | Loss: 0.00001953
Iteration 95/1000 | Loss: 0.00001953
Iteration 96/1000 | Loss: 0.00001953
Iteration 97/1000 | Loss: 0.00001952
Iteration 98/1000 | Loss: 0.00001952
Iteration 99/1000 | Loss: 0.00001952
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001951
Iteration 102/1000 | Loss: 0.00001951
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00001951
Iteration 105/1000 | Loss: 0.00004012
Iteration 106/1000 | Loss: 0.00002512
Iteration 107/1000 | Loss: 0.00002925
Iteration 108/1000 | Loss: 0.00001951
Iteration 109/1000 | Loss: 0.00001950
Iteration 110/1000 | Loss: 0.00001950
Iteration 111/1000 | Loss: 0.00001950
Iteration 112/1000 | Loss: 0.00001950
Iteration 113/1000 | Loss: 0.00001950
Iteration 114/1000 | Loss: 0.00001950
Iteration 115/1000 | Loss: 0.00001949
Iteration 116/1000 | Loss: 0.00001949
Iteration 117/1000 | Loss: 0.00001949
Iteration 118/1000 | Loss: 0.00001949
Iteration 119/1000 | Loss: 0.00001949
Iteration 120/1000 | Loss: 0.00001949
Iteration 121/1000 | Loss: 0.00001949
Iteration 122/1000 | Loss: 0.00001949
Iteration 123/1000 | Loss: 0.00001948
Iteration 124/1000 | Loss: 0.00001948
Iteration 125/1000 | Loss: 0.00001948
Iteration 126/1000 | Loss: 0.00001948
Iteration 127/1000 | Loss: 0.00001948
Iteration 128/1000 | Loss: 0.00001948
Iteration 129/1000 | Loss: 0.00001948
Iteration 130/1000 | Loss: 0.00001948
Iteration 131/1000 | Loss: 0.00001948
Iteration 132/1000 | Loss: 0.00001948
Iteration 133/1000 | Loss: 0.00001948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.948317549249623e-05, 1.948317549249623e-05, 1.948317549249623e-05, 1.948317549249623e-05, 1.948317549249623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.948317549249623e-05

Optimization complete. Final v2v error: 3.7111618518829346 mm

Highest mean error: 4.197455883026123 mm for frame 86

Lowest mean error: 3.3464195728302 mm for frame 57

Saving results

Total time: 134.00197315216064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917528
Iteration 2/25 | Loss: 0.00327123
Iteration 3/25 | Loss: 0.00214477
Iteration 4/25 | Loss: 0.00153098
Iteration 5/25 | Loss: 0.00138231
Iteration 6/25 | Loss: 0.00137135
Iteration 7/25 | Loss: 0.00134513
Iteration 8/25 | Loss: 0.00129449
Iteration 9/25 | Loss: 0.00130270
Iteration 10/25 | Loss: 0.00126911
Iteration 11/25 | Loss: 0.00123136
Iteration 12/25 | Loss: 0.00121643
Iteration 13/25 | Loss: 0.00120787
Iteration 14/25 | Loss: 0.00119752
Iteration 15/25 | Loss: 0.00119932
Iteration 16/25 | Loss: 0.00119741
Iteration 17/25 | Loss: 0.00119152
Iteration 18/25 | Loss: 0.00119296
Iteration 19/25 | Loss: 0.00119001
Iteration 20/25 | Loss: 0.00119667
Iteration 21/25 | Loss: 0.00119281
Iteration 22/25 | Loss: 0.00121634
Iteration 23/25 | Loss: 0.00118551
Iteration 24/25 | Loss: 0.00118228
Iteration 25/25 | Loss: 0.00118110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40275574
Iteration 2/25 | Loss: 0.00391590
Iteration 3/25 | Loss: 0.00339300
Iteration 4/25 | Loss: 0.00339235
Iteration 5/25 | Loss: 0.00339234
Iteration 6/25 | Loss: 0.00339234
Iteration 7/25 | Loss: 0.00339234
Iteration 8/25 | Loss: 0.00339234
Iteration 9/25 | Loss: 0.00339234
Iteration 10/25 | Loss: 0.00339234
Iteration 11/25 | Loss: 0.00339234
Iteration 12/25 | Loss: 0.00339234
Iteration 13/25 | Loss: 0.00339234
Iteration 14/25 | Loss: 0.00339234
Iteration 15/25 | Loss: 0.00339234
Iteration 16/25 | Loss: 0.00339234
Iteration 17/25 | Loss: 0.00339234
Iteration 18/25 | Loss: 0.00339234
Iteration 19/25 | Loss: 0.00339234
Iteration 20/25 | Loss: 0.00339234
Iteration 21/25 | Loss: 0.00339234
Iteration 22/25 | Loss: 0.00339234
Iteration 23/25 | Loss: 0.00339234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0033923378214240074, 0.0033923378214240074, 0.0033923378214240074, 0.0033923378214240074, 0.0033923378214240074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033923378214240074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00339234
Iteration 2/1000 | Loss: 0.00177791
Iteration 3/1000 | Loss: 0.00221459
Iteration 4/1000 | Loss: 0.00890883
Iteration 5/1000 | Loss: 0.00366047
Iteration 6/1000 | Loss: 0.00479233
Iteration 7/1000 | Loss: 0.00166156
Iteration 8/1000 | Loss: 0.00078091
Iteration 9/1000 | Loss: 0.00325438
Iteration 10/1000 | Loss: 0.00184176
Iteration 11/1000 | Loss: 0.00408427
Iteration 12/1000 | Loss: 0.00215650
Iteration 13/1000 | Loss: 0.00174845
Iteration 14/1000 | Loss: 0.00192164
Iteration 15/1000 | Loss: 0.00260170
Iteration 16/1000 | Loss: 0.00097374
Iteration 17/1000 | Loss: 0.00155801
Iteration 18/1000 | Loss: 0.00223036
Iteration 19/1000 | Loss: 0.00044666
Iteration 20/1000 | Loss: 0.00355840
Iteration 21/1000 | Loss: 0.00209961
Iteration 22/1000 | Loss: 0.00159548
Iteration 23/1000 | Loss: 0.00354293
Iteration 24/1000 | Loss: 0.00169656
Iteration 25/1000 | Loss: 0.00139612
Iteration 26/1000 | Loss: 0.00215122
Iteration 27/1000 | Loss: 0.00219503
Iteration 28/1000 | Loss: 0.00224175
Iteration 29/1000 | Loss: 0.00242588
Iteration 30/1000 | Loss: 0.00191801
Iteration 31/1000 | Loss: 0.00226541
Iteration 32/1000 | Loss: 0.00204999
Iteration 33/1000 | Loss: 0.00173348
Iteration 34/1000 | Loss: 0.00343098
Iteration 35/1000 | Loss: 0.00205891
Iteration 36/1000 | Loss: 0.00221464
Iteration 37/1000 | Loss: 0.00456421
Iteration 38/1000 | Loss: 0.00277861
Iteration 39/1000 | Loss: 0.00101264
Iteration 40/1000 | Loss: 0.00184784
Iteration 41/1000 | Loss: 0.00155084
Iteration 42/1000 | Loss: 0.00178486
Iteration 43/1000 | Loss: 0.00148491
Iteration 44/1000 | Loss: 0.00401333
Iteration 45/1000 | Loss: 0.00304986
Iteration 46/1000 | Loss: 0.00342144
Iteration 47/1000 | Loss: 0.00495814
Iteration 48/1000 | Loss: 0.00596287
Iteration 49/1000 | Loss: 0.00552460
Iteration 50/1000 | Loss: 0.00217461
Iteration 51/1000 | Loss: 0.00157444
Iteration 52/1000 | Loss: 0.00080671
Iteration 53/1000 | Loss: 0.00091147
Iteration 54/1000 | Loss: 0.00177105
Iteration 55/1000 | Loss: 0.00159265
Iteration 56/1000 | Loss: 0.00170520
Iteration 57/1000 | Loss: 0.00136043
Iteration 58/1000 | Loss: 0.00176479
Iteration 59/1000 | Loss: 0.00195269
Iteration 60/1000 | Loss: 0.00022290
Iteration 61/1000 | Loss: 0.00021706
Iteration 62/1000 | Loss: 0.00019924
Iteration 63/1000 | Loss: 0.00061422
Iteration 64/1000 | Loss: 0.00365568
Iteration 65/1000 | Loss: 0.00276171
Iteration 66/1000 | Loss: 0.00293431
Iteration 67/1000 | Loss: 0.00209659
Iteration 68/1000 | Loss: 0.00018454
Iteration 69/1000 | Loss: 0.00012138
Iteration 70/1000 | Loss: 0.00024961
Iteration 71/1000 | Loss: 0.00027747
Iteration 72/1000 | Loss: 0.00011065
Iteration 73/1000 | Loss: 0.00332712
Iteration 74/1000 | Loss: 0.00341663
Iteration 75/1000 | Loss: 0.00156120
Iteration 76/1000 | Loss: 0.00042270
Iteration 77/1000 | Loss: 0.00322975
Iteration 78/1000 | Loss: 0.00103089
Iteration 79/1000 | Loss: 0.00115220
Iteration 80/1000 | Loss: 0.00123885
Iteration 81/1000 | Loss: 0.00038737
Iteration 82/1000 | Loss: 0.00073491
Iteration 83/1000 | Loss: 0.00020759
Iteration 84/1000 | Loss: 0.00010753
Iteration 85/1000 | Loss: 0.00017131
Iteration 86/1000 | Loss: 0.00020195
Iteration 87/1000 | Loss: 0.00121133
Iteration 88/1000 | Loss: 0.00052073
Iteration 89/1000 | Loss: 0.00120681
Iteration 90/1000 | Loss: 0.00257698
Iteration 91/1000 | Loss: 0.00009235
Iteration 92/1000 | Loss: 0.00063389
Iteration 93/1000 | Loss: 0.00099208
Iteration 94/1000 | Loss: 0.00054373
Iteration 95/1000 | Loss: 0.00175538
Iteration 96/1000 | Loss: 0.00047600
Iteration 97/1000 | Loss: 0.00016687
Iteration 98/1000 | Loss: 0.00046047
Iteration 99/1000 | Loss: 0.00039322
Iteration 100/1000 | Loss: 0.00045126
Iteration 101/1000 | Loss: 0.00086766
Iteration 102/1000 | Loss: 0.00013279
Iteration 103/1000 | Loss: 0.00008010
Iteration 104/1000 | Loss: 0.00014645
Iteration 105/1000 | Loss: 0.00176142
Iteration 106/1000 | Loss: 0.00349391
Iteration 107/1000 | Loss: 0.00119796
Iteration 108/1000 | Loss: 0.00083946
Iteration 109/1000 | Loss: 0.00037026
Iteration 110/1000 | Loss: 0.00028470
Iteration 111/1000 | Loss: 0.00010328
Iteration 112/1000 | Loss: 0.00115459
Iteration 113/1000 | Loss: 0.00017650
Iteration 114/1000 | Loss: 0.00014791
Iteration 115/1000 | Loss: 0.00012106
Iteration 116/1000 | Loss: 0.00005406
Iteration 117/1000 | Loss: 0.00005048
Iteration 118/1000 | Loss: 0.00004794
Iteration 119/1000 | Loss: 0.00022870
Iteration 120/1000 | Loss: 0.00004594
Iteration 121/1000 | Loss: 0.00016631
Iteration 122/1000 | Loss: 0.00015697
Iteration 123/1000 | Loss: 0.00004371
Iteration 124/1000 | Loss: 0.00004268
Iteration 125/1000 | Loss: 0.00004204
Iteration 126/1000 | Loss: 0.00127390
Iteration 127/1000 | Loss: 0.00097871
Iteration 128/1000 | Loss: 0.00005379
Iteration 129/1000 | Loss: 0.00004215
Iteration 130/1000 | Loss: 0.00003581
Iteration 131/1000 | Loss: 0.00003446
Iteration 132/1000 | Loss: 0.00003329
Iteration 133/1000 | Loss: 0.00003265
Iteration 134/1000 | Loss: 0.00003239
Iteration 135/1000 | Loss: 0.00003212
Iteration 136/1000 | Loss: 0.00003184
Iteration 137/1000 | Loss: 0.00003166
Iteration 138/1000 | Loss: 0.00003159
Iteration 139/1000 | Loss: 0.00006899
Iteration 140/1000 | Loss: 0.00004009
Iteration 141/1000 | Loss: 0.00004643
Iteration 142/1000 | Loss: 0.00003137
Iteration 143/1000 | Loss: 0.00003136
Iteration 144/1000 | Loss: 0.00003136
Iteration 145/1000 | Loss: 0.00003135
Iteration 146/1000 | Loss: 0.00003132
Iteration 147/1000 | Loss: 0.00003122
Iteration 148/1000 | Loss: 0.00003119
Iteration 149/1000 | Loss: 0.00003118
Iteration 150/1000 | Loss: 0.00003118
Iteration 151/1000 | Loss: 0.00003117
Iteration 152/1000 | Loss: 0.00003117
Iteration 153/1000 | Loss: 0.00003116
Iteration 154/1000 | Loss: 0.00003116
Iteration 155/1000 | Loss: 0.00003116
Iteration 156/1000 | Loss: 0.00007682
Iteration 157/1000 | Loss: 0.00003487
Iteration 158/1000 | Loss: 0.00005064
Iteration 159/1000 | Loss: 0.00003113
Iteration 160/1000 | Loss: 0.00003104
Iteration 161/1000 | Loss: 0.00003103
Iteration 162/1000 | Loss: 0.00003103
Iteration 163/1000 | Loss: 0.00003103
Iteration 164/1000 | Loss: 0.00003099
Iteration 165/1000 | Loss: 0.00003099
Iteration 166/1000 | Loss: 0.00003098
Iteration 167/1000 | Loss: 0.00003098
Iteration 168/1000 | Loss: 0.00003097
Iteration 169/1000 | Loss: 0.00003097
Iteration 170/1000 | Loss: 0.00003096
Iteration 171/1000 | Loss: 0.00003096
Iteration 172/1000 | Loss: 0.00003096
Iteration 173/1000 | Loss: 0.00003095
Iteration 174/1000 | Loss: 0.00003095
Iteration 175/1000 | Loss: 0.00003095
Iteration 176/1000 | Loss: 0.00003094
Iteration 177/1000 | Loss: 0.00003094
Iteration 178/1000 | Loss: 0.00003093
Iteration 179/1000 | Loss: 0.00003093
Iteration 180/1000 | Loss: 0.00003092
Iteration 181/1000 | Loss: 0.00003092
Iteration 182/1000 | Loss: 0.00003091
Iteration 183/1000 | Loss: 0.00003091
Iteration 184/1000 | Loss: 0.00003091
Iteration 185/1000 | Loss: 0.00003090
Iteration 186/1000 | Loss: 0.00003090
Iteration 187/1000 | Loss: 0.00003089
Iteration 188/1000 | Loss: 0.00003089
Iteration 189/1000 | Loss: 0.00003088
Iteration 190/1000 | Loss: 0.00003088
Iteration 191/1000 | Loss: 0.00003087
Iteration 192/1000 | Loss: 0.00003087
Iteration 193/1000 | Loss: 0.00003087
Iteration 194/1000 | Loss: 0.00003087
Iteration 195/1000 | Loss: 0.00003086
Iteration 196/1000 | Loss: 0.00003086
Iteration 197/1000 | Loss: 0.00003085
Iteration 198/1000 | Loss: 0.00003083
Iteration 199/1000 | Loss: 0.00003083
Iteration 200/1000 | Loss: 0.00003083
Iteration 201/1000 | Loss: 0.00003083
Iteration 202/1000 | Loss: 0.00003082
Iteration 203/1000 | Loss: 0.00003082
Iteration 204/1000 | Loss: 0.00003082
Iteration 205/1000 | Loss: 0.00003081
Iteration 206/1000 | Loss: 0.00003081
Iteration 207/1000 | Loss: 0.00003081
Iteration 208/1000 | Loss: 0.00003081
Iteration 209/1000 | Loss: 0.00003080
Iteration 210/1000 | Loss: 0.00003080
Iteration 211/1000 | Loss: 0.00003080
Iteration 212/1000 | Loss: 0.00003080
Iteration 213/1000 | Loss: 0.00003080
Iteration 214/1000 | Loss: 0.00003079
Iteration 215/1000 | Loss: 0.00003079
Iteration 216/1000 | Loss: 0.00003079
Iteration 217/1000 | Loss: 0.00003078
Iteration 218/1000 | Loss: 0.00003078
Iteration 219/1000 | Loss: 0.00003078
Iteration 220/1000 | Loss: 0.00003077
Iteration 221/1000 | Loss: 0.00003077
Iteration 222/1000 | Loss: 0.00003077
Iteration 223/1000 | Loss: 0.00003076
Iteration 224/1000 | Loss: 0.00003075
Iteration 225/1000 | Loss: 0.00003075
Iteration 226/1000 | Loss: 0.00003074
Iteration 227/1000 | Loss: 0.00003074
Iteration 228/1000 | Loss: 0.00003073
Iteration 229/1000 | Loss: 0.00003073
Iteration 230/1000 | Loss: 0.00003073
Iteration 231/1000 | Loss: 0.00003072
Iteration 232/1000 | Loss: 0.00003072
Iteration 233/1000 | Loss: 0.00003071
Iteration 234/1000 | Loss: 0.00003071
Iteration 235/1000 | Loss: 0.00003071
Iteration 236/1000 | Loss: 0.00003071
Iteration 237/1000 | Loss: 0.00003070
Iteration 238/1000 | Loss: 0.00003070
Iteration 239/1000 | Loss: 0.00003070
Iteration 240/1000 | Loss: 0.00003070
Iteration 241/1000 | Loss: 0.00003070
Iteration 242/1000 | Loss: 0.00003070
Iteration 243/1000 | Loss: 0.00003069
Iteration 244/1000 | Loss: 0.00003069
Iteration 245/1000 | Loss: 0.00003068
Iteration 246/1000 | Loss: 0.00003068
Iteration 247/1000 | Loss: 0.00003067
Iteration 248/1000 | Loss: 0.00003066
Iteration 249/1000 | Loss: 0.00003065
Iteration 250/1000 | Loss: 0.00003065
Iteration 251/1000 | Loss: 0.00003065
Iteration 252/1000 | Loss: 0.00003065
Iteration 253/1000 | Loss: 0.00003064
Iteration 254/1000 | Loss: 0.00003064
Iteration 255/1000 | Loss: 0.00003064
Iteration 256/1000 | Loss: 0.00003064
Iteration 257/1000 | Loss: 0.00003064
Iteration 258/1000 | Loss: 0.00003064
Iteration 259/1000 | Loss: 0.00003063
Iteration 260/1000 | Loss: 0.00003063
Iteration 261/1000 | Loss: 0.00003063
Iteration 262/1000 | Loss: 0.00003063
Iteration 263/1000 | Loss: 0.00003063
Iteration 264/1000 | Loss: 0.00003063
Iteration 265/1000 | Loss: 0.00003063
Iteration 266/1000 | Loss: 0.00003063
Iteration 267/1000 | Loss: 0.00003063
Iteration 268/1000 | Loss: 0.00003063
Iteration 269/1000 | Loss: 0.00003063
Iteration 270/1000 | Loss: 0.00003063
Iteration 271/1000 | Loss: 0.00003063
Iteration 272/1000 | Loss: 0.00003062
Iteration 273/1000 | Loss: 0.00003062
Iteration 274/1000 | Loss: 0.00003062
Iteration 275/1000 | Loss: 0.00003062
Iteration 276/1000 | Loss: 0.00003062
Iteration 277/1000 | Loss: 0.00003061
Iteration 278/1000 | Loss: 0.00003061
Iteration 279/1000 | Loss: 0.00003061
Iteration 280/1000 | Loss: 0.00003061
Iteration 281/1000 | Loss: 0.00003060
Iteration 282/1000 | Loss: 0.00003060
Iteration 283/1000 | Loss: 0.00003059
Iteration 284/1000 | Loss: 0.00003059
Iteration 285/1000 | Loss: 0.00003059
Iteration 286/1000 | Loss: 0.00003059
Iteration 287/1000 | Loss: 0.00003059
Iteration 288/1000 | Loss: 0.00003058
Iteration 289/1000 | Loss: 0.00003058
Iteration 290/1000 | Loss: 0.00003058
Iteration 291/1000 | Loss: 0.00003058
Iteration 292/1000 | Loss: 0.00003058
Iteration 293/1000 | Loss: 0.00003058
Iteration 294/1000 | Loss: 0.00003058
Iteration 295/1000 | Loss: 0.00003058
Iteration 296/1000 | Loss: 0.00003057
Iteration 297/1000 | Loss: 0.00003057
Iteration 298/1000 | Loss: 0.00003057
Iteration 299/1000 | Loss: 0.00003057
Iteration 300/1000 | Loss: 0.00003056
Iteration 301/1000 | Loss: 0.00003056
Iteration 302/1000 | Loss: 0.00003056
Iteration 303/1000 | Loss: 0.00003056
Iteration 304/1000 | Loss: 0.00003055
Iteration 305/1000 | Loss: 0.00003055
Iteration 306/1000 | Loss: 0.00003054
Iteration 307/1000 | Loss: 0.00003054
Iteration 308/1000 | Loss: 0.00003053
Iteration 309/1000 | Loss: 0.00003053
Iteration 310/1000 | Loss: 0.00003053
Iteration 311/1000 | Loss: 0.00003053
Iteration 312/1000 | Loss: 0.00003053
Iteration 313/1000 | Loss: 0.00003053
Iteration 314/1000 | Loss: 0.00003052
Iteration 315/1000 | Loss: 0.00003052
Iteration 316/1000 | Loss: 0.00003052
Iteration 317/1000 | Loss: 0.00003052
Iteration 318/1000 | Loss: 0.00003052
Iteration 319/1000 | Loss: 0.00003052
Iteration 320/1000 | Loss: 0.00003051
Iteration 321/1000 | Loss: 0.00003051
Iteration 322/1000 | Loss: 0.00003051
Iteration 323/1000 | Loss: 0.00003051
Iteration 324/1000 | Loss: 0.00003051
Iteration 325/1000 | Loss: 0.00003051
Iteration 326/1000 | Loss: 0.00003051
Iteration 327/1000 | Loss: 0.00003051
Iteration 328/1000 | Loss: 0.00003051
Iteration 329/1000 | Loss: 0.00003051
Iteration 330/1000 | Loss: 0.00003051
Iteration 331/1000 | Loss: 0.00003051
Iteration 332/1000 | Loss: 0.00003050
Iteration 333/1000 | Loss: 0.00003050
Iteration 334/1000 | Loss: 0.00003049
Iteration 335/1000 | Loss: 0.00003049
Iteration 336/1000 | Loss: 0.00003049
Iteration 337/1000 | Loss: 0.00003048
Iteration 338/1000 | Loss: 0.00003048
Iteration 339/1000 | Loss: 0.00003048
Iteration 340/1000 | Loss: 0.00003048
Iteration 341/1000 | Loss: 0.00003048
Iteration 342/1000 | Loss: 0.00003048
Iteration 343/1000 | Loss: 0.00003048
Iteration 344/1000 | Loss: 0.00003048
Iteration 345/1000 | Loss: 0.00003048
Iteration 346/1000 | Loss: 0.00003048
Iteration 347/1000 | Loss: 0.00003047
Iteration 348/1000 | Loss: 0.00003047
Iteration 349/1000 | Loss: 0.00003047
Iteration 350/1000 | Loss: 0.00003046
Iteration 351/1000 | Loss: 0.00003046
Iteration 352/1000 | Loss: 0.00003045
Iteration 353/1000 | Loss: 0.00003045
Iteration 354/1000 | Loss: 0.00003045
Iteration 355/1000 | Loss: 0.00003044
Iteration 356/1000 | Loss: 0.00003044
Iteration 357/1000 | Loss: 0.00003043
Iteration 358/1000 | Loss: 0.00003043
Iteration 359/1000 | Loss: 0.00003042
Iteration 360/1000 | Loss: 0.00003042
Iteration 361/1000 | Loss: 0.00003041
Iteration 362/1000 | Loss: 0.00003041
Iteration 363/1000 | Loss: 0.00003040
Iteration 364/1000 | Loss: 0.00003040
Iteration 365/1000 | Loss: 0.00003040
Iteration 366/1000 | Loss: 0.00003039
Iteration 367/1000 | Loss: 0.00003039
Iteration 368/1000 | Loss: 0.00003039
Iteration 369/1000 | Loss: 0.00003039
Iteration 370/1000 | Loss: 0.00003038
Iteration 371/1000 | Loss: 0.00003038
Iteration 372/1000 | Loss: 0.00003038
Iteration 373/1000 | Loss: 0.00003038
Iteration 374/1000 | Loss: 0.00003038
Iteration 375/1000 | Loss: 0.00003037
Iteration 376/1000 | Loss: 0.00003037
Iteration 377/1000 | Loss: 0.00003037
Iteration 378/1000 | Loss: 0.00003037
Iteration 379/1000 | Loss: 0.00003037
Iteration 380/1000 | Loss: 0.00003036
Iteration 381/1000 | Loss: 0.00003036
Iteration 382/1000 | Loss: 0.00003036
Iteration 383/1000 | Loss: 0.00003036
Iteration 384/1000 | Loss: 0.00003035
Iteration 385/1000 | Loss: 0.00003035
Iteration 386/1000 | Loss: 0.00003035
Iteration 387/1000 | Loss: 0.00003034
Iteration 388/1000 | Loss: 0.00003034
Iteration 389/1000 | Loss: 0.00003034
Iteration 390/1000 | Loss: 0.00003033
Iteration 391/1000 | Loss: 0.00003033
Iteration 392/1000 | Loss: 0.00003033
Iteration 393/1000 | Loss: 0.00003033
Iteration 394/1000 | Loss: 0.00003032
Iteration 395/1000 | Loss: 0.00003032
Iteration 396/1000 | Loss: 0.00003032
Iteration 397/1000 | Loss: 0.00003031
Iteration 398/1000 | Loss: 0.00003031
Iteration 399/1000 | Loss: 0.00003031
Iteration 400/1000 | Loss: 0.00003031
Iteration 401/1000 | Loss: 0.00003031
Iteration 402/1000 | Loss: 0.00003030
Iteration 403/1000 | Loss: 0.00003030
Iteration 404/1000 | Loss: 0.00003030
Iteration 405/1000 | Loss: 0.00003030
Iteration 406/1000 | Loss: 0.00051131
Iteration 407/1000 | Loss: 0.00002979
Iteration 408/1000 | Loss: 0.00002895
Iteration 409/1000 | Loss: 0.00002828
Iteration 410/1000 | Loss: 0.00002773
Iteration 411/1000 | Loss: 0.00002733
Iteration 412/1000 | Loss: 0.00002714
Iteration 413/1000 | Loss: 0.00002700
Iteration 414/1000 | Loss: 0.00002699
Iteration 415/1000 | Loss: 0.00002699
Iteration 416/1000 | Loss: 0.00002699
Iteration 417/1000 | Loss: 0.00002699
Iteration 418/1000 | Loss: 0.00002692
Iteration 419/1000 | Loss: 0.00002692
Iteration 420/1000 | Loss: 0.00002690
Iteration 421/1000 | Loss: 0.00002690
Iteration 422/1000 | Loss: 0.00002689
Iteration 423/1000 | Loss: 0.00002689
Iteration 424/1000 | Loss: 0.00002689
Iteration 425/1000 | Loss: 0.00002689
Iteration 426/1000 | Loss: 0.00002689
Iteration 427/1000 | Loss: 0.00002689
Iteration 428/1000 | Loss: 0.00002689
Iteration 429/1000 | Loss: 0.00002688
Iteration 430/1000 | Loss: 0.00002688
Iteration 431/1000 | Loss: 0.00002688
Iteration 432/1000 | Loss: 0.00002687
Iteration 433/1000 | Loss: 0.00002687
Iteration 434/1000 | Loss: 0.00002687
Iteration 435/1000 | Loss: 0.00002686
Iteration 436/1000 | Loss: 0.00002686
Iteration 437/1000 | Loss: 0.00002686
Iteration 438/1000 | Loss: 0.00002686
Iteration 439/1000 | Loss: 0.00002686
Iteration 440/1000 | Loss: 0.00002686
Iteration 441/1000 | Loss: 0.00002686
Iteration 442/1000 | Loss: 0.00002686
Iteration 443/1000 | Loss: 0.00002686
Iteration 444/1000 | Loss: 0.00002685
Iteration 445/1000 | Loss: 0.00002685
Iteration 446/1000 | Loss: 0.00002685
Iteration 447/1000 | Loss: 0.00002683
Iteration 448/1000 | Loss: 0.00002683
Iteration 449/1000 | Loss: 0.00002683
Iteration 450/1000 | Loss: 0.00002683
Iteration 451/1000 | Loss: 0.00002683
Iteration 452/1000 | Loss: 0.00002682
Iteration 453/1000 | Loss: 0.00002682
Iteration 454/1000 | Loss: 0.00002682
Iteration 455/1000 | Loss: 0.00002682
Iteration 456/1000 | Loss: 0.00002681
Iteration 457/1000 | Loss: 0.00002681
Iteration 458/1000 | Loss: 0.00002681
Iteration 459/1000 | Loss: 0.00002681
Iteration 460/1000 | Loss: 0.00002681
Iteration 461/1000 | Loss: 0.00002681
Iteration 462/1000 | Loss: 0.00002681
Iteration 463/1000 | Loss: 0.00002681
Iteration 464/1000 | Loss: 0.00002680
Iteration 465/1000 | Loss: 0.00002680
Iteration 466/1000 | Loss: 0.00002680
Iteration 467/1000 | Loss: 0.00002680
Iteration 468/1000 | Loss: 0.00002680
Iteration 469/1000 | Loss: 0.00002680
Iteration 470/1000 | Loss: 0.00002680
Iteration 471/1000 | Loss: 0.00002680
Iteration 472/1000 | Loss: 0.00002679
Iteration 473/1000 | Loss: 0.00002679
Iteration 474/1000 | Loss: 0.00002679
Iteration 475/1000 | Loss: 0.00002679
Iteration 476/1000 | Loss: 0.00002678
Iteration 477/1000 | Loss: 0.00002678
Iteration 478/1000 | Loss: 0.00002678
Iteration 479/1000 | Loss: 0.00002678
Iteration 480/1000 | Loss: 0.00002678
Iteration 481/1000 | Loss: 0.00002678
Iteration 482/1000 | Loss: 0.00002678
Iteration 483/1000 | Loss: 0.00002677
Iteration 484/1000 | Loss: 0.00002677
Iteration 485/1000 | Loss: 0.00002677
Iteration 486/1000 | Loss: 0.00002677
Iteration 487/1000 | Loss: 0.00002677
Iteration 488/1000 | Loss: 0.00002677
Iteration 489/1000 | Loss: 0.00002677
Iteration 490/1000 | Loss: 0.00002677
Iteration 491/1000 | Loss: 0.00002677
Iteration 492/1000 | Loss: 0.00002677
Iteration 493/1000 | Loss: 0.00002677
Iteration 494/1000 | Loss: 0.00002677
Iteration 495/1000 | Loss: 0.00002676
Iteration 496/1000 | Loss: 0.00002676
Iteration 497/1000 | Loss: 0.00002676
Iteration 498/1000 | Loss: 0.00002675
Iteration 499/1000 | Loss: 0.00002675
Iteration 500/1000 | Loss: 0.00002675
Iteration 501/1000 | Loss: 0.00002675
Iteration 502/1000 | Loss: 0.00002674
Iteration 503/1000 | Loss: 0.00002674
Iteration 504/1000 | Loss: 0.00002674
Iteration 505/1000 | Loss: 0.00002674
Iteration 506/1000 | Loss: 0.00002674
Iteration 507/1000 | Loss: 0.00002674
Iteration 508/1000 | Loss: 0.00002674
Iteration 509/1000 | Loss: 0.00002674
Iteration 510/1000 | Loss: 0.00002674
Iteration 511/1000 | Loss: 0.00002673
Iteration 512/1000 | Loss: 0.00002673
Iteration 513/1000 | Loss: 0.00002673
Iteration 514/1000 | Loss: 0.00002673
Iteration 515/1000 | Loss: 0.00002673
Iteration 516/1000 | Loss: 0.00002673
Iteration 517/1000 | Loss: 0.00002673
Iteration 518/1000 | Loss: 0.00002673
Iteration 519/1000 | Loss: 0.00002673
Iteration 520/1000 | Loss: 0.00002673
Iteration 521/1000 | Loss: 0.00002672
Iteration 522/1000 | Loss: 0.00002672
Iteration 523/1000 | Loss: 0.00002672
Iteration 524/1000 | Loss: 0.00002672
Iteration 525/1000 | Loss: 0.00002672
Iteration 526/1000 | Loss: 0.00002672
Iteration 527/1000 | Loss: 0.00002672
Iteration 528/1000 | Loss: 0.00002672
Iteration 529/1000 | Loss: 0.00002672
Iteration 530/1000 | Loss: 0.00002672
Iteration 531/1000 | Loss: 0.00002672
Iteration 532/1000 | Loss: 0.00002672
Iteration 533/1000 | Loss: 0.00002672
Iteration 534/1000 | Loss: 0.00002671
Iteration 535/1000 | Loss: 0.00002671
Iteration 536/1000 | Loss: 0.00002671
Iteration 537/1000 | Loss: 0.00002671
Iteration 538/1000 | Loss: 0.00002671
Iteration 539/1000 | Loss: 0.00002671
Iteration 540/1000 | Loss: 0.00002671
Iteration 541/1000 | Loss: 0.00002671
Iteration 542/1000 | Loss: 0.00002671
Iteration 543/1000 | Loss: 0.00002671
Iteration 544/1000 | Loss: 0.00002671
Iteration 545/1000 | Loss: 0.00002671
Iteration 546/1000 | Loss: 0.00002671
Iteration 547/1000 | Loss: 0.00002671
Iteration 548/1000 | Loss: 0.00002671
Iteration 549/1000 | Loss: 0.00002671
Iteration 550/1000 | Loss: 0.00002671
Iteration 551/1000 | Loss: 0.00002671
Iteration 552/1000 | Loss: 0.00002670
Iteration 553/1000 | Loss: 0.00002670
Iteration 554/1000 | Loss: 0.00002670
Iteration 555/1000 | Loss: 0.00002670
Iteration 556/1000 | Loss: 0.00002670
Iteration 557/1000 | Loss: 0.00002670
Iteration 558/1000 | Loss: 0.00002670
Iteration 559/1000 | Loss: 0.00002670
Iteration 560/1000 | Loss: 0.00002670
Iteration 561/1000 | Loss: 0.00002670
Iteration 562/1000 | Loss: 0.00002670
Iteration 563/1000 | Loss: 0.00002670
Iteration 564/1000 | Loss: 0.00002670
Iteration 565/1000 | Loss: 0.00002670
Iteration 566/1000 | Loss: 0.00002670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 566. Stopping optimization.
Last 5 losses: [2.6702888135332614e-05, 2.6702888135332614e-05, 2.6702888135332614e-05, 2.6702888135332614e-05, 2.6702888135332614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6702888135332614e-05

Optimization complete. Final v2v error: 4.156780242919922 mm

Highest mean error: 6.48442268371582 mm for frame 41

Lowest mean error: 3.1643919944763184 mm for frame 2

Saving results

Total time: 294.77014803886414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049971
Iteration 2/25 | Loss: 0.01049971
Iteration 3/25 | Loss: 0.01049970
Iteration 4/25 | Loss: 0.01049970
Iteration 5/25 | Loss: 0.00272262
Iteration 6/25 | Loss: 0.00159272
Iteration 7/25 | Loss: 0.00140088
Iteration 8/25 | Loss: 0.00116939
Iteration 9/25 | Loss: 0.00111307
Iteration 10/25 | Loss: 0.00115320
Iteration 11/25 | Loss: 0.00112451
Iteration 12/25 | Loss: 0.00107929
Iteration 13/25 | Loss: 0.00106834
Iteration 14/25 | Loss: 0.00103723
Iteration 15/25 | Loss: 0.00100128
Iteration 16/25 | Loss: 0.00101331
Iteration 17/25 | Loss: 0.00097659
Iteration 18/25 | Loss: 0.00092517
Iteration 19/25 | Loss: 0.00090602
Iteration 20/25 | Loss: 0.00090867
Iteration 21/25 | Loss: 0.00090187
Iteration 22/25 | Loss: 0.00089120
Iteration 23/25 | Loss: 0.00089826
Iteration 24/25 | Loss: 0.00089684
Iteration 25/25 | Loss: 0.00089249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44732702
Iteration 2/25 | Loss: 0.00151137
Iteration 3/25 | Loss: 0.00117618
Iteration 4/25 | Loss: 0.00117618
Iteration 5/25 | Loss: 0.00117617
Iteration 6/25 | Loss: 0.00117617
Iteration 7/25 | Loss: 0.00117617
Iteration 8/25 | Loss: 0.00117617
Iteration 9/25 | Loss: 0.00117617
Iteration 10/25 | Loss: 0.00117617
Iteration 11/25 | Loss: 0.00117617
Iteration 12/25 | Loss: 0.00117617
Iteration 13/25 | Loss: 0.00117617
Iteration 14/25 | Loss: 0.00117617
Iteration 15/25 | Loss: 0.00117617
Iteration 16/25 | Loss: 0.00117617
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011761728674173355, 0.0011761728674173355, 0.0011761728674173355, 0.0011761728674173355, 0.0011761728674173355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011761728674173355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117617
Iteration 2/1000 | Loss: 0.00061356
Iteration 3/1000 | Loss: 0.00059996
Iteration 4/1000 | Loss: 0.00032634
Iteration 5/1000 | Loss: 0.00054637
Iteration 6/1000 | Loss: 0.00037444
Iteration 7/1000 | Loss: 0.00029873
Iteration 8/1000 | Loss: 0.00042000
Iteration 9/1000 | Loss: 0.00046811
Iteration 10/1000 | Loss: 0.00032544
Iteration 11/1000 | Loss: 0.00050443
Iteration 12/1000 | Loss: 0.00088141
Iteration 13/1000 | Loss: 0.00090775
Iteration 14/1000 | Loss: 0.00153757
Iteration 15/1000 | Loss: 0.00079922
Iteration 16/1000 | Loss: 0.00125961
Iteration 17/1000 | Loss: 0.00110125
Iteration 18/1000 | Loss: 0.00056257
Iteration 19/1000 | Loss: 0.00026411
Iteration 20/1000 | Loss: 0.00014931
Iteration 21/1000 | Loss: 0.00035159
Iteration 22/1000 | Loss: 0.00033643
Iteration 23/1000 | Loss: 0.00035760
Iteration 24/1000 | Loss: 0.00052138
Iteration 25/1000 | Loss: 0.00051475
Iteration 26/1000 | Loss: 0.00040569
Iteration 27/1000 | Loss: 0.00109095
Iteration 28/1000 | Loss: 0.00152393
Iteration 29/1000 | Loss: 0.00151267
Iteration 30/1000 | Loss: 0.00098235
Iteration 31/1000 | Loss: 0.00181058
Iteration 32/1000 | Loss: 0.00042281
Iteration 33/1000 | Loss: 0.00192130
Iteration 34/1000 | Loss: 0.00101878
Iteration 35/1000 | Loss: 0.00041424
Iteration 36/1000 | Loss: 0.00034343
Iteration 37/1000 | Loss: 0.00015865
Iteration 38/1000 | Loss: 0.00008203
Iteration 39/1000 | Loss: 0.00102107
Iteration 40/1000 | Loss: 0.00108370
Iteration 41/1000 | Loss: 0.00064528
Iteration 42/1000 | Loss: 0.00095524
Iteration 43/1000 | Loss: 0.00031723
Iteration 44/1000 | Loss: 0.00030099
Iteration 45/1000 | Loss: 0.00040221
Iteration 46/1000 | Loss: 0.00088529
Iteration 47/1000 | Loss: 0.00065630
Iteration 48/1000 | Loss: 0.00057247
Iteration 49/1000 | Loss: 0.00033623
Iteration 50/1000 | Loss: 0.00018914
Iteration 51/1000 | Loss: 0.00042889
Iteration 52/1000 | Loss: 0.00008216
Iteration 53/1000 | Loss: 0.00049903
Iteration 54/1000 | Loss: 0.00016970
Iteration 55/1000 | Loss: 0.00014850
Iteration 56/1000 | Loss: 0.00106905
Iteration 57/1000 | Loss: 0.00087457
Iteration 58/1000 | Loss: 0.00081811
Iteration 59/1000 | Loss: 0.00024997
Iteration 60/1000 | Loss: 0.00020241
Iteration 61/1000 | Loss: 0.00021156
Iteration 62/1000 | Loss: 0.00040103
Iteration 63/1000 | Loss: 0.00028473
Iteration 64/1000 | Loss: 0.00094624
Iteration 65/1000 | Loss: 0.00251381
Iteration 66/1000 | Loss: 0.00026877
Iteration 67/1000 | Loss: 0.00032133
Iteration 68/1000 | Loss: 0.00017231
Iteration 69/1000 | Loss: 0.00013253
Iteration 70/1000 | Loss: 0.00010030
Iteration 71/1000 | Loss: 0.00017652
Iteration 72/1000 | Loss: 0.00011748
Iteration 73/1000 | Loss: 0.00015245
Iteration 74/1000 | Loss: 0.00009942
Iteration 75/1000 | Loss: 0.00024073
Iteration 76/1000 | Loss: 0.00005974
Iteration 77/1000 | Loss: 0.00067758
Iteration 78/1000 | Loss: 0.00004188
Iteration 79/1000 | Loss: 0.00025625
Iteration 80/1000 | Loss: 0.00013819
Iteration 81/1000 | Loss: 0.00006598
Iteration 82/1000 | Loss: 0.00010955
Iteration 83/1000 | Loss: 0.00004053
Iteration 84/1000 | Loss: 0.00005644
Iteration 85/1000 | Loss: 0.00003735
Iteration 86/1000 | Loss: 0.00017486
Iteration 87/1000 | Loss: 0.00003205
Iteration 88/1000 | Loss: 0.00003015
Iteration 89/1000 | Loss: 0.00012181
Iteration 90/1000 | Loss: 0.00033885
Iteration 91/1000 | Loss: 0.00014429
Iteration 92/1000 | Loss: 0.00003176
Iteration 93/1000 | Loss: 0.00013581
Iteration 94/1000 | Loss: 0.00002875
Iteration 95/1000 | Loss: 0.00002727
Iteration 96/1000 | Loss: 0.00012533
Iteration 97/1000 | Loss: 0.00011099
Iteration 98/1000 | Loss: 0.00002542
Iteration 99/1000 | Loss: 0.00003947
Iteration 100/1000 | Loss: 0.00003636
Iteration 101/1000 | Loss: 0.00003187
Iteration 102/1000 | Loss: 0.00003249
Iteration 103/1000 | Loss: 0.00003002
Iteration 104/1000 | Loss: 0.00002941
Iteration 105/1000 | Loss: 0.00014198
Iteration 106/1000 | Loss: 0.00013025
Iteration 107/1000 | Loss: 0.00002984
Iteration 108/1000 | Loss: 0.00022638
Iteration 109/1000 | Loss: 0.00003201
Iteration 110/1000 | Loss: 0.00002526
Iteration 111/1000 | Loss: 0.00003595
Iteration 112/1000 | Loss: 0.00018115
Iteration 113/1000 | Loss: 0.00020100
Iteration 114/1000 | Loss: 0.00023206
Iteration 115/1000 | Loss: 0.00017647
Iteration 116/1000 | Loss: 0.00033087
Iteration 117/1000 | Loss: 0.00013810
Iteration 118/1000 | Loss: 0.00018189
Iteration 119/1000 | Loss: 0.00002809
Iteration 120/1000 | Loss: 0.00003443
Iteration 121/1000 | Loss: 0.00002736
Iteration 122/1000 | Loss: 0.00002361
Iteration 123/1000 | Loss: 0.00002242
Iteration 124/1000 | Loss: 0.00013756
Iteration 125/1000 | Loss: 0.00040378
Iteration 126/1000 | Loss: 0.00023010
Iteration 127/1000 | Loss: 0.00020328
Iteration 128/1000 | Loss: 0.00043245
Iteration 129/1000 | Loss: 0.00057636
Iteration 130/1000 | Loss: 0.00031029
Iteration 131/1000 | Loss: 0.00021543
Iteration 132/1000 | Loss: 0.00017965
Iteration 133/1000 | Loss: 0.00004344
Iteration 134/1000 | Loss: 0.00019043
Iteration 135/1000 | Loss: 0.00002793
Iteration 136/1000 | Loss: 0.00009724
Iteration 137/1000 | Loss: 0.00004412
Iteration 138/1000 | Loss: 0.00002391
Iteration 139/1000 | Loss: 0.00005487
Iteration 140/1000 | Loss: 0.00003200
Iteration 141/1000 | Loss: 0.00002503
Iteration 142/1000 | Loss: 0.00002247
Iteration 143/1000 | Loss: 0.00002206
Iteration 144/1000 | Loss: 0.00041032
Iteration 145/1000 | Loss: 0.00003909
Iteration 146/1000 | Loss: 0.00002724
Iteration 147/1000 | Loss: 0.00002327
Iteration 148/1000 | Loss: 0.00002165
Iteration 149/1000 | Loss: 0.00002033
Iteration 150/1000 | Loss: 0.00001962
Iteration 151/1000 | Loss: 0.00001942
Iteration 152/1000 | Loss: 0.00001927
Iteration 153/1000 | Loss: 0.00001911
Iteration 154/1000 | Loss: 0.00001897
Iteration 155/1000 | Loss: 0.00020847
Iteration 156/1000 | Loss: 0.00020847
Iteration 157/1000 | Loss: 0.00020847
Iteration 158/1000 | Loss: 0.00020847
Iteration 159/1000 | Loss: 0.00021314
Iteration 160/1000 | Loss: 0.00007760
Iteration 161/1000 | Loss: 0.00011754
Iteration 162/1000 | Loss: 0.00022355
Iteration 163/1000 | Loss: 0.00006838
Iteration 164/1000 | Loss: 0.00017394
Iteration 165/1000 | Loss: 0.00020065
Iteration 166/1000 | Loss: 0.00002657
Iteration 167/1000 | Loss: 0.00002494
Iteration 168/1000 | Loss: 0.00002343
Iteration 169/1000 | Loss: 0.00002241
Iteration 170/1000 | Loss: 0.00002172
Iteration 171/1000 | Loss: 0.00002099
Iteration 172/1000 | Loss: 0.00009588
Iteration 173/1000 | Loss: 0.00002028
Iteration 174/1000 | Loss: 0.00001973
Iteration 175/1000 | Loss: 0.00001944
Iteration 176/1000 | Loss: 0.00001936
Iteration 177/1000 | Loss: 0.00001935
Iteration 178/1000 | Loss: 0.00010255
Iteration 179/1000 | Loss: 0.00010255
Iteration 180/1000 | Loss: 0.00002131
Iteration 181/1000 | Loss: 0.00001947
Iteration 182/1000 | Loss: 0.00001920
Iteration 183/1000 | Loss: 0.00001919
Iteration 184/1000 | Loss: 0.00001918
Iteration 185/1000 | Loss: 0.00001918
Iteration 186/1000 | Loss: 0.00001918
Iteration 187/1000 | Loss: 0.00001917
Iteration 188/1000 | Loss: 0.00001917
Iteration 189/1000 | Loss: 0.00001916
Iteration 190/1000 | Loss: 0.00001916
Iteration 191/1000 | Loss: 0.00001916
Iteration 192/1000 | Loss: 0.00001916
Iteration 193/1000 | Loss: 0.00001915
Iteration 194/1000 | Loss: 0.00001915
Iteration 195/1000 | Loss: 0.00001915
Iteration 196/1000 | Loss: 0.00001914
Iteration 197/1000 | Loss: 0.00001914
Iteration 198/1000 | Loss: 0.00001914
Iteration 199/1000 | Loss: 0.00001913
Iteration 200/1000 | Loss: 0.00001913
Iteration 201/1000 | Loss: 0.00001912
Iteration 202/1000 | Loss: 0.00001912
Iteration 203/1000 | Loss: 0.00001912
Iteration 204/1000 | Loss: 0.00001911
Iteration 205/1000 | Loss: 0.00001910
Iteration 206/1000 | Loss: 0.00001910
Iteration 207/1000 | Loss: 0.00001909
Iteration 208/1000 | Loss: 0.00001908
Iteration 209/1000 | Loss: 0.00001908
Iteration 210/1000 | Loss: 0.00001908
Iteration 211/1000 | Loss: 0.00001908
Iteration 212/1000 | Loss: 0.00001908
Iteration 213/1000 | Loss: 0.00001908
Iteration 214/1000 | Loss: 0.00001908
Iteration 215/1000 | Loss: 0.00001908
Iteration 216/1000 | Loss: 0.00001908
Iteration 217/1000 | Loss: 0.00001907
Iteration 218/1000 | Loss: 0.00001907
Iteration 219/1000 | Loss: 0.00001906
Iteration 220/1000 | Loss: 0.00001906
Iteration 221/1000 | Loss: 0.00001906
Iteration 222/1000 | Loss: 0.00001906
Iteration 223/1000 | Loss: 0.00001905
Iteration 224/1000 | Loss: 0.00001905
Iteration 225/1000 | Loss: 0.00001905
Iteration 226/1000 | Loss: 0.00012030
Iteration 227/1000 | Loss: 0.00001931
Iteration 228/1000 | Loss: 0.00001901
Iteration 229/1000 | Loss: 0.00001899
Iteration 230/1000 | Loss: 0.00001898
Iteration 231/1000 | Loss: 0.00001897
Iteration 232/1000 | Loss: 0.00001896
Iteration 233/1000 | Loss: 0.00001895
Iteration 234/1000 | Loss: 0.00001894
Iteration 235/1000 | Loss: 0.00001893
Iteration 236/1000 | Loss: 0.00001892
Iteration 237/1000 | Loss: 0.00001892
Iteration 238/1000 | Loss: 0.00001891
Iteration 239/1000 | Loss: 0.00001891
Iteration 240/1000 | Loss: 0.00001890
Iteration 241/1000 | Loss: 0.00001889
Iteration 242/1000 | Loss: 0.00001889
Iteration 243/1000 | Loss: 0.00001885
Iteration 244/1000 | Loss: 0.00001885
Iteration 245/1000 | Loss: 0.00001885
Iteration 246/1000 | Loss: 0.00001884
Iteration 247/1000 | Loss: 0.00001883
Iteration 248/1000 | Loss: 0.00001878
Iteration 249/1000 | Loss: 0.00001878
Iteration 250/1000 | Loss: 0.00001877
Iteration 251/1000 | Loss: 0.00001876
Iteration 252/1000 | Loss: 0.00001876
Iteration 253/1000 | Loss: 0.00001876
Iteration 254/1000 | Loss: 0.00001876
Iteration 255/1000 | Loss: 0.00001876
Iteration 256/1000 | Loss: 0.00001875
Iteration 257/1000 | Loss: 0.00001875
Iteration 258/1000 | Loss: 0.00001875
Iteration 259/1000 | Loss: 0.00001875
Iteration 260/1000 | Loss: 0.00001875
Iteration 261/1000 | Loss: 0.00001874
Iteration 262/1000 | Loss: 0.00001874
Iteration 263/1000 | Loss: 0.00001873
Iteration 264/1000 | Loss: 0.00001873
Iteration 265/1000 | Loss: 0.00001873
Iteration 266/1000 | Loss: 0.00019915
Iteration 267/1000 | Loss: 0.00002560
Iteration 268/1000 | Loss: 0.00002251
Iteration 269/1000 | Loss: 0.00018035
Iteration 270/1000 | Loss: 0.00002187
Iteration 271/1000 | Loss: 0.00002061
Iteration 272/1000 | Loss: 0.00010129
Iteration 273/1000 | Loss: 0.00002583
Iteration 274/1000 | Loss: 0.00002763
Iteration 275/1000 | Loss: 0.00001979
Iteration 276/1000 | Loss: 0.00019776
Iteration 277/1000 | Loss: 0.00028772
Iteration 278/1000 | Loss: 0.00028573
Iteration 279/1000 | Loss: 0.00018732
Iteration 280/1000 | Loss: 0.00027340
Iteration 281/1000 | Loss: 0.00003267
Iteration 282/1000 | Loss: 0.00002688
Iteration 283/1000 | Loss: 0.00016178
Iteration 284/1000 | Loss: 0.00003297
Iteration 285/1000 | Loss: 0.00002291
Iteration 286/1000 | Loss: 0.00002217
Iteration 287/1000 | Loss: 0.00002122
Iteration 288/1000 | Loss: 0.00002013
Iteration 289/1000 | Loss: 0.00001965
Iteration 290/1000 | Loss: 0.00010412
Iteration 291/1000 | Loss: 0.00001970
Iteration 292/1000 | Loss: 0.00001929
Iteration 293/1000 | Loss: 0.00001920
Iteration 294/1000 | Loss: 0.00001919
Iteration 295/1000 | Loss: 0.00001916
Iteration 296/1000 | Loss: 0.00001913
Iteration 297/1000 | Loss: 0.00001912
Iteration 298/1000 | Loss: 0.00001911
Iteration 299/1000 | Loss: 0.00001911
Iteration 300/1000 | Loss: 0.00001910
Iteration 301/1000 | Loss: 0.00001908
Iteration 302/1000 | Loss: 0.00001907
Iteration 303/1000 | Loss: 0.00001907
Iteration 304/1000 | Loss: 0.00001906
Iteration 305/1000 | Loss: 0.00001906
Iteration 306/1000 | Loss: 0.00001905
Iteration 307/1000 | Loss: 0.00001905
Iteration 308/1000 | Loss: 0.00001905
Iteration 309/1000 | Loss: 0.00001905
Iteration 310/1000 | Loss: 0.00001904
Iteration 311/1000 | Loss: 0.00001904
Iteration 312/1000 | Loss: 0.00001903
Iteration 313/1000 | Loss: 0.00001903
Iteration 314/1000 | Loss: 0.00001903
Iteration 315/1000 | Loss: 0.00001902
Iteration 316/1000 | Loss: 0.00001902
Iteration 317/1000 | Loss: 0.00001902
Iteration 318/1000 | Loss: 0.00001902
Iteration 319/1000 | Loss: 0.00001901
Iteration 320/1000 | Loss: 0.00001901
Iteration 321/1000 | Loss: 0.00001901
Iteration 322/1000 | Loss: 0.00001901
Iteration 323/1000 | Loss: 0.00001901
Iteration 324/1000 | Loss: 0.00001900
Iteration 325/1000 | Loss: 0.00001899
Iteration 326/1000 | Loss: 0.00001898
Iteration 327/1000 | Loss: 0.00001898
Iteration 328/1000 | Loss: 0.00001898
Iteration 329/1000 | Loss: 0.00001897
Iteration 330/1000 | Loss: 0.00001897
Iteration 331/1000 | Loss: 0.00001896
Iteration 332/1000 | Loss: 0.00001896
Iteration 333/1000 | Loss: 0.00001896
Iteration 334/1000 | Loss: 0.00001895
Iteration 335/1000 | Loss: 0.00001895
Iteration 336/1000 | Loss: 0.00001894
Iteration 337/1000 | Loss: 0.00001894
Iteration 338/1000 | Loss: 0.00001894
Iteration 339/1000 | Loss: 0.00001894
Iteration 340/1000 | Loss: 0.00001893
Iteration 341/1000 | Loss: 0.00001893
Iteration 342/1000 | Loss: 0.00001892
Iteration 343/1000 | Loss: 0.00001892
Iteration 344/1000 | Loss: 0.00001892
Iteration 345/1000 | Loss: 0.00001891
Iteration 346/1000 | Loss: 0.00001891
Iteration 347/1000 | Loss: 0.00001887
Iteration 348/1000 | Loss: 0.00001886
Iteration 349/1000 | Loss: 0.00001883
Iteration 350/1000 | Loss: 0.00001882
Iteration 351/1000 | Loss: 0.00001881
Iteration 352/1000 | Loss: 0.00001881
Iteration 353/1000 | Loss: 0.00001881
Iteration 354/1000 | Loss: 0.00001880
Iteration 355/1000 | Loss: 0.00001880
Iteration 356/1000 | Loss: 0.00001878
Iteration 357/1000 | Loss: 0.00001877
Iteration 358/1000 | Loss: 0.00001877
Iteration 359/1000 | Loss: 0.00001876
Iteration 360/1000 | Loss: 0.00001865
Iteration 361/1000 | Loss: 0.00001853
Iteration 362/1000 | Loss: 0.00001851
Iteration 363/1000 | Loss: 0.00001850
Iteration 364/1000 | Loss: 0.00001850
Iteration 365/1000 | Loss: 0.00001850
Iteration 366/1000 | Loss: 0.00001850
Iteration 367/1000 | Loss: 0.00001849
Iteration 368/1000 | Loss: 0.00001849
Iteration 369/1000 | Loss: 0.00001849
Iteration 370/1000 | Loss: 0.00001849
Iteration 371/1000 | Loss: 0.00001848
Iteration 372/1000 | Loss: 0.00001848
Iteration 373/1000 | Loss: 0.00001845
Iteration 374/1000 | Loss: 0.00001845
Iteration 375/1000 | Loss: 0.00001844
Iteration 376/1000 | Loss: 0.00001843
Iteration 377/1000 | Loss: 0.00001842
Iteration 378/1000 | Loss: 0.00001842
Iteration 379/1000 | Loss: 0.00001841
Iteration 380/1000 | Loss: 0.00001840
Iteration 381/1000 | Loss: 0.00001840
Iteration 382/1000 | Loss: 0.00001840
Iteration 383/1000 | Loss: 0.00001840
Iteration 384/1000 | Loss: 0.00001839
Iteration 385/1000 | Loss: 0.00001839
Iteration 386/1000 | Loss: 0.00001839
Iteration 387/1000 | Loss: 0.00001838
Iteration 388/1000 | Loss: 0.00001837
Iteration 389/1000 | Loss: 0.00001835
Iteration 390/1000 | Loss: 0.00001835
Iteration 391/1000 | Loss: 0.00001831
Iteration 392/1000 | Loss: 0.00001826
Iteration 393/1000 | Loss: 0.00001826
Iteration 394/1000 | Loss: 0.00001824
Iteration 395/1000 | Loss: 0.00001824
Iteration 396/1000 | Loss: 0.00001824
Iteration 397/1000 | Loss: 0.00001823
Iteration 398/1000 | Loss: 0.00001823
Iteration 399/1000 | Loss: 0.00001823
Iteration 400/1000 | Loss: 0.00001822
Iteration 401/1000 | Loss: 0.00001822
Iteration 402/1000 | Loss: 0.00001821
Iteration 403/1000 | Loss: 0.00001821
Iteration 404/1000 | Loss: 0.00001821
Iteration 405/1000 | Loss: 0.00001820
Iteration 406/1000 | Loss: 0.00001820
Iteration 407/1000 | Loss: 0.00001819
Iteration 408/1000 | Loss: 0.00001818
Iteration 409/1000 | Loss: 0.00001818
Iteration 410/1000 | Loss: 0.00001817
Iteration 411/1000 | Loss: 0.00001817
Iteration 412/1000 | Loss: 0.00001816
Iteration 413/1000 | Loss: 0.00001816
Iteration 414/1000 | Loss: 0.00001816
Iteration 415/1000 | Loss: 0.00001816
Iteration 416/1000 | Loss: 0.00001816
Iteration 417/1000 | Loss: 0.00001816
Iteration 418/1000 | Loss: 0.00001816
Iteration 419/1000 | Loss: 0.00001815
Iteration 420/1000 | Loss: 0.00001815
Iteration 421/1000 | Loss: 0.00001815
Iteration 422/1000 | Loss: 0.00001815
Iteration 423/1000 | Loss: 0.00001814
Iteration 424/1000 | Loss: 0.00001813
Iteration 425/1000 | Loss: 0.00001813
Iteration 426/1000 | Loss: 0.00001813
Iteration 427/1000 | Loss: 0.00001812
Iteration 428/1000 | Loss: 0.00001812
Iteration 429/1000 | Loss: 0.00001812
Iteration 430/1000 | Loss: 0.00001811
Iteration 431/1000 | Loss: 0.00001811
Iteration 432/1000 | Loss: 0.00016485
Iteration 433/1000 | Loss: 0.00020912
Iteration 434/1000 | Loss: 0.00017007
Iteration 435/1000 | Loss: 0.00017312
Iteration 436/1000 | Loss: 0.00017466
Iteration 437/1000 | Loss: 0.00007227
Iteration 438/1000 | Loss: 0.00002303
Iteration 439/1000 | Loss: 0.00004970
Iteration 440/1000 | Loss: 0.00001969
Iteration 441/1000 | Loss: 0.00001927
Iteration 442/1000 | Loss: 0.00001914
Iteration 443/1000 | Loss: 0.00001903
Iteration 444/1000 | Loss: 0.00001902
Iteration 445/1000 | Loss: 0.00001901
Iteration 446/1000 | Loss: 0.00001895
Iteration 447/1000 | Loss: 0.00001894
Iteration 448/1000 | Loss: 0.00001894
Iteration 449/1000 | Loss: 0.00001893
Iteration 450/1000 | Loss: 0.00001893
Iteration 451/1000 | Loss: 0.00001893
Iteration 452/1000 | Loss: 0.00001892
Iteration 453/1000 | Loss: 0.00001892
Iteration 454/1000 | Loss: 0.00001891
Iteration 455/1000 | Loss: 0.00001891
Iteration 456/1000 | Loss: 0.00001891
Iteration 457/1000 | Loss: 0.00001891
Iteration 458/1000 | Loss: 0.00001891
Iteration 459/1000 | Loss: 0.00001891
Iteration 460/1000 | Loss: 0.00001891
Iteration 461/1000 | Loss: 0.00001891
Iteration 462/1000 | Loss: 0.00001890
Iteration 463/1000 | Loss: 0.00001890
Iteration 464/1000 | Loss: 0.00001890
Iteration 465/1000 | Loss: 0.00001889
Iteration 466/1000 | Loss: 0.00001889
Iteration 467/1000 | Loss: 0.00001889
Iteration 468/1000 | Loss: 0.00001888
Iteration 469/1000 | Loss: 0.00001888
Iteration 470/1000 | Loss: 0.00001888
Iteration 471/1000 | Loss: 0.00001888
Iteration 472/1000 | Loss: 0.00001888
Iteration 473/1000 | Loss: 0.00001888
Iteration 474/1000 | Loss: 0.00001888
Iteration 475/1000 | Loss: 0.00001888
Iteration 476/1000 | Loss: 0.00001888
Iteration 477/1000 | Loss: 0.00001888
Iteration 478/1000 | Loss: 0.00001888
Iteration 479/1000 | Loss: 0.00001887
Iteration 480/1000 | Loss: 0.00001887
Iteration 481/1000 | Loss: 0.00001887
Iteration 482/1000 | Loss: 0.00001887
Iteration 483/1000 | Loss: 0.00001887
Iteration 484/1000 | Loss: 0.00001887
Iteration 485/1000 | Loss: 0.00001886
Iteration 486/1000 | Loss: 0.00001886
Iteration 487/1000 | Loss: 0.00001886
Iteration 488/1000 | Loss: 0.00001885
Iteration 489/1000 | Loss: 0.00001885
Iteration 490/1000 | Loss: 0.00001884
Iteration 491/1000 | Loss: 0.00001884
Iteration 492/1000 | Loss: 0.00001884
Iteration 493/1000 | Loss: 0.00001884
Iteration 494/1000 | Loss: 0.00001883
Iteration 495/1000 | Loss: 0.00001883
Iteration 496/1000 | Loss: 0.00001883
Iteration 497/1000 | Loss: 0.00001882
Iteration 498/1000 | Loss: 0.00001882
Iteration 499/1000 | Loss: 0.00001882
Iteration 500/1000 | Loss: 0.00001882
Iteration 501/1000 | Loss: 0.00001882
Iteration 502/1000 | Loss: 0.00001882
Iteration 503/1000 | Loss: 0.00001882
Iteration 504/1000 | Loss: 0.00001881
Iteration 505/1000 | Loss: 0.00001881
Iteration 506/1000 | Loss: 0.00001881
Iteration 507/1000 | Loss: 0.00001881
Iteration 508/1000 | Loss: 0.00001881
Iteration 509/1000 | Loss: 0.00001881
Iteration 510/1000 | Loss: 0.00001880
Iteration 511/1000 | Loss: 0.00001880
Iteration 512/1000 | Loss: 0.00001880
Iteration 513/1000 | Loss: 0.00001879
Iteration 514/1000 | Loss: 0.00001879
Iteration 515/1000 | Loss: 0.00001879
Iteration 516/1000 | Loss: 0.00001878
Iteration 517/1000 | Loss: 0.00001878
Iteration 518/1000 | Loss: 0.00001878
Iteration 519/1000 | Loss: 0.00001878
Iteration 520/1000 | Loss: 0.00001878
Iteration 521/1000 | Loss: 0.00001878
Iteration 522/1000 | Loss: 0.00001878
Iteration 523/1000 | Loss: 0.00001877
Iteration 524/1000 | Loss: 0.00001877
Iteration 525/1000 | Loss: 0.00001877
Iteration 526/1000 | Loss: 0.00001877
Iteration 527/1000 | Loss: 0.00001877
Iteration 528/1000 | Loss: 0.00001876
Iteration 529/1000 | Loss: 0.00001876
Iteration 530/1000 | Loss: 0.00001876
Iteration 531/1000 | Loss: 0.00001875
Iteration 532/1000 | Loss: 0.00015347
Iteration 533/1000 | Loss: 0.00002282
Iteration 534/1000 | Loss: 0.00001958
Iteration 535/1000 | Loss: 0.00001888
Iteration 536/1000 | Loss: 0.00001880
Iteration 537/1000 | Loss: 0.00001877
Iteration 538/1000 | Loss: 0.00001877
Iteration 539/1000 | Loss: 0.00001872
Iteration 540/1000 | Loss: 0.00009738
Iteration 541/1000 | Loss: 0.00002502
Iteration 542/1000 | Loss: 0.00002061
Iteration 543/1000 | Loss: 0.00001883
Iteration 544/1000 | Loss: 0.00001866
Iteration 545/1000 | Loss: 0.00001866
Iteration 546/1000 | Loss: 0.00001866
Iteration 547/1000 | Loss: 0.00001866
Iteration 548/1000 | Loss: 0.00001865
Iteration 549/1000 | Loss: 0.00001865
Iteration 550/1000 | Loss: 0.00001864
Iteration 551/1000 | Loss: 0.00001864
Iteration 552/1000 | Loss: 0.00001864
Iteration 553/1000 | Loss: 0.00001864
Iteration 554/1000 | Loss: 0.00001864
Iteration 555/1000 | Loss: 0.00001864
Iteration 556/1000 | Loss: 0.00001864
Iteration 557/1000 | Loss: 0.00001863
Iteration 558/1000 | Loss: 0.00001863
Iteration 559/1000 | Loss: 0.00001863
Iteration 560/1000 | Loss: 0.00001861
Iteration 561/1000 | Loss: 0.00001861
Iteration 562/1000 | Loss: 0.00001861
Iteration 563/1000 | Loss: 0.00001861
Iteration 564/1000 | Loss: 0.00001861
Iteration 565/1000 | Loss: 0.00001861
Iteration 566/1000 | Loss: 0.00001860
Iteration 567/1000 | Loss: 0.00001860
Iteration 568/1000 | Loss: 0.00001860
Iteration 569/1000 | Loss: 0.00001860
Iteration 570/1000 | Loss: 0.00001860
Iteration 571/1000 | Loss: 0.00001860
Iteration 572/1000 | Loss: 0.00001860
Iteration 573/1000 | Loss: 0.00001860
Iteration 574/1000 | Loss: 0.00001860
Iteration 575/1000 | Loss: 0.00001860
Iteration 576/1000 | Loss: 0.00001860
Iteration 577/1000 | Loss: 0.00001860
Iteration 578/1000 | Loss: 0.00001860
Iteration 579/1000 | Loss: 0.00001860
Iteration 580/1000 | Loss: 0.00001860
Iteration 581/1000 | Loss: 0.00001859
Iteration 582/1000 | Loss: 0.00001859
Iteration 583/1000 | Loss: 0.00001859
Iteration 584/1000 | Loss: 0.00001859
Iteration 585/1000 | Loss: 0.00001859
Iteration 586/1000 | Loss: 0.00001859
Iteration 587/1000 | Loss: 0.00001859
Iteration 588/1000 | Loss: 0.00001859
Iteration 589/1000 | Loss: 0.00001859
Iteration 590/1000 | Loss: 0.00001859
Iteration 591/1000 | Loss: 0.00001859
Iteration 592/1000 | Loss: 0.00001859
Iteration 593/1000 | Loss: 0.00001859
Iteration 594/1000 | Loss: 0.00001859
Iteration 595/1000 | Loss: 0.00001859
Iteration 596/1000 | Loss: 0.00001859
Iteration 597/1000 | Loss: 0.00001859
Iteration 598/1000 | Loss: 0.00001859
Iteration 599/1000 | Loss: 0.00001858
Iteration 600/1000 | Loss: 0.00001858
Iteration 601/1000 | Loss: 0.00001858
Iteration 602/1000 | Loss: 0.00001858
Iteration 603/1000 | Loss: 0.00001858
Iteration 604/1000 | Loss: 0.00001858
Iteration 605/1000 | Loss: 0.00001858
Iteration 606/1000 | Loss: 0.00001858
Iteration 607/1000 | Loss: 0.00001858
Iteration 608/1000 | Loss: 0.00001858
Iteration 609/1000 | Loss: 0.00001858
Iteration 610/1000 | Loss: 0.00001858
Iteration 611/1000 | Loss: 0.00001858
Iteration 612/1000 | Loss: 0.00001857
Iteration 613/1000 | Loss: 0.00001857
Iteration 614/1000 | Loss: 0.00001857
Iteration 615/1000 | Loss: 0.00001856
Iteration 616/1000 | Loss: 0.00001856
Iteration 617/1000 | Loss: 0.00001855
Iteration 618/1000 | Loss: 0.00001853
Iteration 619/1000 | Loss: 0.00001853
Iteration 620/1000 | Loss: 0.00001852
Iteration 621/1000 | Loss: 0.00001852
Iteration 622/1000 | Loss: 0.00001852
Iteration 623/1000 | Loss: 0.00001851
Iteration 624/1000 | Loss: 0.00001851
Iteration 625/1000 | Loss: 0.00001851
Iteration 626/1000 | Loss: 0.00001850
Iteration 627/1000 | Loss: 0.00001841
Iteration 628/1000 | Loss: 0.00012744
Iteration 629/1000 | Loss: 0.00016644
Iteration 630/1000 | Loss: 0.00010559
Iteration 631/1000 | Loss: 0.00018698
Iteration 632/1000 | Loss: 0.00015355
Iteration 633/1000 | Loss: 0.00002619
Iteration 634/1000 | Loss: 0.00006969
Iteration 635/1000 | Loss: 0.00002139
Iteration 636/1000 | Loss: 0.00002058
Iteration 637/1000 | Loss: 0.00001985
Iteration 638/1000 | Loss: 0.00001947
Iteration 639/1000 | Loss: 0.00001917
Iteration 640/1000 | Loss: 0.00001888
Iteration 641/1000 | Loss: 0.00001867
Iteration 642/1000 | Loss: 0.00001865
Iteration 643/1000 | Loss: 0.00001860
Iteration 644/1000 | Loss: 0.00001859
Iteration 645/1000 | Loss: 0.00001858
Iteration 646/1000 | Loss: 0.00001857
Iteration 647/1000 | Loss: 0.00001855
Iteration 648/1000 | Loss: 0.00001851
Iteration 649/1000 | Loss: 0.00001847
Iteration 650/1000 | Loss: 0.00001846
Iteration 651/1000 | Loss: 0.00001845
Iteration 652/1000 | Loss: 0.00001844
Iteration 653/1000 | Loss: 0.00001844
Iteration 654/1000 | Loss: 0.00001843
Iteration 655/1000 | Loss: 0.00001843
Iteration 656/1000 | Loss: 0.00001843
Iteration 657/1000 | Loss: 0.00001843
Iteration 658/1000 | Loss: 0.00001842
Iteration 659/1000 | Loss: 0.00001842
Iteration 660/1000 | Loss: 0.00001842
Iteration 661/1000 | Loss: 0.00001842
Iteration 662/1000 | Loss: 0.00001842
Iteration 663/1000 | Loss: 0.00001842
Iteration 664/1000 | Loss: 0.00001842
Iteration 665/1000 | Loss: 0.00001842
Iteration 666/1000 | Loss: 0.00001842
Iteration 667/1000 | Loss: 0.00001842
Iteration 668/1000 | Loss: 0.00001842
Iteration 669/1000 | Loss: 0.00001841
Iteration 670/1000 | Loss: 0.00001841
Iteration 671/1000 | Loss: 0.00001841
Iteration 672/1000 | Loss: 0.00001840
Iteration 673/1000 | Loss: 0.00001840
Iteration 674/1000 | Loss: 0.00001840
Iteration 675/1000 | Loss: 0.00001840
Iteration 676/1000 | Loss: 0.00001840
Iteration 677/1000 | Loss: 0.00001840
Iteration 678/1000 | Loss: 0.00001840
Iteration 679/1000 | Loss: 0.00001840
Iteration 680/1000 | Loss: 0.00001839
Iteration 681/1000 | Loss: 0.00001839
Iteration 682/1000 | Loss: 0.00001839
Iteration 683/1000 | Loss: 0.00001839
Iteration 684/1000 | Loss: 0.00001839
Iteration 685/1000 | Loss: 0.00001839
Iteration 686/1000 | Loss: 0.00001839
Iteration 687/1000 | Loss: 0.00001839
Iteration 688/1000 | Loss: 0.00001838
Iteration 689/1000 | Loss: 0.00001838
Iteration 690/1000 | Loss: 0.00001838
Iteration 691/1000 | Loss: 0.00001838
Iteration 692/1000 | Loss: 0.00001837
Iteration 693/1000 | Loss: 0.00001837
Iteration 694/1000 | Loss: 0.00001837
Iteration 695/1000 | Loss: 0.00001837
Iteration 696/1000 | Loss: 0.00001837
Iteration 697/1000 | Loss: 0.00001837
Iteration 698/1000 | Loss: 0.00001837
Iteration 699/1000 | Loss: 0.00001837
Iteration 700/1000 | Loss: 0.00001837
Iteration 701/1000 | Loss: 0.00001837
Iteration 702/1000 | Loss: 0.00001836
Iteration 703/1000 | Loss: 0.00001836
Iteration 704/1000 | Loss: 0.00001836
Iteration 705/1000 | Loss: 0.00001835
Iteration 706/1000 | Loss: 0.00001835
Iteration 707/1000 | Loss: 0.00001835
Iteration 708/1000 | Loss: 0.00001835
Iteration 709/1000 | Loss: 0.00001835
Iteration 710/1000 | Loss: 0.00001835
Iteration 711/1000 | Loss: 0.00001835
Iteration 712/1000 | Loss: 0.00001835
Iteration 713/1000 | Loss: 0.00001834
Iteration 714/1000 | Loss: 0.00001834
Iteration 715/1000 | Loss: 0.00001834
Iteration 716/1000 | Loss: 0.00001834
Iteration 717/1000 | Loss: 0.00001834
Iteration 718/1000 | Loss: 0.00001834
Iteration 719/1000 | Loss: 0.00001834
Iteration 720/1000 | Loss: 0.00001834
Iteration 721/1000 | Loss: 0.00001834
Iteration 722/1000 | Loss: 0.00001834
Iteration 723/1000 | Loss: 0.00001834
Iteration 724/1000 | Loss: 0.00001834
Iteration 725/1000 | Loss: 0.00001834
Iteration 726/1000 | Loss: 0.00001834
Iteration 727/1000 | Loss: 0.00001834
Iteration 728/1000 | Loss: 0.00001834
Iteration 729/1000 | Loss: 0.00001834
Iteration 730/1000 | Loss: 0.00001834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 730. Stopping optimization.
Last 5 losses: [1.8339584130444564e-05, 1.8339584130444564e-05, 1.8339584130444564e-05, 1.8339584130444564e-05, 1.8339584130444564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8339584130444564e-05

Optimization complete. Final v2v error: 3.4806551933288574 mm

Highest mean error: 7.445924282073975 mm for frame 161

Lowest mean error: 3.0095267295837402 mm for frame 58

Saving results

Total time: 486.81948161125183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397351
Iteration 2/25 | Loss: 0.00096212
Iteration 3/25 | Loss: 0.00071886
Iteration 4/25 | Loss: 0.00069143
Iteration 5/25 | Loss: 0.00068338
Iteration 6/25 | Loss: 0.00068117
Iteration 7/25 | Loss: 0.00068070
Iteration 8/25 | Loss: 0.00068070
Iteration 9/25 | Loss: 0.00068070
Iteration 10/25 | Loss: 0.00068070
Iteration 11/25 | Loss: 0.00068070
Iteration 12/25 | Loss: 0.00068070
Iteration 13/25 | Loss: 0.00068070
Iteration 14/25 | Loss: 0.00068070
Iteration 15/25 | Loss: 0.00068070
Iteration 16/25 | Loss: 0.00068070
Iteration 17/25 | Loss: 0.00068070
Iteration 18/25 | Loss: 0.00068070
Iteration 19/25 | Loss: 0.00068070
Iteration 20/25 | Loss: 0.00068070
Iteration 21/25 | Loss: 0.00068070
Iteration 22/25 | Loss: 0.00068070
Iteration 23/25 | Loss: 0.00068070
Iteration 24/25 | Loss: 0.00068070
Iteration 25/25 | Loss: 0.00068070

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45152378
Iteration 2/25 | Loss: 0.00020211
Iteration 3/25 | Loss: 0.00020211
Iteration 4/25 | Loss: 0.00020211
Iteration 5/25 | Loss: 0.00020211
Iteration 6/25 | Loss: 0.00020211
Iteration 7/25 | Loss: 0.00020211
Iteration 8/25 | Loss: 0.00020211
Iteration 9/25 | Loss: 0.00020211
Iteration 10/25 | Loss: 0.00020211
Iteration 11/25 | Loss: 0.00020211
Iteration 12/25 | Loss: 0.00020211
Iteration 13/25 | Loss: 0.00020211
Iteration 14/25 | Loss: 0.00020211
Iteration 15/25 | Loss: 0.00020211
Iteration 16/25 | Loss: 0.00020211
Iteration 17/25 | Loss: 0.00020211
Iteration 18/25 | Loss: 0.00020211
Iteration 19/25 | Loss: 0.00020211
Iteration 20/25 | Loss: 0.00020211
Iteration 21/25 | Loss: 0.00020211
Iteration 22/25 | Loss: 0.00020211
Iteration 23/25 | Loss: 0.00020211
Iteration 24/25 | Loss: 0.00020211
Iteration 25/25 | Loss: 0.00020211

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020211
Iteration 2/1000 | Loss: 0.00002074
Iteration 3/1000 | Loss: 0.00001567
Iteration 4/1000 | Loss: 0.00001363
Iteration 5/1000 | Loss: 0.00001282
Iteration 6/1000 | Loss: 0.00001228
Iteration 7/1000 | Loss: 0.00001187
Iteration 8/1000 | Loss: 0.00001168
Iteration 9/1000 | Loss: 0.00001151
Iteration 10/1000 | Loss: 0.00001144
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001132
Iteration 16/1000 | Loss: 0.00001127
Iteration 17/1000 | Loss: 0.00001119
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001117
Iteration 20/1000 | Loss: 0.00001117
Iteration 21/1000 | Loss: 0.00001116
Iteration 22/1000 | Loss: 0.00001115
Iteration 23/1000 | Loss: 0.00001114
Iteration 24/1000 | Loss: 0.00001114
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001113
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001112
Iteration 29/1000 | Loss: 0.00001111
Iteration 30/1000 | Loss: 0.00001111
Iteration 31/1000 | Loss: 0.00001110
Iteration 32/1000 | Loss: 0.00001110
Iteration 33/1000 | Loss: 0.00001110
Iteration 34/1000 | Loss: 0.00001109
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001109
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001108
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001107
Iteration 45/1000 | Loss: 0.00001107
Iteration 46/1000 | Loss: 0.00001107
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001107
Iteration 50/1000 | Loss: 0.00001107
Iteration 51/1000 | Loss: 0.00001107
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001102
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001101
Iteration 87/1000 | Loss: 0.00001101
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001101
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001100
Iteration 94/1000 | Loss: 0.00001100
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001099
Iteration 99/1000 | Loss: 0.00001099
Iteration 100/1000 | Loss: 0.00001099
Iteration 101/1000 | Loss: 0.00001099
Iteration 102/1000 | Loss: 0.00001099
Iteration 103/1000 | Loss: 0.00001099
Iteration 104/1000 | Loss: 0.00001099
Iteration 105/1000 | Loss: 0.00001099
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001099
Iteration 108/1000 | Loss: 0.00001099
Iteration 109/1000 | Loss: 0.00001099
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001098
Iteration 115/1000 | Loss: 0.00001098
Iteration 116/1000 | Loss: 0.00001098
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001097
Iteration 121/1000 | Loss: 0.00001097
Iteration 122/1000 | Loss: 0.00001097
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001096
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001095
Iteration 130/1000 | Loss: 0.00001095
Iteration 131/1000 | Loss: 0.00001095
Iteration 132/1000 | Loss: 0.00001095
Iteration 133/1000 | Loss: 0.00001095
Iteration 134/1000 | Loss: 0.00001095
Iteration 135/1000 | Loss: 0.00001095
Iteration 136/1000 | Loss: 0.00001095
Iteration 137/1000 | Loss: 0.00001095
Iteration 138/1000 | Loss: 0.00001095
Iteration 139/1000 | Loss: 0.00001094
Iteration 140/1000 | Loss: 0.00001094
Iteration 141/1000 | Loss: 0.00001094
Iteration 142/1000 | Loss: 0.00001094
Iteration 143/1000 | Loss: 0.00001094
Iteration 144/1000 | Loss: 0.00001094
Iteration 145/1000 | Loss: 0.00001094
Iteration 146/1000 | Loss: 0.00001094
Iteration 147/1000 | Loss: 0.00001093
Iteration 148/1000 | Loss: 0.00001092
Iteration 149/1000 | Loss: 0.00001092
Iteration 150/1000 | Loss: 0.00001092
Iteration 151/1000 | Loss: 0.00001092
Iteration 152/1000 | Loss: 0.00001092
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001092
Iteration 158/1000 | Loss: 0.00001092
Iteration 159/1000 | Loss: 0.00001092
Iteration 160/1000 | Loss: 0.00001092
Iteration 161/1000 | Loss: 0.00001092
Iteration 162/1000 | Loss: 0.00001092
Iteration 163/1000 | Loss: 0.00001091
Iteration 164/1000 | Loss: 0.00001091
Iteration 165/1000 | Loss: 0.00001091
Iteration 166/1000 | Loss: 0.00001091
Iteration 167/1000 | Loss: 0.00001091
Iteration 168/1000 | Loss: 0.00001090
Iteration 169/1000 | Loss: 0.00001090
Iteration 170/1000 | Loss: 0.00001090
Iteration 171/1000 | Loss: 0.00001090
Iteration 172/1000 | Loss: 0.00001090
Iteration 173/1000 | Loss: 0.00001090
Iteration 174/1000 | Loss: 0.00001090
Iteration 175/1000 | Loss: 0.00001090
Iteration 176/1000 | Loss: 0.00001090
Iteration 177/1000 | Loss: 0.00001090
Iteration 178/1000 | Loss: 0.00001090
Iteration 179/1000 | Loss: 0.00001090
Iteration 180/1000 | Loss: 0.00001090
Iteration 181/1000 | Loss: 0.00001090
Iteration 182/1000 | Loss: 0.00001090
Iteration 183/1000 | Loss: 0.00001090
Iteration 184/1000 | Loss: 0.00001090
Iteration 185/1000 | Loss: 0.00001090
Iteration 186/1000 | Loss: 0.00001090
Iteration 187/1000 | Loss: 0.00001090
Iteration 188/1000 | Loss: 0.00001090
Iteration 189/1000 | Loss: 0.00001090
Iteration 190/1000 | Loss: 0.00001090
Iteration 191/1000 | Loss: 0.00001090
Iteration 192/1000 | Loss: 0.00001090
Iteration 193/1000 | Loss: 0.00001090
Iteration 194/1000 | Loss: 0.00001090
Iteration 195/1000 | Loss: 0.00001090
Iteration 196/1000 | Loss: 0.00001090
Iteration 197/1000 | Loss: 0.00001090
Iteration 198/1000 | Loss: 0.00001090
Iteration 199/1000 | Loss: 0.00001090
Iteration 200/1000 | Loss: 0.00001090
Iteration 201/1000 | Loss: 0.00001090
Iteration 202/1000 | Loss: 0.00001090
Iteration 203/1000 | Loss: 0.00001090
Iteration 204/1000 | Loss: 0.00001090
Iteration 205/1000 | Loss: 0.00001090
Iteration 206/1000 | Loss: 0.00001090
Iteration 207/1000 | Loss: 0.00001090
Iteration 208/1000 | Loss: 0.00001090
Iteration 209/1000 | Loss: 0.00001090
Iteration 210/1000 | Loss: 0.00001090
Iteration 211/1000 | Loss: 0.00001090
Iteration 212/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.0897440006374381e-05, 1.0897440006374381e-05, 1.0897440006374381e-05, 1.0897440006374381e-05, 1.0897440006374381e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0897440006374381e-05

Optimization complete. Final v2v error: 2.83786940574646 mm

Highest mean error: 2.9065279960632324 mm for frame 84

Lowest mean error: 2.7481186389923096 mm for frame 97

Saving results

Total time: 38.89190220832825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106780
Iteration 2/25 | Loss: 0.01106779
Iteration 3/25 | Loss: 0.00293476
Iteration 4/25 | Loss: 0.00206418
Iteration 5/25 | Loss: 0.00174004
Iteration 6/25 | Loss: 0.00163162
Iteration 7/25 | Loss: 0.00154631
Iteration 8/25 | Loss: 0.00135754
Iteration 9/25 | Loss: 0.00135558
Iteration 10/25 | Loss: 0.00123900
Iteration 11/25 | Loss: 0.00123609
Iteration 12/25 | Loss: 0.00121709
Iteration 13/25 | Loss: 0.00117003
Iteration 14/25 | Loss: 0.00118293
Iteration 15/25 | Loss: 0.00117471
Iteration 16/25 | Loss: 0.00115048
Iteration 17/25 | Loss: 0.00114030
Iteration 18/25 | Loss: 0.00114371
Iteration 19/25 | Loss: 0.00114152
Iteration 20/25 | Loss: 0.00112867
Iteration 21/25 | Loss: 0.00113393
Iteration 22/25 | Loss: 0.00113403
Iteration 23/25 | Loss: 0.00113265
Iteration 24/25 | Loss: 0.00112763
Iteration 25/25 | Loss: 0.00112922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.58680058
Iteration 2/25 | Loss: 0.00158124
Iteration 3/25 | Loss: 0.00138932
Iteration 4/25 | Loss: 0.00138932
Iteration 5/25 | Loss: 0.00138931
Iteration 6/25 | Loss: 0.00138931
Iteration 7/25 | Loss: 0.00138931
Iteration 8/25 | Loss: 0.00138931
Iteration 9/25 | Loss: 0.00138931
Iteration 10/25 | Loss: 0.00138931
Iteration 11/25 | Loss: 0.00138931
Iteration 12/25 | Loss: 0.00138931
Iteration 13/25 | Loss: 0.00138931
Iteration 14/25 | Loss: 0.00138931
Iteration 15/25 | Loss: 0.00138931
Iteration 16/25 | Loss: 0.00138931
Iteration 17/25 | Loss: 0.00138931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001389312674291432, 0.001389312674291432, 0.001389312674291432, 0.001389312674291432, 0.001389312674291432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001389312674291432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138931
Iteration 2/1000 | Loss: 0.00049144
Iteration 3/1000 | Loss: 0.00027602
Iteration 4/1000 | Loss: 0.00023266
Iteration 5/1000 | Loss: 0.00021661
Iteration 6/1000 | Loss: 0.00048375
Iteration 7/1000 | Loss: 0.00012194
Iteration 8/1000 | Loss: 0.00011782
Iteration 9/1000 | Loss: 0.00098572
Iteration 10/1000 | Loss: 0.00241632
Iteration 11/1000 | Loss: 0.00107204
Iteration 12/1000 | Loss: 0.00010801
Iteration 13/1000 | Loss: 0.00240255
Iteration 14/1000 | Loss: 0.00028026
Iteration 15/1000 | Loss: 0.00011787
Iteration 16/1000 | Loss: 0.00007413
Iteration 17/1000 | Loss: 0.00006351
Iteration 18/1000 | Loss: 0.00005808
Iteration 19/1000 | Loss: 0.00010676
Iteration 20/1000 | Loss: 0.00007756
Iteration 21/1000 | Loss: 0.00005279
Iteration 22/1000 | Loss: 0.00006606
Iteration 23/1000 | Loss: 0.00006258
Iteration 24/1000 | Loss: 0.00005003
Iteration 25/1000 | Loss: 0.00006105
Iteration 26/1000 | Loss: 0.00005199
Iteration 27/1000 | Loss: 0.00004886
Iteration 28/1000 | Loss: 0.00005714
Iteration 29/1000 | Loss: 0.00009460
Iteration 30/1000 | Loss: 0.00005358
Iteration 31/1000 | Loss: 0.00006212
Iteration 32/1000 | Loss: 0.00016991
Iteration 33/1000 | Loss: 0.00005333
Iteration 34/1000 | Loss: 0.00004921
Iteration 35/1000 | Loss: 0.00004772
Iteration 36/1000 | Loss: 0.00004766
Iteration 37/1000 | Loss: 0.00004766
Iteration 38/1000 | Loss: 0.00004766
Iteration 39/1000 | Loss: 0.00004766
Iteration 40/1000 | Loss: 0.00004765
Iteration 41/1000 | Loss: 0.00004765
Iteration 42/1000 | Loss: 0.00004765
Iteration 43/1000 | Loss: 0.00004765
Iteration 44/1000 | Loss: 0.00004765
Iteration 45/1000 | Loss: 0.00004765
Iteration 46/1000 | Loss: 0.00004764
Iteration 47/1000 | Loss: 0.00004764
Iteration 48/1000 | Loss: 0.00004884
Iteration 49/1000 | Loss: 0.00004758
Iteration 50/1000 | Loss: 0.00004753
Iteration 51/1000 | Loss: 0.00004753
Iteration 52/1000 | Loss: 0.00004752
Iteration 53/1000 | Loss: 0.00004752
Iteration 54/1000 | Loss: 0.00004751
Iteration 55/1000 | Loss: 0.00004751
Iteration 56/1000 | Loss: 0.00004750
Iteration 57/1000 | Loss: 0.00004806
Iteration 58/1000 | Loss: 0.00009061
Iteration 59/1000 | Loss: 0.00007031
Iteration 60/1000 | Loss: 0.00005950
Iteration 61/1000 | Loss: 0.00010108
Iteration 62/1000 | Loss: 0.00004756
Iteration 63/1000 | Loss: 0.00004740
Iteration 64/1000 | Loss: 0.00004740
Iteration 65/1000 | Loss: 0.00004740
Iteration 66/1000 | Loss: 0.00004739
Iteration 67/1000 | Loss: 0.00004739
Iteration 68/1000 | Loss: 0.00004739
Iteration 69/1000 | Loss: 0.00004739
Iteration 70/1000 | Loss: 0.00004739
Iteration 71/1000 | Loss: 0.00004739
Iteration 72/1000 | Loss: 0.00004739
Iteration 73/1000 | Loss: 0.00004739
Iteration 74/1000 | Loss: 0.00004739
Iteration 75/1000 | Loss: 0.00004739
Iteration 76/1000 | Loss: 0.00004739
Iteration 77/1000 | Loss: 0.00004739
Iteration 78/1000 | Loss: 0.00004739
Iteration 79/1000 | Loss: 0.00004738
Iteration 80/1000 | Loss: 0.00004738
Iteration 81/1000 | Loss: 0.00004737
Iteration 82/1000 | Loss: 0.00004737
Iteration 83/1000 | Loss: 0.00004737
Iteration 84/1000 | Loss: 0.00004737
Iteration 85/1000 | Loss: 0.00004737
Iteration 86/1000 | Loss: 0.00004737
Iteration 87/1000 | Loss: 0.00004736
Iteration 88/1000 | Loss: 0.00004736
Iteration 89/1000 | Loss: 0.00004736
Iteration 90/1000 | Loss: 0.00004736
Iteration 91/1000 | Loss: 0.00004736
Iteration 92/1000 | Loss: 0.00004735
Iteration 93/1000 | Loss: 0.00004735
Iteration 94/1000 | Loss: 0.00004735
Iteration 95/1000 | Loss: 0.00004735
Iteration 96/1000 | Loss: 0.00004734
Iteration 97/1000 | Loss: 0.00004734
Iteration 98/1000 | Loss: 0.00004734
Iteration 99/1000 | Loss: 0.00004734
Iteration 100/1000 | Loss: 0.00004733
Iteration 101/1000 | Loss: 0.00004733
Iteration 102/1000 | Loss: 0.00004733
Iteration 103/1000 | Loss: 0.00004733
Iteration 104/1000 | Loss: 0.00004733
Iteration 105/1000 | Loss: 0.00004733
Iteration 106/1000 | Loss: 0.00004732
Iteration 107/1000 | Loss: 0.00004732
Iteration 108/1000 | Loss: 0.00004732
Iteration 109/1000 | Loss: 0.00004732
Iteration 110/1000 | Loss: 0.00004731
Iteration 111/1000 | Loss: 0.00004731
Iteration 112/1000 | Loss: 0.00004731
Iteration 113/1000 | Loss: 0.00004731
Iteration 114/1000 | Loss: 0.00004731
Iteration 115/1000 | Loss: 0.00004731
Iteration 116/1000 | Loss: 0.00004731
Iteration 117/1000 | Loss: 0.00004731
Iteration 118/1000 | Loss: 0.00004731
Iteration 119/1000 | Loss: 0.00004731
Iteration 120/1000 | Loss: 0.00004731
Iteration 121/1000 | Loss: 0.00004731
Iteration 122/1000 | Loss: 0.00004731
Iteration 123/1000 | Loss: 0.00004731
Iteration 124/1000 | Loss: 0.00004730
Iteration 125/1000 | Loss: 0.00004730
Iteration 126/1000 | Loss: 0.00004730
Iteration 127/1000 | Loss: 0.00004730
Iteration 128/1000 | Loss: 0.00004730
Iteration 129/1000 | Loss: 0.00004730
Iteration 130/1000 | Loss: 0.00004729
Iteration 131/1000 | Loss: 0.00004729
Iteration 132/1000 | Loss: 0.00004729
Iteration 133/1000 | Loss: 0.00004729
Iteration 134/1000 | Loss: 0.00004729
Iteration 135/1000 | Loss: 0.00004729
Iteration 136/1000 | Loss: 0.00004729
Iteration 137/1000 | Loss: 0.00004729
Iteration 138/1000 | Loss: 0.00004729
Iteration 139/1000 | Loss: 0.00004729
Iteration 140/1000 | Loss: 0.00004729
Iteration 141/1000 | Loss: 0.00004729
Iteration 142/1000 | Loss: 0.00004729
Iteration 143/1000 | Loss: 0.00004729
Iteration 144/1000 | Loss: 0.00004729
Iteration 145/1000 | Loss: 0.00004729
Iteration 146/1000 | Loss: 0.00004729
Iteration 147/1000 | Loss: 0.00004729
Iteration 148/1000 | Loss: 0.00004729
Iteration 149/1000 | Loss: 0.00004729
Iteration 150/1000 | Loss: 0.00004729
Iteration 151/1000 | Loss: 0.00004729
Iteration 152/1000 | Loss: 0.00004729
Iteration 153/1000 | Loss: 0.00004729
Iteration 154/1000 | Loss: 0.00004729
Iteration 155/1000 | Loss: 0.00004729
Iteration 156/1000 | Loss: 0.00004729
Iteration 157/1000 | Loss: 0.00004729
Iteration 158/1000 | Loss: 0.00004729
Iteration 159/1000 | Loss: 0.00004729
Iteration 160/1000 | Loss: 0.00004729
Iteration 161/1000 | Loss: 0.00004729
Iteration 162/1000 | Loss: 0.00004729
Iteration 163/1000 | Loss: 0.00004729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [4.7291512601077557e-05, 4.7291512601077557e-05, 4.7291512601077557e-05, 4.7291512601077557e-05, 4.7291512601077557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7291512601077557e-05

Optimization complete. Final v2v error: 5.691175937652588 mm

Highest mean error: 7.303771018981934 mm for frame 7

Lowest mean error: 5.233022689819336 mm for frame 30

Saving results

Total time: 118.11623644828796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029237
Iteration 2/25 | Loss: 0.01029237
Iteration 3/25 | Loss: 0.00266428
Iteration 4/25 | Loss: 0.00216959
Iteration 5/25 | Loss: 0.00170890
Iteration 6/25 | Loss: 0.00149036
Iteration 7/25 | Loss: 0.00146825
Iteration 8/25 | Loss: 0.00145556
Iteration 9/25 | Loss: 0.00137378
Iteration 10/25 | Loss: 0.00129206
Iteration 11/25 | Loss: 0.00129628
Iteration 12/25 | Loss: 0.00128181
Iteration 13/25 | Loss: 0.00117973
Iteration 14/25 | Loss: 0.00119959
Iteration 15/25 | Loss: 0.00111644
Iteration 16/25 | Loss: 0.00111437
Iteration 17/25 | Loss: 0.00111636
Iteration 18/25 | Loss: 0.00108619
Iteration 19/25 | Loss: 0.00105348
Iteration 20/25 | Loss: 0.00103226
Iteration 21/25 | Loss: 0.00102267
Iteration 22/25 | Loss: 0.00101935
Iteration 23/25 | Loss: 0.00099717
Iteration 24/25 | Loss: 0.00098884
Iteration 25/25 | Loss: 0.00098272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74160373
Iteration 2/25 | Loss: 0.00328739
Iteration 3/25 | Loss: 0.00202398
Iteration 4/25 | Loss: 0.00202397
Iteration 5/25 | Loss: 0.00202397
Iteration 6/25 | Loss: 0.00202397
Iteration 7/25 | Loss: 0.00202397
Iteration 8/25 | Loss: 0.00202397
Iteration 9/25 | Loss: 0.00202397
Iteration 10/25 | Loss: 0.00202397
Iteration 11/25 | Loss: 0.00202397
Iteration 12/25 | Loss: 0.00202397
Iteration 13/25 | Loss: 0.00202397
Iteration 14/25 | Loss: 0.00202397
Iteration 15/25 | Loss: 0.00202397
Iteration 16/25 | Loss: 0.00202397
Iteration 17/25 | Loss: 0.00202397
Iteration 18/25 | Loss: 0.00202397
Iteration 19/25 | Loss: 0.00202397
Iteration 20/25 | Loss: 0.00202397
Iteration 21/25 | Loss: 0.00202397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002023969078436494, 0.002023969078436494, 0.002023969078436494, 0.002023969078436494, 0.002023969078436494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002023969078436494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202397
Iteration 2/1000 | Loss: 0.00142639
Iteration 3/1000 | Loss: 0.00066704
Iteration 4/1000 | Loss: 0.00119642
Iteration 5/1000 | Loss: 0.00098268
Iteration 6/1000 | Loss: 0.00137170
Iteration 7/1000 | Loss: 0.00092535
Iteration 8/1000 | Loss: 0.00068973
Iteration 9/1000 | Loss: 0.00072655
Iteration 10/1000 | Loss: 0.00058254
Iteration 11/1000 | Loss: 0.00053931
Iteration 12/1000 | Loss: 0.00029530
Iteration 13/1000 | Loss: 0.00036272
Iteration 14/1000 | Loss: 0.00116534
Iteration 15/1000 | Loss: 0.00057980
Iteration 16/1000 | Loss: 0.00037964
Iteration 17/1000 | Loss: 0.00030088
Iteration 18/1000 | Loss: 0.00081814
Iteration 19/1000 | Loss: 0.00016776
Iteration 20/1000 | Loss: 0.00030230
Iteration 21/1000 | Loss: 0.00035390
Iteration 22/1000 | Loss: 0.00017328
Iteration 23/1000 | Loss: 0.00011678
Iteration 24/1000 | Loss: 0.00049243
Iteration 25/1000 | Loss: 0.00075105
Iteration 26/1000 | Loss: 0.00074371
Iteration 27/1000 | Loss: 0.00020422
Iteration 28/1000 | Loss: 0.00024291
Iteration 29/1000 | Loss: 0.00031830
Iteration 30/1000 | Loss: 0.00017295
Iteration 31/1000 | Loss: 0.00013041
Iteration 32/1000 | Loss: 0.00028618
Iteration 33/1000 | Loss: 0.00020166
Iteration 34/1000 | Loss: 0.00111301
Iteration 35/1000 | Loss: 0.00091543
Iteration 36/1000 | Loss: 0.00043038
Iteration 37/1000 | Loss: 0.00042752
Iteration 38/1000 | Loss: 0.00042023
Iteration 39/1000 | Loss: 0.00022089
Iteration 40/1000 | Loss: 0.00015464
Iteration 41/1000 | Loss: 0.00050610
Iteration 42/1000 | Loss: 0.00025506
Iteration 43/1000 | Loss: 0.00011692
Iteration 44/1000 | Loss: 0.00012603
Iteration 45/1000 | Loss: 0.00021589
Iteration 46/1000 | Loss: 0.00011056
Iteration 47/1000 | Loss: 0.00009674
Iteration 48/1000 | Loss: 0.00018973
Iteration 49/1000 | Loss: 0.00012370
Iteration 50/1000 | Loss: 0.00010411
Iteration 51/1000 | Loss: 0.00018295
Iteration 52/1000 | Loss: 0.00009521
Iteration 53/1000 | Loss: 0.00062322
Iteration 54/1000 | Loss: 0.00042302
Iteration 55/1000 | Loss: 0.00057646
Iteration 56/1000 | Loss: 0.00051229
Iteration 57/1000 | Loss: 0.00023751
Iteration 58/1000 | Loss: 0.00052574
Iteration 59/1000 | Loss: 0.00013891
Iteration 60/1000 | Loss: 0.00015087
Iteration 61/1000 | Loss: 0.00017061
Iteration 62/1000 | Loss: 0.00022387
Iteration 63/1000 | Loss: 0.00020030
Iteration 64/1000 | Loss: 0.00020296
Iteration 65/1000 | Loss: 0.00052690
Iteration 66/1000 | Loss: 0.00018471
Iteration 67/1000 | Loss: 0.00014939
Iteration 68/1000 | Loss: 0.00010651
Iteration 69/1000 | Loss: 0.00012466
Iteration 70/1000 | Loss: 0.00016240
Iteration 71/1000 | Loss: 0.00013743
Iteration 72/1000 | Loss: 0.00014615
Iteration 73/1000 | Loss: 0.00015130
Iteration 74/1000 | Loss: 0.00016221
Iteration 75/1000 | Loss: 0.00016631
Iteration 76/1000 | Loss: 0.00017238
Iteration 77/1000 | Loss: 0.00025260
Iteration 78/1000 | Loss: 0.00018899
Iteration 79/1000 | Loss: 0.00027741
Iteration 80/1000 | Loss: 0.00009860
Iteration 81/1000 | Loss: 0.00021587
Iteration 82/1000 | Loss: 0.00015092
Iteration 83/1000 | Loss: 0.00014427
Iteration 84/1000 | Loss: 0.00018882
Iteration 85/1000 | Loss: 0.00023493
Iteration 86/1000 | Loss: 0.00014270
Iteration 87/1000 | Loss: 0.00014506
Iteration 88/1000 | Loss: 0.00082567
Iteration 89/1000 | Loss: 0.00037799
Iteration 90/1000 | Loss: 0.00010097
Iteration 91/1000 | Loss: 0.00024186
Iteration 92/1000 | Loss: 0.00021420
Iteration 93/1000 | Loss: 0.00025303
Iteration 94/1000 | Loss: 0.00034469
Iteration 95/1000 | Loss: 0.00028895
Iteration 96/1000 | Loss: 0.00013802
Iteration 97/1000 | Loss: 0.00048097
Iteration 98/1000 | Loss: 0.00034122
Iteration 99/1000 | Loss: 0.00045444
Iteration 100/1000 | Loss: 0.00042005
Iteration 101/1000 | Loss: 0.00043570
Iteration 102/1000 | Loss: 0.00011200
Iteration 103/1000 | Loss: 0.00010664
Iteration 104/1000 | Loss: 0.00019896
Iteration 105/1000 | Loss: 0.00075045
Iteration 106/1000 | Loss: 0.00142799
Iteration 107/1000 | Loss: 0.00051848
Iteration 108/1000 | Loss: 0.00015425
Iteration 109/1000 | Loss: 0.00016400
Iteration 110/1000 | Loss: 0.00039948
Iteration 111/1000 | Loss: 0.00020737
Iteration 112/1000 | Loss: 0.00019677
Iteration 113/1000 | Loss: 0.00073202
Iteration 114/1000 | Loss: 0.00045071
Iteration 115/1000 | Loss: 0.00012396
Iteration 116/1000 | Loss: 0.00013755
Iteration 117/1000 | Loss: 0.00019736
Iteration 118/1000 | Loss: 0.00052943
Iteration 119/1000 | Loss: 0.00110891
Iteration 120/1000 | Loss: 0.00077497
Iteration 121/1000 | Loss: 0.00091639
Iteration 122/1000 | Loss: 0.00076977
Iteration 123/1000 | Loss: 0.00088046
Iteration 124/1000 | Loss: 0.00058757
Iteration 125/1000 | Loss: 0.00067515
Iteration 126/1000 | Loss: 0.00035985
Iteration 127/1000 | Loss: 0.00052824
Iteration 128/1000 | Loss: 0.00052717
Iteration 129/1000 | Loss: 0.00035170
Iteration 130/1000 | Loss: 0.00011211
Iteration 131/1000 | Loss: 0.00035273
Iteration 132/1000 | Loss: 0.00034956
Iteration 133/1000 | Loss: 0.00020351
Iteration 134/1000 | Loss: 0.00026058
Iteration 135/1000 | Loss: 0.00030469
Iteration 136/1000 | Loss: 0.00034511
Iteration 137/1000 | Loss: 0.00016489
Iteration 138/1000 | Loss: 0.00008358
Iteration 139/1000 | Loss: 0.00020466
Iteration 140/1000 | Loss: 0.00015480
Iteration 141/1000 | Loss: 0.00079216
Iteration 142/1000 | Loss: 0.00063236
Iteration 143/1000 | Loss: 0.00073354
Iteration 144/1000 | Loss: 0.00044513
Iteration 145/1000 | Loss: 0.00066213
Iteration 146/1000 | Loss: 0.00024370
Iteration 147/1000 | Loss: 0.00082393
Iteration 148/1000 | Loss: 0.00026002
Iteration 149/1000 | Loss: 0.00042806
Iteration 150/1000 | Loss: 0.00015743
Iteration 151/1000 | Loss: 0.00020478
Iteration 152/1000 | Loss: 0.00016567
Iteration 153/1000 | Loss: 0.00019947
Iteration 154/1000 | Loss: 0.00016388
Iteration 155/1000 | Loss: 0.00026147
Iteration 156/1000 | Loss: 0.00014863
Iteration 157/1000 | Loss: 0.00081711
Iteration 158/1000 | Loss: 0.00042769
Iteration 159/1000 | Loss: 0.00061498
Iteration 160/1000 | Loss: 0.00120051
Iteration 161/1000 | Loss: 0.00099648
Iteration 162/1000 | Loss: 0.00107034
Iteration 163/1000 | Loss: 0.00045615
Iteration 164/1000 | Loss: 0.00024434
Iteration 165/1000 | Loss: 0.00020635
Iteration 166/1000 | Loss: 0.00012034
Iteration 167/1000 | Loss: 0.00056791
Iteration 168/1000 | Loss: 0.00023552
Iteration 169/1000 | Loss: 0.00016130
Iteration 170/1000 | Loss: 0.00017235
Iteration 171/1000 | Loss: 0.00013848
Iteration 172/1000 | Loss: 0.00016217
Iteration 173/1000 | Loss: 0.00011883
Iteration 174/1000 | Loss: 0.00018198
Iteration 175/1000 | Loss: 0.00010441
Iteration 176/1000 | Loss: 0.00015987
Iteration 177/1000 | Loss: 0.00012354
Iteration 178/1000 | Loss: 0.00027786
Iteration 179/1000 | Loss: 0.00017481
Iteration 180/1000 | Loss: 0.00010909
Iteration 181/1000 | Loss: 0.00014947
Iteration 182/1000 | Loss: 0.00012467
Iteration 183/1000 | Loss: 0.00023171
Iteration 184/1000 | Loss: 0.00014312
Iteration 185/1000 | Loss: 0.00018508
Iteration 186/1000 | Loss: 0.00028337
Iteration 187/1000 | Loss: 0.00008639
Iteration 188/1000 | Loss: 0.00007835
Iteration 189/1000 | Loss: 0.00007312
Iteration 190/1000 | Loss: 0.00018524
Iteration 191/1000 | Loss: 0.00007017
Iteration 192/1000 | Loss: 0.00006943
Iteration 193/1000 | Loss: 0.00019106
Iteration 194/1000 | Loss: 0.00012759
Iteration 195/1000 | Loss: 0.00016280
Iteration 196/1000 | Loss: 0.00010754
Iteration 197/1000 | Loss: 0.00008069
Iteration 198/1000 | Loss: 0.00006849
Iteration 199/1000 | Loss: 0.00006800
Iteration 200/1000 | Loss: 0.00036297
Iteration 201/1000 | Loss: 0.00022771
Iteration 202/1000 | Loss: 0.00008064
Iteration 203/1000 | Loss: 0.00127375
Iteration 204/1000 | Loss: 0.00080524
Iteration 205/1000 | Loss: 0.00009315
Iteration 206/1000 | Loss: 0.00006858
Iteration 207/1000 | Loss: 0.00052758
Iteration 208/1000 | Loss: 0.00019748
Iteration 209/1000 | Loss: 0.00007697
Iteration 210/1000 | Loss: 0.00007137
Iteration 211/1000 | Loss: 0.00006854
Iteration 212/1000 | Loss: 0.00006795
Iteration 213/1000 | Loss: 0.00056008
Iteration 214/1000 | Loss: 0.00008471
Iteration 215/1000 | Loss: 0.00007183
Iteration 216/1000 | Loss: 0.00007000
Iteration 217/1000 | Loss: 0.00087248
Iteration 218/1000 | Loss: 0.00007886
Iteration 219/1000 | Loss: 0.00007286
Iteration 220/1000 | Loss: 0.00006606
Iteration 221/1000 | Loss: 0.00082683
Iteration 222/1000 | Loss: 0.00022295
Iteration 223/1000 | Loss: 0.00035688
Iteration 224/1000 | Loss: 0.00019757
Iteration 225/1000 | Loss: 0.00033393
Iteration 226/1000 | Loss: 0.00007304
Iteration 227/1000 | Loss: 0.00006933
Iteration 228/1000 | Loss: 0.00006586
Iteration 229/1000 | Loss: 0.00006486
Iteration 230/1000 | Loss: 0.00006379
Iteration 231/1000 | Loss: 0.00006244
Iteration 232/1000 | Loss: 0.00006154
Iteration 233/1000 | Loss: 0.00006080
Iteration 234/1000 | Loss: 0.00022227
Iteration 235/1000 | Loss: 0.00019261
Iteration 236/1000 | Loss: 0.00006056
Iteration 237/1000 | Loss: 0.00006032
Iteration 238/1000 | Loss: 0.00006025
Iteration 239/1000 | Loss: 0.00019441
Iteration 240/1000 | Loss: 0.00019090
Iteration 241/1000 | Loss: 0.00008111
Iteration 242/1000 | Loss: 0.00030128
Iteration 243/1000 | Loss: 0.00007319
Iteration 244/1000 | Loss: 0.00013747
Iteration 245/1000 | Loss: 0.00006524
Iteration 246/1000 | Loss: 0.00006280
Iteration 247/1000 | Loss: 0.00006145
Iteration 248/1000 | Loss: 0.00019818
Iteration 249/1000 | Loss: 0.00006987
Iteration 250/1000 | Loss: 0.00006402
Iteration 251/1000 | Loss: 0.00006058
Iteration 252/1000 | Loss: 0.00021197
Iteration 253/1000 | Loss: 0.00006053
Iteration 254/1000 | Loss: 0.00005997
Iteration 255/1000 | Loss: 0.00005975
Iteration 256/1000 | Loss: 0.00005959
Iteration 257/1000 | Loss: 0.00005958
Iteration 258/1000 | Loss: 0.00005957
Iteration 259/1000 | Loss: 0.00005956
Iteration 260/1000 | Loss: 0.00005939
Iteration 261/1000 | Loss: 0.00005931
Iteration 262/1000 | Loss: 0.00005929
Iteration 263/1000 | Loss: 0.00005915
Iteration 264/1000 | Loss: 0.00005910
Iteration 265/1000 | Loss: 0.00005902
Iteration 266/1000 | Loss: 0.00005901
Iteration 267/1000 | Loss: 0.00005900
Iteration 268/1000 | Loss: 0.00005900
Iteration 269/1000 | Loss: 0.00005899
Iteration 270/1000 | Loss: 0.00005898
Iteration 271/1000 | Loss: 0.00005897
Iteration 272/1000 | Loss: 0.00005897
Iteration 273/1000 | Loss: 0.00005893
Iteration 274/1000 | Loss: 0.00005891
Iteration 275/1000 | Loss: 0.00005891
Iteration 276/1000 | Loss: 0.00005891
Iteration 277/1000 | Loss: 0.00005891
Iteration 278/1000 | Loss: 0.00005891
Iteration 279/1000 | Loss: 0.00005891
Iteration 280/1000 | Loss: 0.00005891
Iteration 281/1000 | Loss: 0.00005890
Iteration 282/1000 | Loss: 0.00005890
Iteration 283/1000 | Loss: 0.00005890
Iteration 284/1000 | Loss: 0.00005890
Iteration 285/1000 | Loss: 0.00005890
Iteration 286/1000 | Loss: 0.00005890
Iteration 287/1000 | Loss: 0.00005890
Iteration 288/1000 | Loss: 0.00005890
Iteration 289/1000 | Loss: 0.00005890
Iteration 290/1000 | Loss: 0.00005890
Iteration 291/1000 | Loss: 0.00005890
Iteration 292/1000 | Loss: 0.00005889
Iteration 293/1000 | Loss: 0.00005889
Iteration 294/1000 | Loss: 0.00005889
Iteration 295/1000 | Loss: 0.00005889
Iteration 296/1000 | Loss: 0.00005888
Iteration 297/1000 | Loss: 0.00005887
Iteration 298/1000 | Loss: 0.00005887
Iteration 299/1000 | Loss: 0.00005887
Iteration 300/1000 | Loss: 0.00005887
Iteration 301/1000 | Loss: 0.00005887
Iteration 302/1000 | Loss: 0.00005887
Iteration 303/1000 | Loss: 0.00005887
Iteration 304/1000 | Loss: 0.00005887
Iteration 305/1000 | Loss: 0.00005887
Iteration 306/1000 | Loss: 0.00005887
Iteration 307/1000 | Loss: 0.00005886
Iteration 308/1000 | Loss: 0.00005886
Iteration 309/1000 | Loss: 0.00005886
Iteration 310/1000 | Loss: 0.00005886
Iteration 311/1000 | Loss: 0.00005885
Iteration 312/1000 | Loss: 0.00005885
Iteration 313/1000 | Loss: 0.00005885
Iteration 314/1000 | Loss: 0.00005885
Iteration 315/1000 | Loss: 0.00005885
Iteration 316/1000 | Loss: 0.00005885
Iteration 317/1000 | Loss: 0.00005884
Iteration 318/1000 | Loss: 0.00005884
Iteration 319/1000 | Loss: 0.00005884
Iteration 320/1000 | Loss: 0.00005883
Iteration 321/1000 | Loss: 0.00005883
Iteration 322/1000 | Loss: 0.00005883
Iteration 323/1000 | Loss: 0.00005883
Iteration 324/1000 | Loss: 0.00005882
Iteration 325/1000 | Loss: 0.00005882
Iteration 326/1000 | Loss: 0.00005882
Iteration 327/1000 | Loss: 0.00005882
Iteration 328/1000 | Loss: 0.00005881
Iteration 329/1000 | Loss: 0.00005881
Iteration 330/1000 | Loss: 0.00005881
Iteration 331/1000 | Loss: 0.00005881
Iteration 332/1000 | Loss: 0.00005881
Iteration 333/1000 | Loss: 0.00005881
Iteration 334/1000 | Loss: 0.00005880
Iteration 335/1000 | Loss: 0.00005880
Iteration 336/1000 | Loss: 0.00005880
Iteration 337/1000 | Loss: 0.00005880
Iteration 338/1000 | Loss: 0.00005880
Iteration 339/1000 | Loss: 0.00005880
Iteration 340/1000 | Loss: 0.00005880
Iteration 341/1000 | Loss: 0.00005880
Iteration 342/1000 | Loss: 0.00005880
Iteration 343/1000 | Loss: 0.00005880
Iteration 344/1000 | Loss: 0.00005880
Iteration 345/1000 | Loss: 0.00005880
Iteration 346/1000 | Loss: 0.00005880
Iteration 347/1000 | Loss: 0.00005880
Iteration 348/1000 | Loss: 0.00005880
Iteration 349/1000 | Loss: 0.00005880
Iteration 350/1000 | Loss: 0.00005880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 350. Stopping optimization.
Last 5 losses: [5.87990834901575e-05, 5.87990834901575e-05, 5.87990834901575e-05, 5.87990834901575e-05, 5.87990834901575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.87990834901575e-05

Optimization complete. Final v2v error: 4.707139492034912 mm

Highest mean error: 13.34708309173584 mm for frame 18

Lowest mean error: 3.2716004848480225 mm for frame 45

Saving results

Total time: 476.9869749546051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816968
Iteration 2/25 | Loss: 0.00107088
Iteration 3/25 | Loss: 0.00076734
Iteration 4/25 | Loss: 0.00073282
Iteration 5/25 | Loss: 0.00072066
Iteration 6/25 | Loss: 0.00071809
Iteration 7/25 | Loss: 0.00071758
Iteration 8/25 | Loss: 0.00071758
Iteration 9/25 | Loss: 0.00071758
Iteration 10/25 | Loss: 0.00071758
Iteration 11/25 | Loss: 0.00071758
Iteration 12/25 | Loss: 0.00071758
Iteration 13/25 | Loss: 0.00071758
Iteration 14/25 | Loss: 0.00071758
Iteration 15/25 | Loss: 0.00071758
Iteration 16/25 | Loss: 0.00071758
Iteration 17/25 | Loss: 0.00071758
Iteration 18/25 | Loss: 0.00071758
Iteration 19/25 | Loss: 0.00071758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007175779901444912, 0.0007175779901444912, 0.0007175779901444912, 0.0007175779901444912, 0.0007175779901444912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007175779901444912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24409783
Iteration 2/25 | Loss: 0.00022085
Iteration 3/25 | Loss: 0.00022085
Iteration 4/25 | Loss: 0.00022085
Iteration 5/25 | Loss: 0.00022085
Iteration 6/25 | Loss: 0.00022085
Iteration 7/25 | Loss: 0.00022085
Iteration 8/25 | Loss: 0.00022085
Iteration 9/25 | Loss: 0.00022085
Iteration 10/25 | Loss: 0.00022085
Iteration 11/25 | Loss: 0.00022085
Iteration 12/25 | Loss: 0.00022085
Iteration 13/25 | Loss: 0.00022085
Iteration 14/25 | Loss: 0.00022085
Iteration 15/25 | Loss: 0.00022085
Iteration 16/25 | Loss: 0.00022085
Iteration 17/25 | Loss: 0.00022085
Iteration 18/25 | Loss: 0.00022085
Iteration 19/25 | Loss: 0.00022085
Iteration 20/25 | Loss: 0.00022085
Iteration 21/25 | Loss: 0.00022085
Iteration 22/25 | Loss: 0.00022085
Iteration 23/25 | Loss: 0.00022085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00022084767988417298, 0.00022084767988417298, 0.00022084767988417298, 0.00022084767988417298, 0.00022084767988417298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022084767988417298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022085
Iteration 2/1000 | Loss: 0.00003889
Iteration 3/1000 | Loss: 0.00002618
Iteration 4/1000 | Loss: 0.00002296
Iteration 5/1000 | Loss: 0.00002163
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00001971
Iteration 8/1000 | Loss: 0.00001918
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001861
Iteration 11/1000 | Loss: 0.00001842
Iteration 12/1000 | Loss: 0.00001833
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001832
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001819
Iteration 17/1000 | Loss: 0.00001819
Iteration 18/1000 | Loss: 0.00001813
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001800
Iteration 22/1000 | Loss: 0.00001800
Iteration 23/1000 | Loss: 0.00001796
Iteration 24/1000 | Loss: 0.00001795
Iteration 25/1000 | Loss: 0.00001795
Iteration 26/1000 | Loss: 0.00001795
Iteration 27/1000 | Loss: 0.00001795
Iteration 28/1000 | Loss: 0.00001795
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001794
Iteration 31/1000 | Loss: 0.00001794
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001784
Iteration 47/1000 | Loss: 0.00001784
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001783
Iteration 51/1000 | Loss: 0.00001783
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001782
Iteration 59/1000 | Loss: 0.00001782
Iteration 60/1000 | Loss: 0.00001782
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001781
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001775
Iteration 93/1000 | Loss: 0.00001775
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001770
Iteration 102/1000 | Loss: 0.00001769
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001766
Iteration 108/1000 | Loss: 0.00001766
Iteration 109/1000 | Loss: 0.00001766
Iteration 110/1000 | Loss: 0.00001766
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001765
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001765
Iteration 120/1000 | Loss: 0.00001765
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001764
Iteration 123/1000 | Loss: 0.00001764
Iteration 124/1000 | Loss: 0.00001764
Iteration 125/1000 | Loss: 0.00001763
Iteration 126/1000 | Loss: 0.00001763
Iteration 127/1000 | Loss: 0.00001763
Iteration 128/1000 | Loss: 0.00001763
Iteration 129/1000 | Loss: 0.00001763
Iteration 130/1000 | Loss: 0.00001762
Iteration 131/1000 | Loss: 0.00001762
Iteration 132/1000 | Loss: 0.00001762
Iteration 133/1000 | Loss: 0.00001762
Iteration 134/1000 | Loss: 0.00001761
Iteration 135/1000 | Loss: 0.00001761
Iteration 136/1000 | Loss: 0.00001761
Iteration 137/1000 | Loss: 0.00001760
Iteration 138/1000 | Loss: 0.00001760
Iteration 139/1000 | Loss: 0.00001760
Iteration 140/1000 | Loss: 0.00001760
Iteration 141/1000 | Loss: 0.00001760
Iteration 142/1000 | Loss: 0.00001760
Iteration 143/1000 | Loss: 0.00001760
Iteration 144/1000 | Loss: 0.00001759
Iteration 145/1000 | Loss: 0.00001759
Iteration 146/1000 | Loss: 0.00001759
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001759
Iteration 151/1000 | Loss: 0.00001759
Iteration 152/1000 | Loss: 0.00001759
Iteration 153/1000 | Loss: 0.00001759
Iteration 154/1000 | Loss: 0.00001759
Iteration 155/1000 | Loss: 0.00001759
Iteration 156/1000 | Loss: 0.00001758
Iteration 157/1000 | Loss: 0.00001758
Iteration 158/1000 | Loss: 0.00001758
Iteration 159/1000 | Loss: 0.00001758
Iteration 160/1000 | Loss: 0.00001758
Iteration 161/1000 | Loss: 0.00001758
Iteration 162/1000 | Loss: 0.00001758
Iteration 163/1000 | Loss: 0.00001758
Iteration 164/1000 | Loss: 0.00001758
Iteration 165/1000 | Loss: 0.00001758
Iteration 166/1000 | Loss: 0.00001758
Iteration 167/1000 | Loss: 0.00001758
Iteration 168/1000 | Loss: 0.00001758
Iteration 169/1000 | Loss: 0.00001758
Iteration 170/1000 | Loss: 0.00001758
Iteration 171/1000 | Loss: 0.00001758
Iteration 172/1000 | Loss: 0.00001758
Iteration 173/1000 | Loss: 0.00001758
Iteration 174/1000 | Loss: 0.00001758
Iteration 175/1000 | Loss: 0.00001758
Iteration 176/1000 | Loss: 0.00001758
Iteration 177/1000 | Loss: 0.00001758
Iteration 178/1000 | Loss: 0.00001758
Iteration 179/1000 | Loss: 0.00001758
Iteration 180/1000 | Loss: 0.00001758
Iteration 181/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.757595600793138e-05, 1.757595600793138e-05, 1.757595600793138e-05, 1.757595600793138e-05, 1.757595600793138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.757595600793138e-05

Optimization complete. Final v2v error: 3.4988880157470703 mm

Highest mean error: 5.096404075622559 mm for frame 69

Lowest mean error: 2.8392856121063232 mm for frame 101

Saving results

Total time: 43.137590169906616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813562
Iteration 2/25 | Loss: 0.00085597
Iteration 3/25 | Loss: 0.00070594
Iteration 4/25 | Loss: 0.00068970
Iteration 5/25 | Loss: 0.00068468
Iteration 6/25 | Loss: 0.00068363
Iteration 7/25 | Loss: 0.00068356
Iteration 8/25 | Loss: 0.00068355
Iteration 9/25 | Loss: 0.00068355
Iteration 10/25 | Loss: 0.00068355
Iteration 11/25 | Loss: 0.00068355
Iteration 12/25 | Loss: 0.00068355
Iteration 13/25 | Loss: 0.00068355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006835472886450589, 0.0006835472886450589, 0.0006835472886450589, 0.0006835472886450589, 0.0006835472886450589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006835472886450589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46417189
Iteration 2/25 | Loss: 0.00020309
Iteration 3/25 | Loss: 0.00020308
Iteration 4/25 | Loss: 0.00020308
Iteration 5/25 | Loss: 0.00020308
Iteration 6/25 | Loss: 0.00020308
Iteration 7/25 | Loss: 0.00020308
Iteration 8/25 | Loss: 0.00020308
Iteration 9/25 | Loss: 0.00020308
Iteration 10/25 | Loss: 0.00020308
Iteration 11/25 | Loss: 0.00020308
Iteration 12/25 | Loss: 0.00020308
Iteration 13/25 | Loss: 0.00020308
Iteration 14/25 | Loss: 0.00020308
Iteration 15/25 | Loss: 0.00020308
Iteration 16/25 | Loss: 0.00020308
Iteration 17/25 | Loss: 0.00020308
Iteration 18/25 | Loss: 0.00020308
Iteration 19/25 | Loss: 0.00020308
Iteration 20/25 | Loss: 0.00020308
Iteration 21/25 | Loss: 0.00020308
Iteration 22/25 | Loss: 0.00020308
Iteration 23/25 | Loss: 0.00020308
Iteration 24/25 | Loss: 0.00020308
Iteration 25/25 | Loss: 0.00020308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020308
Iteration 2/1000 | Loss: 0.00002020
Iteration 3/1000 | Loss: 0.00001326
Iteration 4/1000 | Loss: 0.00001213
Iteration 5/1000 | Loss: 0.00001155
Iteration 6/1000 | Loss: 0.00001082
Iteration 7/1000 | Loss: 0.00001051
Iteration 8/1000 | Loss: 0.00001038
Iteration 9/1000 | Loss: 0.00001038
Iteration 10/1000 | Loss: 0.00001037
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001026
Iteration 13/1000 | Loss: 0.00001019
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001015
Iteration 17/1000 | Loss: 0.00001015
Iteration 18/1000 | Loss: 0.00001013
Iteration 19/1000 | Loss: 0.00001010
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001009
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001007
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001006
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001004
Iteration 29/1000 | Loss: 0.00001004
Iteration 30/1000 | Loss: 0.00001003
Iteration 31/1000 | Loss: 0.00001003
Iteration 32/1000 | Loss: 0.00001002
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001002
Iteration 35/1000 | Loss: 0.00001002
Iteration 36/1000 | Loss: 0.00001001
Iteration 37/1000 | Loss: 0.00001001
Iteration 38/1000 | Loss: 0.00001001
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000999
Iteration 42/1000 | Loss: 0.00000999
Iteration 43/1000 | Loss: 0.00000999
Iteration 44/1000 | Loss: 0.00000999
Iteration 45/1000 | Loss: 0.00000999
Iteration 46/1000 | Loss: 0.00000998
Iteration 47/1000 | Loss: 0.00000998
Iteration 48/1000 | Loss: 0.00000997
Iteration 49/1000 | Loss: 0.00000997
Iteration 50/1000 | Loss: 0.00000996
Iteration 51/1000 | Loss: 0.00000996
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000995
Iteration 54/1000 | Loss: 0.00000995
Iteration 55/1000 | Loss: 0.00000994
Iteration 56/1000 | Loss: 0.00000994
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000994
Iteration 59/1000 | Loss: 0.00000994
Iteration 60/1000 | Loss: 0.00000994
Iteration 61/1000 | Loss: 0.00000994
Iteration 62/1000 | Loss: 0.00000994
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000994
Iteration 65/1000 | Loss: 0.00000994
Iteration 66/1000 | Loss: 0.00000994
Iteration 67/1000 | Loss: 0.00000994
Iteration 68/1000 | Loss: 0.00000994
Iteration 69/1000 | Loss: 0.00000994
Iteration 70/1000 | Loss: 0.00000994
Iteration 71/1000 | Loss: 0.00000994
Iteration 72/1000 | Loss: 0.00000994
Iteration 73/1000 | Loss: 0.00000994
Iteration 74/1000 | Loss: 0.00000994
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [9.938967195921578e-06, 9.938967195921578e-06, 9.938967195921578e-06, 9.938967195921578e-06, 9.938967195921578e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.938967195921578e-06

Optimization complete. Final v2v error: 2.67095685005188 mm

Highest mean error: 2.8258824348449707 mm for frame 57

Lowest mean error: 2.569998025894165 mm for frame 14

Saving results

Total time: 28.854196071624756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671313
Iteration 2/25 | Loss: 0.00096760
Iteration 3/25 | Loss: 0.00083447
Iteration 4/25 | Loss: 0.00078947
Iteration 5/25 | Loss: 0.00078156
Iteration 6/25 | Loss: 0.00078015
Iteration 7/25 | Loss: 0.00078013
Iteration 8/25 | Loss: 0.00078013
Iteration 9/25 | Loss: 0.00078013
Iteration 10/25 | Loss: 0.00078013
Iteration 11/25 | Loss: 0.00078013
Iteration 12/25 | Loss: 0.00078013
Iteration 13/25 | Loss: 0.00078013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007801254978403449, 0.0007801254978403449, 0.0007801254978403449, 0.0007801254978403449, 0.0007801254978403449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007801254978403449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45381963
Iteration 2/25 | Loss: 0.00029114
Iteration 3/25 | Loss: 0.00029113
Iteration 4/25 | Loss: 0.00029113
Iteration 5/25 | Loss: 0.00029113
Iteration 6/25 | Loss: 0.00029113
Iteration 7/25 | Loss: 0.00029113
Iteration 8/25 | Loss: 0.00029113
Iteration 9/25 | Loss: 0.00029113
Iteration 10/25 | Loss: 0.00029113
Iteration 11/25 | Loss: 0.00029113
Iteration 12/25 | Loss: 0.00029113
Iteration 13/25 | Loss: 0.00029113
Iteration 14/25 | Loss: 0.00029113
Iteration 15/25 | Loss: 0.00029113
Iteration 16/25 | Loss: 0.00029113
Iteration 17/25 | Loss: 0.00029113
Iteration 18/25 | Loss: 0.00029113
Iteration 19/25 | Loss: 0.00029113
Iteration 20/25 | Loss: 0.00029113
Iteration 21/25 | Loss: 0.00029113
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00029112942866049707, 0.00029112942866049707, 0.00029112942866049707, 0.00029112942866049707, 0.00029112942866049707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029112942866049707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029113
Iteration 2/1000 | Loss: 0.00004781
Iteration 3/1000 | Loss: 0.00003532
Iteration 4/1000 | Loss: 0.00003258
Iteration 5/1000 | Loss: 0.00003117
Iteration 6/1000 | Loss: 0.00003033
Iteration 7/1000 | Loss: 0.00002969
Iteration 8/1000 | Loss: 0.00002912
Iteration 9/1000 | Loss: 0.00002875
Iteration 10/1000 | Loss: 0.00002847
Iteration 11/1000 | Loss: 0.00002821
Iteration 12/1000 | Loss: 0.00002800
Iteration 13/1000 | Loss: 0.00002792
Iteration 14/1000 | Loss: 0.00002779
Iteration 15/1000 | Loss: 0.00002777
Iteration 16/1000 | Loss: 0.00002775
Iteration 17/1000 | Loss: 0.00002775
Iteration 18/1000 | Loss: 0.00002774
Iteration 19/1000 | Loss: 0.00002774
Iteration 20/1000 | Loss: 0.00002774
Iteration 21/1000 | Loss: 0.00002773
Iteration 22/1000 | Loss: 0.00002773
Iteration 23/1000 | Loss: 0.00002773
Iteration 24/1000 | Loss: 0.00002772
Iteration 25/1000 | Loss: 0.00002772
Iteration 26/1000 | Loss: 0.00002771
Iteration 27/1000 | Loss: 0.00002771
Iteration 28/1000 | Loss: 0.00002770
Iteration 29/1000 | Loss: 0.00002770
Iteration 30/1000 | Loss: 0.00002770
Iteration 31/1000 | Loss: 0.00002769
Iteration 32/1000 | Loss: 0.00002769
Iteration 33/1000 | Loss: 0.00002769
Iteration 34/1000 | Loss: 0.00002768
Iteration 35/1000 | Loss: 0.00002768
Iteration 36/1000 | Loss: 0.00002768
Iteration 37/1000 | Loss: 0.00002767
Iteration 38/1000 | Loss: 0.00002767
Iteration 39/1000 | Loss: 0.00002767
Iteration 40/1000 | Loss: 0.00002767
Iteration 41/1000 | Loss: 0.00002767
Iteration 42/1000 | Loss: 0.00002767
Iteration 43/1000 | Loss: 0.00002766
Iteration 44/1000 | Loss: 0.00002766
Iteration 45/1000 | Loss: 0.00002766
Iteration 46/1000 | Loss: 0.00002766
Iteration 47/1000 | Loss: 0.00002765
Iteration 48/1000 | Loss: 0.00002765
Iteration 49/1000 | Loss: 0.00002765
Iteration 50/1000 | Loss: 0.00002764
Iteration 51/1000 | Loss: 0.00002764
Iteration 52/1000 | Loss: 0.00002764
Iteration 53/1000 | Loss: 0.00002763
Iteration 54/1000 | Loss: 0.00002763
Iteration 55/1000 | Loss: 0.00002763
Iteration 56/1000 | Loss: 0.00002762
Iteration 57/1000 | Loss: 0.00002762
Iteration 58/1000 | Loss: 0.00002761
Iteration 59/1000 | Loss: 0.00002761
Iteration 60/1000 | Loss: 0.00002761
Iteration 61/1000 | Loss: 0.00002760
Iteration 62/1000 | Loss: 0.00002760
Iteration 63/1000 | Loss: 0.00002759
Iteration 64/1000 | Loss: 0.00002759
Iteration 65/1000 | Loss: 0.00002758
Iteration 66/1000 | Loss: 0.00002758
Iteration 67/1000 | Loss: 0.00002758
Iteration 68/1000 | Loss: 0.00002757
Iteration 69/1000 | Loss: 0.00002756
Iteration 70/1000 | Loss: 0.00002756
Iteration 71/1000 | Loss: 0.00002756
Iteration 72/1000 | Loss: 0.00002755
Iteration 73/1000 | Loss: 0.00002755
Iteration 74/1000 | Loss: 0.00002755
Iteration 75/1000 | Loss: 0.00002754
Iteration 76/1000 | Loss: 0.00002754
Iteration 77/1000 | Loss: 0.00002754
Iteration 78/1000 | Loss: 0.00002753
Iteration 79/1000 | Loss: 0.00002753
Iteration 80/1000 | Loss: 0.00002753
Iteration 81/1000 | Loss: 0.00002753
Iteration 82/1000 | Loss: 0.00002753
Iteration 83/1000 | Loss: 0.00002753
Iteration 84/1000 | Loss: 0.00002753
Iteration 85/1000 | Loss: 0.00002753
Iteration 86/1000 | Loss: 0.00002752
Iteration 87/1000 | Loss: 0.00002752
Iteration 88/1000 | Loss: 0.00002752
Iteration 89/1000 | Loss: 0.00002752
Iteration 90/1000 | Loss: 0.00002752
Iteration 91/1000 | Loss: 0.00002752
Iteration 92/1000 | Loss: 0.00002752
Iteration 93/1000 | Loss: 0.00002752
Iteration 94/1000 | Loss: 0.00002752
Iteration 95/1000 | Loss: 0.00002752
Iteration 96/1000 | Loss: 0.00002752
Iteration 97/1000 | Loss: 0.00002751
Iteration 98/1000 | Loss: 0.00002751
Iteration 99/1000 | Loss: 0.00002751
Iteration 100/1000 | Loss: 0.00002751
Iteration 101/1000 | Loss: 0.00002751
Iteration 102/1000 | Loss: 0.00002751
Iteration 103/1000 | Loss: 0.00002751
Iteration 104/1000 | Loss: 0.00002751
Iteration 105/1000 | Loss: 0.00002751
Iteration 106/1000 | Loss: 0.00002751
Iteration 107/1000 | Loss: 0.00002751
Iteration 108/1000 | Loss: 0.00002751
Iteration 109/1000 | Loss: 0.00002751
Iteration 110/1000 | Loss: 0.00002751
Iteration 111/1000 | Loss: 0.00002750
Iteration 112/1000 | Loss: 0.00002750
Iteration 113/1000 | Loss: 0.00002750
Iteration 114/1000 | Loss: 0.00002750
Iteration 115/1000 | Loss: 0.00002750
Iteration 116/1000 | Loss: 0.00002750
Iteration 117/1000 | Loss: 0.00002749
Iteration 118/1000 | Loss: 0.00002749
Iteration 119/1000 | Loss: 0.00002749
Iteration 120/1000 | Loss: 0.00002749
Iteration 121/1000 | Loss: 0.00002749
Iteration 122/1000 | Loss: 0.00002749
Iteration 123/1000 | Loss: 0.00002749
Iteration 124/1000 | Loss: 0.00002749
Iteration 125/1000 | Loss: 0.00002749
Iteration 126/1000 | Loss: 0.00002748
Iteration 127/1000 | Loss: 0.00002748
Iteration 128/1000 | Loss: 0.00002748
Iteration 129/1000 | Loss: 0.00002748
Iteration 130/1000 | Loss: 0.00002748
Iteration 131/1000 | Loss: 0.00002748
Iteration 132/1000 | Loss: 0.00002748
Iteration 133/1000 | Loss: 0.00002747
Iteration 134/1000 | Loss: 0.00002747
Iteration 135/1000 | Loss: 0.00002747
Iteration 136/1000 | Loss: 0.00002747
Iteration 137/1000 | Loss: 0.00002747
Iteration 138/1000 | Loss: 0.00002747
Iteration 139/1000 | Loss: 0.00002747
Iteration 140/1000 | Loss: 0.00002747
Iteration 141/1000 | Loss: 0.00002747
Iteration 142/1000 | Loss: 0.00002747
Iteration 143/1000 | Loss: 0.00002747
Iteration 144/1000 | Loss: 0.00002747
Iteration 145/1000 | Loss: 0.00002747
Iteration 146/1000 | Loss: 0.00002747
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00002747
Iteration 149/1000 | Loss: 0.00002747
Iteration 150/1000 | Loss: 0.00002747
Iteration 151/1000 | Loss: 0.00002747
Iteration 152/1000 | Loss: 0.00002747
Iteration 153/1000 | Loss: 0.00002747
Iteration 154/1000 | Loss: 0.00002747
Iteration 155/1000 | Loss: 0.00002747
Iteration 156/1000 | Loss: 0.00002747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.746902100625448e-05, 2.746902100625448e-05, 2.746902100625448e-05, 2.746902100625448e-05, 2.746902100625448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.746902100625448e-05

Optimization complete. Final v2v error: 4.311666965484619 mm

Highest mean error: 4.67799186706543 mm for frame 27

Lowest mean error: 3.7975869178771973 mm for frame 205

Saving results

Total time: 44.350014448165894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965417
Iteration 2/25 | Loss: 0.00247888
Iteration 3/25 | Loss: 0.00161768
Iteration 4/25 | Loss: 0.00144754
Iteration 5/25 | Loss: 0.00138434
Iteration 6/25 | Loss: 0.00128470
Iteration 7/25 | Loss: 0.00111837
Iteration 8/25 | Loss: 0.00101072
Iteration 9/25 | Loss: 0.00097385
Iteration 10/25 | Loss: 0.00095888
Iteration 11/25 | Loss: 0.00093149
Iteration 12/25 | Loss: 0.00092273
Iteration 13/25 | Loss: 0.00090659
Iteration 14/25 | Loss: 0.00090280
Iteration 15/25 | Loss: 0.00091031
Iteration 16/25 | Loss: 0.00090378
Iteration 17/25 | Loss: 0.00090971
Iteration 18/25 | Loss: 0.00090481
Iteration 19/25 | Loss: 0.00090948
Iteration 20/25 | Loss: 0.00090884
Iteration 21/25 | Loss: 0.00090443
Iteration 22/25 | Loss: 0.00090939
Iteration 23/25 | Loss: 0.00090463
Iteration 24/25 | Loss: 0.00090937
Iteration 25/25 | Loss: 0.00090855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48415363
Iteration 2/25 | Loss: 0.00083239
Iteration 3/25 | Loss: 0.00083239
Iteration 4/25 | Loss: 0.00083239
Iteration 5/25 | Loss: 0.00083239
Iteration 6/25 | Loss: 0.00083239
Iteration 7/25 | Loss: 0.00083239
Iteration 8/25 | Loss: 0.00083239
Iteration 9/25 | Loss: 0.00083239
Iteration 10/25 | Loss: 0.00083239
Iteration 11/25 | Loss: 0.00083239
Iteration 12/25 | Loss: 0.00083239
Iteration 13/25 | Loss: 0.00083239
Iteration 14/25 | Loss: 0.00083239
Iteration 15/25 | Loss: 0.00083239
Iteration 16/25 | Loss: 0.00083239
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008323879446834326, 0.0008323879446834326, 0.0008323879446834326, 0.0008323879446834326, 0.0008323879446834326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008323879446834326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083239
Iteration 2/1000 | Loss: 0.00656288
Iteration 3/1000 | Loss: 0.00300283
Iteration 4/1000 | Loss: 0.00325641
Iteration 5/1000 | Loss: 0.00075877
Iteration 6/1000 | Loss: 0.00125503
Iteration 7/1000 | Loss: 0.00024915
Iteration 8/1000 | Loss: 0.00042534
Iteration 9/1000 | Loss: 0.00026703
Iteration 10/1000 | Loss: 0.00054382
Iteration 11/1000 | Loss: 0.00022985
Iteration 12/1000 | Loss: 0.00021213
Iteration 13/1000 | Loss: 0.00012054
Iteration 14/1000 | Loss: 0.00075250
Iteration 15/1000 | Loss: 0.00426048
Iteration 16/1000 | Loss: 0.00628032
Iteration 17/1000 | Loss: 0.00209766
Iteration 18/1000 | Loss: 0.00048288
Iteration 19/1000 | Loss: 0.00079342
Iteration 20/1000 | Loss: 0.00011964
Iteration 21/1000 | Loss: 0.00052285
Iteration 22/1000 | Loss: 0.00017762
Iteration 23/1000 | Loss: 0.00019930
Iteration 24/1000 | Loss: 0.00010810
Iteration 25/1000 | Loss: 0.00035427
Iteration 26/1000 | Loss: 0.00028643
Iteration 27/1000 | Loss: 0.00049003
Iteration 28/1000 | Loss: 0.00012912
Iteration 29/1000 | Loss: 0.00008634
Iteration 30/1000 | Loss: 0.00005743
Iteration 31/1000 | Loss: 0.00005054
Iteration 32/1000 | Loss: 0.00012082
Iteration 33/1000 | Loss: 0.00004246
Iteration 34/1000 | Loss: 0.00003785
Iteration 35/1000 | Loss: 0.00008077
Iteration 36/1000 | Loss: 0.00034590
Iteration 37/1000 | Loss: 0.00003898
Iteration 38/1000 | Loss: 0.00003514
Iteration 39/1000 | Loss: 0.00036249
Iteration 40/1000 | Loss: 0.00004750
Iteration 41/1000 | Loss: 0.00006315
Iteration 42/1000 | Loss: 0.00025921
Iteration 43/1000 | Loss: 0.00003536
Iteration 44/1000 | Loss: 0.00003116
Iteration 45/1000 | Loss: 0.00006277
Iteration 46/1000 | Loss: 0.00005136
Iteration 47/1000 | Loss: 0.00003254
Iteration 48/1000 | Loss: 0.00003045
Iteration 49/1000 | Loss: 0.00003030
Iteration 50/1000 | Loss: 0.00002791
Iteration 51/1000 | Loss: 0.00002623
Iteration 52/1000 | Loss: 0.00002499
Iteration 53/1000 | Loss: 0.00002375
Iteration 54/1000 | Loss: 0.00002316
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002250
Iteration 57/1000 | Loss: 0.00002236
Iteration 58/1000 | Loss: 0.00002232
Iteration 59/1000 | Loss: 0.00002221
Iteration 60/1000 | Loss: 0.00002205
Iteration 61/1000 | Loss: 0.00002202
Iteration 62/1000 | Loss: 0.00002200
Iteration 63/1000 | Loss: 0.00002200
Iteration 64/1000 | Loss: 0.00002199
Iteration 65/1000 | Loss: 0.00002199
Iteration 66/1000 | Loss: 0.00002190
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002184
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002177
Iteration 72/1000 | Loss: 0.00002177
Iteration 73/1000 | Loss: 0.00002177
Iteration 74/1000 | Loss: 0.00002176
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002176
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00002175
Iteration 79/1000 | Loss: 0.00002175
Iteration 80/1000 | Loss: 0.00002175
Iteration 81/1000 | Loss: 0.00002175
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002174
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002174
Iteration 86/1000 | Loss: 0.00002174
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002173
Iteration 91/1000 | Loss: 0.00002173
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002172
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002171
Iteration 97/1000 | Loss: 0.00002171
Iteration 98/1000 | Loss: 0.00002171
Iteration 99/1000 | Loss: 0.00002171
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002171
Iteration 104/1000 | Loss: 0.00002171
Iteration 105/1000 | Loss: 0.00002171
Iteration 106/1000 | Loss: 0.00002171
Iteration 107/1000 | Loss: 0.00002170
Iteration 108/1000 | Loss: 0.00002170
Iteration 109/1000 | Loss: 0.00002170
Iteration 110/1000 | Loss: 0.00002170
Iteration 111/1000 | Loss: 0.00002170
Iteration 112/1000 | Loss: 0.00002170
Iteration 113/1000 | Loss: 0.00002170
Iteration 114/1000 | Loss: 0.00002170
Iteration 115/1000 | Loss: 0.00002170
Iteration 116/1000 | Loss: 0.00002170
Iteration 117/1000 | Loss: 0.00002170
Iteration 118/1000 | Loss: 0.00002170
Iteration 119/1000 | Loss: 0.00002170
Iteration 120/1000 | Loss: 0.00002170
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002170
Iteration 127/1000 | Loss: 0.00002170
Iteration 128/1000 | Loss: 0.00002170
Iteration 129/1000 | Loss: 0.00002170
Iteration 130/1000 | Loss: 0.00002170
Iteration 131/1000 | Loss: 0.00002170
Iteration 132/1000 | Loss: 0.00002170
Iteration 133/1000 | Loss: 0.00002170
Iteration 134/1000 | Loss: 0.00002170
Iteration 135/1000 | Loss: 0.00002170
Iteration 136/1000 | Loss: 0.00002170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.1695215764339082e-05, 2.1695215764339082e-05, 2.1695215764339082e-05, 2.1695215764339082e-05, 2.1695215764339082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1695215764339082e-05

Optimization complete. Final v2v error: 3.5603322982788086 mm

Highest mean error: 6.032444477081299 mm for frame 100

Lowest mean error: 2.900602102279663 mm for frame 123

Saving results

Total time: 133.11575055122375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00733461
Iteration 2/25 | Loss: 0.00098106
Iteration 3/25 | Loss: 0.00080317
Iteration 4/25 | Loss: 0.00077344
Iteration 5/25 | Loss: 0.00076264
Iteration 6/25 | Loss: 0.00076019
Iteration 7/25 | Loss: 0.00075945
Iteration 8/25 | Loss: 0.00075933
Iteration 9/25 | Loss: 0.00075933
Iteration 10/25 | Loss: 0.00075933
Iteration 11/25 | Loss: 0.00075933
Iteration 12/25 | Loss: 0.00075933
Iteration 13/25 | Loss: 0.00075933
Iteration 14/25 | Loss: 0.00075933
Iteration 15/25 | Loss: 0.00075933
Iteration 16/25 | Loss: 0.00075933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007593297632411122, 0.0007593297632411122, 0.0007593297632411122, 0.0007593297632411122, 0.0007593297632411122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007593297632411122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17336774
Iteration 2/25 | Loss: 0.00027814
Iteration 3/25 | Loss: 0.00027814
Iteration 4/25 | Loss: 0.00027814
Iteration 5/25 | Loss: 0.00027814
Iteration 6/25 | Loss: 0.00027814
Iteration 7/25 | Loss: 0.00027814
Iteration 8/25 | Loss: 0.00027814
Iteration 9/25 | Loss: 0.00027814
Iteration 10/25 | Loss: 0.00027814
Iteration 11/25 | Loss: 0.00027813
Iteration 12/25 | Loss: 0.00027813
Iteration 13/25 | Loss: 0.00027813
Iteration 14/25 | Loss: 0.00027813
Iteration 15/25 | Loss: 0.00027813
Iteration 16/25 | Loss: 0.00027813
Iteration 17/25 | Loss: 0.00027813
Iteration 18/25 | Loss: 0.00027813
Iteration 19/25 | Loss: 0.00027813
Iteration 20/25 | Loss: 0.00027813
Iteration 21/25 | Loss: 0.00027813
Iteration 22/25 | Loss: 0.00027813
Iteration 23/25 | Loss: 0.00027813
Iteration 24/25 | Loss: 0.00027813
Iteration 25/25 | Loss: 0.00027813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027813
Iteration 2/1000 | Loss: 0.00003372
Iteration 3/1000 | Loss: 0.00002315
Iteration 4/1000 | Loss: 0.00002077
Iteration 5/1000 | Loss: 0.00001989
Iteration 6/1000 | Loss: 0.00001909
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001834
Iteration 9/1000 | Loss: 0.00001810
Iteration 10/1000 | Loss: 0.00001788
Iteration 11/1000 | Loss: 0.00001769
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001738
Iteration 17/1000 | Loss: 0.00001731
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001730
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001728
Iteration 22/1000 | Loss: 0.00001728
Iteration 23/1000 | Loss: 0.00001727
Iteration 24/1000 | Loss: 0.00001726
Iteration 25/1000 | Loss: 0.00001726
Iteration 26/1000 | Loss: 0.00001726
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001723
Iteration 31/1000 | Loss: 0.00001722
Iteration 32/1000 | Loss: 0.00001722
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001716
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001705
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001704
Iteration 115/1000 | Loss: 0.00001704
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001702
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001702
Iteration 132/1000 | Loss: 0.00001702
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001702
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001701
Iteration 149/1000 | Loss: 0.00001701
Iteration 150/1000 | Loss: 0.00001701
Iteration 151/1000 | Loss: 0.00001701
Iteration 152/1000 | Loss: 0.00001701
Iteration 153/1000 | Loss: 0.00001701
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001701
Iteration 160/1000 | Loss: 0.00001700
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001700
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001700
Iteration 168/1000 | Loss: 0.00001700
Iteration 169/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.7001881133182906e-05, 1.7001881133182906e-05, 1.7001881133182906e-05, 1.7001881133182906e-05, 1.7001881133182906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7001881133182906e-05

Optimization complete. Final v2v error: 3.469865083694458 mm

Highest mean error: 5.04962158203125 mm for frame 98

Lowest mean error: 3.064640760421753 mm for frame 140

Saving results

Total time: 41.31884407997131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520303
Iteration 2/25 | Loss: 0.00096194
Iteration 3/25 | Loss: 0.00081991
Iteration 4/25 | Loss: 0.00079472
Iteration 5/25 | Loss: 0.00078979
Iteration 6/25 | Loss: 0.00078875
Iteration 7/25 | Loss: 0.00078875
Iteration 8/25 | Loss: 0.00078875
Iteration 9/25 | Loss: 0.00078875
Iteration 10/25 | Loss: 0.00078875
Iteration 11/25 | Loss: 0.00078875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007887535612098873, 0.0007887535612098873, 0.0007887535612098873, 0.0007887535612098873, 0.0007887535612098873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007887535612098873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04417002
Iteration 2/25 | Loss: 0.00033913
Iteration 3/25 | Loss: 0.00033911
Iteration 4/25 | Loss: 0.00033911
Iteration 5/25 | Loss: 0.00033911
Iteration 6/25 | Loss: 0.00033911
Iteration 7/25 | Loss: 0.00033911
Iteration 8/25 | Loss: 0.00033911
Iteration 9/25 | Loss: 0.00033911
Iteration 10/25 | Loss: 0.00033911
Iteration 11/25 | Loss: 0.00033911
Iteration 12/25 | Loss: 0.00033911
Iteration 13/25 | Loss: 0.00033911
Iteration 14/25 | Loss: 0.00033911
Iteration 15/25 | Loss: 0.00033911
Iteration 16/25 | Loss: 0.00033911
Iteration 17/25 | Loss: 0.00033911
Iteration 18/25 | Loss: 0.00033911
Iteration 19/25 | Loss: 0.00033911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003391086938790977, 0.0003391086938790977, 0.0003391086938790977, 0.0003391086938790977, 0.0003391086938790977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003391086938790977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033911
Iteration 2/1000 | Loss: 0.00004168
Iteration 3/1000 | Loss: 0.00003303
Iteration 4/1000 | Loss: 0.00003023
Iteration 5/1000 | Loss: 0.00002823
Iteration 6/1000 | Loss: 0.00002675
Iteration 7/1000 | Loss: 0.00002602
Iteration 8/1000 | Loss: 0.00002570
Iteration 9/1000 | Loss: 0.00002547
Iteration 10/1000 | Loss: 0.00002528
Iteration 11/1000 | Loss: 0.00002510
Iteration 12/1000 | Loss: 0.00002510
Iteration 13/1000 | Loss: 0.00002509
Iteration 14/1000 | Loss: 0.00002508
Iteration 15/1000 | Loss: 0.00002500
Iteration 16/1000 | Loss: 0.00002492
Iteration 17/1000 | Loss: 0.00002492
Iteration 18/1000 | Loss: 0.00002492
Iteration 19/1000 | Loss: 0.00002492
Iteration 20/1000 | Loss: 0.00002492
Iteration 21/1000 | Loss: 0.00002492
Iteration 22/1000 | Loss: 0.00002492
Iteration 23/1000 | Loss: 0.00002491
Iteration 24/1000 | Loss: 0.00002487
Iteration 25/1000 | Loss: 0.00002487
Iteration 26/1000 | Loss: 0.00002485
Iteration 27/1000 | Loss: 0.00002485
Iteration 28/1000 | Loss: 0.00002484
Iteration 29/1000 | Loss: 0.00002484
Iteration 30/1000 | Loss: 0.00002484
Iteration 31/1000 | Loss: 0.00002483
Iteration 32/1000 | Loss: 0.00002483
Iteration 33/1000 | Loss: 0.00002483
Iteration 34/1000 | Loss: 0.00002483
Iteration 35/1000 | Loss: 0.00002483
Iteration 36/1000 | Loss: 0.00002482
Iteration 37/1000 | Loss: 0.00002482
Iteration 38/1000 | Loss: 0.00002482
Iteration 39/1000 | Loss: 0.00002482
Iteration 40/1000 | Loss: 0.00002482
Iteration 41/1000 | Loss: 0.00002482
Iteration 42/1000 | Loss: 0.00002482
Iteration 43/1000 | Loss: 0.00002481
Iteration 44/1000 | Loss: 0.00002481
Iteration 45/1000 | Loss: 0.00002481
Iteration 46/1000 | Loss: 0.00002481
Iteration 47/1000 | Loss: 0.00002481
Iteration 48/1000 | Loss: 0.00002480
Iteration 49/1000 | Loss: 0.00002480
Iteration 50/1000 | Loss: 0.00002480
Iteration 51/1000 | Loss: 0.00002480
Iteration 52/1000 | Loss: 0.00002479
Iteration 53/1000 | Loss: 0.00002479
Iteration 54/1000 | Loss: 0.00002479
Iteration 55/1000 | Loss: 0.00002479
Iteration 56/1000 | Loss: 0.00002479
Iteration 57/1000 | Loss: 0.00002479
Iteration 58/1000 | Loss: 0.00002479
Iteration 59/1000 | Loss: 0.00002478
Iteration 60/1000 | Loss: 0.00002478
Iteration 61/1000 | Loss: 0.00002478
Iteration 62/1000 | Loss: 0.00002478
Iteration 63/1000 | Loss: 0.00002478
Iteration 64/1000 | Loss: 0.00002478
Iteration 65/1000 | Loss: 0.00002478
Iteration 66/1000 | Loss: 0.00002478
Iteration 67/1000 | Loss: 0.00002478
Iteration 68/1000 | Loss: 0.00002477
Iteration 69/1000 | Loss: 0.00002477
Iteration 70/1000 | Loss: 0.00002476
Iteration 71/1000 | Loss: 0.00002476
Iteration 72/1000 | Loss: 0.00002476
Iteration 73/1000 | Loss: 0.00002476
Iteration 74/1000 | Loss: 0.00002476
Iteration 75/1000 | Loss: 0.00002476
Iteration 76/1000 | Loss: 0.00002476
Iteration 77/1000 | Loss: 0.00002476
Iteration 78/1000 | Loss: 0.00002476
Iteration 79/1000 | Loss: 0.00002476
Iteration 80/1000 | Loss: 0.00002476
Iteration 81/1000 | Loss: 0.00002476
Iteration 82/1000 | Loss: 0.00002476
Iteration 83/1000 | Loss: 0.00002476
Iteration 84/1000 | Loss: 0.00002475
Iteration 85/1000 | Loss: 0.00002475
Iteration 86/1000 | Loss: 0.00002475
Iteration 87/1000 | Loss: 0.00002475
Iteration 88/1000 | Loss: 0.00002474
Iteration 89/1000 | Loss: 0.00002474
Iteration 90/1000 | Loss: 0.00002474
Iteration 91/1000 | Loss: 0.00002474
Iteration 92/1000 | Loss: 0.00002474
Iteration 93/1000 | Loss: 0.00002474
Iteration 94/1000 | Loss: 0.00002473
Iteration 95/1000 | Loss: 0.00002473
Iteration 96/1000 | Loss: 0.00002473
Iteration 97/1000 | Loss: 0.00002473
Iteration 98/1000 | Loss: 0.00002473
Iteration 99/1000 | Loss: 0.00002473
Iteration 100/1000 | Loss: 0.00002472
Iteration 101/1000 | Loss: 0.00002472
Iteration 102/1000 | Loss: 0.00002472
Iteration 103/1000 | Loss: 0.00002472
Iteration 104/1000 | Loss: 0.00002472
Iteration 105/1000 | Loss: 0.00002472
Iteration 106/1000 | Loss: 0.00002471
Iteration 107/1000 | Loss: 0.00002471
Iteration 108/1000 | Loss: 0.00002471
Iteration 109/1000 | Loss: 0.00002471
Iteration 110/1000 | Loss: 0.00002470
Iteration 111/1000 | Loss: 0.00002470
Iteration 112/1000 | Loss: 0.00002470
Iteration 113/1000 | Loss: 0.00002470
Iteration 114/1000 | Loss: 0.00002470
Iteration 115/1000 | Loss: 0.00002470
Iteration 116/1000 | Loss: 0.00002470
Iteration 117/1000 | Loss: 0.00002470
Iteration 118/1000 | Loss: 0.00002470
Iteration 119/1000 | Loss: 0.00002470
Iteration 120/1000 | Loss: 0.00002469
Iteration 121/1000 | Loss: 0.00002469
Iteration 122/1000 | Loss: 0.00002469
Iteration 123/1000 | Loss: 0.00002469
Iteration 124/1000 | Loss: 0.00002469
Iteration 125/1000 | Loss: 0.00002469
Iteration 126/1000 | Loss: 0.00002469
Iteration 127/1000 | Loss: 0.00002469
Iteration 128/1000 | Loss: 0.00002469
Iteration 129/1000 | Loss: 0.00002469
Iteration 130/1000 | Loss: 0.00002469
Iteration 131/1000 | Loss: 0.00002469
Iteration 132/1000 | Loss: 0.00002469
Iteration 133/1000 | Loss: 0.00002469
Iteration 134/1000 | Loss: 0.00002469
Iteration 135/1000 | Loss: 0.00002469
Iteration 136/1000 | Loss: 0.00002469
Iteration 137/1000 | Loss: 0.00002469
Iteration 138/1000 | Loss: 0.00002469
Iteration 139/1000 | Loss: 0.00002469
Iteration 140/1000 | Loss: 0.00002469
Iteration 141/1000 | Loss: 0.00002469
Iteration 142/1000 | Loss: 0.00002469
Iteration 143/1000 | Loss: 0.00002469
Iteration 144/1000 | Loss: 0.00002469
Iteration 145/1000 | Loss: 0.00002469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.469063656462822e-05, 2.469063656462822e-05, 2.469063656462822e-05, 2.469063656462822e-05, 2.469063656462822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.469063656462822e-05

Optimization complete. Final v2v error: 4.209005832672119 mm

Highest mean error: 4.594367504119873 mm for frame 83

Lowest mean error: 4.066425800323486 mm for frame 152

Saving results

Total time: 39.03194499015808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00342999
Iteration 2/25 | Loss: 0.00096395
Iteration 3/25 | Loss: 0.00075867
Iteration 4/25 | Loss: 0.00072012
Iteration 5/25 | Loss: 0.00071239
Iteration 6/25 | Loss: 0.00071095
Iteration 7/25 | Loss: 0.00071093
Iteration 8/25 | Loss: 0.00071093
Iteration 9/25 | Loss: 0.00071093
Iteration 10/25 | Loss: 0.00071093
Iteration 11/25 | Loss: 0.00071093
Iteration 12/25 | Loss: 0.00071093
Iteration 13/25 | Loss: 0.00071093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007109340513125062, 0.0007109340513125062, 0.0007109340513125062, 0.0007109340513125062, 0.0007109340513125062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007109340513125062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45664608
Iteration 2/25 | Loss: 0.00021676
Iteration 3/25 | Loss: 0.00021676
Iteration 4/25 | Loss: 0.00021676
Iteration 5/25 | Loss: 0.00021676
Iteration 6/25 | Loss: 0.00021676
Iteration 7/25 | Loss: 0.00021676
Iteration 8/25 | Loss: 0.00021676
Iteration 9/25 | Loss: 0.00021676
Iteration 10/25 | Loss: 0.00021676
Iteration 11/25 | Loss: 0.00021676
Iteration 12/25 | Loss: 0.00021676
Iteration 13/25 | Loss: 0.00021676
Iteration 14/25 | Loss: 0.00021676
Iteration 15/25 | Loss: 0.00021676
Iteration 16/25 | Loss: 0.00021676
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00021675904281437397, 0.00021675904281437397, 0.00021675904281437397, 0.00021675904281437397, 0.00021675904281437397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021675904281437397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021676
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00001975
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001646
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001518
Iteration 8/1000 | Loss: 0.00001486
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001409
Iteration 15/1000 | Loss: 0.00001405
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001401
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001399
Iteration 22/1000 | Loss: 0.00001398
Iteration 23/1000 | Loss: 0.00001397
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001389
Iteration 30/1000 | Loss: 0.00001388
Iteration 31/1000 | Loss: 0.00001387
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001386
Iteration 38/1000 | Loss: 0.00001385
Iteration 39/1000 | Loss: 0.00001385
Iteration 40/1000 | Loss: 0.00001384
Iteration 41/1000 | Loss: 0.00001384
Iteration 42/1000 | Loss: 0.00001384
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001382
Iteration 50/1000 | Loss: 0.00001381
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001381
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001380
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001380
Iteration 62/1000 | Loss: 0.00001380
Iteration 63/1000 | Loss: 0.00001380
Iteration 64/1000 | Loss: 0.00001380
Iteration 65/1000 | Loss: 0.00001380
Iteration 66/1000 | Loss: 0.00001379
Iteration 67/1000 | Loss: 0.00001379
Iteration 68/1000 | Loss: 0.00001379
Iteration 69/1000 | Loss: 0.00001379
Iteration 70/1000 | Loss: 0.00001379
Iteration 71/1000 | Loss: 0.00001379
Iteration 72/1000 | Loss: 0.00001379
Iteration 73/1000 | Loss: 0.00001378
Iteration 74/1000 | Loss: 0.00001378
Iteration 75/1000 | Loss: 0.00001378
Iteration 76/1000 | Loss: 0.00001378
Iteration 77/1000 | Loss: 0.00001378
Iteration 78/1000 | Loss: 0.00001378
Iteration 79/1000 | Loss: 0.00001378
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001378
Iteration 86/1000 | Loss: 0.00001378
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001377
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001377
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00001376
Iteration 94/1000 | Loss: 0.00001376
Iteration 95/1000 | Loss: 0.00001376
Iteration 96/1000 | Loss: 0.00001376
Iteration 97/1000 | Loss: 0.00001376
Iteration 98/1000 | Loss: 0.00001376
Iteration 99/1000 | Loss: 0.00001376
Iteration 100/1000 | Loss: 0.00001376
Iteration 101/1000 | Loss: 0.00001376
Iteration 102/1000 | Loss: 0.00001376
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001375
Iteration 105/1000 | Loss: 0.00001375
Iteration 106/1000 | Loss: 0.00001375
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001375
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001374
Iteration 112/1000 | Loss: 0.00001374
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001373
Iteration 125/1000 | Loss: 0.00001373
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.3726551514992025e-05, 1.3726551514992025e-05, 1.3726551514992025e-05, 1.3726551514992025e-05, 1.3726551514992025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3726551514992025e-05

Optimization complete. Final v2v error: 3.120701789855957 mm

Highest mean error: 3.430732250213623 mm for frame 26

Lowest mean error: 2.7836060523986816 mm for frame 106

Saving results

Total time: 41.832207441329956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391742
Iteration 2/25 | Loss: 0.00090865
Iteration 3/25 | Loss: 0.00075110
Iteration 4/25 | Loss: 0.00073616
Iteration 5/25 | Loss: 0.00072841
Iteration 6/25 | Loss: 0.00072763
Iteration 7/25 | Loss: 0.00072763
Iteration 8/25 | Loss: 0.00072763
Iteration 9/25 | Loss: 0.00072763
Iteration 10/25 | Loss: 0.00072763
Iteration 11/25 | Loss: 0.00072763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007276267861016095, 0.0007276267861016095, 0.0007276267861016095, 0.0007276267861016095, 0.0007276267861016095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007276267861016095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72209549
Iteration 2/25 | Loss: 0.00027402
Iteration 3/25 | Loss: 0.00027402
Iteration 4/25 | Loss: 0.00027402
Iteration 5/25 | Loss: 0.00027402
Iteration 6/25 | Loss: 0.00027402
Iteration 7/25 | Loss: 0.00027402
Iteration 8/25 | Loss: 0.00027402
Iteration 9/25 | Loss: 0.00027402
Iteration 10/25 | Loss: 0.00027402
Iteration 11/25 | Loss: 0.00027402
Iteration 12/25 | Loss: 0.00027402
Iteration 13/25 | Loss: 0.00027402
Iteration 14/25 | Loss: 0.00027402
Iteration 15/25 | Loss: 0.00027402
Iteration 16/25 | Loss: 0.00027402
Iteration 17/25 | Loss: 0.00027402
Iteration 18/25 | Loss: 0.00027402
Iteration 19/25 | Loss: 0.00027402
Iteration 20/25 | Loss: 0.00027402
Iteration 21/25 | Loss: 0.00027402
Iteration 22/25 | Loss: 0.00027402
Iteration 23/25 | Loss: 0.00027402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002740162890404463, 0.0002740162890404463, 0.0002740162890404463, 0.0002740162890404463, 0.0002740162890404463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002740162890404463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027402
Iteration 2/1000 | Loss: 0.00002555
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001532
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00001409
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001346
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001325
Iteration 15/1000 | Loss: 0.00001324
Iteration 16/1000 | Loss: 0.00001322
Iteration 17/1000 | Loss: 0.00001321
Iteration 18/1000 | Loss: 0.00001320
Iteration 19/1000 | Loss: 0.00001317
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001314
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001312
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001306
Iteration 43/1000 | Loss: 0.00001306
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001303
Iteration 48/1000 | Loss: 0.00001303
Iteration 49/1000 | Loss: 0.00001303
Iteration 50/1000 | Loss: 0.00001303
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001302
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001300
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001300
Iteration 62/1000 | Loss: 0.00001300
Iteration 63/1000 | Loss: 0.00001299
Iteration 64/1000 | Loss: 0.00001299
Iteration 65/1000 | Loss: 0.00001299
Iteration 66/1000 | Loss: 0.00001299
Iteration 67/1000 | Loss: 0.00001299
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001299
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001297
Iteration 79/1000 | Loss: 0.00001297
Iteration 80/1000 | Loss: 0.00001297
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001296
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001296
Iteration 88/1000 | Loss: 0.00001296
Iteration 89/1000 | Loss: 0.00001296
Iteration 90/1000 | Loss: 0.00001296
Iteration 91/1000 | Loss: 0.00001296
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001296
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001295
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001294
Iteration 114/1000 | Loss: 0.00001294
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2936337043356616e-05, 1.2936337043356616e-05, 1.2936337043356616e-05, 1.2936337043356616e-05, 1.2936337043356616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2936337043356616e-05

Optimization complete. Final v2v error: 3.0618765354156494 mm

Highest mean error: 3.3351941108703613 mm for frame 46

Lowest mean error: 2.831777811050415 mm for frame 244

Saving results

Total time: 37.94491219520569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465567
Iteration 2/25 | Loss: 0.00099767
Iteration 3/25 | Loss: 0.00079683
Iteration 4/25 | Loss: 0.00075703
Iteration 5/25 | Loss: 0.00074868
Iteration 6/25 | Loss: 0.00074653
Iteration 7/25 | Loss: 0.00074601
Iteration 8/25 | Loss: 0.00074601
Iteration 9/25 | Loss: 0.00074601
Iteration 10/25 | Loss: 0.00074601
Iteration 11/25 | Loss: 0.00074601
Iteration 12/25 | Loss: 0.00074601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007460080669261515, 0.0007460080669261515, 0.0007460080669261515, 0.0007460080669261515, 0.0007460080669261515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007460080669261515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39340043
Iteration 2/25 | Loss: 0.00025002
Iteration 3/25 | Loss: 0.00024998
Iteration 4/25 | Loss: 0.00024998
Iteration 5/25 | Loss: 0.00024998
Iteration 6/25 | Loss: 0.00024998
Iteration 7/25 | Loss: 0.00024998
Iteration 8/25 | Loss: 0.00024998
Iteration 9/25 | Loss: 0.00024998
Iteration 10/25 | Loss: 0.00024998
Iteration 11/25 | Loss: 0.00024998
Iteration 12/25 | Loss: 0.00024998
Iteration 13/25 | Loss: 0.00024998
Iteration 14/25 | Loss: 0.00024998
Iteration 15/25 | Loss: 0.00024998
Iteration 16/25 | Loss: 0.00024998
Iteration 17/25 | Loss: 0.00024998
Iteration 18/25 | Loss: 0.00024998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002499764959793538, 0.0002499764959793538, 0.0002499764959793538, 0.0002499764959793538, 0.0002499764959793538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002499764959793538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024998
Iteration 2/1000 | Loss: 0.00003179
Iteration 3/1000 | Loss: 0.00002307
Iteration 4/1000 | Loss: 0.00002061
Iteration 5/1000 | Loss: 0.00001957
Iteration 6/1000 | Loss: 0.00001868
Iteration 7/1000 | Loss: 0.00001833
Iteration 8/1000 | Loss: 0.00001796
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001766
Iteration 11/1000 | Loss: 0.00001747
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001730
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001727
Iteration 17/1000 | Loss: 0.00001724
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001711
Iteration 20/1000 | Loss: 0.00001710
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001704
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001702
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001697
Iteration 31/1000 | Loss: 0.00001697
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00001692
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001692
Iteration 51/1000 | Loss: 0.00001692
Iteration 52/1000 | Loss: 0.00001692
Iteration 53/1000 | Loss: 0.00001692
Iteration 54/1000 | Loss: 0.00001692
Iteration 55/1000 | Loss: 0.00001692
Iteration 56/1000 | Loss: 0.00001692
Iteration 57/1000 | Loss: 0.00001692
Iteration 58/1000 | Loss: 0.00001692
Iteration 59/1000 | Loss: 0.00001692
Iteration 60/1000 | Loss: 0.00001692
Iteration 61/1000 | Loss: 0.00001692
Iteration 62/1000 | Loss: 0.00001692
Iteration 63/1000 | Loss: 0.00001692
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.692083060333971e-05, 1.692083060333971e-05, 1.692083060333971e-05, 1.692083060333971e-05, 1.692083060333971e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.692083060333971e-05

Optimization complete. Final v2v error: 3.4432566165924072 mm

Highest mean error: 4.469823837280273 mm for frame 56

Lowest mean error: 2.8557076454162598 mm for frame 102

Saving results

Total time: 36.352957010269165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081857
Iteration 2/25 | Loss: 0.00207587
Iteration 3/25 | Loss: 0.00134338
Iteration 4/25 | Loss: 0.00120258
Iteration 5/25 | Loss: 0.00102184
Iteration 6/25 | Loss: 0.00093835
Iteration 7/25 | Loss: 0.00104313
Iteration 8/25 | Loss: 0.00092479
Iteration 9/25 | Loss: 0.00090929
Iteration 10/25 | Loss: 0.00082112
Iteration 11/25 | Loss: 0.00085838
Iteration 12/25 | Loss: 0.00081719
Iteration 13/25 | Loss: 0.00080334
Iteration 14/25 | Loss: 0.00080148
Iteration 15/25 | Loss: 0.00080086
Iteration 16/25 | Loss: 0.00080052
Iteration 17/25 | Loss: 0.00080038
Iteration 18/25 | Loss: 0.00080017
Iteration 19/25 | Loss: 0.00080302
Iteration 20/25 | Loss: 0.00079757
Iteration 21/25 | Loss: 0.00079660
Iteration 22/25 | Loss: 0.00079649
Iteration 23/25 | Loss: 0.00079649
Iteration 24/25 | Loss: 0.00079649
Iteration 25/25 | Loss: 0.00079649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43729258
Iteration 2/25 | Loss: 0.00027732
Iteration 3/25 | Loss: 0.00027732
Iteration 4/25 | Loss: 0.00027732
Iteration 5/25 | Loss: 0.00027732
Iteration 6/25 | Loss: 0.00027732
Iteration 7/25 | Loss: 0.00027731
Iteration 8/25 | Loss: 0.00027731
Iteration 9/25 | Loss: 0.00027731
Iteration 10/25 | Loss: 0.00027731
Iteration 11/25 | Loss: 0.00027731
Iteration 12/25 | Loss: 0.00027731
Iteration 13/25 | Loss: 0.00027731
Iteration 14/25 | Loss: 0.00027731
Iteration 15/25 | Loss: 0.00027731
Iteration 16/25 | Loss: 0.00027731
Iteration 17/25 | Loss: 0.00027731
Iteration 18/25 | Loss: 0.00027731
Iteration 19/25 | Loss: 0.00027731
Iteration 20/25 | Loss: 0.00027731
Iteration 21/25 | Loss: 0.00027731
Iteration 22/25 | Loss: 0.00027731
Iteration 23/25 | Loss: 0.00027731
Iteration 24/25 | Loss: 0.00027731
Iteration 25/25 | Loss: 0.00027731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027731
Iteration 2/1000 | Loss: 0.00003333
Iteration 3/1000 | Loss: 0.00002599
Iteration 4/1000 | Loss: 0.00002466
Iteration 5/1000 | Loss: 0.00002355
Iteration 6/1000 | Loss: 0.00002283
Iteration 7/1000 | Loss: 0.00002232
Iteration 8/1000 | Loss: 0.00002204
Iteration 9/1000 | Loss: 0.00002197
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002149
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002136
Iteration 14/1000 | Loss: 0.00002135
Iteration 15/1000 | Loss: 0.00002135
Iteration 16/1000 | Loss: 0.00002134
Iteration 17/1000 | Loss: 0.00002134
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002129
Iteration 22/1000 | Loss: 0.00002129
Iteration 23/1000 | Loss: 0.00002129
Iteration 24/1000 | Loss: 0.00002129
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002123
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002116
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002114
Iteration 35/1000 | Loss: 0.00002114
Iteration 36/1000 | Loss: 0.00002113
Iteration 37/1000 | Loss: 0.00002113
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002112
Iteration 43/1000 | Loss: 0.00002110
Iteration 44/1000 | Loss: 0.00002109
Iteration 45/1000 | Loss: 0.00002109
Iteration 46/1000 | Loss: 0.00002109
Iteration 47/1000 | Loss: 0.00002109
Iteration 48/1000 | Loss: 0.00002109
Iteration 49/1000 | Loss: 0.00002109
Iteration 50/1000 | Loss: 0.00002108
Iteration 51/1000 | Loss: 0.00002108
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002108
Iteration 54/1000 | Loss: 0.00002108
Iteration 55/1000 | Loss: 0.00002107
Iteration 56/1000 | Loss: 0.00002107
Iteration 57/1000 | Loss: 0.00002107
Iteration 58/1000 | Loss: 0.00002106
Iteration 59/1000 | Loss: 0.00002106
Iteration 60/1000 | Loss: 0.00002106
Iteration 61/1000 | Loss: 0.00002105
Iteration 62/1000 | Loss: 0.00002105
Iteration 63/1000 | Loss: 0.00002105
Iteration 64/1000 | Loss: 0.00002102
Iteration 65/1000 | Loss: 0.00002102
Iteration 66/1000 | Loss: 0.00002102
Iteration 67/1000 | Loss: 0.00002102
Iteration 68/1000 | Loss: 0.00002102
Iteration 69/1000 | Loss: 0.00002102
Iteration 70/1000 | Loss: 0.00002102
Iteration 71/1000 | Loss: 0.00002102
Iteration 72/1000 | Loss: 0.00002102
Iteration 73/1000 | Loss: 0.00002101
Iteration 74/1000 | Loss: 0.00002101
Iteration 75/1000 | Loss: 0.00002101
Iteration 76/1000 | Loss: 0.00002101
Iteration 77/1000 | Loss: 0.00002101
Iteration 78/1000 | Loss: 0.00002101
Iteration 79/1000 | Loss: 0.00002098
Iteration 80/1000 | Loss: 0.00002098
Iteration 81/1000 | Loss: 0.00002098
Iteration 82/1000 | Loss: 0.00002097
Iteration 83/1000 | Loss: 0.00002097
Iteration 84/1000 | Loss: 0.00002097
Iteration 85/1000 | Loss: 0.00002096
Iteration 86/1000 | Loss: 0.00002096
Iteration 87/1000 | Loss: 0.00002096
Iteration 88/1000 | Loss: 0.00002095
Iteration 89/1000 | Loss: 0.00002094
Iteration 90/1000 | Loss: 0.00002094
Iteration 91/1000 | Loss: 0.00002094
Iteration 92/1000 | Loss: 0.00002094
Iteration 93/1000 | Loss: 0.00002094
Iteration 94/1000 | Loss: 0.00002094
Iteration 95/1000 | Loss: 0.00002094
Iteration 96/1000 | Loss: 0.00002093
Iteration 97/1000 | Loss: 0.00002093
Iteration 98/1000 | Loss: 0.00002093
Iteration 99/1000 | Loss: 0.00002093
Iteration 100/1000 | Loss: 0.00002093
Iteration 101/1000 | Loss: 0.00002093
Iteration 102/1000 | Loss: 0.00002093
Iteration 103/1000 | Loss: 0.00002092
Iteration 104/1000 | Loss: 0.00002092
Iteration 105/1000 | Loss: 0.00002092
Iteration 106/1000 | Loss: 0.00002092
Iteration 107/1000 | Loss: 0.00002092
Iteration 108/1000 | Loss: 0.00002092
Iteration 109/1000 | Loss: 0.00002092
Iteration 110/1000 | Loss: 0.00002092
Iteration 111/1000 | Loss: 0.00002092
Iteration 112/1000 | Loss: 0.00002092
Iteration 113/1000 | Loss: 0.00002092
Iteration 114/1000 | Loss: 0.00002091
Iteration 115/1000 | Loss: 0.00002091
Iteration 116/1000 | Loss: 0.00002091
Iteration 117/1000 | Loss: 0.00002091
Iteration 118/1000 | Loss: 0.00002091
Iteration 119/1000 | Loss: 0.00002091
Iteration 120/1000 | Loss: 0.00002091
Iteration 121/1000 | Loss: 0.00002091
Iteration 122/1000 | Loss: 0.00002090
Iteration 123/1000 | Loss: 0.00002090
Iteration 124/1000 | Loss: 0.00002090
Iteration 125/1000 | Loss: 0.00002090
Iteration 126/1000 | Loss: 0.00002090
Iteration 127/1000 | Loss: 0.00002090
Iteration 128/1000 | Loss: 0.00002090
Iteration 129/1000 | Loss: 0.00002090
Iteration 130/1000 | Loss: 0.00002090
Iteration 131/1000 | Loss: 0.00002090
Iteration 132/1000 | Loss: 0.00002090
Iteration 133/1000 | Loss: 0.00002090
Iteration 134/1000 | Loss: 0.00002090
Iteration 135/1000 | Loss: 0.00002090
Iteration 136/1000 | Loss: 0.00002090
Iteration 137/1000 | Loss: 0.00002090
Iteration 138/1000 | Loss: 0.00002090
Iteration 139/1000 | Loss: 0.00002089
Iteration 140/1000 | Loss: 0.00002089
Iteration 141/1000 | Loss: 0.00002089
Iteration 142/1000 | Loss: 0.00002089
Iteration 143/1000 | Loss: 0.00002089
Iteration 144/1000 | Loss: 0.00002089
Iteration 145/1000 | Loss: 0.00002088
Iteration 146/1000 | Loss: 0.00002088
Iteration 147/1000 | Loss: 0.00002088
Iteration 148/1000 | Loss: 0.00002088
Iteration 149/1000 | Loss: 0.00002088
Iteration 150/1000 | Loss: 0.00002088
Iteration 151/1000 | Loss: 0.00002088
Iteration 152/1000 | Loss: 0.00002088
Iteration 153/1000 | Loss: 0.00002088
Iteration 154/1000 | Loss: 0.00002088
Iteration 155/1000 | Loss: 0.00002088
Iteration 156/1000 | Loss: 0.00002088
Iteration 157/1000 | Loss: 0.00002088
Iteration 158/1000 | Loss: 0.00002088
Iteration 159/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.088252585963346e-05, 2.088252585963346e-05, 2.088252585963346e-05, 2.088252585963346e-05, 2.088252585963346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.088252585963346e-05

Optimization complete. Final v2v error: 3.7993972301483154 mm

Highest mean error: 4.163325786590576 mm for frame 239

Lowest mean error: 3.643882989883423 mm for frame 119

Saving results

Total time: 75.24038195610046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850504
Iteration 2/25 | Loss: 0.00087361
Iteration 3/25 | Loss: 0.00072331
Iteration 4/25 | Loss: 0.00070006
Iteration 5/25 | Loss: 0.00069384
Iteration 6/25 | Loss: 0.00069196
Iteration 7/25 | Loss: 0.00069171
Iteration 8/25 | Loss: 0.00069171
Iteration 9/25 | Loss: 0.00069171
Iteration 10/25 | Loss: 0.00069171
Iteration 11/25 | Loss: 0.00069171
Iteration 12/25 | Loss: 0.00069171
Iteration 13/25 | Loss: 0.00069171
Iteration 14/25 | Loss: 0.00069171
Iteration 15/25 | Loss: 0.00069171
Iteration 16/25 | Loss: 0.00069171
Iteration 17/25 | Loss: 0.00069171
Iteration 18/25 | Loss: 0.00069171
Iteration 19/25 | Loss: 0.00069171
Iteration 20/25 | Loss: 0.00069171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006917114369571209, 0.0006917114369571209, 0.0006917114369571209, 0.0006917114369571209, 0.0006917114369571209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006917114369571209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45590079
Iteration 2/25 | Loss: 0.00021351
Iteration 3/25 | Loss: 0.00021349
Iteration 4/25 | Loss: 0.00021349
Iteration 5/25 | Loss: 0.00021349
Iteration 6/25 | Loss: 0.00021349
Iteration 7/25 | Loss: 0.00021349
Iteration 8/25 | Loss: 0.00021349
Iteration 9/25 | Loss: 0.00021349
Iteration 10/25 | Loss: 0.00021349
Iteration 11/25 | Loss: 0.00021349
Iteration 12/25 | Loss: 0.00021349
Iteration 13/25 | Loss: 0.00021349
Iteration 14/25 | Loss: 0.00021349
Iteration 15/25 | Loss: 0.00021349
Iteration 16/25 | Loss: 0.00021349
Iteration 17/25 | Loss: 0.00021349
Iteration 18/25 | Loss: 0.00021349
Iteration 19/25 | Loss: 0.00021349
Iteration 20/25 | Loss: 0.00021349
Iteration 21/25 | Loss: 0.00021349
Iteration 22/25 | Loss: 0.00021349
Iteration 23/25 | Loss: 0.00021349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00021348796144593507, 0.00021348796144593507, 0.00021348796144593507, 0.00021348796144593507, 0.00021348796144593507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021348796144593507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021349
Iteration 2/1000 | Loss: 0.00002093
Iteration 3/1000 | Loss: 0.00001558
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001227
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001223
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001207
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001200
Iteration 16/1000 | Loss: 0.00001200
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001187
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001186
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001185
Iteration 36/1000 | Loss: 0.00001185
Iteration 37/1000 | Loss: 0.00001184
Iteration 38/1000 | Loss: 0.00001184
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001183
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001183
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001182
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001180
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001179
Iteration 63/1000 | Loss: 0.00001179
Iteration 64/1000 | Loss: 0.00001179
Iteration 65/1000 | Loss: 0.00001178
Iteration 66/1000 | Loss: 0.00001178
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001177
Iteration 74/1000 | Loss: 0.00001177
Iteration 75/1000 | Loss: 0.00001177
Iteration 76/1000 | Loss: 0.00001176
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001176
Iteration 80/1000 | Loss: 0.00001176
Iteration 81/1000 | Loss: 0.00001176
Iteration 82/1000 | Loss: 0.00001176
Iteration 83/1000 | Loss: 0.00001176
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001175
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001174
Iteration 96/1000 | Loss: 0.00001174
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001173
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001173
Iteration 126/1000 | Loss: 0.00001173
Iteration 127/1000 | Loss: 0.00001173
Iteration 128/1000 | Loss: 0.00001173
Iteration 129/1000 | Loss: 0.00001173
Iteration 130/1000 | Loss: 0.00001173
Iteration 131/1000 | Loss: 0.00001173
Iteration 132/1000 | Loss: 0.00001173
Iteration 133/1000 | Loss: 0.00001172
Iteration 134/1000 | Loss: 0.00001172
Iteration 135/1000 | Loss: 0.00001172
Iteration 136/1000 | Loss: 0.00001172
Iteration 137/1000 | Loss: 0.00001172
Iteration 138/1000 | Loss: 0.00001172
Iteration 139/1000 | Loss: 0.00001172
Iteration 140/1000 | Loss: 0.00001172
Iteration 141/1000 | Loss: 0.00001172
Iteration 142/1000 | Loss: 0.00001172
Iteration 143/1000 | Loss: 0.00001172
Iteration 144/1000 | Loss: 0.00001172
Iteration 145/1000 | Loss: 0.00001172
Iteration 146/1000 | Loss: 0.00001172
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001171
Iteration 165/1000 | Loss: 0.00001171
Iteration 166/1000 | Loss: 0.00001171
Iteration 167/1000 | Loss: 0.00001171
Iteration 168/1000 | Loss: 0.00001170
Iteration 169/1000 | Loss: 0.00001170
Iteration 170/1000 | Loss: 0.00001170
Iteration 171/1000 | Loss: 0.00001170
Iteration 172/1000 | Loss: 0.00001170
Iteration 173/1000 | Loss: 0.00001170
Iteration 174/1000 | Loss: 0.00001170
Iteration 175/1000 | Loss: 0.00001170
Iteration 176/1000 | Loss: 0.00001170
Iteration 177/1000 | Loss: 0.00001170
Iteration 178/1000 | Loss: 0.00001170
Iteration 179/1000 | Loss: 0.00001170
Iteration 180/1000 | Loss: 0.00001170
Iteration 181/1000 | Loss: 0.00001170
Iteration 182/1000 | Loss: 0.00001169
Iteration 183/1000 | Loss: 0.00001169
Iteration 184/1000 | Loss: 0.00001169
Iteration 185/1000 | Loss: 0.00001169
Iteration 186/1000 | Loss: 0.00001169
Iteration 187/1000 | Loss: 0.00001169
Iteration 188/1000 | Loss: 0.00001169
Iteration 189/1000 | Loss: 0.00001169
Iteration 190/1000 | Loss: 0.00001169
Iteration 191/1000 | Loss: 0.00001169
Iteration 192/1000 | Loss: 0.00001169
Iteration 193/1000 | Loss: 0.00001169
Iteration 194/1000 | Loss: 0.00001169
Iteration 195/1000 | Loss: 0.00001169
Iteration 196/1000 | Loss: 0.00001169
Iteration 197/1000 | Loss: 0.00001168
Iteration 198/1000 | Loss: 0.00001168
Iteration 199/1000 | Loss: 0.00001168
Iteration 200/1000 | Loss: 0.00001168
Iteration 201/1000 | Loss: 0.00001168
Iteration 202/1000 | Loss: 0.00001168
Iteration 203/1000 | Loss: 0.00001168
Iteration 204/1000 | Loss: 0.00001168
Iteration 205/1000 | Loss: 0.00001168
Iteration 206/1000 | Loss: 0.00001167
Iteration 207/1000 | Loss: 0.00001167
Iteration 208/1000 | Loss: 0.00001167
Iteration 209/1000 | Loss: 0.00001167
Iteration 210/1000 | Loss: 0.00001167
Iteration 211/1000 | Loss: 0.00001167
Iteration 212/1000 | Loss: 0.00001167
Iteration 213/1000 | Loss: 0.00001167
Iteration 214/1000 | Loss: 0.00001167
Iteration 215/1000 | Loss: 0.00001167
Iteration 216/1000 | Loss: 0.00001166
Iteration 217/1000 | Loss: 0.00001166
Iteration 218/1000 | Loss: 0.00001166
Iteration 219/1000 | Loss: 0.00001166
Iteration 220/1000 | Loss: 0.00001166
Iteration 221/1000 | Loss: 0.00001166
Iteration 222/1000 | Loss: 0.00001166
Iteration 223/1000 | Loss: 0.00001166
Iteration 224/1000 | Loss: 0.00001166
Iteration 225/1000 | Loss: 0.00001166
Iteration 226/1000 | Loss: 0.00001166
Iteration 227/1000 | Loss: 0.00001166
Iteration 228/1000 | Loss: 0.00001166
Iteration 229/1000 | Loss: 0.00001166
Iteration 230/1000 | Loss: 0.00001165
Iteration 231/1000 | Loss: 0.00001165
Iteration 232/1000 | Loss: 0.00001165
Iteration 233/1000 | Loss: 0.00001165
Iteration 234/1000 | Loss: 0.00001165
Iteration 235/1000 | Loss: 0.00001165
Iteration 236/1000 | Loss: 0.00001165
Iteration 237/1000 | Loss: 0.00001165
Iteration 238/1000 | Loss: 0.00001165
Iteration 239/1000 | Loss: 0.00001165
Iteration 240/1000 | Loss: 0.00001165
Iteration 241/1000 | Loss: 0.00001165
Iteration 242/1000 | Loss: 0.00001165
Iteration 243/1000 | Loss: 0.00001165
Iteration 244/1000 | Loss: 0.00001165
Iteration 245/1000 | Loss: 0.00001165
Iteration 246/1000 | Loss: 0.00001165
Iteration 247/1000 | Loss: 0.00001165
Iteration 248/1000 | Loss: 0.00001165
Iteration 249/1000 | Loss: 0.00001165
Iteration 250/1000 | Loss: 0.00001165
Iteration 251/1000 | Loss: 0.00001165
Iteration 252/1000 | Loss: 0.00001165
Iteration 253/1000 | Loss: 0.00001164
Iteration 254/1000 | Loss: 0.00001164
Iteration 255/1000 | Loss: 0.00001164
Iteration 256/1000 | Loss: 0.00001164
Iteration 257/1000 | Loss: 0.00001164
Iteration 258/1000 | Loss: 0.00001164
Iteration 259/1000 | Loss: 0.00001164
Iteration 260/1000 | Loss: 0.00001164
Iteration 261/1000 | Loss: 0.00001164
Iteration 262/1000 | Loss: 0.00001164
Iteration 263/1000 | Loss: 0.00001164
Iteration 264/1000 | Loss: 0.00001164
Iteration 265/1000 | Loss: 0.00001164
Iteration 266/1000 | Loss: 0.00001164
Iteration 267/1000 | Loss: 0.00001164
Iteration 268/1000 | Loss: 0.00001164
Iteration 269/1000 | Loss: 0.00001164
Iteration 270/1000 | Loss: 0.00001164
Iteration 271/1000 | Loss: 0.00001164
Iteration 272/1000 | Loss: 0.00001164
Iteration 273/1000 | Loss: 0.00001164
Iteration 274/1000 | Loss: 0.00001164
Iteration 275/1000 | Loss: 0.00001163
Iteration 276/1000 | Loss: 0.00001163
Iteration 277/1000 | Loss: 0.00001163
Iteration 278/1000 | Loss: 0.00001163
Iteration 279/1000 | Loss: 0.00001163
Iteration 280/1000 | Loss: 0.00001163
Iteration 281/1000 | Loss: 0.00001163
Iteration 282/1000 | Loss: 0.00001163
Iteration 283/1000 | Loss: 0.00001163
Iteration 284/1000 | Loss: 0.00001163
Iteration 285/1000 | Loss: 0.00001163
Iteration 286/1000 | Loss: 0.00001163
Iteration 287/1000 | Loss: 0.00001163
Iteration 288/1000 | Loss: 0.00001163
Iteration 289/1000 | Loss: 0.00001163
Iteration 290/1000 | Loss: 0.00001163
Iteration 291/1000 | Loss: 0.00001163
Iteration 292/1000 | Loss: 0.00001163
Iteration 293/1000 | Loss: 0.00001163
Iteration 294/1000 | Loss: 0.00001163
Iteration 295/1000 | Loss: 0.00001163
Iteration 296/1000 | Loss: 0.00001163
Iteration 297/1000 | Loss: 0.00001163
Iteration 298/1000 | Loss: 0.00001163
Iteration 299/1000 | Loss: 0.00001163
Iteration 300/1000 | Loss: 0.00001163
Iteration 301/1000 | Loss: 0.00001163
Iteration 302/1000 | Loss: 0.00001163
Iteration 303/1000 | Loss: 0.00001163
Iteration 304/1000 | Loss: 0.00001163
Iteration 305/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [1.163296929007629e-05, 1.163296929007629e-05, 1.163296929007629e-05, 1.163296929007629e-05, 1.163296929007629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.163296929007629e-05

Optimization complete. Final v2v error: 2.899847984313965 mm

Highest mean error: 3.0700299739837646 mm for frame 123

Lowest mean error: 2.795332193374634 mm for frame 38

Saving results

Total time: 39.624746322631836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00682194
Iteration 2/25 | Loss: 0.00116718
Iteration 3/25 | Loss: 0.00086101
Iteration 4/25 | Loss: 0.00080557
Iteration 5/25 | Loss: 0.00079769
Iteration 6/25 | Loss: 0.00079556
Iteration 7/25 | Loss: 0.00079543
Iteration 8/25 | Loss: 0.00079543
Iteration 9/25 | Loss: 0.00079543
Iteration 10/25 | Loss: 0.00079543
Iteration 11/25 | Loss: 0.00079543
Iteration 12/25 | Loss: 0.00079543
Iteration 13/25 | Loss: 0.00079543
Iteration 14/25 | Loss: 0.00079543
Iteration 15/25 | Loss: 0.00079543
Iteration 16/25 | Loss: 0.00079543
Iteration 17/25 | Loss: 0.00079543
Iteration 18/25 | Loss: 0.00079543
Iteration 19/25 | Loss: 0.00079543
Iteration 20/25 | Loss: 0.00079543
Iteration 21/25 | Loss: 0.00079543
Iteration 22/25 | Loss: 0.00079543
Iteration 23/25 | Loss: 0.00079543
Iteration 24/25 | Loss: 0.00079543
Iteration 25/25 | Loss: 0.00079543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92044568
Iteration 2/25 | Loss: 0.00028048
Iteration 3/25 | Loss: 0.00028046
Iteration 4/25 | Loss: 0.00028046
Iteration 5/25 | Loss: 0.00028046
Iteration 6/25 | Loss: 0.00028046
Iteration 7/25 | Loss: 0.00028046
Iteration 8/25 | Loss: 0.00028046
Iteration 9/25 | Loss: 0.00028046
Iteration 10/25 | Loss: 0.00028046
Iteration 11/25 | Loss: 0.00028046
Iteration 12/25 | Loss: 0.00028046
Iteration 13/25 | Loss: 0.00028046
Iteration 14/25 | Loss: 0.00028046
Iteration 15/25 | Loss: 0.00028046
Iteration 16/25 | Loss: 0.00028046
Iteration 17/25 | Loss: 0.00028046
Iteration 18/25 | Loss: 0.00028046
Iteration 19/25 | Loss: 0.00028046
Iteration 20/25 | Loss: 0.00028046
Iteration 21/25 | Loss: 0.00028046
Iteration 22/25 | Loss: 0.00028046
Iteration 23/25 | Loss: 0.00028046
Iteration 24/25 | Loss: 0.00028046
Iteration 25/25 | Loss: 0.00028046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028046
Iteration 2/1000 | Loss: 0.00003135
Iteration 3/1000 | Loss: 0.00002364
Iteration 4/1000 | Loss: 0.00002211
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00002022
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001963
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001946
Iteration 16/1000 | Loss: 0.00001946
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001943
Iteration 19/1000 | Loss: 0.00001943
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001937
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001935
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001934
Iteration 26/1000 | Loss: 0.00001934
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001933
Iteration 29/1000 | Loss: 0.00001932
Iteration 30/1000 | Loss: 0.00001932
Iteration 31/1000 | Loss: 0.00001932
Iteration 32/1000 | Loss: 0.00001931
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001928
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00001927
Iteration 50/1000 | Loss: 0.00001927
Iteration 51/1000 | Loss: 0.00001926
Iteration 52/1000 | Loss: 0.00001926
Iteration 53/1000 | Loss: 0.00001926
Iteration 54/1000 | Loss: 0.00001926
Iteration 55/1000 | Loss: 0.00001925
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001925
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001925
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001924
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001923
Iteration 73/1000 | Loss: 0.00001923
Iteration 74/1000 | Loss: 0.00001923
Iteration 75/1000 | Loss: 0.00001923
Iteration 76/1000 | Loss: 0.00001923
Iteration 77/1000 | Loss: 0.00001923
Iteration 78/1000 | Loss: 0.00001923
Iteration 79/1000 | Loss: 0.00001923
Iteration 80/1000 | Loss: 0.00001922
Iteration 81/1000 | Loss: 0.00001922
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001922
Iteration 85/1000 | Loss: 0.00001922
Iteration 86/1000 | Loss: 0.00001922
Iteration 87/1000 | Loss: 0.00001922
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001921
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001920
Iteration 100/1000 | Loss: 0.00001920
Iteration 101/1000 | Loss: 0.00001920
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001919
Iteration 108/1000 | Loss: 0.00001919
Iteration 109/1000 | Loss: 0.00001919
Iteration 110/1000 | Loss: 0.00001919
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00001919
Iteration 113/1000 | Loss: 0.00001918
Iteration 114/1000 | Loss: 0.00001918
Iteration 115/1000 | Loss: 0.00001918
Iteration 116/1000 | Loss: 0.00001918
Iteration 117/1000 | Loss: 0.00001918
Iteration 118/1000 | Loss: 0.00001918
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001917
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001917
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001916
Iteration 127/1000 | Loss: 0.00001916
Iteration 128/1000 | Loss: 0.00001916
Iteration 129/1000 | Loss: 0.00001916
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001915
Iteration 132/1000 | Loss: 0.00001915
Iteration 133/1000 | Loss: 0.00001915
Iteration 134/1000 | Loss: 0.00001915
Iteration 135/1000 | Loss: 0.00001915
Iteration 136/1000 | Loss: 0.00001914
Iteration 137/1000 | Loss: 0.00001914
Iteration 138/1000 | Loss: 0.00001914
Iteration 139/1000 | Loss: 0.00001914
Iteration 140/1000 | Loss: 0.00001914
Iteration 141/1000 | Loss: 0.00001914
Iteration 142/1000 | Loss: 0.00001914
Iteration 143/1000 | Loss: 0.00001913
Iteration 144/1000 | Loss: 0.00001913
Iteration 145/1000 | Loss: 0.00001913
Iteration 146/1000 | Loss: 0.00001913
Iteration 147/1000 | Loss: 0.00001913
Iteration 148/1000 | Loss: 0.00001913
Iteration 149/1000 | Loss: 0.00001913
Iteration 150/1000 | Loss: 0.00001913
Iteration 151/1000 | Loss: 0.00001912
Iteration 152/1000 | Loss: 0.00001912
Iteration 153/1000 | Loss: 0.00001912
Iteration 154/1000 | Loss: 0.00001912
Iteration 155/1000 | Loss: 0.00001912
Iteration 156/1000 | Loss: 0.00001912
Iteration 157/1000 | Loss: 0.00001911
Iteration 158/1000 | Loss: 0.00001911
Iteration 159/1000 | Loss: 0.00001911
Iteration 160/1000 | Loss: 0.00001911
Iteration 161/1000 | Loss: 0.00001911
Iteration 162/1000 | Loss: 0.00001911
Iteration 163/1000 | Loss: 0.00001911
Iteration 164/1000 | Loss: 0.00001911
Iteration 165/1000 | Loss: 0.00001911
Iteration 166/1000 | Loss: 0.00001911
Iteration 167/1000 | Loss: 0.00001911
Iteration 168/1000 | Loss: 0.00001911
Iteration 169/1000 | Loss: 0.00001911
Iteration 170/1000 | Loss: 0.00001910
Iteration 171/1000 | Loss: 0.00001910
Iteration 172/1000 | Loss: 0.00001910
Iteration 173/1000 | Loss: 0.00001910
Iteration 174/1000 | Loss: 0.00001910
Iteration 175/1000 | Loss: 0.00001910
Iteration 176/1000 | Loss: 0.00001910
Iteration 177/1000 | Loss: 0.00001909
Iteration 178/1000 | Loss: 0.00001909
Iteration 179/1000 | Loss: 0.00001909
Iteration 180/1000 | Loss: 0.00001909
Iteration 181/1000 | Loss: 0.00001909
Iteration 182/1000 | Loss: 0.00001909
Iteration 183/1000 | Loss: 0.00001909
Iteration 184/1000 | Loss: 0.00001909
Iteration 185/1000 | Loss: 0.00001909
Iteration 186/1000 | Loss: 0.00001909
Iteration 187/1000 | Loss: 0.00001909
Iteration 188/1000 | Loss: 0.00001909
Iteration 189/1000 | Loss: 0.00001909
Iteration 190/1000 | Loss: 0.00001909
Iteration 191/1000 | Loss: 0.00001909
Iteration 192/1000 | Loss: 0.00001908
Iteration 193/1000 | Loss: 0.00001908
Iteration 194/1000 | Loss: 0.00001908
Iteration 195/1000 | Loss: 0.00001908
Iteration 196/1000 | Loss: 0.00001908
Iteration 197/1000 | Loss: 0.00001908
Iteration 198/1000 | Loss: 0.00001908
Iteration 199/1000 | Loss: 0.00001908
Iteration 200/1000 | Loss: 0.00001908
Iteration 201/1000 | Loss: 0.00001908
Iteration 202/1000 | Loss: 0.00001908
Iteration 203/1000 | Loss: 0.00001907
Iteration 204/1000 | Loss: 0.00001907
Iteration 205/1000 | Loss: 0.00001907
Iteration 206/1000 | Loss: 0.00001907
Iteration 207/1000 | Loss: 0.00001907
Iteration 208/1000 | Loss: 0.00001907
Iteration 209/1000 | Loss: 0.00001907
Iteration 210/1000 | Loss: 0.00001907
Iteration 211/1000 | Loss: 0.00001907
Iteration 212/1000 | Loss: 0.00001907
Iteration 213/1000 | Loss: 0.00001907
Iteration 214/1000 | Loss: 0.00001907
Iteration 215/1000 | Loss: 0.00001907
Iteration 216/1000 | Loss: 0.00001907
Iteration 217/1000 | Loss: 0.00001907
Iteration 218/1000 | Loss: 0.00001907
Iteration 219/1000 | Loss: 0.00001906
Iteration 220/1000 | Loss: 0.00001906
Iteration 221/1000 | Loss: 0.00001906
Iteration 222/1000 | Loss: 0.00001906
Iteration 223/1000 | Loss: 0.00001906
Iteration 224/1000 | Loss: 0.00001906
Iteration 225/1000 | Loss: 0.00001906
Iteration 226/1000 | Loss: 0.00001906
Iteration 227/1000 | Loss: 0.00001906
Iteration 228/1000 | Loss: 0.00001906
Iteration 229/1000 | Loss: 0.00001906
Iteration 230/1000 | Loss: 0.00001906
Iteration 231/1000 | Loss: 0.00001906
Iteration 232/1000 | Loss: 0.00001906
Iteration 233/1000 | Loss: 0.00001906
Iteration 234/1000 | Loss: 0.00001906
Iteration 235/1000 | Loss: 0.00001906
Iteration 236/1000 | Loss: 0.00001906
Iteration 237/1000 | Loss: 0.00001906
Iteration 238/1000 | Loss: 0.00001905
Iteration 239/1000 | Loss: 0.00001905
Iteration 240/1000 | Loss: 0.00001905
Iteration 241/1000 | Loss: 0.00001905
Iteration 242/1000 | Loss: 0.00001905
Iteration 243/1000 | Loss: 0.00001905
Iteration 244/1000 | Loss: 0.00001905
Iteration 245/1000 | Loss: 0.00001905
Iteration 246/1000 | Loss: 0.00001905
Iteration 247/1000 | Loss: 0.00001905
Iteration 248/1000 | Loss: 0.00001905
Iteration 249/1000 | Loss: 0.00001905
Iteration 250/1000 | Loss: 0.00001905
Iteration 251/1000 | Loss: 0.00001905
Iteration 252/1000 | Loss: 0.00001905
Iteration 253/1000 | Loss: 0.00001905
Iteration 254/1000 | Loss: 0.00001905
Iteration 255/1000 | Loss: 0.00001905
Iteration 256/1000 | Loss: 0.00001905
Iteration 257/1000 | Loss: 0.00001904
Iteration 258/1000 | Loss: 0.00001904
Iteration 259/1000 | Loss: 0.00001904
Iteration 260/1000 | Loss: 0.00001904
Iteration 261/1000 | Loss: 0.00001904
Iteration 262/1000 | Loss: 0.00001904
Iteration 263/1000 | Loss: 0.00001904
Iteration 264/1000 | Loss: 0.00001904
Iteration 265/1000 | Loss: 0.00001904
Iteration 266/1000 | Loss: 0.00001904
Iteration 267/1000 | Loss: 0.00001904
Iteration 268/1000 | Loss: 0.00001904
Iteration 269/1000 | Loss: 0.00001904
Iteration 270/1000 | Loss: 0.00001904
Iteration 271/1000 | Loss: 0.00001904
Iteration 272/1000 | Loss: 0.00001904
Iteration 273/1000 | Loss: 0.00001904
Iteration 274/1000 | Loss: 0.00001904
Iteration 275/1000 | Loss: 0.00001904
Iteration 276/1000 | Loss: 0.00001904
Iteration 277/1000 | Loss: 0.00001904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.904457349155564e-05, 1.904457349155564e-05, 1.904457349155564e-05, 1.904457349155564e-05, 1.904457349155564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.904457349155564e-05

Optimization complete. Final v2v error: 3.6049065589904785 mm

Highest mean error: 4.776629447937012 mm for frame 92

Lowest mean error: 2.9294662475585938 mm for frame 4

Saving results

Total time: 48.764734983444214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823200
Iteration 2/25 | Loss: 0.00157730
Iteration 3/25 | Loss: 0.00129845
Iteration 4/25 | Loss: 0.00122715
Iteration 5/25 | Loss: 0.00140996
Iteration 6/25 | Loss: 0.00116109
Iteration 7/25 | Loss: 0.00109044
Iteration 8/25 | Loss: 0.00094331
Iteration 9/25 | Loss: 0.00091269
Iteration 10/25 | Loss: 0.00090049
Iteration 11/25 | Loss: 0.00078114
Iteration 12/25 | Loss: 0.00077763
Iteration 13/25 | Loss: 0.00078348
Iteration 14/25 | Loss: 0.00076789
Iteration 15/25 | Loss: 0.00074926
Iteration 16/25 | Loss: 0.00074348
Iteration 17/25 | Loss: 0.00074306
Iteration 18/25 | Loss: 0.00074291
Iteration 19/25 | Loss: 0.00074985
Iteration 20/25 | Loss: 0.00073795
Iteration 21/25 | Loss: 0.00073843
Iteration 22/25 | Loss: 0.00073389
Iteration 23/25 | Loss: 0.00073376
Iteration 24/25 | Loss: 0.00073375
Iteration 25/25 | Loss: 0.00073375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45288587
Iteration 2/25 | Loss: 0.00025472
Iteration 3/25 | Loss: 0.00025472
Iteration 4/25 | Loss: 0.00025472
Iteration 5/25 | Loss: 0.00025472
Iteration 6/25 | Loss: 0.00025472
Iteration 7/25 | Loss: 0.00025472
Iteration 8/25 | Loss: 0.00025472
Iteration 9/25 | Loss: 0.00025472
Iteration 10/25 | Loss: 0.00025472
Iteration 11/25 | Loss: 0.00025472
Iteration 12/25 | Loss: 0.00025472
Iteration 13/25 | Loss: 0.00025472
Iteration 14/25 | Loss: 0.00025472
Iteration 15/25 | Loss: 0.00025472
Iteration 16/25 | Loss: 0.00025472
Iteration 17/25 | Loss: 0.00025472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00025471978005953133, 0.00025471978005953133, 0.00025471978005953133, 0.00025471978005953133, 0.00025471978005953133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025471978005953133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025472
Iteration 2/1000 | Loss: 0.00002512
Iteration 3/1000 | Loss: 0.00001923
Iteration 4/1000 | Loss: 0.00001789
Iteration 5/1000 | Loss: 0.00001706
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001622
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001560
Iteration 12/1000 | Loss: 0.00001556
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001551
Iteration 16/1000 | Loss: 0.00001549
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00001545
Iteration 19/1000 | Loss: 0.00001545
Iteration 20/1000 | Loss: 0.00001544
Iteration 21/1000 | Loss: 0.00001543
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001539
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00001538
Iteration 27/1000 | Loss: 0.00001538
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001536
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001535
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001534
Iteration 35/1000 | Loss: 0.00001534
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001532
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001530
Iteration 48/1000 | Loss: 0.00001530
Iteration 49/1000 | Loss: 0.00001530
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001529
Iteration 52/1000 | Loss: 0.00001529
Iteration 53/1000 | Loss: 0.00001529
Iteration 54/1000 | Loss: 0.00001529
Iteration 55/1000 | Loss: 0.00001529
Iteration 56/1000 | Loss: 0.00001528
Iteration 57/1000 | Loss: 0.00001528
Iteration 58/1000 | Loss: 0.00001528
Iteration 59/1000 | Loss: 0.00001528
Iteration 60/1000 | Loss: 0.00001528
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001521
Iteration 113/1000 | Loss: 0.00001521
Iteration 114/1000 | Loss: 0.00001521
Iteration 115/1000 | Loss: 0.00001521
Iteration 116/1000 | Loss: 0.00001521
Iteration 117/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5211009667837061e-05, 1.5211009667837061e-05, 1.5211009667837061e-05, 1.5211009667837061e-05, 1.5211009667837061e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5211009667837061e-05

Optimization complete. Final v2v error: 3.251788854598999 mm

Highest mean error: 3.9949514865875244 mm for frame 136

Lowest mean error: 2.6347544193267822 mm for frame 1

Saving results

Total time: 73.38525795936584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485950
Iteration 2/25 | Loss: 0.00088547
Iteration 3/25 | Loss: 0.00073897
Iteration 4/25 | Loss: 0.00071391
Iteration 5/25 | Loss: 0.00070366
Iteration 6/25 | Loss: 0.00070085
Iteration 7/25 | Loss: 0.00070026
Iteration 8/25 | Loss: 0.00070026
Iteration 9/25 | Loss: 0.00070026
Iteration 10/25 | Loss: 0.00070026
Iteration 11/25 | Loss: 0.00070026
Iteration 12/25 | Loss: 0.00070026
Iteration 13/25 | Loss: 0.00070026
Iteration 14/25 | Loss: 0.00070026
Iteration 15/25 | Loss: 0.00070026
Iteration 16/25 | Loss: 0.00070026
Iteration 17/25 | Loss: 0.00070026
Iteration 18/25 | Loss: 0.00070026
Iteration 19/25 | Loss: 0.00070026
Iteration 20/25 | Loss: 0.00070026
Iteration 21/25 | Loss: 0.00070026
Iteration 22/25 | Loss: 0.00070026
Iteration 23/25 | Loss: 0.00070026
Iteration 24/25 | Loss: 0.00070026
Iteration 25/25 | Loss: 0.00070026

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48411298
Iteration 2/25 | Loss: 0.00024778
Iteration 3/25 | Loss: 0.00024776
Iteration 4/25 | Loss: 0.00024776
Iteration 5/25 | Loss: 0.00024776
Iteration 6/25 | Loss: 0.00024776
Iteration 7/25 | Loss: 0.00024776
Iteration 8/25 | Loss: 0.00024776
Iteration 9/25 | Loss: 0.00024776
Iteration 10/25 | Loss: 0.00024776
Iteration 11/25 | Loss: 0.00024776
Iteration 12/25 | Loss: 0.00024776
Iteration 13/25 | Loss: 0.00024776
Iteration 14/25 | Loss: 0.00024776
Iteration 15/25 | Loss: 0.00024776
Iteration 16/25 | Loss: 0.00024776
Iteration 17/25 | Loss: 0.00024776
Iteration 18/25 | Loss: 0.00024776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00024775922065600753, 0.00024775922065600753, 0.00024775922065600753, 0.00024775922065600753, 0.00024775922065600753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024775922065600753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024776
Iteration 2/1000 | Loss: 0.00002267
Iteration 3/1000 | Loss: 0.00001518
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001294
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001265
Iteration 10/1000 | Loss: 0.00001258
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001251
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001239
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001238
Iteration 28/1000 | Loss: 0.00001237
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001221
Iteration 46/1000 | Loss: 0.00001221
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001219
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001219
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001216
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001216
Iteration 73/1000 | Loss: 0.00001216
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001215
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001215
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001215
Iteration 83/1000 | Loss: 0.00001215
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001215
Iteration 90/1000 | Loss: 0.00001215
Iteration 91/1000 | Loss: 0.00001215
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001215
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.2148762834840454e-05, 1.2148762834840454e-05, 1.2148762834840454e-05, 1.2148762834840454e-05, 1.2148762834840454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2148762834840454e-05

Optimization complete. Final v2v error: 2.9745936393737793 mm

Highest mean error: 3.5234196186065674 mm for frame 105

Lowest mean error: 2.670531988143921 mm for frame 198

Saving results

Total time: 35.76009511947632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00557888
Iteration 2/25 | Loss: 0.00114581
Iteration 3/25 | Loss: 0.00093981
Iteration 4/25 | Loss: 0.00089948
Iteration 5/25 | Loss: 0.00089393
Iteration 6/25 | Loss: 0.00089349
Iteration 7/25 | Loss: 0.00089349
Iteration 8/25 | Loss: 0.00089349
Iteration 9/25 | Loss: 0.00089349
Iteration 10/25 | Loss: 0.00089349
Iteration 11/25 | Loss: 0.00089349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008934949873946607, 0.0008934949873946607, 0.0008934949873946607, 0.0008934949873946607, 0.0008934949873946607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008934949873946607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50272584
Iteration 2/25 | Loss: 0.00034971
Iteration 3/25 | Loss: 0.00034971
Iteration 4/25 | Loss: 0.00034970
Iteration 5/25 | Loss: 0.00034970
Iteration 6/25 | Loss: 0.00034970
Iteration 7/25 | Loss: 0.00034970
Iteration 8/25 | Loss: 0.00034970
Iteration 9/25 | Loss: 0.00034970
Iteration 10/25 | Loss: 0.00034970
Iteration 11/25 | Loss: 0.00034970
Iteration 12/25 | Loss: 0.00034970
Iteration 13/25 | Loss: 0.00034970
Iteration 14/25 | Loss: 0.00034970
Iteration 15/25 | Loss: 0.00034970
Iteration 16/25 | Loss: 0.00034970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000349703012034297, 0.000349703012034297, 0.000349703012034297, 0.000349703012034297, 0.000349703012034297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000349703012034297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034970
Iteration 2/1000 | Loss: 0.00004165
Iteration 3/1000 | Loss: 0.00003070
Iteration 4/1000 | Loss: 0.00002908
Iteration 5/1000 | Loss: 0.00002826
Iteration 6/1000 | Loss: 0.00002757
Iteration 7/1000 | Loss: 0.00002714
Iteration 8/1000 | Loss: 0.00002681
Iteration 9/1000 | Loss: 0.00002658
Iteration 10/1000 | Loss: 0.00002655
Iteration 11/1000 | Loss: 0.00002640
Iteration 12/1000 | Loss: 0.00002630
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002628
Iteration 15/1000 | Loss: 0.00002628
Iteration 16/1000 | Loss: 0.00002628
Iteration 17/1000 | Loss: 0.00002627
Iteration 18/1000 | Loss: 0.00002627
Iteration 19/1000 | Loss: 0.00002627
Iteration 20/1000 | Loss: 0.00002626
Iteration 21/1000 | Loss: 0.00002626
Iteration 22/1000 | Loss: 0.00002626
Iteration 23/1000 | Loss: 0.00002626
Iteration 24/1000 | Loss: 0.00002626
Iteration 25/1000 | Loss: 0.00002622
Iteration 26/1000 | Loss: 0.00002622
Iteration 27/1000 | Loss: 0.00002621
Iteration 28/1000 | Loss: 0.00002620
Iteration 29/1000 | Loss: 0.00002620
Iteration 30/1000 | Loss: 0.00002620
Iteration 31/1000 | Loss: 0.00002620
Iteration 32/1000 | Loss: 0.00002620
Iteration 33/1000 | Loss: 0.00002620
Iteration 34/1000 | Loss: 0.00002620
Iteration 35/1000 | Loss: 0.00002619
Iteration 36/1000 | Loss: 0.00002619
Iteration 37/1000 | Loss: 0.00002619
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002619
Iteration 40/1000 | Loss: 0.00002619
Iteration 41/1000 | Loss: 0.00002618
Iteration 42/1000 | Loss: 0.00002618
Iteration 43/1000 | Loss: 0.00002618
Iteration 44/1000 | Loss: 0.00002618
Iteration 45/1000 | Loss: 0.00002618
Iteration 46/1000 | Loss: 0.00002618
Iteration 47/1000 | Loss: 0.00002618
Iteration 48/1000 | Loss: 0.00002618
Iteration 49/1000 | Loss: 0.00002618
Iteration 50/1000 | Loss: 0.00002618
Iteration 51/1000 | Loss: 0.00002617
Iteration 52/1000 | Loss: 0.00002617
Iteration 53/1000 | Loss: 0.00002617
Iteration 54/1000 | Loss: 0.00002617
Iteration 55/1000 | Loss: 0.00002617
Iteration 56/1000 | Loss: 0.00002617
Iteration 57/1000 | Loss: 0.00002617
Iteration 58/1000 | Loss: 0.00002616
Iteration 59/1000 | Loss: 0.00002616
Iteration 60/1000 | Loss: 0.00002616
Iteration 61/1000 | Loss: 0.00002616
Iteration 62/1000 | Loss: 0.00002616
Iteration 63/1000 | Loss: 0.00002615
Iteration 64/1000 | Loss: 0.00002615
Iteration 65/1000 | Loss: 0.00002615
Iteration 66/1000 | Loss: 0.00002615
Iteration 67/1000 | Loss: 0.00002615
Iteration 68/1000 | Loss: 0.00002615
Iteration 69/1000 | Loss: 0.00002615
Iteration 70/1000 | Loss: 0.00002615
Iteration 71/1000 | Loss: 0.00002615
Iteration 72/1000 | Loss: 0.00002615
Iteration 73/1000 | Loss: 0.00002615
Iteration 74/1000 | Loss: 0.00002614
Iteration 75/1000 | Loss: 0.00002614
Iteration 76/1000 | Loss: 0.00002614
Iteration 77/1000 | Loss: 0.00002614
Iteration 78/1000 | Loss: 0.00002614
Iteration 79/1000 | Loss: 0.00002614
Iteration 80/1000 | Loss: 0.00002614
Iteration 81/1000 | Loss: 0.00002614
Iteration 82/1000 | Loss: 0.00002613
Iteration 83/1000 | Loss: 0.00002613
Iteration 84/1000 | Loss: 0.00002613
Iteration 85/1000 | Loss: 0.00002613
Iteration 86/1000 | Loss: 0.00002613
Iteration 87/1000 | Loss: 0.00002613
Iteration 88/1000 | Loss: 0.00002613
Iteration 89/1000 | Loss: 0.00002612
Iteration 90/1000 | Loss: 0.00002612
Iteration 91/1000 | Loss: 0.00002612
Iteration 92/1000 | Loss: 0.00002612
Iteration 93/1000 | Loss: 0.00002612
Iteration 94/1000 | Loss: 0.00002612
Iteration 95/1000 | Loss: 0.00002612
Iteration 96/1000 | Loss: 0.00002612
Iteration 97/1000 | Loss: 0.00002611
Iteration 98/1000 | Loss: 0.00002611
Iteration 99/1000 | Loss: 0.00002611
Iteration 100/1000 | Loss: 0.00002611
Iteration 101/1000 | Loss: 0.00002611
Iteration 102/1000 | Loss: 0.00002611
Iteration 103/1000 | Loss: 0.00002611
Iteration 104/1000 | Loss: 0.00002610
Iteration 105/1000 | Loss: 0.00002610
Iteration 106/1000 | Loss: 0.00002610
Iteration 107/1000 | Loss: 0.00002610
Iteration 108/1000 | Loss: 0.00002610
Iteration 109/1000 | Loss: 0.00002610
Iteration 110/1000 | Loss: 0.00002610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.6104551579919644e-05, 2.6104551579919644e-05, 2.6104551579919644e-05, 2.6104551579919644e-05, 2.6104551579919644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6104551579919644e-05

Optimization complete. Final v2v error: 4.292706489562988 mm

Highest mean error: 5.0155816078186035 mm for frame 88

Lowest mean error: 3.7667109966278076 mm for frame 142

Saving results

Total time: 34.76528787612915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690236
Iteration 2/25 | Loss: 0.00085216
Iteration 3/25 | Loss: 0.00071492
Iteration 4/25 | Loss: 0.00069465
Iteration 5/25 | Loss: 0.00068780
Iteration 6/25 | Loss: 0.00068649
Iteration 7/25 | Loss: 0.00068607
Iteration 8/25 | Loss: 0.00068606
Iteration 9/25 | Loss: 0.00068606
Iteration 10/25 | Loss: 0.00068606
Iteration 11/25 | Loss: 0.00068606
Iteration 12/25 | Loss: 0.00068606
Iteration 13/25 | Loss: 0.00068606
Iteration 14/25 | Loss: 0.00068606
Iteration 15/25 | Loss: 0.00068606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006860574940219522, 0.0006860574940219522, 0.0006860574940219522, 0.0006860574940219522, 0.0006860574940219522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006860574940219522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56269002
Iteration 2/25 | Loss: 0.00022674
Iteration 3/25 | Loss: 0.00022674
Iteration 4/25 | Loss: 0.00022674
Iteration 5/25 | Loss: 0.00022674
Iteration 6/25 | Loss: 0.00022674
Iteration 7/25 | Loss: 0.00022674
Iteration 8/25 | Loss: 0.00022674
Iteration 9/25 | Loss: 0.00022674
Iteration 10/25 | Loss: 0.00022674
Iteration 11/25 | Loss: 0.00022674
Iteration 12/25 | Loss: 0.00022674
Iteration 13/25 | Loss: 0.00022674
Iteration 14/25 | Loss: 0.00022674
Iteration 15/25 | Loss: 0.00022674
Iteration 16/25 | Loss: 0.00022674
Iteration 17/25 | Loss: 0.00022674
Iteration 18/25 | Loss: 0.00022674
Iteration 19/25 | Loss: 0.00022674
Iteration 20/25 | Loss: 0.00022674
Iteration 21/25 | Loss: 0.00022674
Iteration 22/25 | Loss: 0.00022674
Iteration 23/25 | Loss: 0.00022674
Iteration 24/25 | Loss: 0.00022674
Iteration 25/25 | Loss: 0.00022674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022674
Iteration 2/1000 | Loss: 0.00002109
Iteration 3/1000 | Loss: 0.00001479
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001296
Iteration 6/1000 | Loss: 0.00001270
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001235
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001218
Iteration 11/1000 | Loss: 0.00001217
Iteration 12/1000 | Loss: 0.00001215
Iteration 13/1000 | Loss: 0.00001208
Iteration 14/1000 | Loss: 0.00001202
Iteration 15/1000 | Loss: 0.00001202
Iteration 16/1000 | Loss: 0.00001199
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001190
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001185
Iteration 34/1000 | Loss: 0.00001185
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001184
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001178
Iteration 46/1000 | Loss: 0.00001178
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001176
Iteration 50/1000 | Loss: 0.00001175
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001171
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001170
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001169
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001168
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001163
Iteration 79/1000 | Loss: 0.00001163
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001163
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001162
Iteration 89/1000 | Loss: 0.00001162
Iteration 90/1000 | Loss: 0.00001162
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001161
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001160
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001157
Iteration 128/1000 | Loss: 0.00001157
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001157
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001156
Iteration 139/1000 | Loss: 0.00001156
Iteration 140/1000 | Loss: 0.00001156
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001156
Iteration 153/1000 | Loss: 0.00001156
Iteration 154/1000 | Loss: 0.00001156
Iteration 155/1000 | Loss: 0.00001156
Iteration 156/1000 | Loss: 0.00001156
Iteration 157/1000 | Loss: 0.00001156
Iteration 158/1000 | Loss: 0.00001156
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001155
Iteration 162/1000 | Loss: 0.00001155
Iteration 163/1000 | Loss: 0.00001155
Iteration 164/1000 | Loss: 0.00001155
Iteration 165/1000 | Loss: 0.00001155
Iteration 166/1000 | Loss: 0.00001155
Iteration 167/1000 | Loss: 0.00001155
Iteration 168/1000 | Loss: 0.00001155
Iteration 169/1000 | Loss: 0.00001155
Iteration 170/1000 | Loss: 0.00001155
Iteration 171/1000 | Loss: 0.00001154
Iteration 172/1000 | Loss: 0.00001154
Iteration 173/1000 | Loss: 0.00001154
Iteration 174/1000 | Loss: 0.00001154
Iteration 175/1000 | Loss: 0.00001154
Iteration 176/1000 | Loss: 0.00001154
Iteration 177/1000 | Loss: 0.00001154
Iteration 178/1000 | Loss: 0.00001154
Iteration 179/1000 | Loss: 0.00001154
Iteration 180/1000 | Loss: 0.00001154
Iteration 181/1000 | Loss: 0.00001154
Iteration 182/1000 | Loss: 0.00001154
Iteration 183/1000 | Loss: 0.00001154
Iteration 184/1000 | Loss: 0.00001154
Iteration 185/1000 | Loss: 0.00001154
Iteration 186/1000 | Loss: 0.00001154
Iteration 187/1000 | Loss: 0.00001154
Iteration 188/1000 | Loss: 0.00001154
Iteration 189/1000 | Loss: 0.00001154
Iteration 190/1000 | Loss: 0.00001154
Iteration 191/1000 | Loss: 0.00001154
Iteration 192/1000 | Loss: 0.00001154
Iteration 193/1000 | Loss: 0.00001153
Iteration 194/1000 | Loss: 0.00001153
Iteration 195/1000 | Loss: 0.00001153
Iteration 196/1000 | Loss: 0.00001153
Iteration 197/1000 | Loss: 0.00001153
Iteration 198/1000 | Loss: 0.00001153
Iteration 199/1000 | Loss: 0.00001153
Iteration 200/1000 | Loss: 0.00001153
Iteration 201/1000 | Loss: 0.00001153
Iteration 202/1000 | Loss: 0.00001153
Iteration 203/1000 | Loss: 0.00001153
Iteration 204/1000 | Loss: 0.00001153
Iteration 205/1000 | Loss: 0.00001153
Iteration 206/1000 | Loss: 0.00001153
Iteration 207/1000 | Loss: 0.00001153
Iteration 208/1000 | Loss: 0.00001153
Iteration 209/1000 | Loss: 0.00001152
Iteration 210/1000 | Loss: 0.00001152
Iteration 211/1000 | Loss: 0.00001152
Iteration 212/1000 | Loss: 0.00001152
Iteration 213/1000 | Loss: 0.00001152
Iteration 214/1000 | Loss: 0.00001152
Iteration 215/1000 | Loss: 0.00001152
Iteration 216/1000 | Loss: 0.00001152
Iteration 217/1000 | Loss: 0.00001152
Iteration 218/1000 | Loss: 0.00001152
Iteration 219/1000 | Loss: 0.00001152
Iteration 220/1000 | Loss: 0.00001152
Iteration 221/1000 | Loss: 0.00001152
Iteration 222/1000 | Loss: 0.00001152
Iteration 223/1000 | Loss: 0.00001152
Iteration 224/1000 | Loss: 0.00001152
Iteration 225/1000 | Loss: 0.00001152
Iteration 226/1000 | Loss: 0.00001152
Iteration 227/1000 | Loss: 0.00001152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.1523600733198691e-05, 1.1523600733198691e-05, 1.1523600733198691e-05, 1.1523600733198691e-05, 1.1523600733198691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1523600733198691e-05

Optimization complete. Final v2v error: 2.897408962249756 mm

Highest mean error: 3.548396587371826 mm for frame 73

Lowest mean error: 2.695849895477295 mm for frame 4

Saving results

Total time: 39.5828161239624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695696
Iteration 2/25 | Loss: 0.00150249
Iteration 3/25 | Loss: 0.00101244
Iteration 4/25 | Loss: 0.00097962
Iteration 5/25 | Loss: 0.00097025
Iteration 6/25 | Loss: 0.00096742
Iteration 7/25 | Loss: 0.00096628
Iteration 8/25 | Loss: 0.00096591
Iteration 9/25 | Loss: 0.00096591
Iteration 10/25 | Loss: 0.00096591
Iteration 11/25 | Loss: 0.00096591
Iteration 12/25 | Loss: 0.00096591
Iteration 13/25 | Loss: 0.00096591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009659050265327096, 0.0009659050265327096, 0.0009659050265327096, 0.0009659050265327096, 0.0009659050265327096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009659050265327096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60839701
Iteration 2/25 | Loss: 0.00027663
Iteration 3/25 | Loss: 0.00027663
Iteration 4/25 | Loss: 0.00027663
Iteration 5/25 | Loss: 0.00027663
Iteration 6/25 | Loss: 0.00027663
Iteration 7/25 | Loss: 0.00027663
Iteration 8/25 | Loss: 0.00027663
Iteration 9/25 | Loss: 0.00027663
Iteration 10/25 | Loss: 0.00027663
Iteration 11/25 | Loss: 0.00027663
Iteration 12/25 | Loss: 0.00027663
Iteration 13/25 | Loss: 0.00027663
Iteration 14/25 | Loss: 0.00027663
Iteration 15/25 | Loss: 0.00027663
Iteration 16/25 | Loss: 0.00027663
Iteration 17/25 | Loss: 0.00027663
Iteration 18/25 | Loss: 0.00027663
Iteration 19/25 | Loss: 0.00027663
Iteration 20/25 | Loss: 0.00027663
Iteration 21/25 | Loss: 0.00027663
Iteration 22/25 | Loss: 0.00027663
Iteration 23/25 | Loss: 0.00027663
Iteration 24/25 | Loss: 0.00027663
Iteration 25/25 | Loss: 0.00027663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027663
Iteration 2/1000 | Loss: 0.00004482
Iteration 3/1000 | Loss: 0.00003475
Iteration 4/1000 | Loss: 0.00003184
Iteration 5/1000 | Loss: 0.00003084
Iteration 6/1000 | Loss: 0.00003033
Iteration 7/1000 | Loss: 0.00002965
Iteration 8/1000 | Loss: 0.00002907
Iteration 9/1000 | Loss: 0.00002865
Iteration 10/1000 | Loss: 0.00002832
Iteration 11/1000 | Loss: 0.00002813
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002780
Iteration 14/1000 | Loss: 0.00002772
Iteration 15/1000 | Loss: 0.00002766
Iteration 16/1000 | Loss: 0.00002766
Iteration 17/1000 | Loss: 0.00002765
Iteration 18/1000 | Loss: 0.00002765
Iteration 19/1000 | Loss: 0.00002765
Iteration 20/1000 | Loss: 0.00002764
Iteration 21/1000 | Loss: 0.00002763
Iteration 22/1000 | Loss: 0.00002763
Iteration 23/1000 | Loss: 0.00002763
Iteration 24/1000 | Loss: 0.00002760
Iteration 25/1000 | Loss: 0.00002757
Iteration 26/1000 | Loss: 0.00002757
Iteration 27/1000 | Loss: 0.00002752
Iteration 28/1000 | Loss: 0.00002750
Iteration 29/1000 | Loss: 0.00002737
Iteration 30/1000 | Loss: 0.00002737
Iteration 31/1000 | Loss: 0.00002737
Iteration 32/1000 | Loss: 0.00002737
Iteration 33/1000 | Loss: 0.00002736
Iteration 34/1000 | Loss: 0.00002736
Iteration 35/1000 | Loss: 0.00002736
Iteration 36/1000 | Loss: 0.00002735
Iteration 37/1000 | Loss: 0.00002731
Iteration 38/1000 | Loss: 0.00002730
Iteration 39/1000 | Loss: 0.00002730
Iteration 40/1000 | Loss: 0.00002730
Iteration 41/1000 | Loss: 0.00002730
Iteration 42/1000 | Loss: 0.00002730
Iteration 43/1000 | Loss: 0.00002730
Iteration 44/1000 | Loss: 0.00002730
Iteration 45/1000 | Loss: 0.00002730
Iteration 46/1000 | Loss: 0.00002730
Iteration 47/1000 | Loss: 0.00002730
Iteration 48/1000 | Loss: 0.00002729
Iteration 49/1000 | Loss: 0.00002729
Iteration 50/1000 | Loss: 0.00002729
Iteration 51/1000 | Loss: 0.00002729
Iteration 52/1000 | Loss: 0.00002729
Iteration 53/1000 | Loss: 0.00002729
Iteration 54/1000 | Loss: 0.00002729
Iteration 55/1000 | Loss: 0.00002729
Iteration 56/1000 | Loss: 0.00002729
Iteration 57/1000 | Loss: 0.00002729
Iteration 58/1000 | Loss: 0.00002729
Iteration 59/1000 | Loss: 0.00002729
Iteration 60/1000 | Loss: 0.00002729
Iteration 61/1000 | Loss: 0.00002729
Iteration 62/1000 | Loss: 0.00002728
Iteration 63/1000 | Loss: 0.00002728
Iteration 64/1000 | Loss: 0.00002728
Iteration 65/1000 | Loss: 0.00002728
Iteration 66/1000 | Loss: 0.00002727
Iteration 67/1000 | Loss: 0.00002727
Iteration 68/1000 | Loss: 0.00002727
Iteration 69/1000 | Loss: 0.00002727
Iteration 70/1000 | Loss: 0.00002727
Iteration 71/1000 | Loss: 0.00002727
Iteration 72/1000 | Loss: 0.00002727
Iteration 73/1000 | Loss: 0.00002727
Iteration 74/1000 | Loss: 0.00002727
Iteration 75/1000 | Loss: 0.00002727
Iteration 76/1000 | Loss: 0.00002727
Iteration 77/1000 | Loss: 0.00002727
Iteration 78/1000 | Loss: 0.00002727
Iteration 79/1000 | Loss: 0.00002726
Iteration 80/1000 | Loss: 0.00002726
Iteration 81/1000 | Loss: 0.00002725
Iteration 82/1000 | Loss: 0.00002725
Iteration 83/1000 | Loss: 0.00002725
Iteration 84/1000 | Loss: 0.00002725
Iteration 85/1000 | Loss: 0.00002724
Iteration 86/1000 | Loss: 0.00002724
Iteration 87/1000 | Loss: 0.00002724
Iteration 88/1000 | Loss: 0.00002723
Iteration 89/1000 | Loss: 0.00002722
Iteration 90/1000 | Loss: 0.00002722
Iteration 91/1000 | Loss: 0.00002722
Iteration 92/1000 | Loss: 0.00002722
Iteration 93/1000 | Loss: 0.00002722
Iteration 94/1000 | Loss: 0.00002722
Iteration 95/1000 | Loss: 0.00002722
Iteration 96/1000 | Loss: 0.00002721
Iteration 97/1000 | Loss: 0.00002721
Iteration 98/1000 | Loss: 0.00002721
Iteration 99/1000 | Loss: 0.00002721
Iteration 100/1000 | Loss: 0.00002721
Iteration 101/1000 | Loss: 0.00002721
Iteration 102/1000 | Loss: 0.00002721
Iteration 103/1000 | Loss: 0.00002720
Iteration 104/1000 | Loss: 0.00002719
Iteration 105/1000 | Loss: 0.00002719
Iteration 106/1000 | Loss: 0.00002719
Iteration 107/1000 | Loss: 0.00002718
Iteration 108/1000 | Loss: 0.00002718
Iteration 109/1000 | Loss: 0.00002718
Iteration 110/1000 | Loss: 0.00002717
Iteration 111/1000 | Loss: 0.00002715
Iteration 112/1000 | Loss: 0.00002714
Iteration 113/1000 | Loss: 0.00002714
Iteration 114/1000 | Loss: 0.00002714
Iteration 115/1000 | Loss: 0.00002714
Iteration 116/1000 | Loss: 0.00002713
Iteration 117/1000 | Loss: 0.00002713
Iteration 118/1000 | Loss: 0.00002713
Iteration 119/1000 | Loss: 0.00002713
Iteration 120/1000 | Loss: 0.00002712
Iteration 121/1000 | Loss: 0.00002712
Iteration 122/1000 | Loss: 0.00002711
Iteration 123/1000 | Loss: 0.00002711
Iteration 124/1000 | Loss: 0.00002711
Iteration 125/1000 | Loss: 0.00002710
Iteration 126/1000 | Loss: 0.00002710
Iteration 127/1000 | Loss: 0.00002710
Iteration 128/1000 | Loss: 0.00002710
Iteration 129/1000 | Loss: 0.00002710
Iteration 130/1000 | Loss: 0.00002710
Iteration 131/1000 | Loss: 0.00002710
Iteration 132/1000 | Loss: 0.00002710
Iteration 133/1000 | Loss: 0.00002710
Iteration 134/1000 | Loss: 0.00002710
Iteration 135/1000 | Loss: 0.00002709
Iteration 136/1000 | Loss: 0.00002709
Iteration 137/1000 | Loss: 0.00002709
Iteration 138/1000 | Loss: 0.00002709
Iteration 139/1000 | Loss: 0.00002709
Iteration 140/1000 | Loss: 0.00002708
Iteration 141/1000 | Loss: 0.00002708
Iteration 142/1000 | Loss: 0.00002708
Iteration 143/1000 | Loss: 0.00002708
Iteration 144/1000 | Loss: 0.00002708
Iteration 145/1000 | Loss: 0.00002707
Iteration 146/1000 | Loss: 0.00002707
Iteration 147/1000 | Loss: 0.00002706
Iteration 148/1000 | Loss: 0.00002706
Iteration 149/1000 | Loss: 0.00002706
Iteration 150/1000 | Loss: 0.00002706
Iteration 151/1000 | Loss: 0.00002706
Iteration 152/1000 | Loss: 0.00002705
Iteration 153/1000 | Loss: 0.00002705
Iteration 154/1000 | Loss: 0.00002705
Iteration 155/1000 | Loss: 0.00002705
Iteration 156/1000 | Loss: 0.00002705
Iteration 157/1000 | Loss: 0.00002705
Iteration 158/1000 | Loss: 0.00002705
Iteration 159/1000 | Loss: 0.00002705
Iteration 160/1000 | Loss: 0.00002704
Iteration 161/1000 | Loss: 0.00002704
Iteration 162/1000 | Loss: 0.00002704
Iteration 163/1000 | Loss: 0.00002704
Iteration 164/1000 | Loss: 0.00002704
Iteration 165/1000 | Loss: 0.00002704
Iteration 166/1000 | Loss: 0.00002704
Iteration 167/1000 | Loss: 0.00002703
Iteration 168/1000 | Loss: 0.00002703
Iteration 169/1000 | Loss: 0.00002703
Iteration 170/1000 | Loss: 0.00002703
Iteration 171/1000 | Loss: 0.00002703
Iteration 172/1000 | Loss: 0.00002702
Iteration 173/1000 | Loss: 0.00002702
Iteration 174/1000 | Loss: 0.00002702
Iteration 175/1000 | Loss: 0.00002702
Iteration 176/1000 | Loss: 0.00002702
Iteration 177/1000 | Loss: 0.00002702
Iteration 178/1000 | Loss: 0.00002702
Iteration 179/1000 | Loss: 0.00002702
Iteration 180/1000 | Loss: 0.00002702
Iteration 181/1000 | Loss: 0.00002702
Iteration 182/1000 | Loss: 0.00002701
Iteration 183/1000 | Loss: 0.00002701
Iteration 184/1000 | Loss: 0.00002701
Iteration 185/1000 | Loss: 0.00002701
Iteration 186/1000 | Loss: 0.00002701
Iteration 187/1000 | Loss: 0.00002701
Iteration 188/1000 | Loss: 0.00002701
Iteration 189/1000 | Loss: 0.00002701
Iteration 190/1000 | Loss: 0.00002701
Iteration 191/1000 | Loss: 0.00002701
Iteration 192/1000 | Loss: 0.00002701
Iteration 193/1000 | Loss: 0.00002701
Iteration 194/1000 | Loss: 0.00002700
Iteration 195/1000 | Loss: 0.00002700
Iteration 196/1000 | Loss: 0.00002700
Iteration 197/1000 | Loss: 0.00002700
Iteration 198/1000 | Loss: 0.00002700
Iteration 199/1000 | Loss: 0.00002700
Iteration 200/1000 | Loss: 0.00002700
Iteration 201/1000 | Loss: 0.00002700
Iteration 202/1000 | Loss: 0.00002700
Iteration 203/1000 | Loss: 0.00002700
Iteration 204/1000 | Loss: 0.00002700
Iteration 205/1000 | Loss: 0.00002700
Iteration 206/1000 | Loss: 0.00002700
Iteration 207/1000 | Loss: 0.00002700
Iteration 208/1000 | Loss: 0.00002699
Iteration 209/1000 | Loss: 0.00002699
Iteration 210/1000 | Loss: 0.00002699
Iteration 211/1000 | Loss: 0.00002699
Iteration 212/1000 | Loss: 0.00002699
Iteration 213/1000 | Loss: 0.00002699
Iteration 214/1000 | Loss: 0.00002698
Iteration 215/1000 | Loss: 0.00002698
Iteration 216/1000 | Loss: 0.00002697
Iteration 217/1000 | Loss: 0.00002697
Iteration 218/1000 | Loss: 0.00002696
Iteration 219/1000 | Loss: 0.00002696
Iteration 220/1000 | Loss: 0.00002696
Iteration 221/1000 | Loss: 0.00002696
Iteration 222/1000 | Loss: 0.00002695
Iteration 223/1000 | Loss: 0.00002695
Iteration 224/1000 | Loss: 0.00002695
Iteration 225/1000 | Loss: 0.00002695
Iteration 226/1000 | Loss: 0.00002695
Iteration 227/1000 | Loss: 0.00002695
Iteration 228/1000 | Loss: 0.00002695
Iteration 229/1000 | Loss: 0.00002694
Iteration 230/1000 | Loss: 0.00002694
Iteration 231/1000 | Loss: 0.00002694
Iteration 232/1000 | Loss: 0.00002694
Iteration 233/1000 | Loss: 0.00002694
Iteration 234/1000 | Loss: 0.00002694
Iteration 235/1000 | Loss: 0.00002694
Iteration 236/1000 | Loss: 0.00002694
Iteration 237/1000 | Loss: 0.00002694
Iteration 238/1000 | Loss: 0.00002694
Iteration 239/1000 | Loss: 0.00002694
Iteration 240/1000 | Loss: 0.00002694
Iteration 241/1000 | Loss: 0.00002694
Iteration 242/1000 | Loss: 0.00002694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [2.694342765607871e-05, 2.694342765607871e-05, 2.694342765607871e-05, 2.694342765607871e-05, 2.694342765607871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.694342765607871e-05

Optimization complete. Final v2v error: 4.166677951812744 mm

Highest mean error: 4.6186933517456055 mm for frame 92

Lowest mean error: 3.731043577194214 mm for frame 27

Saving results

Total time: 50.09363055229187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060012
Iteration 2/25 | Loss: 0.00138160
Iteration 3/25 | Loss: 0.00087389
Iteration 4/25 | Loss: 0.00079967
Iteration 5/25 | Loss: 0.00075852
Iteration 6/25 | Loss: 0.00075606
Iteration 7/25 | Loss: 0.00074554
Iteration 8/25 | Loss: 0.00074810
Iteration 9/25 | Loss: 0.00074112
Iteration 10/25 | Loss: 0.00074062
Iteration 11/25 | Loss: 0.00074052
Iteration 12/25 | Loss: 0.00074050
Iteration 13/25 | Loss: 0.00074049
Iteration 14/25 | Loss: 0.00074049
Iteration 15/25 | Loss: 0.00074049
Iteration 16/25 | Loss: 0.00074049
Iteration 17/25 | Loss: 0.00074049
Iteration 18/25 | Loss: 0.00074049
Iteration 19/25 | Loss: 0.00074048
Iteration 20/25 | Loss: 0.00074048
Iteration 21/25 | Loss: 0.00074048
Iteration 22/25 | Loss: 0.00074048
Iteration 23/25 | Loss: 0.00074048
Iteration 24/25 | Loss: 0.00074048
Iteration 25/25 | Loss: 0.00074048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.66065168
Iteration 2/25 | Loss: 0.00024356
Iteration 3/25 | Loss: 0.00024355
Iteration 4/25 | Loss: 0.00024355
Iteration 5/25 | Loss: 0.00024355
Iteration 6/25 | Loss: 0.00024355
Iteration 7/25 | Loss: 0.00024355
Iteration 8/25 | Loss: 0.00024355
Iteration 9/25 | Loss: 0.00024355
Iteration 10/25 | Loss: 0.00024355
Iteration 11/25 | Loss: 0.00024355
Iteration 12/25 | Loss: 0.00024355
Iteration 13/25 | Loss: 0.00024355
Iteration 14/25 | Loss: 0.00024355
Iteration 15/25 | Loss: 0.00024355
Iteration 16/25 | Loss: 0.00024355
Iteration 17/25 | Loss: 0.00024355
Iteration 18/25 | Loss: 0.00024355
Iteration 19/25 | Loss: 0.00024355
Iteration 20/25 | Loss: 0.00024355
Iteration 21/25 | Loss: 0.00024355
Iteration 22/25 | Loss: 0.00024355
Iteration 23/25 | Loss: 0.00024355
Iteration 24/25 | Loss: 0.00024355
Iteration 25/25 | Loss: 0.00024355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024355
Iteration 2/1000 | Loss: 0.00002373
Iteration 3/1000 | Loss: 0.00001805
Iteration 4/1000 | Loss: 0.00001657
Iteration 5/1000 | Loss: 0.00007763
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001476
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001420
Iteration 10/1000 | Loss: 0.00001415
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001410
Iteration 14/1000 | Loss: 0.00001391
Iteration 15/1000 | Loss: 0.00001388
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001370
Iteration 24/1000 | Loss: 0.00001367
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001366
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001363
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001359
Iteration 35/1000 | Loss: 0.00001358
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001357
Iteration 38/1000 | Loss: 0.00001354
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001347
Iteration 53/1000 | Loss: 0.00001347
Iteration 54/1000 | Loss: 0.00001347
Iteration 55/1000 | Loss: 0.00001347
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001344
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001342
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001340
Iteration 74/1000 | Loss: 0.00001340
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001339
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001339
Iteration 83/1000 | Loss: 0.00001339
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001336
Iteration 101/1000 | Loss: 0.00001336
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001335
Iteration 112/1000 | Loss: 0.00001335
Iteration 113/1000 | Loss: 0.00001335
Iteration 114/1000 | Loss: 0.00001335
Iteration 115/1000 | Loss: 0.00001335
Iteration 116/1000 | Loss: 0.00001335
Iteration 117/1000 | Loss: 0.00001335
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001334
Iteration 122/1000 | Loss: 0.00001334
Iteration 123/1000 | Loss: 0.00001334
Iteration 124/1000 | Loss: 0.00001334
Iteration 125/1000 | Loss: 0.00001334
Iteration 126/1000 | Loss: 0.00001334
Iteration 127/1000 | Loss: 0.00001334
Iteration 128/1000 | Loss: 0.00001334
Iteration 129/1000 | Loss: 0.00001334
Iteration 130/1000 | Loss: 0.00001334
Iteration 131/1000 | Loss: 0.00001334
Iteration 132/1000 | Loss: 0.00001334
Iteration 133/1000 | Loss: 0.00001334
Iteration 134/1000 | Loss: 0.00001334
Iteration 135/1000 | Loss: 0.00001334
Iteration 136/1000 | Loss: 0.00001334
Iteration 137/1000 | Loss: 0.00001334
Iteration 138/1000 | Loss: 0.00001334
Iteration 139/1000 | Loss: 0.00001334
Iteration 140/1000 | Loss: 0.00001334
Iteration 141/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.3343496902962215e-05, 1.3343496902962215e-05, 1.3343496902962215e-05, 1.3343496902962215e-05, 1.3343496902962215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3343496902962215e-05

Optimization complete. Final v2v error: 3.067176580429077 mm

Highest mean error: 8.28886604309082 mm for frame 236

Lowest mean error: 2.792532205581665 mm for frame 8

Saving results

Total time: 54.53750419616699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020115
Iteration 2/25 | Loss: 0.00195354
Iteration 3/25 | Loss: 0.00131439
Iteration 4/25 | Loss: 0.00115533
Iteration 5/25 | Loss: 0.00110988
Iteration 6/25 | Loss: 0.00106490
Iteration 7/25 | Loss: 0.00101355
Iteration 8/25 | Loss: 0.00098680
Iteration 9/25 | Loss: 0.00096748
Iteration 10/25 | Loss: 0.00093965
Iteration 11/25 | Loss: 0.00093118
Iteration 12/25 | Loss: 0.00092471
Iteration 13/25 | Loss: 0.00092029
Iteration 14/25 | Loss: 0.00092045
Iteration 15/25 | Loss: 0.00091941
Iteration 16/25 | Loss: 0.00094218
Iteration 17/25 | Loss: 0.00094122
Iteration 18/25 | Loss: 0.00092392
Iteration 19/25 | Loss: 0.00092002
Iteration 20/25 | Loss: 0.00091943
Iteration 21/25 | Loss: 0.00094064
Iteration 22/25 | Loss: 0.00091815
Iteration 23/25 | Loss: 0.00091364
Iteration 24/25 | Loss: 0.00091413
Iteration 25/25 | Loss: 0.00091287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44680214
Iteration 2/25 | Loss: 0.00033686
Iteration 3/25 | Loss: 0.00033686
Iteration 4/25 | Loss: 0.00033686
Iteration 5/25 | Loss: 0.00033686
Iteration 6/25 | Loss: 0.00033686
Iteration 7/25 | Loss: 0.00033686
Iteration 8/25 | Loss: 0.00033686
Iteration 9/25 | Loss: 0.00033686
Iteration 10/25 | Loss: 0.00033686
Iteration 11/25 | Loss: 0.00033686
Iteration 12/25 | Loss: 0.00033686
Iteration 13/25 | Loss: 0.00033686
Iteration 14/25 | Loss: 0.00033686
Iteration 15/25 | Loss: 0.00033686
Iteration 16/25 | Loss: 0.00033686
Iteration 17/25 | Loss: 0.00033686
Iteration 18/25 | Loss: 0.00033686
Iteration 19/25 | Loss: 0.00033686
Iteration 20/25 | Loss: 0.00033686
Iteration 21/25 | Loss: 0.00033686
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003368550387676805, 0.0003368550387676805, 0.0003368550387676805, 0.0003368550387676805, 0.0003368550387676805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003368550387676805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033686
Iteration 2/1000 | Loss: 0.00009193
Iteration 3/1000 | Loss: 0.00040620
Iteration 4/1000 | Loss: 0.00010536
Iteration 5/1000 | Loss: 0.00007545
Iteration 6/1000 | Loss: 0.00007853
Iteration 7/1000 | Loss: 0.00005057
Iteration 8/1000 | Loss: 0.00010044
Iteration 9/1000 | Loss: 0.00028842
Iteration 10/1000 | Loss: 0.00013102
Iteration 11/1000 | Loss: 0.00019752
Iteration 12/1000 | Loss: 0.00015714
Iteration 13/1000 | Loss: 0.00006751
Iteration 14/1000 | Loss: 0.00003597
Iteration 15/1000 | Loss: 0.00003275
Iteration 16/1000 | Loss: 0.00003083
Iteration 17/1000 | Loss: 0.00002955
Iteration 18/1000 | Loss: 0.00002870
Iteration 19/1000 | Loss: 0.00002789
Iteration 20/1000 | Loss: 0.00002740
Iteration 21/1000 | Loss: 0.00002702
Iteration 22/1000 | Loss: 0.00019124
Iteration 23/1000 | Loss: 0.00003084
Iteration 24/1000 | Loss: 0.00002803
Iteration 25/1000 | Loss: 0.00002702
Iteration 26/1000 | Loss: 0.00002620
Iteration 27/1000 | Loss: 0.00002559
Iteration 28/1000 | Loss: 0.00002536
Iteration 29/1000 | Loss: 0.00002519
Iteration 30/1000 | Loss: 0.00002515
Iteration 31/1000 | Loss: 0.00002514
Iteration 32/1000 | Loss: 0.00002510
Iteration 33/1000 | Loss: 0.00002510
Iteration 34/1000 | Loss: 0.00002509
Iteration 35/1000 | Loss: 0.00002509
Iteration 36/1000 | Loss: 0.00002509
Iteration 37/1000 | Loss: 0.00002508
Iteration 38/1000 | Loss: 0.00002504
Iteration 39/1000 | Loss: 0.00002502
Iteration 40/1000 | Loss: 0.00002502
Iteration 41/1000 | Loss: 0.00002502
Iteration 42/1000 | Loss: 0.00002501
Iteration 43/1000 | Loss: 0.00002501
Iteration 44/1000 | Loss: 0.00002501
Iteration 45/1000 | Loss: 0.00002497
Iteration 46/1000 | Loss: 0.00002497
Iteration 47/1000 | Loss: 0.00002497
Iteration 48/1000 | Loss: 0.00002497
Iteration 49/1000 | Loss: 0.00002497
Iteration 50/1000 | Loss: 0.00002497
Iteration 51/1000 | Loss: 0.00002496
Iteration 52/1000 | Loss: 0.00002496
Iteration 53/1000 | Loss: 0.00002496
Iteration 54/1000 | Loss: 0.00002496
Iteration 55/1000 | Loss: 0.00002496
Iteration 56/1000 | Loss: 0.00002496
Iteration 57/1000 | Loss: 0.00002496
Iteration 58/1000 | Loss: 0.00002495
Iteration 59/1000 | Loss: 0.00002495
Iteration 60/1000 | Loss: 0.00002495
Iteration 61/1000 | Loss: 0.00002495
Iteration 62/1000 | Loss: 0.00002494
Iteration 63/1000 | Loss: 0.00002494
Iteration 64/1000 | Loss: 0.00002494
Iteration 65/1000 | Loss: 0.00002494
Iteration 66/1000 | Loss: 0.00002493
Iteration 67/1000 | Loss: 0.00002493
Iteration 68/1000 | Loss: 0.00002493
Iteration 69/1000 | Loss: 0.00002493
Iteration 70/1000 | Loss: 0.00002493
Iteration 71/1000 | Loss: 0.00002492
Iteration 72/1000 | Loss: 0.00002492
Iteration 73/1000 | Loss: 0.00002492
Iteration 74/1000 | Loss: 0.00002492
Iteration 75/1000 | Loss: 0.00002492
Iteration 76/1000 | Loss: 0.00002492
Iteration 77/1000 | Loss: 0.00002492
Iteration 78/1000 | Loss: 0.00002492
Iteration 79/1000 | Loss: 0.00002492
Iteration 80/1000 | Loss: 0.00002492
Iteration 81/1000 | Loss: 0.00002492
Iteration 82/1000 | Loss: 0.00002492
Iteration 83/1000 | Loss: 0.00002492
Iteration 84/1000 | Loss: 0.00002492
Iteration 85/1000 | Loss: 0.00002492
Iteration 86/1000 | Loss: 0.00002492
Iteration 87/1000 | Loss: 0.00002492
Iteration 88/1000 | Loss: 0.00002492
Iteration 89/1000 | Loss: 0.00002492
Iteration 90/1000 | Loss: 0.00002492
Iteration 91/1000 | Loss: 0.00002492
Iteration 92/1000 | Loss: 0.00002492
Iteration 93/1000 | Loss: 0.00002492
Iteration 94/1000 | Loss: 0.00002492
Iteration 95/1000 | Loss: 0.00002492
Iteration 96/1000 | Loss: 0.00002492
Iteration 97/1000 | Loss: 0.00002492
Iteration 98/1000 | Loss: 0.00002492
Iteration 99/1000 | Loss: 0.00002492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.4916062102420256e-05, 2.4916062102420256e-05, 2.4916062102420256e-05, 2.4916062102420256e-05, 2.4916062102420256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4916062102420256e-05

Optimization complete. Final v2v error: 4.196846008300781 mm

Highest mean error: 5.167028903961182 mm for frame 220

Lowest mean error: 3.7129383087158203 mm for frame 212

Saving results

Total time: 107.27250385284424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696604
Iteration 2/25 | Loss: 0.00142427
Iteration 3/25 | Loss: 0.00095212
Iteration 4/25 | Loss: 0.00084475
Iteration 5/25 | Loss: 0.00083448
Iteration 6/25 | Loss: 0.00082225
Iteration 7/25 | Loss: 0.00080729
Iteration 8/25 | Loss: 0.00080348
Iteration 9/25 | Loss: 0.00079784
Iteration 10/25 | Loss: 0.00079007
Iteration 11/25 | Loss: 0.00078816
Iteration 12/25 | Loss: 0.00078760
Iteration 13/25 | Loss: 0.00078416
Iteration 14/25 | Loss: 0.00078347
Iteration 15/25 | Loss: 0.00078341
Iteration 16/25 | Loss: 0.00078341
Iteration 17/25 | Loss: 0.00078340
Iteration 18/25 | Loss: 0.00078340
Iteration 19/25 | Loss: 0.00078340
Iteration 20/25 | Loss: 0.00078340
Iteration 21/25 | Loss: 0.00078340
Iteration 22/25 | Loss: 0.00078340
Iteration 23/25 | Loss: 0.00078340
Iteration 24/25 | Loss: 0.00078340
Iteration 25/25 | Loss: 0.00078340

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17797518
Iteration 2/25 | Loss: 0.00034547
Iteration 3/25 | Loss: 0.00034517
Iteration 4/25 | Loss: 0.00034517
Iteration 5/25 | Loss: 0.00034517
Iteration 6/25 | Loss: 0.00034517
Iteration 7/25 | Loss: 0.00034517
Iteration 8/25 | Loss: 0.00034517
Iteration 9/25 | Loss: 0.00034517
Iteration 10/25 | Loss: 0.00034517
Iteration 11/25 | Loss: 0.00034517
Iteration 12/25 | Loss: 0.00034517
Iteration 13/25 | Loss: 0.00034517
Iteration 14/25 | Loss: 0.00034517
Iteration 15/25 | Loss: 0.00034517
Iteration 16/25 | Loss: 0.00034517
Iteration 17/25 | Loss: 0.00034517
Iteration 18/25 | Loss: 0.00034517
Iteration 19/25 | Loss: 0.00034517
Iteration 20/25 | Loss: 0.00034517
Iteration 21/25 | Loss: 0.00034517
Iteration 22/25 | Loss: 0.00034517
Iteration 23/25 | Loss: 0.00034517
Iteration 24/25 | Loss: 0.00034517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00034516979940235615, 0.00034516979940235615, 0.00034516979940235615, 0.00034516979940235615, 0.00034516979940235615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00034516979940235615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034517
Iteration 2/1000 | Loss: 0.00005121
Iteration 3/1000 | Loss: 0.00003707
Iteration 4/1000 | Loss: 0.00003194
Iteration 5/1000 | Loss: 0.00002953
Iteration 6/1000 | Loss: 0.00002809
Iteration 7/1000 | Loss: 0.00002709
Iteration 8/1000 | Loss: 0.00002651
Iteration 9/1000 | Loss: 0.00002586
Iteration 10/1000 | Loss: 0.00002545
Iteration 11/1000 | Loss: 0.00002516
Iteration 12/1000 | Loss: 0.00002496
Iteration 13/1000 | Loss: 0.00002481
Iteration 14/1000 | Loss: 0.00002471
Iteration 15/1000 | Loss: 0.00002467
Iteration 16/1000 | Loss: 0.00002466
Iteration 17/1000 | Loss: 0.00002460
Iteration 18/1000 | Loss: 0.00002455
Iteration 19/1000 | Loss: 0.00002452
Iteration 20/1000 | Loss: 0.00002450
Iteration 21/1000 | Loss: 0.00002449
Iteration 22/1000 | Loss: 0.00002448
Iteration 23/1000 | Loss: 0.00002441
Iteration 24/1000 | Loss: 0.00002441
Iteration 25/1000 | Loss: 0.00002440
Iteration 26/1000 | Loss: 0.00002438
Iteration 27/1000 | Loss: 0.00002437
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002435
Iteration 30/1000 | Loss: 0.00002435
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002435
Iteration 33/1000 | Loss: 0.00002434
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002433
Iteration 37/1000 | Loss: 0.00002433
Iteration 38/1000 | Loss: 0.00002432
Iteration 39/1000 | Loss: 0.00002432
Iteration 40/1000 | Loss: 0.00002431
Iteration 41/1000 | Loss: 0.00002431
Iteration 42/1000 | Loss: 0.00002430
Iteration 43/1000 | Loss: 0.00002429
Iteration 44/1000 | Loss: 0.00002429
Iteration 45/1000 | Loss: 0.00002428
Iteration 46/1000 | Loss: 0.00002428
Iteration 47/1000 | Loss: 0.00002428
Iteration 48/1000 | Loss: 0.00002428
Iteration 49/1000 | Loss: 0.00002427
Iteration 50/1000 | Loss: 0.00002424
Iteration 51/1000 | Loss: 0.00002424
Iteration 52/1000 | Loss: 0.00002423
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002421
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002420
Iteration 58/1000 | Loss: 0.00002419
Iteration 59/1000 | Loss: 0.00002416
Iteration 60/1000 | Loss: 0.00002416
Iteration 61/1000 | Loss: 0.00002415
Iteration 62/1000 | Loss: 0.00002413
Iteration 63/1000 | Loss: 0.00002409
Iteration 64/1000 | Loss: 0.00002408
Iteration 65/1000 | Loss: 0.00002407
Iteration 66/1000 | Loss: 0.00002407
Iteration 67/1000 | Loss: 0.00002406
Iteration 68/1000 | Loss: 0.00002405
Iteration 69/1000 | Loss: 0.00002405
Iteration 70/1000 | Loss: 0.00002403
Iteration 71/1000 | Loss: 0.00002403
Iteration 72/1000 | Loss: 0.00002402
Iteration 73/1000 | Loss: 0.00002401
Iteration 74/1000 | Loss: 0.00002401
Iteration 75/1000 | Loss: 0.00002401
Iteration 76/1000 | Loss: 0.00002400
Iteration 77/1000 | Loss: 0.00002400
Iteration 78/1000 | Loss: 0.00002400
Iteration 79/1000 | Loss: 0.00002399
Iteration 80/1000 | Loss: 0.00002399
Iteration 81/1000 | Loss: 0.00002398
Iteration 82/1000 | Loss: 0.00002397
Iteration 83/1000 | Loss: 0.00002397
Iteration 84/1000 | Loss: 0.00002397
Iteration 85/1000 | Loss: 0.00002396
Iteration 86/1000 | Loss: 0.00002396
Iteration 87/1000 | Loss: 0.00002396
Iteration 88/1000 | Loss: 0.00002396
Iteration 89/1000 | Loss: 0.00002395
Iteration 90/1000 | Loss: 0.00002395
Iteration 91/1000 | Loss: 0.00002395
Iteration 92/1000 | Loss: 0.00002395
Iteration 93/1000 | Loss: 0.00002395
Iteration 94/1000 | Loss: 0.00002395
Iteration 95/1000 | Loss: 0.00002395
Iteration 96/1000 | Loss: 0.00002394
Iteration 97/1000 | Loss: 0.00002394
Iteration 98/1000 | Loss: 0.00002394
Iteration 99/1000 | Loss: 0.00002393
Iteration 100/1000 | Loss: 0.00002393
Iteration 101/1000 | Loss: 0.00002393
Iteration 102/1000 | Loss: 0.00002393
Iteration 103/1000 | Loss: 0.00002393
Iteration 104/1000 | Loss: 0.00002393
Iteration 105/1000 | Loss: 0.00002393
Iteration 106/1000 | Loss: 0.00002393
Iteration 107/1000 | Loss: 0.00002393
Iteration 108/1000 | Loss: 0.00002393
Iteration 109/1000 | Loss: 0.00002393
Iteration 110/1000 | Loss: 0.00002393
Iteration 111/1000 | Loss: 0.00002392
Iteration 112/1000 | Loss: 0.00002392
Iteration 113/1000 | Loss: 0.00002392
Iteration 114/1000 | Loss: 0.00002392
Iteration 115/1000 | Loss: 0.00002392
Iteration 116/1000 | Loss: 0.00002392
Iteration 117/1000 | Loss: 0.00002392
Iteration 118/1000 | Loss: 0.00002391
Iteration 119/1000 | Loss: 0.00002391
Iteration 120/1000 | Loss: 0.00002391
Iteration 121/1000 | Loss: 0.00002391
Iteration 122/1000 | Loss: 0.00002391
Iteration 123/1000 | Loss: 0.00002391
Iteration 124/1000 | Loss: 0.00002390
Iteration 125/1000 | Loss: 0.00002390
Iteration 126/1000 | Loss: 0.00002390
Iteration 127/1000 | Loss: 0.00002390
Iteration 128/1000 | Loss: 0.00002390
Iteration 129/1000 | Loss: 0.00002390
Iteration 130/1000 | Loss: 0.00002390
Iteration 131/1000 | Loss: 0.00002390
Iteration 132/1000 | Loss: 0.00002390
Iteration 133/1000 | Loss: 0.00002390
Iteration 134/1000 | Loss: 0.00002390
Iteration 135/1000 | Loss: 0.00002390
Iteration 136/1000 | Loss: 0.00002390
Iteration 137/1000 | Loss: 0.00002389
Iteration 138/1000 | Loss: 0.00002389
Iteration 139/1000 | Loss: 0.00002389
Iteration 140/1000 | Loss: 0.00002389
Iteration 141/1000 | Loss: 0.00002388
Iteration 142/1000 | Loss: 0.00002388
Iteration 143/1000 | Loss: 0.00002388
Iteration 144/1000 | Loss: 0.00002388
Iteration 145/1000 | Loss: 0.00002388
Iteration 146/1000 | Loss: 0.00002388
Iteration 147/1000 | Loss: 0.00002388
Iteration 148/1000 | Loss: 0.00002388
Iteration 149/1000 | Loss: 0.00002388
Iteration 150/1000 | Loss: 0.00002387
Iteration 151/1000 | Loss: 0.00002387
Iteration 152/1000 | Loss: 0.00002387
Iteration 153/1000 | Loss: 0.00002386
Iteration 154/1000 | Loss: 0.00002386
Iteration 155/1000 | Loss: 0.00002386
Iteration 156/1000 | Loss: 0.00002385
Iteration 157/1000 | Loss: 0.00002385
Iteration 158/1000 | Loss: 0.00002385
Iteration 159/1000 | Loss: 0.00002385
Iteration 160/1000 | Loss: 0.00002385
Iteration 161/1000 | Loss: 0.00002384
Iteration 162/1000 | Loss: 0.00002384
Iteration 163/1000 | Loss: 0.00002384
Iteration 164/1000 | Loss: 0.00002384
Iteration 165/1000 | Loss: 0.00002384
Iteration 166/1000 | Loss: 0.00002384
Iteration 167/1000 | Loss: 0.00002384
Iteration 168/1000 | Loss: 0.00002384
Iteration 169/1000 | Loss: 0.00002383
Iteration 170/1000 | Loss: 0.00002383
Iteration 171/1000 | Loss: 0.00002383
Iteration 172/1000 | Loss: 0.00002383
Iteration 173/1000 | Loss: 0.00002382
Iteration 174/1000 | Loss: 0.00002382
Iteration 175/1000 | Loss: 0.00002382
Iteration 176/1000 | Loss: 0.00002382
Iteration 177/1000 | Loss: 0.00002382
Iteration 178/1000 | Loss: 0.00002382
Iteration 179/1000 | Loss: 0.00002382
Iteration 180/1000 | Loss: 0.00002382
Iteration 181/1000 | Loss: 0.00002381
Iteration 182/1000 | Loss: 0.00002381
Iteration 183/1000 | Loss: 0.00002381
Iteration 184/1000 | Loss: 0.00002381
Iteration 185/1000 | Loss: 0.00002381
Iteration 186/1000 | Loss: 0.00002381
Iteration 187/1000 | Loss: 0.00002381
Iteration 188/1000 | Loss: 0.00002381
Iteration 189/1000 | Loss: 0.00002381
Iteration 190/1000 | Loss: 0.00002381
Iteration 191/1000 | Loss: 0.00002381
Iteration 192/1000 | Loss: 0.00002381
Iteration 193/1000 | Loss: 0.00002381
Iteration 194/1000 | Loss: 0.00002380
Iteration 195/1000 | Loss: 0.00002380
Iteration 196/1000 | Loss: 0.00002380
Iteration 197/1000 | Loss: 0.00002380
Iteration 198/1000 | Loss: 0.00002380
Iteration 199/1000 | Loss: 0.00002380
Iteration 200/1000 | Loss: 0.00002380
Iteration 201/1000 | Loss: 0.00002380
Iteration 202/1000 | Loss: 0.00002380
Iteration 203/1000 | Loss: 0.00002380
Iteration 204/1000 | Loss: 0.00002380
Iteration 205/1000 | Loss: 0.00002379
Iteration 206/1000 | Loss: 0.00002379
Iteration 207/1000 | Loss: 0.00002379
Iteration 208/1000 | Loss: 0.00002379
Iteration 209/1000 | Loss: 0.00002379
Iteration 210/1000 | Loss: 0.00002379
Iteration 211/1000 | Loss: 0.00002379
Iteration 212/1000 | Loss: 0.00002379
Iteration 213/1000 | Loss: 0.00002379
Iteration 214/1000 | Loss: 0.00002379
Iteration 215/1000 | Loss: 0.00002379
Iteration 216/1000 | Loss: 0.00002379
Iteration 217/1000 | Loss: 0.00002379
Iteration 218/1000 | Loss: 0.00002379
Iteration 219/1000 | Loss: 0.00002379
Iteration 220/1000 | Loss: 0.00002379
Iteration 221/1000 | Loss: 0.00002379
Iteration 222/1000 | Loss: 0.00002379
Iteration 223/1000 | Loss: 0.00002379
Iteration 224/1000 | Loss: 0.00002379
Iteration 225/1000 | Loss: 0.00002379
Iteration 226/1000 | Loss: 0.00002379
Iteration 227/1000 | Loss: 0.00002379
Iteration 228/1000 | Loss: 0.00002379
Iteration 229/1000 | Loss: 0.00002379
Iteration 230/1000 | Loss: 0.00002379
Iteration 231/1000 | Loss: 0.00002379
Iteration 232/1000 | Loss: 0.00002379
Iteration 233/1000 | Loss: 0.00002379
Iteration 234/1000 | Loss: 0.00002379
Iteration 235/1000 | Loss: 0.00002379
Iteration 236/1000 | Loss: 0.00002379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.3792541469447315e-05, 2.3792541469447315e-05, 2.3792541469447315e-05, 2.3792541469447315e-05, 2.3792541469447315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3792541469447315e-05

Optimization complete. Final v2v error: 3.976738452911377 mm

Highest mean error: 6.184429168701172 mm for frame 132

Lowest mean error: 3.099766731262207 mm for frame 173

Saving results

Total time: 73.35299897193909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01127002
Iteration 2/25 | Loss: 0.00148497
Iteration 3/25 | Loss: 0.00094930
Iteration 4/25 | Loss: 0.00088446
Iteration 5/25 | Loss: 0.00088422
Iteration 6/25 | Loss: 0.00086594
Iteration 7/25 | Loss: 0.00085038
Iteration 8/25 | Loss: 0.00086074
Iteration 9/25 | Loss: 0.00085409
Iteration 10/25 | Loss: 0.00083511
Iteration 11/25 | Loss: 0.00084379
Iteration 12/25 | Loss: 0.00084057
Iteration 13/25 | Loss: 0.00085463
Iteration 14/25 | Loss: 0.00083625
Iteration 15/25 | Loss: 0.00083314
Iteration 16/25 | Loss: 0.00083015
Iteration 17/25 | Loss: 0.00082702
Iteration 18/25 | Loss: 0.00082084
Iteration 19/25 | Loss: 0.00082239
Iteration 20/25 | Loss: 0.00081952
Iteration 21/25 | Loss: 0.00081946
Iteration 22/25 | Loss: 0.00081946
Iteration 23/25 | Loss: 0.00081946
Iteration 24/25 | Loss: 0.00081946
Iteration 25/25 | Loss: 0.00081946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10222018
Iteration 2/25 | Loss: 0.00030024
Iteration 3/25 | Loss: 0.00030024
Iteration 4/25 | Loss: 0.00030024
Iteration 5/25 | Loss: 0.00030024
Iteration 6/25 | Loss: 0.00030024
Iteration 7/25 | Loss: 0.00030024
Iteration 8/25 | Loss: 0.00030024
Iteration 9/25 | Loss: 0.00030024
Iteration 10/25 | Loss: 0.00030024
Iteration 11/25 | Loss: 0.00030024
Iteration 12/25 | Loss: 0.00030024
Iteration 13/25 | Loss: 0.00030024
Iteration 14/25 | Loss: 0.00030024
Iteration 15/25 | Loss: 0.00030024
Iteration 16/25 | Loss: 0.00030024
Iteration 17/25 | Loss: 0.00030024
Iteration 18/25 | Loss: 0.00030024
Iteration 19/25 | Loss: 0.00030024
Iteration 20/25 | Loss: 0.00030024
Iteration 21/25 | Loss: 0.00030024
Iteration 22/25 | Loss: 0.00030024
Iteration 23/25 | Loss: 0.00030024
Iteration 24/25 | Loss: 0.00030024
Iteration 25/25 | Loss: 0.00030024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030024
Iteration 2/1000 | Loss: 0.00006472
Iteration 3/1000 | Loss: 0.00004357
Iteration 4/1000 | Loss: 0.00003401
Iteration 5/1000 | Loss: 0.00003130
Iteration 6/1000 | Loss: 0.00006905
Iteration 7/1000 | Loss: 0.00002907
Iteration 8/1000 | Loss: 0.00002795
Iteration 9/1000 | Loss: 0.00023786
Iteration 10/1000 | Loss: 0.00005564
Iteration 11/1000 | Loss: 0.00002634
Iteration 12/1000 | Loss: 0.00003897
Iteration 13/1000 | Loss: 0.00002475
Iteration 14/1000 | Loss: 0.00002420
Iteration 15/1000 | Loss: 0.00002395
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002359
Iteration 18/1000 | Loss: 0.00002353
Iteration 19/1000 | Loss: 0.00002350
Iteration 20/1000 | Loss: 0.00002346
Iteration 21/1000 | Loss: 0.00002333
Iteration 22/1000 | Loss: 0.00002328
Iteration 23/1000 | Loss: 0.00002326
Iteration 24/1000 | Loss: 0.00002326
Iteration 25/1000 | Loss: 0.00002326
Iteration 26/1000 | Loss: 0.00002326
Iteration 27/1000 | Loss: 0.00002326
Iteration 28/1000 | Loss: 0.00002326
Iteration 29/1000 | Loss: 0.00002326
Iteration 30/1000 | Loss: 0.00002325
Iteration 31/1000 | Loss: 0.00002325
Iteration 32/1000 | Loss: 0.00002325
Iteration 33/1000 | Loss: 0.00002325
Iteration 34/1000 | Loss: 0.00002325
Iteration 35/1000 | Loss: 0.00002325
Iteration 36/1000 | Loss: 0.00002325
Iteration 37/1000 | Loss: 0.00002325
Iteration 38/1000 | Loss: 0.00002325
Iteration 39/1000 | Loss: 0.00002325
Iteration 40/1000 | Loss: 0.00002325
Iteration 41/1000 | Loss: 0.00002324
Iteration 42/1000 | Loss: 0.00002323
Iteration 43/1000 | Loss: 0.00002322
Iteration 44/1000 | Loss: 0.00002321
Iteration 45/1000 | Loss: 0.00002321
Iteration 46/1000 | Loss: 0.00002320
Iteration 47/1000 | Loss: 0.00002320
Iteration 48/1000 | Loss: 0.00002319
Iteration 49/1000 | Loss: 0.00002319
Iteration 50/1000 | Loss: 0.00002319
Iteration 51/1000 | Loss: 0.00002319
Iteration 52/1000 | Loss: 0.00002318
Iteration 53/1000 | Loss: 0.00002318
Iteration 54/1000 | Loss: 0.00002318
Iteration 55/1000 | Loss: 0.00002317
Iteration 56/1000 | Loss: 0.00002317
Iteration 57/1000 | Loss: 0.00002317
Iteration 58/1000 | Loss: 0.00002317
Iteration 59/1000 | Loss: 0.00002317
Iteration 60/1000 | Loss: 0.00002317
Iteration 61/1000 | Loss: 0.00002317
Iteration 62/1000 | Loss: 0.00002317
Iteration 63/1000 | Loss: 0.00002317
Iteration 64/1000 | Loss: 0.00002317
Iteration 65/1000 | Loss: 0.00002317
Iteration 66/1000 | Loss: 0.00002317
Iteration 67/1000 | Loss: 0.00002317
Iteration 68/1000 | Loss: 0.00002317
Iteration 69/1000 | Loss: 0.00002316
Iteration 70/1000 | Loss: 0.00002316
Iteration 71/1000 | Loss: 0.00002316
Iteration 72/1000 | Loss: 0.00002316
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00002316
Iteration 75/1000 | Loss: 0.00002316
Iteration 76/1000 | Loss: 0.00002316
Iteration 77/1000 | Loss: 0.00002316
Iteration 78/1000 | Loss: 0.00002316
Iteration 79/1000 | Loss: 0.00002316
Iteration 80/1000 | Loss: 0.00002316
Iteration 81/1000 | Loss: 0.00002316
Iteration 82/1000 | Loss: 0.00002316
Iteration 83/1000 | Loss: 0.00002316
Iteration 84/1000 | Loss: 0.00002316
Iteration 85/1000 | Loss: 0.00002316
Iteration 86/1000 | Loss: 0.00002316
Iteration 87/1000 | Loss: 0.00002316
Iteration 88/1000 | Loss: 0.00002316
Iteration 89/1000 | Loss: 0.00002316
Iteration 90/1000 | Loss: 0.00002316
Iteration 91/1000 | Loss: 0.00002316
Iteration 92/1000 | Loss: 0.00002316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.3164338927017525e-05, 2.3164338927017525e-05, 2.3164338927017525e-05, 2.3164338927017525e-05, 2.3164338927017525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3164338927017525e-05

Optimization complete. Final v2v error: 4.062803745269775 mm

Highest mean error: 4.319324970245361 mm for frame 138

Lowest mean error: 3.8248140811920166 mm for frame 7

Saving results

Total time: 69.17138409614563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418611
Iteration 2/25 | Loss: 0.00087762
Iteration 3/25 | Loss: 0.00073597
Iteration 4/25 | Loss: 0.00070404
Iteration 5/25 | Loss: 0.00069691
Iteration 6/25 | Loss: 0.00069477
Iteration 7/25 | Loss: 0.00069431
Iteration 8/25 | Loss: 0.00069431
Iteration 9/25 | Loss: 0.00069431
Iteration 10/25 | Loss: 0.00069431
Iteration 11/25 | Loss: 0.00069431
Iteration 12/25 | Loss: 0.00069431
Iteration 13/25 | Loss: 0.00069431
Iteration 14/25 | Loss: 0.00069431
Iteration 15/25 | Loss: 0.00069431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006943114567548037, 0.0006943114567548037, 0.0006943114567548037, 0.0006943114567548037, 0.0006943114567548037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006943114567548037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45568156
Iteration 2/25 | Loss: 0.00024924
Iteration 3/25 | Loss: 0.00024924
Iteration 4/25 | Loss: 0.00024924
Iteration 5/25 | Loss: 0.00024924
Iteration 6/25 | Loss: 0.00024924
Iteration 7/25 | Loss: 0.00024924
Iteration 8/25 | Loss: 0.00024923
Iteration 9/25 | Loss: 0.00024923
Iteration 10/25 | Loss: 0.00024923
Iteration 11/25 | Loss: 0.00024923
Iteration 12/25 | Loss: 0.00024923
Iteration 13/25 | Loss: 0.00024923
Iteration 14/25 | Loss: 0.00024923
Iteration 15/25 | Loss: 0.00024923
Iteration 16/25 | Loss: 0.00024923
Iteration 17/25 | Loss: 0.00024923
Iteration 18/25 | Loss: 0.00024923
Iteration 19/25 | Loss: 0.00024923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002492341154720634, 0.0002492341154720634, 0.0002492341154720634, 0.0002492341154720634, 0.0002492341154720634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002492341154720634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024923
Iteration 2/1000 | Loss: 0.00002052
Iteration 3/1000 | Loss: 0.00001560
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001371
Iteration 6/1000 | Loss: 0.00001308
Iteration 7/1000 | Loss: 0.00001276
Iteration 8/1000 | Loss: 0.00001250
Iteration 9/1000 | Loss: 0.00001235
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001197
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001193
Iteration 17/1000 | Loss: 0.00001193
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001189
Iteration 26/1000 | Loss: 0.00001189
Iteration 27/1000 | Loss: 0.00001188
Iteration 28/1000 | Loss: 0.00001188
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001186
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001185
Iteration 35/1000 | Loss: 0.00001185
Iteration 36/1000 | Loss: 0.00001184
Iteration 37/1000 | Loss: 0.00001183
Iteration 38/1000 | Loss: 0.00001183
Iteration 39/1000 | Loss: 0.00001183
Iteration 40/1000 | Loss: 0.00001182
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001176
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001175
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001167
Iteration 117/1000 | Loss: 0.00001167
Iteration 118/1000 | Loss: 0.00001167
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001166
Iteration 122/1000 | Loss: 0.00001166
Iteration 123/1000 | Loss: 0.00001166
Iteration 124/1000 | Loss: 0.00001166
Iteration 125/1000 | Loss: 0.00001166
Iteration 126/1000 | Loss: 0.00001166
Iteration 127/1000 | Loss: 0.00001165
Iteration 128/1000 | Loss: 0.00001165
Iteration 129/1000 | Loss: 0.00001164
Iteration 130/1000 | Loss: 0.00001164
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001163
Iteration 135/1000 | Loss: 0.00001163
Iteration 136/1000 | Loss: 0.00001163
Iteration 137/1000 | Loss: 0.00001163
Iteration 138/1000 | Loss: 0.00001162
Iteration 139/1000 | Loss: 0.00001162
Iteration 140/1000 | Loss: 0.00001162
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Iteration 143/1000 | Loss: 0.00001161
Iteration 144/1000 | Loss: 0.00001161
Iteration 145/1000 | Loss: 0.00001160
Iteration 146/1000 | Loss: 0.00001160
Iteration 147/1000 | Loss: 0.00001160
Iteration 148/1000 | Loss: 0.00001160
Iteration 149/1000 | Loss: 0.00001160
Iteration 150/1000 | Loss: 0.00001159
Iteration 151/1000 | Loss: 0.00001159
Iteration 152/1000 | Loss: 0.00001159
Iteration 153/1000 | Loss: 0.00001159
Iteration 154/1000 | Loss: 0.00001159
Iteration 155/1000 | Loss: 0.00001159
Iteration 156/1000 | Loss: 0.00001159
Iteration 157/1000 | Loss: 0.00001158
Iteration 158/1000 | Loss: 0.00001158
Iteration 159/1000 | Loss: 0.00001158
Iteration 160/1000 | Loss: 0.00001158
Iteration 161/1000 | Loss: 0.00001158
Iteration 162/1000 | Loss: 0.00001158
Iteration 163/1000 | Loss: 0.00001158
Iteration 164/1000 | Loss: 0.00001158
Iteration 165/1000 | Loss: 0.00001158
Iteration 166/1000 | Loss: 0.00001158
Iteration 167/1000 | Loss: 0.00001158
Iteration 168/1000 | Loss: 0.00001158
Iteration 169/1000 | Loss: 0.00001158
Iteration 170/1000 | Loss: 0.00001158
Iteration 171/1000 | Loss: 0.00001158
Iteration 172/1000 | Loss: 0.00001158
Iteration 173/1000 | Loss: 0.00001158
Iteration 174/1000 | Loss: 0.00001158
Iteration 175/1000 | Loss: 0.00001158
Iteration 176/1000 | Loss: 0.00001158
Iteration 177/1000 | Loss: 0.00001158
Iteration 178/1000 | Loss: 0.00001158
Iteration 179/1000 | Loss: 0.00001158
Iteration 180/1000 | Loss: 0.00001158
Iteration 181/1000 | Loss: 0.00001158
Iteration 182/1000 | Loss: 0.00001158
Iteration 183/1000 | Loss: 0.00001158
Iteration 184/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.1582649676711299e-05, 1.1582649676711299e-05, 1.1582649676711299e-05, 1.1582649676711299e-05, 1.1582649676711299e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1582649676711299e-05

Optimization complete. Final v2v error: 2.9123589992523193 mm

Highest mean error: 3.8598690032958984 mm for frame 64

Lowest mean error: 2.648350477218628 mm for frame 96

Saving results

Total time: 43.567057371139526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515478
Iteration 2/25 | Loss: 0.00097041
Iteration 3/25 | Loss: 0.00079413
Iteration 4/25 | Loss: 0.00076737
Iteration 5/25 | Loss: 0.00076072
Iteration 6/25 | Loss: 0.00076024
Iteration 7/25 | Loss: 0.00076024
Iteration 8/25 | Loss: 0.00076024
Iteration 9/25 | Loss: 0.00076024
Iteration 10/25 | Loss: 0.00076024
Iteration 11/25 | Loss: 0.00076024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007602370460517704, 0.0007602370460517704, 0.0007602370460517704, 0.0007602370460517704, 0.0007602370460517704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007602370460517704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86455274
Iteration 2/25 | Loss: 0.00026191
Iteration 3/25 | Loss: 0.00026190
Iteration 4/25 | Loss: 0.00026190
Iteration 5/25 | Loss: 0.00026190
Iteration 6/25 | Loss: 0.00026190
Iteration 7/25 | Loss: 0.00026190
Iteration 8/25 | Loss: 0.00026190
Iteration 9/25 | Loss: 0.00026190
Iteration 10/25 | Loss: 0.00026190
Iteration 11/25 | Loss: 0.00026190
Iteration 12/25 | Loss: 0.00026190
Iteration 13/25 | Loss: 0.00026190
Iteration 14/25 | Loss: 0.00026190
Iteration 15/25 | Loss: 0.00026190
Iteration 16/25 | Loss: 0.00026190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00026189631898887455, 0.00026189631898887455, 0.00026189631898887455, 0.00026189631898887455, 0.00026189631898887455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026189631898887455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026190
Iteration 2/1000 | Loss: 0.00002652
Iteration 3/1000 | Loss: 0.00001949
Iteration 4/1000 | Loss: 0.00001845
Iteration 5/1000 | Loss: 0.00001769
Iteration 6/1000 | Loss: 0.00001713
Iteration 7/1000 | Loss: 0.00001669
Iteration 8/1000 | Loss: 0.00001643
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001634
Iteration 11/1000 | Loss: 0.00001634
Iteration 12/1000 | Loss: 0.00001634
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001633
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001610
Iteration 17/1000 | Loss: 0.00001610
Iteration 18/1000 | Loss: 0.00001609
Iteration 19/1000 | Loss: 0.00001596
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001596
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001589
Iteration 24/1000 | Loss: 0.00001589
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001586
Iteration 27/1000 | Loss: 0.00001585
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001585
Iteration 30/1000 | Loss: 0.00001585
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001584
Iteration 34/1000 | Loss: 0.00001584
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001584
Iteration 38/1000 | Loss: 0.00001583
Iteration 39/1000 | Loss: 0.00001583
Iteration 40/1000 | Loss: 0.00001582
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001582
Iteration 43/1000 | Loss: 0.00001582
Iteration 44/1000 | Loss: 0.00001582
Iteration 45/1000 | Loss: 0.00001581
Iteration 46/1000 | Loss: 0.00001581
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001579
Iteration 49/1000 | Loss: 0.00001578
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001576
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001575
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001573
Iteration 63/1000 | Loss: 0.00001573
Iteration 64/1000 | Loss: 0.00001572
Iteration 65/1000 | Loss: 0.00001572
Iteration 66/1000 | Loss: 0.00001572
Iteration 67/1000 | Loss: 0.00001572
Iteration 68/1000 | Loss: 0.00001572
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001572
Iteration 72/1000 | Loss: 0.00001572
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001571
Iteration 93/1000 | Loss: 0.00001571
Iteration 94/1000 | Loss: 0.00001571
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001571
Iteration 99/1000 | Loss: 0.00001571
Iteration 100/1000 | Loss: 0.00001571
Iteration 101/1000 | Loss: 0.00001571
Iteration 102/1000 | Loss: 0.00001571
Iteration 103/1000 | Loss: 0.00001571
Iteration 104/1000 | Loss: 0.00001571
Iteration 105/1000 | Loss: 0.00001571
Iteration 106/1000 | Loss: 0.00001571
Iteration 107/1000 | Loss: 0.00001571
Iteration 108/1000 | Loss: 0.00001570
Iteration 109/1000 | Loss: 0.00001570
Iteration 110/1000 | Loss: 0.00001570
Iteration 111/1000 | Loss: 0.00001570
Iteration 112/1000 | Loss: 0.00001570
Iteration 113/1000 | Loss: 0.00001570
Iteration 114/1000 | Loss: 0.00001570
Iteration 115/1000 | Loss: 0.00001570
Iteration 116/1000 | Loss: 0.00001570
Iteration 117/1000 | Loss: 0.00001570
Iteration 118/1000 | Loss: 0.00001570
Iteration 119/1000 | Loss: 0.00001570
Iteration 120/1000 | Loss: 0.00001570
Iteration 121/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.570484346302692e-05, 1.570484346302692e-05, 1.570484346302692e-05, 1.570484346302692e-05, 1.570484346302692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.570484346302692e-05

Optimization complete. Final v2v error: 3.400631904602051 mm

Highest mean error: 3.6078004837036133 mm for frame 0

Lowest mean error: 3.2438271045684814 mm for frame 168

Saving results

Total time: 31.08579182624817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01119522
Iteration 2/25 | Loss: 0.00176585
Iteration 3/25 | Loss: 0.00113943
Iteration 4/25 | Loss: 0.00104803
Iteration 5/25 | Loss: 0.00101059
Iteration 6/25 | Loss: 0.00120844
Iteration 7/25 | Loss: 0.00110791
Iteration 8/25 | Loss: 0.00104476
Iteration 9/25 | Loss: 0.00096876
Iteration 10/25 | Loss: 0.00095073
Iteration 11/25 | Loss: 0.00104832
Iteration 12/25 | Loss: 0.00094174
Iteration 13/25 | Loss: 0.00092848
Iteration 14/25 | Loss: 0.00094600
Iteration 15/25 | Loss: 0.00092043
Iteration 16/25 | Loss: 0.00093685
Iteration 17/25 | Loss: 0.00088356
Iteration 18/25 | Loss: 0.00090320
Iteration 19/25 | Loss: 0.00088459
Iteration 20/25 | Loss: 0.00086554
Iteration 21/25 | Loss: 0.00089532
Iteration 22/25 | Loss: 0.00086167
Iteration 23/25 | Loss: 0.00085361
Iteration 24/25 | Loss: 0.00085317
Iteration 25/25 | Loss: 0.00085313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13500178
Iteration 2/25 | Loss: 0.00030050
Iteration 3/25 | Loss: 0.00030049
Iteration 4/25 | Loss: 0.00030049
Iteration 5/25 | Loss: 0.00030049
Iteration 6/25 | Loss: 0.00030049
Iteration 7/25 | Loss: 0.00030049
Iteration 8/25 | Loss: 0.00030049
Iteration 9/25 | Loss: 0.00030049
Iteration 10/25 | Loss: 0.00030049
Iteration 11/25 | Loss: 0.00030049
Iteration 12/25 | Loss: 0.00030049
Iteration 13/25 | Loss: 0.00030049
Iteration 14/25 | Loss: 0.00030049
Iteration 15/25 | Loss: 0.00030049
Iteration 16/25 | Loss: 0.00030049
Iteration 17/25 | Loss: 0.00030049
Iteration 18/25 | Loss: 0.00030049
Iteration 19/25 | Loss: 0.00030049
Iteration 20/25 | Loss: 0.00030049
Iteration 21/25 | Loss: 0.00030049
Iteration 22/25 | Loss: 0.00030049
Iteration 23/25 | Loss: 0.00030049
Iteration 24/25 | Loss: 0.00030049
Iteration 25/25 | Loss: 0.00030049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00030048732878640294, 0.00030048732878640294, 0.00030048732878640294, 0.00030048732878640294, 0.00030048732878640294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030048732878640294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030049
Iteration 2/1000 | Loss: 0.00004991
Iteration 3/1000 | Loss: 0.00003491
Iteration 4/1000 | Loss: 0.00003077
Iteration 5/1000 | Loss: 0.00002944
Iteration 6/1000 | Loss: 0.00002861
Iteration 7/1000 | Loss: 0.00002815
Iteration 8/1000 | Loss: 0.00002770
Iteration 9/1000 | Loss: 0.00002740
Iteration 10/1000 | Loss: 0.00002714
Iteration 11/1000 | Loss: 0.00002711
Iteration 12/1000 | Loss: 0.00002693
Iteration 13/1000 | Loss: 0.00002677
Iteration 14/1000 | Loss: 0.00002668
Iteration 15/1000 | Loss: 0.00002654
Iteration 16/1000 | Loss: 0.00002650
Iteration 17/1000 | Loss: 0.00002648
Iteration 18/1000 | Loss: 0.00002647
Iteration 19/1000 | Loss: 0.00002647
Iteration 20/1000 | Loss: 0.00002647
Iteration 21/1000 | Loss: 0.00002642
Iteration 22/1000 | Loss: 0.00002639
Iteration 23/1000 | Loss: 0.00002638
Iteration 24/1000 | Loss: 0.00002632
Iteration 25/1000 | Loss: 0.00002632
Iteration 26/1000 | Loss: 0.00002632
Iteration 27/1000 | Loss: 0.00002627
Iteration 28/1000 | Loss: 0.00002627
Iteration 29/1000 | Loss: 0.00002624
Iteration 30/1000 | Loss: 0.00002624
Iteration 31/1000 | Loss: 0.00002623
Iteration 32/1000 | Loss: 0.00002622
Iteration 33/1000 | Loss: 0.00002621
Iteration 34/1000 | Loss: 0.00002621
Iteration 35/1000 | Loss: 0.00002619
Iteration 36/1000 | Loss: 0.00002618
Iteration 37/1000 | Loss: 0.00002618
Iteration 38/1000 | Loss: 0.00002617
Iteration 39/1000 | Loss: 0.00002616
Iteration 40/1000 | Loss: 0.00002615
Iteration 41/1000 | Loss: 0.00002615
Iteration 42/1000 | Loss: 0.00002615
Iteration 43/1000 | Loss: 0.00002615
Iteration 44/1000 | Loss: 0.00002614
Iteration 45/1000 | Loss: 0.00002614
Iteration 46/1000 | Loss: 0.00002614
Iteration 47/1000 | Loss: 0.00002613
Iteration 48/1000 | Loss: 0.00002613
Iteration 49/1000 | Loss: 0.00002613
Iteration 50/1000 | Loss: 0.00002612
Iteration 51/1000 | Loss: 0.00002612
Iteration 52/1000 | Loss: 0.00002612
Iteration 53/1000 | Loss: 0.00002611
Iteration 54/1000 | Loss: 0.00002611
Iteration 55/1000 | Loss: 0.00002611
Iteration 56/1000 | Loss: 0.00002611
Iteration 57/1000 | Loss: 0.00002610
Iteration 58/1000 | Loss: 0.00002610
Iteration 59/1000 | Loss: 0.00002610
Iteration 60/1000 | Loss: 0.00002610
Iteration 61/1000 | Loss: 0.00002609
Iteration 62/1000 | Loss: 0.00002609
Iteration 63/1000 | Loss: 0.00002609
Iteration 64/1000 | Loss: 0.00002609
Iteration 65/1000 | Loss: 0.00002609
Iteration 66/1000 | Loss: 0.00002608
Iteration 67/1000 | Loss: 0.00002608
Iteration 68/1000 | Loss: 0.00002608
Iteration 69/1000 | Loss: 0.00002607
Iteration 70/1000 | Loss: 0.00002607
Iteration 71/1000 | Loss: 0.00002607
Iteration 72/1000 | Loss: 0.00002607
Iteration 73/1000 | Loss: 0.00002606
Iteration 74/1000 | Loss: 0.00002606
Iteration 75/1000 | Loss: 0.00002606
Iteration 76/1000 | Loss: 0.00002606
Iteration 77/1000 | Loss: 0.00002606
Iteration 78/1000 | Loss: 0.00002605
Iteration 79/1000 | Loss: 0.00002605
Iteration 80/1000 | Loss: 0.00002605
Iteration 81/1000 | Loss: 0.00002605
Iteration 82/1000 | Loss: 0.00002605
Iteration 83/1000 | Loss: 0.00002605
Iteration 84/1000 | Loss: 0.00002605
Iteration 85/1000 | Loss: 0.00002604
Iteration 86/1000 | Loss: 0.00002604
Iteration 87/1000 | Loss: 0.00002604
Iteration 88/1000 | Loss: 0.00002604
Iteration 89/1000 | Loss: 0.00002604
Iteration 90/1000 | Loss: 0.00002604
Iteration 91/1000 | Loss: 0.00002604
Iteration 92/1000 | Loss: 0.00002604
Iteration 93/1000 | Loss: 0.00002604
Iteration 94/1000 | Loss: 0.00002604
Iteration 95/1000 | Loss: 0.00002604
Iteration 96/1000 | Loss: 0.00002603
Iteration 97/1000 | Loss: 0.00002603
Iteration 98/1000 | Loss: 0.00002603
Iteration 99/1000 | Loss: 0.00002603
Iteration 100/1000 | Loss: 0.00002602
Iteration 101/1000 | Loss: 0.00002602
Iteration 102/1000 | Loss: 0.00002602
Iteration 103/1000 | Loss: 0.00002602
Iteration 104/1000 | Loss: 0.00002601
Iteration 105/1000 | Loss: 0.00002601
Iteration 106/1000 | Loss: 0.00002601
Iteration 107/1000 | Loss: 0.00002601
Iteration 108/1000 | Loss: 0.00002600
Iteration 109/1000 | Loss: 0.00002600
Iteration 110/1000 | Loss: 0.00002600
Iteration 111/1000 | Loss: 0.00002600
Iteration 112/1000 | Loss: 0.00002600
Iteration 113/1000 | Loss: 0.00002599
Iteration 114/1000 | Loss: 0.00002599
Iteration 115/1000 | Loss: 0.00002599
Iteration 116/1000 | Loss: 0.00002599
Iteration 117/1000 | Loss: 0.00002598
Iteration 118/1000 | Loss: 0.00002598
Iteration 119/1000 | Loss: 0.00002598
Iteration 120/1000 | Loss: 0.00002598
Iteration 121/1000 | Loss: 0.00002597
Iteration 122/1000 | Loss: 0.00002597
Iteration 123/1000 | Loss: 0.00002597
Iteration 124/1000 | Loss: 0.00002597
Iteration 125/1000 | Loss: 0.00002597
Iteration 126/1000 | Loss: 0.00002597
Iteration 127/1000 | Loss: 0.00002596
Iteration 128/1000 | Loss: 0.00002596
Iteration 129/1000 | Loss: 0.00002596
Iteration 130/1000 | Loss: 0.00002596
Iteration 131/1000 | Loss: 0.00002596
Iteration 132/1000 | Loss: 0.00002596
Iteration 133/1000 | Loss: 0.00002596
Iteration 134/1000 | Loss: 0.00002596
Iteration 135/1000 | Loss: 0.00002596
Iteration 136/1000 | Loss: 0.00002595
Iteration 137/1000 | Loss: 0.00002595
Iteration 138/1000 | Loss: 0.00002595
Iteration 139/1000 | Loss: 0.00002595
Iteration 140/1000 | Loss: 0.00002595
Iteration 141/1000 | Loss: 0.00002595
Iteration 142/1000 | Loss: 0.00002595
Iteration 143/1000 | Loss: 0.00002595
Iteration 144/1000 | Loss: 0.00002595
Iteration 145/1000 | Loss: 0.00002595
Iteration 146/1000 | Loss: 0.00002595
Iteration 147/1000 | Loss: 0.00002595
Iteration 148/1000 | Loss: 0.00002595
Iteration 149/1000 | Loss: 0.00002595
Iteration 150/1000 | Loss: 0.00002595
Iteration 151/1000 | Loss: 0.00002595
Iteration 152/1000 | Loss: 0.00002595
Iteration 153/1000 | Loss: 0.00002595
Iteration 154/1000 | Loss: 0.00002595
Iteration 155/1000 | Loss: 0.00002595
Iteration 156/1000 | Loss: 0.00002595
Iteration 157/1000 | Loss: 0.00002595
Iteration 158/1000 | Loss: 0.00002595
Iteration 159/1000 | Loss: 0.00002595
Iteration 160/1000 | Loss: 0.00002595
Iteration 161/1000 | Loss: 0.00002595
Iteration 162/1000 | Loss: 0.00002595
Iteration 163/1000 | Loss: 0.00002595
Iteration 164/1000 | Loss: 0.00002595
Iteration 165/1000 | Loss: 0.00002595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.5953768272302113e-05, 2.5953768272302113e-05, 2.5953768272302113e-05, 2.5953768272302113e-05, 2.5953768272302113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5953768272302113e-05

Optimization complete. Final v2v error: 4.0303635597229 mm

Highest mean error: 5.336244106292725 mm for frame 50

Lowest mean error: 3.208038091659546 mm for frame 99

Saving results

Total time: 77.8675639629364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00599993
Iteration 2/25 | Loss: 0.00125737
Iteration 3/25 | Loss: 0.00081235
Iteration 4/25 | Loss: 0.00074440
Iteration 5/25 | Loss: 0.00073394
Iteration 6/25 | Loss: 0.00073123
Iteration 7/25 | Loss: 0.00073097
Iteration 8/25 | Loss: 0.00073097
Iteration 9/25 | Loss: 0.00073097
Iteration 10/25 | Loss: 0.00073097
Iteration 11/25 | Loss: 0.00073097
Iteration 12/25 | Loss: 0.00073097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007309719221666455, 0.0007309719221666455, 0.0007309719221666455, 0.0007309719221666455, 0.0007309719221666455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007309719221666455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30870342
Iteration 2/25 | Loss: 0.00022690
Iteration 3/25 | Loss: 0.00022690
Iteration 4/25 | Loss: 0.00022690
Iteration 5/25 | Loss: 0.00022690
Iteration 6/25 | Loss: 0.00022690
Iteration 7/25 | Loss: 0.00022690
Iteration 8/25 | Loss: 0.00022690
Iteration 9/25 | Loss: 0.00022690
Iteration 10/25 | Loss: 0.00022690
Iteration 11/25 | Loss: 0.00022690
Iteration 12/25 | Loss: 0.00022690
Iteration 13/25 | Loss: 0.00022690
Iteration 14/25 | Loss: 0.00022690
Iteration 15/25 | Loss: 0.00022690
Iteration 16/25 | Loss: 0.00022690
Iteration 17/25 | Loss: 0.00022690
Iteration 18/25 | Loss: 0.00022690
Iteration 19/25 | Loss: 0.00022690
Iteration 20/25 | Loss: 0.00022690
Iteration 21/25 | Loss: 0.00022690
Iteration 22/25 | Loss: 0.00022690
Iteration 23/25 | Loss: 0.00022690
Iteration 24/25 | Loss: 0.00022690
Iteration 25/25 | Loss: 0.00022690

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022690
Iteration 2/1000 | Loss: 0.00002483
Iteration 3/1000 | Loss: 0.00001917
Iteration 4/1000 | Loss: 0.00001615
Iteration 5/1000 | Loss: 0.00001525
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001403
Iteration 9/1000 | Loss: 0.00001384
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001367
Iteration 12/1000 | Loss: 0.00001364
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001359
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001357
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001342
Iteration 23/1000 | Loss: 0.00001342
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001333
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001332
Iteration 33/1000 | Loss: 0.00001332
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001332
Iteration 36/1000 | Loss: 0.00001332
Iteration 37/1000 | Loss: 0.00001331
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001330
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001328
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001326
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001325
Iteration 75/1000 | Loss: 0.00001325
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001325
Iteration 78/1000 | Loss: 0.00001325
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.3252472854219377e-05, 1.3252472854219377e-05, 1.3252472854219377e-05, 1.3252472854219377e-05, 1.3252472854219377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3252472854219377e-05

Optimization complete. Final v2v error: 3.10132098197937 mm

Highest mean error: 3.766110420227051 mm for frame 80

Lowest mean error: 2.9614927768707275 mm for frame 7

Saving results

Total time: 32.430646896362305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029231
Iteration 2/25 | Loss: 0.01029231
Iteration 3/25 | Loss: 0.01029231
Iteration 4/25 | Loss: 0.00370605
Iteration 5/25 | Loss: 0.00183221
Iteration 6/25 | Loss: 0.00153292
Iteration 7/25 | Loss: 0.00129503
Iteration 8/25 | Loss: 0.00157321
Iteration 9/25 | Loss: 0.00149972
Iteration 10/25 | Loss: 0.00107514
Iteration 11/25 | Loss: 0.00098878
Iteration 12/25 | Loss: 0.00090589
Iteration 13/25 | Loss: 0.00088091
Iteration 14/25 | Loss: 0.00087410
Iteration 15/25 | Loss: 0.00087116
Iteration 16/25 | Loss: 0.00087044
Iteration 17/25 | Loss: 0.00087118
Iteration 18/25 | Loss: 0.00087013
Iteration 19/25 | Loss: 0.00087013
Iteration 20/25 | Loss: 0.00087013
Iteration 21/25 | Loss: 0.00087013
Iteration 22/25 | Loss: 0.00087013
Iteration 23/25 | Loss: 0.00087013
Iteration 24/25 | Loss: 0.00087013
Iteration 25/25 | Loss: 0.00087013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45776367
Iteration 2/25 | Loss: 0.00065154
Iteration 3/25 | Loss: 0.00032192
Iteration 4/25 | Loss: 0.00032192
Iteration 5/25 | Loss: 0.00032192
Iteration 6/25 | Loss: 0.00032192
Iteration 7/25 | Loss: 0.00032192
Iteration 8/25 | Loss: 0.00032192
Iteration 9/25 | Loss: 0.00032192
Iteration 10/25 | Loss: 0.00032192
Iteration 11/25 | Loss: 0.00032192
Iteration 12/25 | Loss: 0.00032192
Iteration 13/25 | Loss: 0.00032192
Iteration 14/25 | Loss: 0.00032192
Iteration 15/25 | Loss: 0.00032192
Iteration 16/25 | Loss: 0.00032192
Iteration 17/25 | Loss: 0.00032192
Iteration 18/25 | Loss: 0.00032192
Iteration 19/25 | Loss: 0.00032192
Iteration 20/25 | Loss: 0.00032192
Iteration 21/25 | Loss: 0.00032192
Iteration 22/25 | Loss: 0.00032192
Iteration 23/25 | Loss: 0.00032192
Iteration 24/25 | Loss: 0.00032192
Iteration 25/25 | Loss: 0.00032192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00032192139769904315, 0.00032192139769904315, 0.00032192139769904315, 0.00032192139769904315, 0.00032192139769904315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032192139769904315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032192
Iteration 2/1000 | Loss: 0.00057255
Iteration 3/1000 | Loss: 0.00007885
Iteration 4/1000 | Loss: 0.00007290
Iteration 5/1000 | Loss: 0.00007112
Iteration 6/1000 | Loss: 0.00007781
Iteration 7/1000 | Loss: 0.00011146
Iteration 8/1000 | Loss: 0.00004041
Iteration 9/1000 | Loss: 0.00026351
Iteration 10/1000 | Loss: 0.00004292
Iteration 11/1000 | Loss: 0.00004046
Iteration 12/1000 | Loss: 0.00016394
Iteration 13/1000 | Loss: 0.00004410
Iteration 14/1000 | Loss: 0.00004979
Iteration 15/1000 | Loss: 0.00011472
Iteration 16/1000 | Loss: 0.00039115
Iteration 17/1000 | Loss: 0.00005342
Iteration 18/1000 | Loss: 0.00023390
Iteration 19/1000 | Loss: 0.00004245
Iteration 20/1000 | Loss: 0.00004010
Iteration 21/1000 | Loss: 0.00018027
Iteration 22/1000 | Loss: 0.00004587
Iteration 23/1000 | Loss: 0.00003265
Iteration 24/1000 | Loss: 0.00003806
Iteration 25/1000 | Loss: 0.00043320
Iteration 26/1000 | Loss: 0.00014382
Iteration 27/1000 | Loss: 0.00006308
Iteration 28/1000 | Loss: 0.00008362
Iteration 29/1000 | Loss: 0.00003592
Iteration 30/1000 | Loss: 0.00002706
Iteration 31/1000 | Loss: 0.00003968
Iteration 32/1000 | Loss: 0.00003521
Iteration 33/1000 | Loss: 0.00004781
Iteration 34/1000 | Loss: 0.00008115
Iteration 35/1000 | Loss: 0.00022518
Iteration 36/1000 | Loss: 0.00003471
Iteration 37/1000 | Loss: 0.00002776
Iteration 38/1000 | Loss: 0.00002680
Iteration 39/1000 | Loss: 0.00002735
Iteration 40/1000 | Loss: 0.00002885
Iteration 41/1000 | Loss: 0.00002677
Iteration 42/1000 | Loss: 0.00002677
Iteration 43/1000 | Loss: 0.00002677
Iteration 44/1000 | Loss: 0.00003790
Iteration 45/1000 | Loss: 0.00003503
Iteration 46/1000 | Loss: 0.00002689
Iteration 47/1000 | Loss: 0.00003124
Iteration 48/1000 | Loss: 0.00007151
Iteration 49/1000 | Loss: 0.00028312
Iteration 50/1000 | Loss: 0.00003166
Iteration 51/1000 | Loss: 0.00002678
Iteration 52/1000 | Loss: 0.00002960
Iteration 53/1000 | Loss: 0.00002673
Iteration 54/1000 | Loss: 0.00002894
Iteration 55/1000 | Loss: 0.00002681
Iteration 56/1000 | Loss: 0.00002892
Iteration 57/1000 | Loss: 0.00016176
Iteration 58/1000 | Loss: 0.00016019
Iteration 59/1000 | Loss: 0.00011209
Iteration 60/1000 | Loss: 0.00017144
Iteration 61/1000 | Loss: 0.00012514
Iteration 62/1000 | Loss: 0.00004428
Iteration 63/1000 | Loss: 0.00005410
Iteration 64/1000 | Loss: 0.00004947
Iteration 65/1000 | Loss: 0.00028123
Iteration 66/1000 | Loss: 0.00002886
Iteration 67/1000 | Loss: 0.00004307
Iteration 68/1000 | Loss: 0.00002839
Iteration 69/1000 | Loss: 0.00004605
Iteration 70/1000 | Loss: 0.00002821
Iteration 71/1000 | Loss: 0.00002672
Iteration 72/1000 | Loss: 0.00004021
Iteration 73/1000 | Loss: 0.00002953
Iteration 74/1000 | Loss: 0.00003054
Iteration 75/1000 | Loss: 0.00003331
Iteration 76/1000 | Loss: 0.00003051
Iteration 77/1000 | Loss: 0.00002845
Iteration 78/1000 | Loss: 0.00002699
Iteration 79/1000 | Loss: 0.00002912
Iteration 80/1000 | Loss: 0.00016420
Iteration 81/1000 | Loss: 0.00003466
Iteration 82/1000 | Loss: 0.00003057
Iteration 83/1000 | Loss: 0.00002790
Iteration 84/1000 | Loss: 0.00003411
Iteration 85/1000 | Loss: 0.00002714
Iteration 86/1000 | Loss: 0.00003468
Iteration 87/1000 | Loss: 0.00002663
Iteration 88/1000 | Loss: 0.00002662
Iteration 89/1000 | Loss: 0.00002662
Iteration 90/1000 | Loss: 0.00002662
Iteration 91/1000 | Loss: 0.00002662
Iteration 92/1000 | Loss: 0.00002662
Iteration 93/1000 | Loss: 0.00002662
Iteration 94/1000 | Loss: 0.00002662
Iteration 95/1000 | Loss: 0.00002662
Iteration 96/1000 | Loss: 0.00002662
Iteration 97/1000 | Loss: 0.00002662
Iteration 98/1000 | Loss: 0.00002661
Iteration 99/1000 | Loss: 0.00002661
Iteration 100/1000 | Loss: 0.00002661
Iteration 101/1000 | Loss: 0.00002661
Iteration 102/1000 | Loss: 0.00002661
Iteration 103/1000 | Loss: 0.00002660
Iteration 104/1000 | Loss: 0.00002660
Iteration 105/1000 | Loss: 0.00002659
Iteration 106/1000 | Loss: 0.00002659
Iteration 107/1000 | Loss: 0.00002659
Iteration 108/1000 | Loss: 0.00002659
Iteration 109/1000 | Loss: 0.00002659
Iteration 110/1000 | Loss: 0.00002659
Iteration 111/1000 | Loss: 0.00002659
Iteration 112/1000 | Loss: 0.00002659
Iteration 113/1000 | Loss: 0.00002658
Iteration 114/1000 | Loss: 0.00002694
Iteration 115/1000 | Loss: 0.00005489
Iteration 116/1000 | Loss: 0.00002932
Iteration 117/1000 | Loss: 0.00002657
Iteration 118/1000 | Loss: 0.00002682
Iteration 119/1000 | Loss: 0.00003280
Iteration 120/1000 | Loss: 0.00002725
Iteration 121/1000 | Loss: 0.00002806
Iteration 122/1000 | Loss: 0.00002806
Iteration 123/1000 | Loss: 0.00002961
Iteration 124/1000 | Loss: 0.00003276
Iteration 125/1000 | Loss: 0.00002799
Iteration 126/1000 | Loss: 0.00002758
Iteration 127/1000 | Loss: 0.00002652
Iteration 128/1000 | Loss: 0.00002652
Iteration 129/1000 | Loss: 0.00002652
Iteration 130/1000 | Loss: 0.00002652
Iteration 131/1000 | Loss: 0.00002652
Iteration 132/1000 | Loss: 0.00002652
Iteration 133/1000 | Loss: 0.00002652
Iteration 134/1000 | Loss: 0.00002652
Iteration 135/1000 | Loss: 0.00002652
Iteration 136/1000 | Loss: 0.00002652
Iteration 137/1000 | Loss: 0.00002652
Iteration 138/1000 | Loss: 0.00002652
Iteration 139/1000 | Loss: 0.00002652
Iteration 140/1000 | Loss: 0.00002652
Iteration 141/1000 | Loss: 0.00002652
Iteration 142/1000 | Loss: 0.00002652
Iteration 143/1000 | Loss: 0.00002652
Iteration 144/1000 | Loss: 0.00002652
Iteration 145/1000 | Loss: 0.00002652
Iteration 146/1000 | Loss: 0.00002652
Iteration 147/1000 | Loss: 0.00002652
Iteration 148/1000 | Loss: 0.00002652
Iteration 149/1000 | Loss: 0.00002652
Iteration 150/1000 | Loss: 0.00002652
Iteration 151/1000 | Loss: 0.00002652
Iteration 152/1000 | Loss: 0.00002652
Iteration 153/1000 | Loss: 0.00002652
Iteration 154/1000 | Loss: 0.00002652
Iteration 155/1000 | Loss: 0.00002652
Iteration 156/1000 | Loss: 0.00002652
Iteration 157/1000 | Loss: 0.00002652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.6516057914705016e-05, 2.6516057914705016e-05, 2.6516057914705016e-05, 2.6516057914705016e-05, 2.6516057914705016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6516057914705016e-05

Optimization complete. Final v2v error: 4.267602443695068 mm

Highest mean error: 4.557374000549316 mm for frame 51

Lowest mean error: 4.013800621032715 mm for frame 208

Saving results

Total time: 164.07378339767456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787560
Iteration 2/25 | Loss: 0.00114776
Iteration 3/25 | Loss: 0.00083140
Iteration 4/25 | Loss: 0.00079890
Iteration 5/25 | Loss: 0.00078965
Iteration 6/25 | Loss: 0.00078828
Iteration 7/25 | Loss: 0.00078827
Iteration 8/25 | Loss: 0.00078827
Iteration 9/25 | Loss: 0.00078827
Iteration 10/25 | Loss: 0.00078827
Iteration 11/25 | Loss: 0.00078827
Iteration 12/25 | Loss: 0.00078827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007882658974267542, 0.0007882658974267542, 0.0007882658974267542, 0.0007882658974267542, 0.0007882658974267542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007882658974267542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27500105
Iteration 2/25 | Loss: 0.00025067
Iteration 3/25 | Loss: 0.00025063
Iteration 4/25 | Loss: 0.00025063
Iteration 5/25 | Loss: 0.00025063
Iteration 6/25 | Loss: 0.00025063
Iteration 7/25 | Loss: 0.00025063
Iteration 8/25 | Loss: 0.00025063
Iteration 9/25 | Loss: 0.00025063
Iteration 10/25 | Loss: 0.00025063
Iteration 11/25 | Loss: 0.00025063
Iteration 12/25 | Loss: 0.00025063
Iteration 13/25 | Loss: 0.00025063
Iteration 14/25 | Loss: 0.00025063
Iteration 15/25 | Loss: 0.00025063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00025063171051442623, 0.00025063171051442623, 0.00025063171051442623, 0.00025063171051442623, 0.00025063171051442623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025063171051442623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025063
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00002107
Iteration 4/1000 | Loss: 0.00001950
Iteration 5/1000 | Loss: 0.00001842
Iteration 6/1000 | Loss: 0.00001796
Iteration 7/1000 | Loss: 0.00001760
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001667
Iteration 17/1000 | Loss: 0.00001667
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001664
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001658
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001656
Iteration 30/1000 | Loss: 0.00001656
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001655
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001654
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001653
Iteration 40/1000 | Loss: 0.00001653
Iteration 41/1000 | Loss: 0.00001653
Iteration 42/1000 | Loss: 0.00001652
Iteration 43/1000 | Loss: 0.00001652
Iteration 44/1000 | Loss: 0.00001652
Iteration 45/1000 | Loss: 0.00001652
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001651
Iteration 50/1000 | Loss: 0.00001651
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001650
Iteration 53/1000 | Loss: 0.00001650
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001648
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001647
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001645
Iteration 68/1000 | Loss: 0.00001645
Iteration 69/1000 | Loss: 0.00001645
Iteration 70/1000 | Loss: 0.00001645
Iteration 71/1000 | Loss: 0.00001644
Iteration 72/1000 | Loss: 0.00001644
Iteration 73/1000 | Loss: 0.00001644
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001643
Iteration 76/1000 | Loss: 0.00001643
Iteration 77/1000 | Loss: 0.00001643
Iteration 78/1000 | Loss: 0.00001643
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001643
Iteration 81/1000 | Loss: 0.00001643
Iteration 82/1000 | Loss: 0.00001643
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001641
Iteration 103/1000 | Loss: 0.00001641
Iteration 104/1000 | Loss: 0.00001641
Iteration 105/1000 | Loss: 0.00001641
Iteration 106/1000 | Loss: 0.00001641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.640754271647893e-05, 1.640754271647893e-05, 1.640754271647893e-05, 1.640754271647893e-05, 1.640754271647893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.640754271647893e-05

Optimization complete. Final v2v error: 3.401304244995117 mm

Highest mean error: 4.594494819641113 mm for frame 221

Lowest mean error: 2.941718339920044 mm for frame 239

Saving results

Total time: 37.487897634506226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392811
Iteration 2/25 | Loss: 0.00079628
Iteration 3/25 | Loss: 0.00070775
Iteration 4/25 | Loss: 0.00069006
Iteration 5/25 | Loss: 0.00068523
Iteration 6/25 | Loss: 0.00068416
Iteration 7/25 | Loss: 0.00068405
Iteration 8/25 | Loss: 0.00068405
Iteration 9/25 | Loss: 0.00068405
Iteration 10/25 | Loss: 0.00068405
Iteration 11/25 | Loss: 0.00068405
Iteration 12/25 | Loss: 0.00068405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000684048980474472, 0.000684048980474472, 0.000684048980474472, 0.000684048980474472, 0.000684048980474472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000684048980474472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.04687548
Iteration 2/25 | Loss: 0.00021734
Iteration 3/25 | Loss: 0.00021734
Iteration 4/25 | Loss: 0.00021734
Iteration 5/25 | Loss: 0.00021734
Iteration 6/25 | Loss: 0.00021734
Iteration 7/25 | Loss: 0.00021734
Iteration 8/25 | Loss: 0.00021733
Iteration 9/25 | Loss: 0.00021733
Iteration 10/25 | Loss: 0.00021733
Iteration 11/25 | Loss: 0.00021733
Iteration 12/25 | Loss: 0.00021733
Iteration 13/25 | Loss: 0.00021733
Iteration 14/25 | Loss: 0.00021733
Iteration 15/25 | Loss: 0.00021733
Iteration 16/25 | Loss: 0.00021733
Iteration 17/25 | Loss: 0.00021733
Iteration 18/25 | Loss: 0.00021733
Iteration 19/25 | Loss: 0.00021733
Iteration 20/25 | Loss: 0.00021733
Iteration 21/25 | Loss: 0.00021733
Iteration 22/25 | Loss: 0.00021733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00021733407629653811, 0.00021733407629653811, 0.00021733407629653811, 0.00021733407629653811, 0.00021733407629653811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021733407629653811

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021733
Iteration 2/1000 | Loss: 0.00002091
Iteration 3/1000 | Loss: 0.00001710
Iteration 4/1000 | Loss: 0.00001596
Iteration 5/1000 | Loss: 0.00001500
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001422
Iteration 8/1000 | Loss: 0.00001402
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001366
Iteration 12/1000 | Loss: 0.00001365
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001363
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001352
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001340
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001339
Iteration 25/1000 | Loss: 0.00001338
Iteration 26/1000 | Loss: 0.00001338
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001329
Iteration 29/1000 | Loss: 0.00001328
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001320
Iteration 33/1000 | Loss: 0.00001319
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001318
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001312
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001311
Iteration 45/1000 | Loss: 0.00001311
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001310
Iteration 57/1000 | Loss: 0.00001309
Iteration 58/1000 | Loss: 0.00001309
Iteration 59/1000 | Loss: 0.00001309
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001309
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001308
Iteration 72/1000 | Loss: 0.00001308
Iteration 73/1000 | Loss: 0.00001308
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001307
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001305
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001304
Iteration 98/1000 | Loss: 0.00001304
Iteration 99/1000 | Loss: 0.00001304
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001303
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001303
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001303
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001303
Iteration 122/1000 | Loss: 0.00001303
Iteration 123/1000 | Loss: 0.00001303
Iteration 124/1000 | Loss: 0.00001303
Iteration 125/1000 | Loss: 0.00001303
Iteration 126/1000 | Loss: 0.00001303
Iteration 127/1000 | Loss: 0.00001303
Iteration 128/1000 | Loss: 0.00001303
Iteration 129/1000 | Loss: 0.00001303
Iteration 130/1000 | Loss: 0.00001303
Iteration 131/1000 | Loss: 0.00001303
Iteration 132/1000 | Loss: 0.00001303
Iteration 133/1000 | Loss: 0.00001303
Iteration 134/1000 | Loss: 0.00001303
Iteration 135/1000 | Loss: 0.00001303
Iteration 136/1000 | Loss: 0.00001303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.3031993148615584e-05, 1.3031993148615584e-05, 1.3031993148615584e-05, 1.3031993148615584e-05, 1.3031993148615584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3031993148615584e-05

Optimization complete. Final v2v error: 3.0881755352020264 mm

Highest mean error: 3.3210580348968506 mm for frame 139

Lowest mean error: 2.9725253582000732 mm for frame 127

Saving results

Total time: 40.43964099884033
