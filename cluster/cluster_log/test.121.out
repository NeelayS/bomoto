Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=121, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6776-6831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017311
Iteration 2/25 | Loss: 0.00215221
Iteration 3/25 | Loss: 0.00182091
Iteration 4/25 | Loss: 0.00175912
Iteration 5/25 | Loss: 0.00161112
Iteration 6/25 | Loss: 0.00157270
Iteration 7/25 | Loss: 0.00147663
Iteration 8/25 | Loss: 0.00142680
Iteration 9/25 | Loss: 0.00141384
Iteration 10/25 | Loss: 0.00140702
Iteration 11/25 | Loss: 0.00137685
Iteration 12/25 | Loss: 0.00136599
Iteration 13/25 | Loss: 0.00135624
Iteration 14/25 | Loss: 0.00135755
Iteration 15/25 | Loss: 0.00134451
Iteration 16/25 | Loss: 0.00133896
Iteration 17/25 | Loss: 0.00133516
Iteration 18/25 | Loss: 0.00133511
Iteration 19/25 | Loss: 0.00133603
Iteration 20/25 | Loss: 0.00133449
Iteration 21/25 | Loss: 0.00133620
Iteration 22/25 | Loss: 0.00133562
Iteration 23/25 | Loss: 0.00133601
Iteration 24/25 | Loss: 0.00133478
Iteration 25/25 | Loss: 0.00133637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61306262
Iteration 2/25 | Loss: 0.00105181
Iteration 3/25 | Loss: 0.00105181
Iteration 4/25 | Loss: 0.00105181
Iteration 5/25 | Loss: 0.00105181
Iteration 6/25 | Loss: 0.00105181
Iteration 7/25 | Loss: 0.00105181
Iteration 8/25 | Loss: 0.00105180
Iteration 9/25 | Loss: 0.00105180
Iteration 10/25 | Loss: 0.00105180
Iteration 11/25 | Loss: 0.00105180
Iteration 12/25 | Loss: 0.00105180
Iteration 13/25 | Loss: 0.00105180
Iteration 14/25 | Loss: 0.00105180
Iteration 15/25 | Loss: 0.00105180
Iteration 16/25 | Loss: 0.00105180
Iteration 17/25 | Loss: 0.00105180
Iteration 18/25 | Loss: 0.00105180
Iteration 19/25 | Loss: 0.00105180
Iteration 20/25 | Loss: 0.00105180
Iteration 21/25 | Loss: 0.00105180
Iteration 22/25 | Loss: 0.00105180
Iteration 23/25 | Loss: 0.00105180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010518035851418972, 0.0010518035851418972, 0.0010518035851418972, 0.0010518035851418972, 0.0010518035851418972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010518035851418972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105180
Iteration 2/1000 | Loss: 0.00007214
Iteration 3/1000 | Loss: 0.00023030
Iteration 4/1000 | Loss: 0.00005678
Iteration 5/1000 | Loss: 0.00004354
Iteration 6/1000 | Loss: 0.00003904
Iteration 7/1000 | Loss: 0.00003673
Iteration 8/1000 | Loss: 0.00331707
Iteration 9/1000 | Loss: 0.00496802
Iteration 10/1000 | Loss: 0.00306734
Iteration 11/1000 | Loss: 0.00256354
Iteration 12/1000 | Loss: 0.00172341
Iteration 13/1000 | Loss: 0.00115450
Iteration 14/1000 | Loss: 0.00282948
Iteration 15/1000 | Loss: 0.00104307
Iteration 16/1000 | Loss: 0.00108996
Iteration 17/1000 | Loss: 0.00126035
Iteration 18/1000 | Loss: 0.00019906
Iteration 19/1000 | Loss: 0.00006026
Iteration 20/1000 | Loss: 0.00029896
Iteration 21/1000 | Loss: 0.00190878
Iteration 22/1000 | Loss: 0.00005712
Iteration 23/1000 | Loss: 0.00004934
Iteration 24/1000 | Loss: 0.00107154
Iteration 25/1000 | Loss: 0.00451743
Iteration 26/1000 | Loss: 0.00128539
Iteration 27/1000 | Loss: 0.00038971
Iteration 28/1000 | Loss: 0.00128815
Iteration 29/1000 | Loss: 0.00072191
Iteration 30/1000 | Loss: 0.00128782
Iteration 31/1000 | Loss: 0.00053928
Iteration 32/1000 | Loss: 0.00167993
Iteration 33/1000 | Loss: 0.00078493
Iteration 34/1000 | Loss: 0.00090637
Iteration 35/1000 | Loss: 0.00069216
Iteration 36/1000 | Loss: 0.00144165
Iteration 37/1000 | Loss: 0.00133469
Iteration 38/1000 | Loss: 0.00171083
Iteration 39/1000 | Loss: 0.00109354
Iteration 40/1000 | Loss: 0.00073353
Iteration 41/1000 | Loss: 0.00009700
Iteration 42/1000 | Loss: 0.00004903
Iteration 43/1000 | Loss: 0.00004471
Iteration 44/1000 | Loss: 0.00004323
Iteration 45/1000 | Loss: 0.00004195
Iteration 46/1000 | Loss: 0.00108992
Iteration 47/1000 | Loss: 0.00136579
Iteration 48/1000 | Loss: 0.00076538
Iteration 49/1000 | Loss: 0.00073022
Iteration 50/1000 | Loss: 0.00092792
Iteration 51/1000 | Loss: 0.00028856
Iteration 52/1000 | Loss: 0.00146687
Iteration 53/1000 | Loss: 0.00198816
Iteration 54/1000 | Loss: 0.00023147
Iteration 55/1000 | Loss: 0.00174528
Iteration 56/1000 | Loss: 0.00012718
Iteration 57/1000 | Loss: 0.00009126
Iteration 58/1000 | Loss: 0.00133115
Iteration 59/1000 | Loss: 0.00007465
Iteration 60/1000 | Loss: 0.00005577
Iteration 61/1000 | Loss: 0.00017309
Iteration 62/1000 | Loss: 0.00057254
Iteration 63/1000 | Loss: 0.00174527
Iteration 64/1000 | Loss: 0.00016583
Iteration 65/1000 | Loss: 0.00010374
Iteration 66/1000 | Loss: 0.00015060
Iteration 67/1000 | Loss: 0.00004217
Iteration 68/1000 | Loss: 0.00063434
Iteration 69/1000 | Loss: 0.00003237
Iteration 70/1000 | Loss: 0.00002644
Iteration 71/1000 | Loss: 0.00002324
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00019306
Iteration 74/1000 | Loss: 0.00002966
Iteration 75/1000 | Loss: 0.00002191
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001759
Iteration 79/1000 | Loss: 0.00001686
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001578
Iteration 84/1000 | Loss: 0.00001575
Iteration 85/1000 | Loss: 0.00001575
Iteration 86/1000 | Loss: 0.00001575
Iteration 87/1000 | Loss: 0.00001573
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001559
Iteration 91/1000 | Loss: 0.00001558
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001556
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001550
Iteration 98/1000 | Loss: 0.00001550
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001542
Iteration 109/1000 | Loss: 0.00001542
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001541
Iteration 113/1000 | Loss: 0.00001541
Iteration 114/1000 | Loss: 0.00001541
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001538
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001538
Iteration 134/1000 | Loss: 0.00001538
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001537
Iteration 139/1000 | Loss: 0.00001537
Iteration 140/1000 | Loss: 0.00001537
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001537
Iteration 151/1000 | Loss: 0.00001537
Iteration 152/1000 | Loss: 0.00001537
Iteration 153/1000 | Loss: 0.00001537
Iteration 154/1000 | Loss: 0.00001537
Iteration 155/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.53723140101647e-05, 1.53723140101647e-05, 1.53723140101647e-05, 1.53723140101647e-05, 1.53723140101647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.53723140101647e-05

Optimization complete. Final v2v error: 3.3383703231811523 mm

Highest mean error: 4.089824199676514 mm for frame 122

Lowest mean error: 3.043734312057495 mm for frame 30

Saving results

Total time: 169.4336678981781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772736
Iteration 2/25 | Loss: 0.00137941
Iteration 3/25 | Loss: 0.00127953
Iteration 4/25 | Loss: 0.00126904
Iteration 5/25 | Loss: 0.00126688
Iteration 6/25 | Loss: 0.00126688
Iteration 7/25 | Loss: 0.00126688
Iteration 8/25 | Loss: 0.00126688
Iteration 9/25 | Loss: 0.00126688
Iteration 10/25 | Loss: 0.00126688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012668840354308486, 0.0012668840354308486, 0.0012668840354308486, 0.0012668840354308486, 0.0012668840354308486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012668840354308486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40206838
Iteration 2/25 | Loss: 0.00082615
Iteration 3/25 | Loss: 0.00082614
Iteration 4/25 | Loss: 0.00082614
Iteration 5/25 | Loss: 0.00082614
Iteration 6/25 | Loss: 0.00082614
Iteration 7/25 | Loss: 0.00082614
Iteration 8/25 | Loss: 0.00082614
Iteration 9/25 | Loss: 0.00082614
Iteration 10/25 | Loss: 0.00082614
Iteration 11/25 | Loss: 0.00082614
Iteration 12/25 | Loss: 0.00082614
Iteration 13/25 | Loss: 0.00082614
Iteration 14/25 | Loss: 0.00082614
Iteration 15/25 | Loss: 0.00082614
Iteration 16/25 | Loss: 0.00082614
Iteration 17/25 | Loss: 0.00082614
Iteration 18/25 | Loss: 0.00082614
Iteration 19/25 | Loss: 0.00082614
Iteration 20/25 | Loss: 0.00082614
Iteration 21/25 | Loss: 0.00082614
Iteration 22/25 | Loss: 0.00082614
Iteration 23/25 | Loss: 0.00082614
Iteration 24/25 | Loss: 0.00082614
Iteration 25/25 | Loss: 0.00082614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008261396433226764, 0.0008261396433226764, 0.0008261396433226764, 0.0008261396433226764, 0.0008261396433226764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008261396433226764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082614
Iteration 2/1000 | Loss: 0.00002694
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001539
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001387
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001316
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001256
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001254
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001242
Iteration 18/1000 | Loss: 0.00001242
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001233
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00001231
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001229
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001228
Iteration 33/1000 | Loss: 0.00001228
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001225
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001223
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001219
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001219
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001214
Iteration 78/1000 | Loss: 0.00001214
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001211
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001210
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001209
Iteration 95/1000 | Loss: 0.00001209
Iteration 96/1000 | Loss: 0.00001209
Iteration 97/1000 | Loss: 0.00001209
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001208
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001200
Iteration 140/1000 | Loss: 0.00001200
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001198
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001197
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001197
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001196
Iteration 176/1000 | Loss: 0.00001196
Iteration 177/1000 | Loss: 0.00001196
Iteration 178/1000 | Loss: 0.00001196
Iteration 179/1000 | Loss: 0.00001196
Iteration 180/1000 | Loss: 0.00001196
Iteration 181/1000 | Loss: 0.00001196
Iteration 182/1000 | Loss: 0.00001196
Iteration 183/1000 | Loss: 0.00001196
Iteration 184/1000 | Loss: 0.00001196
Iteration 185/1000 | Loss: 0.00001196
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001195
Iteration 195/1000 | Loss: 0.00001195
Iteration 196/1000 | Loss: 0.00001195
Iteration 197/1000 | Loss: 0.00001195
Iteration 198/1000 | Loss: 0.00001195
Iteration 199/1000 | Loss: 0.00001195
Iteration 200/1000 | Loss: 0.00001195
Iteration 201/1000 | Loss: 0.00001194
Iteration 202/1000 | Loss: 0.00001194
Iteration 203/1000 | Loss: 0.00001194
Iteration 204/1000 | Loss: 0.00001194
Iteration 205/1000 | Loss: 0.00001194
Iteration 206/1000 | Loss: 0.00001194
Iteration 207/1000 | Loss: 0.00001194
Iteration 208/1000 | Loss: 0.00001194
Iteration 209/1000 | Loss: 0.00001194
Iteration 210/1000 | Loss: 0.00001194
Iteration 211/1000 | Loss: 0.00001194
Iteration 212/1000 | Loss: 0.00001194
Iteration 213/1000 | Loss: 0.00001194
Iteration 214/1000 | Loss: 0.00001194
Iteration 215/1000 | Loss: 0.00001194
Iteration 216/1000 | Loss: 0.00001194
Iteration 217/1000 | Loss: 0.00001194
Iteration 218/1000 | Loss: 0.00001194
Iteration 219/1000 | Loss: 0.00001194
Iteration 220/1000 | Loss: 0.00001194
Iteration 221/1000 | Loss: 0.00001193
Iteration 222/1000 | Loss: 0.00001193
Iteration 223/1000 | Loss: 0.00001193
Iteration 224/1000 | Loss: 0.00001193
Iteration 225/1000 | Loss: 0.00001193
Iteration 226/1000 | Loss: 0.00001193
Iteration 227/1000 | Loss: 0.00001193
Iteration 228/1000 | Loss: 0.00001193
Iteration 229/1000 | Loss: 0.00001193
Iteration 230/1000 | Loss: 0.00001193
Iteration 231/1000 | Loss: 0.00001193
Iteration 232/1000 | Loss: 0.00001193
Iteration 233/1000 | Loss: 0.00001193
Iteration 234/1000 | Loss: 0.00001193
Iteration 235/1000 | Loss: 0.00001193
Iteration 236/1000 | Loss: 0.00001192
Iteration 237/1000 | Loss: 0.00001192
Iteration 238/1000 | Loss: 0.00001192
Iteration 239/1000 | Loss: 0.00001192
Iteration 240/1000 | Loss: 0.00001192
Iteration 241/1000 | Loss: 0.00001192
Iteration 242/1000 | Loss: 0.00001192
Iteration 243/1000 | Loss: 0.00001192
Iteration 244/1000 | Loss: 0.00001192
Iteration 245/1000 | Loss: 0.00001192
Iteration 246/1000 | Loss: 0.00001191
Iteration 247/1000 | Loss: 0.00001191
Iteration 248/1000 | Loss: 0.00001191
Iteration 249/1000 | Loss: 0.00001191
Iteration 250/1000 | Loss: 0.00001191
Iteration 251/1000 | Loss: 0.00001191
Iteration 252/1000 | Loss: 0.00001190
Iteration 253/1000 | Loss: 0.00001190
Iteration 254/1000 | Loss: 0.00001190
Iteration 255/1000 | Loss: 0.00001190
Iteration 256/1000 | Loss: 0.00001190
Iteration 257/1000 | Loss: 0.00001190
Iteration 258/1000 | Loss: 0.00001190
Iteration 259/1000 | Loss: 0.00001190
Iteration 260/1000 | Loss: 0.00001190
Iteration 261/1000 | Loss: 0.00001190
Iteration 262/1000 | Loss: 0.00001190
Iteration 263/1000 | Loss: 0.00001190
Iteration 264/1000 | Loss: 0.00001190
Iteration 265/1000 | Loss: 0.00001190
Iteration 266/1000 | Loss: 0.00001189
Iteration 267/1000 | Loss: 0.00001189
Iteration 268/1000 | Loss: 0.00001189
Iteration 269/1000 | Loss: 0.00001189
Iteration 270/1000 | Loss: 0.00001189
Iteration 271/1000 | Loss: 0.00001189
Iteration 272/1000 | Loss: 0.00001189
Iteration 273/1000 | Loss: 0.00001189
Iteration 274/1000 | Loss: 0.00001189
Iteration 275/1000 | Loss: 0.00001189
Iteration 276/1000 | Loss: 0.00001189
Iteration 277/1000 | Loss: 0.00001189
Iteration 278/1000 | Loss: 0.00001189
Iteration 279/1000 | Loss: 0.00001188
Iteration 280/1000 | Loss: 0.00001188
Iteration 281/1000 | Loss: 0.00001188
Iteration 282/1000 | Loss: 0.00001188
Iteration 283/1000 | Loss: 0.00001188
Iteration 284/1000 | Loss: 0.00001188
Iteration 285/1000 | Loss: 0.00001188
Iteration 286/1000 | Loss: 0.00001188
Iteration 287/1000 | Loss: 0.00001188
Iteration 288/1000 | Loss: 0.00001188
Iteration 289/1000 | Loss: 0.00001188
Iteration 290/1000 | Loss: 0.00001188
Iteration 291/1000 | Loss: 0.00001188
Iteration 292/1000 | Loss: 0.00001188
Iteration 293/1000 | Loss: 0.00001188
Iteration 294/1000 | Loss: 0.00001188
Iteration 295/1000 | Loss: 0.00001188
Iteration 296/1000 | Loss: 0.00001188
Iteration 297/1000 | Loss: 0.00001188
Iteration 298/1000 | Loss: 0.00001188
Iteration 299/1000 | Loss: 0.00001188
Iteration 300/1000 | Loss: 0.00001188
Iteration 301/1000 | Loss: 0.00001188
Iteration 302/1000 | Loss: 0.00001188
Iteration 303/1000 | Loss: 0.00001188
Iteration 304/1000 | Loss: 0.00001188
Iteration 305/1000 | Loss: 0.00001188
Iteration 306/1000 | Loss: 0.00001188
Iteration 307/1000 | Loss: 0.00001188
Iteration 308/1000 | Loss: 0.00001188
Iteration 309/1000 | Loss: 0.00001188
Iteration 310/1000 | Loss: 0.00001188
Iteration 311/1000 | Loss: 0.00001188
Iteration 312/1000 | Loss: 0.00001188
Iteration 313/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 313. Stopping optimization.
Last 5 losses: [1.1882982107636053e-05, 1.1882982107636053e-05, 1.1882982107636053e-05, 1.1882982107636053e-05, 1.1882982107636053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1882982107636053e-05

Optimization complete. Final v2v error: 2.9406533241271973 mm

Highest mean error: 3.0839345455169678 mm for frame 63

Lowest mean error: 2.8074774742126465 mm for frame 163

Saving results

Total time: 44.587430477142334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946741
Iteration 2/25 | Loss: 0.00260370
Iteration 3/25 | Loss: 0.00190181
Iteration 4/25 | Loss: 0.00177305
Iteration 5/25 | Loss: 0.00173017
Iteration 6/25 | Loss: 0.00166109
Iteration 7/25 | Loss: 0.00157932
Iteration 8/25 | Loss: 0.00153778
Iteration 9/25 | Loss: 0.00153226
Iteration 10/25 | Loss: 0.00153166
Iteration 11/25 | Loss: 0.00151087
Iteration 12/25 | Loss: 0.00149159
Iteration 13/25 | Loss: 0.00147229
Iteration 14/25 | Loss: 0.00147171
Iteration 15/25 | Loss: 0.00146655
Iteration 16/25 | Loss: 0.00144857
Iteration 17/25 | Loss: 0.00144208
Iteration 18/25 | Loss: 0.00143537
Iteration 19/25 | Loss: 0.00143087
Iteration 20/25 | Loss: 0.00142799
Iteration 21/25 | Loss: 0.00142871
Iteration 22/25 | Loss: 0.00142293
Iteration 23/25 | Loss: 0.00142634
Iteration 24/25 | Loss: 0.00140519
Iteration 25/25 | Loss: 0.00140374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42280805
Iteration 2/25 | Loss: 0.00178468
Iteration 3/25 | Loss: 0.00126865
Iteration 4/25 | Loss: 0.00126865
Iteration 5/25 | Loss: 0.00126865
Iteration 6/25 | Loss: 0.00126865
Iteration 7/25 | Loss: 0.00126865
Iteration 8/25 | Loss: 0.00126865
Iteration 9/25 | Loss: 0.00126865
Iteration 10/25 | Loss: 0.00126865
Iteration 11/25 | Loss: 0.00126865
Iteration 12/25 | Loss: 0.00126865
Iteration 13/25 | Loss: 0.00126865
Iteration 14/25 | Loss: 0.00126865
Iteration 15/25 | Loss: 0.00126865
Iteration 16/25 | Loss: 0.00126865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012686507543548942, 0.0012686507543548942, 0.0012686507543548942, 0.0012686507543548942, 0.0012686507543548942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012686507543548942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126865
Iteration 2/1000 | Loss: 0.00083581
Iteration 3/1000 | Loss: 0.00032917
Iteration 4/1000 | Loss: 0.00051572
Iteration 5/1000 | Loss: 0.00020094
Iteration 6/1000 | Loss: 0.00062342
Iteration 7/1000 | Loss: 0.00059003
Iteration 8/1000 | Loss: 0.00067010
Iteration 9/1000 | Loss: 0.00092458
Iteration 10/1000 | Loss: 0.00095010
Iteration 11/1000 | Loss: 0.00107870
Iteration 12/1000 | Loss: 0.00097873
Iteration 13/1000 | Loss: 0.00058340
Iteration 14/1000 | Loss: 0.00089557
Iteration 15/1000 | Loss: 0.00036923
Iteration 16/1000 | Loss: 0.00007555
Iteration 17/1000 | Loss: 0.00006486
Iteration 18/1000 | Loss: 0.00020977
Iteration 19/1000 | Loss: 0.00045241
Iteration 20/1000 | Loss: 0.00031702
Iteration 21/1000 | Loss: 0.00058314
Iteration 22/1000 | Loss: 0.00030365
Iteration 23/1000 | Loss: 0.00051780
Iteration 24/1000 | Loss: 0.00021711
Iteration 25/1000 | Loss: 0.00021987
Iteration 26/1000 | Loss: 0.00030989
Iteration 27/1000 | Loss: 0.00008351
Iteration 28/1000 | Loss: 0.00023540
Iteration 29/1000 | Loss: 0.00036090
Iteration 30/1000 | Loss: 0.00040002
Iteration 31/1000 | Loss: 0.00022506
Iteration 32/1000 | Loss: 0.00037246
Iteration 33/1000 | Loss: 0.00041013
Iteration 34/1000 | Loss: 0.00035551
Iteration 35/1000 | Loss: 0.00037803
Iteration 36/1000 | Loss: 0.00044103
Iteration 37/1000 | Loss: 0.00036311
Iteration 38/1000 | Loss: 0.00037757
Iteration 39/1000 | Loss: 0.00036853
Iteration 40/1000 | Loss: 0.00040522
Iteration 41/1000 | Loss: 0.00016359
Iteration 42/1000 | Loss: 0.00018183
Iteration 43/1000 | Loss: 0.00023755
Iteration 44/1000 | Loss: 0.00017008
Iteration 45/1000 | Loss: 0.00014775
Iteration 46/1000 | Loss: 0.00017093
Iteration 47/1000 | Loss: 0.00022581
Iteration 48/1000 | Loss: 0.00025438
Iteration 49/1000 | Loss: 0.00027023
Iteration 50/1000 | Loss: 0.00052328
Iteration 51/1000 | Loss: 0.00042798
Iteration 52/1000 | Loss: 0.00049215
Iteration 53/1000 | Loss: 0.00041949
Iteration 54/1000 | Loss: 0.00045030
Iteration 55/1000 | Loss: 0.00022917
Iteration 56/1000 | Loss: 0.00034798
Iteration 57/1000 | Loss: 0.00016607
Iteration 58/1000 | Loss: 0.00019043
Iteration 59/1000 | Loss: 0.00015783
Iteration 60/1000 | Loss: 0.00009338
Iteration 61/1000 | Loss: 0.00006817
Iteration 62/1000 | Loss: 0.00008079
Iteration 63/1000 | Loss: 0.00017469
Iteration 64/1000 | Loss: 0.00017881
Iteration 65/1000 | Loss: 0.00021540
Iteration 66/1000 | Loss: 0.00020908
Iteration 67/1000 | Loss: 0.00014787
Iteration 68/1000 | Loss: 0.00005156
Iteration 69/1000 | Loss: 0.00012911
Iteration 70/1000 | Loss: 0.00010672
Iteration 71/1000 | Loss: 0.00004987
Iteration 72/1000 | Loss: 0.00007136
Iteration 73/1000 | Loss: 0.00021983
Iteration 74/1000 | Loss: 0.00005568
Iteration 75/1000 | Loss: 0.00004827
Iteration 76/1000 | Loss: 0.00018744
Iteration 77/1000 | Loss: 0.00004841
Iteration 78/1000 | Loss: 0.00004563
Iteration 79/1000 | Loss: 0.00018522
Iteration 80/1000 | Loss: 0.00013855
Iteration 81/1000 | Loss: 0.00020386
Iteration 82/1000 | Loss: 0.00040636
Iteration 83/1000 | Loss: 0.00036234
Iteration 84/1000 | Loss: 0.00020189
Iteration 85/1000 | Loss: 0.00032076
Iteration 86/1000 | Loss: 0.00020584
Iteration 87/1000 | Loss: 0.00026650
Iteration 88/1000 | Loss: 0.00019636
Iteration 89/1000 | Loss: 0.00026073
Iteration 90/1000 | Loss: 0.00018994
Iteration 91/1000 | Loss: 0.00043964
Iteration 92/1000 | Loss: 0.00027114
Iteration 93/1000 | Loss: 0.00042036
Iteration 94/1000 | Loss: 0.00016740
Iteration 95/1000 | Loss: 0.00023528
Iteration 96/1000 | Loss: 0.00019557
Iteration 97/1000 | Loss: 0.00026059
Iteration 98/1000 | Loss: 0.00012998
Iteration 99/1000 | Loss: 0.00007678
Iteration 100/1000 | Loss: 0.00007730
Iteration 101/1000 | Loss: 0.00008837
Iteration 102/1000 | Loss: 0.00017135
Iteration 103/1000 | Loss: 0.00016413
Iteration 104/1000 | Loss: 0.00023846
Iteration 105/1000 | Loss: 0.00060944
Iteration 106/1000 | Loss: 0.00068455
Iteration 107/1000 | Loss: 0.00009676
Iteration 108/1000 | Loss: 0.00038108
Iteration 109/1000 | Loss: 0.00021656
Iteration 110/1000 | Loss: 0.00021552
Iteration 111/1000 | Loss: 0.00014878
Iteration 112/1000 | Loss: 0.00018735
Iteration 113/1000 | Loss: 0.00019651
Iteration 114/1000 | Loss: 0.00018299
Iteration 115/1000 | Loss: 0.00018534
Iteration 116/1000 | Loss: 0.00008389
Iteration 117/1000 | Loss: 0.00015136
Iteration 118/1000 | Loss: 0.00016495
Iteration 119/1000 | Loss: 0.00018128
Iteration 120/1000 | Loss: 0.00006392
Iteration 121/1000 | Loss: 0.00019304
Iteration 122/1000 | Loss: 0.00006033
Iteration 123/1000 | Loss: 0.00007656
Iteration 124/1000 | Loss: 0.00019386
Iteration 125/1000 | Loss: 0.00014608
Iteration 126/1000 | Loss: 0.00031740
Iteration 127/1000 | Loss: 0.00020416
Iteration 128/1000 | Loss: 0.00013949
Iteration 129/1000 | Loss: 0.00019409
Iteration 130/1000 | Loss: 0.00018282
Iteration 131/1000 | Loss: 0.00005292
Iteration 132/1000 | Loss: 0.00007141
Iteration 133/1000 | Loss: 0.00008915
Iteration 134/1000 | Loss: 0.00019694
Iteration 135/1000 | Loss: 0.00024733
Iteration 136/1000 | Loss: 0.00033841
Iteration 137/1000 | Loss: 0.00032562
Iteration 138/1000 | Loss: 0.00022366
Iteration 139/1000 | Loss: 0.00070931
Iteration 140/1000 | Loss: 0.00058594
Iteration 141/1000 | Loss: 0.00058693
Iteration 142/1000 | Loss: 0.00043719
Iteration 143/1000 | Loss: 0.00009247
Iteration 144/1000 | Loss: 0.00016252
Iteration 145/1000 | Loss: 0.00006353
Iteration 146/1000 | Loss: 0.00018770
Iteration 147/1000 | Loss: 0.00042522
Iteration 148/1000 | Loss: 0.00139329
Iteration 149/1000 | Loss: 0.00188999
Iteration 150/1000 | Loss: 0.00059338
Iteration 151/1000 | Loss: 0.00185724
Iteration 152/1000 | Loss: 0.00048950
Iteration 153/1000 | Loss: 0.00143652
Iteration 154/1000 | Loss: 0.00017402
Iteration 155/1000 | Loss: 0.00005663
Iteration 156/1000 | Loss: 0.00042775
Iteration 157/1000 | Loss: 0.00004549
Iteration 158/1000 | Loss: 0.00005599
Iteration 159/1000 | Loss: 0.00033572
Iteration 160/1000 | Loss: 0.00006510
Iteration 161/1000 | Loss: 0.00005037
Iteration 162/1000 | Loss: 0.00004346
Iteration 163/1000 | Loss: 0.00004113
Iteration 164/1000 | Loss: 0.00028028
Iteration 165/1000 | Loss: 0.00024230
Iteration 166/1000 | Loss: 0.00025306
Iteration 167/1000 | Loss: 0.00019358
Iteration 168/1000 | Loss: 0.00030068
Iteration 169/1000 | Loss: 0.00016875
Iteration 170/1000 | Loss: 0.00036168
Iteration 171/1000 | Loss: 0.00017132
Iteration 172/1000 | Loss: 0.00030212
Iteration 173/1000 | Loss: 0.00017796
Iteration 174/1000 | Loss: 0.00027307
Iteration 175/1000 | Loss: 0.00014631
Iteration 176/1000 | Loss: 0.00029741
Iteration 177/1000 | Loss: 0.00013956
Iteration 178/1000 | Loss: 0.00028848
Iteration 179/1000 | Loss: 0.00015830
Iteration 180/1000 | Loss: 0.00004129
Iteration 181/1000 | Loss: 0.00018784
Iteration 182/1000 | Loss: 0.00004206
Iteration 183/1000 | Loss: 0.00017551
Iteration 184/1000 | Loss: 0.00014934
Iteration 185/1000 | Loss: 0.00017023
Iteration 186/1000 | Loss: 0.00009475
Iteration 187/1000 | Loss: 0.00013895
Iteration 188/1000 | Loss: 0.00021355
Iteration 189/1000 | Loss: 0.00012921
Iteration 190/1000 | Loss: 0.00008654
Iteration 191/1000 | Loss: 0.00012871
Iteration 192/1000 | Loss: 0.00006588
Iteration 193/1000 | Loss: 0.00012521
Iteration 194/1000 | Loss: 0.00021173
Iteration 195/1000 | Loss: 0.00011860
Iteration 196/1000 | Loss: 0.00014312
Iteration 197/1000 | Loss: 0.00009359
Iteration 198/1000 | Loss: 0.00006673
Iteration 199/1000 | Loss: 0.00037184
Iteration 200/1000 | Loss: 0.00004357
Iteration 201/1000 | Loss: 0.00004127
Iteration 202/1000 | Loss: 0.00004010
Iteration 203/1000 | Loss: 0.00003893
Iteration 204/1000 | Loss: 0.00004212
Iteration 205/1000 | Loss: 0.00003678
Iteration 206/1000 | Loss: 0.00003636
Iteration 207/1000 | Loss: 0.00028087
Iteration 208/1000 | Loss: 0.00028085
Iteration 209/1000 | Loss: 0.00042315
Iteration 210/1000 | Loss: 0.00023289
Iteration 211/1000 | Loss: 0.00028421
Iteration 212/1000 | Loss: 0.00030436
Iteration 213/1000 | Loss: 0.00004526
Iteration 214/1000 | Loss: 0.00021903
Iteration 215/1000 | Loss: 0.00024022
Iteration 216/1000 | Loss: 0.00015706
Iteration 217/1000 | Loss: 0.00017439
Iteration 218/1000 | Loss: 0.00010958
Iteration 219/1000 | Loss: 0.00012679
Iteration 220/1000 | Loss: 0.00020933
Iteration 221/1000 | Loss: 0.00030871
Iteration 222/1000 | Loss: 0.00021248
Iteration 223/1000 | Loss: 0.00005048
Iteration 224/1000 | Loss: 0.00018747
Iteration 225/1000 | Loss: 0.00004214
Iteration 226/1000 | Loss: 0.00019655
Iteration 227/1000 | Loss: 0.00016931
Iteration 228/1000 | Loss: 0.00015717
Iteration 229/1000 | Loss: 0.00005893
Iteration 230/1000 | Loss: 0.00011830
Iteration 231/1000 | Loss: 0.00012952
Iteration 232/1000 | Loss: 0.00009788
Iteration 233/1000 | Loss: 0.00004350
Iteration 234/1000 | Loss: 0.00004127
Iteration 235/1000 | Loss: 0.00023982
Iteration 236/1000 | Loss: 0.00009435
Iteration 237/1000 | Loss: 0.00004930
Iteration 238/1000 | Loss: 0.00004235
Iteration 239/1000 | Loss: 0.00004093
Iteration 240/1000 | Loss: 0.00006477
Iteration 241/1000 | Loss: 0.00004246
Iteration 242/1000 | Loss: 0.00003965
Iteration 243/1000 | Loss: 0.00003929
Iteration 244/1000 | Loss: 0.00074249
Iteration 245/1000 | Loss: 0.00050012
Iteration 246/1000 | Loss: 0.00021267
Iteration 247/1000 | Loss: 0.00010905
Iteration 248/1000 | Loss: 0.00008274
Iteration 249/1000 | Loss: 0.00004497
Iteration 250/1000 | Loss: 0.00032891
Iteration 251/1000 | Loss: 0.00019096
Iteration 252/1000 | Loss: 0.00004817
Iteration 253/1000 | Loss: 0.00004082
Iteration 254/1000 | Loss: 0.00006024
Iteration 255/1000 | Loss: 0.00003793
Iteration 256/1000 | Loss: 0.00004497
Iteration 257/1000 | Loss: 0.00004122
Iteration 258/1000 | Loss: 0.00005052
Iteration 259/1000 | Loss: 0.00003603
Iteration 260/1000 | Loss: 0.00023704
Iteration 261/1000 | Loss: 0.00013250
Iteration 262/1000 | Loss: 0.00016861
Iteration 263/1000 | Loss: 0.00003614
Iteration 264/1000 | Loss: 0.00047252
Iteration 265/1000 | Loss: 0.00031804
Iteration 266/1000 | Loss: 0.00036657
Iteration 267/1000 | Loss: 0.00017968
Iteration 268/1000 | Loss: 0.00010104
Iteration 269/1000 | Loss: 0.00004623
Iteration 270/1000 | Loss: 0.00004720
Iteration 271/1000 | Loss: 0.00003989
Iteration 272/1000 | Loss: 0.00004381
Iteration 273/1000 | Loss: 0.00003719
Iteration 274/1000 | Loss: 0.00013813
Iteration 275/1000 | Loss: 0.00016631
Iteration 276/1000 | Loss: 0.00006590
Iteration 277/1000 | Loss: 0.00030695
Iteration 278/1000 | Loss: 0.00020482
Iteration 279/1000 | Loss: 0.00004165
Iteration 280/1000 | Loss: 0.00003730
Iteration 281/1000 | Loss: 0.00031579
Iteration 282/1000 | Loss: 0.00022992
Iteration 283/1000 | Loss: 0.00003716
Iteration 284/1000 | Loss: 0.00025817
Iteration 285/1000 | Loss: 0.00018463
Iteration 286/1000 | Loss: 0.00019033
Iteration 287/1000 | Loss: 0.00016527
Iteration 288/1000 | Loss: 0.00018418
Iteration 289/1000 | Loss: 0.00003929
Iteration 290/1000 | Loss: 0.00003730
Iteration 291/1000 | Loss: 0.00003673
Iteration 292/1000 | Loss: 0.00036682
Iteration 293/1000 | Loss: 0.00025094
Iteration 294/1000 | Loss: 0.00006891
Iteration 295/1000 | Loss: 0.00013920
Iteration 296/1000 | Loss: 0.00020044
Iteration 297/1000 | Loss: 0.00011641
Iteration 298/1000 | Loss: 0.00031869
Iteration 299/1000 | Loss: 0.00017649
Iteration 300/1000 | Loss: 0.00003657
Iteration 301/1000 | Loss: 0.00031873
Iteration 302/1000 | Loss: 0.00036672
Iteration 303/1000 | Loss: 0.00053734
Iteration 304/1000 | Loss: 0.00024149
Iteration 305/1000 | Loss: 0.00013941
Iteration 306/1000 | Loss: 0.00005751
Iteration 307/1000 | Loss: 0.00004251
Iteration 308/1000 | Loss: 0.00008062
Iteration 309/1000 | Loss: 0.00049033
Iteration 310/1000 | Loss: 0.00007182
Iteration 311/1000 | Loss: 0.00015433
Iteration 312/1000 | Loss: 0.00004387
Iteration 313/1000 | Loss: 0.00005154
Iteration 314/1000 | Loss: 0.00003717
Iteration 315/1000 | Loss: 0.00003602
Iteration 316/1000 | Loss: 0.00003531
Iteration 317/1000 | Loss: 0.00003472
Iteration 318/1000 | Loss: 0.00030132
Iteration 319/1000 | Loss: 0.00018206
Iteration 320/1000 | Loss: 0.00004126
Iteration 321/1000 | Loss: 0.00025621
Iteration 322/1000 | Loss: 0.00021740
Iteration 323/1000 | Loss: 0.00035654
Iteration 324/1000 | Loss: 0.00004042
Iteration 325/1000 | Loss: 0.00006538
Iteration 326/1000 | Loss: 0.00006381
Iteration 327/1000 | Loss: 0.00003978
Iteration 328/1000 | Loss: 0.00003419
Iteration 329/1000 | Loss: 0.00003378
Iteration 330/1000 | Loss: 0.00003338
Iteration 331/1000 | Loss: 0.00003309
Iteration 332/1000 | Loss: 0.00003293
Iteration 333/1000 | Loss: 0.00003292
Iteration 334/1000 | Loss: 0.00003287
Iteration 335/1000 | Loss: 0.00003279
Iteration 336/1000 | Loss: 0.00003277
Iteration 337/1000 | Loss: 0.00003272
Iteration 338/1000 | Loss: 0.00003271
Iteration 339/1000 | Loss: 0.00003264
Iteration 340/1000 | Loss: 0.00003262
Iteration 341/1000 | Loss: 0.00003261
Iteration 342/1000 | Loss: 0.00003260
Iteration 343/1000 | Loss: 0.00003260
Iteration 344/1000 | Loss: 0.00003258
Iteration 345/1000 | Loss: 0.00003258
Iteration 346/1000 | Loss: 0.00003258
Iteration 347/1000 | Loss: 0.00003258
Iteration 348/1000 | Loss: 0.00003258
Iteration 349/1000 | Loss: 0.00003258
Iteration 350/1000 | Loss: 0.00003258
Iteration 351/1000 | Loss: 0.00003258
Iteration 352/1000 | Loss: 0.00003258
Iteration 353/1000 | Loss: 0.00003257
Iteration 354/1000 | Loss: 0.00003254
Iteration 355/1000 | Loss: 0.00003254
Iteration 356/1000 | Loss: 0.00003245
Iteration 357/1000 | Loss: 0.00003240
Iteration 358/1000 | Loss: 0.00003239
Iteration 359/1000 | Loss: 0.00003239
Iteration 360/1000 | Loss: 0.00003238
Iteration 361/1000 | Loss: 0.00003238
Iteration 362/1000 | Loss: 0.00003237
Iteration 363/1000 | Loss: 0.00003237
Iteration 364/1000 | Loss: 0.00003236
Iteration 365/1000 | Loss: 0.00003236
Iteration 366/1000 | Loss: 0.00003236
Iteration 367/1000 | Loss: 0.00003236
Iteration 368/1000 | Loss: 0.00003236
Iteration 369/1000 | Loss: 0.00003236
Iteration 370/1000 | Loss: 0.00003236
Iteration 371/1000 | Loss: 0.00003235
Iteration 372/1000 | Loss: 0.00003235
Iteration 373/1000 | Loss: 0.00003235
Iteration 374/1000 | Loss: 0.00003235
Iteration 375/1000 | Loss: 0.00003235
Iteration 376/1000 | Loss: 0.00003235
Iteration 377/1000 | Loss: 0.00003235
Iteration 378/1000 | Loss: 0.00003234
Iteration 379/1000 | Loss: 0.00003234
Iteration 380/1000 | Loss: 0.00003234
Iteration 381/1000 | Loss: 0.00003234
Iteration 382/1000 | Loss: 0.00003234
Iteration 383/1000 | Loss: 0.00003234
Iteration 384/1000 | Loss: 0.00003234
Iteration 385/1000 | Loss: 0.00003234
Iteration 386/1000 | Loss: 0.00003234
Iteration 387/1000 | Loss: 0.00003234
Iteration 388/1000 | Loss: 0.00003233
Iteration 389/1000 | Loss: 0.00003233
Iteration 390/1000 | Loss: 0.00003233
Iteration 391/1000 | Loss: 0.00003233
Iteration 392/1000 | Loss: 0.00003233
Iteration 393/1000 | Loss: 0.00003233
Iteration 394/1000 | Loss: 0.00003233
Iteration 395/1000 | Loss: 0.00003233
Iteration 396/1000 | Loss: 0.00003233
Iteration 397/1000 | Loss: 0.00003233
Iteration 398/1000 | Loss: 0.00003233
Iteration 399/1000 | Loss: 0.00003233
Iteration 400/1000 | Loss: 0.00003233
Iteration 401/1000 | Loss: 0.00003232
Iteration 402/1000 | Loss: 0.00003232
Iteration 403/1000 | Loss: 0.00003232
Iteration 404/1000 | Loss: 0.00003232
Iteration 405/1000 | Loss: 0.00003232
Iteration 406/1000 | Loss: 0.00003232
Iteration 407/1000 | Loss: 0.00003232
Iteration 408/1000 | Loss: 0.00003232
Iteration 409/1000 | Loss: 0.00003232
Iteration 410/1000 | Loss: 0.00003232
Iteration 411/1000 | Loss: 0.00003232
Iteration 412/1000 | Loss: 0.00003232
Iteration 413/1000 | Loss: 0.00003231
Iteration 414/1000 | Loss: 0.00003231
Iteration 415/1000 | Loss: 0.00003231
Iteration 416/1000 | Loss: 0.00003231
Iteration 417/1000 | Loss: 0.00003231
Iteration 418/1000 | Loss: 0.00003231
Iteration 419/1000 | Loss: 0.00003231
Iteration 420/1000 | Loss: 0.00003231
Iteration 421/1000 | Loss: 0.00003231
Iteration 422/1000 | Loss: 0.00003231
Iteration 423/1000 | Loss: 0.00003231
Iteration 424/1000 | Loss: 0.00003231
Iteration 425/1000 | Loss: 0.00003231
Iteration 426/1000 | Loss: 0.00003230
Iteration 427/1000 | Loss: 0.00003230
Iteration 428/1000 | Loss: 0.00003230
Iteration 429/1000 | Loss: 0.00003230
Iteration 430/1000 | Loss: 0.00003230
Iteration 431/1000 | Loss: 0.00003230
Iteration 432/1000 | Loss: 0.00003230
Iteration 433/1000 | Loss: 0.00003230
Iteration 434/1000 | Loss: 0.00003230
Iteration 435/1000 | Loss: 0.00003230
Iteration 436/1000 | Loss: 0.00003230
Iteration 437/1000 | Loss: 0.00003230
Iteration 438/1000 | Loss: 0.00003230
Iteration 439/1000 | Loss: 0.00003230
Iteration 440/1000 | Loss: 0.00003230
Iteration 441/1000 | Loss: 0.00003230
Iteration 442/1000 | Loss: 0.00003230
Iteration 443/1000 | Loss: 0.00003230
Iteration 444/1000 | Loss: 0.00003230
Iteration 445/1000 | Loss: 0.00003229
Iteration 446/1000 | Loss: 0.00003229
Iteration 447/1000 | Loss: 0.00003229
Iteration 448/1000 | Loss: 0.00003229
Iteration 449/1000 | Loss: 0.00003229
Iteration 450/1000 | Loss: 0.00003229
Iteration 451/1000 | Loss: 0.00003229
Iteration 452/1000 | Loss: 0.00003229
Iteration 453/1000 | Loss: 0.00003229
Iteration 454/1000 | Loss: 0.00003229
Iteration 455/1000 | Loss: 0.00003229
Iteration 456/1000 | Loss: 0.00003229
Iteration 457/1000 | Loss: 0.00003229
Iteration 458/1000 | Loss: 0.00003229
Iteration 459/1000 | Loss: 0.00003229
Iteration 460/1000 | Loss: 0.00003229
Iteration 461/1000 | Loss: 0.00003228
Iteration 462/1000 | Loss: 0.00003228
Iteration 463/1000 | Loss: 0.00003228
Iteration 464/1000 | Loss: 0.00003228
Iteration 465/1000 | Loss: 0.00003228
Iteration 466/1000 | Loss: 0.00003228
Iteration 467/1000 | Loss: 0.00003228
Iteration 468/1000 | Loss: 0.00003228
Iteration 469/1000 | Loss: 0.00003228
Iteration 470/1000 | Loss: 0.00003228
Iteration 471/1000 | Loss: 0.00003228
Iteration 472/1000 | Loss: 0.00003228
Iteration 473/1000 | Loss: 0.00003228
Iteration 474/1000 | Loss: 0.00003228
Iteration 475/1000 | Loss: 0.00003228
Iteration 476/1000 | Loss: 0.00003228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 476. Stopping optimization.
Last 5 losses: [3.228106288588606e-05, 3.228106288588606e-05, 3.228106288588606e-05, 3.228106288588606e-05, 3.228106288588606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.228106288588606e-05

Optimization complete. Final v2v error: 4.574952602386475 mm

Highest mean error: 11.873401641845703 mm for frame 0

Lowest mean error: 3.581362724304199 mm for frame 164

Saving results

Total time: 546.4175124168396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818202
Iteration 2/25 | Loss: 0.00209311
Iteration 3/25 | Loss: 0.00177303
Iteration 4/25 | Loss: 0.00179125
Iteration 5/25 | Loss: 0.00210291
Iteration 6/25 | Loss: 0.00190107
Iteration 7/25 | Loss: 0.00156811
Iteration 8/25 | Loss: 0.00154558
Iteration 9/25 | Loss: 0.00154334
Iteration 10/25 | Loss: 0.00154327
Iteration 11/25 | Loss: 0.00154327
Iteration 12/25 | Loss: 0.00154327
Iteration 13/25 | Loss: 0.00154327
Iteration 14/25 | Loss: 0.00154327
Iteration 15/25 | Loss: 0.00154327
Iteration 16/25 | Loss: 0.00154327
Iteration 17/25 | Loss: 0.00154327
Iteration 18/25 | Loss: 0.00154327
Iteration 19/25 | Loss: 0.00154327
Iteration 20/25 | Loss: 0.00154327
Iteration 21/25 | Loss: 0.00154327
Iteration 22/25 | Loss: 0.00154327
Iteration 23/25 | Loss: 0.00154327
Iteration 24/25 | Loss: 0.00154327
Iteration 25/25 | Loss: 0.00154327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11706269
Iteration 2/25 | Loss: 0.00097220
Iteration 3/25 | Loss: 0.00097220
Iteration 4/25 | Loss: 0.00097220
Iteration 5/25 | Loss: 0.00097220
Iteration 6/25 | Loss: 0.00097220
Iteration 7/25 | Loss: 0.00097220
Iteration 8/25 | Loss: 0.00097220
Iteration 9/25 | Loss: 0.00097220
Iteration 10/25 | Loss: 0.00097219
Iteration 11/25 | Loss: 0.00097219
Iteration 12/25 | Loss: 0.00097219
Iteration 13/25 | Loss: 0.00097219
Iteration 14/25 | Loss: 0.00097219
Iteration 15/25 | Loss: 0.00097219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009721948299556971, 0.0009721948299556971, 0.0009721948299556971, 0.0009721948299556971, 0.0009721948299556971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009721948299556971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097219
Iteration 2/1000 | Loss: 0.00007076
Iteration 3/1000 | Loss: 0.00005620
Iteration 4/1000 | Loss: 0.00005305
Iteration 5/1000 | Loss: 0.00005141
Iteration 6/1000 | Loss: 0.00005031
Iteration 7/1000 | Loss: 0.00004961
Iteration 8/1000 | Loss: 0.00004906
Iteration 9/1000 | Loss: 0.00004864
Iteration 10/1000 | Loss: 0.00004832
Iteration 11/1000 | Loss: 0.00004806
Iteration 12/1000 | Loss: 0.00004794
Iteration 13/1000 | Loss: 0.00004770
Iteration 14/1000 | Loss: 0.00004760
Iteration 15/1000 | Loss: 0.00004755
Iteration 16/1000 | Loss: 0.00004753
Iteration 17/1000 | Loss: 0.00004753
Iteration 18/1000 | Loss: 0.00004752
Iteration 19/1000 | Loss: 0.00004751
Iteration 20/1000 | Loss: 0.00004751
Iteration 21/1000 | Loss: 0.00004750
Iteration 22/1000 | Loss: 0.00004749
Iteration 23/1000 | Loss: 0.00004748
Iteration 24/1000 | Loss: 0.00004748
Iteration 25/1000 | Loss: 0.00004748
Iteration 26/1000 | Loss: 0.00004748
Iteration 27/1000 | Loss: 0.00004747
Iteration 28/1000 | Loss: 0.00004747
Iteration 29/1000 | Loss: 0.00004747
Iteration 30/1000 | Loss: 0.00004747
Iteration 31/1000 | Loss: 0.00004747
Iteration 32/1000 | Loss: 0.00004747
Iteration 33/1000 | Loss: 0.00004747
Iteration 34/1000 | Loss: 0.00004747
Iteration 35/1000 | Loss: 0.00004747
Iteration 36/1000 | Loss: 0.00004747
Iteration 37/1000 | Loss: 0.00004747
Iteration 38/1000 | Loss: 0.00004747
Iteration 39/1000 | Loss: 0.00004747
Iteration 40/1000 | Loss: 0.00004746
Iteration 41/1000 | Loss: 0.00004746
Iteration 42/1000 | Loss: 0.00004746
Iteration 43/1000 | Loss: 0.00004746
Iteration 44/1000 | Loss: 0.00004746
Iteration 45/1000 | Loss: 0.00004746
Iteration 46/1000 | Loss: 0.00004746
Iteration 47/1000 | Loss: 0.00004746
Iteration 48/1000 | Loss: 0.00004746
Iteration 49/1000 | Loss: 0.00004746
Iteration 50/1000 | Loss: 0.00004744
Iteration 51/1000 | Loss: 0.00004744
Iteration 52/1000 | Loss: 0.00004743
Iteration 53/1000 | Loss: 0.00004743
Iteration 54/1000 | Loss: 0.00004743
Iteration 55/1000 | Loss: 0.00004743
Iteration 56/1000 | Loss: 0.00004742
Iteration 57/1000 | Loss: 0.00004742
Iteration 58/1000 | Loss: 0.00004740
Iteration 59/1000 | Loss: 0.00004739
Iteration 60/1000 | Loss: 0.00004738
Iteration 61/1000 | Loss: 0.00004738
Iteration 62/1000 | Loss: 0.00004737
Iteration 63/1000 | Loss: 0.00004737
Iteration 64/1000 | Loss: 0.00004737
Iteration 65/1000 | Loss: 0.00004737
Iteration 66/1000 | Loss: 0.00004736
Iteration 67/1000 | Loss: 0.00004736
Iteration 68/1000 | Loss: 0.00004736
Iteration 69/1000 | Loss: 0.00004736
Iteration 70/1000 | Loss: 0.00004736
Iteration 71/1000 | Loss: 0.00004736
Iteration 72/1000 | Loss: 0.00004736
Iteration 73/1000 | Loss: 0.00004736
Iteration 74/1000 | Loss: 0.00004736
Iteration 75/1000 | Loss: 0.00004736
Iteration 76/1000 | Loss: 0.00004735
Iteration 77/1000 | Loss: 0.00004735
Iteration 78/1000 | Loss: 0.00004735
Iteration 79/1000 | Loss: 0.00004735
Iteration 80/1000 | Loss: 0.00004735
Iteration 81/1000 | Loss: 0.00004734
Iteration 82/1000 | Loss: 0.00004734
Iteration 83/1000 | Loss: 0.00004733
Iteration 84/1000 | Loss: 0.00004733
Iteration 85/1000 | Loss: 0.00004732
Iteration 86/1000 | Loss: 0.00004732
Iteration 87/1000 | Loss: 0.00004731
Iteration 88/1000 | Loss: 0.00004731
Iteration 89/1000 | Loss: 0.00004731
Iteration 90/1000 | Loss: 0.00004731
Iteration 91/1000 | Loss: 0.00004731
Iteration 92/1000 | Loss: 0.00004730
Iteration 93/1000 | Loss: 0.00004729
Iteration 94/1000 | Loss: 0.00004729
Iteration 95/1000 | Loss: 0.00004729
Iteration 96/1000 | Loss: 0.00004729
Iteration 97/1000 | Loss: 0.00004729
Iteration 98/1000 | Loss: 0.00004729
Iteration 99/1000 | Loss: 0.00004729
Iteration 100/1000 | Loss: 0.00004729
Iteration 101/1000 | Loss: 0.00004729
Iteration 102/1000 | Loss: 0.00004728
Iteration 103/1000 | Loss: 0.00004728
Iteration 104/1000 | Loss: 0.00004728
Iteration 105/1000 | Loss: 0.00004728
Iteration 106/1000 | Loss: 0.00004728
Iteration 107/1000 | Loss: 0.00004728
Iteration 108/1000 | Loss: 0.00004728
Iteration 109/1000 | Loss: 0.00004727
Iteration 110/1000 | Loss: 0.00004726
Iteration 111/1000 | Loss: 0.00004726
Iteration 112/1000 | Loss: 0.00004725
Iteration 113/1000 | Loss: 0.00004725
Iteration 114/1000 | Loss: 0.00004725
Iteration 115/1000 | Loss: 0.00004725
Iteration 116/1000 | Loss: 0.00004724
Iteration 117/1000 | Loss: 0.00004724
Iteration 118/1000 | Loss: 0.00004723
Iteration 119/1000 | Loss: 0.00004723
Iteration 120/1000 | Loss: 0.00004722
Iteration 121/1000 | Loss: 0.00004722
Iteration 122/1000 | Loss: 0.00004721
Iteration 123/1000 | Loss: 0.00004721
Iteration 124/1000 | Loss: 0.00004721
Iteration 125/1000 | Loss: 0.00004720
Iteration 126/1000 | Loss: 0.00004720
Iteration 127/1000 | Loss: 0.00004719
Iteration 128/1000 | Loss: 0.00004719
Iteration 129/1000 | Loss: 0.00004719
Iteration 130/1000 | Loss: 0.00004718
Iteration 131/1000 | Loss: 0.00004718
Iteration 132/1000 | Loss: 0.00004718
Iteration 133/1000 | Loss: 0.00004718
Iteration 134/1000 | Loss: 0.00004718
Iteration 135/1000 | Loss: 0.00004718
Iteration 136/1000 | Loss: 0.00004718
Iteration 137/1000 | Loss: 0.00004718
Iteration 138/1000 | Loss: 0.00004718
Iteration 139/1000 | Loss: 0.00004717
Iteration 140/1000 | Loss: 0.00004717
Iteration 141/1000 | Loss: 0.00004717
Iteration 142/1000 | Loss: 0.00004717
Iteration 143/1000 | Loss: 0.00004717
Iteration 144/1000 | Loss: 0.00004717
Iteration 145/1000 | Loss: 0.00004717
Iteration 146/1000 | Loss: 0.00004717
Iteration 147/1000 | Loss: 0.00004717
Iteration 148/1000 | Loss: 0.00004716
Iteration 149/1000 | Loss: 0.00004716
Iteration 150/1000 | Loss: 0.00004716
Iteration 151/1000 | Loss: 0.00004715
Iteration 152/1000 | Loss: 0.00004715
Iteration 153/1000 | Loss: 0.00004715
Iteration 154/1000 | Loss: 0.00004714
Iteration 155/1000 | Loss: 0.00004714
Iteration 156/1000 | Loss: 0.00004714
Iteration 157/1000 | Loss: 0.00004714
Iteration 158/1000 | Loss: 0.00004713
Iteration 159/1000 | Loss: 0.00004713
Iteration 160/1000 | Loss: 0.00004713
Iteration 161/1000 | Loss: 0.00004713
Iteration 162/1000 | Loss: 0.00004713
Iteration 163/1000 | Loss: 0.00004713
Iteration 164/1000 | Loss: 0.00004712
Iteration 165/1000 | Loss: 0.00004712
Iteration 166/1000 | Loss: 0.00004712
Iteration 167/1000 | Loss: 0.00004712
Iteration 168/1000 | Loss: 0.00004712
Iteration 169/1000 | Loss: 0.00004712
Iteration 170/1000 | Loss: 0.00004711
Iteration 171/1000 | Loss: 0.00004711
Iteration 172/1000 | Loss: 0.00004711
Iteration 173/1000 | Loss: 0.00004711
Iteration 174/1000 | Loss: 0.00004711
Iteration 175/1000 | Loss: 0.00004711
Iteration 176/1000 | Loss: 0.00004711
Iteration 177/1000 | Loss: 0.00004711
Iteration 178/1000 | Loss: 0.00004711
Iteration 179/1000 | Loss: 0.00004711
Iteration 180/1000 | Loss: 0.00004710
Iteration 181/1000 | Loss: 0.00004710
Iteration 182/1000 | Loss: 0.00004710
Iteration 183/1000 | Loss: 0.00004710
Iteration 184/1000 | Loss: 0.00004710
Iteration 185/1000 | Loss: 0.00004710
Iteration 186/1000 | Loss: 0.00004710
Iteration 187/1000 | Loss: 0.00004710
Iteration 188/1000 | Loss: 0.00004710
Iteration 189/1000 | Loss: 0.00004710
Iteration 190/1000 | Loss: 0.00004710
Iteration 191/1000 | Loss: 0.00004710
Iteration 192/1000 | Loss: 0.00004710
Iteration 193/1000 | Loss: 0.00004710
Iteration 194/1000 | Loss: 0.00004710
Iteration 195/1000 | Loss: 0.00004710
Iteration 196/1000 | Loss: 0.00004710
Iteration 197/1000 | Loss: 0.00004710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [4.709745917352848e-05, 4.709745917352848e-05, 4.709745917352848e-05, 4.709745917352848e-05, 4.709745917352848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.709745917352848e-05

Optimization complete. Final v2v error: 5.496994972229004 mm

Highest mean error: 5.63169527053833 mm for frame 21

Lowest mean error: 5.429924488067627 mm for frame 100

Saving results

Total time: 48.6152229309082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811081
Iteration 2/25 | Loss: 0.00136313
Iteration 3/25 | Loss: 0.00126861
Iteration 4/25 | Loss: 0.00125671
Iteration 5/25 | Loss: 0.00125484
Iteration 6/25 | Loss: 0.00125484
Iteration 7/25 | Loss: 0.00125484
Iteration 8/25 | Loss: 0.00125484
Iteration 9/25 | Loss: 0.00125484
Iteration 10/25 | Loss: 0.00125484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012548408703878522, 0.0012548408703878522, 0.0012548408703878522, 0.0012548408703878522, 0.0012548408703878522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012548408703878522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39812326
Iteration 2/25 | Loss: 0.00078474
Iteration 3/25 | Loss: 0.00078474
Iteration 4/25 | Loss: 0.00078474
Iteration 5/25 | Loss: 0.00078474
Iteration 6/25 | Loss: 0.00078474
Iteration 7/25 | Loss: 0.00078474
Iteration 8/25 | Loss: 0.00078474
Iteration 9/25 | Loss: 0.00078474
Iteration 10/25 | Loss: 0.00078474
Iteration 11/25 | Loss: 0.00078474
Iteration 12/25 | Loss: 0.00078474
Iteration 13/25 | Loss: 0.00078473
Iteration 14/25 | Loss: 0.00078473
Iteration 15/25 | Loss: 0.00078473
Iteration 16/25 | Loss: 0.00078473
Iteration 17/25 | Loss: 0.00078473
Iteration 18/25 | Loss: 0.00078473
Iteration 19/25 | Loss: 0.00078473
Iteration 20/25 | Loss: 0.00078473
Iteration 21/25 | Loss: 0.00078473
Iteration 22/25 | Loss: 0.00078473
Iteration 23/25 | Loss: 0.00078473
Iteration 24/25 | Loss: 0.00078473
Iteration 25/25 | Loss: 0.00078473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078473
Iteration 2/1000 | Loss: 0.00002473
Iteration 3/1000 | Loss: 0.00001770
Iteration 4/1000 | Loss: 0.00001610
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001356
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001272
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001236
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001230
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001225
Iteration 34/1000 | Loss: 0.00001223
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001207
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001203
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001190
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001184
Iteration 79/1000 | Loss: 0.00001184
Iteration 80/1000 | Loss: 0.00001184
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001183
Iteration 83/1000 | Loss: 0.00001183
Iteration 84/1000 | Loss: 0.00001183
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001181
Iteration 90/1000 | Loss: 0.00001181
Iteration 91/1000 | Loss: 0.00001181
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001177
Iteration 104/1000 | Loss: 0.00001177
Iteration 105/1000 | Loss: 0.00001176
Iteration 106/1000 | Loss: 0.00001176
Iteration 107/1000 | Loss: 0.00001176
Iteration 108/1000 | Loss: 0.00001176
Iteration 109/1000 | Loss: 0.00001176
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001172
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001170
Iteration 123/1000 | Loss: 0.00001170
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001168
Iteration 136/1000 | Loss: 0.00001168
Iteration 137/1000 | Loss: 0.00001168
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001167
Iteration 141/1000 | Loss: 0.00001167
Iteration 142/1000 | Loss: 0.00001167
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001166
Iteration 146/1000 | Loss: 0.00001166
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001166
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001166
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001166
Iteration 166/1000 | Loss: 0.00001166
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001166
Iteration 170/1000 | Loss: 0.00001166
Iteration 171/1000 | Loss: 0.00001166
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001166
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Iteration 184/1000 | Loss: 0.00001166
Iteration 185/1000 | Loss: 0.00001166
Iteration 186/1000 | Loss: 0.00001166
Iteration 187/1000 | Loss: 0.00001166
Iteration 188/1000 | Loss: 0.00001166
Iteration 189/1000 | Loss: 0.00001166
Iteration 190/1000 | Loss: 0.00001166
Iteration 191/1000 | Loss: 0.00001166
Iteration 192/1000 | Loss: 0.00001166
Iteration 193/1000 | Loss: 0.00001166
Iteration 194/1000 | Loss: 0.00001166
Iteration 195/1000 | Loss: 0.00001166
Iteration 196/1000 | Loss: 0.00001166
Iteration 197/1000 | Loss: 0.00001166
Iteration 198/1000 | Loss: 0.00001166
Iteration 199/1000 | Loss: 0.00001166
Iteration 200/1000 | Loss: 0.00001166
Iteration 201/1000 | Loss: 0.00001166
Iteration 202/1000 | Loss: 0.00001166
Iteration 203/1000 | Loss: 0.00001166
Iteration 204/1000 | Loss: 0.00001166
Iteration 205/1000 | Loss: 0.00001166
Iteration 206/1000 | Loss: 0.00001166
Iteration 207/1000 | Loss: 0.00001166
Iteration 208/1000 | Loss: 0.00001166
Iteration 209/1000 | Loss: 0.00001166
Iteration 210/1000 | Loss: 0.00001166
Iteration 211/1000 | Loss: 0.00001166
Iteration 212/1000 | Loss: 0.00001166
Iteration 213/1000 | Loss: 0.00001166
Iteration 214/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.1657160939648747e-05, 1.1657160939648747e-05, 1.1657160939648747e-05, 1.1657160939648747e-05, 1.1657160939648747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1657160939648747e-05

Optimization complete. Final v2v error: 2.92183518409729 mm

Highest mean error: 3.1629786491394043 mm for frame 106

Lowest mean error: 2.750048875808716 mm for frame 19

Saving results

Total time: 46.64080548286438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041688
Iteration 2/25 | Loss: 0.01041688
Iteration 3/25 | Loss: 0.01041688
Iteration 4/25 | Loss: 0.01041687
Iteration 5/25 | Loss: 0.00523305
Iteration 6/25 | Loss: 0.00375977
Iteration 7/25 | Loss: 0.00292970
Iteration 8/25 | Loss: 0.00259633
Iteration 9/25 | Loss: 0.00236887
Iteration 10/25 | Loss: 0.00240987
Iteration 11/25 | Loss: 0.00221967
Iteration 12/25 | Loss: 0.00204514
Iteration 13/25 | Loss: 0.00201100
Iteration 14/25 | Loss: 0.00187767
Iteration 15/25 | Loss: 0.00178076
Iteration 16/25 | Loss: 0.00174484
Iteration 17/25 | Loss: 0.00171636
Iteration 18/25 | Loss: 0.00170274
Iteration 19/25 | Loss: 0.00170201
Iteration 20/25 | Loss: 0.00168279
Iteration 21/25 | Loss: 0.00166626
Iteration 22/25 | Loss: 0.00166910
Iteration 23/25 | Loss: 0.00167306
Iteration 24/25 | Loss: 0.00167124
Iteration 25/25 | Loss: 0.00166804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68690914
Iteration 2/25 | Loss: 0.00202334
Iteration 3/25 | Loss: 0.00202334
Iteration 4/25 | Loss: 0.00202334
Iteration 5/25 | Loss: 0.00202334
Iteration 6/25 | Loss: 0.00202334
Iteration 7/25 | Loss: 0.00202334
Iteration 8/25 | Loss: 0.00202334
Iteration 9/25 | Loss: 0.00202334
Iteration 10/25 | Loss: 0.00202334
Iteration 11/25 | Loss: 0.00202334
Iteration 12/25 | Loss: 0.00202334
Iteration 13/25 | Loss: 0.00202334
Iteration 14/25 | Loss: 0.00202334
Iteration 15/25 | Loss: 0.00202334
Iteration 16/25 | Loss: 0.00202334
Iteration 17/25 | Loss: 0.00202334
Iteration 18/25 | Loss: 0.00202334
Iteration 19/25 | Loss: 0.00202334
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020233357790857553, 0.0020233357790857553, 0.0020233357790857553, 0.0020233357790857553, 0.0020233357790857553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020233357790857553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202334
Iteration 2/1000 | Loss: 0.00114856
Iteration 3/1000 | Loss: 0.00129136
Iteration 4/1000 | Loss: 0.00127678
Iteration 5/1000 | Loss: 0.00050966
Iteration 6/1000 | Loss: 0.00028520
Iteration 7/1000 | Loss: 0.00026553
Iteration 8/1000 | Loss: 0.00127022
Iteration 9/1000 | Loss: 0.00025002
Iteration 10/1000 | Loss: 0.00047754
Iteration 11/1000 | Loss: 0.00074794
Iteration 12/1000 | Loss: 0.00056073
Iteration 13/1000 | Loss: 0.00018336
Iteration 14/1000 | Loss: 0.00018723
Iteration 15/1000 | Loss: 0.00017161
Iteration 16/1000 | Loss: 0.00016417
Iteration 17/1000 | Loss: 0.00015010
Iteration 18/1000 | Loss: 0.00017172
Iteration 19/1000 | Loss: 0.00015230
Iteration 20/1000 | Loss: 0.00016953
Iteration 21/1000 | Loss: 0.00012966
Iteration 22/1000 | Loss: 0.00011186
Iteration 23/1000 | Loss: 0.00019550
Iteration 24/1000 | Loss: 0.00012940
Iteration 25/1000 | Loss: 0.00017797
Iteration 26/1000 | Loss: 0.00013344
Iteration 27/1000 | Loss: 0.00019326
Iteration 28/1000 | Loss: 0.00016215
Iteration 29/1000 | Loss: 0.00010899
Iteration 30/1000 | Loss: 0.00012984
Iteration 31/1000 | Loss: 0.00028354
Iteration 32/1000 | Loss: 0.00051355
Iteration 33/1000 | Loss: 0.00131333
Iteration 34/1000 | Loss: 0.00099085
Iteration 35/1000 | Loss: 0.00023284
Iteration 36/1000 | Loss: 0.00013666
Iteration 37/1000 | Loss: 0.00008950
Iteration 38/1000 | Loss: 0.00022661
Iteration 39/1000 | Loss: 0.00009522
Iteration 40/1000 | Loss: 0.00010054
Iteration 41/1000 | Loss: 0.00008081
Iteration 42/1000 | Loss: 0.00005752
Iteration 43/1000 | Loss: 0.00010012
Iteration 44/1000 | Loss: 0.00011142
Iteration 45/1000 | Loss: 0.00009016
Iteration 46/1000 | Loss: 0.00016470
Iteration 47/1000 | Loss: 0.00026334
Iteration 48/1000 | Loss: 0.00008768
Iteration 49/1000 | Loss: 0.00008345
Iteration 50/1000 | Loss: 0.00008292
Iteration 51/1000 | Loss: 0.00012843
Iteration 52/1000 | Loss: 0.00008523
Iteration 53/1000 | Loss: 0.00027421
Iteration 54/1000 | Loss: 0.00137838
Iteration 55/1000 | Loss: 0.00026011
Iteration 56/1000 | Loss: 0.00011965
Iteration 57/1000 | Loss: 0.00029065
Iteration 58/1000 | Loss: 0.00008769
Iteration 59/1000 | Loss: 0.00011966
Iteration 60/1000 | Loss: 0.00010797
Iteration 61/1000 | Loss: 0.00032936
Iteration 62/1000 | Loss: 0.00011721
Iteration 63/1000 | Loss: 0.00011003
Iteration 64/1000 | Loss: 0.00009553
Iteration 65/1000 | Loss: 0.00015991
Iteration 66/1000 | Loss: 0.00035753
Iteration 67/1000 | Loss: 0.00024027
Iteration 68/1000 | Loss: 0.00008113
Iteration 69/1000 | Loss: 0.00009090
Iteration 70/1000 | Loss: 0.00013684
Iteration 71/1000 | Loss: 0.00010021
Iteration 72/1000 | Loss: 0.00128612
Iteration 73/1000 | Loss: 0.00053298
Iteration 74/1000 | Loss: 0.00030793
Iteration 75/1000 | Loss: 0.00008355
Iteration 76/1000 | Loss: 0.00004794
Iteration 77/1000 | Loss: 0.00011096
Iteration 78/1000 | Loss: 0.00005115
Iteration 79/1000 | Loss: 0.00004162
Iteration 80/1000 | Loss: 0.00003543
Iteration 81/1000 | Loss: 0.00003292
Iteration 82/1000 | Loss: 0.00003203
Iteration 83/1000 | Loss: 0.00003094
Iteration 84/1000 | Loss: 0.00003038
Iteration 85/1000 | Loss: 0.00002955
Iteration 86/1000 | Loss: 0.00002904
Iteration 87/1000 | Loss: 0.00002876
Iteration 88/1000 | Loss: 0.00013241
Iteration 89/1000 | Loss: 0.00003388
Iteration 90/1000 | Loss: 0.00003065
Iteration 91/1000 | Loss: 0.00002932
Iteration 92/1000 | Loss: 0.00028076
Iteration 93/1000 | Loss: 0.00005962
Iteration 94/1000 | Loss: 0.00003676
Iteration 95/1000 | Loss: 0.00003161
Iteration 96/1000 | Loss: 0.00002974
Iteration 97/1000 | Loss: 0.00002856
Iteration 98/1000 | Loss: 0.00002774
Iteration 99/1000 | Loss: 0.00002734
Iteration 100/1000 | Loss: 0.00023143
Iteration 101/1000 | Loss: 0.00011378
Iteration 102/1000 | Loss: 0.00022252
Iteration 103/1000 | Loss: 0.00003891
Iteration 104/1000 | Loss: 0.00003156
Iteration 105/1000 | Loss: 0.00002756
Iteration 106/1000 | Loss: 0.00002649
Iteration 107/1000 | Loss: 0.00002596
Iteration 108/1000 | Loss: 0.00002569
Iteration 109/1000 | Loss: 0.00002539
Iteration 110/1000 | Loss: 0.00002535
Iteration 111/1000 | Loss: 0.00002518
Iteration 112/1000 | Loss: 0.00002515
Iteration 113/1000 | Loss: 0.00002512
Iteration 114/1000 | Loss: 0.00002509
Iteration 115/1000 | Loss: 0.00002509
Iteration 116/1000 | Loss: 0.00002508
Iteration 117/1000 | Loss: 0.00002507
Iteration 118/1000 | Loss: 0.00002507
Iteration 119/1000 | Loss: 0.00002507
Iteration 120/1000 | Loss: 0.00002506
Iteration 121/1000 | Loss: 0.00002506
Iteration 122/1000 | Loss: 0.00002506
Iteration 123/1000 | Loss: 0.00002505
Iteration 124/1000 | Loss: 0.00002504
Iteration 125/1000 | Loss: 0.00002504
Iteration 126/1000 | Loss: 0.00002504
Iteration 127/1000 | Loss: 0.00002504
Iteration 128/1000 | Loss: 0.00002503
Iteration 129/1000 | Loss: 0.00002503
Iteration 130/1000 | Loss: 0.00002502
Iteration 131/1000 | Loss: 0.00002502
Iteration 132/1000 | Loss: 0.00002500
Iteration 133/1000 | Loss: 0.00002500
Iteration 134/1000 | Loss: 0.00002500
Iteration 135/1000 | Loss: 0.00002500
Iteration 136/1000 | Loss: 0.00002500
Iteration 137/1000 | Loss: 0.00002499
Iteration 138/1000 | Loss: 0.00002499
Iteration 139/1000 | Loss: 0.00002499
Iteration 140/1000 | Loss: 0.00002499
Iteration 141/1000 | Loss: 0.00002499
Iteration 142/1000 | Loss: 0.00002499
Iteration 143/1000 | Loss: 0.00002499
Iteration 144/1000 | Loss: 0.00002499
Iteration 145/1000 | Loss: 0.00002499
Iteration 146/1000 | Loss: 0.00002498
Iteration 147/1000 | Loss: 0.00002498
Iteration 148/1000 | Loss: 0.00002498
Iteration 149/1000 | Loss: 0.00002498
Iteration 150/1000 | Loss: 0.00002498
Iteration 151/1000 | Loss: 0.00002498
Iteration 152/1000 | Loss: 0.00002498
Iteration 153/1000 | Loss: 0.00002498
Iteration 154/1000 | Loss: 0.00002498
Iteration 155/1000 | Loss: 0.00002497
Iteration 156/1000 | Loss: 0.00002497
Iteration 157/1000 | Loss: 0.00002497
Iteration 158/1000 | Loss: 0.00002497
Iteration 159/1000 | Loss: 0.00002497
Iteration 160/1000 | Loss: 0.00002497
Iteration 161/1000 | Loss: 0.00002497
Iteration 162/1000 | Loss: 0.00002497
Iteration 163/1000 | Loss: 0.00002497
Iteration 164/1000 | Loss: 0.00002497
Iteration 165/1000 | Loss: 0.00002496
Iteration 166/1000 | Loss: 0.00002496
Iteration 167/1000 | Loss: 0.00002496
Iteration 168/1000 | Loss: 0.00002496
Iteration 169/1000 | Loss: 0.00002495
Iteration 170/1000 | Loss: 0.00002494
Iteration 171/1000 | Loss: 0.00002493
Iteration 172/1000 | Loss: 0.00002493
Iteration 173/1000 | Loss: 0.00002493
Iteration 174/1000 | Loss: 0.00002492
Iteration 175/1000 | Loss: 0.00002492
Iteration 176/1000 | Loss: 0.00002492
Iteration 177/1000 | Loss: 0.00002492
Iteration 178/1000 | Loss: 0.00002492
Iteration 179/1000 | Loss: 0.00002492
Iteration 180/1000 | Loss: 0.00002492
Iteration 181/1000 | Loss: 0.00002492
Iteration 182/1000 | Loss: 0.00002492
Iteration 183/1000 | Loss: 0.00002492
Iteration 184/1000 | Loss: 0.00002492
Iteration 185/1000 | Loss: 0.00002492
Iteration 186/1000 | Loss: 0.00002491
Iteration 187/1000 | Loss: 0.00002491
Iteration 188/1000 | Loss: 0.00002491
Iteration 189/1000 | Loss: 0.00002491
Iteration 190/1000 | Loss: 0.00002489
Iteration 191/1000 | Loss: 0.00002489
Iteration 192/1000 | Loss: 0.00002489
Iteration 193/1000 | Loss: 0.00002488
Iteration 194/1000 | Loss: 0.00002488
Iteration 195/1000 | Loss: 0.00002488
Iteration 196/1000 | Loss: 0.00002488
Iteration 197/1000 | Loss: 0.00002487
Iteration 198/1000 | Loss: 0.00002487
Iteration 199/1000 | Loss: 0.00002487
Iteration 200/1000 | Loss: 0.00002487
Iteration 201/1000 | Loss: 0.00002487
Iteration 202/1000 | Loss: 0.00002486
Iteration 203/1000 | Loss: 0.00002486
Iteration 204/1000 | Loss: 0.00002486
Iteration 205/1000 | Loss: 0.00002486
Iteration 206/1000 | Loss: 0.00002486
Iteration 207/1000 | Loss: 0.00002486
Iteration 208/1000 | Loss: 0.00002486
Iteration 209/1000 | Loss: 0.00002486
Iteration 210/1000 | Loss: 0.00002486
Iteration 211/1000 | Loss: 0.00002486
Iteration 212/1000 | Loss: 0.00002486
Iteration 213/1000 | Loss: 0.00002486
Iteration 214/1000 | Loss: 0.00002486
Iteration 215/1000 | Loss: 0.00002486
Iteration 216/1000 | Loss: 0.00002485
Iteration 217/1000 | Loss: 0.00002485
Iteration 218/1000 | Loss: 0.00002485
Iteration 219/1000 | Loss: 0.00002485
Iteration 220/1000 | Loss: 0.00002485
Iteration 221/1000 | Loss: 0.00002485
Iteration 222/1000 | Loss: 0.00002485
Iteration 223/1000 | Loss: 0.00002485
Iteration 224/1000 | Loss: 0.00002485
Iteration 225/1000 | Loss: 0.00002484
Iteration 226/1000 | Loss: 0.00002484
Iteration 227/1000 | Loss: 0.00002484
Iteration 228/1000 | Loss: 0.00002484
Iteration 229/1000 | Loss: 0.00002484
Iteration 230/1000 | Loss: 0.00002484
Iteration 231/1000 | Loss: 0.00002484
Iteration 232/1000 | Loss: 0.00002484
Iteration 233/1000 | Loss: 0.00002484
Iteration 234/1000 | Loss: 0.00002483
Iteration 235/1000 | Loss: 0.00002483
Iteration 236/1000 | Loss: 0.00002483
Iteration 237/1000 | Loss: 0.00002483
Iteration 238/1000 | Loss: 0.00002483
Iteration 239/1000 | Loss: 0.00002483
Iteration 240/1000 | Loss: 0.00002483
Iteration 241/1000 | Loss: 0.00002483
Iteration 242/1000 | Loss: 0.00002483
Iteration 243/1000 | Loss: 0.00002483
Iteration 244/1000 | Loss: 0.00002483
Iteration 245/1000 | Loss: 0.00002483
Iteration 246/1000 | Loss: 0.00002483
Iteration 247/1000 | Loss: 0.00002483
Iteration 248/1000 | Loss: 0.00002483
Iteration 249/1000 | Loss: 0.00002483
Iteration 250/1000 | Loss: 0.00002483
Iteration 251/1000 | Loss: 0.00002483
Iteration 252/1000 | Loss: 0.00002483
Iteration 253/1000 | Loss: 0.00002483
Iteration 254/1000 | Loss: 0.00002483
Iteration 255/1000 | Loss: 0.00002483
Iteration 256/1000 | Loss: 0.00002482
Iteration 257/1000 | Loss: 0.00002482
Iteration 258/1000 | Loss: 0.00002482
Iteration 259/1000 | Loss: 0.00002482
Iteration 260/1000 | Loss: 0.00002482
Iteration 261/1000 | Loss: 0.00002482
Iteration 262/1000 | Loss: 0.00002482
Iteration 263/1000 | Loss: 0.00002482
Iteration 264/1000 | Loss: 0.00002482
Iteration 265/1000 | Loss: 0.00002482
Iteration 266/1000 | Loss: 0.00002482
Iteration 267/1000 | Loss: 0.00002482
Iteration 268/1000 | Loss: 0.00002482
Iteration 269/1000 | Loss: 0.00002482
Iteration 270/1000 | Loss: 0.00002482
Iteration 271/1000 | Loss: 0.00002482
Iteration 272/1000 | Loss: 0.00002482
Iteration 273/1000 | Loss: 0.00002482
Iteration 274/1000 | Loss: 0.00002482
Iteration 275/1000 | Loss: 0.00002482
Iteration 276/1000 | Loss: 0.00002482
Iteration 277/1000 | Loss: 0.00002482
Iteration 278/1000 | Loss: 0.00002482
Iteration 279/1000 | Loss: 0.00002482
Iteration 280/1000 | Loss: 0.00002482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [2.482244417478796e-05, 2.482244417478796e-05, 2.482244417478796e-05, 2.482244417478796e-05, 2.482244417478796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.482244417478796e-05

Optimization complete. Final v2v error: 4.072788238525391 mm

Highest mean error: 6.563901424407959 mm for frame 59

Lowest mean error: 3.8335201740264893 mm for frame 99

Saving results

Total time: 236.87240171432495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785199
Iteration 2/25 | Loss: 0.00138228
Iteration 3/25 | Loss: 0.00126863
Iteration 4/25 | Loss: 0.00126211
Iteration 5/25 | Loss: 0.00126039
Iteration 6/25 | Loss: 0.00126039
Iteration 7/25 | Loss: 0.00126039
Iteration 8/25 | Loss: 0.00126039
Iteration 9/25 | Loss: 0.00126039
Iteration 10/25 | Loss: 0.00126039
Iteration 11/25 | Loss: 0.00126039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012603853829205036, 0.0012603853829205036, 0.0012603853829205036, 0.0012603853829205036, 0.0012603853829205036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012603853829205036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40221536
Iteration 2/25 | Loss: 0.00082429
Iteration 3/25 | Loss: 0.00082429
Iteration 4/25 | Loss: 0.00082429
Iteration 5/25 | Loss: 0.00082429
Iteration 6/25 | Loss: 0.00082429
Iteration 7/25 | Loss: 0.00082429
Iteration 8/25 | Loss: 0.00082429
Iteration 9/25 | Loss: 0.00082429
Iteration 10/25 | Loss: 0.00082429
Iteration 11/25 | Loss: 0.00082429
Iteration 12/25 | Loss: 0.00082429
Iteration 13/25 | Loss: 0.00082429
Iteration 14/25 | Loss: 0.00082429
Iteration 15/25 | Loss: 0.00082429
Iteration 16/25 | Loss: 0.00082429
Iteration 17/25 | Loss: 0.00082429
Iteration 18/25 | Loss: 0.00082429
Iteration 19/25 | Loss: 0.00082429
Iteration 20/25 | Loss: 0.00082429
Iteration 21/25 | Loss: 0.00082429
Iteration 22/25 | Loss: 0.00082429
Iteration 23/25 | Loss: 0.00082429
Iteration 24/25 | Loss: 0.00082429
Iteration 25/25 | Loss: 0.00082429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082429
Iteration 2/1000 | Loss: 0.00002617
Iteration 3/1000 | Loss: 0.00001757
Iteration 4/1000 | Loss: 0.00001550
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001323
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001251
Iteration 11/1000 | Loss: 0.00001242
Iteration 12/1000 | Loss: 0.00001237
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001229
Iteration 15/1000 | Loss: 0.00001229
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001224
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001208
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001206
Iteration 34/1000 | Loss: 0.00001206
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001205
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001204
Iteration 39/1000 | Loss: 0.00001204
Iteration 40/1000 | Loss: 0.00001204
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001195
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001194
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001190
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001187
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001186
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001185
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001184
Iteration 91/1000 | Loss: 0.00001184
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001177
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001176
Iteration 127/1000 | Loss: 0.00001176
Iteration 128/1000 | Loss: 0.00001176
Iteration 129/1000 | Loss: 0.00001176
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001175
Iteration 132/1000 | Loss: 0.00001175
Iteration 133/1000 | Loss: 0.00001175
Iteration 134/1000 | Loss: 0.00001175
Iteration 135/1000 | Loss: 0.00001175
Iteration 136/1000 | Loss: 0.00001175
Iteration 137/1000 | Loss: 0.00001175
Iteration 138/1000 | Loss: 0.00001175
Iteration 139/1000 | Loss: 0.00001175
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001174
Iteration 142/1000 | Loss: 0.00001174
Iteration 143/1000 | Loss: 0.00001174
Iteration 144/1000 | Loss: 0.00001174
Iteration 145/1000 | Loss: 0.00001174
Iteration 146/1000 | Loss: 0.00001174
Iteration 147/1000 | Loss: 0.00001174
Iteration 148/1000 | Loss: 0.00001174
Iteration 149/1000 | Loss: 0.00001174
Iteration 150/1000 | Loss: 0.00001174
Iteration 151/1000 | Loss: 0.00001174
Iteration 152/1000 | Loss: 0.00001174
Iteration 153/1000 | Loss: 0.00001174
Iteration 154/1000 | Loss: 0.00001174
Iteration 155/1000 | Loss: 0.00001174
Iteration 156/1000 | Loss: 0.00001173
Iteration 157/1000 | Loss: 0.00001173
Iteration 158/1000 | Loss: 0.00001173
Iteration 159/1000 | Loss: 0.00001173
Iteration 160/1000 | Loss: 0.00001173
Iteration 161/1000 | Loss: 0.00001173
Iteration 162/1000 | Loss: 0.00001173
Iteration 163/1000 | Loss: 0.00001172
Iteration 164/1000 | Loss: 0.00001172
Iteration 165/1000 | Loss: 0.00001172
Iteration 166/1000 | Loss: 0.00001172
Iteration 167/1000 | Loss: 0.00001172
Iteration 168/1000 | Loss: 0.00001172
Iteration 169/1000 | Loss: 0.00001172
Iteration 170/1000 | Loss: 0.00001172
Iteration 171/1000 | Loss: 0.00001172
Iteration 172/1000 | Loss: 0.00001172
Iteration 173/1000 | Loss: 0.00001172
Iteration 174/1000 | Loss: 0.00001172
Iteration 175/1000 | Loss: 0.00001172
Iteration 176/1000 | Loss: 0.00001172
Iteration 177/1000 | Loss: 0.00001171
Iteration 178/1000 | Loss: 0.00001171
Iteration 179/1000 | Loss: 0.00001171
Iteration 180/1000 | Loss: 0.00001171
Iteration 181/1000 | Loss: 0.00001171
Iteration 182/1000 | Loss: 0.00001171
Iteration 183/1000 | Loss: 0.00001171
Iteration 184/1000 | Loss: 0.00001171
Iteration 185/1000 | Loss: 0.00001171
Iteration 186/1000 | Loss: 0.00001171
Iteration 187/1000 | Loss: 0.00001171
Iteration 188/1000 | Loss: 0.00001171
Iteration 189/1000 | Loss: 0.00001170
Iteration 190/1000 | Loss: 0.00001170
Iteration 191/1000 | Loss: 0.00001170
Iteration 192/1000 | Loss: 0.00001170
Iteration 193/1000 | Loss: 0.00001170
Iteration 194/1000 | Loss: 0.00001170
Iteration 195/1000 | Loss: 0.00001170
Iteration 196/1000 | Loss: 0.00001170
Iteration 197/1000 | Loss: 0.00001170
Iteration 198/1000 | Loss: 0.00001170
Iteration 199/1000 | Loss: 0.00001170
Iteration 200/1000 | Loss: 0.00001170
Iteration 201/1000 | Loss: 0.00001170
Iteration 202/1000 | Loss: 0.00001170
Iteration 203/1000 | Loss: 0.00001170
Iteration 204/1000 | Loss: 0.00001170
Iteration 205/1000 | Loss: 0.00001170
Iteration 206/1000 | Loss: 0.00001170
Iteration 207/1000 | Loss: 0.00001170
Iteration 208/1000 | Loss: 0.00001170
Iteration 209/1000 | Loss: 0.00001170
Iteration 210/1000 | Loss: 0.00001170
Iteration 211/1000 | Loss: 0.00001169
Iteration 212/1000 | Loss: 0.00001169
Iteration 213/1000 | Loss: 0.00001169
Iteration 214/1000 | Loss: 0.00001169
Iteration 215/1000 | Loss: 0.00001169
Iteration 216/1000 | Loss: 0.00001169
Iteration 217/1000 | Loss: 0.00001169
Iteration 218/1000 | Loss: 0.00001168
Iteration 219/1000 | Loss: 0.00001168
Iteration 220/1000 | Loss: 0.00001168
Iteration 221/1000 | Loss: 0.00001168
Iteration 222/1000 | Loss: 0.00001168
Iteration 223/1000 | Loss: 0.00001168
Iteration 224/1000 | Loss: 0.00001168
Iteration 225/1000 | Loss: 0.00001168
Iteration 226/1000 | Loss: 0.00001168
Iteration 227/1000 | Loss: 0.00001168
Iteration 228/1000 | Loss: 0.00001167
Iteration 229/1000 | Loss: 0.00001167
Iteration 230/1000 | Loss: 0.00001167
Iteration 231/1000 | Loss: 0.00001167
Iteration 232/1000 | Loss: 0.00001167
Iteration 233/1000 | Loss: 0.00001167
Iteration 234/1000 | Loss: 0.00001166
Iteration 235/1000 | Loss: 0.00001166
Iteration 236/1000 | Loss: 0.00001166
Iteration 237/1000 | Loss: 0.00001166
Iteration 238/1000 | Loss: 0.00001166
Iteration 239/1000 | Loss: 0.00001166
Iteration 240/1000 | Loss: 0.00001166
Iteration 241/1000 | Loss: 0.00001166
Iteration 242/1000 | Loss: 0.00001166
Iteration 243/1000 | Loss: 0.00001166
Iteration 244/1000 | Loss: 0.00001166
Iteration 245/1000 | Loss: 0.00001166
Iteration 246/1000 | Loss: 0.00001166
Iteration 247/1000 | Loss: 0.00001166
Iteration 248/1000 | Loss: 0.00001166
Iteration 249/1000 | Loss: 0.00001166
Iteration 250/1000 | Loss: 0.00001166
Iteration 251/1000 | Loss: 0.00001166
Iteration 252/1000 | Loss: 0.00001166
Iteration 253/1000 | Loss: 0.00001166
Iteration 254/1000 | Loss: 0.00001166
Iteration 255/1000 | Loss: 0.00001166
Iteration 256/1000 | Loss: 0.00001166
Iteration 257/1000 | Loss: 0.00001166
Iteration 258/1000 | Loss: 0.00001166
Iteration 259/1000 | Loss: 0.00001166
Iteration 260/1000 | Loss: 0.00001166
Iteration 261/1000 | Loss: 0.00001166
Iteration 262/1000 | Loss: 0.00001166
Iteration 263/1000 | Loss: 0.00001166
Iteration 264/1000 | Loss: 0.00001166
Iteration 265/1000 | Loss: 0.00001166
Iteration 266/1000 | Loss: 0.00001166
Iteration 267/1000 | Loss: 0.00001166
Iteration 268/1000 | Loss: 0.00001166
Iteration 269/1000 | Loss: 0.00001166
Iteration 270/1000 | Loss: 0.00001166
Iteration 271/1000 | Loss: 0.00001166
Iteration 272/1000 | Loss: 0.00001166
Iteration 273/1000 | Loss: 0.00001166
Iteration 274/1000 | Loss: 0.00001166
Iteration 275/1000 | Loss: 0.00001166
Iteration 276/1000 | Loss: 0.00001166
Iteration 277/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.1658672519843094e-05, 1.1658672519843094e-05, 1.1658672519843094e-05, 1.1658672519843094e-05, 1.1658672519843094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1658672519843094e-05

Optimization complete. Final v2v error: 2.909041404724121 mm

Highest mean error: 3.0942838191986084 mm for frame 47

Lowest mean error: 2.7584009170532227 mm for frame 145

Saving results

Total time: 43.979294776916504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946754
Iteration 2/25 | Loss: 0.00314054
Iteration 3/25 | Loss: 0.00218964
Iteration 4/25 | Loss: 0.00198945
Iteration 5/25 | Loss: 0.00182631
Iteration 6/25 | Loss: 0.00171078
Iteration 7/25 | Loss: 0.00165267
Iteration 8/25 | Loss: 0.00162752
Iteration 9/25 | Loss: 0.00161276
Iteration 10/25 | Loss: 0.00159382
Iteration 11/25 | Loss: 0.00159003
Iteration 12/25 | Loss: 0.00157974
Iteration 13/25 | Loss: 0.00157609
Iteration 14/25 | Loss: 0.00157793
Iteration 15/25 | Loss: 0.00157363
Iteration 16/25 | Loss: 0.00157503
Iteration 17/25 | Loss: 0.00157096
Iteration 18/25 | Loss: 0.00156849
Iteration 19/25 | Loss: 0.00156730
Iteration 20/25 | Loss: 0.00156600
Iteration 21/25 | Loss: 0.00156503
Iteration 22/25 | Loss: 0.00156478
Iteration 23/25 | Loss: 0.00156469
Iteration 24/25 | Loss: 0.00156469
Iteration 25/25 | Loss: 0.00156469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37180126
Iteration 2/25 | Loss: 0.00164624
Iteration 3/25 | Loss: 0.00143839
Iteration 4/25 | Loss: 0.00143839
Iteration 5/25 | Loss: 0.00143838
Iteration 6/25 | Loss: 0.00143838
Iteration 7/25 | Loss: 0.00143838
Iteration 8/25 | Loss: 0.00143838
Iteration 9/25 | Loss: 0.00143838
Iteration 10/25 | Loss: 0.00143838
Iteration 11/25 | Loss: 0.00143838
Iteration 12/25 | Loss: 0.00143838
Iteration 13/25 | Loss: 0.00143838
Iteration 14/25 | Loss: 0.00143838
Iteration 15/25 | Loss: 0.00143838
Iteration 16/25 | Loss: 0.00143838
Iteration 17/25 | Loss: 0.00143838
Iteration 18/25 | Loss: 0.00143838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014383834786713123, 0.0014383834786713123, 0.0014383834786713123, 0.0014383834786713123, 0.0014383834786713123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014383834786713123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143838
Iteration 2/1000 | Loss: 0.00030320
Iteration 3/1000 | Loss: 0.00034084
Iteration 4/1000 | Loss: 0.00009636
Iteration 5/1000 | Loss: 0.00005532
Iteration 6/1000 | Loss: 0.00004830
Iteration 7/1000 | Loss: 0.00006998
Iteration 8/1000 | Loss: 0.00005317
Iteration 9/1000 | Loss: 0.00004188
Iteration 10/1000 | Loss: 0.00004107
Iteration 11/1000 | Loss: 0.00004006
Iteration 12/1000 | Loss: 0.00003941
Iteration 13/1000 | Loss: 0.00004802
Iteration 14/1000 | Loss: 0.00003988
Iteration 15/1000 | Loss: 0.00003829
Iteration 16/1000 | Loss: 0.00004975
Iteration 17/1000 | Loss: 0.00003793
Iteration 18/1000 | Loss: 0.00003790
Iteration 19/1000 | Loss: 0.00005220
Iteration 20/1000 | Loss: 0.00004939
Iteration 21/1000 | Loss: 0.00003755
Iteration 22/1000 | Loss: 0.00003746
Iteration 23/1000 | Loss: 0.00003746
Iteration 24/1000 | Loss: 0.00003745
Iteration 25/1000 | Loss: 0.00003744
Iteration 26/1000 | Loss: 0.00003744
Iteration 27/1000 | Loss: 0.00004753
Iteration 28/1000 | Loss: 0.00003735
Iteration 29/1000 | Loss: 0.00003735
Iteration 30/1000 | Loss: 0.00003735
Iteration 31/1000 | Loss: 0.00003735
Iteration 32/1000 | Loss: 0.00003735
Iteration 33/1000 | Loss: 0.00003735
Iteration 34/1000 | Loss: 0.00003735
Iteration 35/1000 | Loss: 0.00003735
Iteration 36/1000 | Loss: 0.00003735
Iteration 37/1000 | Loss: 0.00003735
Iteration 38/1000 | Loss: 0.00003734
Iteration 39/1000 | Loss: 0.00003734
Iteration 40/1000 | Loss: 0.00003734
Iteration 41/1000 | Loss: 0.00003734
Iteration 42/1000 | Loss: 0.00003734
Iteration 43/1000 | Loss: 0.00003733
Iteration 44/1000 | Loss: 0.00003733
Iteration 45/1000 | Loss: 0.00003733
Iteration 46/1000 | Loss: 0.00003733
Iteration 47/1000 | Loss: 0.00003733
Iteration 48/1000 | Loss: 0.00003733
Iteration 49/1000 | Loss: 0.00003733
Iteration 50/1000 | Loss: 0.00003733
Iteration 51/1000 | Loss: 0.00003732
Iteration 52/1000 | Loss: 0.00003732
Iteration 53/1000 | Loss: 0.00003732
Iteration 54/1000 | Loss: 0.00003732
Iteration 55/1000 | Loss: 0.00003732
Iteration 56/1000 | Loss: 0.00003732
Iteration 57/1000 | Loss: 0.00003732
Iteration 58/1000 | Loss: 0.00003732
Iteration 59/1000 | Loss: 0.00003732
Iteration 60/1000 | Loss: 0.00003732
Iteration 61/1000 | Loss: 0.00003732
Iteration 62/1000 | Loss: 0.00003732
Iteration 63/1000 | Loss: 0.00003732
Iteration 64/1000 | Loss: 0.00003731
Iteration 65/1000 | Loss: 0.00003731
Iteration 66/1000 | Loss: 0.00003731
Iteration 67/1000 | Loss: 0.00003730
Iteration 68/1000 | Loss: 0.00003730
Iteration 69/1000 | Loss: 0.00003730
Iteration 70/1000 | Loss: 0.00003730
Iteration 71/1000 | Loss: 0.00003730
Iteration 72/1000 | Loss: 0.00003730
Iteration 73/1000 | Loss: 0.00003730
Iteration 74/1000 | Loss: 0.00003729
Iteration 75/1000 | Loss: 0.00003729
Iteration 76/1000 | Loss: 0.00003729
Iteration 77/1000 | Loss: 0.00003729
Iteration 78/1000 | Loss: 0.00003729
Iteration 79/1000 | Loss: 0.00003729
Iteration 80/1000 | Loss: 0.00003728
Iteration 81/1000 | Loss: 0.00003728
Iteration 82/1000 | Loss: 0.00003727
Iteration 83/1000 | Loss: 0.00003727
Iteration 84/1000 | Loss: 0.00003725
Iteration 85/1000 | Loss: 0.00003725
Iteration 86/1000 | Loss: 0.00003725
Iteration 87/1000 | Loss: 0.00003725
Iteration 88/1000 | Loss: 0.00003725
Iteration 89/1000 | Loss: 0.00003725
Iteration 90/1000 | Loss: 0.00003725
Iteration 91/1000 | Loss: 0.00004541
Iteration 92/1000 | Loss: 0.00004541
Iteration 93/1000 | Loss: 0.00003930
Iteration 94/1000 | Loss: 0.00003784
Iteration 95/1000 | Loss: 0.00003725
Iteration 96/1000 | Loss: 0.00003723
Iteration 97/1000 | Loss: 0.00003722
Iteration 98/1000 | Loss: 0.00003722
Iteration 99/1000 | Loss: 0.00003722
Iteration 100/1000 | Loss: 0.00003722
Iteration 101/1000 | Loss: 0.00003722
Iteration 102/1000 | Loss: 0.00003722
Iteration 103/1000 | Loss: 0.00003722
Iteration 104/1000 | Loss: 0.00003722
Iteration 105/1000 | Loss: 0.00003722
Iteration 106/1000 | Loss: 0.00003722
Iteration 107/1000 | Loss: 0.00003722
Iteration 108/1000 | Loss: 0.00003721
Iteration 109/1000 | Loss: 0.00003721
Iteration 110/1000 | Loss: 0.00003721
Iteration 111/1000 | Loss: 0.00003721
Iteration 112/1000 | Loss: 0.00003721
Iteration 113/1000 | Loss: 0.00003721
Iteration 114/1000 | Loss: 0.00003721
Iteration 115/1000 | Loss: 0.00003721
Iteration 116/1000 | Loss: 0.00003721
Iteration 117/1000 | Loss: 0.00003721
Iteration 118/1000 | Loss: 0.00003721
Iteration 119/1000 | Loss: 0.00003721
Iteration 120/1000 | Loss: 0.00003721
Iteration 121/1000 | Loss: 0.00003721
Iteration 122/1000 | Loss: 0.00003721
Iteration 123/1000 | Loss: 0.00003721
Iteration 124/1000 | Loss: 0.00003721
Iteration 125/1000 | Loss: 0.00003721
Iteration 126/1000 | Loss: 0.00003721
Iteration 127/1000 | Loss: 0.00003721
Iteration 128/1000 | Loss: 0.00003721
Iteration 129/1000 | Loss: 0.00003721
Iteration 130/1000 | Loss: 0.00003721
Iteration 131/1000 | Loss: 0.00003721
Iteration 132/1000 | Loss: 0.00003721
Iteration 133/1000 | Loss: 0.00003721
Iteration 134/1000 | Loss: 0.00003721
Iteration 135/1000 | Loss: 0.00003721
Iteration 136/1000 | Loss: 0.00003721
Iteration 137/1000 | Loss: 0.00003721
Iteration 138/1000 | Loss: 0.00003721
Iteration 139/1000 | Loss: 0.00003721
Iteration 140/1000 | Loss: 0.00003721
Iteration 141/1000 | Loss: 0.00003721
Iteration 142/1000 | Loss: 0.00003721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.720530366990715e-05, 3.720530366990715e-05, 3.720530366990715e-05, 3.720530366990715e-05, 3.720530366990715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.720530366990715e-05

Optimization complete. Final v2v error: 5.100546360015869 mm

Highest mean error: 5.46823263168335 mm for frame 49

Lowest mean error: 4.009593963623047 mm for frame 155

Saving results

Total time: 94.8073661327362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005586
Iteration 2/25 | Loss: 0.00290958
Iteration 3/25 | Loss: 0.00219445
Iteration 4/25 | Loss: 0.00181559
Iteration 5/25 | Loss: 0.00167984
Iteration 6/25 | Loss: 0.00159495
Iteration 7/25 | Loss: 0.00148430
Iteration 8/25 | Loss: 0.00141669
Iteration 9/25 | Loss: 0.00139628
Iteration 10/25 | Loss: 0.00137671
Iteration 11/25 | Loss: 0.00136531
Iteration 12/25 | Loss: 0.00136117
Iteration 13/25 | Loss: 0.00135982
Iteration 14/25 | Loss: 0.00135947
Iteration 15/25 | Loss: 0.00135930
Iteration 16/25 | Loss: 0.00135912
Iteration 17/25 | Loss: 0.00135909
Iteration 18/25 | Loss: 0.00135909
Iteration 19/25 | Loss: 0.00135909
Iteration 20/25 | Loss: 0.00135909
Iteration 21/25 | Loss: 0.00135909
Iteration 22/25 | Loss: 0.00135909
Iteration 23/25 | Loss: 0.00135909
Iteration 24/25 | Loss: 0.00135909
Iteration 25/25 | Loss: 0.00135909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46976197
Iteration 2/25 | Loss: 0.00104048
Iteration 3/25 | Loss: 0.00104047
Iteration 4/25 | Loss: 0.00104047
Iteration 5/25 | Loss: 0.00104047
Iteration 6/25 | Loss: 0.00104047
Iteration 7/25 | Loss: 0.00104047
Iteration 8/25 | Loss: 0.00104047
Iteration 9/25 | Loss: 0.00104047
Iteration 10/25 | Loss: 0.00104047
Iteration 11/25 | Loss: 0.00104047
Iteration 12/25 | Loss: 0.00104047
Iteration 13/25 | Loss: 0.00104047
Iteration 14/25 | Loss: 0.00104047
Iteration 15/25 | Loss: 0.00104047
Iteration 16/25 | Loss: 0.00104047
Iteration 17/25 | Loss: 0.00104047
Iteration 18/25 | Loss: 0.00104047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010404740460217, 0.0010404740460217, 0.0010404740460217, 0.0010404740460217, 0.0010404740460217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010404740460217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104047
Iteration 2/1000 | Loss: 0.00011546
Iteration 3/1000 | Loss: 0.00005712
Iteration 4/1000 | Loss: 0.00016541
Iteration 5/1000 | Loss: 0.00100489
Iteration 6/1000 | Loss: 0.00005769
Iteration 7/1000 | Loss: 0.00004755
Iteration 8/1000 | Loss: 0.00004268
Iteration 9/1000 | Loss: 0.00019663
Iteration 10/1000 | Loss: 0.00009208
Iteration 11/1000 | Loss: 0.00006020
Iteration 12/1000 | Loss: 0.00011122
Iteration 13/1000 | Loss: 0.00006259
Iteration 14/1000 | Loss: 0.00004335
Iteration 15/1000 | Loss: 0.00003724
Iteration 16/1000 | Loss: 0.00003637
Iteration 17/1000 | Loss: 0.00003587
Iteration 18/1000 | Loss: 0.00069274
Iteration 19/1000 | Loss: 0.00128538
Iteration 20/1000 | Loss: 0.00053638
Iteration 21/1000 | Loss: 0.00016214
Iteration 22/1000 | Loss: 0.00004145
Iteration 23/1000 | Loss: 0.00003412
Iteration 24/1000 | Loss: 0.00002974
Iteration 25/1000 | Loss: 0.00011358
Iteration 26/1000 | Loss: 0.00002555
Iteration 27/1000 | Loss: 0.00002411
Iteration 28/1000 | Loss: 0.00006836
Iteration 29/1000 | Loss: 0.00002308
Iteration 30/1000 | Loss: 0.00002136
Iteration 31/1000 | Loss: 0.00005865
Iteration 32/1000 | Loss: 0.00002077
Iteration 33/1000 | Loss: 0.00002026
Iteration 34/1000 | Loss: 0.00001983
Iteration 35/1000 | Loss: 0.00006424
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001916
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00005740
Iteration 44/1000 | Loss: 0.00009151
Iteration 45/1000 | Loss: 0.00039229
Iteration 46/1000 | Loss: 0.00002300
Iteration 47/1000 | Loss: 0.00001997
Iteration 48/1000 | Loss: 0.00001911
Iteration 49/1000 | Loss: 0.00001884
Iteration 50/1000 | Loss: 0.00005030
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001870
Iteration 53/1000 | Loss: 0.00001870
Iteration 54/1000 | Loss: 0.00001869
Iteration 55/1000 | Loss: 0.00001869
Iteration 56/1000 | Loss: 0.00001868
Iteration 57/1000 | Loss: 0.00001868
Iteration 58/1000 | Loss: 0.00001868
Iteration 59/1000 | Loss: 0.00001867
Iteration 60/1000 | Loss: 0.00001867
Iteration 61/1000 | Loss: 0.00001867
Iteration 62/1000 | Loss: 0.00001867
Iteration 63/1000 | Loss: 0.00001866
Iteration 64/1000 | Loss: 0.00001866
Iteration 65/1000 | Loss: 0.00001866
Iteration 66/1000 | Loss: 0.00001866
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001865
Iteration 75/1000 | Loss: 0.00001865
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001865
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.865410740720108e-05, 1.865410740720108e-05, 1.865410740720108e-05, 1.865410740720108e-05, 1.865410740720108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.865410740720108e-05

Optimization complete. Final v2v error: 3.6659910678863525 mm

Highest mean error: 4.30360746383667 mm for frame 57

Lowest mean error: 3.37558650970459 mm for frame 130

Saving results

Total time: 111.91212487220764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426252
Iteration 2/25 | Loss: 0.00134653
Iteration 3/25 | Loss: 0.00129475
Iteration 4/25 | Loss: 0.00128848
Iteration 5/25 | Loss: 0.00128642
Iteration 6/25 | Loss: 0.00128642
Iteration 7/25 | Loss: 0.00128642
Iteration 8/25 | Loss: 0.00128642
Iteration 9/25 | Loss: 0.00128642
Iteration 10/25 | Loss: 0.00128642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012864172458648682, 0.0012864172458648682, 0.0012864172458648682, 0.0012864172458648682, 0.0012864172458648682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012864172458648682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41505134
Iteration 2/25 | Loss: 0.00085844
Iteration 3/25 | Loss: 0.00085844
Iteration 4/25 | Loss: 0.00085844
Iteration 5/25 | Loss: 0.00085844
Iteration 6/25 | Loss: 0.00085844
Iteration 7/25 | Loss: 0.00085844
Iteration 8/25 | Loss: 0.00085844
Iteration 9/25 | Loss: 0.00085844
Iteration 10/25 | Loss: 0.00085844
Iteration 11/25 | Loss: 0.00085844
Iteration 12/25 | Loss: 0.00085844
Iteration 13/25 | Loss: 0.00085844
Iteration 14/25 | Loss: 0.00085844
Iteration 15/25 | Loss: 0.00085844
Iteration 16/25 | Loss: 0.00085844
Iteration 17/25 | Loss: 0.00085844
Iteration 18/25 | Loss: 0.00085844
Iteration 19/25 | Loss: 0.00085844
Iteration 20/25 | Loss: 0.00085844
Iteration 21/25 | Loss: 0.00085844
Iteration 22/25 | Loss: 0.00085844
Iteration 23/25 | Loss: 0.00085844
Iteration 24/25 | Loss: 0.00085844
Iteration 25/25 | Loss: 0.00085844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085844
Iteration 2/1000 | Loss: 0.00002914
Iteration 3/1000 | Loss: 0.00001826
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001578
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001408
Iteration 12/1000 | Loss: 0.00001392
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001375
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001370
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001363
Iteration 34/1000 | Loss: 0.00001363
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001360
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001358
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001358
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001355
Iteration 62/1000 | Loss: 0.00001355
Iteration 63/1000 | Loss: 0.00001354
Iteration 64/1000 | Loss: 0.00001354
Iteration 65/1000 | Loss: 0.00001353
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001353
Iteration 68/1000 | Loss: 0.00001353
Iteration 69/1000 | Loss: 0.00001353
Iteration 70/1000 | Loss: 0.00001353
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001352
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001352
Iteration 87/1000 | Loss: 0.00001352
Iteration 88/1000 | Loss: 0.00001352
Iteration 89/1000 | Loss: 0.00001352
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001352
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001352
Iteration 106/1000 | Loss: 0.00001352
Iteration 107/1000 | Loss: 0.00001352
Iteration 108/1000 | Loss: 0.00001352
Iteration 109/1000 | Loss: 0.00001352
Iteration 110/1000 | Loss: 0.00001352
Iteration 111/1000 | Loss: 0.00001352
Iteration 112/1000 | Loss: 0.00001352
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001352
Iteration 115/1000 | Loss: 0.00001352
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.3523924280889332e-05, 1.3523924280889332e-05, 1.3523924280889332e-05, 1.3523924280889332e-05, 1.3523924280889332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3523924280889332e-05

Optimization complete. Final v2v error: 3.159872055053711 mm

Highest mean error: 3.2595880031585693 mm for frame 37

Lowest mean error: 3.0594871044158936 mm for frame 26

Saving results

Total time: 30.61463475227356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00297988
Iteration 2/25 | Loss: 0.00134568
Iteration 3/25 | Loss: 0.00128432
Iteration 4/25 | Loss: 0.00126472
Iteration 5/25 | Loss: 0.00125700
Iteration 6/25 | Loss: 0.00125529
Iteration 7/25 | Loss: 0.00125522
Iteration 8/25 | Loss: 0.00125522
Iteration 9/25 | Loss: 0.00125522
Iteration 10/25 | Loss: 0.00125522
Iteration 11/25 | Loss: 0.00125522
Iteration 12/25 | Loss: 0.00125522
Iteration 13/25 | Loss: 0.00125522
Iteration 14/25 | Loss: 0.00125522
Iteration 15/25 | Loss: 0.00125522
Iteration 16/25 | Loss: 0.00125522
Iteration 17/25 | Loss: 0.00125522
Iteration 18/25 | Loss: 0.00125522
Iteration 19/25 | Loss: 0.00125522
Iteration 20/25 | Loss: 0.00125522
Iteration 21/25 | Loss: 0.00125522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012552225962281227, 0.0012552225962281227, 0.0012552225962281227, 0.0012552225962281227, 0.0012552225962281227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012552225962281227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26210964
Iteration 2/25 | Loss: 0.00099686
Iteration 3/25 | Loss: 0.00099686
Iteration 4/25 | Loss: 0.00099685
Iteration 5/25 | Loss: 0.00099685
Iteration 6/25 | Loss: 0.00099685
Iteration 7/25 | Loss: 0.00099685
Iteration 8/25 | Loss: 0.00099685
Iteration 9/25 | Loss: 0.00099685
Iteration 10/25 | Loss: 0.00099685
Iteration 11/25 | Loss: 0.00099685
Iteration 12/25 | Loss: 0.00099685
Iteration 13/25 | Loss: 0.00099685
Iteration 14/25 | Loss: 0.00099685
Iteration 15/25 | Loss: 0.00099685
Iteration 16/25 | Loss: 0.00099685
Iteration 17/25 | Loss: 0.00099685
Iteration 18/25 | Loss: 0.00099685
Iteration 19/25 | Loss: 0.00099685
Iteration 20/25 | Loss: 0.00099685
Iteration 21/25 | Loss: 0.00099685
Iteration 22/25 | Loss: 0.00099685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009968517115339637, 0.0009968517115339637, 0.0009968517115339637, 0.0009968517115339637, 0.0009968517115339637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009968517115339637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099685
Iteration 2/1000 | Loss: 0.00003576
Iteration 3/1000 | Loss: 0.00002543
Iteration 4/1000 | Loss: 0.00002315
Iteration 5/1000 | Loss: 0.00002238
Iteration 6/1000 | Loss: 0.00002114
Iteration 7/1000 | Loss: 0.00002057
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00001975
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00001929
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001879
Iteration 16/1000 | Loss: 0.00001879
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00001874
Iteration 20/1000 | Loss: 0.00001874
Iteration 21/1000 | Loss: 0.00001874
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001873
Iteration 24/1000 | Loss: 0.00001873
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001872
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001870
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00001869
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001866
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001865
Iteration 49/1000 | Loss: 0.00001865
Iteration 50/1000 | Loss: 0.00001865
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001857
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001855
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001855
Iteration 80/1000 | Loss: 0.00001855
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001854
Iteration 84/1000 | Loss: 0.00001854
Iteration 85/1000 | Loss: 0.00001854
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001853
Iteration 88/1000 | Loss: 0.00001853
Iteration 89/1000 | Loss: 0.00001853
Iteration 90/1000 | Loss: 0.00001853
Iteration 91/1000 | Loss: 0.00001853
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001852
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001851
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001850
Iteration 105/1000 | Loss: 0.00001850
Iteration 106/1000 | Loss: 0.00001850
Iteration 107/1000 | Loss: 0.00001850
Iteration 108/1000 | Loss: 0.00001850
Iteration 109/1000 | Loss: 0.00001850
Iteration 110/1000 | Loss: 0.00001850
Iteration 111/1000 | Loss: 0.00001850
Iteration 112/1000 | Loss: 0.00001849
Iteration 113/1000 | Loss: 0.00001849
Iteration 114/1000 | Loss: 0.00001849
Iteration 115/1000 | Loss: 0.00001849
Iteration 116/1000 | Loss: 0.00001848
Iteration 117/1000 | Loss: 0.00001848
Iteration 118/1000 | Loss: 0.00001848
Iteration 119/1000 | Loss: 0.00001848
Iteration 120/1000 | Loss: 0.00001847
Iteration 121/1000 | Loss: 0.00001847
Iteration 122/1000 | Loss: 0.00001847
Iteration 123/1000 | Loss: 0.00001847
Iteration 124/1000 | Loss: 0.00001847
Iteration 125/1000 | Loss: 0.00001847
Iteration 126/1000 | Loss: 0.00001846
Iteration 127/1000 | Loss: 0.00001846
Iteration 128/1000 | Loss: 0.00001846
Iteration 129/1000 | Loss: 0.00001846
Iteration 130/1000 | Loss: 0.00001846
Iteration 131/1000 | Loss: 0.00001846
Iteration 132/1000 | Loss: 0.00001846
Iteration 133/1000 | Loss: 0.00001846
Iteration 134/1000 | Loss: 0.00001846
Iteration 135/1000 | Loss: 0.00001845
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Iteration 141/1000 | Loss: 0.00001845
Iteration 142/1000 | Loss: 0.00001845
Iteration 143/1000 | Loss: 0.00001845
Iteration 144/1000 | Loss: 0.00001845
Iteration 145/1000 | Loss: 0.00001845
Iteration 146/1000 | Loss: 0.00001844
Iteration 147/1000 | Loss: 0.00001844
Iteration 148/1000 | Loss: 0.00001844
Iteration 149/1000 | Loss: 0.00001844
Iteration 150/1000 | Loss: 0.00001844
Iteration 151/1000 | Loss: 0.00001844
Iteration 152/1000 | Loss: 0.00001844
Iteration 153/1000 | Loss: 0.00001844
Iteration 154/1000 | Loss: 0.00001844
Iteration 155/1000 | Loss: 0.00001844
Iteration 156/1000 | Loss: 0.00001844
Iteration 157/1000 | Loss: 0.00001844
Iteration 158/1000 | Loss: 0.00001844
Iteration 159/1000 | Loss: 0.00001844
Iteration 160/1000 | Loss: 0.00001844
Iteration 161/1000 | Loss: 0.00001844
Iteration 162/1000 | Loss: 0.00001844
Iteration 163/1000 | Loss: 0.00001844
Iteration 164/1000 | Loss: 0.00001843
Iteration 165/1000 | Loss: 0.00001843
Iteration 166/1000 | Loss: 0.00001843
Iteration 167/1000 | Loss: 0.00001843
Iteration 168/1000 | Loss: 0.00001843
Iteration 169/1000 | Loss: 0.00001843
Iteration 170/1000 | Loss: 0.00001843
Iteration 171/1000 | Loss: 0.00001842
Iteration 172/1000 | Loss: 0.00001842
Iteration 173/1000 | Loss: 0.00001842
Iteration 174/1000 | Loss: 0.00001842
Iteration 175/1000 | Loss: 0.00001842
Iteration 176/1000 | Loss: 0.00001842
Iteration 177/1000 | Loss: 0.00001842
Iteration 178/1000 | Loss: 0.00001842
Iteration 179/1000 | Loss: 0.00001842
Iteration 180/1000 | Loss: 0.00001842
Iteration 181/1000 | Loss: 0.00001842
Iteration 182/1000 | Loss: 0.00001842
Iteration 183/1000 | Loss: 0.00001842
Iteration 184/1000 | Loss: 0.00001841
Iteration 185/1000 | Loss: 0.00001841
Iteration 186/1000 | Loss: 0.00001841
Iteration 187/1000 | Loss: 0.00001841
Iteration 188/1000 | Loss: 0.00001841
Iteration 189/1000 | Loss: 0.00001841
Iteration 190/1000 | Loss: 0.00001841
Iteration 191/1000 | Loss: 0.00001841
Iteration 192/1000 | Loss: 0.00001841
Iteration 193/1000 | Loss: 0.00001840
Iteration 194/1000 | Loss: 0.00001840
Iteration 195/1000 | Loss: 0.00001840
Iteration 196/1000 | Loss: 0.00001840
Iteration 197/1000 | Loss: 0.00001840
Iteration 198/1000 | Loss: 0.00001840
Iteration 199/1000 | Loss: 0.00001840
Iteration 200/1000 | Loss: 0.00001840
Iteration 201/1000 | Loss: 0.00001840
Iteration 202/1000 | Loss: 0.00001840
Iteration 203/1000 | Loss: 0.00001840
Iteration 204/1000 | Loss: 0.00001840
Iteration 205/1000 | Loss: 0.00001840
Iteration 206/1000 | Loss: 0.00001840
Iteration 207/1000 | Loss: 0.00001840
Iteration 208/1000 | Loss: 0.00001840
Iteration 209/1000 | Loss: 0.00001840
Iteration 210/1000 | Loss: 0.00001840
Iteration 211/1000 | Loss: 0.00001840
Iteration 212/1000 | Loss: 0.00001840
Iteration 213/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.8397049643681385e-05, 1.8397049643681385e-05, 1.8397049643681385e-05, 1.8397049643681385e-05, 1.8397049643681385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8397049643681385e-05

Optimization complete. Final v2v error: 3.6408233642578125 mm

Highest mean error: 4.071382522583008 mm for frame 136

Lowest mean error: 3.3965001106262207 mm for frame 155

Saving results

Total time: 44.06570291519165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032507
Iteration 2/25 | Loss: 0.00217017
Iteration 3/25 | Loss: 0.00175102
Iteration 4/25 | Loss: 0.00154964
Iteration 5/25 | Loss: 0.00157807
Iteration 6/25 | Loss: 0.00152015
Iteration 7/25 | Loss: 0.00148139
Iteration 8/25 | Loss: 0.00142796
Iteration 9/25 | Loss: 0.00142732
Iteration 10/25 | Loss: 0.00137288
Iteration 11/25 | Loss: 0.00136774
Iteration 12/25 | Loss: 0.00134025
Iteration 13/25 | Loss: 0.00134173
Iteration 14/25 | Loss: 0.00134347
Iteration 15/25 | Loss: 0.00133973
Iteration 16/25 | Loss: 0.00134444
Iteration 17/25 | Loss: 0.00134424
Iteration 18/25 | Loss: 0.00133238
Iteration 19/25 | Loss: 0.00134087
Iteration 20/25 | Loss: 0.00133212
Iteration 21/25 | Loss: 0.00133205
Iteration 22/25 | Loss: 0.00133204
Iteration 23/25 | Loss: 0.00133204
Iteration 24/25 | Loss: 0.00133204
Iteration 25/25 | Loss: 0.00133204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38100374
Iteration 2/25 | Loss: 0.00254488
Iteration 3/25 | Loss: 0.00101611
Iteration 4/25 | Loss: 0.00101611
Iteration 5/25 | Loss: 0.00101611
Iteration 6/25 | Loss: 0.00101611
Iteration 7/25 | Loss: 0.00101611
Iteration 8/25 | Loss: 0.00101611
Iteration 9/25 | Loss: 0.00101611
Iteration 10/25 | Loss: 0.00101611
Iteration 11/25 | Loss: 0.00101611
Iteration 12/25 | Loss: 0.00101611
Iteration 13/25 | Loss: 0.00101611
Iteration 14/25 | Loss: 0.00101611
Iteration 15/25 | Loss: 0.00101611
Iteration 16/25 | Loss: 0.00101611
Iteration 17/25 | Loss: 0.00101611
Iteration 18/25 | Loss: 0.00101611
Iteration 19/25 | Loss: 0.00101611
Iteration 20/25 | Loss: 0.00101611
Iteration 21/25 | Loss: 0.00101611
Iteration 22/25 | Loss: 0.00101611
Iteration 23/25 | Loss: 0.00101611
Iteration 24/25 | Loss: 0.00101611
Iteration 25/25 | Loss: 0.00101611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101611
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00002494
Iteration 4/1000 | Loss: 0.00002371
Iteration 5/1000 | Loss: 0.00002309
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00002214
Iteration 8/1000 | Loss: 0.00002191
Iteration 9/1000 | Loss: 0.00002160
Iteration 10/1000 | Loss: 0.00002133
Iteration 11/1000 | Loss: 0.00002107
Iteration 12/1000 | Loss: 0.00002083
Iteration 13/1000 | Loss: 0.00002069
Iteration 14/1000 | Loss: 0.00002051
Iteration 15/1000 | Loss: 0.00002036
Iteration 16/1000 | Loss: 0.00002035
Iteration 17/1000 | Loss: 0.00002035
Iteration 18/1000 | Loss: 0.00002034
Iteration 19/1000 | Loss: 0.00002034
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002032
Iteration 22/1000 | Loss: 0.00002032
Iteration 23/1000 | Loss: 0.00002032
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00002031
Iteration 26/1000 | Loss: 0.00002030
Iteration 27/1000 | Loss: 0.00002028
Iteration 28/1000 | Loss: 0.00002028
Iteration 29/1000 | Loss: 0.00002026
Iteration 30/1000 | Loss: 0.00002026
Iteration 31/1000 | Loss: 0.00002026
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002019
Iteration 34/1000 | Loss: 0.00002017
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002014
Iteration 41/1000 | Loss: 0.00002014
Iteration 42/1000 | Loss: 0.00002013
Iteration 43/1000 | Loss: 0.00002013
Iteration 44/1000 | Loss: 0.00002013
Iteration 45/1000 | Loss: 0.00002013
Iteration 46/1000 | Loss: 0.00002013
Iteration 47/1000 | Loss: 0.00002013
Iteration 48/1000 | Loss: 0.00002013
Iteration 49/1000 | Loss: 0.00002013
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002012
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00002012
Iteration 55/1000 | Loss: 0.00002012
Iteration 56/1000 | Loss: 0.00002012
Iteration 57/1000 | Loss: 0.00002012
Iteration 58/1000 | Loss: 0.00002012
Iteration 59/1000 | Loss: 0.00002011
Iteration 60/1000 | Loss: 0.00002011
Iteration 61/1000 | Loss: 0.00002010
Iteration 62/1000 | Loss: 0.00002009
Iteration 63/1000 | Loss: 0.00002008
Iteration 64/1000 | Loss: 0.00002008
Iteration 65/1000 | Loss: 0.00002008
Iteration 66/1000 | Loss: 0.00002008
Iteration 67/1000 | Loss: 0.00002008
Iteration 68/1000 | Loss: 0.00002008
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002007
Iteration 72/1000 | Loss: 0.00002007
Iteration 73/1000 | Loss: 0.00002007
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002005
Iteration 76/1000 | Loss: 0.00002005
Iteration 77/1000 | Loss: 0.00002005
Iteration 78/1000 | Loss: 0.00002004
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00002003
Iteration 81/1000 | Loss: 0.00002003
Iteration 82/1000 | Loss: 0.00002002
Iteration 83/1000 | Loss: 0.00002002
Iteration 84/1000 | Loss: 0.00002002
Iteration 85/1000 | Loss: 0.00002002
Iteration 86/1000 | Loss: 0.00002002
Iteration 87/1000 | Loss: 0.00002001
Iteration 88/1000 | Loss: 0.00002001
Iteration 89/1000 | Loss: 0.00002001
Iteration 90/1000 | Loss: 0.00002001
Iteration 91/1000 | Loss: 0.00002001
Iteration 92/1000 | Loss: 0.00002001
Iteration 93/1000 | Loss: 0.00002001
Iteration 94/1000 | Loss: 0.00002001
Iteration 95/1000 | Loss: 0.00002000
Iteration 96/1000 | Loss: 0.00002000
Iteration 97/1000 | Loss: 0.00002000
Iteration 98/1000 | Loss: 0.00002000
Iteration 99/1000 | Loss: 0.00002000
Iteration 100/1000 | Loss: 0.00002000
Iteration 101/1000 | Loss: 0.00002000
Iteration 102/1000 | Loss: 0.00002000
Iteration 103/1000 | Loss: 0.00002000
Iteration 104/1000 | Loss: 0.00002000
Iteration 105/1000 | Loss: 0.00002000
Iteration 106/1000 | Loss: 0.00002000
Iteration 107/1000 | Loss: 0.00002000
Iteration 108/1000 | Loss: 0.00002000
Iteration 109/1000 | Loss: 0.00002000
Iteration 110/1000 | Loss: 0.00002000
Iteration 111/1000 | Loss: 0.00002000
Iteration 112/1000 | Loss: 0.00002000
Iteration 113/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.0002094970550388e-05, 2.0002094970550388e-05, 2.0002094970550388e-05, 2.0002094970550388e-05, 2.0002094970550388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0002094970550388e-05

Optimization complete. Final v2v error: 3.8103127479553223 mm

Highest mean error: 4.070265293121338 mm for frame 237

Lowest mean error: 3.7009551525115967 mm for frame 22

Saving results

Total time: 75.09019684791565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456669
Iteration 2/25 | Loss: 0.00134616
Iteration 3/25 | Loss: 0.00129848
Iteration 4/25 | Loss: 0.00129078
Iteration 5/25 | Loss: 0.00128885
Iteration 6/25 | Loss: 0.00128885
Iteration 7/25 | Loss: 0.00128885
Iteration 8/25 | Loss: 0.00128885
Iteration 9/25 | Loss: 0.00128885
Iteration 10/25 | Loss: 0.00128885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012888472992926836, 0.0012888472992926836, 0.0012888472992926836, 0.0012888472992926836, 0.0012888472992926836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012888472992926836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41429365
Iteration 2/25 | Loss: 0.00093030
Iteration 3/25 | Loss: 0.00093030
Iteration 4/25 | Loss: 0.00093030
Iteration 5/25 | Loss: 0.00093030
Iteration 6/25 | Loss: 0.00093030
Iteration 7/25 | Loss: 0.00093030
Iteration 8/25 | Loss: 0.00093029
Iteration 9/25 | Loss: 0.00093029
Iteration 10/25 | Loss: 0.00093029
Iteration 11/25 | Loss: 0.00093029
Iteration 12/25 | Loss: 0.00093029
Iteration 13/25 | Loss: 0.00093029
Iteration 14/25 | Loss: 0.00093029
Iteration 15/25 | Loss: 0.00093029
Iteration 16/25 | Loss: 0.00093029
Iteration 17/25 | Loss: 0.00093029
Iteration 18/25 | Loss: 0.00093029
Iteration 19/25 | Loss: 0.00093029
Iteration 20/25 | Loss: 0.00093029
Iteration 21/25 | Loss: 0.00093029
Iteration 22/25 | Loss: 0.00093029
Iteration 23/25 | Loss: 0.00093029
Iteration 24/25 | Loss: 0.00093029
Iteration 25/25 | Loss: 0.00093029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093029
Iteration 2/1000 | Loss: 0.00002859
Iteration 3/1000 | Loss: 0.00002178
Iteration 4/1000 | Loss: 0.00002018
Iteration 5/1000 | Loss: 0.00001927
Iteration 6/1000 | Loss: 0.00001877
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001824
Iteration 9/1000 | Loss: 0.00001794
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001763
Iteration 12/1000 | Loss: 0.00001748
Iteration 13/1000 | Loss: 0.00001745
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001715
Iteration 20/1000 | Loss: 0.00001714
Iteration 21/1000 | Loss: 0.00001713
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001712
Iteration 24/1000 | Loss: 0.00001711
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001708
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001706
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001704
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001702
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001702
Iteration 51/1000 | Loss: 0.00001702
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001687
Iteration 66/1000 | Loss: 0.00001687
Iteration 67/1000 | Loss: 0.00001687
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001681
Iteration 73/1000 | Loss: 0.00001681
Iteration 74/1000 | Loss: 0.00001681
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001680
Iteration 82/1000 | Loss: 0.00001680
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001679
Iteration 86/1000 | Loss: 0.00001679
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001679
Iteration 89/1000 | Loss: 0.00001679
Iteration 90/1000 | Loss: 0.00001679
Iteration 91/1000 | Loss: 0.00001679
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001677
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001676
Iteration 99/1000 | Loss: 0.00001676
Iteration 100/1000 | Loss: 0.00001676
Iteration 101/1000 | Loss: 0.00001676
Iteration 102/1000 | Loss: 0.00001676
Iteration 103/1000 | Loss: 0.00001676
Iteration 104/1000 | Loss: 0.00001676
Iteration 105/1000 | Loss: 0.00001676
Iteration 106/1000 | Loss: 0.00001676
Iteration 107/1000 | Loss: 0.00001676
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001676
Iteration 112/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.676116335147526e-05, 1.676116335147526e-05, 1.676116335147526e-05, 1.676116335147526e-05, 1.676116335147526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.676116335147526e-05

Optimization complete. Final v2v error: 3.447850465774536 mm

Highest mean error: 3.9374372959136963 mm for frame 131

Lowest mean error: 3.1460609436035156 mm for frame 24

Saving results

Total time: 42.259724378585815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806708
Iteration 2/25 | Loss: 0.00137937
Iteration 3/25 | Loss: 0.00127771
Iteration 4/25 | Loss: 0.00126719
Iteration 5/25 | Loss: 0.00126527
Iteration 6/25 | Loss: 0.00126491
Iteration 7/25 | Loss: 0.00126491
Iteration 8/25 | Loss: 0.00126491
Iteration 9/25 | Loss: 0.00126491
Iteration 10/25 | Loss: 0.00126491
Iteration 11/25 | Loss: 0.00126491
Iteration 12/25 | Loss: 0.00126491
Iteration 13/25 | Loss: 0.00126491
Iteration 14/25 | Loss: 0.00126491
Iteration 15/25 | Loss: 0.00126491
Iteration 16/25 | Loss: 0.00126491
Iteration 17/25 | Loss: 0.00126491
Iteration 18/25 | Loss: 0.00126491
Iteration 19/25 | Loss: 0.00126491
Iteration 20/25 | Loss: 0.00126491
Iteration 21/25 | Loss: 0.00126491
Iteration 22/25 | Loss: 0.00126491
Iteration 23/25 | Loss: 0.00126491
Iteration 24/25 | Loss: 0.00126491
Iteration 25/25 | Loss: 0.00126491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39268792
Iteration 2/25 | Loss: 0.00077066
Iteration 3/25 | Loss: 0.00077063
Iteration 4/25 | Loss: 0.00077063
Iteration 5/25 | Loss: 0.00077063
Iteration 6/25 | Loss: 0.00077063
Iteration 7/25 | Loss: 0.00077063
Iteration 8/25 | Loss: 0.00077063
Iteration 9/25 | Loss: 0.00077063
Iteration 10/25 | Loss: 0.00077063
Iteration 11/25 | Loss: 0.00077063
Iteration 12/25 | Loss: 0.00077063
Iteration 13/25 | Loss: 0.00077063
Iteration 14/25 | Loss: 0.00077063
Iteration 15/25 | Loss: 0.00077063
Iteration 16/25 | Loss: 0.00077063
Iteration 17/25 | Loss: 0.00077063
Iteration 18/25 | Loss: 0.00077063
Iteration 19/25 | Loss: 0.00077063
Iteration 20/25 | Loss: 0.00077063
Iteration 21/25 | Loss: 0.00077063
Iteration 22/25 | Loss: 0.00077063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007706312462687492, 0.0007706312462687492, 0.0007706312462687492, 0.0007706312462687492, 0.0007706312462687492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007706312462687492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077063
Iteration 2/1000 | Loss: 0.00002483
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001465
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001259
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001183
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001163
Iteration 13/1000 | Loss: 0.00001157
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001138
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001129
Iteration 23/1000 | Loss: 0.00001128
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001126
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001123
Iteration 28/1000 | Loss: 0.00001123
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001121
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001119
Iteration 36/1000 | Loss: 0.00001119
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001118
Iteration 44/1000 | Loss: 0.00001117
Iteration 45/1000 | Loss: 0.00001117
Iteration 46/1000 | Loss: 0.00001116
Iteration 47/1000 | Loss: 0.00001116
Iteration 48/1000 | Loss: 0.00001116
Iteration 49/1000 | Loss: 0.00001116
Iteration 50/1000 | Loss: 0.00001116
Iteration 51/1000 | Loss: 0.00001115
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001115
Iteration 54/1000 | Loss: 0.00001115
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001114
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001114
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001111
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001111
Iteration 70/1000 | Loss: 0.00001111
Iteration 71/1000 | Loss: 0.00001111
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001109
Iteration 80/1000 | Loss: 0.00001109
Iteration 81/1000 | Loss: 0.00001109
Iteration 82/1000 | Loss: 0.00001108
Iteration 83/1000 | Loss: 0.00001108
Iteration 84/1000 | Loss: 0.00001108
Iteration 85/1000 | Loss: 0.00001108
Iteration 86/1000 | Loss: 0.00001107
Iteration 87/1000 | Loss: 0.00001107
Iteration 88/1000 | Loss: 0.00001107
Iteration 89/1000 | Loss: 0.00001106
Iteration 90/1000 | Loss: 0.00001106
Iteration 91/1000 | Loss: 0.00001106
Iteration 92/1000 | Loss: 0.00001106
Iteration 93/1000 | Loss: 0.00001105
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001105
Iteration 96/1000 | Loss: 0.00001105
Iteration 97/1000 | Loss: 0.00001105
Iteration 98/1000 | Loss: 0.00001105
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001104
Iteration 105/1000 | Loss: 0.00001103
Iteration 106/1000 | Loss: 0.00001103
Iteration 107/1000 | Loss: 0.00001103
Iteration 108/1000 | Loss: 0.00001103
Iteration 109/1000 | Loss: 0.00001102
Iteration 110/1000 | Loss: 0.00001102
Iteration 111/1000 | Loss: 0.00001102
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001101
Iteration 114/1000 | Loss: 0.00001101
Iteration 115/1000 | Loss: 0.00001101
Iteration 116/1000 | Loss: 0.00001101
Iteration 117/1000 | Loss: 0.00001101
Iteration 118/1000 | Loss: 0.00001100
Iteration 119/1000 | Loss: 0.00001100
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001098
Iteration 125/1000 | Loss: 0.00001098
Iteration 126/1000 | Loss: 0.00001098
Iteration 127/1000 | Loss: 0.00001098
Iteration 128/1000 | Loss: 0.00001098
Iteration 129/1000 | Loss: 0.00001098
Iteration 130/1000 | Loss: 0.00001098
Iteration 131/1000 | Loss: 0.00001098
Iteration 132/1000 | Loss: 0.00001097
Iteration 133/1000 | Loss: 0.00001097
Iteration 134/1000 | Loss: 0.00001097
Iteration 135/1000 | Loss: 0.00001097
Iteration 136/1000 | Loss: 0.00001097
Iteration 137/1000 | Loss: 0.00001097
Iteration 138/1000 | Loss: 0.00001096
Iteration 139/1000 | Loss: 0.00001096
Iteration 140/1000 | Loss: 0.00001096
Iteration 141/1000 | Loss: 0.00001096
Iteration 142/1000 | Loss: 0.00001096
Iteration 143/1000 | Loss: 0.00001096
Iteration 144/1000 | Loss: 0.00001096
Iteration 145/1000 | Loss: 0.00001096
Iteration 146/1000 | Loss: 0.00001095
Iteration 147/1000 | Loss: 0.00001095
Iteration 148/1000 | Loss: 0.00001095
Iteration 149/1000 | Loss: 0.00001095
Iteration 150/1000 | Loss: 0.00001095
Iteration 151/1000 | Loss: 0.00001095
Iteration 152/1000 | Loss: 0.00001095
Iteration 153/1000 | Loss: 0.00001095
Iteration 154/1000 | Loss: 0.00001095
Iteration 155/1000 | Loss: 0.00001095
Iteration 156/1000 | Loss: 0.00001095
Iteration 157/1000 | Loss: 0.00001095
Iteration 158/1000 | Loss: 0.00001095
Iteration 159/1000 | Loss: 0.00001095
Iteration 160/1000 | Loss: 0.00001094
Iteration 161/1000 | Loss: 0.00001094
Iteration 162/1000 | Loss: 0.00001094
Iteration 163/1000 | Loss: 0.00001094
Iteration 164/1000 | Loss: 0.00001094
Iteration 165/1000 | Loss: 0.00001094
Iteration 166/1000 | Loss: 0.00001094
Iteration 167/1000 | Loss: 0.00001093
Iteration 168/1000 | Loss: 0.00001093
Iteration 169/1000 | Loss: 0.00001093
Iteration 170/1000 | Loss: 0.00001093
Iteration 171/1000 | Loss: 0.00001093
Iteration 172/1000 | Loss: 0.00001093
Iteration 173/1000 | Loss: 0.00001093
Iteration 174/1000 | Loss: 0.00001093
Iteration 175/1000 | Loss: 0.00001093
Iteration 176/1000 | Loss: 0.00001093
Iteration 177/1000 | Loss: 0.00001093
Iteration 178/1000 | Loss: 0.00001093
Iteration 179/1000 | Loss: 0.00001093
Iteration 180/1000 | Loss: 0.00001093
Iteration 181/1000 | Loss: 0.00001093
Iteration 182/1000 | Loss: 0.00001093
Iteration 183/1000 | Loss: 0.00001093
Iteration 184/1000 | Loss: 0.00001093
Iteration 185/1000 | Loss: 0.00001093
Iteration 186/1000 | Loss: 0.00001093
Iteration 187/1000 | Loss: 0.00001093
Iteration 188/1000 | Loss: 0.00001093
Iteration 189/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.0928871233772952e-05, 1.0928871233772952e-05, 1.0928871233772952e-05, 1.0928871233772952e-05, 1.0928871233772952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0928871233772952e-05

Optimization complete. Final v2v error: 2.8277597427368164 mm

Highest mean error: 3.056532859802246 mm for frame 3

Lowest mean error: 2.716430902481079 mm for frame 85

Saving results

Total time: 39.31080770492554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487875
Iteration 2/25 | Loss: 0.00155625
Iteration 3/25 | Loss: 0.00133946
Iteration 4/25 | Loss: 0.00131401
Iteration 5/25 | Loss: 0.00131098
Iteration 6/25 | Loss: 0.00131033
Iteration 7/25 | Loss: 0.00131033
Iteration 8/25 | Loss: 0.00131033
Iteration 9/25 | Loss: 0.00131033
Iteration 10/25 | Loss: 0.00131033
Iteration 11/25 | Loss: 0.00131033
Iteration 12/25 | Loss: 0.00131033
Iteration 13/25 | Loss: 0.00131033
Iteration 14/25 | Loss: 0.00131033
Iteration 15/25 | Loss: 0.00131033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013103284873068333, 0.0013103284873068333, 0.0013103284873068333, 0.0013103284873068333, 0.0013103284873068333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013103284873068333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41963959
Iteration 2/25 | Loss: 0.00086099
Iteration 3/25 | Loss: 0.00086099
Iteration 4/25 | Loss: 0.00086099
Iteration 5/25 | Loss: 0.00086099
Iteration 6/25 | Loss: 0.00086099
Iteration 7/25 | Loss: 0.00086099
Iteration 8/25 | Loss: 0.00086099
Iteration 9/25 | Loss: 0.00086099
Iteration 10/25 | Loss: 0.00086099
Iteration 11/25 | Loss: 0.00086099
Iteration 12/25 | Loss: 0.00086099
Iteration 13/25 | Loss: 0.00086099
Iteration 14/25 | Loss: 0.00086099
Iteration 15/25 | Loss: 0.00086099
Iteration 16/25 | Loss: 0.00086099
Iteration 17/25 | Loss: 0.00086099
Iteration 18/25 | Loss: 0.00086099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008609857759438455, 0.0008609857759438455, 0.0008609857759438455, 0.0008609857759438455, 0.0008609857759438455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008609857759438455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086099
Iteration 2/1000 | Loss: 0.00003757
Iteration 3/1000 | Loss: 0.00002170
Iteration 4/1000 | Loss: 0.00001913
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001594
Iteration 10/1000 | Loss: 0.00001566
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001546
Iteration 13/1000 | Loss: 0.00001534
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001526
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001523
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001520
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001516
Iteration 25/1000 | Loss: 0.00001515
Iteration 26/1000 | Loss: 0.00001515
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001515
Iteration 30/1000 | Loss: 0.00001514
Iteration 31/1000 | Loss: 0.00001514
Iteration 32/1000 | Loss: 0.00001513
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001511
Iteration 35/1000 | Loss: 0.00001511
Iteration 36/1000 | Loss: 0.00001510
Iteration 37/1000 | Loss: 0.00001510
Iteration 38/1000 | Loss: 0.00001509
Iteration 39/1000 | Loss: 0.00001508
Iteration 40/1000 | Loss: 0.00001506
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001496
Iteration 48/1000 | Loss: 0.00001495
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00001494
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001493
Iteration 56/1000 | Loss: 0.00001492
Iteration 57/1000 | Loss: 0.00001492
Iteration 58/1000 | Loss: 0.00001492
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001491
Iteration 61/1000 | Loss: 0.00001491
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001490
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001486
Iteration 79/1000 | Loss: 0.00001486
Iteration 80/1000 | Loss: 0.00001486
Iteration 81/1000 | Loss: 0.00001486
Iteration 82/1000 | Loss: 0.00001486
Iteration 83/1000 | Loss: 0.00001486
Iteration 84/1000 | Loss: 0.00001486
Iteration 85/1000 | Loss: 0.00001486
Iteration 86/1000 | Loss: 0.00001486
Iteration 87/1000 | Loss: 0.00001485
Iteration 88/1000 | Loss: 0.00001485
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001483
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001483
Iteration 94/1000 | Loss: 0.00001483
Iteration 95/1000 | Loss: 0.00001483
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001480
Iteration 107/1000 | Loss: 0.00001480
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001479
Iteration 113/1000 | Loss: 0.00001479
Iteration 114/1000 | Loss: 0.00001479
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001477
Iteration 119/1000 | Loss: 0.00001477
Iteration 120/1000 | Loss: 0.00001477
Iteration 121/1000 | Loss: 0.00001476
Iteration 122/1000 | Loss: 0.00001476
Iteration 123/1000 | Loss: 0.00001476
Iteration 124/1000 | Loss: 0.00001476
Iteration 125/1000 | Loss: 0.00001476
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001475
Iteration 133/1000 | Loss: 0.00001475
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001474
Iteration 138/1000 | Loss: 0.00001474
Iteration 139/1000 | Loss: 0.00001474
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001472
Iteration 145/1000 | Loss: 0.00001472
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001471
Iteration 151/1000 | Loss: 0.00001471
Iteration 152/1000 | Loss: 0.00001471
Iteration 153/1000 | Loss: 0.00001471
Iteration 154/1000 | Loss: 0.00001470
Iteration 155/1000 | Loss: 0.00001470
Iteration 156/1000 | Loss: 0.00001470
Iteration 157/1000 | Loss: 0.00001470
Iteration 158/1000 | Loss: 0.00001470
Iteration 159/1000 | Loss: 0.00001470
Iteration 160/1000 | Loss: 0.00001469
Iteration 161/1000 | Loss: 0.00001469
Iteration 162/1000 | Loss: 0.00001469
Iteration 163/1000 | Loss: 0.00001469
Iteration 164/1000 | Loss: 0.00001469
Iteration 165/1000 | Loss: 0.00001468
Iteration 166/1000 | Loss: 0.00001468
Iteration 167/1000 | Loss: 0.00001468
Iteration 168/1000 | Loss: 0.00001468
Iteration 169/1000 | Loss: 0.00001468
Iteration 170/1000 | Loss: 0.00001468
Iteration 171/1000 | Loss: 0.00001467
Iteration 172/1000 | Loss: 0.00001467
Iteration 173/1000 | Loss: 0.00001467
Iteration 174/1000 | Loss: 0.00001467
Iteration 175/1000 | Loss: 0.00001467
Iteration 176/1000 | Loss: 0.00001467
Iteration 177/1000 | Loss: 0.00001467
Iteration 178/1000 | Loss: 0.00001467
Iteration 179/1000 | Loss: 0.00001467
Iteration 180/1000 | Loss: 0.00001467
Iteration 181/1000 | Loss: 0.00001466
Iteration 182/1000 | Loss: 0.00001466
Iteration 183/1000 | Loss: 0.00001466
Iteration 184/1000 | Loss: 0.00001466
Iteration 185/1000 | Loss: 0.00001465
Iteration 186/1000 | Loss: 0.00001465
Iteration 187/1000 | Loss: 0.00001465
Iteration 188/1000 | Loss: 0.00001465
Iteration 189/1000 | Loss: 0.00001464
Iteration 190/1000 | Loss: 0.00001464
Iteration 191/1000 | Loss: 0.00001464
Iteration 192/1000 | Loss: 0.00001464
Iteration 193/1000 | Loss: 0.00001464
Iteration 194/1000 | Loss: 0.00001464
Iteration 195/1000 | Loss: 0.00001464
Iteration 196/1000 | Loss: 0.00001463
Iteration 197/1000 | Loss: 0.00001463
Iteration 198/1000 | Loss: 0.00001463
Iteration 199/1000 | Loss: 0.00001463
Iteration 200/1000 | Loss: 0.00001463
Iteration 201/1000 | Loss: 0.00001463
Iteration 202/1000 | Loss: 0.00001463
Iteration 203/1000 | Loss: 0.00001463
Iteration 204/1000 | Loss: 0.00001463
Iteration 205/1000 | Loss: 0.00001463
Iteration 206/1000 | Loss: 0.00001463
Iteration 207/1000 | Loss: 0.00001463
Iteration 208/1000 | Loss: 0.00001463
Iteration 209/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.4631695194111671e-05, 1.4631695194111671e-05, 1.4631695194111671e-05, 1.4631695194111671e-05, 1.4631695194111671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4631695194111671e-05

Optimization complete. Final v2v error: 3.1468851566314697 mm

Highest mean error: 4.459312915802002 mm for frame 81

Lowest mean error: 2.76240873336792 mm for frame 161

Saving results

Total time: 44.30419850349426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00421885
Iteration 2/25 | Loss: 0.00161323
Iteration 3/25 | Loss: 0.00134977
Iteration 4/25 | Loss: 0.00131101
Iteration 5/25 | Loss: 0.00130443
Iteration 6/25 | Loss: 0.00130285
Iteration 7/25 | Loss: 0.00130261
Iteration 8/25 | Loss: 0.00130261
Iteration 9/25 | Loss: 0.00130261
Iteration 10/25 | Loss: 0.00130261
Iteration 11/25 | Loss: 0.00130261
Iteration 12/25 | Loss: 0.00130261
Iteration 13/25 | Loss: 0.00130261
Iteration 14/25 | Loss: 0.00130261
Iteration 15/25 | Loss: 0.00130261
Iteration 16/25 | Loss: 0.00130261
Iteration 17/25 | Loss: 0.00130261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013026120141148567, 0.0013026120141148567, 0.0013026120141148567, 0.0013026120141148567, 0.0013026120141148567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013026120141148567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37102032
Iteration 2/25 | Loss: 0.00075894
Iteration 3/25 | Loss: 0.00075894
Iteration 4/25 | Loss: 0.00075894
Iteration 5/25 | Loss: 0.00075894
Iteration 6/25 | Loss: 0.00075894
Iteration 7/25 | Loss: 0.00075894
Iteration 8/25 | Loss: 0.00075894
Iteration 9/25 | Loss: 0.00075894
Iteration 10/25 | Loss: 0.00075894
Iteration 11/25 | Loss: 0.00075894
Iteration 12/25 | Loss: 0.00075894
Iteration 13/25 | Loss: 0.00075894
Iteration 14/25 | Loss: 0.00075894
Iteration 15/25 | Loss: 0.00075894
Iteration 16/25 | Loss: 0.00075894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007589365704916418, 0.0007589365704916418, 0.0007589365704916418, 0.0007589365704916418, 0.0007589365704916418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007589365704916418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075894
Iteration 2/1000 | Loss: 0.00004356
Iteration 3/1000 | Loss: 0.00002894
Iteration 4/1000 | Loss: 0.00002288
Iteration 5/1000 | Loss: 0.00002123
Iteration 6/1000 | Loss: 0.00002023
Iteration 7/1000 | Loss: 0.00001959
Iteration 8/1000 | Loss: 0.00001892
Iteration 9/1000 | Loss: 0.00001850
Iteration 10/1000 | Loss: 0.00001819
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001770
Iteration 14/1000 | Loss: 0.00001767
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001765
Iteration 17/1000 | Loss: 0.00001763
Iteration 18/1000 | Loss: 0.00001758
Iteration 19/1000 | Loss: 0.00001754
Iteration 20/1000 | Loss: 0.00001753
Iteration 21/1000 | Loss: 0.00001747
Iteration 22/1000 | Loss: 0.00001743
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001741
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001739
Iteration 27/1000 | Loss: 0.00001736
Iteration 28/1000 | Loss: 0.00001735
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001732
Iteration 33/1000 | Loss: 0.00001732
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001730
Iteration 37/1000 | Loss: 0.00001730
Iteration 38/1000 | Loss: 0.00001730
Iteration 39/1000 | Loss: 0.00001730
Iteration 40/1000 | Loss: 0.00001730
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001730
Iteration 44/1000 | Loss: 0.00001729
Iteration 45/1000 | Loss: 0.00001729
Iteration 46/1000 | Loss: 0.00001728
Iteration 47/1000 | Loss: 0.00001728
Iteration 48/1000 | Loss: 0.00001728
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001726
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001725
Iteration 56/1000 | Loss: 0.00001725
Iteration 57/1000 | Loss: 0.00001725
Iteration 58/1000 | Loss: 0.00001724
Iteration 59/1000 | Loss: 0.00001724
Iteration 60/1000 | Loss: 0.00001724
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001722
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001717
Iteration 74/1000 | Loss: 0.00001717
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001716
Iteration 77/1000 | Loss: 0.00001715
Iteration 78/1000 | Loss: 0.00001715
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001714
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001713
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001713
Iteration 87/1000 | Loss: 0.00001712
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001712
Iteration 90/1000 | Loss: 0.00001711
Iteration 91/1000 | Loss: 0.00001711
Iteration 92/1000 | Loss: 0.00001711
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001710
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001708
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001707
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001705
Iteration 123/1000 | Loss: 0.00001705
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001705
Iteration 126/1000 | Loss: 0.00001705
Iteration 127/1000 | Loss: 0.00001705
Iteration 128/1000 | Loss: 0.00001705
Iteration 129/1000 | Loss: 0.00001705
Iteration 130/1000 | Loss: 0.00001705
Iteration 131/1000 | Loss: 0.00001705
Iteration 132/1000 | Loss: 0.00001704
Iteration 133/1000 | Loss: 0.00001704
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001703
Iteration 141/1000 | Loss: 0.00001703
Iteration 142/1000 | Loss: 0.00001703
Iteration 143/1000 | Loss: 0.00001703
Iteration 144/1000 | Loss: 0.00001703
Iteration 145/1000 | Loss: 0.00001703
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001702
Iteration 148/1000 | Loss: 0.00001702
Iteration 149/1000 | Loss: 0.00001702
Iteration 150/1000 | Loss: 0.00001702
Iteration 151/1000 | Loss: 0.00001702
Iteration 152/1000 | Loss: 0.00001702
Iteration 153/1000 | Loss: 0.00001702
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001700
Iteration 160/1000 | Loss: 0.00001700
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001700
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001700
Iteration 168/1000 | Loss: 0.00001700
Iteration 169/1000 | Loss: 0.00001700
Iteration 170/1000 | Loss: 0.00001700
Iteration 171/1000 | Loss: 0.00001700
Iteration 172/1000 | Loss: 0.00001700
Iteration 173/1000 | Loss: 0.00001699
Iteration 174/1000 | Loss: 0.00001699
Iteration 175/1000 | Loss: 0.00001699
Iteration 176/1000 | Loss: 0.00001699
Iteration 177/1000 | Loss: 0.00001699
Iteration 178/1000 | Loss: 0.00001699
Iteration 179/1000 | Loss: 0.00001699
Iteration 180/1000 | Loss: 0.00001699
Iteration 181/1000 | Loss: 0.00001699
Iteration 182/1000 | Loss: 0.00001698
Iteration 183/1000 | Loss: 0.00001698
Iteration 184/1000 | Loss: 0.00001698
Iteration 185/1000 | Loss: 0.00001698
Iteration 186/1000 | Loss: 0.00001698
Iteration 187/1000 | Loss: 0.00001698
Iteration 188/1000 | Loss: 0.00001698
Iteration 189/1000 | Loss: 0.00001698
Iteration 190/1000 | Loss: 0.00001698
Iteration 191/1000 | Loss: 0.00001698
Iteration 192/1000 | Loss: 0.00001698
Iteration 193/1000 | Loss: 0.00001698
Iteration 194/1000 | Loss: 0.00001698
Iteration 195/1000 | Loss: 0.00001697
Iteration 196/1000 | Loss: 0.00001697
Iteration 197/1000 | Loss: 0.00001697
Iteration 198/1000 | Loss: 0.00001697
Iteration 199/1000 | Loss: 0.00001697
Iteration 200/1000 | Loss: 0.00001697
Iteration 201/1000 | Loss: 0.00001697
Iteration 202/1000 | Loss: 0.00001697
Iteration 203/1000 | Loss: 0.00001697
Iteration 204/1000 | Loss: 0.00001697
Iteration 205/1000 | Loss: 0.00001697
Iteration 206/1000 | Loss: 0.00001697
Iteration 207/1000 | Loss: 0.00001697
Iteration 208/1000 | Loss: 0.00001697
Iteration 209/1000 | Loss: 0.00001697
Iteration 210/1000 | Loss: 0.00001697
Iteration 211/1000 | Loss: 0.00001697
Iteration 212/1000 | Loss: 0.00001697
Iteration 213/1000 | Loss: 0.00001697
Iteration 214/1000 | Loss: 0.00001697
Iteration 215/1000 | Loss: 0.00001697
Iteration 216/1000 | Loss: 0.00001697
Iteration 217/1000 | Loss: 0.00001697
Iteration 218/1000 | Loss: 0.00001697
Iteration 219/1000 | Loss: 0.00001697
Iteration 220/1000 | Loss: 0.00001697
Iteration 221/1000 | Loss: 0.00001697
Iteration 222/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.6967554984148592e-05, 1.6967554984148592e-05, 1.6967554984148592e-05, 1.6967554984148592e-05, 1.6967554984148592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6967554984148592e-05

Optimization complete. Final v2v error: 3.489328622817993 mm

Highest mean error: 4.211987018585205 mm for frame 72

Lowest mean error: 3.0137109756469727 mm for frame 13

Saving results

Total time: 43.41437578201294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970594
Iteration 2/25 | Loss: 0.00183000
Iteration 3/25 | Loss: 0.00152331
Iteration 4/25 | Loss: 0.00150362
Iteration 5/25 | Loss: 0.00149685
Iteration 6/25 | Loss: 0.00149583
Iteration 7/25 | Loss: 0.00149583
Iteration 8/25 | Loss: 0.00149583
Iteration 9/25 | Loss: 0.00149583
Iteration 10/25 | Loss: 0.00149583
Iteration 11/25 | Loss: 0.00149583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014958271058276296, 0.0014958271058276296, 0.0014958271058276296, 0.0014958271058276296, 0.0014958271058276296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014958271058276296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82871562
Iteration 2/25 | Loss: 0.00114370
Iteration 3/25 | Loss: 0.00114369
Iteration 4/25 | Loss: 0.00114368
Iteration 5/25 | Loss: 0.00114368
Iteration 6/25 | Loss: 0.00114368
Iteration 7/25 | Loss: 0.00114368
Iteration 8/25 | Loss: 0.00114368
Iteration 9/25 | Loss: 0.00114368
Iteration 10/25 | Loss: 0.00114368
Iteration 11/25 | Loss: 0.00114368
Iteration 12/25 | Loss: 0.00114368
Iteration 13/25 | Loss: 0.00114368
Iteration 14/25 | Loss: 0.00114368
Iteration 15/25 | Loss: 0.00114368
Iteration 16/25 | Loss: 0.00114368
Iteration 17/25 | Loss: 0.00114368
Iteration 18/25 | Loss: 0.00114368
Iteration 19/25 | Loss: 0.00114368
Iteration 20/25 | Loss: 0.00114368
Iteration 21/25 | Loss: 0.00114368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011436814675107598, 0.0011436814675107598, 0.0011436814675107598, 0.0011436814675107598, 0.0011436814675107598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011436814675107598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114368
Iteration 2/1000 | Loss: 0.00007449
Iteration 3/1000 | Loss: 0.00005185
Iteration 4/1000 | Loss: 0.00004139
Iteration 5/1000 | Loss: 0.00003901
Iteration 6/1000 | Loss: 0.00003772
Iteration 7/1000 | Loss: 0.00003687
Iteration 8/1000 | Loss: 0.00003605
Iteration 9/1000 | Loss: 0.00003540
Iteration 10/1000 | Loss: 0.00003504
Iteration 11/1000 | Loss: 0.00003457
Iteration 12/1000 | Loss: 0.00003427
Iteration 13/1000 | Loss: 0.00003398
Iteration 14/1000 | Loss: 0.00003367
Iteration 15/1000 | Loss: 0.00003340
Iteration 16/1000 | Loss: 0.00003314
Iteration 17/1000 | Loss: 0.00003289
Iteration 18/1000 | Loss: 0.00003264
Iteration 19/1000 | Loss: 0.00003246
Iteration 20/1000 | Loss: 0.00003235
Iteration 21/1000 | Loss: 0.00003230
Iteration 22/1000 | Loss: 0.00003230
Iteration 23/1000 | Loss: 0.00003220
Iteration 24/1000 | Loss: 0.00003219
Iteration 25/1000 | Loss: 0.00003219
Iteration 26/1000 | Loss: 0.00003218
Iteration 27/1000 | Loss: 0.00003214
Iteration 28/1000 | Loss: 0.00003211
Iteration 29/1000 | Loss: 0.00003211
Iteration 30/1000 | Loss: 0.00003210
Iteration 31/1000 | Loss: 0.00003210
Iteration 32/1000 | Loss: 0.00003209
Iteration 33/1000 | Loss: 0.00003209
Iteration 34/1000 | Loss: 0.00003209
Iteration 35/1000 | Loss: 0.00003208
Iteration 36/1000 | Loss: 0.00003207
Iteration 37/1000 | Loss: 0.00003207
Iteration 38/1000 | Loss: 0.00003207
Iteration 39/1000 | Loss: 0.00003207
Iteration 40/1000 | Loss: 0.00003207
Iteration 41/1000 | Loss: 0.00003207
Iteration 42/1000 | Loss: 0.00003206
Iteration 43/1000 | Loss: 0.00003206
Iteration 44/1000 | Loss: 0.00003205
Iteration 45/1000 | Loss: 0.00003205
Iteration 46/1000 | Loss: 0.00003205
Iteration 47/1000 | Loss: 0.00003204
Iteration 48/1000 | Loss: 0.00003204
Iteration 49/1000 | Loss: 0.00003204
Iteration 50/1000 | Loss: 0.00003204
Iteration 51/1000 | Loss: 0.00003203
Iteration 52/1000 | Loss: 0.00003203
Iteration 53/1000 | Loss: 0.00003202
Iteration 54/1000 | Loss: 0.00003202
Iteration 55/1000 | Loss: 0.00003202
Iteration 56/1000 | Loss: 0.00003202
Iteration 57/1000 | Loss: 0.00003202
Iteration 58/1000 | Loss: 0.00003202
Iteration 59/1000 | Loss: 0.00003202
Iteration 60/1000 | Loss: 0.00003201
Iteration 61/1000 | Loss: 0.00003200
Iteration 62/1000 | Loss: 0.00003200
Iteration 63/1000 | Loss: 0.00003200
Iteration 64/1000 | Loss: 0.00003200
Iteration 65/1000 | Loss: 0.00003200
Iteration 66/1000 | Loss: 0.00003200
Iteration 67/1000 | Loss: 0.00003200
Iteration 68/1000 | Loss: 0.00003200
Iteration 69/1000 | Loss: 0.00003200
Iteration 70/1000 | Loss: 0.00003199
Iteration 71/1000 | Loss: 0.00003199
Iteration 72/1000 | Loss: 0.00003199
Iteration 73/1000 | Loss: 0.00003198
Iteration 74/1000 | Loss: 0.00003198
Iteration 75/1000 | Loss: 0.00003198
Iteration 76/1000 | Loss: 0.00003198
Iteration 77/1000 | Loss: 0.00003197
Iteration 78/1000 | Loss: 0.00003197
Iteration 79/1000 | Loss: 0.00003197
Iteration 80/1000 | Loss: 0.00003197
Iteration 81/1000 | Loss: 0.00003197
Iteration 82/1000 | Loss: 0.00003197
Iteration 83/1000 | Loss: 0.00003197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [3.197414116584696e-05, 3.197414116584696e-05, 3.197414116584696e-05, 3.197414116584696e-05, 3.197414116584696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.197414116584696e-05

Optimization complete. Final v2v error: 4.6696858406066895 mm

Highest mean error: 5.427870273590088 mm for frame 78

Lowest mean error: 3.7478697299957275 mm for frame 27

Saving results

Total time: 44.7379105091095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042323
Iteration 2/25 | Loss: 0.00158732
Iteration 3/25 | Loss: 0.00135332
Iteration 4/25 | Loss: 0.00131802
Iteration 5/25 | Loss: 0.00132762
Iteration 6/25 | Loss: 0.00131460
Iteration 7/25 | Loss: 0.00132245
Iteration 8/25 | Loss: 0.00132489
Iteration 9/25 | Loss: 0.00133479
Iteration 10/25 | Loss: 0.00132533
Iteration 11/25 | Loss: 0.00131923
Iteration 12/25 | Loss: 0.00132549
Iteration 13/25 | Loss: 0.00132950
Iteration 14/25 | Loss: 0.00132507
Iteration 15/25 | Loss: 0.00132827
Iteration 16/25 | Loss: 0.00132359
Iteration 17/25 | Loss: 0.00132022
Iteration 18/25 | Loss: 0.00132063
Iteration 19/25 | Loss: 0.00131839
Iteration 20/25 | Loss: 0.00131751
Iteration 21/25 | Loss: 0.00131755
Iteration 22/25 | Loss: 0.00131719
Iteration 23/25 | Loss: 0.00131666
Iteration 24/25 | Loss: 0.00131689
Iteration 25/25 | Loss: 0.00131263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16182041
Iteration 2/25 | Loss: 0.00094000
Iteration 3/25 | Loss: 0.00093999
Iteration 4/25 | Loss: 0.00093999
Iteration 5/25 | Loss: 0.00093999
Iteration 6/25 | Loss: 0.00093999
Iteration 7/25 | Loss: 0.00093999
Iteration 8/25 | Loss: 0.00093999
Iteration 9/25 | Loss: 0.00093999
Iteration 10/25 | Loss: 0.00093999
Iteration 11/25 | Loss: 0.00093999
Iteration 12/25 | Loss: 0.00093999
Iteration 13/25 | Loss: 0.00093999
Iteration 14/25 | Loss: 0.00093999
Iteration 15/25 | Loss: 0.00093999
Iteration 16/25 | Loss: 0.00093999
Iteration 17/25 | Loss: 0.00093999
Iteration 18/25 | Loss: 0.00093999
Iteration 19/25 | Loss: 0.00093999
Iteration 20/25 | Loss: 0.00093999
Iteration 21/25 | Loss: 0.00093999
Iteration 22/25 | Loss: 0.00093999
Iteration 23/25 | Loss: 0.00093999
Iteration 24/25 | Loss: 0.00093999
Iteration 25/25 | Loss: 0.00093999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093999
Iteration 2/1000 | Loss: 0.00041816
Iteration 3/1000 | Loss: 0.00024208
Iteration 4/1000 | Loss: 0.00020201
Iteration 5/1000 | Loss: 0.00022490
Iteration 6/1000 | Loss: 0.00011598
Iteration 7/1000 | Loss: 0.00034579
Iteration 8/1000 | Loss: 0.00034093
Iteration 9/1000 | Loss: 0.00035231
Iteration 10/1000 | Loss: 0.00027287
Iteration 11/1000 | Loss: 0.00026511
Iteration 12/1000 | Loss: 0.00028639
Iteration 13/1000 | Loss: 0.00034185
Iteration 14/1000 | Loss: 0.00045232
Iteration 15/1000 | Loss: 0.00026316
Iteration 16/1000 | Loss: 0.00030171
Iteration 17/1000 | Loss: 0.00030749
Iteration 18/1000 | Loss: 0.00024298
Iteration 19/1000 | Loss: 0.00034375
Iteration 20/1000 | Loss: 0.00035703
Iteration 21/1000 | Loss: 0.00021545
Iteration 22/1000 | Loss: 0.00030067
Iteration 23/1000 | Loss: 0.00005815
Iteration 24/1000 | Loss: 0.00023921
Iteration 25/1000 | Loss: 0.00026434
Iteration 26/1000 | Loss: 0.00031897
Iteration 27/1000 | Loss: 0.00025489
Iteration 28/1000 | Loss: 0.00007788
Iteration 29/1000 | Loss: 0.00031307
Iteration 30/1000 | Loss: 0.00012319
Iteration 31/1000 | Loss: 0.00019901
Iteration 32/1000 | Loss: 0.00016000
Iteration 33/1000 | Loss: 0.00003923
Iteration 34/1000 | Loss: 0.00002832
Iteration 35/1000 | Loss: 0.00002594
Iteration 36/1000 | Loss: 0.00002536
Iteration 37/1000 | Loss: 0.00005027
Iteration 38/1000 | Loss: 0.00002310
Iteration 39/1000 | Loss: 0.00007126
Iteration 40/1000 | Loss: 0.00009377
Iteration 41/1000 | Loss: 0.00006655
Iteration 42/1000 | Loss: 0.00010039
Iteration 43/1000 | Loss: 0.00007241
Iteration 44/1000 | Loss: 0.00005271
Iteration 45/1000 | Loss: 0.00008293
Iteration 46/1000 | Loss: 0.00010062
Iteration 47/1000 | Loss: 0.00015529
Iteration 48/1000 | Loss: 0.00013962
Iteration 49/1000 | Loss: 0.00013303
Iteration 50/1000 | Loss: 0.00008268
Iteration 51/1000 | Loss: 0.00004445
Iteration 52/1000 | Loss: 0.00007053
Iteration 53/1000 | Loss: 0.00018850
Iteration 54/1000 | Loss: 0.00014767
Iteration 55/1000 | Loss: 0.00012491
Iteration 56/1000 | Loss: 0.00012936
Iteration 57/1000 | Loss: 0.00012390
Iteration 58/1000 | Loss: 0.00006858
Iteration 59/1000 | Loss: 0.00009945
Iteration 60/1000 | Loss: 0.00004809
Iteration 61/1000 | Loss: 0.00010900
Iteration 62/1000 | Loss: 0.00011019
Iteration 63/1000 | Loss: 0.00010258
Iteration 64/1000 | Loss: 0.00011852
Iteration 65/1000 | Loss: 0.00007129
Iteration 66/1000 | Loss: 0.00009410
Iteration 67/1000 | Loss: 0.00009199
Iteration 68/1000 | Loss: 0.00004453
Iteration 69/1000 | Loss: 0.00004334
Iteration 70/1000 | Loss: 0.00011261
Iteration 71/1000 | Loss: 0.00011229
Iteration 72/1000 | Loss: 0.00006448
Iteration 73/1000 | Loss: 0.00007159
Iteration 74/1000 | Loss: 0.00011782
Iteration 75/1000 | Loss: 0.00007657
Iteration 76/1000 | Loss: 0.00006596
Iteration 77/1000 | Loss: 0.00007387
Iteration 78/1000 | Loss: 0.00007496
Iteration 79/1000 | Loss: 0.00007270
Iteration 80/1000 | Loss: 0.00006994
Iteration 81/1000 | Loss: 0.00009697
Iteration 82/1000 | Loss: 0.00024094
Iteration 83/1000 | Loss: 0.00008461
Iteration 84/1000 | Loss: 0.00009049
Iteration 85/1000 | Loss: 0.00007402
Iteration 86/1000 | Loss: 0.00008986
Iteration 87/1000 | Loss: 0.00010172
Iteration 88/1000 | Loss: 0.00006738
Iteration 89/1000 | Loss: 0.00009228
Iteration 90/1000 | Loss: 0.00006568
Iteration 91/1000 | Loss: 0.00012094
Iteration 92/1000 | Loss: 0.00007843
Iteration 93/1000 | Loss: 0.00003749
Iteration 94/1000 | Loss: 0.00007522
Iteration 95/1000 | Loss: 0.00007345
Iteration 96/1000 | Loss: 0.00010382
Iteration 97/1000 | Loss: 0.00006425
Iteration 98/1000 | Loss: 0.00015545
Iteration 99/1000 | Loss: 0.00011952
Iteration 100/1000 | Loss: 0.00003927
Iteration 101/1000 | Loss: 0.00008188
Iteration 102/1000 | Loss: 0.00003247
Iteration 103/1000 | Loss: 0.00005206
Iteration 104/1000 | Loss: 0.00003213
Iteration 105/1000 | Loss: 0.00004319
Iteration 106/1000 | Loss: 0.00004454
Iteration 107/1000 | Loss: 0.00011790
Iteration 108/1000 | Loss: 0.00006594
Iteration 109/1000 | Loss: 0.00004525
Iteration 110/1000 | Loss: 0.00010613
Iteration 111/1000 | Loss: 0.00006713
Iteration 112/1000 | Loss: 0.00009786
Iteration 113/1000 | Loss: 0.00008155
Iteration 114/1000 | Loss: 0.00008332
Iteration 115/1000 | Loss: 0.00008313
Iteration 116/1000 | Loss: 0.00002492
Iteration 117/1000 | Loss: 0.00014877
Iteration 118/1000 | Loss: 0.00001795
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001546
Iteration 123/1000 | Loss: 0.00001533
Iteration 124/1000 | Loss: 0.00001531
Iteration 125/1000 | Loss: 0.00001531
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00002839
Iteration 128/1000 | Loss: 0.00011784
Iteration 129/1000 | Loss: 0.00011980
Iteration 130/1000 | Loss: 0.00002654
Iteration 131/1000 | Loss: 0.00008126
Iteration 132/1000 | Loss: 0.00009391
Iteration 133/1000 | Loss: 0.00007831
Iteration 134/1000 | Loss: 0.00005482
Iteration 135/1000 | Loss: 0.00012810
Iteration 136/1000 | Loss: 0.00008075
Iteration 137/1000 | Loss: 0.00003363
Iteration 138/1000 | Loss: 0.00009312
Iteration 139/1000 | Loss: 0.00010818
Iteration 140/1000 | Loss: 0.00011471
Iteration 141/1000 | Loss: 0.00008168
Iteration 142/1000 | Loss: 0.00003170
Iteration 143/1000 | Loss: 0.00008161
Iteration 144/1000 | Loss: 0.00002176
Iteration 145/1000 | Loss: 0.00016748
Iteration 146/1000 | Loss: 0.00007067
Iteration 147/1000 | Loss: 0.00007007
Iteration 148/1000 | Loss: 0.00008485
Iteration 149/1000 | Loss: 0.00007494
Iteration 150/1000 | Loss: 0.00009085
Iteration 151/1000 | Loss: 0.00010921
Iteration 152/1000 | Loss: 0.00007041
Iteration 153/1000 | Loss: 0.00008762
Iteration 154/1000 | Loss: 0.00010727
Iteration 155/1000 | Loss: 0.00013437
Iteration 156/1000 | Loss: 0.00006250
Iteration 157/1000 | Loss: 0.00010183
Iteration 158/1000 | Loss: 0.00007156
Iteration 159/1000 | Loss: 0.00008592
Iteration 160/1000 | Loss: 0.00006617
Iteration 161/1000 | Loss: 0.00010127
Iteration 162/1000 | Loss: 0.00006271
Iteration 163/1000 | Loss: 0.00006766
Iteration 164/1000 | Loss: 0.00003586
Iteration 165/1000 | Loss: 0.00008492
Iteration 166/1000 | Loss: 0.00010474
Iteration 167/1000 | Loss: 0.00004823
Iteration 168/1000 | Loss: 0.00009132
Iteration 169/1000 | Loss: 0.00003501
Iteration 170/1000 | Loss: 0.00007299
Iteration 171/1000 | Loss: 0.00003553
Iteration 172/1000 | Loss: 0.00002726
Iteration 173/1000 | Loss: 0.00007580
Iteration 174/1000 | Loss: 0.00011386
Iteration 175/1000 | Loss: 0.00011714
Iteration 176/1000 | Loss: 0.00009848
Iteration 177/1000 | Loss: 0.00008862
Iteration 178/1000 | Loss: 0.00003735
Iteration 179/1000 | Loss: 0.00004489
Iteration 180/1000 | Loss: 0.00003997
Iteration 181/1000 | Loss: 0.00005313
Iteration 182/1000 | Loss: 0.00007941
Iteration 183/1000 | Loss: 0.00005835
Iteration 184/1000 | Loss: 0.00006804
Iteration 185/1000 | Loss: 0.00009967
Iteration 186/1000 | Loss: 0.00002904
Iteration 187/1000 | Loss: 0.00006254
Iteration 188/1000 | Loss: 0.00006213
Iteration 189/1000 | Loss: 0.00012039
Iteration 190/1000 | Loss: 0.00011886
Iteration 191/1000 | Loss: 0.00005746
Iteration 192/1000 | Loss: 0.00004172
Iteration 193/1000 | Loss: 0.00003680
Iteration 194/1000 | Loss: 0.00003301
Iteration 195/1000 | Loss: 0.00008020
Iteration 196/1000 | Loss: 0.00005835
Iteration 197/1000 | Loss: 0.00008631
Iteration 198/1000 | Loss: 0.00013255
Iteration 199/1000 | Loss: 0.00005712
Iteration 200/1000 | Loss: 0.00008904
Iteration 201/1000 | Loss: 0.00008128
Iteration 202/1000 | Loss: 0.00009354
Iteration 203/1000 | Loss: 0.00004890
Iteration 204/1000 | Loss: 0.00011776
Iteration 205/1000 | Loss: 0.00009278
Iteration 206/1000 | Loss: 0.00003575
Iteration 207/1000 | Loss: 0.00001757
Iteration 208/1000 | Loss: 0.00009147
Iteration 209/1000 | Loss: 0.00009797
Iteration 210/1000 | Loss: 0.00002993
Iteration 211/1000 | Loss: 0.00011008
Iteration 212/1000 | Loss: 0.00002964
Iteration 213/1000 | Loss: 0.00014310
Iteration 214/1000 | Loss: 0.00010411
Iteration 215/1000 | Loss: 0.00007168
Iteration 216/1000 | Loss: 0.00010003
Iteration 217/1000 | Loss: 0.00007224
Iteration 218/1000 | Loss: 0.00009511
Iteration 219/1000 | Loss: 0.00011141
Iteration 220/1000 | Loss: 0.00009894
Iteration 221/1000 | Loss: 0.00002173
Iteration 222/1000 | Loss: 0.00004366
Iteration 223/1000 | Loss: 0.00006485
Iteration 224/1000 | Loss: 0.00010177
Iteration 225/1000 | Loss: 0.00009389
Iteration 226/1000 | Loss: 0.00008654
Iteration 227/1000 | Loss: 0.00011311
Iteration 228/1000 | Loss: 0.00008325
Iteration 229/1000 | Loss: 0.00008351
Iteration 230/1000 | Loss: 0.00029263
Iteration 231/1000 | Loss: 0.00038813
Iteration 232/1000 | Loss: 0.00014641
Iteration 233/1000 | Loss: 0.00006967
Iteration 234/1000 | Loss: 0.00004160
Iteration 235/1000 | Loss: 0.00005266
Iteration 236/1000 | Loss: 0.00003920
Iteration 237/1000 | Loss: 0.00001735
Iteration 238/1000 | Loss: 0.00001706
Iteration 239/1000 | Loss: 0.00003637
Iteration 240/1000 | Loss: 0.00002241
Iteration 241/1000 | Loss: 0.00001694
Iteration 242/1000 | Loss: 0.00001628
Iteration 243/1000 | Loss: 0.00004522
Iteration 244/1000 | Loss: 0.00009204
Iteration 245/1000 | Loss: 0.00009664
Iteration 246/1000 | Loss: 0.00009133
Iteration 247/1000 | Loss: 0.00008680
Iteration 248/1000 | Loss: 0.00007151
Iteration 249/1000 | Loss: 0.00005906
Iteration 250/1000 | Loss: 0.00011945
Iteration 251/1000 | Loss: 0.00007880
Iteration 252/1000 | Loss: 0.00008988
Iteration 253/1000 | Loss: 0.00028796
Iteration 254/1000 | Loss: 0.00010904
Iteration 255/1000 | Loss: 0.00006442
Iteration 256/1000 | Loss: 0.00008711
Iteration 257/1000 | Loss: 0.00009199
Iteration 258/1000 | Loss: 0.00006879
Iteration 259/1000 | Loss: 0.00008062
Iteration 260/1000 | Loss: 0.00006620
Iteration 261/1000 | Loss: 0.00008013
Iteration 262/1000 | Loss: 0.00013440
Iteration 263/1000 | Loss: 0.00009013
Iteration 264/1000 | Loss: 0.00009596
Iteration 265/1000 | Loss: 0.00008919
Iteration 266/1000 | Loss: 0.00010465
Iteration 267/1000 | Loss: 0.00007461
Iteration 268/1000 | Loss: 0.00009820
Iteration 269/1000 | Loss: 0.00009103
Iteration 270/1000 | Loss: 0.00005402
Iteration 271/1000 | Loss: 0.00006327
Iteration 272/1000 | Loss: 0.00015838
Iteration 273/1000 | Loss: 0.00006512
Iteration 274/1000 | Loss: 0.00008763
Iteration 275/1000 | Loss: 0.00010151
Iteration 276/1000 | Loss: 0.00008376
Iteration 277/1000 | Loss: 0.00007290
Iteration 278/1000 | Loss: 0.00002496
Iteration 279/1000 | Loss: 0.00002091
Iteration 280/1000 | Loss: 0.00002225
Iteration 281/1000 | Loss: 0.00002902
Iteration 282/1000 | Loss: 0.00001422
Iteration 283/1000 | Loss: 0.00002180
Iteration 284/1000 | Loss: 0.00011383
Iteration 285/1000 | Loss: 0.00008755
Iteration 286/1000 | Loss: 0.00001513
Iteration 287/1000 | Loss: 0.00009137
Iteration 288/1000 | Loss: 0.00018194
Iteration 289/1000 | Loss: 0.00008809
Iteration 290/1000 | Loss: 0.00009347
Iteration 291/1000 | Loss: 0.00011298
Iteration 292/1000 | Loss: 0.00006894
Iteration 293/1000 | Loss: 0.00014775
Iteration 294/1000 | Loss: 0.00009484
Iteration 295/1000 | Loss: 0.00010364
Iteration 296/1000 | Loss: 0.00009328
Iteration 297/1000 | Loss: 0.00002946
Iteration 298/1000 | Loss: 0.00011448
Iteration 299/1000 | Loss: 0.00010152
Iteration 300/1000 | Loss: 0.00008653
Iteration 301/1000 | Loss: 0.00007232
Iteration 302/1000 | Loss: 0.00009565
Iteration 303/1000 | Loss: 0.00007059
Iteration 304/1000 | Loss: 0.00006685
Iteration 305/1000 | Loss: 0.00008088
Iteration 306/1000 | Loss: 0.00007043
Iteration 307/1000 | Loss: 0.00005937
Iteration 308/1000 | Loss: 0.00007996
Iteration 309/1000 | Loss: 0.00005314
Iteration 310/1000 | Loss: 0.00008422
Iteration 311/1000 | Loss: 0.00015119
Iteration 312/1000 | Loss: 0.00002055
Iteration 313/1000 | Loss: 0.00001561
Iteration 314/1000 | Loss: 0.00001472
Iteration 315/1000 | Loss: 0.00001405
Iteration 316/1000 | Loss: 0.00001358
Iteration 317/1000 | Loss: 0.00001315
Iteration 318/1000 | Loss: 0.00001299
Iteration 319/1000 | Loss: 0.00001297
Iteration 320/1000 | Loss: 0.00001297
Iteration 321/1000 | Loss: 0.00001295
Iteration 322/1000 | Loss: 0.00001287
Iteration 323/1000 | Loss: 0.00001286
Iteration 324/1000 | Loss: 0.00001286
Iteration 325/1000 | Loss: 0.00001286
Iteration 326/1000 | Loss: 0.00001286
Iteration 327/1000 | Loss: 0.00001285
Iteration 328/1000 | Loss: 0.00001285
Iteration 329/1000 | Loss: 0.00001284
Iteration 330/1000 | Loss: 0.00001284
Iteration 331/1000 | Loss: 0.00001284
Iteration 332/1000 | Loss: 0.00001284
Iteration 333/1000 | Loss: 0.00001284
Iteration 334/1000 | Loss: 0.00001284
Iteration 335/1000 | Loss: 0.00001284
Iteration 336/1000 | Loss: 0.00001284
Iteration 337/1000 | Loss: 0.00001283
Iteration 338/1000 | Loss: 0.00001283
Iteration 339/1000 | Loss: 0.00001283
Iteration 340/1000 | Loss: 0.00001283
Iteration 341/1000 | Loss: 0.00001283
Iteration 342/1000 | Loss: 0.00001282
Iteration 343/1000 | Loss: 0.00001282
Iteration 344/1000 | Loss: 0.00001282
Iteration 345/1000 | Loss: 0.00001282
Iteration 346/1000 | Loss: 0.00001282
Iteration 347/1000 | Loss: 0.00001281
Iteration 348/1000 | Loss: 0.00001281
Iteration 349/1000 | Loss: 0.00001281
Iteration 350/1000 | Loss: 0.00001281
Iteration 351/1000 | Loss: 0.00001281
Iteration 352/1000 | Loss: 0.00001281
Iteration 353/1000 | Loss: 0.00001281
Iteration 354/1000 | Loss: 0.00001281
Iteration 355/1000 | Loss: 0.00001281
Iteration 356/1000 | Loss: 0.00001280
Iteration 357/1000 | Loss: 0.00001280
Iteration 358/1000 | Loss: 0.00001280
Iteration 359/1000 | Loss: 0.00001280
Iteration 360/1000 | Loss: 0.00001279
Iteration 361/1000 | Loss: 0.00001279
Iteration 362/1000 | Loss: 0.00001279
Iteration 363/1000 | Loss: 0.00001279
Iteration 364/1000 | Loss: 0.00001279
Iteration 365/1000 | Loss: 0.00001279
Iteration 366/1000 | Loss: 0.00001279
Iteration 367/1000 | Loss: 0.00001279
Iteration 368/1000 | Loss: 0.00001279
Iteration 369/1000 | Loss: 0.00001279
Iteration 370/1000 | Loss: 0.00001279
Iteration 371/1000 | Loss: 0.00001279
Iteration 372/1000 | Loss: 0.00001279
Iteration 373/1000 | Loss: 0.00001279
Iteration 374/1000 | Loss: 0.00001279
Iteration 375/1000 | Loss: 0.00001279
Iteration 376/1000 | Loss: 0.00001279
Iteration 377/1000 | Loss: 0.00001279
Iteration 378/1000 | Loss: 0.00001279
Iteration 379/1000 | Loss: 0.00001279
Iteration 380/1000 | Loss: 0.00001279
Iteration 381/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 381. Stopping optimization.
Last 5 losses: [1.2788678759534378e-05, 1.2788678759534378e-05, 1.2788678759534378e-05, 1.2788678759534378e-05, 1.2788678759534378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2788678759534378e-05

Optimization complete. Final v2v error: 3.0503289699554443 mm

Highest mean error: 4.593153953552246 mm for frame 108

Lowest mean error: 2.880466938018799 mm for frame 255

Saving results

Total time: 571.6191647052765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583806
Iteration 2/25 | Loss: 0.00161056
Iteration 3/25 | Loss: 0.00138361
Iteration 4/25 | Loss: 0.00135667
Iteration 5/25 | Loss: 0.00135063
Iteration 6/25 | Loss: 0.00134868
Iteration 7/25 | Loss: 0.00134813
Iteration 8/25 | Loss: 0.00134784
Iteration 9/25 | Loss: 0.00134767
Iteration 10/25 | Loss: 0.00134753
Iteration 11/25 | Loss: 0.00134738
Iteration 12/25 | Loss: 0.00134704
Iteration 13/25 | Loss: 0.00134664
Iteration 14/25 | Loss: 0.00134642
Iteration 15/25 | Loss: 0.00134633
Iteration 16/25 | Loss: 0.00134632
Iteration 17/25 | Loss: 0.00134632
Iteration 18/25 | Loss: 0.00134632
Iteration 19/25 | Loss: 0.00134632
Iteration 20/25 | Loss: 0.00134632
Iteration 21/25 | Loss: 0.00134632
Iteration 22/25 | Loss: 0.00134632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001346319681033492, 0.001346319681033492, 0.001346319681033492, 0.001346319681033492, 0.001346319681033492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001346319681033492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07989621
Iteration 2/25 | Loss: 0.00094459
Iteration 3/25 | Loss: 0.00094455
Iteration 4/25 | Loss: 0.00094455
Iteration 5/25 | Loss: 0.00094455
Iteration 6/25 | Loss: 0.00094455
Iteration 7/25 | Loss: 0.00094455
Iteration 8/25 | Loss: 0.00094455
Iteration 9/25 | Loss: 0.00094455
Iteration 10/25 | Loss: 0.00094455
Iteration 11/25 | Loss: 0.00094455
Iteration 12/25 | Loss: 0.00094455
Iteration 13/25 | Loss: 0.00094455
Iteration 14/25 | Loss: 0.00094455
Iteration 15/25 | Loss: 0.00094455
Iteration 16/25 | Loss: 0.00094455
Iteration 17/25 | Loss: 0.00094455
Iteration 18/25 | Loss: 0.00094455
Iteration 19/25 | Loss: 0.00094455
Iteration 20/25 | Loss: 0.00094455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009445491014048457, 0.0009445491014048457, 0.0009445491014048457, 0.0009445491014048457, 0.0009445491014048457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009445491014048457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094455
Iteration 2/1000 | Loss: 0.00004429
Iteration 3/1000 | Loss: 0.00002729
Iteration 4/1000 | Loss: 0.00002176
Iteration 5/1000 | Loss: 0.00002028
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001804
Iteration 9/1000 | Loss: 0.00001761
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001715
Iteration 12/1000 | Loss: 0.00001699
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001692
Iteration 15/1000 | Loss: 0.00001690
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001669
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001658
Iteration 23/1000 | Loss: 0.00001654
Iteration 24/1000 | Loss: 0.00001654
Iteration 25/1000 | Loss: 0.00001651
Iteration 26/1000 | Loss: 0.00001650
Iteration 27/1000 | Loss: 0.00001650
Iteration 28/1000 | Loss: 0.00001650
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001645
Iteration 35/1000 | Loss: 0.00001645
Iteration 36/1000 | Loss: 0.00001644
Iteration 37/1000 | Loss: 0.00001644
Iteration 38/1000 | Loss: 0.00001643
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001642
Iteration 42/1000 | Loss: 0.00001642
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001639
Iteration 58/1000 | Loss: 0.00001639
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001638
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001638
Iteration 64/1000 | Loss: 0.00001638
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001637
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001634
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00001633
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001633
Iteration 82/1000 | Loss: 0.00001633
Iteration 83/1000 | Loss: 0.00001633
Iteration 84/1000 | Loss: 0.00001632
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001632
Iteration 87/1000 | Loss: 0.00001631
Iteration 88/1000 | Loss: 0.00001631
Iteration 89/1000 | Loss: 0.00001631
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001630
Iteration 93/1000 | Loss: 0.00001630
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001629
Iteration 96/1000 | Loss: 0.00001629
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001628
Iteration 99/1000 | Loss: 0.00001628
Iteration 100/1000 | Loss: 0.00001627
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001626
Iteration 105/1000 | Loss: 0.00001626
Iteration 106/1000 | Loss: 0.00001626
Iteration 107/1000 | Loss: 0.00001626
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001625
Iteration 110/1000 | Loss: 0.00001625
Iteration 111/1000 | Loss: 0.00001625
Iteration 112/1000 | Loss: 0.00001625
Iteration 113/1000 | Loss: 0.00001625
Iteration 114/1000 | Loss: 0.00001625
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001625
Iteration 120/1000 | Loss: 0.00001625
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001624
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001623
Iteration 133/1000 | Loss: 0.00001623
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001622
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001621
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001621
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001619
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001618
Iteration 157/1000 | Loss: 0.00001618
Iteration 158/1000 | Loss: 0.00001618
Iteration 159/1000 | Loss: 0.00001617
Iteration 160/1000 | Loss: 0.00001617
Iteration 161/1000 | Loss: 0.00001617
Iteration 162/1000 | Loss: 0.00001617
Iteration 163/1000 | Loss: 0.00001617
Iteration 164/1000 | Loss: 0.00001617
Iteration 165/1000 | Loss: 0.00001617
Iteration 166/1000 | Loss: 0.00001617
Iteration 167/1000 | Loss: 0.00001617
Iteration 168/1000 | Loss: 0.00001617
Iteration 169/1000 | Loss: 0.00001616
Iteration 170/1000 | Loss: 0.00001616
Iteration 171/1000 | Loss: 0.00001616
Iteration 172/1000 | Loss: 0.00001616
Iteration 173/1000 | Loss: 0.00001616
Iteration 174/1000 | Loss: 0.00001616
Iteration 175/1000 | Loss: 0.00001615
Iteration 176/1000 | Loss: 0.00001615
Iteration 177/1000 | Loss: 0.00001615
Iteration 178/1000 | Loss: 0.00001615
Iteration 179/1000 | Loss: 0.00001615
Iteration 180/1000 | Loss: 0.00001615
Iteration 181/1000 | Loss: 0.00001615
Iteration 182/1000 | Loss: 0.00001615
Iteration 183/1000 | Loss: 0.00001615
Iteration 184/1000 | Loss: 0.00001615
Iteration 185/1000 | Loss: 0.00001615
Iteration 186/1000 | Loss: 0.00001615
Iteration 187/1000 | Loss: 0.00001615
Iteration 188/1000 | Loss: 0.00001615
Iteration 189/1000 | Loss: 0.00001614
Iteration 190/1000 | Loss: 0.00001614
Iteration 191/1000 | Loss: 0.00001614
Iteration 192/1000 | Loss: 0.00001614
Iteration 193/1000 | Loss: 0.00001614
Iteration 194/1000 | Loss: 0.00001614
Iteration 195/1000 | Loss: 0.00001614
Iteration 196/1000 | Loss: 0.00001614
Iteration 197/1000 | Loss: 0.00001614
Iteration 198/1000 | Loss: 0.00001613
Iteration 199/1000 | Loss: 0.00001613
Iteration 200/1000 | Loss: 0.00001613
Iteration 201/1000 | Loss: 0.00001613
Iteration 202/1000 | Loss: 0.00001613
Iteration 203/1000 | Loss: 0.00001613
Iteration 204/1000 | Loss: 0.00001613
Iteration 205/1000 | Loss: 0.00001613
Iteration 206/1000 | Loss: 0.00001613
Iteration 207/1000 | Loss: 0.00001613
Iteration 208/1000 | Loss: 0.00001613
Iteration 209/1000 | Loss: 0.00001613
Iteration 210/1000 | Loss: 0.00001613
Iteration 211/1000 | Loss: 0.00001613
Iteration 212/1000 | Loss: 0.00001613
Iteration 213/1000 | Loss: 0.00001613
Iteration 214/1000 | Loss: 0.00001613
Iteration 215/1000 | Loss: 0.00001612
Iteration 216/1000 | Loss: 0.00001612
Iteration 217/1000 | Loss: 0.00001612
Iteration 218/1000 | Loss: 0.00001612
Iteration 219/1000 | Loss: 0.00001612
Iteration 220/1000 | Loss: 0.00001612
Iteration 221/1000 | Loss: 0.00001612
Iteration 222/1000 | Loss: 0.00001612
Iteration 223/1000 | Loss: 0.00001612
Iteration 224/1000 | Loss: 0.00001612
Iteration 225/1000 | Loss: 0.00001612
Iteration 226/1000 | Loss: 0.00001612
Iteration 227/1000 | Loss: 0.00001612
Iteration 228/1000 | Loss: 0.00001612
Iteration 229/1000 | Loss: 0.00001612
Iteration 230/1000 | Loss: 0.00001612
Iteration 231/1000 | Loss: 0.00001612
Iteration 232/1000 | Loss: 0.00001612
Iteration 233/1000 | Loss: 0.00001612
Iteration 234/1000 | Loss: 0.00001612
Iteration 235/1000 | Loss: 0.00001612
Iteration 236/1000 | Loss: 0.00001612
Iteration 237/1000 | Loss: 0.00001612
Iteration 238/1000 | Loss: 0.00001612
Iteration 239/1000 | Loss: 0.00001612
Iteration 240/1000 | Loss: 0.00001612
Iteration 241/1000 | Loss: 0.00001612
Iteration 242/1000 | Loss: 0.00001612
Iteration 243/1000 | Loss: 0.00001612
Iteration 244/1000 | Loss: 0.00001612
Iteration 245/1000 | Loss: 0.00001612
Iteration 246/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.612376763659995e-05, 1.612376763659995e-05, 1.612376763659995e-05, 1.612376763659995e-05, 1.612376763659995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.612376763659995e-05

Optimization complete. Final v2v error: 3.370039939880371 mm

Highest mean error: 5.615001201629639 mm for frame 92

Lowest mean error: 3.0418484210968018 mm for frame 0

Saving results

Total time: 59.904608488082886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949688
Iteration 2/25 | Loss: 0.00283047
Iteration 3/25 | Loss: 0.00174461
Iteration 4/25 | Loss: 0.00161951
Iteration 5/25 | Loss: 0.00164020
Iteration 6/25 | Loss: 0.00148346
Iteration 7/25 | Loss: 0.00141189
Iteration 8/25 | Loss: 0.00139537
Iteration 9/25 | Loss: 0.00139191
Iteration 10/25 | Loss: 0.00139059
Iteration 11/25 | Loss: 0.00138994
Iteration 12/25 | Loss: 0.00138592
Iteration 13/25 | Loss: 0.00138460
Iteration 14/25 | Loss: 0.00137883
Iteration 15/25 | Loss: 0.00138181
Iteration 16/25 | Loss: 0.00137992
Iteration 17/25 | Loss: 0.00137452
Iteration 18/25 | Loss: 0.00137415
Iteration 19/25 | Loss: 0.00137401
Iteration 20/25 | Loss: 0.00137799
Iteration 21/25 | Loss: 0.00137728
Iteration 22/25 | Loss: 0.00137339
Iteration 23/25 | Loss: 0.00137308
Iteration 24/25 | Loss: 0.00137298
Iteration 25/25 | Loss: 0.00137297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38352096
Iteration 2/25 | Loss: 0.00096627
Iteration 3/25 | Loss: 0.00093962
Iteration 4/25 | Loss: 0.00093962
Iteration 5/25 | Loss: 0.00093962
Iteration 6/25 | Loss: 0.00093962
Iteration 7/25 | Loss: 0.00093962
Iteration 8/25 | Loss: 0.00093962
Iteration 9/25 | Loss: 0.00093962
Iteration 10/25 | Loss: 0.00093962
Iteration 11/25 | Loss: 0.00093962
Iteration 12/25 | Loss: 0.00093962
Iteration 13/25 | Loss: 0.00093962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009396180976182222, 0.0009396180976182222, 0.0009396180976182222, 0.0009396180976182222, 0.0009396180976182222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009396180976182222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093962
Iteration 2/1000 | Loss: 0.00006304
Iteration 3/1000 | Loss: 0.00003986
Iteration 4/1000 | Loss: 0.00003544
Iteration 5/1000 | Loss: 0.00003268
Iteration 6/1000 | Loss: 0.00009635
Iteration 7/1000 | Loss: 0.00003044
Iteration 8/1000 | Loss: 0.00002974
Iteration 9/1000 | Loss: 0.00006406
Iteration 10/1000 | Loss: 0.00002900
Iteration 11/1000 | Loss: 0.00002850
Iteration 12/1000 | Loss: 0.00002806
Iteration 13/1000 | Loss: 0.00008418
Iteration 14/1000 | Loss: 0.00075867
Iteration 15/1000 | Loss: 0.00010692
Iteration 16/1000 | Loss: 0.00003657
Iteration 17/1000 | Loss: 0.00002755
Iteration 18/1000 | Loss: 0.00002671
Iteration 19/1000 | Loss: 0.00002610
Iteration 20/1000 | Loss: 0.00012406
Iteration 21/1000 | Loss: 0.00002669
Iteration 22/1000 | Loss: 0.00034673
Iteration 23/1000 | Loss: 0.00003021
Iteration 24/1000 | Loss: 0.00002615
Iteration 25/1000 | Loss: 0.00005215
Iteration 26/1000 | Loss: 0.00002545
Iteration 27/1000 | Loss: 0.00002493
Iteration 28/1000 | Loss: 0.00002459
Iteration 29/1000 | Loss: 0.00006492
Iteration 30/1000 | Loss: 0.00053421
Iteration 31/1000 | Loss: 0.00049685
Iteration 32/1000 | Loss: 0.00012239
Iteration 33/1000 | Loss: 0.00002518
Iteration 34/1000 | Loss: 0.00002390
Iteration 35/1000 | Loss: 0.00029109
Iteration 36/1000 | Loss: 0.00010244
Iteration 37/1000 | Loss: 0.00020276
Iteration 38/1000 | Loss: 0.00012842
Iteration 39/1000 | Loss: 0.00002340
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00002307
Iteration 42/1000 | Loss: 0.00002305
Iteration 43/1000 | Loss: 0.00002284
Iteration 44/1000 | Loss: 0.00039604
Iteration 45/1000 | Loss: 0.00006649
Iteration 46/1000 | Loss: 0.00002559
Iteration 47/1000 | Loss: 0.00002331
Iteration 48/1000 | Loss: 0.00002276
Iteration 49/1000 | Loss: 0.00033507
Iteration 50/1000 | Loss: 0.00003761
Iteration 51/1000 | Loss: 0.00002322
Iteration 52/1000 | Loss: 0.00002264
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002245
Iteration 55/1000 | Loss: 0.00002241
Iteration 56/1000 | Loss: 0.00002240
Iteration 57/1000 | Loss: 0.00002237
Iteration 58/1000 | Loss: 0.00002231
Iteration 59/1000 | Loss: 0.00002222
Iteration 60/1000 | Loss: 0.00002220
Iteration 61/1000 | Loss: 0.00002215
Iteration 62/1000 | Loss: 0.00002201
Iteration 63/1000 | Loss: 0.00002198
Iteration 64/1000 | Loss: 0.00002186
Iteration 65/1000 | Loss: 0.00002185
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002181
Iteration 68/1000 | Loss: 0.00002178
Iteration 69/1000 | Loss: 0.00002178
Iteration 70/1000 | Loss: 0.00002177
Iteration 71/1000 | Loss: 0.00002177
Iteration 72/1000 | Loss: 0.00002176
Iteration 73/1000 | Loss: 0.00002176
Iteration 74/1000 | Loss: 0.00002176
Iteration 75/1000 | Loss: 0.00002175
Iteration 76/1000 | Loss: 0.00002175
Iteration 77/1000 | Loss: 0.00002175
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002174
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002172
Iteration 85/1000 | Loss: 0.00002172
Iteration 86/1000 | Loss: 0.00002172
Iteration 87/1000 | Loss: 0.00002172
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002172
Iteration 90/1000 | Loss: 0.00002172
Iteration 91/1000 | Loss: 0.00002172
Iteration 92/1000 | Loss: 0.00002172
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002172
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002171
Iteration 99/1000 | Loss: 0.00002171
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002170
Iteration 104/1000 | Loss: 0.00002170
Iteration 105/1000 | Loss: 0.00002170
Iteration 106/1000 | Loss: 0.00002170
Iteration 107/1000 | Loss: 0.00002170
Iteration 108/1000 | Loss: 0.00002170
Iteration 109/1000 | Loss: 0.00002170
Iteration 110/1000 | Loss: 0.00002170
Iteration 111/1000 | Loss: 0.00002169
Iteration 112/1000 | Loss: 0.00002169
Iteration 113/1000 | Loss: 0.00002169
Iteration 114/1000 | Loss: 0.00002169
Iteration 115/1000 | Loss: 0.00002169
Iteration 116/1000 | Loss: 0.00002169
Iteration 117/1000 | Loss: 0.00002169
Iteration 118/1000 | Loss: 0.00002169
Iteration 119/1000 | Loss: 0.00002168
Iteration 120/1000 | Loss: 0.00002168
Iteration 121/1000 | Loss: 0.00002168
Iteration 122/1000 | Loss: 0.00002168
Iteration 123/1000 | Loss: 0.00002168
Iteration 124/1000 | Loss: 0.00002168
Iteration 125/1000 | Loss: 0.00002168
Iteration 126/1000 | Loss: 0.00002168
Iteration 127/1000 | Loss: 0.00002168
Iteration 128/1000 | Loss: 0.00002168
Iteration 129/1000 | Loss: 0.00002168
Iteration 130/1000 | Loss: 0.00002168
Iteration 131/1000 | Loss: 0.00002168
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002167
Iteration 135/1000 | Loss: 0.00002167
Iteration 136/1000 | Loss: 0.00002167
Iteration 137/1000 | Loss: 0.00002167
Iteration 138/1000 | Loss: 0.00002167
Iteration 139/1000 | Loss: 0.00002167
Iteration 140/1000 | Loss: 0.00002167
Iteration 141/1000 | Loss: 0.00002166
Iteration 142/1000 | Loss: 0.00002166
Iteration 143/1000 | Loss: 0.00002166
Iteration 144/1000 | Loss: 0.00002166
Iteration 145/1000 | Loss: 0.00002166
Iteration 146/1000 | Loss: 0.00002166
Iteration 147/1000 | Loss: 0.00002166
Iteration 148/1000 | Loss: 0.00002166
Iteration 149/1000 | Loss: 0.00002166
Iteration 150/1000 | Loss: 0.00002166
Iteration 151/1000 | Loss: 0.00002166
Iteration 152/1000 | Loss: 0.00002166
Iteration 153/1000 | Loss: 0.00002165
Iteration 154/1000 | Loss: 0.00002165
Iteration 155/1000 | Loss: 0.00002165
Iteration 156/1000 | Loss: 0.00002165
Iteration 157/1000 | Loss: 0.00002165
Iteration 158/1000 | Loss: 0.00002165
Iteration 159/1000 | Loss: 0.00002165
Iteration 160/1000 | Loss: 0.00002165
Iteration 161/1000 | Loss: 0.00002165
Iteration 162/1000 | Loss: 0.00002165
Iteration 163/1000 | Loss: 0.00002165
Iteration 164/1000 | Loss: 0.00002164
Iteration 165/1000 | Loss: 0.00002164
Iteration 166/1000 | Loss: 0.00002164
Iteration 167/1000 | Loss: 0.00002164
Iteration 168/1000 | Loss: 0.00002164
Iteration 169/1000 | Loss: 0.00002164
Iteration 170/1000 | Loss: 0.00002164
Iteration 171/1000 | Loss: 0.00002164
Iteration 172/1000 | Loss: 0.00002164
Iteration 173/1000 | Loss: 0.00002164
Iteration 174/1000 | Loss: 0.00002164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.1644040316459723e-05, 2.1644040316459723e-05, 2.1644040316459723e-05, 2.1644040316459723e-05, 2.1644040316459723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1644040316459723e-05

Optimization complete. Final v2v error: 3.6022138595581055 mm

Highest mean error: 11.464232444763184 mm for frame 13

Lowest mean error: 3.0779805183410645 mm for frame 127

Saving results

Total time: 151.21416068077087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402705
Iteration 2/25 | Loss: 0.00132100
Iteration 3/25 | Loss: 0.00126742
Iteration 4/25 | Loss: 0.00125577
Iteration 5/25 | Loss: 0.00125124
Iteration 6/25 | Loss: 0.00125034
Iteration 7/25 | Loss: 0.00125034
Iteration 8/25 | Loss: 0.00125034
Iteration 9/25 | Loss: 0.00125034
Iteration 10/25 | Loss: 0.00125034
Iteration 11/25 | Loss: 0.00125034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012503439793363214, 0.0012503439793363214, 0.0012503439793363214, 0.0012503439793363214, 0.0012503439793363214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012503439793363214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40872312
Iteration 2/25 | Loss: 0.00097678
Iteration 3/25 | Loss: 0.00097678
Iteration 4/25 | Loss: 0.00097678
Iteration 5/25 | Loss: 0.00097678
Iteration 6/25 | Loss: 0.00097678
Iteration 7/25 | Loss: 0.00097678
Iteration 8/25 | Loss: 0.00097678
Iteration 9/25 | Loss: 0.00097678
Iteration 10/25 | Loss: 0.00097678
Iteration 11/25 | Loss: 0.00097678
Iteration 12/25 | Loss: 0.00097678
Iteration 13/25 | Loss: 0.00097678
Iteration 14/25 | Loss: 0.00097678
Iteration 15/25 | Loss: 0.00097678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000976777053438127, 0.000976777053438127, 0.000976777053438127, 0.000976777053438127, 0.000976777053438127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000976777053438127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097678
Iteration 2/1000 | Loss: 0.00003748
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00002250
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00001997
Iteration 7/1000 | Loss: 0.00001919
Iteration 8/1000 | Loss: 0.00001881
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001810
Iteration 11/1000 | Loss: 0.00001781
Iteration 12/1000 | Loss: 0.00001770
Iteration 13/1000 | Loss: 0.00001770
Iteration 14/1000 | Loss: 0.00001770
Iteration 15/1000 | Loss: 0.00001770
Iteration 16/1000 | Loss: 0.00001754
Iteration 17/1000 | Loss: 0.00001748
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001747
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00001746
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001744
Iteration 24/1000 | Loss: 0.00001743
Iteration 25/1000 | Loss: 0.00001743
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001742
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001741
Iteration 30/1000 | Loss: 0.00001740
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001736
Iteration 39/1000 | Loss: 0.00001736
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00001735
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001733
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001732
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001731
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001726
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001724
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001720
Iteration 58/1000 | Loss: 0.00001719
Iteration 59/1000 | Loss: 0.00001719
Iteration 60/1000 | Loss: 0.00001718
Iteration 61/1000 | Loss: 0.00001718
Iteration 62/1000 | Loss: 0.00001718
Iteration 63/1000 | Loss: 0.00001717
Iteration 64/1000 | Loss: 0.00001717
Iteration 65/1000 | Loss: 0.00001717
Iteration 66/1000 | Loss: 0.00001717
Iteration 67/1000 | Loss: 0.00001717
Iteration 68/1000 | Loss: 0.00001717
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001716
Iteration 71/1000 | Loss: 0.00001716
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001714
Iteration 75/1000 | Loss: 0.00001714
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001713
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001712
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001712
Iteration 86/1000 | Loss: 0.00001712
Iteration 87/1000 | Loss: 0.00001712
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001712
Iteration 90/1000 | Loss: 0.00001712
Iteration 91/1000 | Loss: 0.00001711
Iteration 92/1000 | Loss: 0.00001711
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001709
Iteration 99/1000 | Loss: 0.00001709
Iteration 100/1000 | Loss: 0.00001709
Iteration 101/1000 | Loss: 0.00001709
Iteration 102/1000 | Loss: 0.00001709
Iteration 103/1000 | Loss: 0.00001709
Iteration 104/1000 | Loss: 0.00001709
Iteration 105/1000 | Loss: 0.00001709
Iteration 106/1000 | Loss: 0.00001709
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001709
Iteration 110/1000 | Loss: 0.00001709
Iteration 111/1000 | Loss: 0.00001709
Iteration 112/1000 | Loss: 0.00001709
Iteration 113/1000 | Loss: 0.00001709
Iteration 114/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.709123534965329e-05, 1.709123534965329e-05, 1.709123534965329e-05, 1.709123534965329e-05, 1.709123534965329e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.709123534965329e-05

Optimization complete. Final v2v error: 3.4229650497436523 mm

Highest mean error: 4.2621331214904785 mm for frame 120

Lowest mean error: 2.9097542762756348 mm for frame 27

Saving results

Total time: 37.908058166503906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957307
Iteration 2/25 | Loss: 0.00395054
Iteration 3/25 | Loss: 0.00284777
Iteration 4/25 | Loss: 0.00259503
Iteration 5/25 | Loss: 0.00261650
Iteration 6/25 | Loss: 0.00224664
Iteration 7/25 | Loss: 0.00222850
Iteration 8/25 | Loss: 0.00216132
Iteration 9/25 | Loss: 0.00202642
Iteration 10/25 | Loss: 0.00195073
Iteration 11/25 | Loss: 0.00192495
Iteration 12/25 | Loss: 0.00188509
Iteration 13/25 | Loss: 0.00184290
Iteration 14/25 | Loss: 0.00185049
Iteration 15/25 | Loss: 0.00181244
Iteration 16/25 | Loss: 0.00179852
Iteration 17/25 | Loss: 0.00178980
Iteration 18/25 | Loss: 0.00177579
Iteration 19/25 | Loss: 0.00177399
Iteration 20/25 | Loss: 0.00177208
Iteration 21/25 | Loss: 0.00178763
Iteration 22/25 | Loss: 0.00174425
Iteration 23/25 | Loss: 0.00173601
Iteration 24/25 | Loss: 0.00173691
Iteration 25/25 | Loss: 0.00174245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38943577
Iteration 2/25 | Loss: 0.00580629
Iteration 3/25 | Loss: 0.00405981
Iteration 4/25 | Loss: 0.00405981
Iteration 5/25 | Loss: 0.00405981
Iteration 6/25 | Loss: 0.00405981
Iteration 7/25 | Loss: 0.00405981
Iteration 8/25 | Loss: 0.00405981
Iteration 9/25 | Loss: 0.00405981
Iteration 10/25 | Loss: 0.00405981
Iteration 11/25 | Loss: 0.00405981
Iteration 12/25 | Loss: 0.00405981
Iteration 13/25 | Loss: 0.00405981
Iteration 14/25 | Loss: 0.00405981
Iteration 15/25 | Loss: 0.00405981
Iteration 16/25 | Loss: 0.00405981
Iteration 17/25 | Loss: 0.00405981
Iteration 18/25 | Loss: 0.00405981
Iteration 19/25 | Loss: 0.00405981
Iteration 20/25 | Loss: 0.00405981
Iteration 21/25 | Loss: 0.00405981
Iteration 22/25 | Loss: 0.00405981
Iteration 23/25 | Loss: 0.00405981
Iteration 24/25 | Loss: 0.00405981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.004059806000441313, 0.004059806000441313, 0.004059806000441313, 0.004059806000441313, 0.004059806000441313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004059806000441313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00405981
Iteration 2/1000 | Loss: 0.00280530
Iteration 3/1000 | Loss: 0.00265743
Iteration 4/1000 | Loss: 0.00489843
Iteration 5/1000 | Loss: 0.01052131
Iteration 6/1000 | Loss: 0.00246040
Iteration 7/1000 | Loss: 0.00346028
Iteration 8/1000 | Loss: 0.00396354
Iteration 9/1000 | Loss: 0.00433814
Iteration 10/1000 | Loss: 0.00263538
Iteration 11/1000 | Loss: 0.00184107
Iteration 12/1000 | Loss: 0.00061900
Iteration 13/1000 | Loss: 0.00043841
Iteration 14/1000 | Loss: 0.00062167
Iteration 15/1000 | Loss: 0.00198372
Iteration 16/1000 | Loss: 0.00102729
Iteration 17/1000 | Loss: 0.00076355
Iteration 18/1000 | Loss: 0.00036136
Iteration 19/1000 | Loss: 0.00028658
Iteration 20/1000 | Loss: 0.00031785
Iteration 21/1000 | Loss: 0.00119393
Iteration 22/1000 | Loss: 0.00084538
Iteration 23/1000 | Loss: 0.00023688
Iteration 24/1000 | Loss: 0.00019289
Iteration 25/1000 | Loss: 0.00028427
Iteration 26/1000 | Loss: 0.00254781
Iteration 27/1000 | Loss: 0.00563479
Iteration 28/1000 | Loss: 0.00397593
Iteration 29/1000 | Loss: 0.00134161
Iteration 30/1000 | Loss: 0.00175840
Iteration 31/1000 | Loss: 0.00335905
Iteration 32/1000 | Loss: 0.00588064
Iteration 33/1000 | Loss: 0.00618297
Iteration 34/1000 | Loss: 0.00420275
Iteration 35/1000 | Loss: 0.00484780
Iteration 36/1000 | Loss: 0.00283153
Iteration 37/1000 | Loss: 0.00315630
Iteration 38/1000 | Loss: 0.00113756
Iteration 39/1000 | Loss: 0.00182619
Iteration 40/1000 | Loss: 0.00143263
Iteration 41/1000 | Loss: 0.00180396
Iteration 42/1000 | Loss: 0.00229022
Iteration 43/1000 | Loss: 0.00082697
Iteration 44/1000 | Loss: 0.00095330
Iteration 45/1000 | Loss: 0.00144957
Iteration 46/1000 | Loss: 0.00122312
Iteration 47/1000 | Loss: 0.00084711
Iteration 48/1000 | Loss: 0.00074047
Iteration 49/1000 | Loss: 0.00128344
Iteration 50/1000 | Loss: 0.00079735
Iteration 51/1000 | Loss: 0.00060422
Iteration 52/1000 | Loss: 0.00095781
Iteration 53/1000 | Loss: 0.00178805
Iteration 54/1000 | Loss: 0.00165667
Iteration 55/1000 | Loss: 0.00281905
Iteration 56/1000 | Loss: 0.00078200
Iteration 57/1000 | Loss: 0.00153868
Iteration 58/1000 | Loss: 0.00059554
Iteration 59/1000 | Loss: 0.00048526
Iteration 60/1000 | Loss: 0.00022265
Iteration 61/1000 | Loss: 0.00015001
Iteration 62/1000 | Loss: 0.00032170
Iteration 63/1000 | Loss: 0.00103595
Iteration 64/1000 | Loss: 0.00047113
Iteration 65/1000 | Loss: 0.00062911
Iteration 66/1000 | Loss: 0.00073473
Iteration 67/1000 | Loss: 0.00021645
Iteration 68/1000 | Loss: 0.00091699
Iteration 69/1000 | Loss: 0.00061435
Iteration 70/1000 | Loss: 0.00072922
Iteration 71/1000 | Loss: 0.00008536
Iteration 72/1000 | Loss: 0.00007702
Iteration 73/1000 | Loss: 0.00026948
Iteration 74/1000 | Loss: 0.00214084
Iteration 75/1000 | Loss: 0.00096315
Iteration 76/1000 | Loss: 0.00094840
Iteration 77/1000 | Loss: 0.00040655
Iteration 78/1000 | Loss: 0.00048381
Iteration 79/1000 | Loss: 0.00051979
Iteration 80/1000 | Loss: 0.00008740
Iteration 81/1000 | Loss: 0.00006516
Iteration 82/1000 | Loss: 0.00005268
Iteration 83/1000 | Loss: 0.00065709
Iteration 84/1000 | Loss: 0.00035618
Iteration 85/1000 | Loss: 0.00030778
Iteration 86/1000 | Loss: 0.00017309
Iteration 87/1000 | Loss: 0.00019178
Iteration 88/1000 | Loss: 0.00005232
Iteration 89/1000 | Loss: 0.00012181
Iteration 90/1000 | Loss: 0.00051471
Iteration 91/1000 | Loss: 0.00040140
Iteration 92/1000 | Loss: 0.00021010
Iteration 93/1000 | Loss: 0.00022073
Iteration 94/1000 | Loss: 0.00018032
Iteration 95/1000 | Loss: 0.00004047
Iteration 96/1000 | Loss: 0.00003655
Iteration 97/1000 | Loss: 0.00003408
Iteration 98/1000 | Loss: 0.00003195
Iteration 99/1000 | Loss: 0.00039448
Iteration 100/1000 | Loss: 0.00161900
Iteration 101/1000 | Loss: 0.00058530
Iteration 102/1000 | Loss: 0.00021023
Iteration 103/1000 | Loss: 0.00024251
Iteration 104/1000 | Loss: 0.00027978
Iteration 105/1000 | Loss: 0.00023415
Iteration 106/1000 | Loss: 0.00026543
Iteration 107/1000 | Loss: 0.00005709
Iteration 108/1000 | Loss: 0.00003991
Iteration 109/1000 | Loss: 0.00003417
Iteration 110/1000 | Loss: 0.00006882
Iteration 111/1000 | Loss: 0.00003807
Iteration 112/1000 | Loss: 0.00005225
Iteration 113/1000 | Loss: 0.00003269
Iteration 114/1000 | Loss: 0.00002905
Iteration 115/1000 | Loss: 0.00002682
Iteration 116/1000 | Loss: 0.00002902
Iteration 117/1000 | Loss: 0.00004930
Iteration 118/1000 | Loss: 0.00003033
Iteration 119/1000 | Loss: 0.00002480
Iteration 120/1000 | Loss: 0.00002691
Iteration 121/1000 | Loss: 0.00002441
Iteration 122/1000 | Loss: 0.00002412
Iteration 123/1000 | Loss: 0.00005075
Iteration 124/1000 | Loss: 0.00002381
Iteration 125/1000 | Loss: 0.00017337
Iteration 126/1000 | Loss: 0.00017336
Iteration 127/1000 | Loss: 0.00052105
Iteration 128/1000 | Loss: 0.00004739
Iteration 129/1000 | Loss: 0.00002521
Iteration 130/1000 | Loss: 0.00003643
Iteration 131/1000 | Loss: 0.00022215
Iteration 132/1000 | Loss: 0.00003534
Iteration 133/1000 | Loss: 0.00005607
Iteration 134/1000 | Loss: 0.00002850
Iteration 135/1000 | Loss: 0.00002461
Iteration 136/1000 | Loss: 0.00002216
Iteration 137/1000 | Loss: 0.00002234
Iteration 138/1000 | Loss: 0.00002176
Iteration 139/1000 | Loss: 0.00003321
Iteration 140/1000 | Loss: 0.00005461
Iteration 141/1000 | Loss: 0.00003222
Iteration 142/1000 | Loss: 0.00003025
Iteration 143/1000 | Loss: 0.00002264
Iteration 144/1000 | Loss: 0.00002792
Iteration 145/1000 | Loss: 0.00002244
Iteration 146/1000 | Loss: 0.00002131
Iteration 147/1000 | Loss: 0.00002131
Iteration 148/1000 | Loss: 0.00002130
Iteration 149/1000 | Loss: 0.00002129
Iteration 150/1000 | Loss: 0.00002128
Iteration 151/1000 | Loss: 0.00002127
Iteration 152/1000 | Loss: 0.00002126
Iteration 153/1000 | Loss: 0.00002126
Iteration 154/1000 | Loss: 0.00002147
Iteration 155/1000 | Loss: 0.00002127
Iteration 156/1000 | Loss: 0.00002126
Iteration 157/1000 | Loss: 0.00002126
Iteration 158/1000 | Loss: 0.00002126
Iteration 159/1000 | Loss: 0.00002126
Iteration 160/1000 | Loss: 0.00002126
Iteration 161/1000 | Loss: 0.00002126
Iteration 162/1000 | Loss: 0.00002126
Iteration 163/1000 | Loss: 0.00002125
Iteration 164/1000 | Loss: 0.00002128
Iteration 165/1000 | Loss: 0.00002125
Iteration 166/1000 | Loss: 0.00002125
Iteration 167/1000 | Loss: 0.00002125
Iteration 168/1000 | Loss: 0.00002126
Iteration 169/1000 | Loss: 0.00002126
Iteration 170/1000 | Loss: 0.00002126
Iteration 171/1000 | Loss: 0.00002125
Iteration 172/1000 | Loss: 0.00002125
Iteration 173/1000 | Loss: 0.00002124
Iteration 174/1000 | Loss: 0.00002123
Iteration 175/1000 | Loss: 0.00002123
Iteration 176/1000 | Loss: 0.00002123
Iteration 177/1000 | Loss: 0.00002123
Iteration 178/1000 | Loss: 0.00002123
Iteration 179/1000 | Loss: 0.00002123
Iteration 180/1000 | Loss: 0.00002123
Iteration 181/1000 | Loss: 0.00002122
Iteration 182/1000 | Loss: 0.00002122
Iteration 183/1000 | Loss: 0.00002122
Iteration 184/1000 | Loss: 0.00002122
Iteration 185/1000 | Loss: 0.00002122
Iteration 186/1000 | Loss: 0.00002122
Iteration 187/1000 | Loss: 0.00002122
Iteration 188/1000 | Loss: 0.00002122
Iteration 189/1000 | Loss: 0.00002122
Iteration 190/1000 | Loss: 0.00002121
Iteration 191/1000 | Loss: 0.00002121
Iteration 192/1000 | Loss: 0.00002121
Iteration 193/1000 | Loss: 0.00002121
Iteration 194/1000 | Loss: 0.00002121
Iteration 195/1000 | Loss: 0.00002121
Iteration 196/1000 | Loss: 0.00002121
Iteration 197/1000 | Loss: 0.00002121
Iteration 198/1000 | Loss: 0.00002121
Iteration 199/1000 | Loss: 0.00002121
Iteration 200/1000 | Loss: 0.00002120
Iteration 201/1000 | Loss: 0.00002120
Iteration 202/1000 | Loss: 0.00002119
Iteration 203/1000 | Loss: 0.00002119
Iteration 204/1000 | Loss: 0.00002118
Iteration 205/1000 | Loss: 0.00002118
Iteration 206/1000 | Loss: 0.00002117
Iteration 207/1000 | Loss: 0.00002117
Iteration 208/1000 | Loss: 0.00002117
Iteration 209/1000 | Loss: 0.00002117
Iteration 210/1000 | Loss: 0.00002117
Iteration 211/1000 | Loss: 0.00002117
Iteration 212/1000 | Loss: 0.00002117
Iteration 213/1000 | Loss: 0.00002117
Iteration 214/1000 | Loss: 0.00002117
Iteration 215/1000 | Loss: 0.00002116
Iteration 216/1000 | Loss: 0.00002116
Iteration 217/1000 | Loss: 0.00002116
Iteration 218/1000 | Loss: 0.00002116
Iteration 219/1000 | Loss: 0.00002116
Iteration 220/1000 | Loss: 0.00002116
Iteration 221/1000 | Loss: 0.00002115
Iteration 222/1000 | Loss: 0.00002115
Iteration 223/1000 | Loss: 0.00002115
Iteration 224/1000 | Loss: 0.00002115
Iteration 225/1000 | Loss: 0.00002115
Iteration 226/1000 | Loss: 0.00002115
Iteration 227/1000 | Loss: 0.00002115
Iteration 228/1000 | Loss: 0.00002115
Iteration 229/1000 | Loss: 0.00002115
Iteration 230/1000 | Loss: 0.00002114
Iteration 231/1000 | Loss: 0.00002114
Iteration 232/1000 | Loss: 0.00002114
Iteration 233/1000 | Loss: 0.00002114
Iteration 234/1000 | Loss: 0.00002114
Iteration 235/1000 | Loss: 0.00002114
Iteration 236/1000 | Loss: 0.00002113
Iteration 237/1000 | Loss: 0.00002113
Iteration 238/1000 | Loss: 0.00002113
Iteration 239/1000 | Loss: 0.00002113
Iteration 240/1000 | Loss: 0.00002113
Iteration 241/1000 | Loss: 0.00002113
Iteration 242/1000 | Loss: 0.00002113
Iteration 243/1000 | Loss: 0.00002113
Iteration 244/1000 | Loss: 0.00002113
Iteration 245/1000 | Loss: 0.00002113
Iteration 246/1000 | Loss: 0.00002113
Iteration 247/1000 | Loss: 0.00002113
Iteration 248/1000 | Loss: 0.00002113
Iteration 249/1000 | Loss: 0.00002113
Iteration 250/1000 | Loss: 0.00002113
Iteration 251/1000 | Loss: 0.00002113
Iteration 252/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [2.1132231267984025e-05, 2.1132231267984025e-05, 2.1132231267984025e-05, 2.1132231267984025e-05, 2.1132231267984025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1132231267984025e-05

Optimization complete. Final v2v error: 3.7419521808624268 mm

Highest mean error: 5.186171531677246 mm for frame 45

Lowest mean error: 3.0207977294921875 mm for frame 156

Saving results

Total time: 265.58108043670654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822043
Iteration 2/25 | Loss: 0.00134921
Iteration 3/25 | Loss: 0.00128841
Iteration 4/25 | Loss: 0.00127505
Iteration 5/25 | Loss: 0.00127153
Iteration 6/25 | Loss: 0.00127141
Iteration 7/25 | Loss: 0.00127141
Iteration 8/25 | Loss: 0.00127141
Iteration 9/25 | Loss: 0.00127141
Iteration 10/25 | Loss: 0.00127141
Iteration 11/25 | Loss: 0.00127141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012714138720184565, 0.0012714138720184565, 0.0012714138720184565, 0.0012714138720184565, 0.0012714138720184565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012714138720184565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41240299
Iteration 2/25 | Loss: 0.00080260
Iteration 3/25 | Loss: 0.00080260
Iteration 4/25 | Loss: 0.00080259
Iteration 5/25 | Loss: 0.00080259
Iteration 6/25 | Loss: 0.00080259
Iteration 7/25 | Loss: 0.00080259
Iteration 8/25 | Loss: 0.00080259
Iteration 9/25 | Loss: 0.00080259
Iteration 10/25 | Loss: 0.00080259
Iteration 11/25 | Loss: 0.00080259
Iteration 12/25 | Loss: 0.00080259
Iteration 13/25 | Loss: 0.00080259
Iteration 14/25 | Loss: 0.00080259
Iteration 15/25 | Loss: 0.00080259
Iteration 16/25 | Loss: 0.00080259
Iteration 17/25 | Loss: 0.00080259
Iteration 18/25 | Loss: 0.00080259
Iteration 19/25 | Loss: 0.00080259
Iteration 20/25 | Loss: 0.00080259
Iteration 21/25 | Loss: 0.00080259
Iteration 22/25 | Loss: 0.00080259
Iteration 23/25 | Loss: 0.00080259
Iteration 24/25 | Loss: 0.00080259
Iteration 25/25 | Loss: 0.00080259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080259
Iteration 2/1000 | Loss: 0.00002571
Iteration 3/1000 | Loss: 0.00002009
Iteration 4/1000 | Loss: 0.00001821
Iteration 5/1000 | Loss: 0.00001740
Iteration 6/1000 | Loss: 0.00001671
Iteration 7/1000 | Loss: 0.00001623
Iteration 8/1000 | Loss: 0.00001593
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001529
Iteration 14/1000 | Loss: 0.00001529
Iteration 15/1000 | Loss: 0.00001518
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001509
Iteration 18/1000 | Loss: 0.00001504
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001491
Iteration 21/1000 | Loss: 0.00001485
Iteration 22/1000 | Loss: 0.00001484
Iteration 23/1000 | Loss: 0.00001483
Iteration 24/1000 | Loss: 0.00001482
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00001482
Iteration 29/1000 | Loss: 0.00001482
Iteration 30/1000 | Loss: 0.00001481
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001476
Iteration 33/1000 | Loss: 0.00001475
Iteration 34/1000 | Loss: 0.00001475
Iteration 35/1000 | Loss: 0.00001473
Iteration 36/1000 | Loss: 0.00001473
Iteration 37/1000 | Loss: 0.00001471
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001465
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001463
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001461
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001459
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001452
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001449
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001446
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001442
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001441
Iteration 97/1000 | Loss: 0.00001441
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001439
Iteration 102/1000 | Loss: 0.00001439
Iteration 103/1000 | Loss: 0.00001439
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001438
Iteration 113/1000 | Loss: 0.00001438
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001438
Iteration 116/1000 | Loss: 0.00001438
Iteration 117/1000 | Loss: 0.00001437
Iteration 118/1000 | Loss: 0.00001437
Iteration 119/1000 | Loss: 0.00001437
Iteration 120/1000 | Loss: 0.00001437
Iteration 121/1000 | Loss: 0.00001437
Iteration 122/1000 | Loss: 0.00001437
Iteration 123/1000 | Loss: 0.00001437
Iteration 124/1000 | Loss: 0.00001437
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001436
Iteration 131/1000 | Loss: 0.00001436
Iteration 132/1000 | Loss: 0.00001436
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001436
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001436
Iteration 153/1000 | Loss: 0.00001436
Iteration 154/1000 | Loss: 0.00001436
Iteration 155/1000 | Loss: 0.00001436
Iteration 156/1000 | Loss: 0.00001436
Iteration 157/1000 | Loss: 0.00001436
Iteration 158/1000 | Loss: 0.00001436
Iteration 159/1000 | Loss: 0.00001436
Iteration 160/1000 | Loss: 0.00001436
Iteration 161/1000 | Loss: 0.00001436
Iteration 162/1000 | Loss: 0.00001436
Iteration 163/1000 | Loss: 0.00001436
Iteration 164/1000 | Loss: 0.00001436
Iteration 165/1000 | Loss: 0.00001436
Iteration 166/1000 | Loss: 0.00001436
Iteration 167/1000 | Loss: 0.00001436
Iteration 168/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.435628018953139e-05, 1.435628018953139e-05, 1.435628018953139e-05, 1.435628018953139e-05, 1.435628018953139e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.435628018953139e-05

Optimization complete. Final v2v error: 3.200155258178711 mm

Highest mean error: 3.596130847930908 mm for frame 96

Lowest mean error: 3.075023889541626 mm for frame 144

Saving results

Total time: 40.60791325569153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765859
Iteration 2/25 | Loss: 0.00147635
Iteration 3/25 | Loss: 0.00132828
Iteration 4/25 | Loss: 0.00128843
Iteration 5/25 | Loss: 0.00130556
Iteration 6/25 | Loss: 0.00129243
Iteration 7/25 | Loss: 0.00127322
Iteration 8/25 | Loss: 0.00126761
Iteration 9/25 | Loss: 0.00126358
Iteration 10/25 | Loss: 0.00126111
Iteration 11/25 | Loss: 0.00126232
Iteration 12/25 | Loss: 0.00126092
Iteration 13/25 | Loss: 0.00126137
Iteration 14/25 | Loss: 0.00126185
Iteration 15/25 | Loss: 0.00126115
Iteration 16/25 | Loss: 0.00126299
Iteration 17/25 | Loss: 0.00126176
Iteration 18/25 | Loss: 0.00126163
Iteration 19/25 | Loss: 0.00126251
Iteration 20/25 | Loss: 0.00126242
Iteration 21/25 | Loss: 0.00126027
Iteration 22/25 | Loss: 0.00126007
Iteration 23/25 | Loss: 0.00125998
Iteration 24/25 | Loss: 0.00125998
Iteration 25/25 | Loss: 0.00125997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38872242
Iteration 2/25 | Loss: 0.00081205
Iteration 3/25 | Loss: 0.00081204
Iteration 4/25 | Loss: 0.00081204
Iteration 5/25 | Loss: 0.00081204
Iteration 6/25 | Loss: 0.00081204
Iteration 7/25 | Loss: 0.00081204
Iteration 8/25 | Loss: 0.00081204
Iteration 9/25 | Loss: 0.00081204
Iteration 10/25 | Loss: 0.00081204
Iteration 11/25 | Loss: 0.00081204
Iteration 12/25 | Loss: 0.00081204
Iteration 13/25 | Loss: 0.00081204
Iteration 14/25 | Loss: 0.00081204
Iteration 15/25 | Loss: 0.00081204
Iteration 16/25 | Loss: 0.00081204
Iteration 17/25 | Loss: 0.00081204
Iteration 18/25 | Loss: 0.00081204
Iteration 19/25 | Loss: 0.00081204
Iteration 20/25 | Loss: 0.00081204
Iteration 21/25 | Loss: 0.00081204
Iteration 22/25 | Loss: 0.00081204
Iteration 23/25 | Loss: 0.00081204
Iteration 24/25 | Loss: 0.00081204
Iteration 25/25 | Loss: 0.00081204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081204
Iteration 2/1000 | Loss: 0.00005262
Iteration 3/1000 | Loss: 0.00011994
Iteration 4/1000 | Loss: 0.00003777
Iteration 5/1000 | Loss: 0.00007117
Iteration 6/1000 | Loss: 0.00013074
Iteration 7/1000 | Loss: 0.00011232
Iteration 8/1000 | Loss: 0.00008128
Iteration 9/1000 | Loss: 0.00008110
Iteration 10/1000 | Loss: 0.00004602
Iteration 11/1000 | Loss: 0.00018463
Iteration 12/1000 | Loss: 0.00009064
Iteration 13/1000 | Loss: 0.00006969
Iteration 14/1000 | Loss: 0.00003901
Iteration 15/1000 | Loss: 0.00013711
Iteration 16/1000 | Loss: 0.00022324
Iteration 17/1000 | Loss: 0.00005005
Iteration 18/1000 | Loss: 0.00004133
Iteration 19/1000 | Loss: 0.00016238
Iteration 20/1000 | Loss: 0.00013669
Iteration 21/1000 | Loss: 0.00009579
Iteration 22/1000 | Loss: 0.00003973
Iteration 23/1000 | Loss: 0.00011017
Iteration 24/1000 | Loss: 0.00005326
Iteration 25/1000 | Loss: 0.00002988
Iteration 26/1000 | Loss: 0.00002859
Iteration 27/1000 | Loss: 0.00011871
Iteration 28/1000 | Loss: 0.00012080
Iteration 29/1000 | Loss: 0.00007057
Iteration 30/1000 | Loss: 0.00005273
Iteration 31/1000 | Loss: 0.00003483
Iteration 32/1000 | Loss: 0.00002700
Iteration 33/1000 | Loss: 0.00002648
Iteration 34/1000 | Loss: 0.00002620
Iteration 35/1000 | Loss: 0.00012442
Iteration 36/1000 | Loss: 0.00014446
Iteration 37/1000 | Loss: 0.00003501
Iteration 38/1000 | Loss: 0.00003128
Iteration 39/1000 | Loss: 0.00002960
Iteration 40/1000 | Loss: 0.00016209
Iteration 41/1000 | Loss: 0.00010719
Iteration 42/1000 | Loss: 0.00011451
Iteration 43/1000 | Loss: 0.00013504
Iteration 44/1000 | Loss: 0.00018832
Iteration 45/1000 | Loss: 0.00002884
Iteration 46/1000 | Loss: 0.00013352
Iteration 47/1000 | Loss: 0.00015574
Iteration 48/1000 | Loss: 0.00005973
Iteration 49/1000 | Loss: 0.00003793
Iteration 50/1000 | Loss: 0.00011897
Iteration 51/1000 | Loss: 0.00011606
Iteration 52/1000 | Loss: 0.00007595
Iteration 53/1000 | Loss: 0.00010457
Iteration 54/1000 | Loss: 0.00010057
Iteration 55/1000 | Loss: 0.00016185
Iteration 56/1000 | Loss: 0.00012803
Iteration 57/1000 | Loss: 0.00009596
Iteration 58/1000 | Loss: 0.00005860
Iteration 59/1000 | Loss: 0.00008133
Iteration 60/1000 | Loss: 0.00005713
Iteration 61/1000 | Loss: 0.00019455
Iteration 62/1000 | Loss: 0.00015836
Iteration 63/1000 | Loss: 0.00019982
Iteration 64/1000 | Loss: 0.00013323
Iteration 65/1000 | Loss: 0.00018802
Iteration 66/1000 | Loss: 0.00016891
Iteration 67/1000 | Loss: 0.00017435
Iteration 68/1000 | Loss: 0.00015027
Iteration 69/1000 | Loss: 0.00011900
Iteration 70/1000 | Loss: 0.00013079
Iteration 71/1000 | Loss: 0.00002996
Iteration 72/1000 | Loss: 0.00011687
Iteration 73/1000 | Loss: 0.00009587
Iteration 74/1000 | Loss: 0.00012100
Iteration 75/1000 | Loss: 0.00007892
Iteration 76/1000 | Loss: 0.00012184
Iteration 77/1000 | Loss: 0.00011575
Iteration 78/1000 | Loss: 0.00003571
Iteration 79/1000 | Loss: 0.00003257
Iteration 80/1000 | Loss: 0.00003103
Iteration 81/1000 | Loss: 0.00004169
Iteration 82/1000 | Loss: 0.00004222
Iteration 83/1000 | Loss: 0.00011492
Iteration 84/1000 | Loss: 0.00003392
Iteration 85/1000 | Loss: 0.00008408
Iteration 86/1000 | Loss: 0.00008325
Iteration 87/1000 | Loss: 0.00003448
Iteration 88/1000 | Loss: 0.00003010
Iteration 89/1000 | Loss: 0.00015590
Iteration 90/1000 | Loss: 0.00016037
Iteration 91/1000 | Loss: 0.00002813
Iteration 92/1000 | Loss: 0.00002710
Iteration 93/1000 | Loss: 0.00002657
Iteration 94/1000 | Loss: 0.00002609
Iteration 95/1000 | Loss: 0.00009655
Iteration 96/1000 | Loss: 0.00003619
Iteration 97/1000 | Loss: 0.00002914
Iteration 98/1000 | Loss: 0.00006976
Iteration 99/1000 | Loss: 0.00002637
Iteration 100/1000 | Loss: 0.00009333
Iteration 101/1000 | Loss: 0.00006325
Iteration 102/1000 | Loss: 0.00003027
Iteration 103/1000 | Loss: 0.00002866
Iteration 104/1000 | Loss: 0.00012320
Iteration 105/1000 | Loss: 0.00006317
Iteration 106/1000 | Loss: 0.00012154
Iteration 107/1000 | Loss: 0.00010633
Iteration 108/1000 | Loss: 0.00002984
Iteration 109/1000 | Loss: 0.00016107
Iteration 110/1000 | Loss: 0.00012199
Iteration 111/1000 | Loss: 0.00004269
Iteration 112/1000 | Loss: 0.00003737
Iteration 113/1000 | Loss: 0.00003234
Iteration 114/1000 | Loss: 0.00002974
Iteration 115/1000 | Loss: 0.00002880
Iteration 116/1000 | Loss: 0.00002834
Iteration 117/1000 | Loss: 0.00003625
Iteration 118/1000 | Loss: 0.00002730
Iteration 119/1000 | Loss: 0.00002633
Iteration 120/1000 | Loss: 0.00002577
Iteration 121/1000 | Loss: 0.00002543
Iteration 122/1000 | Loss: 0.00002516
Iteration 123/1000 | Loss: 0.00002482
Iteration 124/1000 | Loss: 0.00002459
Iteration 125/1000 | Loss: 0.00002448
Iteration 126/1000 | Loss: 0.00002441
Iteration 127/1000 | Loss: 0.00002438
Iteration 128/1000 | Loss: 0.00002434
Iteration 129/1000 | Loss: 0.00002427
Iteration 130/1000 | Loss: 0.00002426
Iteration 131/1000 | Loss: 0.00002426
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002425
Iteration 134/1000 | Loss: 0.00002424
Iteration 135/1000 | Loss: 0.00002424
Iteration 136/1000 | Loss: 0.00002424
Iteration 137/1000 | Loss: 0.00002423
Iteration 138/1000 | Loss: 0.00002423
Iteration 139/1000 | Loss: 0.00002423
Iteration 140/1000 | Loss: 0.00002422
Iteration 141/1000 | Loss: 0.00002422
Iteration 142/1000 | Loss: 0.00002422
Iteration 143/1000 | Loss: 0.00002422
Iteration 144/1000 | Loss: 0.00002422
Iteration 145/1000 | Loss: 0.00002421
Iteration 146/1000 | Loss: 0.00002421
Iteration 147/1000 | Loss: 0.00002421
Iteration 148/1000 | Loss: 0.00002421
Iteration 149/1000 | Loss: 0.00002420
Iteration 150/1000 | Loss: 0.00002420
Iteration 151/1000 | Loss: 0.00002420
Iteration 152/1000 | Loss: 0.00002420
Iteration 153/1000 | Loss: 0.00002420
Iteration 154/1000 | Loss: 0.00002419
Iteration 155/1000 | Loss: 0.00002419
Iteration 156/1000 | Loss: 0.00002419
Iteration 157/1000 | Loss: 0.00002419
Iteration 158/1000 | Loss: 0.00002419
Iteration 159/1000 | Loss: 0.00002418
Iteration 160/1000 | Loss: 0.00002418
Iteration 161/1000 | Loss: 0.00002418
Iteration 162/1000 | Loss: 0.00002417
Iteration 163/1000 | Loss: 0.00002417
Iteration 164/1000 | Loss: 0.00002417
Iteration 165/1000 | Loss: 0.00002417
Iteration 166/1000 | Loss: 0.00002416
Iteration 167/1000 | Loss: 0.00002416
Iteration 168/1000 | Loss: 0.00002415
Iteration 169/1000 | Loss: 0.00002415
Iteration 170/1000 | Loss: 0.00002415
Iteration 171/1000 | Loss: 0.00002415
Iteration 172/1000 | Loss: 0.00002414
Iteration 173/1000 | Loss: 0.00002414
Iteration 174/1000 | Loss: 0.00002414
Iteration 175/1000 | Loss: 0.00002414
Iteration 176/1000 | Loss: 0.00002413
Iteration 177/1000 | Loss: 0.00002413
Iteration 178/1000 | Loss: 0.00002413
Iteration 179/1000 | Loss: 0.00002413
Iteration 180/1000 | Loss: 0.00002413
Iteration 181/1000 | Loss: 0.00002412
Iteration 182/1000 | Loss: 0.00002412
Iteration 183/1000 | Loss: 0.00002412
Iteration 184/1000 | Loss: 0.00002412
Iteration 185/1000 | Loss: 0.00002412
Iteration 186/1000 | Loss: 0.00002412
Iteration 187/1000 | Loss: 0.00002412
Iteration 188/1000 | Loss: 0.00002412
Iteration 189/1000 | Loss: 0.00002412
Iteration 190/1000 | Loss: 0.00002412
Iteration 191/1000 | Loss: 0.00002412
Iteration 192/1000 | Loss: 0.00002412
Iteration 193/1000 | Loss: 0.00002411
Iteration 194/1000 | Loss: 0.00002411
Iteration 195/1000 | Loss: 0.00002411
Iteration 196/1000 | Loss: 0.00002411
Iteration 197/1000 | Loss: 0.00002411
Iteration 198/1000 | Loss: 0.00002411
Iteration 199/1000 | Loss: 0.00002411
Iteration 200/1000 | Loss: 0.00002411
Iteration 201/1000 | Loss: 0.00002411
Iteration 202/1000 | Loss: 0.00002411
Iteration 203/1000 | Loss: 0.00002411
Iteration 204/1000 | Loss: 0.00002411
Iteration 205/1000 | Loss: 0.00002411
Iteration 206/1000 | Loss: 0.00002411
Iteration 207/1000 | Loss: 0.00002411
Iteration 208/1000 | Loss: 0.00002411
Iteration 209/1000 | Loss: 0.00002411
Iteration 210/1000 | Loss: 0.00002411
Iteration 211/1000 | Loss: 0.00002411
Iteration 212/1000 | Loss: 0.00002411
Iteration 213/1000 | Loss: 0.00002411
Iteration 214/1000 | Loss: 0.00002411
Iteration 215/1000 | Loss: 0.00002411
Iteration 216/1000 | Loss: 0.00002411
Iteration 217/1000 | Loss: 0.00002411
Iteration 218/1000 | Loss: 0.00002411
Iteration 219/1000 | Loss: 0.00002411
Iteration 220/1000 | Loss: 0.00002411
Iteration 221/1000 | Loss: 0.00002411
Iteration 222/1000 | Loss: 0.00002411
Iteration 223/1000 | Loss: 0.00002411
Iteration 224/1000 | Loss: 0.00002411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.4105531338136643e-05, 2.4105531338136643e-05, 2.4105531338136643e-05, 2.4105531338136643e-05, 2.4105531338136643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4105531338136643e-05

Optimization complete. Final v2v error: 4.133703708648682 mm

Highest mean error: 5.232122421264648 mm for frame 26

Lowest mean error: 3.383934736251831 mm for frame 110

Saving results

Total time: 254.38897252082825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941045
Iteration 2/25 | Loss: 0.00185191
Iteration 3/25 | Loss: 0.00150792
Iteration 4/25 | Loss: 0.00148552
Iteration 5/25 | Loss: 0.00147835
Iteration 6/25 | Loss: 0.00147776
Iteration 7/25 | Loss: 0.00147776
Iteration 8/25 | Loss: 0.00147776
Iteration 9/25 | Loss: 0.00147776
Iteration 10/25 | Loss: 0.00147776
Iteration 11/25 | Loss: 0.00147776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001477758982218802, 0.001477758982218802, 0.001477758982218802, 0.001477758982218802, 0.001477758982218802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001477758982218802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82848454
Iteration 2/25 | Loss: 0.00101678
Iteration 3/25 | Loss: 0.00101678
Iteration 4/25 | Loss: 0.00101678
Iteration 5/25 | Loss: 0.00101678
Iteration 6/25 | Loss: 0.00101678
Iteration 7/25 | Loss: 0.00101678
Iteration 8/25 | Loss: 0.00101678
Iteration 9/25 | Loss: 0.00101678
Iteration 10/25 | Loss: 0.00101678
Iteration 11/25 | Loss: 0.00101678
Iteration 12/25 | Loss: 0.00101678
Iteration 13/25 | Loss: 0.00101678
Iteration 14/25 | Loss: 0.00101678
Iteration 15/25 | Loss: 0.00101678
Iteration 16/25 | Loss: 0.00101678
Iteration 17/25 | Loss: 0.00101678
Iteration 18/25 | Loss: 0.00101678
Iteration 19/25 | Loss: 0.00101678
Iteration 20/25 | Loss: 0.00101678
Iteration 21/25 | Loss: 0.00101678
Iteration 22/25 | Loss: 0.00101678
Iteration 23/25 | Loss: 0.00101678
Iteration 24/25 | Loss: 0.00101678
Iteration 25/25 | Loss: 0.00101678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101678
Iteration 2/1000 | Loss: 0.00006243
Iteration 3/1000 | Loss: 0.00004641
Iteration 4/1000 | Loss: 0.00004188
Iteration 5/1000 | Loss: 0.00003957
Iteration 6/1000 | Loss: 0.00003838
Iteration 7/1000 | Loss: 0.00003732
Iteration 8/1000 | Loss: 0.00003672
Iteration 9/1000 | Loss: 0.00003613
Iteration 10/1000 | Loss: 0.00003575
Iteration 11/1000 | Loss: 0.00003533
Iteration 12/1000 | Loss: 0.00003496
Iteration 13/1000 | Loss: 0.00003466
Iteration 14/1000 | Loss: 0.00003438
Iteration 15/1000 | Loss: 0.00003408
Iteration 16/1000 | Loss: 0.00003381
Iteration 17/1000 | Loss: 0.00003360
Iteration 18/1000 | Loss: 0.00003349
Iteration 19/1000 | Loss: 0.00003340
Iteration 20/1000 | Loss: 0.00003338
Iteration 21/1000 | Loss: 0.00003332
Iteration 22/1000 | Loss: 0.00003326
Iteration 23/1000 | Loss: 0.00003321
Iteration 24/1000 | Loss: 0.00003316
Iteration 25/1000 | Loss: 0.00003312
Iteration 26/1000 | Loss: 0.00003308
Iteration 27/1000 | Loss: 0.00003307
Iteration 28/1000 | Loss: 0.00003305
Iteration 29/1000 | Loss: 0.00003305
Iteration 30/1000 | Loss: 0.00003304
Iteration 31/1000 | Loss: 0.00003301
Iteration 32/1000 | Loss: 0.00003301
Iteration 33/1000 | Loss: 0.00003300
Iteration 34/1000 | Loss: 0.00003300
Iteration 35/1000 | Loss: 0.00003300
Iteration 36/1000 | Loss: 0.00003300
Iteration 37/1000 | Loss: 0.00003300
Iteration 38/1000 | Loss: 0.00003300
Iteration 39/1000 | Loss: 0.00003298
Iteration 40/1000 | Loss: 0.00003298
Iteration 41/1000 | Loss: 0.00003297
Iteration 42/1000 | Loss: 0.00003297
Iteration 43/1000 | Loss: 0.00003297
Iteration 44/1000 | Loss: 0.00003297
Iteration 45/1000 | Loss: 0.00003297
Iteration 46/1000 | Loss: 0.00003297
Iteration 47/1000 | Loss: 0.00003297
Iteration 48/1000 | Loss: 0.00003297
Iteration 49/1000 | Loss: 0.00003296
Iteration 50/1000 | Loss: 0.00003296
Iteration 51/1000 | Loss: 0.00003296
Iteration 52/1000 | Loss: 0.00003296
Iteration 53/1000 | Loss: 0.00003296
Iteration 54/1000 | Loss: 0.00003296
Iteration 55/1000 | Loss: 0.00003296
Iteration 56/1000 | Loss: 0.00003294
Iteration 57/1000 | Loss: 0.00003294
Iteration 58/1000 | Loss: 0.00003294
Iteration 59/1000 | Loss: 0.00003294
Iteration 60/1000 | Loss: 0.00003294
Iteration 61/1000 | Loss: 0.00003294
Iteration 62/1000 | Loss: 0.00003294
Iteration 63/1000 | Loss: 0.00003294
Iteration 64/1000 | Loss: 0.00003294
Iteration 65/1000 | Loss: 0.00003294
Iteration 66/1000 | Loss: 0.00003294
Iteration 67/1000 | Loss: 0.00003293
Iteration 68/1000 | Loss: 0.00003293
Iteration 69/1000 | Loss: 0.00003293
Iteration 70/1000 | Loss: 0.00003293
Iteration 71/1000 | Loss: 0.00003292
Iteration 72/1000 | Loss: 0.00003292
Iteration 73/1000 | Loss: 0.00003292
Iteration 74/1000 | Loss: 0.00003292
Iteration 75/1000 | Loss: 0.00003292
Iteration 76/1000 | Loss: 0.00003291
Iteration 77/1000 | Loss: 0.00003291
Iteration 78/1000 | Loss: 0.00003291
Iteration 79/1000 | Loss: 0.00003291
Iteration 80/1000 | Loss: 0.00003290
Iteration 81/1000 | Loss: 0.00003290
Iteration 82/1000 | Loss: 0.00003290
Iteration 83/1000 | Loss: 0.00003290
Iteration 84/1000 | Loss: 0.00003290
Iteration 85/1000 | Loss: 0.00003290
Iteration 86/1000 | Loss: 0.00003290
Iteration 87/1000 | Loss: 0.00003290
Iteration 88/1000 | Loss: 0.00003290
Iteration 89/1000 | Loss: 0.00003290
Iteration 90/1000 | Loss: 0.00003290
Iteration 91/1000 | Loss: 0.00003290
Iteration 92/1000 | Loss: 0.00003290
Iteration 93/1000 | Loss: 0.00003289
Iteration 94/1000 | Loss: 0.00003289
Iteration 95/1000 | Loss: 0.00003289
Iteration 96/1000 | Loss: 0.00003289
Iteration 97/1000 | Loss: 0.00003289
Iteration 98/1000 | Loss: 0.00003289
Iteration 99/1000 | Loss: 0.00003289
Iteration 100/1000 | Loss: 0.00003289
Iteration 101/1000 | Loss: 0.00003289
Iteration 102/1000 | Loss: 0.00003288
Iteration 103/1000 | Loss: 0.00003288
Iteration 104/1000 | Loss: 0.00003288
Iteration 105/1000 | Loss: 0.00003288
Iteration 106/1000 | Loss: 0.00003288
Iteration 107/1000 | Loss: 0.00003288
Iteration 108/1000 | Loss: 0.00003288
Iteration 109/1000 | Loss: 0.00003288
Iteration 110/1000 | Loss: 0.00003288
Iteration 111/1000 | Loss: 0.00003288
Iteration 112/1000 | Loss: 0.00003287
Iteration 113/1000 | Loss: 0.00003287
Iteration 114/1000 | Loss: 0.00003287
Iteration 115/1000 | Loss: 0.00003287
Iteration 116/1000 | Loss: 0.00003287
Iteration 117/1000 | Loss: 0.00003287
Iteration 118/1000 | Loss: 0.00003287
Iteration 119/1000 | Loss: 0.00003287
Iteration 120/1000 | Loss: 0.00003287
Iteration 121/1000 | Loss: 0.00003287
Iteration 122/1000 | Loss: 0.00003287
Iteration 123/1000 | Loss: 0.00003287
Iteration 124/1000 | Loss: 0.00003287
Iteration 125/1000 | Loss: 0.00003287
Iteration 126/1000 | Loss: 0.00003287
Iteration 127/1000 | Loss: 0.00003286
Iteration 128/1000 | Loss: 0.00003286
Iteration 129/1000 | Loss: 0.00003286
Iteration 130/1000 | Loss: 0.00003286
Iteration 131/1000 | Loss: 0.00003286
Iteration 132/1000 | Loss: 0.00003286
Iteration 133/1000 | Loss: 0.00003286
Iteration 134/1000 | Loss: 0.00003286
Iteration 135/1000 | Loss: 0.00003286
Iteration 136/1000 | Loss: 0.00003286
Iteration 137/1000 | Loss: 0.00003285
Iteration 138/1000 | Loss: 0.00003285
Iteration 139/1000 | Loss: 0.00003285
Iteration 140/1000 | Loss: 0.00003285
Iteration 141/1000 | Loss: 0.00003285
Iteration 142/1000 | Loss: 0.00003285
Iteration 143/1000 | Loss: 0.00003284
Iteration 144/1000 | Loss: 0.00003284
Iteration 145/1000 | Loss: 0.00003284
Iteration 146/1000 | Loss: 0.00003284
Iteration 147/1000 | Loss: 0.00003284
Iteration 148/1000 | Loss: 0.00003284
Iteration 149/1000 | Loss: 0.00003284
Iteration 150/1000 | Loss: 0.00003284
Iteration 151/1000 | Loss: 0.00003284
Iteration 152/1000 | Loss: 0.00003284
Iteration 153/1000 | Loss: 0.00003283
Iteration 154/1000 | Loss: 0.00003283
Iteration 155/1000 | Loss: 0.00003283
Iteration 156/1000 | Loss: 0.00003283
Iteration 157/1000 | Loss: 0.00003283
Iteration 158/1000 | Loss: 0.00003282
Iteration 159/1000 | Loss: 0.00003282
Iteration 160/1000 | Loss: 0.00003282
Iteration 161/1000 | Loss: 0.00003282
Iteration 162/1000 | Loss: 0.00003282
Iteration 163/1000 | Loss: 0.00003282
Iteration 164/1000 | Loss: 0.00003282
Iteration 165/1000 | Loss: 0.00003282
Iteration 166/1000 | Loss: 0.00003282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.282040051999502e-05, 3.282040051999502e-05, 3.282040051999502e-05, 3.282040051999502e-05, 3.282040051999502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.282040051999502e-05

Optimization complete. Final v2v error: 4.790318965911865 mm

Highest mean error: 5.744301795959473 mm for frame 88

Lowest mean error: 3.8954169750213623 mm for frame 0

Saving results

Total time: 56.262765645980835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818773
Iteration 2/25 | Loss: 0.00136295
Iteration 3/25 | Loss: 0.00127241
Iteration 4/25 | Loss: 0.00125998
Iteration 5/25 | Loss: 0.00125775
Iteration 6/25 | Loss: 0.00125775
Iteration 7/25 | Loss: 0.00125775
Iteration 8/25 | Loss: 0.00125775
Iteration 9/25 | Loss: 0.00125775
Iteration 10/25 | Loss: 0.00125775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012577479938045144, 0.0012577479938045144, 0.0012577479938045144, 0.0012577479938045144, 0.0012577479938045144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012577479938045144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39667821
Iteration 2/25 | Loss: 0.00075903
Iteration 3/25 | Loss: 0.00075903
Iteration 4/25 | Loss: 0.00075902
Iteration 5/25 | Loss: 0.00075902
Iteration 6/25 | Loss: 0.00075902
Iteration 7/25 | Loss: 0.00075902
Iteration 8/25 | Loss: 0.00075902
Iteration 9/25 | Loss: 0.00075902
Iteration 10/25 | Loss: 0.00075902
Iteration 11/25 | Loss: 0.00075902
Iteration 12/25 | Loss: 0.00075902
Iteration 13/25 | Loss: 0.00075902
Iteration 14/25 | Loss: 0.00075902
Iteration 15/25 | Loss: 0.00075902
Iteration 16/25 | Loss: 0.00075902
Iteration 17/25 | Loss: 0.00075902
Iteration 18/25 | Loss: 0.00075902
Iteration 19/25 | Loss: 0.00075902
Iteration 20/25 | Loss: 0.00075902
Iteration 21/25 | Loss: 0.00075902
Iteration 22/25 | Loss: 0.00075902
Iteration 23/25 | Loss: 0.00075902
Iteration 24/25 | Loss: 0.00075902
Iteration 25/25 | Loss: 0.00075902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075902
Iteration 2/1000 | Loss: 0.00002422
Iteration 3/1000 | Loss: 0.00001737
Iteration 4/1000 | Loss: 0.00001578
Iteration 5/1000 | Loss: 0.00001464
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001351
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001269
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001268
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001266
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001264
Iteration 24/1000 | Loss: 0.00001264
Iteration 25/1000 | Loss: 0.00001264
Iteration 26/1000 | Loss: 0.00001263
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001252
Iteration 29/1000 | Loss: 0.00001252
Iteration 30/1000 | Loss: 0.00001251
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001242
Iteration 34/1000 | Loss: 0.00001237
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001229
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001226
Iteration 43/1000 | Loss: 0.00001226
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001219
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001216
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001215
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001213
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001208
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001207
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001202
Iteration 93/1000 | Loss: 0.00001202
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001201
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001198
Iteration 102/1000 | Loss: 0.00001198
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001197
Iteration 109/1000 | Loss: 0.00001197
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001190
Iteration 134/1000 | Loss: 0.00001190
Iteration 135/1000 | Loss: 0.00001190
Iteration 136/1000 | Loss: 0.00001190
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001189
Iteration 139/1000 | Loss: 0.00001189
Iteration 140/1000 | Loss: 0.00001189
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001187
Iteration 149/1000 | Loss: 0.00001187
Iteration 150/1000 | Loss: 0.00001187
Iteration 151/1000 | Loss: 0.00001187
Iteration 152/1000 | Loss: 0.00001187
Iteration 153/1000 | Loss: 0.00001187
Iteration 154/1000 | Loss: 0.00001187
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001187
Iteration 157/1000 | Loss: 0.00001187
Iteration 158/1000 | Loss: 0.00001187
Iteration 159/1000 | Loss: 0.00001186
Iteration 160/1000 | Loss: 0.00001186
Iteration 161/1000 | Loss: 0.00001186
Iteration 162/1000 | Loss: 0.00001186
Iteration 163/1000 | Loss: 0.00001186
Iteration 164/1000 | Loss: 0.00001186
Iteration 165/1000 | Loss: 0.00001186
Iteration 166/1000 | Loss: 0.00001186
Iteration 167/1000 | Loss: 0.00001186
Iteration 168/1000 | Loss: 0.00001186
Iteration 169/1000 | Loss: 0.00001186
Iteration 170/1000 | Loss: 0.00001186
Iteration 171/1000 | Loss: 0.00001185
Iteration 172/1000 | Loss: 0.00001185
Iteration 173/1000 | Loss: 0.00001185
Iteration 174/1000 | Loss: 0.00001185
Iteration 175/1000 | Loss: 0.00001185
Iteration 176/1000 | Loss: 0.00001185
Iteration 177/1000 | Loss: 0.00001185
Iteration 178/1000 | Loss: 0.00001185
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001185
Iteration 182/1000 | Loss: 0.00001185
Iteration 183/1000 | Loss: 0.00001185
Iteration 184/1000 | Loss: 0.00001185
Iteration 185/1000 | Loss: 0.00001185
Iteration 186/1000 | Loss: 0.00001185
Iteration 187/1000 | Loss: 0.00001185
Iteration 188/1000 | Loss: 0.00001185
Iteration 189/1000 | Loss: 0.00001185
Iteration 190/1000 | Loss: 0.00001185
Iteration 191/1000 | Loss: 0.00001185
Iteration 192/1000 | Loss: 0.00001185
Iteration 193/1000 | Loss: 0.00001185
Iteration 194/1000 | Loss: 0.00001185
Iteration 195/1000 | Loss: 0.00001185
Iteration 196/1000 | Loss: 0.00001185
Iteration 197/1000 | Loss: 0.00001185
Iteration 198/1000 | Loss: 0.00001185
Iteration 199/1000 | Loss: 0.00001185
Iteration 200/1000 | Loss: 0.00001185
Iteration 201/1000 | Loss: 0.00001185
Iteration 202/1000 | Loss: 0.00001185
Iteration 203/1000 | Loss: 0.00001185
Iteration 204/1000 | Loss: 0.00001185
Iteration 205/1000 | Loss: 0.00001185
Iteration 206/1000 | Loss: 0.00001185
Iteration 207/1000 | Loss: 0.00001185
Iteration 208/1000 | Loss: 0.00001185
Iteration 209/1000 | Loss: 0.00001185
Iteration 210/1000 | Loss: 0.00001185
Iteration 211/1000 | Loss: 0.00001185
Iteration 212/1000 | Loss: 0.00001185
Iteration 213/1000 | Loss: 0.00001185
Iteration 214/1000 | Loss: 0.00001185
Iteration 215/1000 | Loss: 0.00001185
Iteration 216/1000 | Loss: 0.00001185
Iteration 217/1000 | Loss: 0.00001185
Iteration 218/1000 | Loss: 0.00001185
Iteration 219/1000 | Loss: 0.00001185
Iteration 220/1000 | Loss: 0.00001185
Iteration 221/1000 | Loss: 0.00001185
Iteration 222/1000 | Loss: 0.00001185
Iteration 223/1000 | Loss: 0.00001185
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.1853762771352194e-05, 1.1853762771352194e-05, 1.1853762771352194e-05, 1.1853762771352194e-05, 1.1853762771352194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1853762771352194e-05

Optimization complete. Final v2v error: 2.950052499771118 mm

Highest mean error: 3.0719029903411865 mm for frame 143

Lowest mean error: 2.766958713531494 mm for frame 251

Saving results

Total time: 46.63118004798889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771715
Iteration 2/25 | Loss: 0.00171891
Iteration 3/25 | Loss: 0.00140074
Iteration 4/25 | Loss: 0.00137470
Iteration 5/25 | Loss: 0.00136940
Iteration 6/25 | Loss: 0.00136847
Iteration 7/25 | Loss: 0.00136847
Iteration 8/25 | Loss: 0.00136847
Iteration 9/25 | Loss: 0.00136847
Iteration 10/25 | Loss: 0.00136847
Iteration 11/25 | Loss: 0.00136847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00136846536770463, 0.00136846536770463, 0.00136846536770463, 0.00136846536770463, 0.00136846536770463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00136846536770463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37934041
Iteration 2/25 | Loss: 0.00073270
Iteration 3/25 | Loss: 0.00073269
Iteration 4/25 | Loss: 0.00073269
Iteration 5/25 | Loss: 0.00073269
Iteration 6/25 | Loss: 0.00073269
Iteration 7/25 | Loss: 0.00073268
Iteration 8/25 | Loss: 0.00073268
Iteration 9/25 | Loss: 0.00073268
Iteration 10/25 | Loss: 0.00073268
Iteration 11/25 | Loss: 0.00073268
Iteration 12/25 | Loss: 0.00073268
Iteration 13/25 | Loss: 0.00073268
Iteration 14/25 | Loss: 0.00073268
Iteration 15/25 | Loss: 0.00073268
Iteration 16/25 | Loss: 0.00073268
Iteration 17/25 | Loss: 0.00073268
Iteration 18/25 | Loss: 0.00073268
Iteration 19/25 | Loss: 0.00073268
Iteration 20/25 | Loss: 0.00073268
Iteration 21/25 | Loss: 0.00073268
Iteration 22/25 | Loss: 0.00073268
Iteration 23/25 | Loss: 0.00073268
Iteration 24/25 | Loss: 0.00073268
Iteration 25/25 | Loss: 0.00073268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073268
Iteration 2/1000 | Loss: 0.00003700
Iteration 3/1000 | Loss: 0.00002823
Iteration 4/1000 | Loss: 0.00002615
Iteration 5/1000 | Loss: 0.00002504
Iteration 6/1000 | Loss: 0.00002405
Iteration 7/1000 | Loss: 0.00002359
Iteration 8/1000 | Loss: 0.00002308
Iteration 9/1000 | Loss: 0.00002268
Iteration 10/1000 | Loss: 0.00002243
Iteration 11/1000 | Loss: 0.00002241
Iteration 12/1000 | Loss: 0.00002232
Iteration 13/1000 | Loss: 0.00002212
Iteration 14/1000 | Loss: 0.00002198
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002193
Iteration 20/1000 | Loss: 0.00002192
Iteration 21/1000 | Loss: 0.00002191
Iteration 22/1000 | Loss: 0.00002190
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002188
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002187
Iteration 27/1000 | Loss: 0.00002187
Iteration 28/1000 | Loss: 0.00002185
Iteration 29/1000 | Loss: 0.00002184
Iteration 30/1000 | Loss: 0.00002184
Iteration 31/1000 | Loss: 0.00002184
Iteration 32/1000 | Loss: 0.00002184
Iteration 33/1000 | Loss: 0.00002184
Iteration 34/1000 | Loss: 0.00002184
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002184
Iteration 38/1000 | Loss: 0.00002184
Iteration 39/1000 | Loss: 0.00002183
Iteration 40/1000 | Loss: 0.00002183
Iteration 41/1000 | Loss: 0.00002183
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002182
Iteration 44/1000 | Loss: 0.00002182
Iteration 45/1000 | Loss: 0.00002181
Iteration 46/1000 | Loss: 0.00002181
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002180
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002180
Iteration 54/1000 | Loss: 0.00002180
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002180
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002179
Iteration 62/1000 | Loss: 0.00002179
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Iteration 68/1000 | Loss: 0.00002178
Iteration 69/1000 | Loss: 0.00002178
Iteration 70/1000 | Loss: 0.00002178
Iteration 71/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [2.1778829250251874e-05, 2.1778829250251874e-05, 2.1778829250251874e-05, 2.1778829250251874e-05, 2.1778829250251874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1778829250251874e-05

Optimization complete. Final v2v error: 3.911328077316284 mm

Highest mean error: 4.084198951721191 mm for frame 213

Lowest mean error: 3.6550824642181396 mm for frame 8

Saving results

Total time: 34.14358425140381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454229
Iteration 2/25 | Loss: 0.00138291
Iteration 3/25 | Loss: 0.00130638
Iteration 4/25 | Loss: 0.00129625
Iteration 5/25 | Loss: 0.00129518
Iteration 6/25 | Loss: 0.00129518
Iteration 7/25 | Loss: 0.00129518
Iteration 8/25 | Loss: 0.00129518
Iteration 9/25 | Loss: 0.00129518
Iteration 10/25 | Loss: 0.00129518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001295178895816207, 0.001295178895816207, 0.001295178895816207, 0.001295178895816207, 0.001295178895816207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001295178895816207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41491556
Iteration 2/25 | Loss: 0.00089210
Iteration 3/25 | Loss: 0.00089210
Iteration 4/25 | Loss: 0.00089210
Iteration 5/25 | Loss: 0.00089210
Iteration 6/25 | Loss: 0.00089210
Iteration 7/25 | Loss: 0.00089210
Iteration 8/25 | Loss: 0.00089210
Iteration 9/25 | Loss: 0.00089210
Iteration 10/25 | Loss: 0.00089210
Iteration 11/25 | Loss: 0.00089210
Iteration 12/25 | Loss: 0.00089210
Iteration 13/25 | Loss: 0.00089210
Iteration 14/25 | Loss: 0.00089210
Iteration 15/25 | Loss: 0.00089210
Iteration 16/25 | Loss: 0.00089210
Iteration 17/25 | Loss: 0.00089210
Iteration 18/25 | Loss: 0.00089210
Iteration 19/25 | Loss: 0.00089210
Iteration 20/25 | Loss: 0.00089210
Iteration 21/25 | Loss: 0.00089210
Iteration 22/25 | Loss: 0.00089210
Iteration 23/25 | Loss: 0.00089210
Iteration 24/25 | Loss: 0.00089210
Iteration 25/25 | Loss: 0.00089210

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089210
Iteration 2/1000 | Loss: 0.00002686
Iteration 3/1000 | Loss: 0.00002008
Iteration 4/1000 | Loss: 0.00001833
Iteration 5/1000 | Loss: 0.00001749
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001677
Iteration 8/1000 | Loss: 0.00001663
Iteration 9/1000 | Loss: 0.00001644
Iteration 10/1000 | Loss: 0.00001617
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001564
Iteration 17/1000 | Loss: 0.00001552
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001550
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001546
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001543
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001532
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001528
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001524
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001523
Iteration 44/1000 | Loss: 0.00001523
Iteration 45/1000 | Loss: 0.00001523
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001521
Iteration 49/1000 | Loss: 0.00001521
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001520
Iteration 52/1000 | Loss: 0.00001520
Iteration 53/1000 | Loss: 0.00001519
Iteration 54/1000 | Loss: 0.00001519
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001519
Iteration 57/1000 | Loss: 0.00001519
Iteration 58/1000 | Loss: 0.00001519
Iteration 59/1000 | Loss: 0.00001519
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001518
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001516
Iteration 65/1000 | Loss: 0.00001516
Iteration 66/1000 | Loss: 0.00001516
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001512
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001511
Iteration 78/1000 | Loss: 0.00001511
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001510
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001508
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001507
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001504
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001500
Iteration 112/1000 | Loss: 0.00001499
Iteration 113/1000 | Loss: 0.00001499
Iteration 114/1000 | Loss: 0.00001499
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001496
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001496
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001496
Iteration 131/1000 | Loss: 0.00001496
Iteration 132/1000 | Loss: 0.00001496
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001495
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001494
Iteration 140/1000 | Loss: 0.00001494
Iteration 141/1000 | Loss: 0.00001494
Iteration 142/1000 | Loss: 0.00001494
Iteration 143/1000 | Loss: 0.00001494
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001493
Iteration 146/1000 | Loss: 0.00001493
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001493
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001493
Iteration 151/1000 | Loss: 0.00001493
Iteration 152/1000 | Loss: 0.00001493
Iteration 153/1000 | Loss: 0.00001493
Iteration 154/1000 | Loss: 0.00001493
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001492
Iteration 158/1000 | Loss: 0.00001492
Iteration 159/1000 | Loss: 0.00001491
Iteration 160/1000 | Loss: 0.00001491
Iteration 161/1000 | Loss: 0.00001491
Iteration 162/1000 | Loss: 0.00001491
Iteration 163/1000 | Loss: 0.00001491
Iteration 164/1000 | Loss: 0.00001491
Iteration 165/1000 | Loss: 0.00001491
Iteration 166/1000 | Loss: 0.00001491
Iteration 167/1000 | Loss: 0.00001491
Iteration 168/1000 | Loss: 0.00001491
Iteration 169/1000 | Loss: 0.00001491
Iteration 170/1000 | Loss: 0.00001490
Iteration 171/1000 | Loss: 0.00001490
Iteration 172/1000 | Loss: 0.00001490
Iteration 173/1000 | Loss: 0.00001490
Iteration 174/1000 | Loss: 0.00001490
Iteration 175/1000 | Loss: 0.00001490
Iteration 176/1000 | Loss: 0.00001490
Iteration 177/1000 | Loss: 0.00001490
Iteration 178/1000 | Loss: 0.00001490
Iteration 179/1000 | Loss: 0.00001490
Iteration 180/1000 | Loss: 0.00001490
Iteration 181/1000 | Loss: 0.00001490
Iteration 182/1000 | Loss: 0.00001490
Iteration 183/1000 | Loss: 0.00001490
Iteration 184/1000 | Loss: 0.00001490
Iteration 185/1000 | Loss: 0.00001489
Iteration 186/1000 | Loss: 0.00001489
Iteration 187/1000 | Loss: 0.00001489
Iteration 188/1000 | Loss: 0.00001489
Iteration 189/1000 | Loss: 0.00001489
Iteration 190/1000 | Loss: 0.00001489
Iteration 191/1000 | Loss: 0.00001489
Iteration 192/1000 | Loss: 0.00001489
Iteration 193/1000 | Loss: 0.00001489
Iteration 194/1000 | Loss: 0.00001489
Iteration 195/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.4893968000251334e-05, 1.4893968000251334e-05, 1.4893968000251334e-05, 1.4893968000251334e-05, 1.4893968000251334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4893968000251334e-05

Optimization complete. Final v2v error: 3.23209285736084 mm

Highest mean error: 3.5056047439575195 mm for frame 96

Lowest mean error: 3.0269176959991455 mm for frame 195

Saving results

Total time: 45.48241829872131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057508
Iteration 2/25 | Loss: 0.00197795
Iteration 3/25 | Loss: 0.00156645
Iteration 4/25 | Loss: 0.00143776
Iteration 5/25 | Loss: 0.00137359
Iteration 6/25 | Loss: 0.00134418
Iteration 7/25 | Loss: 0.00133641
Iteration 8/25 | Loss: 0.00132557
Iteration 9/25 | Loss: 0.00131268
Iteration 10/25 | Loss: 0.00130820
Iteration 11/25 | Loss: 0.00130135
Iteration 12/25 | Loss: 0.00129913
Iteration 13/25 | Loss: 0.00130174
Iteration 14/25 | Loss: 0.00129804
Iteration 15/25 | Loss: 0.00129413
Iteration 16/25 | Loss: 0.00129340
Iteration 17/25 | Loss: 0.00129339
Iteration 18/25 | Loss: 0.00129338
Iteration 19/25 | Loss: 0.00129338
Iteration 20/25 | Loss: 0.00129338
Iteration 21/25 | Loss: 0.00129338
Iteration 22/25 | Loss: 0.00129338
Iteration 23/25 | Loss: 0.00129338
Iteration 24/25 | Loss: 0.00129338
Iteration 25/25 | Loss: 0.00129338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.55389977
Iteration 2/25 | Loss: 0.00088597
Iteration 3/25 | Loss: 0.00083369
Iteration 4/25 | Loss: 0.00083369
Iteration 5/25 | Loss: 0.00083369
Iteration 6/25 | Loss: 0.00083369
Iteration 7/25 | Loss: 0.00083368
Iteration 8/25 | Loss: 0.00083368
Iteration 9/25 | Loss: 0.00083368
Iteration 10/25 | Loss: 0.00083368
Iteration 11/25 | Loss: 0.00083368
Iteration 12/25 | Loss: 0.00083368
Iteration 13/25 | Loss: 0.00083368
Iteration 14/25 | Loss: 0.00083368
Iteration 15/25 | Loss: 0.00083368
Iteration 16/25 | Loss: 0.00083368
Iteration 17/25 | Loss: 0.00083368
Iteration 18/25 | Loss: 0.00083368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008336842292919755, 0.0008336842292919755, 0.0008336842292919755, 0.0008336842292919755, 0.0008336842292919755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008336842292919755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083368
Iteration 2/1000 | Loss: 0.00006466
Iteration 3/1000 | Loss: 0.00002279
Iteration 4/1000 | Loss: 0.00008120
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00005126
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00008174
Iteration 9/1000 | Loss: 0.00003664
Iteration 10/1000 | Loss: 0.00001876
Iteration 11/1000 | Loss: 0.00004775
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00003168
Iteration 14/1000 | Loss: 0.00002135
Iteration 15/1000 | Loss: 0.00002498
Iteration 16/1000 | Loss: 0.00001755
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00002206
Iteration 19/1000 | Loss: 0.00002780
Iteration 20/1000 | Loss: 0.00010115
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00001996
Iteration 23/1000 | Loss: 0.00001696
Iteration 24/1000 | Loss: 0.00001696
Iteration 25/1000 | Loss: 0.00001695
Iteration 26/1000 | Loss: 0.00001693
Iteration 27/1000 | Loss: 0.00001693
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001692
Iteration 30/1000 | Loss: 0.00001692
Iteration 31/1000 | Loss: 0.00001692
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001692
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001692
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001691
Iteration 41/1000 | Loss: 0.00002095
Iteration 42/1000 | Loss: 0.00002329
Iteration 43/1000 | Loss: 0.00001913
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001688
Iteration 49/1000 | Loss: 0.00001688
Iteration 50/1000 | Loss: 0.00001688
Iteration 51/1000 | Loss: 0.00001687
Iteration 52/1000 | Loss: 0.00001687
Iteration 53/1000 | Loss: 0.00001687
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001686
Iteration 57/1000 | Loss: 0.00001686
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001686
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00003190
Iteration 81/1000 | Loss: 0.00001672
Iteration 82/1000 | Loss: 0.00003444
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001665
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001664
Iteration 87/1000 | Loss: 0.00001664
Iteration 88/1000 | Loss: 0.00001664
Iteration 89/1000 | Loss: 0.00001663
Iteration 90/1000 | Loss: 0.00001663
Iteration 91/1000 | Loss: 0.00001663
Iteration 92/1000 | Loss: 0.00001663
Iteration 93/1000 | Loss: 0.00001661
Iteration 94/1000 | Loss: 0.00001661
Iteration 95/1000 | Loss: 0.00001661
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001660
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001658
Iteration 112/1000 | Loss: 0.00001658
Iteration 113/1000 | Loss: 0.00001657
Iteration 114/1000 | Loss: 0.00001657
Iteration 115/1000 | Loss: 0.00001657
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001656
Iteration 118/1000 | Loss: 0.00001656
Iteration 119/1000 | Loss: 0.00001656
Iteration 120/1000 | Loss: 0.00001656
Iteration 121/1000 | Loss: 0.00001656
Iteration 122/1000 | Loss: 0.00001656
Iteration 123/1000 | Loss: 0.00001655
Iteration 124/1000 | Loss: 0.00001655
Iteration 125/1000 | Loss: 0.00001655
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00002777
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001654
Iteration 130/1000 | Loss: 0.00001654
Iteration 131/1000 | Loss: 0.00001654
Iteration 132/1000 | Loss: 0.00001654
Iteration 133/1000 | Loss: 0.00001654
Iteration 134/1000 | Loss: 0.00001654
Iteration 135/1000 | Loss: 0.00001653
Iteration 136/1000 | Loss: 0.00001653
Iteration 137/1000 | Loss: 0.00001653
Iteration 138/1000 | Loss: 0.00001653
Iteration 139/1000 | Loss: 0.00001653
Iteration 140/1000 | Loss: 0.00001653
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001652
Iteration 143/1000 | Loss: 0.00001652
Iteration 144/1000 | Loss: 0.00001652
Iteration 145/1000 | Loss: 0.00001652
Iteration 146/1000 | Loss: 0.00001652
Iteration 147/1000 | Loss: 0.00001652
Iteration 148/1000 | Loss: 0.00001652
Iteration 149/1000 | Loss: 0.00001652
Iteration 150/1000 | Loss: 0.00001651
Iteration 151/1000 | Loss: 0.00001651
Iteration 152/1000 | Loss: 0.00001651
Iteration 153/1000 | Loss: 0.00001651
Iteration 154/1000 | Loss: 0.00001651
Iteration 155/1000 | Loss: 0.00001651
Iteration 156/1000 | Loss: 0.00001651
Iteration 157/1000 | Loss: 0.00001651
Iteration 158/1000 | Loss: 0.00001651
Iteration 159/1000 | Loss: 0.00001651
Iteration 160/1000 | Loss: 0.00001651
Iteration 161/1000 | Loss: 0.00001651
Iteration 162/1000 | Loss: 0.00001650
Iteration 163/1000 | Loss: 0.00001650
Iteration 164/1000 | Loss: 0.00001650
Iteration 165/1000 | Loss: 0.00001650
Iteration 166/1000 | Loss: 0.00001650
Iteration 167/1000 | Loss: 0.00001650
Iteration 168/1000 | Loss: 0.00001650
Iteration 169/1000 | Loss: 0.00001650
Iteration 170/1000 | Loss: 0.00001650
Iteration 171/1000 | Loss: 0.00001650
Iteration 172/1000 | Loss: 0.00001649
Iteration 173/1000 | Loss: 0.00001649
Iteration 174/1000 | Loss: 0.00001649
Iteration 175/1000 | Loss: 0.00001649
Iteration 176/1000 | Loss: 0.00001649
Iteration 177/1000 | Loss: 0.00001649
Iteration 178/1000 | Loss: 0.00001649
Iteration 179/1000 | Loss: 0.00001649
Iteration 180/1000 | Loss: 0.00001649
Iteration 181/1000 | Loss: 0.00001649
Iteration 182/1000 | Loss: 0.00001649
Iteration 183/1000 | Loss: 0.00003010
Iteration 184/1000 | Loss: 0.00001947
Iteration 185/1000 | Loss: 0.00003731
Iteration 186/1000 | Loss: 0.00001649
Iteration 187/1000 | Loss: 0.00001649
Iteration 188/1000 | Loss: 0.00001648
Iteration 189/1000 | Loss: 0.00001648
Iteration 190/1000 | Loss: 0.00001648
Iteration 191/1000 | Loss: 0.00001648
Iteration 192/1000 | Loss: 0.00001648
Iteration 193/1000 | Loss: 0.00001648
Iteration 194/1000 | Loss: 0.00001648
Iteration 195/1000 | Loss: 0.00001905
Iteration 196/1000 | Loss: 0.00002531
Iteration 197/1000 | Loss: 0.00001708
Iteration 198/1000 | Loss: 0.00001650
Iteration 199/1000 | Loss: 0.00001650
Iteration 200/1000 | Loss: 0.00001650
Iteration 201/1000 | Loss: 0.00001650
Iteration 202/1000 | Loss: 0.00001649
Iteration 203/1000 | Loss: 0.00001649
Iteration 204/1000 | Loss: 0.00001649
Iteration 205/1000 | Loss: 0.00001649
Iteration 206/1000 | Loss: 0.00001649
Iteration 207/1000 | Loss: 0.00001649
Iteration 208/1000 | Loss: 0.00001648
Iteration 209/1000 | Loss: 0.00001648
Iteration 210/1000 | Loss: 0.00001648
Iteration 211/1000 | Loss: 0.00001648
Iteration 212/1000 | Loss: 0.00001648
Iteration 213/1000 | Loss: 0.00001648
Iteration 214/1000 | Loss: 0.00001648
Iteration 215/1000 | Loss: 0.00001647
Iteration 216/1000 | Loss: 0.00001647
Iteration 217/1000 | Loss: 0.00001647
Iteration 218/1000 | Loss: 0.00001647
Iteration 219/1000 | Loss: 0.00001647
Iteration 220/1000 | Loss: 0.00001647
Iteration 221/1000 | Loss: 0.00001647
Iteration 222/1000 | Loss: 0.00001647
Iteration 223/1000 | Loss: 0.00001647
Iteration 224/1000 | Loss: 0.00001646
Iteration 225/1000 | Loss: 0.00001646
Iteration 226/1000 | Loss: 0.00001646
Iteration 227/1000 | Loss: 0.00001646
Iteration 228/1000 | Loss: 0.00001646
Iteration 229/1000 | Loss: 0.00001646
Iteration 230/1000 | Loss: 0.00001646
Iteration 231/1000 | Loss: 0.00001646
Iteration 232/1000 | Loss: 0.00001646
Iteration 233/1000 | Loss: 0.00001646
Iteration 234/1000 | Loss: 0.00001646
Iteration 235/1000 | Loss: 0.00001646
Iteration 236/1000 | Loss: 0.00001646
Iteration 237/1000 | Loss: 0.00001646
Iteration 238/1000 | Loss: 0.00001646
Iteration 239/1000 | Loss: 0.00001646
Iteration 240/1000 | Loss: 0.00001646
Iteration 241/1000 | Loss: 0.00001646
Iteration 242/1000 | Loss: 0.00001646
Iteration 243/1000 | Loss: 0.00001646
Iteration 244/1000 | Loss: 0.00001646
Iteration 245/1000 | Loss: 0.00001646
Iteration 246/1000 | Loss: 0.00001646
Iteration 247/1000 | Loss: 0.00001646
Iteration 248/1000 | Loss: 0.00001646
Iteration 249/1000 | Loss: 0.00001646
Iteration 250/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.646191594772972e-05, 1.646191594772972e-05, 1.646191594772972e-05, 1.646191594772972e-05, 1.646191594772972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.646191594772972e-05

Optimization complete. Final v2v error: 3.4417831897735596 mm

Highest mean error: 3.9233648777008057 mm for frame 130

Lowest mean error: 3.111727714538574 mm for frame 95

Saving results

Total time: 108.1098005771637
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831292
Iteration 2/25 | Loss: 0.00176202
Iteration 3/25 | Loss: 0.00143765
Iteration 4/25 | Loss: 0.00141238
Iteration 5/25 | Loss: 0.00140839
Iteration 6/25 | Loss: 0.00140796
Iteration 7/25 | Loss: 0.00140796
Iteration 8/25 | Loss: 0.00140796
Iteration 9/25 | Loss: 0.00140796
Iteration 10/25 | Loss: 0.00140796
Iteration 11/25 | Loss: 0.00140796
Iteration 12/25 | Loss: 0.00140796
Iteration 13/25 | Loss: 0.00140796
Iteration 14/25 | Loss: 0.00140796
Iteration 15/25 | Loss: 0.00140796
Iteration 16/25 | Loss: 0.00140796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014079635730013251, 0.0014079635730013251, 0.0014079635730013251, 0.0014079635730013251, 0.0014079635730013251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014079635730013251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17298448
Iteration 2/25 | Loss: 0.00073734
Iteration 3/25 | Loss: 0.00073734
Iteration 4/25 | Loss: 0.00073734
Iteration 5/25 | Loss: 0.00073734
Iteration 6/25 | Loss: 0.00073734
Iteration 7/25 | Loss: 0.00073734
Iteration 8/25 | Loss: 0.00073734
Iteration 9/25 | Loss: 0.00073734
Iteration 10/25 | Loss: 0.00073734
Iteration 11/25 | Loss: 0.00073734
Iteration 12/25 | Loss: 0.00073734
Iteration 13/25 | Loss: 0.00073734
Iteration 14/25 | Loss: 0.00073734
Iteration 15/25 | Loss: 0.00073734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000737337744794786, 0.000737337744794786, 0.000737337744794786, 0.000737337744794786, 0.000737337744794786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000737337744794786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073734
Iteration 2/1000 | Loss: 0.00005255
Iteration 3/1000 | Loss: 0.00003219
Iteration 4/1000 | Loss: 0.00002798
Iteration 5/1000 | Loss: 0.00002642
Iteration 6/1000 | Loss: 0.00002511
Iteration 7/1000 | Loss: 0.00002394
Iteration 8/1000 | Loss: 0.00002324
Iteration 9/1000 | Loss: 0.00002267
Iteration 10/1000 | Loss: 0.00002219
Iteration 11/1000 | Loss: 0.00002185
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00002132
Iteration 14/1000 | Loss: 0.00002116
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00002107
Iteration 17/1000 | Loss: 0.00002107
Iteration 18/1000 | Loss: 0.00002106
Iteration 19/1000 | Loss: 0.00002106
Iteration 20/1000 | Loss: 0.00002106
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002105
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002104
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00002103
Iteration 29/1000 | Loss: 0.00002103
Iteration 30/1000 | Loss: 0.00002102
Iteration 31/1000 | Loss: 0.00002102
Iteration 32/1000 | Loss: 0.00002102
Iteration 33/1000 | Loss: 0.00002102
Iteration 34/1000 | Loss: 0.00002101
Iteration 35/1000 | Loss: 0.00002101
Iteration 36/1000 | Loss: 0.00002100
Iteration 37/1000 | Loss: 0.00002100
Iteration 38/1000 | Loss: 0.00002100
Iteration 39/1000 | Loss: 0.00002099
Iteration 40/1000 | Loss: 0.00002099
Iteration 41/1000 | Loss: 0.00002099
Iteration 42/1000 | Loss: 0.00002099
Iteration 43/1000 | Loss: 0.00002099
Iteration 44/1000 | Loss: 0.00002099
Iteration 45/1000 | Loss: 0.00002099
Iteration 46/1000 | Loss: 0.00002098
Iteration 47/1000 | Loss: 0.00002098
Iteration 48/1000 | Loss: 0.00002098
Iteration 49/1000 | Loss: 0.00002098
Iteration 50/1000 | Loss: 0.00002098
Iteration 51/1000 | Loss: 0.00002098
Iteration 52/1000 | Loss: 0.00002097
Iteration 53/1000 | Loss: 0.00002097
Iteration 54/1000 | Loss: 0.00002097
Iteration 55/1000 | Loss: 0.00002097
Iteration 56/1000 | Loss: 0.00002097
Iteration 57/1000 | Loss: 0.00002097
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002096
Iteration 61/1000 | Loss: 0.00002096
Iteration 62/1000 | Loss: 0.00002096
Iteration 63/1000 | Loss: 0.00002096
Iteration 64/1000 | Loss: 0.00002096
Iteration 65/1000 | Loss: 0.00002096
Iteration 66/1000 | Loss: 0.00002096
Iteration 67/1000 | Loss: 0.00002096
Iteration 68/1000 | Loss: 0.00002096
Iteration 69/1000 | Loss: 0.00002096
Iteration 70/1000 | Loss: 0.00002096
Iteration 71/1000 | Loss: 0.00002096
Iteration 72/1000 | Loss: 0.00002096
Iteration 73/1000 | Loss: 0.00002096
Iteration 74/1000 | Loss: 0.00002096
Iteration 75/1000 | Loss: 0.00002096
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002096
Iteration 78/1000 | Loss: 0.00002096
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002096
Iteration 84/1000 | Loss: 0.00002096
Iteration 85/1000 | Loss: 0.00002096
Iteration 86/1000 | Loss: 0.00002096
Iteration 87/1000 | Loss: 0.00002096
Iteration 88/1000 | Loss: 0.00002096
Iteration 89/1000 | Loss: 0.00002096
Iteration 90/1000 | Loss: 0.00002096
Iteration 91/1000 | Loss: 0.00002096
Iteration 92/1000 | Loss: 0.00002096
Iteration 93/1000 | Loss: 0.00002096
Iteration 94/1000 | Loss: 0.00002096
Iteration 95/1000 | Loss: 0.00002096
Iteration 96/1000 | Loss: 0.00002096
Iteration 97/1000 | Loss: 0.00002096
Iteration 98/1000 | Loss: 0.00002096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.0958923414582387e-05, 2.0958923414582387e-05, 2.0958923414582387e-05, 2.0958923414582387e-05, 2.0958923414582387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0958923414582387e-05

Optimization complete. Final v2v error: 3.812143564224243 mm

Highest mean error: 4.308845520019531 mm for frame 0

Lowest mean error: 3.4100654125213623 mm for frame 78

Saving results

Total time: 36.65944027900696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822625
Iteration 2/25 | Loss: 0.00162422
Iteration 3/25 | Loss: 0.00139878
Iteration 4/25 | Loss: 0.00137644
Iteration 5/25 | Loss: 0.00137151
Iteration 6/25 | Loss: 0.00138715
Iteration 7/25 | Loss: 0.00138870
Iteration 8/25 | Loss: 0.00137753
Iteration 9/25 | Loss: 0.00137243
Iteration 10/25 | Loss: 0.00136355
Iteration 11/25 | Loss: 0.00135543
Iteration 12/25 | Loss: 0.00135496
Iteration 13/25 | Loss: 0.00134748
Iteration 14/25 | Loss: 0.00134718
Iteration 15/25 | Loss: 0.00134796
Iteration 16/25 | Loss: 0.00134922
Iteration 17/25 | Loss: 0.00134884
Iteration 18/25 | Loss: 0.00134833
Iteration 19/25 | Loss: 0.00134668
Iteration 20/25 | Loss: 0.00134907
Iteration 21/25 | Loss: 0.00134759
Iteration 22/25 | Loss: 0.00134976
Iteration 23/25 | Loss: 0.00134795
Iteration 24/25 | Loss: 0.00134906
Iteration 25/25 | Loss: 0.00134859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.14862919
Iteration 2/25 | Loss: 0.00093325
Iteration 3/25 | Loss: 0.00093317
Iteration 4/25 | Loss: 0.00093317
Iteration 5/25 | Loss: 0.00093317
Iteration 6/25 | Loss: 0.00093317
Iteration 7/25 | Loss: 0.00093317
Iteration 8/25 | Loss: 0.00093317
Iteration 9/25 | Loss: 0.00093317
Iteration 10/25 | Loss: 0.00093317
Iteration 11/25 | Loss: 0.00093317
Iteration 12/25 | Loss: 0.00093317
Iteration 13/25 | Loss: 0.00093317
Iteration 14/25 | Loss: 0.00093317
Iteration 15/25 | Loss: 0.00093317
Iteration 16/25 | Loss: 0.00093317
Iteration 17/25 | Loss: 0.00093317
Iteration 18/25 | Loss: 0.00093317
Iteration 19/25 | Loss: 0.00093317
Iteration 20/25 | Loss: 0.00093317
Iteration 21/25 | Loss: 0.00093317
Iteration 22/25 | Loss: 0.00093317
Iteration 23/25 | Loss: 0.00093317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009331654873676598, 0.0009331654873676598, 0.0009331654873676598, 0.0009331654873676598, 0.0009331654873676598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009331654873676598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093317
Iteration 2/1000 | Loss: 0.00012860
Iteration 3/1000 | Loss: 0.00003912
Iteration 4/1000 | Loss: 0.00004861
Iteration 5/1000 | Loss: 0.00002878
Iteration 6/1000 | Loss: 0.00003439
Iteration 7/1000 | Loss: 0.00004616
Iteration 8/1000 | Loss: 0.00003644
Iteration 9/1000 | Loss: 0.00004126
Iteration 10/1000 | Loss: 0.00004520
Iteration 11/1000 | Loss: 0.00004677
Iteration 12/1000 | Loss: 0.00004750
Iteration 13/1000 | Loss: 0.00005529
Iteration 14/1000 | Loss: 0.00003794
Iteration 15/1000 | Loss: 0.00014546
Iteration 16/1000 | Loss: 0.00010916
Iteration 17/1000 | Loss: 0.00023616
Iteration 18/1000 | Loss: 0.00014883
Iteration 19/1000 | Loss: 0.00010645
Iteration 20/1000 | Loss: 0.00010465
Iteration 21/1000 | Loss: 0.00002791
Iteration 22/1000 | Loss: 0.00002517
Iteration 23/1000 | Loss: 0.00004615
Iteration 24/1000 | Loss: 0.00007700
Iteration 25/1000 | Loss: 0.00015095
Iteration 26/1000 | Loss: 0.00009375
Iteration 27/1000 | Loss: 0.00080670
Iteration 28/1000 | Loss: 0.00014464
Iteration 29/1000 | Loss: 0.00019325
Iteration 30/1000 | Loss: 0.00009581
Iteration 31/1000 | Loss: 0.00006195
Iteration 32/1000 | Loss: 0.00002595
Iteration 33/1000 | Loss: 0.00002334
Iteration 34/1000 | Loss: 0.00002255
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002298
Iteration 37/1000 | Loss: 0.00002134
Iteration 38/1000 | Loss: 0.00002102
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002025
Iteration 41/1000 | Loss: 0.00001994
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001937
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001912
Iteration 47/1000 | Loss: 0.00001855
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001957
Iteration 52/1000 | Loss: 0.00002259
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001854
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001829
Iteration 69/1000 | Loss: 0.00001829
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001828
Iteration 73/1000 | Loss: 0.00001828
Iteration 74/1000 | Loss: 0.00001828
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001830
Iteration 81/1000 | Loss: 0.00001823
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001823
Iteration 90/1000 | Loss: 0.00001823
Iteration 91/1000 | Loss: 0.00001823
Iteration 92/1000 | Loss: 0.00001823
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001823
Iteration 95/1000 | Loss: 0.00001822
Iteration 96/1000 | Loss: 0.00001822
Iteration 97/1000 | Loss: 0.00001822
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001822
Iteration 102/1000 | Loss: 0.00001822
Iteration 103/1000 | Loss: 0.00001822
Iteration 104/1000 | Loss: 0.00001821
Iteration 105/1000 | Loss: 0.00001821
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001820
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001820
Iteration 115/1000 | Loss: 0.00001820
Iteration 116/1000 | Loss: 0.00001820
Iteration 117/1000 | Loss: 0.00001820
Iteration 118/1000 | Loss: 0.00001820
Iteration 119/1000 | Loss: 0.00001819
Iteration 120/1000 | Loss: 0.00001819
Iteration 121/1000 | Loss: 0.00001819
Iteration 122/1000 | Loss: 0.00001819
Iteration 123/1000 | Loss: 0.00001819
Iteration 124/1000 | Loss: 0.00001819
Iteration 125/1000 | Loss: 0.00001818
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00001823
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001821
Iteration 130/1000 | Loss: 0.00001817
Iteration 131/1000 | Loss: 0.00001817
Iteration 132/1000 | Loss: 0.00001817
Iteration 133/1000 | Loss: 0.00001817
Iteration 134/1000 | Loss: 0.00001817
Iteration 135/1000 | Loss: 0.00001909
Iteration 136/1000 | Loss: 0.00001909
Iteration 137/1000 | Loss: 0.00001817
Iteration 138/1000 | Loss: 0.00001817
Iteration 139/1000 | Loss: 0.00001817
Iteration 140/1000 | Loss: 0.00001817
Iteration 141/1000 | Loss: 0.00001817
Iteration 142/1000 | Loss: 0.00001817
Iteration 143/1000 | Loss: 0.00001817
Iteration 144/1000 | Loss: 0.00001817
Iteration 145/1000 | Loss: 0.00001816
Iteration 146/1000 | Loss: 0.00001816
Iteration 147/1000 | Loss: 0.00001816
Iteration 148/1000 | Loss: 0.00001816
Iteration 149/1000 | Loss: 0.00001816
Iteration 150/1000 | Loss: 0.00001816
Iteration 151/1000 | Loss: 0.00001816
Iteration 152/1000 | Loss: 0.00001816
Iteration 153/1000 | Loss: 0.00001816
Iteration 154/1000 | Loss: 0.00001816
Iteration 155/1000 | Loss: 0.00001816
Iteration 156/1000 | Loss: 0.00001816
Iteration 157/1000 | Loss: 0.00001816
Iteration 158/1000 | Loss: 0.00001816
Iteration 159/1000 | Loss: 0.00001816
Iteration 160/1000 | Loss: 0.00001816
Iteration 161/1000 | Loss: 0.00001816
Iteration 162/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.816332405724097e-05, 1.816332405724097e-05, 1.816332405724097e-05, 1.816332405724097e-05, 1.816332405724097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.816332405724097e-05

Optimization complete. Final v2v error: 3.5653774738311768 mm

Highest mean error: 4.601108074188232 mm for frame 58

Lowest mean error: 3.0637261867523193 mm for frame 32

Saving results

Total time: 126.73059105873108
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825815
Iteration 2/25 | Loss: 0.00136934
Iteration 3/25 | Loss: 0.00128450
Iteration 4/25 | Loss: 0.00127595
Iteration 5/25 | Loss: 0.00127450
Iteration 6/25 | Loss: 0.00127450
Iteration 7/25 | Loss: 0.00127450
Iteration 8/25 | Loss: 0.00127450
Iteration 9/25 | Loss: 0.00127450
Iteration 10/25 | Loss: 0.00127450
Iteration 11/25 | Loss: 0.00127450
Iteration 12/25 | Loss: 0.00127450
Iteration 13/25 | Loss: 0.00127450
Iteration 14/25 | Loss: 0.00127450
Iteration 15/25 | Loss: 0.00127450
Iteration 16/25 | Loss: 0.00127450
Iteration 17/25 | Loss: 0.00127450
Iteration 18/25 | Loss: 0.00127450
Iteration 19/25 | Loss: 0.00127450
Iteration 20/25 | Loss: 0.00127450
Iteration 21/25 | Loss: 0.00127450
Iteration 22/25 | Loss: 0.00127450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012744978303089738, 0.0012744978303089738, 0.0012744978303089738, 0.0012744978303089738, 0.0012744978303089738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012744978303089738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38998401
Iteration 2/25 | Loss: 0.00076100
Iteration 3/25 | Loss: 0.00076097
Iteration 4/25 | Loss: 0.00076097
Iteration 5/25 | Loss: 0.00076097
Iteration 6/25 | Loss: 0.00076097
Iteration 7/25 | Loss: 0.00076097
Iteration 8/25 | Loss: 0.00076097
Iteration 9/25 | Loss: 0.00076097
Iteration 10/25 | Loss: 0.00076097
Iteration 11/25 | Loss: 0.00076097
Iteration 12/25 | Loss: 0.00076097
Iteration 13/25 | Loss: 0.00076097
Iteration 14/25 | Loss: 0.00076097
Iteration 15/25 | Loss: 0.00076097
Iteration 16/25 | Loss: 0.00076097
Iteration 17/25 | Loss: 0.00076097
Iteration 18/25 | Loss: 0.00076097
Iteration 19/25 | Loss: 0.00076097
Iteration 20/25 | Loss: 0.00076097
Iteration 21/25 | Loss: 0.00076097
Iteration 22/25 | Loss: 0.00076097
Iteration 23/25 | Loss: 0.00076097
Iteration 24/25 | Loss: 0.00076097
Iteration 25/25 | Loss: 0.00076097

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076097
Iteration 2/1000 | Loss: 0.00002715
Iteration 3/1000 | Loss: 0.00001873
Iteration 4/1000 | Loss: 0.00001558
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001336
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001225
Iteration 10/1000 | Loss: 0.00001213
Iteration 11/1000 | Loss: 0.00001190
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001176
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001163
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001153
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001142
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001141
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001139
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001137
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001136
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001135
Iteration 41/1000 | Loss: 0.00001134
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001132
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001128
Iteration 68/1000 | Loss: 0.00001128
Iteration 69/1000 | Loss: 0.00001128
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001127
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001126
Iteration 79/1000 | Loss: 0.00001125
Iteration 80/1000 | Loss: 0.00001125
Iteration 81/1000 | Loss: 0.00001125
Iteration 82/1000 | Loss: 0.00001124
Iteration 83/1000 | Loss: 0.00001124
Iteration 84/1000 | Loss: 0.00001124
Iteration 85/1000 | Loss: 0.00001124
Iteration 86/1000 | Loss: 0.00001124
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001123
Iteration 90/1000 | Loss: 0.00001123
Iteration 91/1000 | Loss: 0.00001123
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001122
Iteration 96/1000 | Loss: 0.00001122
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001121
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001119
Iteration 102/1000 | Loss: 0.00001119
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001119
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001118
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001116
Iteration 116/1000 | Loss: 0.00001116
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001114
Iteration 123/1000 | Loss: 0.00001114
Iteration 124/1000 | Loss: 0.00001114
Iteration 125/1000 | Loss: 0.00001114
Iteration 126/1000 | Loss: 0.00001114
Iteration 127/1000 | Loss: 0.00001114
Iteration 128/1000 | Loss: 0.00001114
Iteration 129/1000 | Loss: 0.00001114
Iteration 130/1000 | Loss: 0.00001113
Iteration 131/1000 | Loss: 0.00001113
Iteration 132/1000 | Loss: 0.00001112
Iteration 133/1000 | Loss: 0.00001112
Iteration 134/1000 | Loss: 0.00001112
Iteration 135/1000 | Loss: 0.00001112
Iteration 136/1000 | Loss: 0.00001112
Iteration 137/1000 | Loss: 0.00001111
Iteration 138/1000 | Loss: 0.00001111
Iteration 139/1000 | Loss: 0.00001111
Iteration 140/1000 | Loss: 0.00001111
Iteration 141/1000 | Loss: 0.00001111
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Iteration 145/1000 | Loss: 0.00001110
Iteration 146/1000 | Loss: 0.00001110
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001109
Iteration 149/1000 | Loss: 0.00001109
Iteration 150/1000 | Loss: 0.00001108
Iteration 151/1000 | Loss: 0.00001108
Iteration 152/1000 | Loss: 0.00001108
Iteration 153/1000 | Loss: 0.00001108
Iteration 154/1000 | Loss: 0.00001108
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001108
Iteration 157/1000 | Loss: 0.00001108
Iteration 158/1000 | Loss: 0.00001108
Iteration 159/1000 | Loss: 0.00001107
Iteration 160/1000 | Loss: 0.00001107
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Iteration 168/1000 | Loss: 0.00001106
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001105
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001105
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001105
Iteration 179/1000 | Loss: 0.00001104
Iteration 180/1000 | Loss: 0.00001104
Iteration 181/1000 | Loss: 0.00001104
Iteration 182/1000 | Loss: 0.00001104
Iteration 183/1000 | Loss: 0.00001104
Iteration 184/1000 | Loss: 0.00001104
Iteration 185/1000 | Loss: 0.00001104
Iteration 186/1000 | Loss: 0.00001104
Iteration 187/1000 | Loss: 0.00001104
Iteration 188/1000 | Loss: 0.00001104
Iteration 189/1000 | Loss: 0.00001103
Iteration 190/1000 | Loss: 0.00001103
Iteration 191/1000 | Loss: 0.00001103
Iteration 192/1000 | Loss: 0.00001103
Iteration 193/1000 | Loss: 0.00001103
Iteration 194/1000 | Loss: 0.00001103
Iteration 195/1000 | Loss: 0.00001103
Iteration 196/1000 | Loss: 0.00001103
Iteration 197/1000 | Loss: 0.00001103
Iteration 198/1000 | Loss: 0.00001103
Iteration 199/1000 | Loss: 0.00001102
Iteration 200/1000 | Loss: 0.00001102
Iteration 201/1000 | Loss: 0.00001102
Iteration 202/1000 | Loss: 0.00001102
Iteration 203/1000 | Loss: 0.00001102
Iteration 204/1000 | Loss: 0.00001102
Iteration 205/1000 | Loss: 0.00001102
Iteration 206/1000 | Loss: 0.00001102
Iteration 207/1000 | Loss: 0.00001102
Iteration 208/1000 | Loss: 0.00001102
Iteration 209/1000 | Loss: 0.00001102
Iteration 210/1000 | Loss: 0.00001101
Iteration 211/1000 | Loss: 0.00001101
Iteration 212/1000 | Loss: 0.00001101
Iteration 213/1000 | Loss: 0.00001101
Iteration 214/1000 | Loss: 0.00001101
Iteration 215/1000 | Loss: 0.00001101
Iteration 216/1000 | Loss: 0.00001101
Iteration 217/1000 | Loss: 0.00001101
Iteration 218/1000 | Loss: 0.00001101
Iteration 219/1000 | Loss: 0.00001101
Iteration 220/1000 | Loss: 0.00001101
Iteration 221/1000 | Loss: 0.00001101
Iteration 222/1000 | Loss: 0.00001101
Iteration 223/1000 | Loss: 0.00001101
Iteration 224/1000 | Loss: 0.00001101
Iteration 225/1000 | Loss: 0.00001101
Iteration 226/1000 | Loss: 0.00001100
Iteration 227/1000 | Loss: 0.00001100
Iteration 228/1000 | Loss: 0.00001100
Iteration 229/1000 | Loss: 0.00001100
Iteration 230/1000 | Loss: 0.00001100
Iteration 231/1000 | Loss: 0.00001100
Iteration 232/1000 | Loss: 0.00001100
Iteration 233/1000 | Loss: 0.00001100
Iteration 234/1000 | Loss: 0.00001100
Iteration 235/1000 | Loss: 0.00001100
Iteration 236/1000 | Loss: 0.00001100
Iteration 237/1000 | Loss: 0.00001100
Iteration 238/1000 | Loss: 0.00001100
Iteration 239/1000 | Loss: 0.00001100
Iteration 240/1000 | Loss: 0.00001100
Iteration 241/1000 | Loss: 0.00001100
Iteration 242/1000 | Loss: 0.00001100
Iteration 243/1000 | Loss: 0.00001100
Iteration 244/1000 | Loss: 0.00001100
Iteration 245/1000 | Loss: 0.00001100
Iteration 246/1000 | Loss: 0.00001100
Iteration 247/1000 | Loss: 0.00001100
Iteration 248/1000 | Loss: 0.00001100
Iteration 249/1000 | Loss: 0.00001100
Iteration 250/1000 | Loss: 0.00001100
Iteration 251/1000 | Loss: 0.00001100
Iteration 252/1000 | Loss: 0.00001100
Iteration 253/1000 | Loss: 0.00001100
Iteration 254/1000 | Loss: 0.00001100
Iteration 255/1000 | Loss: 0.00001100
Iteration 256/1000 | Loss: 0.00001100
Iteration 257/1000 | Loss: 0.00001100
Iteration 258/1000 | Loss: 0.00001100
Iteration 259/1000 | Loss: 0.00001100
Iteration 260/1000 | Loss: 0.00001100
Iteration 261/1000 | Loss: 0.00001100
Iteration 262/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [1.1001038728863932e-05, 1.1001038728863932e-05, 1.1001038728863932e-05, 1.1001038728863932e-05, 1.1001038728863932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1001038728863932e-05

Optimization complete. Final v2v error: 2.8378043174743652 mm

Highest mean error: 3.0226149559020996 mm for frame 1

Lowest mean error: 2.7294230461120605 mm for frame 116

Saving results

Total time: 44.08843946456909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753421
Iteration 2/25 | Loss: 0.00150622
Iteration 3/25 | Loss: 0.00135097
Iteration 4/25 | Loss: 0.00132668
Iteration 5/25 | Loss: 0.00131998
Iteration 6/25 | Loss: 0.00131896
Iteration 7/25 | Loss: 0.00131896
Iteration 8/25 | Loss: 0.00131896
Iteration 9/25 | Loss: 0.00131896
Iteration 10/25 | Loss: 0.00131896
Iteration 11/25 | Loss: 0.00131896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013189633609727025, 0.0013189633609727025, 0.0013189633609727025, 0.0013189633609727025, 0.0013189633609727025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013189633609727025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44925582
Iteration 2/25 | Loss: 0.00101890
Iteration 3/25 | Loss: 0.00101890
Iteration 4/25 | Loss: 0.00101890
Iteration 5/25 | Loss: 0.00101890
Iteration 6/25 | Loss: 0.00101890
Iteration 7/25 | Loss: 0.00101890
Iteration 8/25 | Loss: 0.00101890
Iteration 9/25 | Loss: 0.00101890
Iteration 10/25 | Loss: 0.00101890
Iteration 11/25 | Loss: 0.00101890
Iteration 12/25 | Loss: 0.00101890
Iteration 13/25 | Loss: 0.00101890
Iteration 14/25 | Loss: 0.00101890
Iteration 15/25 | Loss: 0.00101890
Iteration 16/25 | Loss: 0.00101890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010188973974436522, 0.0010188973974436522, 0.0010188973974436522, 0.0010188973974436522, 0.0010188973974436522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010188973974436522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101890
Iteration 2/1000 | Loss: 0.00003429
Iteration 3/1000 | Loss: 0.00002493
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00002034
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001901
Iteration 10/1000 | Loss: 0.00001871
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001787
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001772
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001766
Iteration 18/1000 | Loss: 0.00001765
Iteration 19/1000 | Loss: 0.00001764
Iteration 20/1000 | Loss: 0.00001764
Iteration 21/1000 | Loss: 0.00001763
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001753
Iteration 26/1000 | Loss: 0.00001753
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001750
Iteration 29/1000 | Loss: 0.00001750
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001747
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001745
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001744
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001739
Iteration 42/1000 | Loss: 0.00001739
Iteration 43/1000 | Loss: 0.00001739
Iteration 44/1000 | Loss: 0.00001739
Iteration 45/1000 | Loss: 0.00001739
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001738
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001735
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001732
Iteration 82/1000 | Loss: 0.00001732
Iteration 83/1000 | Loss: 0.00001732
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001728
Iteration 95/1000 | Loss: 0.00001728
Iteration 96/1000 | Loss: 0.00001728
Iteration 97/1000 | Loss: 0.00001728
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001727
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001725
Iteration 108/1000 | Loss: 0.00001725
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001723
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001722
Iteration 125/1000 | Loss: 0.00001722
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001719
Iteration 134/1000 | Loss: 0.00001719
Iteration 135/1000 | Loss: 0.00001719
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001718
Iteration 138/1000 | Loss: 0.00001718
Iteration 139/1000 | Loss: 0.00001718
Iteration 140/1000 | Loss: 0.00001718
Iteration 141/1000 | Loss: 0.00001718
Iteration 142/1000 | Loss: 0.00001718
Iteration 143/1000 | Loss: 0.00001717
Iteration 144/1000 | Loss: 0.00001717
Iteration 145/1000 | Loss: 0.00001717
Iteration 146/1000 | Loss: 0.00001717
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001717
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001715
Iteration 162/1000 | Loss: 0.00001715
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001715
Iteration 166/1000 | Loss: 0.00001715
Iteration 167/1000 | Loss: 0.00001715
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001714
Iteration 172/1000 | Loss: 0.00001714
Iteration 173/1000 | Loss: 0.00001714
Iteration 174/1000 | Loss: 0.00001714
Iteration 175/1000 | Loss: 0.00001713
Iteration 176/1000 | Loss: 0.00001713
Iteration 177/1000 | Loss: 0.00001713
Iteration 178/1000 | Loss: 0.00001713
Iteration 179/1000 | Loss: 0.00001713
Iteration 180/1000 | Loss: 0.00001713
Iteration 181/1000 | Loss: 0.00001713
Iteration 182/1000 | Loss: 0.00001713
Iteration 183/1000 | Loss: 0.00001713
Iteration 184/1000 | Loss: 0.00001713
Iteration 185/1000 | Loss: 0.00001713
Iteration 186/1000 | Loss: 0.00001713
Iteration 187/1000 | Loss: 0.00001713
Iteration 188/1000 | Loss: 0.00001713
Iteration 189/1000 | Loss: 0.00001713
Iteration 190/1000 | Loss: 0.00001713
Iteration 191/1000 | Loss: 0.00001713
Iteration 192/1000 | Loss: 0.00001713
Iteration 193/1000 | Loss: 0.00001713
Iteration 194/1000 | Loss: 0.00001713
Iteration 195/1000 | Loss: 0.00001713
Iteration 196/1000 | Loss: 0.00001713
Iteration 197/1000 | Loss: 0.00001713
Iteration 198/1000 | Loss: 0.00001713
Iteration 199/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.712773519102484e-05, 1.712773519102484e-05, 1.712773519102484e-05, 1.712773519102484e-05, 1.712773519102484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.712773519102484e-05

Optimization complete. Final v2v error: 3.515357732772827 mm

Highest mean error: 3.871695041656494 mm for frame 25

Lowest mean error: 2.8720526695251465 mm for frame 168

Saving results

Total time: 48.74811315536499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489620
Iteration 2/25 | Loss: 0.00145876
Iteration 3/25 | Loss: 0.00134986
Iteration 4/25 | Loss: 0.00133236
Iteration 5/25 | Loss: 0.00132705
Iteration 6/25 | Loss: 0.00132637
Iteration 7/25 | Loss: 0.00132637
Iteration 8/25 | Loss: 0.00132637
Iteration 9/25 | Loss: 0.00132637
Iteration 10/25 | Loss: 0.00132637
Iteration 11/25 | Loss: 0.00132637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013263652799651027, 0.0013263652799651027, 0.0013263652799651027, 0.0013263652799651027, 0.0013263652799651027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013263652799651027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41845453
Iteration 2/25 | Loss: 0.00085584
Iteration 3/25 | Loss: 0.00085584
Iteration 4/25 | Loss: 0.00085584
Iteration 5/25 | Loss: 0.00085584
Iteration 6/25 | Loss: 0.00085584
Iteration 7/25 | Loss: 0.00085584
Iteration 8/25 | Loss: 0.00085584
Iteration 9/25 | Loss: 0.00085584
Iteration 10/25 | Loss: 0.00085584
Iteration 11/25 | Loss: 0.00085584
Iteration 12/25 | Loss: 0.00085584
Iteration 13/25 | Loss: 0.00085584
Iteration 14/25 | Loss: 0.00085584
Iteration 15/25 | Loss: 0.00085584
Iteration 16/25 | Loss: 0.00085584
Iteration 17/25 | Loss: 0.00085584
Iteration 18/25 | Loss: 0.00085584
Iteration 19/25 | Loss: 0.00085584
Iteration 20/25 | Loss: 0.00085584
Iteration 21/25 | Loss: 0.00085584
Iteration 22/25 | Loss: 0.00085584
Iteration 23/25 | Loss: 0.00085584
Iteration 24/25 | Loss: 0.00085584
Iteration 25/25 | Loss: 0.00085584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085584
Iteration 2/1000 | Loss: 0.00004324
Iteration 3/1000 | Loss: 0.00003131
Iteration 4/1000 | Loss: 0.00002728
Iteration 5/1000 | Loss: 0.00002630
Iteration 6/1000 | Loss: 0.00002553
Iteration 7/1000 | Loss: 0.00002501
Iteration 8/1000 | Loss: 0.00002453
Iteration 9/1000 | Loss: 0.00002423
Iteration 10/1000 | Loss: 0.00002391
Iteration 11/1000 | Loss: 0.00002358
Iteration 12/1000 | Loss: 0.00002336
Iteration 13/1000 | Loss: 0.00002312
Iteration 14/1000 | Loss: 0.00002302
Iteration 15/1000 | Loss: 0.00002287
Iteration 16/1000 | Loss: 0.00002285
Iteration 17/1000 | Loss: 0.00002283
Iteration 18/1000 | Loss: 0.00002281
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002265
Iteration 21/1000 | Loss: 0.00002261
Iteration 22/1000 | Loss: 0.00002258
Iteration 23/1000 | Loss: 0.00002257
Iteration 24/1000 | Loss: 0.00002256
Iteration 25/1000 | Loss: 0.00002256
Iteration 26/1000 | Loss: 0.00002255
Iteration 27/1000 | Loss: 0.00002255
Iteration 28/1000 | Loss: 0.00002254
Iteration 29/1000 | Loss: 0.00002254
Iteration 30/1000 | Loss: 0.00002254
Iteration 31/1000 | Loss: 0.00002253
Iteration 32/1000 | Loss: 0.00002253
Iteration 33/1000 | Loss: 0.00002253
Iteration 34/1000 | Loss: 0.00002252
Iteration 35/1000 | Loss: 0.00002252
Iteration 36/1000 | Loss: 0.00002252
Iteration 37/1000 | Loss: 0.00002251
Iteration 38/1000 | Loss: 0.00002250
Iteration 39/1000 | Loss: 0.00002249
Iteration 40/1000 | Loss: 0.00002248
Iteration 41/1000 | Loss: 0.00002248
Iteration 42/1000 | Loss: 0.00002248
Iteration 43/1000 | Loss: 0.00002247
Iteration 44/1000 | Loss: 0.00002247
Iteration 45/1000 | Loss: 0.00002247
Iteration 46/1000 | Loss: 0.00002246
Iteration 47/1000 | Loss: 0.00002246
Iteration 48/1000 | Loss: 0.00002246
Iteration 49/1000 | Loss: 0.00002246
Iteration 50/1000 | Loss: 0.00002245
Iteration 51/1000 | Loss: 0.00002245
Iteration 52/1000 | Loss: 0.00002245
Iteration 53/1000 | Loss: 0.00002243
Iteration 54/1000 | Loss: 0.00002243
Iteration 55/1000 | Loss: 0.00002243
Iteration 56/1000 | Loss: 0.00002243
Iteration 57/1000 | Loss: 0.00002243
Iteration 58/1000 | Loss: 0.00002243
Iteration 59/1000 | Loss: 0.00002243
Iteration 60/1000 | Loss: 0.00002243
Iteration 61/1000 | Loss: 0.00002243
Iteration 62/1000 | Loss: 0.00002243
Iteration 63/1000 | Loss: 0.00002242
Iteration 64/1000 | Loss: 0.00002242
Iteration 65/1000 | Loss: 0.00002242
Iteration 66/1000 | Loss: 0.00002241
Iteration 67/1000 | Loss: 0.00002241
Iteration 68/1000 | Loss: 0.00002241
Iteration 69/1000 | Loss: 0.00002240
Iteration 70/1000 | Loss: 0.00002240
Iteration 71/1000 | Loss: 0.00002240
Iteration 72/1000 | Loss: 0.00002240
Iteration 73/1000 | Loss: 0.00002239
Iteration 74/1000 | Loss: 0.00002239
Iteration 75/1000 | Loss: 0.00002239
Iteration 76/1000 | Loss: 0.00002239
Iteration 77/1000 | Loss: 0.00002239
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00002239
Iteration 80/1000 | Loss: 0.00002239
Iteration 81/1000 | Loss: 0.00002239
Iteration 82/1000 | Loss: 0.00002238
Iteration 83/1000 | Loss: 0.00002238
Iteration 84/1000 | Loss: 0.00002238
Iteration 85/1000 | Loss: 0.00002238
Iteration 86/1000 | Loss: 0.00002238
Iteration 87/1000 | Loss: 0.00002238
Iteration 88/1000 | Loss: 0.00002237
Iteration 89/1000 | Loss: 0.00002237
Iteration 90/1000 | Loss: 0.00002237
Iteration 91/1000 | Loss: 0.00002237
Iteration 92/1000 | Loss: 0.00002237
Iteration 93/1000 | Loss: 0.00002237
Iteration 94/1000 | Loss: 0.00002237
Iteration 95/1000 | Loss: 0.00002237
Iteration 96/1000 | Loss: 0.00002237
Iteration 97/1000 | Loss: 0.00002237
Iteration 98/1000 | Loss: 0.00002237
Iteration 99/1000 | Loss: 0.00002236
Iteration 100/1000 | Loss: 0.00002236
Iteration 101/1000 | Loss: 0.00002236
Iteration 102/1000 | Loss: 0.00002236
Iteration 103/1000 | Loss: 0.00002236
Iteration 104/1000 | Loss: 0.00002236
Iteration 105/1000 | Loss: 0.00002236
Iteration 106/1000 | Loss: 0.00002235
Iteration 107/1000 | Loss: 0.00002235
Iteration 108/1000 | Loss: 0.00002235
Iteration 109/1000 | Loss: 0.00002235
Iteration 110/1000 | Loss: 0.00002235
Iteration 111/1000 | Loss: 0.00002235
Iteration 112/1000 | Loss: 0.00002235
Iteration 113/1000 | Loss: 0.00002235
Iteration 114/1000 | Loss: 0.00002235
Iteration 115/1000 | Loss: 0.00002234
Iteration 116/1000 | Loss: 0.00002234
Iteration 117/1000 | Loss: 0.00002234
Iteration 118/1000 | Loss: 0.00002234
Iteration 119/1000 | Loss: 0.00002234
Iteration 120/1000 | Loss: 0.00002234
Iteration 121/1000 | Loss: 0.00002234
Iteration 122/1000 | Loss: 0.00002234
Iteration 123/1000 | Loss: 0.00002234
Iteration 124/1000 | Loss: 0.00002234
Iteration 125/1000 | Loss: 0.00002234
Iteration 126/1000 | Loss: 0.00002233
Iteration 127/1000 | Loss: 0.00002233
Iteration 128/1000 | Loss: 0.00002233
Iteration 129/1000 | Loss: 0.00002233
Iteration 130/1000 | Loss: 0.00002233
Iteration 131/1000 | Loss: 0.00002233
Iteration 132/1000 | Loss: 0.00002233
Iteration 133/1000 | Loss: 0.00002233
Iteration 134/1000 | Loss: 0.00002233
Iteration 135/1000 | Loss: 0.00002233
Iteration 136/1000 | Loss: 0.00002233
Iteration 137/1000 | Loss: 0.00002233
Iteration 138/1000 | Loss: 0.00002233
Iteration 139/1000 | Loss: 0.00002233
Iteration 140/1000 | Loss: 0.00002233
Iteration 141/1000 | Loss: 0.00002233
Iteration 142/1000 | Loss: 0.00002233
Iteration 143/1000 | Loss: 0.00002233
Iteration 144/1000 | Loss: 0.00002233
Iteration 145/1000 | Loss: 0.00002233
Iteration 146/1000 | Loss: 0.00002233
Iteration 147/1000 | Loss: 0.00002233
Iteration 148/1000 | Loss: 0.00002233
Iteration 149/1000 | Loss: 0.00002233
Iteration 150/1000 | Loss: 0.00002233
Iteration 151/1000 | Loss: 0.00002233
Iteration 152/1000 | Loss: 0.00002233
Iteration 153/1000 | Loss: 0.00002233
Iteration 154/1000 | Loss: 0.00002233
Iteration 155/1000 | Loss: 0.00002233
Iteration 156/1000 | Loss: 0.00002233
Iteration 157/1000 | Loss: 0.00002233
Iteration 158/1000 | Loss: 0.00002233
Iteration 159/1000 | Loss: 0.00002233
Iteration 160/1000 | Loss: 0.00002233
Iteration 161/1000 | Loss: 0.00002233
Iteration 162/1000 | Loss: 0.00002233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.2329626517603174e-05, 2.2329626517603174e-05, 2.2329626517603174e-05, 2.2329626517603174e-05, 2.2329626517603174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2329626517603174e-05

Optimization complete. Final v2v error: 3.9378864765167236 mm

Highest mean error: 5.370321750640869 mm for frame 39

Lowest mean error: 3.6878864765167236 mm for frame 21

Saving results

Total time: 42.31888175010681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776726
Iteration 2/25 | Loss: 0.00148022
Iteration 3/25 | Loss: 0.00137464
Iteration 4/25 | Loss: 0.00137100
Iteration 5/25 | Loss: 0.00135781
Iteration 6/25 | Loss: 0.00134681
Iteration 7/25 | Loss: 0.00134648
Iteration 8/25 | Loss: 0.00134420
Iteration 9/25 | Loss: 0.00134330
Iteration 10/25 | Loss: 0.00134268
Iteration 11/25 | Loss: 0.00134271
Iteration 12/25 | Loss: 0.00134261
Iteration 13/25 | Loss: 0.00134242
Iteration 14/25 | Loss: 0.00134234
Iteration 15/25 | Loss: 0.00134179
Iteration 16/25 | Loss: 0.00134142
Iteration 17/25 | Loss: 0.00134095
Iteration 18/25 | Loss: 0.00134399
Iteration 19/25 | Loss: 0.00134510
Iteration 20/25 | Loss: 0.00134282
Iteration 21/25 | Loss: 0.00134209
Iteration 22/25 | Loss: 0.00133975
Iteration 23/25 | Loss: 0.00133971
Iteration 24/25 | Loss: 0.00133971
Iteration 25/25 | Loss: 0.00133971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.58183146
Iteration 2/25 | Loss: 0.00080369
Iteration 3/25 | Loss: 0.00080363
Iteration 4/25 | Loss: 0.00080363
Iteration 5/25 | Loss: 0.00080363
Iteration 6/25 | Loss: 0.00080363
Iteration 7/25 | Loss: 0.00080363
Iteration 8/25 | Loss: 0.00080362
Iteration 9/25 | Loss: 0.00080362
Iteration 10/25 | Loss: 0.00080362
Iteration 11/25 | Loss: 0.00080362
Iteration 12/25 | Loss: 0.00080362
Iteration 13/25 | Loss: 0.00080362
Iteration 14/25 | Loss: 0.00080362
Iteration 15/25 | Loss: 0.00080362
Iteration 16/25 | Loss: 0.00080362
Iteration 17/25 | Loss: 0.00080362
Iteration 18/25 | Loss: 0.00080362
Iteration 19/25 | Loss: 0.00080362
Iteration 20/25 | Loss: 0.00080362
Iteration 21/25 | Loss: 0.00080362
Iteration 22/25 | Loss: 0.00080362
Iteration 23/25 | Loss: 0.00080362
Iteration 24/25 | Loss: 0.00080362
Iteration 25/25 | Loss: 0.00080362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080362
Iteration 2/1000 | Loss: 0.00002852
Iteration 3/1000 | Loss: 0.00002267
Iteration 4/1000 | Loss: 0.00002102
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001924
Iteration 8/1000 | Loss: 0.00001896
Iteration 9/1000 | Loss: 0.00001862
Iteration 10/1000 | Loss: 0.00001844
Iteration 11/1000 | Loss: 0.00001827
Iteration 12/1000 | Loss: 0.00001820
Iteration 13/1000 | Loss: 0.00001820
Iteration 14/1000 | Loss: 0.00001819
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001803
Iteration 18/1000 | Loss: 0.00001800
Iteration 19/1000 | Loss: 0.00001799
Iteration 20/1000 | Loss: 0.00001799
Iteration 21/1000 | Loss: 0.00001797
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001788
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001776
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001775
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001774
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001771
Iteration 50/1000 | Loss: 0.00001771
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001770
Iteration 53/1000 | Loss: 0.00001770
Iteration 54/1000 | Loss: 0.00001769
Iteration 55/1000 | Loss: 0.00001767
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001767
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001767
Iteration 60/1000 | Loss: 0.00001767
Iteration 61/1000 | Loss: 0.00001767
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001765
Iteration 65/1000 | Loss: 0.00001765
Iteration 66/1000 | Loss: 0.00001764
Iteration 67/1000 | Loss: 0.00001764
Iteration 68/1000 | Loss: 0.00001764
Iteration 69/1000 | Loss: 0.00001763
Iteration 70/1000 | Loss: 0.00001763
Iteration 71/1000 | Loss: 0.00001763
Iteration 72/1000 | Loss: 0.00001762
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001762
Iteration 75/1000 | Loss: 0.00001762
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001761
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001760
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001759
Iteration 90/1000 | Loss: 0.00001759
Iteration 91/1000 | Loss: 0.00001759
Iteration 92/1000 | Loss: 0.00001758
Iteration 93/1000 | Loss: 0.00001758
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001757
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001756
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001755
Iteration 109/1000 | Loss: 0.00001755
Iteration 110/1000 | Loss: 0.00001755
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001754
Iteration 119/1000 | Loss: 0.00001754
Iteration 120/1000 | Loss: 0.00001754
Iteration 121/1000 | Loss: 0.00001754
Iteration 122/1000 | Loss: 0.00001754
Iteration 123/1000 | Loss: 0.00001754
Iteration 124/1000 | Loss: 0.00001754
Iteration 125/1000 | Loss: 0.00001754
Iteration 126/1000 | Loss: 0.00001754
Iteration 127/1000 | Loss: 0.00001754
Iteration 128/1000 | Loss: 0.00001753
Iteration 129/1000 | Loss: 0.00001753
Iteration 130/1000 | Loss: 0.00001753
Iteration 131/1000 | Loss: 0.00001753
Iteration 132/1000 | Loss: 0.00001753
Iteration 133/1000 | Loss: 0.00001753
Iteration 134/1000 | Loss: 0.00001753
Iteration 135/1000 | Loss: 0.00001753
Iteration 136/1000 | Loss: 0.00001753
Iteration 137/1000 | Loss: 0.00001753
Iteration 138/1000 | Loss: 0.00001753
Iteration 139/1000 | Loss: 0.00001753
Iteration 140/1000 | Loss: 0.00001753
Iteration 141/1000 | Loss: 0.00001753
Iteration 142/1000 | Loss: 0.00001752
Iteration 143/1000 | Loss: 0.00001752
Iteration 144/1000 | Loss: 0.00001752
Iteration 145/1000 | Loss: 0.00001752
Iteration 146/1000 | Loss: 0.00001752
Iteration 147/1000 | Loss: 0.00001752
Iteration 148/1000 | Loss: 0.00001752
Iteration 149/1000 | Loss: 0.00001752
Iteration 150/1000 | Loss: 0.00001752
Iteration 151/1000 | Loss: 0.00001752
Iteration 152/1000 | Loss: 0.00001752
Iteration 153/1000 | Loss: 0.00001752
Iteration 154/1000 | Loss: 0.00001752
Iteration 155/1000 | Loss: 0.00001752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.7521970221423544e-05, 1.7521970221423544e-05, 1.7521970221423544e-05, 1.7521970221423544e-05, 1.7521970221423544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7521970221423544e-05

Optimization complete. Final v2v error: 3.484130620956421 mm

Highest mean error: 6.098364353179932 mm for frame 69

Lowest mean error: 3.047731876373291 mm for frame 109

Saving results

Total time: 79.19687986373901
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404487
Iteration 2/25 | Loss: 0.00134571
Iteration 3/25 | Loss: 0.00126746
Iteration 4/25 | Loss: 0.00125860
Iteration 5/25 | Loss: 0.00125602
Iteration 6/25 | Loss: 0.00125589
Iteration 7/25 | Loss: 0.00125589
Iteration 8/25 | Loss: 0.00125589
Iteration 9/25 | Loss: 0.00125589
Iteration 10/25 | Loss: 0.00125589
Iteration 11/25 | Loss: 0.00125589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012558895396068692, 0.0012558895396068692, 0.0012558895396068692, 0.0012558895396068692, 0.0012558895396068692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012558895396068692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40818787
Iteration 2/25 | Loss: 0.00089358
Iteration 3/25 | Loss: 0.00089358
Iteration 4/25 | Loss: 0.00089358
Iteration 5/25 | Loss: 0.00089358
Iteration 6/25 | Loss: 0.00089358
Iteration 7/25 | Loss: 0.00089357
Iteration 8/25 | Loss: 0.00089357
Iteration 9/25 | Loss: 0.00089357
Iteration 10/25 | Loss: 0.00089357
Iteration 11/25 | Loss: 0.00089357
Iteration 12/25 | Loss: 0.00089357
Iteration 13/25 | Loss: 0.00089357
Iteration 14/25 | Loss: 0.00089357
Iteration 15/25 | Loss: 0.00089357
Iteration 16/25 | Loss: 0.00089357
Iteration 17/25 | Loss: 0.00089357
Iteration 18/25 | Loss: 0.00089357
Iteration 19/25 | Loss: 0.00089357
Iteration 20/25 | Loss: 0.00089357
Iteration 21/25 | Loss: 0.00089357
Iteration 22/25 | Loss: 0.00089357
Iteration 23/25 | Loss: 0.00089357
Iteration 24/25 | Loss: 0.00089357
Iteration 25/25 | Loss: 0.00089357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089357
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00001944
Iteration 4/1000 | Loss: 0.00001617
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001339
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00001175
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001172
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001163
Iteration 21/1000 | Loss: 0.00001158
Iteration 22/1000 | Loss: 0.00001153
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001149
Iteration 25/1000 | Loss: 0.00001148
Iteration 26/1000 | Loss: 0.00001147
Iteration 27/1000 | Loss: 0.00001147
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001135
Iteration 33/1000 | Loss: 0.00001135
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001132
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001129
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001128
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001127
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001123
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001120
Iteration 64/1000 | Loss: 0.00001120
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.1188727512490004e-05, 1.1188727512490004e-05, 1.1188727512490004e-05, 1.1188727512490004e-05, 1.1188727512490004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1188727512490004e-05

Optimization complete. Final v2v error: 2.873379945755005 mm

Highest mean error: 3.731302261352539 mm for frame 71

Lowest mean error: 2.712705373764038 mm for frame 43

Saving results

Total time: 32.395987033843994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927396
Iteration 2/25 | Loss: 0.00166564
Iteration 3/25 | Loss: 0.00146945
Iteration 4/25 | Loss: 0.00144430
Iteration 5/25 | Loss: 0.00143608
Iteration 6/25 | Loss: 0.00142906
Iteration 7/25 | Loss: 0.00142534
Iteration 8/25 | Loss: 0.00142484
Iteration 9/25 | Loss: 0.00142411
Iteration 10/25 | Loss: 0.00142030
Iteration 11/25 | Loss: 0.00141589
Iteration 12/25 | Loss: 0.00141569
Iteration 13/25 | Loss: 0.00141569
Iteration 14/25 | Loss: 0.00141569
Iteration 15/25 | Loss: 0.00141569
Iteration 16/25 | Loss: 0.00141569
Iteration 17/25 | Loss: 0.00141569
Iteration 18/25 | Loss: 0.00141569
Iteration 19/25 | Loss: 0.00141569
Iteration 20/25 | Loss: 0.00141569
Iteration 21/25 | Loss: 0.00141569
Iteration 22/25 | Loss: 0.00141569
Iteration 23/25 | Loss: 0.00141569
Iteration 24/25 | Loss: 0.00141569
Iteration 25/25 | Loss: 0.00141569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11113882
Iteration 2/25 | Loss: 0.00092075
Iteration 3/25 | Loss: 0.00092071
Iteration 4/25 | Loss: 0.00092071
Iteration 5/25 | Loss: 0.00092070
Iteration 6/25 | Loss: 0.00092070
Iteration 7/25 | Loss: 0.00092070
Iteration 8/25 | Loss: 0.00092070
Iteration 9/25 | Loss: 0.00092070
Iteration 10/25 | Loss: 0.00092070
Iteration 11/25 | Loss: 0.00092070
Iteration 12/25 | Loss: 0.00092070
Iteration 13/25 | Loss: 0.00092070
Iteration 14/25 | Loss: 0.00092070
Iteration 15/25 | Loss: 0.00092070
Iteration 16/25 | Loss: 0.00092070
Iteration 17/25 | Loss: 0.00092070
Iteration 18/25 | Loss: 0.00092070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009207024704664946, 0.0009207024704664946, 0.0009207024704664946, 0.0009207024704664946, 0.0009207024704664946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009207024704664946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092070
Iteration 2/1000 | Loss: 0.00004894
Iteration 3/1000 | Loss: 0.00003481
Iteration 4/1000 | Loss: 0.00003115
Iteration 5/1000 | Loss: 0.00002940
Iteration 6/1000 | Loss: 0.00002850
Iteration 7/1000 | Loss: 0.00002785
Iteration 8/1000 | Loss: 0.00002749
Iteration 9/1000 | Loss: 0.00002713
Iteration 10/1000 | Loss: 0.00002679
Iteration 11/1000 | Loss: 0.00002658
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002616
Iteration 14/1000 | Loss: 0.00002600
Iteration 15/1000 | Loss: 0.00002594
Iteration 16/1000 | Loss: 0.00002594
Iteration 17/1000 | Loss: 0.00002593
Iteration 18/1000 | Loss: 0.00002593
Iteration 19/1000 | Loss: 0.00002592
Iteration 20/1000 | Loss: 0.00002586
Iteration 21/1000 | Loss: 0.00002585
Iteration 22/1000 | Loss: 0.00002583
Iteration 23/1000 | Loss: 0.00002583
Iteration 24/1000 | Loss: 0.00002583
Iteration 25/1000 | Loss: 0.00002582
Iteration 26/1000 | Loss: 0.00002580
Iteration 27/1000 | Loss: 0.00002579
Iteration 28/1000 | Loss: 0.00002578
Iteration 29/1000 | Loss: 0.00002575
Iteration 30/1000 | Loss: 0.00002574
Iteration 31/1000 | Loss: 0.00002573
Iteration 32/1000 | Loss: 0.00002573
Iteration 33/1000 | Loss: 0.00002573
Iteration 34/1000 | Loss: 0.00002573
Iteration 35/1000 | Loss: 0.00002573
Iteration 36/1000 | Loss: 0.00002573
Iteration 37/1000 | Loss: 0.00002572
Iteration 38/1000 | Loss: 0.00002571
Iteration 39/1000 | Loss: 0.00002571
Iteration 40/1000 | Loss: 0.00002569
Iteration 41/1000 | Loss: 0.00002569
Iteration 42/1000 | Loss: 0.00002569
Iteration 43/1000 | Loss: 0.00002569
Iteration 44/1000 | Loss: 0.00002567
Iteration 45/1000 | Loss: 0.00002567
Iteration 46/1000 | Loss: 0.00002566
Iteration 47/1000 | Loss: 0.00002566
Iteration 48/1000 | Loss: 0.00002565
Iteration 49/1000 | Loss: 0.00002565
Iteration 50/1000 | Loss: 0.00002564
Iteration 51/1000 | Loss: 0.00002564
Iteration 52/1000 | Loss: 0.00002563
Iteration 53/1000 | Loss: 0.00002563
Iteration 54/1000 | Loss: 0.00002562
Iteration 55/1000 | Loss: 0.00002562
Iteration 56/1000 | Loss: 0.00002561
Iteration 57/1000 | Loss: 0.00002561
Iteration 58/1000 | Loss: 0.00002561
Iteration 59/1000 | Loss: 0.00002560
Iteration 60/1000 | Loss: 0.00002560
Iteration 61/1000 | Loss: 0.00002560
Iteration 62/1000 | Loss: 0.00002559
Iteration 63/1000 | Loss: 0.00002559
Iteration 64/1000 | Loss: 0.00002559
Iteration 65/1000 | Loss: 0.00002559
Iteration 66/1000 | Loss: 0.00002558
Iteration 67/1000 | Loss: 0.00002558
Iteration 68/1000 | Loss: 0.00002558
Iteration 69/1000 | Loss: 0.00002558
Iteration 70/1000 | Loss: 0.00002558
Iteration 71/1000 | Loss: 0.00002558
Iteration 72/1000 | Loss: 0.00002557
Iteration 73/1000 | Loss: 0.00002557
Iteration 74/1000 | Loss: 0.00002557
Iteration 75/1000 | Loss: 0.00002557
Iteration 76/1000 | Loss: 0.00002557
Iteration 77/1000 | Loss: 0.00002557
Iteration 78/1000 | Loss: 0.00002557
Iteration 79/1000 | Loss: 0.00002557
Iteration 80/1000 | Loss: 0.00002557
Iteration 81/1000 | Loss: 0.00002557
Iteration 82/1000 | Loss: 0.00002557
Iteration 83/1000 | Loss: 0.00002557
Iteration 84/1000 | Loss: 0.00002557
Iteration 85/1000 | Loss: 0.00002557
Iteration 86/1000 | Loss: 0.00002557
Iteration 87/1000 | Loss: 0.00002557
Iteration 88/1000 | Loss: 0.00002557
Iteration 89/1000 | Loss: 0.00002557
Iteration 90/1000 | Loss: 0.00002557
Iteration 91/1000 | Loss: 0.00002557
Iteration 92/1000 | Loss: 0.00002557
Iteration 93/1000 | Loss: 0.00002557
Iteration 94/1000 | Loss: 0.00002557
Iteration 95/1000 | Loss: 0.00002557
Iteration 96/1000 | Loss: 0.00002557
Iteration 97/1000 | Loss: 0.00002557
Iteration 98/1000 | Loss: 0.00002557
Iteration 99/1000 | Loss: 0.00002557
Iteration 100/1000 | Loss: 0.00002557
Iteration 101/1000 | Loss: 0.00002557
Iteration 102/1000 | Loss: 0.00002557
Iteration 103/1000 | Loss: 0.00002557
Iteration 104/1000 | Loss: 0.00002557
Iteration 105/1000 | Loss: 0.00002557
Iteration 106/1000 | Loss: 0.00002557
Iteration 107/1000 | Loss: 0.00002557
Iteration 108/1000 | Loss: 0.00002557
Iteration 109/1000 | Loss: 0.00002557
Iteration 110/1000 | Loss: 0.00002557
Iteration 111/1000 | Loss: 0.00002557
Iteration 112/1000 | Loss: 0.00002557
Iteration 113/1000 | Loss: 0.00002557
Iteration 114/1000 | Loss: 0.00002557
Iteration 115/1000 | Loss: 0.00002557
Iteration 116/1000 | Loss: 0.00002557
Iteration 117/1000 | Loss: 0.00002557
Iteration 118/1000 | Loss: 0.00002557
Iteration 119/1000 | Loss: 0.00002557
Iteration 120/1000 | Loss: 0.00002557
Iteration 121/1000 | Loss: 0.00002557
Iteration 122/1000 | Loss: 0.00002557
Iteration 123/1000 | Loss: 0.00002557
Iteration 124/1000 | Loss: 0.00002557
Iteration 125/1000 | Loss: 0.00002557
Iteration 126/1000 | Loss: 0.00002557
Iteration 127/1000 | Loss: 0.00002557
Iteration 128/1000 | Loss: 0.00002557
Iteration 129/1000 | Loss: 0.00002557
Iteration 130/1000 | Loss: 0.00002557
Iteration 131/1000 | Loss: 0.00002557
Iteration 132/1000 | Loss: 0.00002557
Iteration 133/1000 | Loss: 0.00002557
Iteration 134/1000 | Loss: 0.00002557
Iteration 135/1000 | Loss: 0.00002557
Iteration 136/1000 | Loss: 0.00002557
Iteration 137/1000 | Loss: 0.00002557
Iteration 138/1000 | Loss: 0.00002557
Iteration 139/1000 | Loss: 0.00002557
Iteration 140/1000 | Loss: 0.00002557
Iteration 141/1000 | Loss: 0.00002557
Iteration 142/1000 | Loss: 0.00002557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.5568117052898742e-05, 2.5568117052898742e-05, 2.5568117052898742e-05, 2.5568117052898742e-05, 2.5568117052898742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5568117052898742e-05

Optimization complete. Final v2v error: 4.004882335662842 mm

Highest mean error: 4.623750686645508 mm for frame 105

Lowest mean error: 3.5503132343292236 mm for frame 146

Saving results

Total time: 56.0029878616333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00745219
Iteration 2/25 | Loss: 0.00175122
Iteration 3/25 | Loss: 0.00139675
Iteration 4/25 | Loss: 0.00132977
Iteration 5/25 | Loss: 0.00131561
Iteration 6/25 | Loss: 0.00130477
Iteration 7/25 | Loss: 0.00130242
Iteration 8/25 | Loss: 0.00128546
Iteration 9/25 | Loss: 0.00128208
Iteration 10/25 | Loss: 0.00127547
Iteration 11/25 | Loss: 0.00127096
Iteration 12/25 | Loss: 0.00126768
Iteration 13/25 | Loss: 0.00126718
Iteration 14/25 | Loss: 0.00126672
Iteration 15/25 | Loss: 0.00126580
Iteration 16/25 | Loss: 0.00126539
Iteration 17/25 | Loss: 0.00126526
Iteration 18/25 | Loss: 0.00126522
Iteration 19/25 | Loss: 0.00126522
Iteration 20/25 | Loss: 0.00126522
Iteration 21/25 | Loss: 0.00126521
Iteration 22/25 | Loss: 0.00126521
Iteration 23/25 | Loss: 0.00126521
Iteration 24/25 | Loss: 0.00126521
Iteration 25/25 | Loss: 0.00126521

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13618779
Iteration 2/25 | Loss: 0.00087445
Iteration 3/25 | Loss: 0.00086862
Iteration 4/25 | Loss: 0.00086862
Iteration 5/25 | Loss: 0.00086863
Iteration 6/25 | Loss: 0.00086862
Iteration 7/25 | Loss: 0.00086862
Iteration 8/25 | Loss: 0.00086862
Iteration 9/25 | Loss: 0.00086863
Iteration 10/25 | Loss: 0.00086813
Iteration 11/25 | Loss: 0.00086813
Iteration 12/25 | Loss: 0.00086813
Iteration 13/25 | Loss: 0.00086813
Iteration 14/25 | Loss: 0.00086813
Iteration 15/25 | Loss: 0.00086813
Iteration 16/25 | Loss: 0.00086813
Iteration 17/25 | Loss: 0.00086813
Iteration 18/25 | Loss: 0.00086813
Iteration 19/25 | Loss: 0.00086813
Iteration 20/25 | Loss: 0.00086813
Iteration 21/25 | Loss: 0.00086813
Iteration 22/25 | Loss: 0.00086813
Iteration 23/25 | Loss: 0.00086813
Iteration 24/25 | Loss: 0.00086813
Iteration 25/25 | Loss: 0.00086813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086813
Iteration 2/1000 | Loss: 0.00004092
Iteration 3/1000 | Loss: 0.00002782
Iteration 4/1000 | Loss: 0.00006565
Iteration 5/1000 | Loss: 0.00002124
Iteration 6/1000 | Loss: 0.00001992
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00043936
Iteration 10/1000 | Loss: 0.00035545
Iteration 11/1000 | Loss: 0.00002108
Iteration 12/1000 | Loss: 0.00005537
Iteration 13/1000 | Loss: 0.00002292
Iteration 14/1000 | Loss: 0.00001930
Iteration 15/1000 | Loss: 0.00001728
Iteration 16/1000 | Loss: 0.00001782
Iteration 17/1000 | Loss: 0.00012810
Iteration 18/1000 | Loss: 0.00001477
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001983
Iteration 21/1000 | Loss: 0.00006135
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001430
Iteration 25/1000 | Loss: 0.00005263
Iteration 26/1000 | Loss: 0.00001403
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001366
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001366
Iteration 38/1000 | Loss: 0.00001366
Iteration 39/1000 | Loss: 0.00001359
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001355
Iteration 42/1000 | Loss: 0.00001355
Iteration 43/1000 | Loss: 0.00001354
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001339
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001338
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001337
Iteration 68/1000 | Loss: 0.00001336
Iteration 69/1000 | Loss: 0.00001336
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001330
Iteration 75/1000 | Loss: 0.00001330
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001330
Iteration 78/1000 | Loss: 0.00001330
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001329
Iteration 94/1000 | Loss: 0.00001329
Iteration 95/1000 | Loss: 0.00001329
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.3285980458022095e-05, 1.3285980458022095e-05, 1.3285980458022095e-05, 1.3285980458022095e-05, 1.3285980458022095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3285980458022095e-05

Optimization complete. Final v2v error: 3.143328905105591 mm

Highest mean error: 3.6584253311157227 mm for frame 175

Lowest mean error: 2.8958053588867188 mm for frame 9

Saving results

Total time: 86.21364402770996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849181
Iteration 2/25 | Loss: 0.00193387
Iteration 3/25 | Loss: 0.00165207
Iteration 4/25 | Loss: 0.00157871
Iteration 5/25 | Loss: 0.00157654
Iteration 6/25 | Loss: 0.00157158
Iteration 7/25 | Loss: 0.00156819
Iteration 8/25 | Loss: 0.00156595
Iteration 9/25 | Loss: 0.00156282
Iteration 10/25 | Loss: 0.00156216
Iteration 11/25 | Loss: 0.00156306
Iteration 12/25 | Loss: 0.00156118
Iteration 13/25 | Loss: 0.00155965
Iteration 14/25 | Loss: 0.00155935
Iteration 15/25 | Loss: 0.00155928
Iteration 16/25 | Loss: 0.00155927
Iteration 17/25 | Loss: 0.00155927
Iteration 18/25 | Loss: 0.00155927
Iteration 19/25 | Loss: 0.00155927
Iteration 20/25 | Loss: 0.00155927
Iteration 21/25 | Loss: 0.00155927
Iteration 22/25 | Loss: 0.00155927
Iteration 23/25 | Loss: 0.00155927
Iteration 24/25 | Loss: 0.00155926
Iteration 25/25 | Loss: 0.00155926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32661390
Iteration 2/25 | Loss: 0.00135294
Iteration 3/25 | Loss: 0.00124437
Iteration 4/25 | Loss: 0.00124437
Iteration 5/25 | Loss: 0.00124437
Iteration 6/25 | Loss: 0.00124437
Iteration 7/25 | Loss: 0.00124437
Iteration 8/25 | Loss: 0.00124437
Iteration 9/25 | Loss: 0.00124437
Iteration 10/25 | Loss: 0.00124437
Iteration 11/25 | Loss: 0.00124437
Iteration 12/25 | Loss: 0.00124437
Iteration 13/25 | Loss: 0.00124437
Iteration 14/25 | Loss: 0.00124437
Iteration 15/25 | Loss: 0.00124437
Iteration 16/25 | Loss: 0.00124437
Iteration 17/25 | Loss: 0.00124437
Iteration 18/25 | Loss: 0.00124437
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001244369545020163, 0.001244369545020163, 0.001244369545020163, 0.001244369545020163, 0.001244369545020163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001244369545020163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124437
Iteration 2/1000 | Loss: 0.00051065
Iteration 3/1000 | Loss: 0.00039387
Iteration 4/1000 | Loss: 0.00009301
Iteration 5/1000 | Loss: 0.00065712
Iteration 6/1000 | Loss: 0.00004455
Iteration 7/1000 | Loss: 0.00013297
Iteration 8/1000 | Loss: 0.00004042
Iteration 9/1000 | Loss: 0.00045477
Iteration 10/1000 | Loss: 0.00008134
Iteration 11/1000 | Loss: 0.00005158
Iteration 12/1000 | Loss: 0.00007883
Iteration 13/1000 | Loss: 0.00006755
Iteration 14/1000 | Loss: 0.00008151
Iteration 15/1000 | Loss: 0.00003770
Iteration 16/1000 | Loss: 0.00003717
Iteration 17/1000 | Loss: 0.00014035
Iteration 18/1000 | Loss: 0.00003861
Iteration 19/1000 | Loss: 0.00003653
Iteration 20/1000 | Loss: 0.00006334
Iteration 21/1000 | Loss: 0.00003591
Iteration 22/1000 | Loss: 0.00009992
Iteration 23/1000 | Loss: 0.00013760
Iteration 24/1000 | Loss: 0.00005618
Iteration 25/1000 | Loss: 0.00003616
Iteration 26/1000 | Loss: 0.00006086
Iteration 27/1000 | Loss: 0.00003520
Iteration 28/1000 | Loss: 0.00009823
Iteration 29/1000 | Loss: 0.00004351
Iteration 30/1000 | Loss: 0.00004421
Iteration 31/1000 | Loss: 0.00003397
Iteration 32/1000 | Loss: 0.00003365
Iteration 33/1000 | Loss: 0.00006426
Iteration 34/1000 | Loss: 0.00013048
Iteration 35/1000 | Loss: 0.00006537
Iteration 36/1000 | Loss: 0.00005581
Iteration 37/1000 | Loss: 0.00009645
Iteration 38/1000 | Loss: 0.00005131
Iteration 39/1000 | Loss: 0.00006457
Iteration 40/1000 | Loss: 0.00005780
Iteration 41/1000 | Loss: 0.00003352
Iteration 42/1000 | Loss: 0.00007278
Iteration 43/1000 | Loss: 0.00005163
Iteration 44/1000 | Loss: 0.00005629
Iteration 45/1000 | Loss: 0.00003578
Iteration 46/1000 | Loss: 0.00003544
Iteration 47/1000 | Loss: 0.00004842
Iteration 48/1000 | Loss: 0.00006217
Iteration 49/1000 | Loss: 0.00003437
Iteration 50/1000 | Loss: 0.00004940
Iteration 51/1000 | Loss: 0.00003321
Iteration 52/1000 | Loss: 0.00004644
Iteration 53/1000 | Loss: 0.00004659
Iteration 54/1000 | Loss: 0.00003319
Iteration 55/1000 | Loss: 0.00003317
Iteration 56/1000 | Loss: 0.00005811
Iteration 57/1000 | Loss: 0.00004346
Iteration 58/1000 | Loss: 0.00003312
Iteration 59/1000 | Loss: 0.00003311
Iteration 60/1000 | Loss: 0.00003310
Iteration 61/1000 | Loss: 0.00003310
Iteration 62/1000 | Loss: 0.00003309
Iteration 63/1000 | Loss: 0.00003309
Iteration 64/1000 | Loss: 0.00003309
Iteration 65/1000 | Loss: 0.00004588
Iteration 66/1000 | Loss: 0.00003308
Iteration 67/1000 | Loss: 0.00003308
Iteration 68/1000 | Loss: 0.00003308
Iteration 69/1000 | Loss: 0.00003308
Iteration 70/1000 | Loss: 0.00003308
Iteration 71/1000 | Loss: 0.00003307
Iteration 72/1000 | Loss: 0.00003307
Iteration 73/1000 | Loss: 0.00003307
Iteration 74/1000 | Loss: 0.00003307
Iteration 75/1000 | Loss: 0.00003307
Iteration 76/1000 | Loss: 0.00003307
Iteration 77/1000 | Loss: 0.00003307
Iteration 78/1000 | Loss: 0.00003307
Iteration 79/1000 | Loss: 0.00003307
Iteration 80/1000 | Loss: 0.00003307
Iteration 81/1000 | Loss: 0.00003307
Iteration 82/1000 | Loss: 0.00003306
Iteration 83/1000 | Loss: 0.00003306
Iteration 84/1000 | Loss: 0.00003305
Iteration 85/1000 | Loss: 0.00003305
Iteration 86/1000 | Loss: 0.00003305
Iteration 87/1000 | Loss: 0.00006729
Iteration 88/1000 | Loss: 0.00003748
Iteration 89/1000 | Loss: 0.00003376
Iteration 90/1000 | Loss: 0.00008347
Iteration 91/1000 | Loss: 0.00004385
Iteration 92/1000 | Loss: 0.00003318
Iteration 93/1000 | Loss: 0.00009770
Iteration 94/1000 | Loss: 0.00005421
Iteration 95/1000 | Loss: 0.00003337
Iteration 96/1000 | Loss: 0.00010410
Iteration 97/1000 | Loss: 0.00004008
Iteration 98/1000 | Loss: 0.00005621
Iteration 99/1000 | Loss: 0.00003309
Iteration 100/1000 | Loss: 0.00003301
Iteration 101/1000 | Loss: 0.00003301
Iteration 102/1000 | Loss: 0.00003297
Iteration 103/1000 | Loss: 0.00003297
Iteration 104/1000 | Loss: 0.00003296
Iteration 105/1000 | Loss: 0.00003296
Iteration 106/1000 | Loss: 0.00003295
Iteration 107/1000 | Loss: 0.00003295
Iteration 108/1000 | Loss: 0.00003295
Iteration 109/1000 | Loss: 0.00003295
Iteration 110/1000 | Loss: 0.00003295
Iteration 111/1000 | Loss: 0.00003295
Iteration 112/1000 | Loss: 0.00003295
Iteration 113/1000 | Loss: 0.00003295
Iteration 114/1000 | Loss: 0.00003295
Iteration 115/1000 | Loss: 0.00003295
Iteration 116/1000 | Loss: 0.00003295
Iteration 117/1000 | Loss: 0.00003294
Iteration 118/1000 | Loss: 0.00003294
Iteration 119/1000 | Loss: 0.00003294
Iteration 120/1000 | Loss: 0.00003294
Iteration 121/1000 | Loss: 0.00003294
Iteration 122/1000 | Loss: 0.00003294
Iteration 123/1000 | Loss: 0.00003294
Iteration 124/1000 | Loss: 0.00003294
Iteration 125/1000 | Loss: 0.00003294
Iteration 126/1000 | Loss: 0.00003293
Iteration 127/1000 | Loss: 0.00003293
Iteration 128/1000 | Loss: 0.00003293
Iteration 129/1000 | Loss: 0.00003293
Iteration 130/1000 | Loss: 0.00003293
Iteration 131/1000 | Loss: 0.00003293
Iteration 132/1000 | Loss: 0.00003293
Iteration 133/1000 | Loss: 0.00003293
Iteration 134/1000 | Loss: 0.00003293
Iteration 135/1000 | Loss: 0.00003293
Iteration 136/1000 | Loss: 0.00003293
Iteration 137/1000 | Loss: 0.00003293
Iteration 138/1000 | Loss: 0.00003293
Iteration 139/1000 | Loss: 0.00003293
Iteration 140/1000 | Loss: 0.00003293
Iteration 141/1000 | Loss: 0.00003293
Iteration 142/1000 | Loss: 0.00003293
Iteration 143/1000 | Loss: 0.00003293
Iteration 144/1000 | Loss: 0.00003293
Iteration 145/1000 | Loss: 0.00003293
Iteration 146/1000 | Loss: 0.00003293
Iteration 147/1000 | Loss: 0.00003293
Iteration 148/1000 | Loss: 0.00003293
Iteration 149/1000 | Loss: 0.00003293
Iteration 150/1000 | Loss: 0.00003293
Iteration 151/1000 | Loss: 0.00003293
Iteration 152/1000 | Loss: 0.00003293
Iteration 153/1000 | Loss: 0.00003293
Iteration 154/1000 | Loss: 0.00003293
Iteration 155/1000 | Loss: 0.00003293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.292980545666069e-05, 3.292980545666069e-05, 3.292980545666069e-05, 3.292980545666069e-05, 3.292980545666069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.292980545666069e-05

Optimization complete. Final v2v error: 4.627513885498047 mm

Highest mean error: 5.61129093170166 mm for frame 225

Lowest mean error: 4.182052135467529 mm for frame 79

Saving results

Total time: 147.7488555908203
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746944
Iteration 2/25 | Loss: 0.00176386
Iteration 3/25 | Loss: 0.00142645
Iteration 4/25 | Loss: 0.00138525
Iteration 5/25 | Loss: 0.00138220
Iteration 6/25 | Loss: 0.00135740
Iteration 7/25 | Loss: 0.00135452
Iteration 8/25 | Loss: 0.00134873
Iteration 9/25 | Loss: 0.00134180
Iteration 10/25 | Loss: 0.00134275
Iteration 11/25 | Loss: 0.00133805
Iteration 12/25 | Loss: 0.00133486
Iteration 13/25 | Loss: 0.00133353
Iteration 14/25 | Loss: 0.00133293
Iteration 15/25 | Loss: 0.00133267
Iteration 16/25 | Loss: 0.00133255
Iteration 17/25 | Loss: 0.00133255
Iteration 18/25 | Loss: 0.00133255
Iteration 19/25 | Loss: 0.00133255
Iteration 20/25 | Loss: 0.00133255
Iteration 21/25 | Loss: 0.00133255
Iteration 22/25 | Loss: 0.00133255
Iteration 23/25 | Loss: 0.00133254
Iteration 24/25 | Loss: 0.00133252
Iteration 25/25 | Loss: 0.00133252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.63410783
Iteration 2/25 | Loss: 0.00089059
Iteration 3/25 | Loss: 0.00089055
Iteration 4/25 | Loss: 0.00089055
Iteration 5/25 | Loss: 0.00089055
Iteration 6/25 | Loss: 0.00089055
Iteration 7/25 | Loss: 0.00089055
Iteration 8/25 | Loss: 0.00089055
Iteration 9/25 | Loss: 0.00089055
Iteration 10/25 | Loss: 0.00089055
Iteration 11/25 | Loss: 0.00089055
Iteration 12/25 | Loss: 0.00089055
Iteration 13/25 | Loss: 0.00089055
Iteration 14/25 | Loss: 0.00089055
Iteration 15/25 | Loss: 0.00089055
Iteration 16/25 | Loss: 0.00089055
Iteration 17/25 | Loss: 0.00089055
Iteration 18/25 | Loss: 0.00089055
Iteration 19/25 | Loss: 0.00089055
Iteration 20/25 | Loss: 0.00089055
Iteration 21/25 | Loss: 0.00089055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008905506110750139, 0.0008905506110750139, 0.0008905506110750139, 0.0008905506110750139, 0.0008905506110750139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008905506110750139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089055
Iteration 2/1000 | Loss: 0.00003372
Iteration 3/1000 | Loss: 0.00002707
Iteration 4/1000 | Loss: 0.00002271
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001983
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001919
Iteration 11/1000 | Loss: 0.00001914
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00013622
Iteration 14/1000 | Loss: 0.00022887
Iteration 15/1000 | Loss: 0.00011177
Iteration 16/1000 | Loss: 0.00014072
Iteration 17/1000 | Loss: 0.00021829
Iteration 18/1000 | Loss: 0.00038476
Iteration 19/1000 | Loss: 0.00014546
Iteration 20/1000 | Loss: 0.00014173
Iteration 21/1000 | Loss: 0.00016993
Iteration 22/1000 | Loss: 0.00009093
Iteration 23/1000 | Loss: 0.00002521
Iteration 24/1000 | Loss: 0.00002240
Iteration 25/1000 | Loss: 0.00002111
Iteration 26/1000 | Loss: 0.00002034
Iteration 27/1000 | Loss: 0.00035952
Iteration 28/1000 | Loss: 0.00005285
Iteration 29/1000 | Loss: 0.00009382
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00001957
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001852
Iteration 35/1000 | Loss: 0.00001843
Iteration 36/1000 | Loss: 0.00001828
Iteration 37/1000 | Loss: 0.00016063
Iteration 38/1000 | Loss: 0.00002306
Iteration 39/1000 | Loss: 0.00002048
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001831
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001772
Iteration 47/1000 | Loss: 0.00001766
Iteration 48/1000 | Loss: 0.00001760
Iteration 49/1000 | Loss: 0.00001760
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001758
Iteration 52/1000 | Loss: 0.00001756
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001755
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001754
Iteration 60/1000 | Loss: 0.00001754
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001753
Iteration 64/1000 | Loss: 0.00001753
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001753
Iteration 69/1000 | Loss: 0.00001753
Iteration 70/1000 | Loss: 0.00001753
Iteration 71/1000 | Loss: 0.00001753
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001752
Iteration 75/1000 | Loss: 0.00001752
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001751
Iteration 80/1000 | Loss: 0.00001751
Iteration 81/1000 | Loss: 0.00001751
Iteration 82/1000 | Loss: 0.00001750
Iteration 83/1000 | Loss: 0.00001750
Iteration 84/1000 | Loss: 0.00001749
Iteration 85/1000 | Loss: 0.00001749
Iteration 86/1000 | Loss: 0.00001748
Iteration 87/1000 | Loss: 0.00001748
Iteration 88/1000 | Loss: 0.00001748
Iteration 89/1000 | Loss: 0.00001747
Iteration 90/1000 | Loss: 0.00001747
Iteration 91/1000 | Loss: 0.00001746
Iteration 92/1000 | Loss: 0.00001746
Iteration 93/1000 | Loss: 0.00001746
Iteration 94/1000 | Loss: 0.00001746
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001745
Iteration 97/1000 | Loss: 0.00001745
Iteration 98/1000 | Loss: 0.00001745
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001744
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001743
Iteration 109/1000 | Loss: 0.00001743
Iteration 110/1000 | Loss: 0.00001743
Iteration 111/1000 | Loss: 0.00001743
Iteration 112/1000 | Loss: 0.00001743
Iteration 113/1000 | Loss: 0.00001743
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001742
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001742
Iteration 118/1000 | Loss: 0.00001742
Iteration 119/1000 | Loss: 0.00001742
Iteration 120/1000 | Loss: 0.00001742
Iteration 121/1000 | Loss: 0.00001742
Iteration 122/1000 | Loss: 0.00001741
Iteration 123/1000 | Loss: 0.00001741
Iteration 124/1000 | Loss: 0.00001741
Iteration 125/1000 | Loss: 0.00001741
Iteration 126/1000 | Loss: 0.00001741
Iteration 127/1000 | Loss: 0.00001741
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001740
Iteration 134/1000 | Loss: 0.00001740
Iteration 135/1000 | Loss: 0.00001740
Iteration 136/1000 | Loss: 0.00001740
Iteration 137/1000 | Loss: 0.00001740
Iteration 138/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.7401249351678416e-05, 1.7401249351678416e-05, 1.7401249351678416e-05, 1.7401249351678416e-05, 1.7401249351678416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7401249351678416e-05

Optimization complete. Final v2v error: 3.510023832321167 mm

Highest mean error: 5.105865001678467 mm for frame 220

Lowest mean error: 3.047044277191162 mm for frame 207

Saving results

Total time: 111.03822135925293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825312
Iteration 2/25 | Loss: 0.00148371
Iteration 3/25 | Loss: 0.00136124
Iteration 4/25 | Loss: 0.00134425
Iteration 5/25 | Loss: 0.00133930
Iteration 6/25 | Loss: 0.00133785
Iteration 7/25 | Loss: 0.00133701
Iteration 8/25 | Loss: 0.00133863
Iteration 9/25 | Loss: 0.00133871
Iteration 10/25 | Loss: 0.00133957
Iteration 11/25 | Loss: 0.00133788
Iteration 12/25 | Loss: 0.00133834
Iteration 13/25 | Loss: 0.00133901
Iteration 14/25 | Loss: 0.00133902
Iteration 15/25 | Loss: 0.00133973
Iteration 16/25 | Loss: 0.00133881
Iteration 17/25 | Loss: 0.00133768
Iteration 18/25 | Loss: 0.00133583
Iteration 19/25 | Loss: 0.00133529
Iteration 20/25 | Loss: 0.00133435
Iteration 21/25 | Loss: 0.00133411
Iteration 22/25 | Loss: 0.00133400
Iteration 23/25 | Loss: 0.00133400
Iteration 24/25 | Loss: 0.00133400
Iteration 25/25 | Loss: 0.00133400

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40110362
Iteration 2/25 | Loss: 0.00163529
Iteration 3/25 | Loss: 0.00163528
Iteration 4/25 | Loss: 0.00163528
Iteration 5/25 | Loss: 0.00163528
Iteration 6/25 | Loss: 0.00163528
Iteration 7/25 | Loss: 0.00163528
Iteration 8/25 | Loss: 0.00163528
Iteration 9/25 | Loss: 0.00163528
Iteration 10/25 | Loss: 0.00163528
Iteration 11/25 | Loss: 0.00163528
Iteration 12/25 | Loss: 0.00163528
Iteration 13/25 | Loss: 0.00163528
Iteration 14/25 | Loss: 0.00163528
Iteration 15/25 | Loss: 0.00163528
Iteration 16/25 | Loss: 0.00163528
Iteration 17/25 | Loss: 0.00163528
Iteration 18/25 | Loss: 0.00163528
Iteration 19/25 | Loss: 0.00163528
Iteration 20/25 | Loss: 0.00163528
Iteration 21/25 | Loss: 0.00163528
Iteration 22/25 | Loss: 0.00163528
Iteration 23/25 | Loss: 0.00163528
Iteration 24/25 | Loss: 0.00163528
Iteration 25/25 | Loss: 0.00163528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163528
Iteration 2/1000 | Loss: 0.00011255
Iteration 3/1000 | Loss: 0.00007043
Iteration 4/1000 | Loss: 0.00006040
Iteration 5/1000 | Loss: 0.00005450
Iteration 6/1000 | Loss: 0.00005165
Iteration 7/1000 | Loss: 0.00005001
Iteration 8/1000 | Loss: 0.00004883
Iteration 9/1000 | Loss: 0.00004746
Iteration 10/1000 | Loss: 0.00004679
Iteration 11/1000 | Loss: 0.00004633
Iteration 12/1000 | Loss: 0.00004599
Iteration 13/1000 | Loss: 0.00004559
Iteration 14/1000 | Loss: 0.00004526
Iteration 15/1000 | Loss: 0.00004493
Iteration 16/1000 | Loss: 0.00004464
Iteration 17/1000 | Loss: 0.00004434
Iteration 18/1000 | Loss: 0.00004405
Iteration 19/1000 | Loss: 0.00004370
Iteration 20/1000 | Loss: 0.00004322
Iteration 21/1000 | Loss: 0.00004278
Iteration 22/1000 | Loss: 0.00004244
Iteration 23/1000 | Loss: 0.00041817
Iteration 24/1000 | Loss: 0.00010358
Iteration 25/1000 | Loss: 0.00004422
Iteration 26/1000 | Loss: 0.00013372
Iteration 27/1000 | Loss: 0.00004422
Iteration 28/1000 | Loss: 0.00007306
Iteration 29/1000 | Loss: 0.00004217
Iteration 30/1000 | Loss: 0.00004123
Iteration 31/1000 | Loss: 0.00004072
Iteration 32/1000 | Loss: 0.00004020
Iteration 33/1000 | Loss: 0.00003983
Iteration 34/1000 | Loss: 0.00003947
Iteration 35/1000 | Loss: 0.00003926
Iteration 36/1000 | Loss: 0.00003912
Iteration 37/1000 | Loss: 0.00003898
Iteration 38/1000 | Loss: 0.00003887
Iteration 39/1000 | Loss: 0.00003884
Iteration 40/1000 | Loss: 0.00041103
Iteration 41/1000 | Loss: 0.00276242
Iteration 42/1000 | Loss: 0.00056024
Iteration 43/1000 | Loss: 0.00006444
Iteration 44/1000 | Loss: 0.00004743
Iteration 45/1000 | Loss: 0.00004208
Iteration 46/1000 | Loss: 0.00003917
Iteration 47/1000 | Loss: 0.00029241
Iteration 48/1000 | Loss: 0.00003791
Iteration 49/1000 | Loss: 0.00003225
Iteration 50/1000 | Loss: 0.00003036
Iteration 51/1000 | Loss: 0.00002893
Iteration 52/1000 | Loss: 0.00002793
Iteration 53/1000 | Loss: 0.00002733
Iteration 54/1000 | Loss: 0.00002691
Iteration 55/1000 | Loss: 0.00002632
Iteration 56/1000 | Loss: 0.00002581
Iteration 57/1000 | Loss: 0.00002540
Iteration 58/1000 | Loss: 0.00002508
Iteration 59/1000 | Loss: 0.00002476
Iteration 60/1000 | Loss: 0.00002454
Iteration 61/1000 | Loss: 0.00002432
Iteration 62/1000 | Loss: 0.00002428
Iteration 63/1000 | Loss: 0.00002427
Iteration 64/1000 | Loss: 0.00002423
Iteration 65/1000 | Loss: 0.00002421
Iteration 66/1000 | Loss: 0.00002419
Iteration 67/1000 | Loss: 0.00002417
Iteration 68/1000 | Loss: 0.00002416
Iteration 69/1000 | Loss: 0.00002416
Iteration 70/1000 | Loss: 0.00002411
Iteration 71/1000 | Loss: 0.00002410
Iteration 72/1000 | Loss: 0.00002409
Iteration 73/1000 | Loss: 0.00002408
Iteration 74/1000 | Loss: 0.00002408
Iteration 75/1000 | Loss: 0.00002407
Iteration 76/1000 | Loss: 0.00002407
Iteration 77/1000 | Loss: 0.00002404
Iteration 78/1000 | Loss: 0.00002404
Iteration 79/1000 | Loss: 0.00002402
Iteration 80/1000 | Loss: 0.00002399
Iteration 81/1000 | Loss: 0.00002398
Iteration 82/1000 | Loss: 0.00002396
Iteration 83/1000 | Loss: 0.00002395
Iteration 84/1000 | Loss: 0.00002395
Iteration 85/1000 | Loss: 0.00002395
Iteration 86/1000 | Loss: 0.00002394
Iteration 87/1000 | Loss: 0.00002393
Iteration 88/1000 | Loss: 0.00002392
Iteration 89/1000 | Loss: 0.00002388
Iteration 90/1000 | Loss: 0.00002387
Iteration 91/1000 | Loss: 0.00002387
Iteration 92/1000 | Loss: 0.00002387
Iteration 93/1000 | Loss: 0.00002386
Iteration 94/1000 | Loss: 0.00002386
Iteration 95/1000 | Loss: 0.00002386
Iteration 96/1000 | Loss: 0.00002385
Iteration 97/1000 | Loss: 0.00002385
Iteration 98/1000 | Loss: 0.00002385
Iteration 99/1000 | Loss: 0.00002385
Iteration 100/1000 | Loss: 0.00002385
Iteration 101/1000 | Loss: 0.00002385
Iteration 102/1000 | Loss: 0.00002385
Iteration 103/1000 | Loss: 0.00002385
Iteration 104/1000 | Loss: 0.00002385
Iteration 105/1000 | Loss: 0.00002384
Iteration 106/1000 | Loss: 0.00002384
Iteration 107/1000 | Loss: 0.00002384
Iteration 108/1000 | Loss: 0.00002384
Iteration 109/1000 | Loss: 0.00002384
Iteration 110/1000 | Loss: 0.00002383
Iteration 111/1000 | Loss: 0.00002383
Iteration 112/1000 | Loss: 0.00002383
Iteration 113/1000 | Loss: 0.00002383
Iteration 114/1000 | Loss: 0.00002382
Iteration 115/1000 | Loss: 0.00002382
Iteration 116/1000 | Loss: 0.00002382
Iteration 117/1000 | Loss: 0.00002381
Iteration 118/1000 | Loss: 0.00002381
Iteration 119/1000 | Loss: 0.00002380
Iteration 120/1000 | Loss: 0.00002380
Iteration 121/1000 | Loss: 0.00002380
Iteration 122/1000 | Loss: 0.00002380
Iteration 123/1000 | Loss: 0.00002379
Iteration 124/1000 | Loss: 0.00002379
Iteration 125/1000 | Loss: 0.00002379
Iteration 126/1000 | Loss: 0.00002379
Iteration 127/1000 | Loss: 0.00002378
Iteration 128/1000 | Loss: 0.00002378
Iteration 129/1000 | Loss: 0.00002378
Iteration 130/1000 | Loss: 0.00002378
Iteration 131/1000 | Loss: 0.00002378
Iteration 132/1000 | Loss: 0.00002378
Iteration 133/1000 | Loss: 0.00002378
Iteration 134/1000 | Loss: 0.00002378
Iteration 135/1000 | Loss: 0.00002378
Iteration 136/1000 | Loss: 0.00002378
Iteration 137/1000 | Loss: 0.00002377
Iteration 138/1000 | Loss: 0.00002377
Iteration 139/1000 | Loss: 0.00002377
Iteration 140/1000 | Loss: 0.00002377
Iteration 141/1000 | Loss: 0.00002376
Iteration 142/1000 | Loss: 0.00002376
Iteration 143/1000 | Loss: 0.00002376
Iteration 144/1000 | Loss: 0.00002376
Iteration 145/1000 | Loss: 0.00002375
Iteration 146/1000 | Loss: 0.00002375
Iteration 147/1000 | Loss: 0.00002375
Iteration 148/1000 | Loss: 0.00002375
Iteration 149/1000 | Loss: 0.00002374
Iteration 150/1000 | Loss: 0.00002374
Iteration 151/1000 | Loss: 0.00002374
Iteration 152/1000 | Loss: 0.00002374
Iteration 153/1000 | Loss: 0.00002374
Iteration 154/1000 | Loss: 0.00002373
Iteration 155/1000 | Loss: 0.00002373
Iteration 156/1000 | Loss: 0.00002373
Iteration 157/1000 | Loss: 0.00002373
Iteration 158/1000 | Loss: 0.00002373
Iteration 159/1000 | Loss: 0.00002373
Iteration 160/1000 | Loss: 0.00002373
Iteration 161/1000 | Loss: 0.00002372
Iteration 162/1000 | Loss: 0.00002372
Iteration 163/1000 | Loss: 0.00002372
Iteration 164/1000 | Loss: 0.00002372
Iteration 165/1000 | Loss: 0.00002372
Iteration 166/1000 | Loss: 0.00002372
Iteration 167/1000 | Loss: 0.00002371
Iteration 168/1000 | Loss: 0.00002371
Iteration 169/1000 | Loss: 0.00002371
Iteration 170/1000 | Loss: 0.00002371
Iteration 171/1000 | Loss: 0.00002371
Iteration 172/1000 | Loss: 0.00002371
Iteration 173/1000 | Loss: 0.00002371
Iteration 174/1000 | Loss: 0.00002370
Iteration 175/1000 | Loss: 0.00002370
Iteration 176/1000 | Loss: 0.00002370
Iteration 177/1000 | Loss: 0.00002370
Iteration 178/1000 | Loss: 0.00002369
Iteration 179/1000 | Loss: 0.00002369
Iteration 180/1000 | Loss: 0.00002369
Iteration 181/1000 | Loss: 0.00002368
Iteration 182/1000 | Loss: 0.00002368
Iteration 183/1000 | Loss: 0.00002368
Iteration 184/1000 | Loss: 0.00002368
Iteration 185/1000 | Loss: 0.00002368
Iteration 186/1000 | Loss: 0.00002368
Iteration 187/1000 | Loss: 0.00002368
Iteration 188/1000 | Loss: 0.00002367
Iteration 189/1000 | Loss: 0.00002367
Iteration 190/1000 | Loss: 0.00002367
Iteration 191/1000 | Loss: 0.00002367
Iteration 192/1000 | Loss: 0.00002367
Iteration 193/1000 | Loss: 0.00002367
Iteration 194/1000 | Loss: 0.00002367
Iteration 195/1000 | Loss: 0.00002367
Iteration 196/1000 | Loss: 0.00002367
Iteration 197/1000 | Loss: 0.00002367
Iteration 198/1000 | Loss: 0.00002367
Iteration 199/1000 | Loss: 0.00002367
Iteration 200/1000 | Loss: 0.00002367
Iteration 201/1000 | Loss: 0.00002367
Iteration 202/1000 | Loss: 0.00002367
Iteration 203/1000 | Loss: 0.00002367
Iteration 204/1000 | Loss: 0.00002367
Iteration 205/1000 | Loss: 0.00002367
Iteration 206/1000 | Loss: 0.00002367
Iteration 207/1000 | Loss: 0.00002367
Iteration 208/1000 | Loss: 0.00002367
Iteration 209/1000 | Loss: 0.00002367
Iteration 210/1000 | Loss: 0.00002367
Iteration 211/1000 | Loss: 0.00002367
Iteration 212/1000 | Loss: 0.00002367
Iteration 213/1000 | Loss: 0.00002367
Iteration 214/1000 | Loss: 0.00002367
Iteration 215/1000 | Loss: 0.00002367
Iteration 216/1000 | Loss: 0.00002367
Iteration 217/1000 | Loss: 0.00002367
Iteration 218/1000 | Loss: 0.00002367
Iteration 219/1000 | Loss: 0.00002367
Iteration 220/1000 | Loss: 0.00002367
Iteration 221/1000 | Loss: 0.00002367
Iteration 222/1000 | Loss: 0.00002367
Iteration 223/1000 | Loss: 0.00002367
Iteration 224/1000 | Loss: 0.00002367
Iteration 225/1000 | Loss: 0.00002367
Iteration 226/1000 | Loss: 0.00002367
Iteration 227/1000 | Loss: 0.00002367
Iteration 228/1000 | Loss: 0.00002367
Iteration 229/1000 | Loss: 0.00002367
Iteration 230/1000 | Loss: 0.00002367
Iteration 231/1000 | Loss: 0.00002367
Iteration 232/1000 | Loss: 0.00002367
Iteration 233/1000 | Loss: 0.00002367
Iteration 234/1000 | Loss: 0.00002367
Iteration 235/1000 | Loss: 0.00002367
Iteration 236/1000 | Loss: 0.00002367
Iteration 237/1000 | Loss: 0.00002367
Iteration 238/1000 | Loss: 0.00002367
Iteration 239/1000 | Loss: 0.00002367
Iteration 240/1000 | Loss: 0.00002367
Iteration 241/1000 | Loss: 0.00002367
Iteration 242/1000 | Loss: 0.00002367
Iteration 243/1000 | Loss: 0.00002367
Iteration 244/1000 | Loss: 0.00002367
Iteration 245/1000 | Loss: 0.00002367
Iteration 246/1000 | Loss: 0.00002367
Iteration 247/1000 | Loss: 0.00002367
Iteration 248/1000 | Loss: 0.00002367
Iteration 249/1000 | Loss: 0.00002367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.3666423658141866e-05, 2.3666423658141866e-05, 2.3666423658141866e-05, 2.3666423658141866e-05, 2.3666423658141866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3666423658141866e-05

Optimization complete. Final v2v error: 3.1868464946746826 mm

Highest mean error: 11.14074993133545 mm for frame 134

Lowest mean error: 2.6085546016693115 mm for frame 79

Saving results

Total time: 160.46125650405884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815526
Iteration 2/25 | Loss: 0.00146310
Iteration 3/25 | Loss: 0.00135036
Iteration 4/25 | Loss: 0.00133445
Iteration 5/25 | Loss: 0.00133010
Iteration 6/25 | Loss: 0.00132945
Iteration 7/25 | Loss: 0.00132945
Iteration 8/25 | Loss: 0.00132945
Iteration 9/25 | Loss: 0.00132945
Iteration 10/25 | Loss: 0.00132945
Iteration 11/25 | Loss: 0.00132945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001329445163719356, 0.001329445163719356, 0.001329445163719356, 0.001329445163719356, 0.001329445163719356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001329445163719356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75814247
Iteration 2/25 | Loss: 0.00103984
Iteration 3/25 | Loss: 0.00103981
Iteration 4/25 | Loss: 0.00103981
Iteration 5/25 | Loss: 0.00103980
Iteration 6/25 | Loss: 0.00103980
Iteration 7/25 | Loss: 0.00103980
Iteration 8/25 | Loss: 0.00103980
Iteration 9/25 | Loss: 0.00103980
Iteration 10/25 | Loss: 0.00103980
Iteration 11/25 | Loss: 0.00103980
Iteration 12/25 | Loss: 0.00103980
Iteration 13/25 | Loss: 0.00103980
Iteration 14/25 | Loss: 0.00103980
Iteration 15/25 | Loss: 0.00103980
Iteration 16/25 | Loss: 0.00103980
Iteration 17/25 | Loss: 0.00103980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001039801980368793, 0.001039801980368793, 0.001039801980368793, 0.001039801980368793, 0.001039801980368793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001039801980368793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103980
Iteration 2/1000 | Loss: 0.00004569
Iteration 3/1000 | Loss: 0.00002849
Iteration 4/1000 | Loss: 0.00002183
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00001985
Iteration 7/1000 | Loss: 0.00001943
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001835
Iteration 11/1000 | Loss: 0.00001807
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001764
Iteration 16/1000 | Loss: 0.00001756
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001755
Iteration 19/1000 | Loss: 0.00001751
Iteration 20/1000 | Loss: 0.00001749
Iteration 21/1000 | Loss: 0.00001748
Iteration 22/1000 | Loss: 0.00001748
Iteration 23/1000 | Loss: 0.00001747
Iteration 24/1000 | Loss: 0.00001747
Iteration 25/1000 | Loss: 0.00001747
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00001746
Iteration 28/1000 | Loss: 0.00001745
Iteration 29/1000 | Loss: 0.00001743
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001738
Iteration 35/1000 | Loss: 0.00001738
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001734
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001731
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001731
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001730
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001729
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001727
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001725
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001725
Iteration 71/1000 | Loss: 0.00001725
Iteration 72/1000 | Loss: 0.00001724
Iteration 73/1000 | Loss: 0.00001724
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001723
Iteration 76/1000 | Loss: 0.00001723
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00001723
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001722
Iteration 82/1000 | Loss: 0.00001721
Iteration 83/1000 | Loss: 0.00001721
Iteration 84/1000 | Loss: 0.00001721
Iteration 85/1000 | Loss: 0.00001720
Iteration 86/1000 | Loss: 0.00001720
Iteration 87/1000 | Loss: 0.00001720
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001717
Iteration 96/1000 | Loss: 0.00001717
Iteration 97/1000 | Loss: 0.00001717
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001716
Iteration 105/1000 | Loss: 0.00001716
Iteration 106/1000 | Loss: 0.00001716
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001715
Iteration 109/1000 | Loss: 0.00001715
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001714
Iteration 112/1000 | Loss: 0.00001714
Iteration 113/1000 | Loss: 0.00001714
Iteration 114/1000 | Loss: 0.00001714
Iteration 115/1000 | Loss: 0.00001714
Iteration 116/1000 | Loss: 0.00001713
Iteration 117/1000 | Loss: 0.00001713
Iteration 118/1000 | Loss: 0.00001713
Iteration 119/1000 | Loss: 0.00001713
Iteration 120/1000 | Loss: 0.00001713
Iteration 121/1000 | Loss: 0.00001712
Iteration 122/1000 | Loss: 0.00001712
Iteration 123/1000 | Loss: 0.00001712
Iteration 124/1000 | Loss: 0.00001712
Iteration 125/1000 | Loss: 0.00001712
Iteration 126/1000 | Loss: 0.00001712
Iteration 127/1000 | Loss: 0.00001712
Iteration 128/1000 | Loss: 0.00001712
Iteration 129/1000 | Loss: 0.00001712
Iteration 130/1000 | Loss: 0.00001712
Iteration 131/1000 | Loss: 0.00001712
Iteration 132/1000 | Loss: 0.00001711
Iteration 133/1000 | Loss: 0.00001711
Iteration 134/1000 | Loss: 0.00001711
Iteration 135/1000 | Loss: 0.00001711
Iteration 136/1000 | Loss: 0.00001711
Iteration 137/1000 | Loss: 0.00001711
Iteration 138/1000 | Loss: 0.00001711
Iteration 139/1000 | Loss: 0.00001711
Iteration 140/1000 | Loss: 0.00001711
Iteration 141/1000 | Loss: 0.00001711
Iteration 142/1000 | Loss: 0.00001710
Iteration 143/1000 | Loss: 0.00001710
Iteration 144/1000 | Loss: 0.00001710
Iteration 145/1000 | Loss: 0.00001710
Iteration 146/1000 | Loss: 0.00001710
Iteration 147/1000 | Loss: 0.00001710
Iteration 148/1000 | Loss: 0.00001710
Iteration 149/1000 | Loss: 0.00001710
Iteration 150/1000 | Loss: 0.00001710
Iteration 151/1000 | Loss: 0.00001710
Iteration 152/1000 | Loss: 0.00001710
Iteration 153/1000 | Loss: 0.00001710
Iteration 154/1000 | Loss: 0.00001710
Iteration 155/1000 | Loss: 0.00001710
Iteration 156/1000 | Loss: 0.00001710
Iteration 157/1000 | Loss: 0.00001710
Iteration 158/1000 | Loss: 0.00001710
Iteration 159/1000 | Loss: 0.00001710
Iteration 160/1000 | Loss: 0.00001710
Iteration 161/1000 | Loss: 0.00001710
Iteration 162/1000 | Loss: 0.00001710
Iteration 163/1000 | Loss: 0.00001710
Iteration 164/1000 | Loss: 0.00001710
Iteration 165/1000 | Loss: 0.00001710
Iteration 166/1000 | Loss: 0.00001710
Iteration 167/1000 | Loss: 0.00001709
Iteration 168/1000 | Loss: 0.00001709
Iteration 169/1000 | Loss: 0.00001709
Iteration 170/1000 | Loss: 0.00001709
Iteration 171/1000 | Loss: 0.00001709
Iteration 172/1000 | Loss: 0.00001709
Iteration 173/1000 | Loss: 0.00001709
Iteration 174/1000 | Loss: 0.00001709
Iteration 175/1000 | Loss: 0.00001709
Iteration 176/1000 | Loss: 0.00001709
Iteration 177/1000 | Loss: 0.00001709
Iteration 178/1000 | Loss: 0.00001709
Iteration 179/1000 | Loss: 0.00001709
Iteration 180/1000 | Loss: 0.00001709
Iteration 181/1000 | Loss: 0.00001709
Iteration 182/1000 | Loss: 0.00001709
Iteration 183/1000 | Loss: 0.00001709
Iteration 184/1000 | Loss: 0.00001709
Iteration 185/1000 | Loss: 0.00001709
Iteration 186/1000 | Loss: 0.00001709
Iteration 187/1000 | Loss: 0.00001708
Iteration 188/1000 | Loss: 0.00001708
Iteration 189/1000 | Loss: 0.00001708
Iteration 190/1000 | Loss: 0.00001708
Iteration 191/1000 | Loss: 0.00001708
Iteration 192/1000 | Loss: 0.00001708
Iteration 193/1000 | Loss: 0.00001708
Iteration 194/1000 | Loss: 0.00001708
Iteration 195/1000 | Loss: 0.00001708
Iteration 196/1000 | Loss: 0.00001708
Iteration 197/1000 | Loss: 0.00001708
Iteration 198/1000 | Loss: 0.00001708
Iteration 199/1000 | Loss: 0.00001708
Iteration 200/1000 | Loss: 0.00001708
Iteration 201/1000 | Loss: 0.00001708
Iteration 202/1000 | Loss: 0.00001708
Iteration 203/1000 | Loss: 0.00001708
Iteration 204/1000 | Loss: 0.00001708
Iteration 205/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.707904266368132e-05, 1.707904266368132e-05, 1.707904266368132e-05, 1.707904266368132e-05, 1.707904266368132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.707904266368132e-05

Optimization complete. Final v2v error: 3.487999677658081 mm

Highest mean error: 3.727541446685791 mm for frame 112

Lowest mean error: 3.1914467811584473 mm for frame 4

Saving results

Total time: 41.544787883758545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454871
Iteration 2/25 | Loss: 0.00154047
Iteration 3/25 | Loss: 0.00132828
Iteration 4/25 | Loss: 0.00130157
Iteration 5/25 | Loss: 0.00129721
Iteration 6/25 | Loss: 0.00129615
Iteration 7/25 | Loss: 0.00129615
Iteration 8/25 | Loss: 0.00129615
Iteration 9/25 | Loss: 0.00129615
Iteration 10/25 | Loss: 0.00129615
Iteration 11/25 | Loss: 0.00129615
Iteration 12/25 | Loss: 0.00129615
Iteration 13/25 | Loss: 0.00129615
Iteration 14/25 | Loss: 0.00129615
Iteration 15/25 | Loss: 0.00129615
Iteration 16/25 | Loss: 0.00129615
Iteration 17/25 | Loss: 0.00129615
Iteration 18/25 | Loss: 0.00129615
Iteration 19/25 | Loss: 0.00129615
Iteration 20/25 | Loss: 0.00129615
Iteration 21/25 | Loss: 0.00129615
Iteration 22/25 | Loss: 0.00129615
Iteration 23/25 | Loss: 0.00129615
Iteration 24/25 | Loss: 0.00129615
Iteration 25/25 | Loss: 0.00129615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50834346
Iteration 2/25 | Loss: 0.00080784
Iteration 3/25 | Loss: 0.00080784
Iteration 4/25 | Loss: 0.00080784
Iteration 5/25 | Loss: 0.00080784
Iteration 6/25 | Loss: 0.00080784
Iteration 7/25 | Loss: 0.00080784
Iteration 8/25 | Loss: 0.00080784
Iteration 9/25 | Loss: 0.00080784
Iteration 10/25 | Loss: 0.00080784
Iteration 11/25 | Loss: 0.00080784
Iteration 12/25 | Loss: 0.00080784
Iteration 13/25 | Loss: 0.00080784
Iteration 14/25 | Loss: 0.00080784
Iteration 15/25 | Loss: 0.00080784
Iteration 16/25 | Loss: 0.00080784
Iteration 17/25 | Loss: 0.00080784
Iteration 18/25 | Loss: 0.00080784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008078376413322985, 0.0008078376413322985, 0.0008078376413322985, 0.0008078376413322985, 0.0008078376413322985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008078376413322985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080784
Iteration 2/1000 | Loss: 0.00002908
Iteration 3/1000 | Loss: 0.00002110
Iteration 4/1000 | Loss: 0.00001929
Iteration 5/1000 | Loss: 0.00001837
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001671
Iteration 10/1000 | Loss: 0.00001643
Iteration 11/1000 | Loss: 0.00001622
Iteration 12/1000 | Loss: 0.00001612
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001599
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001585
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001578
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001573
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001570
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001569
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001565
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001564
Iteration 42/1000 | Loss: 0.00001562
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001561
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001559
Iteration 50/1000 | Loss: 0.00001559
Iteration 51/1000 | Loss: 0.00001558
Iteration 52/1000 | Loss: 0.00001558
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001550
Iteration 71/1000 | Loss: 0.00001550
Iteration 72/1000 | Loss: 0.00001550
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001549
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001547
Iteration 80/1000 | Loss: 0.00001547
Iteration 81/1000 | Loss: 0.00001547
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001544
Iteration 94/1000 | Loss: 0.00001544
Iteration 95/1000 | Loss: 0.00001543
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001541
Iteration 105/1000 | Loss: 0.00001541
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001541
Iteration 111/1000 | Loss: 0.00001541
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001539
Iteration 117/1000 | Loss: 0.00001539
Iteration 118/1000 | Loss: 0.00001539
Iteration 119/1000 | Loss: 0.00001539
Iteration 120/1000 | Loss: 0.00001539
Iteration 121/1000 | Loss: 0.00001539
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001538
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001538
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001537
Iteration 132/1000 | Loss: 0.00001537
Iteration 133/1000 | Loss: 0.00001537
Iteration 134/1000 | Loss: 0.00001537
Iteration 135/1000 | Loss: 0.00001537
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001537
Iteration 139/1000 | Loss: 0.00001536
Iteration 140/1000 | Loss: 0.00001536
Iteration 141/1000 | Loss: 0.00001535
Iteration 142/1000 | Loss: 0.00001535
Iteration 143/1000 | Loss: 0.00001534
Iteration 144/1000 | Loss: 0.00001534
Iteration 145/1000 | Loss: 0.00001534
Iteration 146/1000 | Loss: 0.00001534
Iteration 147/1000 | Loss: 0.00001534
Iteration 148/1000 | Loss: 0.00001534
Iteration 149/1000 | Loss: 0.00001534
Iteration 150/1000 | Loss: 0.00001534
Iteration 151/1000 | Loss: 0.00001534
Iteration 152/1000 | Loss: 0.00001533
Iteration 153/1000 | Loss: 0.00001533
Iteration 154/1000 | Loss: 0.00001533
Iteration 155/1000 | Loss: 0.00001533
Iteration 156/1000 | Loss: 0.00001533
Iteration 157/1000 | Loss: 0.00001533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.5334750059992075e-05, 1.5334750059992075e-05, 1.5334750059992075e-05, 1.5334750059992075e-05, 1.5334750059992075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5334750059992075e-05

Optimization complete. Final v2v error: 3.29826283454895 mm

Highest mean error: 4.2769269943237305 mm for frame 105

Lowest mean error: 2.9517765045166016 mm for frame 159

Saving results

Total time: 43.24257969856262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074172
Iteration 2/25 | Loss: 0.01074172
Iteration 3/25 | Loss: 0.01074172
Iteration 4/25 | Loss: 0.01074172
Iteration 5/25 | Loss: 0.01074172
Iteration 6/25 | Loss: 0.01074171
Iteration 7/25 | Loss: 0.01074171
Iteration 8/25 | Loss: 0.01074171
Iteration 9/25 | Loss: 0.01074171
Iteration 10/25 | Loss: 0.01074171
Iteration 11/25 | Loss: 0.01074171
Iteration 12/25 | Loss: 0.01074171
Iteration 13/25 | Loss: 0.01074171
Iteration 14/25 | Loss: 0.01074171
Iteration 15/25 | Loss: 0.01074171
Iteration 16/25 | Loss: 0.01074171
Iteration 17/25 | Loss: 0.01074171
Iteration 18/25 | Loss: 0.01074171
Iteration 19/25 | Loss: 0.01074171
Iteration 20/25 | Loss: 0.01074171
Iteration 21/25 | Loss: 0.01074171
Iteration 22/25 | Loss: 0.01074171
Iteration 23/25 | Loss: 0.01074170
Iteration 24/25 | Loss: 0.01074170
Iteration 25/25 | Loss: 0.01074170

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.33985901
Iteration 2/25 | Loss: 0.18002993
Iteration 3/25 | Loss: 0.17780101
Iteration 4/25 | Loss: 0.17752168
Iteration 5/25 | Loss: 0.17748661
Iteration 6/25 | Loss: 0.17748658
Iteration 7/25 | Loss: 0.17748658
Iteration 8/25 | Loss: 0.17748658
Iteration 9/25 | Loss: 0.17748658
Iteration 10/25 | Loss: 0.17748655
Iteration 11/25 | Loss: 0.17748655
Iteration 12/25 | Loss: 0.17748655
Iteration 13/25 | Loss: 0.17748654
Iteration 14/25 | Loss: 0.17748654
Iteration 15/25 | Loss: 0.17748654
Iteration 16/25 | Loss: 0.17748654
Iteration 17/25 | Loss: 0.17748654
Iteration 18/25 | Loss: 0.17748654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.17748653888702393, 0.17748653888702393, 0.17748653888702393, 0.17748653888702393, 0.17748653888702393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17748653888702393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17748654
Iteration 2/1000 | Loss: 0.00685727
Iteration 3/1000 | Loss: 0.00074945
Iteration 4/1000 | Loss: 0.00197793
Iteration 5/1000 | Loss: 0.00046008
Iteration 6/1000 | Loss: 0.00010321
Iteration 7/1000 | Loss: 0.00052957
Iteration 8/1000 | Loss: 0.00033642
Iteration 9/1000 | Loss: 0.00060732
Iteration 10/1000 | Loss: 0.00010812
Iteration 11/1000 | Loss: 0.00011477
Iteration 12/1000 | Loss: 0.00020937
Iteration 13/1000 | Loss: 0.00034659
Iteration 14/1000 | Loss: 0.00003192
Iteration 15/1000 | Loss: 0.00007272
Iteration 16/1000 | Loss: 0.00008081
Iteration 17/1000 | Loss: 0.00002953
Iteration 18/1000 | Loss: 0.00014779
Iteration 19/1000 | Loss: 0.00002631
Iteration 20/1000 | Loss: 0.00021937
Iteration 21/1000 | Loss: 0.00030196
Iteration 22/1000 | Loss: 0.00105050
Iteration 23/1000 | Loss: 0.00008663
Iteration 24/1000 | Loss: 0.00013540
Iteration 25/1000 | Loss: 0.00008837
Iteration 26/1000 | Loss: 0.00002264
Iteration 27/1000 | Loss: 0.00002177
Iteration 28/1000 | Loss: 0.00002104
Iteration 29/1000 | Loss: 0.00022092
Iteration 30/1000 | Loss: 0.00096434
Iteration 31/1000 | Loss: 0.00008627
Iteration 32/1000 | Loss: 0.00016443
Iteration 33/1000 | Loss: 0.00001982
Iteration 34/1000 | Loss: 0.00001921
Iteration 35/1000 | Loss: 0.00006294
Iteration 36/1000 | Loss: 0.00001837
Iteration 37/1000 | Loss: 0.00001797
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00010906
Iteration 46/1000 | Loss: 0.00003662
Iteration 47/1000 | Loss: 0.00025493
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001633
Iteration 53/1000 | Loss: 0.00001614
Iteration 54/1000 | Loss: 0.00001613
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001612
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001609
Iteration 61/1000 | Loss: 0.00001609
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001666
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001596
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001637
Iteration 74/1000 | Loss: 0.00001613
Iteration 75/1000 | Loss: 0.00001594
Iteration 76/1000 | Loss: 0.00001593
Iteration 77/1000 | Loss: 0.00001593
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00001592
Iteration 81/1000 | Loss: 0.00001592
Iteration 82/1000 | Loss: 0.00001592
Iteration 83/1000 | Loss: 0.00001591
Iteration 84/1000 | Loss: 0.00001591
Iteration 85/1000 | Loss: 0.00006848
Iteration 86/1000 | Loss: 0.00001596
Iteration 87/1000 | Loss: 0.00001617
Iteration 88/1000 | Loss: 0.00001616
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00004776
Iteration 91/1000 | Loss: 0.00001625
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001578
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001569
Iteration 112/1000 | Loss: 0.00001569
Iteration 113/1000 | Loss: 0.00001569
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001568
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001568
Iteration 124/1000 | Loss: 0.00001568
Iteration 125/1000 | Loss: 0.00001568
Iteration 126/1000 | Loss: 0.00001568
Iteration 127/1000 | Loss: 0.00001568
Iteration 128/1000 | Loss: 0.00001568
Iteration 129/1000 | Loss: 0.00001568
Iteration 130/1000 | Loss: 0.00001568
Iteration 131/1000 | Loss: 0.00001568
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001567
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001567
Iteration 142/1000 | Loss: 0.00001567
Iteration 143/1000 | Loss: 0.00001567
Iteration 144/1000 | Loss: 0.00001567
Iteration 145/1000 | Loss: 0.00001567
Iteration 146/1000 | Loss: 0.00001567
Iteration 147/1000 | Loss: 0.00001567
Iteration 148/1000 | Loss: 0.00001567
Iteration 149/1000 | Loss: 0.00001567
Iteration 150/1000 | Loss: 0.00001566
Iteration 151/1000 | Loss: 0.00001566
Iteration 152/1000 | Loss: 0.00001566
Iteration 153/1000 | Loss: 0.00001566
Iteration 154/1000 | Loss: 0.00001566
Iteration 155/1000 | Loss: 0.00001566
Iteration 156/1000 | Loss: 0.00001566
Iteration 157/1000 | Loss: 0.00001566
Iteration 158/1000 | Loss: 0.00001566
Iteration 159/1000 | Loss: 0.00001566
Iteration 160/1000 | Loss: 0.00001566
Iteration 161/1000 | Loss: 0.00001566
Iteration 162/1000 | Loss: 0.00001566
Iteration 163/1000 | Loss: 0.00001566
Iteration 164/1000 | Loss: 0.00001566
Iteration 165/1000 | Loss: 0.00001566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.566232822369784e-05, 1.566232822369784e-05, 1.566232822369784e-05, 1.566232822369784e-05, 1.566232822369784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.566232822369784e-05

Optimization complete. Final v2v error: 3.327941656112671 mm

Highest mean error: 8.758523941040039 mm for frame 158

Lowest mean error: 3.043868064880371 mm for frame 11

Saving results

Total time: 112.5352783203125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699402
Iteration 2/25 | Loss: 0.00146243
Iteration 3/25 | Loss: 0.00132326
Iteration 4/25 | Loss: 0.00128774
Iteration 5/25 | Loss: 0.00127926
Iteration 6/25 | Loss: 0.00127699
Iteration 7/25 | Loss: 0.00127655
Iteration 8/25 | Loss: 0.00127620
Iteration 9/25 | Loss: 0.00127597
Iteration 10/25 | Loss: 0.00127609
Iteration 11/25 | Loss: 0.00127579
Iteration 12/25 | Loss: 0.00127527
Iteration 13/25 | Loss: 0.00127505
Iteration 14/25 | Loss: 0.00127505
Iteration 15/25 | Loss: 0.00127505
Iteration 16/25 | Loss: 0.00127505
Iteration 17/25 | Loss: 0.00127504
Iteration 18/25 | Loss: 0.00127504
Iteration 19/25 | Loss: 0.00127504
Iteration 20/25 | Loss: 0.00127504
Iteration 21/25 | Loss: 0.00127504
Iteration 22/25 | Loss: 0.00127504
Iteration 23/25 | Loss: 0.00127504
Iteration 24/25 | Loss: 0.00127504
Iteration 25/25 | Loss: 0.00127504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91926312
Iteration 2/25 | Loss: 0.00091273
Iteration 3/25 | Loss: 0.00091273
Iteration 4/25 | Loss: 0.00091273
Iteration 5/25 | Loss: 0.00091272
Iteration 6/25 | Loss: 0.00091272
Iteration 7/25 | Loss: 0.00091272
Iteration 8/25 | Loss: 0.00091272
Iteration 9/25 | Loss: 0.00091272
Iteration 10/25 | Loss: 0.00091272
Iteration 11/25 | Loss: 0.00091272
Iteration 12/25 | Loss: 0.00091272
Iteration 13/25 | Loss: 0.00091272
Iteration 14/25 | Loss: 0.00091272
Iteration 15/25 | Loss: 0.00091272
Iteration 16/25 | Loss: 0.00091272
Iteration 17/25 | Loss: 0.00091272
Iteration 18/25 | Loss: 0.00091272
Iteration 19/25 | Loss: 0.00091272
Iteration 20/25 | Loss: 0.00091272
Iteration 21/25 | Loss: 0.00091272
Iteration 22/25 | Loss: 0.00091272
Iteration 23/25 | Loss: 0.00091272
Iteration 24/25 | Loss: 0.00091272
Iteration 25/25 | Loss: 0.00091272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091272
Iteration 2/1000 | Loss: 0.00002231
Iteration 3/1000 | Loss: 0.00001758
Iteration 4/1000 | Loss: 0.00007274
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001591
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00007361
Iteration 11/1000 | Loss: 0.00001531
Iteration 12/1000 | Loss: 0.00001440
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001399
Iteration 16/1000 | Loss: 0.00010239
Iteration 17/1000 | Loss: 0.00001905
Iteration 18/1000 | Loss: 0.00002196
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001493
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001362
Iteration 23/1000 | Loss: 0.00001361
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001356
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001348
Iteration 29/1000 | Loss: 0.00001348
Iteration 30/1000 | Loss: 0.00001337
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001332
Iteration 36/1000 | Loss: 0.00001332
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001332
Iteration 39/1000 | Loss: 0.00001332
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001323
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001322
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001321
Iteration 68/1000 | Loss: 0.00001321
Iteration 69/1000 | Loss: 0.00001320
Iteration 70/1000 | Loss: 0.00001320
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001317
Iteration 76/1000 | Loss: 0.00001317
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001311
Iteration 82/1000 | Loss: 0.00001311
Iteration 83/1000 | Loss: 0.00001308
Iteration 84/1000 | Loss: 0.00001308
Iteration 85/1000 | Loss: 0.00001308
Iteration 86/1000 | Loss: 0.00001308
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001307
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001306
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001306
Iteration 109/1000 | Loss: 0.00001306
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001305
Iteration 136/1000 | Loss: 0.00001305
Iteration 137/1000 | Loss: 0.00001305
Iteration 138/1000 | Loss: 0.00001305
Iteration 139/1000 | Loss: 0.00001305
Iteration 140/1000 | Loss: 0.00001305
Iteration 141/1000 | Loss: 0.00001305
Iteration 142/1000 | Loss: 0.00001305
Iteration 143/1000 | Loss: 0.00001305
Iteration 144/1000 | Loss: 0.00001305
Iteration 145/1000 | Loss: 0.00001305
Iteration 146/1000 | Loss: 0.00001305
Iteration 147/1000 | Loss: 0.00001305
Iteration 148/1000 | Loss: 0.00001305
Iteration 149/1000 | Loss: 0.00001305
Iteration 150/1000 | Loss: 0.00001305
Iteration 151/1000 | Loss: 0.00001305
Iteration 152/1000 | Loss: 0.00001305
Iteration 153/1000 | Loss: 0.00001305
Iteration 154/1000 | Loss: 0.00001305
Iteration 155/1000 | Loss: 0.00001305
Iteration 156/1000 | Loss: 0.00001305
Iteration 157/1000 | Loss: 0.00001305
Iteration 158/1000 | Loss: 0.00001305
Iteration 159/1000 | Loss: 0.00001305
Iteration 160/1000 | Loss: 0.00001305
Iteration 161/1000 | Loss: 0.00001305
Iteration 162/1000 | Loss: 0.00001305
Iteration 163/1000 | Loss: 0.00001305
Iteration 164/1000 | Loss: 0.00001305
Iteration 165/1000 | Loss: 0.00001305
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001305
Iteration 169/1000 | Loss: 0.00001305
Iteration 170/1000 | Loss: 0.00001305
Iteration 171/1000 | Loss: 0.00001305
Iteration 172/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.305296245845966e-05, 1.305296245845966e-05, 1.305296245845966e-05, 1.305296245845966e-05, 1.305296245845966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.305296245845966e-05

Optimization complete. Final v2v error: 3.0919411182403564 mm

Highest mean error: 3.422185182571411 mm for frame 126

Lowest mean error: 2.888712167739868 mm for frame 265

Saving results

Total time: 73.21392869949341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412583
Iteration 2/25 | Loss: 0.00137866
Iteration 3/25 | Loss: 0.00132105
Iteration 4/25 | Loss: 0.00131313
Iteration 5/25 | Loss: 0.00131051
Iteration 6/25 | Loss: 0.00130987
Iteration 7/25 | Loss: 0.00130981
Iteration 8/25 | Loss: 0.00130981
Iteration 9/25 | Loss: 0.00130981
Iteration 10/25 | Loss: 0.00130981
Iteration 11/25 | Loss: 0.00130981
Iteration 12/25 | Loss: 0.00130981
Iteration 13/25 | Loss: 0.00130981
Iteration 14/25 | Loss: 0.00130981
Iteration 15/25 | Loss: 0.00130981
Iteration 16/25 | Loss: 0.00130981
Iteration 17/25 | Loss: 0.00130981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00130981404799968, 0.00130981404799968, 0.00130981404799968, 0.00130981404799968, 0.00130981404799968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00130981404799968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47420943
Iteration 2/25 | Loss: 0.00092960
Iteration 3/25 | Loss: 0.00092960
Iteration 4/25 | Loss: 0.00092960
Iteration 5/25 | Loss: 0.00092960
Iteration 6/25 | Loss: 0.00092960
Iteration 7/25 | Loss: 0.00092960
Iteration 8/25 | Loss: 0.00092960
Iteration 9/25 | Loss: 0.00092960
Iteration 10/25 | Loss: 0.00092959
Iteration 11/25 | Loss: 0.00092959
Iteration 12/25 | Loss: 0.00092959
Iteration 13/25 | Loss: 0.00092959
Iteration 14/25 | Loss: 0.00092959
Iteration 15/25 | Loss: 0.00092959
Iteration 16/25 | Loss: 0.00092959
Iteration 17/25 | Loss: 0.00092959
Iteration 18/25 | Loss: 0.00092959
Iteration 19/25 | Loss: 0.00092959
Iteration 20/25 | Loss: 0.00092959
Iteration 21/25 | Loss: 0.00092959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009295946219936013, 0.0009295946219936013, 0.0009295946219936013, 0.0009295946219936013, 0.0009295946219936013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009295946219936013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092959
Iteration 2/1000 | Loss: 0.00003143
Iteration 3/1000 | Loss: 0.00002136
Iteration 4/1000 | Loss: 0.00001884
Iteration 5/1000 | Loss: 0.00001779
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001669
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001605
Iteration 10/1000 | Loss: 0.00001602
Iteration 11/1000 | Loss: 0.00001582
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001547
Iteration 15/1000 | Loss: 0.00001544
Iteration 16/1000 | Loss: 0.00001543
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001521
Iteration 20/1000 | Loss: 0.00001520
Iteration 21/1000 | Loss: 0.00001520
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001506
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001498
Iteration 51/1000 | Loss: 0.00001498
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001497
Iteration 54/1000 | Loss: 0.00001497
Iteration 55/1000 | Loss: 0.00001496
Iteration 56/1000 | Loss: 0.00001496
Iteration 57/1000 | Loss: 0.00001496
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001495
Iteration 60/1000 | Loss: 0.00001495
Iteration 61/1000 | Loss: 0.00001495
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001494
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001492
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001489
Iteration 86/1000 | Loss: 0.00001489
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001488
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001488
Iteration 97/1000 | Loss: 0.00001488
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001486
Iteration 107/1000 | Loss: 0.00001486
Iteration 108/1000 | Loss: 0.00001486
Iteration 109/1000 | Loss: 0.00001486
Iteration 110/1000 | Loss: 0.00001485
Iteration 111/1000 | Loss: 0.00001485
Iteration 112/1000 | Loss: 0.00001485
Iteration 113/1000 | Loss: 0.00001484
Iteration 114/1000 | Loss: 0.00001484
Iteration 115/1000 | Loss: 0.00001484
Iteration 116/1000 | Loss: 0.00001484
Iteration 117/1000 | Loss: 0.00001484
Iteration 118/1000 | Loss: 0.00001483
Iteration 119/1000 | Loss: 0.00001483
Iteration 120/1000 | Loss: 0.00001483
Iteration 121/1000 | Loss: 0.00001483
Iteration 122/1000 | Loss: 0.00001483
Iteration 123/1000 | Loss: 0.00001482
Iteration 124/1000 | Loss: 0.00001482
Iteration 125/1000 | Loss: 0.00001482
Iteration 126/1000 | Loss: 0.00001482
Iteration 127/1000 | Loss: 0.00001482
Iteration 128/1000 | Loss: 0.00001482
Iteration 129/1000 | Loss: 0.00001482
Iteration 130/1000 | Loss: 0.00001482
Iteration 131/1000 | Loss: 0.00001482
Iteration 132/1000 | Loss: 0.00001482
Iteration 133/1000 | Loss: 0.00001482
Iteration 134/1000 | Loss: 0.00001482
Iteration 135/1000 | Loss: 0.00001482
Iteration 136/1000 | Loss: 0.00001481
Iteration 137/1000 | Loss: 0.00001481
Iteration 138/1000 | Loss: 0.00001481
Iteration 139/1000 | Loss: 0.00001481
Iteration 140/1000 | Loss: 0.00001481
Iteration 141/1000 | Loss: 0.00001481
Iteration 142/1000 | Loss: 0.00001480
Iteration 143/1000 | Loss: 0.00001480
Iteration 144/1000 | Loss: 0.00001480
Iteration 145/1000 | Loss: 0.00001480
Iteration 146/1000 | Loss: 0.00001480
Iteration 147/1000 | Loss: 0.00001480
Iteration 148/1000 | Loss: 0.00001480
Iteration 149/1000 | Loss: 0.00001480
Iteration 150/1000 | Loss: 0.00001480
Iteration 151/1000 | Loss: 0.00001480
Iteration 152/1000 | Loss: 0.00001480
Iteration 153/1000 | Loss: 0.00001480
Iteration 154/1000 | Loss: 0.00001480
Iteration 155/1000 | Loss: 0.00001480
Iteration 156/1000 | Loss: 0.00001480
Iteration 157/1000 | Loss: 0.00001479
Iteration 158/1000 | Loss: 0.00001479
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001479
Iteration 162/1000 | Loss: 0.00001479
Iteration 163/1000 | Loss: 0.00001479
Iteration 164/1000 | Loss: 0.00001479
Iteration 165/1000 | Loss: 0.00001479
Iteration 166/1000 | Loss: 0.00001479
Iteration 167/1000 | Loss: 0.00001479
Iteration 168/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.4789665328862611e-05, 1.4789665328862611e-05, 1.4789665328862611e-05, 1.4789665328862611e-05, 1.4789665328862611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4789665328862611e-05

Optimization complete. Final v2v error: 3.258251667022705 mm

Highest mean error: 4.124165058135986 mm for frame 44

Lowest mean error: 2.9899778366088867 mm for frame 8

Saving results

Total time: 39.84044337272644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803180
Iteration 2/25 | Loss: 0.00147977
Iteration 3/25 | Loss: 0.00135335
Iteration 4/25 | Loss: 0.00133111
Iteration 5/25 | Loss: 0.00132334
Iteration 6/25 | Loss: 0.00132126
Iteration 7/25 | Loss: 0.00132084
Iteration 8/25 | Loss: 0.00132084
Iteration 9/25 | Loss: 0.00132084
Iteration 10/25 | Loss: 0.00132084
Iteration 11/25 | Loss: 0.00132084
Iteration 12/25 | Loss: 0.00132084
Iteration 13/25 | Loss: 0.00132084
Iteration 14/25 | Loss: 0.00132084
Iteration 15/25 | Loss: 0.00132084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013208440504968166, 0.0013208440504968166, 0.0013208440504968166, 0.0013208440504968166, 0.0013208440504968166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013208440504968166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46158850
Iteration 2/25 | Loss: 0.00110107
Iteration 3/25 | Loss: 0.00110106
Iteration 4/25 | Loss: 0.00110106
Iteration 5/25 | Loss: 0.00110106
Iteration 6/25 | Loss: 0.00110106
Iteration 7/25 | Loss: 0.00110106
Iteration 8/25 | Loss: 0.00110106
Iteration 9/25 | Loss: 0.00110106
Iteration 10/25 | Loss: 0.00110106
Iteration 11/25 | Loss: 0.00110106
Iteration 12/25 | Loss: 0.00110106
Iteration 13/25 | Loss: 0.00110106
Iteration 14/25 | Loss: 0.00110106
Iteration 15/25 | Loss: 0.00110106
Iteration 16/25 | Loss: 0.00110106
Iteration 17/25 | Loss: 0.00110106
Iteration 18/25 | Loss: 0.00110106
Iteration 19/25 | Loss: 0.00110106
Iteration 20/25 | Loss: 0.00110106
Iteration 21/25 | Loss: 0.00110106
Iteration 22/25 | Loss: 0.00110106
Iteration 23/25 | Loss: 0.00110106
Iteration 24/25 | Loss: 0.00110106
Iteration 25/25 | Loss: 0.00110106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110106
Iteration 2/1000 | Loss: 0.00005776
Iteration 3/1000 | Loss: 0.00004240
Iteration 4/1000 | Loss: 0.00003335
Iteration 5/1000 | Loss: 0.00003156
Iteration 6/1000 | Loss: 0.00003071
Iteration 7/1000 | Loss: 0.00002962
Iteration 8/1000 | Loss: 0.00002904
Iteration 9/1000 | Loss: 0.00002844
Iteration 10/1000 | Loss: 0.00002796
Iteration 11/1000 | Loss: 0.00002766
Iteration 12/1000 | Loss: 0.00002739
Iteration 13/1000 | Loss: 0.00002715
Iteration 14/1000 | Loss: 0.00002709
Iteration 15/1000 | Loss: 0.00002700
Iteration 16/1000 | Loss: 0.00002690
Iteration 17/1000 | Loss: 0.00002689
Iteration 18/1000 | Loss: 0.00002687
Iteration 19/1000 | Loss: 0.00002687
Iteration 20/1000 | Loss: 0.00002683
Iteration 21/1000 | Loss: 0.00002682
Iteration 22/1000 | Loss: 0.00002670
Iteration 23/1000 | Loss: 0.00002667
Iteration 24/1000 | Loss: 0.00002666
Iteration 25/1000 | Loss: 0.00002666
Iteration 26/1000 | Loss: 0.00002666
Iteration 27/1000 | Loss: 0.00002665
Iteration 28/1000 | Loss: 0.00002665
Iteration 29/1000 | Loss: 0.00002665
Iteration 30/1000 | Loss: 0.00002664
Iteration 31/1000 | Loss: 0.00002662
Iteration 32/1000 | Loss: 0.00002662
Iteration 33/1000 | Loss: 0.00002662
Iteration 34/1000 | Loss: 0.00002662
Iteration 35/1000 | Loss: 0.00002662
Iteration 36/1000 | Loss: 0.00002662
Iteration 37/1000 | Loss: 0.00002662
Iteration 38/1000 | Loss: 0.00002661
Iteration 39/1000 | Loss: 0.00002661
Iteration 40/1000 | Loss: 0.00002661
Iteration 41/1000 | Loss: 0.00002660
Iteration 42/1000 | Loss: 0.00002660
Iteration 43/1000 | Loss: 0.00002660
Iteration 44/1000 | Loss: 0.00002660
Iteration 45/1000 | Loss: 0.00002659
Iteration 46/1000 | Loss: 0.00002659
Iteration 47/1000 | Loss: 0.00002659
Iteration 48/1000 | Loss: 0.00002659
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002658
Iteration 51/1000 | Loss: 0.00002658
Iteration 52/1000 | Loss: 0.00002657
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002656
Iteration 55/1000 | Loss: 0.00002656
Iteration 56/1000 | Loss: 0.00002656
Iteration 57/1000 | Loss: 0.00002655
Iteration 58/1000 | Loss: 0.00002655
Iteration 59/1000 | Loss: 0.00002655
Iteration 60/1000 | Loss: 0.00002655
Iteration 61/1000 | Loss: 0.00002654
Iteration 62/1000 | Loss: 0.00002654
Iteration 63/1000 | Loss: 0.00002653
Iteration 64/1000 | Loss: 0.00002653
Iteration 65/1000 | Loss: 0.00002653
Iteration 66/1000 | Loss: 0.00002653
Iteration 67/1000 | Loss: 0.00002652
Iteration 68/1000 | Loss: 0.00002652
Iteration 69/1000 | Loss: 0.00002652
Iteration 70/1000 | Loss: 0.00002652
Iteration 71/1000 | Loss: 0.00002651
Iteration 72/1000 | Loss: 0.00002651
Iteration 73/1000 | Loss: 0.00002651
Iteration 74/1000 | Loss: 0.00002651
Iteration 75/1000 | Loss: 0.00002651
Iteration 76/1000 | Loss: 0.00002651
Iteration 77/1000 | Loss: 0.00002651
Iteration 78/1000 | Loss: 0.00002650
Iteration 79/1000 | Loss: 0.00002650
Iteration 80/1000 | Loss: 0.00002650
Iteration 81/1000 | Loss: 0.00002650
Iteration 82/1000 | Loss: 0.00002650
Iteration 83/1000 | Loss: 0.00002650
Iteration 84/1000 | Loss: 0.00002650
Iteration 85/1000 | Loss: 0.00002650
Iteration 86/1000 | Loss: 0.00002649
Iteration 87/1000 | Loss: 0.00002649
Iteration 88/1000 | Loss: 0.00002649
Iteration 89/1000 | Loss: 0.00002649
Iteration 90/1000 | Loss: 0.00002648
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002648
Iteration 93/1000 | Loss: 0.00002648
Iteration 94/1000 | Loss: 0.00002647
Iteration 95/1000 | Loss: 0.00002647
Iteration 96/1000 | Loss: 0.00002647
Iteration 97/1000 | Loss: 0.00002647
Iteration 98/1000 | Loss: 0.00002647
Iteration 99/1000 | Loss: 0.00002647
Iteration 100/1000 | Loss: 0.00002647
Iteration 101/1000 | Loss: 0.00002646
Iteration 102/1000 | Loss: 0.00002646
Iteration 103/1000 | Loss: 0.00002646
Iteration 104/1000 | Loss: 0.00002646
Iteration 105/1000 | Loss: 0.00002646
Iteration 106/1000 | Loss: 0.00002645
Iteration 107/1000 | Loss: 0.00002645
Iteration 108/1000 | Loss: 0.00002645
Iteration 109/1000 | Loss: 0.00002645
Iteration 110/1000 | Loss: 0.00002645
Iteration 111/1000 | Loss: 0.00002645
Iteration 112/1000 | Loss: 0.00002645
Iteration 113/1000 | Loss: 0.00002645
Iteration 114/1000 | Loss: 0.00002645
Iteration 115/1000 | Loss: 0.00002645
Iteration 116/1000 | Loss: 0.00002645
Iteration 117/1000 | Loss: 0.00002645
Iteration 118/1000 | Loss: 0.00002644
Iteration 119/1000 | Loss: 0.00002644
Iteration 120/1000 | Loss: 0.00002644
Iteration 121/1000 | Loss: 0.00002644
Iteration 122/1000 | Loss: 0.00002644
Iteration 123/1000 | Loss: 0.00002644
Iteration 124/1000 | Loss: 0.00002644
Iteration 125/1000 | Loss: 0.00002644
Iteration 126/1000 | Loss: 0.00002643
Iteration 127/1000 | Loss: 0.00002643
Iteration 128/1000 | Loss: 0.00002643
Iteration 129/1000 | Loss: 0.00002643
Iteration 130/1000 | Loss: 0.00002643
Iteration 131/1000 | Loss: 0.00002643
Iteration 132/1000 | Loss: 0.00002642
Iteration 133/1000 | Loss: 0.00002642
Iteration 134/1000 | Loss: 0.00002642
Iteration 135/1000 | Loss: 0.00002642
Iteration 136/1000 | Loss: 0.00002642
Iteration 137/1000 | Loss: 0.00002642
Iteration 138/1000 | Loss: 0.00002642
Iteration 139/1000 | Loss: 0.00002642
Iteration 140/1000 | Loss: 0.00002642
Iteration 141/1000 | Loss: 0.00002642
Iteration 142/1000 | Loss: 0.00002642
Iteration 143/1000 | Loss: 0.00002641
Iteration 144/1000 | Loss: 0.00002641
Iteration 145/1000 | Loss: 0.00002641
Iteration 146/1000 | Loss: 0.00002641
Iteration 147/1000 | Loss: 0.00002641
Iteration 148/1000 | Loss: 0.00002641
Iteration 149/1000 | Loss: 0.00002641
Iteration 150/1000 | Loss: 0.00002641
Iteration 151/1000 | Loss: 0.00002641
Iteration 152/1000 | Loss: 0.00002641
Iteration 153/1000 | Loss: 0.00002641
Iteration 154/1000 | Loss: 0.00002641
Iteration 155/1000 | Loss: 0.00002641
Iteration 156/1000 | Loss: 0.00002641
Iteration 157/1000 | Loss: 0.00002641
Iteration 158/1000 | Loss: 0.00002641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.6408746634842828e-05, 2.6408746634842828e-05, 2.6408746634842828e-05, 2.6408746634842828e-05, 2.6408746634842828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6408746634842828e-05

Optimization complete. Final v2v error: 4.2876386642456055 mm

Highest mean error: 4.8036274909973145 mm for frame 78

Lowest mean error: 3.4097819328308105 mm for frame 0

Saving results

Total time: 41.25566744804382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487440
Iteration 2/25 | Loss: 0.00136502
Iteration 3/25 | Loss: 0.00130120
Iteration 4/25 | Loss: 0.00129346
Iteration 5/25 | Loss: 0.00129125
Iteration 6/25 | Loss: 0.00129125
Iteration 7/25 | Loss: 0.00129125
Iteration 8/25 | Loss: 0.00129125
Iteration 9/25 | Loss: 0.00129125
Iteration 10/25 | Loss: 0.00129125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012912536039948463, 0.0012912536039948463, 0.0012912536039948463, 0.0012912536039948463, 0.0012912536039948463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012912536039948463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.77347565
Iteration 2/25 | Loss: 0.00084021
Iteration 3/25 | Loss: 0.00084020
Iteration 4/25 | Loss: 0.00084019
Iteration 5/25 | Loss: 0.00084019
Iteration 6/25 | Loss: 0.00084019
Iteration 7/25 | Loss: 0.00084019
Iteration 8/25 | Loss: 0.00084019
Iteration 9/25 | Loss: 0.00084019
Iteration 10/25 | Loss: 0.00084019
Iteration 11/25 | Loss: 0.00084019
Iteration 12/25 | Loss: 0.00084019
Iteration 13/25 | Loss: 0.00084019
Iteration 14/25 | Loss: 0.00084019
Iteration 15/25 | Loss: 0.00084019
Iteration 16/25 | Loss: 0.00084019
Iteration 17/25 | Loss: 0.00084019
Iteration 18/25 | Loss: 0.00084019
Iteration 19/25 | Loss: 0.00084019
Iteration 20/25 | Loss: 0.00084019
Iteration 21/25 | Loss: 0.00084019
Iteration 22/25 | Loss: 0.00084019
Iteration 23/25 | Loss: 0.00084019
Iteration 24/25 | Loss: 0.00084019
Iteration 25/25 | Loss: 0.00084019

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084019
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00002166
Iteration 4/1000 | Loss: 0.00001938
Iteration 5/1000 | Loss: 0.00001836
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001708
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001604
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001570
Iteration 15/1000 | Loss: 0.00001570
Iteration 16/1000 | Loss: 0.00001553
Iteration 17/1000 | Loss: 0.00001552
Iteration 18/1000 | Loss: 0.00001549
Iteration 19/1000 | Loss: 0.00001548
Iteration 20/1000 | Loss: 0.00001547
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001537
Iteration 24/1000 | Loss: 0.00001536
Iteration 25/1000 | Loss: 0.00001534
Iteration 26/1000 | Loss: 0.00001533
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001530
Iteration 29/1000 | Loss: 0.00001529
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001529
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001525
Iteration 41/1000 | Loss: 0.00001525
Iteration 42/1000 | Loss: 0.00001524
Iteration 43/1000 | Loss: 0.00001524
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001517
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001509
Iteration 67/1000 | Loss: 0.00001509
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001508
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001505
Iteration 75/1000 | Loss: 0.00001504
Iteration 76/1000 | Loss: 0.00001504
Iteration 77/1000 | Loss: 0.00001504
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001504
Iteration 80/1000 | Loss: 0.00001503
Iteration 81/1000 | Loss: 0.00001503
Iteration 82/1000 | Loss: 0.00001503
Iteration 83/1000 | Loss: 0.00001503
Iteration 84/1000 | Loss: 0.00001503
Iteration 85/1000 | Loss: 0.00001503
Iteration 86/1000 | Loss: 0.00001503
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001502
Iteration 89/1000 | Loss: 0.00001502
Iteration 90/1000 | Loss: 0.00001501
Iteration 91/1000 | Loss: 0.00001501
Iteration 92/1000 | Loss: 0.00001501
Iteration 93/1000 | Loss: 0.00001501
Iteration 94/1000 | Loss: 0.00001500
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001500
Iteration 97/1000 | Loss: 0.00001500
Iteration 98/1000 | Loss: 0.00001500
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001499
Iteration 101/1000 | Loss: 0.00001499
Iteration 102/1000 | Loss: 0.00001499
Iteration 103/1000 | Loss: 0.00001499
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001498
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001496
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001495
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001494
Iteration 121/1000 | Loss: 0.00001494
Iteration 122/1000 | Loss: 0.00001494
Iteration 123/1000 | Loss: 0.00001494
Iteration 124/1000 | Loss: 0.00001494
Iteration 125/1000 | Loss: 0.00001494
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001493
Iteration 128/1000 | Loss: 0.00001493
Iteration 129/1000 | Loss: 0.00001493
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001493
Iteration 133/1000 | Loss: 0.00001493
Iteration 134/1000 | Loss: 0.00001493
Iteration 135/1000 | Loss: 0.00001493
Iteration 136/1000 | Loss: 0.00001493
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001493
Iteration 139/1000 | Loss: 0.00001493
Iteration 140/1000 | Loss: 0.00001493
Iteration 141/1000 | Loss: 0.00001493
Iteration 142/1000 | Loss: 0.00001493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.4927857591828797e-05, 1.4927857591828797e-05, 1.4927857591828797e-05, 1.4927857591828797e-05, 1.4927857591828797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4927857591828797e-05

Optimization complete. Final v2v error: 3.261471748352051 mm

Highest mean error: 3.6955339908599854 mm for frame 192

Lowest mean error: 2.9560234546661377 mm for frame 214

Saving results

Total time: 42.88155817985535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986473
Iteration 2/25 | Loss: 0.00170323
Iteration 3/25 | Loss: 0.00153307
Iteration 4/25 | Loss: 0.00147877
Iteration 5/25 | Loss: 0.00140622
Iteration 6/25 | Loss: 0.00139977
Iteration 7/25 | Loss: 0.00140057
Iteration 8/25 | Loss: 0.00134818
Iteration 9/25 | Loss: 0.00133816
Iteration 10/25 | Loss: 0.00133404
Iteration 11/25 | Loss: 0.00132970
Iteration 12/25 | Loss: 0.00132910
Iteration 13/25 | Loss: 0.00132863
Iteration 14/25 | Loss: 0.00133218
Iteration 15/25 | Loss: 0.00133245
Iteration 16/25 | Loss: 0.00132586
Iteration 17/25 | Loss: 0.00132530
Iteration 18/25 | Loss: 0.00132520
Iteration 19/25 | Loss: 0.00132520
Iteration 20/25 | Loss: 0.00132520
Iteration 21/25 | Loss: 0.00132520
Iteration 22/25 | Loss: 0.00132519
Iteration 23/25 | Loss: 0.00132519
Iteration 24/25 | Loss: 0.00132519
Iteration 25/25 | Loss: 0.00132519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.18898439
Iteration 2/25 | Loss: 0.00105921
Iteration 3/25 | Loss: 0.00105921
Iteration 4/25 | Loss: 0.00105921
Iteration 5/25 | Loss: 0.00105921
Iteration 6/25 | Loss: 0.00105921
Iteration 7/25 | Loss: 0.00105921
Iteration 8/25 | Loss: 0.00105921
Iteration 9/25 | Loss: 0.00105921
Iteration 10/25 | Loss: 0.00105921
Iteration 11/25 | Loss: 0.00105921
Iteration 12/25 | Loss: 0.00105921
Iteration 13/25 | Loss: 0.00105921
Iteration 14/25 | Loss: 0.00105921
Iteration 15/25 | Loss: 0.00105921
Iteration 16/25 | Loss: 0.00105921
Iteration 17/25 | Loss: 0.00105921
Iteration 18/25 | Loss: 0.00105921
Iteration 19/25 | Loss: 0.00105921
Iteration 20/25 | Loss: 0.00105921
Iteration 21/25 | Loss: 0.00105921
Iteration 22/25 | Loss: 0.00105921
Iteration 23/25 | Loss: 0.00105921
Iteration 24/25 | Loss: 0.00105921
Iteration 25/25 | Loss: 0.00105921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105921
Iteration 2/1000 | Loss: 0.00005068
Iteration 3/1000 | Loss: 0.00003670
Iteration 4/1000 | Loss: 0.00003146
Iteration 5/1000 | Loss: 0.00002842
Iteration 6/1000 | Loss: 0.00002659
Iteration 7/1000 | Loss: 0.00002574
Iteration 8/1000 | Loss: 0.00002506
Iteration 9/1000 | Loss: 0.00002467
Iteration 10/1000 | Loss: 0.00002417
Iteration 11/1000 | Loss: 0.00002380
Iteration 12/1000 | Loss: 0.00107320
Iteration 13/1000 | Loss: 0.00015148
Iteration 14/1000 | Loss: 0.00002530
Iteration 15/1000 | Loss: 0.00002297
Iteration 16/1000 | Loss: 0.00016431
Iteration 17/1000 | Loss: 0.00065892
Iteration 18/1000 | Loss: 0.00003100
Iteration 19/1000 | Loss: 0.00002349
Iteration 20/1000 | Loss: 0.00002107
Iteration 21/1000 | Loss: 0.00001958
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00009693
Iteration 24/1000 | Loss: 0.00018861
Iteration 25/1000 | Loss: 0.00009220
Iteration 26/1000 | Loss: 0.00001865
Iteration 27/1000 | Loss: 0.00001821
Iteration 28/1000 | Loss: 0.00001818
Iteration 29/1000 | Loss: 0.00001799
Iteration 30/1000 | Loss: 0.00001784
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001773
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001752
Iteration 41/1000 | Loss: 0.00001752
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001749
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001747
Iteration 47/1000 | Loss: 0.00001747
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001747
Iteration 51/1000 | Loss: 0.00001747
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00001747
Iteration 54/1000 | Loss: 0.00001747
Iteration 55/1000 | Loss: 0.00001747
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001746
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001746
Iteration 60/1000 | Loss: 0.00001746
Iteration 61/1000 | Loss: 0.00001746
Iteration 62/1000 | Loss: 0.00001746
Iteration 63/1000 | Loss: 0.00001746
Iteration 64/1000 | Loss: 0.00001746
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.7463467884226702e-05, 1.7463467884226702e-05, 1.7463467884226702e-05, 1.7463467884226702e-05, 1.7463467884226702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7463467884226702e-05

Optimization complete. Final v2v error: 3.542524814605713 mm

Highest mean error: 5.019451141357422 mm for frame 156

Lowest mean error: 3.0136992931365967 mm for frame 81

Saving results

Total time: 82.9541826248169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765949
Iteration 2/25 | Loss: 0.00142497
Iteration 3/25 | Loss: 0.00131277
Iteration 4/25 | Loss: 0.00129191
Iteration 5/25 | Loss: 0.00128549
Iteration 6/25 | Loss: 0.00128420
Iteration 7/25 | Loss: 0.00128407
Iteration 8/25 | Loss: 0.00128407
Iteration 9/25 | Loss: 0.00128407
Iteration 10/25 | Loss: 0.00128407
Iteration 11/25 | Loss: 0.00128407
Iteration 12/25 | Loss: 0.00128407
Iteration 13/25 | Loss: 0.00128407
Iteration 14/25 | Loss: 0.00128407
Iteration 15/25 | Loss: 0.00128407
Iteration 16/25 | Loss: 0.00128407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012840748531743884, 0.0012840748531743884, 0.0012840748531743884, 0.0012840748531743884, 0.0012840748531743884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012840748531743884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34550250
Iteration 2/25 | Loss: 0.00100958
Iteration 3/25 | Loss: 0.00100958
Iteration 4/25 | Loss: 0.00100957
Iteration 5/25 | Loss: 0.00100957
Iteration 6/25 | Loss: 0.00100957
Iteration 7/25 | Loss: 0.00100957
Iteration 8/25 | Loss: 0.00100957
Iteration 9/25 | Loss: 0.00100957
Iteration 10/25 | Loss: 0.00100957
Iteration 11/25 | Loss: 0.00100957
Iteration 12/25 | Loss: 0.00100957
Iteration 13/25 | Loss: 0.00100957
Iteration 14/25 | Loss: 0.00100957
Iteration 15/25 | Loss: 0.00100957
Iteration 16/25 | Loss: 0.00100957
Iteration 17/25 | Loss: 0.00100957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010095726465806365, 0.0010095726465806365, 0.0010095726465806365, 0.0010095726465806365, 0.0010095726465806365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010095726465806365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100957
Iteration 2/1000 | Loss: 0.00004556
Iteration 3/1000 | Loss: 0.00003071
Iteration 4/1000 | Loss: 0.00002360
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00001997
Iteration 7/1000 | Loss: 0.00001923
Iteration 8/1000 | Loss: 0.00001875
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001771
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001731
Iteration 14/1000 | Loss: 0.00001714
Iteration 15/1000 | Loss: 0.00001702
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001698
Iteration 19/1000 | Loss: 0.00001696
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00001683
Iteration 24/1000 | Loss: 0.00001680
Iteration 25/1000 | Loss: 0.00001677
Iteration 26/1000 | Loss: 0.00001677
Iteration 27/1000 | Loss: 0.00001676
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001675
Iteration 30/1000 | Loss: 0.00001675
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001673
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001672
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001672
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001670
Iteration 43/1000 | Loss: 0.00001669
Iteration 44/1000 | Loss: 0.00001669
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001668
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001668
Iteration 51/1000 | Loss: 0.00001668
Iteration 52/1000 | Loss: 0.00001668
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001666
Iteration 58/1000 | Loss: 0.00001666
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001665
Iteration 61/1000 | Loss: 0.00001665
Iteration 62/1000 | Loss: 0.00001665
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001664
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00001664
Iteration 67/1000 | Loss: 0.00001664
Iteration 68/1000 | Loss: 0.00001664
Iteration 69/1000 | Loss: 0.00001664
Iteration 70/1000 | Loss: 0.00001664
Iteration 71/1000 | Loss: 0.00001663
Iteration 72/1000 | Loss: 0.00001663
Iteration 73/1000 | Loss: 0.00001663
Iteration 74/1000 | Loss: 0.00001662
Iteration 75/1000 | Loss: 0.00001662
Iteration 76/1000 | Loss: 0.00001662
Iteration 77/1000 | Loss: 0.00001662
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001662
Iteration 80/1000 | Loss: 0.00001662
Iteration 81/1000 | Loss: 0.00001662
Iteration 82/1000 | Loss: 0.00001662
Iteration 83/1000 | Loss: 0.00001662
Iteration 84/1000 | Loss: 0.00001662
Iteration 85/1000 | Loss: 0.00001661
Iteration 86/1000 | Loss: 0.00001661
Iteration 87/1000 | Loss: 0.00001661
Iteration 88/1000 | Loss: 0.00001661
Iteration 89/1000 | Loss: 0.00001661
Iteration 90/1000 | Loss: 0.00001661
Iteration 91/1000 | Loss: 0.00001661
Iteration 92/1000 | Loss: 0.00001660
Iteration 93/1000 | Loss: 0.00001660
Iteration 94/1000 | Loss: 0.00001660
Iteration 95/1000 | Loss: 0.00001660
Iteration 96/1000 | Loss: 0.00001660
Iteration 97/1000 | Loss: 0.00001660
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001659
Iteration 103/1000 | Loss: 0.00001659
Iteration 104/1000 | Loss: 0.00001659
Iteration 105/1000 | Loss: 0.00001659
Iteration 106/1000 | Loss: 0.00001659
Iteration 107/1000 | Loss: 0.00001659
Iteration 108/1000 | Loss: 0.00001659
Iteration 109/1000 | Loss: 0.00001659
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001659
Iteration 113/1000 | Loss: 0.00001659
Iteration 114/1000 | Loss: 0.00001658
Iteration 115/1000 | Loss: 0.00001658
Iteration 116/1000 | Loss: 0.00001658
Iteration 117/1000 | Loss: 0.00001658
Iteration 118/1000 | Loss: 0.00001658
Iteration 119/1000 | Loss: 0.00001658
Iteration 120/1000 | Loss: 0.00001658
Iteration 121/1000 | Loss: 0.00001658
Iteration 122/1000 | Loss: 0.00001658
Iteration 123/1000 | Loss: 0.00001658
Iteration 124/1000 | Loss: 0.00001658
Iteration 125/1000 | Loss: 0.00001657
Iteration 126/1000 | Loss: 0.00001657
Iteration 127/1000 | Loss: 0.00001657
Iteration 128/1000 | Loss: 0.00001657
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001657
Iteration 131/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.6574780602240935e-05, 1.6574780602240935e-05, 1.6574780602240935e-05, 1.6574780602240935e-05, 1.6574780602240935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6574780602240935e-05

Optimization complete. Final v2v error: 3.448535919189453 mm

Highest mean error: 3.839369058609009 mm for frame 97

Lowest mean error: 3.071056604385376 mm for frame 20

Saving results

Total time: 39.059465408325195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00745617
Iteration 2/25 | Loss: 0.00163696
Iteration 3/25 | Loss: 0.00136484
Iteration 4/25 | Loss: 0.00134725
Iteration 5/25 | Loss: 0.00134514
Iteration 6/25 | Loss: 0.00134447
Iteration 7/25 | Loss: 0.00134444
Iteration 8/25 | Loss: 0.00134444
Iteration 9/25 | Loss: 0.00134444
Iteration 10/25 | Loss: 0.00134444
Iteration 11/25 | Loss: 0.00134444
Iteration 12/25 | Loss: 0.00134444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013444354990497231, 0.0013444354990497231, 0.0013444354990497231, 0.0013444354990497231, 0.0013444354990497231]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013444354990497231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38207150
Iteration 2/25 | Loss: 0.00084282
Iteration 3/25 | Loss: 0.00084280
Iteration 4/25 | Loss: 0.00084280
Iteration 5/25 | Loss: 0.00084280
Iteration 6/25 | Loss: 0.00084280
Iteration 7/25 | Loss: 0.00084280
Iteration 8/25 | Loss: 0.00084280
Iteration 9/25 | Loss: 0.00084280
Iteration 10/25 | Loss: 0.00084280
Iteration 11/25 | Loss: 0.00084280
Iteration 12/25 | Loss: 0.00084280
Iteration 13/25 | Loss: 0.00084280
Iteration 14/25 | Loss: 0.00084280
Iteration 15/25 | Loss: 0.00084280
Iteration 16/25 | Loss: 0.00084280
Iteration 17/25 | Loss: 0.00084280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008427986758761108, 0.0008427986758761108, 0.0008427986758761108, 0.0008427986758761108, 0.0008427986758761108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008427986758761108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084280
Iteration 2/1000 | Loss: 0.00003891
Iteration 3/1000 | Loss: 0.00002699
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00001917
Iteration 6/1000 | Loss: 0.00001833
Iteration 7/1000 | Loss: 0.00001794
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001713
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001673
Iteration 13/1000 | Loss: 0.00001667
Iteration 14/1000 | Loss: 0.00001665
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001657
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001650
Iteration 20/1000 | Loss: 0.00001648
Iteration 21/1000 | Loss: 0.00001646
Iteration 22/1000 | Loss: 0.00001645
Iteration 23/1000 | Loss: 0.00001645
Iteration 24/1000 | Loss: 0.00001644
Iteration 25/1000 | Loss: 0.00001643
Iteration 26/1000 | Loss: 0.00001643
Iteration 27/1000 | Loss: 0.00001643
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001639
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001633
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001619
Iteration 40/1000 | Loss: 0.00001615
Iteration 41/1000 | Loss: 0.00001615
Iteration 42/1000 | Loss: 0.00001615
Iteration 43/1000 | Loss: 0.00001615
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001612
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001610
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001607
Iteration 61/1000 | Loss: 0.00001607
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001607
Iteration 64/1000 | Loss: 0.00001607
Iteration 65/1000 | Loss: 0.00001607
Iteration 66/1000 | Loss: 0.00001607
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001606
Iteration 69/1000 | Loss: 0.00001606
Iteration 70/1000 | Loss: 0.00001606
Iteration 71/1000 | Loss: 0.00001606
Iteration 72/1000 | Loss: 0.00001606
Iteration 73/1000 | Loss: 0.00001606
Iteration 74/1000 | Loss: 0.00001605
Iteration 75/1000 | Loss: 0.00001605
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001604
Iteration 79/1000 | Loss: 0.00001604
Iteration 80/1000 | Loss: 0.00001604
Iteration 81/1000 | Loss: 0.00001604
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001604
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001603
Iteration 95/1000 | Loss: 0.00001603
Iteration 96/1000 | Loss: 0.00001603
Iteration 97/1000 | Loss: 0.00001602
Iteration 98/1000 | Loss: 0.00001602
Iteration 99/1000 | Loss: 0.00001602
Iteration 100/1000 | Loss: 0.00001602
Iteration 101/1000 | Loss: 0.00001602
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001602
Iteration 105/1000 | Loss: 0.00001602
Iteration 106/1000 | Loss: 0.00001602
Iteration 107/1000 | Loss: 0.00001602
Iteration 108/1000 | Loss: 0.00001602
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001602
Iteration 119/1000 | Loss: 0.00001602
Iteration 120/1000 | Loss: 0.00001602
Iteration 121/1000 | Loss: 0.00001602
Iteration 122/1000 | Loss: 0.00001602
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001602
Iteration 135/1000 | Loss: 0.00001602
Iteration 136/1000 | Loss: 0.00001602
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.601832809683401e-05, 1.601832809683401e-05, 1.601832809683401e-05, 1.601832809683401e-05, 1.601832809683401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.601832809683401e-05

Optimization complete. Final v2v error: 3.3845183849334717 mm

Highest mean error: 3.803180456161499 mm for frame 109

Lowest mean error: 3.13686203956604 mm for frame 17

Saving results

Total time: 36.2337908744812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452849
Iteration 2/25 | Loss: 0.00131433
Iteration 3/25 | Loss: 0.00125486
Iteration 4/25 | Loss: 0.00124496
Iteration 5/25 | Loss: 0.00124191
Iteration 6/25 | Loss: 0.00124129
Iteration 7/25 | Loss: 0.00124129
Iteration 8/25 | Loss: 0.00124129
Iteration 9/25 | Loss: 0.00124129
Iteration 10/25 | Loss: 0.00124129
Iteration 11/25 | Loss: 0.00124129
Iteration 12/25 | Loss: 0.00124129
Iteration 13/25 | Loss: 0.00124129
Iteration 14/25 | Loss: 0.00124129
Iteration 15/25 | Loss: 0.00124129
Iteration 16/25 | Loss: 0.00124129
Iteration 17/25 | Loss: 0.00124129
Iteration 18/25 | Loss: 0.00124129
Iteration 19/25 | Loss: 0.00124129
Iteration 20/25 | Loss: 0.00124129
Iteration 21/25 | Loss: 0.00124129
Iteration 22/25 | Loss: 0.00124129
Iteration 23/25 | Loss: 0.00124129
Iteration 24/25 | Loss: 0.00124129
Iteration 25/25 | Loss: 0.00124129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53982139
Iteration 2/25 | Loss: 0.00079721
Iteration 3/25 | Loss: 0.00079721
Iteration 4/25 | Loss: 0.00079721
Iteration 5/25 | Loss: 0.00079721
Iteration 6/25 | Loss: 0.00079721
Iteration 7/25 | Loss: 0.00079721
Iteration 8/25 | Loss: 0.00079721
Iteration 9/25 | Loss: 0.00079721
Iteration 10/25 | Loss: 0.00079721
Iteration 11/25 | Loss: 0.00079721
Iteration 12/25 | Loss: 0.00079721
Iteration 13/25 | Loss: 0.00079721
Iteration 14/25 | Loss: 0.00079721
Iteration 15/25 | Loss: 0.00079721
Iteration 16/25 | Loss: 0.00079721
Iteration 17/25 | Loss: 0.00079721
Iteration 18/25 | Loss: 0.00079721
Iteration 19/25 | Loss: 0.00079721
Iteration 20/25 | Loss: 0.00079721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007972065941430628, 0.0007972065941430628, 0.0007972065941430628, 0.0007972065941430628, 0.0007972065941430628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007972065941430628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079721
Iteration 2/1000 | Loss: 0.00002225
Iteration 3/1000 | Loss: 0.00001602
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001364
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001248
Iteration 12/1000 | Loss: 0.00001236
Iteration 13/1000 | Loss: 0.00001235
Iteration 14/1000 | Loss: 0.00001235
Iteration 15/1000 | Loss: 0.00001232
Iteration 16/1000 | Loss: 0.00001224
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001203
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001202
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001196
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001195
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001187
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001183
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001183
Iteration 48/1000 | Loss: 0.00001183
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001179
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001171
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001170
Iteration 80/1000 | Loss: 0.00001170
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001169
Iteration 86/1000 | Loss: 0.00001169
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001169
Iteration 100/1000 | Loss: 0.00001169
Iteration 101/1000 | Loss: 0.00001169
Iteration 102/1000 | Loss: 0.00001169
Iteration 103/1000 | Loss: 0.00001169
Iteration 104/1000 | Loss: 0.00001169
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1687595360854175e-05, 1.1687595360854175e-05, 1.1687595360854175e-05, 1.1687595360854175e-05, 1.1687595360854175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1687595360854175e-05

Optimization complete. Final v2v error: 2.9467198848724365 mm

Highest mean error: 3.089367151260376 mm for frame 39

Lowest mean error: 2.828063726425171 mm for frame 94

Saving results

Total time: 35.0979950428009
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003889
Iteration 2/25 | Loss: 0.00220956
Iteration 3/25 | Loss: 0.00171959
Iteration 4/25 | Loss: 0.00209443
Iteration 5/25 | Loss: 0.00158388
Iteration 6/25 | Loss: 0.00150263
Iteration 7/25 | Loss: 0.00141980
Iteration 8/25 | Loss: 0.00141064
Iteration 9/25 | Loss: 0.00137957
Iteration 10/25 | Loss: 0.00136184
Iteration 11/25 | Loss: 0.00135905
Iteration 12/25 | Loss: 0.00135843
Iteration 13/25 | Loss: 0.00135792
Iteration 14/25 | Loss: 0.00136493
Iteration 15/25 | Loss: 0.00135758
Iteration 16/25 | Loss: 0.00135223
Iteration 17/25 | Loss: 0.00135143
Iteration 18/25 | Loss: 0.00135124
Iteration 19/25 | Loss: 0.00135123
Iteration 20/25 | Loss: 0.00135123
Iteration 21/25 | Loss: 0.00135123
Iteration 22/25 | Loss: 0.00135123
Iteration 23/25 | Loss: 0.00135123
Iteration 24/25 | Loss: 0.00135123
Iteration 25/25 | Loss: 0.00135123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41169405
Iteration 2/25 | Loss: 0.00119301
Iteration 3/25 | Loss: 0.00119301
Iteration 4/25 | Loss: 0.00119301
Iteration 5/25 | Loss: 0.00119301
Iteration 6/25 | Loss: 0.00119301
Iteration 7/25 | Loss: 0.00119301
Iteration 8/25 | Loss: 0.00119301
Iteration 9/25 | Loss: 0.00119301
Iteration 10/25 | Loss: 0.00119301
Iteration 11/25 | Loss: 0.00119301
Iteration 12/25 | Loss: 0.00119301
Iteration 13/25 | Loss: 0.00119301
Iteration 14/25 | Loss: 0.00119301
Iteration 15/25 | Loss: 0.00119301
Iteration 16/25 | Loss: 0.00119301
Iteration 17/25 | Loss: 0.00119301
Iteration 18/25 | Loss: 0.00119301
Iteration 19/25 | Loss: 0.00119301
Iteration 20/25 | Loss: 0.00119301
Iteration 21/25 | Loss: 0.00119301
Iteration 22/25 | Loss: 0.00119301
Iteration 23/25 | Loss: 0.00119301
Iteration 24/25 | Loss: 0.00119301
Iteration 25/25 | Loss: 0.00119301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119301
Iteration 2/1000 | Loss: 0.00008571
Iteration 3/1000 | Loss: 0.00006028
Iteration 4/1000 | Loss: 0.00004957
Iteration 5/1000 | Loss: 0.00004538
Iteration 6/1000 | Loss: 0.00004236
Iteration 7/1000 | Loss: 0.00085737
Iteration 8/1000 | Loss: 0.00004097
Iteration 9/1000 | Loss: 0.00230282
Iteration 10/1000 | Loss: 0.00004251
Iteration 11/1000 | Loss: 0.00003807
Iteration 12/1000 | Loss: 0.00003688
Iteration 13/1000 | Loss: 0.00033025
Iteration 14/1000 | Loss: 0.00192464
Iteration 15/1000 | Loss: 0.00007048
Iteration 16/1000 | Loss: 0.00004620
Iteration 17/1000 | Loss: 0.00003530
Iteration 18/1000 | Loss: 0.00002850
Iteration 19/1000 | Loss: 0.00002381
Iteration 20/1000 | Loss: 0.00002083
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001806
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001683
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001643
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001632
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001625
Iteration 37/1000 | Loss: 0.00001625
Iteration 38/1000 | Loss: 0.00001624
Iteration 39/1000 | Loss: 0.00001623
Iteration 40/1000 | Loss: 0.00001623
Iteration 41/1000 | Loss: 0.00001623
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001622
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001620
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001613
Iteration 72/1000 | Loss: 0.00001613
Iteration 73/1000 | Loss: 0.00001613
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001612
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001611
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001610
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001610
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001610
Iteration 103/1000 | Loss: 0.00001610
Iteration 104/1000 | Loss: 0.00001610
Iteration 105/1000 | Loss: 0.00001610
Iteration 106/1000 | Loss: 0.00001610
Iteration 107/1000 | Loss: 0.00001610
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001609
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001609
Iteration 113/1000 | Loss: 0.00001609
Iteration 114/1000 | Loss: 0.00001609
Iteration 115/1000 | Loss: 0.00001609
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001608
Iteration 118/1000 | Loss: 0.00001608
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001608
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001608
Iteration 127/1000 | Loss: 0.00001607
Iteration 128/1000 | Loss: 0.00001607
Iteration 129/1000 | Loss: 0.00001607
Iteration 130/1000 | Loss: 0.00001607
Iteration 131/1000 | Loss: 0.00001607
Iteration 132/1000 | Loss: 0.00001607
Iteration 133/1000 | Loss: 0.00001607
Iteration 134/1000 | Loss: 0.00001607
Iteration 135/1000 | Loss: 0.00001607
Iteration 136/1000 | Loss: 0.00001607
Iteration 137/1000 | Loss: 0.00001607
Iteration 138/1000 | Loss: 0.00001607
Iteration 139/1000 | Loss: 0.00001607
Iteration 140/1000 | Loss: 0.00001607
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.6068859622464515e-05, 1.6068859622464515e-05, 1.6068859622464515e-05, 1.6068859622464515e-05, 1.6068859622464515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6068859622464515e-05

Optimization complete. Final v2v error: 3.365504503250122 mm

Highest mean error: 4.036721229553223 mm for frame 70

Lowest mean error: 3.096583366394043 mm for frame 107

Saving results

Total time: 78.39305281639099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964890
Iteration 2/25 | Loss: 0.00579977
Iteration 3/25 | Loss: 0.00320225
Iteration 4/25 | Loss: 0.00268055
Iteration 5/25 | Loss: 0.00248244
Iteration 6/25 | Loss: 0.00223707
Iteration 7/25 | Loss: 0.00206482
Iteration 8/25 | Loss: 0.00213051
Iteration 9/25 | Loss: 0.00204448
Iteration 10/25 | Loss: 0.00209331
Iteration 11/25 | Loss: 0.00180617
Iteration 12/25 | Loss: 0.00177029
Iteration 13/25 | Loss: 0.00173065
Iteration 14/25 | Loss: 0.00175072
Iteration 15/25 | Loss: 0.00171745
Iteration 16/25 | Loss: 0.00171435
Iteration 17/25 | Loss: 0.00170911
Iteration 18/25 | Loss: 0.00170458
Iteration 19/25 | Loss: 0.00169399
Iteration 20/25 | Loss: 0.00169999
Iteration 21/25 | Loss: 0.00169022
Iteration 22/25 | Loss: 0.00168905
Iteration 23/25 | Loss: 0.00168850
Iteration 24/25 | Loss: 0.00168780
Iteration 25/25 | Loss: 0.00169419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38159704
Iteration 2/25 | Loss: 0.00578238
Iteration 3/25 | Loss: 0.00232382
Iteration 4/25 | Loss: 0.00232382
Iteration 5/25 | Loss: 0.00232382
Iteration 6/25 | Loss: 0.00232382
Iteration 7/25 | Loss: 0.00232382
Iteration 8/25 | Loss: 0.00232382
Iteration 9/25 | Loss: 0.00232382
Iteration 10/25 | Loss: 0.00232382
Iteration 11/25 | Loss: 0.00232382
Iteration 12/25 | Loss: 0.00232382
Iteration 13/25 | Loss: 0.00232382
Iteration 14/25 | Loss: 0.00232382
Iteration 15/25 | Loss: 0.00232382
Iteration 16/25 | Loss: 0.00232382
Iteration 17/25 | Loss: 0.00232382
Iteration 18/25 | Loss: 0.00232382
Iteration 19/25 | Loss: 0.00232382
Iteration 20/25 | Loss: 0.00232382
Iteration 21/25 | Loss: 0.00232382
Iteration 22/25 | Loss: 0.00232382
Iteration 23/25 | Loss: 0.00232382
Iteration 24/25 | Loss: 0.00232382
Iteration 25/25 | Loss: 0.00232382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232382
Iteration 2/1000 | Loss: 0.00506649
Iteration 3/1000 | Loss: 0.00040893
Iteration 4/1000 | Loss: 0.00037979
Iteration 5/1000 | Loss: 0.00313023
Iteration 6/1000 | Loss: 0.00031788
Iteration 7/1000 | Loss: 0.00020556
Iteration 8/1000 | Loss: 0.00019600
Iteration 9/1000 | Loss: 0.00039940
Iteration 10/1000 | Loss: 0.00022170
Iteration 11/1000 | Loss: 0.00112435
Iteration 12/1000 | Loss: 0.00651255
Iteration 13/1000 | Loss: 0.00054963
Iteration 14/1000 | Loss: 0.00048980
Iteration 15/1000 | Loss: 0.00028767
Iteration 16/1000 | Loss: 0.00011763
Iteration 17/1000 | Loss: 0.00031112
Iteration 18/1000 | Loss: 0.00005874
Iteration 19/1000 | Loss: 0.00074206
Iteration 20/1000 | Loss: 0.00023209
Iteration 21/1000 | Loss: 0.00394228
Iteration 22/1000 | Loss: 0.00282597
Iteration 23/1000 | Loss: 0.00018596
Iteration 24/1000 | Loss: 0.00011472
Iteration 25/1000 | Loss: 0.00004710
Iteration 26/1000 | Loss: 0.00006525
Iteration 27/1000 | Loss: 0.00010412
Iteration 28/1000 | Loss: 0.00003004
Iteration 29/1000 | Loss: 0.00008178
Iteration 30/1000 | Loss: 0.00002428
Iteration 31/1000 | Loss: 0.00008743
Iteration 32/1000 | Loss: 0.00071313
Iteration 33/1000 | Loss: 0.00002148
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001894
Iteration 36/1000 | Loss: 0.00007704
Iteration 37/1000 | Loss: 0.00023925
Iteration 38/1000 | Loss: 0.00006427
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001793
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00009500
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001678
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001677
Iteration 51/1000 | Loss: 0.00001677
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001677
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001677
Iteration 58/1000 | Loss: 0.00001676
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00001675
Iteration 61/1000 | Loss: 0.00001675
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001674
Iteration 65/1000 | Loss: 0.00001674
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00012175
Iteration 70/1000 | Loss: 0.00018491
Iteration 71/1000 | Loss: 0.00059186
Iteration 72/1000 | Loss: 0.00014554
Iteration 73/1000 | Loss: 0.00014997
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00003280
Iteration 76/1000 | Loss: 0.00001670
Iteration 77/1000 | Loss: 0.00001668
Iteration 78/1000 | Loss: 0.00001664
Iteration 79/1000 | Loss: 0.00001663
Iteration 80/1000 | Loss: 0.00001661
Iteration 81/1000 | Loss: 0.00001661
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001654
Iteration 91/1000 | Loss: 0.00001653
Iteration 92/1000 | Loss: 0.00001653
Iteration 93/1000 | Loss: 0.00001653
Iteration 94/1000 | Loss: 0.00001653
Iteration 95/1000 | Loss: 0.00001653
Iteration 96/1000 | Loss: 0.00001653
Iteration 97/1000 | Loss: 0.00001652
Iteration 98/1000 | Loss: 0.00001652
Iteration 99/1000 | Loss: 0.00001652
Iteration 100/1000 | Loss: 0.00001651
Iteration 101/1000 | Loss: 0.00001651
Iteration 102/1000 | Loss: 0.00001651
Iteration 103/1000 | Loss: 0.00001651
Iteration 104/1000 | Loss: 0.00001651
Iteration 105/1000 | Loss: 0.00001651
Iteration 106/1000 | Loss: 0.00001651
Iteration 107/1000 | Loss: 0.00001651
Iteration 108/1000 | Loss: 0.00001650
Iteration 109/1000 | Loss: 0.00001650
Iteration 110/1000 | Loss: 0.00001650
Iteration 111/1000 | Loss: 0.00001650
Iteration 112/1000 | Loss: 0.00001650
Iteration 113/1000 | Loss: 0.00001650
Iteration 114/1000 | Loss: 0.00001650
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001649
Iteration 119/1000 | Loss: 0.00001649
Iteration 120/1000 | Loss: 0.00001649
Iteration 121/1000 | Loss: 0.00001649
Iteration 122/1000 | Loss: 0.00001649
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.648705074330792e-05, 1.648705074330792e-05, 1.648705074330792e-05, 1.648705074330792e-05, 1.648705074330792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.648705074330792e-05

Optimization complete. Final v2v error: 3.4279699325561523 mm

Highest mean error: 3.6498429775238037 mm for frame 92

Lowest mean error: 3.250805377960205 mm for frame 108

Saving results

Total time: 125.74263668060303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477007
Iteration 2/25 | Loss: 0.00138517
Iteration 3/25 | Loss: 0.00130763
Iteration 4/25 | Loss: 0.00129832
Iteration 5/25 | Loss: 0.00129688
Iteration 6/25 | Loss: 0.00129688
Iteration 7/25 | Loss: 0.00129688
Iteration 8/25 | Loss: 0.00129688
Iteration 9/25 | Loss: 0.00129688
Iteration 10/25 | Loss: 0.00129688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012968751834705472, 0.0012968751834705472, 0.0012968751834705472, 0.0012968751834705472, 0.0012968751834705472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012968751834705472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.88680792
Iteration 2/25 | Loss: 0.00088081
Iteration 3/25 | Loss: 0.00088081
Iteration 4/25 | Loss: 0.00088081
Iteration 5/25 | Loss: 0.00088081
Iteration 6/25 | Loss: 0.00088081
Iteration 7/25 | Loss: 0.00088081
Iteration 8/25 | Loss: 0.00088081
Iteration 9/25 | Loss: 0.00088081
Iteration 10/25 | Loss: 0.00088081
Iteration 11/25 | Loss: 0.00088080
Iteration 12/25 | Loss: 0.00088080
Iteration 13/25 | Loss: 0.00088080
Iteration 14/25 | Loss: 0.00088080
Iteration 15/25 | Loss: 0.00088080
Iteration 16/25 | Loss: 0.00088080
Iteration 17/25 | Loss: 0.00088080
Iteration 18/25 | Loss: 0.00088080
Iteration 19/25 | Loss: 0.00088080
Iteration 20/25 | Loss: 0.00088080
Iteration 21/25 | Loss: 0.00088080
Iteration 22/25 | Loss: 0.00088080
Iteration 23/25 | Loss: 0.00088080
Iteration 24/25 | Loss: 0.00088080
Iteration 25/25 | Loss: 0.00088080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088080
Iteration 2/1000 | Loss: 0.00002447
Iteration 3/1000 | Loss: 0.00001968
Iteration 4/1000 | Loss: 0.00001848
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001707
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001596
Iteration 10/1000 | Loss: 0.00001572
Iteration 11/1000 | Loss: 0.00001551
Iteration 12/1000 | Loss: 0.00001535
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001498
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001489
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001488
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001478
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001477
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001476
Iteration 35/1000 | Loss: 0.00001473
Iteration 36/1000 | Loss: 0.00001473
Iteration 37/1000 | Loss: 0.00001473
Iteration 38/1000 | Loss: 0.00001473
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001473
Iteration 42/1000 | Loss: 0.00001473
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001473
Iteration 45/1000 | Loss: 0.00001472
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001472
Iteration 49/1000 | Loss: 0.00001472
Iteration 50/1000 | Loss: 0.00001472
Iteration 51/1000 | Loss: 0.00001472
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001468
Iteration 55/1000 | Loss: 0.00001468
Iteration 56/1000 | Loss: 0.00001467
Iteration 57/1000 | Loss: 0.00001467
Iteration 58/1000 | Loss: 0.00001466
Iteration 59/1000 | Loss: 0.00001466
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001464
Iteration 71/1000 | Loss: 0.00001464
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001463
Iteration 80/1000 | Loss: 0.00001462
Iteration 81/1000 | Loss: 0.00001462
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001461
Iteration 84/1000 | Loss: 0.00001461
Iteration 85/1000 | Loss: 0.00001461
Iteration 86/1000 | Loss: 0.00001461
Iteration 87/1000 | Loss: 0.00001460
Iteration 88/1000 | Loss: 0.00001460
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001459
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001455
Iteration 136/1000 | Loss: 0.00001455
Iteration 137/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.4550570995197631e-05, 1.4550570995197631e-05, 1.4550570995197631e-05, 1.4550570995197631e-05, 1.4550570995197631e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4550570995197631e-05

Optimization complete. Final v2v error: 3.250336170196533 mm

Highest mean error: 3.4273934364318848 mm for frame 245

Lowest mean error: 3.0386409759521484 mm for frame 212

Saving results

Total time: 43.15728449821472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793068
Iteration 2/25 | Loss: 0.00142669
Iteration 3/25 | Loss: 0.00132856
Iteration 4/25 | Loss: 0.00131914
Iteration 5/25 | Loss: 0.00131632
Iteration 6/25 | Loss: 0.00131632
Iteration 7/25 | Loss: 0.00131632
Iteration 8/25 | Loss: 0.00131632
Iteration 9/25 | Loss: 0.00131632
Iteration 10/25 | Loss: 0.00131632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013163199182599783, 0.0013163199182599783, 0.0013163199182599783, 0.0013163199182599783, 0.0013163199182599783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013163199182599783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37627876
Iteration 2/25 | Loss: 0.00073294
Iteration 3/25 | Loss: 0.00073293
Iteration 4/25 | Loss: 0.00073293
Iteration 5/25 | Loss: 0.00073293
Iteration 6/25 | Loss: 0.00073293
Iteration 7/25 | Loss: 0.00073293
Iteration 8/25 | Loss: 0.00073293
Iteration 9/25 | Loss: 0.00073293
Iteration 10/25 | Loss: 0.00073293
Iteration 11/25 | Loss: 0.00073293
Iteration 12/25 | Loss: 0.00073293
Iteration 13/25 | Loss: 0.00073293
Iteration 14/25 | Loss: 0.00073293
Iteration 15/25 | Loss: 0.00073293
Iteration 16/25 | Loss: 0.00073293
Iteration 17/25 | Loss: 0.00073293
Iteration 18/25 | Loss: 0.00073293
Iteration 19/25 | Loss: 0.00073293
Iteration 20/25 | Loss: 0.00073293
Iteration 21/25 | Loss: 0.00073293
Iteration 22/25 | Loss: 0.00073293
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007329285726882517, 0.0007329285726882517, 0.0007329285726882517, 0.0007329285726882517, 0.0007329285726882517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007329285726882517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073293
Iteration 2/1000 | Loss: 0.00003011
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00002107
Iteration 5/1000 | Loss: 0.00002032
Iteration 6/1000 | Loss: 0.00001958
Iteration 7/1000 | Loss: 0.00001934
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001861
Iteration 10/1000 | Loss: 0.00001834
Iteration 11/1000 | Loss: 0.00001829
Iteration 12/1000 | Loss: 0.00001819
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001814
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001812
Iteration 18/1000 | Loss: 0.00001799
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001786
Iteration 21/1000 | Loss: 0.00001783
Iteration 22/1000 | Loss: 0.00001783
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001778
Iteration 25/1000 | Loss: 0.00001776
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001773
Iteration 28/1000 | Loss: 0.00001772
Iteration 29/1000 | Loss: 0.00001772
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001768
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001767
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001763
Iteration 38/1000 | Loss: 0.00001763
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001763
Iteration 41/1000 | Loss: 0.00001763
Iteration 42/1000 | Loss: 0.00001762
Iteration 43/1000 | Loss: 0.00001762
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001759
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001758
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001758
Iteration 52/1000 | Loss: 0.00001758
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001755
Iteration 60/1000 | Loss: 0.00001755
Iteration 61/1000 | Loss: 0.00001754
Iteration 62/1000 | Loss: 0.00001754
Iteration 63/1000 | Loss: 0.00001754
Iteration 64/1000 | Loss: 0.00001754
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001745
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001743
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001743
Iteration 75/1000 | Loss: 0.00001743
Iteration 76/1000 | Loss: 0.00001743
Iteration 77/1000 | Loss: 0.00001743
Iteration 78/1000 | Loss: 0.00001743
Iteration 79/1000 | Loss: 0.00001743
Iteration 80/1000 | Loss: 0.00001742
Iteration 81/1000 | Loss: 0.00001742
Iteration 82/1000 | Loss: 0.00001742
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001741
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001740
Iteration 89/1000 | Loss: 0.00001739
Iteration 90/1000 | Loss: 0.00001739
Iteration 91/1000 | Loss: 0.00001739
Iteration 92/1000 | Loss: 0.00001739
Iteration 93/1000 | Loss: 0.00001738
Iteration 94/1000 | Loss: 0.00001738
Iteration 95/1000 | Loss: 0.00001738
Iteration 96/1000 | Loss: 0.00001737
Iteration 97/1000 | Loss: 0.00001737
Iteration 98/1000 | Loss: 0.00001737
Iteration 99/1000 | Loss: 0.00001737
Iteration 100/1000 | Loss: 0.00001737
Iteration 101/1000 | Loss: 0.00001737
Iteration 102/1000 | Loss: 0.00001737
Iteration 103/1000 | Loss: 0.00001736
Iteration 104/1000 | Loss: 0.00001736
Iteration 105/1000 | Loss: 0.00001736
Iteration 106/1000 | Loss: 0.00001736
Iteration 107/1000 | Loss: 0.00001736
Iteration 108/1000 | Loss: 0.00001736
Iteration 109/1000 | Loss: 0.00001736
Iteration 110/1000 | Loss: 0.00001736
Iteration 111/1000 | Loss: 0.00001736
Iteration 112/1000 | Loss: 0.00001736
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001735
Iteration 115/1000 | Loss: 0.00001735
Iteration 116/1000 | Loss: 0.00001735
Iteration 117/1000 | Loss: 0.00001735
Iteration 118/1000 | Loss: 0.00001734
Iteration 119/1000 | Loss: 0.00001734
Iteration 120/1000 | Loss: 0.00001734
Iteration 121/1000 | Loss: 0.00001734
Iteration 122/1000 | Loss: 0.00001734
Iteration 123/1000 | Loss: 0.00001734
Iteration 124/1000 | Loss: 0.00001734
Iteration 125/1000 | Loss: 0.00001734
Iteration 126/1000 | Loss: 0.00001734
Iteration 127/1000 | Loss: 0.00001734
Iteration 128/1000 | Loss: 0.00001734
Iteration 129/1000 | Loss: 0.00001734
Iteration 130/1000 | Loss: 0.00001734
Iteration 131/1000 | Loss: 0.00001734
Iteration 132/1000 | Loss: 0.00001734
Iteration 133/1000 | Loss: 0.00001734
Iteration 134/1000 | Loss: 0.00001734
Iteration 135/1000 | Loss: 0.00001734
Iteration 136/1000 | Loss: 0.00001734
Iteration 137/1000 | Loss: 0.00001734
Iteration 138/1000 | Loss: 0.00001734
Iteration 139/1000 | Loss: 0.00001734
Iteration 140/1000 | Loss: 0.00001734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.733628050715197e-05, 1.733628050715197e-05, 1.733628050715197e-05, 1.733628050715197e-05, 1.733628050715197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.733628050715197e-05

Optimization complete. Final v2v error: 3.4675374031066895 mm

Highest mean error: 3.6357691287994385 mm for frame 140

Lowest mean error: 3.375053644180298 mm for frame 103

Saving results

Total time: 42.800262212753296
