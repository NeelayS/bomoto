Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=127, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7112-7167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778025
Iteration 2/25 | Loss: 0.00121188
Iteration 3/25 | Loss: 0.00104379
Iteration 4/25 | Loss: 0.00101508
Iteration 5/25 | Loss: 0.00100861
Iteration 6/25 | Loss: 0.00100685
Iteration 7/25 | Loss: 0.00100685
Iteration 8/25 | Loss: 0.00100685
Iteration 9/25 | Loss: 0.00100685
Iteration 10/25 | Loss: 0.00100685
Iteration 11/25 | Loss: 0.00100685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010068450355902314, 0.0010068450355902314, 0.0010068450355902314, 0.0010068450355902314, 0.0010068450355902314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010068450355902314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37818372
Iteration 2/25 | Loss: 0.00133539
Iteration 3/25 | Loss: 0.00133538
Iteration 4/25 | Loss: 0.00133538
Iteration 5/25 | Loss: 0.00133538
Iteration 6/25 | Loss: 0.00133538
Iteration 7/25 | Loss: 0.00133538
Iteration 8/25 | Loss: 0.00133538
Iteration 9/25 | Loss: 0.00133538
Iteration 10/25 | Loss: 0.00133538
Iteration 11/25 | Loss: 0.00133538
Iteration 12/25 | Loss: 0.00133538
Iteration 13/25 | Loss: 0.00133538
Iteration 14/25 | Loss: 0.00133538
Iteration 15/25 | Loss: 0.00133538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013353825779631734, 0.0013353825779631734, 0.0013353825779631734, 0.0013353825779631734, 0.0013353825779631734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013353825779631734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133538
Iteration 2/1000 | Loss: 0.00003155
Iteration 3/1000 | Loss: 0.00001961
Iteration 4/1000 | Loss: 0.00001520
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001256
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001177
Iteration 9/1000 | Loss: 0.00001149
Iteration 10/1000 | Loss: 0.00001130
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001113
Iteration 13/1000 | Loss: 0.00001109
Iteration 14/1000 | Loss: 0.00001106
Iteration 15/1000 | Loss: 0.00001102
Iteration 16/1000 | Loss: 0.00001096
Iteration 17/1000 | Loss: 0.00001094
Iteration 18/1000 | Loss: 0.00001093
Iteration 19/1000 | Loss: 0.00001093
Iteration 20/1000 | Loss: 0.00001092
Iteration 21/1000 | Loss: 0.00001092
Iteration 22/1000 | Loss: 0.00001091
Iteration 23/1000 | Loss: 0.00001091
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001090
Iteration 27/1000 | Loss: 0.00001090
Iteration 28/1000 | Loss: 0.00001090
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001089
Iteration 31/1000 | Loss: 0.00001089
Iteration 32/1000 | Loss: 0.00001088
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001086
Iteration 35/1000 | Loss: 0.00001085
Iteration 36/1000 | Loss: 0.00001085
Iteration 37/1000 | Loss: 0.00001085
Iteration 38/1000 | Loss: 0.00001084
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001083
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001080
Iteration 53/1000 | Loss: 0.00001080
Iteration 54/1000 | Loss: 0.00001079
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001078
Iteration 64/1000 | Loss: 0.00001078
Iteration 65/1000 | Loss: 0.00001078
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001077
Iteration 68/1000 | Loss: 0.00001077
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001076
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001074
Iteration 77/1000 | Loss: 0.00001074
Iteration 78/1000 | Loss: 0.00001073
Iteration 79/1000 | Loss: 0.00001073
Iteration 80/1000 | Loss: 0.00001073
Iteration 81/1000 | Loss: 0.00001073
Iteration 82/1000 | Loss: 0.00001072
Iteration 83/1000 | Loss: 0.00001072
Iteration 84/1000 | Loss: 0.00001072
Iteration 85/1000 | Loss: 0.00001072
Iteration 86/1000 | Loss: 0.00001072
Iteration 87/1000 | Loss: 0.00001072
Iteration 88/1000 | Loss: 0.00001072
Iteration 89/1000 | Loss: 0.00001072
Iteration 90/1000 | Loss: 0.00001072
Iteration 91/1000 | Loss: 0.00001072
Iteration 92/1000 | Loss: 0.00001072
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001071
Iteration 100/1000 | Loss: 0.00001071
Iteration 101/1000 | Loss: 0.00001071
Iteration 102/1000 | Loss: 0.00001070
Iteration 103/1000 | Loss: 0.00001070
Iteration 104/1000 | Loss: 0.00001070
Iteration 105/1000 | Loss: 0.00001070
Iteration 106/1000 | Loss: 0.00001070
Iteration 107/1000 | Loss: 0.00001070
Iteration 108/1000 | Loss: 0.00001070
Iteration 109/1000 | Loss: 0.00001070
Iteration 110/1000 | Loss: 0.00001070
Iteration 111/1000 | Loss: 0.00001070
Iteration 112/1000 | Loss: 0.00001070
Iteration 113/1000 | Loss: 0.00001070
Iteration 114/1000 | Loss: 0.00001069
Iteration 115/1000 | Loss: 0.00001069
Iteration 116/1000 | Loss: 0.00001069
Iteration 117/1000 | Loss: 0.00001069
Iteration 118/1000 | Loss: 0.00001069
Iteration 119/1000 | Loss: 0.00001069
Iteration 120/1000 | Loss: 0.00001069
Iteration 121/1000 | Loss: 0.00001069
Iteration 122/1000 | Loss: 0.00001068
Iteration 123/1000 | Loss: 0.00001068
Iteration 124/1000 | Loss: 0.00001068
Iteration 125/1000 | Loss: 0.00001068
Iteration 126/1000 | Loss: 0.00001068
Iteration 127/1000 | Loss: 0.00001068
Iteration 128/1000 | Loss: 0.00001068
Iteration 129/1000 | Loss: 0.00001068
Iteration 130/1000 | Loss: 0.00001067
Iteration 131/1000 | Loss: 0.00001067
Iteration 132/1000 | Loss: 0.00001067
Iteration 133/1000 | Loss: 0.00001067
Iteration 134/1000 | Loss: 0.00001067
Iteration 135/1000 | Loss: 0.00001067
Iteration 136/1000 | Loss: 0.00001067
Iteration 137/1000 | Loss: 0.00001067
Iteration 138/1000 | Loss: 0.00001067
Iteration 139/1000 | Loss: 0.00001067
Iteration 140/1000 | Loss: 0.00001066
Iteration 141/1000 | Loss: 0.00001066
Iteration 142/1000 | Loss: 0.00001066
Iteration 143/1000 | Loss: 0.00001066
Iteration 144/1000 | Loss: 0.00001066
Iteration 145/1000 | Loss: 0.00001066
Iteration 146/1000 | Loss: 0.00001066
Iteration 147/1000 | Loss: 0.00001066
Iteration 148/1000 | Loss: 0.00001066
Iteration 149/1000 | Loss: 0.00001066
Iteration 150/1000 | Loss: 0.00001066
Iteration 151/1000 | Loss: 0.00001066
Iteration 152/1000 | Loss: 0.00001066
Iteration 153/1000 | Loss: 0.00001066
Iteration 154/1000 | Loss: 0.00001066
Iteration 155/1000 | Loss: 0.00001066
Iteration 156/1000 | Loss: 0.00001066
Iteration 157/1000 | Loss: 0.00001065
Iteration 158/1000 | Loss: 0.00001065
Iteration 159/1000 | Loss: 0.00001065
Iteration 160/1000 | Loss: 0.00001065
Iteration 161/1000 | Loss: 0.00001065
Iteration 162/1000 | Loss: 0.00001065
Iteration 163/1000 | Loss: 0.00001065
Iteration 164/1000 | Loss: 0.00001065
Iteration 165/1000 | Loss: 0.00001065
Iteration 166/1000 | Loss: 0.00001065
Iteration 167/1000 | Loss: 0.00001065
Iteration 168/1000 | Loss: 0.00001065
Iteration 169/1000 | Loss: 0.00001065
Iteration 170/1000 | Loss: 0.00001065
Iteration 171/1000 | Loss: 0.00001065
Iteration 172/1000 | Loss: 0.00001065
Iteration 173/1000 | Loss: 0.00001065
Iteration 174/1000 | Loss: 0.00001064
Iteration 175/1000 | Loss: 0.00001064
Iteration 176/1000 | Loss: 0.00001064
Iteration 177/1000 | Loss: 0.00001064
Iteration 178/1000 | Loss: 0.00001064
Iteration 179/1000 | Loss: 0.00001064
Iteration 180/1000 | Loss: 0.00001064
Iteration 181/1000 | Loss: 0.00001064
Iteration 182/1000 | Loss: 0.00001064
Iteration 183/1000 | Loss: 0.00001064
Iteration 184/1000 | Loss: 0.00001064
Iteration 185/1000 | Loss: 0.00001064
Iteration 186/1000 | Loss: 0.00001064
Iteration 187/1000 | Loss: 0.00001064
Iteration 188/1000 | Loss: 0.00001063
Iteration 189/1000 | Loss: 0.00001063
Iteration 190/1000 | Loss: 0.00001063
Iteration 191/1000 | Loss: 0.00001063
Iteration 192/1000 | Loss: 0.00001063
Iteration 193/1000 | Loss: 0.00001063
Iteration 194/1000 | Loss: 0.00001063
Iteration 195/1000 | Loss: 0.00001063
Iteration 196/1000 | Loss: 0.00001063
Iteration 197/1000 | Loss: 0.00001063
Iteration 198/1000 | Loss: 0.00001062
Iteration 199/1000 | Loss: 0.00001062
Iteration 200/1000 | Loss: 0.00001062
Iteration 201/1000 | Loss: 0.00001062
Iteration 202/1000 | Loss: 0.00001062
Iteration 203/1000 | Loss: 0.00001062
Iteration 204/1000 | Loss: 0.00001062
Iteration 205/1000 | Loss: 0.00001062
Iteration 206/1000 | Loss: 0.00001062
Iteration 207/1000 | Loss: 0.00001061
Iteration 208/1000 | Loss: 0.00001061
Iteration 209/1000 | Loss: 0.00001061
Iteration 210/1000 | Loss: 0.00001061
Iteration 211/1000 | Loss: 0.00001061
Iteration 212/1000 | Loss: 0.00001061
Iteration 213/1000 | Loss: 0.00001061
Iteration 214/1000 | Loss: 0.00001061
Iteration 215/1000 | Loss: 0.00001061
Iteration 216/1000 | Loss: 0.00001061
Iteration 217/1000 | Loss: 0.00001061
Iteration 218/1000 | Loss: 0.00001061
Iteration 219/1000 | Loss: 0.00001061
Iteration 220/1000 | Loss: 0.00001061
Iteration 221/1000 | Loss: 0.00001061
Iteration 222/1000 | Loss: 0.00001060
Iteration 223/1000 | Loss: 0.00001060
Iteration 224/1000 | Loss: 0.00001060
Iteration 225/1000 | Loss: 0.00001060
Iteration 226/1000 | Loss: 0.00001060
Iteration 227/1000 | Loss: 0.00001060
Iteration 228/1000 | Loss: 0.00001060
Iteration 229/1000 | Loss: 0.00001060
Iteration 230/1000 | Loss: 0.00001060
Iteration 231/1000 | Loss: 0.00001060
Iteration 232/1000 | Loss: 0.00001060
Iteration 233/1000 | Loss: 0.00001059
Iteration 234/1000 | Loss: 0.00001059
Iteration 235/1000 | Loss: 0.00001059
Iteration 236/1000 | Loss: 0.00001059
Iteration 237/1000 | Loss: 0.00001059
Iteration 238/1000 | Loss: 0.00001059
Iteration 239/1000 | Loss: 0.00001059
Iteration 240/1000 | Loss: 0.00001059
Iteration 241/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.059421629179269e-05, 1.059421629179269e-05, 1.059421629179269e-05, 1.059421629179269e-05, 1.059421629179269e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.059421629179269e-05

Optimization complete. Final v2v error: 2.7542684078216553 mm

Highest mean error: 3.1005587577819824 mm for frame 207

Lowest mean error: 2.18338942527771 mm for frame 166

Saving results

Total time: 47.70790410041809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718089
Iteration 2/25 | Loss: 0.00236821
Iteration 3/25 | Loss: 0.00143929
Iteration 4/25 | Loss: 0.00117668
Iteration 5/25 | Loss: 0.00112887
Iteration 6/25 | Loss: 0.00111841
Iteration 7/25 | Loss: 0.00106315
Iteration 8/25 | Loss: 0.00104711
Iteration 9/25 | Loss: 0.00101821
Iteration 10/25 | Loss: 0.00100528
Iteration 11/25 | Loss: 0.00100063
Iteration 12/25 | Loss: 0.00100096
Iteration 13/25 | Loss: 0.00099564
Iteration 14/25 | Loss: 0.00099538
Iteration 15/25 | Loss: 0.00099523
Iteration 16/25 | Loss: 0.00099518
Iteration 17/25 | Loss: 0.00099518
Iteration 18/25 | Loss: 0.00099517
Iteration 19/25 | Loss: 0.00099517
Iteration 20/25 | Loss: 0.00099517
Iteration 21/25 | Loss: 0.00099517
Iteration 22/25 | Loss: 0.00099517
Iteration 23/25 | Loss: 0.00099517
Iteration 24/25 | Loss: 0.00099517
Iteration 25/25 | Loss: 0.00099517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65637040
Iteration 2/25 | Loss: 0.00111943
Iteration 3/25 | Loss: 0.00111943
Iteration 4/25 | Loss: 0.00111943
Iteration 5/25 | Loss: 0.00111943
Iteration 6/25 | Loss: 0.00106677
Iteration 7/25 | Loss: 0.00106677
Iteration 8/25 | Loss: 0.00106677
Iteration 9/25 | Loss: 0.00106676
Iteration 10/25 | Loss: 0.00106676
Iteration 11/25 | Loss: 0.00106676
Iteration 12/25 | Loss: 0.00106676
Iteration 13/25 | Loss: 0.00106676
Iteration 14/25 | Loss: 0.00106676
Iteration 15/25 | Loss: 0.00106676
Iteration 16/25 | Loss: 0.00106676
Iteration 17/25 | Loss: 0.00106676
Iteration 18/25 | Loss: 0.00106676
Iteration 19/25 | Loss: 0.00106676
Iteration 20/25 | Loss: 0.00106676
Iteration 21/25 | Loss: 0.00106676
Iteration 22/25 | Loss: 0.00106676
Iteration 23/25 | Loss: 0.00106676
Iteration 24/25 | Loss: 0.00106676
Iteration 25/25 | Loss: 0.00106676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106676
Iteration 2/1000 | Loss: 0.00008289
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00003709
Iteration 5/1000 | Loss: 0.00002117
Iteration 6/1000 | Loss: 0.00003567
Iteration 7/1000 | Loss: 0.00002459
Iteration 8/1000 | Loss: 0.00001517
Iteration 9/1000 | Loss: 0.00003100
Iteration 10/1000 | Loss: 0.00001465
Iteration 11/1000 | Loss: 0.00001440
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001410
Iteration 15/1000 | Loss: 0.00001409
Iteration 16/1000 | Loss: 0.00001408
Iteration 17/1000 | Loss: 0.00003336
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001424
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001385
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00001384
Iteration 29/1000 | Loss: 0.00087018
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00003679
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00005182
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00002025
Iteration 37/1000 | Loss: 0.00001107
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001090
Iteration 41/1000 | Loss: 0.00001090
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001083
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001076
Iteration 50/1000 | Loss: 0.00001076
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001075
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001060
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001057
Iteration 61/1000 | Loss: 0.00002340
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001801
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001042
Iteration 66/1000 | Loss: 0.00001038
Iteration 67/1000 | Loss: 0.00001038
Iteration 68/1000 | Loss: 0.00001038
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001037
Iteration 78/1000 | Loss: 0.00001036
Iteration 79/1000 | Loss: 0.00001036
Iteration 80/1000 | Loss: 0.00001036
Iteration 81/1000 | Loss: 0.00001036
Iteration 82/1000 | Loss: 0.00001036
Iteration 83/1000 | Loss: 0.00001035
Iteration 84/1000 | Loss: 0.00001035
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001035
Iteration 87/1000 | Loss: 0.00001035
Iteration 88/1000 | Loss: 0.00001035
Iteration 89/1000 | Loss: 0.00001035
Iteration 90/1000 | Loss: 0.00001035
Iteration 91/1000 | Loss: 0.00001035
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001035
Iteration 94/1000 | Loss: 0.00001035
Iteration 95/1000 | Loss: 0.00001035
Iteration 96/1000 | Loss: 0.00001035
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001035
Iteration 100/1000 | Loss: 0.00001035
Iteration 101/1000 | Loss: 0.00001035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.0352147000958212e-05, 1.0352147000958212e-05, 1.0352147000958212e-05, 1.0352147000958212e-05, 1.0352147000958212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0352147000958212e-05

Optimization complete. Final v2v error: 2.7731740474700928 mm

Highest mean error: 4.183809757232666 mm for frame 208

Lowest mean error: 2.4144227504730225 mm for frame 0

Saving results

Total time: 82.52107214927673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00350076
Iteration 2/25 | Loss: 0.00115218
Iteration 3/25 | Loss: 0.00102025
Iteration 4/25 | Loss: 0.00098792
Iteration 5/25 | Loss: 0.00097731
Iteration 6/25 | Loss: 0.00097433
Iteration 7/25 | Loss: 0.00097399
Iteration 8/25 | Loss: 0.00097399
Iteration 9/25 | Loss: 0.00097399
Iteration 10/25 | Loss: 0.00097399
Iteration 11/25 | Loss: 0.00097399
Iteration 12/25 | Loss: 0.00097399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009739899542182684, 0.0009739899542182684, 0.0009739899542182684, 0.0009739899542182684, 0.0009739899542182684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009739899542182684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31227434
Iteration 2/25 | Loss: 0.00145599
Iteration 3/25 | Loss: 0.00145599
Iteration 4/25 | Loss: 0.00145599
Iteration 5/25 | Loss: 0.00145599
Iteration 6/25 | Loss: 0.00145599
Iteration 7/25 | Loss: 0.00145599
Iteration 8/25 | Loss: 0.00145599
Iteration 9/25 | Loss: 0.00145599
Iteration 10/25 | Loss: 0.00145599
Iteration 11/25 | Loss: 0.00145599
Iteration 12/25 | Loss: 0.00145599
Iteration 13/25 | Loss: 0.00145599
Iteration 14/25 | Loss: 0.00145599
Iteration 15/25 | Loss: 0.00145599
Iteration 16/25 | Loss: 0.00145599
Iteration 17/25 | Loss: 0.00145599
Iteration 18/25 | Loss: 0.00145599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014559895498678088, 0.0014559895498678088, 0.0014559895498678088, 0.0014559895498678088, 0.0014559895498678088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014559895498678088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145599
Iteration 2/1000 | Loss: 0.00005036
Iteration 3/1000 | Loss: 0.00002983
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001820
Iteration 7/1000 | Loss: 0.00001743
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001680
Iteration 10/1000 | Loss: 0.00001672
Iteration 11/1000 | Loss: 0.00001655
Iteration 12/1000 | Loss: 0.00001652
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001636
Iteration 17/1000 | Loss: 0.00001633
Iteration 18/1000 | Loss: 0.00001633
Iteration 19/1000 | Loss: 0.00001632
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001628
Iteration 22/1000 | Loss: 0.00001628
Iteration 23/1000 | Loss: 0.00001627
Iteration 24/1000 | Loss: 0.00001626
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001624
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001620
Iteration 31/1000 | Loss: 0.00001619
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001609
Iteration 44/1000 | Loss: 0.00001609
Iteration 45/1000 | Loss: 0.00001609
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001608
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001605
Iteration 54/1000 | Loss: 0.00001605
Iteration 55/1000 | Loss: 0.00001605
Iteration 56/1000 | Loss: 0.00001605
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001604
Iteration 59/1000 | Loss: 0.00001604
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001603
Iteration 63/1000 | Loss: 0.00001602
Iteration 64/1000 | Loss: 0.00001602
Iteration 65/1000 | Loss: 0.00001602
Iteration 66/1000 | Loss: 0.00001602
Iteration 67/1000 | Loss: 0.00001602
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001601
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001600
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001598
Iteration 83/1000 | Loss: 0.00001598
Iteration 84/1000 | Loss: 0.00001598
Iteration 85/1000 | Loss: 0.00001598
Iteration 86/1000 | Loss: 0.00001598
Iteration 87/1000 | Loss: 0.00001598
Iteration 88/1000 | Loss: 0.00001597
Iteration 89/1000 | Loss: 0.00001597
Iteration 90/1000 | Loss: 0.00001597
Iteration 91/1000 | Loss: 0.00001597
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001596
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001593
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001592
Iteration 104/1000 | Loss: 0.00001592
Iteration 105/1000 | Loss: 0.00001592
Iteration 106/1000 | Loss: 0.00001591
Iteration 107/1000 | Loss: 0.00001591
Iteration 108/1000 | Loss: 0.00001591
Iteration 109/1000 | Loss: 0.00001590
Iteration 110/1000 | Loss: 0.00001590
Iteration 111/1000 | Loss: 0.00001590
Iteration 112/1000 | Loss: 0.00001590
Iteration 113/1000 | Loss: 0.00001590
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001590
Iteration 116/1000 | Loss: 0.00001590
Iteration 117/1000 | Loss: 0.00001590
Iteration 118/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.58957172970986e-05, 1.58957172970986e-05, 1.58957172970986e-05, 1.58957172970986e-05, 1.58957172970986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.58957172970986e-05

Optimization complete. Final v2v error: 3.186453342437744 mm

Highest mean error: 4.274162769317627 mm for frame 130

Lowest mean error: 2.2598793506622314 mm for frame 162

Saving results

Total time: 41.012768268585205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444444
Iteration 2/25 | Loss: 0.00130718
Iteration 3/25 | Loss: 0.00102964
Iteration 4/25 | Loss: 0.00098964
Iteration 5/25 | Loss: 0.00098420
Iteration 6/25 | Loss: 0.00098228
Iteration 7/25 | Loss: 0.00098223
Iteration 8/25 | Loss: 0.00098223
Iteration 9/25 | Loss: 0.00098223
Iteration 10/25 | Loss: 0.00098223
Iteration 11/25 | Loss: 0.00098223
Iteration 12/25 | Loss: 0.00098223
Iteration 13/25 | Loss: 0.00098223
Iteration 14/25 | Loss: 0.00098223
Iteration 15/25 | Loss: 0.00098223
Iteration 16/25 | Loss: 0.00098223
Iteration 17/25 | Loss: 0.00098223
Iteration 18/25 | Loss: 0.00098223
Iteration 19/25 | Loss: 0.00098223
Iteration 20/25 | Loss: 0.00098223
Iteration 21/25 | Loss: 0.00098223
Iteration 22/25 | Loss: 0.00098223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009822275023907423, 0.0009822275023907423, 0.0009822275023907423, 0.0009822275023907423, 0.0009822275023907423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009822275023907423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44046402
Iteration 2/25 | Loss: 0.00084631
Iteration 3/25 | Loss: 0.00084630
Iteration 4/25 | Loss: 0.00084630
Iteration 5/25 | Loss: 0.00084630
Iteration 6/25 | Loss: 0.00084630
Iteration 7/25 | Loss: 0.00084630
Iteration 8/25 | Loss: 0.00084630
Iteration 9/25 | Loss: 0.00084630
Iteration 10/25 | Loss: 0.00084630
Iteration 11/25 | Loss: 0.00084630
Iteration 12/25 | Loss: 0.00084630
Iteration 13/25 | Loss: 0.00084630
Iteration 14/25 | Loss: 0.00084630
Iteration 15/25 | Loss: 0.00084630
Iteration 16/25 | Loss: 0.00084630
Iteration 17/25 | Loss: 0.00084630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008463021367788315, 0.0008463021367788315, 0.0008463021367788315, 0.0008463021367788315, 0.0008463021367788315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008463021367788315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084630
Iteration 2/1000 | Loss: 0.00002681
Iteration 3/1000 | Loss: 0.00001745
Iteration 4/1000 | Loss: 0.00001367
Iteration 5/1000 | Loss: 0.00001252
Iteration 6/1000 | Loss: 0.00001200
Iteration 7/1000 | Loss: 0.00001169
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001112
Iteration 10/1000 | Loss: 0.00001095
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001087
Iteration 13/1000 | Loss: 0.00001085
Iteration 14/1000 | Loss: 0.00001084
Iteration 15/1000 | Loss: 0.00001084
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001075
Iteration 22/1000 | Loss: 0.00001075
Iteration 23/1000 | Loss: 0.00001075
Iteration 24/1000 | Loss: 0.00001075
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001073
Iteration 28/1000 | Loss: 0.00001072
Iteration 29/1000 | Loss: 0.00001072
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001072
Iteration 33/1000 | Loss: 0.00001072
Iteration 34/1000 | Loss: 0.00001072
Iteration 35/1000 | Loss: 0.00001072
Iteration 36/1000 | Loss: 0.00001072
Iteration 37/1000 | Loss: 0.00001072
Iteration 38/1000 | Loss: 0.00001071
Iteration 39/1000 | Loss: 0.00001071
Iteration 40/1000 | Loss: 0.00001071
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001071
Iteration 43/1000 | Loss: 0.00001071
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001071
Iteration 47/1000 | Loss: 0.00001071
Iteration 48/1000 | Loss: 0.00001071
Iteration 49/1000 | Loss: 0.00001071
Iteration 50/1000 | Loss: 0.00001070
Iteration 51/1000 | Loss: 0.00001070
Iteration 52/1000 | Loss: 0.00001069
Iteration 53/1000 | Loss: 0.00001069
Iteration 54/1000 | Loss: 0.00001068
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001067
Iteration 60/1000 | Loss: 0.00001067
Iteration 61/1000 | Loss: 0.00001066
Iteration 62/1000 | Loss: 0.00001066
Iteration 63/1000 | Loss: 0.00001066
Iteration 64/1000 | Loss: 0.00001066
Iteration 65/1000 | Loss: 0.00001066
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001065
Iteration 68/1000 | Loss: 0.00001065
Iteration 69/1000 | Loss: 0.00001065
Iteration 70/1000 | Loss: 0.00001065
Iteration 71/1000 | Loss: 0.00001064
Iteration 72/1000 | Loss: 0.00001064
Iteration 73/1000 | Loss: 0.00001064
Iteration 74/1000 | Loss: 0.00001064
Iteration 75/1000 | Loss: 0.00001064
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001063
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001063
Iteration 80/1000 | Loss: 0.00001063
Iteration 81/1000 | Loss: 0.00001063
Iteration 82/1000 | Loss: 0.00001063
Iteration 83/1000 | Loss: 0.00001063
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001063
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001063
Iteration 90/1000 | Loss: 0.00001063
Iteration 91/1000 | Loss: 0.00001063
Iteration 92/1000 | Loss: 0.00001062
Iteration 93/1000 | Loss: 0.00001062
Iteration 94/1000 | Loss: 0.00001062
Iteration 95/1000 | Loss: 0.00001062
Iteration 96/1000 | Loss: 0.00001062
Iteration 97/1000 | Loss: 0.00001062
Iteration 98/1000 | Loss: 0.00001061
Iteration 99/1000 | Loss: 0.00001061
Iteration 100/1000 | Loss: 0.00001061
Iteration 101/1000 | Loss: 0.00001061
Iteration 102/1000 | Loss: 0.00001061
Iteration 103/1000 | Loss: 0.00001061
Iteration 104/1000 | Loss: 0.00001061
Iteration 105/1000 | Loss: 0.00001061
Iteration 106/1000 | Loss: 0.00001061
Iteration 107/1000 | Loss: 0.00001061
Iteration 108/1000 | Loss: 0.00001061
Iteration 109/1000 | Loss: 0.00001061
Iteration 110/1000 | Loss: 0.00001061
Iteration 111/1000 | Loss: 0.00001060
Iteration 112/1000 | Loss: 0.00001060
Iteration 113/1000 | Loss: 0.00001060
Iteration 114/1000 | Loss: 0.00001060
Iteration 115/1000 | Loss: 0.00001060
Iteration 116/1000 | Loss: 0.00001060
Iteration 117/1000 | Loss: 0.00001060
Iteration 118/1000 | Loss: 0.00001060
Iteration 119/1000 | Loss: 0.00001060
Iteration 120/1000 | Loss: 0.00001060
Iteration 121/1000 | Loss: 0.00001060
Iteration 122/1000 | Loss: 0.00001060
Iteration 123/1000 | Loss: 0.00001059
Iteration 124/1000 | Loss: 0.00001059
Iteration 125/1000 | Loss: 0.00001059
Iteration 126/1000 | Loss: 0.00001059
Iteration 127/1000 | Loss: 0.00001059
Iteration 128/1000 | Loss: 0.00001059
Iteration 129/1000 | Loss: 0.00001059
Iteration 130/1000 | Loss: 0.00001059
Iteration 131/1000 | Loss: 0.00001059
Iteration 132/1000 | Loss: 0.00001059
Iteration 133/1000 | Loss: 0.00001059
Iteration 134/1000 | Loss: 0.00001059
Iteration 135/1000 | Loss: 0.00001059
Iteration 136/1000 | Loss: 0.00001059
Iteration 137/1000 | Loss: 0.00001059
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001059
Iteration 142/1000 | Loss: 0.00001059
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001059
Iteration 146/1000 | Loss: 0.00001059
Iteration 147/1000 | Loss: 0.00001059
Iteration 148/1000 | Loss: 0.00001059
Iteration 149/1000 | Loss: 0.00001059
Iteration 150/1000 | Loss: 0.00001059
Iteration 151/1000 | Loss: 0.00001059
Iteration 152/1000 | Loss: 0.00001059
Iteration 153/1000 | Loss: 0.00001059
Iteration 154/1000 | Loss: 0.00001059
Iteration 155/1000 | Loss: 0.00001059
Iteration 156/1000 | Loss: 0.00001059
Iteration 157/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.0586606549622957e-05, 1.0586606549622957e-05, 1.0586606549622957e-05, 1.0586606549622957e-05, 1.0586606549622957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0586606549622957e-05

Optimization complete. Final v2v error: 2.660593032836914 mm

Highest mean error: 4.3814520835876465 mm for frame 106

Lowest mean error: 2.130150079727173 mm for frame 221

Saving results

Total time: 37.55723285675049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391685
Iteration 2/25 | Loss: 0.00104828
Iteration 3/25 | Loss: 0.00098210
Iteration 4/25 | Loss: 0.00097556
Iteration 5/25 | Loss: 0.00097326
Iteration 6/25 | Loss: 0.00097301
Iteration 7/25 | Loss: 0.00097301
Iteration 8/25 | Loss: 0.00097301
Iteration 9/25 | Loss: 0.00097301
Iteration 10/25 | Loss: 0.00097301
Iteration 11/25 | Loss: 0.00097301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009730132878758013, 0.0009730132878758013, 0.0009730132878758013, 0.0009730132878758013, 0.0009730132878758013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009730132878758013

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91918337
Iteration 2/25 | Loss: 0.00092458
Iteration 3/25 | Loss: 0.00092458
Iteration 4/25 | Loss: 0.00092458
Iteration 5/25 | Loss: 0.00092458
Iteration 6/25 | Loss: 0.00092458
Iteration 7/25 | Loss: 0.00092457
Iteration 8/25 | Loss: 0.00092457
Iteration 9/25 | Loss: 0.00092457
Iteration 10/25 | Loss: 0.00092457
Iteration 11/25 | Loss: 0.00092457
Iteration 12/25 | Loss: 0.00092457
Iteration 13/25 | Loss: 0.00092457
Iteration 14/25 | Loss: 0.00092457
Iteration 15/25 | Loss: 0.00092457
Iteration 16/25 | Loss: 0.00092457
Iteration 17/25 | Loss: 0.00092457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000924574036616832, 0.000924574036616832, 0.000924574036616832, 0.000924574036616832, 0.000924574036616832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000924574036616832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092457
Iteration 2/1000 | Loss: 0.00001846
Iteration 3/1000 | Loss: 0.00001376
Iteration 4/1000 | Loss: 0.00001202
Iteration 5/1000 | Loss: 0.00001135
Iteration 6/1000 | Loss: 0.00001096
Iteration 7/1000 | Loss: 0.00001050
Iteration 8/1000 | Loss: 0.00001031
Iteration 9/1000 | Loss: 0.00001017
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00001013
Iteration 12/1000 | Loss: 0.00001012
Iteration 13/1000 | Loss: 0.00001011
Iteration 14/1000 | Loss: 0.00001011
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001010
Iteration 17/1000 | Loss: 0.00001010
Iteration 18/1000 | Loss: 0.00001009
Iteration 19/1000 | Loss: 0.00001008
Iteration 20/1000 | Loss: 0.00001008
Iteration 21/1000 | Loss: 0.00001007
Iteration 22/1000 | Loss: 0.00001007
Iteration 23/1000 | Loss: 0.00001007
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001006
Iteration 26/1000 | Loss: 0.00001006
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001005
Iteration 29/1000 | Loss: 0.00001005
Iteration 30/1000 | Loss: 0.00001004
Iteration 31/1000 | Loss: 0.00001003
Iteration 32/1000 | Loss: 0.00001003
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001001
Iteration 35/1000 | Loss: 0.00001001
Iteration 36/1000 | Loss: 0.00001001
Iteration 37/1000 | Loss: 0.00001001
Iteration 38/1000 | Loss: 0.00001001
Iteration 39/1000 | Loss: 0.00001001
Iteration 40/1000 | Loss: 0.00001001
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000996
Iteration 43/1000 | Loss: 0.00000996
Iteration 44/1000 | Loss: 0.00000996
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000996
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000995
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000994
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000993
Iteration 56/1000 | Loss: 0.00000993
Iteration 57/1000 | Loss: 0.00000993
Iteration 58/1000 | Loss: 0.00000993
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000993
Iteration 61/1000 | Loss: 0.00000993
Iteration 62/1000 | Loss: 0.00000993
Iteration 63/1000 | Loss: 0.00000992
Iteration 64/1000 | Loss: 0.00000992
Iteration 65/1000 | Loss: 0.00000992
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000990
Iteration 69/1000 | Loss: 0.00000990
Iteration 70/1000 | Loss: 0.00000990
Iteration 71/1000 | Loss: 0.00000990
Iteration 72/1000 | Loss: 0.00000989
Iteration 73/1000 | Loss: 0.00000989
Iteration 74/1000 | Loss: 0.00000989
Iteration 75/1000 | Loss: 0.00000989
Iteration 76/1000 | Loss: 0.00000989
Iteration 77/1000 | Loss: 0.00000989
Iteration 78/1000 | Loss: 0.00000989
Iteration 79/1000 | Loss: 0.00000988
Iteration 80/1000 | Loss: 0.00000988
Iteration 81/1000 | Loss: 0.00000988
Iteration 82/1000 | Loss: 0.00000988
Iteration 83/1000 | Loss: 0.00000987
Iteration 84/1000 | Loss: 0.00000987
Iteration 85/1000 | Loss: 0.00000987
Iteration 86/1000 | Loss: 0.00000987
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000986
Iteration 89/1000 | Loss: 0.00000986
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000986
Iteration 92/1000 | Loss: 0.00000986
Iteration 93/1000 | Loss: 0.00000986
Iteration 94/1000 | Loss: 0.00000986
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000985
Iteration 98/1000 | Loss: 0.00000984
Iteration 99/1000 | Loss: 0.00000984
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000983
Iteration 102/1000 | Loss: 0.00000983
Iteration 103/1000 | Loss: 0.00000983
Iteration 104/1000 | Loss: 0.00000983
Iteration 105/1000 | Loss: 0.00000983
Iteration 106/1000 | Loss: 0.00000983
Iteration 107/1000 | Loss: 0.00000983
Iteration 108/1000 | Loss: 0.00000982
Iteration 109/1000 | Loss: 0.00000982
Iteration 110/1000 | Loss: 0.00000982
Iteration 111/1000 | Loss: 0.00000981
Iteration 112/1000 | Loss: 0.00000981
Iteration 113/1000 | Loss: 0.00000981
Iteration 114/1000 | Loss: 0.00000981
Iteration 115/1000 | Loss: 0.00000981
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000980
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000980
Iteration 120/1000 | Loss: 0.00000979
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000978
Iteration 123/1000 | Loss: 0.00000978
Iteration 124/1000 | Loss: 0.00000978
Iteration 125/1000 | Loss: 0.00000978
Iteration 126/1000 | Loss: 0.00000978
Iteration 127/1000 | Loss: 0.00000978
Iteration 128/1000 | Loss: 0.00000978
Iteration 129/1000 | Loss: 0.00000977
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000977
Iteration 134/1000 | Loss: 0.00000977
Iteration 135/1000 | Loss: 0.00000977
Iteration 136/1000 | Loss: 0.00000977
Iteration 137/1000 | Loss: 0.00000977
Iteration 138/1000 | Loss: 0.00000977
Iteration 139/1000 | Loss: 0.00000976
Iteration 140/1000 | Loss: 0.00000976
Iteration 141/1000 | Loss: 0.00000976
Iteration 142/1000 | Loss: 0.00000976
Iteration 143/1000 | Loss: 0.00000976
Iteration 144/1000 | Loss: 0.00000976
Iteration 145/1000 | Loss: 0.00000976
Iteration 146/1000 | Loss: 0.00000975
Iteration 147/1000 | Loss: 0.00000975
Iteration 148/1000 | Loss: 0.00000975
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000975
Iteration 152/1000 | Loss: 0.00000975
Iteration 153/1000 | Loss: 0.00000975
Iteration 154/1000 | Loss: 0.00000975
Iteration 155/1000 | Loss: 0.00000974
Iteration 156/1000 | Loss: 0.00000974
Iteration 157/1000 | Loss: 0.00000974
Iteration 158/1000 | Loss: 0.00000974
Iteration 159/1000 | Loss: 0.00000974
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000973
Iteration 165/1000 | Loss: 0.00000973
Iteration 166/1000 | Loss: 0.00000973
Iteration 167/1000 | Loss: 0.00000973
Iteration 168/1000 | Loss: 0.00000973
Iteration 169/1000 | Loss: 0.00000973
Iteration 170/1000 | Loss: 0.00000973
Iteration 171/1000 | Loss: 0.00000973
Iteration 172/1000 | Loss: 0.00000973
Iteration 173/1000 | Loss: 0.00000972
Iteration 174/1000 | Loss: 0.00000972
Iteration 175/1000 | Loss: 0.00000972
Iteration 176/1000 | Loss: 0.00000972
Iteration 177/1000 | Loss: 0.00000972
Iteration 178/1000 | Loss: 0.00000972
Iteration 179/1000 | Loss: 0.00000972
Iteration 180/1000 | Loss: 0.00000972
Iteration 181/1000 | Loss: 0.00000972
Iteration 182/1000 | Loss: 0.00000972
Iteration 183/1000 | Loss: 0.00000972
Iteration 184/1000 | Loss: 0.00000972
Iteration 185/1000 | Loss: 0.00000972
Iteration 186/1000 | Loss: 0.00000972
Iteration 187/1000 | Loss: 0.00000972
Iteration 188/1000 | Loss: 0.00000971
Iteration 189/1000 | Loss: 0.00000971
Iteration 190/1000 | Loss: 0.00000971
Iteration 191/1000 | Loss: 0.00000971
Iteration 192/1000 | Loss: 0.00000971
Iteration 193/1000 | Loss: 0.00000971
Iteration 194/1000 | Loss: 0.00000971
Iteration 195/1000 | Loss: 0.00000971
Iteration 196/1000 | Loss: 0.00000971
Iteration 197/1000 | Loss: 0.00000970
Iteration 198/1000 | Loss: 0.00000970
Iteration 199/1000 | Loss: 0.00000970
Iteration 200/1000 | Loss: 0.00000970
Iteration 201/1000 | Loss: 0.00000970
Iteration 202/1000 | Loss: 0.00000970
Iteration 203/1000 | Loss: 0.00000970
Iteration 204/1000 | Loss: 0.00000970
Iteration 205/1000 | Loss: 0.00000970
Iteration 206/1000 | Loss: 0.00000970
Iteration 207/1000 | Loss: 0.00000970
Iteration 208/1000 | Loss: 0.00000970
Iteration 209/1000 | Loss: 0.00000970
Iteration 210/1000 | Loss: 0.00000970
Iteration 211/1000 | Loss: 0.00000970
Iteration 212/1000 | Loss: 0.00000970
Iteration 213/1000 | Loss: 0.00000970
Iteration 214/1000 | Loss: 0.00000969
Iteration 215/1000 | Loss: 0.00000969
Iteration 216/1000 | Loss: 0.00000969
Iteration 217/1000 | Loss: 0.00000969
Iteration 218/1000 | Loss: 0.00000969
Iteration 219/1000 | Loss: 0.00000969
Iteration 220/1000 | Loss: 0.00000969
Iteration 221/1000 | Loss: 0.00000969
Iteration 222/1000 | Loss: 0.00000969
Iteration 223/1000 | Loss: 0.00000969
Iteration 224/1000 | Loss: 0.00000968
Iteration 225/1000 | Loss: 0.00000968
Iteration 226/1000 | Loss: 0.00000968
Iteration 227/1000 | Loss: 0.00000967
Iteration 228/1000 | Loss: 0.00000967
Iteration 229/1000 | Loss: 0.00000967
Iteration 230/1000 | Loss: 0.00000967
Iteration 231/1000 | Loss: 0.00000967
Iteration 232/1000 | Loss: 0.00000967
Iteration 233/1000 | Loss: 0.00000967
Iteration 234/1000 | Loss: 0.00000967
Iteration 235/1000 | Loss: 0.00000967
Iteration 236/1000 | Loss: 0.00000967
Iteration 237/1000 | Loss: 0.00000967
Iteration 238/1000 | Loss: 0.00000967
Iteration 239/1000 | Loss: 0.00000967
Iteration 240/1000 | Loss: 0.00000967
Iteration 241/1000 | Loss: 0.00000966
Iteration 242/1000 | Loss: 0.00000966
Iteration 243/1000 | Loss: 0.00000966
Iteration 244/1000 | Loss: 0.00000966
Iteration 245/1000 | Loss: 0.00000966
Iteration 246/1000 | Loss: 0.00000966
Iteration 247/1000 | Loss: 0.00000966
Iteration 248/1000 | Loss: 0.00000966
Iteration 249/1000 | Loss: 0.00000966
Iteration 250/1000 | Loss: 0.00000966
Iteration 251/1000 | Loss: 0.00000966
Iteration 252/1000 | Loss: 0.00000966
Iteration 253/1000 | Loss: 0.00000966
Iteration 254/1000 | Loss: 0.00000966
Iteration 255/1000 | Loss: 0.00000966
Iteration 256/1000 | Loss: 0.00000965
Iteration 257/1000 | Loss: 0.00000965
Iteration 258/1000 | Loss: 0.00000965
Iteration 259/1000 | Loss: 0.00000965
Iteration 260/1000 | Loss: 0.00000965
Iteration 261/1000 | Loss: 0.00000965
Iteration 262/1000 | Loss: 0.00000965
Iteration 263/1000 | Loss: 0.00000965
Iteration 264/1000 | Loss: 0.00000965
Iteration 265/1000 | Loss: 0.00000965
Iteration 266/1000 | Loss: 0.00000965
Iteration 267/1000 | Loss: 0.00000965
Iteration 268/1000 | Loss: 0.00000965
Iteration 269/1000 | Loss: 0.00000965
Iteration 270/1000 | Loss: 0.00000965
Iteration 271/1000 | Loss: 0.00000965
Iteration 272/1000 | Loss: 0.00000965
Iteration 273/1000 | Loss: 0.00000965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [9.654022505856119e-06, 9.654022505856119e-06, 9.654022505856119e-06, 9.654022505856119e-06, 9.654022505856119e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.654022505856119e-06

Optimization complete. Final v2v error: 2.6929779052734375 mm

Highest mean error: 3.057988166809082 mm for frame 21

Lowest mean error: 2.289130210876465 mm for frame 0

Saving results

Total time: 38.570239782333374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927567
Iteration 2/25 | Loss: 0.00165829
Iteration 3/25 | Loss: 0.00126241
Iteration 4/25 | Loss: 0.00120003
Iteration 5/25 | Loss: 0.00114990
Iteration 6/25 | Loss: 0.00115169
Iteration 7/25 | Loss: 0.00115714
Iteration 8/25 | Loss: 0.00114273
Iteration 9/25 | Loss: 0.00113092
Iteration 10/25 | Loss: 0.00113056
Iteration 11/25 | Loss: 0.00113043
Iteration 12/25 | Loss: 0.00113043
Iteration 13/25 | Loss: 0.00113042
Iteration 14/25 | Loss: 0.00113042
Iteration 15/25 | Loss: 0.00113955
Iteration 16/25 | Loss: 0.00113039
Iteration 17/25 | Loss: 0.00113025
Iteration 18/25 | Loss: 0.00113005
Iteration 19/25 | Loss: 0.00112983
Iteration 20/25 | Loss: 0.00112974
Iteration 21/25 | Loss: 0.00112974
Iteration 22/25 | Loss: 0.00112973
Iteration 23/25 | Loss: 0.00112973
Iteration 24/25 | Loss: 0.00112973
Iteration 25/25 | Loss: 0.00112973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09351015
Iteration 2/25 | Loss: 0.00103148
Iteration 3/25 | Loss: 0.00103148
Iteration 4/25 | Loss: 0.00103148
Iteration 5/25 | Loss: 0.00103148
Iteration 6/25 | Loss: 0.00103148
Iteration 7/25 | Loss: 0.00103148
Iteration 8/25 | Loss: 0.00103148
Iteration 9/25 | Loss: 0.00103148
Iteration 10/25 | Loss: 0.00103148
Iteration 11/25 | Loss: 0.00103148
Iteration 12/25 | Loss: 0.00103148
Iteration 13/25 | Loss: 0.00103148
Iteration 14/25 | Loss: 0.00103148
Iteration 15/25 | Loss: 0.00103148
Iteration 16/25 | Loss: 0.00103148
Iteration 17/25 | Loss: 0.00103148
Iteration 18/25 | Loss: 0.00103148
Iteration 19/25 | Loss: 0.00103148
Iteration 20/25 | Loss: 0.00103148
Iteration 21/25 | Loss: 0.00103148
Iteration 22/25 | Loss: 0.00103148
Iteration 23/25 | Loss: 0.00103148
Iteration 24/25 | Loss: 0.00103148
Iteration 25/25 | Loss: 0.00103148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103148
Iteration 2/1000 | Loss: 0.00005381
Iteration 3/1000 | Loss: 0.00003801
Iteration 4/1000 | Loss: 0.00222335
Iteration 5/1000 | Loss: 0.00018925
Iteration 6/1000 | Loss: 0.00010836
Iteration 7/1000 | Loss: 0.00002678
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002221
Iteration 10/1000 | Loss: 0.00002163
Iteration 11/1000 | Loss: 0.00002130
Iteration 12/1000 | Loss: 0.00002104
Iteration 13/1000 | Loss: 0.00002076
Iteration 14/1000 | Loss: 0.00002055
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002037
Iteration 17/1000 | Loss: 0.00002026
Iteration 18/1000 | Loss: 0.00002020
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002010
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002007
Iteration 25/1000 | Loss: 0.00002007
Iteration 26/1000 | Loss: 0.00002007
Iteration 27/1000 | Loss: 0.00002006
Iteration 28/1000 | Loss: 0.00002006
Iteration 29/1000 | Loss: 0.00002004
Iteration 30/1000 | Loss: 0.00002004
Iteration 31/1000 | Loss: 0.00002004
Iteration 32/1000 | Loss: 0.00002004
Iteration 33/1000 | Loss: 0.00002004
Iteration 34/1000 | Loss: 0.00002004
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002004
Iteration 37/1000 | Loss: 0.00002004
Iteration 38/1000 | Loss: 0.00002004
Iteration 39/1000 | Loss: 0.00002004
Iteration 40/1000 | Loss: 0.00002003
Iteration 41/1000 | Loss: 0.00002003
Iteration 42/1000 | Loss: 0.00002003
Iteration 43/1000 | Loss: 0.00002002
Iteration 44/1000 | Loss: 0.00002002
Iteration 45/1000 | Loss: 0.00002002
Iteration 46/1000 | Loss: 0.00002002
Iteration 47/1000 | Loss: 0.00002002
Iteration 48/1000 | Loss: 0.00002002
Iteration 49/1000 | Loss: 0.00002002
Iteration 50/1000 | Loss: 0.00002002
Iteration 51/1000 | Loss: 0.00002002
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002002
Iteration 54/1000 | Loss: 0.00002002
Iteration 55/1000 | Loss: 0.00002002
Iteration 56/1000 | Loss: 0.00002001
Iteration 57/1000 | Loss: 0.00002001
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002000
Iteration 62/1000 | Loss: 0.00002000
Iteration 63/1000 | Loss: 0.00002000
Iteration 64/1000 | Loss: 0.00002000
Iteration 65/1000 | Loss: 0.00002000
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00001999
Iteration 68/1000 | Loss: 0.00001999
Iteration 69/1000 | Loss: 0.00001999
Iteration 70/1000 | Loss: 0.00001999
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001999
Iteration 73/1000 | Loss: 0.00001999
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.9987823179690167e-05, 1.9987823179690167e-05, 1.9987823179690167e-05, 1.9987823179690167e-05, 1.9987823179690167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9987823179690167e-05

Optimization complete. Final v2v error: 3.6790528297424316 mm

Highest mean error: 4.290736198425293 mm for frame 63

Lowest mean error: 3.0014092922210693 mm for frame 176

Saving results

Total time: 59.5448784828186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072034
Iteration 2/25 | Loss: 0.01072034
Iteration 3/25 | Loss: 0.01072034
Iteration 4/25 | Loss: 0.01072034
Iteration 5/25 | Loss: 0.01072034
Iteration 6/25 | Loss: 0.01072033
Iteration 7/25 | Loss: 0.01072033
Iteration 8/25 | Loss: 0.01072033
Iteration 9/25 | Loss: 0.01072033
Iteration 10/25 | Loss: 0.01072033
Iteration 11/25 | Loss: 0.01072033
Iteration 12/25 | Loss: 0.01072032
Iteration 13/25 | Loss: 0.01072032
Iteration 14/25 | Loss: 0.01072032
Iteration 15/25 | Loss: 0.01072032
Iteration 16/25 | Loss: 0.01072032
Iteration 17/25 | Loss: 0.01072031
Iteration 18/25 | Loss: 0.01072031
Iteration 19/25 | Loss: 0.01072031
Iteration 20/25 | Loss: 0.01072031
Iteration 21/25 | Loss: 0.01072031
Iteration 22/25 | Loss: 0.01072030
Iteration 23/25 | Loss: 0.01072030
Iteration 24/25 | Loss: 0.01072030
Iteration 25/25 | Loss: 0.01072030

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32403505
Iteration 2/25 | Loss: 0.18127434
Iteration 3/25 | Loss: 0.17870995
Iteration 4/25 | Loss: 0.17847188
Iteration 5/25 | Loss: 0.17847183
Iteration 6/25 | Loss: 0.17847183
Iteration 7/25 | Loss: 0.17847179
Iteration 8/25 | Loss: 0.17847179
Iteration 9/25 | Loss: 0.17847179
Iteration 10/25 | Loss: 0.17847179
Iteration 11/25 | Loss: 0.17847179
Iteration 12/25 | Loss: 0.17847179
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.17847178876399994, 0.17847178876399994, 0.17847178876399994, 0.17847178876399994, 0.17847178876399994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17847178876399994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17847179
Iteration 2/1000 | Loss: 0.00189842
Iteration 3/1000 | Loss: 0.00247685
Iteration 4/1000 | Loss: 0.00061919
Iteration 5/1000 | Loss: 0.00023899
Iteration 6/1000 | Loss: 0.00014223
Iteration 7/1000 | Loss: 0.00021943
Iteration 8/1000 | Loss: 0.00031758
Iteration 9/1000 | Loss: 0.00016214
Iteration 10/1000 | Loss: 0.00004944
Iteration 11/1000 | Loss: 0.00023849
Iteration 12/1000 | Loss: 0.00004370
Iteration 13/1000 | Loss: 0.00023695
Iteration 14/1000 | Loss: 0.00005319
Iteration 15/1000 | Loss: 0.00003053
Iteration 16/1000 | Loss: 0.00005213
Iteration 17/1000 | Loss: 0.00004492
Iteration 18/1000 | Loss: 0.00012262
Iteration 19/1000 | Loss: 0.00002592
Iteration 20/1000 | Loss: 0.00004068
Iteration 21/1000 | Loss: 0.00013834
Iteration 22/1000 | Loss: 0.00002364
Iteration 23/1000 | Loss: 0.00002826
Iteration 24/1000 | Loss: 0.00002117
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002001
Iteration 27/1000 | Loss: 0.00015293
Iteration 28/1000 | Loss: 0.00001899
Iteration 29/1000 | Loss: 0.00001823
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001711
Iteration 32/1000 | Loss: 0.00001676
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00002952
Iteration 35/1000 | Loss: 0.00001666
Iteration 36/1000 | Loss: 0.00001602
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001549
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001529
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001497
Iteration 50/1000 | Loss: 0.00001497
Iteration 51/1000 | Loss: 0.00001497
Iteration 52/1000 | Loss: 0.00001496
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001492
Iteration 59/1000 | Loss: 0.00001828
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001483
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001480
Iteration 66/1000 | Loss: 0.00001480
Iteration 67/1000 | Loss: 0.00001480
Iteration 68/1000 | Loss: 0.00001479
Iteration 69/1000 | Loss: 0.00001479
Iteration 70/1000 | Loss: 0.00001479
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00002219
Iteration 79/1000 | Loss: 0.00001511
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001459
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001456
Iteration 127/1000 | Loss: 0.00001456
Iteration 128/1000 | Loss: 0.00001456
Iteration 129/1000 | Loss: 0.00001456
Iteration 130/1000 | Loss: 0.00001456
Iteration 131/1000 | Loss: 0.00001456
Iteration 132/1000 | Loss: 0.00001456
Iteration 133/1000 | Loss: 0.00001456
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001455
Iteration 136/1000 | Loss: 0.00001455
Iteration 137/1000 | Loss: 0.00001455
Iteration 138/1000 | Loss: 0.00001455
Iteration 139/1000 | Loss: 0.00001455
Iteration 140/1000 | Loss: 0.00001455
Iteration 141/1000 | Loss: 0.00001455
Iteration 142/1000 | Loss: 0.00001455
Iteration 143/1000 | Loss: 0.00001455
Iteration 144/1000 | Loss: 0.00001455
Iteration 145/1000 | Loss: 0.00001455
Iteration 146/1000 | Loss: 0.00001455
Iteration 147/1000 | Loss: 0.00001455
Iteration 148/1000 | Loss: 0.00001455
Iteration 149/1000 | Loss: 0.00001455
Iteration 150/1000 | Loss: 0.00001455
Iteration 151/1000 | Loss: 0.00001455
Iteration 152/1000 | Loss: 0.00001455
Iteration 153/1000 | Loss: 0.00001455
Iteration 154/1000 | Loss: 0.00001455
Iteration 155/1000 | Loss: 0.00001455
Iteration 156/1000 | Loss: 0.00001455
Iteration 157/1000 | Loss: 0.00001454
Iteration 158/1000 | Loss: 0.00001454
Iteration 159/1000 | Loss: 0.00001454
Iteration 160/1000 | Loss: 0.00001454
Iteration 161/1000 | Loss: 0.00001454
Iteration 162/1000 | Loss: 0.00001454
Iteration 163/1000 | Loss: 0.00001454
Iteration 164/1000 | Loss: 0.00001454
Iteration 165/1000 | Loss: 0.00001454
Iteration 166/1000 | Loss: 0.00001454
Iteration 167/1000 | Loss: 0.00001454
Iteration 168/1000 | Loss: 0.00001454
Iteration 169/1000 | Loss: 0.00001454
Iteration 170/1000 | Loss: 0.00001454
Iteration 171/1000 | Loss: 0.00001454
Iteration 172/1000 | Loss: 0.00001454
Iteration 173/1000 | Loss: 0.00001454
Iteration 174/1000 | Loss: 0.00001453
Iteration 175/1000 | Loss: 0.00001453
Iteration 176/1000 | Loss: 0.00001453
Iteration 177/1000 | Loss: 0.00001453
Iteration 178/1000 | Loss: 0.00001453
Iteration 179/1000 | Loss: 0.00001453
Iteration 180/1000 | Loss: 0.00001453
Iteration 181/1000 | Loss: 0.00001453
Iteration 182/1000 | Loss: 0.00001453
Iteration 183/1000 | Loss: 0.00001453
Iteration 184/1000 | Loss: 0.00001453
Iteration 185/1000 | Loss: 0.00001452
Iteration 186/1000 | Loss: 0.00001452
Iteration 187/1000 | Loss: 0.00001452
Iteration 188/1000 | Loss: 0.00001452
Iteration 189/1000 | Loss: 0.00001452
Iteration 190/1000 | Loss: 0.00001452
Iteration 191/1000 | Loss: 0.00001452
Iteration 192/1000 | Loss: 0.00001451
Iteration 193/1000 | Loss: 0.00001451
Iteration 194/1000 | Loss: 0.00001451
Iteration 195/1000 | Loss: 0.00001451
Iteration 196/1000 | Loss: 0.00001450
Iteration 197/1000 | Loss: 0.00001450
Iteration 198/1000 | Loss: 0.00001449
Iteration 199/1000 | Loss: 0.00001449
Iteration 200/1000 | Loss: 0.00001448
Iteration 201/1000 | Loss: 0.00002337
Iteration 202/1000 | Loss: 0.00001445
Iteration 203/1000 | Loss: 0.00001444
Iteration 204/1000 | Loss: 0.00001444
Iteration 205/1000 | Loss: 0.00001444
Iteration 206/1000 | Loss: 0.00001444
Iteration 207/1000 | Loss: 0.00001444
Iteration 208/1000 | Loss: 0.00001444
Iteration 209/1000 | Loss: 0.00001444
Iteration 210/1000 | Loss: 0.00001444
Iteration 211/1000 | Loss: 0.00001443
Iteration 212/1000 | Loss: 0.00001443
Iteration 213/1000 | Loss: 0.00001443
Iteration 214/1000 | Loss: 0.00001443
Iteration 215/1000 | Loss: 0.00001800
Iteration 216/1000 | Loss: 0.00001444
Iteration 217/1000 | Loss: 0.00001442
Iteration 218/1000 | Loss: 0.00001442
Iteration 219/1000 | Loss: 0.00001442
Iteration 220/1000 | Loss: 0.00001442
Iteration 221/1000 | Loss: 0.00001442
Iteration 222/1000 | Loss: 0.00001442
Iteration 223/1000 | Loss: 0.00001442
Iteration 224/1000 | Loss: 0.00001442
Iteration 225/1000 | Loss: 0.00001442
Iteration 226/1000 | Loss: 0.00001441
Iteration 227/1000 | Loss: 0.00001441
Iteration 228/1000 | Loss: 0.00001441
Iteration 229/1000 | Loss: 0.00001441
Iteration 230/1000 | Loss: 0.00001441
Iteration 231/1000 | Loss: 0.00001441
Iteration 232/1000 | Loss: 0.00001441
Iteration 233/1000 | Loss: 0.00001441
Iteration 234/1000 | Loss: 0.00001441
Iteration 235/1000 | Loss: 0.00001441
Iteration 236/1000 | Loss: 0.00001441
Iteration 237/1000 | Loss: 0.00001441
Iteration 238/1000 | Loss: 0.00001441
Iteration 239/1000 | Loss: 0.00001441
Iteration 240/1000 | Loss: 0.00001441
Iteration 241/1000 | Loss: 0.00001441
Iteration 242/1000 | Loss: 0.00001441
Iteration 243/1000 | Loss: 0.00001441
Iteration 244/1000 | Loss: 0.00001441
Iteration 245/1000 | Loss: 0.00001441
Iteration 246/1000 | Loss: 0.00001441
Iteration 247/1000 | Loss: 0.00001441
Iteration 248/1000 | Loss: 0.00001441
Iteration 249/1000 | Loss: 0.00001441
Iteration 250/1000 | Loss: 0.00001441
Iteration 251/1000 | Loss: 0.00001441
Iteration 252/1000 | Loss: 0.00001441
Iteration 253/1000 | Loss: 0.00001441
Iteration 254/1000 | Loss: 0.00001441
Iteration 255/1000 | Loss: 0.00001441
Iteration 256/1000 | Loss: 0.00001441
Iteration 257/1000 | Loss: 0.00001441
Iteration 258/1000 | Loss: 0.00001441
Iteration 259/1000 | Loss: 0.00001441
Iteration 260/1000 | Loss: 0.00001441
Iteration 261/1000 | Loss: 0.00001441
Iteration 262/1000 | Loss: 0.00001441
Iteration 263/1000 | Loss: 0.00001441
Iteration 264/1000 | Loss: 0.00001441
Iteration 265/1000 | Loss: 0.00001441
Iteration 266/1000 | Loss: 0.00001441
Iteration 267/1000 | Loss: 0.00001441
Iteration 268/1000 | Loss: 0.00001441
Iteration 269/1000 | Loss: 0.00001441
Iteration 270/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.440669166186126e-05, 1.440669166186126e-05, 1.440669166186126e-05, 1.440669166186126e-05, 1.440669166186126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.440669166186126e-05

Optimization complete. Final v2v error: 3.104732036590576 mm

Highest mean error: 5.597402572631836 mm for frame 65

Lowest mean error: 2.4246127605438232 mm for frame 223

Saving results

Total time: 98.20038628578186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840595
Iteration 2/25 | Loss: 0.00128202
Iteration 3/25 | Loss: 0.00109001
Iteration 4/25 | Loss: 0.00107499
Iteration 5/25 | Loss: 0.00107301
Iteration 6/25 | Loss: 0.00107301
Iteration 7/25 | Loss: 0.00107299
Iteration 8/25 | Loss: 0.00107299
Iteration 9/25 | Loss: 0.00107299
Iteration 10/25 | Loss: 0.00107299
Iteration 11/25 | Loss: 0.00107299
Iteration 12/25 | Loss: 0.00107299
Iteration 13/25 | Loss: 0.00107299
Iteration 14/25 | Loss: 0.00107299
Iteration 15/25 | Loss: 0.00107299
Iteration 16/25 | Loss: 0.00107299
Iteration 17/25 | Loss: 0.00107299
Iteration 18/25 | Loss: 0.00107299
Iteration 19/25 | Loss: 0.00107299
Iteration 20/25 | Loss: 0.00107299
Iteration 21/25 | Loss: 0.00107299
Iteration 22/25 | Loss: 0.00107299
Iteration 23/25 | Loss: 0.00107299
Iteration 24/25 | Loss: 0.00107299
Iteration 25/25 | Loss: 0.00107299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28295267
Iteration 2/25 | Loss: 0.00114756
Iteration 3/25 | Loss: 0.00114754
Iteration 4/25 | Loss: 0.00114754
Iteration 5/25 | Loss: 0.00114754
Iteration 6/25 | Loss: 0.00114754
Iteration 7/25 | Loss: 0.00114754
Iteration 8/25 | Loss: 0.00114754
Iteration 9/25 | Loss: 0.00114754
Iteration 10/25 | Loss: 0.00114754
Iteration 11/25 | Loss: 0.00114754
Iteration 12/25 | Loss: 0.00114754
Iteration 13/25 | Loss: 0.00114754
Iteration 14/25 | Loss: 0.00114754
Iteration 15/25 | Loss: 0.00114754
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011475416831672192, 0.0011475416831672192, 0.0011475416831672192, 0.0011475416831672192, 0.0011475416831672192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011475416831672192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114754
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001870
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001384
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001323
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001260
Iteration 16/1000 | Loss: 0.00001260
Iteration 17/1000 | Loss: 0.00001260
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001258
Iteration 22/1000 | Loss: 0.00001258
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001253
Iteration 29/1000 | Loss: 0.00001252
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001243
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001243
Iteration 47/1000 | Loss: 0.00001243
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001242
Iteration 53/1000 | Loss: 0.00001242
Iteration 54/1000 | Loss: 0.00001242
Iteration 55/1000 | Loss: 0.00001242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [1.2424500710039865e-05, 1.2424500710039865e-05, 1.2424500710039865e-05, 1.2424500710039865e-05, 1.2424500710039865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2424500710039865e-05

Optimization complete. Final v2v error: 3.0218069553375244 mm

Highest mean error: 3.447762966156006 mm for frame 1

Lowest mean error: 2.6383841037750244 mm for frame 175

Saving results

Total time: 29.20627498626709
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518074
Iteration 2/25 | Loss: 0.00141234
Iteration 3/25 | Loss: 0.00107948
Iteration 4/25 | Loss: 0.00105420
Iteration 5/25 | Loss: 0.00104803
Iteration 6/25 | Loss: 0.00104613
Iteration 7/25 | Loss: 0.00104534
Iteration 8/25 | Loss: 0.00104524
Iteration 9/25 | Loss: 0.00104524
Iteration 10/25 | Loss: 0.00104524
Iteration 11/25 | Loss: 0.00104524
Iteration 12/25 | Loss: 0.00104524
Iteration 13/25 | Loss: 0.00104524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001045240554958582, 0.001045240554958582, 0.001045240554958582, 0.001045240554958582, 0.001045240554958582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001045240554958582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23273861
Iteration 2/25 | Loss: 0.00097641
Iteration 3/25 | Loss: 0.00097639
Iteration 4/25 | Loss: 0.00097639
Iteration 5/25 | Loss: 0.00097639
Iteration 6/25 | Loss: 0.00097639
Iteration 7/25 | Loss: 0.00097639
Iteration 8/25 | Loss: 0.00097639
Iteration 9/25 | Loss: 0.00097639
Iteration 10/25 | Loss: 0.00097639
Iteration 11/25 | Loss: 0.00097639
Iteration 12/25 | Loss: 0.00097639
Iteration 13/25 | Loss: 0.00097639
Iteration 14/25 | Loss: 0.00097639
Iteration 15/25 | Loss: 0.00097639
Iteration 16/25 | Loss: 0.00097639
Iteration 17/25 | Loss: 0.00097639
Iteration 18/25 | Loss: 0.00097639
Iteration 19/25 | Loss: 0.00097639
Iteration 20/25 | Loss: 0.00097639
Iteration 21/25 | Loss: 0.00097639
Iteration 22/25 | Loss: 0.00097639
Iteration 23/25 | Loss: 0.00097639
Iteration 24/25 | Loss: 0.00097639
Iteration 25/25 | Loss: 0.00097639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097639
Iteration 2/1000 | Loss: 0.00004402
Iteration 3/1000 | Loss: 0.00002897
Iteration 4/1000 | Loss: 0.00002497
Iteration 5/1000 | Loss: 0.00002282
Iteration 6/1000 | Loss: 0.00002183
Iteration 7/1000 | Loss: 0.00002130
Iteration 8/1000 | Loss: 0.00002081
Iteration 9/1000 | Loss: 0.00002045
Iteration 10/1000 | Loss: 0.00002020
Iteration 11/1000 | Loss: 0.00002000
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00001963
Iteration 14/1000 | Loss: 0.00001946
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001929
Iteration 18/1000 | Loss: 0.00001927
Iteration 19/1000 | Loss: 0.00001923
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001913
Iteration 23/1000 | Loss: 0.00001912
Iteration 24/1000 | Loss: 0.00001907
Iteration 25/1000 | Loss: 0.00001906
Iteration 26/1000 | Loss: 0.00001906
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001900
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001896
Iteration 37/1000 | Loss: 0.00001896
Iteration 38/1000 | Loss: 0.00001896
Iteration 39/1000 | Loss: 0.00001896
Iteration 40/1000 | Loss: 0.00001896
Iteration 41/1000 | Loss: 0.00001896
Iteration 42/1000 | Loss: 0.00001896
Iteration 43/1000 | Loss: 0.00001896
Iteration 44/1000 | Loss: 0.00001896
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001895
Iteration 47/1000 | Loss: 0.00001895
Iteration 48/1000 | Loss: 0.00001894
Iteration 49/1000 | Loss: 0.00001894
Iteration 50/1000 | Loss: 0.00001894
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001892
Iteration 56/1000 | Loss: 0.00001892
Iteration 57/1000 | Loss: 0.00001891
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001890
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001889
Iteration 66/1000 | Loss: 0.00001889
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001888
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001888
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001887
Iteration 80/1000 | Loss: 0.00001887
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001887
Iteration 87/1000 | Loss: 0.00001887
Iteration 88/1000 | Loss: 0.00001887
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001886
Iteration 91/1000 | Loss: 0.00001886
Iteration 92/1000 | Loss: 0.00001886
Iteration 93/1000 | Loss: 0.00001886
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001885
Iteration 97/1000 | Loss: 0.00001885
Iteration 98/1000 | Loss: 0.00001885
Iteration 99/1000 | Loss: 0.00001885
Iteration 100/1000 | Loss: 0.00001885
Iteration 101/1000 | Loss: 0.00001885
Iteration 102/1000 | Loss: 0.00001885
Iteration 103/1000 | Loss: 0.00001885
Iteration 104/1000 | Loss: 0.00001884
Iteration 105/1000 | Loss: 0.00001884
Iteration 106/1000 | Loss: 0.00001884
Iteration 107/1000 | Loss: 0.00001884
Iteration 108/1000 | Loss: 0.00001883
Iteration 109/1000 | Loss: 0.00001883
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001883
Iteration 116/1000 | Loss: 0.00001883
Iteration 117/1000 | Loss: 0.00001883
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001882
Iteration 120/1000 | Loss: 0.00001882
Iteration 121/1000 | Loss: 0.00001882
Iteration 122/1000 | Loss: 0.00001882
Iteration 123/1000 | Loss: 0.00001882
Iteration 124/1000 | Loss: 0.00001882
Iteration 125/1000 | Loss: 0.00001882
Iteration 126/1000 | Loss: 0.00001882
Iteration 127/1000 | Loss: 0.00001882
Iteration 128/1000 | Loss: 0.00001882
Iteration 129/1000 | Loss: 0.00001882
Iteration 130/1000 | Loss: 0.00001882
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001882
Iteration 136/1000 | Loss: 0.00001882
Iteration 137/1000 | Loss: 0.00001882
Iteration 138/1000 | Loss: 0.00001882
Iteration 139/1000 | Loss: 0.00001882
Iteration 140/1000 | Loss: 0.00001882
Iteration 141/1000 | Loss: 0.00001882
Iteration 142/1000 | Loss: 0.00001882
Iteration 143/1000 | Loss: 0.00001882
Iteration 144/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.8816013835021295e-05, 1.8816013835021295e-05, 1.8816013835021295e-05, 1.8816013835021295e-05, 1.8816013835021295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8816013835021295e-05

Optimization complete. Final v2v error: 3.347404718399048 mm

Highest mean error: 5.630839824676514 mm for frame 59

Lowest mean error: 2.122892141342163 mm for frame 2

Saving results

Total time: 45.2130401134491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652674
Iteration 2/25 | Loss: 0.00117243
Iteration 3/25 | Loss: 0.00107178
Iteration 4/25 | Loss: 0.00106352
Iteration 5/25 | Loss: 0.00106117
Iteration 6/25 | Loss: 0.00106117
Iteration 7/25 | Loss: 0.00106117
Iteration 8/25 | Loss: 0.00106117
Iteration 9/25 | Loss: 0.00106117
Iteration 10/25 | Loss: 0.00106117
Iteration 11/25 | Loss: 0.00106117
Iteration 12/25 | Loss: 0.00106117
Iteration 13/25 | Loss: 0.00106117
Iteration 14/25 | Loss: 0.00106117
Iteration 15/25 | Loss: 0.00106117
Iteration 16/25 | Loss: 0.00106117
Iteration 17/25 | Loss: 0.00106117
Iteration 18/25 | Loss: 0.00106117
Iteration 19/25 | Loss: 0.00106117
Iteration 20/25 | Loss: 0.00106117
Iteration 21/25 | Loss: 0.00106117
Iteration 22/25 | Loss: 0.00106117
Iteration 23/25 | Loss: 0.00106117
Iteration 24/25 | Loss: 0.00106117
Iteration 25/25 | Loss: 0.00106117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07260072
Iteration 2/25 | Loss: 0.00072118
Iteration 3/25 | Loss: 0.00072115
Iteration 4/25 | Loss: 0.00072115
Iteration 5/25 | Loss: 0.00072115
Iteration 6/25 | Loss: 0.00072115
Iteration 7/25 | Loss: 0.00072115
Iteration 8/25 | Loss: 0.00072115
Iteration 9/25 | Loss: 0.00072115
Iteration 10/25 | Loss: 0.00072115
Iteration 11/25 | Loss: 0.00072115
Iteration 12/25 | Loss: 0.00072115
Iteration 13/25 | Loss: 0.00072115
Iteration 14/25 | Loss: 0.00072115
Iteration 15/25 | Loss: 0.00072115
Iteration 16/25 | Loss: 0.00072115
Iteration 17/25 | Loss: 0.00072115
Iteration 18/25 | Loss: 0.00072115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007211487973108888, 0.0007211487973108888, 0.0007211487973108888, 0.0007211487973108888, 0.0007211487973108888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007211487973108888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072115
Iteration 2/1000 | Loss: 0.00003362
Iteration 3/1000 | Loss: 0.00002547
Iteration 4/1000 | Loss: 0.00002249
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00001995
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001847
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001813
Iteration 14/1000 | Loss: 0.00001810
Iteration 15/1000 | Loss: 0.00001799
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001793
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001789
Iteration 27/1000 | Loss: 0.00001788
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001787
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001787
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001786
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001782
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001775
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001771
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001770
Iteration 118/1000 | Loss: 0.00001770
Iteration 119/1000 | Loss: 0.00001770
Iteration 120/1000 | Loss: 0.00001770
Iteration 121/1000 | Loss: 0.00001769
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001769
Iteration 124/1000 | Loss: 0.00001769
Iteration 125/1000 | Loss: 0.00001769
Iteration 126/1000 | Loss: 0.00001769
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001768
Iteration 130/1000 | Loss: 0.00001768
Iteration 131/1000 | Loss: 0.00001768
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001767
Iteration 135/1000 | Loss: 0.00001767
Iteration 136/1000 | Loss: 0.00001767
Iteration 137/1000 | Loss: 0.00001767
Iteration 138/1000 | Loss: 0.00001766
Iteration 139/1000 | Loss: 0.00001766
Iteration 140/1000 | Loss: 0.00001766
Iteration 141/1000 | Loss: 0.00001766
Iteration 142/1000 | Loss: 0.00001765
Iteration 143/1000 | Loss: 0.00001765
Iteration 144/1000 | Loss: 0.00001765
Iteration 145/1000 | Loss: 0.00001765
Iteration 146/1000 | Loss: 0.00001765
Iteration 147/1000 | Loss: 0.00001765
Iteration 148/1000 | Loss: 0.00001765
Iteration 149/1000 | Loss: 0.00001765
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Iteration 154/1000 | Loss: 0.00001764
Iteration 155/1000 | Loss: 0.00001764
Iteration 156/1000 | Loss: 0.00001764
Iteration 157/1000 | Loss: 0.00001764
Iteration 158/1000 | Loss: 0.00001763
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001762
Iteration 165/1000 | Loss: 0.00001762
Iteration 166/1000 | Loss: 0.00001762
Iteration 167/1000 | Loss: 0.00001762
Iteration 168/1000 | Loss: 0.00001762
Iteration 169/1000 | Loss: 0.00001762
Iteration 170/1000 | Loss: 0.00001762
Iteration 171/1000 | Loss: 0.00001762
Iteration 172/1000 | Loss: 0.00001762
Iteration 173/1000 | Loss: 0.00001762
Iteration 174/1000 | Loss: 0.00001762
Iteration 175/1000 | Loss: 0.00001762
Iteration 176/1000 | Loss: 0.00001762
Iteration 177/1000 | Loss: 0.00001762
Iteration 178/1000 | Loss: 0.00001762
Iteration 179/1000 | Loss: 0.00001762
Iteration 180/1000 | Loss: 0.00001762
Iteration 181/1000 | Loss: 0.00001762
Iteration 182/1000 | Loss: 0.00001762
Iteration 183/1000 | Loss: 0.00001762
Iteration 184/1000 | Loss: 0.00001762
Iteration 185/1000 | Loss: 0.00001762
Iteration 186/1000 | Loss: 0.00001762
Iteration 187/1000 | Loss: 0.00001762
Iteration 188/1000 | Loss: 0.00001762
Iteration 189/1000 | Loss: 0.00001762
Iteration 190/1000 | Loss: 0.00001762
Iteration 191/1000 | Loss: 0.00001762
Iteration 192/1000 | Loss: 0.00001762
Iteration 193/1000 | Loss: 0.00001762
Iteration 194/1000 | Loss: 0.00001762
Iteration 195/1000 | Loss: 0.00001762
Iteration 196/1000 | Loss: 0.00001762
Iteration 197/1000 | Loss: 0.00001762
Iteration 198/1000 | Loss: 0.00001762
Iteration 199/1000 | Loss: 0.00001762
Iteration 200/1000 | Loss: 0.00001762
Iteration 201/1000 | Loss: 0.00001762
Iteration 202/1000 | Loss: 0.00001762
Iteration 203/1000 | Loss: 0.00001762
Iteration 204/1000 | Loss: 0.00001762
Iteration 205/1000 | Loss: 0.00001762
Iteration 206/1000 | Loss: 0.00001762
Iteration 207/1000 | Loss: 0.00001762
Iteration 208/1000 | Loss: 0.00001762
Iteration 209/1000 | Loss: 0.00001762
Iteration 210/1000 | Loss: 0.00001762
Iteration 211/1000 | Loss: 0.00001762
Iteration 212/1000 | Loss: 0.00001762
Iteration 213/1000 | Loss: 0.00001762
Iteration 214/1000 | Loss: 0.00001762
Iteration 215/1000 | Loss: 0.00001762
Iteration 216/1000 | Loss: 0.00001762
Iteration 217/1000 | Loss: 0.00001762
Iteration 218/1000 | Loss: 0.00001762
Iteration 219/1000 | Loss: 0.00001762
Iteration 220/1000 | Loss: 0.00001762
Iteration 221/1000 | Loss: 0.00001762
Iteration 222/1000 | Loss: 0.00001762
Iteration 223/1000 | Loss: 0.00001762
Iteration 224/1000 | Loss: 0.00001762
Iteration 225/1000 | Loss: 0.00001762
Iteration 226/1000 | Loss: 0.00001762
Iteration 227/1000 | Loss: 0.00001762
Iteration 228/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.7621521692490205e-05, 1.7621521692490205e-05, 1.7621521692490205e-05, 1.7621521692490205e-05, 1.7621521692490205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7621521692490205e-05

Optimization complete. Final v2v error: 3.478726625442505 mm

Highest mean error: 4.094829082489014 mm for frame 88

Lowest mean error: 2.721712589263916 mm for frame 6

Saving results

Total time: 44.79921889305115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823943
Iteration 2/25 | Loss: 0.00153900
Iteration 3/25 | Loss: 0.00115970
Iteration 4/25 | Loss: 0.00109345
Iteration 5/25 | Loss: 0.00108216
Iteration 6/25 | Loss: 0.00107699
Iteration 7/25 | Loss: 0.00108162
Iteration 8/25 | Loss: 0.00107819
Iteration 9/25 | Loss: 0.00107278
Iteration 10/25 | Loss: 0.00107678
Iteration 11/25 | Loss: 0.00106903
Iteration 12/25 | Loss: 0.00106204
Iteration 13/25 | Loss: 0.00106382
Iteration 14/25 | Loss: 0.00105738
Iteration 15/25 | Loss: 0.00105331
Iteration 16/25 | Loss: 0.00104657
Iteration 17/25 | Loss: 0.00104844
Iteration 18/25 | Loss: 0.00104894
Iteration 19/25 | Loss: 0.00104860
Iteration 20/25 | Loss: 0.00104969
Iteration 21/25 | Loss: 0.00104833
Iteration 22/25 | Loss: 0.00104783
Iteration 23/25 | Loss: 0.00104784
Iteration 24/25 | Loss: 0.00105014
Iteration 25/25 | Loss: 0.00104930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33435214
Iteration 2/25 | Loss: 0.00109670
Iteration 3/25 | Loss: 0.00109670
Iteration 4/25 | Loss: 0.00109670
Iteration 5/25 | Loss: 0.00109670
Iteration 6/25 | Loss: 0.00109670
Iteration 7/25 | Loss: 0.00109670
Iteration 8/25 | Loss: 0.00109670
Iteration 9/25 | Loss: 0.00109670
Iteration 10/25 | Loss: 0.00109670
Iteration 11/25 | Loss: 0.00109670
Iteration 12/25 | Loss: 0.00109670
Iteration 13/25 | Loss: 0.00109670
Iteration 14/25 | Loss: 0.00109670
Iteration 15/25 | Loss: 0.00109670
Iteration 16/25 | Loss: 0.00109670
Iteration 17/25 | Loss: 0.00109670
Iteration 18/25 | Loss: 0.00109670
Iteration 19/25 | Loss: 0.00109670
Iteration 20/25 | Loss: 0.00109670
Iteration 21/25 | Loss: 0.00109670
Iteration 22/25 | Loss: 0.00109670
Iteration 23/25 | Loss: 0.00109670
Iteration 24/25 | Loss: 0.00109670
Iteration 25/25 | Loss: 0.00109670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109670
Iteration 2/1000 | Loss: 0.00007788
Iteration 3/1000 | Loss: 0.00008434
Iteration 4/1000 | Loss: 0.00013367
Iteration 5/1000 | Loss: 0.00009582
Iteration 6/1000 | Loss: 0.00014413
Iteration 7/1000 | Loss: 0.00013688
Iteration 8/1000 | Loss: 0.00018095
Iteration 9/1000 | Loss: 0.00016447
Iteration 10/1000 | Loss: 0.00019504
Iteration 11/1000 | Loss: 0.00010682
Iteration 12/1000 | Loss: 0.00010582
Iteration 13/1000 | Loss: 0.00007698
Iteration 14/1000 | Loss: 0.00011344
Iteration 15/1000 | Loss: 0.00011506
Iteration 16/1000 | Loss: 0.00010184
Iteration 17/1000 | Loss: 0.00018754
Iteration 18/1000 | Loss: 0.00011274
Iteration 19/1000 | Loss: 0.00020220
Iteration 20/1000 | Loss: 0.00017129
Iteration 21/1000 | Loss: 0.00012827
Iteration 22/1000 | Loss: 0.00017127
Iteration 23/1000 | Loss: 0.00016987
Iteration 24/1000 | Loss: 0.00017813
Iteration 25/1000 | Loss: 0.00012063
Iteration 26/1000 | Loss: 0.00010582
Iteration 27/1000 | Loss: 0.00008507
Iteration 28/1000 | Loss: 0.00011740
Iteration 29/1000 | Loss: 0.00010849
Iteration 30/1000 | Loss: 0.00016688
Iteration 31/1000 | Loss: 0.00017620
Iteration 32/1000 | Loss: 0.00017949
Iteration 33/1000 | Loss: 0.00010523
Iteration 34/1000 | Loss: 0.00005766
Iteration 35/1000 | Loss: 0.00008188
Iteration 36/1000 | Loss: 0.00016655
Iteration 37/1000 | Loss: 0.00019369
Iteration 38/1000 | Loss: 0.00014660
Iteration 39/1000 | Loss: 0.00018033
Iteration 40/1000 | Loss: 0.00016320
Iteration 41/1000 | Loss: 0.00016117
Iteration 42/1000 | Loss: 0.00012714
Iteration 43/1000 | Loss: 0.00016157
Iteration 44/1000 | Loss: 0.00017020
Iteration 45/1000 | Loss: 0.00019507
Iteration 46/1000 | Loss: 0.00015405
Iteration 47/1000 | Loss: 0.00017001
Iteration 48/1000 | Loss: 0.00014473
Iteration 49/1000 | Loss: 0.00016819
Iteration 50/1000 | Loss: 0.00016765
Iteration 51/1000 | Loss: 0.00019735
Iteration 52/1000 | Loss: 0.00028209
Iteration 53/1000 | Loss: 0.00019835
Iteration 54/1000 | Loss: 0.00014644
Iteration 55/1000 | Loss: 0.00009894
Iteration 56/1000 | Loss: 0.00012050
Iteration 57/1000 | Loss: 0.00013250
Iteration 58/1000 | Loss: 0.00014606
Iteration 59/1000 | Loss: 0.00011883
Iteration 60/1000 | Loss: 0.00004692
Iteration 61/1000 | Loss: 0.00009245
Iteration 62/1000 | Loss: 0.00009772
Iteration 63/1000 | Loss: 0.00004791
Iteration 64/1000 | Loss: 0.00008125
Iteration 65/1000 | Loss: 0.00005936
Iteration 66/1000 | Loss: 0.00005257
Iteration 67/1000 | Loss: 0.00006726
Iteration 68/1000 | Loss: 0.00010084
Iteration 69/1000 | Loss: 0.00008832
Iteration 70/1000 | Loss: 0.00008959
Iteration 71/1000 | Loss: 0.00009632
Iteration 72/1000 | Loss: 0.00008431
Iteration 73/1000 | Loss: 0.00008845
Iteration 74/1000 | Loss: 0.00007623
Iteration 75/1000 | Loss: 0.00008102
Iteration 76/1000 | Loss: 0.00007406
Iteration 77/1000 | Loss: 0.00007458
Iteration 78/1000 | Loss: 0.00004943
Iteration 79/1000 | Loss: 0.00005240
Iteration 80/1000 | Loss: 0.00008868
Iteration 81/1000 | Loss: 0.00006892
Iteration 82/1000 | Loss: 0.00006900
Iteration 83/1000 | Loss: 0.00007364
Iteration 84/1000 | Loss: 0.00007724
Iteration 85/1000 | Loss: 0.00007514
Iteration 86/1000 | Loss: 0.00007845
Iteration 87/1000 | Loss: 0.00007848
Iteration 88/1000 | Loss: 0.00009687
Iteration 89/1000 | Loss: 0.00007148
Iteration 90/1000 | Loss: 0.00009282
Iteration 91/1000 | Loss: 0.00005237
Iteration 92/1000 | Loss: 0.00004636
Iteration 93/1000 | Loss: 0.00004339
Iteration 94/1000 | Loss: 0.00005370
Iteration 95/1000 | Loss: 0.00006561
Iteration 96/1000 | Loss: 0.00007064
Iteration 97/1000 | Loss: 0.00005854
Iteration 98/1000 | Loss: 0.00004680
Iteration 99/1000 | Loss: 0.00003217
Iteration 100/1000 | Loss: 0.00005121
Iteration 101/1000 | Loss: 0.00007031
Iteration 102/1000 | Loss: 0.00005578
Iteration 103/1000 | Loss: 0.00007764
Iteration 104/1000 | Loss: 0.00003503
Iteration 105/1000 | Loss: 0.00003436
Iteration 106/1000 | Loss: 0.00005801
Iteration 107/1000 | Loss: 0.00003268
Iteration 108/1000 | Loss: 0.00006585
Iteration 109/1000 | Loss: 0.00005116
Iteration 110/1000 | Loss: 0.00002696
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00004064
Iteration 113/1000 | Loss: 0.00002322
Iteration 114/1000 | Loss: 0.00003468
Iteration 115/1000 | Loss: 0.00001676
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00002440
Iteration 118/1000 | Loss: 0.00003623
Iteration 119/1000 | Loss: 0.00002371
Iteration 120/1000 | Loss: 0.00003542
Iteration 121/1000 | Loss: 0.00002222
Iteration 122/1000 | Loss: 0.00003255
Iteration 123/1000 | Loss: 0.00002692
Iteration 124/1000 | Loss: 0.00003999
Iteration 125/1000 | Loss: 0.00001636
Iteration 126/1000 | Loss: 0.00001396
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001160
Iteration 131/1000 | Loss: 0.00001136
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001087
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001079
Iteration 136/1000 | Loss: 0.00001078
Iteration 137/1000 | Loss: 0.00001078
Iteration 138/1000 | Loss: 0.00001076
Iteration 139/1000 | Loss: 0.00001076
Iteration 140/1000 | Loss: 0.00001075
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001071
Iteration 145/1000 | Loss: 0.00001070
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001066
Iteration 149/1000 | Loss: 0.00001066
Iteration 150/1000 | Loss: 0.00001066
Iteration 151/1000 | Loss: 0.00001066
Iteration 152/1000 | Loss: 0.00001065
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001061
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001043
Iteration 157/1000 | Loss: 0.00001042
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001042
Iteration 163/1000 | Loss: 0.00001042
Iteration 164/1000 | Loss: 0.00001041
Iteration 165/1000 | Loss: 0.00001041
Iteration 166/1000 | Loss: 0.00001041
Iteration 167/1000 | Loss: 0.00001041
Iteration 168/1000 | Loss: 0.00001041
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001039
Iteration 172/1000 | Loss: 0.00001038
Iteration 173/1000 | Loss: 0.00001038
Iteration 174/1000 | Loss: 0.00001038
Iteration 175/1000 | Loss: 0.00001038
Iteration 176/1000 | Loss: 0.00001038
Iteration 177/1000 | Loss: 0.00001038
Iteration 178/1000 | Loss: 0.00001038
Iteration 179/1000 | Loss: 0.00001037
Iteration 180/1000 | Loss: 0.00001037
Iteration 181/1000 | Loss: 0.00001037
Iteration 182/1000 | Loss: 0.00001036
Iteration 183/1000 | Loss: 0.00001035
Iteration 184/1000 | Loss: 0.00001035
Iteration 185/1000 | Loss: 0.00001035
Iteration 186/1000 | Loss: 0.00001034
Iteration 187/1000 | Loss: 0.00001033
Iteration 188/1000 | Loss: 0.00001033
Iteration 189/1000 | Loss: 0.00001032
Iteration 190/1000 | Loss: 0.00001032
Iteration 191/1000 | Loss: 0.00001032
Iteration 192/1000 | Loss: 0.00001031
Iteration 193/1000 | Loss: 0.00001031
Iteration 194/1000 | Loss: 0.00001031
Iteration 195/1000 | Loss: 0.00001031
Iteration 196/1000 | Loss: 0.00001031
Iteration 197/1000 | Loss: 0.00001031
Iteration 198/1000 | Loss: 0.00001031
Iteration 199/1000 | Loss: 0.00001031
Iteration 200/1000 | Loss: 0.00001030
Iteration 201/1000 | Loss: 0.00001030
Iteration 202/1000 | Loss: 0.00001030
Iteration 203/1000 | Loss: 0.00001030
Iteration 204/1000 | Loss: 0.00001029
Iteration 205/1000 | Loss: 0.00001029
Iteration 206/1000 | Loss: 0.00001029
Iteration 207/1000 | Loss: 0.00001029
Iteration 208/1000 | Loss: 0.00001028
Iteration 209/1000 | Loss: 0.00001028
Iteration 210/1000 | Loss: 0.00001027
Iteration 211/1000 | Loss: 0.00001027
Iteration 212/1000 | Loss: 0.00001027
Iteration 213/1000 | Loss: 0.00001026
Iteration 214/1000 | Loss: 0.00001026
Iteration 215/1000 | Loss: 0.00001026
Iteration 216/1000 | Loss: 0.00001026
Iteration 217/1000 | Loss: 0.00001026
Iteration 218/1000 | Loss: 0.00001026
Iteration 219/1000 | Loss: 0.00001026
Iteration 220/1000 | Loss: 0.00001025
Iteration 221/1000 | Loss: 0.00001025
Iteration 222/1000 | Loss: 0.00001025
Iteration 223/1000 | Loss: 0.00001025
Iteration 224/1000 | Loss: 0.00001025
Iteration 225/1000 | Loss: 0.00001025
Iteration 226/1000 | Loss: 0.00001024
Iteration 227/1000 | Loss: 0.00001024
Iteration 228/1000 | Loss: 0.00001024
Iteration 229/1000 | Loss: 0.00001024
Iteration 230/1000 | Loss: 0.00001024
Iteration 231/1000 | Loss: 0.00001024
Iteration 232/1000 | Loss: 0.00001024
Iteration 233/1000 | Loss: 0.00001024
Iteration 234/1000 | Loss: 0.00001023
Iteration 235/1000 | Loss: 0.00001023
Iteration 236/1000 | Loss: 0.00001023
Iteration 237/1000 | Loss: 0.00001023
Iteration 238/1000 | Loss: 0.00001023
Iteration 239/1000 | Loss: 0.00001023
Iteration 240/1000 | Loss: 0.00001023
Iteration 241/1000 | Loss: 0.00001023
Iteration 242/1000 | Loss: 0.00001022
Iteration 243/1000 | Loss: 0.00001022
Iteration 244/1000 | Loss: 0.00001022
Iteration 245/1000 | Loss: 0.00001022
Iteration 246/1000 | Loss: 0.00001021
Iteration 247/1000 | Loss: 0.00001021
Iteration 248/1000 | Loss: 0.00001021
Iteration 249/1000 | Loss: 0.00001021
Iteration 250/1000 | Loss: 0.00001021
Iteration 251/1000 | Loss: 0.00001021
Iteration 252/1000 | Loss: 0.00001021
Iteration 253/1000 | Loss: 0.00001021
Iteration 254/1000 | Loss: 0.00001021
Iteration 255/1000 | Loss: 0.00001020
Iteration 256/1000 | Loss: 0.00001020
Iteration 257/1000 | Loss: 0.00001020
Iteration 258/1000 | Loss: 0.00001020
Iteration 259/1000 | Loss: 0.00001020
Iteration 260/1000 | Loss: 0.00001019
Iteration 261/1000 | Loss: 0.00001019
Iteration 262/1000 | Loss: 0.00001018
Iteration 263/1000 | Loss: 0.00001018
Iteration 264/1000 | Loss: 0.00001018
Iteration 265/1000 | Loss: 0.00001018
Iteration 266/1000 | Loss: 0.00001018
Iteration 267/1000 | Loss: 0.00001018
Iteration 268/1000 | Loss: 0.00001017
Iteration 269/1000 | Loss: 0.00001017
Iteration 270/1000 | Loss: 0.00001017
Iteration 271/1000 | Loss: 0.00001017
Iteration 272/1000 | Loss: 0.00001017
Iteration 273/1000 | Loss: 0.00001017
Iteration 274/1000 | Loss: 0.00001016
Iteration 275/1000 | Loss: 0.00001016
Iteration 276/1000 | Loss: 0.00001016
Iteration 277/1000 | Loss: 0.00001016
Iteration 278/1000 | Loss: 0.00001015
Iteration 279/1000 | Loss: 0.00001015
Iteration 280/1000 | Loss: 0.00001015
Iteration 281/1000 | Loss: 0.00001014
Iteration 282/1000 | Loss: 0.00001014
Iteration 283/1000 | Loss: 0.00001014
Iteration 284/1000 | Loss: 0.00001013
Iteration 285/1000 | Loss: 0.00001013
Iteration 286/1000 | Loss: 0.00001013
Iteration 287/1000 | Loss: 0.00001012
Iteration 288/1000 | Loss: 0.00001012
Iteration 289/1000 | Loss: 0.00001012
Iteration 290/1000 | Loss: 0.00001012
Iteration 291/1000 | Loss: 0.00001012
Iteration 292/1000 | Loss: 0.00001012
Iteration 293/1000 | Loss: 0.00001012
Iteration 294/1000 | Loss: 0.00001012
Iteration 295/1000 | Loss: 0.00001012
Iteration 296/1000 | Loss: 0.00001011
Iteration 297/1000 | Loss: 0.00001011
Iteration 298/1000 | Loss: 0.00001011
Iteration 299/1000 | Loss: 0.00001011
Iteration 300/1000 | Loss: 0.00001011
Iteration 301/1000 | Loss: 0.00001011
Iteration 302/1000 | Loss: 0.00001011
Iteration 303/1000 | Loss: 0.00001011
Iteration 304/1000 | Loss: 0.00001011
Iteration 305/1000 | Loss: 0.00001011
Iteration 306/1000 | Loss: 0.00001010
Iteration 307/1000 | Loss: 0.00001010
Iteration 308/1000 | Loss: 0.00001010
Iteration 309/1000 | Loss: 0.00001010
Iteration 310/1000 | Loss: 0.00001010
Iteration 311/1000 | Loss: 0.00001010
Iteration 312/1000 | Loss: 0.00001010
Iteration 313/1000 | Loss: 0.00001010
Iteration 314/1000 | Loss: 0.00001010
Iteration 315/1000 | Loss: 0.00001010
Iteration 316/1000 | Loss: 0.00001010
Iteration 317/1000 | Loss: 0.00001010
Iteration 318/1000 | Loss: 0.00001010
Iteration 319/1000 | Loss: 0.00001010
Iteration 320/1000 | Loss: 0.00001009
Iteration 321/1000 | Loss: 0.00001009
Iteration 322/1000 | Loss: 0.00001009
Iteration 323/1000 | Loss: 0.00001009
Iteration 324/1000 | Loss: 0.00001009
Iteration 325/1000 | Loss: 0.00001009
Iteration 326/1000 | Loss: 0.00001009
Iteration 327/1000 | Loss: 0.00001008
Iteration 328/1000 | Loss: 0.00001008
Iteration 329/1000 | Loss: 0.00001008
Iteration 330/1000 | Loss: 0.00001008
Iteration 331/1000 | Loss: 0.00001008
Iteration 332/1000 | Loss: 0.00001008
Iteration 333/1000 | Loss: 0.00001008
Iteration 334/1000 | Loss: 0.00001008
Iteration 335/1000 | Loss: 0.00001008
Iteration 336/1000 | Loss: 0.00001008
Iteration 337/1000 | Loss: 0.00001008
Iteration 338/1000 | Loss: 0.00001008
Iteration 339/1000 | Loss: 0.00001008
Iteration 340/1000 | Loss: 0.00001008
Iteration 341/1000 | Loss: 0.00001008
Iteration 342/1000 | Loss: 0.00001008
Iteration 343/1000 | Loss: 0.00001008
Iteration 344/1000 | Loss: 0.00001008
Iteration 345/1000 | Loss: 0.00001008
Iteration 346/1000 | Loss: 0.00001008
Iteration 347/1000 | Loss: 0.00001008
Iteration 348/1000 | Loss: 0.00001008
Iteration 349/1000 | Loss: 0.00001008
Iteration 350/1000 | Loss: 0.00001008
Iteration 351/1000 | Loss: 0.00001008
Iteration 352/1000 | Loss: 0.00001008
Iteration 353/1000 | Loss: 0.00001008
Iteration 354/1000 | Loss: 0.00001008
Iteration 355/1000 | Loss: 0.00001008
Iteration 356/1000 | Loss: 0.00001008
Iteration 357/1000 | Loss: 0.00001008
Iteration 358/1000 | Loss: 0.00001008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 358. Stopping optimization.
Last 5 losses: [1.0083443157782312e-05, 1.0083443157782312e-05, 1.0083443157782312e-05, 1.0083443157782312e-05, 1.0083443157782312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0083443157782312e-05

Optimization complete. Final v2v error: 2.7152607440948486 mm

Highest mean error: 3.5408246517181396 mm for frame 81

Lowest mean error: 2.3725998401641846 mm for frame 159

Saving results

Total time: 253.64455008506775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389727
Iteration 2/25 | Loss: 0.00103683
Iteration 3/25 | Loss: 0.00097323
Iteration 4/25 | Loss: 0.00096308
Iteration 5/25 | Loss: 0.00095960
Iteration 6/25 | Loss: 0.00095858
Iteration 7/25 | Loss: 0.00095858
Iteration 8/25 | Loss: 0.00095858
Iteration 9/25 | Loss: 0.00095858
Iteration 10/25 | Loss: 0.00095858
Iteration 11/25 | Loss: 0.00095858
Iteration 12/25 | Loss: 0.00095858
Iteration 13/25 | Loss: 0.00095858
Iteration 14/25 | Loss: 0.00095858
Iteration 15/25 | Loss: 0.00095858
Iteration 16/25 | Loss: 0.00095858
Iteration 17/25 | Loss: 0.00095858
Iteration 18/25 | Loss: 0.00095858
Iteration 19/25 | Loss: 0.00095858
Iteration 20/25 | Loss: 0.00095858
Iteration 21/25 | Loss: 0.00095858
Iteration 22/25 | Loss: 0.00095858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009585772058926523, 0.0009585772058926523, 0.0009585772058926523, 0.0009585772058926523, 0.0009585772058926523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009585772058926523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84252906
Iteration 2/25 | Loss: 0.00099886
Iteration 3/25 | Loss: 0.00099886
Iteration 4/25 | Loss: 0.00099886
Iteration 5/25 | Loss: 0.00099886
Iteration 6/25 | Loss: 0.00099886
Iteration 7/25 | Loss: 0.00099886
Iteration 8/25 | Loss: 0.00099886
Iteration 9/25 | Loss: 0.00099886
Iteration 10/25 | Loss: 0.00099886
Iteration 11/25 | Loss: 0.00099886
Iteration 12/25 | Loss: 0.00099886
Iteration 13/25 | Loss: 0.00099886
Iteration 14/25 | Loss: 0.00099886
Iteration 15/25 | Loss: 0.00099886
Iteration 16/25 | Loss: 0.00099886
Iteration 17/25 | Loss: 0.00099886
Iteration 18/25 | Loss: 0.00099886
Iteration 19/25 | Loss: 0.00099886
Iteration 20/25 | Loss: 0.00099886
Iteration 21/25 | Loss: 0.00099886
Iteration 22/25 | Loss: 0.00099886
Iteration 23/25 | Loss: 0.00099886
Iteration 24/25 | Loss: 0.00099886
Iteration 25/25 | Loss: 0.00099886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099886
Iteration 2/1000 | Loss: 0.00002421
Iteration 3/1000 | Loss: 0.00001376
Iteration 4/1000 | Loss: 0.00001156
Iteration 5/1000 | Loss: 0.00001074
Iteration 6/1000 | Loss: 0.00001032
Iteration 7/1000 | Loss: 0.00000991
Iteration 8/1000 | Loss: 0.00000991
Iteration 9/1000 | Loss: 0.00000968
Iteration 10/1000 | Loss: 0.00000967
Iteration 11/1000 | Loss: 0.00000958
Iteration 12/1000 | Loss: 0.00000952
Iteration 13/1000 | Loss: 0.00000951
Iteration 14/1000 | Loss: 0.00000951
Iteration 15/1000 | Loss: 0.00000945
Iteration 16/1000 | Loss: 0.00000938
Iteration 17/1000 | Loss: 0.00000937
Iteration 18/1000 | Loss: 0.00000936
Iteration 19/1000 | Loss: 0.00000936
Iteration 20/1000 | Loss: 0.00000936
Iteration 21/1000 | Loss: 0.00000935
Iteration 22/1000 | Loss: 0.00000935
Iteration 23/1000 | Loss: 0.00000935
Iteration 24/1000 | Loss: 0.00000934
Iteration 25/1000 | Loss: 0.00000934
Iteration 26/1000 | Loss: 0.00000934
Iteration 27/1000 | Loss: 0.00000934
Iteration 28/1000 | Loss: 0.00000934
Iteration 29/1000 | Loss: 0.00000933
Iteration 30/1000 | Loss: 0.00000933
Iteration 31/1000 | Loss: 0.00000932
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000932
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000931
Iteration 36/1000 | Loss: 0.00000931
Iteration 37/1000 | Loss: 0.00000931
Iteration 38/1000 | Loss: 0.00000931
Iteration 39/1000 | Loss: 0.00000930
Iteration 40/1000 | Loss: 0.00000930
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000930
Iteration 43/1000 | Loss: 0.00000930
Iteration 44/1000 | Loss: 0.00000930
Iteration 45/1000 | Loss: 0.00000930
Iteration 46/1000 | Loss: 0.00000930
Iteration 47/1000 | Loss: 0.00000930
Iteration 48/1000 | Loss: 0.00000930
Iteration 49/1000 | Loss: 0.00000930
Iteration 50/1000 | Loss: 0.00000930
Iteration 51/1000 | Loss: 0.00000930
Iteration 52/1000 | Loss: 0.00000930
Iteration 53/1000 | Loss: 0.00000930
Iteration 54/1000 | Loss: 0.00000930
Iteration 55/1000 | Loss: 0.00000930
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000930
Iteration 58/1000 | Loss: 0.00000930
Iteration 59/1000 | Loss: 0.00000930
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000930
Iteration 62/1000 | Loss: 0.00000930
Iteration 63/1000 | Loss: 0.00000930
Iteration 64/1000 | Loss: 0.00000930
Iteration 65/1000 | Loss: 0.00000930
Iteration 66/1000 | Loss: 0.00000930
Iteration 67/1000 | Loss: 0.00000930
Iteration 68/1000 | Loss: 0.00000930
Iteration 69/1000 | Loss: 0.00000930
Iteration 70/1000 | Loss: 0.00000930
Iteration 71/1000 | Loss: 0.00000930
Iteration 72/1000 | Loss: 0.00000930
Iteration 73/1000 | Loss: 0.00000930
Iteration 74/1000 | Loss: 0.00000930
Iteration 75/1000 | Loss: 0.00000930
Iteration 76/1000 | Loss: 0.00000930
Iteration 77/1000 | Loss: 0.00000930
Iteration 78/1000 | Loss: 0.00000930
Iteration 79/1000 | Loss: 0.00000930
Iteration 80/1000 | Loss: 0.00000930
Iteration 81/1000 | Loss: 0.00000930
Iteration 82/1000 | Loss: 0.00000930
Iteration 83/1000 | Loss: 0.00000930
Iteration 84/1000 | Loss: 0.00000930
Iteration 85/1000 | Loss: 0.00000930
Iteration 86/1000 | Loss: 0.00000930
Iteration 87/1000 | Loss: 0.00000930
Iteration 88/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [9.296645657741465e-06, 9.296645657741465e-06, 9.296645657741465e-06, 9.296645657741465e-06, 9.296645657741465e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.296645657741465e-06

Optimization complete. Final v2v error: 2.5928597450256348 mm

Highest mean error: 3.1672890186309814 mm for frame 59

Lowest mean error: 2.2149295806884766 mm for frame 4

Saving results

Total time: 26.974134922027588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_us_1591/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_us_1591/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368711
Iteration 2/25 | Loss: 0.00108549
Iteration 3/25 | Loss: 0.00102094
Iteration 4/25 | Loss: 0.00101729
Iteration 5/25 | Loss: 0.00101632
Iteration 6/25 | Loss: 0.00101629
Iteration 7/25 | Loss: 0.00101629
Iteration 8/25 | Loss: 0.00101629
Iteration 9/25 | Loss: 0.00101629
Iteration 10/25 | Loss: 0.00101629
Iteration 11/25 | Loss: 0.00101629
Iteration 12/25 | Loss: 0.00101629
Iteration 13/25 | Loss: 0.00101629
Iteration 14/25 | Loss: 0.00101628
Iteration 15/25 | Loss: 0.00101628
Iteration 16/25 | Loss: 0.00101628
Iteration 17/25 | Loss: 0.00101628
Iteration 18/25 | Loss: 0.00101628
Iteration 19/25 | Loss: 0.00101628
Iteration 20/25 | Loss: 0.00101628
Iteration 21/25 | Loss: 0.00101628
Iteration 22/25 | Loss: 0.00101628
Iteration 23/25 | Loss: 0.00101628
Iteration 24/25 | Loss: 0.00101628
Iteration 25/25 | Loss: 0.00101628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35995078
Iteration 2/25 | Loss: 0.00099912
Iteration 3/25 | Loss: 0.00099912
Iteration 4/25 | Loss: 0.00099912
Iteration 5/25 | Loss: 0.00099912
Iteration 6/25 | Loss: 0.00099912
Iteration 7/25 | Loss: 0.00099912
Iteration 8/25 | Loss: 0.00099912
Iteration 9/25 | Loss: 0.00099912
Iteration 10/25 | Loss: 0.00099912
Iteration 11/25 | Loss: 0.00099912
Iteration 12/25 | Loss: 0.00099912
Iteration 13/25 | Loss: 0.00099912
Iteration 14/25 | Loss: 0.00099912
Iteration 15/25 | Loss: 0.00099912
Iteration 16/25 | Loss: 0.00099912
Iteration 17/25 | Loss: 0.00099912
Iteration 18/25 | Loss: 0.00099912
Iteration 19/25 | Loss: 0.00099912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009991165716201067, 0.0009991165716201067, 0.0009991165716201067, 0.0009991165716201067, 0.0009991165716201067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009991165716201067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099912
Iteration 2/1000 | Loss: 0.00002393
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001168
Iteration 13/1000 | Loss: 0.00001162
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001156
Iteration 22/1000 | Loss: 0.00001156
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001156
Iteration 25/1000 | Loss: 0.00001155
Iteration 26/1000 | Loss: 0.00001155
Iteration 27/1000 | Loss: 0.00001155
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001154
Iteration 30/1000 | Loss: 0.00001153
Iteration 31/1000 | Loss: 0.00001153
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001150
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001149
Iteration 38/1000 | Loss: 0.00001149
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001148
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001145
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001145
Iteration 60/1000 | Loss: 0.00001145
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001145
Iteration 63/1000 | Loss: 0.00001145
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001145
Iteration 81/1000 | Loss: 0.00001145
Iteration 82/1000 | Loss: 0.00001145
Iteration 83/1000 | Loss: 0.00001145
Iteration 84/1000 | Loss: 0.00001145
Iteration 85/1000 | Loss: 0.00001145
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00001145
Iteration 88/1000 | Loss: 0.00001145
Iteration 89/1000 | Loss: 0.00001145
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.1450192687334493e-05, 1.1450192687334493e-05, 1.1450192687334493e-05, 1.1450192687334493e-05, 1.1450192687334493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1450192687334493e-05

Optimization complete. Final v2v error: 2.92048978805542 mm

Highest mean error: 3.2037510871887207 mm for frame 55

Lowest mean error: 2.421055555343628 mm for frame 149

Saving results

Total time: 27.489821434020996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424702
Iteration 2/25 | Loss: 0.00124561
Iteration 3/25 | Loss: 0.00117033
Iteration 4/25 | Loss: 0.00115940
Iteration 5/25 | Loss: 0.00115575
Iteration 6/25 | Loss: 0.00115513
Iteration 7/25 | Loss: 0.00115513
Iteration 8/25 | Loss: 0.00115513
Iteration 9/25 | Loss: 0.00115513
Iteration 10/25 | Loss: 0.00115513
Iteration 11/25 | Loss: 0.00115513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011551320785656571, 0.0011551320785656571, 0.0011551320785656571, 0.0011551320785656571, 0.0011551320785656571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011551320785656571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.29179192
Iteration 2/25 | Loss: 0.00134646
Iteration 3/25 | Loss: 0.00134646
Iteration 4/25 | Loss: 0.00134646
Iteration 5/25 | Loss: 0.00134646
Iteration 6/25 | Loss: 0.00134646
Iteration 7/25 | Loss: 0.00134646
Iteration 8/25 | Loss: 0.00134646
Iteration 9/25 | Loss: 0.00134646
Iteration 10/25 | Loss: 0.00134646
Iteration 11/25 | Loss: 0.00134646
Iteration 12/25 | Loss: 0.00134646
Iteration 13/25 | Loss: 0.00134646
Iteration 14/25 | Loss: 0.00134646
Iteration 15/25 | Loss: 0.00134646
Iteration 16/25 | Loss: 0.00134646
Iteration 17/25 | Loss: 0.00134646
Iteration 18/25 | Loss: 0.00134646
Iteration 19/25 | Loss: 0.00134646
Iteration 20/25 | Loss: 0.00134646
Iteration 21/25 | Loss: 0.00134646
Iteration 22/25 | Loss: 0.00134646
Iteration 23/25 | Loss: 0.00134646
Iteration 24/25 | Loss: 0.00134646
Iteration 25/25 | Loss: 0.00134646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134646
Iteration 2/1000 | Loss: 0.00003134
Iteration 3/1000 | Loss: 0.00002177
Iteration 4/1000 | Loss: 0.00001889
Iteration 5/1000 | Loss: 0.00001767
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001622
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001555
Iteration 11/1000 | Loss: 0.00001539
Iteration 12/1000 | Loss: 0.00001522
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001500
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001498
Iteration 23/1000 | Loss: 0.00001498
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001491
Iteration 36/1000 | Loss: 0.00001490
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001489
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001488
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001487
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001486
Iteration 50/1000 | Loss: 0.00001486
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001485
Iteration 53/1000 | Loss: 0.00001485
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001483
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00001482
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001479
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001478
Iteration 83/1000 | Loss: 0.00001478
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001477
Iteration 100/1000 | Loss: 0.00001477
Iteration 101/1000 | Loss: 0.00001477
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001477
Iteration 105/1000 | Loss: 0.00001476
Iteration 106/1000 | Loss: 0.00001476
Iteration 107/1000 | Loss: 0.00001476
Iteration 108/1000 | Loss: 0.00001476
Iteration 109/1000 | Loss: 0.00001476
Iteration 110/1000 | Loss: 0.00001476
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001476
Iteration 117/1000 | Loss: 0.00001476
Iteration 118/1000 | Loss: 0.00001476
Iteration 119/1000 | Loss: 0.00001476
Iteration 120/1000 | Loss: 0.00001476
Iteration 121/1000 | Loss: 0.00001476
Iteration 122/1000 | Loss: 0.00001476
Iteration 123/1000 | Loss: 0.00001476
Iteration 124/1000 | Loss: 0.00001476
Iteration 125/1000 | Loss: 0.00001476
Iteration 126/1000 | Loss: 0.00001475
Iteration 127/1000 | Loss: 0.00001475
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001475
Iteration 133/1000 | Loss: 0.00001475
Iteration 134/1000 | Loss: 0.00001475
Iteration 135/1000 | Loss: 0.00001475
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001475
Iteration 139/1000 | Loss: 0.00001475
Iteration 140/1000 | Loss: 0.00001475
Iteration 141/1000 | Loss: 0.00001475
Iteration 142/1000 | Loss: 0.00001475
Iteration 143/1000 | Loss: 0.00001475
Iteration 144/1000 | Loss: 0.00001475
Iteration 145/1000 | Loss: 0.00001475
Iteration 146/1000 | Loss: 0.00001475
Iteration 147/1000 | Loss: 0.00001475
Iteration 148/1000 | Loss: 0.00001475
Iteration 149/1000 | Loss: 0.00001475
Iteration 150/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.4748755347682163e-05, 1.4748755347682163e-05, 1.4748755347682163e-05, 1.4748755347682163e-05, 1.4748755347682163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4748755347682163e-05

Optimization complete. Final v2v error: 3.3854775428771973 mm

Highest mean error: 3.9041030406951904 mm for frame 56

Lowest mean error: 3.215639114379883 mm for frame 45

Saving results

Total time: 34.68133354187012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00633008
Iteration 2/25 | Loss: 0.00124839
Iteration 3/25 | Loss: 0.00116424
Iteration 4/25 | Loss: 0.00115884
Iteration 5/25 | Loss: 0.00115755
Iteration 6/25 | Loss: 0.00115755
Iteration 7/25 | Loss: 0.00115755
Iteration 8/25 | Loss: 0.00115755
Iteration 9/25 | Loss: 0.00115755
Iteration 10/25 | Loss: 0.00115755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011575522366911173, 0.0011575522366911173, 0.0011575522366911173, 0.0011575522366911173, 0.0011575522366911173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011575522366911173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90905809
Iteration 2/25 | Loss: 0.00136157
Iteration 3/25 | Loss: 0.00136157
Iteration 4/25 | Loss: 0.00136157
Iteration 5/25 | Loss: 0.00136157
Iteration 6/25 | Loss: 0.00136157
Iteration 7/25 | Loss: 0.00136157
Iteration 8/25 | Loss: 0.00136157
Iteration 9/25 | Loss: 0.00136157
Iteration 10/25 | Loss: 0.00136157
Iteration 11/25 | Loss: 0.00136157
Iteration 12/25 | Loss: 0.00136157
Iteration 13/25 | Loss: 0.00136157
Iteration 14/25 | Loss: 0.00136157
Iteration 15/25 | Loss: 0.00136157
Iteration 16/25 | Loss: 0.00136157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001361569156870246, 0.001361569156870246, 0.001361569156870246, 0.001361569156870246, 0.001361569156870246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001361569156870246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136157
Iteration 2/1000 | Loss: 0.00003195
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001943
Iteration 6/1000 | Loss: 0.00001816
Iteration 7/1000 | Loss: 0.00001750
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001682
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001652
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001641
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001627
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001614
Iteration 26/1000 | Loss: 0.00001614
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001611
Iteration 30/1000 | Loss: 0.00001608
Iteration 31/1000 | Loss: 0.00001608
Iteration 32/1000 | Loss: 0.00001607
Iteration 33/1000 | Loss: 0.00001607
Iteration 34/1000 | Loss: 0.00001606
Iteration 35/1000 | Loss: 0.00001606
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001603
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00001602
Iteration 41/1000 | Loss: 0.00001601
Iteration 42/1000 | Loss: 0.00001600
Iteration 43/1000 | Loss: 0.00001600
Iteration 44/1000 | Loss: 0.00001600
Iteration 45/1000 | Loss: 0.00001599
Iteration 46/1000 | Loss: 0.00001599
Iteration 47/1000 | Loss: 0.00001599
Iteration 48/1000 | Loss: 0.00001599
Iteration 49/1000 | Loss: 0.00001599
Iteration 50/1000 | Loss: 0.00001599
Iteration 51/1000 | Loss: 0.00001599
Iteration 52/1000 | Loss: 0.00001599
Iteration 53/1000 | Loss: 0.00001599
Iteration 54/1000 | Loss: 0.00001598
Iteration 55/1000 | Loss: 0.00001598
Iteration 56/1000 | Loss: 0.00001598
Iteration 57/1000 | Loss: 0.00001598
Iteration 58/1000 | Loss: 0.00001598
Iteration 59/1000 | Loss: 0.00001597
Iteration 60/1000 | Loss: 0.00001597
Iteration 61/1000 | Loss: 0.00001597
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001596
Iteration 77/1000 | Loss: 0.00001596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.596457695995923e-05, 1.596457695995923e-05, 1.596457695995923e-05, 1.596457695995923e-05, 1.596457695995923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.596457695995923e-05

Optimization complete. Final v2v error: 3.4921178817749023 mm

Highest mean error: 4.239687919616699 mm for frame 128

Lowest mean error: 2.9963669776916504 mm for frame 150

Saving results

Total time: 31.558937549591064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474623
Iteration 2/25 | Loss: 0.00124605
Iteration 3/25 | Loss: 0.00116700
Iteration 4/25 | Loss: 0.00115651
Iteration 5/25 | Loss: 0.00115347
Iteration 6/25 | Loss: 0.00115297
Iteration 7/25 | Loss: 0.00115297
Iteration 8/25 | Loss: 0.00115297
Iteration 9/25 | Loss: 0.00115297
Iteration 10/25 | Loss: 0.00115297
Iteration 11/25 | Loss: 0.00115297
Iteration 12/25 | Loss: 0.00115297
Iteration 13/25 | Loss: 0.00115297
Iteration 14/25 | Loss: 0.00115297
Iteration 15/25 | Loss: 0.00115297
Iteration 16/25 | Loss: 0.00115297
Iteration 17/25 | Loss: 0.00115297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011529704788699746, 0.0011529704788699746, 0.0011529704788699746, 0.0011529704788699746, 0.0011529704788699746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011529704788699746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20884180
Iteration 2/25 | Loss: 0.00134462
Iteration 3/25 | Loss: 0.00134461
Iteration 4/25 | Loss: 0.00134461
Iteration 5/25 | Loss: 0.00134461
Iteration 6/25 | Loss: 0.00134461
Iteration 7/25 | Loss: 0.00134461
Iteration 8/25 | Loss: 0.00134461
Iteration 9/25 | Loss: 0.00134461
Iteration 10/25 | Loss: 0.00134461
Iteration 11/25 | Loss: 0.00134461
Iteration 12/25 | Loss: 0.00134461
Iteration 13/25 | Loss: 0.00134461
Iteration 14/25 | Loss: 0.00134461
Iteration 15/25 | Loss: 0.00134461
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001344611868262291, 0.001344611868262291, 0.001344611868262291, 0.001344611868262291, 0.001344611868262291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001344611868262291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134461
Iteration 2/1000 | Loss: 0.00003786
Iteration 3/1000 | Loss: 0.00002490
Iteration 4/1000 | Loss: 0.00002164
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001782
Iteration 10/1000 | Loss: 0.00001763
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001751
Iteration 14/1000 | Loss: 0.00001749
Iteration 15/1000 | Loss: 0.00001748
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001737
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001735
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00001733
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001731
Iteration 36/1000 | Loss: 0.00001731
Iteration 37/1000 | Loss: 0.00001731
Iteration 38/1000 | Loss: 0.00001731
Iteration 39/1000 | Loss: 0.00001731
Iteration 40/1000 | Loss: 0.00001730
Iteration 41/1000 | Loss: 0.00001730
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001730
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001729
Iteration 46/1000 | Loss: 0.00001729
Iteration 47/1000 | Loss: 0.00001729
Iteration 48/1000 | Loss: 0.00001728
Iteration 49/1000 | Loss: 0.00001727
Iteration 50/1000 | Loss: 0.00001727
Iteration 51/1000 | Loss: 0.00001727
Iteration 52/1000 | Loss: 0.00001727
Iteration 53/1000 | Loss: 0.00001727
Iteration 54/1000 | Loss: 0.00001727
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001726
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001726
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001725
Iteration 71/1000 | Loss: 0.00001724
Iteration 72/1000 | Loss: 0.00001724
Iteration 73/1000 | Loss: 0.00001724
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001723
Iteration 84/1000 | Loss: 0.00001723
Iteration 85/1000 | Loss: 0.00001723
Iteration 86/1000 | Loss: 0.00001723
Iteration 87/1000 | Loss: 0.00001723
Iteration 88/1000 | Loss: 0.00001723
Iteration 89/1000 | Loss: 0.00001723
Iteration 90/1000 | Loss: 0.00001723
Iteration 91/1000 | Loss: 0.00001722
Iteration 92/1000 | Loss: 0.00001722
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001722
Iteration 96/1000 | Loss: 0.00001722
Iteration 97/1000 | Loss: 0.00001722
Iteration 98/1000 | Loss: 0.00001722
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001722
Iteration 101/1000 | Loss: 0.00001722
Iteration 102/1000 | Loss: 0.00001721
Iteration 103/1000 | Loss: 0.00001721
Iteration 104/1000 | Loss: 0.00001721
Iteration 105/1000 | Loss: 0.00001721
Iteration 106/1000 | Loss: 0.00001721
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001720
Iteration 109/1000 | Loss: 0.00001720
Iteration 110/1000 | Loss: 0.00001720
Iteration 111/1000 | Loss: 0.00001720
Iteration 112/1000 | Loss: 0.00001720
Iteration 113/1000 | Loss: 0.00001720
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001719
Iteration 120/1000 | Loss: 0.00001719
Iteration 121/1000 | Loss: 0.00001719
Iteration 122/1000 | Loss: 0.00001719
Iteration 123/1000 | Loss: 0.00001719
Iteration 124/1000 | Loss: 0.00001719
Iteration 125/1000 | Loss: 0.00001718
Iteration 126/1000 | Loss: 0.00001718
Iteration 127/1000 | Loss: 0.00001718
Iteration 128/1000 | Loss: 0.00001718
Iteration 129/1000 | Loss: 0.00001718
Iteration 130/1000 | Loss: 0.00001718
Iteration 131/1000 | Loss: 0.00001718
Iteration 132/1000 | Loss: 0.00001718
Iteration 133/1000 | Loss: 0.00001718
Iteration 134/1000 | Loss: 0.00001718
Iteration 135/1000 | Loss: 0.00001718
Iteration 136/1000 | Loss: 0.00001718
Iteration 137/1000 | Loss: 0.00001717
Iteration 138/1000 | Loss: 0.00001717
Iteration 139/1000 | Loss: 0.00001717
Iteration 140/1000 | Loss: 0.00001717
Iteration 141/1000 | Loss: 0.00001717
Iteration 142/1000 | Loss: 0.00001717
Iteration 143/1000 | Loss: 0.00001717
Iteration 144/1000 | Loss: 0.00001717
Iteration 145/1000 | Loss: 0.00001717
Iteration 146/1000 | Loss: 0.00001717
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Iteration 164/1000 | Loss: 0.00001716
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001716
Iteration 167/1000 | Loss: 0.00001716
Iteration 168/1000 | Loss: 0.00001716
Iteration 169/1000 | Loss: 0.00001716
Iteration 170/1000 | Loss: 0.00001716
Iteration 171/1000 | Loss: 0.00001716
Iteration 172/1000 | Loss: 0.00001716
Iteration 173/1000 | Loss: 0.00001716
Iteration 174/1000 | Loss: 0.00001716
Iteration 175/1000 | Loss: 0.00001716
Iteration 176/1000 | Loss: 0.00001716
Iteration 177/1000 | Loss: 0.00001716
Iteration 178/1000 | Loss: 0.00001716
Iteration 179/1000 | Loss: 0.00001716
Iteration 180/1000 | Loss: 0.00001716
Iteration 181/1000 | Loss: 0.00001716
Iteration 182/1000 | Loss: 0.00001716
Iteration 183/1000 | Loss: 0.00001716
Iteration 184/1000 | Loss: 0.00001716
Iteration 185/1000 | Loss: 0.00001716
Iteration 186/1000 | Loss: 0.00001716
Iteration 187/1000 | Loss: 0.00001716
Iteration 188/1000 | Loss: 0.00001716
Iteration 189/1000 | Loss: 0.00001716
Iteration 190/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.7159460185212083e-05, 1.7159460185212083e-05, 1.7159460185212083e-05, 1.7159460185212083e-05, 1.7159460185212083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7159460185212083e-05

Optimization complete. Final v2v error: 3.4931282997131348 mm

Highest mean error: 4.1885528564453125 mm for frame 13

Lowest mean error: 3.207145929336548 mm for frame 56

Saving results

Total time: 35.09988212585449
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109927
Iteration 2/25 | Loss: 0.01109927
Iteration 3/25 | Loss: 0.01109927
Iteration 4/25 | Loss: 0.01109927
Iteration 5/25 | Loss: 0.01109927
Iteration 6/25 | Loss: 0.01109927
Iteration 7/25 | Loss: 0.01109927
Iteration 8/25 | Loss: 0.01109927
Iteration 9/25 | Loss: 0.01109927
Iteration 10/25 | Loss: 0.01109927
Iteration 11/25 | Loss: 0.01109926
Iteration 12/25 | Loss: 0.01109926
Iteration 13/25 | Loss: 0.01109926
Iteration 14/25 | Loss: 0.01109926
Iteration 15/25 | Loss: 0.01109926
Iteration 16/25 | Loss: 0.01109926
Iteration 17/25 | Loss: 0.01109926
Iteration 18/25 | Loss: 0.01109926
Iteration 19/25 | Loss: 0.01109926
Iteration 20/25 | Loss: 0.01109926
Iteration 21/25 | Loss: 0.01109926
Iteration 22/25 | Loss: 0.01109926
Iteration 23/25 | Loss: 0.01109926
Iteration 24/25 | Loss: 0.01109926
Iteration 25/25 | Loss: 0.01109926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71036589
Iteration 2/25 | Loss: 0.08125618
Iteration 3/25 | Loss: 0.08119943
Iteration 4/25 | Loss: 0.08096427
Iteration 5/25 | Loss: 0.08096427
Iteration 6/25 | Loss: 0.08096426
Iteration 7/25 | Loss: 0.08096426
Iteration 8/25 | Loss: 0.08096426
Iteration 9/25 | Loss: 0.08096426
Iteration 10/25 | Loss: 0.08096426
Iteration 11/25 | Loss: 0.08096426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.08096425980329514, 0.08096425980329514, 0.08096425980329514, 0.08096425980329514, 0.08096425980329514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08096425980329514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08096426
Iteration 2/1000 | Loss: 0.01094702
Iteration 3/1000 | Loss: 0.00326047
Iteration 4/1000 | Loss: 0.00158863
Iteration 5/1000 | Loss: 0.00294408
Iteration 6/1000 | Loss: 0.00053059
Iteration 7/1000 | Loss: 0.00137748
Iteration 8/1000 | Loss: 0.00440292
Iteration 9/1000 | Loss: 0.00313955
Iteration 10/1000 | Loss: 0.00087293
Iteration 11/1000 | Loss: 0.00013227
Iteration 12/1000 | Loss: 0.00073716
Iteration 13/1000 | Loss: 0.00008279
Iteration 14/1000 | Loss: 0.00006663
Iteration 15/1000 | Loss: 0.00018254
Iteration 16/1000 | Loss: 0.00007245
Iteration 17/1000 | Loss: 0.00004735
Iteration 18/1000 | Loss: 0.00004290
Iteration 19/1000 | Loss: 0.00003937
Iteration 20/1000 | Loss: 0.00034409
Iteration 21/1000 | Loss: 0.00003618
Iteration 22/1000 | Loss: 0.00003345
Iteration 23/1000 | Loss: 0.00003178
Iteration 24/1000 | Loss: 0.00006756
Iteration 25/1000 | Loss: 0.00003707
Iteration 26/1000 | Loss: 0.00015828
Iteration 27/1000 | Loss: 0.00037866
Iteration 28/1000 | Loss: 0.00003896
Iteration 29/1000 | Loss: 0.00003179
Iteration 30/1000 | Loss: 0.00002855
Iteration 31/1000 | Loss: 0.00017244
Iteration 32/1000 | Loss: 0.00003032
Iteration 33/1000 | Loss: 0.00003585
Iteration 34/1000 | Loss: 0.00002691
Iteration 35/1000 | Loss: 0.00005670
Iteration 36/1000 | Loss: 0.00002486
Iteration 37/1000 | Loss: 0.00002425
Iteration 38/1000 | Loss: 0.00002384
Iteration 39/1000 | Loss: 0.00007070
Iteration 40/1000 | Loss: 0.00002373
Iteration 41/1000 | Loss: 0.00006580
Iteration 42/1000 | Loss: 0.00012389
Iteration 43/1000 | Loss: 0.00006631
Iteration 44/1000 | Loss: 0.00009322
Iteration 45/1000 | Loss: 0.00004940
Iteration 46/1000 | Loss: 0.00004799
Iteration 47/1000 | Loss: 0.00002679
Iteration 48/1000 | Loss: 0.00007476
Iteration 49/1000 | Loss: 0.00009361
Iteration 50/1000 | Loss: 0.00002584
Iteration 51/1000 | Loss: 0.00003756
Iteration 52/1000 | Loss: 0.00005860
Iteration 53/1000 | Loss: 0.00007480
Iteration 54/1000 | Loss: 0.00006629
Iteration 55/1000 | Loss: 0.00006227
Iteration 56/1000 | Loss: 0.00005399
Iteration 57/1000 | Loss: 0.00008205
Iteration 58/1000 | Loss: 0.00006877
Iteration 59/1000 | Loss: 0.00002832
Iteration 60/1000 | Loss: 0.00003013
Iteration 61/1000 | Loss: 0.00002396
Iteration 62/1000 | Loss: 0.00002608
Iteration 63/1000 | Loss: 0.00002340
Iteration 64/1000 | Loss: 0.00002203
Iteration 65/1000 | Loss: 0.00002209
Iteration 66/1000 | Loss: 0.00003575
Iteration 67/1000 | Loss: 0.00002443
Iteration 68/1000 | Loss: 0.00003965
Iteration 69/1000 | Loss: 0.00002286
Iteration 70/1000 | Loss: 0.00002101
Iteration 71/1000 | Loss: 0.00002113
Iteration 72/1000 | Loss: 0.00002074
Iteration 73/1000 | Loss: 0.00002034
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00002012
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001993
Iteration 78/1000 | Loss: 0.00001993
Iteration 79/1000 | Loss: 0.00001992
Iteration 80/1000 | Loss: 0.00001992
Iteration 81/1000 | Loss: 0.00001992
Iteration 82/1000 | Loss: 0.00001991
Iteration 83/1000 | Loss: 0.00001991
Iteration 84/1000 | Loss: 0.00001999
Iteration 85/1000 | Loss: 0.00001998
Iteration 86/1000 | Loss: 0.00001998
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001987
Iteration 89/1000 | Loss: 0.00001987
Iteration 90/1000 | Loss: 0.00001987
Iteration 91/1000 | Loss: 0.00001987
Iteration 92/1000 | Loss: 0.00001987
Iteration 93/1000 | Loss: 0.00001986
Iteration 94/1000 | Loss: 0.00001986
Iteration 95/1000 | Loss: 0.00001986
Iteration 96/1000 | Loss: 0.00001986
Iteration 97/1000 | Loss: 0.00001985
Iteration 98/1000 | Loss: 0.00005413
Iteration 99/1000 | Loss: 0.00001999
Iteration 100/1000 | Loss: 0.00001992
Iteration 101/1000 | Loss: 0.00001979
Iteration 102/1000 | Loss: 0.00001979
Iteration 103/1000 | Loss: 0.00001979
Iteration 104/1000 | Loss: 0.00001979
Iteration 105/1000 | Loss: 0.00001979
Iteration 106/1000 | Loss: 0.00001979
Iteration 107/1000 | Loss: 0.00001978
Iteration 108/1000 | Loss: 0.00001978
Iteration 109/1000 | Loss: 0.00001978
Iteration 110/1000 | Loss: 0.00001978
Iteration 111/1000 | Loss: 0.00001977
Iteration 112/1000 | Loss: 0.00001977
Iteration 113/1000 | Loss: 0.00001977
Iteration 114/1000 | Loss: 0.00001977
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001976
Iteration 118/1000 | Loss: 0.00001976
Iteration 119/1000 | Loss: 0.00001976
Iteration 120/1000 | Loss: 0.00001976
Iteration 121/1000 | Loss: 0.00001976
Iteration 122/1000 | Loss: 0.00001975
Iteration 123/1000 | Loss: 0.00001975
Iteration 124/1000 | Loss: 0.00001974
Iteration 125/1000 | Loss: 0.00001974
Iteration 126/1000 | Loss: 0.00001974
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001973
Iteration 129/1000 | Loss: 0.00001973
Iteration 130/1000 | Loss: 0.00001972
Iteration 131/1000 | Loss: 0.00004456
Iteration 132/1000 | Loss: 0.00002405
Iteration 133/1000 | Loss: 0.00002121
Iteration 134/1000 | Loss: 0.00001970
Iteration 135/1000 | Loss: 0.00001966
Iteration 136/1000 | Loss: 0.00001965
Iteration 137/1000 | Loss: 0.00001964
Iteration 138/1000 | Loss: 0.00001964
Iteration 139/1000 | Loss: 0.00001964
Iteration 140/1000 | Loss: 0.00001962
Iteration 141/1000 | Loss: 0.00001961
Iteration 142/1000 | Loss: 0.00001989
Iteration 143/1000 | Loss: 0.00002021
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00001985
Iteration 146/1000 | Loss: 0.00001983
Iteration 147/1000 | Loss: 0.00001984
Iteration 148/1000 | Loss: 0.00001980
Iteration 149/1000 | Loss: 0.00001966
Iteration 150/1000 | Loss: 0.00001963
Iteration 151/1000 | Loss: 0.00001962
Iteration 152/1000 | Loss: 0.00001962
Iteration 153/1000 | Loss: 0.00001962
Iteration 154/1000 | Loss: 0.00001962
Iteration 155/1000 | Loss: 0.00001960
Iteration 156/1000 | Loss: 0.00001960
Iteration 157/1000 | Loss: 0.00001960
Iteration 158/1000 | Loss: 0.00001959
Iteration 159/1000 | Loss: 0.00001959
Iteration 160/1000 | Loss: 0.00001959
Iteration 161/1000 | Loss: 0.00001959
Iteration 162/1000 | Loss: 0.00001958
Iteration 163/1000 | Loss: 0.00001958
Iteration 164/1000 | Loss: 0.00001957
Iteration 165/1000 | Loss: 0.00001957
Iteration 166/1000 | Loss: 0.00001957
Iteration 167/1000 | Loss: 0.00001957
Iteration 168/1000 | Loss: 0.00001957
Iteration 169/1000 | Loss: 0.00001957
Iteration 170/1000 | Loss: 0.00001957
Iteration 171/1000 | Loss: 0.00001957
Iteration 172/1000 | Loss: 0.00001957
Iteration 173/1000 | Loss: 0.00001956
Iteration 174/1000 | Loss: 0.00001956
Iteration 175/1000 | Loss: 0.00001956
Iteration 176/1000 | Loss: 0.00001956
Iteration 177/1000 | Loss: 0.00001956
Iteration 178/1000 | Loss: 0.00001956
Iteration 179/1000 | Loss: 0.00001956
Iteration 180/1000 | Loss: 0.00001956
Iteration 181/1000 | Loss: 0.00001956
Iteration 182/1000 | Loss: 0.00001956
Iteration 183/1000 | Loss: 0.00001956
Iteration 184/1000 | Loss: 0.00001955
Iteration 185/1000 | Loss: 0.00001965
Iteration 186/1000 | Loss: 0.00001965
Iteration 187/1000 | Loss: 0.00001965
Iteration 188/1000 | Loss: 0.00001965
Iteration 189/1000 | Loss: 0.00001965
Iteration 190/1000 | Loss: 0.00001965
Iteration 191/1000 | Loss: 0.00001964
Iteration 192/1000 | Loss: 0.00001964
Iteration 193/1000 | Loss: 0.00001964
Iteration 194/1000 | Loss: 0.00001964
Iteration 195/1000 | Loss: 0.00001963
Iteration 196/1000 | Loss: 0.00001962
Iteration 197/1000 | Loss: 0.00001969
Iteration 198/1000 | Loss: 0.00001967
Iteration 199/1000 | Loss: 0.00001958
Iteration 200/1000 | Loss: 0.00001958
Iteration 201/1000 | Loss: 0.00001958
Iteration 202/1000 | Loss: 0.00001958
Iteration 203/1000 | Loss: 0.00001958
Iteration 204/1000 | Loss: 0.00001958
Iteration 205/1000 | Loss: 0.00001957
Iteration 206/1000 | Loss: 0.00001957
Iteration 207/1000 | Loss: 0.00001957
Iteration 208/1000 | Loss: 0.00001957
Iteration 209/1000 | Loss: 0.00001957
Iteration 210/1000 | Loss: 0.00001957
Iteration 211/1000 | Loss: 0.00001957
Iteration 212/1000 | Loss: 0.00001957
Iteration 213/1000 | Loss: 0.00001957
Iteration 214/1000 | Loss: 0.00001957
Iteration 215/1000 | Loss: 0.00001951
Iteration 216/1000 | Loss: 0.00001951
Iteration 217/1000 | Loss: 0.00001942
Iteration 218/1000 | Loss: 0.00001941
Iteration 219/1000 | Loss: 0.00001932
Iteration 220/1000 | Loss: 0.00001932
Iteration 221/1000 | Loss: 0.00001932
Iteration 222/1000 | Loss: 0.00001931
Iteration 223/1000 | Loss: 0.00001929
Iteration 224/1000 | Loss: 0.00001929
Iteration 225/1000 | Loss: 0.00001928
Iteration 226/1000 | Loss: 0.00001928
Iteration 227/1000 | Loss: 0.00001939
Iteration 228/1000 | Loss: 0.00001939
Iteration 229/1000 | Loss: 0.00001938
Iteration 230/1000 | Loss: 0.00001928
Iteration 231/1000 | Loss: 0.00001928
Iteration 232/1000 | Loss: 0.00001927
Iteration 233/1000 | Loss: 0.00001927
Iteration 234/1000 | Loss: 0.00001926
Iteration 235/1000 | Loss: 0.00001926
Iteration 236/1000 | Loss: 0.00001925
Iteration 237/1000 | Loss: 0.00001935
Iteration 238/1000 | Loss: 0.00001935
Iteration 239/1000 | Loss: 0.00001935
Iteration 240/1000 | Loss: 0.00001935
Iteration 241/1000 | Loss: 0.00001934
Iteration 242/1000 | Loss: 0.00001934
Iteration 243/1000 | Loss: 0.00001925
Iteration 244/1000 | Loss: 0.00001924
Iteration 245/1000 | Loss: 0.00001924
Iteration 246/1000 | Loss: 0.00001923
Iteration 247/1000 | Loss: 0.00001923
Iteration 248/1000 | Loss: 0.00001923
Iteration 249/1000 | Loss: 0.00001922
Iteration 250/1000 | Loss: 0.00001922
Iteration 251/1000 | Loss: 0.00001922
Iteration 252/1000 | Loss: 0.00001922
Iteration 253/1000 | Loss: 0.00001922
Iteration 254/1000 | Loss: 0.00001921
Iteration 255/1000 | Loss: 0.00001921
Iteration 256/1000 | Loss: 0.00001920
Iteration 257/1000 | Loss: 0.00004200
Iteration 258/1000 | Loss: 0.00003342
Iteration 259/1000 | Loss: 0.00004209
Iteration 260/1000 | Loss: 0.00003139
Iteration 261/1000 | Loss: 0.00001968
Iteration 262/1000 | Loss: 0.00001926
Iteration 263/1000 | Loss: 0.00004173
Iteration 264/1000 | Loss: 0.00011013
Iteration 265/1000 | Loss: 0.00010776
Iteration 266/1000 | Loss: 0.00003697
Iteration 267/1000 | Loss: 0.00002477
Iteration 268/1000 | Loss: 0.00002249
Iteration 269/1000 | Loss: 0.00015161
Iteration 270/1000 | Loss: 0.00002094
Iteration 271/1000 | Loss: 0.00002049
Iteration 272/1000 | Loss: 0.00002009
Iteration 273/1000 | Loss: 0.00001986
Iteration 274/1000 | Loss: 0.00001975
Iteration 275/1000 | Loss: 0.00001974
Iteration 276/1000 | Loss: 0.00001970
Iteration 277/1000 | Loss: 0.00001968
Iteration 278/1000 | Loss: 0.00001967
Iteration 279/1000 | Loss: 0.00001966
Iteration 280/1000 | Loss: 0.00001966
Iteration 281/1000 | Loss: 0.00001964
Iteration 282/1000 | Loss: 0.00001964
Iteration 283/1000 | Loss: 0.00001964
Iteration 284/1000 | Loss: 0.00001963
Iteration 285/1000 | Loss: 0.00001963
Iteration 286/1000 | Loss: 0.00001963
Iteration 287/1000 | Loss: 0.00001962
Iteration 288/1000 | Loss: 0.00001962
Iteration 289/1000 | Loss: 0.00001962
Iteration 290/1000 | Loss: 0.00001971
Iteration 291/1000 | Loss: 0.00001970
Iteration 292/1000 | Loss: 0.00001970
Iteration 293/1000 | Loss: 0.00001967
Iteration 294/1000 | Loss: 0.00001967
Iteration 295/1000 | Loss: 0.00001967
Iteration 296/1000 | Loss: 0.00001967
Iteration 297/1000 | Loss: 0.00001961
Iteration 298/1000 | Loss: 0.00001961
Iteration 299/1000 | Loss: 0.00001961
Iteration 300/1000 | Loss: 0.00001960
Iteration 301/1000 | Loss: 0.00001960
Iteration 302/1000 | Loss: 0.00001960
Iteration 303/1000 | Loss: 0.00001960
Iteration 304/1000 | Loss: 0.00001959
Iteration 305/1000 | Loss: 0.00001957
Iteration 306/1000 | Loss: 0.00001957
Iteration 307/1000 | Loss: 0.00001957
Iteration 308/1000 | Loss: 0.00001986
Iteration 309/1000 | Loss: 0.00001986
Iteration 310/1000 | Loss: 0.00001985
Iteration 311/1000 | Loss: 0.00001959
Iteration 312/1000 | Loss: 0.00001952
Iteration 313/1000 | Loss: 0.00001951
Iteration 314/1000 | Loss: 0.00001951
Iteration 315/1000 | Loss: 0.00001951
Iteration 316/1000 | Loss: 0.00001950
Iteration 317/1000 | Loss: 0.00001950
Iteration 318/1000 | Loss: 0.00001950
Iteration 319/1000 | Loss: 0.00001950
Iteration 320/1000 | Loss: 0.00001950
Iteration 321/1000 | Loss: 0.00001950
Iteration 322/1000 | Loss: 0.00001949
Iteration 323/1000 | Loss: 0.00001949
Iteration 324/1000 | Loss: 0.00001949
Iteration 325/1000 | Loss: 0.00001949
Iteration 326/1000 | Loss: 0.00001949
Iteration 327/1000 | Loss: 0.00001949
Iteration 328/1000 | Loss: 0.00001948
Iteration 329/1000 | Loss: 0.00001948
Iteration 330/1000 | Loss: 0.00001948
Iteration 331/1000 | Loss: 0.00001948
Iteration 332/1000 | Loss: 0.00001947
Iteration 333/1000 | Loss: 0.00001947
Iteration 334/1000 | Loss: 0.00001947
Iteration 335/1000 | Loss: 0.00001947
Iteration 336/1000 | Loss: 0.00001946
Iteration 337/1000 | Loss: 0.00001946
Iteration 338/1000 | Loss: 0.00001944
Iteration 339/1000 | Loss: 0.00001943
Iteration 340/1000 | Loss: 0.00001943
Iteration 341/1000 | Loss: 0.00001953
Iteration 342/1000 | Loss: 0.00001952
Iteration 343/1000 | Loss: 0.00001951
Iteration 344/1000 | Loss: 0.00009597
Iteration 345/1000 | Loss: 0.00004549
Iteration 346/1000 | Loss: 0.00001962
Iteration 347/1000 | Loss: 0.00006797
Iteration 348/1000 | Loss: 0.00004594
Iteration 349/1000 | Loss: 0.00005428
Iteration 350/1000 | Loss: 0.00004122
Iteration 351/1000 | Loss: 0.00001951
Iteration 352/1000 | Loss: 0.00002802
Iteration 353/1000 | Loss: 0.00001939
Iteration 354/1000 | Loss: 0.00001926
Iteration 355/1000 | Loss: 0.00001922
Iteration 356/1000 | Loss: 0.00001921
Iteration 357/1000 | Loss: 0.00001934
Iteration 358/1000 | Loss: 0.00001934
Iteration 359/1000 | Loss: 0.00001933
Iteration 360/1000 | Loss: 0.00001932
Iteration 361/1000 | Loss: 0.00001916
Iteration 362/1000 | Loss: 0.00001916
Iteration 363/1000 | Loss: 0.00001916
Iteration 364/1000 | Loss: 0.00001915
Iteration 365/1000 | Loss: 0.00001913
Iteration 366/1000 | Loss: 0.00001913
Iteration 367/1000 | Loss: 0.00001913
Iteration 368/1000 | Loss: 0.00001913
Iteration 369/1000 | Loss: 0.00001913
Iteration 370/1000 | Loss: 0.00001913
Iteration 371/1000 | Loss: 0.00001913
Iteration 372/1000 | Loss: 0.00001913
Iteration 373/1000 | Loss: 0.00001912
Iteration 374/1000 | Loss: 0.00001912
Iteration 375/1000 | Loss: 0.00001912
Iteration 376/1000 | Loss: 0.00001911
Iteration 377/1000 | Loss: 0.00001911
Iteration 378/1000 | Loss: 0.00001911
Iteration 379/1000 | Loss: 0.00001911
Iteration 380/1000 | Loss: 0.00001910
Iteration 381/1000 | Loss: 0.00001910
Iteration 382/1000 | Loss: 0.00001910
Iteration 383/1000 | Loss: 0.00001910
Iteration 384/1000 | Loss: 0.00001910
Iteration 385/1000 | Loss: 0.00001910
Iteration 386/1000 | Loss: 0.00001910
Iteration 387/1000 | Loss: 0.00001910
Iteration 388/1000 | Loss: 0.00001910
Iteration 389/1000 | Loss: 0.00001910
Iteration 390/1000 | Loss: 0.00001910
Iteration 391/1000 | Loss: 0.00001910
Iteration 392/1000 | Loss: 0.00001910
Iteration 393/1000 | Loss: 0.00001910
Iteration 394/1000 | Loss: 0.00001910
Iteration 395/1000 | Loss: 0.00001910
Iteration 396/1000 | Loss: 0.00001910
Iteration 397/1000 | Loss: 0.00001910
Iteration 398/1000 | Loss: 0.00001910
Iteration 399/1000 | Loss: 0.00001910
Iteration 400/1000 | Loss: 0.00001910
Iteration 401/1000 | Loss: 0.00001910
Iteration 402/1000 | Loss: 0.00001910
Iteration 403/1000 | Loss: 0.00001910
Iteration 404/1000 | Loss: 0.00001910
Iteration 405/1000 | Loss: 0.00001910
Iteration 406/1000 | Loss: 0.00001910
Iteration 407/1000 | Loss: 0.00001910
Iteration 408/1000 | Loss: 0.00001910
Iteration 409/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 409. Stopping optimization.
Last 5 losses: [1.9100047211395577e-05, 1.9100047211395577e-05, 1.9100047211395577e-05, 1.9100047211395577e-05, 1.9100047211395577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9100047211395577e-05

Optimization complete. Final v2v error: 3.5357985496520996 mm

Highest mean error: 10.37126350402832 mm for frame 105

Lowest mean error: 2.9975979328155518 mm for frame 110

Saving results

Total time: 218.81860280036926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467853
Iteration 2/25 | Loss: 0.00145586
Iteration 3/25 | Loss: 0.00118652
Iteration 4/25 | Loss: 0.00115543
Iteration 5/25 | Loss: 0.00115133
Iteration 6/25 | Loss: 0.00115049
Iteration 7/25 | Loss: 0.00115049
Iteration 8/25 | Loss: 0.00115049
Iteration 9/25 | Loss: 0.00115049
Iteration 10/25 | Loss: 0.00115049
Iteration 11/25 | Loss: 0.00115049
Iteration 12/25 | Loss: 0.00115049
Iteration 13/25 | Loss: 0.00115049
Iteration 14/25 | Loss: 0.00115049
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011504855938255787, 0.0011504855938255787, 0.0011504855938255787, 0.0011504855938255787, 0.0011504855938255787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011504855938255787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31969559
Iteration 2/25 | Loss: 0.00124832
Iteration 3/25 | Loss: 0.00124832
Iteration 4/25 | Loss: 0.00124832
Iteration 5/25 | Loss: 0.00124832
Iteration 6/25 | Loss: 0.00124832
Iteration 7/25 | Loss: 0.00124832
Iteration 8/25 | Loss: 0.00124832
Iteration 9/25 | Loss: 0.00124832
Iteration 10/25 | Loss: 0.00124831
Iteration 11/25 | Loss: 0.00124831
Iteration 12/25 | Loss: 0.00124831
Iteration 13/25 | Loss: 0.00124831
Iteration 14/25 | Loss: 0.00124831
Iteration 15/25 | Loss: 0.00124831
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001248314743861556, 0.001248314743861556, 0.001248314743861556, 0.001248314743861556, 0.001248314743861556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001248314743861556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124831
Iteration 2/1000 | Loss: 0.00003323
Iteration 3/1000 | Loss: 0.00002219
Iteration 4/1000 | Loss: 0.00001879
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001663
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001509
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001483
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001475
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001468
Iteration 18/1000 | Loss: 0.00001467
Iteration 19/1000 | Loss: 0.00001466
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001457
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001454
Iteration 27/1000 | Loss: 0.00001453
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001450
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001443
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001442
Iteration 49/1000 | Loss: 0.00001442
Iteration 50/1000 | Loss: 0.00001442
Iteration 51/1000 | Loss: 0.00001442
Iteration 52/1000 | Loss: 0.00001442
Iteration 53/1000 | Loss: 0.00001442
Iteration 54/1000 | Loss: 0.00001442
Iteration 55/1000 | Loss: 0.00001441
Iteration 56/1000 | Loss: 0.00001441
Iteration 57/1000 | Loss: 0.00001441
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001441
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001441
Iteration 62/1000 | Loss: 0.00001440
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001439
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001436
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001433
Iteration 105/1000 | Loss: 0.00001433
Iteration 106/1000 | Loss: 0.00001433
Iteration 107/1000 | Loss: 0.00001433
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001433
Iteration 115/1000 | Loss: 0.00001433
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001431
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001430
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001430
Iteration 142/1000 | Loss: 0.00001430
Iteration 143/1000 | Loss: 0.00001430
Iteration 144/1000 | Loss: 0.00001430
Iteration 145/1000 | Loss: 0.00001430
Iteration 146/1000 | Loss: 0.00001430
Iteration 147/1000 | Loss: 0.00001430
Iteration 148/1000 | Loss: 0.00001430
Iteration 149/1000 | Loss: 0.00001430
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001430
Iteration 155/1000 | Loss: 0.00001430
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001430
Iteration 162/1000 | Loss: 0.00001430
Iteration 163/1000 | Loss: 0.00001430
Iteration 164/1000 | Loss: 0.00001430
Iteration 165/1000 | Loss: 0.00001430
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001430
Iteration 175/1000 | Loss: 0.00001430
Iteration 176/1000 | Loss: 0.00001430
Iteration 177/1000 | Loss: 0.00001430
Iteration 178/1000 | Loss: 0.00001430
Iteration 179/1000 | Loss: 0.00001430
Iteration 180/1000 | Loss: 0.00001430
Iteration 181/1000 | Loss: 0.00001430
Iteration 182/1000 | Loss: 0.00001430
Iteration 183/1000 | Loss: 0.00001430
Iteration 184/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.4298156202130485e-05, 1.4298156202130485e-05, 1.4298156202130485e-05, 1.4298156202130485e-05, 1.4298156202130485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4298156202130485e-05

Optimization complete. Final v2v error: 3.2250325679779053 mm

Highest mean error: 4.730335712432861 mm for frame 105

Lowest mean error: 2.6522297859191895 mm for frame 9

Saving results

Total time: 43.753790616989136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106659
Iteration 2/25 | Loss: 0.00246124
Iteration 3/25 | Loss: 0.00211305
Iteration 4/25 | Loss: 0.00193422
Iteration 5/25 | Loss: 0.00181599
Iteration 6/25 | Loss: 0.00155785
Iteration 7/25 | Loss: 0.00134746
Iteration 8/25 | Loss: 0.00123291
Iteration 9/25 | Loss: 0.00121983
Iteration 10/25 | Loss: 0.00119815
Iteration 11/25 | Loss: 0.00119242
Iteration 12/25 | Loss: 0.00119162
Iteration 13/25 | Loss: 0.00119123
Iteration 14/25 | Loss: 0.00119113
Iteration 15/25 | Loss: 0.00119113
Iteration 16/25 | Loss: 0.00119113
Iteration 17/25 | Loss: 0.00119113
Iteration 18/25 | Loss: 0.00119113
Iteration 19/25 | Loss: 0.00119113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011911338660866022, 0.0011911338660866022, 0.0011911338660866022, 0.0011911338660866022, 0.0011911338660866022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011911338660866022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26135707
Iteration 2/25 | Loss: 0.00108766
Iteration 3/25 | Loss: 0.00108766
Iteration 4/25 | Loss: 0.00108766
Iteration 5/25 | Loss: 0.00108766
Iteration 6/25 | Loss: 0.00108766
Iteration 7/25 | Loss: 0.00108766
Iteration 8/25 | Loss: 0.00108766
Iteration 9/25 | Loss: 0.00108766
Iteration 10/25 | Loss: 0.00108766
Iteration 11/25 | Loss: 0.00108766
Iteration 12/25 | Loss: 0.00108766
Iteration 13/25 | Loss: 0.00108766
Iteration 14/25 | Loss: 0.00108766
Iteration 15/25 | Loss: 0.00108766
Iteration 16/25 | Loss: 0.00108766
Iteration 17/25 | Loss: 0.00108766
Iteration 18/25 | Loss: 0.00108766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010876560118049383, 0.0010876560118049383, 0.0010876560118049383, 0.0010876560118049383, 0.0010876560118049383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010876560118049383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108766
Iteration 2/1000 | Loss: 0.00022616
Iteration 3/1000 | Loss: 0.00004479
Iteration 4/1000 | Loss: 0.00003687
Iteration 5/1000 | Loss: 0.00003380
Iteration 6/1000 | Loss: 0.00003146
Iteration 7/1000 | Loss: 0.00002939
Iteration 8/1000 | Loss: 0.00002761
Iteration 9/1000 | Loss: 0.00002621
Iteration 10/1000 | Loss: 0.00002513
Iteration 11/1000 | Loss: 0.00002441
Iteration 12/1000 | Loss: 0.00002375
Iteration 13/1000 | Loss: 0.00002322
Iteration 14/1000 | Loss: 0.00002288
Iteration 15/1000 | Loss: 0.00002269
Iteration 16/1000 | Loss: 0.00002259
Iteration 17/1000 | Loss: 0.00002247
Iteration 18/1000 | Loss: 0.00002244
Iteration 19/1000 | Loss: 0.00002243
Iteration 20/1000 | Loss: 0.00002243
Iteration 21/1000 | Loss: 0.00002241
Iteration 22/1000 | Loss: 0.00002240
Iteration 23/1000 | Loss: 0.00002239
Iteration 24/1000 | Loss: 0.00002239
Iteration 25/1000 | Loss: 0.00002238
Iteration 26/1000 | Loss: 0.00002236
Iteration 27/1000 | Loss: 0.00002429
Iteration 28/1000 | Loss: 0.00002241
Iteration 29/1000 | Loss: 0.00002224
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00002211
Iteration 32/1000 | Loss: 0.00002209
Iteration 33/1000 | Loss: 0.00002208
Iteration 34/1000 | Loss: 0.00002208
Iteration 35/1000 | Loss: 0.00022277
Iteration 36/1000 | Loss: 0.00002897
Iteration 37/1000 | Loss: 0.00002404
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002132
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00002044
Iteration 43/1000 | Loss: 0.00002025
Iteration 44/1000 | Loss: 0.00002009
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001992
Iteration 47/1000 | Loss: 0.00001992
Iteration 48/1000 | Loss: 0.00001990
Iteration 49/1000 | Loss: 0.00001989
Iteration 50/1000 | Loss: 0.00001989
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00001987
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001986
Iteration 60/1000 | Loss: 0.00001985
Iteration 61/1000 | Loss: 0.00001985
Iteration 62/1000 | Loss: 0.00001985
Iteration 63/1000 | Loss: 0.00001985
Iteration 64/1000 | Loss: 0.00001985
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001985
Iteration 69/1000 | Loss: 0.00001985
Iteration 70/1000 | Loss: 0.00001985
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00001984
Iteration 73/1000 | Loss: 0.00001984
Iteration 74/1000 | Loss: 0.00001983
Iteration 75/1000 | Loss: 0.00001983
Iteration 76/1000 | Loss: 0.00001983
Iteration 77/1000 | Loss: 0.00001982
Iteration 78/1000 | Loss: 0.00001982
Iteration 79/1000 | Loss: 0.00001982
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001982
Iteration 82/1000 | Loss: 0.00001982
Iteration 83/1000 | Loss: 0.00001982
Iteration 84/1000 | Loss: 0.00001982
Iteration 85/1000 | Loss: 0.00001981
Iteration 86/1000 | Loss: 0.00001981
Iteration 87/1000 | Loss: 0.00001981
Iteration 88/1000 | Loss: 0.00001981
Iteration 89/1000 | Loss: 0.00001981
Iteration 90/1000 | Loss: 0.00001981
Iteration 91/1000 | Loss: 0.00001981
Iteration 92/1000 | Loss: 0.00001981
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001980
Iteration 95/1000 | Loss: 0.00001980
Iteration 96/1000 | Loss: 0.00001980
Iteration 97/1000 | Loss: 0.00001980
Iteration 98/1000 | Loss: 0.00001980
Iteration 99/1000 | Loss: 0.00001980
Iteration 100/1000 | Loss: 0.00001980
Iteration 101/1000 | Loss: 0.00001980
Iteration 102/1000 | Loss: 0.00001979
Iteration 103/1000 | Loss: 0.00001979
Iteration 104/1000 | Loss: 0.00001979
Iteration 105/1000 | Loss: 0.00001979
Iteration 106/1000 | Loss: 0.00001979
Iteration 107/1000 | Loss: 0.00001979
Iteration 108/1000 | Loss: 0.00001979
Iteration 109/1000 | Loss: 0.00001979
Iteration 110/1000 | Loss: 0.00001979
Iteration 111/1000 | Loss: 0.00001979
Iteration 112/1000 | Loss: 0.00001979
Iteration 113/1000 | Loss: 0.00001979
Iteration 114/1000 | Loss: 0.00001979
Iteration 115/1000 | Loss: 0.00001979
Iteration 116/1000 | Loss: 0.00001979
Iteration 117/1000 | Loss: 0.00001978
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001978
Iteration 121/1000 | Loss: 0.00001978
Iteration 122/1000 | Loss: 0.00001978
Iteration 123/1000 | Loss: 0.00001978
Iteration 124/1000 | Loss: 0.00001978
Iteration 125/1000 | Loss: 0.00001978
Iteration 126/1000 | Loss: 0.00001978
Iteration 127/1000 | Loss: 0.00001978
Iteration 128/1000 | Loss: 0.00001978
Iteration 129/1000 | Loss: 0.00001978
Iteration 130/1000 | Loss: 0.00001978
Iteration 131/1000 | Loss: 0.00001978
Iteration 132/1000 | Loss: 0.00001978
Iteration 133/1000 | Loss: 0.00001978
Iteration 134/1000 | Loss: 0.00001978
Iteration 135/1000 | Loss: 0.00001978
Iteration 136/1000 | Loss: 0.00001978
Iteration 137/1000 | Loss: 0.00001978
Iteration 138/1000 | Loss: 0.00001978
Iteration 139/1000 | Loss: 0.00001978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.9775930923060514e-05, 1.9775930923060514e-05, 1.9775930923060514e-05, 1.9775930923060514e-05, 1.9775930923060514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9775930923060514e-05

Optimization complete. Final v2v error: 3.8021044731140137 mm

Highest mean error: 10.027292251586914 mm for frame 21

Lowest mean error: 3.1116867065429688 mm for frame 0

Saving results

Total time: 75.52405142784119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610230
Iteration 2/25 | Loss: 0.00167371
Iteration 3/25 | Loss: 0.00133073
Iteration 4/25 | Loss: 0.00128737
Iteration 5/25 | Loss: 0.00128571
Iteration 6/25 | Loss: 0.00127151
Iteration 7/25 | Loss: 0.00126636
Iteration 8/25 | Loss: 0.00126926
Iteration 9/25 | Loss: 0.00126400
Iteration 10/25 | Loss: 0.00126309
Iteration 11/25 | Loss: 0.00126303
Iteration 12/25 | Loss: 0.00126303
Iteration 13/25 | Loss: 0.00126303
Iteration 14/25 | Loss: 0.00126303
Iteration 15/25 | Loss: 0.00126302
Iteration 16/25 | Loss: 0.00126302
Iteration 17/25 | Loss: 0.00126302
Iteration 18/25 | Loss: 0.00126302
Iteration 19/25 | Loss: 0.00126302
Iteration 20/25 | Loss: 0.00126301
Iteration 21/25 | Loss: 0.00126301
Iteration 22/25 | Loss: 0.00126301
Iteration 23/25 | Loss: 0.00126301
Iteration 24/25 | Loss: 0.00126301
Iteration 25/25 | Loss: 0.00126301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15192771
Iteration 2/25 | Loss: 0.00153895
Iteration 3/25 | Loss: 0.00153891
Iteration 4/25 | Loss: 0.00153891
Iteration 5/25 | Loss: 0.00153891
Iteration 6/25 | Loss: 0.00153891
Iteration 7/25 | Loss: 0.00153891
Iteration 8/25 | Loss: 0.00153891
Iteration 9/25 | Loss: 0.00153891
Iteration 10/25 | Loss: 0.00153891
Iteration 11/25 | Loss: 0.00153891
Iteration 12/25 | Loss: 0.00153891
Iteration 13/25 | Loss: 0.00153891
Iteration 14/25 | Loss: 0.00153891
Iteration 15/25 | Loss: 0.00153891
Iteration 16/25 | Loss: 0.00153891
Iteration 17/25 | Loss: 0.00153891
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015389085747301579, 0.0015389085747301579, 0.0015389085747301579, 0.0015389085747301579, 0.0015389085747301579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015389085747301579

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153891
Iteration 2/1000 | Loss: 0.00012474
Iteration 3/1000 | Loss: 0.00009245
Iteration 4/1000 | Loss: 0.00008008
Iteration 5/1000 | Loss: 0.00007379
Iteration 6/1000 | Loss: 0.00087463
Iteration 7/1000 | Loss: 0.00030817
Iteration 8/1000 | Loss: 0.00006932
Iteration 9/1000 | Loss: 0.00118165
Iteration 10/1000 | Loss: 0.00108712
Iteration 11/1000 | Loss: 0.00048850
Iteration 12/1000 | Loss: 0.00057627
Iteration 13/1000 | Loss: 0.00006580
Iteration 14/1000 | Loss: 0.00005889
Iteration 15/1000 | Loss: 0.00048654
Iteration 16/1000 | Loss: 0.00009563
Iteration 17/1000 | Loss: 0.00005872
Iteration 18/1000 | Loss: 0.00005002
Iteration 19/1000 | Loss: 0.00004807
Iteration 20/1000 | Loss: 0.00004616
Iteration 21/1000 | Loss: 0.00004479
Iteration 22/1000 | Loss: 0.00015912
Iteration 23/1000 | Loss: 0.00007195
Iteration 24/1000 | Loss: 0.00004707
Iteration 25/1000 | Loss: 0.00004240
Iteration 26/1000 | Loss: 0.00004038
Iteration 27/1000 | Loss: 0.00003932
Iteration 28/1000 | Loss: 0.00003863
Iteration 29/1000 | Loss: 0.00003819
Iteration 30/1000 | Loss: 0.00003788
Iteration 31/1000 | Loss: 0.00003768
Iteration 32/1000 | Loss: 0.00003748
Iteration 33/1000 | Loss: 0.00003742
Iteration 34/1000 | Loss: 0.00003736
Iteration 35/1000 | Loss: 0.00003730
Iteration 36/1000 | Loss: 0.00003730
Iteration 37/1000 | Loss: 0.00003728
Iteration 38/1000 | Loss: 0.00003727
Iteration 39/1000 | Loss: 0.00003727
Iteration 40/1000 | Loss: 0.00003727
Iteration 41/1000 | Loss: 0.00003727
Iteration 42/1000 | Loss: 0.00003722
Iteration 43/1000 | Loss: 0.00003722
Iteration 44/1000 | Loss: 0.00003720
Iteration 45/1000 | Loss: 0.00003719
Iteration 46/1000 | Loss: 0.00003719
Iteration 47/1000 | Loss: 0.00003718
Iteration 48/1000 | Loss: 0.00003718
Iteration 49/1000 | Loss: 0.00003717
Iteration 50/1000 | Loss: 0.00003717
Iteration 51/1000 | Loss: 0.00003717
Iteration 52/1000 | Loss: 0.00003716
Iteration 53/1000 | Loss: 0.00003716
Iteration 54/1000 | Loss: 0.00003716
Iteration 55/1000 | Loss: 0.00003716
Iteration 56/1000 | Loss: 0.00003716
Iteration 57/1000 | Loss: 0.00003715
Iteration 58/1000 | Loss: 0.00003715
Iteration 59/1000 | Loss: 0.00003715
Iteration 60/1000 | Loss: 0.00003714
Iteration 61/1000 | Loss: 0.00003714
Iteration 62/1000 | Loss: 0.00003714
Iteration 63/1000 | Loss: 0.00003713
Iteration 64/1000 | Loss: 0.00003713
Iteration 65/1000 | Loss: 0.00003713
Iteration 66/1000 | Loss: 0.00003713
Iteration 67/1000 | Loss: 0.00005410
Iteration 68/1000 | Loss: 0.00003606
Iteration 69/1000 | Loss: 0.00003556
Iteration 70/1000 | Loss: 0.00003530
Iteration 71/1000 | Loss: 0.00003523
Iteration 72/1000 | Loss: 0.00003500
Iteration 73/1000 | Loss: 0.00003478
Iteration 74/1000 | Loss: 0.00003462
Iteration 75/1000 | Loss: 0.00003461
Iteration 76/1000 | Loss: 0.00003451
Iteration 77/1000 | Loss: 0.00003448
Iteration 78/1000 | Loss: 0.00003447
Iteration 79/1000 | Loss: 0.00003447
Iteration 80/1000 | Loss: 0.00003446
Iteration 81/1000 | Loss: 0.00003446
Iteration 82/1000 | Loss: 0.00003446
Iteration 83/1000 | Loss: 0.00003446
Iteration 84/1000 | Loss: 0.00003446
Iteration 85/1000 | Loss: 0.00003445
Iteration 86/1000 | Loss: 0.00003445
Iteration 87/1000 | Loss: 0.00003445
Iteration 88/1000 | Loss: 0.00003445
Iteration 89/1000 | Loss: 0.00003445
Iteration 90/1000 | Loss: 0.00003445
Iteration 91/1000 | Loss: 0.00003445
Iteration 92/1000 | Loss: 0.00003445
Iteration 93/1000 | Loss: 0.00003445
Iteration 94/1000 | Loss: 0.00003445
Iteration 95/1000 | Loss: 0.00003445
Iteration 96/1000 | Loss: 0.00003445
Iteration 97/1000 | Loss: 0.00003444
Iteration 98/1000 | Loss: 0.00003444
Iteration 99/1000 | Loss: 0.00003444
Iteration 100/1000 | Loss: 0.00003443
Iteration 101/1000 | Loss: 0.00003443
Iteration 102/1000 | Loss: 0.00003443
Iteration 103/1000 | Loss: 0.00003442
Iteration 104/1000 | Loss: 0.00003442
Iteration 105/1000 | Loss: 0.00003442
Iteration 106/1000 | Loss: 0.00003442
Iteration 107/1000 | Loss: 0.00003442
Iteration 108/1000 | Loss: 0.00003442
Iteration 109/1000 | Loss: 0.00003442
Iteration 110/1000 | Loss: 0.00003441
Iteration 111/1000 | Loss: 0.00003441
Iteration 112/1000 | Loss: 0.00003441
Iteration 113/1000 | Loss: 0.00003441
Iteration 114/1000 | Loss: 0.00003441
Iteration 115/1000 | Loss: 0.00003441
Iteration 116/1000 | Loss: 0.00003441
Iteration 117/1000 | Loss: 0.00003441
Iteration 118/1000 | Loss: 0.00003441
Iteration 119/1000 | Loss: 0.00003441
Iteration 120/1000 | Loss: 0.00003441
Iteration 121/1000 | Loss: 0.00003441
Iteration 122/1000 | Loss: 0.00003441
Iteration 123/1000 | Loss: 0.00003441
Iteration 124/1000 | Loss: 0.00003441
Iteration 125/1000 | Loss: 0.00003441
Iteration 126/1000 | Loss: 0.00003441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [3.441344597376883e-05, 3.441344597376883e-05, 3.441344597376883e-05, 3.441344597376883e-05, 3.441344597376883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.441344597376883e-05

Optimization complete. Final v2v error: 3.9955925941467285 mm

Highest mean error: 11.331787109375 mm for frame 90

Lowest mean error: 3.1809208393096924 mm for frame 118

Saving results

Total time: 94.68708324432373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457236
Iteration 2/25 | Loss: 0.00137684
Iteration 3/25 | Loss: 0.00124234
Iteration 4/25 | Loss: 0.00122408
Iteration 5/25 | Loss: 0.00122054
Iteration 6/25 | Loss: 0.00122012
Iteration 7/25 | Loss: 0.00122012
Iteration 8/25 | Loss: 0.00122012
Iteration 9/25 | Loss: 0.00122012
Iteration 10/25 | Loss: 0.00122012
Iteration 11/25 | Loss: 0.00122012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012201234931126237, 0.0012201234931126237, 0.0012201234931126237, 0.0012201234931126237, 0.0012201234931126237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012201234931126237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18213701
Iteration 2/25 | Loss: 0.00128058
Iteration 3/25 | Loss: 0.00128058
Iteration 4/25 | Loss: 0.00128058
Iteration 5/25 | Loss: 0.00128058
Iteration 6/25 | Loss: 0.00128058
Iteration 7/25 | Loss: 0.00128058
Iteration 8/25 | Loss: 0.00128058
Iteration 9/25 | Loss: 0.00128058
Iteration 10/25 | Loss: 0.00128058
Iteration 11/25 | Loss: 0.00128058
Iteration 12/25 | Loss: 0.00128058
Iteration 13/25 | Loss: 0.00128058
Iteration 14/25 | Loss: 0.00128058
Iteration 15/25 | Loss: 0.00128058
Iteration 16/25 | Loss: 0.00128058
Iteration 17/25 | Loss: 0.00128058
Iteration 18/25 | Loss: 0.00128058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001280578668229282, 0.001280578668229282, 0.001280578668229282, 0.001280578668229282, 0.001280578668229282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001280578668229282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128058
Iteration 2/1000 | Loss: 0.00004482
Iteration 3/1000 | Loss: 0.00002990
Iteration 4/1000 | Loss: 0.00002629
Iteration 5/1000 | Loss: 0.00002420
Iteration 6/1000 | Loss: 0.00002314
Iteration 7/1000 | Loss: 0.00002211
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002084
Iteration 10/1000 | Loss: 0.00002045
Iteration 11/1000 | Loss: 0.00002020
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001987
Iteration 14/1000 | Loss: 0.00001977
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00001972
Iteration 17/1000 | Loss: 0.00001970
Iteration 18/1000 | Loss: 0.00001960
Iteration 19/1000 | Loss: 0.00001960
Iteration 20/1000 | Loss: 0.00001959
Iteration 21/1000 | Loss: 0.00001957
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001955
Iteration 25/1000 | Loss: 0.00001954
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001953
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001951
Iteration 30/1000 | Loss: 0.00001951
Iteration 31/1000 | Loss: 0.00001951
Iteration 32/1000 | Loss: 0.00001951
Iteration 33/1000 | Loss: 0.00001951
Iteration 34/1000 | Loss: 0.00001951
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001950
Iteration 38/1000 | Loss: 0.00001950
Iteration 39/1000 | Loss: 0.00001950
Iteration 40/1000 | Loss: 0.00001950
Iteration 41/1000 | Loss: 0.00001950
Iteration 42/1000 | Loss: 0.00001950
Iteration 43/1000 | Loss: 0.00001950
Iteration 44/1000 | Loss: 0.00001950
Iteration 45/1000 | Loss: 0.00001950
Iteration 46/1000 | Loss: 0.00001950
Iteration 47/1000 | Loss: 0.00001949
Iteration 48/1000 | Loss: 0.00001948
Iteration 49/1000 | Loss: 0.00001948
Iteration 50/1000 | Loss: 0.00001948
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001947
Iteration 55/1000 | Loss: 0.00001947
Iteration 56/1000 | Loss: 0.00001947
Iteration 57/1000 | Loss: 0.00001947
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001945
Iteration 73/1000 | Loss: 0.00001945
Iteration 74/1000 | Loss: 0.00001945
Iteration 75/1000 | Loss: 0.00001945
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001945
Iteration 78/1000 | Loss: 0.00001945
Iteration 79/1000 | Loss: 0.00001945
Iteration 80/1000 | Loss: 0.00001944
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001944
Iteration 83/1000 | Loss: 0.00001944
Iteration 84/1000 | Loss: 0.00001944
Iteration 85/1000 | Loss: 0.00001944
Iteration 86/1000 | Loss: 0.00001944
Iteration 87/1000 | Loss: 0.00001944
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001943
Iteration 90/1000 | Loss: 0.00001943
Iteration 91/1000 | Loss: 0.00001943
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Iteration 94/1000 | Loss: 0.00001942
Iteration 95/1000 | Loss: 0.00001942
Iteration 96/1000 | Loss: 0.00001942
Iteration 97/1000 | Loss: 0.00001942
Iteration 98/1000 | Loss: 0.00001941
Iteration 99/1000 | Loss: 0.00001941
Iteration 100/1000 | Loss: 0.00001941
Iteration 101/1000 | Loss: 0.00001940
Iteration 102/1000 | Loss: 0.00001940
Iteration 103/1000 | Loss: 0.00001940
Iteration 104/1000 | Loss: 0.00001940
Iteration 105/1000 | Loss: 0.00001940
Iteration 106/1000 | Loss: 0.00001940
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001939
Iteration 109/1000 | Loss: 0.00001939
Iteration 110/1000 | Loss: 0.00001939
Iteration 111/1000 | Loss: 0.00001939
Iteration 112/1000 | Loss: 0.00001939
Iteration 113/1000 | Loss: 0.00001939
Iteration 114/1000 | Loss: 0.00001939
Iteration 115/1000 | Loss: 0.00001939
Iteration 116/1000 | Loss: 0.00001939
Iteration 117/1000 | Loss: 0.00001939
Iteration 118/1000 | Loss: 0.00001939
Iteration 119/1000 | Loss: 0.00001939
Iteration 120/1000 | Loss: 0.00001939
Iteration 121/1000 | Loss: 0.00001939
Iteration 122/1000 | Loss: 0.00001939
Iteration 123/1000 | Loss: 0.00001939
Iteration 124/1000 | Loss: 0.00001939
Iteration 125/1000 | Loss: 0.00001939
Iteration 126/1000 | Loss: 0.00001939
Iteration 127/1000 | Loss: 0.00001939
Iteration 128/1000 | Loss: 0.00001939
Iteration 129/1000 | Loss: 0.00001939
Iteration 130/1000 | Loss: 0.00001939
Iteration 131/1000 | Loss: 0.00001939
Iteration 132/1000 | Loss: 0.00001939
Iteration 133/1000 | Loss: 0.00001939
Iteration 134/1000 | Loss: 0.00001939
Iteration 135/1000 | Loss: 0.00001939
Iteration 136/1000 | Loss: 0.00001939
Iteration 137/1000 | Loss: 0.00001939
Iteration 138/1000 | Loss: 0.00001939
Iteration 139/1000 | Loss: 0.00001939
Iteration 140/1000 | Loss: 0.00001939
Iteration 141/1000 | Loss: 0.00001939
Iteration 142/1000 | Loss: 0.00001939
Iteration 143/1000 | Loss: 0.00001939
Iteration 144/1000 | Loss: 0.00001939
Iteration 145/1000 | Loss: 0.00001939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.938513378263451e-05, 1.938513378263451e-05, 1.938513378263451e-05, 1.938513378263451e-05, 1.938513378263451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.938513378263451e-05

Optimization complete. Final v2v error: 3.871429920196533 mm

Highest mean error: 4.121302127838135 mm for frame 176

Lowest mean error: 3.3270645141601562 mm for frame 188

Saving results

Total time: 42.100709676742554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047796
Iteration 2/25 | Loss: 0.00195157
Iteration 3/25 | Loss: 0.00134005
Iteration 4/25 | Loss: 0.00124044
Iteration 5/25 | Loss: 0.00122753
Iteration 6/25 | Loss: 0.00122180
Iteration 7/25 | Loss: 0.00122466
Iteration 8/25 | Loss: 0.00122229
Iteration 9/25 | Loss: 0.00121152
Iteration 10/25 | Loss: 0.00120865
Iteration 11/25 | Loss: 0.00120309
Iteration 12/25 | Loss: 0.00119574
Iteration 13/25 | Loss: 0.00119302
Iteration 14/25 | Loss: 0.00119260
Iteration 15/25 | Loss: 0.00119423
Iteration 16/25 | Loss: 0.00119044
Iteration 17/25 | Loss: 0.00118822
Iteration 18/25 | Loss: 0.00118830
Iteration 19/25 | Loss: 0.00118794
Iteration 20/25 | Loss: 0.00118794
Iteration 21/25 | Loss: 0.00118764
Iteration 22/25 | Loss: 0.00118763
Iteration 23/25 | Loss: 0.00118768
Iteration 24/25 | Loss: 0.00118853
Iteration 25/25 | Loss: 0.00118734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20855188
Iteration 2/25 | Loss: 0.00146859
Iteration 3/25 | Loss: 0.00146859
Iteration 4/25 | Loss: 0.00146859
Iteration 5/25 | Loss: 0.00146859
Iteration 6/25 | Loss: 0.00146859
Iteration 7/25 | Loss: 0.00146859
Iteration 8/25 | Loss: 0.00146859
Iteration 9/25 | Loss: 0.00146859
Iteration 10/25 | Loss: 0.00146859
Iteration 11/25 | Loss: 0.00146859
Iteration 12/25 | Loss: 0.00146859
Iteration 13/25 | Loss: 0.00146859
Iteration 14/25 | Loss: 0.00146859
Iteration 15/25 | Loss: 0.00146859
Iteration 16/25 | Loss: 0.00146859
Iteration 17/25 | Loss: 0.00146859
Iteration 18/25 | Loss: 0.00146859
Iteration 19/25 | Loss: 0.00146859
Iteration 20/25 | Loss: 0.00146859
Iteration 21/25 | Loss: 0.00146859
Iteration 22/25 | Loss: 0.00146859
Iteration 23/25 | Loss: 0.00146859
Iteration 24/25 | Loss: 0.00146859
Iteration 25/25 | Loss: 0.00146859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146859
Iteration 2/1000 | Loss: 0.00004809
Iteration 3/1000 | Loss: 0.00003201
Iteration 4/1000 | Loss: 0.00003160
Iteration 5/1000 | Loss: 0.00004143
Iteration 6/1000 | Loss: 0.00002628
Iteration 7/1000 | Loss: 0.00002236
Iteration 8/1000 | Loss: 0.00002717
Iteration 9/1000 | Loss: 0.00003200
Iteration 10/1000 | Loss: 0.00003340
Iteration 11/1000 | Loss: 0.00005453
Iteration 12/1000 | Loss: 0.00003180
Iteration 13/1000 | Loss: 0.00003239
Iteration 14/1000 | Loss: 0.00002875
Iteration 15/1000 | Loss: 0.00004272
Iteration 16/1000 | Loss: 0.00003193
Iteration 17/1000 | Loss: 0.00002257
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002052
Iteration 20/1000 | Loss: 0.00002003
Iteration 21/1000 | Loss: 0.00002272
Iteration 22/1000 | Loss: 0.00002271
Iteration 23/1000 | Loss: 0.00002169
Iteration 24/1000 | Loss: 0.00001983
Iteration 25/1000 | Loss: 0.00002113
Iteration 26/1000 | Loss: 0.00001920
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00002114
Iteration 30/1000 | Loss: 0.00003626
Iteration 31/1000 | Loss: 0.00030442
Iteration 32/1000 | Loss: 0.00004013
Iteration 33/1000 | Loss: 0.00002679
Iteration 34/1000 | Loss: 0.00004586
Iteration 35/1000 | Loss: 0.00002218
Iteration 36/1000 | Loss: 0.00004797
Iteration 37/1000 | Loss: 0.00002016
Iteration 38/1000 | Loss: 0.00002354
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001913
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001690
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001681
Iteration 46/1000 | Loss: 0.00001681
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001680
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001680
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001677
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001674
Iteration 75/1000 | Loss: 0.00001674
Iteration 76/1000 | Loss: 0.00004122
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001671
Iteration 79/1000 | Loss: 0.00001669
Iteration 80/1000 | Loss: 0.00001669
Iteration 81/1000 | Loss: 0.00001668
Iteration 82/1000 | Loss: 0.00001668
Iteration 83/1000 | Loss: 0.00001667
Iteration 84/1000 | Loss: 0.00002737
Iteration 85/1000 | Loss: 0.00006287
Iteration 86/1000 | Loss: 0.00002446
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00002481
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001658
Iteration 91/1000 | Loss: 0.00001658
Iteration 92/1000 | Loss: 0.00001658
Iteration 93/1000 | Loss: 0.00001658
Iteration 94/1000 | Loss: 0.00001658
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001658
Iteration 97/1000 | Loss: 0.00001658
Iteration 98/1000 | Loss: 0.00001658
Iteration 99/1000 | Loss: 0.00001658
Iteration 100/1000 | Loss: 0.00001658
Iteration 101/1000 | Loss: 0.00001658
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001656
Iteration 110/1000 | Loss: 0.00001656
Iteration 111/1000 | Loss: 0.00001656
Iteration 112/1000 | Loss: 0.00001656
Iteration 113/1000 | Loss: 0.00001656
Iteration 114/1000 | Loss: 0.00001656
Iteration 115/1000 | Loss: 0.00001656
Iteration 116/1000 | Loss: 0.00001656
Iteration 117/1000 | Loss: 0.00001655
Iteration 118/1000 | Loss: 0.00001655
Iteration 119/1000 | Loss: 0.00001655
Iteration 120/1000 | Loss: 0.00001655
Iteration 121/1000 | Loss: 0.00001655
Iteration 122/1000 | Loss: 0.00001655
Iteration 123/1000 | Loss: 0.00001655
Iteration 124/1000 | Loss: 0.00001655
Iteration 125/1000 | Loss: 0.00001655
Iteration 126/1000 | Loss: 0.00001655
Iteration 127/1000 | Loss: 0.00001655
Iteration 128/1000 | Loss: 0.00001655
Iteration 129/1000 | Loss: 0.00001907
Iteration 130/1000 | Loss: 0.00005023
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001652
Iteration 133/1000 | Loss: 0.00001652
Iteration 134/1000 | Loss: 0.00001652
Iteration 135/1000 | Loss: 0.00001652
Iteration 136/1000 | Loss: 0.00001652
Iteration 137/1000 | Loss: 0.00001652
Iteration 138/1000 | Loss: 0.00001652
Iteration 139/1000 | Loss: 0.00001652
Iteration 140/1000 | Loss: 0.00001652
Iteration 141/1000 | Loss: 0.00001652
Iteration 142/1000 | Loss: 0.00001651
Iteration 143/1000 | Loss: 0.00002739
Iteration 144/1000 | Loss: 0.00001743
Iteration 145/1000 | Loss: 0.00001734
Iteration 146/1000 | Loss: 0.00001734
Iteration 147/1000 | Loss: 0.00001734
Iteration 148/1000 | Loss: 0.00001733
Iteration 149/1000 | Loss: 0.00001733
Iteration 150/1000 | Loss: 0.00001732
Iteration 151/1000 | Loss: 0.00001732
Iteration 152/1000 | Loss: 0.00007184
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001654
Iteration 155/1000 | Loss: 0.00001654
Iteration 156/1000 | Loss: 0.00001654
Iteration 157/1000 | Loss: 0.00001654
Iteration 158/1000 | Loss: 0.00001653
Iteration 159/1000 | Loss: 0.00001653
Iteration 160/1000 | Loss: 0.00001653
Iteration 161/1000 | Loss: 0.00001653
Iteration 162/1000 | Loss: 0.00001653
Iteration 163/1000 | Loss: 0.00001653
Iteration 164/1000 | Loss: 0.00001653
Iteration 165/1000 | Loss: 0.00001653
Iteration 166/1000 | Loss: 0.00001653
Iteration 167/1000 | Loss: 0.00001653
Iteration 168/1000 | Loss: 0.00001653
Iteration 169/1000 | Loss: 0.00001652
Iteration 170/1000 | Loss: 0.00001652
Iteration 171/1000 | Loss: 0.00001652
Iteration 172/1000 | Loss: 0.00001652
Iteration 173/1000 | Loss: 0.00001652
Iteration 174/1000 | Loss: 0.00001652
Iteration 175/1000 | Loss: 0.00001652
Iteration 176/1000 | Loss: 0.00001652
Iteration 177/1000 | Loss: 0.00001652
Iteration 178/1000 | Loss: 0.00001652
Iteration 179/1000 | Loss: 0.00001652
Iteration 180/1000 | Loss: 0.00001652
Iteration 181/1000 | Loss: 0.00001652
Iteration 182/1000 | Loss: 0.00001652
Iteration 183/1000 | Loss: 0.00001652
Iteration 184/1000 | Loss: 0.00001652
Iteration 185/1000 | Loss: 0.00001652
Iteration 186/1000 | Loss: 0.00001652
Iteration 187/1000 | Loss: 0.00001652
Iteration 188/1000 | Loss: 0.00001651
Iteration 189/1000 | Loss: 0.00001651
Iteration 190/1000 | Loss: 0.00001651
Iteration 191/1000 | Loss: 0.00001651
Iteration 192/1000 | Loss: 0.00001651
Iteration 193/1000 | Loss: 0.00001651
Iteration 194/1000 | Loss: 0.00001651
Iteration 195/1000 | Loss: 0.00001650
Iteration 196/1000 | Loss: 0.00001650
Iteration 197/1000 | Loss: 0.00002072
Iteration 198/1000 | Loss: 0.00001663
Iteration 199/1000 | Loss: 0.00001659
Iteration 200/1000 | Loss: 0.00001658
Iteration 201/1000 | Loss: 0.00001658
Iteration 202/1000 | Loss: 0.00001658
Iteration 203/1000 | Loss: 0.00001658
Iteration 204/1000 | Loss: 0.00001658
Iteration 205/1000 | Loss: 0.00001756
Iteration 206/1000 | Loss: 0.00001656
Iteration 207/1000 | Loss: 0.00001656
Iteration 208/1000 | Loss: 0.00001656
Iteration 209/1000 | Loss: 0.00001656
Iteration 210/1000 | Loss: 0.00001656
Iteration 211/1000 | Loss: 0.00001655
Iteration 212/1000 | Loss: 0.00001655
Iteration 213/1000 | Loss: 0.00001655
Iteration 214/1000 | Loss: 0.00001655
Iteration 215/1000 | Loss: 0.00001655
Iteration 216/1000 | Loss: 0.00001749
Iteration 217/1000 | Loss: 0.00001662
Iteration 218/1000 | Loss: 0.00001663
Iteration 219/1000 | Loss: 0.00001651
Iteration 220/1000 | Loss: 0.00001651
Iteration 221/1000 | Loss: 0.00001651
Iteration 222/1000 | Loss: 0.00001651
Iteration 223/1000 | Loss: 0.00001651
Iteration 224/1000 | Loss: 0.00001651
Iteration 225/1000 | Loss: 0.00001651
Iteration 226/1000 | Loss: 0.00001651
Iteration 227/1000 | Loss: 0.00001651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.6507025065948255e-05, 1.6507025065948255e-05, 1.6507025065948255e-05, 1.6507025065948255e-05, 1.6507025065948255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6507025065948255e-05

Optimization complete. Final v2v error: 3.434173583984375 mm

Highest mean error: 8.305153846740723 mm for frame 124

Lowest mean error: 2.9087090492248535 mm for frame 236

Saving results

Total time: 156.64767599105835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109211
Iteration 2/25 | Loss: 0.00272166
Iteration 3/25 | Loss: 0.00193018
Iteration 4/25 | Loss: 0.00188744
Iteration 5/25 | Loss: 0.00169276
Iteration 6/25 | Loss: 0.00157156
Iteration 7/25 | Loss: 0.00152729
Iteration 8/25 | Loss: 0.00152172
Iteration 9/25 | Loss: 0.00149313
Iteration 10/25 | Loss: 0.00146423
Iteration 11/25 | Loss: 0.00144789
Iteration 12/25 | Loss: 0.00144110
Iteration 13/25 | Loss: 0.00144194
Iteration 14/25 | Loss: 0.00145050
Iteration 15/25 | Loss: 0.00142226
Iteration 16/25 | Loss: 0.00141123
Iteration 17/25 | Loss: 0.00140426
Iteration 18/25 | Loss: 0.00140167
Iteration 19/25 | Loss: 0.00140102
Iteration 20/25 | Loss: 0.00140077
Iteration 21/25 | Loss: 0.00140701
Iteration 22/25 | Loss: 0.00140041
Iteration 23/25 | Loss: 0.00139831
Iteration 24/25 | Loss: 0.00140496
Iteration 25/25 | Loss: 0.00140194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.67028069
Iteration 2/25 | Loss: 0.00288635
Iteration 3/25 | Loss: 0.00288635
Iteration 4/25 | Loss: 0.00288635
Iteration 5/25 | Loss: 0.00288635
Iteration 6/25 | Loss: 0.00288635
Iteration 7/25 | Loss: 0.00288635
Iteration 8/25 | Loss: 0.00288634
Iteration 9/25 | Loss: 0.00288634
Iteration 10/25 | Loss: 0.00288634
Iteration 11/25 | Loss: 0.00288634
Iteration 12/25 | Loss: 0.00288634
Iteration 13/25 | Loss: 0.00288634
Iteration 14/25 | Loss: 0.00288634
Iteration 15/25 | Loss: 0.00288634
Iteration 16/25 | Loss: 0.00288634
Iteration 17/25 | Loss: 0.00288634
Iteration 18/25 | Loss: 0.00288634
Iteration 19/25 | Loss: 0.00288634
Iteration 20/25 | Loss: 0.00288634
Iteration 21/25 | Loss: 0.00288634
Iteration 22/25 | Loss: 0.00288634
Iteration 23/25 | Loss: 0.00288634
Iteration 24/25 | Loss: 0.00288634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0028863439802080393, 0.0028863439802080393, 0.0028863439802080393, 0.0028863439802080393, 0.0028863439802080393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028863439802080393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288634
Iteration 2/1000 | Loss: 0.00036657
Iteration 3/1000 | Loss: 0.00025042
Iteration 4/1000 | Loss: 0.00022038
Iteration 5/1000 | Loss: 0.00045217
Iteration 6/1000 | Loss: 0.00148440
Iteration 7/1000 | Loss: 0.00076028
Iteration 8/1000 | Loss: 0.00022864
Iteration 9/1000 | Loss: 0.00062788
Iteration 10/1000 | Loss: 0.00058321
Iteration 11/1000 | Loss: 0.00017089
Iteration 12/1000 | Loss: 0.00015523
Iteration 13/1000 | Loss: 0.00014593
Iteration 14/1000 | Loss: 0.00013910
Iteration 15/1000 | Loss: 0.00013484
Iteration 16/1000 | Loss: 0.00013091
Iteration 17/1000 | Loss: 0.00012800
Iteration 18/1000 | Loss: 0.00012566
Iteration 19/1000 | Loss: 0.00033475
Iteration 20/1000 | Loss: 0.00034108
Iteration 21/1000 | Loss: 0.00038984
Iteration 22/1000 | Loss: 0.00107041
Iteration 23/1000 | Loss: 0.00083513
Iteration 24/1000 | Loss: 0.00051786
Iteration 25/1000 | Loss: 0.00034094
Iteration 26/1000 | Loss: 0.00024124
Iteration 27/1000 | Loss: 0.00013173
Iteration 28/1000 | Loss: 0.00038487
Iteration 29/1000 | Loss: 0.00012121
Iteration 30/1000 | Loss: 0.00039805
Iteration 31/1000 | Loss: 0.00011552
Iteration 32/1000 | Loss: 0.00010329
Iteration 33/1000 | Loss: 0.00010044
Iteration 34/1000 | Loss: 0.00009771
Iteration 35/1000 | Loss: 0.00015674
Iteration 36/1000 | Loss: 0.00037986
Iteration 37/1000 | Loss: 0.00033499
Iteration 38/1000 | Loss: 0.00022582
Iteration 39/1000 | Loss: 0.00029465
Iteration 40/1000 | Loss: 0.00017496
Iteration 41/1000 | Loss: 0.00033630
Iteration 42/1000 | Loss: 0.00022515
Iteration 43/1000 | Loss: 0.00010516
Iteration 44/1000 | Loss: 0.00009746
Iteration 45/1000 | Loss: 0.00009351
Iteration 46/1000 | Loss: 0.00018557
Iteration 47/1000 | Loss: 0.00035836
Iteration 48/1000 | Loss: 0.00018097
Iteration 49/1000 | Loss: 0.00018275
Iteration 50/1000 | Loss: 0.00032771
Iteration 51/1000 | Loss: 0.00019527
Iteration 52/1000 | Loss: 0.00010108
Iteration 53/1000 | Loss: 0.00033739
Iteration 54/1000 | Loss: 0.00070652
Iteration 55/1000 | Loss: 0.00032289
Iteration 56/1000 | Loss: 0.00042215
Iteration 57/1000 | Loss: 0.00019602
Iteration 58/1000 | Loss: 0.00025176
Iteration 59/1000 | Loss: 0.00057975
Iteration 60/1000 | Loss: 0.00014607
Iteration 61/1000 | Loss: 0.00033977
Iteration 62/1000 | Loss: 0.00011562
Iteration 63/1000 | Loss: 0.00010445
Iteration 64/1000 | Loss: 0.00009876
Iteration 65/1000 | Loss: 0.00009548
Iteration 66/1000 | Loss: 0.00008955
Iteration 67/1000 | Loss: 0.00008722
Iteration 68/1000 | Loss: 0.00008554
Iteration 69/1000 | Loss: 0.00008443
Iteration 70/1000 | Loss: 0.00008384
Iteration 71/1000 | Loss: 0.00008344
Iteration 72/1000 | Loss: 0.00008317
Iteration 73/1000 | Loss: 0.00008295
Iteration 74/1000 | Loss: 0.00008274
Iteration 75/1000 | Loss: 0.00008272
Iteration 76/1000 | Loss: 0.00008266
Iteration 77/1000 | Loss: 0.00008251
Iteration 78/1000 | Loss: 0.00008247
Iteration 79/1000 | Loss: 0.00008245
Iteration 80/1000 | Loss: 0.00008241
Iteration 81/1000 | Loss: 0.00008236
Iteration 82/1000 | Loss: 0.00008236
Iteration 83/1000 | Loss: 0.00008236
Iteration 84/1000 | Loss: 0.00008235
Iteration 85/1000 | Loss: 0.00008235
Iteration 86/1000 | Loss: 0.00008234
Iteration 87/1000 | Loss: 0.00008234
Iteration 88/1000 | Loss: 0.00008233
Iteration 89/1000 | Loss: 0.00008233
Iteration 90/1000 | Loss: 0.00008232
Iteration 91/1000 | Loss: 0.00008231
Iteration 92/1000 | Loss: 0.00008231
Iteration 93/1000 | Loss: 0.00008230
Iteration 94/1000 | Loss: 0.00008230
Iteration 95/1000 | Loss: 0.00008230
Iteration 96/1000 | Loss: 0.00008230
Iteration 97/1000 | Loss: 0.00008229
Iteration 98/1000 | Loss: 0.00008229
Iteration 99/1000 | Loss: 0.00008228
Iteration 100/1000 | Loss: 0.00008228
Iteration 101/1000 | Loss: 0.00008228
Iteration 102/1000 | Loss: 0.00008227
Iteration 103/1000 | Loss: 0.00008227
Iteration 104/1000 | Loss: 0.00008227
Iteration 105/1000 | Loss: 0.00008226
Iteration 106/1000 | Loss: 0.00008226
Iteration 107/1000 | Loss: 0.00008225
Iteration 108/1000 | Loss: 0.00008223
Iteration 109/1000 | Loss: 0.00008222
Iteration 110/1000 | Loss: 0.00008222
Iteration 111/1000 | Loss: 0.00008221
Iteration 112/1000 | Loss: 0.00008221
Iteration 113/1000 | Loss: 0.00008221
Iteration 114/1000 | Loss: 0.00008221
Iteration 115/1000 | Loss: 0.00008221
Iteration 116/1000 | Loss: 0.00008221
Iteration 117/1000 | Loss: 0.00008221
Iteration 118/1000 | Loss: 0.00008221
Iteration 119/1000 | Loss: 0.00008220
Iteration 120/1000 | Loss: 0.00008220
Iteration 121/1000 | Loss: 0.00008220
Iteration 122/1000 | Loss: 0.00008220
Iteration 123/1000 | Loss: 0.00008220
Iteration 124/1000 | Loss: 0.00008220
Iteration 125/1000 | Loss: 0.00008220
Iteration 126/1000 | Loss: 0.00008220
Iteration 127/1000 | Loss: 0.00008220
Iteration 128/1000 | Loss: 0.00008220
Iteration 129/1000 | Loss: 0.00008219
Iteration 130/1000 | Loss: 0.00008218
Iteration 131/1000 | Loss: 0.00008218
Iteration 132/1000 | Loss: 0.00008217
Iteration 133/1000 | Loss: 0.00008217
Iteration 134/1000 | Loss: 0.00008216
Iteration 135/1000 | Loss: 0.00008216
Iteration 136/1000 | Loss: 0.00008216
Iteration 137/1000 | Loss: 0.00008215
Iteration 138/1000 | Loss: 0.00008215
Iteration 139/1000 | Loss: 0.00008215
Iteration 140/1000 | Loss: 0.00008215
Iteration 141/1000 | Loss: 0.00008215
Iteration 142/1000 | Loss: 0.00008215
Iteration 143/1000 | Loss: 0.00008215
Iteration 144/1000 | Loss: 0.00008215
Iteration 145/1000 | Loss: 0.00008214
Iteration 146/1000 | Loss: 0.00008214
Iteration 147/1000 | Loss: 0.00008214
Iteration 148/1000 | Loss: 0.00008214
Iteration 149/1000 | Loss: 0.00008214
Iteration 150/1000 | Loss: 0.00008214
Iteration 151/1000 | Loss: 0.00008213
Iteration 152/1000 | Loss: 0.00008213
Iteration 153/1000 | Loss: 0.00008213
Iteration 154/1000 | Loss: 0.00008213
Iteration 155/1000 | Loss: 0.00008213
Iteration 156/1000 | Loss: 0.00008213
Iteration 157/1000 | Loss: 0.00008212
Iteration 158/1000 | Loss: 0.00008212
Iteration 159/1000 | Loss: 0.00008212
Iteration 160/1000 | Loss: 0.00008212
Iteration 161/1000 | Loss: 0.00008212
Iteration 162/1000 | Loss: 0.00008212
Iteration 163/1000 | Loss: 0.00008212
Iteration 164/1000 | Loss: 0.00008211
Iteration 165/1000 | Loss: 0.00008211
Iteration 166/1000 | Loss: 0.00008211
Iteration 167/1000 | Loss: 0.00008211
Iteration 168/1000 | Loss: 0.00008211
Iteration 169/1000 | Loss: 0.00008211
Iteration 170/1000 | Loss: 0.00008211
Iteration 171/1000 | Loss: 0.00008211
Iteration 172/1000 | Loss: 0.00008211
Iteration 173/1000 | Loss: 0.00008211
Iteration 174/1000 | Loss: 0.00008211
Iteration 175/1000 | Loss: 0.00008211
Iteration 176/1000 | Loss: 0.00008211
Iteration 177/1000 | Loss: 0.00008211
Iteration 178/1000 | Loss: 0.00008211
Iteration 179/1000 | Loss: 0.00008211
Iteration 180/1000 | Loss: 0.00008211
Iteration 181/1000 | Loss: 0.00008210
Iteration 182/1000 | Loss: 0.00008210
Iteration 183/1000 | Loss: 0.00008210
Iteration 184/1000 | Loss: 0.00008210
Iteration 185/1000 | Loss: 0.00008210
Iteration 186/1000 | Loss: 0.00008210
Iteration 187/1000 | Loss: 0.00008210
Iteration 188/1000 | Loss: 0.00008210
Iteration 189/1000 | Loss: 0.00008210
Iteration 190/1000 | Loss: 0.00008210
Iteration 191/1000 | Loss: 0.00008210
Iteration 192/1000 | Loss: 0.00008210
Iteration 193/1000 | Loss: 0.00008210
Iteration 194/1000 | Loss: 0.00008210
Iteration 195/1000 | Loss: 0.00008210
Iteration 196/1000 | Loss: 0.00008210
Iteration 197/1000 | Loss: 0.00008210
Iteration 198/1000 | Loss: 0.00008210
Iteration 199/1000 | Loss: 0.00008210
Iteration 200/1000 | Loss: 0.00008210
Iteration 201/1000 | Loss: 0.00008210
Iteration 202/1000 | Loss: 0.00008210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [8.210384112317115e-05, 8.210384112317115e-05, 8.210384112317115e-05, 8.210384112317115e-05, 8.210384112317115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.210384112317115e-05

Optimization complete. Final v2v error: 4.729480266571045 mm

Highest mean error: 12.053150177001953 mm for frame 53

Lowest mean error: 3.0076634883880615 mm for frame 4

Saving results

Total time: 162.92304301261902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072340
Iteration 2/25 | Loss: 0.00176070
Iteration 3/25 | Loss: 0.00133541
Iteration 4/25 | Loss: 0.00130731
Iteration 5/25 | Loss: 0.00129649
Iteration 6/25 | Loss: 0.00129390
Iteration 7/25 | Loss: 0.00129287
Iteration 8/25 | Loss: 0.00129287
Iteration 9/25 | Loss: 0.00129287
Iteration 10/25 | Loss: 0.00129287
Iteration 11/25 | Loss: 0.00129287
Iteration 12/25 | Loss: 0.00129287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001292874221689999, 0.001292874221689999, 0.001292874221689999, 0.001292874221689999, 0.001292874221689999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001292874221689999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53573066
Iteration 2/25 | Loss: 0.00103074
Iteration 3/25 | Loss: 0.00103074
Iteration 4/25 | Loss: 0.00103074
Iteration 5/25 | Loss: 0.00103074
Iteration 6/25 | Loss: 0.00103074
Iteration 7/25 | Loss: 0.00103074
Iteration 8/25 | Loss: 0.00103074
Iteration 9/25 | Loss: 0.00103074
Iteration 10/25 | Loss: 0.00103074
Iteration 11/25 | Loss: 0.00103074
Iteration 12/25 | Loss: 0.00103074
Iteration 13/25 | Loss: 0.00103074
Iteration 14/25 | Loss: 0.00103074
Iteration 15/25 | Loss: 0.00103074
Iteration 16/25 | Loss: 0.00103074
Iteration 17/25 | Loss: 0.00103074
Iteration 18/25 | Loss: 0.00103074
Iteration 19/25 | Loss: 0.00103074
Iteration 20/25 | Loss: 0.00103074
Iteration 21/25 | Loss: 0.00103074
Iteration 22/25 | Loss: 0.00103074
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010307376505807042, 0.0010307376505807042, 0.0010307376505807042, 0.0010307376505807042, 0.0010307376505807042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010307376505807042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103074
Iteration 2/1000 | Loss: 0.00007786
Iteration 3/1000 | Loss: 0.00004998
Iteration 4/1000 | Loss: 0.00004318
Iteration 5/1000 | Loss: 0.00003981
Iteration 6/1000 | Loss: 0.00003814
Iteration 7/1000 | Loss: 0.00003692
Iteration 8/1000 | Loss: 0.00003615
Iteration 9/1000 | Loss: 0.00003568
Iteration 10/1000 | Loss: 0.00003525
Iteration 11/1000 | Loss: 0.00003494
Iteration 12/1000 | Loss: 0.00003470
Iteration 13/1000 | Loss: 0.00003451
Iteration 14/1000 | Loss: 0.00003432
Iteration 15/1000 | Loss: 0.00003416
Iteration 16/1000 | Loss: 0.00003405
Iteration 17/1000 | Loss: 0.00003395
Iteration 18/1000 | Loss: 0.00003395
Iteration 19/1000 | Loss: 0.00003392
Iteration 20/1000 | Loss: 0.00003385
Iteration 21/1000 | Loss: 0.00003382
Iteration 22/1000 | Loss: 0.00003378
Iteration 23/1000 | Loss: 0.00003377
Iteration 24/1000 | Loss: 0.00003371
Iteration 25/1000 | Loss: 0.00003364
Iteration 26/1000 | Loss: 0.00003362
Iteration 27/1000 | Loss: 0.00003350
Iteration 28/1000 | Loss: 0.00003347
Iteration 29/1000 | Loss: 0.00003334
Iteration 30/1000 | Loss: 0.00003332
Iteration 31/1000 | Loss: 0.00003332
Iteration 32/1000 | Loss: 0.00003328
Iteration 33/1000 | Loss: 0.00003327
Iteration 34/1000 | Loss: 0.00003327
Iteration 35/1000 | Loss: 0.00003327
Iteration 36/1000 | Loss: 0.00003326
Iteration 37/1000 | Loss: 0.00003325
Iteration 38/1000 | Loss: 0.00003325
Iteration 39/1000 | Loss: 0.00003324
Iteration 40/1000 | Loss: 0.00003324
Iteration 41/1000 | Loss: 0.00003323
Iteration 42/1000 | Loss: 0.00003322
Iteration 43/1000 | Loss: 0.00003322
Iteration 44/1000 | Loss: 0.00003322
Iteration 45/1000 | Loss: 0.00003322
Iteration 46/1000 | Loss: 0.00003322
Iteration 47/1000 | Loss: 0.00003322
Iteration 48/1000 | Loss: 0.00003322
Iteration 49/1000 | Loss: 0.00003322
Iteration 50/1000 | Loss: 0.00003321
Iteration 51/1000 | Loss: 0.00003321
Iteration 52/1000 | Loss: 0.00003321
Iteration 53/1000 | Loss: 0.00003321
Iteration 54/1000 | Loss: 0.00003321
Iteration 55/1000 | Loss: 0.00003321
Iteration 56/1000 | Loss: 0.00003320
Iteration 57/1000 | Loss: 0.00003320
Iteration 58/1000 | Loss: 0.00003320
Iteration 59/1000 | Loss: 0.00003320
Iteration 60/1000 | Loss: 0.00003319
Iteration 61/1000 | Loss: 0.00003319
Iteration 62/1000 | Loss: 0.00003319
Iteration 63/1000 | Loss: 0.00003319
Iteration 64/1000 | Loss: 0.00003318
Iteration 65/1000 | Loss: 0.00003318
Iteration 66/1000 | Loss: 0.00003318
Iteration 67/1000 | Loss: 0.00003318
Iteration 68/1000 | Loss: 0.00003318
Iteration 69/1000 | Loss: 0.00003318
Iteration 70/1000 | Loss: 0.00003318
Iteration 71/1000 | Loss: 0.00003318
Iteration 72/1000 | Loss: 0.00003318
Iteration 73/1000 | Loss: 0.00003317
Iteration 74/1000 | Loss: 0.00003317
Iteration 75/1000 | Loss: 0.00003317
Iteration 76/1000 | Loss: 0.00003317
Iteration 77/1000 | Loss: 0.00003317
Iteration 78/1000 | Loss: 0.00003317
Iteration 79/1000 | Loss: 0.00003317
Iteration 80/1000 | Loss: 0.00003317
Iteration 81/1000 | Loss: 0.00003317
Iteration 82/1000 | Loss: 0.00003317
Iteration 83/1000 | Loss: 0.00003316
Iteration 84/1000 | Loss: 0.00003316
Iteration 85/1000 | Loss: 0.00003316
Iteration 86/1000 | Loss: 0.00003316
Iteration 87/1000 | Loss: 0.00003316
Iteration 88/1000 | Loss: 0.00003316
Iteration 89/1000 | Loss: 0.00003315
Iteration 90/1000 | Loss: 0.00003315
Iteration 91/1000 | Loss: 0.00003315
Iteration 92/1000 | Loss: 0.00003315
Iteration 93/1000 | Loss: 0.00003315
Iteration 94/1000 | Loss: 0.00003315
Iteration 95/1000 | Loss: 0.00003314
Iteration 96/1000 | Loss: 0.00003314
Iteration 97/1000 | Loss: 0.00003314
Iteration 98/1000 | Loss: 0.00003314
Iteration 99/1000 | Loss: 0.00003314
Iteration 100/1000 | Loss: 0.00003314
Iteration 101/1000 | Loss: 0.00003314
Iteration 102/1000 | Loss: 0.00003313
Iteration 103/1000 | Loss: 0.00003313
Iteration 104/1000 | Loss: 0.00003313
Iteration 105/1000 | Loss: 0.00003313
Iteration 106/1000 | Loss: 0.00003313
Iteration 107/1000 | Loss: 0.00003313
Iteration 108/1000 | Loss: 0.00003313
Iteration 109/1000 | Loss: 0.00003313
Iteration 110/1000 | Loss: 0.00003313
Iteration 111/1000 | Loss: 0.00003313
Iteration 112/1000 | Loss: 0.00003313
Iteration 113/1000 | Loss: 0.00003312
Iteration 114/1000 | Loss: 0.00003312
Iteration 115/1000 | Loss: 0.00003312
Iteration 116/1000 | Loss: 0.00003312
Iteration 117/1000 | Loss: 0.00003312
Iteration 118/1000 | Loss: 0.00003312
Iteration 119/1000 | Loss: 0.00003312
Iteration 120/1000 | Loss: 0.00003311
Iteration 121/1000 | Loss: 0.00003311
Iteration 122/1000 | Loss: 0.00003311
Iteration 123/1000 | Loss: 0.00003311
Iteration 124/1000 | Loss: 0.00003311
Iteration 125/1000 | Loss: 0.00003311
Iteration 126/1000 | Loss: 0.00003311
Iteration 127/1000 | Loss: 0.00003311
Iteration 128/1000 | Loss: 0.00003311
Iteration 129/1000 | Loss: 0.00003310
Iteration 130/1000 | Loss: 0.00003310
Iteration 131/1000 | Loss: 0.00003310
Iteration 132/1000 | Loss: 0.00003310
Iteration 133/1000 | Loss: 0.00003310
Iteration 134/1000 | Loss: 0.00003310
Iteration 135/1000 | Loss: 0.00003310
Iteration 136/1000 | Loss: 0.00003310
Iteration 137/1000 | Loss: 0.00003310
Iteration 138/1000 | Loss: 0.00003310
Iteration 139/1000 | Loss: 0.00003309
Iteration 140/1000 | Loss: 0.00003309
Iteration 141/1000 | Loss: 0.00003309
Iteration 142/1000 | Loss: 0.00003309
Iteration 143/1000 | Loss: 0.00003309
Iteration 144/1000 | Loss: 0.00003309
Iteration 145/1000 | Loss: 0.00003309
Iteration 146/1000 | Loss: 0.00003309
Iteration 147/1000 | Loss: 0.00003309
Iteration 148/1000 | Loss: 0.00003309
Iteration 149/1000 | Loss: 0.00003309
Iteration 150/1000 | Loss: 0.00003309
Iteration 151/1000 | Loss: 0.00003309
Iteration 152/1000 | Loss: 0.00003309
Iteration 153/1000 | Loss: 0.00003309
Iteration 154/1000 | Loss: 0.00003309
Iteration 155/1000 | Loss: 0.00003309
Iteration 156/1000 | Loss: 0.00003309
Iteration 157/1000 | Loss: 0.00003309
Iteration 158/1000 | Loss: 0.00003309
Iteration 159/1000 | Loss: 0.00003309
Iteration 160/1000 | Loss: 0.00003308
Iteration 161/1000 | Loss: 0.00003308
Iteration 162/1000 | Loss: 0.00003308
Iteration 163/1000 | Loss: 0.00003308
Iteration 164/1000 | Loss: 0.00003308
Iteration 165/1000 | Loss: 0.00003308
Iteration 166/1000 | Loss: 0.00003308
Iteration 167/1000 | Loss: 0.00003308
Iteration 168/1000 | Loss: 0.00003308
Iteration 169/1000 | Loss: 0.00003308
Iteration 170/1000 | Loss: 0.00003308
Iteration 171/1000 | Loss: 0.00003308
Iteration 172/1000 | Loss: 0.00003308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [3.308484883746132e-05, 3.308484883746132e-05, 3.308484883746132e-05, 3.308484883746132e-05, 3.308484883746132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.308484883746132e-05

Optimization complete. Final v2v error: 4.708211421966553 mm

Highest mean error: 5.908661842346191 mm for frame 0

Lowest mean error: 4.388850688934326 mm for frame 20

Saving results

Total time: 51.436397075653076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517992
Iteration 2/25 | Loss: 0.00154859
Iteration 3/25 | Loss: 0.00123145
Iteration 4/25 | Loss: 0.00119956
Iteration 5/25 | Loss: 0.00119443
Iteration 6/25 | Loss: 0.00119285
Iteration 7/25 | Loss: 0.00119282
Iteration 8/25 | Loss: 0.00119282
Iteration 9/25 | Loss: 0.00119282
Iteration 10/25 | Loss: 0.00119282
Iteration 11/25 | Loss: 0.00119282
Iteration 12/25 | Loss: 0.00119282
Iteration 13/25 | Loss: 0.00119282
Iteration 14/25 | Loss: 0.00119282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011928186286240816, 0.0011928186286240816, 0.0011928186286240816, 0.0011928186286240816, 0.0011928186286240816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011928186286240816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19396281
Iteration 2/25 | Loss: 0.00148903
Iteration 3/25 | Loss: 0.00148903
Iteration 4/25 | Loss: 0.00148902
Iteration 5/25 | Loss: 0.00148902
Iteration 6/25 | Loss: 0.00148902
Iteration 7/25 | Loss: 0.00148902
Iteration 8/25 | Loss: 0.00148902
Iteration 9/25 | Loss: 0.00148902
Iteration 10/25 | Loss: 0.00148902
Iteration 11/25 | Loss: 0.00148902
Iteration 12/25 | Loss: 0.00148902
Iteration 13/25 | Loss: 0.00148902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0014890237944200635, 0.0014890237944200635, 0.0014890237944200635, 0.0014890237944200635, 0.0014890237944200635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014890237944200635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148902
Iteration 2/1000 | Loss: 0.00003586
Iteration 3/1000 | Loss: 0.00002259
Iteration 4/1000 | Loss: 0.00001762
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001465
Iteration 9/1000 | Loss: 0.00001428
Iteration 10/1000 | Loss: 0.00001395
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001355
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001345
Iteration 15/1000 | Loss: 0.00001343
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001332
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001332
Iteration 26/1000 | Loss: 0.00001332
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001329
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001329
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001325
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001325
Iteration 60/1000 | Loss: 0.00001325
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001324
Iteration 67/1000 | Loss: 0.00001324
Iteration 68/1000 | Loss: 0.00001324
Iteration 69/1000 | Loss: 0.00001324
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001323
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001322
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001321
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001320
Iteration 88/1000 | Loss: 0.00001320
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001319
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001318
Iteration 97/1000 | Loss: 0.00001318
Iteration 98/1000 | Loss: 0.00001318
Iteration 99/1000 | Loss: 0.00001318
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001317
Iteration 102/1000 | Loss: 0.00001317
Iteration 103/1000 | Loss: 0.00001317
Iteration 104/1000 | Loss: 0.00001317
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001316
Iteration 109/1000 | Loss: 0.00001316
Iteration 110/1000 | Loss: 0.00001316
Iteration 111/1000 | Loss: 0.00001316
Iteration 112/1000 | Loss: 0.00001316
Iteration 113/1000 | Loss: 0.00001316
Iteration 114/1000 | Loss: 0.00001316
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001315
Iteration 119/1000 | Loss: 0.00001315
Iteration 120/1000 | Loss: 0.00001315
Iteration 121/1000 | Loss: 0.00001315
Iteration 122/1000 | Loss: 0.00001315
Iteration 123/1000 | Loss: 0.00001315
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001314
Iteration 127/1000 | Loss: 0.00001314
Iteration 128/1000 | Loss: 0.00001314
Iteration 129/1000 | Loss: 0.00001314
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001314
Iteration 132/1000 | Loss: 0.00001314
Iteration 133/1000 | Loss: 0.00001314
Iteration 134/1000 | Loss: 0.00001314
Iteration 135/1000 | Loss: 0.00001314
Iteration 136/1000 | Loss: 0.00001314
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001313
Iteration 141/1000 | Loss: 0.00001313
Iteration 142/1000 | Loss: 0.00001313
Iteration 143/1000 | Loss: 0.00001313
Iteration 144/1000 | Loss: 0.00001313
Iteration 145/1000 | Loss: 0.00001313
Iteration 146/1000 | Loss: 0.00001313
Iteration 147/1000 | Loss: 0.00001313
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001313
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001313
Iteration 164/1000 | Loss: 0.00001313
Iteration 165/1000 | Loss: 0.00001313
Iteration 166/1000 | Loss: 0.00001313
Iteration 167/1000 | Loss: 0.00001313
Iteration 168/1000 | Loss: 0.00001313
Iteration 169/1000 | Loss: 0.00001313
Iteration 170/1000 | Loss: 0.00001313
Iteration 171/1000 | Loss: 0.00001313
Iteration 172/1000 | Loss: 0.00001313
Iteration 173/1000 | Loss: 0.00001313
Iteration 174/1000 | Loss: 0.00001313
Iteration 175/1000 | Loss: 0.00001313
Iteration 176/1000 | Loss: 0.00001313
Iteration 177/1000 | Loss: 0.00001313
Iteration 178/1000 | Loss: 0.00001313
Iteration 179/1000 | Loss: 0.00001313
Iteration 180/1000 | Loss: 0.00001313
Iteration 181/1000 | Loss: 0.00001313
Iteration 182/1000 | Loss: 0.00001313
Iteration 183/1000 | Loss: 0.00001313
Iteration 184/1000 | Loss: 0.00001313
Iteration 185/1000 | Loss: 0.00001313
Iteration 186/1000 | Loss: 0.00001313
Iteration 187/1000 | Loss: 0.00001313
Iteration 188/1000 | Loss: 0.00001313
Iteration 189/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3129886610840913e-05, 1.3129886610840913e-05, 1.3129886610840913e-05, 1.3129886610840913e-05, 1.3129886610840913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3129886610840913e-05

Optimization complete. Final v2v error: 3.1779329776763916 mm

Highest mean error: 3.518869400024414 mm for frame 0

Lowest mean error: 2.928844690322876 mm for frame 13

Saving results

Total time: 38.14997935295105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00355707
Iteration 2/25 | Loss: 0.00141878
Iteration 3/25 | Loss: 0.00123745
Iteration 4/25 | Loss: 0.00118613
Iteration 5/25 | Loss: 0.00117382
Iteration 6/25 | Loss: 0.00117079
Iteration 7/25 | Loss: 0.00116663
Iteration 8/25 | Loss: 0.00116580
Iteration 9/25 | Loss: 0.00117253
Iteration 10/25 | Loss: 0.00116200
Iteration 11/25 | Loss: 0.00115812
Iteration 12/25 | Loss: 0.00115194
Iteration 13/25 | Loss: 0.00114989
Iteration 14/25 | Loss: 0.00114926
Iteration 15/25 | Loss: 0.00114912
Iteration 16/25 | Loss: 0.00114906
Iteration 17/25 | Loss: 0.00114906
Iteration 18/25 | Loss: 0.00114906
Iteration 19/25 | Loss: 0.00114906
Iteration 20/25 | Loss: 0.00114905
Iteration 21/25 | Loss: 0.00114905
Iteration 22/25 | Loss: 0.00114905
Iteration 23/25 | Loss: 0.00114905
Iteration 24/25 | Loss: 0.00114905
Iteration 25/25 | Loss: 0.00114905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21692932
Iteration 2/25 | Loss: 0.00144227
Iteration 3/25 | Loss: 0.00144227
Iteration 4/25 | Loss: 0.00144227
Iteration 5/25 | Loss: 0.00144227
Iteration 6/25 | Loss: 0.00144227
Iteration 7/25 | Loss: 0.00144227
Iteration 8/25 | Loss: 0.00144226
Iteration 9/25 | Loss: 0.00144226
Iteration 10/25 | Loss: 0.00144226
Iteration 11/25 | Loss: 0.00144226
Iteration 12/25 | Loss: 0.00144226
Iteration 13/25 | Loss: 0.00144226
Iteration 14/25 | Loss: 0.00144226
Iteration 15/25 | Loss: 0.00144226
Iteration 16/25 | Loss: 0.00144226
Iteration 17/25 | Loss: 0.00144226
Iteration 18/25 | Loss: 0.00144226
Iteration 19/25 | Loss: 0.00144226
Iteration 20/25 | Loss: 0.00144226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014422644162550569, 0.0014422644162550569, 0.0014422644162550569, 0.0014422644162550569, 0.0014422644162550569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014422644162550569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144226
Iteration 2/1000 | Loss: 0.00005159
Iteration 3/1000 | Loss: 0.00002690
Iteration 4/1000 | Loss: 0.00001795
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001536
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001461
Iteration 9/1000 | Loss: 0.00001431
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001384
Iteration 12/1000 | Loss: 0.00001366
Iteration 13/1000 | Loss: 0.00001361
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001354
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001348
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001346
Iteration 21/1000 | Loss: 0.00001345
Iteration 22/1000 | Loss: 0.00001345
Iteration 23/1000 | Loss: 0.00001345
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001341
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001338
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001334
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001332
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001330
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001327
Iteration 78/1000 | Loss: 0.00001327
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001326
Iteration 91/1000 | Loss: 0.00001326
Iteration 92/1000 | Loss: 0.00001326
Iteration 93/1000 | Loss: 0.00001326
Iteration 94/1000 | Loss: 0.00001326
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001324
Iteration 109/1000 | Loss: 0.00001324
Iteration 110/1000 | Loss: 0.00001324
Iteration 111/1000 | Loss: 0.00001324
Iteration 112/1000 | Loss: 0.00001324
Iteration 113/1000 | Loss: 0.00001324
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001322
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001321
Iteration 126/1000 | Loss: 0.00001321
Iteration 127/1000 | Loss: 0.00001321
Iteration 128/1000 | Loss: 0.00001321
Iteration 129/1000 | Loss: 0.00001321
Iteration 130/1000 | Loss: 0.00001321
Iteration 131/1000 | Loss: 0.00001321
Iteration 132/1000 | Loss: 0.00001320
Iteration 133/1000 | Loss: 0.00001320
Iteration 134/1000 | Loss: 0.00001320
Iteration 135/1000 | Loss: 0.00001320
Iteration 136/1000 | Loss: 0.00001320
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001318
Iteration 146/1000 | Loss: 0.00001318
Iteration 147/1000 | Loss: 0.00001318
Iteration 148/1000 | Loss: 0.00001318
Iteration 149/1000 | Loss: 0.00001317
Iteration 150/1000 | Loss: 0.00001317
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001317
Iteration 153/1000 | Loss: 0.00001317
Iteration 154/1000 | Loss: 0.00001316
Iteration 155/1000 | Loss: 0.00001316
Iteration 156/1000 | Loss: 0.00001316
Iteration 157/1000 | Loss: 0.00001316
Iteration 158/1000 | Loss: 0.00001316
Iteration 159/1000 | Loss: 0.00001316
Iteration 160/1000 | Loss: 0.00001316
Iteration 161/1000 | Loss: 0.00001315
Iteration 162/1000 | Loss: 0.00001315
Iteration 163/1000 | Loss: 0.00001315
Iteration 164/1000 | Loss: 0.00001315
Iteration 165/1000 | Loss: 0.00001315
Iteration 166/1000 | Loss: 0.00001315
Iteration 167/1000 | Loss: 0.00001315
Iteration 168/1000 | Loss: 0.00001315
Iteration 169/1000 | Loss: 0.00001315
Iteration 170/1000 | Loss: 0.00001315
Iteration 171/1000 | Loss: 0.00001315
Iteration 172/1000 | Loss: 0.00001315
Iteration 173/1000 | Loss: 0.00001315
Iteration 174/1000 | Loss: 0.00001315
Iteration 175/1000 | Loss: 0.00001314
Iteration 176/1000 | Loss: 0.00001314
Iteration 177/1000 | Loss: 0.00001314
Iteration 178/1000 | Loss: 0.00001314
Iteration 179/1000 | Loss: 0.00001314
Iteration 180/1000 | Loss: 0.00001314
Iteration 181/1000 | Loss: 0.00001314
Iteration 182/1000 | Loss: 0.00001314
Iteration 183/1000 | Loss: 0.00001314
Iteration 184/1000 | Loss: 0.00001314
Iteration 185/1000 | Loss: 0.00001314
Iteration 186/1000 | Loss: 0.00001313
Iteration 187/1000 | Loss: 0.00001313
Iteration 188/1000 | Loss: 0.00001313
Iteration 189/1000 | Loss: 0.00001313
Iteration 190/1000 | Loss: 0.00001313
Iteration 191/1000 | Loss: 0.00001313
Iteration 192/1000 | Loss: 0.00001313
Iteration 193/1000 | Loss: 0.00001313
Iteration 194/1000 | Loss: 0.00001313
Iteration 195/1000 | Loss: 0.00001313
Iteration 196/1000 | Loss: 0.00001313
Iteration 197/1000 | Loss: 0.00001313
Iteration 198/1000 | Loss: 0.00001313
Iteration 199/1000 | Loss: 0.00001313
Iteration 200/1000 | Loss: 0.00001313
Iteration 201/1000 | Loss: 0.00001313
Iteration 202/1000 | Loss: 0.00001313
Iteration 203/1000 | Loss: 0.00001313
Iteration 204/1000 | Loss: 0.00001313
Iteration 205/1000 | Loss: 0.00001313
Iteration 206/1000 | Loss: 0.00001312
Iteration 207/1000 | Loss: 0.00001312
Iteration 208/1000 | Loss: 0.00001312
Iteration 209/1000 | Loss: 0.00001312
Iteration 210/1000 | Loss: 0.00001312
Iteration 211/1000 | Loss: 0.00001312
Iteration 212/1000 | Loss: 0.00001312
Iteration 213/1000 | Loss: 0.00001312
Iteration 214/1000 | Loss: 0.00001312
Iteration 215/1000 | Loss: 0.00001312
Iteration 216/1000 | Loss: 0.00001312
Iteration 217/1000 | Loss: 0.00001312
Iteration 218/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.3124667020747438e-05, 1.3124667020747438e-05, 1.3124667020747438e-05, 1.3124667020747438e-05, 1.3124667020747438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3124667020747438e-05

Optimization complete. Final v2v error: 3.12369704246521 mm

Highest mean error: 3.5973525047302246 mm for frame 33

Lowest mean error: 2.7132792472839355 mm for frame 48

Saving results

Total time: 59.79645133018494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972159
Iteration 2/25 | Loss: 0.00156124
Iteration 3/25 | Loss: 0.00127089
Iteration 4/25 | Loss: 0.00123425
Iteration 5/25 | Loss: 0.00123090
Iteration 6/25 | Loss: 0.00123980
Iteration 7/25 | Loss: 0.00123564
Iteration 8/25 | Loss: 0.00122847
Iteration 9/25 | Loss: 0.00122621
Iteration 10/25 | Loss: 0.00122008
Iteration 11/25 | Loss: 0.00121619
Iteration 12/25 | Loss: 0.00120923
Iteration 13/25 | Loss: 0.00121005
Iteration 14/25 | Loss: 0.00121337
Iteration 15/25 | Loss: 0.00120647
Iteration 16/25 | Loss: 0.00119887
Iteration 17/25 | Loss: 0.00119678
Iteration 18/25 | Loss: 0.00119850
Iteration 19/25 | Loss: 0.00119780
Iteration 20/25 | Loss: 0.00119377
Iteration 21/25 | Loss: 0.00119279
Iteration 22/25 | Loss: 0.00119230
Iteration 23/25 | Loss: 0.00119240
Iteration 24/25 | Loss: 0.00119228
Iteration 25/25 | Loss: 0.00119225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53246057
Iteration 2/25 | Loss: 0.00096940
Iteration 3/25 | Loss: 0.00096939
Iteration 4/25 | Loss: 0.00096939
Iteration 5/25 | Loss: 0.00096939
Iteration 6/25 | Loss: 0.00096939
Iteration 7/25 | Loss: 0.00096939
Iteration 8/25 | Loss: 0.00096939
Iteration 9/25 | Loss: 0.00096939
Iteration 10/25 | Loss: 0.00096939
Iteration 11/25 | Loss: 0.00096939
Iteration 12/25 | Loss: 0.00096939
Iteration 13/25 | Loss: 0.00096939
Iteration 14/25 | Loss: 0.00096939
Iteration 15/25 | Loss: 0.00096939
Iteration 16/25 | Loss: 0.00096939
Iteration 17/25 | Loss: 0.00096939
Iteration 18/25 | Loss: 0.00096939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009693910833448172, 0.0009693910833448172, 0.0009693910833448172, 0.0009693910833448172, 0.0009693910833448172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009693910833448172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096939
Iteration 2/1000 | Loss: 0.00010031
Iteration 3/1000 | Loss: 0.00006811
Iteration 4/1000 | Loss: 0.00007172
Iteration 5/1000 | Loss: 0.00006036
Iteration 6/1000 | Loss: 0.00006207
Iteration 7/1000 | Loss: 0.00021337
Iteration 8/1000 | Loss: 0.00005714
Iteration 9/1000 | Loss: 0.00005095
Iteration 10/1000 | Loss: 0.00005480
Iteration 11/1000 | Loss: 0.00151503
Iteration 12/1000 | Loss: 0.00016826
Iteration 13/1000 | Loss: 0.00077065
Iteration 14/1000 | Loss: 0.00016347
Iteration 15/1000 | Loss: 0.00069728
Iteration 16/1000 | Loss: 0.00303177
Iteration 17/1000 | Loss: 0.00233161
Iteration 18/1000 | Loss: 0.00161137
Iteration 19/1000 | Loss: 0.00146005
Iteration 20/1000 | Loss: 0.00125768
Iteration 21/1000 | Loss: 0.00083637
Iteration 22/1000 | Loss: 0.00080491
Iteration 23/1000 | Loss: 0.00011021
Iteration 24/1000 | Loss: 0.00029569
Iteration 25/1000 | Loss: 0.00026801
Iteration 26/1000 | Loss: 0.00057744
Iteration 27/1000 | Loss: 0.00041431
Iteration 28/1000 | Loss: 0.00045382
Iteration 29/1000 | Loss: 0.00015990
Iteration 30/1000 | Loss: 0.00030295
Iteration 31/1000 | Loss: 0.00031952
Iteration 32/1000 | Loss: 0.00024160
Iteration 33/1000 | Loss: 0.00007397
Iteration 34/1000 | Loss: 0.00028140
Iteration 35/1000 | Loss: 0.00021278
Iteration 36/1000 | Loss: 0.00035355
Iteration 37/1000 | Loss: 0.00020158
Iteration 38/1000 | Loss: 0.00035039
Iteration 39/1000 | Loss: 0.00020689
Iteration 40/1000 | Loss: 0.00031968
Iteration 41/1000 | Loss: 0.00007359
Iteration 42/1000 | Loss: 0.00004543
Iteration 43/1000 | Loss: 0.00065037
Iteration 44/1000 | Loss: 0.00038554
Iteration 45/1000 | Loss: 0.00006021
Iteration 46/1000 | Loss: 0.00009676
Iteration 47/1000 | Loss: 0.00007980
Iteration 48/1000 | Loss: 0.00007020
Iteration 49/1000 | Loss: 0.00011166
Iteration 50/1000 | Loss: 0.00023750
Iteration 51/1000 | Loss: 0.00025868
Iteration 52/1000 | Loss: 0.00018595
Iteration 53/1000 | Loss: 0.00028388
Iteration 54/1000 | Loss: 0.00006378
Iteration 55/1000 | Loss: 0.00005526
Iteration 56/1000 | Loss: 0.00009467
Iteration 57/1000 | Loss: 0.00006388
Iteration 58/1000 | Loss: 0.00005214
Iteration 59/1000 | Loss: 0.00005151
Iteration 60/1000 | Loss: 0.00022675
Iteration 61/1000 | Loss: 0.00004404
Iteration 62/1000 | Loss: 0.00004063
Iteration 63/1000 | Loss: 0.00005199
Iteration 64/1000 | Loss: 0.00004676
Iteration 65/1000 | Loss: 0.00004710
Iteration 66/1000 | Loss: 0.00040136
Iteration 67/1000 | Loss: 0.00006075
Iteration 68/1000 | Loss: 0.00009586
Iteration 69/1000 | Loss: 0.00008215
Iteration 70/1000 | Loss: 0.00006309
Iteration 71/1000 | Loss: 0.00012451
Iteration 72/1000 | Loss: 0.00004660
Iteration 73/1000 | Loss: 0.00003539
Iteration 74/1000 | Loss: 0.00016333
Iteration 75/1000 | Loss: 0.00003568
Iteration 76/1000 | Loss: 0.00003144
Iteration 77/1000 | Loss: 0.00003040
Iteration 78/1000 | Loss: 0.00002972
Iteration 79/1000 | Loss: 0.00007256
Iteration 80/1000 | Loss: 0.00003803
Iteration 81/1000 | Loss: 0.00002941
Iteration 82/1000 | Loss: 0.00002804
Iteration 83/1000 | Loss: 0.00002714
Iteration 84/1000 | Loss: 0.00002894
Iteration 85/1000 | Loss: 0.00002566
Iteration 86/1000 | Loss: 0.00002505
Iteration 87/1000 | Loss: 0.00002465
Iteration 88/1000 | Loss: 0.00002454
Iteration 89/1000 | Loss: 0.00002437
Iteration 90/1000 | Loss: 0.00002434
Iteration 91/1000 | Loss: 0.00002417
Iteration 92/1000 | Loss: 0.00002401
Iteration 93/1000 | Loss: 0.00002397
Iteration 94/1000 | Loss: 0.00002389
Iteration 95/1000 | Loss: 0.00002386
Iteration 96/1000 | Loss: 0.00002385
Iteration 97/1000 | Loss: 0.00002383
Iteration 98/1000 | Loss: 0.00002383
Iteration 99/1000 | Loss: 0.00002383
Iteration 100/1000 | Loss: 0.00002383
Iteration 101/1000 | Loss: 0.00002380
Iteration 102/1000 | Loss: 0.00002380
Iteration 103/1000 | Loss: 0.00002377
Iteration 104/1000 | Loss: 0.00002377
Iteration 105/1000 | Loss: 0.00002377
Iteration 106/1000 | Loss: 0.00002377
Iteration 107/1000 | Loss: 0.00002377
Iteration 108/1000 | Loss: 0.00002377
Iteration 109/1000 | Loss: 0.00002377
Iteration 110/1000 | Loss: 0.00002377
Iteration 111/1000 | Loss: 0.00002376
Iteration 112/1000 | Loss: 0.00002376
Iteration 113/1000 | Loss: 0.00002376
Iteration 114/1000 | Loss: 0.00002375
Iteration 115/1000 | Loss: 0.00002375
Iteration 116/1000 | Loss: 0.00002374
Iteration 117/1000 | Loss: 0.00002374
Iteration 118/1000 | Loss: 0.00002374
Iteration 119/1000 | Loss: 0.00002373
Iteration 120/1000 | Loss: 0.00002373
Iteration 121/1000 | Loss: 0.00002372
Iteration 122/1000 | Loss: 0.00002372
Iteration 123/1000 | Loss: 0.00002372
Iteration 124/1000 | Loss: 0.00002372
Iteration 125/1000 | Loss: 0.00002371
Iteration 126/1000 | Loss: 0.00002371
Iteration 127/1000 | Loss: 0.00002371
Iteration 128/1000 | Loss: 0.00002371
Iteration 129/1000 | Loss: 0.00002371
Iteration 130/1000 | Loss: 0.00002371
Iteration 131/1000 | Loss: 0.00002371
Iteration 132/1000 | Loss: 0.00002371
Iteration 133/1000 | Loss: 0.00002370
Iteration 134/1000 | Loss: 0.00002370
Iteration 135/1000 | Loss: 0.00002370
Iteration 136/1000 | Loss: 0.00002370
Iteration 137/1000 | Loss: 0.00002370
Iteration 138/1000 | Loss: 0.00002370
Iteration 139/1000 | Loss: 0.00002370
Iteration 140/1000 | Loss: 0.00002370
Iteration 141/1000 | Loss: 0.00002369
Iteration 142/1000 | Loss: 0.00002369
Iteration 143/1000 | Loss: 0.00002369
Iteration 144/1000 | Loss: 0.00002369
Iteration 145/1000 | Loss: 0.00002369
Iteration 146/1000 | Loss: 0.00002369
Iteration 147/1000 | Loss: 0.00002368
Iteration 148/1000 | Loss: 0.00002368
Iteration 149/1000 | Loss: 0.00002368
Iteration 150/1000 | Loss: 0.00002368
Iteration 151/1000 | Loss: 0.00002368
Iteration 152/1000 | Loss: 0.00002367
Iteration 153/1000 | Loss: 0.00002367
Iteration 154/1000 | Loss: 0.00002367
Iteration 155/1000 | Loss: 0.00002367
Iteration 156/1000 | Loss: 0.00002367
Iteration 157/1000 | Loss: 0.00002366
Iteration 158/1000 | Loss: 0.00002366
Iteration 159/1000 | Loss: 0.00002365
Iteration 160/1000 | Loss: 0.00002365
Iteration 161/1000 | Loss: 0.00002365
Iteration 162/1000 | Loss: 0.00002365
Iteration 163/1000 | Loss: 0.00002364
Iteration 164/1000 | Loss: 0.00002364
Iteration 165/1000 | Loss: 0.00002363
Iteration 166/1000 | Loss: 0.00002363
Iteration 167/1000 | Loss: 0.00002363
Iteration 168/1000 | Loss: 0.00002363
Iteration 169/1000 | Loss: 0.00002362
Iteration 170/1000 | Loss: 0.00002362
Iteration 171/1000 | Loss: 0.00002362
Iteration 172/1000 | Loss: 0.00002362
Iteration 173/1000 | Loss: 0.00002361
Iteration 174/1000 | Loss: 0.00002361
Iteration 175/1000 | Loss: 0.00002361
Iteration 176/1000 | Loss: 0.00002361
Iteration 177/1000 | Loss: 0.00002360
Iteration 178/1000 | Loss: 0.00002360
Iteration 179/1000 | Loss: 0.00002360
Iteration 180/1000 | Loss: 0.00002360
Iteration 181/1000 | Loss: 0.00002360
Iteration 182/1000 | Loss: 0.00002360
Iteration 183/1000 | Loss: 0.00002360
Iteration 184/1000 | Loss: 0.00002360
Iteration 185/1000 | Loss: 0.00002360
Iteration 186/1000 | Loss: 0.00002360
Iteration 187/1000 | Loss: 0.00002359
Iteration 188/1000 | Loss: 0.00002359
Iteration 189/1000 | Loss: 0.00002359
Iteration 190/1000 | Loss: 0.00002359
Iteration 191/1000 | Loss: 0.00002359
Iteration 192/1000 | Loss: 0.00002359
Iteration 193/1000 | Loss: 0.00002359
Iteration 194/1000 | Loss: 0.00002359
Iteration 195/1000 | Loss: 0.00002359
Iteration 196/1000 | Loss: 0.00002359
Iteration 197/1000 | Loss: 0.00002359
Iteration 198/1000 | Loss: 0.00002359
Iteration 199/1000 | Loss: 0.00002359
Iteration 200/1000 | Loss: 0.00002359
Iteration 201/1000 | Loss: 0.00002359
Iteration 202/1000 | Loss: 0.00002359
Iteration 203/1000 | Loss: 0.00002359
Iteration 204/1000 | Loss: 0.00002359
Iteration 205/1000 | Loss: 0.00002359
Iteration 206/1000 | Loss: 0.00002359
Iteration 207/1000 | Loss: 0.00002359
Iteration 208/1000 | Loss: 0.00002359
Iteration 209/1000 | Loss: 0.00002359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.358963683946058e-05, 2.358963683946058e-05, 2.358963683946058e-05, 2.358963683946058e-05, 2.358963683946058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.358963683946058e-05

Optimization complete. Final v2v error: 4.0887274742126465 mm

Highest mean error: 5.084207057952881 mm for frame 55

Lowest mean error: 3.374667167663574 mm for frame 115

Saving results

Total time: 210.62818360328674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110826
Iteration 2/25 | Loss: 0.00169253
Iteration 3/25 | Loss: 0.00141432
Iteration 4/25 | Loss: 0.00147670
Iteration 5/25 | Loss: 0.00132971
Iteration 6/25 | Loss: 0.00125420
Iteration 7/25 | Loss: 0.00118360
Iteration 8/25 | Loss: 0.00118908
Iteration 9/25 | Loss: 0.00129507
Iteration 10/25 | Loss: 0.00113586
Iteration 11/25 | Loss: 0.00112957
Iteration 12/25 | Loss: 0.00116494
Iteration 13/25 | Loss: 0.00112931
Iteration 14/25 | Loss: 0.00112767
Iteration 15/25 | Loss: 0.00112660
Iteration 16/25 | Loss: 0.00112619
Iteration 17/25 | Loss: 0.00115729
Iteration 18/25 | Loss: 0.00115983
Iteration 19/25 | Loss: 0.00116331
Iteration 20/25 | Loss: 0.00115163
Iteration 21/25 | Loss: 0.00116191
Iteration 22/25 | Loss: 0.00115615
Iteration 23/25 | Loss: 0.00115741
Iteration 24/25 | Loss: 0.00115906
Iteration 25/25 | Loss: 0.00114677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38702321
Iteration 2/25 | Loss: 0.00285593
Iteration 3/25 | Loss: 0.00285593
Iteration 4/25 | Loss: 0.00285593
Iteration 5/25 | Loss: 0.00285593
Iteration 6/25 | Loss: 0.00285593
Iteration 7/25 | Loss: 0.00285593
Iteration 8/25 | Loss: 0.00285593
Iteration 9/25 | Loss: 0.00285593
Iteration 10/25 | Loss: 0.00285593
Iteration 11/25 | Loss: 0.00285593
Iteration 12/25 | Loss: 0.00285593
Iteration 13/25 | Loss: 0.00285593
Iteration 14/25 | Loss: 0.00285593
Iteration 15/25 | Loss: 0.00285593
Iteration 16/25 | Loss: 0.00285593
Iteration 17/25 | Loss: 0.00285593
Iteration 18/25 | Loss: 0.00285593
Iteration 19/25 | Loss: 0.00285593
Iteration 20/25 | Loss: 0.00285593
Iteration 21/25 | Loss: 0.00285593
Iteration 22/25 | Loss: 0.00285593
Iteration 23/25 | Loss: 0.00285593
Iteration 24/25 | Loss: 0.00285593
Iteration 25/25 | Loss: 0.00285593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285593
Iteration 2/1000 | Loss: 0.00194658
Iteration 3/1000 | Loss: 0.00144716
Iteration 4/1000 | Loss: 0.00007065
Iteration 5/1000 | Loss: 0.00004604
Iteration 6/1000 | Loss: 0.00003733
Iteration 7/1000 | Loss: 0.00003442
Iteration 8/1000 | Loss: 0.00003210
Iteration 9/1000 | Loss: 0.00003084
Iteration 10/1000 | Loss: 0.00003005
Iteration 11/1000 | Loss: 0.00002935
Iteration 12/1000 | Loss: 0.00156540
Iteration 13/1000 | Loss: 0.00009774
Iteration 14/1000 | Loss: 0.00111069
Iteration 15/1000 | Loss: 0.00015546
Iteration 16/1000 | Loss: 0.00028857
Iteration 17/1000 | Loss: 0.00010306
Iteration 18/1000 | Loss: 0.00009355
Iteration 19/1000 | Loss: 0.00005903
Iteration 20/1000 | Loss: 0.00127422
Iteration 21/1000 | Loss: 0.00173680
Iteration 22/1000 | Loss: 0.00204367
Iteration 23/1000 | Loss: 0.00149821
Iteration 24/1000 | Loss: 0.00094978
Iteration 25/1000 | Loss: 0.00083755
Iteration 26/1000 | Loss: 0.00094119
Iteration 27/1000 | Loss: 0.00006739
Iteration 28/1000 | Loss: 0.00103660
Iteration 29/1000 | Loss: 0.00023191
Iteration 30/1000 | Loss: 0.00030218
Iteration 31/1000 | Loss: 0.00003784
Iteration 32/1000 | Loss: 0.00176672
Iteration 33/1000 | Loss: 0.00109687
Iteration 34/1000 | Loss: 0.00115370
Iteration 35/1000 | Loss: 0.00063794
Iteration 36/1000 | Loss: 0.00042357
Iteration 37/1000 | Loss: 0.00004347
Iteration 38/1000 | Loss: 0.00002429
Iteration 39/1000 | Loss: 0.00002065
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00012338
Iteration 42/1000 | Loss: 0.00002249
Iteration 43/1000 | Loss: 0.00001877
Iteration 44/1000 | Loss: 0.00001766
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00024315
Iteration 47/1000 | Loss: 0.00004046
Iteration 48/1000 | Loss: 0.00024623
Iteration 49/1000 | Loss: 0.00006873
Iteration 50/1000 | Loss: 0.00024459
Iteration 51/1000 | Loss: 0.00003713
Iteration 52/1000 | Loss: 0.00002118
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001589
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001470
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001418
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001414
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001413
Iteration 69/1000 | Loss: 0.00001413
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001411
Iteration 78/1000 | Loss: 0.00001411
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001411
Iteration 83/1000 | Loss: 0.00001411
Iteration 84/1000 | Loss: 0.00001411
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001410
Iteration 87/1000 | Loss: 0.00001410
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001407
Iteration 91/1000 | Loss: 0.00001407
Iteration 92/1000 | Loss: 0.00001407
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001406
Iteration 95/1000 | Loss: 0.00001406
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001405
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001405
Iteration 100/1000 | Loss: 0.00001404
Iteration 101/1000 | Loss: 0.00001404
Iteration 102/1000 | Loss: 0.00001404
Iteration 103/1000 | Loss: 0.00001404
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001402
Iteration 113/1000 | Loss: 0.00001402
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001402
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001401
Iteration 120/1000 | Loss: 0.00001401
Iteration 121/1000 | Loss: 0.00001401
Iteration 122/1000 | Loss: 0.00001401
Iteration 123/1000 | Loss: 0.00001401
Iteration 124/1000 | Loss: 0.00001401
Iteration 125/1000 | Loss: 0.00001400
Iteration 126/1000 | Loss: 0.00001400
Iteration 127/1000 | Loss: 0.00001400
Iteration 128/1000 | Loss: 0.00001400
Iteration 129/1000 | Loss: 0.00001400
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Iteration 132/1000 | Loss: 0.00001400
Iteration 133/1000 | Loss: 0.00001400
Iteration 134/1000 | Loss: 0.00001400
Iteration 135/1000 | Loss: 0.00001400
Iteration 136/1000 | Loss: 0.00001399
Iteration 137/1000 | Loss: 0.00001399
Iteration 138/1000 | Loss: 0.00001399
Iteration 139/1000 | Loss: 0.00001399
Iteration 140/1000 | Loss: 0.00001399
Iteration 141/1000 | Loss: 0.00001399
Iteration 142/1000 | Loss: 0.00001399
Iteration 143/1000 | Loss: 0.00001399
Iteration 144/1000 | Loss: 0.00001399
Iteration 145/1000 | Loss: 0.00001399
Iteration 146/1000 | Loss: 0.00001399
Iteration 147/1000 | Loss: 0.00001399
Iteration 148/1000 | Loss: 0.00001399
Iteration 149/1000 | Loss: 0.00001399
Iteration 150/1000 | Loss: 0.00001399
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Iteration 156/1000 | Loss: 0.00001398
Iteration 157/1000 | Loss: 0.00001398
Iteration 158/1000 | Loss: 0.00001398
Iteration 159/1000 | Loss: 0.00001398
Iteration 160/1000 | Loss: 0.00001398
Iteration 161/1000 | Loss: 0.00001398
Iteration 162/1000 | Loss: 0.00001398
Iteration 163/1000 | Loss: 0.00001398
Iteration 164/1000 | Loss: 0.00001398
Iteration 165/1000 | Loss: 0.00001398
Iteration 166/1000 | Loss: 0.00001398
Iteration 167/1000 | Loss: 0.00001398
Iteration 168/1000 | Loss: 0.00001398
Iteration 169/1000 | Loss: 0.00001398
Iteration 170/1000 | Loss: 0.00001398
Iteration 171/1000 | Loss: 0.00001398
Iteration 172/1000 | Loss: 0.00001398
Iteration 173/1000 | Loss: 0.00001398
Iteration 174/1000 | Loss: 0.00001398
Iteration 175/1000 | Loss: 0.00001398
Iteration 176/1000 | Loss: 0.00001398
Iteration 177/1000 | Loss: 0.00001398
Iteration 178/1000 | Loss: 0.00001398
Iteration 179/1000 | Loss: 0.00001398
Iteration 180/1000 | Loss: 0.00001398
Iteration 181/1000 | Loss: 0.00001398
Iteration 182/1000 | Loss: 0.00001398
Iteration 183/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.3982634300191421e-05, 1.3982634300191421e-05, 1.3982634300191421e-05, 1.3982634300191421e-05, 1.3982634300191421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3982634300191421e-05

Optimization complete. Final v2v error: 3.2172048091888428 mm

Highest mean error: 8.76873779296875 mm for frame 66

Lowest mean error: 2.903310537338257 mm for frame 100

Saving results

Total time: 137.30296635627747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934220
Iteration 2/25 | Loss: 0.00164528
Iteration 3/25 | Loss: 0.00130363
Iteration 4/25 | Loss: 0.00126475
Iteration 5/25 | Loss: 0.00126033
Iteration 6/25 | Loss: 0.00125947
Iteration 7/25 | Loss: 0.00125927
Iteration 8/25 | Loss: 0.00125927
Iteration 9/25 | Loss: 0.00125927
Iteration 10/25 | Loss: 0.00125927
Iteration 11/25 | Loss: 0.00125927
Iteration 12/25 | Loss: 0.00125927
Iteration 13/25 | Loss: 0.00125927
Iteration 14/25 | Loss: 0.00125927
Iteration 15/25 | Loss: 0.00125927
Iteration 16/25 | Loss: 0.00125927
Iteration 17/25 | Loss: 0.00125927
Iteration 18/25 | Loss: 0.00125927
Iteration 19/25 | Loss: 0.00125927
Iteration 20/25 | Loss: 0.00125927
Iteration 21/25 | Loss: 0.00125927
Iteration 22/25 | Loss: 0.00125927
Iteration 23/25 | Loss: 0.00125927
Iteration 24/25 | Loss: 0.00125927
Iteration 25/25 | Loss: 0.00125927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79260021
Iteration 2/25 | Loss: 0.00076487
Iteration 3/25 | Loss: 0.00076487
Iteration 4/25 | Loss: 0.00076487
Iteration 5/25 | Loss: 0.00076487
Iteration 6/25 | Loss: 0.00076487
Iteration 7/25 | Loss: 0.00076487
Iteration 8/25 | Loss: 0.00076486
Iteration 9/25 | Loss: 0.00076486
Iteration 10/25 | Loss: 0.00076486
Iteration 11/25 | Loss: 0.00076486
Iteration 12/25 | Loss: 0.00076486
Iteration 13/25 | Loss: 0.00076486
Iteration 14/25 | Loss: 0.00076486
Iteration 15/25 | Loss: 0.00076486
Iteration 16/25 | Loss: 0.00076486
Iteration 17/25 | Loss: 0.00076486
Iteration 18/25 | Loss: 0.00076486
Iteration 19/25 | Loss: 0.00076486
Iteration 20/25 | Loss: 0.00076486
Iteration 21/25 | Loss: 0.00076486
Iteration 22/25 | Loss: 0.00076486
Iteration 23/25 | Loss: 0.00076486
Iteration 24/25 | Loss: 0.00076486
Iteration 25/25 | Loss: 0.00076486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076486
Iteration 2/1000 | Loss: 0.00005400
Iteration 3/1000 | Loss: 0.00003969
Iteration 4/1000 | Loss: 0.00003687
Iteration 5/1000 | Loss: 0.00003484
Iteration 6/1000 | Loss: 0.00003358
Iteration 7/1000 | Loss: 0.00003249
Iteration 8/1000 | Loss: 0.00003188
Iteration 9/1000 | Loss: 0.00003155
Iteration 10/1000 | Loss: 0.00003116
Iteration 11/1000 | Loss: 0.00003095
Iteration 12/1000 | Loss: 0.00003074
Iteration 13/1000 | Loss: 0.00003071
Iteration 14/1000 | Loss: 0.00003060
Iteration 15/1000 | Loss: 0.00003056
Iteration 16/1000 | Loss: 0.00003056
Iteration 17/1000 | Loss: 0.00003056
Iteration 18/1000 | Loss: 0.00003055
Iteration 19/1000 | Loss: 0.00003055
Iteration 20/1000 | Loss: 0.00003055
Iteration 21/1000 | Loss: 0.00003055
Iteration 22/1000 | Loss: 0.00003055
Iteration 23/1000 | Loss: 0.00003055
Iteration 24/1000 | Loss: 0.00003055
Iteration 25/1000 | Loss: 0.00003055
Iteration 26/1000 | Loss: 0.00003055
Iteration 27/1000 | Loss: 0.00003055
Iteration 28/1000 | Loss: 0.00003055
Iteration 29/1000 | Loss: 0.00003055
Iteration 30/1000 | Loss: 0.00003054
Iteration 31/1000 | Loss: 0.00003054
Iteration 32/1000 | Loss: 0.00003054
Iteration 33/1000 | Loss: 0.00003054
Iteration 34/1000 | Loss: 0.00003054
Iteration 35/1000 | Loss: 0.00003054
Iteration 36/1000 | Loss: 0.00003053
Iteration 37/1000 | Loss: 0.00003053
Iteration 38/1000 | Loss: 0.00003053
Iteration 39/1000 | Loss: 0.00003053
Iteration 40/1000 | Loss: 0.00003053
Iteration 41/1000 | Loss: 0.00003053
Iteration 42/1000 | Loss: 0.00003053
Iteration 43/1000 | Loss: 0.00003053
Iteration 44/1000 | Loss: 0.00003053
Iteration 45/1000 | Loss: 0.00003052
Iteration 46/1000 | Loss: 0.00003052
Iteration 47/1000 | Loss: 0.00003052
Iteration 48/1000 | Loss: 0.00003052
Iteration 49/1000 | Loss: 0.00003052
Iteration 50/1000 | Loss: 0.00003051
Iteration 51/1000 | Loss: 0.00003051
Iteration 52/1000 | Loss: 0.00003051
Iteration 53/1000 | Loss: 0.00003051
Iteration 54/1000 | Loss: 0.00003051
Iteration 55/1000 | Loss: 0.00003051
Iteration 56/1000 | Loss: 0.00003050
Iteration 57/1000 | Loss: 0.00003050
Iteration 58/1000 | Loss: 0.00003050
Iteration 59/1000 | Loss: 0.00003050
Iteration 60/1000 | Loss: 0.00003050
Iteration 61/1000 | Loss: 0.00003050
Iteration 62/1000 | Loss: 0.00003050
Iteration 63/1000 | Loss: 0.00003050
Iteration 64/1000 | Loss: 0.00003048
Iteration 65/1000 | Loss: 0.00003048
Iteration 66/1000 | Loss: 0.00003047
Iteration 67/1000 | Loss: 0.00003047
Iteration 68/1000 | Loss: 0.00003047
Iteration 69/1000 | Loss: 0.00003047
Iteration 70/1000 | Loss: 0.00003047
Iteration 71/1000 | Loss: 0.00003047
Iteration 72/1000 | Loss: 0.00003047
Iteration 73/1000 | Loss: 0.00003047
Iteration 74/1000 | Loss: 0.00003047
Iteration 75/1000 | Loss: 0.00003047
Iteration 76/1000 | Loss: 0.00003047
Iteration 77/1000 | Loss: 0.00003047
Iteration 78/1000 | Loss: 0.00003047
Iteration 79/1000 | Loss: 0.00003047
Iteration 80/1000 | Loss: 0.00003047
Iteration 81/1000 | Loss: 0.00003047
Iteration 82/1000 | Loss: 0.00003047
Iteration 83/1000 | Loss: 0.00003047
Iteration 84/1000 | Loss: 0.00003047
Iteration 85/1000 | Loss: 0.00003047
Iteration 86/1000 | Loss: 0.00003047
Iteration 87/1000 | Loss: 0.00003047
Iteration 88/1000 | Loss: 0.00003047
Iteration 89/1000 | Loss: 0.00003047
Iteration 90/1000 | Loss: 0.00003047
Iteration 91/1000 | Loss: 0.00003047
Iteration 92/1000 | Loss: 0.00003047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [3.046672463824507e-05, 3.046672463824507e-05, 3.046672463824507e-05, 3.046672463824507e-05, 3.046672463824507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.046672463824507e-05

Optimization complete. Final v2v error: 4.672211170196533 mm

Highest mean error: 4.772790908813477 mm for frame 1

Lowest mean error: 4.505243301391602 mm for frame 127

Saving results

Total time: 34.80909776687622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548769
Iteration 2/25 | Loss: 0.00150997
Iteration 3/25 | Loss: 0.00123559
Iteration 4/25 | Loss: 0.00121908
Iteration 5/25 | Loss: 0.00121492
Iteration 6/25 | Loss: 0.00121347
Iteration 7/25 | Loss: 0.00121330
Iteration 8/25 | Loss: 0.00121330
Iteration 9/25 | Loss: 0.00121330
Iteration 10/25 | Loss: 0.00121330
Iteration 11/25 | Loss: 0.00121330
Iteration 12/25 | Loss: 0.00121330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012133007403463125, 0.0012133007403463125, 0.0012133007403463125, 0.0012133007403463125, 0.0012133007403463125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012133007403463125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84335732
Iteration 2/25 | Loss: 0.00113163
Iteration 3/25 | Loss: 0.00113163
Iteration 4/25 | Loss: 0.00113163
Iteration 5/25 | Loss: 0.00113163
Iteration 6/25 | Loss: 0.00113163
Iteration 7/25 | Loss: 0.00113163
Iteration 8/25 | Loss: 0.00113163
Iteration 9/25 | Loss: 0.00113163
Iteration 10/25 | Loss: 0.00113163
Iteration 11/25 | Loss: 0.00113163
Iteration 12/25 | Loss: 0.00113163
Iteration 13/25 | Loss: 0.00113163
Iteration 14/25 | Loss: 0.00113163
Iteration 15/25 | Loss: 0.00113163
Iteration 16/25 | Loss: 0.00113163
Iteration 17/25 | Loss: 0.00113163
Iteration 18/25 | Loss: 0.00113163
Iteration 19/25 | Loss: 0.00113163
Iteration 20/25 | Loss: 0.00113163
Iteration 21/25 | Loss: 0.00113163
Iteration 22/25 | Loss: 0.00113163
Iteration 23/25 | Loss: 0.00113163
Iteration 24/25 | Loss: 0.00113163
Iteration 25/25 | Loss: 0.00113163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113163
Iteration 2/1000 | Loss: 0.00007093
Iteration 3/1000 | Loss: 0.00004496
Iteration 4/1000 | Loss: 0.00003565
Iteration 5/1000 | Loss: 0.00003347
Iteration 6/1000 | Loss: 0.00003200
Iteration 7/1000 | Loss: 0.00003088
Iteration 8/1000 | Loss: 0.00003028
Iteration 9/1000 | Loss: 0.00002963
Iteration 10/1000 | Loss: 0.00002912
Iteration 11/1000 | Loss: 0.00002876
Iteration 12/1000 | Loss: 0.00002843
Iteration 13/1000 | Loss: 0.00002818
Iteration 14/1000 | Loss: 0.00002799
Iteration 15/1000 | Loss: 0.00002777
Iteration 16/1000 | Loss: 0.00002761
Iteration 17/1000 | Loss: 0.00002751
Iteration 18/1000 | Loss: 0.00002743
Iteration 19/1000 | Loss: 0.00002732
Iteration 20/1000 | Loss: 0.00002731
Iteration 21/1000 | Loss: 0.00002728
Iteration 22/1000 | Loss: 0.00002725
Iteration 23/1000 | Loss: 0.00002720
Iteration 24/1000 | Loss: 0.00002719
Iteration 25/1000 | Loss: 0.00002717
Iteration 26/1000 | Loss: 0.00002714
Iteration 27/1000 | Loss: 0.00002711
Iteration 28/1000 | Loss: 0.00002708
Iteration 29/1000 | Loss: 0.00002707
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002701
Iteration 32/1000 | Loss: 0.00002697
Iteration 33/1000 | Loss: 0.00002696
Iteration 34/1000 | Loss: 0.00002694
Iteration 35/1000 | Loss: 0.00002694
Iteration 36/1000 | Loss: 0.00002694
Iteration 37/1000 | Loss: 0.00002693
Iteration 38/1000 | Loss: 0.00002692
Iteration 39/1000 | Loss: 0.00002691
Iteration 40/1000 | Loss: 0.00002690
Iteration 41/1000 | Loss: 0.00002690
Iteration 42/1000 | Loss: 0.00002690
Iteration 43/1000 | Loss: 0.00002690
Iteration 44/1000 | Loss: 0.00002690
Iteration 45/1000 | Loss: 0.00002690
Iteration 46/1000 | Loss: 0.00002690
Iteration 47/1000 | Loss: 0.00002688
Iteration 48/1000 | Loss: 0.00002688
Iteration 49/1000 | Loss: 0.00002688
Iteration 50/1000 | Loss: 0.00002687
Iteration 51/1000 | Loss: 0.00002686
Iteration 52/1000 | Loss: 0.00002686
Iteration 53/1000 | Loss: 0.00002686
Iteration 54/1000 | Loss: 0.00002686
Iteration 55/1000 | Loss: 0.00002686
Iteration 56/1000 | Loss: 0.00002683
Iteration 57/1000 | Loss: 0.00002683
Iteration 58/1000 | Loss: 0.00002683
Iteration 59/1000 | Loss: 0.00002683
Iteration 60/1000 | Loss: 0.00002683
Iteration 61/1000 | Loss: 0.00002683
Iteration 62/1000 | Loss: 0.00002683
Iteration 63/1000 | Loss: 0.00002683
Iteration 64/1000 | Loss: 0.00002683
Iteration 65/1000 | Loss: 0.00002683
Iteration 66/1000 | Loss: 0.00002683
Iteration 67/1000 | Loss: 0.00002683
Iteration 68/1000 | Loss: 0.00002683
Iteration 69/1000 | Loss: 0.00002683
Iteration 70/1000 | Loss: 0.00002682
Iteration 71/1000 | Loss: 0.00002682
Iteration 72/1000 | Loss: 0.00002682
Iteration 73/1000 | Loss: 0.00002682
Iteration 74/1000 | Loss: 0.00002681
Iteration 75/1000 | Loss: 0.00002681
Iteration 76/1000 | Loss: 0.00002681
Iteration 77/1000 | Loss: 0.00002680
Iteration 78/1000 | Loss: 0.00002680
Iteration 79/1000 | Loss: 0.00002680
Iteration 80/1000 | Loss: 0.00002680
Iteration 81/1000 | Loss: 0.00002680
Iteration 82/1000 | Loss: 0.00002680
Iteration 83/1000 | Loss: 0.00002680
Iteration 84/1000 | Loss: 0.00002680
Iteration 85/1000 | Loss: 0.00002679
Iteration 86/1000 | Loss: 0.00002679
Iteration 87/1000 | Loss: 0.00002679
Iteration 88/1000 | Loss: 0.00002679
Iteration 89/1000 | Loss: 0.00002679
Iteration 90/1000 | Loss: 0.00002679
Iteration 91/1000 | Loss: 0.00002679
Iteration 92/1000 | Loss: 0.00002679
Iteration 93/1000 | Loss: 0.00002678
Iteration 94/1000 | Loss: 0.00002678
Iteration 95/1000 | Loss: 0.00002678
Iteration 96/1000 | Loss: 0.00002678
Iteration 97/1000 | Loss: 0.00002678
Iteration 98/1000 | Loss: 0.00002678
Iteration 99/1000 | Loss: 0.00002678
Iteration 100/1000 | Loss: 0.00002678
Iteration 101/1000 | Loss: 0.00002678
Iteration 102/1000 | Loss: 0.00002678
Iteration 103/1000 | Loss: 0.00002678
Iteration 104/1000 | Loss: 0.00002678
Iteration 105/1000 | Loss: 0.00002678
Iteration 106/1000 | Loss: 0.00002678
Iteration 107/1000 | Loss: 0.00002678
Iteration 108/1000 | Loss: 0.00002678
Iteration 109/1000 | Loss: 0.00002678
Iteration 110/1000 | Loss: 0.00002678
Iteration 111/1000 | Loss: 0.00002677
Iteration 112/1000 | Loss: 0.00002677
Iteration 113/1000 | Loss: 0.00002677
Iteration 114/1000 | Loss: 0.00002677
Iteration 115/1000 | Loss: 0.00002676
Iteration 116/1000 | Loss: 0.00002676
Iteration 117/1000 | Loss: 0.00002676
Iteration 118/1000 | Loss: 0.00002676
Iteration 119/1000 | Loss: 0.00002676
Iteration 120/1000 | Loss: 0.00002676
Iteration 121/1000 | Loss: 0.00002676
Iteration 122/1000 | Loss: 0.00002676
Iteration 123/1000 | Loss: 0.00002675
Iteration 124/1000 | Loss: 0.00002675
Iteration 125/1000 | Loss: 0.00002675
Iteration 126/1000 | Loss: 0.00002675
Iteration 127/1000 | Loss: 0.00002675
Iteration 128/1000 | Loss: 0.00002675
Iteration 129/1000 | Loss: 0.00002675
Iteration 130/1000 | Loss: 0.00002675
Iteration 131/1000 | Loss: 0.00002674
Iteration 132/1000 | Loss: 0.00002674
Iteration 133/1000 | Loss: 0.00002674
Iteration 134/1000 | Loss: 0.00002674
Iteration 135/1000 | Loss: 0.00002674
Iteration 136/1000 | Loss: 0.00002673
Iteration 137/1000 | Loss: 0.00002673
Iteration 138/1000 | Loss: 0.00002673
Iteration 139/1000 | Loss: 0.00002673
Iteration 140/1000 | Loss: 0.00002673
Iteration 141/1000 | Loss: 0.00002672
Iteration 142/1000 | Loss: 0.00002672
Iteration 143/1000 | Loss: 0.00002672
Iteration 144/1000 | Loss: 0.00002672
Iteration 145/1000 | Loss: 0.00002672
Iteration 146/1000 | Loss: 0.00002672
Iteration 147/1000 | Loss: 0.00002672
Iteration 148/1000 | Loss: 0.00002672
Iteration 149/1000 | Loss: 0.00002672
Iteration 150/1000 | Loss: 0.00002671
Iteration 151/1000 | Loss: 0.00002671
Iteration 152/1000 | Loss: 0.00002671
Iteration 153/1000 | Loss: 0.00002671
Iteration 154/1000 | Loss: 0.00002670
Iteration 155/1000 | Loss: 0.00002670
Iteration 156/1000 | Loss: 0.00002670
Iteration 157/1000 | Loss: 0.00002670
Iteration 158/1000 | Loss: 0.00002670
Iteration 159/1000 | Loss: 0.00002670
Iteration 160/1000 | Loss: 0.00002669
Iteration 161/1000 | Loss: 0.00002669
Iteration 162/1000 | Loss: 0.00002669
Iteration 163/1000 | Loss: 0.00002669
Iteration 164/1000 | Loss: 0.00002669
Iteration 165/1000 | Loss: 0.00002669
Iteration 166/1000 | Loss: 0.00002669
Iteration 167/1000 | Loss: 0.00002669
Iteration 168/1000 | Loss: 0.00002669
Iteration 169/1000 | Loss: 0.00002669
Iteration 170/1000 | Loss: 0.00002669
Iteration 171/1000 | Loss: 0.00002668
Iteration 172/1000 | Loss: 0.00002668
Iteration 173/1000 | Loss: 0.00002668
Iteration 174/1000 | Loss: 0.00002668
Iteration 175/1000 | Loss: 0.00002668
Iteration 176/1000 | Loss: 0.00002668
Iteration 177/1000 | Loss: 0.00002668
Iteration 178/1000 | Loss: 0.00002668
Iteration 179/1000 | Loss: 0.00002668
Iteration 180/1000 | Loss: 0.00002668
Iteration 181/1000 | Loss: 0.00002667
Iteration 182/1000 | Loss: 0.00002667
Iteration 183/1000 | Loss: 0.00002667
Iteration 184/1000 | Loss: 0.00002667
Iteration 185/1000 | Loss: 0.00002667
Iteration 186/1000 | Loss: 0.00002666
Iteration 187/1000 | Loss: 0.00002666
Iteration 188/1000 | Loss: 0.00002666
Iteration 189/1000 | Loss: 0.00002666
Iteration 190/1000 | Loss: 0.00002666
Iteration 191/1000 | Loss: 0.00002666
Iteration 192/1000 | Loss: 0.00002666
Iteration 193/1000 | Loss: 0.00002666
Iteration 194/1000 | Loss: 0.00002666
Iteration 195/1000 | Loss: 0.00002666
Iteration 196/1000 | Loss: 0.00002666
Iteration 197/1000 | Loss: 0.00002666
Iteration 198/1000 | Loss: 0.00002666
Iteration 199/1000 | Loss: 0.00002666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.6656780391931534e-05, 2.6656780391931534e-05, 2.6656780391931534e-05, 2.6656780391931534e-05, 2.6656780391931534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6656780391931534e-05

Optimization complete. Final v2v error: 4.078784942626953 mm

Highest mean error: 5.543078422546387 mm for frame 19

Lowest mean error: 3.1347084045410156 mm for frame 0

Saving results

Total time: 54.70927548408508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417036
Iteration 2/25 | Loss: 0.00126331
Iteration 3/25 | Loss: 0.00113040
Iteration 4/25 | Loss: 0.00112340
Iteration 5/25 | Loss: 0.00112177
Iteration 6/25 | Loss: 0.00112140
Iteration 7/25 | Loss: 0.00112140
Iteration 8/25 | Loss: 0.00112140
Iteration 9/25 | Loss: 0.00112140
Iteration 10/25 | Loss: 0.00112140
Iteration 11/25 | Loss: 0.00112140
Iteration 12/25 | Loss: 0.00112140
Iteration 13/25 | Loss: 0.00112140
Iteration 14/25 | Loss: 0.00112140
Iteration 15/25 | Loss: 0.00112140
Iteration 16/25 | Loss: 0.00112140
Iteration 17/25 | Loss: 0.00112140
Iteration 18/25 | Loss: 0.00112140
Iteration 19/25 | Loss: 0.00112140
Iteration 20/25 | Loss: 0.00112140
Iteration 21/25 | Loss: 0.00112140
Iteration 22/25 | Loss: 0.00112140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011213968973606825, 0.0011213968973606825, 0.0011213968973606825, 0.0011213968973606825, 0.0011213968973606825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011213968973606825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19889724
Iteration 2/25 | Loss: 0.00150440
Iteration 3/25 | Loss: 0.00150440
Iteration 4/25 | Loss: 0.00150440
Iteration 5/25 | Loss: 0.00150440
Iteration 6/25 | Loss: 0.00150440
Iteration 7/25 | Loss: 0.00150440
Iteration 8/25 | Loss: 0.00150440
Iteration 9/25 | Loss: 0.00150440
Iteration 10/25 | Loss: 0.00150440
Iteration 11/25 | Loss: 0.00150440
Iteration 12/25 | Loss: 0.00150440
Iteration 13/25 | Loss: 0.00150440
Iteration 14/25 | Loss: 0.00150440
Iteration 15/25 | Loss: 0.00150440
Iteration 16/25 | Loss: 0.00150440
Iteration 17/25 | Loss: 0.00150440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015043989988043904, 0.0015043989988043904, 0.0015043989988043904, 0.0015043989988043904, 0.0015043989988043904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015043989988043904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150440
Iteration 2/1000 | Loss: 0.00004636
Iteration 3/1000 | Loss: 0.00002247
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001615
Iteration 6/1000 | Loss: 0.00001528
Iteration 7/1000 | Loss: 0.00001470
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001345
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001319
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001318
Iteration 16/1000 | Loss: 0.00001318
Iteration 17/1000 | Loss: 0.00001314
Iteration 18/1000 | Loss: 0.00001308
Iteration 19/1000 | Loss: 0.00001308
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001307
Iteration 22/1000 | Loss: 0.00001306
Iteration 23/1000 | Loss: 0.00001306
Iteration 24/1000 | Loss: 0.00001306
Iteration 25/1000 | Loss: 0.00001305
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001305
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001297
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001287
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001285
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001285
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001283
Iteration 64/1000 | Loss: 0.00001283
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001282
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001280
Iteration 78/1000 | Loss: 0.00001280
Iteration 79/1000 | Loss: 0.00001280
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001279
Iteration 82/1000 | Loss: 0.00001279
Iteration 83/1000 | Loss: 0.00001279
Iteration 84/1000 | Loss: 0.00001279
Iteration 85/1000 | Loss: 0.00001279
Iteration 86/1000 | Loss: 0.00001278
Iteration 87/1000 | Loss: 0.00001278
Iteration 88/1000 | Loss: 0.00001278
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001275
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001273
Iteration 112/1000 | Loss: 0.00001273
Iteration 113/1000 | Loss: 0.00001273
Iteration 114/1000 | Loss: 0.00001273
Iteration 115/1000 | Loss: 0.00001273
Iteration 116/1000 | Loss: 0.00001273
Iteration 117/1000 | Loss: 0.00001273
Iteration 118/1000 | Loss: 0.00001273
Iteration 119/1000 | Loss: 0.00001273
Iteration 120/1000 | Loss: 0.00001273
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001273
Iteration 124/1000 | Loss: 0.00001273
Iteration 125/1000 | Loss: 0.00001273
Iteration 126/1000 | Loss: 0.00001273
Iteration 127/1000 | Loss: 0.00001273
Iteration 128/1000 | Loss: 0.00001273
Iteration 129/1000 | Loss: 0.00001273
Iteration 130/1000 | Loss: 0.00001273
Iteration 131/1000 | Loss: 0.00001273
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001273
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001273
Iteration 136/1000 | Loss: 0.00001273
Iteration 137/1000 | Loss: 0.00001273
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001273
Iteration 140/1000 | Loss: 0.00001273
Iteration 141/1000 | Loss: 0.00001273
Iteration 142/1000 | Loss: 0.00001273
Iteration 143/1000 | Loss: 0.00001273
Iteration 144/1000 | Loss: 0.00001273
Iteration 145/1000 | Loss: 0.00001273
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Iteration 148/1000 | Loss: 0.00001273
Iteration 149/1000 | Loss: 0.00001273
Iteration 150/1000 | Loss: 0.00001273
Iteration 151/1000 | Loss: 0.00001273
Iteration 152/1000 | Loss: 0.00001273
Iteration 153/1000 | Loss: 0.00001273
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001273
Iteration 156/1000 | Loss: 0.00001273
Iteration 157/1000 | Loss: 0.00001273
Iteration 158/1000 | Loss: 0.00001273
Iteration 159/1000 | Loss: 0.00001273
Iteration 160/1000 | Loss: 0.00001273
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001273
Iteration 164/1000 | Loss: 0.00001273
Iteration 165/1000 | Loss: 0.00001273
Iteration 166/1000 | Loss: 0.00001273
Iteration 167/1000 | Loss: 0.00001273
Iteration 168/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2728884939861018e-05, 1.2728884939861018e-05, 1.2728884939861018e-05, 1.2728884939861018e-05, 1.2728884939861018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2728884939861018e-05

Optimization complete. Final v2v error: 3.037005662918091 mm

Highest mean error: 3.4736504554748535 mm for frame 67

Lowest mean error: 2.531524896621704 mm for frame 136

Saving results

Total time: 37.861234188079834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877874
Iteration 2/25 | Loss: 0.00124775
Iteration 3/25 | Loss: 0.00116487
Iteration 4/25 | Loss: 0.00114116
Iteration 5/25 | Loss: 0.00113506
Iteration 6/25 | Loss: 0.00113422
Iteration 7/25 | Loss: 0.00113422
Iteration 8/25 | Loss: 0.00113422
Iteration 9/25 | Loss: 0.00113422
Iteration 10/25 | Loss: 0.00113422
Iteration 11/25 | Loss: 0.00113422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001134222955442965, 0.001134222955442965, 0.001134222955442965, 0.001134222955442965, 0.001134222955442965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001134222955442965

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.28742790
Iteration 2/25 | Loss: 0.00165242
Iteration 3/25 | Loss: 0.00165237
Iteration 4/25 | Loss: 0.00165237
Iteration 5/25 | Loss: 0.00165237
Iteration 6/25 | Loss: 0.00165237
Iteration 7/25 | Loss: 0.00165237
Iteration 8/25 | Loss: 0.00165237
Iteration 9/25 | Loss: 0.00165237
Iteration 10/25 | Loss: 0.00165237
Iteration 11/25 | Loss: 0.00165237
Iteration 12/25 | Loss: 0.00165237
Iteration 13/25 | Loss: 0.00165237
Iteration 14/25 | Loss: 0.00165237
Iteration 15/25 | Loss: 0.00165237
Iteration 16/25 | Loss: 0.00165237
Iteration 17/25 | Loss: 0.00165237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016523710219189525, 0.0016523710219189525, 0.0016523710219189525, 0.0016523710219189525, 0.0016523710219189525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016523710219189525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165237
Iteration 2/1000 | Loss: 0.00004717
Iteration 3/1000 | Loss: 0.00002210
Iteration 4/1000 | Loss: 0.00001877
Iteration 5/1000 | Loss: 0.00001772
Iteration 6/1000 | Loss: 0.00001700
Iteration 7/1000 | Loss: 0.00001652
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001595
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001549
Iteration 13/1000 | Loss: 0.00001549
Iteration 14/1000 | Loss: 0.00001544
Iteration 15/1000 | Loss: 0.00001543
Iteration 16/1000 | Loss: 0.00001543
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001541
Iteration 22/1000 | Loss: 0.00001540
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001539
Iteration 25/1000 | Loss: 0.00001537
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001536
Iteration 28/1000 | Loss: 0.00001535
Iteration 29/1000 | Loss: 0.00001535
Iteration 30/1000 | Loss: 0.00001535
Iteration 31/1000 | Loss: 0.00001534
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00001534
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001532
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001531
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001530
Iteration 40/1000 | Loss: 0.00001530
Iteration 41/1000 | Loss: 0.00001530
Iteration 42/1000 | Loss: 0.00001529
Iteration 43/1000 | Loss: 0.00001529
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001529
Iteration 46/1000 | Loss: 0.00001528
Iteration 47/1000 | Loss: 0.00001528
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001528
Iteration 50/1000 | Loss: 0.00001528
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001528
Iteration 53/1000 | Loss: 0.00001528
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001527
Iteration 56/1000 | Loss: 0.00001527
Iteration 57/1000 | Loss: 0.00001527
Iteration 58/1000 | Loss: 0.00001527
Iteration 59/1000 | Loss: 0.00001527
Iteration 60/1000 | Loss: 0.00001527
Iteration 61/1000 | Loss: 0.00001527
Iteration 62/1000 | Loss: 0.00001527
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001526
Iteration 68/1000 | Loss: 0.00001526
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001520
Iteration 91/1000 | Loss: 0.00001520
Iteration 92/1000 | Loss: 0.00001520
Iteration 93/1000 | Loss: 0.00001520
Iteration 94/1000 | Loss: 0.00001520
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001516
Iteration 110/1000 | Loss: 0.00001516
Iteration 111/1000 | Loss: 0.00001515
Iteration 112/1000 | Loss: 0.00001515
Iteration 113/1000 | Loss: 0.00001515
Iteration 114/1000 | Loss: 0.00001515
Iteration 115/1000 | Loss: 0.00001515
Iteration 116/1000 | Loss: 0.00001515
Iteration 117/1000 | Loss: 0.00001515
Iteration 118/1000 | Loss: 0.00001515
Iteration 119/1000 | Loss: 0.00001515
Iteration 120/1000 | Loss: 0.00001515
Iteration 121/1000 | Loss: 0.00001515
Iteration 122/1000 | Loss: 0.00001515
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001515
Iteration 129/1000 | Loss: 0.00001515
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001515
Iteration 144/1000 | Loss: 0.00001515
Iteration 145/1000 | Loss: 0.00001515
Iteration 146/1000 | Loss: 0.00001515
Iteration 147/1000 | Loss: 0.00001515
Iteration 148/1000 | Loss: 0.00001515
Iteration 149/1000 | Loss: 0.00001515
Iteration 150/1000 | Loss: 0.00001515
Iteration 151/1000 | Loss: 0.00001515
Iteration 152/1000 | Loss: 0.00001515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.5145174984354526e-05, 1.5145174984354526e-05, 1.5145174984354526e-05, 1.5145174984354526e-05, 1.5145174984354526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5145174984354526e-05

Optimization complete. Final v2v error: 3.311211347579956 mm

Highest mean error: 3.956474781036377 mm for frame 46

Lowest mean error: 2.851498603820801 mm for frame 212

Saving results

Total time: 39.11387777328491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027126
Iteration 2/25 | Loss: 0.00296270
Iteration 3/25 | Loss: 0.00199950
Iteration 4/25 | Loss: 0.00179817
Iteration 5/25 | Loss: 0.00162165
Iteration 6/25 | Loss: 0.00162655
Iteration 7/25 | Loss: 0.00181455
Iteration 8/25 | Loss: 0.00153463
Iteration 9/25 | Loss: 0.00141635
Iteration 10/25 | Loss: 0.00138264
Iteration 11/25 | Loss: 0.00135122
Iteration 12/25 | Loss: 0.00133518
Iteration 13/25 | Loss: 0.00133982
Iteration 14/25 | Loss: 0.00132487
Iteration 15/25 | Loss: 0.00131379
Iteration 16/25 | Loss: 0.00131157
Iteration 17/25 | Loss: 0.00131073
Iteration 18/25 | Loss: 0.00130837
Iteration 19/25 | Loss: 0.00132017
Iteration 20/25 | Loss: 0.00132118
Iteration 21/25 | Loss: 0.00131412
Iteration 22/25 | Loss: 0.00130270
Iteration 23/25 | Loss: 0.00129725
Iteration 24/25 | Loss: 0.00129980
Iteration 25/25 | Loss: 0.00129972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19413006
Iteration 2/25 | Loss: 0.00399002
Iteration 3/25 | Loss: 0.00304528
Iteration 4/25 | Loss: 0.00304528
Iteration 5/25 | Loss: 0.00304528
Iteration 6/25 | Loss: 0.00304528
Iteration 7/25 | Loss: 0.00304528
Iteration 8/25 | Loss: 0.00304528
Iteration 9/25 | Loss: 0.00304527
Iteration 10/25 | Loss: 0.00304527
Iteration 11/25 | Loss: 0.00304527
Iteration 12/25 | Loss: 0.00304527
Iteration 13/25 | Loss: 0.00304527
Iteration 14/25 | Loss: 0.00304527
Iteration 15/25 | Loss: 0.00304527
Iteration 16/25 | Loss: 0.00304527
Iteration 17/25 | Loss: 0.00304527
Iteration 18/25 | Loss: 0.00304527
Iteration 19/25 | Loss: 0.00304527
Iteration 20/25 | Loss: 0.00304527
Iteration 21/25 | Loss: 0.00304527
Iteration 22/25 | Loss: 0.00304527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0030452737119048834, 0.0030452737119048834, 0.0030452737119048834, 0.0030452737119048834, 0.0030452737119048834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030452737119048834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304527
Iteration 2/1000 | Loss: 0.00332697
Iteration 3/1000 | Loss: 0.00110971
Iteration 4/1000 | Loss: 0.00087668
Iteration 5/1000 | Loss: 0.00039157
Iteration 6/1000 | Loss: 0.00048670
Iteration 7/1000 | Loss: 0.00018160
Iteration 8/1000 | Loss: 0.00035124
Iteration 9/1000 | Loss: 0.00015013
Iteration 10/1000 | Loss: 0.00090051
Iteration 11/1000 | Loss: 0.00021841
Iteration 12/1000 | Loss: 0.00072150
Iteration 13/1000 | Loss: 0.00032378
Iteration 14/1000 | Loss: 0.00112571
Iteration 15/1000 | Loss: 0.00046871
Iteration 16/1000 | Loss: 0.00028878
Iteration 17/1000 | Loss: 0.00030523
Iteration 18/1000 | Loss: 0.00009429
Iteration 19/1000 | Loss: 0.00012948
Iteration 20/1000 | Loss: 0.00017310
Iteration 21/1000 | Loss: 0.00045010
Iteration 22/1000 | Loss: 0.00005233
Iteration 23/1000 | Loss: 0.00058972
Iteration 24/1000 | Loss: 0.00019023
Iteration 25/1000 | Loss: 0.00037685
Iteration 26/1000 | Loss: 0.00021305
Iteration 27/1000 | Loss: 0.00007880
Iteration 28/1000 | Loss: 0.00013933
Iteration 29/1000 | Loss: 0.00007340
Iteration 30/1000 | Loss: 0.00010506
Iteration 31/1000 | Loss: 0.00004720
Iteration 32/1000 | Loss: 0.00008232
Iteration 33/1000 | Loss: 0.00035163
Iteration 34/1000 | Loss: 0.00004402
Iteration 35/1000 | Loss: 0.00006700
Iteration 36/1000 | Loss: 0.00004101
Iteration 37/1000 | Loss: 0.00005899
Iteration 38/1000 | Loss: 0.00008644
Iteration 39/1000 | Loss: 0.00003339
Iteration 40/1000 | Loss: 0.00027068
Iteration 41/1000 | Loss: 0.00008094
Iteration 42/1000 | Loss: 0.00048570
Iteration 43/1000 | Loss: 0.00213518
Iteration 44/1000 | Loss: 0.00044185
Iteration 45/1000 | Loss: 0.00028710
Iteration 46/1000 | Loss: 0.00041564
Iteration 47/1000 | Loss: 0.00031526
Iteration 48/1000 | Loss: 0.00036837
Iteration 49/1000 | Loss: 0.00005351
Iteration 50/1000 | Loss: 0.00004723
Iteration 51/1000 | Loss: 0.00003625
Iteration 52/1000 | Loss: 0.00003840
Iteration 53/1000 | Loss: 0.00004012
Iteration 54/1000 | Loss: 0.00003027
Iteration 55/1000 | Loss: 0.00024190
Iteration 56/1000 | Loss: 0.00025802
Iteration 57/1000 | Loss: 0.00021491
Iteration 58/1000 | Loss: 0.00005658
Iteration 59/1000 | Loss: 0.00004385
Iteration 60/1000 | Loss: 0.00030031
Iteration 61/1000 | Loss: 0.00008159
Iteration 62/1000 | Loss: 0.00004312
Iteration 63/1000 | Loss: 0.00003831
Iteration 64/1000 | Loss: 0.00003316
Iteration 65/1000 | Loss: 0.00004498
Iteration 66/1000 | Loss: 0.00002905
Iteration 67/1000 | Loss: 0.00002832
Iteration 68/1000 | Loss: 0.00002776
Iteration 69/1000 | Loss: 0.00002743
Iteration 70/1000 | Loss: 0.00002711
Iteration 71/1000 | Loss: 0.00002682
Iteration 72/1000 | Loss: 0.00013265
Iteration 73/1000 | Loss: 0.00002567
Iteration 74/1000 | Loss: 0.00005529
Iteration 75/1000 | Loss: 0.00003316
Iteration 76/1000 | Loss: 0.00002718
Iteration 77/1000 | Loss: 0.00002539
Iteration 78/1000 | Loss: 0.00002468
Iteration 79/1000 | Loss: 0.00002431
Iteration 80/1000 | Loss: 0.00047110
Iteration 81/1000 | Loss: 0.00003007
Iteration 82/1000 | Loss: 0.00002566
Iteration 83/1000 | Loss: 0.00002435
Iteration 84/1000 | Loss: 0.00006666
Iteration 85/1000 | Loss: 0.00004364
Iteration 86/1000 | Loss: 0.00002810
Iteration 87/1000 | Loss: 0.00002282
Iteration 88/1000 | Loss: 0.00002225
Iteration 89/1000 | Loss: 0.00002195
Iteration 90/1000 | Loss: 0.00002176
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002166
Iteration 93/1000 | Loss: 0.00002165
Iteration 94/1000 | Loss: 0.00002164
Iteration 95/1000 | Loss: 0.00002156
Iteration 96/1000 | Loss: 0.00002155
Iteration 97/1000 | Loss: 0.00002153
Iteration 98/1000 | Loss: 0.00002153
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002151
Iteration 103/1000 | Loss: 0.00002150
Iteration 104/1000 | Loss: 0.00002150
Iteration 105/1000 | Loss: 0.00002150
Iteration 106/1000 | Loss: 0.00002150
Iteration 107/1000 | Loss: 0.00002150
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002149
Iteration 110/1000 | Loss: 0.00002149
Iteration 111/1000 | Loss: 0.00002149
Iteration 112/1000 | Loss: 0.00002149
Iteration 113/1000 | Loss: 0.00002149
Iteration 114/1000 | Loss: 0.00002149
Iteration 115/1000 | Loss: 0.00002148
Iteration 116/1000 | Loss: 0.00002148
Iteration 117/1000 | Loss: 0.00002148
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00002147
Iteration 120/1000 | Loss: 0.00002146
Iteration 121/1000 | Loss: 0.00002146
Iteration 122/1000 | Loss: 0.00002146
Iteration 123/1000 | Loss: 0.00002145
Iteration 124/1000 | Loss: 0.00002144
Iteration 125/1000 | Loss: 0.00002144
Iteration 126/1000 | Loss: 0.00002144
Iteration 127/1000 | Loss: 0.00002144
Iteration 128/1000 | Loss: 0.00002144
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002143
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002143
Iteration 134/1000 | Loss: 0.00002143
Iteration 135/1000 | Loss: 0.00002143
Iteration 136/1000 | Loss: 0.00002143
Iteration 137/1000 | Loss: 0.00002143
Iteration 138/1000 | Loss: 0.00002143
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002142
Iteration 142/1000 | Loss: 0.00002142
Iteration 143/1000 | Loss: 0.00002141
Iteration 144/1000 | Loss: 0.00002141
Iteration 145/1000 | Loss: 0.00002140
Iteration 146/1000 | Loss: 0.00002139
Iteration 147/1000 | Loss: 0.00002138
Iteration 148/1000 | Loss: 0.00002138
Iteration 149/1000 | Loss: 0.00002137
Iteration 150/1000 | Loss: 0.00002137
Iteration 151/1000 | Loss: 0.00002137
Iteration 152/1000 | Loss: 0.00002137
Iteration 153/1000 | Loss: 0.00002137
Iteration 154/1000 | Loss: 0.00002136
Iteration 155/1000 | Loss: 0.00002136
Iteration 156/1000 | Loss: 0.00002136
Iteration 157/1000 | Loss: 0.00002136
Iteration 158/1000 | Loss: 0.00002136
Iteration 159/1000 | Loss: 0.00002135
Iteration 160/1000 | Loss: 0.00002135
Iteration 161/1000 | Loss: 0.00002134
Iteration 162/1000 | Loss: 0.00002134
Iteration 163/1000 | Loss: 0.00002133
Iteration 164/1000 | Loss: 0.00002133
Iteration 165/1000 | Loss: 0.00002132
Iteration 166/1000 | Loss: 0.00002132
Iteration 167/1000 | Loss: 0.00002131
Iteration 168/1000 | Loss: 0.00002131
Iteration 169/1000 | Loss: 0.00002131
Iteration 170/1000 | Loss: 0.00002130
Iteration 171/1000 | Loss: 0.00002130
Iteration 172/1000 | Loss: 0.00002130
Iteration 173/1000 | Loss: 0.00002129
Iteration 174/1000 | Loss: 0.00002129
Iteration 175/1000 | Loss: 0.00002128
Iteration 176/1000 | Loss: 0.00002126
Iteration 177/1000 | Loss: 0.00002125
Iteration 178/1000 | Loss: 0.00002124
Iteration 179/1000 | Loss: 0.00002123
Iteration 180/1000 | Loss: 0.00002123
Iteration 181/1000 | Loss: 0.00002122
Iteration 182/1000 | Loss: 0.00002122
Iteration 183/1000 | Loss: 0.00002121
Iteration 184/1000 | Loss: 0.00002121
Iteration 185/1000 | Loss: 0.00002121
Iteration 186/1000 | Loss: 0.00002121
Iteration 187/1000 | Loss: 0.00002121
Iteration 188/1000 | Loss: 0.00002121
Iteration 189/1000 | Loss: 0.00002121
Iteration 190/1000 | Loss: 0.00002121
Iteration 191/1000 | Loss: 0.00002121
Iteration 192/1000 | Loss: 0.00002120
Iteration 193/1000 | Loss: 0.00002120
Iteration 194/1000 | Loss: 0.00002120
Iteration 195/1000 | Loss: 0.00002119
Iteration 196/1000 | Loss: 0.00002119
Iteration 197/1000 | Loss: 0.00002119
Iteration 198/1000 | Loss: 0.00002119
Iteration 199/1000 | Loss: 0.00002118
Iteration 200/1000 | Loss: 0.00002118
Iteration 201/1000 | Loss: 0.00002118
Iteration 202/1000 | Loss: 0.00002118
Iteration 203/1000 | Loss: 0.00002117
Iteration 204/1000 | Loss: 0.00002117
Iteration 205/1000 | Loss: 0.00002117
Iteration 206/1000 | Loss: 0.00002117
Iteration 207/1000 | Loss: 0.00002116
Iteration 208/1000 | Loss: 0.00002116
Iteration 209/1000 | Loss: 0.00002116
Iteration 210/1000 | Loss: 0.00002116
Iteration 211/1000 | Loss: 0.00002116
Iteration 212/1000 | Loss: 0.00002116
Iteration 213/1000 | Loss: 0.00002116
Iteration 214/1000 | Loss: 0.00002116
Iteration 215/1000 | Loss: 0.00002115
Iteration 216/1000 | Loss: 0.00002115
Iteration 217/1000 | Loss: 0.00002115
Iteration 218/1000 | Loss: 0.00002115
Iteration 219/1000 | Loss: 0.00002115
Iteration 220/1000 | Loss: 0.00002115
Iteration 221/1000 | Loss: 0.00002115
Iteration 222/1000 | Loss: 0.00002114
Iteration 223/1000 | Loss: 0.00002114
Iteration 224/1000 | Loss: 0.00044106
Iteration 225/1000 | Loss: 0.00006303
Iteration 226/1000 | Loss: 0.00002288
Iteration 227/1000 | Loss: 0.00002163
Iteration 228/1000 | Loss: 0.00002069
Iteration 229/1000 | Loss: 0.00002005
Iteration 230/1000 | Loss: 0.00001964
Iteration 231/1000 | Loss: 0.00001938
Iteration 232/1000 | Loss: 0.00001934
Iteration 233/1000 | Loss: 0.00001928
Iteration 234/1000 | Loss: 0.00001921
Iteration 235/1000 | Loss: 0.00001915
Iteration 236/1000 | Loss: 0.00001915
Iteration 237/1000 | Loss: 0.00001913
Iteration 238/1000 | Loss: 0.00001913
Iteration 239/1000 | Loss: 0.00001912
Iteration 240/1000 | Loss: 0.00001912
Iteration 241/1000 | Loss: 0.00001911
Iteration 242/1000 | Loss: 0.00001911
Iteration 243/1000 | Loss: 0.00001911
Iteration 244/1000 | Loss: 0.00001910
Iteration 245/1000 | Loss: 0.00001909
Iteration 246/1000 | Loss: 0.00001909
Iteration 247/1000 | Loss: 0.00001909
Iteration 248/1000 | Loss: 0.00001909
Iteration 249/1000 | Loss: 0.00001909
Iteration 250/1000 | Loss: 0.00001909
Iteration 251/1000 | Loss: 0.00001909
Iteration 252/1000 | Loss: 0.00001909
Iteration 253/1000 | Loss: 0.00001908
Iteration 254/1000 | Loss: 0.00001908
Iteration 255/1000 | Loss: 0.00001908
Iteration 256/1000 | Loss: 0.00001907
Iteration 257/1000 | Loss: 0.00001907
Iteration 258/1000 | Loss: 0.00001907
Iteration 259/1000 | Loss: 0.00001906
Iteration 260/1000 | Loss: 0.00001906
Iteration 261/1000 | Loss: 0.00001906
Iteration 262/1000 | Loss: 0.00001906
Iteration 263/1000 | Loss: 0.00001906
Iteration 264/1000 | Loss: 0.00001906
Iteration 265/1000 | Loss: 0.00001906
Iteration 266/1000 | Loss: 0.00001906
Iteration 267/1000 | Loss: 0.00001906
Iteration 268/1000 | Loss: 0.00001905
Iteration 269/1000 | Loss: 0.00001905
Iteration 270/1000 | Loss: 0.00001905
Iteration 271/1000 | Loss: 0.00001905
Iteration 272/1000 | Loss: 0.00001905
Iteration 273/1000 | Loss: 0.00001905
Iteration 274/1000 | Loss: 0.00001905
Iteration 275/1000 | Loss: 0.00001905
Iteration 276/1000 | Loss: 0.00001905
Iteration 277/1000 | Loss: 0.00001905
Iteration 278/1000 | Loss: 0.00001905
Iteration 279/1000 | Loss: 0.00001905
Iteration 280/1000 | Loss: 0.00001905
Iteration 281/1000 | Loss: 0.00001905
Iteration 282/1000 | Loss: 0.00001905
Iteration 283/1000 | Loss: 0.00001904
Iteration 284/1000 | Loss: 0.00001904
Iteration 285/1000 | Loss: 0.00001904
Iteration 286/1000 | Loss: 0.00001904
Iteration 287/1000 | Loss: 0.00001904
Iteration 288/1000 | Loss: 0.00001904
Iteration 289/1000 | Loss: 0.00001904
Iteration 290/1000 | Loss: 0.00001904
Iteration 291/1000 | Loss: 0.00001904
Iteration 292/1000 | Loss: 0.00001904
Iteration 293/1000 | Loss: 0.00001904
Iteration 294/1000 | Loss: 0.00001904
Iteration 295/1000 | Loss: 0.00001904
Iteration 296/1000 | Loss: 0.00001904
Iteration 297/1000 | Loss: 0.00001903
Iteration 298/1000 | Loss: 0.00001903
Iteration 299/1000 | Loss: 0.00001903
Iteration 300/1000 | Loss: 0.00001903
Iteration 301/1000 | Loss: 0.00001903
Iteration 302/1000 | Loss: 0.00001903
Iteration 303/1000 | Loss: 0.00001903
Iteration 304/1000 | Loss: 0.00001903
Iteration 305/1000 | Loss: 0.00001903
Iteration 306/1000 | Loss: 0.00001903
Iteration 307/1000 | Loss: 0.00001903
Iteration 308/1000 | Loss: 0.00001903
Iteration 309/1000 | Loss: 0.00001902
Iteration 310/1000 | Loss: 0.00001902
Iteration 311/1000 | Loss: 0.00001902
Iteration 312/1000 | Loss: 0.00001902
Iteration 313/1000 | Loss: 0.00001902
Iteration 314/1000 | Loss: 0.00001902
Iteration 315/1000 | Loss: 0.00001902
Iteration 316/1000 | Loss: 0.00001902
Iteration 317/1000 | Loss: 0.00001902
Iteration 318/1000 | Loss: 0.00001902
Iteration 319/1000 | Loss: 0.00001902
Iteration 320/1000 | Loss: 0.00001902
Iteration 321/1000 | Loss: 0.00001902
Iteration 322/1000 | Loss: 0.00001902
Iteration 323/1000 | Loss: 0.00001902
Iteration 324/1000 | Loss: 0.00001902
Iteration 325/1000 | Loss: 0.00001902
Iteration 326/1000 | Loss: 0.00001902
Iteration 327/1000 | Loss: 0.00001902
Iteration 328/1000 | Loss: 0.00001902
Iteration 329/1000 | Loss: 0.00001902
Iteration 330/1000 | Loss: 0.00001902
Iteration 331/1000 | Loss: 0.00001902
Iteration 332/1000 | Loss: 0.00001902
Iteration 333/1000 | Loss: 0.00001902
Iteration 334/1000 | Loss: 0.00001902
Iteration 335/1000 | Loss: 0.00001902
Iteration 336/1000 | Loss: 0.00001902
Iteration 337/1000 | Loss: 0.00001902
Iteration 338/1000 | Loss: 0.00001902
Iteration 339/1000 | Loss: 0.00001902
Iteration 340/1000 | Loss: 0.00001902
Iteration 341/1000 | Loss: 0.00001902
Iteration 342/1000 | Loss: 0.00001902
Iteration 343/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [1.9022403648705222e-05, 1.9022403648705222e-05, 1.9022403648705222e-05, 1.9022403648705222e-05, 1.9022403648705222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9022403648705222e-05

Optimization complete. Final v2v error: 3.513505697250366 mm

Highest mean error: 15.037315368652344 mm for frame 175

Lowest mean error: 2.5809285640716553 mm for frame 23

Saving results

Total time: 236.8510160446167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750847
Iteration 2/25 | Loss: 0.00158351
Iteration 3/25 | Loss: 0.00133983
Iteration 4/25 | Loss: 0.00130223
Iteration 5/25 | Loss: 0.00128809
Iteration 6/25 | Loss: 0.00128984
Iteration 7/25 | Loss: 0.00129053
Iteration 8/25 | Loss: 0.00128446
Iteration 9/25 | Loss: 0.00128205
Iteration 10/25 | Loss: 0.00128057
Iteration 11/25 | Loss: 0.00126850
Iteration 12/25 | Loss: 0.00126482
Iteration 13/25 | Loss: 0.00126447
Iteration 14/25 | Loss: 0.00126446
Iteration 15/25 | Loss: 0.00126446
Iteration 16/25 | Loss: 0.00126446
Iteration 17/25 | Loss: 0.00126446
Iteration 18/25 | Loss: 0.00126446
Iteration 19/25 | Loss: 0.00126446
Iteration 20/25 | Loss: 0.00126446
Iteration 21/25 | Loss: 0.00126446
Iteration 22/25 | Loss: 0.00126446
Iteration 23/25 | Loss: 0.00126446
Iteration 24/25 | Loss: 0.00126446
Iteration 25/25 | Loss: 0.00126446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.03997421
Iteration 2/25 | Loss: 0.00828054
Iteration 3/25 | Loss: 0.00161549
Iteration 4/25 | Loss: 0.00161528
Iteration 5/25 | Loss: 0.00161528
Iteration 6/25 | Loss: 0.00161528
Iteration 7/25 | Loss: 0.00161528
Iteration 8/25 | Loss: 0.00161528
Iteration 9/25 | Loss: 0.00161527
Iteration 10/25 | Loss: 0.00161527
Iteration 11/25 | Loss: 0.00161527
Iteration 12/25 | Loss: 0.00161527
Iteration 13/25 | Loss: 0.00161527
Iteration 14/25 | Loss: 0.00161527
Iteration 15/25 | Loss: 0.00161527
Iteration 16/25 | Loss: 0.00161527
Iteration 17/25 | Loss: 0.00161527
Iteration 18/25 | Loss: 0.00161527
Iteration 19/25 | Loss: 0.00161527
Iteration 20/25 | Loss: 0.00161527
Iteration 21/25 | Loss: 0.00161527
Iteration 22/25 | Loss: 0.00161527
Iteration 23/25 | Loss: 0.00161527
Iteration 24/25 | Loss: 0.00161527
Iteration 25/25 | Loss: 0.00161527
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0016152739990502596, 0.0016152739990502596, 0.0016152739990502596, 0.0016152739990502596, 0.0016152739990502596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016152739990502596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161527
Iteration 2/1000 | Loss: 0.00537460
Iteration 3/1000 | Loss: 0.00609783
Iteration 4/1000 | Loss: 0.00492327
Iteration 5/1000 | Loss: 0.00054858
Iteration 6/1000 | Loss: 0.00036422
Iteration 7/1000 | Loss: 0.00236238
Iteration 8/1000 | Loss: 0.00012336
Iteration 9/1000 | Loss: 0.00610421
Iteration 10/1000 | Loss: 0.00010770
Iteration 11/1000 | Loss: 0.00008430
Iteration 12/1000 | Loss: 0.00007670
Iteration 13/1000 | Loss: 0.00212822
Iteration 14/1000 | Loss: 0.00097580
Iteration 15/1000 | Loss: 0.00638533
Iteration 16/1000 | Loss: 0.00510390
Iteration 17/1000 | Loss: 0.00095141
Iteration 18/1000 | Loss: 0.00081788
Iteration 19/1000 | Loss: 0.00273625
Iteration 20/1000 | Loss: 0.00086602
Iteration 21/1000 | Loss: 0.00221608
Iteration 22/1000 | Loss: 0.00087642
Iteration 23/1000 | Loss: 0.00209629
Iteration 24/1000 | Loss: 0.00058101
Iteration 25/1000 | Loss: 0.00123152
Iteration 26/1000 | Loss: 0.00076786
Iteration 27/1000 | Loss: 0.00116502
Iteration 28/1000 | Loss: 0.00012343
Iteration 29/1000 | Loss: 0.00008297
Iteration 30/1000 | Loss: 0.00019851
Iteration 31/1000 | Loss: 0.00006091
Iteration 32/1000 | Loss: 0.00005603
Iteration 33/1000 | Loss: 0.00005325
Iteration 34/1000 | Loss: 0.00005060
Iteration 35/1000 | Loss: 0.00004896
Iteration 36/1000 | Loss: 0.00064448
Iteration 37/1000 | Loss: 0.00300391
Iteration 38/1000 | Loss: 0.00122452
Iteration 39/1000 | Loss: 0.00017922
Iteration 40/1000 | Loss: 0.00012378
Iteration 41/1000 | Loss: 0.00026514
Iteration 42/1000 | Loss: 0.00035511
Iteration 43/1000 | Loss: 0.00005942
Iteration 44/1000 | Loss: 0.00004773
Iteration 45/1000 | Loss: 0.00004218
Iteration 46/1000 | Loss: 0.00003933
Iteration 47/1000 | Loss: 0.00003736
Iteration 48/1000 | Loss: 0.00003599
Iteration 49/1000 | Loss: 0.00003530
Iteration 50/1000 | Loss: 0.00003484
Iteration 51/1000 | Loss: 0.00003415
Iteration 52/1000 | Loss: 0.00249784
Iteration 53/1000 | Loss: 0.00027705
Iteration 54/1000 | Loss: 0.00043092
Iteration 55/1000 | Loss: 0.00006983
Iteration 56/1000 | Loss: 0.00004954
Iteration 57/1000 | Loss: 0.00003539
Iteration 58/1000 | Loss: 0.00002915
Iteration 59/1000 | Loss: 0.00002711
Iteration 60/1000 | Loss: 0.00002582
Iteration 61/1000 | Loss: 0.00002528
Iteration 62/1000 | Loss: 0.00002492
Iteration 63/1000 | Loss: 0.00002470
Iteration 64/1000 | Loss: 0.00002451
Iteration 65/1000 | Loss: 0.00002439
Iteration 66/1000 | Loss: 0.00002437
Iteration 67/1000 | Loss: 0.00002436
Iteration 68/1000 | Loss: 0.00002436
Iteration 69/1000 | Loss: 0.00002435
Iteration 70/1000 | Loss: 0.00002435
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00002434
Iteration 73/1000 | Loss: 0.00002434
Iteration 74/1000 | Loss: 0.00002434
Iteration 75/1000 | Loss: 0.00002433
Iteration 76/1000 | Loss: 0.00002433
Iteration 77/1000 | Loss: 0.00002433
Iteration 78/1000 | Loss: 0.00002432
Iteration 79/1000 | Loss: 0.00002432
Iteration 80/1000 | Loss: 0.00002430
Iteration 81/1000 | Loss: 0.00002430
Iteration 82/1000 | Loss: 0.00002430
Iteration 83/1000 | Loss: 0.00002430
Iteration 84/1000 | Loss: 0.00002430
Iteration 85/1000 | Loss: 0.00002430
Iteration 86/1000 | Loss: 0.00002430
Iteration 87/1000 | Loss: 0.00002430
Iteration 88/1000 | Loss: 0.00002430
Iteration 89/1000 | Loss: 0.00002429
Iteration 90/1000 | Loss: 0.00002429
Iteration 91/1000 | Loss: 0.00002429
Iteration 92/1000 | Loss: 0.00002429
Iteration 93/1000 | Loss: 0.00002428
Iteration 94/1000 | Loss: 0.00002428
Iteration 95/1000 | Loss: 0.00002428
Iteration 96/1000 | Loss: 0.00002427
Iteration 97/1000 | Loss: 0.00002427
Iteration 98/1000 | Loss: 0.00002427
Iteration 99/1000 | Loss: 0.00002427
Iteration 100/1000 | Loss: 0.00002427
Iteration 101/1000 | Loss: 0.00002427
Iteration 102/1000 | Loss: 0.00002427
Iteration 103/1000 | Loss: 0.00002427
Iteration 104/1000 | Loss: 0.00002426
Iteration 105/1000 | Loss: 0.00002426
Iteration 106/1000 | Loss: 0.00002426
Iteration 107/1000 | Loss: 0.00002426
Iteration 108/1000 | Loss: 0.00002426
Iteration 109/1000 | Loss: 0.00002425
Iteration 110/1000 | Loss: 0.00002425
Iteration 111/1000 | Loss: 0.00002425
Iteration 112/1000 | Loss: 0.00002424
Iteration 113/1000 | Loss: 0.00002424
Iteration 114/1000 | Loss: 0.00002424
Iteration 115/1000 | Loss: 0.00002423
Iteration 116/1000 | Loss: 0.00002423
Iteration 117/1000 | Loss: 0.00002421
Iteration 118/1000 | Loss: 0.00002421
Iteration 119/1000 | Loss: 0.00002421
Iteration 120/1000 | Loss: 0.00002420
Iteration 121/1000 | Loss: 0.00002420
Iteration 122/1000 | Loss: 0.00002420
Iteration 123/1000 | Loss: 0.00002420
Iteration 124/1000 | Loss: 0.00002420
Iteration 125/1000 | Loss: 0.00002420
Iteration 126/1000 | Loss: 0.00002420
Iteration 127/1000 | Loss: 0.00002420
Iteration 128/1000 | Loss: 0.00002419
Iteration 129/1000 | Loss: 0.00002419
Iteration 130/1000 | Loss: 0.00002419
Iteration 131/1000 | Loss: 0.00002419
Iteration 132/1000 | Loss: 0.00002419
Iteration 133/1000 | Loss: 0.00002418
Iteration 134/1000 | Loss: 0.00002418
Iteration 135/1000 | Loss: 0.00002418
Iteration 136/1000 | Loss: 0.00002418
Iteration 137/1000 | Loss: 0.00002418
Iteration 138/1000 | Loss: 0.00002418
Iteration 139/1000 | Loss: 0.00002418
Iteration 140/1000 | Loss: 0.00002418
Iteration 141/1000 | Loss: 0.00002418
Iteration 142/1000 | Loss: 0.00002418
Iteration 143/1000 | Loss: 0.00002418
Iteration 144/1000 | Loss: 0.00002418
Iteration 145/1000 | Loss: 0.00002418
Iteration 146/1000 | Loss: 0.00002418
Iteration 147/1000 | Loss: 0.00002418
Iteration 148/1000 | Loss: 0.00002418
Iteration 149/1000 | Loss: 0.00002418
Iteration 150/1000 | Loss: 0.00002418
Iteration 151/1000 | Loss: 0.00002418
Iteration 152/1000 | Loss: 0.00002418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.418005169602111e-05, 2.418005169602111e-05, 2.418005169602111e-05, 2.418005169602111e-05, 2.418005169602111e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.418005169602111e-05

Optimization complete. Final v2v error: 3.9901909828186035 mm

Highest mean error: 5.956600189208984 mm for frame 56

Lowest mean error: 3.0227749347686768 mm for frame 26

Saving results

Total time: 123.36105465888977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145452
Iteration 2/25 | Loss: 0.00260344
Iteration 3/25 | Loss: 0.00299365
Iteration 4/25 | Loss: 0.00155099
Iteration 5/25 | Loss: 0.00142482
Iteration 6/25 | Loss: 0.00135572
Iteration 7/25 | Loss: 0.00133347
Iteration 8/25 | Loss: 0.00132487
Iteration 9/25 | Loss: 0.00130875
Iteration 10/25 | Loss: 0.00130470
Iteration 11/25 | Loss: 0.00130421
Iteration 12/25 | Loss: 0.00130414
Iteration 13/25 | Loss: 0.00130412
Iteration 14/25 | Loss: 0.00130412
Iteration 15/25 | Loss: 0.00130412
Iteration 16/25 | Loss: 0.00130412
Iteration 17/25 | Loss: 0.00130412
Iteration 18/25 | Loss: 0.00130412
Iteration 19/25 | Loss: 0.00130411
Iteration 20/25 | Loss: 0.00130411
Iteration 21/25 | Loss: 0.00130411
Iteration 22/25 | Loss: 0.00130411
Iteration 23/25 | Loss: 0.00130411
Iteration 24/25 | Loss: 0.00130411
Iteration 25/25 | Loss: 0.00130411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87284207
Iteration 2/25 | Loss: 0.00112660
Iteration 3/25 | Loss: 0.00112660
Iteration 4/25 | Loss: 0.00112660
Iteration 5/25 | Loss: 0.00112660
Iteration 6/25 | Loss: 0.00112660
Iteration 7/25 | Loss: 0.00112660
Iteration 8/25 | Loss: 0.00112660
Iteration 9/25 | Loss: 0.00112659
Iteration 10/25 | Loss: 0.00112659
Iteration 11/25 | Loss: 0.00112659
Iteration 12/25 | Loss: 0.00112659
Iteration 13/25 | Loss: 0.00112659
Iteration 14/25 | Loss: 0.00112659
Iteration 15/25 | Loss: 0.00112659
Iteration 16/25 | Loss: 0.00112659
Iteration 17/25 | Loss: 0.00112659
Iteration 18/25 | Loss: 0.00112659
Iteration 19/25 | Loss: 0.00112659
Iteration 20/25 | Loss: 0.00112659
Iteration 21/25 | Loss: 0.00112659
Iteration 22/25 | Loss: 0.00112659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011265943758189678, 0.0011265943758189678, 0.0011265943758189678, 0.0011265943758189678, 0.0011265943758189678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011265943758189678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112659
Iteration 2/1000 | Loss: 0.00005562
Iteration 3/1000 | Loss: 0.00004191
Iteration 4/1000 | Loss: 0.00003670
Iteration 5/1000 | Loss: 0.00003469
Iteration 6/1000 | Loss: 0.00003342
Iteration 7/1000 | Loss: 0.00003249
Iteration 8/1000 | Loss: 0.00003191
Iteration 9/1000 | Loss: 0.00003140
Iteration 10/1000 | Loss: 0.00003108
Iteration 11/1000 | Loss: 0.00003088
Iteration 12/1000 | Loss: 0.00003072
Iteration 13/1000 | Loss: 0.00003065
Iteration 14/1000 | Loss: 0.00003050
Iteration 15/1000 | Loss: 0.00003035
Iteration 16/1000 | Loss: 0.00003025
Iteration 17/1000 | Loss: 0.00003020
Iteration 18/1000 | Loss: 0.00003016
Iteration 19/1000 | Loss: 0.00003015
Iteration 20/1000 | Loss: 0.00003011
Iteration 21/1000 | Loss: 0.00003011
Iteration 22/1000 | Loss: 0.00003011
Iteration 23/1000 | Loss: 0.00003011
Iteration 24/1000 | Loss: 0.00003010
Iteration 25/1000 | Loss: 0.00003006
Iteration 26/1000 | Loss: 0.00002997
Iteration 27/1000 | Loss: 0.00002992
Iteration 28/1000 | Loss: 0.00002987
Iteration 29/1000 | Loss: 0.00002984
Iteration 30/1000 | Loss: 0.00002983
Iteration 31/1000 | Loss: 0.00002983
Iteration 32/1000 | Loss: 0.00002982
Iteration 33/1000 | Loss: 0.00002982
Iteration 34/1000 | Loss: 0.00002980
Iteration 35/1000 | Loss: 0.00002979
Iteration 36/1000 | Loss: 0.00002979
Iteration 37/1000 | Loss: 0.00002979
Iteration 38/1000 | Loss: 0.00002978
Iteration 39/1000 | Loss: 0.00002977
Iteration 40/1000 | Loss: 0.00002977
Iteration 41/1000 | Loss: 0.00002976
Iteration 42/1000 | Loss: 0.00002976
Iteration 43/1000 | Loss: 0.00002975
Iteration 44/1000 | Loss: 0.00002975
Iteration 45/1000 | Loss: 0.00002974
Iteration 46/1000 | Loss: 0.00002974
Iteration 47/1000 | Loss: 0.00002974
Iteration 48/1000 | Loss: 0.00002974
Iteration 49/1000 | Loss: 0.00002974
Iteration 50/1000 | Loss: 0.00002974
Iteration 51/1000 | Loss: 0.00002974
Iteration 52/1000 | Loss: 0.00002974
Iteration 53/1000 | Loss: 0.00002974
Iteration 54/1000 | Loss: 0.00002974
Iteration 55/1000 | Loss: 0.00002974
Iteration 56/1000 | Loss: 0.00002973
Iteration 57/1000 | Loss: 0.00002973
Iteration 58/1000 | Loss: 0.00002971
Iteration 59/1000 | Loss: 0.00002971
Iteration 60/1000 | Loss: 0.00002971
Iteration 61/1000 | Loss: 0.00002971
Iteration 62/1000 | Loss: 0.00002970
Iteration 63/1000 | Loss: 0.00002970
Iteration 64/1000 | Loss: 0.00002970
Iteration 65/1000 | Loss: 0.00002969
Iteration 66/1000 | Loss: 0.00002969
Iteration 67/1000 | Loss: 0.00002969
Iteration 68/1000 | Loss: 0.00002969
Iteration 69/1000 | Loss: 0.00002969
Iteration 70/1000 | Loss: 0.00002968
Iteration 71/1000 | Loss: 0.00002968
Iteration 72/1000 | Loss: 0.00002968
Iteration 73/1000 | Loss: 0.00002968
Iteration 74/1000 | Loss: 0.00002968
Iteration 75/1000 | Loss: 0.00002968
Iteration 76/1000 | Loss: 0.00002968
Iteration 77/1000 | Loss: 0.00002968
Iteration 78/1000 | Loss: 0.00002968
Iteration 79/1000 | Loss: 0.00002968
Iteration 80/1000 | Loss: 0.00002968
Iteration 81/1000 | Loss: 0.00002968
Iteration 82/1000 | Loss: 0.00002968
Iteration 83/1000 | Loss: 0.00002967
Iteration 84/1000 | Loss: 0.00002967
Iteration 85/1000 | Loss: 0.00002967
Iteration 86/1000 | Loss: 0.00002967
Iteration 87/1000 | Loss: 0.00002966
Iteration 88/1000 | Loss: 0.00002966
Iteration 89/1000 | Loss: 0.00002966
Iteration 90/1000 | Loss: 0.00002966
Iteration 91/1000 | Loss: 0.00002966
Iteration 92/1000 | Loss: 0.00002966
Iteration 93/1000 | Loss: 0.00002965
Iteration 94/1000 | Loss: 0.00002965
Iteration 95/1000 | Loss: 0.00002965
Iteration 96/1000 | Loss: 0.00002965
Iteration 97/1000 | Loss: 0.00002965
Iteration 98/1000 | Loss: 0.00002965
Iteration 99/1000 | Loss: 0.00002964
Iteration 100/1000 | Loss: 0.00002964
Iteration 101/1000 | Loss: 0.00002964
Iteration 102/1000 | Loss: 0.00002964
Iteration 103/1000 | Loss: 0.00002964
Iteration 104/1000 | Loss: 0.00002964
Iteration 105/1000 | Loss: 0.00002964
Iteration 106/1000 | Loss: 0.00002964
Iteration 107/1000 | Loss: 0.00002963
Iteration 108/1000 | Loss: 0.00002963
Iteration 109/1000 | Loss: 0.00002963
Iteration 110/1000 | Loss: 0.00002963
Iteration 111/1000 | Loss: 0.00002963
Iteration 112/1000 | Loss: 0.00002963
Iteration 113/1000 | Loss: 0.00002963
Iteration 114/1000 | Loss: 0.00002963
Iteration 115/1000 | Loss: 0.00002963
Iteration 116/1000 | Loss: 0.00002963
Iteration 117/1000 | Loss: 0.00002963
Iteration 118/1000 | Loss: 0.00002963
Iteration 119/1000 | Loss: 0.00002963
Iteration 120/1000 | Loss: 0.00002962
Iteration 121/1000 | Loss: 0.00002962
Iteration 122/1000 | Loss: 0.00002962
Iteration 123/1000 | Loss: 0.00002962
Iteration 124/1000 | Loss: 0.00002962
Iteration 125/1000 | Loss: 0.00002962
Iteration 126/1000 | Loss: 0.00002962
Iteration 127/1000 | Loss: 0.00002961
Iteration 128/1000 | Loss: 0.00002961
Iteration 129/1000 | Loss: 0.00002961
Iteration 130/1000 | Loss: 0.00002961
Iteration 131/1000 | Loss: 0.00002961
Iteration 132/1000 | Loss: 0.00002961
Iteration 133/1000 | Loss: 0.00002961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.9612891012220643e-05, 2.9612891012220643e-05, 2.9612891012220643e-05, 2.9612891012220643e-05, 2.9612891012220643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9612891012220643e-05

Optimization complete. Final v2v error: 4.488039016723633 mm

Highest mean error: 5.244111061096191 mm for frame 139

Lowest mean error: 3.996950626373291 mm for frame 111

Saving results

Total time: 59.27948808670044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456516
Iteration 2/25 | Loss: 0.00135284
Iteration 3/25 | Loss: 0.00119393
Iteration 4/25 | Loss: 0.00116631
Iteration 5/25 | Loss: 0.00115899
Iteration 6/25 | Loss: 0.00115789
Iteration 7/25 | Loss: 0.00115789
Iteration 8/25 | Loss: 0.00115789
Iteration 9/25 | Loss: 0.00115789
Iteration 10/25 | Loss: 0.00115789
Iteration 11/25 | Loss: 0.00115789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001157894846983254, 0.001157894846983254, 0.001157894846983254, 0.001157894846983254, 0.001157894846983254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001157894846983254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17774177
Iteration 2/25 | Loss: 0.00123927
Iteration 3/25 | Loss: 0.00123927
Iteration 4/25 | Loss: 0.00123927
Iteration 5/25 | Loss: 0.00123927
Iteration 6/25 | Loss: 0.00123926
Iteration 7/25 | Loss: 0.00123926
Iteration 8/25 | Loss: 0.00123926
Iteration 9/25 | Loss: 0.00123926
Iteration 10/25 | Loss: 0.00123926
Iteration 11/25 | Loss: 0.00123926
Iteration 12/25 | Loss: 0.00123926
Iteration 13/25 | Loss: 0.00123926
Iteration 14/25 | Loss: 0.00123926
Iteration 15/25 | Loss: 0.00123926
Iteration 16/25 | Loss: 0.00123926
Iteration 17/25 | Loss: 0.00123926
Iteration 18/25 | Loss: 0.00123926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012392634525895119, 0.0012392634525895119, 0.0012392634525895119, 0.0012392634525895119, 0.0012392634525895119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012392634525895119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123926
Iteration 2/1000 | Loss: 0.00003582
Iteration 3/1000 | Loss: 0.00002336
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001654
Iteration 8/1000 | Loss: 0.00001591
Iteration 9/1000 | Loss: 0.00001567
Iteration 10/1000 | Loss: 0.00001539
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001512
Iteration 15/1000 | Loss: 0.00001506
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001489
Iteration 20/1000 | Loss: 0.00001486
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001478
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001477
Iteration 27/1000 | Loss: 0.00001477
Iteration 28/1000 | Loss: 0.00001476
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001475
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001473
Iteration 36/1000 | Loss: 0.00001473
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001472
Iteration 39/1000 | Loss: 0.00001472
Iteration 40/1000 | Loss: 0.00001472
Iteration 41/1000 | Loss: 0.00001472
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001471
Iteration 48/1000 | Loss: 0.00001471
Iteration 49/1000 | Loss: 0.00001471
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001470
Iteration 56/1000 | Loss: 0.00001470
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001469
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001469
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001468
Iteration 67/1000 | Loss: 0.00001468
Iteration 68/1000 | Loss: 0.00001468
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001467
Iteration 79/1000 | Loss: 0.00001467
Iteration 80/1000 | Loss: 0.00001467
Iteration 81/1000 | Loss: 0.00001467
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001467
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001467
Iteration 95/1000 | Loss: 0.00001467
Iteration 96/1000 | Loss: 0.00001467
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001467
Iteration 99/1000 | Loss: 0.00001467
Iteration 100/1000 | Loss: 0.00001467
Iteration 101/1000 | Loss: 0.00001467
Iteration 102/1000 | Loss: 0.00001467
Iteration 103/1000 | Loss: 0.00001467
Iteration 104/1000 | Loss: 0.00001467
Iteration 105/1000 | Loss: 0.00001467
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001467
Iteration 108/1000 | Loss: 0.00001467
Iteration 109/1000 | Loss: 0.00001467
Iteration 110/1000 | Loss: 0.00001467
Iteration 111/1000 | Loss: 0.00001467
Iteration 112/1000 | Loss: 0.00001467
Iteration 113/1000 | Loss: 0.00001467
Iteration 114/1000 | Loss: 0.00001467
Iteration 115/1000 | Loss: 0.00001467
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.466756293666549e-05, 1.466756293666549e-05, 1.466756293666549e-05, 1.466756293666549e-05, 1.466756293666549e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.466756293666549e-05

Optimization complete. Final v2v error: 3.357447862625122 mm

Highest mean error: 3.60815167427063 mm for frame 202

Lowest mean error: 2.993793249130249 mm for frame 8

Saving results

Total time: 37.889119386672974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134611
Iteration 2/25 | Loss: 0.00212148
Iteration 3/25 | Loss: 0.00131998
Iteration 4/25 | Loss: 0.00121509
Iteration 5/25 | Loss: 0.00121383
Iteration 6/25 | Loss: 0.00118233
Iteration 7/25 | Loss: 0.00115906
Iteration 8/25 | Loss: 0.00113533
Iteration 9/25 | Loss: 0.00113342
Iteration 10/25 | Loss: 0.00112838
Iteration 11/25 | Loss: 0.00112978
Iteration 12/25 | Loss: 0.00112742
Iteration 13/25 | Loss: 0.00113050
Iteration 14/25 | Loss: 0.00112597
Iteration 15/25 | Loss: 0.00112674
Iteration 16/25 | Loss: 0.00112720
Iteration 17/25 | Loss: 0.00112623
Iteration 18/25 | Loss: 0.00112604
Iteration 19/25 | Loss: 0.00112581
Iteration 20/25 | Loss: 0.00112581
Iteration 21/25 | Loss: 0.00112580
Iteration 22/25 | Loss: 0.00112579
Iteration 23/25 | Loss: 0.00112575
Iteration 24/25 | Loss: 0.00112575
Iteration 25/25 | Loss: 0.00112575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28385484
Iteration 2/25 | Loss: 0.00146614
Iteration 3/25 | Loss: 0.00145656
Iteration 4/25 | Loss: 0.00145656
Iteration 5/25 | Loss: 0.00145656
Iteration 6/25 | Loss: 0.00145656
Iteration 7/25 | Loss: 0.00145656
Iteration 8/25 | Loss: 0.00145656
Iteration 9/25 | Loss: 0.00145656
Iteration 10/25 | Loss: 0.00145656
Iteration 11/25 | Loss: 0.00145656
Iteration 12/25 | Loss: 0.00145656
Iteration 13/25 | Loss: 0.00145656
Iteration 14/25 | Loss: 0.00145656
Iteration 15/25 | Loss: 0.00145656
Iteration 16/25 | Loss: 0.00145656
Iteration 17/25 | Loss: 0.00145656
Iteration 18/25 | Loss: 0.00145656
Iteration 19/25 | Loss: 0.00145656
Iteration 20/25 | Loss: 0.00145656
Iteration 21/25 | Loss: 0.00145656
Iteration 22/25 | Loss: 0.00145656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014565594028681517, 0.0014565594028681517, 0.0014565594028681517, 0.0014565594028681517, 0.0014565594028681517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014565594028681517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145656
Iteration 2/1000 | Loss: 0.00004310
Iteration 3/1000 | Loss: 0.00008961
Iteration 4/1000 | Loss: 0.00003110
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00081946
Iteration 8/1000 | Loss: 0.00099651
Iteration 9/1000 | Loss: 0.00068577
Iteration 10/1000 | Loss: 0.00045405
Iteration 11/1000 | Loss: 0.00078222
Iteration 12/1000 | Loss: 0.00043131
Iteration 13/1000 | Loss: 0.00062637
Iteration 14/1000 | Loss: 0.00031378
Iteration 15/1000 | Loss: 0.00003274
Iteration 16/1000 | Loss: 0.00001678
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00002831
Iteration 19/1000 | Loss: 0.00001513
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001845
Iteration 22/1000 | Loss: 0.00003579
Iteration 23/1000 | Loss: 0.00002669
Iteration 24/1000 | Loss: 0.00001427
Iteration 25/1000 | Loss: 0.00001835
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001986
Iteration 28/1000 | Loss: 0.00007867
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00004544
Iteration 31/1000 | Loss: 0.00001110
Iteration 32/1000 | Loss: 0.00001108
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001604
Iteration 35/1000 | Loss: 0.00001732
Iteration 36/1000 | Loss: 0.00001068
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001064
Iteration 42/1000 | Loss: 0.00001064
Iteration 43/1000 | Loss: 0.00001064
Iteration 44/1000 | Loss: 0.00001064
Iteration 45/1000 | Loss: 0.00001287
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001058
Iteration 49/1000 | Loss: 0.00001058
Iteration 50/1000 | Loss: 0.00001058
Iteration 51/1000 | Loss: 0.00001058
Iteration 52/1000 | Loss: 0.00001058
Iteration 53/1000 | Loss: 0.00001058
Iteration 54/1000 | Loss: 0.00001058
Iteration 55/1000 | Loss: 0.00001058
Iteration 56/1000 | Loss: 0.00001058
Iteration 57/1000 | Loss: 0.00001057
Iteration 58/1000 | Loss: 0.00001057
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001054
Iteration 62/1000 | Loss: 0.00001054
Iteration 63/1000 | Loss: 0.00001054
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001054
Iteration 66/1000 | Loss: 0.00001054
Iteration 67/1000 | Loss: 0.00001054
Iteration 68/1000 | Loss: 0.00001054
Iteration 69/1000 | Loss: 0.00001053
Iteration 70/1000 | Loss: 0.00001053
Iteration 71/1000 | Loss: 0.00001053
Iteration 72/1000 | Loss: 0.00001053
Iteration 73/1000 | Loss: 0.00001053
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001053
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001053
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001053
Iteration 83/1000 | Loss: 0.00001053
Iteration 84/1000 | Loss: 0.00001053
Iteration 85/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.0532717169553507e-05, 1.0532717169553507e-05, 1.0532717169553507e-05, 1.0532717169553507e-05, 1.0532717169553507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0532717169553507e-05

Optimization complete. Final v2v error: 2.9125888347625732 mm

Highest mean error: 3.5780680179595947 mm for frame 85

Lowest mean error: 2.7155628204345703 mm for frame 75

Saving results

Total time: 83.7078309059143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_38_us_2848/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_38_us_2848/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498830
Iteration 2/25 | Loss: 0.00127080
Iteration 3/25 | Loss: 0.00116864
Iteration 4/25 | Loss: 0.00115735
Iteration 5/25 | Loss: 0.00115480
Iteration 6/25 | Loss: 0.00115391
Iteration 7/25 | Loss: 0.00115391
Iteration 8/25 | Loss: 0.00115391
Iteration 9/25 | Loss: 0.00115391
Iteration 10/25 | Loss: 0.00115391
Iteration 11/25 | Loss: 0.00115391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011539128609001637, 0.0011539128609001637, 0.0011539128609001637, 0.0011539128609001637, 0.0011539128609001637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011539128609001637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.94119024
Iteration 2/25 | Loss: 0.00131693
Iteration 3/25 | Loss: 0.00131693
Iteration 4/25 | Loss: 0.00131693
Iteration 5/25 | Loss: 0.00131693
Iteration 6/25 | Loss: 0.00131693
Iteration 7/25 | Loss: 0.00131693
Iteration 8/25 | Loss: 0.00131693
Iteration 9/25 | Loss: 0.00131693
Iteration 10/25 | Loss: 0.00131693
Iteration 11/25 | Loss: 0.00131693
Iteration 12/25 | Loss: 0.00131693
Iteration 13/25 | Loss: 0.00131692
Iteration 14/25 | Loss: 0.00131692
Iteration 15/25 | Loss: 0.00131692
Iteration 16/25 | Loss: 0.00131692
Iteration 17/25 | Loss: 0.00131692
Iteration 18/25 | Loss: 0.00131692
Iteration 19/25 | Loss: 0.00131692
Iteration 20/25 | Loss: 0.00131692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001316924812272191, 0.001316924812272191, 0.001316924812272191, 0.001316924812272191, 0.001316924812272191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001316924812272191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131692
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00002207
Iteration 4/1000 | Loss: 0.00001864
Iteration 5/1000 | Loss: 0.00001732
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001574
Iteration 8/1000 | Loss: 0.00001538
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001462
Iteration 26/1000 | Loss: 0.00001461
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001460
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001459
Iteration 32/1000 | Loss: 0.00001459
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001456
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001454
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001446
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001446
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001445
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001442
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001437
Iteration 71/1000 | Loss: 0.00001436
Iteration 72/1000 | Loss: 0.00001436
Iteration 73/1000 | Loss: 0.00001435
Iteration 74/1000 | Loss: 0.00001435
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001434
Iteration 83/1000 | Loss: 0.00001434
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001433
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001433
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001432
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001432
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001431
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001431
Iteration 106/1000 | Loss: 0.00001431
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001431
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001431
Iteration 114/1000 | Loss: 0.00001431
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001431
Iteration 117/1000 | Loss: 0.00001431
Iteration 118/1000 | Loss: 0.00001431
Iteration 119/1000 | Loss: 0.00001431
Iteration 120/1000 | Loss: 0.00001431
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.4310591723187827e-05, 1.4310591723187827e-05, 1.4310591723187827e-05, 1.4310591723187827e-05, 1.4310591723187827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4310591723187827e-05

Optimization complete. Final v2v error: 3.2830262184143066 mm

Highest mean error: 3.753904342651367 mm for frame 137

Lowest mean error: 2.722473382949829 mm for frame 6

Saving results

Total time: 35.122478008270264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925294
Iteration 2/25 | Loss: 0.00087494
Iteration 3/25 | Loss: 0.00070323
Iteration 4/25 | Loss: 0.00065528
Iteration 5/25 | Loss: 0.00064154
Iteration 6/25 | Loss: 0.00063787
Iteration 7/25 | Loss: 0.00063704
Iteration 8/25 | Loss: 0.00063700
Iteration 9/25 | Loss: 0.00063700
Iteration 10/25 | Loss: 0.00063700
Iteration 11/25 | Loss: 0.00063700
Iteration 12/25 | Loss: 0.00063700
Iteration 13/25 | Loss: 0.00063700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006370028131641448, 0.0006370028131641448, 0.0006370028131641448, 0.0006370028131641448, 0.0006370028131641448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006370028131641448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38641286
Iteration 2/25 | Loss: 0.00020436
Iteration 3/25 | Loss: 0.00020436
Iteration 4/25 | Loss: 0.00020435
Iteration 5/25 | Loss: 0.00020435
Iteration 6/25 | Loss: 0.00020435
Iteration 7/25 | Loss: 0.00020435
Iteration 8/25 | Loss: 0.00020435
Iteration 9/25 | Loss: 0.00020435
Iteration 10/25 | Loss: 0.00020435
Iteration 11/25 | Loss: 0.00020435
Iteration 12/25 | Loss: 0.00020435
Iteration 13/25 | Loss: 0.00020435
Iteration 14/25 | Loss: 0.00020435
Iteration 15/25 | Loss: 0.00020435
Iteration 16/25 | Loss: 0.00020435
Iteration 17/25 | Loss: 0.00020435
Iteration 18/25 | Loss: 0.00020435
Iteration 19/25 | Loss: 0.00020435
Iteration 20/25 | Loss: 0.00020435
Iteration 21/25 | Loss: 0.00020435
Iteration 22/25 | Loss: 0.00020435
Iteration 23/25 | Loss: 0.00020435
Iteration 24/25 | Loss: 0.00020435
Iteration 25/25 | Loss: 0.00020435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00020435272017493844, 0.00020435272017493844, 0.00020435272017493844, 0.00020435272017493844, 0.00020435272017493844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020435272017493844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020435
Iteration 2/1000 | Loss: 0.00003284
Iteration 3/1000 | Loss: 0.00002260
Iteration 4/1000 | Loss: 0.00002078
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001793
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001749
Iteration 11/1000 | Loss: 0.00001740
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001733
Iteration 21/1000 | Loss: 0.00001732
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001729
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001724
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001724
Iteration 37/1000 | Loss: 0.00001724
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001724
Iteration 40/1000 | Loss: 0.00001723
Iteration 41/1000 | Loss: 0.00001723
Iteration 42/1000 | Loss: 0.00001722
Iteration 43/1000 | Loss: 0.00001722
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001721
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001720
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001719
Iteration 53/1000 | Loss: 0.00001719
Iteration 54/1000 | Loss: 0.00001719
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001717
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001716
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001714
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001714
Iteration 73/1000 | Loss: 0.00001714
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001713
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001711
Iteration 85/1000 | Loss: 0.00001711
Iteration 86/1000 | Loss: 0.00001711
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Iteration 93/1000 | Loss: 0.00001710
Iteration 94/1000 | Loss: 0.00001710
Iteration 95/1000 | Loss: 0.00001710
Iteration 96/1000 | Loss: 0.00001710
Iteration 97/1000 | Loss: 0.00001710
Iteration 98/1000 | Loss: 0.00001710
Iteration 99/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.710237847873941e-05, 1.710237847873941e-05, 1.710237847873941e-05, 1.710237847873941e-05, 1.710237847873941e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.710237847873941e-05

Optimization complete. Final v2v error: 3.4641635417938232 mm

Highest mean error: 3.8187808990478516 mm for frame 166

Lowest mean error: 2.935884475708008 mm for frame 216

Saving results

Total time: 37.39466691017151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364090
Iteration 2/25 | Loss: 0.00070597
Iteration 3/25 | Loss: 0.00059355
Iteration 4/25 | Loss: 0.00057498
Iteration 5/25 | Loss: 0.00056903
Iteration 6/25 | Loss: 0.00056770
Iteration 7/25 | Loss: 0.00056754
Iteration 8/25 | Loss: 0.00056754
Iteration 9/25 | Loss: 0.00056754
Iteration 10/25 | Loss: 0.00056754
Iteration 11/25 | Loss: 0.00056754
Iteration 12/25 | Loss: 0.00056754
Iteration 13/25 | Loss: 0.00056754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005675422144122422, 0.0005675422144122422, 0.0005675422144122422, 0.0005675422144122422, 0.0005675422144122422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005675422144122422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46362960
Iteration 2/25 | Loss: 0.00020447
Iteration 3/25 | Loss: 0.00020447
Iteration 4/25 | Loss: 0.00020447
Iteration 5/25 | Loss: 0.00020446
Iteration 6/25 | Loss: 0.00020446
Iteration 7/25 | Loss: 0.00020446
Iteration 8/25 | Loss: 0.00020446
Iteration 9/25 | Loss: 0.00020446
Iteration 10/25 | Loss: 0.00020446
Iteration 11/25 | Loss: 0.00020446
Iteration 12/25 | Loss: 0.00020446
Iteration 13/25 | Loss: 0.00020446
Iteration 14/25 | Loss: 0.00020446
Iteration 15/25 | Loss: 0.00020446
Iteration 16/25 | Loss: 0.00020446
Iteration 17/25 | Loss: 0.00020446
Iteration 18/25 | Loss: 0.00020446
Iteration 19/25 | Loss: 0.00020446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00020446343114599586, 0.00020446343114599586, 0.00020446343114599586, 0.00020446343114599586, 0.00020446343114599586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020446343114599586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020446
Iteration 2/1000 | Loss: 0.00001647
Iteration 3/1000 | Loss: 0.00001260
Iteration 4/1000 | Loss: 0.00001189
Iteration 5/1000 | Loss: 0.00001149
Iteration 6/1000 | Loss: 0.00001137
Iteration 7/1000 | Loss: 0.00001137
Iteration 8/1000 | Loss: 0.00001118
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001101
Iteration 12/1000 | Loss: 0.00001101
Iteration 13/1000 | Loss: 0.00001093
Iteration 14/1000 | Loss: 0.00001092
Iteration 15/1000 | Loss: 0.00001087
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001087
Iteration 18/1000 | Loss: 0.00001086
Iteration 19/1000 | Loss: 0.00001086
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001086
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001085
Iteration 24/1000 | Loss: 0.00001083
Iteration 25/1000 | Loss: 0.00001083
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001082
Iteration 28/1000 | Loss: 0.00001082
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001078
Iteration 39/1000 | Loss: 0.00001078
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001076
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001075
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001074
Iteration 50/1000 | Loss: 0.00001074
Iteration 51/1000 | Loss: 0.00001074
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001073
Iteration 54/1000 | Loss: 0.00001073
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001072
Iteration 61/1000 | Loss: 0.00001072
Iteration 62/1000 | Loss: 0.00001072
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001072
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001072
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001072
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001071
Iteration 75/1000 | Loss: 0.00001071
Iteration 76/1000 | Loss: 0.00001071
Iteration 77/1000 | Loss: 0.00001071
Iteration 78/1000 | Loss: 0.00001071
Iteration 79/1000 | Loss: 0.00001071
Iteration 80/1000 | Loss: 0.00001071
Iteration 81/1000 | Loss: 0.00001071
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001071
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001071
Iteration 88/1000 | Loss: 0.00001071
Iteration 89/1000 | Loss: 0.00001071
Iteration 90/1000 | Loss: 0.00001071
Iteration 91/1000 | Loss: 0.00001071
Iteration 92/1000 | Loss: 0.00001071
Iteration 93/1000 | Loss: 0.00001071
Iteration 94/1000 | Loss: 0.00001071
Iteration 95/1000 | Loss: 0.00001071
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001071
Iteration 99/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.0706284228945151e-05, 1.0706284228945151e-05, 1.0706284228945151e-05, 1.0706284228945151e-05, 1.0706284228945151e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0706284228945151e-05

Optimization complete. Final v2v error: 2.746084451675415 mm

Highest mean error: 2.9429237842559814 mm for frame 147

Lowest mean error: 2.514599323272705 mm for frame 42

Saving results

Total time: 29.275237798690796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468767
Iteration 2/25 | Loss: 0.00083136
Iteration 3/25 | Loss: 0.00070025
Iteration 4/25 | Loss: 0.00067016
Iteration 5/25 | Loss: 0.00065771
Iteration 6/25 | Loss: 0.00065462
Iteration 7/25 | Loss: 0.00065319
Iteration 8/25 | Loss: 0.00065304
Iteration 9/25 | Loss: 0.00065304
Iteration 10/25 | Loss: 0.00065304
Iteration 11/25 | Loss: 0.00065304
Iteration 12/25 | Loss: 0.00065304
Iteration 13/25 | Loss: 0.00065304
Iteration 14/25 | Loss: 0.00065304
Iteration 15/25 | Loss: 0.00065304
Iteration 16/25 | Loss: 0.00065304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006530440296046436, 0.0006530440296046436, 0.0006530440296046436, 0.0006530440296046436, 0.0006530440296046436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006530440296046436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17089581
Iteration 2/25 | Loss: 0.00017647
Iteration 3/25 | Loss: 0.00017646
Iteration 4/25 | Loss: 0.00017645
Iteration 5/25 | Loss: 0.00017645
Iteration 6/25 | Loss: 0.00017645
Iteration 7/25 | Loss: 0.00017645
Iteration 8/25 | Loss: 0.00017645
Iteration 9/25 | Loss: 0.00017645
Iteration 10/25 | Loss: 0.00017645
Iteration 11/25 | Loss: 0.00017645
Iteration 12/25 | Loss: 0.00017645
Iteration 13/25 | Loss: 0.00017645
Iteration 14/25 | Loss: 0.00017645
Iteration 15/25 | Loss: 0.00017645
Iteration 16/25 | Loss: 0.00017645
Iteration 17/25 | Loss: 0.00017645
Iteration 18/25 | Loss: 0.00017645
Iteration 19/25 | Loss: 0.00017645
Iteration 20/25 | Loss: 0.00017645
Iteration 21/25 | Loss: 0.00017645
Iteration 22/25 | Loss: 0.00017645
Iteration 23/25 | Loss: 0.00017645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00017645182379055768, 0.00017645182379055768, 0.00017645182379055768, 0.00017645182379055768, 0.00017645182379055768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017645182379055768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017645
Iteration 2/1000 | Loss: 0.00003047
Iteration 3/1000 | Loss: 0.00002277
Iteration 4/1000 | Loss: 0.00002165
Iteration 5/1000 | Loss: 0.00002080
Iteration 6/1000 | Loss: 0.00002035
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001945
Iteration 10/1000 | Loss: 0.00001928
Iteration 11/1000 | Loss: 0.00001917
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001906
Iteration 15/1000 | Loss: 0.00001906
Iteration 16/1000 | Loss: 0.00001906
Iteration 17/1000 | Loss: 0.00001905
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001904
Iteration 21/1000 | Loss: 0.00001903
Iteration 22/1000 | Loss: 0.00001901
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001900
Iteration 25/1000 | Loss: 0.00001900
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001899
Iteration 28/1000 | Loss: 0.00001899
Iteration 29/1000 | Loss: 0.00001899
Iteration 30/1000 | Loss: 0.00001899
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001898
Iteration 34/1000 | Loss: 0.00001898
Iteration 35/1000 | Loss: 0.00001898
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001897
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001897
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001896
Iteration 45/1000 | Loss: 0.00001896
Iteration 46/1000 | Loss: 0.00001896
Iteration 47/1000 | Loss: 0.00001895
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001893
Iteration 58/1000 | Loss: 0.00001893
Iteration 59/1000 | Loss: 0.00001893
Iteration 60/1000 | Loss: 0.00001892
Iteration 61/1000 | Loss: 0.00001892
Iteration 62/1000 | Loss: 0.00001891
Iteration 63/1000 | Loss: 0.00001891
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001889
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001888
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001885
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.885242272692267e-05, 1.885242272692267e-05, 1.885242272692267e-05, 1.885242272692267e-05, 1.885242272692267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.885242272692267e-05

Optimization complete. Final v2v error: 3.6444363594055176 mm

Highest mean error: 4.279090404510498 mm for frame 183

Lowest mean error: 3.31786847114563 mm for frame 147

Saving results

Total time: 35.97592544555664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824803
Iteration 2/25 | Loss: 0.00074405
Iteration 3/25 | Loss: 0.00062145
Iteration 4/25 | Loss: 0.00059553
Iteration 5/25 | Loss: 0.00059193
Iteration 6/25 | Loss: 0.00059141
Iteration 7/25 | Loss: 0.00059141
Iteration 8/25 | Loss: 0.00059141
Iteration 9/25 | Loss: 0.00059141
Iteration 10/25 | Loss: 0.00059141
Iteration 11/25 | Loss: 0.00059141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005914067151024938, 0.0005914067151024938, 0.0005914067151024938, 0.0005914067151024938, 0.0005914067151024938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005914067151024938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46828508
Iteration 2/25 | Loss: 0.00013820
Iteration 3/25 | Loss: 0.00013820
Iteration 4/25 | Loss: 0.00013820
Iteration 5/25 | Loss: 0.00013820
Iteration 6/25 | Loss: 0.00013820
Iteration 7/25 | Loss: 0.00013820
Iteration 8/25 | Loss: 0.00013820
Iteration 9/25 | Loss: 0.00013820
Iteration 10/25 | Loss: 0.00013820
Iteration 11/25 | Loss: 0.00013820
Iteration 12/25 | Loss: 0.00013820
Iteration 13/25 | Loss: 0.00013820
Iteration 14/25 | Loss: 0.00013820
Iteration 15/25 | Loss: 0.00013820
Iteration 16/25 | Loss: 0.00013820
Iteration 17/25 | Loss: 0.00013820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00013819690502714366, 0.00013819690502714366, 0.00013819690502714366, 0.00013819690502714366, 0.00013819690502714366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013819690502714366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013820
Iteration 2/1000 | Loss: 0.00002502
Iteration 3/1000 | Loss: 0.00001957
Iteration 4/1000 | Loss: 0.00001831
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001616
Iteration 8/1000 | Loss: 0.00001593
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001568
Iteration 15/1000 | Loss: 0.00001565
Iteration 16/1000 | Loss: 0.00001565
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001560
Iteration 20/1000 | Loss: 0.00001559
Iteration 21/1000 | Loss: 0.00001555
Iteration 22/1000 | Loss: 0.00001555
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001554
Iteration 25/1000 | Loss: 0.00001554
Iteration 26/1000 | Loss: 0.00001553
Iteration 27/1000 | Loss: 0.00001553
Iteration 28/1000 | Loss: 0.00001553
Iteration 29/1000 | Loss: 0.00001552
Iteration 30/1000 | Loss: 0.00001552
Iteration 31/1000 | Loss: 0.00001552
Iteration 32/1000 | Loss: 0.00001552
Iteration 33/1000 | Loss: 0.00001552
Iteration 34/1000 | Loss: 0.00001552
Iteration 35/1000 | Loss: 0.00001551
Iteration 36/1000 | Loss: 0.00001551
Iteration 37/1000 | Loss: 0.00001551
Iteration 38/1000 | Loss: 0.00001551
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001550
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001550
Iteration 43/1000 | Loss: 0.00001549
Iteration 44/1000 | Loss: 0.00001549
Iteration 45/1000 | Loss: 0.00001549
Iteration 46/1000 | Loss: 0.00001549
Iteration 47/1000 | Loss: 0.00001549
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001549
Iteration 51/1000 | Loss: 0.00001549
Iteration 52/1000 | Loss: 0.00001549
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001543
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001543
Iteration 71/1000 | Loss: 0.00001543
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001542
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00001542
Iteration 82/1000 | Loss: 0.00001542
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001542
Iteration 99/1000 | Loss: 0.00001542
Iteration 100/1000 | Loss: 0.00001542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.5421957868966274e-05, 1.5421957868966274e-05, 1.5421957868966274e-05, 1.5421957868966274e-05, 1.5421957868966274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5421957868966274e-05

Optimization complete. Final v2v error: 3.2508580684661865 mm

Highest mean error: 3.6077349185943604 mm for frame 63

Lowest mean error: 3.122915744781494 mm for frame 145

Saving results

Total time: 30.02824831008911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381812
Iteration 2/25 | Loss: 0.00073525
Iteration 3/25 | Loss: 0.00062838
Iteration 4/25 | Loss: 0.00059107
Iteration 5/25 | Loss: 0.00058478
Iteration 6/25 | Loss: 0.00058410
Iteration 7/25 | Loss: 0.00058410
Iteration 8/25 | Loss: 0.00058410
Iteration 9/25 | Loss: 0.00058410
Iteration 10/25 | Loss: 0.00058410
Iteration 11/25 | Loss: 0.00058410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005841049714945257, 0.0005841049714945257, 0.0005841049714945257, 0.0005841049714945257, 0.0005841049714945257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005841049714945257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56669784
Iteration 2/25 | Loss: 0.00015611
Iteration 3/25 | Loss: 0.00015611
Iteration 4/25 | Loss: 0.00015611
Iteration 5/25 | Loss: 0.00015611
Iteration 6/25 | Loss: 0.00015611
Iteration 7/25 | Loss: 0.00015610
Iteration 8/25 | Loss: 0.00015610
Iteration 9/25 | Loss: 0.00015610
Iteration 10/25 | Loss: 0.00015610
Iteration 11/25 | Loss: 0.00015610
Iteration 12/25 | Loss: 0.00015610
Iteration 13/25 | Loss: 0.00015610
Iteration 14/25 | Loss: 0.00015610
Iteration 15/25 | Loss: 0.00015610
Iteration 16/25 | Loss: 0.00015610
Iteration 17/25 | Loss: 0.00015610
Iteration 18/25 | Loss: 0.00015610
Iteration 19/25 | Loss: 0.00015610
Iteration 20/25 | Loss: 0.00015610
Iteration 21/25 | Loss: 0.00015610
Iteration 22/25 | Loss: 0.00015610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00015610399714205414, 0.00015610399714205414, 0.00015610399714205414, 0.00015610399714205414, 0.00015610399714205414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015610399714205414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015610
Iteration 2/1000 | Loss: 0.00002658
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001661
Iteration 6/1000 | Loss: 0.00001587
Iteration 7/1000 | Loss: 0.00001548
Iteration 8/1000 | Loss: 0.00001532
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001511
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001507
Iteration 15/1000 | Loss: 0.00001506
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001502
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001496
Iteration 24/1000 | Loss: 0.00001495
Iteration 25/1000 | Loss: 0.00001495
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001493
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001492
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001492
Iteration 39/1000 | Loss: 0.00001492
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001491
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001491
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001490
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001489
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001487
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001485
Iteration 60/1000 | Loss: 0.00001485
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001484
Iteration 63/1000 | Loss: 0.00001483
Iteration 64/1000 | Loss: 0.00001483
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001482
Iteration 67/1000 | Loss: 0.00001482
Iteration 68/1000 | Loss: 0.00001482
Iteration 69/1000 | Loss: 0.00001482
Iteration 70/1000 | Loss: 0.00001481
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001478
Iteration 80/1000 | Loss: 0.00001478
Iteration 81/1000 | Loss: 0.00001478
Iteration 82/1000 | Loss: 0.00001478
Iteration 83/1000 | Loss: 0.00001478
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001475
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001473
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001472
Iteration 119/1000 | Loss: 0.00001472
Iteration 120/1000 | Loss: 0.00001472
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001471
Iteration 129/1000 | Loss: 0.00001471
Iteration 130/1000 | Loss: 0.00001471
Iteration 131/1000 | Loss: 0.00001471
Iteration 132/1000 | Loss: 0.00001471
Iteration 133/1000 | Loss: 0.00001471
Iteration 134/1000 | Loss: 0.00001471
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001471
Iteration 138/1000 | Loss: 0.00001471
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001471
Iteration 141/1000 | Loss: 0.00001471
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001471
Iteration 149/1000 | Loss: 0.00001471
Iteration 150/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.470913048251532e-05, 1.470913048251532e-05, 1.470913048251532e-05, 1.470913048251532e-05, 1.470913048251532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.470913048251532e-05

Optimization complete. Final v2v error: 3.220534086227417 mm

Highest mean error: 3.4672529697418213 mm for frame 169

Lowest mean error: 3.026362895965576 mm for frame 178

Saving results

Total time: 36.94191384315491
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822095
Iteration 2/25 | Loss: 0.00128733
Iteration 3/25 | Loss: 0.00083425
Iteration 4/25 | Loss: 0.00077706
Iteration 5/25 | Loss: 0.00075845
Iteration 6/25 | Loss: 0.00075380
Iteration 7/25 | Loss: 0.00075240
Iteration 8/25 | Loss: 0.00075210
Iteration 9/25 | Loss: 0.00075210
Iteration 10/25 | Loss: 0.00075210
Iteration 11/25 | Loss: 0.00075210
Iteration 12/25 | Loss: 0.00075210
Iteration 13/25 | Loss: 0.00075210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007521043880842626, 0.0007521043880842626, 0.0007521043880842626, 0.0007521043880842626, 0.0007521043880842626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007521043880842626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16430306
Iteration 2/25 | Loss: 0.00023876
Iteration 3/25 | Loss: 0.00023876
Iteration 4/25 | Loss: 0.00023876
Iteration 5/25 | Loss: 0.00023875
Iteration 6/25 | Loss: 0.00023875
Iteration 7/25 | Loss: 0.00023875
Iteration 8/25 | Loss: 0.00023875
Iteration 9/25 | Loss: 0.00023875
Iteration 10/25 | Loss: 0.00023875
Iteration 11/25 | Loss: 0.00023875
Iteration 12/25 | Loss: 0.00023875
Iteration 13/25 | Loss: 0.00023875
Iteration 14/25 | Loss: 0.00023875
Iteration 15/25 | Loss: 0.00023875
Iteration 16/25 | Loss: 0.00023875
Iteration 17/25 | Loss: 0.00023875
Iteration 18/25 | Loss: 0.00023875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00023875331680756062, 0.00023875331680756062, 0.00023875331680756062, 0.00023875331680756062, 0.00023875331680756062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023875331680756062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023875
Iteration 2/1000 | Loss: 0.00005138
Iteration 3/1000 | Loss: 0.00004027
Iteration 4/1000 | Loss: 0.00003522
Iteration 5/1000 | Loss: 0.00003338
Iteration 6/1000 | Loss: 0.00003227
Iteration 7/1000 | Loss: 0.00003154
Iteration 8/1000 | Loss: 0.00003071
Iteration 9/1000 | Loss: 0.00003015
Iteration 10/1000 | Loss: 0.00002966
Iteration 11/1000 | Loss: 0.00002925
Iteration 12/1000 | Loss: 0.00002884
Iteration 13/1000 | Loss: 0.00002843
Iteration 14/1000 | Loss: 0.00002814
Iteration 15/1000 | Loss: 0.00002791
Iteration 16/1000 | Loss: 0.00002770
Iteration 17/1000 | Loss: 0.00002757
Iteration 18/1000 | Loss: 0.00002756
Iteration 19/1000 | Loss: 0.00002753
Iteration 20/1000 | Loss: 0.00002752
Iteration 21/1000 | Loss: 0.00002752
Iteration 22/1000 | Loss: 0.00002749
Iteration 23/1000 | Loss: 0.00002748
Iteration 24/1000 | Loss: 0.00002746
Iteration 25/1000 | Loss: 0.00002745
Iteration 26/1000 | Loss: 0.00002742
Iteration 27/1000 | Loss: 0.00002742
Iteration 28/1000 | Loss: 0.00002741
Iteration 29/1000 | Loss: 0.00002737
Iteration 30/1000 | Loss: 0.00002735
Iteration 31/1000 | Loss: 0.00002733
Iteration 32/1000 | Loss: 0.00002733
Iteration 33/1000 | Loss: 0.00002732
Iteration 34/1000 | Loss: 0.00002731
Iteration 35/1000 | Loss: 0.00002730
Iteration 36/1000 | Loss: 0.00002730
Iteration 37/1000 | Loss: 0.00002730
Iteration 38/1000 | Loss: 0.00002729
Iteration 39/1000 | Loss: 0.00002729
Iteration 40/1000 | Loss: 0.00002729
Iteration 41/1000 | Loss: 0.00002729
Iteration 42/1000 | Loss: 0.00002729
Iteration 43/1000 | Loss: 0.00002729
Iteration 44/1000 | Loss: 0.00002729
Iteration 45/1000 | Loss: 0.00002729
Iteration 46/1000 | Loss: 0.00002729
Iteration 47/1000 | Loss: 0.00002728
Iteration 48/1000 | Loss: 0.00002728
Iteration 49/1000 | Loss: 0.00002728
Iteration 50/1000 | Loss: 0.00002728
Iteration 51/1000 | Loss: 0.00002727
Iteration 52/1000 | Loss: 0.00002727
Iteration 53/1000 | Loss: 0.00002727
Iteration 54/1000 | Loss: 0.00002727
Iteration 55/1000 | Loss: 0.00002727
Iteration 56/1000 | Loss: 0.00002727
Iteration 57/1000 | Loss: 0.00002727
Iteration 58/1000 | Loss: 0.00002727
Iteration 59/1000 | Loss: 0.00002727
Iteration 60/1000 | Loss: 0.00002726
Iteration 61/1000 | Loss: 0.00002726
Iteration 62/1000 | Loss: 0.00002725
Iteration 63/1000 | Loss: 0.00002725
Iteration 64/1000 | Loss: 0.00002725
Iteration 65/1000 | Loss: 0.00002725
Iteration 66/1000 | Loss: 0.00002725
Iteration 67/1000 | Loss: 0.00002725
Iteration 68/1000 | Loss: 0.00002724
Iteration 69/1000 | Loss: 0.00002724
Iteration 70/1000 | Loss: 0.00002724
Iteration 71/1000 | Loss: 0.00002724
Iteration 72/1000 | Loss: 0.00002724
Iteration 73/1000 | Loss: 0.00002724
Iteration 74/1000 | Loss: 0.00002724
Iteration 75/1000 | Loss: 0.00002724
Iteration 76/1000 | Loss: 0.00002723
Iteration 77/1000 | Loss: 0.00002722
Iteration 78/1000 | Loss: 0.00002722
Iteration 79/1000 | Loss: 0.00002722
Iteration 80/1000 | Loss: 0.00002722
Iteration 81/1000 | Loss: 0.00002721
Iteration 82/1000 | Loss: 0.00002721
Iteration 83/1000 | Loss: 0.00002721
Iteration 84/1000 | Loss: 0.00002721
Iteration 85/1000 | Loss: 0.00002721
Iteration 86/1000 | Loss: 0.00002720
Iteration 87/1000 | Loss: 0.00002720
Iteration 88/1000 | Loss: 0.00002720
Iteration 89/1000 | Loss: 0.00002720
Iteration 90/1000 | Loss: 0.00002720
Iteration 91/1000 | Loss: 0.00002720
Iteration 92/1000 | Loss: 0.00002720
Iteration 93/1000 | Loss: 0.00002719
Iteration 94/1000 | Loss: 0.00002719
Iteration 95/1000 | Loss: 0.00002719
Iteration 96/1000 | Loss: 0.00002719
Iteration 97/1000 | Loss: 0.00002718
Iteration 98/1000 | Loss: 0.00002718
Iteration 99/1000 | Loss: 0.00002718
Iteration 100/1000 | Loss: 0.00002718
Iteration 101/1000 | Loss: 0.00002718
Iteration 102/1000 | Loss: 0.00002717
Iteration 103/1000 | Loss: 0.00002717
Iteration 104/1000 | Loss: 0.00002717
Iteration 105/1000 | Loss: 0.00002717
Iteration 106/1000 | Loss: 0.00002717
Iteration 107/1000 | Loss: 0.00002717
Iteration 108/1000 | Loss: 0.00002717
Iteration 109/1000 | Loss: 0.00002717
Iteration 110/1000 | Loss: 0.00002716
Iteration 111/1000 | Loss: 0.00002716
Iteration 112/1000 | Loss: 0.00002716
Iteration 113/1000 | Loss: 0.00002716
Iteration 114/1000 | Loss: 0.00002716
Iteration 115/1000 | Loss: 0.00002716
Iteration 116/1000 | Loss: 0.00002715
Iteration 117/1000 | Loss: 0.00002715
Iteration 118/1000 | Loss: 0.00002715
Iteration 119/1000 | Loss: 0.00002715
Iteration 120/1000 | Loss: 0.00002715
Iteration 121/1000 | Loss: 0.00002715
Iteration 122/1000 | Loss: 0.00002714
Iteration 123/1000 | Loss: 0.00002714
Iteration 124/1000 | Loss: 0.00002714
Iteration 125/1000 | Loss: 0.00002714
Iteration 126/1000 | Loss: 0.00002714
Iteration 127/1000 | Loss: 0.00002713
Iteration 128/1000 | Loss: 0.00002713
Iteration 129/1000 | Loss: 0.00002713
Iteration 130/1000 | Loss: 0.00002712
Iteration 131/1000 | Loss: 0.00002712
Iteration 132/1000 | Loss: 0.00002712
Iteration 133/1000 | Loss: 0.00002712
Iteration 134/1000 | Loss: 0.00002712
Iteration 135/1000 | Loss: 0.00002712
Iteration 136/1000 | Loss: 0.00002712
Iteration 137/1000 | Loss: 0.00002712
Iteration 138/1000 | Loss: 0.00002712
Iteration 139/1000 | Loss: 0.00002712
Iteration 140/1000 | Loss: 0.00002711
Iteration 141/1000 | Loss: 0.00002711
Iteration 142/1000 | Loss: 0.00002711
Iteration 143/1000 | Loss: 0.00002711
Iteration 144/1000 | Loss: 0.00002711
Iteration 145/1000 | Loss: 0.00002711
Iteration 146/1000 | Loss: 0.00002711
Iteration 147/1000 | Loss: 0.00002711
Iteration 148/1000 | Loss: 0.00002710
Iteration 149/1000 | Loss: 0.00002710
Iteration 150/1000 | Loss: 0.00002710
Iteration 151/1000 | Loss: 0.00002710
Iteration 152/1000 | Loss: 0.00002710
Iteration 153/1000 | Loss: 0.00002710
Iteration 154/1000 | Loss: 0.00002710
Iteration 155/1000 | Loss: 0.00002710
Iteration 156/1000 | Loss: 0.00002710
Iteration 157/1000 | Loss: 0.00002710
Iteration 158/1000 | Loss: 0.00002710
Iteration 159/1000 | Loss: 0.00002710
Iteration 160/1000 | Loss: 0.00002710
Iteration 161/1000 | Loss: 0.00002710
Iteration 162/1000 | Loss: 0.00002709
Iteration 163/1000 | Loss: 0.00002709
Iteration 164/1000 | Loss: 0.00002709
Iteration 165/1000 | Loss: 0.00002709
Iteration 166/1000 | Loss: 0.00002709
Iteration 167/1000 | Loss: 0.00002709
Iteration 168/1000 | Loss: 0.00002709
Iteration 169/1000 | Loss: 0.00002708
Iteration 170/1000 | Loss: 0.00002708
Iteration 171/1000 | Loss: 0.00002708
Iteration 172/1000 | Loss: 0.00002708
Iteration 173/1000 | Loss: 0.00002708
Iteration 174/1000 | Loss: 0.00002708
Iteration 175/1000 | Loss: 0.00002708
Iteration 176/1000 | Loss: 0.00002708
Iteration 177/1000 | Loss: 0.00002708
Iteration 178/1000 | Loss: 0.00002708
Iteration 179/1000 | Loss: 0.00002708
Iteration 180/1000 | Loss: 0.00002708
Iteration 181/1000 | Loss: 0.00002708
Iteration 182/1000 | Loss: 0.00002707
Iteration 183/1000 | Loss: 0.00002707
Iteration 184/1000 | Loss: 0.00002707
Iteration 185/1000 | Loss: 0.00002707
Iteration 186/1000 | Loss: 0.00002707
Iteration 187/1000 | Loss: 0.00002707
Iteration 188/1000 | Loss: 0.00002707
Iteration 189/1000 | Loss: 0.00002707
Iteration 190/1000 | Loss: 0.00002707
Iteration 191/1000 | Loss: 0.00002707
Iteration 192/1000 | Loss: 0.00002707
Iteration 193/1000 | Loss: 0.00002707
Iteration 194/1000 | Loss: 0.00002707
Iteration 195/1000 | Loss: 0.00002707
Iteration 196/1000 | Loss: 0.00002707
Iteration 197/1000 | Loss: 0.00002707
Iteration 198/1000 | Loss: 0.00002707
Iteration 199/1000 | Loss: 0.00002707
Iteration 200/1000 | Loss: 0.00002707
Iteration 201/1000 | Loss: 0.00002707
Iteration 202/1000 | Loss: 0.00002707
Iteration 203/1000 | Loss: 0.00002707
Iteration 204/1000 | Loss: 0.00002707
Iteration 205/1000 | Loss: 0.00002707
Iteration 206/1000 | Loss: 0.00002707
Iteration 207/1000 | Loss: 0.00002707
Iteration 208/1000 | Loss: 0.00002707
Iteration 209/1000 | Loss: 0.00002707
Iteration 210/1000 | Loss: 0.00002707
Iteration 211/1000 | Loss: 0.00002707
Iteration 212/1000 | Loss: 0.00002707
Iteration 213/1000 | Loss: 0.00002707
Iteration 214/1000 | Loss: 0.00002707
Iteration 215/1000 | Loss: 0.00002707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.7065327230957337e-05, 2.7065327230957337e-05, 2.7065327230957337e-05, 2.7065327230957337e-05, 2.7065327230957337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7065327230957337e-05

Optimization complete. Final v2v error: 4.170517444610596 mm

Highest mean error: 5.6237077713012695 mm for frame 82

Lowest mean error: 3.401963472366333 mm for frame 191

Saving results

Total time: 53.08388113975525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784152
Iteration 2/25 | Loss: 0.00097474
Iteration 3/25 | Loss: 0.00071610
Iteration 4/25 | Loss: 0.00067545
Iteration 5/25 | Loss: 0.00066171
Iteration 6/25 | Loss: 0.00065873
Iteration 7/25 | Loss: 0.00065781
Iteration 8/25 | Loss: 0.00065769
Iteration 9/25 | Loss: 0.00065769
Iteration 10/25 | Loss: 0.00065769
Iteration 11/25 | Loss: 0.00065769
Iteration 12/25 | Loss: 0.00065769
Iteration 13/25 | Loss: 0.00065769
Iteration 14/25 | Loss: 0.00065769
Iteration 15/25 | Loss: 0.00065769
Iteration 16/25 | Loss: 0.00065769
Iteration 17/25 | Loss: 0.00065769
Iteration 18/25 | Loss: 0.00065769
Iteration 19/25 | Loss: 0.00065769
Iteration 20/25 | Loss: 0.00065769
Iteration 21/25 | Loss: 0.00065769
Iteration 22/25 | Loss: 0.00065769
Iteration 23/25 | Loss: 0.00065769
Iteration 24/25 | Loss: 0.00065769
Iteration 25/25 | Loss: 0.00065769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.70442533
Iteration 2/25 | Loss: 0.00027803
Iteration 3/25 | Loss: 0.00027800
Iteration 4/25 | Loss: 0.00027800
Iteration 5/25 | Loss: 0.00027800
Iteration 6/25 | Loss: 0.00027800
Iteration 7/25 | Loss: 0.00027800
Iteration 8/25 | Loss: 0.00027800
Iteration 9/25 | Loss: 0.00027800
Iteration 10/25 | Loss: 0.00027800
Iteration 11/25 | Loss: 0.00027800
Iteration 12/25 | Loss: 0.00027800
Iteration 13/25 | Loss: 0.00027800
Iteration 14/25 | Loss: 0.00027800
Iteration 15/25 | Loss: 0.00027800
Iteration 16/25 | Loss: 0.00027800
Iteration 17/25 | Loss: 0.00027800
Iteration 18/25 | Loss: 0.00027800
Iteration 19/25 | Loss: 0.00027800
Iteration 20/25 | Loss: 0.00027800
Iteration 21/25 | Loss: 0.00027800
Iteration 22/25 | Loss: 0.00027800
Iteration 23/25 | Loss: 0.00027800
Iteration 24/25 | Loss: 0.00027800
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002779987989924848, 0.0002779987989924848, 0.0002779987989924848, 0.0002779987989924848, 0.0002779987989924848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002779987989924848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027800
Iteration 2/1000 | Loss: 0.00004068
Iteration 3/1000 | Loss: 0.00002156
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001712
Iteration 6/1000 | Loss: 0.00001659
Iteration 7/1000 | Loss: 0.00001630
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001603
Iteration 10/1000 | Loss: 0.00001590
Iteration 11/1000 | Loss: 0.00001587
Iteration 12/1000 | Loss: 0.00001586
Iteration 13/1000 | Loss: 0.00001583
Iteration 14/1000 | Loss: 0.00001583
Iteration 15/1000 | Loss: 0.00001583
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001577
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001571
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001569
Iteration 40/1000 | Loss: 0.00001568
Iteration 41/1000 | Loss: 0.00001568
Iteration 42/1000 | Loss: 0.00001568
Iteration 43/1000 | Loss: 0.00001567
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001566
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001565
Iteration 49/1000 | Loss: 0.00001565
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001565
Iteration 53/1000 | Loss: 0.00001564
Iteration 54/1000 | Loss: 0.00001564
Iteration 55/1000 | Loss: 0.00001564
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001564
Iteration 58/1000 | Loss: 0.00001564
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001563
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001562
Iteration 67/1000 | Loss: 0.00001562
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001562
Iteration 73/1000 | Loss: 0.00001562
Iteration 74/1000 | Loss: 0.00001562
Iteration 75/1000 | Loss: 0.00001562
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001560
Iteration 82/1000 | Loss: 0.00001560
Iteration 83/1000 | Loss: 0.00001560
Iteration 84/1000 | Loss: 0.00001560
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001558
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.558393705636263e-05, 1.558393705636263e-05, 1.558393705636263e-05, 1.558393705636263e-05, 1.558393705636263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.558393705636263e-05

Optimization complete. Final v2v error: 3.3497633934020996 mm

Highest mean error: 3.8435325622558594 mm for frame 48

Lowest mean error: 2.8340365886688232 mm for frame 5

Saving results

Total time: 32.4233033657074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980025
Iteration 2/25 | Loss: 0.00158051
Iteration 3/25 | Loss: 0.00116115
Iteration 4/25 | Loss: 0.00106754
Iteration 5/25 | Loss: 0.00103674
Iteration 6/25 | Loss: 0.00098047
Iteration 7/25 | Loss: 0.00097918
Iteration 8/25 | Loss: 0.00095725
Iteration 9/25 | Loss: 0.00095157
Iteration 10/25 | Loss: 0.00095009
Iteration 11/25 | Loss: 0.00094235
Iteration 12/25 | Loss: 0.00093887
Iteration 13/25 | Loss: 0.00093913
Iteration 14/25 | Loss: 0.00094325
Iteration 15/25 | Loss: 0.00094079
Iteration 16/25 | Loss: 0.00093773
Iteration 17/25 | Loss: 0.00093618
Iteration 18/25 | Loss: 0.00093879
Iteration 19/25 | Loss: 0.00094030
Iteration 20/25 | Loss: 0.00093729
Iteration 21/25 | Loss: 0.00093390
Iteration 22/25 | Loss: 0.00093151
Iteration 23/25 | Loss: 0.00093048
Iteration 24/25 | Loss: 0.00092987
Iteration 25/25 | Loss: 0.00092972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95511520
Iteration 2/25 | Loss: 0.00177521
Iteration 3/25 | Loss: 0.00153341
Iteration 4/25 | Loss: 0.00153341
Iteration 5/25 | Loss: 0.00153341
Iteration 6/25 | Loss: 0.00153341
Iteration 7/25 | Loss: 0.00153341
Iteration 8/25 | Loss: 0.00153341
Iteration 9/25 | Loss: 0.00153341
Iteration 10/25 | Loss: 0.00153341
Iteration 11/25 | Loss: 0.00153341
Iteration 12/25 | Loss: 0.00153341
Iteration 13/25 | Loss: 0.00153341
Iteration 14/25 | Loss: 0.00153341
Iteration 15/25 | Loss: 0.00153341
Iteration 16/25 | Loss: 0.00153341
Iteration 17/25 | Loss: 0.00153341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015334074851125479, 0.0015334074851125479, 0.0015334074851125479, 0.0015334074851125479, 0.0015334074851125479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015334074851125479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153341
Iteration 2/1000 | Loss: 0.00137642
Iteration 3/1000 | Loss: 0.00103895
Iteration 4/1000 | Loss: 0.00064102
Iteration 5/1000 | Loss: 0.00134810
Iteration 6/1000 | Loss: 0.00052126
Iteration 7/1000 | Loss: 0.00024014
Iteration 8/1000 | Loss: 0.00011466
Iteration 9/1000 | Loss: 0.00039534
Iteration 10/1000 | Loss: 0.00008696
Iteration 11/1000 | Loss: 0.00083630
Iteration 12/1000 | Loss: 0.00007843
Iteration 13/1000 | Loss: 0.00046386
Iteration 14/1000 | Loss: 0.00016200
Iteration 15/1000 | Loss: 0.00012004
Iteration 16/1000 | Loss: 0.00112418
Iteration 17/1000 | Loss: 0.00009645
Iteration 18/1000 | Loss: 0.00035797
Iteration 19/1000 | Loss: 0.00045178
Iteration 20/1000 | Loss: 0.00013851
Iteration 21/1000 | Loss: 0.00007257
Iteration 22/1000 | Loss: 0.00004742
Iteration 23/1000 | Loss: 0.00065167
Iteration 24/1000 | Loss: 0.00005522
Iteration 25/1000 | Loss: 0.00004230
Iteration 26/1000 | Loss: 0.00057334
Iteration 27/1000 | Loss: 0.00050347
Iteration 28/1000 | Loss: 0.00004523
Iteration 29/1000 | Loss: 0.00003654
Iteration 30/1000 | Loss: 0.00003347
Iteration 31/1000 | Loss: 0.00003239
Iteration 32/1000 | Loss: 0.00003200
Iteration 33/1000 | Loss: 0.00003150
Iteration 34/1000 | Loss: 0.00003118
Iteration 35/1000 | Loss: 0.00003095
Iteration 36/1000 | Loss: 0.00003086
Iteration 37/1000 | Loss: 0.00003067
Iteration 38/1000 | Loss: 0.00003066
Iteration 39/1000 | Loss: 0.00003050
Iteration 40/1000 | Loss: 0.00003050
Iteration 41/1000 | Loss: 0.00003045
Iteration 42/1000 | Loss: 0.00003045
Iteration 43/1000 | Loss: 0.00003045
Iteration 44/1000 | Loss: 0.00003044
Iteration 45/1000 | Loss: 0.00003044
Iteration 46/1000 | Loss: 0.00003043
Iteration 47/1000 | Loss: 0.00003043
Iteration 48/1000 | Loss: 0.00003042
Iteration 49/1000 | Loss: 0.00003029
Iteration 50/1000 | Loss: 0.00003027
Iteration 51/1000 | Loss: 0.00003023
Iteration 52/1000 | Loss: 0.00003020
Iteration 53/1000 | Loss: 0.00003020
Iteration 54/1000 | Loss: 0.00003020
Iteration 55/1000 | Loss: 0.00003019
Iteration 56/1000 | Loss: 0.00003019
Iteration 57/1000 | Loss: 0.00003018
Iteration 58/1000 | Loss: 0.00003016
Iteration 59/1000 | Loss: 0.00003016
Iteration 60/1000 | Loss: 0.00003015
Iteration 61/1000 | Loss: 0.00003013
Iteration 62/1000 | Loss: 0.00003013
Iteration 63/1000 | Loss: 0.00003013
Iteration 64/1000 | Loss: 0.00003013
Iteration 65/1000 | Loss: 0.00003012
Iteration 66/1000 | Loss: 0.00003012
Iteration 67/1000 | Loss: 0.00003012
Iteration 68/1000 | Loss: 0.00003011
Iteration 69/1000 | Loss: 0.00003011
Iteration 70/1000 | Loss: 0.00003010
Iteration 71/1000 | Loss: 0.00003009
Iteration 72/1000 | Loss: 0.00003009
Iteration 73/1000 | Loss: 0.00003009
Iteration 74/1000 | Loss: 0.00003008
Iteration 75/1000 | Loss: 0.00003008
Iteration 76/1000 | Loss: 0.00003006
Iteration 77/1000 | Loss: 0.00003005
Iteration 78/1000 | Loss: 0.00003005
Iteration 79/1000 | Loss: 0.00003004
Iteration 80/1000 | Loss: 0.00003004
Iteration 81/1000 | Loss: 0.00003004
Iteration 82/1000 | Loss: 0.00003003
Iteration 83/1000 | Loss: 0.00003003
Iteration 84/1000 | Loss: 0.00003003
Iteration 85/1000 | Loss: 0.00003003
Iteration 86/1000 | Loss: 0.00003003
Iteration 87/1000 | Loss: 0.00003002
Iteration 88/1000 | Loss: 0.00003002
Iteration 89/1000 | Loss: 0.00003002
Iteration 90/1000 | Loss: 0.00003002
Iteration 91/1000 | Loss: 0.00003001
Iteration 92/1000 | Loss: 0.00003001
Iteration 93/1000 | Loss: 0.00003000
Iteration 94/1000 | Loss: 0.00003000
Iteration 95/1000 | Loss: 0.00002999
Iteration 96/1000 | Loss: 0.00002999
Iteration 97/1000 | Loss: 0.00002999
Iteration 98/1000 | Loss: 0.00002999
Iteration 99/1000 | Loss: 0.00002999
Iteration 100/1000 | Loss: 0.00002999
Iteration 101/1000 | Loss: 0.00002998
Iteration 102/1000 | Loss: 0.00002998
Iteration 103/1000 | Loss: 0.00002998
Iteration 104/1000 | Loss: 0.00002998
Iteration 105/1000 | Loss: 0.00002998
Iteration 106/1000 | Loss: 0.00002998
Iteration 107/1000 | Loss: 0.00002998
Iteration 108/1000 | Loss: 0.00002998
Iteration 109/1000 | Loss: 0.00002998
Iteration 110/1000 | Loss: 0.00002998
Iteration 111/1000 | Loss: 0.00002997
Iteration 112/1000 | Loss: 0.00002997
Iteration 113/1000 | Loss: 0.00002997
Iteration 114/1000 | Loss: 0.00002997
Iteration 115/1000 | Loss: 0.00002997
Iteration 116/1000 | Loss: 0.00002996
Iteration 117/1000 | Loss: 0.00002996
Iteration 118/1000 | Loss: 0.00002996
Iteration 119/1000 | Loss: 0.00002996
Iteration 120/1000 | Loss: 0.00002995
Iteration 121/1000 | Loss: 0.00002995
Iteration 122/1000 | Loss: 0.00002995
Iteration 123/1000 | Loss: 0.00002995
Iteration 124/1000 | Loss: 0.00002994
Iteration 125/1000 | Loss: 0.00002994
Iteration 126/1000 | Loss: 0.00002994
Iteration 127/1000 | Loss: 0.00002994
Iteration 128/1000 | Loss: 0.00002994
Iteration 129/1000 | Loss: 0.00002994
Iteration 130/1000 | Loss: 0.00002993
Iteration 131/1000 | Loss: 0.00002993
Iteration 132/1000 | Loss: 0.00002993
Iteration 133/1000 | Loss: 0.00002993
Iteration 134/1000 | Loss: 0.00002993
Iteration 135/1000 | Loss: 0.00002993
Iteration 136/1000 | Loss: 0.00002992
Iteration 137/1000 | Loss: 0.00002992
Iteration 138/1000 | Loss: 0.00002992
Iteration 139/1000 | Loss: 0.00002992
Iteration 140/1000 | Loss: 0.00002992
Iteration 141/1000 | Loss: 0.00002992
Iteration 142/1000 | Loss: 0.00002992
Iteration 143/1000 | Loss: 0.00002992
Iteration 144/1000 | Loss: 0.00002992
Iteration 145/1000 | Loss: 0.00002992
Iteration 146/1000 | Loss: 0.00002992
Iteration 147/1000 | Loss: 0.00002992
Iteration 148/1000 | Loss: 0.00002991
Iteration 149/1000 | Loss: 0.00002991
Iteration 150/1000 | Loss: 0.00002991
Iteration 151/1000 | Loss: 0.00002991
Iteration 152/1000 | Loss: 0.00002991
Iteration 153/1000 | Loss: 0.00002991
Iteration 154/1000 | Loss: 0.00002991
Iteration 155/1000 | Loss: 0.00002991
Iteration 156/1000 | Loss: 0.00002991
Iteration 157/1000 | Loss: 0.00002991
Iteration 158/1000 | Loss: 0.00002991
Iteration 159/1000 | Loss: 0.00002991
Iteration 160/1000 | Loss: 0.00002991
Iteration 161/1000 | Loss: 0.00002991
Iteration 162/1000 | Loss: 0.00002990
Iteration 163/1000 | Loss: 0.00002990
Iteration 164/1000 | Loss: 0.00002990
Iteration 165/1000 | Loss: 0.00002990
Iteration 166/1000 | Loss: 0.00002990
Iteration 167/1000 | Loss: 0.00002990
Iteration 168/1000 | Loss: 0.00002990
Iteration 169/1000 | Loss: 0.00002990
Iteration 170/1000 | Loss: 0.00002990
Iteration 171/1000 | Loss: 0.00002990
Iteration 172/1000 | Loss: 0.00002990
Iteration 173/1000 | Loss: 0.00002990
Iteration 174/1000 | Loss: 0.00002990
Iteration 175/1000 | Loss: 0.00002990
Iteration 176/1000 | Loss: 0.00002990
Iteration 177/1000 | Loss: 0.00002990
Iteration 178/1000 | Loss: 0.00002990
Iteration 179/1000 | Loss: 0.00002990
Iteration 180/1000 | Loss: 0.00002990
Iteration 181/1000 | Loss: 0.00002990
Iteration 182/1000 | Loss: 0.00002990
Iteration 183/1000 | Loss: 0.00002990
Iteration 184/1000 | Loss: 0.00002990
Iteration 185/1000 | Loss: 0.00002990
Iteration 186/1000 | Loss: 0.00002990
Iteration 187/1000 | Loss: 0.00002990
Iteration 188/1000 | Loss: 0.00002990
Iteration 189/1000 | Loss: 0.00002990
Iteration 190/1000 | Loss: 0.00002990
Iteration 191/1000 | Loss: 0.00002990
Iteration 192/1000 | Loss: 0.00002990
Iteration 193/1000 | Loss: 0.00002990
Iteration 194/1000 | Loss: 0.00002990
Iteration 195/1000 | Loss: 0.00002990
Iteration 196/1000 | Loss: 0.00002990
Iteration 197/1000 | Loss: 0.00002990
Iteration 198/1000 | Loss: 0.00002990
Iteration 199/1000 | Loss: 0.00002990
Iteration 200/1000 | Loss: 0.00002990
Iteration 201/1000 | Loss: 0.00002990
Iteration 202/1000 | Loss: 0.00002990
Iteration 203/1000 | Loss: 0.00002990
Iteration 204/1000 | Loss: 0.00002990
Iteration 205/1000 | Loss: 0.00002990
Iteration 206/1000 | Loss: 0.00002990
Iteration 207/1000 | Loss: 0.00002990
Iteration 208/1000 | Loss: 0.00002990
Iteration 209/1000 | Loss: 0.00002990
Iteration 210/1000 | Loss: 0.00002990
Iteration 211/1000 | Loss: 0.00002990
Iteration 212/1000 | Loss: 0.00002990
Iteration 213/1000 | Loss: 0.00002990
Iteration 214/1000 | Loss: 0.00002990
Iteration 215/1000 | Loss: 0.00002990
Iteration 216/1000 | Loss: 0.00002990
Iteration 217/1000 | Loss: 0.00002990
Iteration 218/1000 | Loss: 0.00002990
Iteration 219/1000 | Loss: 0.00002990
Iteration 220/1000 | Loss: 0.00002990
Iteration 221/1000 | Loss: 0.00002990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [2.9898823413532227e-05, 2.9898823413532227e-05, 2.9898823413532227e-05, 2.9898823413532227e-05, 2.9898823413532227e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9898823413532227e-05

Optimization complete. Final v2v error: 4.427785396575928 mm

Highest mean error: 5.7751946449279785 mm for frame 219

Lowest mean error: 3.098590612411499 mm for frame 206

Saving results

Total time: 127.79703974723816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864223
Iteration 2/25 | Loss: 0.00107177
Iteration 3/25 | Loss: 0.00085555
Iteration 4/25 | Loss: 0.00076997
Iteration 5/25 | Loss: 0.00075120
Iteration 6/25 | Loss: 0.00074922
Iteration 7/25 | Loss: 0.00074905
Iteration 8/25 | Loss: 0.00074905
Iteration 9/25 | Loss: 0.00074905
Iteration 10/25 | Loss: 0.00074905
Iteration 11/25 | Loss: 0.00074905
Iteration 12/25 | Loss: 0.00074905
Iteration 13/25 | Loss: 0.00074905
Iteration 14/25 | Loss: 0.00074905
Iteration 15/25 | Loss: 0.00074905
Iteration 16/25 | Loss: 0.00074905
Iteration 17/25 | Loss: 0.00074905
Iteration 18/25 | Loss: 0.00074905
Iteration 19/25 | Loss: 0.00074905
Iteration 20/25 | Loss: 0.00074905
Iteration 21/25 | Loss: 0.00074905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007490523857995868, 0.0007490523857995868, 0.0007490523857995868, 0.0007490523857995868, 0.0007490523857995868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007490523857995868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41712499
Iteration 2/25 | Loss: 0.00020951
Iteration 3/25 | Loss: 0.00020951
Iteration 4/25 | Loss: 0.00020951
Iteration 5/25 | Loss: 0.00020951
Iteration 6/25 | Loss: 0.00020951
Iteration 7/25 | Loss: 0.00020951
Iteration 8/25 | Loss: 0.00020951
Iteration 9/25 | Loss: 0.00020951
Iteration 10/25 | Loss: 0.00020951
Iteration 11/25 | Loss: 0.00020951
Iteration 12/25 | Loss: 0.00020951
Iteration 13/25 | Loss: 0.00020951
Iteration 14/25 | Loss: 0.00020951
Iteration 15/25 | Loss: 0.00020951
Iteration 16/25 | Loss: 0.00020951
Iteration 17/25 | Loss: 0.00020951
Iteration 18/25 | Loss: 0.00020951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00020950807083863765, 0.00020950807083863765, 0.00020950807083863765, 0.00020950807083863765, 0.00020950807083863765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020950807083863765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020951
Iteration 2/1000 | Loss: 0.00003895
Iteration 3/1000 | Loss: 0.00002883
Iteration 4/1000 | Loss: 0.00002649
Iteration 5/1000 | Loss: 0.00002542
Iteration 6/1000 | Loss: 0.00002440
Iteration 7/1000 | Loss: 0.00002365
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002268
Iteration 10/1000 | Loss: 0.00002246
Iteration 11/1000 | Loss: 0.00002246
Iteration 12/1000 | Loss: 0.00002246
Iteration 13/1000 | Loss: 0.00002246
Iteration 14/1000 | Loss: 0.00002243
Iteration 15/1000 | Loss: 0.00002238
Iteration 16/1000 | Loss: 0.00002227
Iteration 17/1000 | Loss: 0.00002226
Iteration 18/1000 | Loss: 0.00002223
Iteration 19/1000 | Loss: 0.00002223
Iteration 20/1000 | Loss: 0.00002222
Iteration 21/1000 | Loss: 0.00002222
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002220
Iteration 24/1000 | Loss: 0.00002220
Iteration 25/1000 | Loss: 0.00002220
Iteration 26/1000 | Loss: 0.00002219
Iteration 27/1000 | Loss: 0.00002219
Iteration 28/1000 | Loss: 0.00002219
Iteration 29/1000 | Loss: 0.00002218
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00002218
Iteration 32/1000 | Loss: 0.00002218
Iteration 33/1000 | Loss: 0.00002218
Iteration 34/1000 | Loss: 0.00002217
Iteration 35/1000 | Loss: 0.00002217
Iteration 36/1000 | Loss: 0.00002216
Iteration 37/1000 | Loss: 0.00002216
Iteration 38/1000 | Loss: 0.00002216
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002215
Iteration 41/1000 | Loss: 0.00002214
Iteration 42/1000 | Loss: 0.00002214
Iteration 43/1000 | Loss: 0.00002214
Iteration 44/1000 | Loss: 0.00002213
Iteration 45/1000 | Loss: 0.00002213
Iteration 46/1000 | Loss: 0.00002213
Iteration 47/1000 | Loss: 0.00002213
Iteration 48/1000 | Loss: 0.00002212
Iteration 49/1000 | Loss: 0.00002212
Iteration 50/1000 | Loss: 0.00002212
Iteration 51/1000 | Loss: 0.00002211
Iteration 52/1000 | Loss: 0.00002211
Iteration 53/1000 | Loss: 0.00002211
Iteration 54/1000 | Loss: 0.00002211
Iteration 55/1000 | Loss: 0.00002210
Iteration 56/1000 | Loss: 0.00002210
Iteration 57/1000 | Loss: 0.00002210
Iteration 58/1000 | Loss: 0.00002210
Iteration 59/1000 | Loss: 0.00002209
Iteration 60/1000 | Loss: 0.00002209
Iteration 61/1000 | Loss: 0.00002209
Iteration 62/1000 | Loss: 0.00002209
Iteration 63/1000 | Loss: 0.00002209
Iteration 64/1000 | Loss: 0.00002209
Iteration 65/1000 | Loss: 0.00002209
Iteration 66/1000 | Loss: 0.00002208
Iteration 67/1000 | Loss: 0.00002207
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002207
Iteration 70/1000 | Loss: 0.00002207
Iteration 71/1000 | Loss: 0.00002207
Iteration 72/1000 | Loss: 0.00002207
Iteration 73/1000 | Loss: 0.00002206
Iteration 74/1000 | Loss: 0.00002206
Iteration 75/1000 | Loss: 0.00002206
Iteration 76/1000 | Loss: 0.00002206
Iteration 77/1000 | Loss: 0.00002206
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002206
Iteration 80/1000 | Loss: 0.00002206
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00002206
Iteration 83/1000 | Loss: 0.00002206
Iteration 84/1000 | Loss: 0.00002205
Iteration 85/1000 | Loss: 0.00002205
Iteration 86/1000 | Loss: 0.00002205
Iteration 87/1000 | Loss: 0.00002205
Iteration 88/1000 | Loss: 0.00002205
Iteration 89/1000 | Loss: 0.00002205
Iteration 90/1000 | Loss: 0.00002205
Iteration 91/1000 | Loss: 0.00002205
Iteration 92/1000 | Loss: 0.00002205
Iteration 93/1000 | Loss: 0.00002205
Iteration 94/1000 | Loss: 0.00002205
Iteration 95/1000 | Loss: 0.00002204
Iteration 96/1000 | Loss: 0.00002204
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002204
Iteration 99/1000 | Loss: 0.00002204
Iteration 100/1000 | Loss: 0.00002204
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002204
Iteration 103/1000 | Loss: 0.00002204
Iteration 104/1000 | Loss: 0.00002204
Iteration 105/1000 | Loss: 0.00002203
Iteration 106/1000 | Loss: 0.00002203
Iteration 107/1000 | Loss: 0.00002203
Iteration 108/1000 | Loss: 0.00002203
Iteration 109/1000 | Loss: 0.00002202
Iteration 110/1000 | Loss: 0.00002202
Iteration 111/1000 | Loss: 0.00002202
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00002202
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002202
Iteration 117/1000 | Loss: 0.00002202
Iteration 118/1000 | Loss: 0.00002202
Iteration 119/1000 | Loss: 0.00002202
Iteration 120/1000 | Loss: 0.00002202
Iteration 121/1000 | Loss: 0.00002202
Iteration 122/1000 | Loss: 0.00002202
Iteration 123/1000 | Loss: 0.00002201
Iteration 124/1000 | Loss: 0.00002201
Iteration 125/1000 | Loss: 0.00002201
Iteration 126/1000 | Loss: 0.00002201
Iteration 127/1000 | Loss: 0.00002201
Iteration 128/1000 | Loss: 0.00002201
Iteration 129/1000 | Loss: 0.00002201
Iteration 130/1000 | Loss: 0.00002201
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002201
Iteration 138/1000 | Loss: 0.00002201
Iteration 139/1000 | Loss: 0.00002201
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002201
Iteration 143/1000 | Loss: 0.00002201
Iteration 144/1000 | Loss: 0.00002201
Iteration 145/1000 | Loss: 0.00002201
Iteration 146/1000 | Loss: 0.00002201
Iteration 147/1000 | Loss: 0.00002201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.2007550796843134e-05, 2.2007550796843134e-05, 2.2007550796843134e-05, 2.2007550796843134e-05, 2.2007550796843134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2007550796843134e-05

Optimization complete. Final v2v error: 3.913240432739258 mm

Highest mean error: 4.187020778656006 mm for frame 144

Lowest mean error: 3.8254590034484863 mm for frame 34

Saving results

Total time: 35.193718671798706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021934
Iteration 2/25 | Loss: 0.00321738
Iteration 3/25 | Loss: 0.00219518
Iteration 4/25 | Loss: 0.00187183
Iteration 5/25 | Loss: 0.00183801
Iteration 6/25 | Loss: 0.00190082
Iteration 7/25 | Loss: 0.00168322
Iteration 8/25 | Loss: 0.00143179
Iteration 9/25 | Loss: 0.00121977
Iteration 10/25 | Loss: 0.00112949
Iteration 11/25 | Loss: 0.00109682
Iteration 12/25 | Loss: 0.00103754
Iteration 13/25 | Loss: 0.00102583
Iteration 14/25 | Loss: 0.00098943
Iteration 15/25 | Loss: 0.00095192
Iteration 16/25 | Loss: 0.00092957
Iteration 17/25 | Loss: 0.00091955
Iteration 18/25 | Loss: 0.00092864
Iteration 19/25 | Loss: 0.00092137
Iteration 20/25 | Loss: 0.00092299
Iteration 21/25 | Loss: 0.00091222
Iteration 22/25 | Loss: 0.00090477
Iteration 23/25 | Loss: 0.00090014
Iteration 24/25 | Loss: 0.00089159
Iteration 25/25 | Loss: 0.00089985

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42666829
Iteration 2/25 | Loss: 0.00268411
Iteration 3/25 | Loss: 0.00194033
Iteration 4/25 | Loss: 0.00194033
Iteration 5/25 | Loss: 0.00194033
Iteration 6/25 | Loss: 0.00194033
Iteration 7/25 | Loss: 0.00194033
Iteration 8/25 | Loss: 0.00194033
Iteration 9/25 | Loss: 0.00194033
Iteration 10/25 | Loss: 0.00194033
Iteration 11/25 | Loss: 0.00194033
Iteration 12/25 | Loss: 0.00194033
Iteration 13/25 | Loss: 0.00194032
Iteration 14/25 | Loss: 0.00194032
Iteration 15/25 | Loss: 0.00194032
Iteration 16/25 | Loss: 0.00194032
Iteration 17/25 | Loss: 0.00194032
Iteration 18/25 | Loss: 0.00194032
Iteration 19/25 | Loss: 0.00194032
Iteration 20/25 | Loss: 0.00194032
Iteration 21/25 | Loss: 0.00194032
Iteration 22/25 | Loss: 0.00194032
Iteration 23/25 | Loss: 0.00194032
Iteration 24/25 | Loss: 0.00194032
Iteration 25/25 | Loss: 0.00194032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194032
Iteration 2/1000 | Loss: 0.00113798
Iteration 3/1000 | Loss: 0.00052136
Iteration 4/1000 | Loss: 0.00033649
Iteration 5/1000 | Loss: 0.00058432
Iteration 6/1000 | Loss: 0.00052088
Iteration 7/1000 | Loss: 0.00043662
Iteration 8/1000 | Loss: 0.00045117
Iteration 9/1000 | Loss: 0.00049246
Iteration 10/1000 | Loss: 0.00036709
Iteration 11/1000 | Loss: 0.00076679
Iteration 12/1000 | Loss: 0.00068799
Iteration 13/1000 | Loss: 0.00060354
Iteration 14/1000 | Loss: 0.00062613
Iteration 15/1000 | Loss: 0.00050580
Iteration 16/1000 | Loss: 0.00057655
Iteration 17/1000 | Loss: 0.00079256
Iteration 18/1000 | Loss: 0.00043159
Iteration 19/1000 | Loss: 0.00037562
Iteration 20/1000 | Loss: 0.00031012
Iteration 21/1000 | Loss: 0.00032416
Iteration 22/1000 | Loss: 0.00034470
Iteration 23/1000 | Loss: 0.00037918
Iteration 24/1000 | Loss: 0.00035155
Iteration 25/1000 | Loss: 0.00080247
Iteration 26/1000 | Loss: 0.00053602
Iteration 27/1000 | Loss: 0.00050606
Iteration 28/1000 | Loss: 0.00053702
Iteration 29/1000 | Loss: 0.00097397
Iteration 30/1000 | Loss: 0.00056990
Iteration 31/1000 | Loss: 0.00167311
Iteration 32/1000 | Loss: 0.00075478
Iteration 33/1000 | Loss: 0.00109300
Iteration 34/1000 | Loss: 0.00066399
Iteration 35/1000 | Loss: 0.00079136
Iteration 36/1000 | Loss: 0.00068263
Iteration 37/1000 | Loss: 0.00078245
Iteration 38/1000 | Loss: 0.00084648
Iteration 39/1000 | Loss: 0.00115416
Iteration 40/1000 | Loss: 0.00081068
Iteration 41/1000 | Loss: 0.00090896
Iteration 42/1000 | Loss: 0.00043626
Iteration 43/1000 | Loss: 0.00052075
Iteration 44/1000 | Loss: 0.00062945
Iteration 45/1000 | Loss: 0.00062345
Iteration 46/1000 | Loss: 0.00028286
Iteration 47/1000 | Loss: 0.00061024
Iteration 48/1000 | Loss: 0.00080028
Iteration 49/1000 | Loss: 0.00048539
Iteration 50/1000 | Loss: 0.00027105
Iteration 51/1000 | Loss: 0.00019512
Iteration 52/1000 | Loss: 0.00016529
Iteration 53/1000 | Loss: 0.00016360
Iteration 54/1000 | Loss: 0.00021008
Iteration 55/1000 | Loss: 0.00016138
Iteration 56/1000 | Loss: 0.00025446
Iteration 57/1000 | Loss: 0.00019841
Iteration 58/1000 | Loss: 0.00017213
Iteration 59/1000 | Loss: 0.00010762
Iteration 60/1000 | Loss: 0.00012107
Iteration 61/1000 | Loss: 0.00032173
Iteration 62/1000 | Loss: 0.00036000
Iteration 63/1000 | Loss: 0.00045322
Iteration 64/1000 | Loss: 0.00031007
Iteration 65/1000 | Loss: 0.00029118
Iteration 66/1000 | Loss: 0.00027109
Iteration 67/1000 | Loss: 0.00033652
Iteration 68/1000 | Loss: 0.00061300
Iteration 69/1000 | Loss: 0.00047686
Iteration 70/1000 | Loss: 0.00022721
Iteration 71/1000 | Loss: 0.00016751
Iteration 72/1000 | Loss: 0.00016101
Iteration 73/1000 | Loss: 0.00011886
Iteration 74/1000 | Loss: 0.00025517
Iteration 75/1000 | Loss: 0.00025624
Iteration 76/1000 | Loss: 0.00036832
Iteration 77/1000 | Loss: 0.00050393
Iteration 78/1000 | Loss: 0.00042226
Iteration 79/1000 | Loss: 0.00039489
Iteration 80/1000 | Loss: 0.00021500
Iteration 81/1000 | Loss: 0.00016067
Iteration 82/1000 | Loss: 0.00064298
Iteration 83/1000 | Loss: 0.00016863
Iteration 84/1000 | Loss: 0.00022912
Iteration 85/1000 | Loss: 0.00029687
Iteration 86/1000 | Loss: 0.00030056
Iteration 87/1000 | Loss: 0.00048685
Iteration 88/1000 | Loss: 0.00021844
Iteration 89/1000 | Loss: 0.00018621
Iteration 90/1000 | Loss: 0.00020643
Iteration 91/1000 | Loss: 0.00019297
Iteration 92/1000 | Loss: 0.00009760
Iteration 93/1000 | Loss: 0.00022922
Iteration 94/1000 | Loss: 0.00008468
Iteration 95/1000 | Loss: 0.00006242
Iteration 96/1000 | Loss: 0.00007179
Iteration 97/1000 | Loss: 0.00005751
Iteration 98/1000 | Loss: 0.00007978
Iteration 99/1000 | Loss: 0.00005717
Iteration 100/1000 | Loss: 0.00012279
Iteration 101/1000 | Loss: 0.00008053
Iteration 102/1000 | Loss: 0.00015705
Iteration 103/1000 | Loss: 0.00008122
Iteration 104/1000 | Loss: 0.00006507
Iteration 105/1000 | Loss: 0.00005809
Iteration 106/1000 | Loss: 0.00015808
Iteration 107/1000 | Loss: 0.00016223
Iteration 108/1000 | Loss: 0.00024515
Iteration 109/1000 | Loss: 0.00030046
Iteration 110/1000 | Loss: 0.00019490
Iteration 111/1000 | Loss: 0.00026093
Iteration 112/1000 | Loss: 0.00026324
Iteration 113/1000 | Loss: 0.00011875
Iteration 114/1000 | Loss: 0.00013328
Iteration 115/1000 | Loss: 0.00013847
Iteration 116/1000 | Loss: 0.00014000
Iteration 117/1000 | Loss: 0.00018837
Iteration 118/1000 | Loss: 0.00027067
Iteration 119/1000 | Loss: 0.00006273
Iteration 120/1000 | Loss: 0.00015379
Iteration 121/1000 | Loss: 0.00035622
Iteration 122/1000 | Loss: 0.00005600
Iteration 123/1000 | Loss: 0.00009946
Iteration 124/1000 | Loss: 0.00005062
Iteration 125/1000 | Loss: 0.00017281
Iteration 126/1000 | Loss: 0.00020823
Iteration 127/1000 | Loss: 0.00023432
Iteration 128/1000 | Loss: 0.00005203
Iteration 129/1000 | Loss: 0.00023415
Iteration 130/1000 | Loss: 0.00004951
Iteration 131/1000 | Loss: 0.00004826
Iteration 132/1000 | Loss: 0.00012990
Iteration 133/1000 | Loss: 0.00004709
Iteration 134/1000 | Loss: 0.00016607
Iteration 135/1000 | Loss: 0.00018972
Iteration 136/1000 | Loss: 0.00012195
Iteration 137/1000 | Loss: 0.00008875
Iteration 138/1000 | Loss: 0.00005501
Iteration 139/1000 | Loss: 0.00011483
Iteration 140/1000 | Loss: 0.00006104
Iteration 141/1000 | Loss: 0.00005508
Iteration 142/1000 | Loss: 0.00004682
Iteration 143/1000 | Loss: 0.00004602
Iteration 144/1000 | Loss: 0.00008121
Iteration 145/1000 | Loss: 0.00011478
Iteration 146/1000 | Loss: 0.00004478
Iteration 147/1000 | Loss: 0.00004442
Iteration 148/1000 | Loss: 0.00022413
Iteration 149/1000 | Loss: 0.00010110
Iteration 150/1000 | Loss: 0.00004441
Iteration 151/1000 | Loss: 0.00010296
Iteration 152/1000 | Loss: 0.00004653
Iteration 153/1000 | Loss: 0.00004403
Iteration 154/1000 | Loss: 0.00007006
Iteration 155/1000 | Loss: 0.00005361
Iteration 156/1000 | Loss: 0.00004397
Iteration 157/1000 | Loss: 0.00007493
Iteration 158/1000 | Loss: 0.00004990
Iteration 159/1000 | Loss: 0.00004384
Iteration 160/1000 | Loss: 0.00007283
Iteration 161/1000 | Loss: 0.00005068
Iteration 162/1000 | Loss: 0.00007297
Iteration 163/1000 | Loss: 0.00005352
Iteration 164/1000 | Loss: 0.00006980
Iteration 165/1000 | Loss: 0.00005766
Iteration 166/1000 | Loss: 0.00005869
Iteration 167/1000 | Loss: 0.00004400
Iteration 168/1000 | Loss: 0.00004361
Iteration 169/1000 | Loss: 0.00004353
Iteration 170/1000 | Loss: 0.00004353
Iteration 171/1000 | Loss: 0.00004353
Iteration 172/1000 | Loss: 0.00004353
Iteration 173/1000 | Loss: 0.00004353
Iteration 174/1000 | Loss: 0.00004353
Iteration 175/1000 | Loss: 0.00004353
Iteration 176/1000 | Loss: 0.00004353
Iteration 177/1000 | Loss: 0.00004353
Iteration 178/1000 | Loss: 0.00004353
Iteration 179/1000 | Loss: 0.00004353
Iteration 180/1000 | Loss: 0.00004353
Iteration 181/1000 | Loss: 0.00004353
Iteration 182/1000 | Loss: 0.00004353
Iteration 183/1000 | Loss: 0.00004352
Iteration 184/1000 | Loss: 0.00004352
Iteration 185/1000 | Loss: 0.00004352
Iteration 186/1000 | Loss: 0.00004352
Iteration 187/1000 | Loss: 0.00004351
Iteration 188/1000 | Loss: 0.00004351
Iteration 189/1000 | Loss: 0.00004351
Iteration 190/1000 | Loss: 0.00004350
Iteration 191/1000 | Loss: 0.00004350
Iteration 192/1000 | Loss: 0.00004350
Iteration 193/1000 | Loss: 0.00004350
Iteration 194/1000 | Loss: 0.00004349
Iteration 195/1000 | Loss: 0.00004349
Iteration 196/1000 | Loss: 0.00004348
Iteration 197/1000 | Loss: 0.00004348
Iteration 198/1000 | Loss: 0.00004348
Iteration 199/1000 | Loss: 0.00004347
Iteration 200/1000 | Loss: 0.00004347
Iteration 201/1000 | Loss: 0.00004347
Iteration 202/1000 | Loss: 0.00004346
Iteration 203/1000 | Loss: 0.00004346
Iteration 204/1000 | Loss: 0.00004345
Iteration 205/1000 | Loss: 0.00004345
Iteration 206/1000 | Loss: 0.00004345
Iteration 207/1000 | Loss: 0.00004345
Iteration 208/1000 | Loss: 0.00004344
Iteration 209/1000 | Loss: 0.00004344
Iteration 210/1000 | Loss: 0.00004344
Iteration 211/1000 | Loss: 0.00004344
Iteration 212/1000 | Loss: 0.00004344
Iteration 213/1000 | Loss: 0.00004344
Iteration 214/1000 | Loss: 0.00004344
Iteration 215/1000 | Loss: 0.00004343
Iteration 216/1000 | Loss: 0.00004343
Iteration 217/1000 | Loss: 0.00004343
Iteration 218/1000 | Loss: 0.00004343
Iteration 219/1000 | Loss: 0.00004342
Iteration 220/1000 | Loss: 0.00004342
Iteration 221/1000 | Loss: 0.00004342
Iteration 222/1000 | Loss: 0.00004341
Iteration 223/1000 | Loss: 0.00004341
Iteration 224/1000 | Loss: 0.00004341
Iteration 225/1000 | Loss: 0.00004341
Iteration 226/1000 | Loss: 0.00004341
Iteration 227/1000 | Loss: 0.00004341
Iteration 228/1000 | Loss: 0.00004340
Iteration 229/1000 | Loss: 0.00004340
Iteration 230/1000 | Loss: 0.00004340
Iteration 231/1000 | Loss: 0.00004340
Iteration 232/1000 | Loss: 0.00004340
Iteration 233/1000 | Loss: 0.00004340
Iteration 234/1000 | Loss: 0.00004340
Iteration 235/1000 | Loss: 0.00004340
Iteration 236/1000 | Loss: 0.00004339
Iteration 237/1000 | Loss: 0.00004339
Iteration 238/1000 | Loss: 0.00004339
Iteration 239/1000 | Loss: 0.00004338
Iteration 240/1000 | Loss: 0.00004338
Iteration 241/1000 | Loss: 0.00004338
Iteration 242/1000 | Loss: 0.00004338
Iteration 243/1000 | Loss: 0.00004337
Iteration 244/1000 | Loss: 0.00004337
Iteration 245/1000 | Loss: 0.00004337
Iteration 246/1000 | Loss: 0.00017420
Iteration 247/1000 | Loss: 0.00006054
Iteration 248/1000 | Loss: 0.00004768
Iteration 249/1000 | Loss: 0.00016046
Iteration 250/1000 | Loss: 0.00004502
Iteration 251/1000 | Loss: 0.00021674
Iteration 252/1000 | Loss: 0.00004681
Iteration 253/1000 | Loss: 0.00004428
Iteration 254/1000 | Loss: 0.00004351
Iteration 255/1000 | Loss: 0.00004313
Iteration 256/1000 | Loss: 0.00016192
Iteration 257/1000 | Loss: 0.00045762
Iteration 258/1000 | Loss: 0.00052357
Iteration 259/1000 | Loss: 0.00010179
Iteration 260/1000 | Loss: 0.00015517
Iteration 261/1000 | Loss: 0.00005633
Iteration 262/1000 | Loss: 0.00004484
Iteration 263/1000 | Loss: 0.00011469
Iteration 264/1000 | Loss: 0.00004378
Iteration 265/1000 | Loss: 0.00010621
Iteration 266/1000 | Loss: 0.00004306
Iteration 267/1000 | Loss: 0.00004268
Iteration 268/1000 | Loss: 0.00009994
Iteration 269/1000 | Loss: 0.00004266
Iteration 270/1000 | Loss: 0.00004240
Iteration 271/1000 | Loss: 0.00004233
Iteration 272/1000 | Loss: 0.00004232
Iteration 273/1000 | Loss: 0.00004232
Iteration 274/1000 | Loss: 0.00004222
Iteration 275/1000 | Loss: 0.00004222
Iteration 276/1000 | Loss: 0.00004221
Iteration 277/1000 | Loss: 0.00004221
Iteration 278/1000 | Loss: 0.00004221
Iteration 279/1000 | Loss: 0.00004221
Iteration 280/1000 | Loss: 0.00004220
Iteration 281/1000 | Loss: 0.00004220
Iteration 282/1000 | Loss: 0.00004219
Iteration 283/1000 | Loss: 0.00004219
Iteration 284/1000 | Loss: 0.00004218
Iteration 285/1000 | Loss: 0.00004218
Iteration 286/1000 | Loss: 0.00004218
Iteration 287/1000 | Loss: 0.00004218
Iteration 288/1000 | Loss: 0.00004218
Iteration 289/1000 | Loss: 0.00004217
Iteration 290/1000 | Loss: 0.00004217
Iteration 291/1000 | Loss: 0.00004217
Iteration 292/1000 | Loss: 0.00004217
Iteration 293/1000 | Loss: 0.00004217
Iteration 294/1000 | Loss: 0.00004217
Iteration 295/1000 | Loss: 0.00004216
Iteration 296/1000 | Loss: 0.00004216
Iteration 297/1000 | Loss: 0.00004216
Iteration 298/1000 | Loss: 0.00004215
Iteration 299/1000 | Loss: 0.00004215
Iteration 300/1000 | Loss: 0.00004215
Iteration 301/1000 | Loss: 0.00004215
Iteration 302/1000 | Loss: 0.00004214
Iteration 303/1000 | Loss: 0.00004214
Iteration 304/1000 | Loss: 0.00004214
Iteration 305/1000 | Loss: 0.00004214
Iteration 306/1000 | Loss: 0.00004214
Iteration 307/1000 | Loss: 0.00004214
Iteration 308/1000 | Loss: 0.00004214
Iteration 309/1000 | Loss: 0.00004214
Iteration 310/1000 | Loss: 0.00004213
Iteration 311/1000 | Loss: 0.00004213
Iteration 312/1000 | Loss: 0.00004213
Iteration 313/1000 | Loss: 0.00004213
Iteration 314/1000 | Loss: 0.00004213
Iteration 315/1000 | Loss: 0.00004213
Iteration 316/1000 | Loss: 0.00004213
Iteration 317/1000 | Loss: 0.00004213
Iteration 318/1000 | Loss: 0.00004213
Iteration 319/1000 | Loss: 0.00004213
Iteration 320/1000 | Loss: 0.00004212
Iteration 321/1000 | Loss: 0.00004212
Iteration 322/1000 | Loss: 0.00004212
Iteration 323/1000 | Loss: 0.00004212
Iteration 324/1000 | Loss: 0.00004212
Iteration 325/1000 | Loss: 0.00004212
Iteration 326/1000 | Loss: 0.00004212
Iteration 327/1000 | Loss: 0.00004212
Iteration 328/1000 | Loss: 0.00004212
Iteration 329/1000 | Loss: 0.00004212
Iteration 330/1000 | Loss: 0.00004211
Iteration 331/1000 | Loss: 0.00004211
Iteration 332/1000 | Loss: 0.00004211
Iteration 333/1000 | Loss: 0.00004211
Iteration 334/1000 | Loss: 0.00004211
Iteration 335/1000 | Loss: 0.00004211
Iteration 336/1000 | Loss: 0.00004211
Iteration 337/1000 | Loss: 0.00004211
Iteration 338/1000 | Loss: 0.00004211
Iteration 339/1000 | Loss: 0.00004211
Iteration 340/1000 | Loss: 0.00004211
Iteration 341/1000 | Loss: 0.00004211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [4.2111332732019946e-05, 4.2111332732019946e-05, 4.2111332732019946e-05, 4.2111332732019946e-05, 4.2111332732019946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2111332732019946e-05

Optimization complete. Final v2v error: 4.223790168762207 mm

Highest mean error: 12.184843063354492 mm for frame 200

Lowest mean error: 3.548161029815674 mm for frame 10

Saving results

Total time: 377.84339690208435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411960
Iteration 2/25 | Loss: 0.00071556
Iteration 3/25 | Loss: 0.00060967
Iteration 4/25 | Loss: 0.00058167
Iteration 5/25 | Loss: 0.00057236
Iteration 6/25 | Loss: 0.00057117
Iteration 7/25 | Loss: 0.00057095
Iteration 8/25 | Loss: 0.00057095
Iteration 9/25 | Loss: 0.00057095
Iteration 10/25 | Loss: 0.00057095
Iteration 11/25 | Loss: 0.00057095
Iteration 12/25 | Loss: 0.00057095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005709489341825247, 0.0005709489341825247, 0.0005709489341825247, 0.0005709489341825247, 0.0005709489341825247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005709489341825247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.59109688
Iteration 2/25 | Loss: 0.00016276
Iteration 3/25 | Loss: 0.00016275
Iteration 4/25 | Loss: 0.00016275
Iteration 5/25 | Loss: 0.00016275
Iteration 6/25 | Loss: 0.00016274
Iteration 7/25 | Loss: 0.00016274
Iteration 8/25 | Loss: 0.00016274
Iteration 9/25 | Loss: 0.00016274
Iteration 10/25 | Loss: 0.00016274
Iteration 11/25 | Loss: 0.00016274
Iteration 12/25 | Loss: 0.00016274
Iteration 13/25 | Loss: 0.00016274
Iteration 14/25 | Loss: 0.00016274
Iteration 15/25 | Loss: 0.00016274
Iteration 16/25 | Loss: 0.00016274
Iteration 17/25 | Loss: 0.00016274
Iteration 18/25 | Loss: 0.00016274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00016274338122457266, 0.00016274338122457266, 0.00016274338122457266, 0.00016274338122457266, 0.00016274338122457266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00016274338122457266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016274
Iteration 2/1000 | Loss: 0.00002486
Iteration 3/1000 | Loss: 0.00001837
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001585
Iteration 6/1000 | Loss: 0.00001537
Iteration 7/1000 | Loss: 0.00001488
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001447
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001446
Iteration 12/1000 | Loss: 0.00001446
Iteration 13/1000 | Loss: 0.00001445
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001441
Iteration 18/1000 | Loss: 0.00001441
Iteration 19/1000 | Loss: 0.00001441
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001439
Iteration 23/1000 | Loss: 0.00001438
Iteration 24/1000 | Loss: 0.00001438
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001436
Iteration 27/1000 | Loss: 0.00001434
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001434
Iteration 30/1000 | Loss: 0.00001434
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001433
Iteration 34/1000 | Loss: 0.00001433
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001430
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001430
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001429
Iteration 54/1000 | Loss: 0.00001429
Iteration 55/1000 | Loss: 0.00001428
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00001424
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001424
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001423
Iteration 69/1000 | Loss: 0.00001423
Iteration 70/1000 | Loss: 0.00001423
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001421
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001420
Iteration 86/1000 | Loss: 0.00001420
Iteration 87/1000 | Loss: 0.00001420
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001419
Iteration 90/1000 | Loss: 0.00001419
Iteration 91/1000 | Loss: 0.00001419
Iteration 92/1000 | Loss: 0.00001419
Iteration 93/1000 | Loss: 0.00001419
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001419
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001419
Iteration 118/1000 | Loss: 0.00001419
Iteration 119/1000 | Loss: 0.00001419
Iteration 120/1000 | Loss: 0.00001419
Iteration 121/1000 | Loss: 0.00001419
Iteration 122/1000 | Loss: 0.00001419
Iteration 123/1000 | Loss: 0.00001419
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.4193282368069049e-05, 1.4193282368069049e-05, 1.4193282368069049e-05, 1.4193282368069049e-05, 1.4193282368069049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4193282368069049e-05

Optimization complete. Final v2v error: 3.183251142501831 mm

Highest mean error: 3.583127975463867 mm for frame 123

Lowest mean error: 2.903130054473877 mm for frame 191

Saving results

Total time: 32.6365430355072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499949
Iteration 2/25 | Loss: 0.00112821
Iteration 3/25 | Loss: 0.00071853
Iteration 4/25 | Loss: 0.00062184
Iteration 5/25 | Loss: 0.00060372
Iteration 6/25 | Loss: 0.00059928
Iteration 7/25 | Loss: 0.00059813
Iteration 8/25 | Loss: 0.00059792
Iteration 9/25 | Loss: 0.00059792
Iteration 10/25 | Loss: 0.00059792
Iteration 11/25 | Loss: 0.00059792
Iteration 12/25 | Loss: 0.00059792
Iteration 13/25 | Loss: 0.00059792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005979231209494174, 0.0005979231209494174, 0.0005979231209494174, 0.0005979231209494174, 0.0005979231209494174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005979231209494174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51090395
Iteration 2/25 | Loss: 0.00017783
Iteration 3/25 | Loss: 0.00017783
Iteration 4/25 | Loss: 0.00017783
Iteration 5/25 | Loss: 0.00017783
Iteration 6/25 | Loss: 0.00017782
Iteration 7/25 | Loss: 0.00017782
Iteration 8/25 | Loss: 0.00017782
Iteration 9/25 | Loss: 0.00017782
Iteration 10/25 | Loss: 0.00017782
Iteration 11/25 | Loss: 0.00017782
Iteration 12/25 | Loss: 0.00017782
Iteration 13/25 | Loss: 0.00017782
Iteration 14/25 | Loss: 0.00017782
Iteration 15/25 | Loss: 0.00017782
Iteration 16/25 | Loss: 0.00017782
Iteration 17/25 | Loss: 0.00017782
Iteration 18/25 | Loss: 0.00017782
Iteration 19/25 | Loss: 0.00017782
Iteration 20/25 | Loss: 0.00017782
Iteration 21/25 | Loss: 0.00017782
Iteration 22/25 | Loss: 0.00017782
Iteration 23/25 | Loss: 0.00017782
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0001778236764948815, 0.0001778236764948815, 0.0001778236764948815, 0.0001778236764948815, 0.0001778236764948815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001778236764948815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017782
Iteration 2/1000 | Loss: 0.00002277
Iteration 3/1000 | Loss: 0.00001753
Iteration 4/1000 | Loss: 0.00001542
Iteration 5/1000 | Loss: 0.00001464
Iteration 6/1000 | Loss: 0.00001396
Iteration 7/1000 | Loss: 0.00001362
Iteration 8/1000 | Loss: 0.00001339
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001307
Iteration 14/1000 | Loss: 0.00001306
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001298
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001291
Iteration 31/1000 | Loss: 0.00001291
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001289
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001287
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001286
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001285
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001284
Iteration 62/1000 | Loss: 0.00001284
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001283
Iteration 74/1000 | Loss: 0.00001283
Iteration 75/1000 | Loss: 0.00001283
Iteration 76/1000 | Loss: 0.00001283
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001281
Iteration 80/1000 | Loss: 0.00001281
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001280
Iteration 83/1000 | Loss: 0.00001279
Iteration 84/1000 | Loss: 0.00001279
Iteration 85/1000 | Loss: 0.00001278
Iteration 86/1000 | Loss: 0.00001277
Iteration 87/1000 | Loss: 0.00001277
Iteration 88/1000 | Loss: 0.00001277
Iteration 89/1000 | Loss: 0.00001277
Iteration 90/1000 | Loss: 0.00001277
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001273
Iteration 106/1000 | Loss: 0.00001273
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001272
Iteration 111/1000 | Loss: 0.00001272
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001271
Iteration 123/1000 | Loss: 0.00001271
Iteration 124/1000 | Loss: 0.00001271
Iteration 125/1000 | Loss: 0.00001271
Iteration 126/1000 | Loss: 0.00001271
Iteration 127/1000 | Loss: 0.00001271
Iteration 128/1000 | Loss: 0.00001271
Iteration 129/1000 | Loss: 0.00001271
Iteration 130/1000 | Loss: 0.00001270
Iteration 131/1000 | Loss: 0.00001270
Iteration 132/1000 | Loss: 0.00001270
Iteration 133/1000 | Loss: 0.00001270
Iteration 134/1000 | Loss: 0.00001270
Iteration 135/1000 | Loss: 0.00001270
Iteration 136/1000 | Loss: 0.00001270
Iteration 137/1000 | Loss: 0.00001270
Iteration 138/1000 | Loss: 0.00001270
Iteration 139/1000 | Loss: 0.00001270
Iteration 140/1000 | Loss: 0.00001270
Iteration 141/1000 | Loss: 0.00001269
Iteration 142/1000 | Loss: 0.00001269
Iteration 143/1000 | Loss: 0.00001269
Iteration 144/1000 | Loss: 0.00001269
Iteration 145/1000 | Loss: 0.00001269
Iteration 146/1000 | Loss: 0.00001269
Iteration 147/1000 | Loss: 0.00001268
Iteration 148/1000 | Loss: 0.00001268
Iteration 149/1000 | Loss: 0.00001268
Iteration 150/1000 | Loss: 0.00001268
Iteration 151/1000 | Loss: 0.00001268
Iteration 152/1000 | Loss: 0.00001268
Iteration 153/1000 | Loss: 0.00001268
Iteration 154/1000 | Loss: 0.00001268
Iteration 155/1000 | Loss: 0.00001268
Iteration 156/1000 | Loss: 0.00001268
Iteration 157/1000 | Loss: 0.00001267
Iteration 158/1000 | Loss: 0.00001267
Iteration 159/1000 | Loss: 0.00001267
Iteration 160/1000 | Loss: 0.00001267
Iteration 161/1000 | Loss: 0.00001267
Iteration 162/1000 | Loss: 0.00001267
Iteration 163/1000 | Loss: 0.00001267
Iteration 164/1000 | Loss: 0.00001267
Iteration 165/1000 | Loss: 0.00001266
Iteration 166/1000 | Loss: 0.00001266
Iteration 167/1000 | Loss: 0.00001266
Iteration 168/1000 | Loss: 0.00001266
Iteration 169/1000 | Loss: 0.00001266
Iteration 170/1000 | Loss: 0.00001266
Iteration 171/1000 | Loss: 0.00001266
Iteration 172/1000 | Loss: 0.00001266
Iteration 173/1000 | Loss: 0.00001266
Iteration 174/1000 | Loss: 0.00001266
Iteration 175/1000 | Loss: 0.00001266
Iteration 176/1000 | Loss: 0.00001266
Iteration 177/1000 | Loss: 0.00001266
Iteration 178/1000 | Loss: 0.00001265
Iteration 179/1000 | Loss: 0.00001265
Iteration 180/1000 | Loss: 0.00001265
Iteration 181/1000 | Loss: 0.00001265
Iteration 182/1000 | Loss: 0.00001265
Iteration 183/1000 | Loss: 0.00001265
Iteration 184/1000 | Loss: 0.00001265
Iteration 185/1000 | Loss: 0.00001264
Iteration 186/1000 | Loss: 0.00001264
Iteration 187/1000 | Loss: 0.00001264
Iteration 188/1000 | Loss: 0.00001264
Iteration 189/1000 | Loss: 0.00001264
Iteration 190/1000 | Loss: 0.00001264
Iteration 191/1000 | Loss: 0.00001264
Iteration 192/1000 | Loss: 0.00001264
Iteration 193/1000 | Loss: 0.00001263
Iteration 194/1000 | Loss: 0.00001263
Iteration 195/1000 | Loss: 0.00001263
Iteration 196/1000 | Loss: 0.00001263
Iteration 197/1000 | Loss: 0.00001263
Iteration 198/1000 | Loss: 0.00001263
Iteration 199/1000 | Loss: 0.00001263
Iteration 200/1000 | Loss: 0.00001263
Iteration 201/1000 | Loss: 0.00001263
Iteration 202/1000 | Loss: 0.00001262
Iteration 203/1000 | Loss: 0.00001262
Iteration 204/1000 | Loss: 0.00001262
Iteration 205/1000 | Loss: 0.00001262
Iteration 206/1000 | Loss: 0.00001262
Iteration 207/1000 | Loss: 0.00001262
Iteration 208/1000 | Loss: 0.00001262
Iteration 209/1000 | Loss: 0.00001262
Iteration 210/1000 | Loss: 0.00001262
Iteration 211/1000 | Loss: 0.00001262
Iteration 212/1000 | Loss: 0.00001262
Iteration 213/1000 | Loss: 0.00001262
Iteration 214/1000 | Loss: 0.00001262
Iteration 215/1000 | Loss: 0.00001262
Iteration 216/1000 | Loss: 0.00001262
Iteration 217/1000 | Loss: 0.00001262
Iteration 218/1000 | Loss: 0.00001262
Iteration 219/1000 | Loss: 0.00001262
Iteration 220/1000 | Loss: 0.00001261
Iteration 221/1000 | Loss: 0.00001261
Iteration 222/1000 | Loss: 0.00001261
Iteration 223/1000 | Loss: 0.00001261
Iteration 224/1000 | Loss: 0.00001261
Iteration 225/1000 | Loss: 0.00001261
Iteration 226/1000 | Loss: 0.00001261
Iteration 227/1000 | Loss: 0.00001261
Iteration 228/1000 | Loss: 0.00001261
Iteration 229/1000 | Loss: 0.00001261
Iteration 230/1000 | Loss: 0.00001261
Iteration 231/1000 | Loss: 0.00001261
Iteration 232/1000 | Loss: 0.00001261
Iteration 233/1000 | Loss: 0.00001261
Iteration 234/1000 | Loss: 0.00001261
Iteration 235/1000 | Loss: 0.00001261
Iteration 236/1000 | Loss: 0.00001261
Iteration 237/1000 | Loss: 0.00001261
Iteration 238/1000 | Loss: 0.00001261
Iteration 239/1000 | Loss: 0.00001261
Iteration 240/1000 | Loss: 0.00001261
Iteration 241/1000 | Loss: 0.00001261
Iteration 242/1000 | Loss: 0.00001261
Iteration 243/1000 | Loss: 0.00001261
Iteration 244/1000 | Loss: 0.00001261
Iteration 245/1000 | Loss: 0.00001261
Iteration 246/1000 | Loss: 0.00001261
Iteration 247/1000 | Loss: 0.00001261
Iteration 248/1000 | Loss: 0.00001261
Iteration 249/1000 | Loss: 0.00001261
Iteration 250/1000 | Loss: 0.00001261
Iteration 251/1000 | Loss: 0.00001261
Iteration 252/1000 | Loss: 0.00001261
Iteration 253/1000 | Loss: 0.00001261
Iteration 254/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.2613633771252353e-05, 1.2613633771252353e-05, 1.2613633771252353e-05, 1.2613633771252353e-05, 1.2613633771252353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2613633771252353e-05

Optimization complete. Final v2v error: 2.9652206897735596 mm

Highest mean error: 3.981187582015991 mm for frame 75

Lowest mean error: 2.589263677597046 mm for frame 120

Saving results

Total time: 40.58220338821411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002702
Iteration 2/25 | Loss: 0.00161714
Iteration 3/25 | Loss: 0.00100031
Iteration 4/25 | Loss: 0.00092933
Iteration 5/25 | Loss: 0.00091993
Iteration 6/25 | Loss: 0.00091795
Iteration 7/25 | Loss: 0.00091785
Iteration 8/25 | Loss: 0.00091785
Iteration 9/25 | Loss: 0.00091784
Iteration 10/25 | Loss: 0.00091783
Iteration 11/25 | Loss: 0.00091783
Iteration 12/25 | Loss: 0.00091783
Iteration 13/25 | Loss: 0.00091783
Iteration 14/25 | Loss: 0.00091783
Iteration 15/25 | Loss: 0.00091783
Iteration 16/25 | Loss: 0.00091783
Iteration 17/25 | Loss: 0.00091783
Iteration 18/25 | Loss: 0.00091783
Iteration 19/25 | Loss: 0.00091783
Iteration 20/25 | Loss: 0.00091783
Iteration 21/25 | Loss: 0.00091783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009178270120173693, 0.0009178270120173693, 0.0009178270120173693, 0.0009178270120173693, 0.0009178270120173693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009178270120173693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43958125
Iteration 2/25 | Loss: 0.00023965
Iteration 3/25 | Loss: 0.00023965
Iteration 4/25 | Loss: 0.00023965
Iteration 5/25 | Loss: 0.00023965
Iteration 6/25 | Loss: 0.00023965
Iteration 7/25 | Loss: 0.00023965
Iteration 8/25 | Loss: 0.00023965
Iteration 9/25 | Loss: 0.00023965
Iteration 10/25 | Loss: 0.00023965
Iteration 11/25 | Loss: 0.00023965
Iteration 12/25 | Loss: 0.00023965
Iteration 13/25 | Loss: 0.00023965
Iteration 14/25 | Loss: 0.00023965
Iteration 15/25 | Loss: 0.00023965
Iteration 16/25 | Loss: 0.00023965
Iteration 17/25 | Loss: 0.00023965
Iteration 18/25 | Loss: 0.00023965
Iteration 19/25 | Loss: 0.00023965
Iteration 20/25 | Loss: 0.00023965
Iteration 21/25 | Loss: 0.00023965
Iteration 22/25 | Loss: 0.00023965
Iteration 23/25 | Loss: 0.00023965
Iteration 24/25 | Loss: 0.00023965
Iteration 25/25 | Loss: 0.00023965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023965
Iteration 2/1000 | Loss: 0.00003927
Iteration 3/1000 | Loss: 0.00003027
Iteration 4/1000 | Loss: 0.00002801
Iteration 5/1000 | Loss: 0.00002748
Iteration 6/1000 | Loss: 0.00002693
Iteration 7/1000 | Loss: 0.00002663
Iteration 8/1000 | Loss: 0.00002645
Iteration 9/1000 | Loss: 0.00002644
Iteration 10/1000 | Loss: 0.00002640
Iteration 11/1000 | Loss: 0.00002630
Iteration 12/1000 | Loss: 0.00002620
Iteration 13/1000 | Loss: 0.00002613
Iteration 14/1000 | Loss: 0.00002613
Iteration 15/1000 | Loss: 0.00002612
Iteration 16/1000 | Loss: 0.00002612
Iteration 17/1000 | Loss: 0.00002612
Iteration 18/1000 | Loss: 0.00002612
Iteration 19/1000 | Loss: 0.00002612
Iteration 20/1000 | Loss: 0.00002612
Iteration 21/1000 | Loss: 0.00002611
Iteration 22/1000 | Loss: 0.00002611
Iteration 23/1000 | Loss: 0.00002611
Iteration 24/1000 | Loss: 0.00002611
Iteration 25/1000 | Loss: 0.00002611
Iteration 26/1000 | Loss: 0.00002611
Iteration 27/1000 | Loss: 0.00002611
Iteration 28/1000 | Loss: 0.00002611
Iteration 29/1000 | Loss: 0.00002611
Iteration 30/1000 | Loss: 0.00002610
Iteration 31/1000 | Loss: 0.00002610
Iteration 32/1000 | Loss: 0.00002609
Iteration 33/1000 | Loss: 0.00002609
Iteration 34/1000 | Loss: 0.00002609
Iteration 35/1000 | Loss: 0.00002608
Iteration 36/1000 | Loss: 0.00002607
Iteration 37/1000 | Loss: 0.00002607
Iteration 38/1000 | Loss: 0.00002607
Iteration 39/1000 | Loss: 0.00002607
Iteration 40/1000 | Loss: 0.00002607
Iteration 41/1000 | Loss: 0.00002607
Iteration 42/1000 | Loss: 0.00002607
Iteration 43/1000 | Loss: 0.00002606
Iteration 44/1000 | Loss: 0.00002606
Iteration 45/1000 | Loss: 0.00002606
Iteration 46/1000 | Loss: 0.00002606
Iteration 47/1000 | Loss: 0.00002606
Iteration 48/1000 | Loss: 0.00002606
Iteration 49/1000 | Loss: 0.00002606
Iteration 50/1000 | Loss: 0.00002606
Iteration 51/1000 | Loss: 0.00002606
Iteration 52/1000 | Loss: 0.00002606
Iteration 53/1000 | Loss: 0.00002606
Iteration 54/1000 | Loss: 0.00002606
Iteration 55/1000 | Loss: 0.00002606
Iteration 56/1000 | Loss: 0.00002606
Iteration 57/1000 | Loss: 0.00002606
Iteration 58/1000 | Loss: 0.00002606
Iteration 59/1000 | Loss: 0.00002606
Iteration 60/1000 | Loss: 0.00002605
Iteration 61/1000 | Loss: 0.00002605
Iteration 62/1000 | Loss: 0.00002605
Iteration 63/1000 | Loss: 0.00002605
Iteration 64/1000 | Loss: 0.00002605
Iteration 65/1000 | Loss: 0.00002604
Iteration 66/1000 | Loss: 0.00002604
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002604
Iteration 69/1000 | Loss: 0.00002604
Iteration 70/1000 | Loss: 0.00002604
Iteration 71/1000 | Loss: 0.00002604
Iteration 72/1000 | Loss: 0.00002603
Iteration 73/1000 | Loss: 0.00002603
Iteration 74/1000 | Loss: 0.00002603
Iteration 75/1000 | Loss: 0.00002603
Iteration 76/1000 | Loss: 0.00002603
Iteration 77/1000 | Loss: 0.00002603
Iteration 78/1000 | Loss: 0.00002603
Iteration 79/1000 | Loss: 0.00002603
Iteration 80/1000 | Loss: 0.00002603
Iteration 81/1000 | Loss: 0.00002603
Iteration 82/1000 | Loss: 0.00002603
Iteration 83/1000 | Loss: 0.00002603
Iteration 84/1000 | Loss: 0.00002603
Iteration 85/1000 | Loss: 0.00002603
Iteration 86/1000 | Loss: 0.00002603
Iteration 87/1000 | Loss: 0.00002603
Iteration 88/1000 | Loss: 0.00002603
Iteration 89/1000 | Loss: 0.00002603
Iteration 90/1000 | Loss: 0.00002603
Iteration 91/1000 | Loss: 0.00002603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.6028095817309804e-05, 2.6028095817309804e-05, 2.6028095817309804e-05, 2.6028095817309804e-05, 2.6028095817309804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6028095817309804e-05

Optimization complete. Final v2v error: 4.120884418487549 mm

Highest mean error: 4.737370491027832 mm for frame 1

Lowest mean error: 3.760787010192871 mm for frame 40

Saving results

Total time: 30.018935918807983
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528367
Iteration 2/25 | Loss: 0.00097111
Iteration 3/25 | Loss: 0.00073359
Iteration 4/25 | Loss: 0.00066893
Iteration 5/25 | Loss: 0.00064539
Iteration 6/25 | Loss: 0.00062404
Iteration 7/25 | Loss: 0.00061204
Iteration 8/25 | Loss: 0.00061945
Iteration 9/25 | Loss: 0.00063087
Iteration 10/25 | Loss: 0.00060805
Iteration 11/25 | Loss: 0.00059728
Iteration 12/25 | Loss: 0.00059772
Iteration 13/25 | Loss: 0.00059477
Iteration 14/25 | Loss: 0.00059491
Iteration 15/25 | Loss: 0.00059772
Iteration 16/25 | Loss: 0.00059572
Iteration 17/25 | Loss: 0.00059477
Iteration 18/25 | Loss: 0.00059238
Iteration 19/25 | Loss: 0.00059117
Iteration 20/25 | Loss: 0.00058950
Iteration 21/25 | Loss: 0.00058882
Iteration 22/25 | Loss: 0.00058799
Iteration 23/25 | Loss: 0.00058676
Iteration 24/25 | Loss: 0.00058605
Iteration 25/25 | Loss: 0.00058571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49660850
Iteration 2/25 | Loss: 0.00014530
Iteration 3/25 | Loss: 0.00014529
Iteration 4/25 | Loss: 0.00014529
Iteration 5/25 | Loss: 0.00014529
Iteration 6/25 | Loss: 0.00014529
Iteration 7/25 | Loss: 0.00014529
Iteration 8/25 | Loss: 0.00014529
Iteration 9/25 | Loss: 0.00014529
Iteration 10/25 | Loss: 0.00014529
Iteration 11/25 | Loss: 0.00014529
Iteration 12/25 | Loss: 0.00014529
Iteration 13/25 | Loss: 0.00014529
Iteration 14/25 | Loss: 0.00014529
Iteration 15/25 | Loss: 0.00014529
Iteration 16/25 | Loss: 0.00014529
Iteration 17/25 | Loss: 0.00014529
Iteration 18/25 | Loss: 0.00014529
Iteration 19/25 | Loss: 0.00014529
Iteration 20/25 | Loss: 0.00014529
Iteration 21/25 | Loss: 0.00014529
Iteration 22/25 | Loss: 0.00014529
Iteration 23/25 | Loss: 0.00014529
Iteration 24/25 | Loss: 0.00014529
Iteration 25/25 | Loss: 0.00014529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014529
Iteration 2/1000 | Loss: 0.00002348
Iteration 3/1000 | Loss: 0.00001530
Iteration 4/1000 | Loss: 0.00001391
Iteration 5/1000 | Loss: 0.00001327
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001263
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001221
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001210
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001208
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001205
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001204
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001203
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001197
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001192
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001190
Iteration 50/1000 | Loss: 0.00001190
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001189
Iteration 55/1000 | Loss: 0.00001188
Iteration 56/1000 | Loss: 0.00001188
Iteration 57/1000 | Loss: 0.00001188
Iteration 58/1000 | Loss: 0.00001188
Iteration 59/1000 | Loss: 0.00001188
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001188
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001187
Iteration 72/1000 | Loss: 0.00001187
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001186
Iteration 76/1000 | Loss: 0.00001186
Iteration 77/1000 | Loss: 0.00001185
Iteration 78/1000 | Loss: 0.00001185
Iteration 79/1000 | Loss: 0.00001185
Iteration 80/1000 | Loss: 0.00001185
Iteration 81/1000 | Loss: 0.00001185
Iteration 82/1000 | Loss: 0.00001185
Iteration 83/1000 | Loss: 0.00001184
Iteration 84/1000 | Loss: 0.00001184
Iteration 85/1000 | Loss: 0.00001184
Iteration 86/1000 | Loss: 0.00001184
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001183
Iteration 95/1000 | Loss: 0.00001183
Iteration 96/1000 | Loss: 0.00001183
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001182
Iteration 110/1000 | Loss: 0.00001182
Iteration 111/1000 | Loss: 0.00001182
Iteration 112/1000 | Loss: 0.00001182
Iteration 113/1000 | Loss: 0.00001182
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001182
Iteration 119/1000 | Loss: 0.00001182
Iteration 120/1000 | Loss: 0.00001182
Iteration 121/1000 | Loss: 0.00001182
Iteration 122/1000 | Loss: 0.00001182
Iteration 123/1000 | Loss: 0.00001182
Iteration 124/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.1824989996966906e-05, 1.1824989996966906e-05, 1.1824989996966906e-05, 1.1824989996966906e-05, 1.1824989996966906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1824989996966906e-05

Optimization complete. Final v2v error: 2.929081916809082 mm

Highest mean error: 3.812812328338623 mm for frame 38

Lowest mean error: 2.620339870452881 mm for frame 119

Saving results

Total time: 70.639315366745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104113
Iteration 2/25 | Loss: 0.00257944
Iteration 3/25 | Loss: 0.00236770
Iteration 4/25 | Loss: 0.00184754
Iteration 5/25 | Loss: 0.00152651
Iteration 6/25 | Loss: 0.00121835
Iteration 7/25 | Loss: 0.00106957
Iteration 8/25 | Loss: 0.00098634
Iteration 9/25 | Loss: 0.00095347
Iteration 10/25 | Loss: 0.00086496
Iteration 11/25 | Loss: 0.00084805
Iteration 12/25 | Loss: 0.00079707
Iteration 13/25 | Loss: 0.00079475
Iteration 14/25 | Loss: 0.00079295
Iteration 15/25 | Loss: 0.00078191
Iteration 16/25 | Loss: 0.00077863
Iteration 17/25 | Loss: 0.00079897
Iteration 18/25 | Loss: 0.00075688
Iteration 19/25 | Loss: 0.00074637
Iteration 20/25 | Loss: 0.00074765
Iteration 21/25 | Loss: 0.00074245
Iteration 22/25 | Loss: 0.00074990
Iteration 23/25 | Loss: 0.00074427
Iteration 24/25 | Loss: 0.00074554
Iteration 25/25 | Loss: 0.00074356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46068728
Iteration 2/25 | Loss: 0.00130328
Iteration 3/25 | Loss: 0.00098840
Iteration 4/25 | Loss: 0.00098840
Iteration 5/25 | Loss: 0.00098840
Iteration 6/25 | Loss: 0.00098840
Iteration 7/25 | Loss: 0.00098840
Iteration 8/25 | Loss: 0.00098840
Iteration 9/25 | Loss: 0.00098839
Iteration 10/25 | Loss: 0.00098839
Iteration 11/25 | Loss: 0.00098839
Iteration 12/25 | Loss: 0.00098839
Iteration 13/25 | Loss: 0.00098839
Iteration 14/25 | Loss: 0.00098839
Iteration 15/25 | Loss: 0.00098839
Iteration 16/25 | Loss: 0.00098839
Iteration 17/25 | Loss: 0.00098839
Iteration 18/25 | Loss: 0.00098839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009883941384032369, 0.0009883941384032369, 0.0009883941384032369, 0.0009883941384032369, 0.0009883941384032369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009883941384032369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098839
Iteration 2/1000 | Loss: 0.00151745
Iteration 3/1000 | Loss: 0.00163894
Iteration 4/1000 | Loss: 0.00091435
Iteration 5/1000 | Loss: 0.00098098
Iteration 6/1000 | Loss: 0.00081483
Iteration 7/1000 | Loss: 0.00036951
Iteration 8/1000 | Loss: 0.00010711
Iteration 9/1000 | Loss: 0.00074740
Iteration 10/1000 | Loss: 0.00009599
Iteration 11/1000 | Loss: 0.00006408
Iteration 12/1000 | Loss: 0.00030110
Iteration 13/1000 | Loss: 0.00033612
Iteration 14/1000 | Loss: 0.00024749
Iteration 15/1000 | Loss: 0.00030898
Iteration 16/1000 | Loss: 0.00019825
Iteration 17/1000 | Loss: 0.00010060
Iteration 18/1000 | Loss: 0.00029079
Iteration 19/1000 | Loss: 0.00027313
Iteration 20/1000 | Loss: 0.00018744
Iteration 21/1000 | Loss: 0.00015363
Iteration 22/1000 | Loss: 0.00018417
Iteration 23/1000 | Loss: 0.00012918
Iteration 24/1000 | Loss: 0.00084943
Iteration 25/1000 | Loss: 0.00069823
Iteration 26/1000 | Loss: 0.00288085
Iteration 27/1000 | Loss: 0.00060244
Iteration 28/1000 | Loss: 0.00040900
Iteration 29/1000 | Loss: 0.00034962
Iteration 30/1000 | Loss: 0.00076020
Iteration 31/1000 | Loss: 0.00263201
Iteration 32/1000 | Loss: 0.00051482
Iteration 33/1000 | Loss: 0.00013654
Iteration 34/1000 | Loss: 0.00036790
Iteration 35/1000 | Loss: 0.00074904
Iteration 36/1000 | Loss: 0.00006270
Iteration 37/1000 | Loss: 0.00006577
Iteration 38/1000 | Loss: 0.00058255
Iteration 39/1000 | Loss: 0.00013583
Iteration 40/1000 | Loss: 0.00011109
Iteration 41/1000 | Loss: 0.00007150
Iteration 42/1000 | Loss: 0.00004753
Iteration 43/1000 | Loss: 0.00006357
Iteration 44/1000 | Loss: 0.00119397
Iteration 45/1000 | Loss: 0.00011947
Iteration 46/1000 | Loss: 0.00007323
Iteration 47/1000 | Loss: 0.00019625
Iteration 48/1000 | Loss: 0.00010340
Iteration 49/1000 | Loss: 0.00009765
Iteration 50/1000 | Loss: 0.00006163
Iteration 51/1000 | Loss: 0.00007593
Iteration 52/1000 | Loss: 0.00007584
Iteration 53/1000 | Loss: 0.00007764
Iteration 54/1000 | Loss: 0.00031197
Iteration 55/1000 | Loss: 0.00025271
Iteration 56/1000 | Loss: 0.00008040
Iteration 57/1000 | Loss: 0.00007428
Iteration 58/1000 | Loss: 0.00033749
Iteration 59/1000 | Loss: 0.00008084
Iteration 60/1000 | Loss: 0.00009851
Iteration 61/1000 | Loss: 0.00010543
Iteration 62/1000 | Loss: 0.00012004
Iteration 63/1000 | Loss: 0.00042830
Iteration 64/1000 | Loss: 0.00014258
Iteration 65/1000 | Loss: 0.00012760
Iteration 66/1000 | Loss: 0.00020328
Iteration 67/1000 | Loss: 0.00013965
Iteration 68/1000 | Loss: 0.00022933
Iteration 69/1000 | Loss: 0.00030837
Iteration 70/1000 | Loss: 0.00023779
Iteration 71/1000 | Loss: 0.00034810
Iteration 72/1000 | Loss: 0.00012507
Iteration 73/1000 | Loss: 0.00016832
Iteration 74/1000 | Loss: 0.00006481
Iteration 75/1000 | Loss: 0.00004487
Iteration 76/1000 | Loss: 0.00005834
Iteration 77/1000 | Loss: 0.00010856
Iteration 78/1000 | Loss: 0.00010055
Iteration 79/1000 | Loss: 0.00010357
Iteration 80/1000 | Loss: 0.00003995
Iteration 81/1000 | Loss: 0.00005744
Iteration 82/1000 | Loss: 0.00007368
Iteration 83/1000 | Loss: 0.00007347
Iteration 84/1000 | Loss: 0.00010694
Iteration 85/1000 | Loss: 0.00003293
Iteration 86/1000 | Loss: 0.00002592
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002116
Iteration 89/1000 | Loss: 0.00002024
Iteration 90/1000 | Loss: 0.00001940
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001800
Iteration 94/1000 | Loss: 0.00001761
Iteration 95/1000 | Loss: 0.00001739
Iteration 96/1000 | Loss: 0.00001737
Iteration 97/1000 | Loss: 0.00001734
Iteration 98/1000 | Loss: 0.00001734
Iteration 99/1000 | Loss: 0.00001734
Iteration 100/1000 | Loss: 0.00001734
Iteration 101/1000 | Loss: 0.00001733
Iteration 102/1000 | Loss: 0.00001733
Iteration 103/1000 | Loss: 0.00001733
Iteration 104/1000 | Loss: 0.00001733
Iteration 105/1000 | Loss: 0.00001733
Iteration 106/1000 | Loss: 0.00001733
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001731
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001729
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001721
Iteration 115/1000 | Loss: 0.00001714
Iteration 116/1000 | Loss: 0.00001714
Iteration 117/1000 | Loss: 0.00001703
Iteration 118/1000 | Loss: 0.00001702
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001698
Iteration 121/1000 | Loss: 0.00001695
Iteration 122/1000 | Loss: 0.00001695
Iteration 123/1000 | Loss: 0.00001695
Iteration 124/1000 | Loss: 0.00001695
Iteration 125/1000 | Loss: 0.00001694
Iteration 126/1000 | Loss: 0.00001692
Iteration 127/1000 | Loss: 0.00001692
Iteration 128/1000 | Loss: 0.00001691
Iteration 129/1000 | Loss: 0.00001690
Iteration 130/1000 | Loss: 0.00001690
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001685
Iteration 133/1000 | Loss: 0.00001684
Iteration 134/1000 | Loss: 0.00001684
Iteration 135/1000 | Loss: 0.00001684
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001684
Iteration 139/1000 | Loss: 0.00001684
Iteration 140/1000 | Loss: 0.00001683
Iteration 141/1000 | Loss: 0.00001683
Iteration 142/1000 | Loss: 0.00001683
Iteration 143/1000 | Loss: 0.00001683
Iteration 144/1000 | Loss: 0.00001683
Iteration 145/1000 | Loss: 0.00001682
Iteration 146/1000 | Loss: 0.00001681
Iteration 147/1000 | Loss: 0.00001677
Iteration 148/1000 | Loss: 0.00001677
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001677
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001674
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001673
Iteration 155/1000 | Loss: 0.00001673
Iteration 156/1000 | Loss: 0.00001672
Iteration 157/1000 | Loss: 0.00001672
Iteration 158/1000 | Loss: 0.00001670
Iteration 159/1000 | Loss: 0.00001670
Iteration 160/1000 | Loss: 0.00001670
Iteration 161/1000 | Loss: 0.00001669
Iteration 162/1000 | Loss: 0.00001669
Iteration 163/1000 | Loss: 0.00001668
Iteration 164/1000 | Loss: 0.00001667
Iteration 165/1000 | Loss: 0.00001667
Iteration 166/1000 | Loss: 0.00001666
Iteration 167/1000 | Loss: 0.00001666
Iteration 168/1000 | Loss: 0.00001666
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001662
Iteration 173/1000 | Loss: 0.00001662
Iteration 174/1000 | Loss: 0.00001662
Iteration 175/1000 | Loss: 0.00001661
Iteration 176/1000 | Loss: 0.00001661
Iteration 177/1000 | Loss: 0.00001661
Iteration 178/1000 | Loss: 0.00001661
Iteration 179/1000 | Loss: 0.00001660
Iteration 180/1000 | Loss: 0.00001660
Iteration 181/1000 | Loss: 0.00001660
Iteration 182/1000 | Loss: 0.00001660
Iteration 183/1000 | Loss: 0.00001660
Iteration 184/1000 | Loss: 0.00001660
Iteration 185/1000 | Loss: 0.00001660
Iteration 186/1000 | Loss: 0.00001659
Iteration 187/1000 | Loss: 0.00001659
Iteration 188/1000 | Loss: 0.00001659
Iteration 189/1000 | Loss: 0.00001659
Iteration 190/1000 | Loss: 0.00001659
Iteration 191/1000 | Loss: 0.00001659
Iteration 192/1000 | Loss: 0.00001659
Iteration 193/1000 | Loss: 0.00001659
Iteration 194/1000 | Loss: 0.00001658
Iteration 195/1000 | Loss: 0.00001658
Iteration 196/1000 | Loss: 0.00001658
Iteration 197/1000 | Loss: 0.00001658
Iteration 198/1000 | Loss: 0.00001658
Iteration 199/1000 | Loss: 0.00001657
Iteration 200/1000 | Loss: 0.00001657
Iteration 201/1000 | Loss: 0.00001657
Iteration 202/1000 | Loss: 0.00001657
Iteration 203/1000 | Loss: 0.00001657
Iteration 204/1000 | Loss: 0.00001656
Iteration 205/1000 | Loss: 0.00001656
Iteration 206/1000 | Loss: 0.00001656
Iteration 207/1000 | Loss: 0.00001656
Iteration 208/1000 | Loss: 0.00001656
Iteration 209/1000 | Loss: 0.00001656
Iteration 210/1000 | Loss: 0.00001656
Iteration 211/1000 | Loss: 0.00001656
Iteration 212/1000 | Loss: 0.00001656
Iteration 213/1000 | Loss: 0.00001656
Iteration 214/1000 | Loss: 0.00001656
Iteration 215/1000 | Loss: 0.00001655
Iteration 216/1000 | Loss: 0.00001655
Iteration 217/1000 | Loss: 0.00001655
Iteration 218/1000 | Loss: 0.00001655
Iteration 219/1000 | Loss: 0.00001655
Iteration 220/1000 | Loss: 0.00001655
Iteration 221/1000 | Loss: 0.00001655
Iteration 222/1000 | Loss: 0.00001655
Iteration 223/1000 | Loss: 0.00001654
Iteration 224/1000 | Loss: 0.00001654
Iteration 225/1000 | Loss: 0.00001654
Iteration 226/1000 | Loss: 0.00001654
Iteration 227/1000 | Loss: 0.00001654
Iteration 228/1000 | Loss: 0.00001654
Iteration 229/1000 | Loss: 0.00001654
Iteration 230/1000 | Loss: 0.00027443
Iteration 231/1000 | Loss: 0.00002315
Iteration 232/1000 | Loss: 0.00001856
Iteration 233/1000 | Loss: 0.00001758
Iteration 234/1000 | Loss: 0.00001683
Iteration 235/1000 | Loss: 0.00001655
Iteration 236/1000 | Loss: 0.00001623
Iteration 237/1000 | Loss: 0.00001611
Iteration 238/1000 | Loss: 0.00001595
Iteration 239/1000 | Loss: 0.00001593
Iteration 240/1000 | Loss: 0.00001592
Iteration 241/1000 | Loss: 0.00001592
Iteration 242/1000 | Loss: 0.00001591
Iteration 243/1000 | Loss: 0.00001589
Iteration 244/1000 | Loss: 0.00001589
Iteration 245/1000 | Loss: 0.00001589
Iteration 246/1000 | Loss: 0.00001588
Iteration 247/1000 | Loss: 0.00001588
Iteration 248/1000 | Loss: 0.00001588
Iteration 249/1000 | Loss: 0.00001587
Iteration 250/1000 | Loss: 0.00001587
Iteration 251/1000 | Loss: 0.00001587
Iteration 252/1000 | Loss: 0.00001584
Iteration 253/1000 | Loss: 0.00001584
Iteration 254/1000 | Loss: 0.00001584
Iteration 255/1000 | Loss: 0.00001583
Iteration 256/1000 | Loss: 0.00001583
Iteration 257/1000 | Loss: 0.00001583
Iteration 258/1000 | Loss: 0.00001582
Iteration 259/1000 | Loss: 0.00001582
Iteration 260/1000 | Loss: 0.00001582
Iteration 261/1000 | Loss: 0.00001582
Iteration 262/1000 | Loss: 0.00001582
Iteration 263/1000 | Loss: 0.00001582
Iteration 264/1000 | Loss: 0.00001581
Iteration 265/1000 | Loss: 0.00001581
Iteration 266/1000 | Loss: 0.00001581
Iteration 267/1000 | Loss: 0.00001581
Iteration 268/1000 | Loss: 0.00001580
Iteration 269/1000 | Loss: 0.00001580
Iteration 270/1000 | Loss: 0.00001580
Iteration 271/1000 | Loss: 0.00001580
Iteration 272/1000 | Loss: 0.00001580
Iteration 273/1000 | Loss: 0.00001580
Iteration 274/1000 | Loss: 0.00001580
Iteration 275/1000 | Loss: 0.00001580
Iteration 276/1000 | Loss: 0.00001580
Iteration 277/1000 | Loss: 0.00001580
Iteration 278/1000 | Loss: 0.00001580
Iteration 279/1000 | Loss: 0.00001579
Iteration 280/1000 | Loss: 0.00001579
Iteration 281/1000 | Loss: 0.00001579
Iteration 282/1000 | Loss: 0.00001579
Iteration 283/1000 | Loss: 0.00001579
Iteration 284/1000 | Loss: 0.00001579
Iteration 285/1000 | Loss: 0.00001579
Iteration 286/1000 | Loss: 0.00001579
Iteration 287/1000 | Loss: 0.00001579
Iteration 288/1000 | Loss: 0.00001579
Iteration 289/1000 | Loss: 0.00001579
Iteration 290/1000 | Loss: 0.00001579
Iteration 291/1000 | Loss: 0.00001579
Iteration 292/1000 | Loss: 0.00001579
Iteration 293/1000 | Loss: 0.00001579
Iteration 294/1000 | Loss: 0.00001579
Iteration 295/1000 | Loss: 0.00001579
Iteration 296/1000 | Loss: 0.00001579
Iteration 297/1000 | Loss: 0.00001579
Iteration 298/1000 | Loss: 0.00001579
Iteration 299/1000 | Loss: 0.00001579
Iteration 300/1000 | Loss: 0.00001579
Iteration 301/1000 | Loss: 0.00001579
Iteration 302/1000 | Loss: 0.00001579
Iteration 303/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [1.5785821233293973e-05, 1.5785821233293973e-05, 1.5785821233293973e-05, 1.5785821233293973e-05, 1.5785821233293973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5785821233293973e-05

Optimization complete. Final v2v error: 3.3078529834747314 mm

Highest mean error: 4.116417407989502 mm for frame 112

Lowest mean error: 2.6554577350616455 mm for frame 121

Saving results

Total time: 212.81910395622253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439695
Iteration 2/25 | Loss: 0.00074517
Iteration 3/25 | Loss: 0.00060335
Iteration 4/25 | Loss: 0.00058012
Iteration 5/25 | Loss: 0.00056967
Iteration 6/25 | Loss: 0.00056740
Iteration 7/25 | Loss: 0.00056686
Iteration 8/25 | Loss: 0.00056686
Iteration 9/25 | Loss: 0.00056686
Iteration 10/25 | Loss: 0.00056686
Iteration 11/25 | Loss: 0.00056686
Iteration 12/25 | Loss: 0.00056686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005668628728017211, 0.0005668628728017211, 0.0005668628728017211, 0.0005668628728017211, 0.0005668628728017211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005668628728017211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51766026
Iteration 2/25 | Loss: 0.00019903
Iteration 3/25 | Loss: 0.00019902
Iteration 4/25 | Loss: 0.00019902
Iteration 5/25 | Loss: 0.00019902
Iteration 6/25 | Loss: 0.00019902
Iteration 7/25 | Loss: 0.00019902
Iteration 8/25 | Loss: 0.00019902
Iteration 9/25 | Loss: 0.00019902
Iteration 10/25 | Loss: 0.00019902
Iteration 11/25 | Loss: 0.00019902
Iteration 12/25 | Loss: 0.00019902
Iteration 13/25 | Loss: 0.00019902
Iteration 14/25 | Loss: 0.00019902
Iteration 15/25 | Loss: 0.00019902
Iteration 16/25 | Loss: 0.00019902
Iteration 17/25 | Loss: 0.00019902
Iteration 18/25 | Loss: 0.00019902
Iteration 19/25 | Loss: 0.00019902
Iteration 20/25 | Loss: 0.00019902
Iteration 21/25 | Loss: 0.00019902
Iteration 22/25 | Loss: 0.00019902
Iteration 23/25 | Loss: 0.00019902
Iteration 24/25 | Loss: 0.00019902
Iteration 25/25 | Loss: 0.00019902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019902
Iteration 2/1000 | Loss: 0.00002411
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001340
Iteration 5/1000 | Loss: 0.00001287
Iteration 6/1000 | Loss: 0.00001241
Iteration 7/1000 | Loss: 0.00001217
Iteration 8/1000 | Loss: 0.00001203
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001194
Iteration 11/1000 | Loss: 0.00001193
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001186
Iteration 15/1000 | Loss: 0.00001181
Iteration 16/1000 | Loss: 0.00001180
Iteration 17/1000 | Loss: 0.00001176
Iteration 18/1000 | Loss: 0.00001176
Iteration 19/1000 | Loss: 0.00001175
Iteration 20/1000 | Loss: 0.00001175
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001174
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001174
Iteration 25/1000 | Loss: 0.00001172
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001172
Iteration 28/1000 | Loss: 0.00001172
Iteration 29/1000 | Loss: 0.00001172
Iteration 30/1000 | Loss: 0.00001172
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001172
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001171
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001164
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001164
Iteration 64/1000 | Loss: 0.00001164
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001160
Iteration 88/1000 | Loss: 0.00001160
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001159
Iteration 94/1000 | Loss: 0.00001159
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001158
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001158
Iteration 104/1000 | Loss: 0.00001158
Iteration 105/1000 | Loss: 0.00001158
Iteration 106/1000 | Loss: 0.00001158
Iteration 107/1000 | Loss: 0.00001158
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.1581150829442777e-05, 1.1581150829442777e-05, 1.1581150829442777e-05, 1.1581150829442777e-05, 1.1581150829442777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1581150829442777e-05

Optimization complete. Final v2v error: 2.897258996963501 mm

Highest mean error: 3.2083518505096436 mm for frame 84

Lowest mean error: 2.64758563041687 mm for frame 157

Saving results

Total time: 33.88798522949219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450424
Iteration 2/25 | Loss: 0.00077391
Iteration 3/25 | Loss: 0.00059398
Iteration 4/25 | Loss: 0.00057201
Iteration 5/25 | Loss: 0.00056748
Iteration 6/25 | Loss: 0.00056667
Iteration 7/25 | Loss: 0.00056667
Iteration 8/25 | Loss: 0.00056667
Iteration 9/25 | Loss: 0.00056667
Iteration 10/25 | Loss: 0.00056667
Iteration 11/25 | Loss: 0.00056667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005666664219461381, 0.0005666664219461381, 0.0005666664219461381, 0.0005666664219461381, 0.0005666664219461381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005666664219461381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.80178499
Iteration 2/25 | Loss: 0.00013900
Iteration 3/25 | Loss: 0.00013898
Iteration 4/25 | Loss: 0.00013898
Iteration 5/25 | Loss: 0.00013898
Iteration 6/25 | Loss: 0.00013898
Iteration 7/25 | Loss: 0.00013898
Iteration 8/25 | Loss: 0.00013898
Iteration 9/25 | Loss: 0.00013898
Iteration 10/25 | Loss: 0.00013898
Iteration 11/25 | Loss: 0.00013898
Iteration 12/25 | Loss: 0.00013898
Iteration 13/25 | Loss: 0.00013898
Iteration 14/25 | Loss: 0.00013898
Iteration 15/25 | Loss: 0.00013898
Iteration 16/25 | Loss: 0.00013898
Iteration 17/25 | Loss: 0.00013898
Iteration 18/25 | Loss: 0.00013898
Iteration 19/25 | Loss: 0.00013898
Iteration 20/25 | Loss: 0.00013898
Iteration 21/25 | Loss: 0.00013898
Iteration 22/25 | Loss: 0.00013898
Iteration 23/25 | Loss: 0.00013898
Iteration 24/25 | Loss: 0.00013898
Iteration 25/25 | Loss: 0.00013898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013898
Iteration 2/1000 | Loss: 0.00002065
Iteration 3/1000 | Loss: 0.00001354
Iteration 4/1000 | Loss: 0.00001260
Iteration 5/1000 | Loss: 0.00001194
Iteration 6/1000 | Loss: 0.00001173
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001132
Iteration 10/1000 | Loss: 0.00001131
Iteration 11/1000 | Loss: 0.00001130
Iteration 12/1000 | Loss: 0.00001125
Iteration 13/1000 | Loss: 0.00001125
Iteration 14/1000 | Loss: 0.00001124
Iteration 15/1000 | Loss: 0.00001124
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001112
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001110
Iteration 24/1000 | Loss: 0.00001110
Iteration 25/1000 | Loss: 0.00001110
Iteration 26/1000 | Loss: 0.00001109
Iteration 27/1000 | Loss: 0.00001109
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001107
Iteration 30/1000 | Loss: 0.00001107
Iteration 31/1000 | Loss: 0.00001106
Iteration 32/1000 | Loss: 0.00001106
Iteration 33/1000 | Loss: 0.00001106
Iteration 34/1000 | Loss: 0.00001106
Iteration 35/1000 | Loss: 0.00001105
Iteration 36/1000 | Loss: 0.00001105
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001104
Iteration 39/1000 | Loss: 0.00001104
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001101
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001101
Iteration 52/1000 | Loss: 0.00001101
Iteration 53/1000 | Loss: 0.00001100
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001100
Iteration 59/1000 | Loss: 0.00001100
Iteration 60/1000 | Loss: 0.00001100
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001099
Iteration 64/1000 | Loss: 0.00001099
Iteration 65/1000 | Loss: 0.00001099
Iteration 66/1000 | Loss: 0.00001099
Iteration 67/1000 | Loss: 0.00001099
Iteration 68/1000 | Loss: 0.00001098
Iteration 69/1000 | Loss: 0.00001098
Iteration 70/1000 | Loss: 0.00001098
Iteration 71/1000 | Loss: 0.00001098
Iteration 72/1000 | Loss: 0.00001098
Iteration 73/1000 | Loss: 0.00001098
Iteration 74/1000 | Loss: 0.00001098
Iteration 75/1000 | Loss: 0.00001097
Iteration 76/1000 | Loss: 0.00001097
Iteration 77/1000 | Loss: 0.00001097
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001096
Iteration 80/1000 | Loss: 0.00001096
Iteration 81/1000 | Loss: 0.00001096
Iteration 82/1000 | Loss: 0.00001096
Iteration 83/1000 | Loss: 0.00001096
Iteration 84/1000 | Loss: 0.00001095
Iteration 85/1000 | Loss: 0.00001095
Iteration 86/1000 | Loss: 0.00001095
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001095
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001094
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001093
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001093
Iteration 110/1000 | Loss: 0.00001093
Iteration 111/1000 | Loss: 0.00001093
Iteration 112/1000 | Loss: 0.00001093
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001091
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001091
Iteration 119/1000 | Loss: 0.00001091
Iteration 120/1000 | Loss: 0.00001090
Iteration 121/1000 | Loss: 0.00001090
Iteration 122/1000 | Loss: 0.00001090
Iteration 123/1000 | Loss: 0.00001090
Iteration 124/1000 | Loss: 0.00001090
Iteration 125/1000 | Loss: 0.00001090
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Iteration 129/1000 | Loss: 0.00001089
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001088
Iteration 132/1000 | Loss: 0.00001088
Iteration 133/1000 | Loss: 0.00001088
Iteration 134/1000 | Loss: 0.00001088
Iteration 135/1000 | Loss: 0.00001088
Iteration 136/1000 | Loss: 0.00001088
Iteration 137/1000 | Loss: 0.00001088
Iteration 138/1000 | Loss: 0.00001087
Iteration 139/1000 | Loss: 0.00001087
Iteration 140/1000 | Loss: 0.00001087
Iteration 141/1000 | Loss: 0.00001087
Iteration 142/1000 | Loss: 0.00001087
Iteration 143/1000 | Loss: 0.00001087
Iteration 144/1000 | Loss: 0.00001087
Iteration 145/1000 | Loss: 0.00001087
Iteration 146/1000 | Loss: 0.00001087
Iteration 147/1000 | Loss: 0.00001087
Iteration 148/1000 | Loss: 0.00001087
Iteration 149/1000 | Loss: 0.00001086
Iteration 150/1000 | Loss: 0.00001086
Iteration 151/1000 | Loss: 0.00001086
Iteration 152/1000 | Loss: 0.00001086
Iteration 153/1000 | Loss: 0.00001086
Iteration 154/1000 | Loss: 0.00001086
Iteration 155/1000 | Loss: 0.00001086
Iteration 156/1000 | Loss: 0.00001086
Iteration 157/1000 | Loss: 0.00001086
Iteration 158/1000 | Loss: 0.00001086
Iteration 159/1000 | Loss: 0.00001086
Iteration 160/1000 | Loss: 0.00001086
Iteration 161/1000 | Loss: 0.00001086
Iteration 162/1000 | Loss: 0.00001086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.0860392649192363e-05, 1.0860392649192363e-05, 1.0860392649192363e-05, 1.0860392649192363e-05, 1.0860392649192363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0860392649192363e-05

Optimization complete. Final v2v error: 2.8061931133270264 mm

Highest mean error: 3.1613807678222656 mm for frame 131

Lowest mean error: 2.579402446746826 mm for frame 155

Saving results

Total time: 37.70827841758728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456452
Iteration 2/25 | Loss: 0.00099528
Iteration 3/25 | Loss: 0.00065670
Iteration 4/25 | Loss: 0.00061758
Iteration 5/25 | Loss: 0.00060637
Iteration 6/25 | Loss: 0.00060269
Iteration 7/25 | Loss: 0.00060186
Iteration 8/25 | Loss: 0.00060175
Iteration 9/25 | Loss: 0.00060175
Iteration 10/25 | Loss: 0.00060175
Iteration 11/25 | Loss: 0.00060175
Iteration 12/25 | Loss: 0.00060175
Iteration 13/25 | Loss: 0.00060175
Iteration 14/25 | Loss: 0.00060175
Iteration 15/25 | Loss: 0.00060175
Iteration 16/25 | Loss: 0.00060175
Iteration 17/25 | Loss: 0.00060175
Iteration 18/25 | Loss: 0.00060175
Iteration 19/25 | Loss: 0.00060175
Iteration 20/25 | Loss: 0.00060175
Iteration 21/25 | Loss: 0.00060175
Iteration 22/25 | Loss: 0.00060175
Iteration 23/25 | Loss: 0.00060175
Iteration 24/25 | Loss: 0.00060175
Iteration 25/25 | Loss: 0.00060175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58374393
Iteration 2/25 | Loss: 0.00018906
Iteration 3/25 | Loss: 0.00018904
Iteration 4/25 | Loss: 0.00018904
Iteration 5/25 | Loss: 0.00018904
Iteration 6/25 | Loss: 0.00018904
Iteration 7/25 | Loss: 0.00018904
Iteration 8/25 | Loss: 0.00018904
Iteration 9/25 | Loss: 0.00018904
Iteration 10/25 | Loss: 0.00018904
Iteration 11/25 | Loss: 0.00018904
Iteration 12/25 | Loss: 0.00018904
Iteration 13/25 | Loss: 0.00018904
Iteration 14/25 | Loss: 0.00018904
Iteration 15/25 | Loss: 0.00018904
Iteration 16/25 | Loss: 0.00018904
Iteration 17/25 | Loss: 0.00018904
Iteration 18/25 | Loss: 0.00018904
Iteration 19/25 | Loss: 0.00018904
Iteration 20/25 | Loss: 0.00018904
Iteration 21/25 | Loss: 0.00018904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00018904187891166657, 0.00018904187891166657, 0.00018904187891166657, 0.00018904187891166657, 0.00018904187891166657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018904187891166657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018904
Iteration 2/1000 | Loss: 0.00002354
Iteration 3/1000 | Loss: 0.00001806
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00001643
Iteration 6/1000 | Loss: 0.00001603
Iteration 7/1000 | Loss: 0.00001573
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001531
Iteration 10/1000 | Loss: 0.00001530
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001527
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001522
Iteration 19/1000 | Loss: 0.00001521
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001518
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001517
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00001517
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001517
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001514
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001510
Iteration 45/1000 | Loss: 0.00001510
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001509
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001508
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001507
Iteration 53/1000 | Loss: 0.00001507
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001506
Iteration 57/1000 | Loss: 0.00001506
Iteration 58/1000 | Loss: 0.00001506
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001505
Iteration 61/1000 | Loss: 0.00001505
Iteration 62/1000 | Loss: 0.00001505
Iteration 63/1000 | Loss: 0.00001504
Iteration 64/1000 | Loss: 0.00001504
Iteration 65/1000 | Loss: 0.00001504
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001504
Iteration 70/1000 | Loss: 0.00001504
Iteration 71/1000 | Loss: 0.00001504
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001504
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001503
Iteration 78/1000 | Loss: 0.00001503
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00001502
Iteration 81/1000 | Loss: 0.00001502
Iteration 82/1000 | Loss: 0.00001502
Iteration 83/1000 | Loss: 0.00001501
Iteration 84/1000 | Loss: 0.00001501
Iteration 85/1000 | Loss: 0.00001501
Iteration 86/1000 | Loss: 0.00001501
Iteration 87/1000 | Loss: 0.00001501
Iteration 88/1000 | Loss: 0.00001501
Iteration 89/1000 | Loss: 0.00001501
Iteration 90/1000 | Loss: 0.00001501
Iteration 91/1000 | Loss: 0.00001501
Iteration 92/1000 | Loss: 0.00001501
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001500
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001500
Iteration 97/1000 | Loss: 0.00001500
Iteration 98/1000 | Loss: 0.00001500
Iteration 99/1000 | Loss: 0.00001500
Iteration 100/1000 | Loss: 0.00001500
Iteration 101/1000 | Loss: 0.00001500
Iteration 102/1000 | Loss: 0.00001499
Iteration 103/1000 | Loss: 0.00001499
Iteration 104/1000 | Loss: 0.00001499
Iteration 105/1000 | Loss: 0.00001499
Iteration 106/1000 | Loss: 0.00001499
Iteration 107/1000 | Loss: 0.00001499
Iteration 108/1000 | Loss: 0.00001499
Iteration 109/1000 | Loss: 0.00001499
Iteration 110/1000 | Loss: 0.00001498
Iteration 111/1000 | Loss: 0.00001498
Iteration 112/1000 | Loss: 0.00001498
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001497
Iteration 125/1000 | Loss: 0.00001497
Iteration 126/1000 | Loss: 0.00001497
Iteration 127/1000 | Loss: 0.00001497
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001497
Iteration 130/1000 | Loss: 0.00001497
Iteration 131/1000 | Loss: 0.00001497
Iteration 132/1000 | Loss: 0.00001496
Iteration 133/1000 | Loss: 0.00001496
Iteration 134/1000 | Loss: 0.00001496
Iteration 135/1000 | Loss: 0.00001496
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001496
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001495
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001494
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001494
Iteration 150/1000 | Loss: 0.00001494
Iteration 151/1000 | Loss: 0.00001494
Iteration 152/1000 | Loss: 0.00001493
Iteration 153/1000 | Loss: 0.00001493
Iteration 154/1000 | Loss: 0.00001493
Iteration 155/1000 | Loss: 0.00001493
Iteration 156/1000 | Loss: 0.00001493
Iteration 157/1000 | Loss: 0.00001493
Iteration 158/1000 | Loss: 0.00001493
Iteration 159/1000 | Loss: 0.00001493
Iteration 160/1000 | Loss: 0.00001492
Iteration 161/1000 | Loss: 0.00001492
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001492
Iteration 164/1000 | Loss: 0.00001492
Iteration 165/1000 | Loss: 0.00001492
Iteration 166/1000 | Loss: 0.00001492
Iteration 167/1000 | Loss: 0.00001492
Iteration 168/1000 | Loss: 0.00001492
Iteration 169/1000 | Loss: 0.00001492
Iteration 170/1000 | Loss: 0.00001492
Iteration 171/1000 | Loss: 0.00001492
Iteration 172/1000 | Loss: 0.00001492
Iteration 173/1000 | Loss: 0.00001492
Iteration 174/1000 | Loss: 0.00001491
Iteration 175/1000 | Loss: 0.00001491
Iteration 176/1000 | Loss: 0.00001491
Iteration 177/1000 | Loss: 0.00001491
Iteration 178/1000 | Loss: 0.00001491
Iteration 179/1000 | Loss: 0.00001491
Iteration 180/1000 | Loss: 0.00001491
Iteration 181/1000 | Loss: 0.00001491
Iteration 182/1000 | Loss: 0.00001491
Iteration 183/1000 | Loss: 0.00001491
Iteration 184/1000 | Loss: 0.00001491
Iteration 185/1000 | Loss: 0.00001491
Iteration 186/1000 | Loss: 0.00001491
Iteration 187/1000 | Loss: 0.00001490
Iteration 188/1000 | Loss: 0.00001490
Iteration 189/1000 | Loss: 0.00001490
Iteration 190/1000 | Loss: 0.00001490
Iteration 191/1000 | Loss: 0.00001490
Iteration 192/1000 | Loss: 0.00001490
Iteration 193/1000 | Loss: 0.00001490
Iteration 194/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.49046718433965e-05, 1.49046718433965e-05, 1.49046718433965e-05, 1.49046718433965e-05, 1.49046718433965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.49046718433965e-05

Optimization complete. Final v2v error: 3.2361321449279785 mm

Highest mean error: 4.09856653213501 mm for frame 147

Lowest mean error: 2.6251449584960938 mm for frame 7

Saving results

Total time: 38.55230522155762
