Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=26, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1456-1511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811900
Iteration 2/25 | Loss: 0.00172909
Iteration 3/25 | Loss: 0.00135824
Iteration 4/25 | Loss: 0.00131948
Iteration 5/25 | Loss: 0.00127490
Iteration 6/25 | Loss: 0.00125149
Iteration 7/25 | Loss: 0.00123244
Iteration 8/25 | Loss: 0.00122460
Iteration 9/25 | Loss: 0.00122338
Iteration 10/25 | Loss: 0.00122822
Iteration 11/25 | Loss: 0.00122893
Iteration 12/25 | Loss: 0.00122791
Iteration 13/25 | Loss: 0.00122742
Iteration 14/25 | Loss: 0.00122190
Iteration 15/25 | Loss: 0.00122710
Iteration 16/25 | Loss: 0.00122708
Iteration 17/25 | Loss: 0.00122910
Iteration 18/25 | Loss: 0.00122560
Iteration 19/25 | Loss: 0.00122152
Iteration 20/25 | Loss: 0.00122113
Iteration 21/25 | Loss: 0.00122113
Iteration 22/25 | Loss: 0.00122113
Iteration 23/25 | Loss: 0.00122113
Iteration 24/25 | Loss: 0.00122113
Iteration 25/25 | Loss: 0.00122113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.45128345
Iteration 2/25 | Loss: 0.00152459
Iteration 3/25 | Loss: 0.00152445
Iteration 4/25 | Loss: 0.00152445
Iteration 5/25 | Loss: 0.00152445
Iteration 6/25 | Loss: 0.00152445
Iteration 7/25 | Loss: 0.00152445
Iteration 8/25 | Loss: 0.00152445
Iteration 9/25 | Loss: 0.00152445
Iteration 10/25 | Loss: 0.00152445
Iteration 11/25 | Loss: 0.00152445
Iteration 12/25 | Loss: 0.00152445
Iteration 13/25 | Loss: 0.00152445
Iteration 14/25 | Loss: 0.00152445
Iteration 15/25 | Loss: 0.00152445
Iteration 16/25 | Loss: 0.00152445
Iteration 17/25 | Loss: 0.00152445
Iteration 18/25 | Loss: 0.00152445
Iteration 19/25 | Loss: 0.00152445
Iteration 20/25 | Loss: 0.00152445
Iteration 21/25 | Loss: 0.00152445
Iteration 22/25 | Loss: 0.00152445
Iteration 23/25 | Loss: 0.00152445
Iteration 24/25 | Loss: 0.00152445
Iteration 25/25 | Loss: 0.00152445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152445
Iteration 2/1000 | Loss: 0.00016631
Iteration 3/1000 | Loss: 0.00024729
Iteration 4/1000 | Loss: 0.00023033
Iteration 5/1000 | Loss: 0.00020270
Iteration 6/1000 | Loss: 0.00018479
Iteration 7/1000 | Loss: 0.00014318
Iteration 8/1000 | Loss: 0.00017777
Iteration 9/1000 | Loss: 0.00030959
Iteration 10/1000 | Loss: 0.00009774
Iteration 11/1000 | Loss: 0.00015831
Iteration 12/1000 | Loss: 0.00011047
Iteration 13/1000 | Loss: 0.00016129
Iteration 14/1000 | Loss: 0.00015084
Iteration 15/1000 | Loss: 0.00013875
Iteration 16/1000 | Loss: 0.00009643
Iteration 17/1000 | Loss: 0.00026709
Iteration 18/1000 | Loss: 0.00023298
Iteration 19/1000 | Loss: 0.00017370
Iteration 20/1000 | Loss: 0.00025059
Iteration 21/1000 | Loss: 0.00018531
Iteration 22/1000 | Loss: 0.00016211
Iteration 23/1000 | Loss: 0.00022188
Iteration 24/1000 | Loss: 0.00025475
Iteration 25/1000 | Loss: 0.00026149
Iteration 26/1000 | Loss: 0.00025271
Iteration 27/1000 | Loss: 0.00024773
Iteration 28/1000 | Loss: 0.00015260
Iteration 29/1000 | Loss: 0.00019736
Iteration 30/1000 | Loss: 0.00022886
Iteration 31/1000 | Loss: 0.00015813
Iteration 32/1000 | Loss: 0.00013483
Iteration 33/1000 | Loss: 0.00029435
Iteration 34/1000 | Loss: 0.00028859
Iteration 35/1000 | Loss: 0.00018690
Iteration 36/1000 | Loss: 0.00023498
Iteration 37/1000 | Loss: 0.00028309
Iteration 38/1000 | Loss: 0.00018430
Iteration 39/1000 | Loss: 0.00022659
Iteration 40/1000 | Loss: 0.00022654
Iteration 41/1000 | Loss: 0.00029603
Iteration 42/1000 | Loss: 0.00029503
Iteration 43/1000 | Loss: 0.00022398
Iteration 44/1000 | Loss: 0.00036095
Iteration 45/1000 | Loss: 0.00024968
Iteration 46/1000 | Loss: 0.00014785
Iteration 47/1000 | Loss: 0.00022897
Iteration 48/1000 | Loss: 0.00012429
Iteration 49/1000 | Loss: 0.00091378
Iteration 50/1000 | Loss: 0.00015782
Iteration 51/1000 | Loss: 0.00015324
Iteration 52/1000 | Loss: 0.00012055
Iteration 53/1000 | Loss: 0.00023633
Iteration 54/1000 | Loss: 0.00030397
Iteration 55/1000 | Loss: 0.00021687
Iteration 56/1000 | Loss: 0.00035266
Iteration 57/1000 | Loss: 0.00024566
Iteration 58/1000 | Loss: 0.00011299
Iteration 59/1000 | Loss: 0.00009459
Iteration 60/1000 | Loss: 0.00030113
Iteration 61/1000 | Loss: 0.00023983
Iteration 62/1000 | Loss: 0.00040475
Iteration 63/1000 | Loss: 0.00023030
Iteration 64/1000 | Loss: 0.00015453
Iteration 65/1000 | Loss: 0.00026470
Iteration 66/1000 | Loss: 0.00015595
Iteration 67/1000 | Loss: 0.00020245
Iteration 68/1000 | Loss: 0.00019978
Iteration 69/1000 | Loss: 0.00029093
Iteration 70/1000 | Loss: 0.00050647
Iteration 71/1000 | Loss: 0.00016726
Iteration 72/1000 | Loss: 0.00016501
Iteration 73/1000 | Loss: 0.00028719
Iteration 74/1000 | Loss: 0.00019325
Iteration 75/1000 | Loss: 0.00024798
Iteration 76/1000 | Loss: 0.00024935
Iteration 77/1000 | Loss: 0.00022640
Iteration 78/1000 | Loss: 0.00024413
Iteration 79/1000 | Loss: 0.00010832
Iteration 80/1000 | Loss: 0.00008912
Iteration 81/1000 | Loss: 0.00022525
Iteration 82/1000 | Loss: 0.00028344
Iteration 83/1000 | Loss: 0.00017467
Iteration 84/1000 | Loss: 0.00025950
Iteration 85/1000 | Loss: 0.00032343
Iteration 86/1000 | Loss: 0.00040252
Iteration 87/1000 | Loss: 0.00331080
Iteration 88/1000 | Loss: 0.00038879
Iteration 89/1000 | Loss: 0.00013846
Iteration 90/1000 | Loss: 0.00009915
Iteration 91/1000 | Loss: 0.00008595
Iteration 92/1000 | Loss: 0.00007544
Iteration 93/1000 | Loss: 0.00009689
Iteration 94/1000 | Loss: 0.00012107
Iteration 95/1000 | Loss: 0.00011001
Iteration 96/1000 | Loss: 0.00005711
Iteration 97/1000 | Loss: 0.00005206
Iteration 98/1000 | Loss: 0.00004963
Iteration 99/1000 | Loss: 0.00004792
Iteration 100/1000 | Loss: 0.00007669
Iteration 101/1000 | Loss: 0.00005013
Iteration 102/1000 | Loss: 0.00004885
Iteration 103/1000 | Loss: 0.00008585
Iteration 104/1000 | Loss: 0.00005550
Iteration 105/1000 | Loss: 0.00006050
Iteration 106/1000 | Loss: 0.00008856
Iteration 107/1000 | Loss: 0.00006167
Iteration 108/1000 | Loss: 0.00008423
Iteration 109/1000 | Loss: 0.00005116
Iteration 110/1000 | Loss: 0.00004819
Iteration 111/1000 | Loss: 0.00005951
Iteration 112/1000 | Loss: 0.00004713
Iteration 113/1000 | Loss: 0.00006693
Iteration 114/1000 | Loss: 0.00005597
Iteration 115/1000 | Loss: 0.00006055
Iteration 116/1000 | Loss: 0.00005542
Iteration 117/1000 | Loss: 0.00005443
Iteration 118/1000 | Loss: 0.00005554
Iteration 119/1000 | Loss: 0.00005242
Iteration 120/1000 | Loss: 0.00009553
Iteration 121/1000 | Loss: 0.00007411
Iteration 122/1000 | Loss: 0.00008809
Iteration 123/1000 | Loss: 0.00005933
Iteration 124/1000 | Loss: 0.00004656
Iteration 125/1000 | Loss: 0.00004450
Iteration 126/1000 | Loss: 0.00004372
Iteration 127/1000 | Loss: 0.00004315
Iteration 128/1000 | Loss: 0.00004268
Iteration 129/1000 | Loss: 0.00006140
Iteration 130/1000 | Loss: 0.00004660
Iteration 131/1000 | Loss: 0.00005702
Iteration 132/1000 | Loss: 0.00006096
Iteration 133/1000 | Loss: 0.00005783
Iteration 134/1000 | Loss: 0.00007107
Iteration 135/1000 | Loss: 0.00006605
Iteration 136/1000 | Loss: 0.00004477
Iteration 137/1000 | Loss: 0.00005958
Iteration 138/1000 | Loss: 0.00005601
Iteration 139/1000 | Loss: 0.00006014
Iteration 140/1000 | Loss: 0.00005595
Iteration 141/1000 | Loss: 0.00006212
Iteration 142/1000 | Loss: 0.00005746
Iteration 143/1000 | Loss: 0.00006290
Iteration 144/1000 | Loss: 0.00005671
Iteration 145/1000 | Loss: 0.00005910
Iteration 146/1000 | Loss: 0.00005366
Iteration 147/1000 | Loss: 0.00005992
Iteration 148/1000 | Loss: 0.00005419
Iteration 149/1000 | Loss: 0.00006303
Iteration 150/1000 | Loss: 0.00005682
Iteration 151/1000 | Loss: 0.00006056
Iteration 152/1000 | Loss: 0.00005213
Iteration 153/1000 | Loss: 0.00005997
Iteration 154/1000 | Loss: 0.00005508
Iteration 155/1000 | Loss: 0.00005955
Iteration 156/1000 | Loss: 0.00005165
Iteration 157/1000 | Loss: 0.00006109
Iteration 158/1000 | Loss: 0.00005075
Iteration 159/1000 | Loss: 0.00006089
Iteration 160/1000 | Loss: 0.00005101
Iteration 161/1000 | Loss: 0.00005577
Iteration 162/1000 | Loss: 0.00004642
Iteration 163/1000 | Loss: 0.00004489
Iteration 164/1000 | Loss: 0.00004334
Iteration 165/1000 | Loss: 0.00004262
Iteration 166/1000 | Loss: 0.00004193
Iteration 167/1000 | Loss: 0.00004168
Iteration 168/1000 | Loss: 0.00004147
Iteration 169/1000 | Loss: 0.00004141
Iteration 170/1000 | Loss: 0.00004136
Iteration 171/1000 | Loss: 0.00004135
Iteration 172/1000 | Loss: 0.00004126
Iteration 173/1000 | Loss: 0.00004125
Iteration 174/1000 | Loss: 0.00004123
Iteration 175/1000 | Loss: 0.00004123
Iteration 176/1000 | Loss: 0.00004120
Iteration 177/1000 | Loss: 0.00004110
Iteration 178/1000 | Loss: 0.00004110
Iteration 179/1000 | Loss: 0.00004108
Iteration 180/1000 | Loss: 0.00004108
Iteration 181/1000 | Loss: 0.00004107
Iteration 182/1000 | Loss: 0.00004106
Iteration 183/1000 | Loss: 0.00004105
Iteration 184/1000 | Loss: 0.00004105
Iteration 185/1000 | Loss: 0.00004105
Iteration 186/1000 | Loss: 0.00004105
Iteration 187/1000 | Loss: 0.00004101
Iteration 188/1000 | Loss: 0.00004100
Iteration 189/1000 | Loss: 0.00004097
Iteration 190/1000 | Loss: 0.00004096
Iteration 191/1000 | Loss: 0.00004096
Iteration 192/1000 | Loss: 0.00004095
Iteration 193/1000 | Loss: 0.00004095
Iteration 194/1000 | Loss: 0.00004092
Iteration 195/1000 | Loss: 0.00004091
Iteration 196/1000 | Loss: 0.00004091
Iteration 197/1000 | Loss: 0.00004091
Iteration 198/1000 | Loss: 0.00004091
Iteration 199/1000 | Loss: 0.00004091
Iteration 200/1000 | Loss: 0.00004091
Iteration 201/1000 | Loss: 0.00004091
Iteration 202/1000 | Loss: 0.00004090
Iteration 203/1000 | Loss: 0.00004089
Iteration 204/1000 | Loss: 0.00004088
Iteration 205/1000 | Loss: 0.00004088
Iteration 206/1000 | Loss: 0.00004088
Iteration 207/1000 | Loss: 0.00004087
Iteration 208/1000 | Loss: 0.00004087
Iteration 209/1000 | Loss: 0.00004086
Iteration 210/1000 | Loss: 0.00004086
Iteration 211/1000 | Loss: 0.00004086
Iteration 212/1000 | Loss: 0.00004086
Iteration 213/1000 | Loss: 0.00004086
Iteration 214/1000 | Loss: 0.00004086
Iteration 215/1000 | Loss: 0.00004085
Iteration 216/1000 | Loss: 0.00004085
Iteration 217/1000 | Loss: 0.00004084
Iteration 218/1000 | Loss: 0.00004084
Iteration 219/1000 | Loss: 0.00004084
Iteration 220/1000 | Loss: 0.00004084
Iteration 221/1000 | Loss: 0.00004084
Iteration 222/1000 | Loss: 0.00004084
Iteration 223/1000 | Loss: 0.00004083
Iteration 224/1000 | Loss: 0.00004083
Iteration 225/1000 | Loss: 0.00004083
Iteration 226/1000 | Loss: 0.00004083
Iteration 227/1000 | Loss: 0.00004083
Iteration 228/1000 | Loss: 0.00004083
Iteration 229/1000 | Loss: 0.00004083
Iteration 230/1000 | Loss: 0.00004083
Iteration 231/1000 | Loss: 0.00004083
Iteration 232/1000 | Loss: 0.00004083
Iteration 233/1000 | Loss: 0.00004083
Iteration 234/1000 | Loss: 0.00004083
Iteration 235/1000 | Loss: 0.00004082
Iteration 236/1000 | Loss: 0.00004082
Iteration 237/1000 | Loss: 0.00004082
Iteration 238/1000 | Loss: 0.00004082
Iteration 239/1000 | Loss: 0.00004082
Iteration 240/1000 | Loss: 0.00004082
Iteration 241/1000 | Loss: 0.00004081
Iteration 242/1000 | Loss: 0.00004081
Iteration 243/1000 | Loss: 0.00004081
Iteration 244/1000 | Loss: 0.00004081
Iteration 245/1000 | Loss: 0.00004080
Iteration 246/1000 | Loss: 0.00004080
Iteration 247/1000 | Loss: 0.00004080
Iteration 248/1000 | Loss: 0.00004079
Iteration 249/1000 | Loss: 0.00004079
Iteration 250/1000 | Loss: 0.00004079
Iteration 251/1000 | Loss: 0.00004079
Iteration 252/1000 | Loss: 0.00004079
Iteration 253/1000 | Loss: 0.00004078
Iteration 254/1000 | Loss: 0.00004078
Iteration 255/1000 | Loss: 0.00004078
Iteration 256/1000 | Loss: 0.00004078
Iteration 257/1000 | Loss: 0.00004078
Iteration 258/1000 | Loss: 0.00004078
Iteration 259/1000 | Loss: 0.00004077
Iteration 260/1000 | Loss: 0.00004077
Iteration 261/1000 | Loss: 0.00004076
Iteration 262/1000 | Loss: 0.00004076
Iteration 263/1000 | Loss: 0.00004076
Iteration 264/1000 | Loss: 0.00004076
Iteration 265/1000 | Loss: 0.00004076
Iteration 266/1000 | Loss: 0.00004075
Iteration 267/1000 | Loss: 0.00004075
Iteration 268/1000 | Loss: 0.00004075
Iteration 269/1000 | Loss: 0.00004075
Iteration 270/1000 | Loss: 0.00004075
Iteration 271/1000 | Loss: 0.00004075
Iteration 272/1000 | Loss: 0.00004075
Iteration 273/1000 | Loss: 0.00004075
Iteration 274/1000 | Loss: 0.00004075
Iteration 275/1000 | Loss: 0.00004074
Iteration 276/1000 | Loss: 0.00004074
Iteration 277/1000 | Loss: 0.00004074
Iteration 278/1000 | Loss: 0.00004074
Iteration 279/1000 | Loss: 0.00004074
Iteration 280/1000 | Loss: 0.00004073
Iteration 281/1000 | Loss: 0.00004073
Iteration 282/1000 | Loss: 0.00004073
Iteration 283/1000 | Loss: 0.00004073
Iteration 284/1000 | Loss: 0.00004072
Iteration 285/1000 | Loss: 0.00004072
Iteration 286/1000 | Loss: 0.00004072
Iteration 287/1000 | Loss: 0.00004072
Iteration 288/1000 | Loss: 0.00004072
Iteration 289/1000 | Loss: 0.00004072
Iteration 290/1000 | Loss: 0.00004072
Iteration 291/1000 | Loss: 0.00004072
Iteration 292/1000 | Loss: 0.00004072
Iteration 293/1000 | Loss: 0.00004072
Iteration 294/1000 | Loss: 0.00004071
Iteration 295/1000 | Loss: 0.00004071
Iteration 296/1000 | Loss: 0.00004071
Iteration 297/1000 | Loss: 0.00004070
Iteration 298/1000 | Loss: 0.00004070
Iteration 299/1000 | Loss: 0.00004070
Iteration 300/1000 | Loss: 0.00004069
Iteration 301/1000 | Loss: 0.00004069
Iteration 302/1000 | Loss: 0.00004069
Iteration 303/1000 | Loss: 0.00004069
Iteration 304/1000 | Loss: 0.00004068
Iteration 305/1000 | Loss: 0.00004068
Iteration 306/1000 | Loss: 0.00004068
Iteration 307/1000 | Loss: 0.00004068
Iteration 308/1000 | Loss: 0.00004068
Iteration 309/1000 | Loss: 0.00004067
Iteration 310/1000 | Loss: 0.00004067
Iteration 311/1000 | Loss: 0.00004067
Iteration 312/1000 | Loss: 0.00004067
Iteration 313/1000 | Loss: 0.00004067
Iteration 314/1000 | Loss: 0.00004067
Iteration 315/1000 | Loss: 0.00004067
Iteration 316/1000 | Loss: 0.00004067
Iteration 317/1000 | Loss: 0.00004066
Iteration 318/1000 | Loss: 0.00004066
Iteration 319/1000 | Loss: 0.00004066
Iteration 320/1000 | Loss: 0.00004066
Iteration 321/1000 | Loss: 0.00004065
Iteration 322/1000 | Loss: 0.00004065
Iteration 323/1000 | Loss: 0.00004065
Iteration 324/1000 | Loss: 0.00004064
Iteration 325/1000 | Loss: 0.00004064
Iteration 326/1000 | Loss: 0.00004064
Iteration 327/1000 | Loss: 0.00004063
Iteration 328/1000 | Loss: 0.00004063
Iteration 329/1000 | Loss: 0.00004063
Iteration 330/1000 | Loss: 0.00004063
Iteration 331/1000 | Loss: 0.00004062
Iteration 332/1000 | Loss: 0.00004062
Iteration 333/1000 | Loss: 0.00004062
Iteration 334/1000 | Loss: 0.00004062
Iteration 335/1000 | Loss: 0.00004062
Iteration 336/1000 | Loss: 0.00004061
Iteration 337/1000 | Loss: 0.00004061
Iteration 338/1000 | Loss: 0.00004061
Iteration 339/1000 | Loss: 0.00004061
Iteration 340/1000 | Loss: 0.00004061
Iteration 341/1000 | Loss: 0.00004061
Iteration 342/1000 | Loss: 0.00004061
Iteration 343/1000 | Loss: 0.00004060
Iteration 344/1000 | Loss: 0.00004060
Iteration 345/1000 | Loss: 0.00004060
Iteration 346/1000 | Loss: 0.00004060
Iteration 347/1000 | Loss: 0.00004060
Iteration 348/1000 | Loss: 0.00004059
Iteration 349/1000 | Loss: 0.00004059
Iteration 350/1000 | Loss: 0.00004059
Iteration 351/1000 | Loss: 0.00004059
Iteration 352/1000 | Loss: 0.00004059
Iteration 353/1000 | Loss: 0.00004059
Iteration 354/1000 | Loss: 0.00004059
Iteration 355/1000 | Loss: 0.00004059
Iteration 356/1000 | Loss: 0.00004059
Iteration 357/1000 | Loss: 0.00004058
Iteration 358/1000 | Loss: 0.00004058
Iteration 359/1000 | Loss: 0.00004058
Iteration 360/1000 | Loss: 0.00004058
Iteration 361/1000 | Loss: 0.00004058
Iteration 362/1000 | Loss: 0.00004058
Iteration 363/1000 | Loss: 0.00004058
Iteration 364/1000 | Loss: 0.00004058
Iteration 365/1000 | Loss: 0.00004058
Iteration 366/1000 | Loss: 0.00004058
Iteration 367/1000 | Loss: 0.00004058
Iteration 368/1000 | Loss: 0.00004058
Iteration 369/1000 | Loss: 0.00004058
Iteration 370/1000 | Loss: 0.00004058
Iteration 371/1000 | Loss: 0.00004058
Iteration 372/1000 | Loss: 0.00004058
Iteration 373/1000 | Loss: 0.00004058
Iteration 374/1000 | Loss: 0.00004058
Iteration 375/1000 | Loss: 0.00004057
Iteration 376/1000 | Loss: 0.00004057
Iteration 377/1000 | Loss: 0.00004057
Iteration 378/1000 | Loss: 0.00004057
Iteration 379/1000 | Loss: 0.00004057
Iteration 380/1000 | Loss: 0.00004057
Iteration 381/1000 | Loss: 0.00004057
Iteration 382/1000 | Loss: 0.00004057
Iteration 383/1000 | Loss: 0.00004057
Iteration 384/1000 | Loss: 0.00004057
Iteration 385/1000 | Loss: 0.00004057
Iteration 386/1000 | Loss: 0.00004057
Iteration 387/1000 | Loss: 0.00004057
Iteration 388/1000 | Loss: 0.00004057
Iteration 389/1000 | Loss: 0.00004057
Iteration 390/1000 | Loss: 0.00004057
Iteration 391/1000 | Loss: 0.00004057
Iteration 392/1000 | Loss: 0.00004057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 392. Stopping optimization.
Last 5 losses: [4.057409751112573e-05, 4.057409751112573e-05, 4.057409751112573e-05, 4.057409751112573e-05, 4.057409751112573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.057409751112573e-05

Optimization complete. Final v2v error: 4.323445796966553 mm

Highest mean error: 13.078125953674316 mm for frame 62

Lowest mean error: 2.6680800914764404 mm for frame 102

Saving results

Total time: 291.676255941391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464394
Iteration 2/25 | Loss: 0.00137268
Iteration 3/25 | Loss: 0.00120209
Iteration 4/25 | Loss: 0.00118767
Iteration 5/25 | Loss: 0.00118546
Iteration 6/25 | Loss: 0.00118546
Iteration 7/25 | Loss: 0.00118546
Iteration 8/25 | Loss: 0.00118546
Iteration 9/25 | Loss: 0.00118546
Iteration 10/25 | Loss: 0.00118546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011854569893330336, 0.0011854569893330336, 0.0011854569893330336, 0.0011854569893330336, 0.0011854569893330336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011854569893330336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35232854
Iteration 2/25 | Loss: 0.00087744
Iteration 3/25 | Loss: 0.00087743
Iteration 4/25 | Loss: 0.00087743
Iteration 5/25 | Loss: 0.00087743
Iteration 6/25 | Loss: 0.00087743
Iteration 7/25 | Loss: 0.00087742
Iteration 8/25 | Loss: 0.00087742
Iteration 9/25 | Loss: 0.00087742
Iteration 10/25 | Loss: 0.00087742
Iteration 11/25 | Loss: 0.00087742
Iteration 12/25 | Loss: 0.00087742
Iteration 13/25 | Loss: 0.00087742
Iteration 14/25 | Loss: 0.00087742
Iteration 15/25 | Loss: 0.00087742
Iteration 16/25 | Loss: 0.00087742
Iteration 17/25 | Loss: 0.00087742
Iteration 18/25 | Loss: 0.00087742
Iteration 19/25 | Loss: 0.00087742
Iteration 20/25 | Loss: 0.00087742
Iteration 21/25 | Loss: 0.00087742
Iteration 22/25 | Loss: 0.00087742
Iteration 23/25 | Loss: 0.00087742
Iteration 24/25 | Loss: 0.00087742
Iteration 25/25 | Loss: 0.00087742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087742
Iteration 2/1000 | Loss: 0.00004113
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00002319
Iteration 5/1000 | Loss: 0.00002187
Iteration 6/1000 | Loss: 0.00002089
Iteration 7/1000 | Loss: 0.00002024
Iteration 8/1000 | Loss: 0.00001968
Iteration 9/1000 | Loss: 0.00001915
Iteration 10/1000 | Loss: 0.00001877
Iteration 11/1000 | Loss: 0.00001854
Iteration 12/1000 | Loss: 0.00001841
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001811
Iteration 17/1000 | Loss: 0.00001807
Iteration 18/1000 | Loss: 0.00001806
Iteration 19/1000 | Loss: 0.00001805
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001803
Iteration 22/1000 | Loss: 0.00001803
Iteration 23/1000 | Loss: 0.00001802
Iteration 24/1000 | Loss: 0.00001802
Iteration 25/1000 | Loss: 0.00001797
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001794
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001791
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001786
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001784
Iteration 53/1000 | Loss: 0.00001784
Iteration 54/1000 | Loss: 0.00001784
Iteration 55/1000 | Loss: 0.00001784
Iteration 56/1000 | Loss: 0.00001784
Iteration 57/1000 | Loss: 0.00001784
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00001783
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001775
Iteration 86/1000 | Loss: 0.00001775
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001774
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001773
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001772
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001770
Iteration 111/1000 | Loss: 0.00001770
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001770
Iteration 118/1000 | Loss: 0.00001770
Iteration 119/1000 | Loss: 0.00001770
Iteration 120/1000 | Loss: 0.00001769
Iteration 121/1000 | Loss: 0.00001769
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001769
Iteration 124/1000 | Loss: 0.00001769
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001767
Iteration 135/1000 | Loss: 0.00001767
Iteration 136/1000 | Loss: 0.00001767
Iteration 137/1000 | Loss: 0.00001767
Iteration 138/1000 | Loss: 0.00001767
Iteration 139/1000 | Loss: 0.00001767
Iteration 140/1000 | Loss: 0.00001767
Iteration 141/1000 | Loss: 0.00001767
Iteration 142/1000 | Loss: 0.00001767
Iteration 143/1000 | Loss: 0.00001767
Iteration 144/1000 | Loss: 0.00001767
Iteration 145/1000 | Loss: 0.00001767
Iteration 146/1000 | Loss: 0.00001767
Iteration 147/1000 | Loss: 0.00001767
Iteration 148/1000 | Loss: 0.00001767
Iteration 149/1000 | Loss: 0.00001767
Iteration 150/1000 | Loss: 0.00001767
Iteration 151/1000 | Loss: 0.00001767
Iteration 152/1000 | Loss: 0.00001767
Iteration 153/1000 | Loss: 0.00001767
Iteration 154/1000 | Loss: 0.00001767
Iteration 155/1000 | Loss: 0.00001767
Iteration 156/1000 | Loss: 0.00001767
Iteration 157/1000 | Loss: 0.00001767
Iteration 158/1000 | Loss: 0.00001767
Iteration 159/1000 | Loss: 0.00001767
Iteration 160/1000 | Loss: 0.00001767
Iteration 161/1000 | Loss: 0.00001767
Iteration 162/1000 | Loss: 0.00001767
Iteration 163/1000 | Loss: 0.00001767
Iteration 164/1000 | Loss: 0.00001767
Iteration 165/1000 | Loss: 0.00001767
Iteration 166/1000 | Loss: 0.00001767
Iteration 167/1000 | Loss: 0.00001767
Iteration 168/1000 | Loss: 0.00001767
Iteration 169/1000 | Loss: 0.00001767
Iteration 170/1000 | Loss: 0.00001767
Iteration 171/1000 | Loss: 0.00001767
Iteration 172/1000 | Loss: 0.00001767
Iteration 173/1000 | Loss: 0.00001767
Iteration 174/1000 | Loss: 0.00001767
Iteration 175/1000 | Loss: 0.00001767
Iteration 176/1000 | Loss: 0.00001767
Iteration 177/1000 | Loss: 0.00001767
Iteration 178/1000 | Loss: 0.00001767
Iteration 179/1000 | Loss: 0.00001767
Iteration 180/1000 | Loss: 0.00001767
Iteration 181/1000 | Loss: 0.00001767
Iteration 182/1000 | Loss: 0.00001767
Iteration 183/1000 | Loss: 0.00001767
Iteration 184/1000 | Loss: 0.00001767
Iteration 185/1000 | Loss: 0.00001767
Iteration 186/1000 | Loss: 0.00001767
Iteration 187/1000 | Loss: 0.00001767
Iteration 188/1000 | Loss: 0.00001767
Iteration 189/1000 | Loss: 0.00001767
Iteration 190/1000 | Loss: 0.00001767
Iteration 191/1000 | Loss: 0.00001767
Iteration 192/1000 | Loss: 0.00001767
Iteration 193/1000 | Loss: 0.00001767
Iteration 194/1000 | Loss: 0.00001767
Iteration 195/1000 | Loss: 0.00001767
Iteration 196/1000 | Loss: 0.00001767
Iteration 197/1000 | Loss: 0.00001767
Iteration 198/1000 | Loss: 0.00001767
Iteration 199/1000 | Loss: 0.00001767
Iteration 200/1000 | Loss: 0.00001767
Iteration 201/1000 | Loss: 0.00001767
Iteration 202/1000 | Loss: 0.00001767
Iteration 203/1000 | Loss: 0.00001767
Iteration 204/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.766543573467061e-05, 1.766543573467061e-05, 1.766543573467061e-05, 1.766543573467061e-05, 1.766543573467061e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.766543573467061e-05

Optimization complete. Final v2v error: 3.509186267852783 mm

Highest mean error: 3.817202568054199 mm for frame 173

Lowest mean error: 3.0938501358032227 mm for frame 158

Saving results

Total time: 45.62338638305664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854048
Iteration 2/25 | Loss: 0.00115295
Iteration 3/25 | Loss: 0.00108749
Iteration 4/25 | Loss: 0.00108142
Iteration 5/25 | Loss: 0.00107936
Iteration 6/25 | Loss: 0.00107918
Iteration 7/25 | Loss: 0.00107918
Iteration 8/25 | Loss: 0.00107918
Iteration 9/25 | Loss: 0.00107918
Iteration 10/25 | Loss: 0.00107918
Iteration 11/25 | Loss: 0.00107918
Iteration 12/25 | Loss: 0.00107918
Iteration 13/25 | Loss: 0.00107918
Iteration 14/25 | Loss: 0.00107918
Iteration 15/25 | Loss: 0.00107918
Iteration 16/25 | Loss: 0.00107918
Iteration 17/25 | Loss: 0.00107918
Iteration 18/25 | Loss: 0.00107918
Iteration 19/25 | Loss: 0.00107918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010791822569444776, 0.0010791822569444776, 0.0010791822569444776, 0.0010791822569444776, 0.0010791822569444776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010791822569444776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99855065
Iteration 2/25 | Loss: 0.00087238
Iteration 3/25 | Loss: 0.00087238
Iteration 4/25 | Loss: 0.00087238
Iteration 5/25 | Loss: 0.00087238
Iteration 6/25 | Loss: 0.00087238
Iteration 7/25 | Loss: 0.00087238
Iteration 8/25 | Loss: 0.00087238
Iteration 9/25 | Loss: 0.00087238
Iteration 10/25 | Loss: 0.00087238
Iteration 11/25 | Loss: 0.00087238
Iteration 12/25 | Loss: 0.00087238
Iteration 13/25 | Loss: 0.00087238
Iteration 14/25 | Loss: 0.00087238
Iteration 15/25 | Loss: 0.00087238
Iteration 16/25 | Loss: 0.00087238
Iteration 17/25 | Loss: 0.00087238
Iteration 18/25 | Loss: 0.00087238
Iteration 19/25 | Loss: 0.00087238
Iteration 20/25 | Loss: 0.00087238
Iteration 21/25 | Loss: 0.00087238
Iteration 22/25 | Loss: 0.00087238
Iteration 23/25 | Loss: 0.00087238
Iteration 24/25 | Loss: 0.00087238
Iteration 25/25 | Loss: 0.00087238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087238
Iteration 2/1000 | Loss: 0.00002497
Iteration 3/1000 | Loss: 0.00001541
Iteration 4/1000 | Loss: 0.00001263
Iteration 5/1000 | Loss: 0.00001184
Iteration 6/1000 | Loss: 0.00001136
Iteration 7/1000 | Loss: 0.00001095
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001050
Iteration 10/1000 | Loss: 0.00001047
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001011
Iteration 14/1000 | Loss: 0.00001010
Iteration 15/1000 | Loss: 0.00001001
Iteration 16/1000 | Loss: 0.00001001
Iteration 17/1000 | Loss: 0.00001001
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00000997
Iteration 20/1000 | Loss: 0.00000995
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000987
Iteration 24/1000 | Loss: 0.00000987
Iteration 25/1000 | Loss: 0.00000983
Iteration 26/1000 | Loss: 0.00000982
Iteration 27/1000 | Loss: 0.00000982
Iteration 28/1000 | Loss: 0.00000979
Iteration 29/1000 | Loss: 0.00000979
Iteration 30/1000 | Loss: 0.00000978
Iteration 31/1000 | Loss: 0.00000977
Iteration 32/1000 | Loss: 0.00000977
Iteration 33/1000 | Loss: 0.00000976
Iteration 34/1000 | Loss: 0.00000976
Iteration 35/1000 | Loss: 0.00000976
Iteration 36/1000 | Loss: 0.00000976
Iteration 37/1000 | Loss: 0.00000976
Iteration 38/1000 | Loss: 0.00000975
Iteration 39/1000 | Loss: 0.00000975
Iteration 40/1000 | Loss: 0.00000975
Iteration 41/1000 | Loss: 0.00000975
Iteration 42/1000 | Loss: 0.00000975
Iteration 43/1000 | Loss: 0.00000974
Iteration 44/1000 | Loss: 0.00000973
Iteration 45/1000 | Loss: 0.00000971
Iteration 46/1000 | Loss: 0.00000971
Iteration 47/1000 | Loss: 0.00000971
Iteration 48/1000 | Loss: 0.00000971
Iteration 49/1000 | Loss: 0.00000971
Iteration 50/1000 | Loss: 0.00000971
Iteration 51/1000 | Loss: 0.00000971
Iteration 52/1000 | Loss: 0.00000970
Iteration 53/1000 | Loss: 0.00000970
Iteration 54/1000 | Loss: 0.00000970
Iteration 55/1000 | Loss: 0.00000969
Iteration 56/1000 | Loss: 0.00000968
Iteration 57/1000 | Loss: 0.00000968
Iteration 58/1000 | Loss: 0.00000968
Iteration 59/1000 | Loss: 0.00000967
Iteration 60/1000 | Loss: 0.00000967
Iteration 61/1000 | Loss: 0.00000967
Iteration 62/1000 | Loss: 0.00000967
Iteration 63/1000 | Loss: 0.00000966
Iteration 64/1000 | Loss: 0.00000966
Iteration 65/1000 | Loss: 0.00000966
Iteration 66/1000 | Loss: 0.00000966
Iteration 67/1000 | Loss: 0.00000966
Iteration 68/1000 | Loss: 0.00000966
Iteration 69/1000 | Loss: 0.00000965
Iteration 70/1000 | Loss: 0.00000965
Iteration 71/1000 | Loss: 0.00000965
Iteration 72/1000 | Loss: 0.00000964
Iteration 73/1000 | Loss: 0.00000964
Iteration 74/1000 | Loss: 0.00000964
Iteration 75/1000 | Loss: 0.00000964
Iteration 76/1000 | Loss: 0.00000963
Iteration 77/1000 | Loss: 0.00000963
Iteration 78/1000 | Loss: 0.00000963
Iteration 79/1000 | Loss: 0.00000963
Iteration 80/1000 | Loss: 0.00000963
Iteration 81/1000 | Loss: 0.00000963
Iteration 82/1000 | Loss: 0.00000962
Iteration 83/1000 | Loss: 0.00000962
Iteration 84/1000 | Loss: 0.00000962
Iteration 85/1000 | Loss: 0.00000962
Iteration 86/1000 | Loss: 0.00000962
Iteration 87/1000 | Loss: 0.00000962
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000962
Iteration 90/1000 | Loss: 0.00000962
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000961
Iteration 93/1000 | Loss: 0.00000961
Iteration 94/1000 | Loss: 0.00000961
Iteration 95/1000 | Loss: 0.00000961
Iteration 96/1000 | Loss: 0.00000961
Iteration 97/1000 | Loss: 0.00000961
Iteration 98/1000 | Loss: 0.00000961
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000961
Iteration 105/1000 | Loss: 0.00000961
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Iteration 109/1000 | Loss: 0.00000960
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000958
Iteration 118/1000 | Loss: 0.00000958
Iteration 119/1000 | Loss: 0.00000958
Iteration 120/1000 | Loss: 0.00000957
Iteration 121/1000 | Loss: 0.00000957
Iteration 122/1000 | Loss: 0.00000957
Iteration 123/1000 | Loss: 0.00000956
Iteration 124/1000 | Loss: 0.00000956
Iteration 125/1000 | Loss: 0.00000956
Iteration 126/1000 | Loss: 0.00000956
Iteration 127/1000 | Loss: 0.00000955
Iteration 128/1000 | Loss: 0.00000955
Iteration 129/1000 | Loss: 0.00000955
Iteration 130/1000 | Loss: 0.00000955
Iteration 131/1000 | Loss: 0.00000955
Iteration 132/1000 | Loss: 0.00000954
Iteration 133/1000 | Loss: 0.00000954
Iteration 134/1000 | Loss: 0.00000954
Iteration 135/1000 | Loss: 0.00000953
Iteration 136/1000 | Loss: 0.00000953
Iteration 137/1000 | Loss: 0.00000953
Iteration 138/1000 | Loss: 0.00000953
Iteration 139/1000 | Loss: 0.00000952
Iteration 140/1000 | Loss: 0.00000952
Iteration 141/1000 | Loss: 0.00000952
Iteration 142/1000 | Loss: 0.00000952
Iteration 143/1000 | Loss: 0.00000952
Iteration 144/1000 | Loss: 0.00000952
Iteration 145/1000 | Loss: 0.00000952
Iteration 146/1000 | Loss: 0.00000951
Iteration 147/1000 | Loss: 0.00000951
Iteration 148/1000 | Loss: 0.00000951
Iteration 149/1000 | Loss: 0.00000951
Iteration 150/1000 | Loss: 0.00000951
Iteration 151/1000 | Loss: 0.00000950
Iteration 152/1000 | Loss: 0.00000950
Iteration 153/1000 | Loss: 0.00000950
Iteration 154/1000 | Loss: 0.00000950
Iteration 155/1000 | Loss: 0.00000950
Iteration 156/1000 | Loss: 0.00000950
Iteration 157/1000 | Loss: 0.00000950
Iteration 158/1000 | Loss: 0.00000950
Iteration 159/1000 | Loss: 0.00000950
Iteration 160/1000 | Loss: 0.00000950
Iteration 161/1000 | Loss: 0.00000950
Iteration 162/1000 | Loss: 0.00000949
Iteration 163/1000 | Loss: 0.00000949
Iteration 164/1000 | Loss: 0.00000949
Iteration 165/1000 | Loss: 0.00000949
Iteration 166/1000 | Loss: 0.00000949
Iteration 167/1000 | Loss: 0.00000949
Iteration 168/1000 | Loss: 0.00000949
Iteration 169/1000 | Loss: 0.00000949
Iteration 170/1000 | Loss: 0.00000949
Iteration 171/1000 | Loss: 0.00000949
Iteration 172/1000 | Loss: 0.00000949
Iteration 173/1000 | Loss: 0.00000949
Iteration 174/1000 | Loss: 0.00000949
Iteration 175/1000 | Loss: 0.00000949
Iteration 176/1000 | Loss: 0.00000949
Iteration 177/1000 | Loss: 0.00000949
Iteration 178/1000 | Loss: 0.00000949
Iteration 179/1000 | Loss: 0.00000948
Iteration 180/1000 | Loss: 0.00000948
Iteration 181/1000 | Loss: 0.00000948
Iteration 182/1000 | Loss: 0.00000948
Iteration 183/1000 | Loss: 0.00000948
Iteration 184/1000 | Loss: 0.00000948
Iteration 185/1000 | Loss: 0.00000948
Iteration 186/1000 | Loss: 0.00000948
Iteration 187/1000 | Loss: 0.00000948
Iteration 188/1000 | Loss: 0.00000948
Iteration 189/1000 | Loss: 0.00000948
Iteration 190/1000 | Loss: 0.00000948
Iteration 191/1000 | Loss: 0.00000948
Iteration 192/1000 | Loss: 0.00000947
Iteration 193/1000 | Loss: 0.00000947
Iteration 194/1000 | Loss: 0.00000947
Iteration 195/1000 | Loss: 0.00000947
Iteration 196/1000 | Loss: 0.00000947
Iteration 197/1000 | Loss: 0.00000947
Iteration 198/1000 | Loss: 0.00000947
Iteration 199/1000 | Loss: 0.00000947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [9.47436910792021e-06, 9.47436910792021e-06, 9.47436910792021e-06, 9.47436910792021e-06, 9.47436910792021e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.47436910792021e-06

Optimization complete. Final v2v error: 2.5981969833374023 mm

Highest mean error: 3.2662134170532227 mm for frame 52

Lowest mean error: 2.3456668853759766 mm for frame 99

Saving results

Total time: 37.917685985565186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009857
Iteration 2/25 | Loss: 0.00199349
Iteration 3/25 | Loss: 0.00165465
Iteration 4/25 | Loss: 0.00146088
Iteration 5/25 | Loss: 0.00143702
Iteration 6/25 | Loss: 0.00138319
Iteration 7/25 | Loss: 0.00127736
Iteration 8/25 | Loss: 0.00123531
Iteration 9/25 | Loss: 0.00122487
Iteration 10/25 | Loss: 0.00121468
Iteration 11/25 | Loss: 0.00120148
Iteration 12/25 | Loss: 0.00118646
Iteration 13/25 | Loss: 0.00117415
Iteration 14/25 | Loss: 0.00116984
Iteration 15/25 | Loss: 0.00116900
Iteration 16/25 | Loss: 0.00116842
Iteration 17/25 | Loss: 0.00116775
Iteration 18/25 | Loss: 0.00116750
Iteration 19/25 | Loss: 0.00116739
Iteration 20/25 | Loss: 0.00116721
Iteration 21/25 | Loss: 0.00116781
Iteration 22/25 | Loss: 0.00116534
Iteration 23/25 | Loss: 0.00116510
Iteration 24/25 | Loss: 0.00116500
Iteration 25/25 | Loss: 0.00116499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41356599
Iteration 2/25 | Loss: 0.00107787
Iteration 3/25 | Loss: 0.00096589
Iteration 4/25 | Loss: 0.00096589
Iteration 5/25 | Loss: 0.00096589
Iteration 6/25 | Loss: 0.00096589
Iteration 7/25 | Loss: 0.00096589
Iteration 8/25 | Loss: 0.00096589
Iteration 9/25 | Loss: 0.00096589
Iteration 10/25 | Loss: 0.00096589
Iteration 11/25 | Loss: 0.00096589
Iteration 12/25 | Loss: 0.00096589
Iteration 13/25 | Loss: 0.00096589
Iteration 14/25 | Loss: 0.00096589
Iteration 15/25 | Loss: 0.00096589
Iteration 16/25 | Loss: 0.00096589
Iteration 17/25 | Loss: 0.00096589
Iteration 18/25 | Loss: 0.00096589
Iteration 19/25 | Loss: 0.00096589
Iteration 20/25 | Loss: 0.00096589
Iteration 21/25 | Loss: 0.00096589
Iteration 22/25 | Loss: 0.00096589
Iteration 23/25 | Loss: 0.00096589
Iteration 24/25 | Loss: 0.00096589
Iteration 25/25 | Loss: 0.00096589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096589
Iteration 2/1000 | Loss: 0.00016256
Iteration 3/1000 | Loss: 0.00001928
Iteration 4/1000 | Loss: 0.00009857
Iteration 5/1000 | Loss: 0.00001536
Iteration 6/1000 | Loss: 0.00009632
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00016982
Iteration 9/1000 | Loss: 0.00001413
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00043837
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001436
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001314
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001296
Iteration 19/1000 | Loss: 0.00001286
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001280
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001279
Iteration 26/1000 | Loss: 0.00001278
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001276
Iteration 29/1000 | Loss: 0.00001274
Iteration 30/1000 | Loss: 0.00001273
Iteration 31/1000 | Loss: 0.00001272
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001269
Iteration 35/1000 | Loss: 0.00001268
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001266
Iteration 40/1000 | Loss: 0.00001266
Iteration 41/1000 | Loss: 0.00001266
Iteration 42/1000 | Loss: 0.00001265
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001262
Iteration 55/1000 | Loss: 0.00001262
Iteration 56/1000 | Loss: 0.00001262
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001260
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001260
Iteration 69/1000 | Loss: 0.00001260
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001259
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001259
Iteration 78/1000 | Loss: 0.00001259
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001258
Iteration 81/1000 | Loss: 0.00001258
Iteration 82/1000 | Loss: 0.00001258
Iteration 83/1000 | Loss: 0.00001258
Iteration 84/1000 | Loss: 0.00001258
Iteration 85/1000 | Loss: 0.00001258
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001257
Iteration 96/1000 | Loss: 0.00001257
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001256
Iteration 101/1000 | Loss: 0.00001256
Iteration 102/1000 | Loss: 0.00001256
Iteration 103/1000 | Loss: 0.00001256
Iteration 104/1000 | Loss: 0.00001256
Iteration 105/1000 | Loss: 0.00001256
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001255
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001255
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001255
Iteration 113/1000 | Loss: 0.00001255
Iteration 114/1000 | Loss: 0.00001255
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001253
Iteration 136/1000 | Loss: 0.00001253
Iteration 137/1000 | Loss: 0.00001253
Iteration 138/1000 | Loss: 0.00001253
Iteration 139/1000 | Loss: 0.00001253
Iteration 140/1000 | Loss: 0.00001253
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001253
Iteration 145/1000 | Loss: 0.00001253
Iteration 146/1000 | Loss: 0.00001252
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001252
Iteration 149/1000 | Loss: 0.00001252
Iteration 150/1000 | Loss: 0.00001252
Iteration 151/1000 | Loss: 0.00001252
Iteration 152/1000 | Loss: 0.00001252
Iteration 153/1000 | Loss: 0.00001252
Iteration 154/1000 | Loss: 0.00001252
Iteration 155/1000 | Loss: 0.00001252
Iteration 156/1000 | Loss: 0.00001252
Iteration 157/1000 | Loss: 0.00001252
Iteration 158/1000 | Loss: 0.00001251
Iteration 159/1000 | Loss: 0.00001251
Iteration 160/1000 | Loss: 0.00001251
Iteration 161/1000 | Loss: 0.00001251
Iteration 162/1000 | Loss: 0.00001251
Iteration 163/1000 | Loss: 0.00001251
Iteration 164/1000 | Loss: 0.00001251
Iteration 165/1000 | Loss: 0.00001251
Iteration 166/1000 | Loss: 0.00001251
Iteration 167/1000 | Loss: 0.00001251
Iteration 168/1000 | Loss: 0.00001251
Iteration 169/1000 | Loss: 0.00001251
Iteration 170/1000 | Loss: 0.00001251
Iteration 171/1000 | Loss: 0.00001251
Iteration 172/1000 | Loss: 0.00001251
Iteration 173/1000 | Loss: 0.00001250
Iteration 174/1000 | Loss: 0.00001250
Iteration 175/1000 | Loss: 0.00001250
Iteration 176/1000 | Loss: 0.00001250
Iteration 177/1000 | Loss: 0.00001250
Iteration 178/1000 | Loss: 0.00001250
Iteration 179/1000 | Loss: 0.00001250
Iteration 180/1000 | Loss: 0.00001250
Iteration 181/1000 | Loss: 0.00001250
Iteration 182/1000 | Loss: 0.00001250
Iteration 183/1000 | Loss: 0.00001250
Iteration 184/1000 | Loss: 0.00001250
Iteration 185/1000 | Loss: 0.00001250
Iteration 186/1000 | Loss: 0.00001250
Iteration 187/1000 | Loss: 0.00001250
Iteration 188/1000 | Loss: 0.00001250
Iteration 189/1000 | Loss: 0.00001250
Iteration 190/1000 | Loss: 0.00001250
Iteration 191/1000 | Loss: 0.00001250
Iteration 192/1000 | Loss: 0.00001250
Iteration 193/1000 | Loss: 0.00001250
Iteration 194/1000 | Loss: 0.00001250
Iteration 195/1000 | Loss: 0.00001250
Iteration 196/1000 | Loss: 0.00001250
Iteration 197/1000 | Loss: 0.00001250
Iteration 198/1000 | Loss: 0.00001250
Iteration 199/1000 | Loss: 0.00001250
Iteration 200/1000 | Loss: 0.00001250
Iteration 201/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.2501512173912488e-05, 1.2501512173912488e-05, 1.2501512173912488e-05, 1.2501512173912488e-05, 1.2501512173912488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2501512173912488e-05

Optimization complete. Final v2v error: 2.94523549079895 mm

Highest mean error: 8.526738166809082 mm for frame 109

Lowest mean error: 2.625140428543091 mm for frame 26

Saving results

Total time: 79.81634044647217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819531
Iteration 2/25 | Loss: 0.00125158
Iteration 3/25 | Loss: 0.00113243
Iteration 4/25 | Loss: 0.00110693
Iteration 5/25 | Loss: 0.00109711
Iteration 6/25 | Loss: 0.00109478
Iteration 7/25 | Loss: 0.00109409
Iteration 8/25 | Loss: 0.00109407
Iteration 9/25 | Loss: 0.00109407
Iteration 10/25 | Loss: 0.00109407
Iteration 11/25 | Loss: 0.00109407
Iteration 12/25 | Loss: 0.00109407
Iteration 13/25 | Loss: 0.00109407
Iteration 14/25 | Loss: 0.00109407
Iteration 15/25 | Loss: 0.00109407
Iteration 16/25 | Loss: 0.00109407
Iteration 17/25 | Loss: 0.00109407
Iteration 18/25 | Loss: 0.00109407
Iteration 19/25 | Loss: 0.00109407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001094066770747304, 0.001094066770747304, 0.001094066770747304, 0.001094066770747304, 0.001094066770747304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001094066770747304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51998162
Iteration 2/25 | Loss: 0.00102576
Iteration 3/25 | Loss: 0.00102576
Iteration 4/25 | Loss: 0.00102576
Iteration 5/25 | Loss: 0.00102576
Iteration 6/25 | Loss: 0.00102576
Iteration 7/25 | Loss: 0.00102576
Iteration 8/25 | Loss: 0.00102576
Iteration 9/25 | Loss: 0.00102576
Iteration 10/25 | Loss: 0.00102576
Iteration 11/25 | Loss: 0.00102576
Iteration 12/25 | Loss: 0.00102576
Iteration 13/25 | Loss: 0.00102576
Iteration 14/25 | Loss: 0.00102576
Iteration 15/25 | Loss: 0.00102576
Iteration 16/25 | Loss: 0.00102576
Iteration 17/25 | Loss: 0.00102576
Iteration 18/25 | Loss: 0.00102576
Iteration 19/25 | Loss: 0.00102576
Iteration 20/25 | Loss: 0.00102576
Iteration 21/25 | Loss: 0.00102576
Iteration 22/25 | Loss: 0.00102576
Iteration 23/25 | Loss: 0.00102576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001025759382173419, 0.001025759382173419, 0.001025759382173419, 0.001025759382173419, 0.001025759382173419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001025759382173419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102576
Iteration 2/1000 | Loss: 0.00005420
Iteration 3/1000 | Loss: 0.00003889
Iteration 4/1000 | Loss: 0.00002883
Iteration 5/1000 | Loss: 0.00002636
Iteration 6/1000 | Loss: 0.00002484
Iteration 7/1000 | Loss: 0.00002362
Iteration 8/1000 | Loss: 0.00002289
Iteration 9/1000 | Loss: 0.00002239
Iteration 10/1000 | Loss: 0.00002207
Iteration 11/1000 | Loss: 0.00002183
Iteration 12/1000 | Loss: 0.00002157
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002129
Iteration 15/1000 | Loss: 0.00002114
Iteration 16/1000 | Loss: 0.00002111
Iteration 17/1000 | Loss: 0.00002103
Iteration 18/1000 | Loss: 0.00002099
Iteration 19/1000 | Loss: 0.00002097
Iteration 20/1000 | Loss: 0.00002094
Iteration 21/1000 | Loss: 0.00002094
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002091
Iteration 24/1000 | Loss: 0.00002087
Iteration 25/1000 | Loss: 0.00002084
Iteration 26/1000 | Loss: 0.00002079
Iteration 27/1000 | Loss: 0.00002079
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002077
Iteration 30/1000 | Loss: 0.00002076
Iteration 31/1000 | Loss: 0.00002076
Iteration 32/1000 | Loss: 0.00002075
Iteration 33/1000 | Loss: 0.00002075
Iteration 34/1000 | Loss: 0.00002071
Iteration 35/1000 | Loss: 0.00002071
Iteration 36/1000 | Loss: 0.00002071
Iteration 37/1000 | Loss: 0.00002071
Iteration 38/1000 | Loss: 0.00002070
Iteration 39/1000 | Loss: 0.00002070
Iteration 40/1000 | Loss: 0.00002070
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002069
Iteration 43/1000 | Loss: 0.00002069
Iteration 44/1000 | Loss: 0.00002068
Iteration 45/1000 | Loss: 0.00002067
Iteration 46/1000 | Loss: 0.00002067
Iteration 47/1000 | Loss: 0.00002067
Iteration 48/1000 | Loss: 0.00002066
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002066
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002065
Iteration 54/1000 | Loss: 0.00002065
Iteration 55/1000 | Loss: 0.00002065
Iteration 56/1000 | Loss: 0.00002064
Iteration 57/1000 | Loss: 0.00002064
Iteration 58/1000 | Loss: 0.00002064
Iteration 59/1000 | Loss: 0.00002064
Iteration 60/1000 | Loss: 0.00002063
Iteration 61/1000 | Loss: 0.00002063
Iteration 62/1000 | Loss: 0.00002063
Iteration 63/1000 | Loss: 0.00002062
Iteration 64/1000 | Loss: 0.00002062
Iteration 65/1000 | Loss: 0.00002062
Iteration 66/1000 | Loss: 0.00002061
Iteration 67/1000 | Loss: 0.00002061
Iteration 68/1000 | Loss: 0.00002061
Iteration 69/1000 | Loss: 0.00002061
Iteration 70/1000 | Loss: 0.00002061
Iteration 71/1000 | Loss: 0.00002061
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002061
Iteration 74/1000 | Loss: 0.00002060
Iteration 75/1000 | Loss: 0.00002060
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002059
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002059
Iteration 90/1000 | Loss: 0.00002059
Iteration 91/1000 | Loss: 0.00002059
Iteration 92/1000 | Loss: 0.00002059
Iteration 93/1000 | Loss: 0.00002059
Iteration 94/1000 | Loss: 0.00002058
Iteration 95/1000 | Loss: 0.00002058
Iteration 96/1000 | Loss: 0.00002058
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002058
Iteration 99/1000 | Loss: 0.00002058
Iteration 100/1000 | Loss: 0.00002057
Iteration 101/1000 | Loss: 0.00002057
Iteration 102/1000 | Loss: 0.00002057
Iteration 103/1000 | Loss: 0.00002057
Iteration 104/1000 | Loss: 0.00002057
Iteration 105/1000 | Loss: 0.00002057
Iteration 106/1000 | Loss: 0.00002056
Iteration 107/1000 | Loss: 0.00002056
Iteration 108/1000 | Loss: 0.00002056
Iteration 109/1000 | Loss: 0.00002056
Iteration 110/1000 | Loss: 0.00002056
Iteration 111/1000 | Loss: 0.00002055
Iteration 112/1000 | Loss: 0.00002055
Iteration 113/1000 | Loss: 0.00002055
Iteration 114/1000 | Loss: 0.00002055
Iteration 115/1000 | Loss: 0.00002055
Iteration 116/1000 | Loss: 0.00002055
Iteration 117/1000 | Loss: 0.00002055
Iteration 118/1000 | Loss: 0.00002055
Iteration 119/1000 | Loss: 0.00002055
Iteration 120/1000 | Loss: 0.00002055
Iteration 121/1000 | Loss: 0.00002054
Iteration 122/1000 | Loss: 0.00002054
Iteration 123/1000 | Loss: 0.00002054
Iteration 124/1000 | Loss: 0.00002054
Iteration 125/1000 | Loss: 0.00002054
Iteration 126/1000 | Loss: 0.00002054
Iteration 127/1000 | Loss: 0.00002054
Iteration 128/1000 | Loss: 0.00002054
Iteration 129/1000 | Loss: 0.00002054
Iteration 130/1000 | Loss: 0.00002054
Iteration 131/1000 | Loss: 0.00002054
Iteration 132/1000 | Loss: 0.00002054
Iteration 133/1000 | Loss: 0.00002054
Iteration 134/1000 | Loss: 0.00002053
Iteration 135/1000 | Loss: 0.00002053
Iteration 136/1000 | Loss: 0.00002053
Iteration 137/1000 | Loss: 0.00002053
Iteration 138/1000 | Loss: 0.00002053
Iteration 139/1000 | Loss: 0.00002052
Iteration 140/1000 | Loss: 0.00002052
Iteration 141/1000 | Loss: 0.00002052
Iteration 142/1000 | Loss: 0.00002052
Iteration 143/1000 | Loss: 0.00002052
Iteration 144/1000 | Loss: 0.00002052
Iteration 145/1000 | Loss: 0.00002051
Iteration 146/1000 | Loss: 0.00002051
Iteration 147/1000 | Loss: 0.00002051
Iteration 148/1000 | Loss: 0.00002051
Iteration 149/1000 | Loss: 0.00002051
Iteration 150/1000 | Loss: 0.00002051
Iteration 151/1000 | Loss: 0.00002051
Iteration 152/1000 | Loss: 0.00002051
Iteration 153/1000 | Loss: 0.00002051
Iteration 154/1000 | Loss: 0.00002051
Iteration 155/1000 | Loss: 0.00002051
Iteration 156/1000 | Loss: 0.00002050
Iteration 157/1000 | Loss: 0.00002050
Iteration 158/1000 | Loss: 0.00002050
Iteration 159/1000 | Loss: 0.00002050
Iteration 160/1000 | Loss: 0.00002050
Iteration 161/1000 | Loss: 0.00002050
Iteration 162/1000 | Loss: 0.00002050
Iteration 163/1000 | Loss: 0.00002050
Iteration 164/1000 | Loss: 0.00002050
Iteration 165/1000 | Loss: 0.00002050
Iteration 166/1000 | Loss: 0.00002049
Iteration 167/1000 | Loss: 0.00002049
Iteration 168/1000 | Loss: 0.00002049
Iteration 169/1000 | Loss: 0.00002049
Iteration 170/1000 | Loss: 0.00002049
Iteration 171/1000 | Loss: 0.00002049
Iteration 172/1000 | Loss: 0.00002049
Iteration 173/1000 | Loss: 0.00002049
Iteration 174/1000 | Loss: 0.00002049
Iteration 175/1000 | Loss: 0.00002049
Iteration 176/1000 | Loss: 0.00002049
Iteration 177/1000 | Loss: 0.00002049
Iteration 178/1000 | Loss: 0.00002049
Iteration 179/1000 | Loss: 0.00002048
Iteration 180/1000 | Loss: 0.00002048
Iteration 181/1000 | Loss: 0.00002048
Iteration 182/1000 | Loss: 0.00002048
Iteration 183/1000 | Loss: 0.00002048
Iteration 184/1000 | Loss: 0.00002048
Iteration 185/1000 | Loss: 0.00002048
Iteration 186/1000 | Loss: 0.00002047
Iteration 187/1000 | Loss: 0.00002047
Iteration 188/1000 | Loss: 0.00002047
Iteration 189/1000 | Loss: 0.00002047
Iteration 190/1000 | Loss: 0.00002047
Iteration 191/1000 | Loss: 0.00002047
Iteration 192/1000 | Loss: 0.00002047
Iteration 193/1000 | Loss: 0.00002047
Iteration 194/1000 | Loss: 0.00002046
Iteration 195/1000 | Loss: 0.00002046
Iteration 196/1000 | Loss: 0.00002046
Iteration 197/1000 | Loss: 0.00002046
Iteration 198/1000 | Loss: 0.00002046
Iteration 199/1000 | Loss: 0.00002046
Iteration 200/1000 | Loss: 0.00002046
Iteration 201/1000 | Loss: 0.00002046
Iteration 202/1000 | Loss: 0.00002046
Iteration 203/1000 | Loss: 0.00002046
Iteration 204/1000 | Loss: 0.00002046
Iteration 205/1000 | Loss: 0.00002045
Iteration 206/1000 | Loss: 0.00002045
Iteration 207/1000 | Loss: 0.00002045
Iteration 208/1000 | Loss: 0.00002045
Iteration 209/1000 | Loss: 0.00002045
Iteration 210/1000 | Loss: 0.00002045
Iteration 211/1000 | Loss: 0.00002045
Iteration 212/1000 | Loss: 0.00002045
Iteration 213/1000 | Loss: 0.00002045
Iteration 214/1000 | Loss: 0.00002045
Iteration 215/1000 | Loss: 0.00002044
Iteration 216/1000 | Loss: 0.00002044
Iteration 217/1000 | Loss: 0.00002044
Iteration 218/1000 | Loss: 0.00002044
Iteration 219/1000 | Loss: 0.00002044
Iteration 220/1000 | Loss: 0.00002044
Iteration 221/1000 | Loss: 0.00002044
Iteration 222/1000 | Loss: 0.00002044
Iteration 223/1000 | Loss: 0.00002044
Iteration 224/1000 | Loss: 0.00002044
Iteration 225/1000 | Loss: 0.00002044
Iteration 226/1000 | Loss: 0.00002044
Iteration 227/1000 | Loss: 0.00002044
Iteration 228/1000 | Loss: 0.00002043
Iteration 229/1000 | Loss: 0.00002043
Iteration 230/1000 | Loss: 0.00002043
Iteration 231/1000 | Loss: 0.00002043
Iteration 232/1000 | Loss: 0.00002043
Iteration 233/1000 | Loss: 0.00002043
Iteration 234/1000 | Loss: 0.00002042
Iteration 235/1000 | Loss: 0.00002042
Iteration 236/1000 | Loss: 0.00002042
Iteration 237/1000 | Loss: 0.00002042
Iteration 238/1000 | Loss: 0.00002042
Iteration 239/1000 | Loss: 0.00002042
Iteration 240/1000 | Loss: 0.00002042
Iteration 241/1000 | Loss: 0.00002042
Iteration 242/1000 | Loss: 0.00002042
Iteration 243/1000 | Loss: 0.00002042
Iteration 244/1000 | Loss: 0.00002042
Iteration 245/1000 | Loss: 0.00002042
Iteration 246/1000 | Loss: 0.00002042
Iteration 247/1000 | Loss: 0.00002042
Iteration 248/1000 | Loss: 0.00002042
Iteration 249/1000 | Loss: 0.00002042
Iteration 250/1000 | Loss: 0.00002042
Iteration 251/1000 | Loss: 0.00002042
Iteration 252/1000 | Loss: 0.00002042
Iteration 253/1000 | Loss: 0.00002042
Iteration 254/1000 | Loss: 0.00002042
Iteration 255/1000 | Loss: 0.00002042
Iteration 256/1000 | Loss: 0.00002042
Iteration 257/1000 | Loss: 0.00002042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [2.041872539848555e-05, 2.041872539848555e-05, 2.041872539848555e-05, 2.041872539848555e-05, 2.041872539848555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.041872539848555e-05

Optimization complete. Final v2v error: 3.6784090995788574 mm

Highest mean error: 5.571205139160156 mm for frame 123

Lowest mean error: 2.439173936843872 mm for frame 1

Saving results

Total time: 50.08330154418945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009844
Iteration 2/25 | Loss: 0.00182723
Iteration 3/25 | Loss: 0.00154080
Iteration 4/25 | Loss: 0.00142857
Iteration 5/25 | Loss: 0.00144117
Iteration 6/25 | Loss: 0.00130977
Iteration 7/25 | Loss: 0.00122270
Iteration 8/25 | Loss: 0.00121493
Iteration 9/25 | Loss: 0.00122834
Iteration 10/25 | Loss: 0.00123423
Iteration 11/25 | Loss: 0.00120762
Iteration 12/25 | Loss: 0.00119267
Iteration 13/25 | Loss: 0.00119306
Iteration 14/25 | Loss: 0.00118892
Iteration 15/25 | Loss: 0.00118194
Iteration 16/25 | Loss: 0.00117944
Iteration 17/25 | Loss: 0.00117659
Iteration 18/25 | Loss: 0.00117500
Iteration 19/25 | Loss: 0.00117405
Iteration 20/25 | Loss: 0.00117356
Iteration 21/25 | Loss: 0.00117409
Iteration 22/25 | Loss: 0.00117477
Iteration 23/25 | Loss: 0.00117307
Iteration 24/25 | Loss: 0.00117129
Iteration 25/25 | Loss: 0.00117130

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41348767
Iteration 2/25 | Loss: 0.00107947
Iteration 3/25 | Loss: 0.00096801
Iteration 4/25 | Loss: 0.00096801
Iteration 5/25 | Loss: 0.00096801
Iteration 6/25 | Loss: 0.00096801
Iteration 7/25 | Loss: 0.00096801
Iteration 8/25 | Loss: 0.00096801
Iteration 9/25 | Loss: 0.00096801
Iteration 10/25 | Loss: 0.00096801
Iteration 11/25 | Loss: 0.00096801
Iteration 12/25 | Loss: 0.00096801
Iteration 13/25 | Loss: 0.00096801
Iteration 14/25 | Loss: 0.00096801
Iteration 15/25 | Loss: 0.00096801
Iteration 16/25 | Loss: 0.00096801
Iteration 17/25 | Loss: 0.00096801
Iteration 18/25 | Loss: 0.00096801
Iteration 19/25 | Loss: 0.00096801
Iteration 20/25 | Loss: 0.00096801
Iteration 21/25 | Loss: 0.00096801
Iteration 22/25 | Loss: 0.00096801
Iteration 23/25 | Loss: 0.00096800
Iteration 24/25 | Loss: 0.00096800
Iteration 25/25 | Loss: 0.00096800

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096800
Iteration 2/1000 | Loss: 0.00016431
Iteration 3/1000 | Loss: 0.00059608
Iteration 4/1000 | Loss: 0.00002351
Iteration 5/1000 | Loss: 0.00001873
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00019600
Iteration 8/1000 | Loss: 0.00001627
Iteration 9/1000 | Loss: 0.00001569
Iteration 10/1000 | Loss: 0.00016391
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001477
Iteration 15/1000 | Loss: 0.00001465
Iteration 16/1000 | Loss: 0.00001457
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001446
Iteration 19/1000 | Loss: 0.00001445
Iteration 20/1000 | Loss: 0.00001444
Iteration 21/1000 | Loss: 0.00017827
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001432
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001428
Iteration 29/1000 | Loss: 0.00001463
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001427
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001413
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001412
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001409
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001409
Iteration 44/1000 | Loss: 0.00001409
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001409
Iteration 47/1000 | Loss: 0.00001409
Iteration 48/1000 | Loss: 0.00001409
Iteration 49/1000 | Loss: 0.00001409
Iteration 50/1000 | Loss: 0.00001409
Iteration 51/1000 | Loss: 0.00001408
Iteration 52/1000 | Loss: 0.00001408
Iteration 53/1000 | Loss: 0.00001408
Iteration 54/1000 | Loss: 0.00001408
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001406
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001405
Iteration 65/1000 | Loss: 0.00001405
Iteration 66/1000 | Loss: 0.00001405
Iteration 67/1000 | Loss: 0.00001404
Iteration 68/1000 | Loss: 0.00001400
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001390
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001384
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001383
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001376
Iteration 79/1000 | Loss: 0.00001376
Iteration 80/1000 | Loss: 0.00001376
Iteration 81/1000 | Loss: 0.00001376
Iteration 82/1000 | Loss: 0.00001376
Iteration 83/1000 | Loss: 0.00001376
Iteration 84/1000 | Loss: 0.00001375
Iteration 85/1000 | Loss: 0.00001375
Iteration 86/1000 | Loss: 0.00001375
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001374
Iteration 89/1000 | Loss: 0.00002108
Iteration 90/1000 | Loss: 0.00001443
Iteration 91/1000 | Loss: 0.00025118
Iteration 92/1000 | Loss: 0.00003105
Iteration 93/1000 | Loss: 0.00002055
Iteration 94/1000 | Loss: 0.00001812
Iteration 95/1000 | Loss: 0.00001692
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001561
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001270
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001257
Iteration 110/1000 | Loss: 0.00001257
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001252
Iteration 123/1000 | Loss: 0.00001252
Iteration 124/1000 | Loss: 0.00001252
Iteration 125/1000 | Loss: 0.00001252
Iteration 126/1000 | Loss: 0.00001252
Iteration 127/1000 | Loss: 0.00001252
Iteration 128/1000 | Loss: 0.00001252
Iteration 129/1000 | Loss: 0.00001251
Iteration 130/1000 | Loss: 0.00001251
Iteration 131/1000 | Loss: 0.00001251
Iteration 132/1000 | Loss: 0.00001251
Iteration 133/1000 | Loss: 0.00001251
Iteration 134/1000 | Loss: 0.00001251
Iteration 135/1000 | Loss: 0.00001251
Iteration 136/1000 | Loss: 0.00001250
Iteration 137/1000 | Loss: 0.00001250
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001250
Iteration 146/1000 | Loss: 0.00001250
Iteration 147/1000 | Loss: 0.00001250
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001250
Iteration 150/1000 | Loss: 0.00001250
Iteration 151/1000 | Loss: 0.00001249
Iteration 152/1000 | Loss: 0.00001249
Iteration 153/1000 | Loss: 0.00001249
Iteration 154/1000 | Loss: 0.00001249
Iteration 155/1000 | Loss: 0.00001249
Iteration 156/1000 | Loss: 0.00001249
Iteration 157/1000 | Loss: 0.00001249
Iteration 158/1000 | Loss: 0.00001249
Iteration 159/1000 | Loss: 0.00001249
Iteration 160/1000 | Loss: 0.00001249
Iteration 161/1000 | Loss: 0.00001249
Iteration 162/1000 | Loss: 0.00001249
Iteration 163/1000 | Loss: 0.00001249
Iteration 164/1000 | Loss: 0.00001249
Iteration 165/1000 | Loss: 0.00001249
Iteration 166/1000 | Loss: 0.00001249
Iteration 167/1000 | Loss: 0.00001249
Iteration 168/1000 | Loss: 0.00001249
Iteration 169/1000 | Loss: 0.00001249
Iteration 170/1000 | Loss: 0.00001249
Iteration 171/1000 | Loss: 0.00001249
Iteration 172/1000 | Loss: 0.00001249
Iteration 173/1000 | Loss: 0.00001249
Iteration 174/1000 | Loss: 0.00001249
Iteration 175/1000 | Loss: 0.00001249
Iteration 176/1000 | Loss: 0.00001248
Iteration 177/1000 | Loss: 0.00001248
Iteration 178/1000 | Loss: 0.00001248
Iteration 179/1000 | Loss: 0.00001248
Iteration 180/1000 | Loss: 0.00001248
Iteration 181/1000 | Loss: 0.00001248
Iteration 182/1000 | Loss: 0.00001248
Iteration 183/1000 | Loss: 0.00001248
Iteration 184/1000 | Loss: 0.00001248
Iteration 185/1000 | Loss: 0.00001248
Iteration 186/1000 | Loss: 0.00001248
Iteration 187/1000 | Loss: 0.00001248
Iteration 188/1000 | Loss: 0.00001248
Iteration 189/1000 | Loss: 0.00001248
Iteration 190/1000 | Loss: 0.00001248
Iteration 191/1000 | Loss: 0.00001248
Iteration 192/1000 | Loss: 0.00001248
Iteration 193/1000 | Loss: 0.00001248
Iteration 194/1000 | Loss: 0.00001248
Iteration 195/1000 | Loss: 0.00001248
Iteration 196/1000 | Loss: 0.00001248
Iteration 197/1000 | Loss: 0.00001247
Iteration 198/1000 | Loss: 0.00001247
Iteration 199/1000 | Loss: 0.00001247
Iteration 200/1000 | Loss: 0.00001247
Iteration 201/1000 | Loss: 0.00001247
Iteration 202/1000 | Loss: 0.00001247
Iteration 203/1000 | Loss: 0.00001247
Iteration 204/1000 | Loss: 0.00001247
Iteration 205/1000 | Loss: 0.00001247
Iteration 206/1000 | Loss: 0.00001247
Iteration 207/1000 | Loss: 0.00001247
Iteration 208/1000 | Loss: 0.00001247
Iteration 209/1000 | Loss: 0.00001247
Iteration 210/1000 | Loss: 0.00001247
Iteration 211/1000 | Loss: 0.00001247
Iteration 212/1000 | Loss: 0.00001247
Iteration 213/1000 | Loss: 0.00001247
Iteration 214/1000 | Loss: 0.00001247
Iteration 215/1000 | Loss: 0.00001247
Iteration 216/1000 | Loss: 0.00001247
Iteration 217/1000 | Loss: 0.00001247
Iteration 218/1000 | Loss: 0.00001247
Iteration 219/1000 | Loss: 0.00001246
Iteration 220/1000 | Loss: 0.00001246
Iteration 221/1000 | Loss: 0.00001246
Iteration 222/1000 | Loss: 0.00001246
Iteration 223/1000 | Loss: 0.00001246
Iteration 224/1000 | Loss: 0.00001246
Iteration 225/1000 | Loss: 0.00001246
Iteration 226/1000 | Loss: 0.00001246
Iteration 227/1000 | Loss: 0.00001246
Iteration 228/1000 | Loss: 0.00001246
Iteration 229/1000 | Loss: 0.00001246
Iteration 230/1000 | Loss: 0.00001246
Iteration 231/1000 | Loss: 0.00001246
Iteration 232/1000 | Loss: 0.00001246
Iteration 233/1000 | Loss: 0.00001246
Iteration 234/1000 | Loss: 0.00001246
Iteration 235/1000 | Loss: 0.00001246
Iteration 236/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.2462594895623624e-05, 1.2462594895623624e-05, 1.2462594895623624e-05, 1.2462594895623624e-05, 1.2462594895623624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2462594895623624e-05

Optimization complete. Final v2v error: 2.926682472229004 mm

Highest mean error: 8.465045928955078 mm for frame 109

Lowest mean error: 2.60105299949646 mm for frame 26

Saving results

Total time: 104.99586153030396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897452
Iteration 2/25 | Loss: 0.00132970
Iteration 3/25 | Loss: 0.00118660
Iteration 4/25 | Loss: 0.00117070
Iteration 5/25 | Loss: 0.00116535
Iteration 6/25 | Loss: 0.00116430
Iteration 7/25 | Loss: 0.00116430
Iteration 8/25 | Loss: 0.00116430
Iteration 9/25 | Loss: 0.00116430
Iteration 10/25 | Loss: 0.00116430
Iteration 11/25 | Loss: 0.00116430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011643008328974247, 0.0011643008328974247, 0.0011643008328974247, 0.0011643008328974247, 0.0011643008328974247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011643008328974247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33261704
Iteration 2/25 | Loss: 0.00104332
Iteration 3/25 | Loss: 0.00104304
Iteration 4/25 | Loss: 0.00104304
Iteration 5/25 | Loss: 0.00104304
Iteration 6/25 | Loss: 0.00104304
Iteration 7/25 | Loss: 0.00104304
Iteration 8/25 | Loss: 0.00104304
Iteration 9/25 | Loss: 0.00104304
Iteration 10/25 | Loss: 0.00104304
Iteration 11/25 | Loss: 0.00104304
Iteration 12/25 | Loss: 0.00104304
Iteration 13/25 | Loss: 0.00104304
Iteration 14/25 | Loss: 0.00104304
Iteration 15/25 | Loss: 0.00104304
Iteration 16/25 | Loss: 0.00104304
Iteration 17/25 | Loss: 0.00104304
Iteration 18/25 | Loss: 0.00104304
Iteration 19/25 | Loss: 0.00104304
Iteration 20/25 | Loss: 0.00104304
Iteration 21/25 | Loss: 0.00104304
Iteration 22/25 | Loss: 0.00104304
Iteration 23/25 | Loss: 0.00104304
Iteration 24/25 | Loss: 0.00104304
Iteration 25/25 | Loss: 0.00104304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104304
Iteration 2/1000 | Loss: 0.00005811
Iteration 3/1000 | Loss: 0.00003855
Iteration 4/1000 | Loss: 0.00002939
Iteration 5/1000 | Loss: 0.00002698
Iteration 6/1000 | Loss: 0.00002590
Iteration 7/1000 | Loss: 0.00002512
Iteration 8/1000 | Loss: 0.00002468
Iteration 9/1000 | Loss: 0.00002422
Iteration 10/1000 | Loss: 0.00002382
Iteration 11/1000 | Loss: 0.00002378
Iteration 12/1000 | Loss: 0.00002355
Iteration 13/1000 | Loss: 0.00002334
Iteration 14/1000 | Loss: 0.00002330
Iteration 15/1000 | Loss: 0.00002329
Iteration 16/1000 | Loss: 0.00002328
Iteration 17/1000 | Loss: 0.00002325
Iteration 18/1000 | Loss: 0.00002320
Iteration 19/1000 | Loss: 0.00002319
Iteration 20/1000 | Loss: 0.00002313
Iteration 21/1000 | Loss: 0.00002311
Iteration 22/1000 | Loss: 0.00002311
Iteration 23/1000 | Loss: 0.00002308
Iteration 24/1000 | Loss: 0.00002305
Iteration 25/1000 | Loss: 0.00002304
Iteration 26/1000 | Loss: 0.00002303
Iteration 27/1000 | Loss: 0.00002303
Iteration 28/1000 | Loss: 0.00002302
Iteration 29/1000 | Loss: 0.00002302
Iteration 30/1000 | Loss: 0.00002302
Iteration 31/1000 | Loss: 0.00002300
Iteration 32/1000 | Loss: 0.00002300
Iteration 33/1000 | Loss: 0.00002299
Iteration 34/1000 | Loss: 0.00002296
Iteration 35/1000 | Loss: 0.00002296
Iteration 36/1000 | Loss: 0.00002296
Iteration 37/1000 | Loss: 0.00002296
Iteration 38/1000 | Loss: 0.00002296
Iteration 39/1000 | Loss: 0.00002296
Iteration 40/1000 | Loss: 0.00002296
Iteration 41/1000 | Loss: 0.00002296
Iteration 42/1000 | Loss: 0.00002296
Iteration 43/1000 | Loss: 0.00002296
Iteration 44/1000 | Loss: 0.00002296
Iteration 45/1000 | Loss: 0.00002295
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00002295
Iteration 48/1000 | Loss: 0.00002295
Iteration 49/1000 | Loss: 0.00002295
Iteration 50/1000 | Loss: 0.00002292
Iteration 51/1000 | Loss: 0.00002291
Iteration 52/1000 | Loss: 0.00002290
Iteration 53/1000 | Loss: 0.00002290
Iteration 54/1000 | Loss: 0.00002290
Iteration 55/1000 | Loss: 0.00002289
Iteration 56/1000 | Loss: 0.00002289
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00002288
Iteration 59/1000 | Loss: 0.00002287
Iteration 60/1000 | Loss: 0.00002287
Iteration 61/1000 | Loss: 0.00002287
Iteration 62/1000 | Loss: 0.00002287
Iteration 63/1000 | Loss: 0.00002286
Iteration 64/1000 | Loss: 0.00002286
Iteration 65/1000 | Loss: 0.00002286
Iteration 66/1000 | Loss: 0.00002285
Iteration 67/1000 | Loss: 0.00002285
Iteration 68/1000 | Loss: 0.00002285
Iteration 69/1000 | Loss: 0.00002285
Iteration 70/1000 | Loss: 0.00002285
Iteration 71/1000 | Loss: 0.00002285
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002284
Iteration 75/1000 | Loss: 0.00002284
Iteration 76/1000 | Loss: 0.00002283
Iteration 77/1000 | Loss: 0.00002283
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002283
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002282
Iteration 83/1000 | Loss: 0.00002282
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002281
Iteration 87/1000 | Loss: 0.00002281
Iteration 88/1000 | Loss: 0.00002281
Iteration 89/1000 | Loss: 0.00002281
Iteration 90/1000 | Loss: 0.00002280
Iteration 91/1000 | Loss: 0.00002280
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002279
Iteration 94/1000 | Loss: 0.00002279
Iteration 95/1000 | Loss: 0.00002279
Iteration 96/1000 | Loss: 0.00002279
Iteration 97/1000 | Loss: 0.00002279
Iteration 98/1000 | Loss: 0.00002279
Iteration 99/1000 | Loss: 0.00002279
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002278
Iteration 102/1000 | Loss: 0.00002278
Iteration 103/1000 | Loss: 0.00002278
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002276
Iteration 108/1000 | Loss: 0.00002276
Iteration 109/1000 | Loss: 0.00002275
Iteration 110/1000 | Loss: 0.00002275
Iteration 111/1000 | Loss: 0.00002275
Iteration 112/1000 | Loss: 0.00002275
Iteration 113/1000 | Loss: 0.00002275
Iteration 114/1000 | Loss: 0.00002275
Iteration 115/1000 | Loss: 0.00002275
Iteration 116/1000 | Loss: 0.00002275
Iteration 117/1000 | Loss: 0.00002275
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002274
Iteration 120/1000 | Loss: 0.00002274
Iteration 121/1000 | Loss: 0.00002274
Iteration 122/1000 | Loss: 0.00002274
Iteration 123/1000 | Loss: 0.00002274
Iteration 124/1000 | Loss: 0.00002273
Iteration 125/1000 | Loss: 0.00002273
Iteration 126/1000 | Loss: 0.00002273
Iteration 127/1000 | Loss: 0.00002273
Iteration 128/1000 | Loss: 0.00002273
Iteration 129/1000 | Loss: 0.00002273
Iteration 130/1000 | Loss: 0.00002273
Iteration 131/1000 | Loss: 0.00002273
Iteration 132/1000 | Loss: 0.00002273
Iteration 133/1000 | Loss: 0.00002273
Iteration 134/1000 | Loss: 0.00002273
Iteration 135/1000 | Loss: 0.00002273
Iteration 136/1000 | Loss: 0.00002273
Iteration 137/1000 | Loss: 0.00002273
Iteration 138/1000 | Loss: 0.00002273
Iteration 139/1000 | Loss: 0.00002273
Iteration 140/1000 | Loss: 0.00002273
Iteration 141/1000 | Loss: 0.00002273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.272870551678352e-05, 2.272870551678352e-05, 2.272870551678352e-05, 2.272870551678352e-05, 2.272870551678352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.272870551678352e-05

Optimization complete. Final v2v error: 3.5157618522644043 mm

Highest mean error: 5.594761848449707 mm for frame 87

Lowest mean error: 2.416508197784424 mm for frame 48

Saving results

Total time: 38.91782999038696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025232
Iteration 2/25 | Loss: 0.01025232
Iteration 3/25 | Loss: 0.01025232
Iteration 4/25 | Loss: 0.01025232
Iteration 5/25 | Loss: 0.01025231
Iteration 6/25 | Loss: 0.01025231
Iteration 7/25 | Loss: 0.00183524
Iteration 8/25 | Loss: 0.00121390
Iteration 9/25 | Loss: 0.00114536
Iteration 10/25 | Loss: 0.00112442
Iteration 11/25 | Loss: 0.00113446
Iteration 12/25 | Loss: 0.00112990
Iteration 13/25 | Loss: 0.00110977
Iteration 14/25 | Loss: 0.00110375
Iteration 15/25 | Loss: 0.00109908
Iteration 16/25 | Loss: 0.00109797
Iteration 17/25 | Loss: 0.00109763
Iteration 18/25 | Loss: 0.00109752
Iteration 19/25 | Loss: 0.00109752
Iteration 20/25 | Loss: 0.00109752
Iteration 21/25 | Loss: 0.00109752
Iteration 22/25 | Loss: 0.00109752
Iteration 23/25 | Loss: 0.00109752
Iteration 24/25 | Loss: 0.00109752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010975189507007599, 0.0010975189507007599, 0.0010975189507007599, 0.0010975189507007599, 0.0010975189507007599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010975189507007599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37165487
Iteration 2/25 | Loss: 0.00088592
Iteration 3/25 | Loss: 0.00088592
Iteration 4/25 | Loss: 0.00088592
Iteration 5/25 | Loss: 0.00088592
Iteration 6/25 | Loss: 0.00088592
Iteration 7/25 | Loss: 0.00088288
Iteration 8/25 | Loss: 0.00088288
Iteration 9/25 | Loss: 0.00088288
Iteration 10/25 | Loss: 0.00088288
Iteration 11/25 | Loss: 0.00088288
Iteration 12/25 | Loss: 0.00088288
Iteration 13/25 | Loss: 0.00088288
Iteration 14/25 | Loss: 0.00088288
Iteration 15/25 | Loss: 0.00088288
Iteration 16/25 | Loss: 0.00088288
Iteration 17/25 | Loss: 0.00088288
Iteration 18/25 | Loss: 0.00088288
Iteration 19/25 | Loss: 0.00088288
Iteration 20/25 | Loss: 0.00088288
Iteration 21/25 | Loss: 0.00088288
Iteration 22/25 | Loss: 0.00088288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008828775025904179, 0.0008828775025904179, 0.0008828775025904179, 0.0008828775025904179, 0.0008828775025904179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008828775025904179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088288
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00001676
Iteration 4/1000 | Loss: 0.00001839
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00023000
Iteration 8/1000 | Loss: 0.00009323
Iteration 9/1000 | Loss: 0.00010445
Iteration 10/1000 | Loss: 0.00003633
Iteration 11/1000 | Loss: 0.00002752
Iteration 12/1000 | Loss: 0.00007947
Iteration 13/1000 | Loss: 0.00017770
Iteration 14/1000 | Loss: 0.00001628
Iteration 15/1000 | Loss: 0.00003627
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001307
Iteration 18/1000 | Loss: 0.00003787
Iteration 19/1000 | Loss: 0.00001413
Iteration 20/1000 | Loss: 0.00006473
Iteration 21/1000 | Loss: 0.00001604
Iteration 22/1000 | Loss: 0.00005921
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00002478
Iteration 25/1000 | Loss: 0.00004595
Iteration 26/1000 | Loss: 0.00002139
Iteration 27/1000 | Loss: 0.00001919
Iteration 28/1000 | Loss: 0.00005128
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00003468
Iteration 31/1000 | Loss: 0.00004979
Iteration 32/1000 | Loss: 0.00003923
Iteration 33/1000 | Loss: 0.00004807
Iteration 34/1000 | Loss: 0.00002185
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001878
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00005787
Iteration 40/1000 | Loss: 0.00009006
Iteration 41/1000 | Loss: 0.00006239
Iteration 42/1000 | Loss: 0.00002811
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00002971
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00008902
Iteration 47/1000 | Loss: 0.00001833
Iteration 48/1000 | Loss: 0.00002716
Iteration 49/1000 | Loss: 0.00004075
Iteration 50/1000 | Loss: 0.00001467
Iteration 51/1000 | Loss: 0.00003256
Iteration 52/1000 | Loss: 0.00006957
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001190
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001190
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00002163
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001187
Iteration 83/1000 | Loss: 0.00001382
Iteration 84/1000 | Loss: 0.00001317
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001185
Iteration 87/1000 | Loss: 0.00001185
Iteration 88/1000 | Loss: 0.00001185
Iteration 89/1000 | Loss: 0.00001185
Iteration 90/1000 | Loss: 0.00001185
Iteration 91/1000 | Loss: 0.00001185
Iteration 92/1000 | Loss: 0.00001184
Iteration 93/1000 | Loss: 0.00001184
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001184
Iteration 96/1000 | Loss: 0.00001184
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001181
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001180
Iteration 120/1000 | Loss: 0.00001180
Iteration 121/1000 | Loss: 0.00001180
Iteration 122/1000 | Loss: 0.00001180
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001176
Iteration 133/1000 | Loss: 0.00001176
Iteration 134/1000 | Loss: 0.00001176
Iteration 135/1000 | Loss: 0.00001176
Iteration 136/1000 | Loss: 0.00001176
Iteration 137/1000 | Loss: 0.00001176
Iteration 138/1000 | Loss: 0.00001176
Iteration 139/1000 | Loss: 0.00001176
Iteration 140/1000 | Loss: 0.00001175
Iteration 141/1000 | Loss: 0.00001175
Iteration 142/1000 | Loss: 0.00001175
Iteration 143/1000 | Loss: 0.00001175
Iteration 144/1000 | Loss: 0.00001175
Iteration 145/1000 | Loss: 0.00001175
Iteration 146/1000 | Loss: 0.00001175
Iteration 147/1000 | Loss: 0.00001175
Iteration 148/1000 | Loss: 0.00001175
Iteration 149/1000 | Loss: 0.00001175
Iteration 150/1000 | Loss: 0.00001175
Iteration 151/1000 | Loss: 0.00001175
Iteration 152/1000 | Loss: 0.00001175
Iteration 153/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.1754421393561643e-05, 1.1754421393561643e-05, 1.1754421393561643e-05, 1.1754421393561643e-05, 1.1754421393561643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1754421393561643e-05

Optimization complete. Final v2v error: 2.9249351024627686 mm

Highest mean error: 3.4356164932250977 mm for frame 12

Lowest mean error: 2.587892770767212 mm for frame 38

Saving results

Total time: 123.73676466941833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834922
Iteration 2/25 | Loss: 0.00120494
Iteration 3/25 | Loss: 0.00108819
Iteration 4/25 | Loss: 0.00107666
Iteration 5/25 | Loss: 0.00107450
Iteration 6/25 | Loss: 0.00107411
Iteration 7/25 | Loss: 0.00107411
Iteration 8/25 | Loss: 0.00107411
Iteration 9/25 | Loss: 0.00107411
Iteration 10/25 | Loss: 0.00107411
Iteration 11/25 | Loss: 0.00107411
Iteration 12/25 | Loss: 0.00107411
Iteration 13/25 | Loss: 0.00107411
Iteration 14/25 | Loss: 0.00107411
Iteration 15/25 | Loss: 0.00107411
Iteration 16/25 | Loss: 0.00107411
Iteration 17/25 | Loss: 0.00107411
Iteration 18/25 | Loss: 0.00107411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010741082951426506, 0.0010741082951426506, 0.0010741082951426506, 0.0010741082951426506, 0.0010741082951426506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010741082951426506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34272659
Iteration 2/25 | Loss: 0.00070645
Iteration 3/25 | Loss: 0.00070642
Iteration 4/25 | Loss: 0.00070642
Iteration 5/25 | Loss: 0.00070642
Iteration 6/25 | Loss: 0.00070642
Iteration 7/25 | Loss: 0.00070642
Iteration 8/25 | Loss: 0.00070642
Iteration 9/25 | Loss: 0.00070642
Iteration 10/25 | Loss: 0.00070642
Iteration 11/25 | Loss: 0.00070642
Iteration 12/25 | Loss: 0.00070642
Iteration 13/25 | Loss: 0.00070642
Iteration 14/25 | Loss: 0.00070642
Iteration 15/25 | Loss: 0.00070642
Iteration 16/25 | Loss: 0.00070642
Iteration 17/25 | Loss: 0.00070642
Iteration 18/25 | Loss: 0.00070642
Iteration 19/25 | Loss: 0.00070642
Iteration 20/25 | Loss: 0.00070642
Iteration 21/25 | Loss: 0.00070642
Iteration 22/25 | Loss: 0.00070642
Iteration 23/25 | Loss: 0.00070642
Iteration 24/25 | Loss: 0.00070642
Iteration 25/25 | Loss: 0.00070642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070642
Iteration 2/1000 | Loss: 0.00002039
Iteration 3/1000 | Loss: 0.00001268
Iteration 4/1000 | Loss: 0.00001110
Iteration 5/1000 | Loss: 0.00001007
Iteration 6/1000 | Loss: 0.00000950
Iteration 7/1000 | Loss: 0.00000913
Iteration 8/1000 | Loss: 0.00000886
Iteration 9/1000 | Loss: 0.00000878
Iteration 10/1000 | Loss: 0.00000877
Iteration 11/1000 | Loss: 0.00000877
Iteration 12/1000 | Loss: 0.00000875
Iteration 13/1000 | Loss: 0.00000869
Iteration 14/1000 | Loss: 0.00000851
Iteration 15/1000 | Loss: 0.00000843
Iteration 16/1000 | Loss: 0.00000843
Iteration 17/1000 | Loss: 0.00000842
Iteration 18/1000 | Loss: 0.00000839
Iteration 19/1000 | Loss: 0.00000838
Iteration 20/1000 | Loss: 0.00000833
Iteration 21/1000 | Loss: 0.00000831
Iteration 22/1000 | Loss: 0.00000830
Iteration 23/1000 | Loss: 0.00000830
Iteration 24/1000 | Loss: 0.00000829
Iteration 25/1000 | Loss: 0.00000828
Iteration 26/1000 | Loss: 0.00000828
Iteration 27/1000 | Loss: 0.00000823
Iteration 28/1000 | Loss: 0.00000822
Iteration 29/1000 | Loss: 0.00000819
Iteration 30/1000 | Loss: 0.00000818
Iteration 31/1000 | Loss: 0.00000817
Iteration 32/1000 | Loss: 0.00000817
Iteration 33/1000 | Loss: 0.00000817
Iteration 34/1000 | Loss: 0.00000816
Iteration 35/1000 | Loss: 0.00000813
Iteration 36/1000 | Loss: 0.00000811
Iteration 37/1000 | Loss: 0.00000810
Iteration 38/1000 | Loss: 0.00000810
Iteration 39/1000 | Loss: 0.00000809
Iteration 40/1000 | Loss: 0.00000809
Iteration 41/1000 | Loss: 0.00000808
Iteration 42/1000 | Loss: 0.00000807
Iteration 43/1000 | Loss: 0.00000807
Iteration 44/1000 | Loss: 0.00000807
Iteration 45/1000 | Loss: 0.00000807
Iteration 46/1000 | Loss: 0.00000806
Iteration 47/1000 | Loss: 0.00000806
Iteration 48/1000 | Loss: 0.00000806
Iteration 49/1000 | Loss: 0.00000806
Iteration 50/1000 | Loss: 0.00000806
Iteration 51/1000 | Loss: 0.00000806
Iteration 52/1000 | Loss: 0.00000806
Iteration 53/1000 | Loss: 0.00000806
Iteration 54/1000 | Loss: 0.00000805
Iteration 55/1000 | Loss: 0.00000805
Iteration 56/1000 | Loss: 0.00000805
Iteration 57/1000 | Loss: 0.00000805
Iteration 58/1000 | Loss: 0.00000805
Iteration 59/1000 | Loss: 0.00000804
Iteration 60/1000 | Loss: 0.00000804
Iteration 61/1000 | Loss: 0.00000804
Iteration 62/1000 | Loss: 0.00000804
Iteration 63/1000 | Loss: 0.00000804
Iteration 64/1000 | Loss: 0.00000803
Iteration 65/1000 | Loss: 0.00000803
Iteration 66/1000 | Loss: 0.00000803
Iteration 67/1000 | Loss: 0.00000803
Iteration 68/1000 | Loss: 0.00000803
Iteration 69/1000 | Loss: 0.00000803
Iteration 70/1000 | Loss: 0.00000803
Iteration 71/1000 | Loss: 0.00000803
Iteration 72/1000 | Loss: 0.00000803
Iteration 73/1000 | Loss: 0.00000803
Iteration 74/1000 | Loss: 0.00000803
Iteration 75/1000 | Loss: 0.00000802
Iteration 76/1000 | Loss: 0.00000802
Iteration 77/1000 | Loss: 0.00000802
Iteration 78/1000 | Loss: 0.00000802
Iteration 79/1000 | Loss: 0.00000802
Iteration 80/1000 | Loss: 0.00000802
Iteration 81/1000 | Loss: 0.00000802
Iteration 82/1000 | Loss: 0.00000802
Iteration 83/1000 | Loss: 0.00000802
Iteration 84/1000 | Loss: 0.00000802
Iteration 85/1000 | Loss: 0.00000802
Iteration 86/1000 | Loss: 0.00000802
Iteration 87/1000 | Loss: 0.00000802
Iteration 88/1000 | Loss: 0.00000802
Iteration 89/1000 | Loss: 0.00000801
Iteration 90/1000 | Loss: 0.00000801
Iteration 91/1000 | Loss: 0.00000801
Iteration 92/1000 | Loss: 0.00000801
Iteration 93/1000 | Loss: 0.00000801
Iteration 94/1000 | Loss: 0.00000801
Iteration 95/1000 | Loss: 0.00000801
Iteration 96/1000 | Loss: 0.00000801
Iteration 97/1000 | Loss: 0.00000801
Iteration 98/1000 | Loss: 0.00000801
Iteration 99/1000 | Loss: 0.00000800
Iteration 100/1000 | Loss: 0.00000800
Iteration 101/1000 | Loss: 0.00000800
Iteration 102/1000 | Loss: 0.00000800
Iteration 103/1000 | Loss: 0.00000800
Iteration 104/1000 | Loss: 0.00000800
Iteration 105/1000 | Loss: 0.00000800
Iteration 106/1000 | Loss: 0.00000800
Iteration 107/1000 | Loss: 0.00000800
Iteration 108/1000 | Loss: 0.00000800
Iteration 109/1000 | Loss: 0.00000800
Iteration 110/1000 | Loss: 0.00000800
Iteration 111/1000 | Loss: 0.00000799
Iteration 112/1000 | Loss: 0.00000799
Iteration 113/1000 | Loss: 0.00000799
Iteration 114/1000 | Loss: 0.00000799
Iteration 115/1000 | Loss: 0.00000799
Iteration 116/1000 | Loss: 0.00000799
Iteration 117/1000 | Loss: 0.00000799
Iteration 118/1000 | Loss: 0.00000799
Iteration 119/1000 | Loss: 0.00000799
Iteration 120/1000 | Loss: 0.00000799
Iteration 121/1000 | Loss: 0.00000799
Iteration 122/1000 | Loss: 0.00000799
Iteration 123/1000 | Loss: 0.00000798
Iteration 124/1000 | Loss: 0.00000798
Iteration 125/1000 | Loss: 0.00000798
Iteration 126/1000 | Loss: 0.00000798
Iteration 127/1000 | Loss: 0.00000798
Iteration 128/1000 | Loss: 0.00000798
Iteration 129/1000 | Loss: 0.00000798
Iteration 130/1000 | Loss: 0.00000797
Iteration 131/1000 | Loss: 0.00000797
Iteration 132/1000 | Loss: 0.00000797
Iteration 133/1000 | Loss: 0.00000797
Iteration 134/1000 | Loss: 0.00000797
Iteration 135/1000 | Loss: 0.00000797
Iteration 136/1000 | Loss: 0.00000797
Iteration 137/1000 | Loss: 0.00000797
Iteration 138/1000 | Loss: 0.00000797
Iteration 139/1000 | Loss: 0.00000797
Iteration 140/1000 | Loss: 0.00000797
Iteration 141/1000 | Loss: 0.00000796
Iteration 142/1000 | Loss: 0.00000796
Iteration 143/1000 | Loss: 0.00000796
Iteration 144/1000 | Loss: 0.00000796
Iteration 145/1000 | Loss: 0.00000795
Iteration 146/1000 | Loss: 0.00000795
Iteration 147/1000 | Loss: 0.00000794
Iteration 148/1000 | Loss: 0.00000794
Iteration 149/1000 | Loss: 0.00000794
Iteration 150/1000 | Loss: 0.00000794
Iteration 151/1000 | Loss: 0.00000794
Iteration 152/1000 | Loss: 0.00000794
Iteration 153/1000 | Loss: 0.00000793
Iteration 154/1000 | Loss: 0.00000793
Iteration 155/1000 | Loss: 0.00000793
Iteration 156/1000 | Loss: 0.00000793
Iteration 157/1000 | Loss: 0.00000793
Iteration 158/1000 | Loss: 0.00000793
Iteration 159/1000 | Loss: 0.00000793
Iteration 160/1000 | Loss: 0.00000793
Iteration 161/1000 | Loss: 0.00000793
Iteration 162/1000 | Loss: 0.00000792
Iteration 163/1000 | Loss: 0.00000792
Iteration 164/1000 | Loss: 0.00000792
Iteration 165/1000 | Loss: 0.00000791
Iteration 166/1000 | Loss: 0.00000791
Iteration 167/1000 | Loss: 0.00000791
Iteration 168/1000 | Loss: 0.00000791
Iteration 169/1000 | Loss: 0.00000791
Iteration 170/1000 | Loss: 0.00000791
Iteration 171/1000 | Loss: 0.00000791
Iteration 172/1000 | Loss: 0.00000790
Iteration 173/1000 | Loss: 0.00000790
Iteration 174/1000 | Loss: 0.00000790
Iteration 175/1000 | Loss: 0.00000790
Iteration 176/1000 | Loss: 0.00000790
Iteration 177/1000 | Loss: 0.00000789
Iteration 178/1000 | Loss: 0.00000789
Iteration 179/1000 | Loss: 0.00000789
Iteration 180/1000 | Loss: 0.00000789
Iteration 181/1000 | Loss: 0.00000789
Iteration 182/1000 | Loss: 0.00000788
Iteration 183/1000 | Loss: 0.00000788
Iteration 184/1000 | Loss: 0.00000788
Iteration 185/1000 | Loss: 0.00000788
Iteration 186/1000 | Loss: 0.00000788
Iteration 187/1000 | Loss: 0.00000788
Iteration 188/1000 | Loss: 0.00000788
Iteration 189/1000 | Loss: 0.00000788
Iteration 190/1000 | Loss: 0.00000787
Iteration 191/1000 | Loss: 0.00000787
Iteration 192/1000 | Loss: 0.00000787
Iteration 193/1000 | Loss: 0.00000787
Iteration 194/1000 | Loss: 0.00000787
Iteration 195/1000 | Loss: 0.00000787
Iteration 196/1000 | Loss: 0.00000787
Iteration 197/1000 | Loss: 0.00000787
Iteration 198/1000 | Loss: 0.00000787
Iteration 199/1000 | Loss: 0.00000787
Iteration 200/1000 | Loss: 0.00000787
Iteration 201/1000 | Loss: 0.00000787
Iteration 202/1000 | Loss: 0.00000787
Iteration 203/1000 | Loss: 0.00000787
Iteration 204/1000 | Loss: 0.00000787
Iteration 205/1000 | Loss: 0.00000786
Iteration 206/1000 | Loss: 0.00000786
Iteration 207/1000 | Loss: 0.00000786
Iteration 208/1000 | Loss: 0.00000786
Iteration 209/1000 | Loss: 0.00000786
Iteration 210/1000 | Loss: 0.00000786
Iteration 211/1000 | Loss: 0.00000786
Iteration 212/1000 | Loss: 0.00000786
Iteration 213/1000 | Loss: 0.00000785
Iteration 214/1000 | Loss: 0.00000785
Iteration 215/1000 | Loss: 0.00000785
Iteration 216/1000 | Loss: 0.00000785
Iteration 217/1000 | Loss: 0.00000785
Iteration 218/1000 | Loss: 0.00000785
Iteration 219/1000 | Loss: 0.00000785
Iteration 220/1000 | Loss: 0.00000785
Iteration 221/1000 | Loss: 0.00000785
Iteration 222/1000 | Loss: 0.00000785
Iteration 223/1000 | Loss: 0.00000785
Iteration 224/1000 | Loss: 0.00000785
Iteration 225/1000 | Loss: 0.00000785
Iteration 226/1000 | Loss: 0.00000785
Iteration 227/1000 | Loss: 0.00000785
Iteration 228/1000 | Loss: 0.00000784
Iteration 229/1000 | Loss: 0.00000784
Iteration 230/1000 | Loss: 0.00000784
Iteration 231/1000 | Loss: 0.00000784
Iteration 232/1000 | Loss: 0.00000784
Iteration 233/1000 | Loss: 0.00000784
Iteration 234/1000 | Loss: 0.00000784
Iteration 235/1000 | Loss: 0.00000784
Iteration 236/1000 | Loss: 0.00000784
Iteration 237/1000 | Loss: 0.00000784
Iteration 238/1000 | Loss: 0.00000784
Iteration 239/1000 | Loss: 0.00000784
Iteration 240/1000 | Loss: 0.00000783
Iteration 241/1000 | Loss: 0.00000783
Iteration 242/1000 | Loss: 0.00000783
Iteration 243/1000 | Loss: 0.00000783
Iteration 244/1000 | Loss: 0.00000783
Iteration 245/1000 | Loss: 0.00000783
Iteration 246/1000 | Loss: 0.00000783
Iteration 247/1000 | Loss: 0.00000783
Iteration 248/1000 | Loss: 0.00000783
Iteration 249/1000 | Loss: 0.00000783
Iteration 250/1000 | Loss: 0.00000783
Iteration 251/1000 | Loss: 0.00000783
Iteration 252/1000 | Loss: 0.00000783
Iteration 253/1000 | Loss: 0.00000783
Iteration 254/1000 | Loss: 0.00000783
Iteration 255/1000 | Loss: 0.00000783
Iteration 256/1000 | Loss: 0.00000783
Iteration 257/1000 | Loss: 0.00000782
Iteration 258/1000 | Loss: 0.00000782
Iteration 259/1000 | Loss: 0.00000782
Iteration 260/1000 | Loss: 0.00000782
Iteration 261/1000 | Loss: 0.00000782
Iteration 262/1000 | Loss: 0.00000782
Iteration 263/1000 | Loss: 0.00000782
Iteration 264/1000 | Loss: 0.00000782
Iteration 265/1000 | Loss: 0.00000782
Iteration 266/1000 | Loss: 0.00000782
Iteration 267/1000 | Loss: 0.00000782
Iteration 268/1000 | Loss: 0.00000782
Iteration 269/1000 | Loss: 0.00000782
Iteration 270/1000 | Loss: 0.00000782
Iteration 271/1000 | Loss: 0.00000782
Iteration 272/1000 | Loss: 0.00000782
Iteration 273/1000 | Loss: 0.00000782
Iteration 274/1000 | Loss: 0.00000782
Iteration 275/1000 | Loss: 0.00000782
Iteration 276/1000 | Loss: 0.00000781
Iteration 277/1000 | Loss: 0.00000781
Iteration 278/1000 | Loss: 0.00000781
Iteration 279/1000 | Loss: 0.00000781
Iteration 280/1000 | Loss: 0.00000781
Iteration 281/1000 | Loss: 0.00000781
Iteration 282/1000 | Loss: 0.00000781
Iteration 283/1000 | Loss: 0.00000781
Iteration 284/1000 | Loss: 0.00000781
Iteration 285/1000 | Loss: 0.00000781
Iteration 286/1000 | Loss: 0.00000781
Iteration 287/1000 | Loss: 0.00000781
Iteration 288/1000 | Loss: 0.00000781
Iteration 289/1000 | Loss: 0.00000781
Iteration 290/1000 | Loss: 0.00000781
Iteration 291/1000 | Loss: 0.00000781
Iteration 292/1000 | Loss: 0.00000781
Iteration 293/1000 | Loss: 0.00000781
Iteration 294/1000 | Loss: 0.00000781
Iteration 295/1000 | Loss: 0.00000781
Iteration 296/1000 | Loss: 0.00000781
Iteration 297/1000 | Loss: 0.00000781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [7.814895070623606e-06, 7.814895070623606e-06, 7.814895070623606e-06, 7.814895070623606e-06, 7.814895070623606e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.814895070623606e-06

Optimization complete. Final v2v error: 2.4170897006988525 mm

Highest mean error: 2.6407382488250732 mm for frame 26

Lowest mean error: 2.303938627243042 mm for frame 112

Saving results

Total time: 42.60926651954651
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384777
Iteration 2/25 | Loss: 0.00122721
Iteration 3/25 | Loss: 0.00109732
Iteration 4/25 | Loss: 0.00107689
Iteration 5/25 | Loss: 0.00107320
Iteration 6/25 | Loss: 0.00107239
Iteration 7/25 | Loss: 0.00107239
Iteration 8/25 | Loss: 0.00107239
Iteration 9/25 | Loss: 0.00107239
Iteration 10/25 | Loss: 0.00107239
Iteration 11/25 | Loss: 0.00107239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010723870946094394, 0.0010723870946094394, 0.0010723870946094394, 0.0010723870946094394, 0.0010723870946094394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010723870946094394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33531189
Iteration 2/25 | Loss: 0.00096308
Iteration 3/25 | Loss: 0.00096308
Iteration 4/25 | Loss: 0.00096308
Iteration 5/25 | Loss: 0.00096308
Iteration 6/25 | Loss: 0.00096308
Iteration 7/25 | Loss: 0.00096308
Iteration 8/25 | Loss: 0.00096308
Iteration 9/25 | Loss: 0.00096308
Iteration 10/25 | Loss: 0.00096308
Iteration 11/25 | Loss: 0.00096308
Iteration 12/25 | Loss: 0.00096308
Iteration 13/25 | Loss: 0.00096308
Iteration 14/25 | Loss: 0.00096308
Iteration 15/25 | Loss: 0.00096308
Iteration 16/25 | Loss: 0.00096308
Iteration 17/25 | Loss: 0.00096308
Iteration 18/25 | Loss: 0.00096308
Iteration 19/25 | Loss: 0.00096308
Iteration 20/25 | Loss: 0.00096308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009630750864744186, 0.0009630750864744186, 0.0009630750864744186, 0.0009630750864744186, 0.0009630750864744186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009630750864744186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096308
Iteration 2/1000 | Loss: 0.00004363
Iteration 3/1000 | Loss: 0.00003035
Iteration 4/1000 | Loss: 0.00002143
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001531
Iteration 9/1000 | Loss: 0.00001491
Iteration 10/1000 | Loss: 0.00001445
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001408
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001353
Iteration 18/1000 | Loss: 0.00001350
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001347
Iteration 22/1000 | Loss: 0.00001346
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001339
Iteration 25/1000 | Loss: 0.00001338
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001336
Iteration 29/1000 | Loss: 0.00001336
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001334
Iteration 32/1000 | Loss: 0.00001334
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001332
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001327
Iteration 47/1000 | Loss: 0.00001327
Iteration 48/1000 | Loss: 0.00001326
Iteration 49/1000 | Loss: 0.00001326
Iteration 50/1000 | Loss: 0.00001326
Iteration 51/1000 | Loss: 0.00001325
Iteration 52/1000 | Loss: 0.00001325
Iteration 53/1000 | Loss: 0.00001325
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001324
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001323
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001322
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001322
Iteration 65/1000 | Loss: 0.00001322
Iteration 66/1000 | Loss: 0.00001322
Iteration 67/1000 | Loss: 0.00001322
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001321
Iteration 75/1000 | Loss: 0.00001321
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001320
Iteration 78/1000 | Loss: 0.00001320
Iteration 79/1000 | Loss: 0.00001319
Iteration 80/1000 | Loss: 0.00001319
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001318
Iteration 87/1000 | Loss: 0.00001318
Iteration 88/1000 | Loss: 0.00001318
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001317
Iteration 91/1000 | Loss: 0.00001317
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001317
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001316
Iteration 99/1000 | Loss: 0.00001316
Iteration 100/1000 | Loss: 0.00001316
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001315
Iteration 103/1000 | Loss: 0.00001315
Iteration 104/1000 | Loss: 0.00001315
Iteration 105/1000 | Loss: 0.00001315
Iteration 106/1000 | Loss: 0.00001315
Iteration 107/1000 | Loss: 0.00001314
Iteration 108/1000 | Loss: 0.00001314
Iteration 109/1000 | Loss: 0.00001314
Iteration 110/1000 | Loss: 0.00001314
Iteration 111/1000 | Loss: 0.00001313
Iteration 112/1000 | Loss: 0.00001313
Iteration 113/1000 | Loss: 0.00001313
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001311
Iteration 118/1000 | Loss: 0.00001311
Iteration 119/1000 | Loss: 0.00001311
Iteration 120/1000 | Loss: 0.00001311
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001310
Iteration 124/1000 | Loss: 0.00001310
Iteration 125/1000 | Loss: 0.00001310
Iteration 126/1000 | Loss: 0.00001310
Iteration 127/1000 | Loss: 0.00001310
Iteration 128/1000 | Loss: 0.00001310
Iteration 129/1000 | Loss: 0.00001309
Iteration 130/1000 | Loss: 0.00001309
Iteration 131/1000 | Loss: 0.00001309
Iteration 132/1000 | Loss: 0.00001309
Iteration 133/1000 | Loss: 0.00001309
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00001309
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001308
Iteration 138/1000 | Loss: 0.00001308
Iteration 139/1000 | Loss: 0.00001308
Iteration 140/1000 | Loss: 0.00001308
Iteration 141/1000 | Loss: 0.00001308
Iteration 142/1000 | Loss: 0.00001308
Iteration 143/1000 | Loss: 0.00001308
Iteration 144/1000 | Loss: 0.00001308
Iteration 145/1000 | Loss: 0.00001307
Iteration 146/1000 | Loss: 0.00001307
Iteration 147/1000 | Loss: 0.00001307
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001307
Iteration 150/1000 | Loss: 0.00001307
Iteration 151/1000 | Loss: 0.00001307
Iteration 152/1000 | Loss: 0.00001307
Iteration 153/1000 | Loss: 0.00001307
Iteration 154/1000 | Loss: 0.00001307
Iteration 155/1000 | Loss: 0.00001307
Iteration 156/1000 | Loss: 0.00001307
Iteration 157/1000 | Loss: 0.00001307
Iteration 158/1000 | Loss: 0.00001307
Iteration 159/1000 | Loss: 0.00001306
Iteration 160/1000 | Loss: 0.00001306
Iteration 161/1000 | Loss: 0.00001306
Iteration 162/1000 | Loss: 0.00001306
Iteration 163/1000 | Loss: 0.00001306
Iteration 164/1000 | Loss: 0.00001306
Iteration 165/1000 | Loss: 0.00001306
Iteration 166/1000 | Loss: 0.00001306
Iteration 167/1000 | Loss: 0.00001306
Iteration 168/1000 | Loss: 0.00001306
Iteration 169/1000 | Loss: 0.00001306
Iteration 170/1000 | Loss: 0.00001306
Iteration 171/1000 | Loss: 0.00001305
Iteration 172/1000 | Loss: 0.00001305
Iteration 173/1000 | Loss: 0.00001305
Iteration 174/1000 | Loss: 0.00001305
Iteration 175/1000 | Loss: 0.00001305
Iteration 176/1000 | Loss: 0.00001305
Iteration 177/1000 | Loss: 0.00001305
Iteration 178/1000 | Loss: 0.00001304
Iteration 179/1000 | Loss: 0.00001304
Iteration 180/1000 | Loss: 0.00001304
Iteration 181/1000 | Loss: 0.00001304
Iteration 182/1000 | Loss: 0.00001304
Iteration 183/1000 | Loss: 0.00001304
Iteration 184/1000 | Loss: 0.00001304
Iteration 185/1000 | Loss: 0.00001303
Iteration 186/1000 | Loss: 0.00001303
Iteration 187/1000 | Loss: 0.00001303
Iteration 188/1000 | Loss: 0.00001303
Iteration 189/1000 | Loss: 0.00001303
Iteration 190/1000 | Loss: 0.00001303
Iteration 191/1000 | Loss: 0.00001303
Iteration 192/1000 | Loss: 0.00001303
Iteration 193/1000 | Loss: 0.00001302
Iteration 194/1000 | Loss: 0.00001302
Iteration 195/1000 | Loss: 0.00001302
Iteration 196/1000 | Loss: 0.00001302
Iteration 197/1000 | Loss: 0.00001302
Iteration 198/1000 | Loss: 0.00001302
Iteration 199/1000 | Loss: 0.00001302
Iteration 200/1000 | Loss: 0.00001302
Iteration 201/1000 | Loss: 0.00001302
Iteration 202/1000 | Loss: 0.00001302
Iteration 203/1000 | Loss: 0.00001302
Iteration 204/1000 | Loss: 0.00001302
Iteration 205/1000 | Loss: 0.00001302
Iteration 206/1000 | Loss: 0.00001302
Iteration 207/1000 | Loss: 0.00001302
Iteration 208/1000 | Loss: 0.00001302
Iteration 209/1000 | Loss: 0.00001301
Iteration 210/1000 | Loss: 0.00001301
Iteration 211/1000 | Loss: 0.00001301
Iteration 212/1000 | Loss: 0.00001301
Iteration 213/1000 | Loss: 0.00001301
Iteration 214/1000 | Loss: 0.00001301
Iteration 215/1000 | Loss: 0.00001301
Iteration 216/1000 | Loss: 0.00001301
Iteration 217/1000 | Loss: 0.00001301
Iteration 218/1000 | Loss: 0.00001301
Iteration 219/1000 | Loss: 0.00001301
Iteration 220/1000 | Loss: 0.00001301
Iteration 221/1000 | Loss: 0.00001301
Iteration 222/1000 | Loss: 0.00001301
Iteration 223/1000 | Loss: 0.00001301
Iteration 224/1000 | Loss: 0.00001301
Iteration 225/1000 | Loss: 0.00001300
Iteration 226/1000 | Loss: 0.00001300
Iteration 227/1000 | Loss: 0.00001300
Iteration 228/1000 | Loss: 0.00001300
Iteration 229/1000 | Loss: 0.00001300
Iteration 230/1000 | Loss: 0.00001300
Iteration 231/1000 | Loss: 0.00001300
Iteration 232/1000 | Loss: 0.00001300
Iteration 233/1000 | Loss: 0.00001300
Iteration 234/1000 | Loss: 0.00001300
Iteration 235/1000 | Loss: 0.00001300
Iteration 236/1000 | Loss: 0.00001300
Iteration 237/1000 | Loss: 0.00001300
Iteration 238/1000 | Loss: 0.00001300
Iteration 239/1000 | Loss: 0.00001300
Iteration 240/1000 | Loss: 0.00001300
Iteration 241/1000 | Loss: 0.00001300
Iteration 242/1000 | Loss: 0.00001300
Iteration 243/1000 | Loss: 0.00001300
Iteration 244/1000 | Loss: 0.00001300
Iteration 245/1000 | Loss: 0.00001300
Iteration 246/1000 | Loss: 0.00001299
Iteration 247/1000 | Loss: 0.00001299
Iteration 248/1000 | Loss: 0.00001299
Iteration 249/1000 | Loss: 0.00001299
Iteration 250/1000 | Loss: 0.00001299
Iteration 251/1000 | Loss: 0.00001299
Iteration 252/1000 | Loss: 0.00001299
Iteration 253/1000 | Loss: 0.00001299
Iteration 254/1000 | Loss: 0.00001299
Iteration 255/1000 | Loss: 0.00001299
Iteration 256/1000 | Loss: 0.00001299
Iteration 257/1000 | Loss: 0.00001299
Iteration 258/1000 | Loss: 0.00001299
Iteration 259/1000 | Loss: 0.00001299
Iteration 260/1000 | Loss: 0.00001299
Iteration 261/1000 | Loss: 0.00001298
Iteration 262/1000 | Loss: 0.00001298
Iteration 263/1000 | Loss: 0.00001298
Iteration 264/1000 | Loss: 0.00001298
Iteration 265/1000 | Loss: 0.00001298
Iteration 266/1000 | Loss: 0.00001298
Iteration 267/1000 | Loss: 0.00001298
Iteration 268/1000 | Loss: 0.00001298
Iteration 269/1000 | Loss: 0.00001298
Iteration 270/1000 | Loss: 0.00001298
Iteration 271/1000 | Loss: 0.00001298
Iteration 272/1000 | Loss: 0.00001298
Iteration 273/1000 | Loss: 0.00001298
Iteration 274/1000 | Loss: 0.00001298
Iteration 275/1000 | Loss: 0.00001298
Iteration 276/1000 | Loss: 0.00001298
Iteration 277/1000 | Loss: 0.00001297
Iteration 278/1000 | Loss: 0.00001297
Iteration 279/1000 | Loss: 0.00001297
Iteration 280/1000 | Loss: 0.00001297
Iteration 281/1000 | Loss: 0.00001297
Iteration 282/1000 | Loss: 0.00001297
Iteration 283/1000 | Loss: 0.00001297
Iteration 284/1000 | Loss: 0.00001297
Iteration 285/1000 | Loss: 0.00001297
Iteration 286/1000 | Loss: 0.00001297
Iteration 287/1000 | Loss: 0.00001297
Iteration 288/1000 | Loss: 0.00001297
Iteration 289/1000 | Loss: 0.00001297
Iteration 290/1000 | Loss: 0.00001297
Iteration 291/1000 | Loss: 0.00001297
Iteration 292/1000 | Loss: 0.00001296
Iteration 293/1000 | Loss: 0.00001296
Iteration 294/1000 | Loss: 0.00001296
Iteration 295/1000 | Loss: 0.00001296
Iteration 296/1000 | Loss: 0.00001296
Iteration 297/1000 | Loss: 0.00001296
Iteration 298/1000 | Loss: 0.00001296
Iteration 299/1000 | Loss: 0.00001296
Iteration 300/1000 | Loss: 0.00001296
Iteration 301/1000 | Loss: 0.00001296
Iteration 302/1000 | Loss: 0.00001296
Iteration 303/1000 | Loss: 0.00001296
Iteration 304/1000 | Loss: 0.00001296
Iteration 305/1000 | Loss: 0.00001296
Iteration 306/1000 | Loss: 0.00001296
Iteration 307/1000 | Loss: 0.00001296
Iteration 308/1000 | Loss: 0.00001296
Iteration 309/1000 | Loss: 0.00001296
Iteration 310/1000 | Loss: 0.00001296
Iteration 311/1000 | Loss: 0.00001296
Iteration 312/1000 | Loss: 0.00001296
Iteration 313/1000 | Loss: 0.00001296
Iteration 314/1000 | Loss: 0.00001296
Iteration 315/1000 | Loss: 0.00001296
Iteration 316/1000 | Loss: 0.00001296
Iteration 317/1000 | Loss: 0.00001296
Iteration 318/1000 | Loss: 0.00001296
Iteration 319/1000 | Loss: 0.00001296
Iteration 320/1000 | Loss: 0.00001296
Iteration 321/1000 | Loss: 0.00001296
Iteration 322/1000 | Loss: 0.00001296
Iteration 323/1000 | Loss: 0.00001296
Iteration 324/1000 | Loss: 0.00001296
Iteration 325/1000 | Loss: 0.00001296
Iteration 326/1000 | Loss: 0.00001296
Iteration 327/1000 | Loss: 0.00001296
Iteration 328/1000 | Loss: 0.00001296
Iteration 329/1000 | Loss: 0.00001296
Iteration 330/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [1.2956025784660596e-05, 1.2956025784660596e-05, 1.2956025784660596e-05, 1.2956025784660596e-05, 1.2956025784660596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2956025784660596e-05

Optimization complete. Final v2v error: 3.034482955932617 mm

Highest mean error: 3.8850150108337402 mm for frame 24

Lowest mean error: 2.519706964492798 mm for frame 11

Saving results

Total time: 51.84435200691223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860609
Iteration 2/25 | Loss: 0.00130832
Iteration 3/25 | Loss: 0.00122126
Iteration 4/25 | Loss: 0.00120054
Iteration 5/25 | Loss: 0.00119391
Iteration 6/25 | Loss: 0.00119274
Iteration 7/25 | Loss: 0.00119274
Iteration 8/25 | Loss: 0.00119274
Iteration 9/25 | Loss: 0.00119274
Iteration 10/25 | Loss: 0.00119274
Iteration 11/25 | Loss: 0.00119274
Iteration 12/25 | Loss: 0.00119274
Iteration 13/25 | Loss: 0.00119274
Iteration 14/25 | Loss: 0.00119274
Iteration 15/25 | Loss: 0.00119274
Iteration 16/25 | Loss: 0.00119274
Iteration 17/25 | Loss: 0.00119274
Iteration 18/25 | Loss: 0.00119274
Iteration 19/25 | Loss: 0.00119274
Iteration 20/25 | Loss: 0.00119274
Iteration 21/25 | Loss: 0.00119274
Iteration 22/25 | Loss: 0.00119274
Iteration 23/25 | Loss: 0.00119274
Iteration 24/25 | Loss: 0.00119274
Iteration 25/25 | Loss: 0.00119274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28146076
Iteration 2/25 | Loss: 0.00109977
Iteration 3/25 | Loss: 0.00109969
Iteration 4/25 | Loss: 0.00109969
Iteration 5/25 | Loss: 0.00109969
Iteration 6/25 | Loss: 0.00109969
Iteration 7/25 | Loss: 0.00109969
Iteration 8/25 | Loss: 0.00109969
Iteration 9/25 | Loss: 0.00109969
Iteration 10/25 | Loss: 0.00109969
Iteration 11/25 | Loss: 0.00109969
Iteration 12/25 | Loss: 0.00109969
Iteration 13/25 | Loss: 0.00109969
Iteration 14/25 | Loss: 0.00109969
Iteration 15/25 | Loss: 0.00109969
Iteration 16/25 | Loss: 0.00109969
Iteration 17/25 | Loss: 0.00109969
Iteration 18/25 | Loss: 0.00109969
Iteration 19/25 | Loss: 0.00109969
Iteration 20/25 | Loss: 0.00109969
Iteration 21/25 | Loss: 0.00109969
Iteration 22/25 | Loss: 0.00109969
Iteration 23/25 | Loss: 0.00109969
Iteration 24/25 | Loss: 0.00109969
Iteration 25/25 | Loss: 0.00109969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109969
Iteration 2/1000 | Loss: 0.00006713
Iteration 3/1000 | Loss: 0.00003844
Iteration 4/1000 | Loss: 0.00003012
Iteration 5/1000 | Loss: 0.00002650
Iteration 6/1000 | Loss: 0.00002524
Iteration 7/1000 | Loss: 0.00002427
Iteration 8/1000 | Loss: 0.00002362
Iteration 9/1000 | Loss: 0.00002299
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00002216
Iteration 12/1000 | Loss: 0.00002191
Iteration 13/1000 | Loss: 0.00002170
Iteration 14/1000 | Loss: 0.00002158
Iteration 15/1000 | Loss: 0.00002158
Iteration 16/1000 | Loss: 0.00002146
Iteration 17/1000 | Loss: 0.00002139
Iteration 18/1000 | Loss: 0.00002138
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002136
Iteration 21/1000 | Loss: 0.00002136
Iteration 22/1000 | Loss: 0.00002135
Iteration 23/1000 | Loss: 0.00002135
Iteration 24/1000 | Loss: 0.00002134
Iteration 25/1000 | Loss: 0.00002134
Iteration 26/1000 | Loss: 0.00002134
Iteration 27/1000 | Loss: 0.00002133
Iteration 28/1000 | Loss: 0.00002132
Iteration 29/1000 | Loss: 0.00002132
Iteration 30/1000 | Loss: 0.00002132
Iteration 31/1000 | Loss: 0.00002131
Iteration 32/1000 | Loss: 0.00002131
Iteration 33/1000 | Loss: 0.00002131
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002129
Iteration 36/1000 | Loss: 0.00002129
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002127
Iteration 40/1000 | Loss: 0.00002127
Iteration 41/1000 | Loss: 0.00002126
Iteration 42/1000 | Loss: 0.00002126
Iteration 43/1000 | Loss: 0.00002126
Iteration 44/1000 | Loss: 0.00002125
Iteration 45/1000 | Loss: 0.00002125
Iteration 46/1000 | Loss: 0.00002124
Iteration 47/1000 | Loss: 0.00002124
Iteration 48/1000 | Loss: 0.00002124
Iteration 49/1000 | Loss: 0.00002123
Iteration 50/1000 | Loss: 0.00002122
Iteration 51/1000 | Loss: 0.00002122
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002121
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00002120
Iteration 56/1000 | Loss: 0.00002120
Iteration 57/1000 | Loss: 0.00002120
Iteration 58/1000 | Loss: 0.00002119
Iteration 59/1000 | Loss: 0.00002119
Iteration 60/1000 | Loss: 0.00002118
Iteration 61/1000 | Loss: 0.00002118
Iteration 62/1000 | Loss: 0.00002118
Iteration 63/1000 | Loss: 0.00002117
Iteration 64/1000 | Loss: 0.00002117
Iteration 65/1000 | Loss: 0.00002116
Iteration 66/1000 | Loss: 0.00002116
Iteration 67/1000 | Loss: 0.00002115
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002114
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002113
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002111
Iteration 76/1000 | Loss: 0.00002111
Iteration 77/1000 | Loss: 0.00002110
Iteration 78/1000 | Loss: 0.00002110
Iteration 79/1000 | Loss: 0.00002110
Iteration 80/1000 | Loss: 0.00002109
Iteration 81/1000 | Loss: 0.00002109
Iteration 82/1000 | Loss: 0.00002109
Iteration 83/1000 | Loss: 0.00002109
Iteration 84/1000 | Loss: 0.00002108
Iteration 85/1000 | Loss: 0.00002108
Iteration 86/1000 | Loss: 0.00002108
Iteration 87/1000 | Loss: 0.00002107
Iteration 88/1000 | Loss: 0.00002107
Iteration 89/1000 | Loss: 0.00002106
Iteration 90/1000 | Loss: 0.00002106
Iteration 91/1000 | Loss: 0.00002106
Iteration 92/1000 | Loss: 0.00002105
Iteration 93/1000 | Loss: 0.00002105
Iteration 94/1000 | Loss: 0.00002104
Iteration 95/1000 | Loss: 0.00002104
Iteration 96/1000 | Loss: 0.00002104
Iteration 97/1000 | Loss: 0.00002103
Iteration 98/1000 | Loss: 0.00002102
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002102
Iteration 102/1000 | Loss: 0.00002101
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002100
Iteration 106/1000 | Loss: 0.00002100
Iteration 107/1000 | Loss: 0.00002100
Iteration 108/1000 | Loss: 0.00002099
Iteration 109/1000 | Loss: 0.00002099
Iteration 110/1000 | Loss: 0.00002099
Iteration 111/1000 | Loss: 0.00002098
Iteration 112/1000 | Loss: 0.00002098
Iteration 113/1000 | Loss: 0.00002098
Iteration 114/1000 | Loss: 0.00002098
Iteration 115/1000 | Loss: 0.00002097
Iteration 116/1000 | Loss: 0.00002097
Iteration 117/1000 | Loss: 0.00002097
Iteration 118/1000 | Loss: 0.00002097
Iteration 119/1000 | Loss: 0.00002097
Iteration 120/1000 | Loss: 0.00002097
Iteration 121/1000 | Loss: 0.00002097
Iteration 122/1000 | Loss: 0.00002097
Iteration 123/1000 | Loss: 0.00002096
Iteration 124/1000 | Loss: 0.00002096
Iteration 125/1000 | Loss: 0.00002096
Iteration 126/1000 | Loss: 0.00002096
Iteration 127/1000 | Loss: 0.00002096
Iteration 128/1000 | Loss: 0.00002096
Iteration 129/1000 | Loss: 0.00002096
Iteration 130/1000 | Loss: 0.00002095
Iteration 131/1000 | Loss: 0.00002095
Iteration 132/1000 | Loss: 0.00002095
Iteration 133/1000 | Loss: 0.00002095
Iteration 134/1000 | Loss: 0.00002095
Iteration 135/1000 | Loss: 0.00002095
Iteration 136/1000 | Loss: 0.00002095
Iteration 137/1000 | Loss: 0.00002095
Iteration 138/1000 | Loss: 0.00002094
Iteration 139/1000 | Loss: 0.00002094
Iteration 140/1000 | Loss: 0.00002094
Iteration 141/1000 | Loss: 0.00002094
Iteration 142/1000 | Loss: 0.00002094
Iteration 143/1000 | Loss: 0.00002093
Iteration 144/1000 | Loss: 0.00002093
Iteration 145/1000 | Loss: 0.00002093
Iteration 146/1000 | Loss: 0.00002092
Iteration 147/1000 | Loss: 0.00002092
Iteration 148/1000 | Loss: 0.00002092
Iteration 149/1000 | Loss: 0.00002092
Iteration 150/1000 | Loss: 0.00002092
Iteration 151/1000 | Loss: 0.00002092
Iteration 152/1000 | Loss: 0.00002091
Iteration 153/1000 | Loss: 0.00002091
Iteration 154/1000 | Loss: 0.00002091
Iteration 155/1000 | Loss: 0.00002091
Iteration 156/1000 | Loss: 0.00002091
Iteration 157/1000 | Loss: 0.00002091
Iteration 158/1000 | Loss: 0.00002091
Iteration 159/1000 | Loss: 0.00002091
Iteration 160/1000 | Loss: 0.00002091
Iteration 161/1000 | Loss: 0.00002091
Iteration 162/1000 | Loss: 0.00002091
Iteration 163/1000 | Loss: 0.00002091
Iteration 164/1000 | Loss: 0.00002091
Iteration 165/1000 | Loss: 0.00002091
Iteration 166/1000 | Loss: 0.00002091
Iteration 167/1000 | Loss: 0.00002091
Iteration 168/1000 | Loss: 0.00002091
Iteration 169/1000 | Loss: 0.00002091
Iteration 170/1000 | Loss: 0.00002091
Iteration 171/1000 | Loss: 0.00002091
Iteration 172/1000 | Loss: 0.00002091
Iteration 173/1000 | Loss: 0.00002091
Iteration 174/1000 | Loss: 0.00002091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.0906474674120545e-05, 2.0906474674120545e-05, 2.0906474674120545e-05, 2.0906474674120545e-05, 2.0906474674120545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0906474674120545e-05

Optimization complete. Final v2v error: 3.8692727088928223 mm

Highest mean error: 4.179394245147705 mm for frame 108

Lowest mean error: 3.418928861618042 mm for frame 0

Saving results

Total time: 42.94815182685852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400967
Iteration 2/25 | Loss: 0.00119613
Iteration 3/25 | Loss: 0.00109051
Iteration 4/25 | Loss: 0.00107608
Iteration 5/25 | Loss: 0.00107360
Iteration 6/25 | Loss: 0.00107355
Iteration 7/25 | Loss: 0.00107355
Iteration 8/25 | Loss: 0.00107355
Iteration 9/25 | Loss: 0.00107355
Iteration 10/25 | Loss: 0.00107355
Iteration 11/25 | Loss: 0.00107355
Iteration 12/25 | Loss: 0.00107355
Iteration 13/25 | Loss: 0.00107355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001073552411980927, 0.001073552411980927, 0.001073552411980927, 0.001073552411980927, 0.001073552411980927]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001073552411980927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33859932
Iteration 2/25 | Loss: 0.00071302
Iteration 3/25 | Loss: 0.00071301
Iteration 4/25 | Loss: 0.00071301
Iteration 5/25 | Loss: 0.00071301
Iteration 6/25 | Loss: 0.00071301
Iteration 7/25 | Loss: 0.00071301
Iteration 8/25 | Loss: 0.00071301
Iteration 9/25 | Loss: 0.00071301
Iteration 10/25 | Loss: 0.00071301
Iteration 11/25 | Loss: 0.00071301
Iteration 12/25 | Loss: 0.00071301
Iteration 13/25 | Loss: 0.00071301
Iteration 14/25 | Loss: 0.00071301
Iteration 15/25 | Loss: 0.00071301
Iteration 16/25 | Loss: 0.00071301
Iteration 17/25 | Loss: 0.00071301
Iteration 18/25 | Loss: 0.00071301
Iteration 19/25 | Loss: 0.00071301
Iteration 20/25 | Loss: 0.00071301
Iteration 21/25 | Loss: 0.00071301
Iteration 22/25 | Loss: 0.00071301
Iteration 23/25 | Loss: 0.00071301
Iteration 24/25 | Loss: 0.00071301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007130088051781058, 0.0007130088051781058, 0.0007130088051781058, 0.0007130088051781058, 0.0007130088051781058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007130088051781058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071301
Iteration 2/1000 | Loss: 0.00002596
Iteration 3/1000 | Loss: 0.00001917
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001314
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001257
Iteration 12/1000 | Loss: 0.00001246
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001169
Iteration 35/1000 | Loss: 0.00001169
Iteration 36/1000 | Loss: 0.00001169
Iteration 37/1000 | Loss: 0.00001169
Iteration 38/1000 | Loss: 0.00001168
Iteration 39/1000 | Loss: 0.00001168
Iteration 40/1000 | Loss: 0.00001167
Iteration 41/1000 | Loss: 0.00001167
Iteration 42/1000 | Loss: 0.00001166
Iteration 43/1000 | Loss: 0.00001166
Iteration 44/1000 | Loss: 0.00001166
Iteration 45/1000 | Loss: 0.00001166
Iteration 46/1000 | Loss: 0.00001165
Iteration 47/1000 | Loss: 0.00001165
Iteration 48/1000 | Loss: 0.00001165
Iteration 49/1000 | Loss: 0.00001165
Iteration 50/1000 | Loss: 0.00001165
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001164
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001163
Iteration 55/1000 | Loss: 0.00001161
Iteration 56/1000 | Loss: 0.00001161
Iteration 57/1000 | Loss: 0.00001161
Iteration 58/1000 | Loss: 0.00001160
Iteration 59/1000 | Loss: 0.00001160
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001159
Iteration 64/1000 | Loss: 0.00001158
Iteration 65/1000 | Loss: 0.00001158
Iteration 66/1000 | Loss: 0.00001158
Iteration 67/1000 | Loss: 0.00001158
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001157
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001156
Iteration 83/1000 | Loss: 0.00001156
Iteration 84/1000 | Loss: 0.00001156
Iteration 85/1000 | Loss: 0.00001156
Iteration 86/1000 | Loss: 0.00001156
Iteration 87/1000 | Loss: 0.00001155
Iteration 88/1000 | Loss: 0.00001155
Iteration 89/1000 | Loss: 0.00001154
Iteration 90/1000 | Loss: 0.00001154
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001152
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001150
Iteration 105/1000 | Loss: 0.00001150
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001149
Iteration 115/1000 | Loss: 0.00001149
Iteration 116/1000 | Loss: 0.00001149
Iteration 117/1000 | Loss: 0.00001149
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001148
Iteration 126/1000 | Loss: 0.00001148
Iteration 127/1000 | Loss: 0.00001148
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001147
Iteration 133/1000 | Loss: 0.00001147
Iteration 134/1000 | Loss: 0.00001147
Iteration 135/1000 | Loss: 0.00001147
Iteration 136/1000 | Loss: 0.00001147
Iteration 137/1000 | Loss: 0.00001147
Iteration 138/1000 | Loss: 0.00001147
Iteration 139/1000 | Loss: 0.00001146
Iteration 140/1000 | Loss: 0.00001146
Iteration 141/1000 | Loss: 0.00001146
Iteration 142/1000 | Loss: 0.00001146
Iteration 143/1000 | Loss: 0.00001146
Iteration 144/1000 | Loss: 0.00001146
Iteration 145/1000 | Loss: 0.00001146
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001145
Iteration 148/1000 | Loss: 0.00001145
Iteration 149/1000 | Loss: 0.00001145
Iteration 150/1000 | Loss: 0.00001145
Iteration 151/1000 | Loss: 0.00001144
Iteration 152/1000 | Loss: 0.00001144
Iteration 153/1000 | Loss: 0.00001144
Iteration 154/1000 | Loss: 0.00001144
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001142
Iteration 160/1000 | Loss: 0.00001142
Iteration 161/1000 | Loss: 0.00001142
Iteration 162/1000 | Loss: 0.00001142
Iteration 163/1000 | Loss: 0.00001142
Iteration 164/1000 | Loss: 0.00001142
Iteration 165/1000 | Loss: 0.00001142
Iteration 166/1000 | Loss: 0.00001141
Iteration 167/1000 | Loss: 0.00001141
Iteration 168/1000 | Loss: 0.00001141
Iteration 169/1000 | Loss: 0.00001141
Iteration 170/1000 | Loss: 0.00001141
Iteration 171/1000 | Loss: 0.00001141
Iteration 172/1000 | Loss: 0.00001141
Iteration 173/1000 | Loss: 0.00001141
Iteration 174/1000 | Loss: 0.00001141
Iteration 175/1000 | Loss: 0.00001140
Iteration 176/1000 | Loss: 0.00001140
Iteration 177/1000 | Loss: 0.00001140
Iteration 178/1000 | Loss: 0.00001140
Iteration 179/1000 | Loss: 0.00001140
Iteration 180/1000 | Loss: 0.00001140
Iteration 181/1000 | Loss: 0.00001140
Iteration 182/1000 | Loss: 0.00001140
Iteration 183/1000 | Loss: 0.00001140
Iteration 184/1000 | Loss: 0.00001140
Iteration 185/1000 | Loss: 0.00001140
Iteration 186/1000 | Loss: 0.00001139
Iteration 187/1000 | Loss: 0.00001139
Iteration 188/1000 | Loss: 0.00001139
Iteration 189/1000 | Loss: 0.00001139
Iteration 190/1000 | Loss: 0.00001139
Iteration 191/1000 | Loss: 0.00001139
Iteration 192/1000 | Loss: 0.00001139
Iteration 193/1000 | Loss: 0.00001139
Iteration 194/1000 | Loss: 0.00001139
Iteration 195/1000 | Loss: 0.00001139
Iteration 196/1000 | Loss: 0.00001139
Iteration 197/1000 | Loss: 0.00001139
Iteration 198/1000 | Loss: 0.00001139
Iteration 199/1000 | Loss: 0.00001139
Iteration 200/1000 | Loss: 0.00001139
Iteration 201/1000 | Loss: 0.00001139
Iteration 202/1000 | Loss: 0.00001139
Iteration 203/1000 | Loss: 0.00001139
Iteration 204/1000 | Loss: 0.00001139
Iteration 205/1000 | Loss: 0.00001139
Iteration 206/1000 | Loss: 0.00001139
Iteration 207/1000 | Loss: 0.00001139
Iteration 208/1000 | Loss: 0.00001139
Iteration 209/1000 | Loss: 0.00001139
Iteration 210/1000 | Loss: 0.00001139
Iteration 211/1000 | Loss: 0.00001139
Iteration 212/1000 | Loss: 0.00001139
Iteration 213/1000 | Loss: 0.00001139
Iteration 214/1000 | Loss: 0.00001139
Iteration 215/1000 | Loss: 0.00001139
Iteration 216/1000 | Loss: 0.00001139
Iteration 217/1000 | Loss: 0.00001139
Iteration 218/1000 | Loss: 0.00001139
Iteration 219/1000 | Loss: 0.00001139
Iteration 220/1000 | Loss: 0.00001139
Iteration 221/1000 | Loss: 0.00001139
Iteration 222/1000 | Loss: 0.00001139
Iteration 223/1000 | Loss: 0.00001139
Iteration 224/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.1385367542970926e-05, 1.1385367542970926e-05, 1.1385367542970926e-05, 1.1385367542970926e-05, 1.1385367542970926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1385367542970926e-05

Optimization complete. Final v2v error: 2.8518192768096924 mm

Highest mean error: 3.4492766857147217 mm for frame 104

Lowest mean error: 2.4841699600219727 mm for frame 28

Saving results

Total time: 45.758758306503296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814046
Iteration 2/25 | Loss: 0.00156881
Iteration 3/25 | Loss: 0.00128230
Iteration 4/25 | Loss: 0.00125997
Iteration 5/25 | Loss: 0.00125803
Iteration 6/25 | Loss: 0.00125803
Iteration 7/25 | Loss: 0.00125803
Iteration 8/25 | Loss: 0.00125803
Iteration 9/25 | Loss: 0.00125803
Iteration 10/25 | Loss: 0.00125803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012580291368067265, 0.0012580291368067265, 0.0012580291368067265, 0.0012580291368067265, 0.0012580291368067265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012580291368067265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24102473
Iteration 2/25 | Loss: 0.00068785
Iteration 3/25 | Loss: 0.00068782
Iteration 4/25 | Loss: 0.00068782
Iteration 5/25 | Loss: 0.00068782
Iteration 6/25 | Loss: 0.00068782
Iteration 7/25 | Loss: 0.00068782
Iteration 8/25 | Loss: 0.00068782
Iteration 9/25 | Loss: 0.00068782
Iteration 10/25 | Loss: 0.00068782
Iteration 11/25 | Loss: 0.00068782
Iteration 12/25 | Loss: 0.00068782
Iteration 13/25 | Loss: 0.00068782
Iteration 14/25 | Loss: 0.00068782
Iteration 15/25 | Loss: 0.00068782
Iteration 16/25 | Loss: 0.00068782
Iteration 17/25 | Loss: 0.00068782
Iteration 18/25 | Loss: 0.00068782
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006878202548250556, 0.0006878202548250556, 0.0006878202548250556, 0.0006878202548250556, 0.0006878202548250556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006878202548250556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068782
Iteration 2/1000 | Loss: 0.00003756
Iteration 3/1000 | Loss: 0.00002378
Iteration 4/1000 | Loss: 0.00002182
Iteration 5/1000 | Loss: 0.00002116
Iteration 6/1000 | Loss: 0.00002064
Iteration 7/1000 | Loss: 0.00002035
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001981
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001968
Iteration 12/1000 | Loss: 0.00001966
Iteration 13/1000 | Loss: 0.00001959
Iteration 14/1000 | Loss: 0.00001957
Iteration 15/1000 | Loss: 0.00001957
Iteration 16/1000 | Loss: 0.00001957
Iteration 17/1000 | Loss: 0.00001956
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001955
Iteration 20/1000 | Loss: 0.00001955
Iteration 21/1000 | Loss: 0.00001955
Iteration 22/1000 | Loss: 0.00001955
Iteration 23/1000 | Loss: 0.00001954
Iteration 24/1000 | Loss: 0.00001954
Iteration 25/1000 | Loss: 0.00001954
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001951
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001946
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001945
Iteration 32/1000 | Loss: 0.00001944
Iteration 33/1000 | Loss: 0.00001944
Iteration 34/1000 | Loss: 0.00001943
Iteration 35/1000 | Loss: 0.00001943
Iteration 36/1000 | Loss: 0.00001942
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001940
Iteration 45/1000 | Loss: 0.00001940
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001940
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001940
Iteration 51/1000 | Loss: 0.00001940
Iteration 52/1000 | Loss: 0.00001940
Iteration 53/1000 | Loss: 0.00001939
Iteration 54/1000 | Loss: 0.00001939
Iteration 55/1000 | Loss: 0.00001939
Iteration 56/1000 | Loss: 0.00001939
Iteration 57/1000 | Loss: 0.00001939
Iteration 58/1000 | Loss: 0.00001939
Iteration 59/1000 | Loss: 0.00001938
Iteration 60/1000 | Loss: 0.00001938
Iteration 61/1000 | Loss: 0.00001938
Iteration 62/1000 | Loss: 0.00001938
Iteration 63/1000 | Loss: 0.00001938
Iteration 64/1000 | Loss: 0.00001938
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001937
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001937
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001936
Iteration 75/1000 | Loss: 0.00001936
Iteration 76/1000 | Loss: 0.00001936
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001936
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001935
Iteration 82/1000 | Loss: 0.00001935
Iteration 83/1000 | Loss: 0.00001935
Iteration 84/1000 | Loss: 0.00001935
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001934
Iteration 87/1000 | Loss: 0.00001934
Iteration 88/1000 | Loss: 0.00001934
Iteration 89/1000 | Loss: 0.00001934
Iteration 90/1000 | Loss: 0.00001934
Iteration 91/1000 | Loss: 0.00001933
Iteration 92/1000 | Loss: 0.00001933
Iteration 93/1000 | Loss: 0.00001933
Iteration 94/1000 | Loss: 0.00001933
Iteration 95/1000 | Loss: 0.00001932
Iteration 96/1000 | Loss: 0.00001932
Iteration 97/1000 | Loss: 0.00001932
Iteration 98/1000 | Loss: 0.00001931
Iteration 99/1000 | Loss: 0.00001931
Iteration 100/1000 | Loss: 0.00001931
Iteration 101/1000 | Loss: 0.00001930
Iteration 102/1000 | Loss: 0.00001930
Iteration 103/1000 | Loss: 0.00001930
Iteration 104/1000 | Loss: 0.00001930
Iteration 105/1000 | Loss: 0.00001930
Iteration 106/1000 | Loss: 0.00001930
Iteration 107/1000 | Loss: 0.00001929
Iteration 108/1000 | Loss: 0.00001929
Iteration 109/1000 | Loss: 0.00001929
Iteration 110/1000 | Loss: 0.00001929
Iteration 111/1000 | Loss: 0.00001929
Iteration 112/1000 | Loss: 0.00001929
Iteration 113/1000 | Loss: 0.00001929
Iteration 114/1000 | Loss: 0.00001929
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001928
Iteration 119/1000 | Loss: 0.00001928
Iteration 120/1000 | Loss: 0.00001928
Iteration 121/1000 | Loss: 0.00001928
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001926
Iteration 126/1000 | Loss: 0.00001926
Iteration 127/1000 | Loss: 0.00001926
Iteration 128/1000 | Loss: 0.00001926
Iteration 129/1000 | Loss: 0.00001926
Iteration 130/1000 | Loss: 0.00001926
Iteration 131/1000 | Loss: 0.00001926
Iteration 132/1000 | Loss: 0.00001926
Iteration 133/1000 | Loss: 0.00001925
Iteration 134/1000 | Loss: 0.00001925
Iteration 135/1000 | Loss: 0.00001925
Iteration 136/1000 | Loss: 0.00001925
Iteration 137/1000 | Loss: 0.00001924
Iteration 138/1000 | Loss: 0.00001924
Iteration 139/1000 | Loss: 0.00001924
Iteration 140/1000 | Loss: 0.00001924
Iteration 141/1000 | Loss: 0.00001924
Iteration 142/1000 | Loss: 0.00001924
Iteration 143/1000 | Loss: 0.00001924
Iteration 144/1000 | Loss: 0.00001924
Iteration 145/1000 | Loss: 0.00001924
Iteration 146/1000 | Loss: 0.00001924
Iteration 147/1000 | Loss: 0.00001924
Iteration 148/1000 | Loss: 0.00001924
Iteration 149/1000 | Loss: 0.00001924
Iteration 150/1000 | Loss: 0.00001924
Iteration 151/1000 | Loss: 0.00001924
Iteration 152/1000 | Loss: 0.00001924
Iteration 153/1000 | Loss: 0.00001924
Iteration 154/1000 | Loss: 0.00001924
Iteration 155/1000 | Loss: 0.00001924
Iteration 156/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.9242894268245436e-05, 1.9242894268245436e-05, 1.9242894268245436e-05, 1.9242894268245436e-05, 1.9242894268245436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9242894268245436e-05

Optimization complete. Final v2v error: 3.650482177734375 mm

Highest mean error: 3.9018633365631104 mm for frame 157

Lowest mean error: 3.419107437133789 mm for frame 93

Saving results

Total time: 38.22687792778015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093545
Iteration 2/25 | Loss: 0.00167998
Iteration 3/25 | Loss: 0.00129910
Iteration 4/25 | Loss: 0.00126951
Iteration 5/25 | Loss: 0.00126263
Iteration 6/25 | Loss: 0.00126079
Iteration 7/25 | Loss: 0.00126079
Iteration 8/25 | Loss: 0.00126079
Iteration 9/25 | Loss: 0.00126079
Iteration 10/25 | Loss: 0.00126079
Iteration 11/25 | Loss: 0.00126079
Iteration 12/25 | Loss: 0.00126079
Iteration 13/25 | Loss: 0.00126079
Iteration 14/25 | Loss: 0.00126079
Iteration 15/25 | Loss: 0.00126079
Iteration 16/25 | Loss: 0.00126079
Iteration 17/25 | Loss: 0.00126079
Iteration 18/25 | Loss: 0.00126079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001260793418623507, 0.001260793418623507, 0.001260793418623507, 0.001260793418623507, 0.001260793418623507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001260793418623507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15402949
Iteration 2/25 | Loss: 0.00104258
Iteration 3/25 | Loss: 0.00104256
Iteration 4/25 | Loss: 0.00104256
Iteration 5/25 | Loss: 0.00104256
Iteration 6/25 | Loss: 0.00104256
Iteration 7/25 | Loss: 0.00104256
Iteration 8/25 | Loss: 0.00104256
Iteration 9/25 | Loss: 0.00104256
Iteration 10/25 | Loss: 0.00104255
Iteration 11/25 | Loss: 0.00104255
Iteration 12/25 | Loss: 0.00104255
Iteration 13/25 | Loss: 0.00104255
Iteration 14/25 | Loss: 0.00104255
Iteration 15/25 | Loss: 0.00104255
Iteration 16/25 | Loss: 0.00104255
Iteration 17/25 | Loss: 0.00104255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010425546206533909, 0.0010425546206533909, 0.0010425546206533909, 0.0010425546206533909, 0.0010425546206533909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010425546206533909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104255
Iteration 2/1000 | Loss: 0.00007397
Iteration 3/1000 | Loss: 0.00004216
Iteration 4/1000 | Loss: 0.00003176
Iteration 5/1000 | Loss: 0.00002984
Iteration 6/1000 | Loss: 0.00002850
Iteration 7/1000 | Loss: 0.00002789
Iteration 8/1000 | Loss: 0.00002724
Iteration 9/1000 | Loss: 0.00002685
Iteration 10/1000 | Loss: 0.00002648
Iteration 11/1000 | Loss: 0.00002614
Iteration 12/1000 | Loss: 0.00002593
Iteration 13/1000 | Loss: 0.00002576
Iteration 14/1000 | Loss: 0.00002556
Iteration 15/1000 | Loss: 0.00002540
Iteration 16/1000 | Loss: 0.00002539
Iteration 17/1000 | Loss: 0.00002530
Iteration 18/1000 | Loss: 0.00002529
Iteration 19/1000 | Loss: 0.00002517
Iteration 20/1000 | Loss: 0.00002516
Iteration 21/1000 | Loss: 0.00002511
Iteration 22/1000 | Loss: 0.00002508
Iteration 23/1000 | Loss: 0.00002505
Iteration 24/1000 | Loss: 0.00002498
Iteration 25/1000 | Loss: 0.00002497
Iteration 26/1000 | Loss: 0.00002497
Iteration 27/1000 | Loss: 0.00002497
Iteration 28/1000 | Loss: 0.00002493
Iteration 29/1000 | Loss: 0.00002493
Iteration 30/1000 | Loss: 0.00002493
Iteration 31/1000 | Loss: 0.00002493
Iteration 32/1000 | Loss: 0.00002491
Iteration 33/1000 | Loss: 0.00002491
Iteration 34/1000 | Loss: 0.00002491
Iteration 35/1000 | Loss: 0.00002490
Iteration 36/1000 | Loss: 0.00002489
Iteration 37/1000 | Loss: 0.00002488
Iteration 38/1000 | Loss: 0.00002488
Iteration 39/1000 | Loss: 0.00002488
Iteration 40/1000 | Loss: 0.00002487
Iteration 41/1000 | Loss: 0.00002486
Iteration 42/1000 | Loss: 0.00002486
Iteration 43/1000 | Loss: 0.00002485
Iteration 44/1000 | Loss: 0.00002485
Iteration 45/1000 | Loss: 0.00002484
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002484
Iteration 48/1000 | Loss: 0.00002484
Iteration 49/1000 | Loss: 0.00002483
Iteration 50/1000 | Loss: 0.00002483
Iteration 51/1000 | Loss: 0.00002483
Iteration 52/1000 | Loss: 0.00002483
Iteration 53/1000 | Loss: 0.00002483
Iteration 54/1000 | Loss: 0.00002483
Iteration 55/1000 | Loss: 0.00002482
Iteration 56/1000 | Loss: 0.00002482
Iteration 57/1000 | Loss: 0.00002482
Iteration 58/1000 | Loss: 0.00002481
Iteration 59/1000 | Loss: 0.00002481
Iteration 60/1000 | Loss: 0.00002481
Iteration 61/1000 | Loss: 0.00002480
Iteration 62/1000 | Loss: 0.00002480
Iteration 63/1000 | Loss: 0.00002480
Iteration 64/1000 | Loss: 0.00002480
Iteration 65/1000 | Loss: 0.00002479
Iteration 66/1000 | Loss: 0.00002479
Iteration 67/1000 | Loss: 0.00002479
Iteration 68/1000 | Loss: 0.00002478
Iteration 69/1000 | Loss: 0.00002478
Iteration 70/1000 | Loss: 0.00002478
Iteration 71/1000 | Loss: 0.00002478
Iteration 72/1000 | Loss: 0.00002478
Iteration 73/1000 | Loss: 0.00002478
Iteration 74/1000 | Loss: 0.00002477
Iteration 75/1000 | Loss: 0.00002477
Iteration 76/1000 | Loss: 0.00002477
Iteration 77/1000 | Loss: 0.00002477
Iteration 78/1000 | Loss: 0.00002477
Iteration 79/1000 | Loss: 0.00002477
Iteration 80/1000 | Loss: 0.00002477
Iteration 81/1000 | Loss: 0.00002477
Iteration 82/1000 | Loss: 0.00002477
Iteration 83/1000 | Loss: 0.00002477
Iteration 84/1000 | Loss: 0.00002476
Iteration 85/1000 | Loss: 0.00002476
Iteration 86/1000 | Loss: 0.00002476
Iteration 87/1000 | Loss: 0.00002475
Iteration 88/1000 | Loss: 0.00002475
Iteration 89/1000 | Loss: 0.00002475
Iteration 90/1000 | Loss: 0.00002475
Iteration 91/1000 | Loss: 0.00002475
Iteration 92/1000 | Loss: 0.00002475
Iteration 93/1000 | Loss: 0.00002475
Iteration 94/1000 | Loss: 0.00002475
Iteration 95/1000 | Loss: 0.00002475
Iteration 96/1000 | Loss: 0.00002475
Iteration 97/1000 | Loss: 0.00002474
Iteration 98/1000 | Loss: 0.00002474
Iteration 99/1000 | Loss: 0.00002474
Iteration 100/1000 | Loss: 0.00002473
Iteration 101/1000 | Loss: 0.00002473
Iteration 102/1000 | Loss: 0.00002473
Iteration 103/1000 | Loss: 0.00002473
Iteration 104/1000 | Loss: 0.00002473
Iteration 105/1000 | Loss: 0.00002473
Iteration 106/1000 | Loss: 0.00002473
Iteration 107/1000 | Loss: 0.00002473
Iteration 108/1000 | Loss: 0.00002473
Iteration 109/1000 | Loss: 0.00002473
Iteration 110/1000 | Loss: 0.00002473
Iteration 111/1000 | Loss: 0.00002473
Iteration 112/1000 | Loss: 0.00002473
Iteration 113/1000 | Loss: 0.00002473
Iteration 114/1000 | Loss: 0.00002473
Iteration 115/1000 | Loss: 0.00002473
Iteration 116/1000 | Loss: 0.00002473
Iteration 117/1000 | Loss: 0.00002473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.473111635481473e-05, 2.473111635481473e-05, 2.473111635481473e-05, 2.473111635481473e-05, 2.473111635481473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.473111635481473e-05

Optimization complete. Final v2v error: 4.038463115692139 mm

Highest mean error: 5.3914794921875 mm for frame 149

Lowest mean error: 3.2527623176574707 mm for frame 55

Saving results

Total time: 48.24909973144531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819926
Iteration 2/25 | Loss: 0.00155991
Iteration 3/25 | Loss: 0.00127687
Iteration 4/25 | Loss: 0.00125023
Iteration 5/25 | Loss: 0.00124602
Iteration 6/25 | Loss: 0.00124495
Iteration 7/25 | Loss: 0.00124495
Iteration 8/25 | Loss: 0.00124495
Iteration 9/25 | Loss: 0.00124495
Iteration 10/25 | Loss: 0.00124495
Iteration 11/25 | Loss: 0.00124495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012449548812583089, 0.0012449548812583089, 0.0012449548812583089, 0.0012449548812583089, 0.0012449548812583089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012449548812583089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26872826
Iteration 2/25 | Loss: 0.00070601
Iteration 3/25 | Loss: 0.00070595
Iteration 4/25 | Loss: 0.00070595
Iteration 5/25 | Loss: 0.00070595
Iteration 6/25 | Loss: 0.00070595
Iteration 7/25 | Loss: 0.00070595
Iteration 8/25 | Loss: 0.00070595
Iteration 9/25 | Loss: 0.00070595
Iteration 10/25 | Loss: 0.00070595
Iteration 11/25 | Loss: 0.00070595
Iteration 12/25 | Loss: 0.00070595
Iteration 13/25 | Loss: 0.00070595
Iteration 14/25 | Loss: 0.00070595
Iteration 15/25 | Loss: 0.00070595
Iteration 16/25 | Loss: 0.00070595
Iteration 17/25 | Loss: 0.00070595
Iteration 18/25 | Loss: 0.00070595
Iteration 19/25 | Loss: 0.00070595
Iteration 20/25 | Loss: 0.00070595
Iteration 21/25 | Loss: 0.00070595
Iteration 22/25 | Loss: 0.00070595
Iteration 23/25 | Loss: 0.00070595
Iteration 24/25 | Loss: 0.00070595
Iteration 25/25 | Loss: 0.00070595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070595
Iteration 2/1000 | Loss: 0.00004882
Iteration 3/1000 | Loss: 0.00003245
Iteration 4/1000 | Loss: 0.00002621
Iteration 5/1000 | Loss: 0.00002417
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002241
Iteration 8/1000 | Loss: 0.00002175
Iteration 9/1000 | Loss: 0.00002140
Iteration 10/1000 | Loss: 0.00002106
Iteration 11/1000 | Loss: 0.00002068
Iteration 12/1000 | Loss: 0.00002039
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00002021
Iteration 15/1000 | Loss: 0.00002020
Iteration 16/1000 | Loss: 0.00002017
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002015
Iteration 19/1000 | Loss: 0.00002005
Iteration 20/1000 | Loss: 0.00002001
Iteration 21/1000 | Loss: 0.00001995
Iteration 22/1000 | Loss: 0.00001986
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001982
Iteration 28/1000 | Loss: 0.00001982
Iteration 29/1000 | Loss: 0.00001982
Iteration 30/1000 | Loss: 0.00001978
Iteration 31/1000 | Loss: 0.00001977
Iteration 32/1000 | Loss: 0.00001975
Iteration 33/1000 | Loss: 0.00001974
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001973
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001972
Iteration 39/1000 | Loss: 0.00001972
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001971
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001969
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001969
Iteration 49/1000 | Loss: 0.00001969
Iteration 50/1000 | Loss: 0.00001969
Iteration 51/1000 | Loss: 0.00001968
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001967
Iteration 60/1000 | Loss: 0.00001967
Iteration 61/1000 | Loss: 0.00001967
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001966
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001963
Iteration 71/1000 | Loss: 0.00001962
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001957
Iteration 74/1000 | Loss: 0.00001957
Iteration 75/1000 | Loss: 0.00001957
Iteration 76/1000 | Loss: 0.00001957
Iteration 77/1000 | Loss: 0.00001956
Iteration 78/1000 | Loss: 0.00001953
Iteration 79/1000 | Loss: 0.00001952
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001951
Iteration 82/1000 | Loss: 0.00001951
Iteration 83/1000 | Loss: 0.00001951
Iteration 84/1000 | Loss: 0.00001951
Iteration 85/1000 | Loss: 0.00001951
Iteration 86/1000 | Loss: 0.00001951
Iteration 87/1000 | Loss: 0.00001951
Iteration 88/1000 | Loss: 0.00001951
Iteration 89/1000 | Loss: 0.00001951
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001951
Iteration 97/1000 | Loss: 0.00001951
Iteration 98/1000 | Loss: 0.00001951
Iteration 99/1000 | Loss: 0.00001951
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001951
Iteration 102/1000 | Loss: 0.00001951
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.9507719116518274e-05, 1.9507719116518274e-05, 1.9507719116518274e-05, 1.9507719116518274e-05, 1.9507719116518274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9507719116518274e-05

Optimization complete. Final v2v error: 3.796502113342285 mm

Highest mean error: 4.150837421417236 mm for frame 137

Lowest mean error: 3.382606029510498 mm for frame 1

Saving results

Total time: 38.864354610443115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984528
Iteration 2/25 | Loss: 0.00173362
Iteration 3/25 | Loss: 0.00134412
Iteration 4/25 | Loss: 0.00130232
Iteration 5/25 | Loss: 0.00129746
Iteration 6/25 | Loss: 0.00129730
Iteration 7/25 | Loss: 0.00129730
Iteration 8/25 | Loss: 0.00129730
Iteration 9/25 | Loss: 0.00129730
Iteration 10/25 | Loss: 0.00129730
Iteration 11/25 | Loss: 0.00129730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012973039411008358, 0.0012973039411008358, 0.0012973039411008358, 0.0012973039411008358, 0.0012973039411008358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012973039411008358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.82850122
Iteration 2/25 | Loss: 0.00078383
Iteration 3/25 | Loss: 0.00078381
Iteration 4/25 | Loss: 0.00078381
Iteration 5/25 | Loss: 0.00078381
Iteration 6/25 | Loss: 0.00078381
Iteration 7/25 | Loss: 0.00078381
Iteration 8/25 | Loss: 0.00078381
Iteration 9/25 | Loss: 0.00078381
Iteration 10/25 | Loss: 0.00078381
Iteration 11/25 | Loss: 0.00078381
Iteration 12/25 | Loss: 0.00078381
Iteration 13/25 | Loss: 0.00078381
Iteration 14/25 | Loss: 0.00078381
Iteration 15/25 | Loss: 0.00078381
Iteration 16/25 | Loss: 0.00078381
Iteration 17/25 | Loss: 0.00078381
Iteration 18/25 | Loss: 0.00078381
Iteration 19/25 | Loss: 0.00078381
Iteration 20/25 | Loss: 0.00078381
Iteration 21/25 | Loss: 0.00078381
Iteration 22/25 | Loss: 0.00078381
Iteration 23/25 | Loss: 0.00078381
Iteration 24/25 | Loss: 0.00078381
Iteration 25/25 | Loss: 0.00078381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078381
Iteration 2/1000 | Loss: 0.00006535
Iteration 3/1000 | Loss: 0.00004054
Iteration 4/1000 | Loss: 0.00003578
Iteration 5/1000 | Loss: 0.00003447
Iteration 6/1000 | Loss: 0.00003330
Iteration 7/1000 | Loss: 0.00003270
Iteration 8/1000 | Loss: 0.00003224
Iteration 9/1000 | Loss: 0.00003188
Iteration 10/1000 | Loss: 0.00003158
Iteration 11/1000 | Loss: 0.00003134
Iteration 12/1000 | Loss: 0.00003115
Iteration 13/1000 | Loss: 0.00003101
Iteration 14/1000 | Loss: 0.00003087
Iteration 15/1000 | Loss: 0.00003086
Iteration 16/1000 | Loss: 0.00003085
Iteration 17/1000 | Loss: 0.00003076
Iteration 18/1000 | Loss: 0.00003063
Iteration 19/1000 | Loss: 0.00003061
Iteration 20/1000 | Loss: 0.00003058
Iteration 21/1000 | Loss: 0.00003057
Iteration 22/1000 | Loss: 0.00003057
Iteration 23/1000 | Loss: 0.00003056
Iteration 24/1000 | Loss: 0.00003056
Iteration 25/1000 | Loss: 0.00003052
Iteration 26/1000 | Loss: 0.00003052
Iteration 27/1000 | Loss: 0.00003052
Iteration 28/1000 | Loss: 0.00003052
Iteration 29/1000 | Loss: 0.00003052
Iteration 30/1000 | Loss: 0.00003052
Iteration 31/1000 | Loss: 0.00003052
Iteration 32/1000 | Loss: 0.00003052
Iteration 33/1000 | Loss: 0.00003052
Iteration 34/1000 | Loss: 0.00003052
Iteration 35/1000 | Loss: 0.00003052
Iteration 36/1000 | Loss: 0.00003052
Iteration 37/1000 | Loss: 0.00003052
Iteration 38/1000 | Loss: 0.00003052
Iteration 39/1000 | Loss: 0.00003052
Iteration 40/1000 | Loss: 0.00003052
Iteration 41/1000 | Loss: 0.00003052
Iteration 42/1000 | Loss: 0.00003052
Iteration 43/1000 | Loss: 0.00003052
Iteration 44/1000 | Loss: 0.00003052
Iteration 45/1000 | Loss: 0.00003052
Iteration 46/1000 | Loss: 0.00003052
Iteration 47/1000 | Loss: 0.00003052
Iteration 48/1000 | Loss: 0.00003052
Iteration 49/1000 | Loss: 0.00003052
Iteration 50/1000 | Loss: 0.00003052
Iteration 51/1000 | Loss: 0.00003052
Iteration 52/1000 | Loss: 0.00003052
Iteration 53/1000 | Loss: 0.00003052
Iteration 54/1000 | Loss: 0.00003052
Iteration 55/1000 | Loss: 0.00003052
Iteration 56/1000 | Loss: 0.00003052
Iteration 57/1000 | Loss: 0.00003052
Iteration 58/1000 | Loss: 0.00003052
Iteration 59/1000 | Loss: 0.00003052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [3.052161628147587e-05, 3.052161628147587e-05, 3.052161628147587e-05, 3.052161628147587e-05, 3.052161628147587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.052161628147587e-05

Optimization complete. Final v2v error: 4.472228527069092 mm

Highest mean error: 5.447680473327637 mm for frame 0

Lowest mean error: 3.7389323711395264 mm for frame 195

Saving results

Total time: 36.48486876487732
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398531
Iteration 2/25 | Loss: 0.00123550
Iteration 3/25 | Loss: 0.00111951
Iteration 4/25 | Loss: 0.00110220
Iteration 5/25 | Loss: 0.00109707
Iteration 6/25 | Loss: 0.00109550
Iteration 7/25 | Loss: 0.00109526
Iteration 8/25 | Loss: 0.00109526
Iteration 9/25 | Loss: 0.00109526
Iteration 10/25 | Loss: 0.00109526
Iteration 11/25 | Loss: 0.00109526
Iteration 12/25 | Loss: 0.00109526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010952604934573174, 0.0010952604934573174, 0.0010952604934573174, 0.0010952604934573174, 0.0010952604934573174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010952604934573174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32968283
Iteration 2/25 | Loss: 0.00098122
Iteration 3/25 | Loss: 0.00098120
Iteration 4/25 | Loss: 0.00098120
Iteration 5/25 | Loss: 0.00098120
Iteration 6/25 | Loss: 0.00098120
Iteration 7/25 | Loss: 0.00098120
Iteration 8/25 | Loss: 0.00098120
Iteration 9/25 | Loss: 0.00098120
Iteration 10/25 | Loss: 0.00098120
Iteration 11/25 | Loss: 0.00098120
Iteration 12/25 | Loss: 0.00098120
Iteration 13/25 | Loss: 0.00098120
Iteration 14/25 | Loss: 0.00098120
Iteration 15/25 | Loss: 0.00098120
Iteration 16/25 | Loss: 0.00098120
Iteration 17/25 | Loss: 0.00098120
Iteration 18/25 | Loss: 0.00098120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009811990894377232, 0.0009811990894377232, 0.0009811990894377232, 0.0009811990894377232, 0.0009811990894377232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009811990894377232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098120
Iteration 2/1000 | Loss: 0.00003872
Iteration 3/1000 | Loss: 0.00002434
Iteration 4/1000 | Loss: 0.00001989
Iteration 5/1000 | Loss: 0.00001808
Iteration 6/1000 | Loss: 0.00001678
Iteration 7/1000 | Loss: 0.00001594
Iteration 8/1000 | Loss: 0.00001549
Iteration 9/1000 | Loss: 0.00001499
Iteration 10/1000 | Loss: 0.00001474
Iteration 11/1000 | Loss: 0.00001463
Iteration 12/1000 | Loss: 0.00001444
Iteration 13/1000 | Loss: 0.00001425
Iteration 14/1000 | Loss: 0.00001423
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001418
Iteration 18/1000 | Loss: 0.00001418
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001413
Iteration 28/1000 | Loss: 0.00001411
Iteration 29/1000 | Loss: 0.00001411
Iteration 30/1000 | Loss: 0.00001410
Iteration 31/1000 | Loss: 0.00001410
Iteration 32/1000 | Loss: 0.00001409
Iteration 33/1000 | Loss: 0.00001409
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001408
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001407
Iteration 38/1000 | Loss: 0.00001407
Iteration 39/1000 | Loss: 0.00001406
Iteration 40/1000 | Loss: 0.00001406
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001405
Iteration 43/1000 | Loss: 0.00001405
Iteration 44/1000 | Loss: 0.00001404
Iteration 45/1000 | Loss: 0.00001404
Iteration 46/1000 | Loss: 0.00001404
Iteration 47/1000 | Loss: 0.00001404
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001400
Iteration 57/1000 | Loss: 0.00001396
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001394
Iteration 60/1000 | Loss: 0.00001394
Iteration 61/1000 | Loss: 0.00001394
Iteration 62/1000 | Loss: 0.00001394
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001392
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001390
Iteration 70/1000 | Loss: 0.00001390
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001388
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001382
Iteration 87/1000 | Loss: 0.00001382
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001381
Iteration 90/1000 | Loss: 0.00001381
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00001380
Iteration 93/1000 | Loss: 0.00001380
Iteration 94/1000 | Loss: 0.00001379
Iteration 95/1000 | Loss: 0.00001379
Iteration 96/1000 | Loss: 0.00001379
Iteration 97/1000 | Loss: 0.00001378
Iteration 98/1000 | Loss: 0.00001378
Iteration 99/1000 | Loss: 0.00001378
Iteration 100/1000 | Loss: 0.00001378
Iteration 101/1000 | Loss: 0.00001378
Iteration 102/1000 | Loss: 0.00001377
Iteration 103/1000 | Loss: 0.00001377
Iteration 104/1000 | Loss: 0.00001377
Iteration 105/1000 | Loss: 0.00001377
Iteration 106/1000 | Loss: 0.00001377
Iteration 107/1000 | Loss: 0.00001376
Iteration 108/1000 | Loss: 0.00001376
Iteration 109/1000 | Loss: 0.00001376
Iteration 110/1000 | Loss: 0.00001376
Iteration 111/1000 | Loss: 0.00001376
Iteration 112/1000 | Loss: 0.00001376
Iteration 113/1000 | Loss: 0.00001375
Iteration 114/1000 | Loss: 0.00001375
Iteration 115/1000 | Loss: 0.00001375
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001373
Iteration 120/1000 | Loss: 0.00001373
Iteration 121/1000 | Loss: 0.00001373
Iteration 122/1000 | Loss: 0.00001373
Iteration 123/1000 | Loss: 0.00001373
Iteration 124/1000 | Loss: 0.00001372
Iteration 125/1000 | Loss: 0.00001372
Iteration 126/1000 | Loss: 0.00001372
Iteration 127/1000 | Loss: 0.00001372
Iteration 128/1000 | Loss: 0.00001372
Iteration 129/1000 | Loss: 0.00001372
Iteration 130/1000 | Loss: 0.00001372
Iteration 131/1000 | Loss: 0.00001371
Iteration 132/1000 | Loss: 0.00001371
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001371
Iteration 139/1000 | Loss: 0.00001371
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001370
Iteration 144/1000 | Loss: 0.00001369
Iteration 145/1000 | Loss: 0.00001369
Iteration 146/1000 | Loss: 0.00001369
Iteration 147/1000 | Loss: 0.00001369
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001368
Iteration 151/1000 | Loss: 0.00001368
Iteration 152/1000 | Loss: 0.00001368
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001367
Iteration 156/1000 | Loss: 0.00001367
Iteration 157/1000 | Loss: 0.00001367
Iteration 158/1000 | Loss: 0.00001367
Iteration 159/1000 | Loss: 0.00001367
Iteration 160/1000 | Loss: 0.00001367
Iteration 161/1000 | Loss: 0.00001367
Iteration 162/1000 | Loss: 0.00001366
Iteration 163/1000 | Loss: 0.00001366
Iteration 164/1000 | Loss: 0.00001366
Iteration 165/1000 | Loss: 0.00001366
Iteration 166/1000 | Loss: 0.00001366
Iteration 167/1000 | Loss: 0.00001366
Iteration 168/1000 | Loss: 0.00001366
Iteration 169/1000 | Loss: 0.00001366
Iteration 170/1000 | Loss: 0.00001366
Iteration 171/1000 | Loss: 0.00001366
Iteration 172/1000 | Loss: 0.00001366
Iteration 173/1000 | Loss: 0.00001366
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001365
Iteration 176/1000 | Loss: 0.00001365
Iteration 177/1000 | Loss: 0.00001365
Iteration 178/1000 | Loss: 0.00001365
Iteration 179/1000 | Loss: 0.00001365
Iteration 180/1000 | Loss: 0.00001365
Iteration 181/1000 | Loss: 0.00001365
Iteration 182/1000 | Loss: 0.00001365
Iteration 183/1000 | Loss: 0.00001364
Iteration 184/1000 | Loss: 0.00001364
Iteration 185/1000 | Loss: 0.00001364
Iteration 186/1000 | Loss: 0.00001364
Iteration 187/1000 | Loss: 0.00001364
Iteration 188/1000 | Loss: 0.00001364
Iteration 189/1000 | Loss: 0.00001364
Iteration 190/1000 | Loss: 0.00001364
Iteration 191/1000 | Loss: 0.00001364
Iteration 192/1000 | Loss: 0.00001363
Iteration 193/1000 | Loss: 0.00001363
Iteration 194/1000 | Loss: 0.00001363
Iteration 195/1000 | Loss: 0.00001363
Iteration 196/1000 | Loss: 0.00001363
Iteration 197/1000 | Loss: 0.00001363
Iteration 198/1000 | Loss: 0.00001363
Iteration 199/1000 | Loss: 0.00001363
Iteration 200/1000 | Loss: 0.00001363
Iteration 201/1000 | Loss: 0.00001363
Iteration 202/1000 | Loss: 0.00001362
Iteration 203/1000 | Loss: 0.00001362
Iteration 204/1000 | Loss: 0.00001362
Iteration 205/1000 | Loss: 0.00001361
Iteration 206/1000 | Loss: 0.00001361
Iteration 207/1000 | Loss: 0.00001361
Iteration 208/1000 | Loss: 0.00001361
Iteration 209/1000 | Loss: 0.00001360
Iteration 210/1000 | Loss: 0.00001360
Iteration 211/1000 | Loss: 0.00001360
Iteration 212/1000 | Loss: 0.00001360
Iteration 213/1000 | Loss: 0.00001360
Iteration 214/1000 | Loss: 0.00001360
Iteration 215/1000 | Loss: 0.00001359
Iteration 216/1000 | Loss: 0.00001359
Iteration 217/1000 | Loss: 0.00001359
Iteration 218/1000 | Loss: 0.00001359
Iteration 219/1000 | Loss: 0.00001359
Iteration 220/1000 | Loss: 0.00001359
Iteration 221/1000 | Loss: 0.00001359
Iteration 222/1000 | Loss: 0.00001359
Iteration 223/1000 | Loss: 0.00001359
Iteration 224/1000 | Loss: 0.00001359
Iteration 225/1000 | Loss: 0.00001359
Iteration 226/1000 | Loss: 0.00001359
Iteration 227/1000 | Loss: 0.00001358
Iteration 228/1000 | Loss: 0.00001358
Iteration 229/1000 | Loss: 0.00001358
Iteration 230/1000 | Loss: 0.00001358
Iteration 231/1000 | Loss: 0.00001358
Iteration 232/1000 | Loss: 0.00001358
Iteration 233/1000 | Loss: 0.00001358
Iteration 234/1000 | Loss: 0.00001358
Iteration 235/1000 | Loss: 0.00001358
Iteration 236/1000 | Loss: 0.00001358
Iteration 237/1000 | Loss: 0.00001358
Iteration 238/1000 | Loss: 0.00001358
Iteration 239/1000 | Loss: 0.00001357
Iteration 240/1000 | Loss: 0.00001357
Iteration 241/1000 | Loss: 0.00001357
Iteration 242/1000 | Loss: 0.00001357
Iteration 243/1000 | Loss: 0.00001357
Iteration 244/1000 | Loss: 0.00001357
Iteration 245/1000 | Loss: 0.00001357
Iteration 246/1000 | Loss: 0.00001357
Iteration 247/1000 | Loss: 0.00001356
Iteration 248/1000 | Loss: 0.00001356
Iteration 249/1000 | Loss: 0.00001356
Iteration 250/1000 | Loss: 0.00001356
Iteration 251/1000 | Loss: 0.00001356
Iteration 252/1000 | Loss: 0.00001356
Iteration 253/1000 | Loss: 0.00001355
Iteration 254/1000 | Loss: 0.00001355
Iteration 255/1000 | Loss: 0.00001355
Iteration 256/1000 | Loss: 0.00001355
Iteration 257/1000 | Loss: 0.00001355
Iteration 258/1000 | Loss: 0.00001355
Iteration 259/1000 | Loss: 0.00001355
Iteration 260/1000 | Loss: 0.00001355
Iteration 261/1000 | Loss: 0.00001355
Iteration 262/1000 | Loss: 0.00001355
Iteration 263/1000 | Loss: 0.00001355
Iteration 264/1000 | Loss: 0.00001355
Iteration 265/1000 | Loss: 0.00001355
Iteration 266/1000 | Loss: 0.00001355
Iteration 267/1000 | Loss: 0.00001355
Iteration 268/1000 | Loss: 0.00001355
Iteration 269/1000 | Loss: 0.00001354
Iteration 270/1000 | Loss: 0.00001354
Iteration 271/1000 | Loss: 0.00001354
Iteration 272/1000 | Loss: 0.00001354
Iteration 273/1000 | Loss: 0.00001354
Iteration 274/1000 | Loss: 0.00001354
Iteration 275/1000 | Loss: 0.00001354
Iteration 276/1000 | Loss: 0.00001354
Iteration 277/1000 | Loss: 0.00001354
Iteration 278/1000 | Loss: 0.00001353
Iteration 279/1000 | Loss: 0.00001353
Iteration 280/1000 | Loss: 0.00001353
Iteration 281/1000 | Loss: 0.00001353
Iteration 282/1000 | Loss: 0.00001353
Iteration 283/1000 | Loss: 0.00001353
Iteration 284/1000 | Loss: 0.00001353
Iteration 285/1000 | Loss: 0.00001353
Iteration 286/1000 | Loss: 0.00001353
Iteration 287/1000 | Loss: 0.00001353
Iteration 288/1000 | Loss: 0.00001353
Iteration 289/1000 | Loss: 0.00001353
Iteration 290/1000 | Loss: 0.00001353
Iteration 291/1000 | Loss: 0.00001353
Iteration 292/1000 | Loss: 0.00001353
Iteration 293/1000 | Loss: 0.00001353
Iteration 294/1000 | Loss: 0.00001353
Iteration 295/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [1.3532690900319722e-05, 1.3532690900319722e-05, 1.3532690900319722e-05, 1.3532690900319722e-05, 1.3532690900319722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3532690900319722e-05

Optimization complete. Final v2v error: 2.9584338665008545 mm

Highest mean error: 5.0146284103393555 mm for frame 87

Lowest mean error: 2.369964122772217 mm for frame 24

Saving results

Total time: 49.59770107269287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358016
Iteration 2/25 | Loss: 0.00115325
Iteration 3/25 | Loss: 0.00107541
Iteration 4/25 | Loss: 0.00106625
Iteration 5/25 | Loss: 0.00106357
Iteration 6/25 | Loss: 0.00106281
Iteration 7/25 | Loss: 0.00106281
Iteration 8/25 | Loss: 0.00106281
Iteration 9/25 | Loss: 0.00106281
Iteration 10/25 | Loss: 0.00106281
Iteration 11/25 | Loss: 0.00106281
Iteration 12/25 | Loss: 0.00106281
Iteration 13/25 | Loss: 0.00106281
Iteration 14/25 | Loss: 0.00106281
Iteration 15/25 | Loss: 0.00106281
Iteration 16/25 | Loss: 0.00106281
Iteration 17/25 | Loss: 0.00106281
Iteration 18/25 | Loss: 0.00106281
Iteration 19/25 | Loss: 0.00106281
Iteration 20/25 | Loss: 0.00106281
Iteration 21/25 | Loss: 0.00106281
Iteration 22/25 | Loss: 0.00106281
Iteration 23/25 | Loss: 0.00106281
Iteration 24/25 | Loss: 0.00106281
Iteration 25/25 | Loss: 0.00106281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70750391
Iteration 2/25 | Loss: 0.00090300
Iteration 3/25 | Loss: 0.00090300
Iteration 4/25 | Loss: 0.00090300
Iteration 5/25 | Loss: 0.00090299
Iteration 6/25 | Loss: 0.00090299
Iteration 7/25 | Loss: 0.00090299
Iteration 8/25 | Loss: 0.00090299
Iteration 9/25 | Loss: 0.00090299
Iteration 10/25 | Loss: 0.00090299
Iteration 11/25 | Loss: 0.00090299
Iteration 12/25 | Loss: 0.00090299
Iteration 13/25 | Loss: 0.00090299
Iteration 14/25 | Loss: 0.00090299
Iteration 15/25 | Loss: 0.00090299
Iteration 16/25 | Loss: 0.00090299
Iteration 17/25 | Loss: 0.00090299
Iteration 18/25 | Loss: 0.00090299
Iteration 19/25 | Loss: 0.00090299
Iteration 20/25 | Loss: 0.00090299
Iteration 21/25 | Loss: 0.00090299
Iteration 22/25 | Loss: 0.00090299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009029925568029284, 0.0009029925568029284, 0.0009029925568029284, 0.0009029925568029284, 0.0009029925568029284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009029925568029284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090299
Iteration 2/1000 | Loss: 0.00002002
Iteration 3/1000 | Loss: 0.00001215
Iteration 4/1000 | Loss: 0.00001053
Iteration 5/1000 | Loss: 0.00000977
Iteration 6/1000 | Loss: 0.00000928
Iteration 7/1000 | Loss: 0.00000889
Iteration 8/1000 | Loss: 0.00000881
Iteration 9/1000 | Loss: 0.00000874
Iteration 10/1000 | Loss: 0.00000873
Iteration 11/1000 | Loss: 0.00000860
Iteration 12/1000 | Loss: 0.00000852
Iteration 13/1000 | Loss: 0.00000845
Iteration 14/1000 | Loss: 0.00000842
Iteration 15/1000 | Loss: 0.00000839
Iteration 16/1000 | Loss: 0.00000839
Iteration 17/1000 | Loss: 0.00000838
Iteration 18/1000 | Loss: 0.00000833
Iteration 19/1000 | Loss: 0.00000828
Iteration 20/1000 | Loss: 0.00000823
Iteration 21/1000 | Loss: 0.00000823
Iteration 22/1000 | Loss: 0.00000822
Iteration 23/1000 | Loss: 0.00000822
Iteration 24/1000 | Loss: 0.00000821
Iteration 25/1000 | Loss: 0.00000821
Iteration 26/1000 | Loss: 0.00000820
Iteration 27/1000 | Loss: 0.00000819
Iteration 28/1000 | Loss: 0.00000819
Iteration 29/1000 | Loss: 0.00000819
Iteration 30/1000 | Loss: 0.00000816
Iteration 31/1000 | Loss: 0.00000816
Iteration 32/1000 | Loss: 0.00000816
Iteration 33/1000 | Loss: 0.00000816
Iteration 34/1000 | Loss: 0.00000816
Iteration 35/1000 | Loss: 0.00000816
Iteration 36/1000 | Loss: 0.00000815
Iteration 37/1000 | Loss: 0.00000815
Iteration 38/1000 | Loss: 0.00000814
Iteration 39/1000 | Loss: 0.00000814
Iteration 40/1000 | Loss: 0.00000814
Iteration 41/1000 | Loss: 0.00000813
Iteration 42/1000 | Loss: 0.00000813
Iteration 43/1000 | Loss: 0.00000812
Iteration 44/1000 | Loss: 0.00000812
Iteration 45/1000 | Loss: 0.00000811
Iteration 46/1000 | Loss: 0.00000811
Iteration 47/1000 | Loss: 0.00000811
Iteration 48/1000 | Loss: 0.00000811
Iteration 49/1000 | Loss: 0.00000810
Iteration 50/1000 | Loss: 0.00000810
Iteration 51/1000 | Loss: 0.00000810
Iteration 52/1000 | Loss: 0.00000809
Iteration 53/1000 | Loss: 0.00000809
Iteration 54/1000 | Loss: 0.00000808
Iteration 55/1000 | Loss: 0.00000808
Iteration 56/1000 | Loss: 0.00000808
Iteration 57/1000 | Loss: 0.00000808
Iteration 58/1000 | Loss: 0.00000807
Iteration 59/1000 | Loss: 0.00000807
Iteration 60/1000 | Loss: 0.00000807
Iteration 61/1000 | Loss: 0.00000806
Iteration 62/1000 | Loss: 0.00000806
Iteration 63/1000 | Loss: 0.00000806
Iteration 64/1000 | Loss: 0.00000805
Iteration 65/1000 | Loss: 0.00000805
Iteration 66/1000 | Loss: 0.00000805
Iteration 67/1000 | Loss: 0.00000805
Iteration 68/1000 | Loss: 0.00000805
Iteration 69/1000 | Loss: 0.00000805
Iteration 70/1000 | Loss: 0.00000804
Iteration 71/1000 | Loss: 0.00000804
Iteration 72/1000 | Loss: 0.00000804
Iteration 73/1000 | Loss: 0.00000803
Iteration 74/1000 | Loss: 0.00000803
Iteration 75/1000 | Loss: 0.00000803
Iteration 76/1000 | Loss: 0.00000803
Iteration 77/1000 | Loss: 0.00000803
Iteration 78/1000 | Loss: 0.00000803
Iteration 79/1000 | Loss: 0.00000802
Iteration 80/1000 | Loss: 0.00000802
Iteration 81/1000 | Loss: 0.00000802
Iteration 82/1000 | Loss: 0.00000802
Iteration 83/1000 | Loss: 0.00000802
Iteration 84/1000 | Loss: 0.00000802
Iteration 85/1000 | Loss: 0.00000802
Iteration 86/1000 | Loss: 0.00000801
Iteration 87/1000 | Loss: 0.00000801
Iteration 88/1000 | Loss: 0.00000801
Iteration 89/1000 | Loss: 0.00000801
Iteration 90/1000 | Loss: 0.00000801
Iteration 91/1000 | Loss: 0.00000801
Iteration 92/1000 | Loss: 0.00000801
Iteration 93/1000 | Loss: 0.00000801
Iteration 94/1000 | Loss: 0.00000801
Iteration 95/1000 | Loss: 0.00000800
Iteration 96/1000 | Loss: 0.00000800
Iteration 97/1000 | Loss: 0.00000800
Iteration 98/1000 | Loss: 0.00000800
Iteration 99/1000 | Loss: 0.00000799
Iteration 100/1000 | Loss: 0.00000799
Iteration 101/1000 | Loss: 0.00000799
Iteration 102/1000 | Loss: 0.00000799
Iteration 103/1000 | Loss: 0.00000799
Iteration 104/1000 | Loss: 0.00000799
Iteration 105/1000 | Loss: 0.00000799
Iteration 106/1000 | Loss: 0.00000798
Iteration 107/1000 | Loss: 0.00000798
Iteration 108/1000 | Loss: 0.00000798
Iteration 109/1000 | Loss: 0.00000798
Iteration 110/1000 | Loss: 0.00000797
Iteration 111/1000 | Loss: 0.00000797
Iteration 112/1000 | Loss: 0.00000797
Iteration 113/1000 | Loss: 0.00000797
Iteration 114/1000 | Loss: 0.00000797
Iteration 115/1000 | Loss: 0.00000797
Iteration 116/1000 | Loss: 0.00000796
Iteration 117/1000 | Loss: 0.00000796
Iteration 118/1000 | Loss: 0.00000796
Iteration 119/1000 | Loss: 0.00000796
Iteration 120/1000 | Loss: 0.00000796
Iteration 121/1000 | Loss: 0.00000796
Iteration 122/1000 | Loss: 0.00000796
Iteration 123/1000 | Loss: 0.00000796
Iteration 124/1000 | Loss: 0.00000796
Iteration 125/1000 | Loss: 0.00000796
Iteration 126/1000 | Loss: 0.00000796
Iteration 127/1000 | Loss: 0.00000796
Iteration 128/1000 | Loss: 0.00000796
Iteration 129/1000 | Loss: 0.00000796
Iteration 130/1000 | Loss: 0.00000796
Iteration 131/1000 | Loss: 0.00000796
Iteration 132/1000 | Loss: 0.00000796
Iteration 133/1000 | Loss: 0.00000796
Iteration 134/1000 | Loss: 0.00000796
Iteration 135/1000 | Loss: 0.00000796
Iteration 136/1000 | Loss: 0.00000796
Iteration 137/1000 | Loss: 0.00000796
Iteration 138/1000 | Loss: 0.00000796
Iteration 139/1000 | Loss: 0.00000796
Iteration 140/1000 | Loss: 0.00000796
Iteration 141/1000 | Loss: 0.00000796
Iteration 142/1000 | Loss: 0.00000796
Iteration 143/1000 | Loss: 0.00000796
Iteration 144/1000 | Loss: 0.00000796
Iteration 145/1000 | Loss: 0.00000796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [7.957190973684192e-06, 7.957190973684192e-06, 7.957190973684192e-06, 7.957190973684192e-06, 7.957190973684192e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.957190973684192e-06

Optimization complete. Final v2v error: 2.4301917552948 mm

Highest mean error: 2.963494300842285 mm for frame 75

Lowest mean error: 2.3098208904266357 mm for frame 121

Saving results

Total time: 33.95365643501282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528691
Iteration 2/25 | Loss: 0.00141929
Iteration 3/25 | Loss: 0.00122138
Iteration 4/25 | Loss: 0.00120288
Iteration 5/25 | Loss: 0.00120034
Iteration 6/25 | Loss: 0.00119995
Iteration 7/25 | Loss: 0.00119995
Iteration 8/25 | Loss: 0.00119995
Iteration 9/25 | Loss: 0.00119990
Iteration 10/25 | Loss: 0.00119990
Iteration 11/25 | Loss: 0.00119990
Iteration 12/25 | Loss: 0.00119990
Iteration 13/25 | Loss: 0.00119990
Iteration 14/25 | Loss: 0.00119990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011999034322798252, 0.0011999034322798252, 0.0011999034322798252, 0.0011999034322798252, 0.0011999034322798252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011999034322798252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46752691
Iteration 2/25 | Loss: 0.00059989
Iteration 3/25 | Loss: 0.00059988
Iteration 4/25 | Loss: 0.00059988
Iteration 5/25 | Loss: 0.00059988
Iteration 6/25 | Loss: 0.00059988
Iteration 7/25 | Loss: 0.00059988
Iteration 8/25 | Loss: 0.00059988
Iteration 9/25 | Loss: 0.00059988
Iteration 10/25 | Loss: 0.00059988
Iteration 11/25 | Loss: 0.00059988
Iteration 12/25 | Loss: 0.00059988
Iteration 13/25 | Loss: 0.00059988
Iteration 14/25 | Loss: 0.00059988
Iteration 15/25 | Loss: 0.00059988
Iteration 16/25 | Loss: 0.00059988
Iteration 17/25 | Loss: 0.00059988
Iteration 18/25 | Loss: 0.00059988
Iteration 19/25 | Loss: 0.00059988
Iteration 20/25 | Loss: 0.00059988
Iteration 21/25 | Loss: 0.00059988
Iteration 22/25 | Loss: 0.00059988
Iteration 23/25 | Loss: 0.00059988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005998789565637708, 0.0005998789565637708, 0.0005998789565637708, 0.0005998789565637708, 0.0005998789565637708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005998789565637708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059988
Iteration 2/1000 | Loss: 0.00004089
Iteration 3/1000 | Loss: 0.00002996
Iteration 4/1000 | Loss: 0.00002612
Iteration 5/1000 | Loss: 0.00002462
Iteration 6/1000 | Loss: 0.00002361
Iteration 7/1000 | Loss: 0.00002295
Iteration 8/1000 | Loss: 0.00002254
Iteration 9/1000 | Loss: 0.00002215
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00002131
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002110
Iteration 16/1000 | Loss: 0.00002104
Iteration 17/1000 | Loss: 0.00002102
Iteration 18/1000 | Loss: 0.00002102
Iteration 19/1000 | Loss: 0.00002101
Iteration 20/1000 | Loss: 0.00002101
Iteration 21/1000 | Loss: 0.00002101
Iteration 22/1000 | Loss: 0.00002100
Iteration 23/1000 | Loss: 0.00002100
Iteration 24/1000 | Loss: 0.00002100
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002100
Iteration 27/1000 | Loss: 0.00002099
Iteration 28/1000 | Loss: 0.00002099
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002099
Iteration 31/1000 | Loss: 0.00002099
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00002098
Iteration 35/1000 | Loss: 0.00002097
Iteration 36/1000 | Loss: 0.00002097
Iteration 37/1000 | Loss: 0.00002097
Iteration 38/1000 | Loss: 0.00002096
Iteration 39/1000 | Loss: 0.00002096
Iteration 40/1000 | Loss: 0.00002095
Iteration 41/1000 | Loss: 0.00002095
Iteration 42/1000 | Loss: 0.00002095
Iteration 43/1000 | Loss: 0.00002095
Iteration 44/1000 | Loss: 0.00002094
Iteration 45/1000 | Loss: 0.00002094
Iteration 46/1000 | Loss: 0.00002093
Iteration 47/1000 | Loss: 0.00002093
Iteration 48/1000 | Loss: 0.00002093
Iteration 49/1000 | Loss: 0.00002093
Iteration 50/1000 | Loss: 0.00002093
Iteration 51/1000 | Loss: 0.00002093
Iteration 52/1000 | Loss: 0.00002093
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002093
Iteration 57/1000 | Loss: 0.00002092
Iteration 58/1000 | Loss: 0.00002092
Iteration 59/1000 | Loss: 0.00002092
Iteration 60/1000 | Loss: 0.00002092
Iteration 61/1000 | Loss: 0.00002092
Iteration 62/1000 | Loss: 0.00002092
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002092
Iteration 65/1000 | Loss: 0.00002092
Iteration 66/1000 | Loss: 0.00002092
Iteration 67/1000 | Loss: 0.00002092
Iteration 68/1000 | Loss: 0.00002092
Iteration 69/1000 | Loss: 0.00002091
Iteration 70/1000 | Loss: 0.00002091
Iteration 71/1000 | Loss: 0.00002090
Iteration 72/1000 | Loss: 0.00002090
Iteration 73/1000 | Loss: 0.00002090
Iteration 74/1000 | Loss: 0.00002090
Iteration 75/1000 | Loss: 0.00002089
Iteration 76/1000 | Loss: 0.00002089
Iteration 77/1000 | Loss: 0.00002089
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002089
Iteration 80/1000 | Loss: 0.00002089
Iteration 81/1000 | Loss: 0.00002089
Iteration 82/1000 | Loss: 0.00002089
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00002088
Iteration 85/1000 | Loss: 0.00002088
Iteration 86/1000 | Loss: 0.00002088
Iteration 87/1000 | Loss: 0.00002087
Iteration 88/1000 | Loss: 0.00002087
Iteration 89/1000 | Loss: 0.00002087
Iteration 90/1000 | Loss: 0.00002087
Iteration 91/1000 | Loss: 0.00002087
Iteration 92/1000 | Loss: 0.00002087
Iteration 93/1000 | Loss: 0.00002087
Iteration 94/1000 | Loss: 0.00002087
Iteration 95/1000 | Loss: 0.00002087
Iteration 96/1000 | Loss: 0.00002087
Iteration 97/1000 | Loss: 0.00002087
Iteration 98/1000 | Loss: 0.00002086
Iteration 99/1000 | Loss: 0.00002086
Iteration 100/1000 | Loss: 0.00002086
Iteration 101/1000 | Loss: 0.00002085
Iteration 102/1000 | Loss: 0.00002085
Iteration 103/1000 | Loss: 0.00002085
Iteration 104/1000 | Loss: 0.00002085
Iteration 105/1000 | Loss: 0.00002085
Iteration 106/1000 | Loss: 0.00002085
Iteration 107/1000 | Loss: 0.00002085
Iteration 108/1000 | Loss: 0.00002085
Iteration 109/1000 | Loss: 0.00002085
Iteration 110/1000 | Loss: 0.00002085
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002084
Iteration 115/1000 | Loss: 0.00002084
Iteration 116/1000 | Loss: 0.00002084
Iteration 117/1000 | Loss: 0.00002084
Iteration 118/1000 | Loss: 0.00002084
Iteration 119/1000 | Loss: 0.00002084
Iteration 120/1000 | Loss: 0.00002084
Iteration 121/1000 | Loss: 0.00002084
Iteration 122/1000 | Loss: 0.00002084
Iteration 123/1000 | Loss: 0.00002084
Iteration 124/1000 | Loss: 0.00002084
Iteration 125/1000 | Loss: 0.00002084
Iteration 126/1000 | Loss: 0.00002084
Iteration 127/1000 | Loss: 0.00002083
Iteration 128/1000 | Loss: 0.00002083
Iteration 129/1000 | Loss: 0.00002083
Iteration 130/1000 | Loss: 0.00002083
Iteration 131/1000 | Loss: 0.00002083
Iteration 132/1000 | Loss: 0.00002083
Iteration 133/1000 | Loss: 0.00002083
Iteration 134/1000 | Loss: 0.00002083
Iteration 135/1000 | Loss: 0.00002083
Iteration 136/1000 | Loss: 0.00002083
Iteration 137/1000 | Loss: 0.00002083
Iteration 138/1000 | Loss: 0.00002083
Iteration 139/1000 | Loss: 0.00002083
Iteration 140/1000 | Loss: 0.00002083
Iteration 141/1000 | Loss: 0.00002083
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002083
Iteration 144/1000 | Loss: 0.00002083
Iteration 145/1000 | Loss: 0.00002083
Iteration 146/1000 | Loss: 0.00002083
Iteration 147/1000 | Loss: 0.00002083
Iteration 148/1000 | Loss: 0.00002083
Iteration 149/1000 | Loss: 0.00002083
Iteration 150/1000 | Loss: 0.00002083
Iteration 151/1000 | Loss: 0.00002083
Iteration 152/1000 | Loss: 0.00002083
Iteration 153/1000 | Loss: 0.00002083
Iteration 154/1000 | Loss: 0.00002083
Iteration 155/1000 | Loss: 0.00002083
Iteration 156/1000 | Loss: 0.00002083
Iteration 157/1000 | Loss: 0.00002083
Iteration 158/1000 | Loss: 0.00002083
Iteration 159/1000 | Loss: 0.00002083
Iteration 160/1000 | Loss: 0.00002083
Iteration 161/1000 | Loss: 0.00002083
Iteration 162/1000 | Loss: 0.00002083
Iteration 163/1000 | Loss: 0.00002083
Iteration 164/1000 | Loss: 0.00002083
Iteration 165/1000 | Loss: 0.00002083
Iteration 166/1000 | Loss: 0.00002083
Iteration 167/1000 | Loss: 0.00002083
Iteration 168/1000 | Loss: 0.00002083
Iteration 169/1000 | Loss: 0.00002083
Iteration 170/1000 | Loss: 0.00002083
Iteration 171/1000 | Loss: 0.00002083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.0827208572882228e-05, 2.0827208572882228e-05, 2.0827208572882228e-05, 2.0827208572882228e-05, 2.0827208572882228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0827208572882228e-05

Optimization complete. Final v2v error: 3.82159161567688 mm

Highest mean error: 4.274096488952637 mm for frame 78

Lowest mean error: 3.517565965652466 mm for frame 13

Saving results

Total time: 37.481194257736206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844863
Iteration 2/25 | Loss: 0.00117356
Iteration 3/25 | Loss: 0.00108920
Iteration 4/25 | Loss: 0.00107921
Iteration 5/25 | Loss: 0.00107572
Iteration 6/25 | Loss: 0.00107488
Iteration 7/25 | Loss: 0.00107488
Iteration 8/25 | Loss: 0.00107488
Iteration 9/25 | Loss: 0.00107488
Iteration 10/25 | Loss: 0.00107488
Iteration 11/25 | Loss: 0.00107488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010748830391094089, 0.0010748830391094089, 0.0010748830391094089, 0.0010748830391094089, 0.0010748830391094089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010748830391094089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66379905
Iteration 2/25 | Loss: 0.00088631
Iteration 3/25 | Loss: 0.00088631
Iteration 4/25 | Loss: 0.00088631
Iteration 5/25 | Loss: 0.00088631
Iteration 6/25 | Loss: 0.00088631
Iteration 7/25 | Loss: 0.00088631
Iteration 8/25 | Loss: 0.00088631
Iteration 9/25 | Loss: 0.00088631
Iteration 10/25 | Loss: 0.00088631
Iteration 11/25 | Loss: 0.00088631
Iteration 12/25 | Loss: 0.00088631
Iteration 13/25 | Loss: 0.00088631
Iteration 14/25 | Loss: 0.00088631
Iteration 15/25 | Loss: 0.00088631
Iteration 16/25 | Loss: 0.00088631
Iteration 17/25 | Loss: 0.00088631
Iteration 18/25 | Loss: 0.00088631
Iteration 19/25 | Loss: 0.00088631
Iteration 20/25 | Loss: 0.00088631
Iteration 21/25 | Loss: 0.00088631
Iteration 22/25 | Loss: 0.00088631
Iteration 23/25 | Loss: 0.00088631
Iteration 24/25 | Loss: 0.00088631
Iteration 25/25 | Loss: 0.00088631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088631
Iteration 2/1000 | Loss: 0.00001917
Iteration 3/1000 | Loss: 0.00001249
Iteration 4/1000 | Loss: 0.00001106
Iteration 5/1000 | Loss: 0.00001049
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00000968
Iteration 8/1000 | Loss: 0.00000947
Iteration 9/1000 | Loss: 0.00000943
Iteration 10/1000 | Loss: 0.00000938
Iteration 11/1000 | Loss: 0.00000937
Iteration 12/1000 | Loss: 0.00000936
Iteration 13/1000 | Loss: 0.00000919
Iteration 14/1000 | Loss: 0.00000912
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000908
Iteration 17/1000 | Loss: 0.00000905
Iteration 18/1000 | Loss: 0.00000903
Iteration 19/1000 | Loss: 0.00000898
Iteration 20/1000 | Loss: 0.00000897
Iteration 21/1000 | Loss: 0.00000897
Iteration 22/1000 | Loss: 0.00000896
Iteration 23/1000 | Loss: 0.00000894
Iteration 24/1000 | Loss: 0.00000893
Iteration 25/1000 | Loss: 0.00000893
Iteration 26/1000 | Loss: 0.00000892
Iteration 27/1000 | Loss: 0.00000891
Iteration 28/1000 | Loss: 0.00000890
Iteration 29/1000 | Loss: 0.00000890
Iteration 30/1000 | Loss: 0.00000887
Iteration 31/1000 | Loss: 0.00000886
Iteration 32/1000 | Loss: 0.00000886
Iteration 33/1000 | Loss: 0.00000885
Iteration 34/1000 | Loss: 0.00000885
Iteration 35/1000 | Loss: 0.00000885
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000884
Iteration 38/1000 | Loss: 0.00000884
Iteration 39/1000 | Loss: 0.00000884
Iteration 40/1000 | Loss: 0.00000884
Iteration 41/1000 | Loss: 0.00000883
Iteration 42/1000 | Loss: 0.00000883
Iteration 43/1000 | Loss: 0.00000883
Iteration 44/1000 | Loss: 0.00000883
Iteration 45/1000 | Loss: 0.00000882
Iteration 46/1000 | Loss: 0.00000882
Iteration 47/1000 | Loss: 0.00000882
Iteration 48/1000 | Loss: 0.00000881
Iteration 49/1000 | Loss: 0.00000881
Iteration 50/1000 | Loss: 0.00000880
Iteration 51/1000 | Loss: 0.00000880
Iteration 52/1000 | Loss: 0.00000879
Iteration 53/1000 | Loss: 0.00000879
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000875
Iteration 58/1000 | Loss: 0.00000875
Iteration 59/1000 | Loss: 0.00000874
Iteration 60/1000 | Loss: 0.00000874
Iteration 61/1000 | Loss: 0.00000873
Iteration 62/1000 | Loss: 0.00000873
Iteration 63/1000 | Loss: 0.00000872
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000872
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000871
Iteration 68/1000 | Loss: 0.00000871
Iteration 69/1000 | Loss: 0.00000871
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000870
Iteration 72/1000 | Loss: 0.00000870
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000869
Iteration 75/1000 | Loss: 0.00000869
Iteration 76/1000 | Loss: 0.00000869
Iteration 77/1000 | Loss: 0.00000869
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000868
Iteration 80/1000 | Loss: 0.00000868
Iteration 81/1000 | Loss: 0.00000868
Iteration 82/1000 | Loss: 0.00000868
Iteration 83/1000 | Loss: 0.00000868
Iteration 84/1000 | Loss: 0.00000867
Iteration 85/1000 | Loss: 0.00000867
Iteration 86/1000 | Loss: 0.00000867
Iteration 87/1000 | Loss: 0.00000867
Iteration 88/1000 | Loss: 0.00000867
Iteration 89/1000 | Loss: 0.00000867
Iteration 90/1000 | Loss: 0.00000867
Iteration 91/1000 | Loss: 0.00000867
Iteration 92/1000 | Loss: 0.00000867
Iteration 93/1000 | Loss: 0.00000867
Iteration 94/1000 | Loss: 0.00000866
Iteration 95/1000 | Loss: 0.00000866
Iteration 96/1000 | Loss: 0.00000866
Iteration 97/1000 | Loss: 0.00000866
Iteration 98/1000 | Loss: 0.00000866
Iteration 99/1000 | Loss: 0.00000866
Iteration 100/1000 | Loss: 0.00000866
Iteration 101/1000 | Loss: 0.00000865
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000865
Iteration 104/1000 | Loss: 0.00000865
Iteration 105/1000 | Loss: 0.00000865
Iteration 106/1000 | Loss: 0.00000865
Iteration 107/1000 | Loss: 0.00000865
Iteration 108/1000 | Loss: 0.00000865
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000865
Iteration 111/1000 | Loss: 0.00000865
Iteration 112/1000 | Loss: 0.00000865
Iteration 113/1000 | Loss: 0.00000865
Iteration 114/1000 | Loss: 0.00000864
Iteration 115/1000 | Loss: 0.00000864
Iteration 116/1000 | Loss: 0.00000864
Iteration 117/1000 | Loss: 0.00000864
Iteration 118/1000 | Loss: 0.00000864
Iteration 119/1000 | Loss: 0.00000863
Iteration 120/1000 | Loss: 0.00000863
Iteration 121/1000 | Loss: 0.00000863
Iteration 122/1000 | Loss: 0.00000863
Iteration 123/1000 | Loss: 0.00000863
Iteration 124/1000 | Loss: 0.00000863
Iteration 125/1000 | Loss: 0.00000863
Iteration 126/1000 | Loss: 0.00000863
Iteration 127/1000 | Loss: 0.00000863
Iteration 128/1000 | Loss: 0.00000863
Iteration 129/1000 | Loss: 0.00000863
Iteration 130/1000 | Loss: 0.00000862
Iteration 131/1000 | Loss: 0.00000862
Iteration 132/1000 | Loss: 0.00000862
Iteration 133/1000 | Loss: 0.00000862
Iteration 134/1000 | Loss: 0.00000862
Iteration 135/1000 | Loss: 0.00000861
Iteration 136/1000 | Loss: 0.00000861
Iteration 137/1000 | Loss: 0.00000861
Iteration 138/1000 | Loss: 0.00000861
Iteration 139/1000 | Loss: 0.00000861
Iteration 140/1000 | Loss: 0.00000861
Iteration 141/1000 | Loss: 0.00000860
Iteration 142/1000 | Loss: 0.00000860
Iteration 143/1000 | Loss: 0.00000860
Iteration 144/1000 | Loss: 0.00000860
Iteration 145/1000 | Loss: 0.00000860
Iteration 146/1000 | Loss: 0.00000860
Iteration 147/1000 | Loss: 0.00000859
Iteration 148/1000 | Loss: 0.00000859
Iteration 149/1000 | Loss: 0.00000859
Iteration 150/1000 | Loss: 0.00000859
Iteration 151/1000 | Loss: 0.00000859
Iteration 152/1000 | Loss: 0.00000858
Iteration 153/1000 | Loss: 0.00000858
Iteration 154/1000 | Loss: 0.00000858
Iteration 155/1000 | Loss: 0.00000858
Iteration 156/1000 | Loss: 0.00000858
Iteration 157/1000 | Loss: 0.00000858
Iteration 158/1000 | Loss: 0.00000858
Iteration 159/1000 | Loss: 0.00000858
Iteration 160/1000 | Loss: 0.00000858
Iteration 161/1000 | Loss: 0.00000857
Iteration 162/1000 | Loss: 0.00000857
Iteration 163/1000 | Loss: 0.00000857
Iteration 164/1000 | Loss: 0.00000857
Iteration 165/1000 | Loss: 0.00000857
Iteration 166/1000 | Loss: 0.00000856
Iteration 167/1000 | Loss: 0.00000856
Iteration 168/1000 | Loss: 0.00000856
Iteration 169/1000 | Loss: 0.00000856
Iteration 170/1000 | Loss: 0.00000856
Iteration 171/1000 | Loss: 0.00000856
Iteration 172/1000 | Loss: 0.00000856
Iteration 173/1000 | Loss: 0.00000856
Iteration 174/1000 | Loss: 0.00000856
Iteration 175/1000 | Loss: 0.00000856
Iteration 176/1000 | Loss: 0.00000856
Iteration 177/1000 | Loss: 0.00000855
Iteration 178/1000 | Loss: 0.00000855
Iteration 179/1000 | Loss: 0.00000855
Iteration 180/1000 | Loss: 0.00000855
Iteration 181/1000 | Loss: 0.00000855
Iteration 182/1000 | Loss: 0.00000855
Iteration 183/1000 | Loss: 0.00000855
Iteration 184/1000 | Loss: 0.00000855
Iteration 185/1000 | Loss: 0.00000855
Iteration 186/1000 | Loss: 0.00000855
Iteration 187/1000 | Loss: 0.00000855
Iteration 188/1000 | Loss: 0.00000855
Iteration 189/1000 | Loss: 0.00000854
Iteration 190/1000 | Loss: 0.00000854
Iteration 191/1000 | Loss: 0.00000854
Iteration 192/1000 | Loss: 0.00000854
Iteration 193/1000 | Loss: 0.00000854
Iteration 194/1000 | Loss: 0.00000854
Iteration 195/1000 | Loss: 0.00000854
Iteration 196/1000 | Loss: 0.00000854
Iteration 197/1000 | Loss: 0.00000854
Iteration 198/1000 | Loss: 0.00000854
Iteration 199/1000 | Loss: 0.00000854
Iteration 200/1000 | Loss: 0.00000854
Iteration 201/1000 | Loss: 0.00000854
Iteration 202/1000 | Loss: 0.00000854
Iteration 203/1000 | Loss: 0.00000854
Iteration 204/1000 | Loss: 0.00000854
Iteration 205/1000 | Loss: 0.00000854
Iteration 206/1000 | Loss: 0.00000853
Iteration 207/1000 | Loss: 0.00000853
Iteration 208/1000 | Loss: 0.00000853
Iteration 209/1000 | Loss: 0.00000853
Iteration 210/1000 | Loss: 0.00000853
Iteration 211/1000 | Loss: 0.00000853
Iteration 212/1000 | Loss: 0.00000853
Iteration 213/1000 | Loss: 0.00000853
Iteration 214/1000 | Loss: 0.00000853
Iteration 215/1000 | Loss: 0.00000853
Iteration 216/1000 | Loss: 0.00000853
Iteration 217/1000 | Loss: 0.00000853
Iteration 218/1000 | Loss: 0.00000853
Iteration 219/1000 | Loss: 0.00000853
Iteration 220/1000 | Loss: 0.00000853
Iteration 221/1000 | Loss: 0.00000853
Iteration 222/1000 | Loss: 0.00000853
Iteration 223/1000 | Loss: 0.00000853
Iteration 224/1000 | Loss: 0.00000853
Iteration 225/1000 | Loss: 0.00000853
Iteration 226/1000 | Loss: 0.00000853
Iteration 227/1000 | Loss: 0.00000853
Iteration 228/1000 | Loss: 0.00000853
Iteration 229/1000 | Loss: 0.00000853
Iteration 230/1000 | Loss: 0.00000853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [8.531432285963092e-06, 8.531432285963092e-06, 8.531432285963092e-06, 8.531432285963092e-06, 8.531432285963092e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.531432285963092e-06

Optimization complete. Final v2v error: 2.5103158950805664 mm

Highest mean error: 2.9236834049224854 mm for frame 92

Lowest mean error: 2.3022406101226807 mm for frame 128

Saving results

Total time: 38.355509757995605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531858
Iteration 2/25 | Loss: 0.00135665
Iteration 3/25 | Loss: 0.00116668
Iteration 4/25 | Loss: 0.00114755
Iteration 5/25 | Loss: 0.00114334
Iteration 6/25 | Loss: 0.00114435
Iteration 7/25 | Loss: 0.00114079
Iteration 8/25 | Loss: 0.00114059
Iteration 9/25 | Loss: 0.00113382
Iteration 10/25 | Loss: 0.00113343
Iteration 11/25 | Loss: 0.00113790
Iteration 12/25 | Loss: 0.00113795
Iteration 13/25 | Loss: 0.00113300
Iteration 14/25 | Loss: 0.00113202
Iteration 15/25 | Loss: 0.00113201
Iteration 16/25 | Loss: 0.00113282
Iteration 17/25 | Loss: 0.00113151
Iteration 18/25 | Loss: 0.00113151
Iteration 19/25 | Loss: 0.00113151
Iteration 20/25 | Loss: 0.00113151
Iteration 21/25 | Loss: 0.00113151
Iteration 22/25 | Loss: 0.00113151
Iteration 23/25 | Loss: 0.00113151
Iteration 24/25 | Loss: 0.00113151
Iteration 25/25 | Loss: 0.00113151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57475591
Iteration 2/25 | Loss: 0.00098292
Iteration 3/25 | Loss: 0.00095789
Iteration 4/25 | Loss: 0.00095789
Iteration 5/25 | Loss: 0.00095788
Iteration 6/25 | Loss: 0.00095788
Iteration 7/25 | Loss: 0.00095788
Iteration 8/25 | Loss: 0.00095788
Iteration 9/25 | Loss: 0.00095788
Iteration 10/25 | Loss: 0.00095788
Iteration 11/25 | Loss: 0.00095788
Iteration 12/25 | Loss: 0.00095788
Iteration 13/25 | Loss: 0.00095788
Iteration 14/25 | Loss: 0.00095788
Iteration 15/25 | Loss: 0.00095788
Iteration 16/25 | Loss: 0.00095788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009578826720826328, 0.0009578826720826328, 0.0009578826720826328, 0.0009578826720826328, 0.0009578826720826328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009578826720826328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095788
Iteration 2/1000 | Loss: 0.00005752
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001835
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001678
Iteration 7/1000 | Loss: 0.00009699
Iteration 8/1000 | Loss: 0.00001932
Iteration 9/1000 | Loss: 0.00001624
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001363
Iteration 16/1000 | Loss: 0.00001359
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001358
Iteration 19/1000 | Loss: 0.00001357
Iteration 20/1000 | Loss: 0.00001355
Iteration 21/1000 | Loss: 0.00001354
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001351
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001348
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001347
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001334
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001333
Iteration 58/1000 | Loss: 0.00001333
Iteration 59/1000 | Loss: 0.00001333
Iteration 60/1000 | Loss: 0.00001333
Iteration 61/1000 | Loss: 0.00001332
Iteration 62/1000 | Loss: 0.00001332
Iteration 63/1000 | Loss: 0.00001332
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001328
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001327
Iteration 73/1000 | Loss: 0.00001326
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001325
Iteration 76/1000 | Loss: 0.00001325
Iteration 77/1000 | Loss: 0.00001325
Iteration 78/1000 | Loss: 0.00001325
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001325
Iteration 82/1000 | Loss: 0.00001325
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001324
Iteration 87/1000 | Loss: 0.00001324
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001323
Iteration 93/1000 | Loss: 0.00001323
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001321
Iteration 103/1000 | Loss: 0.00001321
Iteration 104/1000 | Loss: 0.00001321
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001320
Iteration 114/1000 | Loss: 0.00001320
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001320
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001319
Iteration 122/1000 | Loss: 0.00001319
Iteration 123/1000 | Loss: 0.00001319
Iteration 124/1000 | Loss: 0.00001319
Iteration 125/1000 | Loss: 0.00001319
Iteration 126/1000 | Loss: 0.00001318
Iteration 127/1000 | Loss: 0.00001318
Iteration 128/1000 | Loss: 0.00001318
Iteration 129/1000 | Loss: 0.00001317
Iteration 130/1000 | Loss: 0.00001317
Iteration 131/1000 | Loss: 0.00001317
Iteration 132/1000 | Loss: 0.00001317
Iteration 133/1000 | Loss: 0.00001317
Iteration 134/1000 | Loss: 0.00001317
Iteration 135/1000 | Loss: 0.00001317
Iteration 136/1000 | Loss: 0.00001316
Iteration 137/1000 | Loss: 0.00001316
Iteration 138/1000 | Loss: 0.00001316
Iteration 139/1000 | Loss: 0.00001316
Iteration 140/1000 | Loss: 0.00001316
Iteration 141/1000 | Loss: 0.00001316
Iteration 142/1000 | Loss: 0.00001316
Iteration 143/1000 | Loss: 0.00001315
Iteration 144/1000 | Loss: 0.00001315
Iteration 145/1000 | Loss: 0.00001315
Iteration 146/1000 | Loss: 0.00001315
Iteration 147/1000 | Loss: 0.00001315
Iteration 148/1000 | Loss: 0.00001315
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001314
Iteration 157/1000 | Loss: 0.00001314
Iteration 158/1000 | Loss: 0.00001314
Iteration 159/1000 | Loss: 0.00001314
Iteration 160/1000 | Loss: 0.00001314
Iteration 161/1000 | Loss: 0.00001314
Iteration 162/1000 | Loss: 0.00001314
Iteration 163/1000 | Loss: 0.00001314
Iteration 164/1000 | Loss: 0.00001314
Iteration 165/1000 | Loss: 0.00001314
Iteration 166/1000 | Loss: 0.00001314
Iteration 167/1000 | Loss: 0.00001313
Iteration 168/1000 | Loss: 0.00001313
Iteration 169/1000 | Loss: 0.00001313
Iteration 170/1000 | Loss: 0.00001313
Iteration 171/1000 | Loss: 0.00001313
Iteration 172/1000 | Loss: 0.00001313
Iteration 173/1000 | Loss: 0.00001313
Iteration 174/1000 | Loss: 0.00001313
Iteration 175/1000 | Loss: 0.00001313
Iteration 176/1000 | Loss: 0.00001313
Iteration 177/1000 | Loss: 0.00001313
Iteration 178/1000 | Loss: 0.00001313
Iteration 179/1000 | Loss: 0.00001313
Iteration 180/1000 | Loss: 0.00001313
Iteration 181/1000 | Loss: 0.00001313
Iteration 182/1000 | Loss: 0.00001313
Iteration 183/1000 | Loss: 0.00001313
Iteration 184/1000 | Loss: 0.00001313
Iteration 185/1000 | Loss: 0.00001313
Iteration 186/1000 | Loss: 0.00001313
Iteration 187/1000 | Loss: 0.00001313
Iteration 188/1000 | Loss: 0.00001313
Iteration 189/1000 | Loss: 0.00001313
Iteration 190/1000 | Loss: 0.00001313
Iteration 191/1000 | Loss: 0.00001313
Iteration 192/1000 | Loss: 0.00001313
Iteration 193/1000 | Loss: 0.00001313
Iteration 194/1000 | Loss: 0.00001313
Iteration 195/1000 | Loss: 0.00001313
Iteration 196/1000 | Loss: 0.00001313
Iteration 197/1000 | Loss: 0.00001313
Iteration 198/1000 | Loss: 0.00001313
Iteration 199/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.3128246791893616e-05, 1.3128246791893616e-05, 1.3128246791893616e-05, 1.3128246791893616e-05, 1.3128246791893616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3128246791893616e-05

Optimization complete. Final v2v error: 3.059100866317749 mm

Highest mean error: 4.246092319488525 mm for frame 55

Lowest mean error: 2.772799015045166 mm for frame 135

Saving results

Total time: 73.01782822608948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00679419
Iteration 2/25 | Loss: 0.00159603
Iteration 3/25 | Loss: 0.00139182
Iteration 4/25 | Loss: 0.00138424
Iteration 5/25 | Loss: 0.00138296
Iteration 6/25 | Loss: 0.00138296
Iteration 7/25 | Loss: 0.00138296
Iteration 8/25 | Loss: 0.00138296
Iteration 9/25 | Loss: 0.00138296
Iteration 10/25 | Loss: 0.00138296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013829623349010944, 0.0013829623349010944, 0.0013829623349010944, 0.0013829623349010944, 0.0013829623349010944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013829623349010944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.62642169
Iteration 2/25 | Loss: 0.00070533
Iteration 3/25 | Loss: 0.00070531
Iteration 4/25 | Loss: 0.00070531
Iteration 5/25 | Loss: 0.00070531
Iteration 6/25 | Loss: 0.00070530
Iteration 7/25 | Loss: 0.00070530
Iteration 8/25 | Loss: 0.00070530
Iteration 9/25 | Loss: 0.00070530
Iteration 10/25 | Loss: 0.00070530
Iteration 11/25 | Loss: 0.00070530
Iteration 12/25 | Loss: 0.00070530
Iteration 13/25 | Loss: 0.00070530
Iteration 14/25 | Loss: 0.00070530
Iteration 15/25 | Loss: 0.00070530
Iteration 16/25 | Loss: 0.00070530
Iteration 17/25 | Loss: 0.00070530
Iteration 18/25 | Loss: 0.00070530
Iteration 19/25 | Loss: 0.00070530
Iteration 20/25 | Loss: 0.00070530
Iteration 21/25 | Loss: 0.00070530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007053031004033983, 0.0007053031004033983, 0.0007053031004033983, 0.0007053031004033983, 0.0007053031004033983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007053031004033983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070530
Iteration 2/1000 | Loss: 0.00008238
Iteration 3/1000 | Loss: 0.00004947
Iteration 4/1000 | Loss: 0.00004009
Iteration 5/1000 | Loss: 0.00003775
Iteration 6/1000 | Loss: 0.00003664
Iteration 7/1000 | Loss: 0.00003578
Iteration 8/1000 | Loss: 0.00003503
Iteration 9/1000 | Loss: 0.00003409
Iteration 10/1000 | Loss: 0.00003335
Iteration 11/1000 | Loss: 0.00003289
Iteration 12/1000 | Loss: 0.00003248
Iteration 13/1000 | Loss: 0.00003218
Iteration 14/1000 | Loss: 0.00003193
Iteration 15/1000 | Loss: 0.00003181
Iteration 16/1000 | Loss: 0.00003160
Iteration 17/1000 | Loss: 0.00003141
Iteration 18/1000 | Loss: 0.00003130
Iteration 19/1000 | Loss: 0.00003127
Iteration 20/1000 | Loss: 0.00003116
Iteration 21/1000 | Loss: 0.00003098
Iteration 22/1000 | Loss: 0.00003085
Iteration 23/1000 | Loss: 0.00003081
Iteration 24/1000 | Loss: 0.00003080
Iteration 25/1000 | Loss: 0.00003079
Iteration 26/1000 | Loss: 0.00003071
Iteration 27/1000 | Loss: 0.00003071
Iteration 28/1000 | Loss: 0.00003070
Iteration 29/1000 | Loss: 0.00003070
Iteration 30/1000 | Loss: 0.00003070
Iteration 31/1000 | Loss: 0.00003065
Iteration 32/1000 | Loss: 0.00003062
Iteration 33/1000 | Loss: 0.00003062
Iteration 34/1000 | Loss: 0.00003061
Iteration 35/1000 | Loss: 0.00003061
Iteration 36/1000 | Loss: 0.00003061
Iteration 37/1000 | Loss: 0.00003061
Iteration 38/1000 | Loss: 0.00003061
Iteration 39/1000 | Loss: 0.00003061
Iteration 40/1000 | Loss: 0.00003061
Iteration 41/1000 | Loss: 0.00003059
Iteration 42/1000 | Loss: 0.00003059
Iteration 43/1000 | Loss: 0.00003059
Iteration 44/1000 | Loss: 0.00003056
Iteration 45/1000 | Loss: 0.00003056
Iteration 46/1000 | Loss: 0.00003055
Iteration 47/1000 | Loss: 0.00003053
Iteration 48/1000 | Loss: 0.00003053
Iteration 49/1000 | Loss: 0.00003053
Iteration 50/1000 | Loss: 0.00003053
Iteration 51/1000 | Loss: 0.00003053
Iteration 52/1000 | Loss: 0.00003053
Iteration 53/1000 | Loss: 0.00003053
Iteration 54/1000 | Loss: 0.00003053
Iteration 55/1000 | Loss: 0.00003053
Iteration 56/1000 | Loss: 0.00003053
Iteration 57/1000 | Loss: 0.00003052
Iteration 58/1000 | Loss: 0.00003052
Iteration 59/1000 | Loss: 0.00003052
Iteration 60/1000 | Loss: 0.00003052
Iteration 61/1000 | Loss: 0.00003051
Iteration 62/1000 | Loss: 0.00003051
Iteration 63/1000 | Loss: 0.00003051
Iteration 64/1000 | Loss: 0.00003051
Iteration 65/1000 | Loss: 0.00003051
Iteration 66/1000 | Loss: 0.00003051
Iteration 67/1000 | Loss: 0.00003050
Iteration 68/1000 | Loss: 0.00003050
Iteration 69/1000 | Loss: 0.00003050
Iteration 70/1000 | Loss: 0.00003050
Iteration 71/1000 | Loss: 0.00003050
Iteration 72/1000 | Loss: 0.00003050
Iteration 73/1000 | Loss: 0.00003050
Iteration 74/1000 | Loss: 0.00003050
Iteration 75/1000 | Loss: 0.00003049
Iteration 76/1000 | Loss: 0.00003049
Iteration 77/1000 | Loss: 0.00003049
Iteration 78/1000 | Loss: 0.00003049
Iteration 79/1000 | Loss: 0.00003049
Iteration 80/1000 | Loss: 0.00003049
Iteration 81/1000 | Loss: 0.00003049
Iteration 82/1000 | Loss: 0.00003048
Iteration 83/1000 | Loss: 0.00003047
Iteration 84/1000 | Loss: 0.00003047
Iteration 85/1000 | Loss: 0.00003047
Iteration 86/1000 | Loss: 0.00003047
Iteration 87/1000 | Loss: 0.00003047
Iteration 88/1000 | Loss: 0.00003047
Iteration 89/1000 | Loss: 0.00003047
Iteration 90/1000 | Loss: 0.00003047
Iteration 91/1000 | Loss: 0.00003047
Iteration 92/1000 | Loss: 0.00003047
Iteration 93/1000 | Loss: 0.00003046
Iteration 94/1000 | Loss: 0.00003046
Iteration 95/1000 | Loss: 0.00003046
Iteration 96/1000 | Loss: 0.00003046
Iteration 97/1000 | Loss: 0.00003045
Iteration 98/1000 | Loss: 0.00003045
Iteration 99/1000 | Loss: 0.00003045
Iteration 100/1000 | Loss: 0.00003045
Iteration 101/1000 | Loss: 0.00003045
Iteration 102/1000 | Loss: 0.00003045
Iteration 103/1000 | Loss: 0.00003045
Iteration 104/1000 | Loss: 0.00003045
Iteration 105/1000 | Loss: 0.00003045
Iteration 106/1000 | Loss: 0.00003045
Iteration 107/1000 | Loss: 0.00003045
Iteration 108/1000 | Loss: 0.00003045
Iteration 109/1000 | Loss: 0.00003045
Iteration 110/1000 | Loss: 0.00003045
Iteration 111/1000 | Loss: 0.00003045
Iteration 112/1000 | Loss: 0.00003045
Iteration 113/1000 | Loss: 0.00003045
Iteration 114/1000 | Loss: 0.00003045
Iteration 115/1000 | Loss: 0.00003045
Iteration 116/1000 | Loss: 0.00003045
Iteration 117/1000 | Loss: 0.00003045
Iteration 118/1000 | Loss: 0.00003045
Iteration 119/1000 | Loss: 0.00003045
Iteration 120/1000 | Loss: 0.00003045
Iteration 121/1000 | Loss: 0.00003045
Iteration 122/1000 | Loss: 0.00003045
Iteration 123/1000 | Loss: 0.00003045
Iteration 124/1000 | Loss: 0.00003045
Iteration 125/1000 | Loss: 0.00003045
Iteration 126/1000 | Loss: 0.00003045
Iteration 127/1000 | Loss: 0.00003045
Iteration 128/1000 | Loss: 0.00003045
Iteration 129/1000 | Loss: 0.00003045
Iteration 130/1000 | Loss: 0.00003045
Iteration 131/1000 | Loss: 0.00003045
Iteration 132/1000 | Loss: 0.00003045
Iteration 133/1000 | Loss: 0.00003045
Iteration 134/1000 | Loss: 0.00003045
Iteration 135/1000 | Loss: 0.00003045
Iteration 136/1000 | Loss: 0.00003045
Iteration 137/1000 | Loss: 0.00003045
Iteration 138/1000 | Loss: 0.00003045
Iteration 139/1000 | Loss: 0.00003045
Iteration 140/1000 | Loss: 0.00003045
Iteration 141/1000 | Loss: 0.00003045
Iteration 142/1000 | Loss: 0.00003045
Iteration 143/1000 | Loss: 0.00003045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.044907316507306e-05, 3.044907316507306e-05, 3.044907316507306e-05, 3.044907316507306e-05, 3.044907316507306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.044907316507306e-05

Optimization complete. Final v2v error: 4.52694034576416 mm

Highest mean error: 5.021225452423096 mm for frame 95

Lowest mean error: 4.0556721687316895 mm for frame 107

Saving results

Total time: 47.529536962509155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919165
Iteration 2/25 | Loss: 0.00132057
Iteration 3/25 | Loss: 0.00113698
Iteration 4/25 | Loss: 0.00111700
Iteration 5/25 | Loss: 0.00111153
Iteration 6/25 | Loss: 0.00111060
Iteration 7/25 | Loss: 0.00111060
Iteration 8/25 | Loss: 0.00111060
Iteration 9/25 | Loss: 0.00111060
Iteration 10/25 | Loss: 0.00111060
Iteration 11/25 | Loss: 0.00111060
Iteration 12/25 | Loss: 0.00111060
Iteration 13/25 | Loss: 0.00111060
Iteration 14/25 | Loss: 0.00111060
Iteration 15/25 | Loss: 0.00111060
Iteration 16/25 | Loss: 0.00111060
Iteration 17/25 | Loss: 0.00111060
Iteration 18/25 | Loss: 0.00111060
Iteration 19/25 | Loss: 0.00111060
Iteration 20/25 | Loss: 0.00111060
Iteration 21/25 | Loss: 0.00111060
Iteration 22/25 | Loss: 0.00111060
Iteration 23/25 | Loss: 0.00111060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011105951853096485, 0.0011105951853096485, 0.0011105951853096485, 0.0011105951853096485, 0.0011105951853096485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011105951853096485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46826267
Iteration 2/25 | Loss: 0.00085463
Iteration 3/25 | Loss: 0.00085462
Iteration 4/25 | Loss: 0.00085462
Iteration 5/25 | Loss: 0.00085462
Iteration 6/25 | Loss: 0.00085462
Iteration 7/25 | Loss: 0.00085462
Iteration 8/25 | Loss: 0.00085462
Iteration 9/25 | Loss: 0.00085462
Iteration 10/25 | Loss: 0.00085462
Iteration 11/25 | Loss: 0.00085462
Iteration 12/25 | Loss: 0.00085462
Iteration 13/25 | Loss: 0.00085462
Iteration 14/25 | Loss: 0.00085462
Iteration 15/25 | Loss: 0.00085462
Iteration 16/25 | Loss: 0.00085462
Iteration 17/25 | Loss: 0.00085462
Iteration 18/25 | Loss: 0.00085462
Iteration 19/25 | Loss: 0.00085462
Iteration 20/25 | Loss: 0.00085462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008546217577531934, 0.0008546217577531934, 0.0008546217577531934, 0.0008546217577531934, 0.0008546217577531934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008546217577531934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085462
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00001733
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001508
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001301
Iteration 12/1000 | Loss: 0.00001296
Iteration 13/1000 | Loss: 0.00001291
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001267
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001265
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001255
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001251
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001247
Iteration 39/1000 | Loss: 0.00001247
Iteration 40/1000 | Loss: 0.00001247
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001244
Iteration 46/1000 | Loss: 0.00001244
Iteration 47/1000 | Loss: 0.00001244
Iteration 48/1000 | Loss: 0.00001243
Iteration 49/1000 | Loss: 0.00001243
Iteration 50/1000 | Loss: 0.00001242
Iteration 51/1000 | Loss: 0.00001242
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001239
Iteration 58/1000 | Loss: 0.00001239
Iteration 59/1000 | Loss: 0.00001239
Iteration 60/1000 | Loss: 0.00001239
Iteration 61/1000 | Loss: 0.00001239
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001237
Iteration 69/1000 | Loss: 0.00001237
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001237
Iteration 73/1000 | Loss: 0.00001237
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001232
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001231
Iteration 124/1000 | Loss: 0.00001231
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001228
Iteration 137/1000 | Loss: 0.00001228
Iteration 138/1000 | Loss: 0.00001228
Iteration 139/1000 | Loss: 0.00001228
Iteration 140/1000 | Loss: 0.00001228
Iteration 141/1000 | Loss: 0.00001228
Iteration 142/1000 | Loss: 0.00001228
Iteration 143/1000 | Loss: 0.00001227
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001226
Iteration 149/1000 | Loss: 0.00001226
Iteration 150/1000 | Loss: 0.00001226
Iteration 151/1000 | Loss: 0.00001226
Iteration 152/1000 | Loss: 0.00001225
Iteration 153/1000 | Loss: 0.00001225
Iteration 154/1000 | Loss: 0.00001225
Iteration 155/1000 | Loss: 0.00001225
Iteration 156/1000 | Loss: 0.00001225
Iteration 157/1000 | Loss: 0.00001225
Iteration 158/1000 | Loss: 0.00001225
Iteration 159/1000 | Loss: 0.00001225
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Iteration 163/1000 | Loss: 0.00001224
Iteration 164/1000 | Loss: 0.00001224
Iteration 165/1000 | Loss: 0.00001224
Iteration 166/1000 | Loss: 0.00001224
Iteration 167/1000 | Loss: 0.00001224
Iteration 168/1000 | Loss: 0.00001224
Iteration 169/1000 | Loss: 0.00001224
Iteration 170/1000 | Loss: 0.00001224
Iteration 171/1000 | Loss: 0.00001224
Iteration 172/1000 | Loss: 0.00001224
Iteration 173/1000 | Loss: 0.00001224
Iteration 174/1000 | Loss: 0.00001224
Iteration 175/1000 | Loss: 0.00001224
Iteration 176/1000 | Loss: 0.00001224
Iteration 177/1000 | Loss: 0.00001224
Iteration 178/1000 | Loss: 0.00001224
Iteration 179/1000 | Loss: 0.00001224
Iteration 180/1000 | Loss: 0.00001224
Iteration 181/1000 | Loss: 0.00001224
Iteration 182/1000 | Loss: 0.00001224
Iteration 183/1000 | Loss: 0.00001224
Iteration 184/1000 | Loss: 0.00001224
Iteration 185/1000 | Loss: 0.00001224
Iteration 186/1000 | Loss: 0.00001224
Iteration 187/1000 | Loss: 0.00001224
Iteration 188/1000 | Loss: 0.00001224
Iteration 189/1000 | Loss: 0.00001224
Iteration 190/1000 | Loss: 0.00001224
Iteration 191/1000 | Loss: 0.00001224
Iteration 192/1000 | Loss: 0.00001224
Iteration 193/1000 | Loss: 0.00001224
Iteration 194/1000 | Loss: 0.00001224
Iteration 195/1000 | Loss: 0.00001224
Iteration 196/1000 | Loss: 0.00001224
Iteration 197/1000 | Loss: 0.00001224
Iteration 198/1000 | Loss: 0.00001224
Iteration 199/1000 | Loss: 0.00001224
Iteration 200/1000 | Loss: 0.00001224
Iteration 201/1000 | Loss: 0.00001224
Iteration 202/1000 | Loss: 0.00001224
Iteration 203/1000 | Loss: 0.00001224
Iteration 204/1000 | Loss: 0.00001224
Iteration 205/1000 | Loss: 0.00001224
Iteration 206/1000 | Loss: 0.00001224
Iteration 207/1000 | Loss: 0.00001224
Iteration 208/1000 | Loss: 0.00001224
Iteration 209/1000 | Loss: 0.00001224
Iteration 210/1000 | Loss: 0.00001224
Iteration 211/1000 | Loss: 0.00001224
Iteration 212/1000 | Loss: 0.00001224
Iteration 213/1000 | Loss: 0.00001224
Iteration 214/1000 | Loss: 0.00001224
Iteration 215/1000 | Loss: 0.00001224
Iteration 216/1000 | Loss: 0.00001224
Iteration 217/1000 | Loss: 0.00001224
Iteration 218/1000 | Loss: 0.00001224
Iteration 219/1000 | Loss: 0.00001224
Iteration 220/1000 | Loss: 0.00001224
Iteration 221/1000 | Loss: 0.00001224
Iteration 222/1000 | Loss: 0.00001224
Iteration 223/1000 | Loss: 0.00001224
Iteration 224/1000 | Loss: 0.00001224
Iteration 225/1000 | Loss: 0.00001224
Iteration 226/1000 | Loss: 0.00001224
Iteration 227/1000 | Loss: 0.00001224
Iteration 228/1000 | Loss: 0.00001224
Iteration 229/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.2241192962392233e-05, 1.2241192962392233e-05, 1.2241192962392233e-05, 1.2241192962392233e-05, 1.2241192962392233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2241192962392233e-05

Optimization complete. Final v2v error: 2.950437545776367 mm

Highest mean error: 3.4737133979797363 mm for frame 5

Lowest mean error: 2.447680711746216 mm for frame 176

Saving results

Total time: 46.31937026977539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971919
Iteration 2/25 | Loss: 0.00255513
Iteration 3/25 | Loss: 0.00209744
Iteration 4/25 | Loss: 0.00195554
Iteration 5/25 | Loss: 0.00184296
Iteration 6/25 | Loss: 0.00174140
Iteration 7/25 | Loss: 0.00168632
Iteration 8/25 | Loss: 0.00162774
Iteration 9/25 | Loss: 0.00155131
Iteration 10/25 | Loss: 0.00151502
Iteration 11/25 | Loss: 0.00149626
Iteration 12/25 | Loss: 0.00147794
Iteration 13/25 | Loss: 0.00147149
Iteration 14/25 | Loss: 0.00146117
Iteration 15/25 | Loss: 0.00146039
Iteration 16/25 | Loss: 0.00145934
Iteration 17/25 | Loss: 0.00145470
Iteration 18/25 | Loss: 0.00145351
Iteration 19/25 | Loss: 0.00145583
Iteration 20/25 | Loss: 0.00145324
Iteration 21/25 | Loss: 0.00145228
Iteration 22/25 | Loss: 0.00145198
Iteration 23/25 | Loss: 0.00145187
Iteration 24/25 | Loss: 0.00145187
Iteration 25/25 | Loss: 0.00145187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33591259
Iteration 2/25 | Loss: 0.00226653
Iteration 3/25 | Loss: 0.00226653
Iteration 4/25 | Loss: 0.00226652
Iteration 5/25 | Loss: 0.00226652
Iteration 6/25 | Loss: 0.00226652
Iteration 7/25 | Loss: 0.00226652
Iteration 8/25 | Loss: 0.00226652
Iteration 9/25 | Loss: 0.00226652
Iteration 10/25 | Loss: 0.00226652
Iteration 11/25 | Loss: 0.00226652
Iteration 12/25 | Loss: 0.00226652
Iteration 13/25 | Loss: 0.00226652
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002266521332785487, 0.002266521332785487, 0.002266521332785487, 0.002266521332785487, 0.002266521332785487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002266521332785487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226652
Iteration 2/1000 | Loss: 0.00032488
Iteration 3/1000 | Loss: 0.00024861
Iteration 4/1000 | Loss: 0.00021745
Iteration 5/1000 | Loss: 0.00019994
Iteration 6/1000 | Loss: 0.00018414
Iteration 7/1000 | Loss: 0.00017333
Iteration 8/1000 | Loss: 0.00016335
Iteration 9/1000 | Loss: 0.00015436
Iteration 10/1000 | Loss: 0.00014845
Iteration 11/1000 | Loss: 0.00014212
Iteration 12/1000 | Loss: 0.00013761
Iteration 13/1000 | Loss: 0.00067643
Iteration 14/1000 | Loss: 0.00832817
Iteration 15/1000 | Loss: 0.00110152
Iteration 16/1000 | Loss: 0.00026814
Iteration 17/1000 | Loss: 0.00018937
Iteration 18/1000 | Loss: 0.00013846
Iteration 19/1000 | Loss: 0.00007929
Iteration 20/1000 | Loss: 0.00005704
Iteration 21/1000 | Loss: 0.00004634
Iteration 22/1000 | Loss: 0.00003975
Iteration 23/1000 | Loss: 0.00003431
Iteration 24/1000 | Loss: 0.00002979
Iteration 25/1000 | Loss: 0.00002689
Iteration 26/1000 | Loss: 0.00002521
Iteration 27/1000 | Loss: 0.00002401
Iteration 28/1000 | Loss: 0.00002312
Iteration 29/1000 | Loss: 0.00002218
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00002114
Iteration 32/1000 | Loss: 0.00002089
Iteration 33/1000 | Loss: 0.00002062
Iteration 34/1000 | Loss: 0.00002062
Iteration 35/1000 | Loss: 0.00002046
Iteration 36/1000 | Loss: 0.00002045
Iteration 37/1000 | Loss: 0.00002041
Iteration 38/1000 | Loss: 0.00002038
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00002038
Iteration 41/1000 | Loss: 0.00002037
Iteration 42/1000 | Loss: 0.00002037
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002035
Iteration 47/1000 | Loss: 0.00002035
Iteration 48/1000 | Loss: 0.00002035
Iteration 49/1000 | Loss: 0.00002034
Iteration 50/1000 | Loss: 0.00002034
Iteration 51/1000 | Loss: 0.00002034
Iteration 52/1000 | Loss: 0.00002033
Iteration 53/1000 | Loss: 0.00002033
Iteration 54/1000 | Loss: 0.00002033
Iteration 55/1000 | Loss: 0.00002033
Iteration 56/1000 | Loss: 0.00002033
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00002032
Iteration 59/1000 | Loss: 0.00002032
Iteration 60/1000 | Loss: 0.00002032
Iteration 61/1000 | Loss: 0.00002032
Iteration 62/1000 | Loss: 0.00002032
Iteration 63/1000 | Loss: 0.00002032
Iteration 64/1000 | Loss: 0.00002032
Iteration 65/1000 | Loss: 0.00002032
Iteration 66/1000 | Loss: 0.00002032
Iteration 67/1000 | Loss: 0.00002031
Iteration 68/1000 | Loss: 0.00002031
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002030
Iteration 72/1000 | Loss: 0.00002030
Iteration 73/1000 | Loss: 0.00002030
Iteration 74/1000 | Loss: 0.00002030
Iteration 75/1000 | Loss: 0.00002030
Iteration 76/1000 | Loss: 0.00002030
Iteration 77/1000 | Loss: 0.00002029
Iteration 78/1000 | Loss: 0.00002029
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002029
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002028
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002028
Iteration 85/1000 | Loss: 0.00002028
Iteration 86/1000 | Loss: 0.00002028
Iteration 87/1000 | Loss: 0.00002028
Iteration 88/1000 | Loss: 0.00002027
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002027
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002027
Iteration 94/1000 | Loss: 0.00002027
Iteration 95/1000 | Loss: 0.00002027
Iteration 96/1000 | Loss: 0.00002027
Iteration 97/1000 | Loss: 0.00002026
Iteration 98/1000 | Loss: 0.00002026
Iteration 99/1000 | Loss: 0.00002026
Iteration 100/1000 | Loss: 0.00002026
Iteration 101/1000 | Loss: 0.00002025
Iteration 102/1000 | Loss: 0.00002025
Iteration 103/1000 | Loss: 0.00002025
Iteration 104/1000 | Loss: 0.00002025
Iteration 105/1000 | Loss: 0.00002025
Iteration 106/1000 | Loss: 0.00002025
Iteration 107/1000 | Loss: 0.00002024
Iteration 108/1000 | Loss: 0.00002024
Iteration 109/1000 | Loss: 0.00002024
Iteration 110/1000 | Loss: 0.00002024
Iteration 111/1000 | Loss: 0.00002023
Iteration 112/1000 | Loss: 0.00002023
Iteration 113/1000 | Loss: 0.00002022
Iteration 114/1000 | Loss: 0.00002022
Iteration 115/1000 | Loss: 0.00002022
Iteration 116/1000 | Loss: 0.00002021
Iteration 117/1000 | Loss: 0.00002021
Iteration 118/1000 | Loss: 0.00002021
Iteration 119/1000 | Loss: 0.00002021
Iteration 120/1000 | Loss: 0.00002021
Iteration 121/1000 | Loss: 0.00002021
Iteration 122/1000 | Loss: 0.00002021
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002021
Iteration 125/1000 | Loss: 0.00002021
Iteration 126/1000 | Loss: 0.00002021
Iteration 127/1000 | Loss: 0.00002021
Iteration 128/1000 | Loss: 0.00002021
Iteration 129/1000 | Loss: 0.00002021
Iteration 130/1000 | Loss: 0.00002021
Iteration 131/1000 | Loss: 0.00002021
Iteration 132/1000 | Loss: 0.00002021
Iteration 133/1000 | Loss: 0.00002021
Iteration 134/1000 | Loss: 0.00002021
Iteration 135/1000 | Loss: 0.00002021
Iteration 136/1000 | Loss: 0.00002021
Iteration 137/1000 | Loss: 0.00002021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.0206729459459893e-05, 2.0206729459459893e-05, 2.0206729459459893e-05, 2.0206729459459893e-05, 2.0206729459459893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0206729459459893e-05

Optimization complete. Final v2v error: 3.8216636180877686 mm

Highest mean error: 4.006065368652344 mm for frame 163

Lowest mean error: 3.7194461822509766 mm for frame 120

Saving results

Total time: 107.5538399219513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986709
Iteration 2/25 | Loss: 0.00219989
Iteration 3/25 | Loss: 0.00164860
Iteration 4/25 | Loss: 0.00150747
Iteration 5/25 | Loss: 0.00164449
Iteration 6/25 | Loss: 0.00159937
Iteration 7/25 | Loss: 0.00145399
Iteration 8/25 | Loss: 0.00135825
Iteration 9/25 | Loss: 0.00130557
Iteration 10/25 | Loss: 0.00128414
Iteration 11/25 | Loss: 0.00120980
Iteration 12/25 | Loss: 0.00117843
Iteration 13/25 | Loss: 0.00117852
Iteration 14/25 | Loss: 0.00115987
Iteration 15/25 | Loss: 0.00117704
Iteration 16/25 | Loss: 0.00115479
Iteration 17/25 | Loss: 0.00116773
Iteration 18/25 | Loss: 0.00115229
Iteration 19/25 | Loss: 0.00114667
Iteration 20/25 | Loss: 0.00114504
Iteration 21/25 | Loss: 0.00114482
Iteration 22/25 | Loss: 0.00114468
Iteration 23/25 | Loss: 0.00114596
Iteration 24/25 | Loss: 0.00114467
Iteration 25/25 | Loss: 0.00114179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45773125
Iteration 2/25 | Loss: 0.00110364
Iteration 3/25 | Loss: 0.00110364
Iteration 4/25 | Loss: 0.00110364
Iteration 5/25 | Loss: 0.00110364
Iteration 6/25 | Loss: 0.00110364
Iteration 7/25 | Loss: 0.00110364
Iteration 8/25 | Loss: 0.00110364
Iteration 9/25 | Loss: 0.00110364
Iteration 10/25 | Loss: 0.00110364
Iteration 11/25 | Loss: 0.00110364
Iteration 12/25 | Loss: 0.00110364
Iteration 13/25 | Loss: 0.00110364
Iteration 14/25 | Loss: 0.00110364
Iteration 15/25 | Loss: 0.00110364
Iteration 16/25 | Loss: 0.00110364
Iteration 17/25 | Loss: 0.00110364
Iteration 18/25 | Loss: 0.00110364
Iteration 19/25 | Loss: 0.00110364
Iteration 20/25 | Loss: 0.00110364
Iteration 21/25 | Loss: 0.00110364
Iteration 22/25 | Loss: 0.00110364
Iteration 23/25 | Loss: 0.00110364
Iteration 24/25 | Loss: 0.00110364
Iteration 25/25 | Loss: 0.00110364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110364
Iteration 2/1000 | Loss: 0.00059678
Iteration 3/1000 | Loss: 0.00041682
Iteration 4/1000 | Loss: 0.00174555
Iteration 5/1000 | Loss: 0.00174867
Iteration 6/1000 | Loss: 0.00115324
Iteration 7/1000 | Loss: 0.00125093
Iteration 8/1000 | Loss: 0.00122673
Iteration 9/1000 | Loss: 0.00086574
Iteration 10/1000 | Loss: 0.00097632
Iteration 11/1000 | Loss: 0.00056896
Iteration 12/1000 | Loss: 0.00115468
Iteration 13/1000 | Loss: 0.00079567
Iteration 14/1000 | Loss: 0.00072195
Iteration 15/1000 | Loss: 0.00085114
Iteration 16/1000 | Loss: 0.00079657
Iteration 17/1000 | Loss: 0.00063380
Iteration 18/1000 | Loss: 0.00104826
Iteration 19/1000 | Loss: 0.00062050
Iteration 20/1000 | Loss: 0.00105444
Iteration 21/1000 | Loss: 0.00137162
Iteration 22/1000 | Loss: 0.00055874
Iteration 23/1000 | Loss: 0.00059093
Iteration 24/1000 | Loss: 0.00059241
Iteration 25/1000 | Loss: 0.00082393
Iteration 26/1000 | Loss: 0.00143180
Iteration 27/1000 | Loss: 0.00161607
Iteration 28/1000 | Loss: 0.00096241
Iteration 29/1000 | Loss: 0.00092681
Iteration 30/1000 | Loss: 0.00078222
Iteration 31/1000 | Loss: 0.00085053
Iteration 32/1000 | Loss: 0.00046823
Iteration 33/1000 | Loss: 0.00092583
Iteration 34/1000 | Loss: 0.00067232
Iteration 35/1000 | Loss: 0.00070917
Iteration 36/1000 | Loss: 0.00003653
Iteration 37/1000 | Loss: 0.00099298
Iteration 38/1000 | Loss: 0.00054984
Iteration 39/1000 | Loss: 0.00132590
Iteration 40/1000 | Loss: 0.00118921
Iteration 41/1000 | Loss: 0.00226638
Iteration 42/1000 | Loss: 0.00167616
Iteration 43/1000 | Loss: 0.00107321
Iteration 44/1000 | Loss: 0.00065321
Iteration 45/1000 | Loss: 0.00081280
Iteration 46/1000 | Loss: 0.00059174
Iteration 47/1000 | Loss: 0.00006563
Iteration 48/1000 | Loss: 0.00005308
Iteration 49/1000 | Loss: 0.00003734
Iteration 50/1000 | Loss: 0.00002818
Iteration 51/1000 | Loss: 0.00002668
Iteration 52/1000 | Loss: 0.00002511
Iteration 53/1000 | Loss: 0.00051800
Iteration 54/1000 | Loss: 0.00008861
Iteration 55/1000 | Loss: 0.00004876
Iteration 56/1000 | Loss: 0.00002359
Iteration 57/1000 | Loss: 0.00142273
Iteration 58/1000 | Loss: 0.00002979
Iteration 59/1000 | Loss: 0.00002023
Iteration 60/1000 | Loss: 0.00001669
Iteration 61/1000 | Loss: 0.00001539
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00019450
Iteration 67/1000 | Loss: 0.00051554
Iteration 68/1000 | Loss: 0.00042328
Iteration 69/1000 | Loss: 0.00004018
Iteration 70/1000 | Loss: 0.00002508
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00033154
Iteration 73/1000 | Loss: 0.00008896
Iteration 74/1000 | Loss: 0.00001560
Iteration 75/1000 | Loss: 0.00031256
Iteration 76/1000 | Loss: 0.00001462
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001351
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001350
Iteration 87/1000 | Loss: 0.00001349
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001347
Iteration 96/1000 | Loss: 0.00001347
Iteration 97/1000 | Loss: 0.00001347
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001345
Iteration 105/1000 | Loss: 0.00001345
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001344
Iteration 109/1000 | Loss: 0.00001344
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001344
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001344
Iteration 114/1000 | Loss: 0.00001343
Iteration 115/1000 | Loss: 0.00001343
Iteration 116/1000 | Loss: 0.00001343
Iteration 117/1000 | Loss: 0.00001343
Iteration 118/1000 | Loss: 0.00001343
Iteration 119/1000 | Loss: 0.00001343
Iteration 120/1000 | Loss: 0.00001343
Iteration 121/1000 | Loss: 0.00001343
Iteration 122/1000 | Loss: 0.00001342
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00019136
Iteration 125/1000 | Loss: 0.00019136
Iteration 126/1000 | Loss: 0.00002586
Iteration 127/1000 | Loss: 0.00001812
Iteration 128/1000 | Loss: 0.00029636
Iteration 129/1000 | Loss: 0.00011644
Iteration 130/1000 | Loss: 0.00002603
Iteration 131/1000 | Loss: 0.00002111
Iteration 132/1000 | Loss: 0.00001841
Iteration 133/1000 | Loss: 0.00001726
Iteration 134/1000 | Loss: 0.00003332
Iteration 135/1000 | Loss: 0.00001776
Iteration 136/1000 | Loss: 0.00001565
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001400
Iteration 141/1000 | Loss: 0.00001387
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001746
Iteration 144/1000 | Loss: 0.00001371
Iteration 145/1000 | Loss: 0.00001371
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001365
Iteration 148/1000 | Loss: 0.00001331
Iteration 149/1000 | Loss: 0.00033759
Iteration 150/1000 | Loss: 0.00001346
Iteration 151/1000 | Loss: 0.00001317
Iteration 152/1000 | Loss: 0.00001308
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001297
Iteration 156/1000 | Loss: 0.00001296
Iteration 157/1000 | Loss: 0.00001295
Iteration 158/1000 | Loss: 0.00001295
Iteration 159/1000 | Loss: 0.00001295
Iteration 160/1000 | Loss: 0.00001294
Iteration 161/1000 | Loss: 0.00001294
Iteration 162/1000 | Loss: 0.00001294
Iteration 163/1000 | Loss: 0.00001293
Iteration 164/1000 | Loss: 0.00001293
Iteration 165/1000 | Loss: 0.00001293
Iteration 166/1000 | Loss: 0.00001293
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001292
Iteration 169/1000 | Loss: 0.00001292
Iteration 170/1000 | Loss: 0.00001292
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Iteration 175/1000 | Loss: 0.00001291
Iteration 176/1000 | Loss: 0.00001291
Iteration 177/1000 | Loss: 0.00001291
Iteration 178/1000 | Loss: 0.00001291
Iteration 179/1000 | Loss: 0.00001291
Iteration 180/1000 | Loss: 0.00001291
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001290
Iteration 183/1000 | Loss: 0.00001290
Iteration 184/1000 | Loss: 0.00001290
Iteration 185/1000 | Loss: 0.00001290
Iteration 186/1000 | Loss: 0.00001290
Iteration 187/1000 | Loss: 0.00001290
Iteration 188/1000 | Loss: 0.00001290
Iteration 189/1000 | Loss: 0.00001290
Iteration 190/1000 | Loss: 0.00001290
Iteration 191/1000 | Loss: 0.00001289
Iteration 192/1000 | Loss: 0.00001289
Iteration 193/1000 | Loss: 0.00001289
Iteration 194/1000 | Loss: 0.00001289
Iteration 195/1000 | Loss: 0.00001289
Iteration 196/1000 | Loss: 0.00001289
Iteration 197/1000 | Loss: 0.00001289
Iteration 198/1000 | Loss: 0.00001288
Iteration 199/1000 | Loss: 0.00001288
Iteration 200/1000 | Loss: 0.00001288
Iteration 201/1000 | Loss: 0.00001288
Iteration 202/1000 | Loss: 0.00001288
Iteration 203/1000 | Loss: 0.00001288
Iteration 204/1000 | Loss: 0.00001288
Iteration 205/1000 | Loss: 0.00001288
Iteration 206/1000 | Loss: 0.00001288
Iteration 207/1000 | Loss: 0.00001287
Iteration 208/1000 | Loss: 0.00001287
Iteration 209/1000 | Loss: 0.00001287
Iteration 210/1000 | Loss: 0.00001287
Iteration 211/1000 | Loss: 0.00001287
Iteration 212/1000 | Loss: 0.00001287
Iteration 213/1000 | Loss: 0.00001287
Iteration 214/1000 | Loss: 0.00001286
Iteration 215/1000 | Loss: 0.00001286
Iteration 216/1000 | Loss: 0.00001286
Iteration 217/1000 | Loss: 0.00001286
Iteration 218/1000 | Loss: 0.00001285
Iteration 219/1000 | Loss: 0.00001285
Iteration 220/1000 | Loss: 0.00001285
Iteration 221/1000 | Loss: 0.00001285
Iteration 222/1000 | Loss: 0.00001284
Iteration 223/1000 | Loss: 0.00001284
Iteration 224/1000 | Loss: 0.00001284
Iteration 225/1000 | Loss: 0.00001284
Iteration 226/1000 | Loss: 0.00001284
Iteration 227/1000 | Loss: 0.00001283
Iteration 228/1000 | Loss: 0.00001283
Iteration 229/1000 | Loss: 0.00001283
Iteration 230/1000 | Loss: 0.00001283
Iteration 231/1000 | Loss: 0.00001283
Iteration 232/1000 | Loss: 0.00001283
Iteration 233/1000 | Loss: 0.00001283
Iteration 234/1000 | Loss: 0.00001282
Iteration 235/1000 | Loss: 0.00001282
Iteration 236/1000 | Loss: 0.00001282
Iteration 237/1000 | Loss: 0.00001282
Iteration 238/1000 | Loss: 0.00001282
Iteration 239/1000 | Loss: 0.00001281
Iteration 240/1000 | Loss: 0.00001281
Iteration 241/1000 | Loss: 0.00001280
Iteration 242/1000 | Loss: 0.00001280
Iteration 243/1000 | Loss: 0.00001280
Iteration 244/1000 | Loss: 0.00001280
Iteration 245/1000 | Loss: 0.00001280
Iteration 246/1000 | Loss: 0.00001280
Iteration 247/1000 | Loss: 0.00001280
Iteration 248/1000 | Loss: 0.00001280
Iteration 249/1000 | Loss: 0.00001280
Iteration 250/1000 | Loss: 0.00001280
Iteration 251/1000 | Loss: 0.00001280
Iteration 252/1000 | Loss: 0.00001280
Iteration 253/1000 | Loss: 0.00001280
Iteration 254/1000 | Loss: 0.00001280
Iteration 255/1000 | Loss: 0.00001280
Iteration 256/1000 | Loss: 0.00001280
Iteration 257/1000 | Loss: 0.00001279
Iteration 258/1000 | Loss: 0.00001279
Iteration 259/1000 | Loss: 0.00001279
Iteration 260/1000 | Loss: 0.00001279
Iteration 261/1000 | Loss: 0.00001279
Iteration 262/1000 | Loss: 0.00001279
Iteration 263/1000 | Loss: 0.00001279
Iteration 264/1000 | Loss: 0.00001279
Iteration 265/1000 | Loss: 0.00001279
Iteration 266/1000 | Loss: 0.00001279
Iteration 267/1000 | Loss: 0.00001279
Iteration 268/1000 | Loss: 0.00001279
Iteration 269/1000 | Loss: 0.00001279
Iteration 270/1000 | Loss: 0.00001279
Iteration 271/1000 | Loss: 0.00001279
Iteration 272/1000 | Loss: 0.00001279
Iteration 273/1000 | Loss: 0.00001279
Iteration 274/1000 | Loss: 0.00001278
Iteration 275/1000 | Loss: 0.00001278
Iteration 276/1000 | Loss: 0.00001278
Iteration 277/1000 | Loss: 0.00001278
Iteration 278/1000 | Loss: 0.00001278
Iteration 279/1000 | Loss: 0.00001278
Iteration 280/1000 | Loss: 0.00001278
Iteration 281/1000 | Loss: 0.00001278
Iteration 282/1000 | Loss: 0.00001278
Iteration 283/1000 | Loss: 0.00001278
Iteration 284/1000 | Loss: 0.00001278
Iteration 285/1000 | Loss: 0.00001278
Iteration 286/1000 | Loss: 0.00001278
Iteration 287/1000 | Loss: 0.00001278
Iteration 288/1000 | Loss: 0.00001278
Iteration 289/1000 | Loss: 0.00001278
Iteration 290/1000 | Loss: 0.00001277
Iteration 291/1000 | Loss: 0.00001277
Iteration 292/1000 | Loss: 0.00001277
Iteration 293/1000 | Loss: 0.00001277
Iteration 294/1000 | Loss: 0.00001277
Iteration 295/1000 | Loss: 0.00001277
Iteration 296/1000 | Loss: 0.00001277
Iteration 297/1000 | Loss: 0.00001277
Iteration 298/1000 | Loss: 0.00001277
Iteration 299/1000 | Loss: 0.00001277
Iteration 300/1000 | Loss: 0.00001277
Iteration 301/1000 | Loss: 0.00001277
Iteration 302/1000 | Loss: 0.00001277
Iteration 303/1000 | Loss: 0.00001277
Iteration 304/1000 | Loss: 0.00001276
Iteration 305/1000 | Loss: 0.00001276
Iteration 306/1000 | Loss: 0.00001276
Iteration 307/1000 | Loss: 0.00001276
Iteration 308/1000 | Loss: 0.00001276
Iteration 309/1000 | Loss: 0.00001276
Iteration 310/1000 | Loss: 0.00001276
Iteration 311/1000 | Loss: 0.00001276
Iteration 312/1000 | Loss: 0.00001276
Iteration 313/1000 | Loss: 0.00001276
Iteration 314/1000 | Loss: 0.00001276
Iteration 315/1000 | Loss: 0.00001276
Iteration 316/1000 | Loss: 0.00001275
Iteration 317/1000 | Loss: 0.00001275
Iteration 318/1000 | Loss: 0.00001275
Iteration 319/1000 | Loss: 0.00001275
Iteration 320/1000 | Loss: 0.00001275
Iteration 321/1000 | Loss: 0.00001275
Iteration 322/1000 | Loss: 0.00001275
Iteration 323/1000 | Loss: 0.00001275
Iteration 324/1000 | Loss: 0.00001275
Iteration 325/1000 | Loss: 0.00001275
Iteration 326/1000 | Loss: 0.00001275
Iteration 327/1000 | Loss: 0.00001275
Iteration 328/1000 | Loss: 0.00001275
Iteration 329/1000 | Loss: 0.00001275
Iteration 330/1000 | Loss: 0.00001275
Iteration 331/1000 | Loss: 0.00001275
Iteration 332/1000 | Loss: 0.00001275
Iteration 333/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [1.2749095731123816e-05, 1.2749095731123816e-05, 1.2749095731123816e-05, 1.2749095731123816e-05, 1.2749095731123816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2749095731123816e-05

Optimization complete. Final v2v error: 2.911463737487793 mm

Highest mean error: 4.6251749992370605 mm for frame 79

Lowest mean error: 2.4787771701812744 mm for frame 125

Saving results

Total time: 198.40387296676636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035200
Iteration 2/25 | Loss: 0.00328871
Iteration 3/25 | Loss: 0.00282481
Iteration 4/25 | Loss: 0.00271678
Iteration 5/25 | Loss: 0.00263070
Iteration 6/25 | Loss: 0.00252954
Iteration 7/25 | Loss: 0.00247458
Iteration 8/25 | Loss: 0.00239387
Iteration 9/25 | Loss: 0.00235295
Iteration 10/25 | Loss: 0.00233860
Iteration 11/25 | Loss: 0.00233634
Iteration 12/25 | Loss: 0.00230603
Iteration 13/25 | Loss: 0.00227218
Iteration 14/25 | Loss: 0.00225957
Iteration 15/25 | Loss: 0.00225854
Iteration 16/25 | Loss: 0.00226008
Iteration 17/25 | Loss: 0.00224459
Iteration 18/25 | Loss: 0.00222881
Iteration 19/25 | Loss: 0.00221152
Iteration 20/25 | Loss: 0.00220694
Iteration 21/25 | Loss: 0.00220254
Iteration 22/25 | Loss: 0.00219794
Iteration 23/25 | Loss: 0.00218639
Iteration 24/25 | Loss: 0.00218896
Iteration 25/25 | Loss: 0.00218137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11166418
Iteration 2/25 | Loss: 0.00452698
Iteration 3/25 | Loss: 0.00452682
Iteration 4/25 | Loss: 0.00452682
Iteration 5/25 | Loss: 0.00452682
Iteration 6/25 | Loss: 0.00452681
Iteration 7/25 | Loss: 0.00452681
Iteration 8/25 | Loss: 0.00452681
Iteration 9/25 | Loss: 0.00452681
Iteration 10/25 | Loss: 0.00452681
Iteration 11/25 | Loss: 0.00452681
Iteration 12/25 | Loss: 0.00452681
Iteration 13/25 | Loss: 0.00452681
Iteration 14/25 | Loss: 0.00452681
Iteration 15/25 | Loss: 0.00452681
Iteration 16/25 | Loss: 0.00452681
Iteration 17/25 | Loss: 0.00452681
Iteration 18/25 | Loss: 0.00452681
Iteration 19/25 | Loss: 0.00452681
Iteration 20/25 | Loss: 0.00452681
Iteration 21/25 | Loss: 0.00452681
Iteration 22/25 | Loss: 0.00452681
Iteration 23/25 | Loss: 0.00452681
Iteration 24/25 | Loss: 0.00452681
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0045268116518855095, 0.0045268116518855095, 0.0045268116518855095, 0.0045268116518855095, 0.0045268116518855095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045268116518855095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00452681
Iteration 2/1000 | Loss: 0.00339474
Iteration 3/1000 | Loss: 0.00268327
Iteration 4/1000 | Loss: 0.00223462
Iteration 5/1000 | Loss: 0.00987834
Iteration 6/1000 | Loss: 0.00450030
Iteration 7/1000 | Loss: 0.01297640
Iteration 8/1000 | Loss: 0.00670615
Iteration 9/1000 | Loss: 0.00203426
Iteration 10/1000 | Loss: 0.00375888
Iteration 11/1000 | Loss: 0.00649944
Iteration 12/1000 | Loss: 0.00182704
Iteration 13/1000 | Loss: 0.00083272
Iteration 14/1000 | Loss: 0.00123066
Iteration 15/1000 | Loss: 0.00337386
Iteration 16/1000 | Loss: 0.00169872
Iteration 17/1000 | Loss: 0.00237114
Iteration 18/1000 | Loss: 0.00299979
Iteration 19/1000 | Loss: 0.00230537
Iteration 20/1000 | Loss: 0.00158031
Iteration 21/1000 | Loss: 0.00136422
Iteration 22/1000 | Loss: 0.00057929
Iteration 23/1000 | Loss: 0.00211976
Iteration 24/1000 | Loss: 0.00335825
Iteration 25/1000 | Loss: 0.00302189
Iteration 26/1000 | Loss: 0.00239711
Iteration 27/1000 | Loss: 0.00231345
Iteration 28/1000 | Loss: 0.00223704
Iteration 29/1000 | Loss: 0.00223707
Iteration 30/1000 | Loss: 0.00602594
Iteration 31/1000 | Loss: 0.03536952
Iteration 32/1000 | Loss: 0.03051859
Iteration 33/1000 | Loss: 0.02321727
Iteration 34/1000 | Loss: 0.02187825
Iteration 35/1000 | Loss: 0.01932765
Iteration 36/1000 | Loss: 0.01558397
Iteration 37/1000 | Loss: 0.01695006
Iteration 38/1000 | Loss: 0.01695885
Iteration 39/1000 | Loss: 0.01134714
Iteration 40/1000 | Loss: 0.00647976
Iteration 41/1000 | Loss: 0.00778949
Iteration 42/1000 | Loss: 0.01080535
Iteration 43/1000 | Loss: 0.00734337
Iteration 44/1000 | Loss: 0.01040306
Iteration 45/1000 | Loss: 0.00927666
Iteration 46/1000 | Loss: 0.00764555
Iteration 47/1000 | Loss: 0.00632821
Iteration 48/1000 | Loss: 0.00549765
Iteration 49/1000 | Loss: 0.00420130
Iteration 50/1000 | Loss: 0.00505993
Iteration 51/1000 | Loss: 0.00409036
Iteration 52/1000 | Loss: 0.00296259
Iteration 53/1000 | Loss: 0.00358204
Iteration 54/1000 | Loss: 0.00602756
Iteration 55/1000 | Loss: 0.00279325
Iteration 56/1000 | Loss: 0.00184383
Iteration 57/1000 | Loss: 0.00169266
Iteration 58/1000 | Loss: 0.00141022
Iteration 59/1000 | Loss: 0.00149042
Iteration 60/1000 | Loss: 0.00244820
Iteration 61/1000 | Loss: 0.00211445
Iteration 62/1000 | Loss: 0.00221940
Iteration 63/1000 | Loss: 0.00301128
Iteration 64/1000 | Loss: 0.00316611
Iteration 65/1000 | Loss: 0.00295882
Iteration 66/1000 | Loss: 0.00217315
Iteration 67/1000 | Loss: 0.00252628
Iteration 68/1000 | Loss: 0.00185244
Iteration 69/1000 | Loss: 0.00433782
Iteration 70/1000 | Loss: 0.00384091
Iteration 71/1000 | Loss: 0.00297292
Iteration 72/1000 | Loss: 0.00191048
Iteration 73/1000 | Loss: 0.00181001
Iteration 74/1000 | Loss: 0.00397143
Iteration 75/1000 | Loss: 0.00272860
Iteration 76/1000 | Loss: 0.00185854
Iteration 77/1000 | Loss: 0.00332837
Iteration 78/1000 | Loss: 0.00252816
Iteration 79/1000 | Loss: 0.00496387
Iteration 80/1000 | Loss: 0.00316743
Iteration 81/1000 | Loss: 0.00306171
Iteration 82/1000 | Loss: 0.00197671
Iteration 83/1000 | Loss: 0.00342972
Iteration 84/1000 | Loss: 0.00261666
Iteration 85/1000 | Loss: 0.00146769
Iteration 86/1000 | Loss: 0.00109240
Iteration 87/1000 | Loss: 0.00117141
Iteration 88/1000 | Loss: 0.00148614
Iteration 89/1000 | Loss: 0.00204116
Iteration 90/1000 | Loss: 0.00198130
Iteration 91/1000 | Loss: 0.00127064
Iteration 92/1000 | Loss: 0.00094164
Iteration 93/1000 | Loss: 0.00134494
Iteration 94/1000 | Loss: 0.00223870
Iteration 95/1000 | Loss: 0.00242225
Iteration 96/1000 | Loss: 0.00239793
Iteration 97/1000 | Loss: 0.00148250
Iteration 98/1000 | Loss: 0.00234268
Iteration 99/1000 | Loss: 0.00145899
Iteration 100/1000 | Loss: 0.00112786
Iteration 101/1000 | Loss: 0.00202270
Iteration 102/1000 | Loss: 0.00264299
Iteration 103/1000 | Loss: 0.00223655
Iteration 104/1000 | Loss: 0.00236502
Iteration 105/1000 | Loss: 0.00183769
Iteration 106/1000 | Loss: 0.00125302
Iteration 107/1000 | Loss: 0.00094900
Iteration 108/1000 | Loss: 0.00153242
Iteration 109/1000 | Loss: 0.00155888
Iteration 110/1000 | Loss: 0.00185785
Iteration 111/1000 | Loss: 0.00191123
Iteration 112/1000 | Loss: 0.00146899
Iteration 113/1000 | Loss: 0.00235128
Iteration 114/1000 | Loss: 0.00178094
Iteration 115/1000 | Loss: 0.00248093
Iteration 116/1000 | Loss: 0.00269313
Iteration 117/1000 | Loss: 0.00176557
Iteration 118/1000 | Loss: 0.00081384
Iteration 119/1000 | Loss: 0.00115854
Iteration 120/1000 | Loss: 0.00131289
Iteration 121/1000 | Loss: 0.00097466
Iteration 122/1000 | Loss: 0.00108831
Iteration 123/1000 | Loss: 0.00140258
Iteration 124/1000 | Loss: 0.00110619
Iteration 125/1000 | Loss: 0.00090881
Iteration 126/1000 | Loss: 0.00094669
Iteration 127/1000 | Loss: 0.00101407
Iteration 128/1000 | Loss: 0.00046174
Iteration 129/1000 | Loss: 0.00039940
Iteration 130/1000 | Loss: 0.00049550
Iteration 131/1000 | Loss: 0.00115500
Iteration 132/1000 | Loss: 0.00075957
Iteration 133/1000 | Loss: 0.00138036
Iteration 134/1000 | Loss: 0.00070187
Iteration 135/1000 | Loss: 0.00142152
Iteration 136/1000 | Loss: 0.00094538
Iteration 137/1000 | Loss: 0.00057395
Iteration 138/1000 | Loss: 0.00163984
Iteration 139/1000 | Loss: 0.00116647
Iteration 140/1000 | Loss: 0.00058248
Iteration 141/1000 | Loss: 0.00061490
Iteration 142/1000 | Loss: 0.00057197
Iteration 143/1000 | Loss: 0.00087597
Iteration 144/1000 | Loss: 0.00078522
Iteration 145/1000 | Loss: 0.00070582
Iteration 146/1000 | Loss: 0.00051071
Iteration 147/1000 | Loss: 0.00039027
Iteration 148/1000 | Loss: 0.00040101
Iteration 149/1000 | Loss: 0.00077812
Iteration 150/1000 | Loss: 0.00048754
Iteration 151/1000 | Loss: 0.00110938
Iteration 152/1000 | Loss: 0.00100034
Iteration 153/1000 | Loss: 0.00044369
Iteration 154/1000 | Loss: 0.00070413
Iteration 155/1000 | Loss: 0.00055743
Iteration 156/1000 | Loss: 0.00045592
Iteration 157/1000 | Loss: 0.00042109
Iteration 158/1000 | Loss: 0.00044216
Iteration 159/1000 | Loss: 0.00031513
Iteration 160/1000 | Loss: 0.00048662
Iteration 161/1000 | Loss: 0.00071774
Iteration 162/1000 | Loss: 0.00038247
Iteration 163/1000 | Loss: 0.00031893
Iteration 164/1000 | Loss: 0.00030586
Iteration 165/1000 | Loss: 0.00038659
Iteration 166/1000 | Loss: 0.00118455
Iteration 167/1000 | Loss: 0.00028334
Iteration 168/1000 | Loss: 0.00033843
Iteration 169/1000 | Loss: 0.00057964
Iteration 170/1000 | Loss: 0.00030380
Iteration 171/1000 | Loss: 0.00061090
Iteration 172/1000 | Loss: 0.00056104
Iteration 173/1000 | Loss: 0.00024157
Iteration 174/1000 | Loss: 0.00035267
Iteration 175/1000 | Loss: 0.00060101
Iteration 176/1000 | Loss: 0.00173754
Iteration 177/1000 | Loss: 0.00057450
Iteration 178/1000 | Loss: 0.00039738
Iteration 179/1000 | Loss: 0.00053279
Iteration 180/1000 | Loss: 0.00021535
Iteration 181/1000 | Loss: 0.00042108
Iteration 182/1000 | Loss: 0.00022856
Iteration 183/1000 | Loss: 0.00091674
Iteration 184/1000 | Loss: 0.00118055
Iteration 185/1000 | Loss: 0.00100930
Iteration 186/1000 | Loss: 0.00058215
Iteration 187/1000 | Loss: 0.00019132
Iteration 188/1000 | Loss: 0.00077265
Iteration 189/1000 | Loss: 0.00132854
Iteration 190/1000 | Loss: 0.00130843
Iteration 191/1000 | Loss: 0.00031344
Iteration 192/1000 | Loss: 0.00017914
Iteration 193/1000 | Loss: 0.00017162
Iteration 194/1000 | Loss: 0.00147438
Iteration 195/1000 | Loss: 0.00194831
Iteration 196/1000 | Loss: 0.00118598
Iteration 197/1000 | Loss: 0.00060289
Iteration 198/1000 | Loss: 0.00058178
Iteration 199/1000 | Loss: 0.00071357
Iteration 200/1000 | Loss: 0.00068379
Iteration 201/1000 | Loss: 0.00039774
Iteration 202/1000 | Loss: 0.00016077
Iteration 203/1000 | Loss: 0.00040294
Iteration 204/1000 | Loss: 0.00050449
Iteration 205/1000 | Loss: 0.00039684
Iteration 206/1000 | Loss: 0.00074081
Iteration 207/1000 | Loss: 0.00105730
Iteration 208/1000 | Loss: 0.00015996
Iteration 209/1000 | Loss: 0.00052570
Iteration 210/1000 | Loss: 0.00025473
Iteration 211/1000 | Loss: 0.00024555
Iteration 212/1000 | Loss: 0.00031470
Iteration 213/1000 | Loss: 0.00021589
Iteration 214/1000 | Loss: 0.00032785
Iteration 215/1000 | Loss: 0.00040950
Iteration 216/1000 | Loss: 0.00036032
Iteration 217/1000 | Loss: 0.00043776
Iteration 218/1000 | Loss: 0.00042133
Iteration 219/1000 | Loss: 0.00013078
Iteration 220/1000 | Loss: 0.00068152
Iteration 221/1000 | Loss: 0.00043963
Iteration 222/1000 | Loss: 0.00032299
Iteration 223/1000 | Loss: 0.00024948
Iteration 224/1000 | Loss: 0.00016543
Iteration 225/1000 | Loss: 0.00011780
Iteration 226/1000 | Loss: 0.00011303
Iteration 227/1000 | Loss: 0.00018203
Iteration 228/1000 | Loss: 0.00023193
Iteration 229/1000 | Loss: 0.00025994
Iteration 230/1000 | Loss: 0.00010286
Iteration 231/1000 | Loss: 0.00014069
Iteration 232/1000 | Loss: 0.00025082
Iteration 233/1000 | Loss: 0.00031673
Iteration 234/1000 | Loss: 0.00026095
Iteration 235/1000 | Loss: 0.00013377
Iteration 236/1000 | Loss: 0.00023538
Iteration 237/1000 | Loss: 0.00018594
Iteration 238/1000 | Loss: 0.00009513
Iteration 239/1000 | Loss: 0.00008952
Iteration 240/1000 | Loss: 0.00012290
Iteration 241/1000 | Loss: 0.00008314
Iteration 242/1000 | Loss: 0.00019872
Iteration 243/1000 | Loss: 0.00007993
Iteration 244/1000 | Loss: 0.00010795
Iteration 245/1000 | Loss: 0.00014056
Iteration 246/1000 | Loss: 0.00010636
Iteration 247/1000 | Loss: 0.00013418
Iteration 248/1000 | Loss: 0.00010352
Iteration 249/1000 | Loss: 0.00014455
Iteration 250/1000 | Loss: 0.00009600
Iteration 251/1000 | Loss: 0.00012576
Iteration 252/1000 | Loss: 0.00006247
Iteration 253/1000 | Loss: 0.00005921
Iteration 254/1000 | Loss: 0.00005711
Iteration 255/1000 | Loss: 0.00005548
Iteration 256/1000 | Loss: 0.00005425
Iteration 257/1000 | Loss: 0.00005328
Iteration 258/1000 | Loss: 0.00005263
Iteration 259/1000 | Loss: 0.00005201
Iteration 260/1000 | Loss: 0.00005148
Iteration 261/1000 | Loss: 0.00005102
Iteration 262/1000 | Loss: 0.00005064
Iteration 263/1000 | Loss: 0.00005031
Iteration 264/1000 | Loss: 0.00005002
Iteration 265/1000 | Loss: 0.00004976
Iteration 266/1000 | Loss: 0.00004944
Iteration 267/1000 | Loss: 0.00004905
Iteration 268/1000 | Loss: 0.00004873
Iteration 269/1000 | Loss: 0.00004844
Iteration 270/1000 | Loss: 0.00004821
Iteration 271/1000 | Loss: 0.00021291
Iteration 272/1000 | Loss: 0.00020554
Iteration 273/1000 | Loss: 0.00015424
Iteration 274/1000 | Loss: 0.00006216
Iteration 275/1000 | Loss: 0.00005413
Iteration 276/1000 | Loss: 0.00005171
Iteration 277/1000 | Loss: 0.00015844
Iteration 278/1000 | Loss: 0.00006194
Iteration 279/1000 | Loss: 0.00005345
Iteration 280/1000 | Loss: 0.00005167
Iteration 281/1000 | Loss: 0.00006110
Iteration 282/1000 | Loss: 0.00006025
Iteration 283/1000 | Loss: 0.00006097
Iteration 284/1000 | Loss: 0.00005926
Iteration 285/1000 | Loss: 0.00005371
Iteration 286/1000 | Loss: 0.00021696
Iteration 287/1000 | Loss: 0.00005745
Iteration 288/1000 | Loss: 0.00005383
Iteration 289/1000 | Loss: 0.00018854
Iteration 290/1000 | Loss: 0.00021957
Iteration 291/1000 | Loss: 0.00005924
Iteration 292/1000 | Loss: 0.00005376
Iteration 293/1000 | Loss: 0.00005091
Iteration 294/1000 | Loss: 0.00005008
Iteration 295/1000 | Loss: 0.00004959
Iteration 296/1000 | Loss: 0.00004909
Iteration 297/1000 | Loss: 0.00004855
Iteration 298/1000 | Loss: 0.00004814
Iteration 299/1000 | Loss: 0.00004786
Iteration 300/1000 | Loss: 0.00004772
Iteration 301/1000 | Loss: 0.00004772
Iteration 302/1000 | Loss: 0.00004771
Iteration 303/1000 | Loss: 0.00004764
Iteration 304/1000 | Loss: 0.00004761
Iteration 305/1000 | Loss: 0.00004761
Iteration 306/1000 | Loss: 0.00004760
Iteration 307/1000 | Loss: 0.00004760
Iteration 308/1000 | Loss: 0.00004760
Iteration 309/1000 | Loss: 0.00004758
Iteration 310/1000 | Loss: 0.00004755
Iteration 311/1000 | Loss: 0.00004755
Iteration 312/1000 | Loss: 0.00004753
Iteration 313/1000 | Loss: 0.00004753
Iteration 314/1000 | Loss: 0.00004753
Iteration 315/1000 | Loss: 0.00004752
Iteration 316/1000 | Loss: 0.00004750
Iteration 317/1000 | Loss: 0.00004750
Iteration 318/1000 | Loss: 0.00004750
Iteration 319/1000 | Loss: 0.00004748
Iteration 320/1000 | Loss: 0.00004748
Iteration 321/1000 | Loss: 0.00004747
Iteration 322/1000 | Loss: 0.00004746
Iteration 323/1000 | Loss: 0.00004745
Iteration 324/1000 | Loss: 0.00004745
Iteration 325/1000 | Loss: 0.00004744
Iteration 326/1000 | Loss: 0.00004744
Iteration 327/1000 | Loss: 0.00004743
Iteration 328/1000 | Loss: 0.00004743
Iteration 329/1000 | Loss: 0.00004742
Iteration 330/1000 | Loss: 0.00004742
Iteration 331/1000 | Loss: 0.00004741
Iteration 332/1000 | Loss: 0.00004741
Iteration 333/1000 | Loss: 0.00004741
Iteration 334/1000 | Loss: 0.00004741
Iteration 335/1000 | Loss: 0.00004741
Iteration 336/1000 | Loss: 0.00004740
Iteration 337/1000 | Loss: 0.00004740
Iteration 338/1000 | Loss: 0.00004740
Iteration 339/1000 | Loss: 0.00004740
Iteration 340/1000 | Loss: 0.00004740
Iteration 341/1000 | Loss: 0.00004739
Iteration 342/1000 | Loss: 0.00004739
Iteration 343/1000 | Loss: 0.00004739
Iteration 344/1000 | Loss: 0.00004739
Iteration 345/1000 | Loss: 0.00004738
Iteration 346/1000 | Loss: 0.00004738
Iteration 347/1000 | Loss: 0.00004738
Iteration 348/1000 | Loss: 0.00004738
Iteration 349/1000 | Loss: 0.00004738
Iteration 350/1000 | Loss: 0.00004738
Iteration 351/1000 | Loss: 0.00004738
Iteration 352/1000 | Loss: 0.00004738
Iteration 353/1000 | Loss: 0.00004738
Iteration 354/1000 | Loss: 0.00004738
Iteration 355/1000 | Loss: 0.00004738
Iteration 356/1000 | Loss: 0.00004737
Iteration 357/1000 | Loss: 0.00004737
Iteration 358/1000 | Loss: 0.00004737
Iteration 359/1000 | Loss: 0.00004737
Iteration 360/1000 | Loss: 0.00004737
Iteration 361/1000 | Loss: 0.00004737
Iteration 362/1000 | Loss: 0.00004737
Iteration 363/1000 | Loss: 0.00004737
Iteration 364/1000 | Loss: 0.00004737
Iteration 365/1000 | Loss: 0.00004737
Iteration 366/1000 | Loss: 0.00004737
Iteration 367/1000 | Loss: 0.00004737
Iteration 368/1000 | Loss: 0.00004737
Iteration 369/1000 | Loss: 0.00004737
Iteration 370/1000 | Loss: 0.00004737
Iteration 371/1000 | Loss: 0.00004737
Iteration 372/1000 | Loss: 0.00004737
Iteration 373/1000 | Loss: 0.00004737
Iteration 374/1000 | Loss: 0.00004737
Iteration 375/1000 | Loss: 0.00004736
Iteration 376/1000 | Loss: 0.00004736
Iteration 377/1000 | Loss: 0.00004736
Iteration 378/1000 | Loss: 0.00004736
Iteration 379/1000 | Loss: 0.00004736
Iteration 380/1000 | Loss: 0.00004736
Iteration 381/1000 | Loss: 0.00004736
Iteration 382/1000 | Loss: 0.00004736
Iteration 383/1000 | Loss: 0.00004736
Iteration 384/1000 | Loss: 0.00004736
Iteration 385/1000 | Loss: 0.00004736
Iteration 386/1000 | Loss: 0.00004736
Iteration 387/1000 | Loss: 0.00004736
Iteration 388/1000 | Loss: 0.00004736
Iteration 389/1000 | Loss: 0.00004736
Iteration 390/1000 | Loss: 0.00004736
Iteration 391/1000 | Loss: 0.00004735
Iteration 392/1000 | Loss: 0.00004735
Iteration 393/1000 | Loss: 0.00004735
Iteration 394/1000 | Loss: 0.00004735
Iteration 395/1000 | Loss: 0.00004735
Iteration 396/1000 | Loss: 0.00004735
Iteration 397/1000 | Loss: 0.00004735
Iteration 398/1000 | Loss: 0.00004735
Iteration 399/1000 | Loss: 0.00004735
Iteration 400/1000 | Loss: 0.00004735
Iteration 401/1000 | Loss: 0.00004735
Iteration 402/1000 | Loss: 0.00004735
Iteration 403/1000 | Loss: 0.00004735
Iteration 404/1000 | Loss: 0.00004735
Iteration 405/1000 | Loss: 0.00004735
Iteration 406/1000 | Loss: 0.00004735
Iteration 407/1000 | Loss: 0.00004735
Iteration 408/1000 | Loss: 0.00004735
Iteration 409/1000 | Loss: 0.00004735
Iteration 410/1000 | Loss: 0.00004734
Iteration 411/1000 | Loss: 0.00004734
Iteration 412/1000 | Loss: 0.00004734
Iteration 413/1000 | Loss: 0.00004734
Iteration 414/1000 | Loss: 0.00004734
Iteration 415/1000 | Loss: 0.00004734
Iteration 416/1000 | Loss: 0.00004734
Iteration 417/1000 | Loss: 0.00004734
Iteration 418/1000 | Loss: 0.00004734
Iteration 419/1000 | Loss: 0.00004734
Iteration 420/1000 | Loss: 0.00004734
Iteration 421/1000 | Loss: 0.00004734
Iteration 422/1000 | Loss: 0.00004734
Iteration 423/1000 | Loss: 0.00004734
Iteration 424/1000 | Loss: 0.00004734
Iteration 425/1000 | Loss: 0.00004734
Iteration 426/1000 | Loss: 0.00004734
Iteration 427/1000 | Loss: 0.00004734
Iteration 428/1000 | Loss: 0.00004734
Iteration 429/1000 | Loss: 0.00004734
Iteration 430/1000 | Loss: 0.00004734
Iteration 431/1000 | Loss: 0.00004734
Iteration 432/1000 | Loss: 0.00004734
Iteration 433/1000 | Loss: 0.00004734
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 433. Stopping optimization.
Last 5 losses: [4.734448521048762e-05, 4.734448521048762e-05, 4.734448521048762e-05, 4.734448521048762e-05, 4.734448521048762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.734448521048762e-05

Optimization complete. Final v2v error: 5.6045331954956055 mm

Highest mean error: 11.539583206176758 mm for frame 119

Lowest mean error: 4.232203960418701 mm for frame 0

Saving results

Total time: 541.5582041740417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906603
Iteration 2/25 | Loss: 0.00180667
Iteration 3/25 | Loss: 0.00146560
Iteration 4/25 | Loss: 0.00139657
Iteration 5/25 | Loss: 0.00138129
Iteration 6/25 | Loss: 0.00130480
Iteration 7/25 | Loss: 0.00127516
Iteration 8/25 | Loss: 0.00125276
Iteration 9/25 | Loss: 0.00125010
Iteration 10/25 | Loss: 0.00124867
Iteration 11/25 | Loss: 0.00125272
Iteration 12/25 | Loss: 0.00124377
Iteration 13/25 | Loss: 0.00124169
Iteration 14/25 | Loss: 0.00123612
Iteration 15/25 | Loss: 0.00122685
Iteration 16/25 | Loss: 0.00122287
Iteration 17/25 | Loss: 0.00122198
Iteration 18/25 | Loss: 0.00122174
Iteration 19/25 | Loss: 0.00122162
Iteration 20/25 | Loss: 0.00122160
Iteration 21/25 | Loss: 0.00122160
Iteration 22/25 | Loss: 0.00122160
Iteration 23/25 | Loss: 0.00122160
Iteration 24/25 | Loss: 0.00122159
Iteration 25/25 | Loss: 0.00122159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33956027
Iteration 2/25 | Loss: 0.00093700
Iteration 3/25 | Loss: 0.00093698
Iteration 4/25 | Loss: 0.00093698
Iteration 5/25 | Loss: 0.00093698
Iteration 6/25 | Loss: 0.00093698
Iteration 7/25 | Loss: 0.00093698
Iteration 8/25 | Loss: 0.00093698
Iteration 9/25 | Loss: 0.00093698
Iteration 10/25 | Loss: 0.00093698
Iteration 11/25 | Loss: 0.00093698
Iteration 12/25 | Loss: 0.00093698
Iteration 13/25 | Loss: 0.00093698
Iteration 14/25 | Loss: 0.00093698
Iteration 15/25 | Loss: 0.00093698
Iteration 16/25 | Loss: 0.00093698
Iteration 17/25 | Loss: 0.00093698
Iteration 18/25 | Loss: 0.00093698
Iteration 19/25 | Loss: 0.00093698
Iteration 20/25 | Loss: 0.00093698
Iteration 21/25 | Loss: 0.00093698
Iteration 22/25 | Loss: 0.00093698
Iteration 23/25 | Loss: 0.00093698
Iteration 24/25 | Loss: 0.00093698
Iteration 25/25 | Loss: 0.00093698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093698
Iteration 2/1000 | Loss: 0.00036006
Iteration 3/1000 | Loss: 0.00043093
Iteration 4/1000 | Loss: 0.00021301
Iteration 5/1000 | Loss: 0.00018010
Iteration 6/1000 | Loss: 0.00013500
Iteration 7/1000 | Loss: 0.00010063
Iteration 8/1000 | Loss: 0.00008755
Iteration 9/1000 | Loss: 0.00007988
Iteration 10/1000 | Loss: 0.00018445
Iteration 11/1000 | Loss: 0.00012533
Iteration 12/1000 | Loss: 0.00016027
Iteration 13/1000 | Loss: 0.00013394
Iteration 14/1000 | Loss: 0.00010717
Iteration 15/1000 | Loss: 0.00021653
Iteration 16/1000 | Loss: 0.00011047
Iteration 17/1000 | Loss: 0.00012902
Iteration 18/1000 | Loss: 0.00009188
Iteration 19/1000 | Loss: 0.00008097
Iteration 20/1000 | Loss: 0.00007136
Iteration 21/1000 | Loss: 0.00010477
Iteration 22/1000 | Loss: 0.00006143
Iteration 23/1000 | Loss: 0.00005328
Iteration 24/1000 | Loss: 0.00004764
Iteration 25/1000 | Loss: 0.00004500
Iteration 26/1000 | Loss: 0.00004246
Iteration 27/1000 | Loss: 0.00003996
Iteration 28/1000 | Loss: 0.00003826
Iteration 29/1000 | Loss: 0.00003718
Iteration 30/1000 | Loss: 0.00005023
Iteration 31/1000 | Loss: 0.00005723
Iteration 32/1000 | Loss: 0.00004894
Iteration 33/1000 | Loss: 0.00013683
Iteration 34/1000 | Loss: 0.00006248
Iteration 35/1000 | Loss: 0.00007835
Iteration 36/1000 | Loss: 0.00005516
Iteration 37/1000 | Loss: 0.00009606
Iteration 38/1000 | Loss: 0.00007587
Iteration 39/1000 | Loss: 0.00005626
Iteration 40/1000 | Loss: 0.00005328
Iteration 41/1000 | Loss: 0.00006657
Iteration 42/1000 | Loss: 0.00006293
Iteration 43/1000 | Loss: 0.00004637
Iteration 44/1000 | Loss: 0.00004502
Iteration 45/1000 | Loss: 0.00005282
Iteration 46/1000 | Loss: 0.00005749
Iteration 47/1000 | Loss: 0.00004959
Iteration 48/1000 | Loss: 0.00003820
Iteration 49/1000 | Loss: 0.00003664
Iteration 50/1000 | Loss: 0.00003884
Iteration 51/1000 | Loss: 0.00007853
Iteration 52/1000 | Loss: 0.00004684
Iteration 53/1000 | Loss: 0.00003936
Iteration 54/1000 | Loss: 0.00003740
Iteration 55/1000 | Loss: 0.00006325
Iteration 56/1000 | Loss: 0.00005996
Iteration 57/1000 | Loss: 0.00006204
Iteration 58/1000 | Loss: 0.00004583
Iteration 59/1000 | Loss: 0.00003780
Iteration 60/1000 | Loss: 0.00007431
Iteration 61/1000 | Loss: 0.00005304
Iteration 62/1000 | Loss: 0.00003936
Iteration 63/1000 | Loss: 0.00004049
Iteration 64/1000 | Loss: 0.00004583
Iteration 65/1000 | Loss: 0.00003512
Iteration 66/1000 | Loss: 0.00003140
Iteration 67/1000 | Loss: 0.00003269
Iteration 68/1000 | Loss: 0.00002976
Iteration 69/1000 | Loss: 0.00002944
Iteration 70/1000 | Loss: 0.00003746
Iteration 71/1000 | Loss: 0.00003303
Iteration 72/1000 | Loss: 0.00002970
Iteration 73/1000 | Loss: 0.00005638
Iteration 74/1000 | Loss: 0.00005067
Iteration 75/1000 | Loss: 0.00002878
Iteration 76/1000 | Loss: 0.00002772
Iteration 77/1000 | Loss: 0.00006748
Iteration 78/1000 | Loss: 0.00006032
Iteration 79/1000 | Loss: 0.00004408
Iteration 80/1000 | Loss: 0.00003181
Iteration 81/1000 | Loss: 0.00003970
Iteration 82/1000 | Loss: 0.00003404
Iteration 83/1000 | Loss: 0.00003711
Iteration 84/1000 | Loss: 0.00003515
Iteration 85/1000 | Loss: 0.00003669
Iteration 86/1000 | Loss: 0.00003599
Iteration 87/1000 | Loss: 0.00003602
Iteration 88/1000 | Loss: 0.00003587
Iteration 89/1000 | Loss: 0.00002871
Iteration 90/1000 | Loss: 0.00002666
Iteration 91/1000 | Loss: 0.00003368
Iteration 92/1000 | Loss: 0.00003076
Iteration 93/1000 | Loss: 0.00002779
Iteration 94/1000 | Loss: 0.00003111
Iteration 95/1000 | Loss: 0.00003234
Iteration 96/1000 | Loss: 0.00003104
Iteration 97/1000 | Loss: 0.00003165
Iteration 98/1000 | Loss: 0.00002841
Iteration 99/1000 | Loss: 0.00003029
Iteration 100/1000 | Loss: 0.00002940
Iteration 101/1000 | Loss: 0.00002642
Iteration 102/1000 | Loss: 0.00002834
Iteration 103/1000 | Loss: 0.00002651
Iteration 104/1000 | Loss: 0.00002764
Iteration 105/1000 | Loss: 0.00002655
Iteration 106/1000 | Loss: 0.00002754
Iteration 107/1000 | Loss: 0.00002613
Iteration 108/1000 | Loss: 0.00002724
Iteration 109/1000 | Loss: 0.00002632
Iteration 110/1000 | Loss: 0.00002697
Iteration 111/1000 | Loss: 0.00002636
Iteration 112/1000 | Loss: 0.00002686
Iteration 113/1000 | Loss: 0.00002621
Iteration 114/1000 | Loss: 0.00002675
Iteration 115/1000 | Loss: 0.00002640
Iteration 116/1000 | Loss: 0.00002695
Iteration 117/1000 | Loss: 0.00002887
Iteration 118/1000 | Loss: 0.00002674
Iteration 119/1000 | Loss: 0.00002805
Iteration 120/1000 | Loss: 0.00005425
Iteration 121/1000 | Loss: 0.00004881
Iteration 122/1000 | Loss: 0.00003297
Iteration 123/1000 | Loss: 0.00003330
Iteration 124/1000 | Loss: 0.00003036
Iteration 125/1000 | Loss: 0.00006828
Iteration 126/1000 | Loss: 0.00006133
Iteration 127/1000 | Loss: 0.00004118
Iteration 128/1000 | Loss: 0.00004972
Iteration 129/1000 | Loss: 0.00003686
Iteration 130/1000 | Loss: 0.00005723
Iteration 131/1000 | Loss: 0.00005828
Iteration 132/1000 | Loss: 0.00003041
Iteration 133/1000 | Loss: 0.00003172
Iteration 134/1000 | Loss: 0.00002876
Iteration 135/1000 | Loss: 0.00002350
Iteration 136/1000 | Loss: 0.00002336
Iteration 137/1000 | Loss: 0.00002332
Iteration 138/1000 | Loss: 0.00002331
Iteration 139/1000 | Loss: 0.00002331
Iteration 140/1000 | Loss: 0.00002331
Iteration 141/1000 | Loss: 0.00002331
Iteration 142/1000 | Loss: 0.00002331
Iteration 143/1000 | Loss: 0.00002331
Iteration 144/1000 | Loss: 0.00002329
Iteration 145/1000 | Loss: 0.00002329
Iteration 146/1000 | Loss: 0.00002327
Iteration 147/1000 | Loss: 0.00002326
Iteration 148/1000 | Loss: 0.00002326
Iteration 149/1000 | Loss: 0.00002326
Iteration 150/1000 | Loss: 0.00002325
Iteration 151/1000 | Loss: 0.00002325
Iteration 152/1000 | Loss: 0.00002324
Iteration 153/1000 | Loss: 0.00002324
Iteration 154/1000 | Loss: 0.00002324
Iteration 155/1000 | Loss: 0.00002321
Iteration 156/1000 | Loss: 0.00002318
Iteration 157/1000 | Loss: 0.00002318
Iteration 158/1000 | Loss: 0.00002317
Iteration 159/1000 | Loss: 0.00002315
Iteration 160/1000 | Loss: 0.00002314
Iteration 161/1000 | Loss: 0.00002310
Iteration 162/1000 | Loss: 0.00002310
Iteration 163/1000 | Loss: 0.00002310
Iteration 164/1000 | Loss: 0.00002309
Iteration 165/1000 | Loss: 0.00002309
Iteration 166/1000 | Loss: 0.00002308
Iteration 167/1000 | Loss: 0.00002307
Iteration 168/1000 | Loss: 0.00002305
Iteration 169/1000 | Loss: 0.00002304
Iteration 170/1000 | Loss: 0.00005011
Iteration 171/1000 | Loss: 0.00003162
Iteration 172/1000 | Loss: 0.00002925
Iteration 173/1000 | Loss: 0.00004249
Iteration 174/1000 | Loss: 0.00003946
Iteration 175/1000 | Loss: 0.00003904
Iteration 176/1000 | Loss: 0.00002543
Iteration 177/1000 | Loss: 0.00002319
Iteration 178/1000 | Loss: 0.00004931
Iteration 179/1000 | Loss: 0.00003864
Iteration 180/1000 | Loss: 0.00004536
Iteration 181/1000 | Loss: 0.00003779
Iteration 182/1000 | Loss: 0.00002648
Iteration 183/1000 | Loss: 0.00002535
Iteration 184/1000 | Loss: 0.00002502
Iteration 185/1000 | Loss: 0.00002485
Iteration 186/1000 | Loss: 0.00002484
Iteration 187/1000 | Loss: 0.00002469
Iteration 188/1000 | Loss: 0.00002437
Iteration 189/1000 | Loss: 0.00002377
Iteration 190/1000 | Loss: 0.00002323
Iteration 191/1000 | Loss: 0.00002306
Iteration 192/1000 | Loss: 0.00002302
Iteration 193/1000 | Loss: 0.00002301
Iteration 194/1000 | Loss: 0.00002297
Iteration 195/1000 | Loss: 0.00002296
Iteration 196/1000 | Loss: 0.00002296
Iteration 197/1000 | Loss: 0.00002296
Iteration 198/1000 | Loss: 0.00002295
Iteration 199/1000 | Loss: 0.00002294
Iteration 200/1000 | Loss: 0.00002291
Iteration 201/1000 | Loss: 0.00002291
Iteration 202/1000 | Loss: 0.00002287
Iteration 203/1000 | Loss: 0.00002284
Iteration 204/1000 | Loss: 0.00002284
Iteration 205/1000 | Loss: 0.00002284
Iteration 206/1000 | Loss: 0.00002280
Iteration 207/1000 | Loss: 0.00002277
Iteration 208/1000 | Loss: 0.00004994
Iteration 209/1000 | Loss: 0.00003089
Iteration 210/1000 | Loss: 0.00002881
Iteration 211/1000 | Loss: 0.00004552
Iteration 212/1000 | Loss: 0.00002611
Iteration 213/1000 | Loss: 0.00003680
Iteration 214/1000 | Loss: 0.00003659
Iteration 215/1000 | Loss: 0.00003436
Iteration 216/1000 | Loss: 0.00003503
Iteration 217/1000 | Loss: 0.00003421
Iteration 218/1000 | Loss: 0.00003188
Iteration 219/1000 | Loss: 0.00002381
Iteration 220/1000 | Loss: 0.00003217
Iteration 221/1000 | Loss: 0.00002958
Iteration 222/1000 | Loss: 0.00003348
Iteration 223/1000 | Loss: 0.00002519
Iteration 224/1000 | Loss: 0.00002995
Iteration 225/1000 | Loss: 0.00003161
Iteration 226/1000 | Loss: 0.00002502
Iteration 227/1000 | Loss: 0.00002502
Iteration 228/1000 | Loss: 0.00002978
Iteration 229/1000 | Loss: 0.00003519
Iteration 230/1000 | Loss: 0.00002740
Iteration 231/1000 | Loss: 0.00002475
Iteration 232/1000 | Loss: 0.00002339
Iteration 233/1000 | Loss: 0.00002298
Iteration 234/1000 | Loss: 0.00002280
Iteration 235/1000 | Loss: 0.00002275
Iteration 236/1000 | Loss: 0.00002271
Iteration 237/1000 | Loss: 0.00002271
Iteration 238/1000 | Loss: 0.00002271
Iteration 239/1000 | Loss: 0.00002271
Iteration 240/1000 | Loss: 0.00002271
Iteration 241/1000 | Loss: 0.00002271
Iteration 242/1000 | Loss: 0.00002271
Iteration 243/1000 | Loss: 0.00002271
Iteration 244/1000 | Loss: 0.00002271
Iteration 245/1000 | Loss: 0.00002270
Iteration 246/1000 | Loss: 0.00002270
Iteration 247/1000 | Loss: 0.00002270
Iteration 248/1000 | Loss: 0.00002270
Iteration 249/1000 | Loss: 0.00002269
Iteration 250/1000 | Loss: 0.00002269
Iteration 251/1000 | Loss: 0.00002268
Iteration 252/1000 | Loss: 0.00002268
Iteration 253/1000 | Loss: 0.00002268
Iteration 254/1000 | Loss: 0.00002267
Iteration 255/1000 | Loss: 0.00002267
Iteration 256/1000 | Loss: 0.00002267
Iteration 257/1000 | Loss: 0.00002267
Iteration 258/1000 | Loss: 0.00002267
Iteration 259/1000 | Loss: 0.00002267
Iteration 260/1000 | Loss: 0.00002266
Iteration 261/1000 | Loss: 0.00002266
Iteration 262/1000 | Loss: 0.00002266
Iteration 263/1000 | Loss: 0.00002266
Iteration 264/1000 | Loss: 0.00002266
Iteration 265/1000 | Loss: 0.00002265
Iteration 266/1000 | Loss: 0.00002265
Iteration 267/1000 | Loss: 0.00002265
Iteration 268/1000 | Loss: 0.00002265
Iteration 269/1000 | Loss: 0.00002264
Iteration 270/1000 | Loss: 0.00002264
Iteration 271/1000 | Loss: 0.00002264
Iteration 272/1000 | Loss: 0.00002264
Iteration 273/1000 | Loss: 0.00002263
Iteration 274/1000 | Loss: 0.00002263
Iteration 275/1000 | Loss: 0.00002263
Iteration 276/1000 | Loss: 0.00002263
Iteration 277/1000 | Loss: 0.00002263
Iteration 278/1000 | Loss: 0.00002263
Iteration 279/1000 | Loss: 0.00002263
Iteration 280/1000 | Loss: 0.00002263
Iteration 281/1000 | Loss: 0.00002262
Iteration 282/1000 | Loss: 0.00002262
Iteration 283/1000 | Loss: 0.00002262
Iteration 284/1000 | Loss: 0.00002262
Iteration 285/1000 | Loss: 0.00002262
Iteration 286/1000 | Loss: 0.00002261
Iteration 287/1000 | Loss: 0.00002261
Iteration 288/1000 | Loss: 0.00002261
Iteration 289/1000 | Loss: 0.00002261
Iteration 290/1000 | Loss: 0.00002260
Iteration 291/1000 | Loss: 0.00002260
Iteration 292/1000 | Loss: 0.00002260
Iteration 293/1000 | Loss: 0.00002260
Iteration 294/1000 | Loss: 0.00002260
Iteration 295/1000 | Loss: 0.00002260
Iteration 296/1000 | Loss: 0.00002260
Iteration 297/1000 | Loss: 0.00002260
Iteration 298/1000 | Loss: 0.00002260
Iteration 299/1000 | Loss: 0.00002260
Iteration 300/1000 | Loss: 0.00002260
Iteration 301/1000 | Loss: 0.00002259
Iteration 302/1000 | Loss: 0.00002259
Iteration 303/1000 | Loss: 0.00002259
Iteration 304/1000 | Loss: 0.00002259
Iteration 305/1000 | Loss: 0.00002259
Iteration 306/1000 | Loss: 0.00002259
Iteration 307/1000 | Loss: 0.00002259
Iteration 308/1000 | Loss: 0.00002259
Iteration 309/1000 | Loss: 0.00002258
Iteration 310/1000 | Loss: 0.00002258
Iteration 311/1000 | Loss: 0.00002258
Iteration 312/1000 | Loss: 0.00002258
Iteration 313/1000 | Loss: 0.00002258
Iteration 314/1000 | Loss: 0.00002258
Iteration 315/1000 | Loss: 0.00002258
Iteration 316/1000 | Loss: 0.00002258
Iteration 317/1000 | Loss: 0.00002258
Iteration 318/1000 | Loss: 0.00002258
Iteration 319/1000 | Loss: 0.00002258
Iteration 320/1000 | Loss: 0.00002257
Iteration 321/1000 | Loss: 0.00002257
Iteration 322/1000 | Loss: 0.00002257
Iteration 323/1000 | Loss: 0.00002257
Iteration 324/1000 | Loss: 0.00002257
Iteration 325/1000 | Loss: 0.00002257
Iteration 326/1000 | Loss: 0.00002257
Iteration 327/1000 | Loss: 0.00002257
Iteration 328/1000 | Loss: 0.00002257
Iteration 329/1000 | Loss: 0.00002257
Iteration 330/1000 | Loss: 0.00002257
Iteration 331/1000 | Loss: 0.00002257
Iteration 332/1000 | Loss: 0.00002257
Iteration 333/1000 | Loss: 0.00002257
Iteration 334/1000 | Loss: 0.00002257
Iteration 335/1000 | Loss: 0.00002257
Iteration 336/1000 | Loss: 0.00002257
Iteration 337/1000 | Loss: 0.00002257
Iteration 338/1000 | Loss: 0.00002257
Iteration 339/1000 | Loss: 0.00002257
Iteration 340/1000 | Loss: 0.00002257
Iteration 341/1000 | Loss: 0.00002257
Iteration 342/1000 | Loss: 0.00002257
Iteration 343/1000 | Loss: 0.00002257
Iteration 344/1000 | Loss: 0.00002257
Iteration 345/1000 | Loss: 0.00002257
Iteration 346/1000 | Loss: 0.00002257
Iteration 347/1000 | Loss: 0.00002257
Iteration 348/1000 | Loss: 0.00002257
Iteration 349/1000 | Loss: 0.00002257
Iteration 350/1000 | Loss: 0.00002257
Iteration 351/1000 | Loss: 0.00002257
Iteration 352/1000 | Loss: 0.00002257
Iteration 353/1000 | Loss: 0.00002257
Iteration 354/1000 | Loss: 0.00002257
Iteration 355/1000 | Loss: 0.00002257
Iteration 356/1000 | Loss: 0.00002257
Iteration 357/1000 | Loss: 0.00002257
Iteration 358/1000 | Loss: 0.00002257
Iteration 359/1000 | Loss: 0.00002257
Iteration 360/1000 | Loss: 0.00002257
Iteration 361/1000 | Loss: 0.00002257
Iteration 362/1000 | Loss: 0.00002257
Iteration 363/1000 | Loss: 0.00002257
Iteration 364/1000 | Loss: 0.00002257
Iteration 365/1000 | Loss: 0.00002257
Iteration 366/1000 | Loss: 0.00002257
Iteration 367/1000 | Loss: 0.00002257
Iteration 368/1000 | Loss: 0.00002257
Iteration 369/1000 | Loss: 0.00002257
Iteration 370/1000 | Loss: 0.00002257
Iteration 371/1000 | Loss: 0.00002257
Iteration 372/1000 | Loss: 0.00002257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 372. Stopping optimization.
Last 5 losses: [2.2569558495888487e-05, 2.2569558495888487e-05, 2.2569558495888487e-05, 2.2569558495888487e-05, 2.2569558495888487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2569558495888487e-05

Optimization complete. Final v2v error: 3.8304288387298584 mm

Highest mean error: 6.431000232696533 mm for frame 103

Lowest mean error: 2.8628122806549072 mm for frame 74

Saving results

Total time: 314.81505393981934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880063
Iteration 2/25 | Loss: 0.00135537
Iteration 3/25 | Loss: 0.00123060
Iteration 4/25 | Loss: 0.00121220
Iteration 5/25 | Loss: 0.00120612
Iteration 6/25 | Loss: 0.00120554
Iteration 7/25 | Loss: 0.00120554
Iteration 8/25 | Loss: 0.00120554
Iteration 9/25 | Loss: 0.00120554
Iteration 10/25 | Loss: 0.00120554
Iteration 11/25 | Loss: 0.00120554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012055360712110996, 0.0012055360712110996, 0.0012055360712110996, 0.0012055360712110996, 0.0012055360712110996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012055360712110996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28889692
Iteration 2/25 | Loss: 0.00090798
Iteration 3/25 | Loss: 0.00090792
Iteration 4/25 | Loss: 0.00090792
Iteration 5/25 | Loss: 0.00090792
Iteration 6/25 | Loss: 0.00090792
Iteration 7/25 | Loss: 0.00090792
Iteration 8/25 | Loss: 0.00090792
Iteration 9/25 | Loss: 0.00090792
Iteration 10/25 | Loss: 0.00090792
Iteration 11/25 | Loss: 0.00090792
Iteration 12/25 | Loss: 0.00090792
Iteration 13/25 | Loss: 0.00090792
Iteration 14/25 | Loss: 0.00090792
Iteration 15/25 | Loss: 0.00090792
Iteration 16/25 | Loss: 0.00090792
Iteration 17/25 | Loss: 0.00090792
Iteration 18/25 | Loss: 0.00090792
Iteration 19/25 | Loss: 0.00090792
Iteration 20/25 | Loss: 0.00090792
Iteration 21/25 | Loss: 0.00090792
Iteration 22/25 | Loss: 0.00090792
Iteration 23/25 | Loss: 0.00090792
Iteration 24/25 | Loss: 0.00090792
Iteration 25/25 | Loss: 0.00090792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009079183801077306, 0.0009079183801077306, 0.0009079183801077306, 0.0009079183801077306, 0.0009079183801077306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009079183801077306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090792
Iteration 2/1000 | Loss: 0.00004844
Iteration 3/1000 | Loss: 0.00003315
Iteration 4/1000 | Loss: 0.00002975
Iteration 5/1000 | Loss: 0.00002787
Iteration 6/1000 | Loss: 0.00002669
Iteration 7/1000 | Loss: 0.00002602
Iteration 8/1000 | Loss: 0.00002558
Iteration 9/1000 | Loss: 0.00002527
Iteration 10/1000 | Loss: 0.00002496
Iteration 11/1000 | Loss: 0.00002466
Iteration 12/1000 | Loss: 0.00002443
Iteration 13/1000 | Loss: 0.00002416
Iteration 14/1000 | Loss: 0.00002405
Iteration 15/1000 | Loss: 0.00002405
Iteration 16/1000 | Loss: 0.00002398
Iteration 17/1000 | Loss: 0.00002393
Iteration 18/1000 | Loss: 0.00002388
Iteration 19/1000 | Loss: 0.00002386
Iteration 20/1000 | Loss: 0.00002382
Iteration 21/1000 | Loss: 0.00002382
Iteration 22/1000 | Loss: 0.00002372
Iteration 23/1000 | Loss: 0.00002370
Iteration 24/1000 | Loss: 0.00002370
Iteration 25/1000 | Loss: 0.00002369
Iteration 26/1000 | Loss: 0.00002368
Iteration 27/1000 | Loss: 0.00002367
Iteration 28/1000 | Loss: 0.00002367
Iteration 29/1000 | Loss: 0.00002367
Iteration 30/1000 | Loss: 0.00002366
Iteration 31/1000 | Loss: 0.00002366
Iteration 32/1000 | Loss: 0.00002365
Iteration 33/1000 | Loss: 0.00002365
Iteration 34/1000 | Loss: 0.00002365
Iteration 35/1000 | Loss: 0.00002365
Iteration 36/1000 | Loss: 0.00002364
Iteration 37/1000 | Loss: 0.00002364
Iteration 38/1000 | Loss: 0.00002364
Iteration 39/1000 | Loss: 0.00002363
Iteration 40/1000 | Loss: 0.00002363
Iteration 41/1000 | Loss: 0.00002363
Iteration 42/1000 | Loss: 0.00002363
Iteration 43/1000 | Loss: 0.00002363
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002363
Iteration 46/1000 | Loss: 0.00002363
Iteration 47/1000 | Loss: 0.00002363
Iteration 48/1000 | Loss: 0.00002362
Iteration 49/1000 | Loss: 0.00002362
Iteration 50/1000 | Loss: 0.00002362
Iteration 51/1000 | Loss: 0.00002361
Iteration 52/1000 | Loss: 0.00002361
Iteration 53/1000 | Loss: 0.00002361
Iteration 54/1000 | Loss: 0.00002361
Iteration 55/1000 | Loss: 0.00002361
Iteration 56/1000 | Loss: 0.00002361
Iteration 57/1000 | Loss: 0.00002361
Iteration 58/1000 | Loss: 0.00002360
Iteration 59/1000 | Loss: 0.00002360
Iteration 60/1000 | Loss: 0.00002359
Iteration 61/1000 | Loss: 0.00002359
Iteration 62/1000 | Loss: 0.00002359
Iteration 63/1000 | Loss: 0.00002358
Iteration 64/1000 | Loss: 0.00002358
Iteration 65/1000 | Loss: 0.00002358
Iteration 66/1000 | Loss: 0.00002358
Iteration 67/1000 | Loss: 0.00002357
Iteration 68/1000 | Loss: 0.00002356
Iteration 69/1000 | Loss: 0.00002355
Iteration 70/1000 | Loss: 0.00002355
Iteration 71/1000 | Loss: 0.00002355
Iteration 72/1000 | Loss: 0.00002355
Iteration 73/1000 | Loss: 0.00002355
Iteration 74/1000 | Loss: 0.00002355
Iteration 75/1000 | Loss: 0.00002355
Iteration 76/1000 | Loss: 0.00002355
Iteration 77/1000 | Loss: 0.00002355
Iteration 78/1000 | Loss: 0.00002355
Iteration 79/1000 | Loss: 0.00002355
Iteration 80/1000 | Loss: 0.00002355
Iteration 81/1000 | Loss: 0.00002355
Iteration 82/1000 | Loss: 0.00002355
Iteration 83/1000 | Loss: 0.00002355
Iteration 84/1000 | Loss: 0.00002355
Iteration 85/1000 | Loss: 0.00002355
Iteration 86/1000 | Loss: 0.00002355
Iteration 87/1000 | Loss: 0.00002355
Iteration 88/1000 | Loss: 0.00002355
Iteration 89/1000 | Loss: 0.00002355
Iteration 90/1000 | Loss: 0.00002355
Iteration 91/1000 | Loss: 0.00002355
Iteration 92/1000 | Loss: 0.00002355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.354692514927592e-05, 2.354692514927592e-05, 2.354692514927592e-05, 2.354692514927592e-05, 2.354692514927592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.354692514927592e-05

Optimization complete. Final v2v error: 4.014133930206299 mm

Highest mean error: 4.340162754058838 mm for frame 149

Lowest mean error: 3.461726427078247 mm for frame 31

Saving results

Total time: 40.06049466133118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081288
Iteration 2/25 | Loss: 0.01081288
Iteration 3/25 | Loss: 0.00335265
Iteration 4/25 | Loss: 0.00174924
Iteration 5/25 | Loss: 0.00169213
Iteration 6/25 | Loss: 0.00192831
Iteration 7/25 | Loss: 0.00158456
Iteration 8/25 | Loss: 0.00138077
Iteration 9/25 | Loss: 0.00128084
Iteration 10/25 | Loss: 0.00127421
Iteration 11/25 | Loss: 0.00128057
Iteration 12/25 | Loss: 0.00126839
Iteration 13/25 | Loss: 0.00122564
Iteration 14/25 | Loss: 0.00122292
Iteration 15/25 | Loss: 0.00122046
Iteration 16/25 | Loss: 0.00121302
Iteration 17/25 | Loss: 0.00120963
Iteration 18/25 | Loss: 0.00120864
Iteration 19/25 | Loss: 0.00120714
Iteration 20/25 | Loss: 0.00120732
Iteration 21/25 | Loss: 0.00120894
Iteration 22/25 | Loss: 0.00120803
Iteration 23/25 | Loss: 0.00120916
Iteration 24/25 | Loss: 0.00120677
Iteration 25/25 | Loss: 0.00120766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53952926
Iteration 2/25 | Loss: 0.00075331
Iteration 3/25 | Loss: 0.00062894
Iteration 4/25 | Loss: 0.00062894
Iteration 5/25 | Loss: 0.00062894
Iteration 6/25 | Loss: 0.00062894
Iteration 7/25 | Loss: 0.00062894
Iteration 8/25 | Loss: 0.00062893
Iteration 9/25 | Loss: 0.00062893
Iteration 10/25 | Loss: 0.00062893
Iteration 11/25 | Loss: 0.00062893
Iteration 12/25 | Loss: 0.00062893
Iteration 13/25 | Loss: 0.00062893
Iteration 14/25 | Loss: 0.00062893
Iteration 15/25 | Loss: 0.00062893
Iteration 16/25 | Loss: 0.00062893
Iteration 17/25 | Loss: 0.00062893
Iteration 18/25 | Loss: 0.00062893
Iteration 19/25 | Loss: 0.00062893
Iteration 20/25 | Loss: 0.00062893
Iteration 21/25 | Loss: 0.00062893
Iteration 22/25 | Loss: 0.00062893
Iteration 23/25 | Loss: 0.00062893
Iteration 24/25 | Loss: 0.00062893
Iteration 25/25 | Loss: 0.00062893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006289341836236417, 0.0006289341836236417, 0.0006289341836236417, 0.0006289341836236417, 0.0006289341836236417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006289341836236417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062893
Iteration 2/1000 | Loss: 0.00024043
Iteration 3/1000 | Loss: 0.00004969
Iteration 4/1000 | Loss: 0.00016919
Iteration 5/1000 | Loss: 0.00007136
Iteration 6/1000 | Loss: 0.00008572
Iteration 7/1000 | Loss: 0.00015640
Iteration 8/1000 | Loss: 0.00008480
Iteration 9/1000 | Loss: 0.00002876
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001705
Iteration 12/1000 | Loss: 0.00002482
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001843
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001712
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001570
Iteration 22/1000 | Loss: 0.00001570
Iteration 23/1000 | Loss: 0.00001569
Iteration 24/1000 | Loss: 0.00001569
Iteration 25/1000 | Loss: 0.00001569
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001626
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00002594
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00002971
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001731
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001662
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001541
Iteration 41/1000 | Loss: 0.00001541
Iteration 42/1000 | Loss: 0.00001541
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001541
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001541
Iteration 48/1000 | Loss: 0.00001541
Iteration 49/1000 | Loss: 0.00001541
Iteration 50/1000 | Loss: 0.00001541
Iteration 51/1000 | Loss: 0.00001541
Iteration 52/1000 | Loss: 0.00001541
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001539
Iteration 56/1000 | Loss: 0.00001539
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00001578
Iteration 62/1000 | Loss: 0.00001680
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001530
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001530
Iteration 72/1000 | Loss: 0.00001530
Iteration 73/1000 | Loss: 0.00001530
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001529
Iteration 76/1000 | Loss: 0.00001529
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.529448854853399e-05, 1.529448854853399e-05, 1.529448854853399e-05, 1.529448854853399e-05, 1.529448854853399e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.529448854853399e-05

Optimization complete. Final v2v error: 3.3449313640594482 mm

Highest mean error: 3.502330780029297 mm for frame 119

Lowest mean error: 3.219187021255493 mm for frame 58

Saving results

Total time: 102.5515604019165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432460
Iteration 2/25 | Loss: 0.00119332
Iteration 3/25 | Loss: 0.00109955
Iteration 4/25 | Loss: 0.00108481
Iteration 5/25 | Loss: 0.00107986
Iteration 6/25 | Loss: 0.00107879
Iteration 7/25 | Loss: 0.00107879
Iteration 8/25 | Loss: 0.00107879
Iteration 9/25 | Loss: 0.00107879
Iteration 10/25 | Loss: 0.00107879
Iteration 11/25 | Loss: 0.00107879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010787852806970477, 0.0010787852806970477, 0.0010787852806970477, 0.0010787852806970477, 0.0010787852806970477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010787852806970477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.58626986
Iteration 2/25 | Loss: 0.00085624
Iteration 3/25 | Loss: 0.00085622
Iteration 4/25 | Loss: 0.00085622
Iteration 5/25 | Loss: 0.00085622
Iteration 6/25 | Loss: 0.00085622
Iteration 7/25 | Loss: 0.00085622
Iteration 8/25 | Loss: 0.00085622
Iteration 9/25 | Loss: 0.00085622
Iteration 10/25 | Loss: 0.00085622
Iteration 11/25 | Loss: 0.00085622
Iteration 12/25 | Loss: 0.00085622
Iteration 13/25 | Loss: 0.00085622
Iteration 14/25 | Loss: 0.00085622
Iteration 15/25 | Loss: 0.00085622
Iteration 16/25 | Loss: 0.00085622
Iteration 17/25 | Loss: 0.00085622
Iteration 18/25 | Loss: 0.00085622
Iteration 19/25 | Loss: 0.00085622
Iteration 20/25 | Loss: 0.00085622
Iteration 21/25 | Loss: 0.00085622
Iteration 22/25 | Loss: 0.00085622
Iteration 23/25 | Loss: 0.00085622
Iteration 24/25 | Loss: 0.00085622
Iteration 25/25 | Loss: 0.00085622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085622
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001593
Iteration 4/1000 | Loss: 0.00001388
Iteration 5/1000 | Loss: 0.00001309
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001204
Iteration 8/1000 | Loss: 0.00001170
Iteration 9/1000 | Loss: 0.00001147
Iteration 10/1000 | Loss: 0.00001118
Iteration 11/1000 | Loss: 0.00001118
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001110
Iteration 15/1000 | Loss: 0.00001107
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001103
Iteration 18/1000 | Loss: 0.00001094
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001085
Iteration 21/1000 | Loss: 0.00001080
Iteration 22/1000 | Loss: 0.00001073
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001065
Iteration 25/1000 | Loss: 0.00001064
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001063
Iteration 29/1000 | Loss: 0.00001061
Iteration 30/1000 | Loss: 0.00001060
Iteration 31/1000 | Loss: 0.00001059
Iteration 32/1000 | Loss: 0.00001058
Iteration 33/1000 | Loss: 0.00001057
Iteration 34/1000 | Loss: 0.00001057
Iteration 35/1000 | Loss: 0.00001056
Iteration 36/1000 | Loss: 0.00001056
Iteration 37/1000 | Loss: 0.00001055
Iteration 38/1000 | Loss: 0.00001055
Iteration 39/1000 | Loss: 0.00001054
Iteration 40/1000 | Loss: 0.00001053
Iteration 41/1000 | Loss: 0.00001053
Iteration 42/1000 | Loss: 0.00001052
Iteration 43/1000 | Loss: 0.00001052
Iteration 44/1000 | Loss: 0.00001051
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001050
Iteration 47/1000 | Loss: 0.00001049
Iteration 48/1000 | Loss: 0.00001049
Iteration 49/1000 | Loss: 0.00001049
Iteration 50/1000 | Loss: 0.00001048
Iteration 51/1000 | Loss: 0.00001047
Iteration 52/1000 | Loss: 0.00001045
Iteration 53/1000 | Loss: 0.00001045
Iteration 54/1000 | Loss: 0.00001045
Iteration 55/1000 | Loss: 0.00001045
Iteration 56/1000 | Loss: 0.00001044
Iteration 57/1000 | Loss: 0.00001044
Iteration 58/1000 | Loss: 0.00001044
Iteration 59/1000 | Loss: 0.00001044
Iteration 60/1000 | Loss: 0.00001043
Iteration 61/1000 | Loss: 0.00001042
Iteration 62/1000 | Loss: 0.00001042
Iteration 63/1000 | Loss: 0.00001042
Iteration 64/1000 | Loss: 0.00001042
Iteration 65/1000 | Loss: 0.00001041
Iteration 66/1000 | Loss: 0.00001041
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001039
Iteration 73/1000 | Loss: 0.00001039
Iteration 74/1000 | Loss: 0.00001039
Iteration 75/1000 | Loss: 0.00001038
Iteration 76/1000 | Loss: 0.00001038
Iteration 77/1000 | Loss: 0.00001038
Iteration 78/1000 | Loss: 0.00001037
Iteration 79/1000 | Loss: 0.00001037
Iteration 80/1000 | Loss: 0.00001037
Iteration 81/1000 | Loss: 0.00001037
Iteration 82/1000 | Loss: 0.00001037
Iteration 83/1000 | Loss: 0.00001037
Iteration 84/1000 | Loss: 0.00001037
Iteration 85/1000 | Loss: 0.00001037
Iteration 86/1000 | Loss: 0.00001037
Iteration 87/1000 | Loss: 0.00001037
Iteration 88/1000 | Loss: 0.00001037
Iteration 89/1000 | Loss: 0.00001036
Iteration 90/1000 | Loss: 0.00001036
Iteration 91/1000 | Loss: 0.00001036
Iteration 92/1000 | Loss: 0.00001036
Iteration 93/1000 | Loss: 0.00001036
Iteration 94/1000 | Loss: 0.00001036
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001036
Iteration 97/1000 | Loss: 0.00001036
Iteration 98/1000 | Loss: 0.00001036
Iteration 99/1000 | Loss: 0.00001036
Iteration 100/1000 | Loss: 0.00001036
Iteration 101/1000 | Loss: 0.00001036
Iteration 102/1000 | Loss: 0.00001036
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001036
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.0363110959588084e-05, 1.0363110959588084e-05, 1.0363110959588084e-05, 1.0363110959588084e-05, 1.0363110959588084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0363110959588084e-05

Optimization complete. Final v2v error: 2.7717998027801514 mm

Highest mean error: 3.388190984725952 mm for frame 92

Lowest mean error: 2.467554807662964 mm for frame 205

Saving results

Total time: 39.129494428634644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00347486
Iteration 2/25 | Loss: 0.00112822
Iteration 3/25 | Loss: 0.00106015
Iteration 4/25 | Loss: 0.00105051
Iteration 5/25 | Loss: 0.00104630
Iteration 6/25 | Loss: 0.00104541
Iteration 7/25 | Loss: 0.00104541
Iteration 8/25 | Loss: 0.00104541
Iteration 9/25 | Loss: 0.00104541
Iteration 10/25 | Loss: 0.00104541
Iteration 11/25 | Loss: 0.00104541
Iteration 12/25 | Loss: 0.00104541
Iteration 13/25 | Loss: 0.00104541
Iteration 14/25 | Loss: 0.00104541
Iteration 15/25 | Loss: 0.00104541
Iteration 16/25 | Loss: 0.00104541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010454095900058746, 0.0010454095900058746, 0.0010454095900058746, 0.0010454095900058746, 0.0010454095900058746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010454095900058746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34913993
Iteration 2/25 | Loss: 0.00098667
Iteration 3/25 | Loss: 0.00098666
Iteration 4/25 | Loss: 0.00098666
Iteration 5/25 | Loss: 0.00098666
Iteration 6/25 | Loss: 0.00098666
Iteration 7/25 | Loss: 0.00098666
Iteration 8/25 | Loss: 0.00098666
Iteration 9/25 | Loss: 0.00098666
Iteration 10/25 | Loss: 0.00098666
Iteration 11/25 | Loss: 0.00098666
Iteration 12/25 | Loss: 0.00098666
Iteration 13/25 | Loss: 0.00098666
Iteration 14/25 | Loss: 0.00098666
Iteration 15/25 | Loss: 0.00098666
Iteration 16/25 | Loss: 0.00098666
Iteration 17/25 | Loss: 0.00098666
Iteration 18/25 | Loss: 0.00098666
Iteration 19/25 | Loss: 0.00098666
Iteration 20/25 | Loss: 0.00098666
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009866621112450957, 0.0009866621112450957, 0.0009866621112450957, 0.0009866621112450957, 0.0009866621112450957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009866621112450957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098666
Iteration 2/1000 | Loss: 0.00001569
Iteration 3/1000 | Loss: 0.00001004
Iteration 4/1000 | Loss: 0.00000892
Iteration 5/1000 | Loss: 0.00000834
Iteration 6/1000 | Loss: 0.00000793
Iteration 7/1000 | Loss: 0.00000778
Iteration 8/1000 | Loss: 0.00000769
Iteration 9/1000 | Loss: 0.00000768
Iteration 10/1000 | Loss: 0.00000752
Iteration 11/1000 | Loss: 0.00000751
Iteration 12/1000 | Loss: 0.00000748
Iteration 13/1000 | Loss: 0.00000747
Iteration 14/1000 | Loss: 0.00000746
Iteration 15/1000 | Loss: 0.00000746
Iteration 16/1000 | Loss: 0.00000744
Iteration 17/1000 | Loss: 0.00000743
Iteration 18/1000 | Loss: 0.00000742
Iteration 19/1000 | Loss: 0.00000742
Iteration 20/1000 | Loss: 0.00000741
Iteration 21/1000 | Loss: 0.00000741
Iteration 22/1000 | Loss: 0.00000741
Iteration 23/1000 | Loss: 0.00000740
Iteration 24/1000 | Loss: 0.00000740
Iteration 25/1000 | Loss: 0.00000740
Iteration 26/1000 | Loss: 0.00000739
Iteration 27/1000 | Loss: 0.00000739
Iteration 28/1000 | Loss: 0.00000739
Iteration 29/1000 | Loss: 0.00000738
Iteration 30/1000 | Loss: 0.00000738
Iteration 31/1000 | Loss: 0.00000737
Iteration 32/1000 | Loss: 0.00000737
Iteration 33/1000 | Loss: 0.00000737
Iteration 34/1000 | Loss: 0.00000736
Iteration 35/1000 | Loss: 0.00000735
Iteration 36/1000 | Loss: 0.00000735
Iteration 37/1000 | Loss: 0.00000735
Iteration 38/1000 | Loss: 0.00000734
Iteration 39/1000 | Loss: 0.00000734
Iteration 40/1000 | Loss: 0.00000734
Iteration 41/1000 | Loss: 0.00000734
Iteration 42/1000 | Loss: 0.00000734
Iteration 43/1000 | Loss: 0.00000733
Iteration 44/1000 | Loss: 0.00000733
Iteration 45/1000 | Loss: 0.00000733
Iteration 46/1000 | Loss: 0.00000731
Iteration 47/1000 | Loss: 0.00000730
Iteration 48/1000 | Loss: 0.00000730
Iteration 49/1000 | Loss: 0.00000730
Iteration 50/1000 | Loss: 0.00000730
Iteration 51/1000 | Loss: 0.00000730
Iteration 52/1000 | Loss: 0.00000730
Iteration 53/1000 | Loss: 0.00000730
Iteration 54/1000 | Loss: 0.00000730
Iteration 55/1000 | Loss: 0.00000730
Iteration 56/1000 | Loss: 0.00000730
Iteration 57/1000 | Loss: 0.00000730
Iteration 58/1000 | Loss: 0.00000730
Iteration 59/1000 | Loss: 0.00000729
Iteration 60/1000 | Loss: 0.00000729
Iteration 61/1000 | Loss: 0.00000729
Iteration 62/1000 | Loss: 0.00000729
Iteration 63/1000 | Loss: 0.00000728
Iteration 64/1000 | Loss: 0.00000728
Iteration 65/1000 | Loss: 0.00000728
Iteration 66/1000 | Loss: 0.00000728
Iteration 67/1000 | Loss: 0.00000727
Iteration 68/1000 | Loss: 0.00000727
Iteration 69/1000 | Loss: 0.00000727
Iteration 70/1000 | Loss: 0.00000726
Iteration 71/1000 | Loss: 0.00000726
Iteration 72/1000 | Loss: 0.00000726
Iteration 73/1000 | Loss: 0.00000725
Iteration 74/1000 | Loss: 0.00000725
Iteration 75/1000 | Loss: 0.00000725
Iteration 76/1000 | Loss: 0.00000724
Iteration 77/1000 | Loss: 0.00000724
Iteration 78/1000 | Loss: 0.00000724
Iteration 79/1000 | Loss: 0.00000724
Iteration 80/1000 | Loss: 0.00000724
Iteration 81/1000 | Loss: 0.00000723
Iteration 82/1000 | Loss: 0.00000723
Iteration 83/1000 | Loss: 0.00000723
Iteration 84/1000 | Loss: 0.00000723
Iteration 85/1000 | Loss: 0.00000722
Iteration 86/1000 | Loss: 0.00000722
Iteration 87/1000 | Loss: 0.00000722
Iteration 88/1000 | Loss: 0.00000722
Iteration 89/1000 | Loss: 0.00000722
Iteration 90/1000 | Loss: 0.00000722
Iteration 91/1000 | Loss: 0.00000722
Iteration 92/1000 | Loss: 0.00000721
Iteration 93/1000 | Loss: 0.00000721
Iteration 94/1000 | Loss: 0.00000721
Iteration 95/1000 | Loss: 0.00000721
Iteration 96/1000 | Loss: 0.00000721
Iteration 97/1000 | Loss: 0.00000721
Iteration 98/1000 | Loss: 0.00000721
Iteration 99/1000 | Loss: 0.00000720
Iteration 100/1000 | Loss: 0.00000720
Iteration 101/1000 | Loss: 0.00000720
Iteration 102/1000 | Loss: 0.00000720
Iteration 103/1000 | Loss: 0.00000720
Iteration 104/1000 | Loss: 0.00000719
Iteration 105/1000 | Loss: 0.00000719
Iteration 106/1000 | Loss: 0.00000719
Iteration 107/1000 | Loss: 0.00000719
Iteration 108/1000 | Loss: 0.00000719
Iteration 109/1000 | Loss: 0.00000719
Iteration 110/1000 | Loss: 0.00000719
Iteration 111/1000 | Loss: 0.00000719
Iteration 112/1000 | Loss: 0.00000719
Iteration 113/1000 | Loss: 0.00000719
Iteration 114/1000 | Loss: 0.00000719
Iteration 115/1000 | Loss: 0.00000718
Iteration 116/1000 | Loss: 0.00000718
Iteration 117/1000 | Loss: 0.00000718
Iteration 118/1000 | Loss: 0.00000718
Iteration 119/1000 | Loss: 0.00000718
Iteration 120/1000 | Loss: 0.00000718
Iteration 121/1000 | Loss: 0.00000717
Iteration 122/1000 | Loss: 0.00000717
Iteration 123/1000 | Loss: 0.00000717
Iteration 124/1000 | Loss: 0.00000717
Iteration 125/1000 | Loss: 0.00000717
Iteration 126/1000 | Loss: 0.00000717
Iteration 127/1000 | Loss: 0.00000717
Iteration 128/1000 | Loss: 0.00000717
Iteration 129/1000 | Loss: 0.00000716
Iteration 130/1000 | Loss: 0.00000716
Iteration 131/1000 | Loss: 0.00000716
Iteration 132/1000 | Loss: 0.00000716
Iteration 133/1000 | Loss: 0.00000716
Iteration 134/1000 | Loss: 0.00000716
Iteration 135/1000 | Loss: 0.00000716
Iteration 136/1000 | Loss: 0.00000716
Iteration 137/1000 | Loss: 0.00000716
Iteration 138/1000 | Loss: 0.00000715
Iteration 139/1000 | Loss: 0.00000715
Iteration 140/1000 | Loss: 0.00000715
Iteration 141/1000 | Loss: 0.00000715
Iteration 142/1000 | Loss: 0.00000714
Iteration 143/1000 | Loss: 0.00000714
Iteration 144/1000 | Loss: 0.00000714
Iteration 145/1000 | Loss: 0.00000714
Iteration 146/1000 | Loss: 0.00000714
Iteration 147/1000 | Loss: 0.00000714
Iteration 148/1000 | Loss: 0.00000714
Iteration 149/1000 | Loss: 0.00000714
Iteration 150/1000 | Loss: 0.00000714
Iteration 151/1000 | Loss: 0.00000714
Iteration 152/1000 | Loss: 0.00000713
Iteration 153/1000 | Loss: 0.00000713
Iteration 154/1000 | Loss: 0.00000713
Iteration 155/1000 | Loss: 0.00000713
Iteration 156/1000 | Loss: 0.00000713
Iteration 157/1000 | Loss: 0.00000713
Iteration 158/1000 | Loss: 0.00000713
Iteration 159/1000 | Loss: 0.00000713
Iteration 160/1000 | Loss: 0.00000713
Iteration 161/1000 | Loss: 0.00000713
Iteration 162/1000 | Loss: 0.00000713
Iteration 163/1000 | Loss: 0.00000713
Iteration 164/1000 | Loss: 0.00000713
Iteration 165/1000 | Loss: 0.00000713
Iteration 166/1000 | Loss: 0.00000712
Iteration 167/1000 | Loss: 0.00000712
Iteration 168/1000 | Loss: 0.00000712
Iteration 169/1000 | Loss: 0.00000712
Iteration 170/1000 | Loss: 0.00000712
Iteration 171/1000 | Loss: 0.00000712
Iteration 172/1000 | Loss: 0.00000711
Iteration 173/1000 | Loss: 0.00000711
Iteration 174/1000 | Loss: 0.00000711
Iteration 175/1000 | Loss: 0.00000711
Iteration 176/1000 | Loss: 0.00000711
Iteration 177/1000 | Loss: 0.00000711
Iteration 178/1000 | Loss: 0.00000711
Iteration 179/1000 | Loss: 0.00000711
Iteration 180/1000 | Loss: 0.00000711
Iteration 181/1000 | Loss: 0.00000710
Iteration 182/1000 | Loss: 0.00000710
Iteration 183/1000 | Loss: 0.00000710
Iteration 184/1000 | Loss: 0.00000709
Iteration 185/1000 | Loss: 0.00000709
Iteration 186/1000 | Loss: 0.00000709
Iteration 187/1000 | Loss: 0.00000708
Iteration 188/1000 | Loss: 0.00000708
Iteration 189/1000 | Loss: 0.00000708
Iteration 190/1000 | Loss: 0.00000707
Iteration 191/1000 | Loss: 0.00000707
Iteration 192/1000 | Loss: 0.00000707
Iteration 193/1000 | Loss: 0.00000706
Iteration 194/1000 | Loss: 0.00000706
Iteration 195/1000 | Loss: 0.00000706
Iteration 196/1000 | Loss: 0.00000706
Iteration 197/1000 | Loss: 0.00000706
Iteration 198/1000 | Loss: 0.00000705
Iteration 199/1000 | Loss: 0.00000705
Iteration 200/1000 | Loss: 0.00000705
Iteration 201/1000 | Loss: 0.00000705
Iteration 202/1000 | Loss: 0.00000704
Iteration 203/1000 | Loss: 0.00000704
Iteration 204/1000 | Loss: 0.00000704
Iteration 205/1000 | Loss: 0.00000704
Iteration 206/1000 | Loss: 0.00000704
Iteration 207/1000 | Loss: 0.00000704
Iteration 208/1000 | Loss: 0.00000704
Iteration 209/1000 | Loss: 0.00000704
Iteration 210/1000 | Loss: 0.00000703
Iteration 211/1000 | Loss: 0.00000703
Iteration 212/1000 | Loss: 0.00000703
Iteration 213/1000 | Loss: 0.00000703
Iteration 214/1000 | Loss: 0.00000703
Iteration 215/1000 | Loss: 0.00000703
Iteration 216/1000 | Loss: 0.00000703
Iteration 217/1000 | Loss: 0.00000703
Iteration 218/1000 | Loss: 0.00000703
Iteration 219/1000 | Loss: 0.00000703
Iteration 220/1000 | Loss: 0.00000703
Iteration 221/1000 | Loss: 0.00000703
Iteration 222/1000 | Loss: 0.00000703
Iteration 223/1000 | Loss: 0.00000703
Iteration 224/1000 | Loss: 0.00000702
Iteration 225/1000 | Loss: 0.00000702
Iteration 226/1000 | Loss: 0.00000702
Iteration 227/1000 | Loss: 0.00000702
Iteration 228/1000 | Loss: 0.00000702
Iteration 229/1000 | Loss: 0.00000702
Iteration 230/1000 | Loss: 0.00000702
Iteration 231/1000 | Loss: 0.00000702
Iteration 232/1000 | Loss: 0.00000702
Iteration 233/1000 | Loss: 0.00000702
Iteration 234/1000 | Loss: 0.00000702
Iteration 235/1000 | Loss: 0.00000702
Iteration 236/1000 | Loss: 0.00000702
Iteration 237/1000 | Loss: 0.00000702
Iteration 238/1000 | Loss: 0.00000702
Iteration 239/1000 | Loss: 0.00000702
Iteration 240/1000 | Loss: 0.00000702
Iteration 241/1000 | Loss: 0.00000702
Iteration 242/1000 | Loss: 0.00000702
Iteration 243/1000 | Loss: 0.00000702
Iteration 244/1000 | Loss: 0.00000702
Iteration 245/1000 | Loss: 0.00000702
Iteration 246/1000 | Loss: 0.00000702
Iteration 247/1000 | Loss: 0.00000702
Iteration 248/1000 | Loss: 0.00000702
Iteration 249/1000 | Loss: 0.00000702
Iteration 250/1000 | Loss: 0.00000702
Iteration 251/1000 | Loss: 0.00000702
Iteration 252/1000 | Loss: 0.00000702
Iteration 253/1000 | Loss: 0.00000702
Iteration 254/1000 | Loss: 0.00000702
Iteration 255/1000 | Loss: 0.00000702
Iteration 256/1000 | Loss: 0.00000702
Iteration 257/1000 | Loss: 0.00000702
Iteration 258/1000 | Loss: 0.00000702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [7.015359187789727e-06, 7.015359187789727e-06, 7.015359187789727e-06, 7.015359187789727e-06, 7.015359187789727e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.015359187789727e-06

Optimization complete. Final v2v error: 2.308276653289795 mm

Highest mean error: 2.599442958831787 mm for frame 134

Lowest mean error: 2.0825746059417725 mm for frame 19

Saving results

Total time: 37.485095262527466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056726
Iteration 2/25 | Loss: 0.00266032
Iteration 3/25 | Loss: 0.00164065
Iteration 4/25 | Loss: 0.00152927
Iteration 5/25 | Loss: 0.00149676
Iteration 6/25 | Loss: 0.00146933
Iteration 7/25 | Loss: 0.00142820
Iteration 8/25 | Loss: 0.00138326
Iteration 9/25 | Loss: 0.00135503
Iteration 10/25 | Loss: 0.00134071
Iteration 11/25 | Loss: 0.00132842
Iteration 12/25 | Loss: 0.00130726
Iteration 13/25 | Loss: 0.00131216
Iteration 14/25 | Loss: 0.00131320
Iteration 15/25 | Loss: 0.00129312
Iteration 16/25 | Loss: 0.00127841
Iteration 17/25 | Loss: 0.00126977
Iteration 18/25 | Loss: 0.00126518
Iteration 19/25 | Loss: 0.00126389
Iteration 20/25 | Loss: 0.00126626
Iteration 21/25 | Loss: 0.00126253
Iteration 22/25 | Loss: 0.00126360
Iteration 23/25 | Loss: 0.00126070
Iteration 24/25 | Loss: 0.00126295
Iteration 25/25 | Loss: 0.00125999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26220560
Iteration 2/25 | Loss: 0.00343687
Iteration 3/25 | Loss: 0.00179779
Iteration 4/25 | Loss: 0.00179779
Iteration 5/25 | Loss: 0.00179779
Iteration 6/25 | Loss: 0.00179779
Iteration 7/25 | Loss: 0.00179779
Iteration 8/25 | Loss: 0.00179779
Iteration 9/25 | Loss: 0.00179779
Iteration 10/25 | Loss: 0.00179779
Iteration 11/25 | Loss: 0.00179779
Iteration 12/25 | Loss: 0.00179779
Iteration 13/25 | Loss: 0.00179779
Iteration 14/25 | Loss: 0.00179779
Iteration 15/25 | Loss: 0.00179779
Iteration 16/25 | Loss: 0.00179779
Iteration 17/25 | Loss: 0.00179779
Iteration 18/25 | Loss: 0.00179779
Iteration 19/25 | Loss: 0.00179779
Iteration 20/25 | Loss: 0.00179779
Iteration 21/25 | Loss: 0.00179779
Iteration 22/25 | Loss: 0.00179779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017977863317355514, 0.0017977863317355514, 0.0017977863317355514, 0.0017977863317355514, 0.0017977863317355514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017977863317355514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179779
Iteration 2/1000 | Loss: 0.00187469
Iteration 3/1000 | Loss: 0.00273078
Iteration 4/1000 | Loss: 0.00162224
Iteration 5/1000 | Loss: 0.00066183
Iteration 6/1000 | Loss: 0.00184470
Iteration 7/1000 | Loss: 0.00014361
Iteration 8/1000 | Loss: 0.00039169
Iteration 9/1000 | Loss: 0.00018484
Iteration 10/1000 | Loss: 0.00027368
Iteration 11/1000 | Loss: 0.00018587
Iteration 12/1000 | Loss: 0.00018711
Iteration 13/1000 | Loss: 0.00098625
Iteration 14/1000 | Loss: 0.00038395
Iteration 15/1000 | Loss: 0.00056647
Iteration 16/1000 | Loss: 0.00043209
Iteration 17/1000 | Loss: 0.00048425
Iteration 18/1000 | Loss: 0.00161242
Iteration 19/1000 | Loss: 0.00373334
Iteration 20/1000 | Loss: 0.00076988
Iteration 21/1000 | Loss: 0.00061583
Iteration 22/1000 | Loss: 0.00287168
Iteration 23/1000 | Loss: 0.00239938
Iteration 24/1000 | Loss: 0.00074184
Iteration 25/1000 | Loss: 0.00132083
Iteration 26/1000 | Loss: 0.00211003
Iteration 27/1000 | Loss: 0.00152669
Iteration 28/1000 | Loss: 0.00099699
Iteration 29/1000 | Loss: 0.00225946
Iteration 30/1000 | Loss: 0.00118007
Iteration 31/1000 | Loss: 0.00145125
Iteration 32/1000 | Loss: 0.00256759
Iteration 33/1000 | Loss: 0.00118356
Iteration 34/1000 | Loss: 0.00126799
Iteration 35/1000 | Loss: 0.00158363
Iteration 36/1000 | Loss: 0.00126185
Iteration 37/1000 | Loss: 0.00073586
Iteration 38/1000 | Loss: 0.00053313
Iteration 39/1000 | Loss: 0.00045123
Iteration 40/1000 | Loss: 0.00110213
Iteration 41/1000 | Loss: 0.00102256
Iteration 42/1000 | Loss: 0.00072933
Iteration 43/1000 | Loss: 0.00119225
Iteration 44/1000 | Loss: 0.00118925
Iteration 45/1000 | Loss: 0.00071839
Iteration 46/1000 | Loss: 0.00213550
Iteration 47/1000 | Loss: 0.00258944
Iteration 48/1000 | Loss: 0.00202814
Iteration 49/1000 | Loss: 0.00389006
Iteration 50/1000 | Loss: 0.00284591
Iteration 51/1000 | Loss: 0.00161389
Iteration 52/1000 | Loss: 0.00089565
Iteration 53/1000 | Loss: 0.00160425
Iteration 54/1000 | Loss: 0.00174950
Iteration 55/1000 | Loss: 0.00060668
Iteration 56/1000 | Loss: 0.00157000
Iteration 57/1000 | Loss: 0.00431916
Iteration 58/1000 | Loss: 0.00138474
Iteration 59/1000 | Loss: 0.00053338
Iteration 60/1000 | Loss: 0.00052015
Iteration 61/1000 | Loss: 0.00016098
Iteration 62/1000 | Loss: 0.00008025
Iteration 63/1000 | Loss: 0.00184918
Iteration 64/1000 | Loss: 0.00007406
Iteration 65/1000 | Loss: 0.00006732
Iteration 66/1000 | Loss: 0.00057050
Iteration 67/1000 | Loss: 0.00138274
Iteration 68/1000 | Loss: 0.00039638
Iteration 69/1000 | Loss: 0.00051116
Iteration 70/1000 | Loss: 0.00008729
Iteration 71/1000 | Loss: 0.00007051
Iteration 72/1000 | Loss: 0.00006071
Iteration 73/1000 | Loss: 0.00217366
Iteration 74/1000 | Loss: 0.00088041
Iteration 75/1000 | Loss: 0.00022817
Iteration 76/1000 | Loss: 0.00021455
Iteration 77/1000 | Loss: 0.00018720
Iteration 78/1000 | Loss: 0.00019450
Iteration 79/1000 | Loss: 0.00013968
Iteration 80/1000 | Loss: 0.00025520
Iteration 81/1000 | Loss: 0.00006290
Iteration 82/1000 | Loss: 0.00004315
Iteration 83/1000 | Loss: 0.00004032
Iteration 84/1000 | Loss: 0.00004531
Iteration 85/1000 | Loss: 0.00039894
Iteration 86/1000 | Loss: 0.00024130
Iteration 87/1000 | Loss: 0.00018548
Iteration 88/1000 | Loss: 0.00003517
Iteration 89/1000 | Loss: 0.00123163
Iteration 90/1000 | Loss: 0.00150739
Iteration 91/1000 | Loss: 0.00106520
Iteration 92/1000 | Loss: 0.00065198
Iteration 93/1000 | Loss: 0.00008985
Iteration 94/1000 | Loss: 0.00003783
Iteration 95/1000 | Loss: 0.00049836
Iteration 96/1000 | Loss: 0.00019845
Iteration 97/1000 | Loss: 0.00011024
Iteration 98/1000 | Loss: 0.00009248
Iteration 99/1000 | Loss: 0.00018282
Iteration 100/1000 | Loss: 0.00013888
Iteration 101/1000 | Loss: 0.00006854
Iteration 102/1000 | Loss: 0.00005820
Iteration 103/1000 | Loss: 0.00002969
Iteration 104/1000 | Loss: 0.00039822
Iteration 105/1000 | Loss: 0.00024097
Iteration 106/1000 | Loss: 0.00028683
Iteration 107/1000 | Loss: 0.00002513
Iteration 108/1000 | Loss: 0.00002367
Iteration 109/1000 | Loss: 0.00002208
Iteration 110/1000 | Loss: 0.00002083
Iteration 111/1000 | Loss: 0.00004028
Iteration 112/1000 | Loss: 0.00003474
Iteration 113/1000 | Loss: 0.00013038
Iteration 114/1000 | Loss: 0.00054719
Iteration 115/1000 | Loss: 0.00008616
Iteration 116/1000 | Loss: 0.00003980
Iteration 117/1000 | Loss: 0.00013821
Iteration 118/1000 | Loss: 0.00002427
Iteration 119/1000 | Loss: 0.00009438
Iteration 120/1000 | Loss: 0.00018032
Iteration 121/1000 | Loss: 0.00016882
Iteration 122/1000 | Loss: 0.00017162
Iteration 123/1000 | Loss: 0.00034042
Iteration 124/1000 | Loss: 0.00021696
Iteration 125/1000 | Loss: 0.00014629
Iteration 126/1000 | Loss: 0.00004092
Iteration 127/1000 | Loss: 0.00001994
Iteration 128/1000 | Loss: 0.00001733
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00004233
Iteration 131/1000 | Loss: 0.00002595
Iteration 132/1000 | Loss: 0.00002871
Iteration 133/1000 | Loss: 0.00002018
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001626
Iteration 136/1000 | Loss: 0.00003682
Iteration 137/1000 | Loss: 0.00001805
Iteration 138/1000 | Loss: 0.00001617
Iteration 139/1000 | Loss: 0.00001574
Iteration 140/1000 | Loss: 0.00002086
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00003092
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001933
Iteration 146/1000 | Loss: 0.00003234
Iteration 147/1000 | Loss: 0.00002186
Iteration 148/1000 | Loss: 0.00001807
Iteration 149/1000 | Loss: 0.00003253
Iteration 150/1000 | Loss: 0.00002137
Iteration 151/1000 | Loss: 0.00002401
Iteration 152/1000 | Loss: 0.00003643
Iteration 153/1000 | Loss: 0.00001806
Iteration 154/1000 | Loss: 0.00003055
Iteration 155/1000 | Loss: 0.00001753
Iteration 156/1000 | Loss: 0.00003103
Iteration 157/1000 | Loss: 0.00003979
Iteration 158/1000 | Loss: 0.00003117
Iteration 159/1000 | Loss: 0.00002376
Iteration 160/1000 | Loss: 0.00003030
Iteration 161/1000 | Loss: 0.00001676
Iteration 162/1000 | Loss: 0.00001584
Iteration 163/1000 | Loss: 0.00001649
Iteration 164/1000 | Loss: 0.00001526
Iteration 165/1000 | Loss: 0.00001517
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001500
Iteration 168/1000 | Loss: 0.00001493
Iteration 169/1000 | Loss: 0.00001491
Iteration 170/1000 | Loss: 0.00001490
Iteration 171/1000 | Loss: 0.00001488
Iteration 172/1000 | Loss: 0.00001486
Iteration 173/1000 | Loss: 0.00002399
Iteration 174/1000 | Loss: 0.00001486
Iteration 175/1000 | Loss: 0.00001482
Iteration 176/1000 | Loss: 0.00001478
Iteration 177/1000 | Loss: 0.00002311
Iteration 178/1000 | Loss: 0.00001464
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001460
Iteration 184/1000 | Loss: 0.00001460
Iteration 185/1000 | Loss: 0.00001460
Iteration 186/1000 | Loss: 0.00001460
Iteration 187/1000 | Loss: 0.00001459
Iteration 188/1000 | Loss: 0.00001459
Iteration 189/1000 | Loss: 0.00001458
Iteration 190/1000 | Loss: 0.00001458
Iteration 191/1000 | Loss: 0.00003167
Iteration 192/1000 | Loss: 0.00001709
Iteration 193/1000 | Loss: 0.00001467
Iteration 194/1000 | Loss: 0.00001437
Iteration 195/1000 | Loss: 0.00001435
Iteration 196/1000 | Loss: 0.00001435
Iteration 197/1000 | Loss: 0.00001435
Iteration 198/1000 | Loss: 0.00001435
Iteration 199/1000 | Loss: 0.00001435
Iteration 200/1000 | Loss: 0.00001435
Iteration 201/1000 | Loss: 0.00001435
Iteration 202/1000 | Loss: 0.00001435
Iteration 203/1000 | Loss: 0.00001435
Iteration 204/1000 | Loss: 0.00001435
Iteration 205/1000 | Loss: 0.00001435
Iteration 206/1000 | Loss: 0.00001435
Iteration 207/1000 | Loss: 0.00001435
Iteration 208/1000 | Loss: 0.00001434
Iteration 209/1000 | Loss: 0.00001434
Iteration 210/1000 | Loss: 0.00001434
Iteration 211/1000 | Loss: 0.00001433
Iteration 212/1000 | Loss: 0.00001433
Iteration 213/1000 | Loss: 0.00001432
Iteration 214/1000 | Loss: 0.00001432
Iteration 215/1000 | Loss: 0.00001432
Iteration 216/1000 | Loss: 0.00001431
Iteration 217/1000 | Loss: 0.00001431
Iteration 218/1000 | Loss: 0.00001431
Iteration 219/1000 | Loss: 0.00001431
Iteration 220/1000 | Loss: 0.00001431
Iteration 221/1000 | Loss: 0.00001430
Iteration 222/1000 | Loss: 0.00001430
Iteration 223/1000 | Loss: 0.00001428
Iteration 224/1000 | Loss: 0.00001427
Iteration 225/1000 | Loss: 0.00001427
Iteration 226/1000 | Loss: 0.00001427
Iteration 227/1000 | Loss: 0.00004022
Iteration 228/1000 | Loss: 0.00004022
Iteration 229/1000 | Loss: 0.00001438
Iteration 230/1000 | Loss: 0.00001423
Iteration 231/1000 | Loss: 0.00001423
Iteration 232/1000 | Loss: 0.00001423
Iteration 233/1000 | Loss: 0.00001422
Iteration 234/1000 | Loss: 0.00001422
Iteration 235/1000 | Loss: 0.00001422
Iteration 236/1000 | Loss: 0.00001422
Iteration 237/1000 | Loss: 0.00001422
Iteration 238/1000 | Loss: 0.00001422
Iteration 239/1000 | Loss: 0.00001422
Iteration 240/1000 | Loss: 0.00001422
Iteration 241/1000 | Loss: 0.00001422
Iteration 242/1000 | Loss: 0.00001422
Iteration 243/1000 | Loss: 0.00001422
Iteration 244/1000 | Loss: 0.00001422
Iteration 245/1000 | Loss: 0.00001421
Iteration 246/1000 | Loss: 0.00001421
Iteration 247/1000 | Loss: 0.00001421
Iteration 248/1000 | Loss: 0.00001421
Iteration 249/1000 | Loss: 0.00001421
Iteration 250/1000 | Loss: 0.00001421
Iteration 251/1000 | Loss: 0.00001421
Iteration 252/1000 | Loss: 0.00001421
Iteration 253/1000 | Loss: 0.00001420
Iteration 254/1000 | Loss: 0.00001420
Iteration 255/1000 | Loss: 0.00001420
Iteration 256/1000 | Loss: 0.00001419
Iteration 257/1000 | Loss: 0.00001419
Iteration 258/1000 | Loss: 0.00001419
Iteration 259/1000 | Loss: 0.00001419
Iteration 260/1000 | Loss: 0.00001419
Iteration 261/1000 | Loss: 0.00001419
Iteration 262/1000 | Loss: 0.00001419
Iteration 263/1000 | Loss: 0.00001419
Iteration 264/1000 | Loss: 0.00001419
Iteration 265/1000 | Loss: 0.00001419
Iteration 266/1000 | Loss: 0.00001418
Iteration 267/1000 | Loss: 0.00001418
Iteration 268/1000 | Loss: 0.00001418
Iteration 269/1000 | Loss: 0.00001418
Iteration 270/1000 | Loss: 0.00001418
Iteration 271/1000 | Loss: 0.00001418
Iteration 272/1000 | Loss: 0.00001418
Iteration 273/1000 | Loss: 0.00001418
Iteration 274/1000 | Loss: 0.00001418
Iteration 275/1000 | Loss: 0.00001418
Iteration 276/1000 | Loss: 0.00001418
Iteration 277/1000 | Loss: 0.00001418
Iteration 278/1000 | Loss: 0.00001418
Iteration 279/1000 | Loss: 0.00001418
Iteration 280/1000 | Loss: 0.00001418
Iteration 281/1000 | Loss: 0.00001418
Iteration 282/1000 | Loss: 0.00001418
Iteration 283/1000 | Loss: 0.00001418
Iteration 284/1000 | Loss: 0.00001418
Iteration 285/1000 | Loss: 0.00001418
Iteration 286/1000 | Loss: 0.00001418
Iteration 287/1000 | Loss: 0.00001418
Iteration 288/1000 | Loss: 0.00001418
Iteration 289/1000 | Loss: 0.00001418
Iteration 290/1000 | Loss: 0.00001418
Iteration 291/1000 | Loss: 0.00001418
Iteration 292/1000 | Loss: 0.00001418
Iteration 293/1000 | Loss: 0.00001418
Iteration 294/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [1.4178290257405024e-05, 1.4178290257405024e-05, 1.4178290257405024e-05, 1.4178290257405024e-05, 1.4178290257405024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4178290257405024e-05

Optimization complete. Final v2v error: 3.178229808807373 mm

Highest mean error: 5.586214065551758 mm for frame 167

Lowest mean error: 2.6938185691833496 mm for frame 58

Saving results

Total time: 338.4185221195221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760256
Iteration 2/25 | Loss: 0.00117036
Iteration 3/25 | Loss: 0.00109219
Iteration 4/25 | Loss: 0.00107948
Iteration 5/25 | Loss: 0.00107566
Iteration 6/25 | Loss: 0.00107486
Iteration 7/25 | Loss: 0.00107486
Iteration 8/25 | Loss: 0.00107486
Iteration 9/25 | Loss: 0.00107486
Iteration 10/25 | Loss: 0.00107486
Iteration 11/25 | Loss: 0.00107486
Iteration 12/25 | Loss: 0.00107486
Iteration 13/25 | Loss: 0.00107486
Iteration 14/25 | Loss: 0.00107486
Iteration 15/25 | Loss: 0.00107486
Iteration 16/25 | Loss: 0.00107486
Iteration 17/25 | Loss: 0.00107486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010748596396297216, 0.0010748596396297216, 0.0010748596396297216, 0.0010748596396297216, 0.0010748596396297216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010748596396297216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31596398
Iteration 2/25 | Loss: 0.00124072
Iteration 3/25 | Loss: 0.00124071
Iteration 4/25 | Loss: 0.00124071
Iteration 5/25 | Loss: 0.00124071
Iteration 6/25 | Loss: 0.00124071
Iteration 7/25 | Loss: 0.00124071
Iteration 8/25 | Loss: 0.00124071
Iteration 9/25 | Loss: 0.00124071
Iteration 10/25 | Loss: 0.00124071
Iteration 11/25 | Loss: 0.00124071
Iteration 12/25 | Loss: 0.00124071
Iteration 13/25 | Loss: 0.00124071
Iteration 14/25 | Loss: 0.00124071
Iteration 15/25 | Loss: 0.00124071
Iteration 16/25 | Loss: 0.00124071
Iteration 17/25 | Loss: 0.00124071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012407077010720968, 0.0012407077010720968, 0.0012407077010720968, 0.0012407077010720968, 0.0012407077010720968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012407077010720968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124071
Iteration 2/1000 | Loss: 0.00003366
Iteration 3/1000 | Loss: 0.00002135
Iteration 4/1000 | Loss: 0.00001589
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001442
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001305
Iteration 11/1000 | Loss: 0.00001282
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001245
Iteration 14/1000 | Loss: 0.00001238
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001230
Iteration 18/1000 | Loss: 0.00001230
Iteration 19/1000 | Loss: 0.00001229
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001224
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001223
Iteration 28/1000 | Loss: 0.00001223
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001220
Iteration 35/1000 | Loss: 0.00001220
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001218
Iteration 41/1000 | Loss: 0.00001218
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001218
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001217
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001216
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001213
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001213
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001212
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001209
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001205
Iteration 117/1000 | Loss: 0.00001205
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001204
Iteration 126/1000 | Loss: 0.00001204
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001203
Iteration 134/1000 | Loss: 0.00001203
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001203
Iteration 142/1000 | Loss: 0.00001203
Iteration 143/1000 | Loss: 0.00001203
Iteration 144/1000 | Loss: 0.00001203
Iteration 145/1000 | Loss: 0.00001203
Iteration 146/1000 | Loss: 0.00001203
Iteration 147/1000 | Loss: 0.00001203
Iteration 148/1000 | Loss: 0.00001203
Iteration 149/1000 | Loss: 0.00001203
Iteration 150/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.2027918273815885e-05, 1.2027918273815885e-05, 1.2027918273815885e-05, 1.2027918273815885e-05, 1.2027918273815885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2027918273815885e-05

Optimization complete. Final v2v error: 2.974045515060425 mm

Highest mean error: 3.2528443336486816 mm for frame 101

Lowest mean error: 2.76192307472229 mm for frame 3

Saving results

Total time: 35.81799817085266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398134
Iteration 2/25 | Loss: 0.00122447
Iteration 3/25 | Loss: 0.00109594
Iteration 4/25 | Loss: 0.00108082
Iteration 5/25 | Loss: 0.00107839
Iteration 6/25 | Loss: 0.00107797
Iteration 7/25 | Loss: 0.00107797
Iteration 8/25 | Loss: 0.00107797
Iteration 9/25 | Loss: 0.00107797
Iteration 10/25 | Loss: 0.00107797
Iteration 11/25 | Loss: 0.00107797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010779688600450754, 0.0010779688600450754, 0.0010779688600450754, 0.0010779688600450754, 0.0010779688600450754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010779688600450754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34501517
Iteration 2/25 | Loss: 0.00064960
Iteration 3/25 | Loss: 0.00064960
Iteration 4/25 | Loss: 0.00064960
Iteration 5/25 | Loss: 0.00064960
Iteration 6/25 | Loss: 0.00064959
Iteration 7/25 | Loss: 0.00064959
Iteration 8/25 | Loss: 0.00064959
Iteration 9/25 | Loss: 0.00064959
Iteration 10/25 | Loss: 0.00064959
Iteration 11/25 | Loss: 0.00064959
Iteration 12/25 | Loss: 0.00064959
Iteration 13/25 | Loss: 0.00064959
Iteration 14/25 | Loss: 0.00064959
Iteration 15/25 | Loss: 0.00064959
Iteration 16/25 | Loss: 0.00064959
Iteration 17/25 | Loss: 0.00064959
Iteration 18/25 | Loss: 0.00064959
Iteration 19/25 | Loss: 0.00064959
Iteration 20/25 | Loss: 0.00064959
Iteration 21/25 | Loss: 0.00064959
Iteration 22/25 | Loss: 0.00064959
Iteration 23/25 | Loss: 0.00064959
Iteration 24/25 | Loss: 0.00064959
Iteration 25/25 | Loss: 0.00064959

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064959
Iteration 2/1000 | Loss: 0.00002116
Iteration 3/1000 | Loss: 0.00001553
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001357
Iteration 6/1000 | Loss: 0.00001293
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001235
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001169
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001168
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001151
Iteration 20/1000 | Loss: 0.00001149
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001148
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001144
Iteration 26/1000 | Loss: 0.00001144
Iteration 27/1000 | Loss: 0.00001143
Iteration 28/1000 | Loss: 0.00001142
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001134
Iteration 34/1000 | Loss: 0.00001134
Iteration 35/1000 | Loss: 0.00001133
Iteration 36/1000 | Loss: 0.00001133
Iteration 37/1000 | Loss: 0.00001133
Iteration 38/1000 | Loss: 0.00001132
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001122
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001112
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001110
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001109
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001108
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001107
Iteration 64/1000 | Loss: 0.00001107
Iteration 65/1000 | Loss: 0.00001107
Iteration 66/1000 | Loss: 0.00001107
Iteration 67/1000 | Loss: 0.00001107
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001106
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001106
Iteration 75/1000 | Loss: 0.00001106
Iteration 76/1000 | Loss: 0.00001106
Iteration 77/1000 | Loss: 0.00001105
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001104
Iteration 81/1000 | Loss: 0.00001104
Iteration 82/1000 | Loss: 0.00001104
Iteration 83/1000 | Loss: 0.00001104
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001100
Iteration 101/1000 | Loss: 0.00001100
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001100
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001099
Iteration 108/1000 | Loss: 0.00001099
Iteration 109/1000 | Loss: 0.00001099
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001097
Iteration 114/1000 | Loss: 0.00001097
Iteration 115/1000 | Loss: 0.00001097
Iteration 116/1000 | Loss: 0.00001097
Iteration 117/1000 | Loss: 0.00001096
Iteration 118/1000 | Loss: 0.00001096
Iteration 119/1000 | Loss: 0.00001096
Iteration 120/1000 | Loss: 0.00001096
Iteration 121/1000 | Loss: 0.00001096
Iteration 122/1000 | Loss: 0.00001096
Iteration 123/1000 | Loss: 0.00001096
Iteration 124/1000 | Loss: 0.00001096
Iteration 125/1000 | Loss: 0.00001096
Iteration 126/1000 | Loss: 0.00001096
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001095
Iteration 131/1000 | Loss: 0.00001095
Iteration 132/1000 | Loss: 0.00001095
Iteration 133/1000 | Loss: 0.00001095
Iteration 134/1000 | Loss: 0.00001095
Iteration 135/1000 | Loss: 0.00001095
Iteration 136/1000 | Loss: 0.00001094
Iteration 137/1000 | Loss: 0.00001094
Iteration 138/1000 | Loss: 0.00001094
Iteration 139/1000 | Loss: 0.00001094
Iteration 140/1000 | Loss: 0.00001094
Iteration 141/1000 | Loss: 0.00001094
Iteration 142/1000 | Loss: 0.00001093
Iteration 143/1000 | Loss: 0.00001093
Iteration 144/1000 | Loss: 0.00001093
Iteration 145/1000 | Loss: 0.00001093
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001092
Iteration 148/1000 | Loss: 0.00001092
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001091
Iteration 155/1000 | Loss: 0.00001091
Iteration 156/1000 | Loss: 0.00001091
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Iteration 160/1000 | Loss: 0.00001091
Iteration 161/1000 | Loss: 0.00001090
Iteration 162/1000 | Loss: 0.00001090
Iteration 163/1000 | Loss: 0.00001090
Iteration 164/1000 | Loss: 0.00001090
Iteration 165/1000 | Loss: 0.00001090
Iteration 166/1000 | Loss: 0.00001090
Iteration 167/1000 | Loss: 0.00001090
Iteration 168/1000 | Loss: 0.00001090
Iteration 169/1000 | Loss: 0.00001090
Iteration 170/1000 | Loss: 0.00001090
Iteration 171/1000 | Loss: 0.00001090
Iteration 172/1000 | Loss: 0.00001090
Iteration 173/1000 | Loss: 0.00001090
Iteration 174/1000 | Loss: 0.00001090
Iteration 175/1000 | Loss: 0.00001090
Iteration 176/1000 | Loss: 0.00001090
Iteration 177/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.0899679182330146e-05, 1.0899679182330146e-05, 1.0899679182330146e-05, 1.0899679182330146e-05, 1.0899679182330146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0899679182330146e-05

Optimization complete. Final v2v error: 2.817162036895752 mm

Highest mean error: 3.0969722270965576 mm for frame 118

Lowest mean error: 2.5531558990478516 mm for frame 4

Saving results

Total time: 41.944538831710815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520595
Iteration 2/25 | Loss: 0.00133822
Iteration 3/25 | Loss: 0.00114318
Iteration 4/25 | Loss: 0.00112095
Iteration 5/25 | Loss: 0.00111738
Iteration 6/25 | Loss: 0.00111658
Iteration 7/25 | Loss: 0.00111658
Iteration 8/25 | Loss: 0.00111658
Iteration 9/25 | Loss: 0.00111658
Iteration 10/25 | Loss: 0.00111658
Iteration 11/25 | Loss: 0.00111658
Iteration 12/25 | Loss: 0.00111658
Iteration 13/25 | Loss: 0.00111658
Iteration 14/25 | Loss: 0.00111658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011165832402184606, 0.0011165832402184606, 0.0011165832402184606, 0.0011165832402184606, 0.0011165832402184606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011165832402184606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33397591
Iteration 2/25 | Loss: 0.00045024
Iteration 3/25 | Loss: 0.00045022
Iteration 4/25 | Loss: 0.00045022
Iteration 5/25 | Loss: 0.00045022
Iteration 6/25 | Loss: 0.00045022
Iteration 7/25 | Loss: 0.00045022
Iteration 8/25 | Loss: 0.00045022
Iteration 9/25 | Loss: 0.00045022
Iteration 10/25 | Loss: 0.00045022
Iteration 11/25 | Loss: 0.00045022
Iteration 12/25 | Loss: 0.00045022
Iteration 13/25 | Loss: 0.00045022
Iteration 14/25 | Loss: 0.00045022
Iteration 15/25 | Loss: 0.00045022
Iteration 16/25 | Loss: 0.00045022
Iteration 17/25 | Loss: 0.00045022
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00045021649566479027, 0.00045021649566479027, 0.00045021649566479027, 0.00045021649566479027, 0.00045021649566479027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045021649566479027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045022
Iteration 2/1000 | Loss: 0.00003595
Iteration 3/1000 | Loss: 0.00002709
Iteration 4/1000 | Loss: 0.00002193
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00002000
Iteration 7/1000 | Loss: 0.00001941
Iteration 8/1000 | Loss: 0.00001878
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001790
Iteration 11/1000 | Loss: 0.00001764
Iteration 12/1000 | Loss: 0.00001745
Iteration 13/1000 | Loss: 0.00001719
Iteration 14/1000 | Loss: 0.00001711
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001697
Iteration 20/1000 | Loss: 0.00001697
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001697
Iteration 24/1000 | Loss: 0.00001697
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001697
Iteration 29/1000 | Loss: 0.00001696
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001696
Iteration 34/1000 | Loss: 0.00001694
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001691
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001689
Iteration 45/1000 | Loss: 0.00001689
Iteration 46/1000 | Loss: 0.00001689
Iteration 47/1000 | Loss: 0.00001689
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00001689
Iteration 50/1000 | Loss: 0.00001689
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001688
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001686
Iteration 62/1000 | Loss: 0.00001686
Iteration 63/1000 | Loss: 0.00001686
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001686
Iteration 70/1000 | Loss: 0.00001686
Iteration 71/1000 | Loss: 0.00001686
Iteration 72/1000 | Loss: 0.00001686
Iteration 73/1000 | Loss: 0.00001686
Iteration 74/1000 | Loss: 0.00001686
Iteration 75/1000 | Loss: 0.00001685
Iteration 76/1000 | Loss: 0.00001685
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001682
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001682
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001679
Iteration 89/1000 | Loss: 0.00001679
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001677
Iteration 93/1000 | Loss: 0.00001677
Iteration 94/1000 | Loss: 0.00001677
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001673
Iteration 98/1000 | Loss: 0.00001673
Iteration 99/1000 | Loss: 0.00001673
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001673
Iteration 104/1000 | Loss: 0.00001673
Iteration 105/1000 | Loss: 0.00001672
Iteration 106/1000 | Loss: 0.00001672
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001671
Iteration 114/1000 | Loss: 0.00001671
Iteration 115/1000 | Loss: 0.00001670
Iteration 116/1000 | Loss: 0.00001670
Iteration 117/1000 | Loss: 0.00001670
Iteration 118/1000 | Loss: 0.00001670
Iteration 119/1000 | Loss: 0.00001670
Iteration 120/1000 | Loss: 0.00001670
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001669
Iteration 123/1000 | Loss: 0.00001669
Iteration 124/1000 | Loss: 0.00001669
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001668
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00001668
Iteration 130/1000 | Loss: 0.00001668
Iteration 131/1000 | Loss: 0.00001667
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001666
Iteration 135/1000 | Loss: 0.00001666
Iteration 136/1000 | Loss: 0.00001666
Iteration 137/1000 | Loss: 0.00001665
Iteration 138/1000 | Loss: 0.00001665
Iteration 139/1000 | Loss: 0.00001665
Iteration 140/1000 | Loss: 0.00001665
Iteration 141/1000 | Loss: 0.00001665
Iteration 142/1000 | Loss: 0.00001665
Iteration 143/1000 | Loss: 0.00001665
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001664
Iteration 146/1000 | Loss: 0.00001664
Iteration 147/1000 | Loss: 0.00001664
Iteration 148/1000 | Loss: 0.00001664
Iteration 149/1000 | Loss: 0.00001663
Iteration 150/1000 | Loss: 0.00001663
Iteration 151/1000 | Loss: 0.00001663
Iteration 152/1000 | Loss: 0.00001663
Iteration 153/1000 | Loss: 0.00001663
Iteration 154/1000 | Loss: 0.00001663
Iteration 155/1000 | Loss: 0.00001663
Iteration 156/1000 | Loss: 0.00001663
Iteration 157/1000 | Loss: 0.00001662
Iteration 158/1000 | Loss: 0.00001662
Iteration 159/1000 | Loss: 0.00001662
Iteration 160/1000 | Loss: 0.00001662
Iteration 161/1000 | Loss: 0.00001662
Iteration 162/1000 | Loss: 0.00001662
Iteration 163/1000 | Loss: 0.00001662
Iteration 164/1000 | Loss: 0.00001662
Iteration 165/1000 | Loss: 0.00001662
Iteration 166/1000 | Loss: 0.00001661
Iteration 167/1000 | Loss: 0.00001661
Iteration 168/1000 | Loss: 0.00001661
Iteration 169/1000 | Loss: 0.00001661
Iteration 170/1000 | Loss: 0.00001661
Iteration 171/1000 | Loss: 0.00001661
Iteration 172/1000 | Loss: 0.00001661
Iteration 173/1000 | Loss: 0.00001660
Iteration 174/1000 | Loss: 0.00001660
Iteration 175/1000 | Loss: 0.00001660
Iteration 176/1000 | Loss: 0.00001660
Iteration 177/1000 | Loss: 0.00001660
Iteration 178/1000 | Loss: 0.00001660
Iteration 179/1000 | Loss: 0.00001660
Iteration 180/1000 | Loss: 0.00001659
Iteration 181/1000 | Loss: 0.00001659
Iteration 182/1000 | Loss: 0.00001659
Iteration 183/1000 | Loss: 0.00001659
Iteration 184/1000 | Loss: 0.00001659
Iteration 185/1000 | Loss: 0.00001659
Iteration 186/1000 | Loss: 0.00001658
Iteration 187/1000 | Loss: 0.00001658
Iteration 188/1000 | Loss: 0.00001658
Iteration 189/1000 | Loss: 0.00001658
Iteration 190/1000 | Loss: 0.00001658
Iteration 191/1000 | Loss: 0.00001658
Iteration 192/1000 | Loss: 0.00001658
Iteration 193/1000 | Loss: 0.00001658
Iteration 194/1000 | Loss: 0.00001658
Iteration 195/1000 | Loss: 0.00001658
Iteration 196/1000 | Loss: 0.00001658
Iteration 197/1000 | Loss: 0.00001658
Iteration 198/1000 | Loss: 0.00001658
Iteration 199/1000 | Loss: 0.00001658
Iteration 200/1000 | Loss: 0.00001658
Iteration 201/1000 | Loss: 0.00001658
Iteration 202/1000 | Loss: 0.00001658
Iteration 203/1000 | Loss: 0.00001658
Iteration 204/1000 | Loss: 0.00001658
Iteration 205/1000 | Loss: 0.00001658
Iteration 206/1000 | Loss: 0.00001658
Iteration 207/1000 | Loss: 0.00001658
Iteration 208/1000 | Loss: 0.00001658
Iteration 209/1000 | Loss: 0.00001658
Iteration 210/1000 | Loss: 0.00001658
Iteration 211/1000 | Loss: 0.00001658
Iteration 212/1000 | Loss: 0.00001658
Iteration 213/1000 | Loss: 0.00001658
Iteration 214/1000 | Loss: 0.00001658
Iteration 215/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.6582094758632593e-05, 1.6582094758632593e-05, 1.6582094758632593e-05, 1.6582094758632593e-05, 1.6582094758632593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6582094758632593e-05

Optimization complete. Final v2v error: 3.484963893890381 mm

Highest mean error: 3.7162957191467285 mm for frame 50

Lowest mean error: 3.294323205947876 mm for frame 148

Saving results

Total time: 42.26850771903992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850372
Iteration 2/25 | Loss: 0.00140564
Iteration 3/25 | Loss: 0.00119461
Iteration 4/25 | Loss: 0.00116920
Iteration 5/25 | Loss: 0.00116255
Iteration 6/25 | Loss: 0.00117399
Iteration 7/25 | Loss: 0.00117212
Iteration 8/25 | Loss: 0.00116091
Iteration 9/25 | Loss: 0.00115191
Iteration 10/25 | Loss: 0.00115215
Iteration 11/25 | Loss: 0.00114395
Iteration 12/25 | Loss: 0.00113590
Iteration 13/25 | Loss: 0.00114382
Iteration 14/25 | Loss: 0.00113546
Iteration 15/25 | Loss: 0.00113706
Iteration 16/25 | Loss: 0.00113246
Iteration 17/25 | Loss: 0.00112487
Iteration 18/25 | Loss: 0.00112187
Iteration 19/25 | Loss: 0.00112110
Iteration 20/25 | Loss: 0.00112086
Iteration 21/25 | Loss: 0.00112074
Iteration 22/25 | Loss: 0.00112065
Iteration 23/25 | Loss: 0.00112058
Iteration 24/25 | Loss: 0.00112058
Iteration 25/25 | Loss: 0.00112058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15986419
Iteration 2/25 | Loss: 0.00107050
Iteration 3/25 | Loss: 0.00101894
Iteration 4/25 | Loss: 0.00101894
Iteration 5/25 | Loss: 0.00101894
Iteration 6/25 | Loss: 0.00101894
Iteration 7/25 | Loss: 0.00101894
Iteration 8/25 | Loss: 0.00101894
Iteration 9/25 | Loss: 0.00101894
Iteration 10/25 | Loss: 0.00101894
Iteration 11/25 | Loss: 0.00101894
Iteration 12/25 | Loss: 0.00101894
Iteration 13/25 | Loss: 0.00101894
Iteration 14/25 | Loss: 0.00101894
Iteration 15/25 | Loss: 0.00101894
Iteration 16/25 | Loss: 0.00101894
Iteration 17/25 | Loss: 0.00101894
Iteration 18/25 | Loss: 0.00101894
Iteration 19/25 | Loss: 0.00101894
Iteration 20/25 | Loss: 0.00101894
Iteration 21/25 | Loss: 0.00101894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010189397726207972, 0.0010189397726207972, 0.0010189397726207972, 0.0010189397726207972, 0.0010189397726207972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010189397726207972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101894
Iteration 2/1000 | Loss: 0.00008780
Iteration 3/1000 | Loss: 0.00010843
Iteration 4/1000 | Loss: 0.00011086
Iteration 5/1000 | Loss: 0.00011450
Iteration 6/1000 | Loss: 0.00014379
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00002683
Iteration 10/1000 | Loss: 0.00002004
Iteration 11/1000 | Loss: 0.00003248
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00003860
Iteration 14/1000 | Loss: 0.00002985
Iteration 15/1000 | Loss: 0.00002885
Iteration 16/1000 | Loss: 0.00002999
Iteration 17/1000 | Loss: 0.00003247
Iteration 18/1000 | Loss: 0.00002945
Iteration 19/1000 | Loss: 0.00003225
Iteration 20/1000 | Loss: 0.00002707
Iteration 21/1000 | Loss: 0.00003035
Iteration 22/1000 | Loss: 0.00002557
Iteration 23/1000 | Loss: 0.00003007
Iteration 24/1000 | Loss: 0.00003109
Iteration 25/1000 | Loss: 0.00003097
Iteration 26/1000 | Loss: 0.00002762
Iteration 27/1000 | Loss: 0.00003596
Iteration 28/1000 | Loss: 0.00002051
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001380
Iteration 32/1000 | Loss: 0.00001326
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001259
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001227
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00022669
Iteration 40/1000 | Loss: 0.00014202
Iteration 41/1000 | Loss: 0.00001282
Iteration 42/1000 | Loss: 0.00023530
Iteration 43/1000 | Loss: 0.00010147
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00022923
Iteration 48/1000 | Loss: 0.00007471
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00022477
Iteration 51/1000 | Loss: 0.00006769
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00024258
Iteration 55/1000 | Loss: 0.00017357
Iteration 56/1000 | Loss: 0.00017668
Iteration 57/1000 | Loss: 0.00017790
Iteration 58/1000 | Loss: 0.00008216
Iteration 59/1000 | Loss: 0.00012576
Iteration 60/1000 | Loss: 0.00011801
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00019099
Iteration 68/1000 | Loss: 0.00001669
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001206
Iteration 82/1000 | Loss: 0.00001206
Iteration 83/1000 | Loss: 0.00001206
Iteration 84/1000 | Loss: 0.00001206
Iteration 85/1000 | Loss: 0.00001206
Iteration 86/1000 | Loss: 0.00001206
Iteration 87/1000 | Loss: 0.00001206
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001204
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001202
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001201
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001199
Iteration 107/1000 | Loss: 0.00001199
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001198
Iteration 112/1000 | Loss: 0.00001198
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001196
Iteration 120/1000 | Loss: 0.00001196
Iteration 121/1000 | Loss: 0.00001196
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001195
Iteration 125/1000 | Loss: 0.00001195
Iteration 126/1000 | Loss: 0.00001195
Iteration 127/1000 | Loss: 0.00001195
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001191
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001190
Iteration 159/1000 | Loss: 0.00001190
Iteration 160/1000 | Loss: 0.00001190
Iteration 161/1000 | Loss: 0.00001189
Iteration 162/1000 | Loss: 0.00001189
Iteration 163/1000 | Loss: 0.00001189
Iteration 164/1000 | Loss: 0.00001189
Iteration 165/1000 | Loss: 0.00001188
Iteration 166/1000 | Loss: 0.00001188
Iteration 167/1000 | Loss: 0.00001188
Iteration 168/1000 | Loss: 0.00001188
Iteration 169/1000 | Loss: 0.00001188
Iteration 170/1000 | Loss: 0.00001188
Iteration 171/1000 | Loss: 0.00001188
Iteration 172/1000 | Loss: 0.00001187
Iteration 173/1000 | Loss: 0.00001187
Iteration 174/1000 | Loss: 0.00001187
Iteration 175/1000 | Loss: 0.00001187
Iteration 176/1000 | Loss: 0.00001186
Iteration 177/1000 | Loss: 0.00001186
Iteration 178/1000 | Loss: 0.00001186
Iteration 179/1000 | Loss: 0.00001186
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001185
Iteration 182/1000 | Loss: 0.00001185
Iteration 183/1000 | Loss: 0.00001185
Iteration 184/1000 | Loss: 0.00001184
Iteration 185/1000 | Loss: 0.00001183
Iteration 186/1000 | Loss: 0.00001182
Iteration 187/1000 | Loss: 0.00001182
Iteration 188/1000 | Loss: 0.00001182
Iteration 189/1000 | Loss: 0.00001181
Iteration 190/1000 | Loss: 0.00001181
Iteration 191/1000 | Loss: 0.00001180
Iteration 192/1000 | Loss: 0.00001180
Iteration 193/1000 | Loss: 0.00001179
Iteration 194/1000 | Loss: 0.00001179
Iteration 195/1000 | Loss: 0.00001178
Iteration 196/1000 | Loss: 0.00001178
Iteration 197/1000 | Loss: 0.00001177
Iteration 198/1000 | Loss: 0.00001177
Iteration 199/1000 | Loss: 0.00001177
Iteration 200/1000 | Loss: 0.00001176
Iteration 201/1000 | Loss: 0.00001176
Iteration 202/1000 | Loss: 0.00001176
Iteration 203/1000 | Loss: 0.00001175
Iteration 204/1000 | Loss: 0.00001175
Iteration 205/1000 | Loss: 0.00001175
Iteration 206/1000 | Loss: 0.00001175
Iteration 207/1000 | Loss: 0.00001175
Iteration 208/1000 | Loss: 0.00001175
Iteration 209/1000 | Loss: 0.00001175
Iteration 210/1000 | Loss: 0.00001175
Iteration 211/1000 | Loss: 0.00001175
Iteration 212/1000 | Loss: 0.00001175
Iteration 213/1000 | Loss: 0.00001174
Iteration 214/1000 | Loss: 0.00001174
Iteration 215/1000 | Loss: 0.00001174
Iteration 216/1000 | Loss: 0.00001174
Iteration 217/1000 | Loss: 0.00001173
Iteration 218/1000 | Loss: 0.00001173
Iteration 219/1000 | Loss: 0.00001173
Iteration 220/1000 | Loss: 0.00001173
Iteration 221/1000 | Loss: 0.00001173
Iteration 222/1000 | Loss: 0.00001172
Iteration 223/1000 | Loss: 0.00001172
Iteration 224/1000 | Loss: 0.00001172
Iteration 225/1000 | Loss: 0.00001172
Iteration 226/1000 | Loss: 0.00001171
Iteration 227/1000 | Loss: 0.00001171
Iteration 228/1000 | Loss: 0.00001169
Iteration 229/1000 | Loss: 0.00001169
Iteration 230/1000 | Loss: 0.00001169
Iteration 231/1000 | Loss: 0.00001169
Iteration 232/1000 | Loss: 0.00001169
Iteration 233/1000 | Loss: 0.00001169
Iteration 234/1000 | Loss: 0.00001169
Iteration 235/1000 | Loss: 0.00001169
Iteration 236/1000 | Loss: 0.00001168
Iteration 237/1000 | Loss: 0.00001166
Iteration 238/1000 | Loss: 0.00001165
Iteration 239/1000 | Loss: 0.00001165
Iteration 240/1000 | Loss: 0.00001165
Iteration 241/1000 | Loss: 0.00001165
Iteration 242/1000 | Loss: 0.00001164
Iteration 243/1000 | Loss: 0.00001164
Iteration 244/1000 | Loss: 0.00001164
Iteration 245/1000 | Loss: 0.00001164
Iteration 246/1000 | Loss: 0.00001164
Iteration 247/1000 | Loss: 0.00001164
Iteration 248/1000 | Loss: 0.00001164
Iteration 249/1000 | Loss: 0.00001164
Iteration 250/1000 | Loss: 0.00001164
Iteration 251/1000 | Loss: 0.00001163
Iteration 252/1000 | Loss: 0.00001163
Iteration 253/1000 | Loss: 0.00001163
Iteration 254/1000 | Loss: 0.00001162
Iteration 255/1000 | Loss: 0.00001162
Iteration 256/1000 | Loss: 0.00001162
Iteration 257/1000 | Loss: 0.00001162
Iteration 258/1000 | Loss: 0.00001161
Iteration 259/1000 | Loss: 0.00001161
Iteration 260/1000 | Loss: 0.00001161
Iteration 261/1000 | Loss: 0.00001160
Iteration 262/1000 | Loss: 0.00001160
Iteration 263/1000 | Loss: 0.00001160
Iteration 264/1000 | Loss: 0.00001160
Iteration 265/1000 | Loss: 0.00001159
Iteration 266/1000 | Loss: 0.00001159
Iteration 267/1000 | Loss: 0.00001159
Iteration 268/1000 | Loss: 0.00001159
Iteration 269/1000 | Loss: 0.00001158
Iteration 270/1000 | Loss: 0.00001158
Iteration 271/1000 | Loss: 0.00001158
Iteration 272/1000 | Loss: 0.00001158
Iteration 273/1000 | Loss: 0.00001158
Iteration 274/1000 | Loss: 0.00001158
Iteration 275/1000 | Loss: 0.00001158
Iteration 276/1000 | Loss: 0.00001158
Iteration 277/1000 | Loss: 0.00001158
Iteration 278/1000 | Loss: 0.00001158
Iteration 279/1000 | Loss: 0.00001157
Iteration 280/1000 | Loss: 0.00001157
Iteration 281/1000 | Loss: 0.00001157
Iteration 282/1000 | Loss: 0.00001157
Iteration 283/1000 | Loss: 0.00001157
Iteration 284/1000 | Loss: 0.00001157
Iteration 285/1000 | Loss: 0.00001157
Iteration 286/1000 | Loss: 0.00001157
Iteration 287/1000 | Loss: 0.00001156
Iteration 288/1000 | Loss: 0.00001156
Iteration 289/1000 | Loss: 0.00001156
Iteration 290/1000 | Loss: 0.00001156
Iteration 291/1000 | Loss: 0.00001156
Iteration 292/1000 | Loss: 0.00001156
Iteration 293/1000 | Loss: 0.00001156
Iteration 294/1000 | Loss: 0.00001155
Iteration 295/1000 | Loss: 0.00001155
Iteration 296/1000 | Loss: 0.00001155
Iteration 297/1000 | Loss: 0.00001155
Iteration 298/1000 | Loss: 0.00001155
Iteration 299/1000 | Loss: 0.00001155
Iteration 300/1000 | Loss: 0.00001155
Iteration 301/1000 | Loss: 0.00001155
Iteration 302/1000 | Loss: 0.00001155
Iteration 303/1000 | Loss: 0.00001155
Iteration 304/1000 | Loss: 0.00001154
Iteration 305/1000 | Loss: 0.00001154
Iteration 306/1000 | Loss: 0.00001154
Iteration 307/1000 | Loss: 0.00001154
Iteration 308/1000 | Loss: 0.00001154
Iteration 309/1000 | Loss: 0.00001154
Iteration 310/1000 | Loss: 0.00001154
Iteration 311/1000 | Loss: 0.00001154
Iteration 312/1000 | Loss: 0.00001154
Iteration 313/1000 | Loss: 0.00001154
Iteration 314/1000 | Loss: 0.00001154
Iteration 315/1000 | Loss: 0.00001154
Iteration 316/1000 | Loss: 0.00001153
Iteration 317/1000 | Loss: 0.00001153
Iteration 318/1000 | Loss: 0.00001153
Iteration 319/1000 | Loss: 0.00001153
Iteration 320/1000 | Loss: 0.00001153
Iteration 321/1000 | Loss: 0.00001153
Iteration 322/1000 | Loss: 0.00001153
Iteration 323/1000 | Loss: 0.00001153
Iteration 324/1000 | Loss: 0.00001152
Iteration 325/1000 | Loss: 0.00001152
Iteration 326/1000 | Loss: 0.00001152
Iteration 327/1000 | Loss: 0.00001152
Iteration 328/1000 | Loss: 0.00001152
Iteration 329/1000 | Loss: 0.00001152
Iteration 330/1000 | Loss: 0.00001152
Iteration 331/1000 | Loss: 0.00001151
Iteration 332/1000 | Loss: 0.00001151
Iteration 333/1000 | Loss: 0.00001151
Iteration 334/1000 | Loss: 0.00001151
Iteration 335/1000 | Loss: 0.00001151
Iteration 336/1000 | Loss: 0.00001151
Iteration 337/1000 | Loss: 0.00001151
Iteration 338/1000 | Loss: 0.00001151
Iteration 339/1000 | Loss: 0.00001151
Iteration 340/1000 | Loss: 0.00001151
Iteration 341/1000 | Loss: 0.00001151
Iteration 342/1000 | Loss: 0.00001150
Iteration 343/1000 | Loss: 0.00001150
Iteration 344/1000 | Loss: 0.00001150
Iteration 345/1000 | Loss: 0.00001150
Iteration 346/1000 | Loss: 0.00001150
Iteration 347/1000 | Loss: 0.00001150
Iteration 348/1000 | Loss: 0.00001150
Iteration 349/1000 | Loss: 0.00001150
Iteration 350/1000 | Loss: 0.00001150
Iteration 351/1000 | Loss: 0.00001150
Iteration 352/1000 | Loss: 0.00001150
Iteration 353/1000 | Loss: 0.00001150
Iteration 354/1000 | Loss: 0.00001150
Iteration 355/1000 | Loss: 0.00001150
Iteration 356/1000 | Loss: 0.00001150
Iteration 357/1000 | Loss: 0.00001149
Iteration 358/1000 | Loss: 0.00001149
Iteration 359/1000 | Loss: 0.00001149
Iteration 360/1000 | Loss: 0.00001149
Iteration 361/1000 | Loss: 0.00001149
Iteration 362/1000 | Loss: 0.00001149
Iteration 363/1000 | Loss: 0.00001149
Iteration 364/1000 | Loss: 0.00001149
Iteration 365/1000 | Loss: 0.00001149
Iteration 366/1000 | Loss: 0.00001149
Iteration 367/1000 | Loss: 0.00001149
Iteration 368/1000 | Loss: 0.00001149
Iteration 369/1000 | Loss: 0.00001149
Iteration 370/1000 | Loss: 0.00001149
Iteration 371/1000 | Loss: 0.00001148
Iteration 372/1000 | Loss: 0.00001148
Iteration 373/1000 | Loss: 0.00001148
Iteration 374/1000 | Loss: 0.00001148
Iteration 375/1000 | Loss: 0.00001148
Iteration 376/1000 | Loss: 0.00001148
Iteration 377/1000 | Loss: 0.00001148
Iteration 378/1000 | Loss: 0.00001148
Iteration 379/1000 | Loss: 0.00001148
Iteration 380/1000 | Loss: 0.00001148
Iteration 381/1000 | Loss: 0.00001148
Iteration 382/1000 | Loss: 0.00001148
Iteration 383/1000 | Loss: 0.00001148
Iteration 384/1000 | Loss: 0.00001148
Iteration 385/1000 | Loss: 0.00001148
Iteration 386/1000 | Loss: 0.00001148
Iteration 387/1000 | Loss: 0.00001148
Iteration 388/1000 | Loss: 0.00001148
Iteration 389/1000 | Loss: 0.00001148
Iteration 390/1000 | Loss: 0.00001148
Iteration 391/1000 | Loss: 0.00001148
Iteration 392/1000 | Loss: 0.00001148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 392. Stopping optimization.
Last 5 losses: [1.1483188245620113e-05, 1.1483188245620113e-05, 1.1483188245620113e-05, 1.1483188245620113e-05, 1.1483188245620113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1483188245620113e-05

Optimization complete. Final v2v error: 2.819089889526367 mm

Highest mean error: 5.808913707733154 mm for frame 166

Lowest mean error: 2.4615204334259033 mm for frame 51

Saving results

Total time: 165.61008667945862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403646
Iteration 2/25 | Loss: 0.00129655
Iteration 3/25 | Loss: 0.00116735
Iteration 4/25 | Loss: 0.00115416
Iteration 5/25 | Loss: 0.00115017
Iteration 6/25 | Loss: 0.00114893
Iteration 7/25 | Loss: 0.00114876
Iteration 8/25 | Loss: 0.00114876
Iteration 9/25 | Loss: 0.00114876
Iteration 10/25 | Loss: 0.00114876
Iteration 11/25 | Loss: 0.00114876
Iteration 12/25 | Loss: 0.00114876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001148759271018207, 0.001148759271018207, 0.001148759271018207, 0.001148759271018207, 0.001148759271018207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001148759271018207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44624126
Iteration 2/25 | Loss: 0.00090125
Iteration 3/25 | Loss: 0.00090124
Iteration 4/25 | Loss: 0.00090124
Iteration 5/25 | Loss: 0.00090124
Iteration 6/25 | Loss: 0.00090124
Iteration 7/25 | Loss: 0.00090124
Iteration 8/25 | Loss: 0.00090124
Iteration 9/25 | Loss: 0.00090124
Iteration 10/25 | Loss: 0.00090124
Iteration 11/25 | Loss: 0.00090124
Iteration 12/25 | Loss: 0.00090124
Iteration 13/25 | Loss: 0.00090124
Iteration 14/25 | Loss: 0.00090124
Iteration 15/25 | Loss: 0.00090124
Iteration 16/25 | Loss: 0.00090124
Iteration 17/25 | Loss: 0.00090124
Iteration 18/25 | Loss: 0.00090124
Iteration 19/25 | Loss: 0.00090124
Iteration 20/25 | Loss: 0.00090124
Iteration 21/25 | Loss: 0.00090124
Iteration 22/25 | Loss: 0.00090124
Iteration 23/25 | Loss: 0.00090124
Iteration 24/25 | Loss: 0.00090124
Iteration 25/25 | Loss: 0.00090124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090124
Iteration 2/1000 | Loss: 0.00003691
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00001848
Iteration 5/1000 | Loss: 0.00001667
Iteration 6/1000 | Loss: 0.00001590
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001494
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001441
Iteration 12/1000 | Loss: 0.00001420
Iteration 13/1000 | Loss: 0.00001420
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001396
Iteration 16/1000 | Loss: 0.00001390
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001365
Iteration 25/1000 | Loss: 0.00001364
Iteration 26/1000 | Loss: 0.00001363
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001361
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001361
Iteration 32/1000 | Loss: 0.00001360
Iteration 33/1000 | Loss: 0.00001360
Iteration 34/1000 | Loss: 0.00001360
Iteration 35/1000 | Loss: 0.00001360
Iteration 36/1000 | Loss: 0.00001360
Iteration 37/1000 | Loss: 0.00001360
Iteration 38/1000 | Loss: 0.00001360
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001357
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001357
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001357
Iteration 55/1000 | Loss: 0.00001357
Iteration 56/1000 | Loss: 0.00001357
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001356
Iteration 59/1000 | Loss: 0.00001356
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001355
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001353
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001353
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001350
Iteration 97/1000 | Loss: 0.00001350
Iteration 98/1000 | Loss: 0.00001350
Iteration 99/1000 | Loss: 0.00001350
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001349
Iteration 104/1000 | Loss: 0.00001349
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001349
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001348
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001346
Iteration 124/1000 | Loss: 0.00001346
Iteration 125/1000 | Loss: 0.00001346
Iteration 126/1000 | Loss: 0.00001346
Iteration 127/1000 | Loss: 0.00001346
Iteration 128/1000 | Loss: 0.00001346
Iteration 129/1000 | Loss: 0.00001346
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001344
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001342
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001342
Iteration 158/1000 | Loss: 0.00001342
Iteration 159/1000 | Loss: 0.00001342
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001341
Iteration 165/1000 | Loss: 0.00001341
Iteration 166/1000 | Loss: 0.00001341
Iteration 167/1000 | Loss: 0.00001341
Iteration 168/1000 | Loss: 0.00001341
Iteration 169/1000 | Loss: 0.00001341
Iteration 170/1000 | Loss: 0.00001341
Iteration 171/1000 | Loss: 0.00001341
Iteration 172/1000 | Loss: 0.00001341
Iteration 173/1000 | Loss: 0.00001341
Iteration 174/1000 | Loss: 0.00001341
Iteration 175/1000 | Loss: 0.00001341
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00001340
Iteration 178/1000 | Loss: 0.00001340
Iteration 179/1000 | Loss: 0.00001340
Iteration 180/1000 | Loss: 0.00001340
Iteration 181/1000 | Loss: 0.00001340
Iteration 182/1000 | Loss: 0.00001340
Iteration 183/1000 | Loss: 0.00001340
Iteration 184/1000 | Loss: 0.00001340
Iteration 185/1000 | Loss: 0.00001340
Iteration 186/1000 | Loss: 0.00001340
Iteration 187/1000 | Loss: 0.00001340
Iteration 188/1000 | Loss: 0.00001340
Iteration 189/1000 | Loss: 0.00001340
Iteration 190/1000 | Loss: 0.00001340
Iteration 191/1000 | Loss: 0.00001340
Iteration 192/1000 | Loss: 0.00001340
Iteration 193/1000 | Loss: 0.00001340
Iteration 194/1000 | Loss: 0.00001340
Iteration 195/1000 | Loss: 0.00001340
Iteration 196/1000 | Loss: 0.00001340
Iteration 197/1000 | Loss: 0.00001340
Iteration 198/1000 | Loss: 0.00001340
Iteration 199/1000 | Loss: 0.00001340
Iteration 200/1000 | Loss: 0.00001340
Iteration 201/1000 | Loss: 0.00001340
Iteration 202/1000 | Loss: 0.00001340
Iteration 203/1000 | Loss: 0.00001340
Iteration 204/1000 | Loss: 0.00001340
Iteration 205/1000 | Loss: 0.00001340
Iteration 206/1000 | Loss: 0.00001340
Iteration 207/1000 | Loss: 0.00001340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.3397956536209676e-05, 1.3397956536209676e-05, 1.3397956536209676e-05, 1.3397956536209676e-05, 1.3397956536209676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3397956536209676e-05

Optimization complete. Final v2v error: 3.104358673095703 mm

Highest mean error: 4.385418891906738 mm for frame 67

Lowest mean error: 2.756626605987549 mm for frame 43

Saving results

Total time: 41.54323410987854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475092
Iteration 2/25 | Loss: 0.00123701
Iteration 3/25 | Loss: 0.00114740
Iteration 4/25 | Loss: 0.00113128
Iteration 5/25 | Loss: 0.00112655
Iteration 6/25 | Loss: 0.00112566
Iteration 7/25 | Loss: 0.00112566
Iteration 8/25 | Loss: 0.00112566
Iteration 9/25 | Loss: 0.00112566
Iteration 10/25 | Loss: 0.00112566
Iteration 11/25 | Loss: 0.00112566
Iteration 12/25 | Loss: 0.00112566
Iteration 13/25 | Loss: 0.00112566
Iteration 14/25 | Loss: 0.00112566
Iteration 15/25 | Loss: 0.00112566
Iteration 16/25 | Loss: 0.00112566
Iteration 17/25 | Loss: 0.00112566
Iteration 18/25 | Loss: 0.00112566
Iteration 19/25 | Loss: 0.00112566
Iteration 20/25 | Loss: 0.00112566
Iteration 21/25 | Loss: 0.00112566
Iteration 22/25 | Loss: 0.00112566
Iteration 23/25 | Loss: 0.00112566
Iteration 24/25 | Loss: 0.00112566
Iteration 25/25 | Loss: 0.00112566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38840079
Iteration 2/25 | Loss: 0.00095302
Iteration 3/25 | Loss: 0.00095301
Iteration 4/25 | Loss: 0.00095301
Iteration 5/25 | Loss: 0.00095300
Iteration 6/25 | Loss: 0.00095300
Iteration 7/25 | Loss: 0.00095300
Iteration 8/25 | Loss: 0.00095300
Iteration 9/25 | Loss: 0.00095300
Iteration 10/25 | Loss: 0.00095300
Iteration 11/25 | Loss: 0.00095300
Iteration 12/25 | Loss: 0.00095300
Iteration 13/25 | Loss: 0.00095300
Iteration 14/25 | Loss: 0.00095300
Iteration 15/25 | Loss: 0.00095300
Iteration 16/25 | Loss: 0.00095300
Iteration 17/25 | Loss: 0.00095300
Iteration 18/25 | Loss: 0.00095300
Iteration 19/25 | Loss: 0.00095300
Iteration 20/25 | Loss: 0.00095300
Iteration 21/25 | Loss: 0.00095300
Iteration 22/25 | Loss: 0.00095300
Iteration 23/25 | Loss: 0.00095300
Iteration 24/25 | Loss: 0.00095300
Iteration 25/25 | Loss: 0.00095300

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095300
Iteration 2/1000 | Loss: 0.00002400
Iteration 3/1000 | Loss: 0.00001744
Iteration 4/1000 | Loss: 0.00001599
Iteration 5/1000 | Loss: 0.00001531
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001459
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001425
Iteration 10/1000 | Loss: 0.00001398
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001370
Iteration 14/1000 | Loss: 0.00001368
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001362
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001362
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001361
Iteration 23/1000 | Loss: 0.00001361
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001360
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001357
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001349
Iteration 34/1000 | Loss: 0.00001349
Iteration 35/1000 | Loss: 0.00001347
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001345
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001345
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001344
Iteration 44/1000 | Loss: 0.00001344
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001343
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001335
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001333
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001329
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001328
Iteration 87/1000 | Loss: 0.00001328
Iteration 88/1000 | Loss: 0.00001328
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001327
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001327
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001321
Iteration 121/1000 | Loss: 0.00001321
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001321
Iteration 126/1000 | Loss: 0.00001321
Iteration 127/1000 | Loss: 0.00001321
Iteration 128/1000 | Loss: 0.00001320
Iteration 129/1000 | Loss: 0.00001320
Iteration 130/1000 | Loss: 0.00001320
Iteration 131/1000 | Loss: 0.00001320
Iteration 132/1000 | Loss: 0.00001320
Iteration 133/1000 | Loss: 0.00001320
Iteration 134/1000 | Loss: 0.00001319
Iteration 135/1000 | Loss: 0.00001319
Iteration 136/1000 | Loss: 0.00001319
Iteration 137/1000 | Loss: 0.00001319
Iteration 138/1000 | Loss: 0.00001319
Iteration 139/1000 | Loss: 0.00001319
Iteration 140/1000 | Loss: 0.00001319
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3185629541112576e-05, 1.3185629541112576e-05, 1.3185629541112576e-05, 1.3185629541112576e-05, 1.3185629541112576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3185629541112576e-05

Optimization complete. Final v2v error: 3.0548126697540283 mm

Highest mean error: 3.5647823810577393 mm for frame 60

Lowest mean error: 2.5687928199768066 mm for frame 55

Saving results

Total time: 41.248130083084106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047165
Iteration 2/25 | Loss: 0.00297642
Iteration 3/25 | Loss: 0.00215190
Iteration 4/25 | Loss: 0.00232579
Iteration 5/25 | Loss: 0.00244089
Iteration 6/25 | Loss: 0.00197868
Iteration 7/25 | Loss: 0.00176466
Iteration 8/25 | Loss: 0.00156750
Iteration 9/25 | Loss: 0.00150306
Iteration 10/25 | Loss: 0.00146877
Iteration 11/25 | Loss: 0.00142405
Iteration 12/25 | Loss: 0.00141878
Iteration 13/25 | Loss: 0.00140751
Iteration 14/25 | Loss: 0.00140160
Iteration 15/25 | Loss: 0.00139255
Iteration 16/25 | Loss: 0.00138231
Iteration 17/25 | Loss: 0.00137477
Iteration 18/25 | Loss: 0.00137707
Iteration 19/25 | Loss: 0.00136755
Iteration 20/25 | Loss: 0.00136657
Iteration 21/25 | Loss: 0.00137036
Iteration 22/25 | Loss: 0.00136721
Iteration 23/25 | Loss: 0.00135798
Iteration 24/25 | Loss: 0.00136275
Iteration 25/25 | Loss: 0.00136278

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63893312
Iteration 2/25 | Loss: 0.00097528
Iteration 3/25 | Loss: 0.00097527
Iteration 4/25 | Loss: 0.00097527
Iteration 5/25 | Loss: 0.00097527
Iteration 6/25 | Loss: 0.00097527
Iteration 7/25 | Loss: 0.00097527
Iteration 8/25 | Loss: 0.00097527
Iteration 9/25 | Loss: 0.00097527
Iteration 10/25 | Loss: 0.00097527
Iteration 11/25 | Loss: 0.00097527
Iteration 12/25 | Loss: 0.00097527
Iteration 13/25 | Loss: 0.00097527
Iteration 14/25 | Loss: 0.00097527
Iteration 15/25 | Loss: 0.00097527
Iteration 16/25 | Loss: 0.00097527
Iteration 17/25 | Loss: 0.00097527
Iteration 18/25 | Loss: 0.00097527
Iteration 19/25 | Loss: 0.00097527
Iteration 20/25 | Loss: 0.00097527
Iteration 21/25 | Loss: 0.00097527
Iteration 22/25 | Loss: 0.00097527
Iteration 23/25 | Loss: 0.00097527
Iteration 24/25 | Loss: 0.00097527
Iteration 25/25 | Loss: 0.00097527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097527
Iteration 2/1000 | Loss: 0.00043541
Iteration 3/1000 | Loss: 0.00034316
Iteration 4/1000 | Loss: 0.00031433
Iteration 5/1000 | Loss: 0.00020172
Iteration 6/1000 | Loss: 0.00031534
Iteration 7/1000 | Loss: 0.00039375
Iteration 8/1000 | Loss: 0.00030947
Iteration 9/1000 | Loss: 0.00060022
Iteration 10/1000 | Loss: 0.00035130
Iteration 11/1000 | Loss: 0.00039505
Iteration 12/1000 | Loss: 0.00045448
Iteration 13/1000 | Loss: 0.00029428
Iteration 14/1000 | Loss: 0.00030393
Iteration 15/1000 | Loss: 0.00042821
Iteration 16/1000 | Loss: 0.00022485
Iteration 17/1000 | Loss: 0.00044265
Iteration 18/1000 | Loss: 0.00033092
Iteration 19/1000 | Loss: 0.00031790
Iteration 20/1000 | Loss: 0.00043591
Iteration 21/1000 | Loss: 0.00038388
Iteration 22/1000 | Loss: 0.00030591
Iteration 23/1000 | Loss: 0.00030947
Iteration 24/1000 | Loss: 0.00032891
Iteration 25/1000 | Loss: 0.00030924
Iteration 26/1000 | Loss: 0.00043196
Iteration 27/1000 | Loss: 0.00039823
Iteration 28/1000 | Loss: 0.00040370
Iteration 29/1000 | Loss: 0.00034941
Iteration 30/1000 | Loss: 0.00032399
Iteration 31/1000 | Loss: 0.00031220
Iteration 32/1000 | Loss: 0.00042567
Iteration 33/1000 | Loss: 0.00038141
Iteration 34/1000 | Loss: 0.00039455
Iteration 35/1000 | Loss: 0.00050526
Iteration 36/1000 | Loss: 0.00047169
Iteration 37/1000 | Loss: 0.00042856
Iteration 38/1000 | Loss: 0.00033522
Iteration 39/1000 | Loss: 0.00031971
Iteration 40/1000 | Loss: 0.00032002
Iteration 41/1000 | Loss: 0.00049085
Iteration 42/1000 | Loss: 0.00045957
Iteration 43/1000 | Loss: 0.00051689
Iteration 44/1000 | Loss: 0.00054306
Iteration 45/1000 | Loss: 0.00031717
Iteration 46/1000 | Loss: 0.00035234
Iteration 47/1000 | Loss: 0.00087952
Iteration 48/1000 | Loss: 0.00053033
Iteration 49/1000 | Loss: 0.00029086
Iteration 50/1000 | Loss: 0.00045961
Iteration 51/1000 | Loss: 0.00050151
Iteration 52/1000 | Loss: 0.00033699
Iteration 53/1000 | Loss: 0.00050768
Iteration 54/1000 | Loss: 0.00034022
Iteration 55/1000 | Loss: 0.00044928
Iteration 56/1000 | Loss: 0.00054491
Iteration 57/1000 | Loss: 0.00148913
Iteration 58/1000 | Loss: 0.00041771
Iteration 59/1000 | Loss: 0.00030567
Iteration 60/1000 | Loss: 0.00035119
Iteration 61/1000 | Loss: 0.00038245
Iteration 62/1000 | Loss: 0.00031608
Iteration 63/1000 | Loss: 0.00048102
Iteration 64/1000 | Loss: 0.00028834
Iteration 65/1000 | Loss: 0.00030349
Iteration 66/1000 | Loss: 0.00027717
Iteration 67/1000 | Loss: 0.00020942
Iteration 68/1000 | Loss: 0.00030954
Iteration 69/1000 | Loss: 0.00031604
Iteration 70/1000 | Loss: 0.00030652
Iteration 71/1000 | Loss: 0.00029990
Iteration 72/1000 | Loss: 0.00031105
Iteration 73/1000 | Loss: 0.00053825
Iteration 74/1000 | Loss: 0.00047677
Iteration 75/1000 | Loss: 0.00030146
Iteration 76/1000 | Loss: 0.00031672
Iteration 77/1000 | Loss: 0.00050027
Iteration 78/1000 | Loss: 0.00033460
Iteration 79/1000 | Loss: 0.00046906
Iteration 80/1000 | Loss: 0.00033942
Iteration 81/1000 | Loss: 0.00035050
Iteration 82/1000 | Loss: 0.00044870
Iteration 83/1000 | Loss: 0.00030523
Iteration 84/1000 | Loss: 0.00032922
Iteration 85/1000 | Loss: 0.00028861
Iteration 86/1000 | Loss: 0.00031182
Iteration 87/1000 | Loss: 0.00029501
Iteration 88/1000 | Loss: 0.00030353
Iteration 89/1000 | Loss: 0.00030491
Iteration 90/1000 | Loss: 0.00031857
Iteration 91/1000 | Loss: 0.00030112
Iteration 92/1000 | Loss: 0.00033054
Iteration 93/1000 | Loss: 0.00030012
Iteration 94/1000 | Loss: 0.00031099
Iteration 95/1000 | Loss: 0.00030431
Iteration 96/1000 | Loss: 0.00030461
Iteration 97/1000 | Loss: 0.00030877
Iteration 98/1000 | Loss: 0.00030951
Iteration 99/1000 | Loss: 0.00031934
Iteration 100/1000 | Loss: 0.00031840
Iteration 101/1000 | Loss: 0.00029646
Iteration 102/1000 | Loss: 0.00032794
Iteration 103/1000 | Loss: 0.00033272
Iteration 104/1000 | Loss: 0.00029616
Iteration 105/1000 | Loss: 0.00030147
Iteration 106/1000 | Loss: 0.00026368
Iteration 107/1000 | Loss: 0.00032645
Iteration 108/1000 | Loss: 0.00030551
Iteration 109/1000 | Loss: 0.00032268
Iteration 110/1000 | Loss: 0.00035047
Iteration 111/1000 | Loss: 0.00031791
Iteration 112/1000 | Loss: 0.00030298
Iteration 113/1000 | Loss: 0.00029935
Iteration 114/1000 | Loss: 0.00022669
Iteration 115/1000 | Loss: 0.00021994
Iteration 116/1000 | Loss: 0.00023093
Iteration 117/1000 | Loss: 0.00020829
Iteration 118/1000 | Loss: 0.00020952
Iteration 119/1000 | Loss: 0.00022222
Iteration 120/1000 | Loss: 0.00021929
Iteration 121/1000 | Loss: 0.00028493
Iteration 122/1000 | Loss: 0.00029093
Iteration 123/1000 | Loss: 0.00023811
Iteration 124/1000 | Loss: 0.00021917
Iteration 125/1000 | Loss: 0.00025288
Iteration 126/1000 | Loss: 0.00028020
Iteration 127/1000 | Loss: 0.00028401
Iteration 128/1000 | Loss: 0.00028417
Iteration 129/1000 | Loss: 0.00029347
Iteration 130/1000 | Loss: 0.00017566
Iteration 131/1000 | Loss: 0.00025545
Iteration 132/1000 | Loss: 0.00027383
Iteration 133/1000 | Loss: 0.00027833
Iteration 134/1000 | Loss: 0.00031621
Iteration 135/1000 | Loss: 0.00033864
Iteration 136/1000 | Loss: 0.00032136
Iteration 137/1000 | Loss: 0.00030393
Iteration 138/1000 | Loss: 0.00031177
Iteration 139/1000 | Loss: 0.00029387
Iteration 140/1000 | Loss: 0.00030631
Iteration 141/1000 | Loss: 0.00031452
Iteration 142/1000 | Loss: 0.00028547
Iteration 143/1000 | Loss: 0.00028813
Iteration 144/1000 | Loss: 0.00034325
Iteration 145/1000 | Loss: 0.00030000
Iteration 146/1000 | Loss: 0.00028673
Iteration 147/1000 | Loss: 0.00029121
Iteration 148/1000 | Loss: 0.00031166
Iteration 149/1000 | Loss: 0.00029507
Iteration 150/1000 | Loss: 0.00030278
Iteration 151/1000 | Loss: 0.00030443
Iteration 152/1000 | Loss: 0.00027753
Iteration 153/1000 | Loss: 0.00031480
Iteration 154/1000 | Loss: 0.00029235
Iteration 155/1000 | Loss: 0.00027456
Iteration 156/1000 | Loss: 0.00024637
Iteration 157/1000 | Loss: 0.00030473
Iteration 158/1000 | Loss: 0.00034995
Iteration 159/1000 | Loss: 0.00033474
Iteration 160/1000 | Loss: 0.00019540
Iteration 161/1000 | Loss: 0.00031816
Iteration 162/1000 | Loss: 0.00029596
Iteration 163/1000 | Loss: 0.00036786
Iteration 164/1000 | Loss: 0.00028639
Iteration 165/1000 | Loss: 0.00033989
Iteration 166/1000 | Loss: 0.00028987
Iteration 167/1000 | Loss: 0.00032601
Iteration 168/1000 | Loss: 0.00024678
Iteration 169/1000 | Loss: 0.00019293
Iteration 170/1000 | Loss: 0.00024810
Iteration 171/1000 | Loss: 0.00025716
Iteration 172/1000 | Loss: 0.00013623
Iteration 173/1000 | Loss: 0.00010638
Iteration 174/1000 | Loss: 0.00010831
Iteration 175/1000 | Loss: 0.00007721
Iteration 176/1000 | Loss: 0.00009752
Iteration 177/1000 | Loss: 0.00007010
Iteration 178/1000 | Loss: 0.00008571
Iteration 179/1000 | Loss: 0.00010258
Iteration 180/1000 | Loss: 0.00009124
Iteration 181/1000 | Loss: 0.00007198
Iteration 182/1000 | Loss: 0.00007818
Iteration 183/1000 | Loss: 0.00008093
Iteration 184/1000 | Loss: 0.00007353
Iteration 185/1000 | Loss: 0.00008435
Iteration 186/1000 | Loss: 0.00006828
Iteration 187/1000 | Loss: 0.00007949
Iteration 188/1000 | Loss: 0.00007557
Iteration 189/1000 | Loss: 0.00007122
Iteration 190/1000 | Loss: 0.00008727
Iteration 191/1000 | Loss: 0.00008626
Iteration 192/1000 | Loss: 0.00007695
Iteration 193/1000 | Loss: 0.00007688
Iteration 194/1000 | Loss: 0.00008322
Iteration 195/1000 | Loss: 0.00007290
Iteration 196/1000 | Loss: 0.00006943
Iteration 197/1000 | Loss: 0.00007791
Iteration 198/1000 | Loss: 0.00004297
Iteration 199/1000 | Loss: 0.00007029
Iteration 200/1000 | Loss: 0.00008504
Iteration 201/1000 | Loss: 0.00007935
Iteration 202/1000 | Loss: 0.00008188
Iteration 203/1000 | Loss: 0.00007401
Iteration 204/1000 | Loss: 0.00008294
Iteration 205/1000 | Loss: 0.00007416
Iteration 206/1000 | Loss: 0.00008423
Iteration 207/1000 | Loss: 0.00006629
Iteration 208/1000 | Loss: 0.00027652
Iteration 209/1000 | Loss: 0.00021218
Iteration 210/1000 | Loss: 0.00029014
Iteration 211/1000 | Loss: 0.00008728
Iteration 212/1000 | Loss: 0.00007749
Iteration 213/1000 | Loss: 0.00012255
Iteration 214/1000 | Loss: 0.00020118
Iteration 215/1000 | Loss: 0.00007284
Iteration 216/1000 | Loss: 0.00024313
Iteration 217/1000 | Loss: 0.00021800
Iteration 218/1000 | Loss: 0.00012394
Iteration 219/1000 | Loss: 0.00014136
Iteration 220/1000 | Loss: 0.00018869
Iteration 221/1000 | Loss: 0.00020735
Iteration 222/1000 | Loss: 0.00007401
Iteration 223/1000 | Loss: 0.00007038
Iteration 224/1000 | Loss: 0.00006837
Iteration 225/1000 | Loss: 0.00006970
Iteration 226/1000 | Loss: 0.00006785
Iteration 227/1000 | Loss: 0.00020488
Iteration 228/1000 | Loss: 0.00016655
Iteration 229/1000 | Loss: 0.00019077
Iteration 230/1000 | Loss: 0.00014216
Iteration 231/1000 | Loss: 0.00006424
Iteration 232/1000 | Loss: 0.00004606
Iteration 233/1000 | Loss: 0.00005910
Iteration 234/1000 | Loss: 0.00005027
Iteration 235/1000 | Loss: 0.00006169
Iteration 236/1000 | Loss: 0.00003813
Iteration 237/1000 | Loss: 0.00004747
Iteration 238/1000 | Loss: 0.00004766
Iteration 239/1000 | Loss: 0.00006689
Iteration 240/1000 | Loss: 0.00004852
Iteration 241/1000 | Loss: 0.00005684
Iteration 242/1000 | Loss: 0.00006680
Iteration 243/1000 | Loss: 0.00005041
Iteration 244/1000 | Loss: 0.00006019
Iteration 245/1000 | Loss: 0.00005874
Iteration 246/1000 | Loss: 0.00005063
Iteration 247/1000 | Loss: 0.00005083
Iteration 248/1000 | Loss: 0.00005321
Iteration 249/1000 | Loss: 0.00005392
Iteration 250/1000 | Loss: 0.00005332
Iteration 251/1000 | Loss: 0.00005621
Iteration 252/1000 | Loss: 0.00006551
Iteration 253/1000 | Loss: 0.00005783
Iteration 254/1000 | Loss: 0.00005181
Iteration 255/1000 | Loss: 0.00005183
Iteration 256/1000 | Loss: 0.00005416
Iteration 257/1000 | Loss: 0.00005575
Iteration 258/1000 | Loss: 0.00005698
Iteration 259/1000 | Loss: 0.00006101
Iteration 260/1000 | Loss: 0.00008185
Iteration 261/1000 | Loss: 0.00005473
Iteration 262/1000 | Loss: 0.00005297
Iteration 263/1000 | Loss: 0.00005666
Iteration 264/1000 | Loss: 0.00006654
Iteration 265/1000 | Loss: 0.00005646
Iteration 266/1000 | Loss: 0.00004947
Iteration 267/1000 | Loss: 0.00005359
Iteration 268/1000 | Loss: 0.00006275
Iteration 269/1000 | Loss: 0.00005658
Iteration 270/1000 | Loss: 0.00006388
Iteration 271/1000 | Loss: 0.00005598
Iteration 272/1000 | Loss: 0.00005686
Iteration 273/1000 | Loss: 0.00005627
Iteration 274/1000 | Loss: 0.00005499
Iteration 275/1000 | Loss: 0.00005131
Iteration 276/1000 | Loss: 0.00005659
Iteration 277/1000 | Loss: 0.00005355
Iteration 278/1000 | Loss: 0.00005558
Iteration 279/1000 | Loss: 0.00005716
Iteration 280/1000 | Loss: 0.00007541
Iteration 281/1000 | Loss: 0.00005959
Iteration 282/1000 | Loss: 0.00005293
Iteration 283/1000 | Loss: 0.00005001
Iteration 284/1000 | Loss: 0.00007471
Iteration 285/1000 | Loss: 0.00005648
Iteration 286/1000 | Loss: 0.00005513
Iteration 287/1000 | Loss: 0.00005807
Iteration 288/1000 | Loss: 0.00004875
Iteration 289/1000 | Loss: 0.00005450
Iteration 290/1000 | Loss: 0.00005113
Iteration 291/1000 | Loss: 0.00005740
Iteration 292/1000 | Loss: 0.00005602
Iteration 293/1000 | Loss: 0.00006893
Iteration 294/1000 | Loss: 0.00007098
Iteration 295/1000 | Loss: 0.00005332
Iteration 296/1000 | Loss: 0.00004904
Iteration 297/1000 | Loss: 0.00004653
Iteration 298/1000 | Loss: 0.00004717
Iteration 299/1000 | Loss: 0.00004238
Iteration 300/1000 | Loss: 0.00004622
Iteration 301/1000 | Loss: 0.00003600
Iteration 302/1000 | Loss: 0.00003167
Iteration 303/1000 | Loss: 0.00004494
Iteration 304/1000 | Loss: 0.00004371
Iteration 305/1000 | Loss: 0.00004490
Iteration 306/1000 | Loss: 0.00004176
Iteration 307/1000 | Loss: 0.00004298
Iteration 308/1000 | Loss: 0.00004210
Iteration 309/1000 | Loss: 0.00004094
Iteration 310/1000 | Loss: 0.00020603
Iteration 311/1000 | Loss: 0.00016657
Iteration 312/1000 | Loss: 0.00019301
Iteration 313/1000 | Loss: 0.00008809
Iteration 314/1000 | Loss: 0.00002990
Iteration 315/1000 | Loss: 0.00002789
Iteration 316/1000 | Loss: 0.00002708
Iteration 317/1000 | Loss: 0.00002664
Iteration 318/1000 | Loss: 0.00021161
Iteration 319/1000 | Loss: 0.00014803
Iteration 320/1000 | Loss: 0.00006627
Iteration 321/1000 | Loss: 0.00002958
Iteration 322/1000 | Loss: 0.00002746
Iteration 323/1000 | Loss: 0.00002616
Iteration 324/1000 | Loss: 0.00002540
Iteration 325/1000 | Loss: 0.00002499
Iteration 326/1000 | Loss: 0.00002464
Iteration 327/1000 | Loss: 0.00002434
Iteration 328/1000 | Loss: 0.00002413
Iteration 329/1000 | Loss: 0.00002401
Iteration 330/1000 | Loss: 0.00002395
Iteration 331/1000 | Loss: 0.00002388
Iteration 332/1000 | Loss: 0.00002385
Iteration 333/1000 | Loss: 0.00002380
Iteration 334/1000 | Loss: 0.00002378
Iteration 335/1000 | Loss: 0.00002378
Iteration 336/1000 | Loss: 0.00002378
Iteration 337/1000 | Loss: 0.00002378
Iteration 338/1000 | Loss: 0.00002378
Iteration 339/1000 | Loss: 0.00002378
Iteration 340/1000 | Loss: 0.00002378
Iteration 341/1000 | Loss: 0.00002377
Iteration 342/1000 | Loss: 0.00002377
Iteration 343/1000 | Loss: 0.00002377
Iteration 344/1000 | Loss: 0.00002377
Iteration 345/1000 | Loss: 0.00002377
Iteration 346/1000 | Loss: 0.00002377
Iteration 347/1000 | Loss: 0.00002377
Iteration 348/1000 | Loss: 0.00002377
Iteration 349/1000 | Loss: 0.00002377
Iteration 350/1000 | Loss: 0.00002377
Iteration 351/1000 | Loss: 0.00002377
Iteration 352/1000 | Loss: 0.00002377
Iteration 353/1000 | Loss: 0.00002377
Iteration 354/1000 | Loss: 0.00002377
Iteration 355/1000 | Loss: 0.00002377
Iteration 356/1000 | Loss: 0.00002377
Iteration 357/1000 | Loss: 0.00002377
Iteration 358/1000 | Loss: 0.00002376
Iteration 359/1000 | Loss: 0.00002376
Iteration 360/1000 | Loss: 0.00002376
Iteration 361/1000 | Loss: 0.00002376
Iteration 362/1000 | Loss: 0.00002376
Iteration 363/1000 | Loss: 0.00002376
Iteration 364/1000 | Loss: 0.00002376
Iteration 365/1000 | Loss: 0.00002375
Iteration 366/1000 | Loss: 0.00002375
Iteration 367/1000 | Loss: 0.00002375
Iteration 368/1000 | Loss: 0.00002375
Iteration 369/1000 | Loss: 0.00002375
Iteration 370/1000 | Loss: 0.00002375
Iteration 371/1000 | Loss: 0.00002374
Iteration 372/1000 | Loss: 0.00002374
Iteration 373/1000 | Loss: 0.00002374
Iteration 374/1000 | Loss: 0.00002374
Iteration 375/1000 | Loss: 0.00002374
Iteration 376/1000 | Loss: 0.00002374
Iteration 377/1000 | Loss: 0.00002374
Iteration 378/1000 | Loss: 0.00002374
Iteration 379/1000 | Loss: 0.00002374
Iteration 380/1000 | Loss: 0.00002374
Iteration 381/1000 | Loss: 0.00002374
Iteration 382/1000 | Loss: 0.00002374
Iteration 383/1000 | Loss: 0.00002374
Iteration 384/1000 | Loss: 0.00002374
Iteration 385/1000 | Loss: 0.00002374
Iteration 386/1000 | Loss: 0.00002373
Iteration 387/1000 | Loss: 0.00002373
Iteration 388/1000 | Loss: 0.00002373
Iteration 389/1000 | Loss: 0.00002373
Iteration 390/1000 | Loss: 0.00002373
Iteration 391/1000 | Loss: 0.00002373
Iteration 392/1000 | Loss: 0.00002373
Iteration 393/1000 | Loss: 0.00002373
Iteration 394/1000 | Loss: 0.00002373
Iteration 395/1000 | Loss: 0.00002373
Iteration 396/1000 | Loss: 0.00002373
Iteration 397/1000 | Loss: 0.00002373
Iteration 398/1000 | Loss: 0.00002373
Iteration 399/1000 | Loss: 0.00002373
Iteration 400/1000 | Loss: 0.00002373
Iteration 401/1000 | Loss: 0.00002373
Iteration 402/1000 | Loss: 0.00002373
Iteration 403/1000 | Loss: 0.00002372
Iteration 404/1000 | Loss: 0.00002372
Iteration 405/1000 | Loss: 0.00002372
Iteration 406/1000 | Loss: 0.00002372
Iteration 407/1000 | Loss: 0.00002372
Iteration 408/1000 | Loss: 0.00002372
Iteration 409/1000 | Loss: 0.00002372
Iteration 410/1000 | Loss: 0.00002372
Iteration 411/1000 | Loss: 0.00002372
Iteration 412/1000 | Loss: 0.00002371
Iteration 413/1000 | Loss: 0.00002371
Iteration 414/1000 | Loss: 0.00002371
Iteration 415/1000 | Loss: 0.00002371
Iteration 416/1000 | Loss: 0.00002371
Iteration 417/1000 | Loss: 0.00002371
Iteration 418/1000 | Loss: 0.00002371
Iteration 419/1000 | Loss: 0.00002371
Iteration 420/1000 | Loss: 0.00002371
Iteration 421/1000 | Loss: 0.00002370
Iteration 422/1000 | Loss: 0.00002370
Iteration 423/1000 | Loss: 0.00002370
Iteration 424/1000 | Loss: 0.00002370
Iteration 425/1000 | Loss: 0.00002370
Iteration 426/1000 | Loss: 0.00002370
Iteration 427/1000 | Loss: 0.00002370
Iteration 428/1000 | Loss: 0.00002370
Iteration 429/1000 | Loss: 0.00002370
Iteration 430/1000 | Loss: 0.00002370
Iteration 431/1000 | Loss: 0.00002370
Iteration 432/1000 | Loss: 0.00002370
Iteration 433/1000 | Loss: 0.00002370
Iteration 434/1000 | Loss: 0.00002370
Iteration 435/1000 | Loss: 0.00002370
Iteration 436/1000 | Loss: 0.00002370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 436. Stopping optimization.
Last 5 losses: [2.3703660190221854e-05, 2.3703660190221854e-05, 2.3703660190221854e-05, 2.3703660190221854e-05, 2.3703660190221854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3703660190221854e-05

Optimization complete. Final v2v error: 4.043128967285156 mm

Highest mean error: 4.92427921295166 mm for frame 120

Lowest mean error: 3.5583698749542236 mm for frame 38

Saving results

Total time: 525.1989755630493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983514
Iteration 2/25 | Loss: 0.00212460
Iteration 3/25 | Loss: 0.00161825
Iteration 4/25 | Loss: 0.00151931
Iteration 5/25 | Loss: 0.00145963
Iteration 6/25 | Loss: 0.00144986
Iteration 7/25 | Loss: 0.00150945
Iteration 8/25 | Loss: 0.00153124
Iteration 9/25 | Loss: 0.00146790
Iteration 10/25 | Loss: 0.00138328
Iteration 11/25 | Loss: 0.00127387
Iteration 12/25 | Loss: 0.00125443
Iteration 13/25 | Loss: 0.00124051
Iteration 14/25 | Loss: 0.00123990
Iteration 15/25 | Loss: 0.00120944
Iteration 16/25 | Loss: 0.00121961
Iteration 17/25 | Loss: 0.00119545
Iteration 18/25 | Loss: 0.00118543
Iteration 19/25 | Loss: 0.00118265
Iteration 20/25 | Loss: 0.00118189
Iteration 21/25 | Loss: 0.00118446
Iteration 22/25 | Loss: 0.00118222
Iteration 23/25 | Loss: 0.00117556
Iteration 24/25 | Loss: 0.00117163
Iteration 25/25 | Loss: 0.00117017

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37306082
Iteration 2/25 | Loss: 0.00086117
Iteration 3/25 | Loss: 0.00086116
Iteration 4/25 | Loss: 0.00086116
Iteration 5/25 | Loss: 0.00086116
Iteration 6/25 | Loss: 0.00086116
Iteration 7/25 | Loss: 0.00086116
Iteration 8/25 | Loss: 0.00086116
Iteration 9/25 | Loss: 0.00086116
Iteration 10/25 | Loss: 0.00086116
Iteration 11/25 | Loss: 0.00086116
Iteration 12/25 | Loss: 0.00086116
Iteration 13/25 | Loss: 0.00086116
Iteration 14/25 | Loss: 0.00086116
Iteration 15/25 | Loss: 0.00086116
Iteration 16/25 | Loss: 0.00086116
Iteration 17/25 | Loss: 0.00086116
Iteration 18/25 | Loss: 0.00086116
Iteration 19/25 | Loss: 0.00086116
Iteration 20/25 | Loss: 0.00086116
Iteration 21/25 | Loss: 0.00086116
Iteration 22/25 | Loss: 0.00086116
Iteration 23/25 | Loss: 0.00086116
Iteration 24/25 | Loss: 0.00086116
Iteration 25/25 | Loss: 0.00086116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086116
Iteration 2/1000 | Loss: 0.00004913
Iteration 3/1000 | Loss: 0.00003465
Iteration 4/1000 | Loss: 0.00002952
Iteration 5/1000 | Loss: 0.00015847
Iteration 6/1000 | Loss: 0.00020190
Iteration 7/1000 | Loss: 0.00004259
Iteration 8/1000 | Loss: 0.00003323
Iteration 9/1000 | Loss: 0.00002864
Iteration 10/1000 | Loss: 0.00002977
Iteration 11/1000 | Loss: 0.00003491
Iteration 12/1000 | Loss: 0.00003558
Iteration 13/1000 | Loss: 0.00002872
Iteration 14/1000 | Loss: 0.00003847
Iteration 15/1000 | Loss: 0.00003176
Iteration 16/1000 | Loss: 0.00003771
Iteration 17/1000 | Loss: 0.00003319
Iteration 18/1000 | Loss: 0.00003653
Iteration 19/1000 | Loss: 0.00003181
Iteration 20/1000 | Loss: 0.00003560
Iteration 21/1000 | Loss: 0.00003192
Iteration 22/1000 | Loss: 0.00002746
Iteration 23/1000 | Loss: 0.00002470
Iteration 24/1000 | Loss: 0.00002307
Iteration 25/1000 | Loss: 0.00002245
Iteration 26/1000 | Loss: 0.00002218
Iteration 27/1000 | Loss: 0.00044647
Iteration 28/1000 | Loss: 0.00042470
Iteration 29/1000 | Loss: 0.00002444
Iteration 30/1000 | Loss: 0.00002093
Iteration 31/1000 | Loss: 0.00015701
Iteration 32/1000 | Loss: 0.00001830
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001694
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001649
Iteration 37/1000 | Loss: 0.00001627
Iteration 38/1000 | Loss: 0.00001603
Iteration 39/1000 | Loss: 0.00001577
Iteration 40/1000 | Loss: 0.00001562
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001549
Iteration 44/1000 | Loss: 0.00001549
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001546
Iteration 48/1000 | Loss: 0.00001546
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001546
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001542
Iteration 60/1000 | Loss: 0.00001542
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001542
Iteration 63/1000 | Loss: 0.00001542
Iteration 64/1000 | Loss: 0.00001541
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001541
Iteration 67/1000 | Loss: 0.00001541
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001541
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001540
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001538
Iteration 76/1000 | Loss: 0.00001538
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001537
Iteration 79/1000 | Loss: 0.00001537
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001536
Iteration 84/1000 | Loss: 0.00001536
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001535
Iteration 90/1000 | Loss: 0.00001535
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001533
Iteration 98/1000 | Loss: 0.00001532
Iteration 99/1000 | Loss: 0.00001532
Iteration 100/1000 | Loss: 0.00001532
Iteration 101/1000 | Loss: 0.00001532
Iteration 102/1000 | Loss: 0.00001531
Iteration 103/1000 | Loss: 0.00001531
Iteration 104/1000 | Loss: 0.00001531
Iteration 105/1000 | Loss: 0.00001530
Iteration 106/1000 | Loss: 0.00001530
Iteration 107/1000 | Loss: 0.00001530
Iteration 108/1000 | Loss: 0.00001530
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001529
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001529
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001529
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001528
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001528
Iteration 124/1000 | Loss: 0.00001528
Iteration 125/1000 | Loss: 0.00001528
Iteration 126/1000 | Loss: 0.00001528
Iteration 127/1000 | Loss: 0.00001528
Iteration 128/1000 | Loss: 0.00001528
Iteration 129/1000 | Loss: 0.00001528
Iteration 130/1000 | Loss: 0.00001528
Iteration 131/1000 | Loss: 0.00001528
Iteration 132/1000 | Loss: 0.00001528
Iteration 133/1000 | Loss: 0.00001528
Iteration 134/1000 | Loss: 0.00001528
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001528
Iteration 140/1000 | Loss: 0.00001528
Iteration 141/1000 | Loss: 0.00001528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.5277893908205442e-05, 1.5277893908205442e-05, 1.5277893908205442e-05, 1.5277893908205442e-05, 1.5277893908205442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5277893908205442e-05

Optimization complete. Final v2v error: 3.318294048309326 mm

Highest mean error: 5.5960612297058105 mm for frame 202

Lowest mean error: 2.875537872314453 mm for frame 5

Saving results

Total time: 124.93879652023315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981375
Iteration 2/25 | Loss: 0.00261094
Iteration 3/25 | Loss: 0.00207966
Iteration 4/25 | Loss: 0.00194067
Iteration 5/25 | Loss: 0.00190236
Iteration 6/25 | Loss: 0.00175556
Iteration 7/25 | Loss: 0.00159669
Iteration 8/25 | Loss: 0.00157224
Iteration 9/25 | Loss: 0.00153635
Iteration 10/25 | Loss: 0.00144469
Iteration 11/25 | Loss: 0.00143754
Iteration 12/25 | Loss: 0.00141768
Iteration 13/25 | Loss: 0.00141130
Iteration 14/25 | Loss: 0.00140155
Iteration 15/25 | Loss: 0.00140053
Iteration 16/25 | Loss: 0.00140026
Iteration 17/25 | Loss: 0.00140017
Iteration 18/25 | Loss: 0.00140015
Iteration 19/25 | Loss: 0.00140015
Iteration 20/25 | Loss: 0.00140015
Iteration 21/25 | Loss: 0.00140015
Iteration 22/25 | Loss: 0.00140015
Iteration 23/25 | Loss: 0.00140015
Iteration 24/25 | Loss: 0.00140015
Iteration 25/25 | Loss: 0.00140014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33786964
Iteration 2/25 | Loss: 0.00117415
Iteration 3/25 | Loss: 0.00117415
Iteration 4/25 | Loss: 0.00117415
Iteration 5/25 | Loss: 0.00117415
Iteration 6/25 | Loss: 0.00117415
Iteration 7/25 | Loss: 0.00117415
Iteration 8/25 | Loss: 0.00117415
Iteration 9/25 | Loss: 0.00117415
Iteration 10/25 | Loss: 0.00117415
Iteration 11/25 | Loss: 0.00117415
Iteration 12/25 | Loss: 0.00117415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011741496855393052, 0.0011741496855393052, 0.0011741496855393052, 0.0011741496855393052, 0.0011741496855393052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011741496855393052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117415
Iteration 2/1000 | Loss: 0.00150298
Iteration 3/1000 | Loss: 0.00329152
Iteration 4/1000 | Loss: 0.00083657
Iteration 5/1000 | Loss: 0.00205822
Iteration 6/1000 | Loss: 0.00013378
Iteration 7/1000 | Loss: 0.00014766
Iteration 8/1000 | Loss: 0.00012583
Iteration 9/1000 | Loss: 0.00034738
Iteration 10/1000 | Loss: 0.00024035
Iteration 11/1000 | Loss: 0.00008277
Iteration 12/1000 | Loss: 0.00035567
Iteration 13/1000 | Loss: 0.00007965
Iteration 14/1000 | Loss: 0.00007800
Iteration 15/1000 | Loss: 0.00007583
Iteration 16/1000 | Loss: 0.00007413
Iteration 17/1000 | Loss: 0.00007273
Iteration 18/1000 | Loss: 0.00007139
Iteration 19/1000 | Loss: 0.00007040
Iteration 20/1000 | Loss: 0.00006953
Iteration 21/1000 | Loss: 0.00021120
Iteration 22/1000 | Loss: 0.00087595
Iteration 23/1000 | Loss: 0.00013792
Iteration 24/1000 | Loss: 0.00007002
Iteration 25/1000 | Loss: 0.00013530
Iteration 26/1000 | Loss: 0.00006850
Iteration 27/1000 | Loss: 0.00006786
Iteration 28/1000 | Loss: 0.00006740
Iteration 29/1000 | Loss: 0.00020399
Iteration 30/1000 | Loss: 0.00006697
Iteration 31/1000 | Loss: 0.00006664
Iteration 32/1000 | Loss: 0.00079749
Iteration 33/1000 | Loss: 0.00133554
Iteration 34/1000 | Loss: 0.00214846
Iteration 35/1000 | Loss: 0.00179252
Iteration 36/1000 | Loss: 0.00069346
Iteration 37/1000 | Loss: 0.00182421
Iteration 38/1000 | Loss: 0.00144612
Iteration 39/1000 | Loss: 0.00045615
Iteration 40/1000 | Loss: 0.00158631
Iteration 41/1000 | Loss: 0.00146150
Iteration 42/1000 | Loss: 0.00172776
Iteration 43/1000 | Loss: 0.00065104
Iteration 44/1000 | Loss: 0.00190779
Iteration 45/1000 | Loss: 0.00201700
Iteration 46/1000 | Loss: 0.00119008
Iteration 47/1000 | Loss: 0.00110451
Iteration 48/1000 | Loss: 0.00122919
Iteration 49/1000 | Loss: 0.00012034
Iteration 50/1000 | Loss: 0.00041928
Iteration 51/1000 | Loss: 0.00031551
Iteration 52/1000 | Loss: 0.00007702
Iteration 53/1000 | Loss: 0.00013707
Iteration 54/1000 | Loss: 0.00014720
Iteration 55/1000 | Loss: 0.00006606
Iteration 56/1000 | Loss: 0.00068330
Iteration 57/1000 | Loss: 0.00099302
Iteration 58/1000 | Loss: 0.00026695
Iteration 59/1000 | Loss: 0.00006122
Iteration 60/1000 | Loss: 0.00005887
Iteration 61/1000 | Loss: 0.00005699
Iteration 62/1000 | Loss: 0.00005508
Iteration 63/1000 | Loss: 0.00005369
Iteration 64/1000 | Loss: 0.00014257
Iteration 65/1000 | Loss: 0.00102383
Iteration 66/1000 | Loss: 0.00013925
Iteration 67/1000 | Loss: 0.00018420
Iteration 68/1000 | Loss: 0.00016756
Iteration 69/1000 | Loss: 0.00005400
Iteration 70/1000 | Loss: 0.00016420
Iteration 71/1000 | Loss: 0.00045467
Iteration 72/1000 | Loss: 0.00027283
Iteration 73/1000 | Loss: 0.00013700
Iteration 74/1000 | Loss: 0.00019354
Iteration 75/1000 | Loss: 0.00014349
Iteration 76/1000 | Loss: 0.00005910
Iteration 77/1000 | Loss: 0.00005243
Iteration 78/1000 | Loss: 0.00036617
Iteration 79/1000 | Loss: 0.00028777
Iteration 80/1000 | Loss: 0.00005065
Iteration 81/1000 | Loss: 0.00004966
Iteration 82/1000 | Loss: 0.00004933
Iteration 83/1000 | Loss: 0.00004902
Iteration 84/1000 | Loss: 0.00004880
Iteration 85/1000 | Loss: 0.00004865
Iteration 86/1000 | Loss: 0.00004846
Iteration 87/1000 | Loss: 0.00004842
Iteration 88/1000 | Loss: 0.00004826
Iteration 89/1000 | Loss: 0.00004814
Iteration 90/1000 | Loss: 0.00004795
Iteration 91/1000 | Loss: 0.00004782
Iteration 92/1000 | Loss: 0.00004778
Iteration 93/1000 | Loss: 0.00021632
Iteration 94/1000 | Loss: 0.00004787
Iteration 95/1000 | Loss: 0.00004746
Iteration 96/1000 | Loss: 0.00004727
Iteration 97/1000 | Loss: 0.00041789
Iteration 98/1000 | Loss: 0.00067438
Iteration 99/1000 | Loss: 0.00031727
Iteration 100/1000 | Loss: 0.00049275
Iteration 101/1000 | Loss: 0.00026423
Iteration 102/1000 | Loss: 0.00010336
Iteration 103/1000 | Loss: 0.00005135
Iteration 104/1000 | Loss: 0.00004768
Iteration 105/1000 | Loss: 0.00017774
Iteration 106/1000 | Loss: 0.00004668
Iteration 107/1000 | Loss: 0.00004641
Iteration 108/1000 | Loss: 0.00004625
Iteration 109/1000 | Loss: 0.00004622
Iteration 110/1000 | Loss: 0.00004621
Iteration 111/1000 | Loss: 0.00004621
Iteration 112/1000 | Loss: 0.00004621
Iteration 113/1000 | Loss: 0.00004620
Iteration 114/1000 | Loss: 0.00004618
Iteration 115/1000 | Loss: 0.00004618
Iteration 116/1000 | Loss: 0.00004617
Iteration 117/1000 | Loss: 0.00004617
Iteration 118/1000 | Loss: 0.00004617
Iteration 119/1000 | Loss: 0.00004617
Iteration 120/1000 | Loss: 0.00004617
Iteration 121/1000 | Loss: 0.00004617
Iteration 122/1000 | Loss: 0.00004617
Iteration 123/1000 | Loss: 0.00004617
Iteration 124/1000 | Loss: 0.00004617
Iteration 125/1000 | Loss: 0.00004617
Iteration 126/1000 | Loss: 0.00004617
Iteration 127/1000 | Loss: 0.00004617
Iteration 128/1000 | Loss: 0.00004616
Iteration 129/1000 | Loss: 0.00004616
Iteration 130/1000 | Loss: 0.00004616
Iteration 131/1000 | Loss: 0.00004616
Iteration 132/1000 | Loss: 0.00004616
Iteration 133/1000 | Loss: 0.00004616
Iteration 134/1000 | Loss: 0.00004616
Iteration 135/1000 | Loss: 0.00004616
Iteration 136/1000 | Loss: 0.00004616
Iteration 137/1000 | Loss: 0.00004616
Iteration 138/1000 | Loss: 0.00004616
Iteration 139/1000 | Loss: 0.00004616
Iteration 140/1000 | Loss: 0.00004616
Iteration 141/1000 | Loss: 0.00004616
Iteration 142/1000 | Loss: 0.00004616
Iteration 143/1000 | Loss: 0.00004615
Iteration 144/1000 | Loss: 0.00004615
Iteration 145/1000 | Loss: 0.00004615
Iteration 146/1000 | Loss: 0.00004615
Iteration 147/1000 | Loss: 0.00004615
Iteration 148/1000 | Loss: 0.00004615
Iteration 149/1000 | Loss: 0.00004615
Iteration 150/1000 | Loss: 0.00004615
Iteration 151/1000 | Loss: 0.00004615
Iteration 152/1000 | Loss: 0.00004615
Iteration 153/1000 | Loss: 0.00004615
Iteration 154/1000 | Loss: 0.00004615
Iteration 155/1000 | Loss: 0.00004615
Iteration 156/1000 | Loss: 0.00004614
Iteration 157/1000 | Loss: 0.00004614
Iteration 158/1000 | Loss: 0.00004614
Iteration 159/1000 | Loss: 0.00004614
Iteration 160/1000 | Loss: 0.00004614
Iteration 161/1000 | Loss: 0.00004614
Iteration 162/1000 | Loss: 0.00004614
Iteration 163/1000 | Loss: 0.00004614
Iteration 164/1000 | Loss: 0.00004614
Iteration 165/1000 | Loss: 0.00004614
Iteration 166/1000 | Loss: 0.00004614
Iteration 167/1000 | Loss: 0.00004614
Iteration 168/1000 | Loss: 0.00004614
Iteration 169/1000 | Loss: 0.00004614
Iteration 170/1000 | Loss: 0.00004614
Iteration 171/1000 | Loss: 0.00004614
Iteration 172/1000 | Loss: 0.00004614
Iteration 173/1000 | Loss: 0.00004614
Iteration 174/1000 | Loss: 0.00004614
Iteration 175/1000 | Loss: 0.00004614
Iteration 176/1000 | Loss: 0.00004614
Iteration 177/1000 | Loss: 0.00004614
Iteration 178/1000 | Loss: 0.00004614
Iteration 179/1000 | Loss: 0.00004614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [4.6136308810673654e-05, 4.6136308810673654e-05, 4.6136308810673654e-05, 4.6136308810673654e-05, 4.6136308810673654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6136308810673654e-05

Optimization complete. Final v2v error: 4.886658668518066 mm

Highest mean error: 11.211324691772461 mm for frame 119

Lowest mean error: 4.436299800872803 mm for frame 51

Saving results

Total time: 207.60431814193726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823142
Iteration 2/25 | Loss: 0.00124202
Iteration 3/25 | Loss: 0.00114492
Iteration 4/25 | Loss: 0.00113508
Iteration 5/25 | Loss: 0.00113184
Iteration 6/25 | Loss: 0.00113132
Iteration 7/25 | Loss: 0.00113132
Iteration 8/25 | Loss: 0.00113132
Iteration 9/25 | Loss: 0.00113132
Iteration 10/25 | Loss: 0.00113132
Iteration 11/25 | Loss: 0.00113132
Iteration 12/25 | Loss: 0.00113132
Iteration 13/25 | Loss: 0.00113132
Iteration 14/25 | Loss: 0.00113132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011313189752399921, 0.0011313189752399921, 0.0011313189752399921, 0.0011313189752399921, 0.0011313189752399921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011313189752399921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61843193
Iteration 2/25 | Loss: 0.00086224
Iteration 3/25 | Loss: 0.00086224
Iteration 4/25 | Loss: 0.00086224
Iteration 5/25 | Loss: 0.00086224
Iteration 6/25 | Loss: 0.00086224
Iteration 7/25 | Loss: 0.00086224
Iteration 8/25 | Loss: 0.00086224
Iteration 9/25 | Loss: 0.00086224
Iteration 10/25 | Loss: 0.00086224
Iteration 11/25 | Loss: 0.00086224
Iteration 12/25 | Loss: 0.00086224
Iteration 13/25 | Loss: 0.00086224
Iteration 14/25 | Loss: 0.00086224
Iteration 15/25 | Loss: 0.00086224
Iteration 16/25 | Loss: 0.00086224
Iteration 17/25 | Loss: 0.00086224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008622396271675825, 0.0008622396271675825, 0.0008622396271675825, 0.0008622396271675825, 0.0008622396271675825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008622396271675825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086224
Iteration 2/1000 | Loss: 0.00003572
Iteration 3/1000 | Loss: 0.00002432
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001785
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001580
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001528
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001491
Iteration 15/1000 | Loss: 0.00001487
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001473
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001465
Iteration 29/1000 | Loss: 0.00001464
Iteration 30/1000 | Loss: 0.00001462
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001456
Iteration 37/1000 | Loss: 0.00001456
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001453
Iteration 41/1000 | Loss: 0.00001452
Iteration 42/1000 | Loss: 0.00001452
Iteration 43/1000 | Loss: 0.00001451
Iteration 44/1000 | Loss: 0.00001451
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001446
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001444
Iteration 65/1000 | Loss: 0.00001444
Iteration 66/1000 | Loss: 0.00001444
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001444
Iteration 71/1000 | Loss: 0.00001444
Iteration 72/1000 | Loss: 0.00001444
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001444
Iteration 82/1000 | Loss: 0.00001444
Iteration 83/1000 | Loss: 0.00001444
Iteration 84/1000 | Loss: 0.00001444
Iteration 85/1000 | Loss: 0.00001444
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.4435815501201432e-05, 1.4435815501201432e-05, 1.4435815501201432e-05, 1.4435815501201432e-05, 1.4435815501201432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4435815501201432e-05

Optimization complete. Final v2v error: 3.1791510581970215 mm

Highest mean error: 4.291923522949219 mm for frame 70

Lowest mean error: 2.572547197341919 mm for frame 129

Saving results

Total time: 33.755672454833984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780099
Iteration 2/25 | Loss: 0.00117774
Iteration 3/25 | Loss: 0.00107698
Iteration 4/25 | Loss: 0.00106767
Iteration 5/25 | Loss: 0.00106532
Iteration 6/25 | Loss: 0.00106532
Iteration 7/25 | Loss: 0.00106532
Iteration 8/25 | Loss: 0.00106532
Iteration 9/25 | Loss: 0.00106532
Iteration 10/25 | Loss: 0.00106532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00106532359495759, 0.00106532359495759, 0.00106532359495759, 0.00106532359495759, 0.00106532359495759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00106532359495759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35524940
Iteration 2/25 | Loss: 0.00078925
Iteration 3/25 | Loss: 0.00078925
Iteration 4/25 | Loss: 0.00078925
Iteration 5/25 | Loss: 0.00078925
Iteration 6/25 | Loss: 0.00078925
Iteration 7/25 | Loss: 0.00078925
Iteration 8/25 | Loss: 0.00078925
Iteration 9/25 | Loss: 0.00078925
Iteration 10/25 | Loss: 0.00078925
Iteration 11/25 | Loss: 0.00078925
Iteration 12/25 | Loss: 0.00078925
Iteration 13/25 | Loss: 0.00078925
Iteration 14/25 | Loss: 0.00078925
Iteration 15/25 | Loss: 0.00078925
Iteration 16/25 | Loss: 0.00078925
Iteration 17/25 | Loss: 0.00078925
Iteration 18/25 | Loss: 0.00078925
Iteration 19/25 | Loss: 0.00078925
Iteration 20/25 | Loss: 0.00078925
Iteration 21/25 | Loss: 0.00078925
Iteration 22/25 | Loss: 0.00078925
Iteration 23/25 | Loss: 0.00078925
Iteration 24/25 | Loss: 0.00078925
Iteration 25/25 | Loss: 0.00078925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078925
Iteration 2/1000 | Loss: 0.00002232
Iteration 3/1000 | Loss: 0.00001406
Iteration 4/1000 | Loss: 0.00001194
Iteration 5/1000 | Loss: 0.00001115
Iteration 6/1000 | Loss: 0.00001059
Iteration 7/1000 | Loss: 0.00001008
Iteration 8/1000 | Loss: 0.00000980
Iteration 9/1000 | Loss: 0.00000973
Iteration 10/1000 | Loss: 0.00000945
Iteration 11/1000 | Loss: 0.00000929
Iteration 12/1000 | Loss: 0.00000923
Iteration 13/1000 | Loss: 0.00000920
Iteration 14/1000 | Loss: 0.00000912
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000910
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000908
Iteration 19/1000 | Loss: 0.00000908
Iteration 20/1000 | Loss: 0.00000907
Iteration 21/1000 | Loss: 0.00000906
Iteration 22/1000 | Loss: 0.00000906
Iteration 23/1000 | Loss: 0.00000905
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000901
Iteration 26/1000 | Loss: 0.00000900
Iteration 27/1000 | Loss: 0.00000899
Iteration 28/1000 | Loss: 0.00000899
Iteration 29/1000 | Loss: 0.00000899
Iteration 30/1000 | Loss: 0.00000898
Iteration 31/1000 | Loss: 0.00000898
Iteration 32/1000 | Loss: 0.00000898
Iteration 33/1000 | Loss: 0.00000897
Iteration 34/1000 | Loss: 0.00000897
Iteration 35/1000 | Loss: 0.00000896
Iteration 36/1000 | Loss: 0.00000895
Iteration 37/1000 | Loss: 0.00000895
Iteration 38/1000 | Loss: 0.00000894
Iteration 39/1000 | Loss: 0.00000894
Iteration 40/1000 | Loss: 0.00000894
Iteration 41/1000 | Loss: 0.00000894
Iteration 42/1000 | Loss: 0.00000893
Iteration 43/1000 | Loss: 0.00000892
Iteration 44/1000 | Loss: 0.00000892
Iteration 45/1000 | Loss: 0.00000891
Iteration 46/1000 | Loss: 0.00000891
Iteration 47/1000 | Loss: 0.00000891
Iteration 48/1000 | Loss: 0.00000890
Iteration 49/1000 | Loss: 0.00000890
Iteration 50/1000 | Loss: 0.00000890
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000889
Iteration 53/1000 | Loss: 0.00000889
Iteration 54/1000 | Loss: 0.00000888
Iteration 55/1000 | Loss: 0.00000887
Iteration 56/1000 | Loss: 0.00000887
Iteration 57/1000 | Loss: 0.00000886
Iteration 58/1000 | Loss: 0.00000886
Iteration 59/1000 | Loss: 0.00000883
Iteration 60/1000 | Loss: 0.00000879
Iteration 61/1000 | Loss: 0.00000878
Iteration 62/1000 | Loss: 0.00000878
Iteration 63/1000 | Loss: 0.00000878
Iteration 64/1000 | Loss: 0.00000877
Iteration 65/1000 | Loss: 0.00000877
Iteration 66/1000 | Loss: 0.00000877
Iteration 67/1000 | Loss: 0.00000876
Iteration 68/1000 | Loss: 0.00000876
Iteration 69/1000 | Loss: 0.00000875
Iteration 70/1000 | Loss: 0.00000875
Iteration 71/1000 | Loss: 0.00000874
Iteration 72/1000 | Loss: 0.00000874
Iteration 73/1000 | Loss: 0.00000874
Iteration 74/1000 | Loss: 0.00000874
Iteration 75/1000 | Loss: 0.00000874
Iteration 76/1000 | Loss: 0.00000874
Iteration 77/1000 | Loss: 0.00000874
Iteration 78/1000 | Loss: 0.00000874
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000873
Iteration 81/1000 | Loss: 0.00000873
Iteration 82/1000 | Loss: 0.00000873
Iteration 83/1000 | Loss: 0.00000873
Iteration 84/1000 | Loss: 0.00000872
Iteration 85/1000 | Loss: 0.00000872
Iteration 86/1000 | Loss: 0.00000871
Iteration 87/1000 | Loss: 0.00000871
Iteration 88/1000 | Loss: 0.00000871
Iteration 89/1000 | Loss: 0.00000871
Iteration 90/1000 | Loss: 0.00000871
Iteration 91/1000 | Loss: 0.00000871
Iteration 92/1000 | Loss: 0.00000870
Iteration 93/1000 | Loss: 0.00000870
Iteration 94/1000 | Loss: 0.00000870
Iteration 95/1000 | Loss: 0.00000870
Iteration 96/1000 | Loss: 0.00000869
Iteration 97/1000 | Loss: 0.00000869
Iteration 98/1000 | Loss: 0.00000869
Iteration 99/1000 | Loss: 0.00000868
Iteration 100/1000 | Loss: 0.00000868
Iteration 101/1000 | Loss: 0.00000868
Iteration 102/1000 | Loss: 0.00000867
Iteration 103/1000 | Loss: 0.00000867
Iteration 104/1000 | Loss: 0.00000867
Iteration 105/1000 | Loss: 0.00000867
Iteration 106/1000 | Loss: 0.00000866
Iteration 107/1000 | Loss: 0.00000866
Iteration 108/1000 | Loss: 0.00000866
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000865
Iteration 111/1000 | Loss: 0.00000865
Iteration 112/1000 | Loss: 0.00000865
Iteration 113/1000 | Loss: 0.00000865
Iteration 114/1000 | Loss: 0.00000865
Iteration 115/1000 | Loss: 0.00000864
Iteration 116/1000 | Loss: 0.00000864
Iteration 117/1000 | Loss: 0.00000864
Iteration 118/1000 | Loss: 0.00000864
Iteration 119/1000 | Loss: 0.00000864
Iteration 120/1000 | Loss: 0.00000864
Iteration 121/1000 | Loss: 0.00000864
Iteration 122/1000 | Loss: 0.00000863
Iteration 123/1000 | Loss: 0.00000863
Iteration 124/1000 | Loss: 0.00000863
Iteration 125/1000 | Loss: 0.00000863
Iteration 126/1000 | Loss: 0.00000863
Iteration 127/1000 | Loss: 0.00000863
Iteration 128/1000 | Loss: 0.00000863
Iteration 129/1000 | Loss: 0.00000863
Iteration 130/1000 | Loss: 0.00000862
Iteration 131/1000 | Loss: 0.00000862
Iteration 132/1000 | Loss: 0.00000862
Iteration 133/1000 | Loss: 0.00000862
Iteration 134/1000 | Loss: 0.00000861
Iteration 135/1000 | Loss: 0.00000861
Iteration 136/1000 | Loss: 0.00000861
Iteration 137/1000 | Loss: 0.00000861
Iteration 138/1000 | Loss: 0.00000861
Iteration 139/1000 | Loss: 0.00000861
Iteration 140/1000 | Loss: 0.00000861
Iteration 141/1000 | Loss: 0.00000861
Iteration 142/1000 | Loss: 0.00000861
Iteration 143/1000 | Loss: 0.00000861
Iteration 144/1000 | Loss: 0.00000861
Iteration 145/1000 | Loss: 0.00000861
Iteration 146/1000 | Loss: 0.00000860
Iteration 147/1000 | Loss: 0.00000860
Iteration 148/1000 | Loss: 0.00000860
Iteration 149/1000 | Loss: 0.00000860
Iteration 150/1000 | Loss: 0.00000860
Iteration 151/1000 | Loss: 0.00000860
Iteration 152/1000 | Loss: 0.00000860
Iteration 153/1000 | Loss: 0.00000860
Iteration 154/1000 | Loss: 0.00000860
Iteration 155/1000 | Loss: 0.00000860
Iteration 156/1000 | Loss: 0.00000859
Iteration 157/1000 | Loss: 0.00000859
Iteration 158/1000 | Loss: 0.00000859
Iteration 159/1000 | Loss: 0.00000859
Iteration 160/1000 | Loss: 0.00000859
Iteration 161/1000 | Loss: 0.00000859
Iteration 162/1000 | Loss: 0.00000859
Iteration 163/1000 | Loss: 0.00000859
Iteration 164/1000 | Loss: 0.00000859
Iteration 165/1000 | Loss: 0.00000859
Iteration 166/1000 | Loss: 0.00000859
Iteration 167/1000 | Loss: 0.00000858
Iteration 168/1000 | Loss: 0.00000858
Iteration 169/1000 | Loss: 0.00000858
Iteration 170/1000 | Loss: 0.00000858
Iteration 171/1000 | Loss: 0.00000858
Iteration 172/1000 | Loss: 0.00000858
Iteration 173/1000 | Loss: 0.00000858
Iteration 174/1000 | Loss: 0.00000858
Iteration 175/1000 | Loss: 0.00000858
Iteration 176/1000 | Loss: 0.00000858
Iteration 177/1000 | Loss: 0.00000858
Iteration 178/1000 | Loss: 0.00000858
Iteration 179/1000 | Loss: 0.00000857
Iteration 180/1000 | Loss: 0.00000857
Iteration 181/1000 | Loss: 0.00000857
Iteration 182/1000 | Loss: 0.00000857
Iteration 183/1000 | Loss: 0.00000857
Iteration 184/1000 | Loss: 0.00000857
Iteration 185/1000 | Loss: 0.00000857
Iteration 186/1000 | Loss: 0.00000857
Iteration 187/1000 | Loss: 0.00000857
Iteration 188/1000 | Loss: 0.00000857
Iteration 189/1000 | Loss: 0.00000857
Iteration 190/1000 | Loss: 0.00000857
Iteration 191/1000 | Loss: 0.00000857
Iteration 192/1000 | Loss: 0.00000857
Iteration 193/1000 | Loss: 0.00000857
Iteration 194/1000 | Loss: 0.00000857
Iteration 195/1000 | Loss: 0.00000857
Iteration 196/1000 | Loss: 0.00000857
Iteration 197/1000 | Loss: 0.00000857
Iteration 198/1000 | Loss: 0.00000857
Iteration 199/1000 | Loss: 0.00000857
Iteration 200/1000 | Loss: 0.00000857
Iteration 201/1000 | Loss: 0.00000857
Iteration 202/1000 | Loss: 0.00000857
Iteration 203/1000 | Loss: 0.00000857
Iteration 204/1000 | Loss: 0.00000857
Iteration 205/1000 | Loss: 0.00000857
Iteration 206/1000 | Loss: 0.00000857
Iteration 207/1000 | Loss: 0.00000857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [8.567953955207486e-06, 8.567953955207486e-06, 8.567953955207486e-06, 8.567953955207486e-06, 8.567953955207486e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.567953955207486e-06

Optimization complete. Final v2v error: 2.5126869678497314 mm

Highest mean error: 2.6881754398345947 mm for frame 79

Lowest mean error: 2.383037567138672 mm for frame 165

Saving results

Total time: 40.46786904335022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852910
Iteration 2/25 | Loss: 0.00137273
Iteration 3/25 | Loss: 0.00123480
Iteration 4/25 | Loss: 0.00122418
Iteration 5/25 | Loss: 0.00122160
Iteration 6/25 | Loss: 0.00122160
Iteration 7/25 | Loss: 0.00122160
Iteration 8/25 | Loss: 0.00122156
Iteration 9/25 | Loss: 0.00122156
Iteration 10/25 | Loss: 0.00122156
Iteration 11/25 | Loss: 0.00122156
Iteration 12/25 | Loss: 0.00122156
Iteration 13/25 | Loss: 0.00122156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012215649476274848, 0.0012215649476274848, 0.0012215649476274848, 0.0012215649476274848, 0.0012215649476274848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012215649476274848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30416203
Iteration 2/25 | Loss: 0.00078802
Iteration 3/25 | Loss: 0.00078801
Iteration 4/25 | Loss: 0.00078801
Iteration 5/25 | Loss: 0.00078801
Iteration 6/25 | Loss: 0.00078801
Iteration 7/25 | Loss: 0.00078801
Iteration 8/25 | Loss: 0.00078801
Iteration 9/25 | Loss: 0.00078801
Iteration 10/25 | Loss: 0.00078801
Iteration 11/25 | Loss: 0.00078801
Iteration 12/25 | Loss: 0.00078801
Iteration 13/25 | Loss: 0.00078801
Iteration 14/25 | Loss: 0.00078801
Iteration 15/25 | Loss: 0.00078801
Iteration 16/25 | Loss: 0.00078801
Iteration 17/25 | Loss: 0.00078801
Iteration 18/25 | Loss: 0.00078801
Iteration 19/25 | Loss: 0.00078801
Iteration 20/25 | Loss: 0.00078801
Iteration 21/25 | Loss: 0.00078801
Iteration 22/25 | Loss: 0.00078801
Iteration 23/25 | Loss: 0.00078801
Iteration 24/25 | Loss: 0.00078801
Iteration 25/25 | Loss: 0.00078801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078801
Iteration 2/1000 | Loss: 0.00007285
Iteration 3/1000 | Loss: 0.00004028
Iteration 4/1000 | Loss: 0.00003214
Iteration 5/1000 | Loss: 0.00002968
Iteration 6/1000 | Loss: 0.00002815
Iteration 7/1000 | Loss: 0.00002716
Iteration 8/1000 | Loss: 0.00002633
Iteration 9/1000 | Loss: 0.00002588
Iteration 10/1000 | Loss: 0.00002559
Iteration 11/1000 | Loss: 0.00002539
Iteration 12/1000 | Loss: 0.00002522
Iteration 13/1000 | Loss: 0.00002516
Iteration 14/1000 | Loss: 0.00002502
Iteration 15/1000 | Loss: 0.00002500
Iteration 16/1000 | Loss: 0.00002490
Iteration 17/1000 | Loss: 0.00002484
Iteration 18/1000 | Loss: 0.00002481
Iteration 19/1000 | Loss: 0.00002481
Iteration 20/1000 | Loss: 0.00002480
Iteration 21/1000 | Loss: 0.00002480
Iteration 22/1000 | Loss: 0.00002478
Iteration 23/1000 | Loss: 0.00002477
Iteration 24/1000 | Loss: 0.00002474
Iteration 25/1000 | Loss: 0.00002473
Iteration 26/1000 | Loss: 0.00002472
Iteration 27/1000 | Loss: 0.00002472
Iteration 28/1000 | Loss: 0.00002472
Iteration 29/1000 | Loss: 0.00002472
Iteration 30/1000 | Loss: 0.00002472
Iteration 31/1000 | Loss: 0.00002470
Iteration 32/1000 | Loss: 0.00002470
Iteration 33/1000 | Loss: 0.00002469
Iteration 34/1000 | Loss: 0.00002469
Iteration 35/1000 | Loss: 0.00002469
Iteration 36/1000 | Loss: 0.00002469
Iteration 37/1000 | Loss: 0.00002469
Iteration 38/1000 | Loss: 0.00002468
Iteration 39/1000 | Loss: 0.00002467
Iteration 40/1000 | Loss: 0.00002467
Iteration 41/1000 | Loss: 0.00002467
Iteration 42/1000 | Loss: 0.00002467
Iteration 43/1000 | Loss: 0.00002467
Iteration 44/1000 | Loss: 0.00002467
Iteration 45/1000 | Loss: 0.00002467
Iteration 46/1000 | Loss: 0.00002466
Iteration 47/1000 | Loss: 0.00002466
Iteration 48/1000 | Loss: 0.00002466
Iteration 49/1000 | Loss: 0.00002466
Iteration 50/1000 | Loss: 0.00002466
Iteration 51/1000 | Loss: 0.00002466
Iteration 52/1000 | Loss: 0.00002466
Iteration 53/1000 | Loss: 0.00002466
Iteration 54/1000 | Loss: 0.00002466
Iteration 55/1000 | Loss: 0.00002466
Iteration 56/1000 | Loss: 0.00002465
Iteration 57/1000 | Loss: 0.00002465
Iteration 58/1000 | Loss: 0.00002465
Iteration 59/1000 | Loss: 0.00002465
Iteration 60/1000 | Loss: 0.00002465
Iteration 61/1000 | Loss: 0.00002464
Iteration 62/1000 | Loss: 0.00002464
Iteration 63/1000 | Loss: 0.00002464
Iteration 64/1000 | Loss: 0.00002464
Iteration 65/1000 | Loss: 0.00002464
Iteration 66/1000 | Loss: 0.00002464
Iteration 67/1000 | Loss: 0.00002464
Iteration 68/1000 | Loss: 0.00002464
Iteration 69/1000 | Loss: 0.00002464
Iteration 70/1000 | Loss: 0.00002464
Iteration 71/1000 | Loss: 0.00002464
Iteration 72/1000 | Loss: 0.00002464
Iteration 73/1000 | Loss: 0.00002464
Iteration 74/1000 | Loss: 0.00002463
Iteration 75/1000 | Loss: 0.00002463
Iteration 76/1000 | Loss: 0.00002462
Iteration 77/1000 | Loss: 0.00002462
Iteration 78/1000 | Loss: 0.00002462
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00002462
Iteration 81/1000 | Loss: 0.00002462
Iteration 82/1000 | Loss: 0.00002461
Iteration 83/1000 | Loss: 0.00002461
Iteration 84/1000 | Loss: 0.00002461
Iteration 85/1000 | Loss: 0.00002461
Iteration 86/1000 | Loss: 0.00002461
Iteration 87/1000 | Loss: 0.00002461
Iteration 88/1000 | Loss: 0.00002461
Iteration 89/1000 | Loss: 0.00002461
Iteration 90/1000 | Loss: 0.00002460
Iteration 91/1000 | Loss: 0.00002460
Iteration 92/1000 | Loss: 0.00002460
Iteration 93/1000 | Loss: 0.00002459
Iteration 94/1000 | Loss: 0.00002459
Iteration 95/1000 | Loss: 0.00002459
Iteration 96/1000 | Loss: 0.00002459
Iteration 97/1000 | Loss: 0.00002458
Iteration 98/1000 | Loss: 0.00002457
Iteration 99/1000 | Loss: 0.00002457
Iteration 100/1000 | Loss: 0.00002457
Iteration 101/1000 | Loss: 0.00002457
Iteration 102/1000 | Loss: 0.00002457
Iteration 103/1000 | Loss: 0.00002457
Iteration 104/1000 | Loss: 0.00002457
Iteration 105/1000 | Loss: 0.00002457
Iteration 106/1000 | Loss: 0.00002457
Iteration 107/1000 | Loss: 0.00002457
Iteration 108/1000 | Loss: 0.00002456
Iteration 109/1000 | Loss: 0.00002456
Iteration 110/1000 | Loss: 0.00002456
Iteration 111/1000 | Loss: 0.00002456
Iteration 112/1000 | Loss: 0.00002456
Iteration 113/1000 | Loss: 0.00002456
Iteration 114/1000 | Loss: 0.00002456
Iteration 115/1000 | Loss: 0.00002455
Iteration 116/1000 | Loss: 0.00002455
Iteration 117/1000 | Loss: 0.00002455
Iteration 118/1000 | Loss: 0.00002455
Iteration 119/1000 | Loss: 0.00002455
Iteration 120/1000 | Loss: 0.00002455
Iteration 121/1000 | Loss: 0.00002455
Iteration 122/1000 | Loss: 0.00002455
Iteration 123/1000 | Loss: 0.00002455
Iteration 124/1000 | Loss: 0.00002455
Iteration 125/1000 | Loss: 0.00002455
Iteration 126/1000 | Loss: 0.00002455
Iteration 127/1000 | Loss: 0.00002455
Iteration 128/1000 | Loss: 0.00002455
Iteration 129/1000 | Loss: 0.00002455
Iteration 130/1000 | Loss: 0.00002455
Iteration 131/1000 | Loss: 0.00002455
Iteration 132/1000 | Loss: 0.00002455
Iteration 133/1000 | Loss: 0.00002455
Iteration 134/1000 | Loss: 0.00002455
Iteration 135/1000 | Loss: 0.00002455
Iteration 136/1000 | Loss: 0.00002455
Iteration 137/1000 | Loss: 0.00002455
Iteration 138/1000 | Loss: 0.00002455
Iteration 139/1000 | Loss: 0.00002455
Iteration 140/1000 | Loss: 0.00002455
Iteration 141/1000 | Loss: 0.00002455
Iteration 142/1000 | Loss: 0.00002455
Iteration 143/1000 | Loss: 0.00002455
Iteration 144/1000 | Loss: 0.00002455
Iteration 145/1000 | Loss: 0.00002455
Iteration 146/1000 | Loss: 0.00002455
Iteration 147/1000 | Loss: 0.00002455
Iteration 148/1000 | Loss: 0.00002455
Iteration 149/1000 | Loss: 0.00002455
Iteration 150/1000 | Loss: 0.00002455
Iteration 151/1000 | Loss: 0.00002455
Iteration 152/1000 | Loss: 0.00002455
Iteration 153/1000 | Loss: 0.00002455
Iteration 154/1000 | Loss: 0.00002455
Iteration 155/1000 | Loss: 0.00002455
Iteration 156/1000 | Loss: 0.00002455
Iteration 157/1000 | Loss: 0.00002455
Iteration 158/1000 | Loss: 0.00002455
Iteration 159/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.454516288707964e-05, 2.454516288707964e-05, 2.454516288707964e-05, 2.454516288707964e-05, 2.454516288707964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.454516288707964e-05

Optimization complete. Final v2v error: 3.8808162212371826 mm

Highest mean error: 5.535216331481934 mm for frame 101

Lowest mean error: 2.856933355331421 mm for frame 141

Saving results

Total time: 41.24667549133301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718326
Iteration 2/25 | Loss: 0.00121651
Iteration 3/25 | Loss: 0.00112424
Iteration 4/25 | Loss: 0.00111359
Iteration 5/25 | Loss: 0.00111154
Iteration 6/25 | Loss: 0.00111154
Iteration 7/25 | Loss: 0.00111154
Iteration 8/25 | Loss: 0.00111154
Iteration 9/25 | Loss: 0.00111154
Iteration 10/25 | Loss: 0.00111154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011115431552752852, 0.0011115431552752852, 0.0011115431552752852, 0.0011115431552752852, 0.0011115431552752852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011115431552752852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47913373
Iteration 2/25 | Loss: 0.00079457
Iteration 3/25 | Loss: 0.00079456
Iteration 4/25 | Loss: 0.00079456
Iteration 5/25 | Loss: 0.00079456
Iteration 6/25 | Loss: 0.00079456
Iteration 7/25 | Loss: 0.00079456
Iteration 8/25 | Loss: 0.00079456
Iteration 9/25 | Loss: 0.00079456
Iteration 10/25 | Loss: 0.00079456
Iteration 11/25 | Loss: 0.00079456
Iteration 12/25 | Loss: 0.00079456
Iteration 13/25 | Loss: 0.00079456
Iteration 14/25 | Loss: 0.00079456
Iteration 15/25 | Loss: 0.00079456
Iteration 16/25 | Loss: 0.00079456
Iteration 17/25 | Loss: 0.00079456
Iteration 18/25 | Loss: 0.00079456
Iteration 19/25 | Loss: 0.00079456
Iteration 20/25 | Loss: 0.00079456
Iteration 21/25 | Loss: 0.00079456
Iteration 22/25 | Loss: 0.00079456
Iteration 23/25 | Loss: 0.00079456
Iteration 24/25 | Loss: 0.00079456
Iteration 25/25 | Loss: 0.00079456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079456
Iteration 2/1000 | Loss: 0.00002304
Iteration 3/1000 | Loss: 0.00001417
Iteration 4/1000 | Loss: 0.00001275
Iteration 5/1000 | Loss: 0.00001211
Iteration 6/1000 | Loss: 0.00001166
Iteration 7/1000 | Loss: 0.00001134
Iteration 8/1000 | Loss: 0.00001129
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001103
Iteration 11/1000 | Loss: 0.00001081
Iteration 12/1000 | Loss: 0.00001064
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001046
Iteration 15/1000 | Loss: 0.00001041
Iteration 16/1000 | Loss: 0.00001036
Iteration 17/1000 | Loss: 0.00001032
Iteration 18/1000 | Loss: 0.00001029
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001024
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001021
Iteration 24/1000 | Loss: 0.00001021
Iteration 25/1000 | Loss: 0.00001020
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001012
Iteration 29/1000 | Loss: 0.00001010
Iteration 30/1000 | Loss: 0.00001010
Iteration 31/1000 | Loss: 0.00001010
Iteration 32/1000 | Loss: 0.00001010
Iteration 33/1000 | Loss: 0.00001010
Iteration 34/1000 | Loss: 0.00001009
Iteration 35/1000 | Loss: 0.00001009
Iteration 36/1000 | Loss: 0.00001006
Iteration 37/1000 | Loss: 0.00001004
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001003
Iteration 40/1000 | Loss: 0.00001002
Iteration 41/1000 | Loss: 0.00001001
Iteration 42/1000 | Loss: 0.00001001
Iteration 43/1000 | Loss: 0.00001000
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00001000
Iteration 46/1000 | Loss: 0.00001000
Iteration 47/1000 | Loss: 0.00000999
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000998
Iteration 50/1000 | Loss: 0.00000997
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00000997
Iteration 53/1000 | Loss: 0.00000996
Iteration 54/1000 | Loss: 0.00000996
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000996
Iteration 57/1000 | Loss: 0.00000996
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000995
Iteration 60/1000 | Loss: 0.00000992
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000987
Iteration 64/1000 | Loss: 0.00000987
Iteration 65/1000 | Loss: 0.00000986
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000985
Iteration 71/1000 | Loss: 0.00000985
Iteration 72/1000 | Loss: 0.00000985
Iteration 73/1000 | Loss: 0.00000985
Iteration 74/1000 | Loss: 0.00000985
Iteration 75/1000 | Loss: 0.00000984
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000983
Iteration 79/1000 | Loss: 0.00000983
Iteration 80/1000 | Loss: 0.00000983
Iteration 81/1000 | Loss: 0.00000983
Iteration 82/1000 | Loss: 0.00000982
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000980
Iteration 93/1000 | Loss: 0.00000980
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000978
Iteration 110/1000 | Loss: 0.00000978
Iteration 111/1000 | Loss: 0.00000978
Iteration 112/1000 | Loss: 0.00000978
Iteration 113/1000 | Loss: 0.00000978
Iteration 114/1000 | Loss: 0.00000978
Iteration 115/1000 | Loss: 0.00000978
Iteration 116/1000 | Loss: 0.00000978
Iteration 117/1000 | Loss: 0.00000978
Iteration 118/1000 | Loss: 0.00000978
Iteration 119/1000 | Loss: 0.00000977
Iteration 120/1000 | Loss: 0.00000977
Iteration 121/1000 | Loss: 0.00000977
Iteration 122/1000 | Loss: 0.00000977
Iteration 123/1000 | Loss: 0.00000977
Iteration 124/1000 | Loss: 0.00000977
Iteration 125/1000 | Loss: 0.00000977
Iteration 126/1000 | Loss: 0.00000977
Iteration 127/1000 | Loss: 0.00000977
Iteration 128/1000 | Loss: 0.00000977
Iteration 129/1000 | Loss: 0.00000977
Iteration 130/1000 | Loss: 0.00000977
Iteration 131/1000 | Loss: 0.00000977
Iteration 132/1000 | Loss: 0.00000976
Iteration 133/1000 | Loss: 0.00000976
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000976
Iteration 138/1000 | Loss: 0.00000976
Iteration 139/1000 | Loss: 0.00000976
Iteration 140/1000 | Loss: 0.00000976
Iteration 141/1000 | Loss: 0.00000976
Iteration 142/1000 | Loss: 0.00000976
Iteration 143/1000 | Loss: 0.00000976
Iteration 144/1000 | Loss: 0.00000976
Iteration 145/1000 | Loss: 0.00000976
Iteration 146/1000 | Loss: 0.00000976
Iteration 147/1000 | Loss: 0.00000976
Iteration 148/1000 | Loss: 0.00000975
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000975
Iteration 152/1000 | Loss: 0.00000975
Iteration 153/1000 | Loss: 0.00000975
Iteration 154/1000 | Loss: 0.00000975
Iteration 155/1000 | Loss: 0.00000975
Iteration 156/1000 | Loss: 0.00000975
Iteration 157/1000 | Loss: 0.00000975
Iteration 158/1000 | Loss: 0.00000975
Iteration 159/1000 | Loss: 0.00000975
Iteration 160/1000 | Loss: 0.00000975
Iteration 161/1000 | Loss: 0.00000975
Iteration 162/1000 | Loss: 0.00000975
Iteration 163/1000 | Loss: 0.00000975
Iteration 164/1000 | Loss: 0.00000974
Iteration 165/1000 | Loss: 0.00000974
Iteration 166/1000 | Loss: 0.00000974
Iteration 167/1000 | Loss: 0.00000974
Iteration 168/1000 | Loss: 0.00000974
Iteration 169/1000 | Loss: 0.00000974
Iteration 170/1000 | Loss: 0.00000974
Iteration 171/1000 | Loss: 0.00000974
Iteration 172/1000 | Loss: 0.00000974
Iteration 173/1000 | Loss: 0.00000974
Iteration 174/1000 | Loss: 0.00000974
Iteration 175/1000 | Loss: 0.00000974
Iteration 176/1000 | Loss: 0.00000974
Iteration 177/1000 | Loss: 0.00000974
Iteration 178/1000 | Loss: 0.00000974
Iteration 179/1000 | Loss: 0.00000974
Iteration 180/1000 | Loss: 0.00000974
Iteration 181/1000 | Loss: 0.00000974
Iteration 182/1000 | Loss: 0.00000974
Iteration 183/1000 | Loss: 0.00000974
Iteration 184/1000 | Loss: 0.00000974
Iteration 185/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [9.741583198774606e-06, 9.741583198774606e-06, 9.741583198774606e-06, 9.741583198774606e-06, 9.741583198774606e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.741583198774606e-06

Optimization complete. Final v2v error: 2.675856113433838 mm

Highest mean error: 2.9897148609161377 mm for frame 20

Lowest mean error: 2.4362964630126953 mm for frame 45

Saving results

Total time: 41.461918354034424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00542510
Iteration 2/25 | Loss: 0.00135707
Iteration 3/25 | Loss: 0.00114747
Iteration 4/25 | Loss: 0.00112522
Iteration 5/25 | Loss: 0.00112277
Iteration 6/25 | Loss: 0.00112021
Iteration 7/25 | Loss: 0.00112011
Iteration 8/25 | Loss: 0.00112021
Iteration 9/25 | Loss: 0.00112014
Iteration 10/25 | Loss: 0.00111965
Iteration 11/25 | Loss: 0.00111964
Iteration 12/25 | Loss: 0.00111964
Iteration 13/25 | Loss: 0.00111964
Iteration 14/25 | Loss: 0.00111964
Iteration 15/25 | Loss: 0.00111964
Iteration 16/25 | Loss: 0.00111964
Iteration 17/25 | Loss: 0.00111964
Iteration 18/25 | Loss: 0.00111964
Iteration 19/25 | Loss: 0.00111964
Iteration 20/25 | Loss: 0.00111964
Iteration 21/25 | Loss: 0.00111964
Iteration 22/25 | Loss: 0.00111964
Iteration 23/25 | Loss: 0.00111964
Iteration 24/25 | Loss: 0.00111964
Iteration 25/25 | Loss: 0.00111964

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.91660595
Iteration 2/25 | Loss: 0.00082548
Iteration 3/25 | Loss: 0.00078575
Iteration 4/25 | Loss: 0.00078575
Iteration 5/25 | Loss: 0.00078575
Iteration 6/25 | Loss: 0.00078575
Iteration 7/25 | Loss: 0.00078575
Iteration 8/25 | Loss: 0.00078575
Iteration 9/25 | Loss: 0.00078575
Iteration 10/25 | Loss: 0.00078575
Iteration 11/25 | Loss: 0.00078575
Iteration 12/25 | Loss: 0.00078575
Iteration 13/25 | Loss: 0.00078575
Iteration 14/25 | Loss: 0.00078575
Iteration 15/25 | Loss: 0.00078575
Iteration 16/25 | Loss: 0.00078575
Iteration 17/25 | Loss: 0.00078575
Iteration 18/25 | Loss: 0.00078575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007857517339289188, 0.0007857517339289188, 0.0007857517339289188, 0.0007857517339289188, 0.0007857517339289188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007857517339289188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078575
Iteration 2/1000 | Loss: 0.00006980
Iteration 3/1000 | Loss: 0.00002880
Iteration 4/1000 | Loss: 0.00006253
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001312
Iteration 8/1000 | Loss: 0.00002952
Iteration 9/1000 | Loss: 0.00001275
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001253
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001235
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001210
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001195
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00002982
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001190
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001187
Iteration 53/1000 | Loss: 0.00001187
Iteration 54/1000 | Loss: 0.00001187
Iteration 55/1000 | Loss: 0.00001187
Iteration 56/1000 | Loss: 0.00001187
Iteration 57/1000 | Loss: 0.00001187
Iteration 58/1000 | Loss: 0.00001186
Iteration 59/1000 | Loss: 0.00001186
Iteration 60/1000 | Loss: 0.00001186
Iteration 61/1000 | Loss: 0.00001186
Iteration 62/1000 | Loss: 0.00001186
Iteration 63/1000 | Loss: 0.00001186
Iteration 64/1000 | Loss: 0.00001186
Iteration 65/1000 | Loss: 0.00001186
Iteration 66/1000 | Loss: 0.00001186
Iteration 67/1000 | Loss: 0.00001185
Iteration 68/1000 | Loss: 0.00001185
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001181
Iteration 71/1000 | Loss: 0.00001181
Iteration 72/1000 | Loss: 0.00001180
Iteration 73/1000 | Loss: 0.00001180
Iteration 74/1000 | Loss: 0.00001180
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001179
Iteration 80/1000 | Loss: 0.00001179
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001179
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001178
Iteration 96/1000 | Loss: 0.00001178
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001175
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001175
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001174
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001173
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001172
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001170
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001169
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001168
Iteration 155/1000 | Loss: 0.00001168
Iteration 156/1000 | Loss: 0.00001168
Iteration 157/1000 | Loss: 0.00001168
Iteration 158/1000 | Loss: 0.00001168
Iteration 159/1000 | Loss: 0.00001168
Iteration 160/1000 | Loss: 0.00001168
Iteration 161/1000 | Loss: 0.00001168
Iteration 162/1000 | Loss: 0.00001168
Iteration 163/1000 | Loss: 0.00001168
Iteration 164/1000 | Loss: 0.00001167
Iteration 165/1000 | Loss: 0.00001167
Iteration 166/1000 | Loss: 0.00001167
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001166
Iteration 170/1000 | Loss: 0.00001166
Iteration 171/1000 | Loss: 0.00001166
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001166
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001165
Iteration 178/1000 | Loss: 0.00001165
Iteration 179/1000 | Loss: 0.00001165
Iteration 180/1000 | Loss: 0.00001165
Iteration 181/1000 | Loss: 0.00001165
Iteration 182/1000 | Loss: 0.00001165
Iteration 183/1000 | Loss: 0.00001165
Iteration 184/1000 | Loss: 0.00001165
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001165
Iteration 189/1000 | Loss: 0.00001165
Iteration 190/1000 | Loss: 0.00001165
Iteration 191/1000 | Loss: 0.00001165
Iteration 192/1000 | Loss: 0.00001165
Iteration 193/1000 | Loss: 0.00001165
Iteration 194/1000 | Loss: 0.00001165
Iteration 195/1000 | Loss: 0.00001165
Iteration 196/1000 | Loss: 0.00001165
Iteration 197/1000 | Loss: 0.00001165
Iteration 198/1000 | Loss: 0.00001165
Iteration 199/1000 | Loss: 0.00001165
Iteration 200/1000 | Loss: 0.00001165
Iteration 201/1000 | Loss: 0.00001165
Iteration 202/1000 | Loss: 0.00001165
Iteration 203/1000 | Loss: 0.00001165
Iteration 204/1000 | Loss: 0.00001165
Iteration 205/1000 | Loss: 0.00001165
Iteration 206/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.165073990705423e-05, 1.165073990705423e-05, 1.165073990705423e-05, 1.165073990705423e-05, 1.165073990705423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.165073990705423e-05

Optimization complete. Final v2v error: 2.8928277492523193 mm

Highest mean error: 3.252237319946289 mm for frame 195

Lowest mean error: 2.6493752002716064 mm for frame 176

Saving results

Total time: 60.29097318649292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047765
Iteration 2/25 | Loss: 0.01047765
Iteration 3/25 | Loss: 0.01047765
Iteration 4/25 | Loss: 0.01047765
Iteration 5/25 | Loss: 0.01047765
Iteration 6/25 | Loss: 0.01047765
Iteration 7/25 | Loss: 0.01047765
Iteration 8/25 | Loss: 0.01047765
Iteration 9/25 | Loss: 0.01047765
Iteration 10/25 | Loss: 0.01047765
Iteration 11/25 | Loss: 0.01047765
Iteration 12/25 | Loss: 0.01047764
Iteration 13/25 | Loss: 0.01047764
Iteration 14/25 | Loss: 0.01047764
Iteration 15/25 | Loss: 0.01047764
Iteration 16/25 | Loss: 0.01047764
Iteration 17/25 | Loss: 0.01047764
Iteration 18/25 | Loss: 0.01047764
Iteration 19/25 | Loss: 0.01047764
Iteration 20/25 | Loss: 0.01047764
Iteration 21/25 | Loss: 0.01047764
Iteration 22/25 | Loss: 0.01047764
Iteration 23/25 | Loss: 0.01047764
Iteration 24/25 | Loss: 0.01047764
Iteration 25/25 | Loss: 0.01047764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46131980
Iteration 2/25 | Loss: 0.12655762
Iteration 3/25 | Loss: 0.12643310
Iteration 4/25 | Loss: 0.12598848
Iteration 5/25 | Loss: 0.12598845
Iteration 6/25 | Loss: 0.12598845
Iteration 7/25 | Loss: 0.12598842
Iteration 8/25 | Loss: 0.12598841
Iteration 9/25 | Loss: 0.12598841
Iteration 10/25 | Loss: 0.12598839
Iteration 11/25 | Loss: 0.12598838
Iteration 12/25 | Loss: 0.12598835
Iteration 13/25 | Loss: 0.12598833
Iteration 14/25 | Loss: 0.12598833
Iteration 15/25 | Loss: 0.12598832
Iteration 16/25 | Loss: 0.12598829
Iteration 17/25 | Loss: 0.12598829
Iteration 18/25 | Loss: 0.12598827
Iteration 19/25 | Loss: 0.12598826
Iteration 20/25 | Loss: 0.12598826
Iteration 21/25 | Loss: 0.12598825
Iteration 22/25 | Loss: 0.12598822
Iteration 23/25 | Loss: 0.12598820
Iteration 24/25 | Loss: 0.12598820
Iteration 25/25 | Loss: 0.12598819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12598099
Iteration 2/1000 | Loss: 0.00074201
Iteration 3/1000 | Loss: 0.00024403
Iteration 4/1000 | Loss: 0.00017808
Iteration 5/1000 | Loss: 0.00013391
Iteration 6/1000 | Loss: 0.00006788
Iteration 7/1000 | Loss: 0.00003129
Iteration 8/1000 | Loss: 0.00003384
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00003874
Iteration 11/1000 | Loss: 0.00002663
Iteration 12/1000 | Loss: 0.00008802
Iteration 13/1000 | Loss: 0.00022631
Iteration 14/1000 | Loss: 0.00009503
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00002687
Iteration 17/1000 | Loss: 0.00052991
Iteration 18/1000 | Loss: 0.00028093
Iteration 19/1000 | Loss: 0.00004879
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00007103
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00002436
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00005497
Iteration 27/1000 | Loss: 0.00012479
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00002887
Iteration 30/1000 | Loss: 0.00005528
Iteration 31/1000 | Loss: 0.00008003
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00003014
Iteration 34/1000 | Loss: 0.00001752
Iteration 35/1000 | Loss: 0.00000972
Iteration 36/1000 | Loss: 0.00000961
Iteration 37/1000 | Loss: 0.00000951
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00005708
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00002660
Iteration 42/1000 | Loss: 0.00000896
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00000890
Iteration 45/1000 | Loss: 0.00001415
Iteration 46/1000 | Loss: 0.00001415
Iteration 47/1000 | Loss: 0.00001415
Iteration 48/1000 | Loss: 0.00019359
Iteration 49/1000 | Loss: 0.00002158
Iteration 50/1000 | Loss: 0.00003005
Iteration 51/1000 | Loss: 0.00002985
Iteration 52/1000 | Loss: 0.00000865
Iteration 53/1000 | Loss: 0.00000864
Iteration 54/1000 | Loss: 0.00001304
Iteration 55/1000 | Loss: 0.00001502
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00000872
Iteration 58/1000 | Loss: 0.00000892
Iteration 59/1000 | Loss: 0.00000926
Iteration 60/1000 | Loss: 0.00000907
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00003227
Iteration 63/1000 | Loss: 0.00005524
Iteration 64/1000 | Loss: 0.00003790
Iteration 65/1000 | Loss: 0.00002350
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001800
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00003949
Iteration 70/1000 | Loss: 0.00000854
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00000851
Iteration 73/1000 | Loss: 0.00000832
Iteration 74/1000 | Loss: 0.00000831
Iteration 75/1000 | Loss: 0.00000831
Iteration 76/1000 | Loss: 0.00000835
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00001038
Iteration 79/1000 | Loss: 0.00000829
Iteration 80/1000 | Loss: 0.00000828
Iteration 81/1000 | Loss: 0.00000828
Iteration 82/1000 | Loss: 0.00000828
Iteration 83/1000 | Loss: 0.00000828
Iteration 84/1000 | Loss: 0.00000828
Iteration 85/1000 | Loss: 0.00000828
Iteration 86/1000 | Loss: 0.00000828
Iteration 87/1000 | Loss: 0.00000828
Iteration 88/1000 | Loss: 0.00000828
Iteration 89/1000 | Loss: 0.00000828
Iteration 90/1000 | Loss: 0.00000828
Iteration 91/1000 | Loss: 0.00000828
Iteration 92/1000 | Loss: 0.00000828
Iteration 93/1000 | Loss: 0.00000828
Iteration 94/1000 | Loss: 0.00000828
Iteration 95/1000 | Loss: 0.00000828
Iteration 96/1000 | Loss: 0.00000828
Iteration 97/1000 | Loss: 0.00000828
Iteration 98/1000 | Loss: 0.00000828
Iteration 99/1000 | Loss: 0.00000828
Iteration 100/1000 | Loss: 0.00000828
Iteration 101/1000 | Loss: 0.00000828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [8.279550456791185e-06, 8.279550456791185e-06, 8.279550456791185e-06, 8.279550456791185e-06, 8.279550456791185e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.279550456791185e-06

Optimization complete. Final v2v error: 2.4952917098999023 mm

Highest mean error: 2.7914271354675293 mm for frame 191

Lowest mean error: 2.2556498050689697 mm for frame 134

Saving results

Total time: 112.56656622886658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000345
Iteration 2/25 | Loss: 0.00195784
Iteration 3/25 | Loss: 0.00140680
Iteration 4/25 | Loss: 0.00134448
Iteration 5/25 | Loss: 0.00136516
Iteration 6/25 | Loss: 0.00135000
Iteration 7/25 | Loss: 0.00127984
Iteration 8/25 | Loss: 0.00125309
Iteration 9/25 | Loss: 0.00124047
Iteration 10/25 | Loss: 0.00120453
Iteration 11/25 | Loss: 0.00120355
Iteration 12/25 | Loss: 0.00118178
Iteration 13/25 | Loss: 0.00118208
Iteration 14/25 | Loss: 0.00117225
Iteration 15/25 | Loss: 0.00116955
Iteration 16/25 | Loss: 0.00116464
Iteration 17/25 | Loss: 0.00115567
Iteration 18/25 | Loss: 0.00115783
Iteration 19/25 | Loss: 0.00115420
Iteration 20/25 | Loss: 0.00114979
Iteration 21/25 | Loss: 0.00114889
Iteration 22/25 | Loss: 0.00114416
Iteration 23/25 | Loss: 0.00114210
Iteration 24/25 | Loss: 0.00113966
Iteration 25/25 | Loss: 0.00113905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40901923
Iteration 2/25 | Loss: 0.00198995
Iteration 3/25 | Loss: 0.00181142
Iteration 4/25 | Loss: 0.00181142
Iteration 5/25 | Loss: 0.00181142
Iteration 6/25 | Loss: 0.00181142
Iteration 7/25 | Loss: 0.00181142
Iteration 8/25 | Loss: 0.00181142
Iteration 9/25 | Loss: 0.00181142
Iteration 10/25 | Loss: 0.00181142
Iteration 11/25 | Loss: 0.00181142
Iteration 12/25 | Loss: 0.00181142
Iteration 13/25 | Loss: 0.00181142
Iteration 14/25 | Loss: 0.00181142
Iteration 15/25 | Loss: 0.00181142
Iteration 16/25 | Loss: 0.00181142
Iteration 17/25 | Loss: 0.00181142
Iteration 18/25 | Loss: 0.00181142
Iteration 19/25 | Loss: 0.00181142
Iteration 20/25 | Loss: 0.00181142
Iteration 21/25 | Loss: 0.00181142
Iteration 22/25 | Loss: 0.00181142
Iteration 23/25 | Loss: 0.00181142
Iteration 24/25 | Loss: 0.00181142
Iteration 25/25 | Loss: 0.00181142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181142
Iteration 2/1000 | Loss: 0.00024609
Iteration 3/1000 | Loss: 0.00111805
Iteration 4/1000 | Loss: 0.00020899
Iteration 5/1000 | Loss: 0.00017598
Iteration 6/1000 | Loss: 0.00014451
Iteration 7/1000 | Loss: 0.00048115
Iteration 8/1000 | Loss: 0.00007591
Iteration 9/1000 | Loss: 0.00022704
Iteration 10/1000 | Loss: 0.00054177
Iteration 11/1000 | Loss: 0.00024051
Iteration 12/1000 | Loss: 0.00025953
Iteration 13/1000 | Loss: 0.00019990
Iteration 14/1000 | Loss: 0.00031608
Iteration 15/1000 | Loss: 0.00031607
Iteration 16/1000 | Loss: 0.00023073
Iteration 17/1000 | Loss: 0.00008500
Iteration 18/1000 | Loss: 0.00009227
Iteration 19/1000 | Loss: 0.00007318
Iteration 20/1000 | Loss: 0.00007371
Iteration 21/1000 | Loss: 0.00014090
Iteration 22/1000 | Loss: 0.00008144
Iteration 23/1000 | Loss: 0.00007585
Iteration 24/1000 | Loss: 0.00008039
Iteration 25/1000 | Loss: 0.00009833
Iteration 26/1000 | Loss: 0.00012302
Iteration 27/1000 | Loss: 0.00002054
Iteration 28/1000 | Loss: 0.00007905
Iteration 29/1000 | Loss: 0.00010010
Iteration 30/1000 | Loss: 0.00018033
Iteration 31/1000 | Loss: 0.00002007
Iteration 32/1000 | Loss: 0.00002772
Iteration 33/1000 | Loss: 0.00001837
Iteration 34/1000 | Loss: 0.00001561
Iteration 35/1000 | Loss: 0.00002479
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001371
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00002331
Iteration 42/1000 | Loss: 0.00002507
Iteration 43/1000 | Loss: 0.00002494
Iteration 44/1000 | Loss: 0.00001306
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00002477
Iteration 51/1000 | Loss: 0.00001286
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001277
Iteration 58/1000 | Loss: 0.00001277
Iteration 59/1000 | Loss: 0.00001277
Iteration 60/1000 | Loss: 0.00001277
Iteration 61/1000 | Loss: 0.00001276
Iteration 62/1000 | Loss: 0.00004620
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00011024
Iteration 65/1000 | Loss: 0.00016376
Iteration 66/1000 | Loss: 0.00003021
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00002143
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00003431
Iteration 71/1000 | Loss: 0.00001539
Iteration 72/1000 | Loss: 0.00001352
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Iteration 78/1000 | Loss: 0.00001261
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.261428860743763e-05, 1.261428860743763e-05, 1.261428860743763e-05, 1.261428860743763e-05, 1.261428860743763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.261428860743763e-05

Optimization complete. Final v2v error: 3.003661870956421 mm

Highest mean error: 5.400838375091553 mm for frame 99

Lowest mean error: 2.410851001739502 mm for frame 194

Saving results

Total time: 149.51319336891174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819293
Iteration 2/25 | Loss: 0.00137897
Iteration 3/25 | Loss: 0.00117182
Iteration 4/25 | Loss: 0.00115304
Iteration 5/25 | Loss: 0.00115149
Iteration 6/25 | Loss: 0.00115149
Iteration 7/25 | Loss: 0.00115149
Iteration 8/25 | Loss: 0.00115149
Iteration 9/25 | Loss: 0.00115149
Iteration 10/25 | Loss: 0.00115149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011514886282384396, 0.0011514886282384396, 0.0011514886282384396, 0.0011514886282384396, 0.0011514886282384396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011514886282384396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97109807
Iteration 2/25 | Loss: 0.00044414
Iteration 3/25 | Loss: 0.00044413
Iteration 4/25 | Loss: 0.00044413
Iteration 5/25 | Loss: 0.00044413
Iteration 6/25 | Loss: 0.00044413
Iteration 7/25 | Loss: 0.00044413
Iteration 8/25 | Loss: 0.00044413
Iteration 9/25 | Loss: 0.00044413
Iteration 10/25 | Loss: 0.00044413
Iteration 11/25 | Loss: 0.00044413
Iteration 12/25 | Loss: 0.00044413
Iteration 13/25 | Loss: 0.00044413
Iteration 14/25 | Loss: 0.00044413
Iteration 15/25 | Loss: 0.00044413
Iteration 16/25 | Loss: 0.00044413
Iteration 17/25 | Loss: 0.00044413
Iteration 18/25 | Loss: 0.00044413
Iteration 19/25 | Loss: 0.00044413
Iteration 20/25 | Loss: 0.00044413
Iteration 21/25 | Loss: 0.00044413
Iteration 22/25 | Loss: 0.00044413
Iteration 23/25 | Loss: 0.00044413
Iteration 24/25 | Loss: 0.00044413
Iteration 25/25 | Loss: 0.00044413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044413
Iteration 2/1000 | Loss: 0.00002934
Iteration 3/1000 | Loss: 0.00002352
Iteration 4/1000 | Loss: 0.00002179
Iteration 5/1000 | Loss: 0.00002084
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001897
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001890
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001889
Iteration 20/1000 | Loss: 0.00001882
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001870
Iteration 28/1000 | Loss: 0.00001870
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001870
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001870
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001869
Iteration 36/1000 | Loss: 0.00001869
Iteration 37/1000 | Loss: 0.00001869
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001864
Iteration 45/1000 | Loss: 0.00001864
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001863
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001859
Iteration 51/1000 | Loss: 0.00001859
Iteration 52/1000 | Loss: 0.00001859
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001859
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001858
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001858
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001858
Iteration 76/1000 | Loss: 0.00001858
Iteration 77/1000 | Loss: 0.00001858
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001858
Iteration 80/1000 | Loss: 0.00001858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.8577500668470748e-05, 1.8577500668470748e-05, 1.8577500668470748e-05, 1.8577500668470748e-05, 1.8577500668470748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8577500668470748e-05

Optimization complete. Final v2v error: 3.5860369205474854 mm

Highest mean error: 3.719268321990967 mm for frame 106

Lowest mean error: 3.4580554962158203 mm for frame 35

Saving results

Total time: 28.845993041992188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871316
Iteration 2/25 | Loss: 0.00258385
Iteration 3/25 | Loss: 0.00184463
Iteration 4/25 | Loss: 0.00153798
Iteration 5/25 | Loss: 0.00152501
Iteration 6/25 | Loss: 0.00149492
Iteration 7/25 | Loss: 0.00148761
Iteration 8/25 | Loss: 0.00146100
Iteration 9/25 | Loss: 0.00144416
Iteration 10/25 | Loss: 0.00143687
Iteration 11/25 | Loss: 0.00143576
Iteration 12/25 | Loss: 0.00143368
Iteration 13/25 | Loss: 0.00143098
Iteration 14/25 | Loss: 0.00142981
Iteration 15/25 | Loss: 0.00142766
Iteration 16/25 | Loss: 0.00143149
Iteration 17/25 | Loss: 0.00143001
Iteration 18/25 | Loss: 0.00142579
Iteration 19/25 | Loss: 0.00142067
Iteration 20/25 | Loss: 0.00142362
Iteration 21/25 | Loss: 0.00142651
Iteration 22/25 | Loss: 0.00142544
Iteration 23/25 | Loss: 0.00142272
Iteration 24/25 | Loss: 0.00142433
Iteration 25/25 | Loss: 0.00142290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.28017282
Iteration 2/25 | Loss: 0.00369058
Iteration 3/25 | Loss: 0.00369058
Iteration 4/25 | Loss: 0.00369058
Iteration 5/25 | Loss: 0.00369058
Iteration 6/25 | Loss: 0.00369058
Iteration 7/25 | Loss: 0.00369058
Iteration 8/25 | Loss: 0.00369058
Iteration 9/25 | Loss: 0.00369058
Iteration 10/25 | Loss: 0.00369058
Iteration 11/25 | Loss: 0.00369058
Iteration 12/25 | Loss: 0.00369058
Iteration 13/25 | Loss: 0.00369058
Iteration 14/25 | Loss: 0.00369058
Iteration 15/25 | Loss: 0.00369058
Iteration 16/25 | Loss: 0.00369058
Iteration 17/25 | Loss: 0.00369058
Iteration 18/25 | Loss: 0.00369058
Iteration 19/25 | Loss: 0.00369058
Iteration 20/25 | Loss: 0.00369058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003690578043460846, 0.003690578043460846, 0.003690578043460846, 0.003690578043460846, 0.003690578043460846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003690578043460846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00369058
Iteration 2/1000 | Loss: 0.00076196
Iteration 3/1000 | Loss: 0.00129405
Iteration 4/1000 | Loss: 0.00061717
Iteration 5/1000 | Loss: 0.00070909
Iteration 6/1000 | Loss: 0.00034753
Iteration 7/1000 | Loss: 0.00053232
Iteration 8/1000 | Loss: 0.00046776
Iteration 9/1000 | Loss: 0.00023338
Iteration 10/1000 | Loss: 0.00019069
Iteration 11/1000 | Loss: 0.00013324
Iteration 12/1000 | Loss: 0.00004960
Iteration 13/1000 | Loss: 0.00004456
Iteration 14/1000 | Loss: 0.00004048
Iteration 15/1000 | Loss: 0.00003813
Iteration 16/1000 | Loss: 0.00003664
Iteration 17/1000 | Loss: 0.00003552
Iteration 18/1000 | Loss: 0.00015569
Iteration 19/1000 | Loss: 0.00003787
Iteration 20/1000 | Loss: 0.00003354
Iteration 21/1000 | Loss: 0.00003381
Iteration 22/1000 | Loss: 0.00003161
Iteration 23/1000 | Loss: 0.00003066
Iteration 24/1000 | Loss: 0.00002992
Iteration 25/1000 | Loss: 0.00002930
Iteration 26/1000 | Loss: 0.00002885
Iteration 27/1000 | Loss: 0.00002840
Iteration 28/1000 | Loss: 0.00002792
Iteration 29/1000 | Loss: 0.00002761
Iteration 30/1000 | Loss: 0.00002733
Iteration 31/1000 | Loss: 0.00002713
Iteration 32/1000 | Loss: 0.00002706
Iteration 33/1000 | Loss: 0.00002706
Iteration 34/1000 | Loss: 0.00002700
Iteration 35/1000 | Loss: 0.00002699
Iteration 36/1000 | Loss: 0.00002697
Iteration 37/1000 | Loss: 0.00002696
Iteration 38/1000 | Loss: 0.00002684
Iteration 39/1000 | Loss: 0.00002683
Iteration 40/1000 | Loss: 0.00002681
Iteration 41/1000 | Loss: 0.00002680
Iteration 42/1000 | Loss: 0.00002679
Iteration 43/1000 | Loss: 0.00002678
Iteration 44/1000 | Loss: 0.00002674
Iteration 45/1000 | Loss: 0.00002670
Iteration 46/1000 | Loss: 0.00002663
Iteration 47/1000 | Loss: 0.00002661
Iteration 48/1000 | Loss: 0.00002660
Iteration 49/1000 | Loss: 0.00002659
Iteration 50/1000 | Loss: 0.00002658
Iteration 51/1000 | Loss: 0.00002658
Iteration 52/1000 | Loss: 0.00002656
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002655
Iteration 55/1000 | Loss: 0.00002652
Iteration 56/1000 | Loss: 0.00002652
Iteration 57/1000 | Loss: 0.00002650
Iteration 58/1000 | Loss: 0.00002647
Iteration 59/1000 | Loss: 0.00002640
Iteration 60/1000 | Loss: 0.00002638
Iteration 61/1000 | Loss: 0.00002636
Iteration 62/1000 | Loss: 0.00002636
Iteration 63/1000 | Loss: 0.00002635
Iteration 64/1000 | Loss: 0.00002634
Iteration 65/1000 | Loss: 0.00002632
Iteration 66/1000 | Loss: 0.00002630
Iteration 67/1000 | Loss: 0.00002617
Iteration 68/1000 | Loss: 0.00002611
Iteration 69/1000 | Loss: 0.00002596
Iteration 70/1000 | Loss: 0.00002596
Iteration 71/1000 | Loss: 0.00002587
Iteration 72/1000 | Loss: 0.00002571
Iteration 73/1000 | Loss: 0.00002558
Iteration 74/1000 | Loss: 0.00002548
Iteration 75/1000 | Loss: 0.00002543
Iteration 76/1000 | Loss: 0.00002539
Iteration 77/1000 | Loss: 0.00002538
Iteration 78/1000 | Loss: 0.00002532
Iteration 79/1000 | Loss: 0.00023606
Iteration 80/1000 | Loss: 0.00006877
Iteration 81/1000 | Loss: 0.00053948
Iteration 82/1000 | Loss: 0.00021579
Iteration 83/1000 | Loss: 0.00009509
Iteration 84/1000 | Loss: 0.00038181
Iteration 85/1000 | Loss: 0.00003153
Iteration 86/1000 | Loss: 0.00002817
Iteration 87/1000 | Loss: 0.00002733
Iteration 88/1000 | Loss: 0.00002699
Iteration 89/1000 | Loss: 0.00002678
Iteration 90/1000 | Loss: 0.00002669
Iteration 91/1000 | Loss: 0.00002668
Iteration 92/1000 | Loss: 0.00002668
Iteration 93/1000 | Loss: 0.00002666
Iteration 94/1000 | Loss: 0.00002666
Iteration 95/1000 | Loss: 0.00002665
Iteration 96/1000 | Loss: 0.00002664
Iteration 97/1000 | Loss: 0.00002663
Iteration 98/1000 | Loss: 0.00002663
Iteration 99/1000 | Loss: 0.00002663
Iteration 100/1000 | Loss: 0.00002662
Iteration 101/1000 | Loss: 0.00002662
Iteration 102/1000 | Loss: 0.00002662
Iteration 103/1000 | Loss: 0.00002661
Iteration 104/1000 | Loss: 0.00002657
Iteration 105/1000 | Loss: 0.00002657
Iteration 106/1000 | Loss: 0.00002657
Iteration 107/1000 | Loss: 0.00002657
Iteration 108/1000 | Loss: 0.00002657
Iteration 109/1000 | Loss: 0.00002656
Iteration 110/1000 | Loss: 0.00002656
Iteration 111/1000 | Loss: 0.00002656
Iteration 112/1000 | Loss: 0.00002656
Iteration 113/1000 | Loss: 0.00002655
Iteration 114/1000 | Loss: 0.00002655
Iteration 115/1000 | Loss: 0.00002655
Iteration 116/1000 | Loss: 0.00002654
Iteration 117/1000 | Loss: 0.00002654
Iteration 118/1000 | Loss: 0.00002654
Iteration 119/1000 | Loss: 0.00002653
Iteration 120/1000 | Loss: 0.00002653
Iteration 121/1000 | Loss: 0.00002652
Iteration 122/1000 | Loss: 0.00002652
Iteration 123/1000 | Loss: 0.00002652
Iteration 124/1000 | Loss: 0.00002651
Iteration 125/1000 | Loss: 0.00002651
Iteration 126/1000 | Loss: 0.00002651
Iteration 127/1000 | Loss: 0.00002649
Iteration 128/1000 | Loss: 0.00002649
Iteration 129/1000 | Loss: 0.00002646
Iteration 130/1000 | Loss: 0.00002645
Iteration 131/1000 | Loss: 0.00002645
Iteration 132/1000 | Loss: 0.00002644
Iteration 133/1000 | Loss: 0.00002644
Iteration 134/1000 | Loss: 0.00002644
Iteration 135/1000 | Loss: 0.00002644
Iteration 136/1000 | Loss: 0.00002643
Iteration 137/1000 | Loss: 0.00002643
Iteration 138/1000 | Loss: 0.00002643
Iteration 139/1000 | Loss: 0.00002643
Iteration 140/1000 | Loss: 0.00002642
Iteration 141/1000 | Loss: 0.00002642
Iteration 142/1000 | Loss: 0.00002642
Iteration 143/1000 | Loss: 0.00002642
Iteration 144/1000 | Loss: 0.00002642
Iteration 145/1000 | Loss: 0.00002642
Iteration 146/1000 | Loss: 0.00002641
Iteration 147/1000 | Loss: 0.00002641
Iteration 148/1000 | Loss: 0.00002641
Iteration 149/1000 | Loss: 0.00002641
Iteration 150/1000 | Loss: 0.00002641
Iteration 151/1000 | Loss: 0.00002641
Iteration 152/1000 | Loss: 0.00002640
Iteration 153/1000 | Loss: 0.00002640
Iteration 154/1000 | Loss: 0.00002640
Iteration 155/1000 | Loss: 0.00002640
Iteration 156/1000 | Loss: 0.00002640
Iteration 157/1000 | Loss: 0.00002640
Iteration 158/1000 | Loss: 0.00002640
Iteration 159/1000 | Loss: 0.00002639
Iteration 160/1000 | Loss: 0.00002639
Iteration 161/1000 | Loss: 0.00002639
Iteration 162/1000 | Loss: 0.00002639
Iteration 163/1000 | Loss: 0.00002639
Iteration 164/1000 | Loss: 0.00002639
Iteration 165/1000 | Loss: 0.00002639
Iteration 166/1000 | Loss: 0.00002639
Iteration 167/1000 | Loss: 0.00002638
Iteration 168/1000 | Loss: 0.00002638
Iteration 169/1000 | Loss: 0.00002638
Iteration 170/1000 | Loss: 0.00002637
Iteration 171/1000 | Loss: 0.00002637
Iteration 172/1000 | Loss: 0.00002637
Iteration 173/1000 | Loss: 0.00002636
Iteration 174/1000 | Loss: 0.00002636
Iteration 175/1000 | Loss: 0.00002636
Iteration 176/1000 | Loss: 0.00002636
Iteration 177/1000 | Loss: 0.00002636
Iteration 178/1000 | Loss: 0.00002636
Iteration 179/1000 | Loss: 0.00002636
Iteration 180/1000 | Loss: 0.00002636
Iteration 181/1000 | Loss: 0.00002635
Iteration 182/1000 | Loss: 0.00002635
Iteration 183/1000 | Loss: 0.00002635
Iteration 184/1000 | Loss: 0.00002635
Iteration 185/1000 | Loss: 0.00002635
Iteration 186/1000 | Loss: 0.00002633
Iteration 187/1000 | Loss: 0.00002633
Iteration 188/1000 | Loss: 0.00002633
Iteration 189/1000 | Loss: 0.00002633
Iteration 190/1000 | Loss: 0.00002633
Iteration 191/1000 | Loss: 0.00002633
Iteration 192/1000 | Loss: 0.00002632
Iteration 193/1000 | Loss: 0.00002632
Iteration 194/1000 | Loss: 0.00002631
Iteration 195/1000 | Loss: 0.00002631
Iteration 196/1000 | Loss: 0.00002631
Iteration 197/1000 | Loss: 0.00002630
Iteration 198/1000 | Loss: 0.00002630
Iteration 199/1000 | Loss: 0.00002630
Iteration 200/1000 | Loss: 0.00002629
Iteration 201/1000 | Loss: 0.00002629
Iteration 202/1000 | Loss: 0.00002628
Iteration 203/1000 | Loss: 0.00002628
Iteration 204/1000 | Loss: 0.00002628
Iteration 205/1000 | Loss: 0.00002628
Iteration 206/1000 | Loss: 0.00002627
Iteration 207/1000 | Loss: 0.00002626
Iteration 208/1000 | Loss: 0.00002626
Iteration 209/1000 | Loss: 0.00002626
Iteration 210/1000 | Loss: 0.00002625
Iteration 211/1000 | Loss: 0.00002623
Iteration 212/1000 | Loss: 0.00002623
Iteration 213/1000 | Loss: 0.00002622
Iteration 214/1000 | Loss: 0.00002619
Iteration 215/1000 | Loss: 0.00002600
Iteration 216/1000 | Loss: 0.00002590
Iteration 217/1000 | Loss: 0.00002590
Iteration 218/1000 | Loss: 0.00002581
Iteration 219/1000 | Loss: 0.00002575
Iteration 220/1000 | Loss: 0.00002573
Iteration 221/1000 | Loss: 0.00002565
Iteration 222/1000 | Loss: 0.00002550
Iteration 223/1000 | Loss: 0.00002539
Iteration 224/1000 | Loss: 0.00002530
Iteration 225/1000 | Loss: 0.00024930
Iteration 226/1000 | Loss: 0.00009772
Iteration 227/1000 | Loss: 0.00016482
Iteration 228/1000 | Loss: 0.00006332
Iteration 229/1000 | Loss: 0.00017097
Iteration 230/1000 | Loss: 0.00012530
Iteration 231/1000 | Loss: 0.00005665
Iteration 232/1000 | Loss: 0.00002925
Iteration 233/1000 | Loss: 0.00002811
Iteration 234/1000 | Loss: 0.00002763
Iteration 235/1000 | Loss: 0.00002702
Iteration 236/1000 | Loss: 0.00016655
Iteration 237/1000 | Loss: 0.00013652
Iteration 238/1000 | Loss: 0.00014985
Iteration 239/1000 | Loss: 0.00010023
Iteration 240/1000 | Loss: 0.00014311
Iteration 241/1000 | Loss: 0.00003190
Iteration 242/1000 | Loss: 0.00003131
Iteration 243/1000 | Loss: 0.00002711
Iteration 244/1000 | Loss: 0.00002667
Iteration 245/1000 | Loss: 0.00002655
Iteration 246/1000 | Loss: 0.00002631
Iteration 247/1000 | Loss: 0.00002605
Iteration 248/1000 | Loss: 0.00002582
Iteration 249/1000 | Loss: 0.00002573
Iteration 250/1000 | Loss: 0.00002572
Iteration 251/1000 | Loss: 0.00002560
Iteration 252/1000 | Loss: 0.00002549
Iteration 253/1000 | Loss: 0.00002545
Iteration 254/1000 | Loss: 0.00002542
Iteration 255/1000 | Loss: 0.00002541
Iteration 256/1000 | Loss: 0.00002539
Iteration 257/1000 | Loss: 0.00002539
Iteration 258/1000 | Loss: 0.00002537
Iteration 259/1000 | Loss: 0.00002537
Iteration 260/1000 | Loss: 0.00002536
Iteration 261/1000 | Loss: 0.00002535
Iteration 262/1000 | Loss: 0.00002535
Iteration 263/1000 | Loss: 0.00002535
Iteration 264/1000 | Loss: 0.00002534
Iteration 265/1000 | Loss: 0.00002534
Iteration 266/1000 | Loss: 0.00002533
Iteration 267/1000 | Loss: 0.00002532
Iteration 268/1000 | Loss: 0.00002532
Iteration 269/1000 | Loss: 0.00002531
Iteration 270/1000 | Loss: 0.00002531
Iteration 271/1000 | Loss: 0.00002530
Iteration 272/1000 | Loss: 0.00002530
Iteration 273/1000 | Loss: 0.00002528
Iteration 274/1000 | Loss: 0.00002528
Iteration 275/1000 | Loss: 0.00002527
Iteration 276/1000 | Loss: 0.00002527
Iteration 277/1000 | Loss: 0.00002527
Iteration 278/1000 | Loss: 0.00002526
Iteration 279/1000 | Loss: 0.00002525
Iteration 280/1000 | Loss: 0.00002525
Iteration 281/1000 | Loss: 0.00002522
Iteration 282/1000 | Loss: 0.00002522
Iteration 283/1000 | Loss: 0.00002518
Iteration 284/1000 | Loss: 0.00002517
Iteration 285/1000 | Loss: 0.00002514
Iteration 286/1000 | Loss: 0.00002514
Iteration 287/1000 | Loss: 0.00002513
Iteration 288/1000 | Loss: 0.00002512
Iteration 289/1000 | Loss: 0.00002509
Iteration 290/1000 | Loss: 0.00002507
Iteration 291/1000 | Loss: 0.00002507
Iteration 292/1000 | Loss: 0.00022437
Iteration 293/1000 | Loss: 0.00023770
Iteration 294/1000 | Loss: 0.00026774
Iteration 295/1000 | Loss: 0.00027811
Iteration 296/1000 | Loss: 0.00009014
Iteration 297/1000 | Loss: 0.00023027
Iteration 298/1000 | Loss: 0.00012371
Iteration 299/1000 | Loss: 0.00002469
Iteration 300/1000 | Loss: 0.00002402
Iteration 301/1000 | Loss: 0.00025296
Iteration 302/1000 | Loss: 0.00003889
Iteration 303/1000 | Loss: 0.00003122
Iteration 304/1000 | Loss: 0.00002618
Iteration 305/1000 | Loss: 0.00002403
Iteration 306/1000 | Loss: 0.00002277
Iteration 307/1000 | Loss: 0.00002238
Iteration 308/1000 | Loss: 0.00002220
Iteration 309/1000 | Loss: 0.00002209
Iteration 310/1000 | Loss: 0.00002196
Iteration 311/1000 | Loss: 0.00002193
Iteration 312/1000 | Loss: 0.00002191
Iteration 313/1000 | Loss: 0.00002188
Iteration 314/1000 | Loss: 0.00002188
Iteration 315/1000 | Loss: 0.00002188
Iteration 316/1000 | Loss: 0.00002187
Iteration 317/1000 | Loss: 0.00002187
Iteration 318/1000 | Loss: 0.00002187
Iteration 319/1000 | Loss: 0.00002186
Iteration 320/1000 | Loss: 0.00002186
Iteration 321/1000 | Loss: 0.00002185
Iteration 322/1000 | Loss: 0.00002185
Iteration 323/1000 | Loss: 0.00002185
Iteration 324/1000 | Loss: 0.00002185
Iteration 325/1000 | Loss: 0.00002185
Iteration 326/1000 | Loss: 0.00002184
Iteration 327/1000 | Loss: 0.00002184
Iteration 328/1000 | Loss: 0.00002184
Iteration 329/1000 | Loss: 0.00002184
Iteration 330/1000 | Loss: 0.00002184
Iteration 331/1000 | Loss: 0.00002184
Iteration 332/1000 | Loss: 0.00002183
Iteration 333/1000 | Loss: 0.00002183
Iteration 334/1000 | Loss: 0.00002183
Iteration 335/1000 | Loss: 0.00002183
Iteration 336/1000 | Loss: 0.00002182
Iteration 337/1000 | Loss: 0.00002182
Iteration 338/1000 | Loss: 0.00002181
Iteration 339/1000 | Loss: 0.00002181
Iteration 340/1000 | Loss: 0.00002177
Iteration 341/1000 | Loss: 0.00002173
Iteration 342/1000 | Loss: 0.00002173
Iteration 343/1000 | Loss: 0.00002172
Iteration 344/1000 | Loss: 0.00002171
Iteration 345/1000 | Loss: 0.00002171
Iteration 346/1000 | Loss: 0.00002170
Iteration 347/1000 | Loss: 0.00002170
Iteration 348/1000 | Loss: 0.00002170
Iteration 349/1000 | Loss: 0.00002169
Iteration 350/1000 | Loss: 0.00002168
Iteration 351/1000 | Loss: 0.00002168
Iteration 352/1000 | Loss: 0.00002167
Iteration 353/1000 | Loss: 0.00002167
Iteration 354/1000 | Loss: 0.00002167
Iteration 355/1000 | Loss: 0.00002167
Iteration 356/1000 | Loss: 0.00002166
Iteration 357/1000 | Loss: 0.00002166
Iteration 358/1000 | Loss: 0.00002166
Iteration 359/1000 | Loss: 0.00002166
Iteration 360/1000 | Loss: 0.00002165
Iteration 361/1000 | Loss: 0.00002165
Iteration 362/1000 | Loss: 0.00002165
Iteration 363/1000 | Loss: 0.00002165
Iteration 364/1000 | Loss: 0.00002165
Iteration 365/1000 | Loss: 0.00002165
Iteration 366/1000 | Loss: 0.00002164
Iteration 367/1000 | Loss: 0.00002164
Iteration 368/1000 | Loss: 0.00002164
Iteration 369/1000 | Loss: 0.00002164
Iteration 370/1000 | Loss: 0.00002164
Iteration 371/1000 | Loss: 0.00002164
Iteration 372/1000 | Loss: 0.00002164
Iteration 373/1000 | Loss: 0.00002164
Iteration 374/1000 | Loss: 0.00002164
Iteration 375/1000 | Loss: 0.00002164
Iteration 376/1000 | Loss: 0.00002164
Iteration 377/1000 | Loss: 0.00002163
Iteration 378/1000 | Loss: 0.00002163
Iteration 379/1000 | Loss: 0.00002163
Iteration 380/1000 | Loss: 0.00002163
Iteration 381/1000 | Loss: 0.00002163
Iteration 382/1000 | Loss: 0.00002163
Iteration 383/1000 | Loss: 0.00002163
Iteration 384/1000 | Loss: 0.00002163
Iteration 385/1000 | Loss: 0.00002163
Iteration 386/1000 | Loss: 0.00002162
Iteration 387/1000 | Loss: 0.00002162
Iteration 388/1000 | Loss: 0.00002162
Iteration 389/1000 | Loss: 0.00002162
Iteration 390/1000 | Loss: 0.00002162
Iteration 391/1000 | Loss: 0.00002162
Iteration 392/1000 | Loss: 0.00002162
Iteration 393/1000 | Loss: 0.00002162
Iteration 394/1000 | Loss: 0.00002162
Iteration 395/1000 | Loss: 0.00002162
Iteration 396/1000 | Loss: 0.00002162
Iteration 397/1000 | Loss: 0.00002162
Iteration 398/1000 | Loss: 0.00002162
Iteration 399/1000 | Loss: 0.00002162
Iteration 400/1000 | Loss: 0.00002161
Iteration 401/1000 | Loss: 0.00002161
Iteration 402/1000 | Loss: 0.00002161
Iteration 403/1000 | Loss: 0.00002161
Iteration 404/1000 | Loss: 0.00002161
Iteration 405/1000 | Loss: 0.00002161
Iteration 406/1000 | Loss: 0.00002161
Iteration 407/1000 | Loss: 0.00002161
Iteration 408/1000 | Loss: 0.00002161
Iteration 409/1000 | Loss: 0.00002161
Iteration 410/1000 | Loss: 0.00002161
Iteration 411/1000 | Loss: 0.00002161
Iteration 412/1000 | Loss: 0.00002161
Iteration 413/1000 | Loss: 0.00002161
Iteration 414/1000 | Loss: 0.00002161
Iteration 415/1000 | Loss: 0.00002161
Iteration 416/1000 | Loss: 0.00002161
Iteration 417/1000 | Loss: 0.00002161
Iteration 418/1000 | Loss: 0.00002161
Iteration 419/1000 | Loss: 0.00002161
Iteration 420/1000 | Loss: 0.00002161
Iteration 421/1000 | Loss: 0.00002160
Iteration 422/1000 | Loss: 0.00002160
Iteration 423/1000 | Loss: 0.00002160
Iteration 424/1000 | Loss: 0.00002160
Iteration 425/1000 | Loss: 0.00002160
Iteration 426/1000 | Loss: 0.00002160
Iteration 427/1000 | Loss: 0.00002160
Iteration 428/1000 | Loss: 0.00002160
Iteration 429/1000 | Loss: 0.00002160
Iteration 430/1000 | Loss: 0.00002160
Iteration 431/1000 | Loss: 0.00002160
Iteration 432/1000 | Loss: 0.00002160
Iteration 433/1000 | Loss: 0.00002160
Iteration 434/1000 | Loss: 0.00002160
Iteration 435/1000 | Loss: 0.00002160
Iteration 436/1000 | Loss: 0.00002160
Iteration 437/1000 | Loss: 0.00002160
Iteration 438/1000 | Loss: 0.00002160
Iteration 439/1000 | Loss: 0.00002160
Iteration 440/1000 | Loss: 0.00002160
Iteration 441/1000 | Loss: 0.00002160
Iteration 442/1000 | Loss: 0.00002160
Iteration 443/1000 | Loss: 0.00002160
Iteration 444/1000 | Loss: 0.00002160
Iteration 445/1000 | Loss: 0.00002160
Iteration 446/1000 | Loss: 0.00002160
Iteration 447/1000 | Loss: 0.00002160
Iteration 448/1000 | Loss: 0.00002160
Iteration 449/1000 | Loss: 0.00002160
Iteration 450/1000 | Loss: 0.00002160
Iteration 451/1000 | Loss: 0.00002160
Iteration 452/1000 | Loss: 0.00002160
Iteration 453/1000 | Loss: 0.00002160
Iteration 454/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 454. Stopping optimization.
Last 5 losses: [2.1602756532956846e-05, 2.1602756532956846e-05, 2.1602756532956846e-05, 2.1602756532956846e-05, 2.1602756532956846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1602756532956846e-05

Optimization complete. Final v2v error: 3.5530850887298584 mm

Highest mean error: 12.675076484680176 mm for frame 56

Lowest mean error: 2.7375612258911133 mm for frame 45

Saving results

Total time: 236.5240797996521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817454
Iteration 2/25 | Loss: 0.00120521
Iteration 3/25 | Loss: 0.00108121
Iteration 4/25 | Loss: 0.00106542
Iteration 5/25 | Loss: 0.00106224
Iteration 6/25 | Loss: 0.00106224
Iteration 7/25 | Loss: 0.00106224
Iteration 8/25 | Loss: 0.00106224
Iteration 9/25 | Loss: 0.00106224
Iteration 10/25 | Loss: 0.00106224
Iteration 11/25 | Loss: 0.00106224
Iteration 12/25 | Loss: 0.00106224
Iteration 13/25 | Loss: 0.00106221
Iteration 14/25 | Loss: 0.00106221
Iteration 15/25 | Loss: 0.00106221
Iteration 16/25 | Loss: 0.00106221
Iteration 17/25 | Loss: 0.00106221
Iteration 18/25 | Loss: 0.00106221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010622091358527541, 0.0010622091358527541, 0.0010622091358527541, 0.0010622091358527541, 0.0010622091358527541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010622091358527541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34649611
Iteration 2/25 | Loss: 0.00075908
Iteration 3/25 | Loss: 0.00075908
Iteration 4/25 | Loss: 0.00075908
Iteration 5/25 | Loss: 0.00075908
Iteration 6/25 | Loss: 0.00075907
Iteration 7/25 | Loss: 0.00075907
Iteration 8/25 | Loss: 0.00075907
Iteration 9/25 | Loss: 0.00075907
Iteration 10/25 | Loss: 0.00075907
Iteration 11/25 | Loss: 0.00075907
Iteration 12/25 | Loss: 0.00075907
Iteration 13/25 | Loss: 0.00075907
Iteration 14/25 | Loss: 0.00075907
Iteration 15/25 | Loss: 0.00075907
Iteration 16/25 | Loss: 0.00075907
Iteration 17/25 | Loss: 0.00075907
Iteration 18/25 | Loss: 0.00075907
Iteration 19/25 | Loss: 0.00075907
Iteration 20/25 | Loss: 0.00075907
Iteration 21/25 | Loss: 0.00075907
Iteration 22/25 | Loss: 0.00075907
Iteration 23/25 | Loss: 0.00075907
Iteration 24/25 | Loss: 0.00075907
Iteration 25/25 | Loss: 0.00075907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075907
Iteration 2/1000 | Loss: 0.00001733
Iteration 3/1000 | Loss: 0.00001300
Iteration 4/1000 | Loss: 0.00001183
Iteration 5/1000 | Loss: 0.00001113
Iteration 6/1000 | Loss: 0.00001042
Iteration 7/1000 | Loss: 0.00001015
Iteration 8/1000 | Loss: 0.00000987
Iteration 9/1000 | Loss: 0.00000964
Iteration 10/1000 | Loss: 0.00000956
Iteration 11/1000 | Loss: 0.00000954
Iteration 12/1000 | Loss: 0.00000953
Iteration 13/1000 | Loss: 0.00000943
Iteration 14/1000 | Loss: 0.00000941
Iteration 15/1000 | Loss: 0.00000940
Iteration 16/1000 | Loss: 0.00000938
Iteration 17/1000 | Loss: 0.00000938
Iteration 18/1000 | Loss: 0.00000937
Iteration 19/1000 | Loss: 0.00000935
Iteration 20/1000 | Loss: 0.00000933
Iteration 21/1000 | Loss: 0.00000933
Iteration 22/1000 | Loss: 0.00000932
Iteration 23/1000 | Loss: 0.00000932
Iteration 24/1000 | Loss: 0.00000932
Iteration 25/1000 | Loss: 0.00000931
Iteration 26/1000 | Loss: 0.00000931
Iteration 27/1000 | Loss: 0.00000930
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000929
Iteration 30/1000 | Loss: 0.00000929
Iteration 31/1000 | Loss: 0.00000928
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000928
Iteration 34/1000 | Loss: 0.00000927
Iteration 35/1000 | Loss: 0.00000927
Iteration 36/1000 | Loss: 0.00000926
Iteration 37/1000 | Loss: 0.00000926
Iteration 38/1000 | Loss: 0.00000924
Iteration 39/1000 | Loss: 0.00000924
Iteration 40/1000 | Loss: 0.00000923
Iteration 41/1000 | Loss: 0.00000923
Iteration 42/1000 | Loss: 0.00000923
Iteration 43/1000 | Loss: 0.00000923
Iteration 44/1000 | Loss: 0.00000923
Iteration 45/1000 | Loss: 0.00000923
Iteration 46/1000 | Loss: 0.00000920
Iteration 47/1000 | Loss: 0.00000919
Iteration 48/1000 | Loss: 0.00000919
Iteration 49/1000 | Loss: 0.00000919
Iteration 50/1000 | Loss: 0.00000918
Iteration 51/1000 | Loss: 0.00000918
Iteration 52/1000 | Loss: 0.00000918
Iteration 53/1000 | Loss: 0.00000917
Iteration 54/1000 | Loss: 0.00000914
Iteration 55/1000 | Loss: 0.00000913
Iteration 56/1000 | Loss: 0.00000912
Iteration 57/1000 | Loss: 0.00000912
Iteration 58/1000 | Loss: 0.00000909
Iteration 59/1000 | Loss: 0.00000908
Iteration 60/1000 | Loss: 0.00000905
Iteration 61/1000 | Loss: 0.00000901
Iteration 62/1000 | Loss: 0.00000901
Iteration 63/1000 | Loss: 0.00000901
Iteration 64/1000 | Loss: 0.00000901
Iteration 65/1000 | Loss: 0.00000901
Iteration 66/1000 | Loss: 0.00000901
Iteration 67/1000 | Loss: 0.00000901
Iteration 68/1000 | Loss: 0.00000901
Iteration 69/1000 | Loss: 0.00000901
Iteration 70/1000 | Loss: 0.00000901
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000898
Iteration 73/1000 | Loss: 0.00000898
Iteration 74/1000 | Loss: 0.00000898
Iteration 75/1000 | Loss: 0.00000897
Iteration 76/1000 | Loss: 0.00000897
Iteration 77/1000 | Loss: 0.00000897
Iteration 78/1000 | Loss: 0.00000897
Iteration 79/1000 | Loss: 0.00000897
Iteration 80/1000 | Loss: 0.00000897
Iteration 81/1000 | Loss: 0.00000897
Iteration 82/1000 | Loss: 0.00000897
Iteration 83/1000 | Loss: 0.00000897
Iteration 84/1000 | Loss: 0.00000897
Iteration 85/1000 | Loss: 0.00000897
Iteration 86/1000 | Loss: 0.00000897
Iteration 87/1000 | Loss: 0.00000895
Iteration 88/1000 | Loss: 0.00000894
Iteration 89/1000 | Loss: 0.00000894
Iteration 90/1000 | Loss: 0.00000894
Iteration 91/1000 | Loss: 0.00000894
Iteration 92/1000 | Loss: 0.00000893
Iteration 93/1000 | Loss: 0.00000893
Iteration 94/1000 | Loss: 0.00000893
Iteration 95/1000 | Loss: 0.00000892
Iteration 96/1000 | Loss: 0.00000892
Iteration 97/1000 | Loss: 0.00000892
Iteration 98/1000 | Loss: 0.00000891
Iteration 99/1000 | Loss: 0.00000891
Iteration 100/1000 | Loss: 0.00000891
Iteration 101/1000 | Loss: 0.00000889
Iteration 102/1000 | Loss: 0.00000889
Iteration 103/1000 | Loss: 0.00000889
Iteration 104/1000 | Loss: 0.00000889
Iteration 105/1000 | Loss: 0.00000889
Iteration 106/1000 | Loss: 0.00000889
Iteration 107/1000 | Loss: 0.00000889
Iteration 108/1000 | Loss: 0.00000889
Iteration 109/1000 | Loss: 0.00000889
Iteration 110/1000 | Loss: 0.00000888
Iteration 111/1000 | Loss: 0.00000888
Iteration 112/1000 | Loss: 0.00000888
Iteration 113/1000 | Loss: 0.00000887
Iteration 114/1000 | Loss: 0.00000887
Iteration 115/1000 | Loss: 0.00000887
Iteration 116/1000 | Loss: 0.00000886
Iteration 117/1000 | Loss: 0.00000886
Iteration 118/1000 | Loss: 0.00000886
Iteration 119/1000 | Loss: 0.00000885
Iteration 120/1000 | Loss: 0.00000885
Iteration 121/1000 | Loss: 0.00000885
Iteration 122/1000 | Loss: 0.00000884
Iteration 123/1000 | Loss: 0.00000884
Iteration 124/1000 | Loss: 0.00000884
Iteration 125/1000 | Loss: 0.00000884
Iteration 126/1000 | Loss: 0.00000884
Iteration 127/1000 | Loss: 0.00000884
Iteration 128/1000 | Loss: 0.00000884
Iteration 129/1000 | Loss: 0.00000883
Iteration 130/1000 | Loss: 0.00000883
Iteration 131/1000 | Loss: 0.00000883
Iteration 132/1000 | Loss: 0.00000883
Iteration 133/1000 | Loss: 0.00000883
Iteration 134/1000 | Loss: 0.00000883
Iteration 135/1000 | Loss: 0.00000883
Iteration 136/1000 | Loss: 0.00000883
Iteration 137/1000 | Loss: 0.00000883
Iteration 138/1000 | Loss: 0.00000883
Iteration 139/1000 | Loss: 0.00000883
Iteration 140/1000 | Loss: 0.00000883
Iteration 141/1000 | Loss: 0.00000883
Iteration 142/1000 | Loss: 0.00000883
Iteration 143/1000 | Loss: 0.00000883
Iteration 144/1000 | Loss: 0.00000882
Iteration 145/1000 | Loss: 0.00000882
Iteration 146/1000 | Loss: 0.00000882
Iteration 147/1000 | Loss: 0.00000882
Iteration 148/1000 | Loss: 0.00000882
Iteration 149/1000 | Loss: 0.00000882
Iteration 150/1000 | Loss: 0.00000882
Iteration 151/1000 | Loss: 0.00000882
Iteration 152/1000 | Loss: 0.00000881
Iteration 153/1000 | Loss: 0.00000881
Iteration 154/1000 | Loss: 0.00000881
Iteration 155/1000 | Loss: 0.00000881
Iteration 156/1000 | Loss: 0.00000881
Iteration 157/1000 | Loss: 0.00000881
Iteration 158/1000 | Loss: 0.00000880
Iteration 159/1000 | Loss: 0.00000880
Iteration 160/1000 | Loss: 0.00000880
Iteration 161/1000 | Loss: 0.00000880
Iteration 162/1000 | Loss: 0.00000880
Iteration 163/1000 | Loss: 0.00000880
Iteration 164/1000 | Loss: 0.00000880
Iteration 165/1000 | Loss: 0.00000879
Iteration 166/1000 | Loss: 0.00000878
Iteration 167/1000 | Loss: 0.00000877
Iteration 168/1000 | Loss: 0.00000877
Iteration 169/1000 | Loss: 0.00000877
Iteration 170/1000 | Loss: 0.00000877
Iteration 171/1000 | Loss: 0.00000876
Iteration 172/1000 | Loss: 0.00000876
Iteration 173/1000 | Loss: 0.00000876
Iteration 174/1000 | Loss: 0.00000876
Iteration 175/1000 | Loss: 0.00000876
Iteration 176/1000 | Loss: 0.00000875
Iteration 177/1000 | Loss: 0.00000875
Iteration 178/1000 | Loss: 0.00000875
Iteration 179/1000 | Loss: 0.00000874
Iteration 180/1000 | Loss: 0.00000874
Iteration 181/1000 | Loss: 0.00000874
Iteration 182/1000 | Loss: 0.00000874
Iteration 183/1000 | Loss: 0.00000873
Iteration 184/1000 | Loss: 0.00000873
Iteration 185/1000 | Loss: 0.00000873
Iteration 186/1000 | Loss: 0.00000873
Iteration 187/1000 | Loss: 0.00000873
Iteration 188/1000 | Loss: 0.00000873
Iteration 189/1000 | Loss: 0.00000872
Iteration 190/1000 | Loss: 0.00000872
Iteration 191/1000 | Loss: 0.00000872
Iteration 192/1000 | Loss: 0.00000872
Iteration 193/1000 | Loss: 0.00000872
Iteration 194/1000 | Loss: 0.00000872
Iteration 195/1000 | Loss: 0.00000872
Iteration 196/1000 | Loss: 0.00000872
Iteration 197/1000 | Loss: 0.00000871
Iteration 198/1000 | Loss: 0.00000871
Iteration 199/1000 | Loss: 0.00000871
Iteration 200/1000 | Loss: 0.00000871
Iteration 201/1000 | Loss: 0.00000871
Iteration 202/1000 | Loss: 0.00000870
Iteration 203/1000 | Loss: 0.00000870
Iteration 204/1000 | Loss: 0.00000870
Iteration 205/1000 | Loss: 0.00000870
Iteration 206/1000 | Loss: 0.00000870
Iteration 207/1000 | Loss: 0.00000870
Iteration 208/1000 | Loss: 0.00000870
Iteration 209/1000 | Loss: 0.00000870
Iteration 210/1000 | Loss: 0.00000870
Iteration 211/1000 | Loss: 0.00000870
Iteration 212/1000 | Loss: 0.00000870
Iteration 213/1000 | Loss: 0.00000869
Iteration 214/1000 | Loss: 0.00000869
Iteration 215/1000 | Loss: 0.00000869
Iteration 216/1000 | Loss: 0.00000869
Iteration 217/1000 | Loss: 0.00000869
Iteration 218/1000 | Loss: 0.00000869
Iteration 219/1000 | Loss: 0.00000869
Iteration 220/1000 | Loss: 0.00000868
Iteration 221/1000 | Loss: 0.00000868
Iteration 222/1000 | Loss: 0.00000868
Iteration 223/1000 | Loss: 0.00000868
Iteration 224/1000 | Loss: 0.00000868
Iteration 225/1000 | Loss: 0.00000868
Iteration 226/1000 | Loss: 0.00000868
Iteration 227/1000 | Loss: 0.00000868
Iteration 228/1000 | Loss: 0.00000868
Iteration 229/1000 | Loss: 0.00000868
Iteration 230/1000 | Loss: 0.00000868
Iteration 231/1000 | Loss: 0.00000867
Iteration 232/1000 | Loss: 0.00000867
Iteration 233/1000 | Loss: 0.00000867
Iteration 234/1000 | Loss: 0.00000867
Iteration 235/1000 | Loss: 0.00000867
Iteration 236/1000 | Loss: 0.00000867
Iteration 237/1000 | Loss: 0.00000867
Iteration 238/1000 | Loss: 0.00000867
Iteration 239/1000 | Loss: 0.00000867
Iteration 240/1000 | Loss: 0.00000867
Iteration 241/1000 | Loss: 0.00000866
Iteration 242/1000 | Loss: 0.00000866
Iteration 243/1000 | Loss: 0.00000866
Iteration 244/1000 | Loss: 0.00000866
Iteration 245/1000 | Loss: 0.00000866
Iteration 246/1000 | Loss: 0.00000866
Iteration 247/1000 | Loss: 0.00000866
Iteration 248/1000 | Loss: 0.00000866
Iteration 249/1000 | Loss: 0.00000866
Iteration 250/1000 | Loss: 0.00000866
Iteration 251/1000 | Loss: 0.00000866
Iteration 252/1000 | Loss: 0.00000866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [8.661297215439845e-06, 8.661297215439845e-06, 8.661297215439845e-06, 8.661297215439845e-06, 8.661297215439845e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.661297215439845e-06

Optimization complete. Final v2v error: 2.522326946258545 mm

Highest mean error: 2.69032883644104 mm for frame 33

Lowest mean error: 2.368478775024414 mm for frame 241

Saving results

Total time: 49.327582597732544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771660
Iteration 2/25 | Loss: 0.00164985
Iteration 3/25 | Loss: 0.00133120
Iteration 4/25 | Loss: 0.00129361
Iteration 5/25 | Loss: 0.00126652
Iteration 6/25 | Loss: 0.00127364
Iteration 7/25 | Loss: 0.00128237
Iteration 8/25 | Loss: 0.00125534
Iteration 9/25 | Loss: 0.00124495
Iteration 10/25 | Loss: 0.00124441
Iteration 11/25 | Loss: 0.00124929
Iteration 12/25 | Loss: 0.00124149
Iteration 13/25 | Loss: 0.00123733
Iteration 14/25 | Loss: 0.00123101
Iteration 15/25 | Loss: 0.00122591
Iteration 16/25 | Loss: 0.00122510
Iteration 17/25 | Loss: 0.00122936
Iteration 18/25 | Loss: 0.00122441
Iteration 19/25 | Loss: 0.00122261
Iteration 20/25 | Loss: 0.00122119
Iteration 21/25 | Loss: 0.00122055
Iteration 22/25 | Loss: 0.00122015
Iteration 23/25 | Loss: 0.00122425
Iteration 24/25 | Loss: 0.00121867
Iteration 25/25 | Loss: 0.00121763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33462250
Iteration 2/25 | Loss: 0.00071392
Iteration 3/25 | Loss: 0.00071392
Iteration 4/25 | Loss: 0.00071392
Iteration 5/25 | Loss: 0.00071392
Iteration 6/25 | Loss: 0.00071392
Iteration 7/25 | Loss: 0.00071392
Iteration 8/25 | Loss: 0.00071392
Iteration 9/25 | Loss: 0.00071392
Iteration 10/25 | Loss: 0.00071392
Iteration 11/25 | Loss: 0.00071392
Iteration 12/25 | Loss: 0.00071392
Iteration 13/25 | Loss: 0.00071392
Iteration 14/25 | Loss: 0.00071392
Iteration 15/25 | Loss: 0.00071392
Iteration 16/25 | Loss: 0.00071392
Iteration 17/25 | Loss: 0.00071392
Iteration 18/25 | Loss: 0.00071392
Iteration 19/25 | Loss: 0.00071392
Iteration 20/25 | Loss: 0.00071392
Iteration 21/25 | Loss: 0.00071392
Iteration 22/25 | Loss: 0.00071392
Iteration 23/25 | Loss: 0.00071392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007139211520552635, 0.0007139211520552635, 0.0007139211520552635, 0.0007139211520552635, 0.0007139211520552635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007139211520552635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071392
Iteration 2/1000 | Loss: 0.00004893
Iteration 3/1000 | Loss: 0.00003137
Iteration 4/1000 | Loss: 0.00002828
Iteration 5/1000 | Loss: 0.00002672
Iteration 6/1000 | Loss: 0.00002597
Iteration 7/1000 | Loss: 0.00002536
Iteration 8/1000 | Loss: 0.00002487
Iteration 9/1000 | Loss: 0.00002444
Iteration 10/1000 | Loss: 0.00002415
Iteration 11/1000 | Loss: 0.00002388
Iteration 12/1000 | Loss: 0.00002370
Iteration 13/1000 | Loss: 0.00002349
Iteration 14/1000 | Loss: 0.00002332
Iteration 15/1000 | Loss: 0.00002326
Iteration 16/1000 | Loss: 0.00002323
Iteration 17/1000 | Loss: 0.00021162
Iteration 18/1000 | Loss: 0.00004740
Iteration 19/1000 | Loss: 0.00003050
Iteration 20/1000 | Loss: 0.00002443
Iteration 21/1000 | Loss: 0.00002347
Iteration 22/1000 | Loss: 0.00002280
Iteration 23/1000 | Loss: 0.00002247
Iteration 24/1000 | Loss: 0.00002230
Iteration 25/1000 | Loss: 0.00002230
Iteration 26/1000 | Loss: 0.00002229
Iteration 27/1000 | Loss: 0.00002227
Iteration 28/1000 | Loss: 0.00002226
Iteration 29/1000 | Loss: 0.00002223
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00002215
Iteration 32/1000 | Loss: 0.00002215
Iteration 33/1000 | Loss: 0.00002215
Iteration 34/1000 | Loss: 0.00002214
Iteration 35/1000 | Loss: 0.00002214
Iteration 36/1000 | Loss: 0.00002213
Iteration 37/1000 | Loss: 0.00002213
Iteration 38/1000 | Loss: 0.00002212
Iteration 39/1000 | Loss: 0.00002212
Iteration 40/1000 | Loss: 0.00002212
Iteration 41/1000 | Loss: 0.00002212
Iteration 42/1000 | Loss: 0.00002212
Iteration 43/1000 | Loss: 0.00002212
Iteration 44/1000 | Loss: 0.00002212
Iteration 45/1000 | Loss: 0.00002212
Iteration 46/1000 | Loss: 0.00002212
Iteration 47/1000 | Loss: 0.00002211
Iteration 48/1000 | Loss: 0.00002211
Iteration 49/1000 | Loss: 0.00002211
Iteration 50/1000 | Loss: 0.00002211
Iteration 51/1000 | Loss: 0.00002211
Iteration 52/1000 | Loss: 0.00002211
Iteration 53/1000 | Loss: 0.00002211
Iteration 54/1000 | Loss: 0.00002211
Iteration 55/1000 | Loss: 0.00002211
Iteration 56/1000 | Loss: 0.00002211
Iteration 57/1000 | Loss: 0.00002211
Iteration 58/1000 | Loss: 0.00002211
Iteration 59/1000 | Loss: 0.00002211
Iteration 60/1000 | Loss: 0.00002211
Iteration 61/1000 | Loss: 0.00002211
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [2.2113967133918777e-05, 2.2113967133918777e-05, 2.2113967133918777e-05, 2.2113967133918777e-05, 2.2113967133918777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2113967133918777e-05

Optimization complete. Final v2v error: 3.839359760284424 mm

Highest mean error: 4.636854648590088 mm for frame 94

Lowest mean error: 3.143146514892578 mm for frame 188

Saving results

Total time: 90.3537905216217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045798
Iteration 2/25 | Loss: 0.01045798
Iteration 3/25 | Loss: 0.01045798
Iteration 4/25 | Loss: 0.01045797
Iteration 5/25 | Loss: 0.01045797
Iteration 6/25 | Loss: 0.01045797
Iteration 7/25 | Loss: 0.01045797
Iteration 8/25 | Loss: 0.01045797
Iteration 9/25 | Loss: 0.01045797
Iteration 10/25 | Loss: 0.01045796
Iteration 11/25 | Loss: 0.01045796
Iteration 12/25 | Loss: 0.00270226
Iteration 13/25 | Loss: 0.00196441
Iteration 14/25 | Loss: 0.00187938
Iteration 15/25 | Loss: 0.00169194
Iteration 16/25 | Loss: 0.00155819
Iteration 17/25 | Loss: 0.00147155
Iteration 18/25 | Loss: 0.00142241
Iteration 19/25 | Loss: 0.00138698
Iteration 20/25 | Loss: 0.00134935
Iteration 21/25 | Loss: 0.00134976
Iteration 22/25 | Loss: 0.00132789
Iteration 23/25 | Loss: 0.00132259
Iteration 24/25 | Loss: 0.00132069
Iteration 25/25 | Loss: 0.00132311

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38883841
Iteration 2/25 | Loss: 0.00401020
Iteration 3/25 | Loss: 0.00374753
Iteration 4/25 | Loss: 0.00374753
Iteration 5/25 | Loss: 0.00374753
Iteration 6/25 | Loss: 0.00374753
Iteration 7/25 | Loss: 0.00374753
Iteration 8/25 | Loss: 0.00374753
Iteration 9/25 | Loss: 0.00374753
Iteration 10/25 | Loss: 0.00374753
Iteration 11/25 | Loss: 0.00374753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0037475326098501682, 0.0037475326098501682, 0.0037475326098501682, 0.0037475326098501682, 0.0037475326098501682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037475326098501682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374753
Iteration 2/1000 | Loss: 0.00502921
Iteration 3/1000 | Loss: 0.00398811
Iteration 4/1000 | Loss: 0.00336349
Iteration 5/1000 | Loss: 0.00515927
Iteration 6/1000 | Loss: 0.00506517
Iteration 7/1000 | Loss: 0.00443929
Iteration 8/1000 | Loss: 0.00563634
Iteration 9/1000 | Loss: 0.00570007
Iteration 10/1000 | Loss: 0.00483819
Iteration 11/1000 | Loss: 0.00518263
Iteration 12/1000 | Loss: 0.00541617
Iteration 13/1000 | Loss: 0.00660869
Iteration 14/1000 | Loss: 0.00533684
Iteration 15/1000 | Loss: 0.00599787
Iteration 16/1000 | Loss: 0.00510847
Iteration 17/1000 | Loss: 0.00555363
Iteration 18/1000 | Loss: 0.00614397
Iteration 19/1000 | Loss: 0.00475819
Iteration 20/1000 | Loss: 0.00431077
Iteration 21/1000 | Loss: 0.00416782
Iteration 22/1000 | Loss: 0.00464220
Iteration 23/1000 | Loss: 0.00654385
Iteration 24/1000 | Loss: 0.00459748
Iteration 25/1000 | Loss: 0.00712331
Iteration 26/1000 | Loss: 0.00534179
Iteration 27/1000 | Loss: 0.00653619
Iteration 28/1000 | Loss: 0.00448094
Iteration 29/1000 | Loss: 0.00439007
Iteration 30/1000 | Loss: 0.00400769
Iteration 31/1000 | Loss: 0.00458561
Iteration 32/1000 | Loss: 0.00475639
Iteration 33/1000 | Loss: 0.00411807
Iteration 34/1000 | Loss: 0.00581827
Iteration 35/1000 | Loss: 0.00492738
Iteration 36/1000 | Loss: 0.00500272
Iteration 37/1000 | Loss: 0.00509431
Iteration 38/1000 | Loss: 0.00638115
Iteration 39/1000 | Loss: 0.00404041
Iteration 40/1000 | Loss: 0.00470749
Iteration 41/1000 | Loss: 0.00484789
Iteration 42/1000 | Loss: 0.00600926
Iteration 43/1000 | Loss: 0.00446999
Iteration 44/1000 | Loss: 0.00451411
Iteration 45/1000 | Loss: 0.00533820
Iteration 46/1000 | Loss: 0.00428432
Iteration 47/1000 | Loss: 0.00446727
Iteration 48/1000 | Loss: 0.00471475
Iteration 49/1000 | Loss: 0.00529404
Iteration 50/1000 | Loss: 0.00569747
Iteration 51/1000 | Loss: 0.00595762
Iteration 52/1000 | Loss: 0.00509643
Iteration 53/1000 | Loss: 0.00371451
Iteration 54/1000 | Loss: 0.00383999
Iteration 55/1000 | Loss: 0.00556239
Iteration 56/1000 | Loss: 0.00480995
Iteration 57/1000 | Loss: 0.00439469
Iteration 58/1000 | Loss: 0.00440575
Iteration 59/1000 | Loss: 0.00511464
Iteration 60/1000 | Loss: 0.00429689
Iteration 61/1000 | Loss: 0.00489212
Iteration 62/1000 | Loss: 0.00376999
Iteration 63/1000 | Loss: 0.00366327
Iteration 64/1000 | Loss: 0.00602831
Iteration 65/1000 | Loss: 0.00452620
Iteration 66/1000 | Loss: 0.00347008
Iteration 67/1000 | Loss: 0.00430307
Iteration 68/1000 | Loss: 0.00452043
Iteration 69/1000 | Loss: 0.00327372
Iteration 70/1000 | Loss: 0.00407929
Iteration 71/1000 | Loss: 0.00393346
Iteration 72/1000 | Loss: 0.00350958
Iteration 73/1000 | Loss: 0.00354696
Iteration 74/1000 | Loss: 0.00349222
Iteration 75/1000 | Loss: 0.00357657
Iteration 76/1000 | Loss: 0.00376962
Iteration 77/1000 | Loss: 0.00400648
Iteration 78/1000 | Loss: 0.00429862
Iteration 79/1000 | Loss: 0.00455090
Iteration 80/1000 | Loss: 0.00439663
Iteration 81/1000 | Loss: 0.00444920
Iteration 82/1000 | Loss: 0.00490577
Iteration 83/1000 | Loss: 0.00434530
Iteration 84/1000 | Loss: 0.00511817
Iteration 85/1000 | Loss: 0.00429303
Iteration 86/1000 | Loss: 0.00374883
Iteration 87/1000 | Loss: 0.00439149
Iteration 88/1000 | Loss: 0.00374840
Iteration 89/1000 | Loss: 0.00464273
Iteration 90/1000 | Loss: 0.00393952
Iteration 91/1000 | Loss: 0.00422580
Iteration 92/1000 | Loss: 0.00535462
Iteration 93/1000 | Loss: 0.00599311
Iteration 94/1000 | Loss: 0.00445387
Iteration 95/1000 | Loss: 0.00688969
Iteration 96/1000 | Loss: 0.00421723
Iteration 97/1000 | Loss: 0.00445439
Iteration 98/1000 | Loss: 0.00592521
Iteration 99/1000 | Loss: 0.00443869
Iteration 100/1000 | Loss: 0.00444894
Iteration 101/1000 | Loss: 0.00433686
Iteration 102/1000 | Loss: 0.00650481
Iteration 103/1000 | Loss: 0.00427083
Iteration 104/1000 | Loss: 0.00387735
Iteration 105/1000 | Loss: 0.00376953
Iteration 106/1000 | Loss: 0.00540966
Iteration 107/1000 | Loss: 0.00457135
Iteration 108/1000 | Loss: 0.00514042
Iteration 109/1000 | Loss: 0.00477507
Iteration 110/1000 | Loss: 0.00368804
Iteration 111/1000 | Loss: 0.00368745
Iteration 112/1000 | Loss: 0.00301578
Iteration 113/1000 | Loss: 0.00368869
Iteration 114/1000 | Loss: 0.00396814
Iteration 115/1000 | Loss: 0.00410578
Iteration 116/1000 | Loss: 0.00430501
Iteration 117/1000 | Loss: 0.00422025
Iteration 118/1000 | Loss: 0.00402645
Iteration 119/1000 | Loss: 0.00415131
Iteration 120/1000 | Loss: 0.00445607
Iteration 121/1000 | Loss: 0.00481029
Iteration 122/1000 | Loss: 0.00312616
Iteration 123/1000 | Loss: 0.00327893
Iteration 124/1000 | Loss: 0.00340378
Iteration 125/1000 | Loss: 0.00489083
Iteration 126/1000 | Loss: 0.00365079
Iteration 127/1000 | Loss: 0.00375495
Iteration 128/1000 | Loss: 0.00501716
Iteration 129/1000 | Loss: 0.00381425
Iteration 130/1000 | Loss: 0.00363463
Iteration 131/1000 | Loss: 0.00386671
Iteration 132/1000 | Loss: 0.00391487
Iteration 133/1000 | Loss: 0.00399716
Iteration 134/1000 | Loss: 0.00400494
Iteration 135/1000 | Loss: 0.00491685
Iteration 136/1000 | Loss: 0.00378573
Iteration 137/1000 | Loss: 0.00525147
Iteration 138/1000 | Loss: 0.00534911
Iteration 139/1000 | Loss: 0.00449795
Iteration 140/1000 | Loss: 0.00407581
Iteration 141/1000 | Loss: 0.00373840
Iteration 142/1000 | Loss: 0.00371398
Iteration 143/1000 | Loss: 0.00399992
Iteration 144/1000 | Loss: 0.00481148
Iteration 145/1000 | Loss: 0.00381059
Iteration 146/1000 | Loss: 0.00414348
Iteration 147/1000 | Loss: 0.00393728
Iteration 148/1000 | Loss: 0.00485811
Iteration 149/1000 | Loss: 0.00341342
Iteration 150/1000 | Loss: 0.00790205
Iteration 151/1000 | Loss: 0.00471791
Iteration 152/1000 | Loss: 0.00476620
Iteration 153/1000 | Loss: 0.00307347
Iteration 154/1000 | Loss: 0.00307169
Iteration 155/1000 | Loss: 0.00300087
Iteration 156/1000 | Loss: 0.00353303
Iteration 157/1000 | Loss: 0.00379625
Iteration 158/1000 | Loss: 0.00383503
Iteration 159/1000 | Loss: 0.00382780
Iteration 160/1000 | Loss: 0.00332410
Iteration 161/1000 | Loss: 0.00364892
Iteration 162/1000 | Loss: 0.00343802
Iteration 163/1000 | Loss: 0.00394216
Iteration 164/1000 | Loss: 0.00361512
Iteration 165/1000 | Loss: 0.00327458
Iteration 166/1000 | Loss: 0.00282459
Iteration 167/1000 | Loss: 0.00343581
Iteration 168/1000 | Loss: 0.00277894
Iteration 169/1000 | Loss: 0.00400552
Iteration 170/1000 | Loss: 0.00226619
Iteration 171/1000 | Loss: 0.00296319
Iteration 172/1000 | Loss: 0.00407511
Iteration 173/1000 | Loss: 0.00358092
Iteration 174/1000 | Loss: 0.00344830
Iteration 175/1000 | Loss: 0.00355072
Iteration 176/1000 | Loss: 0.00357960
Iteration 177/1000 | Loss: 0.00366633
Iteration 178/1000 | Loss: 0.00336567
Iteration 179/1000 | Loss: 0.00382182
Iteration 180/1000 | Loss: 0.00353716
Iteration 181/1000 | Loss: 0.00367709
Iteration 182/1000 | Loss: 0.00380605
Iteration 183/1000 | Loss: 0.00399014
Iteration 184/1000 | Loss: 0.00379012
Iteration 185/1000 | Loss: 0.00273753
Iteration 186/1000 | Loss: 0.00209240
Iteration 187/1000 | Loss: 0.00255781
Iteration 188/1000 | Loss: 0.00292549
Iteration 189/1000 | Loss: 0.00332828
Iteration 190/1000 | Loss: 0.00323532
Iteration 191/1000 | Loss: 0.00322980
Iteration 192/1000 | Loss: 0.00395279
Iteration 193/1000 | Loss: 0.00312446
Iteration 194/1000 | Loss: 0.00195395
Iteration 195/1000 | Loss: 0.00292599
Iteration 196/1000 | Loss: 0.00292673
Iteration 197/1000 | Loss: 0.00343702
Iteration 198/1000 | Loss: 0.00314355
Iteration 199/1000 | Loss: 0.00288616
Iteration 200/1000 | Loss: 0.00484654
Iteration 201/1000 | Loss: 0.00298783
Iteration 202/1000 | Loss: 0.00297484
Iteration 203/1000 | Loss: 0.00277301
Iteration 204/1000 | Loss: 0.00294657
Iteration 205/1000 | Loss: 0.00463351
Iteration 206/1000 | Loss: 0.00404564
Iteration 207/1000 | Loss: 0.00366464
Iteration 208/1000 | Loss: 0.00346848
Iteration 209/1000 | Loss: 0.00308120
Iteration 210/1000 | Loss: 0.00334274
Iteration 211/1000 | Loss: 0.00325193
Iteration 212/1000 | Loss: 0.00372715
Iteration 213/1000 | Loss: 0.00417877
Iteration 214/1000 | Loss: 0.00459703
Iteration 215/1000 | Loss: 0.00288965
Iteration 216/1000 | Loss: 0.00288030
Iteration 217/1000 | Loss: 0.00310065
Iteration 218/1000 | Loss: 0.00341713
Iteration 219/1000 | Loss: 0.00333764
Iteration 220/1000 | Loss: 0.00451291
Iteration 221/1000 | Loss: 0.00334443
Iteration 222/1000 | Loss: 0.00356836
Iteration 223/1000 | Loss: 0.00393329
Iteration 224/1000 | Loss: 0.00398092
Iteration 225/1000 | Loss: 0.00411036
Iteration 226/1000 | Loss: 0.00315217
Iteration 227/1000 | Loss: 0.00287826
Iteration 228/1000 | Loss: 0.00315523
Iteration 229/1000 | Loss: 0.00297656
Iteration 230/1000 | Loss: 0.00312584
Iteration 231/1000 | Loss: 0.00391799
Iteration 232/1000 | Loss: 0.00426942
Iteration 233/1000 | Loss: 0.00397242
Iteration 234/1000 | Loss: 0.00239793
Iteration 235/1000 | Loss: 0.00307976
Iteration 236/1000 | Loss: 0.00374748
Iteration 237/1000 | Loss: 0.00309452
Iteration 238/1000 | Loss: 0.00377798
Iteration 239/1000 | Loss: 0.00343077
Iteration 240/1000 | Loss: 0.00262957
Iteration 241/1000 | Loss: 0.00308190
Iteration 242/1000 | Loss: 0.00317769
Iteration 243/1000 | Loss: 0.00295748
Iteration 244/1000 | Loss: 0.00347976
Iteration 245/1000 | Loss: 0.00232712
Iteration 246/1000 | Loss: 0.00232265
Iteration 247/1000 | Loss: 0.00208380
Iteration 248/1000 | Loss: 0.00236734
Iteration 249/1000 | Loss: 0.00333411
Iteration 250/1000 | Loss: 0.00330948
Iteration 251/1000 | Loss: 0.00252433
Iteration 252/1000 | Loss: 0.00282432
Iteration 253/1000 | Loss: 0.00249567
Iteration 254/1000 | Loss: 0.00266438
Iteration 255/1000 | Loss: 0.00299035
Iteration 256/1000 | Loss: 0.00509288
Iteration 257/1000 | Loss: 0.00253732
Iteration 258/1000 | Loss: 0.00258944
Iteration 259/1000 | Loss: 0.00239092
Iteration 260/1000 | Loss: 0.00247592
Iteration 261/1000 | Loss: 0.00251489
Iteration 262/1000 | Loss: 0.00226287
Iteration 263/1000 | Loss: 0.00247512
Iteration 264/1000 | Loss: 0.00260107
Iteration 265/1000 | Loss: 0.00278005
Iteration 266/1000 | Loss: 0.00249614
Iteration 267/1000 | Loss: 0.00271477
Iteration 268/1000 | Loss: 0.00253426
Iteration 269/1000 | Loss: 0.00249613
Iteration 270/1000 | Loss: 0.00160771
Iteration 271/1000 | Loss: 0.00183949
Iteration 272/1000 | Loss: 0.00220824
Iteration 273/1000 | Loss: 0.00247003
Iteration 274/1000 | Loss: 0.00196390
Iteration 275/1000 | Loss: 0.00374729
Iteration 276/1000 | Loss: 0.00185986
Iteration 277/1000 | Loss: 0.00178369
Iteration 278/1000 | Loss: 0.00247138
Iteration 279/1000 | Loss: 0.00305144
Iteration 280/1000 | Loss: 0.00314506
Iteration 281/1000 | Loss: 0.00262881
Iteration 282/1000 | Loss: 0.00286587
Iteration 283/1000 | Loss: 0.00359827
Iteration 284/1000 | Loss: 0.00246216
Iteration 285/1000 | Loss: 0.00232381
Iteration 286/1000 | Loss: 0.00268455
Iteration 287/1000 | Loss: 0.00257203
Iteration 288/1000 | Loss: 0.00225260
Iteration 289/1000 | Loss: 0.00229184
Iteration 290/1000 | Loss: 0.00210148
Iteration 291/1000 | Loss: 0.00310236
Iteration 292/1000 | Loss: 0.00292785
Iteration 293/1000 | Loss: 0.00246882
Iteration 294/1000 | Loss: 0.00307128
Iteration 295/1000 | Loss: 0.00259933
Iteration 296/1000 | Loss: 0.00209247
Iteration 297/1000 | Loss: 0.00197643
Iteration 298/1000 | Loss: 0.00220805
Iteration 299/1000 | Loss: 0.00204937
Iteration 300/1000 | Loss: 0.00180095
Iteration 301/1000 | Loss: 0.00191742
Iteration 302/1000 | Loss: 0.00217326
Iteration 303/1000 | Loss: 0.00196542
Iteration 304/1000 | Loss: 0.00268936
Iteration 305/1000 | Loss: 0.00220828
Iteration 306/1000 | Loss: 0.00311445
Iteration 307/1000 | Loss: 0.00336739
Iteration 308/1000 | Loss: 0.00198645
Iteration 309/1000 | Loss: 0.00223018
Iteration 310/1000 | Loss: 0.00230362
Iteration 311/1000 | Loss: 0.00204794
Iteration 312/1000 | Loss: 0.00242682
Iteration 313/1000 | Loss: 0.00212042
Iteration 314/1000 | Loss: 0.00201265
Iteration 315/1000 | Loss: 0.00213729
Iteration 316/1000 | Loss: 0.00217154
Iteration 317/1000 | Loss: 0.00203532
Iteration 318/1000 | Loss: 0.00201328
Iteration 319/1000 | Loss: 0.00206714
Iteration 320/1000 | Loss: 0.00206303
Iteration 321/1000 | Loss: 0.00286525
Iteration 322/1000 | Loss: 0.00412816
Iteration 323/1000 | Loss: 0.00576982
Iteration 324/1000 | Loss: 0.00374951
Iteration 325/1000 | Loss: 0.00574160
Iteration 326/1000 | Loss: 0.00336513
Iteration 327/1000 | Loss: 0.00192993
Iteration 328/1000 | Loss: 0.00245769
Iteration 329/1000 | Loss: 0.00214924
Iteration 330/1000 | Loss: 0.00204163
Iteration 331/1000 | Loss: 0.00194883
Iteration 332/1000 | Loss: 0.00195794
Iteration 333/1000 | Loss: 0.00203402
Iteration 334/1000 | Loss: 0.00182638
Iteration 335/1000 | Loss: 0.00205095
Iteration 336/1000 | Loss: 0.00205965
Iteration 337/1000 | Loss: 0.00213706
Iteration 338/1000 | Loss: 0.00226554
Iteration 339/1000 | Loss: 0.00212498
Iteration 340/1000 | Loss: 0.00283668
Iteration 341/1000 | Loss: 0.00175492
Iteration 342/1000 | Loss: 0.00235149
Iteration 343/1000 | Loss: 0.00184311
Iteration 344/1000 | Loss: 0.00283157
Iteration 345/1000 | Loss: 0.00339124
Iteration 346/1000 | Loss: 0.00246729
Iteration 347/1000 | Loss: 0.00273816
Iteration 348/1000 | Loss: 0.00285187
Iteration 349/1000 | Loss: 0.00303031
Iteration 350/1000 | Loss: 0.00222677
Iteration 351/1000 | Loss: 0.00222074
Iteration 352/1000 | Loss: 0.00173497
Iteration 353/1000 | Loss: 0.00190268
Iteration 354/1000 | Loss: 0.00251844
Iteration 355/1000 | Loss: 0.00216764
Iteration 356/1000 | Loss: 0.00194309
Iteration 357/1000 | Loss: 0.00213028
Iteration 358/1000 | Loss: 0.00219405
Iteration 359/1000 | Loss: 0.00163963
Iteration 360/1000 | Loss: 0.00147145
Iteration 361/1000 | Loss: 0.00173714
Iteration 362/1000 | Loss: 0.00182885
Iteration 363/1000 | Loss: 0.00204868
Iteration 364/1000 | Loss: 0.00197664
Iteration 365/1000 | Loss: 0.00206064
Iteration 366/1000 | Loss: 0.00204100
Iteration 367/1000 | Loss: 0.00218037
Iteration 368/1000 | Loss: 0.00196831
Iteration 369/1000 | Loss: 0.00193856
Iteration 370/1000 | Loss: 0.00258927
Iteration 371/1000 | Loss: 0.00178723
Iteration 372/1000 | Loss: 0.00243815
Iteration 373/1000 | Loss: 0.00169386
Iteration 374/1000 | Loss: 0.00208717
Iteration 375/1000 | Loss: 0.00239760
Iteration 376/1000 | Loss: 0.00197275
Iteration 377/1000 | Loss: 0.00199383
Iteration 378/1000 | Loss: 0.00261278
Iteration 379/1000 | Loss: 0.00235550
Iteration 380/1000 | Loss: 0.00360487
Iteration 381/1000 | Loss: 0.00260483
Iteration 382/1000 | Loss: 0.00242103
Iteration 383/1000 | Loss: 0.00233721
Iteration 384/1000 | Loss: 0.00199677
Iteration 385/1000 | Loss: 0.00245234
Iteration 386/1000 | Loss: 0.00264992
Iteration 387/1000 | Loss: 0.00245207
Iteration 388/1000 | Loss: 0.00269222
Iteration 389/1000 | Loss: 0.00241281
Iteration 390/1000 | Loss: 0.00212202
Iteration 391/1000 | Loss: 0.00243624
Iteration 392/1000 | Loss: 0.00232994
Iteration 393/1000 | Loss: 0.00210060
Iteration 394/1000 | Loss: 0.00165640
Iteration 395/1000 | Loss: 0.00172678
Iteration 396/1000 | Loss: 0.00203836
Iteration 397/1000 | Loss: 0.00259690
Iteration 398/1000 | Loss: 0.00460174
Iteration 399/1000 | Loss: 0.00509022
Iteration 400/1000 | Loss: 0.00308960
Iteration 401/1000 | Loss: 0.00477842
Iteration 402/1000 | Loss: 0.00466851
Iteration 403/1000 | Loss: 0.00569553
Iteration 404/1000 | Loss: 0.00448072
Iteration 405/1000 | Loss: 0.00676860
Iteration 406/1000 | Loss: 0.00276811
Iteration 407/1000 | Loss: 0.00142265
Iteration 408/1000 | Loss: 0.00140883
Iteration 409/1000 | Loss: 0.00145343
Iteration 410/1000 | Loss: 0.00160871
Iteration 411/1000 | Loss: 0.00255750
Iteration 412/1000 | Loss: 0.00145600
Iteration 413/1000 | Loss: 0.00190115
Iteration 414/1000 | Loss: 0.00206957
Iteration 415/1000 | Loss: 0.00165615
Iteration 416/1000 | Loss: 0.00180523
Iteration 417/1000 | Loss: 0.00195648
Iteration 418/1000 | Loss: 0.00197702
Iteration 419/1000 | Loss: 0.00183669
Iteration 420/1000 | Loss: 0.00220087
Iteration 421/1000 | Loss: 0.00235357
Iteration 422/1000 | Loss: 0.00149619
Iteration 423/1000 | Loss: 0.00177128
Iteration 424/1000 | Loss: 0.00157560
Iteration 425/1000 | Loss: 0.00171905
Iteration 426/1000 | Loss: 0.00127733
Iteration 427/1000 | Loss: 0.00126881
Iteration 428/1000 | Loss: 0.00217700
Iteration 429/1000 | Loss: 0.00173716
Iteration 430/1000 | Loss: 0.00134862
Iteration 431/1000 | Loss: 0.00173563
Iteration 432/1000 | Loss: 0.00183777
Iteration 433/1000 | Loss: 0.00155413
Iteration 434/1000 | Loss: 0.00124802
Iteration 435/1000 | Loss: 0.00143425
Iteration 436/1000 | Loss: 0.00208569
Iteration 437/1000 | Loss: 0.00169306
Iteration 438/1000 | Loss: 0.00179064
Iteration 439/1000 | Loss: 0.00156486
Iteration 440/1000 | Loss: 0.00167873
Iteration 441/1000 | Loss: 0.00154820
Iteration 442/1000 | Loss: 0.00125738
Iteration 443/1000 | Loss: 0.00117548
Iteration 444/1000 | Loss: 0.00106447
Iteration 445/1000 | Loss: 0.00122067
Iteration 446/1000 | Loss: 0.00181423
Iteration 447/1000 | Loss: 0.00152525
Iteration 448/1000 | Loss: 0.00174710
Iteration 449/1000 | Loss: 0.00159025
Iteration 450/1000 | Loss: 0.00218244
Iteration 451/1000 | Loss: 0.00175497
Iteration 452/1000 | Loss: 0.00155057
Iteration 453/1000 | Loss: 0.00155168
Iteration 454/1000 | Loss: 0.00095357
Iteration 455/1000 | Loss: 0.00094221
Iteration 456/1000 | Loss: 0.00084924
Iteration 457/1000 | Loss: 0.00123701
Iteration 458/1000 | Loss: 0.00167333
Iteration 459/1000 | Loss: 0.00137351
Iteration 460/1000 | Loss: 0.00143838
Iteration 461/1000 | Loss: 0.00173816
Iteration 462/1000 | Loss: 0.00102815
Iteration 463/1000 | Loss: 0.00095134
Iteration 464/1000 | Loss: 0.00094957
Iteration 465/1000 | Loss: 0.00087453
Iteration 466/1000 | Loss: 0.00090807
Iteration 467/1000 | Loss: 0.00092721
Iteration 468/1000 | Loss: 0.00093937
Iteration 469/1000 | Loss: 0.00096771
Iteration 470/1000 | Loss: 0.00104166
Iteration 471/1000 | Loss: 0.00140848
Iteration 472/1000 | Loss: 0.00119416
Iteration 473/1000 | Loss: 0.00085668
Iteration 474/1000 | Loss: 0.00096052
Iteration 475/1000 | Loss: 0.00118330
Iteration 476/1000 | Loss: 0.00194497
Iteration 477/1000 | Loss: 0.00075013
Iteration 478/1000 | Loss: 0.00082293
Iteration 479/1000 | Loss: 0.00097116
Iteration 480/1000 | Loss: 0.00101755
Iteration 481/1000 | Loss: 0.00098755
Iteration 482/1000 | Loss: 0.00107338
Iteration 483/1000 | Loss: 0.00128800
Iteration 484/1000 | Loss: 0.00066255
Iteration 485/1000 | Loss: 0.00117609
Iteration 486/1000 | Loss: 0.00150934
Iteration 487/1000 | Loss: 0.00093909
Iteration 488/1000 | Loss: 0.00110862
Iteration 489/1000 | Loss: 0.00082400
Iteration 490/1000 | Loss: 0.00083346
Iteration 491/1000 | Loss: 0.00093689
Iteration 492/1000 | Loss: 0.00089621
Iteration 493/1000 | Loss: 0.00091444
Iteration 494/1000 | Loss: 0.00092692
Iteration 495/1000 | Loss: 0.00089863
Iteration 496/1000 | Loss: 0.00152937
Iteration 497/1000 | Loss: 0.00126814
Iteration 498/1000 | Loss: 0.00065770
Iteration 499/1000 | Loss: 0.00086696
Iteration 500/1000 | Loss: 0.00082801
Iteration 501/1000 | Loss: 0.00192670
Iteration 502/1000 | Loss: 0.00088699
Iteration 503/1000 | Loss: 0.00093139
Iteration 504/1000 | Loss: 0.00101669
Iteration 505/1000 | Loss: 0.00120566
Iteration 506/1000 | Loss: 0.00089797
Iteration 507/1000 | Loss: 0.00101699
Iteration 508/1000 | Loss: 0.00126900
Iteration 509/1000 | Loss: 0.00102971
Iteration 510/1000 | Loss: 0.00138686
Iteration 511/1000 | Loss: 0.00128983
Iteration 512/1000 | Loss: 0.00086274
Iteration 513/1000 | Loss: 0.00097259
Iteration 514/1000 | Loss: 0.00081786
Iteration 515/1000 | Loss: 0.00108267
Iteration 516/1000 | Loss: 0.00067936
Iteration 517/1000 | Loss: 0.00059939
Iteration 518/1000 | Loss: 0.00119962
Iteration 519/1000 | Loss: 0.00070906
Iteration 520/1000 | Loss: 0.00106077
Iteration 521/1000 | Loss: 0.00076288
Iteration 522/1000 | Loss: 0.00102349
Iteration 523/1000 | Loss: 0.00079392
Iteration 524/1000 | Loss: 0.00138313
Iteration 525/1000 | Loss: 0.00070495
Iteration 526/1000 | Loss: 0.00065763
Iteration 527/1000 | Loss: 0.00077133
Iteration 528/1000 | Loss: 0.00084632
Iteration 529/1000 | Loss: 0.00066845
Iteration 530/1000 | Loss: 0.00066050
Iteration 531/1000 | Loss: 0.00072263
Iteration 532/1000 | Loss: 0.00083609
Iteration 533/1000 | Loss: 0.00081586
Iteration 534/1000 | Loss: 0.00088708
Iteration 535/1000 | Loss: 0.00081909
Iteration 536/1000 | Loss: 0.00093259
Iteration 537/1000 | Loss: 0.00084937
Iteration 538/1000 | Loss: 0.00103016
Iteration 539/1000 | Loss: 0.00095215
Iteration 540/1000 | Loss: 0.00153996
Iteration 541/1000 | Loss: 0.00097083
Iteration 542/1000 | Loss: 0.00084422
Iteration 543/1000 | Loss: 0.00084858
Iteration 544/1000 | Loss: 0.00132818
Iteration 545/1000 | Loss: 0.00082684
Iteration 546/1000 | Loss: 0.00069958
Iteration 547/1000 | Loss: 0.00092262
Iteration 548/1000 | Loss: 0.00129133
Iteration 549/1000 | Loss: 0.00100113
Iteration 550/1000 | Loss: 0.00127027
Iteration 551/1000 | Loss: 0.00102823
Iteration 552/1000 | Loss: 0.00118683
Iteration 553/1000 | Loss: 0.00104840
Iteration 554/1000 | Loss: 0.00109463
Iteration 555/1000 | Loss: 0.00099341
Iteration 556/1000 | Loss: 0.00102289
Iteration 557/1000 | Loss: 0.00088440
Iteration 558/1000 | Loss: 0.00100996
Iteration 559/1000 | Loss: 0.00086982
Iteration 560/1000 | Loss: 0.00101054
Iteration 561/1000 | Loss: 0.00079174
Iteration 562/1000 | Loss: 0.00105723
Iteration 563/1000 | Loss: 0.00118780
Iteration 564/1000 | Loss: 0.00084890
Iteration 565/1000 | Loss: 0.00064668
Iteration 566/1000 | Loss: 0.00093446
Iteration 567/1000 | Loss: 0.00086189
Iteration 568/1000 | Loss: 0.00085869
Iteration 569/1000 | Loss: 0.00097682
Iteration 570/1000 | Loss: 0.00078123
Iteration 571/1000 | Loss: 0.00075725
Iteration 572/1000 | Loss: 0.00117592
Iteration 573/1000 | Loss: 0.00078303
Iteration 574/1000 | Loss: 0.00101794
Iteration 575/1000 | Loss: 0.00078324
Iteration 576/1000 | Loss: 0.00076177
Iteration 577/1000 | Loss: 0.00073324
Iteration 578/1000 | Loss: 0.00079130
Iteration 579/1000 | Loss: 0.00131088
Iteration 580/1000 | Loss: 0.00097808
Iteration 581/1000 | Loss: 0.00080965
Iteration 582/1000 | Loss: 0.00079374
Iteration 583/1000 | Loss: 0.00081132
Iteration 584/1000 | Loss: 0.00074607
Iteration 585/1000 | Loss: 0.00078222
Iteration 586/1000 | Loss: 0.00074507
Iteration 587/1000 | Loss: 0.00081705
Iteration 588/1000 | Loss: 0.00085566
Iteration 589/1000 | Loss: 0.00078562
Iteration 590/1000 | Loss: 0.00078490
Iteration 591/1000 | Loss: 0.00073025
Iteration 592/1000 | Loss: 0.00078098
Iteration 593/1000 | Loss: 0.00093224
Iteration 594/1000 | Loss: 0.00101885
Iteration 595/1000 | Loss: 0.00087155
Iteration 596/1000 | Loss: 0.00085987
Iteration 597/1000 | Loss: 0.00125878
Iteration 598/1000 | Loss: 0.00088073
Iteration 599/1000 | Loss: 0.00075954
Iteration 600/1000 | Loss: 0.00071047
Iteration 601/1000 | Loss: 0.00107888
Iteration 602/1000 | Loss: 0.00112121
Iteration 603/1000 | Loss: 0.00093447
Iteration 604/1000 | Loss: 0.00089456
Iteration 605/1000 | Loss: 0.00082413
Iteration 606/1000 | Loss: 0.00108478
Iteration 607/1000 | Loss: 0.00096511
Iteration 608/1000 | Loss: 0.00081199
Iteration 609/1000 | Loss: 0.00098563
Iteration 610/1000 | Loss: 0.00074550
Iteration 611/1000 | Loss: 0.00072139
Iteration 612/1000 | Loss: 0.00076879
Iteration 613/1000 | Loss: 0.00106504
Iteration 614/1000 | Loss: 0.00062504
Iteration 615/1000 | Loss: 0.00076136
Iteration 616/1000 | Loss: 0.00086014
Iteration 617/1000 | Loss: 0.00080455
Iteration 618/1000 | Loss: 0.00095231
Iteration 619/1000 | Loss: 0.00082650
Iteration 620/1000 | Loss: 0.00065187
Iteration 621/1000 | Loss: 0.00062025
Iteration 622/1000 | Loss: 0.00066651
Iteration 623/1000 | Loss: 0.00065321
Iteration 624/1000 | Loss: 0.00089549
Iteration 625/1000 | Loss: 0.00074451
Iteration 626/1000 | Loss: 0.00070702
Iteration 627/1000 | Loss: 0.00077258
Iteration 628/1000 | Loss: 0.00064424
Iteration 629/1000 | Loss: 0.00096663
Iteration 630/1000 | Loss: 0.00089506
Iteration 631/1000 | Loss: 0.00084894
Iteration 632/1000 | Loss: 0.00109334
Iteration 633/1000 | Loss: 0.00087492
Iteration 634/1000 | Loss: 0.00090862
Iteration 635/1000 | Loss: 0.00093337
Iteration 636/1000 | Loss: 0.00109643
Iteration 637/1000 | Loss: 0.00123751
Iteration 638/1000 | Loss: 0.00092367
Iteration 639/1000 | Loss: 0.00062747
Iteration 640/1000 | Loss: 0.00083019
Iteration 641/1000 | Loss: 0.00076597
Iteration 642/1000 | Loss: 0.00086407
Iteration 643/1000 | Loss: 0.00084150
Iteration 644/1000 | Loss: 0.00076422
Iteration 645/1000 | Loss: 0.00088872
Iteration 646/1000 | Loss: 0.00099913
Iteration 647/1000 | Loss: 0.00070946
Iteration 648/1000 | Loss: 0.00099648
Iteration 649/1000 | Loss: 0.00117919
Iteration 650/1000 | Loss: 0.00101614
Iteration 651/1000 | Loss: 0.00066069
Iteration 652/1000 | Loss: 0.00097642
Iteration 653/1000 | Loss: 0.00081868
Iteration 654/1000 | Loss: 0.00094697
Iteration 655/1000 | Loss: 0.00083124
Iteration 656/1000 | Loss: 0.00119524
Iteration 657/1000 | Loss: 0.00089863
Iteration 658/1000 | Loss: 0.00046246
Iteration 659/1000 | Loss: 0.00064351
Iteration 660/1000 | Loss: 0.00082811
Iteration 661/1000 | Loss: 0.00076517
Iteration 662/1000 | Loss: 0.00071106
Iteration 663/1000 | Loss: 0.00060299
Iteration 664/1000 | Loss: 0.00108894
Iteration 665/1000 | Loss: 0.00067067
Iteration 666/1000 | Loss: 0.00055330
Iteration 667/1000 | Loss: 0.00044972
Iteration 668/1000 | Loss: 0.00055692
Iteration 669/1000 | Loss: 0.00053795
Iteration 670/1000 | Loss: 0.00053021
Iteration 671/1000 | Loss: 0.00065313
Iteration 672/1000 | Loss: 0.00085717
Iteration 673/1000 | Loss: 0.00054542
Iteration 674/1000 | Loss: 0.00064224
Iteration 675/1000 | Loss: 0.00058684
Iteration 676/1000 | Loss: 0.00057437
Iteration 677/1000 | Loss: 0.00053038
Iteration 678/1000 | Loss: 0.00050820
Iteration 679/1000 | Loss: 0.00064683
Iteration 680/1000 | Loss: 0.00071689
Iteration 681/1000 | Loss: 0.00053544
Iteration 682/1000 | Loss: 0.00096294
Iteration 683/1000 | Loss: 0.00051247
Iteration 684/1000 | Loss: 0.00047592
Iteration 685/1000 | Loss: 0.00049886
Iteration 686/1000 | Loss: 0.00049932
Iteration 687/1000 | Loss: 0.00069547
Iteration 688/1000 | Loss: 0.00065725
Iteration 689/1000 | Loss: 0.00056324
Iteration 690/1000 | Loss: 0.00086430
Iteration 691/1000 | Loss: 0.00047141
Iteration 692/1000 | Loss: 0.00044117
Iteration 693/1000 | Loss: 0.00042274
Iteration 694/1000 | Loss: 0.00053836
Iteration 695/1000 | Loss: 0.00053762
Iteration 696/1000 | Loss: 0.00075945
Iteration 697/1000 | Loss: 0.00074322
Iteration 698/1000 | Loss: 0.00059536
Iteration 699/1000 | Loss: 0.00057471
Iteration 700/1000 | Loss: 0.00063381
Iteration 701/1000 | Loss: 0.00075556
Iteration 702/1000 | Loss: 0.00060591
Iteration 703/1000 | Loss: 0.00053552
Iteration 704/1000 | Loss: 0.00056541
Iteration 705/1000 | Loss: 0.00063253
Iteration 706/1000 | Loss: 0.00055889
Iteration 707/1000 | Loss: 0.00060607
Iteration 708/1000 | Loss: 0.00057171
Iteration 709/1000 | Loss: 0.00071744
Iteration 710/1000 | Loss: 0.00057938
Iteration 711/1000 | Loss: 0.00059518
Iteration 712/1000 | Loss: 0.00053769
Iteration 713/1000 | Loss: 0.00049498
Iteration 714/1000 | Loss: 0.00055839
Iteration 715/1000 | Loss: 0.00055242
Iteration 716/1000 | Loss: 0.00085218
Iteration 717/1000 | Loss: 0.00060759
Iteration 718/1000 | Loss: 0.00068714
Iteration 719/1000 | Loss: 0.00068690
Iteration 720/1000 | Loss: 0.00039967
Iteration 721/1000 | Loss: 0.00050879
Iteration 722/1000 | Loss: 0.00057730
Iteration 723/1000 | Loss: 0.00054610
Iteration 724/1000 | Loss: 0.00048245
Iteration 725/1000 | Loss: 0.00058902
Iteration 726/1000 | Loss: 0.00064044
Iteration 727/1000 | Loss: 0.00055020
Iteration 728/1000 | Loss: 0.00060505
Iteration 729/1000 | Loss: 0.00058638
Iteration 730/1000 | Loss: 0.00058449
Iteration 731/1000 | Loss: 0.00050621
Iteration 732/1000 | Loss: 0.00056058
Iteration 733/1000 | Loss: 0.00062883
Iteration 734/1000 | Loss: 0.00072503
Iteration 735/1000 | Loss: 0.00053652
Iteration 736/1000 | Loss: 0.00037995
Iteration 737/1000 | Loss: 0.00047801
Iteration 738/1000 | Loss: 0.00050796
Iteration 739/1000 | Loss: 0.00067993
Iteration 740/1000 | Loss: 0.00059661
Iteration 741/1000 | Loss: 0.00062075
Iteration 742/1000 | Loss: 0.00069471
Iteration 743/1000 | Loss: 0.00080266
Iteration 744/1000 | Loss: 0.00067804
Iteration 745/1000 | Loss: 0.00055635
Iteration 746/1000 | Loss: 0.00063556
Iteration 747/1000 | Loss: 0.00062317
Iteration 748/1000 | Loss: 0.00073857
Iteration 749/1000 | Loss: 0.00064876
Iteration 750/1000 | Loss: 0.00047129
Iteration 751/1000 | Loss: 0.00028358
Iteration 752/1000 | Loss: 0.00079387
Iteration 753/1000 | Loss: 0.00050731
Iteration 754/1000 | Loss: 0.00034931
Iteration 755/1000 | Loss: 0.00055073
Iteration 756/1000 | Loss: 0.00058505
Iteration 757/1000 | Loss: 0.00060546
Iteration 758/1000 | Loss: 0.00062255
Iteration 759/1000 | Loss: 0.00061948
Iteration 760/1000 | Loss: 0.00058515
Iteration 761/1000 | Loss: 0.00067153
Iteration 762/1000 | Loss: 0.00061088
Iteration 763/1000 | Loss: 0.00061778
Iteration 764/1000 | Loss: 0.00061840
Iteration 765/1000 | Loss: 0.00061842
Iteration 766/1000 | Loss: 0.00073990
Iteration 767/1000 | Loss: 0.00053859
Iteration 768/1000 | Loss: 0.00049276
Iteration 769/1000 | Loss: 0.00063386
Iteration 770/1000 | Loss: 0.00074361
Iteration 771/1000 | Loss: 0.00065022
Iteration 772/1000 | Loss: 0.00057265
Iteration 773/1000 | Loss: 0.00077672
Iteration 774/1000 | Loss: 0.00053869
Iteration 775/1000 | Loss: 0.00142914
Iteration 776/1000 | Loss: 0.00059988
Iteration 777/1000 | Loss: 0.00046983
Iteration 778/1000 | Loss: 0.00050416
Iteration 779/1000 | Loss: 0.00027905
Iteration 780/1000 | Loss: 0.00052919
Iteration 781/1000 | Loss: 0.00047219
Iteration 782/1000 | Loss: 0.00077042
Iteration 783/1000 | Loss: 0.00041088
Iteration 784/1000 | Loss: 0.00044215
Iteration 785/1000 | Loss: 0.00048355
Iteration 786/1000 | Loss: 0.00041268
Iteration 787/1000 | Loss: 0.00044734
Iteration 788/1000 | Loss: 0.00058687
Iteration 789/1000 | Loss: 0.00049502
Iteration 790/1000 | Loss: 0.00063677
Iteration 791/1000 | Loss: 0.00058843
Iteration 792/1000 | Loss: 0.00077537
Iteration 793/1000 | Loss: 0.00035273
Iteration 794/1000 | Loss: 0.00025935
Iteration 795/1000 | Loss: 0.00019869
Iteration 796/1000 | Loss: 0.00026294
Iteration 797/1000 | Loss: 0.00030280
Iteration 798/1000 | Loss: 0.00034704
Iteration 799/1000 | Loss: 0.00024109
Iteration 800/1000 | Loss: 0.00031506
Iteration 801/1000 | Loss: 0.00011143
Iteration 802/1000 | Loss: 0.00033507
Iteration 803/1000 | Loss: 0.00023539
Iteration 804/1000 | Loss: 0.00033482
Iteration 805/1000 | Loss: 0.00027990
Iteration 806/1000 | Loss: 0.00032513
Iteration 807/1000 | Loss: 0.00030389
Iteration 808/1000 | Loss: 0.00028359
Iteration 809/1000 | Loss: 0.00032437
Iteration 810/1000 | Loss: 0.00037231
Iteration 811/1000 | Loss: 0.00032042
Iteration 812/1000 | Loss: 0.00017334
Iteration 813/1000 | Loss: 0.00015619
Iteration 814/1000 | Loss: 0.00023545
Iteration 815/1000 | Loss: 0.00023579
Iteration 816/1000 | Loss: 0.00022271
Iteration 817/1000 | Loss: 0.00020034
Iteration 818/1000 | Loss: 0.00025148
Iteration 819/1000 | Loss: 0.00025596
Iteration 820/1000 | Loss: 0.00013468
Iteration 821/1000 | Loss: 0.00005652
Iteration 822/1000 | Loss: 0.00001853
Iteration 823/1000 | Loss: 0.00002813
Iteration 824/1000 | Loss: 0.00005232
Iteration 825/1000 | Loss: 0.00001214
Iteration 826/1000 | Loss: 0.00002516
Iteration 827/1000 | Loss: 0.00002085
Iteration 828/1000 | Loss: 0.00001036
Iteration 829/1000 | Loss: 0.00000979
Iteration 830/1000 | Loss: 0.00000906
Iteration 831/1000 | Loss: 0.00000857
Iteration 832/1000 | Loss: 0.00000814
Iteration 833/1000 | Loss: 0.00000783
Iteration 834/1000 | Loss: 0.00000752
Iteration 835/1000 | Loss: 0.00000743
Iteration 836/1000 | Loss: 0.00000742
Iteration 837/1000 | Loss: 0.00000739
Iteration 838/1000 | Loss: 0.00000725
Iteration 839/1000 | Loss: 0.00000723
Iteration 840/1000 | Loss: 0.00000715
Iteration 841/1000 | Loss: 0.00000715
Iteration 842/1000 | Loss: 0.00000714
Iteration 843/1000 | Loss: 0.00000709
Iteration 844/1000 | Loss: 0.00000709
Iteration 845/1000 | Loss: 0.00000708
Iteration 846/1000 | Loss: 0.00000704
Iteration 847/1000 | Loss: 0.00000704
Iteration 848/1000 | Loss: 0.00000703
Iteration 849/1000 | Loss: 0.00000700
Iteration 850/1000 | Loss: 0.00000699
Iteration 851/1000 | Loss: 0.00000699
Iteration 852/1000 | Loss: 0.00000699
Iteration 853/1000 | Loss: 0.00000698
Iteration 854/1000 | Loss: 0.00000698
Iteration 855/1000 | Loss: 0.00000697
Iteration 856/1000 | Loss: 0.00000697
Iteration 857/1000 | Loss: 0.00000697
Iteration 858/1000 | Loss: 0.00000696
Iteration 859/1000 | Loss: 0.00000696
Iteration 860/1000 | Loss: 0.00000696
Iteration 861/1000 | Loss: 0.00000696
Iteration 862/1000 | Loss: 0.00000696
Iteration 863/1000 | Loss: 0.00000695
Iteration 864/1000 | Loss: 0.00000695
Iteration 865/1000 | Loss: 0.00000695
Iteration 866/1000 | Loss: 0.00000695
Iteration 867/1000 | Loss: 0.00000694
Iteration 868/1000 | Loss: 0.00000694
Iteration 869/1000 | Loss: 0.00000693
Iteration 870/1000 | Loss: 0.00000693
Iteration 871/1000 | Loss: 0.00000693
Iteration 872/1000 | Loss: 0.00000692
Iteration 873/1000 | Loss: 0.00000692
Iteration 874/1000 | Loss: 0.00000692
Iteration 875/1000 | Loss: 0.00000692
Iteration 876/1000 | Loss: 0.00000692
Iteration 877/1000 | Loss: 0.00000692
Iteration 878/1000 | Loss: 0.00000692
Iteration 879/1000 | Loss: 0.00000692
Iteration 880/1000 | Loss: 0.00000692
Iteration 881/1000 | Loss: 0.00000692
Iteration 882/1000 | Loss: 0.00000692
Iteration 883/1000 | Loss: 0.00000692
Iteration 884/1000 | Loss: 0.00000691
Iteration 885/1000 | Loss: 0.00000691
Iteration 886/1000 | Loss: 0.00000691
Iteration 887/1000 | Loss: 0.00000691
Iteration 888/1000 | Loss: 0.00000690
Iteration 889/1000 | Loss: 0.00000690
Iteration 890/1000 | Loss: 0.00000690
Iteration 891/1000 | Loss: 0.00000690
Iteration 892/1000 | Loss: 0.00000689
Iteration 893/1000 | Loss: 0.00000689
Iteration 894/1000 | Loss: 0.00000689
Iteration 895/1000 | Loss: 0.00000689
Iteration 896/1000 | Loss: 0.00000689
Iteration 897/1000 | Loss: 0.00000688
Iteration 898/1000 | Loss: 0.00000688
Iteration 899/1000 | Loss: 0.00000688
Iteration 900/1000 | Loss: 0.00000688
Iteration 901/1000 | Loss: 0.00000688
Iteration 902/1000 | Loss: 0.00000688
Iteration 903/1000 | Loss: 0.00000688
Iteration 904/1000 | Loss: 0.00000688
Iteration 905/1000 | Loss: 0.00000687
Iteration 906/1000 | Loss: 0.00000687
Iteration 907/1000 | Loss: 0.00000687
Iteration 908/1000 | Loss: 0.00000687
Iteration 909/1000 | Loss: 0.00000687
Iteration 910/1000 | Loss: 0.00000686
Iteration 911/1000 | Loss: 0.00000686
Iteration 912/1000 | Loss: 0.00000686
Iteration 913/1000 | Loss: 0.00000686
Iteration 914/1000 | Loss: 0.00000686
Iteration 915/1000 | Loss: 0.00000686
Iteration 916/1000 | Loss: 0.00000686
Iteration 917/1000 | Loss: 0.00000686
Iteration 918/1000 | Loss: 0.00000685
Iteration 919/1000 | Loss: 0.00000685
Iteration 920/1000 | Loss: 0.00000685
Iteration 921/1000 | Loss: 0.00000685
Iteration 922/1000 | Loss: 0.00000685
Iteration 923/1000 | Loss: 0.00000685
Iteration 924/1000 | Loss: 0.00000685
Iteration 925/1000 | Loss: 0.00000684
Iteration 926/1000 | Loss: 0.00000684
Iteration 927/1000 | Loss: 0.00000684
Iteration 928/1000 | Loss: 0.00000684
Iteration 929/1000 | Loss: 0.00000684
Iteration 930/1000 | Loss: 0.00000684
Iteration 931/1000 | Loss: 0.00000684
Iteration 932/1000 | Loss: 0.00000684
Iteration 933/1000 | Loss: 0.00000684
Iteration 934/1000 | Loss: 0.00000684
Iteration 935/1000 | Loss: 0.00000684
Iteration 936/1000 | Loss: 0.00000684
Iteration 937/1000 | Loss: 0.00000684
Iteration 938/1000 | Loss: 0.00000684
Iteration 939/1000 | Loss: 0.00000684
Iteration 940/1000 | Loss: 0.00000684
Iteration 941/1000 | Loss: 0.00000684
Iteration 942/1000 | Loss: 0.00000684
Iteration 943/1000 | Loss: 0.00000684
Iteration 944/1000 | Loss: 0.00000684
Iteration 945/1000 | Loss: 0.00000684
Iteration 946/1000 | Loss: 0.00000684
Iteration 947/1000 | Loss: 0.00000684
Iteration 948/1000 | Loss: 0.00000684
Iteration 949/1000 | Loss: 0.00000684
Iteration 950/1000 | Loss: 0.00000684
Iteration 951/1000 | Loss: 0.00000684
Iteration 952/1000 | Loss: 0.00000684
Iteration 953/1000 | Loss: 0.00000684
Iteration 954/1000 | Loss: 0.00000684
Iteration 955/1000 | Loss: 0.00000684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 955. Stopping optimization.
Last 5 losses: [6.835499334556516e-06, 6.835499334556516e-06, 6.835499334556516e-06, 6.835499334556516e-06, 6.835499334556516e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.835499334556516e-06

Optimization complete. Final v2v error: 2.2723636627197266 mm

Highest mean error: 3.2348875999450684 mm for frame 110

Lowest mean error: 2.164107084274292 mm for frame 3

Saving results

Total time: 1269.9825630187988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008337
Iteration 2/25 | Loss: 0.01008336
Iteration 3/25 | Loss: 0.00251922
Iteration 4/25 | Loss: 0.00197451
Iteration 5/25 | Loss: 0.00189127
Iteration 6/25 | Loss: 0.00174075
Iteration 7/25 | Loss: 0.00143828
Iteration 8/25 | Loss: 0.00127045
Iteration 9/25 | Loss: 0.00120538
Iteration 10/25 | Loss: 0.00117602
Iteration 11/25 | Loss: 0.00115995
Iteration 12/25 | Loss: 0.00116253
Iteration 13/25 | Loss: 0.00115744
Iteration 14/25 | Loss: 0.00114203
Iteration 15/25 | Loss: 0.00113423
Iteration 16/25 | Loss: 0.00113530
Iteration 17/25 | Loss: 0.00114221
Iteration 18/25 | Loss: 0.00113977
Iteration 19/25 | Loss: 0.00113717
Iteration 20/25 | Loss: 0.00112940
Iteration 21/25 | Loss: 0.00112779
Iteration 22/25 | Loss: 0.00112688
Iteration 23/25 | Loss: 0.00112344
Iteration 24/25 | Loss: 0.00112291
Iteration 25/25 | Loss: 0.00112347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31265354
Iteration 2/25 | Loss: 0.00055647
Iteration 3/25 | Loss: 0.00055647
Iteration 4/25 | Loss: 0.00055647
Iteration 5/25 | Loss: 0.00055647
Iteration 6/25 | Loss: 0.00055647
Iteration 7/25 | Loss: 0.00055647
Iteration 8/25 | Loss: 0.00055647
Iteration 9/25 | Loss: 0.00055647
Iteration 10/25 | Loss: 0.00055647
Iteration 11/25 | Loss: 0.00055647
Iteration 12/25 | Loss: 0.00055647
Iteration 13/25 | Loss: 0.00055647
Iteration 14/25 | Loss: 0.00055647
Iteration 15/25 | Loss: 0.00055647
Iteration 16/25 | Loss: 0.00055647
Iteration 17/25 | Loss: 0.00055647
Iteration 18/25 | Loss: 0.00055647
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005564662278629839, 0.0005564662278629839, 0.0005564662278629839, 0.0005564662278629839, 0.0005564662278629839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005564662278629839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055647
Iteration 2/1000 | Loss: 0.00002963
Iteration 3/1000 | Loss: 0.00003019
Iteration 4/1000 | Loss: 0.00002492
Iteration 5/1000 | Loss: 0.00003186
Iteration 6/1000 | Loss: 0.00003077
Iteration 7/1000 | Loss: 0.00002189
Iteration 8/1000 | Loss: 0.00003076
Iteration 9/1000 | Loss: 0.00002281
Iteration 10/1000 | Loss: 0.00002791
Iteration 11/1000 | Loss: 0.00003353
Iteration 12/1000 | Loss: 0.00002777
Iteration 13/1000 | Loss: 0.00003596
Iteration 14/1000 | Loss: 0.00002239
Iteration 15/1000 | Loss: 0.00001848
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001558
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001502
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001484
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001459
Iteration 32/1000 | Loss: 0.00001455
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001449
Iteration 36/1000 | Loss: 0.00001449
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001446
Iteration 41/1000 | Loss: 0.00001446
Iteration 42/1000 | Loss: 0.00001446
Iteration 43/1000 | Loss: 0.00001446
Iteration 44/1000 | Loss: 0.00001446
Iteration 45/1000 | Loss: 0.00001443
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001441
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001440
Iteration 50/1000 | Loss: 0.00001440
Iteration 51/1000 | Loss: 0.00001439
Iteration 52/1000 | Loss: 0.00001439
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00001438
Iteration 55/1000 | Loss: 0.00001438
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001437
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001437
Iteration 61/1000 | Loss: 0.00001437
Iteration 62/1000 | Loss: 0.00001437
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001436
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001436
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001434
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001434
Iteration 77/1000 | Loss: 0.00001434
Iteration 78/1000 | Loss: 0.00001434
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001432
Iteration 83/1000 | Loss: 0.00001432
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001429
Iteration 87/1000 | Loss: 0.00001429
Iteration 88/1000 | Loss: 0.00001429
Iteration 89/1000 | Loss: 0.00001429
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001429
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001426
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001426
Iteration 112/1000 | Loss: 0.00001425
Iteration 113/1000 | Loss: 0.00001425
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001424
Iteration 116/1000 | Loss: 0.00001424
Iteration 117/1000 | Loss: 0.00001424
Iteration 118/1000 | Loss: 0.00001423
Iteration 119/1000 | Loss: 0.00001423
Iteration 120/1000 | Loss: 0.00001423
Iteration 121/1000 | Loss: 0.00001423
Iteration 122/1000 | Loss: 0.00001423
Iteration 123/1000 | Loss: 0.00001423
Iteration 124/1000 | Loss: 0.00001422
Iteration 125/1000 | Loss: 0.00001422
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001420
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001420
Iteration 134/1000 | Loss: 0.00001420
Iteration 135/1000 | Loss: 0.00001420
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00001419
Iteration 140/1000 | Loss: 0.00001419
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001418
Iteration 143/1000 | Loss: 0.00001418
Iteration 144/1000 | Loss: 0.00001418
Iteration 145/1000 | Loss: 0.00001418
Iteration 146/1000 | Loss: 0.00001418
Iteration 147/1000 | Loss: 0.00001418
Iteration 148/1000 | Loss: 0.00001417
Iteration 149/1000 | Loss: 0.00001417
Iteration 150/1000 | Loss: 0.00001417
Iteration 151/1000 | Loss: 0.00001417
Iteration 152/1000 | Loss: 0.00001417
Iteration 153/1000 | Loss: 0.00001417
Iteration 154/1000 | Loss: 0.00001417
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001416
Iteration 157/1000 | Loss: 0.00001416
Iteration 158/1000 | Loss: 0.00001416
Iteration 159/1000 | Loss: 0.00001416
Iteration 160/1000 | Loss: 0.00001416
Iteration 161/1000 | Loss: 0.00001416
Iteration 162/1000 | Loss: 0.00001416
Iteration 163/1000 | Loss: 0.00001416
Iteration 164/1000 | Loss: 0.00001415
Iteration 165/1000 | Loss: 0.00001415
Iteration 166/1000 | Loss: 0.00001415
Iteration 167/1000 | Loss: 0.00001415
Iteration 168/1000 | Loss: 0.00001415
Iteration 169/1000 | Loss: 0.00001415
Iteration 170/1000 | Loss: 0.00001415
Iteration 171/1000 | Loss: 0.00001415
Iteration 172/1000 | Loss: 0.00001415
Iteration 173/1000 | Loss: 0.00001415
Iteration 174/1000 | Loss: 0.00001415
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001415
Iteration 177/1000 | Loss: 0.00001415
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.4153380107018165e-05, 1.4153380107018165e-05, 1.4153380107018165e-05, 1.4153380107018165e-05, 1.4153380107018165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4153380107018165e-05

Optimization complete. Final v2v error: 3.150529623031616 mm

Highest mean error: 3.836649179458618 mm for frame 50

Lowest mean error: 3.000627040863037 mm for frame 11

Saving results

Total time: 99.46713614463806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792454
Iteration 2/25 | Loss: 0.00117583
Iteration 3/25 | Loss: 0.00107849
Iteration 4/25 | Loss: 0.00106865
Iteration 5/25 | Loss: 0.00106653
Iteration 6/25 | Loss: 0.00106649
Iteration 7/25 | Loss: 0.00106649
Iteration 8/25 | Loss: 0.00106649
Iteration 9/25 | Loss: 0.00106649
Iteration 10/25 | Loss: 0.00106649
Iteration 11/25 | Loss: 0.00106649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010664929868653417, 0.0010664929868653417, 0.0010664929868653417, 0.0010664929868653417, 0.0010664929868653417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010664929868653417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35436332
Iteration 2/25 | Loss: 0.00081319
Iteration 3/25 | Loss: 0.00081319
Iteration 4/25 | Loss: 0.00081319
Iteration 5/25 | Loss: 0.00081319
Iteration 6/25 | Loss: 0.00081319
Iteration 7/25 | Loss: 0.00081319
Iteration 8/25 | Loss: 0.00081319
Iteration 9/25 | Loss: 0.00081318
Iteration 10/25 | Loss: 0.00081318
Iteration 11/25 | Loss: 0.00081318
Iteration 12/25 | Loss: 0.00081318
Iteration 13/25 | Loss: 0.00081318
Iteration 14/25 | Loss: 0.00081318
Iteration 15/25 | Loss: 0.00081318
Iteration 16/25 | Loss: 0.00081318
Iteration 17/25 | Loss: 0.00081318
Iteration 18/25 | Loss: 0.00081318
Iteration 19/25 | Loss: 0.00081318
Iteration 20/25 | Loss: 0.00081318
Iteration 21/25 | Loss: 0.00081318
Iteration 22/25 | Loss: 0.00081318
Iteration 23/25 | Loss: 0.00081318
Iteration 24/25 | Loss: 0.00081318
Iteration 25/25 | Loss: 0.00081318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081318
Iteration 2/1000 | Loss: 0.00002071
Iteration 3/1000 | Loss: 0.00001274
Iteration 4/1000 | Loss: 0.00001132
Iteration 5/1000 | Loss: 0.00001067
Iteration 6/1000 | Loss: 0.00001013
Iteration 7/1000 | Loss: 0.00000975
Iteration 8/1000 | Loss: 0.00000967
Iteration 9/1000 | Loss: 0.00000953
Iteration 10/1000 | Loss: 0.00000931
Iteration 11/1000 | Loss: 0.00000920
Iteration 12/1000 | Loss: 0.00000916
Iteration 13/1000 | Loss: 0.00000916
Iteration 14/1000 | Loss: 0.00000914
Iteration 15/1000 | Loss: 0.00000914
Iteration 16/1000 | Loss: 0.00000911
Iteration 17/1000 | Loss: 0.00000910
Iteration 18/1000 | Loss: 0.00000910
Iteration 19/1000 | Loss: 0.00000901
Iteration 20/1000 | Loss: 0.00000899
Iteration 21/1000 | Loss: 0.00000899
Iteration 22/1000 | Loss: 0.00000898
Iteration 23/1000 | Loss: 0.00000898
Iteration 24/1000 | Loss: 0.00000898
Iteration 25/1000 | Loss: 0.00000898
Iteration 26/1000 | Loss: 0.00000898
Iteration 27/1000 | Loss: 0.00000898
Iteration 28/1000 | Loss: 0.00000897
Iteration 29/1000 | Loss: 0.00000893
Iteration 30/1000 | Loss: 0.00000892
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000891
Iteration 33/1000 | Loss: 0.00000889
Iteration 34/1000 | Loss: 0.00000888
Iteration 35/1000 | Loss: 0.00000888
Iteration 36/1000 | Loss: 0.00000887
Iteration 37/1000 | Loss: 0.00000887
Iteration 38/1000 | Loss: 0.00000886
Iteration 39/1000 | Loss: 0.00000885
Iteration 40/1000 | Loss: 0.00000884
Iteration 41/1000 | Loss: 0.00000884
Iteration 42/1000 | Loss: 0.00000884
Iteration 43/1000 | Loss: 0.00000884
Iteration 44/1000 | Loss: 0.00000883
Iteration 45/1000 | Loss: 0.00000883
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000879
Iteration 51/1000 | Loss: 0.00000878
Iteration 52/1000 | Loss: 0.00000878
Iteration 53/1000 | Loss: 0.00000878
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000878
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000876
Iteration 63/1000 | Loss: 0.00000876
Iteration 64/1000 | Loss: 0.00000876
Iteration 65/1000 | Loss: 0.00000875
Iteration 66/1000 | Loss: 0.00000873
Iteration 67/1000 | Loss: 0.00000873
Iteration 68/1000 | Loss: 0.00000872
Iteration 69/1000 | Loss: 0.00000872
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000870
Iteration 72/1000 | Loss: 0.00000870
Iteration 73/1000 | Loss: 0.00000869
Iteration 74/1000 | Loss: 0.00000869
Iteration 75/1000 | Loss: 0.00000869
Iteration 76/1000 | Loss: 0.00000869
Iteration 77/1000 | Loss: 0.00000868
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000868
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000867
Iteration 82/1000 | Loss: 0.00000867
Iteration 83/1000 | Loss: 0.00000866
Iteration 84/1000 | Loss: 0.00000866
Iteration 85/1000 | Loss: 0.00000866
Iteration 86/1000 | Loss: 0.00000866
Iteration 87/1000 | Loss: 0.00000866
Iteration 88/1000 | Loss: 0.00000866
Iteration 89/1000 | Loss: 0.00000866
Iteration 90/1000 | Loss: 0.00000865
Iteration 91/1000 | Loss: 0.00000865
Iteration 92/1000 | Loss: 0.00000865
Iteration 93/1000 | Loss: 0.00000865
Iteration 94/1000 | Loss: 0.00000865
Iteration 95/1000 | Loss: 0.00000865
Iteration 96/1000 | Loss: 0.00000865
Iteration 97/1000 | Loss: 0.00000865
Iteration 98/1000 | Loss: 0.00000865
Iteration 99/1000 | Loss: 0.00000864
Iteration 100/1000 | Loss: 0.00000864
Iteration 101/1000 | Loss: 0.00000864
Iteration 102/1000 | Loss: 0.00000864
Iteration 103/1000 | Loss: 0.00000864
Iteration 104/1000 | Loss: 0.00000864
Iteration 105/1000 | Loss: 0.00000863
Iteration 106/1000 | Loss: 0.00000863
Iteration 107/1000 | Loss: 0.00000863
Iteration 108/1000 | Loss: 0.00000863
Iteration 109/1000 | Loss: 0.00000863
Iteration 110/1000 | Loss: 0.00000863
Iteration 111/1000 | Loss: 0.00000863
Iteration 112/1000 | Loss: 0.00000862
Iteration 113/1000 | Loss: 0.00000862
Iteration 114/1000 | Loss: 0.00000862
Iteration 115/1000 | Loss: 0.00000862
Iteration 116/1000 | Loss: 0.00000862
Iteration 117/1000 | Loss: 0.00000861
Iteration 118/1000 | Loss: 0.00000861
Iteration 119/1000 | Loss: 0.00000861
Iteration 120/1000 | Loss: 0.00000861
Iteration 121/1000 | Loss: 0.00000861
Iteration 122/1000 | Loss: 0.00000860
Iteration 123/1000 | Loss: 0.00000860
Iteration 124/1000 | Loss: 0.00000860
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000859
Iteration 128/1000 | Loss: 0.00000859
Iteration 129/1000 | Loss: 0.00000859
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000858
Iteration 139/1000 | Loss: 0.00000858
Iteration 140/1000 | Loss: 0.00000858
Iteration 141/1000 | Loss: 0.00000858
Iteration 142/1000 | Loss: 0.00000858
Iteration 143/1000 | Loss: 0.00000857
Iteration 144/1000 | Loss: 0.00000857
Iteration 145/1000 | Loss: 0.00000856
Iteration 146/1000 | Loss: 0.00000856
Iteration 147/1000 | Loss: 0.00000856
Iteration 148/1000 | Loss: 0.00000856
Iteration 149/1000 | Loss: 0.00000856
Iteration 150/1000 | Loss: 0.00000856
Iteration 151/1000 | Loss: 0.00000856
Iteration 152/1000 | Loss: 0.00000856
Iteration 153/1000 | Loss: 0.00000856
Iteration 154/1000 | Loss: 0.00000856
Iteration 155/1000 | Loss: 0.00000856
Iteration 156/1000 | Loss: 0.00000856
Iteration 157/1000 | Loss: 0.00000856
Iteration 158/1000 | Loss: 0.00000856
Iteration 159/1000 | Loss: 0.00000856
Iteration 160/1000 | Loss: 0.00000856
Iteration 161/1000 | Loss: 0.00000856
Iteration 162/1000 | Loss: 0.00000856
Iteration 163/1000 | Loss: 0.00000856
Iteration 164/1000 | Loss: 0.00000856
Iteration 165/1000 | Loss: 0.00000856
Iteration 166/1000 | Loss: 0.00000856
Iteration 167/1000 | Loss: 0.00000856
Iteration 168/1000 | Loss: 0.00000855
Iteration 169/1000 | Loss: 0.00000855
Iteration 170/1000 | Loss: 0.00000855
Iteration 171/1000 | Loss: 0.00000855
Iteration 172/1000 | Loss: 0.00000855
Iteration 173/1000 | Loss: 0.00000855
Iteration 174/1000 | Loss: 0.00000855
Iteration 175/1000 | Loss: 0.00000855
Iteration 176/1000 | Loss: 0.00000855
Iteration 177/1000 | Loss: 0.00000855
Iteration 178/1000 | Loss: 0.00000855
Iteration 179/1000 | Loss: 0.00000854
Iteration 180/1000 | Loss: 0.00000854
Iteration 181/1000 | Loss: 0.00000854
Iteration 182/1000 | Loss: 0.00000854
Iteration 183/1000 | Loss: 0.00000854
Iteration 184/1000 | Loss: 0.00000854
Iteration 185/1000 | Loss: 0.00000854
Iteration 186/1000 | Loss: 0.00000854
Iteration 187/1000 | Loss: 0.00000854
Iteration 188/1000 | Loss: 0.00000854
Iteration 189/1000 | Loss: 0.00000854
Iteration 190/1000 | Loss: 0.00000854
Iteration 191/1000 | Loss: 0.00000854
Iteration 192/1000 | Loss: 0.00000854
Iteration 193/1000 | Loss: 0.00000854
Iteration 194/1000 | Loss: 0.00000854
Iteration 195/1000 | Loss: 0.00000854
Iteration 196/1000 | Loss: 0.00000854
Iteration 197/1000 | Loss: 0.00000854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [8.541662282368634e-06, 8.541662282368634e-06, 8.541662282368634e-06, 8.541662282368634e-06, 8.541662282368634e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.541662282368634e-06

Optimization complete. Final v2v error: 2.4914071559906006 mm

Highest mean error: 2.6711013317108154 mm for frame 66

Lowest mean error: 2.339887857437134 mm for frame 149

Saving results

Total time: 37.00707721710205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_005/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_005/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115264
Iteration 2/25 | Loss: 0.00173362
Iteration 3/25 | Loss: 0.00131870
Iteration 4/25 | Loss: 0.00127886
Iteration 5/25 | Loss: 0.00127145
Iteration 6/25 | Loss: 0.00126929
Iteration 7/25 | Loss: 0.00126896
Iteration 8/25 | Loss: 0.00126896
Iteration 9/25 | Loss: 0.00126896
Iteration 10/25 | Loss: 0.00126896
Iteration 11/25 | Loss: 0.00126896
Iteration 12/25 | Loss: 0.00126896
Iteration 13/25 | Loss: 0.00126896
Iteration 14/25 | Loss: 0.00126896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012689630966633558, 0.0012689630966633558, 0.0012689630966633558, 0.0012689630966633558, 0.0012689630966633558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012689630966633558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96505678
Iteration 2/25 | Loss: 0.00102663
Iteration 3/25 | Loss: 0.00102659
Iteration 4/25 | Loss: 0.00102659
Iteration 5/25 | Loss: 0.00102659
Iteration 6/25 | Loss: 0.00102659
Iteration 7/25 | Loss: 0.00102659
Iteration 8/25 | Loss: 0.00102659
Iteration 9/25 | Loss: 0.00102659
Iteration 10/25 | Loss: 0.00102659
Iteration 11/25 | Loss: 0.00102659
Iteration 12/25 | Loss: 0.00102659
Iteration 13/25 | Loss: 0.00102659
Iteration 14/25 | Loss: 0.00102659
Iteration 15/25 | Loss: 0.00102659
Iteration 16/25 | Loss: 0.00102659
Iteration 17/25 | Loss: 0.00102659
Iteration 18/25 | Loss: 0.00102659
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010265903547406197, 0.0010265903547406197, 0.0010265903547406197, 0.0010265903547406197, 0.0010265903547406197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010265903547406197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102659
Iteration 2/1000 | Loss: 0.00007400
Iteration 3/1000 | Loss: 0.00004600
Iteration 4/1000 | Loss: 0.00003383
Iteration 5/1000 | Loss: 0.00003104
Iteration 6/1000 | Loss: 0.00002919
Iteration 7/1000 | Loss: 0.00002838
Iteration 8/1000 | Loss: 0.00002756
Iteration 9/1000 | Loss: 0.00002708
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002634
Iteration 12/1000 | Loss: 0.00002611
Iteration 13/1000 | Loss: 0.00002592
Iteration 14/1000 | Loss: 0.00002573
Iteration 15/1000 | Loss: 0.00002572
Iteration 16/1000 | Loss: 0.00002562
Iteration 17/1000 | Loss: 0.00002556
Iteration 18/1000 | Loss: 0.00002546
Iteration 19/1000 | Loss: 0.00002539
Iteration 20/1000 | Loss: 0.00002535
Iteration 21/1000 | Loss: 0.00002530
Iteration 22/1000 | Loss: 0.00002530
Iteration 23/1000 | Loss: 0.00002529
Iteration 24/1000 | Loss: 0.00002528
Iteration 25/1000 | Loss: 0.00002526
Iteration 26/1000 | Loss: 0.00002525
Iteration 27/1000 | Loss: 0.00002523
Iteration 28/1000 | Loss: 0.00002521
Iteration 29/1000 | Loss: 0.00002520
Iteration 30/1000 | Loss: 0.00002519
Iteration 31/1000 | Loss: 0.00002519
Iteration 32/1000 | Loss: 0.00002518
Iteration 33/1000 | Loss: 0.00002517
Iteration 34/1000 | Loss: 0.00002516
Iteration 35/1000 | Loss: 0.00002516
Iteration 36/1000 | Loss: 0.00002515
Iteration 37/1000 | Loss: 0.00002515
Iteration 38/1000 | Loss: 0.00002513
Iteration 39/1000 | Loss: 0.00002513
Iteration 40/1000 | Loss: 0.00002512
Iteration 41/1000 | Loss: 0.00002512
Iteration 42/1000 | Loss: 0.00002509
Iteration 43/1000 | Loss: 0.00002508
Iteration 44/1000 | Loss: 0.00002508
Iteration 45/1000 | Loss: 0.00002508
Iteration 46/1000 | Loss: 0.00002506
Iteration 47/1000 | Loss: 0.00002506
Iteration 48/1000 | Loss: 0.00002505
Iteration 49/1000 | Loss: 0.00002504
Iteration 50/1000 | Loss: 0.00002504
Iteration 51/1000 | Loss: 0.00002504
Iteration 52/1000 | Loss: 0.00002504
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002503
Iteration 56/1000 | Loss: 0.00002503
Iteration 57/1000 | Loss: 0.00002503
Iteration 58/1000 | Loss: 0.00002503
Iteration 59/1000 | Loss: 0.00002502
Iteration 60/1000 | Loss: 0.00002502
Iteration 61/1000 | Loss: 0.00002502
Iteration 62/1000 | Loss: 0.00002501
Iteration 63/1000 | Loss: 0.00002501
Iteration 64/1000 | Loss: 0.00002501
Iteration 65/1000 | Loss: 0.00002500
Iteration 66/1000 | Loss: 0.00002500
Iteration 67/1000 | Loss: 0.00002500
Iteration 68/1000 | Loss: 0.00002500
Iteration 69/1000 | Loss: 0.00002500
Iteration 70/1000 | Loss: 0.00002500
Iteration 71/1000 | Loss: 0.00002500
Iteration 72/1000 | Loss: 0.00002499
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002499
Iteration 76/1000 | Loss: 0.00002499
Iteration 77/1000 | Loss: 0.00002499
Iteration 78/1000 | Loss: 0.00002499
Iteration 79/1000 | Loss: 0.00002499
Iteration 80/1000 | Loss: 0.00002499
Iteration 81/1000 | Loss: 0.00002499
Iteration 82/1000 | Loss: 0.00002499
Iteration 83/1000 | Loss: 0.00002499
Iteration 84/1000 | Loss: 0.00002499
Iteration 85/1000 | Loss: 0.00002499
Iteration 86/1000 | Loss: 0.00002498
Iteration 87/1000 | Loss: 0.00002498
Iteration 88/1000 | Loss: 0.00002498
Iteration 89/1000 | Loss: 0.00002498
Iteration 90/1000 | Loss: 0.00002498
Iteration 91/1000 | Loss: 0.00002497
Iteration 92/1000 | Loss: 0.00002497
Iteration 93/1000 | Loss: 0.00002497
Iteration 94/1000 | Loss: 0.00002497
Iteration 95/1000 | Loss: 0.00002497
Iteration 96/1000 | Loss: 0.00002497
Iteration 97/1000 | Loss: 0.00002497
Iteration 98/1000 | Loss: 0.00002497
Iteration 99/1000 | Loss: 0.00002497
Iteration 100/1000 | Loss: 0.00002497
Iteration 101/1000 | Loss: 0.00002497
Iteration 102/1000 | Loss: 0.00002497
Iteration 103/1000 | Loss: 0.00002497
Iteration 104/1000 | Loss: 0.00002497
Iteration 105/1000 | Loss: 0.00002497
Iteration 106/1000 | Loss: 0.00002497
Iteration 107/1000 | Loss: 0.00002497
Iteration 108/1000 | Loss: 0.00002497
Iteration 109/1000 | Loss: 0.00002497
Iteration 110/1000 | Loss: 0.00002497
Iteration 111/1000 | Loss: 0.00002497
Iteration 112/1000 | Loss: 0.00002497
Iteration 113/1000 | Loss: 0.00002497
Iteration 114/1000 | Loss: 0.00002497
Iteration 115/1000 | Loss: 0.00002497
Iteration 116/1000 | Loss: 0.00002497
Iteration 117/1000 | Loss: 0.00002497
Iteration 118/1000 | Loss: 0.00002497
Iteration 119/1000 | Loss: 0.00002497
Iteration 120/1000 | Loss: 0.00002497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.4971383027150296e-05, 2.4971383027150296e-05, 2.4971383027150296e-05, 2.4971383027150296e-05, 2.4971383027150296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4971383027150296e-05

Optimization complete. Final v2v error: 4.1123247146606445 mm

Highest mean error: 5.4780778884887695 mm for frame 67

Lowest mean error: 3.2380571365356445 mm for frame 27

Saving results

Total time: 49.49286723136902
